% This paper uses the template
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
\usepackage[letterpaper]{geometry}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage[utf8]{inputenc}
\usepackage{amsmath,mathtools}
\let\proof\relax\let\endproof\relax
\usepackage{amsthm}
\usepackage{color}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{cite}
\usepackage{url}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{lipsum,booktabs}

%% version control
% \usepackage[markup=default]{changes}
% Use "final" option to remove all tracking markups
\usepackage[final]{changes}
%%in \replaced{}{} 2nd option (the underlined version) drops

% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\renewcommand\UrlFont{\color{blue}\rmfamily}

\DeclareMathOperator\supp{supp}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\vc}[1]{\mathbf{#1}}
\newcommand{\cvx}{\texttt{conv}}
\newcommand{\iset}[2]{\mathcal{#1}_{#2}}
\newtheorem{assumption}{Assumption}
%% Rather hacky definition of an "annote"
%% by riding on \added
% \newcommand{\note}[2][]{\added[#1,comment={#2}]{}}

%Uncomment in short version (PAKDD - anonym)
% \newcommand{\fullversion}[2]{#1}

%Uncomment in full version (arXiv)
\newcommand{\fullversion}[2]{#2}

% \hyphenation{op-tical net-works semi-conduc-tor}



\begin{document}

%
\newcommand\relatedversion{}
% \renewcommand\relatedversion{\thanks{The full version of the paper can be accessed at \protect\url{https://arxiv.org/abs/XXXX.XXXXX}}} % Replace URL with link to full paper or comment out this line

\makeatletter
\def\thanks#1{\protected@xdef\@thanks{\@thanks
        \protect\footnotetext{#1}}}
\makeatother

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Stochastic Submodular Maximization \\via Polynomial Estimators}%\thanks{Supported by NSF grant CCF-1750539.}\relatedversion}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{G\"{o}zde \"{O}zcan\inst{}\orcidID{0000-0002-2957-6893}
\and Stratis Ioannidis\inst{}\orcidID{0000-0001-8355-4751}}


\authorrunning{\replaced{G. \"{O}zcan and S. Ioannidis}{Anonymous Authors}}

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Electrical and Computer Engineering Department, Northeastern University, Boston MA 02115, USA\\
\email{\{gozcan, ioannidis\}@ece.neu.edu}}


% \renewcommand{\shortauthors}{\"{O}zcan and Ioannidis}
\maketitle
% typeset the header of the contribution
%
    
\begin{abstract}
\input{01_abstract}
\end{abstract}

\section{Introduction}
\input{01_introduction}

\section{Related Work}
\input{02_related}

\section{Technical Preliminary}
\input{03_technical.tex}

\section{Main Results}


\subsection{Polynomial Estimator}
To leverage Lem.~\ref{lem:relaxation_of_multi} to the case of stochastic submodular functions, we make the following  assumption:
\begin{assumption} \label{asmp:boundedPoly}
    For all $z %\textcolor{red}{ \sim P}$
    \in V_z$, there exists a sequence of polynomials $\{\hat{f}_z^L\}_{L=1}^{\infty}$, $\hat{f}_z^L: \mathbb{R}^n \rightarrow \mathbb{R}$ such that  
    %\begin{align}
    $\lim_{L \rightarrow \infty} |f_z(\mathbf{x}) - \hat{f}_z^L(\mathbf{x})| = 0,$ uniformly over $\mathbf{x} \in \{0, 1\}^n,$
    %\end{align}
    i.e. there exists $\varepsilon_z (L) \geq 0$ such that $\lim_{L \rightarrow \infty} \varepsilon_z (L) = 0$ and $|f_z(\mathbf{x}) - \hat{f}_z^L(\mathbf{x})| \leq \varepsilon_z(L)$,  for all $\mathbf{x} \in \{0, 1\}^n$.
    % There exists a polynomial  $\hat{f_z^L}: [a, b]^n \rightarrow \mathbb{R}$ of degree $L$ for $L \in \mathbb{N}$, such that $|f_z(\mathbf{x}) - \hat{f_z^L}(\mathbf{x})| \leq \varepsilon_L$, where $\lim_{L \rightarrow \infty} \varepsilon_L = 0$, for all $\mathbf{x} \in [a, b]^n$.
\end{assumption}
%The existence of such polynomials can be guaranteed by the Stone-Weierstrass theorem. 


% Moreover, these polynomials can be described as the following
%Moreover, when defined over the set $\{0, 1\}^n$, function $\hat{f_z^L}: \{0, 1\}^n \rightarrow \mathbb{R}$ can be expressed as a multilinear function. 
% For each polynomial function described in the format above, we can define the multilinear version of that function by limiting $k_i \leq 1$ as the following


 In other words, we assume that we can asymptotically approximate every function $f_z$ with a polynomial arbitrarily well. Note that there already exists a polynomial function that approximates each $f_z$ \emph{perfectly} (i.e., $\epsilon_z=0$), namely, its multilinear relaxation $G_z$. However, the number of terms in this polynomial is exponential in $n$. In contrast, Asm.~\ref{asmp:boundedPoly} requires exact recovery only asymptotically. In many cases, this allows us to construct polynomials with only a handful (i.e., polynomial in $n$) terms, that can approximate $f_z$. We will indeed present such polynomials for several applications of interest in Section~\ref{sec: examples}.
 %
 Armed with this assumption, we define an estimator $\widehat{\nabla G_z^L}$ of the gradient  of the multilinear relaxation $G$ as follows:
\begin{equation}
    \begin{split}
        \frac{\widehat{{\partial G_z^L}}}{\partial y_i}\big|_{\vc{y}} &\equiv \mathbb{E}_{\vc{x}\sim\vc{y}}[\hat{f}_z^{L}([\vc{x}]_{+i})] - \mathbb{E}_{\vc{x}\sim\vc{y}}[\hat{f}_z^{L}([\vc{x}]_{-i})]  %\stackrel{\text{Eq.}~\ref{eq:multilin_rel}}{=} \mathbb{E}_{\vc{x}\sim\vc{y}}[\dot{\hat{f}}_z^{L}([\vc{x}]_{+i})] - \mathbb{E}_{\vc{x}\sim\vc{y}}[\dot{\hat{f}}_z^{L}([\vc{x}]_{-i})]  \\
         \stackrel{\text{Lem.}~\ref{lem:relaxation_of_multi}}{=} \dot{\hat{f}}_z^{L}([\vc{y}]_{+i}) - \dot{\hat{f}}_z^{L}([\vc{y}]_{-i}), \text{ for all $i \in V$}.\label{eq: poly_estimator}
    \end{split}
\end{equation}
In other words, our estimator is constructed by replacing the multilinear relaxation $G_z$ in Eq.~\eqref{eq:partialGrad} with the multilinear relaxation of the approximating polynomial $\hat{f}_z$. In turn,  by Lem.~\ref{lem:relaxation_of_multi}, \emph{the latter can be computed deterministically  (without any sampling of the Bernoulli variables $\mathbf{x}\sim \mathbf{y}$)}, in closed form: the latter is given by the multilinearization  $\dot{\hat{f}}_z^L$ of polynomial ${\hat{f}}_z^L$.

Nevertheless, our deterministic estimator given by Eq.~\eqref{eq: poly_estimator} has a \emph{bias}, precisely because of our approximation of $f_z$ via the polynomial $\hat{f}_z^L$. We characterize this bias via the following lemma:
\begin{lemma} \label{lem:gradientBias}
% might be moved to appendix
Assume that function $f_z$ satisfies Asm.~\ref{asmp:boundedPoly}. Let $\nabla G_z$ be the unbiased stochastic gradient for a given $f_z$ and let $\widehat{\nabla G_z^L}$ be the estimator of the multilinear relaxation given by \eqref{eq: poly_estimator}. Then, 
%\begin{equation}\label{eq:estimator_bound}
 $   \big\|\nabla G_z(\vc{y}) - \widehat{\nabla G_z^L}(\vc{y})\big\|_2 \leq 2\sqrt{n}\varepsilon_z (L),$ for all $\mathbf{y} \in \mathcal{C}$.
%\end{equation}
% where $\epsilon_z(L) = [\epsilon_{i, z}(L)]_{i=1}^n \in \reals^n$ and
% \begin{equation}
%     \epsilon_{i, z}(L) \triangleq \mathbb{E}_{\vc{y}}[\varepsilon (L)_{+i})]+\mathbb{E}_{\vc{y}}[\varepsilon (L)_{-i})].
% \end{equation}
% Moreover, $\lim_{L \to \infty} \|\epsilon_z(L)\|_2= 0,$ uniformly on $[0,1]^n$.
\end{lemma}

The proof can be found in App.~\ref{app:proof_gradBiasLemma}\deleted{of the supplement}. Hence, we can approximate $\nabla G$ arbitrarily well, uniformly over all $\vc{x}\in[0,1]^n$.
% \begin{assumption} \label{asm:Lipschitz}
%     The function $G$ is $DR-$submodular and monotone. Further, its gradients are $L-$Lipschitz continuous over the set $\mathcal{X}$, i.e., for all $\mathbf{x}$, $\mathbf{y} \in \mathcal{X}$ $$\|\nabla G(\mathbf{x}) - \nabla G(\mathbf{y})\| \leq L\|\mathbf{x} - \mathbf{y}\|.$$
% \end{assumption}
We can thus use our estimator in the SCG algorithm instead of of the sample estimator of the gradient (Eq.~\eqref{eq:sampleEst}). %To characterize the end-to-end behavior of SCG under our estimation, we make one final additional assumption:
%\begin{assumption} \label{asm:boundedVar0}
%    The variance of the unbiased stochastic gradients $G_{z}(\mathbf{y})$ is bounded above by $\sigma_0^2$, i.e., for any vector $\mathbf{y} \in \mathcal{C}$, there exists a $\sigma_0^2$ s.t. 
%    $$\mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right\|^2 \right] \leq \sigma_0^2, $$ 
%    where the expectation is with respect to the randomness of $z \sim P$.
%\end{assumption}
%Note that the variance is this assumption is fully governed by $z \sim P$: in contrast to the sample estimator, the the variability due to sampling $\mathbf{x}\sim\mathbf{y}$ bears no role in this assumption, nor in our final guarantee, that 
We prove that this yields the following guarantee:
\begin{theorem}\label{thm:main}
    % \hl{\textup{[Main Theorem]}} 
    Consider Stochastic Continuous Greedy (SCG) outlined in Algorithm~\ref{alg: SCG}. Recall the definition of the multilinear extension function $G$ in \eqref{eq: multilinear}%and the definitions of $r$ and $m_f$ in Lemma ?
    . If Asm.~\ref{asm:monSub}  %~\ref{asm:Lipschitz}\&~\ref{asm:boundedVar0} 
    is satisfied and $\rho_t = 4/(t+8)^{2/3}$%, and the function $f$ is monotone and submodular
    , then the objective function value for the iterates generated by SCG satisfies the inequality
        \begin{align*}
            \mathbb{E}[G(\mathbf{y}_T)] \geq (1-1/e) OPT - \frac{15 D K}{T^{1/3}} - \frac{f_{\max} r D^2}{2T},
        \end{align*}
    where $K = \max\{3\|\nabla G(\mathbf{y}_0 - \mathbf{d}_0)\|^2, \sqrt{16\sigma_0^2 + 224\sqrt{n}\varepsilon (L)} + 2\sqrt{r}f_{\max}D\},$ $OPT = \max_{\mathbf{y} \in \mathcal{C}} G(\mathbf{y})$, $r$ is the rank of the matroid $\mathcal{I}$, $\varepsilon(L)=\mathbb{E}_{z\sim P}[\varepsilon_z(L)]$,  $f_{\max}$ is the maximum marginal value of the function $f$, i.e., $f_{\max} = \max_{i \in \{1, \ldots, n\}} f(\{i\})$, and 
    %\begin{equation}\label{eq:boundedVar0}
    $\sigma_0^2 = \sup_{\mathbf{y}\in \mathcal{C}} \mathbb{E}_{z\sim P}\left[\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right\|^2 \right].$
    %\end{equation}
    \end{theorem}
The proof of the theorem can be found
 \fullversion{in App.~\ref{app: proofMainThm} \deleted{of the supplementary material.}}{in App.~\ref{app: proofMainThm} \deleted{of the supplementary material.}} Our proof follows the main steps of \cite{mokhtari2020stochastic}%Mokhtari et al.
 , using however the bias guarantee from Lem.~\ref{lem:gradientBias}; to do so, we need to deal with the fact that our estimator is not unbiased, but also that stochasticity is still present (as variables $z$ are still sampled randomly). This is also reflected in our bound, that contais both a bias term (via $\varepsilon(L)$) and a variance term (via $\sigma_0$).
 
 Comparing our guarantee to Thm.~\ref{thm:theirs}, we observe two main differences. On one hand, we have replaced the uniform bound of the variance $\sigma^2$ with the smaller quantity $\sigma^2_0$: the latter is quantifying the gradient variance w.r.t.~$z$, and is thus smaller than $\sigma$, that depends on the variance of  \emph{both} $z$ \emph{and} $\mathbf{x}\sim \mathbf{y}$. Crucially, $\sigma^2_0$ is an ``inherent'' variance, \emph{independent of the gradient estimation process}: it is the variance due to the  randomness $z$, which is inherent in how we access our  stochastic submodular objective and thus cannot be avoided.  On the other hand, this variance reduction comes at the expense of introducing a bias term. This, however, can be suppressed via Asm.~\ref{asmp:boundedPoly}; as we discuss in the next section, for several problems of interest, this can be made arbitrarily small using only a polynomial number of terms in $\hat{f}^L_z$.


\section{Problem Examples}\label{sec: examples}
\input{05_problems.tex}
\section{Experiments}
\input{06_experiments}

\section{Conclusions}
\input{07_conclusion}

% \section*{Acknowledgment}
% This work is supported by NSF grant CCF-1750539.


% \bibliography{09_references}
% \bibliographystyle{splncs04}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{splncs04}
\bibliography{08_ref}

\newpage
\section*{Appendix}
\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}
\input{09_appendix.tex}
% that's all folks
\end{document}


