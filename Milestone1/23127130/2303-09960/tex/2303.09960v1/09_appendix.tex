\subsection{Proof of Lemma~\ref{lem:gradientBias}} \label{app:proof_gradBiasLemma}
%\begin{proof}
    We start by showing that the norm of the residual error vector of the estimator converges to $0$. 
    Recall that, by Asm.~\ref{asmp:boundedPoly} the residual error of the polynomial estimation $\hat{f}_{z}^L(\mathbf{x})$ is bounded by $\varepsilon (L)$. Thus, for functions $f_z:\{0,1\}^n\rightarrow\reals_+$ satisfying Asm.~\ref{asmp:boundedPoly}, we have that for all $\mathbf{y} \in [0, 1]^n$,
    \begin{align}\label{eq:epsilon_zL}
        \lvert f_z(\mathbf{x}) - \hat{f}_z^L(\mathbf{x})\rvert\leq \varepsilon_z (L).
    \end{align}
    Since $\lim_{L\to\infty}\varepsilon_z (L)=0$ for all $\mathbf{y} \in [0,1]^n$, we get that
    \begin{align}
        \lim_{L\to\infty}\lvert f_z(\vc{y})-\hat{f_z^L}(\vc{y})\rvert\leq\lim_{L\to\infty}\varepsilon_z(L)=0.\label{eq:rl}
    \end{align}
    Moreover,
    \begin{align*}
         \left|\frac{\partial G_z(\vc{y})}{\partial y_i}-\frac{\widehat{\partial G_z^L}(\vc{y})}{\partial y_i}\right|&=\big|\mathbb{E}_{\vc{x}\sim\vc{y}}[f_z([\vc{x}]_{+i})]-\mathbb{E}_{\vc{x}\sim\vc{y}}[f_z([\vc{x}]_{-i})]\displaybreak[0] - \mathbb{E}_{\vc{x}\sim\vc{y}}[\hat{f_z^L}([\vc{x}]_{+i})] + \mathbb{E}_{\vc{x}\sim\vc{y}}[\hat{f_z^L}([\vc{x}]_{-i})]\big|\\
        &\leq \mathbb{E}_{\vc{x}\sim\vc{y}}[|f_z([\vc{x}]_{+i})-\hat{f_z^L}(\vc{[\vc{x}]_{+i}})|]\displaybreak[0] + \mathbb{E}_{\vc{x}\sim\vc{y}}[|f_z([\vc{x}]_{-i}) - \hat{f_z^L}([\vc{x}]_{-i})|] \\
        &\stackrel{\mbox{\tiny{(\ref{eq:epsilon_zL})}}}{\leq} \mathbb{E}_{\vc{x}\sim\vc{y}}[\varepsilon_z (L))]+\mathbb{E}_{\vc{x}\sim\vc{y}}[\varepsilon_z (L))] = 2\varepsilon_z(L).
    \end{align*}
    Hence, we conclude that
 %   \begin{align*}
   $     \big\|\nabla G_z(\vc{y}) - \widehat{\nabla G_z^L}(\vc{y})\big\|_2 \leq 2\sqrt{n}\varepsilon_z(L).$
  %  \end{align*}
%\end{proof}
\hspace{\stretch{1}} \qed

\subsection{Proof of Theorem~\ref{thm:main}} \label{app: proofMainThm}
\begin{proof}
    \begin{lemma} \label{lem:bounded_EGy}
        Consider Stochastic Continuous Greedy (SCG) outlined in Algorithm~\ref{alg: SCG}, and recall the definitions of the function $G$, the rank $r$, and $f_{\max} = \max_{i \in \{1, \ldots, n\}} f(\{i\})$. If Assumption~\ref{asm:boundedNorm} is satisfied, then for $t = 0, \ldots, T$ and for $j = 1, \ldots, n$ we have
            \begin{equation}
                \begin{split}
                \mathbb{E}\left[G(\mathbf{y}_{t+1})\right] &\geq \mathbb{E}\left[G(\mathbf{y}_t)\right] + \frac{1}{T} \mathbb{E}\left[G(\mathbf{y}^*) - G(\mathbf{y}_t)\right] - \frac{f_{\max}r D^2}{2T^2}\\
                &\quad - \frac{1}{2T}\left(4\beta_t D^2 - \frac{\mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2\right]}{\beta_t} \right).
            \end{split}
            \end{equation}
    \end{lemma}

\begin{proof}
    According to the Taylor's expansion of the function $G$ near the point $\mathbf{y}_t$ we can write
        \begin{equation*}
            \begin{split}
                G(\mathbf{y}_{t+1}) &= G(\mathbf{y}_{t}) + \langle \nabla G(\mathbf{y}_{t}), \mathbf{y}_{t+1} - \mathbf{y}_{t} \rangle + \frac{1}{2} \langle \mathbf{y}_{t+1} - \mathbf{y}_{t}, \mathbf{H}(\tilde{\mathbf{y}}_t)(\mathbf{y}_{t+1} - \mathbf{y}_{t}) \rangle,\\
                &= G(\mathbf{y}_{t}) + \frac{1}{T}\langle \nabla G(\mathbf{y}_{t}), \mathbf{v}_{t} \rangle + \frac{1}{2T^2} \langle \mathbf{v}_{t}, \mathbf{H}(\tilde{\mathbf{y}}_t)\mathbf{v}_{t} \rangle
            \end{split}
        \end{equation*}
    where $\tilde{\mathbf{y}}_t$ is a convex combination of $\mathbf{y}_t$ and $\mathbf{y}_t + \frac{1}{T}\mathbf{v}_{t}$ and $\mathbf{H}(\tilde{\mathbf{y}}_t) = \nabla^2 G(\tilde{\mathbf{y}}_t)$. Note here that the elements of the matrix $\mathbf{H}(\tilde{\mathbf{y}}_t)$ are less than the maximum marginal value (i.e. 
    $\max_{i, j} |H_{i, j}(\tilde{\mathbf{y}}_t)| \leq \max_{i \in \{1, \ldots, n\}} f(\{i\}) = f_{\max}$). Therefore, we can lower bound $H_{ij}$ by $-f_{\max}$.
        \begin{equation*}
            \begin{split}
                \langle \mathbf{v}_{t}, \mathbf{H}(\tilde{\mathbf{y}}_t)\mathbf{v}_{t} \rangle &= \sum_{j=1}^n \sum_{i=1}^n v_{i, t} v_{j, t} H_{ij}(\tilde{\mathbf{y}}_t)\\
                &\geq -f_{\max} \sum_{j=1}^n \sum_{i=1}^n v_{i, t} v_{j, t} \\
                &= -f_{\max} \left(\sum_{i=1}^n v_{i, t}\right)^2 = -f_{\max} r \|\mathbf{v}_t\|^2,
            \end{split}
        \end{equation*}
    where the last equality is because $\mathbf{v}_t$ is a vector with $r$ ones and $n-r$ zeros. Replacing $\langle \mathbf{v}_{t}, \mathbf{H}(\tilde{\mathbf{y}}_t)\mathbf{v}_{t} \rangle$ by its lower bound $-f_{\max} r \|\mathbf{v}_t\|^2$ to obtain
        \begin{equation}\label{eq:maintheostep1}
            G(\mathbf{y}_{t+1}) \geq G(\mathbf{y}_{t}) + \frac{1}{T}\langle\nabla G(\mathbf{y}_t), \mathbf{v}_t\rangle - \frac{f_{\max}r}{2T^2} \|\mathbf{v}_t\|^2.
        \end{equation}
     
    Let $\mathbf{y}^*$ be the global maximizer within the constraint set $\mathcal{C}$. Since $\mathbf{v}_t$ is in the set $\mathcal{C}$, it follows from Assumption~\ref{asm:boundedNorm} that the norm $\|\mathbf{v}_t\|^2$ is bounded above by $D^2$. Apply this substitution and add and subtract the inner product $\frac{1}{T} \langle \mathbf{v}_t, \mathbf{d}_t \rangle$ to the right hand side of (\ref{eq:maintheostep1}) to obtain
        \begin{equation}
            \begin{split}
                G(\mathbf{y}_{t+1}) &\geq G(\mathbf{y}_t) + \frac{1}{T} \langle \mathbf{v}_t, \mathbf{d}_t \rangle + \frac{1}{T} \langle \mathbf{v}_t, \nabla G(\mathbf{y}_t) - \mathbf{d}_t \rangle - \frac{f_{\max}r D^2}{2T^2} \\
                &\geq G(\mathbf{y}_t) + \frac{1}{T} \langle \mathbf{y}^*, \mathbf{d}_t \rangle + \frac{1}{T} \langle \mathbf{v}_t, \nabla G(\mathbf{y}_t) - \mathbf{d}_t \rangle - \frac{f_{\max}r D^2}{2T^2}.\label{eq:maintheostep2}
            \end{split}
        \end{equation}
    Note that the second inequality in (\ref{eq:maintheostep2}) holds since $\mathbf{v}_t \in \arg\max_{\mathbf{v} \in \mathcal{C}} \{\mathbf{d}_t^T \mathbf{v}\}$, we can write
        \begin{equation}
            \langle \mathbf{y}^*, \mathbf{d}_t \rangle \leq \max_{\mathbf{v} \in \mathcal{C}} \{\langle \mathbf{v}, \mathbf{d}_t \rangle \} = \langle \mathbf{v}_t, \mathbf{d}_t \rangle.
        \end{equation}
    Now add and subtract the inner product $\langle \mathbf{y}^*, \nabla G(\mathbf{x}_t) - \mathbf{d}_t \rangle / T$ to the right hand side of (\ref{eq:maintheostep2}) to get
        \begin{equation}
            \begin{split}
                G(\mathbf{y}_{t+1}) &\geq G(\mathbf{y}_t) + \frac{1}{T} \langle \mathbf{y}^*, \nabla G(\mathbf{y}_t) \rangle + \frac{1}{T} \langle \mathbf{v}_t - \mathbf{y}^*, \nabla G(\mathbf{y}_t) - \mathbf{d}_t \rangle - \frac{f_{\max}r D^2}{2T^2}.
            \end{split} \label{eq:maintheostep3}
        \end{equation}
    We further have $\langle \mathbf{y}^*, \nabla G(\mathbf{y}_t) - \mathbf{d}_t \rangle \geq G(\mathbf{y}^*) - G(\mathbf{y}_t);$ this follows from monotonicity of $G$ as well as concavity of $G$ along positive directions; see, e.g., \cite{calinescu2011maximizing}. Moreover, by Young's inequality we can show that the inner product $\langle \mathbf{v}_t - \mathbf{y}^*, \nabla G(\mathbf{y}_t) - \mathbf{d}_t \rangle$ is lower bounded by
        \begin{equation}
            \begin{split}
                \langle \mathbf{v}_t - \mathbf{y}^*, \nabla G(\mathbf{x}_t) - \mathbf{d}_t \rangle \geq &
                -\frac{\beta_t}{2} \|\mathbf{v}_t - \mathbf{y}^*\|^2 - \frac{1}{2\beta_t} \|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2,
            \end{split}
        \end{equation}
    for any $\beta_t > 0.$ By applying these substitutions into (\ref{eq:maintheostep3}) we obtain
        \begin{equation}
            \begin{split}
                G(\mathbf{y}_{t+1}) &\geq G(\mathbf{y}_t) + \frac{1}{T} (G(\mathbf{y}^*) - G(\mathbf{y}_t)) - \frac{f_{\max}r D^2}{2T^2} - \frac{1}{2T}\left(\beta_t \|\mathbf{v}_t - \mathbf{y}^*\|^2 - \frac{\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2}{\beta_t} \right)  .\label{eq:maintheostep4}
            \end{split}
        \end{equation}
    Replace $\|\mathbf{v}_t - \mathbf{y}^*\|^2$ by its upper bound $4D^2$ and compute the expected value of (\ref{eq:maintheostep4}) to write
        \begin{equation}
            \begin{split}
                \mathbb{E}\left[G(\mathbf{y}_{t+1})\right] &\geq \mathbb{E}\left[G(\mathbf{y}_t)\right] + \frac{1}{T} \mathbb{E}\left[G(\mathbf{y}^*) - G(\mathbf{y}_t)\right] - \frac{f_{\max}r D^2}{2T^2}\\
                &\quad - \frac{1}{2T}\left(4\beta_t D^2 - \frac{\mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2\right]}{\beta_t} \right).
            \end{split}
        \end{equation}\label{eq:maintheostep5}
    \end{proof}
    
    We need to introduce Lemma~\ref{lem:bounded_dt} to provide a bound for $\mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2\right]$ and in order to prove Lemma~\ref{lem:bounded_dt}, we need three new lemmas, Lemmas~\ref{lem:pointwiseLipschitz},~\ref{lem:bias_bound}, and \ref{lem:recursive}, respectively.
    
    \begin{lemma} \label{lem:pointwiseLipschitz}
        Consider Stochastic Continuous Greedy (SCG) outlined in Algorithm~\ref{alg: SCG} with iterates $\mathbf{y}_t$, and recall the definition of the multilinear extension function $G$ in (\ref{eq: multilinear}). If we define $r$ as the rank of the matroid $\mathcal{I}$ and $f_{\max} = \max_{i\in\{1, \ldots, n\}} f(i),$ then the following holds
        $$|\nabla_j G(\mathbf{y}_{t+1}) - \nabla_j G(\mathbf{y}_t)| \leq f_{\max} \sqrt{r} \|\mathbf{y}_{t+1} - \mathbf{y}_t\|,$$
        for $j = 1, \ldots, n.$
    \end{lemma}
    
    \begin{proof}
        Same as the proof of Lemma~11 in \cite{mokhtari2020stochastic}.
    \end{proof}
    
    \begin{lemma}\label{lem:bias_bound}
    The variance of the biased stochastic gradients $\widehat{\nabla G_{z_t}^L(\mathbf{y}_t)}$ is bounded above by $\left(\sigma_0 + 2\sqrt{n}\varepsilon (L)\right)^2$, i.e., for any vector $\mathbf{y} \in \mathcal{C}$ we can write
        \begin{equation}
            \begin{split}
            \mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \right] &\leq (1 + \beta_0)\sigma_0^2 + \left(1 + \frac{1}{\beta_0}\right)2\sqrt{n}\varepsilon (L),
            \end{split}
        \end{equation}
    where the expectation is with respect to the randomness of $z \sim P$.
    \end{lemma}
    \begin{proof}    
    Adding and subtracting $\nabla G_{z_t}(\mathbf{y}_t)$ inside the norm, we obtain
        \begin{equation}
            \begin{split}
                \mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \widehat{\nabla G_{z_t}^L(\mathbf{y})}\right\|^2 \right]  &= \mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y}) + \nabla G_{z}(\mathbf{y}) -  \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \right],\\
                &= \mathbb{E}\bigg[\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right\|^2 \\
                &+ 2\left(\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right)^T\left(\nabla G_{z}(\mathbf{y}) -  \widehat{\nabla G_{z}^L(\mathbf{y})}\right) \\
                &+ \left\|\nabla G_{z}(\mathbf{y}) -  \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \bigg],\\
                &= \mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right\|^2 \right] \\
                &+ 2\mathbb{E}\left[\left(\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right)^T\left(\nabla G_{z}(\mathbf{y}) -  \widehat{\nabla G_{z}^L(\mathbf{y})}\right) \right] \\
                &+ \mathbb{E}\left[\left\|\nabla G_{z}(\mathbf{y}) -  \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \right].
            \end{split}
        \end{equation}
        
    Using Young's inequality $\left(\langle \mathbf{a}, \mathbf{b}\rangle \leq \frac{1}{\beta}\frac{\|\mathbf{a}\|^2}{2} + \beta\frac{\|\mathbf{b}\|^2}{2}\right)$, also known as Peter-Paul inequality, to substitute the inner products with summations to obtain
    \begin{equation}
            \begin{split}
                \mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \widehat{\nabla G_{z_t}^L(\mathbf{y})}\right\|^2 \right]  &= \mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y}) + \nabla G_{z}(\mathbf{y}) -  \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \right],\\
                &\leq \mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right\|^2 \right] \\
                &\quad + \mathbb{E}\bigg[\beta_0\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right\|^2 + \frac{1}{\beta_0}\left\|\nabla G_{z}(\mathbf{y}) -  \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \bigg] \\
                &\quad + \mathbb{E}\left[\left\|\nabla G_{z}(\mathbf{y}) -  \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \right],\\
                &\leq (1 + \beta_0)\mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right\|^2 \right] \\
                &\quad + \left(1 + \frac{1}{\beta_0}\right)\mathbb{E}\left[\left\|\nabla G_{z}(\mathbf{y}) -  \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \right].
            \end{split}
        \end{equation}
    
     Replacing $\mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \nabla G_{z}(\mathbf{y})\right\|^2 \right]$ by its upper bound $\sigma_0^2$ and using the result of Lemma~\ref{lem:gradientBias} to replace $\mathbb{E}\left[\left\|\nabla G_{z}(\mathbf{y}) - \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \right]$ by its upper bound $2\sqrt{n}\varepsilon (L)$, we obtain
    \begin{equation}
            \begin{split}
                \mathbb{E}\left[\left\|\nabla G(\mathbf{y}) - \widehat{\nabla G_{z}^L(\mathbf{y})}\right\|^2 \right] &\leq (1 + \beta_0)\sigma_0^2 + \left(1 + \frac{1}{\beta_0}\right)2\sqrt{n}\varepsilon (L).
            \end{split}
        \end{equation} 
    \end{proof}

    \begin{lemma}\label{lem:recursive}
        (Directly from \cite{mokhtari2020stochastic}) Consider the scalars $b \geq 0$ and $c > 1.$ Let $\phi_t$ be a sequence of real numbers satisfying $$\phi_t \leq \left(1 - \frac{c}{(t+t_0)^{\alpha}}\right) \phi_{t-1} + \frac{b}{(t+t_0)^{2\alpha}},$$ for some $0 \leq \alpha \leq 1$ and $t_0 \geq 0.$ Then, the sequence $\phi_t$ converges to zero at the following rate $$\phi_t \leq \frac{Q}{(t+t_0+1)^{\alpha}},$$ where $Q = \max\{\phi_0 (t_0 + 1)^{\alpha}, b/(c-1)\}.$
    \end{lemma}
    \begin{proof}
    Proof of the lemma can be found in the Appendix~C of \cite{mokhtari2020stochastic}.
    \end{proof}
    
    \begin{lemma} \label{lem:bounded_dt}
        Consider Stochastic Continuous Greedy (SCG) outlined in Algorithm~\ref{alg: SCG}, and recall the definitions of the function $G$, the rank $r$, the upper bound $\sigma_0$ defined as in Thm.~\ref{thm:main} and $f_{\max} = \max_{i \in \{1, \ldots, n\}} f(\{i\})$. If Assumption~\ref{asm:boundedNorm} %, \ref{asm:boundedVar0} are 
        is satisfied and $\rho_t = \frac{4}{(t+8)^{2/3}},$ then for $t = 0, \ldots, T$ and for $j = 1, \ldots, n$ we have
            \begin{equation}
                \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2\right] \leq \frac{Q}{(t+9)^{2/3}},
            \end{equation}
        where $Q = \max\{5\|\nabla G(\mathbf{y}_0 - \mathbf{d}_0)\|^2, 32\sigma_0^2 + 224\sqrt{n}\varepsilon (L) + 4f_{\max}^2rD^2\}.$
    \end{lemma}

    \begin{proof}
    Use the definition $\mathbf{d}_t = (1 - \rho_t) \mathbf{d}_{t-1} + \rho_t \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}$ to write $\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2$ as
        \begin{equation}
            \begin{split}
                \|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2 = \|&\nabla G(\mathbf{y}_t) - (1 - \rho_t) \mathbf{d}_{t-1} - \rho_t \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2.\label{eq:dt_bound_step1}
            \end{split}
        \end{equation}
    Add and subtract the term $(1 - \rho_t) \nabla G(\mathbf{y}_{t-1})$ to the right hand side of (\ref{eq:dt_bound_step1}), regroup the terms and expand the squared term to obtain
    \begin{equation}
        \begin{split}
            \|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2 &= \|\nabla G(\mathbf{y}_t) - (1 - \rho_t) \nabla G(\mathbf{y}_{t-1}) + (1 - \rho_t) \nabla G(\mathbf{y}_{t-1}) \\
            &\quad - (1 - \rho_t) \mathbf{d}_{t-1} - \rho_t \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2\\
            &= \|\rho_t(\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}) + (1 - \rho_t) (\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})) \\
            &\quad + (1 - \rho_t) (\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1}) \|^2\\
            &= \rho_t^2 \|\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2
            + (1 - \rho_t)^2 \|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2 \\
            &\quad + (1 - \rho_t)^2 \|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1} \|^2 \\
            & \quad + 2\rho_t(1 - \rho_t)(\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})^T (\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})) \\
            &\quad + 2\rho_t(1 - \rho_t)(\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})^T (\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1})\\
            &\quad + 2(1 - \rho_t)^2(\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})^T (\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1}).
        \end{split} \label{eq:dt_bound_step4}
    \end{equation}
    
    Computing the conditional expectation $\mathbb{E}[(\cdot) | \mathcal{F}_t]$ for both sides we obtain
    \begin{equation}
        \begin{split}
            \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2 | \mathcal{F}_t \right] &= \rho_t^2 \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2 | \mathcal{F}_t \right] + (1 - \rho_t)^2 \|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2 \\
            &\quad + (1 - \rho_t)^2 \|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1} \|^2 \\
            &\quad + 2\rho_t(1 - \rho_t)\mathbb{E}\Big[(\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})^T (\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1}))\Big] \\
            &\quad + 2\rho_t(1 - \rho_t)\mathbb{E}\Big[(\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})^T (\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1})\Big]\\
            &\quad + 2(1 - \rho_t)^2(\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1}))^T (\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1}).
        \end{split}
    \end{equation}
    
    Let's focus on the $\mathbb{E}\left[(\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})^T \allowbreak (\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1}))\right]$ term before moving further and call it $A$. By adding and subtracting $\nabla G_{z_t}(\mathbf{y}_t)$ inside $(\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})$ we obtain
    \begin{equation}
        \begin{split}
            A &= \mathbb{E}\Big[[(\nabla G(\mathbf{y}_t) - \nabla G_{z_t}(\mathbf{y}_t)) + (\nabla G_{z_t}(\mathbf{y}_t)) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}]^T  (\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1}))\Big]\\
            &= \mathbb{E}\Big[(\nabla G(\mathbf{y}_t) - \nabla G_{z_t}(\mathbf{y}_t))^T(\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})) \\
            &+ (\nabla G_{z_t}(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})^T (\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1}))\Big]\\
            &= \mathbb{E}\Big[(\nabla G(\mathbf{y}_t) - \nabla G_{z_t}(\mathbf{y}_t))^T(\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1}))\Big] \\
            &+ \mathbb{E}\Big[(G_{z_t}(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})^T (\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1}))\Big].
        \end{split}
    \end{equation}
    Using the fact that $\nabla G_{z_t}(\mathbf{y}_t)$ is an unbiased estimator of the gradient $\nabla G(\mathbf{y}_t)$, i.e., $\mathbb{E}\left[\nabla G_{z_t}(\mathbf{y}_t) | \mathcal{F}_t \right] = \nabla G(\mathbf{y}_t)$, and replacing $(G_{z_t}(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})$ with its upper bound $2\sqrt{n}\varepsilon (L)$ to obtain
    \begin{equation}
        A \leq \mathbb{E}\left[(2\sqrt{n}\varepsilon (L))^T (\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1}))\right].
    \end{equation}
    Applying a similar process to the $\mathbb{E}\left[(\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)})^T (\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1})| \mathcal{F}_t \right]$ term and using Young's inequality $\left(\langle \mathbf{a}, \mathbf{b}\rangle \leq \frac{1}{\beta}\frac{\|\mathbf{a}\|^2}{2} + \beta\frac{\|\mathbf{b}\|^2}{2}\right)$, also known as Peter-Paul inequality, to substitute the inner products with summations to obtain
    \begin{equation}
        \begin{split}
            \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2 | \mathcal{F}_t \right] &\leq \rho_t^2 \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2 | \mathcal{F}_t \right] + (1 - \rho_t)^2 \|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2 \\
            &+ (1 - \rho_t)^2 \|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1} \|^2 \\
            &+ (\rho_t - \rho_t^2)\mathbb{E}\left[\beta_1 2\sqrt{n}\varepsilon (L) +  \frac{\|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2}{\beta_1}\right] \\
            &+ (\rho_t - \rho_t^2)\mathbb{E}\left[\beta_2 2\sqrt{n}\varepsilon (L) +  \frac{\|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1}\|^2}{\beta_2}\right]\\
            &+ (1 - \rho_t)^2\bigg(\beta_3\|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})^T\|^2 + \frac{1}{\beta_3}\|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1}\|^2\bigg),\\
            &= \rho_t^2 \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2 | \mathcal{F}_t \right] + (1 - \rho_t)^2 \|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2 \\
            &+ (1 - \rho_t)^2 \|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1} \|^2 \\
            &+ (\rho_t - \rho_t^2)\left(\beta_1 2\sqrt{n}\varepsilon (L) +  \frac{\|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2}{\beta_1}\right) \\
            &+ (\rho_t - \rho_t^2)\left(\beta_2 2\sqrt{n}\varepsilon (L) +  \frac{\|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1}\|^2}{\beta_2}\right)\\
            &+ (1 - \rho_t)^2\bigg(\beta_3\|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})^T\|^2 + \frac{1}{\beta_3}\|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1}\|^2\bigg),\\
            &= \rho_t^2 \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2 | \mathcal{F}_t \right] + \rho_t (1-\rho_t)(\beta_1 + \beta_2) 2\sqrt{n}\varepsilon (L) \\
            &+ \left((1-\rho_t)^2(1+\beta_3) + \frac{\rho_t(1-\rho_t)}{\beta_1}\right) \|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2 \\
            &+ \left((1 - \rho_t)^2(1+\frac{1}{\beta_3}) + \frac{\rho_t (1 - \rho_t)}{\beta_2}\right) \|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1} \|^2. 
        \end{split}
    \end{equation}
Since we assume that $\rho_t \leq 1$ we can replace all the $(1-\rho_t)^2$ terms by $(1-\rho_t)$. Applying this substitution and setting $\beta_1 =\rho_t$, $\beta_2 = 4$ and $\beta_3 = 4/\rho_t$, we get
    \begin{equation}
        \begin{split}
            \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2 | \mathcal{F}_t \right] &\leq \rho_t^2 \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2 | \mathcal{F}_t \right] + \rho_t (1-\rho_t)(\rho_t + 4) 2\sqrt{n}\varepsilon (L)\\
            &\quad + 2(1-\rho_t)\left(1 + \frac{2}{\rho_t}\right) \|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2 \\
            &\quad + (1-\rho_t)\left(1+\frac{\rho_t}{2}\right) \|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1} \|^2.
        \end{split}
    \end{equation}
    Now using the inequalities $2(1-\rho_t)(1+(2/\rho_t)) \leq (4/\rho_t)$ and $(1-\rho_t)(1+(\rho_t/2)) \leq (1-(\rho_t/2))$ we obtain
    \begin{equation}
        \begin{split}
            \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2 \right] &\leq \rho_t^2 \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2 \right] + \rho_t (1-\rho_t)(\rho_t + 4) 2\sqrt{n}\varepsilon (L)\\
            &+ \frac{4}{\rho_t} \|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2 + \left(1-\frac{\rho_t}{2}\right) \|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1} \|^2.
        \end{split}
    \end{equation}
 \sloppy   Now we need two auxiliary lemmas (Lemma~\ref{lem:pointwiseLipschitz} \& Lemma~\ref{lem:bias_bound}) to provide bounds for $\mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2 \right]$ and $\|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2$, respectively.
    
 \fussy   The term $\mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \nabla \widehat{G_{z_t}^L (\mathbf{y}_t)}\|^2 \right]$ can be bounded above by $(1 + \beta_0)\sigma_0^2 + \left(1 + \frac{1}{\beta_0}\right)2\sqrt{n}\varepsilon (L)$ according to Lemma~\ref{lem:bias_bound}. Based on Assumption~\ref{asm:boundedNorm} %\& ~\ref{asm:Lipschitz}
    and Lemma~\ref{lem:pointwiseLipschitz}, we can also show that the squared norm $\|\nabla G(\mathbf{y}_t) - \nabla G(\mathbf{y}_{t-1})\|^2$ is upper bounded by $f_{\max}^2rD^2/T^2$. Applying these substitutions yields
    \begin{equation*}
        \begin{split}
            \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2\right] &\leq \rho_t^2 (1 + \beta_0)\sigma_0^2 + \left(1 + \frac{1}{\beta_0} + \rho_t (1-\rho_t)(\rho_t + 4)\right)2\sqrt{n}\varepsilon (L) \\
            &\quad+ \frac{4}{\rho_t} f_{\max}^2rD^2/T^2 + \left(1-\frac{\rho_t}{2}\right)  \|\nabla G(\mathbf{y}_{t-1})- \mathbf{d}_{t-1} \|^2.
        \end{split}
    \end{equation*}
        
    We introduce one more Lemma~\ref{lem:recursive} to bound $\mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2\right]$ recursively.
    
    Now define $\phi_t = \mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2 | \mathcal{F}_t \right]$ and set $\rho_t = \frac{4}{(t+8)^{2/3}}$ to obtain
    \begin{equation*}
        \begin{split}
            \phi_t &\leq \left(1-\frac{2}{(t+8)^{2/3}}\right) \phi_{t-1} + \frac{16}{(t+8)^{4/3}} (1 + \beta_0)\sigma_0^2 \\
            &+ \Bigg(1 + \frac{1}{\beta_0} + \frac{4}{(t+8)^{2/3}} \left(1-\frac{4}{(t+8)^{2/3}}\right)\left(\frac{4}{(t+8)^{2/3}} + 4\right)\Bigg) 2\sqrt{n}\varepsilon (L) \\
            &+  \frac{f_{\max}^2rD^2(t+8)^{2/3}}{T^2}.
        \end{split}
    \end{equation*}
    Now use the conditions $8 \leq T$ and $t \leq T$ to replace $1/T$ by its upper bound $2/(t+8)$ and choose $\beta_0=1$:
    \begin{equation*}
        \begin{split}
            \phi_t \leq &\left(1-\frac{2}{(t+8)^{2/3}}\right) \phi_{t-1} + \frac{32}{(t+8)^{4/3}} \sigma_0^2 \\
            &+ \bigg(2 + \frac{4}{(t+8)^{2/3}} \left(1-\frac{4}{(t+8)^{2/3}}\right)\left(\frac{4}{(t+8)^{2/3}} + 4\right)\bigg) 2\sqrt{n}\varepsilon (L) +  \frac{4f_{\max}^2rD^2}{(t+8)^{4/3}}.
        \end{split}
    \end{equation*}
    Now using the inequality $\left(2 + \frac{4}{(t+8)^{2/3}} \left(1-\frac{4}{(t+8)^{2/3}}\right)\left(\frac{4}{(t+8)^{2/3}} + 4\right)\right) \leq \frac{112}{(t+8)^{2/3}}$ for $t \geq 0$
    \begin{equation*}
        \begin{split}
            \phi_t \leq &\left(1-\frac{2}{(t+8)^{2/3}}\right) \phi_{t-1} + \frac{32\sigma_0^2 + 224\sqrt{n}\varepsilon (L) + 4f_{\max}^2rD^2}{(t+8)^{4/3}}.
        \end{split}
    \end{equation*}
    Now using the result in Lemma~\ref{lem:recursive}, we obtain that 
    \begin{equation}
        \phi_t \leq \frac{Q}{(t+9)^{2/3}},
    \end{equation}
    where $Q = \max\{5\|\nabla G(\mathbf{y}_0 - \mathbf{d}_0)\|^2, 32\sigma_0^2 + 224\sqrt{n}\varepsilon (L) + 4f_{\max}^2rD^2\}.$ 
\end{proof}

Substitute $\mathbb{E}\left[\|\nabla G(\mathbf{y}_t) - \mathbf{d}_t\|^2\right]$ by its upper bound $Q/((t+9)^{2/3})$ according to the result of Lemma~\ref{lem:bounded_dt}. Further, set $\beta_t = \frac{Q^{1/2}}{2D(t+9)^{1/3}}$ and regroup the resulted expression to obtain
    \begin{equation}
        \begin{split}
            \mathbb{E}\left[G(\mathbf{y}^*) - G(\mathbf{y}_{t+1})\right] \leq &\left(1-\frac{1}{T}\right) \mathbb{E}\left[G(\mathbf{y}^*) - G(\mathbf{y}_t)\right] + \frac{2DQ^{1/2}}{(t+9)^{1/3}T} + \frac{f_{\max}r D^2}{2T^2}.\label{eq:maintheostep6}
        \end{split}
    \end{equation}
    By applying the inequality in (\ref{eq:maintheostep6}) recursively for $t = 0, \ldots, {T-1}$ we obtain
    \begin{equation}
        \begin{split}
            \mathbb{E}\left[G(\mathbf{y}^*) - G(\mathbf{y}_{T})\right] \leq &\left(1-\frac{1}{T}\right)^T \mathbb{E}\left[G(\mathbf{y}^*) - G(\mathbf{y}_0)\right] + \sum_{t=0}^{T-1} \frac{2DQ^{1/2}}{(t+9)^{1/3}T} + \sum_{t=0}^{T-1} \frac{f_{\max}r D^2}{2T^2}.\label{eq:maintheostep7}
        \end{split}
    \end{equation}
    
    Note that we can write
    \begin{equation}\label{eq:simple_sum}
        \begin{split}
            \sum_{t=0}^{T-1} \frac{1}{(t+9)^{1/3}} &\leq \frac{1}{9^{1/3}} + \int_{t=0}^{T-1} \frac{1}{(t+9)^{1/3}} dt\\
            &= \frac{1}{9^{1/3}} + \frac{3}{2} (t+9)^{2/3} \Big|_{t=T-1} - \frac{3}{2} (t+9)^{2/3} \Big|_{t=0}\\
            &\leq \frac{3}{2} (T+8)^{2/3} \leq \frac{15}{2} T^{2/3}
        \end{split}
    \end{equation}
    where the last inequality holds since $(T+8)^{2/3} \leq 5T^{2/3}$ for any $T \geq 1.$ By simplifying the terms on the right hand side of (\ref{eq:maintheostep7}) and using the inequality in (\ref{eq:simple_sum}) we can write
    \begin{equation}
        \begin{split}
            \mathbb{E}\left[G(\mathbf{y}^*) - G(\mathbf{y}_{T})\right] \leq &\frac{1}{e} \mathbb{E}\left[G(\mathbf{y}^*) - G(\mathbf{y}_0)\right] + \frac{15DQ^{1/2}}{T^{1/3}} + \frac{f_{\max}r D^2}{2T}.\label{eq:maintheostep8}
        \end{split}
    \end{equation}
    Here, we use the fact that $G(\mathbf{y}_0) \geq 0$, and hence the expression in (\ref{eq:maintheostep8}) can be simplified to 
    \begin{equation*}
        \mathbb{E}\left[G(\mathbf{y}_{T})\right] \geq (1-1/e) \mathbb{E}\left[G(\mathbf{y}^*))\right] - \frac{15DQ^{1/2}}{T^{1/3}} - \frac{f_{\max}r D^2}{2T},\label{eq:maintheostep9}
    \end{equation*}
    where $Q = \max\{5\|\nabla G(\mathbf{y}_0 - \mathbf{d}_0)\|^2, 32\sigma_0^2 + 224\sqrt{n}\varepsilon (L) + 4f_{\max}^2rD^2\}$ and $K = Q^{1/2} = \max\{3\|\nabla G(\mathbf{y}_0 - \mathbf{d}_0)\|^2, \sqrt{16\sigma_0^2 + 224\sqrt{n}\varepsilon (L)} + 2\sqrt{r}f_{\max}D\}.$
\end{proof}

\subsection{Custom Biases for the Problems in Sec.~\ref{sec: examples}}\label{app:customBias}
\subsubsection{Estimator Bias for Summarization Problems}%Proof of Theorem \ref{thm: epsilon_bound}.} 
\label{app: proof_bias_bound}
\begin{theorem} \label{thm: epsilon_bound}
Assume a diversity reward function $f:~\{0,1\}^n \rightarrow \reals_+$ with $h(s)=\log(1+s)$. Then, consider the estimator $\widehat{\nabla G_z^L}(\mathbf{y}_K)$ given in (\ref{eq: poly_estimator}) using $\hat{f}_z^{L}(\vc{x})$,  the $L^{th}$ Taylor polynomial of $f(\vc{x})$ around $1/2$, given by \eqref{eq: taylor_f_iL}. Then, the bias of the estimator satisfies %bounded by %\begin{equation*}
   $\big\|\nabla G_z(\vc{y}) - \widehat{\nabla G_z^L}(\vc{y})\big\|_2 \leq \frac{\sqrt{n}}{(L+1) 2^{L}}.$
%\end{equation*}
%where the bias is the $\ell_2$ norm of the residual error vector, $M$ is the number of input partitions and $N$ is the size of the ground set.
\end{theorem}
% The proof \deleted{of this theorem }can be found in \fullversion{\cite{ozcan2021submodular}}{App.~\ref{app: proof_bias_bound}}. 
\begin{proof}
    We begin by characterizing the residual error of the Taylor series of $h(s)=\log(1+s)$ around $1/2$:
    \begin{lemma} \label{lem: R_iL_bound}
    Let $\hat{h}^{L}(s)$ be the $L^{\text{th}}$ order Taylor approximation of $h(s) = \log(1 + s)$ around $1/2$, given by \eqref{eq: taylor_f_iL}. Then, $\hat{f}_z^L(\mathbf{x}) = \hat{h}^L(g_z(\mathbf{x}))$, satisfies Asm.~\ref{asmp:boundedPoly}, with:  
    \begin{equation}
        \varepsilon_{z}(L) =\frac{1}{(L+1) 2^{L+1}}.
    \end{equation}
    \end{lemma}
    \begin{proof}
    %The $L^{th}$ Taylor polynomial of $h_i(s)$ around $1/2$ is
    %\begin{equation}
    %    \hat{h}_{i, L}(s) = \sum_{l = 0}^L \frac{h_i^{(l)}(1/2)}{l!} (s - {1}/{2})^l
    %\end{equation}
    %where $h_i^{(l)}(s) = (-1)^{l-1} \frac{(l-1)!}{(1+s)^{l}}$ for $h_i(s) = \log(1 + s)$. 
    By the Lagrange remainder theorem,
    \begin{equation*}
    \begin{split}
     \left\lvert h(s)-\hat{h}^{ L}(s)\right\rvert&=\left\lvert\frac{h^{(L+1)}(s')}{(L+1)!} \left(s-\frac{1}{2}\right)^{L+1}\right\rvert \\
     &= \left\rvert \frac{\left(s-{1}/{2}\right)^{L+1}}{(L+1)\left(1+s'\right)^{L+1}} \right\lvert
    \end{split}
    \end{equation*}
    for some $s'$ between $s$ and $1/2$. Since $s \in [0, 1]$, (a) $|s-\frac{1}{2}|\leq \frac{1}{2}$, and (b) $s'\in[0, 1]$.  Hence
    %\begin{equation*}
    %\begin{split}
     $\left\lvert h(s) - \hat{h}^{L}(s) \right\rvert \leq \frac{1}{(L+1)2^{L+1}} .$% = R_{i,L}(s).
    %\end{split}
    %\end{equation*}
    \end{proof}
    To conclude the theorem, observe that:
    \begin{align*}
        \big\|\nabla G_z(\vc{y}) - \widehat{\nabla G_z^L}(\vc{y})\big\|_2 \leq 2\sqrt{n}\varepsilon (L) = \frac{\sqrt{n}}{(L+1) 2^{L}}.
    \end{align*}
    % Then, $\varepsilon(L) \leq \frac{M\sqrt{N}}{(L+1) 2^{L}}$. 
    % \hspace{\stretch{1}} \qed
\end{proof}

\subsubsection{Estimator Bias for Influence Maximization Problems}%Proof of Theorem \ref{thm: epsilon_bound_IM}.}
\label{app: proof_bias_bound_IM}%\& \ref{thm: epsilon_bound_FL}}

\begin{theorem} \label{thm: epsilon_bound_IM}
For function $f:~\{0,1\}^n \rightarrow \reals_+$ that  given by $f(\mathbf{x}) = \mathbb{E}_{z \sim P} [f_z(\mathbf{x})]$, where  
consider the estimator $\widehat{\nabla G_z^L}$   given in (\ref{eq: poly_estimator}) using  $\hat{f}_z^L$, the $L^{\text{th}}$-order Taylor approximation of $f_z$ around $1/2$, given by \eqref{eq: taylor_f_iL}. Then, the bias of estimator $\widehat{\nabla G_z^L}$  satisfies
%\begin{equation}
 $    \big\|\nabla G_z(\vc{y}) - \widehat{\nabla G_z^L}(\vc{y})\big\|_2 \leq \frac{\sqrt{n}}{(L+1) 2^{L}}.$ %\label{eq:eLbound}
%\end{equation}
%where the bias is the $\ell_2$ norm of the residual error vector and $N$ is the size of the ground set.
\end{theorem}
\begin{proof}
    To prove the theorem, observe that for all $\vc{y}\in [0,1]^n$:
\begin{equation*}
\begin{split}
\big\|\nabla G_z(\vc{y}) - \widehat{\nabla G_z^L}(\vc{y})\big\|_2 \leq 2\sqrt{n}\varepsilon (L) = \frac{\sqrt{n}}{(L+1) 2^{L}}.
\end{split}
\end{equation*}
% Hence, , $\varepsilon(L) \leq \frac{\sqrt{N}}{(L+1) 2^{L}}$. \hspace{\stretch{1}} \qed
\end{proof}


\subsubsection{Cache Networks (CN)\cite{mahdian2020kelly}}%Proof of Theorem~\ref{thm:epsilon_bound_CN}} 
\label{app:proof_bound_CN}
% \subsection{Cache Networks (CN)\cite{mahdian2020kelly}} \label{prob:CN} 
A Kelly cache network can be represented by a graph $G(V, E)$, $|E|=M$, service rates $\mu_j$, $j \in E$, storage capacities $c_v$, $v \in V$, a set of requests $\mathcal{R}$, and arrival rates $\lambda_r$, for $r \in \mathcal{R}$. Each request is characterized by an item $i^r\in\mathcal{C}$ requested, and a path $p^r\subset V$ that the request follows.  For a detailed description of these variables, please refer to \cite{mahdian2020kelly}. Requests are forwarded on a path until they meet a cache storing the requested item. In steady-state,  the traffic load on an edge $(u,v)$ is given by  
\begin{equation} \label{eq:CNgi}
    g_{(u, v)}(\vc{x}) = \frac{1}{\mu_{u, v}}\sum_{r \in \mathcal{R}:(v, u)\in p^r} \lambda^r \prod_{k'=1}^{k_{p^r}(v)}(1-x_{p_k^r, i^r}).
\end{equation}
where $\vc{x}\in \{0,1\}^{|V||\mathcal{C}|}$ is a vector of binary coordinates $x_{vi}$ indicating if $i\in \mathcal{C}$ is stored in node $v\in V$. If $s$ is the load on an edge,  the expected total number of packets in the system is given by $h(s)=\frac{s}{1-s}$.
Then using the notation $z=(u, v) \in E$ to index edges, the expected total number of packets in the system in steady state can indeed be written as $\mathbb{E}_{z \sim P}\left[h(g_z(\vc{x}))\right]$ \cite{mahdian2020kelly}. %, which is indeed convex and non-decreasing for $g_i \in [0, 1)$.
Mahdian et al.~maximize the  \emph{caching gain} $f: \{0, 1\}^{|V||\mathcal{C}|} \rightarrow \reals_+$ as 
\begin{equation} \label{eq:CN}
    f(\vc{x}) = \textstyle\mathbb{E}_{z \sim P} \left[h(g_z(\vc{0})) -  h(g_z(\vc{x}))\right]
\end{equation}
subject to the capacity constraints in each class.
The caching gain $f(\vc{x})$ is  monotone and submodular, and the capacity constraints form a partition matroid \cite{mahdian2020kelly}. %Thus, Eq.~(\ref{eq:CN}) satisfies Asm.~\ref{asmp: mon_sub}. %Additionally, it is clear to see that for $j=(u, v)$, $g_j$ defined in (\ref{eq:CNgi}), is multilinear. 
Moreover, $h(s)=\frac{s}{1-s}$ can be approximated within arbitrary accuracy by its $L^{\text{th}}$-order Taylor approximation around $0$, given by:
\begin{equation} \label{eq: f_iL_CN}
    \hat{h}^{L}(s) = \textstyle\sum_{\ell = 1}^L s^\ell
\end{equation}
We show in %following lemma 
App.~\ref{app:proof_bound_CN} 
that this estimator ensures that $f$ indeed satisfies Asm.~\ref{asmp:boundedPoly}. Proof of this lemma can be found in App.~\ref{app:proof_bound_CN}.
% According to this lemma, we can use
% as the polynomial estimator of degree $L$ of $h_{j}(s)$ and use it in (\ref{eq: poly_estimator}) to find a polynomial estimator of $G(\vc{y})$. 
%we can show that the residual error vector $\epsilon_L(\vc{y})$ of this estimator $\widehat{\nabla G(\vc{y})}$ is bounded.
Furthermore, we  bound the estimator bias appearing in Thm.~\ref{thm:main} as follows:
%The proof of the theorem can be found 
% in App.~\ref{app:proof_bound_CN}.
\begin{theorem} \label{thm:epsilon_bound_CN}
Assume a caching gain function $f:~\{0,1\}^{|V||\mathcal{C}|} \rightarrow \reals_+$ that is given by (\ref{eq:CN}). Then, consider Algorithm \ref{alg: SCG} in which $\nabla G(\mathbf{y}_K)$ is estimated via the polynomial estimator given in (\ref{eq: poly_estimator}) where $\hat{f}_z^{L}(\vc{x})$ is the $L^{th}$ Taylor polynomial of $f(\vc{x})$ around $0$. Then, the bias of the estimator is bounded by
\begin{equation}
    \big\|\nabla G_z(\vc{y}) - \widehat{\nabla G_z^L}(\vc{y})\big\|_2 \leq 2\sqrt{{|V||\mathcal{C}|}}\frac{\bar{s}^{L+1}}{1-\bar{s}},
\end{equation}
where $\bar{s}<1$ is the largest load among all edges when caches are empty.  %the bias is the $\ell_2$ norm of the residual error vector and $N$ is the size of the ground set.
\end{theorem}
\begin{proof}
\begin{lemma} \label{lem:bound_CN}
Let $\hat{h}^{L}(s)$ be the $L^{th}$ Taylor polynomial of $h(s)=\frac{s}{1-s}$ around $0$. Then, $h(s)$ and its polynomial estimator of degree $L$, $\hat{h}^{L}(s)$, satisfy Asm.~\ref{asmp:boundedPoly} where 
\begin{equation}
    \varepsilon(L) = \frac{\bar{s}^{L+1}}{1 - \bar{s}}.
\end{equation}
\end{lemma}
\begin{proof}
$L^{th}$ Taylor polynomial of $h(s)$ around $0$ is
\begin{equation}
    \hat{h}_{L}(s) = \textstyle\sum_{l = 0}^L \frac{h^{(\ell)}(0)}{\ell!} s^{\ell} = \sum_{\ell=1}^{L} s^{\ell}
\end{equation}
where $h^{(\ell)}(s) = \frac{\ell!}{(1-s)^{\ell+1}}$ for $h(s) = \frac{s}{1-s}$. 
% By the Lagrange remainder theorem,
% \begin{equation*}
% \begin{split}
%  \left\lvert f_i(s) - \hat{f}_{i, L}(s) \right\rvert &= \left\lvert \frac{f_i^{(L+1)}(s')}{(L+1)!} s^{L+1} \right\rvert \\
%  &= \left\rvert \frac{s^{L+1}}{(L+1)(1-s')^{L+2}} \right\lvert
% \end{split}
% \end{equation*}
% for some $s'$ between $s$ and $0$. Since $s \in [0, 1)$, $s'$ is also between $[0, 1)$. Hence, 
% \begin{equation*}
% \begin{split}
%  \left\lvert f_i(s) - \hat{f}_{i, L}(s) \right\rvert \leq = R_{i,L}(s).
% \end{split}
% \end{equation*}
\begin{align*}
    h(s) &= \frac{s}{1 - s} = \textstyle\sum_{\ell=1}^{\infty} s^{\ell} = \sum_{\ell=1}^{L} s^{\ell} + \sum_{\ell=L+1}^{\infty} s^{\ell} \\
    &= \textstyle\sum_{\ell=1}^{L}s^{\ell}+s^L\sum_{\ell=1}^{\infty}s^{\ell}=\sum_{\ell=1}^{L} s^{\ell} + \frac{s^{L+1}}{1-s}
\end{align*}
Then, the bias of the Taylor Series Estimation around $0$ becomes:
 \begin{align*}
    \left| \frac{s}{1 - s} - \textstyle\sum_{\ell=1}^{L} s^{\ell}\right|  =  \frac{s^{L+1}}{1-s} \leq \frac{\bar{s}^{L+1}}{1 - \bar{s}} = \varepsilon(L).
\end{align*}
for all $s \in [0, \bar{s}]$ where $\bar{s} = \max_{z \sim P} s_z$. \hspace{\stretch{1}} \qed

% \begin{lemma} \label{lem: bound_CN}
% Consider a queue size function $f_i: [0, 1]^N \rightarrow \reals_+$ for which $g_i$ is $L + 1$ differentiable for all $i$ and its Taylor expansion exists at $0$. Then, the gradient of the multilinear relaxation can be approximated by,
% \begin{align*}
%     \nabla G_i(\mathbf{y}) \approx \mathbb{E}[\hat{C}_L(g) | g_i = 1] - \mathbb{E}[\hat{C}_L(g) | g_i = 0]
% \end{align*}
% where $\hat{C}_L(g_i) = \sum_{l = 0}^L \frac{C^{(l)}(0)}{l!} g_i^l$ and the error of the approximation is $2D \frac{\bar{g}^{L+1}}{1 - \bar{g}}$ where $D = \max_{\mathbf{v} \in \mathcal{D}} \|\mathbf{v}\|_2$ and $\bar{g} = max_i g_i$.
% \end{lemma}
% Proof of this lemma can be found in Appendix \ref{app: proof_bound_CN}.
% \section{Proof of Lemma \ref{lem:bound_CN}} \label{app:proof_bound_CN}
% \section{Proof of Theorem \ref{thm:epsilon_bound_CN}} \label{app:proof_epsilon_bound_CN}

Since $\lim_{L\to \infty} \frac{\bar{s}^{L+1}}{1 - \bar{s}} = 0$, for all $\bar{s} \in [0, 1)$, Taylor approximation gives an approximation guarantee for maximizing the queue size function by Asm.~\ref{asmp:boundedPoly},  where the error of the approximation is given by Lem.~\ref{lem:gradientBias} as
\begin{align*}
\big\|\nabla G_z(\vc{y}) - \widehat{\nabla G_z^L}(\vc{y})\big\|_2 \leq 2\sqrt{|V||\mathcal{C}|}\varepsilon (L) = 2\sqrt{|V||\mathcal{C}|}\frac{s^{L+1}}{1-s}.
\end{align*} Then, $\varepsilon(L) \leq 2\sqrt{{|V||\mathcal{C}|}}\frac{\bar{s}^{L+1}}{1-\bar{s}}$.
% \begin{align*}
%     &\epsilon_{i, L}(\vc{y}) = \mathbb{E}_{\vc{y}}\left[R_L(\vc{x}) \mid x_{i+}\right] + \mathbb{E}_{\vc{y}}\left[R_L(x) \mid x_{i-}\right]\\
%     &=  \mathbb{E}_{\vc{y}} \left[\sum_{i \in \mathcal{D}} \left|R_{i,L}(s)\right| \mid x_{i+}\right] + \mathbb{E}_{\vc{y}} \left[\sum_{i \in \mathcal{D}} |R_{i,L}(s)| \mid x_{i-} \right]\\
%     &\leq \sum_{i \in \mathcal{D}} \mathbb{E}_{\vc{y}}[|R_{i,L}(s)| \mid x_{i+}] + \sum_{i \in \mathcal{D}} \mathbb{E}_{\vc{y}}[|R_{i,L}(s)| \mid x_{i-}]\\
%     &= \sum_{i \in \mathcal{D}} \mathbb{E}_{\vc{y}}\left[\frac{\rho_i^{L+1}}{1-\rho_i} \mid x_{i+}\right] + \sum_{i \in \mathcal{D}} \mathbb{E}_{\vc{y}}\left[\frac{\rho_i^{L+1}}{1-\rho_i} \mid x_{i-}\right]\\
%     &\leq 2 \sum_{i \in \mathcal{D}}\frac{\bar{\rho}^{L+1}}{1 - \bar{\rho}} \leq 2D \frac{\bar{\rho}^{L+1}}{1 - \bar{\rho}}
% \end{align*}
\end{proof}
\end{proof}

\subsection{Experimentation Details}\label{app:exps}

\textbf{Synthetic Datasets.} We generate directed bipartite graph instances with number of instances $|z| = 1$, $5$, $10$, $100$ and number of nodes $n = 200$, $400$, $1000$. Nodes are equally divided into left ($V_1$) and bottom ($V_2$) nodes where $|V_1| = |V_2|$. The seeds are always selected from $V_1$ and edges are placed between $V_1$ and $V_2$ so that the degrees of the nodes either follow a uniform or power law distribution. %We also generate a specific type of directed bipartite graphs where the famous greedy algorithm performs sub-optimally. For each $z$, we choose three nodes from the left nodes uniformly at random and add edges from these three nodes to the right nodes as shown in Figure~\ref{fig:greedysub}. In this example graph, we take partitions to be $\{0\}$ and $V_1 + V_2 \setminus \{0\}$ and set $k=1$ for each partition.

\noindent\textbf{Real Datasets.} We use two real-world datasets \texttt{Zachary Karate Club} (\texttt{ZKC}) \cite{zachary1977information} and \texttt{MovieLens} \cite{movielens}.
%, and \texttt{Twitter} \cite{pmlr-v97-kazemi19a}. 


% \begin{figure}[ht]
% \includegraphics[width=\linewidth/2]{images/greedy_trick.png}
% \centering
% \label{fig:greedysub}
% \end{figure}

\noindent\textbf{Influence Maximization. } We experiment on three synthetic datasets and one real dataset. For synthetic data, We generate directed bipartite graph instances with number of instances $|z| = 1$, $5$, $10$, $100$ and number of nodes $n = 200$, $400$, $1000$. Nodes are equally divided into left ($V_1$) and bottom ($V_2$) nodes where $|V_1| = |V_2|$. The seeds are always selected from $V_1$ and edges are placed between $V_1$ and $V_2$ so that the degrees of the nodes %either 
follow %a uniform (\texttt{SyntheticBipartiteUniform}) or 
power law (\texttt{SyntheticBipartitePowerlaw}) distribution. We construct a partition matroid of $m=4$ equal-size partitions of $V_1$ and set $k=1$ or $2$ elements from each partition. The real dataset is the \texttt{Zachary Karate Club} (\texttt{ZKC}) \cite{zachary1977information}. We generate $|z| = 20$ cascades following the independent cascade model \cite{kempe2003maximizing} using \texttt{Network Diffusion Library} \cite{rossetti2017ndlib}. The probability for each node to influence its neighbors is set to $p=0.5$. We divide the dataset into two partitions following its existing labels and set $k=3$.

\noindent\textbf{Facility Location. } For \emph{facility location} problems, we experiment on the \texttt{MovieLens} dataset \cite{movielens}. it has 1M ratings from $n = 6041$ users for $|z| = 4000$ movies. We treat movies as facilities, users as customers and ratings as $w_{i, j}$. We divide the movies into $m = 10$ partitions based on the first genre name listed for each movie and finally we set $k=2$. 

% \textbf{Summarization. }
% We run experiments on the \texttt{Twitter} dataset provided in \cite{kazemi2019submodular} for the summarization task. In this dataset, the keywords from each tweet are extracted and weighted proportionally according to the number of retweets each tweet received. There are $n = 42104$ tweets in this dataset and the tweets are already divided into $m = 30$ different news accounts. For each keyword $z$, the reward of $r_i$ is normalized with $\sum_i r_i$. Finally, we set $k = 2$.