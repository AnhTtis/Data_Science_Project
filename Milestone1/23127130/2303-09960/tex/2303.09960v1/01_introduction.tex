Submodular maximization is a true workhorse of data mining, arising in settings as diverse as  hyper-parameter optimization \cite{susubmodular}, feature compression \cite{bateni2019categorical}, text classification \cite{lei2019discrete}, and influence maximization \cite{goyal2011data, kempe2003maximizing}. Many of these interesting problems as well as variants can be cast as maximizing a submodular set function $f(S)$, defined over sets $S\subseteq V$ for some ground set $V$,  subject to a matroid constraint. Despite the NP-hardness of these problems, the so-called \emph{continuous-greedy} (CG) algorithm~\cite{calinescu2011maximizing}, can be used to construct a $1-1/e$-approximate solution in polynomial time. Interestingly, the solution is generated by first transferring the problem to the continuous domain, and solving a continuous optimization problem via gradient techniques. The  solution to this continuous optimization problem is subsequently rounded (via techniques such as pipage rounding \cite{ageev2004pipage} and swap rounding \cite{calinescu2011maximizing}), to produce an integral solution within a $1-1/e$ factor from the optimal.
 The continuous optimization problem solved by the CG  algorithm amounts to maximizing so-called  \emph{multilinear relaxation} of the original, combinatorial submodular objective.
In short, the multilinear relaxation  of a submodular function $f(S)$ is its expectation assuming its input $S$ is generated via independent Bernoulli trials, and is typically computed via sampling \cite{calinescu2011maximizing, vondrak2008optimal}.

Recently, a series of papers have studied an interesting variant called the \emph{stochastic submodular optimization} setting~\cite{asadpour2008stochastic, zhang2022stochastic, chen2018online,karimi2017stochastic,hassani2017gradient,mokhtari2020stochastic}. In this setting, the submodular objective function to be optimized is assumed to be of the form of an expectation, i.e., $f(S)=\mathbb{E}_{z\sim P}[f_z(S)]$, where $z$ is a random variable. Moreover, the optimization algorithm does not have access to the a function oracle (i.e., cannot compute the function itself). Instead it can only sample a random instantiation of $f_z(\cdot)$, different each time. This setting is of course of interest when the system or process that $f$ models is inherently stochastic (e.g., involves a system dependent on, e.g., user behavior or random arrivals) and the distribution governing this distribution is not a priori known. It is also of interest when the support of distribution $P$ is very large, so that the expectation cannot be completed efficiently. A classic example of the latter case is influence maximization (c.f.~Sec.~\ref{sec:probdef}), where the expectation  $f(S)$ cannot be computed efficiently or even in a closed form, even though samples $z\sim P$ can be drawn.

Interestingly, the fact that the classic continuous greedy algorithm operates in the continuous domain gives rise to a \emph{stochastic continuous greedy} (SCG) method for tackling the stochastic optimization problem~\cite{mokhtari2020stochastic}.  In a manner very similar to stochastic gradient descent, the continuous greedy algorithm can be modified to use \emph{stochastic gradients}, i.e., random variables whose expectations equal the gradient of the multilinear relaxation. In practice, these are computed by sampling \emph{two random variables in tandem}: $z\sim P$, which is needed to generate a random instance $f_z$, and $S$, the random input needed to compute the multilinear relaxation. As a result, the complexity of the SCG algorithm depends on the variance due to \emph{both} of these two variables.

We make the following contributions:
\begin{itemize}
    \item We use polynomial approximators, originally proposed by \"Ozcan et al.~\cite{ozcan2021submodular}, to reduce the variance of the stochastic continuous greedy algorithm. In particular, we eliminate one of the two sources of randomness of SCG, namely, sampling $S$. We do this by replacing the sampling estimator by a deterministic estimator constructed by approximating each $f_z(\cdot)$ with a polynomial function.
    \item We show that doing so \emph{reduces the variance} of the gradient estimation procedure used by SCG, but introduces a \emph{bias}. We then characterize the performance of SCG in terms of both the (reduced) variance and new bias term.
    \item We show that for several interesting stochastic submodular maximization problems, including influence maximization, the bias can be well-controlled, decaying exponentially with the degree of our polynomial approximators.
    \item Finally, we illustrate the advantage of our approach experimentally, over both synthetic and real-life datasets.
\end{itemize}