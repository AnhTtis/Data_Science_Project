In this section, we list several problems that can be tackled through our approach, also summarized in Tab.~\ref{table: problems}; these are similar to the problems considered by \"Ozcan et al.~\cite{ozcan2021submodular}, but cast into the stochastic submodular optimization setting. All problems correspond to trivially bounded variances $\sigma_0^2$ (again, because functions $f_z$ are bounded); we thus focus on determining their bias $\epsilon(L)$. For space reasons, we report Cache Networks (CN) in Table~\ref{table: problems}, but provide details for it in the \deleted{supplement }\deleted{(}App.~\ref{app:customBias}\deleted{)}.

\begin{table*}[ht] 
\caption{Summary of problems satisfying Asm.~\ref{asm:monSub}\&~\ref{asmp:boundedPoly}.}
\centering \label{table: problems}
\resizebox{\textwidth}{!}{
    \begin{tabular}{ |c|c|c|c|c|c| } 
    % \hline
    %  & \thead{variables} & \thead{$g_i(\mathbf{x})$} & \thead{$f_i(g_i(\mathbf{x}))$} & \thead{$f(S) \triangleq f(\vc{x})$} \\
     \cline{2-6}
     \multicolumn{1}{c}{} 
     & \multicolumn{1}{|c|}{\thead{Input}} 
     & \multicolumn{1}{|c|}{\thead{$g_z: \{0, 1\}^{|V|} \rightarrow [0, 1]$\\
                                   $\vc{x} \rightarrow g_z(\vc{x})$}} 
     % & \multicolumn{1}{|c|}{\thead{$h: [0, 1] \rightarrow \reals_+$\\
     %                               $s \rightarrow h(s)$}} 
     & \multicolumn{1}{|c|}{\thead{$f_z:\{0, 1\}^{|V|} \rightarrow \reals_+$\\
                                   $\vc{x} \rightarrow f_z(\vc{x})$}} %$F(S) \triangleq F(\vc{x}) = \sum\limits_i w_i f(s_i)$}
     & \multicolumn{1}{|c|}{\thead{$\hat{f}_z^L:\{0, 1\}^{|V|} \rightarrow \reals_+$\\
                                   $\vc{x} \rightarrow \hat{f}_z^L(\vc{x})$}}
     & \multicolumn{1}{|c|}{\thead{Bias\\
                                   $\varepsilon(L)$}}\\
     \hline
     \thead{SM} 
     & \makecell{Weighted bipartite graph \\
     %Partitions $\bigcup_{j=1}^M\{P_j\} = V$ \\
                $G = (V \cup P)$ weights $\vc{r}_z\in\reals_+^{n}$, \\
                and $\sum_{i=1}^n r_{i, z}=1$
                } 
     & $\sum_{i \in V\cap P_j} r_{i, z}x_i$ 
     & \makecell{$\sum_{j=1}^J h\left(g_z(\mathbf{x})\right)$,\\ where\\
     $h(s) = \log(1+s)$} 
     & \makecell{$\hat{h}^L(g_z(\mathbf{x}))$, \\
     where $\hat{h}^L$ is %given by 
     Eq.~\eqref{eq: taylor_f_iL}}
     & $\frac{1}{(L+1) 2^{L+1}}$ \\
     \hline
     \thead{IM} 
     & \makecell{Instances $G = (V, E)$\\
                 of a directed graph, \\
                 partitions $P_{v}^z \subset V$
                 %Sets $G$'s are instances\\
                 %that share set $V$ of\\
                 %nodes by the IC \cite{kempe2003maximizing} \\
                 %model, $v \in V$ are nodes\\
                 %of a directed graph, $P_v$\\
                 %is the set of all nodes\\
                 %having a directed path\\
                 %to $v$.
                 } 
     & $\sum\limits_{i \in V}\frac{1}{N}\Big(1 - \prod\limits_{u \in P_{i}^z}(1-x_u)\Big)$ 
     & \makecell{$h\left(g_z(\vc{x})\right)$\\
     where\\
     $h(s) = \log(1+s)$
     } 
     & \makecell{$\hat{h}^L(g_z(\mathbf{x}))$, \\
     where $\hat{h}^L$ is %given by 
     Eq.~\eqref{eq: taylor_f_iL}}
     & $\frac{1}{(L+1) 2^{L+1}}$ \\
     \hline
     \thead{FL} 
     & \makecell{Complete weighted bipartite\\
                 graph $G = (V \cup V')$\\
                 weights $w_{i_\ell, z} \in [0, 1]^{N \times |z|}$
                 } 
     & $\sum\limits_{\ell=1}^{N}(w_{i_\ell, z}-w_{i_{\ell+1}, z})\left(1-\prod\limits_{k=1}^\ell(1-x_{i_k})\right)$ 
     & \makecell{$h\left(g_z(\vc{x})\right)$\\
     where\\
     $h(s) = \log(1+s)$
     } 
     & \makecell{$\hat{h}^L(g_z(\mathbf{x}))$, \\
     where $\hat{h}^L$ is %given by 
     Eq.~\eqref{eq: taylor_f_iL}} 
     & $\frac{1}{(L+1) 2^{L+1}}$ \\
     \hline
     \thead{CN} 
     & \makecell{Graph $G = (V, E)$,\\
                 service rates $\mu \in \reals_+^{|z|}$, \\
                 requests $r \in \mathcal{R}$, $P_z$ path of $r$, \\
                 arrival rates $\lambda \in \reals_+^{|\mathcal{R}|}$\\
                 %$i \in E$ is an edge on\\
                 %a Kelly Cache Network,\\ 
                 %$\mu_i$ is the service rate \\ 
                 %of the queue in $i$,\\ 
                 %$\lambda^r$ is the arrival rate\\  
                 %of class $r \in \mathcal{R}$, \\
                 %$j$ is node in $P_i$ where \\ 
                 %$P_i$ is the path request \\
                 %$r$ follows.
                 } 
     & $\frac{1}{\mu_z}\sum_{r \in \mathcal{R}:z\in p^r} \lambda^r \prod_{k'=1}^{k_{p^r}(v)}(1-x_{p_k^r, i^r})$ 
     & \makecell{$h(g_z(\mathbf{0})) - h(g_z(\mathbf{x}))$\\
     where \\
     $h(s) = %\frac
     {s}/{(1 - s)}$} 
     & \makecell{$\hat{h}^L(g_z(\mathbf{x}))$, \\
     where $\hat{h}^L$ is %given by 
     Eq.~\eqref{eq: f_iL_CN}} 
     & $\frac{\bar{s}^{L+1}}{1-\bar{s}}$ \\
     \hline
    \end{tabular}}
    \vspace*{-10pt}
\end{table*}

\subsection{Data Summarization (SM)\cite{lin2011class, mirzasoleiman2016fast,kazemi2019submodular}}
In data summarization, ground set $V$ is a set of tokens, representing, e.g., words or sentences in a document. A corpus of documents $V_z$ is presented to us sequentially, and the goal is to select a ``summary'' $S\subseteq V$ that is representative of $V_z$. The summary should be simultaneously (a) representative of the corpus, and (b) diverse. %We present here the submodular objective function proposed by Kazemi et al. \cite{kazemi2019submodular}. %Assume that each token $i$ has a value $r_i\in [0,1]$, where $\sum_i r_i=1$. 

To be representative, the summary $S\subset V$ should contain tokens of high value, where the value of a token is document-dependent: %but should simultaneously be diverse. %Kazemi et al.~\cite{kazemi2019submodular} achieve this by mapping $V$ to a set $P$ of keywords. 
 for  document $z \in V_z$,  token $i \in V$  has a value $r_{i, z}\in [0,1]$, where $\sum_i r_{i, z}=1$. An example of such a value is the term frequency, i.e., the number of times the token appears in the document, divided by the document's length (in tokens). To be diverse, the summary should contain tokens that cover different subjects. To that end, if tokens are partitioned in to subjects, represented by a partition $\{P_j\}_{j=1}^J$ of $V$, the objective is given by $f(\mathbf{x}) = \mathbb{E}_z(f_z(\mathbf{x}))$ where %partitioning $V$ to sets $\{P_j\}_{j=1}^M$, where each set $P_j\subset V$ contains tokens that are similar. They then seek a summary that maximizes 
%\begin{equation}\label{eq:smob}
 $   f_z(\mathbf{x}) = \textstyle\sum_{j=1}^J h\left(\sum_{i \in V\cap P_j} r_{i, z}x_i\right),$
%\end{equation}%$S\subseteq V$ that covers high value topics; i.e., we 
and $h(s)=\log (1+s)$ is a non-decreasing concave function. %(e.g., , $h(s)=s^\alpha$, where $\alpha<1$, etc.). 
Intuitively, the concavity of $h$ suppresses the selection of similar tokens (corresponding to the same subject), even if they have high value, thereby promoting diversity.
% Objective \eqref{eq:smob} is clearly of form \eqref{eq:objFunc}. For example, for $h=\log(1+s)$, $f$ is monotone and submodular  \cite{lin2011class}, and is the sum of  compositions of $h$ with multilinear functions $g_z(\vc{x})=\sum_{i\in V} r_{i, z} x_{i},$ as illustrated in Tab.~\ref{table: problems}. Moreover, 
 Functions $f_z$ (and, thereby, also $f$) are monotone and submodular, and we can construct polynomial approximators $\hat{f}^L_z$ for them as indicated in Table~\ref{table: problems} by replacing $h$ with its %Taylor approximation
 %$h$ is analytic and can be approximated within arbitrary accuracy by its 
 $L^{\text{th}}$-order Taylor approximation around 1/2, given by:
\begin{equation} \label{eq: taylor_f_iL}
    \hat{h}^{L}(s) = \textstyle\sum_{\ell = 0}^L \frac{h^{(\ell)}(1/2)}{\ell!} (s - {1}/{2})^\ell.
\end{equation}
This is because the composition of polynomial $\hat{f}^L_z$ with  polynomial  $g_z$ in Table~\ref{table: problems} is again a polynomial. 
We show in \fullversion{\cite{ozcan2021submodular}}{App.~\ref{app: proof_bias_bound} \deleted{of the supplement}} that this estimator ensures that $f$ indeed satisfies Asm.~\ref{asmp:boundedPoly}. Moreover,  %$\epsilon_L(\vc{y})$ of this estimator $\widehat{\nabla G(\vc{y})}$ is bounded.
the estimator bias \emph{decays exponentially} with degree $L$ (see Tab.~\ref{table: problems} and App.~\ref{app:customBias}), meaning that polynomial number of terms suffice to reduce the bias to a desired level. %:
 %Therefore, it satisfies Asm.~\ref{asmp: mon_sub}.  Using $\texttt{supp}(\vc{x}_S)$, instead of $S$ as discussed in Section \ref{sec:tech}, we rewrite the diversity reward function as:
%\begin{equation} \label{eq: diversity_reward}
%    f(\vc{x}) = \sum_{j=1}^M \log\left(\sum_{i\in P_j} r_i x_{i} + 1\right)
%\end{equation}
%where $x_{i}=1$, if $i\in S$ and $x_{i}=0$ otherwise. It is straightforward to see that for $g_j(\vc{x})\equiv\sum_{i\in P_j}r_i x_{i}$, $g_j(\vc{x})$ is multilinear and for $h_j(s) \equiv \log(1 + s)$, $h_j(s)$ is non-decreasing and concave. Hence (\ref{eq: diversity_reward}) satisfies Asm.~\ref{asmp: mon_sub}. 
%Additionally, for the polynomial estimator %$\hat{h}_{L}(s)$ described in Eq.~\ref{eq: taylor_f_iL}, $h_j(s)$ and $\hat{h}_{L}(s)$ satisfy Asm.~\ref{asmp: f_isInForm}. According to this, we can use the $L^{th}$ Taylor polynomial of $h_j(s)$ around $1/2$ as its polynomial estimator $\hat{h}_{L}(s)$. %and use this polynomial estimator in (\ref{eq: poly_estimator}) to find a polynomial estimator of $G(\vc{y})$. 
%Furthermore, we can show that the estimator bias $\epsilon(L)$ of this estimator $\widehat{\nabla G(\vc{y})}$ is bounded.
%Our work directly allows for the optimization of such objectives over matroid constraints. For example, 
A partition matroid could be used with this objective to enforce that no more than $k_\ell$ sentences come from $\ell$-th user, etc. 
% \begin{lemma}
% Consider a diversity reward function $f: [0, 1]^d \rightarrow \reals_+$ which is $L + 1$ differentiable and for which the Taylor expansion exists at $\mathcal{1}/2$. Then, the gradient of the multilinear relaxation can be approximated by,
% \begin{align*}
%     \nabla G_i(\mathbf{y}) \approx \mathbb{E}[\hat{f}_L(x) | x_i = 1] - \mathbb{E}[\hat{f}_L(x) | x_i = 0]
% \end{align*}
% where $\hat{f}_L(x) = \sum_{l = 0}^L \frac{f^{(l)}(1/2)}{l!} (x - \frac{1}{2})^l$ and the error of the approximation is $\frac{D}{L \cdot 2^L}$ where $D = \max_{\mathbf{v} \in \mathcal{D}} \|\mathbf{v}\|_2$ and L is the order of the Taylor approximation.
% \end{lemma}
% \begin{proof}
% \begin{align*} 
%  \big\lvert f_i(g_i(x)) - \hat{f_L}(g_i(x)) \big\rvert = \Bigg\lvert \frac{f_i^{(L+1)}(s'_i)}{(L+1)!} \bigg(g_i(x)-\frac{1}{2}\bigg)^{L+1} \Bigg\rvert
% \end{align*}
% by the \textit{Lagrange remainder theorem}, where $s'_i$ is between $1/2$ and $g_i(x)$. For $f_i(g_i(x)) = log(1 + g_i(x))$, $\frac{d^{L+1}f_i(g_i(x))}{dg_i^{L+1}}$ is $(-1)^L \frac{L!}{(1+g_i(x))^{L+1}}$. Then, 
% \begin{align*}
%  \big\lvert f_i(g_i(x)) - \hat{f_L}(g_i(x)) \big\rvert = \Bigg\rvert \frac{\big(g_i(x)-\frac{1}{2}\big)^{L+1}}{L\big(1+s'_i\big)^{L+1}} \Bigg\lvert
% \end{align*}
% For $g_i(x), s_i' \in [0, 1]$,
% \begin{align*}
%  \big\lvert f_i(g_i(x)) - \hat{f_L}(g_i(x)) \big\rvert \leq \frac{\frac{1}{2}^{L+1}}{L\big(1+s'_i\big)^{L+1}} \leq \frac{1}{L \cdot 2^{L+1}}
% \end{align*}
% where $\lim_{L\to \infty} \frac{1}{L \cdot 2^{L+1}} = 0$.
% Then by Asm.~\ref{asmp: f_iL_exists}, Taylor approximation gives an approximation guarantee for maximizing diversity reward function where the error of the approximation is given by Lem.~\ref{lem: gradientBias} as
% \begin{align*}
%     &\epsilon_{i, L}(y) = \mathbb{E}_y[R_L(x) \mid x_i = 1] + \mathbb{E}_y[R_L(x) \mid x_i = 0]\\
%     &=  \mathbb{E}_y[\sum_{i \in \mathcal{D}} |R_{i,L}(g_i(x))| \mid x_i = 1] + \mathbb{E}_y[\sum_{i \in \mathcal{D}} |R_{i,L}(g_i(x))| \mid x_i = 0] \\
%     &\text{by Lem.~\ref{lem: gradientBias}}\\
%     &\leq \sum_{i \in \mathcal{D}} \mathbb{E}_y[|R_{i,L}(g_i(x))| \mid x_i = 1] + \sum_{i \in \mathcal{D}} \mathbb{E}_y[|R_{i,L}(g_i(x))| \mid x_i = 0]\\
%     &= \sum_{i \in \mathcal{D}} \mathbb{E}_y\Biggl[\Biggl|\frac{1}{L\bigl(2+2g_i(x)\bigl)^{L+1}}\Biggr| \mid x_i = 1\Biggr] \\
%     &+ \sum_{i \in \mathcal{D}} \mathbb{E}_y\Biggl[\Biggl|\frac{1}{L\bigl(2+2g_i(x)\bigr)^{L+1}}\Biggr| \mid x_i = 0\Biggr]\\
%     &\leq 2 \sum_{i \in \mathcal{D}}\frac{1}{L\big(2+2g_i(x)\big)^{L+1}} \leq \frac{D}{L \cdot 2^L}
% \end{align*} 
% \end{proof}
 \subsection{Influence Maximization (IM) \cite{kempe2003maximizing, chen2009efficient}} \label{sec: IM}
Given a directed graph $G = (V, E)$, we wish to maximize the expected fraction of nodes reached if we infect a set of nodes $S\subseteq V$ and the infection spreads via, e.g., the Independent Cascade (IC) model \cite{kempe2003maximizing}. Adding a concave utility to the fraction can enhance the value of nodes reached in  early stages.  
Formally, let $z$ can be a random simulation trace of the IC model, and $P_v^z\subseteq V$ is the set of nodes reachable from $v$ in a random simulation of the IC model. Then, the objective can be written as $f(\mathbf{x}) = \mathbb{E}_{z \sim P} [f_z(\mathbf{x})]$ where
%\begin{align}f(\vc{x}) = \textstyle\mathbb{E}_{z \sim P} \left[\frac{1}{N}\sum_{v \in V}\left(1 - \prod_{i \in P_v^z}(1-x_i)\right)\right],\!\!\! \end{align}
%where $P_v^z\subseteq V$ is the set of nodes reachable from $v$ in a random simulation of the IC model. %This is 
%defines a distribution $\mathcal{G}$ over $M$ instances $j \sim \mathcal{G}$ that share a set $V$ of nodes. The influence $g_j(S)$ of a set of nodes $S$ in instance $j$ is the fraction of nodes reachable from $S$ using the edges $E(j)$. Using $\texttt{supp}(\vc{x}_S)$, influence of $j$ defined by \cite{Karimi2017} as the following:
%This is a multilinear function. Our approach allows us to extend this  to maximizing the expectation of \emph{analytic functions} $h$ of the fraction of infected nodes. For example, 
%For a particular random instant $z$ of the IC model and for $h(s)=\log (1+s)$, we can express:
% \begin{equation} \label{eq: inf_max_z}
    $f_z(\vc{x}) =\textstyle h\left(g_z(\vc{x}) \right),$ %,
% \end{equation}
$h(s)=\log (1+s)$, and
%\begin{equation} \label{eq: IM_gz}
    $g_z(\vc{x}) =\textstyle \sum_{v \in V}\frac{1}{N}\big(1 - \prod_{i \in P_v^z}(1-x_i)\big)$ is the number of infected nodes under seed set  $\mathbf{x}$.  
%\end{equation}
%As a result our objective function becomes 
%\begin{equation} \label{eq:inf_max}
%    f(\mathbf{x}) = \mathbb{E}_{z \sim P} [f_z(\mathbf{x})].
%\end{equation}
%Definition of $g_j(\vc{x})$ in (\ref{eq: IM_gi}) is clearly multilinear and proven to be monotone and submodular \cite{krause2014submodular}, \cite{Karimi2017}. 
Since functions $g_z:[0,1]^N\to [0,1]$ are multilinear, monotone submodular and $h:[0,1]\to\reals$  is non-decreasing and concave, %Therefore, (\ref{eq: inf_max}) satisfies Asm.~\ref{asmp: mon_sub}. 
%As a result,
$f$ satisfies Asm.~\ref{asm:monSub} \cite{ozcan2021submodular}. %Moreover, 
Again, we can construct $\hat{f}^L$ by replacing $h$ by $\hat{h}^L$, given by Eq.~\eqref{eq: taylor_f_iL}.
%\begin{equation} \label{eq: taylor_f_iL}
%    \hat{h}_{L}(s) = \sum_{\ell = 0}^L \frac{h^{(\ell)}(1/2)}{\ell!} (s - {1}/{2})^\ell
%\end{equation}
%We show in App.~\ref{app: proof_bias_bound_IM} that this estimator 
This again ensures that $f$ indeed satisfies Asm.~\ref{asmp:boundedPoly}, and %$\epsilon_L(\vc{y})$ of this estimator $\widehat{\nabla G(\vc{y})}$ is bounded.
 the estimator bias again decays exponentially %as follows:
%The proof \deleted{of the theorem }can be found 
(see Tab.~\ref{table: problems}  and \fullversion{\cite{ozcan2021submodular}}{App.~\ref{app:customBias}}). Partition matroid constraints could be used in this setting to bound the number of seeds from some group (e.g., males/females, people in a zip code, etc.).

\subsection{Facility Location (FL)\cite{mokhtari2018conditional}}
%, cornuejols1977location, krause2014submodular}.} 
Given a weighted bipartite graph $G = (V \cup V_z)$ and weights $w_{i, z} \in [0, 1]$, $ i \in V$, $ z \in V_z$, we wish to maximize:
\begin{equation}\label{eq:originalFL}
    f(S) = \textstyle\mathbb{E}_{z \sim P} \left[h(\max_{i \in S} w_{i, z})\right], 
\end{equation}
where $h(s)=\log(1+s)$.
Intuitively, $V$ and $V'$ represent facilities and customers respectively and $w_{v, v'}$ is the utility of facility $v$ for customer $v'$. The goal is to select a subset of facility locations $S\subset{V}$ to maximize the total utility, assuming every customer chooses the facility with the highest utility in the selection $S$; again, adding the concave function $h$ adds diversity, favoring the satisfaction of customers that are not already covered. This too becomes a coverage problem 
%In order to reformulate $\max_{i \in S} w_{i, j}$ using the binary support vector $\vc{x}$ of $S$, we 
by observing that \cite{karimi2017stochastic}:
%\begin{equation}
    $\max_{i \in S} w_{i, z}  = \sum\limits_{\ell=1}^{n}(w_{i_\ell, z}-w_{i_{\ell+1}, z})\big(1-\prod\limits_{k=1}^\ell(1-x_{i_k})\big),$
%\end{equation}
where,
 for a given $z \in V_z$, weights have been pre-sorted in a descending order as $w_{i_1,z} \geq \ldots \geq w_{i_n,z}$. %where $i_1 \in V$ is the facility providing the maximum utility to customer $j$ and $i_n \in V$ is the facility providing the minimum utility to customer $j$. Then we rewrite $\max_{i \in S} w_{i, j}$ as
and %$l_j^{(i)} = w_{j,i}$ for each $i \in M$ and $j \in V %= \{1, \ldots, n\}$, with 
$w_{i_{n+1},j} \triangleq 0$. %We can again extend this problem to maximizing analytic functions $h$ of the utility of a user. For example, for $h(s)=\log(1+s)$, we can maximize 
%\begin{equation} \label{eq:FLlog}
%    f_z(\mathbf{x}) = \textstyle \log\left(1+g_z(\vc{x}) \right).
%\end{equation}
In a manner similar to Sec~\ref{sec: IM}, we can show that this function again satisfies Asm.~\ref{asm:monSub} and~\ref{asmp:boundedPoly}, using again the $L^{\text{th}}$-order Taylor approximation of $h$, given by Eq.~\eqref{eq: taylor_f_iL}; this will again lead to a bias that decays exponentially (see Tab.~\ref{table: problems} and App.~\ref{app:customBias}). 
We can again optimize such an objective over arbitrary matroids, which can enforce, e.g., that no more than $k$ facilities are selected from a geographic area or some other partition of $V$.%bounded by \eqref{eq:eLbound}.
% \begin{theorem} \label{thm: epsilon_bound_FL} \textcolor{red}{Practically the same with Theorem 6.1} Assume a facility location function $f:~\{0,1\}^{N} \rightarrow \reals_+$ that is given by (\ref{eq:FLlog}). Then, consider Algorithm \ref{alg:cont-greed} in which $\nabla G(\mathbf{y}_K)$ is estimated via the polynomial estimator given in (\ref{eq: poly_estimator}) where $\hat{f}_{L}(\vc{x})$ is the $L^{th}$ Taylor polynomial of $f(\vc{x})$ around $1/2$. Then, the bias of the estimator is bounded by 
% \begin{equation*}
%     \| \epsilon_L \|_2 \leq \frac{\sqrt{N}}{(L+1) 2^{L}}
% \end{equation*}
% where the bias is the $l_2$ norm of the residual error vector and $N$ is the number of facilities. \end{theorem}
% The proof of the theorem is the same as the proof of Thm.~ \ref{thm: epsilon_bound_IM} and can be found in App. \ref{app: proof_bias_bound_IM}.
%, set $V=[N]$ of facility locations to serve a set $[M]$ of customers. Each facility $i \in [N]$ provides a service value $M_{ij}$ to each customer $j\in [M]$.  The goal is to select a subset of  facility locations to maximize the total service value, assuming every customer chooses the facility with the highest service value.
             %Given a complete\\
             %weighted bipartite graph\\
             % ;\\
             %, and $w_{x,y}$ \\
             %represent  and utilities\\
             %respectively. \\
             %and $m_{n+1} = 0$.
%Formally, we wish to choose a set $S\subseteq V$ to maximize:
% where $f(\emptyset)=0$. If $M_{ij} \geq 0$ for all $i, j$, then $f(S)$ is monotone submodular \cite{frieze1974cost}.  
% Karimi et. al. \cite{Karimi2017} suggested the following  reformulation for  the facility location problem \eqref{eq:originalFL}:
% \begin{align}\label{eq:FL}
%      f(S) &= \sum_{j=1}^M g_j(\vc{x}_S),
% \end{align}
% and the functions $g_j:[0,1]^N \to \reals_+$ are defined as follows
% \begin{align*}
%     g_j(\vc{x}) = \sum_{i=1}^{N}(m^{(j)}_i-m^{(j)}_{i+1})\Big(1-\prod_{k=1}^i(1-x_k)\Big)
% \end{align*}
% where $m^{(j)}_i=M_{lj}$ with $m_{N+1}=0 \leq m^{(j)}_N\leq \ldots \leq  m^{(j)}_1.$ We see that \eqref{eq:FL} has the form \eqref{eq:hatW}, where, $\mathcal{J} = [N] \times [M],$  $\beta_{(i,j)} = m^{(j)}_i-m^{(j)}_{i+1},$ and $S_{(i,j)} = [i].$
% which as a result of Lem.~\ref{lem:compwmnf} is monotone and submodular. 
% \begin{align*}
%     F(\mathbf{x}) = \mathbb{E}_S [f(S)] = \frac{1}{|Y|} \sum_{y \in Y} \log\left(f_y(\mathbf{x}) + 1 \right),
% \end{align*}




