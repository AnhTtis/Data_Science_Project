
\subsubsection{Submodularity and Matroids.}\label{sec:submat}
Given a ground set $V = \{1, \ldots, n\}$ of $n$ elements, a set function $f:2^V\rightarrow\reals_+$ is submodular if and only if $f(B \cup \{e\}) - f(B) \leq f(A\cup \{e\}) - f(A)$, for all $A\subseteq B\subseteq V$ and $e\in V$. Function $f$ is \emph{monotone} if $f(A)\leq f(B)$, for every $A\subseteq B$.

\noindent \textbf{Matroids.} Given a ground set $V$, a matroid is a pair $\mathcal{M}=(V, \mathcal{I})$, where $\mathcal{I}\subseteq 2^V$ is a collection of \emph{independent sets}, for which the following hold: 
(a) if $B\in \mathcal{I}$ and $A \subset B$, then $A \in \mathcal{I}$, and (b)
  if $A, B\in \mathcal{I}$ and $|A|< |B|,$ there exists $x \in B\setminus A$ s.t. $A\cup\{x\}\in \mathcal{I}$.
 The \emph{rank} of a matroid $r_{\mathcal{M}}(V)$ is the largest cardinality of its elements, i.e.:
%\begin{align*}
  $  r_{\mathcal{M}}(V) = \max\{|A|: {A}\in\mathcal{I}\}.$
%\end{align*}
We introduce two examples of matroids:
\begin{enumerate}
    \item \textbf{Uniform Matroids.} The uniform matroid with cardinality $k$ is $\mathcal{I}=\{S\subseteq V, \, |S|\leq k\}$.
    \item \textbf{Partition Matroids.} Let $\mathcal{B}_1,\ldots, \mathcal{B}_m\subseteq V$ be a partitioning of $V$, i.e., $ \bigcap_{\ell=1}^m\mathcal{B}_\ell =\emptyset$ and $\bigcup_{\ell=1}^m\mathcal{B}_\ell = V$. Let also $k_\ell\in \mathbb{N}, \ell=1,\ldots,m$, be a set of cardinalities.   A partition matroid is defined as $\mathcal{I}=\{S\subseteq 2^V \, \mid  \, |S\cap \mathcal{B}_\ell|\leq k_{\ell}, \text{ for all } \ell=1,\ldots, m\}.$  
\end{enumerate}


\subsection{Problem Definition}\label{sec:probdef}
In this work, we focus on \textit{discrete  stochastic submodular maximization} problems. More specifically, we consider set function $f: 2^V \rightarrow \mathbb{R}_+$ of the form:
%\begin{equation} \label{eq:objFunc}
 $   f(S) = \mathbb{E}_{z \sim P}[f_z(S)],$ $S \subseteq V,$
%\end{equation}
where  $z$ is the realization of the random variable $Z$ drawn from a distribution $P$ over a probability space $(V_z,P)$. For each realization of $z \sim P$, the set function $f_z: 2^V \rightarrow \mathbb{R}_+$ is monotone and submodular. Hence, $f$ itself is monotone and submodular. The objective is to maximize $f$ subject to some constraints (e.g., cardinality or matroid constraints) by only accessing to i.i.d. samples of $f_{z \sim P}$. In other words, we wish to solve:
\begin{equation} \label{prob:stochsubmax}
    \max_{S \in \mathcal{I}} f(S) = \max_{S \in \mathcal{I}} \mathbb{E}_{z \sim P}[f_z(S)],
\end{equation}
where  $\mathcal{I}$ is a general matroid constraint.

Stochastic submodular maximization problems are of interest in the absence of the oracle that provides the exact value of $f(S)$: one can only access $f_z(S)$, for random instantiations $z\sim P$.  A well-known motivational example is contagion propagation in a network (a.k.a., the influence maximization problem \cite{kempe2003maximizing}). Given a graph with node set $V$, reachability of nodes from seeds are determined by sampling sub-graph $G=(V, E)$, via, e.g., the Independent Cascade or the Linear Threshold model~\cite{kempe2003maximizing}. The random edge set, in this case, plays the role of $z$, and the distribution over graphs the role of $P$.  The function $f_z(S)$ represents the ratio of nodes reachable from the seeds $S$ under the connectivity induced by edges $E$ in this particular realization of $z$. %The Independent Cascade model \cite{kempe2003maximizing} introduces a probabilistic model to deal with the uncertainties arising from each realization by defining a distribution $z$ over instances $z \sim P$ that share a set $V$ of nodes. The influence a set of seeds $S$ is then represented by the expectation 
The goal is to select seeds $S$ that maximize $f(S) = \mathbb{E}_{z \sim P}[f_z(S)]$; both $f$ and $f_z$ are monotone submodular functions; however computing $f$ in a closed form is hard, and $f(\cdot)$ can only be accessed through random instantiations of $f_z(\cdot)$.

% Instead of solving the problem in (\ref{prob:stochsubmax}) one can solve the continuous optimization problem
% \textcolor{red}{define G(y) in terms of expectation over S expectation of x sampled from y}

\subsection{Change of Variables and Multiliear Relaxation}  
There is a 1-to-1 correspondence between a binary vector $\vc{x}\in \{0,1\}^{n}$ and its support $S=\texttt{supp}(\vc{x})$. Hence, a set function $f: 2^V \rightarrow \reals_+$ can be interpreted as $f: \{0,1\}^n \rightarrow \reals_+$ via: 
$f(\vc{x}) \triangleq f(\texttt{supp}(\vc{x}))$ for $\vc{x} \in \{0,1\}^n$. We adopt this convention for the remainder of the paper. 
We also treat matroids as subsets of $\{0,1\}^n$, defined consistently with this change of variables via $\mathcal{M}=\{\vc{x}\in\{0,1\}^n: \supp(\vc{x})\in \mathcal{I}\}.$ For example, a partition matroid is: 
%\begin{align} \label{eq:part_mat}
$\mathcal{M} = \textstyle\left\{\vc{x} \in \{0,1\}^n\,\mid \bigcap_{\ell=1}^m  \left(\sum_{i\in B_\ell} x_i\leq k_\ell\right)\right\}.$
%\end{align} 
The \emph{matroid polytope} $\mathcal{C}\subseteq [0,1]^{n}$ is the convex hull of matroid $\mathcal{M}$, i.e., $\mathcal{C} = \cvx(\mathcal{M}).$


We define the \emph{multilinear relaxation} of $f$ as:
\begin{align}\label{eq: multilinear}
\begin{split}
    G(\mathbf{y}) & = \mathbb{E}_{S \sim \mathbf{y}}[f(S)] = \sum_{S \subseteq V} f(S) \prod_{i \in S} y_i \prod_{j \notin S} (1 - y_j)\\&=\mathbb{E}_{\mathbf{x} \sim \mathbf{y}}[f(\mathbf{x})]=\sum_{\mathbf{x}\in \{0,1\}^n} f(\mathbf{x}) \prod_{i\in V}y_i^{x_i}(1-y_i)^{(1-x_i)},  \quad \text{for}~\mathbf{y} \in [0, 1]^n.
    \end{split}
\end{align}
In other words, $G:[0,1]^n\to\reals_+$ is the expectation of $f$, assuming that $S$ is random and generated from independent Bernoulli trials: for every $i\in V$, $P(i\in S)=y_i$. 
%
The multilinear relaxation of $f$ satisfies several properties. First, it is indeed a relaxation/extension of $f$ over the (larger) domain $[0,1]^n$: for $\mathbf{x}\in \{0,1\}^n$, $G(\mathbf{x})=f(\mathbf{x})$, i.e., $G$ agrees with $f$ on integral inputs. Second, it is \emph{multilinear} (c.f.~Sec.~\ref{sec:multilinear}), i.e., affine w.r.t.~any single coordinate $y_i$, $i\in V$, when keeping all other coordinates $\mathbf{y}_{-i}=[y_j]_{j\neq i}$ fixed. 
%
Finally, in the context of stochastic submodular optimization,  it is an expectation that involves \emph{two sources of randomness}: (a) $z\sim P$, i.e., the random instantiation of the objective, \emph{as well as } (b) $\mathbf{x}\sim \mathbf{y}$, i.e., the independent sampling of the Bernoulli variables (i.e., the set $S$). In particular,  we can write: 
\begin{align}
    G(\mathbf{y}) = \mathbb{E}_{z\sim P}[G_z(\mathbf{y})],~\text{where}~G_z(\mathbf{y}) = \mathbb{E}_{\mathbf{x}\sim \mathbf{y}}[f_z(x)]~\text{is the multilinear relaxation of}~ f_z(\cdot).  
\end{align}

%is the multilinear extension of the function $f$ assuming $S$ is sampled at random, where $i \in S$ with probability $y_i$ and $y_i$ denotes the $i-$th element of the vector $\mathbf{y}$ and the convex set $\mathcal{C} = \text{conv}\{1_{I} : I \in \mathcal{I}\}$ is the down-closed matroid polytope.


%\textbf{Change of Variables. } There is a one-to-one correspondence between a binary vector $\mathbf{x}\in \{0,1\}^{n}$ and its support $S=\texttt{supp}(\mathbf{x})$. Hence, a set function $f: 2^V \rightarrow \reals_+$ can be interpreted as $f: \{0,1\}^n \rightarrow \reals_+$ via: 
%$f(\mathbf{x}) \triangleq f(\texttt{supp}(\mathbf{x}))$ for $\mathbf{x} \in \{0,1\}^n$. We adopt this convention for the remainder of the paper.

\subsection{Stochastic Continuous Greedy Algorithm}
The stochastic nature of the set function $f(S)$ requires the use the \emph{Stochastic Continuous Greedy (SCG)} algorithm \cite{mokhtari2020stochastic}. This is a stochastic variant of the  continuous greedy algorithm (method) \cite{vondrak2008optimal}, to solve (\ref{prob:stochsubmax}). The SCG algorithm uses a common averaging technique in stochastic optimization and computes the estimated gradient $\mathbf{d}_t$ by the recursion
\begin{equation}\label{eq:avgGrad}
    \mathbf{d}_t = (1 - \rho_t)\mathbf{d}_{t-1} + \rho_t \nabla G_{z_t} (\mathbf{y}_t),
\end{equation}
where $\rho_t$ is a positive step size and the algorithm initially starts with $\mathbf{d}_0 = \mathbf{y}_0 = \mathbf{0}$. Then, it proceeds in iterations, where in the $t$-th iteration it finds a feasible solution as follows
\begin{equation}
    \mathbf{v}_t \in \arg\max_{\mathbf{v} \in \mathcal{C}} \{\mathbf{d}_t^T\mathbf{v}\},
\end{equation}
where $\mathcal{C}$ is the matroid polytope (i.e., convex hull) of matroid $\mathcal{M}$.
After finding the ascent direction $\mathbf{v}_t$, the current solution $\mathbf{y}_t$ is updated as
\begin{equation}
    \mathbf{y}_{t+1} = \mathbf{y}_t + \frac{1}{T}\mathbf{\replaced{v}{y}}_t, 
\end{equation}
where $1/T$ is the step size. The steps of the stochastic continuous greedy algorithm are outlined in Algorithm~\ref{alg: SCG}.
The (fractional) output of Algorithm~\ref{alg: SCG} is within a $1-1/e$ factor from the optimal solution to Problem~(\ref{prob:stochsubmax}) (see Theorem~\ref{thm:main} below). This fractional solution can subsequently be rounded in polynomial time to produce a solution  with the same approximation guarantee w.r.t.~to Problem~(\ref{prob:stochsubmax}) using, e.g., either the pipage rounding \cite{ageev2004pipage} or the swap rounding \cite{chekuri2010dependent} methods.
\begin{algorithm}[!t]
    \caption{Stochastic Continuous Greedy (SCG)}\label{alg: SCG}
    \textbf{Require:} Step sizes $\rho_t > 0$. Initialize $\mathbf{d}_0 = \mathbf{y}_0 = \mathbf{0}.$
    \begin{algorithmic}[1] % The number tells where the line numbering should start
            \For{$t = 1, 2, \ldots, T$}
                \State Compute $\mathbf{d}_t = (1 - \rho_t) \mathbf{d}_{t-1} + \rho_t \nabla G_{z_t}(\mathbf{y}_t)$;
                \State Compute $\mathbf{v}_t \in \arg\max_{\mathbf{v} \in \mathcal{C}} \{\mathbf{d}_t^T \mathbf{v}\}$;
                \State Update the variable $\mathbf{y}_{t+1} = \mathbf{y}_t + \frac{1}{T} \mathbf{v}_t$;
            \EndFor
    \end{algorithmic}
\end{algorithm}
\subsubsection{Sample Estimator.}
The gradient $\nabla G_{z_t}$ is needed to perform step \eqref{eq:avgGrad}; computing it directly via Eq.~\eqref{eq: multilinear}. requires exponentially many calculations. Instead, both Calinescu et al. \cite{calinescu2011maximizing} and Mokhtari et al. \cite{mokhtari2020stochastic} estimate it via \emph{sampling}. % Since $f(S) = \mathbb{E}_{z \sim P}[f_z(S)]$, we obtain $G(\mathbf{y}) = \mathbb{E}_{z \sim P}[G_z(\mathbf{y})]$ where $G$ and $G_z$ denote the multilinear extension of $f$ and $f_z$, respectively. 
In particular, due to multilinearity (i.e., the fact that $G_z$ is affine w.r.t. a coordinate $x_i$, we have: 
\begin{equation}\label{eq:partialGrad}
    \frac{\partial G_z(\mathbf{y})}{\partial x_i} = G_z ([\mathbf{y}]_{+i}) - G_z ([\mathbf{y}]_{-i}),\quad\text{for all}~i\in V,
\end{equation}
where $[\mathbf{y}]_{+i}$ and $[\mathbf{y}]_{-i}$ are equal to the vector $\mathbf{y}$ with the $i$-th coordinate set to $1$ and $0$, respectively. The gradient of $G$ can thus be estimated by (a) producing $N$ random samples $\mathbf{x}^{(l)}$, for $l \in \{1, \ldots, N\}$ of the random vector $\mathbf{x}$, and (b) computing the empirical mean of the r.h.s. of (\ref{eq:partialGrad}), yielding
\begin{equation}\label{eq:sampleEst}
    \frac{\partial \widehat{G_{z}(\mathbf{y})}}{\partial x_i} = \frac{1}{N} \sum_{l=1}^N \left(f_z ([\mathbf{x}^{(l)}]_{+i}) - f_z ([\mathbf{x}^{(l)}]_{-i})\right), \quad\text{for all}~i\in V.
\end{equation}


Mokhtari et al. \cite{mokhtari2020stochastic} make the following assumptions:
\begin{assumption} \label{asm:monSub}
    Function $f: \{0, 1\}^n \rightarrow \mathbb{R}_+$ is monotone and submodular.
\end{assumption}

\begin{assumption} \label{asm:boundedNorm}
    The Euclidean norm of the elements in the constraint set $\mathcal{C}$ are uniformly bounded, i.e., for all $\mathbf{y} \in \mathcal{C}$, there exists a $D$ s.t. $\|\mathbf{y}\| \leq D.$
\end{assumption}


Under these assumptions, SCG combined with the sampling estimator in Eq.~\eqref{eq:partialGrad}, yields the following guarantee:
\begin{theorem} \label{thm:theirs}
    % \hl{\textup{[Their Theorem]}} 
    [Mokhtari et al. \cite{mokhtari2020stochastic}] Consider Stochastic Continuous Greedy (SCG) outlined in Algorithm~\ref{alg: SCG}, with $\nabla G_{z_t}(\mathbf{y}_t)$ replaced by $\nabla \widehat{G_{z_t}(\mathbf{y}_t)}$ given by (\ref{eq:sampleEst}). Recall the definition of the multilinear extension function $G$ in \eqref{eq: multilinear} and set the averaging parameter as $\rho_t = 4/(t+8)^{2/3}$. If Assumptions~\ref{asm:monSub}~\&~\ref{asm:boundedNorm} are satisfied, then the iterate $\mathbf{y}_T$ generated by SCG satisfies the inequality
    \begin{equation}
        \mathbb{E}\left[G(\mathbf{y}_T)\right] \geq (1-1/e)OPT - \frac{15DK}{T^{1/3}} - \frac{f_{\max}rD^2}{2T},
    \end{equation}
    where $OPT = \max_{\mathbf{y} \in \mathcal{C}} G(\mathbf{y})$ and $K = \max\{3\|\nabla G(\mathbf{y}_0) - \mathbf{d}_0\|, 4 \sigma + \sqrt{3r} f_{\max}D\},$ where $D$ is the diameter of the convex hull $\mathcal{C}$, $f_{\max}$ is the maximum marginal value of the function $f$, i.e., $f_{\max} = \max_{i \in \{1, \ldots, n\}} f(\{i\})$,  $r$ is the rank of the matroid $\mathcal{I}$, and
    %\begin{align}
    $\sigma^2 =\sup_{\mathbf{y}\in \mathcal{C}} \mathbb{E}\left[\|\widehat{\nabla G_z(\mathbf{y}) } - G(\mathbf{y})  \|\right],$
    %\end{align}
    where $\widehat{\nabla G_z}$ is the sample estimator given by Eq.~\eqref{eq:sampleEst}.
    %2\sqrt{n}\sqrt{\max_{j \in [n]} \mathbb{E}[f_z(\{j\})^2]}
\end{theorem}
Thus, by appropriately setting the number of iterations $T$, we can produce a solution that is arbitrarily close to $1-1/e$ from the optimal (fractional) solution. Again, this can be subsequently rounded (see, e.g., \cite{ageev2004pipage,calinescu2011maximizing}) to produce an integer solution with the same approximation guarantee.
%
It is important to note that the number of steps required depends on $\sigma^2$, which is a (uniform over $\mathcal{C}$) bound on the variance of the estimator given by Eq.~\eqref{eq:sampleEst}. This variance contains \emph{two sources of randomness}, namely $z\sim P$, the random instantiation, and $\mathbf{x}\sim\mathbf{y}$, as multiple such integer vectors/sets are sampled in Eq~\eqref{eq:sampleEst}. In general, the variance will depend on the number of samples $N$ in the estimator, and will be bounded (as $G$ is bounded).\footnote{For example, even for $N=1$, the submodularity of $f_z$ and Eq.~\eqref{eq:partialGrad} imply that $\sigma^2\leq 2 {n}{\max_{j \in [n]} \mathbb{E}[f_z(\{j\})^2]}$ \cite{mokhtari2020stochastic}, though this bound is loose/a worst-case bound.} 
\subsection{Multilinear Functions and the Multilinear Relaxation of a Polynomial}\label{sec:multilinear}
Recall that a \emph{polynomial} function $p: \reals^n \rightarrow \mathbb{R}$ can be written as a linear combination of several monomials, i.e.,
\begin{equation} \label{eq:polyFormat}
     p(\mathbf{y}) = c_0+\sum_{\ell \in \mathcal{I}} c_{\ell} \prod_{i \in \iset{J}{\ell}} y_i^{k_i^{\ell}},
\end{equation}
where $c_{\ell}~\in~\reals$ for $\ell$ in some index set $\mathcal{I}$, subsets $\iset{J}{\ell}~\subseteq~V$ determine the terms of each monomial, and %\footnote{By convention, if $\mathcal{J}_{\ell} = \emptyset$, we set $\prod_{i\in\mathcal{J}_{\ell}} y_i = 1$.}
, and $\{k_i^{\ell}\}_{i \in \iset{J}{\ell}} \subset \mathbb{N}$ are natural exponents. W.l.o.g. we assume that $k_i^{\ell} \geq 1$ (as variables with zero exponents can be ommited). The degree of the monomial indexed by $\ell\in\iset{I}{}$ is $k^\ell=\sum_{i\in \iset{J}{\ell}} {k_i^\ell}$, and the degree of polynomial $p$ is $\max_{\ell\in \iset{I}{}} k^\ell$, i.e., the largest degree across monomials.


A function $f: \reals^N \rightarrow \reals$ is \emph{multilinear} if it is affine w.r.t.~each of its coordinates \cite{broida1989comprehensive}. %A function $g: \reals^n \rightarrow \reals$ that can be written as the sum of monomials is called a \emph{polynomial}. 
Alternatively, multilinear functions are polynomial functions in which the degree of each variable in a monomial is at most $1$; that is, multilinear functions can be written as:
\begin{equation} \label{eq:multi}
    f(\mathbf{y}) = c_0'+\sum_{\ell \in \mathcal{I}} c_{\ell}' \prod_{i \in \iset{J}{\ell}} y_i,
\end{equation}
where $c_{\ell}~\in~\reals$ for $\ell$ in some index set $\mathcal{I}$, and subsets $\iset{J}{\ell}~\subseteq~V$, again determining monomials of degree \emph{exactly equal to} ${|\iset{J}{\ell}|}.$ %\footnote{By convention, if $\mathcal{J}_{\ell} = \emptyset$, we set $\prod_{i\in\mathcal{J}_{\ell}} x_i = 1$.}
Given a polynomial $p$ defined by the parameters in Eq.~(\ref{eq:polyFormat}), let  %a \textcolor{red}{multilinearization operator} as follows
\begin{equation} \label{eq:multiFormat}
     \dot{p}(\mathbf{y}) = c_0+\sum_{\ell \in \mathcal{I}} c_{\ell} \prod_{i \in \iset{J}{\ell}} y_i,
\end{equation}
be the multilinear function resulting from  $p$, \emph{by replacing all its exponents $k_i^{\ell} \geq 1$ with $1$}. We call this function the \emph{multilinearization} of $p$. The multilinearization of $p$ is inherently linked to its multilinear relaxation:
\begin{lemma}[\"Ozcan et al.~\cite{ozcan2021submodular}] \label{lem:relaxation_of_multi}
Let $p: [0, 1]^n \rightarrow \mathbb{R}$ be an arbitrary polynomial and let $\dot{p}:\reals^n \rightarrow \reals_+$ be  its multilinearization, given by Eq.~\eqref{eq:multiFormat}. Let $\vc{x} \in \{0, 1\}^n$ be a random vector of independent Bernoulli coordinates parameterized by $\vc{y}\in~[0, 1]^n$. Then, $\mathbb{E}_{\vc{x}\sim\vc{y}}[p(\vc{x})] = \mathbb{E}_{\vc{x}\sim\vc{y}}[\dot{p}(\vc{x})] =\dot{p}(\vc{y}).$
\end{lemma}
\begin{proof}
Observe that 
%\begin{equation} \label{eq:multilin_rel}
 $   \dot{p}(\mathbf{x}) = p(\mathbf{x})$, for all $\mathbf{x} \in \{0, 1\}^n.$ 
%\end{equation} 
This is precisely because $x^{k} = x$ for  $x \in \{0, 1\}$ and all $k \geq 1$.  The first equality therefore follows. On the other hand, $\dot{p}(\mathbf{x})$ is the multilinear function given by Eq.~\eqref{eq:multiFormat}. Hence
%\begin{align*}
 $   \mathbb{E}_{\vc{x}\sim\vc{y}}[\dot{p}(\vc{x})] = \mathbb{E}_{\vc{x}\sim\vc{y}}\left[ c_0+\sum_{\ell \in \mathcal{I}} c_{\ell} \prod_{i \in \iset{J}{\ell}} x_i\right] = c_0+\sum_{\ell \in \mathcal{I}} \mathbb{E}_{\vc{x}\sim\vc{y}}\left[ \prod_{i \in \iset{J}{\ell}} x_i\right]=c_0+\sum_{\ell \in \mathcal{I}}  \prod_{i \in \iset{J}{\ell}}\mathbb{E}_{\vc{x}\sim\vc{y}}\left[ x_i\right]=\dot{p}(\mathbf{y}),$
%\end{align*}
where the second to last equality holds by the independence across $x_i$, $i\in V$.
\end{proof}
An immediate consequence of this lemma is that the multilinear relaxation of any polynomial function can be computed \emph{without sampling}, by simply computing its multilinearization. This is of particular interest of course for submodular functions that are themselves polynomials (e.g., coverage functions~\cite{karimi2017stochastic}). \"Ozcan et al.~extend this to submodular functions that can be written as compositions of a scalar and a polynomial function, by approximating the former via its Taylor expansion. We extend  and generalize this to the case of stochastic submodular functions, so long as the latter can be approximated arbitrarily well by polynomials.  