  While  submodular optimization problems are generally NP-hard, the celebrated greedy algorithm \cite{nemhauser1978analysis} attains a $(1-1/e)$ approximation ratio for  submodular maximization subject to uniform matroids and a $1/2$ approximation ratio for general matroid constraints. As discussed in the introduction, the  continuous greedy algorithm \cite{calinescu2011maximizing} restores the $(1-1/e)$ approximation ratio by lifting the discrete problem to the continuous domain via the multilinear relaxation. %It is worth to mention here that the multilinear relaxation is a DR-submodular function, a.k.a. a continuous function with the diminishing returns property.

Stochastic submodular maximization, in which the objective is expressed as an expectation, has gained a lot of interest in the recent years \cite{asadpour2008stochastic, zhang2022stochastic, chen2018online}. Karimi et al. \cite{karimi2017stochastic} use a concave relaxation method that achieves the $(1-1/e)$ approximation guarantee, but only  for the class of submodular coverage functions. Hassani et al.~\cite{hassani2017gradient} provide projected gradients methods for the general case of stochastic submodular problems that achieve $1/2$ approximation guarantee.  Mokhtari et al. \cite{mokhtari2020stochastic} propose stochastic  conditional gradient methods for solving both minimization and maximization  stochastic submodular optimization problems. Their method for maximization, Stochastic Continous Greedy (SCG) can be interpreted as a stochastic variant of the continuous greedy algorithm \cite{vondrak2008optimal, calinescu2011maximizing} and achieves a tight $(1-1/e)$ approximation guarantee for monotone and submodular functions. %However, all these methods suffer from two sources of randomness (one comes from sampling the objective function and the other comes from estimating the multilinear relaxation via sampling its inputs).

Our work builds upon and relies on the approach by  \"{O}zcan et al.~\cite{ozcan2021submodular}, who studied ways of accelerating the computation of gradients via a polynomial estimator. Extending on the work of Mahdian et al.~\cite{mahdian2020kelly},  \"{O}zcan et al. show that submodular functions that can be written as compositions of (a) an analytic function and (b) a multilinear function can be arbitrarily well approximated via Taylor polynomials; in turn, this gives rise to a method for approximating their multilinear relaxation in a closed form, without sampling. We leverage this method in the context of stochastic submodular optimization, showing that it can also be applied in combination with SCG of Mokhtari et al.~\cite{mokhtari2020stochastic}: this eliminates one of the two sources of randomness, thereby reducing variance at the expense of added bias. From a technical standpoint, this requires controlling the error introduced by the bias of the polynomial estimator, while simultaneously accounting for the variance inherent in SCG, due to sampling instances.   %: this eliminates the latter source of randomness by utilizing the properties of deep submodular models that result from composition over multiple layers. In order to do so, we combine the stochastic continuous greedy algorithm proposed by Mokthari et al. \cite{mokhtari2020stochastic} with the deterministic estimator proposed by