% \documentclass[lettersize,journal]{IEEEtran}
\documentclass[10pt,journal,compsoc]{IEEEtran}


\usepackage{amsmath,amsfonts}
% \usepackage{algorithmic}
% \usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\usepackage{times}
\usepackage{epsfig}
\usepackage[dvipsnames]{xcolor}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{booktabs}
\usepackage{hyperref}
% 我加的
\usepackage{booktabs}
\usepackage{color}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{tabularx}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=None,
    language=Python,
    %linewidth=0.45\textwidth, 
    %aboveskip=-2mm,
    belowskip=0mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\scriptsize\sffamily},
    %numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    %breaklines=true,
    %breakatwhitespace=true,
    tabsize=1,
    emph={SpikeStream, Interpolation, Cropping, Periodic, Padding},
    emphstyle={\color{blue}}
}



\begin{document}

\title{Exploring Asymmetric Tunable Blind-Spots for Self-supervised Denoising in Real-World Scenarios}

\author{\IEEEauthorblockN{Shiyan Chen, Jiyuan Zhang, Zhaofei Yu, and Tiejun Huang}
\thanks{S. Chen and J. Zhang are with the School of Computer Science, Peking University, Beijing, China. E-mail:\{strerichia002p, jyzhang\}@stu.pku.edu.cn.
}
\thanks{Z. Yu and T. Huang are with the Institute for Artificial Intelligence, Peking University, Beijing, China, and also with school of Computer Science, Peking University, Beijing, China. E-mail:\{yuzf12, tjhuang\}@pku.edu.cn}
}


% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% % The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% % Remember, if you use this you must call \IEEEpubidadjcol in the second
% % column for its text to clear the IEEEpubid mark.


\IEEEtitleabstractindextext{
\begin{abstract}
     Self-supervised denoising has attracted widespread attention due to its ability to train without clean images. However, noise in real-world scenarios is often spatially correlated, which causes many self-supervised algorithms based on the pixel-wise independent noise assumption to perform poorly on real-world images. Recently, asymmetric pixel-shuffle downsampling (AP) has been proposed to disrupt the spatial correlation of noise. However, downsampling introduces aliasing effects, and the post-processing to eliminate these effects can destroy the spatial structure and high-frequency details of the image, in addition to being time-consuming. In this paper, we systematically analyze downsampling-based methods and propose an Asymmetric Tunable Blind-Spot Network (AT-BSN) to address these issues. We design a blind-spot network with a freely tunable blind-spot size, using a large blind-spot during training to suppress local spatially correlated noise while minimizing  damage to the global structure, and a small blind-spot during inference to minimize information loss. Moreover, we propose blind-spot self-ensemble and distillation of non-blind-spot network to further improve performance and reduce computational complexity. Experimental results demonstrate that our method achieves state-of-the-art results while comprehensively outperforming other self-supervised methods in terms of image texture maintaining, parameter count, computation cost, and inference time.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Self-supervised denoising, Blind-spot network, Real-world denoising.
\end{IEEEkeywords}}

\maketitle

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle



%%%%%%%%% BODY TEXT

\section{Introduction}
Image denoising is an essential low-level computer vision problem designed to recover clean signals from given noisy observations. With the advancements in deep learning and convolutional neural networks (CNNs), an increasing number of studies are focused on supervised learning using clean-noisy pairs~\cite{RIDNet,CBDNet,FADNet,DANet,DnCNN,FFDNet}. Typically, additive white Gaussian noise (AWGN) is introduced into clean datasets to synthesize clean-noisy denoising datasets. However, real-world noise is known to be spatially correlated~\cite{chatterjee2011noise,jin2020review,park2009case}, and the simple assumption of synthetic datasets regarding noise can lead to poor generalization in real-world scenarios. 
Some generative-based methods attempt to synthesize real-world noise from existing clean data~\cite{GAN2GAN,GCBD,UIDNet,C2N,DBSN} and then employ supervised learning. However, synthesizing real-world noise remains challenging, and unpaired methods still suffer from the issue of mismatch between the distribution of existing clean data and the desired scenario.
To address the issue, some researchers attempt to capture clean-noisy pairs in real-world scenarios~\cite{SIDD,NIND}. A typical work is the SIDD dataset~\cite{SIDD}, which consists of clean-noisy pairs captured under various scenes and illuminations. These datasets enable supervised training on real-world data directly~\cite{RIDNet,CBDNet,InvDN,DANet,DnCNN}. However, constructing such datasets can be extremely laborious and time-consuming work. In certain scenarios, such as medical imaging and electron microscopy, constructing such datasets can be impractical or even infeasible. These limitations curtail the scope of supervised denoising algorithms.

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{figure/head.pdf}
    \caption{Comparison of our AT-BSN and other methods on the DND benchmark dataset~\cite{DND}. (a) CVF-SID~\cite{CVFSID} and (b) AP-BSN~\cite{APBSN} are two self-supervised algorithms designed for real-world noise removing, both are trained with only noisy images. Our method preserves the most details while removing noise.}
    \label{fig:head}
\end{figure}

Self-supervised denoising algorithms, represented by Noise2Noise~\cite{Noise2Noise}, have brought new life to the denoising field.  These methods only require noisy observations to train the denoising model. However, in real-world scenarios, noise often exhibits spatial correlation, which contradicts the pixel-wise independent noise assumption~\cite{Noise2Void,Noise2Noise} that most self-supervised algorithms~\cite{Noise2Self,Neighbor2Neighbor,Noise2Void,Noise2Noise,Blind2Unblind} rely on. As a result, despite their ability to train directly on noisy images, self-supervised denoising algorithms perform poorly in real-world scenarios.

Recent studies have proposed self-supervised denoising algorithms suitable for real-world scenarios~\cite{APBSN,CVFSID}. 
% CVF-SID~\cite{CVFSID} has introduced a cyclic multi-variate function to decompose real-world noisy images into clean images, signal-dependent noise, and signal-independent noise. Although this method has achieved considerable results, it relies on a simple network without residual connections to avoid learning an identity mapping to the noise signal. Additionally, the simple assumption about real-world noise signals has resulted in its vague denoising results. 
The current state-of-the-art approach AP-BSN~\cite{APBSN} utilized pixel-shuffle downsampling (PD)~\cite{APBSN,whenAWGN} to disrupt the spatial correlation of real-world noisy images and employed asymmetric PD stride factors for training and inference. Furthermore, they proposed random-replacing refinement (R$^3$)\cite{APBSN} post-processing to further improve the performance. However, AP-BSN suffers from various limitations, such as global structure destruction, high-frequency detail loss caused by aliasing effect, and time-consuming inference process. 


In this paper, we propose a novel paradigm to break the spatial correlation of noise for real-world self-supervised learning while avoiding the issues of the methods mentioned above. To be specific, we propose a novel BSN with tunable blind-spot size, where a larger blind-spot is used during training to mask only local neighboring pixels to break the local spatial correlation of noise. Compared to AP-BSN, our blind-spot strategy causes minimal damage to the image structure and the global information, allowing the network to learn from all pixels outside the blind-spot for signal restoration, resulting in a larger effective receptive field. During inference, we use a smaller blind-spot to further reduce the loss of local neighboring signals. Since no downsampling process is used, no post-processing is required to mitigate aliasing artifacts.

To implement a BSN with tunable blind-spot sizes, we adapted the original approach proposed by \cite{Laine2019}. We further introduce blind-spot self-ensemble to integrate the advantages of different blind-spot sizes. However, the four-fold input of the network results in computational redundancy. To address this issue, we propose to distill the knowledge of 
blind-spots in different sizes into a lightweight non-blind-spot network (NBSN). Ultimately, we employed the lightweight NBSN for efficient inference.

The main contributions are summarized as follows:
\begin{itemize}
% \setlength{\itemsep}{0pt}
% \setlength{\parsep}{0pt}
% \setlength{\parskip}{0pt}
    \item We present a novel paradigm to break the spatial correlation of real-world noise while causing less damage to the global structure. We propose a tunable blind-spot strategy and employ asymmetric blind-spot sizes for training and inference, respectively. 
    \item To further improve the performance and tackle the computational redundancy in BSN, we propose blind-spot self-ensemble and distill the knowledge of blind-spots in different sizes to a lightweight and computationally efficient Non-BSN for inference.
    \item Experimental results on real-world datasets demonstrate that our method outperforms the other self-supervised methods with fewer parameters and less inference time. In addition, qualitative results show that our method is capable of recovering more texture information, producing desirable results.
\end{itemize}



\section{Related Work}
\noindent
\textbf{Supervised Image Denoising.}
Deep learning has made remarkable advances in image denoising in recent years. Zhang et al.~\cite{DnCNN} introduced DnCNN, the first CNN-based method for supervised denoising, which significantly outperformed traditional methods~\cite{Chambolle2004AnAF,BM3D,Elad2006ImageDV,WNNM,Vese2003ModelingTW}.The following work aimed to enhance the performance of supervised denoising, such as FFDNet~\cite{FFDNet}, CBDNet~\cite{CBDNet}, RIDNet~\cite{RIDNet}, DANet~\cite{DANet}, FADNet~\cite{FADNet}, and so on. However, supervised-based methods require large amounts of aligned clean-noisy pairs as training data, which are usually difficult and costly to obtain in formal scenarios.

\noindent
\textbf{Unpaired Image Denoising.}
To tackle the challenge in supervised learning, some generative-based~\cite{gans} approaches synthesize noisy samples from clean images~\cite{GAN2GAN,GCBD,UIDNet,C2N,DBSN}. The simulated clean-noisy pairs can be further used to train a supervised denoising model. However, the performance of unpaired image denoising methods can be limited when the existing clean images do not match the distribution of the current scene.

\noindent
\textbf{Self-Supervised Image Denoising.}
Lehtinen et al.~\cite{Noise2Noise} proposed Noise2Noise, which demonstrated that a denoising network could be trained with two independent noisy observations of the same scene. However, even if Noise2Noise relaxes the clean image requirement, obtaining two aligned noisy images in real-world scenarios remains difficult. Noise2Void~\cite{Noise2Void} and Noise2Self~\cite{Noise2Self} proposed a blind-spot strategy to learn denoising from only single noisy images. Further works~\cite{Laine2019,DBSN} extended the paradigm to blind-spot network (BSN) through shifted convolutions~\cite{Laine2019} and dilated
convolutions~\cite{DBSN}. Blind-spot means the network is designed to denoise each pixel from its surrounding spatial neighborhood without itself, thus, the identity mapping to the noisy image itself can be avoided. Noisier2Noise~\cite{Noisier2Noise}, Noisy-As-Clean (NAC)~\cite{NAC}, Recorrupted-to-Recorrupted (R2R)~\cite{R2R}, and IDR~\cite{IDR} generated noisy training pairs by adding synthetic noise to given noisy inputs.
Recently, Neighbor2Neighbor~\cite{Neighbor2Neighbor} proposed to subsample the noisy input images to obtain noisy pairs for Noise2Noise-like training. Blind2Unblind~\cite{Blind2Unblind} proposes a global-aware mask mapper and re-visible loss to fully excavate the information in the blind-spot for Noise2Void-like training. 




\noindent
\textbf{Real-World Image Denoising.}
Some works~\cite{SIDD,NIND} attempt to capture clean-noisy pairs in real-world scenarios. Abdelhamed et al.~\cite{SIDD} carefully took and aligned clean-noisy pairs from different scenes and lighting conditions using five representative smartphone cameras, and proposed the SIDD dataset. These datasets enable supervised methods~\cite{NBNet,P3AN,AINDNet,InvDN,VDN,DANet} to train on real-world clean-noisy pairs. However, constructing real datasets requires tremendous human effort and time. Moreover, real-world noise tends to exhibit spatial correlation, which contradicts the premise of Noise2Noise~\cite{Noise2Noise} that noise follows an independent and identically distributed pattern, rendering it and its subsequent variants unsuitable for direct application to real-world scenarios. In order to apply self-supervised learning to real-world settings, Neshatavar et al.~\cite{CVFSID} introduced a cyclic multi-variate function to disentangle clean images, signal-dependent noise, and signal-independent noise from noisy images.
However, the method relies on a simple network without residual connections to avoid learning an identity mapping to the noise signal. Additionally, the simple assumption about real-world noise signals has resulted in its vague denoising results. 
Lee et al.~\cite{APBSN} employed pixel-shuffle downsampling (PD)~\cite{whenAWGN} to disrupt the spatial correlation of noise and introduced different PD stride factors
for training and inference for better performance.



\section{Motivation}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/correlation.pdf}
    \caption{ (a) Spatial correlation on real-world noise. The depth of color represents the degree of correlation. We find the correlation becomes negligible outside of the green $7\times 7$ patch. (b) The ERF of our AT-BSN during training. The size of the blind-spot is $7\times 7$. }
    \label{fig:corr}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/erf.pdf}
    \caption{Effective Receptive Field analysis of AP-BSN and our AT-BSN. We adopt the UNet-like BSN proposed by Laine et al.~\cite{Laine2019} for both methods for a fair comparison. ERFs are superimposed on top of the images and displayed in blue color.}
    \label{fig:erf}
\end{figure}

Due to the effect of image signal processors (ISP), e.g. image demosaicking~\cite{chatterjee2011noise,jin2020review,park2009case}, real-world noise is generally known to be spatially correlated and pixel-wise dependent. Lee et al.~\cite{APBSN} analyzed the spatial correlation of real-world noise and found that different camera devices in the SIDD dataset show similar noise behaviors in terms of spatial correlation. According to Fig.~\ref{fig:corr}(a), the correlation of noise presents a Gaussian distribution that decays as distance increases. This correlation of noise violates the pixel-wise independent noise assumption of the BSN, rendering it inadequate for real noise removal.

The state-of-the-art algorithm AP-BSN~\cite{APBSN} employs asymmetric PD to suppress the spatial correlation. Nevertheless, it suffers from various limitations, such as global structure destruction, high-frequency detail loss caused by the aliasing effect, and a time-consuming inference process. 

% In this Section, we first revisit a classical BSN structure and briefly introduce its implementation principles. Subsequently, we examine the framework of AP-BSN, analyze and discuss its shortcomings. In the next Section, we will modify the BSN to address the limitations of AP-BSN.
In this Section, we revisit the framework of AP-BSN, analyze and discuss its limitations. In the next Section, we will modify a classical BSN to address the issues of AP-BSN. 

% \subsection{Blind-Spot Network Revisit}

\subsection{AP-BSN Revisit}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/architecture.pdf}
    \caption{Overview of the proposed AT-BSN framework. (a) We employ asymmetric blind-spots for training and inference to balance the suppression of noise spatial correlation and local information preservation. The rotated input is fed into the network, and then the output features are shifted by $s$ pixels, restricting the receptive field to grow in only one half-plane direction beyond $s$ rows above the current location. Afterward, the features are rotated back and linearly combined through $1\times1$ convolutions, resulting in a binld-spot with the size of $k=2s-1$. (b) Illustrations of asymmetric blind-spots for training and inference. }
    \label{fig:arch}
\end{figure*}


AP-BSN employs asymmetric PD stride factors for training and inference in conjunction with BSN. During training, AP-BSN uses PD with stride factor 5 to suppress the spatial correlation between noise signals in training samples. To be specific, a dilated
convolutions based BSN $B_{\theta}(\cdot)$ is applied to the downsampled images $PD_5{(I_{noisy})}$, where the PD-inverse operation $PD_5^{-1}(\cdot)$  follows to reconstruct a full-sized output $I_{pred}$. AP-BSN aims to minimize the following loss function:
\begin{align}
    \label{apbsn}
    \mathcal{L}_{apbsn} = {\Vert PD_5^{-1}(B_{\theta}(PD_5{(I_{noisy})})) - I_{noisy} \Vert}_1
\end{align}
During inference, AP-BSN adopts the PD stride factor of 2 to achieve a balance between aliasing artifacts and noise correlation breaking.

However, AP-BSN suffers from the following issues: 
1) The PD process unavoidably destruct detailed textures and global structures, posing challenges for the model to learn the necessary clues for recovering the clean signal.
% However, noise correlation is generally confined to local regions. Therefore, it is unnecessary to disrupt regions beyond the local neighbors of a pixel. During training, using PD to disrupt the spatial correlation of noise also disrupts the spatial correlation of the clean signal, posing challenges for the model to learn the necessary clues for recovering the clean signal.
2) The aliasing artifacts still persist even with the smallest PD stride factor during inference, which leads to the loss of high-frequency information. 3) Moreover, AP-BSN proposed R$^3$ post-process to reduce the aliasing artifacts by averaging 8 noisy images synthesized from the initially denoised results. Nevertheless, R$^3$ operations not only result in a significant increase in inference time but also introduce blurring.


\subsection{Effective Receptive Field Analysis of AP-BSN}
During training, using PD to disrupt the spatial correlation of noise also disrupts the spatial correlation of the clean signal, which destructs detailed textures and global structures of images. Moreover, noise correlation is generally confined to local regions, according to Lee's statistics~\cite{APBSN} and Fig.~\ref{fig:corr}. Therefore, it is unnecessary to disrupt regions beyond the local neighbors of a pixel. We utilize Effective Receptive Field (ERF)~\cite{ERF} to explicate this point. We consider the ERF of the central pixel.
Fig.~\ref{fig:erf} shows the noisy image and PD result with stride factors of 5, which is used for AP-BSN training. Fig.~\ref{fig:erf}(a) illustrates that the central pixel has been moved to the top left, as well as its ERF. Fig.~\ref{fig:erf}(b) presents the ERF mapped back to the original position. 

One can find that the ERF of AP-BSN in the original image manifests as a sparse grid-like pattern, akin to the effect of simple stacking of dilated convolutions~\cite{yu2015multi}. This characteristic comes with similar drawbacks to the dilated convolutions~\cite{chen2017rethinking,wang2018understanding,yu2015multi}, namely 1) the loss of local information, posing challenges for the model to learn clues from the grid-like discontinuous sub-image for recovering the clean signal, and 2) long-ranged information might be not relevant. Moreover, the loss of information continuity can also introduce aliasing artifacts.

From Fig.~\ref{fig:erf}(c), it is apparent that our AT-BSN only loses a portion of the information within the blind-spot area, while the ERF outside the blind-spot remains unaffected. Also see Fig~\ref{fig:corr}, the $7\times7$ green box covers the main areas in the noise correlation map. This inspires us to set the size of blind-spot to 7 during training.
Due to the higher correlation of the signal compared to the noise, the central pixel can be recovered using the pixels outside the blind-spot that are less correlated with it in the noise domain. It is worth noting that the size of the blind-spot can be minimized during inference to reduce information loss.



\section{Method}
\subsection{Tunable Blind-Spot}
BSN~\cite{Noise2Void,APBSN,DBSN} is designed to denoise each pixel from its surrounding spatial neighborhood without itself. Thus, the identity mapping to the noisy image itself can be avoided. Typically, BSN can be constructed through shifted convolutions~\cite{Laine2019} or dilated convolution~\cite{DBSN}. 
% We will introduce the version implemented by Laine et al. ~\cite{Laine2019}.
We will modify the version of Laine's BSN ~\cite{Laine2019} .




\noindent
\textbf{Restricted Receptive Fields.}
Our AT-BSN is inspired by Laine's approach~\cite{Laine2019}, which combines four branches with restricted receptive fields, each of which is limited to a half-plane that excludes the central pixel.
Specifically, we modify the convolution and pooling layers of the network, restricting the receptive field to grow in only one half-plane direction, such as upwards. 

In convolution layers, we choose zero-padding and shift the feature maps downwards before the convolution operation. For a convolution kernel of size $h \times h$, a downwards offset of $d=\lfloor h/2 \rfloor$ pixels is needed. We append $d$ rows of zeros at the top of the feature map, apply the convolution, and finally crop the last $d$ rows of the feature map to achieve the desired shift operation.

In the pooling layer, we perform a $2\times2$ average pooling operation. Similarly, we add one row of zeros at the top of the feature map and crop the last row of the feature map before pooling to implement the shift operation. As the receptive field of the downsampling layer has already been restricted, we do not need to modify the upsampling layer.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/shift.pdf}
    \caption{Implementation principle of the tunable blind-spot. The leftmost column of each figure is the input, and the network becomes deeper from left to right. The feature map obtained after each convolution is shown in the middle of each figure. (a) Before the last shift operation $M(\cdot; s)$, the receptive field includes the central pixel itself. (b) After the last shift $M(\cdot; s)$, a blind-spot area is obtained by shifting $s$ pixels. The rightmost layer is the feature map obtained after $1 \times 1$ convolutions.}
    \label{fig:shift}
\end{figure}

\noindent
\textbf{Tunable Blind-Spot.}
After applying a series of modified convolution and pooling layers together with upsampling layers to the input noisy image $I_{noisy}$, we obtain the resulting feature map denoted as $f_{up}$, whose receptive field is fully contained within an upward half-plane, including the center row. Note that the receptive field at this point includes the pixel itself. See Fig.~\ref{fig:shift} for more details.

To exclude the center pixel from the receptive field, we shift the feature map $f_{up}$ downward by $s$ pixels, resulting in a shifted feature map $f_{up}^s$. 
\begin{align}
    \label{shift}
    f_{up}^s = M(f_{up}; s),
\end{align}
where $M(\cdot; s)$ denotes the shift operation of a factor $s$.
At this point, the receptive field of the central pixel only includes \textbf{the positions beyond $s$ rows above the current location}. And then, to expand the receptive field of a pixel to all directions around it, we rotate the input image by multiples of $90^{\circ}$ and feed them into the network. This results in feature maps $f_{up}^s$, $f_{down}^s$, $f_{left}^s$, and $f_{right}^s$, each with its receptive field limited to its respective half-plane.
Finally, the four feature maps are rotated to the correct orientation and linearly combined through several $1\times1$ convolutions $h(\cdot)$ to produce the final output $I_{pred}$.
\begin{align}
    \label{conv1x1}
    I_{pred} = h([\hat{f}_{up}^s, \hat{f}_{down}^s, \hat{f}_{left}^s, \hat{f}_{right}^s]),
\end{align}
where $[,]$ denotes feature concatenation, $\hat{f}^s$ denotes the corresponding $f^s$ in the correct orientation.

Now for a given pixel, its receptive field is composed of the receptive fields from four different orientations, while a blind-spot area with a length of $k=2s-1$ is formed in the center around it. We can freely tune the size $k$ of the blind-spot by adjusting the shift factor $s$ of the feature map $f^s$ before the $1\times1$ convolutions. So far, we have achieved a BSN with a tunable blind-spot size. We denote the whole network parameterized by $\theta$ as $F_{\theta}(\cdot; s)$, the entire process can be formulated as:
\begin{align}
    \label{wholebsn}
    I_{pred} = F_{\theta}(I_{noisy}; s).
\end{align}

\subsection{Asymmetric Blind-Spots}



In real-world scenarios, the pixel-wise independent noise assumption of BSN is not satisfied. For smaller blind-spots, the central pixel can be inferred using the neighboring noisy pixels as clues. Based on the fact that the noise correlation is less than the signal correlation, we can use larger blind-spots to suppress the noise correlation while minimizing the impact on signal correlation. The central pixel within a large blind-spot can be inferred from pixels outside the blind-spot that are less correlated with it in the noise domain. During training, we aim to minimize the spatial correlation of noise by setting appropriate blind-spot size $k$ to achieve self-supervised learning of BSN. We minimize the following loss to train the network:
\begin{align}
    \label{atbsn}
    \mathcal{L}_{atbsn} &= {\Vert F_{\theta}(I_{noisy}; s) - I_{noisy} \Vert}_1 \nonumber \\
    &= {\Vert I_{pred} - I_{noisy} \Vert}_1
\end{align}
Following AP-BSN, we use $L^1$ norm for better generalization \cite{APBSN}. In practice, we choose $k=7$, that is, $s=4$, during training. 


During inference, the selection of the blind-spot size requires a trade-off between information loss and the level of disruption to the noise spatial correlation. We propose to achieve a balance between training and inference by employing asymmetric blind-spots. Since larger blind-spots have already been utilized during training to enable the BSN to learn to denoise, we can select smaller blind-spots during inference to minimize information loss. Our experiments demonstrate that $k=1$ or $k=3$ during testing leads to better results. Additionally, since our method does not involve downsampling, it avoids aliasing effects and better preserves image texture. The overall scheme can be found in Fig.~\ref{fig:arch}. In Sec.~\ref{sec_abla}, we will demonstrate the robustness of our approach to different blind-spot settings.


\begin{table*}[t]
\setlength\tabcolsep{13.0pt}
\caption{Comparison among different denoising methods on real-world datasets. Note that at the time we finished the experiment, \textbf{the SIDD benchmark website is not accessible}, so we collect the SIDD validation results from various papers. We will include the SIDD benchmark results in the future version. The $\diamond$ marks indicate the results are evaluated by ourselves (For Laine-BSN, we train the network from scratch. For AP-BSN, we evaluate the pre-trained weight provided by AP-BSN official website). The $\dagger$ marks indicate the method is trained directly on the DND benchmark dataset in a fully self-supervised manner.}
\begin{center}
\small
    % \begin{tabularx}{\textwidth}{lcccccc}
    \begin{tabular}{lcccccc}
    \toprule
    &\multirow{2}{*}{Methods}&  
    \multicolumn{2}{c}{SIDD Validation}&\multicolumn{2}{c}{DND Benchmark} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} & & PSNR$^\uparrow$ (dB)  & SSIM$^\uparrow$ & PSNR$^\uparrow$ (dB) & SSIM$^\uparrow$ \\
    \midrule
    \multirow{4}{*}{Non-Learning}& BM3D\cite{BM3D}   & 25.65 & 0.475 & 34.51 & 0.851      \\
    & WNNM\cite{WNNM}   & 26.20 & 0.693 & 34.67 & 0.865      \\
    & MCWNNM\cite{MCWNNM}   & 33.40 & 0.815 & - & -      \\
    & NC\cite{NC}   & 31.31 & 0.725 & 35.43 & 0.884      \\
    \midrule
    \multirow{3}{*}{Supervised}& DnCNN-B\cite{DnCNN}  & 38.41 & 0.909 & 37.90 & 0.943      \\
    & MLP\cite{MLP}  & - & - & 34.23 & 0.833      \\
    & FFDNet\cite{FFDNet}  & - & - & 34.40 & 0.847      \\
    & CBDNet\cite{CBDNet}   & 38.68 & 0.901 & 38.05 & 0.942      \\
    & RIDNet\cite{RIDNet}   & 38.71 & 0.913 & 39.25 & 0.952      \\
    & VDN\cite{VDN}   & 39.28 & 0.909 & 39.38 & 0.952      \\
    & AINDNet(R)\cite{AINDNet}   & 38.81 & - & 39.34 & 0.952      \\
    & DANet\cite{DANet}  & 39.47 & 0.918 & 39.58 & 0.955      \\
    & InvDN\cite{InvDN}  & 38.88 & - & 39.57 & 0.952      \\
    \midrule
    \multirow{2}{*}{Unpaired}
    & GCBD\cite{GCBD}   & - & - & 35.58 & 0.922      \\
    & D-BSN\cite{DBSN} $+$ MWCNN\cite{MWCNN}   & - & - & 37.93 & 0.937      \\
    & C2N\cite{C2N} $+$ DnCNN\cite{DnCNN}   & 34.08 & - & 36.08 & 0.903      \\
    \midrule
    \multirow{3}{*}{Self-Supervised}& Noise2Void\cite{Noise2Void}   & 29.35 & 0.651 & - & -      \\
    & Laine-BSN\cite{Laine2019}   & 23.80$^\diamond$ & 0.493$^\diamond$ & - & -      \\
    & Noise2Self\cite{Noise2Self}   & 30.72 & 0.787 & - & -      \\
    & NAC\cite{NAC}   & - & - & 36.20 & 0.925      \\ 
    & R2R\cite{R2R}   & 35.04 & 0.844 & - & -      \\
    & CVF-SID (S$^2$)\cite{CVFSID}   & 34.81 & 0.944 & 36.31 / 36.50$^\dagger$ & 0.923 / 0.924$^\dagger$      \\
    & AP-BSN\cite{APBSN}   & 35.49$^\diamond$ & 0.901$^\diamond$ & 37.46$^\dagger$ & 0.924$^\dagger$      \\
    & AP-BSN $+$ R$^3$\cite{APBSN}   & 36.48$^\diamond$ & 0.924$^\diamond$ & 38.09$^\dagger$ & 0.937$^\dagger$      \\
    % & AP-BSN   & 34.86 & - & 37.46 & 0.924      \\
    % & AP-BSN(R$^3$)   & 35.76 & - & 38.09 & 0.937      \\
    & \textbf{AT-BSN (Ours)}   & \textbf{36.64} & \textbf{0.940} & 37.81 / 37.96$^\dagger$ & \textbf{0.938} / 0.935$^\dagger$     \\
    & \textbf{AT-BSN (S) (Ours)}   & \textbf{37.13} & \textbf{0.945} & \textbf{38.27 / 38.54}$^\dagger$ & \textbf{0.940 / 0.940}$^\dagger$      \\
    \bottomrule
    \end{tabular}
    % \end{tabularx}
\end{center}

\label{tab:dabiao}
\end{table*}



\subsection{Blind-Spot Self Ensemble and Distillation}
While larger blind-spots can more effectively suppress spatial correlations between neighboring
noise signals, they also result in more loss of information. Conversely, smaller blind-spots exhibit an opposite trend.
To better integrate the advantages of different blind-spot sizes, we propose blind-spot self-ensemble. We perform inference with various blind-spot sizes to obtain denoised images, and average them as the final denoised result. Nevertheless, the self-ensemble during inference inevitably increases computation time. In addition, the rotation operation in our AT-BSN results in computation redundancy. 
To address these issues, we propose to distill the knowledge of blind-spots in different sizes to a non-blind-spot network (NBSN) using the denoised results obtained from self-ensemble and the original noisy images. 
% we follow Wu et al.\cite{wu2020unpaired} and train a non-blind-spot network (NBSN) using the denoised results obtained from self-ensemble and the original noisy images. 
Specifically, we remove the shift, rotation operations and the last $1 \times 1$ convolution layers in the BSN and modify the output channels of the last convolution layer to obtain the NBSN.

% \subsection{Self-Supervised Training}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figure/results.pdf}
    \caption{Quantitative comparisons on SIDD validation dataset.}
    \label{fig:sidd}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.72\linewidth]{figure/dnd.pdf}
    \caption{Quantitative comparisons on DND benchmark dataset.}
    \label{fig:dnd}
\end{figure*}


\section{Experiment}

\subsection{Experimental Configurations}
\paragraph{Implementation Details.}
We adapt Laine's UNet-like BSN~\cite{Laine2019} as our network. We set $k=7$ and $k=1$ for training and inference, respectively. The input images are cropped into $128 \times 128$ patches with a batch size of 8. The network is optimized with an Adam optimizers with a learning rate of $2\times10^{-4}$ and $[\beta_1, \beta_2]$ of $[0.9, 0.999]$. We train the network for 60k iterations, and the learning rate is half-decayed per 10k iterations. For blind-spot self ensemble, we choose to average $k\in{ \{0,1,3,5\} }$. All experiments are finished on an Nvidia RTX 3080.

\noindent
\textbf{Real-World Datasets.}
We train and evaluate our method on two well-known real-world image denoising datasets, Smartphone Image Denoising Dataset \textbf{(SIDD)}~\cite{SIDD} and Darmstadt Noise Dataset\textbf{ (DND)}~\cite{DND}. 
SIDD-Medium training dataset consists of 320 clean-noisy pairs captured under various scenes and illuminations. Note that the SIDD online benchmark website is not accessible, so we adopt the SIDD validation dataset, containing 1280 noisy patches with a size of $256\times256$ for validation and performance evaluation. 

DND benchmark consists of 50 noisy images captured with consumer-grade cameras of various sensor sizes and does not provide clean images. DND dataset is captured under normal lighting conditions compared to the SIDD dataset, and therefore presenting less noise. We adopt widely-used
peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) metrics to evaluate our method.

\subsection{Comparisons for Real-World Denoising}

\noindent
\textbf{Quantitative Measure.}
Tab.~\ref{tab:dabiao} presents quantitative comparisons with other methods on the SIDD validation and DND benchmark datasets. We denote the distilled network as AT-BSN (S), where S means small and self-ensemble. Note that AT-BSN (S) is actually a Non-BSN despite its name. As a self-supervised algorithm, our method outperforms all existing unpaired and self-supervised methods, achieving state-of-the-art performance on both datasets. Our results without $\dagger$ marks indicate we employ the model trained on SIDD-Medium directly on the DND benchmark. These results show the generalization ability of our method. Note that results with $\dagger$ show the advantage of our fully self-supervised method.
% Our baseline outperforms AP-BSN(R$^3$) by approximately 0.9dB on the SIDD validation dataset. It is worth noting that AP-BSN with R$^3$ requires additional post-processing to mitigate aliasing effects, which significantly increases inference time, while our baseline generates denoised results directly without any post-processing. 
Furthermore, the results of AT-BSN (S) demonstrate the potential to enhance performance by integrating the advantages of different blind-spot sizes.


% Notably, our approach requires no clean data during training and can be trained directly on noisy datasets. 


\noindent
\textbf{Qualitative Measure.}
Fig.~\ref{fig:sidd} and Fig.~\ref{fig:dnd} present the qualitative comparisons. It is evident that our method is capable of preserving the most texture details. As no downsampling process is used, no aliasing artifacts occur. In addition, using a smaller blind-spot size during inference minimizes information loss and maintains most image textures and spatial structure. On the other hand, AP-BSN without R$^3$ shows obvious aliasing artifacts, while the images tend to be excessively smoothing with R$^3$. Interestingly, if we set the blind-spot size $k$ to 1 during training, our method will degenerate into Laine's BSN\cite{Laine2019}. Such a network is found to learn an approximately identity mapping that is close to the noisy input itself, which indicates that highly spatial-correlated noise in real-world scenarios is challenging to classical self-supervised methods.

\subsection{Complexity Analysis}
We also present a comparison of our method with several recent methods in terms of inference time, the number of parameters, and computation cost. As shown in Tab.~\ref{tab:Complexity}, our method achieves the best performance while having the smallest amount of parameters. Furthermore, our AT-BSN (S) owns even fewer parameters, and since no rotation operation is required, the inference time is significantly reduced. Although CVF-SID owns little parameters, the final denoised results are restored by two successive CVF-SID models, leading to longer time consumption. We also found that the inference time of AP-BSN + R$^3$ is significantly increased compared to AP-BSN, as it requires R$^3$ post-process to eliminate the aliasing effect caused by downsampling. In contrast, our method does not suffer from the aliasing effect and does not require any post-processing.

\begin{table}[t] 
\small
\caption{Complexity Analysis. The multiplier-accumulator operations (MACs) and inference time are measured on $512 \times 512$ patches from DND benchmark datasets.}
\begin{center}
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Methods}& Params$^\downarrow$   & MACs$^\downarrow$  & Time$^\downarrow$  & PSNR$^\uparrow$  \\
    & (M)  & (G) & (ms) & (dB) \\
    % Methods  & Params$^\downarrow$ (M)  & MACs$^\downarrow$ (G) & Time$^\downarrow$ (ms) & PSNR$^\uparrow$ (dB) \\
    \midrule
    CVF-SID (S$^2$)\cite{CVFSID}    & 1.19   & 311.44  & 106.2  & 34.81      \\
    AP-BSN\cite{APBSN}    & 3.66   & 838.92  & 78.6  & 35.49      \\
    AP-BSN $+$ R$^3$\cite{APBSN}    & 3.66   & 7653.97  & 1168.4  & 36.48      \\
    \textbf{AT-BSN (Ours)}    & 1.12   & 560.06  & 28.2  & 36.64     \\
    \textbf{AT-BSN (S) (Ours)}    & \textbf{0.86}   & \textbf{106.53}  & \textbf{2.4}  & \textbf{37.13}     \\
    \bottomrule
    \end{tabular}
\end{center}
\label{tab:Complexity}
\end{table}


% robustness of size
% \paragraph{Effectiveness of Mutual Learning.}
\subsection{Analysis of Asymmetric Blind-Spots}
\label{sec_abla}
We conduct an ablation study on the combination of different blind-spot sizes during training and inference to evaluate the effectiveness of the asymmetric blind-spots strategy and the robustness of our proposed method. Specifically, we performed experiments on the combinations of training blind-spot sizes $k_a\in\{5, 7, 9, 11, 13, 15\}$ and inference blind-spot sizes $k_b\in \{0,1,3,5,7,9,11,13,15\} $, using AT-BSN trained on the SIDD validation dataset. The results are shown in Fig.~\ref{fig:robust}.

According to Fig.~\ref{fig:corr}, the noise spatial correlation of the SIDD dataset becomes negligible outside the $7\times 7$ patch. As a consequence, our method could achieve the best performance at $k_a=7$. In addition, although the combination of $k_a=7$ and $k_b=1$ achieved the best performance for inference, we found that our method exhibited robustness to different experimental groups. That is, during training, overly large blind-spot sizes present little impact on the final performance, and using slightly larger blind-spot sizes during inference also imposes few impact on the results. However, models trained with $k_a\leq5$ perform poorly as the noise spatial correlation is not suppressed sufficiently.

We also note that the performance of AP-BSN sharply declined with the increase of the inference PD stride factor as reported in~\cite{APBSN}. We attempted to explain this from the perspective of the ERFs~\cite{ERF}. As shown in Fig.~\ref{fig:erf}, the ERF of AP-BSN mapped to the original image expands dramatically with the expansion of the PD stride factor, causing severe local information loss and resulting in significant aliasing artifacts. In contrast, our method only loses the central $k*k$ pixels and therefore exhibits relatively better robustness.

\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{figure/stridecurve.pdf}
    \caption{Ablation experiments on the combinations of different blind-spots between training and inference. Extreme values such as $k=15$ is also included. The results indicate that our method presents robustness in the selection of $k$.}
    \label{fig:robust}
\end{figure}

\section{Conclusion}
In this paper, we propose a novel paradigm to break the spatial correlation of noise in real-world scenes while minimizing the disruption of global structure and high-frequency information, thus preserving fine-grained details. Specifically, we introduce a blind-spot network with tunable blind-spot sizes, which strikes a balance between suppressing noise spatial correlation and preserving local information by employing asymmetric blind-spots during training and inference. Furthermore, to better integrate the advantages of different blind-spot sizes, we propose blind-spot self-ensemble to improve performance. Additionally, we employ the output of the self-ensemble to distill a non-blind-spot network, further enhancing performance and reducing computational redundancy. The experimental results on the real-world noise datasets demonstrate the comprehensive superiority of our method in terms of performance, computational complexity, and inference time, achieving a new state-of-the-art in the self-supervised denoising field.


% \clearpage
% \clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}


