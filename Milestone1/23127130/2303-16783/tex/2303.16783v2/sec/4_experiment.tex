\section{Experiments}
\label{sec:exp}
\subsection{Experimental Configurations}


\noindent
\textbf{Real-World Datasets.}
We conduct experiments on two real-world image denoising datasets, SIDD~\cite{SIDD} and DND~\cite{DND}.
SIDD-Medium training dataset consists of 320 clean-noisy pairs captured under various scenes and illuminations. The SIDD validation dataset contains 1280 noisy patches with a size of $256\times256$ for performance evaluation. 
DND benchmark consists of 50 noisy images captured with consumer-grade cameras of various sensor sizes and does not provide clean images. DND dataset is captured under normal lighting conditions compared to the SIDD dataset, and therefore presenting less noise. We adopt PSNR and SSIM metrics to evaluate our method. We set $k_a=9$ and $k_b=3$ for training and inference, respectively. For multi teacher distillation, we sample blind-spots from $Ks\in{ \{0,1,3,5,7,9,11\} }$. More implementation details can be found in supplementary materials.

\subsection{Comparisons for Real-World Denoising}

\noindent
\textbf{Quantitative Measure.}
Tab.~\ref{tab:dabiao} presents quantitative comparisons with other methods. We denote the distilled network as AT-BSN (D). Note that AT-BSN (D) is actually a Non-BSN despite its name. As a self-supervised algorithm, our method outperforms all existing unpaired and self-supervised methods, achieving state-of-the-art performance. Our results without $\dagger$ marks indicate we employ the model trained on SIDD-Medium directly on the benchmark. These results show the generalization ability of our method. Our results with $\dagger$ show the potential of fully self supervised learning.
Furthermore, the results of AT-BSN (D) demonstrate the potential to enhance performance by integrating the advantages of different blind-spot sizes through multi teacher distillation.




\noindent
\textbf{Qualitative Measure.}
Fig.~\ref{fig:sidd} presents the qualitative comparisons. Our method is capable of preserving the most texture details. In addition, results of downsampling-based methods~\cite{APBSN,pan2023random} tend to transition smoothly. LG-BPN~\cite{wang2023lg}, Spatially Adaptive~\cite{li2023spatially}, and our AT-BSN, as training on the original resolution structure, can preserve better results. Specifically, although AP-BSN uses R$^3$ post-processing, its visualization still exhibits aliasing effects. More SIDD and DND benchmark results are in the supplementary materials.






\begin{table}[t] 
\setlength\tabcolsep{4.6pt}%调列距
\small
\begin{center}
    \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Blind-Spots}& \{1,3,   & \{0,1,3,  & \{0,1,3, & \{0,1,3& \{0,1,3,5\\
    & 5,7\}   & 5,7\}  & 5,7,9\} & 5,7,9,11\}& 7,9,11,13\}      \\
    \midrule
    PSNR    & 37.31   & 37.32  & 37.41 & \textbf{37.47} &37.40       \\
    SSIM    & 0.943   & 0.943  & 0.944  & \textbf{0.945} &0.944      \\
    \bottomrule
    \end{tabular}
\end{center}
\vspace{-5mm}
\caption{Ablation study of the ensemble of teacher networks.}
\label{tab:ManyBlindSpots}
\end{table}

\begin{table}[t] 
\small
\vspace{-1.5mm}
\begin{center}
    \begin{tabular}{lccc}
    \toprule
    \multirow{2}{*}{}& Mean Teacher   & Multi Teacher  & Param  \\
    \midrule
    Student A    & 36.79 / 0.942   & 36.98 / 0.943  &  \textbf{0.12 M}       \\
    Student B    & 37.60 / 0.945   & 37.76 / 0.946  & 0.86 M        \\
    Student C    & \textbf{37.72 / 0.945}   & \textbf{37.88 / 0.946}  & 1.02 M       \\
    \bottomrule
    \end{tabular}
\end{center}
\vspace{-5mm}
\caption{Ablation study of different distillation methods on students with different parameters.}
% \vspace{-5mm}
\label{tab:mean_and_multi}
\end{table}


\begin{table}[t] 
\setlength\tabcolsep{2.6pt}%调列距
\small
\vspace{-1.5mm}
\begin{center}
    \begin{tabular}{lccc}
    \toprule
    \multirow{2}{*}{Methods}& Params$^\downarrow$   & MACs$^\downarrow$  & PSNR$^\uparrow$  \\
    & (M)  & (G)  & (dB) \\
    \midrule
    CVF-SID
    \cite{CVFSID}    & 1.19   & 311.44    & 34.81      \\
    AP-BSN $+$ R$^3$\cite{APBSN}    & 3.66   & 7653.97    & 36.48      \\
    SDAP (E)\cite{pan2023random}    & 3.66   & 1628.57    & 37.30     \\
    LGBPN\cite{wang2023lg}    & 4.56   & 12168.22    & 37.32     \\
    Spatially-Adaptive (UNet)\cite{li2023spatially}    & 1.08   & 70.11    & 37.39     \\
    \textbf{AT-BSN (Ours)}    & 1.27   & 330.51    & 36.80     \\
    \textbf{AT-BSN (D) (Ours)}    & \textbf{1.02}   & \textbf{48.92}    & \textbf{37.88}     \\
    \bottomrule
    \end{tabular}
\end{center}
\vspace{-5mm}
\caption{Complexity Analysis. The multiplier-accumulator operations (MACs) are measured on $512 \times 512$ patches.}
\label{tab:Complexity}
\vspace{-3mm}
\end{table}


% \vspace{-3mm}
\subsection{Ablation Study}
\label{sec_abla}

\noindent
\textbf{Analysis of Asymmetric Blind-Spots.}
We perform experiments on the combinations of training blind-spot sizes $k_a\in\{7, 9, 11\}$ and inference blind-spot sizes $k_b\in \{0,1,3,5,7,9,11,13\} $, testing on the SIDD validation dataset. 
From Fig.~\ref{fig:robust}, Our method could achieve the best performance at $k_a=9$ and $k_b=3$. This is due to the noise correlation is fully suppressed during training, the network has learned well denoising ability. During testing, only small blind-spot is needed to suppress the areas with the highest noise correlation, while maximizing the preservation of local information.
Performance degradation is observed in the $k_a=7$ setting. To address this, we introduce early stopping in this setting ($k_a=7*$), resulting in relatively better results. This suggests that a blind-spot with $k_a=7$ can partially suppress spatial noise but not entirely. Increasing epochs leads the network to learn from outside the blind-spot to reconstruct central noise.




\noindent
\textbf{Ensemble of Teacher Networks.}
To investigate how the student network benefits from learning various teachers, we conducted additional ablation experiments. A straightforward approach is to ensemble multiple teacher networks with different blind-spots by averaging their outputs for the final denoising result. Tab.~\ref{tab:ManyBlindSpots} displays the performance achieved by averaging results from teacher networks across different sets $Ks$. Performance tends to increase with larger $Ks$, indicating that teacher networks with distinct blind-spots perform differently across image areas, and weighting them effectively enhances performance. However, excessively large $Ks$ can lead to overly smooth average images, resulting in decreased performance.

\noindent
\textbf{Different Approaches of Distillation. }
We compare two methods of distillation: mean teacher distillation, utilizing the average outputs of various teachers for student training, and multi-teacher distillation, our chosen method in this study. Tab.~\ref{tab:mean_and_multi} presents the outcomes of distilling three UNets with varying parameters using these approaches, with the latter consistently yielding superior performance.
We omit a prior for distinguishing image areas, allowing the student to adaptively capture complementary information from different teachers, learning from multiple perspectives and thus enhancing generalization~\cite{fukuda2017efficient}. Conversely, mean teacher distillation employs pixel averaging as a prior, reducing student network robustness and performance.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{sec/figure/cvpr_stridecurve.pdf}
    \vspace{-3mm}
    \caption{Ablation experiments on the combinations of different blind-spots between training and inference. }
    \vspace{-3mm}
    \label{fig:robust}
\end{figure}
% \vspace{-3mm}
% \subsection{Complexity Analysis}
\noindent
\textbf{Complexity Analysis. }
Tab.~\ref{tab:Complexity} compares the complexity of different methods. Our method attains superior performance with the lowest computational overhead. It's worth noting that both AP-BSN and LG-BPN utilize R$^3$~\cite{APBSN} operations, substantially increasing computational overhead.





