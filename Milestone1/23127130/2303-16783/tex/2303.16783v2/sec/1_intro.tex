\section{Introduction}
\label{sec:intro}




Image denoising is an essential low-level computer vision problem. With the advancements in deep learning, an increasing number of studies are focused on supervised learning using clean-noisy pairs~\cite{RIDNet,CBDNet,FADNet,DANet,DnCNN,FFDNet}. Typically, additive white Gaussian noise (AWGN) is introduced into clean datasets to synthesize clean-noisy denoising datasets. However, real-world noise is known to be spatially correlated~\cite{chatterjee2011noise,jin2020review,park2009case}.
Some generative-based methods attempt to synthesize real-world noise from existing clean data~\cite{GAN2GAN,GCBD,UIDNet,C2N,DBSN}. However, synthesizing real-world noise remains challenging, and suffers from generalization issues.
To address the issue, some researchers attempt to capture clean-noisy pairs in real-world scenarios~\cite{SIDD,NIND}. However, in certain scenarios, such as medical imaging and electron microscopy, constructing such datasets can be impractical or even infeasible. 
% These limitations curtail the scope of supervised denoising algorithms.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{sec/figure/head.pdf}
    \caption{Comparisons of our AT-BSN with other methods. Our method recovers more high frequency texture details.}
    \vspace{-3mm}
    \label{fig:head}
\end{figure}

Self-supervised denoising algorithms, represented by Noise2Noise~\cite{Noise2Noise}, have brought new life to the denoising field.  These methods only require noisy observations to train the denoising model. However, in real-world scenarios, noise often exhibits spatial correlation, which contradicts the pixel-wise independent noise assumption~\cite{Noise2Void,Noise2Noise} that most self-supervised algorithms~\cite{Noise2Self,Neighbor2Neighbor,Noise2Void,Noise2Noise,Blind2Unblind} rely on. Recent studies have proposed self-supervised denoising algorithms suitable for real-world scenarios~\cite{APBSN,CVFSID,li2023spatially,wang2023lg,pan2023random,jang2023self}. These methods mainly disrupt the noise correlation by downsampling~\cite{APBSN,pan2023random,jang2023self} or neighborhood masking~\cite{li2023spatially,wang2023lg}. 
The representative work of the former is AP-BSN~\cite{APBSN}, which utilized pixel-shuffle downsampling (PD)~\cite{APBSN,whenAWGN} to disrupt the noise correlation and employed asymmetric PD stride factors for training and inference. 
However, according to the Nyquist-Shannon sampling theorem, downsampling methods disrupt image spatial structure during inference, leading to lower sampling density and loss of high-frequency details. Conversely, neighborhood masking methods denoise at original resolution structure, retaining more texture information.

In this paper, we carefully analyze the existing related work and point out that \textbf{training at the original resolution structure} and \textbf{using asymmetric operations during training and inference} are key to producing high-quality, texture-rich clear images in self-supervised real noise removal tasks. Based on these observations, we propose a novel paradigm called \textbf{Asymmetric Tunable Blind-Spot Network(AT-BSN)}, where the blind-spot size can be freely adjusted to balance between noise correlation suppression and image local structure destruction. Furthermore, the flexible tunable blind-spot allows us to obtain a potential teacher network distribution, where each sampled teacher has a different blind-spot, making each teacher networkâ€™s ability to handle flat/texture areas different. We propose a \textbf{Blind-Spots Based Multi-Teacher Distillation} strategy, which significantly improves performance and further reduces computational overhead. 
Experimental results demonstrate the effectiveness of the proposed method.


The main contributions are summarized as follows:
\begin{itemize}
    \item We carefully analyze existing methods and point out that training at the original resolution structure and using asymmetric operations during training and inference are key to producing high-quality, texture-rich results in self-supervised denoising tasks.
    \item We propose AT-BSN, which can better balance the suppression of noise correlation and the destruction of image's local spatial structure by applying asymmetric blind-spots during training and inference.
    \item We propose a Blind-Spots Based Multi-Teacher Distillation strategy, which significantly improves performance by distilling a lightweight student network from teachers with different blind-spots sampled from the teacher network distribution.
    \item Experimental results on multiple real-world datasets show our method achieves state-of-the-art performance, with clear advantages in computational complexity and preservation of high-frequency texture details.
\end{itemize}
