\section{Methods}
\subsection{Revisit of Various Methods to Disrupt Noise Spatial Correlation}
\label{sec:3-1}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{sec/figure/blindspots.pdf}
    \caption{Effective Receptive Field analysis of AP-BSN and our AT-BSN. Each column has the same center blind-spot size. The downsampling operation of AP-BSN can cause aliasing effects.}
    \label{fig:erf}
\end{figure}


Due to the effect of image signal processors (ISP), e.g. image demosaicking~\cite{chatterjee2011noise,jin2020review,park2009case}, real-world noise is generally known to be spatially correlated and pixel-wise dependent. Lee \etal ~\cite{APBSN} analyzed the spatial correlation of real-world noise and found that different camera devices in the SIDD dataset show similar noise behaviors in terms of spatial correlation. According to Lee \etal ~\cite{APBSN}'s analysis, the correlation of noise presents a Gaussian distribution that decays as distance increases. This correlation of noise violates the pixel-wise independent noise assumption of the BSN, rendering it inadequate for real noise removal.

Recently, some methods have been proposed to break the spatial correlation of noise, so that BSN can be used for real-world noise removal. Basically, these methods can be divided into downsampling based approaches and neighborhood masking based approaches.

% \paragraph{Downsampling Based Approaches.}
\noindent
\textbf{Downsampling Based Approaches.}
AP-BSN~\cite{APBSN} first introduced the pixel-shuffle downsampling operation~\cite{whenAWGN} into the self-supervised denoising task to break the noise correlation, and proposed asymmetric PD to balance between noise correlation removal and image structure damage. Jang \etal ~\cite{jang2023self} design a conditional blind-spot network, which selectively controls the blindness of the network to use the center pixel information. By retaining some information of the blind-spot part at test time, this method achieved better results. Further, Pan \etal ~\cite{pan2023random} propose random sub-sampling to address the data-hungry issue of training BSN with real noisy images, and further propose to use sampling difference as a perturbation to improve performance. Regardless of the exact downsampling method used, these methods aim to optimize the BSN $B_{\theta}{(\cdot)}$ mainly by minimizing the following loss functions:
\begin{align}
    \label{Downsamplingloss}
    \mathcal{L}_{down} &= {\Vert D_m^{-1}(B_{\theta}(D_m{(I_{noisy})})) - I_{noisy} \Vert}_1, \nonumber \\
    &or\ {\Vert B_{\theta}(D_m{(I_{noisy})}) - D_m{(I_{noisy})} \Vert}_1,
\end{align}
where $D_m{(\cdot)}$ denotes a certain downsampling method with a factor of $m$, $D_m^{-1}{(\cdot)}$ denotes its inverse operation.
Although the downsampling based methods can effectively break the spatial correlation of noise, the results of these methods are often blurry and lack texture details. According to the Nyquist-Shannon sampling theorem, the fidelity of the results is positively correlated with the sampling density. These methods train on degraded low-resolution images, and their receptive fields are present as sparse and diffuse grids, which makes it difficult to learn structural information. Moreover, these methods still test on low-resolution sub-images, which greatly reduces their sampling density and produces aliasing effects, leading to high-frequency detail loss. Additional time-consuming post-processing is also needed to eliminate aliasing effects.

\noindent
\textbf{Neighborhood Masking Based Approaches.}
The neighborhood masking based schemes attempt to train the denoising network on the original resolution structure, the optimization objective is as follows:
\begin{align}
    \label{fullresloss}
    \mathcal{L}_{neighbor} = {\Vert B^{'}_{\theta}(I_{noisy}) - I_{noisy} \Vert}_1,
\end{align}
where $B^{'}_{\theta}(\cdot)$ denotes a carefully modified BSN, in which the blind-spot size is enlarged.
LG-BPN~\cite{wang2023lg} proposed a densely-sampled patch-masked convolution, which breaks the spatial correlation of noise by masking the center part of a large convolution kernel. LG-BPN also proposed to squeeze the convolution kernel weights during inference to reduce the masked part, to balance between local structure damage and noise correlation removal. Note that this idea is similar to the asymmetric PD in AP-BSN, but the introduction of large convolution kernels brings high computational overhead, and the squeeze of convolution kernel weights is limited to specific convolution kernel sizes, which is inflexible to adjust the masked region. Li\etal ~\cite{li2023spatially} proposed to use a larger blind-neighborhood to break the noise correlation, and trained another network with a receptive field limited to the blind-neighborhood to fill in the information loss within the large blind-neighborhood position. However, this scheme utilizes the same large blind-neighborhood during training and inference, which lacks the consideration of local spatial structure damage.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{sec/figure/overall.pdf}
    \caption{Overview of the proposed AT-BSN framework. We employ asymmetric blind-spots for training and inference to balance the suppression of noise spatial correlation and local information preservation. We regard the trained AT-BSN as a meta-teacher network, generate multiple teacher networks by sampling different blind-spots, and execute multi-teacher distillation on a lightweight network.}
    \vspace{-2mm}
    \label{fig:arch}
\end{figure*}

\noindent
\textbf{Effective Receptive Field Analysis.}
We further compare the effective receptive fields (ERF) of the two schemes to demonstrate the importance of training at full resolution structure.
Noise correlation is generally confined to local regions, according to Lee's statistics~\cite{APBSN}. Therefore, it is unnecessary to disrupt regions beyond the local neighbors. We consider the ERF of the central pixel, and take our method and PD operation in AP-BSN as an example, as shown in Fig.~\ref{fig:erf}. The ERF of AP-BSN is calculated on the subgraph and remapped back to the original resolution structure. 
One can find that the ERF of AP-BSN in the original image manifests as a sparse grid-like pattern, akin to the effect of simple stacking of dilated convolutions~\cite{yu2015multi}. This characteristic comes with similar drawbacks to the dilated convolutions~\cite{chen2017rethinking,wang2018understanding,yu2015multi}, namely \textbf{1)} the loss of local information, posing challenges for the model to learn clues from the grid-like discontinuous sub-image for recovering the clean signal, and \textbf{2)} long-ranged information might be not relevant. Moreover, the loss of information continuity outside the center blind-spot can also introduce aliasing artifacts.
Nevertheless, it is apparent that our method only loses a portion of the information within the blind-spot area, while the ERF outside the blind-spot remains unaffected. This inspires us to set the size of blind-spot to 9 during training.
Due to the higher correlation of the signal compared to the noise, the central pixel can be recovered using the pixels outside the blind-spot that are less correlated with it in the noise domain. It is worth noting that the size of the blind-spot can be minimized during inference to reduce information loss.

Based on the analysis of the existing approaches, we draw the following conclusions. In order to recover clean images with clear textures from noisy images, \textbf{the following two points} are crucial:
% \begin{enumerate}[1)]
%     \item According to the Nyquist-Shannon sampling theorem, both the training and inference stages of the network need to be conducted on original input resolution structure to ensure sampling density.
% \item Asymmetric operation during training and inference is crucial to strike a balance between the disruption of noise spatial correlation and the destruction of local spatial structure. 
% \end{enumerate}
\begin{itemize}
    \item[{1)}] According to the Nyquist-Shannon sampling theorem, both the training and inference stages of the network need to be conducted on original input resolution structure to ensure sampling density.
\item[{2)}] Asymmetric operation during training and inference is crucial to strike a balance between the disruption of noise spatial correlation and the destruction of local spatial structure. 
\end{itemize}
The quantitative analysis of the second point can be found in the supplementary materials.
Based on these two observations, we propose AT-BSN, a blind-spot network that can flexibly adjust the blind-spot size during training and inference. Fig.~\ref{fig:three_asym} shows three schematic diagrams of asymmetric operations during training and inference. Compared with Fig.~\ref{fig:three_asym} (a), AT-BSN operates on original resolution structure to maximize sampling density. Compared with Figure Fig.~\ref{fig:three_asym} (b), AT-BSN has a lower computational cost and can adjust the blind-spot size at will, which prompts us to further propose a multi-teacher knowledge distillation strategy based on various blind-spots to further improve performance and reduce network complexity.

\subsection{Tunable Blind-Spot}
BSN~\cite{Noise2Void,APBSN,DBSN} is designed to denoise each pixel from its surrounding spatial neighborhood without itself. 
Typically, BSN can be constructed through shifted convolutions~\cite{Laine2019} or dilated convolution~\cite{DBSN}. 

\noindent
\textbf{Restricted Receptive Fields.}
Our AT-BSN is inspired by Laine's approach~\cite{Laine2019}, which combines four branches with restricted receptive fields, each of which is limited to a half-plane that excludes the central pixel.
For a $h \times h$ convolution kernel, we append $d=\lfloor h/2 \rfloor$ rows of zeros at the top of the feature map, apply the convolution, and finally crop the last $d$ rows of the feature map.
For the $2\times2$ pooling layer, we pad the top of the feature map and crop the last row of it before pooling.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{sec/figure/shift.pdf}
    \caption{Implementation principle of the tunable blind-spot. 
    }
    \vspace{-3.5mm}
    \label{fig:shift}
\end{figure}

\noindent
\textbf{Tunable Blind-Spot.}
After applying the shifted-conv based UNet to $I_{noisy}$, we obtain the resulting feature map denoted as $f_{up}$, whose receptive field is fully contained within an upward half-plane, including the center row. See Fig.~\ref{fig:shift} for more details.

We further shift the feature map $f_{up}$ downward by $s$ pixels, resulting in a shifted feature map $f_{up}^s$. 
\begin{align}
    \label{shift}
    f_{up}^s = M(f_{up}; s),
\end{align}
where $M(\cdot; s)$ denotes the shift operation of a offset $s$.
At this point, the receptive field of the central pixel only includes the positions beyond $s$ rows above the current location. 
Moreover, \textbf{the use of $M(\cdot; s)$ on the feature domain is decoupled from feature extraction, which grants it more flexibility}.
To expand the receptive field of a pixel to all directions around it, we rotate the input image by multiples of $90^{\circ}$ and feed them into the network. This results in feature maps $f_{up}^s$, $f_{down}^s$, $f_{left}^s$, and $f_{right}^s$.
Finally, the four feature maps are rotated to the correct orientation and linearly combined through several $1\times1$ convolutions to produce the final output $I_{pred}$.
\begin{align}
    \label{conv1x1}
    I_{pred} = Conv_{1\times1}([\hat{f}_{up}^s, \hat{f}_{down}^s, \hat{f}_{left}^s, \hat{f}_{right}^s]),
\end{align}
where $[,]$ denotes feature concatenation, $\hat{f}^s$ denotes the corresponding $f^s$ in the correct orientation.

Now a blind-spot area with a length of $k=2s-1$ is established in the center around each pixel. We can freely tune the size $k$ of the blind-spot by adjusting the shift factor $s$ of the feature map $f^s$ before the $1\times1$ convolutions. So far, we have achieved a BSN with a tunable blind-spot size. We denote the main network parameterized by $\theta$ as $F_{\theta}(\cdot; s)$, the entire process can be formulated as:
\begin{align}
    \label{wholebsn}
    I_{pred} = Conv_{1\times1}{(F_{\theta}(I_{noisy}; s))}.
\end{align}

\subsection{Asymmetric Blind-Spots}

Based on the fact that the noise correlation is less than the signal correlation, we can use appropriate blind-spot $k$ to suppress the noise correlation while minimizing the impact on signal correlation. The central pixel within a large blind-spot can be inferred from pixels outside the blind-spot that are less correlated with it in the noise domain. 
We minimize the following loss to train the network:
\begin{align}
    \label{atbsn-self}
    % \mathcal{L}_{self} &= {\Vert F_{\theta}(I_{noisy}; s) - I_{noisy} \Vert}_1 \nonumber \\
    % &= {\Vert I_{pred} - I_{noisy} \Vert}_1
    \mathcal{L}_{self} &= {\Vert Conv_{1\times1}{(F_{\theta}(I_{noisy}; s))} - I_{noisy} \Vert}_1 \nonumber \\
    &= {\Vert I_{pred} - I_{noisy} \Vert}_1.
\end{align}
Following previous works, we use $L^1$ norm for better generalization \cite{APBSN}. In practice, we choose $k=9$, that is, $s=4$, during training. 

We propose to achieve a balance between training and inference by employing asymmetric blind-spots, so that the trade-off between noise correlation removal and image structure damage can be achieved.
Since larger blind-spots have already been utilized during training to enable the BSN to learn to denoise, we can select smaller blind-spots during inference to minimize information loss. We denote the $k$ used in training and inference as $k_a$ and $k_b$, respectively. Similar notation rule is also used for $s$.
In Sec.~\ref{sec_abla}, we will demonstrate the robustness of our approach to different blind-spot combinations.


\begin{table*}[t]
% \setlength\tabcolsep{13.0pt}%调列距
% \setlength\tabcolsep{5.6pt}%调列距
\setlength\tabcolsep{2.6pt}%调列距
\begin{center}
\small
    % \begin{tabularx}{\textwidth}{lcccccc}
    \begin{tabular}{lcccccccc}
    \toprule
    &\multirow{2}{*}{Methods}&  
    \multicolumn{2}{c}{SIDD Benchmark}&\multicolumn{2}{c}{SIDD Validation }&\multicolumn{2}{c}{DND Benchmark} \\
    \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} & & PSNR$^\uparrow$ (dB)  & SSIM$^\uparrow$ & PSNR$^\uparrow$ (dB) & SSIM$^\uparrow$ & PSNR$^\uparrow$ (dB) & SSIM$^\uparrow$ \\
    \midrule
    \multirow{2}{*}{Non-Learning}& BM3D\cite{BM3D}   & 25.65& 0.685 & 31.75 & 0.706 & 34.51 & 0.851      \\
    & WNNM\cite{WNNM}   & 25.78 & 0.809 & 26.31 & 0.524 & 34.67 & 0.865      \\
    \midrule
    \multirow{3}{*}{Supervised}& DnCNN\cite{DnCNN}  & 37.61& 0.941& 37.73 & 0.943 & 37.90 & 0.943      \\
    & CBDNet\cite{CBDNet}   & 33.28 & 0.868 & 30.83 & 0.754 & 38.05 & 0.942      \\
    & RIDNet\cite{RIDNet}   & 37.87 & 0.943& 38.76 & 0.913 & 39.25 & 0.952      \\
    & VDN\cite{VDN}   & 39.26& 0.955 & 39.29 & 0.911 & 39.38 & 0.952      \\
    & AINDNet(R)\cite{AINDNet}   & 38.84& 0.951& 38.81 & - & 39.34 & 0.952      \\
    & DANet\cite{DANet}  &39.25 &0.955 & 39.47 & 0.918 & 39.58 & 0.955      \\
    & InvDN\cite{InvDN}  &39.28 & 0.955& 38.88 & - & 39.57 & 0.952      \\
    \midrule
    \multirow{2}{*}{Unpaired}
    & GCBD\cite{GCBD}   &- &- & - & - & 35.58 & 0.922      \\
    & D-BSN\cite{DBSN} $+$ MWCNN\cite{MWCNN}   & -& -& - & - & 37.93 & 0.937      \\
    & C2N\cite{C2N}   & 35.35& 0.937& 35.36 & 0.932 & 37.28 & 0.924      \\
    \midrule
    \multirow{3}{*}{Self-Supervised} & Noise2Void\cite{Noise2Void}   & 27.68&0.668 & 29.35 & 0.651 & - & -      \\
    & Laine-BSN\cite{Laine2019}   &- &- & 23.80$^\diamond$ & 0.493$^\diamond$ & - & -      \\
    & Noise2Self\cite{Noise2Self}   & 29.56&0.808 & 30.72 & 0.787 & - & -      \\
    & NAC\cite{NAC}   & -& -& - & - & 36.20 & 0.925      \\ 
    & R2R\cite{R2R}   & 34.78&0.898 & 35.04 & 0.844 & - & -      \\
    & CVF-SID\cite{CVFSID}   & 34.43 / 34.71$^\dagger$& 0.912 / 0.917$^\dagger$& 34.51 & {\color{blue}\textbf{0.941}} & 36.31 / 36.50$^\dagger$ & 0.923 / 0.924$^\dagger$      \\
    & AP-BSN $+$ R$^3$\cite{APBSN}   & 35.97 / 36.91$^\dagger$&0.925 / 0.931$^\dagger$ & 35.76 & - & \phantom{xx}-\phantom{xx} / 38.09$^\dagger$ & \phantom{xx}-\phantom{xx} / 0.937$^\dagger$      \\
    & C-BSN\cite{jang2023self}   & 36.82 / \phantom{xx}-\phantom{xxx}&0.934 / \phantom{xx}-\phantom{xxx} &  36.22& 0.935 & {\color{red}\textbf{38.45}} / {\color{blue}\textbf{38.60}}$^\dagger$ & {\color{red}\textbf{0.939}} / {\color{blue}\textbf{0.941}}$^\dagger$      \\
    & SDAP (E)\cite{pan2023random}   & 37.24 / {\color{blue}\textbf{37.53}}$^\dagger$&{\color{blue}\textbf{0.936}} / {\color{blue}\textbf{0.936}}$^\dagger$ &  37.30& 0.894 & 37.86 / 38.56$^\dagger$ & 0.937 / 0.940$^\dagger$      \\
    & LG-BPN\cite{wang2023lg}   & 37.28 / \phantom{xx}-\phantom{xxx}&{\color{blue}\textbf{0.936}} / \phantom{xx}-\phantom{xxx} &  37.32& 0.886 & \phantom{xx}-\phantom{xx} / 38.43$^\dagger$ & \phantom{xx}-\phantom{xx} / {\color{red}\textbf{0.942}}$^\dagger$      \\
    & Spatially-Adaptive (UNet)\cite{li2023spatially}   & {\color{blue}\textbf{37.41}} / 37.37$^\dagger$&0.934 / 0.929$^\dagger$ &  {\color{blue}\textbf{37.39}}& 0.934 & 38.18 / 38.58$^\dagger$ & {\color{blue}\textbf{0.938}} / 0.936$^\dagger$      \\ 
    & \textbf{AT-BSN (Ours)}  &36.73 / 36.74$^\dagger$ & 0.924 / 0.925$^\dagger$  & 36.80  & 0.934  &  37.76 / 38.19$^\dagger$ &  0.934 / 0.939$^\dagger$    \\
    & \textbf{AT-BSN (D) (Ours)}  & {\color{red}\textbf{37.77}} / {\color{red}\textbf{37.78}}$^\dagger$ & {\color{red}\textbf{0.942}} / {\color{red}\textbf{0.944}}$^\dagger$ & {\color{red}\textbf{37.88}} & {\color{red}\textbf{0.946}} & {\color{blue}\textbf{38.29}} / {\color{red}\textbf{38.68}}$^\dagger$ & {\color{red}\textbf{0.939}} / {\color{red}\textbf{0.942}}$^\dagger$      \\
    \bottomrule
    \end{tabular}
    % \end{tabularx}
\end{center}
\vspace{-3.5mm}
\caption{Comparison among different denoising methods on real-world datasets. We report the official results from the benchmark website or related paper. The $\dagger$ marks indicate the method is trained directly on the corresponding benchmark dataset in a fully self-supervised manner. The $\diamond$ marks indicate the result is measured by ourselves. }
\vspace{-1mm}
\label{tab:dabiao}
\end{table*}



% \subsection{Blind-Spot Self Ensemble and Distillation}
\subsection{Blind-Spots Based Multi-Teacher Distillation}
\label{sec:3p4}
While larger blind-spots can more effectively suppress spatial correlations between neighboring
noise signals, they also result in more loss of information. Conversely, smaller blind-spots exhibit an opposite trend.

To better integrate the advantages of our tunable blind-spot nature, we propose a blind-spot based multi-teacher distillation strategy.
Under different blind-spot sizes (or different $M(\cdot; s_i)$), the features extracted by AT-BSN satisfy the distribution $P_{\theta}(\hat{f}|s)$, and the restored clear images follow $P_{\theta}(I_{pred}|s)$. We consider the trained AT-BSN as a meta-teacher network that can generate many potential teacher networks almost \textbf{cost-free} by adjusting the size of blind-spots. Therefore, we obtain multiple potential teacher networks, where different teachers can provide different knowledge, i.e., different teachers handle smooth/texture areas differently (detailed analysis can be found in the supplementary materials). This characteristic is a key strength of our approach, allowing the student network to learn from various teachers.

Specifically, we pass $I_{noisy}$ through the trained network to get the feature $f$. Subsequently, we sample multiple $k$ from $Ks\in \{0,1,3,...,2S-1\}$ (where $S$ denotes the preset maximum offset), apply $M(\cdot; s_i)$ to $f$ multiple times, and obtain the features $\hat{f}^{s_i}$ under different blind-spot sizes. Then we apply trained $1\times1$ convolutions to get the clear images $I^{s_i}_{pred}$. Finally, we use $I^{s_i}_{pred}$ to distill a lightweight non-blind-spot network $N_{\theta}(\cdot)$. In order for the student network to learn fairly from $P_{\theta}(I_{pred}|s)$, we do not distinguish between different teacher signals explicitly. We set the same weight $\alpha_i=1$ for each teacher.
The student network is distilled by optimizing the following objective:
\begin{align}
    \label{atbsn-distill}
    \mathcal{L}_{distill} = \sum_{s_i\in K_s}\alpha_{i}{\Vert N_{\theta}(I_{noisy}) - sg(I^{s_i}_{pred}) \Vert}_1,
\end{align}
where $sg(\cdot)$ denotes stop gradient operation.
Under the multi-teacher distillation scheme, our student network can be lightweight and avoid the additional computational cost brought by the rotation operation of the BSN. 

Moreover, the distillation itself is also computationally efficient, as multiple teachers share the same feature $f$. The specific complexity analysis can be found in the supplementary materials.
The overall scheme of our methods can be found in Fig.~\ref{fig:arch}. 




\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{sec/figure/sidd_val_compare.pdf}
    \vspace{-2mm}
    \caption{Quantitative comparisons on SIDD validation dataset.}
    \vspace{-3.5mm}
    \label{fig:sidd}
\end{figure*}

