
@inproceedings{yu_robust_2022,
	title = {Robust {Weight} {Perturbation} for {Adversarial} {Training}},
	volume = {4},
	url = {https://www.ijcai.org/proceedings/2022/512},
	doi = {10.24963/ijcai.2022/512},
	abstract = {Electronic proceedings of IJCAI 2022},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Thirty-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Yu, Chaojian and Han, Bo and Gong, Mingming and Shen, Li and Ge, Shiming and Bo, Du and Liu, Tongliang},
	month = jul,
	year = {2022},
	note = {ISSN: 1045-0823},
	pages = {3688--3694},
}

@inproceedings{rade_reducing_2022,
	title = {Reducing {Excessive} {Margin} to {Achieve} a {Better} {Accuracy} vs. {Robustness} {Trade}-off},
	url = {https://openreview.net/forum?id=Azh9QBQ4tR7},
	abstract = {While adversarial training has become the de facto approach for training robust classifiers, it leads to a drop in accuracy. This has led to prior works postulating that accuracy is inherently at...},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Rade, Rahul and Moosavi-Dezfooli, Seyed-Mohsen},
	month = mar,
	year = {2022},
}

@inproceedings{wang_self-ensemble_2022,
	title = {Self-ensemble {Adversarial} {Training} for {Improved} {Robustness}},
	url = {https://openreview.net/forum?id=oU3aTsmeRQV},
	abstract = {Due to numerous breakthroughs in real-world applications brought by machine intelligence, deep neural networks (DNNs) are widely employed in critical applications. However, predictions of DNNs are...},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wang, Hongjun and Wang, Yisen},
	month = may,
	year = {2022},
}

@inproceedings{li_data_2022,
	title = {Data {Augmentation} {Alone} {Can} {Improve} {Adversarial} {Training}},
	booktitle = {in submission},
	author = {Li, Lin and Spratling, Michael},
	year = {2022},
}

@inproceedings{smith_cyclical_2017,
	title = {Cyclical {Learning} {Rates} for {Training} {Neural} {Networks}},
	doi = {10.1109/WACV.2017.58},
	abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" - linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
	booktitle = {2017 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Smith, Leslie N.},
	month = mar,
	year = {2017},
	keywords = {Computational efficiency, Computer architecture, Neural networks, Schedules, Training, Tuning},
	pages = {464--472},
}

@inproceedings{arjovsky_towards_2017,
	title = {Towards {Principled} {Methods} for {Training} {Generative} {Adversarial} {Networks}},
	url = {https://openreview.net/forum?id=Hk4_qw5xe},
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	language = {en},
	urldate = {2022-12-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Arjovsky, Martin and Bottou, Leon},
	year = {2017},
}

@misc{liu_convnet_2022,
	title = {A {ConvNet} for the 2020s},
	url = {http://arxiv.org/abs/2201.03545},
	doi = {10.48550/arXiv.2201.03545},
	abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	month = mar,
	year = {2022},
	note = {arXiv:2201.03545 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{croce_robustbench_2021,
	title = {{RobustBench}: a standardized adversarial robustness benchmark},
	shorttitle = {{RobustBench}},
	url = {https://openreview.net/forum?id=SSKZPJCt7B},
	abstract = {As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in \${\textbackslash}ell\_{\textbackslash}infty\$- and \${\textbackslash}ell\_2\$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability.},
	language = {en},
	urldate = {2022-12-14},
	booktitle = {Thirty-fifth {Conference} on {Neural} {Information} {Processing} {Systems} {Datasets} and {Benchmarks} {Track} ({Round} 2)},
	author = {Croce, Francesco and Andriushchenko, Maksym and Sehwag, Vikash and Debenedetti, Edoardo and Flammarion, Nicolas and Chiang, Mung and Mittal, Prateek and Hein, Matthias},
	month = oct,
	year = {2021},
}

@article{li_understanding_2023,
	title = {Understanding and combating robust overfitting via input loss landscape analysis and regularization},
	volume = {136},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322007087},
	doi = {10.1016/j.patcog.2022.109229},
	abstract = {Adversarial training is widely used to improve the robustness of deep neural networks to adversarial attack. However, adversarial training is prone to overfitting, and the cause is far from clear. This work sheds light on the mechanisms underlying overfitting through analyzing the loss landscape w.r.t. the input. We find that robust overfitting results from standard training, specifically the minimization of the clean loss, and can be mitigated by regularization of the loss gradients. Moreover, we find that robust overfitting turns severer during adversarial training partially because the gradient regularization effect of adversarial training becomes weaker due to the increase in the loss landscape’s curvature. To improve robust generalization, we propose a new regularizer to smooth the loss landscape by penalizing the weighted logits variation along the adversarial direction. Our method significantly mitigates robust overfitting and achieves the highest robustness and efficiency compared to similar previous methods. Code is available at https://github.com/TreeLLi/Combating-RO-AdvLC.},
	language = {en},
	urldate = {2022-12-13},
	journal = {Pattern Recognition},
	author = {Li, Lin and Spratling, Michael},
	month = apr,
	year = {2023},
	keywords = {Adversarial robustness, Adversarial training, Logit regularization, Loss landscape analysis, Robust overfitting},
	pages = {109229},
}

@article{obaid_picture_2022,
	title = {A picture is worth a thousand words: {Measuring} investor sentiment by combining machine learning and photos from news},
	volume = {144},
	issn = {0304-405X},
	shorttitle = {A picture is worth a thousand words},
	url = {https://www.sciencedirect.com/science/article/pii/S0304405X21002683},
	doi = {10.1016/j.jfineco.2021.06.002},
	abstract = {By applying machine learning to the accurate and cost-effective classification of photos based on sentiment, we introduce a daily market-level investor sentiment index (Photo Pessimism) obtained from a large sample of news photos. Consistent with behavioral models, Photo Pessimism predicts market return reversals and trading volume. The relation is strongest among stocks with high limits to arbitrage and during periods of elevated fear. We examine whether Photo Pessimism and pessimism embedded in news text act as complements or substitutes for each other in predicting stock returns and find evidence that the two are substitutes.},
	language = {en},
	number = {1},
	urldate = {2022-12-08},
	journal = {Journal of Financial Economics},
	author = {Obaid, Khaled and Pukthuanthong, Kuntara},
	month = apr,
	year = {2022},
	keywords = {Behavioral finance, Big data, Deep learning, Investor sentiment, Machine learning, Return predictability},
	pages = {273--297},
}

@inproceedings{zhang_rethinking_2022,
	title = {Rethinking {Lipschitz} {Neural} {Networks} and {Certified} {Robustness}: {A} {Boolean} {Function} {Perspective}},
	shorttitle = {Rethinking {Lipschitz} {Neural} {Networks} and {Certified} {Robustness}},
	url = {https://openreview.net/forum?id=xaWO6bAY0xM},
	abstract = {Designing neural networks with bounded Lipschitz constant is a promising way to obtain certifiably robust classifiers against adversarial examples. However, the relevant progress for the important \${\textbackslash}ell\_{\textbackslash}infty\$ perturbation setting is rather limited, and a principled understanding of how to design expressive \${\textbackslash}ell\_{\textbackslash}infty\$ Lipschitz networks is still lacking. In this paper, we bridge the gap by studying certified \${\textbackslash}ell\_{\textbackslash}infty\$ robustness from a novel perspective of representing Boolean functions. We derive two fundamental impossibility results that hold for any standard Lipschitz network: one for robust classification on finite datasets, and the other for Lipschitz function approximation. These results identify that networks built upon norm-bounded affine layers and Lipschitz activations intrinsically lose expressive power even in the two-dimensional case, and shed light on how recently proposed Lipschitz networks (e.g., GroupSort and \${\textbackslash}ell\_{\textbackslash}infty\$-distance nets) bypass these impossibilities by leveraging order statistic functions. Finally, based on these insights, we develop a unified Lipschitz network that generalizes prior works, and design a practical version that can be efficiently trained (making certified robust training free). Extensive experiments show that our approach is scalable, efficient, and consistently yields better certified robustness across multiple datasets and perturbation radii than prior Lipschitz networks.},
	language = {en},
	urldate = {2022-12-06},
	author = {Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
	month = oct,
	year = {2022},
}

@article{gu_empirical_2020,
	title = {Empirical {Asset} {Pricing} via {Machine} {Learning}},
	volume = {33},
	issn = {0893-9454},
	url = {https://doi.org/10.1093/rfs/hhaa009},
	doi = {10.1093/rfs/hhaa009},
	abstract = {We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.},
	number = {5},
	urldate = {2022-12-05},
	journal = {The Review of Financial Studies},
	author = {Gu, Shihao and Kelly, Bryan and Xiu, Dacheng},
	month = may,
	year = {2020},
	pages = {2223--2273},
}

@article{leippold_machine_2022,
	title = {Machine learning in the {Chinese} stock market},
	volume = {145},
	issn = {0304-405X},
	url = {https://www.sciencedirect.com/science/article/pii/S0304405X21003743},
	doi = {10.1016/j.jfineco.2021.08.017},
	abstract = {We add to the emerging literature on empirical asset pricing in the Chinese stock market by building and analyzing a comprehensive set of return prediction factors using various machine learning algorithms. Contrasting previous studies for the US market, liquidity emerges as the most important predictor, leading us to closely examine the impact of transaction costs. The retail investors’ dominating presence positively affects short-term predictability, particularly for small stocks. Another feature that distinguishes the Chinese market from the US market is the high predictability of large stocks and state-owned enterprises over longer horizons. The out-of-sample performance remains economically significant after transaction costs.},
	language = {en},
	number = {2, Part A},
	urldate = {2022-12-05},
	journal = {Journal of Financial Economics},
	author = {Leippold, Markus and Wang, Qian and Zhou, Wenyu},
	month = aug,
	year = {2022},
	keywords = {Chinese stock market, Factor investing, Machine learning, Model selection},
	pages = {64--82},
}

@article{bianchi_bond_2021,
	title = {Bond {Risk} {Premiums} with {Machine} {Learning}},
	volume = {34},
	issn = {0893-9454},
	url = {https://doi.org/10.1093/rfs/hhaa062},
	doi = {10.1093/rfs/hhaa062},
	abstract = {We show that machine learning methods, in particular, extreme trees and neural networks (NNs), provide strong statistical evidence in favor of bond return predictability. NN forecasts based on macroeconomic and yield information translate into economic gains that are larger than those obtained using yields alone. Interestingly, the nature of unspanned factors changes along the yield curve: stock- and labor-market-related variables are more relevant for short-term maturities, whereas output and income variables matter more for longer maturities. Finally, NN forecasts correlate with proxies for time-varying risk aversion and uncertainty, lending support to models featuring both channels.},
	number = {2},
	urldate = {2022-12-05},
	journal = {The Review of Financial Studies},
	author = {Bianchi, Daniele and Büchner, Matthias and Tamoni, Andrea},
	month = feb,
	year = {2021},
	pages = {1046--1089},
}

@misc{chen_deep_2021,
	title = {Deep {Learning} in {Asset} {Pricing}},
	url = {http://arxiv.org/abs/1904.00745},
	doi = {10.48550/arXiv.1904.00745},
	abstract = {We use deep neural networks to estimate an asset pricing model for individual stock returns that takes advantage of the vast amount of conditioning information, while keeping a fully flexible form and accounting for time-variation. The key innovations are to use the fundamental no-arbitrage condition as criterion function, to construct the most informative test assets with an adversarial approach and to extract the states of the economy from many macroeconomic time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in terms of Sharpe ratio, explained variation and pricing errors and identifies the key factors that drive asset prices.},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Chen, Luyang and Pelger, Markus and Zhu, Jason},
	month = aug,
	year = {2021},
	note = {arXiv:1904.00745 [q-fin, stat]},
	keywords = {Quantitative Finance - Statistical Finance, Statistics - Methodology},
}

@inproceedings{cui_learnable_2021,
	title = {Learnable {Boundary} {Guided} {Adversarial} {Training}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Cui_Learnable_Boundary_Guided_Adversarial_Training_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-12-01},
	author = {Cui, Jiequan and Liu, Shu and Wang, Liwei and Jia, Jiaya},
	year = {2021},
	pages = {15721--15730},
}

@inproceedings{cazenavette_architectural_2021,
	title = {Architectural {Adversarial} {Robustness}: {The} {Case} for {Deep} {Pursuit}},
	shorttitle = {Architectural {Adversarial} {Robustness}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Cazenavette_Architectural_Adversarial_Robustness_The_Case_for_Deep_Pursuit_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-11-28},
	author = {Cazenavette, George and Murdock, Calvin and Lucey, Simon},
	year = {2021},
	pages = {7150--7158},
}

@article{zeng_are_2021,
	title = {Are {Adversarial} {Examples} {Created} {Equal}? {A} {Learnable} {Weighted} {Minimax} {Risk} for {Robustness} under {Non}-uniform {Attacks}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Are {Adversarial} {Examples} {Created} {Equal}?},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17292},
	doi = {10.1609/aaai.v35i12.17292},
	abstract = {Adversarial Training is proved to be an efficient method to defend against adversarial examples, being one of the few defenses that withstand strong attacks. However, traditional defense mechanisms assume a uniform attack over the examples according to the underlying data distribution, which is apparently unrealistic as the attacker could choose to focus on more vulnerable examples. We present a weighted minimax risk optimization that defends against non-uniform attacks, achieving robustness against adversarial examples under perturbed test data distributions. Our modified risk considers importance weights of different adversarial examples and focuses adaptively on harder examples that are wrongly classified or at higher risk of being classified incorrectly. The designed risk allows the training process to learn a strong defense through optimizing the importance weights. The experiments show that our model significantly improves state-of-the-art adversarial accuracy under non-uniform attacks without a significant drop under uniform attacks.},
	language = {en},
	number = {12},
	urldate = {2022-11-28},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zeng, Huimin and Zhu, Chen and Goldstein, Tom and Huang, Furong},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {Adversarial Learning \& Robustness},
	pages = {10815--10823},
}

@inproceedings{mao_enhance_2022,
	title = {Enhance the {Visual} {Representation} via {Discrete} {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=qtZac7A3-F},
	abstract = {Adversarial Training (AT), which is commonly accepted as one of the most effective approaches defending against adversarial examples, can largely harm the standard performance, thus has limited usefulness on industrial-scale production and applications. Surprisingly, this phenomenon is totally opposite in Natural Language Processing (NLP) task, where AT can even benefit for generalization. We notice the merit of AT in NLP tasks could derive from the discrete and symbolic input space. For borrowing the advantage from NLP-style AT, we propose Discrete Adversarial Training (DAT). DAT leverages VQGAN to reform the image data to discrete text-like inputs, i.e. visual words. Then it minimizes the maximal risk on such discrete images with symbolic adversarial perturbations. We further give an explanation from the perspective of distribution to demonstrate the effectiveness of DAT. As a plug-and-play technique for enhancing the visual representation, DAT achieves significant improvement on multiple tasks including image classification, object detection and self-supervised learning. Especially, the model pre-trained with Masked Auto-Encoding (MAE) and fine-tuned by our DAT without extra data can get 31.40 mCE on ImageNet-C and 32.77\% top-1 accuracy on Stylized-ImageNet, building the new state-of-the-art. The code will be available at https://github.com/alibaba/easyrobust.},
	language = {en},
	urldate = {2022-11-28},
	author = {Mao, Xiaofeng and Chen, YueFeng and Duan, Ranjie and Zhu, Yao and Qi, Gege and Ye, Shaokai and Li, Xiaodan and Zhang, Rong and Xue', Hui},
	month = oct,
	year = {2022},
}

@inproceedings{gan_large-scale_2020,
	title = {Large-{Scale} {Adversarial} {Training} for {Vision}-and-{Language} {Representation} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/49562478de4c54fafd4ec46fdb297de5-Abstract.html},
	abstract = {We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the free'' adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.},
	urldate = {2022-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
	year = {2020},
	pages = {6616--6628},
}

@inproceedings{modas_prime_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{PRIME}: {A} {Few} {Primitives} {Can} {Boost} {Robustness} to {Common} {Corruptions}},
	isbn = {978-3-031-19806-9},
	shorttitle = {{PRIME}},
	doi = {10.1007/978-3-031-19806-9_36},
	abstract = {Despite their impressive performance on image classification tasks, deep networks have a hard time generalizing to unforeseen corruptions of their data. To fix this vulnerability, prior works have built complex data augmentation strategies, combining multiple methods to enrich the training data. However, introducing intricate design choices or heuristics makes it hard to understand which elements of these methods are indeed crucial for improving robustness. In this work, we take a step back and follow a principled approach to achieve robustness to common corruptions. We propose PRIME, a general data augmentation scheme that relies on simple yet rich families of max-entropy image transformations. PRIME outperforms the prior art in terms of corruption robustness, while its simplicity and plug-and-play nature enable combination with other methods to further boost their robustness. We analyze PRIME to shed light on the importance of the mixing strategy on synthesizing corrupted images, and to reveal the robustness-accuracy trade-offs arising in the context of common corruptions. Finally, we show that the computational efficiency of our method allows it to be easily used in both on-line and off-line data augmentation schemes. Our code is available at https://github.com/amodas/PRIME-augmentations.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Modas, Apostolos and Rade, Rahul and Ortiz-Jiménez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	pages = {623--640},
}

@article{karim_adversarial_2021,
	title = {Adversarial {Attacks} on {Time} {Series}},
	volume = {43},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.2986319},
	abstract = {Time series classification models have been garnering significant importance in the research community. However, not much research has been done on generating adversarial samples for these models. These adversarial samples can become a security concern. In this paper, we propose utilizing an adversarial transformation network (ATN) on a distilled model to attack various time series classification models. The proposed attack on the classification model utilizes a distilled model as a surrogate that mimics the behavior of the attacked classical time series classification models. Our proposed methodology is applied onto 1-nearest neighbor dynamic time warping (1-NN DTW) and a fully convolutional network (FCN), all of which are trained on 42 University of California Riverside (UCR) datasets. In this paper, we show both models were susceptible to attacks on all 42 datasets. When compared to Fast Gradient Sign Method, the proposed attack generates a larger faction of successful adversarial black-box attacks. A simple defense mechanism is successfully devised to reduce the fraction of successful adversarial samples. Finally, we recommend future researchers that develop time series classification models to incorporating adversarial data samples into their training data sets to improve resilience on adversarial samples.},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Karim, Fazle and Majumdar, Somshubra and Darabi, Houshang},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computational modeling, Computer vision, Data models, Machine learning, Neural networks, Time series analysis, Time series classification, Training, adversarial machine learning, deep learning, perturbation methods},
	pages = {3309--3320},
}

@inproceedings{li_why_2022,
	title = {Why {Robust} {Generalization} in {Deep} {Learning} is {Difficult}: {Perspective} of {Expressive} {Power}},
	shorttitle = {Why {Robust} {Generalization} in {Deep} {Learning} is {Difficult}},
	url = {https://openreview.net/forum?id=Z26xiZkbjgE},
	abstract = {It is well-known that modern neural networks are vulnerable to adversarial examples. To mitigate this problem, a series of robust learning algorithms have been proposed. However, although the robust training error can be near zero via some methods, all existing algorithms lead to a high robust generalization error. In this paper, we provide a theoretical understanding of this puzzling phenomenon from the perspective of expressive power for deep neural networks. Specifically, for binary classification problems with well-separated data, we show that, for ReLU networks, while mild over-parameterization is sufficient for high robust training accuracy, there exists a constant robust generalization gap unless the size of the neural network is exponential in the data dimension \$d\$. This result holds even if the data is linear separable (which means achieving standard generalization is easy), and more generally for any parameterized function classes as long as their VC dimension is at most polynomial in the number of parameters. Moreover, we establish an improved upper bound of \${\textbackslash}exp(\{{\textbackslash}mathcal\{O\}\}(k))\$ for the network size to achieve low robust generalization error when the data lies on a manifold with intrinsic dimension \$k\$ (\$k {\textbackslash}ll d\$). Nonetheless, we also have a lower bound that grows exponentially with respect to \$k\$ --- the curse of dimensionality is inevitable. By demonstrating an exponential separation between the network size for achieving low robust training and generalization error, our results reveal that the hardness of robust generalization may stem from the expressive power of practical models.},
	language = {en},
	urldate = {2022-11-24},
	author = {Li, Binghui and Jin, Jikai and Zhong, Han and Hopcroft, John E. and Wang, Liwei},
	month = oct,
	year = {2022},
}

@inproceedings{jia_-at_2022,
	title = {{LAS}-{AT}: {Adversarial} {Training} {With} {Learnable} {Attack} {Strategy}},
	shorttitle = {{LAS}-{AT}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Jia_LAS-AT_Adversarial_Training_With_Learnable_Attack_Strategy_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Jia, Xiaojun and Zhang, Yong and Wu, Baoyuan and Ma, Ke and Wang, Jue and Cao, Xiaochun},
	year = {2022},
	pages = {13398--13408},
}

@inproceedings{zhang_geometry-aware_2021,
	title = {Geometry-aware {Instance}-reweighted {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=iAX0l6Cz8ub},
	abstract = {In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve...},
	language = {en},
	urldate = {2021-09-10},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Jingfeng and Zhu, Jianing and Niu, Gang and Han, Bo and Sugiyama, Masashi and Kankanhalli, Mohan},
	year = {2021},
}

@inproceedings{tsiligkaridis_understanding_2022,
	title = {Understanding and {Increasing} {Efficiency} of {Frank}-{Wolfe} {Adversarial} {Training}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Tsiligkaridis_Understanding_and_Increasing_Efficiency_of_Frank-Wolfe_Adversarial_Training_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-19},
	author = {Tsiligkaridis, Theodoros and Roberts, Jay},
	year = {2022},
	pages = {50--59},
}

@inproceedings{zhang_towards_2022,
	title = {Towards {Efficient} {Data} {Free} {Black}-{Box} {Adversarial} {Attack}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Towards_Efficient_Data_Free_Black-Box_Adversarial_Attack_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-11-19},
	author = {Zhang, Jie and Li, Bo and Xu, Jianghe and Wu, Shuang and Ding, Shouhong and Zhang, Lei and Wu, Chao},
	year = {2022},
	pages = {15115--15125},
}

@inproceedings{li_visualizing_2018,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html},
	abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	urldate = {2022-11-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	year = {2018},
}

@article{zhao_well-classified_2022,
	title = {Well-{Classified} {Examples} {Are} {Underestimated} in {Classification} with {Deep} {Neural} {Networks}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20904},
	doi = {10.1609/aaai.v36i8.20904},
	abstract = {The conventional wisdom behind learning deep classification models is to focus on bad-classified examples and ignore well-classified examples that are far from the decision boundary. For instance, when training with cross-entropy loss, examples with higher likelihoods (i.e., well-classified examples) contribute smaller gradients in back-propagation. However, we theoretically show that this common practice hinders representation learning, energy optimization, and margin growth. To counteract this deficiency, we propose to reward well-classified examples with additive bonuses to revive their contribution to the learning process. This counterexample theoretically addresses these three issues. We empirically support this claim by directly verifying the theoretical results or significant performance improvement with our counterexample on diverse tasks, including image classification, graph classification, and machine translation. Furthermore, this paper shows that we can deal with complex scenarios, such as imbalanced classification, OOD detection, and applications under adversarial attacks because our idea can solve these three issues. Code is available at https://github.com/lancopku/well-classified-examples-are-underestimated.},
	language = {en},
	number = {8},
	urldate = {2022-11-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhao, Guangxiang and Yang, Wenkai and Ren, Xuancheng and Li, Lei and Wu, Yunfang and Sun, Xu},
	month = jun,
	year = {2022},
	note = {Number: 8},
	keywords = {Computer Vision (CV)},
	pages = {9180--9189},
}

@misc{smith_super-convergence_2018,
	title = {Super-{Convergence}: {Very} {Fast} {Training} of {Neural} {Networks} {Using} {Large} {Learning} {Rates}},
	shorttitle = {Super-{Convergence}},
	url = {http://arxiv.org/abs/1708.07120},
	doi = {10.48550/arXiv.1708.07120},
	abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
	urldate = {2022-11-14},
	publisher = {arXiv},
	author = {Smith, Leslie N. and Topin, Nicholay},
	month = may,
	year = {2018},
	note = {arXiv:1708.07120 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{hendrycks_ausing_2019,
	title = {{aUsing} {Self}-{Supervised} {Learning} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/a2b15837edac15df90721968986f7f8e-Abstract.html},
	abstract = {Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating or reducing the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research.},
	urldate = {2022-07-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
	year = {2019},
}

@inproceedings{cohen_certified_2019,
	title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
	url = {https://proceedings.mlr.press/v97/cohen19c.html},
	abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the L2 norm. While this "randomized smoothing" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between L2 robustness and Gaussian noise. We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49\% under adversarial perturbations with L2 norm less than 0.5 (=127/255). Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified L2 robustness are viable, smoothing delivers higher certified accuracies. The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification.},
	language = {en},
	urldate = {2022-11-12},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1310--1320},
}

@inproceedings{huang_calibrated_2021,
	title = {calibrated adversarial training},
	url = {https://proceedings.mlr.press/v157/huang21a.html},
	abstract = {Adversarial training is an approach of increasing the robustness of models to adversarial attacks by including adversarial examples in the training set. One major challenge of producing adversarial examples is to contain sufficient perturbation in the example to flip the model’s output while not making severe changes in the example’s semantical content. Exuberant change in the semantical content could also change the true label of the example. Adding such examples to the training set results in adverse effects. In this paper, we present the Calibrated Adversarial Training, a method that reduces the adverse effects of semantic perturbations in adversarial training. The method produces pixel-level adaptations to the perturbations based on novel calibrated robust error. We provide theoretical analysis on the calibrated robust error and derive an upper bound for it. Our empirical results show a superior performance of the Calibrated Adversarial Training over a number of public datasets.},
	language = {en},
	urldate = {2022-10-30},
	booktitle = {Proceedings of {The} 13th {Asian} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Huang, Tianjin and Menkovski, Vlado and Pei, Yulong and Pechenizkiy, Mykola},
	month = nov,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {626--641},
}

@inproceedings{poursaeed_generative_2018,
	title = {Generative {Adversarial} {Perturbations}},
	doi = {10.1109/CVPR.2018.00465},
	abstract = {In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Poursaeed, Omid and Katsman, Isay and Gao, Bicheng and Belongie, Serge},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Generators, Image segmentation, Iterative methods, Perturbation methods, Semantics, Task analysis, Training},
	pages = {4422--4431},
}

@inproceedings{poursaeed_generative_2018-1,
	title = {Generative {Adversarial} {Perturbations}},
	doi = {10.1109/CVPR.2018.00465},
	abstract = {In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling both classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Poursaeed, Omid and Katsman, Isay and Gao, Bicheng and Belongie, Serge},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Generators, Image segmentation, Iterative methods, Perturbation methods, Semantics, Task analysis, Training},
	pages = {4422--4431},
}

@inproceedings{athalye_obfuscated_2018,
	title = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}: {Circumventing} {Defenses} to {Adversarial} {Examples}},
	shorttitle = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}},
	url = {http://arxiv.org/abs/1802.00420},
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimizationbased attacks, we ﬁnd defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining noncertiﬁed white-box-secure defenses at ICLR 2018, we ﬁnd obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{biggio_wild_2018,
	title = {Wild patterns: {Ten} years after the rise of adversarial machine learning},
	volume = {84},
	issn = {0031-3203},
	shorttitle = {Wild patterns},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318302565},
	doi = {10.1016/j.patcog.2018.07.023},
	abstract = {Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.},
	language = {en},
	urldate = {2022-10-28},
	journal = {Pattern Recognition},
	author = {Biggio, Battista and Roli, Fabio},
	month = dec,
	year = {2018},
	keywords = {Adversarial examples, Adversarial machine learning, Deep learning, Evasion attacks, Poisoning attacks, Secure learning},
	pages = {317--331},
}

@article{yu_improving_2023,
	title = {Improving adversarial robustness by learning shared information},
	volume = {134},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322005349},
	doi = {10.1016/j.patcog.2022.109054},
	abstract = {We consider the problem of improving the adversarial robustness of neural networks while retaining natural accuracy. Motivated by the multi-view information bottleneck formalism, we seek to learn a representation that captures the shared information between clean samples and their corresponding adversarial samples while discarding these samples’ view-specific information. We show that this approach leads to a novel multi-objective loss function, and we provide mathematical motivation for its components towards improving the robust vs. natural accuracy tradeoff. We demonstrate enhanced tradeoff compared to current state-of-the-art methods with extensive evaluation on various benchmark image datasets and architectures. Ablation studies indicate that learning shared representations is key to improving performance.},
	language = {en},
	urldate = {2022-10-28},
	journal = {Pattern Recognition},
	author = {Yu, Xi and Smedemark-Margulies, Niklas and Aeron, Shuchin and Koike-Akino, Toshiaki and Moulin, Pierre and Brand, Matthew and Parsons, Kieran and Wang, Ye},
	month = feb,
	year = {2023},
	keywords = {Adversarial robustness, Information bottleneck, Multi-view learning, Shared information,},
	pages = {109054},
}

@misc{machiraju_bio-inspired_2021,
	title = {Bio-inspired {Robustness}: {A} {Review}},
	shorttitle = {Bio-inspired {Robustness}},
	url = {http://arxiv.org/abs/2103.09265},
	doi = {10.48550/arXiv.2103.09265},
	abstract = {Deep convolutional neural networks (DCNNs) have revolutionized computer vision and are often advocated as good models of the human visual system. However, there are currently many shortcomings of DCNNs, which preclude them as a model of human vision. For example, in the case of adversarial attacks, where adding small amounts of noise to an image, including an object, can lead to strong misclassification of that object. But for humans, the noise is often invisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot be taken as serious models of human vision. Many studies have tried to add features of the human visual system to DCNNs to make them robust against adversarial attacks. However, it is not fully clear whether human vision inspired components increase robustness because performance evaluations of these novel components in DCNNs are often inconclusive. We propose a set of criteria for proper evaluation and analyze different models according to these criteria. We finally sketch future efforts to make DCCNs one step closer to the model of human vision.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Machiraju, Harshitha and Choung, Oh-Hyeon and Frossard, Pascal and Herzog, Michael H.},
	month = mar,
	year = {2021},
	note = {arXiv:2103.09265 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{vuyyuru_biologically_2020,
	title = {Biologically {Inspired} {Mechanisms} for {Adversarial} {Robustness}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/17256f049f1e3fede17c7a313f7657f4-Abstract.html},
	abstract = {A convolutional neural network strongly robust to adversarial perturbations at reasonable computational and performance cost has not yet been demonstrated. The primate visual ventral stream seems to be robust to small perturbations in visual stimuli but the underlying mechanisms that give rise to this robust perception are not understood. In this work, we investigate the role of two biologically plausible mechanisms in adversarial robustness. We demonstrate that the non-uniform sampling performed by the primate retina and the presence of multiple receptive fields with a range of receptive field sizes at each eccentricity improve the robustness of neural networks to small adversarial perturbations. We verify that these two mechanisms do not suffer from gradient obfuscation and study their contribution to adversarial robustness through ablation studies.},
	urldate = {2022-10-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vuyyuru, Manish Reddy and Banburski, Andrzej and Pant, Nishka and Poggio, Tomaso},
	year = {2020},
	pages = {2135--2146},
}

@inproceedings{tramer_fundamental_2020,
	title = {Fundamental {Tradeoffs} between {Invariance} and {Sensitivity} to {Adversarial} {Perturbations}},
	url = {https://proceedings.mlr.press/v119/tramer20a.html},
	abstract = {Adversarial examples are malicious inputs crafted to induce misclassification. Commonly studied {\textbackslash}emph\{sensitivity-based\} adversarial examples introduce semantically-small changes to an input that result in a different model prediction. This paper studies a complementary failure mode, {\textbackslash}emph\{invariance-based\} adversarial examples, that introduce minimal semantic changes that modify an input’s true label yet preserve the model’s prediction. We demonstrate fundamental tradeoffs between these two types of adversarial examples. We show that defenses against sensitivity-based attacks actively harm a model’s accuracy on invariance-based attacks, and that new approaches are needed to resist both attack types. In particular, we break state-of-the-art adversarially-trained and {\textbackslash}emph\{certifiably-robust\} models by generating small perturbations that the models are (provably) robust to, yet that change an input’s class according to human labelers. Finally, we formally show that the existence of excessively invariant classifiers arises from the presence of {\textbackslash}emph\{overly-robust\} predictive features in standard datasets.},
	language = {en},
	urldate = {2022-10-26},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tramer, Florian and Behrmann, Jens and Carlini, Nicholas and Papernot, Nicolas and Jacobsen, Joern-Henrik},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {9561--9571},
}

@inproceedings{yang_one_2022,
	title = {One {Size} {Does} {NOT} {Fit} {All}: {Data}-{Adaptive} {Adversarial} {Training}},
	abstract = {Adversarial robustness is critical for deep learning models to defend against adversarial attacks. Although adversarial training is considered to be one of the most effective ways to improve the model’s adversarial robustness, it usually yields models with lower natural accuracy. In this paper, we argue that, for the attackable examples, traditional adversarial training which utilizes a fixed size perturbation ball can create adversarial examples that deviate far away from the original class towards the target class. Thus, the model’s performance on the natural target class will drop drastically, which leads to the decline of natural accuracy. To this end, we propose the Data-Adaptive Adversarial Training (DAAT) which adaptively adjusts the perturbation ball to a proper size for each of the natural examples with the help of a natural trained calibration network. Besides, a dynamic training strategy empowers the DAAT models with impressive robustness while retaining remarkable natural accuracy. Based on a toy example, we theoretically prove the recession of the natural accuracy caused by adversarial training and show how the data-adaptive perturbation size helps the model resist it. Finally, empirical experiments on benchmark datasets demonstrate the significant improvement of DAAT models on natural accuracy compared with strong baselines.},
	language = {en},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Yang, Shuo and Xu, Chang},
	year = {2022},
	pages = {16},
}

@article{zhang_towards_2020,
	title = {Towards {Robust} {Pattern} {Recognition}: {A} {Review}},
	volume = {108},
	issn = {1558-2256},
	shorttitle = {Towards {Robust} {Pattern} {Recognition}},
	doi = {10.1109/JPROC.2020.2989782},
	abstract = {The accuracies for many pattern recognition tasks have increased rapidly year by year, achieving or even outperforming human performance. From the perspective of accuracy, pattern recognition seems to be a nearly solved problem. However, once launched in real applications, the high-accuracy pattern recognition systems may become unstable and unreliable due to the lack of robustness in open and changing environments. In this article, we present a comprehensive review of research toward robust pattern recognition from the perspective of breaking three basic and implicit assumptions: closed-world assumption, independent and identically distributed assumption, and clean and big data assumption, which form the foundation of most pattern recognition models. Actually, our brain is robust at learning concepts continually and incrementally, in complex, open, and changing environments, with different contexts, modalities, and tasks, by showing only a few examples, under weak or noisy supervision. These are the major differences between human intelligence and machine intelligence, which are closely related to the above three assumptions. After witnessing the significant progress in accuracy improvement nowadays, this review paper will enable us to analyze the shortcomings and limitations of current methods and identify future research directions for robust pattern recognition.},
	number = {6},
	journal = {Proceedings of the IEEE},
	author = {Zhang, Xu-Yao and Liu, Cheng-Lin and Suen, Ching Y.},
	month = jun,
	year = {2020},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Big Data, Clean and big data, Distributed control, Machine intelligence, Neural networks, Pattern recognition, Robustness, Task analysis, closed world, independent and identically distributed, robust pattern recognition},
	pages = {894--922},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	language = {en},
	number = {3},
	urldate = {2022-10-19},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	keywords = {Benchmark, Dataset, Large-scale, Object detection, Object recognition},
	pages = {211--252},
}

@article{qian_survey_2022,
	title = {A survey of robust adversarial training in pattern recognition: {Fundamental}, theory, and methodologies},
	volume = {131},
	issn = {0031-3203},
	shorttitle = {A survey of robust adversarial training in pattern recognition},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322003703},
	doi = {10.1016/j.patcog.2022.108889},
	abstract = {Deep neural networks have achieved remarkable success in machine learning, computer vision, and pattern recognition in the last few decades. Recent studies, however, show that neural networks (both shallow and deep) may be easily fooled by certain imperceptibly perturbed input samples called adversarial examples. Such security vulnerability has resulted in a large body of research in recent years because real-world threats could be introduced due to the vast applications of neural networks. To address the robustness issue to adversarial examples particularly in pattern recognition, robust adversarial training has become one mainstream. Various ideas, methods, and applications have boomed in the field. Yet, a deep understanding of adversarial training including characteristics, interpretations, theories, and connections among different models has remained elusive. This paper presents a comprehensive survey trying to offer a systematic and structured investigation on robust adversarial training in pattern recognition. We start with fundamentals including definition, notations, and properties of adversarial examples. We then introduce a general theoretical framework with gradient regularization for defending against adversarial samples - robust adversarial training with visualizations and interpretations on why adversarial training can lead to model robustness. Connections will also be established between adversarial training and other traditional learning theories. After that, we summarize, review, and discuss various methodologies with defense/training algorithms in a structured way. Finally, we present analysis, outlook, and remarks on adversarial training.},
	language = {en},
	urldate = {2022-10-19},
	journal = {Pattern Recognition},
	author = {Qian, Zhuang and Huang, Kaizhu and Wang, Qiu-Feng and Zhang, Xu-Yao},
	month = nov,
	year = {2022},
	keywords = {Adversarial examples, Adversarial training, Robust learning},
	pages = {108889},
}

@inproceedings{sehwag_robust_2022,
	title = {Robust {Learning} {Meets} {Generative} {Models}: {Can} {Proxy} {Distributions} {Improve} {Adversarial} {Robustness}?},
	shorttitle = {Robust {Learning} {Meets} {Generative} {Models}},
	url = {https://openreview.net/forum?id=WVX0NNVBBkV},
	abstract = {While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets. For example, we improve robust accuracy by up to \$7.5\$\% and \$6.7\$\% in \${\textbackslash}ell\_\{{\textbackslash}infty\}\$ and \${\textbackslash}ell\_2\$ threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset. We also improve certified robust accuracy by \$7.6\$\% on the CIFAR-10 dataset. We further demonstrate that different generative models brings a disparate improvement in the performance in robust training. We propose a robust discrimination approach to characterize the impact and further provide a deeper understanding of why diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks.},
	language = {en},
	urldate = {2022-10-16},
	author = {Sehwag, Vikash and Mahloujifar, Saeed and Handina, Tinashe and Dai, Sihui and Xiang, Chong and Chiang, Mung and Mittal, Prateek},
	month = mar,
	year = {2022},
}

@article{ning_improving_2021,
	title = {Improving {Model} {Robustness} by {Adaptively} {Correcting} {Perturbation} {Levels} with {Active} {Queries}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17106},
	doi = {10.1609/aaai.v35i10.17106},
	abstract = {In addition to high accuracy, robustness is becoming increasingly important for machine learning models in various applications. Recently, much research has been devoted to improving the model robustness by training with noise perturbations. Most existing studies assume a fixed perturbation level for all training examples, which however hardly holds in real tasks. In fact, excessive perturbations may destroy the discriminative content of an example, while deficient perturbations may fail to provide helpful information for improving the robustness. Motivated by this observation, we propose to adaptively adjust the perturbation levels for each example in the training process. Specifically, a novel active learning framework is proposed to allow the model interactively querying the correct perturbation level from human experts. By designing a cost-effective sampling strategy along with a new query type, the robustness can be significantly improved with a few queries. Both theoretical analysis and experimental studies validate the effectiveness of the proposed approach.},
	language = {en},
	number = {10},
	urldate = {2022-10-16},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ning, Kun-Peng and Tao, Lue and Chen, Songcan and Huang, Sheng-Jun},
	month = may,
	year = {2021},
	note = {Number: 10},
	keywords = {Adversarial Learning \& Robustness},
	pages = {9161--9169},
}

@misc{yu_strength-adaptive_2022,
	title = {Strength-{Adaptive} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2210.01288},
	doi = {10.48550/arXiv.2210.01288},
	abstract = {Adversarial training (AT) is proved to reliably improve network's robustness against adversarial data. However, current AT with a pre-specified perturbation budget has limitations in learning a robust network. Firstly, applying a pre-specified perturbation budget on networks of various model capacities will yield divergent degree of robustness disparity between natural and robust accuracies, which deviates from robust network's desideratum. Secondly, the attack strength of adversarial training data constrained by the pre-specified perturbation budget fails to upgrade as the growth of network robustness, which leads to robust overfitting and further degrades the adversarial robustness. To overcome these limitations, we propose {\textbackslash}emph\{Strength-Adaptive Adversarial Training\} (SAAT). Specifically, the adversary employs an adversarial loss constraint to generate adversarial training data. Under this constraint, the perturbation budget will be adaptively adjusted according to the training state of adversarial data, which can effectively avoid robust overfitting. Besides, SAAT explicitly constrains the attack strength of training data through the adversarial loss, which manipulates model capacity scheduling during training, and thereby can flexibly control the degree of robustness disparity and adjust the tradeoff between natural accuracy and robustness. Extensive experiments show that our proposal boosts the robustness of adversarial training.},
	urldate = {2022-10-15},
	publisher = {arXiv},
	author = {Yu, Chaojian and Zhou, Dawei and Shen, Li and Yu, Jun and Han, Bo and Gong, Mingming and Wang, Nannan and Liu, Tongliang},
	month = oct,
	year = {2022},
	note = {arXiv:2210.01288 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{yang_one_nodate,
	title = {One {Size} {Does} {NOT} {Fit} {All}: {Data}-{Adaptive} {Adversarial} {Training}},
	abstract = {Adversarial robustness is critical for deep learning models to defend against adversarial attacks. Although adversarial training is considered to be one of the most effective ways to improve the model’s adversarial robustness, it usually yields models with lower natural accuracy. In this paper, we argue that, for the attackable examples, traditional adversarial training which utilizes a fixed size perturbation ball can create adversarial examples that deviate far away from the original class towards the target class. Thus, the model’s performance on the natural target class will drop drastically, which leads to the decline of natural accuracy. To this end, we propose the Data-Adaptive Adversarial Training (DAAT) which adaptively adjusts the perturbation ball to a proper size for each of the natural examples with the help of a natural trained calibration network. Besides, a dynamic training strategy empowers the DAAT models with impressive robustness while retaining remarkable natural accuracy. Based on a toy example, we theoretically prove the recession of the natural accuracy caused by adversarial training and show how the data-adaptive perturbation size helps the model resist it. Finally, empirical experiments on benchmark datasets demonstrate the significant improvement of DAAT models on natural accuracy compared with strong baselines.},
	language = {en},
	author = {Yang, Shuo and Xu, Chang},
	pages = {16},
}

@inproceedings{sarkar_adversarial_2021,
	title = {Adversarial {Robustness} without {Adversarial} {Training}: {A} {Teacher}-{Guided} {Curriculum} {Learning} {Approach}},
	shorttitle = {Adversarial {Robustness} without {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=MqCzSKCQ1QB},
	abstract = {Current SOTA adversarially robust models are mostly based on adversarial training (AT) and differ only by some regularizers either at inner maximization or outer minimization steps. Being repetitive in nature during the inner maximization step, they take a huge time to train. We propose a non-iterative method that enforces the following ideas during training. Attribution maps are more aligned to the actual object in the image for adversarially robust models compared to naturally trained models. Also, the allowed set of pixels to perturb an image (that changes model decision) should be restricted to the object pixels only, which reduces the attack strength by limiting the attack space. Our method achieves significant performance gains with a little extra effort (10-20\%) over existing AT models and outperforms all other methods in terms of adversarial as well as natural accuracy. We have performed extensive experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and reported results against many popular strong adversarial attacks to prove the effectiveness of our method.},
	language = {en},
	urldate = {2022-10-15},
	author = {Sarkar, Anindya and Sarkar, Anirban and Gali, Sowrya and Balasubramanian, Vineeth N.},
	month = oct,
	year = {2021},
}

@misc{gowal_alternative_2019,
	title = {An {Alternative} {Surrogate} {Loss} for {PGD}-based {Adversarial} {Testing}},
	url = {http://arxiv.org/abs/1910.09338},
	doi = {10.48550/arXiv.1910.09338},
	abstract = {Adversarial testing methods based on Projected Gradient Descent (PGD) are widely used for searching norm-bounded perturbations that cause the inputs of neural networks to be misclassified. This paper takes a deeper look at these methods and explains the effect of different hyperparameters (i.e., optimizer, step size and surrogate loss). We introduce the concept of MultiTargeted testing, which makes clever use of alternative surrogate losses, and explain when and how MultiTargeted is guaranteed to find optimal perturbations. Finally, we demonstrate that MultiTargeted outperforms more sophisticated methods and often requires less iterative steps than other variants of PGD found in the literature. Notably, MultiTargeted ranks first on MadryLab's white-box MNIST and CIFAR-10 leaderboards, reducing the accuracy of their MNIST model to 88.36\% (with \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations of \${\textbackslash}epsilon = 0.3\$) and the accuracy of their CIFAR-10 model to 44.03\% (at \${\textbackslash}epsilon = 8/255\$). MultiTargeted also ranks first on the TRADES leaderboard reducing the accuracy of their CIFAR-10 model to 53.07\% (with \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations of \${\textbackslash}epsilon = 0.031\$).},
	urldate = {2022-10-15},
	publisher = {arXiv},
	author = {Gowal, Sven and Uesato, Jonathan and Qin, Chongli and Huang, Po-Sen and Mann, Timothy and Kohli, Pushmeet},
	month = oct,
	year = {2019},
	note = {arXiv:1910.09338 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{tashiro_diversity_2020,
	title = {Diversity can be {Transferred}: {Output} {Diversification} for {White}- and {Black}-box {Attacks}},
	volume = {33},
	shorttitle = {Diversity can be {Transferred}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/30da227c6b5b9e2482b6b221c711edfd-Abstract.html},
	abstract = {Adversarial attacks often involve random perturbations of the inputs drawn from uniform or Gaussian distributions, e.g. to initialize optimization-based white-box attacks or generate update directions in black-box attacks. These simple perturbations, however, could be sub-optimal as they are agnostic to the model being attacked. To improve the efficiency of these attacks, we propose Output Diversified Sampling (ODS), a novel sampling strategy that attempts to maximize diversity in the target model's outputs among the generated samples. While ODS is a gradient-based strategy, the diversity offered by ODS is transferable and can be helpful for both white-box and black-box attacks via surrogate models. Empirically, we demonstrate that ODS significantly improves the performance of existing white-box and black-box attacks. In particular, ODS reduces the number of queries needed for state-of-the-art black-box attacks on ImageNet by a factor of two.},
	urldate = {2022-10-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tashiro, Yusuke and Song, Yang and Ermon, Stefano},
	year = {2020},
	pages = {4536--4548},
}

@misc{schwinn_exploring_2021,
	title = {Exploring {Misclassifications} of {Robust} {Neural} {Networks} to {Enhance} {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/2105.10304},
	doi = {10.48550/arXiv.2105.10304},
	abstract = {Progress in making neural networks more robust against adversarial attacks is mostly marginal, despite the great efforts of the research community. Moreover, the robustness evaluation is often imprecise, making it difficult to identify promising approaches. We analyze the classification decisions of 19 different state-of-the-art neural networks trained to be robust against adversarial attacks. Our findings suggest that current untargeted adversarial attacks induce misclassification towards only a limited amount of different classes. Additionally, we observe that both over- and under-confidence in model predictions result in an inaccurate assessment of model robustness. Based on these observations, we propose a novel loss function for adversarial attacks that consistently improves attack success rate compared to prior loss functions for 19 out of 19 analyzed models.},
	urldate = {2022-10-14},
	publisher = {arXiv},
	author = {Schwinn, Leo and Raab, René and Nguyen, An and Zanca, Dario and Eskofier, Bjoern},
	month = may,
	year = {2021},
	note = {arXiv:2105.10304 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{croce_provable_2019,
	title = {Provable {Robustness} of {ReLU} networks via {Maximization} of {Linear} {Regions}},
	url = {https://proceedings.mlr.press/v89/croce19a.html},
	abstract = {It has been shown that neural network classifiers are not robust. This raises concerns about their usage in safety-critical systems. We propose in this paper a regularization scheme for ReLU networks which provably improves the robustness of the classifier by maximizing the linear regions of the classifier as well as the distance to the decision boundary. Using our regularization we can even find the minimal adversarial perturbation for a certain fraction of test points for large networks. In the experiments we show that our approach improves upon pure adversarial training both in terms of lower and upper bounds on the robustness and is comparable or better than the state of the art in terms of test error and robustness.},
	language = {en},
	urldate = {2022-10-14},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Croce, Francesco and Andriushchenko, Maksym and Hein, Matthias},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2057--2066},
}

@article{jiang_imbalanced_2021,
	title = {Imbalanced {Gradients}: {A} {New} {Cause} of {Overestimated} {Adversarial} {Robustness}},
	shorttitle = {Imbalanced {Gradients}},
	url = {https://openreview.net/forum?id=8SP2-AiWttb},
	abstract = {Evaluating the robustness of a defense model is a challenging task in adversarial robustness research. Obfuscated gradients, a type of gradient masking, have previously been found to exist in many defense methods and cause a false signal of robustness. In this paper, we identify a more subtle situation called {\textbackslash}emph\{Imbalanced Gradients\} that can also cause overestimated adversarial robustness. The phenomenon of imbalanced gradients occurs when the gradient of one term of the margin loss dominates and pushes the attack towards to a suboptimal direction. To exploit imbalanced gradients, we formulate a {\textbackslash}emph\{Margin Decomposition (MD)\} attack that decomposes a margin loss into individual terms and then explores the attackability of these terms separately via a two-stage process. We examine 12 state-of-the-art defense models, and find that models exploiting label smoothing easily cause imbalanced gradients, and on which our MD attacks can decrease their PGD robustness (evaluated by PGD attack) by over 23{\textbackslash}\%. For 6 out of the 12 defenses, our attack can reduce their PGD robustness by at least 9{\textbackslash}\%. The results suggest that imbalanced gradients need to be carefully addressed for more reliable adversarial robustness.},
	language = {en},
	urldate = {2022-10-14},
	author = {Jiang, Linxi and Ma, Xingjun and Weng, Zejia and Bailey, James and Jiang, Yu-Gang},
	month = mar,
	year = {2021},
}

@article{xu_towards_2021,
	title = {Towards evaluating the robustness of deep diagnostic models by adversarial attack},
	volume = {69},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521000232},
	doi = {10.1016/j.media.2021.101977},
	abstract = {Deep learning models (with neural networks) have been widely used in challenging tasks such as computer-aided disease diagnosis based on medical images. Recent studies have shown deep diagnostic models may not be robust in the inference process and may pose severe security concerns in clinical practice. Among all the factors that make the model not robust, the most serious one is adversarial examples. The so-called “adversarial example” is a well-designed perturbation that is not easily perceived by humans but results in a false output of deep diagnostic models with high confidence. In this paper, we evaluate the robustness of deep diagnostic models by adversarial attack. Specifically, we have performed two types of adversarial attacks to three deep diagnostic models in both single-label and multi-label classification tasks, and found that these models are not reliable when attacked by adversarial example. We have further explored how adversarial examples attack the models, by analyzing their quantitative classification results, intermediate features, discriminability of features and correlation of estimated labels for both original/clean images and those adversarial ones. We have also designed two new defense methods to handle adversarial examples in deep diagnostic models, i.e., Multi-Perturbations Adversarial Training (MPAdvT) and Misclassification-Aware Adversarial Training (MAAdvT). The experimental results have shown that the use of defense methods can significantly improve the robustness of deep diagnostic models against adversarial attacks.},
	language = {en},
	urldate = {2022-10-12},
	journal = {Medical Image Analysis},
	author = {Xu, Mengting and Zhang, Tao and Li, Zhongnian and Liu, Mingxia and Zhang, Daoqiang},
	month = apr,
	year = {2021},
	keywords = {Adversarial attack, Deep diagnostic models, Defense, Robustness},
	pages = {101977},
}

@article{shi_robust_2022,
	title = {Robust convolutional neural networks against adversarial attacks on medical images},
	volume = {132},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320322004046},
	doi = {10.1016/j.patcog.2022.108923},
	abstract = {Convolutional neural networks (CNNs) have been widely applied to medical images. However, medical images are vulnerable to adversarial attacks by perturbations that are undetectable to human experts. This poses significant security risks and challenges to CNN-based applications in clinic practice. In this work, we quantify the scale of adversarial perturbation imperceptible to clinical practitioners and investigate the cause of the vulnerability in CNNs. Specifically, we discover that noise (i.e., irrelevant or corrupted discriminative information) in medical images might be a key contributor to performance deterioration of CNNs against adversarial perturbations, as noisy features are learned unconsciously by CNNs in feature representations and magnified by adversarial perturbations. In response, we propose a novel defense method by embedding sparsity denoising operators in CNNs for improved robustness. Tested with various state-of-the-art attacking methods on two distinct medical image modalities, we demonstrate that the proposed method can successfully defend against those unnoticeable adversarial attacks by retaining as much as over 90\% of its original performance. We believe our findings are critical for improving and deploying CNN-based medical applications in real-world scenarios.},
	language = {en},
	urldate = {2022-10-12},
	journal = {Pattern Recognition},
	author = {Shi, Xiaoshuang and Peng, Yifan and Chen, Qingyu and Keenan, Tiarnan and Thavikulwat, Alisa T. and Lee, Sungwon and Tang, Yuxing and Chew, Emily Y. and Summers, Ronald M. and Lu, Zhiyong},
	month = dec,
	year = {2022},
	keywords = {Adversarial examples, CNNs, Sparsity denoising},
	pages = {108923},
}

@article{ma_understanding_2021,
	title = {Understanding adversarial attacks on deep learning based medical image analysis systems},
	volume = {110},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320320301357},
	doi = {10.1016/j.patcog.2020.107332},
	abstract = {Deep neural networks (DNNs) have become popular for medical image analysis tasks like cancer diagnosis and lesion detection. However, a recent study demonstrates that medical deep learning systems can be compromised by carefully-engineered adversarial examples/attacks with small imperceptible perturbations. This raises safety concerns about the deployment of these systems in clinical settings. In this paper, we provide a deeper understanding of adversarial examples in the context of medical images. We find that medical DNN models can be more vulnerable to adversarial attacks compared to models for natural images, according to two different viewpoints. Surprisingly, we also find that medical adversarial attacks can be easily detected, i.e., simple detectors can achieve over 98\% detection AUC against state-of-the-art attacks, due to fundamental feature differences compared to normal examples. We believe these findings may be a useful basis to approach the design of more explainable and secure medical deep learning systems.},
	language = {en},
	urldate = {2022-10-12},
	journal = {Pattern Recognition},
	author = {Ma, Xingjun and Niu, Yuhao and Gu, Lin and Wang, Yisen and Zhao, Yitian and Bailey, James and Lu, Feng},
	month = feb,
	year = {2021},
	keywords = {Adversarial attack, Adversarial example detection, Deep learning, Medical image analysis},
	pages = {107332},
}

@article{hirano_universal_2021,
	title = {Universal adversarial attacks on deep neural networks for medical image classification},
	volume = {21},
	issn = {1471-2342},
	url = {https://doi.org/10.1186/s12880-020-00530-y},
	doi = {10.1186/s12880-020-00530-y},
	abstract = {Deep neural networks (DNNs) are widely investigated in medical image classification to achieve automated support for clinical diagnosis. It is necessary to evaluate the robustness of medical DNN tasks against adversarial attacks, as high-stake decision-making will be made based on the diagnosis. Several previous studies have considered simple adversarial attacks. However, the vulnerability of DNNs to more realistic and higher risk attacks, such as universal adversarial perturbation (UAP), which is a single perturbation that can induce DNN failure in most classification tasks has not been evaluated yet.},
	language = {en},
	number = {1},
	urldate = {2022-10-12},
	journal = {BMC Medical Imaging},
	author = {Hirano, Hokuto and Minagi, Akinori and Takemoto, Kazuhiro},
	month = jan,
	year = {2021},
	keywords = {Adversarial attacks, Deep neural networks, Medical imaging, Security and privacy},
	pages = {9},
}

@article{apostolidis_survey_2021,
	title = {A {Survey} on {Adversarial} {Deep} {Learning} {Robustness} in {Medical} {Image} {Analysis}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/10/17/2132},
	doi = {10.3390/electronics10172132},
	abstract = {In the past years, deep neural networks (DNN) have become popular in many disciplines such as computer vision (CV), natural language processing (NLP), etc. The evolution of hardware has helped researchers to develop many powerful Deep Learning (DL) models to face numerous challenging problems. One of the most important challenges in the CV area is Medical Image Analysis in which DL models process medical images—such as magnetic resonance imaging (MRI), X-ray, computed tomography (CT), etc.—using convolutional neural networks (CNN) for diagnosis or detection of several diseases. The proper function of these models can significantly upgrade the health systems. However, recent studies have shown that CNN models are vulnerable under adversarial attacks with imperceptible perturbations. In this paper, we summarize existing methods for adversarial attacks, detections and defenses on medical imaging. Finally, we show that many attacks, which are undetectable by the human eye, can degrade the performance of the models, significantly. Nevertheless, some effective defense and attack detection methods keep the models safe to an extent. We end with a discussion on the current state-of-the-art and future challenges.},
	language = {en},
	number = {17},
	urldate = {2022-10-12},
	journal = {Electronics},
	author = {Apostolidis, Kyriakos D. and Papakostas, George A.},
	month = jan,
	year = {2021},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adversarial attack, computer vision, convolutional neural networks, deep learning, medical image analysis},
	pages = {2132},
}

@article{finlayson_adversarial_2019,
	title = {Adversarial attacks on medical machine learning},
	volume = {363},
	url = {https://www.science.org/doi/10.1126/science.aaw4399},
	doi = {10.1126/science.aaw4399},
	number = {6433},
	urldate = {2022-10-11},
	journal = {Science},
	author = {Finlayson, Samuel G. and Bowers, John D. and Ito, Joichi and Zittrain, Jonathan L. and Beam, Andrew L. and Kohane, Isaac S.},
	month = mar,
	year = {2019},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1287--1289},
}

@article{bortsova_adversarial_2021,
	title = {Adversarial attack vulnerability of medical image analysis systems: {Unexplored} factors},
	volume = {73},
	issn = {1361-8415},
	shorttitle = {Adversarial attack vulnerability of medical image analysis systems},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521001870},
	doi = {10.1016/j.media.2021.102141},
	abstract = {Adversarial attacks are considered a potentially serious security threat for machine learning systems. Medical image analysis (MedIA) systems have recently been argued to be vulnerable to adversarial attacks due to strong financial incentives and the associated technological infrastructure. In this paper, we study previously unexplored factors affecting adversarial attack vulnerability of deep learning MedIA systems in three medical domains: ophthalmology, radiology, and pathology. We focus on adversarial black-box settings, in which the attacker does not have full access to the target model and usually uses another model, commonly referred to as surrogate model, to craft adversarial examples that are then transferred to the target model. We consider this to be the most realistic scenario for MedIA systems. Firstly, we study the effect of weight initialization (pre-training on ImageNet or random initialization) on the transferability of adversarial attacks from the surrogate model to the target model, i.e., how effective attacks crafted using the surrogate model are on the target model. Secondly, we study the influence of differences in development (training and validation) data between target and surrogate models. We further study the interaction of weight initialization and data differences with differences in model architecture. All experiments were done with a perturbation degree tuned to ensure maximal transferability at minimal visual perceptibility of the attacks. Our experiments show that pre-training may dramatically increase the transferability of adversarial examples, even when the target and surrogate’s architectures are different: the larger the performance gain using pre-training, the larger the transferability. Differences in the development data between target and surrogate models considerably decrease the performance of the attack; this decrease is further amplified by difference in the model architecture. We believe these factors should be considered when developing security-critical MedIA systems planned to be deployed in clinical practice. We recommend avoiding using only standard components, such as pre-trained architectures and publicly available datasets, as well as disclosure of design specifications, in addition to using adversarial defense methods. When evaluating the vulnerability of MedIA systems to adversarial attacks, various attack scenarios and target-surrogate differences should be simulated to achieve realistic robustness estimates. The code and all trained models used in our experiments are publicly available.3},
	language = {en},
	urldate = {2022-10-11},
	journal = {Medical Image Analysis},
	author = {Bortsova, Gerda and González-Gonzalo, Cristina and Wetstein, Suzanne C. and Dubost, Florian and Katramados, Ioannis and Hogeweg, Laurens and Liefers, Bart and van Ginneken, Bram and Pluim, Josien P. W. and Veta, Mitko and Sánchez, Clara I. and de Bruijne, Marleen},
	month = oct,
	year = {2021},
	keywords = {Adversarial attacks, Cybersecurity, Deep learning, Medical imaging},
	pages = {102141},
}

@inproceedings{huang_exploring_2022,
	title = {Exploring {Architectural} {Ingredients} of {Adversarially} {Robust} {Deep} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=OdklztJBBYH},
	abstract = {We provide both theoretical analysis and experimental evidence show that larger higher model capacity does not necessarily help adversarial robustness.},
	language = {en},
	urldate = {2022-10-11},
	author = {Huang, Hanxun and Wang, Yisen and Erfani, Sarah Monazam and Gu, Quanquan and Bailey, James and Ma, Xingjun},
	month = jan,
	year = {2022},
}

@article{zhang_adversarial_2019,
	title = {Adversarial {Interpolation} {Training}: {A} {Simple} {Approach} for {Improving} {Model} {Robustness}},
	shorttitle = {Adversarial {Interpolation} {Training}},
	url = {https://openreview.net/forum?id=Syejj0NYvr},
	abstract = {adversarial interpolation training: a simple, intuitive and effective approach for improving model robustness},
	language = {en},
	urldate = {2022-10-09},
	author = {Zhang, Haichao and Xu, Wei},
	month = dec,
	year = {2019},
}

@article{perrault-archambault_mixup_2019,
	title = {{MixUp} as {Directional} {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=SkgjKR4YwH},
	abstract = {We present a novel interpretation of MixUp as belonging to a class highly analogous to adversarial training, and on this basis we introduce a simple generalization which outperforms MixUp},
	language = {en},
	urldate = {2022-10-09},
	author = {Perrault-Archambault, Guillaume and Mao, Yongyi and Guo, Hongyu and Zhang, Richong},
	month = dec,
	year = {2019},
}

@inproceedings{song_robust_2020,
	title = {Robust {Local} {Features} for {Improving} the {Generalization} of {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=H1lZJpVFvr},
	abstract = {We propose a new stream of adversarial training approach called Robust Local Features for Adversarial Training (RLFAT) that significantly improves both the adversarially robust generalization and...},
	language = {en},
	urldate = {2022-10-09},
	author = {Song, Chuanbiao and He, Kun and Lin, Jiadong and Wang, Liwei and Hopcroft, John E.},
	month = mar,
	year = {2020},
}

@inproceedings{muller_when_2019,
	title = {When does label smoothing help?},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/f1748d6b0fd9d439f71450117eba2725-Abstract.html},
	abstract = {The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective.  To explain these observations, we visualize how label smoothing changes the  representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.},
	urldate = {2022-10-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Müller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
	year = {2019},
}

@inproceedings{li_what_2022,
	title = {What {Happens} after {SGD} {Reaches} {Zero} {Loss}? --{A} {Mathematical} {Framework}},
	shorttitle = {What {Happens} after {SGD} {Reaches} {Zero} {Loss}?},
	url = {https://openreview.net/forum?id=siCt4xZn5Ve},
	abstract = {Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss...},
	language = {en},
	urldate = {2022-10-03},
	author = {Li, Zhiyuan and Wang, Tianhao and Arora, Sanjeev},
	month = mar,
	year = {2022},
}

@inproceedings{kong_resolving_2022,
	title = {Resolving {Training} {Biases} via {Influence}-based {Data} {Relabeling}},
	url = {https://openreview.net/forum?id=EskfH0bwNVn},
	abstract = {The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that...},
	language = {en},
	urldate = {2022-10-03},
	author = {Kong, Shuming and Shen, Yanyan and Huang, Linpeng},
	month = mar,
	year = {2022},
}

@inproceedings{zhao_comparing_2022,
	title = {Comparing {Distributions} by {Measuring} {Differences} that {Affect} {Decision} {Making}},
	url = {https://openreview.net/forum?id=KB5onONJIAU},
	abstract = {Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a...},
	language = {en},
	urldate = {2022-10-03},
	author = {Zhao, Shengjia and Sinha, Abhishek and He, Yutong and Perreault, Aidan and Song, Jiaming and Ermon, Stefano},
	month = mar,
	year = {2022},
}

@inproceedings{singla_improved_2022,
	title = {Improved deterministic l2 robustness on {CIFAR}-10 and {CIFAR}-100},
	url = {https://openreview.net/forum?id=tD7eCtaSkR},
	abstract = {Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the \$l\_\{2\}\$ norm is useful for provable adversarial robustness, interpretable gradients and stable training....},
	language = {en},
	urldate = {2022-10-03},
	author = {Singla, Sahil and Singla, Surbhi and Feizi, Soheil},
	month = mar,
	year = {2022},
}

@inproceedings{zhang_boosting_2022,
	title = {Boosting the {Certified} {Robustness} of {L}-infinity {Distance} {Nets}},
	url = {https://openreview.net/forum?id=Q76Y7wkiji},
	abstract = {Recently, Zhang et al. (2021) developed a new neural network architecture based on \${\textbackslash}ell\_{\textbackslash}infty\$-distance functions, which naturally possesses certified \${\textbackslash}ell\_{\textbackslash}infty\$ robustness by its...},
	language = {en},
	urldate = {2022-10-03},
	author = {Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
	month = mar,
	year = {2022},
}

@inproceedings{harrington_finding_2022,
	title = {Finding {Biological} {Plausibility} for {Adversarially} {Robust} {Features} via {Metameric} {Tasks}},
	url = {https://openreview.net/forum?id=yeP_zx9vqNm},
	abstract = {Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such...},
	language = {en},
	urldate = {2022-10-03},
	author = {Harrington, Anne and Deza, Arturo},
	month = feb,
	year = {2022},
}

@inproceedings{fiez_minimax_2022,
	title = {Minimax {Optimization} with {Smooth} {Algorithmic} {Adversaries}},
	url = {https://openreview.net/forum?id=UdxJ2fJx7N0},
	abstract = {This paper considers minimax optimization \${\textbackslash}min\_x {\textbackslash}max\_y f(x, y)\$ in the challenging setting where \$f\$ can be both nonconvex in \$x\$ and nonconcave in \$y\$. Though such optimization problems arise in...},
	language = {en},
	urldate = {2022-10-03},
	author = {Fiez, Tanner and Jin, Chi and Netrapalli, Praneeth and Ratliff, Lillian J.},
	month = mar,
	year = {2022},
}

@misc{tishby_information_2000,
	title = {The information bottleneck method},
	url = {http://arxiv.org/abs/physics/0004057},
	doi = {10.48550/arXiv.physics/0004057},
	abstract = {We define the relevant information in a signal \$x{\textbackslash}in X\$ as being the information that this signal provides about another signal \$y{\textbackslash}in {\textbackslash}Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \${\textbackslash}X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \${\textbackslash}X\$ that preserves the maximum information about \${\textbackslash}Y\$. That is, we squeeze the information that \${\textbackslash}X\$ provides about \${\textbackslash}Y\$ through a `bottleneck' formed by a limited set of codewords \${\textbackslash}tX\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,{\textbackslash}x)\$ emerges from the joint statistics of \${\textbackslash}X\$ and \${\textbackslash}Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X {\textbackslash}to {\textbackslash}tX\$ and \${\textbackslash}tX {\textbackslash}to {\textbackslash}Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	month = apr,
	year = {2000},
	note = {arXiv:physics/0004057},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Data Analysis, Statistics and Probability},
}

@article{xu_infoat_2022,
	title = {{InfoAT}: {Improving} {Adversarial} {Training} {Using} the {Information} {Bottleneck} {Principle}},
	issn = {2162-2388},
	shorttitle = {{InfoAT}},
	doi = {10.1109/TNNLS.2022.3183095},
	abstract = {Adversarial training (AT) has shown excellent high performance in defending against adversarial examples. Recent studies demonstrate that examples are not equally important to the final robustness of models during AT, that is, the so-called hard examples that can be attacked easily exhibit more influence than robust examples on the final robustness. Therefore, guaranteeing the robustness of hard examples is crucial for improving the final robustness of the model. However, defining effective heuristics to search for hard examples is still difficult. In this article, inspired by the information bottleneck (IB) principle, we uncover that an example with high mutual information of the input and its associated latent representation is more likely to be attacked. Based on this observation, we propose a novel and effective adversarial training method (InfoAT). InfoAT is encouraged to find examples with high mutual information and exploit them efficiently to improve the final robustness of models. Experimental results show that InfoAT achieves the best robustness among different datasets and models in comparison with several state-of-the-art methods.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Xu, Mengting and Zhang, Tao and Li, Zhongnian and Zhang, Daoqiang},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Adversarial training (AT), Linear programming, Mutual information, Neural networks, Perturbation methods, Robustness, Standards, Training, information bottleneck (IB), mutual information, robustness},
	pages = {1--10},
}

@inproceedings{zhang_attacks_2020,
	title = {Attacks {Which} {Do} {Not} {Kill} {Training} {Make} {Adversarial} {Learning} {Stronger}},
	url = {https://proceedings.mlr.press/v119/zhang20z.html},
	abstract = {Adversarial training based on the minimax formulation is necessary for obtaining adversarial robustness of trained models. However, it is conservative or even pessimistic so that it sometimes hurts the natural generalization. In this paper, we raise a fundamental question\{—\}do we have to trade off natural generalization for adversarial robustness? We argue that adversarial training is to employ confident adversarial data for updating the current model. We propose a novel formulation of friendly adversarial training (FAT): rather than employing most adversarial data maximizing the loss, we search for least adversarial data (i.e., friendly adversarial data) minimizing the loss, among the adversarial data that are confidently misclassified. Our novel formulation is easy to implement by just stopping the most adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD. Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively\{—\}adversarial robustness can indeed be achieved without compromising the natural generalization.},
	language = {en},
	urldate = {2022-10-02},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Jingfeng and Xu, Xilie and Han, Bo and Niu, Gang and Cui, Lizhen and Sugiyama, Masashi and Kankanhalli, Mohan},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {11278--11287},
}

@inproceedings{zhou_enhancing_2022,
	title = {Enhancing {Adversarial} {Robustness} for {Deep} {Metric} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Enhancing_Adversarial_Robustness_for_Deep_Metric_Learning_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-10-02},
	author = {Zhou, Mo and Patel, Vishal M.},
	year = {2022},
	pages = {15325--15334},
}

@misc{yang_adaptive_2022,
	title = {Adaptive {Regularization} for {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2206.03353},
	doi = {10.48550/arXiv.2206.03353},
	abstract = {Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to use a data-adaptive regularization for robustifying a prediction model. We apply more regularization to data which are more vulnerable to adversarial attacks and vice versa. Even though the idea of data-adaptive regularization is not new, our data-adaptive regularization has a firm theoretical base of reducing an upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on clean samples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Yang, Dongyoon and Kong, Insung and Kim, Yongdai},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03353 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{atienza_improving_2022,
	title = {Improving {Model} {Generalization} by {Agreement} of {Learned} {Representations} {From} {Data} {Augmentation}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Atienza_Improving_Model_Generalization_by_Agreement_of_Learned_Representations_From_Data_WACV_2022_paper.html},
	language = {en},
	urldate = {2022-10-02},
	author = {Atienza, Rowel},
	year = {2022},
	pages = {372--381},
}

@misc{modas_prime_2022-1,
	title = {{PRIME}: {A} few primitives can boost robustness to common corruptions},
	shorttitle = {{PRIME}},
	url = {http://arxiv.org/abs/2112.13547},
	doi = {10.48550/arXiv.2112.13547},
	abstract = {Despite their impressive performance on image classification tasks, deep networks have a hard time generalizing to unforeseen corruptions of their data. To fix this vulnerability, prior works have built complex data augmentation strategies, combining multiple methods to enrich the training data. However, introducing intricate design choices or heuristics makes it hard to understand which elements of these methods are indeed crucial for improving robustness. In this work, we take a step back and follow a principled approach to achieve robustness to common corruptions. We propose PRIME, a general data augmentation scheme that relies on simple yet rich families of max-entropy image transformations. PRIME outperforms the prior art in terms of corruption robustness, while its simplicity and plug-and-play nature enable combination with other methods to further boost their robustness. We analyze PRIME to shed light on the importance of the mixing strategy on synthesizing corrupted images, and to reveal the robustness-accuracy trade-offs arising in the context of common corruptions. Finally, we show that the computational efficiency of our method allows it to be easily used in both on-line and off-line data augmentation schemes.},
	urldate = {2022-10-02},
	publisher = {arXiv},
	author = {Modas, Apostolos and Rade, Rahul and Ortiz-Jiménez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	month = mar,
	year = {2022},
	note = {arXiv:2112.13547 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{cheng_cat_2022,
	address = {Vienna, Austria},
	title = {{CAT}: {Customized} {Adversarial} {Training} for {Improved} {Robustness}},
	isbn = {978-1-956792-00-3},
	shorttitle = {{CAT}},
	url = {https://www.ijcai.org/proceedings/2022/95},
	doi = {10.24963/ijcai.2022/95},
	abstract = {Adversarial training has become one of the most effective methods for improving robustness of neural networks. However, it often suffers from poor generalization on both clean and perturbed data. Current robust training method always use a uniformed perturbation strength for every samples to generate adversarial examples during model training for improving adversarial robustness. However, we show it would lead worse training and generalizaiton error and forcing the prediction to match one-hot label. In this paper, therefore, we propose a new algorithm, named Customized Adversarial Training (CAT), which adaptively customizes the perturbation level and the corresponding label for each training sample in adversarial training. We ﬁrst show theoretically the CAT scheme improves the generalization. Also, through extensive experiments, we show that the proposed algorithm achieves better clean and robust accuracy than previous adversarial training methods. The full version of this paper is available at https://arxiv.org/abs/2002.06789.},
	language = {en},
	urldate = {2022-10-01},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Cheng, Minhao and Lei, Qi and Chen, Pin-Yu and Dhillon, Inderjit and Hsieh, Cho-Jui},
	month = jul,
	year = {2022},
	pages = {673--679},
}

@inproceedings{wang_improving_2020,
	title = {Improving {Adversarial} {Robustness} {Requires} {Revisiting} {Misclassified} {Examples}},
	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by imperceptible perturbations. A range of defense techniques have been proposed to improve DNN robustness to adversarial examples, among which adversarial training has been demonstrated to be the most effective. Adversarial training is often formulated as a min-max optimization problem, with the inner maximization for generating adversarial examples. However, there exists a simple, yet easily overlooked fact that adversarial examples are only deﬁned on correctly classiﬁed (natural) examples, but inevitably, some (natural) examples will be misclassiﬁed during training. In this paper, we investigate the distinctive inﬂuence of misclassiﬁed and correctly classiﬁed examples on the ﬁnal robustness of adversarial training. Speciﬁcally, we ﬁnd that misclassiﬁed examples indeed have a signiﬁcant impact on the ﬁnal robustness. More surprisingly, we ﬁnd that different maximization techniques on misclassiﬁed examples may have a negligible inﬂuence on the ﬁnal robustness, while different minimization techniques are crucial. Motivated by the above discovery, we propose a new defense algorithm called Misclassiﬁcation Aware adveRsarial Training (MART), which explicitly differentiates the misclassiﬁed and correctly classiﬁed examples during the training. We also propose a semi-supervised extension of MART, which can leverage the unlabeled data to further improve the robustness. Experimental results show that MART and its variant could signiﬁcantly improve the state-of-the-art adversarial robustness.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wang, Yisen and Zou, Difan and Yi, Jinfeng and Bailey, James and Ma, Xingjun and Gu, Quanquan},
	year = {2020},
	pages = {14},
}

@inproceedings{ding_sensitivity_2019,
	title = {On the {Sensitivity} of {Adversarial} {Robustness} to {Input} {Data} {Distributions}},
	url = {https://openreview.net/forum?id=S1xNEhR9KX},
	abstract = {Robustness performance of PGD trained models are sensitive to semantics-preserving transformation of image datasets, which implies the trickiness of evaluation of robust learning algorithms in...},
	language = {en},
	urldate = {2021-09-12},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ding, Gavin Weiguang and Lui, Kry Yik Chau and Jin, Xiaomeng and Wang, Luyu and Huang, Ruitong},
	year = {2019},
}

@inproceedings{ding_mma_2020,
	title = {{MMA} {Training}: {Direct} {Input} {Space} {Margin} {Maximization} through {Adversarial} {Training}},
	shorttitle = {{MMA} {Training}},
	url = {https://openreview.net/forum?id=HkeryxBtPB},
	abstract = {We propose MMA training to directly maximize input space margin in order to improve adversarial robustness primarily by removing the requirement of specifying a fixed distortion bound.},
	language = {en},
	urldate = {2021-09-10},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ding, Gavin Weiguang and Sharma, Yash and Lui, Kry Yik Chau and Huang, Ruitong},
	year = {2020},
}

@inproceedings{yun_cutmix_2019,
	title = {{CutMix}: {Regularization} {Strategy} to {Train} {Strong} {Classifiers} {With} {Localizable} {Features}},
	shorttitle = {{CutMix}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.html},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
	year = {2019},
	pages = {6023--6032},
}

@inproceedings{singla_low_2021,
	title = {Low {Curvature} {Activations} {Reduce} {Overfitting} in {Adversarial} {Training}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Singla_Low_Curvature_Activations_Reduce_Overfitting_in_Adversarial_Training_ICCV_2021_paper.html},
	language = {en},
	urldate = {2021-11-17},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Singla, Vasu and Singla, Sahil and Feizi, Soheil and Jacobs, David},
	year = {2021},
	pages = {16423--16433},
}

@inproceedings{gowal_improving_2021,
	title = {Improving {Robustness} using {Generated} {Data}},
	abstract = {Recent work argues that robust training requires substantially larger datasets than those required for standard classiﬁcation. On CIFAR-10 and CIFAR-100, this translates into a sizable robust-accuracy gap between models trained solely on data from the original training set and those trained with additional data extracted from the “80 Million Tiny Images” dataset (80M-TI). In this paper, we explore how generative models trained solely on the original training set can be leveraged to artiﬁcially increase the size of the original training set and improve adversarial robustness to p norm-bounded perturbations. We identify the sufﬁcient conditions under which incorporating additional generated data can improve robustness, and demonstrate that it is possible to signiﬁcantly reduce the robust-accuracy gap to models trained with additional real data. Surprisingly, we show that even the addition of non-realistic random data (generated by Gaussian sampling) can improve robustness. We evaluate our approach on CIFAR-10, CIFAR-100, SVHN and TINYIMAGENET against ∞ and 2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements in robust accuracy compared to previous state-of-the-art methods. Against ∞ normbounded perturbations of size = 8/255, our models achieve 66.10\% and 33.49\% robust accuracy on CIFAR-10 and CIFAR-100, respectively (improving upon the state-of-the-art by +8.96\% and +3.29\%). Against 2 norm-bounded perturbations of size = 128/255, our model achieves 78.31\% on CIFAR-10 (+3.81\%). These results beat most prior works that use external data.},
	language = {en},
	booktitle = {Thirty-{Fifth} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Gowal, Sven and Rebufﬁ, Sylvestre-Alvise and Wiles, Olivia and Stimberg, Florian and Calian, Dan and Mann, Timothy},
	year = {2021},
	pages = {16},
}

@inproceedings{chen_robust_2021,
	title = {Robust {Overfitting} may be mitigated by properly learned smoothening},
	url = {https://openreview.net/forum?id=qZzy5urZw9},
	abstract = {A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training...},
	language = {en},
	urldate = {2021-02-23},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Chen, Tianlong and Zhang, Zhenyu and Liu, Sijia and Chang, Shiyu and Wang, Zhangyang},
	year = {2021},
}

@inproceedings{dong_exploring_2022,
	title = {Exploring {Memorization} in {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=7gE9V9GBZaI},
	abstract = {Deep learning models have a propensity for fitting the entire training set even with random labels, which requires memorization of every training sample. In this paper, we explore the memorization...},
	language = {en},
	urldate = {2022-04-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dong, Yinpeng and Xu, Ke and Yang, Xiao and Pang, Tianyu and Deng, Zhijie and Su, Hang and Zhu, Jun},
	year = {2022},
}

@inproceedings{gontijo-lopes_tradeoffs_2021,
	title = {Tradeoffs in {Data} {Augmentation}: {An} {Empirical} {Study}},
	shorttitle = {Tradeoffs in {Data} {Augmentation}},
	url = {https://openreview.net/forum?id=ZcKPWuhG6wy},
	abstract = {Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In...},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Gontijo-Lopes, Raphael and Smullin, Sylvia and Cubuk, Ekin Dogus and Dyer, Ethan},
	year = {2021},
}

@misc{wang_resmooth_2022,
	title = {{ReSmooth}: {Detecting} and {Utilizing} {OOD} {Samples} when {Training} with {Data} {Augmentation}},
	shorttitle = {{ReSmooth}},
	url = {http://arxiv.org/abs/2205.12606},
	doi = {10.48550/arXiv.2205.12606},
	abstract = {Data augmentation (DA) is a widely used technique for enhancing the training of deep neural networks. Recent DA techniques which achieve state-of-the-art performance always meet the need for diversity in augmented training samples. However, an augmentation strategy that has a high diversity usually introduces out-of-distribution (OOD) augmented samples and these samples consequently impair the performance. To alleviate this issue, we propose ReSmooth, a framework that firstly detects OOD samples in augmented samples and then leverages them. To be specific, we first use a Gaussian mixture model to fit the loss distribution of both the original and augmented samples and accordingly split these samples into in-distribution (ID) samples and OOD samples. Then we start a new training where ID and OOD samples are incorporated with different smooth labels. By treating ID samples and OOD samples unequally, we can make better use of the diverse augmented data. Further, we incorporate our ReSmooth framework with negative data augmentation strategies. By properly handling their intentionally created ODD samples, the classification performance of negative data augmentations is largely ameliorated. Experiments on several classification benchmarks show that ReSmooth can be easily extended to existing augmentation strategies (such as RandAugment, rotate, and jigsaw) and improve on them.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Wang, Chenyang and Jiang, Junjun and Zhou, Xiong and Liu, Xianming},
	month = may,
	year = {2022},
	note = {arXiv:2205.12606 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{addepalli_efficient_2022,
	title = {Efficient and {Effective} {Augmentation} {Strategy} for {Adversarial} {Training}},
	abstract = {The sample complexity of Adversarial training is known to be significantly higher than standard ERM training. Although complex augmentation techniques have led to large gains in standard training, they have not been successful with Adversarial Training. In this work, we propose Diverse Augmentation based Joint Adversarial Training (DAJAT) that uses a combination of simple and complex augmentations with separate batch normalization layers to handle the conflicting goals of enhancing the diversity of the training dataset, while being close to the test distribution. We further introduce a Jensen-Shannon divergence loss to encourage the joint learning of the diverse augmentations, thereby allowing simple augmentations to guide the learning of complex ones. Lastly, to improve the computational efficiency of the proposed method, we propose and utilize a two-step defense, Ascending Constraint Adversarial Training (ACAT) that uses an increasing epsilon schedule and weight-space smoothing to prevent gradient masking.},
	language = {en},
	author = {Addepalli, Sravanti and Jain, Samyak and Babu, R Venkatesh},
	year = {2022},
	pages = {4},
}

@inproceedings{xie_intriguing_2020,
	title = {Intriguing {Properties} of {Adversarial} {Training} at {Scale}},
	url = {https://openreview.net/forum?id=HyxJhCEFDS},
	abstract = {The first rigor diagnose of large-scale adversarial training on ImageNet},
	language = {en},
	urldate = {2022-09-26},
	author = {Xie, Cihang and Yuille, Alan},
	month = mar,
	year = {2020},
}

@inproceedings{wang_once-for-all_2020,
	title = {Once-for-{All} {Adversarial} {Training}: {In}-{Situ} {Tradeoff} between {Robustness} and {Accuracy} for {Free}},
	volume = {33},
	shorttitle = {Once-for-{All} {Adversarial} {Training}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/537d9b6c927223c796cac288cced29df-Abstract.html},
	abstract = {Adversarial training and its many variants substantially improve deep network robustness, yet at the cost of compromising standard accuracy. Moreover, the training process is heavy and hence it becomes impractical to thoroughly explore the trade-off between accuracy and robustness. This paper asks this new question: how to quickly calibrate a trained model in-situ, to examine the achievable trade-offs between its standard and robust accuracies, without (re-)training it many times? Our proposed framework, Once-for-all Adversarial Training (OAT), is built on an innovative model-conditional training framework, with a controlling hyper-parameter as the input. The trained model could be adjusted among different standard and robust accuracies “for free” at testing time. As an important knob, we exploit dual batch normalization to separate standard and adversarial feature statistics, so that they can be learned in one model without degrading performance. We further extend OAT to a Once-for-all Adversarial Training and Slimming (OATS) framework, that allows for the joint trade-off among accuracy, robustness and runtime efficiency. Experiments show that, without any re-training nor ensembling, OAT/OATS achieve similar or even superior performance compared to dedicatedly trained models at various configurations. Our codes and pretrained models are available at: https://github.com/VITA-Group/Once-for-All-Adversarial-Training.},
	urldate = {2022-09-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Haotao and Chen, Tianlong and Gui, Shupeng and Hu, TingKuei and Liu, Ji and Wang, Zhangyang},
	year = {2020},
	pages = {7449--7461},
}

@misc{merchant_does_2020,
	title = {Does {Data} {Augmentation} {Benefit} from {Split} {BatchNorms}},
	url = {http://arxiv.org/abs/2010.07810},
	doi = {10.48550/arXiv.2010.07810},
	abstract = {Data augmentation has emerged as a powerful technique for improving the performance of deep neural networks and led to state-of-the-art results in computer vision. However, state-of-the-art data augmentation strongly distorts training images, leading to a disparity between examples seen during training and inference. In this work, we explore a recently proposed training paradigm in order to correct for this disparity: using an auxiliary BatchNorm for the potentially out-of-distribution, strongly augmented images. Our experiments then focus on how to define the BatchNorm parameters that are used at evaluation. To eliminate the train-test disparity, we experiment with using the batch statistics defined by clean training images only, yet surprisingly find that this does not yield improvements in model performance. Instead, we investigate using BatchNorm parameters defined by weak augmentations and find that this method significantly improves the performance of common image classification benchmarks such as CIFAR-10, CIFAR-100, and ImageNet. We then explore a fundamental trade-off between accuracy and robustness coming from using different BatchNorm parameters, providing greater insight into the benefits of data augmentation on model performance.},
	urldate = {2022-09-26},
	publisher = {arXiv},
	author = {Merchant, Amil and Zoph, Barret and Cubuk, Ekin Dogus},
	month = oct,
	year = {2020},
	note = {arXiv:2010.07810 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{izmailov_averaging_2018,
	series = {34th {Conference} on {Uncertainty} in {Artificial} {Intelligence} 2018, {UAI} 2018},
	title = {Averaging weights leads to wider optima and better generalization},
	shorttitle = {Averaging weights leads to wider optima and better generalization},
	url = {http://www.scopus.com/inward/record.url?scp=85059432227&partnerID=8YFLogxK},
	abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensem-bling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and ShakeShake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
	urldate = {2022-07-25},
	booktitle = {34th {Conference} on {Uncertainty} in {Artificial} {Intelligence} 2018, {UAI} 2018},
	publisher = {Association For Uncertainty in Artificial Intelligence (AUAI)},
	author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
	editor = {Silva, Ricardo and Globerson, Amir and Globerson, Amir},
	year = {2018},
	pages = {876--885},
}

@inproceedings{yi_reweighting_2021,
	title = {Reweighting {Augmented} {Samples} by {Minimizing} the {Maximal} {Expected} {Loss}},
	url = {https://openreview.net/forum?id=9G5MIc-goqB},
	abstract = {Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without...},
	language = {en},
	urldate = {2021-12-19},
	author = {Yi, Mingyang and Hou, Lu and Shang, Lifeng and Jiang, Xin and Liu, Qun and Ma, Zhi-Ming},
	year = {2021},
}

@inproceedings{lamb_interpolated_2019,
	address = {New York, NY, USA},
	series = {{AISec}'19},
	title = {Interpolated {Adversarial} {Training}: {Achieving} {Robust} {Neural} {Networks} {Without} {Sacrificing} {Too} {Much} {Accuracy}},
	isbn = {978-1-4503-6833-9},
	shorttitle = {Interpolated {Adversarial} {Training}},
	url = {https://doi.org/10.1145/3338501.3357369},
	doi = {10.1145/3338501.3357369},
	abstract = {Adversarial robustness has become a central goal in deep learning, both in theory and in practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how achieving adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error (when there is no adversary) from 4.43\% to 12.32\%, whereas with our Interpolated adversarial training we retain adversarial robustness while achieving a standard test error of only 6.45\%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1\% to just 45.5\%.},
	urldate = {2022-09-20},
	booktitle = {Proceedings of the 12th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Lamb, Alex and Verma, Vikas and Kannala, Juho and Bengio, Yoshua},
	month = nov,
	year = {2019},
	keywords = {adversarial robustness, interpolation based training, manifold mixup, mixup, neural networks, standard test error},
	pages = {95--103},
}

@article{li_understanding_2022,
	title = {Understanding and {Combating} {Robust} {Overfitting} via {Input} {Loss} {Landscape} {Analysis} and {Regularization}},
	abstract = {Adversarial training is widely used to improve the robustness of deep neural networks to adversarial attack. However, adversarial training is prone to overfitting, and the cause is far from clear. This work sheds light on the mechanisms underlying overfitting through analyzing the loss landscape w.r.t. the input. We find that robust overfitting results from standard training, specifically the minimization of the clean loss, and can be mitigated by regularization of the loss gradients. Moreover, we find that robust overfitting turns severer during adversarial training partially because the gradient regularization effect of adversarial training becomes weaker due to the increase in the loss landscape’s curvature. To improve robust generalization, we propose a new regularizer to smooth the loss landscape by penalizing the weighted logits variation along the adversarial direction. Our method significantly mitigates robust overfitting and achieves the highest robustness and efficiency compared to similar previous methods.},
	language = {en},
	journal = {submitted},
	author = {Li, Lin and Spratling, Michael},
	month = aug,
	year = {2022},
	pages = {29},
}

@article{han_advancing_2021,
	title = {Advancing diagnostic performance and clinical usability of neural networks via adversarial training and dual batch normalization},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-24464-3},
	doi = {10.1038/s41467-021-24464-3},
	abstract = {Unmasking the decision making process of machine learning models is essential for implementing diagnostic support systems in clinical practice. Here, we demonstrate that adversarially trained models can significantly enhance the usability of pathology detection as compared to their standard counterparts. We let six experienced radiologists rate the interpretability of saliency maps in datasets of X-rays, computed tomography, and magnetic resonance imaging scans. Significant improvements are found for our adversarial models, which are further improved by the application of dual-batch normalization. Contrary to previous research on adversarially trained models, we find that accuracy of such models is equal to standard models, when sufficiently large datasets and dual batch norm training are used. To ensure transferability, we additionally validate our results on an external test set of 22,433 X-rays. These findings elucidate that different paths for adversarial and real images are needed during training to achieve state of the art results with superior clinical interpretability.},
	language = {en},
	number = {1},
	urldate = {2022-09-19},
	journal = {Nature Communications},
	author = {Han, Tianyu and Nebelung, Sven and Pedersoli, Federico and Zimmermann, Markus and Schulze-Hagen, Maximilian and Ho, Michael and Haarburger, Christoph and Kiessling, Fabian and Kuhl, Christiane and Schulz, Volkmar and Truhn, Daniel},
	month = jul,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Medical imaging, Predictive medicine},
	pages = {4315},
}

@inproceedings{wong_wasserstein_2019,
	title = {Wasserstein {Adversarial} {Examples} via {Projected} {Sinkhorn} {Iterations}},
	url = {https://proceedings.mlr.press/v97/wong19a.html},
	abstract = {A rapidly growing area of work has studied the existence of adversarial examples, datapoints which have been perturbed to fool a classifier, but the vast majority of these works have focused primarily on threat models defined by ℓpℓp{\textbackslash}ell\_p norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance. In the image classification setting, such distances measure the cost of moving pixel mass, which can naturally represent “standard” image manipulations such as scaling, rotation, translation, and distortion (and can potentially be applied to other settings as well). To generate Wasserstein adversarial examples, we develop a procedure for approximate projection onto the Wasserstein ball, based upon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3\% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10\% of the image mass 1 pixel), and we demonstrate that PGD-based adversarial training can improve this adversarial accuracy to 76\%. In total, this work opens up a new direction of study in adversarial robustness, more formally considering convex metrics that accurately capture the invariances that we typically believe should exist in classifiers, and code for all experiments in the paper is available at https://github.com/locuslab/projected\_sinkhorn.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wong, Eric and Schmidt, Frank and Kolter, Zico},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6808--6817},
}

@inproceedings{wu_stronger_2020,
	title = {Stronger and {Faster} {Wasserstein} {Adversarial} {Attacks}},
	url = {https://proceedings.mlr.press/v119/wu20d.html},
	abstract = {Deep models, while being extremely flexible and accurate, are surprisingly vulnerable to “small, imperceptible” perturbations known as adversarial attacks. While the majority of existing attacks focus on measuring perturbations under the ℓpℓp{\textbackslash}ell\_p metric, Wasserstein distance, which takes geometry in pixel space into account, has long been known to be a suitable metric for measuring image quality and has recently risen as a compelling alternative to the ℓpℓp{\textbackslash}ell\_p metric in adversarial attacks. However, constructing an effective attack under the Wasserstein metric is computationally much more challenging and calls for better optimization algorithms. We address this gap in two ways: (a) we develop an exact yet efficient projection operator to enable a stronger projected gradient attack; (b) we show that the Frank-Wolfe method equipped with a suitable linear minimization oracle works extremely fast under Wasserstein constraints. Our algorithms not only converge faster but also generate much stronger attacks. For instance, we decrease the accuracy of a residual network on CIFAR-10 to 3.43.43.4\% within a Wasserstein perturbation ball of radius 0.0050.0050.005, in contrast to 65.665.665.6\% using the previous Wasserstein attack based on an {\textbackslash}emph\{approximate\} projection operator. Furthermore, employing our stronger attacks in adversarial training significantly improves the robustness of adversarially trained models.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wu, Kaiwen and Wang, Allen and Yu, Yaoliang},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {10377--10387},
}

@inproceedings{nanda_measuring_2022,
	title = {Measuring {Representational} {Robustness} of {Neural} {Networks} {Through} {Shared} {Invariances}},
	url = {https://proceedings.mlr.press/v162/nanda22a.html},
	abstract = {A major challenge in studying robustness in deep learning is defining the set of “meaningless” perturbations to which a given Neural Network (NN) should be invariant. Most work on robustness implicitly uses a human as the reference model to define such perturbations. Our work offers a new view on robustness by using another reference NN to define the set of perturbations a given NN should be invariant to, thus generalizing the reliance on a reference “human NN” to any NN. This makes measuring robustness equivalent to measuring the extent to which two NNs share invariances. We propose a measure called {\textbackslash}stir, which faithfully captures the extent to which two NNs share invariances. {\textbackslash}stir re-purposes existing representation similarity measures to make them suitable for measuring shared invariances. Using our measure, we are able to gain insights about how shared invariances vary with changes in weight initialization, architecture, loss functions, and training dataset. Our implementation is available at: {\textbackslash}url\{https://github.com/nvedant07/STIR\}.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nanda, Vedant and Speicher, Till and Kolling, Camila and Dickerson, John P. and Gummadi, Krishna and Weller, Adrian},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {16368--16382},
}

@inproceedings{zhou_understanding_2022,
	title = {Understanding {The} {Robustness} in {Vision} {Transformers}},
	url = {https://proceedings.mlr.press/v162/zhou22m.html},
	abstract = {Recent studies show that Vision Transformers (ViTs) exhibit strong robustness against various corruptions. Although this property is partly attributed to the self-attention mechanism, there is still a lack of an explanatory framework towards a more systematic understanding. In this paper, we examine the role of self-attention in learning robust representations. Our study is motivated by the intriguing properties of self-attention in visual grouping which indicate that self-attention could promote improved mid-level representation and robustness. We thus propose a family of fully attentional networks (FANs) that incorporate self-attention in both token mixing and channel processing. We validate the design comprehensively on various hierarchical backbones. Our model with a DeiT architecture achieves a state-of-the-art 47.6\% mCE on ImageNet-C with 29M parameters. We also demonstrate significantly improved robustness in two downstream tasks: semantic segmentation and object detection},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhou, Daquan and Yu, Zhiding and Xie, Enze and Xiao, Chaowei and Anandkumar, Animashree and Feng, Jiashi and Alvarez, Jose M.},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {27378--27394},
}

@inproceedings{teti_lcanets_2022,
	title = {{LCANets}: {Lateral} {Competition} {Improves} {Robustness} {Against} {Corruption} and {Attack}},
	shorttitle = {{LCANets}},
	url = {https://proceedings.mlr.press/v162/teti22a.html},
	abstract = {Although Convolutional Neural Networks (CNNs) achieve high accuracy on image recognition tasks, they lack robustness against realistic corruptions and fail catastrophically when deliberately attacked. Previous CNNs with representations similar to primary visual cortex (V1) were more robust to adversarial attacks on images than current adversarial defense techniques, but they required training on large-scale neural recordings or handcrafting neuroscientific models. Motivated by evidence that neural activity in V1 is sparse, we develop a class of hybrid CNNs, called LCANets, which feature a frontend that performs sparse coding via local lateral competition. We demonstrate that LCANets achieve competitive clean accuracy to standard CNNs on action and image recognition tasks and significantly greater accuracy under various image corruptions. We also perform the first adversarial attacks with full knowledge of a sparse coding CNN layer by attacking LCANets with white-box and black-box attacks, and we show that, contrary to previous hypotheses, sparse coding layers are not very robust to white-box attacks. Finally, we propose a way to use sparse coding layers as a plug-and-play robust frontend by showing that they significantly increase the robustness of adversarially-trained CNNs over corruptions and attacks.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Teti, Michael and Kenyon, Garrett and Migliori, Ben and Moore, Juston},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {21232--21252},
}

@inproceedings{xue_investigating_2022,
	title = {Investigating {Why} {Contrastive} {Learning} {Benefits} {Robustness} against {Label} {Noise}},
	url = {https://proceedings.mlr.press/v162/xue22a.html},
	abstract = {Self-supervised Contrastive Learning (CL) has been recently shown to be very effective in preventing deep networks from overfitting noisy labels. Despite its empirical success, the theoretical understanding of the effect of contrastive learning on boosting robustness is very limited. In this work, we rigorously prove that the representation matrix learned by contrastive learning boosts robustness, by having: (i) one prominent singular value corresponding to each sub-class in the data, and significantly smaller remaining singular values; and (ii) a large alignment between the prominent singular vectors and the clean labels of each sub-class. The above properties enable a linear layer trained on such representations to effectively learn the clean labels without overfitting the noise. We further show that the low-rank structure of the Jacobian of deep networks pre-trained with contrastive learning allows them to achieve a superior performance initially, when fine-tuned on noisy labels. Finally, we demonstrate that the initial robustness provided by contrastive learning enables robust training methods to achieve state-of-the-art performance under extreme noise levels, e.g., an average of 27.18\% and 15.58\% increase in accuracy on CIFAR-10 and CIFAR-100 with 80\% symmetric noisy labels, and 4.11\% increase in accuracy on WebVision.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xue, Yihao and Whitecross, Kyle and Mirzasoleiman, Baharan},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {24851--24871},
}

@inproceedings{zhang_branch_2022,
	title = {A {Branch} and {Bound} {Framework} for {Stronger} {Adversarial} {Attacks} of {ReLU} {Networks}},
	url = {https://proceedings.mlr.press/v162/zhang22ae.html},
	abstract = {Strong adversarial attacks are important for evaluating the true robustness of deep neural networks. Most existing attacks search in the input space, e.g., using gradient descent, and may miss adversarial examples due to non-convexity. In this work, we systematically search adversarial examples in the activation space of ReLU networks to tackle hard instances where none of the existing adversarial attacks succeed. Unfortunately, searching the activation space typically relies on generic mixed integer programming (MIP) solvers and is limited to small networks and easy problem instances. To improve scalability and practicability, we use branch and bound (BaB) with specialized GPU-based bound propagation methods, and propose a top-down beam-search approach to quickly identify the subspace that may contain adversarial examples. Moreover, we build an adversarial candidates pool using cheap attacks to further assist the search in activation space via diving techniques and a bottom-up large neighborhood search. Our adversarial attack framework, BaB-Attack, opens up a new opportunity for designing novel adversarial attacks not limited to searching the input space, and enables us to borrow techniques from integer programming theory and neural network verification. In experiments, we can successfully generate adversarial examples when existing attacks on input space fail. Compared to off-the-shelf MIP solver based attacks that requires significant computations, we outperform in both success rates and efficiency.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Huan and Wang, Shiqi and Xu, Kaidi and Wang, Yihan and Jana, Suman and Hsieh, Cho-Jui and Kolter, Zico},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {26591--26604},
}

@inproceedings{dominguez-olmedo_adversarial_2022,
	title = {On the {Adversarial} {Robustness} of {Causal} {Algorithmic} {Recourse}},
	url = {https://proceedings.mlr.press/v162/dominguez-olmedo22a.html},
	abstract = {Algorithmic recourse seeks to provide actionable recommendations for individuals to overcome unfavorable classification outcomes from automated decision-making systems. Recourse recommendations should ideally be robust to reasonably small uncertainty in the features of the individual seeking recourse. In this work, we formulate the adversarially robust recourse problem and show that recourse methods that offer minimally costly recourse fail to be robust. We then present methods for generating adversarially robust recourse for linear and for differentiable classifiers. Finally, we show that regularizing the decision-making classifier to behave locally linearly and to rely more strongly on actionable features facilitates the existence of adversarially robust recourse.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dominguez-Olmedo, Ricardo and Karimi, Amir H. and Schölkopf, Bernhard},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {5324--5342},
}

@inproceedings{guo_adversarially_2022,
	title = {Adversarially trained neural representations are already as robust as biological neural representations},
	url = {https://proceedings.mlr.press/v162/guo22d.html},
	abstract = {Visual systems of primates are the gold standard of robust perception. There is thus a general belief that mimicking the neural representations that underlie those systems will yield artificial visual systems that are adversarially robust. In this work, we develop a method for performing adversarial visual attacks directly on primate brain activity. We then leverage this method to demonstrate that the above-mentioned belief might not be well-founded. Specifically, we report that the biological neurons that make up visual systems of primates exhibit susceptibility to adversarial perturbations that is comparable in magnitude to existing (robustly trained) artificial neural networks.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Guo, Chong and Lee, Michael and Leclerc, Guillaume and Dapello, Joel and Rao, Yug and Madry, Aleksander and Dicarlo, James},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {8072--8081},
}

@inproceedings{gao_fast_2022,
	title = {Fast and {Reliable} {Evaluation} of {Adversarial} {Robustness} with {Minimum}-{Margin} {Attack}},
	url = {https://proceedings.mlr.press/v162/gao22i.html},
	abstract = {The AutoAttack (AA) has been the most reliable method to evaluate adversarial robustness when considerable computational resources are available. However, the high computational cost (e.g., 100 times more than that of the project gradient descent attack) makes AA infeasible for practitioners with limited computational resources, and also hinders applications of AA in the adversarial training (AT). In this paper, we propose a novel method, minimum-margin (MM) attack, to fast and reliably evaluate adversarial robustness. Compared with AA, our method achieves comparable performance but only costs 3\% of the computational time in extensive experiments. The reliability of our method lies in that we evaluate the quality of adversarial examples using the margin between two targets that can precisely identify the most adversarial example. The computational efficiency of our method lies in an effective Sequential TArget Ranking Selection (STARS) method, ensuring that the cost of the MM attack is independent of the number of classes. The MM attack opens a new way for evaluating adversarial robustness and provides a feasible and reliable way to generate high-quality adversarial examples in AT.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gao, Ruize and Wang, Jiongxiao and Zhou, Kaiwen and Liu, Feng and Xie, Binghui and Niu, Gang and Han, Bo and Cheng, James},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {7144--7163},
}

@inproceedings{zhou_improving_2022,
	title = {Improving {Adversarial} {Robustness} via {Mutual} {Information} {Estimation}},
	url = {https://proceedings.mlr.press/v162/zhou22j.html},
	abstract = {Deep neural networks (DNNs) are found to be vulnerable to adversarial noise. They are typically misled by adversarial samples to make wrong predictions. To alleviate this negative effect, in this paper, we investigate the dependence between outputs of the target model and input adversarial samples from the perspective of information theory, and propose an adversarial defense method. Specifically, we first measure the dependence by estimating the mutual information (MI) between outputs and the natural patterns of inputs (called natural MI) and MI between outputs and the adversarial patterns of inputs (called adversarial MI), respectively. We find that adversarial samples usually have larger adversarial MI and smaller natural MI compared with those w.r.t. natural samples. Motivated by this observation, we propose to enhance the adversarial robustness by maximizing the natural MI and minimizing the adversarial MI during the training process. In this way, the target model is expected to pay more attention to the natural pattern that contains objective semantics. Empirical evaluations demonstrate that our method could effectively improve the adversarial accuracy against multiple attacks.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhou, Dawei and Wang, Nannan and Gao, Xinbo and Han, Bo and Wang, Xiaoyu and Zhan, Yibing and Liu, Tongliang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {27338--27352},
}

@inproceedings{yao_improving_2022,
	title = {Improving {Out}-of-{Distribution} {Robustness} via {Selective} {Augmentation}},
	url = {https://proceedings.mlr.press/v162/yao22b.html},
	abstract = {Machine learning algorithms typically assume that training and test examples are drawn from the same distribution. However, distribution shift is a common problem in real-world applications and can cause models to perform dramatically worse at test time. In this paper, we specifically consider the problems of subpopulation shifts (e.g., imbalanced data) and domain shifts. While prior works often seek to explicitly regularize internal representations or predictors of the model to be domain invariant, we instead aim to learn invariant predictors without restricting the model’s internal representations or predictors. This leads to a simple mixup-based technique which learns invariant predictors via selective augmentation called LISA. LISA selectively interpolates samples either with the same labels but different domains or with the same domain but different labels. Empirically, we study the effectiveness of LISA on nine benchmarks ranging from subpopulation shifts to domain shifts, and we find that LISA consistently outperforms other state-of-the-art methods and leads to more invariant predictors. We further analyze a linear setting and theoretically show how LISA leads to a smaller worst-group error.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yao, Huaxiu and Wang, Yu and Li, Sai and Zhang, Linjun and Liang, Weixin and Zou, James and Finn, Chelsea},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {25407--25437},
}

@inproceedings{yamamura_diversified_2022,
	title = {Diversified {Adversarial} {Attacks} based on {Conjugate} {Gradient} {Method}},
	url = {https://proceedings.mlr.press/v162/yamamura22a.html},
	abstract = {Deep learning models are vulnerable to adversarial examples, and adversarial attacks used to generate such examples have attracted considerable research interest. Although existing methods based on the steepest descent have achieved high attack success rates, ill-conditioned problems occasionally reduce their performance. To address this limitation, we utilize the conjugate gradient (CG) method, which is effective for this type of problem, and propose a novel attack algorithm inspired by the CG method, named the Auto Conjugate Gradient (ACG) attack. The results of large-scale evaluation experiments conducted on the latest robust models show that, for most models, ACG was able to find more adversarial examples with fewer iterations than the existing SOTA algorithm Auto-PGD (APGD). We investigated the difference in search performance between ACG and APGD in terms of diversification and intensification, and define a measure called Diversity Index (DI) to quantify the degree of diversity. From the analysis of the diversity using this index, we show that the more diverse search of the proposed method remarkably improves its attack success rate.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yamamura, Keiichiro and Sato, Haruki and Tateiwa, Nariaki and Hata, Nozomi and Mitsutake, Toru and Oe, Issa and Ishikura, Hiroki and Fujisawa, Katsuki},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {24872--24894},
}

@inproceedings{croce_adversarial_2022,
	title = {Adversarial {Robustness} against {Multiple} and {Single} \$l\_p\$-{Threat} {Models} via {Quick} {Fine}-{Tuning} of {Robust} {Classifiers}},
	url = {https://proceedings.mlr.press/v162/croce22b.html},
	abstract = {A major drawback of adversarially robust models, in particular for large scale datasets like ImageNet, is the extremely long training time compared to standard models. Moreover, models should be robust not only to one lplpl\_p-threat model but ideally to all of them. In this paper we propose Extreme norm Adversarial Training (E-AT) for multiple-norm robustness which is based on geometric properties of lplpl\_p-balls. E-AT costs up to three times less than other adversarial training methods for multiple-norm robustness. Using E-AT we show that for ImageNet a single epoch and for CIFAR-10 three epochs are sufficient to turn any lplpl\_p-robust model into a multiple-norm robust model. In this way we get the first multiple-norm robust model for ImageNet and boost the state-of-the-art for multiple-norm robustness to more than 515151\% on CIFAR-10. Finally, we study the general transfer via fine-tuning of adversarial robustness between different individual lplpl\_p-threat models and improve the previous SOTA l1l1l\_1-robustness on both CIFAR-10 and ImageNet. Extensive experiments show that our scheme works across datasets and architectures including vision transformers.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Croce, Francesco and Hein, Matthias},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {4436--4454},
}

@inproceedings{dbouk_adversarial_2022,
	title = {Adversarial {Vulnerability} of {Randomized} {Ensembles}},
	url = {https://proceedings.mlr.press/v162/dbouk22a.html},
	abstract = {Despite the tremendous success of deep neural networks across various tasks, their vulnerability to imperceptible adversarial perturbations has hindered their deployment in the real world. Recently, works on randomized ensembles have empirically demonstrated significant improvements in adversarial robustness over standard adversarially trained (AT) models with minimal computational overhead, making them a promising solution for safety-critical resource-constrained applications. However, this impressive performance raises the question: Are these robustness gains provided by randomized ensembles real? In this work we address this question both theoretically and empirically. We first establish theoretically that commonly employed robustness evaluation methods such as adaptive PGD provide a false sense of security in this setting. Subsequently, we propose a theoretically-sound and efficient adversarial attack algorithm (ARC) capable of compromising random ensembles even in cases where adaptive PGD fails to do so. We conduct comprehensive experiments across a variety of network architectures, training schemes, datasets, and norms to support our claims, and empirically establish that randomized ensembles are in fact more vulnerable to ℓpℓp{\textbackslash}ell\_p-bounded adversarial perturbations than even standard AT models. Our code can be found at https://github.com/hsndbk4/ARC.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dbouk, Hassan and Shanbhag, Naresh},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {4890--4917},
}

@inproceedings{kou_certified_2022,
	title = {Certified {Adversarial} {Robustness} {Under} the {Bounded} {Support} {Set}},
	url = {https://proceedings.mlr.press/v162/kou22a.html},
	abstract = {Deep neural networks (DNNs) have revealed severe vulnerability to adversarial perturbations, beside empirical adversarial training for robustness, the design of provably robust classifiers attracts more and more attention. Randomized smoothing methods provide the certified robustness with agnostic architecture, which is further extended to a provable robustness framework using f-divergence. While these methods cannot be applied to smoothing measures with bounded support set such as uniform probability measure due to the use of likelihood ratio in their certification methods. In this paper, we generalize the fff-divergence-based framework to a Wasserstein-distance-based and total-variation-distance-based framework that is first able to analyze robustness properties of bounded support set smoothing measures both theoretically and experimentally. By applying our methodology to uniform probability measures with support set lp(p=1,2,∞ and general)lp(p=1,2,∞ and general)l\_p (p=1,2,{\textbackslash}infty{\textbackslash}text\{ and general\}) ball, we prove negative certified robustness properties with respect to lq(q=1,2,∞)lq(q=1,2,∞)l\_q (q=1, 2, {\textbackslash}infty) perturbations and present experimental results on CIFAR-10 dataset with ResNet to validate our theory. And it is also worth mentioning that our certification procedure only costs constant computation time.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kou, Yiwen and Zheng, Qinyuan and Wang, Yisen},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {11559--11597},
}

@inproceedings{pang_robustness_2022,
	title = {Robustness and {Accuracy} {Could} {Be} {Reconcilable} by ({Proper}) {Definition}},
	url = {https://proceedings.mlr.press/v162/pang22a.html},
	abstract = {The trade-off between robustness and accuracy has been widely studied in the adversarial literature. Although still controversial, the prevailing view is that this trade-off is inherent, either empirically or theoretically. Thus, we dig for the origin of this trade-off in adversarial training and find that it may stem from the improperly defined robust error, which imposes an inductive bias of local invariance — an overcorrection towards smoothness. Given this, we advocate employing local equivariance to describe the ideal behavior of a robust model, leading to a self-consistent robust error named SCORE. By definition, SCORE facilitates the reconciliation between robustness and accuracy, while still handling the worst-case uncertainty via robust optimization. By simply substituting KL divergence with variants of distance metrics, SCORE can be efficiently minimized. Empirically, our models achieve top-rank performance on RobustBench under AutoAttack. Besides, SCORE provides instructive insights for explaining the overfitting phenomenon and semantic input gradients observed on robust models.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pang, Tianyu and Lin, Min and Yang, Xiao and Zhu, Jun and Yan, Shuicheng},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {17258--17277},
}

@inproceedings{gao_rethinking_2022,
	title = {Rethinking {Image}-{Scaling} {Attacks}: {The} {Interplay} {Between} {Vulnerabilities} in {Machine} {Learning} {Systems}},
	shorttitle = {Rethinking {Image}-{Scaling} {Attacks}},
	url = {https://proceedings.mlr.press/v162/gao22g.html},
	abstract = {As real-world images come in varying sizes, the machine learning model is part of a larger system that includes an upstream image scaling algorithm. In this paper, we investigate the interplay between vulnerabilities of the image scaling procedure and machine learning models in the decision-based black-box setting. We propose a novel sampling strategy to make a black-box attack exploit vulnerabilities in scaling algorithms, scaling defenses, and the final machine learning model in an end-to-end manner. Based on this scaling-aware attack, we reveal that most existing scaling defenses are ineffective under threat from downstream models. Moreover, we empirically observe that standard black-box attacks can significantly improve their performance by exploiting the vulnerable scaling procedure. We further demonstrate this problem on a commercial Image Analysis API with decision-based black-box attacks.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gao, Yue and Shumailov, Ilia and Fawaz, Kassem},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {7102--7121},
}

@inproceedings{xu_adversarially_2022,
	title = {Adversarially {Robust} {Models} may not {Transfer} {Better}: {Sufficient} {Conditions} for {Domain} {Transferability} from the {View} of {Regularization}},
	shorttitle = {Adversarially {Robust} {Models} may not {Transfer} {Better}},
	url = {https://proceedings.mlr.press/v162/xu22n.html},
	abstract = {Machine learning (ML) robustness and domain generalization are fundamentally correlated: they essentially concern data distribution shifts under adversarial and natural settings, respectively. On one hand, recent studies show that more robust (adversarially trained) models are more generalizable. On the other hand, there is a lack of theoretical understanding of their fundamental connections. In this paper, we explore the relationship between regularization and domain transferability considering different factors such as norm regularization and data augmentations (DA). We propose a general theoretical framework proving that factors involving the model function class regularization are sufficient conditions for relative domain transferability. Our analysis implies that “robustness" is neither necessary nor sufficient for transferability; rather, regularization is a more fundamental perspective for understanding domain transferability. We then discuss popular DA protocols (including adversarial training) and show when they can be viewed as the function class regularization under certain conditions and therefore improve generalization. We conduct extensive experiments to verify our theoretical findings and show several counterexamples where robustness and generalization are negatively correlated on different datasets.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xu, Xiaojun and Zhang, Jacky Y. and Ma, Evelyn and Son, Hyun Ho and Koyejo, Sanmi and Li, Bo},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {24770--24802},
}

@inproceedings{li_double_2022,
	title = {Double {Sampling} {Randomized} {Smoothing}},
	url = {https://proceedings.mlr.press/v162/li22aa.html},
	abstract = {Neural networks (NNs) are known to be vulnerable against adversarial perturbations, and thus there is a line of work aiming to provide robustness certification for NNs, such as randomized smoothing, which samples smoothing noises from a certain distribution to certify the robustness for a smoothed classifier. However, as previous work shows, the certified robust radius in randomized smoothing suffers from scaling to large datasets ("curse of dimensionality"). To overcome this hurdle, we propose a Double Sampling Randomized Smoothing (DSRS) framework, which exploits the sampled probability from an additional smoothing distribution to tighten the robustness certification of the previous smoothed classifier. Theoretically, under mild assumptions, we prove that DSRS can certify Θ(d−−√)Θ(d){\textbackslash}Theta({\textbackslash}sqrt d) robust radius under ℓ2ℓ2{\textbackslash}ell\_2 norm where ddd is the input dimension, which implies that DSRS may be able to break the curse of dimensionality of randomized smoothing. We instantiate DSRS for a generalized family of Gaussian smoothing and propose an efficient and sound computing method based on customized dual optimization considering sampling error. Extensive experiments on MNIST, CIFAR-10, and ImageNet verify our theory and show that DSRS certifies larger robust radii than existing baselines consistently under different settings. Code is available at https://github.com/llylly/DSRS.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Linyi and Zhang, Jiawei and Xie, Tao and Li, Bo},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {13163--13208},
}

@inproceedings{mustafa_generalization_2022,
	title = {On the {Generalization} {Analysis} of {Adversarial} {Learning}},
	url = {https://proceedings.mlr.press/v162/mustafa22a.html},
	abstract = {Many recent studies have highlighted the susceptibility of virtually all machine-learning models to adversarial attacks. Adversarial attacks are imperceptible changes to an input example of a given prediction model. Such changes are carefully designed to alter the otherwise correct prediction of the model. In this paper, we study the generalization properties of adversarial learning. In particular, we derive high-probability generalization bounds on the adversarial risk in terms of the empirical adversarial risk, the complexity of the function class and the adversarial noise set. Our bounds are generally applicable to many models, losses, and adversaries. We showcase its applicability by deriving adversarial generalization bounds for the multi-class classification setting and various prediction models (including linear models and Deep Neural Networks). We also derive optimistic adversarial generalization bounds for the case of smooth losses. These are the first fast-rate bounds valid for adversarial deep learning to the best of our knowledge.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mustafa, Waleed and Lei, Yunwen and Kloft, Marius},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {16174--16196},
}

@inproceedings{sukenik_intriguing_2022,
	title = {Intriguing {Properties} of {Input}-{Dependent} {Randomized} {Smoothing}},
	url = {https://proceedings.mlr.press/v162/sukeni-k22a.html},
	abstract = {Randomized smoothing is currently considered the state-of-the-art method to obtain certifiably robust classifiers. Despite its remarkable performance, the method is associated with various serious problems such as “certified accuracy waterfalls”, certification vs. accuracy trade-off, or even fairness issues. Input-dependent smoothing approaches have been proposed with intention of overcoming these flaws. However, we demonstrate that these methods lack formal guarantees and so the resulting certificates are not justified. We show that in general, the input-dependent smoothing suffers from the curse of dimensionality, forcing the variance function to have low semi-elasticity. On the other hand, we provide a theoretical and practical framework that enables the usage of input-dependent smoothing even in the presence of the curse of dimensionality, under strict restrictions. We present one concrete design of the smoothing variance function and test it on CIFAR10 and MNIST. Our design mitigates some of the problems of classical smoothing and is formally underlined, yet further improvement of the design is still necessary.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Súkenı́k, Peter and Kuvshinov, Aleksei and Günnemann, Stephan},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {20697--20743},
}

@inproceedings{wei_smooth_2022,
	title = {To {Smooth} or {Not}? {When} {Label} {Smoothing} {Meets} {Noisy} {Labels}},
	shorttitle = {To {Smooth} or {Not}?},
	url = {https://proceedings.mlr.press/v162/wei22b.html},
	abstract = {Label smoothing (LS) is an arising learning paradigm that uses the positively weighted average of both the hard training labels and uniformly distributed soft labels. It was shown that LS serves as a regularizer for training data with hard labels and therefore improves the generalization of the model. Later it was reported LS even helps with improving robustness when learning with noisy labels. However, we observed that the advantage of LS vanishes when we operate in a high label noise regime. Intuitively speaking, this is due to the increased entropy of P(noisy label{\textbar}X) when the noise rate is high, in which case, further applying LS tends to “over-smooth” the estimated posterior. We proceeded to discover that several learning-with-noisy-labels solutions in the literature instead relate more closely to negative/not label smoothing (NLS), which acts counter to LS and defines as using a negative weight to combine the hard and soft labels! We provide understandings for the properties of LS and NLS when learning with noisy labels. Among other established properties, we theoretically show NLS is considered more beneficial when the label noise rates are high. We provide extensive experimental results on multiple benchmarks to support our findings too. Code is publicly available at https://github.com/UCSC-REAL/negative-label-smoothing.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wei, Jiaheng and Liu, Hangyu and Liu, Tongliang and Niu, Gang and Sugiyama, Masashi and Liu, Yang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {23589--23614},
}

@inproceedings{sitawarin_demystifying_2022,
	title = {Demystifying the {Adversarial} {Robustness} of {Random} {Transformation} {Defenses}},
	url = {https://proceedings.mlr.press/v162/sitawarin22a.html},
	abstract = {Neural networks’ lack of robustness against attacks raises concerns in security-sensitive settings such as autonomous vehicles. While many countermeasures may look promising, only a few withstand rigorous evaluation. Defenses using random transformations (RT) have shown impressive results, particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of defense has not been rigorously evaluated, leaving its robustness properties poorly understood. Their stochastic properties make evaluation more challenging and render many proposed attacks on deterministic models inapplicable. First, we show that the BPDA attack (Athalye et al., 2018a) used in BaRT’s evaluation is ineffective and likely overestimates its robustness. We then attempt to construct the strongest possible RT defense through the informed selection of transformations and Bayesian optimization for tuning their parameters. Furthermore, we create the strongest possible attack to evaluate our RT defense. Our new attack vastly outperforms the baseline, reducing the accuracy by 83\% compared to the 19\% reduction by the commonly used EoT attack (4.3×4.3×4.3{\textbackslash}times improvement). Our result indicates that the RT defense on the Imagenette dataset (a ten-class subset of ImageNet) is not robust against adversarial examples. Extending the study further, we use our new attack to adversarially train RT defense (called AdvRT), resulting in a large robustness gain. Code is available at https://github.com/wagnergroup/demystify-random-transform.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sitawarin, Chawin and Golan-Strieb, Zachary J. and Wagner, David},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {20232--20252},
}

@inproceedings{shen_data_2022,
	title = {Data {Augmentation} as {Feature} {Manipulation}},
	url = {https://proceedings.mlr.press/v162/shen22a.html},
	abstract = {Data augmentation is a cornerstone of the machine learning pipeline, yet its theoretical underpinnings remain unclear. Is it merely a way to artificially augment the data set size? Or is it about encouraging the model to satisfy certain invariances? In this work we consider another angle, and we study the effect of data augmentation on the dynamic of the learning process. We find that data augmentation can alter the relative importance of various features, effectively making certain informative but hard to learn features more likely to be captured in the learning process. Importantly, we show that this effect is more pronounced for non-linear models, such as neural networks. Our main contribution is a detailed analysis of data augmentation on the learning dynamic for a two layer convolutional neural network in the recently proposed multi-view model by Z. Allen-Zhu and Y. Li. We complement this analysis with further experimental evidence that data augmentation can be viewed as a form of feature manipulation.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shen, Ruoqi and Bubeck, Sebastien and Gunasekar, Suriya},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {19773--19808},
}

@inproceedings{han_you_2022,
	title = {You {Only} {Cut} {Once}: {Boosting} {Data} {Augmentation} with a {Single} {Cut}},
	shorttitle = {You {Only} {Cut} {Once}},
	url = {https://proceedings.mlr.press/v162/han22a.html},
	abstract = {We present You Only Cut Once (YOCO) for performing data augmentations. YOCO cuts one image into two pieces and performs data augmentations individually within each piece. Applying YOCO improves the diversity of the augmentation per sample and encourages neural networks to recognize objects from partial information. YOCO enjoys the properties of parameter-free, easy usage, and boosting almost all augmentations for free. Thorough experiments are conducted to evaluate its effectiveness. We first demonstrate that YOCO can be seamlessly applied to varying data augmentations, neural network architectures, and brings performance gains on CIFAR and ImageNet classification tasks, sometimes surpassing conventional image-level augmentation by large margins. Moreover, we show YOCO benefits contrastive pre-training toward a more powerful representation that can be better transferred to multiple downstream tasks. Finally, we study a number of variants of YOCO and empirically analyze the performance for respective settings.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Han, Junlin and Fang, Pengfei and Li, Weihao and Hong, Jie and Armin, Mohammad Ali and Reid, Ian and Petersson, Lars and Li, Hongdong},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {8196--8212},
}

@inproceedings{zhou_modeling_2022,
	title = {Modeling {Adversarial} {Noise} for {Adversarial} {Training}},
	url = {https://proceedings.mlr.press/v162/zhou22k.html},
	abstract = {Deep neural networks have been demonstrated to be vulnerable to adversarial noise, promoting the development of defense against adversarial attacks. Motivated by the fact that adversarial noise contains well-generalizing features and that the relationship between adversarial data and natural data can help infer natural data and make reliable predictions, in this paper, we study to model adversarial noise by learning the transition relationship between adversarial labels (i.e. the flipped labels used to generate adversarial data) and natural labels (i.e. the ground truth labels of the natural data). Specifically, we introduce an instance-dependent transition matrix to relate adversarial labels and natural labels, which can be seamlessly embedded with the target model (enabling us to model stronger adaptive adversarial noise). Empirical evaluations demonstrate that our method could effectively improve adversarial accuracy.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhou, Dawei and Wang, Nannan and Han, Bo and Liu, Tongliang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {27353--27366},
}

@inproceedings{zhang_revisiting_2022,
	title = {Revisiting and {Advancing} {Fast} {Adversarial} {Training} {Through} {The} {Lens} of {Bi}-{Level} {Optimization}},
	url = {https://proceedings.mlr.press/v162/zhang22ak.html},
	abstract = {Adversarial training (AT) is a widely recognized defense mechanism to gain the robustness of deep neural networks against adversarial attacks. It is built on min-max optimization (MMO), where the minimizer (i.e., defender) seeks a robust model to minimize the worst-case training loss in the presence of adversarial examples crafted by the maximizer (i.e., attacker). However, the conventional MMO method makes AT hard to scale. Thus, Fast-AT and other recent algorithms attempt to simplify MMO by replacing its maximization step with the single gradient sign-based attack generation step. Although easy to implement, FAST-AT lacks theoretical guarantees, and its empirical performance is unsatisfactory due to the issue of robust catastrophic overfitting when training with strong adversaries. In this paper, we advance Fast-AT from the fresh perspective of bi-level optimization (BLO). We first show that the commonly-used Fast-AT is equivalent to using a stochastic gradient algorithm to solve a linearized BLO problem involving a sign operation. However, the discrete nature of the sign operation makes it difficult to understand the algorithm performance. Inspired by BLO, we design and analyze a new set of robust training algorithms termed Fast Bi-level AT (Fast-BAT), which effectively defends sign-based projected gradient descent (PGD) attacks without using any gradient sign method or explicit robust regularization. In practice, we show that our method yields substantial robustness improvements over multiple baselines across multiple models and datasets.},
	language = {en},
	urldate = {2022-09-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Yihua and Zhang, Guanhua and Khanduri, Prashant and Hong, Mingyi and Chang, Shiyu and Liu, Sijia},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {26693--26712},
}

@inproceedings{alayrac_are_2019,
	title = {Are {Labels} {Required} for {Improving} {Adversarial} {Robustness}?},
	abstract = {Recent work has uncovered the interesting (and somewhat surprising) ﬁnding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classiﬁcation. This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive. Our main insight is that unlabeled data can be a competitive alternative to labeled data for training adversarially robust models. Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors. On standard datasets like CIFAR10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7\% over using 4K supervised examples alone, and captures over 95\% of the improvement from the same number of labeled examples. Finally, we report an improvement of 4\% over the previous state-of-theart on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset. This demonstrates that our ﬁnding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems} ({NeurIPS} 2019)},
	author = {Alayrac, Jean-Baptiste and Uesato, Jonathan and Huang, Po-Sen and Fawzi, Alhussein and Stanforth, Robert and Kohli, Pushmeet},
	year = {2019},
	pages = {10},
}

@inproceedings{sitawarin_sat_2021,
	title = {{SAT}: {Improving} {Adversarial} {Training} via {Curriculum}-{Based} {Loss} {Smoothing}},
	shorttitle = {{SAT}},
	url = {http://arxiv.org/abs/2003.09347},
	doi = {10.1145/3474369.3486878},
	abstract = {Adversarial training (AT) has become a popular choice for training robust networks. However, it tends to sacrifice clean accuracy heavily in favor of robustness and suffers from a large generalization error. To address these concerns, we propose Smooth Adversarial Training (SAT), guided by our analysis on the eigenspectrum of the loss Hessian. We find that curriculum learning, a scheme that emphasizes on starting "easy" and gradually ramping up on the "difficulty" of training, smooths the adversarial loss landscape for a suitably chosen difficulty metric. We present a general formulation for curriculum learning in the adversarial setting and propose two difficulty metrics based on the maximal Hessian eigenvalue (H-SAT) and the softmax probability (P-SA). We demonstrate that SAT stabilizes network training even for a large perturbation norm and allows the network to operate at a better clean accuracy versus robustness trade-off curve compared to AT. This leads to a significant improvement in both clean accuracy and robustness compared to AT, TRADES, and other baselines. To highlight a few results, our best model improves normal and robust accuracy by 6\% and 1\% on CIFAR-100 compared to AT, respectively. On Imagenette, a ten-class subset of ImageNet, our model outperforms AT by 23\% and 3\% on normal and robust accuracy respectively.},
	urldate = {2022-09-15},
	booktitle = {Proceedings of the 14th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	author = {Sitawarin, Chawin and Chakraborty, Supriyo and Wagner, David},
	month = nov,
	year = {2021},
	note = {arXiv:2003.09347 [cs, stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {25--36},
}

@inproceedings{kireev_effectiveness_2022,
	title = {On the effectiveness of adversarial training against common corruptions},
	url = {https://proceedings.mlr.press/v180/kireev22a.html},
	abstract = {The literature on robustness towards common corruptions shows no consensus on whether adversarial training can improve the performance in this setting. First, we show that, when used with an appropriately selected perturbation radius, Lp adversarial training can serve as a strong baseline against common corruptions improving both accuracy and calibration. Then we explain why adversarial training performs better than data augmentation with simple Gaussian noise which has been observed to be a meaningful baseline on common corruptions. Related to this, we identify the sigma-overfitting phenomenon when Gaussian augmentation overfits to a particular standard deviation used for training which has a significant detrimental effect on common corruption accuracy. We discuss how to alleviate this problem and then how to further enhance Lp adversarial training by introducing an efficient relaxation of adversarial training with learned perceptual image patch similarity as the distance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that our approach does not only improve the Lp adversarial training baseline but also has cumulative gains with data augmentation methods such as AugMix, DeepAugment, ANT, and SIN, leading to state-of-the-art performance on common corruptions.},
	language = {en},
	urldate = {2022-09-15},
	booktitle = {Proceedings of the {Thirty}-{Eighth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Kireev, Klim and Andriushchenko, Maksym and Flammarion, Nicolas},
	month = aug,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {1012--1021},
}

@inproceedings{huang_transferable_2022,
	title = {Transferable {Adversarial} {Attack} based on {Integrated} {Gradients}},
	url = {https://openreview.net/forum?id=DesNW4-5ai9},
	abstract = {The vulnerability of deep neural networks to adversarial examples has drawn tremendous attention from the community. Three approaches, optimizing standard objective functions, exploiting attention...},
	language = {en},
	urldate = {2022-04-06},
	author = {Huang, Yi and Kong, Adams Wai-Kin},
	year = {2022},
}

@inproceedings{lv_implicit_2022,
	title = {Implicit {Bias} of {Adversarial} {Training} for {Deep} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=l8It-0lE5e7},
	abstract = {We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without any explicit regularization. In particular, for deep linear...},
	language = {en},
	urldate = {2022-04-06},
	author = {Lv, Bochen and Zhu, Zhanxing},
	year = {2022},
}

@inproceedings{agarwal_neural_2021,
	title = {Neural {Additive} {Models}: {Interpretable} {Machine} {Learning} with {Neural} {Nets}},
	volume = {34},
	shorttitle = {Neural {Additive} {Models}},
	url = {https://papers.nips.cc/paper/2021/hash/251bd0442dfcc53b5a761e050f8022b8-Abstract.html},
	abstract = {Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but are more flexible because they are based on neural nets instead of boosted trees. To demonstrate this, we show how NAMs can be used for multitask learning on synthetic data and on the COMPAS recidivism data due to their composability, and demonstrate that the differentiability of NAMs allows them to train more complex interpretable models for COVID-19.},
	urldate = {2022-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Agarwal, Rishabh and Melnick, Levi and Frosst, Nicholas and Zhang, Xuezhou and Lengerich, Ben and Caruana, Rich and Hinton, Geoffrey E},
	year = {2021},
	pages = {4699--4711},
}

@inproceedings{naseer_intriguing_2021,
	title = {Intriguing {Properties} of {Vision} {Transformers}},
	volume = {34},
	url = {https://papers.nips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html},
	abstract = {Vision transformers (ViT) have demonstrated impressive performance across numerous machine vision tasks. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility (in attending image-wide context conditioned on a given patch) can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and provide comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a)Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60\% top-1 accuracy on ImageNet even after randomly occluding 80\% of the image content. (b)The robustness towards occlusions is not due to texture bias, instead we show that ViTs are significantly less biased towards local textures, compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c)Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d)Off-the-shelf features from a single ViT model can be combined to create a feature ensemble,  leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms.  We show effective features of ViTs are due to flexible and dynamic receptive fields possible via self-attention mechanisms. Our code will be publicly released.},
	urldate = {2022-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Naseer, Muhammad Muzammal and Ranasinghe, Kanchana and Khan, Salman H and Hayat, Munawar and Shahbaz Khan, Fahad and Yang, Ming-Hsuan},
	year = {2021},
	pages = {23296--23308},
}

@inproceedings{vo_query_2022,
	title = {Query {Efficient} {Decision} {Based} {Sparse} {Attacks} {Against} {Black}-box {Deep} {Learning} {Models}},
	url = {https://openreview.net/forum?id=73MEhZ0anV},
	abstract = {Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a...},
	language = {en},
	urldate = {2022-04-06},
	author = {Vo, Viet and Abbasnejad, Ehsan M. and Ranasinghe, Damith},
	year = {2022},
}

@article{nakkiran_deep_2021,
	title = {Deep double descent: where bigger models and more data hurt},
	volume = {2021},
	issn = {1742-5468},
	shorttitle = {Deep double descent},
	url = {https://doi.org/10.1088/1742-5468/ac3a74},
	doi = {10.1088/1742-5468/ac3a74},
	abstract = {We show that a variety of modern deep learning tasks exhibit a ‘double-descent’ phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	language = {en},
	number = {12},
	urldate = {2022-08-31},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {124003},
}

@inproceedings{dabouei_supermix_2021,
	title = {{SuperMix}: {Supervising} the {Mixing} {Data} {Augmentation}},
	shorttitle = {{SuperMix}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Dabouei_SuperMix_Supervising_the_Mixing_Data_Augmentation_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-09-02},
	author = {Dabouei, Ali and Soleymani, Sobhan and Taherkhani, Fariborz and Nasrabadi, Nasser M.},
	year = {2021},
	pages = {13794--13803},
}

@inproceedings{muller_trivialaugment_2021,
	title = {{TrivialAugment}: {Tuning}-{Free} {Yet} {State}-of-the-{Art} {Data} {Augmentation}},
	shorttitle = {{TrivialAugment}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Muller_TrivialAugment_Tuning-Free_Yet_State-of-the-Art_Data_Augmentation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-08},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Müller, Samuel G. and Hutter, Frank},
	year = {2021},
	pages = {774--782},
}

@inproceedings{kuchnik_efficient_2019,
	title = {Efficient {Augmentation} via {Data} {Subsampling}},
	url = {https://openreview.net/forum?id=Byxpfh0cFm},
	abstract = {Selectively augmenting difficult to classify points results in efficient training.},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kuchnik, Michael and Smith, Virginia},
	year = {2019},
}

@inproceedings{zheng_deep_2021,
	title = {Deep {AutoAugment}},
	url = {https://openreview.net/forum?id=St-53J9ZARf},
	abstract = {While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this...},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zheng, Yu and Zhang, Zhi and Yan, Shen and Zhang, Mi},
	month = sep,
	year = {2021},
}

@inproceedings{chen_more_2020,
	title = {More {Data} {Can} {Expand} {The} {Generalization} {Gap} {Between} {Adversarially} {Robust} and {Standard} {Models}},
	url = {https://proceedings.mlr.press/v119/chen20q.html},
	abstract = {Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under \${\textbackslash}ell\_{\textbackslash}infty\$ attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.},
	language = {en},
	urldate = {2022-07-30},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Lin and Min, Yifei and Zhang, Mingrui and Karbasi, Amin},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1670--1680},
}

@misc{chen_guided_2021,
	title = {Guided {Interpolation} for {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2102.07327},
	doi = {10.48550/arXiv.2102.07327},
	abstract = {To enhance adversarial robustness, adversarial training learns deep neural networks on the adversarial variants generated by their natural data. However, as the training progresses, the training data becomes less and less attackable, undermining the robustness enhancement. A straightforward remedy is to incorporate more training data, but sometimes incurring an unaffordable cost. In this paper, to mitigate this issue, we propose the guided interpolation framework (GIF): in each epoch, the GIF employs the previous epoch's meta information to guide the data's interpolation. Compared with the vanilla mixup, the GIF can provide a higher ratio of attackable data, which is beneficial to the robustness enhancement; it meanwhile mitigates the model's linear behavior between classes, where the linear behavior is favorable to generalization but not to the robustness. As a result, the GIF encourages the model to predict invariantly in the cluster of each class. Experiments demonstrate that the GIF can indeed enhance adversarial robustness on various adversarial training methods and various datasets.},
	urldate = {2022-08-31},
	publisher = {arXiv},
	author = {Chen, Chen and Zhang, Jingfeng and Xu, Xilie and Hu, Tianlei and Niu, Gang and Chen, Gang and Sugiyama, Masashi},
	month = feb,
	year = {2021},
	note = {arXiv:2102.07327 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{rebuffi_data_2021,
	title = {Data {Augmentation} {Can} {Improve} {Robustness}},
	url = {https://openreview.net/forum?id=kgVJBBThdSZ},
	abstract = {We demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy.},
	language = {en},
	urldate = {2021-12-02},
	booktitle = {Neural {Information} {Processing} {Systems}},
	author = {Rebuffi, Sylvestre-Alvise and Gowal, Sven and Calian, Dan Andrei and Stimberg, Florian and Wiles, Olivia and Mann, Timothy},
	month = may,
	year = {2021},
}

@inproceedings{carmon_unlabeled_2019,
	title = {Unlabeled {Data} {Improves} {Adversarial} {Robustness}},
	abstract = {We demonstrate, theoretically and empirically, that adversarial robustness can signiﬁcantly beneﬁt from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. [41] that shows a sample complexity gap between standard and robust classiﬁcation. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) `1 robustness against several strong attacks via adversarial training and (ii) certiﬁed `2 and `1 robustness via randomized smoothing. On SVHN, adding the dataset’s own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
	year = {2019},
	pages = {12},
}

@article{tack_consistency_2022,
	title = {Consistency {Regularization} for {Adversarial} {Robustness}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20817},
	doi = {10.1609/aaai.v36i8.20817},
	abstract = {Adversarial training (AT) is currently one of the most successful methods to obtain the adversarial robustness of deep neural networks. However, the phenomenon of robust overfitting, i.e., the robustness starts to decrease significantly during AT, has been problematic, not only making practitioners consider a bag of tricks for a successful training, e.g., early stopping, but also incurring a significant generalization gap in the robustness. In this paper, we propose an effective regularization technique that prevents robust overfitting by optimizing an auxiliary ‘consistency’ regularization loss during AT. Specifically, we discover that data augmentation is a quite effective tool to mitigate the overfitting in AT, and develop a regularization that forces the predictive distributions after attacking from two different augmentations of the same instance to be similar with each other. Our experimental results demonstrate that such a simple regularization technique brings significant improvements in the test robust accuracy of a wide range of AT methods. More remarkably, we also show that our method could significantly help the model to generalize its robustness against unseen adversaries, e.g., other types or larger perturbations compared to those used during training. Code is available at https://github.com/alinlab/consistency-adversarial.},
	language = {en},
	number = {8},
	urldate = {2022-08-13},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tack, Jihoon and Yu, Sihyun and Jeong, Jongheon and Kim, Minseon and Hwang, Sung Ju and Shin, Jinwoo},
	month = jun,
	year = {2022},
	pages = {8414--8422},
}

@article{chen_decision_2022,
	title = {Decision {Boundary}-aware {Data} {Augmentation} for {Adversarial} {Training}},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2022.3165889},
	abstract = {Adversarial training (AT) is a typical method to learn adversarially robust deep neural networks via training on the adversarial variants generated by their natural examples. However, as training progresses, the training data becomes less attackable, which may undermine the enhancement of model robustness. A straightforward remedy is to incorporate more training data, but it may incur an unaffordable cost. To mitigate this issue, in this paper, we propose a deCisiOn bounDary-aware data Augmentation framework (CODA): in each epoch, the CODA directly employs the meta information of the previous epoch to guide the augmentation process and generate more data that are close to the decision boundary, i.e., attackable data. Compared with the vanilla mixup, our proposed CODA can provide a higher ratio of attackable data, which is beneficial to enhance model robustness; it meanwhile mitigates the model's linear behavior between classes, where the linear behavior is favorable to the standard training for generalization but not to the adversarial training for robustness. As a result, our proposed CODA encourages the model to predict invariantly in the cluster of each class. Experiments demonstrate that our proposed CODA can indeed enhance adversarial robustness across various adversarial training methods and multiple datasets.},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Chen, Chen and Zhang, Jingfeng and Xu, Xilie and Lyu, Lingjuan and Chen, Chaochao and Hu, Tianlei and Chen, Gang},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Dependable and Secure Computing},
	keywords = {Data models, Predictive models, Principal component analysis, Robustness, Standards, Training, Training data, adversarial robustness, data augmentation F},
	pages = {1--1},
}

@inproceedings{min_curious_2021,
	title = {The curious case of adversarially robust models: {More} data can help, double descend, or hurt generalization},
	shorttitle = {The curious case of adversarially robust models},
	url = {https://proceedings.mlr.press/v161/min21a.html},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of a decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in classification problems. We first investigate the Gaussian mixture classification with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we analyze a two-dimensional classification problem with a 0-1 loss. We prove that more data always hurts generalization of adversarially trained models with large perturbations. Empirical studies confirm our theoretical results.},
	language = {en},
	urldate = {2022-08-12},
	booktitle = {Proceedings of the {Thirty}-{Seventh} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = dec,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {129--139},
}

@misc{chen_benign_2021,
	title = {Benign {Overfitting} in {Adversarially} {Robust} {Linear} {Classification}},
	url = {http://arxiv.org/abs/2112.15250},
	doi = {10.48550/arXiv.2112.15250},
	abstract = {"Benign overfitting", where classifiers memorize noisy training data yet still achieve a good generalization performance, has drawn great attention in the machine learning community. To explain this surprising phenomenon, a series of works have provided theoretical justification in over-parameterized linear regression, classification, and kernel methods. However, it is not clear if benign overfitting still occurs in the presence of adversarial examples, i.e., examples with tiny and intentional perturbations to fool the classifiers. In this paper, we show that benign overfitting indeed occurs in adversarial training, a principled approach to defend against adversarial examples. In detail, we prove the risk bounds of the adversarially trained linear classifier on the mixture of sub-Gaussian data under \${\textbackslash}ell\_p\$ adversarial perturbations. Our result suggests that under moderate perturbations, adversarially trained linear classifiers can achieve the near-optimal standard and adversarial risks, despite overfitting the noisy training data. Numerical experiments validate our theoretical findings.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Chen, Jinghui and Cao, Yuan and Gu, Quanquan},
	month = dec,
	year = {2021},
	note = {arXiv:2112.15250 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{xu_wemix_2020,
	title = {{WeMix}: {How} to {Better} {Utilize} {Data} {Augmentation}},
	shorttitle = {{WeMix}},
	url = {http://arxiv.org/abs/2010.01267},
	doi = {10.48550/arXiv.2010.01267},
	abstract = {Data augmentation is a widely used training trick in deep learning to improve the network generalization ability. Despite many encouraging results, several recent studies did point out limitations of the conventional data augmentation scheme in certain scenarios, calling for a better theoretical understanding of data augmentation. In this work, we develop a comprehensive analysis that reveals pros and cons of data augmentation. The main limitation of data augmentation arises from the data bias, i.e. the augmented data distribution can be quite different from the original one. This data bias leads to a suboptimal performance of existing data augmentation methods. To this end, we develop two novel algorithms, termed "AugDrop" and "MixLoss", to correct the data bias in the data augmentation. Our theoretical analysis shows that both algorithms are guaranteed to improve the effect of data augmentation through the bias correction, which is further validated by our empirical studies. Finally, we propose a generic algorithm "WeMix" by combining AugDrop and MixLoss, whose effectiveness is observed from extensive empirical evaluations.},
	urldate = {2022-08-12},
	publisher = {arXiv},
	author = {Xu, Yi and Noy, Asaf and Lin, Ming and Qian, Qi and Li, Hao and Jin, Rong},
	month = oct,
	year = {2020},
	note = {arXiv:2010.01267 [cs, math, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{harris_fmix_2021,
	title = {{FMix}: {Enhancing} {Mixed} {Sample} {Data} {Augmentation}},
	shorttitle = {{FMix}},
	url = {https://openreview.net/forum?id=oev4KdikGjy},
	abstract = {Mixed Sample Data Augmentation (MSDA) has received increasing attention in recent years, with many successful variants such as MixUp and CutMix. We analyse MSDA from an information theoretic...},
	language = {en},
	urldate = {2022-08-03},
	author = {Harris, Ethan and Marcu, Antonia and Painter, Matthew and Niranjan, Mahesan and Prugel-Bennett, Adam and Hare, Jonathon},
	month = mar,
	year = {2021},
}

@inproceedings{takahashi_ricap_2018,
	title = {{RICAP}: {Random} {Image} {Cropping} and {Patching} {Data} {Augmentation} for {Deep} {CNNs}},
	shorttitle = {{RICAP}},
	url = {https://proceedings.mlr.press/v95/takahashi18a.html},
	abstract = {Deep convolutional neural networks (CNNs) have demonstrated remarkable results in image recognition owing to their rich expression ability and numerous parameters. However, an excessive expression ability compared to the variety of training images often has a risk of overfitting. Data augmentation techniques have been proposed to address this problem as they enrich datasets by flipping, cropping, resizing, and color-translating images. They enable deep CNNs to achieve an impressive performance. In this study, we propose a new data augmentation technique called {\textbackslash}emph\{random image cropping and patching\} ({\textbackslash}emph\{RICAP\}), which randomly crops four images and patches them to construct a new training image. Hence, RICAP randomly picks up subsets of original features among the four images and discard others, enriching the variety of training images. Also, RICAP mixes the class labels of the four images and enjoys a benefit similar to label smoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., shake-shake regularization model) and achieved a new state-of-the-art test error of {\textbackslash}textcolor\{red\}\{2.232.232.23\%\} on CIFAR-10 among competitive data augmentation techniques such as cutout and mixup. We also confirmed that deep CNNs with RICAP achieved better results on CIFAR-100 and ImageNet than those results obtained by other techniques.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {Proceedings of {The} 10th {Asian} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},
	month = nov,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {786--798},
}

@misc{mounsaveng_adversarial_2019,
	title = {Adversarial {Learning} of {General} {Transformations} for {Data} {Augmentation}},
	url = {http://arxiv.org/abs/1909.09801},
	doi = {10.48550/arXiv.1909.09801},
	abstract = {Data augmentation (DA) is fundamental against overfitting in large convolutional neural networks, especially with a limited training dataset. In images, DA is usually based on heuristic transformations, like geometric or color transformations. Instead of using predefined transformations, our work learns data augmentation directly from the training data by learning to transform images with an encoder-decoder architecture combined with a spatial transformer network. The transformed images still belong to the same class but are new, more complex samples for the classifier. Our experiments show that our approach is better than previous generative data augmentation methods, and comparable to predefined transformation methods when training an image classifier.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Mounsaveng, Saypraseuth and Vazquez, David and Ayed, Ismail Ben and Pedersoli, Marco},
	month = sep,
	year = {2019},
	note = {arXiv:1909.09801 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@inproceedings{cheung_modals_2022,
	title = {{MODALS}: {Modality}-agnostic {Automated} {Data} {Augmentation} in the {Latent} {Space}},
	shorttitle = {{MODALS}},
	url = {https://openreview.net/forum?id=XjYgR6gbCEc},
	abstract = {Data augmentation is an efficient way to expand a training dataset by creating additional artificial data. While data augmentation is found to be effective in improving the generalization...},
	language = {en},
	urldate = {2022-08-03},
	author = {Cheung, Tsz-Him and Yeung, Dit-Yan},
	month = feb,
	year = {2022},
}

@article{agarwal_cognitive_2021,
	title = {Cognitive data augmentation for adversarial defense via pixel masking},
	volume = {146},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865521000519},
	doi = {10.1016/j.patrec.2021.01.032},
	abstract = {The vulnerability of deep networks towards adversarial perturbations has motivated the researchers to design detection and mitigation algorithms. Inspired by the dropout and dropconnect algorithms as well as augmentation techniques, this paper presents “PixelMask” based data augmentation as an efficient method of reducing the sensitivity of convolutional neural networks (CNNs) towards adversarial attacks. In the proposed approach, samples generated using PixelMask are used as augmented data, which helps in learning robust CNN models. Experiments performed with multiple databases and architectures show that the proposed PixelMask based data augmentation approach improves the classification performance on adversarially perturbed images. The proposed defense mechanism can be applied effectively for different adversarial attacks and can easily be combined with any deep neural network (DNN) architecture to increase the robustness. The effectiveness of the proposed defense is demonstrated in gray-box, white-box, and unseen train-test attack scenarios. For example, on the CIFAR-10 database under adaptive attack (i.e., projected gradient descent), the proposed PixelMask is able to improve the recognition performance of CNN by at-least 22.69\%. Another advantage of the proposed algorithm over several existing defense algorithms is that the proposed defense either is able to retain or increase the classification accuracy of clean examples.},
	language = {en},
	urldate = {2022-08-03},
	journal = {Pattern Recognition Letters},
	author = {Agarwal, Akshay and Vatsa, Mayank and Singh, Richa and Ratha, Nalini},
	month = jun,
	year = {2021},
	keywords = {Adversarial attacks, Data augmentation, Deep learning},
	pages = {244--251},
}

@misc{eghbal-zadeh_data_2020,
	title = {On {Data} {Augmentation} and {Adversarial} {Risk}: {An} {Empirical} {Analysis}},
	shorttitle = {On {Data} {Augmentation} and {Adversarial} {Risk}},
	url = {http://arxiv.org/abs/2007.02650},
	doi = {10.48550/arXiv.2007.02650},
	abstract = {Data augmentation techniques have become standard practice in deep learning, as it has been shown to greatly improve the generalisation abilities of models. These techniques rely on different ideas such as invariance-preserving transformations (e.g, expert-defined augmentation), statistical heuristics (e.g, Mixup), and learning the data distribution (e.g, GANs). However, in the adversarial settings it remains unclear under what conditions such data augmentation methods reduce or even worsen the misclassification risk. In this paper, we therefore analyse the effect of different data augmentation techniques on the adversarial risk by three measures: (a) the well-known risk under adversarial attacks, (b) a new measure of prediction-change stress based on the Laplacian operator, and (c) the influence of training examples on prediction. The results of our empirical analysis disprove the hypothesis that an improvement in the classification performance induced by a data augmentation is always accompanied by an improvement in the risk under adversarial attack. Further, our results reveal that the augmented data has more influence than the non-augmented data, on the resulting models. Taken together, our results suggest that general-purpose data augmentations that do not take into the account the characteristics of the data and the task, must be applied with care.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Eghbal-zadeh, Hamid and Koutini, Khaled and Primus, Paul and Haunschmid, Verena and Lewandowski, Michal and Zellinger, Werner and Moser, Bernhard A. and Widmer, Gerhard},
	month = jul,
	year = {2020},
	note = {arXiv:2007.02650 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kim_local_2021,
	title = {Local {Augment}: {Utilizing} {Local} {Bias} {Property} of {Convolutional} {Neural} {Networks} for {Data} {Augmentation}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Local {Augment}},
	doi = {10.1109/ACCESS.2021.3050758},
	abstract = {Data augmentation is an effective way to increase the diversity of existing training datasets that result in improved generalization ability of convolutional neural networks (CNNs). The augmentation effect is usually global for the existing methods i.e., a single augmentation effect is applied to the whole image, thus limiting the diversity of local characteristics in augmented images. Moreover, the global augmentation effect does not support the most fundamental behavior of CNNs i.e., they focus more on local features (local texture, tiny noise etc.) than global shapes. We refer to this behavior as local bias property. In this paper, we propose a new data augmentation method, called Local Augment (LA), which highly alters the local bias property so that it can generate significantly diverse augmented images and offers the network with a better augmentation effect. First, we select few local patches in an image, then apply different types of augmentation strategies to each local patch. This augmentation process collapses the global structure of the object but creates locally diversified samples, which helps the network to learn the local bias property in a more generalized way. As a result, it increases the generalizability and the prediction accuracy of the network. To verify the effectiveness of the proposed method, we perform comprehensive experiments on image classification with benchmark datasets, where the proposed method outperforms the sate-of-the-art data augmentation techniques on ImageNet and STL10 and shows competitive performance on CIFAR100.},
	journal = {IEEE Access},
	author = {Kim, Youmin and Uddin, A. F. M. Shahab and Bae, Sung-Ho},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Image classification, Interpolation, Licenses, Robustness, Shape, Task analysis, Training, Training data, data augmentation, local bias property, multiple augmentation effects, overfitting},
	pages = {15191--15199},
}

@inproceedings{zhao_maximum-entropy_2020,
	title = {Maximum-{Entropy} {Adversarial} {Data} {Augmentation} for {Improved} {Generalization} and {Robustness}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/a5bfc9e07964f8dddeb95fc584cd965d-Abstract.html},
	abstract = {Adversarial data augmentation has shown promise for training robust deep neural networks against unforeseen data shifts or corruptions. However, it is difficult to define heuristics to generate effective fictitious target distributions containing "hard" adversarial perturbations that are largely different from the source distribution. In this paper, we propose a novel and effective regularization term for adversarial data augmentation. We theoretically derive it from the information bottleneck principle, which results in a maximum-entropy formulation. Intuitively, this regularization term encourages perturbing the underlying source distribution to enlarge predictive uncertainty of the current model, so that the generated "hard" adversarial perturbations can improve the model robustness during training. Experimental results on three standard benchmarks demonstrate that our method consistently outperforms the existing state of the art by a statistically significant margin.},
	urldate = {2022-08-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhao, Long and Liu, Ting and Peng, Xi and Metaxas, Dimitris},
	year = {2020},
	pages = {14435--14447},
}

@misc{yu_pda_2020,
	title = {{PDA}: {Progressive} {Data} {Augmentation} for {General} {Robustness} of {Deep} {Neural} {Networks}},
	shorttitle = {{PDA}},
	url = {http://arxiv.org/abs/1909.04839},
	doi = {10.48550/arXiv.1909.04839},
	abstract = {Adversarial images are designed to mislead deep neural networks (DNNs), attracting great attention in recent years. Although several defense strategies achieved encouraging robustness against adversarial samples, most of them fail to improve the robustness on common corruptions such as noise, blur, and weather/digital effects (e.g. frost, pixelate). To address this problem, we propose a simple yet effective method, named Progressive Data Augmentation (PDA), which enables general robustness of DNNs by progressively injecting diverse adversarial noises during training. In other words, DNNs trained with PDA are able to obtain more robustness against both adversarial attacks as well as common corruptions than the recent state-of-the-art methods. We also find that PDA is more efficient than prior arts and able to prevent accuracy drop on clean samples without being attacked. Furthermore, we theoretically show that PDA can control the perturbation bound and guarantee better generalization ability than existing work. Extensive experiments on many benchmarks such as CIFAR-10, SVHN, and ImageNet demonstrate that PDA significantly outperforms its counterparts in various experimental setups.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Yu, Hang and Liu, Aishan and Liu, Xianglong and Li, Gengchao and Luo, Ping and Cheng, Ran and Yang, Jichen and Zhang, Chongzhi},
	month = feb,
	year = {2020},
	note = {arXiv:1909.04839 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lopes_improving_2019,
	title = {Improving {Robustness} {Without} {Sacrificing} {Accuracy} with {Patch} {Gaussian} {Augmentation}},
	url = {http://arxiv.org/abs/1906.02611},
	doi = {10.48550/arXiv.1906.02611},
	abstract = {Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging. Prior work has argued that there is an inherent trade-off between robustness and accuracy, which is exemplified by standard data augment techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. To overcome this trade-off, we introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image. Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNetCommon Corruptions benchmarks while also improving accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise(similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). Finally, we show that Patch Gaussian can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment, and improves performance on the COCO object detection benchmark.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Lopes, Raphael Gontijo and Yin, Dong and Poole, Ben and Gilmer, Justin and Cubuk, Ekin D.},
	month = jun,
	year = {2019},
	note = {arXiv:1906.02611 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lemley_smart_2017,
	title = {Smart {Augmentation} {Learning} an {Optimal} {Data} {Augmentation} {Strategy}},
	volume = {5},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2017.2696121},
	abstract = {A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks. There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method, which we call smart augmentation and we show how to use it to increase the accuracy and reduce over fitting on a target network. Smart augmentation works, by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network. Smart augmentation has shown the potential to increase accuracy by demonstrably significant measures on all data sets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.},
	journal = {IEEE Access},
	author = {Lemley, Joseph and Bazrafkan, Shabab and Corcoran, Peter},
	year = {2017},
	note = {Conference Name: IEEE Access},
	keywords = {Artificial intelligence, Biological neural networks, Data models, Electronic mail, Informatics, Machine learning, Training, artificial neural networks, computer vision supervised learning, image databases, machine learning, machine learning algorithms},
	pages = {5858--5869},
}

@inproceedings{ratner_learning_2017,
	title = {Learning to {Compose} {Domain}-{Specific} {Transformations} for {Data} {Augmentation}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/f26dab9bf6a137c3b6782e562794c2f2-Abstract.html},
	abstract = {Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.},
	urldate = {2022-08-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ratner, Alexander J and Ehrenberg, Henry and Hussain, Zeshan and Dunnmon, Jared and Ré, Christopher},
	year = {2017},
}

@inproceedings{fawzi_adaptive_2016,
	title = {Adaptive data augmentation for image classification},
	doi = {10.1109/ICIP.2016.7533048},
	abstract = {Data augmentation is the process of generating samples by transforming training data, with the target of improving the accuracy and robustness of classifiers. In this paper, we propose a new automatic and adaptive algorithm for choosing the transformations of the samples used in data augmentation. Specifically, for each sample, our main idea is to seek a small transformation that yields maximal classification loss on the transformed sample. We employ a trust-region optimization strategy, which consists of solving a sequence of linear programs. Our data augmentation scheme is then integrated into a Stochastic Gradient Descent algorithm for training deep neural networks. We perform experiments on two datasets, and show that that the proposed scheme outperforms random data augmentation algorithms in terms of accuracy and robustness, while yielding comparable or superior results with respect to existing selective sampling approaches.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Fawzi, Alhussein and Samulowitz, Horst and Turaga, Deepak and Frossard, Pascal},
	month = sep,
	year = {2016},
	note = {ISSN: 2381-8549},
	keywords = {Approximation algorithms, Data augmentation, Neural networks, Optimization, Robustness, Training, Training data, Transforms, image robustness, transformation invariance, trust-region optimization},
	pages = {3688--3692},
}

@inproceedings{wang_removing_2022,
	title = {Removing {Batch} {Normalization} {Boosts} {Adversarial} {Training}},
	url = {https://proceedings.mlr.press/v162/wang22ap.html},
	abstract = {Adversarial training (AT) defends deep neural networks against adversarial attacks. One challenge that limits its practical application is the performance degradation on clean samples. A major bottleneck identified by previous works is the widely used batch normalization (BN), which struggles to model the different statistics of clean and adversarial training samples in AT. Although the dominant approach is to extend BN to capture this mixture of distribution, we propose to completely eliminate this bottleneck by removing all BN layers in AT. Our normalizer-free robust training (NoFrost) method extends recent advances in normalizer-free networks to AT for its unexplored advantage on handling the mixture distribution challenge. We show that NoFrost achieves adversarial robustness with only a minor sacrifice on clean sample accuracy. On ImageNet with ResNet50, NoFrost achieves \$74.06\%\$ clean accuracy, which drops merely \$2.00\%\$ from standard training. In contrast, BN-based AT obtains \$59.28\%\$ clean accuracy, suffering a significant \$16.78\%\$ drop from standard training. In addition, NoFrost achieves a \$23.56\%\$ adversarial robustness against PGD attack, which improves the \$13.57\%\$ robustness in BN-based AT. We observe better model smoothness and larger decision margins from NoFrost, which make the models less sensitive to input perturbations and thus more robust. Moreover, when incorporating more data augmentations into NoFrost, it achieves comprehensive robustness against multiple distribution shifts. Code and pre-trained models are public at https://github.com/amazon-research/normalizer-free-robust-training.},
	language = {en},
	urldate = {2022-08-02},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Haotao and Zhang, Aston and Zheng, Shuai and Shi, Xingjian and Li, Mu and Wang, Zhangyang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {23433--23445},
}

@misc{noauthor_kings_2022,
	title = {King's {Computational} {Research}, {Engineering} and {Technology} {Environment} ({CREATE})},
	url = {https://doi.org/10.18742/rnvf-m076},
	publisher = {King's College London},
	year = {2022},
}

@inproceedings{lee_smoothmix_2020,
	title = {{SmoothMix}: {A} {Simple} {Yet} {Effective} {Data} {Augmentation} to {Train} {Robust} {Classifiers}},
	shorttitle = {{SmoothMix}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Lee_SmoothMix_A_Simple_Yet_Effective_Data_Augmentation_to_Train_Robust_CVPRW_2020_paper.html},
	urldate = {2022-07-31},
	author = {Lee, Jin-Ha and Zaheer, Muhammad Zaigham and Astrid, Marcella and Lee, Seung-Ik},
	year = {2020},
	pages = {756--757},
}

@inproceedings{kar_3d_2022,
	title = {{3D} {Common} {Corruptions} and {Data} {Augmentation}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Kar_3D_Common_Corruptions_and_Data_Augmentation_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-31},
	author = {Kar, Oğuzhan Fatih and Yeo, Teresa and Atanov, Andrei and Zamir, Amir},
	year = {2022},
	pages = {18963--18974},
}

@inproceedings{hong_stylemix_2021,
	title = {{StyleMix}: {Separating} {Content} and {Style} for {Enhanced} {Data} {Augmentation}},
	shorttitle = {{StyleMix}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Hong_StyleMix_Separating_Content_and_Style_for_Enhanced_Data_Augmentation_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-07-31},
	author = {Hong, Minui and Choi, Jinwoo and Kim, Gunhee},
	year = {2021},
	pages = {14862--14870},
}

@inproceedings{xie_unsupervised_2020,
	title = {Unsupervised {Data} {Augmentation} for {Consistency} {Training}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/44feb0096faa8326192570788b38c1d1-Abstract.html},
	abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
	urldate = {2022-07-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
	year = {2020},
	pages = {6256--6268},
}

@inproceedings{zhang_defense_2019,
	title = {Defense {Against} {Adversarial} {Attacks} {Using} {Feature} {Scattering}-based {Adversarial} {Training}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/d8700cbd38cc9f30cecb34f0c195b137-Abstract.html},
	abstract = {We introduce a feature scattering-based adversarial training approach for improving model robustness against adversarial attacks.
Conventional adversarial training approaches leverage a supervised scheme (either targeted or non-targeted) in generating attacks for training, which typically suffer from issues such as label leaking as noted in recent works.
Differently, the proposed approach generates adversarial images for training through feature scattering in the latent space, which is unsupervised in nature and avoids label leaking. More importantly, this new approach generates perturbed images in a collaborative fashion, taking the inter-sample relationships into consideration. We conduct analysis on model robustness and demonstrate the effectiveness of the proposed approach  through extensively experiments on different datasets compared with state-of-the-art approaches.},
	urldate = {2022-07-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Haichao and Wang, Jianyu},
	year = {2019},
}

@misc{erichson_noisymix_2022,
	title = {{NoisyMix}: {Boosting} {Model} {Robustness} to {Common} {Corruptions}},
	shorttitle = {{NoisyMix}},
	url = {http://arxiv.org/abs/2202.01263},
	doi = {10.48550/arXiv.2202.01263},
	abstract = {For many real-world applications, obtaining stable and robust statistical performance is more important than simply achieving state-of-the-art predictive test accuracy, and thus robustness of neural networks is an increasingly important topic. Relatedly, data augmentation schemes have been shown to improve robustness with respect to input perturbations and domain shifts. Motivated by this, we introduce NoisyMix, a novel training scheme that promotes stability as well as leverages noisy augmentations in input and feature space to improve both model robustness and in-domain accuracy. NoisyMix produces models that are consistently more robust and that provide well-calibrated estimates of class membership probabilities. We demonstrate the benefits of NoisyMix on a range of benchmark datasets, including ImageNet-C, ImageNet-R, and ImageNet-P. Moreover, we provide theory to understand implicit regularization and robustness of NoisyMix.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Erichson, N. Benjamin and Lim, Soon Hoe and Xu, Winnie and Utrera, Francisco and Cao, Ziang and Mahoney, Michael W.},
	month = may,
	year = {2022},
	note = {arXiv:2202.01263 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{yu_understanding_2022,
	title = {Understanding {Robust} {Overfitting} of {Adversarial} {Training} and {Beyond}},
	url = {https://proceedings.mlr.press/v162/yu22b.html},
	abstract = {Robust overfitting widely exists in adversarial training of deep networks. The exact underlying reasons for this are still not completely understood. Here, we explore the causes of robust overfitting by comparing the data distribution of non-overfit (weak adversary) and overfitted (strong adversary) adversarial training, and observe that the distribution of the adversarial data generated by weak adversary mainly contain small-loss data. However, the adversarial data generated by strong adversary is more diversely distributed on the large-loss data and the small-loss data. Given these observations, we further designed data ablation adversarial training and identify that some small-loss data which are not worthy of the adversary strength cause robust overfitting in the strong adversary mode. To relieve this issue, we propose minimum loss constrained adversarial training (MLCAT): in a minibatch, we learn large-loss data as usual, and adopt additional measures to increase the loss of the small-loss data. Technically, MLCAT hinders data fitting when they become easy to learn to prevent robust overfitting; philosophically, MLCAT reflects the spirit of turning waste into treasure and making the best use of each adversarial data; algorithmically, we designed two realizations of MLCAT, and extensive experiments demonstrate that MLCAT can eliminate robust overfitting and further boost adversarial robustness.},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yu, Chaojian and Han, Bo and Shen, Li and Yu, Jun and Gong, Chen and Gong, Mingming and Liu, Tongliang},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {25595--25610},
}

@inproceedings{park_reliably_2021,
	address = {Montreal, QC, Canada},
	title = {Reliably fast adversarial training via latent adversarial perturbation},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710792/},
	doi = {10.1109/ICCV48922.2021.00766},
	abstract = {While multi-step adversarial training is widely popular as an effective defense method against strong adversarial attacks, its computational cost is notoriously expensive, compared to standard training. Several single-step adversarial training methods have been proposed to mitigate the abovementioned overhead cost; however, their performance is not sufficiently reliable depending on the optimization setting. To overcome such limitations, we deviate from the existing input-space-based adversarial training regime and propose a single-step latent adversarial training method (SLAT), which leverages the gradients of latent representation as the latent adversarial perturbation. We demonstrate that the {\textbackslash}ell \_1 norm of feature gradients is implicitly regularized through the adopted latent perturbation, thereby recovering local linearity and ensuring reliable performance, compared to the existing single-step adversarial training methods. Because latent perturbation is based on the gradients of the latent representations which can be obtained for free in the process of input gradients computation, the proposed method costs roughly the same time as the fast gradient sign method. Experiment results demonstrate that the proposed method, despite its structural simplicity, outperforms state-of-the-art accelerated adversarial training methods.},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Park, Geon Yeong and Wan Lee, Sang},
	month = oct,
	year = {2021},
	pages = {7738--7747},
}

@inproceedings{zhang_towards_2021,
	title = {Towards {Better} {Robust} {Generalization} with {Shift} {Consistency} {Regularization}},
	url = {https://proceedings.mlr.press/v139/zhang21p.html},
	abstract = {While adversarial training becomes one of the most promising defending approaches against adversarial attacks for deep neural networks, the conventional wisdom through robust optimization may usually not guarantee good generalization for robustness. Concerning with robust generalization over unseen adversarial data, this paper investigates adversarial training from a novel perspective of shift consistency in latent space. We argue that the poor robust generalization of adversarial training is owing to the significantly dispersed latent representations generated by training and test adversarial data, as the adversarial perturbations push the latent features of natural examples in the same class towards diverse directions. This is underpinned by the theoretical analysis of the robust generalization gap, which is upper-bounded by the standard one over the natural data and a term of feature inconsistent shift caused by adversarial perturbation \{–\} a measure of latent dispersion. Towards better robust generalization, we propose a new regularization method \{–\} shift consistency regularization (SCR) \{–\} to steer the same-class latent features of both natural and adversarial data into a common direction during adversarial training. The effectiveness of SCR in adversarial training is evaluated through extensive experiments over different datasets, such as CIFAR-10, CIFAR-100, and SVHN, against several competitive methods.},
	language = {en},
	urldate = {2022-07-27},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Shufei and Qian, Zhuang and Huang, Kaizhu and Wang, Qiufeng and Zhang, Rui and Yi, Xinping},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12524--12534},
}

@article{montavon_explaining_2017,
	title = {Explaining nonlinear classification decisions with deep {Taylor} decomposition},
	volume = {65},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320316303582},
	doi = {10.1016/j.patcog.2016.11.008},
	abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
	language = {en},
	urldate = {2022-07-26},
	journal = {Pattern Recognition},
	author = {Montavon, Grégoire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and Müller, Klaus-Robert},
	month = may,
	year = {2017},
	keywords = {Deep neural networks, Heatmapping, Image recognition, Relevance propagation, Taylor decomposition},
	pages = {211--222},
}

@inproceedings{liu_practical_2022,
	title = {Practical {Evaluation} of {Adversarial} {Robustness} via {Adaptive} {Auto} {Attack}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Practical_Evaluation_of_Adversarial_Robustness_via_Adaptive_Auto_Attack_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-26},
	author = {Liu, Ye and Cheng, Yaya and Gao, Lianli and Liu, Xianglong and Zhang, Qilong and Song, Jingkuan},
	year = {2022},
	pages = {15105--15114},
}

@article{le_tiny_2015,
	title = {Tiny imagenet visual recognition challenge},
	volume = {7},
	number = {7},
	journal = {CS 231N},
	author = {Le, Ya and Yang, Xuan},
	year = {2015},
	pages = {3},
}

@misc{kim_torchattacks_2021,
	title = {Torchattacks: {A} {PyTorch} {Repository} for {Adversarial} {Attacks}},
	shorttitle = {Torchattacks},
	url = {http://arxiv.org/abs/2010.01950},
	doi = {10.48550/arXiv.2010.01950},
	abstract = {Torchattacks is a PyTorch library that contains adversarial attacks to generate adversarial examples and to verify the robustness of deep learning models. The code can be found at https://github.com/Harry24k/adversarial-attacks-pytorch.},
	urldate = {2022-07-09},
	publisher = {arXiv},
	author = {Kim, Hoki},
	month = feb,
	year = {2021},
	note = {arXiv:2010.01950 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{chen_efficient_2022,
	title = {Efficient {Robust} {Training} via {Backward} {Smoothing}},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20571},
	doi = {10.1609/aaai.v36i6.20571},
	abstract = {Adversarial training is so far the most effective strategy in defending against adversarial examples. However, it suffers from high computational costs due to the iterative adversarial attacks in each training step. Recent studies show that it is possible to achieve fast Adversarial Training by performing a single-step attack with random initialization. However, such an approach still lags behind state-of-the-art adversarial training algorithms on both stability and model robustness. In this work, we develop a new understanding towards Fast Adversarial Training, by viewing random initialization as performing randomized smoothing for better optimization of the inner maximization problem. Following this new perspective, we also propose a new initialization strategy, backward smoothing, to further improve the stability and model robustness over single-step robust training methods.
 Experiments on multiple benchmarks demonstrate that our method achieves similar model robustness as the original TRADES method while using much less training time ({\textasciitilde}3x improvement with the same training schedule).},
	language = {en},
	number = {6},
	urldate = {2022-07-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Jinghui and Cheng, Yu and Gan, Zhe and Gu, Quanquan and Liu, Jingjing},
	month = jun,
	year = {2022},
	note = {Number: 6},
	keywords = {Machine Learning (ML)},
	pages = {6222--6230},
}

@inproceedings{li_subspace_2022,
	title = {Subspace {Adversarial} {Training}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Li_Subspace_Adversarial_Training_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-07-06},
	author = {Li, Tao and Wu, Yingwen and Chen, Sizhe and Fang, Kun and Huang, Xiaolin},
	year = {2022},
	pages = {13409--13418},
}

@article{mandery_unifying_2016,
	title = {Unifying {Representations} and {Large}-{Scale} {Whole}-{Body} {Motion} {Databases} for {Studying} {Human} {Motion}},
	volume = {32},
	issn = {1941-0468},
	doi = {10.1109/TRO.2016.2572685},
	abstract = {Large-scale human motion databases are key for research questions ranging from human motion analysis and synthesis, biomechanics of human motion, data-driven learning of motion primitives, and rehabilitation robotics to the design of humanoid robots and wearable robots such as exoskeletons. In this paper we present a large-scale database of whole-body human motion with methods and tools, which allows a unifying representation of captured human motion, and efficient search in the database, as well as the transfer of subject-specific motions to robots with different embodiments. To this end, captured subject-specific motion is normalized regarding the subject's height and weight by using a reference kinematics and dynamics model of the human body, the master motor map (MMM). In contrast with previous approaches and human motion databases, the motion data in our database consider not only the motions of the human subject but the position and motion of objects with which the subject is interacting as well. In addition to the description of the MMM reference model, we present procedures and techniques for the systematic recording, labeling, and organization of human motion capture data, object motions as well as the subject–object relations. To allow efficient search for certain motion types in the database, motion recordings are manually annotated with motion description tags organized in a tree structure. We demonstrate the transfer of human motion to humanoid robots and provide several examples of motion analysis using the database.},
	number = {4},
	journal = {IEEE Transactions on Robotics},
	author = {Mandery, Christian and Terlemez, Ömer and Do, Martin and Vahrenkamp, Nikolaus and Asfour, Tamim},
	month = aug,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Animation, Data models, Databases, Humanoid robots, Legged locomotion, Motion measurement, models of the human body, whole-body human motion databases},
	pages = {796--809},
}

@inproceedings{cubuk_autoaugment_2019,
	address = {Long Beach, CA, USA},
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{AutoAugment}},
	url = {https://ieeexplore.ieee.org/document/8953317/},
	doi = {10.1109/CVPR.2019.00020},
	abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classiﬁers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many subpolicies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to ﬁnd the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On CIFAR-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-theart. Augmentation policies we ﬁnd are transferable between datasets. The policy learned on ImageNet transfers well to achieve signiﬁcant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	month = jun,
	year = {2019},
	pages = {113--123},
}

@inproceedings{bs_regularizer_2019,
	address = {Long Beach, CA, USA},
	title = {Regularizer to {Mitigate} {Gradient} {Masking} {Effect} {During} {Single}-{Step} {Adversarial} {Training}},
	isbn = {978-1-72812-506-0},
	url = {https://ieeexplore.ieee.org/document/9025502/},
	doi = {10.1109/CVPRW.2019.00014},
	abstract = {Neural networks are susceptible to adversarial samples: samples with imperceptible noise, crafted to manipulate network’s prediction. In order to learn robust models, a training procedure, called Adversarial Training has been introduced. During adversarial training, models are trained with mini-batch containing adversarial samples. In order to scale adversarial training for large datasets and networks, fast and simple methods (e.g., FGSM:Fast Gradient Sign Method) of generating adversarial samples are used while training. It has been shown that models trained using single-step adversarial training methods (i.e., adversarial samples generated using non-iterative methods such as FGSM) are not robust, instead they learn to generate weaker adversaries by masking the gradients. In this work, we propose a regularization term in the training loss, to mitigate the effect of gradient masking during single-step adversarial training. The proposed regularization term causes training loss to increase when the distance between logits (i.e., pre-softmax output of a classiﬁer) for FGSM and R-FGSM (small random noise is added to the clean sample before computing its FGSM sample) adversaries of a clean sample becomes large. The proposed single-step adversarial training is faster than computationally expensive state-of-the-art PGD adversarial training method, and also achieves on par results.},
	language = {en},
	urldate = {2022-06-07},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {B.S., Vivek and Baburaj, Arya and Babu, R. Venkatesh},
	month = jun,
	year = {2019},
	pages = {66--73},
}

@inproceedings{sriramanan_guided_2020,
	title = {Guided {Adversarial} {Attack} for {Evaluating} and {Enhancing} {Adversarial} {Defenses}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/ea3ed20b6b101a09085ef09c97da1597-Abstract.html},
	abstract = {Advances in the development of adversarial attacks have been fundamental to the progress of adversarial defense research. Efficient and effective attacks are crucial for reliable evaluation of defenses, and also for developing robust models. Adversarial attacks are often generated by maximizing standard losses such as the cross-entropy loss or maximum-margin loss within a constraint set using Projected Gradient Descent (PGD). In this work, we introduce a relaxation term to the standard loss, that finds more suitable gradient-directions, increases attack efficacy and leads to more efficient adversarial training. We propose Guided Adversarial Margin Attack (GAMA), which utilizes function mapping of the clean image to guide the generation of adversaries, thereby resulting in stronger attacks. We evaluate our attack against multiple defenses and show improved performance when compared to existing attacks. Further, we propose Guided Adversarial Training (GAT), which achieves state-of-the-art performance amongst single-step defenses by utilizing the proposed relaxation term for both attack generation and training.},
	urldate = {2022-06-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sriramanan, Gaurang and Addepalli, Sravanti and Baburaj, Arya and R, Venkatesh Babu},
	year = {2020},
	pages = {20297--20308},
}

@inproceedings{chen_sparsity_2022,
	title = {Sparsity {Winning} {Twice}: {Better} {Robust} {Generalization} from {More} {Efficient} {Training}},
	shorttitle = {Sparsity {Winning} {Twice}},
	url = {https://openreview.net/forum?id=SYuJXrXq8tw},
	abstract = {Recent studies demonstrate the deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more...},
	language = {en},
	urldate = {2022-04-06},
	author = {Chen, Tianlong and Zhang, Zhenyu and Wang, Pengjun and Balachandra, Santosh and Ma, Haoyu and Wang, Zehao and Wang, Zhangyang},
	year = {2022},
}

@techreport{rebuffi_fixing_2021,
	title = {Fixing {Data} {Augmentation} to {Improve} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2103.01946},
	abstract = {Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on both heuristics-driven and data-driven augmentations as a means to reduce robust overfitting. First, we demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Second, we explore how state-of-the-art generative models can be leveraged to artificially increase the size of the training set and further improve adversarial robustness. Finally, we evaluate our approach on CIFAR-10 against \${\textbackslash}ell\_{\textbackslash}infty\$ and \${\textbackslash}ell\_2\$ norm-bounded perturbations of size \${\textbackslash}epsilon = 8/255\$ and \${\textbackslash}epsilon = 128/255\$, respectively. We show large absolute improvements of +7.06\% and +5.88\% in robust accuracy compared to previous state-of-the-art methods. In particular, against \${\textbackslash}ell\_{\textbackslash}infty\$ norm-bounded perturbations of size \${\textbackslash}epsilon = 8/255\$, our model reaches 64.20\% robust accuracy without using any external data, beating most prior works that use external data.},
	number = {arXiv:2103.01946},
	urldate = {2022-06-05},
	institution = {arXiv},
	author = {Rebuffi, Sylvestre-Alvise and Gowal, Sven and Calian, Dan A. and Stimberg, Florian and Wiles, Olivia and Mann, Timothy},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2103.01946},
	note = {arXiv:2103.01946 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{dong_boosting_2018,
	address = {Salt Lake City, UT},
	title = {Boosting {Adversarial} {Attacks} with {Momentum}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8579055/},
	doi = {10.1109/CVPR.2018.00957},
	abstract = {Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the ﬁrst places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.},
	language = {en},
	urldate = {2022-05-30},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
	month = jun,
	year = {2018},
	pages = {9185--9193},
}

@techreport{yao_taking_2019,
	title = {Taking {Human} out of {Learning} {Applications}: {A} {Survey} on {Automated} {Machine} {Learning}},
	shorttitle = {Taking {Human} out of {Learning} {Applications}},
	url = {http://arxiv.org/abs/1810.13306},
	abstract = {Machine learning techniques have deeply rooted in our everyday life. However, since it is knowledge- and labor-intensive to pursue good learning performance, human experts are heavily involved in every aspect of machine learning. In order to make machine learning techniques easier to apply and reduce the demand for experienced human experts, automated machine learning (AutoML) has emerged as a hot topic with both industrial and academic interest. In this paper, we provide an up to date survey on AutoML. First, we introduce and define the AutoML problem, with inspiration from both realms of automation and machine learning. Then, we propose a general AutoML framework that not only covers most existing approaches to date but also can guide the design for new methods. Subsequently, we categorize and review the existing works from two aspects, i.e., the problem setup and the employed techniques. Finally, we provide a detailed analysis of AutoML approaches and explain the reasons underneath their successful applications. We hope this survey can serve as not only an insightful guideline for AutoML beginners but also an inspiration for future research.},
	number = {arXiv:1810.13306},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Yao, Quanming and Wang, Mengshuo and Chen, Yuqiang and Dai, Wenyuan and Li, Yu-Feng and Tu, Wei-Wei and Yang, Qiang and Yu, Yang},
	month = dec,
	year = {2019},
	doi = {10.48550/arXiv.1810.13306},
	note = {arXiv:1810.13306 [cs, stat]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{bischl_hyperparameter_2021,
	title = {Hyperparameter {Optimization}: {Foundations}, {Algorithms}, {Best} {Practices} and {Open} {Challenges}},
	shorttitle = {Hyperparameter {Optimization}},
	url = {http://arxiv.org/abs/2107.05847},
	abstract = {Most machine learning algorithms are configured by one or several hyperparameters that must be carefully chosen and often considerably impact performance. To avoid a time consuming and unreproducible manual trial-and-error process to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods, e.g., based on resampling error estimation for supervised machine learning, can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods such as grid or random search, evolutionary algorithms, Bayesian optimization, Hyperband and racing. It gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with ML pipelines, runtime improvements, and parallelization. This work is accompanied by an appendix that contains information on specific software packages in R and Python, as well as information and recommended hyperparameter search spaces for specific learning algorithms. We also provide notebooks that demonstrate concepts from this work as supplementary files.},
	number = {arXiv:2107.05847},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2107.05847},
	note = {arXiv:2107.05847 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{elsken_neural_2019,
	title = {Neural architecture search: a survey},
	volume = {20},
	issn = {1532-4435},
	shorttitle = {Neural architecture search},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	month = jan,
	year = {2019},
	keywords = {autoDL, autoML, neural architecture search, performance estimation strategy, search space design, search strategy},
	pages = {1997--2017},
}

@incollection{feragen_enabling_2021,
	address = {Cham},
	title = {Enabling {Data} {Diversity}: {Efficient} {Automatic} {Augmentation} via {Regularized} {Adversarial} {Training}},
	volume = {12729},
	isbn = {978-3-030-78190-3 978-3-030-78191-0},
	shorttitle = {Enabling {Data} {Diversity}},
	url = {https://link.springer.com/10.1007/978-3-030-78191-0_7},
	abstract = {Data augmentation has proved extremely useful by increasing training data variance to alleviate overﬁtting and improve deep neural networks’ generalization performance. In medical image analysis, a welldesigned augmentation policy usually requires much expert knowledge and is diﬃcult to generalize to multiple tasks due to the vast discrepancies among pixel intensities, image appearances, and object shapes in diﬀerent medical tasks. To automate medical data augmentation, we propose a regularized adversarial training framework via two min-max objectives and three diﬀerentiable augmentation models covering aﬃne transformation, deformation, and appearance changes. Our method is more automatic and eﬃcient than previous automatic augmentation methods, which still rely on pre-deﬁned operations with human-speciﬁed ranges and costly bi-level optimization. Extensive experiments demonstrated that our approach, with less training overhead, achieves superior performance over state-of-the-art auto-augmentation methods on both tasks of 2D skin cancer classiﬁcation and 3D organs-at-risk segmentation.},
	language = {en},
	urldate = {2022-05-26},
	booktitle = {Information {Processing} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Gao, Yunhe and Tang, Zhiqiang and Zhou, Mu and Metaxas, Dimitris},
	editor = {Feragen, Aasa and Sommer, Stefan and Schnabel, Julia and Nielsen, Mads},
	year = {2021},
	doi = {10.1007/978-3-030-78191-0_7},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {85--97},
}

@inproceedings{peng_jointly_2018,
	address = {Salt Lake City, UT},
	title = {Jointly {Optimize} {Data} {Augmentation} and {Network} {Training}: {Adversarial} {Data} {Augmentation} in {Human} {Pose} {Estimation}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Jointly {Optimize} {Data} {Augmentation} and {Network} {Training}},
	url = {https://ieeexplore.ieee.org/document/8578335/},
	doi = {10.1109/CVPR.2018.00237},
	language = {en},
	urldate = {2022-05-26},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Peng, Xi and Tang, Zhiqiang and Yang, Fei and Feris, Rogerio S. and Metaxas, Dimitris},
	month = jun,
	year = {2018},
	pages = {2226--2234},
}

@inproceedings{zoph_learning_2018,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.html},
	urldate = {2022-05-25},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	year = {2018},
	pages = {8697--8710},
}

@article{tkach_online_2017,
	title = {Online generative model personalization for hand tracking},
	volume = {36},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3130800.3130830},
	doi = {10.1145/3130800.3130830},
	abstract = {We present a new algorithm for real-time hand tracking on commodity depth-sensing devices. Our method does not require a user-specific calibration session, but rather learns the geometry as the user performs live in front of the camera, thus enabling seamless virtual interaction at the consumer level. The key novelty in our approach is an online optimization algorithm that jointly estimates pose and shape in each frame, and determines the uncertainty in such estimates. This knowledge allows the algorithm to integrate per-frame estimates over time, and build a personalized geometric model of the captured user. Our approach can easily be integrated in state-of-the-art continuous generative motion tracking software. We provide a detailed evaluation that shows how our approach achieves accurate motion tracking for real-time applications, while significantly simplifying the workflow of accurate hand performance capture. We also provide quantitative evaluation datasets at http://gfx.uvic.ca/datasets/handy},
	number = {6},
	urldate = {2022-04-19},
	journal = {ACM Transactions on Graphics},
	author = {Tkach, Anastasia and Tagliasacchi, Andrea and Remelli, Edoardo and Pauly, Mark and Fitzgibbon, Andrew},
	month = nov,
	year = {2017},
	keywords = {articulated registration, generative tracking, motion capture, real-time calibration, real-time hand tracking},
	pages = {243:1--243:11},
}

@article{glauser_deformation_2019,
	title = {Deformation {Capture} via {Soft} and {Stretchable} {Sensor} {Arrays}},
	volume = {38},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3311972},
	doi = {10.1145/3311972},
	abstract = {We propose a hardware and software pipeline to fabricate flexible wearable sensors and use them to capture deformations without line-of-sight. Our first contribution is a low-cost fabrication pipeline to embed multiple aligned conductive layers with complex geometries into silicone compounds. Overlapping conductive areas from separate layers form local capacitors that measure dense area changes. Contrary to existing fabrication methods, the proposed technique only requires hardware that is readily available in modern fablabs. While area measurements alone are not enough to reconstruct the full 3D deformation of a surface, they become sufficient when paired with a data-driven prior. A novel semi-automatic tracking algorithm, based on an elastic surface geometry deformation, allows us to capture ground-truth data with an optical mocap system, even under heavy occlusions or partially unobservable markers. The resulting dataset is used to train a regressor based on deep neural networks, directly mapping the area readings to global positions of surface vertices. We demonstrate the flexibility and accuracy of the proposed hardware and software in a series of controlled experiments and design a prototype of wearable wrist, elbow, and biceps sensors, which do not require line-of-sight and can be worn below regular clothing.},
	number = {2},
	urldate = {2022-04-19},
	journal = {ACM Transactions on Graphics},
	author = {Glauser, Oliver and Panozzo, Daniele and Hilliges, Otmar and Sorkine-Hornung, Olga},
	month = mar,
	year = {2019},
	keywords = {Deformation capture, capacitive, sensor array, stretchable},
	pages = {16:1--16:16},
}

@article{glauser_interactive_2019,
	title = {Interactive hand pose estimation using a stretch-sensing soft glove},
	volume = {38},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3306346.3322957},
	doi = {10.1145/3306346.3322957},
	abstract = {We propose a stretch-sensing soft glove to interactively capture hand poses with high accuracy and without requiring an external optical setup. We demonstrate how our device can be fabricated and calibrated at low cost, using simple tools available in most fabrication labs. To reconstruct the pose from the capacitive sensors embedded in the glove, we propose a deep network architecture that exploits the spatial layout of the sensor itself. The network is trained only once, using an inexpensive off-the-shelf hand pose reconstruction system to gather the training data. The per-user calibration is then performed on-the-fly using only the glove. The glove's capabilities are demonstrated in a series of ablative experiments, exploring different models and calibration methods. Comparing against commercial data gloves, we achieve a 35\% improvement in reconstruction accuracy.},
	number = {4},
	urldate = {2022-04-19},
	journal = {ACM Transactions on Graphics},
	author = {Glauser, Oliver and Wu, Shihao and Panozzo, Daniele and Hilliges, Otmar and Sorkine-Hornung, Olga},
	month = jul,
	year = {2019},
	keywords = {data glove, hand tracking, sensor array, stretch-sensing},
	pages = {41:1--41:15},
}

@article{chen_survey_2020,
	title = {A {Survey} on {Hand} {Pose} {Estimation} with {Wearable} {Sensors} and {Computer}-{Vision}-{Based} {Methods}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/4/1074},
	doi = {10.3390/s20041074},
	abstract = {Real-time sensing and modeling of the human body, especially the hands, is an important research endeavor for various applicative purposes such as in natural human computer interactions. Hand pose estimation is a big academic and technical challenge due to the complex structure and dexterous movement of human hands. Boosted by advancements from both hardware and artificial intelligence, various prototypes of data gloves and computer-vision-based methods have been proposed for accurate and rapid hand pose estimation in recent years. However, existing reviews either focused on data gloves or on vision methods or were even based on a particular type of camera, such as the depth camera. The purpose of this survey is to conduct a comprehensive and timely review of recent research advances in sensor-based hand pose estimation, including wearable and vision-based solutions. Hand kinematic models are firstly discussed. An in-depth review is conducted on data gloves and vision-based sensor systems with corresponding modeling methods. Particularly, this review also discusses deep-learning-based methods, which are very promising in hand pose estimation. Moreover, the advantages and drawbacks of the current hand gesture estimation methods, the applicative scope, and related challenges are also discussed.},
	language = {en},
	number = {4},
	urldate = {2022-04-19},
	journal = {Sensors},
	author = {Chen, Weiya and Yu, Chenchen and Tu, Chenyu and Lyu, Zehua and Tang, Jing and Ou, Shiqi and Fu, Yan and Xue, Zhidong},
	month = jan,
	year = {2020},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {computer vision, data gloves, deep learning, hand pose estimation, human–computer interaction, wearable devices},
	pages = {1074},
}

@inproceedings{yuan_bighand22m_2017,
	address = {Honolulu, HI},
	title = {{BigHand2}.{2M} {Benchmark}: {Hand} {Pose} {Dataset} and {State} of the {Art} {Analysis}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {{BigHand2}.{2M} {Benchmark}},
	url = {http://ieeexplore.ieee.org/document/8099762/},
	doi = {10.1109/CVPR.2017.279},
	abstract = {In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difﬁculty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a signiﬁcantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate signiﬁcant improvements in cross-benchmark performance. We also show signiﬁcant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.},
	language = {en},
	urldate = {2022-04-19},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yuan, Shanxin and Ye, Qi and Stenger, Bjorn and Jain, Siddhant and Kim, Tae-Kyun},
	month = jul,
	year = {2017},
	pages = {2605--2613},
}

@inproceedings{zhao_differentiable_2020,
	title = {Differentiable {Augmentation} for {Data}-{Efficient} {GAN} {Training}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/55479c55ebd1efd3ff125f1337100388-Abstract.html},
	abstract = {The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminatorsis memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve astate-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128×128 and 2-4× reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore, with only 20\% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at https://github.com/mit-han-lab/data-efficient-gans.},
	urldate = {2022-04-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song},
	year = {2020},
	pages = {7559--7570},
}

@article{antoniou_data_2018,
	title = {Data {Augmentation} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1711.04340},
	abstract = {Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13\% increase in accuracy in the low-data regime experiments in Omniglot (from 69\% to 82\%), EMNIST (73.9\% to 76\%) and VGG-Face (4.5\% to 12\%); in Matching Networks for Omniglot we observe an increase of 0.5\% (from 96.9\% to 97.4\%) and an increase of 1.8\% in EMNIST (from 59.5\% to 61.3\%).},
	urldate = {2022-04-09},
	journal = {arXiv:1711.04340 [cs, stat]},
	author = {Antoniou, Antreas and Storkey, Amos and Edwards, Harrison},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.04340},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{ortiz_depth_2018,
	title = {Depth {Data} {Error} {Modeling} of the {ZED} {3D} {Vision} {Sensor} from {Stereolabs}},
	volume = {17},
	copyright = {Copyright (c)},
	issn = {1577-5097},
	url = {https://raco.cat/index.php/ELCVIA/article/view/v17-n1-ortiz},
	abstract = {The ZED camera is binocular vision system that can be used to provide a 3D perception of the world. It can be applied in autonomous robot navigation, virtual reality, tracking, motion analysis and so on. This paper proposes a mathematical error model for depth data estimated by the ZED camera with its several resolutions of operation. For doing that, the ZED is attached to a Nvidia Jetson TK1 board providing an embedded system that is used for processing raw data acquired by ZED from a 3D checkerboard. Corners are extracted from the checkerboard using RGB data, and a 3D reconstruction is done for these points using disparity data calculated from the ZED camera, coming up with a partially ordered, and regularly distributed (in 3D space) point cloud of corners with given coordinates, which are computed by the device software. These corners also have their ideal world (3D) positions known with respect to the coordinate frame origin that is empirically set in the pattern. Both given (computed)  coordinates from the camera’s data and known (ideal) coordinates of a corner can, thus, be compared for estimating the error between the given and ideal point locations of the detected corner cloud. Subsequently, using a curve fitting technique, we obtain the equations that model the RMS (Root Mean Square) error. This procedure is repeated for several resolutions of the ZED sensor, and at several distances. Results showed its best effectiveness with a maximum distance of approximately sixteen meters, in real time, which allows its use in robotic or other online applications.},
	language = {eng},
	number = {1},
	urldate = {2022-04-09},
	journal = {ELCVIA: electronic letters on computer vision and image analysis},
	author = {Ortiz, Luis Enrique and Cabrera, Viviana Elizabeth and Goncalves, Luiz M. G.},
	month = jun,
	year = {2018},
	note = {Number: 1},
	keywords = {3D and Stereo, Sensor Systems},
	pages = {1--15},
}

@article{tadic_perspectives_2022,
	title = {Perspectives of {RealSense} and {ZED} {Depth} {Sensors} for {Robotic} {Vision} {Applications}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-1702},
	url = {https://www.mdpi.com/2075-1702/10/3/183},
	doi = {10.3390/machines10030183},
	abstract = {This review paper presents an overview of depth cameras. Our goal is to describe the features and capabilities of the introduced depth sensors in order to determine their possibilities in robotic applications, focusing on objects that might appear in applications with high accuracy requirements. A series of experiments was conducted, and various depth measuring conditions were examined in order to compare the measurement results of all the depth cameras. Based on the results, all the examined depth sensors were appropriate for applications where obstacle avoidance and robot spatial orientation were required in coexistence with image vision algorithms. In robotic vision applications where high accuracy and precision were obligatory, the ZED depth sensors achieved better measurement results.},
	language = {en},
	number = {3},
	urldate = {2022-04-09},
	journal = {Machines},
	author = {Tadic, Vladimir and Toth, Attila and Vizvari, Zoltan and Klincsik, Mihaly and Sari, Zoltan and Sarcevic, Peter and Sarosi, Jozsef and Biro, Istvan},
	month = mar,
	year = {2022},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {RealSense, ZED, ZED 2i, depth map, depth sensors, robotic applications},
	pages = {183},
}

@article{caeiro-rodriguez_systematic_2021,
	title = {A {Systematic} {Review} of {Commercial} {Smart} {Gloves}: {Current} {Status} and {Applications}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {A {Systematic} {Review} of {Commercial} {Smart} {Gloves}},
	url = {https://www.mdpi.com/1424-8220/21/8/2667},
	doi = {10.3390/s21082667},
	abstract = {Smart gloves have been under development during the last 40 years to support human-computer interaction based on hand and finger movement. Despite the many devoted efforts and the multiple advances in related areas, these devices have not become mainstream yet. Nevertheless, during recent years, new devices with improved features have appeared, being used for research purposes too. This paper provides a review of current commercial smart gloves focusing on three main capabilities: (i) hand and finger pose estimation and motion tracking, (ii) kinesthetic feedback, and (iii) tactile feedback. For the first capability, a detailed reference model of the hand and finger basic movements (known as degrees of freedom) is proposed. Based on the PRISMA guidelines for systematic reviews for the period 2015–2021, 24 commercial smart gloves have been identified, while many others have been discarded because they did not meet the inclusion criteria: currently active commercial and fully portable smart gloves providing some of the three main capabilities for the whole hand. The paper reviews the technologies involved, main applications and it discusses about the current state of development. Reference models to support end users and researchers comparing and selecting the most appropriate devices are identified as a key need.},
	language = {en},
	number = {8},
	urldate = {2022-04-09},
	journal = {Sensors},
	author = {Caeiro-Rodríguez, Manuel and Otero-González, Iván and Mikic-Fonte, Fernando A. and Llamas-Nistal, Martín},
	month = jan,
	year = {2021},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {extended reality, hand and finger pose estimation and motion tracking, haptic feedback, kinesthetic feedback, smart gloves, tactile feedback},
	pages = {2667},
}

@article{bergstra_random_2012,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efﬁcient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conﬁgure neural networks and deep belief networks. Compared with neural networks conﬁgured by a pure grid search, we ﬁnd that random search over the same domain is able to ﬁnd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search ﬁnds better models by effectively searching a larger, less promising conﬁguration space. Compared with deep belief networks conﬁgured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conﬁguration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conﬁguring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput” methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	language = {en},
	author = {Bergstra, James and Bengio, Yoshua},
	year = {2012},
	pages = {25},
}

@inproceedings{franceschi_forward_2017,
	title = {Forward and {Reverse} {Gradient}-{Based} {Hyperparameter} {Optimization}},
	url = {https://proceedings.mlr.press/v70/franceschi17a.html},
	abstract = {We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters of any iterative learning algorithm such as stochastic gradient descent. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs in terms of running time and space requirements. Our formulation of the reverse-mode procedure is linked to previous work by Maclaurin et al (2015) but does not require reversible dynamics. Additionally, we explore the use of constraints on the hyperparameters. The forward-mode procedure is suitable for real-time hyperparameter updates, which may significantly speedup hyperparameter optimization on large datasets. We present a series of experiments on image and phone classification tasks. In the second task, previous gradient-based approaches are prohibitive. We show that our real-time algorithm yields state-of-the-art results in affordable time.},
	language = {en},
	urldate = {2022-04-09},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1165--1173},
}

@inproceedings{snoek_practical_2012,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	volume = {25},
	url = {https://papers.nips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including Latent Dirichlet Allocation, Structured SVMs and convolutional neural networks.},
	urldate = {2022-04-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	year = {2012},
}

@article{zhou_metaaugment_2021,
	title = {{MetaAugment}: {Sample}-{Aware} {Data} {Augmentation} {Policy} {Learning}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{MetaAugment}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17324},
	abstract = {Automated data augmentation has shown superior performance in image recognition. Existing works search for dataset-level augmentation policies without considering individual sample variations, which are likely to be sub-optimal. On the other hand, learning different policies for different samples naively could greatly increase the computing cost. In this paper, we learn a sample-aware data augmentation policy efficiently by formulating it as a sample reweighting problem. Specifically, an augmentation policy network takes a transformation and the corresponding augmented image as inputs, and outputs a weight to adjust the augmented image loss computed by a task network. At training stage, the task network minimizes the weighted losses of augmented training images, while the policy network minimizes the loss of the task network on a validation set via meta-learning. We theoretically prove the convergence of the training procedure and further derive the exact convergence rate. Superior performance is achieved on widely-used benchmarks including CIFAR-10/100, Omniglot, and ImageNet.},
	language = {en},
	number = {12},
	urldate = {2022-04-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhou, Fengwei and Li, Jiawei and Xie, Chuanlong and Chen, Fei and Hong, Lanqing and Sun, Rui and Li, Zhenguo},
	month = may,
	year = {2021},
	note = {Number: 12},
	keywords = {(Deep) Neural Network Algorithms},
	pages = {11097--11105},
}

@article{chen_group-theoretic_2020,
	title = {A {Group}-{Theoretic} {Framework} for {Data} {Augmentation}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-163.html},
	abstract = {Data augmentation is a widely used trick when training deep neural networks: in addition to the original data, properly transformed data are also added to the training set. However, to the best of our knowledge, a clear mathematical framework to explain the performance benefits of data augmentation is not available. In this paper, we develop such a theoretical framework. We show data augmentation is equivalent to an averaging operation over the orbits of a certain group that keeps the data distribution approximately invariant. We prove that it leads to variance reduction. We study empirical risk minimization, and the examples of exponential families, linear regression, and certain two-layer neural networks. We also discuss how data augmentation could be used in problems with symmetry where other approaches are prevalent, such as in cryo-electron microscopy (cryo-EM).},
	number = {245},
	urldate = {2022-04-08},
	journal = {Journal of Machine Learning Research},
	author = {Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H.},
	year = {2020},
	pages = {1--71},
}

@inproceedings{hoffer_augment_2020,
	address = {Seattle, WA, USA},
	title = {Augment {Your} {Batch}: {Improving} {Generalization} {Through} {Instance} {Repetition}},
	isbn = {978-1-72817-168-5},
	shorttitle = {Augment {Your} {Batch}},
	url = {https://ieeexplore.ieee.org/document/9157738/},
	doi = {10.1109/CVPR42600.2020.00815},
	abstract = {Large-batch SGD is important for scaling training of deep neural networks. However, without ﬁne-tuning hyperparameter schedules, the generalization of the model may be hampered. We propose to use batch augmentation: replicating instances of samples within the same batch with different data augmentations. Batch augmentation acts as a regularizer and an accelerator, increasing both generalization and performance scaling for a ﬁxed budget of optimization steps. We analyze the effect of batch augmentation on gradient variance and show that it empirically improves convergence for a wide variety of networks and datasets. Our results show that batch augmentation reduces the number of necessary SGD updates to achieve the same accuracy as the state-of-the-art. Overall, this simple yet effective method enables faster training and better generalization by allowing more computational resources to be used concurrently.},
	language = {en},
	urldate = {2022-04-08},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hoffer, Elad and Ben-Nun, Tal and Hubara, Itay and Giladi, Niv and Hoefler, Torsten and Soudry, Daniel},
	month = jun,
	year = {2020},
	pages = {8126--8135},
}

@inproceedings{jaderberg_spatial_2015,
	title = {Spatial {Transformer} {Networks}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/33ceb07bf4eeb3da587e268d663aba1a-Abstract.html},
	abstract = {Convolutional Neural Networks define an exceptionallypowerful class of model, but are still limited by the lack of abilityto be spatially invariant to the input data in a computationally and parameterefficient manner. In this work we introduce a new learnable module, theSpatial Transformer, which explicitly allows the spatial manipulation ofdata within the network. This differentiable module can be insertedinto existing convolutional architectures, giving neural networks the ability toactively spatially transform feature maps, conditional on the feature map itself,without any extra training supervision or modification to the optimisation process. We show that the useof spatial transformers results in models which learn invariance to translation,scale, rotation and more generic warping, resulting in state-of-the-artperformance on several benchmarks, and for a numberof classes of transformations.},
	urldate = {2022-04-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and kavukcuoglu, koray},
	year = {2015},
}

@inproceedings{li_pointaugment_2020,
	address = {Seattle, WA, USA},
	title = {{PointAugment}: {An} {Auto}-{Augmentation} {Framework} for {Point} {Cloud} {Classification}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{PointAugment}},
	url = {https://ieeexplore.ieee.org/document/9156333/},
	doi = {10.1109/CVPR42600.2020.00641},
	abstract = {We present PointAugment1, a new auto-augmentation framework that automatically optimizes and augments point cloud samples to enrich the data diversity when we train a classiﬁcation network. Different from existing autoaugmentation methods for 2D images, PointAugment is sample-aware and takes an adversarial learning strategy to jointly optimize an augmentor network and a classiﬁer network, such that the augmentor can learn to produce augmented samples that best ﬁt the classiﬁer. Moreover, we formulate a learnable point augmentation function with a shape-wise transformation and a point-wise displacement, and carefully design loss functions to adopt the augmented samples based on the learning progress of the classiﬁer. Extensive experiments also conﬁrm PointAugment’s effectiveness and robustness to improve the performance of various networks on shape classiﬁcation and retrieval.},
	language = {en},
	urldate = {2022-04-08},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Ruihui and Li, Xianzhi and Heng, Pheng-Ann and Fu, Chi-Wing},
	month = jun,
	year = {2020},
	pages = {6377--6386},
}

@inproceedings{xiong_explore_2021,
	title = {Explore {Visual} {Concept} {Formation} for {Image} {Classification}},
	url = {https://proceedings.mlr.press/v139/xiong21a.html},
	abstract = {Human beings acquire the ability of image classification through visual concept learning, in which the process of concept formation involves intertwined searches of common properties and concept descriptions. However, in most image classification algorithms using deep convolutional neural network (ConvNet), the representation space is constructed under the premise that concept descriptions are fixed as one-hot codes, which limits the mining of properties and the ability of identifying unseen samples. Inspired by this, we propose a learning strategy of visual concept formation (LSOVCF) based on the ConvNet, in which the two intertwined parts of concept formation, i.e. feature extraction and concept description, are learned together. First, LSOVCF takes sample response in the last layer of ConvNet to induct concept description being assumed as Gaussian distribution, which is part of the training process. Second, the exploration and experience loss is designed for optimization, which adopts experience cache pool to speed up convergence. Experiments show that LSOVCF improves the ability of identifying unseen samples on cifar10, STL10, flower17 and ImageNet based on several backbones, from the classic VGG to the SOTA Ghostnet. The code is available at {\textbackslash}url\{https://github.com/elvintanhust/LSOVCF\}.},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xiong, Shengzhou and Tan, Yihua and Wang, Guoyou},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11470--11479},
}

@inproceedings{ren_interpreting_2021,
	title = {Interpreting and {Disentangling} {Feature} {Components} of {Various} {Complexity} from {DNNs}},
	url = {https://proceedings.mlr.press/v139/ren21b.html},
	abstract = {This paper aims to define, visualize, and analyze the feature complexity that is learned by a DNN. We propose a generic definition for the feature complexity. Given the feature of a certain layer in the DNN, our method decomposes and visualizes feature components of different complexity orders from the feature. The feature decomposition enables us to evaluate the reliability, the effectiveness, and the significance of over-fitting of these feature components. Furthermore, such analysis helps to improve the performance of DNNs. As a generic method, the feature complexity also provides new insights into existing deep-learning techniques, such as network compression and knowledge distillation.},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ren, Jie and Li, Mingjie and Liu, Zexu and Zhang, Quanshi},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8971--8981},
}

@article{schneider_explaining_2022,
	title = {Explaining classifiers by constructing familiar concepts},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-022-06157-0},
	doi = {10.1007/s10994-022-06157-0},
	abstract = {Interpreting a large number of neurons in deep learning is difficult. Our proposed ‘CLAssifier-DECoder’ architecture (ClaDec) facilitates the understanding of the output of an arbitrary layer of neurons or subsets thereof. It uses a decoder that transforms the incomprehensible representation of the given neurons to a representation that is more similar to the domain a human is familiar with. In an image recognition problem, one can recognize what information (or concepts) a layer maintains by contrasting reconstructed images of ClaDec with those of a conventional auto-encoder(AE) serving as reference. An extension of ClaDec allows trading comprehensibility and fidelity. We evaluate our approach for image classification using convolutional neural networks. We show that reconstructed visualizations using encodings from a classifier capture more relevant classification information than conventional AEs. This holds although AEs contain more information on the original input. Our user study highlights that even non-experts can identify a diverse set of concepts contained in images that are relevant (or irrelevant) for the classifier. We also compare against saliency based methods that focus on pixel relevance rather than concepts. We show that ClaDec tends to highlight more relevant input areas to classification though outcomes depend on classifier architecture. Code is at https://github.com/JohnTailor/ClaDec},
	language = {en},
	urldate = {2022-04-06},
	journal = {Machine Learning},
	author = {Schneider, Johannes and Vlachos, Michalis},
	month = mar,
	year = {2022},
}

@inproceedings{ghandeharioun_dissect_2021,
	title = {{DISSECT}: {Disentangled} {Simultaneous} {Explanations} via {Concept} {Traversals}},
	shorttitle = {{DISSECT}},
	url = {https://openreview.net/forum?id=qY79G8jGsep},
	abstract = {Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars....},
	language = {en},
	urldate = {2022-04-06},
	author = {Ghandeharioun, Asma and Kim, Been and Li, Chun-Liang and Jou, Brendan and Eoff, Brian and Picard, Rosalind},
	month = sep,
	year = {2021},
}

@inproceedings{ivankay_fooling_2021,
	title = {Fooling {Explanations} in {Text} {Classifiers}},
	url = {https://openreview.net/forum?id=j3krplz_4w6},
	abstract = {State-of-the-art text classification models are becoming increasingly reliant on deep neural networks (DNNs). Due to their black-box nature, faithful and robust explanation methods need to...},
	language = {en},
	urldate = {2022-04-06},
	author = {Ivankay, Adam and Girardi, Ivan and Marchiori, Chiara and Frossard, Pascal},
	month = sep,
	year = {2021},
}

@inproceedings{lim_noisy_2021,
	title = {Noisy {Feature} {Mixup}},
	url = {https://openreview.net/forum?id=vJb4I2ANmy},
	abstract = {We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method for data augmentation that combines the best of interpolation based training and noise injection schemes. Rather than...},
	language = {en},
	urldate = {2022-04-06},
	author = {Lim, Soon Hoe and Erichson, N. Benjamin and Utrera, Francisco and Xu, Winnie and Mahoney, Michael W.},
	month = sep,
	year = {2021},
}

@inproceedings{cheung_adaaug_2021,
	title = {{AdaAug}: {Learning} {Class}- and {Instance}-adaptive {Data} {Augmentation} {Policies}},
	shorttitle = {{AdaAug}},
	url = {https://openreview.net/forum?id=rWXfFogxRJN},
	abstract = {Data augmentation is an effective way to improve the generalization capability of modern deep learning models. However, the underlying augmentation methods mostly rely on handcrafted operations....},
	language = {en},
	urldate = {2022-04-06},
	author = {Cheung, Tsz-Him and Yeung, Dit-Yan},
	month = sep,
	year = {2021},
}

@inproceedings{rigotti_attention-based_2021,
	title = {Attention-based {Interpretability} with {Concept} {Transformers}},
	url = {https://openreview.net/forum?id=kAa9eDS0RdO},
	abstract = {Attention is a mechanism that has been instrumental in driving remarkable performance gains of deep neural network models in a host of visual, NLP and multimodal tasks.
One additional notable...},
	language = {en},
	urldate = {2022-04-06},
	author = {Rigotti, Mattia and Miksovic, Christoph and Giurgiu, Ioana and Gschwind, Thomas and Scotton, Paolo},
	month = sep,
	year = {2021},
}

@inproceedings{zhang_adversarial_2021,
	title = {Adversarial {Robustness} {Through} the {Lens} of {Causality}},
	url = {https://openreview.net/forum?id=cZAi1yWpiXQ},
	abstract = {The adversarial vulnerability of deep neural networks has attracted signiﬁcant attention in machine learning. As causal reasoning has an instinct for modeling distribution change, it is essential...},
	language = {en},
	urldate = {2022-04-06},
	author = {Zhang, Yonggang and Gong, Mingming and Liu, Tongliang and Niu, Gang and Tian, Xinmei and Han, Bo and Schölkopf, Bernhard and Zhang, Kun},
	month = sep,
	year = {2021},
}

@inproceedings{he_gda-am_2021,
	title = {{GDA}-{AM}: {ON} {THE} {EFFECTIVENESS} {OF} {SOLVING} {MIN}-{IMAX} {OPTIMIZATION} {VIA} {ANDERSON} {MIXING}},
	shorttitle = {{GDA}-{AM}},
	url = {https://openreview.net/forum?id=3YqeuCVwy1d},
	abstract = {Many modern machine learning algorithms such as generative adversarial networks (GANs) and adversarial training can be formulated as minimax optimization.Gradient descent ascent (GDA) is the most...},
	language = {en},
	urldate = {2022-04-06},
	author = {He, Huan and Zhao, Shifan and Xi, Yuanzhe and Ho, Joyce and Saad, Yousef},
	month = sep,
	year = {2021},
}

@inproceedings{singla_salient_2021,
	title = {Salient {ImageNet}: {How} to discover spurious features in {Deep} {Learning}?},
	shorttitle = {Salient {ImageNet}},
	url = {https://openreview.net/forum?id=XVPqLyNxSyh},
	abstract = {Deep neural networks can be unreliable in the real world especially when they heavily use \{{\textbackslash}it spurious\} features for their predictions. Focusing on image classifications, we define \{\vphantom{\}}{\textbackslash}it core...},
	language = {en},
	urldate = {2022-04-06},
	author = {Singla, Sahil and Feizi, Soheil},
	month = sep,
	year = {2021},
}

@inproceedings{antverg_pitfalls_2021,
	title = {On the {Pitfalls} of {Analyzing} {Individual} {Neurons} in {Language} {Models}},
	url = {https://openreview.net/forum?id=8uz0EWPQIMu},
	abstract = {While many studies have shown that linguistic information is encoded in hidden word representations, few have studied individual neurons, to show how and in which neurons it is encoded.
Among these...},
	language = {en},
	urldate = {2022-04-06},
	author = {Antverg, Omer and Belinkov, Yonatan},
	month = sep,
	year = {2021},
}

@inproceedings{rommel_cadda_2021,
	title = {{CADDA}: {Class}-wise {Automatic} {Differentiable} {Data} {Augmentation} for {EEG} {Signals}},
	shorttitle = {{CADDA}},
	url = {https://openreview.net/forum?id=6IYp-35L-xJ},
	abstract = {Data augmentation is a key element of deep learning pipelines, as it informs the network during training about transformations of the input data that keep the label unchanged. Manually finding...},
	language = {en},
	urldate = {2022-04-06},
	author = {Rommel, Cédric and Moreau, Thomas and Paillard, Joseph and Gramfort, Alexandre},
	month = sep,
	year = {2021},
}

@inproceedings{liang_metashift_2021,
	title = {{MetaShift}: {A} {Dataset} of {Datasets} for {Evaluating} {Contextual} {Distribution} {Shifts} and {Training} {Conflicts}},
	shorttitle = {{MetaShift}},
	url = {https://openreview.net/forum?id=MTex8qKavoS},
	abstract = {Understanding the performance of machine learning models across diverse data distributions is critically important for reliable applications. Motivated by this, there is a growing focus on curating...},
	language = {en},
	urldate = {2022-04-06},
	author = {Liang, Weixin and Zou, James},
	month = sep,
	year = {2021},
}

@inproceedings{schott_visual_2021,
	title = {Visual {Representation} {Learning} {Does} {Not} {Generalize} {Strongly} {Within} the {Same} {Domain}},
	url = {https://openreview.net/forum?id=9RUHPlladgh},
	abstract = {An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world.
In this...},
	language = {en},
	urldate = {2022-04-06},
	author = {Schott, Lukas and Kügelgen, Julius Von and Träuble, Frederik and Gehler, Peter Vincent and Russell, Chris and Bethge, Matthias and Schölkopf, Bernhard and Locatello, Francesco and Brendel, Wieland},
	month = sep,
	year = {2021},
}

@inproceedings{scimeca_which_2021,
	title = {Which {Shortcut} {Cues} {Will} {DNNs} {Choose}? {A} {Study} from the {Parameter}-{Space} {Perspective}},
	shorttitle = {Which {Shortcut} {Cues} {Will} {DNNs} {Choose}?},
	url = {https://openreview.net/forum?id=qRDQi3ocgR3},
	abstract = {Deep neural networks (DNNs) often rely on easy–to–learn discriminatory features, or cues, that are not necessarily essential to the problem at hand. For example, ducks in an image may be recognized...},
	language = {en},
	urldate = {2022-04-06},
	author = {Scimeca, Luca and Oh, Seong Joon and Chun, Sanghyuk and Poli, Michael and Yun, Sangdoo},
	month = sep,
	year = {2021},
}

@inproceedings{harris_joint_2021,
	title = {Joint {Shapley} values: a measure of joint feature importance},
	shorttitle = {Joint {Shapley} values},
	url = {https://openreview.net/forum?id=vcUmUvQCloe},
	abstract = {The Shapley value is one of the most widely used measures of feature importance partly as it measures a feature's average effect on a model's prediction.  We introduce joint Shapley values, which...},
	language = {en},
	urldate = {2022-04-06},
	author = {Harris, Chris and Pymar, Richard and Rowat, Colin},
	month = sep,
	year = {2021},
}

@inproceedings{hernandez_natural_2021,
	title = {Natural {Language} {Descriptions} of {Deep} {Features}},
	url = {https://openreview.net/forum?id=NudBMY-tzDr},
	abstract = {Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that...},
	language = {en},
	urldate = {2022-04-06},
	author = {Hernandez, Evan and Schwettmann, Sarah and Bau, David and Bagashvili, Teona and Torralba, Antonio and Andreas, Jacob},
	month = sep,
	year = {2021},
}

@inproceedings{deng_discovering_2021,
	title = {{DISCOVERING} {AND} {EXPLAINING} {THE} {REPRESENTATION} {BOTTLENECK} {OF} {DNNS}},
	url = {https://openreview.net/forum?id=iRCUlgmdfHJ},
	abstract = {This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this...},
	language = {en},
	urldate = {2022-04-06},
	author = {Deng, Huiqi and Ren, Qihan and Zhang, Hao and Zhang, Quanshi},
	month = sep,
	year = {2021},
}

@inproceedings{wang_hidden_2021,
	title = {The {Hidden} {Convex} {Optimization} {Landscape} of {Regularized} {Two}-{Layer} {ReLU} {Networks}: an {Exact} {Characterization} of {Optimal} {Solutions}},
	shorttitle = {The {Hidden} {Convex} {Optimization} {Landscape} of {Regularized} {Two}-{Layer} {ReLU} {Networks}},
	url = {https://openreview.net/forum?id=Z7Lk2cQEG8a},
	abstract = {We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all...},
	language = {en},
	urldate = {2022-04-06},
	author = {Wang, Yifei and Lacotte, Jonathan and Pilanci, Mert},
	month = sep,
	year = {2021},
}

@inproceedings{chang_node-gam_2021,
	title = {{NODE}-{GAM}: {Neural} {Generalized} {Additive} {Model} for {Interpretable} {Deep} {Learning}},
	shorttitle = {{NODE}-{GAM}},
	url = {https://openreview.net/forum?id=g8NJR6fCCl8},
	abstract = {Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability....},
	language = {en},
	urldate = {2022-04-06},
	author = {Chang, Chun-Hao and Caruana, Rich and Goldenberg, Anna},
	month = sep,
	year = {2021},
}

@inproceedings{masoomi_explanations_2021,
	title = {Explanations of {Black}-{Box} {Models} based on {Directional} {Feature} {Interactions}},
	url = {https://openreview.net/forum?id=45Mr7LeKR9},
	abstract = {As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent.  Several recent works explain black-box models...},
	language = {en},
	urldate = {2022-04-06},
	author = {Masoomi, Aria and Hill, Davin and Xu, Zhonghui and Hersh, Craig P. and Silverman, Edwin K. and Castaldi, Peter J. and Ioannidis, Stratis and Dy, Jennifer},
	month = sep,
	year = {2021},
}

@inproceedings{chidambaram_towards_2021,
	title = {Towards {Understanding} the {Data} {Dependency} of {Mixup}-style {Training}},
	url = {https://openreview.net/forum?id=ieNJYujcGDO},
	abstract = {In the Mixup training paradigm, a model is trained using convex combinations of data points and their associated labels. Despite seeing very few true data points during training, models trained...},
	language = {en},
	urldate = {2022-04-06},
	author = {Chidambaram, Muthu and Wang, Xiang and Hu, Yuzheng and Wu, Chenwei and Ge, Rong},
	month = sep,
	year = {2021},
}

@inproceedings{harrington_finding_2021,
	title = {Finding {Biological} {Plausibility} for {Adversarially} {Robust} {Features} via {Metameric} {Tasks}},
	url = {https://openreview.net/forum?id=yeP_zx9vqNm},
	abstract = {Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such...},
	language = {en},
	urldate = {2022-04-06},
	author = {Harrington, Anne and Deza, Arturo},
	month = sep,
	year = {2021},
}

@inproceedings{arbel_amortized_2021,
	title = {Amortized {Implicit} {Differentiation} for {Stochastic} {Bilevel} {Optimization}},
	url = {https://openreview.net/forum?id=3PN4iyXBeF},
	abstract = {We study a class of algorithms for solving bilevel optimization problems in both stochastic and deterministic settings when the inner-level objective is strongly convex. Specifically, we consider...},
	language = {en},
	urldate = {2022-04-06},
	author = {Arbel, Michael and Mairal, Julien},
	month = sep,
	year = {2021},
}

@inproceedings{zhou_deep_2021,
	title = {Do deep networks transfer invariances across classes?},
	url = {https://openreview.net/forum?id=Fn7i_r5rR0q},
	abstract = {In order to generalize well, classifiers must learn to be invariant to nuisance transformations that do not alter an input's class. Many problems have "class-agnostic" nuisance transformations that...},
	language = {en},
	urldate = {2022-04-06},
	author = {Zhou, Allan and Tajwar, Fahim and Robey, Alexander and Knowles, Tom and Pappas, George J. and Hassani, Hamed and Finn, Chelsea},
	month = sep,
	year = {2021},
}

@inproceedings{calian_defending_2021,
	title = {Defending {Against} {Image} {Corruptions} {Through} {Adversarial} {Augmentations}},
	url = {https://openreview.net/forum?id=jJOjjiZHy3h},
	abstract = {Modern neural networks excel at image classification, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as...},
	language = {en},
	urldate = {2022-04-06},
	author = {Calian, Dan Andrei and Stimberg, Florian and Wiles, Olivia and Rebuffi, Sylvestre-Alvise and György, András and Mann, Timothy A. and Gowal, Sven},
	month = sep,
	year = {2021},
}

@inproceedings{sixt_users_2021,
	title = {Do {Users} {Benefit} {From} {Interpretable} {Vision}? {A} {User} {Study}, {Baseline}, {And} {Dataset}},
	shorttitle = {Do {Users} {Benefit} {From} {Interpretable} {Vision}?},
	url = {https://openreview.net/forum?id=v6s3HVjPerv},
	abstract = {A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model’s respective predictions...},
	language = {en},
	urldate = {2022-04-06},
	author = {Sixt, Leon and Schuessler, Martin and Popescu, Oana-Iuliana and Weiß, Philipp and Landgraf, Tim},
	month = sep,
	year = {2021},
}

@inproceedings{larsen_how_2021,
	title = {How many degrees of freedom do we need to train deep networks: a loss landscape perspective},
	shorttitle = {How many degrees of freedom do we need to train deep networks},
	url = {https://openreview.net/forum?id=ChMLTGRjFcU},
	abstract = {A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the...},
	language = {en},
	urldate = {2022-04-06},
	author = {Larsen, Brett W. and Fort, Stanislav and Becker, Nic and Ganguli, Surya},
	month = sep,
	year = {2021},
}

@inproceedings{black_consistent_2021,
	title = {Consistent {Counterfactuals} for {Deep} {Models}},
	url = {https://openreview.net/forum?id=St6eyiTEHnG},
	abstract = {Counterfactual examples are one of the most commonly-cited methods for explaining the predictions of machine learning models in key areas such as finance and medical diagnosis. Counterfactuals are...},
	language = {en},
	urldate = {2022-04-06},
	author = {Black, Emily and Wang, Zifan and Fredrikson, Matt},
	month = sep,
	year = {2021},
}

@inproceedings{jethani_fastshap_2021,
	title = {{FastSHAP}: {Real}-{Time} {Shapley} {Value} {Estimation}},
	shorttitle = {{FastSHAP}},
	url = {https://openreview.net/forum?id=Zq2G_VTV53T},
	abstract = {Although Shapley values are theoretically appealing for explaining black-box models, they are costly to calculate and thus impractical in settings that involve large, high-dimensional models. To...},
	language = {en},
	urldate = {2022-04-06},
	author = {Jethani, Neil and Sudarshan, Mukund and Covert, Ian Connick and Lee, Su-In and Ranganath, Rajesh},
	month = sep,
	year = {2021},
}

@inproceedings{mei_falcon_2021,
	title = {{FALCON}: {Fast} {Visual} {Concept} {Learning} by {Integrating} {Images}, {Linguistic} descriptions, and {Conceptual} {Relations}},
	shorttitle = {{FALCON}},
	url = {https://openreview.net/forum?id=htWIlvDcY8},
	abstract = {We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images...},
	language = {en},
	urldate = {2022-04-06},
	author = {Mei, Lingjie and Mao, Jiayuan and Wang, Ziqi and Gan, Chuang and Tenenbaum, Joshua B.},
	month = sep,
	year = {2021},
}

@inproceedings{hamilton_axiomatic_2021,
	title = {Axiomatic {Explanations} for {Visual} {Search}, {Retrieval}, and {Similarity} {Learning}},
	url = {https://openreview.net/forum?id=TqNsv1TuCX9},
	abstract = {Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret...},
	language = {en},
	urldate = {2022-04-06},
	author = {Hamilton, Mark and Lundberg, Scott and Fu, Stephanie and Zhang, Lei and Freeman, William T.},
	month = sep,
	year = {2021},
}

@inproceedings{kim_what_2021,
	title = {What {Makes} {Better} {Augmentation} {Strategies}? {Augment} {Difficult} but {Not} too {Different}},
	shorttitle = {What {Makes} {Better} {Augmentation} {Strategies}?},
	url = {https://openreview.net/forum?id=Ucx3DQbC9GH},
	abstract = {The practice of data augmentation has been extensively used to boost the performance of deep neural networks for various NLP tasks. It is more effective when only a limited number of labeled...},
	language = {en},
	urldate = {2022-04-06},
	author = {Kim, Jaehyung and Kang, Dongyeop and Ahn, Sungsoo and Shin, Jinwoo},
	month = sep,
	year = {2021},
}

@inproceedings{chen_recursive_2022,
	title = {Recursive {Disentanglement} {Network}},
	url = {https://openreview.net/forum?id=CSfcOznpDY},
	abstract = {Disentangled feature representation is essential for data-efficient learning. The feature space of deep models is inherently compositional. Existing \${\textbackslash}beta\$-VAE-based methods, which only apply...},
	language = {en},
	urldate = {2022-04-06},
	author = {Chen, Yixuan and Shi, Yubin and Li, Dongsheng and Wang, Yujiang and Dong, Mingzhi and Zhao, Yingying and Dick, Robert and Lv, Qin and Yang, Fan and Shang, Li},
	year = {2022},
}

@inproceedings{adebayo_post_2022,
	title = {Post hoc {Explanations} may be {Ineffective} for {Detecting} {Unknown} {Spurious} {Correlation}},
	url = {https://openreview.net/forum?id=xNOVfCCvDpM},
	abstract = {We investigate whether three types of post hoc model explanations--feature attribution, concept activation, and training point ranking--are effective for detecting a model's reliance on spurious...},
	language = {en},
	urldate = {2022-04-06},
	author = {Adebayo, Julius and Muelly, Michael and Abelson, Harold and Kim, Been},
	year = {2022},
}

@inproceedings{sreedharan_bridging_2021,
	title = {Bridging the {Gap}: {Providing} {Post}-{Hoc} {Symbolic} {Explanations} for {Sequential} {Decision}-{Making} {Problems} with {Inscrutable} {Representations}},
	shorttitle = {Bridging the {Gap}},
	url = {https://openreview.net/forum?id=o-1v9hdSult},
	abstract = {As increasingly complex AI systems are introduced into our daily lives, it becomes important for such systems to be capable of explaining the rationale for their decisions and allowing users to...},
	language = {en},
	urldate = {2022-04-06},
	author = {Sreedharan, Sarath and Soni, Utkarsh and Verma, Mudit and Srivastava, Siddharth and Kambhampati, Subbarao},
	month = sep,
	year = {2021},
}

@inproceedings{zhao_quantitative_2021,
	title = {Quantitative {Performance} {Assessment} of {CNN} {Units} via {Topological} {Entropy} {Calculation}},
	url = {https://openreview.net/forum?id=xFOyMwWPkz},
	abstract = {Identifying the status of individual network units is critical for understanding the mechanism of convolutional neural networks (CNNs). However, it is still challenging to reliably give a general...},
	language = {en},
	urldate = {2022-04-06},
	author = {Zhao, Yang and Zhang, Hao},
	month = sep,
	year = {2021},
}

@inproceedings{jia_zest_2021,
	title = {A {Zest} of {LIME}: {Towards} {Architecture}-{Independent} {Model} {Distances}},
	shorttitle = {A {Zest} of {LIME}},
	url = {https://openreview.net/forum?id=OUz_9TiTv9j},
	abstract = {Definitions of the distance between two machine learning models either characterize the similarity of the models' predictions or of their weights. While similarity of weights is attractive because...},
	language = {en},
	urldate = {2022-04-06},
	author = {Jia, Hengrui and Chen, Hongyu and Guan, Jonas and Shamsabadi, Ali Shahin and Papernot, Nicolas},
	month = sep,
	year = {2021},
}

@inproceedings{lord_attacking_2021,
	title = {Attacking deep networks with surrogate-based adversarial black-box methods is easy},
	url = {https://openreview.net/forum?id=Zf4ZdI4OQPV},
	abstract = {A recent line of work on black-box adversarial attacks has revived the use of transfer from surrogate models by integrating it into query-based search. However, we find that existing approaches of...},
	language = {en},
	urldate = {2022-04-06},
	author = {Lord, Nicholas A. and Mueller, Romain and Bertinetto, Luca},
	month = sep,
	year = {2021},
}

@inproceedings{gontijo-lopes_no_2021,
	title = {No {One} {Representation} to {Rule} {Them} {All}: {Overlapping} {Features} of {Training} {Methods}},
	shorttitle = {No {One} {Representation} to {Rule} {Them} {All}},
	url = {https://openreview.net/forum?id=BK-4qbGgIE3},
	abstract = {Despite being able to capture a range of features of the data, high accuracy models trained with supervision tend to make similar predictions. This seemingly implies that high-performing models...},
	language = {en},
	urldate = {2022-04-06},
	author = {Gontijo-Lopes, Raphael and Dauphin, Yann and Cubuk, Ekin Dogus},
	month = sep,
	year = {2021},
}

@inproceedings{lin_online_2019,
	title = {Online {Hyper}-{Parameter} {Learning} for {Auto}-{Augmentation} {Strategy}},
	doi = {10.1109/ICCV.2019.00668},
	abstract = {Data augmentation is critical to the success of modern deep learning techniques. In this paper, we propose Online Hyper-parameter Learning for Auto-Augmentation (OHL-Auto-Aug), an economical solution that learns the augmentation policy distribution along with network training. Unlike previous methods on auto-augmentation that search augmentation strategies in an offline manner, our method formulates the augmentation policy as a parameterized probability distribution, thus allowing its parameters to be optimized jointly with network parameters. Our proposed OHL-Auto-Aug eliminates the need of re-training and dramatically reduces the cost of the overall search process, while establishes significantly accuracy improvements over baseline models. On both CIFAR-10 and ImageNet, our method achieves remarkable on search accuracy, 60x faster on CIFAR-10 and 24x faster on ImageNet, while maintaining competitive accuracies.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Lin, Chen and Guo, Minghao and Li, Chuming and Yuan, Xin and Wu, Wei and Yan, Junjie and Lin, Dahua and Ouyang, Wanli},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {Biological system modeling, Computational modeling, Computer architecture, Data models, Optimization, Probability distribution, Training},
	pages = {6578--6587},
}

@inproceedings{tian_improving_2020,
	title = {Improving {Auto}-{Augment} via {Augmentation}-{Wise} {Weight} {Sharing}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/dc49dfebb0b00fd44aeff5c60cc1f825-Abstract.html},
	abstract = {The recent progress on automatically searching augmentation policies has boosted the performance substantially for various tasks.
A key component of automatic augmentation search is the evaluation process for a particular augmentation policy, which is utilized to return reward and usually runs thousands of times.
A plain evaluation process, which includes full model training and validation, would be time-consuming.
To achieve efficiency, many choose to sacrifice evaluation reliability for speed.
In this paper, we dive into the dynamics of augmented training of the model.
This inspires us to design a powerful and efficient proxy task based on the Augmentation-Wise Weight Sharing (AWS) to form a fast yet accurate evaluation process in an elegant way.
Comprehensive analysis verifies the superiority of this approach in terms of effectiveness and efficiency.
The augmentation policies found by our method achieve superior accuracies compared with existing auto-augmentation search methods.
On CIFAR-10, we achieve a top-1 error rate of 1.24\%, which is currently the best performing single model without extra training data.
On ImageNet, we get a top-1 error rate of 20.36\% for ResNet-50, which leads to 3.34\% absolute error rate reduction over the baseline augmentation.},
	urldate = {2022-04-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tian, Keyu and Lin, Chen and Sun, Ming and Zhou, Luping and Yan, Junjie and Ouyang, Wanli},
	year = {2020},
	pages = {19088--19098},
}

@article{zhang_noise_2022,
	title = {Noise {Augmentation} {Is} {All} {You} {Need} {For} {FGSM} {Fast} {Adversarial} {Training}: {Catastrophic} {Overfitting} {And} {Robust} {Overfitting} {Require} {Different} {Augmentation}},
	shorttitle = {Noise {Augmentation} {Is} {All} {You} {Need} {For} {FGSM} {Fast} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2202.05488},
	abstract = {Adversarial training (AT) and its variants are the most effective approaches for obtaining adversarially robust models. A unique characteristic of AT is that an inner maximization problem needs to be solved repeatedly before the model weights can be updated, which makes the training slow. FGSM AT significantly improves its efficiency but it fails when the step size grows. The SOTA GradAlign makes FGSM AT compatible with a higher step size, however, its regularization on input gradient makes it 3 to 4 times slower than FGSM AT. Our proposed NoiseAug removes the extra computation overhead by directly regularizing on the input itself. The key contribution of this work lies in an empirical finding that single-step FGSM AT is not as hard as suggested in the past line of work: noise augmentation is all you need for (FGSM) fast AT. Towards understanding the success of our NoiseAug, we perform an extensive analysis and find that mitigating Catastrophic Overfitting (CO) and Robust Overfitting (RO) need different augmentations. Instead of more samples caused by data augmentation, we identify what makes NoiseAug effective for preventing CO might lie in its improved local linearity.},
	urldate = {2022-04-04},
	journal = {arXiv:2202.05488 [cs]},
	author = {Zhang, Chaoning and Zhang, Kang and Niu, Axi and Zhang, Chenshuang and Feng, Jiu and Yoo, Chang D. and Kweon, In So},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.05488},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{donhauser_interpolation_2021,
	title = {Interpolation can hurt robust generalization even when there is no noise},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/c4f2c88e16a579900657c18726641c81-Abstract.html},
	abstract = {Numerous recent works show that overparameterization implicitly reduces variance for min-norm interpolators and max-margin classifiers. These findings suggest that ridge regularization has vanishing benefits in high dimensions.  We challenge this narrative by showing that, even in the absence of noise, avoiding interpolation through ridge regularization can significantly improve generalization.  We prove this phenomenon for the robust risk of both linear regression and classification, and hence provide the first theoretical result on {\textbackslash}emph\{robust overfitting\}.},
	urldate = {2022-04-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Donhauser, Konstantin and Tifrea, Alexandru and Aerni, Michael and Heckel, Reinhard and Yang, Fanny},
	year = {2021},
	pages = {23465--23477},
}

@inproceedings{yang_boundary_2020,
	title = {Boundary thickness and robustness in learning models},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/44e76e99b5e194377e955b13fb12f630-Abstract.html},
	abstract = {Robustness of machine learning models to various adversarial and non-adversarial corruptions continues to be of interest. In this paper, we introduce the notion of the boundary thickness of a classifier, and we describe its connection with and usefulness for model robustness. Thick decision boundaries lead to improved performance, while thin decision boundaries lead to overfitting (e.g., measured by the robust generalization gap between training and testing) and lower robustness. We show that a thicker boundary helps improve robustness against adversarial examples (e.g., improving the robust test accuracy of adversarial training), as well as so-called out-of-distribution (OOD) transforms, and we show that many commonly-used regularization and data augmentation procedures can increase boundary thickness. On the theoretical side, we establish that maximizing boundary thickness is akin to minimizing the so-called mixup loss. Using these observations, we can show that noise-augmentation on mixup training further increases boundary thickness, thereby combating vulnerability to various forms of adversarial attacks and OOD transforms. We can also show that the performance improvement in several recent lines of work happens in conjunction with a thicker boundary.},
	urldate = {2022-04-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yang, Yaoqing and Khanna, Rajiv and Yu, Yaodong and Gholami, Amir and Keutzer, Kurt and Gonzalez, Joseph E and Ramchandran, Kannan and Mahoney, Michael W},
	year = {2020},
	pages = {6223--6234},
}

@article{bunk_adversarially_2021,
	title = {Adversarially {Optimized} {Mixup} for {Robust} {Classification}},
	url = {http://arxiv.org/abs/2103.11589},
	abstract = {Mixup is a procedure for data augmentation that trains networks to make smoothly interpolated predictions between datapoints. Adversarial training is a strong form of data augmentation that optimizes for worst-case predictions in a compact space around each data-point, resulting in neural networks that make much more robust predictions. In this paper, we bring these ideas together by adversarially probing the space between datapoints, using projected gradient descent (PGD). The fundamental approach in this work is to leverage backpropagation through the mixup interpolation during training to optimize for places where the network makes unsmooth and incongruous predictions. Additionally, we also explore several modifications and nuances, like optimization of the mixup ratio and geometrical label assignment, and discuss their impact on enhancing network robustness. Through these ideas, we have been able to train networks that robustly generalize better; experiments on CIFAR-10 and CIFAR-100 demonstrate consistent improvements in accuracy against strong adversaries, including the recent strong ensemble attack AutoAttack. Our source code would be released for reproducibility.},
	urldate = {2022-04-04},
	journal = {arXiv:2103.11589 [cs]},
	author = {Bunk, Jason and Chattopadhyay, Srinjoy and Manjunath, B. S. and Chandrasekaran, Shivkumar},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.11589},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{suzuki_teachaugment_2022,
	title = {{TeachAugment}: {Data} {Augmentation} {Optimization} {Using} {Teacher} {Knowledge}},
	shorttitle = {{TeachAugment}},
	url = {http://arxiv.org/abs/2202.12513},
	abstract = {Optimization of image transformation functions for the purpose of data augmentation has been intensively studied. In particular, adversarial data augmentation strategies, which search augmentation maximizing task loss, show significant improvement in the model generalization for many tasks. However, the existing methods require careful parameter tuning to avoid excessively strong deformations that take away image features critical for acquiring generalization. In this paper, we propose a data augmentation optimization method based on the adversarial strategy called TeachAugment, which can produce informative transformed images to the model without requiring careful tuning by leveraging a teacher model. Specifically, the augmentation is searched so that augmented images are adversarial for the target model and recognizable for the teacher model. We also propose data augmentation using neural networks, which simplifies the search space design and allows for updating of the data augmentation using the gradient method. We show that TeachAugment outperforms existing methods in experiments of image classification, semantic segmentation, and unsupervised representation learning tasks.},
	urldate = {2022-04-04},
	journal = {arXiv:2202.12513 [cs]},
	author = {Suzuki, Teppei},
	month = mar,
	year = {2022},
	note = {arXiv: 2202.12513},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{gavrikov_cnn_2022,
	title = {{CNN} {Filter} {DB}: {An} {Empirical} {Investigation} of {Trained} {Convolutional} {Filters}},
	shorttitle = {{CNN} {Filter} {DB}},
	url = {http://arxiv.org/abs/2203.15331},
	abstract = {Currently, many theoretical as well as practically relevant questions towards the transferability and robustness of Convolutional Neural Networks (CNNs) remain unsolved. While ongoing research efforts are engaging these problems from various angles, in most computer vision related cases these approaches can be generalized to investigations of the effects of distribution shifts in image data. In this context, we propose to study the shifts in the learned weights of trained CNN models. Here we focus on the properties of the distributions of dominantly used 3x3 convolution filter kernels. We collected and publicly provide a dataset with over 1.4 billion filters from hundreds of trained CNNs, using a wide range of datasets, architectures, and vision tasks. In a first use case of the proposed dataset, we can show highly relevant properties of many publicly available pre-trained models for practical applications: I) We analyze distribution shifts (or the lack thereof) between trained filters along different axes of meta-parameters, like visual category of the dataset, task, architecture, or layer depth. Based on these results, we conclude that model pre-training can succeed on arbitrary datasets if they meet size and variance conditions. II) We show that many pre-trained models contain degenerated filters which make them less robust and less suitable for fine-tuning on target applications. Data \& Project website: https://github.com/paulgavrikov/cnn-filter-db},
	urldate = {2022-04-04},
	journal = {arXiv:2203.15331 [cs]},
	author = {Gavrikov, Paul and Keuper, Janis},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.15331},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{liu_direct_2021,
	title = {Direct {Differentiable} {Augmentation} {Search}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Direct_Differentiable_Augmentation_Search_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-04-04},
	author = {Liu, Aoming and Huang, Zehao and Huang, Zhiwu and Wang, Naiyan},
	year = {2021},
	pages = {12219--12228},
}

@inproceedings{ho_population_2019,
	title = {Population {Based} {Augmentation}: {Efficient} {Learning} of {Augmentation} {Policy} {Schedules}},
	shorttitle = {Population {Based} {Augmentation}},
	url = {https://proceedings.mlr.press/v97/ho19b.html},
	abstract = {A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46\%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.},
	language = {en},
	urldate = {2022-04-03},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ho, Daniel and Liang, Eric and Chen, Xi and Stoica, Ion and Abbeel, Pieter},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2731--2741},
}

@inproceedings{hataya_meta_2022,
	title = {Meta {Approach} to {Data} {Augmentation} {Optimization}},
	url = {https://openaccess.thecvf.com/content/WACV2022/html/Hataya_Meta_Approach_to_Data_Augmentation_Optimization_WACV_2022_paper.html},
	language = {en},
	urldate = {2022-04-01},
	author = {Hataya, Ryuichiro and Zdenek, Jan and Yoshizoe, Kazuki and Nakayama, Hideki},
	year = {2022},
	pages = {2574--2583},
}

@inproceedings{liu_data_2020,
	address = {Online},
	title = {Data {Boost}: {Text} {Data} {Augmentation} {Through} {Reinforcement} {Learning} {Guided} {Conditional} {Generation}},
	shorttitle = {Data {Boost}},
	url = {https://aclanthology.org/2020.emnlp-main.726},
	doi = {10.18653/v1/2020.emnlp-main.726},
	abstract = {Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We evaluate Data Boost on three diverse text classification tasks under five different classifier architectures. The result shows that Data Boost can boost the performance of classifiers especially in low-resource data scenarios. For instance, Data Boost improves F1 for the three tasks by 8.7\% on average when given only 10\% of the whole data for training. We also compare Data Boost with six prior text augmentation methods. Through human evaluations (N=178), we confirm that Data Boost augmentation has comparable quality as the original data with respect to readability and class consistency.},
	urldate = {2022-04-01},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Ruibo and Xu, Guangxuan and Jia, Chenyan and Ma, Weicheng and Wang, Lili and Vosoughi, Soroush},
	month = nov,
	year = {2020},
	pages = {9031--9041},
}

@inproceedings{li_differentiable_2020,
	address = {Cham},
	title = {Differentiable {Automatic} {Data} {Augmentation}},
	isbn = {978-3-030-58542-6},
	doi = {10.1007/978-3-030-58542-6_35},
	abstract = {Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup works such as Population Based Augmentation (PBA) and Fast AutoAugment improved efficiency, but their optimization speed remains a bottleneck. In this paper, we propose Differentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a differentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an efficient and effective one-pass optimization strategy to learn an efficient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-the-art while achieving very comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Li, Yonggang and Hu, Guosheng and Wang, Yongtao and Hospedales, Timothy and Robertson, Neil M. and Yang, Yongxin},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {580--595},
}

@article{machkour_classical_2021,
	title = {Classical and {Deep} {Learning} based {Visual} {Servoing} {Systems}: a {Survey} on {State} of the {Art}},
	volume = {104},
	issn = {1573-0409},
	shorttitle = {Classical and {Deep} {Learning} based {Visual} {Servoing} {Systems}},
	url = {https://doi.org/10.1007/s10846-021-01540-w},
	doi = {10.1007/s10846-021-01540-w},
	abstract = {Computer vision, together with bayesian estimation algorithms, sensors, and actuators, are used in robotics to solve a variety of critical tasks such as localization, obstacle avoidance, and navigation. Classical approaches in visual servoing systems relied on extracting features from images to control robot movements. Now, state of the art computer vision systems use deep neural networks in tasks such as object recognition, detection, segmentation, and tracking. These networks and specialized controllers play a predominant role in the design and implementation of modern visual servoing systems due to their accuracy, flexibility, and adaptability. Recent research in direct systems for visual servoing has created robotic systems capable of relying only on the information contained in the whole image. Furthermore, end-to-end systems learn the control laws during training, eliminating entirely the controller. This paper presents a comprehensive survey on the state of the art in visual servoing systems, discussing the latest classical methods not included in other surveys but emphasizing the new approaches based on deep neural networks and their applications in a broad variety of applications within robotics.},
	language = {en},
	number = {1},
	urldate = {2022-03-31},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Machkour, Zakariae and Ortiz-Arroyo, Daniel and Durdevic, Petar},
	month = dec,
	year = {2021},
	pages = {11},
}

@article{wu_survey_2022,
	title = {A survey {Of} learning-{Based} control of robotic visual servoing systems},
	volume = {359},
	issn = {0016-0032},
	url = {https://www.sciencedirect.com/science/article/pii/S0016003221006621},
	doi = {10.1016/j.jfranklin.2021.11.009},
	abstract = {Major difficulties and challenges of modern robotics systems focus on how to give robots self-learning and self-decision-making ability. Visual servoing control strategy is an important strategy of robotic systems to perceive the environment via the vision. The vision can guide new robotic systems to complete more complicated tasks in complex working environments. This survey aims at describing the state-of-the-art learning-based algorithms, especially those algorithms that combine with model predictive control (MPC) used in visual servoing systems, and providing some pioneering and advanced references with several numerical simulations. The general modeling methods of visual servo and the influence of traditional control strategies on robotic visual servoing systems are introduced. The advantages of introducing neural-network-based algorithms and reinforcement-learning-based algorithms into the systems are discussed. Finally, according to the existing research progress and references, the future directions of robotic visual servoing systems are summarized and prospected.},
	language = {en},
	number = {1},
	urldate = {2022-03-31},
	journal = {Journal of the Franklin Institute},
	author = {Wu, Jinhui and Jin, Zhehao and Liu, Andong and Yu, Li and Yang, Fuwen},
	month = jan,
	year = {2022},
	pages = {556--577},
}

@inproceedings{kwon_h2o_2021,
	title = {{H2O}: {Two} {Hands} {Manipulating} {Objects} for {First} {Person} {Interaction} {Recognition}},
	shorttitle = {{H2O}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Kwon_H2O_Two_Hands_Manipulating_Objects_for_First_Person_Interaction_Recognition_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-03-31},
	author = {Kwon, Taein and Tekin, Bugra and Stühmer, Jan and Bogo, Federica and Pollefeys, Marc},
	year = {2021},
	pages = {10138--10148},
}

@inproceedings{maqueda_event-based_2018,
	title = {Event-{Based} {Vision} {Meets} {Deep} {Learning} on {Steering} {Prediction} for {Self}-{Driving} {Cars}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Maqueda_Event-Based_Vision_Meets_CVPR_2018_paper.html},
	urldate = {2022-03-22},
	author = {Maqueda, Ana I. and Loquercio, Antonio and Gallego, Guillermo and García, Narciso and Scaramuzza, Davide},
	year = {2018},
	pages = {5419--5427},
}

@article{gallego_event-based_2022,
	title = {Event-{Based} {Vision}: {A} {Survey}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Event-{Based} {Vision}},
	doi = {10.1109/TPAMI.2020.3008413},
	abstract = {Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of μμs), very high dynamic range (140 dB versus 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gallego, Guillermo and Delbrück, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, Jörg and Daniilidis, Kostas and Scaramuzza, Davide},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Brightness, Cameras, Event cameras, Retina, Robot vision systems, Voltage control, asynchronous sensor, bio-inspired vision, high dynamic range, low latency, low power},
	pages = {154--180},
}

@inproceedings{gong_maxup_2021,
	title = {{MaxUp}: {Lightweight} {Adversarial} {Training} {With} {Data} {Augmentation} {Improves} {Neural} {Network} {Training}},
	shorttitle = {{MaxUp}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Gong_MaxUp_Lightweight_Adversarial_Training_With_Data_Augmentation_Improves_Neural_Network_CVPR_2021_paper.html?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%28ja%29&utm_medium=email&utm_source=Revue%20newsletter},
	language = {en},
	urldate = {2022-02-11},
	author = {Gong, Chengyue and Ren, Tongzheng and Ye, Mao and Liu, Qiang},
	year = {2021},
	pages = {2474--2483},
}

@article{yuan_adaptive_2021,
	title = {Adaptive {Image} {Transformations} for {Transfer}-based {Adversarial} {Attack}},
	url = {http://arxiv.org/abs/2111.13844},
	abstract = {Adversarial attacks provide a good way to study the robustness of deep learning models. One category of methods in transfer-based black-box attack utilizes several image transformation operations to improve the transferability of adversarial examples, which is effective, but fails to take the specific characteristic of the input image into consideration. In this work, we propose a novel architecture, called Adaptive Image Transformation Learner (AITL), which incorporates different image transformation operations into a unified framework to further improve the transferability of adversarial examples. Unlike the fixed combinational transformations used in existing works, our elaborately designed transformation learner adaptively selects the most effective combination of image transformations specific to the input image. Extensive experiments on ImageNet demonstrate that our method significantly improves the attack success rates on both normally trained models and defense models under various settings.},
	urldate = {2022-02-08},
	journal = {arXiv:2111.13844 [cs]},
	author = {Yuan, Zheng and Zhang, Jie and Shan, Shiguang},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.13844},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{shu_adversarial_2021,
	title = {Adversarial {Differentiable} {Data} {Augmentation} for {Autonomous} {Systems}},
	doi = {10.1109/ICRA48506.2021.9561205},
	abstract = {Autonomous systems often rely on neural networks to achieve high performance on planning and control problems. Unfortunately, neural networks suffer severely when input images become degraded in ways that are not reflected in the training data. This is particularly problematic for robotic systems like autonomous vehicles (AV) for which reliability is paramount. In this work, we consider robust optimization methods for hardening control systems against image corruptions and other unexpected domain shifts. Recent work on robust optimization for neural nets has been focused largely on combating adversarial attacks. In this work, we borrow ideas from the adversarial training and data augmentation literature to enhance robustness to image corruptions and domain shifts. To this end, we train networks while augmenting image data with a battery of image degradations. Unlike traditional augmentation methods, we choose the parameters for each degradation adversarially so as to maximize system performance. By formulating image degradations in a way that is differentiable with respect to degradation parameters, we enable the use of efficient optimization methods (PGD) for choosing worst-case augmentation parameters. We demonstrate the efficacy of this method on the learning to steer task for AVs. By adversarially training against image corruptions, we produce networks that are highly robust to image corruptions. We show that the proposed differentiable augmentation schemes result in higher levels of robustness and accuracy for a range of settings as compared to baseline and state-of-the-art augmentation methods.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Shu, Manli and Shen, Yu and Lin, Ming C. and Goldstein, Tom},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Data models, Degradation, Neural networks, Optimization methods, Robustness, Training, Training data},
	pages = {14069--14075},
}

@inproceedings{perez_enhancing_2021,
	title = {Enhancing {Adversarial} {Robustness} via {Test}-{Time} {Transformation} {Ensembling}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Perez_Enhancing_Adversarial_Robustness_via_Test-Time_Transformation_Ensembling_ICCVW_2021_paper.html},
	language = {en},
	urldate = {2022-02-08},
	author = {Pérez, Juan C. and Alfarra, Motasem and Jeanneret, Guillaume and Rueda, Laura and Thabet, Ali and Ghanem, Bernard and Arbeláez, Pablo},
	year = {2021},
	pages = {81--91},
}

@inproceedings{tang_onlineaugment_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{OnlineAugment}: {Online} {Data} {Augmentation} with {Less} {Domain} {Knowledge}},
	isbn = {978-3-030-58571-6},
	shorttitle = {{OnlineAugment}},
	doi = {10.1007/978-3-030-58571-6_19},
	abstract = {Data augmentation is one of the most important tools in training modern deep neural networks. Recently, great advances have been made in searching for optimal augmentation policies in the image classification domain. However, two key points related to data augmentation remain uncovered by the current methods. First is that most if not all modern augmentation search methods are offline and learning policies are isolated from their usage. The learned policies are mostly constant throughout the training process and are not adapted to the current training model state. Second, the policies rely on class-preserving image processing functions. Hence applying current offline methods to new tasks may require domain knowledge to specify such kind of operations. In this work, we offer an orthogonal online data augmentation scheme together with three new augmentation networks, co-trained with the target learning task. It is both more efficient, in the sense that it does not require expensive offline training when entering a new domain, and more adaptive as it adapts to the learner state. Our augmentation networks require less domain knowledge and are easily applicable to new tasks. Extensive experiments demonstrate that the proposed scheme alone performs on par with the state-of-the-art offline data augmentation methods, as well as improving upon the state-of-the-art in combination with those methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Tang, Zhiqiang and Gao, Yunhe and Karlinsky, Leonid and Sattigeri, Prasanna and Feris, Rogerio and Metaxas, Dimitris},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {313--329},
}

@inproceedings{volpi_generalizing_2018,
	title = {Generalizing to {Unseen} {Domains} via {Adversarial} {Data} {Augmentation}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/1d94108e907bb8311d8802b48fd54b4a-Abstract.html},
	urldate = {2022-02-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Volpi, Riccardo and Namkoong, Hongseok and Sener, Ozan and Duchi, John C and Murino, Vittorio and Savarese, Silvio},
	year = {2018},
}

@article{guo_connections_2021,
	title = {On {Connections} {Between} {Regularizations} for {Improving} {DNN} {Robustness}},
	volume = {43},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.3006917},
	abstract = {This paper analyzes regularization terms proposed recently for improving the adversarial robustness of deep neural networks (DNNs), from a theoretical point of view. Specifically, we study possible connections between several effective methods, including input-gradient regularization, Jacobian regularization, curvature regularization, and a cross-Lipschitz functional. We investigate them on DNNs with general rectified linear activations, which constitute one of the most prevalent families of models for image classification and a host of other machine learning applications. We shed light on essential ingredients of these regularizations and re-interpret their functionality. Through the lens of our study, more principled and efficient regularizations can possibly be invented in the near future.},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Guo, Yiwen and Chen, Long and Chen, Yurong and Zhang, Changshui},
	month = dec,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Computational modeling, Deep neural networks, Jacobian matrices, Neural networks, Perturbation methods, Robustness, Task analysis, Training data, adversarial robustness, network property, regularizations},
	pages = {4469--4476},
}

@inproceedings{cisse_parseval_2017,
	title = {Parseval {Networks}: {Improving} {Robustness} to {Adversarial} {Examples}},
	shorttitle = {Parseval {Networks}},
	url = {https://proceedings.mlr.press/v70/cisse17a.html},
	abstract = {We introduce Parseval networks, a form of deep neural networks in which the Lipschitz constant of linear, convolutional and aggregation layers is constrained to be smaller than \$1\$. Parseval networks are empirically and theoretically motivated by an analysis of the robustness of the predictions made by deep neural networks when their input is subject to an adversarial perturbation. The most important feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (approximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art regarding accuracy on CIFAR-10/100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterpart against adversarial examples. Incidentally, Parseval networks also tend to train faster and make a better usage of the full capacity of the networks.},
	language = {en},
	urldate = {2022-01-25},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard and Dauphin, Yann and Usunier, Nicolas},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {854--863},
}

@inproceedings{kaushik_explaining_2021,
	title = {Explaining the {Efficacy} of {Counterfactually} {Augmented} {Data}},
	url = {https://openreview.net/forum?id=HHiiQKWsOcV},
	abstract = {In attempts to produce machine learning models less reliant on spurious patterns in NLP datasets, researchers have recently proposed curating counterfactually augmented data (CAD) via a...},
	language = {en},
	urldate = {2021-12-19},
	author = {Kaushik, Divyansh and Setlur, Amrith and Hovy, Eduard H. and Lipton, Zachary Chase},
	year = {2021},
}

@inproceedings{engstrom_exploring_2019,
	title = {Exploring the {Landscape} of {Spatial} {Robustness}},
	url = {https://proceedings.mlr.press/v97/engstrom19a.html},
	abstract = {The study of adversarial robustness has so far largely focused on perturbations bound in \${\textbackslash}ell\_p\$-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network–based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and test-time input aggregation to significantly improve robustness. Finally we find that, in contrast to the \${\textbackslash}ell\_p\$-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study.},
	language = {en},
	urldate = {2022-01-22},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1802--1811},
}

@article{wang_image_2004,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	issn = {1941-0042},
	shorttitle = {Image quality assessment},
	doi = {10.1109/TIP.2003.819861},
	abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
	number = {4},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Data mining, Degradation, Humans, Image quality, Indexes, Layout, Quality assessment, Transform coding, Visual perception, Visual system},
	pages = {600--612},
}

@article{inoue_data_2018,
	title = {Data {Augmentation} by {Pairing} {Samples} for {Images} {Classification}},
	url = {http://arxiv.org/abs/1801.02929},
	abstract = {Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate \$N{\textasciicircum}2\$ new samples from \$N\$ training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5\% to 29.0\% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22\% to 6.93\% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.},
	urldate = {2022-01-10},
	journal = {arXiv:1801.02929 [cs, stat]},
	author = {Inoue, Hiroshi},
	month = apr,
	year = {2018},
	note = {arXiv: 1801.02929},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sun_can_2021,
	title = {Can {Shape} {Structure} {Features} {Improve} {Model} {Robustness} {Under} {Diverse} {Adversarial} {Settings}?},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Sun_Can_Shape_Structure_Features_Improve_Model_Robustness_Under_Diverse_Adversarial_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-08},
	author = {Sun, Mingjie and Li, Zichao and Xiao, Chaowei and Qiu, Haonan and Kailkhura, Bhavya and Liu, Mingyan and Li, Bo},
	year = {2021},
	pages = {7526--7535},
}

@inproceedings{li_towards_2021,
	title = {Towards {Robustness} of {Deep} {Neural} {Networks} via {Regularization}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Li_Towards_Robustness_of_Deep_Neural_Networks_via_Regularization_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-08},
	author = {Li, Yao and Min, Martin Renqiang and Lee, Thomas and Yu, Wenchao and Kruus, Erik and Wang, Wei and Hsieh, Cho-Jui},
	year = {2021},
	pages = {7496--7505},
}

@inproceedings{huang_stochastic_2021,
	title = {Stochastic {Partial} {Swap}: {Enhanced} {Model} {Generalization} and {Interpretability} for {Fine}-{Grained} {Recognition}},
	shorttitle = {Stochastic {Partial} {Swap}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Huang_Stochastic_Partial_Swap_Enhanced_Model_Generalization_and_Interpretability_for_Fine-Grained_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-08},
	author = {Huang, Shaoli and Wang, Xinchao and Tao, Dacheng},
	year = {2021},
	pages = {620--629},
}

@inproceedings{li_scouter_2021,
	title = {{SCOUTER}: {Slot} {Attention}-{Based} {Classifier} for {Explainable} {Image} {Recognition}},
	shorttitle = {{SCOUTER}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Li_SCOUTER_Slot_Attention-Based_Classifier_for_Explainable_Image_Recognition_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-07},
	author = {Li, Liangzhi and Wang, Bowen and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
	year = {2021},
	pages = {1046--1055},
}

@inproceedings{islam_global_2021,
	title = {Global {Pooling}, {More} {Than} {Meets} the {Eye}: {Position} {Information} {Is} {Encoded} {Channel}-{Wise} in {CNNs}},
	shorttitle = {Global {Pooling}, {More} {Than} {Meets} the {Eye}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Islam_Global_Pooling_More_Than_Meets_the_Eye_Position_Information_Is_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-07},
	author = {Islam, Md Amirul and Kowal, Matthew and Jia, Sen and Derpanis, Konstantinos G. and Bruce, Neil D. B.},
	year = {2021},
	pages = {793--801},
}

@inproceedings{zhu_toward_2021,
	title = {Toward {Human}-{Like} {Grasp}: {Dexterous} {Grasping} via {Semantic} {Representation} of {Object}-{Hand}},
	shorttitle = {Toward {Human}-{Like} {Grasp}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Zhu_Toward_Human-Like_Grasp_Dexterous_Grasping_via_Semantic_Representation_of_Object-Hand_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-05},
	author = {Zhu, Tianqiang and Wu, Rina and Lin, Xiangbo and Sun, Yi},
	year = {2021},
	pages = {15741--15751},
}

@inproceedings{ruiz_generating_2021,
	title = {Generating {Attribution} {Maps} {With} {Disentangled} {Masked} {Backpropagation}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Ruiz_Generating_Attribution_Maps_With_Disentangled_Masked_Backpropagation_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-05},
	author = {Ruiz, Adria and Agudo, Antonio and Moreno-Noguer, Francesc},
	year = {2021},
	pages = {905--914},
}

@inproceedings{lang_explaining_2021,
	title = {Explaining in {Style}: {Training} a {GAN} {To} {Explain} a {Classifier} in {StyleSpace}},
	shorttitle = {Explaining in {Style}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Lang_Explaining_in_Style_Training_a_GAN_To_Explain_a_Classifier_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-05},
	author = {Lang, Oran and Gandelsman, Yossi and Yarom, Michal and Wald, Yoav and Elidan, Gal and Hassidim, Avinatan and Freeman, William T. and Isola, Phillip and Globerson, Amir and Irani, Michal and Mosseri, Inbar},
	year = {2021},
	pages = {693--702},
}

@article{chen_deep_2021,
	title = {Deep reinforcement learning based moving object grasping},
	volume = {565},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521001158},
	doi = {10.1016/j.ins.2021.01.077},
	abstract = {Traditional grasping methods for locating unpredictable positions of moving objects under an unstructured environment cannot achieve good performance. This paper studies the utilization of deep reinforcement learning (DRL) with a Kinect depth sensor to resolve this challenging problem. The proposed grasping system integrates the DRL algorithm, Soft-Actor-Critic, and object detection techniques to implement an approaching-tracking-grasping scheme. Considering the state and action space for the high-degree-of-freedom manipulator, we employ an improved Soft-Actor-Critic algorithm to speed up the learning process. The proposed system can decouple object detection from the DRL control, which allows us to generalize the framework from a simulation environment to a real robot. Experimental results demonstrate that the developed system can autonomously grasp a moving object with different moving trajectories.},
	language = {en},
	urldate = {2022-01-05},
	journal = {Information Sciences},
	author = {Chen, Pengzhan and Lu, Weiqing},
	month = jul,
	year = {2021},
	keywords = {Grasping planning, Moving object, Object detection, Soft-Actor-Critic algorithm},
	pages = {62--76},
}

@article{carneiro_robot_2021,
	title = {Robot {Anticipation} {Learning} {System} for {Ball} {Catching}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2218-6581/10/4/113},
	doi = {10.3390/robotics10040113},
	abstract = {Catching flying objects is a challenging task in human–robot interaction. Traditional techniques predict the intersection position and time using the information obtained during the free-flying ball motion. A common pain point in these systems is the short ball flight time and uncertainties in the ball’s trajectory estimation. In this paper, we present the Robot Anticipation Learning System (RALS) that accounts for the information obtained from observation of the thrower’s hand motion before the ball is released. RALS takes extra time for the robot to start moving in the direction of the target before the opponent finishes throwing. To the best of our knowledge, this is the first robot control system for ball-catching with anticipation skills. Our results show that the information fused from both throwing and flying motions improves the ball-catching rate by up to 20\% compared to the baseline approach, with the predictions relying only on the information acquired during the flight phase.},
	language = {en},
	number = {4},
	urldate = {2022-01-05},
	journal = {Robotics},
	author = {Carneiro, Diogo and Silva, Filipe and Georgieva, Petia},
	month = dec,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {anticipation learning, ball catching, human–robot interaction, neural network, trajectory prediction},
	pages = {113},
}

@inproceedings{yu_neural_2021,
	title = {Neural {Motion} {Prediction} for {In}-flight {Uneven} {Object} {Catching}},
	doi = {10.1109/IROS51168.2021.9635983},
	abstract = {In-flight objects capture is extremely challenging. The robot is required to complete trajectory prediction, interception position calculation and motion planning within tens of milliseconds. As in-flight uneven objects are affected by various kinds of forces, which leads to the time-varying acceleration, motion prediction for them is difficult. In order to compensate the system’s non-linearity, we propose using a recurrent neural network model, which we call the Neural Acceleration Estimator (NAE), to estimate the varying acceleration by observing a small fragment of previous deflected trajectory without any prior information. Moreover, end-to-end training with Differantiable Filter (NAE-DF) gives a supervision for measurement uncertainty and further improves the prediction accuracy. Experimental results show that motion prediction with NAE and NAE-DF is superior to other methods and has a good generalization performance on unseen objects. We test our methods on a robot, performing velocity control in real world and respectively achieve 83.3\% and 86.7\% success rate on a ploy urethane banana and a gourd. We also release an object in-flight dataset containing 1,500 trajectorys for uneven objects, which can be found on the project website:https://sites.google.com/view/neural-motion-prediction.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Yu, Hongxiang and Guo, Dashun and Yin, Huan and Chen, Anzhe and Xu, Kechun and Chen, Zexi and Wang, Minhang and Tan, Qimeng and Wang, Yue and Xiong, Rong},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Accelerometers, Measurement uncertainty, Predictive models, Robot kinematics, Training, Uncertainty, Velocity control},
	pages = {4662--4669},
}

@article{kim_catching_2014,
	title = {Catching {Objects} in {Flight}},
	volume = {30},
	issn = {1941-0468},
	doi = {10.1109/TRO.2014.2316022},
	abstract = {We address the difficult problem of catching in-flight objects with uneven shapes. This requires the solution of three complex problems: accurate prediction of the trajectory of fastmoving objects, predicting the feasible catching configuration, and planning the arm motion, and all within milliseconds. We follow a programming-by-demonstration approach in order to learn, from throwing examples, models of the object dynamics and arm movement. We propose a new methodology to find a feasible catching configuration in a probabilistic manner. We use the dynamical systems approach to encode motion from several demonstrations. This enables a rapid and reactive adaptation of the arm motion in the presence of sensor uncertainty. We validate the approach in simulation with the iCub humanoid robot and in real-world experiments with the KUKA LWR 4+ (7-degree-of-freedom arm robot) to catch a hammer, a tennis racket, an empty bottle, a partially filled bottle, and a cardboard box.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Kim, Seungsu and Shukla, Ashwini and Billard, Aude},
	month = oct,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Aerospace electronics, Catching, Dynamics, Gaussian mixture model, Grasping, Robot kinematics, Robot sensing systems, Trajectory, machine learning, robot control, support vector machines},
	pages = {1049--1065},
}

@inproceedings{luo_catching_2021,
	address = {New York, NY, USA},
	series = {{MIG} '21},
	title = {Catching and {Throwing} {Control} of a {Physically} {Simulated} {Hand}},
	isbn = {978-1-4503-9131-3},
	url = {https://doi.org/10.1145/3487983.3488300},
	doi = {10.1145/3487983.3488300},
	abstract = {We design a nominal controller for animating an articulated physics-based human arm model, including the hands and fingers, to catch and throw objects. The controller is based on a finite state machine that defines the target poses for proportional-derivative control of the hand, as well as the orientation and position of the center of the palm using the solution of an inverse kinematics solver. We then use reinforcement learning to train agents to improve the robustness of the nominal controller for achieving many different goals. Imitation learning based on trajectories output by a numerical optimization is used to accelerate the training process. The success of our controllers is demonstrated by a variety of throwing and catching tasks, including flipping objects, hitting targets, and throwing objects to a desired height, and for several different objects, such as cans, spheres, and rods. We also discuss ways to extend our approach so that more challenging tasks, such as juggling, may be accomplished.},
	urldate = {2022-01-04},
	booktitle = {Motion, {Interaction} and {Games}},
	publisher = {Association for Computing Machinery},
	author = {Luo, Yunhao and Xie, Kaixiang and Andrews, Sheldon and Kry, Paul},
	month = nov,
	year = {2021},
	keywords = {catching, grasping, hand simulation, physics-based animation, throwing},
	pages = {1--7},
}

@inproceedings{baxter_exploring_2021,
	title = {Exploring {Learning} for {Intercepting} {Projectiles} with a {Robot}-{Held} {Stick}},
	doi = {10.1109/IROS51168.2021.9635993},
	abstract = {For many tasks, including table tennis, catching, and sword fighting, a critical step is intercepting the incoming object with a robot arm or held tool. Solutions to robot arm interception via learning, specifically reinforcement learning (RL), have become prevalent, as they provide robust solutions to the robot arm interception problem, even for high degree of freedom robotic systems. Despite numerous solutions, there has been little exploration into the factors of learning that impact solution quality. Thus, there is little insight into what problem features lead to better learning success. In this paper, we explore the parameters that impact solution quality. We find that link position observations outperform joint angle observations in terms of learning speed, performance, ability to utilize more than one frame of observation, and generalization to situations not trained for. These results are immediately applicable to RL for robot arm interception tasks.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Baxter, John E. G. and Adamson, Torin and Sugaya, Satomi and Tapia, Lydia},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Arms, Kinematics, Manipulators, Neurons, Projectiles, Reinforcement learning, Task analysis},
	pages = {369--376},
}

@article{maurice_human_2019,
	title = {Human movement and ergonomics: {An} industry-oriented dataset for collaborative robotics},
	abstract = {Improving work conditions in industry is a major challenge that can be addressed with new emerging technologies such as collaborative robots. Machine learning techniques can improve the performance of those robots, by endowing them with a degree of awareness of the human state and ergonomics condition. The availability of appropriate datasets to learn models and test prediction and control algorithms, however, remains an issue. This article presents a dataset of human motions in industry-like activities, fully labeled according to the ergonomics assessment worksheet EAWS, widely used in industries such as car manufacturing. Thirteen participants performed several series of activities, such as screwing and manipulating loads under different conditions, resulting in more than 5 hours of data. The dataset contains the participants’ whole-body kinematics recorded both with wearable inertial sensors and marker-based optical motion capture, finger pressure force, video recordings, and annotations by three independent annotators of the performed action and the adopted posture following the EAWS postural grid. Sensor data are available in different formats to facilitate their reuse. The dataset is intended for use by researchers developing algorithms for classifying, predicting, or evaluating human motion in industrial settings, as well as researchers developing collaborative robotics solutions that aim at improving the workers’ ergonomics. The annotation of the whole dataset following an ergonomics standard makes it valuable for ergonomics-related applications, but we expect its use to be broader in the robotics, machine learning, and human movement communities.},
	language = {en},
	journal = {The International Journal of Robotics Research},
	author = {Maurice, Pauline and Malaisé, Adrien and Amiot, Clélie and Paris, Nicolas and Richard, Guy-Junior and Rochel, Olivier and Ivaldi, Serena},
	year = {2019},
	pages = {9},
}

@inproceedings{zeng_visual_2020,
	title = {Visual {Reaction}: {Learning} to {Play} {Catch} {With} {Your} {Drone}},
	shorttitle = {Visual {Reaction}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zeng_Visual_Reaction_Learning_to_Play_Catch_With_Your_Drone_CVPR_2020_paper.html},
	urldate = {2022-01-05},
	author = {Zeng, Kuo-Hao and Mottaghi, Roozbeh and Weihs, Luca and Farhadi, Ali},
	year = {2020},
	pages = {11573--11582},
}

@inproceedings{kim_keep_2021,
	title = {Keep {CALM} and {Improve} {Visual} {Feature} {Attribution}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Kim_Keep_CALM_and_Improve_Visual_Feature_Attribution_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Kim, Jae Myung and Choe, Junsuk and Akata, Zeynep and Oh, Seong Joon},
	year = {2021},
	pages = {8350--8360},
}

@inproceedings{jung_towards_2021,
	title = {Towards {Better} {Explanations} of {Class} {Activation} {Mapping}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Jung_Towards_Better_Explanations_of_Class_Activation_Mapping_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Jung, Hyungsik and Oh, Youngrock},
	year = {2021},
	pages = {1336--1344},
}

@inproceedings{benz_batch_2021,
	title = {Batch {Normalization} {Increases} {Adversarial} {Vulnerability} and {Decreases} {Adversarial} {Transferability}: {A} {Non}-{Robust} {Feature} {Perspective}},
	shorttitle = {Batch {Normalization} {Increases} {Adversarial} {Vulnerability} and {Decreases} {Adversarial} {Transferability}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Benz_Batch_Normalization_Increases_Adversarial_Vulnerability_and_Decreases_Adversarial_Transferability_A_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Benz, Philipp and Zhang, Chaoning and Kweon, In So},
	year = {2021},
	pages = {7818--7827},
}

@inproceedings{stutz_relating_2021,
	title = {Relating {Adversarially} {Robust} {Generalization} to {Flat} {Minima}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Stutz_Relating_Adversarially_Robust_Generalization_to_Flat_Minima_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	year = {2021},
	pages = {7807--7817},
}

@inproceedings{chockler_explanations_2021,
	title = {Explanations for {Occluded} {Images}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Chockler_Explanations_for_Occluded_Images_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Chockler, Hana and Kroening, Daniel and Sun, Youcheng},
	year = {2021},
	pages = {1234--1243},
}

@inproceedings{li_discover_2021,
	title = {Discover the {Unknown} {Biased} {Attribute} of an {Image} {Classifier}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Li_Discover_the_Unknown_Biased_Attribute_of_an_Image_Classifier_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Li, Zhiheng and Xu, Chenliang},
	year = {2021},
	pages = {14970--14979},
}

@inproceedings{lerman_explaining_2021,
	title = {Explaining {Local}, {Global}, and {Higher}-{Order} {Interactions} in {Deep} {Learning}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Lerman_Explaining_Local_Global_and_Higher-Order_Interactions_in_Deep_Learning_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Lerman, Samuel and Venuto, Charles and Kautz, Henry and Xu, Chenliang},
	year = {2021},
	pages = {1224--1233},
}

@inproceedings{rodriguez_beyond_2021,
	title = {Beyond {Trivial} {Counterfactual} {Explanations} {With} {Diverse} {Valuable} {Explanations}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Rodriguez_Beyond_Trivial_Counterfactual_Explanations_With_Diverse_Valuable_Explanations_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Rodríguez, Pau and Caccia, Massimo and Lacoste, Alexandre and Zamparo, Lee and Laradji, Issam and Charlin, Laurent and Vazquez, David},
	year = {2021},
	pages = {1056--1065},
}

@inproceedings{zhu_towards_2021,
	title = {Towards {Understanding} the {Generative} {Capability} of {Adversarially} {Robust} {Classifiers}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Zhu_Towards_Understanding_the_Generative_Capability_of_Adversarially_Robust_Classifiers_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Zhu, Yao and Ma, Jiacheng and Sun, Jiacheng and Chen, Zewei and Jiang, Rongxin and Chen, Yaowu and Li, Zhenguo},
	year = {2021},
	pages = {7728--7737},
}

@inproceedings{sariyildiz_concept_2021,
	title = {Concept {Generalization} in {Visual} {Representation} {Learning}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Sariyildiz_Concept_Generalization_in_Visual_Representation_Learning_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Sariyildiz, Mert Bulent and Kalantidis, Yannis and Larlus, Diane and Alahari, Karteek},
	year = {2021},
	pages = {9629--9639},
}

@inproceedings{wang_causal_2021,
	title = {Causal {Attention} for {Unbiased} {Visual} {Recognition}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Causal_Attention_for_Unbiased_Visual_Recognition_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-01-04},
	author = {Wang, Tan and Zhou, Chang and Sun, Qianru and Zhang, Hanwang},
	year = {2021},
	pages = {3091--3100},
}

@article{lingchen_uniformaugment_2020,
	title = {{UniformAugment}: {A} {Search}-free {Probabilistic} {Data} {Augmentation} {Approach}},
	shorttitle = {{UniformAugment}},
	url = {http://arxiv.org/abs/2003.14348},
	abstract = {Augmenting training datasets has been shown to improve the learning effectiveness for several computer vision tasks. A good augmentation produces an augmented dataset that adds variability while retaining the statistical properties of the original dataset. Some techniques, such as AutoAugment and Fast AutoAugment, have introduced a search phase to find a set of suitable augmentation policies for a given model and dataset. This comes at the cost of great computational overhead, adding up to several thousand GPU hours. More recently RandAugment was proposed to substantially speedup the search phase by approximating the search space by a couple of hyperparameters, but still incurring non-negligible cost for tuning those. In this paper we show that, under the assumption that the augmentation space is approximately distribution invariant, a uniform sampling over the continuous space of augmentation transformations is sufficient to train highly effective models. Based on that result we propose UniformAugment, an automated data augmentation approach that completely avoids a search phase. In addition to discussing the theoretical underpinning supporting our approach, we also use the standard datasets, as well as established models for image classification, to show that UniformAugment's effectiveness is comparable to the aforementioned methods, while still being highly efficient by virtue of not requiring any search.},
	urldate = {2022-01-04},
	journal = {arXiv:2003.14348 [cs]},
	author = {LingChen, Tom Ching and Khonsari, Ava and Lashkari, Amirreza and Nazari, Mina Rafi and Sambee, Jaspreet Singh and Nascimento, Mario A.},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.14348},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{zhang_adversarial_2020,
	title = {Adversarial {AutoAugment}},
	url = {https://openreview.net/forum?id=ByxdUySKvS},
	abstract = {We introduce the idea of adversarial learning into automatic data augmentation to improve the generalization  of a targe network.},
	language = {en},
	urldate = {2021-12-20},
	author = {Zhang, Xinyu and Wang, Qiang and Zhang, Jian and Zhong, Zhao},
	year = {2020},
}

@inproceedings{tran_bayesian_2017,
	title = {A {Bayesian} {Data} {Augmentation} {Approach} for {Learning} {Deep} {Models}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/076023edc9187cf1ac1f1163470e479a-Abstract.html},
	urldate = {2021-12-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tran, Toan and Pham, Trung and Carneiro, Gustavo and Palmer, Lyle and Reid, Ian},
	year = {2017},
}

@inproceedings{ye_h2o_2021,
	title = {{H2O}: {A} {Benchmark} for {Visual} {Human}-human {Object} {Handover} {Analysis}},
	shorttitle = {{H2O}},
	url = {http://arxiv.org/abs/2104.11466},
	abstract = {Object handover is a common human collaboration behavior that attracts attention from researchers in Robotics and Cognitive Science. Though visual perception plays an important role in the object handover task, the whole handover process has been specifically explored. In this work, we propose a novel rich-annotated dataset, H2O, for visual analysis of human-human object handovers. The H2O, which contains 18K video clips involving 15 people who hand over 30 objects to each other, is a multi-purpose benchmark. It can support several vision-based tasks, from which, we specifically provide a baseline method, RGPNet, for a less-explored task named Receiver Grasp Prediction. Extensive experiments show that the RGPNet can produce plausible grasps based on the giver's hand-object states in the pre-handover phase. Besides, we also report the hand and object pose errors with existing baselines and show that the dataset can serve as the video demonstrations for robot imitation learning on the handover task. Dataset, model and code will be made public.},
	urldate = {2021-12-24},
	booktitle = {International {Conference} on {Computer} {Vision}},
	author = {Ye, Ruolin and Xu, Wenqiang and Xue, Zhendong and Tang, Tutian and Wang, Yanfeng and Lu, Cewu},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.11466},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@inproceedings{katharopoulos_not_2018,
	title = {Not {All} {Samples} {Are} {Created} {Equal}: {Deep} {Learning} with {Importance} {Sampling}},
	shorttitle = {Not {All} {Samples} {Are} {Created} {Equal}},
	url = {https://proceedings.mlr.press/v80/katharopoulos18a.html},
	abstract = {Deep Neural Network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on "informative" examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard SGD procedure, and we demonstrate experimentally on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5\% and 17\%.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Katharopoulos, Angelos and Fleuret, Francois},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2525--2534},
}

@article{he_data_2019,
	title = {Data {Augmentation} {Revisited}: {Rethinking} the {Distribution} {Gap} between {Clean} and {Augmented} {Data}},
	shorttitle = {Data {Augmentation} {Revisited}},
	url = {http://arxiv.org/abs/1909.09148},
	abstract = {Data augmentation has been widely applied as an effective methodology to improve generalization in particular when training deep neural networks. Recently, researchers proposed a few intensive data augmentation techniques, which indeed improved accuracy, yet we notice that these methods augment data have also caused a considerable gap between clean and augmented data. In this paper, we revisit this problem from an analytical perspective, for which we estimate the upper-bound of expected risk using two terms, namely, empirical risk and generalization error, respectively. We develop an understanding of data augmentation as regularization, which highlights the major features. As a result, data augmentation significantly reduces the generalization error, but meanwhile leads to a slightly higher empirical risk. On the assumption that data augmentation helps models converge to a better region, the model can benefit from a lower empirical risk achieved by a simple method, i.e., using less-augmented data to refine the model trained on fully-augmented data. Our approach achieves consistent accuracy gain on a few standard image classification benchmarks, and the gain transfers to object detection.},
	urldate = {2021-12-20},
	journal = {arXiv:1909.09148 [cs, stat]},
	author = {He, Zhuoxun and Xie, Lingxi and Chen, Xin and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
	month = nov,
	year = {2019},
	note = {arXiv: 1909.09148},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{hataya_faster_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Faster {AutoAugment}: {Learning} {Augmentation} {Strategies} {Using} {Backpropagation}},
	isbn = {978-3-030-58595-2},
	shorttitle = {Faster {AutoAugment}},
	doi = {10.1007/978-3-030-58595-2_1},
	abstract = {Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as a differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented and original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior methods without a performance drop.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Hataya, Ryuichiro and Zdenek, Jan and Yoshizoe, Kazuki and Nakayama, Hideki},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {1--16},
}

@inproceedings{lim_fast_2019,
	title = {Fast {AutoAugment}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/6add07cf50424b14fdf649da87843d01-Abstract.html},
	urldate = {2021-12-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
	year = {2019},
}

@article{inoue_data_2018,
	title = {Data {Augmentation} by {Pairing} {Samples} for {Images} {Classification}},
	url = {https://arxiv.org/abs/1801.02929v2},
	abstract = {Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate \$N{\textasciicircum}2\$ new samples from \$N\$ training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5\% to 29.0\% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22\% to 6.93\% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.},
	language = {en},
	urldate = {2021-12-20},
	author = {Inoue, Hiroshi},
	month = jan,
	year = {2018},
}

@inproceedings{zhong_random_2020,
	title = {Random {Erasing} {Data} {Augmentation}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7000},
	doi = {10.1609/aaai.v34i07.7000},
	abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {13001--13008},
}

@inproceedings{marques-silva_explanations_2021,
	title = {Explanations for {Monotonic} {Classifiers}},
	url = {https://proceedings.mlr.press/v139/marques-silva21a.html},
	abstract = {In many classification tasks there is a requirement of monotonicity. Concretely, if all else remains constant, increasing (resp. decreasing) the value of one or more features must not decrease (resp. increase) the value of the prediction. Despite comprehensive efforts on learning monotonic classifiers, dedicated approaches for explaining monotonic classifiers are scarce and classifier-specific. This paper describes novel algorithms for the computation of one formal explanation of a (black-box) monotonic classifier. These novel algorithms are polynomial (indeed linear) in the run time complexity of the classifier. Furthermore, the paper presents a practically efficient model-agnostic algorithm for enumerating formal explanations.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Marques-Silva, Joao and Gerspacher, Thomas and Cooper, Martin C. and Ignatiev, Alexey and Narodytska, Nina},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {7469--7479},
}

@inproceedings{xu_be_2021,
	title = {To be {Robust} or to be {Fair}: {Towards} {Fairness} in {Adversarial} {Training}},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Xu, Han and Liu, Xiaorui and Li, Yaxin and Jain, Anil K and Tang, Jiliang},
	year = {2021},
	pages = {10},
}

@inproceedings{trauble_disentangled_2021,
	title = {On {Disentangled} {Representations} {Learned} from {Correlated} {Data}},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Träuble, Frederik and Creager, Elliot and Kilbertus, Niki and Locatello, Francesco and Dittadi, Andrea and Goyal, Anirudh and Schölkopf, Bernhard and Bauer, Stefan},
	year = {2021},
	pages = {12},
}

@inproceedings{oberst_regularizing_2021,
	title = {Regularizing towards {Causal} {Invariance}: {Linear} {Models} with {Proxies}},
	abstract = {We propose a method for learning linear models whose predictive performance is robust to causal interventions on unobserved variables, when noisy proxies of those variables are available. Our approach takes the form of a regularization term that trades off between in-distribution performance and robustness to interventions. Under the assumption of a linear structural causal model, we show that a single proxy can be used to create estimators that are prediction optimal under interventions of bounded strength. This strength depends on the magnitude of the measurement noise in the proxy, which is, in general, not identiﬁable. In the case of two proxy variables, we propose a modiﬁed estimator that is prediction optimal under interventions up to a known strength. We further show how to extend these estimators to scenarios where additional information about the “test time” intervention is available during training. We evaluate our theoretical ﬁndings in synthetic experiments and using real data of hourly pollution levels across several cities in China.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Oberst, Michael and Thams, Nikolaj and Peters, Jonas and Sontag, David},
	year = {2021},
	pages = {11},
}

@inproceedings{moshkovitz_connecting_2021,
	title = {Connecting {Interpretability} and {Robustness} in {Decision} {Trees} through {Separation}},
	abstract = {Recent research has recognized interpretability and robustness as essential properties of trustworthy classiﬁcation. Curiously, a connection between robustness and interpretability was empirically observed, but the theoretical reasoning behind it remained elusive. In this paper, we rigorously investigate this connection. Speciﬁcally, we focus on interpretation using decision trees and robustness to l1-perturbation. Previous works deﬁned the notion of r-separation as a sufﬁcient condition for robustness. We prove upper and lower bounds on the tree size in case the data is r-separated. We then show that a tighter bound on the size is possible when the data is linearly separated. We provide the ﬁrst algorithm with provable guarantees both on robustness, interpretability, and accuracy in the context of decision trees. Experiments conﬁrm that our algorithm yields classiﬁers that are both interpretable and robust and have high accuracy. The code for the experiments is available at https://github.com/yangarbiter/ interpretable-robust-trees.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Moshkovitz, Michal and Yang, Yao-Yuan and Chaudhuri, Kamalika},
	year = {2021},
	pages = {11},
}

@inproceedings{mahajan_domain_2021,
	title = {Domain {Generalization} using {Causal} {Matching}},
	abstract = {In the domain generalization literature, a common objective is to learn representations independent of the domain after conditioning on the class label. We show that this objective is not sufﬁcient: there exist counter-examples where a model fails to generalize to unseen domains even after satisfying class-conditional domain invariance. We formalize this observation through a structural causal model and show the importance of modeling within-class variations for generalization. Speciﬁcally, classes contain objects that characterize speciﬁc causal features, and domains can be interpreted as interventions on these objects that change non-causal features. We highlight an alternative condition: inputs across domains should have the same representation if they are derived from the same object. Based on this objective, we propose matching-based algorithms when base objects are observed (e.g., through data augmentation) and approximate the objective when objects are not observed (MatchDG). Our simple matching-based algorithms are competitive to prior work on out-of-domain accuracy for rotated MNIST, Fashion-MNIST, PACS, and Chest-Xray datasets. Our method MatchDG also recovers ground-truth object matches: on MNIST and Fashion-MNIST, top-10 matches from MatchDG have over 50\% overlap with ground-truth matches.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Mahajan, Divyat and Tople, Shruti and Sharma, Amit},
	year = {2021},
	pages = {12},
}

@inproceedings{laber_price_2021,
	title = {On the price of explainability for some clustering problems},
	abstract = {The price of explainability for a clustering task can be deﬁned as the unavoidable loss, in terms of the objective function, if we force the ﬁnal partition to be explainable. Here, we study this price for the following clustering problems: k-means, k-medians, k-centers and maximum-spacing. We provide upper and lower bounds for a natural model where explainability is achieved via decision trees. For the k-means and k-medians problems our upper bounds improve those obtained by [Dasgupta et. al, ICML 20] for low dimensions. Another contribution is a simple and efﬁcient algorithm for building explainable clusterings for the k-means problem. We provide empirical evidence that its performance is better than the current state of the art for decision-tree based explainable clustering.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Laber, Eduardo and Murtinho, Lucas},
	year = {2021},
	pages = {11},
}

@inproceedings{gurel_knowledge_2021,
	title = {Knowledge {Enhanced} {Machine} {Learning} {Pipeline}  against {Diverse} {Adversarial} {Attacks}},
	abstract = {Despite the great successes achieved by deep neural networks (DNNs), recent studies show that they are vulnerable against adversarial examples, which aim to mislead DNNs by adding small adversarial perturbations. Several defenses have been proposed against such attacks, while many of them have been adaptively attacked. In this work, we aim to enhance the ML robustness from a different perspective by leveraging domain knowledge: We propose a Knowledge Enhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e., logic relationships among different predictions) into a probabilistic graphical model via ﬁrst-order logic rules. In particular, we develop KEMLP by integrating a diverse set of weak auxiliary models based on their logical relationships to the main DNN model that performs the target task. Theoretically, we provide convergence results and prove that, under mild conditions, the prediction of KEMLP is more robust than that of the main DNN model. Empirically, we take road sign recognition as an example and leverage the relationships between road signs and their shapes and contents as domain knowledge. We show that compared with adversarial training and other baselines, KEMLP achieves higher robustness against physical attacks, Lp bounded attacks, unforeseen attacks, and natural corruptions under both whitebox and blackbox settings, while still maintaining high clean accuracy.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Gürel, Nezihe Merve and Qi, Xiangyu and Rimanic, Luka and Zhang, Ce and Li, Bo},
	year = {2021},
	pages = {12},
}

@inproceedings{garreau_what_2021,
	title = {What {Does} {LIME} {Really} {See} in {Images}?},
	abstract = {The performance of modern algorithms on certain computer vision tasks such as object recognition is now close to that of humans. This success was achieved at the price of complicated architectures depending on millions of parameters and it has become quite challenging to understand how particular predictions are made. Interpretability methods propose to give us this understanding. In this paper, we study LIME, perhaps one of the most popular. On the theoretical side, we show that when the number of generated examples is large, LIME explanations are concentrated around a limit explanation for which we give an explicit expression. We further this study for elementary shape detectors and linear models. As a consequence of this analysis, we uncover a connection between LIME and integrated gradients, another explanation method. More precisely, the LIME explanations are similar to the sum of integrated gradients over the superpixels used in the preprocessing step of LIME.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Garreau, Damien and Mardaoui, Dina},
	year = {2021},
	pages = {10},
}

@inproceedings{caucheteux_disentangling_2021,
	title = {Disentangling {Syntax} and {Semantics} in the {Brain} with {Deep} {Networks}},
	abstract = {The activations of language transformers like GPT-2 have been shown to linearly map onto brain activity during speech comprehension. However, the nature of these activations remains largely unknown and presumably conﬂate distinct linguistic classes. Here, we propose a taxonomy to factorize the high-dimensional activations of language models into four combinatorial classes: lexical, compositional, syntactic, and semantic representations. We then introduce a statistical method to decompose, through the lens of GPT-2’s activations, the brain activity of 345 subjects recorded with functional magnetic resonance imaging (fMRI) during the listening of 4.6 hours of narrated text. The results highlight two ﬁndings. First, compositional representations recruit a more widespread cortical network than lexical ones, and encompass the bilateral temporal, parietal and prefrontal cortices. Second, contrary to previous claims, syntax and semantics are not associated with separated modules, but, instead, appear to share a common and distributed neural substrate. Overall, this study introduces a versatile framework to isolate, in the brain activity, the distributed representations of linguistic constructs.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Remi},
	year = {2021},
	pages = {13},
}

@inproceedings{biggs_model_2021,
	title = {Model {Distillation} for {Revenue} {Optimization}: {Interpretable} {Personalized} {Pricing}},
	abstract = {Data-driven pricing strategies are becoming increasingly common, where customers are offered a personalized price based on features that are predictive of their valuation of a product. It is desirable for this pricing policy to be simple and interpretable, so it can be veriﬁed, checked for fairness, and easily implemented. However, efforts to incorporate machine learning into a pricing framework often lead to complex pricing policies which are not interpretable, resulting in slow adoption in practice. We present a customized, prescriptive tree-based algorithm that distills knowledge from a complex black-box machine learning algorithm, segments customers with similar valuations and prescribes prices in such a way that maximizes revenue while maintaining interpretability. We quantify the regret of a resulting policy and demonstrate its efﬁcacy in applications with both synthetic and real-world datasets.},
	language = {en},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Biggs, Max and Sun, Wei and Ettl, Markus},
	year = {2021},
	pages = {11},
}

@inproceedings{wong_leveraging_2021,
	title = {Leveraging {Sparse} {Linear} {Layers} for {Debuggable} {Deep} {Networks}},
	url = {https://proceedings.mlr.press/v139/wong21b.html},
	abstract = {We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantitatively and via human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wong, Eric and Santurkar, Shibani and Madry, Aleksander},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11205--11216},
}

@inproceedings{afchar_towards_2021,
	title = {Towards {Rigorous} {Interpretations}: a {Formalisation} of {Feature} {Attribution}},
	shorttitle = {Towards {Rigorous} {Interpretations}},
	url = {https://proceedings.mlr.press/v139/afchar21a.html},
	abstract = {Feature attribution is often loosely presented as the process of selecting a subset of relevant features as a rationale of a prediction. Task-dependent by nature, precise definitions of "relevance" encountered in the literature are however not always consistent. This lack of clarity stems from the fact that we usually do not have access to any notion of ground-truth attribution and from a more general debate on what good interpretations are. In this paper we propose to formalise feature selection/attribution based on the concept of relaxed functional dependence. In particular, we extend our notions to the instance-wise setting and derive necessary properties for candidate selection solutions, while leaving room for task-dependence. By computing ground-truth attributions on synthetic datasets, we evaluate many state-of-the-art attribution methods and show that, even when optimised, some fail to verify the proposed properties and provide wrong solutions.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Afchar, Darius and Guigue, Vincent and Hennequin, Romain},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {76--86},
}

@inproceedings{catav_marginal_2021,
	title = {Marginal {Contribution} {Feature} {Importance} - an {Axiomatic} {Approach} for {Explaining} {Data}},
	url = {https://proceedings.mlr.press/v139/catav21a.html},
	abstract = {In recent years, methods were proposed for assigning feature importance scores to measure the contribution of individual features. While in some cases the goal is to understand a specific model, in many cases the goal is to understand the contribution of certain properties (features) to a real-world phenomenon. Thus, a distinction has been made between feature importance scores that explain a model and scores that explain the data. When explaining the data, machine learning models are used as proxies in settings where conducting many real-world experiments is expensive or prohibited. While existing feature importance scores show great success in explaining models, we demonstrate their limitations when explaining the data, especially in the presence of correlations between features. Therefore, we develop a set of axioms to capture properties expected from a feature importance score when explaining data and prove that there exists only one score that satisfies all of them, the Marginal Contribution Feature Importance (MCI). We analyze the theoretical properties of this score function and demonstrate its merits empirically.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Catav, Amnon and Fu, Boyang and Zoabi, Yazeed and Meilik, Ahuva Libi Weiss and Shomron, Noam and Ernst, Jason and Sankararaman, Sriram and Gilad-Bachrach, Ran},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1324--1335},
}

@inproceedings{hilgard_learning_2021,
	title = {Learning {Representations} by {Humans}, for {Humans}},
	url = {https://proceedings.mlr.press/v139/hilgard21a.html},
	abstract = {When machine predictors can achieve higher performance than the human decision-makers they support, improving the performance of human decision-makers is often conflated with improving machine accuracy. Here we propose a framework to directly support human decision-making, in which the role of machines is to reframe problems rather than to prescribe actions through prediction. Inspired by the success of representation learning in improving performance of machine predictors, our framework learns human-facing representations optimized for human performance. This “Mind Composed with Machine” framework incorporates a human decision-making model directly into the representation learning paradigm and is trained with a novel human-in-the-loop training procedure. We empirically demonstrate the successful application of the framework to various tasks and representational forms.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hilgard, Sophie and Rosenfeld, Nir and Banaji, Mahzarin R. and Cao, Jack and Parkes, David},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {4227--4238},
}

@inproceedings{lu_dance_2021,
	title = {{DANCE}: {Enhancing} saliency maps using decoys},
	shorttitle = {{DANCE}},
	url = {https://proceedings.mlr.press/v139/lu21b.html},
	abstract = {Saliency methods can make deep neural network predictions more interpretable by identifying a set of critical features in an input sample, such as pixels that contribute most strongly to a prediction made by an image classifier. Unfortunately, recent evidence suggests that many saliency methods poorly perform, especially in situations where gradients are saturated, inputs contain adversarial perturbations, or predictions rely upon inter-feature dependence. To address these issues, we propose a framework, DANCE, which improves the robustness of saliency methods by following a two-step procedure. First, we introduce a perturbation mechanism that subtly varies the input sample without changing its intermediate representations. Using this approach, we can gather a corpus of perturbed ("decoy") data samples while ensuring that the perturbed and original input samples follow similar distributions. Second, we compute saliency maps for the decoy samples and propose a new method to aggregate saliency maps. With this design, we offset influence of gradient saturation. From a theoretical perspective, we show that the aggregated saliency map not only captures inter-feature dependence but, more importantly, is robust against previously described adversarial perturbation methods. Our empirical results suggest that, both qualitatively and quantitatively, DANCE outperforms existing methods in a variety of application domains.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lu, Yang Young and Guo, Wenbo and Xing, Xinyu and Noble, William Stafford},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {7124--7133},
}

@book{williamson_design_2011,
	address = {Cambridge},
	title = {The {Design} of {Approximation} {Algorithms}},
	isbn = {978-0-511-92173-5},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511921735},
	language = {en},
	urldate = {2021-12-19},
	publisher = {Cambridge University Press},
	author = {Williamson, David P. and Shmoys, David B.},
	year = {2011},
	doi = {10.1017/CBO9780511921735},
}

@inproceedings{su_learning_2018,
	address = {Salt Lake City, UT},
	title = {Learning {Visual} {Knowledge} {Memory} {Networks} for {Visual} {Question} {Answering}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578905/},
	doi = {10.1109/CVPR.2018.00807},
	abstract = {Visual question answering (VQA) requires joint comprehension of images and natural language questions, where many questions can’t be directly or clearly answered from visual content but require reasoning from structured human knowledge with conﬁrmation from visual content. This paper proposes visual knowledge memory network (VKMN) to address this issue, which seamlessly incorporates structured human knowledge and deep visual features into memory networks in an end-to-end learning framework. Comparing to existing methods for leveraging external knowledge for supporting VQA, this paper stresses more on two missing mechanisms. First is the mechanism for integrating visual contents with knowledge facts. VKMN handles this issue by embedding knowledge triples (subject, relation, target) and deep visual features jointly into the visual knowledge features. Second is the mechanism for handling multiple knowledge facts expanding from question and answer pairs. VKMN stores joint embedding using key-value pair structure in the memory networks so that it is easy to handle multiple facts. Experiments show that the proposed method achieves promising results on both VQA v1.0 and v2.0 benchmarks, while outperforms state-of-the-art methods on the knowledge-reasoning related questions.},
	language = {en},
	urldate = {2021-12-19},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Su, Zhou and Zhu, Chen and Dong, Yinpeng and Cai, Dongqi and Chen, Yurong and Li, Jianguo},
	month = jun,
	year = {2018},
	pages = {7736--7745},
}

@article{denton_image_2020,
	title = {Image {Counterfactual} {Sensitivity} {Analysis} for {Detecting} {Unintended} {Bias}},
	url = {http://arxiv.org/abs/1906.06439},
	abstract = {Facial analysis models are increasingly used in applications that have serious impacts on people’s lives, ranging from authentication to surveillance tracking. It is therefore critical to develop techniques that can reveal unintended biases in facial classiﬁers to help guide the ethical use of facial analysis technology. This work proposes a framework called image counterfactual sensitivity analysis, which we explore as a proof-of-concept in analyzing a smiling attribute classiﬁer trained on faces of celebrities. The framework utilizes counterfactuals to examine how a classiﬁer’s prediction changes if a face characteristic slightly changes. We leverage recent advances in generative adversarial networks to build a realistic generative model of face images that affords controlled manipulation of speciﬁc image characteristics. We then introduce a set of metrics that measure the effect of manipulating a speciﬁc property on the output of the trained classiﬁer. Empirically, we ﬁnd several different factors of variation that affect the predictions of the smiling classiﬁer. This proof-of-concept demonstrates potential ways generative models can be leveraged for ﬁnegrained analysis of bias and fairness.},
	language = {en},
	urldate = {2021-12-19},
	journal = {arXiv:1906.06439 [cs, stat]},
	author = {Denton, Emily and Hutchinson, Ben and Mitchell, Margaret and Gebru, Timnit and Zaldivar, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv: 1906.06439},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sanh_learning_2020,
	title = {Learning from others' mistakes: {Avoiding} dataset biases without modeling them},
	shorttitle = {Learning from others' mistakes},
	url = {https://openreview.net/forum?id=Hf3qXoiNkR},
	abstract = {State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous...},
	language = {en},
	urldate = {2021-12-19},
	author = {Sanh, Victor and Wolf, Thomas and Belinkov, Yonatan and Rush, Alexander M.},
	month = sep,
	year = {2020},
}

@inproceedings{ding_prototypical_2020,
	title = {Prototypical {Representation} {Learning} for {Relation} {Extraction}},
	url = {https://openreview.net/forum?id=aCgLmfhIy_f},
	abstract = {Recognizing relations between entities is a pivotal task of relational learning.  
Learning relation representations from distantly-labeled datasets is difficult because of the abundant label noise...},
	language = {en},
	urldate = {2021-12-19},
	author = {Ding, Ning and Wang, Xiaobin and Fu, Yao and Xu, Guangwei and Wang, Rui and Xie, Pengjun and Shen, Ying and Huang, Fei and Zheng, Hai-Tao and Zhang, Rui},
	month = sep,
	year = {2020},
}

@inproceedings{xia_robust_2020,
	title = {Robust early-learning: {Hindering} the memorization of noisy labels},
	shorttitle = {Robust early-learning},
	url = {https://openreview.net/forum?id=Eql5b1_hTE4},
	abstract = {The {\textbackslash}textit\{memorization effects\} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The {\textbackslash}textit\{early stopping\} method therefore...},
	language = {en},
	urldate = {2021-12-19},
	author = {Xia, Xiaobo and Liu, Tongliang and Han, Bo and Gong, Chen and Wang, Nannan and Ge, Zongyuan and Chang, Yi},
	month = sep,
	year = {2020},
}

@inproceedings{barrett_implicit_2020,
	title = {Implicit {Gradient} {Regularization}},
	url = {https://openreview.net/forum?id=3q5IqUrkcF},
	abstract = {Gradient descent can be surprisingly good at optimizing deep neural networks without overfitting and without explicit regularization. We find that the discrete steps of gradient descent implicitly...},
	language = {en},
	urldate = {2021-12-19},
	author = {Barrett, David and Dherin, Benoit},
	month = sep,
	year = {2020},
}

@inproceedings{smith_origin_2020,
	title = {On the {Origin} of {Implicit} {Regularization} in {Stochastic} {Gradient} {Descent}},
	url = {https://openreview.net/forum?id=rq_Qr0c1Hyo},
	abstract = {For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher...},
	language = {en},
	urldate = {2021-12-19},
	author = {Smith, Samuel L. and Dherin, Benoit and Barrett, David and De, Soham},
	month = sep,
	year = {2020},
}

@inproceedings{montero_role_2020,
	title = {The role of {Disentanglement} in {Generalisation}},
	url = {https://openreview.net/forum?id=qbH974jKUVy},
	abstract = {Combinatorial generalisation — the ability to understand and produce novel combinations of familiar elements — is a core capacity of human intelligence that current AI systems struggle with....},
	language = {en},
	urldate = {2021-12-19},
	author = {Montero, Milton Llera and Ludwig, Casimir JH and Costa, Rui Ponte and Malhotra, Gaurav and Bowers, Jeffrey},
	month = sep,
	year = {2020},
}

@inproceedings{honke_representation_2020,
	title = {Representation learning for improved interpretability and classification accuracy of clinical factors from {EEG}},
	url = {https://openreview.net/forum?id=TVjLza1t4hI},
	abstract = {Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can...},
	language = {en},
	urldate = {2021-12-19},
	author = {Honke, Garrett and Higgins, Irina and Thigpen, Nina and Miskovic, Vladimir and Link, Katie and Duan, Sunny and Gupta, Pramod and Klawohn, Julia and Hajcak, Greg},
	month = sep,
	year = {2020},
}

@inproceedings{santurkar_breeds_2020,
	title = {{BREEDS}: {Benchmarks} for {Subpopulation} {Shift}},
	shorttitle = {{BREEDS}},
	url = {https://openreview.net/forum?id=mQPBmvyAuk},
	abstract = {We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during...},
	language = {en},
	urldate = {2021-12-19},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Madry, Aleksander},
	month = sep,
	year = {2020},
}

@inproceedings{liu_certifying_2019,
	title = {On {Certifying} {Non}-{Uniform} {Bounds} against {Adversarial} {Attacks}},
	url = {https://proceedings.mlr.press/v97/liu19h.html},
	abstract = {This work studies the robustness certification problem of neural network models, which aims to find certified adversary-free regions as large as possible around data points. In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models. We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method. Our experiments show the non-uniform bounds have larger volumes than uniform ones. Compared with normal models, the robust models have even larger non-uniform bounds and better interpretability. Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of input features’ robustness.},
	language = {en},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liu, Chen and Tomioka, Ryota and Cevher, Volkan},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4072--4081},
}

@inproceedings{guo_lemna_2018,
	address = {New York, NY, USA},
	series = {{CCS} '18},
	title = {{LEMNA}: {Explaining} {Deep} {Learning} based {Security} {Applications}},
	isbn = {978-1-4503-5693-0},
	shorttitle = {{LEMNA}},
	url = {https://doi.org/10.1145/3243734.3243792},
	doi = {10.1145/3243734.3243792},
	abstract = {While deep learning has shown a great potential in various domains, the lack of transparency has limited its application in security or safety-critical areas. Existing research has attempted to develop explanation techniques to provide interpretable explanations for each classification decision. Unfortunately, current methods are optimized for non-security tasks ( e.g., image analysis). Their key assumptions are often violated in security applications, leading to a poor explanation fidelity. In this paper, we propose LEMNA, a high-fidelity explanation method dedicated for security applications. Given an input data sample, LEMNA generates a small set of interpretable features to explain how the input sample is classified. The core idea is to approximate a local area of the complex deep learning decision boundary using a simple interpretable model. The local interpretable model is specially designed to (1) handle feature dependency to better work with security applications ( e.g., binary code analysis); and (2) handle nonlinear local boundaries to boost explanation fidelity. We evaluate our system using two popular deep learning applications in security (a malware classifier, and a function start detector for binary reverse-engineering). Extensive evaluations show that LEMNA's explanation has a much higher fidelity level compared to existing methods. In addition, we demonstrate practical use cases of LEMNA to help machine learning developers to validate model behavior, troubleshoot classification errors, and automatically patch the errors of the target models.},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Guo, Wenbo and Mu, Dongliang and Xu, Jun and Su, Purui and Wang, Gang and Xing, Xinyu},
	month = oct,
	year = {2018},
	keywords = {binary analysis, deep recurrent neural networks, explainable AI},
	pages = {364--379},
}

@inproceedings{pierazzi_intriguing_2020,
	title = {Intriguing {Properties} of {Adversarial} {ML} {Attacks} in the {Problem} {Space}},
	doi = {10.1109/SP40000.2020.00073},
	abstract = {Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored.This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains.Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that "adversarial-malware as a service" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Yet, out of the 1600+ papers on adversarial ML published in the past six years, roughly 40 focus on malware [15]-and many remain only in the feature space.Our formalization of problem-space attacks paves the way to more principled research in this domain. We responsibly release the code and dataset of our novel attack to other researchers, to encourage future work on defenses in the problem space.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Pierazzi, Fabio and Pendlebury, Feargus and Cortellazzi, Jacopo and Cavallaro, Lorenzo},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Androids, Humanoid robots, Malware, Perturbation methods, Robustness, Semantics, adversarial machine learning, evasion, input space, malware, problem space, program analysis},
	pages = {1332--1349},
}

@article{carratino_mixup_2020,
	title = {On {Mixup} {Regularization}},
	url = {http://arxiv.org/abs/2006.06049},
	abstract = {Mixup is a data augmentation technique that creates new examples as convex combinationsof training points and labels. This simple technique has empirically shown to improvethe accuracy of many state-of-the-art models in different settings and applications, butthe reasons behind this empirical success remain poorly understood. In this paper wetake a substantial step in explaining the theoretical foundations of Mixup, by clarifyingits regularization effects. We show that Mixup can be interpreted as standard empiricalrisk minimization estimator subject to a combination of data transformation and randomperturbation of the transformed data. We gain two core insights from this new interpretation.First, the data transformation suggests that, at test time, a model trained with Mixup shouldalso be applied to transformed data, a one-line change in code that we show empirically toimprove both accuracy and calibration of the prediction. Second, we show how the randomperturbation of the new interpretation of Mixup induces multiple known regularizationschemes, including label smoothing and reduction of the Lipschitz constant of the estimator.These schemes interact synergistically with each other, resulting in a self calibrated andeffective regularization effect that prevents overfitting and overconfident predictions. Wecorroborate our theoretical analysis with experiments that support our conclusions.},
	urldate = {2021-12-17},
	journal = {arXiv:2006.06049 [cs, stat]},
	author = {Carratino, Luigi and Cissé, Moustapha and Jenatton, Rodolphe and Vert, Jean-Philippe},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.06049},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{faramarzi_patchup_2020,
	title = {{PatchUp}: {A} {Regularization} {Technique} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{PatchUp}},
	url = {http://arxiv.org/abs/2006.07794},
	abstract = {Large capacity deep learning models are often prone to a high generalization gap when trained with a limited amount of labeled training data. A recent class of methods to address this problem uses various ways to construct a new training sample by mixing a pair (or more) of training samples. We propose PatchUp, a hidden state block-level regularization technique for Convolutional Neural Networks (CNNs), that is applied on selected contiguous blocks of feature maps from a random pair of samples. Our approach improves the robustness of CNN models against the manifold intrusion problem that may occur in other state-of-the-art mixing approaches like Mixup and CutMix. Moreover, since we are mixing the contiguous block of features in the hidden space, which has more dimensions than the input space, we obtain more diverse samples for training towards different dimensions. Our experiments on CIFAR-10, CIFAR-100, and SVHN datasets with PreactResnet18, PreactResnet34, and WideResnet-28-10 models show that PatchUp improves upon, or equals, the performance of current state-of-the-art regularizers for CNNs. We also show that PatchUp can provide better generalization to affine transformations of samples and is more robust against adversarial attacks.},
	urldate = {2021-12-17},
	journal = {arXiv:2006.07794 [cs, stat]},
	author = {Faramarzi, Mojtaba and Amini, Mohammad and Badrinaaraayanan, Akilesh and Verma, Vikas and Chandar, Sarath},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07794},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{verma_manifold_2019,
	title = {Manifold {Mixup}: {Better} {Representations} by {Interpolating} {Hidden} {States}},
	shorttitle = {Manifold {Mixup}},
	url = {https://proceedings.mlr.press/v97/verma19a.html},
	abstract = {Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose {\textbackslash}manifoldmixup\{\}, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. {\textbackslash}manifoldmixup\{\} leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with {\textbackslash}manifoldmixup\{\} learn flatter class-representations, that is, with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it empirically on practical situations, and connect it to the previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, {\textbackslash}manifoldmixup\{\} improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.},
	language = {en},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Lopez-Paz, David and Bengio, Yoshua},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6438--6447},
}

@inproceedings{kim_puzzle_2020,
	title = {Puzzle {Mix}: {Exploiting} {Saliency} and {Local} {Statistics} for {Optimal} {Mixup}},
	shorttitle = {Puzzle {Mix}},
	url = {https://proceedings.mlr.press/v119/kim20b.html},
	abstract = {While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets, and the source code is available at https://github.com/snu-mllab/PuzzleMix.},
	language = {en},
	urldate = {2021-12-17},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Jang-Hyun and Choo, Wonho and Song, Hyun Oh},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {5275--5285},
}

@inproceedings{zhang_how_2021,
	title = {How {Does} {Mixup} {Help} {With} {Robustness} and {Generalization}?},
	url = {https://openreview.net/forum?id=8yKEo06dKNo},
	abstract = {Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's...},
	language = {en},
	urldate = {2021-12-16},
	author = {Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Ghorbani, Amirata and Zou, James},
	year = {2021},
}

@inproceedings{robinson_contrastive_2020,
	title = {Contrastive {Learning} with {Hard} {Negative} {Samples}},
	url = {https://openreview.net/forum?id=CR1XOQ0UTh-},
	abstract = {We consider the question: how can you sample good negative examples for contrastive learning? We argue that, as with metric learning, learning contrastive representations benefits from hard...},
	language = {en},
	urldate = {2021-12-16},
	author = {Robinson, Joshua David and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
	month = sep,
	year = {2020},
}

@inproceedings{nguyen_wide_2020,
	title = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}? {Uncovering} {How} {Neural} {Network} {Representations} {Vary} with {Width} and {Depth}},
	shorttitle = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}?},
	url = {https://openreview.net/forum?id=KJNcAkY8tY4},
	abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design...},
	language = {en},
	urldate = {2021-12-16},
	author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
	month = sep,
	year = {2020},
}

@inproceedings{jeong_training_2020,
	title = {Training {GANs} with {Stronger} {Augmentations} via {Contrastive} {Discriminator}},
	url = {https://openreview.net/forum?id=eo6U4CAwVmg},
	abstract = {Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear...},
	language = {en},
	urldate = {2021-12-16},
	author = {Jeong, Jongheon and Shin, Jinwoo},
	month = sep,
	year = {2020},
}

@inproceedings{xiao_noise_2020,
	title = {Noise or {Signal}: {The} {Role} of {Image} {Backgrounds} in {Object} {Recognition}},
	shorttitle = {Noise or {Signal}},
	url = {https://openreview.net/forum?id=gl3D-xY7wLq},
	abstract = {We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet...},
	language = {en},
	urldate = {2021-12-16},
	author = {Xiao, Kai Yuanqing and Engstrom, Logan and Ilyas, Andrew and Madry, Aleksander},
	month = sep,
	year = {2020},
}

@inproceedings{kleinman_usable_2020,
	title = {Usable {Information} and {Evolution} of {Optimal} {Representations} {During} {Training}},
	url = {https://openreview.net/forum?id=p8agn6bmTbr},
	abstract = {We introduce a notion of usable information contained in the representation learned by a deep network, and use it to study how optimal representations for the task emerge during training. We show...},
	language = {en},
	urldate = {2021-12-16},
	author = {Kleinman, Michael and Achille, Alessandro and Idnani, Daksh and Kao, Jonathan},
	month = sep,
	year = {2020},
}

@inproceedings{li_shape-texture_2020,
	title = {Shape-{Texture} {Debiased} {Neural} {Network} {Training}},
	url = {https://openreview.net/forum?id=Db4yerZTYkz},
	abstract = {Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the...},
	language = {en},
	urldate = {2021-12-16},
	author = {Li, Yingwei and Yu, Qihang and Tan, Mingxing and Mei, Jieru and Tang, Peng and Shen, Wei and Yuille, Alan and Xie, Cihang},
	month = sep,
	year = {2020},
}

@inproceedings{xu_robust_2020,
	title = {Robust and {Generalizable} {Visual} {Representation} {Learning} via {Random} {Convolutions}},
	url = {https://openreview.net/forum?id=BVSM0x3EDK6},
	abstract = {While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we...},
	language = {en},
	urldate = {2021-12-16},
	author = {Xu, Zhenlin and Liu, Deyi and Yang, Junlin and Raffel, Colin and Niethammer, Marc},
	month = sep,
	year = {2020},
}

@inproceedings{hendrycks_aligning_2020,
	title = {Aligning {AI} {With} {Shared} {Human} {Values}},
	url = {https://openreview.net/forum?id=dNy_RKzJacY},
	abstract = {We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and...},
	language = {en},
	urldate = {2021-12-16},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
	month = sep,
	year = {2020},
}

@inproceedings{sinha_negative_2020,
	title = {Negative {Data} {Augmentation}},
	url = {https://openreview.net/forum?id=Ovp8dvB8IBH},
	abstract = {Data augmentation is often used to enlarge datasets with synthetic samples generated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore...},
	language = {en},
	urldate = {2021-12-16},
	author = {Sinha, Abhishek and Ayush, Kumar and Song, Jiaming and Uzkent, Burak and Jin, Hongxia and Ermon, Stefano},
	month = sep,
	year = {2020},
}

@inproceedings{qu_coda_2020,
	title = {{CoDA}: {Contrast}-enhanced and {Diversity}-promoting {Data} {Augmentation} for {Natural} {Language} {Understanding}},
	shorttitle = {{CoDA}},
	url = {https://openreview.net/forum?id=Ozk9MrX1hvA},
	abstract = {Data augmentation has been demonstrated as an effective strategy for improving model generalization and data efficiency.  However, due to the discrete nature of natural language, designing...},
	language = {en},
	urldate = {2021-12-16},
	author = {Qu, Yanru and Shen, Dinghan and Shen, Yelong and Sajeev, Sandra and Chen, Weizhu and Han, Jiawei},
	month = sep,
	year = {2020},
}

@inproceedings{raghu_teaching_2020,
	title = {Teaching with {Commentaries}},
	url = {https://openreview.net/forum?id=4RbdgBh9gE},
	abstract = {Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training...},
	language = {en},
	urldate = {2021-12-16},
	author = {Raghu, Aniruddh and Raghu, Maithra and Kornblith, Simon and Duvenaud, David and Hinton, Geoffrey},
	month = sep,
	year = {2020},
}

@inproceedings{sahoo_scaling_2020,
	title = {Scaling {Symbolic} {Methods} using {Gradients} for {Neural} {Model} {Explanation}},
	url = {https://openreview.net/forum?id=V5j-jdoDDP},
	abstract = {Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to...},
	language = {en},
	urldate = {2021-12-16},
	author = {Sahoo, Subham Sekhar and Venugopalan, Subhashini and Li, Li and Singh, Rishabh and Riley, Patrick},
	month = sep,
	year = {2020},
}

@inproceedings{cheung_modals_2020,
	title = {{MODALS}: {Modality}-agnostic {Automated} {Data} {Augmentation} in the {Latent} {Space}},
	shorttitle = {{MODALS}},
	url = {https://openreview.net/forum?id=XjYgR6gbCEc},
	abstract = {Data augmentation is an efficient way to expand a training dataset by creating additional artificial data. While data augmentation is found to be effective in improving the generalization...},
	language = {en},
	urldate = {2021-12-16},
	author = {Cheung, Tsz-Him and Yeung, Dit-Yan},
	month = sep,
	year = {2020},
}

@inproceedings{lee_removing_2020,
	title = {Removing {Undesirable} {Feature} {Contributions} {Using} {Out}-of-{Distribution} {Data}},
	url = {https://openreview.net/forum?id=eIHYL6fpbkA},
	abstract = {Several data augmentation methods deploy unlabeled-in-distribution (UID) data to bridge the gap between the training and inference of neural networks. However, these methods have clear limitations...},
	language = {en},
	urldate = {2021-12-16},
	author = {Lee, Saehyung and Park, Changhwa and Lee, Hyungyu and Yi, Jihun and Lee, Jonghyun and Yoon, Sungroh},
	month = sep,
	year = {2020},
}

@inproceedings{chuang_fair_2020,
	title = {Fair {Mixup}: {Fairness} via {Interpolation}},
	shorttitle = {Fair {Mixup}},
	url = {https://openreview.net/forum?id=DNl5s5BXeBn},
	abstract = {Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during...},
	language = {en},
	urldate = {2021-12-16},
	author = {Chuang, Ching-Yao and Mroueh, Youssef},
	month = sep,
	year = {2020},
}

@inproceedings{heckel_early_2020,
	title = {Early {Stopping} in {Deep} {Networks}: {Double} {Descent} and {How} to {Eliminate} it},
	shorttitle = {Early {Stopping} in {Deep} {Networks}},
	url = {https://openreview.net/forum?id=tlV90jvZbw},
	abstract = {Over-parameterized models, such as large deep networks, often exhibit a double descent phenomenon, whereas a function of model size, error first decreases, increases, and decreases at last. This...},
	language = {en},
	urldate = {2021-12-16},
	author = {Heckel, Reinhard and Yilmaz, Fatih Furkan},
	month = sep,
	year = {2020},
}

@inproceedings{dittadi_transfer_2020,
	title = {On the {Transfer} of {Disentangled} {Representations} in {Realistic} {Settings}},
	url = {https://openreview.net/forum?id=8VXvj1QNRl1},
	abstract = {Learning meaningful representations that disentangle the underlying structure of the data generating process is considered to be of key importance in machine learning. While disentangled...},
	language = {en},
	urldate = {2021-12-16},
	author = {Dittadi, Andrea and Träuble, Frederik and Locatello, Francesco and Wuthrich, Manuel and Agrawal, Vaibhav and Winther, Ole and Bauer, Stefan and Schölkopf, Bernhard},
	month = sep,
	year = {2020},
}

@inproceedings{parascandolo_learning_2020,
	title = {Learning explanations that are hard to vary},
	url = {https://openreview.net/forum?id=hb1sDDSLbV},
	abstract = {In this paper, we investigate the principle that good explanations are hard to vary in the context of deep learning.
We show that averaging gradients across examples -- akin to a logical OR of...},
	language = {en},
	urldate = {2021-12-16},
	author = {Parascandolo, Giambattista and Neitz, Alexander and Orvieto, Antonio and Gresele, Luigi and Schölkopf, Bernhard},
	month = sep,
	year = {2020},
}

@inproceedings{mendez_lifelong_2020,
	title = {Lifelong {Learning} of {Compositional} {Structures}},
	url = {https://openreview.net/forum?id=ADWd4TJO13G},
	abstract = {A hallmark of human intelligence is the ability to construct self-contained chunks of knowledge and adequately reuse them in novel combinations for solving different yet structurally related...},
	language = {en},
	urldate = {2021-12-16},
	author = {Mendez, Jorge A. and Eaton, Eric},
	month = sep,
	year = {2020},
}

@inproceedings{cao_concept_2020,
	title = {Concept {Learners} for {Few}-{Shot} {Learning}},
	url = {https://openreview.net/forum?id=eJIJF3-LoZO},
	abstract = {Developing algorithms that are able to generalize to a novel task given only a few labeled examples represents a fundamental challenge in closing the gap between machine- and human-level...},
	language = {en},
	urldate = {2021-12-16},
	author = {Cao, Kaidi and Brbic, Maria and Leskovec, Jure},
	month = sep,
	year = {2020},
}

@inproceedings{pang_bag_2021,
	title = {Bag of {Tricks} for {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=Xb8xvrtB8Ce},
	abstract = {Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective...},
	language = {en},
	urldate = {2021-02-23},
	author = {Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Su, Hang and Zhu, Jun},
	year = {2021},
}

@inproceedings{wong_learning_2020,
	title = {Learning perturbation sets for robust machine learning},
	url = {https://openreview.net/forum?id=MIDckA56aD},
	abstract = {Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in...},
	language = {en},
	urldate = {2021-12-16},
	author = {Wong, Eric and Kolter, J. Zico},
	month = sep,
	year = {2020},
}

@inproceedings{lurz_generalization_2020,
	title = {Generalization in data-driven models of primary visual cortex},
	url = {https://openreview.net/forum?id=Tp7kI90Htd},
	abstract = {Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons...},
	language = {en},
	urldate = {2021-12-16},
	author = {Lurz, Konstantin-Klemens and Bashiri, Mohammad and Willeke, Konstantin and Jagadish, Akshay and Wang, Eric and Walker, Edgar Y. and Cadena, Santiago A. and Muhammad, Taliah and Cobos, Erick and Tolias, Andreas S. and Ecker, Alexander S. and Sinz, Fabian H.},
	month = sep,
	year = {2020},
}

@inproceedings{pope_intrinsic_2020,
	title = {The {Intrinsic} {Dimension} of {Images} and {Its} {Impact} on {Learning}},
	url = {https://openreview.net/forum?id=XJk19XzGq2J},
	abstract = {It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations.  This idea underlies a common intuition for...},
	language = {en},
	urldate = {2021-12-16},
	author = {Pope, Phil and Zhu, Chen and Abdelkader, Ahmed and Goldblum, Micah and Goldstein, Tom},
	month = sep,
	year = {2020},
}

@inproceedings{chen_noise_2021,
	title = {Noise against noise: stochastic label noise helps combat inherent label noise},
	shorttitle = {Noise against noise},
	url = {https://openreview.net/forum?id=80FMcTSZ6J0},
	abstract = {The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect, previously studied in optimization by analyzing the dynamics of parameter updates. In this paper...},
	language = {en},
	urldate = {2021-12-16},
	author = {Chen, Pengfei and Chen, Guangyong and Ye, Junjie and Zhao, Jingwei and Heng, Pheng-Ann},
	year = {2021},
}

@inproceedings{xu_understanding_2020,
	title = {Understanding the role of importance weighting for deep learning},
	url = {https://openreview.net/forum?id=_WnwtieRHxM},
	abstract = {The recent paper by Byrd \& Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe...},
	language = {en},
	urldate = {2021-12-16},
	author = {Xu, Da and Ye, Yuting and Ruan, Chuanwei},
	month = sep,
	year = {2020},
}

@inproceedings{zhang_learning_2021,
	title = {Learning with {Feature}-{Dependent} {Label} {Noise}: {A} {Progressive} {Approach}},
	shorttitle = {Learning with {Feature}-{Dependent} {Label} {Noise}},
	url = {https://openreview.net/forum?id=ZPa2SyGcbwh},
	abstract = {Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to...},
	language = {en},
	urldate = {2021-12-16},
	author = {Zhang, Yikai and Zheng, Songzhu and Wu, Pengxiang and Goswami, Mayank and Chen, Chao},
	year = {2021},
}

@inproceedings{kim_learning_2010,
	title = {Learning motion dynamics to catch a moving object},
	doi = {10.1109/ICHR.2010.5686332},
	abstract = {In this paper, we consider a novel approach to control the timing of motions when these are encoded with autonomous dynamical systems (DS). Accurate timing of motion is crucial if a robot must synchronize its movement with that of a fast moving object. In previous work of ours [1], we developed an approach to encode robot motion into DS. Such a time-independent encoding is advantageous in that it offers robustness against violent perturbation by adapting on the fly the trajectory while ensuring high accuracy at the target. We propose here an extension of the system that allows to control the timing of the motion while still benefitting from all the robustness properties deriving from the time-independent encoding of the DS. We validate the approach in experiments where the iCub robot learns from human demonstrations to catch a ball on the fly.},
	booktitle = {2010 10th {IEEE}-{RAS} {International} {Conference} on {Humanoid} {Robots}},
	author = {Kim, Seungsu and Gribovskaya, Elena and Billard, Aude},
	month = dec,
	year = {2010},
	note = {ISSN: 2164-0580},
	keywords = {Dynamics, Joints, Robot kinematics, Robot sensing systems, Timing, Trajectory},
	pages = {106--111},
}

@article{carneiro_robot_2021,
	title = {Robot {Anticipation} {Learning} {System} for {Ball} {Catching}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2218-6581/10/4/113},
	doi = {10.3390/robotics10040113},
	abstract = {Catching flying objects is a challenging task in human–robot interaction. Traditional techniques predict the intersection position and time using the information obtained during the free-flying ball motion. A common pain point in these systems is the short ball flight time and uncertainties in the ball’s trajectory estimation. In this paper, we present the Robot Anticipation Learning System (RALS) that accounts for the information obtained from observation of the thrower’s hand motion before the ball is released. RALS takes extra time for the robot to start moving in the direction of the target before the opponent finishes throwing. To the best of our knowledge, this is the first robot control system for ball-catching with anticipation skills. Our results show that the information fused from both throwing and flying motions improves the ball-catching rate by up to 20\% compared to the baseline approach, with the predictions relying only on the information acquired during the flight phase.},
	language = {en},
	number = {4},
	urldate = {2021-12-13},
	journal = {Robotics},
	author = {Carneiro, Diogo and Silva, Filipe and Georgieva, Petia},
	month = dec,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {anticipation learning, ball catching, human–robot interaction, neural network, trajectory prediction},
	pages = {113},
}

@article{abdelkhalek_trajectory-based_2021,
	title = {Trajectory-based fast ball detection and tracking for an autonomous industrial robot system},
	volume = {20},
	issn = {1740-8865},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJISTA.2021.119029},
	doi = {10.1504/IJISTA.2021.119029},
	abstract = {Autonomising industrial robots is the main goal in this paper; imagine humanoid robots that have several degrees of freedom (DOF) mechanisms as their arms. What if the humanoid's arms could be programmed to be responsive to their surrounding environment, without any hard-coding assigned? This paper presents the idea of an autonomous system, where the system observes the surrounding environment and takes action on its observation. The application here is that of rebuffing an object that is thrown towards a robotic arm's workspace. This application mimics the idea of high dynamic responsiveness of a robot's arm. This paper will present a trajectory generation framework for rebuffing incoming flying objects. The framework bases its assumptions on inputs acquired through image processing and object detection. After extensive testing, it can be said that the proposed framework managed to fulfil the real-time system requirements for this application, with an 80\% successful rebuffing rate.},
	number = {2},
	urldate = {2021-12-13},
	journal = {International Journal of Intelligent Systems Technologies and Applications},
	author = {AbdElKhalek, Youssef M. and Awad, Mohammed Ibrahim and Munim, Hossam E. Abd El and Maged, Shady A.},
	month = jan,
	year = {2021},
	note = {Publisher: Inderscience Publishers},
	keywords = {depth image processing, infrared image processing, object detection, object tracking, ping-pong ball, real-time, serial robot, stereo vision, table tennis, trajectory prediction},
	pages = {126--145},
}

@inproceedings{namiki_robotic_2003,
	title = {Robotic catching using a direct mapping from visual information to motor command},
	volume = {2},
	doi = {10.1109/ROBOT.2003.1241952},
	abstract = {In this paper a robotic catching algorithm based on a nonlinear mapping of visual information to the desired trajectory is proposed. The nonlinear mapping is optimized by learning based on constraints of dynamics and kinematics. As a result a reactive and flexible motion is obtained due to the real-time high-speed visual information. Experimental results on catching a moving object using a high-speed vision system and a manipulation are presented.},
	booktitle = {2003 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({Cat}. {No}.{03CH37422})},
	author = {Namiki, A. and Ishikawa, M.},
	month = sep,
	year = {2003},
	note = {ISSN: 1050-4729},
	keywords = {Constraint optimization, Humans, Kinematics, Machine vision, Manipulator dynamics, Motor drives, Nonlinear dynamical systems, Physics computing, Robots, Trajectory},
	pages = {2400--2405 vol.2},
}

@article{kim_catching_2014,
	title = {Catching {Objects} in {Flight}},
	volume = {30},
	issn = {1941-0468},
	doi = {10.1109/TRO.2014.2316022},
	abstract = {We address the difficult problem of catching in-flight objects with uneven shapes. This requires the solution of three complex problems: accurate prediction of the trajectory of fastmoving objects, predicting the feasible catching configuration, and planning the arm motion, and all within milliseconds. We follow a programming-by-demonstration approach in order to learn, from throwing examples, models of the object dynamics and arm movement. We propose a new methodology to find a feasible catching configuration in a probabilistic manner. We use the dynamical systems approach to encode motion from several demonstrations. This enables a rapid and reactive adaptation of the arm motion in the presence of sensor uncertainty. We validate the approach in simulation with the iCub humanoid robot and in real-world experiments with the KUKA LWR 4+ (7-degree-of-freedom arm robot) to catch a hammer, a tennis racket, an empty bottle, a partially filled bottle, and a cardboard box.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {Kim, Seungsu and Shukla, Ashwini and Billard, Aude},
	month = oct,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Aerospace electronics, Catching, Dynamics, Gaussian mixture model, Grasping, Robot kinematics, Robot sensing systems, Trajectory, machine learning, robot control, support vector machines},
	pages = {1049--1065},
}

@inproceedings{yang_human_2020,
	title = {Human {Grasp} {Classification} for {Reactive} {Human}-to-{Robot} {Handovers}},
	doi = {10.1109/IROS45743.2020.9341004},
	abstract = {Transfer of objects between humans and robots is a critical capability for collaborative robots. Although there has been a recent surge of interest in human-robot handovers, most prior research focus on robot-to-human handovers. Further, work on the equally critical human-to-robot handovers often assumes humans can place the object in the robot's gripper. In this paper, we propose an approach for human-to-robot handovers in which the robot meets the human halfway, by classifying the human's grasp of the object and quickly planning a trajectory accordingly to take the object from the human's hand according to their intent. To do this, we collect a human grasp dataset which covers typical ways of holding objects with various hand shapes and poses, and learn a deep model on this dataset to classify the hand grasps into one of these categories. We present a planning and execution approach that takes the object from the human hand according to the detected grasp and hand position, and replans as necessary when the handover is interrupted. Through a systematic evaluation, we demonstrate that our system results in more fluent handovers versus two baselines. We also present findings from a user study (N = 9) demonstrating the effectiveness and usability of our approach with naive users in different scenarios. More information can be found at http://wyang.me/handovers.},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Yang, Wei and Paxton, Chris and Cakmak, Maya and Fox, Dieter},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Collaboration, Handover, Planning, Surges, Systematics, Trajectory, Usability},
	pages = {11123--11130},
}

@article{feix_grasp_2016,
	title = {The {GRASP} {Taxonomy} of {Human} {Grasp} {Types}},
	volume = {46},
	issn = {2168-2305},
	doi = {10.1109/THMS.2015.2470657},
	abstract = {In this paper, we analyze and compare existing human grasp taxonomies and synthesize them into a single new taxonomy (dubbed “The GRASP Taxonomy” after the GRASP project funded by the European Commission). We consider only static and stable grasps performed by one hand. The goal is to extract the largest set of different grasps that were referenced in the literature and arrange them in a systematic way. The taxonomy provides a common terminology to define human hand configurations and is important in many domains such as human-computer interaction and tangible user interfaces where an understanding of the human is basis for a proper interface. Overall, 33 different grasp types are found and arranged into the GRASP taxonomy. Within the taxonomy, grasps are arranged according to 1) opposition type, 2) the virtual finger assignments, 3) type in terms of power, precision, or intermediate grasp, and 4) the position of the thumb. The resulting taxonomy incorporates all grasps found in the reviewed taxonomies that complied with the grasp definition. We also show that due to the nature of the classification, the 33 grasp types might be reduced to a set of 17 more general grasps if only the hand configuration is considered without the object shape/size.},
	number = {1},
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Feix, Thomas and Romero, Javier and Schmiedmayer, Heinz-Bodo and Dollar, Aaron M. and Kragic, Danica},
	month = feb,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Human-Machine Systems},
	keywords = {Force, Grasping, Hand/wrist posture, Man machine systems, Robots, Shape, Taxonomy, Thumb, human factors, human–robot interaction, robotics, taxonomies},
	pages = {66--77},
}

@incollection{cutkosky_human_1990,
	address = {New York, NY},
	title = {Human {Grasp} {Choice} and {Robotic} {Grasp} {Analysis}},
	isbn = {978-1-4613-8974-3},
	url = {https://doi.org/10.1007/978-1-4613-8974-3_1},
	abstract = {In studying grasping and manipulation we find two very different approaches to the subject: knowledge-based approaches based primarily on empirical studies of human grasping and manipulation, and analytical approaches based primarily on physical models of the manipulation process. This chapter begins with a review of studies of human grasping, in particular our development of a grasp taxonomy and an expert system for predicting human grasp choice. These studies show how object geometry and task requirements (as well as hand capabilities and tactile sensing) combine to dictate grasp choice. We then consider analytic models of grasping and manipulation with robotic hands. To keep the mathematics tractable, these models require numerous simplifications which restrict their generality. Despite their differences, the two approaches can be correlated. This provides insight into why people grasp and manipulate objects as they do, and suggests different approaches for robotic grasp and manipulation planning. The results also bear upon such issues such as object representation and hand design.},
	language = {en},
	urldate = {2021-12-13},
	booktitle = {Dextrous {Robot} {Hands}},
	publisher = {Springer},
	author = {Cutkosky, Mark R. and Howe, Robert D.},
	editor = {Venkataraman, Subramanian T. and Iberall, Thea},
	year = {1990},
	doi = {10.1007/978-1-4613-8974-3_1},
	keywords = {Force Closure, Power Grasp, Robot Hand, Thrust Force, Virtual Finger},
	pages = {5--31},
}

@article{ortenzi_object_2021,
	title = {Object {Handovers}: {A} {Review} for {Robotics}},
	volume = {37},
	issn = {1941-0468},
	shorttitle = {Object {Handovers}},
	doi = {10.1109/TRO.2021.3075365},
	abstract = {This article surveys the literature on human–robot object handovers. A handover is a collaborative joint action, where an agent, the giver, gives an object to another agent, the receiver. The physical exchange starts when the receiver first contacts the object held by the giver and ends when the giver fully releases the object to the receiver. However, important cognitive and physical processes begin before the physical exchange, including initiating implicit agreement with respect to the location and timing of the exchange. From this perspective, we structure our review into the two main phases delimited by the aforementioned events: a prehandover phase and the physical exchange. We focus our analysis on the two actors (giver and receiver) and report the state of the art of robotic givers (robot-to-human handovers) and the robotic receivers (human-to-robot handovers). We report a comprehensive list of qualitative and quantitative metrics commonly used to assess the interaction. While focusing our review on the cognitive level (e.g., prediction, perception, motion planning, and learning) and the physical level (e.g., motion, grasping, and grip release) of the handover, we also discuss safety. We compare the behaviors displayed during human-to-human handovers to the state of the art of robotic assistants and identify the major areas of improvement for robotic assistants to reach performance comparable to human interactions. Finally, we propose a minimal set of metrics that should be used in order to enable a fair comparison among the approaches.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Ortenzi, Valerio and Cosgun, Akansel and Pardi, Tommaso and P. Chan, Wesley and Croft, Elizabeth and Kulić, Dana},
	month = dec,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Handover, Human–robot interaction (HRI), Planning, Receivers, Robot kinematics, Robots, Task analysis, Tools, object handover},
	pages = {1855--1873},
}

@article{babin_mechanisms_2021,
	title = {Mechanisms for {Robotic} {Grasping} and {Manipulation}},
	volume = {4},
	url = {https://doi.org/10.1146/annurev-control-061520-010405},
	doi = {10.1146/annurev-control-061520-010405},
	abstract = {This article reviews the literature on the design of robotic mechanical grippers, with a focus on the mechanical aspects, which are believed to be the main bottleneck for effective designs. Our discussion includes gripper architectures and means of actuation, anthropomorphism and grasp planning, and robotic manipulation, emphasizing the complementary concepts of intrinsic and extrinsic dexterity. We also consider interactions of robotic grippers with the environment and with the objects to be grasped and argue that the proper handling of such interactions is key to the development of grasping and manipulation tools and scenarios. Finally, we briefly present examples of recent designs to support the discussion.},
	number = {1},
	urldate = {2021-12-13},
	journal = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Babin, Vincent and Gosselin, Clément},
	year = {2021},
	note = {\_eprint: https://doi.org/10.1146/annurev-control-061520-010405},
	keywords = {anthropomorphic grippers, compliant grippers, extrinsic dexterity, grasp quality, intrinsic dexterity, manipulation, physical interactions, robotic hands, soft grippers, underactuation},
	pages = {573--593},
}

@article{liu_systematic_2021,
	title = {A {Systematic} {Analysis} of {Hand} {Movement} {Functionality}: {Qualitative} {Classification} and {Quantitative} {Investigation} of {Hand} {Grasp} {Behavior}},
	volume = {15},
	issn = {1662-5218},
	shorttitle = {A {Systematic} {Analysis} of {Hand} {Movement} {Functionality}},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2021.658075},
	doi = {10.3389/fnbot.2021.658075},
	abstract = {Understanding human hand movement functionality is fundamental in neuroscience, robotics, prosthetics, and rehabilitation. People are used to investigate movement functionality separately from qualitative or quantitative perspectives. However, it is still limited to providing an integral framework from both perspectives in a logical manner. In this paper, we provide a systematic framework to qualitatively classify hand movement functionality, build prehensile taxonomy to explore the general influence factors of human prehension, and accordingly design a behavioral experiment to quantitatively understand the hand grasp. In qualitative analysis, two facts are explicitly proposed: (1) the arm and wrist make a vital contribution to hand movement functionality; (2) the relative position (relative position in this paper is defined as the distance between the center of the human wrist and the object center of gravity) is a general influence factor significantly impacting human prehension. In quantitative analysis, the significant influence of three factors, object shape, size, and relative position, is quantitatively demonstrated. Simultaneously considering the impact of relative position, object shape, and size, the prehensile taxonomy and behavioral experiment results presented here should be more representative and complete to understand human grasp functionality. The systematic framework presented here is general and applicable to other body parts, such as wrist, arm, etc. Finally, many potential applications and the limitations are clarified.},
	urldate = {2021-12-13},
	journal = {Frontiers in Neurorobotics},
	author = {Liu, Yuan and Jiang, Li and Liu, Hong and Ming, Dong},
	year = {2021},
	pages = {46},
}

@inproceedings{li_learning_2021,
	title = {Learning {Task}-{Oriented} {Dexterous} {Grasping} from {Human} {Knowledge}},
	doi = {10.1109/ICRA48506.2021.9562073},
	abstract = {Industrial automation requires robot dexterity to automate many processes such as product assembling, packaging, and material handling. The existing robotic systems lack the capability to determining proper grasp strategies in the context of object affordances and task designations. In this paper, a framework of task-oriented dexterous grasping is proposed to learn grasp knowledge from human experience and to deploy the grasp strategies while adapting to grasp context. Grasp topology is defined and grasp strategies are learned from an established dataset for task-oriented dexterous manipulation. To adapt to various grasp context, a reinforcement-learning based grasping policy was implemented to deploy different task-oriented strategies. The performance of the system was evaluated in a simulated grasping environment by using an AR10 anthropomorphic hand installed in a Sawyer robotic arm. The proposed framework achieved a hit rate of 100\% for grasp strategies and an overall top-3 match rate of 95.6\%. The success rate of grasping was 85.6\% during 2700 grasping experiments for manipulation tasks given in natural-language instructions.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Li, Hui and Zhang, Yinlong and Li, Yanan and He, Hongsheng},
	month = may,
	year = {2021},
	note = {ISSN: 2577-087X},
	keywords = {Affordances, Automation, Dexterous Grasping, Grasp Topology, Grasping, Network topology, Prediction algorithms, Reinforcement Learning, Reinforcement learning, Service robots, Task-Oriented Grasping},
	pages = {6192--6198},
}

@article{roa_grasp_2015,
	title = {Grasp quality measures: review and performance},
	volume = {38},
	issn = {1573-7527},
	shorttitle = {Grasp quality measures},
	url = {https://doi.org/10.1007/s10514-014-9402-3},
	doi = {10.1007/s10514-014-9402-3},
	abstract = {The correct grasp of objects is a key aspect for the right fulfillment of a given task. Obtaining a good grasp requires algorithms to automatically determine proper contact points on the object as well as proper hand configurations, especially when dexterous manipulation is desired, and the quantification of a good grasp requires the definition of suitable grasp quality measures. This article reviews the quality measures proposed in the literature to evaluate grasp quality. The quality measures are classified into two groups according to the main aspect they evaluate: location of contact points on the object and hand configuration. The approaches that combine different measures from the two previous groups to obtain a global quality measure are also reviewed, as well as some measures related to human hand studies and grasp performance. Several examples are presented to illustrate and compare the performance of the reviewed measures.},
	language = {en},
	number = {1},
	urldate = {2021-12-13},
	journal = {Autonomous Robots},
	author = {Roa, Máximo A. and Suárez, Raúl},
	month = jan,
	year = {2015},
	pages = {65--88},
}

@article{bekey_knowledge-based_1993,
	title = {Knowledge-based control of grasping in robot hands using heuristics from human motor skills},
	volume = {9},
	issn = {2374-958X},
	doi = {10.1109/70.265915},
	abstract = {The development of a grasp planner for multifingered robot hands is described. The planner is knowledge-based, selecting grasp postures by reasoning from symbolic information on target object geometry and the nature of the task. The ability of the planner to utilize task information is based on an attempt to mimic human grasping behavior. Several task attributes and a set of heuristics derived from observation of human motor skills are included in the system. The paper gives several examples of the reasoning of the system in selecting the appropriate grasp mode for spherical and cylindrical objects for different tasks.{\textless}{\textgreater}},
	number = {6},
	journal = {IEEE Transactions on Robotics and Automation},
	author = {Bekey, G.A. and Liu, Huan and Tomovic, R. and Karplus, W.J.},
	month = dec,
	year = {1993},
	note = {Conference Name: IEEE Transactions on Robotics and Automation},
	keywords = {Computational geometry, Computer science, Control systems, Equations, Grasping, Humans, Information geometry, Knowledge based systems, Robots, Shape},
	pages = {709--722},
}

@article{feix_analysis_2014,
	title = {Analysis of {Human} {Grasping} {Behavior}: {Object} {Characteristics} and {Grasp} {Type}},
	volume = {7},
	issn = {2329-4051},
	shorttitle = {Analysis of {Human} {Grasping} {Behavior}},
	doi = {10.1109/TOH.2014.2326871},
	abstract = {This paper is the first of a two-part series analyzing human grasping behavior during a wide range of unstructured tasks. The results help clarify overall characteristics of human hand to inform many domains, such as the design of robotic manipulators, targeting rehabilitation toward important hand functionality, and designing haptic devices for use by the hand. It investigates the properties of objects grasped by two housekeepers and two machinists during the course of almost 10,000 grasp instances and correlates the grasp types used to the properties of the object. We establish an object classification that assigns each object properties from a set of seven classes, including mass, shape and size of the grasp location, grasped dimension, rigidity, and roundness. The results showed that 55 percent of grasped objects had at least one dimension larger than 15 cm, suggesting that more than half of objects cannot physically be grasped using their largest axis. Ninety-two percent of objects had a mass of 500 g or less, implying that a high payload capacity may be unnecessary to accomplish a large subset of human grasping behavior. In terms of grasps, 96 percent of grasp locations were 7 cm or less in width, which can help to define requirements for hand rehabilitation and defines a reasonable grasp aperture size for a robotic hand. Subjects grasped the smallest overall major dimension of the object in 94 percent of the instances. This suggests that grasping the smallest axis of an object could be a reliable default behavior to implement in grasp planners.},
	number = {3},
	journal = {IEEE Transactions on Haptics},
	author = {Feix, Thomas and Bullock, Ian M. and Dollar, Aaron M.},
	month = jul,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Haptics},
	keywords = {Force, Grasping, Human grasping, Joints, Robots, Shape, Thumb, activities of daily living, manipulation, prosthetics, robotic hands},
	pages = {311--323},
}

@article{feix_analysis_2014-1,
	title = {Analysis of {Human} {Grasping} {Behavior}: {Correlating} {Tasks}, {Objects} and {Grasps}},
	volume = {7},
	issn = {2329-4051},
	shorttitle = {Analysis of {Human} {Grasping} {Behavior}},
	doi = {10.1109/TOH.2014.2326867},
	abstract = {This paper is the second in a two-part series analyzing human grasping behavior during a wide range of unstructured tasks. It investigates the tasks performed during the daily work of two housekeepers and two machinists and correlates grasp type and object properties with the attributes of the tasks being performed. The task or activity is classified according to the force required, the degrees of freedom, and the functional task type. We found that 46 percent of tasks are constrained, where the manipulated object is not allowed to move in a full six degrees of freedom. Analyzing the interrelationships between the grasp, object, and task data show that the best predictors of the grasp type are object size, task constraints, and object mass. Using these attributes, the grasp type can be predicted with 47 percent accuracy. Those parameters likely make useful heuristics for grasp planning systems. The results further suggest the common sub-categorization of grasps into power, intermediate, and precision categories may not be appropriate, indicating that grasps are generally more multi-functional than previously thought. We find large and heavy objects are grasped with a power grasp, but small and lightweight objects are not necessarily grasped with precision grasps-even with grasped object size less than 2 cm and mass less than 20 g, precision grasps are only used 61 percent of the time. These results have important implications for robotic hand design and grasp planners, since it appears while power grasps are frequently used for heavy objects, they can still be quite practical for small, lightweight objects.},
	number = {4},
	journal = {IEEE Transactions on Haptics},
	author = {Feix, Thomas and Bullock, Ian M. and Dollar, Aaron M.},
	month = oct,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Haptics},
	keywords = {Grasping, Human grasping, Robots, Shape analysis, Thumb, activities of daily living, manipulation, prosthetics, robotic hands},
	pages = {430--441},
}

@inproceedings{mason_dynamic_1993,
	title = {Dynamic manipulation},
	volume = {1},
	doi = {10.1109/IROS.1993.583093},
	abstract = {Dynamic manipulation is defined, and a brief survey of dynamic operations is given. The design, control, and planning of dynamic manipulation is addressed. An example of dynamic manipulation, club-throwing using dynamic closure, is described.},
	booktitle = {Proceedings of 1993 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS} '93)},
	author = {Mason, M.T. and Lynch, K.M.},
	month = jul,
	year = {1993},
	keywords = {Acceleration, Computer science, Grippers, Kinematics, Manipulator dynamics, Motion planning, Programming profession, Robots, Stability, Taxonomy},
	pages = {152--159 vol.1},
}

@article{su_deep_2021,
	title = {Deep {Neural} {Network} {Approach} in {EMG}-{Based} {Force} {Estimation} for {Human}–{Robot} {Interaction}},
	volume = {2},
	issn = {2691-4581},
	url = {https://ieeexplore.ieee.org/document/9380441/},
	doi = {10.1109/TAI.2021.3066565},
	abstract = {In the human–robot interaction, especially when hand contact appears directly on the robot arm, the dynamics of the human arm presents an essential component in human–robot interaction and object manipulation. Modeling and estimation of the human arm dynamics show great potential for achieving more natural and safer interaction. To enrich the dexterity and guarantee the accuracy of the manipulation, mapping the motor functionality of muscle using biosignals becomes a popular topic. In this article, a novel algorithm was constructed using deep learning to explore the potential model between surface electromyography (sEMG) signals of the human arm and interaction force for human–robot interaction. Its features were extracted by adopting the convolutional neural network from the sEMG signals automatically without using prior knowledge of the biomechanical model. The experiments prove the lower error ({\textless} 0.4 N ) of the designed regression by comparing it with other approaches, such as artiﬁcial neural network and long short-term memory. It should be also mentioned that the antinoise ability is an important index to apply this technique in practical applications. Hence, we also add different Gaussian noises into the dataset to demonstrate the robustness against measurement noises by using the proposed model. Finally, it demonstrates the performance of the proposed algorithm using the Myo controller and KUKA LWR4+ robot.},
	language = {en},
	number = {5},
	urldate = {2021-12-13},
	journal = {IEEE Transactions on Artificial Intelligence},
	author = {Su, Hang and Qi, Wen and Li, Zhijun and Chen, Ziyang and Ferrigno, Giancarlo and De Momi, Elena},
	month = oct,
	year = {2021},
	pages = {404--412},
}

@article{bullock_yale_2015,
	title = {The {Yale} human grasping dataset: {Grasp}, object, and task data in household and machine shop environments},
	volume = {34},
	issn = {0278-3649, 1741-3176},
	shorttitle = {The {Yale} human grasping dataset},
	url = {http://journals.sagepub.com/doi/10.1177/0278364914555720},
	doi = {10.1177/0278364914555720},
	abstract = {This paper presents a dataset of human grasping behavior in unstructured environments. Wide-angle head-mounted camera video was recorded from two housekeepers and two machinists during their regular work activities, and the grasp types, objects, and tasks were analyzed and coded by study staff. The full dataset contains 27.7 hours of tagged video and represents a wide range of manipulative behaviors spanning much of the typical human hand usage. We provide the original videos, a spreadsheet including the tagged grasp type, object, and task parameters, time information for each successive grasp, and video screenshots for each instance. Example code is provided for MATLAB and R, demonstrating how to load in the dataset and produce simple plots.},
	language = {en},
	number = {3},
	urldate = {2021-12-13},
	journal = {The International Journal of Robotics Research},
	author = {Bullock, Ian M. and Feix, Thomas and Dollar, Aaron M.},
	month = mar,
	year = {2015},
	pages = {251--255},
}

@article{calli_benchmarking_2015,
	title = {Benchmarking in {Manipulation} {Research}: {Using} the {Yale}-{CMU}-{Berkeley} {Object} and {Model} {Set}},
	volume = {22},
	issn = {1558-223X},
	shorttitle = {Benchmarking in {Manipulation} {Research}},
	doi = {10.1109/MRA.2015.2448951},
	abstract = {In this article, we present the Yale-Carnegie Mellon University (CMU)-Berkeley (YCB) object and model set, intended to be used to facilitate benchmarking in robotic manipulation research. The objects in the set are designed to cover a wide range of aspects of the manipulation problem. The set includes objects of daily life with different shapes, sizes, textures, weights, and rigidities as well as some widely used manipulation tests. The associated database provides high-resolution red, green, blue, plus depth (RGB-D) scans, physical properties, and geometric models of the objects for easy incorporation into manipulation and planning software platforms. In addition to describing the objects and models in the set along with how they were chosen and derived, we provide a framework and a number of example task protocols, laying out how the set can be used to quantitatively evaluate a range of manipulation approaches, including planning, learning, mechanical design, control, and many others. A comprehensive literature survey on the existing benchmarks and object data sets is also presented, and their scope and limitations are discussed. The YCB set will be freely distributed to research groups worldwide at a series of tutorials at robotics conferences. Subsequent sets will be, otherwise, available to purchase at a reasonable cost. It is our hope that the ready availability of this set along with the ground laid in terms of protocol templates will enable the community of manipulation researchers to more easily compare approaches as well as continually evolve standardized benchmarking tests and metrics as the field matures.},
	number = {3},
	journal = {IEEE Robotics Automation Magazine},
	author = {Calli, Berk and Walsman, Aaron and Singh, Arjun and Srinivasa, Siddhartha and Abbeel, Pieter and Dollar, Aaron M.},
	month = sep,
	year = {2015},
	note = {Conference Name: IEEE Robotics Automation Magazine},
	keywords = {Benchmark testing, Data models, Databases, Object detection, Prosthetics, Robots, Solid modeling},
	pages = {36--52},
}

@inproceedings{kim_co-mixup_2021,
	title = {Co-{Mixup}: {Saliency} {Guided} {Joint} {Mixup} with {Supermodular} {Diversity}},
	shorttitle = {Co-{Mixup}},
	url = {https://openreview.net/forum?id=gvxJzw8kW4b},
	abstract = {While deep neural networks show great performance on fitting to the training distribution, improving the networks' generalization performance to the test distribution and robustness to the...},
	language = {en},
	urldate = {2021-12-07},
	author = {Kim, JangHyun and Choo, Wonho and Jeong, Hosan and Song, Hyun Oh},
	year = {2021},
}

@inproceedings{vedaldi_defense_2020,
	address = {Cham},
	title = {Defense {Against} {Adversarial} {Attacks} via {Controlling} {Gradient} {Leaking} on {Embedded} {Manifolds}},
	volume = {12373},
	isbn = {978-3-030-58603-4 978-3-030-58604-1},
	url = {http://link.springer.com/10.1007/978-3-030-58604-1_45},
	doi = {10.1007/978-3-030-58604-1_45},
	abstract = {Deep neural networks are vulnerable to adversarial attacks. Though various attempts have been made, it is still largely open to fully understand the existence of adversarial samples and thereby develop effective defense strategies. In this paper, we present a new perspective, namely gradient leaking hypothesis, to understand the existence of adversarial examples and to further motivate eﬀective defense strategies. Speciﬁcally, we consider the low dimensional manifold structure of natural images, and empirically verify that the leakage of the gradient (w.r.t input) along the (approximately) perpendicular direction to the tangent space of data manifold is a reason for the vulnerability over adversarial attacks. Based on our investigation, we further present a new robust learning algorithm which encourages a larger gradient component in the tangent space of data manifold, suppressing the gradient leaking phenomenon consequently. Experiments on various tasks demonstrate the eﬀectiveness of our algorithm despite its simplicity.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Li, Yueru and Cheng, Shuyu and Su, Hang and Zhu, Jun},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {753--769},
}

@inproceedings{shafahi_universal_2020,
	title = {Universal {Adversarial} {Training}},
	volume = {34},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/6017},
	doi = {10.1609/aaai.v34i04.6017},
	abstract = {Standard adversarial attacks change the predicted class label of a selected image by adding specially tailored small perturbations to its pixels. In contrast, a universal perturbation is an update that can be added to any image in a broad class of images, while still changing the predicted class label. We study the efﬁcient generation of universal adversarial perturbations, and also efﬁcient methods for hardening networks to these attacks. We propose a simple optimization-based universal attack that reduces the top-1 accuracy of various network architectures on ImageNet to less than 20\%, while learning the universal perturbation 13× faster than the standard method.},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Shafahi, Ali and Najibi, Mahyar and Xu, Zheng and Dickerson, John and Davis, Larry S. and Goldstein, Tom},
	month = apr,
	year = {2020},
	pages = {5636--5643},
}

@inproceedings{vedaldi_simple_2020,
	address = {Cham},
	title = {A {Simple} {Way} to {Make} {Neural} {Networks} {Robust} {Against} {Diverse} {Image} {Corruptions}},
	volume = {12348},
	isbn = {978-3-030-58579-2 978-3-030-58580-8},
	url = {https://link.springer.com/10.1007/978-3-030-58580-8_4},
	doi = {10.1007/978-3-030-58580-8_4},
	abstract = {The human visual system is remarkably robust against a wide range of naturally occurring variations and corruptions like rain or snow. In contrast, the performance of modern image recognition models strongly degrades when evaluated on previously unseen corruptions. Here, we demonstrate that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the state of the art on the corruption benchmark ImageNet-C (with ResNet50) and on MNIST-C. We build on top of these strong baseline results and show that an adversarial training of the recognition model against locally correlated worst-case noise distributions leads to an additional increase in performance. This regularization can be combined with previously proposed defense methods for further improvement.},
	language = {en},
	urldate = {2021-12-06},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Rusak, Evgenia and Schott, Lukas and Zimmermann, Roland S. and Bitterwolf, Julian and Bringmann, Oliver and Bethge, Matthias and Brendel, Wieland},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {53--69},
}

@inproceedings{laidlaw_perceptual_2020,
	title = {Perceptual {Adversarial} {Robustness}: {Defense} {Against} {Unseen} {Threat} {Models}},
	shorttitle = {Perceptual {Adversarial} {Robustness}},
	url = {https://openreview.net/forum?id=dFwBosAcJkN},
	abstract = {A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the definition of adversarial attacks that are imperceptible to human...},
	language = {en},
	urldate = {2021-12-04},
	author = {Laidlaw, Cassidy and Singla, Sahil and Feizi, Soheil},
	month = sep,
	year = {2020},
}

@article{gowal_uncovering_2021,
	title = {Uncovering the {Limits} of {Adversarial} {Training} against {Norm}-{Bounded} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/2010.03593},
	abstract = {Adversarial training and its variants have become de facto standards for learning robust deep neural networks. In this paper, we explore the landscape around adversarial training in a bid to uncover its limits. We systematically study the effect of different training losses, model sizes, activation functions, the addition of unlabeled data (through pseudo-labeling) and other factors on adversarial robustness. We discover that it is possible to train robust models that go well beyond state-of-the-art results by combining larger models, Swish/SiLU activations and model weight averaging. We demonstrate large improvements on CIFAR-10 and CIFAR-100 against \${\textbackslash}ell\_{\textbackslash}infty\$ and \${\textbackslash}ell\_2\$ norm-bounded perturbations of size \$8/255\$ and \$128/255\$, respectively. In the setting with additional unlabeled data, we obtain an accuracy under attack of 65.88\% against \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations of size \$8/255\$ on CIFAR-10 (+6.35\% with respect to prior art). Without additional data, we obtain an accuracy under attack of 57.20\% (+3.46\%). To test the generality of our findings and without any additional modifications, we obtain an accuracy under attack of 80.53\% (+7.62\%) against \${\textbackslash}ell\_2\$ perturbations of size \$128/255\$ on CIFAR-10, and of 36.88\% (+8.46\%) against \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations of size \$8/255\$ on CIFAR-100. All models are available at https://github.com/deepmind/deepmind-research/tree/master/adversarial\_robustness.},
	urldate = {2021-12-04},
	journal = {arXiv:2010.03593 [cs, stat]},
	author = {Gowal, Sven and Qin, Chongli and Uesato, Jonathan and Mann, Timothy and Kohli, Pushmeet},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.03593},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{cubuk_randaugment_2020,
	title = {Randaugment: {Practical} automated data augmentation with a reduced search space},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
	year = {2020},
}

@inproceedings{calian_defending_2021,
	title = {Defending {Against} {Image} {Corruptions} through {Adversarial} {Augmentation}},
	abstract = {Modern neural networks excel at image classiﬁcation, achieving superhuman performance, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as AugMix and DeepAugment, introduce defenses that operate in expectation over a distribution of image corruptions. In contrast, the literature on p-norm bounded perturbations focuses on defenses against worst-case corruptions. In this work, we reconcile both approaches by proposing AdversarialAugment, a technique which optimizes the parameters of image-to-image models to generate adversarially corrupted augmented images. Our classiﬁers improve upon the state-of-the-art on common image corruption benchmarks conducted in expectation on CIFAR-10-C and improve worst-case performance against p-norm bounded perturbations.},
	language = {en},
	booktitle = {{ICLR} {Workshop} on {RobustML}},
	author = {Calian, Dan A and Stimberg, Florian and Rebufﬁ, Sylvestre-Alvise and Wiles, Olivia and Gyorgy, Andras and Mann, Timothy and Gowal, Sven},
	year = {2021},
	pages = {13},
}

@inproceedings{gowal_achieving_2020,
	address = {Seattle, WA, USA},
	title = {Achieving {Robustness} in the {Wild} via {Adversarial} {Mixing} {With} {Disentangled} {Representations}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156449/},
	doi = {10.1109/CVPR42600.2020.00129},
	abstract = {Recent research has made the surprising ﬁnding that state-of-the-art deep learning models sometimes fail to generalize to small variations of the input. Adversarial training has been shown to be an effective approach to overcome this problem. However, its application has been limited to enforcing invariance to analytically deﬁned transformations like ℓp-norm bounded perturbations. Such perturbations do not necessarily cover plausible real-world variations that preserve the semantics of the input (such as a change in lighting conditions). In this paper, we propose a novel approach to express and formalize robustness to these kinds of real-world transformations of the input. The two key ideas underlying our formulation are (1) leveraging disentangled representations of the input to deﬁne different factors of variations, and (2) generating new input images by adversarially composing the representations of different images. We use a StyleGAN model to demonstrate the efﬁcacy of this framework. Specifically, we leverage the disentangled latent representations computed by a StyleGAN model to generate perturbations of an image that are similar to real-world variations (like adding make-up, or changing the skin-tone of a person) and train models to be invariant to these perturbations. Extensive experiments show that our method improves generalization and reduces the effect of spurious correlations (reducing the error rate of a “smile” detector by 21\% for example).},
	language = {en},
	urldate = {2021-12-03},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gowal, Sven and Qin, Chongli and Huang, Po-Sen and Cemgil, Taylan and Dvijotham, Krishnamurthy and Mann, Timothy and Kohli, Pushmeet},
	month = jun,
	year = {2020},
	pages = {1208--1217},
}

@inproceedings{hendrycks_many_2021,
	title = {The {Many} {Faces} of {Robustness}: {A} {Critical} {Analysis} of {Out}-of-{Distribution} {Generalization}},
	abstract = {We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We ﬁnd that using larger models and artiﬁcial data augmentations can improve robustness on realworld distribution shifts, contrary to claims in prior work. We ﬁnd improvements in artiﬁcial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000× more labeled data. Overall we ﬁnd that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.},
	language = {en},
	booktitle = {International {Conference} on {Computer} {Vision}},
	author = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and Song, Dawn and Steinhardt, Jacob and Gilmer, Justin},
	year = {2021},
	pages = {10},
}

@inproceedings{hendrycks_augmix_2020,
	title = {{AugMix}: {A} {Simple} {Data} {Processing} {Method} to {Improve} {Robustness} and {Uncertainty}},
	shorttitle = {{AugMix}},
	url = {https://openreview.net/forum?id=S1gmrxHFvB},
	abstract = {We obtain state-of-the-art on robustness to data shifts, and we maintain calibration under data shift even though even when accuracy drops},
	language = {en},
	urldate = {2021-12-03},
	author = {Hendrycks*, Dan and Mu*, Norman and Cubuk, Ekin Dogus and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
	year = {2020},
}

@inproceedings{hase_out--distribution_2021,
	title = {The {Out}-of-{Distribution} {Problem} in {Explainability} and {Search} {Methods} for {Feature} {Importance} {Explanations}},
	url = {https://openreview.net/forum?id=HCrp4pdk2i},
	abstract = {We present a solution to the out-of-distribution problem which feature importance estimates suffer from and propose a new search-based explanation methods that outperforms strong baselines on...},
	language = {en},
	urldate = {2021-12-02},
	author = {Hase, Peter and Xie, Harry and Bansal, Mohit},
	month = may,
	year = {2021},
}

@inproceedings{aitken_understanding_2021,
	title = {Understanding {How} {Encoder}-{Decoder} {Architectures} {Attend}},
	url = {https://openreview.net/forum?id=503UwCYEe5},
	abstract = {We investigate the dynamics behind networks trained on sequence to sequence tasks with and without attention.},
	language = {en},
	urldate = {2021-12-02},
	author = {Aitken, Kyle and Ramasesh, Vinay Venkatesh and Cao, Yuan and Maheswaranathan, Niru},
	month = may,
	year = {2021},
}

@inproceedings{lee_towards_2021,
	title = {Towards {Better} {Understanding} of {Training} {Certifiably} {Robust} {Models} against {Adversarial} {Examples}},
	url = {https://openreview.net/forum?id=b18Az57ioHn},
	abstract = {We identify a key factor that influences the performance of certifiable training: smoothness of the loss landscape.},
	language = {en},
	urldate = {2021-12-02},
	author = {Lee, Sungyoon and Lee, Woojin and Park, Jinseong and Lee, Jaewook},
	month = may,
	year = {2021},
}

@inproceedings{lubana_beyond_2021,
	title = {Beyond {BatchNorm}: {Towards} a {Unified} {Understanding} of {Normalization} in {Deep} {Learning}},
	shorttitle = {Beyond {BatchNorm}},
	url = {https://openreview.net/forum?id=DbxKZvfOIhu},
	abstract = {We identify key properties in randomly initialized networks that accurately determine success and failure modes of different normalization layers},
	language = {en},
	urldate = {2021-12-02},
	author = {Lubana, Ekdeep Singh and Dick, Robert and Tanaka, Hidenori},
	month = may,
	year = {2021},
}

@inproceedings{bai_understanding_2021,
	title = {Understanding and {Improving} {Early} {Stopping} for {Learning} with {Noisy} {Labels}},
	url = {https://openreview.net/forum?id=KbV-UZRKb3g},
	abstract = {The memorization effect of deep neural network (DNN) plays a pivotal role in many state-of-the-art label-noise learning methods.  To exploit this property, the early stopping trick, which stops the...},
	language = {en},
	urldate = {2021-12-02},
	author = {Bai, Yingbin and Yang, Erkun and Han, Bo and Yang, Yanhua and Li, Jiatong and Mao, Yinian and Niu, Gang and Liu, Tongliang},
	month = may,
	year = {2021},
}

@inproceedings{zimmermann_how_2021,
	title = {How {Well} do {Feature} {Visualizations} {Support} {Causal} {Understanding} of {CNN} {Activations}?},
	url = {https://openreview.net/forum?id=vLPqnPf9k0},
	abstract = {Using psychophysical experiments, we show that state-of-the-art synthetic feature visualizations do not support causal understanding much better than no visualizations, and only similarly well as...},
	language = {en},
	urldate = {2021-12-02},
	author = {Zimmermann, Roland Simon and Borowski, Judy and Geirhos, Robert and Bethge, Matthias and Wallis, Thomas S. A. and Brendel, Wieland},
	month = may,
	year = {2021},
}

@inproceedings{liu_learning_2021,
	title = {Learning {Causal} {Semantic} {Representation} for {Out}-of-{Distribution} {Prediction}},
	url = {https://openreview.net/forum?id=-msETI57gCH},
	abstract = {A supervised generative model with guarantees on the identifiability of the latent cause of prediction and on the generalizability to out-of-distribution cases.},
	language = {en},
	urldate = {2021-12-02},
	author = {Liu, Chang and Sun, Xinwei and Wang, Jindong and Tang, Haoyue and Li, Tao and Qin, Tao and Chen, Wei and Liu, Tie-Yan},
	month = may,
	year = {2021},
}

@inproceedings{ling_editgan_2021,
	title = {{EditGAN}: {High}-{Precision} {Semantic} {Image} {Editing}},
	shorttitle = {{EditGAN}},
	url = {https://openreview.net/forum?id=jLHWRxwc7_f},
	abstract = {Here, we propose EditGAN, a novel method for high-quality, high-precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks.},
	language = {en},
	urldate = {2021-12-02},
	author = {Ling, Huan and Kreis, Karsten and Li, Daiqing and Kim, Seung Wook and Torralba, Antonio and Fidler, Sanja},
	month = may,
	year = {2021},
}

@inproceedings{qin_improving_2021,
	title = {Improving {Calibration} through the {Relationship} with {Adversarial} {Robustness}},
	url = {https://openreview.net/forum?id=NJex-5TZIQa},
	abstract = {Neural networks lack adversarial robustness, i.e., they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions. Further, trust is undermined...},
	language = {en},
	urldate = {2021-12-02},
	author = {Qin, Yao and Wang, Xuezhi and Beutel, Alex and Chi, Ed},
	month = may,
	year = {2021},
}

@inproceedings{wu_wider_2021,
	title = {Do {Wider} {Neural} {Networks} {Really} {Help} {Adversarial} {Robustness}?},
	url = {https://openreview.net/forum?id=wxjtOI_8jO},
	abstract = {We study the relationship between adversarial robustness and the width of deep neural networks.},
	language = {en},
	urldate = {2021-12-02},
	author = {Wu, Boxi and Chen, Jinghui and Cai, Deng and He, Xiaofei and Gu, Quanquan},
	month = may,
	year = {2021},
}

@inproceedings{shu_encoding_2021,
	title = {Encoding {Robustness} to {Image} {Style} via {Adversarial} {Feature} {Perturbations}},
	url = {https://openreview.net/forum?id=A-RON3lv-aR},
	abstract = {This work proposes an adversarial feature perturbation method based on feature normalization statistics, to improve model's generalization to out-of-distribution data.},
	language = {en},
	urldate = {2021-12-02},
	author = {Shu, Manli and Wu, Zuxuan and Goldblum, Micah and Goldstein, Tom},
	month = may,
	year = {2021},
}

@inproceedings{fowl_adversarial_2021,
	title = {Adversarial {Examples} {Make} {Strong} {Poisons}},
	url = {https://openreview.net/forum?id=DE8MOQIgFTK},
	abstract = {We find that adversarial examples make stronger availability poisons than other methods designed specifically for data poisoning.},
	language = {en},
	urldate = {2021-12-02},
	author = {Fowl, Liam H. and Goldblum, Micah and Chiang, Ping-yeh and Geiping, Jonas and Czaja, Wojciech and Goldstein, Tom},
	month = may,
	year = {2021},
}

@inproceedings{xing_algorithmic_2021,
	title = {On the {Algorithmic} {Stability} of {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=xz80iPFIjvG},
	abstract = {Theoretical investigation on the algorithmic stability of adversarial training.},
	language = {en},
	urldate = {2021-12-02},
	author = {Xing, Yue and Song, Qifan and Cheng, Guang},
	month = may,
	year = {2021},
}

@inproceedings{erdemir_adversarial_2021,
	title = {Adversarial {Robustness} with {Non}-uniform {Perturbations}},
	url = {https://openreview.net/forum?id=cQLkLAQgZ5I},
	abstract = {Adversarial training with non-uniform perturbations across the features provides better robustness against the real-world attacks than the uniform approach.},
	language = {en},
	urldate = {2021-12-02},
	author = {Erdemir, Ecenaz and Bickford, Jeffrey and Melis, Luca and Aydore, Sergul},
	month = may,
	year = {2021},
}

@inproceedings{tsai_formalizing_2021,
	title = {Formalizing {Generalization} and {Adversarial} {Robustness} of {Neural} {Networks} to {Weight} {Perturbations}},
	url = {https://openreview.net/forum?id=hOG8swMRmY},
	abstract = {We provide a comprehensive theoretical analysis on the generalization and robustness of neural networks against weight perturbations, and propose a new theory-driven training loss.},
	language = {en},
	urldate = {2021-12-02},
	author = {Tsai, Yu-Lin and Hsu, Chia-Yi and Yu, Chia-Mu and Chen, Pin-Yu},
	month = may,
	year = {2021},
}

@inproceedings{slack_counterfactual_2021,
	title = {Counterfactual {Explanations} {Can} {Be} {Manipulated}},
	url = {https://openreview.net/forum?id=iaO_IH7CnGJ},
	abstract = {Counterfactual explanations that hill climb the counterfactual objective are not robust and vulnerable to manipulation by adversaries.},
	language = {en},
	urldate = {2021-12-02},
	author = {Slack, Dylan Z. and Hilgard, Sophie and Lakkaraju, Himabindu and Singh, Sameer},
	month = may,
	year = {2021},
}

@inproceedings{crabbe_explaining_2021,
	title = {Explaining {Latent} {Representations} with a {Corpus} of {Examples}},
	url = {https://openreview.net/forum?id=PIcuKeiWvj-},
	abstract = {We introduce SimplEx, a method that decomposes latent representations of test examples in terms of the representations of a corpus of examples.},
	language = {en},
	urldate = {2021-12-02},
	author = {Crabbé, Jonathan and Qian, Zhaozhi and Imrie, Fergus and Schaar, Mihaela van der},
	month = may,
	year = {2021},
}

@inproceedings{lu_influence_2021,
	title = {Influence {Patterns} for {Explaining} {Information} {Flow} in {BERT}},
	url = {https://openreview.net/forum?id=FYDE3I9fev0},
	abstract = {This paper introduces a new technique for explaining the information flow in BERT's computational graph using gradient-based method.},
	language = {en},
	urldate = {2021-12-02},
	author = {Lu, Kaiji and Wang, Zifan and Mardziel, Piotr and Datta, Anupam},
	month = may,
	year = {2021},
}

@inproceedings{paleja_utility_2021,
	title = {The {Utility} of {Explainable} {AI} in {Ad} {Hoc} {Human}-{Machine} {Teaming}},
	url = {https://openreview.net/forum?id=w6U6g5Bvug},
	abstract = {We present two human-subject studies quantifying the benefits of deploying Explainable AI techniques within a human-machine teaming scenario, finding that the benefits of xAI are not universal.},
	language = {en},
	urldate = {2021-12-02},
	author = {Paleja, Rohan R. and Ghuy, Muyleng and Arachchige, Nadun Ranawaka and Jensen, Reed and Gombolay, Matthew},
	month = may,
	year = {2021},
}

@inproceedings{zhang_fine-grained_2021,
	title = {Fine-{Grained} {Neural} {Network} {Explanation} by {Identifying} {Input} {Features} with {Predictive} {Information}},
	url = {https://openreview.net/forum?id=HglgPZAYhcG},
	abstract = {We propose a feature attribution method via identifying input features with predictive information.},
	language = {en},
	urldate = {2021-12-02},
	author = {Zhang, Yang and Khakzar, Ashkan and Li, Yawei and Farshad, Azade and Kim, Seong Tae and Navab, Nassir},
	month = may,
	year = {2021},
}

@inproceedings{teso_interactive_2021,
	title = {Interactive {Label} {Cleaning} with {Example}-based {Explanations}},
	url = {https://openreview.net/forum?id=T6m9bNI7C__},
	abstract = {Approach that enables humans to improve data and models by interacting via example-based explanations selected using influence functions},
	language = {en},
	urldate = {2021-12-02},
	author = {Teso, Stefano and Bontempelli, Andrea and Giunchiglia, Fausto and Passerini, Andrea},
	month = may,
	year = {2021},
}

@inproceedings{wickramanayake_explanation-based_2021,
	title = {Explanation-based {Data} {Augmentation} for {Image} {Classification}},
	url = {https://openreview.net/forum?id=Ydlco-tfIG},
	abstract = {An explanation based data augmentation approach is proposed to improve accuracy of image classifiers.},
	language = {en},
	urldate = {2021-12-02},
	author = {Wickramanayake, Sandareka and Hsu, Wynne and Lee, Mong-Li},
	month = may,
	year = {2021},
}

@inproceedings{ghalebikesabi_locality_2021,
	title = {On {Locality} of {Local} {Explanation} {Models}},
	url = {https://openreview.net/forum?id=UKoV0-BamX4},
	abstract = {We consider the formulation of  neighbourhood reference distributions that improve the local interpretability of Shapley values.},
	language = {en},
	urldate = {2021-12-02},
	author = {Ghalebikesabi, Sahra and Ter-Minassian, Lucile and DiazOrdaz, Karla and Holmes, Christopher C.},
	month = may,
	year = {2021},
}

@inproceedings{kumar_shapley_2021,
	title = {Shapley {Residuals}: {Quantifying} the limits of the {Shapley} value for explanations},
	shorttitle = {Shapley {Residuals}},
	url = {https://openreview.net/forum?id=0XJDcC07tQs},
	abstract = {We propose expressing the information lost by the computation of Shapley values with a residual which quantify the extent to which interaction effects are being "lost."},
	language = {en},
	urldate = {2021-12-02},
	author = {Kumar, I. Elizabeth and Scheidegger, Carlos and Venkatasubramanian, Suresh and Friedler, Sorelle},
	month = may,
	year = {2021},
}

@inproceedings{chan_salkg_2021,
	title = {{SalKG}: {Learning} {From} {Knowledge} {Graph} {Explanations} for {Commonsense} {Reasoning}},
	shorttitle = {{SalKG}},
	url = {https://openreview.net/forum?id=FUxXaBop-J_},
	abstract = {Train KG-augmented models using extra supervision from KG saliency explanations.},
	language = {en},
	urldate = {2021-12-02},
	author = {Chan, Aaron and Xu, Jiashu and Long, Boyuan and Sanyal, Soumya and Gupta, Tanishq and Ren, Xiang},
	month = may,
	year = {2021},
}

@inproceedings{slack_reliable_2021,
	title = {Reliable {Post} hoc {Explanations}: {Modeling} {Uncertainty} in {Explainability}},
	shorttitle = {Reliable {Post} hoc {Explanations}},
	url = {https://openreview.net/forum?id=rqfq0CYIekd},
	abstract = {Black box explanations with uncertainty estimates are more consistent, stable, and reliable, while providing guarantees on the uncertainty.},
	language = {en},
	urldate = {2021-12-02},
	author = {Slack, Dylan Z. and Hilgard, Sophie and Singh, Sameer and Lakkaraju, Himabindu},
	month = may,
	year = {2021},
}

@inproceedings{fel_look_2021,
	title = {Look at the {Variance}! {Efficient} {Black}-box {Explanations} with {Sobol}-based {Sensitivity} {Analysis}},
	url = {https://openreview.net/forum?id=hA-PHQGOjqQ},
	abstract = {We explain black-box models by measuring how their predictions vary when we perturb their input at various location using Sobol.},
	language = {en},
	urldate = {2021-12-02},
	author = {Fel, Thomas and Cadene, Remi and Chalvidal, Mathieu and Cord, Matthieu and Vigouroux, David and Serre, Thomas},
	month = may,
	year = {2021},
}

@inproceedings{blanc_provably_2021,
	title = {Provably efficient, succinct, and precise explanations},
	url = {https://openreview.net/forum?id=DV06vy74q92},
	abstract = {We design an efficient algorithm for explaining the predictions of an arbitrary blackbox model, with provable guarantees on the succinctness and precision of the explanations that it returns.},
	language = {en},
	urldate = {2021-12-02},
	author = {Blanc, Guy and Lange, Jane and Tan, Li-Yang},
	month = may,
	year = {2021},
}

@inproceedings{yao_refining_2021,
	title = {Refining {Language} {Models} with {Compositional} {Explanations}},
	url = {https://openreview.net/forum?id=dkw9OQMn1t},
	abstract = {Pre-trained language models have been successful on text classification tasks, but are prone to learning spurious correlations from biased datasets, and are thus vulnerable when making inferences...},
	language = {en},
	urldate = {2021-12-02},
	author = {Yao, Huihan and Chen, Ying and Ye, Qinyuan and Jin, Xisen and Ren, Xiang},
	month = may,
	year = {2021},
}

@inproceedings{shitole_one_2021,
	title = {One {Explanation} is {Not} {Enough}: {Structured} {Attention} {Graphs} for {Image} {Classification}},
	shorttitle = {One {Explanation} is {Not} {Enough}},
	url = {https://openreview.net/forum?id=k5Kbs9uPGP9},
	abstract = {We introduce multiple minimally sufficient explanations for deep image classification with techniques to efficiently search them and succinctly present them.},
	language = {en},
	urldate = {2021-12-02},
	author = {Shitole, Vivswan and Fuxin, Li and Kahng, Minsuk and Tadepalli, Prasad and Fern, Alan},
	month = may,
	year = {2021},
}

@inproceedings{bai_clustering_2021,
	title = {Clustering {Effect} of {Adversarial} {Robust} {Models}},
	url = {https://openreview.net/forum?id=5ga5mfbGsRM},
	abstract = {Clustering Effect of (Linearized) Adversarial Robust Models},
	language = {en},
	urldate = {2021-12-02},
	author = {Bai, Yang and Yan, Xin and Jiang, Yong and Xia, Shu-Tao and Wang, Yisen},
	month = may,
	year = {2021},
}

@inproceedings{singla_shift_2021,
	title = {Shift {Invariance} {Can} {Reduce} {Adversarial} {Robustness}},
	url = {https://openreview.net/forum?id=tqi_45ApQzF},
	abstract = {We provide theoretical and empirical evidence that the property of shift invariance in convolutional neural networks can decrease adversarial robustness.},
	language = {en},
	urldate = {2021-12-02},
	author = {Singla, Vasu and Ge, Songwei and Basri, Ronen and Jacobs, David},
	month = may,
	year = {2021},
}

@inproceedings{sriramanan_towards_2021,
	title = {Towards {Efficient} and {Effective} {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=kuK2VARZGnI},
	abstract = {We propose methods to improve the efficiency and effectiveness of Adversarial Training},
	language = {en},
	urldate = {2021-12-02},
	author = {Sriramanan, Gaurang and Addepalli, Sravanti and Baburaj, Arya and Radhakrishnan, Venkatesh Babu},
	month = may,
	year = {2021},
}

@inproceedings{kim_distilling_2021,
	title = {Distilling {Robust} and {Non}-{Robust} {Features} in {Adversarial} {Examples} by {Information} {Bottleneck}},
	url = {https://openreview.net/forum?id=90M-91IZ0JC},
	abstract = {We present a way of distilling the robust and non-robust features in adversarial examples, using Information Bottleneck.},
	language = {en},
	urldate = {2021-12-02},
	author = {Kim, Junho and Lee, Byung-Kwan and Ro, Yong Man},
	month = may,
	year = {2021},
}

@inproceedings{yang_class-disentanglement_2021,
	title = {Class-{Disentanglement} and {Applications} in {Adversarial} {Detection} and {Defense}},
	url = {https://openreview.net/forum?id=jFMzBeLyTc0},
	abstract = {We propose a simple VAE+classifier structure to separate the class information from an image by decomposing it into two part.},
	language = {en},
	urldate = {2021-12-02},
	author = {Yang, Kaiwen and Zhou, Tianyi and Zhang, Yonggang and Tian, Xinmei and Tao, Dacheng},
	month = may,
	year = {2021},
}

@inproceedings{ren_towards_2021,
	title = {Towards a {Unified} {Game}-{Theoretic} {View} of {Adversarial} {Perturbations} and {Robustness}},
	url = {https://openreview.net/forum?id=fMaIxda5Y6K},
	abstract = {This paper provides a unified view to explain different adversarial attacks and defense methods, i.e. the view of multi-order interactions between input variables of DNNs.},
	language = {en},
	urldate = {2021-12-02},
	author = {Ren, Jie and Zhang, Die and Wang, Yisen and Chen, Lu and Zhou, Zhanpeng and Chen, Yiting and Cheng, Xu and Wang, Xin and Zhou, Meng and Shi, Jie and Zhang, Quanshi},
	month = may,
	year = {2021},
}

@inproceedings{arenas_foundations_2021,
	title = {Foundations of {Symbolic} {Languages} for {Model} {Interpretability}},
	url = {https://openreview.net/forum?id=Jyxmk4wUoQV},
	abstract = {We propose a systematic study of model interpretability by considering a minimalistic symbolic query language tailored for interpretability, studying its complexity of evaluation and presenting a...},
	language = {en},
	urldate = {2021-12-02},
	author = {Arenas, Marcelo and Báez, Daniel and Barcelo, Pablo and Pérez, Jorge and Subercaseaux, Bernardo},
	month = may,
	year = {2021},
}

@inproceedings{pan_ia-red2_2021,
	title = {{IA}-{RED2}: {Interpretability}-{Aware} {Redundancy} {Reduction} for {Vision} {Transformers}},
	shorttitle = {{IA}-{RED}\${\textasciicircum}2\$},
	url = {https://openreview.net/forum?id=7X_sBjIwtm9},
	abstract = {An input-dependent and interpretable dynamic inference framework for vision transformer, which adaptively decides the patch tokens to compute per input instance.},
	language = {en},
	urldate = {2021-12-02},
	author = {Pan, Bowen and Panda, Rameswar and Jiang, Yifan and Wang, Zhangyang and Feris, Rogerio and Oliva, Aude},
	month = may,
	year = {2021},
}

@inproceedings{ismail_improving_2021,
	title = {Improving {Deep} {Learning} {Interpretability} by {Saliency} {Guided} {Training}},
	url = {https://openreview.net/forum?id=x4zs7eC-BsI},
	abstract = {We introduce a training procedure to improve model interpretability},
	language = {en},
	urldate = {2021-12-02},
	author = {Ismail, Aya Abdelsalam and Bravo, Hector Corrada and Feizi, Soheil},
	month = may,
	year = {2021},
}

@inproceedings{puri_cofrnets_2021,
	title = {{CoFrNets}: {Interpretable} {Neural} {Architecture} {Inspired} by {Continued} {Fractions}},
	shorttitle = {{CoFrNets}},
	url = {https://openreview.net/forum?id=kGXlIEQgvC},
	abstract = {Propose a new interpretable neural architecture inspired by continued fractions},
	language = {en},
	urldate = {2021-12-02},
	author = {Puri, Isha and Dhurandhar, Amit and Pedapati, Tejaswini and Shanmugam, Karthikeyan and Wei, Dennis and Varshney, Kush R.},
	month = may,
	year = {2021},
}

@inproceedings{agarwal_neural_2021,
	title = {Neural {Additive} {Models}: {Interpretable} {Machine} {Learning} with {Neural} {Nets}},
	shorttitle = {Neural {Additive} {Models}},
	url = {https://openreview.net/forum?id=wHkKTW2wrmm},
	abstract = {We propose Neural Additive Models that combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models.},
	language = {en},
	urldate = {2021-12-02},
	author = {Agarwal, Rishabh and Melnick, Levi and Frosst, Nicholas and Zhang, Xuezhou and Lengerich, Ben and Caruana, Rich and Hinton, Geoffrey},
	month = may,
	year = {2021},
}

@inproceedings{wang_self-interpretable_2021,
	title = {Self-{Interpretable} {Model} with {Transformation} {Equivariant} {Interpretation}},
	url = {https://openreview.net/forum?id=YlM3tey8Z5I},
	abstract = {We propose a self-interpretable model that has transformation-equivariant interpretations, and comparable expressive power as benchmark black-box model.},
	language = {en},
	urldate = {2021-12-02},
	author = {Wang, Yipei and Wang, Xiaoqian},
	month = may,
	year = {2021},
}

@inproceedings{wang_augmax_2021,
	title = {{AugMax}: {Adversarial} {Composition} of {Random} {Augmentations} for {Robust} {Training}},
	shorttitle = {{AugMax}},
	url = {https://openreview.net/forum?id=P5MtdcVdFZ4},
	abstract = {Data augmentation is a simple yet effective way to improve the robustness of deep neural networks (DNNs). Diversity and hardness are two complementary dimensions of data augmentation to achieve...},
	language = {en},
	urldate = {2021-12-02},
	author = {Wang, Haotao and Xiao, Chaowei and Kossaifi, Jean and Yu, Zhiding and Anandkumar, Anima and Wang, Zhangyang},
	month = may,
	year = {2021},
}

@article{wang_hybrid_2021,
	title = {Hybrid {Predictive} {Models}: {When} an {Interpretable} {Model} {Collaborates} with a {Black}-box {Model}},
	volume = {22},
	issn = {1533-7928},
	shorttitle = {Hybrid {Predictive} {Models}},
	url = {http://jmlr.org/papers/v22/19-325.html},
	language = {en},
	number = {137},
	urldate = {2021-12-01},
	journal = {Journal of Machine Learning Research},
	author = {Wang, Tong and Lin, Qihang},
	year = {2021},
	pages = {1--38},
}

@article{xie_smooth_2020,
	title = {Smooth {Adversarial} {Training}},
	url = {https://arxiv.org/abs/2006.14536v2},
	abstract = {It is commonly believed that networks cannot be both accurate and robust, that gaining robustness means losing accuracy. It is also generally believed that, unless making networks larger, network architectural elements would otherwise matter little in improving adversarial robustness. Here we present evidence to challenge these common beliefs by a careful study about adversarial training. Our key observation is that the widely-used ReLU activation function significantly weakens adversarial training due to its non-smooth nature. Hence we propose smooth adversarial training (SAT), in which we replace ReLU with its smooth approximations to strengthen adversarial training. The purpose of smooth activation functions in SAT is to allow it to find harder adversarial examples and compute better gradient updates during adversarial training. Compared to standard adversarial training, SAT improves adversarial robustness for "free", i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50's robustness from 33.0\% to 42.3\%, while also improving accuracy by 0.9\% on ImageNet. SAT also works well with larger networks: it helps EfficientNet-L1 to achieve 82.2\% accuracy and 58.6\% robustness on ImageNet, outperforming the previous state-of-the-art defense by 9.5\% for accuracy and 11.6\% for robustness. Models are available at https://github.com/cihangxie/SmoothAdversarialTraining.},
	language = {en},
	urldate = {2021-11-17},
	author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Yuille, Alan and Le, Quoc V.},
	month = jun,
	year = {2020},
}

@inproceedings{panda_instance-wise_2021,
	address = {Nashville, TN, USA},
	title = {Instance-wise {Causal} {Feature} {Selection} for {Model} {Interpretation}},
	isbn = {978-1-66544-899-4},
	url = {https://ieeexplore.ieee.org/document/9523197/},
	doi = {10.1109/CVPRW53098.2021.00194},
	abstract = {We formulate a causal extension to the recently introduced paradigm of instance-wise feature selection to explain black-box visual classiﬁers. Our method selects a subset of input features that has the greatest causal effect on the model’s output. We quantify the causal inﬂuence of a subset of features by the Relative Entropy Distance measure. Under certain assumptions this is equivalent to the conditional mutual information between the selected subset and the output variable. The resulting causal selections are sparser and cover salient objects in the scene. We show the efﬁcacy of our approach on multiple vision datasets by measuring the post-hoc accuracy and Average Causal Effect of selected features on the model’s output.},
	language = {en},
	urldate = {2021-11-12},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Panda, Pranoy and Kancheti, Sai Srinivas and Balasubramanian, Vineeth N},
	month = jun,
	year = {2021},
	pages = {1756--1759},
}

@inproceedings{miyato_spectral_2018,
	title = {Spectral {Normalization} for {Generative} {Adversarial} {Networks}},
	url = {https://openreview.net/forum?id=B1QRgziT-},
	abstract = {We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.},
	language = {en},
	urldate = {2021-10-29},
	author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	month = feb,
	year = {2018},
}

@inproceedings{hendrycks_using_2019,
	title = {Using {Pre}-{Training} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	url = {https://proceedings.mlr.press/v97/hendrycks19a.html},
	abstract = {He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on label corruption, class imbalance, adversarial examples, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We show approximately a 10\% absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2712--2721},
}

@article{recht_cifar-10_2018,
	title = {Do {CIFAR}-10 {Classifiers} {Generalize} to {CIFAR}-10?},
	url = {http://arxiv.org/abs/1806.00451},
	abstract = {Machine learning is currently dominated by largely experimental work focused on improvements in a few key tasks. However, the impressive accuracy numbers of the best performing models are questionable because the same test sets have been used to select these models for multiple years now. To understand the danger of overﬁtting, we measure the accuracy of CIFAR-10 classiﬁers by creating a new test set of truly unseen images. Although we ensure that the new test set is as close to the original data distribution as possible, we ﬁnd a large drop in accuracy (4\% to 10\%) for a broad range of deep learning models. Yet, more recent models with higher original accuracy show a smaller drop and better overall performance, indicating that this drop is likely not due to overﬁtting based on adaptivity. Instead, we view our results as evidence that current accuracy numbers are brittle and susceptible to even minute natural variations in the data distribution.},
	language = {en},
	urldate = {2021-10-22},
	journal = {arXiv:1806.00451 [cs, stat]},
	author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.00451},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{najafi_robustness_2019,
	title = {Robustness to {Adversarial} {Perturbations} in {Learning} from {Incomplete} {Data}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/60ad83801910ec976590f69f638e0d6d-Abstract.html},
	urldate = {2021-10-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Najafi, Amir and Maeda, Shin-ichi and Koyama, Masanori and Miyato, Takeru},
	year = {2019},
}

@inproceedings{dombrowski_explanations_2019,
	title = {Explanations can be manipulated and geometry is to blame},
	abstract = {Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network’s output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dombrowski, Ann-Kathrin and Alber, Maximillian and Anders, Christopher and Ackermann, Marcel and Müller, Klaus-Robert and Kessel, Pan},
	year = {2019},
	pages = {12},
}

@article{dombrowski_towards_2022,
	title = {Towards robust explanations for deep neural networks},
	volume = {121},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321003769},
	doi = {10.1016/j.patcog.2021.108194},
	abstract = {Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks. But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches.},
	language = {en},
	urldate = {2021-09-29},
	journal = {Pattern Recognition},
	author = {Dombrowski, Ann-Kathrin and Anders, Christopher J. and Müller, Klaus-Robert and Kessel, Pan},
	month = jan,
	year = {2022},
	keywords = {Adversarial attacks, Explanation method, Manipulation, Neural networks,, Saliency map},
	pages = {108194},
}

@inproceedings{guo_calibration_2017,
	title = {On {Calibration} of {Modern} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v70/guo17a.html},
	abstract = {Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a single-parameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	language = {en},
	urldate = {2021-09-24},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1321--1330},
}

@inproceedings{cheng_self-progressing_2021,
	title = {Self-{Progressing} {Robust} {Training}},
	volume = {35},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16874},
	abstract = {Enhancing model robustness under new and even adversarial environments is a crucial milestone toward building trustworthy machine learning systems. Current robust training methods such as adversarial training explicitly uses an ``attack'' (e.g., l\_infty-norm bounded perturbation) to generate adversarial examples during model training for improving adversarial robustness. In this paper, we take a different perspective and propose a new framework SPROUT, self-progressing robust training. During model training, SPROUT progressively adjusts training label distribution via our proposed parametrized label smoothing technique, making training free of attack generation and more scalable. We also motivate SPROUT using a general formulation based on vicinity risk minimization, which includes many robust training methods as special cases.
Compared with state-of-the-art adversarial training methods (PGD-l\_infty and TRADES) under l\_infty-norm bounded attacks and various invariance tests, SPROUT consistently attains superior performance and is more scalable to large neural networks. Our results shed new light on scalable, effective and attack-independent robust training methods.},
	language = {en},
	urldate = {2021-09-24},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Cheng, Minhao and Chen, Pin-Yu and Liu, Sijia and Chang, Shiyu and Hsieh, Cho-Jui and Das, Payel},
	month = may,
	year = {2021},
	note = {Number: 8},
	keywords = {(Deep) Neural Network Algorithms},
	pages = {7107--7115},
}

@inproceedings{li_gradient_2020,
	title = {Gradient {Descent} with {Early} {Stopping} is {Provably} {Robust} to {Label} {Noise} for {Overparameterized} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v108/li20j.html},
	abstract = {Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including significantly corrupted ones. Despite this (over)fitting capacity in this paper we demonstrate that such overparameterized networks have an intriguing robustness capability: they are surprisingly robust to label noise when first order methods with early stopping is used to train them. This paper also takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) To start to overfit to the noisy labels network must stray rather far from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4313--4324},
}

@inproceedings{huang_self-adaptive_2020,
	title = {Self-{Adaptive} {Training}: beyond {Empirical} {Risk} {Minimization}},
	abstract = {We propose self-adaptive training—a new training algorithm that dynamically calibrates training process by model predictions without incurring extra computational cost—to improve generalization of deep learning for potentially corrupted training data. This problem is important to robustly learning from data that are corrupted by, e.g., random noise and adversarial examples. The standard empirical risk minimization (ERM) for such data, however, may easily overﬁt noise and thus suffers from sub-optimal performance. In this paper, we observe that model predictions can substantially beneﬁt the training process: self-adaptive training signiﬁcantly mitigates the overﬁtting issue and improves generalization over ERM under both random and adversarial noise. Besides, in sharp contrast to the recently-discovered double-descent phenomenon in ERM, self-adaptive training exhibits a single-descent error-capacity curve, indicating that such a phenomenon might be a result of overﬁtting of noise. Experiments on the CIFAR and ImageNet datasets verify the effectiveness of our approach in two applications: classiﬁcation with label noise and selective classiﬁcation. The code is available at https://github.com/LayneH/self-adaptive-training.},
	language = {en},
	booktitle = {34th {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Huang, Lang and Zhang, Chao and Zhang, Hongyang},
	year = {2020},
	pages = {12},
}

@inproceedings{song_improving_2019,
	title = {Improving the {Generalization} of {Adversarial} {Training} with {Domain} {Adaptation}},
	url = {https://openreview.net/forum?id=SyfIfnC5Ym},
	abstract = {We propose a novel adversarial training with domain adaptation method that significantly improves the generalization ability on adversarial examples from different attacks.},
	language = {en},
	urldate = {2021-09-22},
	author = {Song, Chuanbiao and He, Kun and Wang, Liwei and Hopcroft, John E.},
	year = {2019},
}

@inproceedings{gilmer_adversarial_2018,
	title = {Adversarial {Spheres}},
	url = {https://openreview.net/forum?id=SkthlLkPf},
	abstract = {We explore the phenomenon of adversarial examples on a synthetic dataset},
	language = {en},
	urldate = {2021-09-21},
	booktitle = {International {Conference} on {Learning} {Representations} {Workshop}},
	author = {Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
	month = feb,
	year = {2018},
}

@inproceedings{lee_adversarial_2020,
	title = {Adversarial {Vertex} {Mixup}: {Toward} {Better} {Adversarially} {Robust} {Generalization}},
	shorttitle = {Adversarial {Vertex} {Mixup}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Lee_Adversarial_Vertex_Mixup_Toward_Better_Adversarially_Robust_Generalization_CVPR_2020_paper.html},
	urldate = {2021-09-15},
	author = {Lee, Saehyung and Lee, Hyungyu and Yoon, Sungroh},
	year = {2020},
	pages = {272--281},
}

@inproceedings{farnia_generalizable_2018,
	title = {Generalizable {Adversarial} {Training} via {Spectral} {Normalization}},
	url = {https://openreview.net/forum?id=Hyx4knR9Ym},
	abstract = {Deep neural networks (DNNs) have set benchmarks on a wide array of supervised learning tasks. Trained DNNs, however, often lack robustness to minor adversarial perturbations to the input, which...},
	language = {en},
	urldate = {2021-09-15},
	author = {Farnia, Farzan and Zhang, Jesse and Tse, David},
	month = sep,
	year = {2018},
}

@inproceedings{buckman_thermometer_2018,
	title = {Thermometer {Encoding}: {One} {Hot} {Way} to {Resist} {Adversarial} {Examples}},
	abstract = {It is well known that it is possible to construct “adversarial examples” for neural networks: inputs which are misclassiﬁed by the network yet indistinguishable from true data. We propose a simple modiﬁcation to standard neural network architectures, thermometer encoding, which signiﬁcantly increases the robustness of the network to adversarial examples. We demonstrate this robustness with experiments on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that models with thermometer-encoded inputs consistently have higher accuracy on adversarial examples, without decreasing generalization. State-of-the-art accuracy under the strongest known white-box attack was increased from 93.20\% to 94.30\% on MNIST and 50.00\% to 79.16\% on CIFAR-10. We explore the properties of these networks, providing evidence that thermometer encodings help neural networks to ﬁnd more-non-linear decision boundaries.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Buckman, Jacob and Roy, Aurko and Raffel, Colin and Goodfellow, Ian},
	year = {2018},
	pages = {22},
}

@inproceedings{hacohen_lets_2020,
	title = {Let’s {Agree} to {Agree}: {Neural} {Networks} {Share} {Classification} {Order} on {Real} {Datasets}},
	shorttitle = {Let’s {Agree} to {Agree}},
	url = {https://proceedings.mlr.press/v119/hacohen20a.html},
	language = {en},
	urldate = {2021-09-12},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hacohen, Guy and Choshen, Leshem and Weinshall, Daphna},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3950--3960},
}

@inproceedings{toneva_empirical_2018,
	title = {An {Empirical} {Study} of {Example} {Forgetting} during {Deep} {Neural} {Network} {Learning}},
	url = {https://openreview.net/forum?id=BJlxm30cKm},
	abstract = {We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss...},
	language = {en},
	urldate = {2021-09-12},
	author = {Toneva*, Mariya and Sordoni*, Alessandro and Combes*, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J.},
	month = sep,
	year = {2018},
}

@inproceedings{shafahi_are_2018,
	title = {Are adversarial examples inevitable?},
	url = {https://openreview.net/forum?id=r1lWUoA9FQ},
	abstract = {This paper identifies classes of problems for which adversarial examples are inescapable, and derives fundamental bounds on the susceptibility of any classifier to adversarial examples.},
	language = {en},
	urldate = {2021-09-12},
	author = {Shafahi, Ali and Huang, W. Ronny and Studer, Christoph and Feizi, Soheil and Goldstein, Tom},
	month = sep,
	year = {2018},
}

@inproceedings{stutz_confidence-calibrated_2020,
	title = {Confidence-{Calibrated} {Adversarial} {Training}: {Generalizing} to {Unseen} {Attacks}},
	abstract = {Adversarial training yields robust models against a speciﬁc threat model, e.g., L∞ adversarial examples. Typically robustness does not generalize to previously unseen threat models, e.g., other Lp norms, or larger perturbations. Our conﬁdencecalibrated adversarial training (CCAT) tackles this problem by biasing the model towards low conﬁdence predictions on adversarial examples. By allowing to reject examples with low conﬁdence, robustness generalizes beyond the threat model employed during training. CCAT, trained only on L∞ adversarial examples, increases robustness against larger L∞, L2, L1 and L0 attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training. For thorough evaluation we developed novel white- and black-box attacks directly attacking CCAT by maximizing conﬁdence. For each threat model, we use 7 attacks with up to 50 restarts and 5000 iterations and report worst-case robust test error, extended to our conﬁdence-thresholded setting, across all attacks.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	year = {2020},
	pages = {12},
}

@article{zhu_understanding_2021,
	title = {Understanding the {Interaction} of {Adversarial} {Training} with {Noisy} {Labels}},
	url = {https://arxiv.org/abs/2102.03482v2},
	abstract = {Noisy labels (NL) and adversarial examples both undermine trained models, but interestingly they have hitherto been studied independently. A recent adversarial training (AT) study showed that the number of projected gradient descent (PGD) steps to successfully attack a point (i.e., find an adversarial example in its proximity) is an effective measure of the robustness of this point. Given that natural data are clean, this measure reveals an intrinsic geometric property -- how far a point is from its class boundary. Based on this breakthrough, in this paper, we figure out how AT would interact with NL. Firstly, we find if a point is too close to its noisy-class boundary (e.g., one step is enough to attack it), this point is likely to be mislabeled, which suggests to adopt the number of PGD steps as a new criterion for sample selection for correcting NL. Secondly, we confirm AT with strong smoothing effects suffers less from NL (without NL corrections) than standard training (ST), which suggests AT itself is an NL correction. Hence, AT with NL is helpful for improving even the natural accuracy, which again illustrates the superiority of AT as a general-purpose robust learning criterion.},
	language = {en},
	urldate = {2021-09-10},
	author = {Zhu, Jianing and Zhang, Jingfeng and Han, Bo and Liu, Tongliang and Niu, Gang and Yang, Hongxia and Kankanhalli, Mohan and Sugiyama, Masashi},
	month = feb,
	year = {2021},
}

@article{allen-zhu_feature_2020,
	title = {Feature {Purification}: {How} {Adversarial} {Training} {Performs} {Robust} {Deep} {Learning}},
	shorttitle = {Feature {Purification}},
	url = {https://arxiv.org/abs/2005.10190v3},
	abstract = {Despite the empirical success of using Adversarial Training to defend deep learning models against adversarial perturbations, so far, it still remains rather unclear what the principles are behind the existence of adversarial perturbations, and what adversarial training does to the neural network to remove them. In this paper, we present a principle that we call Feature Purification, where we show one of the causes of the existence of adversarial examples is the accumulation of certain small dense mixtures in the hidden weights during the training process of a neural network; and more importantly, one of the goals of adversarial training is to remove such mixtures to purify hidden weights. We present both experiments on the CIFAR-10 dataset to illustrate this principle, and a theoretical result proving that for certain natural classification tasks, training a two-layer neural network with ReLU activation using randomly initialized gradient descent indeed satisfies this principle. Technically, we give, to the best of our knowledge, the first result proving that the following two can hold simultaneously for training a neural network with ReLU activation. (1) Training over the original data is indeed non-robust to small adversarial perturbations of some radius. (2) Adversarial training, even with an empirical perturbation algorithm such as FGM, can in fact be provably robust against ANY perturbations of the same radius. Finally, we also prove a complexity lower bound, showing that low complexity models such as linear classifiers, low-degree polynomials, or even the neural tangent kernel for this network, CANNOT defend against perturbations of this same radius, no matter what algorithms are used to train them.},
	language = {en},
	urldate = {2021-09-09},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	month = may,
	year = {2020},
}

@article{dong_data_2021,
	title = {Data {Profiling} for {Adversarial} {Training}: {On} the {Ruin} of {Problematic} {Data}},
	shorttitle = {Data {Profiling} for {Adversarial} {Training}},
	url = {https://arxiv.org/abs/2102.07437v2},
	abstract = {There are multiple intriguing problems hovering in adversarial training, including robustness-accuracy trade-off, robust overfitting, and robustness overestimation. These problems pose great challenges to both reliable evaluation and practical deployment. Here, we show that these problems share one common cause -- low quality samples in the dataset. We first identify an intrinsic property of the data called {\textbackslash}emph\{problematic score\} and then design controlled experiments to investigate its connections with these problems. Specifically, we find that when problematic data is removed, robust overfitting and robustness overestimation can be largely alleviated; and robustness-accuracy trade-off becomes less significant. These observations not only verify our intuition about data quality but also open new opportunities to advance adversarial training. Interestingly, simply removing problematic data from adversarial training, while making the training set smaller, yields better robustness for leading adversarial training strategies.},
	language = {en},
	urldate = {2021-09-09},
	author = {Dong, Chengyu and Liu, Liyuan and Shang, Jingbo},
	month = feb,
	year = {2021},
}

@inproceedings{donhauser_maximizing_2021,
	title = {Maximizing the robust margin provably overfits on noiseless data},
	url = {https://openreview.net/forum?id=ujQKWaxFkrL},
	abstract = {We show that the robust max-margin solution overfits even on noiseless data yielding a worse performance than ridge regularized and early stopped robust logistic regression.},
	language = {en},
	urldate = {2021-09-09},
	author = {Donhauser, Konstantin and Tifrea, Alexandru and Aerni, Michael and Heckel, Reinhard and Yang, Fanny},
	month = jun,
	year = {2021},
}

@inproceedings{xing_generalization_2021,
	title = {On the {Generalization} {Properties} of {Adversarial} {Training}},
	url = {https://proceedings.mlr.press/v130/xing21b.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Xing, Yue and Song, Qifan and Cheng, Guang},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {505--513},
}

@inproceedings{song_robust_2019,
	title = {Robust {Local} {Features} for {Improving} the {Generalization} of {Adversarial} {Training}},
	url = {https://openreview.net/forum?id=H1lZJpVFvr},
	abstract = {We propose a new stream of adversarial training approach called Robust Local Features for Adversarial Training (RLFAT) that significantly improves both the adversarially robust generalization and...},
	language = {en},
	urldate = {2021-09-09},
	author = {Song, Chuanbiao and He, Kun and Lin, Jiadong and Wang, Liwei and Hopcroft, John E.},
	month = sep,
	year = {2019},
}

@article{zhang_noilin_2021,
	title = {{NoiLIn}: {Do} {Noisy} {Labels} {Always} {Hurt} {Adversarial} {Training}?},
	shorttitle = {{NoiLIn}},
	url = {https://arxiv.org/abs/2105.14676v1},
	abstract = {Adversarial training (AT) based on minimax optimization is a popular learning style that enhances the model's adversarial robustness. Noisy labels (NL) commonly undermine the learning and hurt the model's performance. Interestingly, both research directions hardly crossover and hit sparks. In this paper, we raise an intriguing question -- Does NL always hurt AT? Firstly, we find that NL injection in inner maximization for generating adversarial data augments natural data implicitly, which benefits AT's generalization. Secondly, we find NL injection in outer minimization for the learning serves as regularization that alleviates robust overfitting, which benefits AT's robustness. To enhance AT's adversarial robustness, we propose "NoiLIn" that gradually increases {\textbackslash}underline\{Noi\}sy {\textbackslash}underline\{L\}abels {\textbackslash}underline\{In\}jection over the AT's training process. Empirically, NoiLIn answers the previous question negatively -- the adversarial robustness can be indeed enhanced by NL injection. Philosophically, we provide a new perspective of the learning with NL: NL should not always be deemed detrimental, and even in the absence of NL in the training set, we may consider injecting it deliberately.},
	language = {en},
	urldate = {2021-09-09},
	author = {Zhang, Jingfeng and Xu, Xilie and Han, Bo and Liu, Tongliang and Niu, Gang and Cui, Lizhen and Sugiyama, Masashi},
	month = may,
	year = {2021},
}

@article{stutz_relating_2021,
	title = {Relating {Adversarially} {Robust} {Generalization} to {Flat} {Minima}},
	url = {https://arxiv.org/abs/2104.04448v1},
	abstract = {Adversarial training (AT) has become the de-facto standard to obtain models robust against adversarial examples. However, AT exhibits severe robust overfitting: cross-entropy loss on adversarial examples, so-called robust loss, decreases continuously on training examples, while eventually increasing on test examples. In practice, this leads to poor robust generalization, i.e., adversarial robustness does not generalize well to new examples. In this paper, we study the relationship between robust generalization and flatness of the robust loss landscape in weight space, i.e., whether robust loss changes significantly when perturbing weights. To this end, we propose average- and worst-case metrics to measure flatness in the robust loss landscape and show a correlation between good robust generalization and flatness. For example, throughout training, flatness reduces significantly during overfitting such that early stopping effectively finds flatter minima in the robust loss landscape. Similarly, AT variants achieving higher adversarial robustness also correspond to flatter minima. This holds for many popular choices, e.g., AT-AWP, TRADES, MART, AT with self-supervision or additional unlabeled examples, as well as simple regularization techniques, e.g., AutoAugment, weight decay or label noise. For fair comparison across these approaches, our flatness measures are specifically designed to be scale-invariant and we conduct extensive experiments to validate our findings.},
	language = {en},
	urldate = {2021-09-09},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	month = apr,
	year = {2021},
}

@article{xu_towards_2021,
	title = {Towards the {Memorization} {Effect} of {Neural} {Networks} in {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2106.04794},
	abstract = {Recent studies suggest that ``memorization'' is one important factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: (a) Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose Benign Adversarial Training (BAT) which can facilitate adversarial training to avoid fitting ``harmful'' atypical samples and fit as more ``benign'' atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets such as CIFAR100 and Tiny{\textasciitilde}ImageNet.},
	urldate = {2021-09-09},
	journal = {arXiv:2106.04794 [cs]},
	author = {Xu, Han and Liu, Xiaorui and Wang, Wentao and Ding, Wenbiao and Wu, Zhongqin and Liu, Zitao and Jain, Anil and Tang, Jiliang},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.04794},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{han_co-teaching_2018,
	title = {Co-teaching: {Robust} training of deep neural networks with extremely noisy labels},
	volume = {31},
	shorttitle = {Co-teaching},
	url = {https://papers.nips.cc/paper/2018/hash/a19744e268754fb0148b017647355b7b-Abstract.html},
	urldate = {2021-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
	year = {2018},
}

@inproceedings{cao_generalization_2019,
	title = {Generalization {Bounds} of {Stochastic} {Gradient} {Descent} for {Wide} and {Deep} {Neural} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/cf9dc5e4e194fc21f397b4cac9cc3ae9-Abstract.html},
	urldate = {2021-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cao, Yuan and Gu, Quanquan},
	year = {2019},
}

@inproceedings{arora_stronger_2018,
	title = {Stronger {Generalization} {Bounds} for {Deep} {Nets} via a {Compression} {Approach}},
	url = {https://proceedings.mlr.press/v80/arora18b.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {254--263},
}

@inproceedings{arpit_closer_2017,
	title = {A {Closer} {Look} at {Memorization} in {Deep} {Networks}},
	url = {https://proceedings.mlr.press/v70/arpit17a.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arpit, Devansh and Jastrzębski, Stanisław and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {233--242},
}

@inproceedings{du_gradient_2019,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v97/du19c.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1675--1685},
}

@inproceedings{allen-zhu_convergence_2019,
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	url = {https://proceedings.mlr.press/v97/allen-zhu19a.html},
	language = {en},
	urldate = {2021-09-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {242--252},
}

@inproceedings{jiang_fantastic_2019,
	title = {Fantastic {Generalization} {Measures} and {Where} to {Find} {Them}},
	url = {https://openreview.net/forum?id=SJgIPJBFvH},
	abstract = {We empirically study generalization measures over more than 2000 models, identify common pitfall in existing practice of studying generalization measures and provide some new bounds based on...},
	language = {en},
	urldate = {2021-09-09},
	author = {Jiang*, Yiding and Neyshabur*, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	month = sep,
	year = {2019},
}

@inproceedings{neyshabur_exploring_2017,
	title = {Exploring {Generalization} in {Deep} {Learning}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/10ce03a1ed01077e3e289f3e53c72813-Abstract.html},
	urldate = {2021-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
	year = {2017},
}

@inproceedings{novak_sensitivity_2018,
	title = {Sensitivity and {Generalization} in {Neural} {Networks}: an {Empirical} {Study}},
	shorttitle = {Sensitivity and {Generalization} in {Neural} {Networks}},
	url = {https://openreview.net/forum?id=HJC2SzZCW},
	abstract = {We perform massive experimental studies characterizing the relationships between Jacobian norms, linear regions, and generalization.},
	language = {en},
	urldate = {2021-09-09},
	author = {Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	month = feb,
	year = {2018},
}

@inproceedings{yin_rademacher_2019,
	title = {Rademacher {Complexity} for {Adversarially} {Robust} {Generalization}},
	url = {https://proceedings.mlr.press/v97/yin19b.html},
	language = {en},
	urldate = {2021-09-08},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yin, Dong and Kannan, Ramchandran and Bartlett, Peter},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7085--7094},
}

@inproceedings{feldman_what_2020,
	title = {What {Neural} {Networks} {Memorize} and {Why}: {Discovering} the {Long} {Tail} via {Influence} {Estimation}},
	volume = {33},
	shorttitle = {What {Neural} {Networks} {Memorize} and {Why}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html},
	urldate = {2021-09-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Feldman, Vitaly and Zhang, Chiyuan},
	year = {2020},
	pages = {2881--2891},
}

@inproceedings{zhang_understanding_2017,
	title = {Understanding {Deep} {Learning} {Requires} {Rethinking} {Generalization}},
	abstract = {Despite their massive size, successful deep artiﬁcial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz},
	year = {2017},
	pages = {15},
}

@inproceedings{maennel_what_2020,
	title = {What {Do} {Neural} {Networks} {Learn} {When} {Trained} {With} {Random} {Labels}?},
	abstract = {We study deep neural networks (DNNs) trained on natural image data with entirely random labels. Despite its popularity in the literature, where it is often used to study memorization, generalization, and other phenomena, little is known about what DNNs learn in this setting. In this paper, we show analytically for convolutional and fully connected networks that an alignment between the principal components of network parameters and data takes place when training with random labels. We study this alignment effect by investigating neural networks pre-trained on randomly labelled image data and subsequently ﬁne-tuned on disjoint datasets with random or real labels. We show how this alignment produces a positive transfer: networks pre-trained with random labels train faster downstream compared to training from scratch even after accounting for simple effects, such as weight scaling. We analyze how competing effects, such as specialization at later layers, may hide the positive transfer. These effects are studied in several network architectures, including VGG16 and ResNet18, on CIFAR10 and ImageNet.},
	language = {en},
	booktitle = {Neural {Information} {Processing} {Systems}},
	author = {Maennel, Hartmut and Alabdulmohsin, Ibrahim and Tolstikhin, Ilya and Baldock, Robert J N and Bousquet, Olivier and Gelly, Sylvain and Keysers, Daniel},
	year = {2020},
	pages = {12},
}

@inproceedings{gao_convergence_2019,
	title = {Convergence of {Adversarial} {Training} in {Overparametrized} {Neural} {Networks}},
	abstract = {Neural networks are vulnerable to adversarial examples, i.e. inputs that are imperceptibly perturbed from natural data and yet incorrectly classiﬁed by the network. Adversarial training [31], a heuristic form of robust optimization that alternates between minimization and maximization steps, has proven to be among the most successful methods to train networks to be robust against a pre-deﬁned family of perturbations. This paper provides a partial answer to the success of adversarial training, by showing that it converges to a network where the surrogate loss with respect to the the attack algorithm is within of the optimal robust loss. Then we show that the optimal robust loss is also close to zero, hence adversarial training ﬁnds a robust classiﬁer. The analysis technique leverages recent work on the analysis of neural networks via Neural Tangent Kernel (NTK), combined with motivation from online-learning when the maximization is solved by a heuristic, and the expressiveness of the NTK kernel in the ∞-norm. In addition, we also prove that robust interpolation requires more model capacity, supporting the evidence that adversarial training requires wider networks.},
	language = {en},
	author = {Gao, Ruiqi and Cai, Tianle and Li, Haochuan and Hsieh, Cho-Jui and Wang, Liwei and Lee, Jason D},
	year = {2019},
	pages = {12},
}

@inproceedings{mao_metric_2019,
	title = {Metric {Learning} for {Adversarial} {Robustness}},
	abstract = {Deep networks are well-known to be fragile to adversarial attacks. We conduct an empirical analysis of deep representations under the state-of-the-art attack method called PGD, and ﬁnd that the attack causes the internal representation to shift closer to the “false” class. Motivated by this observation, we propose to regularize the representation space under attack with metric learning to produce more robust classiﬁers. By carefully sampling examples for metric learning, our learned representation not only increases robustness, but also detects previously unseen adversarial samples. Quantitative experiments show improvement of robustness accuracy by up to 4\% and detection efﬁciency by up to 6\% according to Area Under Curve score over prior work. The code of our work is available at https: //github.com/columbia/Metric\_Learning\_Adversarial\_Robustness.},
	language = {en},
	author = {Mao, Chengzhi and Zhong, Ziyuan and Yang, Junfeng and Vondrick, Carl and Ray, Baishakhi},
	year = {2019},
	pages = {12},
}

@inproceedings{tramer_adversarial_2019,
	title = {Adversarial {Training} and {Robustness} for {Multiple} {Perturbations}},
	abstract = {Defenses against adversarial examples, such as adversarial training, are typically tailored to a single perturbation type (e.g., small ∞-noise). For other perturbations, these defenses offer no guarantees and, at times, even increase the model’s vulnerability. Our aim is to understand the reasons underlying this robustness trade-off, and to train models that are simultaneously robust to multiple perturbation types.},
	language = {en},
	author = {Tramer, Florian and Boneh, Dan},
	year = {2019},
	pages = {11},
}

@inproceedings{xie_adversarial_2020,
	title = {Adversarial {Examples} {Improve} {Image} {Recognition}},
	abstract = {Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overﬁtting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfﬁcientNet-B7 [41] on ImageNet, we achieve signiﬁcant improvements on ImageNet (+0.7\%), ImageNet-C (+6.5\%), ImageNet-A (+7.0\%) and StylizedImageNet (+4.8\%). With an enhanced EfﬁcientNet-B8, our method achieves the state-of-the-art 85.5\% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [24] which is trained with 3.5B Instagram images (∼3000× more than ImageNet) and ∼9.4× more parameters. Models are available at https://github.com/tensorflow/tpu/tree/ master/models/official/efficientnet.},
	language = {en},
	author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L and Le, Quoc V},
	year = {2020},
	pages = {10},
}

@article{yu_understanding_2021,
	title = {Understanding {Generalization} in {Adversarial} {Training} via the {Bias}-{Variance} {Decomposition}},
	url = {https://arxiv.org/abs/2103.09947v2},
	abstract = {Adversarially trained models exhibit a large generalization gap: they can interpolate the training set even for large perturbation radii, but at the cost of large test error on clean samples. To investigate this gap, we decompose the test risk into its bias and variance components and study their behavior as a function of adversarial training perturbation radii (\${\textbackslash}varepsilon\$). We find that the bias increases monotonically with \${\textbackslash}varepsilon\$ and is the dominant term in the risk. Meanwhile, the variance is unimodal as a function of \${\textbackslash}varepsilon\$, peaking near the interpolation threshold for the training set. This characteristic behavior occurs robustly across different datasets and also for other robust training procedures such as randomized smoothing. It thus provides a test for proposed explanations of the generalization gap. We find that some existing explanations fail this test--for instance, by predicting a monotonically increasing variance curve. This underscores the power of bias-variance decompositions in modern settings-by providing two measurements instead of one, they can rule out more explanations than test accuracy alone. We also show that bias and variance can provide useful guidance for scalably reducing the generalization gap, highlighting pre-training and unlabeled data as promising routes.},
	language = {en},
	urldate = {2021-09-07},
	author = {Yu, Yaodong and Yang, Zitong and Dobriban, Edgar and Steinhardt, Jacob and Ma, Yi},
	month = mar,
	year = {2021},
}

@article{golgooni_zerograd_2021,
	title = {{ZeroGrad} : {Mitigating} and {Explaining} {Catastrophic} {Overfitting} in {FGSM} {Adversarial} {Training}},
	shorttitle = {{ZeroGrad}},
	url = {http://arxiv.org/abs/2103.15476},
	abstract = {Making deep neural networks robust to small adversarial noises has recently been sought in many applications. Adversarial training through iterative projected gradient descent (PGD) has been established as one of the mainstream ideas to achieve this goal. However, PGD is computationally demanding and often prohibitive in case of large datasets and models. For this reason, single-step PGD, also known as FGSM, has recently gained interest in the field. Unfortunately, FGSM-training leads to a phenomenon called ``catastrophic overfitting," which is a sudden drop in the adversarial accuracy under the PGD attack. In this paper, we support the idea that small input gradients play a key role in this phenomenon, and hence propose to zero the input gradient elements that are small for crafting FGSM attacks. Our proposed idea, while being simple and efficient, achieves competitive adversarial accuracy on various datasets.},
	urldate = {2021-09-07},
	journal = {arXiv:2103.15476 [cs]},
	author = {Golgooni, Zeinab and Saberi, Mehrdad and Eskandar, Masih and Rohban, Mohammad Hossein},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.15476},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{yu_interpreting_2018,
	title = {Interpreting {Adversarial} {Robustness}: {A} {View} from {Decision} {Surface} in {Input} {Space}},
	shorttitle = {Interpreting {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/1810.00144},
	abstract = {One popular hypothesis of neural network generalization is that the flat local minima of loss surface in parameter space leads to good generalization. However, we demonstrate that loss surface in parameter space has no obvious relationship with generalization, especially under adversarial settings. Through visualizing decision surfaces in both parameter space and input space, we instead show that the geometry property of decision surface in input space correlates well with the adversarial robustness. We then propose an adversarial robustness indicator, which can evaluate a neural network's intrinsic robustness property without testing its accuracy under adversarial attacks. Guided by it, we further propose our robust training method. Without involving adversarial training, our method could enhance network's intrinsic adversarial robustness against various adversarial attacks.},
	urldate = {2021-09-07},
	journal = {arXiv:1810.00144 [cs, stat]},
	author = {Yu, Fuxun and Liu, Chenchen and Wang, Yanzhi and Zhao, Liang and Chen, Xiang},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.00144},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{schmidt_adversarially_2018,
	title = {Adversarially {Robust} {Generalization} {Requires} {More} {Data}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/f708f064faaf32a43e4d3c784e6af9ea-Abstract.html},
	urldate = {2021-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and Madry, Aleksander},
	year = {2018},
}

@article{devries_improved_2017,
	title = {Improved {Regularization} of {Convolutional} {Neural} {Networks} with {Cutout}},
	url = {http://arxiv.org/abs/1708.04552},
	abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overﬁtting and therefore require proper regularization in order to generalize well.},
	language = {en},
	urldate = {2021-09-05},
	journal = {arXiv:1708.04552 [cs]},
	author = {DeVries, Terrance and Taylor, Graham W.},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.04552},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {https://openreview.net/forum?id=r1Ddp1-Rb&;noteId=r1Ddp1-Rb),},
	abstract = {Training on convex combinations between random training examples and their labels improves generalization in deep neural networks},
	language = {en},
	urldate = {2021-09-05},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = feb,
	year = {2018},
}

@inproceedings{dohmatob_generalized_2019,
	title = {Generalized {No} {Free} {Lunch} {Theorem} for {Adversarial} {Robustness}},
	url = {http://proceedings.mlr.press/v97/dohmatob19a.html},
	language = {en},
	urldate = {2021-08-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dohmatob, Elvis},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1646--1654},
}

@inproceedings{konam_understanding_2017,
	title = {Understanding {Convolutional} {Networks} with {APPLE} : {Automatic} {Patch} {Pattern} {Labeling} for {Explanation}},
	abstract = {With the success of deep learning, recent efforts have been focused on analyzing how learned networks make their classiﬁcations. We are interested in analyzing the network output based on the network structure and information ﬂow through the network layers. We contribute an algorithm for 1) analyzing a deep network to ﬁnd neurons that are “important" in terms of the network classiﬁcation outcome, and 2) automatically labeling the patches of the input image that activate these important neurons. We propose several measures of importance for neurons and demonstrate that our technique can be used to gain insight into, and explain how a network decomposes an image to make its ﬁnal classiﬁcation.},
	language = {en},
	booktitle = {{AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	author = {Konam, Sandeep and Quah, Ian and Rosenthal, Stephanie and Veloso, Manuela},
	year = {2017},
	pages = {7},
}

@inproceedings{yu_nisp_2018,
	title = {{NISP}: {Pruning} {Networks} {Using} {Neuron} {Importance} {Score} {Propagation}},
	shorttitle = {{NISP}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Yu_NISP_Pruning_Networks_CVPR_2018_paper.html},
	urldate = {2021-07-27},
	author = {Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I. and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S.},
	year = {2018},
	pages = {9194--9203},
}

@inproceedings{gan_semantic_2017,
	title = {Semantic {Compositional} {Networks} for {Visual} {Captioning}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.html},
	urldate = {2021-07-17},
	author = {Gan, Zhe and Gan, Chuang and He, Xiaodong and Pu, Yunchen and Tran, Kenneth and Gao, Jianfeng and Carin, Lawrence and Deng, Li},
	year = {2017},
	pages = {5630--5639},
}

@inproceedings{wang_semantic_2015,
	title = {Semantic {Part} {Segmentation} {Using} {Compositional} {Model} {Combining} {Shape} and {Appearance}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Wang_Semantic_Part_Segmentation_2015_CVPR_paper.html},
	urldate = {2021-07-17},
	author = {Wang, Jianyu and Yuille, Alan L.},
	year = {2015},
	pages = {1788--1797},
}

@article{salakhutdinov_learning_2013,
	title = {Learning with {Hierarchical}-{Deep} {Models}},
	volume = {35},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2012.269},
	abstract = {We introduce HD (or “Hierarchical-Deep”) models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian (HB) models. Specifically, we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training example by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Salakhutdinov, Ruslan and Tenenbaum, Joshua B. and Torralba, Antonio},
	month = aug,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Approximation methods, Bayesian methods, Computational modeling, Deep networks, Machine learning, Stochastic processes, Training, Vectors, deep Boltzmann machines, hierarchical Bayesian models, one-shot learning},
	pages = {1958--1971},
}

@inproceedings{wang_learning_2020,
	title = {Learning from {Explanations} with {Neural} {Execution} {Tree}},
	url = {https://openreview.net/forum?id=rJlUt0EYwS},
	abstract = {While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where...},
	language = {en},
	urldate = {2021-07-09},
	author = {Wang*, Ziqi and Qin*, Yujia and Zhou, Wenxuan and Yan, Jun and Ye, Qinyuan and Neves, Leonardo and Liu, Zhiyuan and Ren, Xiang},
	year = {2020},
}

@inproceedings{sagawa_distributionally_2020,
	title = {Distributionally {Robust} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=ryxGuJrFvS},
	abstract = {Overparameterized neural networks can be distributionally robust, but only when you account for generalization.},
	language = {en},
	urldate = {2021-07-01},
	author = {Sagawa*, Shiori and Koh*, Pang Wei and Hashimoto, Tatsunori B. and Liang, Percy},
	year = {2020},
}

@article{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/15-239.html},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.

The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages.

We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	number = {59},
	urldate = {2021-07-15},
	journal = {Journal of Machine Learning Research},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and March, Mario and Lempitsky, Victor},
	year = {2016},
	pages = {1--35},
}

@article{kubilius_deep_2016,
	title = {Deep {Neural} {Networks} as a {Computational} {Model} for {Human} {Shape} {Sensitivity}},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004896},
	doi = {10.1371/journal.pcbi.1004896},
	abstract = {Theories of object recognition agree that shape is of primordial importance, but there is no consensus about how shape might be represented, and so far attempts to implement a model of shape perception that would work with realistic stimuli have largely failed. Recent studies suggest that state-of-the-art convolutional ‘deep’ neural networks (DNNs) capture important aspects of human object perception. We hypothesized that these successes might be partially related to a human-like representation of object shape. Here we demonstrate that sensitivity for shape features, characteristic to human and primate vision, emerges in DNNs when trained for generic object recognition from natural photographs. We show that these models explain human shape judgments for several benchmark behavioral and neural stimulus sets on which earlier models mostly failed. In particular, although never explicitly trained for such stimuli, DNNs develop acute sensitivity to minute variations in shape and to non-accidental properties that have long been implicated to form the basis for object recognition. Even more strikingly, when tested with a challenging stimulus set in which shape and category membership are dissociated, the most complex model architectures capture human shape sensitivity as well as some aspects of the category structure that emerges from human judgments. As a whole, these results indicate that convolutional neural networks not only learn physically correct representations of object categories but also develop perceptually accurate representational spaces of shapes. An even more complete model of human object representations might be in sight by training deep architectures for multiple tasks, which is so characteristic in human development.},
	language = {en},
	number = {4},
	urldate = {2021-07-15},
	journal = {PLOS Computational Biology},
	author = {Kubilius, Jonas and Bracci, Stefania and Beeck, Hans P. Op de},
	month = apr,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Computer object recognition, Human performance, Monkeys, Neural networks, Sensory perception, Vision, Visual cortex, Visual object recognition},
	pages = {e1004896},
}

@inproceedings{ritter_cognitive_2017,
	title = {Cognitive {Psychology} for {Deep} {Neural} {Networks}: {A} {Shape} {Bias} {Case} {Study}},
	shorttitle = {Cognitive {Psychology} for {Deep} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v70/ritter17a.html},
	language = {en},
	urldate = {2021-07-15},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {2940--2949},
}

@article{bruce_saliency_2009,
	title = {Saliency, attention, and visual search: {An} information theoretic approach},
	volume = {9},
	issn = {1534-7362},
	shorttitle = {Saliency, attention, and visual search},
	url = {https://jov.arvojournals.org/article.aspx?articleid=2193531},
	doi = {10.1167/9.3.5},
	language = {en},
	number = {3},
	urldate = {2021-07-09},
	journal = {Journal of Vision},
	author = {Bruce, Neil D. B. and Tsotsos, John K.},
	month = mar,
	year = {2009},
	note = {Publisher: The Association for Research in Vision and Ophthalmology},
	pages = {5--5},
}

@inproceedings{bruce_saliency_2005,
	title = {Saliency {Based} on {Information} {Maximization}},
	abstract = {A model of bottom-up overt attention is proposed based on the principle of maximizing information sampled from a scene. The proposed operation is based on Shannon's self-information measure and is achieved in a neural circuit, which is demonstrated as having close ties with the circuitry existent in the primate visual cortex. It is further shown that the proposed saliency measure may be extended to address issues that currently elude explanation in the domain of saliency based models. Resu lts on natural images are compared with experimental eye tracking data revealing the efficacy of the model in predicting the deployment of overt attention as compared with existing efforts.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bruce, Neil and Tsotsos, John},
	year = {2005},
	pages = {8},
}

@inproceedings{carlucci_domain_2019,
	title = {Domain {Generalization} by {Solving} {Jigsaw} {Puzzles}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Carlucci_Domain_Generalization_by_Solving_Jigsaw_Puzzles_CVPR_2019_paper.html},
	urldate = {2021-07-09},
	author = {Carlucci, Fabio M. and D'Innocente, Antonio and Bucci, Silvia and Caputo, Barbara and Tommasi, Tatiana},
	year = {2019},
	pages = {2229--2238},
}

@inproceedings{radenovic_deep_2018,
	title = {Deep {Shape} {Matching}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Filip_Radenovic_Deep_Shape_Matching_ECCV_2018_paper.html},
	urldate = {2021-07-09},
	author = {Radenovic, Filip and Tolias, Giorgos and Chum, Ondrej},
	year = {2018},
	pages = {751--767},
}

@inproceedings{zhang_rationale-augmented_2016,
	address = {Austin, Texas},
	title = {Rationale-{Augmented} {Convolutional} {Neural} {Networks} for {Text} {Classification}},
	url = {https://www.aclweb.org/anthology/D16-1076},
	doi = {10.18653/v1/D16-1076},
	urldate = {2021-06-25},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Ye and Marshall, Iain and Wallace, Byron C.},
	month = nov,
	year = {2016},
	keywords = {nlp},
	pages = {795--804},
}

@inproceedings{lei_rationalizing_2016,
	address = {Austin, Texas},
	title = {Rationalizing {Neural} {Predictions}},
	url = {https://www.aclweb.org/anthology/D16-1011},
	doi = {10.18653/v1/D16-1011},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
	month = nov,
	year = {2016},
	keywords = {nlp},
	pages = {107--117},
}

@inproceedings{murty_expbert_2020,
	address = {Online},
	title = {{ExpBERT}: {Representation} {Engineering} with {Natural} {Language} {Explanations}},
	shorttitle = {{ExpBERT}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.190},
	doi = {10.18653/v1/2020.acl-main.190},
	abstract = {Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to “interpret” these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3–20x less labeled data and improves on the baseline by 3–10 F1 points with the same amount of labeled data.},
	urldate = {2021-06-09},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Murty, Shikhar and Koh, Pang Wei and Liang, Percy},
	month = jul,
	year = {2020},
	keywords = {nlp},
	pages = {2106--2113},
}

@inproceedings{camburu_e-snli_2018,
	title = {e-{SNLI}: {Natural} {Language} {Inference} with {Natural} {Language} {Explanations}},
	volume = {31},
	shorttitle = {e-{SNLI}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html},
	language = {en},
	urldate = {2021-07-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Camburu, Oana-Maria and Rocktäschel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
	year = {2018},
}

@inproceedings{rajani_explain_2019,
	address = {Florence, Italy},
	title = {Explain {Yourself}! {Leveraging} {Language} {Models} for {Commonsense} {Reasoning}},
	url = {https://aclanthology.org/P19-1487},
	doi = {10.18653/v1/P19-1487},
	abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
	month = jul,
	year = {2019},
	pages = {4932--4942},
}

@inproceedings{hancock_training_2018,
	address = {Melbourne, Australia},
	title = {Training {Classifiers} with {Natural} {Language} {Explanations}},
	url = {https://aclanthology.org/P18-1175},
	doi = {10.18653/v1/P18-1175},
	abstract = {Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hancock, Braden and Varma, Paroma and Wang, Stephanie and Bringmann, Martin and Liang, Percy and Ré, Christopher},
	month = jul,
	year = {2018},
	pages = {1884--1895},
}

@inproceedings{srivastava_joint_2017,
	address = {Copenhagen, Denmark},
	title = {Joint {Concept} {Learning} and {Semantic} {Parsing} from {Natural} {Language} {Explanations}},
	url = {https://aclanthology.org/D17-1161},
	doi = {10.18653/v1/D17-1161},
	abstract = {Natural language constitutes a predominant medium for much of human learning and pedagogy. We consider the problem of concept learning from natural language explanations, and a small number of labeled examples of the concept. For example, in learning the concept of a phishing email, one might say `this is a phishing email because it asks for your bank account number'. Solving this problem involves both learning to interpret open ended natural language statements, and learning the concept itself. We present a joint model for (1) language interpretation (semantic parsing) and (2) concept learning (classification) that does not require labeling statements with logical forms. Instead, the model prefers discriminative interpretations of statements in context of observable features of the data as a weak signal for parsing. On a dataset of email-related concepts, our approach yields across-the-board improvements in classification performance, with a 30\% relative improvement in F1 score over competitive methods in the low data regime.},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Srivastava, Shashank and Labutov, Igor and Mitchell, Tom},
	month = sep,
	year = {2017},
	pages = {1527--1536},
}

@inproceedings{dong_towards_2019,
	title = {Towards {Interpretable} {Deep} {Neural} {Networks} by {Leveraging} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1708.05493},
	abstract = {Deep neural networks (DNNs) have demonstrated impressive performance on a wide array of tasks, but they are usually considered opaque since internal structure and learned parameters are not interpretable. In this paper, we re-examine the internal representations of DNNs using adversarial images, which are generated by an ensembleoptimization algorithm. We ﬁnd that: (1) the neurons in DNNs do not truly detect semantic objects/parts, but respond to objects/parts only as recurrent discriminative patches; (2) deep visual representations are not robust distributed codes of visual concepts because the representations of adversarial images are largely not consistent with those of real images, although they have similar visual appearance, both of which are different from previous ﬁndings. To further improve the interpretability of DNNs, we propose an adversarial training scheme with a consistent loss such that the neurons are endowed with human-interpretable concepts. The induced interpretable representations enable us to trace eventual outcomes back to inﬂuential neurons. Therefore, human users can know how the models make predictions, as well as when and why they make errors.},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {{AAAI}-19 {Workshop} on {Network} {Interpretability} for {Deep} {Learning}},
	author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Bao, Fan},
	year = {2019},
	note = {arXiv: 1708.05493},
}

@inproceedings{chalasani_concise_2020,
	title = {Concise {Explanations} of {Neural} {Networks} using {Adversarial} {Training}},
	url = {http://proceedings.mlr.press/v119/chalasani20a.html},
	abstract = {We show new connections between adversarial learning and explainability for deep neural networks (DNNs). One form of explanation of the output of a neural network model in terms of its input featur...},
	language = {en},
	urldate = {2021-07-08},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chalasani, Prasad and Chen, Jiefeng and Chowdhury, Amrita Roy and Wu, Xi and Jha, Somesh},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1383--1391},
}

@article{franca_fast_2014,
	title = {Fast relational learning using bottom clause propositionalization with artificial neural networks},
	volume = {94},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-013-5392-1},
	doi = {10.1007/s10994-013-5392-1},
	abstract = {Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph mining and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integrated BCP with a well-known neural-symbolic system, C-IL2P, to perform learning from numerical vectors. C-IL2P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, handles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary results indicating that a reduction of more than 90 \% of features can be achieved with a small loss of accuracy.},
	language = {en},
	number = {1},
	urldate = {2021-07-08},
	journal = {Machine Learning},
	author = {França, Manoel V. M. and Zaverucha, Gerson and d’Avila Garcez, Artur S.},
	month = jan,
	year = {2014},
	pages = {81--104},
}

@inproceedings{kulkarni_deep_2015,
	title = {Deep {Convolutional} {Inverse} {Graphics} {Network}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/ced556cd9f9c0c8315cfbe0744a3baf0-Abstract.html},
	language = {en},
	urldate = {2021-07-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kulkarni, Tejas D. and Whitney, William F. and Kohli, Pushmeet and Tenenbaum, Josh},
	year = {2015},
}

@inproceedings{nguyen_deep_2015,
	title = {Deep {Neural} {Networks} {Are} {Easily} {Fooled}: {High} {Confidence} {Predictions} for {Unrecognizable} {Images}},
	shorttitle = {Deep {Neural} {Networks} {Are} {Easily} {Fooled}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Nguyen_Deep_Neural_Networks_2015_CVPR_paper.html},
	urldate = {2021-07-08},
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	year = {2015},
	pages = {427--436},
}

@inproceedings{bojarski_visualbackprop_2018,
	title = {{VisualBackProp}: {Efficient} {Visualization} of {CNNs} for {Autonomous} {Driving}},
	shorttitle = {{VisualBackProp}},
	doi = {10.1109/ICRA.2018.8461053},
	abstract = {This paper proposes a new method, that we call VisualBackProp, for visualizing which sets of pixels of the input image contribute most to the predictions made by the convolutional neural network (CNN). The method heavily hinges on exploring the intuition that the feature maps contain less and less irrelevant information to the prediction decision when moving deeper into the network. The technique we propose is dedicated for CNN-based systems for steering self-driving cars and is therefore required to run in real-time. This makes the proposed visualization method a valuable debugging tool which can be easily used during both training and inference. We justify our approach with theoretical arguments and confirm that the proposed method identifies sets of input pixels, rather than individual pixels, that collaboratively contribute to the prediction. We utilize the proposed visualization tool in the NVIDIA neural-network-based end-to-end learning system for autonomous driving, known as PilotNet. We demonstrate that VisualBackProp determines which elements in the road image most influence PilotNet's steering decision and indeed captures relevant objects on the road. The empirical evaluation furthermore shows the plausibility of the proposed approach on public road video data as well as in other applications and reveals that it compares favorably to the layer-wise relevance propagation approach, i.e. it obtains similar visualization results and achieves order of magnitude speed-ups.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Bojarski, Mariusz and Choromanska, Anna and Choromanski, Krzysztof and Firner, Bernhard and Ackel, Larry J and Muller, Urs and Yeres, Phil and Zieba, Karol},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Biological neural networks, Data visualization, Deconvolution, Neurons, Roads, Tools, Visualization},
	pages = {4701--4708},
}

@inproceedings{ferrari_convnets_2018,
	address = {Cham},
	title = {{ConvNets} and {ImageNet} {Beyond} {Accuracy}: {Understanding} {Mistakes} and {Uncovering} {Biases}},
	volume = {11210},
	isbn = {978-3-030-01230-4 978-3-030-01231-1},
	shorttitle = {{ConvNets} and {ImageNet} {Beyond} {Accuracy}},
	url = {http://link.springer.com/10.1007/978-3-030-01231-1_31},
	doi = {10.1007/978-3-030-01231-1_31},
	abstract = {ConvNets and ImageNet have driven the recent success of deep learning for image classiﬁcation. However, the marked slowdown in performance improvement combined with the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases question the reliability of these methods. This work investigates these questions from the perspective of the end-user by using human subject studies and explanations. The contribution of this study is threefold. We ﬁrst experimentally demonstrate that the accuracy and robustness of ConvNets measured on Imagenet are vastly underestimated. Next, we show that explanations can mitigate the impact of misclassiﬁed adversarial examples from the perspective of the end-user. We ﬁnally introduce a novel tool for uncovering the undesirable biases learned by a model. These contributions also show that explanations are a valuable tool both for improving our understanding of ConvNets’ predictions and for designing more reliable models.},
	language = {en},
	urldate = {2021-07-07},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Stock, Pierre and Cisse, Moustapha},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {504--519},
}

@inproceedings{zhao_men_2017,
	address = {Copenhagen, Denmark},
	title = {Men {Also} {Like} {Shopping}: {Reducing} {Gender} {Bias} {Amplification} using {Corpus}-level {Constraints}},
	shorttitle = {Men {Also} {Like} {Shopping}},
	url = {https://aclanthology.org/D17-1323},
	doi = {10.18653/v1/D17-1323},
	abstract = {Language is increasingly being used to de-fine rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33\% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68\% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5\% and 40.5\% for multilabel classification and visual semantic role labeling, respectively。},
	urldate = {2021-07-07},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	month = sep,
	year = {2017},
	pages = {2979--2989},
}

@inproceedings{wang_learning_2018,
	address = {New York, NY, USA},
	series = {{KDD} '18},
	title = {Learning {Credible} {Models}},
	isbn = {978-1-4503-5552-0},
	url = {https://doi.org/10.1145/3219819.3220070},
	doi = {10.1145/3219819.3220070},
	abstract = {In many settings, it is important that a model be capable of providing reasons for its predictions (ıe, the model must be interpretable). However, the model's reasoning may not conform with well-established knowledge. In such cases, while interpretable, the model lacks credibility. In this work, we formally define credibility in the linear setting and focus on techniques for learning models that are both accurate and credible. In particular, we propose a regularization penalty, expert yielded estimates (EYE), that incorporates expert knowledge about well-known relationships among covariates and the outcome of interest. We give both theoretical and empirical results comparing our proposed method to several other regularization techniques. Across a range of settings, experiments on both synthetic and real data show that models learned using the EYE penalty are significantly more credible than those learned using other penalties. Applied to two large-scale patient risk stratification task, our proposed technique results in a model whose top features overlap significantly with known clinical risk factors, while still achieving good predictive performance.},
	urldate = {2020-12-28},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Jiaxuan and Oh, Jeeheh and Wang, Haozhu and Wiens, Jenna},
	month = jul,
	year = {2018},
	keywords = {linear, logistic},
	pages = {2417--2426},
}

@inproceedings{bao_deriving_2018,
	address = {Brussels, Belgium},
	title = {Deriving {Machine} {Attention} from {Human} {Rationales}},
	url = {https://www.aclweb.org/anthology/D18-1216},
	doi = {10.18653/v1/D18-1216},
	abstract = {Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15\% average error reduction on benchmark datasets.},
	urldate = {2020-06-24},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bao, Yujia and Chang, Shiyu and Yu, Mo and Barzilay, Regina},
	month = oct,
	year = {2018},
	keywords = {nlp},
	pages = {1903--1913},
}

@inproceedings{chen_robust_2019,
	title = {Robust {Attribution} {Regularization}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/172ef5a94b4dd0aa120c6878fc29f70c-Abstract.html},
	language = {en},
	urldate = {2020-12-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Jiefeng and Wu, Xi and Rastogi, Vaibhav and Liang, Yingyu and Jha, Somesh},
	year = {2019},
	keywords = {image classification},
	pages = {14300--14310},
}

@inproceedings{barrett_sequence_2018,
	address = {Brussels, Belgium},
	title = {Sequence {Classification} with {Human} {Attention}},
	url = {https://www.aclweb.org/anthology/K18-1030},
	doi = {10.18653/v1/K18-1030},
	abstract = {Learning attention functions requires large volumes of data, but many NLP tasks simulate human behavior, and in this paper, we show that human attention really does provide a good inductive bias on many attention functions in NLP. Specifically, we use estimated human attention derived from eye-tracking corpora to regularize attention functions in recurrent neural networks. We show substantial improvements across a range of tasks, including sentiment analysis, grammatical error detection, and detection of abusive language.},
	urldate = {2020-12-28},
	booktitle = {Proceedings of the 22nd {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Barrett, Maria and Bingel, Joachim and Hollenstein, Nora and Rei, Marek and Søgaard, Anders},
	month = oct,
	year = {2018},
	keywords = {nlp},
	pages = {302--312},
}

@inproceedings{choi_why_2019,
	title = {Why {Can}'t {I} {Dance} in the {Mall}? {Learning} to {Mitigate} {Scene} {Bias} in {Action} {Recognition}},
	volume = {32},
	shorttitle = {Why {Can}'t {I} {Dance} in the {Mall}?},
	url = {https://papers.nips.cc/paper/2019/hash/ab817c9349cf9c4f6877e1894a1faa00-Abstract.html},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Choi, Jinwoo and Gao, Chen and Messou, Joseph C. E. and Huang, Jia-Bin},
	year = {2019},
	keywords = {video recognition},
	pages = {853--865},
}

@inproceedings{stone_teaching_2017,
	title = {Teaching {Compositionality} to {CNNs}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Stone_Teaching_Compositionality_to_CVPR_2017_paper.html},
	urldate = {2021-07-07},
	author = {Stone, Austin and Wang, Huayan and Stark, Michael and Liu, Yi and Scott Phoenix, D. and George, Dileep},
	year = {2017},
	pages = {5058--5067},
}

@inproceedings{liao_learning_2016,
	title = {Learning {Deep} {Parsimonious} {Representations}},
	abstract = {In this paper we aim at facilitating generalization for deep networks while supporting interpretability of the learned representations. Towards this goal, we propose a clustering based regularization that encourages parsimonious representations. Our k-means style objective is easy to optimize and ﬂexible, supporting various forms of clustering, such as sample clustering, spatial clustering, as well as co-clustering. We demonstrate the effectiveness of our approach on the tasks of unsupervised learning, classiﬁcation, ﬁne grained categorization, and zero-shot learning.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liao, Renjie and Schwing, Alex and Zemel, Richard and Urtasun, Raquel},
	year = {2016},
	pages = {9},
}

@inproceedings{dosovitskiy_inverting_2016,
	title = {Inverting {Visual} {Representations} {With} {Convolutional} {Networks}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Dosovitskiy_Inverting_Visual_Representations_CVPR_2016_paper.html},
	urldate = {2021-07-07},
	author = {Dosovitskiy, Alexey and Brox, Thomas},
	year = {2016},
	pages = {4829--4837},
}

@inproceedings{zhang_examining_2018,
	title = {Examining {CNN} {Representations} {With} {Respect} to {Dataset} {Bias}},
	volume = {32},
	copyright = {Copyright (c)},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11833},
	language = {en},
	urldate = {2021-07-07},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Quanshi and Wang, Wenguan and Zhu, Song-Chun},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Knowledge representation},
}

@inproceedings{li_resound_2018,
	title = {{RESOUND}: {Towards} {Action} {Recognition} without {Representation} {Bias}},
	shorttitle = {{RESOUND}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Yingwei_Li_RESOUND_Towards_Action_ECCV_2018_paper.html},
	urldate = {2021-07-07},
	author = {Li, Yingwei and Li, Yi and Vasconcelos, Nuno},
	year = {2018},
	pages = {513--528},
}

@article{deng_leveraging_2016,
	title = {Leveraging the {Wisdom} of the {Crowd} for {Fine}-{Grained} {Recognition}},
	volume = {38},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2015.2439285},
	abstract = {Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, fine-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of a stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called “Bubbles” that reveals discriminative features humans use. The player's goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions (“bubbles”), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the “BubbleBank” representation that uses the human selected bubbles to improve machine recognition performance. Finally, we demonstrate how to extend BubbleBank to a view-invariant 3D representation. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Deng, Jia and Krause, Jonathan and Stark, Michael and Fei-Fei, Li},
	month = apr,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Birds, Crowdsourcing, Detectors, Games, Gamification, Human Computation, Object Recognition, Object recognition, Pattern recognition, Three-dimensional displays, Visualization, human computation},
	pages = {666--676},
}

@inproceedings{linsley_what_2017,
	title = {What are the {Visual} {Features} {Underlying} {Human} {Versus} {Machine} {Vision}?},
	isbn = {978-1-5386-1034-3},
	url = {https://www.computer.org/csdl/proceedings-article/iccvw/2017/1034c706/12OmNCcKQEj},
	doi = {10.1109/ICCVW.2017.331},
	abstract = {Although Deep Convolutional Networks (DCNs) are approaching the accuracy of human observers at object recognition, it is unknown whether they leverage similar visual representations to achieve this performance. To address this, we introduce Clicktionary, a web-based game for identifying visual features used by human observers during object recognition. Importance maps derived from the game are consistent across participants and uncorrelated with image saliency measures. These results suggest that Clicktionary identifies image regions that are meaningful and diagnostic for object recognition but different than those driving eye movements. Surprisingly, Clicktionary importance maps are only weakly correlated with relevance maps derived from DCNs trained for object recognition. Our study demonstrates that the narrowing gap between the object recognition accuracy of human observers and DCNs obscures distinct visual strategies used by each to achieve this performance.},
	language = {English},
	urldate = {2021-07-06},
	publisher = {IEEE Computer Society},
	author = {Linsley, D. and Eberhardt, S. and Sharma, T. and Gupta, P. and Serre, T.},
	month = oct,
	year = {2017},
	note = {ISSN: 2473-9944},
	pages = {2706--2714},
}

@inproceedings{vondrick_learning_2015,
	title = {Learning visual biases from human imagination},
	volume = {28},
	url = {https://papers.nips.cc/paper/2015/hash/8f53295a73878494e9bc8dd6c3c7104f-Abstract.html},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Vondrick, Carl and Pirsiavash, Hamed and Oliva, Aude and Torralba, Antonio},
	year = {2015},
}

@inproceedings{du_learning_2019,
	title = {Learning {Credible} {Deep} {Neural} {Networks} with {Rationale} {Regularization}},
	doi = {10.1109/ICDM.2019.00025},
	abstract = {Recent explainability related studies have shown that state-of-the-art DNNs do not always adopt correct evidences to make decisions. It not only hampers their generalization but also makes them less likely to be trusted by end-users. In pursuit of developing more credible DNNs, in this paper we propose CREX, which encourages DNN models to focus more on evidences that actually matter for the task at hand, and to avoid overfitting to data-dependent bias and artifacts. Specifically, CREX regularizes the training process of DNNs with rationales, i.e., a subset of features highlighted by domain experts as justifications for predictions, to enforce DNNs to generate local explanations that conform with expert rationales. Even when rationales are not available, CREX still could be useful by requiring the generated explanations to be sparse. Experimental results on two text classification datasets demonstrate the increased credibility of DNNs trained with CREX. Comprehensive analysis further shows that while CREX does not always improve prediction accuracy on the held-out test set, it significantly increases DNN accuracy on new and previously unseen data beyond test set, highlighting the advantage of the increased credibility.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Du, M. and Liu, N. and Yang, F. and Hu, X.},
	month = nov,
	year = {2019},
	note = {ISSN: 2374-8486},
	keywords = {nlp},
	pages = {150--159},
}

@inproceedings{simpson_gradmask_2019,
	title = {{GradMask}: {Reduce} {Overfitting} by {Regularizing} {Saliency}},
	shorttitle = {{GradMask}},
	url = {https://openreview.net/forum?id=Syx2z2aMqE},
	abstract = {Regularizing saliency maps to prevent overfitting from incorrect feature attribution},
	language = {en},
	urldate = {2021-06-23},
	author = {Simpson, Becks and Dutil, Francis and Bengio, Yoshua and Cohen, Joseph Paul},
	month = apr,
	year = {2019},
	keywords = {image classification},
}

@inproceedings{toneva_interpreting_2019,
	title = {Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/749a8e6c231831ef7756db230b4359c8-Abstract.html},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Toneva, Mariya and Wehbe, Leila},
	year = {2019},
	keywords = {nlp},
	pages = {14954--14964},
}

@inproceedings{selvaraju_taking_2019,
	title = {Taking a {HINT}: {Leveraging} {Explanations} to {Make} {Vision} and {Language} {Models} {More} {Grounded}},
	shorttitle = {Taking a {HINT}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Selvaraju_Taking_a_HINT_Leveraging_Explanations_to_Make_Vision_and_Language_ICCV_2019_paper.html},
	urldate = {2020-12-28},
	author = {Selvaraju, Ramprasaath R. and Lee, Stefan and Shen, Yilin and Jin, Hongxia and Ghosh, Shalini and Heck, Larry and Batra, Dhruv and Parikh, Devi},
	year = {2019},
	keywords = {visual question answer},
	pages = {2591--2600},
}

@inproceedings{liu_incorporating_2019,
	address = {Florence, Italy},
	title = {Incorporating {Priors} with {Feature} {Attribution} on {Text} {Classification}},
	url = {https://www.aclweb.org/anthology/P19-1631},
	doi = {10.18653/v1/P19-1631},
	abstract = {Feature attribution methods, proposed recently, help users interpret the predictions of complex models. Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building. To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classiﬁers by neutralizing identity terms; (2) improving classiﬁer performance in a scarce data setting by forcing the model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-speciﬁc prior values to the objective. Our experiments show that i) a classiﬁer trained with our technique reduces undesired model biases without a tradeoff on the original task; ii) incorporating priors helps model performance in scarce data settings.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Frederick and Avci, Besim},
	year = {2019},
	keywords = {nlp},
	pages = {6274--6283},
}

@inproceedings{selvaraju_choose_2018,
	title = {Choose {Your} {Neuron}: {Incorporating} {Domain} {Knowledge} through {Neuron}-{Importance}},
	shorttitle = {Choose {Your} {Neuron}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Ramprasaath_Ramasamy_Selvaraju_Choose_Your_Neuron_ECCV_2018_paper.html},
	urldate = {2021-07-06},
	author = {Selvaraju, Ramprasaath R. and Chattopadhyay, Prithvijit and Elhoseiny, Mohamed and Sharma, Tilak and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	year = {2018},
	pages = {526--541},
}

@inproceedings{liu_attention_2017,
	title = {Attention {Correctness} in {Neural} {Image} {Captioning}},
	volume = {31},
	copyright = {Copyright (c)},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11197},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Chenxi and Mao, Junhua and Sha, Fei and Yuille, Alan},
	month = feb,
	year = {2017},
	note = {Number: 1},
}

@inproceedings{qiao_exploring_2018,
	title = {Exploring {Human}-{Like} {Attention} {Supervision} in {Visual} {Question} {Answering}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16485},
	abstract = {Attention mechanisms have been widely applied in the Visual Question Answering (VQA) task, as they help to focus on the area-of-interest of both visual and textual information. To answer the questions correctly, the model needs to selectively target different areas of an image, which suggests that an attention-based model may benefit from an explicit attention supervision. In this work, we aim to address the problem of adding attention supervision to VQA models. Since there is a lack of human attention data, we first propose a Human Attention Network (HAN) to generate human-like attention maps, training on a recently released dataset called Human ATtention Dataset (VQA-HAT). Then, we apply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the human-like attention maps for all image-question pairs. The generated human-like attention map dataset for the VQA v2.0 dataset is named as Human-Like ATtention (HLAT) dataset. Finally, we apply human-like attention supervision to an attention-based VQA model. The experiments show that adding human-like supervision yields a more accurate attention together with a better performance, showing a promising future for human-like attention supervision in VQA.},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Qiao, Tingting and Dong, Jianfeng and Xu, Duanqing},
	month = apr,
	year = {2018},
}

@article{zhang_top-down_2018,
	title = {Top-{Down} {Neural} {Attention} by {Excitation} {Backprop}},
	volume = {126},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-017-1059-x},
	doi = {10.1007/s11263-017-1059-x},
	abstract = {We aim to model the top-down attention of a convolutional neural network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. We show a theoretic connection between the proposed contrastive attention formulation and the Class Activation Map computation. Efficient implementation of Excitation Backprop for common neural network layers is also presented. In experiments, we visualize the evidence of a model’s classification decision by computing the proposed top-down attention maps. For quantitative evaluation, we report the accuracy of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images. Finally, we demonstrate applications of our method in model interpretation and data annotation assistance for facial expression analysis and medical imaging tasks.},
	language = {en},
	number = {10},
	urldate = {2021-07-06},
	journal = {International Journal of Computer Vision},
	author = {Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
	month = oct,
	year = {2018},
	pages = {1084--1102},
}

@inproceedings{goyal_making_2017,
	title = {Making the v in {VQA} {Matter}: {Elevating} the {Role} of {Image} {Understanding} in {Visual} {Question} {Answering}},
	shorttitle = {Making the v in {VQA} {Matter}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Goyal_Making_the_v_CVPR_2017_paper.html},
	urldate = {2021-07-06},
	author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
	year = {2017},
	pages = {6904--6913},
}

@inproceedings{agrawal_dont_2018,
	title = {Don't {Just} {Assume}; {Look} and {Answer}: {Overcoming} {Priors} for {Visual} {Question} {Answering}},
	shorttitle = {Don't {Just} {Assume}; {Look} and {Answer}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_Just_Assume_CVPR_2018_paper.html},
	urldate = {2021-07-06},
	author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
	year = {2018},
	pages = {4971--4980},
}

@article{kortylewski_compositional_2021,
	title = {Compositional {Convolutional} {Neural} {Networks}: {A} {Robust} and {Interpretable} {Model} for {Object} {Recognition} {Under} {Occlusion}},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {Compositional {Convolutional} {Neural} {Networks}},
	url = {https://doi.org/10.1007/s11263-020-01401-3},
	doi = {10.1007/s11263-020-01401-3},
	abstract = {Computer vision systems in real-world applications need to be robust to partial occlusion while also being explainable. In this work, we show that black-box deep convolutional neural networks (DCNNs) have only limited robustness to partial occlusion. We overcome these limitations by unifying DCNNs with part-based models into Compositional Convolutional Neural Networks (CompositionalNets)—an interpretable deep architecture with innate robustness to partial occlusion. Specifically, we propose to replace the fully connected classification head of DCNNs with a differentiable compositional model that can be trained end-to-end. The structure of the compositional model enables CompositionalNets to decompose images into objects and context, as well as to further decompose object representations in terms of individual parts and the objects’ pose. The generative nature of our compositional model enables it to localize occluders and to recognize objects based on their non-occluded parts. We conduct extensive experiments in terms of image classification and object detection on images of artificially occluded objects from the PASCAL3D+ and ImageNet dataset, and real images of partially occluded vehicles from the MS-COCO dataset. Our experiments show that CompositionalNets made from several popular DCNN backbones (VGG-16, ResNet50, ResNext) improve by a large margin over their non-compositional counterparts at classifying and detecting partially occluded objects. Furthermore, they can localize occluders accurately despite being trained with class-level supervision only. Finally, we demonstrate that CompositionalNets provide human interpretable predictions as their individual components can be understood as detecting parts and estimating an objects’ viewpoint.},
	language = {en},
	number = {3},
	urldate = {2021-07-03},
	journal = {International Journal of Computer Vision},
	author = {Kortylewski, Adam and Liu, Qing and Wang, Angtian and Sun, Yihong and Yuille, Alan},
	month = mar,
	year = {2021},
	pages = {736--760},
}

@inproceedings{zhang_mining_2017,
	title = {Mining {Object} {Parts} {From} {CNNs} via {Active} {Question}-{Answering}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Mining_Object_Parts_CVPR_2017_paper.html},
	urldate = {2021-07-03},
	author = {Zhang, Quanshi and Cao, Ruiming and Nian Wu, Ying and Zhu, Song-Chun},
	year = {2017},
	pages = {346--355},
}

@inproceedings{tsang_feature_2020,
	title = {Feature {Interaction} {Interpretability}: {A} {Case} for {Explaining} {Ad}-{Recommendation} {Systems} via {Neural} {Interaction} {Detection}},
	shorttitle = {Feature {Interaction} {Interpretability}},
	url = {https://openreview.net/forum?id=BkgnhTEtDS&fbclid=IwAR2apcGWGmr5dvVPZgLwz-BGXcQ4mQecsUrdSfXG1Rgo8sqoeDmnBZnIPa4},
	abstract = {Proposed methods to extract and leverage interpretations of feature interactions},
	language = {en},
	urldate = {2021-01-16},
	author = {Tsang, Michael and Cheng, Dehua and Liu, Hanpeng and Feng, Xue and Zhou, Eric and Liu, Yan},
	year = {2020},
	keywords = {recommendation},
}

@inproceedings{weinberger_learning_2020,
	title = {Learning {Deep} {Attribution} {Priors} {Based} {On} {Prior} {Knowledge}},
	volume = {33},
	url = {https://proceedings.neurips.cc//paper/2020/hash/a19883fca95d0e5ec7ee6c94c6c32028-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Weinberger, Ethan and Janizek, Joseph and Lee, Su-In},
	year = {2020},
	keywords = {image classification},
}

@inproceedings{wu_regional_2020,
	title = {Regional {Tree} {Regularization} for {Interpretability} in {Deep} {Neural} {Networks}},
	volume = {34},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/6112},
	doi = {10.1609/aaai.v34i04.6112},
	abstract = {The lack of interpretability remains a barrier to adopting deep neural networks across many safety-critical domains. Tree regularization was recently proposed to encourage a deep neural network’s decisions to resemble those of a globally compact, axis-aligned decision tree. However, it is often unreasonable to expect a single tree to predict well across all possible inputs. In practice, doing so could lead to neither interpretable nor performant optima. To address this issue, we propose regional tree regularization – a method that encourages a deep model to be well-approximated by several separate decision trees speciﬁc to predeﬁned regions of the input space. Across many datasets, including two healthcare applications, we show our approach delivers simpler explanations than other regularization schemes without compromising accuracy. Speciﬁcally, our regional regularizer ﬁnds many more “desirable” optima compared to global analogues.},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Wu, Mike and Parbhoo, Sonali and Hughes, Michael and Kindle, Ryan and Celi, Leo and Zazzi, Maurizio and Roth, Volker and Doshi-Velez, Finale},
	month = apr,
	year = {2020},
	keywords = {image classification},
	pages = {6413--6421},
}

@article{schramowski_making_2020,
	title = {Making deep neural networks right for the right scientific reasons by interacting with their explanations},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-0212-3},
	doi = {10.1038/s42256-020-0212-3},
	abstract = {Deep neural networks have demonstrated excellent performances in many real-world applications. Unfortunately, they may show Clever Hans-like behaviour (making use of confounding factors within datasets) to achieve high performance. In this work we introduce the novel learning setting of explanatory interactive learning and illustrate its benefits on a plant phenotyping research task. Explanatory interactive learning adds the scientist into the training loop, who interactively revises the original model by providing feedback on its explanations. Our experimental results demonstrate that explanatory interactive learning can help to avoid Clever Hans moments in machine learning and encourages (or discourages, if appropriate) trust in the underlying model.},
	language = {en},
	number = {8},
	urldate = {2020-12-28},
	journal = {Nature Machine Intelligence},
	author = {Schramowski, Patrick and Stammer, Wolfgang and Teso, Stefano and Brugger, Anna and Herbert, Franziska and Shao, Xiaoting and Luigs, Hans-Georg and Mahlein, Anne-Katrin and Kersting, Kristian},
	month = aug,
	year = {2020},
	note = {Number: 8
Publisher: Nature Publishing Group},
	keywords = {image classification},
	pages = {476--486},
}

@inproceedings{rieger_interpretations_2020,
	title = {Interpretations are {Useful}: {Penalizing} {Explanations} to {Align} {Neural} {Networks} with {Prior} {Knowledge}},
	shorttitle = {Interpretations are {Useful}},
	url = {http://proceedings.mlr.press/v119/rieger20a.html},
	abstract = {For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective. Too often, the litany o...},
	language = {en},
	urldate = {2020-12-24},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rieger, Laura and Singh, Chandan and Murdoch, William and Yu, Bin},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {image classification},
	pages = {8116--8126},
}

@inproceedings{sanh_learning_2021,
	title = {Learning from others' mistakes: {Avoiding} dataset biases without modeling them},
	shorttitle = {Learning from others' mistakes},
	url = {https://openreview.net/forum?id=Hf3qXoiNkR},
	abstract = {State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous...},
	language = {en},
	urldate = {2021-06-23},
	author = {Sanh, Victor and Wolf, Thomas and Belinkov, Yonatan and Rush, Alexander M.},
	year = {2021},
	keywords = {nlp},
}

@inproceedings{nuriel_permuted_2021,
	title = {Permuted {AdaIN}: {Reducing} the {Bias} {Towards} {Global} {Statistics} in {Image} {Classification}},
	shorttitle = {Permuted {AdaIN}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Nuriel_Permuted_AdaIN_Reducing_the_Bias_Towards_Global_Statistics_in_Image_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Nuriel, Oren and Benaim, Sagie and Wolf, Lior},
	year = {2021},
	keywords = {image classification},
	pages = {9482--9491},
}

@inproceedings{wang_learning_2019,
	title = {Learning {Robust} {Representations} by {Projecting} {Superficial} {Statistics} {Out}},
	url = {https://openreview.net/forum?id=rJEjjoR9K7},
	abstract = {Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training.},
	language = {en},
	urldate = {2021-07-03},
	author = {Wang, Haohan and He, Zexue and Lipton, Zachary C. and Xing, Eric P.},
	year = {2019},
}

@inproceedings{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	volume = {29},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	url = {https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y. and Saligrama, Venkatesh and Kalai, Adam T.},
	year = {2016},
}

@article{obermeyer_dissecting_2019,
	title = {Dissecting racial bias in an algorithm used to manage the health of populations},
	volume = {366},
	copyright = {Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/366/6464/447},
	doi = {10.1126/science.aax2342},
	abstract = {{\textless}p{\textgreater}Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.{\textless}/p{\textgreater}},
	language = {en},
	number = {6464},
	urldate = {2021-07-03},
	journal = {Science},
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	month = oct,
	year = {2019},
	pmid = {31649194},
	note = {Publisher: American Association for the Advancement of Science
Section: Research Article},
	pages = {447--453},
}

@article{winkler_association_2019,
	title = {Association {Between} {Surgical} {Skin} {Markings} in {Dermoscopic} {Images} and {Diagnostic} {Performance} of a {Deep} {Learning} {Convolutional} {Neural} {Network} for {Melanoma} {Recognition}},
	volume = {155},
	issn = {2168-6068},
	url = {https://doi.org/10.1001/jamadermatol.2019.1735},
	doi = {10.1001/jamadermatol.2019.1735},
	abstract = {Deep learning convolutional neural networks (CNNs) have shown a performance at the level of dermatologists in the diagnosis of melanoma. Accordingly, further exploring the potential limitations of CNN technology before broadly applying it is of special interest.To investigate the association between gentian violet surgical skin markings in dermoscopic images and the diagnostic performance of a CNN approved for use as a medical device in the European market.A cross-sectional analysis was conducted from August 1, 2018, to November 30, 2018, using a CNN architecture trained with more than 120 000 dermoscopic images of skin neoplasms and corresponding diagnoses. The association of gentian violet skin markings in dermoscopic images with the performance of the CNN was investigated in 3 image sets of 130 melanocytic lesions each (107 benign nevi, 23 melanomas).The same lesions were sequentially imaged with and without the application of a gentian violet surgical skin marker and then evaluated by the CNN for their probability of being a melanoma. In addition, the markings were removed by manually cropping the dermoscopic images to focus on the melanocytic lesion.Sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curve for the CNN’s diagnostic classification in unmarked, marked, and cropped images.In all, 130 melanocytic lesions (107 benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN achieved a sensitivity of 95.7\% (95\% CI, 79\%-99.2\%) and a specificity of 84.1\% (95\% CI, 76.0\%-89.8\%). The ROC AUC was 0.969. In marked lesions, an increase in melanoma probability scores was observed that resulted in a sensitivity of 100\% (95\% CI, 85.7\%-100\%) and a significantly reduced specificity of 45.8\% (95\% CI, 36.7\%-55.2\%, P \&lt; .001). The ROC AUC was 0.922. Cropping images led to the highest sensitivity of 100\% (95\% CI, 85.7\%-100\%), specificity of 97.2\% (95\% CI, 92.1\%-99.0\%), and ROC AUC of 0.993. Heat maps created by vanilla gradient descent backpropagation indicated that the blue markings were associated with the increased false-positive rate.This study’s findings suggest that skin markings significantly interfered with the CNN’s correct diagnosis of nevi by increasing the melanoma probability scores and consequently the false-positive rate. A predominance of skin markings in melanoma training images may have induced the CNN’s association of markings with a melanoma diagnosis. Accordingly, these findings suggest that skin markings should be avoided in dermoscopic images intended for analysis by a CNN.German Clinical Trial Register (DRKS) Identifier: DRKS00013570},
	number = {10},
	urldate = {2021-07-03},
	journal = {JAMA Dermatology},
	author = {Winkler, Julia K. and Fink, Christine and Toberer, Ferdinand and Enk, Alexander and Deinlein, Teresa and Hofmann-Wellenhof, Rainer and Thomas, Luc and Lallas, Aimilios and Blum, Andreas and Stolz, Wilhelm and Haenssle, Holger A.},
	month = oct,
	year = {2019},
	pages = {1135--1141},
}

@article{erion_improving_2021,
	title = {Improving performance of deep learning models with axiomatic attribution priors and expected gradients},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00343-w},
	doi = {10.1038/s42256-021-00343-w},
	abstract = {Recent research has demonstrated that feature attribution methods for deep networks can themselves be incorporated into training; these attribution priors optimize for a model whose attributions have certain desirable properties—most frequently, that particular features are important or unimportant. These attribution priors are often based on attribution methods that are not guaranteed to satisfy desirable interpretability axioms, such as completeness and implementation invariance. Here we introduce attribution priors to optimize for higher-level properties of explanations, such as smoothness and sparsity, enabled by a fast new attribution method formulation called expected gradients that satisfies many important interpretability axioms. This improves model performance on many real-world tasks where previous attribution priors fail. Our experiments show that the gains from combining higher-level attribution priors with expected gradients attributions are consistent across image, gene expression and healthcare datasets. We believe that this work motivates and provides the necessary tools to support the widespread adoption of axiomatic attribution priors in many areas of applied machine learning. The implementations and our results have been made freely available to academic communities.},
	language = {en},
	urldate = {2021-07-02},
	journal = {Nature Machine Intelligence},
	author = {Erion, Gabriel and Janizek, Joseph D. and Sturmfels, Pascal and Lundberg, Scott M. and Lee, Su-In},
	month = may,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computer science;Regulatory networks;Risk factors
Subject\_term\_id: computer-science;regulatory-networks;risk-factors},
	keywords = {image classification},
	pages = {1--12},
}

@inproceedings{ebrahimi_remembering_2021,
	title = {Remembering for the {Right} {Reasons}: {Explanations} {Reduce} {Catastrophic} {Forgetting}},
	shorttitle = {Remembering for the {Right} {Reasons}},
	url = {https://openreview.net/forum?id=tHgJoMfy6nI},
	abstract = {The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a...},
	language = {en},
	urldate = {2021-06-23},
	author = {Ebrahimi, Sayna and Petryk, Suzanne and Gokul, Akash and Gan, William and Gonzalez, Joseph E. and Rohrbach, Marcus and Darrell, Trevor},
	year = {2021},
	keywords = {continual learning, image classification},
}

@inproceedings{wang_learning_2019-1,
	title = {Learning {Robust} {Global} {Representations} by {Penalizing} {Local} {Predictive} {Power}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-Abstract.html},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Haohan and Ge, Songwei and Lipton, Zachary and Xing, Eric P.},
	year = {2019},
}

@inproceedings{shi_informative_2020,
	title = {Informative {Dropout} for {Robust} {Representation} {Learning}: {A} {Shape}-bias {Perspective}},
	shorttitle = {Informative {Dropout} for {Robust} {Representation} {Learning}},
	url = {http://proceedings.mlr.press/v119/shi20e.html},
	abstract = {Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN’s texture-bi...},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shi, Baifeng and Zhang, Dinghuai and Dai, Qi and Zhu, Zhanxing and Mu, Yadong and Wang, Jingdong},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {8828--8839},
}

@inproceedings{hermann_origins_2020,
	title = {The {Origins} and {Prevalence} of {Texture} {Bias} in {Convolutional} {Neural} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/db5f9f42a7157abe65bb145000b5871a-Abstract.html},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hermann, Katherine and Chen, Ting and Kornblith, Simon},
	year = {2020},
	pages = {19000--19015},
}

@inproceedings{gatys_texture_2015,
	title = {Texture {Synthesis} {Using} {Convolutional} {Neural} {Networks}},
	volume = {28},
	url = {https://proceedings.neurips.cc/paper/2015/hash/a5e00132373a7031000fd987a3c9f87b-Abstract.html},
	language = {en},
	urldate = {2021-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Gatys, Leon and Ecker, Alexander S. and Bethge, Matthias},
	year = {2015},
}

@inproceedings{stammer_right_2021,
	title = {Right for the {Right} {Concept}: {Revising} {Neuro}-{Symbolic} {Concepts} by {Interacting} {With} {Their} {Explanations}},
	shorttitle = {Right for the {Right} {Concept}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Stammer_Right_for_the_Right_Concept_Revising_Neuro-Symbolic_Concepts_by_Interacting_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Stammer, Wolfgang and Schramowski, Patrick and Kersting, Kristian},
	year = {2021},
	keywords = {image classification},
	pages = {3619--3629},
}

@inproceedings{chen_neural_2018,
	address = {Melbourne, Australia},
	title = {Neural {Natural} {Language} {Inference} {Models} {Enhanced} with {External} {Knowledge}},
	url = {https://aclanthology.org/P18-1224},
	doi = {10.18653/v1/P18-1224},
	abstract = {Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.},
	urldate = {2021-07-02},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Inkpen, Diana and Wei, Si},
	month = jul,
	year = {2018},
	pages = {2406--2417},
}

@inproceedings{madotto_mem2seq_2018,
	address = {Melbourne, Australia},
	title = {{Mem2Seq}: {Effectively} {Incorporating} {Knowledge} {Bases} into {End}-to-{End} {Task}-{Oriented} {Dialog} {Systems}},
	shorttitle = {{Mem2Seq}},
	url = {https://aclanthology.org/P18-1136},
	doi = {10.18653/v1/P18-1136},
	abstract = {End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.},
	urldate = {2021-07-02},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Madotto, Andrea and Wu, Chien-Sheng and Fung, Pascale},
	month = jul,
	year = {2018},
	pages = {1468--1478},
}

@inproceedings{mihaylov_knowledgeable_2018,
	address = {Melbourne, Australia},
	title = {Knowledgeable {Reader}: {Enhancing} {Cloze}-{Style} {Reading} {Comprehension} with {External} {Commonsense} {Knowledge}},
	shorttitle = {Knowledgeable {Reader}},
	url = {https://aclanthology.org/P18-1076},
	doi = {10.18653/v1/P18-1076},
	abstract = {We introduce a neural reading comprehension model that integrates external commonsense knowledge, encoded as a key-value memory, in a cloze-style setting. Instead of relying only on document-to-question interaction or discrete features as in prior work, our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer. This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text, but that is relevant for inferring the answer. Our model improves results over a very strong baseline on a hard Common Nouns dataset, making it a strong competitor of much more complex models. By including knowledge explicitly, our model can also provide evidence about the background knowledge used in the RC process.},
	urldate = {2021-07-02},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Mihaylov, Todor and Frank, Anette},
	month = jul,
	year = {2018},
	pages = {821--832},
}

@inproceedings{zhang_ernie_2019,
	address = {Florence, Italy},
	title = {{ERNIE}: {Enhanced} {Language} {Representation} with {Informative} {Entities}},
	shorttitle = {{ERNIE}},
	url = {https://aclanthology.org/P19-1139},
	doi = {10.18653/v1/P19-1139},
	abstract = {Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.},
	urldate = {2021-07-02},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
	month = jul,
	year = {2019},
	pages = {1441--1451},
}

@inproceedings{johnson_clevr_2017,
	address = {Honolulu, HI},
	title = {{CLEVR}: {A} {Diagnostic} {Dataset} for {Compositional} {Language} and {Elementary} {Visual} {Reasoning}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {{CLEVR}},
	url = {https://ieeexplore.ieee.org/document/8099698/},
	doi = {10.1109/CVPR.2017.215},
	abstract = {When building artiﬁcial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conﬂate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.},
	language = {en},
	urldate = {2021-07-02},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Zitnick, C. Lawrence and Girshick, Ross},
	month = jul,
	year = {2017},
	pages = {1988--1997},
}

@inproceedings{teso_explanatory_2019,
	address = {New York, NY, USA},
	series = {{AIES} '19},
	title = {Explanatory {Interactive} {Machine} {Learning}},
	isbn = {978-1-4503-6324-2},
	url = {https://doi.org/10.1145/3306618.3314293},
	doi = {10.1145/3306618.3314293},
	abstract = {Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.},
	urldate = {2021-07-01},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Teso, Stefano and Kersting, Kristian},
	month = jan,
	year = {2019},
	keywords = {active learning, explainable artificial intelligence, interpretability, machine learning},
	pages = {239--245},
}

@article{lapuschkin_unmasking_2019,
	title = {Unmasking {Clever} {Hans} predictors and assessing what machines really learn},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-08987-4},
	doi = {10.1038/s41467-019-08987-4},
	abstract = {Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.},
	language = {en},
	number = {1},
	urldate = {2021-07-01},
	journal = {Nature Communications},
	author = {Lapuschkin, Sebastian and Wäldchen, Stephan and Binder, Alexander and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
	month = mar,
	year = {2019},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Applied mathematics;Computer science;Machine learning
Subject\_term\_id: applied-mathematics;computer-science;machine-learning},
	pages = {1096},
}

@inproceedings{tartaglione_end_2021,
	title = {{EnD}: {Entangling} and {Disentangling} {Deep} {Representations} for {Bias} {Correction}},
	shorttitle = {{EnD}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Tartaglione_EnD_Entangling_and_Disentangling_Deep_Representations_for_Bias_Correction_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Tartaglione, Enzo and Barbano, Carlo Alberto and Grangetto, Marco},
	year = {2021},
	keywords = {image classification},
	pages = {13508--13517},
}

@inproceedings{cadene_rubi_2019,
	title = {{RUBi}: {Reducing} {Unimodal} {Biases} for {Visual} {Question} {Answering}},
	abstract = {Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classiﬁed without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by inﬂuencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is speciﬁcally designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training.},
	language = {en},
	author = {Cadene, Remi and Dancette, Corentin},
	year = {2019},
	pages = {12},
}

@inproceedings{viviano_saliency_2021,
	title = {Saliency is a {Possible} {Red} {Herring} {When} {Diagnosing} {Poor} {Generalization}},
	url = {https://openreview.net/forum?id=c9-WeM-ceB},
	abstract = {Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image...},
	language = {en},
	urldate = {2021-02-24},
	author = {Viviano, Joseph D. and Simpson, Becks and Dutil, Francis and Bengio, Yoshua and Cohen, Joseph Paul},
	year = {2021},
	keywords = {image classification},
}

@article{jo_measuring_2017,
	title = {Measuring the tendency of {CNNs} to {Learn} {Surface} {Statistical} {Regularities}},
	url = {http://arxiv.org/abs/1711.11561},
	abstract = {Deep CNNs are known to exhibit the following peculiarity: on the one hand they generalize extremely well to a test set, while on the other hand they are extremely sensitive to so-called adversarial perturbations. The extreme sensitivity of high performance CNNs to adversarial examples casts serious doubt that these networks are learning high level abstractions in the dataset. We are concerned with the following question: How can a deep CNN that does not learn any high level semantics of the dataset manage to generalize so well? The goal of this article is to measure the tendency of CNNs to learn surface statistical regularities of the dataset. To this end, we use Fourier ﬁltering to construct datasets which share the exact same high level abstractions but exhibit qualitatively different surface statistical regularities. For the SVHN and CIFAR-10 datasets, we present two Fourier ﬁltered variants: a low frequency variant and a randomly ﬁltered variant. Each of the Fourier ﬁltering schemes is tuned to preserve the recognizability of the objects. Our main ﬁnding is that CNNs exhibit a tendency to latch onto the Fourier image statistics of the training dataset, sometimes exhibiting up to a 28\% generalization gap across the various test sets. Moreover, we observe that signiﬁcantly increasing the depth of a network has a very marginal impact on closing the aforementioned generalization gap. Thus we provide quantitative evidence supporting the hypothesis that deep CNNs tend to learn surface statistical regularities in the dataset rather than higher-level abstract concepts.},
	language = {en},
	urldate = {2021-07-01},
	journal = {arXiv:1711.11561 [cs, stat]},
	author = {Jo, Jason and Bengio, Yoshua},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11561},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{kim_why_2019,
	title = {Why are {Saliency} {Maps} {Noisy}? {Cause} of and {Solution} to {Noisy} {Saliency} {Maps}},
	shorttitle = {Why are {Saliency} {Maps} {Noisy}?},
	doi = {10.1109/ICCVW.2019.00510},
	abstract = {Saliency Map, the gradient of the score function with respect to the input, is the most basic technique for interpreting deep neural network decisions. However, saliency maps are often visually noisy. Although several hypotheses were proposed to account for this phenomenon, there are few works that provide rigorous analyses of noisy saliency maps. In this paper, we first propose a new hypothesis that noise may occur in saliency maps when irrelevant features pass through ReLU activation functions. Then, we propose Rectified Gradient, a method that alleviates this problem through layer-wise thresholding during backpropagation. Experiments with neural networks trained on CIFAR-10 and ImageNet showed effectiveness of our method and its superiority to other attribution methods.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	author = {Kim, Beomsu and Seo, Junghoon and Jeon, Seunghyeon and Koo, Jamyoung and Choe, Jeongyeol and Jeon, Taegyun},
	month = oct,
	year = {2019},
	note = {ISSN: 2473-9944},
	keywords = {Attribution-Map, Attribution-Method, Backpropagation, Deconvolution, Heating systems, Interpretability, Neural networks, Noise measurement, Training, Visualization},
	pages = {4149--4157},
}

@inproceedings{zhuang_care_2019,
	title = {{CARE}: {Class} {Attention} to {Regions} of {Lesion} for {Classification} on {Imbalanced} {Data}},
	shorttitle = {{CARE}},
	url = {http://proceedings.mlr.press/v102/zhuang19a.html},
	abstract = {To date, it is still an open and challenging problem for intelligent diagnosis systems to effectively learn from imbalanced data, especially with large samples of common diseases and much smaller s...},
	language = {en},
	urldate = {2021-07-01},
	booktitle = {International {Conference} on {Medical} {Imaging} with {Deep} {Learning}},
	publisher = {PMLR},
	author = {Zhuang, Jiaxin and Cai, Jiabin and Wang, Ruixuan and Zhang, Jianguo and Zheng, Weishi},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {588--597},
}

@inproceedings{wang_removing_2021,
	title = {Removing the {Background} by {Adding} the {Background}: {Towards} {Background} {Robust} {Self}-{Supervised} {Video} {Representation} {Learning}},
	shorttitle = {Removing the {Background} by {Adding} the {Background}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Removing_the_Background_by_Adding_the_Background_Towards_Background_Robust_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Wang, Jinpeng and Gao, Yuting and Li, Ke and Lin, Yiqi and Ma, Andy J. and Cheng, Hao and Peng, Pai and Huang, Feiyue and Ji, Rongrong and Sun, Xing},
	year = {2021},
	keywords = {video recognition},
	pages = {11804--11813},
}

@inproceedings{zhang_explicit_2021,
	title = {Explicit {Knowledge} {Incorporation} for {Visual} {Reasoning}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Explicit_Knowledge_Incorporation_for_Visual_Reasoning_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, Yifeng and Jiang, Ming and Zhao, Qi},
	year = {2021},
	keywords = {visual reasoning},
	pages = {1356--1365},
}

@inproceedings{zunino_explainable_2021,
	title = {Explainable {Deep} {Classification} {Models} for {Domain} {Generalization}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Zunino_Explainable_Deep_Classification_Models_for_Domain_Generalization_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	author = {Zunino, Andrea and Bargal, Sarah Adel and Volpi, Riccardo and Sameki, Mehrnoosh and Zhang, Jianming and Sclaroff, Stan and Murino, Vittorio and Saenko, Kate},
	year = {2021},
	keywords = {image classification},
	pages = {3233--3242},
}

@inproceedings{geirhos_imagenet-trained_2019,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {http://arxiv.org/abs/1811.12231},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conﬂicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conﬂict. We show that ImageNettrained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classiﬁcation strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on ‘StylizedImageNet’, a stylized version of ImageNet. This provides a much better ﬁt for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent beneﬁts such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	year = {2019},
}

@inproceedings{li_deep_2018,
	title = {Deep {Learning} for {Case}-{Based} {Reasoning} {Through} {Prototypes}: {A} {Neural} {Network} {That} {Explains} {Its} {Predictions}},
	volume = {32},
	copyright = {Copyright (c)},
	shorttitle = {Deep {Learning} for {Case}-{Based} {Reasoning} {Through} {Prototypes}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11771},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
	month = apr,
	year = {2018},
	note = {Number: 1},
}

@inproceedings{marcos_semantically_2019,
	title = {Semantically {Interpretable} {Activation} {Maps}: what-where-how explanations within {CNNs}},
	shorttitle = {Semantically {Interpretable} {Activation} {Maps}},
	doi = {10.1109/ICCVW.2019.00518},
	abstract = {A main issue preventing the use of Convolutional Neural Networks (CNN) in end user applications is the low level of transparency in the decision process. Previous work on CNN interpretability has mostly focused either on localizing the regions of the image that contribute to the result or on building an external model that generates plausible explanations. However, the former does not provide any semantic information and the latter does not guarantee the faithfulness of the explanation. We propose an intermediate representation composed of multiple Semantically Interpretable Activation Maps (SIAM) indicating the presence of predefined attributes at different locations of the image. These attribute maps are then linearly combined to produce the final output. This gives the user insight into what the model has seen, where, and a final output directly linked to this information in a comprehensive and interpretable way. We test the method on the task of landscape scenicness (aesthetic value) estimation, using an intermediate representation of 33 attributes from the SUN Attributes database. The results confirm that SIAM makes it possible to understand what attributes in the image are contributing to the final score and where they are located. Since it is based on learning from multiple tasks and datasets, SIAM improve the explanability of the prediction without additional annotation efforts or computational overhead at inference time, while keeping good performances on both the final and intermediate tasks.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	author = {Marcos, D. and Lobry, S. and Tuia, D.},
	month = oct,
	year = {2019},
	note = {ISSN: 2473-9944},
	pages = {4207--4215},
}

@inproceedings{wu_beyond_2017,
	title = {Beyond {Sparsity}: {Tree} {Regularization} of {Deep} {Models} for {Interpretability}},
	shorttitle = {Beyond {Sparsity}},
	url = {http://arxiv.org/abs/1711.06178},
	abstract = {The lack of interpretability remains a key barrier to the adoption of deep models in many applications. In this work, we explicitly regularize deep models so human users might step through the process behind their predictions in little time. Specifically, we train deep time-series models so their class-probability predictions have high accuracy while being closely modeled by decision trees with few nodes. Using intuitive toy examples as well as medical tasks for treating sepsis and HIV, we demonstrate that this new tree regularization yields models that are easier for humans to simulate than simpler L1 or L2 penalties without sacrificing predictive power.},
	urldate = {2020-12-25},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Wu, Mike and Hughes, Michael C. and Parbhoo, Sonali and Zazzi, Maurizio and Roth, Volker and Doshi-Velez, Finale},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.06178},
}

@article{mitsuhara_embedding_2019,
	title = {Embedding {Human} {Knowledge} into {Deep} {Neural} {Network} via {Attention} {Map}},
	url = {http://arxiv.org/abs/1905.03540},
	abstract = {In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a ﬁne-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our ﬁne-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the ﬁne-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classiﬁcation performance. Our ﬁndings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1905.03540 [cs]},
	author = {Mitsuhara, Masahiro and Fukui, Hiroshi and Sakashita, Yusuke and Ogata, Takanori and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
	month = dec,
	year = {2019},
	note = {arXiv: 1905.03540},
}

@inproceedings{jiang_learning_2017,
	title = {Learning {Discriminative} {Features} via {Label} {Consistent} {Neural} {Network}},
	doi = {10.1109/WACV.2017.30},
	abstract = {Deep Convolutional Neural Networks (CNN) enforce supervised information only at the output layer, and hidden layers are trained by back propagating the prediction error from the output layer without explicit supervision. We propose a supervised feature learning approach, Label Consistent Neural Network, which enforces direct supervision in late hidden layers in a novel way. We associate each neuron in a hidden layer with a particular class label and encourage it to be activated for input signals from the same class. More specifically, we introduce a label consistency regularization called "discriminative representation error" loss for late hidden layers and combine it with classification error loss to build our overall objective function. This label consistency constraint alleviates the common problem of gradient vanishing and tends to faster convergence, it also makes the features derived from late hidden layers discriminative enough for classification even using a simple k-NN classifier. Experimental results demonstrate that our approach achieves state-of-the-art performances on several public datasets for action and object category recognition.},
	booktitle = {2017 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Jiang, Z. and Wang, Y. and Davis, L. and Andrews, W. and Rozgic, V.},
	month = mar,
	year = {2017},
	pages = {207--216},
}

@article{zhang_interpretable_2020,
	title = {Interpretable {CNNs} for {Object} {Classification}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.2982882},
	abstract = {This paper proposes a generic method to learn interpretable convolutional filters in a deep convolutional neural network (CNN) for object classification, where each interpretable filter encodes features of a specific object part. Our method does not require additional annotations of object parts or textures for supervision. Instead, we use the same training data as traditional CNNs. Our method automatically assigns each interpretable filter in a high conv-layer with an object part of a certain category during the learning process. Such explicit knowledge representations in conv-layers of CNN help people clarify the logic encoded in the CNN, i.e., answering what patterns the CNN extracts from an input image and uses for prediction. We have tested our method using different benchmark CNNs with various structures to demonstrate the broad applicability of our method. Experiments have shown that our interpretable filters are much more semantically meaningful than traditional filters.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, Q. and Wang, X. and Wu, Y. N. and Zhou, H. and Zhu, S.-C.},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {1--1},
}

@inproceedings{zeiler_adaptive_2011,
	title = {Adaptive deconvolutional networks for mid and high level feature learning},
	doi = {10.1109/ICCV.2011.6126474},
	abstract = {We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	author = {Zeiler, Matthew D. and Taylor, Graham W. and Fergus, Rob},
	month = nov,
	year = {2011},
	note = {ISSN: 2380-7504},
	pages = {2018--2025},
}

@inproceedings{abbasi-asl_interpreting_2017,
	title = {Interpreting {Convolutional} {Neural} {Networks} {Through} {Compression}},
	url = {http://arxiv.org/abs/1711.02329},
	abstract = {Convolutional neural networks (CNNs) achieve state-of-the-art performance in a wide variety of tasks in computer vision. However, interpreting CNNs still remains a challenge. This is mainly due to the large number of parameters in these networks. Here, we investigate the role of compression and particularly pruning filters in the interpretation of CNNs. We exploit our recently-proposed greedy structural compression scheme that prunes filters in a trained CNN. In our compression, the filter importance index is defined as the classification accuracy reduction (CAR) of the network after pruning that filter. The filters are then iteratively pruned based on the CAR index. We demonstrate the interpretability of CAR-compressed CNNs by showing that our algorithm prunes filters with visually redundant pattern selectivity. Specifically, we show the importance of shape-selective filters for object recognition, as opposed to color-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of them in the first layer and 14 of them in the second layer are color-selective filters. Finally, we introduce a variant of our CAR importance index that quantifies the importance of each image class to each CNN filter. We show that the most and the least important class labels present a meaningful interpretation of each filter that is consistent with the visualized pattern selectivity of that filter.},
	urldate = {2021-01-16},
	booktitle = {{NeurIPS} 2017 {Symposium} on {Interpretable} {Machine} {Learning}},
	author = {Abbasi-Asl, Reza and Yu, Bin},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.02329},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{linsley_learning_2019,
	title = {Learning what and where to attend},
	url = {https://openreview.net/forum?id=BJgLg3R9KQ},
	abstract = {A large-scale dataset for training attention models for object recognition leads to more accurate, interpretable, and human-like object recognition.},
	language = {en},
	urldate = {2021-01-16},
	author = {Linsley, Drew and Shiebler, Dan and Eberhardt, Sven and Serre, Thomas},
	year = {2019},
}

@inproceedings{singh_hierarchical_2019,
	title = {Hierarchical interpretations for neural network predictions},
	url = {https://openreview.net/forum?id=SkEqro0ctQ},
	abstract = {We introduce and validate hierarchical local interpretations, the first technique to automatically search for and display important interactions for individual predictions made by LSTMs and CNNs.},
	language = {en},
	urldate = {2021-01-16},
	author = {Singh, Chandan and Murdoch, W. James and Yu, Bin},
	year = {2019},
}

@inproceedings{gu_understanding_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Understanding {Individual} {Decisions} of {CNNs} via {Contrastive} {Backpropagation}},
	isbn = {978-3-030-20893-6},
	doi = {10.1007/978-3-030-20893-6_8},
	abstract = {A number of backpropagation-based approaches such as DeConvNets, vanilla Gradient Visualization and Guided Backpropagation have been proposed to better understand individual decisions of deep convolutional neural networks. The saliency maps produced by them are proven to be non-discriminative. Recently, the Layer-wise Relevance Propagation (LRP) approach was proposed to explain the classification decisions of rectifier neural networks. In this work, we evaluate the discriminativeness of the generated explanations and analyze the theoretical foundation of LRP, i.e. Deep Taylor Decomposition. The experiments and analysis conclude that the explanations generated by LRP are not class-discriminative. Based on LRP, we propose Contrastive Layer-wise Relevance Propagation (CLRP), which is capable of producing instance-specific, class-discriminative, pixel-wise explanations. In the experiments, we use the CLRP to explain the decisions and understand the difference between neurons in individual classification decisions. We also evaluate the explanations quantitatively with a Pointing Game and an ablation study. Both qualitative and quantitative evaluations show that the CLRP generates better explanations than the LRP.},
	language = {en},
	booktitle = {Asian {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Gu, Jindong and Yang, Yinchong and Tresp, Volker},
	editor = {Jawahar, C. V. and Li, Hongdong and Mori, Greg and Schindler, Konrad},
	year = {2019},
	keywords = {Discriminative saliency maps, Explainable deep learning, LRP},
	pages = {119--134},
}

@inproceedings{ross_evaluating_2021,
	address = {New York, NY, USA},
	title = {Evaluating the {Interpretability} of {Generative} {Models} by {Interactive} {Reconstruction}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445296},
	abstract = {For machine learning models to be most useful in numerous sociotechnical systems, many have argued that they must be human-interpretable. However, despite increasing interest in interpretability, there remains no firm consensus on how to measure it. This is especially true in representation learning, where interpretability research has focused on “disentanglement” measures only applicable to synthetic datasets and not grounded in human factors. We introduce a task to quantify the human-interpretability of generative model representations, where users interactively modify representations to reconstruct target instances. On synthetic datasets, we find performance on this task much more reliably differentiates entangled and disentangled models than baseline approaches. On a real dataset, we find it differentiates between representation learning methods widely believed but never shown to produce more or less interpretable models. In both cases, we ran small-scale think-aloud studies and large-scale experiments on Amazon Mechanical Turk to confirm that our qualitative and quantitative results agreed.},
	urldate = {2021-06-25},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ross, Andrew and Chen, Nina and Hang, Elisa Zhao and Glassman, Elena L. and Doshi-Velez, Finale},
	month = may,
	year = {2021},
	keywords = {evaluation methods, interpretability, representation learning},
	pages = {1--15},
}

@inproceedings{ibrahim_global_2019,
	address = {New York, NY, USA},
	series = {{AIES} '19},
	title = {Global {Explanations} of {Neural} {Networks}: {Mapping} the {Landscape} of {Predictions}},
	isbn = {978-1-4503-6324-2},
	shorttitle = {Global {Explanations} of {Neural} {Networks}},
	url = {https://doi.org/10.1145/3306618.3314230},
	doi = {10.1145/3306618.3314230},
	abstract = {A barrier to the wider adoption of neural networks is their lack of interpretability. While local explanation methods exist for one prediction, most global attributions still reduce neural network decisions to a single set of features. In response, we present an approach for generating global attributions called GAM, which explains the landscape of neural network predictions across subpopulations. GAM augments global explanations with the proportion of samples that each attribution best explains and specifies which samples are described by each attribution. Global explanations also have tunable granularity to detect more or fewer subpopulations. We demonstrate that GAM's global explanations 1) yield the known feature importances of simulated data, 2) match feature weights of interpretable statistical models on real data, and 3) are intuitive to practitioners through user studies. With more transparent predictions, GAM can help ensure neural network decisions are generated for the right reasons.},
	urldate = {2021-06-25},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Ibrahim, Mark and Louie, Melissa and Modarres, Ceena and Paisley, John},
	month = jan,
	year = {2019},
	keywords = {explainable deep learning, global interpretability, neural networks},
	pages = {279--287},
}

@article{moraffah_causal_2020,
	title = {Causal {Interpretability} for {Machine} {Learning} - {Problems}, {Methods} and {Evaluation}},
	volume = {22},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/3400051.3400058},
	doi = {10.1145/3400051.3400058},
	abstract = {Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more humanfriendly explanations, recent work on interpretability tries to answer questions related to causality such as "Why does this model makes such decisions?" or "Was it a specific feature that caused the decision made by the model?". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable models from the aspects of the problems and methods. In addition, this survey provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.},
	number = {1},
	urldate = {2020-12-21},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Moraffah, Raha and Karami, Mansooreh and Guo, Ruocheng and Raglin, Adrienne and Liu, Huan},
	month = may,
	year = {2020},
	keywords = {causal inference, counterfactuals, explainability, interpratablity, machine learning},
	pages = {18--33},
}

@article{patterson_sun_2014,
	title = {The {SUN} {Attribute} {Database}: {Beyond} {Categories} for {Deeper} {Scene} {Understanding}},
	volume = {108},
	issn = {1573-1405},
	shorttitle = {The {SUN} {Attribute} {Database}},
	url = {https://doi.org/10.1007/s11263-013-0695-z},
	doi = {10.1007/s11263-013-0695-z},
	abstract = {In this paper we present the first large-scale scene attribute database. First, we perform crowdsourced human studies to find a taxonomy of 102 discriminative attributes. We discover attributes related to materials, surface properties, lighting, affordances, and spatial layout. Next, we build the “SUN attribute database” on top of the diverse SUN categorical database. We use crowdsourcing to annotate attributes for 14,340 images from 707 scene categories. We perform numerous experiments to study the interplay between scene attributes and scene categories. We train and evaluate attribute classifiers and then study the feasibility of attributes as an intermediate scene representation for scene classification, zero shot learning, automatic image captioning, semantic image search, and parsing natural images. We show that when used as features for these tasks, low dimensional scene attributes can compete with or improve on the state of the art performance. The experiments suggest that scene attributes are an effective low-dimensional feature for capturing high-level context and semantics in scenes.},
	language = {en},
	number = {1},
	urldate = {2021-06-25},
	journal = {International Journal of Computer Vision},
	author = {Patterson, Genevieve and Xu, Chen and Su, Hang and Hays, James},
	month = may,
	year = {2014},
	pages = {59--81},
}

@inproceedings{marcos_contextual_2020,
	title = {Contextual {Semantic} {Interpretability}},
	url = {https://openaccess.thecvf.com/content/ACCV2020/html/Marcos_Contextual_Semantic_Interpretability_ACCV_2020_paper.html},
	language = {en},
	urldate = {2021-06-25},
	author = {Marcos, Diego and Fong, Ruth and Lobry, Sylvain and Flamary, Remi and Courty, Nicolas and Tuia, Devis},
	year = {2020},
}

@inproceedings{jin_towards_2020,
	title = {Towards {Hierarchical} {Importance} {Attribution}: {Explaining} {Compositional} {Semantics} for {Neural} {Sequence} {Models}},
	shorttitle = {Towards {Hierarchical} {Importance} {Attribution}},
	url = {https://openreview.net/forum?id=BkxRRkSKwr},
	abstract = {We propose measurement of phrase importance and algorithms for hierarchical explanation of neural sequence model predictions},
	language = {en},
	urldate = {2021-01-16},
	author = {Jin, Xisen and Wei, Zhongyu and Du, Junyi and Xue, Xiangyang and Ren, Xiang},
	year = {2020},
}

@inproceedings{yang_learn_2020,
	title = {Learn to {Explain} {Efficiently} via {Neural} {Logic} {Inductive} {Learning}},
	url = {https://openreview.net/forum?id=SJlh8CEYDB},
	abstract = {An efficient differentiable ILP model that learns first-order logic rules that can explain the data.},
	language = {en},
	urldate = {2021-01-16},
	author = {Yang, Yuan and Song, Le},
	year = {2020},
}

@inproceedings{chen_explaining_2019,
	title = {Explaining {Neural} {Networks} {Semantically} and {Quantitatively}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Explaining_Neural_Networks_Semantically_and_Quantitatively_ICCV_2019_paper.html},
	urldate = {2021-06-25},
	author = {Chen, Runjin and Chen, Hao and Ren, Jie and Huang, Ge and Zhang, Quanshi},
	year = {2019},
	pages = {9187--9196},
}

@inproceedings{barcelo_model_2020,
	title = {Model {Interpretability} through the lens of {Computational} {Complexity}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/b1adda14824f50ef24ff1c05bb66faf3-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Barceló, Pablo and Monet, Mikaël and Pérez, Jorge and Subercaseaux, Bernardo},
	year = {2020},
}

@inproceedings{bass_icam_2020,
	title = {{ICAM}: {Interpretable} {Classification} via {Disentangled} {Representations} and {Feature} {Attribution} {Mapping}},
	volume = {33},
	shorttitle = {{ICAM}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/56f9f88906aebf4ad985aaec7fa01313-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bass, Cher and da Silva, Mariana and Sudre, Carole and Tudosiu, Petru-Daniel and Smith, Stephen and Robinson, Emma},
	year = {2020},
}

@inproceedings{chen_understanding_2020,
	title = {Understanding {Deep} {Architecture} with {Reasoning} {Layer}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/0d82627e10660af39ea7eb69c3568955-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Xinshi and Zhang, Yufei and Reisinger, Christoph and Song, Le},
	year = {2020},
}

@inproceedings{covert_understanding_2020,
	title = {Understanding {Global} {Feature} {Contributions} {With} {Additive} {Importance} {Measures}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c7bf0b7c1a86d5eb3be2c722cf2cf746-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Covert, Ian and Lundberg, Scott M. and Lee, Su-In},
	year = {2020},
}

@inproceedings{crabbe_learning_2020,
	title = {Learning outside the {Black}-{Box}: {The} pursuit of interpretable models},
	volume = {33},
	shorttitle = {Learning outside the {Black}-{Box}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/ce758408f6ef98d7c7a7b786eca7b3a8-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Crabbe, Jonathan and Zhang, Yao and Zame, William and van der Schaar, Mihaela},
	year = {2020},
}

@inproceedings{jeyakumar_how_2020,
	title = {How {Can} {I} {Explain} {This} to {You}? {An} {Empirical} {Study} of {Deep} {Neural} {Network} {Explanation} {Methods}},
	volume = {33},
	shorttitle = {How {Can} {I} {Explain} {This} to {You}?},
	url = {https://proceedings.neurips.cc/paper/2020/hash/2c29d89cc56cdb191c60db2f0bae796b-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jeyakumar, Jeya Vikranth and Noor, Joseph and Cheng, Yu-Hsi and Garcia, Luis and Srivastava, Mani},
	year = {2020},
}

@inproceedings{laina_quantifying_2020,
	title = {Quantifying {Learnability} and {Describability} of {Visual} {Concepts} {Emerging} in {Representation} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/98dce83da57b0395e163467c9dae521b-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Laina, Iro and Fong, Ruth and Vedaldi, Andrea},
	year = {2020},
}

@inproceedings{lakshminarayanan_neural_2020,
	title = {Neural {Path} {Features} and {Neural} {Path} {Kernel} : {Understanding} the role of gates in deep learning},
	volume = {33},
	shorttitle = {Neural {Path} {Features} and {Neural} {Path} {Kernel}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/37f76c6fe3ab45e0cd7ecb176b5a046d-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lakshminarayanan, Chandrashekar and Vikram Singh, Amit},
	year = {2020},
}

@inproceedings{natesan_ramamurthy_model_2020,
	title = {Model {Agnostic} {Multilevel} {Explanations}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/426f990b332ef8193a61cc90516c1245-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Natesan Ramamurthy, Karthikeyan and Vinzamuri, Bhanukiran and Zhang, Yunfeng and Dhurandhar, Amit},
	year = {2020},
}

@inproceedings{oshaughnessy_generative_2020,
	title = {Generative causal explanations of black-box classifiers},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/3a93a609b97ec0ab0ff5539eb79ef33a-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {O'Shaughnessy, Matthew and Canal, Gregory and Connor, Marissa and Rozell, Christopher and Davenport, Mark},
	year = {2020},
}

@inproceedings{pedapati_learning_2020,
	title = {Learning {Global} {Transparent} {Models} consistent with {Local} {Contrastive} {Explanations}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/24aef8cb3281a2422a59b51659f1ad2e-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Pedapati, Tejaswini and Balakrishnan, Avinash and Shanmugam, Karthikeyan and Dhurandhar, Amit},
	year = {2020},
}

@inproceedings{tsang_how_2020,
	title = {How does {This} {Interaction} {Affect} {Me}? {Interpretable} {Attribution} for {Feature} {Interactions}},
	volume = {33},
	shorttitle = {How does {This} {Interaction} {Affect} {Me}?},
	url = {https://proceedings.neurips.cc/paper/2020/hash/443dec3062d0286986e21dc0631734c9-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tsang, Michael and Rambhatla, Sirisha and Liu, Yan},
	year = {2020},
}

@inproceedings{wang_smoothed_2020,
	title = {Smoothed {Geometry} for {Robust} {Attribution}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/9d94c8981a48d12adfeecfe1ae6e0ec1-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Zifan and Wang, Haofan and Ramkumar, Shakul and Mardziel, Piotr and Fredrikson, Matt and Datta, Anupam},
	year = {2020},
}

@inproceedings{zhou_learning_2020,
	title = {Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-{VAE}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/510f2318f324cf07fce24c3a4b89c771-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhou, Ding and Wei, Xue-Xin},
	year = {2020},
}

@inproceedings{zhou_towards_2020,
	title = {Towards {Interpretable} {Natural} {Language} {Understanding} with {Explanations} as {Latent} {Variables}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4be2c8f27b8a420492f2d44463933eb6-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhou, Wangchunshu and Hu, Jinyi and Zhang, Hanlin and Liang, Xiaodan and Sun, Maosong and Xiong, Chenyan and Tang, Jian},
	year = {2020},
}

@inproceedings{patir_interpretability_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Interpretability of {Black} {Box} {Models} {Through} {Data}-{View} {Extraction} and {Shadow} {Model} {Creation}},
	isbn = {978-3-030-63823-8},
	doi = {10.1007/978-3-030-63823-8_44},
	abstract = {Deep learning models trained using massive amounts of data, tend to capture one view of the data and its associated mapping. Different deep learning models built on the same training data may capture different views of the data based on the underlying techniques used. For explaining the decisions arrived by Black box deep learning models, we argue that it is essential to reproduce that model’s view of the training data faithfully. This faithful reproduction can then be used for explanation generation. We investigate two methods for data-view extraction: Hill Climbing approach and a GAN-driven approach. We then use this synthesized data for explanation generation by using methods such as Decision-Trees and Permutation Importance. We evaluate these approaches on a Black box model trained on public datasets and show its usefulness in explanation generation.},
	language = {en},
	booktitle = {International {Conference} on {Neural} {Information} {Processing}},
	publisher = {Springer International Publishing},
	author = {Patir, Rupam and Singhal, Shubham and Anantaram, C. and Goyal, Vikram},
	editor = {Yang, Haiqin and Pasupa, Kitsuchart and Leung, Andrew Chi-Sing and Kwok, James T. and Chan, Jonathan H. and King, Irwin},
	year = {2020},
	keywords = {Data synthesis, Data-view extraction, Interpretability},
	pages = {378--385},
}

@inproceedings{yeh_completeness-aware_2020,
	title = {On {Completeness}-aware {Concept}-{Based} {Explanations} in {Deep} {Neural} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html},
	language = {en},
	urldate = {2021-06-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yeh, Chih-Kuan and Kim, Been and Arik, Sercan and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep},
	year = {2020},
	pages = {20554--20565},
}

@inproceedings{ignatiev_towards_2020,
	title = {Towards {Trustable} {Explainable} {AI}},
	volume = {5},
	url = {https://www.ijcai.org/proceedings/2020/726},
	doi = {10.24963/ijcai.2020/726},
	abstract = {Electronic proceedings of IJCAI 2020},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Ignatiev, Alexey},
	month = jul,
	year = {2020},
	note = {ISSN: 1045-0823},
	pages = {5154--5158},
}

@inproceedings{alsallakh_mind_2021,
	title = {Mind the {Pad} -- {CNNs} {Can} {Develop} {Blind} {Spots}},
	url = {https://openreview.net/forum?id=m1CD7tPubNy},
	abstract = {We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or...},
	language = {en},
	urldate = {2021-06-23},
	author = {Alsallakh, Bilal and Kokhlikyan, Narine and Miglani, Vivek and Yuan, Jun and Reblitz-Richardson, Orion},
	year = {2021},
}

@inproceedings{antoran_getting_2021,
	title = {Getting a {CLUE}: {A} {Method} for {Explaining} {Uncertainty} {Estimates}},
	shorttitle = {Getting a {CLUE}},
	url = {https://openreview.net/forum?id=XSLF1XFq5h},
	abstract = {Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address...},
	language = {en},
	urldate = {2021-06-23},
	author = {Antoran, Javier and Bhatt, Umang and Adel, Tameem and Weller, Adrian and Hernández-Lobato, José Miguel},
	year = {2021},
}

@inproceedings{basu_influence_2021,
	title = {Influence {Functions} in {Deep} {Learning} {Are} {Fragile}},
	url = {https://openreview.net/forum?id=xHKVVHGDOEk},
	abstract = {Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A...},
	language = {en},
	urldate = {2021-06-23},
	author = {Basu, Samyadeep and Pope, Phil and Feizi, Soheil},
	year = {2021},
}

@inproceedings{frye_shapley_2021,
	title = {Shapley explainability on the data manifold},
	url = {https://openreview.net/forum?id=OPyWRrcjVQw},
	abstract = {Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model’s...},
	language = {en},
	urldate = {2021-02-23},
	author = {Frye, Christopher and Mijolla, Damien de and Begley, Tom and Cowton, Laurence and Stanley, Megan and Feige, Ilya},
	year = {2021},
}

@inproceedings{islam_shape_2021,
	title = {Shape or {Texture}: {Understanding} {Discriminative} {Features} in {CNNs}},
	shorttitle = {Shape or {Texture}},
	url = {https://openreview.net/forum?id=NcFEZOi-rLa},
	abstract = {Contrasting the previous evidence that neurons in the later layers of a Convolutional Neural Network (CNN) respond to complex object shapes, recent studies have shown that CNNs actually exhibit a...},
	language = {en},
	urldate = {2021-06-23},
	author = {Islam, Md Amirul and Kowal, Matthew and Esser, Patrick and Jia, Sen and Ommer, Björn and Derpanis, Konstantinos G. and Bruce, Neil},
	year = {2021},
}

@inproceedings{leavitt_selectivity_2021,
	title = {Selectivity considered harmful: evaluating the causal impact of class selectivity in {DNNs}},
	shorttitle = {Selectivity considered harmful},
	url = {https://openreview.net/forum?id=8nl0k08uMi},
	abstract = {The properties of individual neurons are often analyzed in order to understand the biological and artificial neural networks in which they're embedded. Class selectivity—typically defined as how...},
	language = {en},
	urldate = {2021-06-23},
	author = {Leavitt, Matthew L. and Morcos, Ari S.},
	year = {2021},
}

@inproceedings{sahoo_scaling_2021,
	title = {Scaling {Symbolic} {Methods} using {Gradients} for {Neural} {Model} {Explanation}},
	url = {https://openreview.net/forum?id=V5j-jdoDDP},
	abstract = {Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to...},
	language = {en},
	urldate = {2021-06-23},
	author = {Sahoo, Subham Sekhar and Venugopalan, Subhashini and Li, Li and Singh, Rishabh and Riley, Patrick},
	year = {2021},
}

@inproceedings{sanyal_how_2020,
	title = {How {Benign} is {Benign} {Overfitting} ?},
	url = {https://openreview.net/forum?id=g-wu9TMPODo},
	abstract = {We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training...},
	language = {en},
	urldate = {2021-06-23},
	author = {Sanyal, Amartya and Dokania, Puneet K. and Kanade, Varun and Torr, Philip},
	month = sep,
	year = {2020},
}

@inproceedings{uddin_saliencymix_2020,
	title = {{SaliencyMix}: {A} {Saliency} {Guided} {Data} {Augmentation} {Strategy} for {Better} {Regularization}},
	shorttitle = {{SaliencyMix}},
	url = {https://openreview.net/forum?id=-M0QkvBGTTq},
	abstract = {Advanced data augmentation strategies have widely been studied to improve the generalization ability of deep learning models. Regional dropout is one of the popular solutions that guides the model...},
	language = {en},
	urldate = {2021-06-23},
	author = {Uddin, A. F. M. Shahab and Monira, Mst Sirazam and Shin, Wheemyung and Chung, TaeChoong and Bae, Sung-Ho},
	month = sep,
	year = {2020},
}

@inproceedings{wang_shapley_2021,
	title = {Shapley {Explanation} {Networks}},
	url = {https://openreview.net/forum?id=vsU0efpivw},
	abstract = {Shapley values have become one of the most popular feature attribution explanation methods. However, most prior work has focused on post-hoc Shapley explanations, which can be computationally...},
	language = {en},
	urldate = {2021-02-23},
	author = {Wang, Rui and Wang, Xiaoqian and Inouye, David I.},
	year = {2021},
}

@inproceedings{srinivas_rethinking_2021,
	title = {Rethinking the {Role} of {Gradient}-based {Attribution} {Methods} for {Model} {Interpretability}},
	url = {https://openreview.net/forum?id=dYeAHXnpWJ4},
	abstract = {Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common...},
	language = {en},
	urldate = {2021-06-22},
	author = {Srinivas, Suraj and Fleuret, Francois},
	year = {2021},
}

@article{li_scouter_2020,
	title = {{SCOUTER}: {Slot} {Attention}-based {Classifier} for {Explainable} {Image} {Recognition}},
	shorttitle = {{SCOUTER}},
	url = {https://arxiv.org/abs/2009.06138v3},
	abstract = {Explainable artificial intelligence has been gaining attention in the past few years. However, most existing methods are based on gradients or intermediate features, which are not directly involved in the decision-making process of the classifier. In this paper, we propose a slot attention-based classifier called SCOUTER for transparent yet accurate classification. Two major differences from other attention-based methods include: (a) SCOUTER's explanation is involved in the final confidence for each category, offering more intuitive interpretation, and (b) all the categories have their corresponding positive or negative explanation, which tells "why the image is of a certain category" or "why the image is not of a certain category." We design a new loss tailored for SCOUTER that controls the model's behavior to switch between positive and negative explanations, as well as the size of explanatory regions. Experimental results show that SCOUTER can give better visual explanations while keeping good accuracy on small and medium-sized datasets.},
	language = {en},
	urldate = {2021-06-22},
	author = {Li, Liangzhi and Wang, Bowen and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
	month = sep,
	year = {2020},
}

@inproceedings{raghu_svcca_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {{SVCCA}: singular vector canonical correlation analysis for deep learning dynamics and interpretability},
	isbn = {978-1-5108-6096-4},
	shorttitle = {{SVCCA}},
	abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	month = dec,
	year = {2017},
	pages = {6078--6087},
}

@inproceedings{rombach_making_2020,
	title = {Making {Sense} of {CNNs}: {Interpreting} {Deep} {Representations} \& {Their} {Invariances} with {INNs}},
	abstract = {To tackle increasingly complex tasks, it has become an essential ability of neural networks to learn abstract representations. These task-speciﬁc representations and, particularly, the invariances they capture turn neural networks into black box models that lack interpretability. To open such a black box, it is, therefore, crucial to uncover the diﬀerent semantic concepts a model has learned as well as those that it has learned to be invariant to. We present an approach based on INNs that (i) recovers the task-speciﬁc, learned invariances by disentangling the remaining factor of variation in the data and that (ii) invertibly transforms these recovered invariances combined with the model representation into an equally expressive one with accessible semantic concepts. As a consequence, neural network representations become understandable by providing the means to (i) expose their semantic meaning, (ii) semantically modify a representation, and (iii) visualize individual learned semantic concepts and invariances. Our invertible approach signiﬁcantly extends the abilities to understand black box models by enabling posthoc interpretations of state-of-the-art networks without compromising their performance.},
	language = {en},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Rombach, Robin and Esser, Patrick and Ommer, Bjorn},
	year = {2020},
	pages = {18},
}

@inproceedings{srinivas_full-gradient_2019,
	title = {Full-{Gradient} {Representation} for {Neural} {Network} {Visualization}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/80537a945c7aaa788ccfcdf1b99b5d8f-Abstract.html},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Srinivas, Suraj and Fleuret, François},
	year = {2019},
}

@inproceedings{zhang_interpreting_2018,
	title = {Interpreting {CNN} {Knowledge} via an {Explanatory} {Graph}},
	volume = {32},
	copyright = {Copyright (c)},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11819},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Quanshi and Cao, Ruiming and Shi, Feng and Wu, Ying Nian and Zhu, Song-Chun},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Interpretable Model},
}

@inproceedings{hase_evaluating_2020,
	address = {Online},
	title = {Evaluating {Explainable} {AI}: {Which} {Algorithmic} {Explanations} {Help} {Users} {Predict} {Model} {Behavior}?},
	shorttitle = {Evaluating {Explainable} {AI}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.491},
	doi = {10.18653/v1/2020.acl-main.491},
	abstract = {Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hase, Peter and Bansal, Mohit},
	month = jul,
	year = {2020},
	pages = {5540--5552},
}

@incollection{zhou_comparing_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Comparing the {Interpretability} of {Deep} {Networks} via {Network} {Dissection}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_12},
	abstract = {In this chapter, we introduce Network Dissection (The complete paper and code are available at http://netdissect.csail.mit.edu), a general framework to quantify the interpretability of the units inside a deep convolutional neural networks (CNNs). We compare the different vocabularies of interpretable units as concept detectors emerged from the networks trained to solve different supervised learning tasks such as object recognition on ImageNet and scene classification on Places. The network dissection is further applied to analyze how the units acting as semantic detectors grow and evolve over the training iterations both in the scenario of the train-from-scratch and in the stage of the fine-tuning between data sources. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into their hierarchical structure.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_12},
	keywords = {Deep neural networks, Interpretable machine learning, Model visualization},
	pages = {243--252},
}

@incollection{hu_unsupervised_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Unsupervised {Discrete} {Representation} {Learning}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_6},
	abstract = {Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation. The task includes clustering and hash learning as special cases. Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets. However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful and interpretable representations that exhibit intended invariance for applications of interest. To this end, we propose a method called Information Maximizing Self-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose the invariance on discrete representations. More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion. At the same time, we maximize the information-theoretic dependency between data and their predicted discrete representations. Our IMSAT is able to discover interpretable representations that exhibit intended invariance. Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Hu, Weihua and Miyato, Takeru and Tokui, Seiya and Matsumoto, Eiichi and Sugiyama, Masashi},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_6},
	keywords = {Clustering, Discrete representation learning, Hash learning},
	pages = {97--119},
}

@incollection{montavon_gradient-based_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Gradient-{Based} {Vs}. {Propagation}-{Based} {Explanations}: {An} {Axiomatic} {Comparison}},
	isbn = {978-3-030-28954-6},
	shorttitle = {Gradient-{Based} {Vs}. {Propagation}-{Based} {Explanations}},
	url = {https://doi.org/10.1007/978-3-030-28954-6_13},
	abstract = {Deep neural networks, once considered to be inscrutable black-boxes, are now supplemented with techniques that can explain how these models decide. This raises the question whether the produced explanations are reliable. In this chapter, we consider two popular explanation techniques, one based on gradient computation and one based on a propagation mechanism. We evaluate them using three “axiomatic” properties: conservation, continuity, and implementation invariance. These properties are tested on the overall explanation, but also at intermediate layers, where our analysis brings further insights on how the explanation is being formed.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Montavon, Grégoire},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_13},
	keywords = {Axioms, Deep neural networks, Explanations},
	pages = {253--265},
}

@incollection{oh_towards_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Reverse}-{Engineering} {Black}-{Box} {Neural} {Networks}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_7},
	abstract = {Much progress in interpretable AI is built around scenarios where the user, one who interprets the model, has a full ownership of the model to be diagnosed. The user either owns the training data and computing resources to train an interpretable model herself or owns a full access to an already trained model to be interpreted post-hoc. In this chapter, we consider a less investigated scenario of diagnosing black-box neural networks, where the user can only send queries and read off outputs. Black-box access is a common deployment mode for many public and commercial models, since internal details, such as architecture, optimisation procedure, and training data, can be proprietary and aggravate their vulnerability to attacks like adversarial examples. We propose a method for exposing internals of black-box models and show that the method is surprisingly effective at inferring a diverse set of internal information. We further show how the exposed internals can be exploited to strengthen adversarial examples against the model. Our work starts an important discussion on the security implications of diagnosing deployed models with limited accessibility. The code is available at goo.gl/MbYfsv.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Oh, Seong Joon and Schiele, Bernt and Fritz, Mario},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_7},
	keywords = {Black box, Explainability, Machine Learning, Security},
	pages = {121--144},
}

@incollection{hong_interpretable_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Interpretable {Text}-to-{Image} {Synthesis} with {Hierarchical} {Semantic} {Layout} {Generation}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_5},
	abstract = {Generating images from natural language description has drawn a lot of attention in the research community for its practical usefulness and for understanding the method in which the model relates text with visual concepts by synthesizing them. Deep generative models have been successfully employed to address this task, which formulates the problem as a translation task from text to image. However, learning a direct mapping from text to image is challenging due to the complexity of the mapping and makes it difficult to understand the underlying generation process. To address these issues, we propose a novel hierarchical approach for text-to-image synthesis by inferring a semantic layout. Our algorithm decomposes the generation process into multiple steps. First, it constructs a semantic layout from the text using the layout generator and then converts the layout to an image with the image generator. The proposed layout generator progressively constructs a semantic layout in a coarse-to-fine manner by generating object bounding boxes and refining each box by estimating the object shapes inside the box. The image generator synthesizes an image conditioned on the inferred semantic layout, which provides a useful semantic structure of an image matching the text description. Conditioning the generation with the inferred semantic layout allows our model to generate semantically more meaningful images and provides interpretable representations to allow users to interactively control the generation process by modifying the layout. We demonstrate the capability of the proposed model on the challenging MS-COCO dataset and show that the model can substantially improve the image quality and interpretability of the output and semantic alignment to input text over existing approaches.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Hong, Seunghoon and Yang, Dingdong and Choi, Jongwook and Lee, Honglak},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_5},
	pages = {77--95},
}

@incollection{fong_explanations_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Explanations for {Attributing} {Deep} {Neural} {Network} {Predictions}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_8},
	abstract = {Given the recent success of deep neural networks and their applications to more high impact and high risk applications, like autonomous driving and healthcare decision-making, there is a great need for faithful and interpretable explanations of “why” an algorithm is making a certain prediction. In this chapter, we introduce 1. Meta-Predictors as Explanations, a principled framework for learning explanations for any black box algorithm, and 2. Meaningful Perturbations, an instantiation of our paradigm applied to the problem of attribution, which is concerned with attributing what features of an input (i.e., regions of an input image) are responsible for a model’s output (i.e., a CNN classifier’s object class prediction). We first introduced these contributions in [8]. We also briefly survey existing visual attribution methods and highlight how they faith to be both faithful and interpretable.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Fong, Ruth and Vedaldi, Andrea},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_8},
	keywords = {Computer vision, Explainable artificial intelligence, Machine learning},
	pages = {149--167},
}

@incollection{kindermans_reliability_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The ({Un})reliability of {Saliency} {Methods}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_14},
	abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step which can be compensated for easily—adding a constant shift to the input data—to show that a transformation with no effect on how the model makes the decision can cause numerous methods to attribute incorrectly. In order to guarantee reliability, we believe that the explanation should not change when we can guarantee that two networks process the images in identical manners. We show, through several examples, that saliency methods that do not satisfy this requirement result in misleading attribution. The approach can be seen as a type of unit test; we construct a narrow ground truth to measure one stated desirable property. As such, we hope the community will embrace the development of additional tests.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_14},
	pages = {267--280},
}

@incollection{montavon_layer-wise_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Layer-{Wise} {Relevance} {Propagation}: {An} {Overview}},
	isbn = {978-3-030-28954-6},
	shorttitle = {Layer-{Wise} {Relevance} {Propagation}},
	url = {https://doi.org/10.1007/978-3-030-28954-6_10},
	abstract = {For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a ‘deep Taylor decomposition’, (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Montavon, Grégoire and Binder, Alexander and Lapuschkin, Sebastian and Samek, Wojciech and Müller, Klaus-Robert},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_10},
	keywords = {Deep Neural Networks, Deep Taylor Decomposition, Explanations, Layer-wise Relevance Propagation},
	pages = {193--209},
}

@incollection{weller_transparency_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Transparency: {Motivations} and {Challenges}},
	isbn = {978-3-030-28954-6},
	shorttitle = {Transparency},
	url = {https://doi.org/10.1007/978-3-030-28954-6_2},
	abstract = {Transparency is often deemed critical to enable effective real-world deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns, particularly when agents have misaligned interests. We highlight and review settings where transparency may cause harm, discussing connections across privacy, multi-agent game theory, economics, fairness and trust.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Weller, Adrian},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_2},
	keywords = {Explainable, Interpretability, Social good, Transparency},
	pages = {23--40},
}

@incollection{samek_towards_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Towards {Explainable} {Artificial} {Intelligence}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_1},
	abstract = {In recent years, machine learning (ML) has become a key enabling technology for the sciences and industry. Especially through improvements in methodology, the availability of large databases and increased computational power, today’s ML algorithms are able to achieve excellent performance (at times even exceeding the human level) on an increasing number of complex tasks. Deep learning models are at the forefront of this development. However, due to their nested non-linear structure, these powerful models have been generally considered “black boxes”, not providing any information about what exactly makes them arrive at their predictions. Since in many applications, e.g., in the medical domain, such lack of transparency may be not acceptable, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This introductory paper presents recent developments and applications in this field and makes a plea for a wider use of explainable learning algorithms in practice.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Samek, Wojciech and Müller, Klaus-Robert},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_1},
	keywords = {Deep learning, Explainable artificial intelligence, Interpretability, Model transparency, Neural networks},
	pages = {5--22},
}

@incollection{arras_explaining_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Explaining and {Interpreting} {LSTMs}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_11},
	abstract = {While neural networks have acted as a strong unifying force in the design of modern AI systems, the neural network architectures themselves remain highly heterogeneous due to the variety of tasks to be solved. In this chapter, we explore how to adapt the Layer-wise Relevance Propagation (LRP) technique used for explaining the predictions of feed-forward networks to the LSTM architecture used for sequential data modeling and forecasting. The special accumulators and gated interactions present in the LSTM require both a new propagation scheme and an extension of the underlying theoretical framework to deliver faithful explanations.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Arras, Leila and Arjona-Medina, José and Widrich, Michael and Montavon, Grégoire and Gillhofer, Michael and Müller, Klaus-Robert and Hochreiter, Sepp and Samek, Wojciech},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_11},
	keywords = {Explainable artificial intelligence, Interpretability, LSTM, Model transparency, Recurrent neural networks},
	pages = {211--238},
}

@incollection{ancona_gradient-based_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Gradient-{Based} {Attribution} {Methods}},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_9},
	abstract = {The problem of explaining complex machine learning models, including Deep Neural Networks, has gained increasing attention over the last few years. While several methods have been proposed to explain network predictions, the definition itself of explanation is still debated. Moreover, only a few attempts to compare explanation methods from a theoretical perspective has been done. In this chapter, we discuss the theoretical properties of several attribution methods and show how they share the same idea of using the gradient information as a descriptive factor for the functioning of a model. Finally, we discuss the strengths and limitations of these methods and compare them with available alternatives.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_9},
	keywords = {Attribution methods, Deep Neural Networks, Explainable artificial intelligence},
	pages = {169--191},
}

@incollection{nguyen_understanding_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Understanding {Neural} {Networks} via {Feature} {Visualization}: {A} {Survey}},
	isbn = {978-3-030-28954-6},
	shorttitle = {Understanding {Neural} {Networks} via {Feature} {Visualization}},
	url = {https://doi.org/10.1007/978-3-030-28954-6_4},
	abstract = {A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) [10] or Feature Visualization via Optimization. In this chapter, we (1) review existing AM techniques in the literature; (2) discuss a probabilistic interpretation for AM; and (3) review the applications of AM in debugging and explaining networks.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_4},
	keywords = {Activation Maximization, Feature visualization, Generative models, Generator network, Neural networks, Optimization},
	pages = {55--76},
}

@incollection{hansen_interpretability_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Interpretability in {Intelligent} {Systems} – {A} {New} {Concept}?},
	isbn = {978-3-030-28954-6},
	url = {https://doi.org/10.1007/978-3-030-28954-6_3},
	abstract = {The very active community for interpretable machine learning can learn from the rich 50+ year history of explainable AI. We here give two specific examples from this legacy that could enrich current interpretability work: First, Explanation desiderata were we point to the rich set of ideas developed in the ‘explainable expert systems’ field and, second, tools for quantification of uncertainty of high-dimensional feature importance maps which have been developed in the field of computational neuroimaging.},
	language = {en},
	urldate = {2021-06-22},
	booktitle = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	publisher = {Springer International Publishing},
	author = {Hansen, Lars Kai and Rieger, Laura},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6_3},
	keywords = {Interpretable AI, Machine learning, Uncertainty quantification},
	pages = {41--49},
}

@inproceedings{mothilal_explaining_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Explaining machine learning classifiers through diverse counterfactual explanations},
	isbn = {978-1-4503-6936-7},
	url = {https://doi.org/10.1145/3351095.3372850},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	pages = {607--617},
}

@inproceedings{borowski_exemplary_2021,
	title = {Exemplary {Natural} {Images} {Explain} {CNN} {Activations} {Better} than {State}-of-the-{Art} {Feature} {Visualization}},
	url = {https://openreview.net/forum?id=QO9-y8also-},
	abstract = {Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At...},
	language = {en},
	urldate = {2021-06-22},
	author = {Borowski, Judy and Zimmermann, Roland Simon and Schepers, Judith and Geirhos, Robert and Wallis, Thomas S. A. and Bethge, Matthias and Brendel, Wieland},
	year = {2021},
}

@inproceedings{hsieh_evaluations_2021,
	title = {Evaluations and {Methods} for {Explanation} through {Robustness} {Analysis}},
	url = {https://openreview.net/forum?id=4dXmpCDGNp7},
	abstract = {Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel...},
	language = {en},
	urldate = {2021-02-24},
	author = {Hsieh, Cheng-Yu and Yeh, Chih-Kuan and Liu, Xuanqing and Ravikumar, Pradeep Kumar and Kim, Seungyeon and Kumar, Sanjiv and Hsieh, Cho-Jui},
	year = {2021},
}

@inproceedings{yu_interpreting_2019,
	title = {Interpreting and {Evaluating} {Neural} {Network} {Robustness}},
	url = {https://www.ijcai.org/proceedings/2019/583},
	abstract = {Electronic proceedings of IJCAI 2019},
	urldate = {2021-06-22},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Yu, Fuxun and Qin, Zhuwei and Liu, Chenchen and Zhao, Liang and Wang, Yanzhi and Chen, Xiang},
	year = {2019},
	pages = {4199--4205},
}

@inproceedings{zhang_interpretable_2020-1,
	title = {Interpretable {Deep} {Learning} under {Fire}},
	isbn = {978-1-939133-17-5},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/zhang-xinyang},
	language = {en},
	urldate = {2021-06-22},
	author = {Zhang, Xinyang and Wang, Ningfei and Shen, Hua and Ji, Shouling and Luo, Xiapu and Wang, Ting},
	year = {2020},
	pages = {1659--1676},
}

@inproceedings{hanawa_evaluation_2021,
	title = {Evaluation of {Similarity}-based {Explanations}},
	url = {https://openreview.net/forum?id=9uvhpyQwzM_},
	abstract = {Explaining the predictions made by complex machine learning models helps users to understand and accept the predicted outputs with confidence. One promising way is to use similarity-based...},
	language = {en},
	urldate = {2021-06-22},
	author = {Hanawa, Kazuaki and Yokoi, Sho and Hara, Satoshi and Inui, Kentaro},
	year = {2021},
}

@inproceedings{mummadi_does_2020,
	title = {Does enhanced shape bias improve neural network robustness to common corruptions?},
	url = {https://openreview.net/forum?id=yUxUNaj2Sl},
	abstract = {Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs...},
	language = {en},
	urldate = {2021-02-24},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Mummadi, Chaithanya Kumar and Subramaniam, Ranjitha and Hutmacher, Robin and Vitay, Julien and Fischer, Volker and Metzen, Jan Hendrik},
	month = sep,
	year = {2020},
}

@inproceedings{tsirtsis_decisions_2020,
	title = {Decisions, {Counterfactual} {Explanations} and {Strategic} {Behavior}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c2ba1bc54b239208cb37b901c0d3b363-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tsirtsis, Stratis and Gomez Rodriguez, Manuel},
	year = {2020},
}

@inproceedings{adebayo_debugging_2020,
	title = {Debugging {Tests} for {Model} {Explanations}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/075b051ec3d22dac7b33f788da631fd4-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Adebayo, Julius and Muelly, Michael and Liccardi, Ilaria and Kim, Been},
	year = {2020},
}

@inproceedings{schwab_cxplain_2019,
	title = {{CXPlain}: {Causal} {Explanations} for {Model} {Interpretation} under {Uncertainty}},
	abstract = {Feature importance estimates that inform users about the degree to which given inputs inﬂuence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantiﬁcation of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is signiﬁcantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we conﬁrm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Schwab, Patrick and Karlen, Walter},
	year = {2019},
	pages = {11},
}

@inproceedings{mu_compositional_2020,
	title = {Compositional {Explanations} of {Neurons}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c74956ffb38ba48ed6ce977af6727275-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mu, Jesse and Andreas, Jacob},
	year = {2020},
}

@inproceedings{higgins_beta-vae_2017,
	title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
	shorttitle = {beta-{VAE}},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	abstract = {We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.},
	language = {en},
	urldate = {2021-06-09},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	year = {2017},
}

@inproceedings{heskes_causal_2020,
	title = {Causal {Shapley} {Values}: {Exploiting} {Causal} {Knowledge} to {Explain} {Individual} {Predictions} of {Complex} {Models}},
	volume = {33},
	shorttitle = {Causal {Shapley} {Values}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Heskes, Tom and Sijben, Evi and Bucur, Ioan Gabriel and Claassen, Tom},
	year = {2020},
}

@inproceedings{choksi_brain-inspired_2020,
	title = {Brain-inspired predictive coding dynamics improve the robustness of deep neural networks},
	abstract = {Deep neural networks excel at image classiﬁcation, but their performance is far less robust to input perturbations than human perception. In this work we address this shortcoming by incorporating brain-inspired recurrent dynamics in deep convolutional networks. We augment a pretrained feedforward classiﬁcation model (VGG16 trained on ImageNet) with a “predictive coding” strategy: a framework popular in neuroscience for characterizing cortical function. At each layer of the hierarchical model, generative feedback “predicts” (i.e., reconstructs) the pattern of activity in the previous layer. The reconstruction errors are used to iteratively update the network’s representations across timesteps, and to optimize the network’s feedback weights over the natural image dataset–a form of unsupervised training. We demonstrate that this results in a network with improved robustness compared to the corresponding feedforward baseline, not only against various types of noise but also against a suite of adversarial attacks. We propose that most feedforward models could be equipped with these brain-inspired feedback dynamics, thus improving their robustness to input perturbations.},
	language = {en},
	booktitle = {2nd {Workshop} on {Shared} {Visual} {Representations} in {Human} and {Machine} {Intelligence} ({SVRHM}), {NeurIPS}},
	author = {Choksi, Bhavin and Mozafari, Milad and O’May, Callum Biggs and Ador, Benjamin and Alamia, Andrea and VanRullen, Ruﬁn},
	year = {2020},
	pages = {13},
}

@inproceedings{nie_bongard-logo_2020,
	title = {Bongard-{LOGO}: {A} {New} {Benchmark} for {Human}-{Level} {Concept} {Learning} and {Reasoning}},
	volume = {33},
	shorttitle = {Bongard-{LOGO}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/bf15e9bbff22c7719020f9df4badc20a-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nie, Weili and Yu, Zhiding and Mao, Lei and Patel, Ankit B. and Zhu, Yuke and Anandkumar, Anima},
	year = {2020},
}

@article{adler_auditing_2018,
	title = {Auditing black-box models for indirect influence},
	volume = {54},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-017-1116-3},
	doi = {10.1007/s10115-017-1116-3},
	abstract = {Data-trained predictive models see widespread use, but for the most part they are used as black boxes which output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior and in particular how different features influence the model prediction. This is important when interpreting the behavior of complex models or asserting that certain problematic attributes (such as race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models, which lets us study the extent to which existing models take advantage of particular features in the data set, without knowing how the models work. Our work focuses on the problem of indirect influence: how some features might indirectly influence outcomes via other, related features. As a result, we can find attribute influences even in cases where, upon further direct examination of the model, the attribute is not referred to by the model at all. Our approach does not require the black-box model to be retrained. This is important if, for example, the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence such as feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available data sets and models. We also validate our procedure using techniques from interpretable learning and feature selection, as well as against other black-box auditing procedures. To further demonstrate the effectiveness of this technique, we use it to audit a black-box recidivism prediction algorithm.},
	language = {en},
	number = {1},
	urldate = {2021-06-19},
	journal = {Knowledge and Information Systems},
	author = {Adler, Philip and Falk, Casey and Friedler, Sorelle A. and Nix, Tionney and Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon and Venkatasubramanian, Suresh},
	month = jan,
	year = {2018},
	pages = {95--122},
}

@inproceedings{frye_asymmetric_2020,
	title = {Asymmetric {Shapley} values: incorporating causal knowledge into model-agnostic explainability},
	volume = {33},
	shorttitle = {Asymmetric {Shapley} values},
	url = {https://proceedings.neurips.cc/paper/2020/hash/0d770c496aa3da6d2c3f2bd19e7b9d6b-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Frye, Christopher and Rowat, Colin and Feige, Ilya},
	year = {2020},
}

@inproceedings{zhang_are_2019,
	title = {Are {All} {Layers} {Created} {Equal}?},
	abstract = {Understanding learning with deep architectures has been a major research objective in the recent years with notable theoretical progress. A main focal point of those studies stems from the success of excessively large networks. We study empirically the layer-wise functional structure of overparameterized deep models. We provide evidence for the heterogeneous characteristic of layers. To do so, we introduce the notion of (post training) re-initialization and re-randomization robustness. We show that layers can be categorized into either “robust” or “critical”. In contrast to critical layers, resetting the robust layers to their initial value has no negative consequence, and in many cases they barely change throughout training. Our study provides evidence atness or robustness analysis of the model parameters needs to respect the network architectures.},
	language = {en},
	booktitle = {{ICML} {Workshop} on {Deep} {Phenomena}},
	author = {Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
	year = {2019},
	pages = {18},
}

@article{yoon_rl-lim_2019,
	title = {{RL}-{LIM}: {Reinforcement} {Learning}-based {Locally} {Interpretable} {Modeling}},
	shorttitle = {{RL}-{LIM}},
	url = {https://arxiv.org/abs/1909.12367v1},
	abstract = {Understanding black-box machine learning models is important towards their widespread adoption. However, developing globally interpretable models that explain the behavior of the entire model is challenging. An alternative approach is to explain black-box models through explaining individual prediction using a locally interpretable model. In this paper, we propose a novel method for locally interpretable modeling - Reinforcement Learning-based Locally Interpretable Modeling (RL-LIM). RL-LIM employs reinforcement learning to select a small number of samples and distill the black-box model prediction into a low-capacity locally interpretable model. Training is guided with a reward that is obtained directly by measuring agreement of the predictions from the locally interpretable model with the black-box model. RL-LIM near-matches the overall prediction performance of black-box models while yielding human-like interpretability, and significantly outperforms state of the art locally interpretable models in terms of overall prediction performance and fidelity.},
	language = {en},
	urldate = {2021-06-18},
	author = {Yoon, Jinsung and Arik, Sercan O. and Pfister, Tomas},
	month = sep,
	year = {2019},
}

@inproceedings{plumb_model_2018,
	title = {Model {Agnostic} {Supervised} {Local} {Explanations}},
	abstract = {Model interpretability is an increasingly important component of practical machine learning. Some of the most common forms of interpretability systems are example-based, local, and global explanations. One of the main challenges in interpretability is designing explanation systems that can capture aspects of each of these explanation types, in order to develop a more thorough understanding of the model. We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method). MAPLE has two fundamental advantages over existing interpretability systems. First, while it is effective as a black-box explanation system, MAPLE itself is a highly accurate predictive model that provides faithful self explanations, and thus sidesteps the typical accuracy-interpretability trade-off. Speciﬁcally, we demonstrate, on several UCI datasets, that MAPLE is at least as accurate as random forests and that it produces more faithful local explanations than LIME, a popular interpretability system. Second, MAPLE provides both example-based and local explanations and can detect global patterns, which allows it to diagnose limitations in its local explanations.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Plumb, Gregory and Molitor, Denali and Talwalkar, Ameet S},
	year = {2018},
	pages = {10},
}

@inproceedings{li_learning_2021,
	title = {A {Learning} {Theoretic} {Perspective} on {Local} {Explainability}},
	url = {https://openreview.net/forum?id=7aL-OtQrBWD},
	abstract = {In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of...},
	language = {en},
	urldate = {2021-06-18},
	author = {Li, Jeffrey and Nagarajan, Vaishnavh and Plumb, Gregory and Talwalkar, Ameet},
	year = {2021},
}

@inproceedings{wang_mtunet_2021,
	title = {{MTUNet}: {Few}-{Shot} {Image} {Classification} {With} {Visual} {Explanations}},
	shorttitle = {{MTUNet}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Wang_MTUNet_Few-Shot_Image_Classification_With_Visual_Explanations_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Wang, Bowen and Li, Liangzhi and Verma, Manisha and Nakashima, Yuta and Kawasaki, Ryo and Nagahara, Hajime},
	year = {2021},
	pages = {2294--2298},
}

@inproceedings{poppi_revisiting_2021,
	title = {Revisiting the {Evaluation} of {Class} {Activation} {Mapping} for {Explainability}: {A} {Novel} {Metric} and {Experimental} {Analysis}},
	shorttitle = {Revisiting the {Evaluation} of {Class} {Activation} {Mapping} for {Explainability}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/RCV/html/Poppi_Revisiting_the_Evaluation_of_Class_Activation_Mapping_for_Explainability_A_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Poppi, Samuele and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
	year = {2021},
	pages = {2299--2304},
}

@inproceedings{rahnama_adversarial_2021,
	title = {An {Adversarial} {Approach} for {Explaining} the {Predictions} of {Deep} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/TCV/html/Rahnama_An_Adversarial_Approach_for_Explaining_the_Predictions_of_Deep_Neural_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
	author = {Rahnama, Arash and Tseng, Andrew},
	year = {2021},
	pages = {3253--3262},
}

@inproceedings{gu_interpreting_2021,
	title = {Interpreting {Super}-{Resolution} {Networks} {With} {Local} {Attribution} {Maps}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Gu_Interpreting_Super-Resolution_Networks_With_Local_Attribution_Maps_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Gu, Jinjin and Dong, Chao},
	year = {2021},
	pages = {9199--9208},
}

@inproceedings{singla_understanding_2021,
	title = {Understanding {Failures} of {Deep} {Networks} via {Robust} {Feature} {Extraction}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Singla_Understanding_Failures_of_Deep_Networks_via_Robust_Feature_Extraction_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Singla, Sahil and Nushi, Besmira and Shah, Shital and Kamar, Ece and Horvitz, Eric},
	year = {2021},
	pages = {12853--12862},
}

@inproceedings{nauta_neural_2021,
	title = {Neural {Prototype} {Trees} for {Interpretable} {Fine}-{Grained} {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Nauta, Meike and van Bree, Ron and Seifert, Christin},
	year = {2021},
	pages = {14933--14943},
}

@inproceedings{mao_generative_2021,
	title = {Generative {Interventions} for {Causal} {Learning}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Mao_Generative_Interventions_for_Causal_Learning_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Mao, Chengzhi and Cha, Augustine and Gupta, Amogh and Wang, Hao and Yang, Junfeng and Vondrick, Carl},
	year = {2021},
	pages = {3947--3956},
}

@inproceedings{bohle_convolutional_2021,
	title = {Convolutional {Dynamic} {Alignment} {Networks} for {Interpretable} {Classifications}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Bohle_Convolutional_Dynamic_Alignment_Networks_for_Interpretable_Classifications_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Bohle, Moritz and Fritz, Mario and Schiele, Bernt},
	year = {2021},
	pages = {10029--10038},
}

@inproceedings{elliott_explaining_2021,
	title = {Explaining {Classifiers} {Using} {Adversarial} {Perturbations} on the {Perceptual} {Ball}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Elliott_Explaining_Classifiers_Using_Adversarial_Perturbations_on_the_Perceptual_Ball_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-17},
	author = {Elliott, Andrew and Law, Stephen and Russell, Chris},
	year = {2021},
	pages = {10693--10702},
}

@inproceedings{jaume_quantifying_2021,
	title = {Quantifying {Explainers} of {Graph} {Neural} {Networks} in {Computational} {Pathology}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Jaume_Quantifying_Explainers_of_Graph_Neural_Networks_in_Computational_Pathology_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Jaume, Guillaume and Pati, Pushpak and Bozorgtabar, Behzad and Foncubierta, Antonio and Anniciello, Anna Maria and Feroce, Florinda and Rau, Tilman and Thiran, Jean-Philippe and Gabrani, Maria and Goksel, Orcun},
	year = {2021},
	pages = {8106--8116},
}

@inproceedings{shen_closed-form_2021,
	title = {Closed-{Form} {Factorization} of {Latent} {Semantics} in {GANs}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Shen_Closed-Form_Factorization_of_Latent_Semantics_in_GANs_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Shen, Yujun and Zhou, Bolei},
	year = {2021},
	pages = {1532--1540},
}

@inproceedings{ramaswamy_fair_2021,
	title = {Fair {Attribute} {Classification} {Through} {Latent} {Space} {De}-{Biasing}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ramaswamy_Fair_Attribute_Classification_Through_Latent_Space_De-Biasing_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Ramaswamy, Vikram V. and Kim, Sunnie S. Y. and Russakovsky, Olga},
	year = {2021},
	pages = {9301--9310},
}

@inproceedings{zhu_where_2021,
	title = {Where and {What}? {Examining} {Interpretable} {Disentangled} {Representations}},
	shorttitle = {Where and {What}?},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Where_and_What_Examining_Interpretable_Disentangled_Representations_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Zhu, Xinqi and Xu, Chang and Tao, Dacheng},
	year = {2021},
	pages = {5861--5870},
}

@inproceedings{xu_linear_2021,
	title = {Linear {Semantics} in {Generative} {Adversarial} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Linear_Semantics_in_Generative_Adversarial_Networks_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Xu, Jianjin and Zheng, Changxi},
	year = {2021},
	pages = {9351--9360},
}

@inproceedings{pham_learning_2021,
	title = {Learning {To} {Predict} {Visual} {Attributes} in the {Wild}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Pham_Learning_To_Predict_Visual_Attributes_in_the_Wild_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Pham, Khoi and Kafle, Kushal and Lin, Zhe and Ding, Zhihong and Cohen, Scott and Tran, Quan and Shrivastava, Abhinav},
	year = {2021},
	pages = {13018--13028},
}

@inproceedings{chefer_transformer_2021,
	title = {Transformer {Interpretability} {Beyond} {Attention} {Visualization}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
	year = {2021},
	pages = {782--791},
}

@inproceedings{ge_peek_2021,
	title = {A {Peek} {Into} the {Reasoning} of {Neural} {Networks}: {Interpreting} {With} {Structural} {Visual} {Concepts}},
	shorttitle = {A {Peek} {Into} the {Reasoning} of {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Ge, Yunhao and Xiao, Yao and Xu, Zhi and Zheng, Meng and Karanam, Srikrishna and Chen, Terrence and Itti, Laurent and Wu, Ziyan},
	year = {2021},
	pages = {2195--2204},
}

@inproceedings{khakzar_neural_2021,
	title = {Neural {Response} {Interpretation} {Through} the {Lens} of {Critical} {Pathways}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Khakzar_Neural_Response_Interpretation_Through_the_Lens_of_Critical_Pathways_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Khakzar, Ashkan and Baselizadeh, Soroosh and Khanduja, Saurabh and Rupprecht, Christian and Kim, Seong Tae and Navab, Nassir},
	year = {2021},
	pages = {13528--13538},
}

@inproceedings{zhao_graph-based_2021,
	title = {Graph-{Based} {High}-{Order} {Relation} {Discovery} for {Fine}-{Grained} {Recognition}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Zhao_Graph-Based_High-Order_Relation_Discovery_for_Fine-Grained_Recognition_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Zhao, Yifan and Yan, Ke and Huang, Feiyue and Li, Jia},
	year = {2021},
	pages = {15079--15088},
}

@inproceedings{lim_building_2021,
	title = {Building {Reliable} {Explanations} of {Unreliable} {Neural} {Networks}: {Locally} {Smoothing} {Perspective} of {Model} {Interpretation}},
	shorttitle = {Building {Reliable} {Explanations} of {Unreliable} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Lim_Building_Reliable_Explanations_of_Unreliable_Neural_Networks_Locally_Smoothing_Perspective_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Lim, Dohun and Lee, Hyeonseok and Kim, Sungchan},
	year = {2021},
	pages = {6468--6477},
}

@inproceedings{lee_relevance-cam_2021,
	title = {Relevance-{CAM}: {Your} {Model} {Already} {Knows} {Where} {To} {Look}},
	shorttitle = {Relevance-{CAM}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Lee_Relevance-CAM_Your_Model_Already_Knows_Where_To_Look_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Lee, Jeong Ryong and Kim, Sewon and Park, Inyong and Eo, Taejoon and Hwang, Dosik},
	year = {2021},
	pages = {14944--14953},
}

@inproceedings{yang_causalvae_2021,
	title = {{CausalVAE}: {Disentangled} {Representation} {Learning} via {Neural} {Structural} {Causal} {Models}},
	shorttitle = {{CausalVAE}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Yang_CausalVAE_Disentangled_Representation_Learning_via_Neural_Structural_Causal_Models_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
	year = {2021},
	pages = {9593--9602},
}

@inproceedings{mackowiak_generative_2021,
	title = {Generative {Classifiers} as a {Basis} for {Trustworthy} {Image} {Classification}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Mackowiak_Generative_Classifiers_as_a_Basis_for_Trustworthy_Image_Classification_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Mackowiak, Radek and Ardizzone, Lynton and Kothe, Ullrich and Rother, Carsten},
	year = {2021},
	pages = {2971--2981},
}

@inproceedings{hendrycks_natural_2021,
	title = {Natural {Adversarial} {Examples}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Hendrycks_Natural_Adversarial_Examples_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-06-16},
	author = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
	year = {2021},
	pages = {15262--15271},
}

@article{chan_redunet_2021,
	title = {{ReduNet}: {A} {White}-box {Deep} {Network} from the {Principle} of {Maximizing} {Rate} {Reduction}},
	shorttitle = {{ReduNet}},
	url = {https://arxiv.org/abs/2105.10446v1},
	abstract = {This work attempts to provide a plausible theoretical framework that aims to interpret modern deep (convolutional) networks from the principles of data compression and discriminative representation. We show that for high-dimensional multi-class data, the optimal linear discriminative representation maximizes the coding rate difference between the whole dataset and the average of all the subsets. We show that the basic iterative gradient ascent scheme for optimizing the rate reduction objective naturally leads to a multi-layer deep network, named ReduNet, that shares common characteristics of modern deep networks. The deep layered architectures, linear and nonlinear operators, and even parameters of the network are all explicitly constructed layer-by-layer via forward propagation, instead of learned via back propagation. All components of so-obtained "white-box" network have precise optimization, statistical, and geometric interpretation. Moreover, all linear operators of the so-derived network naturally become multi-channel convolutions when we enforce classification to be rigorously shift-invariant. The derivation also indicates that such a deep convolution network is significantly more efficient to construct and learn in the spectral domain. Our preliminary simulations and experiments clearly verify the effectiveness of both the rate reduction objective and the associated ReduNet. All code and data are available at https://github.com/Ma-Lab-Berkeley.},
	language = {en},
	urldate = {2021-06-10},
	author = {Chan, Kwan Ho Ryan and Yu, Yaodong and You, Chong and Qi, Haozhi and Wright, John and Ma, Yi},
	month = may,
	year = {2021},
}

@inproceedings{cellier_quantifying_2020,
	address = {Cham},
	title = {Quantifying {Model} {Complexity} via {Functional} {Decomposition} for {Better} {Post}-hoc {Interpretability}},
	volume = {1167},
	isbn = {978-3-030-43822-7 978-3-030-43823-4},
	url = {http://link.springer.com/10.1007/978-3-030-43823-4_17},
	doi = {10.1007/978-3-030-43823-4_17},
	abstract = {Post-hoc model-agnostic interpretation methods such as partial dependence plots can be employed to interpret complex machine learning models. While these interpretation methods can be applied regardless of model complexity, they can produce misleading and verbose results if the model is too complex, especially w.r.t. feature interactions. To quantify the complexity of arbitrary machine learning models, we propose model-agnostic complexity measures based on functional decomposition: number of features used, interaction strength and main eﬀect complexity. We show that post-hoc interpretation of models that minimize the three measures is more reliable and compact. Furthermore, we demonstrate the application of these measures in a multi-objective optimization approach which simultaneously minimizes loss and complexity.},
	language = {en},
	urldate = {2021-06-10},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
	editor = {Cellier, Peggy and Driessens, Kurt},
	year = {2020},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {193--204},
}

@article{hinton_how_2021,
	title = {How to represent part-whole hierarchies in a neural network},
	url = {http://arxiv.org/abs/2102.12627},
	abstract = {This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several diﬀerent groups to be combined into an imaginary system called GLOM1. The advances include transformers, neural ﬁelds, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a ﬁxed architecture parse an image into a partwhole hierarchy which has a diﬀerent structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should signiﬁcantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.},
	language = {en},
	urldate = {2021-06-10},
	journal = {arXiv:2102.12627 [cs]},
	author = {Hinton, Geoffrey},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.12627},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.2.6, I.4.8},
}

@article{alfonseca_superintelligence_2021,
	title = {Superintelligence {Cannot} be {Contained}: {Lessons} from {Computability} {Theory}},
	volume = {70},
	copyright = {Copyright (c)},
	issn = {1076-9757},
	shorttitle = {Superintelligence {Cannot} be {Contained}},
	url = {https://www.jair.org/index.php/jair/article/view/12202},
	doi = {10.1613/jair.1.12202},
	language = {en},
	urldate = {2021-06-10},
	journal = {Journal of Artificial Intelligence Research},
	author = {Alfonseca, Manuel and Cebrian, Manuel and Anta, Antonio Fernandez and Coviello, Lorenzo and Abeliuk, Andrés and Rahwan, Iyad},
	month = jan,
	year = {2021},
	keywords = {computational social systems, mathematical foundations, philosophical foundations},
	pages = {65--76},
}

@inproceedings{singla_second-order_2020,
	title = {Second-{Order} {Provable} {Defenses} against {Adversarial} {Attacks}},
	url = {http://proceedings.mlr.press/v119/singla20a.html},
	abstract = {A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For {\textbackslash}emph\{any\} perturbation of the in...},
	language = {en},
	urldate = {2021-06-10},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Singla, Sahil and Feizi, Soheil},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {8981--8991},
}

@inproceedings{yao_pyhessian_2020,
	title = {{PyHessian}: {Neural} {Networks} {Through} the {Lens} of the {Hessian}},
	shorttitle = {{PyHessian}},
	doi = {10.1109/BigData50022.2020.9378171},
	abstract = {We present PYHESSIAN, a new scalable framework that enables fast computation of Hessian (i.e., second-order derivative) information for deep neural networks. PYHESSIAN enables fast computations of the top Hessian eigenvalues, the Hessian trace, and the full Hessian eigenvalue/spectral density; it supports distributed-memory execution on cloud/supercomputer systems; and it is available as open source [1]. This general framework can be used to analyze neural network models, including the topology of the loss landscape (i.e., curvature information) to gain insight into the behavior of different models/optimizers. As an example, we analyze the effect of residual connections and Batch Normalization layers on the trainability of neural networks. One recent claim, based on simpler first-order analysis, is that residual connections and Batch Normalization make the loss landscape "smoother," thus making it easier for Stochastic Gradient Descent to converge to a good solution. Our second-order analysis, easily enabled by PYHESSIAN, shows new finer-scale insights, demonstrating that while conventional wisdom is sometimes validated, in other cases it is simply incorrect. In particular, we find that Batch Normalization does not necessarily make the loss landscape smoother, especially for shallow networks.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
	month = dec,
	year = {2020},
	keywords = {Analytical models, Artificial neural networks, Big Data, Eigenvalues and eigenfunctions, Electrostatic discharges, Lenses, Training},
	pages = {581--590},
}

@article{lee_first-order_2019,
	title = {First-order methods almost always avoid strict saddle points},
	volume = {176},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-019-01374-3},
	doi = {10.1007/s10107-019-01374-3},
	abstract = {We establish that first-order methods avoid strict saddle points for almost all initializations. Our results apply to a wide variety of first-order methods, including (manifold) gradient descent, block coordinate descent, mirror descent and variants thereof. The connecting thread is that such algorithms can be studied from a dynamical systems perspective in which appropriate instantiations of the Stable Manifold Theorem allow for a global stability analysis. Thus, neither access to second-order derivative information nor randomness beyond initialization is necessary to provably avoid strict saddle points.},
	language = {en},
	number = {1},
	urldate = {2021-06-10},
	journal = {Mathematical Programming},
	author = {Lee, Jason D. and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
	month = jul,
	year = {2019},
	pages = {311--337},
}

@inproceedings{guo_explaining_2018,
	title = {Explaining {Deep} {Learning} {Models} -- {A} {Bayesian} {Non}-parametric {Approach}},
	abstract = {Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Guo, Wenbo and Huang, Sui and Tao, Yunzhe and Xing, Xinyu and Lin, Lin},
	year = {2018},
	pages = {11},
}

@article{goyal_explaining_2020,
	title = {Explaining {Classifiers} with {Causal} {Concept} {Effect} ({CaCE})},
	url = {http://arxiv.org/abs/1907.07165},
	abstract = {How can we understand classiﬁcation decisions made by deep neural networks? Many existing explainability methods rely solely on correlations and fail to account for confounding, which may result in potentially misleading explanations. To overcome this problem, we deﬁne the Causal Concept Effect (CaCE) as the causal effect of (the presence or absence of) a human-interpretable concept on a deep neural net’s predictions. We show that the CaCE measure can avoid errors stemming from confounding. Estimating CaCE is difﬁcult in situations where we cannot easily simulate the do-operator. To mitigate this problem, we use a generative model, speciﬁcally a Variational AutoEncoder (VAE), to measure VAE-CaCE. In an extensive experimental analysis, we show that the VAE-CaCE is able to estimate the true concept causal effect, compared to baselines for a number of datasets including high dimensional images.},
	language = {en},
	urldate = {2021-06-09},
	journal = {arXiv:1907.07165 [cs, stat]},
	author = {Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been},
	month = feb,
	year = {2020},
	note = {arXiv: 1907.07165},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{huang_part-stacked_2016,
	title = {Part-{Stacked} {CNN} for {Fine}-{Grained} {Visual} {Categorization}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Huang_Part-Stacked_CNN_for_CVPR_2016_paper.html},
	urldate = {2021-06-09},
	author = {Huang, Shaoli and Xu, Zhe and Tao, Dacheng and Zhang, Ya},
	year = {2016},
	pages = {1173--1182},
}

@article{zhu_robustness_2019,
	title = {Robustness of {Object} {Recognition} under {Extreme} {Occlusion} in {Humans} and {Computational} {Models}},
	url = {http://arxiv.org/abs/1905.04598},
	abstract = {Most objects in the visual world are partially occluded, but humans can recognize them without difficulty. However, it remains unknown whether object recognition models like convolutional neural networks (CNNs) can handle real-world occlusion. It is also a question whether efforts to make these models robust to constant mask occlusion are effective for real-world occlusion. We test both humans and the above-mentioned computational models in a challenging task of object recognition under extreme occlusion, where target objects are heavily occluded by irrelevant real objects in real backgrounds. Our results show that human vision is very robust to extreme occlusion while CNNs are not, even with modifications to handle constant mask occlusion. This implies that the ability to handle constant mask occlusion does not entail robustness to real-world occlusion. As a comparison, we propose another computational model that utilizes object parts/subparts in a compositional manner to build robustness to occlusion. This performs significantly better than CNN-based models on our task with error patterns similar to humans. These findings suggest that testing under extreme occlusion can better reveal the robustness of visual recognition, and that the principle of composition can encourage such robustness.},
	urldate = {2021-06-09},
	journal = {arXiv:1905.04598 [cs]},
	author = {Zhu, Hongru and Tang, Peng and Park, Jeongho and Park, Soojin and Yuille, Alan},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.04598},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_concept_2020,
	title = {Concept whitening for interpretable image recognition},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00265-z},
	doi = {10.1038/s42256-020-00265-z},
	abstract = {What does a neural network encode about a concept as we traverse through the layers? Interpretability in machine learning is undoubtedly important, but the calculations of neural networks are very challenging to understand. Attempts to see inside their hidden layers can be misleading, unusable or rely on the latent space to possess properties that it may not have. Here, rather than attempting to analyse a neural network post hoc, we introduce a mechanism, called concept whitening (CW), to alter a given layer of the network to allow us to better understand the computation leading up to that layer. When a concept whitening module is added to a convolutional neural network, the latent space is whitened (that is, decorrelated and normalized) and the axes of the latent space are aligned with known concepts of interest. By experiment, we show that CW can provide us with a much clearer understanding of how the network gradually learns concepts over layers. CW is an alternative to a batch normalization layer in that it normalizes, and also decorrelates (whitens), the latent space. CW can be used in any layer of the network without hurting predictive performance.},
	language = {en},
	number = {12},
	urldate = {2021-06-09},
	journal = {Nature Machine Intelligence},
	author = {Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
	month = dec,
	year = {2020},
	note = {Number: 12
Publisher: Nature Publishing Group},
	pages = {772--782},
}

@inproceedings{jacquot_can_2020,
	title = {Can {Deep} {Learning} {Recognize} {Subtle} {Human} {Activities}?},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Jacquot_Can_Deep_Learning_Recognize_Subtle_Human_Activities_CVPR_2020_paper.html},
	urldate = {2021-06-09},
	author = {Jacquot, Vincent and Ying, Zhuofan and Kreiman, Gabriel},
	year = {2020},
	pages = {14244--14253},
}

@inproceedings{kortylewski_compositional_2020,
	title = {Compositional {Convolutional} {Neural} {Networks}: {A} {Deep} {Architecture} {With} {Innate} {Robustness} to {Partial} {Occlusion}},
	shorttitle = {Compositional {Convolutional} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Kortylewski_Compositional_Convolutional_Neural_Networks_A_Deep_Architecture_With_Innate_Robustness_CVPR_2020_paper.html},
	urldate = {2021-06-09},
	author = {Kortylewski, Adam and He, Ju and Liu, Qing and Yuille, Alan L.},
	year = {2020},
	pages = {8940--8949},
}

@article{yuille_deep_2021,
	title = {Deep {Nets}: {What} have {They} {Ever} {Done} for {Vision}?},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {Deep {Nets}},
	url = {https://doi.org/10.1007/s11263-020-01405-z},
	doi = {10.1007/s11263-020-01405-z},
	abstract = {This is an opinion paper about the strengths and weaknesses of Deep Nets for vision. They are at the heart of the enormous recent progress in artificial intelligence and are of growing importance in cognitive science and neuroscience. They have had many successes but also have several limitations and there is limited understanding of their inner workings. At present Deep Nets perform very well on specific visual tasks with benchmark datasets but they are much less general purpose, flexible, and adaptive than the human visual system. We argue that Deep Nets in their current form are unlikely to be able to overcome the fundamental problem of computer vision, namely how to deal with the combinatorial explosion, caused by the enormous complexity of natural images, and obtain the rich understanding of visual scenes that the human visual achieves. We argue that this combinatorial explosion takes us into a regime where “big data is not enough” and where we need to rethink our methods for benchmarking performance and evaluating vision algorithms. We stress that, as vision algorithms are increasingly used in real world applications, that performance evaluation is not merely an academic exercise but has important consequences in the real world. It is impractical to review the entire Deep Net literature so we restrict ourselves to a limited range of topics and references which are intended as entry points into the literature. The views expressed in this paper are our own and do not necessarily represent those of anybody else in the computer vision community.},
	language = {en},
	number = {3},
	urldate = {2021-06-09},
	journal = {International Journal of Computer Vision},
	author = {Yuille, Alan L. and Liu, Chenxi},
	month = mar,
	year = {2021},
	pages = {781--802},
}

@inproceedings{koh_concept_2020,
	title = {Concept {Bottleneck} {Models}},
	url = {http://proceedings.mlr.press/v119/koh20a.html},
	abstract = {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art...},
	language = {en},
	urldate = {2021-06-02},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {5338--5348},
}

@article{nicolae_adversarial_2019,
	title = {Adversarial {Robustness} {Toolbox} v1.0.0},
	url = {http://arxiv.org/abs/1807.01069},
	abstract = {Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (http://adversarial-robustness-toolbox.readthedocs.io).},
	urldate = {2021-05-28},
	journal = {arXiv:1807.01069 [cs, stat]},
	author = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian M. and Edwards, Ben},
	month = nov,
	year = {2019},
	note = {arXiv: 1807.01069},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{montufar_number_2014,
	title = {On the {Number} of {Linear} {Regions} of {Deep} {Neural} {Networks}},
	volume = {27},
	url = {https://papers.nips.cc/paper/2014/hash/109d2dd3608f669ca17920c511c2a41e-Abstract.html},
	language = {en},
	urldate = {2021-05-22},
	journal = {Advances in Neural Information Processing Systems},
	author = {Montufar, Guido F. and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
	year = {2014},
}

@inproceedings{kurakin_placeholder_2017,
	title = {Placeholder {Adversarial} {Examples} in {The} {Physical} {World}},
	abstract = {Most existing machine learning classiﬁers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modiﬁed very slightly in a way that is intended to cause a machine learning classiﬁer to misclassify it. In many cases, these modiﬁcations can be so subtle that a human observer does not even notice the modiﬁcation at all, yet the classiﬁer still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work has assumed a threat model in which the adversary can feed data directly into the machine learning classiﬁer. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from a cell-phone camera to an ImageNet Inception classiﬁer and measuring the classiﬁcation accuracy of the system. We ﬁnd that a large fraction of adversarial examples are classiﬁed incorrectly even when perceived through the camera.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations} {Workshop}},
	author = {Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
	year = {2017},
	pages = {14},
}

@inproceedings{kim_understanding_2020,
	title = {Understanding {Catastrophic} {Overfitting} in {Single}-step {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2010.01799},
	abstract = {Although fast adversarial training has demonstrated both robustness and efﬁciency, the problem of “catastrophic overﬁtting” has been observed. This is a phenomenon in which, during single-step adversarial training, robust accuracy against projected gradient descent (PGD) suddenly decreases to 0\% after a few epochs, whereas robust accuracy against fast gradient sign method (FGSM) increases to 100\%. In this paper, we demonstrate that catastrophic overﬁtting is very closely related to the characteristic of single-step adversarial training which uses only adversarial examples with the maximum perturbation, and not all adversarial examples in the adversarial direction, which leads to decision boundary distortion and a highly curved loss surface. Based on this observation, we propose a simple method that not only prevents catastrophic overﬁtting, but also overrides the belief that it is difﬁcult to prevent multi-step adversarial attacks with single-step adversarial training.},
	language = {en},
	urldate = {2021-02-19},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Kim, Hoki and Lee, Woojin and Lee, Jaewook},
	month = dec,
	year = {2020},
	note = {arXiv: 2010.01799},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@inproceedings{chan_what_2020,
	title = {What {It} {Thinks} {Is} {Important} {Is} {Important}: {Robustness} {Transfers} {Through} {Input} {Gradients}},
	shorttitle = {What {It} {Thinks} {Is} {Important} {Is} {Important}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Chan_What_It_Thinks_Is_Important_Is_Important_Robustness_Transfers_Through_CVPR_2020_paper.html},
	urldate = {2021-04-29},
	author = {Chan, Alvin and Tay, Yi and Ong, Yew-Soon},
	year = {2020},
	pages = {332--341},
}

@inproceedings{croce_reliable_2020,
	title = {Reliable {Evaluation} of {Adversarial} {Robustness} with an {Ensemble} of {Diverse} {Parameter}-free {Attacks}},
	abstract = {The feld of defense strategies against adversarial attacks has signifcantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insuffcient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it diffcult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we frst propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
	language = {en},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	author = {Croce, Francesco and Hein, Matthias},
	year = {2020},
	pages = {11},
}

@article{mosbach_logit_2019,
	title = {Logit {Pairing} {Methods} {Can} {Fool} {Gradient}-{Based} {Attacks}},
	url = {http://arxiv.org/abs/1810.12042},
	abstract = {Recently, Kannan et al. [2018] proposed several logit regularization methods to improve the adversarial robustness of classifiers. We show that the computationally fast methods they propose - Clean Logit Pairing (CLP) and Logit Squeezing (LSQ) - just make the gradient-based optimization problem of crafting adversarial examples harder without providing actual robustness. We find that Adversarial Logit Pairing (ALP) may indeed provide robustness against adversarial examples, especially when combined with adversarial training, and we examine it in a variety of settings. However, the increase in adversarial accuracy is much smaller than previously claimed. Finally, our results suggest that the evaluation against an iterative PGD attack relies heavily on the parameters used and may result in false conclusions regarding robustness of a model.},
	urldate = {2021-04-15},
	journal = {arXiv:1810.12042 [cs, stat]},
	author = {Mosbach, Marius and Andriushchenko, Maksym and Trost, Thomas and Hein, Matthias and Klakow, Dietrich},
	month = mar,
	year = {2019},
	note = {arXiv: 1810.12042},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{croce_scaling_2020,
	title = {Scaling up the {Randomized} {Gradient}-{Free} {Adversarial} {Attack} {Reveals} {Overestimation} of {Robustness} {Using} {Established} {Attacks}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-019-01213-0},
	doi = {10.1007/s11263-019-01213-0},
	abstract = {Modern neural networks are highly non-robust against adversarial manipulation. A significant amount of work has been invested in techniques to compute lower bounds on robustness through formal guarantees and to build provably robust models. However, it is still difficult to get guarantees for larger networks or robustness against larger perturbations. Thus attack strategies are needed to provide tight upper bounds on the actual robustness. We significantly improve the randomized gradient-free attack for ReLU networks (Croce and Hein in GCPR, 2018), in particular by scaling it up to large networks. We show that our attack achieves similar or significantly smaller robust accuracy than state-of-the-art attacks like PGD or the one of Carlini and Wagner, thus revealing an overestimation of the robustness by these state-of-the-art methods. Our attack is not based on a gradient descent scheme and in this sense gradient-free, which makes it less sensitive to the choice of hyperparameters as no careful selection of the stepsize is required.},
	language = {en},
	number = {4},
	urldate = {2021-04-15},
	journal = {International Journal of Computer Vision},
	author = {Croce, Francesco and Rauber, Jonas and Hein, Matthias},
	month = apr,
	year = {2020},
	pages = {1028--1046},
}

@inproceedings{qin_adversarial_2019,
	title = {Adversarial {Robustness} through {Local} {Linearization}},
	abstract = {Adversarial training is an effective methodology to train deep neural networks which are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained signiﬁcantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47\% adversarial accuracy for ImageNet with ∞ adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems} ({NeurIPS} 2019)},
	author = {Qin, Chongli and Martens, James and Gowal, Sven and Krishnan, Dilip and Dvijotham, Krishnamurthy and Fawzi, Alhussein and De, Soham and Stanforth, Robert and Kohli, Pushmeet},
	year = {2019},
	pages = {10},
}

@inproceedings{jetley_friends_2018,
	title = {With {Friends} {Like} {These}, {Who} {Needs} {Adversaries}?},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/803a82dee7e3fbb3438a149508484250-Abstract.html},
	language = {en},
	urldate = {2021-03-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jetley, Saumya and Lord, Nicholas and Torr, Philip},
	year = {2018},
}

@inproceedings{ghorbani_investigation_2019,
	title = {An {Investigation} into {Neural} {Net} {Optimization} via {Hessian} {Eigenvalue} {Density}},
	url = {http://proceedings.mlr.press/v97/ghorbani19b.html},
	abstract = {To understand the dynamics of training in deep neural networks, we study the evolution of the Hessian eigenvalue density throughout the optimization process. In non-batch normalized networks, we ob...},
	language = {en},
	urldate = {2021-03-10},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2232--2241},
}

@inproceedings{wu_adversarial_2020,
	title = {Adversarial {Weight} {Perturbation} {Helps} {Robust} {Generalization}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1ef91c212e30e14bf125e9374262401f-Abstract.html},
	language = {en},
	urldate = {2021-03-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wu, Dongxian and Xia, Shu-Tao and Wang, Yisen},
	year = {2020},
	pages = {2958--2969},
}

@inproceedings{yang_closer_2020,
	title = {A {Closer} {Look} at {Accuracy} vs. {Robustness}},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33 ({NeurIPS} 2020)},
	author = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Ruslan and Chaudhuri, Kamalika},
	year = {2020},
	pages = {14},
}

@inproceedings{moosavi-dezfooli_robustness_2019,
	title = {Robustness via {Curvature} {Regularization}, and {Vice} {Versa}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Moosavi-Dezfooli_Robustness_via_Curvature_Regularization_and_Vice_Versa_CVPR_2019_paper},
	urldate = {2021-02-19},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Uesato, Jonathan and Frossard, Pascal},
	year = {2019},
	pages = {9078--9086},
}

@inproceedings{liang_can_2020,
	title = {Can a {Fruit} {Fly} {Learn} {Word} {Embeddings}?},
	url = {https://openreview.net/forum?id=xfmSoxdxFCG},
	abstract = {The mushroom body of the fruit fly brain is one of the best studied systems in neuroscience. At its core it consists of a population of Kenyon cells, which receive inputs from multiple sensory...},
	language = {en},
	urldate = {2021-02-24},
	author = {Liang, Yuchen and Ryali, Chaitanya and Hoover, Benjamin and Navlakha, Saket and Grinberg, Leopold and Zaki, Mohammed J. and Krotov, Dmitry},
	month = sep,
	year = {2020},
}

@inproceedings{frankle_training_2020,
	title = {Training {BatchNorm} and {Only} {BatchNorm}: {On} the {Expressive} {Power} of {Random} {Features} in {CNNs}},
	shorttitle = {Training {BatchNorm} and {Only} {BatchNorm}},
	url = {https://openreview.net/forum?id=vYeQQ29Tbvx},
	abstract = {A wide variety of deep learning techniques from style transfer to multitask learning rely on training affine transformations of features. Most prominent among these is the popular feature...},
	language = {en},
	urldate = {2021-02-24},
	author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	month = sep,
	year = {2020},
}

@inproceedings{xiao_noise_2020,
	title = {Noise or {Signal}: {The} {Role} of {Image} {Backgrounds} in {Object} {Recognition}},
	shorttitle = {Noise or {Signal}},
	url = {https://openreview.net/forum?id=gl3D-xY7wLq},
	abstract = {We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet...},
	language = {en},
	urldate = {2021-02-23},
	author = {Xiao, Kai Yuanqing and Engstrom, Logan and Ilyas, Andrew and Madry, Aleksander},
	month = sep,
	year = {2020},
}

@inproceedings{wang_towards_2020,
	title = {Towards {A} {Unified} {Understanding} and {Improving} of {Adversarial} {Transferability}},
	url = {https://openreview.net/forum?id=X76iqnUbBjz},
	abstract = {In this paper, we use the interaction inside adversarial perturbations to explain and boost the adversarial transferability. We discover and prove the negative correlation between the adversarial...},
	language = {en},
	urldate = {2021-02-23},
	author = {Wang, Xin and Ren, Jie and Lin, Shuyun and Zhu, Xiangming and Wang, Yisen and Zhang, Quanshi},
	month = sep,
	year = {2020},
}

@inproceedings{utrera_adversarially-trained_2020,
	title = {Adversarially-{Trained} {Deep} {Nets} {Transfer} {Better}},
	url = {https://openreview.net/forum?id=ijJZbomCJIm},
	abstract = {Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network...},
	language = {en},
	urldate = {2021-02-23},
	author = {Utrera, Francisco and Kravitz, Evan and Erichson, N. Benjamin and Khanna, Rajiv and Mahoney, Michael W.},
	month = sep,
	year = {2020},
}

@inproceedings{nguyen_wide_2020,
	title = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}? {Uncovering} {How} {Neural} {Network} {Representations} {Vary} with {Width} and {Depth}},
	shorttitle = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}?},
	url = {https://openreview.net/forum?id=KJNcAkY8tY4},
	abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design...},
	language = {en},
	urldate = {2021-02-23},
	author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
	month = sep,
	year = {2020},
}

@inproceedings{bahadori_debiasing_2020,
	title = {Debiasing {Concept}-based {Explanations} with {Causal} {Analysis}},
	url = {https://openreview.net/forum?id=6puUoArESGp},
	abstract = {Studying the concept-based explanation techniques, we provided evidences for potential existence of spurious association between the features and concepts due to  unobserved latent variables or...},
	language = {en},
	urldate = {2021-02-23},
	author = {Bahadori, Mohammad Taha and Heckerman, David},
	month = sep,
	year = {2020},
}

@inproceedings{pope_intrinsic_2020,
	title = {The {Intrinsic} {Dimension} of {Images} and {Its} {Impact} on {Learning}},
	url = {https://openreview.net/forum?id=XJk19XzGq2J},
	abstract = {It is widely believed that natural image data exhibits low-dimensional structure despite being embedded in a high-dimensional pixel space. This idea underlies a common intuition for the success of...},
	language = {en},
	urldate = {2021-02-23},
	author = {Pope, Phil and Zhu, Chen and Abdelkader, Ahmed and Goldblum, Micah and Goldstein, Tom},
	month = sep,
	year = {2020},
}

@inproceedings{zhang_how_2020,
	title = {How {Does} {Mixup} {Help} {With} {Robustness} and {Generalization}?},
	url = {https://openreview.net/forum?id=8yKEo06dKNo},
	abstract = {Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's...},
	language = {en},
	urldate = {2021-02-23},
	author = {Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Ghorbani, Amirata and Zou, James},
	month = sep,
	year = {2020},
}

@inproceedings{bai_improving_2020,
	title = {Improving {Adversarial} {Robustness} via {Channel}-wise {Activation} {Suppressing}},
	url = {https://openreview.net/forum?id=zQTezqCCtNx},
	abstract = {The study of adversarial examples and their activations have attracted significant attention for secure and robust learning with deep neural networks (DNNs).  Different from existing works, in this...},
	language = {en},
	urldate = {2021-02-23},
	author = {Bai, Yang and Zeng, Yuyuan and Jiang, Yong and Xia, Shu-Tao and Ma, Xingjun and Wang, Yisen},
	month = sep,
	year = {2020},
}

@inproceedings{glorot_deep_2011,
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v15/glorot11a.html},
	abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neu...},
	language = {en},
	urldate = {2021-01-29},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	month = jun,
	year = {2011},
	note = {ISSN: 1938-7228},
	pages = {315--323},
}

@inproceedings{yao_hessian-based_2018,
	title = {Hessian-based {Analysis} of {Large} {Batch} {Training} and {Robustness} to {Adversaries}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html},
	language = {en},
	urldate = {2021-01-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W.},
	year = {2018},
	pages = {4949--4959},
}

@inproceedings{liu_loss_2020,
	title = {On the {Loss} {Landscape} of {Adversarial} {Training}: {Identifying} {Challenges} and {How} to {Overcome} {Them}},
	volume = {33},
	shorttitle = {On the {Loss} {Landscape} of {Adversarial} {Training}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f56d8183992b6c54c92c16a8519a6e2b-Abstract.html},
	language = {en},
	urldate = {2021-01-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Liu, Chen and Salzmann, Mathieu and Lin, Tao and Tomioka, Ryota and Süsstrunk, Sabine},
	year = {2020},
}

@article{pearlmutter_fast_1994,
	title = {Fast {Exact} {Multiplication} by the {Hessian}},
	volume = {6},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1994.6.1.147},
	doi = {10.1162/neco.1994.6.1.147},
	abstract = {Just storing the Hessian H (the matrix of second derivatives δ2E/δwiδwj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rv\{f(w)\} = (δ/δr)f(w + rv){\textbar}r=0, note that Rv\{▽w\} = Hv and Rv\{w\} = v, and then apply Rv\{·\} to the equations used to compute ▽w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian.},
	number = {1},
	urldate = {2021-01-20},
	journal = {Neural Computation},
	author = {Pearlmutter, Barak A.},
	month = jan,
	year = {1994},
	note = {Publisher: MIT Press},
	pages = {147--160},
}

@inproceedings{simon-gabriel_first-order_2019,
	title = {First-{Order} {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://proceedings.mlr.press/v97/simon-gabriel19a.html},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversa...},
	language = {en},
	urldate = {2021-01-19},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Leon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5809--5817},
}

@article{lee_gradient_2021,
	title = {Gradient {Masking} of {Label} {Smoothing} in {Adversarial} {Robustness}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3048120},
	abstract = {Deep neural networks (DNNs) have achieved impressive results in several image classification tasks. However, these architectures are unstable for adversarial examples (AEs) such as inputs crafted by a hardly perceptible perturbation with the intent of causing neural networks to make errors. AEs must be considered to prevent accidents in areas such as unmanned car driving using visual object detection in Internet of Things (IoT) networks. Gaussian noise with label smoothing or logit squeezing can be used to increase the robustness against AEs in the training of DNNs. However, from a model interpretability aspect, Gaussian noise with label smoothing does not increase the adversarial robustness of the model. To resolve this problem, we tackle the AE instead of measuring the accuracy of the model against AEs. Considering that a robust model shows a small curvature of the loss surface, we propose a metric to measure the strength of the AEs and the robustness of the model. Furthermore, we introduce a method to verify the existence of the obfuscated gradients of the model based on the black-box attack sanity check method. The proposed method enables us to identify a gradient masking problem wherein the model does not provide useful gradients and exploits false defenses. We evaluate our technique against representative adversarially trained models using the CIFAR10, CIFAR100, SVHN, and Restricted ImageNet datasets. Our results show that the performance of some false defense models decreases by up to 32\% compared to the previous evaluation metrics. Moreover, our metric reveals that traditional metrics used to measure the robustness of the model may produce false results.},
	journal = {IEEE Access},
	author = {Lee, H. and Bae, H. and Yoon, S.},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Adversarial learning, Data models, Gaussian noise, IoT, IoT security, Measurement, Perturbation methods, Robustness, Smoothing methods, Training, deep learning, evasion attack, gradient masking, interpretability, label smoothing},
	pages = {6453--6464},
}

@article{su_one_2019,
	title = {One {Pixel} {Attack} for {Fooling} {Deep} {Neural} {Networks}},
	volume = {23},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2019.2890858},
	abstract = {Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97\% of the natural images in Kaggle CIFAR-10 test dataset and 16.04\% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03\% and 22.91\% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
	number = {5},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Su, J. and Vargas, D. V. and Sakurai, K.},
	month = oct,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Evolutionary Computation},
	keywords = {Additives, CIFAR-10 dataset, Convolutional neural network, DNN, ILSVRC 2012, Image color analysis, Image recognition, ImageNet test images, Neural networks, Perturbation methods, Robustness, adversarial information, adversarial machine learning, deep neural networks, differential evolution, differential evolution (DE), evolutionary computation, extreme limited scenario, feature extraction, image classification, image recognition, information security, input vector, learning (artificial intelligence), low dimension attacks, low-cost adversarial attacks, natural images, neural nets, object recognition, one-pixel adversarial perturbations, pixel attack},
	pages = {828--841},
}

@inproceedings{scholbeck_sampling_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Sampling, {Intervention}, {Prediction}, {Aggregation}: {A} {Generalized} {Framework} for {Model}-{Agnostic} {Interpretations}},
	isbn = {978-3-030-43823-4},
	shorttitle = {Sampling, {Intervention}, {Prediction}, {Aggregation}},
	doi = {10.1007/978-3-030-43823-4_18},
	abstract = {Model-agnostic interpretation techniques allow us to explain the behavior of any predictive model. Due to different notations and terminology, it is difficult to see how they are related. A unified view on these methods has been missing. We present the generalized SIPA (sampling, intervention, prediction, aggregation) framework of work stages for model-agnostic interpretations and demonstrate how several prominent methods for feature effects can be embedded into the proposed framework. Furthermore, we extend the framework to feature importance computations by pointing out how variance-based and performance-based importance measures are based on the same work stages. The SIPA framework reduces the diverse set of model-agnostic techniques to a single methodology and establishes a common terminology to discuss them in future work.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer International Publishing},
	author = {Scholbeck, Christian A. and Molnar, Christoph and Heumann, Christian and Bischl, Bernd and Casalicchio, Giuseppe},
	editor = {Cellier, Peggy and Driessens, Kurt},
	year = {2020},
	keywords = {Explainable AI, Feature Effect, Feature Importance, Interpretable Machine Learning, Model-Agnostic, Partial Dependence},
	pages = {205--216},
}

@inproceedings{nam_relative_2020,
	title = {Relative {Attributing} {Propagation}: {Interpreting} the {Comparative} {Contributions} of {Individual} {Units} in {Deep} {Neural} {Networks}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	shorttitle = {Relative {Attributing} {Propagation}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/5632},
	doi = {10.1609/aaai.v34i03.5632},
	language = {en},
	urldate = {2021-01-17},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Nam, Woo-Jeoung and Gur, Shir and Choi, Jaesik and Wolf, Lior and Lee, Seong-Whan},
	month = apr,
	year = {2020},
	note = {Number: 03},
	pages = {2501--2508},
}

@inproceedings{ancona_towards_2018,
	title = {Towards better understanding of gradient-based attribution methods for {Deep} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=Sy21R9JAW&noteId=Sy21R9JAW},
	abstract = {Four existing backpropagation-based attribution methods are fundamentally similar. How to assess it?},
	language = {en},
	urldate = {2021-01-17},
	author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
	month = feb,
	year = {2018},
}

@article{lundberg_local_2020,
	title = {From local explanations to global understanding with explainable {AI} for trees},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-019-0138-9},
	doi = {10.1038/s42256-019-0138-9},
	abstract = {Tree-based machine learning models such as random forests, decision trees and gradient boosted trees are popular nonlinear predictive models, yet comparatively little attention has been paid to explaining their predictions. Here we improve the interpretability of tree-based models through three main contributions. (1) A polynomial time algorithm to compute optimal explanations based on game theory. (2) A new type of explanation that directly measures local feature interaction effects. (3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to (1) identify high-magnitude but low-frequency nonlinear mortality risk factors in the US population, (2) highlight distinct population subgroups with shared risk characteristics, (3) identify nonlinear interaction effects among risk factors for chronic kidney disease and (4) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model’s performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
	language = {en},
	number = {1},
	urldate = {2021-01-17},
	journal = {Nature Machine Intelligence},
	author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
	month = jan,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {56--67},
}

@article{merrick_randomized_2019,
	title = {Randomized {Ablation} {Feature} {Importance}},
	url = {http://arxiv.org/abs/1910.00174},
	abstract = {Given a model \$f\$ that predicts a target \$y\$ from a vector of input features \${\textbackslash}pmb\{x\} = x\_1, x\_2, {\textbackslash}ldots, x\_M\$, we seek to measure the importance of each feature with respect to the model's ability to make a good prediction. To this end, we consider how (on average) some measure of goodness or badness of prediction (which we term "loss" \${\textbackslash}ell\$), changes when we hide or ablate each feature from the model. To ablate a feature, we replace its value with another possible value randomly. By averaging over many points and many possible replacements, we measure the importance of a feature on the model's ability to make good predictions. Furthermore, we present statistical measures of uncertainty that quantify how confident we are that the feature importance we measure from our finite dataset and finite number of ablations is close to the theoretical true importance value.},
	urldate = {2021-01-17},
	journal = {arXiv:1910.00174 [cs, stat]},
	author = {Merrick, Luke},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.00174},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {0004-3702},
	shorttitle = {Explanation in artificial intelligence},
	url = {http://www.sciencedirect.com/science/article/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	language = {en},
	urldate = {2021-01-17},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	keywords = {Explainability, Explainable AI, Explanation, Interpretability, Transparency},
	pages = {1--38},
}

@inproceedings{garreau_explaining_2020,
	title = {Explaining the {Explainer}: {A} {First} {Theoretical} {Analysis} of {LIME}},
	shorttitle = {Explaining the {Explainer}},
	url = {http://proceedings.mlr.press/v108/garreau20a.html},
	abstract = {Machine learning is used more and more often for sensitive applications, sometimes replacing humans in critical decision-making processes. As such, interpretability of these algorithms is a pressin...},
	language = {en},
	urldate = {2021-01-17},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Garreau, Damien and Luxburg, Ulrike},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1287--1296},
}

@article{sokol_one_2020,
	title = {One {Explanation} {Does} {Not} {Fit} {All}},
	volume = {34},
	issn = {1610-1987},
	url = {https://doi.org/10.1007/s13218-020-00637-y},
	doi = {10.1007/s13218-020-00637-y},
	abstract = {The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.},
	language = {en},
	number = {2},
	urldate = {2021-01-17},
	journal = {KI - Künstliche Intelligenz},
	author = {Sokol, Kacper and Flach, Peter},
	month = jun,
	year = {2020},
	pages = {235--250},
}

@inproceedings{sokol_explainability_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Explainability fact sheets: a framework for systematic assessment of explainable approaches},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Explainability fact sheets},
	url = {https://doi.org/10.1145/3351095.3372870},
	doi = {10.1145/3351095.3372870},
	abstract = {Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Sokol, Kacper and Flach, Peter},
	month = jan,
	year = {2020},
	keywords = {AI, ML, desiderata, explainability, fact sheet, interpretability, taxonomy, transparency, work sheet},
	pages = {56--67},
}

@inproceedings{poyiadzi_face_2020,
	address = {New York, NY, USA},
	series = {{AIES} '20},
	title = {{FACE}: {Feasible} and {Actionable} {Counterfactual} {Explanations}},
	isbn = {978-1-4503-7110-0},
	shorttitle = {{FACE}},
	url = {https://doi.org/10.1145/3375627.3375850},
	doi = {10.1145/3375627.3375850},
	abstract = {Work in Counterfactual Explanations tends to focus on the principle of "the closest possible world" that identifies small changes leading to the desired outcome. In this paper we argue that while this approach might initially seem intuitively appealing it exhibits shortcomings not addressed in the current literature. First, a counterfactual example generated by the state-of-the-art systems is not necessarily representative of the underlying data distribution, and may therefore prescribe unachievable goals (e.g., an unsuccessful life insurance applicant with severe disability may be advised to do more sports). Secondly, the counterfactuals may not be based on a "feasible path" between the current state of the subject and the suggested one, making actionable recourse infeasible (e.g., low-skilled unsuccessful mortgage applicants may be told to double their salary, which may be hard without first increasing their skill level). These two shortcomings may render counterfactual explanations impractical and sometimes outright offensive. To address these two major flaws, first of all, we propose a new line of Counterfactual Explanations research aimed at providing actionable and feasible paths to transform a selected instance into one that meets a certain goal. Secondly, we propose FACE: an algorithmically sound way of uncovering these "feasible paths" based on the shortest path distances defined via density-weighted metrics. Our approach generates counterfactuals that are coherent with the underlying data distribution and supported by the "feasible paths" of change, which are achievable and can be tailored to the problem at hand.},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and De Bie, Tijl and Flach, Peter},
	month = feb,
	year = {2020},
	keywords = {black-box models, counterfactuals, explainability, interpretability},
	pages = {344--350},
}

@inproceedings{le_grace_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {{GRACE}: {Generating} {Concise} and {Informative} {Contrastive} {Sample} to {Explain} {Neural} {Network} {Model}'s {Prediction}},
	isbn = {978-1-4503-7998-4},
	shorttitle = {{GRACE}},
	url = {https://doi.org/10.1145/3394486.3403066},
	doi = {10.1145/3394486.3403066},
	abstract = {Despite the recent development in the topic of explainable AI/ML for image and text data, the majority of current solutions are not suitable to explain the prediction of neural network models when the datasets are tabular and their features are in high-dimensional vectorized formats. To mitigate this limitation, therefore, we borrow two notable ideas (i.e., "explanation by intervention" from causality and "explanation are contrastive" from philosophy) and propose a novel solution, named as GRACE, that better explains neural network models' predictions for tabular datasets. In particular, given a model's prediction as label X, GRACE intervenes and generates a minimally-modified contrastive sample to be classified as Y, with an intuitive textual explanation, answering the question of "Why X rather than Y?" We carry out comprehensive experiments using eleven public datasets of different scales and domains (e.g., \# of features ranges from 5 to 216) and compare GRACE with competing baselines on different measures: fidelity, conciseness, info-gain, and influence. The user-studies show that our generated explanation is not only more intuitive and easy-to-understand but also facilitates end-users to make as much as 60\% more accurate post-explanation decisions than that of Lime.},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Le, Thai and Wang, Suhang and Lee, Dongwon},
	month = aug,
	year = {2020},
	keywords = {contrastive samples, counterfactual samples, data generation, deep learning, explainability, interpretability, neural networks},
	pages = {238--248},
}

@inproceedings{rasouli_explan_2020,
	title = {{EXPLAN}: {Explaining} {Black}-box {Classifiers} using {Adaptive} {Neighborhood} {Generation}},
	shorttitle = {{EXPLAN}},
	doi = {10.1109/IJCNN48605.2020.9206710},
	abstract = {Defining a representative locality is an urgent challenge in perturbation-based explanation methods, which influences the fidelity and soundness of explanations. We address this issue by proposing a robust and intuitive approach for EXPLaining black-box classifiers using Adaptive Neighborhood generation (EXPLAN). EXPLAN is a module-based algorithm consisted of dense data generation, representative data selection, data balancing, and rule-based interpretable model. It takes into account the adjacency information derived from the black-box decision function and the structure of the data for creating a representative neighborhood for the instance being explained. As a local model-agnostic explanation method, EXPLAN generates explanations in the form of logical rules that are highly interpretable and well-suited for qualitative analysis of the model's behavior. We discuss fidelity-interpretability trade-offs and demonstrate the performance of the proposed algorithm by a comprehensive comparison with state-of-the-art explanation methods LIME, LORE, and Anchor. The conducted experiments on real-world data sets show our method achieves solid empirical results in terms of fidelity, precision, and stability of explanations.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Rasouli, P. and Yu, I. C.},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {Adaptive Neighborhood generation, Analytical models, Data Sampling, Data models, Decision trees, EXPLAN, Interpretable Machine Learning, Machine learning, Neural networks, Perturbation-based Explanation Methods, Predictive models, Training data, XAI, black-box decision function, data balancing, data structure, data structures, dense data generation, explaining black-box classifiers, explanation, feature selection, knowledge based systems, learning (artificial intelligence), local model-agnostic explanation, pattern classification, perturbation-based explanation, representative data selection, representative neighborhood, rule-based interpretable model},
	pages = {1--9},
}

@book{samek_explainable_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	volume = {11700},
	isbn = {978-3-030-28953-9 978-3-030-28954-6},
	shorttitle = {Explainable {AI}},
	url = {http://link.springer.com/10.1007/978-3-030-28954-6},
	language = {en},
	urldate = {2021-01-16},
	publisher = {Springer International Publishing},
	editor = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Müller, Klaus-Robert},
	year = {2019},
	doi = {10.1007/978-3-030-28954-6},
}

@inproceedings{kenny_twin-systems_2019,
	address = {Macao, China},
	title = {Twin-{Systems} to {Explain} {Artificial} {Neural} {Networks} using {Case}-{Based} {Reasoning}: {Comparative} {Tests} of {Feature}-{Weighting} {Methods} in {ANN}-{CBR} {Twins} for {XAI}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {Twin-{Systems} to {Explain} {Artificial} {Neural} {Networks} using {Case}-{Based} {Reasoning}},
	url = {https://www.ijcai.org/proceedings/2019/376},
	doi = {10.24963/ijcai.2019/376},
	abstract = {In this paper, twin-systems are described to address the eXplainable artiﬁcial intelligence (XAI) problem, where a black box model is mapped to a white box “twin” that is more interpretable, with both systems using the same dataset. The framework is instantiated by twinning an artiﬁcial neural network (ANN; black box) with a case-based reasoning system (CBR; white box), and mapping the feature weights from the former to the latter to ﬁnd cases that explain the ANN’s outputs. Using a novel evaluation method, the effectiveness of this twin-system approach is demonstrated by showing that nearest neighbor cases can be found to match the ANN predictions for benchmark datasets. Several feature-weighting methods are competitively tested in two experiments, including our novel, contributions-based method (called COLE) that is found to perform best. The tests consider the ”twinning” of traditional multilayer perceptron (MLP) networks and convolutional neural networks (CNN) with CBR systems. For the CNNs trained on image data, qualitative evidence shows that cases provide plausible explanations for the CNN’s classiﬁcations.},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Kenny, Eoin M. and Keane, Mark T.},
	month = aug,
	year = {2019},
	pages = {2708--2715},
}

@inproceedings{papenmeier_how_2019,
	title = {How model accuracy and explanation fidelity influence user trust in {AI}},
	url = {https://research.utwente.nl/en/publications/how-model-accuracy-and-explanation-fidelity-influence-user-trust-},
	language = {English},
	urldate = {2021-01-16},
	author = {Papenmeier, Andrea and Englebienne, Gwenn and Seifert, Christin},
	year = {2019},
}

@inproceedings{gilpin_explaining_2018,
	title = {Explaining {Explanations} to {Society}},
	abstract = {There is a disconnect between explanatory artiﬁcial intelligence (XAI) methods and the types of explanations that are useful for and demanded by society (policy makers, government ofﬁcials, etc.) Questions that experts in artiﬁcial intelligence (AI) ask opaque systems provide inside explanations, focused on debugging, reliability, and validation. These are different from those that society will ask of these systems to build trust and conﬁdence in their decisions. Although explanatory AI systems can answer many questions that experts desire, they often don’t explain why they made decisions in a way that is precise (true to the model) and understandable to humans. These outside explanations can be used to build trust, comply with regulatory and policy changes, and act as external validation. In this paper, we focus on XAI methods for deep neural networks (DNNs) because of DNNs’ use in decision-making and inherent opacity. We explore the types of questions that explanatory DNN systems can answer and discuss challenges in building explanatory systems that provide outside explanations for societal requirements and beneﬁt.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Gilpin, Leilani H and Testart, Cecilia and Fruchter, Nathaniel and Adebayo, Julius},
	year = {2018},
	pages = {6},
}

@article{roscher_explainable_2020,
	title = {Explainable {Machine} {Learning} for {Scientific} {Insights} and {Discoveries}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2976199},
	abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.},
	journal = {IEEE Access},
	author = {Roscher, R. and Bohn, B. and Duarte, M. F. and Garcke, J.},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Approximation algorithms, Biological system modeling, Data mining, Data models, Explainable machine learning, Kernel, Machine learning, Mathematical model, explainability, informed machine learning, interpretability, learning (artificial intelligence), machine learning methods, natural sciences, natural sciences computing, scientific consistency, scientific discoveries, scientific insights, scientific outcome, transparency},
	pages = {42200--42216},
}

@inproceedings{mittelstadt_explaining_2019,
	address = {New York, NY, USA},
	series = {{FAT}* '19},
	title = {Explaining {Explanations} in {AI}},
	isbn = {978-1-4503-6125-5},
	url = {https://doi.org/10.1145/3287560.3287574},
	doi = {10.1145/3287560.3287574},
	abstract = {Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that "All models are wrong but some are useful." We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a "do it yourself kit" for explanations, allowing a practitioner to directly answer "what if questions" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.},
	urldate = {2021-01-15},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
	month = jan,
	year = {2019},
	keywords = {Accountability, Explanations, Interpretability, Philosophy of Science},
	pages = {279--288},
}

@inproceedings{dhurandhar_explanations_2018,
	title = {Explanations based on the {Missing}: {Towards} {Contrastive} {Explanations} with {Pertinent} {Negatives}},
	volume = {31},
	shorttitle = {Explanations based on the {Missing}},
	url = {https://papers.nips.cc/paper/2018/hash/c5ff2543b53f4cc0ad3819a36752467b-Abstract.html},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
	year = {2018},
	pages = {592--603},
}

@inproceedings{lucic_why_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Why does my model fail? contrastive local explanations for retail forecasting},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Why does my model fail?},
	url = {https://doi.org/10.1145/3351095.3372824},
	doi = {10.1145/3351095.3372824},
	abstract = {In various business settings, there is an interest in using more complex machine learning techniques for sales forecasting. It is difficult to convince analysts, along with their superiors, to adopt these techniques since the models are considered to be "black boxes," even if they perform better than current models in use. We examine the impact of contrastive explanations about large errors on users' attitudes towards a "black-box" model. We propose an algorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error, MC-BRP determines (1) feature values that would result in a reasonable prediction, and (2) general trends between each feature and the target, both based on Monte Carlo simulations. We evaluate on a real dataset with real users by conducting a user study with 75 participants to determine if explanations generated by MC-BRP help users understand why a prediction results in a large error, and if this promotes trust in an automatically-learned model. Our study shows that users are able to answer objective questions about the model's predictions with overall 81.1\% accuracy when provided with these contrastive explanations. We show that users who saw MC-BRP explanations understand why the model makes large errors in predictions significantly more than users in the control group. We also conduct an in-depth analysis of the difference in attitudes between Practitioners and Researchers, and confirm that our results hold when conditioning on the users' background.},
	urldate = {2021-01-15},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Lucic, Ana and Haned, Hinda and de Rijke, Maarten},
	month = jan,
	year = {2020},
	keywords = {erroneous predictions, explainability, interpretability},
	pages = {90--98},
}

@article{guidotti_survey_2018,
	title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
	volume = {51},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3236009},
	doi = {10.1145/3236009},
	abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
	number = {5},
	urldate = {2021-01-16},
	journal = {ACM Computing Surveys},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	month = aug,
	year = {2018},
	keywords = {Open the black box, explanations, interpretability, transparent models},
	pages = {93:1--93:42},
}

@inproceedings{wang_reinforcement_2018,
	title = {A {Reinforcement} {Learning} {Framework} for {Explainable} {Recommendation}},
	doi = {10.1109/ICDM.2018.00074},
	abstract = {Explainable recommendation, which provides explanations about why an item is recommended, has attracted increasing attention due to its ability in helping users make better decisions and increasing users' trust in the system. Existing explainable recommendation methods either ignore the working mechanism of the recommendation model or are designed for a specific recommendation model. Moreover, it is difficult for existing methods to ensure the presentation quality of the explanations (e.g., consistency). To solve these problems, we design a reinforcement learning framework for explainable recommendation. Our framework can explain any recommendation model (model-agnostic) and can flexibly control the explanation quality based on the application scenario. To demonstrate the effectiveness of our framework, we show how it can be used for generating sentence-level explanations. Specifically, we instantiate the explanation generator in the framework with a personalized-attention-based neural network. Offline experiments demonstrate that our method can well explain both collaborative filtering methods and deep-learning-based models. Evaluation with human subjects shows that the explanations generated by our method are significantly more useful than the explanations generated by the baselines.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Wang, X. and Chen, Y. and Yang, J. and Wu, L. and Wu, Z. and Xie, X.},
	month = nov,
	year = {2018},
	note = {ISSN: 2374-8486},
	keywords = {Collaboration, Explainable recommendation, reinforcement learning, personalized explanation, attention networks, Neural networks, Predictive models, Quality control, Recommender systems, Reinforcement learning, Transforms, collaborative filtering methods, deep-learning-based models, explainable recommendation methods, explanation generator, explanation quality, learning (artificial intelligence), neural nets, personalized-attention-based neural network, recommendation model, recommender systems, reinforcement learning framework, sentence-level explanations},
	pages = {587--596},
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2021-01-16},
	journal = {arXiv:1806.01261 [cs, stat]},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.01261},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{barrett_measuring_2018,
	title = {Measuring abstract reasoning in neural networks},
	url = {http://proceedings.mlr.press/v80/barrett18a.html},
	abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe ab...},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Barrett, David and Hill, Felix and Santoro, Adam and Morcos, Ari and Lillicrap, Timothy},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {511--520},
}

@inproceedings{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	url = {https://openreview.net/forum?id=BJJsrmfCZ},
	abstract = {A summary of automatic differentiation techniques employed in PyTorch library, including novelties like support for in-place modification in presence of objects aliasing the same data, performance...},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {31st {Conference} on {Neural} {Information} {Processing} {Systems} {Workshop}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	month = oct,
	year = {2017},
}

@article{baydin_automatic_2017,
	title = {Automatic differentiation in machine learning: a survey},
	volume = {18},
	issn = {1532-4435},
	shorttitle = {Automatic differentiation in machine learning},
	abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Baydin, Atılım Günes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
	month = jan,
	year = {2017},
	keywords = {backpropagation, differentiable programming},
	pages = {5595--5637},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, J. and Dong, W. and Socher, R. and Li, L. and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, ImageNet database, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine, computer vision, image resolution, image retrieval, large-scale hierarchical image database, large-scale ontology, multimedia computing, multimedia data, ontologies (artificial intelligence), subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://papers.nips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	pages = {8026--8037},
}

@inproceedings{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} {System} for {Large}-{Scale} {Machine} {Learning}},
	isbn = {978-1-931971-33-1},
	shorttitle = {{TensorFlow}},
	url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
	language = {en},
	urldate = {2021-01-16},
	author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	year = {2016},
	pages = {265--283},
}

@inproceedings{tomsett_why_2018,
	title = {Why the {Failure}? {How} {Adversarial} {Examples} {Can} {Provide} {Insights} for {Interpretable} {Machine} {Learning}},
	shorttitle = {Why the {Failure}?},
	doi = {10.23919/ICIF.2018.8455710},
	abstract = {Recent advances in Machine Learning (ML) have profoundly changed many detection, classification, recognition and inference tasks. Given the complexity of the battlespace, ML has the potential to revolutionise how Coalition Situation Understanding is synthesised and revised. However, many issues must be overcome before its widespread adoption. In this paper we consider two - interpretability and adversarial attacks. Interpretability is needed because military decision-makers must be able to justify their decisions. Adversarial attacks arise because many ML algorithms are very sensitive to certain kinds of input perturbations. In this paper, we argue that these two issues are conceptually linked, and insights in one can provide insights in the other. We illustrate these ideas with relevant examples from the literature and our own experiments.},
	booktitle = {2018 21st {International} {Conference} on {Information} {Fusion} ({FUSION})},
	author = {Tomsett, R. and Widdicombe, A. and Xing, T. and Chakraborty, S. and Julier, S. and Gurram, P. and Rao, R. and Srivastava, M.},
	month = jul,
	year = {2018},
	keywords = {AI alignment, Coalition Situation Understanding, Data models, Internet, ML algorithms, Machine learning, Measurement, Sensors, Task analysis, Taxonomy, adversarial attacks, adversarial examples, adversarial machine learning, decision making, deep learning, explainable AI, inference mechanisms, inference tasks, internet of battlefield things, interpretability, interpretable machine learning, learning (artificial intelligence), military computing, military decision-makers},
	pages = {838--845},
}

@article{sangkloy_sketchy_2016,
	title = {The sketchy database: learning to retrieve badly drawn bunnies},
	volume = {35},
	issn = {0730-0301},
	shorttitle = {The sketchy database},
	url = {https://doi.org/10.1145/2897824.2925954},
	doi = {10.1145/2897824.2925954},
	abstract = {We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.},
	number = {4},
	urldate = {2021-01-16},
	journal = {ACM Transactions on Graphics},
	author = {Sangkloy, Patsorn and Burnell, Nathan and Ham, Cusuh and Hays, James},
	month = jul,
	year = {2016},
	keywords = {deep learning, image synthesis, siamese network, sketch-based image retrieval, triplet network},
	pages = {119:1--119:12},
}

@article{mytkowicz_producing_2009,
	title = {Producing wrong data without doing anything obviously wrong!},
	volume = {44},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/1508284.1508275},
	doi = {10.1145/1508284.1508275},
	abstract = {This paper presents a surprising result: changing a seemingly innocuous aspect of an experimental setup can cause a systems researcher to draw wrong conclusions from an experiment. What appears to be an innocuous aspect in the experimental setup may in fact introduce a significant bias in an evaluation. This phenomenon is called measurement bias in the natural and social sciences. Our results demonstrate that measurement bias is significant and commonplace in computer system evaluation. By significant we mean that measurement bias can lead to a performance analysis that either over-states an effect or even yields an incorrect conclusion. By commonplace we mean that measurement bias occurs in all architectures that we tried (Pentium 4, Core 2, and m5 O3CPU), both compilers that we tried (gcc and Intel's C compiler), and most of the SPEC CPU2006 C programs. Thus, we cannot ignore measurement bias. Nevertheless, in a literature survey of 133 recent papers from ASPLOS, PACT, PLDI, and CGO, we determined that none of the papers with experimental results adequately consider measurement bias. Inspired by similar problems and their solutions in other sciences, we describe and demonstrate two methods, one for detecting (causal analysis) and one for avoiding (setup randomization) measurement bias.},
	number = {3},
	urldate = {2021-01-16},
	journal = {ACM SIGPLAN Notices},
	author = {Mytkowicz, Todd and Diwan, Amer and Hauswirth, Matthias and Sweeney, Peter F.},
	month = mar,
	year = {2009},
	keywords = {bias, measurement, performance},
	pages = {265--276},
}

@article{fleming_how_1986,
	title = {How not to lie with statistics: the correct way to summarize benchmark results},
	volume = {29},
	issn = {0001-0782},
	shorttitle = {How not to lie with statistics},
	url = {https://doi.org/10.1145/5666.5673},
	doi = {10.1145/5666.5673},
	abstract = {Using the arithmetic mean to summarize normalized benchmark results leads to mistaken conclusions that can be avoided by using the preferred method: the geometric mean.},
	number = {3},
	urldate = {2021-01-16},
	journal = {Communications of the ACM},
	author = {Fleming, Philip J. and Wallace, John J.},
	month = mar,
	year = {1986},
	pages = {218--221},
}

@inproceedings{hosseini_dropping_2019,
	title = {Dropping {Pixels} for {Adversarial} {Robustness}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2019/html/CV-COPS/Hosseini_Dropping_Pixels_for_Adversarial_Robustness_CVPRW_2019_paper.html},
	urldate = {2021-01-16},
	author = {Hosseini, Hossein and Kannan, Sreeram and Poovendran, Radha},
	year = {2019},
	pages = {0--0},
}

@inproceedings{murdoch_beyond_2018,
	title = {Beyond {Word} {Importance}: {Contextual} {Decomposition} to {Extract} {Interactions} from {LSTMs}},
	shorttitle = {Beyond {Word} {Importance}},
	url = {https://openreview.net/forum?id=rkRwGg-0Z&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=NLP%20News},
	abstract = {We introduce contextual decompositions, an interpretation algorithm for LSTMs capable of extracting word, phrase and interaction-level importance score},
	language = {en},
	urldate = {2021-01-16},
	author = {Murdoch, W. James and Liu, Peter J. and Yu, Bin},
	month = feb,
	year = {2018},
}

@inproceedings{elsayed_adversarial_2018,
	title = {Adversarial {Examples} that {Fool} both {Computer} {Vision} and {Time}-{Limited} {Humans}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/8562ae5e286544710b2e7ebe9858833b-Abstract.html},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Elsayed, Gamaleldin and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alexey and Goodfellow, Ian and Sohl-Dickstein, Jascha},
	year = {2018},
	pages = {3910--3920},
}

@article{mohseni_multidisciplinary_2020,
	title = {A {Multidisciplinary} {Survey} and {Framework} for {Design} and {Evaluation} of {Explainable} {AI} {Systems}},
	url = {http://arxiv.org/abs/1811.11839},
	abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable intelligent systems are designed to self-explain the reasoning behind system decisions and predictions, and researchers from different disciplines work together to define, design, and evaluate interpretable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of interpretable machine learning research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of interpretable machine learning design goals and evaluation methods to show a mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
	urldate = {2021-01-16},
	journal = {arXiv:1811.11839 [cs]},
	author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
	month = aug,
	year = {2020},
	note = {arXiv: 1811.11839},
	keywords = {Computer Science - Human-Computer Interaction},
}

@article{zhang_visual_2018,
	title = {Visual interpretability for deep learning: a survey},
	volume = {19},
	issn = {2095-9230},
	shorttitle = {Visual interpretability for deep learning},
	url = {https://doi.org/10.1631/FITEE.1700808},
	doi = {10.1631/FITEE.1700808},
	abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
	language = {en},
	number = {1},
	urldate = {2021-01-16},
	journal = {Frontiers of Information Technology \& Electronic Engineering},
	author = {Zhang, Quan-shi and Zhu, Song-chun},
	month = jan,
	year = {2018},
	pages = {27--39},
}

@inproceedings{wang_residual_2017,
	title = {Residual {Attention} {Network} for {Image} {Classification}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Residual_Attention_Network_CVPR_2017_paper.html},
	urldate = {2021-01-16},
	author = {Wang, Fei and Jiang, Mengqing and Qian, Chen and Yang, Shuo and Li, Cheng and Zhang, Honggang and Wang, Xiaogang and Tang, Xiaoou},
	year = {2017},
	pages = {3156--3164},
}

@inproceedings{dabkowski_real_2017,
	title = {Real {Time} {Image} {Saliency} for {Black} {Box} {Classifiers}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html},
	language = {en},
	urldate = {2021-01-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Dabkowski, Piotr and Gal, Yarin},
	year = {2017},
	pages = {6967--6976},
}

@inproceedings{sutskever_importance_2013,
	title = {On the importance of initialization and momentum in deep learning},
	url = {http://proceedings.mlr.press/v28/sutskever13.html},
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this pa...},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {1139--1147},
}

@inproceedings{keskar_large-batch_2016,
	title = {On {Large}-{Batch} {Training} for {Deep} {Learning}: {Generalization} {Gap} and {Sharp} {Minima}},
	shorttitle = {On {Large}-{Batch} {Training} for {Deep} {Learning}},
	url = {https://openreview.net/forum?id=H1oyRlYgg},
	abstract = {We present numerical evidence for the argument that if deep networks are trained using large (mini-)batches, they converge to sharp minimizers, and these minimizers have poor generalization...},
	language = {en},
	urldate = {2021-01-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	month = nov,
	year = {2016},
}

@article{zhang_interpreting_2021,
	title = {Interpreting and {Improving} {Adversarial} {Robustness} of {Deep} {Neural} {Networks} {With} {Neuron} {Sensitivity}},
	volume = {30},
	issn = {1941-0042},
	doi = {10.1109/TIP.2020.3042083},
	abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in the adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in the adversarial setting. Based on that, we further propose to improve adversarial robustness by stabilizing the behaviors of sensitive neurons. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities, which in turn confirms the strong connections between adversarial robustness and neuron sensitivity. Extensive experiments on various datasets demonstrate that our algorithm effectively achieves excellent results. To the best of our knowledge, we are the first to study adversarial robustness using neuron sensitivities.},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, C. and Liu, A. and Liu, X. and Xu, Y. and Yu, H. and Ma, Y. and Li, T.},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Analytical models, Computational modeling, Deep learning, Model interpretation, Neurons, Robustness, Sensitivity, Training, adversarial examples, neuron sensitivity},
	pages = {1291--1304},
}

@book{molnar_interpretable_2020,
	title = {Interpretable {Machine} {Learning}},
	isbn = {978-0-244-76852-2},
	abstract = {This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. Later chapters focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.},
	language = {en},
	publisher = {Lulu.com},
	author = {Molnar, Christoph},
	month = feb,
	year = {2020},
	note = {Google-Books-ID: jBm3DwAAQBAJ},
}

@inproceedings{kim_interpretable_2017,
	title = {Interpretable {Learning} for {Self}-{Driving} {Cars} by {Visualizing} {Causal} {Attention}},
	url = {https://openaccess.thecvf.com/content_iccv_2017/html/Kim_Interpretable_Learning_for_ICCV_2017_paper.html},
	urldate = {2021-01-16},
	author = {Kim, Jinkyu and Canny, John},
	year = {2017},
	pages = {2942--2950},
}

@article{britton_vine_2019,
	title = {{VINE}: {Visualizing} {Statistical} {Interactions} in {Black} {Box} {Models}},
	shorttitle = {{VINE}},
	url = {http://arxiv.org/abs/1904.00561},
	abstract = {As machine learning becomes more pervasive, there is an urgent need for interpretable explanations of predictive models. Prior work has developed effective methods for visualizing global model behavior, as well as generating local (instance-specific) explanations. However, relatively little work has addressed regional explanations - how groups of similar instances behave in a complex model, and the related issue of visualizing statistical feature interactions. The lack of utilities available for these analytical needs hinders the development of models that are mission-critical, transparent, and align with social goals. We present VINE (Visual INteraction Effects), a novel algorithm to extract and visualize statistical interaction effects in black box models. We also present a novel evaluation metric for visualizations in the interpretable ML space.},
	urldate = {2021-01-16},
	journal = {arXiv:1904.00561 [cs, stat]},
	author = {Britton, Matthew},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.00561},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_hopskipjumpattack_2020,
	title = {{HopSkipJumpAttack}: {A} {Query}-{Efficient} {Decision}-{Based} {Attack}},
	shorttitle = {{HopSkipJumpAttack}},
	doi = {10.1109/SP40000.2020.00045},
	abstract = {The goal of a decision-based adversarial attack on a trained model is to generate adversarial examples based solely on observing output labels returned by the targeted model. We develop HopSkipJumpAttack, a family of algorithms based on a novel estimate of the gradient direction using binary information at the decision boundary. The proposed family includes both untargeted and targeted attacks optimized for ℓ and ℓ∞ similarity metrics respectively. Theoretical analysis is provided for the proposed algorithms and the gradient direction estimate. Experiments show HopSkipJumpAttack requires significantly fewer model queries than several state-of-the-art decision-based adversarial attacks. It also achieves competitive performance in attacking several widely-used defense mechanisms.},
	booktitle = {2020 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Chen, J. and Jordan, M. I. and Wainwright, M. J.},
	month = may,
	year = {2020},
	note = {ISSN: 2375-1207},
	keywords = {Estimation, HopSkipJumpAttack, Iterative methods, Measurement, Neural networks, Optimization, Perturbation methods, Predictive models, adversarial examples, binary information, decision boundary, decision-based adversarial attacks, gradient direction estimate, inference mechanisms, learning (artificial intelligence), model queries, optimisation, output labels, query processing, query-efficient decision-based attack, security of data, target tracking, targeted model, trained model, untargeted attacks, ℓ similarity metrics, ℓ∞ similarity metrics},
	pages = {1277--1294},
}

@inproceedings{benz_revisiting_2021,
	title = {Revisiting {Batch} {Normalization} for {Improving} {Corruption} {Robustness}},
	url = {https://openaccess.thecvf.com/content/WACV2021/html/Benz_Revisiting_Batch_Normalization_for_Improving_Corruption_Robustness_WACV_2021_paper.html},
	language = {en},
	urldate = {2021-01-15},
	author = {Benz, Philipp and Zhang, Chaoning and Karjauv, Adil and Kweon, In So},
	year = {2021},
	pages = {494--503},
}

@article{geirhos_shortcut_2020,
	title = {Shortcut learning in deep neural networks},
	volume = {2},
	copyright = {2020 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00257-z},
	doi = {10.1038/s42256-020-00257-z},
	abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today’s machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
	language = {en},
	number = {11},
	urldate = {2021-01-14},
	journal = {Nature Machine Intelligence},
	author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
	month = nov,
	year = {2020},
	note = {Number: 11
Publisher: Nature Publishing Group},
	pages = {665--673},
}

@inproceedings{brendel_accurate_2019,
	title = {Accurate, reliable and fast robustness evaluation},
	abstract = {Throughout the past ﬁve years, the susceptibility of neural networks to minimal adversarial perturbations has moved from a peculiar phenomenon to a core issue in Deep Learning. Despite much attention, however, progress towards more robust models is signiﬁcantly impaired by the difﬁculty of evaluating the robustness of neural network models. Today’s methods are either fast but brittle (gradient-based attacks), or they are fairly reliable but slow (score- and decision-based attacks). We here develop a new set of gradient-based adversarial attacks which (a) are more reliable in the face of gradient-masking than other gradient-based attacks, (b) perform better and are more query efﬁcient than current state-of-the-art gradientbased attacks, (c) can be ﬂexibly adapted to a wide range of adversarial criteria and (d) require virtually no hyperparameter tuning. These ﬁndings are carefully validated across a diverse set of six different models and hold for L0, L1, L2 and L∞ in both targeted as well as untargeted scenarios. Implementations will soon be available in all major toolboxes (Foolbox, CleverHans and ART). We hope that this class of attacks will make robustness evaluations easier and more reliable, thus contributing to more signal in the search for more robust machine learning models.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Brendel, Wieland and Rauber, Jonas and Kümmerer, Matthias and Ustyuzhaninov, Ivan and Bethge, Matthias},
	year = {2019},
	pages = {11},
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
	author = {Krizhevsky, Alex},
	year = {2009},
}

@inproceedings{netzer_reading_2011,
	title = {Reading {Digits} in {Natural} {Images} with {Unsupervised} {Feature} {Learning}},
	url = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
	urldate = {2021-01-04},
	booktitle = {{NIPS} {Workshop} on {Deep} {Learning} and {Unsupervised} {Feature} {Learning} 2011},
	author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y.},
	year = {2011},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {2D shape variability, Character recognition, Feature extraction, GTN, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis, back-propagation, backpropagation, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, handwritten character recognition, handwritten digit recognition task, high-dimensional patterns, language modeling, multilayer neural networks, multilayer perceptrons, multimodule systems, optical character recognition, performance measure minimization, segmentation recognition},
	pages = {2278--2324},
}

@inproceedings{boopathy_proper_2020,
	title = {Proper {Network} {Interpretability} {Helps} {Adversarial} {Robustness} in {Classiﬁcation}},
	abstract = {Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), or interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with a proper measurement of interpretation, it is actually difﬁcult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy, as conﬁrmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization). We show that our defense achieves both robust classiﬁcation and robust interpretation, outperforming state-of-theart adversarial training methods against attacks of large perturbation in particular.},
	language = {en},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	author = {Boopathy, Akhilan and Liu, Sijia and Zhang, Gaoyuan and Liu, Cynthia and Chen, Pin-Yu and Chang, Shiyu and Daniel, Luca},
	year = {2020},
	pages = {10},
}

@article{samek_evaluating_2017,
	title = {Evaluating the {Visualization} of {What} a {Deep} {Neural} {Network} {Has} {Learned}},
	volume = {28},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2016.2599820},
	abstract = {Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.},
	number = {11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Samek, W. and Binder, A. and Montavon, G. and Lapuschkin, S. and Müller, K.},
	month = nov,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Algorithm design and analysis, Biological neural networks, Convolutional neural networks, DNN, Deconvolution, Heating, ILSVRC2012, Learning systems, MIT Places data sets, Neurons, SUN397, Sensitivity, complex machine learning tasks, data visualisation, data visualization, deconvolution method, deep neural network, explaining classification, heatmap, image classification, interpretable machine learning, learning (artificial intelligence), multilayer nonlinear structure, neural nets, relevance models, relevance propagation algorithm, sensitivity-based approach},
	pages = {2660--2673},
}

@inproceedings{hanson_minkowski-r_1988,
	title = {Minkowski-r {Back}-{Propagation}: {Learning} in {Connectionist} {Models} with {Non}-{Euclidian} {Error} {Signals}},
	shorttitle = {Minkowski-r {Back}-{Propagation}},
	url = {https://proceedings.neurips.cc/paper/1987/file/fc490ca45c00b1249bbe3554a4fdf6fb-Paper.pdf},
	urldate = {2021-01-01},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {American Institute of Physics},
	author = {Hanson, Stephen and Burr, David},
	editor = {Anderson, D.},
	year = {1988},
	pages = {348--357},
}

@article{wachter_counterfactual_2017,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	volume = {31},
	shorttitle = {Counterfactual {Explanations} without {Opening} the {Black} {Box}},
	url = {https://heinonline.org/HOL/P?h=hein.journals/hjlt31&i=860},
	language = {eng},
	number = {2},
	urldate = {2020-12-31},
	journal = {Harvard Journal of Law \& Technology (Harvard JOLT)},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	year = {2017},
	pages = {841--888},
}

@inproceedings{lin_microsoft_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	isbn = {978-3-319-10602-1},
	shorttitle = {Microsoft {COCO}},
	doi = {10.1007/978-3-319-10602-1_48},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Common Object, Object Category, Object Detection, Object Instance, Scene Understanding},
	pages = {740--755},
}

@inproceedings{tao_attacks_2018,
	title = {Attacks {Meet} {Interpretability}: {Attribute}-steered {Detection} of {Adversarial} {Samples}},
	volume = {31},
	shorttitle = {Attacks {Meet} {Interpretability}},
	url = {https://papers.nips.cc/paper/2018/hash/b994697479c5716eda77e8e9713e5f0f-Abstract.html},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tao, Guanhong and Ma, Shiqing and Liu, Yingqi and Zhang, Xiangyu},
	year = {2018},
	pages = {7717--7728},
}

@inproceedings{chen_detect_2014,
	title = {Detect {What} {You} {Can}: {Detecting} and {Representing} {Objects} using {Holistic} {Models} and {Body} {Parts}},
	shorttitle = {Detect {What} {You} {Can}},
	url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Chen_Detect_What_You_2014_CVPR_paper.html},
	urldate = {2020-12-29},
	author = {Chen, Xianjie and Mottaghi, Roozbeh and Liu, Xiaobai and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
	year = {2014},
	pages = {1971--1978},
}

@inproceedings{ghorbani_interpretation_2019,
	title = {Interpretation of {Neural} {Networks} {Is} {Fragile}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4252},
	doi = {10.1609/aaai.v33i01.33013681},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {3681--3688},
}

@article{antunes_structuring_2008,
	title = {Structuring dimensions for collaborative systems evaluation},
	volume = {44},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/2089125.2089128},
	doi = {10.1145/2089125.2089128},
	abstract = {Collaborative systems evaluation is always necessary to determine the impact a solution will have on the individuals, groups, and the organization. Several methods of evaluation have been proposed. These methods comprise a variety of approaches with various goals. Thus, the need for a strategy to select the most appropriate method for a specific case is clear. This research work presents a detailed framework to evaluate collaborative systems according to given variables and performance levels. The proposal assumes that evaluation is an evolving process during the system lifecycle. Therefore, the framework, illustrated with two examples, is complemented with a collection of guidelines to evaluate collaborative systems according to product development status.},
	number = {2},
	urldate = {2020-12-29},
	journal = {ACM Computing Surveys},
	author = {Antunes, Pedro and Herskovic, Valeria and Ochoa, Sergio F. and Pino, Jose A.},
	month = mar,
	year = {2008},
	keywords = {Collaborative systems evaluation, evaluation dimensions, evaluation guidelines, human-computer interaction, interaction assessment},
	pages = {8:1--8:28},
}

@inproceedings{hu_harnessing_2016,
	address = {Berlin, Germany},
	title = {Harnessing {Deep} {Neural} {Networks} with {Logic} {Rules}},
	url = {https://www.aclweb.org/anthology/P16-1228},
	doi = {10.18653/v1/P16-1228},
	urldate = {2020-12-28},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
	month = aug,
	year = {2016},
	pages = {2410--2420},
}

@inproceedings{jin_collaborative_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Collaborative {Layer}-{Wise} {Discriminative} {Learning} in {Deep} {Neural} {Networks}},
	isbn = {978-3-319-46478-7},
	doi = {10.1007/978-3-319-46478-7_45},
	abstract = {Intermediate features at different layers of a deep neural network are known to be discriminative for visual patterns of different complexities. However, most existing works ignore such cross-layer heterogeneities when classifying samples of different complexities. For example, if a training sample has already been correctly classified at a specific layer with high confidence, we argue that it is unnecessary to enforce rest layers to classify this sample correctly and a better strategy is to encourage those layers to focus on other samples.In this paper, we propose a layer-wise discriminative learning method to enhance the discriminative capability of a deep network by allowing its layers to work collaboratively for classification. Towards this target, we introduce multiple classifiers on top of multiple layers. Each classifier not only tries to correctly classify the features from its input layer, but also coordinates with other classifiers to jointly maximize the final classification performance. Guided by the other companion classifiers, each classifier learns to concentrate on certain training examples and boosts the overall performance. Allowing for end-to-end training, our method can be conveniently embedded into state-of-the-art deep networks. Experiments with multiple popular deep networks, including Network in Network, GoogLeNet and VGGNet, on scale-various object classification benchmarks, including CIFAR100, MNIST and ImageNet, and scene classification benchmarks, including MIT67, SUN397 and Places205, demonstrate the effectiveness of our method. In addition, we also analyze the relationship between the proposed method and classical conditional random fields models.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Jin, Xiaojie and Chen, Yunpeng and Dong, Jian and Feng, Jiashi and Yan, Shuicheng},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Companion Classifier, Conditional Random Field, Deep Neural Network, Hide Layer, Prediction Score},
	pages = {733--749},
}

@inproceedings{agrawal_analyzing_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Analyzing the {Performance} of {Multilayer} {Neural} {Networks} for {Object} {Recognition}},
	isbn = {978-3-319-10584-0},
	doi = {10.1007/978-3-319-10584-0_22},
	abstract = {In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Agrawal, Pulkit and Girshick, Ross and Malik, Jitendra},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {convolutional neural networks, empirical analysis, object recognition},
	pages = {329--344},
}

@article{hurley_comparing_2009,
	title = {Comparing {Measures} of {Sparsity}},
	volume = {55},
	issn = {1557-9654},
	doi = {10.1109/TIT.2009.2027527},
	abstract = {Sparsity of representations of signals has been shown to be a key concept of fundamental importance in fields such as blind source separation, compression, sampling and signal analysis. The aim of this paper is to compare several commonly-used sparsity measures based on intuitive attributes. Intuitively, a sparse representation is one in which a small number of coefficients contain a large proportion of the energy. In this paper, six properties are discussed: (Robin Hood, Scaling, Rising Tide, Cloning, Bill Gates, and Babies), each of which a sparsity measure should have. The main contributions of this paper are the proofs and the associated summary table which classify commonly-used sparsity measures based on whether or not they satisfy these six propositions. Only two of these measures satisfy all six: the pq-mean with p les 1, q {\textgreater} 1 and the Gini index.},
	number = {10},
	journal = {IEEE Transactions on Information Theory},
	author = {Hurley, N. and Rickard, S.},
	month = oct,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Adaptive signal processing, Blind source separation, Cloning, Gini index, Image coding, Machine learning, Measures of sparsity, Sampling methods, Sea measurements, Signal analysis, Source separation, Tides, blind source separation, compression analysis, information theory, measuring sparsity, sampling analysis, signal analysis, sparse distribution, sparse representation, sparsity, sparsity measures},
	pages = {4723--4741},
}

@misc{noauthor_inceptionism_nodate,
	title = {Inceptionism: {Going} {Deeper} into {Neural} {Networks}},
	shorttitle = {Inceptionism},
	url = {http://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html},
	abstract = {Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer Update - ...},
	language = {en},
	urldate = {2020-12-28},
	journal = {Google AI Blog},
}

@article{lavrac_selected_1999,
	series = {Data {Mining} {Techniques} and {Applications} in {Medicine}},
	title = {Selected techniques for data mining in medicine},
	volume = {16},
	issn = {0933-3657},
	url = {http://www.sciencedirect.com/science/article/pii/S0933365798000621},
	doi = {10.1016/S0933-3657(98)00062-1},
	abstract = {Widespread use of medical information systems and explosive growth of medical databases require traditional manual data analysis to be coupled with methods for efficient computer-assisted analysis. This paper presents selected data mining techniques that can be applied in medicine, and in particular some machine learning techniques including the mechanisms that make them better suited for the analysis of medical databases (derivation of symbolic rules, use of background knowledge, sensitivity and specificity of induced descriptions). The importance of the interpretability of results of data analysis is discussed and illustrated on selected medical applications.},
	language = {en},
	number = {1},
	urldate = {2020-12-28},
	journal = {Artificial Intelligence in Medicine},
	author = {Lavrač, Nada},
	month = may,
	year = {1999},
	keywords = {Data mining, Machine learning, Medical applications},
	pages = {3--23},
}

@inproceedings{altendorf_learning_2005,
	address = {Arlington, Virginia, USA},
	series = {{UAI}'05},
	title = {Learning from sparse data by exploiting monotonicity constraints},
	isbn = {978-0-9749039-1-0},
	abstract = {When training data is sparse, more domain knowledge must be incorporated into the learning algorithm in order to reduce the effective size of the hypothesis space. This paper builds on previous work in which knowledge about qualitative monotonicities was formally represented and incorporated into learning algorithms (e.g., Clark \& Matwin's work with the CN2 rule learning algorithm). We show how to interpret knowledge of qualitative influences, and in particular of monotonicities, as constraints on probability distributions, and to incorporate this knowledge into Bayesian network learning algorithms. We show that this yields improved accuracy, particularly with very small training sets (e.g. less than 10 examples).},
	urldate = {2020-12-27},
	booktitle = {Proceedings of the {Twenty}-{First} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Altendorf, Eric E. and Restificar, Angelo C. and Dietterich, Thomas G.},
	month = jul,
	year = {2005},
	pages = {18--26},
}

@inproceedings{wang_interpret_2018,
	title = {Interpret {Neural} {Networks} by {Identifying} {Critical} {Data} {Routing} {Paths}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Interpret_Neural_Networks_CVPR_2018_paper.html},
	urldate = {2020-12-27},
	author = {Wang, Yulong and Su, Hang and Zhang, Bo and Hu, Xiaolin},
	year = {2018},
	pages = {8906--8914},
}

@inproceedings{wang_learning_2018,
	title = {Learning a {Discriminative} {Filter} {Bank} {Within} a {CNN} for {Fine}-{Grained} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Learning_a_Discriminative_CVPR_2018_paper.html},
	urldate = {2020-12-27},
	author = {Wang, Yaming and Morariu, Vlad I. and Davis, Larry S.},
	year = {2018},
	pages = {4148--4157},
}

@article{freitas_comprehensible_2014,
	title = {Comprehensible classification models: a position paper},
	volume = {15},
	issn = {1931-0145},
	shorttitle = {Comprehensible classification models},
	url = {https://doi.org/10.1145/2594473.2594475},
	doi = {10.1145/2594473.2594475},
	abstract = {The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.},
	number = {1},
	urldate = {2020-12-27},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Freitas, Alex A.},
	month = mar,
	year = {2014},
	keywords = {Bayesian network classifiers, decision table, decision tree, monotonicity constraint, nearest neighbors, rule induction},
	pages = {1--10},
}

@inproceedings{ross_neural_2017,
	title = {The {Neural} {LASSO}: {Local} {Linear} {Sparsity} for {Interpretable} {Explanations}},
	abstract = {Neural networks often perform better on prediction problems than simpler classes of models, but their behavior is difﬁcult to explain. This makes it challenging to trust their predictions in safety critical domains. Recent work has focused on explaining their predictions using local linear approximations [1, 10], but these explanations can be complex when they depend on many features and it is unclear if they can be used to understand global trends in model behavior. In this work, we train neural networks to have sparse local explanations by applying L1 penalties to their input gradients. We show explanations of these networks depend on fewer inputs while their performance remains comparable across datasets and architectures. We illustrate how our approach encourages a different kind of sparsity than L1 weight decay. In a case study with ICU data, we observe that gradients vary smoothly over the input space, which suggests they can be used to gain insight into the global behavior of the model.},
	language = {en},
	booktitle = {31st {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Ross, Andrew Slavin and Lage, Isaac},
	year = {2017},
	pages = {5},
}

@inproceedings{lage_human---loop_2020,
	title = {Human-in-the-{Loop} {Learning} of {Interpretable} and {Intuitive} {Representations}},
	volume = {1},
	booktitle = {{ICML} {Workshop} on {Human} {Interpretability} in {Machine} {Learning},},
	author = {Lage, I. and Doshi-Velez, F.},
	year = {2020},
	pages = {1--10},
}

@inproceedings{lund_labeled_2018,
	address = {Brussels, Belgium},
	title = {Labeled {Anchors} and a {Scalable}, {Transparent}, and {Interactive} {Classifier}},
	url = {https://www.aclweb.org/anthology/D18-1095},
	doi = {10.18653/v1/D18-1095},
	abstract = {We propose Labeled Anchors, an interactive and supervised topic model based on the anchor words algorithm (Arora et al., 2013). Labeled Anchors is similar to Supervised Anchors (Nguyen et al., 2014) in that it extends the vector-space representation of words to include document labels. However, our formulation also admits a classifier which requires no training beyond inferring topics, which means our approach is also fast enough to be interactive. We run a small user study that demonstrates that untrained users can interactively update topics in order to improve classification accuracy.},
	urldate = {2020-12-27},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lund, Jeffrey and Cowley, Stephen and Fearn, Wilson and Hales, Emily and Seppi, Kevin},
	month = oct,
	year = {2018},
	pages = {824--829},
}

@article{poursabzi-sangdeh_manipulating_2019,
	title = {Manipulating and {Measuring} {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1802.07810},
	abstract = {With the increased use of machine learning in decision-making scenarios, there has been a growing interest in creating human-interpretable machine learning models. While many such models have been proposed, there have been relatively few experimental studies of whether these models achieve their intended effects, such as encouraging people to follow the model's predictions when the model is correct and to deviate when it makes a mistake. We present a series of randomized, pre-registered experiments comprising 3,800 participants in which people were shown functionally identical models that varied only in two factors thought to influence interpretability: the number of input features and the model transparency (clear or black-box). Predictably, participants who were shown a clear model with a small number of features were better able to simulate the model's predictions. However, contrary to what one might expect when manipulating interpretability, we found no improvements in the degree to which participants followed the model's predictions when it was beneficial to do so. Even more surprisingly, increased transparency hampered people's ability to detect when the model makes a sizable mistake and correct for it, seemingly due to information overload. These counterintuitive results suggest that decision scientists creating interpretable models should harbor a healthy skepticism of their intuitions and empirically verify that interpretable models achieve their intended effects.},
	urldate = {2020-12-27},
	journal = {arXiv:1802.07810 [cs]},
	author = {Poursabzi-Sangdeh, Forough and Goldstein, Daniel G. and Hofman, Jake M. and Vaughan, Jennifer Wortman and Wallach, Hanna},
	month = nov,
	year = {2019},
	note = {arXiv: 1802.07810},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@inproceedings{lage_evaluation_2018,
	title = {An {Evaluation} of the {Human}-{Interpretability} of {Explanation}},
	abstract = {The evaluation of interpretable machine learning systems is challenging, as explanation is almost always a means toward some downstream task. In this work, we carefully control a number of properties of logic-based explanations (overall length, number of repeated terms, etc.) to determine their effect on human ability to perform three basic tasks: simulating the system’s response, veriﬁcation of a suggested response, and counterfactual reasoning. Our ﬁndings about how each of these properties affect the ability of humans to perform each task provide insights on how we might construct regularizers to optimize for task performance.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Samuel J and Doshi-Velez, Finale},
	year = {2018},
	pages = {7},
}

@inproceedings{lage_human_2019,
	title = {Human {Evaluation} of {Models} {Built} for {Interpretability}},
	volume = {7},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/HCOMP/article/view/5280},
	language = {en},
	urldate = {2020-12-27},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Human} {Computation} and {Crowdsourcing}},
	author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Samuel J. and Doshi-Velez, Finale},
	month = oct,
	year = {2019},
	note = {Number: 1},
	pages = {59--67},
}

@article{ross_learning_2018,
	title = {Learning {Qualitatively} {Diverse} and {Interpretable} {Rules} for {Classification}},
	url = {http://arxiv.org/abs/1806.08716},
	abstract = {There has been growing interest in developing accurate models that can also be explained to humans. Unfortunately, if there exist multiple distinct but accurate models for some dataset, current machine learning methods are unlikely to find them: standard techniques will likely recover a complex model that combines them. In this work, we introduce a way to identify a maximal set of distinct but accurate models for a dataset. We demonstrate empirically that, in situations where the data supports multiple accurate classifiers, we tend to recover simpler, more interpretable classifiers rather than more complex ones.},
	language = {en},
	urldate = {2020-12-27},
	journal = {arXiv:1806.08716 [cs, stat]},
	author = {Ross, Andrew Slavin and Pan, Weiwei and Doshi-Velez, Finale},
	month = jul,
	year = {2018},
	note = {arXiv: 1806.08716},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{achille_emergence_2018,
	title = {Emergence of invariance and disentanglement in deep representations},
	volume = {19},
	issn = {1532-4435},
	abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Achille, Alessandro and Soatto, Stefano},
	month = jan,
	year = {2018},
	keywords = {PAC-bayes, flat minima, generalization, independence, information bottleneck, invariance, representation learning},
	pages = {1947--1980},
}

@inproceedings{fong_understanding_2019,
	title = {Understanding {Deep} {Networks} via {Extremal} {Perturbations} and {Smooth} {Masks}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Fong_Understanding_Deep_Networks_via_Extremal_Perturbations_and_Smooth_Masks_ICCV_2019_paper.html},
	urldate = {2020-12-26},
	author = {Fong, Ruth and Patrick, Mandela and Vedaldi, Andrea},
	year = {2019},
	pages = {2950--2958},
}

@article{zhang_causal_2020,
	title = {A {Causal} {View} on {Robustness} of {Neural} {Networks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/02ed812220b0705fabb868ddbf17ea20-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Zhang, Cheng and Zhang, Kun and Li, Yingzhen},
	year = {2020},
}

@article{chen_towards_2020,
	title = {Towards {Understanding} {Hierarchical} {Learning}: {Benefits} of {Neural} {Representations}},
	volume = {33},
	shorttitle = {Towards {Understanding} {Hierarchical} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/fb647ca6672b0930e9d00dc384d8b16f-Abstract.html},
	language = {en},
	urldate = {2020-12-26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Minshuo and Bai, Yu and Lee, Jason D. and Zhao, Tuo and Wang, Huan and Xiong, Caiming and Socher, Richard},
	year = {2020},
}

@inproceedings{park_attribution_2020,
	title = {Attribution {Preservation} in {Network} {Compression} for {Reliable} {Network} {Interpretation}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/35adf1ae7eb5734122c84b7a9ea5cc13-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Park, Geondo and Yang, June Yong and Hwang, Sung Ju and Yang, Eunho},
	year = {2020},
}

@inproceedings{lage_human---loop_2018,
	title = {Human-in-the-{Loop} {Interpretability} {Prior}},
	abstract = {We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to ﬁnd models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Lage, Isaac and Ross, Andrew and Gershman, Samuel J and Kim, Been and Doshi-Velez, Finale},
	year = {2018},
	pages = {10},
}

@inproceedings{zintgraf_visualizing_2017,
	title = {Visualizing {Deep} {Neural} {Network} {Decisions}: {Prediction} {Difference} {Analysis}},
	abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a speciﬁc input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classiﬁers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classiﬁers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zintgraf, Luisa M and Cohen, Taco S and Adel, Tameem and Welling, Max},
	year = {2017},
	pages = {12},
}

@inproceedings{chen_this_2019,
	title = {This {Looks} {Like} {That}: {Deep} {Learning} for {Interpretable} {Image} {Recognition}},
	abstract = {When we are faced with challenging image classiﬁcation tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our ﬁnal decision. In this work, we introduce a deep network architecture –prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by ﬁnding prototypical parts, and combines evidence from the prototypes to make a ﬁnal classiﬁcation. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classiﬁcation tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.},
	language = {en},
	booktitle = {33rd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K},
	year = {2019},
	pages = {12},
}

@inproceedings{xie_relating_2017,
	title = {Relating {Input} {Concepts} to {Convolutional} {Neural} {Network} {Decisions}},
	abstract = {Many current methods to interpret convolutional neural networks (CNNs) use visualization techniques and words to highlight concepts of the input seemingly relevant to a CNN’s decision. The methods hypothesize that the recognition of these concepts are instrumental in the decision a CNN reaches, but the nature of this relationship has not been well explored. To address this gap, this paper examines the quality of a concept’s recognition by a CNN and the degree to which the recognitions are associated with CNN decisions. The study considers a CNN trained for scene recognition over the ADE20k dataset. It uses a novel approach to ﬁnd and score the strength of minimally distributed representations of input concepts (deﬁned by objects in scene images) across late stage feature maps. Subsequent analysis ﬁnds evidence that concept recognition impacts decision making. Strong recognition of concepts frequently-occurring in few scenes are indicative of correct decisions, but recognizing concepts common to many scenes may mislead the network.},
	language = {en},
	booktitle = {31st {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Xie, Ning and Sarker, Kamruzzaman and Doran, Derek and Hitzler, Pascal and Raymer, Michael},
	year = {2017},
	pages = {10},
}

@inproceedings{lee_deeply-supervised_2015,
	title = {Deeply-{Supervised} {Nets}},
	url = {http://proceedings.mlr.press/v38/lee15a.html},
	abstract = {We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our ...},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
	month = feb,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {562--570},
}

@inproceedings{ravanelli_interpretable_2018,
	title = {Interpretable {Convolutional} {Filters} with {SincNet}},
	abstract = {Deep learning is currently playing a crucial role toward higher levels of artiﬁcial intelligence. This paradigm allows neural networks to learn complex and abstract representations, that are progressively obtained by combining simpler ones. Nevertheless, the internal "black-box" representations automatically discovered by current neural architectures often suffer from a lack of interpretability, making of primary interest the study of explainable machine learning techniques.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems} {IRASL} workshop},
	author = {Ravanelli, Mirco and Bengio, Yoshua},
	year = {2018},
	pages = {13},
}

@inproceedings{borowski_natural_2020,
	title = {Natural {Images} are {More} {Informative} for {Interpreting} {CNN} {Activations} than {State}-of-the-{Art} {Synthetic} {Feature} {Visualizations}},
	abstract = {Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs’ inner workings. Here, we measure how much extremely activating images help humans in predicting CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et al. [45] with a simple baseline visualization, namely natural images that also strongly activate a speciﬁc feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiment is designed to maximize participants’ performance, and is the ﬁrst to probe intermediate instead of ﬁnal layer representations. We ﬁnd that synthetic images indeed provide helpful information about feature map activations (82 ± 4\% accuracy; chance would be 50\%). However, natural images—originally intended to be a baseline—outperform these synthetic images by a wide margin (92 ± 2\% accuracy). The superiority of natural images holds across the investigated network and various conditions. Therefore, we argue that visualization methods should improve over this simple baseline.},
	language = {en},
	booktitle = {2nd {Workshop} on {Shared} {Visual} {Representations} in {Human} and {Machine} {Intelligence} ({SVRHM}), {NeurIPS}},
	author = {Borowski, Judy and Zimmermann, Roland S and Schepers, Judith and Geirhos, Robert and Wallis, Thomas S A and Bethge, Matthias and Brendel, Wieland},
	year = {2020},
	pages = {19},
}

@inproceedings{kumarl_ibrahim_ben_daya_beyond_2019,
	title = {Beyond {Explainability}: {Leveraging} {Interpretability} for {Improved} {Adversarial} {Learning}},
	shorttitle = {Beyond {Explainability}},
	url = {https://openaccess.thecvf.com/content_CVPRW_2019/html/Explainable_AI/Daya_Beyond_Explainability_Leveraging_Interpretability_for_Improved_Adversarial_Learning_CVPRW_2019_paper.html},
	urldate = {2020-12-25},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Kumarl Ibrahim Ben Daya, Devinder and Vats, Kanav and Feng, Jeffery and Taylor, Graham and Wong, Alexander},
	year = {2019},
	pages = {16--19},
}

@inproceedings{chen_infogan_2016,
	title = {{InfoGAN}: {Interpretable} {Representation} {Learning} by {Information} {Maximizing} {Generative} {Adversarial} {Nets}},
	volume = {29},
	shorttitle = {{InfoGAN}},
	url = {https://papers.nips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	year = {2016},
	pages = {2172--2180},
}

@inproceedings{mascharka_transparency_2018,
	title = {Transparency by {Design}: {Closing} the {Gap} {Between} {Performance} and {Interpretability} in {Visual} {Reasoning}},
	shorttitle = {Transparency by {Design}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Mascharka_Transparency_by_Design_CVPR_2018_paper.html},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Mascharka, David and Tran, Philip and Soklaski, Ryan and Majumdar, Arjun},
	year = {2018},
	pages = {4942--4950},
}

@inproceedings{liang_training_2020,
	address = {Cham},
	title = {Training {Interpretable} {Convolutional} {Neural} {Networks} by {Differentiating} {Class}-{Specific} {Filters}},
	volume = {12347},
	isbn = {978-3-030-58535-8 978-3-030-58536-5},
	url = {http://link.springer.com/10.1007/978-3-030-58536-5_37},
	doi = {10.1007/978-3-030-58536-5_37},
	abstract = {Convolutional neural networks (CNNs) have been successfully used in a range of tasks. However, CNNs are often viewed as “blackbox” and lack of interpretability. One main reason is due to the ﬁlter-class entanglement – an intricate many-to-many correspondence between ﬁlters and classes. Most existing works attempt post-hoc interpretation on a pre-trained model, while neglecting to reduce the entanglement underlying the model. In contrast, we focus on alleviating ﬁlter-class entanglement during training. Inspired by cellular diﬀerentiation, we propose a novel strategy to train interpretable CNNs by encouraging class-speciﬁc ﬁlters, among which each ﬁlter responds to only one (or few) class. Concretely, we design a learnable sparse Class-Speciﬁc Gate (CSG) structure to assign each ﬁlter with one (or few) class in a ﬂexible way. The gate allows a ﬁlter’s activation to pass only when the input samples come from the speciﬁc class. Extensive experiments demonstrate the fabulous performance of our method in generating a sparse and highly classrelated representation of the input, which leads to stronger interpretability. Moreover, comparing with the standard training strategy, our model displays beneﬁts in applications like object localization and adversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Liang, Haoyu and Ouyang, Zhihao and Zeng, Yuyuan and Su, Hang and He, Zihao and Xia, Shu-Tao and Zhu, Jun and Zhang, Bo and He, Zihao and Xia, Shu-Tao and Zhu, Jun and Zhang, Bo},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {622--638},
}

@inproceedings{lee_towards_2019,
	title = {Towards {Robust}, {Locally} {Linear} {Deep} {Network}},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Lee, Guang-He and Alvarez-Melis, David and Jaakkola, Tommi S},
	year = {2019},
	pages = {21},
}

@inproceedings{chang_invariant_2020,
	title = {Invariant {Rationalization}},
	url = {http://proceedings.mlr.press/v119/chang20c.html},
	abstract = {Selective rationalization improves neural network interpretability by identifying a small subset of input features \{—\} the rationale \{—\} that best explains or supports the prediction. A typical rat...},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chang, Shiyu and Zhang, Yang and Yu, Mo and Jaakkola, Tommi},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1448--1458},
}

@inproceedings{melis_towards_2018,
	title = {Towards {Robust} {Interpretability} with {Self}-{Explaining} {Neural} {Networks}},
	abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around speciﬁc predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classiﬁers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization speciﬁcally tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
	language = {en},
	booktitle = {32nd {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Melis, David Alvarez and Jaakkola, Tommi},
	year = {2018},
	pages = {10},
}

@inproceedings{jiang_trust_2018,
	title = {To {Trust} {Or} {Not} {To} {Trust} {A} {Classifier}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/7180cffd6a8e829dacfc2a31b3f72ece-Abstract.html},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jiang, Heinrich and Kim, Been and Guan, Melody and Gupta, Maya},
	year = {2018},
	pages = {5541--5552},
}

@inproceedings{bansal_sam_2020,
	title = {{SAM}: {The} {Sensitivity} of {Attribution} {Methods} to {Hyperparameters}},
	shorttitle = {{SAM}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Bansal_SAM_The_Sensitivity_of_Attribution_Methods_to_Hyperparameters_CVPR_2020_paper.html},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Bansal, Naman and Agarwal, Chirag and Nguyen, Anh},
	year = {2020},
	pages = {8673--8683},
}

@inproceedings{ernst_oscar_2020,
	title = {{OSCAR}: {Occluded} {Stereo} dataset for {Convolutional} {Architectures} with {Recurrence}},
	copyright = {Embargoed Access},
	shorttitle = {{OSCAR}},
	url = {https://zenodo.org/record/3540900},
	doi = {10.5281/ZENODO.3540900},
	abstract = {Recurrent connectivity in the visual cortex is believed to aid object recognition for challenging conditions such as occlusion. Here we investigate if and how artiﬁcial neural networks also beneﬁt from recurrence. We compare architectures composed of bottom-up, lateral and top-down connections and evaluate their performance using two novel stereoscopic occluded object datasets. We ﬁnd that classiﬁcation accuracy is signiﬁcantly higher for recurrent models when compared to feedforward models of matched parametric complexity. Additionally we show that for challenging stimuli, the recurrent feedback is able to correctly revise the initial feedforward guess.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {European {Symposium} on {Artificial} {Neural} {Networks}},
	publisher = {Zenodo},
	author = {Ernst, Markus Roland and Triesch, Jochen and Burwick, Thomas},
	year = {2020},
	note = {Version Number: 1.0
type: dataset},
	keywords = {Computer Vision, Dataset, Deep Learning, Machine Learning, Object Recognition, Occlusion, Recurrent Neural Networks},
}

@inproceedings{liang_knowledge_2020,
	title = {Knowledge {Consistency} {Between} {Neural} {Networks} and {Beyond}},
	abstract = {This paper aims to analyze knowledge consistency between pre-trained deep neural networks. We propose a generic deﬁnition for knowledge consistency between neural networks at different fuzziness levels. A task-agnostic method is designed to disentangle feature components, which represent the consistent knowledge, from raw intermediate-layer features of each neural network. As a generic tool, our method can be broadly used for different applications. In preliminary experiments, we have used knowledge consistency as a tool to diagnose representations of neural networks. Knowledge consistency provides new insights to explain the success of existing deep-learning techniques, such as knowledge distillation and network compression. More crucially, knowledge consistency can also be used to reﬁne pre-trained networks and boost performance.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Liang, Ruofan and Li, Tianlin and Li, Longfei and Wang, Jing and Zhang, Quanshi},
	year = {2020},
	pages = {15},
}

@inproceedings{li_interpretable_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Interpretable {Neural} {Network} {Decoupling}},
	isbn = {978-3-030-58555-6},
	doi = {10.1007/978-3-030-58555-6_39},
	abstract = {The remarkable performance of convolutional neural networks (CNNs) is entangled with their huge number of uninterpretable parameters, which has become the bottleneck limiting the exploitation of their full potential. Towards network interpretation, previous endeavors mainly resort to the single filter analysis, which however ignores the relationship between filters. In this paper, we propose a novel architecture decoupling method to interpret the network from a perspective of investigating its calculation paths. More specifically, we introduce a novel architecture controlling module in each layer to encode the network architecture by a vector. By maximizing the mutual information between the vectors and input images, the module is trained to select specific filters to distill a unique calculation path for each input. Furthermore, to improve the interpretability and compactness of the decoupled network, the output of each layer is encoded to align the architecture encoding vector with the constraint of sparsity regularization. Unlike conventional pixel-level or filter-level network interpretation methods, we propose a path-level analysis to explore the relationship between the combination of filter and semantic concepts, which is more suitable to interpret the working rationale of the decoupled network. Extensive experiments show that the decoupled network achieves several applications, i.e., network interpretation, network acceleration, and adversarial samples detection.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Li, Yuchao and Ji, Rongrong and Lin, Shaohui and Zhang, Baochang and Yan, Chenqian and Wu, Yongjian and Huang, Feiyue and Shao, Ling},
	year = {2020},
	keywords = {Architecture decoupling, Network interpretation},
	pages = {653--669},
}

@inproceedings{konforti_inference_2020,
	address = {Cham},
	title = {Inference {Graphs} for {CNN} {Interpretation}},
	volume = {12370},
	isbn = {978-3-030-58594-5 978-3-030-58595-2},
	url = {http://link.springer.com/10.1007/978-3-030-58595-2_5},
	doi = {10.1007/978-3-030-58595-2_5},
	abstract = {Convolutional neural networks (CNNs) have achieved superior accuracy in many visual related tasks. However, the inference process through intermediate layers is opaque, making it diﬃcult to interpret such networks or develop trust in their operation. We propose to model the network hidden layers activity using probabilistic models. The activity patterns in layers of interest are modeled as Gaussian mixture models, and transition probabilities between clusters in consecutive modeled layers are estimated. Based on maximum-likelihood considerations, nodes and paths relevant for network prediction are chosen, connected, and visualized as an inference graph. We show that such graphs are useful for understanding the general inference process of a class, as well as explaining decisions the network makes regarding speciﬁc images.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Konforti, Yael and Shpigler, Alon and Lerner, Boaz and Bar-Hillel, Aharon and Konforti, Yael and Shpigler, Alon and Lerner, Boaz and Bar-Hillel, Aharon},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {69--84},
}

@inproceedings{jung_icaps_2020,
	address = {Cham},
	title = {{iCaps}: {An} {Interpretable} {Classifier} via {Disentangled} {Capsule} {Networks}},
	volume = {12364},
	isbn = {978-3-030-58528-0 978-3-030-58529-7},
	shorttitle = {{iCaps}},
	url = {http://link.springer.com/10.1007/978-3-030-58529-7_19},
	doi = {10.1007/978-3-030-58529-7_19},
	abstract = {We propose an interpretable Capsule Network, iCaps, for image classiﬁcation. A capsule is a group of neurons nested inside each layer, and the one in the last layer is called a class capsule, which is a vector whose norm indicates a predicted probability for the class. Using the class capsule, existing Capsule Networks already provide some level of interpretability. However, there are two limitations which degrade its interpretability: 1) the class capsule also includes classiﬁcation-irrelevant information, and 2) entities represented by the class capsule overlap. In this work, we address these two limitations using a novel class-supervised disentanglement algorithm and an additional regularizer, respectively. Through quantitative and qualitative evaluations on three datasets, we demonstrate that the resulting classiﬁer, iCaps, provides a prediction along with clear rationales behind it with no performance degradation.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Jung, Dahuin and Lee, Jonghyun and Yi, Jihun and Yoon, Sungroh and Jung, Dahuin and Lee, Jonghyun and Yi, Jihun and Yoon, Sungroh},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {314--330},
}

@inproceedings{ortiz-jimenez_hold_2020,
	title = {Hold me tight! {Inﬂuence} of discriminative features on deep network boundaries},
	abstract = {Important insights towards the explainability of neural networks reside in the characteristics of their decision boundaries. In this work, we borrow tools from the ﬁeld of adversarial robustness, and propose a new perspective that relates dataset features to the distance of samples to the decision boundary. This enables us to carefully tweak the position of the training samples and measure the induced changes on the boundaries of CNNs trained on large-scale vision datasets. We use this framework to reveal some intriguing properties of CNNs. Speciﬁcally, we rigorously conﬁrm that neural networks exhibit a high invariance to non-discriminative features, and show that the decision boundaries of a DNN can only exist as long as the classiﬁer is trained with some features that hold them together. Finally, we show that the construction of the decision boundary is extremely sensitive to small perturbations of the training samples, and that changes in certain directions can lead to sudden invariances in the orthogonal ones. This is precisely the mechanism that adversarial training uses to achieve robustness.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ortiz-Jiménez, Guillermo and Moosavi-Dezfooli, Seyed-Mohsen},
	year = {2020},
	pages = {12},
}

@inproceedings{nie_theoretical_2018,
	title = {A {Theoretical} {Explanation} for {Perplexing} {Behaviors} of {Backpropagation}-based {Visualizations}},
	abstract = {Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Speciﬁcally, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.},
	language = {en},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	author = {Nie, Weili and Zhang, Yang and Patel, Ankit B},
	year = {2018},
	pages = {10},
}

@inproceedings{wang_high-frequency_2020,
	title = {High-{Frequency} {Component} {Helps} {Explain} the {Generalization} of {Convolutional} {Neural} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks_CVPR_2020_paper.html},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Haohan and Wu, Xindi and Huang, Zeyi and Xing, Eric P.},
	year = {2020},
	pages = {8684--8694},
}

@article{lakkaraju_interpretable_2017,
	title = {Interpretable \& {Explorable} {Approximations} of {Black} {Box} {Models}},
	url = {http://arxiv.org/abs/1707.01154},
	abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
	urldate = {2020-12-25},
	journal = {arXiv:1707.01154 [cs]},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01154},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{dinu_challenging_2020,
	title = {Challenging common interpretability assumptions in feature attribution explanations},
	abstract = {As machine learning and algorithmic decision making systems are increasingly being leveraged in high-stakes human-in-the-loop settings, there is a pressing need to understand the rationale of their predictions. Researchers have responded to this need with explainable AI (XAI), but often proclaim interpretability axiomatically without evaluation. When these systems are evaluated, they are often tested through ofﬂine simulations with proxy metrics of interpretability (such as model complexity). We empirically evaluate the veracity of three common interpretability assumptions through a large scale human-subjects experiment with a simple “placebo explanation” control. We ﬁnd that feature attribution explanations provide marginal utility in our task for a human decision maker and in certain cases result in worse decisions due to cognitive and contextual confounders. This result challenges the assumed universal beneﬁt of applying these methods and we hope this work will underscore the importance of human evaluation in XAI research. Supplemental materials—including anonymized data from the experiment, code to replicate the study, an interactive demo of the experiment, and the models used in the analysis—can be found at: https://doi.pizza/challenging-xai.},
	language = {en},
	booktitle = {{NeurIPS}  {Workshop}: {ML} {Retrospectives}, {Surveys} \& {Meta}-{Analyses} ({ML}-{RSA}).},
	author = {Dinu, Jonathan and Bigham, Jeffrey and Kolter, J Zico},
	year = {2020},
	pages = {15},
}

@inproceedings{rafique_transparency_2020,
	title = {Transparency {Promotion} with {Model}-{Agnostic} {Linear} {Competitors}},
	url = {http://proceedings.mlr.press/v119/rafique20a.html},
	abstract = {We propose a novel type of hybrid model for multi-class classification, which utilizes competing linear models to collaborate with an existing black-box model, promoting transparency in the decisio...},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rafique, Hassan and Wang, Tong and Lin, Qihang and Singhani, Arshia},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7898--7908},
}

@inproceedings{pan_interpretable_2020,
	title = {Interpretable {Companions} for {Black}-{Box} {Models}},
	url = {http://proceedings.mlr.press/v108/pan20a.html},
	abstract = {We present an interpretable companion model for any pre-trained black-box classifiers. The idea is that for any input, a user can decide to either receive a prediction from the black-box model, wit...},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Pan, Danqing and Wang, Tong and Hara, Satoshi},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2444--2454},
}

@inproceedings{kim_examples_2016,
	title = {Examples are not enough, learn to criticize! {Criticism} for {Interpretability}},
	volume = {29},
	url = {https://papers.nips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html},
	abstract = {Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufﬁcient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efﬁciently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classiﬁer, showing competitive performance compared to baselines.},
	language = {en},
	urldate = {2020-12-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi O.},
	year = {2016},
	pages = {2280--2288},
}

@article{qi_embedding_2021,
	title = {Embedding {Deep} {Networks} into {Visual} {Explanations}},
	volume = {292},
	issn = {00043702},
	url = {http://arxiv.org/abs/1709.05360},
	doi = {10.1016/j.artint.2020.103435},
	abstract = {In this paper, we propose a novel Explanation Neural Network (XNN) to explain the predictions made by a deep network. The XNN works by learning a nonlinear embedding of a high-dimensional activation vector of a deep network layer into a low-dimensional explanation space while retaining faithfulness i.e., the original deep learning predictions can be constructed from the few concepts extracted by our explanation network. We then visualize such concepts for human to learn about the high-level concepts that the deep network is using to make decisions. We propose an algorithm called Sparse Reconstruction Autoencoder (SRAE) for learning the embedding to the explanation space. SRAE aims to reconstruct part of the original feature space while retaining faithfulness. A pull-away term is applied to SRAE to make the bases of the explanation space more orthogonal to each other. A visualization system is then introduced for human understanding of the features in the explanation space. The proposed method is applied to explain CNN models in image classiﬁcation tasks. We conducted a human study, which shows that the proposed approach outperforms single saliency map baselines, and improves human performance on a difﬁcult classiﬁcation tasks. Also, several novel metrics are introduced to evaluate the performance of explanations quantitatively without human involvement.},
	language = {en},
	urldate = {2020-12-24},
	journal = {Artificial Intelligence},
	author = {Qi, Zhongang and Khorram, Saeed and Li, Fuxin},
	month = mar,
	year = {2021},
	note = {arXiv: 1709.05360},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	pages = {103435},
}

@article{higgins_towards_2018,
	title = {Towards a {Definition} of {Disentangled} {Representations}},
	url = {http://arxiv.org/abs/1812.02230},
	abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
	language = {en},
	urldate = {2020-12-24},
	journal = {arXiv:1812.02230 [cs, stat]},
	author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.02230},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{van_steenkiste_are_2019,
	title = {Are {Disentangled} {Representations} {Helpful} for {Abstract} {Visual} {Reasoning}?},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/bc3c4a6331a8a9950945a1aa8c95ab8a-Abstract.html},
	language = {en},
	urldate = {2020-12-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {van Steenkiste, Sjoerd and Locatello, Francesco and Schmidhuber, Jürgen and Bachem, Olivier},
	year = {2019},
	pages = {14245--14258},
}

@article{rudin_stop_2019,
	title = {Stop {Explaining} {Black} {Box} {Machine} {Learning} {Models} for {High} {Stakes} {Decisions} and {Use} {Interpretable} {Models} {Instead}},
	url = {http://arxiv.org/abs/1811.10154},
	abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to explain black box models, rather than creating models that are interpretable in the ﬁrst place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward – it is to design models that are inherently interpretable. This manuscript clariﬁes the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identiﬁes challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
	language = {en},
	urldate = {2020-12-23},
	journal = {arXiv:1811.10154 [cs, stat]},
	author = {Rudin, Cynthia},
	month = sep,
	year = {2019},
	note = {arXiv: 1811.10154},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wang_interpretability_2020,
	address = {Virtual Event CA USA},
	title = {Interpretability is a {Kind} of {Safety}: {An} {Interpreter}-based {Ensemble} for {Adversary} {Defense}},
	isbn = {978-1-4503-7998-4},
	shorttitle = {Interpretability is a {Kind} of {Safety}},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403044},
	doi = {10.1145/3394486.3403044},
	abstract = {While having achieved great success in rich real-life applications, deep neural network (DNN) models have long been criticized for their vulnerability to adversarial attacks. Tremendous research efforts have been dedicated to mitigating the threats of adversarial attacks, but the essential trait of adversarial examples is not yet clear, and most existing methods are yet vulnerable to hybrid attacks and suffer from counterattacks. In light of this, in this paper, we first reveal a gradient-based correlation between sensitivity analysisbased DNN interpreters and the generation process of adversarial examples, which indicates the Achilles’s heel of adversarial attacks and sheds light on linking together the two long-standing challenges of DNN: fragility and unexplainability. We then propose an interpreter-based ensemble framework called X-Ensemble for robust adversary defense. X-Ensemble adopts a novel detectionrectification process and features in building multiple sub-detectors and a rectifier upon various types of interpretation information toward target classifiers. Moreover, X-Ensemble employs the Random Forests (RF) model to combine sub-detectors into an ensemble detector for adversarial hybrid attacks defense. The non-differentiable property of RF further makes it a precious choice against the counterattack of adversaries. Extensive experiments under various types of state-of-the-art attacks and diverse attack scenarios demonstrate the advantages of X-Ensemble to competitive baseline methods.},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Wang, Jingyuan and Wu, Yufan and Li, Mingxuan and Lin, Xin and Wu, Junjie and Li, Chao},
	month = aug,
	year = {2020},
	pages = {15--24},
}

@article{papernot_practical_2017,
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	url = {http://arxiv.org/abs/1602.02697},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modiﬁed to yield erroneous model outputs, while appearing unmodiﬁed to human observers. Potential attacks include having malicious content like malware identiﬁed as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the ﬁrst practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and ﬁnd that they are misclassiﬁed by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We ﬁnd that their DNN misclassiﬁes 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassiﬁed by Amazon and Google at rates of 96.19\% and 88.94\%. We also ﬁnd that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	language = {en},
	urldate = {2020-12-23},
	journal = {arXiv:1602.02697 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	month = mar,
	year = {2017},
	note = {arXiv: 1602.02697},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{lanfredi_quantifying_2020,
	title = {Quantifying the {Preferential} {Direction} of the {Model} {Gradient} in {Adversarial} {Training} {With} {Projected} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/2009.04709},
	abstract = {Adversarial training, especially projected gradient descent (PGD), has been the most successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs are meaningful and interpretable by humans. However, the concept of interpretability is not mathematically well established, making it difﬁcult to evaluate it quantitatively. We deﬁne interpretability as the alignment of the model gradient with the vector pointing toward the closest point of the support of the other class. We propose a method for measuring this alignment for binary classiﬁcation problems, using generative adversarial model training to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models are more interpretable than the baseline according to our deﬁnition, and our metric presents higher alignment values than a competing metric formulation. We also show that enforcing this alignment increases the robustness of models without adversarial training.},
	language = {en},
	urldate = {2020-12-23},
	journal = {arXiv:2009.04709 [cs, stat]},
	author = {Lanfredi, Ricardo Bigolin and Schroeder, Joyce D. and Tasdizen, Tolga},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.04709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{fukui_attention_2019,
	address = {Long Beach, CA, USA},
	title = {Attention {Branch} {Network}: {Learning} of {Attention} {Mechanism} for {Visual} {Explanation}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Attention {Branch} {Network}},
	url = {https://ieeexplore.ieee.org/document/8953929/},
	doi = {10.1109/CVPR.2019.01096},
	abstract = {Visual explanation enables humans to understand the decision making of deep convolutional neural network (CNN), but it is insufﬁcient to contribute to improving CNN performance. In this paper, we focus on the attention map for visual explanation, which represents a high response value as the attention location in image recognition. This attention region signiﬁcantly improves the performance of CNN by introducing an attention mechanism that focuses on a speciﬁc region in an image. In this work, we propose Attention Branch Network (ABN), which extends a response-based visual explanation model by introducing a branch structure with an attention mechanism. ABN can be applicable to several image recognition tasks by introducing a branch for the attention mechanism and is trainable for visual explanation and image recognition in an end-to-end manner. We evaluate ABN on several image recognition tasks such as image classiﬁcation, ﬁne-grained recognition, and multiple facial attribute recognition. Experimental results indicate that ABN outperforms the baseline models on these image recognition tasks while generating an attention map for visual explanation. Our code is available 1.},
	language = {en},
	urldate = {2020-12-23},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Fukui, Hiroshi and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
	month = jun,
	year = {2019},
	pages = {10697--10706},
}

@article{noack_empirical_2020,
	title = {An {Empirical} {Study} on the {Relation} between {Network} {Interpretability} and {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/1912.03430},
	abstract = {Deep neural networks (DNNs) have had many successes, but they suffer from two major issues: (1) a vulnerability to adversarial examples and (2) a tendency to elude human interpretation. Interestingly, recent empirical and theoretical evidence suggests these two seemingly disparate issues are actually connected. In particular, robust models tend to provide more interpretable gradients than non-robust models. However, whether this relationship works in the opposite direction remains obscure. With this paper, we seek empirical answers to the following question: can models acquire adversarial robustness when they are trained to have interpretable gradients? We introduce a theoretically inspired technique called Interpretation Regularization (IR), which encourages a model's gradients to (1) match the direction of interpretable target salience maps and (2) have small magnitude. To assess model performance and tease apart factors that contribute to adversarial robustness, we conduct extensive experiments on MNIST and CIFAR-10 with both \${\textbackslash}ell\_2\$ and \${\textbackslash}ell\_{\textbackslash}infty\$ attacks. We demonstrate that training the networks to have interpretable gradients improves their robustness to adversarial perturbations. Applying the network interpretation technique SmoothGrad yields additional performance gains, especially in cross-norm attacks and under heavy perturbations. The results indicate that the interpretability of the model gradients is a crucial factor for adversarial robustness. Code for the experiments can be found at https://github.com/a1noack/interp\_regularization.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1912.03430 [cs, stat]},
	author = {Noack, Adam and Ahern, Isaac and Dou, Dejing and Li, Boyang},
	month = dec,
	year = {2020},
	note = {arXiv: 1912.03430},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yuan_causal_2020,
	title = {Causal inference using deep neural networks},
	url = {http://arxiv.org/abs/2011.12508},
	abstract = {Causal inference from observation data is a core problem in many scientiﬁc ﬁelds. Here we present a general supervised deep learning framework that infers causal interactions by transforming the input vectors to an image-like representation for every pair of inputs. Given a training dataset we ﬁrst construct a normalized empirical probability density distribution (N EP DF ) matrix. We then train a convolutional neural network (CNN) on N EP DF s for causality predictions. We tested the method on several different simulated and real world data and compared it to prior methods for causal inference. As we show, the method is general, can efﬁciently handle very large datasets and improves upon prior methods.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:2011.12508 [cs, stat]},
	author = {Yuan, Ye and Ding, Xueying and Bar-Joseph, Ziv},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.12508},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{balaji_instance_2019,
	title = {Instance adaptive adversarial training: {Improved} accuracy tradeoffs in neural nets},
	shorttitle = {Instance adaptive adversarial training},
	url = {http://arxiv.org/abs/1910.08051},
	abstract = {Adversarial training is by far the most successful strategy for improving robustness of neural networks to adversarial attacks. Despite its success as a defense mechanism, adversarial training fails to generalize well to unperturbed test set. We hypothesize that this poor generalization is a consequence of adversarial training with uniform perturbation radius around every training sample. Samples close to decision boundary can be morphed into a different class under a small perturbation budget, and enforcing large margins around these samples produce poor decision boundaries that generalize poorly. Motivated by this hypothesis, we propose instance adaptive adversarial training – a technique that enforces samplespeciﬁc perturbation margins around every training sample. We show that using our approach, test accuracy on unperturbed samples improve with a marginal drop in robustness. Extensive experiments on CIFAR-10, CIFAR-100 and Imagenet datasets demonstrate the effectiveness of our proposed approach.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1910.08051 [cs, stat]},
	author = {Balaji, Yogesh and Goldstein, Tom and Hoffman, Judy},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.08051},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sitawarin_improving_2020,
	title = {Improving {Adversarial} {Robustness} {Through} {Progressive} {Hardening}},
	url = {http://arxiv.org/abs/2003.09347},
	abstract = {Adversarial training (AT) has become a popular choice for training robust networks. However, it tends to sacriﬁce clean accuracy heavily in favor of robustness, and with a large perturbation, it can cause models to learn a trivial solution, always predicting the same class. To address the above concerns, we propose Adversarial Training with Early Stopping (ATES), guided by principles from curriculum learning that emphasizes on starting “easy” and gradually ramping up on the “difﬁculty” of training. ATES is derived from our formulation for curriculum learning in the adversarial setting which introduces an additional curriculum constraint to the normal adversarial loss. To satisfy this constraint, we apply early stopping on the adversarial example generation step when a speciﬁed level of difﬁculty is reached. ATES stabilizes network training even for a large perturbation norm and allows the network to operate at a better clean accuracy versus robustness trade-off curve compared to AT. This leads to a signiﬁcant improvement in both clean accuracy and robustness compared to AT, TRADES, and the other baselines.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:2003.09347 [cs, stat]},
	author = {Sitawarin, Chawin and Chakraborty, Supriyo and Wagner, David},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.09347},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zhang_theoretically_2019,
	title = {Theoretically {Principled} {Trade}-off between {Robustness} and {Accuracy}},
	url = {http://proceedings.mlr.press/v97/zhang19p.html},
	abstract = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empi...},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and Ghaoui, Laurent El and Jordan, Michael},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7472--7482},
}

@article{miyato_virtual_2019,
	title = {Virtual {Adversarial} {Training}: {A} {Regularization} {Method} for {Supervised} and {Semi}-{Supervised} {Learning}},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Virtual {Adversarial} {Training}},
	url = {https://ieeexplore.ieee.org/document/8417973/},
	doi = {10.1109/TPAMI.2018.2858821},
	abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is deﬁned as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method deﬁnes the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only “virtually” adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.},
	language = {en},
	number = {8},
	urldate = {2020-12-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Miyato, Takeru and Maeda, Shin-Ichi and Koyama, Masanori and Ishii, Shin},
	month = aug,
	year = {2019},
	pages = {1979--1993},
}

@inproceedings{stutz_disentangling_2019,
	address = {Long Beach, CA, USA},
	title = {Disentangling {Adversarial} {Robustness} and {Generalization}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953455/},
	doi = {10.1109/CVPR.2019.00714},
	abstract = {Obtaining deep networks that are robust against adversarial examples and generalize well is an open problem. A recent hypothesis [102, 95] even states that both robust and accurate models are impossible, i.e., adversarial robustness and generalization are conﬂicting goals. In an effort to clarify the relationship between robustness and generalization, we assume an underlying, low-dimensional data manifold and show that: 1. regular adversarial examples leave the manifold; 2. adversarial examples constrained to the manifold, i.e., on-manifold adversarial examples, exist; 3. on–manifold adversarial examples are generalization errors, and on-manifold adversarial training boosts generalization; 4. regular robustness and generalization are not necessarily contradicting goals. These assumptions imply that both robust and accurate models are possible. However, different models (architectures, training strategies etc.) can exhibit different robustness and generalization characteristics. To conﬁrm our claims, we present extensive experiments on synthetic data (with known manifold) as well as on EMNIST [19], Fashion-MNIST [106] and CelebA [58].},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	month = jun,
	year = {2019},
	pages = {6969--6980},
}

@article{cocarascu_argumentation_nodate,
	title = {Argumentation for {Machine} {Learning}: {A} {Survey}},
	abstract = {Existing approaches using argumentation to aid or improve machine learning differ in the type of machine learning technique they consider, in their use of argumentation and in their choice of argumentation framework and semantics. This paper presents a survey of this relatively young ﬁeld highlighting, in particular, its achievements to date, the applications it has been used for as well as the beneﬁts brought about by the use of argumentation, with an eye towards its future.},
	language = {en},
	author = {Cocarascu, Oana and Toni, Francesca},
	pages = {12},
}

@incollection{vedaldi_semanticadv_2020,
	address = {Cham},
	title = {{SemanticAdv}: {Generating} {Adversarial} {Examples} via {Attribute}-{Conditioned} {Image} {Editing}},
	volume = {12359},
	isbn = {978-3-030-58567-9 978-3-030-58568-6},
	shorttitle = {{SemanticAdv}},
	url = {http://link.springer.com/10.1007/978-3-030-58568-6_2},
	abstract = {Recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee “subtle perturbation” by limiting the Lp norm of the perturbation. In this paper, we propose SemanticAdv to generate a new type of semantically realistic adversarial examples via attribute-conditioned image editing. Compared to existing methods, our SemanticAdv enables ﬁne-grained analysis and evaluation of DNNs with input variations in the attribute space. We conduct comprehensive experiments to show that our adversarial examples not only exhibit semantically meaningful appearances but also achieve high targeted attack success rates under both whitebox and blackbox settings. Moreover, we show that the existing pixel-based and attribute-based defense methods fail to defend against SemanticAdv . We demonstrate the applicability of SemanticAdv on both face recognition and general street-view images to show its generalization. We believe that our work can shed light on further understanding about vulnerabilities of DNNs as well as novel defense approaches. Our implementation is available at https://github.com/AIsecure/SemanticAdv.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Qiu, Haonan and Xiao, Chaowei and Yang, Lei and Yan, Xinchen and Lee, Honglak and Li, Bo},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58568-6_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {19--37},
}

@inproceedings{vedaldi_adversarial_2020,
	address = {Cham},
	title = {Adversarial {Robustness} on {In}- and {Out}-{Distribution} {Improves} {Explainability}},
	volume = {12371},
	isbn = {978-3-030-58573-0 978-3-030-58574-7},
	url = {http://link.springer.com/10.1007/978-3-030-58574-7_14},
	doi = {10.1007/978-3-030-58574-7_14},
	abstract = {Neural networks have led to major improvements in image classiﬁcation but suﬀer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Outdistribution, which leads to robust models with reliable and robust conﬁdence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class speciﬁc features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art l2-adversarial robustness on CIFAR10 and maintains better clean accuracy.},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Augustin, Maximilian and Meinke, Alexander and Hein, Matthias},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {228--245},
}

@article{goyal_inductive_2020,
	title = {Inductive {Biases} for {Deep} {Learning} of {Higher}-{Level} {Cognition}},
	url = {http://arxiv.org/abs/2011.15091},
	abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be suﬃcient to predict the behavior of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems beneﬁting from humans’ abilities in terms of ﬂexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:2011.15091 [cs, stat]},
	author = {Goyal, Anirudh and Bengio, Yoshua},
	month = dec,
	year = {2020},
	note = {arXiv: 2011.15091},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bastani_interpreting_2019,
	title = {Interpreting {Blackbox} {Models} via {Model} {Extraction}},
	url = {http://arxiv.org/abs/1705.08504},
	abstract = {Interpretability has become incredibly important as machine learning is increasingly used to inform consequential decisions. We propose to construct global explanations of complex, blackbox models in the form of a decision tree approximating the original model—as long as the decision tree is a good approximation, then it mirrors the computation performed by the blackbox model. We devise a novel algorithm for extracting decision tree explanations that actively samples new training points to avoid overﬁtting. We evaluate our algorithm on a random forest to predict diabetes risk and a learned controller for cart-pole. Compared to several baselines, our decision trees are both substantially more accurate and equally or more interpretable based on a user study. Finally, we describe several insights provided by our interpretations, including a causal issue validated by a physician.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1705.08504 [cs]},
	author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
	month = jan,
	year = {2019},
	note = {arXiv: 1705.08504},
	keywords = {Computer Science - Machine Learning},
}

@article{bouchacourt_educe_2019,
	title = {{EDUCE}: {Explaining} model {Decisions} through {Unsupervised} {Concepts} {Extraction}},
	shorttitle = {{EDUCE}},
	url = {http://arxiv.org/abs/1905.11852},
	abstract = {Providing explanations along with predictions is crucial in some text processing tasks. Therefore, we propose a new self-interpretable model that performs output prediction and simultaneously provides an explanation in terms of the presence of particular concepts in the input. To do so, our model’s prediction relies solely on a low-dimensional binary representation of the input, where each feature denotes the presence or absence of concepts. The presence of a concept is decided from an excerpt i.e. a small sequence of consecutive words in the text. Relevant concepts for the prediction task at hand are automatically deﬁned by our model, avoiding the need for concept-level annotations. To ease interpretability, we enforce that for each concept, the corresponding excerpts share similar semantics and are differentiable from each others. We experimentally demonstrate the relevance of our approach on text classiﬁcation and multi-sentiment analysis tasks.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1905.11852 [cs, stat]},
	author = {Bouchacourt, Diane and Denoyer, Ludovic},
	month = sep,
	year = {2019},
	note = {arXiv: 1905.11852},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{puri_magix_2018,
	title = {{MAGIX}: {Model} {Agnostic} {Globally} {Interpretable} {Explanations}},
	shorttitle = {{MAGIX}},
	url = {http://arxiv.org/abs/1706.07160},
	abstract = {Explaining the behavior of a black box machine learning model at the instance level is useful for building trust. However, it is also important to understand how the model behaves globally. Such an understanding provides insight into both the data on which the model was trained and the patterns that it learned. We present here an approach that learns if-then rules to globally explain the behavior of black box machine learning models that have been used to solve classiﬁcation problems. The approach works by ﬁrst extracting conditions that were important at the instance level and then evolving rules through a genetic algorithm with an appropriate ﬁtness function. Collectively, these rules represent the patterns followed by the model for decisioning and are useful for understanding its behavior. We demonstrate the validity and usefulness of the approach by interpreting black box models created using publicly available data sets as well as a private digital marketing data set.},
	language = {en},
	urldate = {2020-12-22},
	journal = {arXiv:1706.07160 [cs]},
	author = {Puri, Nikaash and Gupta, Piyush and Agarwal, Pratiksha and Verma, Sukriti and Krishnamurthy, Balaji},
	month = jun,
	year = {2018},
	note = {arXiv: 1706.07160},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{he_extract_2020,
	title = {Extract interpretability-accuracy balanced rules from artificial neural networks: {A} review},
	volume = {387},
	issn = {0925-2312},
	shorttitle = {Extract interpretability-accuracy balanced rules from artificial neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231220300801},
	doi = {10.1016/j.neucom.2020.01.036},
	abstract = {Artificial neural networks (ANN) have been widely used and have achieved remarkable achievements. However, neural networks with high accuracy and good performance often have extremely complex internal structures such as deep neural networks (DNN). This shortcoming makes the neural networks as incomprehensible as a black box, which is unacceptable in some practical applications. But pursuing excessive interpretation of the neural networks will make the performance of the model worse. Based on this contradictory issue, we first summarize the mainstream methods about quantitatively evaluating the accuracy and interpretability of rule set. And then review existing methods on extracting rules from Multilayer Perceptron (MLP) and DNN in three categories: Decomposition Approach (Extract rules in neuron level such as visualizing the structure of network), Pedagogical Approach (By studying the correspondence between input and output such as by computing gradient) and Eclectics Approach (Combine the above two ideas). Some potential research directions about extracting rules from DNN are discussed in the last.},
	language = {en},
	urldate = {2020-12-22},
	journal = {Neurocomputing},
	author = {He, Congjie and Ma, Meng and Wang, Ping},
	month = apr,
	year = {2020},
	keywords = {Accuracy, Deep neural network, Interpretability, Multilayer Perceptron, Rule extraction},
	pages = {346--358},
}

@inproceedings{foerster_input_2017,
	title = {Input {Switched} {Affine} {Networks}: {An} {RNN} {Architecture} {Designed} for {Interpretability}},
	shorttitle = {Input {Switched} {Affine} {Networks}},
	url = {http://proceedings.mlr.press/v70/foerster17a.html},
	abstract = {There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transf...},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Foerster, Jakob N. and Gilmer, Justin and Sohl-Dickstein, Jascha and Chorowski, Jan and Sussillo, David},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1136--1145},
}

@inproceedings{lakkaraju_faithful_2019,
	address = {New York, NY, USA},
	series = {{AIES} '19},
	title = {Faithful and {Customizable} {Explanations} of {Black} {Box} {Models}},
	isbn = {978-1-4503-6324-2},
	url = {https://doi.org/10.1145/3306618.3314229},
	doi = {10.1145/3306618.3314229},
	abstract = {As predictive models increasingly assist human experts (e.g., doctors) in day-to-day decision making, it is crucial for experts to be able to explore and understand how such models behave in different feature subspaces in order to know if and when to trust them. To this end, we propose Model Understanding through Subspace Explanations (MUSE), a novel model agnostic framework which facilitates understanding of a given black box model by explaining how it behaves in subspaces characterized by certain features of interest. Our framework provides end users (e.g., doctors) with the flexibility of customizing the model explanations by allowing them to input the features of interest. The construction of explanations is guided by a novel objective function that we propose to simultaneously optimize for fidelity to the original model, unambiguity and interpretability of the explanation. More specifically, our objective allows us to learn, with optimality guarantees, a small number of compact decision sets each of which captures the behavior of a given black box model in unambiguous, well-defined regions of the feature space. Experimental evaluation with real-world datasets and user studies demonstrate that our approach can generate customizable, highly compact, easy-to-understand, yet accurate explanations of various kinds of predictive models compared to state-of-the-art baselines.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	month = jan,
	year = {2019},
	keywords = {black box models, decision making, interpretable machine learning},
	pages = {131--138},
}

@inproceedings{kumar_problems_2020,
	title = {Problems with {Shapley}-value-based explanations as feature importance measures},
	url = {http://proceedings.mlr.press/v119/kumar20e.html},
	abstract = {Game-theoretic formulations of feature importance have become popular as a way to},
	language = {en},
	urldate = {2020-12-22},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kumar, I. Elizabeth and Venkatasubramanian, Suresh and Scheidegger, Carlos and Friedler, Sorelle},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {5491--5500},
}

@inproceedings{lakkaraju_interpretable_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {Interpretable {Decision} {Sets}: {A} {Joint} {Framework} for {Description} and {Prediction}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {Interpretable {Decision} {Sets}},
	url = {https://doi.org/10.1145/2939672.2939874},
	doi = {10.1145/2939672.2939874},
	abstract = {One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Lakkaraju, Himabindu and Bach, Stephen H. and Leskovec, Jure},
	month = aug,
	year = {2016},
	keywords = {classification, decision sets, interpretable machine learning, submodularity},
	pages = {1675--1684},
}

@inproceedings{tan_distill-and-compare_2018,
	address = {New York, NY, USA},
	series = {{AIES} '18},
	title = {Distill-and-{Compare}: {Auditing} {Black}-{Box} {Models} {Using} {Transparent} {Model} {Distillation}},
	isbn = {978-1-4503-6012-8},
	shorttitle = {Distill-and-{Compare}},
	url = {https://doi.org/10.1145/3278721.3278725},
	doi = {10.1145/3278721.3278725},
	abstract = {Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models. We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground truth outcomes, and use differences between the two models to gain insight into the black-box model. We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},
	month = dec,
	year = {2018},
	keywords = {black-box models, distillation, fairness, interpretability},
	pages = {303--310},
}

@inproceedings{slack_fooling_2020,
	address = {New York, NY, USA},
	series = {{AIES} '20},
	title = {Fooling {LIME} and {SHAP}: {Adversarial} {Attacks} on {Post} hoc {Explanation} {Methods}},
	isbn = {978-1-4503-7110-0},
	shorttitle = {Fooling {LIME} and {SHAP}},
	url = {https://doi.org/10.1145/3375627.3375830},
	doi = {10.1145/3375627.3375830},
	abstract = {As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
	month = feb,
	year = {2020},
	keywords = {adversarial attacks, bias detection, black box explanations, model interpretability},
	pages = {180--186},
}

@article{lee_game-theoretic_2018,
	title = {Game-{Theoretic} {Interpretability} for {Temporal} {Modeling}},
	url = {http://arxiv.org/abs/1807.00130},
	abstract = {Interpretability has arisen as a key desideratum of machine learning models alongside performance. Approaches so far have been primarily concerned with ﬁxed dimensional inputs emphasizing feature relevance or selection. In contrast, we focus on temporal modeling and the problem of tailoring the predictor, functionally, towards an interpretable family. To this end, we propose a co-operative game between the predictor and an explainer without any a priori restrictions on the functional class of the predictor. The goal of the explainer is to highlight, locally, how well the predictor conforms to the chosen interpretable family of temporal models. Our co-operative game is setup asymmetrically in terms of information sets for efﬁciency reasons. We develop and illustrate the framework in the context of temporal sequence models with examples.},
	language = {en},
	urldate = {2020-12-21},
	journal = {arXiv:1807.00130 [cs, stat]},
	author = {Lee, Guang-He and Alvarez-Melis, David and Jaakkola, Tommi S.},
	month = jun,
	year = {2018},
	note = {arXiv: 1807.00130},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{saxe_information_2019,
	title = {On the information bottleneck theory of deep learning},
	volume = {2019},
	issn = {1742-5468},
	url = {https://doi.org/10.1088/1742-5468/ab3985},
	doi = {10.1088/1742-5468/ab3985},
	abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case, and instead reflect assumptions made to compute a finite mutual information metric in deterministic networks. When computed using simple binning, we demonstrate through a combination of analytical results and simulation that the information plane trajectory observed in prior work is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period.},
	language = {en},
	number = {12},
	urldate = {2020-12-21},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Saxe, Andrew M. and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D. and Cox, David D.},
	month = dec,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {124020},
}

@inproceedings{locatello_challenging_2019,
	title = {Challenging {Common} {Assumptions} in the {Unsupervised} {Learning} of {Disentangled} {Representations}},
	url = {http://proceedings.mlr.press/v97/locatello19a.html},
	abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised l...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4114--4124},
}

@inproceedings{liu_adversarial_2018,
	address = {New York, NY, USA},
	series = {{KDD} '18},
	title = {Adversarial {Detection} with {Model} {Interpretation}},
	isbn = {978-1-4503-5552-0},
	url = {https://doi.org/10.1145/3219819.3220027},
	doi = {10.1145/3219819.3220027},
	abstract = {Machine learning (ML) systems have been increasingly applied in web security applications such as spammer detection, malware detection and fraud detection. These applications have an intrinsic adversarial nature where intelligent attackers can adaptively change their behaviors to avoid being detected by the deployed detectors. Existing efforts against adversaries are usually limited by the type of applied ML models or the specific applications such as image classification. Additionally, the working mechanisms of ML models usually cannot be well understood by users, which in turn impede them from understanding the vulnerabilities of models nor improving their robustness. To bridge the gap, in this paper, we propose to investigate whether model interpretation could potentially help adversarial detection. Specifically, we develop a novel adversary-resistant detection framework by utilizing the interpretation of ML models. The interpretation process explains the mechanism of how the target ML model makes prediction for a given instance, thus providing more insights for crafting adversarial samples. The robustness of detectors is then improved through adversarial training with the adversarial samples. A data-driven method is also developed to empirically estimate costs of adversaries in feature manipulation. Our approach is model-agnostic and can be applied to various types of classification models. Our experimental results on two real-world datasets demonstrate the effectiveness of interpretation-based attacks and how estimated feature manipulation cost would affect the behavior of adversaries.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Ninghao and Yang, Hongxia and Hu, Xia},
	month = jul,
	year = {2018},
	keywords = {adversarial detection, machine learning interpretation, spammer detection},
	pages = {1803--1811},
}

@inproceedings{alvarez-melis_causal_2017,
	address = {Copenhagen, Denmark},
	title = {A causal framework for explaining the predictions of black-box sequence-to-sequence models},
	url = {https://www.aclweb.org/anthology/D17-1042},
	doi = {10.18653/v1/D17-1042},
	abstract = {We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an “explanation” consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Alvarez-Melis, David and Jaakkola, Tommi},
	month = sep,
	year = {2017},
	pages = {412--421},
}

@inproceedings{serrano_is_2019,
	address = {Florence, Italy},
	title = {Is {Attention} {Interpretable}?},
	url = {https://www.aclweb.org/anthology/P19-1282},
	doi = {10.18653/v1/P19-1282},
	abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Serrano, Sofia and Smith, Noah A.},
	month = jul,
	year = {2019},
	pages = {2931--2951},
}

@inproceedings{lee_functional_2019,
	title = {Functional {Transparency} for {Structured} {Data}: a {Game}-{Theoretic} {Approach}},
	shorttitle = {Functional {Transparency} for {Structured} {Data}},
	url = {http://proceedings.mlr.press/v97/lee19b.html},
	abstract = {We provide a new approach to training neural models to exhibit transparency in a well-defined, functional manner. Our approach naturally operates over structured data and tailors the predictor, fun...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lee, Guang-He and Jin, Wengong and Alvarez-Melis, David and Jaakkola, Tommi},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {3723--3733},
}

@article{darlow_what_2020,
	title = {What {Information} {Does} a {ResNet} {Compress}?},
	url = {http://arxiv.org/abs/2003.06254},
	abstract = {The information bottleneck principle (Shwartz-Ziv \& Tishby, 2017) suggests that SGD-based training of deep neural networks results in optimally compressed hidden layers, from an information theoretic perspective. However, this claim was established on toy data. The goal of the work we present here is to test whether the information bottleneck principle is applicable to a realistic setting using a larger and deeper convolutional architecture, a ResNet model. We trained PixelCNN++ models as inverse representation decoders to measure the mutual information between hidden layers of a ResNet and input image data, when trained for (1) classification and (2) autoencoding. We find that two stages of learning happen for both training regimes, and that compression does occur, even for an autoencoder. Sampling images by conditioning on hidden layers' activations offers an intuitive visualisation to understand what a ResNets learns to forget.},
	urldate = {2020-12-21},
	journal = {arXiv:2003.06254 [cs, stat]},
	author = {Darlow, Luke Nicholas and Storkey, Amos},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.06254},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{rahaman_spectral_2019,
	title = {On the {Spectral} {Bias} of {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v97/rahaman19a.html},
	abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100\% accuracy. In this work we present properties of neural networks that c...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5301--5310},
}

@inproceedings{bhatt_explainable_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Explainable machine learning in deployment},
	isbn = {978-1-4503-6936-7},
	url = {https://doi.org/10.1145/3351095.3375624},
	doi = {10.1145/3351095.3375624},
	abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, José M. F. and Eckersley, Peter},
	month = jan,
	year = {2020},
	keywords = {deployed systems, explainability, machine learning, qualitative study, transparency},
	pages = {648--657},
}

@inproceedings{zhou_interpretable_2018,
	title = {Interpretable {Basis} {Decomposition} for {Visual} {Explanation}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Antonio_Torralba_Interpretable_Basis_Decomposition_ECCV_2018_paper.html},
	urldate = {2020-12-21},
	author = {Zhou, Bolei and Sun, Yiyou and Bau, David and Torralba, Antonio},
	year = {2018},
	pages = {119--134},
}

@article{xu_causality_2020,
	title = {Causality {Learning}: {A} {New} {Perspective} for {Interpretable} {Machine} {Learning}},
	shorttitle = {Causality {Learning}},
	url = {http://arxiv.org/abs/2006.16789},
	abstract = {Recent years have witnessed the rapid growth of machine learning in a wide range of fields such as image recognition, text classification, credit scoring prediction, recommendation system, etc. In spite of their great performance in different sectors, researchers still concern about the mechanism under any machine learning (ML) techniques that are inherently black-box and becoming more complex to achieve higher accuracy. Therefore, interpreting machine learning model is currently a mainstream topic in the research community. However, the traditional interpretable machine learning focuses on the association instead of the causality. This paper provides an overview of causal analysis with the fundamental background and key concepts, and then summarizes most recent causal approaches for interpretable machine learning. The evaluation techniques for assessing method quality, and open problems in causal interpretability are also discussed in this paper.},
	urldate = {2020-12-21},
	journal = {arXiv:2006.16789 [cs, stat]},
	author = {Xu, Guandong and Duong, Tri Dung and Li, Qian and Liu, Shaowu and Wang, Xianzhi},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.16789},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{parafita_explaining_2019,
	title = {Explaining {Visual} {Models} by {Causal} {Attribution}},
	url = {http://arxiv.org/abs/1909.08891},
	abstract = {Model explanations based on pure observational data cannot compute the effects of features reliably, due to their inability to estimate how each factor alteration could affect the rest. We argue that explanations should be based on the causal model of the data and the derived intervened causal models, that represent the data distribution subject to interventions. With these models, we can compute counterfactuals, new samples that will inform us how the model reacts to feature changes on our input. We propose a novel explanation methodology based on Causal Counterfactuals and identify the limitations of current Image Generative Models in their application to counterfactual creation.},
	urldate = {2020-12-21},
	journal = {arXiv:1909.08891 [cs, stat]},
	author = {Parafita, Álvaro and Vitrià, Jordi},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.08891},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{khanna_interpreting_2019,
	title = {Interpreting {Black} {Box} {Predictions} using {Fisher} {Kernels}},
	url = {http://proceedings.mlr.press/v89/khanna19a.html},
	abstract = {Research in both machine learning and psychology suggests that salient examples can help humans to interpret learning models. To this end, we take a novel look at black box interpretation of test p...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Khanna, Rajiv and Kim, Been and Ghosh, Joydeep and Koyejo, Sanmi},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {3382--3390},
}

@inproceedings{carter_what_2019,
	title = {What made you do this? {Understanding} black-box decisions with sufficient input subsets},
	shorttitle = {What made you do this?},
	url = {http://proceedings.mlr.press/v89/carter19a.html},
	abstract = {Local explanation frameworks aim to rationalize particular decisions made by a black-box prediction model.  Existing techniques are often restricted to a specific type of predictor or based on inpu...},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Carter, Brandon and Mueller, Jonas and Jain, Siddhartha and Gifford, David},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {567--576},
}

@inproceedings{ghorbani_towards_2019,
	title = {Towards {Automatic} {Concept}-based {Explanations}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ghorbani, Amirata and Wexler, James and Zou, James Y. and Kim, Been},
	year = {2019},
	pages = {9277--9286},
}

@inproceedings{grimsley_why_2020,
	address = {Marseille, France},
	title = {Why {Attention} is {Not} {Explanation}: {Surgical} {Intervention} and {Causal} {Reasoning} about {Neural} {Models}},
	isbn = {979-10-95546-34-4},
	shorttitle = {Why {Attention} is {Not} {Explanation}},
	url = {https://www.aclweb.org/anthology/2020.lrec-1.220},
	abstract = {As the demand for explainable deep learning grows in the evaluation of language technologies, the value of a principled grounding for those explanations grows as well. Here we study the state-of-the-art in explanation for neural models for NLP tasks from the viewpoint of philosophy of science. We focus on recent evaluation work that finds brittleness in explanations obtained through attention mechanisms. We harness philosophical accounts of explanation to suggest broader conclusions from these studies. From this analysis, we assert the impossibility of causal explanations from attention layers over text data. We then introduce NLP researchers to contemporary philosophy of science theories that allow robust yet non-causal reasoning in explanation, giving computer scientists a vocabulary for future research.},
	language = {English},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Grimsley, Christopher and Mayfield, Elijah and R.S. Bursten, Julia},
	month = may,
	year = {2020},
	pages = {1780--1790},
}

@inproceedings{plumb_regularizing_2020,
	title = {Regularizing {Black}-box {Models} for {Improved} {Interpretability}},
	abstract = {Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, EXPO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to deﬁne. We demonstrate that post-hoc explanations for EXPO-regularized models have better explanation quality, as measured by the common ﬁdelity and stability metrics. We verify that improving these metrics leads to signiﬁcantly more useful explanations with a user study on a realistic task.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Plumb, Gregory and Al-Shedivat, Maruan and Cabrera, Ángel Alexander and Perer, Adam and Xing, Eric and Talwalkar, Ameet},
	year = {2020},
	pages = {11},
}

@inproceedings{ignatiev_relating_2019,
	title = {On {Relating} {Explanations} and {Adversarial} {Examples}},
	abstract = {The importance of explanations (XP’s) of machine learning (ML) model predictions and of adversarial examples (AE’s) cannot be overstated, with both arguably being essential for the practical success of ML in different settings. There has been recent work on understanding and assessing the relationship between XP’s and AE’s. However, such work has been mostly experimental and a sound theoretical relationship has been elusive. This paper demonstrates that explanations and adversarial examples are related by a generalized form of hitting set duality, which extends earlier work on hitting set duality observed in model-based diagnosis and knowledge compilation. Furthermore, the paper proposes algorithms, which enable computing adversarial examples from explanations and vice-versa.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ignatiev, Alexey and Narodytska, Nina and Marques-Silva, Joao},
	year = {2019},
	pages = {11},
}

@inproceedings{shih_symbolic_2018,
	address = {Stockholm, Sweden},
	title = {A {Symbolic} {Approach} to {Explaining} {Bayesian} {Network} {Classifiers}},
	isbn = {978-0-9992411-2-7},
	url = {https://www.ijcai.org/proceedings/2018/708},
	doi = {10.24963/ijcai.2018/708},
	abstract = {We propose an approach for explaining Bayesian network classiﬁers, which is based on compiling such classiﬁers into decision functions that have a tractable and symbolic form. We introduce two types of explanations for why a classiﬁer may have classiﬁed an instance positively or negatively and suggest algorithms for computing these explanations. The ﬁrst type of explanation identiﬁes a minimal set of the currently active features that is responsible for the current classiﬁcation, while the second type of explanation identiﬁes a minimal set of features whose current state (active or not) is sufﬁcient for the classiﬁcation. We consider in particular the compilation of Naive and Latent-Tree Bayesian network classiﬁers into Ordered Decision Diagrams (ODDs), providing a context for evaluating our proposal using case studies and experiments based on classiﬁers from the literature.},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Shih, Andy and Choi, Arthur and Darwiche, Adnan},
	month = jul,
	year = {2018},
	pages = {5103--5111},
}

@inproceedings{ignatiev_abduction-based_2019,
	title = {Abduction-{Based} {Explanations} for {Machine} {Learning} {Models}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/3964},
	doi = {10.1609/aaai.v33i01.33011511},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ignatiev, Alexey and Narodytska, Nina and Marques-Silva, Joao},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {1511--1519},
}

@article{nguyen_quantitative_2020,
	title = {On quantitative aspects of model interpretability},
	url = {http://arxiv.org/abs/2007.07584},
	abstract = {Despite the growing body of work in interpretable machine learning, it remains unclear how to evaluate different explainability methods without resorting to qualitative assessment and user-studies. While interpretability is an inherently subjective matter, previous works in cognitive science and epistemology have shown that good explanations do possess aspects that can be objectively judged apart from ﬁdelity), such as simplicity and broadness. In this paper we propose a set of metrics to programmatically evaluate interpretability methods along these dimensions. In particular, we argue that the performance of methods along these dimensions can be orthogonally imputed to two conceptual parts, namely the feature extractor and the actual explainability method. We experimentally validate our metrics on different benchmark tasks and show how they can be used to guide a practitioner in the selection of the most appropriate method for the task at hand.},
	language = {en},
	urldate = {2020-12-16},
	journal = {arXiv:2007.07584 [cs, stat]},
	author = {Nguyen, An-phi and Martínez, María Rodríguez},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.07584},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lin_what_2020,
	title = {What {Do} {You} {See}? {Evaluation} of {Explainable} {Artificial} {Intelligence} ({XAI}) {Interpretability} through {Neural} {Backdoors}},
	shorttitle = {What {Do} {You} {See}?},
	url = {http://arxiv.org/abs/2009.10639},
	abstract = {EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the parts of the inputs deemed important to arrive a decision at a speciﬁc target. However, it remains challenging to quantify correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns–hidden malicious functionalities that cause misclassiﬁcation–to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identiﬁed by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassiﬁcation, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for systematic evaluation of explanations that an XAI method generates and evaluate seven state-of-the-art model-free and model-speciﬁc posthoc methods through 36 models trojaned with speciﬁcally crafted triggers using color, shape, texture, location, and size. We discovered six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region.},
	language = {en},
	urldate = {2020-12-16},
	journal = {arXiv:2009.10639 [cs]},
	author = {Lin, Yi-Shan and Lee, Wen-Chuan and Celik, Z. Berkay},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.10639},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{zhang_improving_2020,
	title = {Improving {Interpretability} of {CNN} {Models} {Using} {Non}-{Negative} {Concept} {Activation} {Vectors}},
	url = {http://arxiv.org/abs/2006.15417},
	abstract = {Convolutional neural network (CNN) models for computer vision are powerful but lack explainability in their most basic form. This deﬁciency remains a key challenge when applying CNNs in important domains. Recent work for explanations through feature importance of approximate linear models has moved from input-level features (pixels or segments) to features from mid-layer feature maps in the guise of concept activation vectors (CAVs). CAVs contain concept-level information and could be learnt via Clustering. In this work, we rethink the ACE algorithm of Ghorbani et al., proposing an alternative concept-based explanation framework. Based on the requirements of ﬁdelity (approximate models) and interpretability (being meaningful to people), we design measurements and evaluate a range of dimensionality reduction methods for alignment with our framework. We ﬁnd that non-negative concept activation vectors from non-negative matrix factorization provide superior performance in interpretability and ﬁdelity based on computational and human subject experiments. Our framework provides both local and global concept-level explanations for pre-trained CNN models.},
	language = {en},
	urldate = {2020-12-16},
	journal = {arXiv:2006.15417 [cs]},
	author = {Zhang, Ruihan and Madumal, Prashan and Miller, Tim and Ehinger, Krista A. and Rubinstein, Benjamin I. P.},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.15417},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{wang_gaining_2019,
	title = {Gaining {Free} or {Low}-{Cost} {Interpretability} with {Interpretable} {Partial} {Substitute}},
	url = {http://proceedings.mlr.press/v97/wang19a.html},
	abstract = {This work addresses the situation where a black-box model with good predictive performance is chosen over its interpretable competitors, and we show interpretability is still achievable in this cas...},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Tong},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6505--6514},
}

@inproceedings{amorim_interpretability_2020,
	title = {Interpretability vs. {Complexity}: {The} {Friction} in {Deep} {Neural} {Networks}},
	shorttitle = {Interpretability vs. {Complexity}},
	doi = {10.1109/IJCNN48605.2020.9206800},
	abstract = {Saliency maps have been used as one possibility to interpret deep neural networks. This method estimates the relevance of each pixel in the image classification, with higher values representing pixels which contribute positively to classification. The goal of this study is to understand how the complexity of the network affects the interpretabilty of the saliency maps in classification tasks. To achieve that, we investigate how changes in the regularization affects the saliency maps produced, and their fidelity to the overall classification process of the network.The experimental setup consists in the calculation of the fidelity of five saliency map methods that were compare, applying them to models trained on the CIFAR-10 dataset, using different levels of weight decay on some or all the layers. Achieved results show that models with lower regularization are statistically (significance of 5\%) more interpretable than the other models. Also, regularization applied only to the higher convolutional layers or fully-connected layers produce saliency maps with more fidelity.},
	booktitle = {2020 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Amorim, J. P. and Abreu, P. H. and Reyes, M. and Santos, J.},
	month = jul,
	year = {2020},
	note = {ISSN: 2161-4407},
	keywords = {CIFAR-10 dataset, CNN training, Complexity theory, Convolutional neural network, Data models, Mathematical model, Measurement, Neural networks, Perturbation methods, Training, complexity, convolutional neural nets, convolutional neural network, deep neural networks, feature extraction, image classification, image representation, interpretability, learning (artificial intelligence), pixel representation, saliency map, saliency map methods, saliency maps},
	pages = {1--7},
}

@inproceedings{dong_improving_2017,
	title = {Improving {Interpretability} of {Deep} {Neural} {Networks} {With} {Semantic} {Information}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Dong_Improving_Interpretability_of_CVPR_2017_paper.html},
	urldate = {2020-12-16},
	author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Zhang, Bo},
	year = {2017},
	pages = {4306--4314},
}

@inproceedings{tsang_neural_2018,
	title = {Neural {Interaction} {Transparency} ({NIT}): {Disentangling} {Learned} {Interactions} for {Improved} {Interpretability}},
	volume = {31},
	shorttitle = {Neural {Interaction} {Transparency} ({NIT})},
	url = {https://proceedings.neurips.cc/paper/2018/hash/74378afe5e8b20910cf1f939e57f0480-Abstract.html},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Tsang, Michael and Liu, Hanpeng and Purushotham, Sanjay and Murali, Pavankumar and Liu, Yan},
	year = {2018},
	pages = {5804--5813},
}

@inproceedings{gilpin_explaining_2018,
	title = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	shorttitle = {Explaining {Explanations}},
	doi = {10.1109/DSAA.2018.00018},
	abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
	booktitle = {2018 {IEEE} 5th {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Gilpin, L. H. and Bau, D. and Yuan, B. Z. and Bajwa, A. and Specter, M. and Kagal, L.},
	month = oct,
	year = {2018},
	keywords = {Artificial intelligence, Biological neural networks, Complexity theory, Computational modeling, Decision trees, Deep learning and deep analytics, Fairness and transparency in data science, Machine learning theories, Models and systems, Taxonomy, XAI, algorithmic fairness, artificial intelligence, complex machines, data analysis, explaining explanations, explanatory artificial intelligence, learning (artificial intelligence), machine learning, neural nets, potential bias-problems, suggested future research directions, training data},
	pages = {80--89},
}

@inproceedings{kim_interpretability_2018,
	title = {Interpretability {Beyond} {Feature} {Attribution}: {Quantitative} {Testing} with {Concept} {Activation} {Vectors} ({TCAV})},
	shorttitle = {Interpretability {Beyond} {Feature} {Attribution}},
	url = {http://proceedings.mlr.press/v80/kim18d.html},
	abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level ...},
	language = {en},
	urldate = {2020-12-16},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {2668--2677},
}

@article{nakkiran_adversarial_2019,
	title = {Adversarial {Robustness} {May} {Be} at {Odds} {With} {Simplicity}},
	url = {http://arxiv.org/abs/1901.00532},
	abstract = {Current techniques in machine learning are so far are unable to learn classiﬁers that are robust to adversarial perturbations. However, they are able to learn non-robust classiﬁers with very high accuracy, even in the presence of random perturbations. Towards explaining this gap, we highlight the hypothesis that robust classiﬁcation may require more complex classiﬁers (i.e. more capacity) than standard classiﬁcation.},
	language = {en},
	urldate = {2020-12-16},
	journal = {arXiv:1901.00532 [cs, stat]},
	author = {Nakkiran, Preetum},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.00532},
	keywords = {Computer Science - Computational Complexity, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{jain_attention_2019,
	address = {Minneapolis, Minnesota},
	title = {Attention is not {Explanation}},
	url = {https://www.aclweb.org/anthology/N19-1357},
	doi = {10.18653/v1/N19-1357},
	abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.},
	urldate = {2020-12-16},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Jain, Sarthak and Wallace, Byron C.},
	month = jun,
	year = {2019},
	pages = {3543--3556},
}

@incollection{bostrom_ethics_2014,
	title = {The {Ethics} of {Artificial} {Intelligence}},
	abstract = {The possibility of creating thinking machines raises a host of ethical issues. These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves. The first section discusses issues that may arise in the near future of AI. The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence. The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status. In the fourth section, we consider how AIs might diﬀer from humans in certain basic respects relevant to our ethical assessment of them. The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill.},
	language = {en},
	booktitle = {The {Cambridge} {Handbook} of {Artificial} {Intelligence}},
	publisher = {Cambridge University Press},
	author = {Bostrom, Nick and Yudkowsky, Eliezer},
	month = jun,
	year = {2014},
	pages = {21},
}

@inproceedings{herlocker_explaining_2000,
	address = {New York, NY, USA},
	series = {{CSCW} '00},
	title = {Explaining collaborative filtering recommendations},
	isbn = {978-1-58113-222-9},
	url = {https://doi.org/10.1145/358916.358995},
	doi = {10.1145/358916.358995},
	abstract = {Automated collaborative filtering (ACF) systems predict a person's affinity for items or information by connecting that person's recorded interests with the recorded interests of a community of people and sharing ratings between like-minded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems - how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the user's conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users.},
	urldate = {2020-12-14},
	booktitle = {Proceedings of the 2000 {ACM} conference on {Computer} supported cooperative work},
	publisher = {Association for Computing Machinery},
	author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Riedl, John},
	month = dec,
	year = {2000},
	keywords = {GroupLens, MoviesLens, collaborative filtering, explanations, recommender systems},
	pages = {241--250},
}

@article{ching_opportunities_2018,
	title = {Opportunities and obstacles for deep learning in biology and medicine},
	volume = {15},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2017.0387},
	doi = {10.1098/rsif.2017.0387},
	abstract = {Deep learning describes a class of machine learning algorithms that are capable of combining raw inputs into layers of intermediate features. These algorithms have recently shown impressive results across a variety of domains. Biology and medicine are data-rich disciplines, but the data are complex and often ill-understood. Hence, deep learning techniques may be particularly well suited to solve problems of these fields. We examine applications of deep learning to a variety of biomedical problems—patient classification, fundamental biological processes and treatment of patients—and discuss whether deep learning will be able to transform these tasks or if the biomedical sphere poses unique challenges. Following from an extensive literature review, we find that deep learning has yet to revolutionize biomedicine or definitively resolve any of the most pressing challenges in the field, but promising advances have been made on the prior state of the art. Even though improvements over previous baselines have been modest in general, the recent progress indicates that deep learning methods will provide valuable means for speeding up or aiding human investigation. Though progress has been made linking a specific neural network's prediction to input features, understanding how users should interpret these models to make testable hypotheses about the system under study remains an open challenge. Furthermore, the limited amount of labelled data for training presents problems in some domains, as do legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine.},
	number = {141},
	urldate = {2020-12-14},
	journal = {Journal of The Royal Society Interface},
	author = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M. and Xie, Wei and Rosen, Gail L. and Lengerich, Benjamin J. and Israeli, Johnny and Lanchantin, Jack and Woloszynek, Stephen and Carpenter, Anne E. and Shrikumar, Avanti and Xu, Jinbo and Cofer, Evan M. and Lavender, Christopher A. and Turaga, Srinivas C. and Alexandari, Amr M. and Lu, Zhiyong and Harris, David J. and DeCaprio, Dave and Qi, Yanjun and Kundaje, Anshul and Peng, Yifan and Wiley, Laura K. and Segler, Marwin H. S. and Boca, Simina M. and Swamidass, S. Joshua and Huang, Austin and Gitter, Anthony and Greene, Casey S.},
	month = apr,
	year = {2018},
	note = {Publisher: Royal Society},
	pages = {20170387},
}

@article{goodman_european_2017,
	title = {European {Union} {Regulations} on {Algorithmic} {Decision}-{Making} and a “{Right} to {Explanation}”},
	volume = {38},
	copyright = {Copyright (c) 2017 AI Magazine},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/index.php/aimagazine/article/view/2741},
	doi = {10.1609/aimag.v38i3.2741},
	language = {en},
	number = {3},
	urldate = {2020-12-14},
	journal = {AI Magazine},
	author = {Goodman, Bryce and Flaxman, Seth},
	month = oct,
	year = {2017},
	note = {Number: 3},
	pages = {50--57},
}

@article{dzindolet_role_2003,
	series = {Trust and {Technology}},
	title = {The role of trust in automation reliance},
	volume = {58},
	issn = {1071-5819},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581903000387},
	doi = {10.1016/S1071-5819(03)00038-7},
	abstract = {A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.},
	language = {en},
	number = {6},
	urldate = {2020-12-14},
	journal = {International Journal of Human-Computer Studies},
	author = {Dzindolet, Mary T. and Peterson, Scott A. and Pomranky, Regina A. and Pierce, Linda G. and Beck, Hall P.},
	month = jun,
	year = {2003},
	keywords = {Automation reliance, Automation trust, Disuse, Misuse},
	pages = {697--718},
}

@inproceedings{kim_bridging_2019,
	title = {Bridging {Adversarial} {Robustness} and {Gradient} {Interpretability}},
	url = {http://arxiv.org/abs/1903.11626},
	abstract = {Adversarial training is a training scheme designed to counter adversarial attacks by augmenting the training dataset with adversarial examples. Surprisingly, several studies have observed that loss gradients from adversarially trained DNNs are visually more interpretable than those from standard DNNs. Although this phenomenon is interesting, there are only few works that have offered an explanation. In this paper, we attempted to bridge this gap between adversarial robustness and gradient interpretability. To this end, we identiﬁed that loss gradients from adversarially trained DNNs align better with human perception because adversarial training restricts gradients closer to the image manifold. We then demonstrated that adversarial training causes loss gradients to be quantitatively meaningful. Finally, we showed that under the adversarial training framework, there exists an empirical trade-off between test accuracy and loss gradient interpretability and proposed two potential approaches to resolving this trade-off.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {International {Conference} on {Learning} {Representations} {Workshop}},
	author = {Kim, Beomsu and Seo, Junghoon and Jeon, Taegyun},
	month = apr,
	year = {2019},
	note = {arXiv: 1903.11626},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{andriushchenko_understanding_2020,
	title = {Understanding and {Improving} {Fast} {Adversarial} {Training}},
	abstract = {A recent line of work focused on making adversarial training computationally efﬁcient for deep learning models. In particular, Wong et al. [47] showed that ∞-adversarial training with fast gradient sign method (FGSM) can fail due to a phenomenon called catastrophic overﬁtting, when the model quickly loses its robustness over a single epoch of training. We show that adding a random step to FGSM, as proposed in [47], does not prevent catastrophic overﬁtting, and that randomness is not important per se — its main role being simply to reduce the magnitude of the perturbation. Moreover, we show that catastrophic overﬁtting is not inherent to deep and overparametrized networks, but can occur in a single-layer convolutional network with a few ﬁlters. In an extreme case, even a single ﬁlter can make the network highly non-linear locally, which is the main reason why FGSM training fails. Based on this observation, we propose a new regularization method, GradAlign, that prevents catastrophic overﬁtting by explicitly maximizing the gradient alignment inside the perturbation set and improves the quality of the FGSM solution. As a result, GradAlign allows to successfully apply FGSM training also for larger ∞-perturbations and reduce the gap to multi-step adversarial training. The code of our experiments is available at https://github.com/tml-epfl/ understanding-fast-adv-training.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Andriushchenko, Maksym and Flammarion, Nicolas},
	year = {2020},
	pages = {12},
}

@inproceedings{cai_curriculum_2018,
	title = {Curriculum {Adversarial} {Training}},
	abstract = {Recently, deep learning has been applied to many security-sensitive applications, such as facial authentication. The existence of adversarial examples hinders such applications. The state-of-theart result on defense shows that adversarial training can be applied to train a robust model on MNIST against adversarial examples; but it fails to achieve a high empirical worst-case accuracy on a more complex task, such as CIFAR-10 and SVHN. In our work, we propose curriculum adversarial training (CAT) to resolve this issue. The basic idea is to develop a curriculum of adversarial examples generated by attacks with a wide range of strengths. With two techniques to mitigate the catastrophic forgetting and the generalization issues, we demonstrate that CAT can improve the prior art’s empirical worst-case accuracy by a large margin of 25\% on CIFAR-10 and 35\% on SVHN. At the same, the model’s performance on non-adversarial inputs is comparable to the state-of-the-art models.},
	language = {en},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {International} {Joint} {Conference} on {Artificial} {Intelligence} ({IJCAI}-18)},
	author = {Cai, Qi-Zhi and Liu, Chang and Song, Dawn},
	year = {2018},
	pages = {8},
}

@inproceedings{wang_convergence_2019,
	title = {On the {Convergence} and {Robustness} of {Adversarial} {Training}},
	abstract = {Improving the robustness of deep neural networks (DNNs) to adversarial examples is an important yet challenging problem for secure deep learning. Across existing defense techniques, adversarial training with Projected Gradient Decent (PGD) is amongst the most effective. Adversarial training solves a min-max optimization problem, with the inner maximization generating adversarial examples by maximizing the classiﬁcation loss, and the outer minimization ﬁnding model parameters by minimizing the loss on adversarial examples generated from the inner maximization. A criterion that measures how well the inner maximization is solved is therefore crucial for adversarial training. In this paper, we propose such a criterion, namely First-Order Stationary Condition for constrained optimization (FOSC), to quantitatively evaluate the convergence quality of adversarial examples found in the inner maximization. With FOSC, we ﬁnd that to ensure better robustness, it is essential to use adversarial examples with better convergence quality at the later stages of training. Yet at the early stages, high convergence quality adversarial examples are not necessary and may even lead to poor robustness. Based on these observations, we propose a dynamic training strategy to gradually increase the convergence quality of the generated adversarial examples, which signiﬁcantly improves the robustness of adversarial training. Our theoretical and empirical results show the effectiveness of the proposed method.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Wang, Yisen and Ma, Xingjun and Bailey, James and Yi, Jinfeng and Zhou, Bowen and Gu, Quanquan},
	year = {2019},
	pages = {13},
}

@article{wang_initializing_2020,
	title = {Initializing {Perturbations} in {Multiple} {Directions} for {Fast} {Adversarial} {Training}},
	url = {http://arxiv.org/abs/2005.07606},
	abstract = {Recent developments in the ﬁled of Deep Learning have demonstrated that Deep Neural Networks(DNNs) are vulnerable to adversarial examples. Speciﬁcally, in image classiﬁcation, an adversarial example can fool the well trained deep neural networks by adding barely imperceptible perturbations to clean images. Adversarial Training, one of the most direct and effective methods, minimizes the losses of perturbed-data to learn robust deep networks against adversarial attacks. It has been proven that using the fast gradient sign method (FGSM) can achieve Fast Adversarial Training. However, FGSM-based adversarial training may ﬁnally obtain a failed model because of overﬁtting to FGSM samples. In this paper, we proposed the Diversiﬁed Initialized Perturbations Adversarial Training (DIPFAT) which involves seeking the initialization of the perturbation via enlarging the output distances of the target model in a random directions. Due to the diversity of random directions, the embedded fast adversarial training using FGSM increases the information from the adversary and reduces the possibility of overﬁtting. In addition to preventing overﬁtting, the extensive results show that our proposed DIP-FAT technique can also improve the accuracy of the clean data. The biggest advantage of DIP-FAT method: achieving the best banlance among clean-data, perturbed-data and efﬁciency.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:2005.07606 [cs, stat]},
	author = {Wang, Xunguang and Xu, Ship Peng and Wang, Eric Ke},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07606},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{maini_adversarial_2020,
	title = {Adversarial {Robustness} {Against} the {Union} of {Multiple} {Perturbation} {Models}},
	url = {http://arxiv.org/abs/1909.04068},
	abstract = {Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certiﬁably) robust classiﬁers. While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks. However, these methods can be difﬁcult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union. In this work, we develop a natural generalization of the standard PGD-based procedure to incorporate multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions. This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against ∞, 2, and 1 attacks, outperforming past approaches on the MNIST and CIFAR10 datasets and achieving adversarial accuracy of 47.0\% against the union of ( ∞, 2, 1) perturbations with radius = (0.03, 0.5, 12) on the latter, improving upon previous approaches which achieve 40.6\% accuracy.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:1909.04068 [cs, stat]},
	author = {Maini, Pratyush and Wong, Eric and Kolter, J. Zico},
	month = jul,
	year = {2020},
	note = {arXiv: 1909.04068},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{etmann_closer_2019,
	title = {A {Closer} {Look} at {Double} {Backpropagation}},
	url = {http://arxiv.org/abs/1906.06637},
	abstract = {In recent years, an increasing number of neural network models have included derivatives with respect to inputs in their loss functions, resulting in so-called double backpropagation for ﬁrstorder optimization. However, so far no general description of the involved derivatives exists. Here, we cover a wide array of special cases in a very general Hilbert space framework, which allows us to provide optimized backpropagation rules for many real-world scenarios. This includes the reduction of calculations for Frobenius-norm-penalties on Jacobians by roughly a third for locally linear activation functions. Furthermore, we provide a description of the discontinuous loss surface of ReLU networks both in the inputs and the parameters and demonstrate why the discontinuities do not pose a big problem in reality.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:1906.06637 [cs, math, stat]},
	author = {Etmann, Christian},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.06637},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{mustafa_adversarial_2019,
	address = {Seoul, Korea (South)},
	title = {Adversarial {Defense} by {Restricting} the {Hidden} {Space} of {Deep} {Neural} {Networks}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010713/},
	doi = {10.1109/ICCV.2019.00348},
	abstract = {Deep neural networks are vulnerable to adversarial attacks, which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to ﬁnd strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Speciﬁcally, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classiﬁcation performance on clean images. We report extensive evaluations in both black-box and whitebox attack scenarios and show signiﬁcant gains in comparison to state-of-the art defenses1.},
	language = {en},
	urldate = {2020-09-30},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Mustafa, Aamir and Khan, Salman and Hayat, Munawar and Goecke, Roland and Shen, Jianbing and Shao, Ling},
	month = oct,
	year = {2019},
	pages = {3384--3393},
}

@article{aggarwal_benefits_2020,
	title = {On the {Benefits} of {Models} with {Perceptually}-{Aligned} {Gradients}},
	url = {http://arxiv.org/abs/2005.01499},
	abstract = {Adversarial robust models have been shown to learn more robust and interpretable features than standard trained models. As shown in [Tsipras et al. (2018)], such robust models inherit useful interpretable properties where the gradient aligns perceptually well with images, and adding a large targeted adversarial perturbation leads to an image resembling the target class. We perform experiments to show that interpretable and perceptually aligned gradients are present even in models that do not show high robustness to adversarial attacks. Speciﬁcally, we perform adversarial training with attack for different max-perturbation bound. Adversarial training with low max-perturbation bound results in models that have interpretable features with only slight drop in performance over clean samples. In this paper, we leverage models with interpretable perceptually-aligned features and show that adversarial training with low max-perturbation bound can improve the performance of models for zero-shot and weakly supervised localization tasks.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:2005.01499 [cs]},
	author = {Aggarwal, Gunjan and Sinha, Abhishek and Kumari, Nupur and Singh, Mayank},
	month = may,
	year = {2020},
	note = {arXiv: 2005.01499},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{shaeiri_towards_2020,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Large} {Perturbations}},
	url = {http://arxiv.org/abs/2003.13370},
	abstract = {Adversarial robustness has proven to be a required property of machine learning algorithms. A key and often overlooked aspect of this problem is to try to make the adversarial noise magnitude as large as possible to enhance the beneﬁts of the model robustness. We show that the wellestablished algorithm called “adversarial training” fails to train a deep neural network given a large, but reasonable, perturbation magnitude. In this paper, we propose a simple yet effective initialization of the network weights that makes learning on higher levels of noise possible. We next evaluate this idea rigorously on MNIST ( up to ≈ 0.40) and CIFAR10 ( up to ≈ 32/255) datasets assuming the ∞ attack model. Additionally, in order to establish the limits of in which the learning is feasible, we study the optimal robust classiﬁer assuming full access to the joint data and label distribution. Then, we provide some theoretical results on the adversarial accuracy for a simple multi-dimensional Bernoulli distribution, which yields some insights on the range of feasible perturbations for the MNIST dataset.},
	language = {en},
	urldate = {2020-09-30},
	journal = {arXiv:2003.13370 [cs, stat]},
	author = {Shaeiri, Amirreza and Nobahari, Rozhin and Rohban, Mohammad Hossein},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.13370},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bau_understanding_2020,
	title = {Understanding the role of individual units in a deep neural network},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1907375117},
	doi = {10.1073/pnas.1907375117},
	abstract = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large datasets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.},
	language = {en},
	urldate = {2020-09-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
	month = sep,
	year = {2020},
	pages = {201907375},
}

@inproceedings{hein_formal_2017,
	title = {Formal {Guarantees} on the {Robustness} of a {Classifier} against {Adversarial} {Manipulation}},
	url = {http://papers.nips.cc/paper/6821-formal-guarantees-on-the-robustness-of-a-classifier-against-adversarial-manipulation.pdf},
	urldate = {2020-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Hein, Matthias and Andriushchenko, Maksym},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {2266--2276},
}

@inproceedings{kalimeris_sgd_2019,
	title = {{SGD} on {Neural} {Networks} {Learns} {Functions} of {Increasing} {Complexity}},
	url = {http://papers.nips.cc/paper/8609-sgd-on-neural-networks-learns-functions-of-increasing-complexity.pdf},
	urldate = {2020-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Kalimeris, Dimitris and Kaplun, Gal and Nakkiran, Preetum and Edelman, Benjamin and Yang, Tristan and Barak, Boaz and Zhang, Haofeng},
	year = {2019},
	pages = {3496--3506},
}

@inproceedings{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	author = {Uesato, Jonathan and O’Donoghue, Brendan},
	year = {2018},
	pages = {10},
}

@inproceedings{rice_overfitting_2020,
	title = {Overfitting in adversarially robust deep learning},
	abstract = {It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classiﬁer. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We ﬁnd that overﬁtting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models (`∞ and `2). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overﬁtting. Finally, we study several classical and modern deep learning remedies for overﬁtting, including regularization and data augmentation, and ﬁnd that no approach in isolation improves signiﬁcantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at https://github.com/ locuslab/robust\_overfitting.},
	language = {en},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	author = {Rice, Leslie and Wong, Eric and Kolter, J Zico},
	year = {2020},
	pages = {12},
}

@inproceedings{zheng_efficient_2020,
	address = {Seattle, WA, USA},
	title = {Efficient {Adversarial} {Training} {With} {Transferable} {Adversarial} {Examples}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157681/},
	doi = {10.1109/CVPR42600.2020.00126},
	abstract = {Adversarial training is an effective defense method to protect classiﬁcation models against adversarial attacks. However, one limitation of this approach is that it can require orders of magnitude additional training time due to high cost of generating strong adversarial examples during training. In this paper, we ﬁrst show that there is high transferability between models from neighboring epochs in the same training process, i.e., adversarial examples from one epoch continue to be adversarial in subsequent epochs. Leveraging this property, we propose a novel method, Adversarial Training with Transferable Adversarial Examples (ATTA), that can enhance the robustness of trained models and greatly improve the training efﬁciency by accumulating adversarial perturbations through epochs. Compared to state-of-the-art adversarial training methods, ATTA enhances adversarial accuracy by up to 7.2\% on CIFAR10 and requires 12 ∼ 14× less training time on MNIST and CIFAR10 datasets with comparable model robustness.},
	language = {en},
	urldate = {2020-08-27},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zheng, Haizhong and Zhang, Ziqi and Gu, Juncheng and Lee, Honglak and Prakash, Atul},
	month = jun,
	year = {2020},
	pages = {1178--1187},
}

@inproceedings{xiao_one_2020,
	address = {Seattle, WA, USA},
	title = {One {Man}’s {Trash} {Is} {Another} {Man}’s {Treasure}: {Resisting} {Adversarial} {Examples} by {Adversarial} {Examples}},
	isbn = {978-1-72817-168-5},
	shorttitle = {One {Man}’s {Trash} {Is} {Another} {Man}’s {Treasure}},
	url = {https://ieeexplore.ieee.org/document/9156804/},
	doi = {10.1109/CVPR42600.2020.00049},
	abstract = {Modern image classiﬁcation systems are often built on deep neural networks, which suffer from adversarial examples—images with deliberately crafted, imperceptible noise to mislead the network’s classiﬁcation. To defend against adversarial examples, a plausible idea is to obfuscate the network’s gradient with respect to the input image. This general idea has inspired a long line of defense methods. Yet, almost all of them have proven vulnerable.},
	language = {en},
	urldate = {2020-08-27},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Xiao, Chang and Zheng, Changxi},
	month = jun,
	year = {2020},
	pages = {409--418},
}

@inproceedings{gupta_improving_2020,
	address = {Seattle, WA, USA},
	title = {Improving the affordability of robustness training for {DNNs}},
	isbn = {978-1-72819-360-1},
	url = {https://ieeexplore.ieee.org/document/9150984/},
	doi = {10.1109/CVPRW50498.2020.00398},
	abstract = {Projected Gradient Descent (PGD) based adversarial training has become one of the most prominent methods for building robust deep neural network models. However, the computational complexity associated with this approach, due to the maximization of the loss function when ﬁnding adversaries, is a longstanding problem and may be prohibitive when using larger and more complex models. In this paper we show that the initial phase of adversarial training is redundant and can be replaced with natural training which signiﬁcantly improves the computational efﬁciency. We demonstrate that this efﬁciency gain can be achieved without any loss in accuracy on natural and adversarial test samples. We support our argument with insights on the nature of the adversaries and their relative strength during the training process. We show that our proposed method can reduce the training time by a factor of up to 2.5 with comparable or better model test accuracy and generalization on various strengths of adversarial attacks.},
	language = {en},
	urldate = {2020-08-27},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Gupta, Sidharth and Dube, Parijat and Verma, Ashish},
	month = jun,
	year = {2020},
	pages = {3383--3392},
}

@inproceedings{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	urldate = {2020-08-06},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{s_single-step_2020,
	title = {Single-{Step} {Adversarial} {Training} {With} {Dropout} {Scheduling}},
	abstract = {Deep learning models have shown impressive performance across a spectrum of computer vision applications including medical diagnosis and autonomous driving. One of the major concerns that these models face is their susceptibility to adversarial attacks. Realizing the importance of this issue, more researchers are working towards developing robust models that are less affected by adversarial attacks. Adversarial training method shows promising results in this direction. In adversarial training regime, models are trained with mini-batches augmented with adversarial samples. Fast and simple methods (e.g., single-step gradient ascent) are used for generating adversarial samples, in order to reduce computational complexity. It is shown that models trained using single-step adversarial training method (adversarial samples are generated using noniterative method) are pseudo robust. Further, this pseudo robustness of models is attributed to the gradient masking effect. However, existing works fail to explain when and why gradient masking effect occurs during single-step adversarial training. In this work, (i) we show that models trained using single-step adversarial training method learn to prevent the generation of single-step adversaries, and this is due to over-ﬁtting of the model during the initial stages of training, and (ii) to mitigate this effect, we propose a singlestep adversarial training method with dropout scheduling. Unlike models trained using existing single-step adversarial training methods, models trained using the proposed single-step adversarial training method are robust against both single-step and multi-step adversarial attacks, and the performance is on par with models trained using computationally expensive multi-step adversarial training methods, in white-box and black-box settings.},
	language = {en},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {S., Vivek B. and Babu, R. Venkatesh},
	year = {2020},
	pages = {10},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} classification with deep convolutional neural networks},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	urldate = {2020-08-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
}

@inproceedings{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://proceedings.mlr.press/v97/tan19a.html},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically stud...},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Tan, Mingxing and Le, Quoc},
	month = may,
	year = {2019},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {6105--6114},
}

@article{lai_recurrent_nodate,
	title = {Recurrent {Convolutional} {Neural} {Networks} for {Text} {Classification}},
	abstract = {Text classiﬁcation is a foundational task in many NLP applications. Traditional text classiﬁers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classiﬁcation without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classiﬁcation to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets.},
	language = {en},
	author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
	pages = {7},
}

@article{kalchbrenner_recurrent_nodate,
	title = {Recurrent {Convolutional} {Neural} {Networks} for {Discourse} {Compositionality}},
	abstract = {The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. The discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers. Without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classiﬁcation experiment.},
	language = {en},
	author = {Kalchbrenner, Nal and Blunsom, Phil},
	pages = {8},
}

@article{zagoruyko_paying_2017,
	title = {{PAYING} {MORE} {ATTENTION} {TO} {ATTENTION}: {IMPROVING} {THE} {PERFORMANCE} {OF} {CONVOLUTIONAL} {NEURAL} {NETWORKS} {VIA} {ATTENTION} {TRANSFER}},
	abstract = {Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artiﬁcial neural networks to a variety of tasks from ﬁelds such as computer vision and NLP. In this work we show that, by properly deﬁning attention for convolutional neural networks, we can actually use this type of information in order to signiﬁcantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer.},
	language = {en},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	year = {2017},
	pages = {13},
}

@techreport{kubilius_cornet_2018,
	type = {preprint},
	title = {{CORnet}: {Modeling} the {Neural} {Mechanisms} of {Core} {Object} {Recognition}},
	shorttitle = {{CORnet}},
	url = {http://biorxiv.org/lookup/doi/10.1101/408385},
	abstract = {Abstract
          Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NAS-Net architectures, demonstrating increasingly better object categorization performance and increasingly better explanatory power of both neural and behavioral responses. However, from the neuroscientist’s point of view, the relationship between such very deep architectures and the ventral visual pathway is incomplete in at least two ways. On the one hand, current state-of-the-art ANNs appear to be too complex (e.g., now over 100 levels) compared with the relatively shallow cortical hierarchy (4-8 levels), which makes it difficult to map their elements to those in the ventral visual stream and to understand what they are doing. On the other hand, current state-of-the-art ANNs appear to be not complex enough in that they lack recurrent connections and the resulting neural response dynamics that are commonplace in the ventral visual stream. Here we describe our ongoing efforts to resolve both of these issues by developing a “CORnet” family of deep neural network architectures. Rather than just seeking high object recognition performance (as the state-of-the-art ANNs above), we instead try to reduce the model family to its most important elements and then gradually build new ANNs with recurrent and skip connections while monitoring both performance and the match between each new CORnet model and a large body of primate brain and behavioral data. We report here that our current best ANN model derived from this approach (CORnet-S) is among the top models on Brain-Score, a composite benchmark for comparing models to the brain, but is simpler than other deep ANNs in terms of the number of convolutions performed along the longest path of information processing in the model. All CORnet models are available at github.com/dicarlolab/CORnet, and we plan to up-date this manuscript and the available models in this family as they are produced.},
	language = {en},
	urldate = {2020-07-23},
	institution = {Neuroscience},
	author = {Kubilius, Jonas and Schrimpf, Martin and Nayebi, Aran and Bear, Daniel and Yamins, Daniel L. K. and DiCarlo, James J.},
	month = sep,
	year = {2018},
	doi = {10.1101/408385},
}

@article{mhaskar_learning_nodate,
	title = {Learning {Real} and {Boolean} {Functions}: {When} {Is} {Deep} {Better} {Than} {Shallow}},
	language = {en},
	author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
	pages = {11},
}

@article{bear_learning_2020,
	title = {Learning {Physical} {Graph} {Representations} from {Visual} {Scenes}},
	url = {http://arxiv.org/abs/2006.12373},
	abstract = {Convolutional Neural Networks (CNNs) have proved exceptional at learning representations for visual object categorization. However, CNNs do not explicitly encode objects, parts, and their physical properties, which has limited CNNs’ success on tasks that require structured understanding of visual scenes. To overcome these limitations, we introduce the idea of “Physical Scene Graphs” (PSGs), which represent scenes as hierarchical graphs, with nodes in the hierarchy corresponding intuitively to object parts at different scales, and edges to physical connections between parts. Bound to each node is a vector of latent attributes that intuitively represent object properties such as surface shape and texture. We also describe PSGNet, a network architecture that learns to extract PSGs by reconstructing scenes through a PSG-structured bottleneck. PSGNet augments standard CNNs by including: recurrent feedback connections to combine low and high-level image information; graph pooling and vectorization operations that convert spatially-uniform feature maps into object-centric graph structures; and perceptual grouping principles to encourage the identiﬁcation of meaningful scene elements. We show that PSGNet outperforms alternative self-supervised scene representation algorithms at scene segmentation tasks, especially on complex real-world images, and generalizes well to unseen object types and scene arrangements. PSGNet is also able learn from physical motion, enhancing scene estimates even for static images. We present a series of ablation studies illustrating the importance of each component of the PSGNet architecture, analyses showing that learned latent attributes capture intuitive scene properties, and illustrate the use of PSGs for compositional scene inference.},
	language = {en},
	urldate = {2020-07-23},
	journal = {arXiv:2006.12373 [cs]},
	author = {Bear, Daniel M. and Fan, Chaofei and Mrowca, Damian and Li, Yunzhu and Alter, Seth and Nayebi, Aran and Schwartz, Jeremy and Fei-Fei, Li and Wu, Jiajun and Tenenbaum, Joshua B. and Yamins, Daniel L. K.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.12373},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, I.4.8},
}

@article{collins_capacity_2017,
	title = {{CAPACITY} {AND} {TRAINABILITY} {IN} {RECURRENT} {NEURAL} {NETWORKS}},
	abstract = {Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further ﬁnd that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difﬁculty for several architectures, and show that vanilla RNNs are far more difﬁcult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.},
	language = {en},
	author = {Collins, Jasmine and Sohl-Dickstein, Jascha and Sussillo, David},
	year = {2017},
	pages = {17},
}

@article{xu_show_nodate,
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {CaptionGeneration} with {Visual} {Attention}},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to ﬁx its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.},
	language = {en},
	author = {Xu, Kelvin and Lei, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S and Bengio, Yoshua},
	pages = {10},
}

@incollection{leibe_deep_2016,
	address = {Cham},
	title = {Deep {Networks} with {Stochastic} {Depth}},
	volume = {9908},
	isbn = {978-3-319-46492-3 978-3-319-46493-0},
	url = {http://link.springer.com/10.1007/978-3-319-46493-0_39},
	abstract = {Very deep convolutional networks with hundreds of layers have led to signiﬁcant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward ﬂow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error signiﬁcantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 \% on CIFAR-10).},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46493-0_39},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {646--661},
}

@article{zilly_recurrent_nodate,
	title = {Recurrent {Highway} {Networks}},
	abstract = {Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with "deep" transition functions remain difﬁcult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Geršgorin’s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efﬁcient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.},
	language = {en},
	author = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutník, Jan and Schmidhuber, Jürgen},
	pages = {10},
}

@article{zhang_architectural_nodate,
	title = {Architectural {Complexity} {Measures} of {Recurrent} {Neural} {Networks}},
	abstract = {In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: ﬁrst, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN’s over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the “depth” in feedforward neural networks (FNNs)), and (c) the recurrent skip coefﬁcient which captures how rapidly the information propagates over time. We rigorously prove each measure’s existence and computability. Our experimental results show that RNNs might beneﬁt from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefﬁcient offers performance boosts on long term dependency problems.},
	language = {en},
	author = {Zhang, Saizheng and Wu, Yuhuai and Che, Tong and Lin, Zhouhan and Memisevic, Roland and Salakhutdinov, Russ R and Bengio, Yoshua},
	pages = {9},
}

@inproceedings{zaremoodi_adaptive_2018,
	address = {Melbourne, Australia},
	title = {Adaptive {Knowledge} {Sharing} in {Multi}-{Task} {Learning}: {Improving} {Low}-{Resource} {Neural} {Machine} {Translation}},
	shorttitle = {Adaptive {Knowledge} {Sharing} in {Multi}-{Task} {Learning}},
	url = {http://aclweb.org/anthology/P18-2104},
	doi = {10.18653/v1/P18-2104},
	abstract = {Neural Machine Translation (NMT) is notorious for its need for large amounts of bilingual data. An effective approach to compensate for this requirement is MultiTask Learning (MTL) to leverage different linguistic resources as a source of inductive bias. Current MTL architectures are based on the SEQ2SEQ transduction, and (partially) share different components of the models among the tasks. However, this MTL approach often suffers from task interference, and is not able to fully capture commonalities among subsets of tasks. We address this issue by extending the recurrent units with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks conditioned on the task at hand, input, and model state. Empirical evaluation of two low-resource translation tasks, English to Vietnamese and Farsi, show +1 BLEU score improvements compared to strong baselines.},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zaremoodi, Poorya and Buntine, Wray and Haffari, Gholamreza},
	year = {2018},
	pages = {656--661},
}

@inproceedings{xie_mitigating_2018,
	title = {Mitigating {Adversarial} {Effects} through {Randomization}},
	abstract = {Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Speciﬁcally, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or ﬁne-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https: //github.com/cihangxie/NIPS2017\_adv\_challenge\_defense.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Xie, Cihang and Zhang, Zhishuai and Yuille, Alan L and Wang, Jianyu and Ren, Zhou},
	year = {2018},
	pages = {16},
}

@inproceedings{rony_decoupling_2019,
	address = {Long Beach, CA, USA},
	title = {Decoupling {Direction} and {Norm} for {Efficient} {Gradient}-{Based} {L2} {Adversarial} {Attacks} and {Defenses}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954314/},
	doi = {10.1109/CVPR.2019.00445},
	abstract = {Research on adversarial examples in computer vision tasks has shown that small, often imperceptible changes to an image can induce misclassiﬁcation, which has security implications for a wide range of image processing systems. Considering L2 norm distortions, the Carlini and Wagner attack is presently the most effective white-box attack in the literature. However, this method is slow since it performs a line-search for one of the optimization terms, and often requires thousands of iterations. In this paper, an efﬁcient approach is proposed to generate gradient-based attacks that induce misclassiﬁcations with low L2 norm, by decoupling the direction and the norm of the adversarial perturbation that is added to the image. Experiments conducted on the MNIST, CIFAR-10 and ImageNet datasets indicate that our attack achieves comparable results to the state-of-the-art (in terms of L2 norm) with considerably fewer iterations (as few as 100 iterations), which opens the possibility of using these attacks for adversarial training. Models trained with our attack achieve state-of-the-art robustness against whitebox gradient-based L2 attacks on the MNIST and CIFAR-10 datasets, outperforming the Madry defense when the attacks are limited to a maximum norm.},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Rony, Jerome and Hafemann, Luiz G. and Oliveira, Luiz S. and Ben Ayed, Ismail and Sabourin, Robert and Granger, Eric},
	month = jun,
	year = {2019},
	pages = {4317--4325},
}

@article{kannan_adversarial_2018,
	title = {Adversarial {Logit} {Pairing}},
	url = {http://arxiv.org/abs/1803.06373},
	abstract = {In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting—an important open scientiﬁc question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also ﬁnd that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on Imagenet against PGD white box attacks, with an accuracy improvement from 1.5\% to 27.9\%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on Imagenet (Trame`r et al., 2018), dropping its accuracy from 66.6\% to 47.1\%. With this new accuracy drop, adversarial logit pairing ties with Trame`r et al. (2018) for the state of the art on black box attacks on ImageNet.},
	language = {en},
	urldate = {2020-07-23},
	journal = {arXiv:1803.06373 [cs, stat]},
	author = {Kannan, Harini and Kurakin, Alexey and Goodfellow, Ian},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.06373},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{szegedy_inception-v4_2017,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any beneﬁts to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks signiﬁcantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classiﬁcation task signiﬁcantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classiﬁcation (CLS) challenge.},
	language = {en},
	booktitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence} ({AAAI}-17)},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
	year = {2017},
	pages = {7},
}

@inproceedings{zhang_you_2019,
	title = {You {Only} {Propagate} {Once}: {Accelerating} {Adversarial} {Training} via {Maximal} {Principle}},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zhang, Dinghuai and Zhang, Tianyuan and Lu, Yiping and Zhu, Zhanxing and Dong, Bin},
	year = {2019},
	pages = {12},
}

@inproceedings{vivek_gray-box_2018,
	title = {Gray-{Box} {Adversarial} {Training}},
	volume = {11219},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_13},
	doi = {10.1007/978-3-030-01267-0_13},
	abstract = {Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks.},
	language = {en},
	urldate = {2020-07-23},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Vivek, B. S. and Mopuri, Konda Reddy and Babu, R. Venkatesh},
	year = {2018},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {213--228},
}

@inproceedings{wang_bilateral_2019,
	address = {Seoul, Korea (South)},
	title = {Bilateral {Adversarial} {Training}: {Towards} {Fast} {Training} of {More} {Robust} {Models} {Against} {Adversarial} {Attacks}},
	isbn = {978-1-72814-803-8},
	shorttitle = {Bilateral {Adversarial} {Training}},
	url = {https://ieeexplore.ieee.org/document/9009088/},
	doi = {10.1109/ICCV.2019.00673},
	abstract = {In this paper, we study fast training of adversarially robust models. From the analyses of the state-of-the-art defense method, i.e., the multi-step adversarial training [34], we hypothesize that the gradient magnitude links to the model robustness. Motivated by this, we propose to perturb both the image and the label during training, which we call Bilateral Adversarial Training (BAT). To generate the adversarial label, we derive an closed-form heuristic solution. To generate the adversarial image, we use one-step targeted attack with the target label being the most confusing class. In the experiment, we ﬁrst show that random start and the most confusing target attack effectively prevent the label leaking and gradient masking problem. Then coupled with the adversarial label part, our model significantly improves the state-of-the-art results. For example, against PGD100 white-box attack with cross-entropy loss, on CIFAR10, we achieve 63.7\% versus 47.2\%; on SVHN, we achieve 59.1\% versus 42.1\%. At last, the experiment on the very (computationally) challenging ImageNet dataset further demonstrates the effectiveness of our fast method.},
	language = {en},
	urldate = {2020-07-22},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wang, Jianyu and Zhang, Haichao},
	month = oct,
	year = {2019},
	pages = {6628--6637},
}

@inproceedings{kornblith_better_2019,
	address = {Long Beach, CA, USA},
	title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954384/},
	doi = {10.1109/CVPR.2019.00277},
	abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on ImageNet provides minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
	language = {en},
	urldate = {2020-07-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
	month = jun,
	year = {2019},
	pages = {2656--2666},
}

@article{schmidhuber_deep_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {0893-6080},
	shorttitle = {Deep learning in neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	language = {en},
	urldate = {2020-07-19},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {2015},
	keywords = {Deep learning, Evolutionary computation, Reinforcement learning, Supervised learning, Unsupervised learning},
	pages = {85--117},
}

@inproceedings{greff_highway_2017,
	title = {Highway and {Residual} {Networks} {Learn} {Unrolled} {Iterative} {Estimation}},
	abstract = {The past year saw the introduction of new architectures such as Highway networks (Srivastava et al., 2015a) and Residual networks (He et al., 2015) which, for the ﬁrst time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Greff, Klaus and Srivastava, Rupesh K and Schmidhuber, Jürgen},
	year = {2017},
	pages = {14},
}

@inproceedings{jastrze_residual_2018,
	title = {Residual {Connections} {Encourage} {Iterative} {Inference}},
	abstract = {Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative reﬁnement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative reﬁnement in Resnets by showing that residual connections naturally encourage features of residual blocks to move along the negative gradient of loss as we go from one block to the next. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative reﬁnement. In general, a Resnet block tends to concentrate representation learning behavior in the ﬁrst few layers while higher layers perform iterative reﬁnement of features. Finally we observe that sharing residual layers naively leads to representation explosion and counterintuitively, overﬁtting, and we show that simple existing strategies can help alleviating this problem.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jastrze, Stanisław and Che, Tong and Bengio, Yoshua},
	year = {2018},
	pages = {14},
}

@inproceedings{veit_residual_2016,
	title = {Residual {Networks} {Behave} {Like} {Ensembles} of {Relatively} {Shallow} {Networks}},
	abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
	year = {2016},
	pages = {9},
}

@inproceedings{savarese_learning_2019,
	title = {Learning {Implicitly} {Recurrent} {CNNs} through {Parameter} {Sharing}},
	abstract = {We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are deﬁned by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a ﬂexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classiﬁcation tasks, while maintaining accuracy.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Savarese, Pedro and Maire, Michael},
	year = {2019},
	pages = {15},
}

@inproceedings{parikh_interactively_2011,
	address = {Colorado Springs, CO, USA},
	title = {Interactively building a discriminative vocabulary of nameable attributes},
	isbn = {978-1-4577-0394-2},
	url = {http://ieeexplore.ieee.org/document/5995451/},
	doi = {10.1109/CVPR.2011.5995451},
	abstract = {Human-nameable visual attributes offer many advantages when used as mid-level features for object recognition, but existing techniques to gather relevant attributes can be inefﬁcient (costing substantial effort or expertise) and/or insufﬁcient (descriptive properties need not be discriminative). We introduce an approach to deﬁne a vocabulary of attributes that is both human understandable and discriminative. The system takes object/scene-labeled images as input, and returns as output a set of attributes elicited from human annotators that distinguish the categories of interest. To ensure a compact vocabulary and efﬁcient use of annotators’ effort, we 1) show how to actively augment the vocabulary such that new attributes resolve inter-class confusions, and 2) propose a novel “nameability” manifold that prioritizes candidate attributes by their likelihood of being associated with a nameable property. We demonstrate the approach with multiple datasets, and show its clear advantages over baselines that lack a nameability model or rely on a list of expert-provided attributes.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Parikh, Devi and Grauman, Kristen},
	month = jun,
	year = {2011},
	pages = {1681--1688},
}

@inproceedings{yu_multi-scale_2016,
	title = {Multi-scale {Context} {Aggregation} by {Dilated} {Convolutions}},
	abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classiﬁcation. However, dense prediction problems such as semantic segmentation are structurally different from image classiﬁcation. In this work, we develop a new convolutional network module that is speciﬁcally designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multiscale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive ﬁeld without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classiﬁcation networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Yu, Fisher and Koltun, Vladlen},
	year = {2016},
	pages = {13},
}

@inproceedings{dai_unsupervised_2014,
	address = {Columbus, OH, USA},
	title = {Unsupervised {Learning} of {Dictionaries} of {Hierarchical} {Compositional} {Models}},
	isbn = {978-1-4799-5118-5},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909717},
	doi = {10.1109/CVPR.2014.321},
	abstract = {This paper proposes an unsupervised method for learning dictionaries of hierarchical compositional models for representing natural images. Each model is in the form of a template that consists of a small group of part templates that are allowed to shift their locations and orientations relative to each other, and each part template is in turn a composition of Gabor wavelets that are also allowed to shift their locations and orientations relative to each other. Given a set of unannotated training images, a dictionary of such hierarchical templates are learned so that each training image can be represented by a small number of templates that are spatially translated, rotated and scaled versions of the templates in the learned dictionary. The learning algorithm iterates between the following two steps: (1) Image encoding by a template matching pursuit process that involves a bottom-up template matching sub-process and a top-down template localization sub-process. (2) Dictionary re-learning by a shared matching pursuit process. Experimental results show that the proposed approach is capable of learning meaningful templates, and the learned templates are useful for tasks such as domain adaption and image cosegmentation.},
	language = {en},
	urldate = {2020-07-11},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Dai, Jifeng and Hong, Yi and Hu, Wenze and Zhu, Song-Chun and Wu, Ying Nian},
	month = jun,
	year = {2014},
	pages = {2505--2512},
}

@misc{noauthor_zoom_nodate,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	url = {https://distill.pub/2020/circuits/zoom-in/},
	urldate = {2020-07-11},
}

@article{friston_free-energy_2010,
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	copyright = {2010 Nature Publishing Group},
	issn = {1471-0048},
	shorttitle = {The free-energy principle},
	url = {https://www.nature.com/articles/nrn2787/boxes/bx1},
	doi = {10.1038/nrn2787},
	abstract = {Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
	language = {en},
	number = {2},
	urldate = {2020-07-11},
	journal = {Nature Reviews Neuroscience},
	author = {Friston, Karl},
	month = feb,
	year = {2010},
	note = {Number: 2
Publisher: Nature Publishing Group},
	pages = {127--138},
}

@inproceedings{srivastava_training_2015,
	title = {Training {Very} {Deep} {Networks}},
	abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difﬁcult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information ﬂow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information ﬂow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efﬁcient architectures.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, Jürgen},
	year = {2015},
	pages = {9},
}

@inproceedings{chen_dual_2017,
	title = {Dual {Path} {Networks}},
	abstract = {In this work, we present a simple, highly efﬁcient and modularized Dual Path Network (DPN) for image classiﬁcation which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we ﬁnd that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the beneﬁts from both path topologies, our proposed Dual Path Network shares common features while maintaining the ﬂexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64 × 4d) with 26\% smaller model size, 25\% less computational cost and 8\% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
	year = {2017},
	pages = {9},
}

@article{huang_predictive_2011,
	title = {Predictive coding},
	volume = {2},
	copyright = {Copyright © 2011 John Wiley \& Sons, Ltd.},
	issn = {1939-5086},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.142},
	doi = {10.1002/wcs.142},
	abstract = {Predictive coding is a unifying framework for understanding redundancy reduction and efficient coding in the nervous system. By transmitting only the unpredicted portions of an incoming sensory signal, predictive coding allows the nervous system to reduce redundancy and make full use of the limited dynamic range of neurons. Starting with the hypothesis of efficient coding as a design principle in the sensory system, predictive coding provides a functional explanation for a range of neural responses and many aspects of brain organization. The lateral and temporal antagonism in receptive fields in the retina and lateral geniculate nucleus occur naturally as a consequence of predictive coding of natural images. In the higher visual system, predictive coding provides an explanation for oriented receptive fields and contextual effects as well as the hierarchical reciprocally connected organization of the cortex. Predictive coding has also been found to be consistent with a variety of neurophysiological and psychophysical data obtained from different areas of the brain. WIREs Cogni Sci 2011 2 580–593 DOI: 10.1002/wcs.142 This article is categorized under: Computer Science {\textgreater} Neural Networks},
	language = {en},
	number = {5},
	urldate = {2020-07-11},
	journal = {WIREs Cognitive Science},
	author = {Huang, Yanping and Rao, Rajesh P. N.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.142},
	pages = {580--593},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2020-07-09},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@article{clarke_understanding_2015,
	title = {Understanding {What} {We} {See}: {How} {We} {Derive} {Meaning} {From} {Vision}},
	volume = {19},
	issn = {1364-6613},
	shorttitle = {Understanding {What} {We} {See}},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315001989},
	doi = {10.1016/j.tics.2015.08.008},
	abstract = {Recognising objects goes beyond vision, and requires models that incorporate different aspects of meaning. Most models focus on superordinate categories (e.g., animals, tools) which do not capture the richness of conceptual knowledge. We argue that object recognition must be seen as a dynamic process of transformation from low-level visual input through categorical organisation to specific conceptual representations. Cognitive models based on large normative datasets are well-suited to capture statistical regularities within and between concepts, providing both category structure and basic-level individuation. We highlight recent research showing how such models capture important properties of the ventral visual pathway. This research demonstrates that significant advances in understanding conceptual representations can be made by shifting the focus from studying superordinate categories to basic-level concepts.},
	language = {en},
	number = {11},
	urldate = {2020-07-09},
	journal = {Trends in Cognitive Sciences},
	author = {Clarke, Alex and Tyler, Lorraine K.},
	month = nov,
	year = {2015},
	keywords = {Concepts, category, fusiform gyrus, perirhinal cortex, semantics, ventral visual pathway},
	pages = {677--687},
}

@article{guclu_deep_2015,
	title = {Deep {Neural} {Networks} {Reveal} a {Gradient} in the {Complexity} of {Neural} {Representations} across the {Ventral} {Stream}},
	volume = {35},
	copyright = {Copyright © 2015 the authors 0270-6474/15/3510005-10\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/35/27/10005},
	doi = {10.1523/JNEUROSCI.5023-14.2015},
	abstract = {Converging evidence suggests that the primate ventral visual pathway encodes increasingly complex stimulus features in downstream areas. We quantitatively show that there indeed exists an explicit gradient for feature complexity in the ventral pathway of the human brain. This was achieved by mapping thousands of stimulus features of increasing complexity across the cortical sheet using a deep neural network. Our approach also revealed a fine-grained functional specialization of downstream areas of the ventral stream. Furthermore, it allowed decoding of representations from human brain activity at an unsurpassed degree of accuracy, confirming the quality of the developed approach. Stimulus features that successfully explained neural responses indicate that population receptive fields were explicitly tuned for object categorization. This provides strong support for the hypothesis that object categorization is a guiding principle in the functional organization of the primate ventral stream.},
	language = {en},
	number = {27},
	urldate = {2020-07-09},
	journal = {Journal of Neuroscience},
	author = {Güçlü, Umut and Gerven, Marcel A. J. van},
	month = jul,
	year = {2015},
	pmid = {26157000},
	note = {Publisher: Society for Neuroscience
Section: Articles},
	keywords = {deep learning, functional magnetic resonance imaging, neural coding},
	pages = {10005--10014},
}

@article{yamins_hierarchical_nodate,
	title = {Hierarchical {Modular} {Optimization} of {Convolutional} {Networks} {Achieves} {Representations} {Similar} to {Macaque} {IT} and {Human} {Ventral} {Stream}},
	abstract = {Humans recognize visually-presented objects rapidly and accurately. To understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition. One tool to assess the quality of a model of the ventral stream is the Representational Dissimilarity Matrix (RDM), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e. fMRI voxel responses, neural ﬁring rates) or in models (features). Previous work has shown that all known models of the ventral stream fail to capture the RDM pattern observed in either IT cortex, the highest ventral area, or in the human ventral stream. In this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce RDMs resembling both macaque IT and human ventral stream. The model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition.},
	language = {en},
	author = {Yamins, Daniel L and Hong, Ha and Cadieu, Charles and DiCarlo, James J},
	pages = {9},
}

@article{liao_how_nodate,
	title = {How {Important} {Is} {Weight} {Symmetry} in {Backpropagation}?},
	abstract = {Gradient backpropagation (BP) requires symmetric feedforward and feedback connections—the same weights must be used for forward and backward passes. This “weight transport problem” (Grossberg 1987) is thought to be one of the main reasons to doubt BP’s biologically plausibility. Using 15 different classiﬁcation datasets, we systematically investigate to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.’s demonstration (Lillicrap et al. 2014) but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter—the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100\% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a “Batch Manhattan” (BM) update rule.},
	language = {en},
	author = {Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso},
	pages = {8},
}

@article{lamme_feedforward_1998,
	title = {Feedforward, horizontal, and feedback processing in the visual cortex},
	volume = {8},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438898800421},
	doi = {10.1016/S0959-4388(98)80042-1},
	abstract = {The cortical visual system consists of many richly interconnected areas. Each area is characterized by more or less specific receptive field tuning properties. However, these tuning properties reflect only a subset of the interactions that occur within and between areas. Neuronal responses may be modulation by perceptual context or attention. These modulations reflect lateral interactions within areas and feedback from higher to lower areas. Recent work is beginning to unravel how horizontal and feedback connections each contribute to modulatory effects and what the role of these modulations is in vision. Whereas receptive field tuning properties reflect feedforward processing, modulations evoked by horizontal and feedback connections may reflect the integration of information that underlies perception.},
	language = {en},
	number = {4},
	urldate = {2020-07-07},
	journal = {Current Opinion in Neurobiology},
	author = {Lamme, Victor AF and Supèr, Hans and Spekreijse, Henk},
	month = aug,
	year = {1998},
	pages = {529--535},
}

@article{serre_feedforward_2007,
	title = {A feedforward architecture accounts for rapid categorization},
	volume = {104},
	copyright = {© 2007 by The National Academy of Sciences of the USA.                    Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/104/15/6424},
	doi = {10.1073/pnas.0700622104},
	abstract = {Primates are remarkably good at recognizing objects. The level of performance of their visual system and its robustness to image degradations still surpasses the best computer vision systems despite decades of engineering effort. In particular, the high accuracy of primates in ultra rapid object categorization and rapid serial visual presentation tasks is remarkable. Given the number of processing stages involved and typical neural latencies, such rapid visual processing is likely to be mostly feedforward. Here we show that a specific implementation of a class of feedforward theories of object recognition (that extend the Hubel and Wiesel simple-to-complex cell hierarchy and account for many anatomical and physiological constraints) can predict the level and the pattern of performance achieved by humans on a rapid masked animal vs. non-animal categorization task.},
	language = {en},
	number = {15},
	urldate = {2020-07-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Serre, Thomas and Oliva, Aude and Poggio, Tomaso},
	month = apr,
	year = {2007},
	pmid = {17404214},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {computational model, natural scenes, object recognition, preattentive vision, visual cortex},
	pages = {6424--6429},
}

@inproceedings{huang_densely_2017,
	address = {Honolulu, HI},
	title = {Densely {Connected} {Convolutional} {Networks}},
	isbn = {978-1-5386-0457-1},
	url = {https://ieeexplore.ieee.org/document/8099726/},
	doi = {10.1109/CVPR.2017.243},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jul,
	year = {2017},
	pages = {2261--2269},
}

@article{kubilius_deep_2016,
	title = {Deep {Neural} {Networks} as a {Computational} {Model} for {Human} {Shape} {Sensitivity}},
	volume = {12},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004896},
	doi = {10.1371/journal.pcbi.1004896},
	abstract = {Theories of object recognition agree that shape is of primordial importance, but there is no consensus about how shape might be represented, and so far attempts to implement a model of shape perception that would work with realistic stimuli have largely failed. Recent studies suggest that state-of-the-art convolutional ‘deep’ neural networks (DNNs) capture important aspects of human object perception. We hypothesized that these successes might be partially related to a human-like representation of object shape. Here we demonstrate that sensitivity for shape features, characteristic to human and primate vision, emerges in DNNs when trained for generic object recognition from natural photographs. We show that these models explain human shape judgments for several benchmark behavioral and neural stimulus sets on which earlier models mostly failed. In particular, although never explicitly trained for such stimuli, DNNs develop acute sensitivity to minute variations in shape and to non-accidental properties that have long been implicated to form the basis for object recognition. Even more strikingly, when tested with a challenging stimulus set in which shape and category membership are dissociated, the most complex model architectures capture human shape sensitivity as well as some aspects of the category structure that emerges from human judgments. As a whole, these results indicate that convolutional neural networks not only learn physically correct representations of object categories but also develop perceptually accurate representational spaces of shapes. An even more complete model of human object representations might be in sight by training deep architectures for multiple tasks, which is so characteristic in human development.},
	language = {en},
	number = {4},
	urldate = {2020-07-07},
	journal = {PLOS Computational Biology},
	author = {Kubilius, Jonas and Bracci, Stefania and Beeck, Hans P. Op de},
	month = apr,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Human performance, Monkeys, Neural networks, Object recognition, Sensory perception, Vision, Visual cortex},
	pages = {e1004896},
}

@inproceedings{zuo_convolutional_2015,
	address = {Boston, MA, USA},
	title = {Convolutional recurrent neural networks: {Learning} spatial dependencies for image representation},
	isbn = {978-1-4673-6759-2},
	shorttitle = {Convolutional recurrent neural networks},
	url = {http://ieeexplore.ieee.org/document/7301268/},
	doi = {10.1109/CVPRW.2015.7301268},
	abstract = {In existing convolutional neural networks (CNNs), both convolution and pooling are locally performed for image regions separately, no contextual dependencies between different image regions have been taken into consideration. Such dependencies represent useful spatial structure information in images. Whereas recurrent neural networks (RNNs) are designed for learning contextual dependencies among sequential data by using the recurrent (feedback) connections. In this work, we propose the convolutional recurrent neural network (C-RNN), which learns the spatial dependencies between image regions to enhance the discriminative power of image representation. The C-RNN is trained in an end-to-end manner from raw pixel images. CNN layers are ﬁrstly processed to generate middle level features. RNN layer is then learned to encode spatial dependencies. The C-RNN can learn better image representation, especially for images with obvious spatial contextual dependencies. Our method achieves competitive performance on ILSVRC 2012, SUN 397, and MIT indoor.},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Zuo, Zhen and Shuai, Bing and Wang, Gang and Liu, Xiao and Wang, Xingxing and Wang, Bing and Chen, Yushi},
	month = jun,
	year = {2015},
	pages = {18--26},
}

@inproceedings{zhang_progressive_2018,
	address = {Salt Lake City, UT},
	title = {Progressive {Attention} {Guided} {Recurrent} {Network} for {Salient} {Object} {Detection}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578179/},
	doi = {10.1109/CVPR.2018.00081},
	abstract = {Effective convolutional features play an important role in saliency estimation but how to learn powerful features for saliency is still a challenging task. FCN-based methods directly apply multi-level convolutional features without distinction, which leads to sub-optimal results due to the distraction from redundant details. In this paper, we propose a novel attention guided network which selectively integrates multi-level contextual information in a progressive manner. Attentive features generated by our network can alleviate distraction of background thus achieve better performance. On the other hand, it is observed that most of existing algorithms conduct salient object detection by exploiting side-output features of the backbone feature extraction network. However, shallower layers of backbone network lack the ability to obtain global semantic information, which limits the effective feature learning. To address the problem, we introduce multi-path recurrent feedback to enhance our proposed progressive attention driven framework. Through multi-path recurrent connections, global semantic information from the top convolutional layer is transferred to shallower layers, which intrinsically reﬁnes the entire network. Experimental results on six benchmark datasets demonstrate that our algorithm performs favorably against the state-of-the-art approaches.},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Xiaoning and Wang, Tiantian and Qi, Jinqing and Lu, Huchuan and Wang, Gang},
	month = jun,
	year = {2018},
	pages = {714--722},
}

@inproceedings{liu_semantic_2017,
	address = {Honolulu, HI},
	title = {Semantic {Regularisation} for {Recurrent} {Image} {Annotation}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099926/},
	doi = {10.1109/CVPR.2017.443},
	abstract = {The “CNN-RNN” design pattern is increasingly widely applied in a variety of image annotation tasks including multi-label classiﬁcation and captioning. Existing models use the weakly semantic CNN hidden layer or its transform as the image embedding that provides the interface between the CNN and RNN. This leaves the RNN overstretched with two jobs: predicting the visual concepts and modelling their correlations for generating structured annotation output. Importantly this makes the end-to-end training of the CNN and RNN slow and ineffective due to the difﬁculty of back propagating gradients through the RNN to train the CNN. We propose a simple modiﬁcation to the design pattern that makes learning more effective and efﬁcient. Speciﬁcally, we propose to use a semantically regularised embedding layer as the interface between the CNN and RNN. Regularising the interface can partially or completely decouple the learning problems, allowing each to be more effectively trained and jointly training much more efﬁcient. Extensive experiments show that state-of-the art performance is achieved on multi-label classiﬁcation as well as image captioning.},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Feng and Xiang, Tao and Hospedales, Timothy M. and Yang, Wankou and Sun, Changyin},
	month = jul,
	year = {2017},
	pages = {4160--4168},
}

@article{kim_recurrent_nodate,
	title = {Recurrent {Transformer} {Networks} for {Semantic} {Correspondence}},
	abstract = {We present recurrent transformer networks (RTNs) for obtaining dense correspondences between semantically similar images. Our networks accomplish this through an iterative process of estimating spatial transformations between the input images and using these transformations to generate aligned convolutional activations. By directly estimating the transformations between an image pair, rather than employing spatial transformer networks to independently normalize each individual image, we show that greater accuracy can be achieved. This process is conducted in a recursive manner to reﬁne both the transformation estimates and the feature representations. In addition, a technique is presented for weakly-supervised training of RTNs that is based on a proposed classiﬁcation loss. With RTNs, state-of-the-art performance is attained on several benchmarks for semantic correspondence.},
	language = {en},
	author = {Kim, Seungryong and Lin, Stephen and Jeon, Sang Ryul and Min, Dongbo and Sohn, Kwanghoon},
	pages = {11},
}

@article{wyatte_early_2014,
	title = {Early recurrent feedback facilitates visual object recognition under challenging conditions},
	volume = {5},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00674/full},
	doi = {10.3389/fpsyg.2014.00674},
	abstract = {Standard models of the visual object recognition pathway hold that a largely feedforward process from the retina through inferotemporal cortex leads to object identification. A subsequent feedback process originating in frontoparietal areas through reciprocal connections to striate cortex provides attentional support to salient or behaviorally-relevant features. Here, we review mounting evidence that feedback signals also originate within extrastriate regions and begin during the initial feedforward process. This feedback process is temporally dissociable from attention and provides important functions such as grouping, associational reinforcement, and filling-in of features. Local feedback signals operating concurrently with feedforward processing are important for object identification in noisy real-world situations, particularly when objects are partially occluded, unclear, or otherwise ambiguous. Altogether, the dissociation of early and late feedback processes presented here expands on current models of object identification, and suggests a dual role for descending feedback projections.},
	language = {English},
	urldate = {2020-07-06},
	journal = {Frontiers in Psychology},
	author = {Wyatte, Dean and Jilk, David J. and O'Reilly, Randall C.},
	year = {2014},
	note = {Publisher: Frontiers},
	keywords = {Feedback, Illusory contours, amodal completion, object recognition, top-down attention},
}

@inproceedings{lee_recursive_2016,
	address = {Las Vegas, NV, USA},
	title = {Recursive {Recurrent} {Nets} with {Attention} {Modeling} for {OCR} in the {Wild}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780614/},
	doi = {10.1109/CVPR.2016.245},
	abstract = {We present recursive recurrent neural networks with attention modeling (R2AM) for lexicon-free optical character recognition in natural scene images. The primary advantages of the proposed method are: (1) use of recursive convolutional neural networks (CNNs), which allow for parametrically efﬁcient and effective image feature extraction; (2) an implicitly learned character-level language model, embodied in a recurrent neural network which avoids the need to use N-grams; and (3) the use of a soft-attention mechanism, allowing the model to selectively exploit image features in a coordinated way, and allowing for end-to-end training within a standard backpropagation framework.},
	language = {en},
	urldate = {2020-07-06},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lee, Chen-Yu and Osindero, Simon},
	month = jun,
	year = {2016},
	pages = {2231--2239},
}

@inproceedings{shuai_dag-recurrent_2016,
	address = {Las Vegas, NV, USA},
	title = {{DAG}-{Recurrent} {Neural} {Networks} for {Scene} {Labeling}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780763/},
	doi = {10.1109/CVPR.2016.394},
	abstract = {In image labeling, local representations for image units are usually generated from their surrounding image patches, thus long-range contextual information is not effectively encoded. In this paper, we introduce recurrent neural networks (RNNs) to address this issue. Speciﬁcally, directed acyclic graph RNNs (DAG-RNNs) are proposed to process DAG-structured images, which enables the network to model long-range semantic dependencies among image units. Our DAG-RNNs are capable of tremendously enhancing the discriminative power of local representations, which signiﬁcantly beneﬁts the local classiﬁcation. Meanwhile, we propose a novel class weighting function that attends to rare classes, which phenomenally boosts the recognition accuracy for non-frequent classes. Integrating with convolution and deconvolution layers, our DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow, CamVid and Barcelona benchmarks.},
	language = {en},
	urldate = {2020-07-06},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shuai, Bing and Zuo, Zhen and Wang, Bing and Wang, Gang},
	month = jun,
	year = {2016},
	pages = {3620--3629},
}

@article{schrimpf_brain-score_nodate,
	title = {Brain-{Score}: {Which} {Artificial} {Neural} {Network} for {Object} {Recognition} is most {Brain}-{Like}?},
	language = {en},
	author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J and Rajalingham, Rishi and Issa, Elias B and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Geiger, Franziska and Schmidt, Kailyn and Yamins, Daniel L K and DiCarlo, James J},
	pages = {9},
}

@inproceedings{bashivan_teacher_2019,
	address = {Seoul, Korea (South)},
	title = {Teacher {Guided} {Architecture} {Search}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010650/},
	doi = {10.1109/ICCV.2019.00542},
	abstract = {Much of the recent improvement in neural networks for computer vision has resulted from discovery of new networks architectures. Most prior work has used the performance of candidate models following limited training to automatically guide the search in a feasible way. Could further gains in computational efﬁciency be achieved by guiding the search via measurements of a high performing network with unknown detailed architecture (e.g. the primate visual system)? As one step toward this goal, we use representational similarity analysis to evaluate the similarity of internal activations of candidate networks with those of a (ﬁxed, high performing) teacher network. We show that adopting this evaluation metric could produce up to an order of magnitude in search efﬁciency over performanceguided methods. Our approach ﬁnds a convolutional cell structure with similar performance as was previously found using other methods but at a total computational cost that is two orders of magnitude lower than Neural Architecture Search (NAS) and more than four times lower than progressive neural architecture search (PNAS). We further show that measurements from only ∼300 neurons from primate visual system provides enough signal to ﬁnd a network with an Imagenet top-1 error that is signiﬁcantly lower than that achieved by performance-guided architecture search alone. These results suggest that representational matching can be used to accelerate network architecture search in cases where one has access to some or all of the internal representations of a teacher network of interest, such as the brain’s sensory processing networks.},
	language = {en},
	urldate = {2020-07-06},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Bashivan, Pouya and Tensen, Mark and Dicarlo, James},
	month = oct,
	year = {2019},
	pages = {5319--5328},
}

@inproceedings{jin_multi-path_2017,
	title = {Multi-{Path} {Feedback} {Recurrent} {Neural} {Networks} for {Scene} {Parsing}},
	abstract = {In this paper, we consider the scene parsing problem and propose a novel Multi-Path Feedback recurrent neural network (MPF-RNN) for parsing scene images. MPF-RNN can enhance the capability of RNNs in modeling long-range context information at multiple levels and better distinguish pixels that are easy to confuse. Different from feedforward CNNs and RNNs with only single feedback, MPF-RNN propagates the contextual features learned at top layer through multiple weighted recurrent connections to learn bottom features. For better training MPF-RNN, we propose a new strategy that considers accumulative loss at multiple recurrent steps to improve performance of the MPF-RNN on parsing small objects. With these two novel components, MPF-RNN has achieved signiﬁcant improvement over strong baselines (VGG16 and Res101) on ﬁve challenging scene parsing benchmarks, including traditional SiftFlow, Barcelona, CamVid, Stanford Background as well as the recently released large-scale ADE20K.},
	language = {en},
	booktitle = {Thirty-{First} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Jin, Xiaojie and Chen, Yunpeng and Jie, Zequn and Feng, Jiashi and Yan, Shuicheng},
	year = {2017},
	pages = {7},
}

@article{zuo_learning_2016,
	title = {Learning {Contextual} {Dependence} {With} {Convolutional} {Hierarchical} {Recurrent} {Neural} {Networks}},
	volume = {25},
	issn = {1941-0042},
	doi = {10.1109/TIP.2016.2548241},
	abstract = {Deep convolutional neural networks (CNNs) have shown their great success on image classification. CNNs mainly consist of convolutional and pooling layers, both of which are performed on local image areas without considering the dependence among different image regions. However, such dependence is very important for generating explicit image representation. In contrast, recurrent neural networks (RNNs) are well known for their ability of encoding contextual information in sequential data, and they only require a limited number of network parameters. Thus, we proposed the hierarchical RNNs (HRNNs) to encode the contextual dependence in image representation. In HRNNs, each RNN layer focuses on modeling spatial dependence among image regions from the same scale but different locations. While the cross RNN scale connections target on modeling scale dependencies among regions from the same location but different scales. Specifically, we propose two RNN models: 1) hierarchical simple recurrent network (HSRN), which is fast and has low computational cost and 2) hierarchical long-short term memory recurrent network, which performs better than HSRN with the price of higher computational cost. In this paper, we integrate CNNs with HRNNs, and develop end-to-end convolutional hierarchical RNNs (C-HRNNs) for image classification. C-HRNNs not only utilize the discriminative representation power of CNNs, but also utilize the contextual dependence learning ability of our HRNNs. On four of the most challenging object/scene image classification benchmarks, our C-HRNNs achieve the state-of-the-art results on Places 205, SUN 397, and MIT indoor, and the competitive results on ILSVRC 2012.},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Zuo, Zhen and Shuai, Bing and Wang, Gang and Liu, Xiao and Wang, Xingxing and Wang, Bing and Chen, Yushi},
	month = jul,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {C-HRNN, Computer vision, Context modeling, Convolutional Neural Networks, Deep Learning, Deep learning, Image Classification, Image representation, Logic gates, Natural language processing, Recurrent Neural Networks, Recurrent neural networks, computational complexity, computational cost, contextual dependence learning, convolutional hierarchical recurrent neural networks, convolutional neural networks, hierarchical long-short term memory recurrent network, hierarchical simple recurrent network, image classification, image regions spatial dependence, image representation, object image classification, recurrent neural nets, recurrent neural networks, scene image classification},
	pages = {2983--2996},
}

@inproceedings{ronneberger_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	volume = {9351},
	isbn = {978-3-319-24573-7 978-3-319-24574-4},
	shorttitle = {U-{Net}},
	url = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {International {Conference} on {Medical} {Image} {Computing} and {Computer}-{Assisted} {Intervention}},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
}

@inproceedings{wen_deep_2018,
	title = {Deep {Predictive} {Coding} {Network} for {Object} {Recognition}},
	url = {http://proceedings.mlr.press/v80/wen18a.html},
	abstract = {Based on the predictive coding theory in neuro- science, we designed a bi-directional and recur- rent neural net, namely deep predictive coding networks (PCN), that has feedforward, feedback, and r...},
	language = {en},
	urldate = {2020-07-06},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wen, Haiguang and Han, Kuan and Shi, Junxing and Zhang, Yizhen and Culurciello, Eugenio and Liu, Zhongming},
	month = jul,
	year = {2018},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {5266--5275},
}

@article{manassi_what_2016,
	title = {What crowding can tell us about object representations},
	volume = {16},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?articleid=2497927},
	doi = {10.1167/16.3.35},
	language = {en},
	number = {3},
	urldate = {2020-07-06},
	journal = {Journal of Vision},
	author = {Manassi, Mauro and Lonchampt, Sophie and Clarke, Aaron and Herzog, Michael H.},
	month = feb,
	year = {2016},
	note = {Publisher: The Association for Research in Vision and Ophthalmology},
	pages = {35--35},
}

@article{doerig_crowding_2020,
	title = {Crowding reveals fundamental differences in local vs. global processing in humans and machines},
	volume = {167},
	issn = {0042-6989},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698919302299},
	doi = {10.1016/j.visres.2019.12.006},
	abstract = {Feedforward Convolutional Neural Networks (ffCNNs) have become state-of-the-art models both in computer vision and neuroscience. However, human-like performance of ffCNNs does not necessarily imply human-like computations. Previous studies have suggested that current ffCNNs do not make use of global shape information. However, it is currently unclear whether this reflects fundamental differences between ffCNN and human processing or is merely an artefact of how ffCNNs are trained. Here, we use visual crowding as a well-controlled, specific probe to test global shape computations. Our results provide evidence that ffCNNs cannot produce human-like global shape computations for principled architectural reasons. We lay out approaches that may address shortcomings of ffCNNs to provide better models of the human visual system.},
	language = {en},
	urldate = {2020-07-06},
	journal = {Vision Research},
	author = {Doerig, A. and Bornet, A. and Choung, O. H. and Herzog, M. H.},
	month = feb,
	year = {2020},
	keywords = {Convolutional Neural Networks, Crowding, Deep Neural Networks, Global processing, Grouping, Segmentation},
	pages = {39--45},
}

@article{kietzmann_recurrence_2019,
	title = {Recurrence is required to capture the representational dynamics of the human visual system},
	volume = {116},
	copyright = {Copyright © 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/43/21854},
	doi = {10.1073/pnas.1905544116},
	abstract = {The human visual system is an intricate network of brain regions that enables us to recognize the world around us. Despite its abundant lateral and feedback connections, object processing is commonly viewed and studied as a feedforward process. Here, we measure and model the rapid representational dynamics across multiple stages of the human ventral stream using time-resolved brain imaging and deep learning. We observe substantial representational transformations during the first 300 ms of processing within and across ventral-stream regions. Categorical divisions emerge in sequence, cascading forward and in reverse across regions, and Granger causality analysis suggests bidirectional information flow between regions. Finally, recurrent deep neural network models clearly outperform parameter-matched feedforward models in terms of their ability to capture the multiregion cortical dynamics. Targeted virtual cooling experiments on the recurrent deep network models further substantiate the importance of their lateral and top-down connections. These results establish that recurrent models are required to understand information processing in the human ventral stream.},
	language = {en},
	number = {43},
	urldate = {2020-07-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kietzmann, Tim C. and Spoerer, Courtney J. and Sörensen, Lynn K. A. and Cichy, Radoslaw M. and Hauk, Olaf and Kriegeskorte, Nikolaus},
	month = oct,
	year = {2019},
	pmid = {31591217},
	note = {Publisher: National Academy of Sciences
Section: PNAS Plus},
	keywords = {deep recurrent neural networks, magnetoencephalography, object recognition, representational dynamics, virtual cooling},
	pages = {21854--21863},
}

@article{torralba_contextual_2006,
	title = {Contextual guidance of eye movements and attention in real-world scenes: {The} role of global features in object search.},
	volume = {113},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Contextual guidance of eye movements and attention in real-world scenes},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.113.4.766},
	doi = {10.1037/0033-295X.113.4.766},
	abstract = {Many experiments have shown that the human visual system makes extensive use of contextual information for facilitating object search in natural scenes. However, the question of how to formally model contextual influences is still open. On the basis of a Bayesian framework, the authors present an original approach of attentional guidance by global scene context. The model comprises 2 parallel pathways; one pathway computes local features (saliency) and the other computes global (scenecentered) features. The contextual guidance model of attention combines bottom-up saliency, scene context, and top-down mechanisms at an early stage of visual processing and predicts the image regions likely to be fixated by human observers performing natural search tasks in real-world scenes.},
	language = {en},
	number = {4},
	urldate = {2020-07-06},
	journal = {Psychological Review},
	author = {Torralba, Antonio and Oliva, Aude and Castelhano, Monica S. and Henderson, John M.},
	month = oct,
	year = {2006},
	pages = {766--786},
}

@article{hayhoe_eye_2005,
	title = {Eye movements in natural behavior},
	volume = {9},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661305000598},
	doi = {10.1016/j.tics.2005.02.009},
	abstract = {The classic experiments of Yarbus over 50 years ago revealed that saccadic eye movements reflect cognitive processes. But it is only recently that three separate advances have greatly expanded our understanding of the intricate role of eye movements in cognitive function. The first is the demonstration of the pervasive role of the task in guiding where and when to fixate. The second has been the recognition of the role of internal reward in guiding eye and body movements, revealed especially in neurophysiological studies. The third important advance has been the theoretical developments in the fields of reinforcement learning and graphic simulation. All of these advances are proving crucial for understanding how behavioral programs control the selection of visual information.},
	language = {en},
	number = {4},
	urldate = {2020-07-06},
	journal = {Trends in Cognitive Sciences},
	author = {Hayhoe, Mary and Ballard, Dana},
	month = apr,
	year = {2005},
	pages = {188--194},
}

@inproceedings{alexe_what_2010,
	title = {What is an object?},
	doi = {10.1109/CVPR.2010.5540226},
	abstract = {We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any class. We explicitly train it to distinguish objects with a well-defined boundary in space, such as cows and telephones, from amorphous background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring characteristics of objects, such as appearing different from their surroundings and having a closed boundary. This includes an innovative cue measuring the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we show this new cue to outperform a state-of-the-art saliency measure, and the combined measure to perform better than any cue alone. Finally, we show how to sample windows from an image according to their objectness distribution and give an algorithm to employ them as location priors for modern class-specific object detectors. In experiments on PASCAL VOC 07 we show this greatly reduces the number of windows evaluated by class-specific object detectors.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Alexe, Bogdan and Deselaers, Thomas and Ferrari, Vittorio},
	month = jun,
	year = {2010},
	note = {ISSN: 1063-6919},
	keywords = {Bayesian framework, Bayesian methods, Detectors, Image color analysis, Image edge detection, Image segmentation, Pixel, Training, belief networks, image cues measurement, image recognition, image window, object detection, objectness measurement},
	pages = {73--80},
}

@article{rensink_dynamic_2000,
	title = {The {Dynamic} {Representation} of {Scenes}},
	volume = {7},
	issn = {1350-6285},
	url = {https://doi.org/10.1080/135062800394667},
	doi = {10.1080/135062800394667},
	abstract = {One of the more powerful impressions created by vision is that of a coherent, richly detailed world where everything is present simultaneously. Indeed, this impression is so compelling that we tend to ascribe these properties not only to the external world, but to our internal representations as well. But results from several recent experiments argue against this latter ascription. For example, changes in images of real-world scenes often go unnoticed when made during a saccade, flicker, blink, or movie cut. This “change blindness” provides strong evidence against the idea that our brains contain a picture-like representation of the scene that is everywhere detailed and coherent. How then do we represent a scene? It is argued here that focused attention provides spatiotemporal coherence for the stable representation of one object at a time. It is then argued that the allocation of attention can be co-ordinated to create a “virtual representation”. In such a scheme, a stable object representation is formed whenever needed, making it appear to higher levels as if all objects in the scene are represented in detail simultaneously.},
	number = {1-3},
	urldate = {2020-07-06},
	journal = {Visual Cognition},
	author = {Rensink, Ronald A.},
	month = jan,
	year = {2000},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/135062800394667},
	pages = {17--42},
}

@article{oreilly_recurrent_2013,
	title = {Recurrent {Processing} during {Object} {Recognition}},
	volume = {4},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00124/full},
	doi = {10.3389/fpsyg.2013.00124},
	abstract = {How does the brain learn to recognize objects visually, and perform this difficult feat robustly in the face of many sources of ambiguity and variability? We present a computational model based on the biology of the relevant visual pathways that learns to reliably recognize 100 different object categories in the face of of naturally-occurring variability in location, rotation, size, and lighting. The model exhibits robustness to highly ambiguous, partially occluded inputs. Both the unified, biologically plausible learning mechanism and the robustness to occlusion derive from the role that recurrent connectivity and recurrent processing mechanisms play in the model. Furthermore, this interaction of recurrent connectivity and learning predicts that high-level visual representations should be shaped by error signals from nearby, associated brain areas over the course of visual learning. Consistent with this prediction, we show how semantic knowledge about object categories changes the nature of their learned visual representations, as well as how this representational shift supports the mapping between perceptual and conceptual knowledge. Altogether, these findings support the potential importance of ongoing recurrent processing throughout the brain's visual system and suggest ways in which object recognition can be understood in terms of interactions within and between processes over time.},
	language = {English},
	urldate = {2020-07-05},
	journal = {Frontiers in Psychology},
	author = {O'Reilly, Randall C. and Wyatte, Dean and Herd, Seth and Mingus, Brian and Jilk, David J.},
	year = {2013},
	note = {Publisher: Frontiers},
	keywords = {Feedback, Winners-take-all mechanism, computational model, object recognition, recurrent processing},
}

@article{felleman_distributed_1991,
	title = {Distributed {Hierarchical} {Processing} in the {Primate} {Cerebral} {Cortex}},
	volume = {1},
	issn = {1047-3211, 1460-2199},
	url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/1.1.1},
	doi = {10.1093/cercor/1.1.1},
	abstract = {Europe PMC is an archive of life sciences journal literature., Distributed hierarchical processing in the primate cerebral cortex.},
	language = {en},
	number = {1},
	urldate = {2020-07-05},
	journal = {Cerebral Cortex},
	author = {Felleman, D. J. and Van Essen, D. C.},
	month = jan,
	year = {1991},
	pages = {1--47},
}

@article{wyatte_limits_2012,
	title = {The {Limits} of {Feedforward} {Vision}: {Recurrent} {Processing} {Promotes} {Robust} {Object} {Recognition} when {Objects} {Are} {Degraded}},
	volume = {24},
	issn = {0898-929X, 1530-8898},
	shorttitle = {The {Limits} of {Feedforward} {Vision}},
	url = {http://www.mitpressjournals.org/doi/10.1162/jocn_a_00282},
	doi = {10.1162/jocn_a_00282},
	language = {en},
	number = {11},
	urldate = {2020-07-05},
	journal = {Journal of Cognitive Neuroscience},
	author = {Wyatte, Dean and Curran, Tim and O'Reilly, Randall},
	month = nov,
	year = {2012},
	pages = {2248--2261},
}

@article{hupe_cortical_1998,
	title = {Cortical feedback improves discrimination between figure and background by {V1}, {V2} and {V3} neurons},
	volume = {394},
	copyright = {1998 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/29537/},
	doi = {10.1038/29537},
	abstract = {A single visual stimulus activates neurons in many different cortical areas. A major challenge in cortical physiology is to understand how the neural activity in these numerous active zones leads to a unified percept of the visual scene. The anatomical basis for these interactions is the dense network of connections that link the visual areas. Within this network, feedforward connections transmit signals from lower-order areas such as V1 or V2 to higher-order areas. In addition, there is a dense web of feedback connections which, despite their anatomical prominence1,2,3,4, remain functionally mysterious5,6,7,8. Here we show, using reversible inactivation of a higher-order area (monkey area V5/MT), that feedback connections serve to amplify and focus activity of neurons in lower-order areas, and that they are important in the differentiation of figure from ground, particularly in the case of stimuli of low visibility. More specifically, we show that feedback connections facilitate responses to objects moving within the classical receptive field; enhance suppression evoked by background stimuli in the surrounding region; and have the strongest effects for stimuli of low salience.},
	language = {en},
	number = {6695},
	urldate = {2020-07-05},
	journal = {Nature},
	author = {Hupé, J. M. and James, A. C. and Payne, B. R. and Lomber, S. G. and Girard, P. and Bullier, J.},
	month = aug,
	year = {1998},
	note = {Number: 6695
Publisher: Nature Publishing Group},
	pages = {784--787},
}

@article{gilbert_brain_2007,
	title = {Brain {States}: {Top}-{Down} {Influences} in {Sensory} {Processing}},
	volume = {54},
	issn = {0896-6273},
	shorttitle = {Brain {States}},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627307003765},
	doi = {10.1016/j.neuron.2007.05.019},
	abstract = {All cortical and thalamic levels of sensory processing are subject to powerful top-down influences, the shaping of lower-level processes by more complex information. New findings on the diversity of top-down interactions show that cortical areas function as adaptive processors, being subject to attention, expectation, and perceptual task. Brain states are determined by the interactions between multiple cortical areas and the modulation of intrinsic circuits by feedback connections. In perceptual learning, both the encoding and recall of learned information involves a selection of the appropriate inputs that convey information about the stimulus being discriminated. Disruption of this interaction may lead to behavioral disorders, including schizophrenia.},
	language = {en},
	number = {5},
	urldate = {2020-07-05},
	journal = {Neuron},
	author = {Gilbert, Charles D. and Sigman, Mariano},
	month = jun,
	year = {2007},
	pages = {677--696},
}

@inproceedings{goodfellow_maxout_2013,
	title = {Maxout {Networks}},
	url = {http://proceedings.mlr.press/v28/goodfellow13.html},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its ...},
	language = {en},
	urldate = {2020-07-05},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	author = {Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	month = feb,
	year = {2013},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {1319--1327},
}

@inproceedings{wang_saliency_2016,
	address = {Cham},
	title = {Saliency {Detection} with {Recurrent} {Fully} {Convolutional} {Networks}},
	volume = {9908},
	isbn = {978-3-319-46492-3 978-3-319-46493-0},
	url = {http://link.springer.com/10.1007/978-3-319-46493-0_50},
	doi = {10.1007/978-3-319-46493-0_50},
	abstract = {Deep networks have been proved to encode high level semantic features and delivered superior performance in saliency detection. In this paper, we go one step further by developing a new saliency model using recurrent fully convolutional networks (RFCNs). Compared with existing deep network based methods, the proposed network is able to incorporate saliency prior knowledge for more accurate inference. In addition, the recurrent architecture enables our method to automatically learn to reﬁne the saliency map by correcting its previous errors. To train such a network with numerous parameters, we propose a pre-training strategy using semantic segmentation data, which simultaneously leverages the strong supervision of segmentation tasks for better training and enables the network to capture generic representations of objects for saliency detection. Through extensive experimental evaluations, we demonstrate that the proposed method compares favorably against stateof-the-art approaches, and that the proposed recurrent deep model as well as the pre-training method can signiﬁcantly improve performance.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Wang, Linzhao and Wang, Lijun and Lu, Huchuan and Zhang, Pingping and Ruan, Xiang},
	year = {2016},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {825--841},
}

@article{eigen_understanding_2014,
	title = {Understanding {Deep} {Architectures} using a {Recursive} {Convolutional} {Network}},
	url = {http://arxiv.org/abs/1312.1847},
	abstract = {A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these inﬂuence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We ﬁnd that while increasing the numbers of layers and parameters each have clear beneﬁt, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and ﬁnds most of its beneﬁt through the introduction of more weights. Our results (i) empirically conﬁrm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.},
	language = {en},
	urldate = {2020-07-04},
	journal = {arXiv:1312.1847 [cs]},
	author = {Eigen, David and Rolfe, Jason and Fergus, Rob and LeCun, Yann},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.1847},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{zheng_conditional_2015,
	address = {Santiago, Chile},
	title = {Conditional {Random} {Fields} as {Recurrent} {Neural} {Networks}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410536/},
	doi = {10.1109/ICCV.2015.179},
	abstract = {Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixellevel labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-ﬁeld approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding ofﬂine post-processing methods for object delineation.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H. S.},
	month = dec,
	year = {2015},
	pages = {1529--1537},
}

@inproceedings{chatfield_return_2014,
	address = {Nottingham},
	title = {Return of the {Devil} in the {Details}: {Delving} {Deep} into {Convolutional} {Nets}},
	isbn = {978-1-901725-52-0},
	shorttitle = {Return of the {Devil} in the {Details}},
	url = {http://www.bmva.org/bmvc/2014/papers/paper054/index.html},
	doi = {10.5244/C.28.6},
	abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, signiﬁcantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced signiﬁcantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {Proceedings of the {British} {Machine} {Vision} {Conference} 2014},
	publisher = {British Machine Vision Association},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year = {2014},
	pages = {6.1--6.12},
}

@inproceedings{ballas_delving_2016,
	title = {Delving {Deeper} into {Convolutional} {Networks} for {Learning} {Video} {Representations}},
	url = {http://arxiv.org/abs/1511.06432},
	abstract = {We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call “percepts” using Gated-Recurrent-Unit Recurrent Networks (GRUs). Our method relies on percepts that are extracted from all levels of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model ﬁner motion patterns.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ballas, Nicolas and Yao, Li and Pal, Chris and Courville, Aaron},
	year = {2016},
	note = {arXiv: 1511.06432},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{alom_inception_2017,
	title = {Inception {Recurrent} {Convolutional} {Neural} {Network} for {Object} {Recognition}},
	url = {http://arxiv.org/abs/1704.07709},
	abstract = {Deep convolutional neural networks (DCNNs) are an inﬂuential tool for solving various problems in the machine learning and computer vision ﬁelds. In this paper, we introduce a new deep learning model called an InceptionRecurrent Convolutional Neural Network (IRCNN), which utilizes the power of an inception network combined with recurrent layers in DCNN architecture. We have empirically evaluated the recognition performance of the proposed IRCNN model using different benchmark datasets such as MNIST, CIFAR-10, CIFAR100, and SVHN. Experimental results show similar or higher recognition accuracy when compared to most of the popular DCNNs including the RCNN. Furthermore, we have investigated IRCNN performance against equivalent Inception Networks and Inception-Residual Networks using the CIFAR-100 dataset. We report about 3.5\%, 3.47\% and 2.54\% improvement in classiﬁcation accuracy when compared to the RCNN, equivalent Inception Networks, and InceptionResidual Networks on the augmented CIFAR100 dataset respectively.},
	language = {en},
	urldate = {2020-07-04},
	journal = {arXiv:1704.07709 [cs]},
	author = {Alom, Md Zahangir and Hasan, Mahmudul and Yakopcic, Chris and Taha, Tarek M.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.07709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{cukur_attention_2013,
	title = {Attention during natural vision warps semantic representation across the human brain},
	volume = {16},
	copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.3381},
	doi = {10.1038/nn.3381},
	abstract = {The authors use functional magnetic resonance imaging to measure how the semantic representation changes when searching for different object categories in natural movies. They find tuning shifts that expand the representation of the attended category and of semantically related, but unattended, categories, and compress the representation of categories semantically dissimilar to the target.},
	language = {en},
	number = {6},
	urldate = {2020-07-04},
	journal = {Nature Neuroscience},
	author = {Çukur, Tolga and Nishimoto, Shinji and Huth, Alexander G. and Gallant, Jack L.},
	month = jun,
	year = {2013},
	note = {Number: 6
Publisher: Nature Publishing Group},
	pages = {763--770},
}

@article{rothenstein_attention_2008,
	series = {Cognitive {Vision}-{Special} {Issue}},
	title = {Attention links sensing to recognition},
	volume = {26},
	issn = {0262-8856},
	url = {http://www.sciencedirect.com/science/article/pii/S0262885606000680},
	doi = {10.1016/j.imavis.2005.08.011},
	abstract = {This paper presents arguments that explicit strategies for visual attentional selection are important for cognitive vision systems, and shows that a number of proposals currently exist for exactly how parts of this goal may be accomplished. A comprehensive survey of approaches to computational attention is given. A key characteristic of virtually all the models surveyed here is that they receive significant inspiration from the neurobiology and psychophysics of human and primate vision. This, although not necessarily a key component of mainstream computer vision, seems very appropriate for cognitive vision systems given a definition of the topic that always includes the goal of human-like visual performance. A particular model, the Selective Tuning model, is overviewed in some detail. The growing neurobiological and psychophysical evidence for its biological plausibility is cited highlighting the fact that it has more biological support than other models; it is further claimed that it may form an appropriate starting point for the difficult task of integrating attention into cognitive vision systems.},
	language = {en},
	number = {1},
	urldate = {2020-07-04},
	journal = {Image and Vision Computing},
	author = {Rothenstein, Albert L. and Tsotsos, John K.},
	month = jan,
	year = {2008},
	keywords = {Attention, Cognitive vision, Recognition, Selective tuning},
	pages = {114--126},
}

@article{baluch_mechanisms_2011,
	title = {Mechanisms of top-down attention},
	volume = {34},
	issn = {0166-2236},
	url = {http://www.sciencedirect.com/science/article/pii/S0166223611000191},
	doi = {10.1016/j.tins.2011.02.003},
	abstract = {Attention exhibits characteristic neural signatures in brain regions that process sensory signals. An important area of future research is to understand the nature of top-down signals that facilitate attentional guidance towards behaviorally relevant locations and features. In this review, we discuss recent studies that have made progress towards understanding: (i) the brain structures and circuits involved in attentional allocation; (ii) top-down attention pathways, particularly as elucidated by microstimulation and lesion studies; (iii) top-down modulatory influences involving subcortical structures and reward systems; (iv) plausible substrates and embodiments of top-down signals; and (v) information processing and theoretical constraints that might be helpful in guiding future experiments. Understanding top-down attention is crucial for elucidating the mechanisms by which we can filter sensory information to pay attention to the most behaviorally relevant events.},
	language = {en},
	number = {4},
	urldate = {2020-07-04},
	journal = {Trends in Neurosciences},
	author = {Baluch, Farhan and Itti, Laurent},
	month = apr,
	year = {2011},
	pages = {210--224},
}

@article{thorpe_speed_1996,
	title = {Speed of processing in the human visual system},
	volume = {381},
	copyright = {1996 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/381520a0},
	doi = {10.1038/381520a0},
	abstract = {How long does it take for the human visual system to process a complex natural image? Subjectively, recognition of familiar objects and scenes appears to be virtually instantaneous, but measuring this processing time experimentally has proved difficult. Behavioural measures such as reaction times can be used1, but these include not only visual processing but also the time required for response execution. However, event-related potentials (ERPs) can sometimes reveal signs of neural processing well before the motor output2. Here we use a go/no-go categorization task in which subjects have to decide whether a previously unseen photograph, flashed on for just 20 ms, contains an animal. ERP analysis revealed a frontal negativity specific to no-go trials that develops roughly 150 ms after stimulus onset. We conclude that the visual processing needed to perform this highly demanding task can be achieved in under 150 ms.},
	language = {en},
	number = {6582},
	urldate = {2020-07-04},
	journal = {Nature},
	author = {Thorpe, Simon and Fize, Denis and Marlot, Catherine},
	month = jun,
	year = {1996},
	note = {Number: 6582
Publisher: Nature Publishing Group},
	pages = {520--522},
}

@inproceedings{tianjun_xiao_application_2015,
	address = {Boston, MA, USA},
	title = {The application of two-level attention models in deep convolutional neural network for fine-grained image classification},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298685/},
	doi = {10.1109/CVPR.2015.7298685},
	abstract = {Fine-grained classiﬁcation is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difﬁcult. Most ﬁne-grained classiﬁcation systems follow the pipeline of ﬁnding foreground object or object parts (where) to extract discriminative features (what).},
	language = {en},
	urldate = {2020-07-02},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {{Tianjun Xiao} and {Yichong Xu} and {Kuiyuan Yang} and {Jiaxing Zhang} and {Yuxin Peng} and Zhang, Zheng},
	month = jun,
	year = {2015},
	pages = {842--850},
}

@inproceedings{wang_attentional_2014,
	title = {Attentional {Neural} {Network}: {Feature} {Selection} {Using} {Cognitive} {Feedback}},
	abstract = {Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down inﬂuence is especially effective when dealing with high noise or difﬁcult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classiﬁcation accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Qian and Zhang, Jiaxing and Song, Sen and Zhang, Zheng},
	year = {2014},
	pages = {9},
}

@inproceedings{stollenga_deep_2014,
	title = {Deep {Networks} with {Internal} {Selective} {Attention} through {Feedback} {Connections}},
	abstract = {Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet’s feedback structure can dynamically alter its convolutional ﬁlter sensitivities during classiﬁcation. It harnesses the power of sequential processing to improve classiﬁcation performance, by allowing the network to iteratively focus its internal attention on some of its convolutional ﬁlters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Stollenga, Marijn F and Masci, Jonathan and Gomez, Faustino and Schmidhuber, Jürgen},
	year = {2014},
	pages = {9},
}

@inproceedings{salakhutdinov_deep_2009,
	title = {Deep {Boltzmann} {Machines}},
	url = {http://proceedings.mlr.press/v5/salakhutdinov09a.html},
	abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to fo...},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {Proceedings of the {Twelth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
	month = apr,
	year = {2009},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {448--455},
}

@article{dhingra_differentiable_2020,
	title = {Differentiable {Reasoning} {Over} {A} {Virtual} {Knowledge} {Base}},
	language = {en},
	author = {Dhingra, Bhuwan and Zaheer, Manzil and Balachandran, Vidhisha and Neubig, Graham and Salakhutdinov, Ruslan and Cohen, William W},
	year = {2020},
	pages = {16},
}

@article{lee_hierarchical_2003,
	title = {Hierarchical {Bayesian} inference in the visual cortex},
	volume = {20},
	issn = {1084-7529, 1520-8532},
	url = {https://www.osapublishing.org/abstract.cfm?URI=josaa-20-7-1434},
	doi = {10.1364/JOSAA.20.001434},
	language = {en},
	number = {7},
	urldate = {2020-07-04},
	journal = {Journal of the Optical Society of America A},
	author = {Lee, Tai Sing and Mumford, David},
	month = jul,
	year = {2003},
	pages = {1434},
}

@article{cichy_resolving_2014,
	title = {Resolving human object recognition in space and time},
	volume = {17},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/nn.3635},
	doi = {10.1038/nn.3635},
	language = {en},
	number = {3},
	urldate = {2020-07-04},
	journal = {Nature Neuroscience},
	author = {Cichy, Radoslaw Martin and Pantazis, Dimitrios and Oliva, Aude},
	month = mar,
	year = {2014},
	pages = {455--462},
}

@article{felzenszwalb_object_2010,
	title = {Object {Detection} with {Discriminatively} {Trained} {Part}-{Based} {Models}},
	volume = {32},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2009.167},
	abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI–SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
	number = {9},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
	month = sep,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Algorithms, Artificial Intelligence, Bicycles, Computer Society, Computer vision, Deformable models, Discriminant Analysis, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Iterative algorithms, Lighting, Object detection, Object recognition, PASCAL object detection, Pattern Recognition, Automated, Reproducibility of Results, Sensitivity and Specificity, Shape, Speech recognition, Support vector machines, data mining, deformable models, discriminative trained part-based models, discriminative training, iterative methods, iterative training algorithm, latent SVM objective function, latent SVM., margin-sensitive approach, multiscale deformable part models, object detection, object detection system, object recognition, pictorial structures, support vector machine, support vector machines},
	pages = {1627--1645},
}

@inproceedings{szegedy_going_2015,
	address = {Boston, MA, USA},
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classiﬁcation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classiﬁcation and detection.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	pages = {1--9},
}

@inproceedings{he_delving_2015,
	address = {Santiago, Chile},
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://ieeexplore.ieee.org/document/7410480/},
	doi = {10.1109/ICCV.2015.123},
	abstract = {Rectiﬁed activation units (rectiﬁers) are essential for state-of-the-art neural networks. In this work, we study rectiﬁer neural networks for image classiﬁcation from two aspects. First, we propose a Parametric Rectiﬁed Linear Unit (PReLU) that generalizes the traditional rectiﬁed unit. PReLU improves model ﬁtting with nearly zero extra computational cost and little overﬁtting risk. Second, we derive a robust initialization method that particularly considers the rectiﬁer nonlinearities. This method enables us to train extremely deep rectiﬁed models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classiﬁcation dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the ﬁrst1 to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	pages = {1026--1034},
}

@article{kruger_deep_2013,
	title = {Deep {Hierarchies} in the {Primate} {Visual} {Cortex}: {What} {Can} {We} {Learn} for {Computer} {Vision}?},
	volume = {35},
	issn = {1939-3539},
	shorttitle = {Deep {Hierarchies} in the {Primate} {Visual} {Cortex}},
	doi = {10.1109/TPAMI.2012.272},
	abstract = {Computational modeling of the primate visual system yields insights of potential relevance to some of the challenges that computer vision is facing, such as object recognition and categorization, motion detection and activity recognition, or vision-based navigation and manipulation. This paper reviews some functional principles and structures that are generally thought to underlie the primate visual cortex, and attempts to extract biological principles that could further advance computer vision research. Organized for a computer vision audience, we present functional principles of the processing hierarchies present in the primate visual system considering recent discoveries in neurophysiology. The hierarchical processing in the primate visual system is characterized by a sequence of different levels of processing (on the order of 10) that constitute a deep hierarchy in contrast to the flat vision architectures predominantly used in today's mainstream computer vision. We hope that the functional description of the deep hierarchies realized in the primate visual system provides valuable insights for the design of computer vision algorithms, fostering increasingly productive interaction between biological and computer vision research.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kruger, Norbert and Janssen, Peter and Kalkan, Sinan and Lappe, Markus and Leonardis, Ales and Piater, Justus and Rodriguez-Sanchez, Antonio J. and Wiskott, Laurenz},
	month = aug,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Animals, Artificial Intelligence, Computer vision, Humans, Neurons, Organizations, Pattern Recognition, Visual, Primates, Retina, Vision, Ocular, Visual Cortex, Visual systems, Visualization, activity recognition, advance computer vision research, biological modeling, biological principle extraction, biology computing, computational modeling, computer vision, computer vision algorithms, computer vision audience, deep hierarchies, flat vision architectures, functional principles, hierarchical processing, motion detection, neurophysiology, object categorization, object recognition, primate visual cortex, primate visual system, vision-based manipulation, vision-based navigation},
	pages = {1847--1871},
}

@article{beck_top-down_2009,
	series = {Visual {Attention}: {Psychophysics}, electrophysiology and neuroimaging},
	title = {Top-down and bottom-up mechanisms in biasing competition in the human brain},
	volume = {49},
	issn = {0042-6989},
	url = {http://www.sciencedirect.com/science/article/pii/S0042698908003635},
	doi = {10.1016/j.visres.2008.07.012},
	abstract = {The biased competition theory of selective attention has been an influential neural theory of attention, motivating numerous animal and human studies of visual attention and visual representation. There is now neural evidence in favor of all three of its most basic principles: that representation in the visual system is competitive; that both top-down and bottom-up biasing mechanisms influence the ongoing competition; and that competition is integrated across brain systems. We review the evidence in favor of these three principles, and in particular, findings related to six more specific neural predictions derived from these original principles.},
	language = {en},
	number = {10},
	urldate = {2020-07-04},
	journal = {Vision Research},
	author = {Beck, Diane M. and Kastner, Sabine},
	month = jun,
	year = {2009},
	keywords = {Bias, Brain, Suppression, Visual attention, fMRI},
	pages = {1154--1165},
}

@inproceedings{jetley_learn_2018,
	title = {Learn to {Pay} {Attention}},
	abstract = {We propose an end-to-end-trainable attention module for convolutional neural network (CNN) architectures built for image classiﬁcation. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modiﬁed through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parameterised by the score matrices, must alone be used for classiﬁcation. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of attention values. Our experimental observations provide clear evidence to this effect: the learned attention maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classiﬁcation, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our attention maps outperform other CNN-based attention maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jetley, Saumya and Lord, Nicholas A and Lee, Namhoon and Torr, Philip H S},
	year = {2018},
	pages = {14},
}

@inproceedings{cao_look_2015,
	address = {Santiago, Chile},
	title = {Look and {Think} {Twice}: {Capturing} {Top}-{Down} {Visual} {Attention} with {Feedback} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {Look and {Think} {Twice}},
	url = {http://ieeexplore.ieee.org/document/7410695/},
	doi = {10.1109/ICCV.2015.338},
	abstract = {While feedforward deep convolutional neural networks (CNNs) have been a great success in computer vision, it is important to note that the human visual cortex generally contains more feedback than feedforward connections. In this paper, we will brieﬂy introduce the background of feedbacks in the human visual cortex, which motivates us to develop a computational feedback mechanism in deep neural networks. In addition to the feedforward inference in traditional neural networks, a feedback loop is introduced to infer the activation status of hidden layer neurons according to the “goal” of the network, e.g., high-level semantic labels. We analogize this mechanism as “Look and Think Twice.” The feedback networks help better visualize and understand how deep neural networks work, and capture visual attention on expected objects, even in images with cluttered background and multiple objects. Experiments on ImageNet dataset demonstrate its effectiveness in solving tasks such as image classiﬁcation and object localization.},
	language = {en},
	urldate = {2020-07-04},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Cao, Chunshui and Liu, Xianming and Yang, Yi and Yu, Yinan and Wang, Jiang and Wang, Zilei and Huang, Yongzhen and Wang, Liang and Huang, Chang and Xu, Wei and Ramanan, Deva and Huang, Thomas S.},
	month = dec,
	year = {2015},
	pages = {2956--2964},
}

@article{bengio_learning_1994,
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	issn = {1941-0093},
	doi = {10.1109/72.279181},
	abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Bengio, Y. and Simard, P. and Frasconi, P.},
	month = mar,
	year = {1994},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Computer networks, Cost function, Delay effects, Discrete transforms, Displays, Intelligent networks, Neural networks, Neurofeedback, Production, Recurrent neural networks, efficient learning, gradient descent, input/output sequence mapping, learning (artificial intelligence), long-term dependencies, numerical analysis, prediction problems, production problems, recognition, recurrent neural nets, recurrent neural network training, temporal contingencies},
	pages = {157--166},
}

@inproceedings{moosavi-dezfooli_universal_2017,
	title = {Universal {Adversarial} {Perturbations}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.html},
	urldate = {2020-07-03},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	year = {2017},
	pages = {1765--1773},
}

@inproceedings{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	doi = {10.1109/SP.2017.49},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95\% to 0.5\%. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100\% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
	booktitle = {2017 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Carlini, Nicholas and Wagner, David},
	month = may,
	year = {2017},
	note = {ISSN: 2375-1207},
	keywords = {Malware, Measurement, Neural networks, Resists, Robustness, Security, Speech recognition, attack algorithms, defensive distillation, distance metrics, high-confidence adversarial examples, machine learning, neural nets, neural networks, security of data, transferability test},
	pages = {39--57},
}

@inproceedings{papernot_limitations_2016,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	doi = {10.1109/EuroSP.2016.36},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	booktitle = {2016 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS} {P})},
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Biological neural networks, DNN, Distortion, Force, Machine learning, Neurons, Training, adversarial samples, adversarial samples:, computer vision, deep neural networks, hardness measure, human subjects, image classification, input features, large datasets, learning (artificial intelligence), neural nets, target classification, training algorithms, training phase},
	pages = {372--387},
}

@inproceedings{brown_adversarial_2017,
	title = {Adversarial {Patch}},
	url = {http://arxiv.org/abs/1712.09665},
	abstract = {We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classiﬁer to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classiﬁers; even when the patches are small, they cause the classiﬁers to ignore the other items in the scene and report a chosen target class.},
	language = {en},
	urldate = {2020-07-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} {Workshop}},
	author = {Brown, Tom B. and Mané, Dandelion and Roy, Aurko and Abadi, Martín and Gilmer, Justin},
	year = {2017},
	note = {arXiv: 1712.09665},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{li_contour_2006,
	title = {Contour {Saliency} in {Primary} {Visual} {Cortex}},
	volume = {50},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627306004156},
	doi = {10.1016/j.neuron.2006.04.035},
	abstract = {Contour integration is an important intermediate stage of object recognition, in which line segments belonging to an object boundary are perceptually linked and segmented from complex backgrounds. Contextual influences observed in primary visual cortex (V1) suggest the involvement of V1 in contour integration. Here, we provide direct evidence that, in monkeys performing a contour detection task, there was a close correlation between the responses of V1 neurons and the perceptual saliency of contours. Receiver operating characteristic analysis showed that single neuronal responses encode the presence or absence of a contour as reliably as the animal's behavioral responses. We also show that the same visual contours elicited significantly weaker neuronal responses when they were not detected in the detection task, or when they were unattended. Our results demonstrate that contextual interactions in V1 play a pivotal role in contour integration and saliency.},
	language = {en},
	number = {6},
	urldate = {2020-07-02},
	journal = {Neuron},
	author = {Li, Wu and Piëch, Valentin and Gilbert, Charles D.},
	month = jun,
	year = {2006},
	keywords = {SYSNEURO},
	pages = {951--962},
}

@incollection{angelucci_contribution_2006,
	series = {Visual {Perception}},
	title = {Contribution of feedforward, lateral and feedback connections to the classical receptive field center and extra-classical receptive field surround of primate {V1} neurons},
	volume = {154},
	url = {http://www.sciencedirect.com/science/article/pii/S0079612306540051},
	abstract = {A central question in visual neuroscience is what circuits generate the responses of neurons in the primary visual cortex (V1). V1 neurons respond best to oriented stimuli of optimal size within their receptive field (RF) center. This size tuning is contrast dependent, i.e. a neuron's optimal stimulus size measured at high contrast (the high-contrast summation RF, or hsRF) is smaller than when measured using low-contrast stimuli (the low-contrast summation RF, or lsRF). Responses to stimuli in the RF center are usually suppressed by iso-oriented stimuli in the extra-classical RF surround. Iso-orientation surround suppression is fast and long range, extending well beyond the size of V1 cells’ lsRF. Geniculocortical feedforward (FF), V1 lateral and extrastriate feedback (FB) connections to V1 could all contribute to generating the RF center and surround of V1 neurons. Studies on the spatio-temporal properties and functional organization of these connections can help disclose their specific contributions to the responses of V1 cells. These studies, reviewed in this chapter, have shown that FF afferents to V1 integrate signals within the hsRF of V1 cells; V1 lateral connections are commensurate with the size of the lsRF and may, thus, underlie contrast-dependent changes in spatial summation, and modulatory effects arising from the surround region closer to the RF center (the “near” surround). The spatial and temporal properties of lateral connections cannot account for the dimensions and onset latency of modulation arising from more distant regions of the surround (the “far” surround). Inter-areal FB connections to V1, instead, are commensurate with the full spatial range of center and surround responses, and show fast conduction velocity consistent with the short onset latency of modulation arising from the “far” surround. We review data showing that a subset of FB connections terminate in a patchy fashion in V1, and show modular and orientation specificity, consistent with their proposed role in orientation-specific center–surround interactions. We propose specific mechanisms by which each connection type contributes to the RF center and surround of V1 neurons, and implement these hypotheses into a recurrent network model. We show physiological data in support of the model's predictions, revealing that modulation from the “far” surround is not always suppressive, but can be facilitatory under specific stimulus conditions.},
	language = {en},
	urldate = {2020-07-02},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Angelucci, Alessandra and Bressloff, Paul C.},
	editor = {Martinez-Conde, S. and Macknik, S. L. and Martinez, L. M. and Alonso, J. -M. and Tse, P. U.},
	month = jan,
	year = {2006},
	doi = {10.1016/S0079-6123(06)54005-1},
	keywords = {extrastriate, geniculocortical, horizontal connections, macaque., striate cortex, surround modulation},
	pages = {93--120},
}

@inproceedings{wang_gated_2017,
	title = {Gated {Recurrent} {Convolution} {Neural} {Network} for {OCR}},
	abstract = {Optical Character Recognition (OCR) aims to recognize text in natural images. Inspired by a recently proposed model for general image classiﬁcation, Recurrent Convolution Neural Network (RCNN), we propose a new architecture named Gated RCNN (GRCNN) for solving this problem. Its critical component, Gated Recurrent Convolution Layer (GRCL), is constructed by adding a gate to the Recurrent Convolution Layer (RCL), the critical component of RCNN. The gate controls the context modulation in RCL and balances the feed-forward information and the recurrent information. In addition, an efﬁcient Bidirectional Long ShortTerm Memory (BLSTM) is built for sequence modeling. The GRCNN is combined with BLSTM to recognize text in natural images. The entire GRCNN-BLSTM model can be trained end-to-end. Experiments show that the proposed model outperforms existing methods on several benchmark datasets including the IIIT-5K, Street View Text (SVT) and ICDAR.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Jianfeng and Hu, Xiaolin},
	year = {2017},
	pages = {10},
}

@inproceedings{lin_feature_2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html},
	urldate = {2020-06-29},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	year = {2017},
	pages = {2117--2125},
}

@article{liu_deep_2018,
	title = {A {Deep} {Spatial} {Contextual} {Long}-{Term} {Recurrent} {Convolutional} {Network} for {Saliency} {Detection}},
	volume = {27},
	issn = {1941-0042},
	doi = {10.1109/TIP.2018.2817047},
	abstract = {Traditional saliency models usually adopt hand-crafted image features and human-designed mechanisms to calculate local or global contrast. In this paper, we propose a novel computational saliency model, i.e., deep spatial contextual long-term recurrent convolutional network (DSCLRCN), to predict where people look in natural scenes. DSCLRCN first automatically learns saliency related local features on each image location in parallel. Then, in contrast with most other deep network based saliency models which infer saliency in local contexts, DSCLRCN can mimic the cortical lateral inhibition mechanisms in human visual system to incorporate global contexts to assess the saliency of each image location by leveraging the deep spatial long short-term memory (DSLSTM) model. Moreover, we also integrate scene context modulation in DSLSTM for saliency inference, leading to a novel deep spatial contextual LSTM (DSCLSTM) model. The whole network can be trained end-to-end and works efficiently when testing. Experimental results on two benchmark datasets show that DSCLRCN can achieve state-of-the-art performance on saliency detection. Furthermore, the proposed DSCLSTM model can significantly boost the saliency detection performance by incorporating both global spatial interconnections and scene context modulation, which may uncover novel inspirations for studies on them in computational saliency models.},
	number = {7},
	journal = {IEEE Transactions on Image Processing},
	author = {Liu, Nian and Han, Junwei},
	month = jul,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Computational modeling, Context modeling, DSCLRCN, DSCLSTM model, Feature extraction, Modulation, Saliency detection, Task analysis, Visualization, computational saliency model, convolutional neural networks, cortical lateral inhibition mechanisms, deep spatial contextual LSTM, deep spatial contextual LSTM model, deep spatial contextual long-term recurrent convolutional network, deep spatial long short-term memory, eye fixation prediction, feature extraction, global context, global spatial interconnections, hand-crafted image features, human visual system, human-designed mechanisms, image location, learning (artificial intelligence), long short-term memory, natural scenes, recurrent neural nets, saliency detection performance, saliency inference, saliency related local features, scene context, scene context modulation, short-term memory model, traditional saliency models},
	pages = {3264--3274},
}

@inproceedings{cho_learning_2014,
	address = {Doha, Qatar},
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/D14-1179},
	doi = {10.3115/v1/D14-1179},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	language = {en},
	urldate = {2020-06-29},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	year = {2014},
	pages = {1724--1734},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2020-06-29},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	note = {Number: 6088
Publisher: Nature Publishing Group},
	pages = {533--536},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	language = {en},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@inproceedings{shi_convolutional_2015,
	title = {Convolutional {LSTM} {Network}: {A} {Machine} {Learning} {Approach} for {Precipitation} {Nowcasting}},
	abstract = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-theart operational ROVER algorithm for precipitation nowcasting.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
	year = {2015},
	pages = {9},
}

@article{bar_top-down_2006,
	title = {Top-down facilitation of visual recognition},
	volume = {103},
	copyright = {Copyright © 2006, The National Academy of Sciences},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/103/2/449},
	doi = {10.1073/pnas.0507062103},
	abstract = {Cortical analysis related to visual object recognition is traditionally thought to propagate serially along a bottom-up hierarchy of ventral areas. Recent proposals gradually promote the role of top-down processing in recognition, but how such facilitation is triggered remains a puzzle. We tested a specific model, proposing that low spatial frequencies facilitate visual object recognition by initiating top-down processes projected from orbitofrontal to visual cortex. The present study combined magnetoencephalography, which has superior temporal resolution, functional magnetic resonance imaging, and a behavioral task that yields successful recognition with stimulus repetitions. Object recognition elicited differential activity that developed in the left orbitofrontal cortex 50 ms earlier than it did in recognition-related areas in the temporal cortex. This early orbitofrontal activity was directly modulated by the presence of low spatial frequencies in the image. Taken together, the dynamics we revealed provide strong support for the proposal of how top-down facilitation of object recognition is initiated, and our observations are used to derive predictions for future research.},
	language = {en},
	number = {2},
	urldate = {2020-06-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bar, M. and Kassam, K. S. and Ghuman, A. S. and Boshyan, J. and Schmid, A. M. and Dale, A. M. and Hämäläinen, M. S. and Marinkovic, K. and Schacter, D. L. and Rosen, B. R. and Halgren, E.},
	month = jan,
	year = {2006},
	pmid = {16407167},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {feedback, low spatial frequency, object recognition, orbitofrontal cortex, visual cortex},
	pages = {449--454},
}

@article{lamme_distinct_2000,
	title = {The distinct modes of vision offered by feedforward and recurrent processing},
	volume = {23},
	issn = {0166-2236},
	url = {http://www.sciencedirect.com/science/article/pii/S016622360001657X},
	doi = {10.1016/S0166-2236(00)01657-X},
	abstract = {An analysis of response latencies shows that when an image is presented to the visual system, neuronal activity is rapidly routed to a large number of visual areas. However, the activity of cortical neurons is not determined by this feedforward sweep alone. Horizontal connections within areas, and higher areas providing feedback, result in dynamic changes in tuning. The differences between feedforward and recurrent processing could prove pivotal in understanding the distinctions between attentive and pre-attentive vision as well as between conscious and unconscious vision. The feedforward sweep rapidly groups feature constellations that are hardwired in the visual brain, yet is probably incapable of yielding visual awareness; in many cases, recurrent processing is necessary before the features of an object are attentively grouped and the stimulus can enter consciousness.},
	language = {en},
	number = {11},
	urldate = {2020-06-29},
	journal = {Trends in Neurosciences},
	author = {Lamme, Victor A. F. and Roelfsema, Pieter R.},
	month = nov,
	year = {2000},
	pages = {571--579},
}

@article{werbos_backpropagation_1990,
	title = {Backpropagation through time: what it does and how to do it},
	volume = {78},
	issn = {1558-2256},
	shorttitle = {Backpropagation through time},
	doi = {10.1109/5.58337},
	abstract = {Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with the method are described. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed. The focus is on designing a simpler version of backpropagation which can be translated into computer code and applied directly by neutral network users.{\textless}{\textgreater}},
	number = {10},
	journal = {Proceedings of the IEEE},
	author = {Werbos, P.J.},
	month = oct,
	year = {1990},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Artificial neural networks, Backpropagation, Books, Control systems, Equations, Fluid dynamics, Neural networks, Pattern recognition, Power system modeling, Supervised learning, backpropagation, fault diagnosis, identification, neural nets, neural networks, pattern recognition, pseudocode, systems identification},
	pages = {1550--1560},
}

@inproceedings{mnih_recurrent_2014,
	title = {Recurrent {Models} of {Visual} {Attention}},
	abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-speciﬁc policies. We evaluate our model on several image classiﬁcation tasks, where it signiﬁcantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex},
	year = {2014},
	pages = {9},
}

@inproceedings{pinheiro_recurrent_2014,
	title = {Recurrent {Convolutional} {Neural} {Networks} for {Scene} {Labeling}},
	abstract = {The goal of the scene labeling task is to assign a class label to each pixel in an image. To ensure a good visual coherence and a high class accuracy, it is essential for a model to capture long range (pixel) label dependencies in images. In a feed-forward architecture, this can be achieved simply by considering a sufﬁciently large input context patch, around each pixel to be labeled. We propose an approach that consists of a recurrent convolutional neural network which allows us to consider a large input context while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation technique nor any taskspeciﬁc features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identiﬁes and corrects its own errors. Our approach yields state-ofthe-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time.},
	language = {en},
	booktitle = {Proceedings of the 31th {International} {Conference} on {Machine} {Learning}},
	author = {Pinheiro, Pedro O and Collobert, Ronan},
	year = {2014},
	pages = {9},
}

@inproceedings{donahue_long-term_2015,
	title = {Long-{Term} {Recurrent} {Convolutional} {Networks} for {Visual} {Recognition} and {Description}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html},
	urldate = {2020-06-29},
	author = {Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
	year = {2015},
	pages = {2625--2634},
}

@inproceedings{linsley_learning_2018,
	title = {Learning long-range spatial dependencies with horizontal gated-recurrent units},
	abstract = {Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching – and sometimes even surpassing – human accuracy on a variety of visual recognition tasks. Here, however, we show that these neural networks and their recent extensions struggle in recognition tasks where co-dependent visual features must be detected over long spatial ranges. We introduce a visual challenge, Pathﬁnder, and describe a novel recurrent neural network architecture called the horizontal gated recurrent unit (hGRU) to learn intrinsic horizontal connections – both within and across feature columns. We demonstrate that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures with orders of magnitude more parameters.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Serre, Thomas},
	year = {2018},
}

@inproceedings{hendricks_generating_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Generating {Visual} {Explanations}},
	isbn = {978-3-319-46493-0},
	doi = {10.1007/978-3-319-46493-0_1},
	abstract = {Clearly explaining a rationale for a classification decision to an end user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. Through a novel loss function based on sampling and reinforcement learning, our model learns to generate sentences that realize a global sentence property, such as class specificity. Our results on the CUB dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.},
	language = {en},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
	year = {2016},
	keywords = {Image description, Language and vision, Visual explanation},
	pages = {3--19},
}

@inproceedings{murdoch_automatic_2017,
	title = {Automatic {Rule} {Extraction} from {Long} {Short} {Term} {Memory} {Networks}},
	url = {http://arxiv.org/abs/1702.02540},
	abstract = {Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classiﬁer which approximates the output of the LSTM.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Murdoch, W. James and Szlam, Arthur},
	year = {2017},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{parkash_attributes_2012,
	address = {Berlin, Heidelberg},
	title = {Attributes for {Classifier} {Feedback}},
	volume = {7574},
	isbn = {978-3-642-33711-6 978-3-642-33712-3},
	url = {http://link.springer.com/10.1007/978-3-642-33712-3_26},
	doi = {10.1007/978-3-642-33712-3_26},
	abstract = {Traditional active learning allows a (machine) learner to query the (human) teacher for labels on examples it ﬁnds confusing. The teacher then provides a label for only that instance. This is quite restrictive. In this paper, we propose a learning paradigm in which the learner communicates its belief (i.e. predicted label) about the actively chosen example to the teacher. The teacher then conﬁrms or rejects the predicted label. More importantly, if rejected, the teacher communicates an explanation for why the learner’s belief was wrong. This explanation allows the learner to propagate the feedback provided by the teacher to many unlabeled images. This allows a classiﬁer to better learn from its mistakes, leading to accelerated discriminative learning of visual concepts even with few labeled images. In order for such communication to be feasible, it is crucial to have a language that both the human supervisor and the machine learner understand. Attributes provide precisely this channel. They are human-interpretable mid-level visual concepts shareable across categories e.g. “furry”, “spacious”, etc. We advocate the use of attributes for a supervisor to provide feedback to a classiﬁer and directly communicate his knowledge of the world. We employ a straightforward approach to incorporate this feedback in the classiﬁer, and demonstrate its power on a variety of visual recognition scenarios such as image classiﬁcation and annotation. This application of attributes for providing classiﬁers feedback is very powerful, and has not been explored in the community. It introduces a new mode of supervision, and opens up several avenues for future research.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer Berlin Heidelberg},
	author = {Parkash, Amar and Parikh, Devi and Parkash, Amar and Parikh, Devi},
	editor = {Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
	year = {2012},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {354--368},
}

@inproceedings{li_are_2019,
	title = {Are {Generative} {Classiﬁers} {More} {Robust} to {Adversarial} {Attacks}?},
	abstract = {There is a rising interest in studying the robustness of deep neural network classiﬁers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classiﬁers, which only model the conditional distribution of the labels given the inputs. In this paper, we propose and investigate the deep Bayes classiﬁer, which improves classical naive Bayes with conditional deep generative models. We further develop detection methods for adversarial examples, which reject inputs with low likelihood under the generative model. Experimental results suggest that deep Bayes classiﬁers are more robust than deep discriminative classiﬁers, and that the proposed detection methods are effective against many recently proposed attacks.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Li, Yingzhen and Bradshaw, John and Sharma, Yash},
	year = {2019},
	pages = {11},
}

@inproceedings{brendel_approximating_2019,
	title = {Approximating {CNNs} with {Bag}-of-local-features {Models} {Works} {Surprisingly} {Well} on {ImageNet}},
	abstract = {Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difﬁcult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet50 architecture called BagNet, classiﬁes an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6\% top-5 for 33 × 33 px features and Alexnet performance for 17 × 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image inﬂuences the classiﬁcation. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classiﬁers in the last few years is mostly achieved by better ﬁne-tuning rather than by qualitatively different decision strategies.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Brendel, Wieland and Bethge, Matthias},
	year = {2019},
	pages = {15},
}

@inproceedings{ribeiro_anchors_2018,
	title = {Anchors: {High} {Precision} {Model}-{Agnostic} {Explanations}},
	abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufﬁcient” conditions for predictions. We propose an algorithm to efﬁciently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the ﬂexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
	language = {en},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2018},
	pages = {9},
}

@inproceedings{zhang_growing_2017,
	title = {Growing {Interpretable} {Part} {Graphs} on {ConvNets} via {Multi}-{Shot} {Learning}},
	abstract = {This paper proposes a learning strategy that extracts objectpart concepts from a pre-trained convolutional neural network (CNN), in an attempt to 1) explore explicit semantics hidden in CNN units and 2) gradually grow a semantically interpretable graphical model on the pre-trained CNN for hierarchical object understanding. Given part annotations on very few (e.g. 3–12) objects, our method mines certain latent patterns from the pre-trained CNN and associates them with different semantic parts. We use a four-layer And-Or graph to organize the mined latent patterns, so as to clarify their internal semantic hierarchy. Our method is guided by a small number of part annotations, and it achieves superior performance (about 13\%–107\% improvement) in part center prediction on the PASCAL VOC and ImageNet datasets1.},
	language = {en},
	booktitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Zhang, Quanshi and Cao, Ruiming and Wu, Ying Nian and Zhu, Song-Chun},
	year = {2017},
	pages = {9},
}

@inproceedings{sabour_adversarial_2016,
	title = {Adversarial {Manipulation} of {Deep} {Representations}},
	abstract = {We show that the image representations in a deep neural network (DNN) can be manipulated to mimic those of other natural images, with only minor, imperceptible perturbations to the original image. Previous methods for generating adversarial images focused on image perturbations designed to produce erroneous class labels. Here we instead concentrate on the internal layers of DNN representations, to produce a new class of adversarial images that differs qualitatively from others. While the adversary is perceptually similar to one image, its internal representation appears remarkably similar to a different image, from a different class and bearing little if any apparent similarity to the input. Further, they appear generic and consistent with the space of natural images. This phenomenon demonstrates the possibility to trick a DNN to confound almost any image with any other chosen image, and raises questions about DNN representations, as well as the properties of natural images themselves.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sabour, Sara and Cao, Yanshuai and Faghri, Fartash and Fleet, David J},
	year = {2016},
	pages = {18},
}

@inproceedings{singla_understanding_2019,
	title = {Understanding {Impacts} of {High}-{Order} {Loss} {Approximations} and {Features} in {Deep} {Learning} {Interpretation}},
	url = {http://proceedings.mlr.press/v97/singla19a.html},
	abstract = {Current saliency map interpretations for neural networks generally rely on two key assumptions. First, they use first-order approximations of the loss function, neglecting higher-order terms such a...},
	language = {en},
	urldate = {2020-06-27},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Singla, Sahil and Wallace, Eric and Feng, Shi and Feizi, Soheil},
	month = may,
	year = {2019},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {5848--5856},
}

@article{olah_building_2018,
	title = {The {Building} {Blocks} of {Interpretability}},
	volume = {3},
	issn = {2476-0757},
	url = {https://distill.pub/2018/building-blocks},
	doi = {10.23915/distill.00010},
	abstract = {Interpretability techniques are normally studied in isolation. We explore the powerful interfaces that arise when you combine them -- and the rich structure of this combinatorial space.},
	language = {en},
	number = {3},
	urldate = {2020-06-25},
	journal = {Distill},
	author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
	month = mar,
	year = {2018},
	pages = {e10},
}

@article{khaligh-razavi_deep_2014,
	title = {Deep {Supervised}, but {Not} {Unsupervised}, {Models} {May} {Explain} {IT} {Cortical} {Representation}},
	volume = {10},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003915},
	doi = {10.1371/journal.pcbi.1003915},
	abstract = {Inferior temporal (IT) cortex in human and nonhuman primates serves visual object recognition. Computational objectvision models, although continually improving, do not yet reach human performance. It is unclear to what extent the internal representations of computational models can explain the IT representation. Here we investigate a wide range of computational model representations (37 in total), testing their categorization performance and their ability to account for the IT representational geometry. The models include well-known neuroscientific object-recognition models (e.g. HMAX, VisNet) along with several models from computer vision (e.g. SIFT, GIST, self-similarity features, and a deep convolutional neural network). We compared the representational dissimilarity matrices (RDMs) of the model representations with the RDMs obtained from human IT (measured with fMRI) and monkey IT (measured with cell recording) for the same set of stimuli (not used in training the models). Better performing models were more similar to IT in that they showed greater clustering of representational patterns by category. In addition, better performing models also more strongly resembled IT in terms of their within-category representational dissimilarities. Representational geometries were significantly correlated between IT and many of the models. However, the categorical clustering observed in IT was largely unexplained by the unsupervised models. The deep convolutional network, which was trained by supervision with over a million categorylabeled images, reached the highest categorization performance and also best explained IT, although it did not fully explain the IT data. Combining the features of this model with appropriate weights and adding linear combinations that maximize the margin between animate and inanimate objects and between faces and other objects yielded a representation that fully explained our IT data. Overall, our results suggest that explaining IT requires computational features trained through supervised learning to emphasize the behaviorally important categorical divisions prominently reflected in IT.},
	language = {en},
	number = {11},
	urldate = {2020-06-24},
	journal = {PLoS Computational Biology},
	author = {Khaligh-Razavi, Seyed-Mahdi and Kriegeskorte, Nikolaus},
	editor = {Diedrichsen, Jörn},
	month = nov,
	year = {2014},
	pages = {e1003915},
}

@inproceedings{hendrycks_benchmarking_2019,
	title = {Benchmarking {Neural} {Network} {Robustness} to {Common} {Corruptions} and {Perturbations}},
	url = {http://arxiv.org/abs/1903.12261},
	abstract = {In this paper we establish rigorous benchmarks for image classiﬁer robustness. Our ﬁrst benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classiﬁers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classiﬁer’s robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We ﬁnd that there are negligible changes in relative corruption robustness from AlexNet classiﬁers to ResNet classiﬁers. Afterward we discover ways to enhance corruption and perturbation robustness. We even ﬁnd that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	year = {2019},
	keywords = {dataset},
}

@inproceedings{yin_fourier_2019,
	title = {A {Fourier} {Perspective} on {Model} {Robustness} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/1906.08988},
	abstract = {Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed tradeoffs caused by Gaussian data augmentation and adversarial training. We ﬁnd that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations. Towards this end we observe that AutoAugment [6], a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C [17] benchmark.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yin, Dong and Lopes, Raphael Gontijo and Shlens, Jonathon and Cubuk, Ekin D. and Gilmer, Justin},
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{han_deep_2018,
	title = {Deep {Predictive} {Coding} {Network} with {Local} {Recurrent} {Processing} for {Object} {Recognition}},
	abstract = {Inspired by "predictive coding" - a theory in neuroscience, we develop a bidirectional and dynamic neural network with local recurrent processing, namely predictive coding network (PCN). Unlike feedforward-only convolutional neural networks, PCN includes both feedback connections, which carry top-down predictions, and feedforward connections, which carry bottom-up errors of prediction. Feedback and feedforward connections enable adjacent layers to interact locally and recurrently to reﬁne representations towards minimization of layer-wise prediction errors. When unfolded over time, the recurrent processing gives rise to an increasingly deeper hierarchy of non-linear transformation, allowing a shallow network to dynamically extend itself into an arbitrarily deep network. We train and test PCN for image classiﬁcation with SVHN, CIFAR and ImageNet datasets. Despite notably fewer layers and parameters, PCN achieves competitive performance compared to classical and state-of-the-art models. Further analysis shows that the internal representations in PCN converge over time and yield increasingly better accuracy in object recognition. Errors of top-down prediction also reveal visual saliency or bottom-up attention.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Han, Kuan and Wen, Haiguang and Zhang, Yizhen and Fu, Di and Culurciello, Eugenio and Liu, Zhongming},
	year = {2018},
	pages = {13},
}

@article{lotter_deep_2017,
	title = {Deep {Predictive} {Coding} {Networks} for {Video} {Prediction} and {Unsupervised} {Learning}},
	abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning — leveraging unlabeled examples to learn about the structure of a domain — remains a difﬁcult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (“PredNet”) architecture that is inspired by the concept of “predictive coding” from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
	language = {en},
	author = {Lotter, William and Kreiman, Gabriel and Cox, David},
	year = {2017},
	pages = {18},
}

@incollection{calders_knowledge-powered_2014,
	address = {Berlin, Heidelberg},
	title = {Knowledge-{Powered} {Deep} {Learning} for {Word} {Embedding}},
	volume = {8724},
	isbn = {978-3-662-44847-2 978-3-662-44848-9},
	url = {http://link.springer.com/10.1007/978-3-662-44848-9_9},
	abstract = {The basis of applying deep learning to solve natural language processing tasks is to obtain high-quality distributed representations of words, i.e., word embeddings, from large amounts of text data. However, text itself usually contains incomplete and ambiguous information, which makes necessity to leverage extra knowledge to understand it. Fortunately, text itself already contains well-deﬁned morphological and syntactic knowledge; moreover, the large amount of texts on the Web enable the extraction of plenty of semantic knowledge. Therefore, it makes sense to design novel deep learning algorithms and systems in order to leverage the above knowledge to compute more effective word embeddings. In this paper, we conduct an empirical study on the capacity of leveraging morphological, syntactic, and semantic knowledge to achieve high-quality word embeddings. Our study explores these types of knowledge to deﬁne new basis for word representation, provide additional input information, and serve as auxiliary supervision in deep learning, respectively. Experiments on an analogical reasoning task, a word similarity task, and a word completion task have all demonstrated that knowledge-powered deep learning can enhance the effectiveness of word embedding.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bian, Jiang and Gao, Bin and Liu, Tie-Yan},
	editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
	year = {2014},
	doi = {10.1007/978-3-662-44848-9_9},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {132--148},
}

@incollection{hutchison_visual_2010,
	address = {Berlin, Heidelberg},
	title = {Visual {Recognition} with {Humans} in the {Loop}},
	volume = {6314},
	isbn = {978-3-642-15560-4 978-3-642-15561-1},
	url = {http://link.springer.com/10.1007/978-3-642-15561-1_32},
	abstract = {We present an interactive, hybrid human-computer method for object classiﬁcation. The method applies to classes of objects that are recognizable by people with appropriate expertise (e.g., animal species or airplane model), but not (in general) by people without such expertise. It can be seen as a visual version of the 20 questions game, where questions based on simple visual attributes are posed interactively. The goal is to identify the true class while minimizing the number of questions asked, using the visual content of the image. We introduce a general framework for incorporating almost any oﬀ-the-shelf multi-class object recognition algorithm into the visual 20 questions game, and provide methodologies to account for imperfect user responses and unreliable computer vision algorithms. We evaluate our methods on Birds-200, a diﬃcult dataset of 200 tightly-related bird species, and on the Animals With Attributes dataset. Our results demonstrate that incorporating user input drives up recognition accuracy to levels that are good enough for practical applications, while at the same time, computer vision reduces the amount of human interaction required.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Computer {Vision} – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Branson, Steve and Wah, Catherine and Schroff, Florian and Babenko, Boris and Welinder, Peter and Perona, Pietro and Belongie, Serge},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	year = {2010},
	doi = {10.1007/978-3-642-15561-1_32},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {438--451},
}

@inproceedings{wagner_interpretable_2019,
	address = {Long Beach, CA, USA},
	title = {Interpretable and {Fine}-{Grained} {Visual} {Explanations} for {Convolutional} {Neural} {Networks}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953597/},
	doi = {10.1109/CVPR.2019.00931},
	abstract = {To verify and validate networks, it is essential to gain insight into their decisions, limitations as well as possible shortcomings of training data. In this work, we propose a post-hoc, optimization based visual explanation method, which highlights the evidence in the input image for a speciﬁc prediction. Our approach is based on a novel technique to defend against adversarial evidence (i.e. faulty evidence due to artefacts) by ﬁltering gradients during optimization. The defense does not depend on human-tuned parameters. It enables explanations which are both ﬁne-grained and preserve the characteristics of images, such as edges and colors. The explanations are interpretable, suited for visualizing detailed evidence and can be tested as they are valid model inputs. We qualitatively and quantitatively evaluate our approach on a multitude of models and datasets.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wagner, Jorg and Kohler, Jan Mathias and Gindele, Tobias and Hetzel, Leon and Wiedemer, Jakob Thaddaus and Behnke, Sven},
	month = jun,
	year = {2019},
	pages = {9089--9099},
}

@inproceedings{donahue_annotator_2011,
	address = {Barcelona, Spain},
	title = {Annotator rationales for visual recognition},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	url = {http://ieeexplore.ieee.org/document/6126394/},
	doi = {10.1109/ICCV.2011.6126394},
	abstract = {Traditional supervised visual learning simply asks annotators “what” label an image should have. We propose an approach for image classiﬁcation problems requiring subjective judgment that also asks “why”, and uses that information to enrich the learned model. We develop two forms of visual annotator rationales: in the ﬁrst, the annotator highlights the spatial region of interest he found most inﬂuential to the label selected, and in the second, he comments on the visual attributes that were most important. For either case, we show how to map the response to synthetic contrast examples, and then exploit an existing large-margin learning technique to reﬁne the decision boundary accordingly. Results on multiple scene categorization and human attractiveness tasks show the promise of our approach, which can more accurately learn complex categories with the explanations behind the label choices.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Donahue, Jeff and Grauman, Kristen},
	month = nov,
	year = {2011},
	pages = {1395--1402},
}

@inproceedings{branson_strong_2011,
	address = {Barcelona, Spain},
	title = {Strong supervision from weak annotation: {Interactive} training of deformable part models},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	shorttitle = {Strong supervision from weak annotation},
	url = {http://ieeexplore.ieee.org/document/6126450/},
	doi = {10.1109/ICCV.2011.6126450},
	abstract = {We propose a framework for large scale learning and annotation of structured models. The system interleaves interactive labeling (where the current model is used to semiautomate the labeling of a new example) and online learning (where a newly labeled example is used to update the current model parameters). This framework is scalable to large datasets and complex image models and is shown to have excellent theoretical and practical properties in terms of train time, optimality guarantees, and bounds on the amount of annotation effort per image. We apply this framework to part-based detection, and introduce a novel algorithm for interactive labeling of deformable part models. The labeling tool updates and displays in real-time the maximum likelihood location of all parts as the user clicks and drags the location of one or more parts. We demonstrate that the system can be used to efﬁciently and robustly train part and pose detectors on the CUB Birds-200–a challenging dataset of birds in unconstrained pose and environment.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Branson, Steve and Perona, Pietro and Belongie, Serge},
	month = nov,
	year = {2011},
	pages = {1832--1839},
}

@inproceedings{deng_fine-grained_2013,
	address = {Portland, OR, USA},
	title = {Fine-{Grained} {Crowdsourcing} for {Fine}-{Grained} {Recognition}},
	isbn = {978-0-7695-4989-7},
	url = {http://ieeexplore.ieee.org/document/6618925/},
	doi = {10.1109/CVPR.2013.81},
	abstract = {Fine-grained recognition concerns categorization at sub-ordinate levels, where the distinction between object classes is highly local. Compared to basic level recognition, ﬁne-grained categorization can be more challenging as there are in general less data and fewer discriminative features. This necessitates the use of stronger prior for feature selection. In this work, we include humans in the loop to help computers select discriminative features. We introduce a novel online game called “Bubbles” that reveals discriminative features humans use. The player’s goal is to identify the category of a heavily blurred image. During the game, the player can choose to reveal full details of circular regions (“bubbles”), with a certain penalty. With proper setup the game generates discriminative bubbles with assured quality. We next propose the “BubbleBank” algorithm that uses the human selected bubbles to improve machine recognition performance. Experiments demonstrate that our approach yields large improvements over the previous state of the art on challenging benchmarks.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Krause, Jonathan and Fei-Fei, Li},
	month = jun,
	year = {2013},
	pages = {580--587},
}

@inproceedings{kun_duan_discovering_2012,
	address = {Providence, RI},
	title = {Discovering localized attributes for fine-grained recognition},
	isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
	url = {http://ieeexplore.ieee.org/document/6248089/},
	doi = {10.1109/CVPR.2012.6248089},
	abstract = {Attributes are visual concepts that can be detected by machines, understood by humans, and shared across categories. They are particularly useful for ﬁne-grained domains where categories are closely related to one other (e.g. bird species recognition). In such scenarios, relevant attributes are often local (e.g. “white belly”), but the question of how to choose these local attributes remains largely unexplored. In this paper, we propose an interactive approach that discovers local attributes that are both discriminative and semantically meaningful from image datasets annotated only with ﬁne-grained category labels and object bounding boxes. Our approach uses a latent conditional random ﬁeld model to discover candidate attributes that are detectable and discriminative, and then employs a recommender system that selects attributes likely to be semantically meaningful. Human interaction is used to provide semantic names for the discovered attributes. We demonstrate our method on two challenging datasets, CaltechUCSD Birds-200-2011 and Leeds Butterﬂies, and ﬁnd that our discovered attributes outperform those generated by traditional approaches.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {{Kun Duan} and Parikh, D. and Crandall, D. and Grauman, K.},
	month = jun,
	year = {2012},
	pages = {3474--3481},
}

@article{visin_renet_2015,
	title = {{ReNet}: {A} {Recurrent} {Neural} {Network} {Based} {Alternative} to {Convolutional} {Networks}},
	shorttitle = {{ReNet}},
	url = {http://arxiv.org/abs/1505.00393},
	abstract = {In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed.},
	language = {en},
	urldate = {2020-06-23},
	journal = {arXiv:1505.00393 [cs]},
	author = {Visin, Francesco and Kastner, Kyle and Cho, Kyunghyun and Matteucci, Matteo and Courville, Aaron and Bengio, Yoshua},
	month = jul,
	year = {2015},
	note = {arXiv: 1505.00393},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kriegeskorte_deep_2015,
	title = {Deep {Neural} {Networks}: {A} {New} {Framework} for {Modeling} {Biological} {Vision} and {Brain} {Information} {Processing}},
	volume = {1},
	issn = {2374-4642, 2374-4650},
	shorttitle = {Deep {Neural} {Networks}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-vision-082114-035447},
	doi = {10.1146/annurev-vision-082114-035447},
	abstract = {Recent advances in neural network modeling have enabled major strides in computer vision and other artiﬁcial intelligence applications. Human-level visual recognition abilities are coming within reach of artiﬁcial systems. Artiﬁcial neural networks are inspired by the brain, and their computations could be implemented in biological neurons. Convolutional feedforward networks, which now dominate computer vision, take further inspiration from the architecture of the primate visual hierarchy. However, the current models are designed with engineering goals, not to model brain computations. Nevertheless, initial studies comparing internal representations between these models and primate brains ﬁnd surprisingly similar representational spaces. With human-level performance no longer out of reach, we are entering an exciting new era, in which we will be able to build biologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence, including vision.},
	language = {en},
	number = {1},
	urldate = {2020-06-23},
	journal = {Annual Review of Vision Science},
	author = {Kriegeskorte, Nikolaus},
	month = nov,
	year = {2015},
	pages = {417--446},
}

@article{liang_incorporating_2016,
	title = {Incorporating image priors with deep convolutional neural networks for image super-resolution},
	volume = {194},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231216002836},
	doi = {10.1016/j.neucom.2016.02.046},
	abstract = {Deep convolutional neural network has been applied for single image super-resolution problem and demonstrated state-of-the-art quality. This paper presents several prior information that could be utilized during the training process of the deep convolutional neural network. The ﬁrst type of prior focuses on edges and texture restoration in the output, and the second type of prior utilizes multiple upscaling factors to consider the structure recurrence across different scales. As demonstrated by our experimental results, the proposed framework could signiﬁcantly accelerate the training speed for more than ten times and at the same time lead to better image quality. The generated super-resolution image achieves visually sharper and more pleasant restoration as well as superior objectively evaluation results compared to state-of-the-art methods.},
	language = {en},
	urldate = {2020-06-23},
	journal = {Neurocomputing},
	author = {Liang, Yudong and Wang, Jinjun and Zhou, Sanping and Gong, Yihong and Zheng, Nanning},
	month = jun,
	year = {2016},
	pages = {340--347},
}

@article{ghosh_incorporating_2016,
	title = {Incorporating priors for medical image segmentation using a genetic algorithm},
	volume = {195},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231216001065},
	doi = {10.1016/j.neucom.2015.09.123},
	abstract = {Medical image segmentation is typically performed manually by a physician to delineate gross tumor volumes for treatment planning and diagnosis. Manual segmentation is performed by medical experts using prior knowledge of organ shapes and locations but is prone to reader subjectivity and inconsistency. Automating the process is challenging due to poor tissue contrast and ill-deﬁned organ/tissue boundaries in medical images. This paper presents a genetic algorithm for combining representations of learned information such as known shapes, regional properties and relative position of objects into a single framework to perform automated three-dimensional segmentation. The algorithm has been tested for prostate segmentation on pelvic computed tomography and magnetic resonance images.},
	language = {en},
	urldate = {2020-06-23},
	journal = {Neurocomputing},
	author = {Ghosh, Payel and Mitchell, Melanie and Tanyi, James A. and Hung, Arthur Y.},
	month = jun,
	year = {2016},
	pages = {181--194},
}

@article{sinha_curriculum_2020,
	title = {Curriculum {By} {Texture}},
	url = {http://arxiv.org/abs/2003.01367},
	abstract = {Convolutional Neural Networks (CNNs) have shown impressive performance in computer vision tasks such as image classiﬁcation and segmentation. One factor for the success of CNNs is that they have an inductive bias that assumes a certain type of spatial structure is present in the data. Recent work by Geirhos et al. (2018) shows how learning in CNNs causes the learned CNN models to be biased towards high-frequency textural information, compared to low-frequency shape information in images. Many tasks generally requires both shape and textural information. Hence, we propose a simple curriculum based scheme which improves the ability of CNNs to be less biased towards textural information, and at the same time, being able to represent both the shape and textural information. We propose to augment the training of CNNs by controlling the amount of textural information that is available to the CNNs during the training process, by convolving the output of a CNN layer with a low-pass ﬁlter, or simply a Gaussian kernel. By reducing the standard deviation of the Gaussian kernel, we are able to gradually increase the amount of textural information available as training progresses, and hence reduce the texture bias. Such an augmented training scheme significantly improves the performance of CNNs on various image classiﬁcation tasks, while adding no additional trainable parameters or auxiliary regularization objectives. We also observe signiﬁcant improvements when using the trained CNNs to perform transfer learning on a different dataset, and transferring to a different task which shows how the learned CNNs using the proposed method act as better feature extractors.},
	language = {en},
	urldate = {2020-06-23},
	journal = {arXiv:2003.01367 [cs, stat]},
	author = {Sinha, Samarth and Garg, Animesh and Larochelle, Hugo},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.01367},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{dharmaretnam_emergence_2018,
	address = {New Orleans, Louisiana},
	title = {The {Emergence} of {Semantics} in {Neural} {Network} {Representations} of {Visual} {Information}},
	url = {http://aclweb.org/anthology/N18-2122},
	doi = {10.18653/v1/N18-2122},
	abstract = {Word vector models learn about semantics through corpora. Convolutional Neural Networks (CNNs) can learn about semantics through images. At the most abstract level, some of the information in these models must be shared, as they model the same real-world phenomena. Here we employ techniques previously used to detect semantic representations in the human brain to detect semantic representations in CNNs. We show the accumulation of semantic information in the layers of the CNN, and discover that, for misclassiﬁed images, the correct class can be recovered in intermediate layers of a CNN.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of           the {Association} for {Computational} {Linguistics}: {Human} {Language}           {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dharmaretnam, Dhanush and Fyshe, Alona},
	year = {2018},
	pages = {776--780},
}

@article{baker_deep_2018,
	title = {Deep convolutional networks do not classify based on global object shape},
	volume = {14},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1006613},
	doi = {10.1371/journal.pcbi.1006613},
	abstract = {Deep convolutional networks (DCNNs) are achieving previously unseen performance in object classification, raising questions about whether DCNNs operate similarly to human vision. In biological vision, shape is arguably the most important cue for recognition. We tested the role of shape information in DCNNs trained to recognize objects. In Experiment 1, we presented a trained DCNN with object silhouettes that preserved overall shape but were filled with surface texture taken from other objects. Shape cues appeared to play some role in the classification of artifacts, but little or none for animals. In Experiments 2–4, DCNNs showed no ability to classify glass figurines or outlines but correctly classified some silhouettes. Aspects of these results led us to hypothesize that DCNNs do not distinguish object’s bounding contours from other edges, and that DCNNs access some local shape features, but not global shape. In Experiment 5, we tested this hypothesis with displays that preserved local features but disrupted global shape, and vice versa. With disrupted global shape, which reduced human accuracy to 28\%, DCNNs gave the same classification labels as with ordinary shapes. Conversely, local contour changes eliminated accurate DCNN classification but caused no difficulty for human observers. These results provide evidence that DCNNs have access to some local shape information in the form of local edge relations, but they have no access to global object shapes.},
	language = {en},
	number = {12},
	urldate = {2020-06-23},
	journal = {PLOS Computational Biology},
	author = {Baker, Nicholas and Lu, Hongjing and Erlikhman, Gennady and Kellman, Philip J.},
	editor = {Einhäuser, Wolfgang},
	month = dec,
	year = {2018},
	pages = {e1006613},
}

@inproceedings{kortylewski_combining_2020,
	address = {Snowmass Village, CO, USA},
	title = {Combining {Compositional} {Models} and {Deep} {Networks} {For} {Robust} {Object} {Classification} under {Occlusion}},
	isbn = {978-1-72816-553-0},
	url = {https://ieeexplore.ieee.org/document/9093560/},
	doi = {10.1109/WACV45572.2020.9093560},
	abstract = {Deep convolutional neural networks (DCNNs) are powerful models that yield impressive results at object classiﬁcation. However, recent work has shown that they do not generalize well to partially occluded objects and to mask attacks. In contrast to DCNNs, compositional models are robust to partial occlusion, however, they are not as discriminative as deep models. In this work, we combine DCNNs and compositional object models to retain the best of both approaches: a discriminative model that is robust to partial occlusion and mask attacks. Our model is learned in two steps. First, a standard DCNN is trained for image classiﬁcation. Subsequently, we cluster the DCNN features into dictionaries. We show that the dictionary components resemble object part detectors and learn the spatial distribution of parts for each object class. We propose mixtures of compositional models to account for large changes in the spatial activation patterns (e.g. due to changes in the 3D pose of an object). At runtime, an image is ﬁrst classiﬁed by the DCNN in a feedforward manner. The prediction uncertainty is used to detect partially occluded objects, which in turn are classiﬁed by the compositional model. Our experimental results demonstrate that combining compositional models and DCNNs resolves a fundamental problem of current deep learning approaches to computer vision: The combined model recognizes occluded objects, even when it has not been exposed to occluded objects during training, while at the same time maintaining high discriminative performance for non-occluded objects.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Kortylewski, Adam and Liu, Qing and Wang, Huiyu and Zhang, Zhishuai and Yuille, Alan},
	month = mar,
	year = {2020},
	pages = {1322--1330},
}

@inproceedings{wang_detecting_2017,
	address = {London, UK},
	title = {Detecting {Semantic} {Parts} on {Partially} {Occluded} {Objects}},
	isbn = {978-1-901725-60-5},
	url = {http://www.bmva.org/bmvc/2017/papers/paper073/index.html},
	doi = {10.5244/C.31.73},
	abstract = {In this paper, we address the task of detecting semantic parts on partially occluded objects. We consider a scenario where the model is trained using non-occluded images but tested on occluded images. The motivation is that there are inﬁnite number of occlusion patterns in real world, which cannot be fully covered in the training data. So the models should be inherently robust and adaptive to occlusions instead of ﬁtting / learning the occlusion patterns in the training data. Our approach detects semantic parts by accumulating the conﬁdence of local visual cues. Speciﬁcally, the method uses a simple voting method, based on log-likelihood ratio tests and spatial constraints, to combine the evidence of local cues. These cues are called visual concepts, which are derived by clustering the internal states of deep networks. We evaluate our voting scheme on the VehicleSemanticPart dataset with dense part annotations. We randomly place two, three or four irrelevant objects onto the target object to generate testing images with various occlusions. Experiments show that our algorithm outperforms several competitors in semantic part detection when occlusions are present.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2017},
	publisher = {British Machine Vision Association},
	author = {Wang, Jianyu and Zhang, Zhishuai and Xie, Cihang and Zhu, Jun and Xie, Lingxi and Yuille, Alan},
	year = {2017},
	pages = {73},
}

@article{villalobos_neural_nodate,
	title = {Do {Neural} {Networks} for {Segmentation} {Understand} {Insideness}?},
	abstract = {The insideness problem is an image segmentation modality that consists of determining which pixels are inside and outside a region. Deep Neural Networks (DNNs) excel in segmentation benchmarks, but it is unclear that they have the ability to solve the insideness problem as it requires evaluating longrange spatial dependencies. In this paper, the insideness problem is analysed in isolation, without texture or semantic cues, such that other aspects of segmentation do not interfere in the analysis. We demonstrate that DNNs for segmentation with few units have sufﬁcient complexity to solve insideness for any curve. Yet, such DNNs have severe problems to learn general solutions. Only recurrent networks trained with small images learn solutions that generalize well to almost any curve. Recurrent networks can decompose the evaluation of long-range dependencies into a sequence of local operations, and learning with small images alleviates the common difﬁculties of training recurrent networks with a large number of unrolling steps.},
	language = {en},
	author = {Villalobos, Kimberly and Štih, Vilim and Ahmadinejad, Amineh and Sundaram, Shobhita and Dozier, Jamell and Francl, Andrew and Azevedo, Frederico and Sasaki, Tomotake and Boix, Xavier},
	pages = {25},
}

@article{lindsay_feature-based_2015,
	title = {Feature-based {Attention} in {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06408},
	abstract = {Convolutional neural networks (CNNs) have proven effective for image processing tasks, such as object recognition and classification. Recently, CNNs have been enhanced with concepts of attention, similar to those found in biology. Much of this work on attention has focused on effective serial spatial processing. In this paper, I introduce a simple procedure for applying feature-based attention (FBA) to CNNs and compare multiple implementation options. FBA is a top-down signal applied globally to an input image which aides in detecting chosen objects in cluttered or noisy settings. The concept of FBA and the implementation details tested here were derived from what is known (and debated) about biological object- and feature-based attention. The implementations of FBA described here increase performance on challenging object detection tasks using a procedure that is simple, fast, and does not require additional iterative training. Furthermore, the comparisons performed here suggest that a proposed model of biological FBA (the "feature similarity gain model") is effective in increasing performance.},
	language = {en},
	urldate = {2020-06-23},
	journal = {arXiv:1511.06408 [cs]},
	author = {Lindsay, Grace W.},
	month = dec,
	year = {2015},
	note = {arXiv: 1511.06408},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{mottaghi_role_2014,
	title = {The {Role} of {Context} for {Object} {Detection} and {Semantic} {Segmentation} in the {Wild}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Mottaghi_The_Role_of_2014_CVPR_paper.html},
	urldate = {2020-06-23},
	author = {Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
	year = {2014},
	pages = {891--898},
}

@inproceedings{zhang_making_2019,
	title = {Making {Convolutional} {Networks} {Shift}-{Invariant} {Again}},
	url = {http://proceedings.mlr.press/v97/zhang19a.html},
	abstract = {Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, stride...},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Zhang, Richard},
	month = may,
	year = {2019},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {7324--7334},
}

@article{azulay_why_2019,
	title = {Why do deep convolutional networks generalize so poorly to small image transformations?},
	volume = {20},
	url = {http://jmlr.org/papers/v20/19-519.html},
	abstract = {Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network’s prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are suﬃcient to achieve the desired invariance. Speciﬁcally, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.},
	number = {184},
	journal = {Journal of Machine Learning Research},
	author = {Azulay, Aharon and Weiss, Yair},
	year = {2019},
	pages = {1--25},
}

@inproceedings{nayebi_task-driven_2018,
	title = {Task-{Driven} {Convolutional} {Recurrent} {Models} of the {Visual} {System}},
	abstract = {Feed-forward convolutional neural networks (CNNs) are currently state-of-the-art for object classiﬁcation tasks such as ImageNet. Further, they are quantitatively accurate models of temporally-averaged responses of neurons in the primate brain’s visual system. However, biological visual systems have two ubiquitous architectural features not shared with typical CNNs: local recurrence within cortical areas, and long-range feedback from downstream areas to upstream areas. Here we explored the role of recurrence in improving classiﬁcation performance. We found that standard forms of recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the ImageNet task. In contrast, novel cells that incorporated two structural features, bypassing and gating, were able to boost task accuracy substantially. We extended these design principles in an automated search over thousands of model architectures, which identiﬁed novel local recurrent cells and long-range feedback connections useful for object recognition. Moreover, these task-optimized ConvRNNs matched the dynamics of neural activity in the primate visual system better than feedforward networks, suggesting a role for the brain’s recurrent connections in performing difﬁcult visual behaviors.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nayebi, Aran and Bear, Daniel and Kubilius, Jonas and Kar, Kohitij and Ganguli, Surya and Sussillo, David and DiCarlo, James J and Yamins, Daniel L},
	year = {2018},
	pages = {12},
}

@inproceedings{zamir_feedback_2017,
	title = {Feedback {Networks}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Zamir_Feedback_Networks_CVPR_2017_paper.html},
	urldate = {2020-06-23},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zamir, Amir R. and Wu, Te-Lin and Sun, Lin and Shen, William B. and Shi, Bertram E. and Malik, Jitendra and Savarese, Silvio},
	year = {2017},
	pages = {1308--1317},
}

@article{gilbert_top-down_2013,
	title = {Top-down influences on visual processing},
	volume = {14},
	copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn3476},
	doi = {10.1038/nrn3476},
	abstract = {Vision is an active process. Higher-order cognitive influences, including attention, expectation and perceptual task, as well as motor signals, are fed into the sensory apparatus. This enables neurons to dynamically tune their receptive field properties to carry information that is relevant for executing the current behavioural tasks.},
	language = {en},
	number = {5},
	urldate = {2020-06-23},
	journal = {Nature Reviews Neuroscience},
	author = {Gilbert, Charles D. and Li, Wu},
	month = may,
	year = {2013},
	note = {Number: 5
Publisher: Nature Publishing Group},
	pages = {350--363},
}

@article{rajaei_beyond_2019,
	title = {Beyond core object recognition: {Recurrent} processes account for object recognition under occlusion},
	volume = {15},
	issn = {1553-7358},
	shorttitle = {Beyond core object recognition},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007001},
	doi = {10.1371/journal.pcbi.1007001},
	abstract = {Core object recognition, the ability to rapidly recognize objects despite variations in their appearance, is largely solved through the feedforward processing of visual information. Deep neural networks are shown to achieve human-level performance in these tasks, and explain the primate brain representation. On the other hand, object recognition under more challenging conditions (i.e. beyond the core recognition problem) is less characterized. One such example is object recognition under occlusion. It is unclear to what extent feedforward and recurrent processes contribute in object recognition under occlusion. Furthermore, we do not know whether the conventional deep neural networks, such as AlexNet, which were shown to be successful in solving core object recognition, can perform similarly well in problems that go beyond the core recognition. Here, we characterize neural dynamics of object recognition under occlusion, using magnetoencephalography (MEG), while participants were presented with images of objects with various levels of occlusion. We provide evidence from multivariate analysis of MEG data, behavioral data, and computational modelling, demonstrating an essential role for recurrent processes in object recognition under occlusion. Furthermore, the computational model with local recurrent connections, used here, suggests a mechanistic explanation of how the human brain might be solving this problem.},
	language = {en},
	number = {5},
	urldate = {2020-06-23},
	journal = {PLOS Computational Biology},
	author = {Rajaei, Karim and Mohsenzadeh, Yalda and Ebrahimpour, Reza and Khaligh-Razavi, Seyed-Mahdi},
	month = may,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Human performance, Information processing, Magnetoencephalography, Recurrent neural networks, Support vector machines, Vision, Visual object recognition},
	pages = {e1007001},
}

@article{rajalingham_large-scale_2018,
	title = {Large-{Scale}, {High}-{Resolution} {Comparison} of the {Core} {Visual} {Object} {Recognition} {Behavior} of {Humans}, {Monkeys}, and {State}-of-the-{Art} {Deep} {Artificial} {Neural} {Networks}},
	volume = {38},
	copyright = {Copyright © 2018 the authors 0270-6474/18/387255-15\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/38/33/7255},
	doi = {10.1523/JNEUROSCI.0388-18.2018},
	abstract = {Primates, including humans, can typically recognize objects in visual images at a glance despite naturally occurring identity-preserving image transformations (e.g., changes in viewpoint). A primary neuroscience goal is to uncover neuron-level mechanistic models that quantitatively explain this behavior by predicting primate performance for each and every image. Here, we applied this stringent behavioral prediction test to the leading mechanistic models of primate vision (specifically, deep, convolutional, artificial neural networks; ANNs) by directly comparing their behavioral signatures against those of humans and rhesus macaque monkeys. Using high-throughput data collection systems for human and monkey psychophysics, we collected more than one million behavioral trials from 1472 anonymous humans and five male macaque monkeys for 2400 images over 276 binary object discrimination tasks. Consistent with previous work, we observed that state-of-the-art deep, feedforward convolutional ANNs trained for visual categorization (termed DCNNIC models) accurately predicted primate patterns of object-level confusion. However, when we examined behavioral performance for individual images within each object discrimination task, we found that all tested DCNNIC models were significantly nonpredictive of primate performance and that this prediction failure was not accounted for by simple image attributes nor rescued by simple model modifications. These results show that current DCNNIC models cannot account for the image-level behavioral patterns of primates and that new ANN models are needed to more precisely capture the neural mechanisms underlying primate object vision. To this end, large-scale, high-resolution primate behavioral benchmarks such as those obtained here could serve as direct guides for discovering such models.
SIGNIFICANCE STATEMENT Recently, specific feedforward deep convolutional artificial neural networks (ANNs) models have dramatically advanced our quantitative understanding of the neural mechanisms underlying primate core object recognition. In this work, we tested the limits of those ANNs by systematically comparing the behavioral responses of these models with the behavioral responses of humans and monkeys at the resolution of individual images. Using these high-resolution metrics, we found that all tested ANN models significantly diverged from primate behavior. Going forward, these high-resolution, large-scale primate behavioral benchmarks could serve as direct guides for discovering better ANN models of the primate visual system.},
	language = {en},
	number = {33},
	urldate = {2020-06-23},
	journal = {Journal of Neuroscience},
	author = {Rajalingham, Rishi and Issa, Elias B. and Bashivan, Pouya and Kar, Kohitij and Schmidt, Kailyn and DiCarlo, James J.},
	month = aug,
	year = {2018},
	pmid = {30006365},
	note = {Publisher: Society for Neuroscience
Section: Research Articles},
	keywords = {deep neural network, human, monkey, object recognition, vision},
	pages = {7255--7269},
}

@article{dicarlo_how_2012,
	title = {How {Does} the {Brain} {Solve} {Visual} {Object} {Recognition}?},
	volume = {73},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S089662731200092X},
	doi = {10.1016/j.neuron.2012.01.010},
	abstract = {Mounting evidence suggests that ‘core object recognition,’ the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. However, the algorithm that produces this solution remains poorly understood. Here we review evidence ranging from individual neurons and neuronal populations to behavior and computational models. We propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical subnetworks with a common functional goal.},
	language = {en},
	number = {3},
	urldate = {2020-06-23},
	journal = {Neuron},
	author = {DiCarlo, James J. and Zoccolan, Davide and Rust, Nicole C.},
	month = feb,
	year = {2012},
	pages = {415--434},
}

@article{pinto_why_2008,
	title = {Why is {Real}-{World} {Visual} {Object} {Recognition} {Hard}?},
	volume = {4},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0040027},
	doi = {10.1371/journal.pcbi.0040027},
	abstract = {Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain's anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, “natural” images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled “natural” images in guiding that progress. In particular, we show that a simple V1-like model—a neuroscientist's “null” model, which should perform poorly at real-world visual object recognition tasks—outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a “simpler” recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition—real-world image variation.},
	language = {en},
	number = {1},
	urldate = {2020-06-23},
	journal = {PLOS Computational Biology},
	author = {Pinto, Nicolas and Cox, David D. and DiCarlo, James J.},
	month = jan,
	year = {2008},
	note = {Publisher: Public Library of Science},
	keywords = {Grayscale, Human performance, Imaging techniques, Object recognition, Primates, Principal component analysis, Support vector machines, Vision},
	pages = {e27},
}

@inproceedings{koporec_deep_2019,
	address = {Seoul, Korea (South)},
	title = {Deep {Learning} {Performance} in the {Presence} of {Significant} {Occlusions} - {An} {Intelligent} {Household} {Refrigerator} {Case}},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9022381/},
	doi = {10.1109/ICCVW.2019.00310},
	abstract = {Real-world environments, inhabited by people, still pose signiﬁcant challenges to deep learning methods. Object occlusion is one of such problems. Humans deal with the occlusion in a complex way, by changing the viewpoint and using hands to manipulate the scene. However, not all robotic systems can do that due to cost or design constraints. The question we address in this paper is, how well modern object detection methods work on a model case of an intelligent household refrigerator, where numerous occlusions occur. To motivate our research, we actually performed a worldwide survey of refrigerator occupancy to realistically judge the extent of the problem, but the results could be generalized to any unstructured storage environment where people are in charge. The survey results enabled us to generate a dataset of photo-realistic renderings of a typical refrigerator interior, where the object identity, location, and the degree of the refrigerator occupancy are all readily available. Our results are represented as the Average Precision depending on a refrigerator occupancy for two well known deep models.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	publisher = {IEEE},
	author = {Koporec, Gregor and Pers, Janez},
	month = oct,
	year = {2019},
	keywords = {dataset},
	pages = {2532--2540},
}

@inproceedings{geirhos_generalisation_2018,
	title = {Generalisation in humans and deep neural networks},
	abstract = {We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we ﬁnd the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classiﬁcation error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Geirhos, Robert and Temme, Carlos R M and Rauber, Jonas and Schütt, Heiko H and Bethge, Matthias and Wichmann, Felix A},
	year = {2018},
	keywords = {dataset},
	pages = {13},
}

@inproceedings{recht_imagenet_2019,
	title = {Do {ImageNet} {Classiﬁers} {Generalize} to {ImageNet}?},
	abstract = {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overﬁtting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classiﬁcation models generalize to new data. We evaluate a broad range of models and ﬁnd accuracy drops of 3\% – 15\% on CIFAR-10 and 11\% – 14\% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models’ inability to generalize to slightly “harder” images than those found in the original test sets.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	year = {2019},
	pages = {12},
}

@article{li_learning_2018,
	title = {Learning with rethinking: {Recurrently} improving convolutional neural networks through feedback},
	volume = {79},
	issn = {0031-3203},
	shorttitle = {Learning with rethinking},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320318300153},
	doi = {10.1016/j.patcog.2018.01.015},
	abstract = {Recent years have witnessed the great success of convolutional neural network (CNN) based models in the field of computer vision. CNN is able to learn hierarchically abstracted features from images in an end-to-end training manner. However, most of the existing CNN models only learn features through a feedforward structure and no feedback information from top to bottom layers is exploited to enable the networks to refine themselves. In this paper, we propose a Learning with Rethinking algorithm. By adding a feedback layer and producing the emphasis vector, the model is able to recurrently boost the performance based on previous prediction. Particularly, it can be employed to boost any pre-trained models. This algorithm is tested on four object classification benchmark datasets: CIFAR-100, CIFAR-10, MNIST-background-image and ILSVRC-2012 dataset, and the results have demonstrated the advantage of training CNN models with the proposed feedback mechanism.},
	language = {en},
	urldate = {2020-06-23},
	journal = {Pattern Recognition},
	author = {Li, Xin and Jie, Zequn and Feng, Jiashi and Liu, Changsong and Yan, Shuicheng},
	month = jul,
	year = {2018},
	keywords = {Convolutional neural network, Deep learning, Image classification},
	pages = {183--194},
}

@book{poggio_visual_2016,
	title = {Visual {Cortex} and {Deep} {Networks}: {Learning} {Invariant} {Representations}},
	isbn = {978-0-262-03472-2},
	shorttitle = {Visual {Cortex} and {Deep} {Networks}},
	abstract = {A mathematical framework that describes learning of invariant representations in the ventral stream, offering both theoretical development and applications.The ventral visual stream is believed to underlie object recognition in primates. Over the past fifty years, researchers have developed a series of quantitative models that are increasingly faithful to the biological architecture. Recently, deep learning convolution networks—which do not reflect several important features of the ventral stream architecture and physiology—have been trained with extremely large datasets, resulting in model neurons that mimic object recognition but do not explain the nature of the computations carried out in the ventral stream. This book develops a mathematical framework that describes learning of invariant representations of the ventral stream and is particularly relevant to deep convolutional learning networks. The authors propose a theory based on the hypothesis that the main computational goal of the ventral stream is to compute neural representations of images that are invariant to transformations commonly encountered in the visual environment and are learned from unsupervised experience. They describe a general theoretical framework of a computational theory of invariance (with details and proofs offered in appendixes) and then review the application of the theory to the feedforward path of the ventral stream in the primate visual cortex.},
	language = {en},
	publisher = {MIT Press},
	author = {Poggio, Tomaso and Anselmi, Fabio},
	month = sep,
	year = {2016},
	note = {Google-Books-ID: RP8WDQAAQBAJ},
	keywords = {Computers / Intelligence (AI) \& Semantics, Science / Life Sciences / Neuroscience},
}

@article{tramer_space_2017,
	title = {The {Space} of {Transferable} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1704.03453},
	abstract = {Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.},
	language = {en},
	urldate = {2020-06-15},
	journal = {arXiv:1704.03453 [cs, stat]},
	author = {Tramèr, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
	month = may,
	year = {2017},
	keywords = {Computer Science - Cryptography and Security, Statistics - Machine Learning},
}

@inproceedings{lipton_mythos_2017,
	title = {The {Mythos} of {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspeciﬁed. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to reﬁne the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, ﬁnding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {{ICML} {Workshop} on {Human} {Interpretability} in {Machine} {Learning}},
	author = {Lipton, Zachary C.},
	year = {2017},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{garcez_neural-symbolic_2019,
	title = {Neural-{Symbolic} {Computing}: {An} {Effective} {Methodology} for {Principled} {Integration} of {Machine} {Learning} and {Reasoning}},
	volume = {6},
	shorttitle = {Neural-{Symbolic} {Computing}},
	url = {http://arxiv.org/abs/1905.06088},
	abstract = {Current advances in Artiﬁcial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by inﬂuential thinkers. In spite of the recent impact of AI, several works have identiﬁed the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the eﬀectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.},
	language = {en},
	number = {4},
	urldate = {2020-06-14},
	journal = {Journal of Applied Logics},
	author = {Garcez, Artur d'Avila and Gori, Marco and Lamb, Luis C. and Serafini, Luciano and Spranger, Michael and Tran, Son N.},
	month = may,
	year = {2019},
	pages = {611--631},
}

@incollection{escalante_considerations_2018,
	title = {Considerations for {Evaluation} and {Generalization} in {Interpretable} {Machine} {Learning}},
	isbn = {978-3-319-98130-7 978-3-319-98131-4},
	url = {http://link.springer.com/10.1007/978-3-319-98131-4_1},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Explainable and {Interpretable} {Models} in {Computer} {Vision} and {Machine} {Learning}},
	publisher = {Springer International Publishing},
	author = {Doshi-Velez, Finale and Kim, Been},
	editor = {Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle and Baró, Xavier and Güçlütürk, Yağmur and Güçlü, Umut and van Gerven, Marcel},
	year = {2018},
	doi = {10.1007/978-3-319-98131-4_1},
	note = {Series Title: The Springer Series on Challenges in Machine Learning},
	pages = {3--17},
}

@inproceedings{ribeiro_why_2016,
	address = {San Francisco, California, USA},
	series = {{KDD} '16},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://doi.org/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2020-06-20},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
	pages = {1135--1144},
}

@inproceedings{fleet_visualizing_2014,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	volume = {8689},
	isbn = {978-3-319-10589-5 978-3-319-10590-1},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classiﬁcation performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classiﬁer. Used in a diagnostic role, these visualizations allow us to ﬁnd model architectures that outperform Krizhevsky et al. on the ImageNet classiﬁcation benchmark. We also perform an ablation study to discover the performance contribution from diﬀerent model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classiﬁer is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	pages = {818--833},
}

@inproceedings{koh_understanding_2017,
	title = {Understanding {Black}-box {Predictions} via {Influence} {Functions}},
	url = {http://arxiv.org/abs/1703.04730},
	abstract = {How can we explain the predictions of a blackbox model? In this paper, we use inﬂuence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up inﬂuence functions to modern machine learning settings, we develop a simple, efﬁcient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to inﬂuence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that inﬂuence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visuallyindistinguishable training-set attacks.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	author = {Koh, Pang Wei and Liang, Percy},
	year = {2017},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{nguyen_synthesizing_2016,
	title = {Synthesizing the preferred inputs for neurons in neural networks via deep generator networks},
	abstract = {Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classiﬁcation problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right—similar to why we study the human brain—and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
	year = {2016},
	pages = {9},
}

@inproceedings{springenberg_striving_2015,
	title = {Striving for {Simplicity}: {The} {All} {Convolutional} {Net}},
	shorttitle = {Striving for {Simplicity}},
	url = {http://arxiv.org/abs/1412.6806},
	abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We ﬁnd that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this ﬁnding – and building on other recent work for ﬁnding simple network structures – we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the “deconvolution approach” for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	year = {2015},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{smilkov_smoothgrad_2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	shorttitle = {{SmoothGrad}},
	url = {http://arxiv.org/abs/1706.03825},
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classiﬁer, one type of explanation is to identify pixels that strongly inﬂuence the ﬁnal decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SMOOTHGRAD, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {{ICML} {Workshop} on {Visualization} for {Deep} {Learning}},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	year = {2017},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{bau_network_2017,
	title = {Network {Dissection}: {Quantifying} {Interpretability} of {Deep} {Visual} {Representations}},
	shorttitle = {Network {Dissection}},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.html},
	urldate = {2020-06-21},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	year = {2017},
	pages = {6541--6549},
}

@inproceedings{shrikumar_learning_2017,
	title = {Learning {Important} {Features} {Through} {Propagating} {Activation} {Differences}},
	url = {http://proceedings.mlr.press/v70/shrikumar17a.html},
	abstract = {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a meth...},
	language = {en},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	month = jul,
	year = {2017},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {3145--3153},
}

@article{zhou_interpreting_2019,
	title = {Interpreting {Deep} {Visual} {Representations} via {Network} {Dissection}},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8417924/},
	doi = {10.1109/TPAMI.2018.2858759},
	abstract = {The success of recent deep convolutional neural networks (CNNs) depends on learning hidden representations that can summarize the important factors of variation behind the data. In this work, we describe Network Dissection, a method that interprets networks by providing meaningful labels to their individual units. The proposed method quantiﬁes the interpretability of CNN representations by evaluating the alignment between individual hidden units and visual semantic concepts. By identifying the best alignments, units are given interpretable labels ranging from colors, materials, textures, parts, objects and scenes. The method reveals that deep representations are more transparent and interpretable than they would be under a random equivalently powerful basis. We apply our approach to interpret and compare the latent representations of several network architectures trained to solve a wide range of supervised and self-supervised tasks. We then examine factors affecting the network interpretability such as the number of the training iterations, regularizations, different initialization parameters, as well as networks depth and width. Finally we show that the interpreted units can be used to provide explicit explanations of a given CNN prediction for an image. Our results highlight that interpretability is an important property of deep neural networks that provides new insights into what hierarchical structures can learn.},
	language = {en},
	number = {9},
	urldate = {2020-06-21},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhou, Bolei and Bau, David and Oliva, Aude and Torralba, Antonio},
	month = sep,
	year = {2019},
	pages = {2131--2145},
}

@inproceedings{zhou_learning_2016,
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	year = {2016},
	pages = {2921--2929},
}

@article{baehrens_how_2010,
	title = {How to {Explain} {Individual} {Classification} {Decisions}},
	volume = {11},
	url = {http://jmlr.org/papers/v11/baehrens10a.html},
	abstract = {After building a classiﬁer with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most inﬂuential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classiﬁcation method.},
	number = {61},
	journal = {Journal of Machine Learning Research},
	author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and Müller, Klaus-Robert},
	year = {2010},
	pages = {1803--1831},
}

@inproceedings{selvaraju_grad-cam_2017,
	title = {Grad-{CAM}: {Visual} {Explanations} {From} {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	shorttitle = {Grad-{CAM}},
	url = {http://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	year = {2017},
	pages = {618--626},
}

@inproceedings{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a uniﬁed framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identiﬁcation of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class uniﬁes six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this uniﬁcation, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
	pages = {10},
}

@inproceedings{sundararajan_axiomatic_2017,
	title = {Axiomatic {Attribution} for {Deep} {Networks}},
	url = {http://proceedings.mlr.press/v70/sundararajan17a.html},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms—Sensitivity and I...},
	language = {en},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = jul,
	year = {2017},
	pages = {3319--3328},
}

@inproceedings{hendricks_women_2018,
	address = {Cham},
	title = {Women {Also} {Snowboard}: {Overcoming} {Bias} in {Captioning} {Models}},
	volume = {11207},
	isbn = {978-3-030-01218-2 978-3-030-01219-9},
	shorttitle = {Women {Also} {Snowboard}},
	url = {http://link.springer.com/10.1007/978-3-030-01219-9_47},
	doi = {10.1007/978-3-030-01219-9_47},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and Darrell, Trevor and Rohrbach, Anna},
	year = {2018},
	pages = {793--811},
}

@inproceedings{donadello_logic_2017,
	title = {Logic {Tensor} {Networks} for {Semantic} {Image} {Interpretation}},
	url = {http://arxiv.org/abs/1705.08968},
	abstract = {Semantic Image Interpretation (SII) is the task of extracting structured semantic descriptions from images. It is widely agreed that the combined use of visual data and background knowledge is of great importance for SII. Recently, Statistical Relational Learning (SRL) approaches have been developed for reasoning under uncertainty and learning in the presence of data and rich knowledge. Logic Tensor Networks (LTNs) are an SRL framework which integrates neural networks with ﬁrst-order fuzzy logic to allow (i) efﬁcient learning from noisy data in the presence of logical constraints, and (ii) reasoning with logical formulas describing general properties of the data. In this paper, we develop and apply LTNs to two of the main tasks of SII, namely, the classiﬁcation of an image’s bounding boxes and the detection of the relevant part-of relations between objects. To the best of our knowledge, this is the ﬁrst successful application of SRL to such SII tasks. The proposed approach is evaluated on a standard image processing benchmark. Experiments show that the use of background knowledge in the form of logical constraints can improve the performance of purely data-driven approaches, including the state-of-the-art Fast Region-based Convolutional Neural Networks (Fast R-CNN). Moreover, we show that the use of logical background knowledge adds robustness to the learning system when errors are present in the labels of the training data.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Donadello, Ivan and Serafini, Luciano and Garcez, Artur d'Avila},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08968},
}

@inproceedings{hooker_benchmark_2019,
	title = {A {Benchmark} for {Interpretability} {Methods} in {Deep} {Neural} {Networks}},
	abstract = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classiﬁcation datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches—VarGrad and SmoothGrad-Squared—outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
	year = {2019},
	pages = {12},
}

@inproceedings{alvarez-melis_robustness_2018,
	title = {On the {Robustness} of {Interpretability} {Methods}},
	url = {http://arxiv.org/abs/1806.08049},
	abstract = {We argue that robustness of explanations---i.e., that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {{ICML} {Workshop} on {Human} {Interpretability} in {Machine} {Learning} ({WHI} 2018)},
	author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
	year = {2018},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{strout_human_2019,
	address = {Florence, Italy},
	title = {Do {Human} {Rationales} {Improve} {Machine} {Explanations}?},
	url = {https://www.aclweb.org/anthology/W19-4807},
	doi = {10.18653/v1/W19-4807},
	abstract = {Work on “learning with rationales” shows that humans providing explanations to a machine learning system can improve the system's predictive accuracy. However, this work has not been connected to work in “explainable AI” which concerns machines explaining their reasoning to humans. In this work, we show that learning with rationales can also improve the quality of the machine's explanations as evaluated by human judges. Specifically, we present experiments showing that, for CNN-based text classification, explanations generated using “supervised attention” are judged superior to explanations generated using normal unsupervised attention.},
	urldate = {2020-06-21},
	booktitle = {Proceedings of the 2019 {ACL} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Strout, Julia and Zhang, Ye and Mooney, Raymond},
	month = aug,
	year = {2019},
	pages = {56--62},
}

@article{tang_recurrent_2018,
	title = {Recurrent computations for visual pattern completion},
	volume = {115},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/35/8835},
	doi = {10.1073/pnas.1719397115},
	abstract = {Making inferences from partial information constitutes a critical aspect of cognition. During visual perception, pattern completion enables recognition of poorly visible or occluded objects. We combined psychophysics, physiology, and computational models to test the hypothesis that pattern completion is implemented by recurrent computations and present three pieces of evidence that are consistent with this hypothesis. First, subjects robustly recognized objects even when they were rendered {\textless}15\% visible, but recognition was largely impaired when processing was interrupted by backward masking. Second, invasive physiological responses along the human ventral cortex exhibited visually selective responses to partially visible objects that were delayed compared with whole objects, suggesting the need for additional computations. These physiological delays were correlated with the effects of backward masking. Third, state-of-the-art feed-forward computational architectures were not robust to partial visibility. However, recognition performance was recovered when the model was augmented with attractor-based recurrent connectivity. The recurrent model was able to predict which images of heavily occluded objects were easier or harder for humans to recognize, could capture the effect of introducing a backward mask on recognition behavior, and was consistent with the physiological delays along the human ventral visual stream. These results provide a strong argument of plausibility for the role of recurrent computations in making visual inferences from partial information.},
	language = {en},
	number = {35},
	urldate = {2020-06-21},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Tang, Hanlin and Schrimpf, Martin and Lotter, William and Moerman, Charlotte and Paredes, Ana and Caro, Josue Ortega and Hardesty, Walter and Cox, David and Kreiman, Gabriel},
	month = aug,
	year = {2018},
	pmid = {30104363},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {artificial intelligence, computational neuroscience, machine learning, pattern completion, visual object recognition},
	pages = {8835--8840},
}

@inproceedings{kubilius_brain-like_2019,
	title = {Brain-{Like} {Object} {Recognition} with {High}-{Performing} {Shallow} {Recurrent} {ANNs}},
	url = {http://arxiv.org/abs/1909.06161},
	abstract = {Deep convolutional artiﬁcial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain’s anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional ﬁdelity of models of the primate ventral visual stream. Despite being signiﬁcantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both BrainScore and ImageNet top-1 performance. Finally, we report that the temporal evolution of the CORnet-S "IT" neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Kubilius, Jonas and Schrimpf, Martin and Kar, Kohitij and Rajalingham, Rishi and Hong, Ha and Majaj, Najib J. and Issa, Elias B. and Bashivan, Pouya and Prescott-Roy, Jonathan and Schmidt, Kailyn and Nayebi, Aran and Bear, Daniel and Yamins, Daniel L. K. and DiCarlo, James J.},
	year = {2019},
	keywords = {Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Neurons and Cognition},
}

@article{bengio_representation_2013,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	volume = {35},
	issn = {1939-3539},
	shorttitle = {Representation {Learning}},
	doi = {10.1109/TPAMI.2013.50},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = aug,
	year = {2013},
	keywords = {AI, Abstracts, Algorithms, Artificial Intelligence, Boltzmann machine, Deep learning, Feature extraction, Humans, Learning systems, Machine learning, Manifolds, Neural Networks (Computer), Neural networks, Speech recognition, artificial intelligence, autoencoder, autoencoders, data representation, data structures, density estimation, feature learning, geometrical connections, machine learning algorithms, manifold learning, neural nets, probabilistic models, probability, representation learning, unsupervised feature learning, unsupervised learning},
	pages = {1798--1828},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2020-06-21},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@inproceedings{cremers_part_2015,
	title = {Part {Detector} {Discovery} in {Deep} {Convolutional} {Neural} {Networks}},
	volume = {9004},
	isbn = {978-3-319-16807-4 978-3-319-16808-1},
	url = {http://link.springer.com/10.1007/978-3-319-16808-1_12},
	abstract = {Current ﬁne-grained classiﬁcation approaches often rely on a robust localization of object parts to extract localized feature representations suitable for discrimination. However, part localization is a challenging task due to the large variation of appearance and pose. In this paper, we show how pre-trained convolutional neural networks can be used for robust and eﬃcient object part discovery and localization without the necessity to actually train the network on the current dataset. Our approach called “part detector discovery” (PDD) is based on analyzing the gradient maps of the network outputs and ﬁnding activation centers spatially related to annotated semantic parts or bounding boxes. This allows us not just to obtain excellent performance on the CUB200-2011 dataset, but in contrast to previous approaches also to perform detection and bird classiﬁcation jointly without requiring a given bounding box annotation during testing and ground-truth parts during training.},
	language = {en},
	urldate = {2020-06-21},
	booktitle = {Asian {Conference} on {Computer} {Vision}},
	publisher = {Springer International Publishing},
	author = {Simon, Marcel and Rodner, Erik and Denzler, Joachim},
	editor = {Cremers, Daniel and Reid, Ian and Saito, Hideo and Yang, Ming-Hsuan},
	year = {2015},
	pages = {162--177},
}

@inproceedings{morcos_importance_2018,
	title = {On the importance of single directions for generalization},
	url = {http://arxiv.org/abs/1803.06959},
	abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (deﬁned as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network’s reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodiﬁed labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we ﬁnd that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
	year = {2018},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{zhou_object_2015,
	title = {Object {Detectors} {Emerge} in {Deep} {Scene} {CNNs}},
	url = {http://arxiv.org/abs/1412.6856},
	abstract = {With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classiﬁcation. As scenes are composed of objects, the CNN for scene classiﬁcation automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	year = {2015},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://arxiv.org/abs/1411.1792},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the ﬁrst layer they learn features similar to Gabor ﬁlters and color blobs. Such ﬁrst-layer features appear not to be speciﬁc to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to speciﬁc by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus speciﬁcity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difﬁculties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A ﬁnal surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after ﬁne-tuning to the target dataset.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	year = {2014},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@article{gonzalez-garcia_semantic_2018,
	title = {Do {Semantic} {Parts} {Emerge} in {Convolutional} {Neural} {Networks}?},
	volume = {126},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-017-1048-0},
	doi = {10.1007/s11263-017-1048-0},
	abstract = {Semantic object parts can be useful for several visual recognition tasks. Lately, these tasks have been addressed using Convolutional Neural Networks (CNN), achieving outstanding results. In this work we study whether CNNs learn semantic parts in their internal representation. We investigate the responses of convolutional filters and try to associate their stimuli with semantic parts. We perform two extensive quantitative analyses. First, we use ground-truth part bounding-boxes from the PASCAL-Part dataset to determine how many of those semantic parts emerge in the CNN. We explore this emergence for different layers, network depths, and supervision levels. Second, we collect human judgements in order to study what fraction of all filters systematically fire on any semantic part, even if not annotated in PASCAL-Part. Moreover, we explore several connections between discriminative power and semantics. We find out which are the most discriminative filters for object recognition, and analyze whether they respond to semantic parts or to other image patches. We also investigate the other direction: we determine which semantic parts are the most discriminative and whether they correspond to those parts emerging in the network. This enables to gain an even deeper understanding of the role of semantic parts in the network.},
	language = {en},
	number = {5},
	urldate = {2020-06-21},
	journal = {International Journal of Computer Vision},
	author = {Gonzalez-Garcia, Abel and Modolo, Davide and Ferrari, Vittorio},
	month = may,
	year = {2018},
	pages = {476--494},
}

@article{yan_recurrent_2019,
	title = {Recurrent {Feedback} {Improves} {Feedforward} {Representations} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1912.10489},
	abstract = {The abundant recurrent horizontal and feedback connections in the primate visual cortex are thought to play an important role in bringing global and semantic contextual information to early visual areas during perceptual inference, helping to resolve local ambiguity and ﬁll in missing details. In this study, we ﬁnd that introducing feedback loops and horizontal recurrent connections to a deep convolution neural network (VGG16) allows the network to become more robust against noise and occlusion during inference, even in the initial feedforward pass. This suggests that recurrent feedback and contextual modulation transform the feedforward representations of the network in a meaningful and interesting way. We study the population codes of neurons in the network, before and after learning with feedback, and ﬁnd that learning with feedback yielded an increase in discriminability (measured by d-prime) between the different object classes in the population codes of the neurons in the feedforward path, even at the earliest layer that receives feedback. We ﬁnd that recurrent feedback, by injecting top-down semantic meaning to the population activities, helps the network learn better feedforward paths to robustly map noisy image patches to the latent representations corresponding to important visual concepts of each object class, resulting in greater robustness of the network against noises and occlusion as well as better ﬁne-grained recognition.},
	language = {en},
	urldate = {2020-06-20},
	journal = {arXiv:1912.10489 [cs, q-bio]},
	author = {Yan, Siming and Fang, Xuyang and Xiao, Bowen and Rockwell, Harold and Zhang, Yimeng and Lee, Tai Sing},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.10489},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
}

@inproceedings{zagoruyko_wide_2016,
	address = {York, UK},
	title = {Wide {Residual} {Networks}},
	isbn = {978-1-901725-59-9},
	url = {http://www.bmva.org/bmvc/2016/papers/paper087/index.html},
	doi = {10.5244/C.30.87},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efﬁciency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN. Our code is available at https://github.com/szagoruyko/wide-residual-networks.},
	language = {en},
	urldate = {2020-06-20},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2016},
	publisher = {British Machine Vision Association},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	year = {2016},
	pages = {87.1--87.12},
}

@inproceedings{he_identity_2016,
	title = {Identity {Mappings} in {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1603.05027},
	abstract = {Deep residual networks [1] have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/ resnet-1k-layers.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{madry_towards_2018,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	year = {2018},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{chan_jacobian_2020,
	title = {Jacobian {Adversarially} {Regularized} {Networks} {For} {Robustness}},
	abstract = {Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with afﬁrmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classiﬁer’s Jacobian by adversarially regularizing the model’s Jacobian to resemble natural training images1. Image classiﬁers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training examples.},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Chan, Alvin and Tay, Yi and Ong, Yew-Soon and Fu, Jie},
	year = {2020},
	pages = {13},
}

@inproceedings{tramer_ensemble_2018,
	title = {Ensemble {Adversarial} {Training}: {Attacks} and {Defenses}},
	shorttitle = {Ensemble {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1705.07204},
	abstract = {Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model’s loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we ﬁnd that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Tramèr, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Statistics - Machine Learning},
}

@inproceedings{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{wong_fast_2020,
	title = {Fast is better than free: {Revisiting} adversarial training},
	shorttitle = {Fast is better than free},
	url = {http://arxiv.org/abs/2001.03994},
	abstract = {Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a ﬁrst-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Speciﬁcally, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has signiﬁcantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efﬁcient training of deep networks, allowing us to learn a robust CIFAR10 classiﬁer with 45\% robust accuracy to PGD attacks with = 8/255 in 6 minutes, and a robust ImageNet classiﬁer with 43\% robust accuracy at = 2/255 in 12 hours, in comparison to past work based on “free” adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as “catastrophic overﬁtting” which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at https://github.com/locuslab/fast\_adversarial.},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wong, Eric and Rice, Leslie and Kolter, J. Zico},
	year = {2020},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{xie_feature_2019,
	title = {Feature {Denoising} for {Improving} {Adversarial} {Robustness}},
	abstract = {Adversarial attacks to image classiﬁcation systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Speciﬁcally, our networks contain blocks that denoise the features using non-local means or other ﬁlters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9\% accuracy, our method achieves 55.7\%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6\% accuracy. Our method was ranked ﬁrst in Competition on Adversarial Attacks and Defenses (CAAD) 2018 — it achieved 50.6\% classiﬁcation accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ∼10\%. Code is available at https://github.com/facebookresearch/ ImageNet-Adversarial-Training.},
	language = {en},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Xie, Cihang and Wu, Yuxin and Maaten, Laurens van der and Yuille, Alan and He, Kaiming},
	year = {2019},
	pages = {9},
}

@inproceedings{ross_improving_2018,
	title = {Improving the {Adversarial} {Robustness} and {Interpretability} of {Deep} {Neural} {Networks} by {Regularizing} their {Input} {Gradients}},
	url = {https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17337},
	abstract = {Deep neural networks have proven remarkably effective at solving many classiﬁcation problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we ﬁnd that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also ﬁnd that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more “legitimate,” interpretable misclassiﬁcations as rated by people (which we conﬁrm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.},
	language = {en},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Ross, Andrew Slavin and Doshi-Velez, Finale},
	year = {2018},
	pages = {10},
}

@inproceedings{jakubovitz_improving_2018,
	title = {Improving {DNN} {Robustness} to {Adversarial} {Attacks} using {Jacobian} {Regularization}},
	url = {http://openaccess.thecvf.com/content_ECCV_2018/html/Daniel_Jakubovitz_Improving_DNN_Robustness_ECCV_2018_paper.html},
	urldate = {2020-06-20},
	booktitle = {Proceedings of the {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Jakubovitz, Daniel and Giryes, Raja},
	year = {2018},
	pages = {514--529},
}

@inproceedings{kurakin_adversarial_2017,
	title = {Adversarial {Machine} {Learning} at {Scale}},
	url = {http://arxiv.org/abs/1611.01236},
	abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model’s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the ﬁnding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a “label leaking” effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	year = {2017},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {37th {IEEE} {Symposium} on {Security} \& {Privacy}},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{shafahi_adversarial_2019,
	title = {Adversarial {Training} for {Free}!},
	url = {http://arxiv.org/abs/1904.12843},
	abstract = {Adversarial training, in which a network is trained on adversarial examples, is one of the few defenses against adversarial attacks that withstands strong attacks. Unfortunately, the high cost of generating strong adversarial examples makes standard adversarial training impractical on large-scale problems like ImageNet. We present an algorithm that eliminates the overhead cost of generating adversarial examples by recycling the gradient information computed when updating model parameters. Our “free” adversarial training algorithm achieves state-of-the-art robustness on CIFAR-10 and CIFAR-100 datasets at negligible additional cost compared to natural training, and can be 7 to 30 times faster than other strong adversarial training methods. Using a single workstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model for the large-scale ImageNet classiﬁcation task that maintains 40\% accuracy against PGD attacks.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Shafahi, Ali and Najibi, Mahyar and Ghiasi, Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S. and Taylor, Gavin and Goldstein, Tom},
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{tsipras_robustness_2019,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	url = {http://arxiv.org/abs/1805.12152},
	abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Speciﬁcally, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These ﬁndings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classiﬁers learning fundamentally different feature representations than standard classiﬁers. These differences, in particular, seem to result in unexpected beneﬁts: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{etmann_connection_2019,
	title = {On the {Connection} {Between} {Adversarial} {Robustness} and {Saliency} {Map} {Interpretability}},
	abstract = {Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their nonrobust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models. We conﬁrm these theoretical ﬁndings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.},
	language = {en},
	booktitle = {{thProceedings} of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Etmann, Christian and Lunz, Sebastian and Maass, Peter and Schonlieb, Carola-Bibiane},
	year = {2019},
	pages = {10},
}

@inproceedings{zhang_interpreting_2019,
	title = {Interpreting {Adversarially} {Trained} {Convolutional} {Neural} {Networks}},
	abstract = {We attempt to interpret how adversarially trained convolutional neural networks (AT-CNNs) recognize objects. We design systematic approaches to interpret AT-CNNs in both qualitative and quantitative ways and compare them with normally trained models. Surprisingly, we ﬁnd that adversarial training alleviates the texture bias of standard CNNs when trained on object recognition tasks, and helps CNNs learn a more shape-biased representation. We validate our hypothesis from two aspects. First, we compare the salience maps of AT-CNNs and standard CNNs on clean images and images under different transformations. The comparison could visually show that the prediction of the two types of CNNs is sensitive to dramatically different types of features. Second, to achieve quantitative veriﬁcation, we construct additional test datasets that destroy either textures or shapes, such as style-transferred version of clean data, saturated images and patch-shufﬂed ones, and then evaluate the classiﬁcation accuracy of AT-CNNs and normal CNNs on these datasets. Our ﬁndings shed some light on why AT-CNNs are more robust than those normally trained ones and contribute to a better understanding of adversarial training over CNNs from an interpretation perspective.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Zhang, Tianyuan and Zhu, Zhanxing},
	year = {2019},
	pages = {10},
}

@inproceedings{ilyas_adversarial_2019,
	title = {Adversarial {Examples} {Are} {Not} {Bugs}, {They} {Are} {Features}},
	url = {https://papers.nips.cc/paper/8307-adversarial-examples-are-not-bugs-they-are-features.pdf},
	abstract = {Adversarial examples have attracted signiﬁcant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-speciﬁed) notion of robustness and the inherent geometry of the data.},
	language = {en},
	urldate = {2020-06-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{galloway_batch_2019,
	title = {Batch {Normalization} is a {Cause} of {Adversarial} {Vulnerability}},
	url = {http://arxiv.org/abs/1905.02161},
	abstract = {Batch normalization (batch norm) is often used in an attempt to stabilize and accelerate training in deep neural networks. In many cases it indeed decreases the number of parameter updates required to achieve low training error. However, it also reduces robustness to small adversarial input perturbations and noise by double-digit percentages, as we show on ﬁve standard datasets. Furthermore, substituting weight decay for batch norm is suﬃcient to nullify the relationship between adversarial vulnerability and the input dimension. Our work is consistent with a mean-ﬁeld analysis that found that batch norm causes exploding gradients.},
	language = {en},
	urldate = {2020-06-15},
	journal = {arXiv:1905.02161 [cs, stat]},
	author = {Galloway, Angus and Golubeva, Anna and Tanay, Thomas and Moussa, Medhat and Taylor, Graham W.},
	month = may,
	year = {2019},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = {2014},
	keywords = {Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{sinha_harnessing_2019,
	title = {Harnessing the {Vulnerability} of {Latent} {Layers} in {Adversarially} {Trained} {Models}},
	abstract = {Neural networks are vulnerable to adversarial attacks - small visually imperceptible crafted noise which when added to the input drastically changes the output. The most effective method of defending against these adversarial attacks is to use the methodology of adversarial training. We analyze the adversarially trained robust models to study their vulnerability against adversarial attacks at the level of the latent layers. Our analysis reveals that contrary to the input layer which is robust to adversarial attack, the latent layer of these robust models are highly susceptible to adversarial perturbations of small magnitude. Leveraging this information, we introduce a new technique Latent Adversarial Training (LAT) which comprises of ﬁne-tuning the adversarially trained models to ensure the robustness at the feature layers. We also propose Latent Attack (LA), a novel algorithm for construction of adversarial examples. LAT results in minor improvement in test accuracy and leads to a state-of-the-art adversarial accuracy against the universal ﬁrst-order adversarial PGD attack which is shown for the MNIST, CIFAR-10, CIFAR-100 datasets.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sinha, Abhishek and Singh, Mayank and Kumari, Nupur and Krishnamurthy, Balaji and Machiraju, Harshitha and Balasubramanian, Vineeth N},
	year = {2019},
}

@inproceedings{brendel_decision-based_2018,
	title = {Decision-{Based} {Adversarial} {Attacks}: {Reliable} {Attacks} {Against} {Black}-{Box} {Machine} {Learning} {Models}},
	shorttitle = {Decision-{Based} {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1712.04248},
	abstract = {Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on conﬁdence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the ﬁnal model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox (https://github.com/bethgelab/foolbox).},
	language = {en},
	urldate = {2020-06-14},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
	month = feb,
	year = {2018},
	note = {arXiv: 1712.04248},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{zhang_interpreting_2019-1,
	address = {Long Beach, CA, USA},
	title = {Interpreting {CNNs} via {Decision} {Trees}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953917/},
	doi = {10.1109/CVPR.2019.00642},
	abstract = {This paper1 aims to quantitatively explain the rationales of each prediction that is made by a pre-trained convolutional neural network (CNN). We propose to learn a decision tree, which clariﬁes the speciﬁc reason for each prediction made by the CNN at the semantic level. I.e. the decision tree decomposes feature representations in high conv-layers of the CNN into elementary concepts of object parts. In this way, the decision tree tells people which object parts activate which ﬁlters for the prediction and how much each object part contributes to the prediction score. Such semantic and quantitative explanations for CNN predictions have speciﬁc values beyond the traditional pixel-level analysis of CNNs. More speciﬁcally, our method mines all potential decision modes of the CNN, where each mode represents a typical case of how the CNN uses object parts for prediction. The decision tree organizes all potential decision modes in a coarse-to-ﬁne manner to explain CNN predictions at different ﬁne-grained levels. Experiments have demonstrated the effectiveness of the proposed method.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhang, Quanshi and Yang, Yu and Ma, Haotian and Wu, Ying Nian},
	month = jun,
	year = {2019},
	pages = {6254--6263},
}

@inproceedings{zhang_interpretable_2018,
	address = {Salt Lake City, UT},
	title = {Interpretable {Convolutional} {Neural} {Networks}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8579018/},
	doi = {10.1109/CVPR.2018.00920},
	abstract = {This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each ﬁlter in a high conv-layer represents a speciﬁc object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each ﬁlter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e. what patterns are memorized by the CNN for prediction. Experiments have shown that ﬁlters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https: //github.com/zqs1022/interpretableCNN .},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
	month = jun,
	year = {2018},
	pages = {8827--8836},
}

@article{lu_visual_2016,
	title = {Visual {Relationship} {Detection} with {Language} {Priors}},
	url = {http://arxiv.org/abs/1608.00187},
	abstract = {Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. “man riding bicycle” and “man pushing bicycle”). Consequently, the set of possible relationships is extremely large and it is diﬃcult to obtain suﬃcient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. “man” and “bicycle”) and predicates (e.g. “riding” and “pushing”) independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to ﬁnetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1608.00187 [cs]},
	author = {Lu, Cewu and Krishna, Ranjay and Bernstein, Michael and Fei-Fei, Li},
	month = jul,
	year = {2016},
	note = {arXiv: 1608.00187},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{richardson_markov_2006,
	title = {Markov logic networks},
	volume = {62},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-006-5833-1},
	doi = {10.1007/s10994-006-5833-1},
	abstract = {We propose a simple approach to combining ﬁrst-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a ﬁrstorder knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it speciﬁes a ground Markov network containing one feature for each possible grounding of a ﬁrst-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efﬁciently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.},
	language = {en},
	number = {1-2},
	urldate = {2020-06-18},
	journal = {Machine Learning},
	author = {Richardson, Matthew and Domingos, Pedro},
	month = feb,
	year = {2006},
	pages = {107--136},
}

@article{reed_training_2015,
	title = {Training {Deep} {Neural} {Networks} on {Noisy} {Labels} with {Bootstrapping}},
	url = {http://arxiv.org/abs/1412.6596},
	abstract = {Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overﬁtting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-theart results, and can also beneﬁt from unlabeled face images with no modiﬁcation to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1412.6596 [cs]},
	author = {Reed, Scott and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1412.6596},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@incollection{fleet_reasoning_2014,
	address = {Cham},
	title = {Reasoning about {Object} {Affordances} in a {Knowledge} {Base} {Representation}},
	volume = {8690},
	isbn = {978-3-319-10604-5 978-3-319-10605-2},
	url = {http://link.springer.com/10.1007/978-3-319-10605-2_27},
	abstract = {Reasoning about objects and their aﬀordances is a fundamental problem for visual intelligence. Most of the previous work casts this problem as a classiﬁcation task where separate classiﬁers are trained to label objects, recognize attributes, or assign aﬀordances. In this work, we consider the problem of object aﬀordance reasoning using a knowledge base representation. Diverse information of objects are ﬁrst harvested from images and other meta-data sources. We then learn a knowledge base (KB) using a Markov Logic Network (MLN). Given the learned KB, we show that a diverse set of visual inference tasks can be done in this uniﬁed framework without training separate classiﬁers, including zeroshot aﬀordance prediction and object recognition given human poses.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zhu, Yuke and Fathi, Alireza and Fei-Fei, Li},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	doi = {10.1007/978-3-319-10605-2_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {408--424},
}

@inproceedings{marszalek_semantic_2007,
	address = {Minneapolis, MN, USA},
	title = {Semantic {Hierarchies} for {Visual} {Object} {Recognition}},
	isbn = {978-1-4244-1179-5 978-1-4244-1180-1},
	url = {http://ieeexplore.ieee.org/document/4270297/},
	doi = {10.1109/CVPR.2007.383272},
	abstract = {In this paper we propose to use lexical semantic networks to extend the state-of-the-art object recognition techniques. We use the semantics of image labels to integrate prior knowledge about inter-class relationships into the visual appearance learning. We show how to build and train a semantic hierarchy of discriminative classiﬁers and how to use it to perform object detection. We evaluate how our approach inﬂuences the classiﬁcation accuracy and speed on the PASCAL VOC challenge 2006 dataset, a set of challenging real-world images. We also demonstrate additional features that become available to object recognition due to the extension with semantic inference tools—we can classify high-level categories, such as animals, and we can train part detectors, for example a window detector, by pure inference in the semantic network.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2007 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Marszalek, Marcin and Schmid, Cordelia},
	month = jun,
	year = {2007},
	pages = {1--7},
}

@article{schott_towards_2019,
	title = {Towards {The} {First} {Adversarially} {Robust} {Neural} {Network} {Model} on {MNIST}},
	abstract = {Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful L∞ defense by Madry et al. (1) has lower L0 robustness than undefended networks and is still highly susceptible to L2 perturbations, (2) classiﬁes unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classiﬁcation model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L∞ perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.},
	language = {en},
	author = {Schott, Lukas and Rauber, Jonas and Bethge, Matthias and Brendel, Wieland},
	year = {2019},
	pages = {17},
}

@article{grill-spector_human_2004,
	title = {The {Human} {Visual} {Cortex}},
	volume = {27},
	issn = {0147-006X, 1545-4126},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.neuro.27.070203.144220},
	doi = {10.1146/annurev.neuro.27.070203.144220},
	language = {en},
	number = {1},
	urldate = {2020-06-18},
	journal = {Annual Review of Neuroscience},
	author = {Grill-Spector, Kalanit and Malach, Rafael},
	month = jul,
	year = {2004},
	pages = {649--677},
}

@inproceedings{ramanathan_learning_2015,
	address = {Boston, MA, USA},
	title = {Learning semantic relationships for better action retrieval in images},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298713/},
	doi = {10.1109/CVPR.2015.7298713},
	abstract = {Human actions capture a wide variety of interactions between people and objects. As a result, the set of possible actions is extremely large and it is difﬁcult to obtain sufﬁcient training examples for all actions. However, we could compensate for this sparsity in supervision by leveraging the rich semantic relationship between different actions. A single action is often composed of other smaller actions and is exclusive of certain others. We need a method which can reason about such relationships and extrapolate unobserved actions from known actions. Hence, we propose a novel neural network framework which jointly extracts the relationship between actions and uses them for training better action retrieval models. Our model incorporates linguistic, visual and logical consistency based cues to effectively identify these relationships. We train and test our model on a largescale image dataset of human actions. We show a signiﬁcant improvement in mean AP compared to different baseline methods including the HEX-graph approach from Deng et al. [8].},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ramanathan, Vignesh and Li, Congcong and Deng, Jia and Han, Wei and Li, Zhen and Gu, Kunlong and Song, Yang and Bengio, Samy and Rossenberg, Chuck and Fei-Fei, Li},
	month = jun,
	year = {2015},
	pages = {1100--1109},
}

@article{percy_need_nodate,
	title = {The {Need} for {Knowledge} {Extraction}: {Understanding} {Harmful} {Gambling} {Behavior} with {Neural} {Networks}},
	language = {en},
	author = {Percy, Chris and França, Manoel V M and Slabaugh, Greg and Weyde, Tillman},
	pages = {8},
}

@article{hinton_fast_2006,
	title = {A {Fast} {Learning} {Algorithm} for {Deep} {Belief} {Nets}},
	volume = {18},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527},
	doi = {10.1162/neco.2006.18.7.1527},
	language = {en},
	number = {7},
	urldate = {2020-06-18},
	journal = {Neural Computation},
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
	month = jul,
	year = {2006},
	pages = {1527--1554},
}

@inproceedings{feng_pathologies_2018,
	address = {Brussels, Belgium},
	title = {Pathologies of {Neural} {Models} {Make} {Interpretations} {Difficult}},
	url = {http://aclweb.org/anthology/D18-1407},
	doi = {10.18653/v1/D18-1407},
	abstract = {One way to interpret neural model predictions is to highlight the most important input features—for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word’s importance is determined by either input perturbation—measuring the decrease in model conﬁdence when that word is removed—or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and do not match the words interpretation methods deem important. As we conﬁrm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high conﬁdence. To explain these counterintuitive results, we draw connections to adversarial examples and conﬁdence calibration: pathological behaviors reveal difﬁculties in interpreting neural models trained with maximum likelihood. To mitigate their deﬁciencies, we ﬁne-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Shi and Wallace, Eric and Grissom II, Alvin and Iyyer, Mohit and Rodriguez, Pedro and Boyd-Graber, Jordan},
	year = {2018},
	pages = {3719--3728},
}

@article{li_visualizing_2016,
	title = {Visualizing and {Understanding} {Neural} {Models} in {NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difﬁcult to interpret. For example it’s not clear how they achieve compositionality, building sentence meaning from the meanings of words and phrases. In this paper we describe strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We ﬁrst plot unit values to visualize compositionality of negation, intensiﬁcation, and concessive clauses, allowing us to see wellknown markedness asymmetries in negation. We then introduce methods for visualizing a unit’s salience, the amount that it contributes to the ﬁnal composed meaning from ﬁrst-order derivatives. Our generalpurpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1506.01066 [cs]},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01066},
	keywords = {Computer Science - Computation and Language},
}

@article{frosst_distilling_2017,
	title = {Distilling a {Neural} {Network} {Into} a {Soft} {Decision} {Tree}},
	url = {http://arxiv.org/abs/1711.09784},
	abstract = {Deep neural networks have proved to be a very eﬀective way to perform classiﬁcation tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large [Szegedy et al., 2015, Wu et al., 2016, Jozefowicz et al., 2016, Graves et al., 2013]. But it is hard to explain why a learned network makes a particular classiﬁcation decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1711.09784 [cs, stat]},
	author = {Frosst, Nicholas and Hinton, Geoffrey},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.09784},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{guidotti_local_2018,
	title = {Local {Rule}-{Based} {Explanations} of {Black} {Box} {Decision} {Systems}},
	url = {http://arxiv.org/abs/1805.10820},
	abstract = {The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance’s features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1805.10820 [cs]},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
	month = may,
	year = {2018},
	note = {arXiv: 1805.10820},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{pedreschi_open_2018,
	title = {Open the {Black} {Box} {Data}-{Driven} {Explanation} of {Black} {Box} {Decision} {Systems}},
	url = {http://arxiv.org/abs/1806.09936},
	abstract = {Black box systems for automated decision making, often based on machine learning over (big) data, map a user's features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases hidden in the algorithms, due to human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We introduce the local-to-global framework for black box explanation, a novel approach with promising early results, which paves the road for a wide spectrum of future developments along three dimensions: (i) the language for expressing explanations in terms of highly expressive logic-based rules, with a statistical and causal interpretation; (ii) the inference of local explanations aimed at revealing the logic of the decision adopted for a specific instance by querying and auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of the many local explanations into simple global ones, with algorithms that optimize the quality and comprehensibility of explanations.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1806.09936 [cs]},
	author = {Pedreschi, Dino and Giannotti, Fosca and Guidotti, Riccardo and Monreale, Anna and Pappalardo, Luca and Ruggieri, Salvatore and Turini, Franco},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.09936},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{park_multimodal_2018,
	address = {Salt Lake City, UT},
	title = {Multimodal {Explanations}: {Justifying} {Decisions} and {Pointing} to the {Evidence}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Multimodal {Explanations}},
	url = {https://ieeexplore.ieee.org/document/8579013/},
	doi = {10.1109/CVPR.2018.00915},
	abstract = {Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justiﬁcations. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to deﬁne and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets deﬁne visual and textual justiﬁcations of a classiﬁcation decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justiﬁcation models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer signiﬁcant beneﬁts over unimodal approaches.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Park, Dong Huk and Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Anna and Schiele, Bernt and Darrell, Trevor and Rohrbach, Marcus},
	month = jun,
	year = {2018},
	pages = {8779--8788},
}

@inproceedings{wang_vqa-machine_2017,
	address = {Honolulu, HI},
	title = {The {VQA}-{Machine}: {Learning} {How} to {Use} {Existing} {Vision} {Algorithms} to {Answer} {New} {Questions}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {The {VQA}-{Machine}},
	url = {https://ieeexplore.ieee.org/document/8099899/},
	doi = {10.1109/CVPR.2017.416},
	abstract = {One of the most intriguing features of the Visual Question Answering (VQA) challenge is the unpredictability of the questions. Extracting the information required to answer them demands a variety of image operations from detection and counting, to segmentation and reconstruction. To train a method to perform even one of these operations accurately from \{image,question,answer\} tuples would be challenging, but to aim to achieve them all with a limited set of such training data seems ambitious at best. We propose here instead a more general and scalable approach which exploits the fact that very good methods to achieve these operations already exist, and thus do not need to be trained. Our method thus learns how to exploit a set of external off-the-shelf algorithms to achieve its goal, an approach that has something in common with the Neural Turing Machine [10]. The core of our proposed method is a new co-attention model. In addition, the proposed approach generates human-readable reasons for its decision, and can still be trained end-to-end without ground truth reasons being given. We demonstrate the effectiveness on two publicly available datasets, Visual Genome and VQA, and show that it produces the state-of-the-art results in both cases.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Peng and Wu, Qi and Shen, Chunhua and van den Hengel, Anton},
	month = jul,
	year = {2017},
	pages = {3909--3918},
}

@article{mccoy_rnns_2019,
	title = {{RNNs} {Implicitly} {Implement} {Tensor} {Product} {Representations}},
	url = {http://arxiv.org/abs/1812.08718},
	abstract = {Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing ﬁllers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structuresensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag of words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufﬁcient for inducing robust structural representations.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1812.08718 [cs]},
	author = {McCoy, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
	month = mar,
	year = {2019},
	note = {arXiv: 1812.08718},
	keywords = {Computer Science - Computation and Language},
}

@article{arras_explaining_2017,
	title = {Explaining {Recurrent} {Neural} {Network} {Predictions} in {Sentiment} {Analysis}},
	url = {http://arxiv.org/abs/1706.07206},
	abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classiﬁcation decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a speciﬁc propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a ﬁve-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1706.07206 [cs, stat]},
	author = {Arras, Leila and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
	month = aug,
	year = {2017},
	note = {arXiv: 1706.07206},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can signiﬁcantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish ﬁne-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{han_neural_nodate,
	title = {Neural {Knowledge} {Acquisition} via {Mutual} {Attention} between {Knowledge} {Graph} and {Text}},
	abstract = {We propose a general joint representation learning framework for knowledge acquisition (KA) on two tasks, knowledge graph completion (KGC) and relation extraction (RE) from text. In this framework, we learn representations of knowledge graphs (KGs) and text within a uniﬁed parameter sharing semantic space. To achieve better fusion, we propose an effective mutual attention between KGs and text. The reciprocal attention mechanism enables us to highlight important features and perform better KGC and RE. Different from conventional joint models, no complicated linguistic analysis or strict alignments between KGs and text are required to train our models. Experiments on relation extraction and entity link prediction show that models trained under our joint framework are signiﬁcantly improved in comparison with other baselines. Most existing methods for KGC and RE can be easily integrated into our framework due to its ﬂexible architectures. The source code of this paper can be obtained from https://github.com/thunlp/JointNRE.},
	language = {en},
	author = {Han, Xu and Liu, Zhiyuan and Sun, Maosong},
	pages = {8},
}

@article{saphra_sparsity_nodate,
	title = {Sparsity {Emerges} {Naturally} in {Neural} {Language} {Models}},
	abstract = {Concerns about interpretability, computational resources, and principled inductive priors have motivated efforts to engineer sparse neural models for NLP tasks. If sparsity is important for NLP, might well-trained neural models naturally become roughly sparse? Using the Taxi-Euclidean norm to measure sparsity, we ﬁnd that frequent input words are associated with concentrated or sparse activations, while frequent target words are associated with dispersed activations but concentrated gradients. We ﬁnd that gradients associated with function words are more concentrated than the gradients of content words, even controlling for word frequency.},
	language = {en},
	author = {Saphra, Naomi and Lopez, Adam},
	pages = {5},
}

@inproceedings{liu_multi-task_2015,
	address = {Boston, MA, USA},
	title = {Multi-task deep visual-semantic embedding for video thumbnail selection},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298994/},
	doi = {10.1109/CVPR.2015.7298994},
	abstract = {Given the tremendous growth of online videos, video thumbnail, as the common visualization form of video content, is becoming increasingly important to inﬂuence user’s browsing and searching experience. However, conventional methods for video thumbnail selection often fail to produce satisfying results as they ignore the side semantic information (e.g., title, description, and query) associated with the video. As a result, the selected thumbnail cannot always represent video semantics and the click-through rate is adversely affected even when the retrieved videos are relevant. In this paper, we have developed a multi-task deep visualsemantic embedding model, which can automatically select query-dependent video thumbnails according to both visual and side information. Different from most existing methods, the proposed approach employs the deep visual-semantic embedding model to directly compute the similarity between the query and video thumbnails by mapping them into a common latent semantic space, where even unseen querythumbnail pairs can be correctly matched. In particular, we train the embedding model by exploring the large-scale and freely accessible click-through video and image data, as well as employing a multi-task learning strategy to holistically exploit the query-thumbnail relevance from these two highly related datasets. Finally, a thumbnail is selected by fusing both the representative and query relevance scores. The evaluations on 1,000 query-thumbnail dataset labeled by 191 workers in Amazon Mechanical Turk have demonstrated the effectiveness of our proposed method.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Wu and Mei, Tao and Zhang, Yongdong and Che, Cherry and Luo, Jiebo},
	month = jun,
	year = {2015},
	pages = {3707--3715},
}

@inproceedings{lee_convolutional_2009,
	address = {Montreal, Quebec, Canada},
	title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553453},
	doi = {10.1145/1553374.1553453},
	abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a diﬃcult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports eﬃcient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
	year = {2009},
	pages = {1--8},
}

@article{lee_sparse_nodate,
	title = {Sparse deep belief net model for visual area {V2}},
	abstract = {Motivated in part by the hierarchical organization of the cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or “deep,” structure from unlabeled data. While several authors have formally or informally compared their algorithms to computations performed in visual area V1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their ﬁdelity for mimicking computations at deeper levels in the cortical hierarchy. This paper presents an unsupervised learning model that faithfully mimics certain properties of visual area V2. Speciﬁcally, we develop a sparse variant of the deep belief networks of Hinton et al. (2006). We learn two layers of nodes in the network, and demonstrate that the ﬁrst layer, similar to prior work on sparse coding and ICA, results in localized, oriented, edge ﬁlters, similar to the Gabor functions known to model V1 cell receptive ﬁelds. Further, the second layer in our model encodes correlations of the ﬁrst layer responses in the data. Speciﬁcally, it picks up both colinear (“contour”) features as well as corners and junctions. More interestingly, in a quantitative comparison, the encoding of these more complex “corner” features matches well with the results from the Ito \& Komatsu’s study of biological V2 responses. This suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features.},
	language = {en},
	author = {Lee, Honglak and Ekanadham, Chaitanya and Ng, Andrew Y},
	pages = {8},
}

@article{bengio_consciousness_2019,
	title = {The {Consciousness} {Prior}},
	url = {http://arxiv.org/abs/1709.08568},
	abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. Instead of making predictions in the sensory (e.g. pixel) space, one can thus make predictions in this high-level abstract space, which do not have to be limited to just the next time step but can relate events far away from each other in time. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efﬁcient search mechanisms implemented by attention mechanisms.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1709.08568 [cs, stat]},
	author = {Bengio, Yoshua},
	month = dec,
	year = {2019},
	note = {arXiv: 1709.08568},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{das_human_2017,
	title = {Human {Attention} in {Visual} {Question} {Answering}: {Do} {Humans} and {Deep} {Networks} {Look} at the {Same} {Regions}?},
	volume = {163},
	issn = {10773142},
	shorttitle = {Human {Attention} in {Visual} {Question} {Answering}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314217301649},
	doi = {10.1016/j.cviu.2017.10.001},
	abstract = {We conduct large-scale studies on ‘human attention’ in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans. Finally, we train VQA models with explicit attention supervision, and ﬁnd that it improves VQA performance.},
	language = {en},
	urldate = {2020-06-18},
	journal = {Computer Vision and Image Understanding},
	author = {Das, Abhishek and Agrawal, Harsh and Zitnick, Larry and Parikh, Devi and Batra, Dhruv},
	month = oct,
	year = {2017},
	pages = {90--100},
}

@article{towell_knowledge-based_1994,
	title = {Knowledge-based artificial neural networks},
	volume = {70},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370294901058},
	doi = {10.1016/0004-3702(94)90105-8},
	language = {en},
	number = {1-2},
	urldate = {2020-06-18},
	journal = {Artificial Intelligence},
	author = {Towell, Geoffrey G. and Shavlik, Jude W.},
	month = oct,
	year = {1994},
	pages = {119--165},
}

@article{shokri-kojori_network_2012,
	title = {The {Network} {Architecture} of {Cortical} {Processing} in {Visuo}-spatial {Reasoning}},
	volume = {2},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep00411},
	doi = {10.1038/srep00411},
	language = {en},
	number = {1},
	urldate = {2020-06-18},
	journal = {Scientific Reports},
	author = {Shokri-Kojori, Ehsan and Motes, Michael A. and Rypma, Bart and Krawczyk, Daniel C.},
	month = dec,
	year = {2012},
	pages = {411},
}

@inproceedings{szegedy_rethinking_2016,
	address = {Las Vegas, NV, USA},
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780677/},
	doi = {10.1109/CVPR.2016.308},
	abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efﬁciency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efﬁciently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classiﬁcation challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error on the validation set and 3.6\% top-5 error on the ofﬁcial test set.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	month = jun,
	year = {2016},
	pages = {2818--2826},
}

@article{lake_building_2017,
	title = {Building machines that learn and think like people},
	volume = {40},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X16001837/type/journal_article},
	doi = {10.1017/S0140525X16001837},
	abstract = {Recent progress in artiﬁcial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Speciﬁcally, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	language = {en},
	urldate = {2020-06-18},
	journal = {Behavioral and Brain Sciences},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	year = {2017},
	pages = {e253},
}

@techreport{kietzmann_deep_2017,
	type = {preprint},
	title = {Deep {Neural} {Networks} in {Computational} {Neuroscience}},
	url = {http://biorxiv.org/lookup/doi/10.1101/133504},
	abstract = {The goal of computational neuroscience is to find mechanistic explanations of how the nervous system processes information to give rise to cognitive function and behaviour. At the heart of the field are its models, i.e. mathematical and computational descriptions of the system being studied, which map sensory stimuli to neural responses and/or neural to behavioural responses. These models range from simple to complex. Recently, deep neural networks (DNNs) have come to dominate several domains of artificial intelligence (AI). As the term “neural network” suggests, these models are inspired by biological brains. However, current DNNs neglect many details of biological neural networks. These simplifications contribute to their computational efficiency, enabling them to perform complex feats of intelligence, ranging from perceptual (e.g. visual object and auditory speech recognition) to cognitive tasks (e.g. machine translation), and on to motor control (e.g. playing computer games or controlling a robot arm). In addition to their ability to model complex intelligent behaviours, DNNs excel at predicting neural responses to novel sensory stimuli with accuracies well beyond any other currently available model type. DNNs can have millions of parameters, which are required to capture the domain knowledge needed for successful task performance. Contrary to the intuition that this renders them into impenetrable black boxes, the computational properties of the network units are the result of four directly manipulable elements: input statistics, network structure, functional objective, and learning algorithm. With full access to the activity and connectivity of all units, advanced visualization techniques, and analytic tools to map network representations to neural data, DNNs represent a powerful framework for building task-performing models and will drive substantial insights in computational neuroscience.},
	language = {en},
	urldate = {2020-06-18},
	institution = {Neuroscience},
	author = {Kietzmann, Tim C and McClure, Patrick and Kriegeskorte, Nikolaus},
	month = may,
	year = {2017},
	doi = {10.1101/133504},
}

@inproceedings{chen_deep_2013,
	address = {Portland, OR, USA},
	title = {Deep {Learning} {Shape} {Priors} for {Object} {Segmentation}},
	isbn = {978-0-7695-4989-7},
	url = {http://ieeexplore.ieee.org/document/6619088/},
	doi = {10.1109/CVPR.2013.244},
	abstract = {In this paper we introduce a new shape-driven approach for object segmentation. Given a training set of shapes, we first use deep Boltzmann machine to learn the hierarchical architecture of shape priors. This learned hierarchical architecture is then used to model shape variations of global and local structures in an energetic form. Finally, it is applied to data-driven variational methods to perform object extraction of corrupted data based on shape probabilistic representation. Experiments demonstrate that our model can be applied to dataset of arbitrary prior shapes, and can cope with image noise and clutter, as well as partial occlusions.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Fei and Yu, Huimin and Hu, Roland and Zeng, Xunxun},
	month = jun,
	year = {2013},
	pages = {1870--1877},
}

@article{si_learning_2013,
	title = {Learning {AND}-{OR} {Templates} for {Object} {Recognition} and {Detection}},
	volume = {35},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6425379/},
	doi = {10.1109/TPAMI.2013.35},
	abstract = {This paper presents a framework for unsupervised learning of a hierarchical reconfigurable image template—the AND-OR Template (AOT) for visual objects. The AOT includes: 1) hierarchical composition as “AND” nodes, 2) deformation and articulation of parts as geometric “OR” nodes, and 3) multiple ways of composition as structural “OR” nodes. The terminal nodes are hybrid image templates (HIT) [17] that are fully generative to the pixels. We show that both the structures and parameters of the AOT model can be learned in an unsupervised way from images using an information projection principle. The learning algorithm consists of two steps: 1) a recursive block pursuit procedure to learn the hierarchical dictionary of primitives, parts, and objects, and 2) a graph compression procedure to minimize model structure for better generalizability. We investigate the factors that influence how well the learning algorithm can identify the underlying AOT. And we propose a number of ways to evaluate the performance of the learned AOTs through both synthesized examples and real-world images. Our model advances the state of the art for object detection by improving the accuracy of template matching.},
	language = {en},
	number = {9},
	urldate = {2020-06-18},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Si, Zhangzhang and Zhu, Song-Chun},
	month = sep,
	year = {2013},
	pages = {2189--2205},
}

@inproceedings{ming_liang_recurrent_2015,
	address = {Boston, MA, USA},
	title = {Recurrent convolutional neural network for object recognition},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298958/},
	doi = {10.1109/CVPR.2015.7298958},
	abstract = {In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain. A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the input is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the activities of its neighboring units. This property enhances the ability of the model to integrate the context information, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a ﬁxed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning process. The model is tested on four benchmark object recognition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increasing the number of parameters leads to even better performance. These results demonstrate the advantage of the recurrent structure over purely feed-forward structure for object recognition.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {{Ming Liang} and {Xiaolin Hu}},
	month = jun,
	year = {2015},
	pages = {3367--3375},
}

@article{spoerer_recurrent_2017,
	title = {Recurrent {Convolutional} {Neural} {Networks}: {A} {Better} {Model} of {Biological} {Object} {Recognition}},
	volume = {8},
	issn = {1664-1078},
	shorttitle = {Recurrent {Convolutional} {Neural} {Networks}},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2017.01551/full},
	doi = {10.3389/fpsyg.2017.01551},
	abstract = {Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and non-human primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efﬁcacy of the models, digit clutter (where multiple target digits occlude one another) and digit debris (where target digits are occluded by digit fragments). We ﬁnd that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognizing objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognize objects, especially under challenging conditions. This work shows that computer vision can beneﬁt from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.},
	language = {en},
	urldate = {2020-06-18},
	journal = {Frontiers in Psychology},
	author = {Spoerer, Courtney J. and McClure, Patrick and Kriegeskorte, Nikolaus},
	month = sep,
	year = {2017},
	pages = {1551},
}

@article{liao_bridging_2016,
	title = {Bridging the {Gaps} {Between} {Residual} {Learning}, {Recurrent} {Neural} {Networks} and {Visual} {Cortex}},
	url = {http://arxiv.org/abs/1604.03640},
	abstract = {We discuss relations between Residual Networks (ResNet), Recurrent Neural Networks (RNNs) and the primate visual cortex. We begin with the observation that a shallow RNN is exactly equivalent to a very deep ResNet with weight sharing among the layers. A direct implementation of such a RNN, although having orders of magnitude fewer parameters, leads to a performance similar to the corresponding ResNet. We propose 1) a generalization of both RNN and ResNet architectures and 2) the conjecture that a class of moderately deep RNNs is a biologically-plausible model of the ventral stream in visual cortex. We demonstrate the eﬀectiveness of the architectures by testing them on the CIFAR-10 dataset.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:1604.03640 [cs]},
	author = {Liao, Qianli and Poggio, Tomaso},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.03640},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{kar_evidence_2019,
	title = {Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior},
	volume = {22},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-019-0392-5},
	doi = {10.1038/s41593-019-0392-5},
	language = {en},
	number = {6},
	urldate = {2020-06-18},
	journal = {Nature Neuroscience},
	author = {Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B. and DiCarlo, James J.},
	month = jun,
	year = {2019},
	pages = {974--983},
}

@article{sabour_dynamic_nodate,
	title = {Dynamic {Routing} {Between} {Capsules}},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a speciﬁc type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	language = {en},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
	pages = {11},
}

@article{levi_crowdingessential_2008,
	title = {Crowding—{An} essential bottleneck for object recognition: {A} mini-review},
	volume = {48},
	issn = {00426989},
	shorttitle = {Crowding—{An} essential bottleneck for object recognition},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0042698907005561},
	doi = {10.1016/j.visres.2007.12.009},
	abstract = {Crowding, generally deﬁned as the deleterious inﬂuence of nearby contours on visual discrimination, is ubiquitous in spatial vision. Crowding impairs the ability to recognize objects in clutter. It has been extensively studied over the last 80 years or so, and much of the renewed interest is the hope that studying crowding may lead to a better understanding of the processes involved in object recognition. Crowding also has important clinical implications for patients with macular degeneration, amblyopia and dyslexia.},
	language = {en},
	number = {5},
	urldate = {2020-06-18},
	journal = {Vision Research},
	author = {Levi, Dennis M.},
	month = feb,
	year = {2008},
	pages = {635--654},
}

@article{whitney_visual_2011,
	title = {Visual crowding: a fundamental limit on conscious perception and object recognition},
	volume = {15},
	issn = {13646613},
	shorttitle = {Visual crowding},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661311000325},
	doi = {10.1016/j.tics.2011.02.005},
	language = {en},
	number = {4},
	urldate = {2020-06-18},
	journal = {Trends in Cognitive Sciences},
	author = {Whitney, David and Levi, Dennis M.},
	month = apr,
	year = {2011},
	pages = {160--168},
}

@inproceedings{ross_right_2017,
	address = {Melbourne, Australia},
	title = {Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}},
	isbn = {978-0-9992411-0-3},
	shorttitle = {Right for the {Right} {Reasons}},
	url = {https://www.ijcai.org/proceedings/2017/371},
	doi = {10.24963/ijcai.2017/371},
	abstract = {Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difﬁcult to trust in critical applications, especially if conditions in training may differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efﬁciently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients. We apply these penalties both based on expert annotation and in an unsupervised fashion that produces multiple classiﬁers with qualitatively different decision boundaries. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {Proceedings of the {Twenty}-{Sixth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	month = aug,
	year = {2017},
	pages = {2662--2670},
}

@article{manhaeve_deepproblog_nodate,
	title = {{DeepProbLog}:  {Neural} {Probabilistic} {Logic} {Programming}},
	abstract = {We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the ﬁrst to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
	language = {en},
	author = {Manhaeve, Robin and Dumancic, Sebastijan and Kimmig, Angelika and Demeester, Thomas and Raedt, Luc De},
	pages = {11},
}

@article{bach_pixel-wise_2015,
	title = {On {Pixel}-{Wise} {Explanations} for {Non}-{Linear} {Classifier} {Decisions} by {Layer}-{Wise} {Relevance} {Propagation}},
	volume = {10},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0130140},
	doi = {10.1371/journal.pone.0130140},
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	language = {en},
	number = {7},
	urldate = {2020-06-16},
	journal = {PLOS ONE},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
	editor = {Suarez, Oscar Deniz},
	month = jul,
	year = {2015},
	pages = {e0130140},
}

@article{tran_deep_2018,
	title = {Deep {Logic} {Networks}: {Inserting} and {Extracting} {Knowledge} {From} {Deep} {Belief} {Networks}},
	volume = {29},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Deep {Logic} {Networks}},
	url = {http://ieeexplore.ieee.org/document/7738566/},
	doi = {10.1109/TNNLS.2016.2603784},
	abstract = {Developments in deep learning have seen the use of layerwise unsupervised learning combined with supervised learning for ﬁne-tuning. With this layerwise approach, a deep network can be seen as a more modular system that lends itself well to learning representations. In this paper, we investigate whether such modularity can be useful to the insertion of background knowledge into deep networks, whether it can improve learning performance when it is available, and to the extraction of knowledge from trained deep networks, and whether it can offer a better understanding of the representations learned by such networks. To this end, we use a simple symbolic language—a set of logical rules that we call conﬁdence rules—and show that it is suitable for the representation of quantitative reasoning in deep networks. We show by knowledge extraction that conﬁdence rules can offer a low-cost representation for layerwise networks (or restricted Boltzmann machines). We also show that layerwise extraction can produce an improvement in the accuracy of deep belief networks. Furthermore, the proposed symbolic characterization of deep networks provides a novel method for the insertion of prior knowledge and training of deep networks. With the use of this method, a deep neural–symbolic system is proposed and evaluated, with the experimental results indicating that modularity through the use of conﬁdence rules and knowledge insertion can be beneﬁcial to network performance.},
	language = {en},
	number = {2},
	urldate = {2020-06-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tran, Son N. and d'Avila Garcez, Artur S.},
	month = feb,
	year = {2018},
	pages = {246--258},
}

@article{zhou_revisiting_2018,
	title = {Revisiting the {Importance} of {Individual} {Units} in {CNNs} via {Ablation}},
	url = {http://arxiv.org/abs/1806.02891},
	abstract = {We revisit the importance of the individual units in Convolutional Neural Networks (CNNs) for visual recognition. By conducting unit ablation experiments on CNNs trained on large scale image datasets, we demonstrate that, though ablating any individual unit does not hurt overall classiﬁcation accuracy, it does lead to signiﬁcant damage on the accuracy of speciﬁc classes. This result shows that an individual unit is specialized to encode information relevant to a subset of classes. We compute the correlation between the accuracy drop under unit ablation and various attributes of an individual unit such as class selectivity and weight L1 norm. We conﬁrm that unit attributes such as class selectivity are a poor predictor for impact on overall accuracy as found previously in recent work [11]. However, our results show that class selectivity along with other attributes are good predictors of the importance of one unit to individual classes. We evaluate the impact of random rotation, batch normalization, and dropout to the importance of units to speciﬁc classes. Our results show that units with high selectivity play an important role in network classiﬁcation power at the individual class level. Understanding and interpreting the behavior of these units is necessary and meaningful.},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1806.02891 [cs]},
	author = {Zhou, Bolei and Sun, Yiyou and Bau, David and Torralba, Antonio},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.02891},
}

@inproceedings{subramanya_fooling_2019,
	address = {Seoul, Korea (South)},
	title = {Fooling {Network} {Interpretation} in {Image} {Classification}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010911/},
	doi = {10.1109/ICCV.2019.00211},
	abstract = {Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassiﬁcation. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network’s underlying decision making process.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Subramanya, Akshayvarun and Pillai, Vipin and Pirsiavash, Hamed},
	month = oct,
	year = {2019},
	pages = {2020--2029},
}

@article{kindermans_reliability_2017,
	title = {The ({Un})reliability of saliency methods},
	url = {http://arxiv.org/abs/1711.00867},
	abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step —adding a constant shift to the input data— to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulﬁll input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1711.00867 [cs, stat]},
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.00867},
	keywords = {Statistics - Machine Learning},
}

@article{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classiﬁcation models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv: 1312.6034},
}

@inproceedings{mahendran_understanding_2015,
	address = {Boston, MA, USA},
	title = {Understanding deep image representations by inverting them},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7299155/},
	doi = {10.1109/CVPR.2015.7299155},
	abstract = {Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-theart CNN image representations for the ﬁrst time. Among our ﬁndings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Mahendran, Aravindh and Vedaldi, Andrea},
	month = jun,
	year = {2015},
	pages = {5188--5196},
}

@inproceedings{fong_interpretable_2017,
	address = {Venice},
	title = {Interpretable {Explanations} of {Black} {Boxes} by {Meaningful} {Perturbation}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237633/},
	doi = {10.1109/ICCV.2017.371},
	abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks “look” in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints.},
	language = {en},
	urldate = {2020-06-16},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Fong, Ruth C. and Vedaldi, Andrea},
	month = oct,
	year = {2017},
	pages = {3449--3457},
}

@article{montavon_methods_2018,
	title = {Methods for {Interpreting} and {Understanding} {Deep} {Neural} {Networks}},
	volume = {73},
	issn = {10512004},
	url = {http://arxiv.org/abs/1706.07979},
	doi = {10.1016/j.dsp.2017.10.011},
	abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most eﬃcient use of these techniques on real data. It also discusses a number of practical applications.},
	language = {en},
	urldate = {2020-06-16},
	journal = {Digital Signal Processing},
	author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
	month = feb,
	year = {2018},
	note = {arXiv: 1706.07979},
	keywords = {Statistics - Machine Learning},
	pages = {1--15},
}

@article{doshi-velez_towards_2017,
	title = {Towards {A} {Rigorous} {Science} of {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1702.08608},
	abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
	language = {en},
	urldate = {2020-06-16},
	journal = {arXiv:1702.08608 [cs, stat]},
	author = {Doshi-Velez, Finale and Kim, Been},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.08608},
	keywords = {Statistics - Machine Learning},
}

@article{adadi_peeking_2018,
	title = {Peeking {Inside} the {Black}-{Box}: {A} {Survey} on {Explainable} {Artificial} {Intelligence} ({XAI})},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Peeking {Inside} the {Black}-{Box}},
	url = {https://ieeexplore.ieee.org/document/8466590/},
	doi = {10.1109/ACCESS.2018.2870052},
	abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artiﬁcial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research ﬁeld holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
	language = {en},
	urldate = {2020-06-16},
	journal = {IEEE Access},
	author = {Adadi, Amina and Berrada, Mohammed},
	year = {2018},
	pages = {52138--52160},
}

@article{carlini_evaluating_2019,
	title = {On {Evaluating} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/1902.06705},
	abstract = {Correctly evaluating defenses against adversarial examples has proven to be extremely diﬃcult. Despite the signiﬁcant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.},
	language = {en},
	urldate = {2020-06-15},
	journal = {arXiv:1902.06705 [cs, stat]},
	author = {Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06705},
	keywords = {Computer Science - Cryptography and Security, Statistics - Machine Learning},
}

@article{van_bergen_going_2020,
	title = {Going in circles is the way forward: the role of recurrence in visual inference},
	shorttitle = {Going in circles is the way forward},
	url = {http://arxiv.org/abs/2003.12128},
	abstract = {Biological visual systems exhibit abundant recurrent connectivity. State-of-the-art neural network models for visual recognition, by contrast, rely heavily or exclusively on feedforward computation. Any finite-time recurrent neural network (RNN) can be unrolled along time to yield an equivalent feedforward neural network (FNN). This important insight suggests that computational neuroscientists may not need to engage recurrent computation, and that computer-vision engineers may be limiting themselves to a special case of FNN if they build recurrent models. Here we argue, to the contrary, that FNNs are a special case of RNNs and that computational neuroscientists and engineers should engage recurrence to understand how brains and machines can (1) achieve greater and more flexible computational depth, (2) compress complex computations into limited hardware, (3) integrate priors and priorities into visual inference through expectation and attention, (4) exploit sequential dependencies in their data for better inference and prediction, and (5) leverage the power of iterative computation.},
	language = {en},
	urldate = {2020-06-14},
	journal = {arXiv:2003.12128 [cs, q-bio]},
	author = {van Bergen, Ruben S. and Kriegeskorte, Nikolaus},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.12128},
	keywords = {Quantitative Biology - Neurons and Cognition},
}

@article{noauthor_neural-symbolic_nodate,
	title = {Neural-{Symbolic} {Learning} and {Reasoning}: {Contributions} and {Challenges}},
	abstract = {The goal of neural-symbolic computation is to integrate robust connectionist learning and sound symbolic reasoning. With the recent advances in connectionist learning, in particular deep neural networks, forms of representation learning have emerged. However, such representations have not become useful for reasoning. Results from neural-symbolic computation have shown to offer powerful alternatives for knowledge representation, learning and reasoning in neural computation. This paper recalls the main contributions and discusses key challenges for neural-symbolic integration which have been identified at a recent Dagstuhl seminar.},
	language = {en},
	pages = {4},
}

@article{besold_neural-symbolic_2017,
	title = {Neural-{Symbolic} {Learning} and {Reasoning}: {A} {Survey} and {Interpretation}},
	shorttitle = {Neural-{Symbolic} {Learning} and {Reasoning}},
	url = {http://arxiv.org/abs/1711.03902},
	abstract = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
	language = {en},
	urldate = {2020-06-14},
	journal = {arXiv:1711.03902 [cs]},
	author = {Besold, Tarek R. and Garcez, Artur d'Avila and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.03902},
}

@article{hailesilassie_rule_2016,
	title = {Rule {Extraction} {Algorithm} for {Deep} {Neural} {Networks}: {A} {Review}},
	volume = {14},
	abstract = {Despite the highest classification accuracy in wide varieties of application areas, artificial neural network has one disadvantage. The way this Network comes to a decision is not easily comprehensible. The lack of explanation ability reduces the acceptability of neural network in data mining and decision system. This drawback is the reason why researchers have proposed many rule extraction algorithms to solve the problem. Recently, Deep Neural Network (DNN) is achieving a profound result over the standard neural network for classification and recognition problems. It is a hot machine learning area proven both useful and innovative. This paper has thoroughly reviewed various rule extraction algorithms, considering the classification scheme: decompositional, pedagogical, and eclectics. It also presents the evaluation of these algorithms based on the neural network structure with which the algorithm is intended to work. The main contribution of this review is to show that there is a limited study of rule extraction algorithm from DNN.},
	language = {en},
	number = {7},
	author = {Hailesilassie, Tameru},
	year = {2016},
	pages = {6},
}

@article{futia_integration_2020,
	title = {On the {Integration} of {Knowledge} {Graphs} into {Deep} {Learning} {Models} for a {More} {Comprehensible} {AI}—{Three} {Challenges} for {Future} {Research}},
	volume = {11},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/11/2/122},
	doi = {10.3390/info11020122},
	abstract = {Deep learning models contributed to reaching unprecedented results in prediction and classiﬁcation tasks of Artiﬁcial Intelligence (AI) systems. However, alongside this notable progress, they do not provide human-understandable insights on how a speciﬁc result was achieved. In contexts where the impact of AI on human life is relevant (e.g., recruitment tools, medical diagnoses, etc.), explainability is not only a desirable property, but it is -or, in some cases, it will be soon-a legal requirement. Most of the available approaches to implement eXplainable Artiﬁcial Intelligence (XAI) focus on technical solutions usable only by experts able to manipulate the recursive mathematical functions in deep learning algorithms. A complementary approach is represented by symbolic AI, where symbols are elements of a lingua franca between humans and deep learning. In this context, Knowledge Graphs (KGs) and their underlying semantic technologies are the modern implementation of symbolic AI—while being less ﬂexible and robust to noise compared to deep learning models, KGs are natively developed to be explainable. In this paper, we review the main XAI approaches existing in the literature, underlying their strengths and limitations, and we propose neural-symbolic integration as a cornerstone to design an AI which is closer to non-insiders comprehension. Within such a general direction, we identify three speciﬁc challenges for future research—knowledge matching, cross-disciplinary explanations and interactive explanations.},
	language = {en},
	number = {2},
	urldate = {2020-06-14},
	journal = {Information},
	author = {Futia, Giuseppe and Vetrò, Antonio},
	month = feb,
	year = {2020},
	pages = {122},
}

@article{townsend_extracting_2019,
	title = {Extracting {Relational} {Explanations} {From} {Deep} {Neural} {Networks}: {A} {Survey} {From} a {Neural}-{Symbolic} {Perspective}},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Extracting {Relational} {Explanations} {From} {Deep} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8889997/},
	doi = {10.1109/TNNLS.2019.2944672},
	abstract = {The term “explainable AI” refers to the goal of producing artiﬁcially intelligent agents that are capable of providing explanations for their decisions. Some models (e.g., rulebased systems) are designed to be explainable, while others are less explicit “black boxes” for which their reasoning remains a mystery. One example of the latter is the neural network, and over the past few decades, researchers in the ﬁeld of neuralsymbolic integration (NSI) have sought to extract relational knowledge from such networks. Extraction from deep neural networks, however, has remained a challenge until recent years in which many methods of extracting distinct, salient features from input or hidden feature spaces of deep neural networks have been proposed. Furthermore, methods of identifying relationships between these features have also emerged. This article presents examples of old and new developments in extracting relational explanations in order to argue that the latter have analogies in the former and, as such, can be described in terms of long-established taxonomies and frameworks presented in early neural-symbolic literature. We also outline potential future research directions that come to light from this refreshed perspective.},
	language = {en},
	urldate = {2020-06-14},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Townsend, Joe and Chaton, Thomas and Monteiro, Joao M.},
	year = {2019},
	pages = {1--15},
}

@article{lamb_graph_2020,
	title = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}: {A} {Survey} and {Perspective}},
	shorttitle = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}},
	url = {http://arxiv.org/abs/2003.00330},
	abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNNs) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientiﬁc domains. The need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as their relationship to current developments in neural-symbolic computing.},
	language = {en},
	urldate = {2020-06-14},
	journal = {arXiv:2003.00330 [cs]},
	author = {Lamb, Luis C. and Garcez, Artur and Gori, Marco and Prates, Marcelo and Avelar, Pedro and Vardi, Moshe},
	month = may,
	year = {2020},
	note = {arXiv: 2003.00330},
}
