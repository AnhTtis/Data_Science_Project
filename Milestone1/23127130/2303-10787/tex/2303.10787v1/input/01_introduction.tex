\section{Introduction}
Document creation involves many steps from generating textual content, organizing additional media, and producing a layout that makes the information comprehensible. Layout generation is a key step in document creation. Layouts differ to best convey the appropriate kind of information for different domains of documents. To model many different domains of document layouts, general yet powerful methods need to be developed. We undertake that goal in this paper.  

Methodological and domain considerations for layout generation have arisen as a topic of interest recently in the Computer Vision and Machine Learning communities~\cite{layoutgan,patil2020read,gupta2021layouttransformer}. Fixed length generative methods leveraging adversarial training were shown effective at producing realistic but limited documents in LayoutGAN~\cite{layoutgan}. In READ~\cite{patil2020read} an approach to resolve shortcomings of the prior work was developed, namely permitting more complex structures in the layout, as well as introducing document metric considerations in the generative model literature. This work proposed a recursive autoencoder to iteratively extend the layout based on past generated layouts. However, READ~\cite{patil2020read} relies on a hierarchical document model that may not apply well in a wide range of document types, and is dependent on hyperparameters that dictate document layout length.  This work can be seen as a segway to autoregressive methods such as Layout Transformer~\cite{gupta2021layouttransformer} where the modern generative techniques of language modeling have been applied to the field. Autoregressive generation is outlined in more detail below. This method allows for adaptive stopping that comes from the encoder states themselves, which change progressively during the generation. The result is simpler and improved modeling of layouts.

In this paper, we develop a new approach to layout generation using the recently emerging area of diffusion probabilistic models. The key idea is that when a diffusion process consists of small steps of Gaussian noise conditioned on the data, then the reversing process can be approximated by a conditional Gaussian as well. To use the conventional diffusion methods for discrete sequence generation, rounding and embedding steps have to be introduced~\cite{DiffusionLM2022}. 

Our main contributions are introducing a novel document comparison metric with several useful properties and being the first work to employ discrete-sequence diffusion for layout generation. We show that the proposed metric behaves well qualitatively and quantitatively by comparing the performance of different algorithms on well-known datasets. We also show how synthetic layout training data compares to real document data on an end-to-end task: layout detection. We compare across different synthetic data generation algorithms by the mean average precision (mAP) of a trained layout detector of a fixed architecture. Finally, we provide ablation of the proposed method to several variables.

