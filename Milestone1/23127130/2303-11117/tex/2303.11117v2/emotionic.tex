%-----------------------------------------------------------------------
% Template File for Science China Information Sciences
% Downloaded from http://scis.scichina.com
% Please compile the tex file using LATEX or PDF-LATEX or CCT-LATEX
%-----------------------------------------------------------------------

\documentclass{SCIS2019}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Author's definitions for this manuscript
%%% 作者附加的定义
%%% 常用环境已经加载好, 不需要重复加载
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{multirow}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
% \usepackage[numbers]{natbib}
\usepackage[numbers,sort&compress]{natbib}
% \usepackage[sort&compress]{gbt7714}
% \nocite{*}
\setlength{\bibsep}{0.1ex}    %vertical spacing between references

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Begin. 开始
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%\oa
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Authors do not modify the information below
%%% 作者不需要修改此处信息
\ArticleType{RESEARCH PAPER}
%\SpecialTopic{}
\Year{2023}
\Month{}
\Vol{}
\No{}
\DOI{}
\ArtNo{}
\ReceiveDate{}
\ReviseDate{}
\AcceptDate{}
\OnlineDate{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% title: 标题
%%%   \title{title}{title for citation}
\title{EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation}{EmotionIC: emotional inertia and contagion-driven dependency modelling for emotion recognition in conversation}

%%% Corresponding author: 通信作者
%%%   \author[number]{Full name}{{email@xxx.com}}
%%% General author: 一般作者
%%%   \author[number]{Full name}{}
\author[]{Yingjian LIU}{}
% \author[]{Jiang LI}{}
\author[]{Jiang LI$^\star$}{lijfrank@hust.edu.cn (Li J)}
\author[]{Xiaoping WANG$^\star$}{wangxiaoping@hust.edu.cn (Wang X P)}
\author[]{Zhigang ZENG}{}

%%% Author information for page head. 页眉中的作者信息
\AuthorMark{Liu Y J, Li J}

%%% Authors for citation. 首页引用中的作者信息
\AuthorCitation{Liu Y J, Li J, Wang X P, et al.}

%%% Authors' contribution. 同等贡献
\contributions{Yingjian LIU and Jiang LI have the same contribution to this work.}

%%% Address. 地址
%%%   \address[number]{Affiliation, City {\rm Postcode}, Country}
\address[]{School of Artificial Intelligence and Automation,\\
Key Laboratory of Image Processing and Intelligent Control of Education Ministry of China, \\
Huazhong University of Science and Technology, Wuhan 430074, China}
% \address[2]{Affiliation, City {\rm 000000}, Country}
% \address[3]{Affiliation, City {\rm 000000}, Country}

%%% Abstract. 摘要
\abstract{Emotion Recognition in Conversation (ERC) has attracted growing attention in recent years as a result of the advancement and implementation of human-computer interface technologies. However, previous approaches to modeling global and local context dependencies lost the diversity of dependency information and do not take the context dependency into account at the classification level. In this paper, we propose a novel approach to dependency modeling driven by Emotional Inertia and Contagion (EmotionIC) for conversational emotion recognition at the feature extraction and classification levels. At the feature extraction level, our designed Identity Masked Multi-head Attention (IM-MHA) captures the identity-based long-distant context in the dialogue to contain the diverse influence of different participants and construct the global emotional atmosphere, while the devised Dialogue-based Gate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of dyadic dialogue is applied to refine the contextual features with inter- and intra-speaker dependencies. At the classification level, by introducing skip connections in Conditional Random Field (CRF), we elaborate the Skip-chain CRF (SkipCRF) to capture the high-order dependencies within and between speakers, and to emulate the emotional flow of distant participants. Experimental results show that our method can significantly outperform the state-of-the-art models on four benchmark datasets. The ablation studies confirm that our modules can effectively model emotional inertia and contagion.}

%%% Keywords. 关键词
\keywords{emotion recognition in conversation, emotional inertia and contagion, multi-head attention, gated recurrent unit, skip connection, conditional random field}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% The main text. 正文部分
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Emotion recognition in conversation (ERC) is one of the most focusing research fields in natural language processing (NLP), which aims to identify the emotion of each utterance in a conversation. This task has recently received considerable attention from NLP researchers due to its potential applications in multiple domains such as opinion mining in social media~\cite{8267597}, empathic dialogue system construction~\cite{majumder2019dialoguernn, lin2019moel}, and smart home systems~\cite{young2018augmenting}.
Emotions are often reflected in interpersonal interactions, and analyzing the emotions of a single utterance out of the conversational context may lead to ambiguity~\cite{zhou2018emotional}. Therefore, ERC incorporating conversational context information significantly contributes to model performance. %rashkin2018towards,

The effective use of contextual information in dialogues lies at the heart of ERC~\cite{poria2019emotion}. There are numerous efforts have been developed to encode the contextual information in the dialogue, including graph-based methods~\cite{ghosal2019dialoguegcn,shen-etal-2021-directed,li2022graphcfc}, recurrence-based methods~\cite{majumder2019dialoguernn, jiao2019higru,Ghosal2020}, and attention-based methods~\cite{zhong2019knowledge,zhu2021kat,li2022ga2mif}. Li et al.~\cite{li2021past} proposed a psychological-knowledge-aware interaction graph, which established four relations in a local connectivity graph to simulate the psychological state of the speaker. Hu et al.~\cite{hu2021dialoguecrn} designed a multi-turn reasoning module based on the Recurrent Neural Network (RNN), which iteratively performed the intuitive retrieval process and conscious reasoning process to extract and integrate emotional cues from a cognitive perspective. Zhu et al.~\cite{zhu2021kat} proposed a topic-driven and knowledge-aware Transformer model that incorporated topic representation and the commonsense knowledge from ATOMIC for emotion detection in dialogues. %, hu2022mm,Mao2020DialogueTRMET

However, both graph-based and recurrence-based methods tend to use only relatively limited information from recent utterances to update the state of current utterances, which makes it difficult to achieve satisfying performance. Attention-based approaches ignore the importance of neighboring utterances because of their global relevance and do not perform well in modeling speaker-specific contexts. In addition, the above methods only consider the context dependencies in the feature extraction level, ignoring the significant dependencies among the emotional label sequences in the dialogue.

According to the above analysis, a better way to solve ERC is to combine the strengths of attention-based methods and recurrence-based models. Despite weak influence on the current utterance, long-distance contextual information implies the global atmosphere, which can assist the classification of utterances without explicit emotions. Therefore, we take advantage of the global perception of attention mechanism to build the global emotional atmosphere. Besides, the modeling of interpersonal dependency in proximity often reflects emotional inertia and contagion, which decreases with time. So we employ gate recurrent unit (GRU)~\cite{chung2014empirical} to combine the emotional inertia of the current speaker with the emotional contagion of the corresponding interlocutor. In addition to the context dependence at the feature extraction level, the sequence of emotional labels in conversations is significantly dependent. Therefore, with the help of the validity of conditional random field (CRF) for label dependence modeling, we design a novel structure based on self-dependence and others-dependence to build the consistency of emotional labels. Specifically, we introduce skip connections in the CRF to realize the high-order dependencies modeling within and between speakers and complete the final emotional classification.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=3.0in]{firstCase.pdf}
	\caption{The dependencies of feature extraction level and classification level in dialogue. The solid and dashed lines represent the transmission of information between different speakers and within the same speaker, respectively.}
	\label{fig:firstCase}
\end{figure}
In this paper, we propose a novel approach to emotional inertia and contagion-driven dependency modelling named EmotionIC. The proposed model explicitly constructs an emotion flow graph from the feature extraction level and the classification level, respectively, to fully account for inter- and intra-speaker contextual emotional dependencies. Figure~\ref{fig:firstCase} shows the dependencies of feature extraction level and classification level. At the feature extraction level, we design a speaker-based identity masked multi-head attention (IM-MHA), which captures dependencies within and between the speaker in the distant utterances from two features subspaces, respectively, to build the global emotional atmosphere. In addition, to refine the contextual features with inter- and intra-speaker dependencies, we devise a speaker- and position-aware dialogue GRU (DialogGRU). The crafted DialogGRU introduces the emotional tendencies of the current speaker and the corresponding interlocutor at the previous moment into a single GRU cell. At the classification level, we elaborate a novel structure called Skip-chain CRF (SkipCRF) to explicitly model emotional inertia and contagion and thus obtain the globally optimal emotional label sequence. SkipCRF can combine the global historical information and the local speaker dependency to deal with the complex interaction of different participants in the dialogue. Experimental results demonstrate the superiority of our model compared with state-of-the-art models, and several studies are conducted to illustrate the effectiveness of each module of EmotionIC.

Our main contributions are summarized as follows: %\textbf{(1)} We propose a novel emotional inertia- and contagion-driven model called EmotionIC for conversational emotion recognition. EmotionIC can establish context dependencies at the feature extraction and classification levels and consists of three main components, i.e., IM-MHA, DialogGRU, and SkipCRF. \textbf{(2)} At the feature extraction level, the devised IM-MHA can effectively preserve the diversity of the influence of different participants and construct global emotional atmosphere with the ability of multi-headed attention that can capture long-distant context, and our designed DialogGRU is combined with emotional decay for refinement modeling local dependencies. \textbf{(3)} At the classification level, the elaborate SkipCRF can simulate the flow of emotion among different speakers in the conversation and obtain the globally optimal emotional label sequence, which takes into account the high-order dependencies of inter- and intra-speaker in emotion labels. \textbf{(4)} We perform extensive experiments on the IEMOCAP, Dailydialog, MELD, and EmoryNLP datasets and obtain the most advanced performance, which demonstrates the superiority of the proposed method.
\begin{itemize}
	\item[\ding{172}] We propose a novel emotional inertia- and contagion-driven model called EmotionIC for conversational emotion recognition. EmotionIC can establish context dependencies at the feature extraction and classification levels and consists of three main components, i.e., IM-MHA, DialogGRU, and SkipCRF.
	\item[\ding{173}] At the feature extraction level, the devised IM-MHA can effectively preserve the diversity of the influence of different participants and construct global emotional atmosphere with the ability of multi-headed attention that can capture long-distant context, and our designed DialogGRU is combined with emotional decay for refinement modeling local dependencies.
	\item[\ding{174}] At the classification level, the elaborate SkipCRF can simulate the flow of emotion among different speakers in the conversation and obtain the globally optimal emotional label sequence, which takes into account the high-order dependencies of inter- and intra-speaker in emotion labels.
	\item[\ding{175}] We perform extensive experiments on the IEMOCAP, Dailydialog, MELD, and EmoryNLP datasets and obtain the most advanced performance, which demonstrates the superiority of the proposed method.
\end{itemize}

The rest of this paper is organized as follows. Section~\ref{sec:related_work} presents a brief review of related works. Section~\ref{sec:our_approach} and Section~\ref{sec:experimental_settings} detail our proposed model and experimental settings, respectively. Section~\ref{sec:results_analysis} provides the corresponding experimental results and analysis. 
Finally, in Section~\ref{sec:conclusion}, we summarize this paper and discuss possible future work.


\section{Related Works}\label{sec:related_work}
Emotion recognition is an interdisciplinary field of research, with contributions from different fields such as natural language processing, psychological cognitive science, etc~\cite{picard2010affective}. In this section, we mainly introduce the related works of emotion recognition in conversation and conditional random field. Moreover, we briefly introduce the application of CRF in ERC tasks.

\subsection{Emotion Recognition in Conversation}
Distinct from traditional emotion recognition which treats emotion as a static state, ERC takes full consideration of emotion to be dynamic and flow between speaker interactions. Hazarika et al. \cite{hazarika2018icon} proposed a LSTM-based model to enable current utterance to capture contextual information in historical conversations. CMN \cite{hazarika2018conversational} employed a skip attention mechanism to merge contextual information in a historical conversation. Jiao et al.~\cite{jiao2019higru} proposed a hierarchical GRU to address the difficulty of capturing long-distance contextual information effectively. By distinguishing specific speakers, DialogueRNN~\cite{majumder2019dialoguernn} modeled emotions dynamically based on the current speaker, contextual content, and emotional state. Zhong et al.~\cite{zhong2019knowledge} proposed Knowledge-Enriched Transformer, which dynamically exploited external commonsense knowledge through hierarchical self-attention and context-aware graph attention. By building directed graphical structures over the input utterance sequences with speaker information, DialogueGCN~\cite{ghosal2019dialoguegcn} applied graph convolution network to construct inter- and intra-dependencies among distant utterances. COSMIC~\cite{Ghosal2020} combined different commonsense knowledge and learned the interaction between the interlocutors in the dialogue. DialogXL~\cite{shen2021dialogxl} modified the memory block in XLNet~\cite{yang2019xlnet} to store longer historical contexts and conversation-aware self-attention to handle multi-party structures. Wang et al.~\cite{wang2020relational} proposed a relational graph attention network to encode the tree structure for sentiment prediction. DAG-ERC~\cite{shen-etal-2021-directed} treated the internal structure of dialogue as a directed acyclic graph, which intuitively model the way information flows between long and short distance contexts. Considering that utterances with similar semantics may have distinctive emotions under different contexts, CoG-BART~\cite{li2022contrast} adopted supervised contrastive learning to enhance the model's ability to handle context information. GAR-Net~\cite{xu2022gar} was an end-to-end graph attention reasoning network that took both word-level and utterance-level context into concern, aiming to emphasize the importance of contextual reasoning. 

\subsection{Conditional Random Field}
Conditional random fields (CRFs)~\cite{lafferty2001} are a class of probabilistic graphical modeling methods that aim to model the conditional distribution $\mathcal{P}_{(Y|X)}$ by a set of observed variables $X$, and another set of unobserved variables $Y$, and the structural information among different variables. In addition, CRF relaxes the strong dependency assumptions in other Bayesian models based on directed graphical models and enables the establishment of higher-order dependency, which means that the result of CRF is closer to the real distribution of data~\cite{li2020building}. CRF has recently attracted the interest of researchers in the ERC field~\cite{wang2020contextualized, song2022emotionflow, liang2021s+}, and these methods demonstrate the effectiveness of CRF for modeling emotion dependency at the classification level. CRF utilizes the potential function with clusters on the graph structure to define the conditional probability $\mathcal{P}_{(Y|X)}$. Since there are primarily two types of clusters on labels in the linear-chain CRF frequently employed in ERC models, two forms of exponential potential functions are added as feature functions. $\mathcal{P}_{(Y|X)}$ is defined formally as:
\begin{equation}
	\resizebox{.91\linewidth}{!}{$
	\displaystyle
	% \begin{aligned}
		\mathcal{P}_{(Y|X)} = \frac{1}{\mathbf{Z}(X)}\exp\big[\sum_{n,t}\lambda_n \mathbf{g}_n(y_{t-1},y_t,X,t)+\sum_{l,t}\mu_l \mathbf{f}_l(y_t,X,t)\big]
		=  \frac{1}{\mathbf{Z}(X)}\exp\sum_{k,t}\omega_k\mathbf{F}_k(y_{t-1},y_t,X,t),
	% \end{aligned}
	$}
\label{eq:linercrf}
\end{equation}
where $\mathbf{Z}(X)$ represents normalization factor; $\mathbf{F}_k(\cdot)$ is feature function of linear-chain CRF, which consists of the local feature function $\mathbf{g}_n(\cdot)$ and the nodal feature function $\mathbf{f}_l(\cdot)$; $\mathbf{g}_n(\cdot)$ is defined on the context-connected edge of node $Y$, $\mathbf{f}_l(\cdot)$ is defined on node $Y$; $\omega_k$ consists of $\lambda_n$ and $\mu_l$, which are learnable weights of the corresponding feature function. In the existing ERC methods for modeling this dependency, only the linear-chain CRF with first-order dependency is applied, i.e., only the dependency between neighbor labels are considered. This simple form is difficult to cope with complex interaction situations of different participants in dialogue scenarios. So we construct SkipCRF that introduces high-order dependency through skip-chain connections to model emotional inertia and contagion at the classification level.

\section{Our Approach}\label{sec:our_approach}
In this section, we will introduce the main components of our approach. First, we present the problem definitions and make certain transformations of the original problem according to the requirements of the proposed model. Then, we illustrate the architecture of our model as in Figure~\ref{fig:allmodel}, which contains three components: (1) Identity Masked Multi-head Attention (IM-MHA), which captures long-distance historical information to build the global emotional atmosphere; (2) Dialogue Gated Recurrent Units (DialogGRU), which focuses on the local inter- and intra-speaker dependencies of the current utterance; (3) Skip-chain Conditional Random Field (SkipCRF), which explicitly simulates the emotional interaction at the classification level after fusing global and local contextual features to obtain the globally optimal sequence of emotion labels. In the following subsections, the key elements in these three components are described in detail.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=6.0in]{AllModel_5.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
	\caption{The architecture of our EmotionIC. Firstly, the global and local context dependencies are extracted through IM-MHA and DialogGRU, respectively. Then, we concatenate global context features, local context features and utterance features. Note that combining utterance features that are not processed by IM-MHA and DialogGRU is to prevent the original semantics of utterances with weak context dependencies from being obscured. Finally, the emotional inertia and contagion are modeled at the classification level based on SkipCRF to obtain the globally optimal emotional label sequence.}
	\label{fig:allmodel}
\end{figure}

\subsection{Preliminaries}
\textbf{Problem Definition.} The ERC task is to predict the corresponding emotion label $y_t$ for the $t$-th utterance $u_{t}^{p_i}$ in a sequence $\{u_{1}^{p_1}, u_{2}^{p_2},\cdots,u_{\mathcal{T}}^{p_m}\}$ from the conversation $C$ containing $m$ participants, where $p_i\in \{p_1,p_2,\cdots,p_m\}$ is the speaker identity, $t$ describes the order of the utterance and also represents the moment corresponding to the current utterance, $\mathcal{T}$ represents the length of conversation $C$. 

\textbf{Speaker-specific Utterance Block.} Considering the continuous utterances of the same participant in the conversation as a speaker-specific utterance block, the original utterances sequence can be divided into multiple blocks sequence $\{b_{1}^{p_1}, b_{2}^{p_2},\cdots, b_{\mathcal{B}}^{p_m}\} (\mathcal{B}\le \mathcal{T})$, where $\mathcal{B}$ represents the number of utterance block. Note that a speaker-specific utterance block may contain one or more original utterances.

\textbf{Utterance Moment Function.} To explicitly capture emotional interactions between participants, two utterance moment functions (i.e., $\mathbf{s}(\cdot)$ and $\mathbf{o}(\cdot)$) are defined in conversation. Here, $\mathbf{s}(\cdot)$ is utilized to output the moment of the previous utterance that belongs to the same speaker as the current utterance and is the nearest to the current utterance, and $\mathbf{o}(\cdot)$ is employed to output the moment of the previous utterance that is spoken by the interlocutor and is the nearest to the current utterance. For instance, assuming the current utterance $u_{t}^{p_i}$ exists, then $u_{\mathbf{s}(t)}^{p_i}$ represents the nearest and previous utterance uttered by the speaker $p_i$, and $u_{\mathbf{o}(t)}^{p_j}$ indicates the most recent utterance spoken by the interlocutor $p_j$.

\textbf{Emotional Inertia and Emotional Contagion.} In addition to context dependency, speaker identity information is also shown to be critical to ERC~\cite{lili2020}. Research results in the field of psychology analyzed how emotions are transmitted in conversations~\cite{Hatfield1993}. Specifically, the transmission of emotion in interpersonal communication and dialogue is mainly driven by two factors: emotional inertia which means that a speaker in a conversation tends to maintain a particular emotional state (i.e., intra-speaker dependency), and emotional contagion which describes the emotional stimulation of other participants' utterances (i.e., inter-speaker dependency). In this paper, these two factors run through our methodology.

\subsection{Identity Masked Multi-head Attention}
Taking advantage of the ability of global context-awareness in conversation, attention mechanism is widely applied in many ERC models~\cite{zhu2021kat,zhong2019knowledge}. However, these approaches didn't explicitly encode speaker identity information. In this subsection, we design a new identity masked multi-head attention (IM-MHA) that can effectively combine the identity information of participants to capture inter- and intra-speaker long-distance dependencies in dialogue. The devised IM-MHA can distinguish the contextual information within and between speakers to maintain the diversity of the influence of different participants on the current utterance in the conversation.

\begin{figure}[htbp]
	\begin{minipage}[c]{0.38\textwidth}
		\centering
		\includegraphics[width=2.2in]{Masks_2.pdf}
		\caption{IM-MHA. $M_s$ and $M_o$ are two mask matrices, which mask the effects of other participants and current speaker, respectively, while also masking future information. $\mathbf{Mask}$ denotes the masking operation.} %$\mathbf{MatMul}$ indicates the matrix multiplication operation. 
		\label{fig:maskcase}
	\end{minipage}
	\hspace{0.02\textwidth}
	\begin{minipage}[c]{0.58\textwidth}
	\centering
	\captionsetup[subfigure]{font={scriptsize}}
	\subfloat[]{\includegraphics[width=1.45in]{dialogue-GRU.pdf}
		\label{fig:dialogGRU}
%		\captionsetup{font={small}}
	}
	\subfloat[]{\includegraphics[width=2.1in]{DialogCase.pdf}	
		\label{fig:dialogcase}
	}	
	\caption{Illustration of DialogGRUs. (a) An single DialogGRU cell. Here, $x^{p_i}_{t}$, $h^{p_i}_{t}$, $h^{p_i}_{\mathbf{s}(t)}$, and $h^{p_j}_{\mathbf{o}(t)}$ are the vector representation of the utterance $u_t^{p_i}$, the hidden state of the current moment, the emotional hidden state of the speaker $p_i$, and the hidden state of the corresponding interlocutor $p_j$, respectively. (b) An example of DialogGRU. The dotted line and solid line represent self-dependence and others-dependence, respectively, and the thickness indicates the strength of dependency adding decay factor.}
	\label{fig:dialoggrus}
	\end{minipage}
\end{figure}
As shown in the left of Figure~\ref{fig:maskcase}, we employ two mask matrices (i.e., $M_s$ and $M_o$) to capture intra- and inter-speaker contextual information, respectively. Here, $M_s$ is utilized to mask the influence of other participants on the current utterance (i.e., only the mutual influence of the same participant's utterances is retained), and $M_o$ is implemented to mask the influence of current speaker on the current utterance (i.e., only the mutual influence of different participants' utterances is maintained). Note that although we draw on multi-headed self-attention~\cite{vaswani2017attention}, we treat utterances as inputs to IM-MHA rather than tokens. For the capture of intra-speaker contextual information, we first matrix-multiply the query utterance and the key utterance, and then operate the resulting result with the matrix $M_s$ for masking. In the actual implementation, we perform the untied absolute position encoding before the mask operation by referring to Ke et al.~\cite{KeG2020rethink}. Similar steps as above can be adopted to capture the inter-speaker contextual information. Finally, the captured intra-speaker and inter-speaker contextual information is summed, which in turn is passed through the softmax layer to obtain the attention matrix of IM-MHA. The whole process can be seen in the right of Figure~\ref{fig:maskcase} and can be described with the following formulation:
\begin{equation}
	% \begin{aligned}
	{\rm{IM}}\text{-}{\rm{Attn}}= {\mathbf{Softmax}}\big((Q_{s,u} K_u^\top + Q_{s,p} K_p^\top) \odot M_s
	+ (Q_{o,u} K_u^\top + Q_{o,p} K_p^\top) \odot  M_o\big),
	% \end{aligned}
	\end{equation}
where $Q_{s,u}$ and $Q_{o,u}$ represent the utterance query matrices, which are the results of mapping the utterances to different subspaces; $Q_{s,p}$ and $Q_{o,p}$ indicate the position query matrices; $K_w$ and $K_p$ are the utterance key matrices, which are the results of mapping the utterances to the same subspace; $\odot$ stands for element-wise product. The obtained IM-MHA attention matrix is matrix multiplied with the value utterance matrix to obtain the final output of IM-MHA. In addition, inspired by the excellent architecture of Transformer~\cite{vaswani2017attention}, we also pass the output of IM-MHA through the residual, the normalization, and the feedforward layers.

\subsection{Dialogue Gated Recurrent Units}
IM-MHA captures the historical information of all participants to construct the global emotional atmosphere, while ignoring the emotional inertia of the current speaker and the emotional contagion of the corresponding interlocutor in the local context. Consequently, we design a new dialogue-based network named DialogGRU with reference to the structure of GRU~\cite{chung2014empirical} to aggregate the inter- and intra-speaker dependencies in the DialogGRU cell. The architecture of single DialogGRU cell is shown in Figure~\ref{fig:dialogGRU}.

% \begin{figure}[htbp]
% 	\centering
% 	\includegraphics[width=2.5in]{dialogue-GRU.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
% 	\caption{The illustration of single Dialogue Gated Recurrent Unit cell. Here, $x^{p_i}_{t}$, $h^{p_i}_{t}$, $h^{p_i}_{\mathbf{s}(t)}$, and $h^{p_j}_{\mathbf{o}(t)}$ are the vector representation of the utterance $u_t^{p_i}$, the hidden state of the current moment, the emotional hidden state of the speaker $p_i$, and the hidden state of the corresponding interlocutor $p_j$, respectively.}
% 	\label{fig:dialogGRU}
% \end{figure}
% \begin{figure}[htbp]
% 	\centering
% 	\includegraphics[width=3.4in]{DialogCase.pdf}
% 	\caption{An example of DialogGRU. The dotted line and solid line represent self-dependence and others-dependence, respectively, and the thickness indicates the strength of dependency adding decay factor.}
% 	\label{fig:dialogcase}
% \end{figure}

DialogGRU sets the self-dependence and others-dependence reset gates. Firstly, a single DialogGRU cell calculates the corresponding forgetting degrees (i.e., reset gates) $s_t$ and $r_t$ based on the hidden state $h^{p_i}_{\mathbf{s}(t)}$ of self-context and the hidden state $h^{p_j}_{\mathbf{o}(t)}$ of others-context, respectively. Here, $s_t$ approaches $1$ indicates strong emotional inertia, while $r_t$ approaches $1$ indicates strong emotional contagion. Then, the candidate hidden state $\tilde{h}_t$ and update gate $z_t$ are generated by the joint calculation of $s_t$ and $r_t$. Finally, the hidden state $h_{t}^{p_i}$ is obtained by fusing candidate hidden states $\tilde{h}_t$ and $h^{p_i}_{\mathbf{s}(t)}$ based on update gate $z_t$. A single DialogGRU cell can be formalized as follows:
\begin{equation}
	\begin{aligned}
		&r_t 		= \sigma(W_{r}[x^{p_i}_{t}; \; h^{p_j}_{\mathbf{o}(t)}] + b_{r}), \ 
		s_t 		= \sigma(W_{s}[x^{p_i}_{t}; \; h^{p_i}_{\mathbf{s}(t)}] + b_{s}), \ 
		z_t 		= \sigma(W_{z}[x^{p_i}_{t}; \; h^{p_i}_{\mathbf{s}(t)}; \; h^{p_j}_{\mathbf{o}(t)}] + b_{z}), \\
		&\tilde{h}_t = \tanh(W_{h}[x^{p_i}_{t}; \; s_t \odot h^{p_i}_{\mathbf{s}(t)}; \; r_t\odot h^{p_j}_{\mathbf{o}(t)}] + b_h), \ 
		h^{p_i}_t 		= (1 - z_t)\odot h^{p_i}_{\mathbf{s}(t)} + z_t\odot\tilde{h}_t,
	\end{aligned}
	\label{eq:gru}
	\end{equation}
where $\sigma$ is sigmoid function, $[\cdot;\cdot]$ denotes concatenation operation; $r_t$, $s_t$, and $z_t$ are the weights of self-dependence reset gate, others-dependence reset gate, and update gate, respectively; the remaining $W_r$, $W_s$, $W_z$, $W_h$, $b_r$, $b_s$, $b_z$, and $b_h$ are the learnable parameters.

In addition, as shown in Figure~\ref{fig:dialogcase}, a speaker-specific utterance block of the speaker in a conversation may contain multiple utterances. If $h_{\mathbf{s}(t)}^{p_i}$ and $h_{\mathbf{o}(t)}^{p_j}$ are equally exerted on each DialogGRU cell, the others-dependence will dominate the hidden state $h_t^{p_i}$ of the current moment. Inspired by ECM~\cite{zhou2018emotional}, we apply two exponential decay factors (i.e., $\beta_{s,t}$ and $\beta_{o,t}$) for self-dependence and others-dependence according to the time interval between two utterances. Our aim is that the longer the interval, the smaller the decay factor. The formula is as follows:
\begin{equation}
	\resizebox{.91\linewidth}{!}{$
	\begin{aligned}
		&\beta_{s,t}   = \frac{1}{1+\exp\left[{(t-\mathbf{s}(t)-\mu)}/{\gamma}\right]},\ 
		\beta_{o,t}   = \frac{1}{1+\exp\left[{(t-\mathbf{o}(t)-\mu)}/{\gamma}\right]},\ 		
		h_t^{p_i} = \mathbf{DialogGRU}\left(x_t^{p_i}, (\beta_{s,t})h_{\mathbf{s}(t)}^{p_i}, (\beta_{o,t})h_{\mathbf{o}(t)}^{p_j}\right),
	\end{aligned}
	$}
\end{equation}
where $\mu$ and $\gamma>0$ are the position and shape hyperparameter, which are utilized to control the speed of emotional decay; $\mathbf{DialogGRU}(\cdot)$ is a simplified function of Equation~\ref{eq:gru}.

\subsection{Skip-chain Conditional Random Field}
At the feature extraction level, the global contextual features extracted by IM-MHA and the local contextual features extracted by DialogGRU are combined to obtain identity-enhanced utterance features. Considering that there are significant dependencies between emotion labels, we explicitly model the interaction of emotion labels at the classification level leveraging CRF to capture the flow of emotions from different participants as well as to obtain the globally optimal label sequence.

Because of the complexity of computing the normalized factor in the graphical model, only first-order dependency is commonly exploited in existing ERC methods~\cite{wang2020contextualized, song2022emotionflow}. In other words, only the dependency between neighbor labels is taken into account. This simple processing makes it challenging to capture the complex interaction information of distinct speakers in the dialogue, while not clearly expressing the meaning represented by the non-normalized transition probability matrix in CRF. Therefore, by introducing higher-order dependency into the traditional linear-chain CRF, we elaborate a novel strategy named SkipCRF to model emotional inertia and contagion in emotion labels and enhance the accuracy of emotion classification.

According to the relevant settings of the CRF, $X=(x_1,\cdots,x_{\mathcal{T}})$ is defined as the utterance feature extracted by encoder, $Y=(y_1,\cdots,y_{\mathcal{T}})$ as the corresponding emotional label. Referring to Equation~\ref{eq:linercrf}, $\mathbf{g}_n$ represents the contextual local feature function, and $\mathbf{f}_l$ is the nodal feature function. Subject to Markov property, the contextual local feature function $\mathbf{g}_n$ of the linear-chain CRF adopt only the neighbor position information with unknown speaker identity, resulting in its inability to distinguish the impact brought by the speaker and the interlocutor. In contrast, SkipCRF, base on the view of emotional inertia and contagion, subdivides contextual local feature function $\mathbf{g}_n$ into self-dependent feature function $\mathbf{h}(y_{\mathbf{s}(t)},y_{t})$ and others-dependent feature function $\mathbf{g}(y_{\mathbf{o}(t)},y_t)$ by introducing speaker identity through skip-chain connections. The conditional probability $\mathcal{P}_{(Y|X)}$ is defined as follows: 
\begin{equation}
	\resizebox{.91\linewidth}{!}{$
		\displaystyle
		% \begin{aligned}
			\mathcal{P}_{(Y|X)}  = \frac{1}{\mathbf{Z}(X)}\exp\big[\sum_{i,j,t}\left(\lambda_i \mathbf{h}_i(y_{\mathbf{s}(t)},  y_{t})  +  \eta_j \mathbf{g}_j(y_{\mathbf{o}(t)},  y_t)\right) 
			+\sum_{l,t}\mu_l \mathbf{f}_l(y_t,X) \big] \\
			= \frac{1}{\mathbf{Z}(X)}\exp\big[\sum_{n,t}\omega_n \mathbf{F}_n(y_{\mathbf{s}(t)},y_{\mathbf{o}(t)},y_{t},X)\big], 
		% \end{aligned}
		$}
\end{equation}
where $\mathbf{Z}(X)$ is the normalization factor of all state sequences; $\lambda$, $\eta$, and $\mu$ constitute $\omega$, which represent the learnable weights of the corresponding feature functions in $\mathbf{F}_{n}(y_{\mathbf{s}(t)},y_{\mathbf{o}(t)},y_t,X)$.

In the training process, given the utterance feature $X$ and ground-truth label sequence ${Y}^*$, the objective is to maximize $\mathcal{P}_{({Y}^*|X)}$ of the ground-truth label sequence. It is converted into the following minimization objective by the negative logarithmic function: 
\begin{equation}
	\begin{aligned}
		\mathcal{L}(\Theta) = \min \{-\log(\mathcal{P}(Y^*|X))\},
		% \Theta &= \arg\min -\log(\mathcal{P}_{(\hat{Y}|X)}),\\
	\end{aligned}
\end{equation}
where $\Theta$ consists of the SkipCRF parameter $\Theta_\mathrm{crf}$ and the feature extraction parameter $\Theta_\mathrm{ext}$, these two types of parameters can be updated theoretically by back propagation algorithm. It is noted that CRF belongs to the probabilistic graphical model, and the difficulty lies in the calculation of the normalization factor with exponential complexity. Therefore, we employ forward-backward algorithm~\cite{binder1997space} to recursively calculate $\mathcal{P}_{(Y|X)}$, making it linearly complex.

Distinct from the linear-chain CRF that do not consider the non-normalized probability of participant identity~\cite{wang2020contextualized,song2022emotionflow,liang2021s+}, the SkipCRF we designed distinguish the interlocutor identify. We define $\alpha_t(y_{\mathbf{o}(t)}^{p_j},y_t^{p_i}|X)$ as the non-normalized probability of partial label sequences before the moment $t$ when the speaker's emotion is $y_t^{p_i}$ and the previous interlocutor's emotion is $y_{\mathbf{o}(t)}^{p_j}$. Assuming the total number of possible labels is $\mathcal{K}$, we define $\mathrm{A}_t(X)$ as a forward matrix/tensor consisting of $\mathcal{K}\times \mathcal{K}$ values:
\begin{equation}
	% \resizebox{.89\linewidth}{!}{$
		\mathrm{A}_t(X)=
		\begin{bmatrix}
			\alpha_t(y_{\mathbf{o}(t)}^{p_j}=1, y_t^{p_i}=1|X), &\cdots, &\alpha_t(y_{\mathbf{o}(t)}^{p_j}=1, y_t^{p_i}=\mathcal{K}|X)\\
			\vdots & \ddots &  \vdots\\
				\alpha_t(y_{\mathbf{o}(t)}^{p_j}=\mathcal{K}, y_t^{p_i}=1|X), &\cdots, &\alpha_t(y_{\mathbf{o}(t)}^{p_j}=\mathcal{K}, y_t^{p_i}=\mathcal{K}|X)\\
		\end{bmatrix}.
		% $}
\end{equation}

Following the setting of the forward-backward algorithm, the non-normalized transition probability $\mathrm{m}_{t}$ is defined as:
\begin{equation}
	% \resizebox{.89\linewidth}{!}{$
	\mathrm{m}_{t}(y_{\mathbf{s}(t)},y_{\mathbf{o}(t)},y_{t},X) = \exp\big[\sum_{n,t}\omega_n \mathbf{F}_n(y_{\mathbf{s}(t)},y_{\mathbf{o}(t)},y_{t},X)\big].
	% $}
\end{equation}
Then, the $\mathcal{K}\times \mathcal{K}\times \mathcal{K}$-dimensional non-normalized transition probability tensor $\mathrm{M}_{t}$ is constructed through transition probability $\mathrm{m}_{t}$. Therefore, the recursive equations for the forward probability matrix $\mathrm{A}_{t}$ and backward probability matrix $\mathrm{B}_t$, and the expression of normalization factor $\mathbf{Z}(X)$ are as follows:
\begin{equation}
	\mathrm{A}_{t}^\top(X) = \mathrm{A}_{t-1}^\top(X)\mathrm{M}_{t}(X),\ 
	\mathrm{B}_{t}(X) = \mathrm{M}_{t+1}(X)\mathrm{B}_{t+1}(X),\ 
	\mathbf{Z}(X) = \boldsymbol{1}^\top \mathrm{A}_{\mathcal{T}}(X)   \boldsymbol{1} = \boldsymbol{1}^\top   \mathrm{B}_{1}(X)  \boldsymbol{1},
\end{equation}
% Similarly, for backward probability matrix $\mathrm{B}_t$ after the moment $t$, the recursive equation is as follows:
% \begin{equation}
% 	\mathrm{B}_{t}(X) = \mathrm{M}_{t+1}(X)\mathrm{B}_{t+1}(X).
% \end{equation}
% Therefore, the expression of normalization factor $\mathbf{Z}(X)$ is as follows: 
% \begin{equation}
% 	\mathbf{Z}(X) = \boldsymbol{1}^\top \mathrm{A}_{\mathcal{T}}(X)   \boldsymbol{1} = \boldsymbol{1}^\top   \mathrm{B}_{1}(X)  \boldsymbol{1},
% \end{equation}
where $\boldsymbol{1}$ denotes the $\mathcal{K}$-dimensional vector whose elements are all $1$.

After completing conditional probability modeling, the decoding problem of SkipCRF requires to be solved. Given the conditional probability equation $\mathcal{P}_{(Y|X)}$ and the input feature $X$, the emotion sequence $Y$ when $\mathcal{P}_{(Y|X)}$ takes the maximum is obtained. We adopt the Viterbi algorithm~\cite{forney2005viterbi} for solving the problem. Similar to the non-normalized probability $\alpha_t(y_{o(t)}^{p_j},y_t^{p_i}|X)$, we define a local state $\delta_{t}(y_{\mathbf{o}(t)}^{p_j},y_{t}^{p_i})$, which represents the maximum non-normalized probability corresponding to all possible values of $y_{o(t)}^{p_j}$ and $y_t^{p_i}$. Here, the normalization factor does not affect the comparison of the maximum values, so only the non-normalization probability is needed. The recursive equation of local state can be formalized as follows: 
\begin{equation}
	% \resizebox{.89\linewidth}{!}{$
	\displaystyle
	% \begin{aligned}
		\delta_{t}(\hat{k}_1,\hat{k}_2) =\max_{1\le k_1,k_2\le \mathcal{K}}\{\delta_{t-1}(k_1, k_2)
		+\sum_{n}\omega_n\mathbf{F}_n(y_{\mathbf{s}(t)}, y_{\mathbf{o}(t)}=\hat{k}_1,y_{t}=\hat{k}_2,X)\},
	% \end{aligned}
		% $}
\end{equation}
where $k_1$ and $k_2$ denote the emotion labels corresponding to the utterances. In addition, when $\delta_{t}(y_{o(t)}^{p_j},y_t^{p_i})$ reaches the maximum, the local state $\Psi_{t}({y}_{o(t)}^{p_j},{y}_t^{p_i})$ is used to record the emotion values of $y_{o(t)}^{p_j}$ and $y_t^{p_i}$, which is applied to trace the optimal solution. 
\begin{equation}
	% \resizebox{.89\linewidth}{!}{$
		\displaystyle
		% \begin{aligned}
			\Psi_{t}(\hat{k}_1,\hat{k}_2) =\mathop{\mathrm{argmax}}_{1\le k_1,k_2\le \mathcal{K}}\{\delta_{t-1}(k_1, k_2)
			+\sum_{n}\omega_n\mathbf{F}_n(y_{\mathbf{s}(t)} , y_{\mathbf{o}(t)}=\hat{k}_1 ,y_{t}=\hat{k}_2,X)\}.
		% \end{aligned}
		% $}
\end{equation}

It is noted that the above analysis is discussed under the premise of dyadic dialogue scenario. In the case of multi-person dialogue, the order of the forward tensor is equal to the number of participants due to the need to preserve the emotional state of the participants simultaneously, which results in an exponential increase in computational complexity with the number of participants. Therefore, we do some simplifications, i.e., eliminating those skip-chain connections that span the dyadic dialogue in order to make it a multi-segment dyadic dialogue (note that this is still a full undirected graphical model). The specific case presented in Figure~\ref{fig:conversation_all_del} provides a visual illustration of the discrepancy before and after elimination. Note that the connection between the moment $1$ and moment $6$, the moment $3$ and moment $7$ has been removed, converting the situation of multi-party dialogue into four dyadic dialogue scenarios. The implication is that only the interaction between two participants needs to be explicitly considered in the multi-party conversation, while the influence of other participants' utterances is implicitly encoded. 
\begin{figure}[htbp]
	\centering
	\includegraphics[width=3.0in]{multiperson_all_del_1.pdf}
	\caption{An example of three-person dialogue with simplifications. Nodes with different colors represent different participants' node features. The dotted and solid lines represent the weights of self-dependent and others-dependent features, respectively. The black node represents the actual tag result, and the red line connects the optimal label sequence and represents the optimal path.}
	\label{fig:conversation_all_del}
\end{figure}

\section{Experimental Settings}\label{sec:experimental_settings}
\subsection{Implementation Details}
In this subsection, we mainly describe the implementation details of our proposed method. Since our approach considers only dialogue-level modeling, the RoBERTa model fine-tuned~\cite{Ghosal2020} by utterance classification task is used to extract context-independent utterance level feature with the dimensionality of 1024 as input in advance. We train our model for 100 epochs. For the stability of training, we utilize a mixture of cross-entropy loss and CRF loss in the early stage of training, and attenuate the proportion of the former with the increase of epoch. During training, the Adam optimizer~\cite{loshchilov2018fixing} is adopted for optimization. Due to the small number of parameters in the SkipCRF, a larger learning rate is set separately for it in order to avoid underlearning during the optimization process. We search for hyperparameters on each dataset with the grid search, where the hyperparameters include learning rate, dropout rate, and number of network layers~\cite{Akiba2019}. All experiments are conducted on a server with an NVIDIA 1080Ti GPU.

\subsection{Evaluation Metrics and Datasets}
We evaluate EmotionIC on four benchmark ERC datasets.
\textbf{IEMOCAP}~\cite{busso2008iemocap} is a dataset recorded as two-way conversational video clips, containing five sessions, where each session contains dyadic dialogues between two participators. Each dialogue is further segmented into utterances that are annotated with one of six emotion labels, including \texttt{happy}, \texttt{sad}, \texttt{neutral}, \texttt{angry}, \texttt{excited}, and \texttt{frustrated}. 
\textbf{DailyDialog}~\cite{li2017dailydialog} is a multi-turn dialogue dataset, consisting of human-written daily communications. The language in DailyDialog dataset is human-written and has less noise. Each utterance has a emotion label from one of seven categories (\texttt{neutral}, \texttt{joy}, \texttt{surprise}, \texttt{sad}, \texttt{angry}, \texttt{disgust}, and \texttt{fear}). 
\textbf{MELD}~\cite{poria2019meld} is a multimodal dataset containing about 13,000 utterances from 1,433 dialogues from the TV series $Friends$. MELD consists of multi-party conversations, making it more challenging to classify than dyadic dialogue. Each utterance in each conversation is labeled with one of the seven emotion categories (\texttt{angry}, \texttt{disgust}, \texttt{sad}, \texttt{joy}, \texttt{neutral}, \texttt{surprise}, and \texttt{fear}). 
\textbf{EmoryNLP}~\cite{zahiri2017emotion} is also a multi-dialogue dataset collected from $Friends$ TV script, which comprises 97 episodes, 897 scenes, and 12,606 utterances, where each utterance is annotated with one of the seven emotions (\texttt{neural}, \texttt{sad}, \texttt{mad}, \texttt{scared}, \texttt{powerful}, \texttt{peaceful}, and \texttt{joyful}).

Except for IEMOCAP, there is class imbalance in all datasets, especially DailyDialog. So we choose weighted-average F1 and accuracy to validate the proposed model for IEMOCAP, macro-averaged F1 and micro-averaged F1 excluding the majority class (\texttt{neutral}) for DailyDialog, and weighted-average F1 and micro-averaged F1 for MELD and EmoryNLP. In addition, all data segmentation and pre-processing is consistent with COSMIC~\cite{Ghosal2020}.

\subsection{Baseline Methods}
To evaluate the effectiveness of our EmotionIC, we choose the following competitive ERC models as comparison methods. Since our experiments are conducted on the four previously mentioned datasets, we select the models that have been validated on these datasets.
\textbf{HiGRU}~\cite{jiao2019higru} captured word-level input and utterance-level embedded context by constructing two hierarchical GRU frameworks, i.e., HiGRU with single feature fusion, and HiGRU with self-attention and feature fusion.
\textbf{DialogueGCN}~\cite{ghosal2019dialoguegcn} built directed graphical structures over the input utterance sequences with speaker information, and applied graph convolutional networks to construct inter- and intra-dependencies between distant utterances.
\textbf{KET}~\cite{zhong2019knowledge} proposed hierarchical self-attention to compute utterance and context representation, respectively, and applied commonsense knowledge ConceptNet and sentiment lexicon NRC\_VAD to context-aware effective graph attention.
\textbf{COSMIC}~\cite{Ghosal2020} trained commonsense conversational model COMET to extract commonsense features and extracted contextual features from pre-trained transformer language model to model internal state, external state and intent state.
\textbf{RGAT}~\cite{wang2020relational} encoded aspect-oriented dependency tree using relational graph attention networks containing sequential information so that it could capture both the speaker dependency and the sequential information.
\textbf{DialogXL}~\cite{shen2021dialogxl} used enhanced memory to store longer historical context, and introduced dialog-aware self-attention to capture inter- and intra-speaker dependencies.
\textbf{DAG-ERC}~\cite{shen-etal-2021-directed} treated the internal structure of dialogue as a directed acyclic graph to encode utterance, thus a more intuitive way to model the flow of information between long-distance conversation background and nearby context.
\textbf{CoG-BART}~\cite{li2022contrast} adopted supervised contrastive learning to enhance the model's ability to handle context information.
\textbf{GAR-Net}~\cite{xu2022gar} was an end-to-end graph attention reasoning network that took both word-level and utterance-level context into concern.
% \\ \noindent 
% \textbf{Emo-Caps (2022)}~\cite{li2022emocaps} proposed a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vectors to be an emotion capsule.

\section{Results and Analysis}\label{sec:results_analysis}
\subsection{Comparison with the State-of-the-Art Methods}
\begin{table}[htbp]
	\caption{\label{tab:main} The experimental results of proposed EmotionIC against state-of-the-art models on four datasets. 
}	
	\begin{center}
		\fontsize{8pt}{7pt}\selectfont
		\begin{tabular}{l|l|l|l|l|l|l|l|l}
			\toprule[1.2pt]
%			\hline
			\multicolumn{1}{l|}{\multirow{3}{*}{Models}} &  \multicolumn{2}{c|}{DailyDialog} &  \multicolumn{2}{c|}{MELD} &  \multicolumn{2}{c|}{IEMOCAP} &  \multicolumn{2}{c}{EmoryNLP} \\
			\cline{2-9}
			& \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{Macro-F1\\ - neutral}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{Micro-F1\\ - neutral}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{weighted \\ Avg-F1}}} & \multicolumn{1}{c|}{\multirow{2}{*}{Micro-F1}} & \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{weighted\\ Avg-F1}}} & \multicolumn{1}{c|}{\multirow{2}{*}{Accuracy}} & \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{weighted\\ Avg-F1}}} & \multicolumn{1}{c}{\multirow{2}{*}{Micro-F1}} \\
			& & & & & & & &\\
			\hline
			HiGRU 		& 49.04 & 51.90 & 56.81 & 54.52 & 58.54 & 58.35 & 34.48 & 33.54\\
			DialogueGCN & 49.95 & 53.73 & 58.37 & 56.17 & 64.18 & 65.25 & 34.29 & 33.13\\			
			%			DialogueRNN & -- 	& -- 	& 57.03 & --	& 62.75 & --	& -- 	& -- \\			
			KET 		& -- 	& 53.48 & 58.18 & --  	& 59.56 & -- 	& 34.39 & --\\
			COSMIC 		& 51.05 & 58.48 & 65.21 & -- 	& 65.28 & -- 	& 38.11 & --\\
			RGAT		& -- & 54.31 &60.91 &-- &65.22 &-- &34.42 &-- \\		
			DialogXL	& -- & 54.93 &62.41 &-- &65.94 &-- &34.73 &-- \\
			DAG-ERC 	& -- 	& 59.33 & 63.65 & --  	& 68.03 & -- 	& 39.02 & --\\
			CoG-BART 	& -- 	& 56.29 & 64.81 & 65.95  	& 66.18 & 66.71 	& 39.04 & 42.58\\
			GAR-Net 	& 45.81 	& 56.97 & 62.11 & --  	& 67.41 & -- 	& -- & --\\
			% Emo-Caps 	& -- 	& -- & 63.51 & --  	& 69.49 & -- 	& -- & --\\				
%			\hline
			\hline
			{EmotionIC} 		 & \textbf{51.79} & \textbf{59.79} 	& \textbf{66.40}	& \textbf{66.21} & \textbf{69.50} 	& \textbf{69.50} 	& \textbf{40.01} 		& \textbf{44.00}\\								
			\bottomrule[1.2pt]
		\end{tabular}
	\end{center}
\end{table}

The performance of our proposed EmotionIC is compared with the current state-of-the-art methods, as shown in Table~\ref{tab:main}. The experimental results demonstrate that our EmotionIC outperforms all the baseline methods and markedly exceeds in some indicators.
% \\ \noindent 

\textbf{(1)} IEMOCAP. The average session length of IEMOCAP is far more than the other datasets, and thus contains richer contextual information. Compared to the recurrence-based methods in Table \ref{tab:main}, EmotionIC achieves great improvements, demonstrating its ability to capture long-distance context dependencies. Coupled with its use of DialogGRU to fine-tune the local inter- and intra-speaker dependencies, which enables our model to achieve state-of-the-art performance.
% \\ \noindent 

\textbf{(2)} DailyDialog. As with IEMOCAP, DailyDialog contains dyadic conversations, with the difference that it suffers from a severe class imbalance, where the \texttt{neutral} accounts for $83\%$. Coupled with the much shorter conversation length, the performance improvement of our model on the DailyDialog dataset is insignificant.
% \\ \noindent 

\textbf{(3)} MELD. The utterances in MELD are much shorter than those in IEMOCAP, which means that emotional modeling is highly context-dependent. In addition, the fact that there are often two or more speakers in the conversation, as well as the lesser amount of speech from each participant, makes it difficult to model emotional inertia and contagion. While the F1 score of our EmotionIC reach $66.40\%$, even surpassing that of COSMIC, which adds additional commonsense knowledge. We attribute the improvement to our contextual modeling at the classification level, i.e., SkipCRF.
% \\ \noindent 

\textbf{(4)} EmoryNLP. Compared to the baseline model, our model still achieves competitive performance with an improvement of $0.99\%$ in F1 score. However, compared with on the MELD dataset, EmotionIC shows limited improvement on EmoryNLP for the same problem. The probable reason is that EmoryNLP requires more commonsense knowledge. So the models with pre-training and knowledge enhancement, such as RGAT, DAG-ERC, and KET have excellent performance on EmoryNLP.

\begin{figure}[htbp]
	\centering
	\captionsetup[subfigure]{font={scriptsize}}
	\subfloat[IEMOCAP dataset.]{\includegraphics[width=1.55in]{iemocap_confusion.pdf}
		\label{fig_iemocap}
%		\captionsetup{font={small}}
	}
	% \hfil
	\subfloat[DailyDialog dataset.]{\includegraphics[width=1.55in]{dailydialog_confusion.pdf}	
		\label{fig_dailydialog}
	}
	% \hfil
	\subfloat[MELD dataset.]{\includegraphics[width=1.55in]{meld_confusion.pdf}
	\label{fig_meld}
	}
	% \hfil
	\subfloat[EmoryNLP dataset.]{\includegraphics[width=1.55in]{emorynlp_confusion.pdf}	
		\label{fig_emorynlp}
	}	
	\caption{Confusion matrices of the testing set on four benchmark datasets. The horizontal coordinate and vertical coordinate indicate the ground-truth and predicted emotion, respectively.}
	\label{fig_confusion}
\end{figure}
To comprehensively analyze the performance of EmotionIC, we present the confusion matrices on the four benchmark datasets in Figure~\ref{fig_confusion}. The horizontal coordinate represents the prediction category and the vertical coordinate represents the true label. For the IEMOCAP dataset, due to the proximity of the activation and the valence domains, positive and negative emotions are easily confused by similar emotions, and \texttt{neutral} emotions can easily be confused with positive and negative emotions. For the DailyDialog dataset, our model has a high F1-score. Even if the \texttt{happy} emotion accounts for a high proportion in DailyDialog, our model can still distinguish it well from other emotions. Compared with the first two datasets, the performance of the model on the MELD and EmoryNLP, two multi-person conversation datasets, is somewhat unsatisfactory: on the MELD dataset, our model tends to confuse \texttt{fear}, \texttt{sad} and \texttt{disgust} with \texttt{neutral} emotion. The performance on the EmoryNLP dataset is similar since the dialogue samples extracted from the TV series $Friends$ may need to be supplemented with commonsense knowledge.

\subsection{Impact of Different Modules}
To analyze the impact of different modules in EmotionIC, we observe performance degradation by replacing or removing each module. The results are shown in Table~\ref{tab:modules}.
\begin{table}[htbp]
	\centering	
	\caption{The impact of different modules on the performance of EmotionIC. The table shows the results on the IEMOCAP and MELD datasets. + indicates replacement with another module, - indicates removal of the corresponding module.}
	\label{tab:modules}	
	\fontsize{8pt}{7pt}\selectfont
		\begin{tabular}{l|l|l}
			\toprule[1.2pt]
%			\hline
			\multirow{2}*{Method} & \multicolumn{2}{c}{weighted Avg-F1}\\ 
			\cline{2-3}
			&\multicolumn{1}{c|}{IEMOCAP} & \multicolumn{1}{c}{MELD}\\
			\hline
			EmotionIC	 			& \textbf{69.50}						& \textbf{66.40}\\
			\hline 
			+ MHA			   		& 68.30  ($\downarrow$1.20)				& 66.19 ($\downarrow$0.21)\\
			+ GRU		   			& 68.51  ($\downarrow$0.99)				& 65.97 ($\downarrow$0.43)\\
			+ linear-chain CRF      & 69.12  ($\downarrow$0.38)				& 66.11 ($\downarrow$0.29)\\				
			\hline
			- IM-MHA				    & 67.83  ($\downarrow$\textbf{1.67})	& 66.24 ($\downarrow$0.16)\\ 
			- Dialogue GRU	          & 68.69  ($\downarrow$0.81)				& 65.43  ($\downarrow$0.97)\\ 
			- SkipCRF		   & 68.94  ($\downarrow$0.56)				& 64.52 ($\downarrow$\textbf{1.88})\\		
			\bottomrule[1.2pt]
		\end{tabular}
	% }
\end{table}

The results of replacing each module show that \textbf{(1)} multi-head attention (MHA) cannot make effective use of identity information to create the global atmosphere; \textbf{(2)} gated recurrent unit (GRU) doesn't discriminate between inter- and intra-speaker dependencies, resulting in low performance; and \textbf{(3)} linear-chain CRF focuses only on the first-order dependencies, making it lacking in classification. It is necessary to introduce speaker identity to establish a probabilistic graph to simulate emotional flow in dialogue.

In addition, the experimental results listed in Table~\ref{tab:modules} can demonstrate the following observations: 
\textbf{(1)} For the long-dialogue dataset IEMOCAP, the capability of the model to capture long distances decreases sharply after removing IM-MHA, resulting in the most performance degradation. 
\textbf{(2)} For the MELD dataset with fast emotional variations, the removal of DialogGRU leads to more performance degradation, suggesting that local contextual information is more critical for datasets with violent emotional fluctuations.
\textbf{(3)} After removing SkipCRF and employing Softmax for emotion classification, the F1-score on MELD drops more severely than that on IEMOCAP, which can be attributed to the fact that explicit modeling of emotion dependencies at the classification level is more essential on the multi-person datasets such as MELD.

\subsection{Effectiveness of the Elaborate SkipCRF}
To further prove that considering context dependencies at the classification level can effectively improve the performance of the ERC models, we conduct a case study with a dialogue in the MELD dataset as shown in Figure~\ref{fig:CaseStudy}. 
\begin{figure}[htbp]
	\begin{minipage}[c]{0.44\textwidth}
		\centering
		\includegraphics[width=2.7in]{case_rep.pdf}
		\caption{A case study on the MELD dataset, which is exploited to demonstrate the validity of SkipCRF.}
		\label{fig:CaseStudy}
	\end{minipage}
	\hspace{0.02\textwidth}
	\begin{minipage}[c]{0.53\textwidth}
		\centering
		\includegraphics[width=3.3in]{classimbalance.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
		\caption{In DailyDialog (-neutral), the class imbalance in the dialogue and the dataset jointly affect Micro-F1 score.}
		\label{fig:imbalance}	
	\end{minipage}
\end{figure}

The dialogue presents emotional contagion between participants and their own emotional inertia. It can be observed that the first utterance of Person B is incorrectly classified as \texttt{surprise} due to the lack of reliable historical emotional information. From the misclassification of the $3$-th and $5$-th utterance, neutral emotion is easily misclassified as negative emotion by most ERC models. In addition, compared to Softmax-based classification, our elaborate SkipCRF has significantly superior performance in modeling emotional inertia as well as emotional contagion, proving the effectiveness of modeling context dependencies at the classification level.

\subsection{Class Imbalance in both the Dialogue and the Dataset}
We conduct experiment on the DailyDialog dataset to investigate that the class imbalance in the dialogue and that in the dataset jointly affect the performance of our model. Since the ERC model tends to be processed in terms of dialogues, class imbalance in the dialogue affects its performance to some extent. For the emotion with a small sample size, if it is uniformly distributed in the dataset, i.e., it appears only a few times in each conversation. It is difficult for the model to obtain enough contextual information to express that emotion. On the contrary, if the emotion appears concentrated in several conversations, the model can effectively model emotional inertia and contagion.

We count the average proportion of each emotion in the dialogue, the proportion of each emotion in the dataset and the corresponding F1 scores from DailyDialog except for \texttt{neutral}, as shown in Figure~\ref{fig:imbalance}. As can be seen, although the proportions of \texttt{sad} and \texttt{fear} are quite different in the dataset, they have roughly the same proportion of emotions in the conversation, resulting in similar F1 scores. The same goes for \texttt{disgust} and \texttt{anger}. However, the F1 score of the \texttt{suprise} with the lowest proportion in dialogue is higher than that of the other four emotions except for \texttt{joy}, which can be attributed to the fact that the high proportion in the dataset also played a role in the training process. Note that \texttt{joy} has a large proportion in both the dataset and the dialogue. Although the proportion of \texttt{joy} in the dataset is several times higher than those of other emotions, it has only a slightly higher F1 score than \texttt{surprise}, which confirms that the proportion in the dialogue is more critical to the model performance than the proportion in the dataset.
% \begin{figure}[htbp]
% 	\centering
% 	\includegraphics[width=3.4in]{classimbalance.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
% 	\caption{In DailyDialog (-neutral), the class imbalance in the dialogue and the dataset jointly affect Micro-F1 score.}
% 	\label{fig:imbalance}
% \end{figure}

\subsection{Results of Transition Matrices}
To illustrate that the proposed EmotionIC can effectively model emotional inertia and contagion, we visualize the self-dependent and others-dependent transition matrices, as shown in Figure~\ref{fig:transfer}. The diagonal element in the self-dependent matrix $H$ is the largest, which proves the emotional inertia of the same participant. For example, we can reveal from the left of Figure~\ref{fig:transfer23} that \texttt{happy}, \texttt{sad} and \texttt{neutral} tend to remain unchanged. Of course, the emotion of the speaker may also change, for instance, \texttt{angry} tends to be transformed into \texttt{sad} with a certain probability. Others-dependent matrix $G$ reflects the tendency of emotional contagion between different participants. For instance, as we can see in the right of Figure~\ref{fig:transfer23}, \texttt{angry} is more inclined to convert to \texttt{frustrated}, and \texttt{excited} is more inclined to convert to \texttt{happy}.
\begin{figure}[htbp]
	\centering
	\captionsetup[subfigure]{font={scriptsize}}
	\subfloat[]{\includegraphics[width=3.4in]{transfer1.pdf}
		\label{fig:transfer1}
%		\captionsetup{font={small}}
	}
  \subfloat[]{\includegraphics[width=1.3in]{transfer2.pdf}	
  \includegraphics[width=1.3in]{transfer3.pdf}
		\label{fig:transfer23}
  }
	\caption{Visualization of the transition matrix. The dashed (solid) line represents the learnable weight of self-dependent (others-dependent) function, which can form self-dependent matrix $H$ (others-dependent matrix $G$). Note that $H$ and $G$ are non-normalized probability transition matrices, and the results are on the IEMOCAP dataset.}
	\label{fig:transfer}
\end{figure}

\subsection{Comparison of Modeling Emotional Inertia and Contagion}
In this subsection, we discuss the discrepancy between modeling emotional inertia and modeling emotional contagion in the proposed model. First, we extracted data samples with emotional inertia and emotional contagion in the test dataset. Then, we evaluate these samples by employing the test F1 scores, i.e., as indicators of emotional inertia and contagion evaluation. Note that the data samples with emotional inertia and emotional contagion are extracted by: 
\\ \noindent 
\textbf{(1)} Emotional inertia samples. If the emotion $y_{t}^{p_i}$ of participant $p_i$'s utterance $u_{t}^{p_i}$ at moment $t$ is the same as the emotion $y_{\mathbf{s}(t)}^{p_i}$ of $p_i$'s utterance $u_{\mathbf{s}(t)}^{p_i}$ at the previous moment $\mathbf{s}(t)$ (i.e., $y_{t}^{p_i}=y_{\mathbf{s}(t)}^{p_i}$), then $u_{t}^{p_i}$ and $u_{\mathbf{s}(t)}^{p_i}$ are taken as emotional inertia samples. 
\\ \noindent 
\textbf{(2)} Emotional contagion samples. If the emotion $y_{t}^{p_i}$ of participant $p_i$'s utterance $u_{t}^{p_i}$ at moment $t$ is the same as the emotion $y_{\mathbf{o}(t)}^{p_j}$ of interlocutor $p_j$'s utterance $u_{\mathbf{o}(t)}^{p_j}$ at the moment $\mathbf{o}(t)$ (i.e., $y_{t}^{p_i}=y_{\mathbf{o}(t)}^{p_j}$), then $u_{t}^{p_i}$ and $u_{\mathbf{o}(t)}^{p_j}$ are taken as emotional contagion samples. 

Table~\ref{tab:InerandCont} shows that our model is better at modeling emotional inertia than it is at modeling emotional contagion. It is intuitive that the ability of the ERC model to simulate emotional inertia and contagion is to some extent related to the number of corresponding samples. That is, the larger the sample size of emotional inertia or infection, the better the proposed model should perform on the corresponding samples. However, the results in Table~\ref{tab:InerandCont} suggest that it is not the case. Except for the IEMOCAP dataset, all other datasets show the opposite results. In other words, although the sample size of emotional contagion is larger than that of emotional inertia, the corresponding F1 scores are still lower, suggesting that utterances involving emotional contagion in conversations are more difficult to classify accurately. We further count the number of samples with emotional inertia and infection for different emotions in the DailyDialog dataset, as shown in Table~\ref{tab:InerandCont_DD}. EmotionIC performs well on the samples involving emotional inertia. However, the results on the samples involving emotional contagion show large variance due to the extreme imbalance of the categories.
\begin{table}[htbp]
	\begin{minipage}[c]{0.495\textwidth}
		\caption{\label{tab:InerandCont} Test F1-score of the proposed EmotionIC on the samples with emotional inertia and emotional contagion.}	
		\begin{center}
			\fontsize{8pt}{8pt}\selectfont
			\begin{tabular}{c|cc|cc}
	%			\thickhline
				\toprule[1.2pt]
				\multicolumn{1}{c|}{\multirow{2}{*}{Dataset}} &  \multicolumn{2}{c|}{Inertia} &  \multicolumn{2}{c}{Contagion}\\ 
				\cline{2-5}
				%			& \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{Micro-F1 \\ score}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{Proportion \\ - neutral}}} & \multicolumn{1}{c}{\multirow{2}{*}{\tabincell{c}{Proportion \\ - neutral}}}\\
				&\# Samples & F1 & \# Samples & F1 \\
				\hline
				IEMOCAP 	& 1151  & 77.12 & 410   & 49.82\\
				DailyDialog   & 454    & 74.08 & 670  & 73.60\\
				MELD	 	   & 861    & 72.59 & 1003  & 61.07\\
				EmoryNLP 	& 242    & 47.04 & 497   & 31.66\\
				\bottomrule[1.2pt]
			\end{tabular}
		\end{center}
	\end{minipage}
	\hspace{0.02\textwidth}
	\begin{minipage}[c]{0.475\textwidth}
		\caption{\label{tab:InerandCont_DD} Test F1-score of EmotionIC on the samples with emotional inertia and contagion for different emotions.}	
	\begin{center}
		\fontsize{7pt}{6pt}\selectfont
		\begin{tabular}{c|cc|cc}
			%			\thickhline
			\toprule[1.2pt]
			\multicolumn{1}{c|}{\multirow{2}{*}{Emotion}} &  \multicolumn{2}{c|}{Inertia} &  \multicolumn{2}{c}{Contagion}\\ 
			\cline{2-5}
			%			& \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{Micro-F1 \\ score}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{Proportion \\ - neutral}}} & \multicolumn{1}{c}{\multirow{2}{*}{\tabincell{c}{Proportion \\ - neutral}}}\\
			&\# Samples & F1 & \# Samples & F1 \\
			\hline
			Joy 	    	& 330    & 79.05 & 483  & 80.54\\
			Anger    	   & 60      & 63.27 & 34   & 42.55\\
			Sadness		  & 31      & 55.81 & 46   & 57.58\\
			Fear    	    & 5        & 75.00 & 9     & 18.18\\
			Surprise	   & 8        & 62.50 & 86   & 66.67\\
		    Disgust        & 20      & 57.14 & 12   & 35.29\\			
			\bottomrule[1.2pt]
		\end{tabular}
	\end{center}
	\end{minipage}
\end{table}
% \begin{table}[htbp]
% 	\caption{\label{tab:InerandCont_DD} Test F1-score of the proposed EmotionIC on the samples with emotional inertia and emotional contagion for different emotions in DailyDialog dataset.}	
% 	\begin{center}
% 		\fontsize{10pt}{10pt}\selectfont
% 		\begin{tabular}{c|cc|cc}
% 			%			\thickhline
% 			\toprule[1.2pt]
% 			\multicolumn{1}{c|}{\multirow{2}{*}{Emotion}} &  \multicolumn{2}{c|}{Inertia} &  \multicolumn{2}{c}{Contagion}\\ 
% 			\cline{2-5}
% 			%			& \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{Micro-F1 \\ score}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\tabincell{c}{Proportion \\ - neutral}}} & \multicolumn{1}{c}{\multirow{2}{*}{\tabincell{c}{Proportion \\ - neutral}}}\\
% 			&\# Samples & F1 & \# Samples & F1 \\
% 			\hline
% 			Joy 	    	& 330    & 79.05 & 483  & 80.54\\
% 			Anger    	   & 60      & 63.27 & 34   & 42.55\\
% 			Sadness		  & 31      & 55.81 & 46   & 57.58\\
% 			Fear    	    & 5        & 75.00 & 9     & 18.18\\
% 			Surprise	   & 8        & 62.50 & 86   & 66.67\\
% 		    Disgust        & 20      & 57.14 & 12   & 35.29\\			
% 			\bottomrule[1.2pt]
% 		\end{tabular}
% 	\end{center}
% \end{table}

\section{Conclusion}\label{sec:conclusion}
Our proposed EmotionIC is a novel approach to conversational sentiment recognition driven by emotional inertia and contagion. We design the IM-MHA and DialogGRU modules exploiting the speaker's identity to capture speaker-specific emotional tendencies from both global and local perspectives. To explicitly model emotional inertia and contagion in the classification stage, we elaborate SkipCRF to extract the emotional flow of distinct speakers and capture the high-order dependencies of different speaker, and thus achieve the globally optimal emotional label sequence. We capture both inter- and intra-speaker dependencies at the feature extraction level and classification level. Extensive experimental results on the benchmark dataset confirm that the proposed EmotionIC can efficiently model emotional inertia and contagion, outperforming all baseline models. In order to further strengthen the capacity of our model to emulate emotional contagion, we intend to investigate the effects of external commonsense knowledge and multimodal methods on emotional mutations in future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Acknowledgements. 致谢
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hspace*{\fill}

\noindent
\textbf{Acknowledgements} \ This work was supported in part by the National Natural Science Foundation of China under Grant 62236005, 61876209 and 61936004.
% \Acknowledgements{This work was supported in part by the National Natural Science Foundation of China under Grant 62236005, 61876209 and 61936004.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Supplements. 补充材料, 非必选
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \Supplements{Appendix A.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Reference section. 参考文献
%%% citation in the content using "some words~\cite{1,2}".
%%% ~ is needed to make the reference number is on the same line with the word before it.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{thebibliography}{99}

% \bibitem{1} Author A, Author B, Author C. Reference title. Journal, Year, Vol: Number or pages

% \bibitem{2} Author A, Author B, Author C, et al. Reference title. In: Proceedings of Conference, Place, Year. Number or pages

% \end{thebibliography}
% \small
% \footnotesize
\scriptsize
\bibliographystyle{gbt7714-numerical} % a modified version of IEEEtran.bst
% \bibliographystyle{gbt7714-author-year}
\bibliography{emotionic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Appendix sections. 附录章节, 非必选
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{appendix}
%\section{Name}

%\end{appendix}

\end{document}
