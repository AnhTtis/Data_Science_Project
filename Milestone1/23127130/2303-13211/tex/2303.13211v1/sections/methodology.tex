\section{Properties of Frequency Backdoor Attacks}\label{sec:introduction}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/topk.pdf}
    % \caption{\textbf{Visualizing the Indices of the top-k Most Sensitive Frequencies.} Visualizing the indices of the top-k values (k=1000) of $\nabla_{\text{FREAK}}(x) = \nabla_{|\mathcal{F}(x)|} \max f_{\theta} (x)$ allows us to understand the frequency bases, to which the network is most sensitive for a particular sample. We visualize the selected top-$1000$ indices for a Clean model with a clean input, and for FIBA, FTrojan, and CYO poisoned models with both clean and poisoned inputs.}
    \caption{\textbf{Visualizing the Indices of the top-k Most Sensitive Frequencies.} By visualizing the top-$1000$ indices of the FREAK gradient, $\nabla_{\text{FREAK}}(x)$, we can identify the frequency bases that a neural network is most sensitive to for a particular input. We show these indices for a Clean model with a clean input, and for models that have been poisoned with FIBA, FTrojan, and CYO attacks, with both clean and poisoned inputs. This allows us to gain insight into the specific frequencies that are most important to each model and how different attacks affect the network's sensitivity.}
    \label{fig:analysis1}
\end{figure*}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/topk_analysis.pdf}
    \caption{\textbf{Visualizing the Distribution of the Indices of the top-k Most Sensitive Frequencies.} Analyzing the distribution of the top-k most sensitive frequency indices can help us detect whether a sample has been poisoned. We compare the distribution shifts for clean and poisoned samples using three attacks: FIBA, FTrojan, and CYO. Backdoored models experience a drastic shift in frequency sensitivity in the presence of the backdoor trigger. This provides valuable insights into the effects of backdoor attacks on the network's sensitivity to frequency bases.}
    \label{fig:analysis2}
\end{figure}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fixed_figure_final.pdf}
    \caption{\textbf{FREAK Poisoned Sample Detection.} To construct FREAK detector, we first find the distance in Equation \ref{freak_eq} between samples from a clean experimental set $\mathcal{D}_c$ and the average distribution of the indices top-$k$ held-out set $\mathcal{D}_h$, referred to as $\hat{\nabla}_h$. The obtained distances are stored in a vector $\mathbf{\Gamma}$, whose rows represent the distance of one sample from $\mathcal{D}_c$ to $\hat{\nabla}_h$. Next, we fit a Gaussian distribution over the rows of $\mathbf{\Gamma}$, referred to as  $\mathcal{N}({\mu}_\Gamma, {\sigma}^2_{\Gamma})$, and compute the log-likelihood values of the rows of $\mathbf{\Gamma}$ belonging to that distribution. We store the mean and the standard deviation of the log-likelihood values in $\mu_c$ and $\sigma_c$. When a new sample $\tilde{x}$ is to be inspected, we compute the distance of $\tilde{x}$ to $\hat{\nabla}_h$ and compute the likelihood value of this distance belonging to $\mathcal{N}({\mu}_\Gamma, {\sigma}^2_{\Gamma})$, if the value falls within $\alpha$ standard-deviations of the mean then the sample is clean, otherwise it is poisoned.  } 
    \label{pipeline}
\end{figure*}


\paragraph{Analysis. }
Existing frequency-based backdoor attacks, \eg, FIBA \cite{Feng2021FIBAFB}, CYO \cite{Hammoud2021CheckYO}, and FTrojan \cite{Wang2021BackdoorAT}, embed poisoned information into specific frequency bases. Therefore, one would expect that in the presence of the backdoor trigger, such models would attend to the poisoned bases for classifying the poisoned sample. To verify this, we devise an approach similar to the spatial attention technique, GradCAM \cite{Selvaraju2019GradCAMVE}. The idea is to compute the gradient of the maximal logit of the network with respect to the Fourier transform, specifically, the Fourier magnitude. %\bibi{the previous sentence reads wrong} 
Afterwards, we visualize the indices of the $k$ most sensitive frequency bases, \ie, top-$k$ gradient values.

Formally, let $f_\theta : \mathcal{X} \rightarrow \mathcal{Y}$ be a classifier parameterized by $\theta$ mapping images $x\in \mathcal{X}$ to class labels $y\in \mathcal{Y}$. We denote the most probable class prediction for image $x$ by $c_A$, where $c_A = \argmax \limits_{c \in \mathcal{Y}} f_\theta^c(x)$. Let $\mathcal{G}_\eta: \mathcal{X}\rightarrow \mathcal{X}$ denote an attacker-specific poisoned image generator which is parameterized by $\eta$. Finally, let $\mathcal{F}(x)$ be the 2D Discrete Fourier Transforms (DFT) of an image $x$. 
The gradient we are interested in computing is

\begin{equation}
    \nabla_{\text{FREAK}}(x) = \nabla_{\mid\mathcal{F}(x)\mid}~f_\theta^{c_A}(x).
\end{equation}
%\bibi{Above equation is weird. max in non differentiable operator which makes it harder to interpret. How about you define an index $c_A = \text{argmax}_c f^i(x)$ and define that $f^i$ is the ith element then take gradient of $f_\theta^{c_A}$}
The above quantity expresses the sensitivity of the classifier's prediction with respect to the Fourier magnitude. 

We are interested in comparing the above gradient for clean samples, $x_c$ and their poisoned counterparts $x_p = \mathcal{G}_\eta(x_c)$. Figure \ref{fig:analysis1}, presents a binary map that highlights the indices of the top-$k$ values of $\nabla_{\text{FREAK}}(x_c)$ and $ \nabla_{\text{FREAK}}(x_p)$ where as Figure \ref{fig:analysis2} 
%\bibi{what is 3 you mean to say Figure 3?}
shows the distribution of those indices. %We make the following two key observations: \textbf{(1)} In the presence of a clean input, frequency attacks \emph{partially} focus on the poisoned bases; \textbf{(2)} In the presence of a poisoned input, 
We make the following key observation; \emph{the frequency bases the network attends to for predicting clean samples differ drastically from those for poisoned samples.} This observation is the fundamental observation behind FREAK.


\paragraph{FREAK Defense. } %Given that there is a significant shift in the bases the network attends to when presented with clean samples versus poisoned samples, can we devise 
 During inference time, the victim is presented with a sample which may or may not be poisoned. Since the victim has access to clean samples from their test set, we find that a simple mechanism to detect poisoned samples is computing a statistical metric, such as Wasserstein distance, between the distribution of the indices of the top-$k$ most sensitive frequency bases of a sample under inspection sample and that of clean samples. More precisely, we define $\hat{\nabla}_{\text{FREAK}}(x)$ as 

%\bibi{what user? be more passive here and say for example at inference time when a samples is not known to be x or y}
%\bibi{be affirmative. instead say based on our observation}

\begin{equation}
\begin{aligned}
\small
\hat{\nabla} _{\text{FREAK}}(x)[i,j] = \begin{cases}
                          1 & \text{if } {\nabla} _{\text{FREAK}}(x)[i,j] \in \text{top-}k({\nabla} _{\text{FREAK}}(x)), \\
                          0 & \text{otherwise}.
                        \end{cases}
\end{aligned}
\end{equation}

\noindent \ie $\hat{\nabla}_{\text{FREAK}}(x)$ is a binary matrix with value 1 in the locations of the top-$k$ values of $\nabla_{\text{FREAK}}(x)$. The distance between the distribution of the indices of the top-$k$ most sensitive frequency bases can be written as, 
\begin{equation}\label{freak_eq}
    \gamma(x,y) = d(\text{pool}(x),\text{pool}(y)),
\end{equation}
where $d$ is the Wasserstein distance and $\text{pool}$ is a simple sum-pooling function %\bibi{is this necessary; maybe move this detail to supp}, 
that aggregates values to obtain a distribution like mapping out of the binary matrices. 
% The first Wasserstein distance is computed as, 

% \begin{equation}
% \begin{aligned}
% d = W_1 (u, v) = \inf_{\pi \in \zeta (u, v)} \int_{\mathbb{R} \times
%         \mathbb{R}} |x-y| \mathrm{d} \pi (x, y),
% \end{aligned}
% \end{equation}
%\bibi{do not forget to punchtuate equations, treat them as text. see above.}
% \bibi{why not add the definition in discrete form?}
% \noindent where $\zeta (u, v)$ is the set of (probability) distributions on $\mathbb{R} \times \mathbb{R}$ whose marginals are $u$ and $v$ on the first and second factors respectively. \bibi{the above is unnecessarily complicated, people know what wasserstien distance is.}


% \paragraph{Defense. } %Given that there is a significant shift in the bases the network attends to when presented with clean samples versus poisoned samples, can we devise 
% The previous observation is the fundamental observation behind FREAK. Since the user has access to clean samples, a simple mechanism for detecting poisoned samples could be computing the statistical measure, such as Wasserstein distance, between the top-$k$ most sensitive frequency bases of an inspection sample and that of clean samples.


\vspace{0.3cm}
\noindent The recipe for FREAK is visually presented in Figure \ref{pipeline} and is described below.
\begin{enumerate}
    \item From the test set $\mathcal{D}_{test}$, create two subsets of samples, a held-out set $\mathcal{D}_h = \{x_{h_{1}},x_{h_{2}},..,x_{h_{n}}\}$ and a clean-experimental set $\mathcal{D}_c = \{x_{c_1},x_{c_2},..,x_{c_m}\}$  where $\mathcal{D}_c\cap \mathcal{D}_h = \phi$ and $\mathcal{D}_c \cup \mathcal{D}_h \subseteq \mathcal{D}_{test}$.
    \item Compute   $\hat{\nabla}_{h} = \frac{1}{n} \sum_{j=1}^{n}
    \hat{\nabla}_{\text{FREAK}} (x_{h_j})$. 
    \item Compute and store the distance vector $\mathbf{\Gamma}_{i} = \gamma(\hat{\nabla}_{\text{FREAK}}(x_{c_i}),\hat{\nabla}_h)$ for $ i=1,..,m$. 
    \item Fit a Gaussian distribution over the rows of $\mathbf{\Gamma}$. The obtained distribution is denoted by $\mathcal{N}(\mu_\mathbf{\Gamma},\sigma_\mathbf{\Gamma}^2)$ where ${\mu}_\Gamma$ and ${\sigma}_\Gamma^2$ are the mean and covariance of the fit Gaussian distribution.
    \item Compute the log-likelihood values of the samples of $\mathcal{D}_c$ belonging to the previously fit Gaussian, 
\begin{equation}
\small
\begin{aligned}
\mathcal{L}\mathcal{L}_{c_i} = 
\text{log } p(x \mid \mu_\mathbf{\Gamma}, \sigma_\mathbf{\Gamma}^2) = -\frac{1}{2} \text{log}(2\pi\sigma_\mathbf{\Gamma}^2) - \frac{(x - \mu_\mathbf{\Gamma})^2}{2\sigma_\mathbf{\Gamma}^2}
\end{aligned}
\end{equation}  




    %$\{\mathcal{L}\mathcal{L}_{c_{1}},...,\mathcal{L}\mathcal{L}_{c_{m}}\}$
    for $i=1,...,m$, and store the mean $\mu_c$ %$$\mu_{c} = \frac{1}{m} \sum_{i=1}^m \mathcal{L}\mathcal{L}_{c_{i}}$$ 
    and the standard deviation $\sigma_c$, % $$\sigma_{c} = \sqrt{\frac{\sum_{i=1}^{m}(\mathcal{L}\mathcal{L}_{c_{i}} - \mu_c)^2}{m}}$$ 
    of the log-likelihood scores.
    \item When a new sample $\Tilde{x}$ is presented for inspection, calculate the distance  $\gamma(\Tilde{x}, \hat{\nabla_h})$ and compute the log-likelihood of the vector belonging to the previously fit Gaussian distribution.  If $\mathcal{L}\mathcal{L}_{\Tilde{x}}>\mu_c+\alpha\sigma_c$ or $\mathcal{L}\mathcal{L}_{\Tilde{x}}<\mu_c-\alpha\sigma_c$ then the sample is poisoned, otherwise it's clean.
\end{enumerate}



% \subsection{Purification}
% Given a new sample $\tilde{x}$, we find that the vector of distances to the held-out dataset $\gamma(\tilde{x},\textbf{x}_h) \in \mathbb{R}^n$ is such that $\mathbb{P}(\gamma(\tilde{x},\textbf{x}_h) | ~ \mu_\Gamma, \Sigma_\Gamma) \ge \mu_c +3\sigma_c$ or that $\log \left(\mathbb{P}(\gamma(\tilde{x},\textbf{x}_h) | ~ \mu_\Gamma, \Sigma_\Gamma)\right) \leq \mu_c -3\sigma_c$. This implies that $\tilde{x}$ potentially is poisoned. Therefore, we seek to find a minimal perturbation $\delta$ such that $\mu_c -3\sigma_c \leq \log \left(\mathbb{P}(\gamma(\tilde{x} +\delta,\textbf{x}_h) | ~ \mu_\Gamma, \Sigma_\Gamma)\right) \leq \mu_c +3\sigma_c$. We formualte the following optimization problem,

% \begin{equation}
%     \begin{aligned}
%     &\min_{\delta} \|\delta\| \\
%     & \mu_c -3\sigma_c \leq \log\left(\mathbb{P}(\gamma(\tilde{x} +\delta,\textbf{x}_h) | ~ \mu_\Gamma, \Sigma_\Gamma) \right) \leq \mu_c +3\sigma_c.
%     \end{aligned}
% \end{equation}
% The previous method can be directly solved with a penalty method. However the key challenge is towards computing a differentiable $\gamma(\tilde{x} + \delta, \mathbf{x}_h)$. We denote the former with $\gamma_{\tilde{x}+\delta}$ for ease. Observe that the softmax operation with a temperature $t$, apprimxate the index of the highest value, i.e., for a temperature $t > 0$, we have
% \begin{align*}
% \sigma^{[1]} = \text{softmax}_j(\gamma_{\tilde{x}+\delta}) = \frac{\exp(\gamma^j_{\tilde{x}+\delta}/ t)}{\sum_i^n \exp(\gamma^i_{\tilde{x}+\delta} / t)}.
% \end{align*}
% Note that for $t \approx 0$, we have that $\text{softmax}$ is a one hot vector encoding of the largest element. Then we have that $\sigma^{[1]}$ is the $\text{top-1}$ element. Finding the second largest element simply reduces to finding the top largest element from the vector $\text{softmax}$ without the top element. That is consider the following softmax:
% \begin{align*}
% \sigma^{[2]} = \frac{(1 - \sigma^{[1]})\exp(\gamma_{\tilde{x}+\delta}/ t)}{\sum_i^n (1 - \sigma^{[1]})^{j} \exp(\gamma^i_{\tilde{x}+\delta} / t)}.
% \end{align*}
% Note that, with the introduced weights $1 - \sigma^{[1]}$ ,we are taking a softmax over all elements except for the second top element. Then, we have that $\text{top-2} = \sigma^{[2]} + \sigma^{[1]}$. Therefore, we have the function $\text{top-k} = \sum_{i=1}^k \sigma^{[k]}$ to be a soft approximation to the top-$k$ function where,
% \begin{align*}
% \sigma^{[v]} = \frac{(1 - \sigma^{[v-1]})\exp(\gamma_{\tilde{x}+\delta}/ t)}{\sum_i^n (1 - \sigma^{[v-1]})^{j} \exp(\gamma^i_{\tilde{x}+\delta} / t)} ~~ \forall v  \ge 2
% \end{align*}
% and $\sigma^{[1]}$ is as defined above.
% \textcolor{red}{general comment: While you might want to implement the above, the top-k function in pytorch is differentiable. They simply take gradients for the places where it is identity and zero everywhere in a similar fashion to the ReLu function (derivative exists everywhere except at the nonlinear locations which are finitely many).}