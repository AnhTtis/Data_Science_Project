\section{Introduction}\label{sec:introduction}


% Backdoor attacks on deep neural networks (DNNs) involve the injection of malicious inputs into the training process, resulting in a network that behaves normally in most cases but produces an attacker specified label in the presence of an attacker specified trigger. This in turn poses a serious threat to the security and integrity of artificial intelligence systems.

% A particularly interesting class of backdoor attacks is frequency-based backdoor attacks \cite{Hammoud2021CheckYO,Feng2021FIBAFB,back 


Deep Neural Networks (DNNs) have revolutionized machine learning leading to remarkable advances in various domains such as autonomous vehicles \cite{Grigorescu2020ASO}, medical imagery analysis \cite{Danaee2017ADL}, and fraud detection \cite{Zhang2021HOBAAN}. The increased deployment of DNNs in life-critical applications, such as autonomous driving and medical diagnosis, raised concerns particularly with the uncovered vulnerabilities in the form of adversarial attacks.


One extremely insidious form of adversarial attacks is known as backdoor attacks. Backdoor attacks inject malicious behaviour through compromising the training procedure \cite{Li2020BackdoorLA,Doan2021LIRALI}, where at inference time, the attacker introduces a special input pattern, known as a trigger, inducing a targeted prediction. % In backdoor attacks, an attacker injects malicious behaviors into a DNN which compromises the model's integrity and reliability. The attacker typically introduces a special input pattern, known as a trigger, which can induce the misclassification of arbitrary input samples to a specific target label. 
% The attacker achieves this target by training the DNN on data poisoned by the trigger \cite{Li2020BackdoorLA,Doan2021LIRALI}.
The true danger of backdoor attacks lies in their ability to bypass the normal validation procedures that ensure the accuracy and reliability of DNNs \cite{Gu2019BadNetsEB}. A backdoored model can behave normally on clean inputs and evade detection while misclassifying inputs that contain the trigger leading to severe consequences in high-stakes applications such as action recognition in surveillance systems \cite{hammoud2023look}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/pull_fig_v3.png}
    \caption{\textbf{FREAK PCA Features for Different Attacks.} The 2D PCA projection of the features extracted by FREAK are linearly separable which allows for the successful detection and separation of poisoned and clean samples. This observation holds true for frequency backdoor attacks (CYO, FTrojan and FIBA) and spatial backdoor attacks (BadNet). }
    \label{fig:my_label}
\end{figure}

Backdoor triggers were typically created in either the spatial \cite{Chen2017TargetedBA,Nguyen2021WaNetI,Doan2021LIRALI} or the latent domain \cite{Doan2021BackdoorAW,Yao2019LatentBA}. However, recent works have revealed that backdoor attacks could also be created in the frequency domain \cite{Hammoud2021CheckYO,Feng2021FIBAFB,Wang2021BackdoorAT}. Frequency-based backdoor attacks were shown to achieve high attack success rates with a capacity to elude state-of-the-art (SOTA) spatial and latent backdoor defenses. Given that adversaries have the ability to embed their poison in any frequency location across the input image channels, basic filtering techniques such as low-pass, band-pass, or high-pass filtering may not be able to eradicate the trigger.


In response to this challenge, researchers behind frequency-based backdoor attacks have proposed more advanced defenses. For instance, leveraging an autoencoder or JPEG compression \cite{Hammoud2021CheckYO} to manipulate the Fourier transform of tainted images and filter out the backdoor trigger was shown to be effective. On a different note, FTrojan \cite{Wang2021BackdoorAT} introduced two adaptive defenses that rely on either anomaly detection or signal smoothing. Nevertheless, these defenses are hampered by one or more limitations: (1) they function in the spatial domain (autoencoder \cite{Hammoud2021CheckYO}); (2) they can be circumvented by data augmentation (autoencoder and JPEG compression (\cite{Hammoud2021CheckYO}) and signal smoothing (\cite{Wang2021BackdoorAT}); (3) they cause significant drops in model accuracy on clean data (signal smoothing (\cite{Wang2021BackdoorAT}); or (4) they fail to detect the backdoor in the first place (anomaly detection \cite{Wang2021BackdoorAT}).



In this work, we analyze the distribution of the most sensitive frequency components when the DNN is presented with clean versus poisoned samples. Our analysis reveals that the frequency sensitivity to poisoned samples is considerably distinct from that of clean samples. Drawing on these findings, we present FREAK, a simple yet effective algorithm for identifying poisoned samples based on the distribution of the sensitive frequency components. Our algorithm achieves a high success rate in detecting poisoned samples %\bibi{check this for correctness} 
while maintaining a low false positive rate. Surprisingly, FREAK is not only effective against frequency-based backdoor attacks but also against some spatial backdoor attacks. %Effective detection is a crucial component for mitigation, and FREAK offers an effective solution for identifying poisoned samples. With FREAK, proper detection of poisoned samples is nearly guaranteed.


%Detection is the cornerstone of mitigation.
%In this work we analyze the distribution of the most sensitive frequency components/basis for clean samples vs poisoned samples where we discover the network frequency sensitivity to poisoned samples is significantly more different than that of clean samples. Based on those findings we propose FREAK a simple yet highly effective poisoned sample detection algorithm.  FREAK is capable of detecting poisoned samples with up to 100\% success rate with an extremely low false positive rate. FREAK is shown not only to be effective against frequency-based backdoor attacks but also against spatial backdoor attacks where were can also achieve 100\% success rate with low false positive rates. Detection is the cornerstone of mitigation and with FREAK proper detection of poisoned samples is almost guaranteed.
