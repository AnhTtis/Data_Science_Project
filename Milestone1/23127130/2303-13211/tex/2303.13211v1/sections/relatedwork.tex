\section{Related Work}\label{sec:relatedwork}

In recent years, a variety of backdoor attacks have been proposed, each of which can differ in two key aspects: the method used to generate the trigger and whether or not the labels are manipulated. In response to these attacks, a number of backdoor defenses have been developed, which can be categorized as follows: (1) defenses aimed at detecting whether a model or a set of samples have been poisoned; (2) defenses aimed at mitigating the backdoor attack; and (3) defenses that aim to both detect and mitigate the attack simultaneously.




\textbf{Backdoor Attacks.} In early backdoor attack methods, backdoor triggers were designed in the spatial domain. For instance, \cite{Gu2019BadNetsEB} proposed poisoning the data by adding a black square in the corner of a few training samples. \cite{Liu2018TrojaningAO} solved an optimization problem to find an optimal backdoor trigger for a given mask, such as the square trigger introduced in \cite{Gu2019BadNetsEB}. However, as research progressed, the importance of invisible triggers that can bypass human inspection became evident, leading to the development of invisible backdoor attacks. This area of research has since evolved, with works such as \cite{Liu2020ReflectionBA,Chen2017TargetedBA,LSB,Chen2021UsePN,Wang2019NeuralCI,Doan2021LIRALI,Bagdasaryan2021BlindBI,Zhang2021PoisonIR,Ren2021SimtrojanSB,Liao2020BackdoorEI} paving the way. \cite{Chen2017TargetedBA} proposed blending the backdoor trigger with clean images instead of stamping it. \cite{LSB} and \cite{Li2021InvisibleBA} adopted least significant bit and textual string encoding algorithms from steganography to poison the data, respectively. \cite{Nguyen2021WaNetI} used image warping as a poisoning technique, while \cite{Doan2021LIRALI} emphasized the importance of having learnable transformations to embed an optimal backdoor trigger into the poisoned samples. \cite{Chen2021UsePN} showed that procedural noise, such as Gabor and Perlin noise, could be used as a backdoor trigger. \cite{Turner2018CleanLabelBA,Barni2019ANB,Turner2019LabelConsistentBA} suggested clean-label backdoor attacks, which apply a backdoor trigger without manipulating the class label of the images.

More recent works suggested exploring alternative domains. For instance, \cite{Doan2021BackdoorAW} generated imperceptible backdoor triggers by minimizing the Wasserstein distance (\cite{Kolouri2019GeneralizedSW}) between latent representations of clean and poisoned samples. \cite{Zeng2021RethinkingTB} analyzed the characteristics of spatial backdoor attacks in the Fourier domain and present a technique to generate smooth spatially visible triggers that are smooth in the frequency domain. Finally, and most relevant to our work, \cite{Hammoud2021CheckYO,Feng2021FIBAFB,Wang2021BackdoorAT} showed the power of embedding backdoor attacks in the frequency domain. \cite{Hammoud2021CheckYO} utilized the concept of Fourier heatmaps from \cite{Yin2019AFP} to detect the DNNâ€™s most sensitive frequency bases, which are then used to mount the poisoning information. \cite{Feng2021FIBAFB} suggested blending the low-frequency content of a trigger image with those of clean samples to generate poisoned data. \cite{Wang2021BackdoorAT} converted the color channels from RGB to YUV representation, after which a mix of mid- and high-frequency components is poisoned to bypass possible low-pass or high-pass filtering.

\textit{In this work we analyze the properties of frequency-based backdoor attacks. Based on the uncovered properties we propose a new defense.}



\textbf{Backdoor Defenses:} As mentioned above, backdoor defenses try to detect the attack \cite{Gao2019STRIPAD,Liu2019ABSSN,Huster2021TOPBD,Zheng2021TopologicalDO,Hammoudeh2021SimpleAD,Chen2021DePoisAA,Soremekun2020ExposingBI,Tang2021DemonIT,Jin2020AUF}, mitigate the attack \cite{Liu2018FinePruningDA,Liu2017NeuralT,Cheng2020DefendingAB,Li2020RethinkingTT}, or both detect and mitigate the attack \cite{Tran2018SpectralSI,Chen2019DetectingBA,Wang2019NeuralCI,Guo2019TABORAH,Liu2019ABSSN,Doan2020FebruusIP,Hayase2021SPECTREDA,Jiang2021InterpretabilityGuidedDA,Qiao2019DefendingNB,Chen2019DeepInspectAB}.

Early backdoor defenses, such as neural cleanse \cite{Wang2019NeuralCI}, observed that backdoor attacks create an anomalously small distance between all classes and the poisoned class. On the basis of this observation, the authors proposed solving an optimization problem to detect whether a model has been poisoned after which the backdoor trigger is reverse engineered. Later, improved versions of this defense were introduced by TABOR \cite{Guo2019TABORAH} and ABS \cite{Liu2019ABSSN}.

Other backdoor attacks focused on understanding the activations of backdoor attacked models. Fine pruning \cite{Liu2018FinePruningDA} argued that backdoor attacks could be detected by pruning neurons that are dormant in the presence of clean samples; activation clustering \cite{Chen2019DetectingBA} and \cite{Tran2018SpectralSI, Hayase2021SPECTREDA} applied cluster analysis and robust statistics to detect and mitigate backdoor attacks. \cite{Doan2020FebruusIP} observed that backdoor attacks shift the network's attention away from the object, and therefore proposed applying image restoration to reconstruct the spatially poisoned region. Recently, \cite{Zheng2021TopologicalDO} used homology from topological analysis to uncover structural abnormalities unique to poisoned models.

Unfortunately, existing defenses against frequency backdoor attacks are very scarce and have been shown to fail in certain scenarios. For example, \cite{Hammoud2021CheckYO} proposed using an autoencoder or JPEG compression to manipulate the Fourier transform of poisoned images and hence neutralize the effect of the backdoor trigger. \cite{Wang2021BackdoorAT} proposed applying preprocessing techniques such as Gaussian and Wiener filtering to remove the backdoor. However, these defenses cause a huge drop in clean data accuracy or fail to neutralize the backdoor attack.

\textit{Considering the limited success of existing defenses in defending against both spatial and frequency backdoor attacks and the critical importance of detecting poisoned samples, FREAK stands out as an effective and necessary addition to the arsenal of defenses against backdoor attacks.}




