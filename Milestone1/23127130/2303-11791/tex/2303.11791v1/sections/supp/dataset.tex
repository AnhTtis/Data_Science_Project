\section{NLOS-Track Dataset}

\subsection{Real-shot Data}

\subsubsection{Video collection and position calibration}
To obtain paired wall-shooting videos and ground truth trajectories, we record the relay wall with one camera and stick another camera on the ceiling to have a corresponding top-viewed video at the same time.
Subjects walk around the hidden room with Aruco code on head so that we can track him with the top-viewed video. To improve the accuracy and robustness of tracking, we put four Aruco codes on the four corners of a hard and flat board. 
We use the Aruco API provided in OpenCV~\cite{bradski2000opencv} to obtain the 3D pose of Aruco codes with respect to ceiling camera. 
In each frame of the top-viewed video, only when all four codes are detected, we take the average of four codes' coordinates as the character's position coordinate. Then translation and rotation are applied to transform the character's coordinate from the camera system to the world system.

\subsubsection{Stream alignment and cropping}
We use two pulse signals that only last for a short moment to mark the beginning and the ending time point of a single clip. Two signals are designed to be visible by exposing them to both two cameras.
Since the time intervals of the two signals in the two videos are equal, we are allowed to manually align the two videos at a frame-level precision by aligning the beginning time point and the ending time point in the timeline. 
\looseness=-1

\subsubsection{Data cleaning}
Although we use four codes to improve the tracking robustness, there are still some point-sequences that we fail to track due to jitters and blurs of pictures. 
Assume that the missing points in one sequence are $\{\Vec{p}_i\}$, we use a linear interpolation to complete them:
\begin{equation}
\begin{aligned}
    \Vec{p}_{i} &\approx  \frac{N-i}{N}\Vec{p}_0 + \frac{i}{N}\Vec{p}_N \\
    &= \Vec{p}_0 + \frac{i}{N}\left( \Vec{p}_N - \Vec{p}_0 \right),\quad i=1,2,...,N-1,
\end{aligned}
\end{equation}
where $i$ is the index of the missing point. $\Vec{p}_0$ denotes the recorded point previous to the missing sequence while $\Vec{p}_N$ denotes the subsequent recorded point. To perform this interpolation, we make a reasonable assumption that there are no sharp turns or changes in speed within $N$ frames. In addition, we set a threshold of $N \le 10$ when conducting the interpolation, which reinforces this assumption so that we won't introduce unbearable errors during data cleaning.

After completing missing points and excluding clips that can not be completed, we crop the remaining paired videos and ground truth trajectories into many 250-frame clips. The videos are save as \verb|.npy| files and trajectories are saved as \verb|.mat| files, along with corresponding room size.

\subsection{Random Trajectory Generation}

To simulate the realistic situation of people walking in the room, we use a heuristic algorithm for generating near-real continuous trajectories frame by frame. Given the room size, we first select a random position $\Vec{p}_0=(p^x_0, p^y_0)$\footnote{Here we use $p$ to denote position instead of $x$, which is distinguished from $x$ used to denote direction.} within the room as the initial position and a random vector $\Vec{v}_0=(v^x_0, v^y_0)$ as the initial walking direction. Considering the effect of inertia and momentum in real world, in each subsequent frame we make a slight and random change $\Delta \Vec{v}_{t}$ to the current walking direction as that of the next step:
\begin{equation}
    \Vec{v}_{t+1} = \Vec{v}_{t} + \tau \cdot \Delta \Vec{v}_{t} ~,~ t=0,1,...,
\end{equation}
where $\tau$ is an adjustable parameter called \textit{turning rate}.
By setting $\tau$ properly, one can control the curvature degree when generating trajectories. Specifically, we set the turning rate $\tau$ to 0.15 in our dataset because this value guarantees a reasonable continuity and rotation magnitude of generated trajectories.
In each frame, the character takes a step forward in the new direction $\Vec{v}_{t+1}$ and a continuous trajectory can be generated:
\begin{equation}
    \Vec{p}_{t+1} = \Vec{p}_{t} + \Vec{v}_{t} ~,~ t=0,1,...
\end{equation}
Note that the distance (in meters) of each frame step is not fixed, but is randomly sampled with a uniform distribution $U(0.03, 0.04)$ for the sake of being realistic.

In addition, we ensured that the characters did not cast any direct shadow on the wall that would be visible to the naked eye while generating the trajectory.

\subsection{Data Format in Dataset}

We store all video clips in \verb|.npy| format after cropping them to squares and resizing them to a dimension of $T \times C \times H \times W$, where $C=3$ is the number of channels and $H=W=128$ is the spatial dimension. $T=250$ in real-shot data while $T=320$ in synthetic data.
