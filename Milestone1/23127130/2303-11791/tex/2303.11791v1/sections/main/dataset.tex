\subsection{NLOS-Track Dataset}

Compared to existing datasets for passive NLOS imaging (\cref{tab:datasets}), NLOS-Track is focused on fitting realistic dynamic scenes. Therefore, rather than including tons of static photographs or using unrealistic objects as tracking targets (\eg, humanoid dolls or cardboard mannequins), NLOS-Track contains more than one thousand clips of videos that shoot at a blank relay wall while a real person is walking around the hidden room. We also render rich realistic scenes that are diverse in room size, walking character, wall material, and lighting condition.


\subsubsection{Real-shot data collection}

\begin{figure}[t]
  \centering
  \begin{subfigure}{\linewidth}
    \includegraphics[width=0.48\linewidth]{images/out_view.jpg}
    \includegraphics[width=0.48\linewidth]{images/camera_view.jpg}
    \caption{~}
    \label{fig:dataset-real}
  \end{subfigure} \\
  \begin{subfigure}[t]{\linewidth}
    % \includegraphics[width=0.9\linewidth]{images/render_setup.png} \\
    \adjustbox{valign=t}{\includegraphics[width=0.48\linewidth]{images/characters.png}}
    \adjustbox{valign=t}{\includegraphics[width=0.48\linewidth]{images/dataset_dist.pdf}}
    \caption{~}
    \label{fig:dataset-synthetic}
  \end{subfigure}
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \caption{(a) A setup scenario with a camera observing the wall from outside the room. Left: A panorama. Right: View from the camera side. (b) Left: Part of the characters we use in synthetic data. Right: Room size (in meters) distribution and each dot represents a room. }
   \label{fig:dataset}
  \vspace{-10pt}
\end{figure}

We use a consumer-grade micro SLR camera (Canon EOS RP) to capture videos of the relay wall at 25 FPS. To obtain the ground truth of the walking trajectory, we stick a USB camera (HIKVISION E14a) to the ceiling, which records the whole process of people walking from a top view at 25 FPS as well. From the top viewed videos we are allowed to use Aruco codes to locate the walking person's coordinate frame by frame at a sub-centimeter precision. 
Post-processing of recorded videos includes manually aligning the wall-shooting video stream and coordinate stream and cropping them into video clips of 250 frames.
We ask 3 subjects (each change 3 different clothes) to walk at various speeds to generalize the dataset.
Please consult supplementary materials for details about the real-shot dataset.
\looseness=-1

\subsubsection{Rendering setup}

In order to reduce the gap between the synthetic data and the photo-realistic data, we use the Cycles render engine in Blender \cite{blender}, which is a physically-based path tracer and provides excellent performance in rendering realistic images. All 3D human-like characters and skeleton models for walking animation are acquired from \href{https://www.mixamo.com/}{Mixamo}, a free animation platform of Adobe. After the character is imported and the walking animation is bound to the character\footnote{Refer to supplementary materials for more details about our random trajectory generation strategy.}, we use the Cycles render engine to conduct steady-state rendering of the relay wall frame by frame. All video clips have 320 frames each, at 30 FPS, and a resolution of $256 \times 256$ pixels. The videos are firstly rendered into \verb|.png| sequences with 8-bit RGB color depth and then pre-processed into \verb|.npy| files for IO efficiency.

On an NVIDIA A100 graphics card, the rendering speed is about 0.8 seconds per frame at a resolution of $256 \times 256$ pixels. In total, we spend about 70 hours rendering the full synthetic dataset of 1000 video clips.

\begin{table}
  \centering
  \scalebox{0.7}{
  \begin{tabular}{@{}lcccc@{}}
    \toprule
    Dataset & Modal & Data Source  & Setup & Size \\
    \midrule
    \multirow{2}*{Platform\cite{klein2018quantitative}} & \multirow{2}*{Transient}  & \multirow{2}*{Synthetic}  & Static \&     & \multirow{2}*{/} \\
    ~                                                   & ~                         & ~                         & Dynamic \\
    Z-NLOS\cite{galindo2019dataset}                     & Transient                 & Synthetic                 & Static        & 300 measurements  \\
    % Peng \etal \cite{peng2021towards}                   & Transient                 & Real-shot                 & Static        & 400 measurements  \\
    NLOS-Passive\cite{geng2022passive}                  & Steady                    & Real-shot                 & Static        & 3,200,000 images\\
    NLOS-ES\cite{Wang2022event}                         & Event                     & Real-shot                 & Dynamic       & 4,180 images\\
    \midrule
    \specialrule{0em}{1pt}{1pt}
    \midrule
    NLOS-Track                                          & \multirow{2}*{Steady}     & Real-shot                 & \multirow{2}*{Dynamic} & $\sim$1,500 videos\\
    (Ours)                                              & ~                         & \& Synthetic              & ~             & (445,000 frames)\\
    \bottomrule
  \end{tabular}
  }
  \caption{\textbf{A brief summary of existing NLOS datasets.} Transient and Steady under the Modal column denotes time-resolved transient-state and conventional RGB steady-state respectively. ``/" denotes difficulty to report the data size due to mixed data types.}
  \label{tab:datasets}
  \vspace{-10pt}
\end{table}% 这个表格没有体现出咱们数据集的优势

\vspace{-5pt}
\subsubsection{Other considerations on generalization} \label{sec:dataset_generalization}

We randomize several settings before rendering each clip of the video for generalization purposes. The room sizes (in meters) are sampled from a uniform distribution $U(3, 7)$, as shown in \cref{fig:dataset-synthetic}. Besides, the character is randomly selected among more than 20 characters with a variety of outfits. We also change the position and luminosity of light sources and the camera position. Four different floor styles are applied randomly. The texture and roughness of the photographed wall are randomized as well. All these settings are recorded in a \verb|.yaml| file to guarantee reproducibility.

Besides, we simulate the real-world noise in synthetic data to make them more realistic. We first compute the mean and standard deviation of real data and synthetic data at the pixel level and then aligned their statistics by adding noise to the rendering results.

% When training the model, although each video has 320 frames in total, we only randomly select a 128-frame continuous clip from each video sample. There are two reasons for doing so. On the one hand, this practice could equivalently increase the sample size in the dataset. On the other hand, we don't want the model to always warm up on the same frames in training, which will be discussed with more details in \cref{sec:warmup}.
