\pdfoutput=1
% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)
% \special{dvipdfmx:config z 0}
\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
% \usepackage[draft]{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{xcolor}
\usepackage{multirow}
\usepackage{stfloats}
\usepackage{adjustbox}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{1722} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\definecolor{myGreen}{HTML}{559D64}
\definecolor{myBlue}{HTML}{3D69B9}

\newcommand{\ToBeRevised}[1]{\textcolor{red}{#1}}
\newcommand{\Revised}[1]{\textcolor{orange}{#1}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\newcommand{\ins}[1]{$^{#1}$}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Propagate And Calibrate: Real-time Passive Non-line-of-sight Tracking}

\author{Yihao Wang\ins{1}\thanks{Equal contribution.} \quad Zhigang Wang\ins{1}\samethanks[1] \quad Bin Zhao\ins{1,2}\thanks{Corresponding author.} \quad Dong Wang\ins{1} \quad Mulin Chen\ins{1,2} \quad Xuelong Li\ins{1,2}\samethanks[2]\\
\ins{1}Shanghai AI Laboratory \qquad \ins{2}Northwestern Polytechnical University\\
{\tt\small \{wangyihao,wangzhigang,zhaobin,wangdong,chenmulin\}@pjlab.org.cn \qquad li@nwpu.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% second author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
    \vspace{-3mm}
    Non-line-of-sight (NLOS) tracking has drawn increasing attention in recent years, due to its ability to detect object motion out of sight. Most previous works on NLOS tracking rely on active illumination, \eg, laser, and suffer from high cost and elaborate experimental conditions. Besides, these techniques are still far from practical application due to oversimplified settings. In contrast, we propose a purely passive method to track a person walking in an invisible room by only observing a relay wall, which is more in line with real application scenarios, \eg, security. To excavate imperceptible changes in videos of the relay wall, we introduce difference frames as an essential carrier of temporal-local motion messages. In addition, we propose PAC-Net, which consists of alternating propagation and calibration, making it capable of leveraging both dynamic and static messages on a frame-level granularity. To evaluate the proposed method, we build and publish the first dynamic passive NLOS tracking dataset, NLOS-Track, which fills the vacuum of realistic NLOS datasets. NLOS-Track contains thousands of NLOS video clips and corresponding trajectories. Both real-shot and synthetic data are included. Our codes and dataset are available \href{https://againstentropy.github.io/NLOS-Track/}{here}.
    \looseness=-1
\end{abstract}

%%%%%%%%% BODY TEXT
\input{sections/main/introduction}

\input{sections/main/related_work}

\input{sections/main/problems_formulation}

\input{sections/main/tracking_method}

\input{sections/main/experiments}

\input{sections/main/discussion}

\input{sections/main/conclusion}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{bib_main}
}

\end{document}
