\section{Problem Formulation and Signal Extraction}

\subsection{NLOS Tracking Problem}

Passive NLOS imaging aims to excavate information about a hidden scene through the diffuse reflection of ambient light. This task can be accomplished by observing and analyzing a relay wall. Every slight change in the hidden room could induce an imperceptible variation of the reflection, thus influencing the wall image.
The complicated optical system can be formulated as an imaging function $\mathcal{F}$:
% \looseness=-1
\begin{equation} \label{eq:img_map}
\vspace{-1pt}
    I = \mathcal{F}(\Vec{x}, \Theta),
\end{equation}
where $I$ denotes the photo of the relay wall, depending on the position $\Vec{x}$ of a person and other scene configuration $\Theta$. The scene configuration $\Theta$ mainly includes the appearance of the person, the material of the wall, and the illumination condition. The imaging function $\mathcal{F}$ \textit{compresses} the light field within the hidden region and casts it onto the relay wall.
According to \cref{eq:img_map}, a change in the scene (\eg, a person's position) will cause a change of the \textit{shadow} on the relay wall correspondingly. Mathematically, this change can be formulated as the partial derivative of \cref{eq:img_map}:
% \looseness=-1
\begin{equation} \label{eq:diff_I}
    \frac{\partial I}{\partial \Vec{x}} = \frac{\partial \mathcal{F}(\Vec{x}, \Theta)}{\partial \Vec{x}}.
\end{equation}

Through the guidance of \cref{eq:img_map} and \cref{eq:diff_I}, it is possible to track a person out of LOS by scrutinizing the faint shadow changes. Given a series of discrete observations over time $\{I_0, ..., I_t, ...\}$, \ie, raw frames of a video, NLOS tracking aims to find an inverse imaging function, $\mathcal{F}^{-1}$, which reconstructs the causes $\{x_0, ..., x_t, ...\}$, \ie, the trajectory of the moving objects in real time.


\begin{figure}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/diff.pdf}
  \caption{\textbf{Visualization of raw frames and difference frames.} The ``diff." is the abbreviation for ``difference", which means subtracting the previous frame from the current frame. We visualize difference frames after taking absolute value and normalizing to $[0, 1]$ for higher contrast.}
  \label{fig:raw_diff}
\end{figure}

\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=\linewidth]{images/pipeline.pdf}

  \caption{\textbf{Visualization of tracking pipeline with PAC-Net.}
  There are two stages in the whole pipeline, \textit{Warm-up Stage} and \textit{Tracking Stage}. Specifically, given a live streaming video, the Warm-up Stage leverages the first $W$-th frames to ``warm up" the hidden state $\mathbf{h}$ using the Warm-up PAC-Cell. Then the Tracking PAC-Cell infers the trajectory in the Tracking Stage. Both two cells take in dynamic feature (dark green arrow) and static feature (dark blue arrow) and updates the hidden state (light yellow arrow) alternately. Note that PAC-Net can process every incoming frame in an online manner. (Best viewed in color.)
  }
  \label{fig:pipeline}
\end{figure*}

\subsection{Difference Frames} \label{sec:diff_frame}

Most previous works neglect the motion information in tracking tasks, while it can play an important role in guiding the tracking process.
To extract motion information explicitly, we can limit our vision to the vicinity of a single moment $t$. Along with \cref{eq:diff_I}, we can derive the relation between the relay wall's variation $\Delta I_t$ with the person's movement $\Delta \Vec{x_t}$ in the form of finite difference:
\looseness=-1
\begin{equation} \label{eq:delta_I}
\begin{aligned}
\left.\frac{\Delta I}{\Delta \vec{x}} \right|_t &\approx \left. \frac{\partial \mathcal{F}(\vec{x}, \Theta)}{\partial \vec{x}} \right|_t \\
\Longrightarrow I_{t+1} - I_t = \Delta I_t &\approx \left. \frac{\partial \mathcal{F}(\vec{x}, \Theta)}{\partial \vec{x}} \right|_{\vec{x} = \vec{x}_t}  \Delta \vec{x_t} \\
& = \mathcal{G}(\vec{x_t}, \Delta \vec{x_t}, \Theta),
\end{aligned}
\end{equation}
where $\Delta I_t$ is the \textit{difference frame}, which is obtained by subtracting the previous frame from the current frame in the video (\cref{fig:raw_diff}), and $\mathcal{G}$ denotes the imaging function of difference frame. As shown in \cref{eq:delta_I}, the person's movement $\Delta \Vec{x_t}$ directly influences the difference frame $\Delta I_t$. Given a combination of position and motion $\left( \vec{x}, \Delta \vec{x} \right)$, $\mathcal{G}$ maps the tuple to a corresponding difference frame. Therefore, through excavating difference frames, we can further leverage dynamic motion information beyond static positions, which significantly benefits the NLOS tracking task. See \cref{sec:p-net} for more details.

Furthermore, the temporally local nature of difference frames enables them to provide ``clean" motion information. In comparison, although background frame estimation and subtraction~\cite{bouman2017turning, PrafullSharma2021WhatYC, he2022non} can increase SNR, this practice inevitably introduces the information of other time to every single frame, thus making static information ``dirty".
