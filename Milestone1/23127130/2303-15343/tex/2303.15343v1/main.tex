\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{lipsum}
\usepackage{url}

\usepackage{tabulary}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{xcolor,colortbl}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{setspace}

% For subfigs
\usepackage{caption}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}

%itemize
\usepackage{enumitem}

%---------------------
\usepackage{listings}
\usepackage{upquote}  % Straight ' in all listings
\usepackage{xcolor,colortbl}
\lstdefinestyle{overleaf}{
    backgroundcolor=\color[rgb]{0.95,0.95,0.92},   
    commentstyle=\color[rgb]{0,0.6,0},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color[rgb]{0.5,0.5,0.5},
    stringstyle=\color[rgb]{0.58,0,0.82},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstdefinestyle{simple}{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.5pt}{7.5pt}\ttfamily\selectfont,
  keywordstyle=\fontsize{7.5pt}{7.5pt}\ttfamily\selectfont,
}
\lstset{style=overleaf}
%--------------------------------

% \usepackage{svg}


%----------------------------------------------------------
% this is for adding footnote after algorithm
\usepackage{etoolbox}
\makeatletter
\AfterEndEnvironment{algorithm}{\let\@algcomment\relax}
\AtEndEnvironment{algorithm}{\kern2pt\hrule\relax\vskip3pt\@algcomment}
\let\@algcomment\relax
\newcommand\algcomment[1]{\def\@algcomment{\footnotesize#1}}
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
  \def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
  \def\@fs@post{}%
  \def\@fs@mid{\kern2pt\hrule\kern2pt}%
  \let\@fs@iftopcapt\iftrue}
\makeatother
%----------------------------------------------------------

\newcommand{\PAR}[1]{\vskip4pt \noindent {\bf #1~}}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{***} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\def \slit {SigLiT\xspace}
\def \slip {SigLIP\xspace}
\newcommand{\authsep}{\;\;}

\def \frozen {\raisebox{-.1\height}{\includegraphics[height=0.7\baselineskip]{figs/frozen_emoji_svg-tex.pdf}}\xspace}
\def \unlocked {\raisebox{-.1\height}{\includegraphics[height=0.7\baselineskip]{figs/unlocked_emoji_svg-tex.pdf}}\xspace}

\begin{document}

%%%%%%%%% TITLE
\title{Sigmoid Loss for Language Image Pre-Training}

\author{Xiaohua Zhai$^{\star}$ \authsep Basil Mustafa \authsep Alexander Kolesnikov \authsep Lucas Beyer$^{\star}$\\
Google Research, Brain Team, ZÃ¼rich \\
{\tt\small \{xzhai, basilm, akolesnikov, lbeyer\}@google.com}
}

\maketitle
{\let\thefootnote\relax\footnote{{$^{\star}$equal contribution}}}

% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi


% ==================================================================
% ==================================================================
\begin{abstract}
We propose a simple pairwise sigmoid loss for image-text pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4\,k batch size and a Large LiT model at 20\,k batch size, the latter achieves 84.5\% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32\,k being sufficient. We hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.
\end{abstract}

% ==================================================================
% ==================================================================
\section{Introduction}\label{sec:intro}
% ==================================================================
% ==================================================================

Contrastive pre-training using weak supervision from image-text pairs found on the web is becoming the go-to method for obtaining generic computer vision backbones, slowly replacing pre-training on large labelled multi-class datasets.
The high-level idea is to simultaneously learn an aligned representation space for images and texts using paired data.
Seminal works CLIP~\cite{clip} and ALIGN~\cite{align} established the viability of this approach at a large scale, and following their success, many large image-text datasets became available privately~\cite{lit,pali,lemon,git} and publicly~\cite{laion5b, coyo700m, redcaps, cc12m, wit_dataset}.

The standard recipe to pre-train such models leverages the image-text contrastive objective. It aligns the image and text embeddings for matching (positive) image-text pairs while making sure that unrelated (negative) image-text pairs are dissimilar in the embedding space.
This is achieved via a batch-level softmax-based contrastive loss, applied twice to normalize the pairwise similarity scores across all images, then all texts. 
A naive implementation of the softmax is numerically unstable; it is usually stabilized by subtracting the maximum input value before applying the softmax~\cite{goodfellow16}, which requires another pass over the full batch.

\begin{table}[t]
  \setlength{\tabcolsep}{0pt}
  \setlength{\extrarowheight}{5pt}
  \renewcommand{\arraystretch}{0.75}
  \newcolumntype{C}{>{\centering\arraybackslash}X}
  \newcolumntype{R}{>{\raggedleft\arraybackslash}X}
  \centering
  %\vspace{-2em}
  \caption{\textbf{\slit and \slip results}. 
  Sigmoid loss is memory efficient, allows larger batch sizes (BS) that unlocks language image pre-training with a small number of chips.
   \protect\slit model with a \textit{frozen public} \protect\frozen L/16 checkpoint~\cite{augreg}, trained on the LiT image-text dataset~\cite{lit} using four TPU-v4 chips for one day, achieves 79.7\% 0-shot accuracy on ImageNet.
   The same setup with a g/14 checkpoint~\cite{vitg} leads to 84.5\% accuracy, trained for two days.
   With a \textit{public unlocked} \protect\unlocked B/16 image checkpoint~\cite{augreg}, trained on the WebLI dataset~\cite{pali}, \slip achieves 71.0\% 0-shot accuracy using 16 TPU-v4 chips for three days.
   The last two rows show results with randomly initialized models.
  }\label{tbl:teaser}
  \vspace{0.5mm}
  \begin{tabularx}{\linewidth}{p{1.1cm}p{0.1cm}Rp{0.01cm}Cp{0.01cm}Cp{0.01cm}Cp{0.01cm}Cp{0.01cm}C}
    \toprule[1pt]
     && Image && Text && BS && \#TPUv4 && Days && INet-0 \\
    \arrayrulecolor{black!20}\midrule
    \slit  && \frozen B/8\,\, && \,\,L$^*$  && 32\,k && 4 && 1 && 79.7 \\
    \slit  && \frozen g/14 && L  && 20\,k && 4 && 2 && 84.5  \\
    \arrayrulecolor{black!20}\midrule
    \slip  && \unlocked B/16 && B  && 16\,k && 16 && 3 && 71.0 \\
    \slip  && B/16 && B  && 32\,k && 32 && 2 && 72.1 \\
    \slip  && B/16 && B  && 32\,k && 32 && 5 && 73.4 \\
    \arrayrulecolor{black}\bottomrule
  \end{tabularx}
  \vspace{-0.35cm}  % Flushright adds annoying gap.
  \begin{flushright}
  \footnotesize{\textit{$^*$ We use a variant of the L model with 12 layers.}}
  \end{flushright}
\end{table}

In this paper, we propose a simpler alternative: the sigmoid loss.
It does not require any operation across the full batch and hence greatly simplifies the distributed loss implementation and boosts efficiency.
Additionally, it conceptually decouples the batch size from the definition of the task.
We compare the proposed sigmoid loss with the standard softmax loss across multiple setups. 
In particular, we investigate sigmoid-based loss with two prominent approaches for image-text learning: CLIP~\cite{clip} and LiT~\cite{lit}, which we call sigmoid language image pre-training (\textit{\slip}) and sigmoid LiT (\textit{\slit}), respectively. 
We find that the sigmoid loss performs significantly better than the softmax loss when the batch size is smaller than 16\,k.
As the train batch size grows, the gap closes.
Importantly, the sigmoid loss is symmetric, requires just a single pass, and a typical implementation requires less memory than the softmax loss.
This enables successful training of a \slit model at a batch size of \textit{one million}.
However, we find that the performance saturates with growing batch size, both for softmax and sigmoid.
The good news is that a reasonable batch size, i.e. 32\,k, is sufficient for image-text pre-training.
This conclusion also holds for multilingual \slip training on over 100 languages. 

In Table~\ref{tbl:teaser}, we present setups for image-text pre-training that require a moderate amount of TPUv4 chips for training.
\slit is surprisingly efficient, reaching 79.7\% zero-shot accuracy on ImageNet in just a single day on four chips.
\slip's more demanding from-scratch training reaches 73.4\% zero-shot accuracy in 5 days with 32 TPUv4 chips. This compares favorably to prior works such as FLIP~\cite{flip} and CLIP~\cite{clip}, which require approximately 5 and 10 days respectively on 256 TPUv3 cores.
When fine-tuning a pre-trained vision backbone in \slip, denoted as \protect\unlocked in Table~\ref{tbl:teaser},
we found that disabling the weight decay on the pre-trained backbone leads to better results (see Figure~\ref{fig:unlocked_finetuning} for details).
We hope our work paves the way for making the nascent language-image pre-training field more accessible.

% ==================================================================
% ==================================================================
\section{Related Work}\label{sec:related_work}
% ==================================================================
% ==================================================================

\PAR{Contrastive learning with the sigmoid loss.} One prior work proposes a similar sigmoid loss for the task of unsupervised  dimensionality reduction~\cite{hadsell2006dimensionality}; in the scope of contrastive image-text learning, the vast majority of works rely on the softmax-based InfoNCE loss as popularized by~\cite{cpc}.

\PAR{Contrastive language-image pre-training} has become popular since CLIP~\cite{clip} and ALIGN~\cite{align} applied softmax contrastive learning~\cite{medical_contrastive,cpc,simclr,khosla_supervised_contrastive_learning} to large-scale image-text datasets.
Both models perform very well on zero-shot transfer tasks, including classification and retrieval.
Follow-up works show that contrastively pre-trained models produce good representations for fine-tuning~\cite{wortsman_robust,dong_clip}, linear regression~\cite{align}, object detection~\cite{owl_vit}, semantic segmentation~\cite{mukhoti2022open} and video tasks~\cite{florence}.

\PAR{Generative language-image pre-training}
Besides softmax contrastive pre-training, various alternatives have been proposed. 
GIT~\cite{git}, SimVLM~\cite{simvlm}, and LEMON~\cite{lemon} successfully pre-train models using a generative text decoder instead, while CoCa~\cite{coca} adds such a decoder to the discriminative CLIP/ALIGN setup, thus combining the pros and cons of both approaches into a single very capable model. 
BLIP~\cite{blip} further proposes CapFilt which uses the generative decoder to create better captions and the discriminative part of the model to filter pairs.
Language-Image pre-training is a very active field and surveys~\cite{chen_vlp_survey} rapidly become outdated.

\PAR{Efficient language-image pre-training}
On the other hand, few works have tried making language image pre-training more efficient.
LiT~\cite{lit} and FLIP~\cite{flip} are notable attempts, the former requires a pre-trained and locked backbone, and the latter sacrifices quality by randomly dropping visual tokens.
BASIC~\cite{basic} and LAION~\cite{laionblog} look at scaling batch-size but only go up to 16\,k and 160\,k respectively, by using many hundreds of chips, and for the former also mixing in a large private classification dataset~\cite{basic,unicl}.
The recent Lion optimizer~\cite{chen2023symbolic} claims to be able to reduce the training cost to reach similar quality.

\begin{algorithm}[t]
\caption{Sigmoid loss pseudo-implementation.}\label{alg:slip}
\newcommand{\hlbox}[1]{%
  \fboxsep=1.2pt\hspace*{-\fboxsep}\colorbox{black!10}{\detokenize{#1}}%
}
\lstset{style=simple}
% \vspace{-3pt}
\begin{lstlisting}[language=Python]
# img_emb      : image model embedding [n, dim]
# txt_emb      : text model embedding [n, dim]
# t_prime, b   : learnable temperature and bias
# n            : mini-batch size

t = exp(t_prime)
zimg = l2_normalize(img_emb)
ztxt = l2_normalize(txt_emb)
logits = -dot(zimg, ztxt.T) * t + b
labels = 2 * eye(n) - ones(n) # -1 with diagonal 1
l = -sum(log_sigmoid(labels * logits)) / n
\end{lstlisting}
% \vspace{-4pt}
\end{algorithm}
% .......................

% ==================================================================
% ==================================================================
\section{Method}\label{sec:method}
% ==================================================================
% ==================================================================

In this section, we first review the widely-used softmax-based contrastive loss.
We then introduce the pairwise sigmoid loss and discuss its efficient implementation.

Given a mini-batch $\mathcal{B}=\{(I_1, T_1), (I_2, T_2), \dots\}$ of image-text pairs, the contrastive learning objective encourages embeddings of matching pairs $(I_i, T_i)$ to align with each other, while pushing embeddings of unmatched pairs $(I_i, T_{j{\neq}i})$ apart.
For practical purposes, it is assumed that for all images $i$, the text associated with a different image $j$ is not related to $i$, and vice-versa.
This assumption is usually noisy and imperfect.

\begin{figure*}[t]
\centering
 \begin{subfigure}[t]{0.24\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figs/chunk_1.pdf}
     \caption{Initially each device holds 4 image and 4 text representations. Each device needs to see the representations from other devices to calculate the full loss.}
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.24\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figs/chunk_2.pdf}
     \caption{They each compute the component of the loss (highlighted) for their representations, which includes the positives.}
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.24\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figs/chunk_3.pdf}
     \caption{Texts are swapped across the devices, so device 1 now has $I_{1:4}$ and $T_{5:8}$ etc. The new loss is computed and accumulated with the previous.}
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.24\textwidth}
     \centering
     \includegraphics[width=\textwidth]{figs/chunk_4.pdf}
     \caption{This repeats till every image \& text pair have interacted, e.g. device 1 has the loss of $I_{1:4}$ and $T_{1:12}$. A final cross-device sum brings everything together.}
 \end{subfigure}
 \hfill
\caption{\textbf{Efficient loss implementation} demonstrated via a mock setup with 3 devices and a global batch size of 12. There are no all-gathers, and at any point in time only the bright yellow square (size $4 \times 4$) is materialized in memory.}
\label{fig:chunky_loss}
\end{figure*}

% ------------------------------------------------------------------
\subsection{Softmax loss for language image pre-training}\label{sec:method:clip}
% ------------------------------------------------------------------

When using the softmax loss to formalize this objective, an image model $f(\cdot)$ and a text model $g(\cdot)$ are trained to minimize the following objective:
\begin{equation*}
-\frac 1 {2|\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|} \left(
\overbrace{\log \frac {e^{t\mathbf{x}_i \cdot \mathbf{y}_i}} {\sum_{j=1}^{|\mathcal{B}|} e^{t\mathbf{x}_i \cdot \mathbf{y}_j}}}^\text{image$\rightarrow$text softmax}
+
\overbrace{\log \frac {e^{t\mathbf{x}_i \cdot \mathbf{y}_i}} {\sum_{j=1}^{|\mathcal{B}|} e^{t\mathbf{x}_j \cdot \mathbf{y}_i}}}^\text{text$\rightarrow$image softmax}
\right)
\end{equation*}
where $\mathbf{x}_i=\frac {f(I_i)} {\|f(I_i)\|_2}$ and $\mathbf{y}_i=\frac {g(T_i)} {\|g(T_i)\|_2}$.
In this paper, we adopt the vision transformer architecture~\cite{vit} for images and the transformer architecture~\cite{transformer} for texts.
Note that due to the asymmetry of the softmax loss, the normalization is independently performed two times: across images and across texts~\cite{clip}. 
The scalar $t$ is parametrized as $\exp(t')$, where $t'$ is a global freely learnable parameter.



% ------------------------------------------------------------------
\subsection{Sigmoid loss for language image pre-training}\label{sec:method:slip}
% ------------------------------------------------------------------

Instead of the softmax-based contrastive loss, we propose a simpler alternative that does not require computing global normalization factors.
The sigmoid-based loss processes every image-text pair independently, effectively turning the learning problem into the standard binary classification on the dataset of all pair combinations, with a positive labels for the matching pairs $(I_i, T_i)$ and negative labels for all other pairs $(I_i, T_{j{\neq}i})$.
It is defined as follows:
$$
-\frac 1 {|\mathcal{B}|} \sum_{i=1}^{|\mathcal{B}|} \sum_{j=1}^{|\mathcal{B}|} \underbrace{\log\frac 1 {1+e^{z_{ij}(-t\mathbf{x}_i \cdot \mathbf{y}_j  + b)}}}_{\mathcal{L}_{ij}}
$$
where $z_{ij}$ is the label for a given image and text input, which equals 1 if they are paired and $-1$ otherwise.
Note that at initialization, the heavy imbalance coming from the many negatives dominates the loss, leading to large initial optimization steps attempting to correct this bias.
To alleviate this, we introduce an additional learnable bias term $b$ similar to the temperature $t$.
We initialize $t'$ and $b$ to 10 and -10 respectively.
This makes sure the training starts roughly close to the prior and does not require massive over-correction.
Algorithm~\ref{alg:slip} presents a pseudocode implementation of the proposed sigmoid loss for language image pre-training.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.98\linewidth]{figs/bs_curves.pdf}
  \caption{The effect of pre-training batch size. \textbf{Left: \slit results}, trained for 18B seen examples.
  Sigmoid loss outperforms the softmax loss significantly with small batch sizes, and performs similarly at larger batch sizes.
  We successfully trained an \slit model with up to \textit{one million} batch size.
  However, performance for both sigmoid and softmax saturate at around 32\,k batch size.
  \textbf{Middle: \slip results}, trained for 9B seen examples. Both sigmoid loss and softmax loss saturate at a reasonable batch size, while the peak of the sigmoid loss comes earlier and slightly outperforms the peak of the softmax loss. A very large batch size hurts both losses.
  \textbf{Right: m\slip results}, trained for 30B seen examples.
  With a multilingual setup using over 100 languages, 32\,k batch size is surprisingly sufficient and scaling beyond that hurts performance on a 36-language cross-modal retrieval task.
  }
  \label{fig:bs_study}
\end{figure*}

% ------------------------------------------------------------------
\subsection{Efficient ``chunked'' implementation}\label{sec:method:impl}
% ------------------------------------------------------------------
Contrastive training typically utilizes data parallelism.
Computing the loss when data is split across $D$ devices necessitates gathering all embeddings~\cite{lit} with expensive all-gathers and, more importantly, the materialization of a memory-intensive $|{\mathcal{B}}| \times |{\mathcal{B}}|$ matrix of pairwise similarities.

The sigmoid loss, however, is particularly amenable to a memory efficient, fast, and numerically stable implementation that ameliorates both these issues.
Denoting the per-device batch size as $b = \frac{|\mathcal{B}|}{D}$, the loss is reformulated as:
$$
-\frac 1 {|\mathcal{B}|} 
\underbrace{\sum_{d_i=1}^D}_{\textbf{A: } \forall \text{ device } d_i}
\overbrace{\sum_{d_j=1}^D}^{\substack{\text{\textbf{B:} swap negs} \\ \text{across devices}}}
\overbrace{
\underbrace{\sum_{i=b d_i}^{b(d_i + 1)}}_{\substack{\text{all local} \\ \text{positives}}}
\underbrace{\sum_{j=bd_j}^{b(d_j + 1)}}_{\substack{\text{negs from} \\ \text{next device}}} \mathcal{L}_{ij}}^{\substack{\text{\textbf{C:} per device} \\ \text{loss}}}
$$
This is particularly simple for the sigmoid loss as each pair is an independent term in the loss. 
Figure~\ref{fig:chunky_loss} illustrates this method.
In words, we first compute the component of the loss corresponding to the positive pairs, and $b - 1$ negative pairs.
We then permute representations across devices, so each device takes negatives from its neighbouring device (next iteration of sum \textbf{B}).
The loss is then calculated with respect to this chunk (sum \textbf{C}).
This is done independently in each device, such that each device computes the loss with respect to its local batch $b$.
Losses can then simply be summed across all devices (sum \textbf{A}).
Individual collective permutes (for sum \textbf{B}) are fast (and indeed $D$ collective permutes is typically faster than two all-gathers between $D$ devices), and the memory cost at any given moment is reduced from $|\mathcal{B}|^2$ to $b^2$ (for sum \textbf{C}).
Usually $b$ is constant as scaling $|\mathcal{B}|$ is achieved by increasing the number of accelerators.
Due to being quadratic with respect to the batch size, the vanilla loss computation rapidly bottlenecks scaling up.
This chunked approach enabled training with batch sizes over 1 million on relatively few devices.



% ==================================================================
% ==================================================================
\section{Results}\label{sec:results}
% ==================================================================
% ==================================================================

In this section, we evaluate the proposed \slit and \slip models across a wide range of batch sizes.
We discuss what can be achieved with a small number of accelerator chips, using both \slit and \slip recipes.
We also briefly discuss the impact of batch size on multilingual language image pre-training. We ablate the importance of our large-batch stabilization modification and the introduced learned bias term and present a study on the effect of positive and negative pairs ratio in the sigmoid loss. Lastly, we explore \slip{}'s data noise robustness.

To validate our models, we report zero-shot transfer results on the ImageNet dataset~\cite{imagenet} and zero-shot retrieval results across 36 languages on the XM3600 dataset~\cite{xm3600}.
We use the ScalingViT-Adafactor optimizer~\cite{vitg} by default for all our experiments.



% ------------------------------------------------------------------
\subsection{\slit: Scaling batch size to the limit}\label{sec:results:slit}
% ------------------------------------------------------------------

Following~\cite{lit}, we use the same precomputed embeddings for the images using a ViT-g vision model, and train a base size text tower from scratch with the same hyperparameters using the LiT image-text dataset~\cite{lit}.

We perform a study over a wide range of batch sizes, from 512 to $1\,M$, demonstrating the impact of batch size for contrastive learning.
Results are presented in Figure~\ref{fig:bs_study} (left).
When the batch size is smaller than $16\,k$, sigmoid loss outperforms softmax loss by a large margin. 
With growing batch sizes, we observe that softmax loss quickly catches up and potentially slightly underperforms sigmoid loss with a large enough batch size.
Overall, we recommend using the \slip recipe for large batch sizes as well, due to the simplicity, compute savings, and straightforward memory efficient implementation.

There is a consensus that contrastive learning benefits from large batch sizes, while most of the existing studies stop at 64\,k batch size~\cite{lit, basic, simclr}.
We successfully trained an \slit model at one million batch size, to explore the limit of contrastive learning.
To our surprise, the performance saturates at 32\,k batch size, further scaling up the batch size only gives a minor boost, and the model peaks at 256\,k batch size.
Our best \slit with a \textit{B}-sized text mode achieves 84.7\% zero-shot transfer accuracy on ImageNet, while the original LiT paper reports a slightly better 85.2\% score with a 10 times larger \textit{g}-sized text model. 
Figure~\ref{fig:slit_duration} presents the impact of training duration for different batch sizes.
It demonstrates that large, $262\,k$ batch size significantly outperforms smaller $8\,k$ batch size when trained for a sufficiently long time. Note, that for short training durations, large batch size leads to the fewer absolute number of update steps and thus needs more time to ramp up.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figs/s_lit_te.pdf}
  \caption{\textbf{\slit ImageNet 0-shot transfer results with different training durations.} 
  Large batch size results in a big performance boost, but needs a sufficiently long schedule to ramp up,
  as for short schedules, very large batch size results in a small number of gradient update steps.
  }
  \label{fig:slit_duration}
  % \vspace{-8px}
\end{figure}

% ------------------------------------------------------------------
\subsection{\slip: Sigmoid loss is beneficial for language-image pre-training}\label{sec:results:slip}
% ------------------------------------------------------------------

We pre-train \slip models on the WebLI dataset~\cite{pali}, using only English image and text pairs.
We use moderately-sized models: B/16 ViT for image embeddings and B-sized transformer for text embeddings. 
The input images are resized to 224$\times$224 resolution. 
The text is tokenized by a 32\,k vocabulary sentencepiece tokenizer~\cite{sentencepiece} trained on the English C4 dataset~\cite{2019t5}, and a maximum of 16 text tokens are kept.
Figure~\ref{fig:bs_study} middle plot shows \slip results,
With less than 32\,k batch size, \slip outperforms CLIP baselines with the standard softmax loss.
On the other end of the scale, the memory efficiency of the sigmoid loss enabled much larger batch sizes. For example, with four TPU-v4 chips, we could fit a batch size of 4096 with a Base \slip but only 2048 with a corresponding CLIP model.
The two advantages together demonstrate significant benefits of the sigmoid loss for language image pre-training with fixed resources, which will be discussed in Section~\ref{sec:results:16chips}.

As batch size increases, the gap between the sigmoid and the softmax losses diminish.
\slip{} performs best at batch size 32\,k, whereas the softmax loss required 98\,k for optimal performance and still didn't outperform the sigmoid based variant.
Scaling further, a larger batch size like 307\,k hurts both losses.


% ------------------------------------------------------------------
\subsection{m\slip: Multi-lingual pre-training}\label{sec:results:multilingual}
% ------------------------------------------------------------------

\begin{table}[t]
  \setlength{\tabcolsep}{0pt}
  \setlength{\extrarowheight}{5pt}
  \renewcommand{\arraystretch}{0.75}
  \newcolumntype{C}{>{\centering\arraybackslash}X}
  \newcolumntype{R}{>{\raggedleft\arraybackslash}X}
  \centering
  %\vspace{-2em}
  \caption{Multilingual \slip results with various batch sizes, pre-trained for 30 billion seen examples. We report zero-shot transfer results on ImageNet (INet-0) and averaged text to image retrieval results across 36 languages on the crossmodal 3600 dataset (XM). The full table on 36 languages can be found in Appendix.}\label{tbl:mSLIP}
  \vspace{0.5mm}
  \begin{tabularx}{\linewidth}{p{1.3cm}p{0.1cm}Cp{0.01cm}Cp{0.01cm}Cp{0.01cm}Cp{0.01cm}C}
    \toprule[1pt]
     && 16\,k && 32\,k && 64\,k && 128\,k && 240\,k \\
    \midrule
    INet-0  && 71.6 && 73.2 && 73.2 && 73.2 && 73.1  \\
    \arrayrulecolor{black!20}\midrule
    XM avg && 34.8 && 34.9 && 34.4 && 33.6 && 32.7 \\
    \arrayrulecolor{black!20}\midrule
    XM de && 54.7 && 54.8 && 55.4 && 54.3 && 54.7 \\
    XM en && 46.5 && 46.2 && 46.5 && 46.6 && 46.6 \\
    XM hi && 9.1 && 8.5 && 7.9 && 8.1 && 7.3 \\ 
    XM ru && 50.1 && 49.9 && 49.7 && 48.6 && 49.3 \\
    XM zh && 30.7 && 32.5 && 32.0 && 30.6 && 23.7 \\
    \arrayrulecolor{black}\bottomrule
  \end{tabularx}
\end{table}

We further scale up the training data by keeping all the \textit{100 languages} from the WebLI dataset~\cite{pali}.
With multilingual data, one usually needs to use a larger international vocabulary.
We first verify the impact of two tokenizers: a small multilingual vocabulary with 32\,k tokens~\cite{2019t5}, and a large multilingual vocabulary with 250\,k tokens~\cite{mt5}. 
We train B-sized ViT and text models for $900\,M$ total examples seen, and observe slightly more than 1\% improvement when using a larger vocabulary.

However, the token embeddings become huge for very large vocabulary sizes.
Following the standard setup, we would need to store a $N\times W$ token embedding lookup table to train the multilingual model,
where $N$ is the vocabulary size mentioned above and $W$ is the embedding dimension of the text model.
To save memory, we propose to use a ``bottlenecked'' token embedding.
We use $N\times K$ embedding matrix and additional $K\times W$ projection, where the bottleneck $K$ is much smaller than $W$.

In our experiments, we observed that using a large multilingual vocabulary with a bottleneck can be scaled up as efficiently as using a small multilingual vocabulary.
Specifically, by enabling the bottleneck of size $K=96$ for Base architecture with $W=768$, we only see about a half percent quality drop on ImageNet zero-shot transfer, compared to using the full $250k$ vocabulary.

With the memory improvements, we train m\slip models for various batch sizes, for a total of 30 billion examples seen.
Table~\ref{tbl:mSLIP} and Figure~\ref{fig:bs_study} (right plot) show the results.
A batch size of 32\,k is sufficient for a multilingual setup as well. 
On the XM3600 cross-modal retrieval tasks, we found that going beyond 32\,k batch size leads to worse results on average while on ImageNet zero-shot transfer it stays flat.
m\slip sets the new state-of-the-art on XM3600 text to image retrieval task, with only a Base size model.
Our best result is 34.9\%, which is more than 6\% higher than the previously reported result 28.5\%~\cite{pali} with a standard LiT model~\cite{lit} using a much larger four billion ViT-e model.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{figs/finetune_slip.pdf}
  \caption{
  \textbf{Top}: \slip with pre-trained encoders ramps up quickly. However, only disabling weight decay on the pre-trained encoder weights leads to stable behavior and good ImageNet 0-shot transfer results.
  \textbf{Bottom}: ImageNet 10-shot transfer results, where decaying the pre-trained weights leads to deterioration of the pre-trained model visual representation quality.
  Disabling weight decay makes the curve flatter.
  }
  \label{fig:unlocked_finetuning}
\end{figure}

% ------------------------------------------------------------------
\subsection{\slit with four TPU-v4 chips}\label{sec:results:4chips}
% ------------------------------------------------------------------

For many practitioners, the important question usually is ``what can be trained with a limited amount of resources?".
We explore the usage of \slit models in this section with only four TPU-v4 chips, as the memory efficient sigmoid loss is suitable for this application scenario.

We follow the same setup as in section~\ref{sec:results:slit}.
We use the publicly available ViT-Augreg-B/8~\cite{augreg} model as the frozen (\frozen) vision tower, and precompute embeddings to accelerate the training~\cite{lit}. 
The text model is Large, but restricted to 12 layers. It is trained using the LION~\cite{chen2023symbolic} optimizer with decoupled weight decay $1\times{10}^{-7}$, linearly warmuping learning rate for 6.5k up to a peak of $1\times{10}^{-4}$, which cosine decays to 0.  We train in total using batch size 32k for 65000 steps -- just under one day of training. Table~\ref{tbl:teaser} shows the results when training a model on four chips for one day, achieving 79.7\% 0-shot ImageNet classification accuracy; very competitive in this limited resource regime.
With a ViT-g/14~\cite{vitg} model as the vision tower and a Large text tower, we can train at 20\,k batch size on four chips for 107\,k steps in under two days.
This further pushes the 0-shot ImageNet classification accuracy up to 84.5\%.

% ------------------------------------------------------------------
\subsection{\slip with a small amount of TPU-v4 chips}\label{sec:results:16chips}
% ------------------------------------------------------------------



\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/beta2.pdf}
  \caption{\textbf{The effect of Adam and AdaFactor's $\beta_2$.} As we increase batch-size, we observe more frequent training instability. This instability can mainly be seen in the loss curves (top) and is caused by spikes in the gradient norm (middle) which results in large parameter updates (bottom). Decreasing the $\beta_2$ momentum value stabilizes the training. Even though occasional gradient spikes still happen (see step at 2B), they do not destabilize the training process.}
  \label{fig:beta2}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/batch_composition.pdf}
  \caption{\textbf{The effect of batch composition.}
  We simulate various batch compositions by masking out negatives, either randomly, keeping only the hardest, or the easiest.
  With no masking, we have 16\,k negatives for each positive in the batch (1:16\,k) and the strongest masking we apply (1:1.6) results in almost balanced minibatches.
  In one setting we \emph{match total pairs} seen by training for significantly longer.
  We observe ImageNet 0-shot score, the final value of the learned bias, and the average logits of positive and negative pairs.
  Overall, the imbalance does not seem to be detrimental, but finding an \emph{efficient} way of mining negatives might be beneficial.
  }
  \label{fig:composition}
  % \vspace{-5px}
\end{figure*}

It's resource demanding to train a CLIP model from-scratch in general, with \slip it's possible to fit a larger train batch size with fewer amount of chips.
In this section, we explore ways to train \slip models efficiently with pre-trained weights.
We use pre-trained weights to initialize the image model to accelerate the pre-training, which was originally discussed in~\cite{lit}. 
We use the public and unlocked \unlocked ViT-Augreg-B/16~\cite{augreg} model to initialize our vision tower and fine-tune on the same WebLI English data as used for \slip.
In all the experiments, we apply a 0.1 learning rate multiplier to the pre-trained image tower to make it suitable for fine-tuning.

Figure~\ref{fig:unlocked_finetuning} presents unlocked \unlocked fine-tuning results alongside from-scratch randomly initialized baselines.
We used 16 TPU-v4 chips and train at 16\,k batch size for 2.4\,B examples seen.
We found that the fine-tuning setup doesn't perform well out-of-the-box; this is consistent with prior works~\cite{lit} where finetuning image models degraded visual representation quality. 
This is evidenced by ImageNet 10-shot linear classification, where in Figure~\ref{fig:unlocked_finetuning} the fine-tuned setup is barely better than the from-scratch baseline.

We hypothesize that the default weight decay applied to the pre-trained weights reduces their effectiveness.
Motivated by the fine-tuning recipe from ~\cite{vit,vitg,bit}, that uses no weight decay, we also propose disabling weight decay on the pre-trained weights for \slip training.
Weight decay is therefore only applied to the randomly initialized weights in the text model.
This simple modification significantly improved \slip results.
Figure~\ref{fig:unlocked_finetuning} shows that with our improved recipe, \slip reaches 71\% 0-shot accuracy on ImageNet, using $16k$ batch size, trained on 16 chips for three days.
We also present from-scratch results in the bottom rows of Table~\ref{tbl:teaser}: with 32 TPUv4 chips for only two days, \slip{} achieves 72.1\% 0-shot accuracy.
This presents a significant training cost reduction e.g. compared to CLIP (approx. 2500 TPUv3-days for 72.6\%) reported in~\cite{flip}.

% ------------------------------------------------------------------
\subsection{Stabilizing large-batch training}\label{sec:results:tricks}
% ------------------------------------------------------------------

As we move to large batch sizes, the language image pre-training using transformers becomes increasingly more unstable, even when using a modestly-sized model (e.g. Base size).
The reason for these instabilities is large spikes in the gradient norms, which translate to large-magnitude changes in the weights that may destabilize the training process, see Figure~\ref{fig:beta2}.
We observe that reducing $\beta_2$ in Adam and AdaFactor from its default 0.999 to 0.95 (which was suggested in~\cite{mae, igpt}) is enough to stabilize the training.
\footnote{Redacted for anonymity.}
Intuitively, this allows recovering from gradient spikes quicker.
We opt for setting $\beta_2=0.95$ by default for all our experiments.


\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/noise_xp.pdf}
  \caption{\textbf{Sigmoid-training increases robustness} to data noise. Titles show the type of corruption applied, and x-axes show the probability with which they are applied. With increasing corruption severity, M-scale models trained with sigmoid loss for 3.6 billion examples retain superiority over corresponding softmax baseline.}
  \label{fig:noise}
\end{figure*}

% ------------------------------------------------------------------
\subsection{Negative ratio in sigmoid loss}\label{sec:results:negative_ratio}
% ------------------------------------------------------------------


One question which arises when shifting the perspective from the softmax's ``pick the right class'' view to the sigmoid's ``rate this pair'' view, is the imbalance in positive versus negative pairs.
For a batch size $|\mathcal{B}|$, the batch contains $|\mathcal{B}|$ positive pairs, but $|\mathcal{B}|^2-|\mathcal{B}|$ negative examples.
In the modest batch-size of 16\,k, there are actually 268\,M negative examples for only 16\,k positive ones.
At the same time, because the sigmoid loss decomposes into a sum of per-example losses, we can perform controlled experiments to study the effect of the mini-batch composition and distribution of examples visited.
We run experiments in the \slit setup at batch-size 16\,k for 900\,M steps and vary the composition of the batch by masking out (\ie ignoring) enough negative examples to reach a target ``positive : negative'' ratio, masking in the following ways:
\begin{itemize}[itemsep=2pt,topsep=2pt,parsep=2pt,partopsep=2pt]
\item \textbf{Random:} Randomly choose negative pairs to mask.
\item \textbf{Hard:} Keep hardest negative pairs (highest loss).
\item \textbf{Easy:} Keep easiest negatives pairs (lowest loss).
\item \textbf{Hard + matching total pairs seen:} Masking examples while training for a fixed number of steps does decrease the total number of \emph{pairs} seen during training. Hence in the \emph{matched pairs} setting, we increase the number of training steps by the masking ratio in order to keep the number of pairs seen constant.
\end{itemize}


Figure~\ref{fig:composition} shows the effect of the various masking strategies.
Randomly removing negatives to rebalance does deteriorate performance.
Keeping the easiest examples does not work at all, while keeping the hardest negatives does almost maintain the quality, indicating that, as could be expected, a lot of the learning on the negative side comes from the harder examples.
This is further confirmed by the slightly increased performance of training longer on the hardest examples in order to match the total pairs seen.

We also look at the value of the learned bias at the end of training as well as the average logit value for positive and negative examples across these settings, and find the result mostly follows what one would expect:
as fewer negatives are present, the bias and logits become more positive overall.
Interestingly, when training with more hard negative pairs, the average logits of positive pairs stays mostly flat.

This study confirms that (1) the imbalance does not seem to be a major reason for concern, while at the same time (2) coming up with an \emph{efficient} way of including more negative examples can be promising but is not trivial.


\begin{table}[b]
  \setlength{\tabcolsep}{0pt}
  \setlength{\extrarowheight}{5pt}
  \renewcommand{\arraystretch}{0.75}
  \newcolumntype{C}{>{\centering\arraybackslash}X}
  \newcolumntype{R}{>{\raggedleft\arraybackslash}X}
  \centering
  %\vspace{-2em}
  \caption{\textbf{Bias (b) and temperature (t$'$) initialization.} Results are reported using Base architecture, 8\,k batch size, trained for 900M examples. 
  Enabling the bias term b with $-10$ initialization improves results consistently.
  }\label{tbl:bias}
  \vspace{0.5mm}
  \begin{tabularx}{\linewidth}{p{0.8cm}p{0.0cm}Cp{0.1cm}Cp{0.1cm}Cp{0.1cm}Cp{0.1cm}C}
    \toprule[1pt]
    b && t$'$ && INet-0 && Pet-0 && C100-0 \\
    \arrayrulecolor{black!20}\midrule
    n/a && 10 && 62.0 && 81.8 && 59.9 \\
    -10 && 10 && \textbf{63.0} && 
    \textbf{82.4} && \textbf{61.0} \\
    -10 && 1 && 61.0 && 80.0 && 60.4 \\
    0 && 10 && 61.7 && 79.9 && 59.0 \\
    0 && 1 && 53.7 && 73.2 && 53.8 \\
    \arrayrulecolor{black}\bottomrule
  \end{tabularx}
  \vspace{-5pt}
\end{table}

% ------------------------------------------------------------------
\subsection{Bias term in sigmoid loss}\label{sec:results:bias}
% ------------------------------------------------------------------

We ablate the bias term in the loss function, using the Base architecture with an 8\,k batch size, trained for 900M examples with the \slip setup.
Zero-shot transfer results are reported on ImageNet~\cite{imagenet}, Oxford-iiit pet~\cite{parkhi12a} and Cifar100~\cite{cifar}. 
Table~\ref{tbl:bias} presents results with and without a bias term in the sigmoid loss.

Enabling the bias term with a $-10$ initialization consistently improves performance across all tasks.
This is because the bias term ensures that the training starts close to the prior, preventing dramatic over-correction in early optimization.
In contrast, a randomly chosen bias term initialization, such as the 0 initialization in Table~\ref{tbl:bias}, fails to address the over-correction issue, leading to significantly worse results.
This effect is particularly noticeable when using a small temperature t$'$ initialization.
We set the bias and temperature initialization to $-10$ and 10, respectively, as the default for all experiments.





% ------------------------------------------------------------------
\subsection{Label noise robustness}\label{sec:results:robust}
% ------------------------------------------------------------------
Prior works demonstrated improved robustness against label noise when using the sigmoid loss for classification models~\cite{imagenet_real}. This property would be particularly useful here in the face of the famously noisy nature of popular large-scale image-text datasets.
In order to study this for \slip{}, we train M/16 image models alongside an M text model at batch size 16384 for 3.6 billion seen examples. We corrupt the training data using one of the following methods:

\begin{itemize}[itemsep=2pt,topsep=2pt,parsep=2pt,partopsep=2pt]
    \item \textbf{Image}: With probability $p$, replace the image with uniform random noise.
    \item \textbf{Text}: With probability $p$, replace tokenized text with a new sequence of randomly sampled tokens, up to some (sampled) sequence length.
    \item \textbf{Batch alignment}: Randomly shuffle the ordering of $p$\% of the batch.
    \item \textbf{Image \& text}: Independently apply (1) and (2), each with probability $p$.
    \item \textbf{Image, text \& batch}: Alongside (4), also shuffle fraction $p$ of alignments.
\end{itemize}
Results from varying the likelihood of the corruption are shown in Figure~\ref{fig:noise}.
Models trained with sigmoid loss are increasingly robust to all kinds of added noise.


% ==================================================================
% ==================================================================
\section{Conclusion}\label{sec:conclusion}
% ==================================================================
% ==================================================================
We conducted a study on two language-image pre-training instances that used the sigmoid loss: \slit and \slip.
Our results demonstrate that the sigmoid loss performs better than the softmax baseline, particularly for small train batch sizes.
This loss function is also more memory efficient, which allows larger train batch sizes without requiring additional resources.
We performed a thorough investigation of the batch size in contrastive learning. Surprisingly, we found that a relatively modest batch size of 32\,k yielded nearly optimal performance.
Further studies have been performed to understand better the introduced bias term in the sigmoid loss, robustness to data noises and the impact of positive and negative pairs ratio in the sigmoid loss.
We hope this work will facilitate language-image pre-training research with limited resources.

\paragraph{Acknowledgements.} We thank Daniel Keysers, Ilya Tolstikhin, Olivier Bousquet and Michael Tschannen for their valuable feedback and discussions on
this paper; the Google Brain team at large for providing a supportive research environment.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix

\section{More results for \slit}\label{app:slit}
In section~\ref{sec:results:slit}, we use the same precomputed embeddings for the images using a ViT-g vision model from~\cite{lit}.
Only resize augmentation is applied, to a fixed $288\times288$ resolution.
We train a standard base size text tower, using the ScalingViT-Adafactor optimizer~\cite{vitg} with $\beta_1 = 0.9$ and $\beta_2 = 0.95$.
We use 0.001 learning rate with a linear warmup schedule for the first 200\,M examples seen, and then the learning rate is decayed to zero with a cosine learning rate schedule.
Weight decay is set to 0.0001 for all the experiments.
The text is tokenized by a 32 k vocabulary sentencepiece tokenizer~\cite{sentencepiece} trained on the English C4 dataset~\cite{2019t5}, and a maximum of 16 text tokens are kept.
Table~\ref{app:table:slit_results} shows results with multiple train examples seen and batch sizes, for both the sigmoid loss and the softmax loss baseline.

For training~\slit{} in under a day with 4 chips (Section~\ref{sec:results:4chips}), we used the LION optimizer with peak learning rate $1\times{10}^{-4}$ and weight decay  $1\times{10}^{-7}$. The learning rate was warmed linearly to the peak in 6.5\,k steps, then cosine decayed to zero for the remaining 58.5\,k steps.




\section{More results for \slip}\label{app:slip}

In Table~\ref{app:table:slip_results}, we present more results for \slip with multiple train examples seen: 3 billion examples and 9 billion examples respectively.


\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Batch Size\ \ } & \multicolumn{2}{c}{3 B} & \multicolumn{2}{c}{9 B} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} 
 & sigmoid & softmax & sigmoid & softmax \\
\midrule
512    &      \textbf{51.5} &    47.7 &     - &     - \\
1 k   &     \textbf{57.3} &    53.2 &     - &     - \\
2 k   &   \textbf{62.1} &    59.3 &     - &     - \\
4 k   &    \textbf{65.3} &    63.8 &    \textbf{68.4} &    66.6 \\
8 k   &     \textbf{68.6} &    66.6 &    \textbf{70.6} &    69.4 \\
32 k  &   \underline{\textbf{69.9}} &    \underline{\textbf{69.9}} &    \underline{\textbf{73.4}} &    72.9 \\
98 k  &     69.5 &    \textbf{69.7} &    73.0 &    \textbf{73.2} \\
307 k &     - &     - &    71.6 &    \textbf{72.6} \\
\bottomrule
\end{tabular}
\caption{\textbf{\slip zeor-shot accuracy (\%) on the ImageNet benchmark.} Both the sigmoid loss and the softmax loss baseline are presented.
Experiments are performed on multiple train examples seen (3\,B, 9\,B) and train batch sizes (from 512 to 307\,k).
When trained for 9\,B examples, the peak of the sigmoid loss comes earlier at 32\,k than the peak of the softmax loss at 98\,k. 
Together with the memory efficient advantage for the sigmoid loss, it allows one to train the best language-image model with much fewer amount of accelerators.
}
\label{app:table:slip_results}
\end{table}

\section{More results for m\slip}\label{app:mslip}

We present the crossmodal retrieval results on the Crossmodal-3600 dataset, across all the 36 langauges in Figure~\ref{app:fig:xm_i2t}, Figure~\ref{app:fig:xm_t2i} and Table~\ref{app:table:crossmodal_retrieval}.




\begin{table*}[ht]
\centering
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{Batch Size\ \ } & \multicolumn{2}{c}{450 M} & \multicolumn{2}{c}{900 M} & \multicolumn{2}{c}{3 B} & \multicolumn{2}{c}{18 B} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
 & sigmoid & softmax & sigmoid & softmax & sigmoid & softmax & sigmoid & softmax \\
\midrule
512     &    72.5 &    69.5 &    75.0 &    72.8 &    77.2 &    74.6 &     - &     - \\
1 k    &    75.5 &    73.6 &    77.2 &    76.0 &    79.6 &    77.9 &    - &     - \\
2 k    &    77.1 &    76.3 &    79.3 &    78.1 &    81.3 &    80.1 &    82.2 &    81.2 \\
4 k    &    79.2 &    78.3 &    80.8 &    79.8 &    82.4 &    81.2 &    83.0 &    82.0 \\
8 k    &    80.8 &    79.7 &    82.0 &    81.0 &    83.1 &    82.6 &    83.6 &    83.1 \\
16 k   &    81.2 &    81.2 &    82.7 &    82.1 &    83.8 &    83.5 &    84.2 &    84.1 \\
32 k   &    81.9 &    81.4 &    83.1 &    82.7 &    84.2 &    84.0 &    84.6 &    84.4 \\
64 k   &    81.6 &    81.6 &    83.0 &    82.8 &    84.3 &    84.1 &    84.7 &    84.4 \\
128 k  &    80.5 &    80.0 &    83.1 &    83.2 &    84.2 &    84.4 &    84.7 &    84.6 \\
256 k  &    72.8 &    72.2 &    82.1 &    81.7 &    84.3 &    84.2 &    84.7 &    84.6 \\
1024 k &     - &     - &     - &     - &     - &     - &    84.7 &     - \\
\bottomrule
\end{tabular}
\caption{\textbf{\slit zero-shot accuracy (\%) on the ImageNet benchmark.} Both the sigmoid loss and the softmax loss baseline are presented.
Extensive experiments are performed on multiple train examples seen (450\,M, 900\,M, 3\,B, 18\,B) and train batch sizes (from 512 to 1\,M).}
\label{app:table:slit_results}
\end{table*}


\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/crossmodal_img2txt.pdf}
  \caption{\textbf{Zero-shot image to text retrieval results on the Crossmodal-3600 benchmark.} Multiple batch sizes are reported.
  }
  \label{app:fig:xm_i2t}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figs/crossmodal_txt2img.pdf}
  \caption{\textbf{Zero-shot text to image retrieval results on the Crossmodal-3600 benchmark.} Multiple batch sizes are reported.
  }
  \label{app:fig:xm_t2i}
\end{figure*}


\label{sec:more_zero_shot}
\begin{table*}[ht]
\centering
% \resizebox{\linewidth}{!}{%
% \small
\begin{tabular}{lrrrrrrrrrrr}
\toprule
\multirow{2}{*}{Lang.\ \ } & \multicolumn{5}{c}{Image-to-text} & &  \multicolumn{5}{c}{Text-to-image} \\
\cmidrule{2-6} \cmidrule{8-12}
 & 16 k & 32 k & 64 k & 128 k & 240 k & & 16 k & 32 k & 64 k & 128 k & 240 k \\
\midrule
ar & 52.39 & 51.33 & 51.50 & 51.53 & 51.06 &  & 37.61 & 37.37 & 37.14 & 36.31 & 35.98 \\
bn & 11.39 & 10.78 & 10.44 & 10.31 & 9.89 &  & 5.53 & 6.25 & 4.94 & 5.14 & 4.36 \\
cs & 54.08 & 53.69 & 53.67 & 52.75 & 51.78 &  & 41.82 & 41.64 & 41.46 & 39.93 & 39.38 \\
da & 62.72 & 62.42 & 62.00 & 60.42 & 59.33 &  & 47.01 & 47.01 & 45.55 & 43.03 & 43.47 \\
de & 70.33 & 71.42 & 71.19 & 71.11 & 70.25 &  & 54.70 & 54.83 & 55.36 & 54.31 & 54.71 \\
el & 36.94 & 35.81 & 35.06 & 34.53 & 33.81 &  & 22.42 & 22.78 & 22.00 & 21.25 & 20.79 \\
en & 50.11 & 50.53 & 50.22 & 49.94 & 50.67 &  & 46.46 & 46.21 & 46.55 & 46.60 & 46.60 \\
es & 64.69 & 64.94 & 67.19 & 65.31 & 65.61 &  & 54.81 & 55.04 & 55.51 & 54.47 & 55.24 \\
fa & 57.03 & 57.75 & 56.06 & 55.28 & 54.64 &  & 39.61 & 40.15 & 38.43 & 38.36 & 38.30 \\
fi & 54.94 & 54.08 & 53.78 & 51.69 & 51.67 &  & 37.70 & 37.14 & 36.38 & 33.98 & 34.50 \\
fil & 23.22 & 22.75 & 22.92 & 21.39 & 21.22 &  & 12.83 & 12.93 & 12.41 & 12.19 & 11.34 \\
fr & 65.69 & 66.92 & 66.97 & 66.14 & 66.47 &  & 55.92 & 57.08 & 55.50 & 54.39 & 54.29 \\
hi & 19.86 & 18.81 & 19.89 & 19.53 & 17.36 &  & 9.09 & 8.55 & 7.86 & 8.06 & 7.28 \\
hr & 52.67 & 53.03 & 52.97 & 49.92 & 49.58 &  & 38.16 & 37.09 & 36.37 & 35.25 & 34.33 \\
hu & 56.97 & 57.11 & 56.33 & 54.83 & 53.03 &  & 41.37 & 40.20 & 40.22 & 38.55 & 38.25 \\
id & 64.83 & 67.06 & 66.56 & 65.39 & 64.72 &  & 48.53 & 49.42 & 49.49 & 47.82 & 47.29 \\
it & 65.86 & 66.42 & 67.11 & 65.25 & 66.08 &  & 55.52 & 56.39 & 55.85 & 54.75 & 54.11 \\
iw & 48.36 & 47.86 & 47.72 & 46.06 & 45.25 &  & 31.78 & 31.76 & 31.89 & 30.08 & 30.12 \\
ja & 46.42 & 45.94 & 42.89 & 43.72 & 30.17 &  & 31.04 & 31.32 & 29.21 & 28.87 & 18.50 \\
ko & 50.78 & 49.53 & 49.44 & 50.22 & 46.78 &  & 34.44 & 34.72 & 33.15 & 33.06 & 31.54 \\
mi & 0.36 & 0.42 & 0.58 & 0.56 & 0.42 &  & 0.16 & 0.22 & 0.19 & 0.19 & 0.19 \\
nl & 59.56 & 60.36 & 58.94 & 58.31 & 57.86 &  & 48.95 & 49.55 & 48.95 & 48.37 & 47.88 \\
no & 61.36 & 62.39 & 61.97 & 60.89 & 59.86 &  & 45.25 & 46.21 & 45.04 & 43.53 & 43.71 \\
pl & 62.19 & 62.03 & 61.97 & 61.11 & 60.50 &  & 48.80 & 47.36 & 48.70 & 46.79 & 46.72 \\
pt & 63.14 & 63.61 & 64.89 & 64.31 & 63.25 &  & 52.41 & 52.34 & 52.30 & 51.93 & 52.37 \\
quz & 6.78 & 6.42 & 6.36 & 6.64 & 6.67 &  & 2.74 & 2.57 & 2.67 & 2.69 & 2.79 \\
ro & 52.06 & 51.44 & 50.97 & 50.58 & 49.31 &  & 37.20 & 35.60 & 34.34 & 34.52 & 32.50 \\
ru & 62.22 & 63.64 & 63.08 & 62.69 & 63.08 &  & 50.11 & 49.89 & 49.71 & 48.61 & 49.31 \\
sv & 62.33 & 63.53 & 63.53 & 63.06 & 61.19 &  & 47.89 & 48.18 & 47.64 & 46.17 & 46.16 \\
sw & 14.83 & 14.42 & 14.31 & 14.17 & 13.78 &  & 7.81 & 7.17 & 7.11 & 6.94 & 6.34 \\
te & 1.25 & 1.25 & 1.19 & 1.69 & 1.06 &  & 0.40 & 0.29 & 0.32 & 0.47 & 0.32 \\
th & 36.11 & 35.78 & 35.64 & 35.56 & 28.33 &  & 21.58 & 23.08 & 22.22 & 21.62 & 16.76 \\
tr & 53.08 & 54.50 & 53.72 & 52.94 & 51.25 &  & 37.33 & 37.38 & 37.81 & 36.97 & 36.08 \\
uk & 51.42 & 51.50 & 51.17 & 49.86 & 49.22 &  & 34.54 & 33.21 & 33.79 & 32.49 & 32.39 \\
vi & 59.58 & 59.78 & 59.53 & 58.53 & 58.83 &  & 41.43 & 41.92 & 41.85 & 40.63 & 40.26 \\
zh & 44.11 & 45.67 & 44.11 & 41.92 & 36.08 &  & 30.74 & 32.45 & 32.05 & 30.61 & 23.72 \\
\midrule
avg & 47.21 & 47.36 & 47.11 & 46.34 & 45.00 & & 34.82 & 34.87 & 34.44 & 33.58 & 32.72 \\
\bottomrule
\end{tabular}
%}
\caption{\textbf{Image-to-text and text-to-image zero-shot retrieval results on all 36 languages of Crossmodal-3600}, with m\slip models trained at different batch sizes for 30\,B examples seen.}
\label{app:table:crossmodal_retrieval}
\end{table*}


\section{Label noise experiments}\label{app:label noise}
All models had an M/16 image tower and a M text tower. They were trained from random initialisation for 3.6B examples seen, with a batch size of 16384. 
\end{document}