\section{Experimental Setup}

\subsection{Classification Metrics}
As described in Section \ref{sec:extraction_task}, the MTC extraction task is an information extraction text-to-structure task. We choose this task structure to ensure generalizability since there are many possible MTCs according to the proposed CFG and many possible sources of MTCs. For simplicity, however, we evaluate the MTC extraction task as a multiclass multilabel classification task, with each unique extracted MTC treated as a label for a given DUG statement. We consider the union of the MTCs present in the FDA, Medscape, and EHR datasets as the label space, and include an "undefined" label for predicted MTCs that either do not conform to the CFG or do not match any MTCs in the label space. Doing so allows us to use standard multilabel classification evaluation metrics to measure model performance in the MTC extraction task. Unless stated otherwise, we henceforth report the macro average of precision, recall, and F1 \cite{scikitlearn}. 
% Correct MTCs may exist inside the CFG but outside the label space. For example, the MTC "not before operating heavy machinery" is a valid MTC, however none of the DUGs across our three datasets contain this MTC. As we use classification metrics to measure model performance only across the MTCs in our label space, we specify that these results do not extend to valid out-of-sample MTCs, only to MTCs seen in the the Medscape, FDA, and EHR datasets. 
% \textcolor{blue}{this sentence is not clear. Perhaps show an example? "Since correct MTCs may exist inside the CFG but outside the label space, we specify that these results do not extend to valid out-of-sample MTCs, only to MTCs seen in the the Medscape, FDA, and EHR datasets. "}

\subsection{Validity}
In addition to standard classification evaluation metrics, we experiment using a simple heuristic for determining whether an extracted MTC is valid. We define a \textbf{valid} MTC as one which conforms to our proposed CFG, making it able to be represented computationally and thus more useful in downstream tasks. While all invalid extracted MTCs will be incorrect classifications, some incorrectly extracted MTCs will still be valid, indicating that the LLM is learning to format output according to the CFG. Hence, we report the percentage of extracted MTCs that are valid.

\subsection{Label Specifics}
In the \textit{specialized} model, since not every DUG contains an MTC of each type, some will have an empty label. We simply insert the label "NONE" for these guidelines to allow the LLM to give a non-empty response. Since there is a large prevalence of empty labels, when investigating the \textit{specialized} model specifically, we report both the positive class metrics (i.e. macro average metrics across guidelines when excluding guidelines with empty labels) and the macro average metrics across all DUGs.

%\subsection{MTC Type 5 Labels}
We note that MTC Type 5 only occurs once across the three datasets, as seen in Fig. \ref{fig:mtc_types_dist}, and is consequently omitted from experimental results. Although MTC type 5 is discarded from further evaluation, our proposed solutions can be extended to MTC type 5 when there is relevant data.

 %Valid yet incorrect MTCs are considered model "hallucinations."  Hallucination occurs when a text generation model produces output that is unfaithful to the provided input, and is a common problem when working with LLMs as we do in ICL \cite{ji2022survey}. We explore the frequency of hallucination errors in MTC extraction in Section \ref{section:error_analysis}.

%\subsection{Experiments}


\section{Experimental Results}
\label{sec:results}

We thoroughly examine the scope of ICL for MTC extraction. First, we compare the \textit{simple}, \textit{guided}, and \textit{specialized} ICL prompting strategies for the MTC extraction task. Next we compare ICL performance against a rule-based MTC type classification model. We then further evaluate the \textit{specialized} ICL model responses, first by dataset and then by MTC type. Finally, we explore the effectiveness of the ICL model responses to extract valid structures from text in the MTC extraction task.



\subsection{Prompting Strategies Comparison}
Macro average results on the MTC extraction task for the \textit{simple} and \textit{guided} prompts, along with the \textit{specialized} model, are displayed in Table \ref{table:in_context_results}. While the \textit{simple} and \textit{guided} prompts produce poor results overall, the \textit{specialized} model is able to competently extract MTCs with an F1 score of $0.59$. We hypothesize that extracting MTCs of each type separately, as in the \textit{specialized} model, allows the LLM to contextualize each MTC type more quickly with fewer examples.

\subfile{../tables/in_context_results}

\subsection{Rule-based Baseline Comparison}
To demonstrate the generalizability of the \textit{specialized} ICL model, we develop a simple rule-based MTC type classification model using common phrases in the Medscape dataset as guidance. As extracting specific MTCs using a simple set of search rules would be difficult, we instead attempt only to identify which MTC types (1-7) occur in a given DUG. We use only the Medscape dataset when developing the rule base, as it has the greatest variety of MTCs, then use the same rule base to identify MTC types across all three datasets. We see in Table \ref{table:rulebase_comparison} that even when only attempting to identify MTC types in a DUG, a much simpler task than MTC extraction, a rule base developed for the Medscape dataset fails to generalize to either the FDA dataset or the EHR dataset. In comparison, the \textit{specialized} model generalizes across all 3 datasets in the MTC extraction task. This demonstrates the generalizability of using ICL for the MTC extraction task.

\subfile{../tables/rulebase_comparison_results}
\subsection{Specialized ICL Results By Dataset}
In Table \ref{table:icl_by_dataset} we see the results of the \textit{specialized} model across each of the three datasets\footnote{\label{refnote}As explained in Section \ref{sec:data_char}, only one imprecise time-of-day dependency MTC (type 5) occurs across the EHR, FDA, and Medscape datasets. Hence, we do not attempt to extract this MTC type.}. The \textit{specialized} model performs the best across the EHR dataset, with a macro average F1 score of $0.65$. The EHR dataset presents possibly the easiest of the three MTC extraction tasks because all labeled MTCs are mapped one-to-one with medical abbreviations, and there are only two MTC types present in the dataset. 

\subfile{../tables/icl_by_dataset}

\subsection{Specialized ICL Results By MTC Type}
In Table \ref{table:in_context_by_mtc} we see the results of the \textit{specialized} model by MTC type\footref{refnote}. While the model is able to accurately extract MTCs of most types, it performs best on interval constraints (MTC type 3) with a positive class macro average F1 score of $0.72$. The most difficult MTC type for the model to extract is the consistency MTC (type 6), with a positive class macro average F1 score of $0.33$. We see that the \textit{specialized} model frequently hallucinates consistency MTCs and discusses other potential sources of error in Section \ref{section:error_analysis}.

\subfile{../tables/in_context_by_mtc}

\subsection{Validity}
Finally, we explore the ability of the ICL models to produce parsable outputs. We see in Table \ref{table:validity} that the \textit{specialized} model is far more competent at producing parsable output which conforms to the CFG with minimal post-processing, with a $0.99$ proportion of valid outputs compared to $0.29$ and $0.37$ in the \textit{simple} and \textit{guided} models, respectively.

\subfile{../tables/validity}

\section{Error Analysis}
\label{section:error_analysis}

To investigate model strengths and weaknesses, we sample 60 errors made by the \textit{specialized} model, 10 of each MTC type. A single human annotator then categorizes each model error, providing one possible reason for each failed MTC extraction. The three most frequent error categories in this sample are hallucinations, semantic overlap, and nonvalidity. The sample distribution of these error categorizations across MTC types is provided in Table \ref{table:error_analysis}. Examples of each of these common error types are given in Table \ref{error_examples}. We now describe each of the three common error types. 

\subfile{../tables/error_analysis}

\subfile{../tables/error_examples}

The most common error type is \textbf{hallucination}, an error common in LLMs such as GPT-3 \cite{ji2022survey}. This occurs when the model outputs an MTC or a list of MTCs that are valid, but not found in the text sample. 43\% of all labeled model errors are due to hallucinations. A common cause of hallucination occurs in activity selection. Definitive and imprecise dependency constraints (types 1 and 4, respectively) occur when the medication intake activity is temporally dependent on another patient's activity. Human annotators were instructed to normalize activities when labeling MTCs. For example, phrases like "before bedtime" and "before sleeping" were both normalized to "before sleep." The \textit{specialized} model occasionally either hallucinates an activity, fails to normalize an activity or both. Hallucinations were especially common in consistency (type 6) and time-of-day (type 7) MTCs, accounting for 70\% of these errors in the categorized sample. An example of a consistency (type 6) MTC hallucination is given in the second row of Table \ref{error_examples}. Hallucinations seem to be a primary reason for poor model performance when extracting consistency MTCs (type 6) specifically, as the positive class macro average F1 was quite low ($0.33$) but overall performance was much higher ($0.63$ macro average F1). Reducing hallucinations in LLMs is an active area of research that could lead to better results on the MTC extraction task \cite{sun2022contrastive}.

Another common error is \textbf{nonvalidity}, in which LLM model output is unable to be parsed according to the CFG, after minimal post-processing. Take the first DUG in Table \ref{error_examples}, from the Medscape dataset. While the \textit{specialized} model output "2 times day OR 3 times day" is not semantically incorrect, the inclusion of the "OR" makes this output nonvalid according to the CFG. Nonvalidity was the primary error type of 15\% of the categorized model errors.

The final common error type among the labeled sample errors is \textbf{semantic overlap}. Under the proposed CFG, certain MTCs can be accurately represented by different MTC types. Consider, for example, the last DUG in Table \ref{error_examples}, from the FDA dataset. While the direction to take the medication "at morning and at noon" could potentially imply the "6 hours apart" interval constraint, as extracted by the \textit{specialized} model, this was instead labeled as the two semantically-related time-of-day MTCs "in the morning" and "at noon". Semantically-overlapping errors primarily occurred in 15\% of the categorized sample errors.

