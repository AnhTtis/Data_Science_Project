\documentclass[10pt]{article}
\usepackage[preprint]{tmlr}
\usepackage[utf8]{inputenc}

\newif\iftmlr
\usepackage{amsmath, amssymb, amsthm}
\usepackage[colorlinks, allcolors=blue]{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{fullpage}
\usepackage{xcolor}
\usepackage{nicefrac}
\usepackage{url}
\usepackage{natbib}
\usepackage{bm}
\usepackage{xspace}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{multirow}

\usepackage[colorinlistoftodos,prependcaption]{todonotes}
% Author inline
% \newcommand{\alekh}[1]{\textcolor{blue}{\textbf{AA:} #1}}
% \definecolor{darkgreen}{rgb}{0,0.4,0.0}
% \newcommand{\mcm}[1]{\textcolor{darkgreen}{\textbf{BM:} #1}}
% \newcommand{\zx}[1]{\textcolor{cyan}{\textbf{ZX:} #1}}
\newcommand{\alekh}[1]{}
\definecolor{darkgreen}{rgb}{0,0.4,0.0}
\newcommand{\mcm}[1]{}
\newcommand{\zx}[1]{}

\newcommand{\cF}{\ensuremath{\mathcal{F}}}
\newcommand{\cX}{\ensuremath{\mathcal{X}}}
\newcommand{\cA}{\ensuremath{\mathcal{A}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\DeclareMathOperator*{\argmax}{\text{argmax}}
\DeclareMathOperator*{\argmin}{\text{argmin}}
\DeclareMathOperator*{\argsup}{\text{argsup}}
\DeclareMathOperator*{\arginf}{\text{arginf}}
\newcommand{\sqcb}{\texttt{SquareCB}}
\newcommand{\cC}{\ensuremath{\mathcal{C}}}
\newcommand{\cdist}{p}
\newcommand{\clients}{\cC}
\newcommand{\order}{\ensuremath{\mathcal{O}}}
\newcommand{\fedopt}{\ensuremath{\mathrm{fed\text{-}opt}}}
\newcommand{\unif}{\ensuremath{\mathrm{Unif}}}
\newcommand{\epsfed}{\ensuremath{\epsilon_{\fedopt}}}
\newcommand{\squarecb}{\texttt{SqureCB}\xspace}
\newcommand{\falcon}{\texttt{FALCON}\xspace}
\newcommand{\cZ}{\ensuremath{\mathcal{Z}}}
\newcommand{\fedavg}{\textsc{FedAvg}\xspace}
\newcommand{\regret}{\ensuremath{\mathrm{Regret}}}
\newcommand{\fedregret}{\ensuremath{\mathrm{Fed\text{-}Regret}}}

\newcommand{\emnist}{\texttt{EMNIST}\xspace}
\newcommand{\solong}{\texttt{StackOverflow}\xspace}
\newcommand{\so}{\texttt{SO}\xspace}

\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

\newcommand{\activeClients}{\mathcal{S}}
\newcommand{\ind}{\ensuremath{\bm{1}}}
\newcommand{\greedy}{\texttt{Greedy}\xspace}
\newcommand{\egreedy}{$\epsilon$-\greedy}
\newcommand{\softmax}{\texttt{Softmax}\xspace}

\newcommand{\scratch}{\texttt{scratch}\xspace}
\newcommand{\init}{\texttt{init}\xspace}
\newcommand{\initshift}{\texttt{init-shift}\xspace}
\newcommand{\ninf}{\ensuremath{n_{\text{inf}}}\xspace}
\newcommand{\rscalar}{\ensuremath{\mathfrak{r}}}
\newcommand{\cdim}{d}  % Dimension of context.

%% Specialist box related
\usepackage[T1]{fontenc}
\usepackage{kantlipsum}
\usepackage[breakable, theorems, skins]{tcolorbox}
\tcbset{enhanced}
\newtcolorbox{specialistbox}[1]{fonttitle=\bfseries,title=#1,colframe=gray!75!white}

\title{An Empirical Evaluation of \\ Federated Contextual Bandit Algorithms}
\author{\name Alekh Agarwal \email alekhagarwal@google.com \\ \addr Google Research 
\AND 
\name H. Brendan McMahan \email mcmahan@google.com \\ \addr Google Research
\AND 
\name Zheng Xu \email xuzheng@google.com \\ \addr Google Research
}
\date{}

\begin{document}

\setboolean{tmlr}{false} 

\maketitle

\begin{abstract}
    As the adoption of federated learning increases for learning from sensitive data local to user devices, it is natural to ask if the learning can be done using implicit signals generated as users interact with the applications of interest, rather than requiring access to explicit labels which can be difficult to acquire in many tasks. We approach such problems with the framework of federated contextual bandits, and develop variants of prominent contextual bandit algorithms from the centralized seting for the federated setting. We carefully evaluate these algorithms in a range of scenarios simulated using publicly available datasets. Our simulations model typical setups encountered in the real-world, such as various misalignments between an initial pre-trained model and the subsequent user interactions due to  non-stationarity in the data and/or heterogeneity across clients. Our experiments reveal the surprising effectiveness of the simple and commonly used softmax heuristic in balancing the well-know exploration-exploitation tradeoff across the breadth of our settings.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Federated learning~\citep{konevcny2016federated,mcmahan2017fedavg,kairouz2021advances} has emerged as an important machine learning paradigm for settings where the raw training data remains decentralized across a potentially heterogeneous collection of devices. A key motivation for cross-device federated learning (henceforth FL) arises from scenarios where these devices belong to various users of a service, and the goal is to learn predictive models from the data generated when the user interacts with the service. This has benefits from a privacy perspective, and can also allow the development of more expressive models that leverage contextual features that would be unavailable in the datacenter. 

Federated learning directly encodes the data minimization privacy principle including focused collection and ephemeral updates with immediate aggregation \citep{bonawitz22cacm}, and much recent work has shown it is possible to combine these benefits with data anonymization for the trained models via differential privacy \citep{mcmahan18learning,kairouz21practical}. 
Following standard terminology in this area, we refer to the end devices as the clients, and the learning process is orchestrated by a centralized server.

From a machine learning perspective, the bulk of federated learning algorithms can be effectively seen as a particular category of decentralized learning algorithms which aim to solve a loss minimization problem over the entire dataset distributed across the clients, without explicitly performing such a data collection \citep{wang21fedopt}. This is an attractive framework, as it captures many supervised machine learning settings, such as classification and regression, as long as the label signal arises naturally from user interaction. This is the case for example with next-word-prediction \citep{hard2018gboard}, where if the system does not correctly predict the next word, it is reasonable to assume the user will simply tap/swipe the desired word without using the next-word suggestions.

However, many real-world federated learning scenarios do not provide such a complete feedback signal. For example, consider an application where we want to display a featured image from a user's phone every time they open the photo gallery. Other applications could be to annotate each image and/or text message with a label corresponding to its category from a predefined set, or to suggest emoji and stickers (where the user does not know the full set of options) in a mobile keyboard. 
In all these examples, the underlying training data for learning is highly sensitive to the user, and collecting ground truth labels from third-party human labelers is not feasible.  Furthermore, even if privacy allowed the use of human labelers, in the first example of selecting a featured image, it is nearly impossible for a labeler to guess which image from a user's collection appeals to them, and it is impractical for a user to respond with the best choice of a featured image from their entire collection. A much more natural feedback modality in all these settings is to make a recommendation (of an image, label, emoji, or sticker) to the user, and observe and learn from their response to that recommendation. Further, note that both user preferences and the set of available recommendations may evolve over time. Supervised learning fails to properly capture such settings where we only observe feedback on the choices driven by the learning algorithm, and reinforcement learning (RL) offers a much better fit for these problems where we seek to learn from user feedback. 

A particular subset of RL which is quite effective at capturing several recommendation settings is that of contextual bandits (CB)~\citep{langford2007epoch,auer2002using,agarwal2014taming}. A key difference between RL/CB and more traditional supervised learning approaches is the explicit recognition that the algorithm only collects feedback for the choices it presents to the user, and hence it is important to navigate the \emph{exploration/exploitation} tradeoff. That is, the algorithm should explore over a diverse set of plausibly good choices in any situation, and use the feedback to further prune the set of plausible choices. Motivated by the twin concerns of learning from user feedback in a decentralized and private manner, there is an emerging literature on federated CB learning~\citep{huang2021federated,dai2022federated,dubey2020differentially}. However, the bulk of the existing work is theoretical in nature, with a focus on simple models such as multi-armed or linear bandits, with a key focus on exploration in the federated setting. An important aspect of several works here is also developing the right notions of privacy suited to the interactive learning setting~\citep{shariff2018differentially,dubey2020differentially}.

In this work, we study federated CB learning with a complementary focus to the aforementioned works. We design federated adaptations of practical state-of-the-art CB algorithms from the centralized setting, and conduct an extensive empirical evaluation in a range of realistic settings. Algorithmically, we focus on a black-box approach, where we isolate a component of the centralized CB algorithms which relies on solving a classification or regression problem, and replace this with a federated learning counterpart. This is practically desirable, as it makes it easy to incorporate latest advances from federated optimization into the CB algorithms as drop-in replacements. The isolated federated optimization can also be combined with complementary privacy techniques such as secure aggregation~\citep{bonawitz2017practical} and differential privacy~\citep{mcmahan18learning,kairouz21practical}.

We primarily consider cross-device FL \citep{kairouz2021advances,wang21fedopt}, which is typically more challenging than cross-silo FL due to constraints such as limited computation resources, bandwidth, and client availability; hence, our proposed framework and algorithms are also applicable to cross-silo FL, though more sophisticated cross-silo algorithms can be possibly designed if we relax the on-device constraints and tackle other challenges like feature alignment \citep{yang2019federated}

Even in the centralized setting, empirical evaluation of CB methods is limited to just a few works~\citep{bietti2021contextual,foster2020instance}, and often ignores practical concerns such as data non-stationarity and the impracticality of updating the CB model after each example. The federated setting adds further challenges related to data heterogeneity across clients, greater delays in model updates on clients and configuring the settings of the underlying federated optimization approach as some examples. Our work uses two popular FL benchmarks, \emnist and \solong (\so for short), and turns them into simulators for the federated CB setting by adapting and extending the ideas from the work of~\citet{bietti2021contextual}. Within this simulation, we evaluate federated adaptations of several centralized CB algorithms in both stationary and realistic simulations of non-stationary settings. We also study the influence of providing a small amount of labeled data to create an initial model, which is typical in practice. 

\citet{bietti2021contextual} observed that the greedy approach offers an extremely strong baseline in stationary centralized settings. We show this result can extend to the federated setting, and in particular that a greedy strategy is highly effective when the problem is stationary and the model can be updated frequently. However, exploration becomes critical under delayed updates and/or non-stationarity. The use of a strong initial model can mitigate this to a reasonable degree, particularly in stationary settings. When exploration strategies are necessary, we find federated versions of a simple softmax exploration strategy, and an adaptation of \falcon, to be the best performing across the range of settings, with softmax being easier to tune than \falcon.

We emphasize our goal is not to show that bandit algorithms ``win'' against baselines. Rather, we hope that this study can both provide a valuable resource in terms of a strong evaluation setup for future research on federated CBs, as well as offer practical recipes for practitioners facing the federated CB setting and needing to decide whether the additional complexity of deploying a bandit algorithm with an explicit exploration strategy is likely to be beneficial.

\section{Preliminaries}
\label{sec:prelim}

We begin by briefly recalling the federated learning and contextual bandit paradigms in this section. We then build on these to set up the federated contextual bandit setting in the next section.

\subsection{Federated Learning}
\label{sec:federated}

In a federated learning problem, we are given a distribution $\cdist$ over a population $\clients$ of clients. Client $c\in\clients$ has an associated data distribution $D_c$ over samples $z\in\cZ$. The learning algorithm aims to find a good model $f\in\cF$ under some loss function $\ell~:~\cF\times\cZ\to\R$, so as to minimize the objective:

\begin{equation}
    \min_{f\in\cF} \E_{c\sim \cdist} \E_{z\sim D_c} \ell(f,z). 
    \label{eq:fed-pop}
\end{equation}
%

Like most learning algorithms, the objective~\eqref{eq:fed-pop} is optimized approximately using a sample-based approximation. Unique to federated learning, however, the datasets stay local to each client, while model updates  from each client are aggregated and applied on the central server. For intuition, a canonical federated learning algorithm is \fedavg~\citep{mcmahan2017fedavg}, in which a random subset of the clients each use their local data to compute an update to the shared model by performing a few stochastic gradient steps with the local dataset. The updates are then communicated to the server which averages these local model changes and uses this average to update the shared global model at the server.

\subsection{Contextual Bandits}
\label{sec:cb}

Contextual bandits are a paradigm to learn from interaction data where each interaction consists of observing a context $x \in \R^\cdim$  from some fixed and unknown distribution, choosing an action $a\in\cA$ from some action set $\cA$ and observing some reward $r(x,a)\in\R$ 
specifying the quality of the action $a$ for context $x$. Crucially, the learner receives no signal on the quality of actions $a'\ne a$ which were not chosen. We let $D$ represent the joint distribution of $(x, r)$, but also overload it to denote the marginal distribution over $x$, when it is clear from the context. We view $r$ as a random variable, where $r(x, a)$ is the $a_{th}$ entry in the reward vector $r \sim D(\cdot | x)$. As mentioned above, the learner never observes the full reward vector $r$, but only the entry $r(x,a)$ when it chooses action $a$ upon observing context $x$. The learner has access to a policy class $\Pi \subseteq \{\cX\to\cA\}$, where a policy is a mapping from contexts to actions. For deterministic policies we write $\pi(x) \in \cA$; we generalize this to randomized policies where $\pi(a | x) \in [0, 1]$ below. The goal of learning is to find a policy that maximizes the expected reward, and the quality of some policy $\pi$ is measured in terms of regret, defined as\footnote{For a randomized policy, we can replace $\pi(x)$ with an expectation over $a\sim \pi(\cdot | x)$.}
\begin{equation}
    \regret(\pi) = \E_{(x,r)\sim D} [r(x,\pi(x))] - \max_{\pi'\in\Pi} \E_{(x,r)\sim D}[r(x,\pi'(x))].
    \label{eq:regret-cent}
\end{equation}

For intuition, a deterministic policy class $\Pi$ might be induced by a regression function class $\cF$ as $\Pi = \{\pi_f~:~\pi_f(x) = \argmax_{a\in\cA} f(x,a), f\in\cF\}$, where the functions $f$ are trained to predict the expected reward using regression. That is, given a dataset $(x_s, a_s, r_s)_{s=1}^{t-1}$ of historical examples (where $r_s \in \R$ represent the realization of the random variable $r(x_s, a_s)$), we train the reward estimator $f_t = \argmin_{f\in\cF} \sum_{s=1}^{t-1} (f(x_s, a_s) - r_s)^2$. A common choice we will use in most of our setup is when the functions $f$ are parameterized as $f_\theta$ for some parameter $\theta \in \Theta$, where $\theta$ might denote the weights of a linear function or a neural network, for instance. 

There are several standard ways of extracting a randomized policy $\pi_t$ from $f_t$, some of which we discuss below.
\begin{itemize}
    \item \greedy corresponds to the standard supervised learning approach, where we always choose the best action according to $f_t$,
    \begin{equation} 
    \Pi = \big\{\pi_f~:~\pi_f(a| x) = 1 \text{ if } a = \argmax_{a'\in\cA} f(x, a') \text{ and $0$ otherwise}, f\in\cF\big\}
    \end{equation}
    (with ties broken arbitrarily). 
    \item \egreedy chooses the greedy action with probability $1-\epsilon$, and with probability $\epsilon$, picks an action uniformly from $\cA$. The extra exploration helps in collecting a more diverse dataset to train $f_t$, with the parameter $\epsilon$ providing a tradeoff between exploration and exploitation. For any data distribution $D$, the regret of \egreedy is known to be bounded, whenever the class $\cF$ is sufficiently expressive~\citep{langford2007epoch,agarwal2012contextual}. 
    \item \softmax is another variant of \greedy, where the policy uses a softmax distribution on the predicted rewards by the underlying model: $\pi_t(a|x) \propto \exp(f(x,a)/\beta)$. When $\beta$ approaches zero, the $\pi_t$ approaches the greedy policy, and diffuses to a uniform exploration for $\beta = \infty$. In general, this strategy does not have theoretical guarantees on the regret, but is often practically used owing to its simplicity. 
    \item \falcon is provably optimal in the worst-case~\citep{simchi2022bypassing,foster2020beyond} and uses a more carefully crafted distribution over actions, given $f_t$ (see line~\ref{algline:falcon} in Algorithm~\ref{alg:fedcb_inference}). The degree of exploration is governed by two hyperparameters $\gamma$ and $\mu$, which makes this strategy a little harder to tune in practice. For setting these hyperparameters, we depart from the theoretical recommendation in~\citet{simchi2022bypassing} of using a careful schedule and use a best constant setting closer to \citet{foster2020beyond}, as some of the quantities in the theoretical recommendations depending on the function class complexity and failure probability are unknown in practice.
\end{itemize}



\section{Federated Contextual Bandits}
\label{sec:alg}

We begin with the high-level problem setting and the algorithmic framework. We then present detailed federated variants of popular CB algorithms. 

\subsection{Problem Setting}
\label{sec:fedcb-setting}

\begin{algorithm}[htb]
\caption{Federated Contextual Bandits} \label{alg:fedcb_framework}
\begin{algorithmic}[1]
\Require Communication rounds $T$ per period; training periods $I \ge 1$; initial inference model $\theta_0$
\For{$i=1,2,\ldots,I$}
\State Deploy inference policy $\pi$ parameterized by $\theta_{i-1}$ to all clients \clients
\For{each $c \in \clients$ {\it \bf in parallel}} \label{algline:inf-begin}
   \State $B_c \gets \text{BanditInference}(\pi, \theta_{i-1})$  \Comment{\Cref{alg:fedcb_inference}}
\EndFor \label{algline:inf-end}
\State $\triangleright$ In a real deployment, training and inference might occur in parallel, but we simulate sequentially:
\State Initialize optimization $\theta^{(0)} \gets \theta_{i-1}$ \label{algline:train-begin}
\For{$t=1,2,\ldots,T$} 
  \State $\theta^{(t)} \gets \text{FederatedRound}(\theta^{(t-1)}) $  \Comment{\Cref{alg:fedcb_optimization}}
\EndFor
\State $\theta_i \gets \theta^{(T)}$ \label{algline:train-end}
\EndFor
%\State (Optional) \Return $\theta_Q$
\end{algorithmic}
\end{algorithm}


With both the FL and CB settings defined individually, we now describe the federated CB setting. The high-level framework for the algorithms and the interaction with the environment is presented in Algorithm~\ref{alg:fedcb_framework}. In a federated CB problem, there is a distribution $\cdist$ over clients $c \in \clients$, with each client having a joint distribution $D_c$ over context and reward pairs. The server maintains a global policy $\pi \in \Pi$, which is now learned in a federated manner. That is, each client maintains some (potentially stale) version of the server's policy locally, which we denote as $\pi_c$.\footnote{Federated learning makes the possibility of learning a personalized policy $\pi_c$ much easier, but we focus on global policies in this work. Note that even with a global policy, the contextual features $x$ can be used to encode substantial information about the particular client $c$, and hence can lead to strongly personalized recommendations.} Each client $c$ collects data by choosing actions on observed contexts according to $\pi_c$ and logs the reward received (lines \ref{algline:inf-begin}-\ref{algline:inf-end} in Algorithm~\ref{alg:fedcb_framework}), and we call this operation \emph{bandit inference}. Some subset of the clients periodically participate in \emph{federated training} to update the policy $\pi$ at the server, using their local data (lines \ref{algline:train-begin}-\ref{algline:train-end}). We explain the details of inference and training rounds in detail below.

\paragraph{Bandit inference.} Bandit inference refers to the user-visible use of the policy $\pi_c$ locally at a client, whenever it is queried for an action with a context.  For instance, this might correspond to choosing a featured image or an emoji recommendation upon observing the user's photo album or text message in our previous examples. Formally, at an inference step, a client $c$ observes a context $x\sim D_c$, chooses an action $a \sim \pi_c(\cdot | x)$ and observes the reward $r \sim D(\cdot | x,a)$.
The inference steps happen asynchronously at the clients and do not require any communication, since the client only invokes a locally stored version of the policy to choose actions. The agent also maintains an internal log of inference tuples of the form $(x,a,r, \pi_c(a|x)) \in (\R^\cdim, \cA, \R, [0, 1])$, which are saved in data cache~\citep{hard2018gboard} and later used to update the server policy in the training rounds which we describe next. 

\paragraph{Federated training.} Periodically, the server polls a subset of the clients to participate in federated training. Roughly, this corresponds to using the inference logs across the participating clients to improve the regression model $f(\cdot, \cdot; \theta)$. However, this federated training for policy improvement happens in a decentralized manner with no explicit data pooling. For instance, each participating client $c$ downloads the current server regression parameters $\theta^{(t)}$ and uses its local logs to compute a local gradient direction, which is communicated to the server. The server then accumulates the gradients across the clients to update $\theta^{(t)}$ to form $\theta^{(t+1)}$. After several communication rounds,the training period concludes and the server can broadcast the updated regression parameters (and hence updated policy) to all the clients, or rely on the clients to pull an updated policy periodically. 


\paragraph{Federated regret.} The typical performance metric in a centralized CB setting is regret, which measures the average performance of a policy $\pi$ when we perform inference according to it, relative to the best possible policy in some class $\Pi$ as in \cref{eq:regret-cent}. Analogously in the federated CB problem, we seek to control the bandit inference regret. Informally, federated regret is the average expected regret that is incurred across the client population, during the bandit inference process. To define the metric formally, let us consider a policy $\pi$ which is used simultaneously by a population of clients distributed according to $\cdist$. Then the average federated inference regret of this policy $\pi$, relative to a policy class $\Pi$ is given by:
\begin{equation}
    \fedregret(\pi) = \E_{c\sim \cdist}\E_{(x,r)\sim D_c, a\sim \pi(\cdot | x)} [r(x,a)] - \max_{\pi'\in\Pi} \E_{c\sim \cdist}\E_{(x,r)\sim D_c, a\sim \pi'(\cdot | x)}[r(x,a)].
    \label{eq:regret-fed}
\end{equation}
%
We evaluate a common policy $\pi$ at all the clients, instead of allowing a separate policy $\pi_c$ at each client, because we do not consider client-specific policies in this work. The client policies might further differ in practical settings, where the client might poll parameters from the server asynchronously, and we omit such generalizations here to focus on the key ideas in the presentation.
    
\subsection{Federated CB algorithms}
\label{sec:cb_alg}

\begin{algorithm}[htb]
\caption{Bandit Inference on Client $c$} \label{alg:fedcb_inference}
\begin{algorithmic}[1]
\Require Model parameters $\theta$; number of actions $K = |\cA|$; data cache size $M$; exploration parameter \colorbox{blue!25}{$\epsilon$ for \egreedy}, \colorbox{green!25}{$\beta$ for \softmax}, \colorbox{yellow!25}{$\mu,\gamma$ for \falcon}
%\Function{BanditsInference}{$\pi, \theta_{q-1}, c$}
\State (Optional) initialize data cache $B_c = \emptyset$  \Comment{The cache can be reset for simplicity in simulation}
\For {$j =1,\dots, M$} \Comment{We only simulate sufficient user interactions to fill the cache}
    \State Observe $x^j \sim D_c$. Let $a_\theta^j = \argmax_{a\in\cA} f_\theta(x^j, a)$
    \State \colorbox{blue!25}{$\pi(a | x^j) = 1 - \epsilon + \epsilon/K$ if $a = a_\theta^j$ else $\epsilon/K$} \Comment{\egreedy} \label{algline:egreedy}
    \State  \colorbox{green!25}{$\pi(a | x^j) = \exp(f_\theta(x^j, a)/\beta) / \sum_b \exp(f_\theta(x^j, b)/\beta)$ } \Comment{\softmax} \label{algline:softmax}
    \State \colorbox{yellow!25}{$\pi(a | x^j) = 1/\big(\mu + \gamma(f_\theta(x^j, a_\theta^j) - f_\theta(x^j, a))\big)$ if $a \neq a_\theta^j)$ else $1-\sum_{b \neq a_\theta^j} \pi(b | x^j)$} \Comment{\falcon} \label{algline:falcon}
    \State Sample $a^j \sim \pi(\cdot | x^j)$ and observe $r^j$ for $a^j$
    \State $B_c \gets B_c\cup \{(x^j, a^j, r^j, \pi(a^j | x^j))\}$  \label{algline:locdata}
\EndFor
\State \Return $B_c$
%\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
\caption{One Round of Federated Optimization} \label{alg:fedcb_optimization}
\begin{algorithmic}[1]
\Require Global model $\theta^{(t-1)}$ from the previous round; subset of clients $\activeClients^{(t)}\subset \clients$
%\Function{FederatedRound}{$\theta^{(t-1)}$}
\State Broadcast $\theta^{(t-1)}$ from server to clients $\activeClients^{(t)}$
\For{each $c \in \activeClients^{(t)}$ {\it \bf in parallel}}
    \State $\Delta _c^{(t)} = \text{ClientUpdate}(\theta^{(t-1)}, B_c)$
\EndFor
\State $\Delta^{(t)} = \text{aggregate}(\Delta _c^{(t)})$ \Comment{Compatible with SecAgg and DP}
\State \Return $\theta^{(t)}=\text{server-optimizer}(\theta^{(t-1)},\, \Delta ^{(t)})$ 
%\EndFunction
\vspace{2mm}
\Function{ClientUpdate}{$\omega^0, B_c$}
\For {$k =1,\dots, N$} 
\State Sample a minibatch  $G \subset B_c$
\State \colorbox{cyan!25}{Compute gradient $g =  \frac{\partial}{\partial \theta}\sum_{(x,a,r,\rho) \in G} \,  \frac{1}{2}(f(x,a) - r)^2 $}
\Comment{Regression-based loss}
\State \colorbox{orange!25}{Compute gradient $g =  \frac{\partial}{\partial \theta}\sum_{(x,a,r,\rho) \in G} \, \frac{1}{2\rho}(f(x,a) - r)^2 $}
\Comment{Importance weighting loss}
\State $\omega^k = \text{client-optimizer}(\omega^{k-1},\, \text{g})$
\EndFor
\State \Return $\Delta _c^{(t)} \gets \omega^N - \omega^0$
\EndFunction
\end{algorithmic}
\end{algorithm}

In this section, we describe the federated CB algorithms that are developed and studied in this paper. 
The federated CB algorithms that we design are federated versions of the centralized CB algorithms described in \cref{sec:cb}. Recalling the general framework of \Cref{alg:fedcb_framework}, we consider a meta iterator in the outer-loop named period, which can possibly run forever in an online setting, i.e., $I=\infty$. Each period simulates the deployment of  a machine learning model parameterized by some parameters $\theta_{i-1}$, which can be less frequent for on-device applications compared to a web service. We focus on regression-based CB algorithms as in \cref{sec:cb}, where the parameters $\theta$ induce a regression model which predicts the expected reward of actions $a$, given context $x$. Each period $i$ consists of some number of bandit inference steps followed by a training. At the beginning of each period, an inference model is deployed to all clients, and the model is trained with bandits data generated by a (delayed) inference model from the last period. For simplicity of presentation, we use the same number of examples at each client in inference, and do not incorporate heterogeneous delays in model deployment across clients as mentioned before.

Algorithm~\ref{alg:fedcb_inference} describes the details of the inference procedure that happens asynchronously at each client. Client $c$ observes a context $x\sim D_c$. Given the current model parameters $\theta = \theta_{i-1}$, we use $f_\theta$ to refer to the induced reward predictor. This reward predictor $f_\theta$ is used to define a probability distribution over the actions as described in lines \ref{algline:egreedy}-\ref{algline:falcon}. The \greedy strategy is implemented by setting $\epsilon = 0$ in \egreedy. The chosen action $a$ is subsequently drawn from this probability distribution, and the observed reward is logged along with the context, action and sampling probability in a local data log $B_c$ (line~\ref{algline:locdata}). In practice, where the number of inference examples handled at a client is exogenously determined, each client observes a potentially different number of inference examples in a period,  $B_c$ is maintained locally on client and can be configured with suitable cap $M$ on the size of the local data log to respect memory and system constraints. Local cache $B_c$ potentially contains inference examples predicted by multiple previous model $\theta_{0}, \ldots, \theta_{i-1}$ due to heterogeneous delays of model deployment. When the deployment period is large, most of the clients participate in training contain lcoal cache of examples predicted by the most recent inference model $\theta_{i-1}$, and hence we reset $B_c$ every round for simplicity in simulation when used \cref{alg:fedcb_simulation}.

Next, we discuss the algorithmic details of the training period, described in Algorithm~\ref{alg:fedcb_optimization}. At a high-level, this procedure boils down to identifying an appropriate optimization objective on the local data logs of all the clients, which can then be optimized by any standard federated optimization algorithm. We consider two optimization objectives, motivated by the two predominant algorithmic settings in centralized CB. We describe their expected versions here, with the understanding that actual implementations use sample averages. The simplest objective is a regression on observed rewards as described before~\citep{agarwal2012contextual,foster2020beyond,simchi2022bypassing}: 
\begin{equation}
    \text{Regression:}\quad \min_{f\in\cF} \E_{c\sim \cdist} \sum_{(x,a,r,\rho)\in B_c} (f(x,a) - r)^2.
    \label{eq:regression-unweight}
\end{equation}
%
When the class $\cF$ is rich enough to satisfy $\E[r | x,a] \in \cF$, this objective is natural, as the minimizer converges to the true expected rewards. However, if this assumption is grossly violated, then the regression objective can learn an unreliable predictor. A potentially preferable objective in such contexts is the following importance weighted regression variant~\citep{bietti2021contextual}:
\begin{equation}
    \text{Importance-weighted regression:} \quad \min_{f\in\cF} \E_{c\sim\cdist} \sum_{(x,a,r, \rho)\in B_c} \frac{1}{\rho}(f(x,a) - r)^2,
    \label{eq:regression-weight}
\end{equation}
%
where $\rho$ is the recorded probability of choosing $a$ given $x$ in the local data log.
Importance-weighting ensures that the objective is an unbiased estimator of $\E_{c\sim\cdist} \E_{(x,r)\sim D_c} \sum_{a\in\cA} (f(x,a) - r)^2$, so that the learned reward estimator is uniformly good for all the actions. This leads to strong guarantees for any function class $\cF$, at the cost of a harder to optimize and higher variance training objective. We note that the application of \falcon with importance weighted updates is not considered in the original paper~\citep{simchi2022bypassing}. For our experiments, we primarily focus on the regression version as it displays superior empirical performance.

For either objective, we note that the underlying optimization problem clearly fits the form of the standard federated learning objective~\eqref{eq:fed-pop}, meaning that off-the-shelf federated optimization algorithms can be readily applied. Federated Averaging (FedAvg) \citep{mcmahan2017fedavg} is a popular choice in pracitce, as it achieves both communication efficiency and fast convergence under heterogeneity \citep{wang2022unreasonable}. In \cref{alg:fedcb_optimization}, we adopt the generalized FedAvg algorithm \citep{reddi2021adaptive,wang21fedopt}, which views FL algorithms as two stage optimization: clients perform local training to compute model update $\Delta_c$, and the server uses the averaged $\Delta$ as a pseudo gradient to update the global model $\theta$. The server performs such updates for $T$ rounds, sampling a fresh subset of clients at each round. Subsequently, the updated parameters are communicated to the clients for bandits inference, as mentioned earlier.

The updates on client and server require the specification of optimizers to be used. We follow standard practice and use stochastic gradient descent (SGD) as the client-optimizer as it works well and incurs no additional memory or computation overhead. We use Adam \citep{kingma2014adam} as the server-optimizer following \citet{reddi2021adaptive}.
%

\paragraph{Differential privacy (DP).} The privacy properties of \Cref{alg:fedcb_optimization} can be further improved via techniques like secure aggregation~\citep{bonawitz2017practical} for the model updates, and by replacing FedAvg with variants that offer differential privacy~\citep{mcmahan18learning,kairouz21practical,choquette22multiepoch}. We apply adaptive clipping~\citep{andrew2019differentially} with zero noise in aggregation as this has been shown to improve robustness with minimal computation and communication cost \citep{charles2021large} in the bulk of our evaluation. In some of our experiments, we show the easy composition with differential privacy by introducing two additional operations for DP-FedAvg~\citep{mcmahan18learning}: clip the model update $\widetilde{\Delta} _c^{(t)}=\min{\left(1, \frac{C}{||\Delta _c^{(t)}||}\right)} \Delta _c^{(t)}$ with clip norm $C$ estimated by adaptive clipping~\citep{andrew2019differentially}; add Gaussian noise with standard deviation $\sigma C$ to $\Delta^{(t)}=\text{aggregate}(\widetilde{\Delta} _c^{(t)})$, where $\sigma$ is noise multiplier and $C$ is the clip norm. 


\section{Simulation Setup}
\label{sims}

In this section, we describe the setup used for our simulations of real-world federated CB problems. We describe the datasets used in our simulation, a detailed specification of the algorithms in the simulation setting, and the various settings that we simulate. 
\ifthenelse{\boolean{tmlr}}{Our code will be open-sourced}{Our code is open-sourced and can be found at  \url{https://github.com/google-research/federated/tree/master/bandits}}.

\subsection{Datasets for Simulating Federated CB}
The methods that we evaluate roughly correspond to those outlined in Sections~\ref{sec:cb} and \ref{sec:alg}. Concretely, we evaluate the \greedy, \egreedy, \softmax and \falcon strategies described above. For each strategy, we consider a few choices of the hyperparameters and mainly show the results for the best choice in a particular experimental condition. Details of the hyperparameters used can be found in Appendix~\ref{app:hyper}.

We use two datasets two evaluate these methods across a range of simulation settings in this work. The datasets are \emnist and \solong(\so), both of which have been used in prior works on federated learning. \emnist is a handwritten character recognition dataset, comprising of digits (0-9) and letter (a-z, A-Z) inducing a multi-class classification problem with 62 labels. The dataset consists of characters written by different individuals, which are mapped to the different clients in the federated setting.  We use the EMNIST dataset of 3400 clients provided by Tensorflow Federated~\citep{emnist} to train a two-layer convolutional neural network (CNN) \citep{mcmahan2017fedavg,reddi2021adaptive}. In bandit interaction, the learner predicts a class label upon seeing a character, and only gets a feedback about the correctness of this prediction, but does not observe the ground-truth label when this prediction is wrong, following the setup from prior works~\citep{dudik2014doubly,bietti2021contextual}. 

\so~\citep{stackoverflow} is a language dataset of processed question and answer text with additional metadata such as tags. The dataset contains 342,477 unique users as training clients. We consider the tag prediction task and use a linear model based on the bag of words features for the sentences in each client. A vocabulary of 10,000 most frequent words is used. To make exploration feasible, we limit the tag set to the 50 most frequent tags. The original tag prediction is a multi-label and multi-class classification problem, and similar to EMNIST in bandit interaction, the learner will only get feedback about the correctness of a single predicted tag without observing the ground-truth label. 

Next we discuss the various simulation setups used in this work.

\begin{algorithm}[htb]
\caption{Federated Contextual Bandits in Simulations} \label{alg:fedcb_simulation}
\begin{algorithmic}[1]
\Require Communication rounds $T$ per period; training periods $I$; initial inference model $\theta_0$; bandits inference algorithm and hyparameters in \cref{alg:fedcb_inference}; federated optimization algorithms and hyparameters in \cref{alg:fedcb_optimization}.
\For{$t=1,2,\ldots,IT$} 
   \State $i = \lceil t/T \rceil $
    \State Send training model $\theta^{(t-1)}$, inference model $\theta_{i-1}$ from server to a subset clients $\activeClients^{(t)}$
    \For{each $c \in \activeClients^{(t)}$ {\it \bf in parallel}}
        \State $B_c \gets \text{BanditInference}(\pi, \theta_{i-1})$  \Comment{\Cref{alg:fedcb_inference}}
         \State $\Delta _c^{(t)} = \text{ClientUpdate}(\theta^{(t-1)}, B_c)$ \Comment{\Cref{alg:fedcb_optimization}}
    \EndFor
    \State $\Delta^{(t)} = \text{aggregate}(\Delta _c^{(t)})$
    \State $\theta^{(t)}=\text{server-optimizer}(\theta^{(t-1)},\, \Delta ^{(t)})$ 
    \If{$t \mod T == 0$}
      \State $\theta_i \gets \theta^{(t)}$
    \EndIf
\EndFor
%\State (Optional) \Return $\theta^{(IT)}$
\end{algorithmic}
\end{algorithm}



\subsection{Simulation Scenarios} \label{sec:exp_sim_setup}

We consider three simulation scenarios in this paper. They roughly correspond to the scenarios where the CB agent starts from scratch, as is typically assumed in theory, as well as two settings where it starts from an initial model pre-trained with supervised data from a small number of clients, before being deployed in the CB setting. In the first pre-training setting, the reward distribution is the same in the pre-training and deployment phases, while the second one considers a distribution shift on the rewards. We begin with the high-level details of mapping the abstract federated CB framework of Algorithm~\ref{alg:fedcb_framework} into a simulation setting, before describing the 3 variants below.

\paragraph{Simulated federated CB:} When simulating a federated CB problem from a supervised dataset like \emnist or \so, we need to choose the inference and training periods. For simplicity, we consider each period $i$ in Algorithm~\ref{alg:fedcb_framework} to consist of $T$ communication rounds in Algorithm~\ref{alg:fedcb_simulation}, which contains detailed implementation of the simulation framework. In each round $t\in[T]$ of a period $i\in[I]$, we choose a subset $\activeClients^{(t)}$ of the client population. This represents the clients which participate in federated training at round $t$ in period $i$ in Algorithms~\ref{alg:fedcb_framework} and~\ref{alg:fedcb_optimization}. We limit the inference to only happen at the clients selected for training at this round, since the inference data generated at the other clients does not matter for model updates. While the inference rewards at all the clients are needed for measuring the performance of the deployed model, the average over the selected clients provides an unbiased approximation and makes the simulation computationally more tractable. Upon generating the inference data log $B_c$ at all the selected clients $B_c$, we then perform $N$ local updates, followed by an aggregated update of the server parameters. Upon the completion of $T$ such rounds, the client parameters are updated at each client and a new period starts. In this manner, each client has parameters delayed by up to $T$ rounds relative to the server. Note that a minor mismatch between the descriptions of Algorithms~\ref{alg:fedcb_framework} and~\ref{alg:fedcb_simulation} is that if a client is selected at two different rounds within a period, then it uses an identical data log $B_c$ at both the periods in Algorithm~\ref{alg:fedcb_framework}, but samples a fresh log $B_c$ in Algorithm~\ref{alg:fedcb_simulation}. 

Next we describe how the client distributions are simulated using the supervised learning datasets in multiple ways below.

\paragraph{Training from scratch with no initial model (\scratch)} This scenario is the closest to the federated generalization of the standard CB setting studied in most papers. The server and clients start with some randomly initialized model $\theta_0$. The model is used to choose actions for the inference period. The rewards of chosen actions are based on the classification loss, namely $1$ for the action corresponding to a correct label and $0$ otherwise. 

\paragraph{Initial model on a subset of clients (\init)} This scenario roughly captures a common practice in the industry where some small number of users (say employees of the organization) might try the application before broader deployment. In such cases, these initial users might even supply a richer feedback on the algorithm's chosen actions, an extreme case of which is providing the ground-truth label on each example, which allows the instantiation of rewards of all the actions. We model this by selecting a small number of clients for pre-training, and use supervised learning to minimize the squared loss across all the actions for each $x$, given the full reward vector. With this initial model, we then deploy to a broader client pool. Subsequently, the model is again updated in every training period in the same manner as the \scratch scenario. We choose the number of initial clients to be 100 for both \emnist and \so. 

\paragraph{Initial model on a subset of clients with reward distribution shift (\initshift)} In practice, it is often unrealistic to assume that the reward distribution for model pre-training will match that during deployment due to a number of factors. The distribution of best actions within a subset of initial users (such as within an organization) might be heavily skewed relative to the general population. If the supervision for the initial model is instead obtained by third-party labelers, then there can be a mismatch between their preferences and those of the users. Finally, even beyond these, most practical problems exhibit non-stationarity~\citep{wang21fedopt,zhu2021diurnal,wu2022motley,jiang2022test} due to seasonal or other periodic effects, drifts in user preferences over time, changes in the action set etc. For example, emoji and users' preference can gradually change in an emoji recommendation application \citep{ramaswamy2019federated}. In a way, some distributional mismatch between initial and deployment phases is likely most representative of the current practical scenario, and we treat this as our \emph{default scenario}. 

In \emnist, we simulate this distribution shift by setting the reward in the initial training to be 1 if the label is correct, 0.5 if the true label is an upper-case letter and we predict its lower-case version and 0 otherwise. During the subsequent bandit simulation, we use the 0-1 valued rewards for exact match with the label, causing a label distribution shift.
 
In \so, we model distribution shift from two sources. The initial training only gets a multilabel $0/1$ feedback based on tags in the 10 most frequent tags. That is, the learner sees a vector of labels of size $10$, which has value $1$ for all the tags in which are present in the example and $0$ otherwise. However, the tag-set is expanded to the top 50 tags in the deployment phase, where the reward of a tag is defined as inversely proportional to the frequency of the tag in the corpus. Thus, the algorithm gets a higher reward for correctly predicting rare tags, which are not likely to be observed in the pre-training phase. 

\paragraph{Simulation durations}
Throughout the experiments, we use a total of 800 communication rounds (corresponding to $IT$ in Algorithm~\ref{alg:fedcb_simulation}) for \emnist and 1600 communication rounds for the larger \so benchmark, and randomly sample 64 clients in each round. The number of training periods $T$ is set to 4 for \emnist and 8 for \so unless otherwise specified, corresponding to the deployment of a new model every 200 communication rounds. For \init and \initshift, where we train an initial model for 100 iterations of supervised training, we only perform 700 (respectively 1500) rounds in the bandit phase for \emnist (respectively \so). The comparison across settings at the end of training is not completely fair, however, as 100 rounds of supervised training provide significantly more information than 100 rounds of bandit interactions, since we observe feedback on all the actions in the supervised setup. We note that the scale of rewards also changes due to the rewards configuration in the \initshift setting.

\section{Empirical Evaluation Results}
\label{sec:expt}

\begin{figure}[t!]
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{EMNIST-scratch.pdf}
        \caption{\emnist scratch}
        \label{fig:emnist-scratch}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{EMNIST-init.pdf}
        \caption{\emnist init}
        \label{fig:emnist-init}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{EMNIST-init-shift.pdf}~\\
        \caption{\emnist init shift}
        \label{fig:emnist-init-shift}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{EMNIST-init-shift-deploy40.pdf}
        \caption{\emnist init shift with deploy\_freq = 40, vs 200 in (a)-(c)}
        \label{fig:emnist-deploy40}
    \end{subfigure}
    \caption{\emnist experiments, without importance weighting. The $y$-axis gives \emph{running average} reward, with different scales for each plot. While the regression model is the same for the first 200 rounds of each scenario, cumulative rewards are different depending on the amount of exploration done by the policy. The ``Init'' lines correspond to the greedy policy on the initial model, with no additional training. All the plots use the exploration parameters $\beta = 0.05$ and $\epsilon = 0.05$ for \softmax and \egreedy respectively. Learning rate and exploration parameter values for each algorithm are detailed in Tables~\ref{tab:emnist-scratch}-\ref{tab:emnist-deploy40} for Figures~\ref{fig:emnist-scratch}-\ref{fig:emnist-deploy40} respectively.}
    \label{fig:emnist-algs-comp}
\end{figure}

\begin{figure}[t!]
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{SO-scratch.pdf}
        \caption{\so scratch}
        \label{fig:so-scratch}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{SO-init.pdf}
        \caption{\so init}
        \label{fig:so-init}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{SO-init-shift.pdf}~\\
        \caption{\so init shift}
        \label{fig:so-init-shift}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{SO-init-shift-deploy40.pdf}
        \caption{\so init shift with deploy\_freq = 40, vs 200 in (a)-(c)}
        \label{fig:so-deploy40}
    \end{subfigure}
    \caption{\solong experiments. Note the different $y$-axis reward scales on the different plots. Learning rate and exploration parameter values for each algorithm are detailed in Tables~\ref{tab:so-scratch}-\ref{tab:so-deploy40} for Figures~\ref{fig:so-scratch}-\ref{fig:so-deploy40} respectively.}
    \label{fig:so-algs-comp}
\end{figure}

We begin with an evaluation of the baselines mentioned in the previous section across all the different experimental settings, before studying the effect of changing some important aspects of the setup as well as algorithmic choices.

\subsection{Results for the three simulation settings}

 In Figures~\ref{fig:emnist-algs-comp} and~\ref{fig:so-algs-comp}, we show a comparison of the different bandit algorithms on the \emnist and \so benchmarks, respectively, across a range of experimental settings. In most of the experiments, we deploy a new model every 200 communication rounds, while the settings vary in $\{\scratch, \init, \initshift\}$, with the results summarized in Figures~\ref{fig:emnist-scratch}-\ref{fig:emnist-init-shift} and~\ref{fig:so-scratch}-\ref{fig:so-init-shift} for the two benchmarks. 

As a first takeaway, we note that \emph{exploration almost always helps} relative to the baseline \greedy strategy, and never hurts, even as the extent of gains can be dependent on the setting. When starting without an initial model in the \textbf{\scratch setting}, exploration is typically crucial since the initial model can arbitrarily prefer certain actions. This is most clearly reflected in Figure~\ref{fig:emnist-scratch} for the \emnist benchmark, although the absolute reward is quite low in both \emnist and \so at the end of the experiment in both the cases for this setting, meaning that the regime might be less relevant practically. While exploration is generally helpful, it is critical to balance the explore-exploit tradeoff, and best performance is generally achieved for parameter settings that result in fairly aggressive exploration early on, before converging closer to a greedy choice towards the end of training in both \falcon and \softmax algorithms. In \cref{sec:dp}, we quantify this phenomenon for \softmax in \cref{fig:emnist-dp-prob,fig:so-dp-prob} while also showing noise added for differential privacy also has an effect. 

In the \textbf{\init setting}, the results are more mixed since the algorithms start with an initial model which already has a strong performance. For instance, the initial model has a higher reward than the performance at the end of training from scratch in both Figures~\ref{fig:emnist-init} and~\ref{fig:so-init}. Consequently, there is little benefit from additional learning, and we find that the best results are attained for hyperparameters that favor little exploration, and small optimization updates through small learning rates.
This is also reflected in the nearly identical behavior as \greedy for most exploration strategies other than \falcon for \so in Figure~\ref{fig:so-init}. We expect that the performance of \greedy deteriorates with respect to the initial model, because a smaller learning rates close to zero outside our search grid can be preferable when initial model is very strong. 
Nevertheless, the overarching conclusion we draw here is that even small amounts of high quality \emph{fully supervised} data can be very powerful, when the downstream model does not encounter any subsequent distribution shift.

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{EMNIST-var.pdf}
    \caption{Variance across \softmax trials for \emnist}
    \label{fig:emnist-var}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
    \includegraphics[width=\textwidth]{SO-var.pdf}
    \caption{Variance across \softmax trials for \so}
    \label{fig:so-var}
    \end{subfigure}
    \caption{Variance across 5 trials of \softmax in the \initshift setting for \emnist and \so}
    \label{fig:var}
\end{figure}

Expecting stationarity after deployment, or fully representative labeled set in training the initial model, however, is an unrealistic assumption, which is the reason we focus on the \textbf{\initshift setting} as our primary one. Here, we again find that \emph{exploration helps substantially}, and the preferred hyperparameters result in more aggressive exploration as well as larger optimization steps. This is particularly pronounced in Figure~\ref{fig:so-init-shift}, where the initial model is quite poor, \greedy gets a middling improvement on it while the exploration algorithms all reach significantly larger rewards. For Figure~\ref{fig:emnist-init-shift}, the preferred exploration parameters are comparitively less aggressive, and this is also reflected in a smaller edge over \greedy. Overall, this reinforces the intuition that some amount of persistent exploration is beneficial in dynamic, non-stationary environments. 

Given this evaluation across settings and algorithms, we are ready to present the first high-level takeaway from our experiments for practitioners:

\begin{specialistbox}{Takeway 1: Effectiveness of \softmax.} \emph{We find that the \softmax approach, while being a simple modification of the \greedy strategy, has a remarkably strong performance across benchmarks and experimental settings, always either performing the best or close to it. While \falcon performs comparably well, the fact that getting strong exploration performance requires tuning two unrelated hyperparameters is a serious practical drawback. Consequently, we recommend \softmax as an effective default strategy for practitioners.}
\end{specialistbox}

\paragraph{Variance across repeated trials.} All our algorithms are randomized due to the random sampling involved in exploration. The simulation itself has many random choices such as the choice of which clients participate in a training round and example selection in each mini-batch. The conclusions discussed so far are remarkably robust to this randomness, and we show the stability in our results for the recommended \softmax strategy in the \initshift setting in Figure~\ref{fig:var}. As we see, the variation in rewards across repeated trials is negligible.


\subsection{A Closer look at some choices in the algorithms and setup}
\label{sec:exp-choices}
We now take a deeper look into some of the choices both in our setup and the design and implementation of the algorithms which can lead to a significant change in the results, and hence are important to be aware of in practice. We start with a common practical question of the effect of model deployment frequency, corresponding to the number of model updates and training rounds that the algorithm faces.

\paragraph{Effect of deployment frequency.} So far, we have discussed results where new models are deployed once every 200 communication rounds. The choice of deployment frequency is itself a tunable parameter in practice, although very small frequencies are typically infeasible from system considerations, and often undesirable from a stability perspective. In Figures~\ref{fig:emnist-deploy40} and~\ref{fig:so-deploy40}, we investigate the performance of algorithms in the \initshift setting, when the deployment frequency is reduced to just 40 rounds. 
This means that we get a total of 20 training periods in \emnist and 40 periods in \so. The first observation is that the absolute performance of all the methods improves over the corresponding Figures~\ref{fig:emnist-init-shift} and~\ref{fig:so-init-shift} with a frequency of 200 in the same setting. This is not surprising as better models are deployed early with a smaller deployment frequency, giving a longer time to effectively exploit the gains from exploration. This confirms the intuition that smaller deployment frequencies are preferable from a learning perspective, as long as the rest of the system architecture allows it. 

Next we study the effect of varying some important elements in Algorithm~\ref{alg:fedcb_simulation}.

\paragraph{Effect of optimizer choice.} Algorithm~\ref{alg:fedcb_optimization} allows us to choose different client and server optimizers. We fix client optimizer to SGD throuhgout, but use ADAM~\citep{kingma2014adam} as the default choice for server optimizer, consistent with prior works on supervised federated learning~\citep{reddi2021adaptive}. We test the effectiveness of this choice by changing the server optimizer to SGD for \softmax in the \initshift setting in both \so and \emnist. While there is no change in the final performance at tuned hyperparameters for \emnist, the average bandit reward at the end of 1500 communication rounds drops from 0.81 to 0.62. This mirrors prior results in the supervised setting~\citep{reddi2021adaptive}, where ADAM is found to be superior for the \so task, due to the presence of sparse, high-dimensional features. 

\paragraph{Effect of importance sampling.} As we discuss in \cref{sec:cb_alg}, several prior works train an importance weighted regressor~\citep{bietti2021contextual} to form the underlying greedy policy in \egreedy, while we adopt an unweighted regression. This is due to the destabilizing effects of the variance from importance weighting on the learning process. Indeed, we find that changing the \egreedy approach to use weighted regression worsens the performance in the \initshift setting from 0.71 to 0.6 for \emnist and from 0.72 to 0.47 for \so. There is a wealth of literature on variance reduction techniques with importance weighting, such as the doubly robust methods~\citep{dudik2014doubly}. However, given the strong performance of unweighted methods here, we do not investigate these additional techniques due to added challenges with hyperparameters and learning complexity in practice. While the theoretical foundations of the unweighted approach here rely on an expressivity assumption on the underlying function class as we discuss in the next section, we find that this is less of a concern in modern systems with powerful, over-parameterized regression models.

\begin{specialistbox}{Takeaway 2: Importance of variance control.} \emph{Both the choice of ADAM versus SGD as server optimizer and the use or not of importance weights eventually control the variance in the training process, and crucially modulate the sample efficiency in our experiments. We find the choices of ADAM and regression-based loss to be effective across settings, and recommend them to practitioners.}
\end{specialistbox}

\paragraph{Choosing hyperparameters.} While hyperparameter choice is a process fraught with some overhead in all learning pipelines, it is particularly challenging in bandit settings, where each hyperparameter drives different data collection and hence tuning is not so straightforward. Unfortunately, we find that while the exploration parameters show remarkable stability for most approaches and regimes, the optimization learning rates are more sensitive. For \softmax, a temperature parameter of 0.05 performs the best in all regimes other than \initshift, where a slightly higher choice of 0.1 does somewhat better, though 0.05 is still reasonably good. Similarly $\epsilon=0.05$ works best in most cases for \egreedy. \falcon, in contrast, requires very different choices across datasets and settings, explaining our preference of \softmax over \falcon. For optimization parameters, we find that higher learning rates are preferred in \scratch and \initshift settings, while \init prefers smaller learning rates due to the high-quality initial model. Since practical setups typically use fairly large deployment frequencies, it is reasonable to pick the optimization hyperparameters through offline off-policy evaluation style approaches~\citep{dudik2014doubly} from the accumulated training data. See \cref{app:hyper} for hyperparameter tuning details.

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{EMNIST-tmp0p05-dp.pdf}
        \caption{\emnist \scratch with DP}
        \label{fig:emnist-dp}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{EMNIST-tmp0p05-dp-prob.pdf}
        \caption{Exploration-exploitation in \cref{fig:emnist-dp}
        }
        \label{fig:emnist-dp-prob}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{SO-tmp0p05-dp.pdf}
        \caption{\so \scratch with DP}
        \label{fig:so-dp}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{SO-tmp0p05-dp-prob.pdf}
        \caption{Exploration-exploitation in \cref{fig:so-dp}}
        \label{fig:so-dp-prob}
    \end{subfigure}
    \caption{Differential privacy for \softmax variations in the \scratch setting. Hyperparameters are detailed in \cref{tab:dp-hyper}}
    \label{fig:dp}
\end{figure}

\subsection{Incorporating differential privacy.} \label{sec:dp}
We provide preliminary results on adding differential privacy to the federated CB experiments by applying DP-FedAvg~\citep{mcmahan18learning} in \cref{alg:fedcb_optimization}, as discussed in \cref{sec:alg}. We consider the \scratch setting in \cref{fig:dp}, but same approach can be applied in the \init and \initshift settings after accounting the privacy budget for pretraining or pretraining on public data. We follow the strategy in \citet{xu2022learning} to tune the hyperparameters: we first estimate an (aggressive) clip norm with adaptive clipping~\citep{andrew2019differentially} of target quantile 0.5 and noise multiplier 0, and a small grid of learning rates around the best learning rates tuned in no DP settings; we fix the clip norm to 0.1 for \emnist and 0.8 for \solong and then choose a small and large noise multiplier respectively for \emnist and \so; we further tune the learning rates in a small grid based on the learning rates chosen for adaptive clipping experiments, and select the best hyperparameters based on the final (averaged) reward. 

\cref{fig:dp} compares four approaches:
\begin{itemize}
    \item \textbf{No DP} shows vanilla FedAvg with adaptive clipping of to a large target quantile (0.8) that clips rarely, without noise.
    \item \textbf{NM=0} uses a fixed clipping norm with no added noise.
    \item \textbf{NM=0.01 or 0.3}, small noise multipliers for \emnist and \so respectively.
    \item \textbf{NM=0.1 or 0.7}, corresponding large noise multipliers. 
\end{itemize}
The large noise multiplier will conceptually result in stronger privacy guarantees, however, for the small \emnist dataset of 3400 clients, even NM=0.1 is not large enough to achieve meaningful formal DP guarantees.  
%\mcm{We could give numbers for scaling up the clients/round and dataset size by 10 or 20x to show that under realistic settings, these could give meaningful privacy guarnatees.} \zx{Added below, to also address the other comments.} 
For \so of $\sim$0.34M clients, when assuming Poisson sampling and add-or-remove-one adjacency, we use RDP~\citep{abadi2016deep,mironov2017renyi} accounting to compute  privacy guarantees measured by $(\epsilon, \delta)$-DP. Fixing $\delta=10^{-6}$, the noise multipliers 0.3 and 0.7 can lead to $\epsilon=$ 15.8 and 1.5 respectively. 

\cref{fig:emnist-dp} (\emnist) and \cref{fig:so-dp} (\so) show the running average reward of these approaches, and suggest that clipping alone does not necessarily degrade the model utility measured by reward, and noise multiplier controls the privacy-utility trade-off.
The observations of the DP effect in bandits settings are similar to the previous observation in supervised settings~\citep{andrew2019differentially,kairouz21practical}. The preliminary DP results are provided to show the proposed federated bandit algorithms are indeed compatible with differential privacy. There are many potential tuning strategies to achieve stronger privacy-utility trade-offs \citep{ponomareva2023dp}. A particular useful tuning strategy for DP is to sample large number of clients per round. Following \citep{mcmahan18learning,kairouz21practical,xu2022learning}, we can extrapolate the privacy and utility in a more realistic setting by assuming larger number of total clients, and linearly increasing noise multiplier and clients per round. \Cref{fig:emnist-dp} shows that \textbf{NM=0.01} can achieve strong utility. When $\sim$ 0.34M total clients can participate in training, and scaling up NM from 0.01 to 1, RDP accounting can achieve $(\epsilon=4.13, \delta=10^{-6})$-DP. If we also linearly scale up clients per round from 64 to 6400, the utility measured by reward is expected to be similar to the strong utility of \textbf{NM=0.01} in \cref{fig:emnist-dp}.

In  \cref{fig:emnist-dp-prob,fig:so-dp-prob}, we further report the probability of the chosen action ($p_j(a^j)$ in \cref{alg:fedcb_inference}) averaged for data of the sampled clients in each round, which is an indicator of the exploration-exploitation trade-off of the \softmax algorithm. The \softmax algorithm has an interesting annealing effect of exploration: the probability of chosen actions gradually increase as the models become more confident in their predictions after training. DP seems to have a larger effect on the probability at the early stage of training for \so, while the effect happens at later stage for \emnist. The relationship of randomness in bandits exploration and the noise addition of DP can be an interesting topic for future study.   

\section{A theoretical model}

We now present a simple theoretical model to understand some of the key considerations in federated CB learning. Using the same high-level setup as \cref{sec:fedcb-setting}, we abstract the inference and training periods as described below.

\paragraph{Inference:} At inference period $i$, each client $c$ simultaneously uses the currently available model $\pi_i$ to choose actions for any contexts $x\sim D_c$ that it observes, and logs $(x,a,\rscalar,\pi_i(a|x))$, with $a\sim \pi_i(\cdot|x)$ and $\rscalar = r(x,a)$ for $(x,r)\sim D_c$.

\paragraph{Training:} At each training period $i=1,2,\ldots$, the server updates the model using a total of $n$ new training log entries for this training period, distributed across the clients participating in the training period. To abstract away the specifics of client sampling and its effects, we consider the $n$ samples to be i.i.d. according to the choice of a client $c\sim \cdist$ and $(x,r)\sim D_c$.

We make an additional assumption on the problem setup which leads to computationally nicer algorithms. Concretely, we assume that our CB algorithm models the rewards, and has access to a function class $\cF \subseteq \{\cX\times\cA\to [0,1]\}$, so that each $f\in\cF$ predicts rewards, given a context, action pair as the input. To obtain theoretical justification for the use of such a parameterization, centralized CB algorithms make the so-called \emph{realizability assumption} that for some $f^\star \in \cF$, $\E[r | x,a] = f^\star(x,a)$ for all $x,a$. However, in the federated setting, we have heterogeneous data distributions across clients. Nevertheless, we use a common set of parameters to predict the rewards at each client, which motivates the following realizability assumption in the federated setting.

\begin{assumption}[Realizability in Federated CBs]
There exists $f^\star\in\cF$ such that $\E_{D_c}[r | x,a] = f^\star(x,a)$ for all $x\in\cX, a\in\cA$ and $c\in\clients$.
\label{ass:realizable}
\end{assumption}

Importantly, this assumption does not contradict the substantial heterogeneity in client preferences that may naturally arise in federated settings, as such heterogeneity can be modeled via appropriate distributions $D_c$, allowing a single $f^\star$ to effectively behave arbitrarily differently on different clients (e.g., in the extreme case where the support of the clients $D_c$ is non-overlapping).

Under the realizability assumption, it is natural to learn the regression function using the unweighted regression objective~\eqref{eq:regression-unweight}.
To abstract away the details of the underlying FL algorithms, we assume access to a federated regression oracle which can optimize such objectives, formally:

\begin{definition}[Federated Regression Oracle]
    Given clients $c_1,\ldots,c_m$ with local datasets $S^c_1,S^c_2,\ldots,S^c_m$ satisfying $|S^c_1 \cup S^c_2 \cup S^c_m| = n$ , a federated regression oracle returns a function $\hat f$, using a federated learning protocol, which satisfies:
    \begin{equation*}
        \frac{1}{n}\sum_{i=1}^m \sum_{(x,a,\rscalar)\in S^c_m} (\hat f(x,a) - \rscalar)^2 \leq \frac{1}{n}\min_{f\in\cF} \sum_{i=1}^m \sum_{(x,a,\rscalar)\in S^c_m} ( f(x,a) - r)^2 + \epsfed. 
    \end{equation*}
    \label{def:fedreg}
\end{definition}

The parameter $\epsfed$ captures the accuracy of solving the regression problem over $n$ examples distributed over $m$ clients in a federated manner, and will in general depend on the choice of the federated learning method, settings of hyperparameters such as communication rounds, etc. We assume that the clients $c_1,\ldots,c_m$ are chosen i.i.d. from the underlying distribution $\cdist$, and that the effective training set for the regression problem $S^c_1 \cup S^c_2 \cup S^c_m$ (which is never explicitly materialized in one place) is of a fixed size $n$,  with samples i.i.d. from the ideal sampling distribution $c\sim p$ and $(x,r)\sim D_c$. 

\paragraph{Federated inference regret of \egreedy}
With this background, it is straightforward to analyze a simple regression-based \egreedy method for the federated setting. Let $\hat f_{i+1}$ be the regressor computed at the training period $i$. Furthermore, for any $f\in\cF$, let $\pi_f(x) = \argmax_a f(x,a)$ denote the greedy policy, with ties broken in an arbitrary manner, and let $\pi_i(x) = (1-\epsilon)\pi_{f_i}(x) + \epsilon \unif(\cA)$ denote the inference policy deployed at inference period $i$ and $\pi^\star = \pi_{f^\star}$ denote the optimal policy. Since $f, r$ are both bounded in $[0,1]$ and we use $n$ fresh training samples at each training period $i$ to have a total of $ni$ samples after $i$ periods, it can be show that (see e.g.~\citep{agarwal2012contextual}) with probability at least $1-\delta$, the following generalization bound for the regression performance of $\hat{f}_{u+1}$ holds:
\begin{equation}\label{eq:genregression}
    \frac{1}{i}\sum_{j=1}^i\E_j \left[(\hat f_{i+1}(x,a) - r)^2 - (f^\star(x,a) - r)^2\right] = \order\left(\frac{\ln(|\cF|/\delta)}{ni} + \epsfed\right). 
\end{equation}

Here we use $\E_j$ as a shorthand to denote expectation over random variables $c\sim p, x,r\sim D$ and $a\sim \pi_j(\cdot | x)$. We also assume that the class $\cF$ is finite for our analysis here for convenience. Using standard arguments, a similar result can also be obtained for infinite function classes through the use of covering. Under Assumption~\ref{ass:realizable}, the proof of Lemma 4.3 of~\citet{agarwal2012contextual} further implies that 
\begin{align}
    \E_{c\sim p} \E_{(x,r)\sim D_c} & \left[r(\pi^\star(x)) - r(\pi_{\hat{f}_{i+1}}(x))\right] \leq \sqrt{\E_{c\sim p} \E_{(x,r)\sim D_c} \left[\big(r(\pi^\star(x)) - r(\pi_{\hat{f}_{i+1}}(x))\big)^2\right]} \nonumber\\ 
    \leq& \sqrt{\frac{2K}{\epsilon} \frac{1}{i}\sum_{j=1}^i\E_j \left[(\hat f_{i+1}(x,a) - r)^2 - (f^\star(x,a) - r)^2\right]} \nonumber\\ 
    = & \order\left(\sqrt{\frac{2K}\epsilon\bigg(\frac{\ln(|\cF|i/\delta)}{ni} + \epsfed\bigg)}\right),
    \label{eq:epsg-realizable-reg}
\end{align}
%
where the first inequality follows from Jensen's inequality, the second inequality uses Lemma 4.3 of \citet{agarwal2012contextual}, and in the last step we use \cref{eq:genregression}. Since our actual inference policy $\pi_{i+1}$ is $\epsilon$-greedy, the per-round inference regret after $I$ training rounds is at most 
\begin{equation*}
    \order\left(\epsilon + \sqrt{\frac{2K}\epsilon\bigg(\frac{\ln(|\cF|I/\delta)}{nI} + \epsfed\bigg)}\right).
\end{equation*}

To better contrast this result with standard CB guarantees in the centralized setting, we make a simplifying assumption that we have only 1 client in the pool, and that the number of samples per inference period is the same as the size of our training pool for each period, equal to $n$. Then the cumulative inference regret after $I$ periods is at most
\begin{equation}
    \left(\epsilon + \sqrt{\frac{2K}{\epsilon}\epsfed}\right) nI + n + \sum_{i=2}^I \sqrt{\frac{2K}\epsilon\cdot\frac{n\ln(|\cF|I/\delta)}{(i-1)}}.
    \label{eq:inf-fed}
\end{equation} 

In comparison, under the same assumptions, updating the regressor after each inference round yields a regret of at most 
\begin{equation}
    \left(\epsilon + \sqrt{\frac{2K}{\epsilon}\epsilon_{\mathrm{opt}}}\right) nI + 1 + \sum_{j=2}^{nI} \sqrt{\frac{2K}\epsilon\cdot\frac{\ln(|\cF|nI/\delta)}{j-1}},
    \label{eq:inf-cent}
\end{equation}
%
where $\epsilon_{\mathrm{opt}}$ is the accuracy of the centralized regression oracle. Assuming that the two optimization errors are of a comparable order, then the main difference in the two bounds arises due to the delay of roughly one inference period in the model updates in the federated setting. Clearly the gap is at most of a constant factor and decreases over time, which is consistent with prior results on delayed bandit learning. As we have already observed in the empirical evaluation, however, when the number of inference and training periods, given by $I$ above, is relatively small, then this delay has a non-trivial effect on the performance (see e.g. the effect of deployment frequency in \cref{sec:exp-choices}). An extreme case of this can be observed by setting $I = 1$, whence the bound in~\eqref{eq:inf-fed} becomes vacuously large in the final term, while that in \eqref{eq:inf-cent} still decreases as $\widetilde{\order}(1/\sqrt{n})$ in the final term.

Note that our calculations above assume that our regression solution $\hat f_i$ fits all the training data accumulated over prior training periods $1,2,\ldots,i-1$. In practice, depending on the implementation details, it might only incorporate the data from the most recent, or roughly a constant number of past training periods, but where the optimizer is warm-started from the previous solution. As long as the optimizer does provide guarantees of approximately fitting the entire data through the warm-start however, our conclusions continue to hold in this setting.

\paragraph{Federated inference regret of \falcon}. While our analysis of the $\epsilon$-greedy approach above serves to illustrate most of the key ideas and modifications in the federated setting from a centralized one, it has the drawback of a weak overall regret bound due to the simplistic uniform exploration. In the centralized setting, recent algorithms~\citep{foster2020beyond,simchi2022bypassing} have leveraged Assumption~\ref{ass:realizable} to give statistically optimal CB results, and can be computationally implemented using regression oracles. For the federated setting, the \falcon algorithm of~\citet{simchi2022bypassing} is particularly attractive, since it takes an offline squared loss regression oracle as an input, which can be instantiated with a federated regression oracle in the federated setting. This combination allows us to get a per-round inference regret after $I$ training rounds of 

\begin{equation*}
    \order\left(\sqrt{K\bigg(\frac{\ln(|\cF|I/\delta)}{nI} + \epsfed\bigg)}\right),
\end{equation*}
%
which removes the undesirable scaling of $\order(1/\epsilon)$ on a fixed exploration parameter through a more adaptive exploration-exploitation tradeoff. The effect of delays and other aspects of the comparison with the centralized setting remain unchanged.

\section{Conclusion}

This paper aims to provide a practical perspective on the important problem of federated contextual bandits, with a goal of both highlighting the relevance of this paradigm to real-world applications, and to demonstrate the effectiveness of simple strategies when instantiated with the right choices. An additional goal and contribution of this work is to develop a robust simulation methodology for the federated CB setting, which incorporates practical concerns such as leveraging small amounts of pre-training data, potentially mis-aligned with the eventual performance metrics, as well as non-stationarity and distributional shifts. Indeed some of these factors are rarely incorporated even in the most comprehensive centralized CB evaluation, and are of independent interest to the bandit community. For practitioners, we hope that the takeaways from our simulations on which algorithmic choices work well can be a useful guide to applying these ideas.

More generally, as we see an ever increasing focus on personalization and fine-tuning of large, general purpose models with RL, the availability of technologies such as federated CB and more general forms of federated RL are essential to our ability to learn in a private and responsible manner. Extending these ideas to more general forms of RL is an important direction for future work, as is a deeper understanding of the interplay between privacy and the RL setting.

\ifthenelse{\boolean{tmlr}}{}{
\subsubsection*{Acknowledgement}
The authors thank Zachary Garrett and the Tensorflow Federated Team for technical support on implementation. 
}

\bibliographystyle{tmlr}
\bibliography{refs}

\appendix

\section{Detailed Hyperparameter Settings}
\label{app:hyper}

We now give the detailed hyperparameter settings for the different simulation scenarios and algorithms. \ifthenelse{\boolean{tmlr}}{}{Example command lines to reproduce our experiments can be found along with our open source code at \url{https://github.com/google-research/federated/tree/master/bandits}. }

Where we discuss choosing hyperparameters from a grid, unless otherwise noted we ran all combinations of the hyperparameters for each (scenario $\times$ algorithm $\times$ dataset) configuration, and report the runs which achieved the best running average reward at the end (last round) of training. As described in \cref{alg:fedcb_simulation}, the same set of clients are used for bandit inference and federated training. For EMNIST, a client may be revisited $64 \times  800 / 3400\sim15$ times while for \so, as the dataset is large, it would be rare to revisit the same client twice.  

\subsection{Settings for the exploration parameters}
\label{app:hyper-exp}

We begin with \egreedy and \softmax, which use fixed hyperparameters across all simulations. The preferred choices which result in the highest CB reward at the end of the experiment are indicated in bold.
\begin{itemize}
    \item $\epsilon$ for \egreedy: $\epsilon \in \{\textbf{0.05}, 0.1\}$.
    \item $\beta$ for \softmax: $\beta \in \{0.02, \textbf{0.05}, 0.1\}$.
\end{itemize}

For the \falcon algorithm, we found setting the two hyperparameters of $\gamma$ and $\mu$ to be significantly more challenging. To have a standardized way of setting these across both the datasets, we first chose $\mu \in \{1, 0.1, 0.01\}K/\epsilon$ with $\epsilon = 0.05$, so that the contribution of this term is in various multiples of our preferred parameter in \egreedy. Since the number of actions is different in the two cases, this results in $\mu \in \{12, 124, 1240\}$ and $\mu \in \{10, 100, 1000\}$ for \emnist and \so respectively.  For $\gamma$, we further tune it in the set $\gamma \in \{1000, 5000\}$, which we found to be reasonable for both the datasets. 

\subsection{Settings for the optimization hyperparameters}
\label{app:hyper-opt}

Next we describe the optimization hyperparameters which are more sensitive to the dataset and the simulation setting used. We always choose learning rates from a grid of the form $\{1,2,5\}*10^{-n}$, where $n$ is chosen appropriately for each setting. We used a fixed grid across algorithms and scenarios for each dataset, and when the best settings fell on the edge for an algorithm in a setting, we ran additional runs to confirm that expanding the grid does not improve the results. We start with the parameters for \emnist.

\begin{itemize}
    \item learning rate for client optimizer (SGD) $\in \{0.01,0.02,0.05,0.1,0.2,0.5\}$.
    \item learning rate for server optimizer (ADAM) $\in \{0.0005, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05\}$.
\end{itemize}
%
The corresponding settings for \so are:
\begin{itemize}
    \item learning rate for client optimizer (SGD) $\in \{0.02,0.05,0.1,0.2,0.5,1,2,5\}$.
    \item learning rate for server optimizer (ADAM) $\in \{0.0002, 0.0005, 0.001,0.002,0.005,0.01,0.02,0.05\}$
\end{itemize}

We use the default values in Keras for the remaining ADAM hyperparameters such as $\beta_1, \beta_2$ and $\epsilon$. The large grids for the server optimizer are primarily because the \init setting prefers a much smaller learning rate at the server than the other settings.

We fix other federated optimization parameters in all experiments: each client run one epoch on their local logged data for training; minibatch size of 16 is used on clients; 64 clients are sampled per round; the maximum number of samples per client on \so is capped at 256. 

We conclude this section by giving tables of learning rate settings for each of the plots in Figures~\ref{fig:emnist-algs-comp},~\ref{fig:so-algs-comp} and~\ref{fig:dp}.

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
         Algorithm &  Server learning rate & Client learning rate & Exploration param\\ \hline
         \softmax & 0.002 & 0.1 & $\beta = 0.05$ \\ \hline
         \falcon & 0.002 & 0.1 & $\mu = 12, \gamma = 1000$\\ \hline
         \greedy & 0.001 & 0.2 & $\cdot$ \\ \hline
         \egreedy & 0.01 & 0.1 & $\epsilon = 0.05$\\ \hline
    \end{tabular}
    \caption{Hyperparameter settings for the \emnist dataset and the \scratch scenario (\cref{fig:emnist-scratch})}
    \label{tab:emnist-scratch}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|} 
        \hline
         Algorithm &  Server learning rate & Client learning rate & Exploration param\\ \hline
         Init (supervised) & 0.5 & 0.5 & $\cdot$ \\ \hline
         \softmax & 0.005 & 0.1 & $\beta = 0.05$ \\ \hline
         \falcon & 0.002 & 0.2 & $\mu = 12, \gamma = 5000$\\ \hline
         \greedy & 0.002 & 0.1 & $\cdot$ \\ \hline
         \egreedy & 0.002 & 0.1 & $\epsilon = 0.05$\\ \hline
    \end{tabular}
    \caption{Hyperparameter settings for the \emnist dataset and the \init scenario (\cref{fig:emnist-init})}
    \label{tab:emnist-init}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
         Algorithm &  Server learning rate & Client learning rate & Exploration param\\ \hline
         Init (supervised) & 0.5 & 0.5 & $\cdot$ \\ \hline
         \softmax & 0.005 & 0.1 & $\beta = 0.05$ \\ \hline
         \falcon & 0.005 & 0.2 & $\mu = 12, \gamma = 5000$\\ \hline
         \greedy & 0.001 & 0.1 & $\cdot$ \\ \hline
         \egreedy & 0.01 & 0.1 & $\epsilon = 0.05$\\ \hline
    \end{tabular}
    \caption{Hyperparameter settings for the \emnist dataset and the \initshift scenario (\cref{fig:emnist-init-shift})}
    \label{tab:emnist-init-shift}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
         Algorithm &  Server learning rate & Client learning rate & Exploration param\\ \hline
         Init (supervised) & 0.5 & 0.5 & $\cdot$ \\ \hline
         \softmax & 0.005 & 0.2 & $\beta = 0.05$ \\ \hline
         \falcon & 0.005 & 0.1 & $\mu = 12, \gamma = 5000$\\ \hline
         \greedy & 0.002 & 0.1 & $\cdot$ \\ \hline
         \egreedy & 0.005 & 0.2 & $\epsilon = 0.05$\\ \hline
    \end{tabular}
    \caption{Hyperparameter settings for the \emnist dataset and the \initshift scenario with deploy\_freq = 40 (\cref{fig:emnist-deploy40})}
    \label{tab:emnist-deploy40}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
         Algorithm &  Server learning rate & Client learning rate & Exploration param\\ \hline
         \softmax & 0.01 & 1 & $\beta = 0.05$ \\ \hline
         \falcon & 0.01 & 0.05 & $\mu = 10, \gamma = 5000$\\ \hline
         \greedy & 0.01 & 0.1 & $\cdot$ \\ \hline
         \egreedy & 0.01 & 0.2 & $\epsilon = 0.05$\\ \hline
    \end{tabular}
    \caption{Hyperparameter settings for the \so dataset and the \scratch scenario (\cref{fig:so-scratch})}
    \label{tab:so-scratch}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|} 
        \hline
         Algorithm &  Server learning rate & Client learning rate & Exploration param\\ \hline
         Init (supervised) & 0.05 & 0.2 & $\cdot$ \\ \hline
         \softmax & 0.005 & 0.05 & $\beta = 0.05$ \\ \hline
         \falcon & 0.0005 & 0.1 & $\mu = 10, \gamma = 5000$\\ \hline
         \greedy & 0.001 & 0.02 & $\cdot$ \\ \hline
         \egreedy & 0.005 & 0.1 & $\epsilon = 0.05$\\ \hline
    \end{tabular}
    \caption{Hyperparameter settings for the \so dataset and the \init scenario (\cref{fig:so-init})}
    \label{tab:so-init}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
         Algorithm &  Server learning rate & Client learning rate & Exploration param\\ \hline
         Init (supervised) & 0.05 & 0.05 & $\cdot$ \\ \hline
         \softmax & 0.02 & 2 & $\beta = 0.1$ \\ \hline
         \falcon & 0.05 & 0.2 & $\mu = 100, \gamma = 1000$\\ \hline
         \greedy & 0.05 & 1 & $\cdot$ \\ \hline
         \egreedy & 0.05 & 0.2 & $\epsilon = 0.05$\\ \hline
    \end{tabular}
    \caption{Hyperparameter settings for the \so dataset and the \initshift scenario (\cref{fig:so-init-shift})}
    \label{tab:so-init-shift}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
         Algorithm &  Server learning rate & Client learning rate & Exploration param\\ \hline
         Init (supervised) & 0.05 & 0.05 & $\cdot$ \\ \hline
         \softmax & 0.05 & 1 & $\beta = 0.1$ \\ \hline
         \falcon & 0.05 & 0.05 & $\mu = 10, \gamma = 1000$\\ \hline
         \greedy & 0.05 & 0.1 & $\cdot$ \\ \hline
         \egreedy & 0.05 & 0.05 & $\epsilon = 0.05$\\ \hline
    \end{tabular}
    \caption{Hyperparameter settings for the \so dataset and the \initshift scenario with deploy\_freq = 40 (\cref{fig:so-deploy40})}
    \label{tab:so-deploy40}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         Dataset & Clip Norm & Noise Multiplier &  Server learning rate & Client learning rate \\ \hline
          \multirow{3}{*}{\emnist} &  \multirow{3}{*}{0.1} & 0 & 0.002 & 0.2 \\ \cline{3-5}
        &  &  0.01 & 0.002 & 0.1 \\ \cline{3-5}
         &  & 0.1 & 0.002 & 0.2 \\ \hline
         \multirow{3}{*}{\so} &  \multirow{3}{*}{0.8} & 0 & 0.02 & 0.5 \\ \cline{3-5}
          &  & 0.3 & 0.01 & 2 \\ \cline{3-5}
          & & 0.7 & 0.01 & 2\\ \hline
    \end{tabular}
    \caption{Hyperparameter settings for the DP experiments using \softmax $\beta=0.05$ in the \scratch scenario (\cref{fig:dp}).}
    \label{tab:dp-hyper}
\end{table}
\end{document}



