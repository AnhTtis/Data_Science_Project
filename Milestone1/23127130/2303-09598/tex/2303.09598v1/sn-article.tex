%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-chicago]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>
%%%%
\usepackage{natbib}
%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\jyear{2022}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

%% New comments below
\newcommand{\vecY}{{\mathbf{Y}}}
\newcommand{\bphi}{\mbox{\boldmath${\phi}$}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\I}{ {\rm{I}}}
\newcommand{\addeq}{ \overset{\textrm{\tiny{+}}}{\approx}}
\newcommand{\EZi}{\E_{q^{*}(Z_i)}\big(\I(Z_i=k)\big)}
\newcommand{\vecX}{{\mathbf{X}}}

%% New definitions below
\def\negr#1{\mbox{\boldmath$#1$}}
\def\E{{\mathbb E}} 


%% Some additional packages
\usepackage{graphicx} 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}


\begin{document}

\title[Variational Bayes Inference of Survival Data using Log-logistic AFT Model]{Variational Bayes Inference of Survival Data using Log-logistic Accelerated Failure Time Model}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Chengqian} \sur{Xian}}\email{cxian3@uwo.ca}

\author[1]{\fnm{Camila} \sur{P. E. de Souza}}\email{camila.souza@uwo.ca}

\author[1]{\fnm{Wenqing} \sur{He}}\email{whe23@uwo.ca}

\author[1,2]{\fnm{Felipe} \sur{F. Rodrigues}}\email{frodrig7@uwo.ca}

\author[2]{\fnm{Renfang} \sur{Tian}}\email{rtian2@uwo.ca}

\affil*[1]{\orgdiv{Department of Statistical and Actuarial Sciences}, \orgname{University of Western Ontario}, \orgaddress{\street{1151 Richmond Street}, \city{London}, \postcode{N6A 5B7}, \state{Ontario}, \country{Canada}}}

\affil[2]{\orgdiv{School of Management, Economics, and Mathematics}, \orgname{King's University College
at Western University}, \orgaddress{\street{266 Epworth Avenue}, \city{London}, \postcode{N6A 2M3}, \state{Ontario}, \country{Canada}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{The log-logistic regression model is one of the most commonly used accelerated failure time (AFT) models in survival analysis, for which statistical inference methods are mainly established under the frequentist framework. Recently, Bayesian inference for log-logistic AFT models using Markov chain Monte Carlo (MCMC) techniques has also been widely developed. In this work, we develop an alternative approach to MCMC methods and infer the parameters of the log-logistic AFT model via a mean-field variational Bayes (VB) algorithm. A piece-wise approximation technique is embedded in deriving the update equations in the VB algorithm to achieve conjugacy. The proposed VB algorithm is evaluated and compared with typical frequentist inferences using simulated data under various scenarios, and a publicly available dataset is employed for illustration. We demonstrate that the proposed VB algorithm can achieve good estimation accuracy and is not sensitive to sample sizes, censoring rates, and prior information. }

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Survival analysis, Accelerated failure time, Bayesian variational inference, Right censoring}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}

As an alternative to Cox proportional hazards (Cox PH) model, the accelerated failure time (AFT) model has been widely utilized in survival analysis recently \citep{Webber_2022, Longo_2022, Xu_2022} due to its intuitive interpretation\citep{Wei_1992}. Estimation of parameters and inference under an AFT model are usually likelihood-based  under a frequentist framework \citep{Kalbfleisch_2002, Lawless_2003}. However, recent developments in computing have made Bayesian estimation and inference for an AFT model an attractive alternative to likelihood-based methods \citep{Joseph_2001}. Implementations of the AFT model under the framework of Bayesian survival analysis can be found in different scenarios; see, for example, \citet{Lambert_2004, Lesaffre_2008, Zhang_2011} and \citet{Tang_2022}. As for the distributions considered in the parametric AFT model, common choices include log-logistic, Weibull, log-normal, and Gamma. The log-logistic distribution, exhibiting a non-monotonic hazard function, is more commonly used in survival analysis when the hazard function presents an inverse U-shape. Empirical analyses in various applications show that log-logistic distribution is well-suited to model survival data \citep{Patel_2006, Weng_2014, Thiruvengadam_2021, Rivas_2022}. 

Variational inference (VI), a method developed from machine learning, is used to approximate the posterior distribution of a Bayesian model via optimization \citep{Jordan_1999,Bishop_2006}. \citet{David_2017} presented a comprehensive review of VI from a statistical perspective. As an alternative to Markov Chain Monte Carlo (MCMC) algorithms in Bayesian analysis, the main advantage of VI is its much lower computational cost as shown in \citet{David_2017}. In addition, as a Bayesian approach, VI can make use of the prior information obtained from similar studies in the past, which are commonly available in survival analysis. Another advantage of VI is that it enables us to conduct inference for small sample size since it does not rely on asymptotics \citep{Joseph_2001}. However, asymptotic properties for VI methods may still be obtained in some scenarios. For example, \citet{Wang_2019} provided a study on the frequentist consistency of VI when the Kullbackâ€“Leibler (KL) minimizer \citep{Kullback_1951} of a normal distribution is considered. 

Variational Bayes (VB) is the name given for one type of variational inference method in which we consider the KL divergence as a criterion to measure the closeness between a possible approximated posterior density and the exact posterior density in the optimization. VB has been utilized in regression analysis to solve different statistical problems, such as parametric and
nonparametric regression with missing data \citep{Faes_2011}, nonparametric regression with measurement error \citep{Pham_2013}, count response semiparametric regression \citep{Luts_2015}, high-dimensional linear regression with sparse priors \citep{Ray_2022}, clustering of functional data \citep{xian2022clustering}.

In this paper, we consider the AFT survival model with survival times following a log-logistic distribution and being right censored. We take on a Bayesian approach and develop a VB algorithm to infer the model parameters. To the best of our knowledge, we are the first to build and investigate a VB approach for the AFT survival regression analysis. 

The remainder of the paper is organized as follows. Section \ref{sec.method} presents an overview of variational Bayes, our model setting and proposed algorithm. In Section \ref{Sec.sim}, we conduct simulation studies to assess the performance of our method under various scenarios. In Section \ref{Sec.real.data}, we apply our proposed method to a real dataset. A conclusion of our study and a discussion on the proposed method are provided in Section \ref{Sec.con.dis}.

\section{Methodology}\label{sec.method}

\subsection{Overview of Variational Bayes}
In a generic Bayesian model, we aim to obtain the posterior density of the parameters of interest to conductstatistical inference. Denote the parameter vector in the Bayesian model by $\negr{\theta} \in \Theta$ and the observed data $\vect Y$. We assume that both $\negr{\theta}$ and $\vect Y$ are continuous random vectors for simplicity and without loss of generality. Therefore, using the Bayes' theorem, the posterior density function can be obtained by 
\begin{equation}
p(\negr{\theta} \vert \vect Y)=\frac{p(\negr{\theta}, \vect Y)}{p(\vect Y)}.
\label{eq:Bayes_theorem}
\end{equation}
However, calculating the posterior density in (\ref{eq:Bayes_theorem}) might not be an easy task if there are many parameters and non-conjugate prior distributions. Therefore, one may change its goal to find an approximation to the posterior.

The idea of variational Bayes is that we find a variational density denoted by $q^*(\negr{\theta})$ coming from a family of possible densities $Q$ to approximate $p(\negr{\theta} \vert \vect Y)$, which can be solved in terms of an optimization problem with the Kullback-Leibler (KL) divergence as a minimization criterion. The KL divergence measures the closeness between the possible densities $q$ in the family $Q$ and the exact posterior density $p$. The optimization problem can be expressed as 
\begin{eqnarray}
 q^*(\negr{\theta}) = \underset{q \in Q}{\mathrm{argmin}}\,\mbox{KL}(q(\negr{\theta}) \Vert p(\negr{\theta} \vert \vect Y)).   
 \label{eq:KL}
\end{eqnarray}
\cite{Jordan_1999} and \citet{David_2017} show that minimizing the KL divergence is equivalent to maximizing the so-called evidence lower bound (ELBO). The KL divergence is defined as
$$\mbox{KL}(q(\negr{\theta}) \Vert p(\negr{\theta} \vert \vect Y)) := \int_\Theta q(\negr{\theta})\log\frac{q(\negr{\theta})}{p(\negr{\theta} \vert \vect Y)}d\negr{\theta},$$
and it can be shown that $$\int_\Theta q(\negr{\theta})\log\frac{q(\negr{\theta})}{p(\negr{\theta} \vert \vect Y)}d\negr{\theta} =\log p(\vect Y)-\int_\Theta q(\negr{\theta})\log\frac{p(\negr{\theta}, \vect Y)}{q(\negr{\theta})}d\negr{\theta}$$
where the last term is the ELBO. Since $\log p(\vect Y)$ is a constant with respect to $q(\negr{\theta})$, this changes the problem in (\ref{eq:KL}) to
\begin{eqnarray}
q^*(\negr{\theta}) = \underset{q \in Q}{\mathrm{argmax}}\,\mbox{ELBO}(q(\negr{\theta})).
\label{eq:maxELBO}
\end{eqnarray}

We, therefore, derive a VB algorithm for modelling right censored survival data via a log-logistic AFT regression model. In Sections \ref{sec.VB.algo} and \ref{sec:ELBO}, we consider the mean-field variational family and apply the coordinate ascent variational inference algorithm \citep{Bishop_2006} to solve the optimization problem in (\ref{eq:maxELBO}).

\subsection{Assumptions and model settings}
Let $T_i, i=1,2,...,n,$ be the well-defined time-to-event or survival time for the $i^{th}$ observation, and assume that it follows a log-logistic distribution. We consider the accelerated failure time (AFT) model
\begin{equation}
    \log(T_i):=Y_i=\vect{X}_i^T\negr{\beta} + bz_i 
\label{AFT.model}
\end{equation}
where $\vect{X}_i$ is the column vector with length $p, p \geq 2,$ containing $p-1$ fixed effects and a constant one to incorporate the intercept (i.e., $\vect{X}_i = (1, x_{i1}, ..., x_{i(p-1)})^T$) for the $i^{th}$ observation, $\negr{\beta}$ is the corresponding coefficients vector for the fixed effects, $z_i$ is a random variable following a standard logistic distribution, and $b$ is a scale parameter. 

For the standard logistic distribution, the survival function and density are
$$S_0(z)=\frac{1}{1+e^z} \quad \text{and}\quad f_0(z)=\frac{e^z}{(1+e^z)^2}, \;-\infty<z<\infty, \;\text{respectively.}$$

We consider a right censoring scheme for the survival data. Let $\delta_i,i=1,..., n,$ be the right censoring indicator for the $i^{th}$ observation, i.e., $\delta_i= 1$  if $T_i$ is observed, while $\delta_i= 0$ if $T_i$ is right censored. Then the log-likelihood for $\negr{\beta}$ and $b$ is
\begin{eqnarray}
l(\negr{\beta}, b)=-r\log b+\sum_{i=1}^{n}\big[\delta_i\log f_0(z_i)+(1-\delta_i)\log S_0(z_i)\big],
\label{eq:loglike}
\end{eqnarray}
where $r=\sum_{i=1}^n\delta_i$ which is the number of uncensored survival times, and $z_i=(Y_i-\vect{X}_i^T\negr{\beta})/b.$

We further assume the following prior distributions for the model parameters $\negr{\beta}$ and $b$.
\begin{itemize}
    \item $\negr{\beta} \sim N_p(\negr\mu_0, \sigma_0^2 I_{p\times p})$ with precision $v_0=1/\sigma_0^2$
    \item $b \sim \text{Inverse-Gamma}\,(\alpha_0, \omega_0)$
\end{itemize}

\subsection{Variational Bayes algorithm}\label{sec.VB.algo}
Our goal is to approximate the posterior joint distribution of $\beta$ and $b$ given the data $\vect{Y}$, $p(\negr{\beta}, b\,\vert\,\vect{Y})$,
by finding $q^*$ such that 
$$q^* = \mathrm{argmin}_{q\in Q_{MF}} \mbox{KL}(q\,\vert\vert\,p),$$
which is equivalent to maximizing the ELBO. We consider the mean-field variational family ($Q_{MF}$) in which the parameters in the variational distribution, $q(\negr{\beta}, b)$, are mutually independent, and each is governed by a distinct factor in the variational density. In our model, that is 
$$q(\negr{\beta}, b)=q(\negr{\beta})\, q(b),$$
which is called variational density (VD) factorization. Furthermore, the complete-data log-likelihood is
$$\log p(\vect{Y}, \negr{\beta}, b)=\log p(\vect{Y}\,\vert\,\negr{\beta},b)+\log p(\negr{\beta})+\log p(b),$$
where $\log p(\vect{Y}, \negr{\beta}, b)$ is as in (\ref{eq:loglike}).
By maximizing the ELBO, we have the following solutions \citep{Bishop_2006}:
$$\log q^*(\negr{\beta}) \addeq \E_{q(b)}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)+\log p(\negr{\beta})],$$
and
$$\log q^*(b) \addeq \E_{q(\negr{\beta})}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)+\log p(b)].$$

In what follows, we derive the update equation for each component in our model. We use $\addeq$ to denote equality up to a constant additive factor for convenience.

\subsubsection{VB update equations}\label{sub.sec.updates}
\noindent\textbf{(1) Update for $q^*(\negr{\beta})$}
\begin{eqnarray}
\log q^*(\negr{\beta}) &\addeq& \E_{q(b)}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)+\log p(\negr{\beta})] \nonumber \\
&=&\E_{q(b)}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)] + \E_{q(b)}[\log p(\negr{\beta})] := \large\textcircled{\small{1}} + \large\textcircled{\small{2}}\nonumber
\end{eqnarray}
For the first term, we have
\begin{eqnarray}
\large\textcircled{\small{1}} &=&  \E_{q(b)}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)] \nonumber \\
 &=& \E_{q(b)}\Bigg[-r\log b+\sum_{i=1}^n \Bigg(\delta_i \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \nonumber \\
 && -(1+\delta_i)\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T \negr{\beta}}{b}\big)\Big)\Bigg) \Bigg]\nonumber \\
 &=& -r\E_{q(b)}(\log b) + \sum_{i=1}^n \Bigg(\delta_i(Y_i-\vect{X}_i^T\negr{\beta})\E_{q(b)}\Big(\frac{1}{b}\Big) \nonumber \\
 && -(1+\delta_i)\E_{q(b)}\bigg[\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big)\bigg]\Bigg) 
 \label{update_beta_1}
\end{eqnarray}
To calculate the expectation in (\ref{update_beta_1}) and achieve conjugacy, we then propose and apply a quadratic piece-wise approximation of $\log(1+\exp(x))$ (see Equation (\ref{appro_2}) in Section \ref{sub.sec.piecewise}) to $\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big)$ obtaining:
\begin{eqnarray}
&&\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big) \nonumber\\
&\addeq& 0^{\nu_{i1}}\times 0.1696^{\nu_{i2}}\times 0.5^{\nu_{i3}}\times 0.8303^{\nu_{i4}}\times 1^{{1-\sum_{j=1}^4\nu_{ij}}}\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \nonumber \\
&& +\, 0^{\nu_{i1}}\times 0.0189^{\nu_{i2}}\times 0.1138^{\nu_{i3}}\times 0.0190^{\nu_{i4}}\times 0^{{1-\sum_{j=1}^4\nu_{ij}}}\Bigg(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\Bigg)^2, \nonumber
\end{eqnarray}
where
$$\nu_{i1}=\begin{cases} 1  &  \text{if}\; \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \leq -5\\ 0 &  \text{otherwise} \end{cases} ,\quad\nu_{i2}=\begin{cases} 1  &  \text{if}\; -5 < \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \leq -1.7\\ 0 &  \text{otherwise} \end{cases},$$
$$\nu_{i3}=\begin{cases} 1  &  \text{if}\; -1.7 < \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \leq 1.7\\ 0 &  \text{otherwise} \end{cases} \quad \text{and} \quad\nu_{i4}=\begin{cases} 1  &  \text{if}\; 1.7 < \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \leq 5\\ 0 &  \text{otherwise} \end{cases}.$$
Let $A_i:=0^{\nu_{i1}}\times 0.1696^{\nu_{i2}}\times 0.5^{\nu_{i3}}\times 0.8303^{\nu_{i4}}\times 1^{{1-\sum_{j=1}^4\nu_{ij}}}$ and $B_i:=0^{\nu_{i1}}\times 0.0189^{\nu_{i2}}\times 0.1138^{\nu_{i3}}\times 0.0190^{\nu_{i4}}\times 0^{{1-\sum_{j=1}^4\nu_{ij}}}$,
we obtain 
\begin{eqnarray}
\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big) \addeq A_i\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} + B_i \Bigg(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\Bigg)^2.\nonumber
\end{eqnarray}
More details about the proposed quadratic piece-wise approximation can be found in Section \ref{sub.sec.piecewise}. 

Therefore, we can write Equation (\ref{update_beta_1}) as
\begin{eqnarray}
\large\textcircled{\small{1}} &=&  \E_{q(b)}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)] \nonumber \\
 &\addeq& -r\E_{q(b)}[\log b] + \sum_{i=1}^n \Bigg(\delta_i(Y_i-\vect{X}_i^T\negr{\beta})\E_{q(b)}\big(\frac{1}{b}\big)\nonumber\\
 &&-\,(1+\delta_i)\E_{q(b)}\Big[A_i\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} + B_i \Big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\Big)^2\Big]\Bigg)  \nonumber \\
 &\addeq& \sum_{i=1}^n \Bigg(-\delta_i\vect{X}_i^T\negr{\beta}\,\E_{q(b)}\big(\frac{1}{b}\big)-(1+\delta_i)\Big(-A_i\vect{X}_i^T\negr{\beta}\,\E_{q(b)}\big(\frac{1}{b}\big)\nonumber\\
&&+\,B_i(-2Y_i\vect{X}_i^T\negr{\beta}+\negr{\beta}^T\vect{X}_i\vect{X}_i^T\negr{\beta})\E_{q(b)}\big(\frac{1}{b^2}\big)\Big)\Bigg)  \nonumber \\
 &=& \sum_{i=1}^n \Bigg(\E_{q(b)}\big(\frac{1}{b}\big)\Big(-\delta_i+(1+\delta_i)A_i\Big)\vect{X}_i^T+2\E_{q(b)}\big(\frac{1}{b^2}\big) (1+\delta_i)B_i Y_i\vect{X}_i^T\Bigg)\,\negr{\beta} \nonumber\\
 &&-\,\negr{\beta}^T \Bigg(\E_{q(b)}\big(\frac{1}{b^2}\big) \sum_{i=1}^n(1+\delta_i)B_i\vect{X}_i\vect{X}_i^T\Bigg)\,\negr{\beta} \nonumber
 \label{update_beta_1_new}
\end{eqnarray}
and note that
\begin{eqnarray}
\large\textcircled{\small{2}} &\addeq& \frac{p}{2}\log v_0-\frac{1}{2}v_0(\negr{\beta}-\negr{\mu}_0)^T(\negr{\beta}-\negr{\mu}_0) \nonumber \\
 &\addeq& -\frac{1}{2}v_0\,\big[\negr{\beta}^T\negr{\beta}-2\negr{\mu}_0^T\negr{\beta}\big] \nonumber \\
 &=& v_0\,\negr{\mu}_0^T\negr{\beta} -\frac{1}{2}v_0\negr{\beta}^T\negr{\beta} \nonumber
\end{eqnarray}
Combining $\large\textcircled{\small{1}}$ and $\large\textcircled{\small{2}}$, we have
\begin{eqnarray}
\log q^*(\beta) &\addeq& \Bigg[v_0\,\negr{\mu}_0^T + \sum_{i=1}^n \Bigg(\E_{q(b)}\Big(\frac{1}{b}\Big)\Big(-\delta_i+(1+\delta_i)A_i\Big)\vect{X}_i^T\nonumber\\
&& +\,2\E_{q(b)}\Big(\frac{1}{b^2}\Big) (1+\delta_i)B_i Y_i\vect{X}_i^T\Bigg)\Bigg]\,\negr{\beta} \nonumber \\
&& -\, \frac{1}{2}\negr{\beta}^T\,\bigg[v_0\textbf{I}+2\E_{q(b)}\Big(\frac{1}{b^2}\Big) \sum_{i=1}^n(1+\delta_i)B_i\vect{X}_i\vect{X}_i^T\bigg]\,\negr{\beta} .\nonumber
\end{eqnarray}
Let 
\begin{eqnarray}
\Sigma^*:= \bigg[v_0\textbf{I}+2\E_{q(b)}\Big(\frac{1}{b^2}\Big) \sum_{i=1}^n(1+\delta_i)B_i\vect{X}_i\vect{X}_i^T\bigg]^{-1},
\label{eq_update_sigma}
\end{eqnarray}
and 
\begin{eqnarray}
\negr{\mu}^*&:=&\Bigg[v_0\,\negr{\mu}_0^T + \sum_{i=1}^n \Bigg(\E_{q(b)}\Big(\frac{1}{b}\Big)\Big(-\delta_i+(1+\delta_i)A_i\Big)\vect{X}_i^T\nonumber\\
&&+\,2\E_{q(b)}\Big(\frac{1}{b^2}\Big) (1+\delta_i)B_i Y_i\vect{X}_i^T\Bigg)\Bigg]\,\Sigma^*.
\label{eq_update_mu}
\end{eqnarray}
Then, $q^*(\negr{\beta})$ is $N(\negr{\mu}^{*T}, \Sigma^*)$. Therefore, we have the conjugate multivariate normal posterior distribution of $\negr{\beta}$ after applying the piece-wise approximation to $\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big)$.

\vspace{0.5cm}
\noindent\textbf{(2) Update for $q^*(b)$}
\begin{eqnarray}
\log q^*(b) &\addeq& \E_{q(\negr{\beta})}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)+\log p(b)] \nonumber\\
&=& \E_{q(\negr{\beta})}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)]+\E_{q(\negr{\beta})}[\log p(b)]
:= \large\textcircled{\small{3}} + \large\textcircled{\small{4}}\nonumber
\end{eqnarray}
\begin{eqnarray}
\large\textcircled{\small{3}} &=&  \E_{q(\negr{\beta})}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)] \nonumber \\
 &=& \E_{q(\negr{\beta})}\Bigg[-r\log b+\sum_{i=1}^n \Bigg(\delta_i \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\nonumber\\
 &&-\,(1+\delta_i)\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big)\Bigg) \Bigg]\nonumber \\
 &=& -r\log b + \sum_{i=1}^n \E_{q(\negr{\beta})} \Bigg[\delta_i\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\nonumber\\
 &&-\,(1+\delta_i)\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big)\Bigg]
 \label{update_b_1}
\end{eqnarray}
We then propose and apply a linear piece-wise approximation of $\log(1+\exp(x))$ (see Equation (\ref{appro_1}) in Section \ref{sub.sec.piecewise}) to $\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big)$ obtaining:
\begin{eqnarray}
\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big) &\addeq& 0^{\eta_{i1}}\times 0.0426^{\eta_{i2}}\times 0.3052^{\eta_{i3}}\times 0.6950^{\eta_{i4}}\nonumber\\ && \times\, 0.9574^{\eta_{i5}}
 \times  1^{1-\sum_{j=1}^5\eta_{ij}}\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b},\nonumber
\end{eqnarray}
where
$$\eta_{i1}=\begin{cases} 1  &  \text{if}\; \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \leq -5\\ 0 &  \text{otherwise} \end{cases} ,\quad\eta_{i2}=\begin{cases} 1  &  \text{if}\; -5 < \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \leq -1.701\\ 0 &  \text{otherwise} \end{cases} ,$$
$$\quad\eta_{i3}=\begin{cases} 1  &  \text{if}\; -1.701 < \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \leq 0\\ 0 &  \text{otherwise} \end{cases}, \quad \eta_{i4}=\begin{cases} 1  &  \text{if}\; 0 < \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \leq 1.702\\ 0 &  \text{otherwise} \end{cases}$$
$\text{and} \quad \eta_{i5}=\begin{cases} 1  &  \text{if}\; 1.702 < \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b} \leq 5\\ 0 &  \text{otherwise} \end{cases}.$

Let 
\begin{eqnarray}
C_i&:=&0^{\eta_{i1}}\times 0.0426^{\eta_{i2}}\times 0.3052^{\eta_{i3}}\times 0.6950^{\eta_{i4}}\nonumber\\ && \times\, 0.9574^{\eta_{i5}}
 \times  1^{1-\sum_{j=1}^5\eta_{ij}},
 \label{eq:C_i}
\end{eqnarray}
we obtain 
\begin{eqnarray}
\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big) \addeq C_i\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}. \nonumber
\end{eqnarray}
More details about the proposed linear piece-wise approximation can be found in Section \ref{sub.sec.piecewise}. 

Therefore, we can write Equation (\ref{update_b_1}) as
\begin{eqnarray}
\large\textcircled{\small{3}} &=&  \E_{q(\negr{\beta})}[\log p(\vect{Y}\,\vert\,\negr{\beta},b)] \nonumber \\
 &\addeq& -r\log b + \sum_{i=1}^n \E_{q(\negr{\beta})} \Bigg[\delta_i\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}-(1+\delta_i)C_i\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\Bigg] \nonumber \\
 &\addeq& -r\log b + \frac{1}{b}\sum_{i=1}^n \Big(\delta_i-(1+\delta_i)C_i\Big)\Big(Y_i-\vect{X}_i^T\E_{q(\negr{\beta})}(\negr{\beta})\Big), \nonumber
 \label{update_b_1_new}
\end{eqnarray}
and note that
\begin{eqnarray}
\large\textcircled{\small{4}} &=&  \E_{q(\negr{\beta})}[\log p(b)] \nonumber \\
 &\addeq& -(\alpha_0+1)\log b -\frac{\omega_0}{b} \nonumber
\end{eqnarray}
Combining $\large\textcircled{\small{3}}$ and $\large\textcircled{\small{4}}$, we have
\begin{eqnarray}
\log q^*(b) &\addeq& -(\alpha_0+r+1)\log b \nonumber\\
&&-\, \frac{1}{b}\bigg(\omega_0-\sum_{i=1}^n \Big(\delta_i-(1+\delta_i)C_i\Big)\Big(Y_i-\vect{X}_i^T\E_{q(\negr{\beta})}(\negr{\beta})\Big)\bigg)\nonumber
\end{eqnarray}
Let 
\begin{eqnarray}
\alpha^*=\alpha_0+r,
\label{alpha_star}
\end{eqnarray}
and 
\begin{eqnarray}
\omega^*=\omega_0-\sum_{i=1}^n \Big(\delta_i-(1+\delta_i)C_i\Big)\Big(Y_i-\vect{X}_i^T\E_{q(\negr{\beta})}(\negr{\beta})\Big),
\label{omega_star}
\end{eqnarray}
then $q^*(b)$ is Inverse-Gamma$(\alpha^*, \omega^*)$.

\subsubsection{Expectations}\label{sub.sec.expect}
In what follows, we calculate the expectations in the update equations. All the expectations should be taken with respect to the approximated variational distributions. First, using the fact that $1/b$ follows a Gamma$(\alpha^*, \omega^*)$, we obtain
\begin{eqnarray}
\E_{q(b)}\Big(\frac{1}{b}\Big)=\frac{\alpha^*}{\omega^*}, 
\label{expect_b_inverse}
\end{eqnarray}
and
\begin{eqnarray}
\E_{q(b)}\Big( \frac{1}{b^2} \Big) &=& \E_{q(b)}\Big[\Big( \frac{1}{b} \Big)^2 \Big] \nonumber \\
&=&\text{Var}_{q(b)}\Big[(\frac{1}{b})\Big]+\Big[\E_{q(b)}(\frac{1}{b})\Big]^2 \nonumber  \\
&=&\frac{\alpha^*}{\omega^{*2}}+\frac{\alpha^{*2}}{\omega^{*2}}= \frac{\alpha^*+\alpha^{*2}}{\omega^{*2}}
\label{expect_b_square_inverse}
\end{eqnarray}
Besides,
\begin{eqnarray}
\E_{q(\negr{\beta})}\,(\negr{\beta})=\negr{\mu}^{*T}
\label{expect_beta}
\end{eqnarray}
with $\negr{\mu}^*$ defined as Equation (\ref{eq_update_mu}).

\subsection{ELBO calculation}\label{sec:ELBO}
Since our goal is to find $q(\cdot)$ that maximizes the ELBO, the ELBO is used as the convergence criterion of our VB algorithm, which is defined as follows:
\begin{eqnarray}
ELBO(q)=\E_{q}[\log p(\vect{Y}, \negr{\beta}, b)]-\E_{q}[\log q(\negr{\beta}, b)], \nonumber
\end{eqnarray}
where
$$\log p(\vect{Y}, \negr{\beta}, b)=\log p(\vect{Y}\,\vert\, \negr{\beta}, b)+\log p(\negr{\beta})+\log p(b),$$
and
$$\log q(\negr{\beta}, b)=\log q(\negr{\beta})+\log q(b).$$
Let 
$$\textit{diff}_{\negr{\beta}}=\E_{q}[\log p(\negr{\beta})]-\E_{q}[\log q(\negr{\beta})]$$
and
$$\textit{diff}_{b}=\E_{q}[\log p(b)]-\E_{q}[\log q(b)],$$
then we can write the ELBO as
\begin{eqnarray}
ELBO(q)=\E_{q}[\log p(\vect{Y}\,\vert\, \negr{\beta}, b)]+\textit{diff}_{\negr{\beta}}+\textit{diff}_{b}
\label{ELBO.cal}
\end{eqnarray}
We next present how to calculate each term in Equation (\ref{ELBO.cal}) with expectations taken with respect to the approximated variational distributions denoted by $q^*(\cdot)$. When calculating the first term, $\E_{q^*}[\log p(\vect{Y}\,\vert\, \negr{\beta}, b)]$, we apply the linear piece-wise approximation to $\log(1+\exp(x))$ again.
\begin{eqnarray}
&&\E_{q^*}[\log p(\vect{Y}\,\vert\, \negr{\beta}, b)] \nonumber\\&=& \E_{q^*}\Bigg[-r\log b+\sum_{i=1}^n \Bigg(\delta_i \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}-(1+\delta_i)\log\Big(1+\exp\big(\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\big)\Big)\Bigg) \Bigg]\nonumber \\
&\approx&  \E_{q^*}\Bigg[-r\log b+\sum_{i=1}^n \Bigg(\delta_i \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}-(1+\delta_i)C_i\frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\Bigg) \Bigg]\nonumber \\
 &\addeq& -r\E_{q^*(b)}\Big(\log b\Big)+\sum_{i=1}^n \Big[\delta_i -(1+\delta_i)C_i\Big]\E_{q^*(b)}\Bigg[\E_{q^*(\negr{\beta})} \frac{Y_i-\vect{X}_i^T\negr{\beta}}{b}\Bigg]\nonumber \\
  &=& -r\E_{q^*(b)}\Big(\log b\Big) +\E_{q^*(b)}\Big(\frac{1}{b}\Big)\sum_{i=1}^n \big(\delta_i -(1+\delta_i)C_i\big)\,\big(Y_i-\vect{X}_i^T\E_{q^*(\negr{\beta})}\,(\negr{\beta})\big),\nonumber
 \label{ELBO.term.1}
\end{eqnarray}
where $C_i$ is defined as Equation (\ref{eq:C_i}). Let $ \negr{\Psi}$ be the digamma function defined as
\begin{equation}
 \negr{\Psi}(x)=\frac{d}{dx}\log \Gamma(x), 
 \label{eq:digamma}
\end{equation}
\noindent which can be easily calculated via numerical approximation. Then $\E_{q^*(b)}\log b$ can be calculated by $\E_{q^*(b)}\log b=\log(\omega^*)-\negr{\Psi}(\alpha^*).$ For $\textit{diff}_{\negr{\beta}}$, we derive its calculation as follows, using the fact that $\E(\vect X^T \vect X) = \mbox{trace}[\mbox{Var}(\vect X)] + \E(\vect X)^T\E(\vect X)$ where $\vect X$ is a column vector:
\begin{eqnarray}
\textit{diff}_{\negr{\beta}} &=&\E_{q^*}[\log p(\negr{\beta})] - \E_{q^*}[\log q(\negr{\beta})] \nonumber \\
&\addeq& \E_{q^*}[-\frac{1}{2}v_0(\negr{\beta}-\negr{\mu}_0)^T(\negr{\beta}-\negr{\mu}_0)] \nonumber\\
&&- \,\E_{q^*}[-\frac{1}{2}\log (\vert\Sigma^*\vert)-\frac{1}{2}(\negr{\beta}-\negr{\mu}^*)^T(\Sigma^*)^{-1}(\negr{\beta}-\negr{\mu}^*)] \nonumber \\
&\addeq& -\frac{1}{2}v_0[\text{trace}(\Sigma^*)+(\negr{\mu}^*-\negr{\mu}_0)^T(\negr{\mu}^*-\negr{\mu}_0)] + \frac{1}{2}\log (\vert\Sigma^*\vert). \nonumber
\label{ELBO.term.2}
\end{eqnarray}
Note that
$$\E_{q^*}[\frac{1}{2}(\negr{\beta}-\negr{\mu}^*)^T(\Sigma^*)^{-1}(\negr{\beta}-\negr{\mu}^*)]=\frac{p}{2},$$
which is always a constant at each iteration and therefore we ignore it. For $\textit{diff}_{b}$, we have
\begin{eqnarray}
\textit{diff}_{b}&=&\E_{q^*}[\log p(b)]-\E_{q^*}[\log q(b)]\nonumber \\
&\addeq& \E_{q^*}\Big[-(\alpha_0+1)\log b-\frac{\omega_0}{b}\Big] \nonumber\\
&&-\, \E_{q^*}\Big[\alpha^*\log \omega^* - \log (\Gamma(\alpha^*))-(\alpha^*+1)\log b-\frac{\omega^*}{b}\Big] \nonumber \\
&=& (\alpha^*-\alpha_0)\E_{q^*(b)}(\log b) + (\omega^*-\omega_0)\E_{q^*(b)}\big(\frac{1}{b}\big) - \alpha^*\log \omega^*. \nonumber
\label{ELBO.term.3}
\end{eqnarray}
Since $\alpha^*$ does not change at each iteration, we remove $\log (\Gamma(\alpha^*))$ in the calculation of the ELBO.

\subsection{Piece-wise approximations of $\log(1+\exp(x))$}\label{sub.sec.piecewise}

This section presents the idea and details of the piece-wise approximations of $\log(1+\exp(x))$. In order to have the conjugacy in our variational Bayes algorithm, we apply piece-wise approximations to $\log(1+\exp(x))$, which are used in Section \ref{sec.VB.algo}. We know that $\log(1+\exp(x))$ is monotonically increasing in $(-\infty, \infty)$, and when $x$ is approaching $-\infty$, $\log(1+\exp(x))$ approaches 0, while when $x$ is approaching $\infty$, $\log(1+\exp(x))$ approaches $x$. Furthermore, when $x \leq -5$, $\log(1+\exp(x)) \approx 0$, and when $x \geq 5$, $\log(1+\exp(x)) \approx x$ since $\log(1+\exp(-5)) = 0.0067$ and $\log(1+\exp(5)) = 5.0067$. Therefore, our goal is to find appropriate piece-wise approximations of $\log(1+\exp(x))$ in $[-5, 5]$ whose plot is presented in Figure \ref{Piece-wise_apprroximation_plots} Left. To do this, we apply the method introduced by \citet{Muggeo_2003} implemented in R with a package called \textit{segmented} which can help find the optimal piece-wise linear approximation using regression.

First, we generate $10, 000$ data points from $\log(1+\exp(x))$ at equally spaced grid $x_i, i=1, ..., 10000$ in $[-5, 5]$. One, two, three, four, and five breakpoints are considered, which correspond to two, three, four, five, and six pieces. The sum of squared error (SSE) is used to evaluate the performance of the fitted model on the generated data. Finally, the optimal number of breakpoints is chosen at the knee of the plot of SSE versus the number of breakpoints. From Figure \ref{Piece-wise_apprroximation_plots} Right, the best number of breakpoints is three with an SSE of $3.3527$ and an $R^2$ of $0.9999$. A comparison of the fitted lines on the true curves with 2, 3, and 4 breakpoints is shown in Figure \ref{Comp_Piece-wise_plots}. The optimal fitted model with three break points (those three optimal breakpoints are -1.701, 0, and 1.702), $\hat{f}(x)$ is
\begin{eqnarray}
\hat{f}(x) &=& 0.1938 + 0.0426x + 0.2626 (x-(-1.701))_+ \nonumber \\
 && +\,0.3898 (x-0)_+ + 0.2624(x-1.702)_+, \quad x\in [-5, 5]\nonumber
\end{eqnarray}
where $(x -a)_+ :=\text{max}(x-a, 0)$ for any $a \in (-\infty, \infty)$. 

Therefore, we can approximate $\log(1+\exp(x))$ in $(-\infty, \infty)$ by
\begin{eqnarray}
\hat{f}(x)= \widehat{\log(1+\exp(x))}=\begin{cases} 0 & \text{if}\;x \leq -5 \\ 
0.1938 + 0.0426x & \text{if}\;-5< x \leq -1.701 \\ 
0.6405 + 0.3052x & \text{if}\;-1.701< x \leq 0  \\ 
0.6405 + 0.6950x & \text{if}\; 0< x \leq 1.702 \\ 
0.1939 + 0.9574x & \text{if}\;1.702< x \leq 5  \\ 
x &  \text{if}\;5 < x 
\end{cases}
\label{appro_1}
\end{eqnarray}
We ignore the two minor jumps at $x=-5$ and $x=5$ since we focus on the approximation of the function, and manually changing the structure of the piece-wise approximations will affect the optimum of the approximation.

We construct the quadratic piece-wise approximation based on the linear piece-wise approximation as Equation (\ref{appro_2}). We also ignore the 
discontinuity (minor jump) at each breakpoint. The SSE using quadratic piece-wise approximation is $0.1188$, and the $R^2$ of the fitted models is $1$.
\begin{eqnarray}
\hat{f}(x) &=& \widehat{\log(1+\exp(x))}\nonumber\\
&=&\begin{cases} 0 & \text{if}\;x \leq -5 \\ 
0.3893 + 0.1696x + 0.0189 x^2 & \text{if}\;-5< x \leq -1.7 \\ 
0.6962 + 0.5000x + 0.1138 x^2 & \text{if}\;-1.7< x \leq 1.7  \\ 
0.3894 + 0.8303x + 0.0190 x^2 & \text{if}\;1.7< x \leq 5  \\ 
x &  \text{if}\;5 < x 
\end{cases}
\label{appro_2}
\end{eqnarray}

\begin{figure}[!ht]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height = 6cm]{Figures/Plot_1.pdf}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[height = 6cm]{Figures/Plot_2.pdf}
\end{subfigure}
\caption{Left: Plot of $\log(1+\exp(x))$ versus $x$ for $x \in [-5, 5]$. Right: The plot of the sum of squared errors (SSE) versus the number of breakpoints in linear piece-wise approximation via regression modelling.}
\label{Piece-wise_apprroximation_plots}
\end{figure}

\begin{figure}[!ht]
\centering
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[height = 5.8cm]{Figures/Fitted_Plot_2.pdf}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[height = 5.8cm]{Figures/Fitted_Plot_3.pdf}
\end{subfigure} 
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[height = 5.8cm]{Figures/Fitted_Plot_4.pdf}
\end{subfigure}%
\begin{subfigure}{.48\textwidth}
  \centering
  \includegraphics[height = 5.8cm]{Figures/Blank_graph.png}
\end{subfigure} 
\caption{A comparison of the fitted lines on the true curves using 2, 3, and 4 break points with sum of squared errors (SSE) and R squared added to the plots.}
\label{Comp_Piece-wise_plots}
\end{figure}



\section{Simulation study}\label{Sec.sim}
In this section, we present three sets of simulation studies: simulation study one in Section \ref{Sim.study.one}, simulation study two in Section \ref{Sim.study.two}, and a prior sensitivity analysis in Section \ref{Sim.sensitivity}. In simulation study one, we investigate the performance of our proposed methodology using simulated data from six different scenarios. A further simulation study, simulation study two, is conducted to investigate the effect of sample size and censoring rate in our proposed methodology. Finally, we conduct a sensitivity analysis on the strength of the prior information.  

\subsection{Simulation study one}\label{Sim.study.one} 
\subsubsection{Simulation scenarios}\label{Sub.sec.scenarios}
Our simulation study comprises six scenarios that can be grouped into two categories. In the first group, we consider the same fixed values for the parameters (i.e., $\negr{\beta}$ and $b$) in each simulated data set, which is a common approach in frequentist analyses. In the second group, we consider randomly generated values for parameters in the different simulated data sets. The intuition for constructing scenarios under the second group is that we assume prior distributions for our parameters under a Bayesian framework. We name our six scenarios by Scenarios $i$, $ii$, and $iii$ from the first group and Scenarios $iv$, $v$, and $vi$ from the second group. We present the simulation scheme of each scenario as follows.
\vspace{0.5cm}

\noindent\textbf{\textit{(1) Scenarios in the first group}}

\noindent\textit{\textbf{Scenario $i$, no censoring data}}

In Scenario $i$, we consider complete data (i.e., without censoring) generated according to the following model:
$$\log(T)=Y=0.5+0.2 x_1 + 0.8 x_2 + 0.8 z,$$
where $x_1$, $x_2$ and $z$ are mutually independently generated with $x_1\sim N(0.25, 0.04^2)$, $x_2\sim N(1, 0.2^2)$ and $z \sim \text{logistic}(0,1)$. 

\vspace{0.5cm}
\noindent\textit{\textbf{Scenario $ii$, two continuous covariates}}

In Scenario $ii$, we consider the right censored data. The censoring times for observations are generated from a uniform distribution. Data are generated using: 
$$\log(T)=Y=0.5+0.2 x_1 + 0.8 x_2 + 0.8 z$$
where $x_1$, $x_2$ and $z$ are mutually independently generated with $x_1\sim N(0.2, 0.04^2)$, $x_2\sim N(0.8, 0.08^2)$ and $z \sim \text{logistic}(0,1)$. Censoring times $c_i, i=1, ..., 1000,$ are independently generated from a $\text{uniform}(0, 30)$, resulting in around $20\%$ of right censored data. 

\vspace{0.5cm}
\noindent\textit{\textbf{Scenario $iii$, one continuous covariate and one binary covariate}}

In Scenario $iii$, we consider that one of the two covariates is binary. Data are generated as follows:
$$\log(T)=Y=0.5+0.2 x_1 + 0.8 x_2 + 0.8 z,$$
where $x_1$, $x_2$, and $z$ are mutually independently generated with $x_1\sim N(1, 0.2^2)$, $x_2\sim \text{Bernoulli}(0.4)$ and $z \sim \text{logistic}(0,1)$. We consider around $20\%$ of right censored data as in Scenario $ii$.

In scenarios from the first group, the values of hyperparameters (parameters in the prior distributions) are set as follows: $\negr{\mu}_0 = (0.40, 0.15, 0.7)^T$, $v_0=10$, $\alpha_0=31$ and $\omega_0=24$. The sample size of each simulated data set is $1,000$. The convergence threshold is set as $0.001$ with a preset maximum number of iterations of $1,000$. 

\vspace{1cm}
\noindent\textbf{\textit{(2) Scenarios in the second group}}

\noindent\textit{\textbf{Scenario $iv$, no censoring data}}

Scenario $iv$ is modified from Scenario $i$. When generating each data set, we generate our parameters from known distributions, same for Scenarios $v$ and $vi$ below.

$$\log(T)=Y=\beta_0+\beta_1 x_1 + \beta_2 x_2 + b z$$
where $\beta_0 \sim N(0.5, 0.1)$, $\beta_1 \sim N(0.2, 0.1)$, $\beta_2 \sim N(0.8, 0.1)$, $b \sim \text{Inverse-Gamma}(31, 24)$, $x_1\sim N(0.25, 0.04^2)$, $x_2\sim N(1, 0.2^2)$ and $z \sim \text{logistic}(0,1)$.

\vspace{0.5cm}
\noindent\textit{\textbf{Scenario $v$, two continuous covariates}}

In Scenario $v$, we consider the right censored data. 
$$\log(T)=Y=\beta_0+\beta_1 x_1 + \beta_2 x_2 + b z$$
where $\beta_0 \sim N(0.5, 0.1)$, $\beta_1 \sim N(0.2, 0.1)$, $\beta_2 \sim N(0.8, 0.1)$, $b \sim \text{Inverse-Gamma}(31, 24)$, $x_1\sim N(0.25, 0.04^2)$, $x_2\sim N(1, 0.2^2)$ and $z \sim \text{logistic}(0,1)$.

\vspace{0.5cm}
\noindent\textit{\textbf{Scenario $vi$, one continuous covariate and one binary covariate}}

In Scenario $vi$, we consider that one of the two covariates is binary. 

$$\log(T)=Y=\beta_0+\beta_1 x_1 + \beta_2 x_2 + b z$$
where $\beta_0 \sim N(0.5, 0.1)$, $\beta_1 \sim N(0.2, 0.1)$, $\beta_2 \sim N(0.8, 0.1)$, $b \sim \text{Inverse-Gamma}(31, 24)$, $x_1\sim N(1, 0.2^2)$, $x_2\sim \text{Bernoulli}(0.4)$ and $z \sim  \text{logistic}(0,1)$.

In scenarios from the second group, the values of hyperparameters are set as follows: $\negr{\mu}_0 = (0.5, 0.2, 0.8)^T$, $v_0=10$, $\alpha_0=31$ and $\omega_0=24$. Regarding the sample size, convergence threshold, and the maximum number of iterations, we use the same values as in simulation scenarios from the first group.


\subsubsection{Performance metrics}\label{Sub.sec.performance}
To assess the performance of our proposed method and compare it with other methods, we apply it to 500 different simulated data sets in each scenario. We now turn attention to the issue of performance evaluation. 
\vspace{0.5cm}

\noindent\textbf{\textit{(1) Empirical bias, standard deviation and MSE of estimates}}

In each simulation scenario, we calculate each parameter estimate's empirical bias and standard deviation (SD) from 500 different replicates. In each replicate, we apply our proposed method to obtain each parameter's approximated posterior distribution and use the posterior's mean as the estimate. We also calculate the empirical mean squared errors (MSE) of each estimate which is defined as
$$\text{MSE}=\frac{\sum_{i=1}^N (\theta -\hat{\theta}_i)^2}{N}$$
where $N$ is the number of replicates, $\hat{\theta}_i$ is the estimate of parameter $\theta$ in the $i$th replicate.
\vspace{0.5cm}

\noindent\textbf{\textit{(2) Credible interval coverage and average interval length}}

In Bayesian statistics, credible interval coverage is another crucial evaluation metric of estimation accuracy. We compare the advertised coverage of approximate credible intervals based on Variational Bayes with true coverage for scenarios in the second group, which are more integrated with the Bayesian statistical framework. To compute the empirical coverage percentage of the true value for each parameter, we construct the 95\% credible interval based on the approximated posterior distribution in each replicate of the simulated data set. We then use the interval to check if it covers the true value or not, and calculate the percentage of coverage from 500 replicates. For fixed effects $\negr{\beta}$, we use equal-tailed interval (ETI) while highest density interval (HDI) for scale parameter $b$ since Inverse-Gamma distribution is not a symmetric distribution and HDI is better than ETI \citep{Kruschke_2015}. We also present the average interval length from these 500 replicates as an additional dimension of estimation precision.


%\subsubsection{Computational cost}
%High computational cost is a common issue in MCMC based Bayesian inference algorithms. To investigate the computational cost of our proposed VB algorithm, we keep track of times used for getting parametric estimations in each scenario from the second group based on 500 replicates.

%\textbf{Comments and Notes:} In the future work, I will compare them with using the MCMC algorithm from the R package \textit{rjags} \citep{Alvares_2021, Plummer_2022}. Right now, for example, the running time of our proposed VB method under Scenario i for 500 replicates is 2.6 hours. On average, it took 18.7 seconds for each replicate.

\subsubsection{Simulation results}\label{Sub.sec.results}

\noindent\textbf{\textit{(1) Simulation results for the first group}}

The simulation schemes in the first group are under the framework of frequentist statistics. Therefore, we compare results from our proposed method with those based on maximum likelihood estimation implemented by the function \textit{survreg} in the R package \textit{survival}. Numerical results, including the empirical bias, SD, and MSE from 500 replicates, are presented in Table \ref{tab.results.1}. To visualize the distribution of the estimate for each parameter between the two methods, we construct side-by-side boxplots in Figure \ref{Scenario.2.1.Boxplots} for each parameter under Scenario $ii$ as an example. In most cases, we can not observe a significant difference in bias when estimating the parameters between the proposed VB algorithm and the likelihood-based \textit{survreg} method in each scenario. However, the VB method provides smaller SDs for all cases and, therefore, smaller MSE than the \textit{survreg} method. The higher estimation efficiency from the VB method is particularly prominent in the fixed effects, which can be seen from the boxplots that the corresponding ranges of estimates from 500 replicates based on the VB method are much smaller.

\begin{table}[h]
\begin{center}
\begin{minipage}{\textwidth}
\caption{\textit{Results for the first group scenarios under simulation study one.} Empirical bias, standard deviation (SD) and mean squared errors (MSE) of estimates in each scenario from our proposed VB algorithm, and from \textit{survreg} in the R package \textit{survival}.}\label{tab.results.1}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}cccccccc@{\extracolsep{\fill}}}
\toprule%
& & \multicolumn{3}{@{}c@{}}{VB algorithm} & \multicolumn{3}{@{}c@{}}{\textit{survreg}}\\\cmidrule{3-5}\cmidrule{6-8}%
Scenario  & parameter & bias & SD & MSE & bias & SD & MSE\\
\midrule
$i$ & $\beta_0=0.5$ & 0.0072 & 0.1082 & 0.0117 & 0.0131 & 0.3543 & 0.1255\\
    & $\beta_1=0.2$ & -0.0254 & 0.0824 & 0.0074 & -0.0003 & 1.1115  & 1.2329\\
    & $\beta_2=0.8$ & -0.0045 & 0.1110 & 0.0123 & -0.0122 & 0.2234 & 0.0500\\
    & $b=0.8$ & -0.0020 & 0.0205 & 0.0004 & -0.0015 & 0.0222 & 0.0005\\
\botrule
$ii$ & $\beta_0=0.5$ & 0.0096 & 0.0770 & 0.0060 & 0.0251 & 0.5063 & 0.2565\\
    & $\beta_1=0.2$ & -0.0282 & 0.0811 & 0.0074 & -0.0015 & 1.1324 & 1.2799\\
    & $\beta_2=0.8$ & -0.0091 & 0.0945 &  0.0090 & -0.0278 & 0.5715 & 0.3267\\
    & $b=0.8$ & -0.0014 & 0.0241 & 0.0005 & -0.0003 & 0.0247 & 0.0006\\
\botrule
$iii$ & $\beta_0=0.5$ & -0.0165 & 0.1122 & 0.0128 & -0.0042 & 0.2437 & 0.0593\\
    & $\beta_1=0.2$ & 0.0099 & 0.1146 & 0.0132 & 0.0032 & 0.2379 & 0.0565\\
    & $\beta_2=0.8$ & -0.0046 & 0.0879 & 0.0077 & 0.0034 & 0.0953 & 0.0091\\
    & $b=0.8$ & -0.0025 & 0.0232 & 0.0005 & -0.0023 & 0.0224 & 0.0005\\
\botrule
\end{tabular*}
\end{minipage}
\end{center}
\end{table}



\begin{figure}[!ht]
\centering
  \includegraphics[height = 8cm, width = 10cm]{Figures/Scenario_2_1_Boxplots.pdf}
\caption{\textit{Results for the first group scenario $ii$ under simulation study one.} Boxplots of estimates for each parameter based on our VB algorithm and \textit{survreg} in the R package \textit{survival}.}
\label{Scenario.2.1.Boxplots}
\end{figure}

\vspace{0.5cm}
\noindent\textbf{\textit{(2) Simulation results for the second group}}

For scenarios from the second group, we present numerical results in Table \ref{tab.results.2} including the empirical bias, SD, and MSE from 500 replicates as we did in simulation scenarios from the first group. Among these scenarios where the parameters were generated differently in each replicate, we observe a more considerable bias in Scenarios $iv$ and $v$ but a more negligible bias in Scenario $vi$ compared with scenarios in the first group. Since we have more variability in generating the simulated data, the standard deviation and the MSE of estimates in each scenario increase. Table \ref{tab.results.2} also presents the credible interval coverage with average interval length for each scenario. The coverage rates are around 95\%, showing effective estimation results obtained by our proposed VB algorithm.

% For the computational cost, the run times of the proposed VB algorithm for 500 simulated datasets under each scenario from the first and second groups are 2.6 hours, 2.95 hours, 2.4 hours, 2.23 hours, 2.6 hours, and 2.05 hours, respectively. Taking Scenario vi as an example, it took 14.76 seconds on average for each replicate. The algorithm was implemented in R version 3.6.3 on a computer using the Mac OS X operating system with an 1.6 GHz processor and 8 GBytes of random access memory.  

\begin{table}[h]
\begin{center}
\begin{minipage}{\textwidth}
\caption{\textit{Results for the second group under simulation study one.} Empirical bias, standard deviation (SD), mean squared errors (MSE),  and credible interval coverage (coverage) with average interval length (avg.length) of estimates in each scenario from our proposed VB algorithm.} \label{tab.results.2}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ccccccc@{\extracolsep{\fill}}}
\toprule%
Scenario  & parameter & bias & SD & MSE & coverage & avg. length\\
\midrule
$iv$ & $\beta_0=0.5$ & 0.0284 & 0.4278 & 0.1252 & 0.954 & 0.6523\\
    & $\beta_1=0.2$ & 0.0220 & 0.4107 & 0.1846 & 0.962 & 1.1603\\
    & $\beta_2=0.8$ & 0.0276 & 0.4970 & 0.1204 & 0.948 & 0.6064\\
    & $b=0.8$ & -0.0132 & 0.1467 & 0.0140 & 0.961 & 0.0936\\
\botrule
$v$ & $\beta_0=0.5$ & 0.0254 & 0.4545 & 0.1644 & 0.945 & 0.7294 \\
    & $\beta_1=0.2$ & 0.0233 & 0.4486 & 0.2179 & 0.962 & 1.1736 \\
    & $\beta_2=0.8$ & 0.0252 & 0.5028 & 0.1785 & 0.949 & 0.8864 \\
    & $b=0.8$ & -0.0114 & 0.1431 & 0.0149 & 0.954 & 0.1062 \\
\botrule
$vi$ & $\beta_0=0.5$ & 0.0073 & 0.4672 & 0.1547 & 0.947 & 0.6265 \\
    & $\beta_1=0.2$ & 0.0008 & 0.5069 & 0.1537 & 0.942 & 0.6080  \\
    & $\beta_2=0.8$ & 0.0083 & 0.5486 & 0.1359 & 0.947 &  0.3354 \\
    & $b=0.8$ & -0.0229 & 0.1388 & 0.0148 & 0.962 & 0.1037 \\
\botrule
\end{tabular*}
\end{minipage}
\end{center}
\end{table}

%\textbf{Comments and Notes:} In future work, I will compare them with with using the MCMC algorithm since it is not appropriate to compare with those results from \textit{survreg} in the R package \textit{survival} since it somehow violates from the framework of frequentist statistics and \textit{survreg} does not work.

\subsection{Simulation study two}\label{Sim.study.two}
In this section, we further investigate the performance of our proposed model via simulation study in terms of sample size and censoring rate under Scenario $iii$. We use non-informative priors with $\negr\mu_0 = (0, 0, 0)^T$, $v_0=1$, $\alpha_0=31$ and $\omega_0=24$, to decrease the dominance effect of prior information. The same performance metrics introduced in Section \ref{Sub.sec.performance} are applied to simulation study two.

\subsubsection{Different sample sizes}
In Section \ref{Sub.sec.scenarios}, we considered simulated data sets with a sample size of 1000. In this section, under a fixed censoring rate of 20\%, we further consider sample sizes of 100, 200, and 400, respectively, to assess how the performance of our model changes. Results are presented in Table \ref{sen.tab.sizes} and Figure \ref{sen.size.fig}. As the sample size increases from 100 to 400, estimates' bias and standard deviation decrease, resulting in a steady decline in MSE. A consistent conclusion can be drawn from the density plots. The gap between the true value of fixed effects and the median estimate from 500 replicates becomes smaller as the sample size increases. Furthermore, sharper peaks on density plots in Figure \ref{sen.size.fig} under a larger sample size indicate a minor variation in estimates.

\begin{table}[h]
\begin{center}
\begin{minipage}{\textwidth}
\caption{\textit{Results for simulation study two under different sample sizes.} Empirical bias (SD, MSE) of estimates in Scenario $iii$.}\label{sen.tab.sizes}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}cccc@{\extracolsep{\fill}}}
\toprule%
& \multicolumn{3}{@{}c@{}}{sample size, $n$}\\\cmidrule{2-4}%
parameter  & $n=100$ & $n=200$ & $n=400$ \\
\midrule
$\beta_0=0.5$ & -0.0658 (0.3472, 0.1246) & -0.0317 (0.3257, 0.1069) &  -0.0308 (0.3006, 0.0911) \\
$\beta_1 =0.2$& 0.0771 (0.3426, 0.1231) & 0.0377 (0.3170, 0.1017) &   0.0280 (0.2854, 0.0821)\\
$\beta_2=0.8$& -0.0696 (0.2555, 0.0700) & -0.0238 (0.2038, 0.0420) &  -0.0096 (0.1407, 0.0199)\\
$b=0.8$ & -0.0062 (0.0614, 0.0038) & -0.0083 (0.0478, 0.0023)       &  -0.0034 (0.0337, 0.0011) \\
\botrule
\end{tabular*}
\end{minipage}
\end{center}
\end{table}

\begin{figure}[!ht]
\centering
\includegraphics[height = 3.5cm]{Figures/sensitivity/Scenario_3_1_size_100.pdf}\\
\vspace{0.1cm} 
\includegraphics[height = 3.5cm]{Figures/sensitivity/Scenario_3_1_size_200.pdf}\\
\vspace{0.1cm}
\includegraphics[height = 3.5cm]{Figures/sensitivity/Scenario_3_1_size_400.pdf}\\
\caption{\textit{Results for simulation study two under different sample sizes.} Densities of estimates of $\beta_1$ and $\beta_2$. Vertical black lines represent the true values of the corresponding parameters while the vertical green lines represent the median estimates from 500 replicates.}\label{sen.size.fig}
\end{figure}

\subsubsection{Different censoring rates}
The censoring rate (i.e., the percentage of censored data) is also attractive in survival analysis. In this part, under a fixed sample size of 1000, we investigate the performance of the proposed model in different censoring rates of 10\%, 20\%, and 30\%. Numerical results are presented in Table \ref{sen.tab.cen.rates}. Unlike analysis under different sample sizes, we do not observe an asymptotic property as the censoring rate decreases. Instead, when the censoring rate increases (i.e., the proportion of complete data decreases), bias and standard deviation of estimates show some rises and falls. However, the estimation performance under a high censoring rate (e.g., 30\%) is still satisfactory, which can be shown by the density plots of estimates for fixed effects in Figure \ref{sen.cen.fig}. The gap between the true and the median estimate values is minor, and no significant differences appear among scenarios with different censoring rates.

\begin{table}[h]
\begin{center}
\begin{minipage}{\textwidth}
\caption{\textit{Results for simulation study two under different censoring rates.} Empirical bias (SD, MSE) of estimates in Scenario $iii$.}\label{sen.tab.cen.rates}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}cccc@{\extracolsep{\fill}}}
\toprule%
& \multicolumn{3}{@{}c@{}}{censoring rate, $r$ (\%)}\\\cmidrule{2-4}%
parameter  & $r=10\%$ & $r=20\%$ & $r=30\%$ \\
\midrule
$\beta_0=0.5$ & -0.0137 (0.2187, 0.0479) & -0.0050 (0.2159, 0.0465) &  -0.0113 (0.2223, 0.0494) \\
$\beta_1 =0.2$ & 0.0104 (0.2124, 0.0451) & 0.0001 (0.2116, 0.0447) &  0.0085 (0.2181, 0.0476)\\
$\beta_2=0.8$ & -0.0043 (0.0928, 0.0086)  & 0.0016 (0.0929, 0.0086) &  -0.0024 (0.0910, 0.0083) \\
$b=0.8$ & 0.0000 (0.0461, 0.0021) & -0.0036 (0.0495, 0.0025) & -0.0240 (0.0360, 0.0019) \\
\botrule
\end{tabular*}
\end{minipage}
\end{center}
\end{table}

\begin{figure}[!ht]
\centering
\includegraphics[height = 3.5cm]{Figures/sensitivity/Scenario_3_1_cen_10.pdf}\\
\vspace{0.1cm} 
\includegraphics[height = 3.5cm]{Figures/sensitivity/Scenario_3_1_cen_20.pdf}\\
\vspace{0.1cm}
\includegraphics[height = 3.5cm]{Figures/sensitivity/Scenario_3_1_cen_30.pdf}\\
\caption{\textit{Results for simulation study two under different censoring rates.} Densities of estimates of $\beta_1$ and $\beta_2$. Vertical black lines represent the true values of the corresponding parameters while the vertical green lines represent the median estimates from 500 replicates.}\label{sen.cen.fig}
\end{figure}

\subsection{Prior sensitivity analysis}\label{Sim.sensitivity}

In this section, we investigate the sensitivity of our VB algorithm to different priors. We consider three different situations under Scenario $iii$ described in Section \ref{Sub.sec.scenarios}. In the first situation, we assume we have informative priors knowledge of mean and variance for the coefficients $\negr\beta$ and the scale parameter $b$. In the second situation, we consider a large variance for $\negr\beta$ and $b$ in their prior distributions but accurate location information (i.e., we use the true mean as the prior mean). In this situation, the intuitive is that although we have an informative knowledge of the location of $\negr\beta$ and $b$, the flexibility for them to find an estimation of the true value is considerable. In the third situation, we consider non-informative priors. Details on the priors for these three different situations are as follows. Note that we use a sample size of $1000$ in each replicate. 

\begin{itemize}
    \item Situation 1: $\negr\mu_0 = (0.5, 0.2, 0.8)^T$, $v_0=10$, $\alpha_0=26$ and $\omega_0=20$. Situation 1 is the case where we have the strongest prior information. The prior's means are set as true means, and the prior's variances are small. In this case, the variance of $b$ in its prior is $\omega_0^2 / ((\alpha_0-1)^2(\alpha_0 - 2)) = 0.027$.
    \item Situation 2: $\negr\mu_0 = (0.4, 0.15, 0.7)^T$, $v_0=5$, $\alpha_0=17$ and $\omega_0=20$. In this situation, the variance of $\negr\beta$ in its prior is $0.2$, one time larger than that in Situation 1, and the variance of $b$ in its prior is $0.104$, three times larger than that in Situation 1.
    \item Situation 3: $\negr\mu_0 = (0, 0, 0)^T$, $v_0=1$, $\alpha_0=8$ and $\omega_0=9$. In this case, the prior mean for $b$ is $1$, and the variance is $0.14$, which is five times larger than that in Situation 1.
\end{itemize}

Table \ref{sen.tab.prior} presents the results from the sensitivity analysis in different prior settings. As expected, more accurate and efficient estimates with negligible bias and SD were returned when we had informative priors in Situation 1. In Situation 2, where weaker prior knowledge was given, the estimation performance decreased but was insignificant. We still obtain satisfactory estimation results even with non-informative priors (i.e., Situation 3). 

\begin{table}[h]
\begin{center}
\begin{minipage}{\textwidth}
\caption{\textit{Prior sensitivity analysis results.} Empirical bias (SD, MSE) of estimates in Scenario $iii$ with different priors.}\label{sen.tab.prior}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}cccc@{\extracolsep{\fill}}}
\toprule%
& \multicolumn{3}{@{}c@{}}{Prior setting}\\\cmidrule{2-4}%
parameter  & Situation 1 & Situation 2 & Situation 3 \\
\midrule
$\beta_0=0.5$ & -0.0081 (0.1131, 0.0128) & -0.0094 (0.2094, 0.0438) &  -0.0175 (0.2214, 0.0492) \\
$\beta_1 =0.2$ & -0.0001 (0.1156, 0.0133) & 0.0011 (0.2077, 0.0431) &  0.0128 (0.2190, 0.0480)\\
$\beta_2=0.8$ & -0.0002 (0.0857, 0.0073) & 0.0008 (0.0926, 0.0086) & -0.0001 (0.0928, 0.0086) \\
$b=0.8$ & 0.0043 (0.0524, 0.0028) & 0.0175 (0.0598, 0.0039) &  0.0222 (0.0799, 0.0069) \\
\botrule
\end{tabular*}
\end{minipage}
\end{center}
\end{table}



\section{Application to real data} \label{Sec.real.data}
In this section, we apply our proposed VB algorithm in Section \ref{sec.VB.algo} to a real data set, \textbf{rhDNASE}, which is publicly available in the R package \textit{survival}. The data, first introduced in \citet{Fuchs_1994} and further analyzed in \citet{Therneau_1997}, were used to investigate the effect of recombinant human deoxyribonuclease I (rhDNase) on pulmonary function among patients with cystic fibrosis. The rhDNase can digest extracellular DNA released by leukocytes that accumulate in the airways in response to chronic bacterial infection. Therefore, administering rhDNase would reduce the incidence of exacerbation and improve lung function. Among 645 subjects, 324 were randomly assigned to the Placebo group, and the rest were assigned to the treatment group (i.e., the rhDNase group). The event time, $T$, was defined as the time until the first pulmonary exacerbation, and the follow-up period was 169 days. The forced expiratory volume (FEV) at enrollment was considered a risk factor (i.e., covariate) measuring lung capacity. In \citet{Lawless_2003}, a log-logistic AFT model was applied to this data set, and estimates were obtained by maximizing the likelihood. Model diagnostic in \citet{Lawless_2003} shows that the parametric assumption that the event time follows a log-logistic distribution was satisfied. Therefore, we want to fit an AFT regression model via our proposed VB algorithm,
$$\log(T):= Y=\beta_0 + \beta_1 x_1 +\beta_2 x_2 + b z,$$
where $x_1=I(\text{treatment = rhDNase})$, $x_2$ is the FEV, and $z$ follows a standard logistic distribution with a scale parameter $b$.

Unlike simulation studies, we do not have informative priors in real data. However, we can choose priors using historical data and similar analyses on this type of data. In a similar study in \citet{Shah_1996} on the effect of rhDNase on improving lung function, researchers found that daily treatment of rhDNase could reduce the risk of developing an exacerbation by 28\%. That is, a daily administration of rhDNase can prolong the occurrence of an exacerbation by 28\%. Therefore, we can choose $\log(1.28)=0.25$ as the prior mean of $\beta_1$. Similarly, based on \citet{Block_2006}, the odds ratio of developing an exacerbation with one unit increase of FEV is 0.96, which indicates the corresponding time to an exacerbation occurrence increase by 4\%. Therefore, we can choose $\log(1.04)=0.04$ as the prior mean of $\beta_2$. For the mean of the intercept (i.e., $\beta_0$) prior distribution, we can choose the log of half of the follow-up period length, $\log(169/2) = 4.4$. For the precision hyperparameter $v_0$, we use a low precision, with $v_0=1$, to obtain a flat prior. For the prior of the scale parameter, we use $\alpha_0 = 1100$ and $\omega_0=1000$ to have a mean scale around $0.9$. To summarize, we use the following priors, 
\begin{itemize}
    \item $\negr{\beta} \sim N_p(\negr\mu_0, \sigma_0^2 I_{p\times p})$ with $\negr\mu_0 = (4.4, 0.25, 0.04)^T$ and $v_0=1/\sigma_0^2=1$
    \item $b \sim \text{Inverse-Gamma}\,(\alpha_0, \omega_0)$ with $\alpha_0 = 1100$ and $\omega_0=1000$.
\end{itemize}

We compare the estimation results of the regression coefficients from our proposed VB algorithm to those from the likelihood-based survival regression, presented in Table \ref{tab.results.real}. Based on the results from our method, the estimated coefficient of the treatment, rhDNase, is 0.4082 with a 95\% credible interval of $[0.1381, 0.6783]$, indicating that rhDNase can significantly prolong the time to the first pulmonary exacerbation. Furthermore, the acceleration factor is $\exp(0.4082) = 1.504$ with a 95\% credible interval of $[1.148, 1.971]$ for a patient treated with rhDNase. The time to the first pulmonary exacerbation of a patient treated with rhDNase is therefore delayed by a factor of about 1.5 compared to a patient from the placebo group with the same FEV under a log-logistic AFT model. Besides, FEV is a significant risk factor on the event time, with an estimated coefficient of 0.0207 (95\% credible interval $[0.0152, 0.0262]$). The acceleration factor of FEV is $\exp(0.0207) = 1.021$, meaning that one unit increase in FEV would delay the event time by 2.1\% with a 95\% credible interval of $[1.5\%, 2.7\%]$. Our results regarding $\beta_0$, $\beta_1$ and $\beta_2$ agree with the results obtained by \textit{survreg} within the corresponding confidence intervals.

\begin{table}[h]
\begin{center}
\begin{minipage}{\textwidth}
\caption{\textit{Results from analysis on rhDNASE data.} Posterior means (Post. Mean) 
with posterior standard errors (Post. SE), and 95\% credible intervals (95\% Cred. Int.) 
from our proposed VB algorithm. Point estimates (Est.) with standard errors (SE), and 95\% confidence interval (95\% Conf. Int.) from \textit{survreg} in the R package \textit{survival}. }
\label{tab.results.real}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ccccccc@{\extracolsep{\fill}}}
\toprule%
& \multicolumn{3}{@{}c@{}}{VB algorithm} & \multicolumn{3}{@{}c@{}}{\textit{survreg}}\\\cmidrule{2-4}\cmidrule{5-7}%
 & Post. Mean & Post. SE & 95\% Cred. Int.\footnotemark[1] & Est. & SE & 95\% Conf. Int.\footnotemark[2]\\
\midrule
 $\beta_0$ & 4.1158 & 0.1859 & [3.7514, 4.4802] & 4.0858 & 0.1749 & [3.7430, 4.4286]\\
 $\beta_1$ & 0.4082 & 0.1378 & [0.1381, 0.6783] & 0.4016 & 0.1304  & [0.1460, 0.6572]\\
 $\beta_2$ & 0.0207 & 0.0028 & [0.0152, 0.0262] & 0.0207 & 0.0028 & [0.0152, 0.0263]\\
 $b$ & 0.8765 & 0.0005 & [0.8300, 0.9238] & 0.7960 & 0.0570 \footnotemark[3] & [0.7121, 0.8905] \\
 \botrule
\end{tabular*}
\footnotetext[1]{95\% Cred. Int.: highest density interval (HDI) was applied. Note that for a symmetric distribution, HDI is the same as the equal-tailed interval (ETI).}
\footnotetext[2]{95\% Conf. Int.: for regression coefficient estimates, the likelihood-based confidence interval was used, while for the scale estimate, the Wald-based interval was used.}
\footnotetext[3]{Standard error (SE) for the scale estimate is not available in the R package, but the SE for log scale which is 0.0570, is provided.}
\end{minipage}
\end{center}
\end{table}

For the log-logistic AFT model, the fitted survival function, $\hat S(t)$, can be obtained by
$$\hat S(t\vert \hat b, \hat{\negr{\beta}})=\bigg[1+\bigg(\frac{t}{\exp(\vect{X}^T\hat{\negr{\beta}})}\bigg)^{1/\hat b}\bigg]^{-1}.$$
Figure \ref{Fitted.survival} provides a comparison of the fitted survival functions from our proposed method and \textit{survreg} in the \textit{survival} package. The fitted curves resemble the
quantitative result that a patient treated with rhDNase would slow the time to the first pulmonary exacerbation compared with a patient from the placebo group with the same FEV equal to a median FEV of 57.6. There is no big difference in the fitted survival curves between the two methods, which is consistent with estimation results in Table \ref{tab.results.real}.

\begin{figure}[!ht]
\centering
  \includegraphics[width=.8\linewidth]{Figures/fitted_survival_comp.pdf}
\caption{\textit{Results from analysis on rhDNASE data.} Fitted survival functions for the placebo and treatment groups when FEV = 57.6 (the median FEV), based on our VB algorithm and \textit{survreg} in the R package \textit{survival}.}
\label{Fitted.survival}
\end{figure}

\section{Conclusion and Discussion} \label{Sec.con.dis}
In this paper, we propose an alternative algorithm of MCMC-based Bayesian techniques to model survival data following log-logistic distribution with the right censorship. The mean-field variational Bayes (VB) is considered, and the coordinate ascent variational inference is applied to derive the update equations within the VB algorithm. To achieve conjugacy under the Bayesian framework, the linear and quadratic piece-wise approximations of $\log(1+\exp(x))$ for $x\in [-5, 5]$ are embedded in the update equations for parameters. Simulation studies and the application to a real data set show that our proposed VB algorithm provides satisfactory results for modelling survival data.

The advantages of our method are mainly from two aspects. As with other general Bayesian inferences, the proposed VB method can incorporate prior information about a parameter from historical data or similar studies, which is more common in clinical research. Furthermore, based on our sensitivity analysis, the VB algorithm does not necessarily rely on asymptotic approximation and is not sensitive to sample size and censoring rate. However, our proposed VB method can empirically provide similar asymptotic properties concerning sample size. 

In principle, VB can be applied to the AFT regression model with different censoring schemes, including right-censored, left-censored, and interval censored, under different parametric distribution assumptions of survival data. To the best of our knowledge, we are the first to apply Bayesian variational inference to model survival data via AFT regression. In addition, satisfactory estimation performance from simulation studies and real data analysis indicates that embedding an accurate approximation inside the update equations under variational Bayes, which is itself an approximation technique to obtain posterior distributions, can still provide effective estimation results.   

\bibliography{sn-bibliography}

% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%\input sn-article.bbl

%% Default %%
%%\input sn-sample-bib.tex%

\end{document}
