@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@misc{Gordon2020,
doi = {10.48550/ARXIV.2002.08307},

url = {https://arxiv.org/abs/2002.08307},

author = {Gordon, Mitchell A. and Duh, Kevin and Andrews, Nicholas},

keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Compressing {BERT}: Studying the Effects of Weight Pruning on Transfer Learning},

publisher = {arXiv},

year = {2020},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Han2016,
doi = {10.48550/ARXIV.1602.01528},

url = {https://arxiv.org/abs/1602.01528},

author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},

keywords = {Computer Vision and Pattern Recognition (cs.CV), Hardware Architecture (cs.AR), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {{EIE}: Efficient Inference Engine on Compressed Deep Neural Network},

publisher = {arXiv},

year = {2016},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Zhu2017,
doi = {10.48550/ARXIV.1710.01878},

url = {https://arxiv.org/abs/1710.01878},

author = {Zhu, Michael and Gupta, Suyog},

keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {To prune, or not to prune: exploring the efficacy of pruning for model compression},

publisher = {arXiv},

year = {2017},

copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{BowIPU,

url = {https://www.graphcore.ai/bow-processors},

author = {Graphcore},

title = {Bow {IPU} processor},

year = {2022}
}

@misc{BowSpec,

url = {https://docs.graphcore.ai/projects/bow-2000-datasheet/en/latest/product-description.html#technical-specifications},

author = {Graphcore},

title = {Bow-2000 {IPU}-Machine Technical specifications},

year = {2022}
}

@misc{cusparse,

url = {https://docs.nvidia.com/cuda/cusparse/},

author = {NVIDIA},

title = {{API} reference guide for {cuSPARSE}},

year = {2022}
}

@misc{cublas,

url = {https://docs.nvidia.com/cuda/cublas/},

author = {NVIDIA},

title = {{API} Reference guide for {cuBLAS}},

year = {2022}
}

@misc{tensorcores,

url = {https://www.nvidia.com/en-gb/data-center/tensor-cores/},

author = {NVIDIA},

title = {{NVIDIA} Tensor Cores},

year = {2022}
}

@misc{Poplar,

url = {https://docs.graphcore.ai/projects/poplar-user-guide/en/latest/index.html
},

author = {Graphcore},

title = {Poplar and PopLibs User Guide},

year = {2022}
}

@misc{AMP,

url = {https://docs.graphcore.ai/projects/ai-float-white-paper/en/latest/ai-float.html#lower-precision-for-higher-power-efficiency},

author = {Graphcore},

title = {Mixed-Precision Arithmetic for {AI}: A Hardware Perspective},

year = {2022}
}

@misc{IPUprogramming,

url = {https://docs.graphcore.ai/projects/ipu-programmers-guide/en/latest/programming_model.html#loading-and-running-programsy},

author = {Graphcore},

title = {{IPU} PROGRAMMER’S GUIDE},

year = {2022}
}

@misc{Qin2022,
doi = {10.48550/ARXIV.2201.08916},

url = {https://arxiv.org/abs/2201.08916},

author = {Qin, Eric and Garg, Raveesh and Bambhaniya, Abhimanyu and Pellauer, Michael and Parashar, Angshuman and Rajamanickam, Sivasankaran and Hao, Cong and Krishna, Tushar},

keywords = {Hardware Architecture (cs.AR), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Enabling Flexibility for Sparse Tensor Acceleration via Heterogeneity},

publisher = {arXiv},

year = {2022},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Hoefler2021,
doi = {10.48550/ARXIV.2102.00554},

url = {https://arxiv.org/abs/2102.00554},

author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},

keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Hardware Architecture (cs.AR), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},

publisher = {arXiv},

year = {2021},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Dao2021,
doi = {10.48550/ARXIV.2112.00029},

url = {https://arxiv.org/abs/2112.00029},

author = {Dao, Tri and Chen, Beidi and Liang, Kaizhao and Yang, Jiaming and Song, Zhao and Rudra, Atri and Ré, Christopher},

keywords = {Machine Learning (cs.LG), {FOS}: Computer and information sciences, FOS: Computer and information sciences},

title = {Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models},

publisher = {arXiv},

year = {2021},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Dietrich2021,
doi = {10.48550/ARXIV.2108.06277},

url = {https://arxiv.org/abs/2108.06277},

author = {Dietrich, Anastasia and Gressmann, Frithjof and Orr, Douglas and Chelombiev, Ivan and Justus, Daniel and Luschi, Carlo},

keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Towards Structured Dynamic Sparse Pre-Training of {BERT}},

publisher = {arXiv},

year = {2021},

copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@misc{Mostafa2019,
doi = {10.48550/ARXIV.1902.05967},

url = {https://arxiv.org/abs/1902.05967},

author = {Mostafa, Hesham and Wang, Xin},

keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization},

publisher = {arXiv},

year = {2019},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Helal2022,
doi = {10.48550/ARXIV.2211.13853},

url = {https://arxiv.org/abs/2211.13853},

author = {Helal, Hatem and Firoz, Jesun and Bilbrey, Jenna and Krell, Mario Michael and Murray, Tom and Li, Ang and Xantheas, Sotiris and Choudhury, Sutanay},

keywords = {Machine Learning (cs.LG), Hardware Architecture (cs.AR), Chemical Physics (physics.chem-ph), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},

title = {Extreme Acceleration of Graph Neural Network-based Prediction Models for Quantum Chemistry},

publisher = {arXiv},

year = {2022},

copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{Yang2018,
  title={Design principles for sparse matrix multiplication on the {GPU}},
  author={Yang, Carl and Bulu{\c{c}}, Ayd{\i}n and Owens, John D},
  booktitle={Euro-Par 2018: Parallel Processing: 24th International Conference on Parallel and Distributed Computing, Turin, Italy, August 27-31, 2018, Proceedings},
  year={2018}
}

@inproceedings{Huang2020,
  title={Ge-spmm: General-purpose sparse matrix-matrix multiplication on {GPU}s for graph neural networks},
  author={Huang, Guyue and Dai, Guohao and Wang, Yu and Yang, Huazhong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  year={2020},
  organization={IEEE}
}

@misc{Wang2020,
doi = {10.48550/ARXIV.2008.11849},

url = {https://arxiv.org/abs/2008.11849},

author = {Wang, Ziheng},

keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {SparseRT: Accelerating Unstructured Sparsity on {GPU}s for Deep Learning Inference},

publisher = {arXiv},

year = {2020},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Gale2020,
doi = {10.48550/ARXIV.2006.10901},

url = {https://arxiv.org/abs/2006.10901},

author = {Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},

keywords = {Machine Learning (cs.LG), Distributed, Parallel, and Cluster Computing (cs.DC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Sparse {GPU} Kernels for Deep Learning},

publisher = {arXiv},

year = {2020},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Hong2019,
title={Adaptive sparse tiling for sparse matrix multiplication},
author={Changwan Hong and Aravind Sukumaran-Rajam and Israt Nisa and Kunal Singh and P. Sadayappan},
journal={Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
year={2019}
}

@article{Yao2018,
doi = {10.1609/aaai.v33i01.33015676},

url = {https://doi.org/10.1609%2Faaai.v33i01.33015676},

year = 2019,
month = {jul},

publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},

volume = {33},

number = {01},

pages = {5676--5683},

author = {Zhuliang Yao and Shijie Cao and Wencong Xiao and Chen Zhang and Lanshun Nie},

title = {Balanced Sparsity for Efficient {DNN} Inference on {GPU}
},

journal = {Proceedings of the {AAAI} Conference on Artificial Intelligence}
}

@inproceedings{Gray2017,
title={{GPU} Kernels for Block-Sparse Weights},
author={Scott Gray and Alec Radford and Diederik P. Kingma},
year={2017}
}

@misc{Anwar2015,
doi = {10.48550/ARXIV.1512.08571},

url = {https://arxiv.org/abs/1512.08571},

author = {Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},

keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Structured Pruning of Deep Convolutional Neural Networks},

publisher = {arXiv},

year = {2015},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{He2017,
doi = {10.48550/ARXIV.1707.06168},

url = {https://arxiv.org/abs/1707.06168},

author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},

keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Channel Pruning for Accelerating Very Deep Neural Networks},

publisher = {arXiv},

year = {2017},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Lin2019,
doi = {10.48550/ARXIV.1901.07827},

url = {https://arxiv.org/abs/1901.07827},

author = {Lin, Shaohui and Ji, Rongrong and Li, Yuchao and Deng, Cheng and Li, Xuelong},

keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {Towards Compact {ConvNets} via Structure-Sparsity Regularized Filter Pruning},

publisher = {arXiv},

year = {2019},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Gale2022,
doi = {10.48550/ARXIV.2211.15841},

url = {https://arxiv.org/abs/2211.15841},

author = {Gale, Trevor and Narayanan, Deepak and Young, Cliff and Zaharia, Matei},

keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Distributed, Parallel, and Cluster Computing (cs.DC), FOS: Computer and information sciences, FOS: Computer and information sciences},

title = {MegaBlocks: Efficient Sparse Training with Mixture-of-Experts},

publisher = {arXiv},

year = {2022},

copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{mishra2021,
  doi = {10.48550/ARXIV.2104.08378},
  
  url = {https://arxiv.org/abs/2104.08378},
  
  author = {Mishra, Asit and Latorre, Jorge Albericio and Pool, Jeff and Stosic, Darko and Stosic, Dusan and Venkatesh, Ganesh and Yu, Chong and Micikevicius, Paulius},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Hardware Architecture (cs.AR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Accelerating Sparse Deep Neural Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{evci2019,
  doi = {10.48550/ARXIV.1911.11134},
  
  url = {https://arxiv.org/abs/1911.11134},
  
  author = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Rigging the Lottery: Making All Tickets Winners},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{Ebrahimi2021,
  doi = {10.48550/ARXIV.2111.08577},
  
  url = {https://arxiv.org/abs/2111.08577},
  
  author = {Ebrahimi, Abdolghani and Klabjan, Diego},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neuron-based Pruning of Deep Neural Networks with Better Generalization using Kronecker Factored Curvature Approximation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{narang2017,
  doi = {10.48550/ARXIV.1704.05119},
  
  url = {https://arxiv.org/abs/1704.05119},
  
  author = {Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Exploring Sparsity in Recurrent Neural Networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
