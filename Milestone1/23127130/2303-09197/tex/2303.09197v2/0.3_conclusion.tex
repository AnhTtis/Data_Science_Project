\section{Conclusion}
\label{sec:conclusion}

This paper has proposed a formalisation of acyclic abstract argumentation systems in the action language of~\cite{sarmiento_action_2022-1}, establishing its formal properties: it first allows increasing the expressiveness of these models, through the integration of temporality, making it possible to examine the effect of the order of the argument enunciation. Moreover, it allows us to exploit the notion of causality associated to the action language, offering the possibility to give rich information about the argument acceptance or rejection and  justifications about the latter. The paper has proposed two types of graphical representations of the argumentation process that can be used as visual support, opening the way for new forms of argumentation explanations. 

Future works will aim at developing such explanations, applying the principles developed  in the context of eXplainable Artificial Intelligence (XAI), e.g. detailed in \cite{miller_explanation_2018}: causal chains are established as  essential for explanations, but they must also be short. The question of which relations to emphasise remains open, as well as the way  in which they can be used to define contrastive explanations, requiring to be able to reason about counterfactual scenarios. 
%\mj{pas très inspirée par la conclusion, à relire demain matin}
%\mj{pas plus inspirée...}

%In this paper we have proposed a rewriting of acyclic abstract argumentation systems in the action language of~\cite{sarmiento_action_2022-1} and an ASP implementation. Such a transformation allows addressing issues related to temporality as well as causality in the context of argumentation. These issues are particularly relevant to the generation of human-oriented explanations. With this in mind, we have proposed two graphical representations of the argumentative process that can be used as visual support for an explanation but do not provide one independently. 

%The perspectives of this work concern the use of this framework to generate explanations, accompanied or not by a graphical representation. The work in progress aims to define a contrastive explanation in the causal formalism and to identify the causes of the causal chain that are `useful' for the explanation. Finally, with the aim of applying our method to more complex real-life examples, we are also interested in extending the abstract argumentation framework, in particular by adding a binary relation, the support~\cite{amgoud2008bipolarity}. \mj{je trouve que le support tombe un peu comme un cheveu sur la soupe, non ?}

%\mj{This example illustrates the benefits of the proposed formalisation of AAF in  action language: beyond the increased expressiveness offered by the temporality integration, it allows to exploit the notion of actual causality and to get rich information about the argument acceptance or rejection.  }     
    
%\mj{en fait, on ne va pas, ici, taper sur la proposition ! On va déplacer ces deux paragraphes dans les perspectivses pour des travaux futurs, en soulignant que ce qui est proposé est hyper top, et fournit les outils pour faire encore mieux :-) et ici, on va juste dire que c'est top} 
%The previous example shows that causal chains are made of a large number of relations even if the number of states studied is small. In the context of eXplainable Artificial Intelligence (XAI), \cite{miller_explanation_2018} explains that the causal chain is essential for an explanation. However, he adds that an explanation must also be short. As a matter of fact, the question of which relations to emphasise remains to be solved so as to use this method to generate explanations. 
    
%Furthermore,~\cite{miller_explanation_2018} also states that an explanation should be contrastive. Therefore, in the explanation-seeking process, it is important to be able to reason about counterfactual scenarios in order to provide contrastive explanations. However, as described above, the causal approach used does not include such a reasoning.
