\begin{figure}[t] 
    \centering
    \includegraphics[width=\linewidth]{Figures/epoch.png}
    \caption{
    \textbf{Illustration of $\partial_{\ell_t}$MELTR.} 
    The $\partial_{\ell_t}$MELTR$(\boldsymbol{\ell};\phi$) values for the pretext task losses are plotted for each epoch, when training for video captioning. 
    The gradients are computed by taking the average for each data sample per epoch. 
    In the first few epochs, all task losses possess a similar scale of gradients. 
    As training continues, the most relevant losses, Decoder and M-Decoder, receives larger gradients, while the least relevant, CMFM, has the smallest gradient value.}
    \label{fig:epoch}
    \vspace{-3mm}
\end{figure}