\section{Related work}
\noindent \textbf{Video foundation models.}
With sufficient computational power and an abundant source of data, there have been attempts to build a single large-scale foundation model that can be adapted to diverse downstream tasks.
Along with the success of foundations models in the natural language processing domain~\cite{brown2020language,chen2021evaluating,devlin2019bert} and in computer vision~\cite{bertasius2021space,jia2021scaling,radford2021learning}, video data has become another data type of interest, as it has grown in scale due to numerous internet video-sharing platforms.
Accordingly, several methods to train a video foundation model have been proposed.
Due to the innate multi-modality of video data, \textit{i.e.}, a combination of visual $\cdot$ vocal $\cdot$ textual context, most works have centered around the variations of the cross-modal attention mechanism \cite{akbari2021vatt,bertasius2021space,gabeur2020multi,luo2020univl,neimark2021video,tan2021look,wei2020multi,yang2021taco}.
In addition, as most video data lack proper labels or descriptions, contrastive learning methods were studied to learn meaningful feature representations or enhance video-text alignment in a self-supervised manner \cite{akbari2021vatt,kuang2021video,luo2020univl,yang2021taco}.

More specifically, MERLOT \cite{zellers2021merlot} proposed a multi-modal representation learning method for visual commonsense reasoning, which also performed well in twelve video reasoning tasks.
VATT \cite{akbari2021vatt} introduced a multi-modal learning method via contrastive learning. 
The pre-trained model performed well in a variety of vision tasks from image classification to video action recognition and zero-shot video retrieval.
Another representative work, UniVL \cite{luo2020univl} proposed a straightforward pre-training method with auxiliary loss functions. 
After fine-tuning on a specific task, the pre-trained model performed outstandingly in a wide range of tasks of text-to-video retrieval, action segmentation, action step localization, video sentiment analysis, and video captioning.
Other foundation models for multiple video tasks include \cite{li2020hero,sun2019learning,sun2019videobert,zhu2020actbert,fu2021violet,wang2022all}. 

\noindent \textbf{Auxiliary learning.}
In order to enhance the performance of one or a multitude of primary tasks, auxiliary learning methods can be incorporated.
\cite{ruder2017overview} introduced Multi-task learning (MTL) to the deep neural networks by training a single model with multiple task losses to assist learning on the main task.
Such a method is generally adapted to pre-train the foundation models in the self-supervised manner~\cite{li2020hero,sun2019learning,sun2019videobert,zhu2020actbert,fu2021violet,wang2022all}.
However, these various pretext task losses used in the pre-training phase are ignored in the fine-tuning phase, and only the primary task loss is minimized.

Recently, meta-learning methods have been introduced for auxiliary learning.
\cite{liu2019self,navon2020auxiliary,shu2019meta} proposed a meta-learning method in which the model learns auxiliary tasks to generalize well to unseen data. 
In these settings, a separate subset of data is held out as the primary task, while the others are used as auxiliary tasks that aid the primary task's performance.
Similar methods were adopted for computer vision tasks such as semantic segmentation \cite{xu2021leveraging}.
Other domain applications include navigation tasks with reinforcement learning \cite{ye2021auxiliary}, or self-supervised learning methods on graph data \cite{hwang2020self}.