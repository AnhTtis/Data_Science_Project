\subsection{Objective function and optimization}
\label{subsec:objective}
MELTR learns how to fine-tune a model by non-linearly combining the auxiliary losses.
This can be viewed as hyperparameter optimization, which can be formulated as a bi-level optimization given as:
\begin{equation}
    \begin{split}
        & \phi^* = \argmin_{\phi} \:\pri(w^*(\phi)) \\
        \text{s.t.} \:\: & w^*(\phi) = \argmin_w \:\aux(w, \phi),
    \end{split}
    \label{eq:objective_function}
\end{equation}
where $\phi$ denotes the (meta) parameter of MELTR, and $w$ denotes the parameters of our backbone foundation model.
Then, we adopt one variant of the Approximate Implicit Differentiation (AID) scheme to optimize \eqref{eq:objective_function}.
Specifically, to optimize \eqref{eq:objective_function}, we first factorize the \textit{hypergradient}, which is the gradient of $\pri$ with respect to $\phi$ as $\nabla_\phi \pri = \nabla_w \pri \cdot \nabla_\phi w^*$, where
$\nabla_\phi w^* = -(\nabla_w^2 \aux)^{-1} \cdot \nabla_\phi\nabla_w \aux$ by the implicit function theorem (IFT).
Then, the hypergradient can be written as:
\begin{equation}
    \nabla_\phi \pri(w^*(\phi)) = -\nabla_w \pri \cdot \left(\nabla_w^2 \aux\right)^{-1} \cdot \nabla_\phi\nabla_w \aux.
    \label{eq:hypergrad}
\end{equation}
The evaluation of hypergradient entails the computation of the inverse of second-order derivatives.
In the literature~\cite{lorraine2020optimizing,navon2020auxiliary}, to accelerate the computation, the Neumann series is commonly adopted as:
\begin{equation}
    \left(\nabla_w^2 \aux\right)^{-1} = \lim_{i \rightarrow \infty} \sum_{j=0}^i \left(\mathrm{I} - \nabla_w^2 \aux\right)^j.
    \label{eq:neumann}
\end{equation}
In practice, the summation of an infinite series in \eqref{eq:neumann} is approximated by a finite sequence.
For instance, the number of iterations $i$ is usually truncated to a small integer (\textit{e.g.}, $i = 3$ in \cite{navon2020auxiliary}) in exchange for slight performance decay. 

\noindent However, this still requires considerable amount of time in the iterative computation of the Hessian matrix in \eqref{eq:neumann}.
We further simplify it by approximating the Hessian matrix in \eqref{eq:hypergrad} as the identity matrix $\mathrm{I}$. 
Then, our approximated gradient is given as follows:
\begin{equation}
    \nabla_\phi \pri(w^*(\phi)) \approx - \nabla_w \pri \cdot \nabla_\phi\nabla_w \aux.
    \label{eq:approximated}
\end{equation}
This completely removes the need for computation of the inverse Hessian matrix, which otherwise would have required a time complexity of $O(n^3)$. In our experiments, we observe that there is no significant degradation in terms of the performance of a fine-tuned model, see in Section~\ref{subsec:efficiency}.

Finally, with the approximated hypergradient \eqref{eq:approximated}, we utilize one variant of the AID scheme as an efficient optimization algorithm for MELTR.
We first optimize $w$ for $K$ steps by:
\begin{equation}
    w^{(k+1)} = w^{(k)} - \alpha \cdot \nabla_{w} \aux.
    \label{eq:update_w}
\end{equation}
After $K$ steps of \eqref{eq:update_w}, we then optimize for $\phi$ with:
\begin{equation}
    \begin{split}
        \phi^* & = \phi - \beta \cdot \nabla_\phi\pri(w^{(K)}(\phi)) \\
        & = \phi + \beta \cdot\left(\nabla_w\pri\cdot \nabla_\phi\nabla_w \aux\right),
    \end{split}
    \label{eq:update_phi}
\end{equation}
where $\alpha$ and $\beta$ are the learning rates of the backbone foundation model and MELTR, respectively.
The pseudo-code of our training scheme is provided in Algorithm~\ref{alg:main}.