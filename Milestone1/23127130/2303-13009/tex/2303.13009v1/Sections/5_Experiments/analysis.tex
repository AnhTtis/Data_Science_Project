\subsection{Analysis on MELTR}

We discuss \textbf{Q2} by analyzing how MELTR combines the losses.
We first analyze the non-linear relationship between the input and output loss values of MELTR, and examine how MELTR adaptively re-weights the auxiliary tasks.
Note, we use MELTR trained for the video captioning task on YouCook2 for these analyses, and abbreviate $\partial_{\ell_t} \text{MELTR}(\Ell;\phi):=\frac{\partial}{\partial \ell_t} \text{MELTR}(\Ell;\phi)$ hereafter.

\input{Tables/sentiment}
\input{Figures/epoch}

\noindent \textbf{Non-linear loss transformation.}
Figure~\ref{fig:scale}(a) shows that all auxiliary losses are positively correlated with the output loss. 
We can also observe that the output and input are non-linearly correlated.
On the other hand, Figure~\ref{fig:scale}(b) shows that $\partial_{\ell_t} \text{MELTR}(\Ell;\phi)$ have relatively higher values around $\ell_t = 0.5$ and the gradient becomes smaller as $\ell_t$ increases.
This indicates that $\text{MELTR}$ guides the learner to focus on reasonably challenging samples and if the loss is too large, it becomes less sensitive (\ie, too large an input loss is interpreted as noise and it tends to be downweighted).
Also, given the primary task loss $\Ldecoder$ for video captioning, the MELTR is more sensitive to the change of $\Ldecoder$ and $\Lmdecoder$ rather than $\Lcmfm$.
In other words, MELTR learned that text generation task losses ($\Ldecoder$ and $\Lmdecoder$) are more relevant to the video captioning than masked frame generation ($\Lcmfm$).

\input{Tables/manual}

\noindent \textbf{Adaptive task re-weighting.}
As an extension of the above observation, we visualize $\partial_{\ell_t} \text{MELTR}(\Ell;\phi)$ for each epoch in Figure~\ref{fig:epoch}.
At the beginning of training, MELTR equally takes into account all the auxiliary tasks.
As training proceeds, MELTR evaluates that $\Ldecoder$ and $\Lmdecoder$ are effective for the primary loss $\Ldecoder$, while $\Lcmfm$ is relatively less beneficial, if not harmful.
This is consistent with our observation in Figure~\ref{fig:scale} since $\Ldecoder$ and $\Lmdecoder$ mainly conduct the text generation task while $\Lcmfm$ is for masked frame generation.

In Table~\ref{tab:manual}, we also compare MELTR with five manually designed multi-task learning schemes, each combining the task losses with different linear coefficients.
First, by comparing (A) and (B), an auxiliary loss $\Lmdecoder$ assists learning of the video captioning task.
However, (A) and (C) demonstrate that multi-task learning is not always beneficial, and it sometimes hinders fine-tuning if the auxiliary tasks include harmful task losses.
By dropping $\Lcmfm$ from (C), the model performance is slightly improved in (D).
Interestingly, this matches our observation that $\Lcmfm$ is disadvantageous for video captioning (Figure~\ref{fig:epoch}).
Furthermore, (E) outperforms (D), implying that re-weighting among the auxiliary tasks can be beneficial for multi-task learning.
Finally, our MELTR surpasses all the multi-task learning schemes above.
These experimental results indicate that MELTR effectively learns to fine-tune by adaptively re-weighting the auxiliary tasks, compared to the coarse and heuristically designed multi-task learning schemes.