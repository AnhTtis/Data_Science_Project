\subsection{Evaluation on downstream tasks}
\label{subsec:downstream}

We here answer \textbf{Q1} (the effectiveness of MELTR) by applying our framework to fine-tune the pretrained foundation models on various downstream tasks: text-to-video retrieval, video question answering, video captioning, and multi-modal sentiment analysis.

\noindent \textbf{Text-to-Video retrieval.} 
We evaluate text-to-video retrieval task performance on YouCook2 and MSRVTT.
Table~\ref{tab:cook_ret} shows that our method outperforms all the baseline models by plugging MELTR in the standard UniVL.
Specifically, the R@1 is improved by 4.8\% compared to UniVL, and 1.5\% compared to the previous SOTA, VideoCLIP~\cite{yang2021taco}.
Also, METLR with the regularization term $\mathcal{L}^\text{reg}$ improves all the performance metrics compared to MELTR$^-$, which does not use $\mathcal{L}^\text{reg}$.
This optional regularization term confines the loss value to a reasonable bound, which prevents meta-overfitting.

\input{Tables/qa}
\input{Tables/cap_cook}

In Table~\ref{tab:vtt_ret}, our model outperforms all the baselines including foundation models and task-specific methods in all the retrieval metrics.
Specifically, MELTR improved three baseline foundation models: UniVL, Violet, and All-in-one.
For each model, R@1 is improved by a margin of 7.3\%, 1.9\%, and 4.2\% on MSRVTT-7k by plugging in MELTR.
The R@1 is also improved by a large margin of 4.8\%, 7.3\%, and 3.9\% respectively on YouCook2, MSRVTT-7k, and MSRVTT-9k as well, compared to the standard UniVL variants denoted UniVL-Joint or UniVL-Align.

\noindent \textbf{Video question answering.}
We experiment video question answering on TGIF-QA and MSVD-QA in Table~\ref{tab:qa}.
Plugging MELTR in the foundation model outperforms all the baselines.
Especially in MSVD-QA, MELTR obtains a large margin of 3.8\% improvement over the standard Violet.

\noindent \textbf{Video captioning.}
In Table~\ref{tab:cook_cap} and Table~\ref{tab:vtt_cap}, we evaluate video captioning task performance on YouCook2 and MSRVTT.
In the case of YouCook2, we conduct experiments on the `video-input-only' setting and additionally experiment on `video + text (transcript)' input, following previous works.
MELTR outperforms all the baseline models, in terms of all metrics.
In the case of MSRVTT, the performance of MELTR significantly improves BLEU scores, which are the major performance metric, BLEU-4.

\input{Tables/cap_vtt}
\input{Figures/scale}

\noindent \textbf{Multi-modal sentiment analysis.}
We also experiment the multi-modal sentiment analysis task on CMU-MOSI.
Table~\ref{tab:sentiment} shows that MELTR surpasses all the baselines.
These experimental results indicate that MELTR is successful in adaptively combining the auxiliary losses across backbone model architectures on various tasks.