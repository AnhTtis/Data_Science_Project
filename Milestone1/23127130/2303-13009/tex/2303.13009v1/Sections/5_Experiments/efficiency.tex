\subsection{Efficient optimization algorithm}
\label{subsec:efficiency}

We also discuss the optimization algorithms for training MELTR to answer the last question \textbf{Q3}.
We compare various bi-level optimization algorithms based on ITD or AID schemes.
To analyze the efficiency, we measure the latency of an epoch and the performance on the text-to-video retrieval task with MSRVTT-7k.
We identically use the MELTR module as the loss combining network with all optimization algorithms for fair comparisons.
We also compare with the multi-task learning setting where the model is fine-tuned with a linearly summed loss.

In Table~\ref{tab:efficiency}, the multi-task learning (MTL) method is faster than meta-learning-based algorithms (denoted by `\textbf{MELTR} + $\alpha$')
since MTL is formulated as a uni-level optimization problem.
We observe that all MELTR with various bi-level optimization schemes outperform a Multi-task Learning in terms of the target task performance R@1.
Among the bi-level optimization schemes, our training scheme denoted as \textbf{MELTR + AID-FP-Lite$^{\dagger}$} introduces only 4.9\% overhead in training time than multi-task learning, while improving performance by 2.4\%.
This is a significant improvement considering that Meta-Weight Net (ITD) takes longer than twice the time required by Multi-task Learning and AID-FP-Lite.
Our optimization in Algorithm~\ref{alg:opt}, which approximates $\nabla^2_w \aux$ in \eqref{eq:hypergrad} with the identity matrix $\mathrm{I}$, is the fastest bi-level optimization scheme in Table~\ref{tab:efficiency} while achieving a strong R@1 performance. 