\subsection{Bi-level optimization and hypergradient approximation}
Bi-level optimization commonly arises in meta-learning and hyperparameter optimization.
One general class of bi-level problems is given as:
\begin{equation}
    \begin{split}
        & \min_{\phi \in \mathrm{\Phi}} f(\phi) := \mathcal{L}_\mathcal{V}(w^*(\phi), \phi) \\
        \text{s.t.} & \:\: w^*(\phi) = \argmin_{w} \mathcal{L}_\mathcal{T}(w, \phi),
    \end{split}
    \label{eq:bilevel1}
\end{equation}
where $\phi \in \mathrm{\Phi}$ is an upper-level decision vector, and $w \in \bbR^d$ is a lower-level decision vector. 
 $\mathcal{L}_\mathcal{V}:\bbR^d \times \mathrm{\phi} \rightarrow \bbR$ and $\mathcal{L}_\mathcal{T}: \bbR^d \times \mathrm{\phi} \rightarrow \bbR$ are upper-level and lower-level loss functions, respectively. 
For instance, in hyperparameter optimization, $\phi$ is a set of hyperparameters and $w$ is model parameters.
$\mathcal{L}_\mathcal{T}$ and $\mathcal{L}_\mathcal{V}$ can be mapped to training and validation loss functions, respectively.

Grazzi \etal. \cite{grazzi2020iteration} have investigated bi-level optimization algorithms from the {\em hypergradient} approximation perspective. 
Hypergradient is the gradient of upper-level objective (\textit{i.e.}, $\nabla \LcVc$) and it is used for updating upper-level decision vector $\phi$.
Popular approaches in the literature~\cite{grazzi2020iteration} can be categorized into two groups: Iterative Differentiation (ITD) and Approximate Implicit Differentiation (AID).

\noindent \textbf{Iterative Differentiation}~\cite{finn2017model,franceschi2017forward,franceschi2018bilevel,maclaurin2015gradient,shu2019meta}.
This algorithm unrolls the upper-level optimization into two stages by defining a fixed-point parameter, $\hat{w}_k(\phi)$, where $k$ denotes the upper-level optimization step.
$\hat{w}_k(\phi)$ is derived by taking iterative learning steps from $w_k$.
Then, assuming this as a contraction mapping with respect to $w_k$~\cite{grazzi2020iteration}, the hypergradient $\nabla \mathcal{L}_\mathcal{V}(w_k, \phi_k)$ can be approximated with $\nabla \mathcal{L}_\mathcal{V}(\hat{w}_k, \phi_k)$, as in Figure \ref{fig:itd_aid}(a) step 2.
Then, with the updated upper-level decision vector $\phi_{k+1}$, model parameter $w_{k+1}$ is computed in the final step 3. 

\noindent \textbf{Approximate Implicit Differentiation}~\cite{lorraine2020optimizing,pedregosa2016hyperparameter,rajeswaran2019meta}.
In this optimization scheme, the hypergradient $\nabla \mathcal{L}_\mathcal{V}(w_k, \phi_k)$ is factorized by the implicit function theorem (IFT). 
This is then solved with a 2-step algorithm which requires inverse Hessian computation (Figure \ref{fig:itd_aid}(b)).
In practice, the inverse Hessian matrix is generally approximated to avoid its computation overhead of $O(n^3)$.
Then, similarly to ITD, model parameter $w_{k}$ is updated to $w_{k+1}$.