\subsection{UniVL}
\label{subsec:univl}
UniVL~\cite{luo2020univl} is a video foundation model pre-trained on the HowTo100M~\cite{miech2019howto100m} dataset via multi-modal self-supervised learning.
It is a unified video and language pre-training model for both video understanding and text generation tasks.
It consists of four transformer-based modules (two single-modal encoders, a cross-modal encoder, and a decoder).
It is pre-trained with five pretext tasks including the video-text joint ($\Ljoint$), the conditioned masked language model (CMLM; $\Lcmlm$), the conditioned masked frame model (CMFM; $\Lcmfm$), a video-text alignment ($\Lalign$), and the language generation task ($\Ldecoder$).
UniVL trains the model simultaneously for five pretext tasks by optimizing the sum of pretext loss functions given as:
\begin{align}
    \Lunivl &= \Ljoint + \Lcmlm + \Lcmfm + \Lalign + \Ldecoder.
\end{align}
Although UniVL minimizes \textit{multiple} pretext loss functions during pre-training, 
it optimizes only \textit{one} target task loss for fine-tuning, \eg, $\Lalign$ for video retrieval and $\Ldecoder$ for video captioning.
That is, other loss functions, which are potentially helpful for the target downstream task, are not utilized during fine-tuning.
This observation motivates our framework that automatically learns how to combine multiple losses for fine-tuning.
This can be viewed as hyperparameter optimization via meta-learning.