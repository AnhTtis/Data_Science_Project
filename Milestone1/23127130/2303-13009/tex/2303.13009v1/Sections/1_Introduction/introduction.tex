\section{Introduction}
Large-scale models trained on a huge amount of data have gained attention due to their adaptability to a wide range of downstream tasks. 
As introduced in~\cite{bommasani2021opportunities}, deep learning models with the generalizability are referred to as foundation models. 
In recent years, several foundation models for various domains have been proposed (\textit{e.g.}, \cite{bertasius2021space,brown2020language} for natural language processing, \cite{radford2021learning,jia2021scaling} for images and language, and \cite{akbari2021vatt,luo2020univl,zellers2021merlot} for videos) and they mainly focus on \textit{pretrain} the model often with various \textit{multiple} pretext tasks. 
On the other hand, strategies for fine-tuning on downstream tasks are less explored.
For instance, a recently proposed video foundation model UniVL~\cite{luo2020univl} is pretrained with a \textit{linear} combination of several pretext tasks such as text-video alignment, masked language/frame modeling, and caption generation.
However, like other domains, fine-tuning is simply performed by minimizing a \textit{single} target loss. 
Other potentially beneficial pretext tasks have remained largely unexplored for fine-tuning. 

Auxiliary learning is a natural way to utilize multiple pretext task losses for learning.
Contrary to multi-task learning that aims for generalization across tasks, auxiliary learning focuses only on the primary task by taking advantage of several auxiliary tasks.
Most auxiliary learning frameworks~\cite{liebel2018auxiliary,Toshniwal2017} manually selected auxiliary tasks, which require domain knowledge and may not always be beneficial for the primary task.
To automate task selection, meta learning was integrated into auxiliary learning~\cite{liu2019self,navon2020auxiliary,shu2019meta}.
Here, the model learns to adaptively leverage multiple auxiliary tasks to assist learning of the primary task.
Likewise, the pretext task losses can be unified into a single auxiliary loss to be optimized in a way that helps the target downstream task.

To this end, we propose Meta Loss Transformer (MELTR), a plug-in module that \textit{automatically} and \textit{non-linearly} transforms various auxiliary losses into a unified loss.
MELTR built on Transformers~\cite{vaswani2017attention} takes the target task loss as well as pretext task losses as input and  learns their relationship via self-attention.
In other words, MELTR learns to fine-tune a foundation model by combining the primary task with multiple auxiliary tasks, and this can be viewed as a meta-learning (or `learning-to-learn') problem.
Similar to meta-learning-based auxiliary learning frameworks~\cite{hwang2020self,shu2019meta}, this can be formulated as a bi-level optimization problem, which generally involves a heavy computational cost due to the second-order derivative and its inverse, \textit{e.g.}, the inverse Hessian matrix. 
To circumvent this, we present an efficient training scheme that approximates the inverse Hessian matrix.
We further provide empirical analyses on the time-performance trade-off of various optimization algorithms.
 
To verify the generality of our proposed method, we apply it to three video foundation models: UniVL~\cite{luo2020univl}, Violet~\cite{fu2021violet}, and All-in-one~\cite{wang2022all}.
These foundation models are originally pretrained with a linear combination of several pretext tasks such as text-video alignment, masked language/frame modeling, and caption generation.  
We experiment by fine-tuning on the text-to-video retrieval, video question answering, video captioning, and multi-modal sentiment analysis task with five datasets: YouCook2, MSRVTT, TGIF, MSVD, and CMU-MOSI.
For each task and dataset, our MELTR improves both previous foundation models and task-specific models by large margins.
Furthermore, our extensive qualitative analyses and ablation studies demonstrate that MELTR effectively learns to non-linearly combine pretext task losses, and adaptively re-weights them for the target downstream task.

\noindent To sum up, our \textbf{contributions} are threefold:
\begin{itemize}
    \item[\textbullet] We propose \textbf{ME}ta \textbf{Lo}ss \textbf{TR}ansformer (\textbf{MELTR}), a novel fine-tuning framework for video foundation models. 
    We also present an efficient optimization algorithm to alleviate the heavy computational cost of bi-level optimization.
    \item[\textbullet] We apply our framework to three video foundation models in four downstream tasks on five benchmark video datasets, where MELTR significantly outperforms the baselines fine-tuned with single-task and multi-task learning schemes.
    \item[\textbullet] We provide in-depth qualitative analyses on how MELTR non-linearly transforms individual loss functions and combines them into an effective unified loss for the target downstream task.
\end{itemize}