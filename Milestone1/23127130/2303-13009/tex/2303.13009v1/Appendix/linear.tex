\section{Effectiveness of transformer architecture}

In this section, we conduct an ablation study for the architecture type of MELTR on the text-to-video retrieval on MSRVTT by replacing the transformer with a linear layer.
Table~\ref{tab:apdx_arch} demonstrates that the transformer architecture improves by margin of 1\% than the linear layer by taking advantage of the self-attention layer.
Furthermore, we use both the scale embedding and task embedding (\textbf{SE} + \textbf{TE}) as the input of MELTR.
Only with \textbf{SE}, MELTR cannot consider task information and hence the performance decreases.
However, only with \textbf{TE}, MELTR cannot be trained since the input losses are not passed to MELTR, \textit{i.e.}, $\nabla_w \aux$ is always zero.