\section{Effectiveness of input type}
In this section, we provide a qualitative analysis for each input type (\textbf{SE} only, \textbf{TE} only, and \textbf{SE} + \textbf{TE}).
We visualize $\partial_{\ell_t} \text{MELTR}(\Ell;\phi)$ denoted in Section 5.2 of the main paper.
We calculate it in the same way as in the main paper for three input types on the video captioning task of YouCook2.

Figure~\ref{fig:apdx_grad} illustrates $\partial_{\ell_t} \text{MELTR}(\Ell;\phi)$ with respect to the scales of the input loss values.
When only the \textbf{SE} is fed in Figure~\ref{fig:apdx_grad}(a), MELTR tends to focus on reasonably challenging samples and downweight the noisy samples as discussed in Section 5.2 of the main paper.
Also note that without task information, we observe that the tendency is separated into two clusters with respect to $\partial_{\ell_t} \text{MELTR}(\Ell;\phi)$: ($\Lcmlm$, $\Lcmfm$) and ($\Ljoint$, $\Lmjoint$, $\Lalign$, $\Lmalign$, $\Ldecoder$, $\Lmdecoder$).
We believe that this is because the auxiliary losses are grouped based on the ranges of each loss, as seen in Figure~\ref{fig:box}, and MELTR distinguishes the tasks to some extent by learning the range of losses without the \textbf{TE}.
As for the \textbf{TE} in Figure~\ref{fig:apdx_grad}(b), $\partial_{\ell_t} \text{MELTR}(\Ell;\phi)$ is obviously invariant to the scale of losses and depend only on the task types.
$\Ldecoder$ and $\Lmdecoder$ rank high because they improve the performance on the video captioning task.
In Figure~\ref{fig:apdx_grad}(c), MELTR finally takes into account the tasks which are advantageous on the primary task, and guides a learner to focus on a reasonably challenging samples as discussed in Section 5.2 of the main paper, when using the summation of two embeddings (\textbf{SE} + \textbf{TE}).