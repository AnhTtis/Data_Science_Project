\section{Implementation Details}

\subsection{Backbone Foundation Models}
\noindent \textbf{UniVL~\cite{luo2020univl}.}
Our implementation is based on the official code of UniVL~\cite{univl_github} pretrained on the HowTo100M dataset~\cite{miech2019howto100m}.
As in the main paper, we use eight auxiliary loss functions: $\Ljoint$, $\Lmjoint$, $\Lalign$, $\Lmalign$, $\Lcmlm$, $\Lcmfm$, $\Ldecoder$, and $\Lmdecoder$.
For the primary losses for text-to-video retrieval and video captioning tasks are $\Lalign$ and $\Ldecoder$, respectively.
$\Lalign$ is also used as the primary loss function for multi-modal sentiment analysis.

\noindent \textbf{Violet~\cite{fu2021violet}.}
We implement MELTR based on the official Violet github~\cite{violet_github} pretrained on the YT-Temporal 180M~\cite{zellers2021merlot}, WebVid~\cite{bain2021frozen}, and CC3M~\cite{sharma2018conceptual}.
For text-to-video retrieval, we adopt three auxiliary losses: video-text matching loss, masked text modeling loss, and masked visual-token modeling. 
We use the former one as the primary task loss.
We use additional classification loss for video question answering.

\noindent \textbf{All-in-one~\cite{wang2022all}.}
Our implementation for All-in-one is based on \cite{all_github} and it is pretrained on WebVid~\cite{bain2021frozen}, YT-Temporal 180M~\cite{zellers2021merlot}, HowTo100M~\cite{miech2019howto100m}, CC3M~\cite{sharma2018conceptual}, CC12M~\cite{changpinyo2021cc12m}, COCO~\cite{lin2014microsoft}, VisualGenome~\cite{krishna2017visual}, and SBU~\cite{ordonez2011im2text}.
When conducting text-to-video retrieval task, video-text matching loss and masked language modeling loss are adopted and the former one is used as the primary loss.

\subsection{Evaluation metrics}
For the video retrieval task, we report the standard retrieval metrics, Recall at K (R@K) metric (K=1,5,10) and Median Rank (MedR).
Accuracy metric is reported for video question answering task which includes both multi-choice and open-ended questions.
As for video captioning, BLEU~\cite{papineni2002bleu}, METEOR~\cite{banerjee2005meteor}, ROUGE-L~\cite{lin2004rouge}, and CIDEr~\cite{vedantam2015cider} are reported.

\subsection{MELTR Details.}
We use the Adam~\cite{kingma2015adam} optimizer with an initial learning rate $\alpha$ = 3e-5 and $\beta$ = 1e-4 with a linear learning rate decay strategy.
For MELTR, we use one transformer encoder layer with 8 attention heads and 512 hidden dimensions.
We trained 40, 20, and 20 epochs on the text-to-video retrieval, video question answering, and video captioning tasks with 8 $\times$ Tesla A100 GPUs, respectively.
We search $\gamma$ in $\{0.1, 0.3, 0.5\}$ for the regularization term and use $K = 3$ in Eq. (13) of the main paper.