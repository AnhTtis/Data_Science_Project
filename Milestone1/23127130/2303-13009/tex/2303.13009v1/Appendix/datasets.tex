\section{Dataset Details}

\noindent \textbf{YouCook2}. 
YouCook2~\cite{zhou2018towards} consists of 2k videos, which cover 89 types of recipes. 
Each video contains multiple video clips accompanied by text descriptions. 
The train dataset contains 1,261 samples, and the test set contains 439 samples, respectively.

\noindent \textbf{MSRVTT.}
The original MSRVTT-full~\cite{xu2016msr} dataset, used on video captioning task, contains 6,513 train, 497 validation, and 2,990 test samples.
However, we have observed a wide range of dataset split variations throughout research on text-to-video retrieval.
One split variant randomly samples 1,000 clip-text pairs from the test set for evaluation and uses the rest of the 9,000 samples as train data~\cite{yu2018joint}, which is commonly denoted as the 1kA split.
On the other hand, the 1kB split uses the identical 1,000 test split of 1kA for the test, whereas the train set is a subset of 1kA's containing 6,656 samples~\cite{miech2018learning}.
Another commonly used data split also uses the identical 1,000 test set, while adopting both the train and validation set from the standard MSRVTT for training.
We evaluated our method on two split protocols most prominently observed in the literature, 1kA, and 7k.
For convenience, we denote the former as MSRVTT-9k and the latter as MSRVTT-7k.

\noindent \textbf{TGIF-QA.}
TGIF-QA~\cite{jang2017tgif} contains 165k QA pairs of animated GIFs.
The dataset provides three different subtasks:  TGIF-Action, TGIF-Transition and TGIF-Frame.
TGIF-Action is to identify repeated actions, TGIF-Transition is to identify the transition between states, and TGIF-Frame is to answer questions given a GIF frame.
TGIF-Action and TGIF-Transition are conducted under the multi-choice question answering setting, predicting the best answer given five options.
TGIF-Frame is experimented as the open-ended question answering with 1,540 most frequent answer candidates.

\noindent \textbf{MSVD-QA.}
MSVD-QA~\cite{xu2017video} contains 47k open-ended questions on 2k videos, derived from the original MSVD dataset~\cite{chen2011collecting}.
We construct the answer set with 1,000 most frequently appeared answers.

\noindent \textbf{CMU-MOSI.}
For the multi-modal sentiment analysis task, we adopt the CMU-MOSI dataset~\cite{zadeh2016mosi} which consists of 2,199 opinion video clips annotated with sentiment intensity values from -3 to 3.
