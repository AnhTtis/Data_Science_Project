\section{Experimental Evaluation}\label{sec:experiments}
Next, we experimentally investigate the information leakage of face recognition models with \textsc{Caia}. We provide our source code with all hyperparameters to reproduce the experiments and facilitate future research. More experimental details are provided in \cref{appx:experimental_details}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/accuracy_comparison.pdf}
    \caption{Attack accuracy for different target model architectures and CelebA attribute datasets. Results are averaged over three models and three attack datasets. Except for ResNet-18, the attacks are comparably successful on the different models. }
    \label{fig:accuracy_results}
\end{figure*}

\subsection{Experimental Setup}
\textbf{Datasets.} We used the CelebA facial attributes dataset~\citep{celeba} to train our target face recognition systems. CelebA contains images of 10,177 individuals and 40 binary attribute annotations per image. We selected the attributes \textit{gender} $=\{\text{female}, \text{male}\}$, \textit{eyeglasses} $=\{\text{no glasses}, \text{glasses}\}$, and  \textit{hair color} $= \{ \text{black}, \text{blond}, \text{brown}, \text{gray}\}$ as sensitive attributes, and additionally applied a pretrained \textit{FairFace}~\citep{karkkainen21fairface} classifier to add \textit{racial appearance} $= \{\text{Asian}, \text{Black}, \text{Indian}, \text{White}\}$ labels to each image. Since the provided attributes and labels are often inconsistent for samples of one identity, e.g., people do not always wear eyeglasses or might dye their hair, we created custom subsets for each sensitive attribute group by selecting an equal number of identities for each attribute value and removed samples with inconsistent labels. This is important for evaluation since otherwise the training samples of an identity depict various values for the sensitive attribute, and no clear ground-truth value can be defined. 

We trained various target models on these datasets to predict a person's identity. Since not every attribute is present with every identity, we selected the 100 identities for each attribute value of \textit{hair color}, \textit{eyeglasses}, and \textit{racial appearance}, respectively, with the most training samples available. For \textit{gender}, we selected 250 identities per attribute value. We also created datasets with 1,000 identities to see if more classes influence the attack success. Additional models were trained on the FaceScrub~\citep{facescrub} facial image dataset, which contains images of 530 identities with equal gender split. We split all datasets into 90\% for training and 10\% for measuring the prediction accuracy. Details on the different dataset statistics are provided in \cref{appx:dataset_details}.

To craft the attack datasets, we used the Flickr-Faces-HQ (FFHQ)~\cite{Karras_stylegan1} and CelebAHQ~\citep{karras18progressive} datasets. We generated and filtered images with attribute manipulations to collect 300 attack image tuples for each attribute group. We set the filter threshold $\tau=0.6$ in all experiments. We visualize randomly selected training and attack samples in \cref{appx:sample_visualizations}.

\textbf{Models.} We trained ResNet-18, ResNet-101, ResNet-152~\citep{resnet_he}, ResNeSt-101~\citep{zhang2020resnest}, and DenseNet169~\citep{densenet} target models. For each dataset-architecture combination, we trained three models with different seeds. We further trained ResNet-50 filter models on the CelebA attributes to classify \textit{gender}, \textit{glasses}, and \textit{hair color}. To mitigate the overconfidence of neural networks, we trained all filter models with label smoothing~\citep{szegedy16labelsmoothing, mueller19labelsmoothing} to calibrate the models and make them more suitable for the confidence threshold approach. To filter images depicting different \textit{racial appearances}, we relied on the pre-trained \textit{FairFace} classifier. The attack datasets were generated with Stable Diffusion v1.5~\citep{Rombach2022}.

To investigate effects of model robustness on information leakage, we trained adversarial robust models on FaceScrub with standard adversarial training~\citep{goodfellow15adv}. We perturbed training samples with Projected Gradient Descent~\citep{madry18pgd} with 7 steps, $\epsilon=4/255$, and step size $\alpha=1/255$\footnote{Corresponds to images with pixel values in range $[0,1]$.}.

\textbf{Metrics.} We computed the precision, recall, and F1 score for each attribute value, together with the overall prediction accuracy for all attributes. All experimental results are averaged over three models and three disjoint subsets of attack samples, resulting in nine runs per configuration.

\textbf{Baselines.} We took random guessing as a naive baseline. Since the sensitive attributes are equally distributed among the different identities, random guessing corresponds to the underlying prior attribute distribution. Since existing AIAs are not applicable to our use case, we instead performed the state-of-the-art Plug \& Play model inversion attack (PPA)~\citep{struppek22_mia} to synthesize characteristic samples for each class. We then used our filter models to predict the sensitive attribute value for each sample and take the majority vote for each targeted identity as the attribute prediction. We emphasize that, unlike \textsc{Caia}, PPA requires white-box access to the target model and a pre-trained GAN, for which we used the official StyleGAN2~\citep{Karras2019stylegan2} FFHQ model. Due to the high computational effort, we limit the PPA comparison to individual ResNet-101 models for each setting.

\begin{figure*}[t]
     \begin{subfigure}[c]{\textwidth}
         \centering
         \includegraphics[height=0.025\textwidth]{images/legend_alt.pdf}
     \end{subfigure}
     \begin{subfigure}[b]{0.45\textwidth}
        \centering
         \includegraphics[width=\textwidth]{images/barplots/resnet101_gender_ppa.pdf}
         \caption{Gender (CelebA)}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/barplots/resnet101_eyeglasses_ppa.pdf}
         \caption{Eyeglasses (CelebA)}
     \end{subfigure}
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/barplots/resnet101_hair_color_ppa.pdf}
         \caption{Hair Color (CelebA)}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/barplots/resnet101_race_ppa.pdf}
         \caption{Racial Appearance (CelebA)}
     \end{subfigure}
    \caption{Evaluation results for \textsc{Caia} performed on ResNet-101 CelebA models to infer four different target attributes. The black horizontal lines denote the standard deviation over nine runs. We further state random guessing (dashed line) and Plug and Play Attacks (PPA, green dots) for comparison. While \textsc{Caia} outperforms random guessing by a large margin, it extracts information on racial appearance and if someone is wearing eyeglasses even more reliably than the white-box PPA attack.}
    \label{resnet101_results}
\end{figure*}

\subsection{Extracting Sensitive Class Information}
In the main part of the paper, we focus on results on ResNet-101 models using FFHQ attack samples. We provide more detailed results for the other architectures, CelebAHQ as attack dataset, and experiments with 1,000 identities in \cref{appx:add_results_celeba}. We note that the attack success for an increased number of identities did not change substantially. The attack accuracy for different models and target attributes in \cref{fig:accuracy_results} demonstrates that \textsc{Caia} performed comparably well on different architectures and predicted the sensitive attribute values correctly in over 90\% of the cases for the attributes \textit{gender} and \textit{eyeglasses}, and about 80\% for the \textit{hair color} and \textit{racial appearance}. Only the attack results of ResNet-18 stood out and were a few percentage points lower than those of the other architectures, which we attribute to the small number of model parameters (only about a quarter of ResNet-101). Still, all attacks reliably inferred the sensitive attributes in most cases.

Next, we investigate the attribute leakage more closely. Therefore, we performed a more detailed analysis of the attribute leakage of ResNet-101 models, for which the results are depicted in \cref{resnet101_results}. For all four attributes, \textsc{Caia} significantly beat the random guessing baseline by a large margin. Whereas \textit{gender} and \textit{eyeglasses} were predicted correctly in about 94\% of the cases, \textit{racial appearance} could be inferred correctly in 84\% of the cases. The attack accuracy for \textit{hair color} was also about 82\% on average, but the attack success varied substantially between the different attribute values. Blond hair seems to be the hardest value to predict, which is not unexpected since hair colors have different shades of which blond might be the broadest one. Corresponding confusion matrices are visualized in \cref{appx:confusion_matrices}.

In contrast, we also investigated gradient-based model inversion attacks, here PPA, to compare to state-of-the-art white-box methods that reconstruct characteristic class inputs. On the ResNet-101 models, PPA achieved perfect attack results for inferring an identity's \textit{gender}. It also precisely revealed \textit{hair color} and outperformed the black-box \textsc{Caia} for both settings. However, for inferring whether an identity is wearing \textit{eyeglasses}, PPA fell significantly behind \textsc{Caia}. Regarding \textit{racial appearance}, PPA's attack accuracy was comparable to \textsc{Caia} but less consistent between different attribute values. We suspect the reason to be the uneven distribution of the \textit{racial appearance} and \textit{eyeglasses} attribute values in the underlying StyleGAN2's training data~\citep{karakas22fairstyle}. Since PPA starts from randomly sampled latent vectors, the biased attribute distributions also influence the attack success in terms of generating and inferring sensitive attributes. Nevertheless, \textsc{Caia} shows competitive and impressive performance given that it accesses the target model only in a black-box fashion and has no access to internal gradient information.

\subsection{Robustness Increases Privacy Leakage}
\begin{figure*}[t]
     \begin{subfigure}[c]{\textwidth}
         \centering
         \includegraphics[height=0.025\textwidth]{images/robust_legend.pdf}
     \end{subfigure}
     \begin{subfigure}[c]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/facescrub_results/facescrub_attack_accuracy.pdf}
         \caption{Gender (FaceScrub Cropped)}
         \label{fig:facescrub_cropped}
     \end{subfigure}
     \hfill
    \begin{subfigure}[c]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/facescrub_results/facescrub_uncropped_attack_accuracy.pdf}
         \caption{Gender (FaceScrub Uncropped)}
         \label{fig:facescrub_uncropped}
     \end{subfigure}

    \caption{Gender appearance inference results for \textsc{Caia} performed on ResNet-101 models trained on cropped \textbf{(a)} and uncropped \textbf{(b)} FaceScrub samples. We compared standard models to robust models trained with adversarial training (hatched bars). The results demonstrate that robust models indeed increase the information leakage (blue bars), even if the underlying models' prediction accuracy (orange bars) is significantly below that of non-robust models.}
    \label{fig:facescrub_results}
\end{figure*}

Since adversarial examples are a common weakness of deep learning models, adversarial training is widely adopted to make models robust against this security threat. To establish a connection between privacy and security, we now extend our investigation to robustly trained models and show that robustness even increases privacy leakage. However, training robust models requires larger datasets due to increased sample complexity~\citep{schmidt18advrobust}. The limited number of CelebA samples, which provides approx. 30 samples per identity, makes it difficult to train stable and robust models. Therefore, this section focuses on models trained on FaceScrub, which provides an average of 70 samples per identity and facilitates stable adversarial training. We trained ResNet-101 models on both cropped images containing only faces and uncropped images showing a large amount of content unrelated to an identity. For a visual comparison of the datasets, we refer the reader to \cref{appx:dataset_samples}.

The attack results for ResNet-101 models are shown in \cref{fig:facescrub_results}, while \cref{appx:add_results_Facescrub} presents results for other model architectures. Comparing the attack accuracy against non-robust models ($93.08\%$) with that of robust models ($96.62\%$) trained on cropped images suggests that robust models tend to leak more \textit{gender} information about their learned identities than non-robust models, even if the models' clean prediction accuracy is roughly five percentage points lower. A similar pattern holds for models trained on uncropped images, with robust models still exhibiting higher information leakage. However, it is important to note that the standard model's prediction accuracy ($78.75\%$) is significantly higher than that of robust models ($55.23\%$). This suggests that a model's prediction accuracy is not necessarily indicative of its privacy leakage. We hypothesize that robust models tend to exhibit higher privacy leakage due to their concentration on more robust features for their predictions. We formalize this hypothesis in the remaining section.

Our formal analysis builds upon the robust feature model for binary classifiers of \citet{ilyas19bugs}, which we extend to the privacy leakage setting. Let $\mathcal{M}:X \to Y$ be a model trained to predict a label $y\in Y$ for each input $x\in X$. We divide a model's inputs $x=(\tilde{x},\bar{x})$ into predictive features $\tilde{x}$ and non-predictive features $\bar{x}$. The $i$-th predictive feature $\tilde{x}^i$ is positively correlated with a sample's true label: $E_{(x,y)\sim \mathcal{D}} \left[ \, \hat{y}\cdot\mathcal{M}(\tilde{x}^i)_y \, \right] \geq \rho$ for a sufficiently large $\rho$. Here, we deviate slightly from our previous notation of $y$ and define $\hat{y}\in \{-1,1\}$ for each class, with $\hat{y}=1$ indicating the ground-truth label. We further assume that all model outputs $\mathcal{M}_y$ are centered around zero. Predictive features are (ideally) learned by a model to predict the label. Conversely, a non-predictive feature $\bar{x}^j$ has no correlation with the true label and should be ignored during inference if $E_{(x,y)\sim \mathcal{D}} \left[ \, \hat{y}\cdot\mathcal{M}(\bar{x}^j)_y \, \right] < \rho$. 

We further explore the nature of the predictive features by categorizing them into two groups: robust features $\tilde{x}_\textit{robust}$ and non-robust features $\tilde{x}_\textit{non-robust}$. Robust predictive features remain predictive even under adversarial perturbations $\delta$ with $\| \delta \| \leq \epsilon$, satisfying $E_{(x,y)\sim \mathcal{D}} \left[ \, \hat{y}\cdot\mathcal{M}(\tilde{x}_{robust}^i + \delta)_y \, \right] \geq \rho$, while non-robust predictive features lose their predictive power under adversarial perturbations. Neural networks rely not only on salient features, such as hair color and other facial characteristics, for image processing, but also on subtle, non-robust image features that can still be highly predictive.

Adversarial perturbations can significantly disrupt non-robust models' predictions by manipulating $\tilde{x}_\textit{non-robust}$ in an $\epsilon$-environment that is imperceptible to humans. Adversarial training can help reduce the impact of adversarial examples by focusing the model's prediction on $\tilde{x}_\textit{robust}$. However, we stress that adversarial training's potential to enhance model robustness comes at the cost of increased privacy leakage. To explain why adversarial training increases a model's privacy leakage, we hypothesize that human-recognizable, sensitive attributes are part of $\tilde{x}_\textit{robust}$. These attributes are highly predictive because they enable discrimination between samples of different classes based on the assumption that they remain constant within samples of a specific class. We also argue that these attributes are robust features since they are easily identifiable by humans, and small adversarial perturbations are unlikely to affect their value.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[height=0.6\textwidth, keepaspectratio]{images/integrated_gradients.pdf}
          \caption{Visualized Attribution with Integrated Gradients.}
          \label{fig:qualitative_ig}
     \end{subfigure}
     \hfill
    \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[height=0.6\textwidth, keepaspectratio]{images/rel_attribution.pdf}
          \caption{Relative Attribution for facial features.}
          \label{fig:quantative_ig}
     \end{subfigure}
    \caption{The comparison of absolute attribution based on integrated gradients between a robust and a standard ResNet-101 model in \textbf{(a)} shows that robust models assign perceptually more attuned attribution to facial features. A quantitative comparison of relative attribution in \textbf{(b)} further highlights that the robust model assigns more attribution to unique facial features, such as eyes and hair, while the standard model assigns most attribution to non-specific parts of the skin.}
    \label{fig:integrated_gradients}
\end{figure*}

In the context of face recognition, \textit{gender} is a sensitive attribute that is highly predictive, enabling us to distinguish individuals from people with other \textit{gender} appearances. Furthermore, it is a robust feature, as altering a person's \textit{gender} appearance in an image would require significant changes to the pixels. To showcase the importance of robust features in robust models, we utilized the axiomatic attribution method Integrated Gradients~\citep{sundararajan17ig} on two ResNet-101 models, one trained using standard training, and the other with adversarial training. \cref{fig:qualitative_ig} visualizes the computed gradients for four input images from our attack datasets. While the standard model relies on noisy, non-robust features, such as image background, the robust model concentrates more on facial attributes, including hair, eyes, and facial structure.

To also provide quantitative support for our analysis, we used Integrated Gradients to measure a model's relative attribution to specific image parts. For this, we applied a pre-trained face segmentation model~\citep{lee20maskgan} to locate various attributes in facial images. Be $H_{Z}(x) \in \{0,1\}^{H\times W}$ the binary segmentation mask for image $x$ and attribute class $Z$, such as a person's hair. Be further $\mathcal{A}(\mathcal{M}, x)\in \mathbb{R}^{H\times W}$ the absolute pixel-wise attribution by Integrated Gradients for model $\mathcal{M}$ and image $x\in \mathbb{R}^{H\times W}$. Let also $\| \cdot \|_1$ denote the sum norm and $\odot$ the Hadamard product. The relative attribution for a dataset $X$ is then computed by: 

\begin{equation}
    \mathcal{A}_\mathit{rel}(\mathcal{M}, X) = \frac{1}{|X|} \sum_{x\in X} \frac{\| \mathcal{A}(\mathcal{M}, x) \odot H_Z(x) \|_1}{\| \mathcal{A}(\mathcal{M}, x) \|_1} \, .
\end{equation}

By analyzing the relative attribution of seven attributes in 100 FFHQ samples and over 530 target identities of ResNet-101 models, \cref{fig:quantative_ig} demonstrates that robust models assign more importance to distinct features like eyes or hair, while standard models prioritize general skin areas. This highlights the greater importance placed by robustly trained models on sensitive attributes encoded in images and reflected in their outputs. As \textsc{Caia} exploits the differences in logits, robust models appear to be more susceptible to such privacy attacks and leak more sensitive attribute information. Thus, there exists a trade-off between model robustness and sensitive class information leakage.