\section{Class Attributes Inference Attacks}\label{sec:caia}
We now introduce the novel \textit{\textbf{C}lass \textbf{A}ttribute \textbf{I}nference \textbf{A}ttack} (\textsc{Caia}), which consists of two stages. First, the adversary generates a set of attack samples by creating different versions of images through a generative approach that alters the sensitive attribute values. In the second stage, these attack samples are used to infer the sensitive attribute values for the identities in a face recognition model. Before delving into the details, we outline our general threat model. 

\textbf{Adversary's Goal.}
Let $\mathcal{M}\colon X \to \mathbb{R}^{|Y|}$ denote the trained target image classifier, which takes input images $x\in X$ and computes prediction scores for each class label $y \in Y$. The model's training data $S_\mathit{train}$ consisted of labeled data samples $(x, y) \sim \mathcal{D}$. The underlying attack assumption is that samples of a certain class share a constant sensitive attribute value $z \in Z$, which is not part of the class label but is implicitly encoded in the image features. For example, a face recognition model is trained to predict the identity $y$ of each facial image $x$. A sensitive attribute $z$ in this context might be the gender appearance or hair color of a specific identity. The attack goal is to infer the value of this sensitive attribute for each individual class. 

\textbf{Adversary's Capabilities.} The adversary has only black-box access to the target model, i.e., the adversary can query the target model and observe its output logits. Furthermore, the adversary knows the domain of the target model's training data, e.g., facial images, but not the exact training data distribution. For the sensitive attributes, the adversary defines an individual set of possible values to infer. We emphasize that \textsc{Caia} is model- and task-agnostic and does not require white-box access to the target model. Information about the prior distribution of the sensitive attributes is neither available nor required.

\subsection{Crafting Attack Samples}\label{sec:craft_attack_samples}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/attack_concept.pdf}
    \caption{Overview of the class attribute inference step. Each image tuple from the attack set is fed sequentially into the target model to compute the logits for the target class. The relative advantage is then computed by subtracting the second highest logit value from the maximum value, and this difference is added to a running sum for each sensitive attribute value. The final prediction for the sensitive attribute is the value with the highest relative advantage computed across all attack samples.}
    \label{fig:inferring_concept}
\end{figure*}

While it is easy for tabular data to vary the values of attributes in an inference sample, this is a non-trivial task on images since attributes are encoded in pixels, and multiple pixels define not only a single attribute but a set of possibly entangled attributes. For example, human faces contain information such as age and skin color, among many more. Furthermore, changing the value for any of these attributes requires semantically consistent changes in multiple pixels. 

To enable meaningful image manipulations, we utilize recent advances in text-to-image synthesis. Systems like Stable Diffusion~\citep{Rombach2022} are able to generate high-quality images following a user-provided text description $p$. \citet{mokady22nullinversion} recently proposed \textit{Null-text Inversion} to encode real images into the domain of diffusion models and enable text-based editing while keeping the overall image composition and content fixed. In combination with \textit{Prompt-to-Prompt}~\citep{hertz2022prompt}, it allows a user to instruct image edits $x_\mathit{edit}=E(x, p)$ on real images $x$ conditioned on a description $p$. We use \textit{Null-text Inversion} to generate variations of existing images by changing only the sensitive attribute values, while aiming to leave other image aspects unchanged. 

\cref{fig:synthesis_concept} illustrates the crafting process for the attack samples. Formally, the adversary has access to a data distribution $\mathcal{\hat{D}}$ from which to sample images $x$. Note that the attack does not require the attack distribution $\mathcal{\hat{D}}$ to be the same as the training data distribution $\mathcal{D}$ but only that both data distributions are from the same domain, e.g., facial images. As we will show in our experimental evaluation, even if the style, size, and quality of images between both distributions vary significantly, the attack is still highly successful. The adversary defines the target attribute with a set of $k$ possible attribute values $Z=\{z_1,\ldots, z_k \}$ and corresponding edit prompts $p_z$ that describe the general domain and explicitly state an attribute value $z\in Z$. For example, $p_z=\text{"A photo of a person, } \langle \text{gender}\rangle\text{"}$, where $\langle\text{gender}\rangle$ is replaced by $z\in \{\text{female appearance}, \text{male appearance}\}$. The candidate dataset $S_\mathit{candidate}$ is then constructed as $S_\mathit{candidate}=\{E(x, p_z) | z\in Z, x \sim \mathcal{\hat{D}}\}$, consisting of image tuples $\mathbf{x}=(x_1,\ldots,x_k)$, each containing $k$ images with different sensitive attribute values. 

However, the attribute manipulation might not always succeed in changing an image attribute to the desired value. This can be due to interfering concepts already present in the image that are strongly entangled with the target attribute. For example, changing the hair color of a person with a dark skin tone to blonde often fails because such attribute combinations are rather rare in the actual training data, which is also reflected in the underlying diffusion model. We, therefore, employ a filtering approach, i.e., filtering out all sample tuples $\mathbf{x}$ that are not correctly depicting the various attribute values. For this, we use a trained attribute classifier $\mathcal{F}_\tau \colon X \to Z$ to create a subset $S_\mathit{attack}=\{\mathbf{x} \in S_\mathit{candidate} | \mathcal{F}_\tau(x_z)=z, \forall z\in Z \}$ of sample tuples $\mathbf{x}=(x_1,\ldots,x_k)$. We also add a threshold $\tau$ on the softmax scores of the attribute classifier and classify only predictions with a softmax score $\geq \tau$ as correct. This removes images for which the attribute classifier has only low confidence in its prediction.


\subsection{Revealing Sensitive Attributes}
With the attack samples, we can now begin with inferring the sensitive class attributes learned by the target classifier. \cref{fig:inferring_concept} illustrates the basic concept of the inference. While our filtering approach ensures that the variations of an image depict the different values of the sensitive attribute, other confounding behavior might still remain unintentionally. For example, changing a person's hair color to gray might also influence the depicted age. To mitigate such influences, we predict the sensitive attribute with a variety of different attack samples to, in turn, reduce the influence of these confounding factors over a larger number of samples.

Be $\mathcal{M}(x)_y\colon X \to \mathbb{R}$ the pre-softmax logits computed by the target model $\mathcal{M}$ on image $x$ for class $y$. To infer the sensitive attribute value $z$ of class $y$, we query the target model consecutively with multiple sample tuples, $\mathbf{x}\in S_\mathit{attack}$. For each $x_z \in \mathbf{x}$, we compute its relative advantage $A(\mathbf{x})_z$ by
\begin{equation}
    A(\mathbf{x})_z = \max\left(0, \, \mathcal{M}(x_z)_y - \max_{\tilde{x} \in \mathbf{x}, \tilde{x}\neq x_z}{\mathcal{M}(\tilde{x})_y} \right).
\end{equation}


The relative advantage computes the difference between the highest and the second-highest logit values and assigns this difference to the attribute sample $x_z \in \mathbf{x}$ with the highest logit. For all other samples $x\in\mathbf{x}$, the relative advantage is set to zero. \cref{fig:inferring_concept} illustrates the relative advantage computation for a single $\mathbf{x}$. The final attribute prediction $\hat{z}$ is then done by taking the attribute with the highest relative advantage summed up over all attack samples: $\hat{z}=\argmax_{z\in Z} \sum_{\mathbf{x} \in S_\mathit{attack}}A(\mathbf{x})_z$. We emphasize that only a single forward pass for all attack samples is sufficient to compute the relative advantage for all classes, which makes the inferring step computationally very cheap, e.g., the gender inference for a ResNet-101 model and 500 identities took roughly 2 seconds in our experiments.