\section{Introduction}
Classifying images with neural networks is widely adopted in various domains~\citep{esteva21medical, ibrahim22cancer, baumann18roads, barua21covid19}. Face recognition systems, for example, take facial images as input and attempt to predict the depicted person's identity. In the pursuit of enhancing a model's predictive performance, privacy concerns of the acquired knowledge are often disregarded and moved into the background. However, correctly assessing and mitigating the risk of compromising private information is crucial in privacy-sensitive domains, as neglect can lead to the disclosure of private information~\citep{shokri2017, struppek22_mia, hintersdorf_clip}. For example, smart home devices contain face recognition models for access control. Users expect these models to recognize them reliably, but at the same time to not reveal information about their appearance to third parties. However, this assumption does not necessarily hold true, and malicious parties could extract sensitive features about users by simply interacting with the model in a black-box fashion.

We investigate the privacy leakage of image classifiers and demonstrate that models indeed leak sensitive class information even without any specific information about the classes, training samples, or attribute distributions available. We focus on face recognition models and show that these models reveal sensitive details within their outputs, such as gender, hair color, and racial appearance, for the different identities they have been trained to recognize. 

Within our analysis, we introduce a \textit{\textbf{C}lass \textbf{A}ttribute \textbf{I}nference \textbf{A}ttack} (\textsc{Caia}), which enables an adversary to infer sensitive attributes of a class with high accuracy. Phrased differently, \textsc{Caia} allows creating a profile of the individual classes by interacting with the trained classifier through extraction of class attributes that have not been explicitly part of the training objective. We utilize recent advances in text-to-image synthesis to craft images that only differ in one attribute by editing real images with textual guidance. We then exploit that image classifiers, as we show, assign higher logits to inputs that share the sensitive attribute with the training samples of a class, which allows us to infer class information by only observing the input-output relation of the model. Compared to related inference attacks~\citep{fredrikson14pharmacogenetics,fredriskon15mia,secret_revealer, struppek22_mia}, \textsc{Caia} is model-agnostic and requires only black-box access and basic domain knowledge. Once the attack images are crafted, attribute inference requires only a single model forward pass of the generated samples.

In our extensive evaluation, we demonstrate the success of \textsc{Caia} and show that robust models trained with adversarial training~\citep{goodfellow15adv, madry18pgd} are even more susceptible to these attacks. They leak more information about their individual classes than non-robust models, even if their prediction accuracy is substantially lower. This indicates a trade-off for model designers: making a model robust to adversarial examples comes at the expense of higher privacy leakage.

\noindent\textbf{Disclaimer:} \textit{This paper investigates the extraction of sensitive identity information, including gender and racial appearance of people. The groups used (Asian, Black, Indian, White) are in line with current research and follow the taxonomy of \citet{karkkainen21fairface}, which itself is adopted from the US Census Bureau~\citep{uscensus22}. Importantly, we emphasize that this work's goal is to investigate the leakage of sensitive attributes in image classifiers. We do not intend to discriminate against identity groups or cultures in any way.}