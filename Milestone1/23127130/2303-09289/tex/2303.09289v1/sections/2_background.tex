\section{Background and Related Work}\label{sec:background}

\textbf{Attribute Inference Attacks.}
In recent years, various types of inference attacks have been proposed. Those include membership inference attacks~\citep{shokri2017, yeom2018privacy, choquette2021, hintersdorf22trust, hintersdorf_clip} and property inference attacks~\citep{ganju18property, parisot21propertycnn, zhou22property, wang22group}. Most related to our work are attribute inference attacks (AIAs)~\citep{fredrikson14pharmacogenetics}, which aim to infer sensitive attribute values of an incomplete data record in the context of classification and regression models. More specifically, the adversary has access to a target model $M_\mathit{target}$, which has been trained on a dataset $S_\mathit{train}$, sampled from the distribution $\mathcal{D}$. Each training sample is a triplet $(x_s, x_n, y)$ and consists of some sensitive attributes $x_s$ and non-sensitive attributes $x_n$ together with a ground-truth label $y$. The adversary has access to a set of candidates $(x_n, y)\subseteq S_\mathit{train}$ with the sensitive attribute values missing. AIAs try to infer the sensitive values $x_s$ by exploiting $M_\mathit{target}$ and its learned information about distribution $\mathcal{D}$. 

Fredrikson et al.~\citep{fredrikson14pharmacogenetics, fredriskon15mia} proposed maximum-a-posterior AIAs that, assuming all attributes are independent, predict the sensitive attribute value that minimizes the adversary's expected misclassification rate. \citet{yeom2018privacy} extended the approach and combined attribute inference with membership inference attacks. \citet{mehnaz22attributes} introduced an attack based on the softmax scores of the target model, assuming that the model's prediction is more likely to be correct and confident if the input sample contains the true sensitive attribute value. Common AIAs make strong assumptions regarding the adversary's knowledge that is generally hard to gain under realistic assumptions, e.g., the adversary knows the marginal prior of sample attributes~\citep{fredrikson14pharmacogenetics, fredriskon15mia, yeom2018privacy} or the target model's confusion matrix on its training data~\citep{fredriskon15mia,mehnaz22attributes}. \citet{jayaraman22imputation} questioned previous black-box AIAs and empirically showed that those attacks could not reveal more private information than a comparable adversary without access to the target model. They conclude that black-box AIAs perform similarly well as data imputation techniques to fill the missing attribute values. To improve AIAs over data imputation, they proposed a white-box attack that outperforms imputation in settings with limited data and skewed distributions.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/synthesis_concept.pdf}
    \caption{Overview of our attack dataset crafting process for the sensitive attribute \textit{hair color}, which has four possible values. Real images are used to generate image variations by modifying characteristics associated with the sensitive attribute. The resulting candidate images are then filtered to ensure that each sample accurately reflects the intended attribute values. The final output of this process is the set of attack samples.}
    \label{fig:synthesis_concept}
\end{figure*}

\textbf{Model Inversion Attacks.}
All presented AIAs are limited to tabular data and are not applicable to image classification since the variation of single image attributes, e.g., changing the hair color in a facial image, is not trivially possible. Moreover, the AIA setting itself is not transferable to the vision domain, since it is unclear how an adversary can have access to incomplete images with only one attribute missing. However, model inversion attacks (MIAs) fill this gap for image classification. We note that the notion of MIAs is not consistent in the literature, and the term is sometimes also used for AIAs. Generally, given a classification model, an adversary attempts to create synthetic input samples that either reconstruct samples from the model's training data~\citep{fredriskon15mia, secret_revealer,knowledge_mia,kahla22labelmia} or craft synthetic samples that reflect the characteristics of a specific class~\citep{variational_mia,struppek22_mia}. While most MIAs require access to samples from the target training distribution to train a custom generative adversarial network (GAN), \citet{struppek22_mia} recently proposed Plug \& Play (PPA) MIAs, which make the attacks agnostic to the target model, increase their flexibility, and enhance their inference speed by utilizing pre-trained GANs. We will use PPA as a baseline for comparing with our \textsc{Caia}.

\textbf{Novelty of \textsc{Caia}.} 
In contrast to previous work on AIAs, we move the scope of inference attacks from the sample level to a class level in the vision domain, with the goal of inferring sensitive information about the distinct classes learned by a model. To achieve this, we use the latest advancements in text-to-image synthesis to manipulate single attributes of input images, resulting in consistent images that differ only in the targeted attribute. Our approach is more efficient than MIAs as it requires only black-box access to the target model and no extensive knowledge of the training data distribution. Furthermore, the inference step is done in seconds, since \textsc{Caia} does not require any further sample optimization after constructing the initial attack dataset. This makes \textsc{Caia} more flexible and target-independent than previous AIAs and MIAs. Throughout this work, we focus on the privacy-sensitive domain of face recognition systems. The goal is to infer sensitive information about identities, e.g., their gender or racial appearance, without any specific information about the individual identity or underlying training set distributions available. 

\textbf{Adversarial Robust Training.}
Besides privacy attacks, neural networks are known to be susceptible to adversarial examples~\citep{szegedy14intriguing, goodfellow15adv}. Formally, adversarial examples are crafted by adding an optimized perturbation $\delta$ with $\|\delta\|\leq \epsilon$ to a model input $x$ to maximize the model's loss $\mathcal{L}(\mathcal{M}(x+\delta),y)$ for the true label $y$. One of the most reliable and commonly used defenses is adversarial training~\citep{goodfellow15adv, madry18pgd}, which updates a model's weights $\theta$ on adversarial examples. During each training step, a sample-wise worst-case perturbation is computed in an $\epsilon$-environment around the clean samples to maximize the model's confusion. These perturbations are then added to the clean samples to train the model and make it robust against such manipulations. Formally, this comes down to a min-max optimization: 

\begin{equation}
    \min_\theta \sum_{(x,y)\in S_\mathit{train}} \max_{\|\delta\| \leq \epsilon} \mathcal{L}(\mathcal{M}(x+\delta), y) \,.
\end{equation}

Since the inner maximization problem cannot be solved numerically in tractable time, local search algorithms are applied to craft adversarial examples, e.g., FGSM~\citep{goodfellow15adv, wong20ffgsm} or PGD~\citep{madry18pgd}. By training on adversarial examples, the model becomes more robust against adversarial perturbations. In our experiments, we also investigate the influence of model robustness on its privacy leakage. 