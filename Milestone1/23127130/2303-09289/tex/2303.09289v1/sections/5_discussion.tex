\section{Discussion and Challenges}
In this paper, we introduced \textsc{Caia}, a novel privacy attack that enables the inference of sensitive attribute information about classes learned by standard image classifiers. Our extensive experiments demonstrate that classifiers indeed leak sensitive class information, with significant implications for the secure application of machine learning models. While black-box models have traditionally been practically secure against MIAs and AIAs, our research shows that models in the vision domain are not immune to attribute inference. By leveraging recent advances in text-to-image synthesis, \textsc{Caia} infers sensitive information from image classifiers with high accuracy, comparable to white-box MIAs. Furthermore, our findings suggest a trade-off between adversarial training and a model's information leakage, and future adversarial defenses should account for the privacy leakage of models to avoid creating new vulnerabilities.

Although \textsc{Caia} offers reliable attribute inference capabilities, it still faces some challenges and limitations. Accurately evaluating class information leakage requires high-quality data and consistent labeling, which heavily relies on the underlying dataset. We note that the sample quality of CelebA and FaceScrub images varies widely in terms of resolution, sharpness, and coloring, and the labeling is not always consistent within an identity or attribute class, with also falsely labeled samples contained in the datasets. For instance, the boundaries between gray and blond hair are not well-defined, which can lead to reduced inference accuracy for these attributes. However, \textsc{Caia} is generally successful in inferring sensitive attribute values in most cases. The attack metrics might even underestimate its effectiveness since false attribute predictions might still provide some identity information, e.g., black skin tone predictions make white as ground truth rather unlikely. Although, the attack may be more powerful for more consistently labeled datasets with higher image quality.