\section{Introduction}
Classifying images with neural networks is widely adopted in various domains~\citep{esteva21medical, ibrahim22cancer, baumann18roads, barua21covid19}. Face recognition systems~\citep{guo19face_rec}, for example, take facial images as input and attempt to predict the depicted person's identity. In the pursuit of enhancing a model's predictive performance, privacy concerns of the acquired knowledge are often disregarded and moved into the background. However, correctly assessing and mitigating the risk of compromising private information is crucial in privacy-sensitive domains, as neglect can lead to the disclosure of private information~\citep{shokri2017, struppek22_mia, hintersdorf_clip}. Face recognition techniques are a fundamental component of many real-life applications, e.g., criminal identification, video surveillance, access control, and autonomous vehicles.

Smart home devices~\citep{abuassoa21accesscontrol}, for example, contain face recognition models for access control and user authorization. Users expect these models to recognize them reliably, but at the same time to not reveal information about their appearance to third parties. However, this assumption does not necessarily hold true, and malicious parties could extract sensitive features about users without any further information about their appearance by simply interacting with the trained classification model in a black-box fashion.

We investigate the privacy leakage of image classifiers and demonstrate that models indeed leak sensitive class information even without any specific information about the classes, training samples, or attribute distributions required by the adversary. We focus our investigation on face recognition models that are trained on labeled datasets where each class corresponds to a different person's identity. During the training stage, the victim has access to a private dataset of facial images and trains an image classifier on it. After training, the face recognition model makes a prediction on the identity of a given input facial image. 

Our research shows that these models reveal sensitive details about the different identities within their outputs, such as gender, hair color, and racial appearance. \cref{fig:caia_overview} illustrates the basic setting of our investigation, in which the adversary has only black-box access to the target model and no specific information about the appearance of individual identities. The goal of the attacking stage is then to infer sensitive information about the identities from the target model's training data.

\begin{figure*}[ht]
     \centering
     \begin{subfigure}[b]{0.42\textwidth}
         \centering
         \includegraphics[height=2.9cm]{images/setting_top.pdf}
         \caption{The target model is trained on private identities.}
         \label{fig:caia_overview_victim}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.575\textwidth}
         \centering
         \includegraphics[height=2.9cm]{images/setting_bottom.pdf}
         \caption{The adversary tries to infer sensitive information about the private identities.}
         \label{fig:caia_overview_adv}
     \end{subfigure}
        \caption{Overview of the setting of our proposed Class Attribute Inference Attack (\textsc{Caia}). We investigate the setting of face recognition systems, which is a multi-class classification task for which each class corresponds to the identity of an individual. The victim (\ref{fig:caia_overview_victim}) first trains a model on a private training set $S_{train}$ to predict the identity of a given input image. Whereas the victim has access to a labeled training set and knows all the identities of the training samples, the adversary (\ref{fig:caia_overview_adv}) only has black-box access to the trained model without any information about the individual classes or training samples. The adversary then tries to infer sensitive attributes about the individual class identities, e.g., their hair color, gender, or racial appearance. For this, the adversary has access to a separate attack dataset $S_{attack}$ of publicly available facial images but has no information about $S_{train}$.}
        \label{fig:caia_overview}
\end{figure*}

Within our analysis, we introduce a \textit{\textbf{C}lass \textbf{A}ttribute \textbf{I}nference \textbf{A}ttack} (\textsc{Caia}), which enables an adversary to infer sensitive attributes of a specific class from a trained image classifier with high accuracy. Phrased differently, \textsc{Caia} allows the creation of a profile of the individual classes by only interacting with the trained classifier through the extraction of class attributes that have not been explicitly part of the training objective. For the attack, we utilize recent advances in text-to-image synthesis to craft images that only differ in one attribute by editing real images with textual guidance. We then exploit that image classifiers, as we show, assign higher logits to inputs that share the same sensitive attribute with the training samples of a class, which allows us to infer class information by only observing the input-output relation of the trained target model. 

Compared to related inference attacks~\citep{fredrikson14pharmacogenetics,fredriskon15mia,secret_revealer, struppek22_mia}, \textsc{Caia} is model-agnostic and requires only black-box access and basic domain knowledge. Once the attack images are crafted, attribute inference requires only a single model forward pass of the generated samples.

In our extensive evaluation, we demonstrate the success of \textsc{Caia} and show that robust models trained with adversarial training~\citep{goodfellow15adv, madry18pgd} are even more susceptible to these attacks. They leak more information about their individual classes than non-robust models, even if their prediction accuracy is substantially lower. We provide a formal explanation based on the insights that robust models tend to lay their focus on robust image features, which are connected to sensitive attributes. This indicates a trade-off for model designers: making a model robust to adversarial examples comes at the expense of higher privacy leakage.

In summary, we make the following contributions:
\begin{itemize}
    \item We introduce a novel \textit{\textbf{C}lass \textbf{A}ttribute \textbf{I}nference \textbf{A}ttack} (\textsc{Caia}) to infer sensitive attributes of specific classes from image classifiers.
    \item By utilizing recent image manipulation capabilities of diffusion models, \textsc{Caia} can infer sensitive attributes with high precision.
    \item We show that robustly-trained models even leak more information, indicating a trade-off between model robustness and privacy leakage.
\end{itemize}

The paper is structured as follows: We first provide in \cref{sec:background} an overview of the background and related work, including attribute inference and model inversion attacks. We then introduce the setting and procedure of our Class Attribute Inference Attack (\textsc{Caia}) in \cref{sec:caia}. An extensive experimental evaluation in \cref{sec:experiments} demonstrates the strong performance of \textsc{Caia} and investigates the trade-off between robust models and their privacy leakage. An ablation and sensitivity analysis concludes our experiments. We discuss our findings together with open challenges in \cref{sec:discussion}, as well as ethical considerations of our research in \cref{sec:ethical_considerations}. A summary and overview of future avenues conclude the paper in \cref{sec:conclusion}.
\\
\\
\noindent\textbf{Disclaimer:} \textit{This paper investigates the extraction of sensitive identity information, including gender and racial appearance of people. The groups used (Asian, Black, Indian, White) are in line with current research and follow the taxonomy of \citet{karkkainen21fairface}, which itself is adopted from the US Census Bureau~\citep{uscensus22}. Importantly, we emphasize that this work's goal is to investigate the leakage of sensitive attributes in image classifiers. We do not intend to discriminate against identity groups or cultures in any way.}
