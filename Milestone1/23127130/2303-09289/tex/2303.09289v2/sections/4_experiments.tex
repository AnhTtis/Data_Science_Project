\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{  
\begin{tabular}{lllllc}
\toprule
    \textbf{Attribute} & \textbf{Value} & \textbf{Prompt} \\
\midrule
    \multirow{2}{*}{Gender}     & Female & \textit{female appearance} \\
                                & Male & \textit{male appearance} \\
\midrule
    \multirow{2}{*}{Glasses}    & No Glasses & \textit{no eyeglasses} \\
                                & Glasses & \textit{wearing eyeglasses} \\
\midrule
                                & Asian & \textit{with asian appearance} \\
    Racial                      & Black & \textit{with black skin} \\
    Appearance                  & Indian & \textit{with indian appearance} \\
                                & White &  \textit{with white skin} \\
\midrule
    \multirow{4}{*}{Hair Color} & Black & \textit{with black hair} \\
                                & Blond & \textit{with blond hair} \\
                                & Brown &  \textit{with brown hair} \\
                                & Gray & \textit{with gray hair} \\
\bottomrule
\end{tabular}
}
\caption{Prompts for attack dataset generation. Each prompt is appended to the string \textit{"A photo of a person, }$\langle\text{ }\rangle$\textit{"} by replacing $\langle\text{ }\rangle$ with the attribute-specific prompt.}
\label{tab:attack_prompts}
\end{table}


\section{Experimental Evaluation}\label{sec:experiments}
Next, we experimentally investigate the information leakage of face recognition models with \textsc{Caia}. We provide our source code with all hyperparameters to reproduce the experiments and facilitate future research. More experimental details are provided in \cref{appx:experimental_details}.

\subsection{Experimental Setup}
\paragraph{Training Datasets.} We used the CelebA facial attributes dataset~\citep{celeba} to train our target face recognition systems. CelebA contains labeled images of 10,177 individuals and additional 40 binary attribute annotations per image. We selected the following sensitive attributes:
\begin{itemize}
    \item \textit{gender} $=\{\text{female}, \text{male}\}$
    \item \textit{eyeglasses} $=\{\text{no eyeglasses}, \text{eyeglasses}\}$
    \item \textit{hair color} $= \{ \text{black}, \text{blond}, \text{brown}, \text{gray}\}$
    \item \textit{racial appearance} $= \{\text{Asian}, \text{Black}, \text{Indian}, \text{White}\}$
\end{itemize}

For the first three attributes, the CelebA dataset already provides corresponding labels. To infer the attribute \textit{racial appearance}, we applied a pretrained \textit{FairFace}~\citep{karkkainen21fairface} classifier to label each image. Since the provided attributes and labels are often inconsistent for samples of one identity, e.g., people do not always wear eyeglasses or might dye their hair, we created custom subsets for each sensitive attribute group by selecting an equal number of identities for each attribute value and removed samples with inconsistent labels. This is important for evaluation because otherwise the training samples of an identity depict various values for the sensitive attribute, and no clear ground-truth value can be defined. 

We trained various target models on these datasets with the standard cross-entropy loss to predict a person's identity. Note that the attribute labels were not part of the training process. Since not every attribute is present with every identity, we selected the 100 identities for each attribute value of \textit{hair color}, \textit{eyeglasses}, and \textit{racial appearance}, respectively, with the most training samples available. For \textit{gender}, we selected 250 identities per attribute value. We also created larger training datasets with a total of 1,000 identities to see if more identities influence the attack's success. 

Additional target models were trained on the FaceScrub~\citep{facescrub} facial image dataset, which contains images of 530 identities with equal gender split. Images are available in a cropped version that only contains a person's face, and an uncropped version that contains the original image before cropping. Those uncropped images often not only depict a single person but also shows other people in the background which makes training a face recognition system more challenging. We split all datasets into 90\% for training and 10\% for testing the models' prediction accuracy. Further details on the different dataset statistics are provided in \cref{appx:dataset_details}.

\paragraph{Attack Datasets.}
To craft the attack datasets, we used the Flickr-Faces-HQ (FFHQ)~\cite{Karras_stylegan1} and CelebAHQ~\citep{karras18progressive} datasets. Both dataset contain facial images, but compared to CelebA and FaceScrub, these datasets have a much higher resolution, depict a person's whole head and additional background information. Since the FFHQ dataset consists of images collected from the image hosting platform Flickr, we simulate an adversary who collects samples from public sources, e.g., social media platforms. We then generated and filtered images with attribute manipulations to collect 300 attack image tuples for each attribute group. \cref{tab:attack_prompts} states the editing prompts used to manipulate the features of the attack dataset samples. We further set the filter threshold $\tau=0.6$ in all experiments. We visualize randomly selected training and attack samples in \cref{appx:sample_visualizations}.

\paragraph{Training Hyperparameters.}
We trained ResNet-18, ResNet-101, ResNet-152~\citep{resnet_he}, ResNeSt-101~\citep{zhang2020resnest}, and DenseNet169~\citep{densenet} target models. For each dataset-architecture combination, we trained three models with different seeds.  We emphasize that we did not aim for achieving state-of-the-art performances but rather tried to train models with generally good prediction performance.

To investigate the effects of model robustness on information leakage, we trained adversarially robust models on FaceScrub with standard adversarial training~\citep{goodfellow15adv}. We perturbed training samples with Projected Gradient Descent~\citep{madry18pgd} with 7 steps, $\epsilon=4/255$, and step size $\alpha=1/255$\footnote{Corresponds to images with pixel values in range $[0,1]$.}.

We further trained ResNet-50 models on the CelebA attributes for gender, eyeglasses, and hair color for filtering the candidate images. To mitigate the overconfidence of neural networks, we trained all filter models with label smoothing~\citep{szegedy16labelsmoothing, mueller19labelsmoothing} with a smoothing factor of $\alpha=0.1$ to calibrate the models and make them more suitable for the confidence threshold filtering. Since some attributes are more frequent in the CelebA dataset, we draw the same number of samples for each attribute value by oversampling/undersampling from the attribute images. To filter images depicting different \textit{racial appearances}, we relied on the pre-trained \textit{FairFace} classifier. Details on the training of FairFace for filtering the racial appearance images can be found at \url{https://github.com/dchen236/FairFace}.

We state additional training details, including image preprocessing, augmentation, and optimization parameters in \cref{appx:training_hyperparameters}.

\paragraph{Metrics.} We computed the standard classification metrics of precision, recall, and F1 score for each attribute value, together with the overall prediction accuracy for all attributes. All experimental results are averaged over three independently trained models and three disjoint subsets resulting in nine runs per configuration.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/accuracy_comparison.pdf}
    \caption{Attack accuracy for different target model architectures and CelebA attribute datasets. Results are averaged over three models and three attack datasets. Except for ResNet-18, the attacks are comparably successful on the different models. }
    \label{fig:accuracy_results}
    \vspace{0.2cm}
\end{figure*}

\begin{figure*}[t]
     \begin{subfigure}[c]{\textwidth}
         \centering
         \includegraphics[height=0.025\textwidth]{images/legend_alt.pdf}
     \end{subfigure}
     \begin{subfigure}[b]{0.48\textwidth}
        \centering
         \includegraphics[width=\textwidth]{images/barplots/resnet101_gender_ppa.pdf}
         \caption{Gender (CelebA)}
         \vspace{0.2cm}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/barplots/resnet101_eyeglasses_ppa.pdf}
         \caption{Eyeglasses (CelebA)}
         \vspace{0.2cm}
     \end{subfigure}
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/barplots/resnet101_hair_color_ppa.pdf}
         \caption{Hair Color (CelebA)}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/barplots/resnet101_race_ppa.pdf}
         \caption{Racial Appearance (CelebA)}
     \end{subfigure}
    \caption{Evaluation results for \textsc{Caia} performed on ResNet-101 CelebA models to infer four different target attributes. The black horizontal lines denote the standard deviation over nine runs. We further state random guessing (dashed line) and Plug and Play Attacks (PPA, green dots) for comparison. While \textsc{Caia} outperforms random guessing by a large margin, it extracts information on racial appearance and if someone is wearing eyeglasses even more reliably than the white-box PPA attack.}
    \label{resnet101_results}
\end{figure*}

\paragraph{Attack Hyperparameters.}
To create the attack dataset, we first applied Null-Text Inversion~\citep{mokady22nullinversion} with 50 DDIM steps and a guidance scale of 7.5 on Stable Diffusion v1.5\footnote{Available at \url{https://huggingface.co/runwayml/stable-diffusion-v1-5}.}. We further used the generic prompt \textit{“A photo of a person”} for all samples. After the inversion, we generated image variations by adding the sensitive attribute values to the prompt using prompt-to-prompt, e.g., \textit{“A photo of a person, female appearance”} and \textit{“A photo of a person, male appearance”} to generate gender variations. See \cref{tab:attack_prompts} for all prompts used for generating the different feature representations. For prompt-to-prompt, the cross-replace steps were set to 1.0, and the self-replace steps to 0.4 for gender and eyeglasses and 0.6 for hair color and racial appearance. The confidence threshold for the filter models was set to $0.6$ for all models. We then generated and filtered attribute variations one after another until we collected a total of $300$ candidates for each attribute category. If not stated otherwise, all attacks were then performed on subsets of $100$ candidates.

\paragraph{Baselines.} We took random guessing as a naive baseline. Because the sensitive attributes are equally distributed among the different identities, random guessing corresponds to the underlying prior attribute distribution. Since existing AIAs are not applicable to our use case, we instead performed the state-of-the-art Plug \& Play model inversion attack (PPA)~\citep{struppek22_mia} to synthesize characteristic samples for each class. We then used our filter models to predict the sensitive attribute value for each sample and take the majority vote for each targeted identity as the attribute prediction. We emphasize that, unlike \textsc{Caia}, PPA requires white-box access to the target model and a pre-trained GAN, for which we used the StyleGAN2~\citep{Karras2019stylegan2} FFHQ model. Due to the high computational effort, we limit the PPA comparison to individual ResNet-101 models for each setting. Based on the results stated in the paper~\citep{struppek22_mia}, we expect PPA to perform comparably on the other architectures.

\begin{figure*}[t]
     \begin{subfigure}[b]{0.24\textwidth}
        \captionsetup{justification=centering}
        \centering
         \includegraphics[width=\textwidth]{images/cm/cm_gender_small.pdf}
         \caption{Gender (FFHQ) \\ (250 true positives per class)}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
        \captionsetup{justification=centering}
         \centering
         \includegraphics[width=\textwidth]{images/cm/cm_eyeglasses_small.pdf}
         \caption{Eyeglasses (FFHQ) \\ (100 true positives per class)}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.24\textwidth}
        \captionsetup{justification=centering}
         \centering
         \includegraphics[width=\textwidth]{images/cm/cm_hair_color_small.pdf}
         \caption{Hair Color (FFHQ) \\ (100 true positives per class)}
     \end{subfigure}
    \hfill
     \begin{subfigure}[b]{0.24\textwidth}
        \captionsetup{justification=centering}
         \centering
         \includegraphics[width=\textwidth]{images/cm/cm_race_small.pdf}
         \caption{Racial Appearance (FFHQ) \\ (100 true positives per class)}
     \end{subfigure}
    \caption{Confusion Matrices for ResNet-101 models trained on the four different CelebA subsets. The results demonstrate that most attribute values can be inferred with similar precision. However, the attacks sometimes tend to falsely predict \textit{blond} hair color as \textit{gray} and \textit{Asian} appearance as \textit{white}.}
    \label{fig:confusion_matrizes}
\end{figure*}

\subsection{Extracting Sensitive Class Information}
In the main part of the paper, we focus on results on ResNet-101 models using FFHQ attack samples. We provide more detailed results for the other architectures, CelebAHQ as attack dataset, and experiments with 1,000 target identities in \cref{appx:add_results_celeba}. We note that the attack success for an increased number of identities did not change substantially, even if fewer samples of each identity were available during training. Also, the attack success using CelebAHQ instead of FFHQ as dataset to craft the attack samples has no significant impact on the results and demonstrates that \textsc{Caia} is not dependent on a specific attack dataset.

The attack accuracy for different models and target attributes in \cref{fig:accuracy_results} demonstrates that \textsc{Caia} performed comparably well on different architectures and predicted the sensitive attribute values correctly in over 90\% of the cases for the attributes \textit{gender} and \textit{eyeglasses} and about 80\% for the \textit{hair color} and \textit{racial appearance}. Only the attack results of ResNet-18 stand out and are a few percentage points lower than those of the other architectures, which we attribute to the small number of model parameters (only about a quarter of ResNet-101). Still, all attacks reliably inferred the sensitive attributes in most cases.

Next, we investigate the attribute leakage more closely. Therefore, we performed a more detailed analysis of the attribute leakage of ResNet-101 models, for which the results are depicted in \cref{resnet101_results}. For all four attributes, \textsc{Caia} significantly beats the random guessing baseline by a large margin. Whereas \textit{gender} and \textit{eyeglasses} were predicted correctly in about 94\% of the cases, \textit{racial appearance} could be inferred correctly in 84\%. The attack accuracy for \textit{hair color} was also about 82\% on average, but the attack success varied substantially between the different attribute values. \cref{fig:confusion_matrizes} shows the confusion matrices for each attribute. For the hair color, blond hair seems to be the hardest value to predict and is frequently confused with gray hair, which is not unexpected since hair colors have different shades, of which blond might be the broadest one. Another reason for the confusion in these attributes is that the CelebA dataset also contains numerous training images of poor quality and sometimes disturbing lighting, which could interfere with the ground-truth identity attributes.


\subsection{Comparison with White-Box Attacks}
We also investigated gradient-based model inversion attacks, here PPA, to compare to state-of-the-art white-box model inversion methods that reconstruct characteristic class inputs. On the ResNet-101 models, PPA achieved perfect attack results for inferring an identity's \textit{gender}. It also precisely revealed \textit{hair color} and outperformed the black-box \textsc{Caia} for both settings. However, for inferring whether an identity is wearing \textit{eyeglasses}, PPA fell significantly behind \textsc{Caia}. Regarding \textit{racial appearance}, PPA's attack accuracy was comparable to \textsc{Caia} but less consistent between different attribute values. We suspect the reason to be the uneven distribution of the \textit{racial appearance} and \textit{eyeglasses} attribute values in the underlying StyleGAN2's training data~\citep{karakas22fairstyle}. Since PPA starts from randomly sampled latent vectors, the biased attribute distributions also influence the attack's success in terms of generating and inferring sensitive attributes. Nevertheless, \textsc{Caia} shows competitive and impressive performance given that it accesses the target model only in a black-box fashion and has no access to internal gradient information.

\begin{figure*}[t]
     \begin{subfigure}[c]{\textwidth}
         \centering
        \includegraphics[width=\textwidth]{images/robust_legend_long.pdf}
     \end{subfigure}
     \begin{subfigure}[c]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/facescrub_results/facescrub_robust_ffhq_resnet101.pdf}
         \caption{Gender (FaceScrub Cropped)}
         \label{fig:facescrub_cropped}
     \end{subfigure}
     \hfill
    \begin{subfigure}[c]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/facescrub_results/facescrub_uncropped_robust_ffhq_resnet101.pdf}
         \caption{Gender (FaceScrub Uncropped)}
         \label{fig:facescrub_uncropped}
     \end{subfigure}

    \caption{Gender appearance inference results for \textsc{Caia} performed on ResNet-101 models trained on cropped \textbf{(a)} and uncropped \textbf{(b)} FaceScrub samples. We compared standard models to robust models trained with adversarial training (hatched bars). The results demonstrate that robust models indeed increase the information leakage, even if the underlying models' prediction accuracy (orange bars) is significantly below that of non-robust models.}
    \label{fig:facescrub_results}
\end{figure*}

\subsection{Robustness Increases Privacy Leakage}
Since adversarial examples are a common weakness of deep learning models, adversarial training is widely adopted to make models robust against this security threat. To establish a connection between privacy and security, we now extend our investigation to robustly trained models and show that robustness even increases privacy leakage. However, training robust models requires larger datasets due to increased sample complexity~\citep{schmidt18advrobust}. The limited size of the CelebA dataset, which provides approx. 30 samples per identity, makes it difficult to train stable and robust models. Therefore, this section focuses on models trained on FaceScrub, which provides an average of 70 samples per identity and, by this, facilitates stable adversarial training. We trained ResNet-101 models on both cropped images containing only faces and uncropped images, showing a large amount of content unrelated to an identity. For a visual comparison of the datasets, we refer the reader to \cref{appx:dataset_samples}.

The attack results for ResNet-101 models are shown in \cref{fig:facescrub_results}, while \cref{appx:add_results_Facescrub} presents results for other model architectures. Comparing the attack accuracy of non-robust models ($93.08\%$) against that of robust models ($96.62\%$) trained on cropped images suggests that robust models tend to leak more \textit{gender} information about their learned identities than non-robust models, even if the models' clean prediction accuracy is roughly five percentage points lower. A similar pattern holds for models trained on uncropped images, with robust models still exhibiting higher information leakage. However, it is important to note that the standard model's prediction accuracy ($78.75\%$) on uncropped images is significantly higher than that of robust models ($55.23\%$). This suggests that a model's prediction accuracy is not necessarily indicative of its privacy leakage. We hypothesize that robust models tend to exhibit higher privacy leakage due to their concentration on more robust features for their predictions. We formalize and investigate this hypothesis in the remaining section.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[height=0.6\textwidth, keepaspectratio]{images/integrated_gradients.pdf}
          \caption{Visualized Attribution with Integrated Gradients.}
          \label{fig:qualitative_ig}
     \end{subfigure}
     \hfill
    \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[height=0.6\textwidth, keepaspectratio]{images/rel_attribution.pdf}
          \caption{Relative Attribution for facial features.}
          \label{fig:quantative_ig}
     \end{subfigure}
    \caption{The comparison of absolute attribution based on integrated gradients between a robust and a standard ResNet-101 model in \textbf{(a)} shows that robust models assign perceptually more attuned attribution to facial features. A quantitative comparison of relative attribution in \textbf{(b)} further highlights that the robust model assigns more attribution to unique facial features, such as eyes and hair, while the standard model assigns most attribution to non-specific parts of the skin.}
    \label{fig:integrated_gradients}
\end{figure*}

Our formal analysis builds upon the robust feature model for binary classifiers of \citet{ilyas19bugs}, which we extend to the privacy leakage setting. Let $\mathcal{M}:X \to Y$ be a model trained to predict a label $y\in Y$ for each input $x\in X$. We divide a model's inputs $x=(\tilde{x},\bar{x})$ into predictive features $\tilde{x}$ and non-predictive features $\bar{x}$. The $i$-th predictive feature $\tilde{x}^i$ is positively correlated with a sample's true label: $E_{(x,y)\sim \mathcal{D}} \left[ \, \hat{y}\cdot\mathcal{M}(\tilde{x}^i)_y \, \right] \geq \rho$ for a sufficiently large $\rho$. Here, we deviate slightly from our previous notation of $y$ and define $\hat{y}\in \{-1,1\}$ for each class, with $\hat{y}=1$ indicating the ground-truth label. We further assume that all model outputs $\mathcal{M}_y$ are centered around zero. Predictive features are (ideally) learned by a model to predict the label. Conversely, a non-predictive feature $\bar{x}^j$ does not correlate with the true label and should be ignored during inference if $E_{(x,y)\sim \mathcal{D}} \left[ \, \hat{y}\cdot\mathcal{M}(\bar{x}^j)_y \, \right] < \rho$. 

We further explore the nature of the predictive features by categorizing them into two groups: robust features $\tilde{x}_\textit{robust}$ and non-robust features $\tilde{x}_\textit{non-robust}$. Robust predictive features remain predictive even under adversarial perturbations $\delta$ with $\| \delta \| \leq \epsilon$, satisfying $E_{(x,y)\sim \mathcal{D}} \left[ \, \hat{y}\cdot\mathcal{M}(\tilde{x}_{robust}^i + \delta)_y \, \right] \geq \rho$, while non-robust predictive features lose their predictive power under adversarial perturbations. Neural networks rely not only on salient features, such as hair color and other facial characteristics, for image processing, but also on subtle, non-robust image features that can still be highly predictive.

Adversarial perturbations can significantly disrupt non-robust models' predictions by manipulating $\tilde{x}_\textit{non-robust}$ in an $\epsilon$-environment that is imperceptible to humans. Adversarial training can help reduce the impact of adversarial examples by focusing the model's prediction on $\tilde{x}_\textit{robust}$. However, we stress that adversarial training's potential to enhance model robustness comes at the cost of increased privacy leakage. To explain why adversarial training increases a model's privacy leakage, we hypothesize that human-recognizable, sensitive attributes are part of $\tilde{x}_\textit{robust}$. These attributes are highly predictive because they enable discrimination between samples of different classes based on the assumption that they remain constant within samples of a specific class. We also argue that these attributes are robust features since they are easily identifiable by humans, and small adversarial perturbations are unlikely to affect their value.

In the context of face recognition, \textit{gender} is a sensitive attribute that is highly predictive, enabling us to distinguish individuals from people with other \textit{gender} appearances. Furthermore, it is a robust feature, as altering a person's \textit{gender} appearance in an image would require significant changes to the pixels. To showcase the importance of robust features in robust models, we utilized the axiomatic attribution method Integrated Gradients~\citep{sundararajan17ig} on two ResNet-101 models, one trained using standard training and the other with adversarial training. \cref{fig:qualitative_ig} visualizes the computed gradients for four input images from our attack datasets. While the standard model relies on noisy, non-robust features, such as image background, the robust model concentrates more on human-recognizable facial attributes, including hair, eyes, eyeglasses, and facial structure.

To also provide quantitative support for our analysis, we used Integrated Gradients to measure a model's relative attribution to specific image parts. For this, we applied a pre-trained face segmentation model~\citep{lee20maskgan} to locate various attributes in facial images. Be $H_{Z}(x) \in \{0,1\}^{H\times W}$ the binary segmentation mask for image $x$ and attribute class $Z$, such as a person's hair. Be further $\mathcal{A}(\mathcal{M}, x)\in \mathbb{R}^{H\times W}$ the absolute pixel-wise attribution by Integrated Gradients for model $\mathcal{M}$ and image $x\in \mathbb{R}^{H\times W}$. Let also $\| \cdot \|_1$ denote the sum norm and $\odot$ the Hadamard product. The relative attribution for a dataset $X$ is then computed by: 
\begin{equation}
    \mathcal{A}_\mathit{rel}(\mathcal{M}, X) = \frac{1}{|X|} \sum_{x\in X} \frac{\| \mathcal{A}(\mathcal{M}, x) \odot H_Z(x) \|_1}{\| \mathcal{A}(\mathcal{M}, x) \|_1} \, .
\end{equation}
The relative attribution computes the average share of total importance a model assigns to a particular facial attribute.

By analyzing the relative attribution of seven attributes in 100 FFHQ samples and over 530 target identities of ResNet-101 models, \cref{fig:quantative_ig} demonstrates that robust models assign more importance to distinct features like eyes or hair, while standard models prioritize general skin areas. This highlights the greater importance placed by robustly trained models on sensitive attributes encoded in images and reflected in their outputs. As \textsc{Caia} exploits the differences in logits, robust models appear to be more susceptible to such privacy attacks and leak more sensitive attribute information in their outputs. Thus, there exists a trade-off between model robustness and sensitive class information leakage.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/num_attack_samples.pdf}
    \caption{Attack accuracy achieved with a varying number of attack samples $\mathbf{x}$ available. Each result has been computed across three ResNet-101 CelebA models and with different subsets of the attack dataset. The results demonstrate that already 10 to 25 attack examples are sufficient to infer the sensitive attributes reliably.}
    \label{fig:num_attack_samples}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/performing_samples.pdf}
    \caption{Attack accuracy and illustration of the attack samples $\mathbf{x}$ that achieve the highest and lowest accuracy, respectively, on ResNet-101 CelebA models. It becomes clear that samples that represent the different attribute values in a clear way perform best. On the contrary, samples with inconsistent attribute representation are far less informative and fail to extract sensitive values.}
    \label{fig:performing_samples}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/filtering.pdf}
    \caption{Attack accuracy achieved with unfiltered samples and samples filtered without any confidence threshold compared to our default approach with a confidence threshold of $\tau=0.6$. The filtering approach mainly improves the results for the attributes \textit{Eyeglasses} and \textit{Hair Color}, and only has a small impact on the attack success for the attributes \textit{Gender} and \textit{Racial Appearance}.}
    \label{fig:filtering}
\end{figure*}

\subsection{Ablation and Sensitivity Analysis}
To provide a complete picture of \textsc{Caia}, we also performed an ablation and sensitivity analysis. We first investigated the influence of the number of attack samples available to the adversary. For this, we repeated the attacks against the ResNet-101 CelebA models and varied the number of attack samples used to compute the relative advantage. As before, results are averaged across three models. We then built nine disjoint attack datasets for each setting and used three of them for each target model. Our experimental results, which are shown in \cref{fig:num_attack_samples}, demonstrate that 10 to 25 attack samples $\mathbf{x}=(x_1, \ldots, x_k)$ are already sufficient to infer the sensitive attributes reliably. Increasing the number any further only improves the results slightly. Even using a single attack example already beats the random guessing baselines significantly. But we also observe a high variance in the results for attacks with only a few attack samples available.

To gain further insights, we selected the attack samples $\mathbf{x}$ that achieved the highest and lowest attack accuracy when performing the attacks only with a single sample. \cref{fig:performing_samples} shows the corresponding images and their achieved attack accuracy for all four attribute settings. The best-performing samples (top row) all clearly portray the different characteristics of the respective target attributes. For example, the \textit{hair color} samples mainly differ in the depicted color of the hair, whereas the face of the woman has barely changed. The samples with the lowest attack accuracy (bottom row) generally have low-quality attribute representations and, in some cases, fail to depict any attribute differences. The results also demonstrate that even if carefully editing the images with \textit{Prompt-to-Prompt} might lead to spurious attribute changes, e.g., removing the eyeglasses could make the person's appearance more female. Similarly, changing the hair color to gray is also entangled with the perceived age of the depicted person. Future, more reliable synthesis models and image editing techniques will probably overcome such feature entanglements and allow attribute manipulations without introducing unwanted image changes. We expect this to improve the performance of \textsc{Caia} even further.

We also measured the impact of the filtering approach during the attack dataset synthesis and repeated the process using the FFHQ dataset with two modifications. First, we set the filter threshold to $\tau=0.0$ and accepted all generated images for which the attribute classifiers predict the target attributes independent of the assigned softmax scores. And second, we completely removed the filtering step and accepted all generated image variations of the first 300 FFHQ samples. We then repeated the attacks on the ResNet-101 CelebA models and compared the results to the ones with the standard filtering approach using a threshold of $\tau=0.6$ applied. \cref{fig:filtering} illustrates the mean attack accuracy for each setting. The results show that the filtering approach significantly improves the attack accuracy for the attributes \textit{Eyeglasses} and \textit{Hair Color}, whereas the accuracy for \textit{Gender} and \textit{Racial Appearance} stays comparable even without any filtering applied. We assume the reason to be that, e.g., adding or removing eyeglasses to a human face is much harder to realize for the generative model than changing a person's gender appearance. Therefore, the importance of the filter model ensuring that only consistent image variations make it into the attack dataset is higher and has a larger influence on the attack results.
