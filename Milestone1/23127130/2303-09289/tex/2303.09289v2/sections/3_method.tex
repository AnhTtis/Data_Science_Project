\section{Class Attributes Inference Attacks}\label{sec:caia}
We now introduce the novel \textit{\textbf{C}lass \textbf{A}ttribute \textbf{I}nference \textbf{A}ttack} (\textsc{Caia}), which consists of two steps. First, the adversary generates a set of attack samples by creating different versions of images through a generative approach that alters the sensitive attribute values. In the second step, these attack samples are used to infer the sensitive attribute values for the identities in a face recognition model. The underlying assumption of the attack is that image classifiers assign higher prediction scores to samples that share the sensitive attribute value with the training samples of a class. Before delving into the details, we outline our general threat model. 

\paragraph{Adversary's Goal.}
Let $\mathcal{M}\colon X \to \mathbb{R}^{|Y|}$ denote the trained target image classifier, which takes input images $x\in X$ and computes prediction scores for each class label $y \in Y$. The model's training data $S_\mathit{train}$ consisted of labeled data samples $(x, y) \sim \mathcal{D}$. The underlying attack assumption is that samples of a certain class share a constant sensitive attribute value $z \in Z$, which is not part of the class label but is implicitly encoded in the image features. For example, a face recognition model is a classifier trained to predict the identity $y$ of each facial image $x$. A sensitive attribute $z$ in this context might be the gender appearance or hair color of a specific identity. The attack goal is to infer the value of this sensitive attribute for each individual class. \cref{fig:caia_overview} demonstrates the general setting, in which the victim trains the target model on labeled images of various identities. In the depicted case, class $y$ corresponds to the identity of the actress \textit{Gillian Anderson}. The adversary then tries to predict sensitive attributes about the class $y$ without any further information such as the name or training samples of the person's available. We note that multiple classes can share the same attribute, e.g., having the same hair color.

\paragraph{Adversary's Capabilities.} The adversary has only black-box access to the trained target model $\mathcal{M}$, i.e., the adversary can query the target model and observe its output logits. Furthermore, the adversary knows the domain of the target model's training data, e.g., facial images, but not the exact training data distribution or labels. Instead, the adversary can sample images from a data distribution $\hat{\mathcal{D}}$ from the same domain. Note that the images available to the adversary contain no identity labels and have no overlapping with the target models training data. For the sensitive attributes, the adversary defines an individual set of possible values to infer. We emphasize that \textsc{Caia} is model- and task-agnostic and does not require white-box access to the target model. Information about the prior distribution of the sensitive attributes is neither available nor required.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/synthesis_concept.pdf}
    \caption{Overview of our attack dataset crafting process for the sensitive attribute \textit{hair color}, which has four possible values. Real images are used to generate image variations by modifying characteristics associated with the sensitive attribute. The resulting candidate images are then filtered to ensure that each sample accurately reflects the intended attribute values. The final output of this process is the set of attack samples.}
    \label{fig:synthesis_concept}
\end{figure*}

\subsection{Crafting Attack Samples}\label{sec:craft_attack_samples}
While it is easy for tabular data to vary the values of attributes in an inference sample, this is a non-trivial task on images since attributes are encoded in pixels, and multiple pixels define not only a single attribute but a set of possibly entangled attributes. For example, human faces contain information such as age and skin color, among many more. Furthermore, changing the value for any of these attributes requires semantically consistent changes in multiple pixels. 

To enable meaningful image manipulations, we utilize recent advances in text-to-image synthesis. Recent generative adversarial models (GANs)~\citep{goodfellow_gan, Karras_stylegan1, Karras2019stylegan2} also offer attribute manipulations by first projecting images into the latent space and then changing the individual latent vector by moving into directions correlated with the target attribute. However, this procedure requires a faithful inversion process to compute the corresponding latent vectors~\citep{subramanyam22styleganinversion,alaluf22hyperstyle,dinh22hyperinverter} and a preceding analysis of the latent space to discover directions~\citep{parihar22exploration,pajouheshgar22optimizing,abdal22clip2stylegan} for the desired content changes. Both components are challenging on their own. We, therefore, choose text-guided image synthesis, which allows us to describe desired content changes with natural language and does not require explicit exploration of the latent space.

Text-to-image synthesis systems like Stable Diffusion~\citep{Rombach2022} are able to generate high-quality images following a user-provided text description $p$. \citet{mokady22nullinversion} recently proposed \textit{Null-text Inversion} to encode real images into the domain of diffusion models and enable text-based editing while keeping the overall image composition and content fixed. In combination with \textit{Prompt-to-Prompt}~\citep{hertz2022prompt}, it allows a user to instruct image edits $x_\mathit{edit}=E(x, p)$ on images $x$ conditioned on a description $p$. We apply \textit{Null-text Inversion} to generate variations of existing images by changing only the sensitive attribute values, such as the hair color or gender appearance while aiming to leave other image aspects unchanged. 

\cref{fig:synthesis_concept} illustrates the crafting process for the attack samples. Formally, the adversary has access to a data distribution $\mathcal{\hat{D}}$ from which to sample images $x$. Note that the attack does not require the attack distribution $\mathcal{\hat{D}}$ to be the same as the training data distribution $\mathcal{D}$ but only that both data distributions are from the same domain, e.g., facial images. As we will show in our experimental evaluation, even if the style, size, and quality of images between both distributions vary significantly, the attack is still highly successful. 

The adversary defines the target attribute with a set of $k$ possible attribute values $Z=\{z_1,\ldots, z_k \}$ and corresponding edit prompts $p_z$ that describe the general domain and explicitly state an attribute value $z\in Z$. For example, $p_z=\text{"A photo of a person, } \langle \text{gender}\rangle\text{"}$, where $\langle\text{gender}\rangle$ is replaced by $z\in \{\text{female appearance}, \text{male appearance}\}$. The candidate dataset $S_\mathit{candidate}$ is then constructed as
\begin{equation}
    S_\mathit{candidate}=\{E(x, p_z) | z\in Z, x \sim \mathcal{\hat{D}}\},
\end{equation} 
consisting of image tuples $\mathbf{x}=(x_1,\ldots,x_k)$, each containing $k$ images with different sensitive attribute values. 

However, the attribute manipulation might not always succeed in changing an image attribute to the desired value. This can be due to interfering concepts already present in the image that are strongly entangled with the target attribute. For example, changing the hair color of a person with a dark skin tone to blonde often fails because such attribute combinations are rather rare in the actual training data, which is also reflected in the underlying diffusion model. We, therefore, employ a filtering approach, i.e., filtering out all sample tuples $\mathbf{x}$ that are not correctly depicting the various attribute values. For this, we use a trained attribute classifier $\mathcal{F}_\tau \colon X \to Z$ to create a subset 
\begin{equation}
    S_\mathit{attack}=\{\mathbf{x} \in S_\mathit{candidate} | \mathcal{F}_\tau(x_z)=z, \forall z\in Z \}
\end{equation}
of sample tuples $\mathbf{x}=(x_1,\ldots,x_k)$. The attribute classifier computes for each input image a probability score that the attribute value $z\in \mathcal{Z}$ is present in the image. We also add a threshold $\tau$ on the softmax scores of the attribute classifier and classify only predictions with a softmax score $\geq \tau$ as correct. This removes images for which the attribute classifier has only low confidence in its prediction. For the example of hair color, each resulting tuple $\mathbf{x}\in S_{attack}$ consists of four facial images $(x_1, \ldots, x_4)$ that only differ in the depicted hair color of the person. This use case is also depicted in the attack dataset crafting process in \cref{fig:synthesis_concept}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/attack_concept.pdf}
    \caption{Overview of the class attribute inference step. Each image tuple from the attack set is fed sequentially into the target model to compute the logits for the target class. The relative advantage is then computed by subtracting the second-highest logit value from the maximum value, and this difference is added to a running sum for each sensitive attribute value. The final prediction for the sensitive attribute is the value with the highest relative advantage computed across all attack samples.}
    \label{fig:inferring_concept}
\end{figure*}

\subsection{Revealing Sensitive Attributes}
After crafting the attack samples, we can now begin inferring the sensitive class attributes learned by the target classifier. We recall that the adversary has no specific information about the distinct classes and particularly no access to the training data samples. \cref{fig:inferring_concept} illustrates the basic concept of the inference process. In the depicted case, the adversary tries to infer the sensitive attribute \textit{hair color} of the first identity, which corresponds to the first class of the face recognition model.

While our filtering approach ensures that the variations of an image depict the different values of the sensitive attribute, other confounding behavior might still remain unintentionally. For example, changing a person's hair color to gray might also influence the depicted age. To mitigate such influences, we predict the sensitive attribute with a variety of different attack samples to, in turn, reduce the influence of these confounding factors over a larger number of samples.

Be $\mathcal{M}(x)_y\colon X \to \mathbb{R}$ the pre-softmax logits computed by the target model $\mathcal{M}$ on input image $x$ for class $y$. To infer the sensitive attribute value $z$ of class $y$, we query the target model consecutively with multiple sample tuples $\mathbf{x}\in S_\mathit{attack}$. We then compute for each tuple $\mathbf{x}$ the relative advantage $A(\mathbf{x})\in \mathbb{R}^{|Z|}$. For each $x_z \in \mathbf{x}$, the relative advantage component $A(\mathbf{x})_z$ is defined by
\begin{equation}
    A(\mathbf{x})_z = \max\left(0, \, \mathcal{M}(x_z)_y - \max_{\tilde{x} \in \mathbf{x}, \tilde{x}\neq x_z}{\mathcal{M}(\tilde{x})_y} \right).
\end{equation}

The relative advantage computes the difference between the highest and the second-highest logit values and assigns this difference to the attribute sample $x_z \in \mathbf{x}$ with the highest logit. For all other samples $\tilde{x}\in\mathbf{x}$ with $\tilde{x} \neq x_z$, the relative advantage is set to zero. \cref{fig:inferring_concept} illustrates the relative advantage computation for a single $\mathbf{x}$. In the depicted case, the sample depicting \textit{brown hair color} achieves the highest logit value and its relative advantage of $A(\mathbf{x})_{brown}=0.17$ describes the difference to the second highest logit value assigned to the sample with the attribute \textit{black hair color}. The relative advantage of the other three attribute values is consequently set to zero.

The final attribute prediction $\hat{z}$ is then done by taking the attribute with the highest relative advantage summed up over all attack samples: 
\begin{equation}
    \hat{z}=\argmax_{z\in Z} \sum_{\mathbf{x} \in S_\mathit{attack}}A(\mathbf{x})_z \, .
\end{equation} 

We emphasize that only a single forward pass on all attack samples is sufficient to compute the relative advantage for all target classes, which makes the inferring step computationally very cheap, e.g., the gender inference for a ResNet-101 model and 500 identities took roughly 2 seconds in our experiments.