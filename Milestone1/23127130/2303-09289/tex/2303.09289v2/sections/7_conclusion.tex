\section{Conclusion}\label{sec:conclusion}
To summarize, our research provides novel insights into the privacy of image classifiers and shows that models leak more sensitive information than previously assumed. Our experimental results and formal analysis demonstrate that robust models are particularly susceptible to such attacks, which introduce a novel trade-off between model robustness and privacy. It combines both areas of research, which have largely been studied separately in previous research. We hope our work motivates future security research and defense endeavors in building secure and private models.

Expanding on the mentioned challenges, we expect that with the upcoming developments in text-guided image manipulation, we propose to enhance \textsc{Caia} and extend its use to continuous features like age or more detailed skin tone grading. For example, the attack samples could be generated to reflect various shades of dark skin color or fine-grained age representations. Moreover, in its current implementation, \textsc{Caia} infers each attribute independently. However, in the real world, different attributes often correlate with each other. For example, information inferred on racial appearance influences the probability distribution of hair colors. To this end, \textsc{Caia} could leverage already inferred attributes as a prior for inferring additional attributes. We also envision the use of \textsc{Caia} beyond privacy analyses. For example, it is exciting to explore it in the context of explainable AI to determine the importance of features by analyzing prediction scores for different feature characteristics. Similarly, it could be employed to assess the fairness of models and identify potential biases associated with certain classes.