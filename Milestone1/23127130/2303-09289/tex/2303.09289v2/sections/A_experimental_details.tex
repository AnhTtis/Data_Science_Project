\section{Experimental Details}\label{appx:experimental_details}

\subsection{Hard- and Software Details}\label{app:hardware_details}
We performed our experiments on NVIDIA DGX machines running NVIDIA DGX Server Version 5.1.0 and Ubuntu 20.04.4 LTS. The machines have 2TB of RAM and contain NVIDIA A100-SXM4-40GB GPUs and AMD EPYC 7742 CPUs. We further relied on CUDA 11.4, Python 3.10.0, and PyTorch 1.12.1 with Torchvision 0.13.1~\cite{pytorch} for our experiments. If not stated otherwise, we used the model architecture implementations and pre-trained ImageNet weights provided by Torchvision. We further provide a Dockerfile together with our code to make the reproduction of our results easier. In addition, all training and attack configuration files are available to reproduce the results stated in this paper.

To perform Plug \& Play Attacks (PPA), we used a NVIDIA DGX machine running NVIDIA DGX Server Version 5.2.0 and Ubuntu 20.04.4 LTS. The machines has 2TB of RAM and contain NVIDIA A100-SXM4-80GB GPUs and AMD EPYC 7742 CPUs. The experiments were performed with CUDA 11.4, Python 3.8.10, and PyTorch 1.10.0 with Torchvision 0.11.0. We further used PPA in combination with the pre-trained FFHQ StyleGAN-2 and relied on the standard CelebA attack parameters, as stated in \citet{struppek22_mia} and \url{https://github.com/LukasStruppek/Plug-and-Play-Attacks}. We only changed the number of candidates to 50 and the final samples per target to 25, to speed up the attack process.


\subsection{Dataset Details}\label{appx:dataset_details}
We state the number of identities and samples for each custom CelebA subset in \cref{tab:celeba_subsets}. We further state the lowest, median and maximum number of samples of a single identity per attribute and for the total dataset. For the filter models, the datasets contained 155,304 samples (gender and eyeglasses) and 93,942 (hair color), respectively.

\begin{table}[ht]
\centering
\resizebox{0.8\linewidth}{!}{  
\begin{tabular}{llcccccc}
\toprule
\multirow{2}{*}{\textbf{Attribute}} & \multirow{2}{*}{\textbf{Value}}     & \multirow{2}{*}{\textbf{Identities}} & \textbf{Total} & \textbf{Min} & \textbf{Median} & \textbf{Max} & \textbf{Avg}\\
          &           &            & \textbf{Samples} & \textbf{Samples} & \textbf{Samples} & \textbf{Samples} & \textbf{Samples}\\
\midrule
    \parbox[t]{1mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox{1.2cm}{\centering \textbf{CelebA} \\ \textbf{Gender}}}}}  & Female & 250 & 7,639 & 30 & 30 & 35 & 30.56\\
    & Male        & 250 & 7,652 & 30 & 30 & 36 & 30.61 \\
    \cmidrule{2-8}
    & Total       & 500 & 15,291 & 30 & 30 & 36 & 30.58 \\
\midrule
    \parbox[t]{1mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox{1.2cm}{\centering \textbf{CelebA} \\ \textbf{Glasses}}}}} & No Glasses & 100 & 3186 & 31 & 32 & 36 & 31.86\\
    & Glasses        & 100 & 2,346 & 20 & 22 & 31 & 23.46 \\
     \cmidrule{2-8}
    & Total          & 200 & 5532 & 20 & 31 & 36 & 27.66 \\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\parbox{1.2cm}{\centering \textbf{CelebA} \\ \textbf{Race}}}}} & Asian & 100 & 2944 & 28 & 30 & 31 & 29.44 \\
    & Black   & 100 & 2876    & 26   & 29    & 34 & 28.76 \\
    & Indian  & 100 & 2281    & 19   & 22    & 30 & 22.81 \\
    & White   & 100 & 3164    & 31   & 31    & 35 & 31.64\\
     \cmidrule{2-8}
    & Total   &  400 & 11,265  & 19   & 29    & 35 & 28.16\\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\parbox{1.2cm}{\centering \textbf{CelebA} \\\textbf{Hair Color}}}}} & Black Hair & 100 & 2,840 & 23 & 28 & 32 & 28.40\\
    & Blond Hair & 100 & 2,867 & 25 & 29 & 31 & 28.67 \\
    & Brown Hair & 100 & 2,564 & 17 & 26 & 30 & 25.64 \\
    & Gray Hair  & 100 & 2,117 & 14 & 20 & 31 & 21.17 \\
     \cmidrule{2-8}
   &  Total      & 400 & 10,388 & 14 & 27 & 32 & 25.97\\
\midrule
    \parbox[t]{1mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox{1.5cm}{\centering \textbf{FaceScrub} \\ \textbf{Gender}}}}} & Female & 265 & 17,743 & 9 & 65 & 140 & 66.95 \\
    & Male        &  265 & 20,136 & 25 & 77 & 128 & 75.98\\
     \cmidrule{2-8}
    & Total          & 530 & 37,879 & 9 & 72 & 140 & 71.47 \\
\bottomrule
\end{tabular}
}
\label{tab:celeba_subsets}
\caption{Dataset identity statistics}
\end{table}

\newpage
\subsection{Training Hyperparameters}\label{appx:training_hyperparameters}
We emphasize that we did not aim for achieving state-of-the-art performances but rather tried to train models with generally good prediction performance. All non-robust models were initialized with pre-trained ImageNet weights and trained for 80 epochs with an initial learning rate of $0.1$ and a batch size of 128. We multiplied the learning rate by factor $0.1$ after 60 and 70 epochs. For the models trained with adversarial training, set the number of epochs to 100 and reduced the learning rate after 80 and 90 epochs. Optimization was done with SGD and a momentum of $0.9$. Images were resized to $224\times224$ and normalized with $\mu=\sigma=0.5$ to set the pixel value range to $[-1, 1]$. As augmentation, we applied random horizontal flipping with $p=0.5$ flipping probability and random resized cropping with $\text{scale}=[0.8, 1.0]$ and $\text{ratio}=[0.8, 1.25]$. Cropped images were resized to size $224\times 224$. Due to the limited number of training samples available for each identity, we did not use a validation set and early stopping. For each architecture and dataset, we trained three models with different seeds. Performance evaluation was done on a holdout test set.

We further trained ResNet-50 models on the CelebA attributes for gender, eyeglasses, and hair color for filtering the candidate images. All models were initialized with pre-trained ImageNet weights and trained for 10 epochs with an initial learning rate of $1e-3$ and a batch size of 128. We multiplied the learning rate by factor 0.1 after 9 epochs. We used the Adam optimizer~\citep{adam_optimizer} with $\beta=(0.9, 0.999)$ and no weight decay. Normalization and augmentation were identical to training the target models, except the scale and ratio parameters of the random resized cropping, which we set to $[0.9, 1.0]$ and $[1.0, 1.0]$ to prevent cutting out the attributes from the training samples. We again used no validation set for early stopping. To calibrate the models, we applied label smoothing with smoothing factor $\alpha=0.1$ during training. Since some attributes are more frequent in the CelebA dataset, we draw the same number of samples for each attribute by oversampling / undersampling from the attribute images. Details on the training of FairFace for filtering the racial appearance images can be found at \url{https://github.com/dchen236/FairFace}.

\begin{table}[ht]
\begin{subtable}[h]{0.45\textwidth}
\centering
\resizebox{!}{4.4cm}{  
\begin{tabular}{lllc}
\toprule
    \textbf{Dataset} & \textbf{Num Classes} &  \textbf{Architecture} & \textbf{Accuracy} \\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\footnotesize\textbf{Gender}}}} & \multirow{5}{*}{{500}} 
    &  ResNet-18 & $85.99\%\pm0.21$ \\
    & & ResNet-101 & $86.78\%\pm0.66$ \\
    & & ResNet-152 &  $86.62\%\pm0.62$ \\
    & & DenseNet-169 &  $75.93\%\pm5.54$ \\
    & & ResNeSt-101 &  	$81.53\%\pm4.40$ \\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\footnotesize\textbf{Eyeglasses}}}} & \multirow{5}{*}{{200}} 
    &  ResNet-18 & $93.08\%\pm0.58$ \\
    & & ResNet-101 & $90.97\%\pm1.41$ \\
    & & ResNet-152 & $91.10\%\pm0.85$ \\
    & & DenseNet-169 & $93.56\%\pm0.38$ \\
    & & ResNeSt-101 & $90.73\%\pm0.21$ \\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\footnotesize\textbf{Race}}}} &
    \multirow{5}{*}{{400}} 
    &  ResNet-18 & $84.65\%\pm0.44$ \\
    & & ResNet-101 & $82.99\%\pm0.40$ \\
    & & ResNet-152 & $84.06\%\pm1.19$ \\
    & & DenseNet-169 & $85.00\%\pm0.96$ \\
    & & ResNeSt-101 & $81.13\%\pm1.04$ \\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\footnotesize\textbf{Hair Color}}}} &
    \multirow{5}{*}{{400}} 
    &  ResNet-18 & $87.94\%\pm0.56$ \\
    & & ResNet-101 & $87.65\%\pm0.70$ \\
    & & ResNet-152 & $87.20\%\pm0.67$ \\
    & & DenseNet-169 & $88.45\%\pm1.68$ \\
    & & ResNeSt-101 &  $85.53\%\pm0.53$ \\
\bottomrule
\end{tabular}
}
\caption{CelebA}
\end{subtable}
\hfill
\begin{subtable}[h]{0.45\textwidth}
\centering
\resizebox{!}{4.4cm}{  
\begin{tabular}{lllc}
\toprule
    \textbf{Dataset} & \textbf{Num Classes} &  \textbf{Architecture} & \textbf{Accuracy} \\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\footnotesize \parbox{1.5cm}{\centering \textbf{FaceScrub} \\ \textbf{Cropped}}}}} &
    \multirow{5}{*}{{530}} 
    &  ResNet-18 & $93.92\%\pm0.17$ \\
    & & ResNet-101 & $94.93\%\pm0.43$ \\
    & & ResNet-152 & $95.32\%\pm0.22$ \\
    & & DenseNet-169 & $91.93\%\pm0.65$ \\
    & & ResNeSt-101 & $93.50\%\pm0.70$ \\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\footnotesize \parbox{1.5cm}{\centering \textbf{FaceScrub} \\ \textbf{Cropped} \\ \textbf{Robust}}}}} &
    \multirow{5}{*}{{530}} 
    &  ResNet-18 & $83.79\%\pm0.21$ \\
    & & ResNet-101 & $88.12\%\pm0.24$ \\
    & & ResNet-152 & $87.11\%\pm0.28$ \\
    & & DenseNet-169 & $88.53\%\pm0.76$ \\
    & & ResNeSt-101 & $90.04\%\pm0.48$ \\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\footnotesize \parbox{1.5cm}{\centering \textbf{FaceScrub} \\ \textbf{Uncropped}}}}} &
    \multirow{5}{*}{{530}} 
    &  ResNet-18 & $70.29\%\pm2.71$ \\
    & & ResNet-101 & $78.75\%\pm0.84$ \\
    & & ResNet-152 & $78.52\%\pm0.58$ \\
    & & DenseNet-169 & $\,\,\,69.11\%\pm12.73$ \\
    & & ResNeSt-101 & $77.32\%\pm0.54$ \\
\midrule
    \parbox[t]{1mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\footnotesize \parbox{1.5cm}{\centering \textbf{FaceScrub} \\ \textbf{Uncropped} \\ \textbf{Robust}}}}} &
    \multirow{5}{*}{{530}} 
    &  ResNet-18 & $41.58\%\pm0.63$ \\
    & & ResNet-101 & $55.23\%\pm0.60$ \\
    & & ResNet-152 & $55.69\%\pm1.24$ \\
    & & DenseNet-169 & $54.65\%\pm2.98$ \\
    & & ResNeSt-101 & $58.74\%\pm0.84$ \\
\bottomrule
\end{tabular}
}
\caption{FaceScrub}
\end{subtable}

\caption{Prediction accuracy of the target models on the individual test sets.}
\end{table}

\newpage
\subsection{Attack Dataset Generation}
To create the attack dataset, we first applied Null-Text Inversion~\citep{mokady22nullinversion} with 50 DDIM steps  and a guidance scale of 7.5 on Stable Diffusion v1.5\footnote{Available at \url{https://huggingface.co/runwayml/stable-diffusion-v1-5}.}. We further used the generic prompt \textit{“A photo of a person”} for all samples. After the inversion, we generated image variations by adding the sensitive attribute values to the prompt using prompt-to-prompt, e.g., \textit{“A photo of a person, female appearance”} and \textit{“A photo of a person, male appearance”} to generate gender variations. We set the cross replace steps to 1.0 and the self replace steps to 0.4 for gender and eyeglasses, and to 0.6 for hair color and racial appearance. The confidence threshold for the filter models was set to $0.6$ for all models. We then generated and filtered attribute variations one after another, until we collected 300 candidates for each attribute category. \cref{tab:attack_prompts} states all prompts used for generating.

\begin{table}[ht]
\centering
\resizebox{0.4\linewidth}{!}{  
\begin{tabular}{lllllc}
\toprule
    \textbf{Attribute} & \textbf{Value} & \textbf{Prompt} \\
\midrule
    \multirow{2}{*}{Gender}     & Female & \textit{female appearance} \\
                                & Male & \textit{male appearance} \\
\midrule
    \multirow{2}{*}{Glasses}    & No Glasses & \textit{no eyeglasses} \\
                                & Glasses & \textit{wearing eyeglasses} \\
\midrule
                                & Asian & \textit{with asian appearance} \\
    Racial                      & Black & \textit{with black skin} \\
    Appearance                  & Indian & \textit{with indian appearance} \\
                                & White &  \textit{with white skin} \\
\midrule
    \multirow{4}{*}{Hair Color} & Black & \textit{with black hair} \\
                                & Blond & \textit{with blond hair} \\
                                & Brown &  \textit{with brown hair} \\
                                & Gray & \textit{with gray hair} \\
\bottomrule
\end{tabular}
}
\caption{Prompts for attack dataset generation. Each prompt is appended to the string \textit{"A photo of a person, }$\langle\text{ }\rangle$\textit{"} by replacing $\langle\text{ }\rangle$ with the attribute-specific prompt.}
\end{table}


\begin{table}[ht]
\centering
\resizebox{0.6\linewidth}{!}{  
\begin{tabular}{llllc}
\toprule
    \textbf{Attribute}  & \textbf{Num Classes} & \textbf{Training Samples} & \textbf{Architecture} & \textbf{Accuracy} \\
\midrule
    Gender & 2 & 155,304 & ResNet-50 &  $98.98\%$ \\
    Glasses & 2 & 155,304 & ResNet-50 & $98.81\%$ \\
    Hair Color & 4 & 93,940 & ResNet-50 & $93.51\%$ \\
\bottomrule
\end{tabular}
}
\caption{Prediction accuracy of the filter models on the individual test sets.}
\end{table}
