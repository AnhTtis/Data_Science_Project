\section{Background and Related Work}\label{sec:background}
We start by introducing the background and related work for our analysis. We focus here on attribute inference attacks, which exploit models trained on tabular data to infer hidden attributes for a given input sample, and model inversion attacks, which aim to reconstruct the appearance of training images with the help of generative models. We then highlight the novelty of \textsc{Caia} compared to existing inference attacks. Furthermore, we present adversarial training to make a model robust against adversarial examples. 


\subsection{Attribute Inference Attacks}
In recent years, various types of inference attacks have been proposed. Those include membership inference attacks~\citep{shokri2017, yeom2018privacy, choquette2021, hintersdorf22trust, hintersdorf_clip}, which attempt to identify training samples from a larger set of candidates, and property inference attacks~\citep{ganju18property, parisot21propertycnn, zhou22property, wang22group}, which try to infer general properties and statistical information about a model's training data. Most related to our work are attribute inference attacks (AIAs)~\citep{fredrikson14pharmacogenetics}, which aim to infer sensitive attribute values of an incomplete data record in the context of classification and regression models. More specifically, the adversary has access to a target model $\mathcal{M}$, which has been trained on a tabular dataset $S_\mathit{train}$, sampled from the distribution $\mathcal{D}$. Each training sample is a triplet $(x_s, x_n, y)$ and consists of some sensitive attributes $x_s$ and non-sensitive attributes $x_n$ together with a ground-truth label $y$. The adversary has access to a set of candidates $(x_n, y)\subseteq S_\mathit{train}$ with the sensitive attribute values missing. AIAs try to infer the sensitive values $x_s$ by exploiting $\mathcal{M}$ and its learned information about distribution $\mathcal{D}$. 

Fredrikson et al.~\citep{fredrikson14pharmacogenetics, fredriskon15mia} proposed maximum-a-posterior AIAs that, assuming all attributes are independent, predict the sensitive attribute value that minimizes the adversary's expected misclassification rate. \citet{yeom2018privacy} extended the approach and combined attribute inference with membership inference attacks. \citet{mehnaz22attributes} introduced an attack based on the softmax scores of the target model, assuming that the model's prediction is more likely to be correct and confident if the input sample contains the true sensitive attribute value. Common AIAs make strong assumptions regarding the adversary's knowledge that is generally hard to gain under realistic assumptions, e.g., the adversary knows the marginal prior of sample attributes~\citep{fredrikson14pharmacogenetics, fredriskon15mia, yeom2018privacy} or the target model's confusion matrix on its training data~\citep{fredriskon15mia,mehnaz22attributes}. \citet{jayaraman22imputation} questioned previous black-box AIAs and empirically showed that those attacks could not reveal more private information than a comparable adversary without access to the target model. They conclude that black-box AIAs perform similarly well as data imputation techniques to fill the missing attribute values. To improve AIAs over data imputation, they proposed a white-box attack that outperforms imputation in settings with limited data and skewed distributions.

\subsection{Model Inversion Attacks}
All presented AIAs are limited to tabular data and are not applicable to image classification since the variation of single image attributes, e.g., changing the hair color in a facial image, is not trivially possible. Moreover, the AIA setting itself is not transferable to the vision domain, since it is unclear how an adversary can have access to incomplete images with only one attribute missing. This fact also makes it impossible to directly compare our \textit{\textbf{C}lass \textbf{A}ttribute \textbf{I}nference \textbf{A}ttack} (\textsc{Caia}) with common AIAs and corresponding defenses.

Another class of attacks, so-called model inversion attacks (MIAs), try to fill this gap for image classification. We note that the notion of MIAs is not consistent in the literature, and the term is sometimes also used for AIAs. Generally, given a classification model, an adversary attempts to create synthetic input samples that either reconstruct samples from the model's training data~\citep{fredriskon15mia, secret_revealer,knowledge_mia,kahla22labelmia} or craft synthetic samples that reflect the characteristics of a specific class~\citep{variational_mia,struppek22_mia}. Whereas most MIAs require access to samples from the target training distribution for training a custom generative adversarial network (GAN), \citet{struppek22_mia} recently proposed Plug \& Play (PPA) MIAs, which make the attacks agnostic to the target model, increase their flexibility, and enhance their inference speed by utilizing pre-trained GANs. We will, therefore, use PPA as a baseline for comparing with our \textsc{Caia}.

Formally, generative model inversion attacks rely on a generative model $G:W\to X$, which is trained to map latent vectors $w\in W$ sampled from a probability distribution to the image space $X$ to synthesize facial images. The adversary then optimizes the sampled latent vectors using the target model $\mathcal{M}$ to find meaningful representations of the target identity on the generative model's learned image manifold. By analyzing the corresponding generated images, the adversary is able to infer sensitive visual features of the individual identities for which $\mathcal{M}$ has been trained to recognize. The optimization goal can be formulated as
\begin{equation}
\min_{\hat{w}} \mathcal{L}(\mathcal{M}(G(\hat{w}), y),
\end{equation}
which optimizes latent vector $\hat{w}$ for the target class $y$ using a suitable loss function $\mathcal{L}$. For the optimization, PPA minimizes a Poincar√© loss~\citep{poincare} between the generated images and the target identity to overcome the vanishing gradient problem and also performs random transformations on the generated images to avoid overfitting and misleading results. The basic underlying generative model used in PPA is a pre-trained StyleGAN2~\citep{Karras2019stylegan2}.

\subsection{Novelty of \textsc{Caia}} 
In contrast to previous work on AIAs, we move the scope of inference attacks from the sample level to a class level in the vision domain, with the goal of inferring sensitive information about the distinct classes learned by a model. To achieve this, we use the latest advancements in text-to-image synthesis to manipulate single attributes of input images, resulting in consistent images that differ only in the targeted attribute. Our approach is more efficient than MIAs as it requires only black-box access to the target model and no extensive knowledge of the training data distribution. Furthermore, the inference step is done in seconds, since \textsc{Caia} does not require any further sample optimization after constructing the initial attack dataset. This makes \textsc{Caia} more flexible and target-independent than previous AIAs and MIAs. Throughout this work, we focus on the privacy-sensitive domain of face recognition systems. The goal is to infer sensitive information about identities, e.g., their gender or racial appearance, without any specific information about the individual identity or underlying training set distributions available to the adversary. 

\subsection{Adversarial Robust Training}
Besides privacy attacks, neural networks are known to be susceptible to adversarial examples~\citep{szegedy14intriguing, goodfellow15adv}. Formally, adversarial examples are crafted by adding an optimized perturbation $\delta$ with $\|\delta\|\leq \epsilon$ to a model input $x$ to maximize the model's loss $\mathcal{L}(\mathcal{M}(x+\delta),y)$ for the true label $y$. One of the most reliable and commonly used defenses is adversarial training~\citep{goodfellow15adv, madry18pgd}, which updates a model's weights $\theta$ on adversarial examples. During each training step, a sample-wise worst-case perturbation is computed in an $\epsilon$-environment around the clean samples to maximize the model's confusion. These perturbations are then added to the clean samples to train the model and make it robust against such manipulations. Formally, this comes down to a min-max optimization: 

\begin{equation}
    \min_\theta \sum_{(x,y)\in S_\mathit{train}} \max_{\|\delta\| \leq \epsilon} \mathcal{L}(\mathcal{M}(x+\delta), y) \,.
\end{equation}

Since the inner maximization problem cannot be solved numerically in tractable time, local search algorithms are applied to craft adversarial examples, e.g., FGSM~\citep{goodfellow15adv, wong20ffgsm} or PGD~\citep{madry18pgd}. By training on adversarial examples, the model becomes more robust against adversarial perturbations. In our experiments, we also investigate the influence of model robustness on its privacy leakage. 