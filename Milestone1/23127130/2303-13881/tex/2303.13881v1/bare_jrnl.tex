


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}

\usepackage{graphics}
\usepackage{amsmath}
\usepackage{import}
\usepackage{times}
\renewcommand*\ttdefault{txtt}
\usepackage{soul}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{import}
\usepackage{multirow, multicol}
\usepackage{subcaption}
\usepackage{array}
\urlstyle{same}
\usepackage{comment}
\usepackage{color, colortbl}
\usepackage{amsfonts}
\usepackage[dvipsnames]{xcolor}
\definecolor{Gray}{gray}{0.9}
\definecolor{green}{rgb}{0.4, 1.0, 0.0}

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Symbolic Music Structure Analysis with Graph Representations and Changepoint Detection Methods}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Carlos~Hernandez-Olivan,~Sonia~Rubio~Llamas,~and~Jose~R.~Beltran% <-this % stops a space
\thanks{Corresponding author: Carlos Hernandez-Olivan, Department
of Electronic Engineering and Communications, University of Zaragoza, Zaragoza 50018 Spain.\\ e-mail: carloshero@unizar.es.}% <-this % stops a space
\thanks{}% <-this % stops a space
\thanks{Preprint.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Preprint}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Music Structure Analysis is an open research task in Music Information Retrieval (MIR). In the past, there have been several works that attempt to segment music into the audio and symbolic domains, however, the identification and segmentation of the music structure at different levels is still an open research problem in this area.
In this work we propose three methods, two of which are novel graph-based algorithms that aim to segment symbolic music by its form or structure: Norm, G-PELT and G-Window. We performed an ablation study with two public datasets that have different forms or structures in order to compare such methods varying their parameter values and comparing the performance against different music styles. We have found that encoding symbolic music with graph representations and computing the novelty of Adjacency Matrices obtained from graphs represent the structure of symbolic music pieces well without the need to extract features from it. We are able to detect the boundaries with an online unsupervised changepoint detection method with a $F_1$ of 0.5640 for a 1 bar tolerance in one of the public datasets that we used for testing our methods. We also provide the performance results of the algorithms at different levels of structure, high, medium and low, to show how the parameters of the proposed methods have to be adjusted depending on the level.
We added the best performing method with its parameters for each structure level to musicaiz, an open source python package, to facilitate the reproducibility and usability of this work.
We hope that this methods could be used to improve other MIR tasks such as music generation with structure, music classification or key changes detection.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Music Information Retrieval, Music Structure Analysis, Graph, symbolic music, signal processing, machine learning.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction} \label{sec:intro}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{M}{usic} Structure (or \textit{form}) Analysis (MSA) is a field of the Music Information Retrieval (MIR) which consists on predicting the structure or \textit{form} of music pieces. Music structure is a music principle \cite{review} that is closely related to other music principles like the harmony. Music presents a hierarchical structure in which sections are ordered in a coherent way. Such sections contain different rhythmic patterns and harmonic progressions that express different ideas. However, in Western classical music such sections can be connected between themselves with the so called cadences or bridges. From the music cognition perspective, there are different ways of understanding the hierarchical structure in music: grouping structure, metrical structure, time-span reduction, and prolongational \cite{lerdahl1983overview}.

Attending to computational approaches of MSA, there are different subtasks that have been previously studied like \textit{melodic segmentation}, \textit{motif discovery} and \textit{structural segmentation} \cite{lopez2012automatic}. Structural segmentation can be also divided in two smaller tasks: \textit{boundary detection} which aims to segment a piece by its structure boundaries, and \textit{segment labeling} which aims to label or name the sections of a piece. In this paper we focus on the task of \textit{boundary detection}, as it is the first step in analyzing the form of a music piece.


The MSA has been studied from two different perspectives according to the nature of the input music: the symbolic domain and the audio domain. Whereas the symbolic domain refers to the music data that contain the basic information about the notes, instruments, etc (MIDI or MusicXML files), the audio domain refers to the digital musical signal and transforms that can be computed from it. This makes the MSA task harder in the audio domain since there is no high-level musical information in the digitized music signal. In both domains, the first step usually consists on predicting the boundaries or segmenting music, and the second step (if applies) labels the segments according to their similarities.

In the symbolic domain, segmentation has been studied for decades \cite{lopez2012automatic} specially regarding monophonic melodic segmentation \cite{rodriguez2014comparing}, \cite{lattner2015probabilistic}, \cite{bassan2022unsupervised}.


MSA present the following challenges:
\begin{itemize}
    \item Each music genre or style has a different form or structure, even in the same music genre there might be multiple forms.
    \item Each piece (even pieces of the same genre) has a different section lengths, number of themes and/or musical phrases.
    This means that we do not now \textit{a priori} the number of sections or themes in a piece. In addition, this problem leads to non-balanced data.
    \item The musical content of the sections with the same name in different pieces is very different, as the nomenclature is intrinsic to each piece. This makes it difficult to train a supervised approach that labels sections by their name.
    \item Boundaries represent small percentage of the data if we compare them to the number of notes in a piece, which makes this and the previous point difficult when it comes to train deep neural networks for section segmentation and/or labelling.
    \item There are different structure levels in music, e.g., in Western classical music form we can find the following levels: \textit{high} structure  (sections), \textit{mid} structure (themes or music phrases) and \textit{low} structure (motifs). However, not all the music can be divided in those levels, specially non-Western classical music.
    \item There are only a few datasets in MIR of symbolic music that contain structure annotations.
\end{itemize}

MSA not only covers the task of music analysis but it can also be used in music generation systems to reinforce such models to generate coherent music \cite{review}, \cite{arxiv.2205.08579}.

In this paper, we propose and compare three methods that predict the segment boundaries in symbolic music without any previous information about the number of boundaries in the pieces, nor the time signature or any other feature different from the note on, note off or pitch of every note in the piece. In addition, the methods are unsupervised and do not need a training algorithm. We validate our methods with two public multi-instrument datasets with different structures and music genres.

\subsection{Contributions}
The main contributions of this work are:
\begin{itemize}
    \item A novel fast online method that uses graphs to encode symbolic music which only needs temporal information for segmenting music by its structure (Figure \ref{fig:schema}).
    \item An ablation study about the segmentation of the three structure levels: low, mid and high.
    \item A comparison of the performance of the method with two different public datasets that contain different forms and instruments. Both datasets contain polyphonic multi-instrument music.
\end{itemize}

We make the code publicly available for reproducibility\footnote{\url{https://github.com/carlosholivan/symbolic-music-structure-analysis}, accessed February 2023}, and we describe some applications that can benefit from this work in Section \ref{sec:conclusions}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{schema.png}
    \caption{Our proposed method for symbolic music segmentation.}
    \label{fig:schema}
\end{figure}

\subsection{Paper Organization}
This paper is organized a follows: in Section \ref{sec:related} we discuss previous work done in MSA, in Section \ref{sec:boundaries} we describe the proposed methods, in Section \ref{sec:ablation} we compare them, in Section \ref{sec:eval} we evaluate our methods on two public datasets, in Section \ref{sec:discussion} we discuss the results and in Section \ref{sec:conclusions} we give conclusions and future work that could benefit from our work.


\section{Related Work} \label{sec:related}

\subsection{Audio Music Structure Analysis}
Music structure analysis have been studied in both the audio and symbolic domains \cite{msareview}. Music segmentation can be based on the following principles: homogeneity, repetition and novelty \cite{muller2015}. In the audio domain, one of the representations that is commonly used is the Self-Similarity Matrix (SSM) \cite{foote1999visualizing}. SSMs have been used as inputs of deep learning models in previous studies of boundary detection 
\cite{boundaries2021hernandezolivan}. Other works use variations of such matrices for unsupervised audio segmentation \cite{serra2012unsupervised}.

In Eq. \ref{eq:ssm} we show the expression of the SSM for a time series $Y = [y_1, ..., y_n] \in R^n$.

\begin{equation}
    \text{SSM}_{i, j} = distance(y_i, y_j)
    \label{eq:ssm}
\end{equation}

Also in the audio domain $l_1$-graph representations of audio features have been proposed for MSA \cite{PanagakisKA11}. $l_1$-graphs are a type of graphs that in which the edges between nodes are weighted based on the absolute difference between the values of their corresponding features. In this approach, audio features such as the \textit{Auditory Temporal Modulations} (ATMs),
the \textit{Mel-frequency Cepstral Coefficients} (MFCCs), and the
\textit{Chroma} features are used as inputs.


\subsection{Symbolic Music Structure Analysis}
In the symbolic domain, there have been proposed different techniques to address the melody segmentation and structure segmentation tasks.

Boundary detection in symbolic music has been studied for decades \cite{lopez2012automatic} mostly for monophonic melodic segmentation \cite{rodriguez2014comparing}, \cite{lattner2015probabilistic}, \cite{bassan2022unsupervised}. These methods can be applied to annotate large corpus of data such as \texttt{SIATEC} and \texttt{COSIATEC} algorithms \cite{meredith2002algorithms}.
The vast majority of these feature-based methods use rule-based systems or need to extract features such as the \textit{Inter Onset Intervals} (IOIs, Eq. \ref{eq:iois}) from the symbolic data to find a structural description of the music \cite{lerdahl1996generative}. Examples of that are the Local Boundary Detection Model (LBDM) \cite{Cambouropoulos01} that uses the IOIs to segment music phrases or the Grouper Program \cite{temperley2004cognition} which uses the \textit{Offset to Offset Intervals} (OOI) in addition to the IOIs. Extensions of these methods use variations of this features (e.g. local variations of IOIs) to perform the same task \cite{cenkerova2018crossing}. Similarly, the Pattern Boundary Detection Model (PAT) \cite{cambouropoulos2004influence} uses symbolic features to perform phrase segmentation. Adaptive melodic segmentation in MIDI files have also been proposed \cite{wilder2008adaptive}. 

Rule mining techniques \cite{Kranenburg20} and pseudo-supervised methods \cite{LattnerCG15} are other techniques to perform the melody segmentation task. For this purpose, there are annotated corpus \cite{LopezV13} like TAVERN \cite{tavern}. The dataset contains 27 sets of theme and variations for piano by Mozart and Beethoven.

Focusing on higher level structures, there have been proposed computational analysis techniques of different musical forms. From J. S. Bach fugues \cite{10.2307/43829264} to the Sonata form structure that has been studied from W. A. Mozart string quartets \cite{allegraud2019learning}. The techniques used in these works are mostly feature-based with harmonic and rhythmic features such as the rhythm break or the triple hammer blow.

\subsection{Symbolic Music as Graphs}
A graph $G(V, E)$ can be defined as a collection of points that have different types of connections. The points are called nodes $V$ and the connections between the nodes are the edges $E$. Graph representations are becoming popular in applications such as proteins discovery \cite{jumper2021highly} or to simulate complex physics \cite{physics}. In symbolic music, graph representations have been proposed for diverse purposes that concern MSA or music classification \cite{978-3-642-02124-4_5}.

Simonetta et al. \cite{SimonettaCOR18} used a graph-based representation to find similarities in symbolic music pieces. Jeong et al. \cite{jeong2019graph} used a graph enconding to learn note representations from music scores. Graphs have also demonstrated to be valid representations for Perfect Authentic Cadences (PAC) identification, which is closely related to structure segmentation. Karystianos and Widmer \cite{abs-2208-14819} proposed a stochastic Graph Convolutional Network (SGSMOTE) that adresses this problem in Bach Fugues, and Haydn and Mozart String Quartets. To construct the graph, notes and rests are nodes of the graph, and three types of undirected edges between nodes are defined as for consecutive notes or rests in time, for notes with the same onset and for notes whose onset happens when a longer note is already being played.

\section{Proposed methods}
\label{sec:boundaries}
Music Structure Analysis have been studied in the audio domain with the self-similarity matrices and derivations of them. In the symbolic domain, although boundary detection might seem simpler because we have basic information about notes such as pitch or note onset and offset, to our knowledge, there is not much work that has attempted to segment music structure at different levels. In this section, we propose three algorithms for music structure detection in symbolic music: Norm, G-PELT and G-Window. 

We implemented the Norm algorithm based on previous work. The method extracts the IOIs and pitch direction of the music to obtain the boundaries of the structure segments. G-PELT and G-Window are novel graph-based methods that segment different levels in the structure of symbolic music depending on the parameters values (see Table \ref{tab:optimal}) The algorithms do not need the symbolic music to be quantized, not the time signature nor the beats per minute information, which means that we can use both methods in non curated MIDI files or in musicXML files.


\subsection{Method 1: Norm}
This method is based on the normalization of the IOIs and pitch direction that gives us boundaries candidates that are used to construct a self-similarity-matrix. Since in symbolic music we do have more information about the notes than in audio signals, we construct the SSM by grouping the notes in segments and measuring the distances between the segments. In Algorithm \ref{alg:norm} we provide the pseudo-code of the Norm algorithm. The algorithm consists of the following steps:

The first step to identify the boundaries in a symbolic note sequence is to sort the notes by their \textit{onset} or \texttt{Note ON} (in MIDI notation) and compute the IOIs vector $\mathbf{x} \in R^{N-1}$ where $N$ is the number of notes in the music piece (Eq. \ref{eq:iois}):
\begin{equation} \label{eq:iois}
    x_i = \textit{onset}_{i+1} - \textit{onset}_{i} \quad i = 1 \ldots N
%\concat_{i=1}^N  onset_{i+1} - onset_{i}
\end{equation}

We define the local direction or pitch contour vector $\mathbf{l} \in R^{N-1}$ similar to previous works in which the contour intervals were stored in an array of contour intervals called COM-matrix \cite{10.2307/745814}. In this work, we compute $\mathbf{l}$ as follows: if a note is followed by a note with a higher pitch (ascendent), $l_i=1$ and $l_i=-1$ if it is lower (descendent). If the pitch of the following note is equal to the current pitch then $l_i=0$. In Eq. \ref{eq:l} we show the expression of the local direction vector:

\begin{equation}
    l_i = \left\{ \begin{array}{lcc}
             1 &   \forall  & p_{i+1} > p_i 
             \\ -1 &   \forall  & p_{i+1} < p_i 
             \\ 0 &   \forall  & p_{i+1} = p_i 
             \end{array}
             \quad i = 1 \ldots N
   \right.
    \label{eq:l}
\end{equation}
where $p_i$ refers to the pitch of the $ith$ note.

After computing $\mathbf{x}$ and $\mathbf{l}$ we sum them to construct the vector $\mathbf{\tilde{x}}$. We normalize this vector to obtain the $\mathbf{\hat{x}}$ vector by applying the z-score normalization (Eq. \ref{eq:norm}):
\begin{equation} \label{eq:norm}
    \hat{x}_i = \frac{x_i - \mu }{\sigma} \quad i = 1 \ldots N
\end{equation}
where $\mu$ is the mean and $\sigma$ the standard deviation of the whole IOIs vector.

With $\mathbf{\tilde{x}}$, we now apply a peak picking strategy. By doing this, we extract the segment boundaries candidates $\mathbf{b} = [b_1, ..., b_S] \in R^C$ with $C$ the number of boundary candidates. We select a window size $w_1$ and a threshold $\tau_1$ (standard deviation above the mean). The window size $w_1$ is defined as a function of the number of notes $N$ so $w=\frac{\alpha}{\hat{n}}$ where $\hat{n}=15$ and $\alpha$ is a constant parameter. We fixed $\hat{n}=15$ to optimize the algorithm performance.

After that, we can compute the SSM, $\mathbf{S} \in R^C \times R^C$, by grouping the notes in the obtained boundary candidates. The distance to construct the $\mathbf{S}$
is the euclidean distance. Since not all the segments have the same length, we add zeros to the segments of lesser length to calculate the distance between all of them.

Once $\mathbf{S}$ is computed, we can get its novelty curve $\mathbf{c}$ (Eq. \ref{eq:nov}) and perform again the peak picking strategy with a sliding window $w_2$ and a threshold $\tau_2$. This will give us the predicted boundaries $\mathbf{\hat{b}}$. 
\begin{equation}
    c_i = norm(s_{i+1}, s_i)
    \label{eq:nov}
\end{equation}
where \textit{norm} refers to the euclidean norm.

In Fig. \ref{ssm:a} we show the identified peaks (boundary candidates) in the normalized 
$\mathbf{\hat{b}}$ vector, and in Fig. \ref{ssm:b} the SSM with the novelty curve and the predicted boundaries for the Schubert\_D911-01.mid file of the Schubert Winterreise Dataset (SWD) .


\begin{figure}[!h]
\subfloat[Normalized IOIs and local direction vector with the boundary candidates detected after the peak picking (in red).\label{ssm:a}]{%
    \centering
    \includegraphics[width=\columnwidth]{peaks.png}
} \\
\subfloat[SSM and novelty curve with predicted boundaries (in red)\label{ssm:b}]{%
    \centering
    \includegraphics[width=\columnwidth]{ssm.png}
}

\caption{Boundary candidates (a) and SSM with its novelty curve (b) in the sample nÂº1 of Beethoven Piano Sonatas Dataset.}
\label{fig:msa_norm}
\end{figure}

The procedure of the Norm method is summed up below, and its pseudo-code can be found in Algorithm \ref{alg:norm}.

\begin{itemize}
    \item Calculate the IOI's $\mathbf{x}$ and the local direction $\mathbf{l}$ vectors of the file and sum them: $\mathbf{\tilde{x}}$.
    \item Apply z-score normalization to $\mathbf{\tilde{x}}$: $\mathbf{\hat{x}}$.
    \item Calculate the boundary candidates, $\mathbf{b}$, by peak picking with window $w_{1}$ and threshold $\tau_{1}$.
    \item Construct a Self-Similarity Matrix, $\mathbf{S}$, with the segments grouped by the boundary candidates obtained in the previous step.
    \item Get the novelty curve $\mathbf{c}$ from $\mathbf{S}$.
    \item Calculate the boundaries  $\mathbf{\hat{b}}$ by peak picking with window $w_{2}$ and threshold $\tau_{2}$.
\end{itemize}

\begin{algorithm}
    \caption{Norm}\label{alg:norm}
    \begin{algorithmic}
    \Require $w_{1}, \tau_{1}, w_{2}, \tau_{2}$ \Comment{params}
    \State $n \gets read\_file$
    \State \Comment{calculate candidates $b$}
    \State $x \rightarrow ioi(n)$
    \State $l \rightarrow local\_direction(n)$
    $\hat{x} \gets sum(x, l )$
    \State $z = z\_normalization(\hat{x})$
    \State $b \rightarrow peak\_picking(z, w_{1}, \tau_{1})$
    
    \State \Comment{group notes by candidates $b$}
    \State $g \gets$ empty list of lists
    \For{$i$ in $b$} 
    \State $g \gets append(n_i, n_{i+1})$
    \EndFor
    
    \State \Comment{construct $S$}
    \State $S \gets array_{len(b), len(b)}$
    \For{$i$ in $g$}
    \State $v_i \gets sum(ioi(g_i) \lvert\lvert local\_direction(g_i))$
    \For{$j$ in $g$}
    \State $v_j \gets sum(ioi(g_j) \lvert\lvert local\_direction(g_j))$
    \State $S_{i,j} = euclidean\_dist(v_i, v_j)$
    \EndFor \EndFor
    \State $c \gets novelty(S)$
    
    \State \Comment{get $b\prime$ and the respective note positions}
    \State $b^\prime \rightarrow peak\_picking(n, w_{2}, \tau_{2})$
    \For{$k$ in $b^\prime$}
    \State $b^\prime\_notes \gets append(b_k)$
    \EndFor
    \end{algorithmic}
\end{algorithm}


\subsection{Graph Representation} \label{sec:graph}
Our graph representation is based on previous works \cite{abs-2208-14819}, \cite{jeong2019graph}, \cite{szeto2006graph}. Since our representation is based on MIDI files, which might do not contain time signature and tempo information, neither bars nor beats will be taken into account in our representation. We only extract the time signatures from the datasets metadata for measuring the performance of our methods in Section \ref{sec:ablation}. Similar to \cite{abs-2208-14819}, we create three types of undirected connections
only between notes (nodes), however we do not encode rests since MIDI files do not provide them directly. In Eq. \ref{eq:edges} we can see the expression of the edges of our graph representation: $E^{\textit{on}}$ represent the edges between notes that occur on the same onset; $E^{\textit{cons}}$ are the edges between consecutive notes in time,
and edges $E^{h}$ represent overlapping notes in time (notes that start when there is already a note being played). The edges of the graph $E$ are defined as $E \subseteq E^{\textit{on}} \cup E^{\textit{cons}} \cup E^{h}$:

\begin{equation}
\begin{split}
    &e^{\textit{on}}_{i,j} =  \{ \textit{on}(v_i) = \textit{on}(v_j) \}
    \\
    &e^{\textit{cons}}_{i,j} = \{\textit{off}(v_i) = \textit{on}(v_j) \}
    \\
    &e^{h}_{i,j} =  \{ [\textit{off}(v_i) > \textit{on}(v_j)] \wedge [\textit{on}(v_i) < \textit{on}(v_j) ] \}
\end{split}
\label{eq:edges}
\end{equation}
where \textit{on} and \textit{off} refer to the notes \textit{onsets} and \textit{offsets}, respectively.

Nodes are not connected between themselves. In Fig. \ref{fig:repr} we show an example of 2 bars in a score, pianoroll and our graph representation.

\begin{figure}[!h]
\includegraphics[width=\columnwidth]{repr.png}
\caption{From top to down: score, pianoroll and our proposed graph representation of 2 bars of symbolic music.}
\label{fig:repr}
\end{figure}

\subsubsection{PELT Algorithm}
In this method, we encode symbolic music as a graph as we described previously to then obtain the adjacency matrix $\mathbb{A}$ of the graph. Note that constructing $\mathbb{A}$ has a quadratic complexity with the number of notes $n$ in the file: $\mathcal{O}(n^2)$. We then compute the novelty curve $\mathbb{c}$ of $\mathbb{A}$ with the expression in Eq. \ref{eq:nov}. After that, we run the PELT algorithm with \texttt{ruptures} package\footnote{\url{https://github.com/deepcharles/ruptures}, accessed February 2023.} \cite{TruongOV20}.

PELT \cite{pelt} is a changepoint detection algorithm in time series. It detects the change points of a signal by optimizing a cost function $C$ as defined in Eq. \ref{eq:cost}. 

To calculate the optimal changepoint location, the algorithm starts initializing the cost of the entire series $C_i$ to infinity, then calculates the cost of each possible changepoint location per step, selects the point $i$ that minimizes the cost and then updates the cost function with the new location $i^\prime$. In Eq. \ref{eq:cost} we show the expressions that correspond to this process. After that, the windows are reduced in size by a factor $\delta$ and the process in Eq. \ref{eq:cost} is repeated.

\begin{equation}
\begin{gathered}
   C_i = min(C_j) + \lambda(i, j) + p
    \\
    i^\prime = argmin(C_i)
    \\
    C_i = min(C_i, C_{i^\prime})
\end{gathered}
\label{eq:cost}
\end{equation}
where $i, j$ is the segment to be processed, and $p$ is the penalty, which controls the number of the detected changepoints. 

The window size $w$ controls the size of the analyzed segments per iteration, and the jump value $j$ is related to the number of changepoints skipped in each iteration. We also defined in this case the window size $w$ as a function of the number of notes $N$ so $w=\frac{\alpha}{N}$ where $N=15$ and $\alpha$ is a constant parameter. We define the jump value $j=\beta \cdot w$  as a function of the window size $w$ and a constant $\beta$. We find the optimal values of parameters $\alpha$ and $\beta$ in Section \ref{sec:ablation}.

The complexity of the method depends on the cost function used. In our case, we use the Kernelized mean change cost function RBF of Eq. \ref{eq:cost_f} which has a quadratic complexity corresponding to the sum of the computation of the kernel density estimate (KDE),  $\mathcal{O}(n^2)$, and the computation of the cost at each candidate is $\mathcal{O}(n)$. 
This leads to an overall complexity of the PELT method of $\mathcal{O}(n^2 + n log(n))$, since the number of iterations is determined by the logarithm of the ratio of the window size and the minimum size $\mathcal{O}(log(n))$.

\begin{equation}
    C(\theta) = \sum_{t=1}^{T} \left\lVert y_t - \sum_{j=1}^{k} w_j \phi\left(\frac{\left\lVert x_t - \mu_j\right\rVert^2}{\sigma^2}\right)\right\rVert^2
    \label{eq:cost_f}
\end{equation}
where $C(\theta)$ is the cost function,
$\theta = {w_j, \mu_j, \sigma^2}$ is a set of parameters,
$T$ is the number of points in the signal,
$y_t$ is the observed value at time $t$,
$x_t$ is the feature value at time $t$,
$w_j$ is the weight for the $j^{th}$ RBF component,
$\mu_j$ is the mean of the $j^{th}$ RBF component,
$\sigma^2$ is the variance of the RBF components, and
$\phi(u) = \exp(-u)$ is the RBF function
$\left\lVert \cdot \right\rVert$ is the Euclidean norm.

In Fig. \ref{fig:msa-graph} we can see the predicted boundaries in a sample of the Shuber Winterreise Dataset (SWD). We can observe how the algorithm predicts different boundaries at different levels of structure.

In Algorithm \ref{alg:g-pelt} we provide the pseudo-code of the PELT algorithm re-adapted for our graph representation and renamed to G-PELT.

\begin{algorithm}
    \caption{G-PELT}\label{alg:g-pelt}
    \begin{algorithmic}
    \Require w, j, p \Comment{params}
    \State $n \gets read\_file$
    \State $G \gets midi\_to\_graph(n)$
    \State $A \gets adjacency\_matrix(G)$
    \State $c \gets novelty(A)$
    \State $b^\prime \gets$ PELT$(n, w, j, p)$
    \end{algorithmic}
\end{algorithm}

\begin{figure*}[!t]
\centering
    \includegraphics[width=\textwidth]{ans.png}
    \caption{Adjacency matrix novelty curve of the graph built of the Beethoven piano sonata n11, 1st movement. In the figure, gt and pred refers to the ground truth annotations and predicted boundaries with out G-PELT method, respectively. The x axis refers to the notes in the file and the y axis is the value of the novelty.}
\label{fig:msa-graph}
\end{figure*}

\subsubsection{Window Algorithm}
In this method, we also encode symbolic music as a graph and compute the novelty $\mathbf{c}$ from the adjacency matrix $\mathbf{A}$ of the graph as we do in the G-PELT algorithm.
After that, we applied a sliding window algorithm. The algorithm uses two windows $y_{i, j}$ and $y_{j, k}$, that are compared by computing a discrepancy measure with the cost function $C$ of each window as we show in Eq. \ref{eq:window}.

\begin{equation}
    d(y_{i,j}, y_{j,k}) = C(y_{i,j}) - C(y_{i,k}) - C(y_{j,k})
    \label{eq:window}
\end{equation}

The window size $W$ defines the segment splits for each signal point $i$ as: $y_{i-w/2, i}, y_{i, i+w/2}$.

The computational complexity of the algorithm is
$\mathcal{O}(nw)$. We need to add the complexity of the cost function $C$. We use the same function defined for G-PELT algorithm (see Eq. \ref{eq:cost_f}), which computational complexity in this case is $\mathcal{O}(nw)$ since the complexity of computing the KDE cost function for a single window is $\mathcal{O}(w^2)$ and since the window is moved along the data, the total time required to compute the cost function for the entire data set is $\mathcal{O}(nw)$. This leads to an overall complexity of the sliding window algorithm to $\mathcal{O}(2nw)$.


In Algorithm \ref{alg:g-clasp} we provide the pseudo-code of the G-Window algorithm and in Table \ref{tab:algs} we show the proposed algorithms for the symbolic music segmentation task with their parameters and complexity.


\begin{algorithm}
    \caption{G-Window}\label{alg:g-clasp}
    \begin{algorithmic}
    \Require w, n\_p \Comment{params}
    \State $n \gets read\_file$
    \State $G \gets midi\_to\_graph(n)$
    \State $A \gets adjacency\_matrix(G)$
    \State $c \gets novelty(A)$
    \State $b^\prime \gets$ Window$(n, w, n\_p)$
    \end{algorithmic}
\end{algorithm}

\begin{table}[!h]
\caption{Parameters and complexity of the algorithms for symbolic music structure segmentation. Note that the complexity of constructing the adjacency matrix is separated from the complexity of PELT and Window algorithms.}
    \centering
    \begin{tabular}{
    p{1.5cm}
    >{\centering\arraybackslash}p{2cm}
    >{\centering\arraybackslash}p{3.5cm}
    }
    \toprule
         Algorithm & Params. & Complexity\\
         \hline
        Norm & $\alpha_{1}, \tau_{1}, w_{2}, \tau_{2}$ & $\mathcal{O}(n) + \mathcal{O}(b^2) $ \\
        G-PELT & $\alpha, \beta, p$ & $\mathcal{O}(n^2)$ + $\mathcal{O}(n^2 + n log(n))$ \\
        G-Window & $\alpha, \beta, p$ & $\mathcal{O}(n^2)$ + $\mathcal{O}(2nw)$ \\
        \bottomrule
    \end{tabular}
    \label{tab:algs}
\end{table}


\section{Ablation Study} \label{sec:ablation}
In this section we perform an ablation study to compare the methods described in Section \ref{sec:boundaries} with different parameter values. The goal of this comparison is to find the optimal parameter values for each method and dataset, and to determine the method that outperforms the others.

As we mentioned in Section \ref{sec:boundaries}, our representation is based on MIDI files where no information about bars nor beats is provided. This makes the algorithms more robust since MIDI files may not be quantized, especially files containing expressive performances. Thus, it is easier to convert a MusicXML file to MIDI and use our method for score segmentation rather than converting a MIDI file to MusicXML due to the ornamentations and the lack of symbolic information that a MIDI file provides in comparison to a score.

Being that said, we convert MIDI files into graphs. The implementation has been added to \texttt{musicaiz} package \cite{musicaiz} which uses \texttt{NetworkX} package\footnote{\url{https://github.com/networkx/networkx}, accessed November 2022.} \cite{networkx} to do the conversion. We test the algorithms performance with \texttt{mir\_eval} package \cite{raffel2014mir_eval}.

For testing, we select 2 tolerances: 1 beat and 1 bar tolerances. The 1 beat tolerance has been used previously in the cadences identification task \cite{abs-2208-14819}. We added the 1 bar tolerance to give further insights about the performance of the algorithms \footnote{Note that our tolerance values are an analogy to the boundaries detection task in the audio domain, where there are 2 tolerance values 0.5 and 3 seconds. 1 beat tolerance means that in a 3/8 bar, the beat will be the crotchet }.

\subsection{Datasets}
We use the Schubert Winterreise Dataset (SWD) which contains 24 files (MIDI, MusicXML, PDF and audios) annotated with harmony and form analysis (our task).
Since we aim to test our methods for different music forms, we also test the algorithms with the Beethoven Sonatas Dataset (BPS) which contains the annotations of the 32 first movements of Beethoven piano sonatas. However, this dataset provides the symbolic music files as \texttt{csv}, so we converted them to MIDI format with \texttt{musicaiz} package to be able to read, process and measure them against the structure annotations provided by the dataset. In Table \ref{tab:data} we show the metadata of both datasets\footnote{We exclude file 11 since it contained tempo changes}. Whereas SWD has only annotations of the structure in the middle level, BPS dataset has annotations for the three levels that we called: low, mid and high.

\begin{table}[!t]
    \normalsize
    \centering
    \caption{Datasets analytics.}
    \begin{tabular}{
    p{1.2cm}|  
    >{\centering\arraybackslash}p{0.4cm}|
    >{\centering\arraybackslash}p{1.2cm}|
    >{\centering\arraybackslash}p{1.5cm}|
    >{\centering\arraybackslash}p{2.4cm}
    }
    \hline
    Dataset & files & TSig: files & Total Bound. & Boundaries per file\\
    \cmidrule{1-5}
    \multirow{6}{*}{SWD} & \multirow{6}{*}{23} & 2/4: 7 & \multirow{6}{*}{mid: 192} & \multirow{6}{*}{mid: $8.41_{\pm 2.79}$}\\
     & & 3/4: 7 & & \\
     & & 4/4: 4 & & \\
     & & 3/8: 1 & & \\
     & & 6/8: 3 & & \\
     & & 12/8: 1 &  & \\
    \cmidrule{1-5}
    \multirow{6}{*}{BPS} & \multirow{6}{*}{31} & 2/4: 8 & \multirow{2}{*}{low: 1.439} & \multirow{2}{*}{low: $46.42_{\pm 21.55}$}\\
     & & 3/4: 6 & & \\
     & & 4/4: 13 & \multirow{2}{*}{mid: 438} & \multirow{2}{*}{mid: $14.13_{\pm 3.46}$}\\
     & & 3/8: 1 & & \\
     & & 6/8: 2 & \multirow{2}{*}{high: 115} & \multirow{2}{*}{high: $3.8_{\pm 1.05}$}\\
     & & 12/8: 1 &  & \\
    \bottomrule
    \end{tabular}
    \label{tab:data}
\end{table}

To compare the performance of our methods, we set the lower bound (renamed to baselines from now on) for each dataset following the procedure introduced in previous works \cite{grill2015music}. This limit is the worst case scenario since boundaries are placed randomly in each file. For the SWD dataset we set 5 synthetic equidistant boundaries per file and in the BPS dataset we set 4, 14 and 46 boundaries per file which correspond to the mean of the boundaries per file and level (see Table \ref{tab:data}).

\subsection{Schubert Winterreise Dataset (SWD)}
In this subsection, we test the performance of the algorithms in the SWD. To be able to process the information and convert the symbolic information to graphs, we use \texttt{musicaiz} package \cite{musicaiz} which works only with MIDI files, thus, we use the raw MIDI files of SWD.

\subsubsection{Norm}
We test the Norm algorithm varying its parameters $\alpha_1$, $\tau_1$, $w_2$, and $\tau_2$ in order to find their optimal values. In Fig. \ref{fig:norm_swd_a} we show the performance of the Norm method varying $\alpha_1$ and in Fig. \ref{fig:norm_swd_c} we vary $w_2$ with the optimal $\alpha_1$ value for 1 beat tolerance. In Figs. \ref{fig:norm_swd_b} and \ref{fig:norm_swd_d} we repeat the same procedure but testing the algorithm with 1 bar tolerance.

\begin{figure*}[!ht]
\subfloat[Norm 1 beat with $\alpha_1$ variation and $\tau_1$=1, $w_2$=2, $\tau_2$=0.5. \label{fig:norm_swd_a}]{%
    \centering
    \includegraphics[width=\columnwidth]{norm_1_beat_alpha1.png}
}
\subfloat[Norm 1 bar with $\alpha_1$ variation and $\tau_1$=1, $w_2$=2, $\tau_2$=0.5. \label{fig:norm_swd_b}]{%
    \centering
    \includegraphics[width=\columnwidth]{norm_1_bar_alpha1.png}
} \\
\subfloat[Norm 1 beat with $\alpha_1$ variation and $\tau_1$=1, $w_2$=2, $\tau_2$=0.5. \label{fig:norm_swd_c}]{%
    \centering
    \includegraphics[width=\columnwidth]{norm_1_beat_w2.png}
}
\subfloat[Norm 1 bar with $w_2$ variation and $\tau_1$=1, $\alpha_1$=0.6, $\tau_2$=0.5.  \label{fig:norm_swd_d}]{%
    \centering
    \includegraphics[width=\columnwidth]{norm_1_bar_w2.png}
} \\
\caption{Comparison of Norm method in the SWD dataset setting different $\alpha_1$ and $w_2$ values for 1 beat and 1 bar tolerances.}
\label{fig:norm_w}
\end{figure*}

The results show that there is no a huge variation of the metrics when measuring the results with 1 beat tolerance, however, when testing the results with 1 bar tolerance the sensibility of the results against $\alpha$ and $w_2$ increases. The best performing parameters for both 1 beat and 1 bar tolerances are $\alpha$=0.6,  $w_2$=2, $\tau_1$=1 and $\tau_2$=0.5 with a $F_1$=0.3029 for 1 beat and $F_1$=0.4228 for 1 bar tolerances (see Table \ref{tab:msa}).

\begin{table}[!t]
    \centering
    \caption{MSA results for the algorithms with optimal parameters and measured with SWD.}
    \begin{tabular}{
    p{1.4cm}|  
    >{\centering\arraybackslash}p{1.5cm}
    >{\centering\arraybackslash}p{1.5cm}
    >{\centering\arraybackslash}p{1.5cm}
    }
    \cmidrule{1-4}
    & \multicolumn{3}{c|}{SWD}\\
    \hline
    & P & R & $F_1$\\
    \cmidrule{1-4}
        \rowcolor{Gray}
        & \multicolumn{3}{c|}{Tolerance: 1 beat}\\
        \cmidrule{1-4}
        baseline & $0.2782_{\pm 0.12}$ & $0.1980_{\pm 0.08}$ & $0.2258_{\pm 0.08}$\\
        Norm $\mathbf{b}$ & $0.2017_{\pm 0.14}$ & $0.4352_{\pm 0.31}$ & $0.2556_{\pm 0.17}$\\
        Norm $\mathbf{b}^\prime$ & $0.4398_{\pm 0.30}$ & $0.2450_{\pm 0.15}$ & $0.3029_{\pm 0.19}$\\
        G-PELT & $0.2665_{\pm 0.15}$ & $0.5265_{\pm 0.28}$ & $\mathbf{0.3455_{\pm 0.18}}$\\
        G-Window & $0.3066_{\pm 0.21}$ & $0.3372_{\pm 0.29}$ & $0.2869_{\pm 0.20}$\\
        \cmidrule{1-4}
        \rowcolor{Gray}
        & \multicolumn{3}{c|}{Tolerance: 1 bar}\\
        \cmidrule{1-4}
        baseline & $0.4956_{\pm 0.21}$ & $0.3619_{\pm 0.19}$ & $0.4078_{\pm 0.18}$\\
        Norm $\mathbf{b}$ & $0.3415_{\pm 0.18}$ & $0.7199_{\pm 0.31}$ & $0.4298_{\pm 0.18}$\\
        Norm $\mathbf{b}^\prime$ & $0.5884_{\pm 0.32}$ & $ 0.3588_{\pm 0.24}$ & $ 0.4228_{\pm 0.24}$\\
        G-PELT & $0.4366_{\pm 0.13}$ & $0.8473_{\pm 0.17}$ & $\mathbf{0.5640_{\pm 0.12}}$\\
        G-Window & $0.5371_{\pm 0.23}$ & $0.5527_{\pm 0.29}$ & $0.4863_{\pm 0.19}$\\
         \bottomrule
         %\cmidrule{3-4}
    \end{tabular}
    \label{tab:msa}
\end{table}

\subsubsection{G-PELT}
We follow the same procedure as done with the Norm method to find the optimal parameter values for the G-PELT method: $\alpha$ and $\beta$. We fix the penalty $p$ to 0.7 which we found to perform better than other values. In Fig. \ref{fig:pel_min_size} we show the results of our experiments with G-PELT method in the SWD.

\begin{figure*}[!ht]
\subfloat[G-PELT 1\_beat with $\alpha$ variation and $\beta$=0.15.\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{pelt_1_beat_alpha.png}
}
\subfloat[G-PELT 1\_bar with $\alpha$ variation and $\beta$=0.15\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{pelt_1_bar_alpha.png}
} \\
\subfloat[G-PELT 1\_beat with $\beta$ variation and $\alpha$=0.6\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{pelt_1_beat_jump.png}
}
\subfloat[G-PELT 1\_bar with $\beta$ variation and $\alpha$=0.6\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{pelt_1_bar_jump.png}
} \\
\caption{Comparison of G-PELT method in the SWD dataset setting different $\alpha$ and $\beta$ values for 1 beat and 1 bar tolerances. The penalty value is fixed to 0.7.}
\label{fig:pel_min_size}
\end{figure*}

Looking at the results in Fig. \ref{fig:pel_min_size}, we can see that the method is sensible to the $\alpha$ parameter, specially in the Recall. The best performing parameter values are $\alpha$=0.6, $\beta$=0.15 and $p$=0.7 for both 1 beat and 1 bar tolerances with a $F_1$=0.3455 for 1 beat and $F_1$=0.5640 for 1 bar tolerances (see Table \ref{tab:msa}).


\subsubsection{G-Window}
As we did with the previous methods, we find the optimal parameter values for the G-Window method: $\alpha$ and $\beta$. We fix the penalty $p$ to 0.5 in this case. In Fig. \ref{fig:window_min_size} we show the results of our experiments with G-Window method in the SWD.


\begin{figure*}[!ht]
\subfloat[G-Window 1\_beat with $\alpha$ variation.\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{window_1_beat_alpha.png}
}
\subfloat[G-Window 1\_bar with $\alpha$ variation.\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{window_1_bar_alpha.png}
} \\
\caption{Comparison of G-Window method in the SWD dataset setting different $\alpha$ values for \texttt{1\_beat} and \texttt{1\_bar} tolerances. The penalty value is fixed to 0.5.}
\label{fig:window_min_size}
\end{figure*}

The results in Fig. \ref{fig:window_min_size} show that the G-Window method is also sensible to its parameter $\alpha$. We found that the best performing parameter values are $\alpha$=1 and $p$=0.5 for both 1 beat and 1 bar tolerances with a $F_1$=0.2869 for 1 beat and $F_1$=0.4863 (for 1 bar tolerances see Table \ref{tab:msa}).


\subsection{Beethoven sonatas for Piano (BPS)}
After testing the performance of our algorithms with the SWD, we now follow a similar procedure with the BPS dataset. In spite that the BPS dataset was originally proposed for symbolic harmonic analysis \cite{bps}, it also contains structure annotations of the first movements of Beethoven sonatas for piano at 3 levels, which makes it suitable for MSA \cite{GothamI19}. The structure annotations for each file are stored in \texttt{phrases.xlsx}\footnote{File 31 end has an error since a segment start must be minor than the end. We fixed it changing the value 246 to 346 that are the actual beats in the file.}. To test the performance of our methods with this dataset, we follow the same procedure that we described previously for the SWD. The only difference between this dataset and the SWD is that SWD provides only middle level annotations whereas in the BPS we will analyzed the low, middle and high structure levels since the dataset provides the annotations per level.
To show how the novelty of the graph adjacency matrix represents well the structure of the symbolic music, and to show an example of a file, we present 
in Fig. \ref{fig:msa-graph} the predicted boundaries in a sample of the BPS dataset with the G-PELT method. 

\subsubsection{G-PELT}
As we did for the SWD, we optimize the parameter values for the BPS dataset. However, since we found that the best performing method in the SWD was G-PELT, we will optimize the values of the parameters for this algorithm for the three structure levels and we will compare the performance in each level with the other methods (Norm and G-Window). In Table \ref{tab:msa_levels} we show the results of the ablation of the three methods for each structure level and tolerance.

Similar to the ablation with the SWD, we first vary $\alpha$ and then with the optimal $\alpha$ value we find the optimal $\beta$. The penalty is fixed and obtained for each structure level. In Fig. \ref{fig:pelt_bps} we show a comparison of different parameter values for G-PELT method in the structure levels (low, mid and high) in the BPS dataset.

\begin{figure*}[!ht]
\subfloat[High-level G-PELT 1\_bar with $\alpha$ variation and $\beta$=1.5.\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{pelt_bps_1_bar_alpha_high.png}
}
\subfloat[High-level G-PELT 1\_bar with $\beta$ variation and $\alpha$=2.3.\label{1a}]{%
    \centering
\includegraphics[width=\columnwidth]{pelt_bps_1_bar_jump_high.png}
} \\
\subfloat[Mid-level G-PELT 1\_beat with $\alpha$ variation and $\beta$=0.01.\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{pelt_bps_1_beat_alpha_mid.png}
}
\subfloat[Mid-level G-PELT 1\_beat with $\beta$ variation and $\alpha$=1.\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{pelt_bps_1_beat_jump_mid.png}
} \\
\subfloat[Low-level G-PELT 1\_beat with $\alpha$ variation and $\beta$=0.15.\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{pelt_bps_1_beat_alpha_low.png}
}
\subfloat[Low-level G-PELT 1\_beat with $\beta$ variation and $\alpha$=0.1.\label{1a}]{%
    \centering
    \includegraphics[width=\columnwidth]{pelt_bps_1_beat_jump_low.png}
} \\
\caption{Comparison of G-PELT method in the BPS dataset setting different $\alpha$ and $\beta$ values for the high, mid and low levels. High level is tested with 1 bar and mid and low levels with 1 beat tolerances. The penalty value is fixed to 4, 0.5 and 0.1 for the high, mid and low levels, respectively.}
\label{fig:pelt_bps}
\end{figure*}

Comparing the results with the SWD in general, we found similarities as for the high Recall values in comparison with the Precision. However, in the mid level the Precision is higher than the Recall for almost all the $\alpha$ and $\beta$ tested values. This means that the method predicts lower False Positives and higher False Negatives, or in other words, that the method will miss real boundaries (lower Recall) but the predicted boundaries will be more likely to be the real ones (higher Precision).

After the ablation of the parameters for each model and dataset, in Table \ref{tab:optimal} we show the optimal parameters for both SWD and BPS datasets.

\begin{table}[!h]
\caption{Optimal parameters and for each algorithm and dataset.}
    \centering
    \begin{tabular}{
    p{.5cm}
    >
    {\centering\arraybackslash}p{.5cm}
    >{\centering\arraybackslash}p{1.52cm}
    >{\centering\arraybackslash}p{4cm}
    }
    \toprule
         Dataset & level & Algorithm & Params\\
         \hline
        \multirow{4}{*}{SWD} & \multirow{4}{*}{mid} & \multirow{2}{*}{Norm} & $\alpha_{1}=0.6$, $\tau_{1}=1 $ \\
         & & & $\alpha_{2}=2$, $\tau_{2}=0.5$ \\
        & & G-PELT & $\alpha=0.6$, $\beta=0.15$, $p=0.7$ \\
        & & G-Window & $\alpha=1$, $p=0.5$ \\
         \cmidrule{1-4}
         \multirow{3}{*}{BPS} & \multirow{1}{*}{high} & G-PELT & $\alpha=2.3$, $\beta=1.5$, $p=4$ \\
        \cmidrule{2-4}
        & mid & G-PELT & $\alpha=1$, $\beta=0.01$, $p=0.5$ \\
        \cmidrule{2-4}
        & low & G-PELT & $\alpha=0.1$, $\beta=0.15$, $p=0.1$ \\
        \bottomrule
    \end{tabular}
    \label{tab:optimal}
\end{table}

\section{Evaluation} \label{sec:eval}
After finding the optimal parameter values of the algorithms in Section \ref{sec:ablation}, we test the model in the two datasets with the best hyperparameters. We also make an analysis of the errors in beats that the algorithm commits when it makes a prediction to proof its precision.

We measured the errors across the datasets to give more insights about the algorithms precision in terms of time tolerance. In Fig. \ref{fig:errors} we show the histogram of errors in beats that the G-PELT algorithm makes in relation with the number of boundaries detected.
We can observe that the most common error rage is from 0 to 10 beats, which means that when the algorithm makes a prediction, it is relatively close to the real boundary.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{errors.png}
    \caption{Number of errors (vertical axis) in beat units (horizontal axis) measured in the predicted boundaries with G-PELT method in the SWD dataset.}
    \label{fig:errors}
\end{figure}


The results in Table \ref{tab:msa} show  the performance in the SWD, however, the analysis can be confusing due to the fact that the SWD does not have annotations per structure level. In order to do a deeper analysis and show how the algorithms perform in the different levels of the structure of music, we also test the algorithms with the BPS dataset. This dataset provides annotations at the 3 structure levels in Western classical music. Since the dataset contains the Beethoven piano sonatas, the levels are organized as: high-level structure (exposition, development and recapitulation), themes (1st, tr, 2nd, coda, etc), and phrases or motifs (a, a', etc). Low level boundaries contain the mid and high level ones, and mid level boundaries also contain high level ones. With this annotations we can measure the algorithms performance in various levels and demonstrate in which level they perform better.
In Table \ref{tab:msa_levels} we show the performance metrics of the algorithms measured with the BPS dataset.

\begin{table*}[!t]
    \centering
    \caption{MSA results for the algorithms with optimal parameter values measured with the BPS dataset.}
    \begin{tabular}{
    p{1.1cm}|  
    >{\centering\arraybackslash}p{1.2cm}
    >{\centering\arraybackslash}p{1.2cm}
    >{\centering\arraybackslash}p{1.5cm}|
    >{\centering\arraybackslash}p{1.2cm}
    >{\centering\arraybackslash}p{1.2cm}
    >{\centering\arraybackslash}p{1.5cm}|
    >{\centering\arraybackslash}p{1.2cm}
    >{\centering\arraybackslash}p{1.2cm}
    >{\centering\arraybackslash}p{1.15cm}
    }
    \cmidrule{1-10}
    & \multicolumn{3}{c|}{High Level} & \multicolumn{3}{c|}{Mid Level} & \multicolumn{3}{c|}{Low Level}\\
    \hline
    & P & R & $F_1$ & P & R & $F_1$ & P & R & $F_1$\\
    \cmidrule{1-10}
        \rowcolor{Gray}
        & \multicolumn{9}{c|}{Tolerance: 1 beat}\\
        \cmidrule{1-10}
        baseline & $0.1875_{\pm 0.10}$ & $0.3041_{\pm 0.21}$ & $0.2233_{\pm 0.13}$ & $0.1272_{\pm 0.06}$ & $0.1169_{\pm 0.08}$ & $0.1155_{\pm 0.06}$ & $0.2533_{\pm 0.09}$ & $0.2656_{\pm 0.17}$ & $0.2417_{\pm 0.09}$ \\
        Norm $\mathbf{b}$ & $0.0579_{\pm 0.03}$ & $0.4559_{\pm 0.23}$ & $0.0987_{\pm 0.05}$ &  $0.1036_{\pm 0.04}$ & $0.3773_{\pm 0.23}$ & 
        $0.1573_{\pm 0.07}$ &
        $0.1829_{\pm 0.06}$ & $0.7612_{\pm 0.19}$ & $0.2857_{\pm 0.08}$ \\
        Norm $\mathbf{b}^\prime$  & $0.2585_{\pm 0.13}$ & $0.3906_{\pm 0.15}$ & $\mathbf{0.2880_{\pm 0.10}}$ &  $0.2378_{\pm 0.14}$ & $0.1038_{\pm 0.07}$  & $0.1308_{\pm 0.07}$ & $0.2969_{\pm 0.12}$ & $0.2450_{\pm 0.17}$ & $0.2489_{\pm 0.13}$\\
        G-PELT & $0.2765_{\pm 0.16}$ & $0.3114_{\pm 0.20}$ & $0.2686_{\pm 0.15}$ & $0.2540_{\pm 0.13}$ & $0.2045_{\pm 0.17}$ & $\mathbf{0.2137_{\pm 0.13}}$ & $0.2896_{\pm 0.07}$ & $0.6033_{\pm 0.20}$ & $\mathbf{0.3716_{\pm 0.08}}$\\
        G-Wind. & $0.2092_{\pm 0.25}$ & $0.2614_{\pm 0.31}$ & $0.2183_{\pm 0.26}$ & $0.2436_{\pm 0.15}$ & $0.1531_{\pm 0.11}$ & $0.1724_{\pm 0.10}$ & $0.4816_{\pm 0.22}$ & $0.1205_{\pm 0.08}$ & $0.1849_{\pm 0.11}$ \\
        \hline
        \rowcolor{Gray}
        & \multicolumn{9}{c|}{Tolerance: 1 bar}\\
        \hline
        baseline & $0.2578_{\pm 0.07}$ & $0.4088_{\pm 0.18}$ & $0.3040_{\pm 0.10}$ & $0.2611_{\pm 0.08}$ & $0.2557_{\pm 0.17}$ & $0.2447_{\pm 0.11}$ & $0.4415_{\pm 0.08}$ & $0.4676_{\pm 0.25}$ & $0.4254_{\pm 0.13}$ \\
        Norm $\mathbf{b}$ & $0.0862_{\pm 0.06}$ & $0.6432_{\pm 0.26}$ & $0.1455_{\pm 0.09}$ & $0.1820_{\pm 0.06}$ & $0.6347_{\pm 0.25}$ & $0.2755_{\pm 0.09}$ & $0.2506_{\pm 0.06}$ & $0.9711_{\pm 0.19}$ & $0.3854_{\pm 0.08}$ \\
        Norm $\mathbf{b}^\prime$ & $0.2708_{\pm 0.13}$ & $0.4140_{\pm 0.18}$ & $0.3028_{\pm 0.11}$ & $0.3674_{\pm 0.20}$ & $0.1721_{\pm 0.13}$ & $0.2132_{\pm 0.13}$ & $0.5904_{\pm 0.12}$ & $0.4458_{\pm 0.17}$ & $0.4696_{\pm 0.13}$ \\
        G-PELT & $0.3437_{\pm 0.27}$ & $0.3585_{\pm 0.30}$ & $\mathbf{0.3361_{\pm 0.26}}$ & $0.3865_{\pm 0.17}$ & $0.3024_{\pm 0.19}$ & $\mathbf{0.3224_{\pm 0.16}}$ & $0.4370_{\pm 0.15}$ & $0.8434_{\pm 0.13}$ & $\mathbf{0.5473_{\pm 0.12}}$ \\
        G-Wind. & $0.2466_{\pm 0.27}$ & $0.3192_{\pm 0.36}$ & $0.2617_{\pm 0.28}$ & $0.3566_{\pm 0.19}$ & $0.2345_{\pm 0.16}$ & $0.2611_{\pm 0.14}$ & $0.7538_{\pm 0.17}$ & $0.1989_{\pm 0.12}$ & $0.2999_{\pm 0.15}$ \\
         \bottomrule
         %\cmidrule{3-4}
    \end{tabular}
    \label{tab:msa_levels}
\end{table*}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{ans_bps.png}
    \caption{Beginning of the 1st movement of the Beethoven Piano Sonata n2 segmented by our G-PELT method with optimal parameter values. Predicted segments are colored in red and blue. Ground truth is marked in bright green (low level, 5th column in the csv) and dark green (mid level, 6th column in the csv) lines above the staff.}
    \label{fig:test}
\end{figure*}

In \ref{ap:2} we see another example of the G-PELT algorithm performance in a Debussy piece, which differs in period and form from the SWD for wich the parameters have beeen optimized.


\section{Discussion} \label{sec:discussion}
The best-performing algorithm for the SWD is G-PELT as we show in Table \ref{tab:msa} with $P=0.4366$, $R=0.8473$ and $F_1=0.5640$ (optimal parameters are shown in Table \ref{tab:optimal}). The high Recall value means that the algorithm is capable of identify boundaries in the ground truth and that it does not predict boundaries that are not in the ground truth (low number of False Negatives). However, the low Precision shows that the algorithm does not detect a high amount of the real boundaries (high number of False Positives). This is an indicator that the algorithm could be used in scenarios where the recall of the algorithm is a priority.
 

When testing the performance of the methods in the BPS dataset, we find differences in the results depending on the structure level we are working with. About the performance in the high-level, we should clarify that it is more difficult to predict only 3 to 5 boundaries versus the 40 to 60 that the low-level presents per file (see Table \ref{tab:data}). However, the algorithms detect the end of the file boundary (that have been added also in the baselines), which is translated as the addition of a True Positive. The impact of this in the high-level, that has less boundaries per file, leads to having a higher impact than in the mid and low levels. The reason why the expectation of a decrease in the performance in the high-level analysis come from the fact that if a file has $\sim$800 beats (BPS file n1) and 3 boundaries in the high level, the boundaries represent less than the 0.5\% of the beats making it difficult for the algorithm to perform more accurately at lower tolerance levels (\texttt{1\_beat}). It is worth noting that the same applies to the baseline, as the number of boundaries in annotations increases, it becomes more likely to increase the number of true positives because there are also more synthetic boundaries per file.

An example of the performance of the G-PELT method in a Beethoven piano sonata is shown in Fig. \ref{fig:test}. In the figure, we show the predicted sections with colors and the ground truth annotations of the BPS dataset in the green lines above the score\footnote{The ground truth has been extracted from \url{https://github.com/MarkGotham/Taking-Form/blob/master/corpus/Beethoven_Sonatas/sonata2op2no2movt1.csv}}. We would like to highlight how the algorithm identifies sections that are not labeled in the dataset annotations, such as cadences. As an example, there is an annotation in bar 58 (theme D start) that, for our understanding, does not correspond to the real boundary. In this case, the method identifies as the real boundary the bar 59 which is more likely to be the real one due to the rhythm changes in the left hand. The same happens in bar 32 (theme C) and in bar 66 where the annotation seem to be displaced from the real boundary. Being that said, we would suggest that the BPS dataset structure annotations should be revised.



\section{Conclusion} \label{sec:conclusions}

We presented three methods that aim to segment symbolic music in its structure or form. After measuring the performance of each method in the context of symbolic structure segmentation with two public datasets, we can conclude that the best performing method is G-PELT for both SWD and BPS datasets and structure levels.
We give evidence of how by changing the parameters of the algorithms, music can be segmented into different levels of structure, opening new possibilities to better understand how music is made and opening new scenarios in applications such as music generation or classification.
The proposed methods are online and unsupervised which makes them usable to process large corpus of data, however, for a deeper analysis of a particular form, a learning algorithm might be needed to improve the results of this work.
We suggest to revisit the annotations of the BPS dataset by a group of expert musicians. This could be done with the help of our G-PELT method which is our best performing method, as a guide when analyzing the low level structure.

\subsection{Applications}
The presented segmentation method can be used in applications such as music generation or data augmentation. In music generation, current tokenizers such as the Multi-Track Music Machine (MMM) \cite{ens2020mmm} do not provide section tokens due to the fact that they are not encoded in MIDI-like nor score-like formats. With our method, these section tokens could be included (at beat or bar level) to generate symbolic music and being able to inpaint sections, e.g., readapting the MMMBar tokenization to include section tokens. The fast online proposed method helps tokenizing the files faster than offline methods.

Apart from that, in applications such as fingering, data augmentation is difficult due to the fact that common techniques (pich shifting, etc) cannot be applied since they change the fingering or difficulty of the task. Being able to segment music in coherent parts might help these models for training purposes. Automatic structural segmentation can also help annotators when it comes to annotating the structure at a large corpus of data.

Future lines of work might be building a deep neural graph network that segments music based on this methods. In addition, having a better performance in the segmentation task might open future works about structure labelling or classification. Apart from that, since we only use temporal information to construct the graph, we propose adding harmonic information in the novelty curve obtained from the adjacency matrix to improve the results of this work. We did not include them due to the fact that these features need to be predicted as they are not encoded in a MIDI file, but a future work might be to encode the chord progression predicted with recent work on harmonic analysis (e.g. AugmentedNet \cite{augmentednet}) to better capture the structure boundaries since harmony and structure are closely related to each other.





%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-harv} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.


\appendix
\section{Structure Encoding Example}
Sctructure segmentation can be used to tokenize sections and train large language models that generate music. In Fig. \ref{fig:tokens} we show an example based on the MMMTrack tokenizer \cite{ens2020mmm} that includes the section tokens. 

\begin{figure*}[!h]
 \centerline{
 \includegraphics[width=.7\textwidth]{map_tokens.png}}
 \caption{A general encoding based on the MMM. The figure shows a general scheme configuration of how the MMMTrack encoding extended for conditioning the generation at different levels. We extend the three levels proposed in the original MMM by adding a structure level.}
 \label{fig:tokens}
\end{figure*}

\section{G-PELT Test Example} \label{ap:2}

With the G-PELT algorithm with optimized parameters with SWD we show how the algorithm works in other music styles and periods. In Fig. \ref{fig:test} we show an example of a Vals by Debussy which is a different form and a composer from a different period in the Western classical music than the Schubert's Winterreise songs.
In the figure, it can be seen that the algorithm segments coherent sections attending to the rhtyhm and texture of the piece, in spite that the parameter values have been optimized for other form and style.


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{debussy.png}

\caption{Vals romantique by Debussy (bars 63-93) segmented by our G-PELT method with parameters optimized for Shubert Winterreise Dataset.}
\label{fig:test}
\end{figure*}

% use section* for acknowledgment
\section*{Acknowledgment}


The authors would like to thank Pedro Ramoneda for his insightful comments.

This research has been partially supported by the Spanish Science, Innovation and University Ministry by the RTI2018-096986-B-C31 contract and the Aragonese Government by the AffectiveLab-T60-20R project.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)


% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\bibliographystyle{IEEEtran}
\bibliography{references}


% that's all folks
\end{document}


