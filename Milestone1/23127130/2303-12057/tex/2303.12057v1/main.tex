\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}
\usepackage{graphicx} 
\usepackage{url}
\usepackage{dcolumn}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{appendix}
\usepackage{amsmath}
\usepackage{cmbright}
\usepackage{subcaption}

\usepackage{natbib}
\renewcommand\bibname{References}
\bibliographystyle{chicago2}

\title{Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting}
\author{Patrick Y. Wu \and Joshua A. Tucker \and Jonathan Nagler \and Solomon Messing}
\date{Center for Social Media and Politics, New York University \\ \today}

\begin{document}

\maketitle

\begin{abstract}
\singlespacing
\noindent The mass aggregation of knowledge embedded in large language models (LLMs) holds the promise of new solutions to problems of observability and measurement in the social sciences. We examine the utility of one such model for a particularly difficult measurement task: measuring the latent ideology of lawmakers, which allows us to better understand functions that are core to democracy, such as how politics shape policy and how political actors represent their constituents. We scale the senators of the 116th United States Congress along the liberal-conservative spectrum by prompting ChatGPT to select the more liberal (or conservative) senator in pairwise comparisons. We show that the LLM produced stable answers across repeated iterations, did not hallucinate, and was not simply regurgitating information from a single source. This new scale strongly correlates with pre-existing liberal-conservative scales such as NOMINATE, but also differs in several important ways, such as correctly placing senators who vote \textit{against} their party for far-left or far-right ideological reasons on the extreme ends. The scale also highly correlates with ideological measures based on campaign giving and political activists' perceptions of these senators. In addition to the potential for better-automated data collection and information retrieval, our results suggest LLMs are likely to open new avenues for measuring latent constructs like ideology that rely on aggregating large quantities of data from public sources. 
\end{abstract}

\newpage
\section{Introduction}
\label{sec:intro}
We evaluate whether generative large language models can be useful for scaling in the social sciences. Measuring latent ideology reduces the dimensionality of complex actions and stances lawmakers take into a single measure, which when combined with other data allows us to assess core democratic functions: how well lawmakers represent their constituents, how electoral competition impacts centrist and more extreme policies, and whether enacted policies have broad support or are driven by one part of the ideological spectrum \citep[see, e.g.,][]{poole1997ideology,swers_1998,clinton_jackman_rivers_2004,highton_rocca_2005,russell_2021}. This problem has been extensively studied in the U.S. national legislature, providing a widely-accepted and well-validated set of scales to which we can compare LLM-driven scaling. At the same time, traditional approaches to measuring latent ideology have limitations that our LLM-driven approach may be able to overcome. 

To do so, we attempt to estimate the latent ideological scores of the 116th United States Congress. We propose a method that begins with prompting ChatGPT\footnote{We will be using ChatGPT based on GPT-3.5 for this paper. As of the writing of this paper, ChatGPT based on GPT-4 was just made available, but there is no API access for GPT-4 and the number of messages that could be sent per hour using the web interface was severely limited.} to pick the senator that is more liberal (or conservative) in a given pair. This is a zero-shot learning setting because we do not provide ChatGPT with \textit{any additional information} besides the senator's name, party affiliation, and state represented, and we do not include any examples of pairwise comparisons. We use the Bradley-Terry model \citep{bradleyterry1952} to estimate a latent ideological score for each senator, which we call ChatScores.

We use pairwise comparisons because ChatGPT often does not consistently return a full list of senators ranked from most liberal to most conservative: it will sometimes only return partial lists, the most ideologically extreme senators, or simply refuse to create such a list. Pairwise comparisons have also been used extensively in social science scaling applications, such as ranking the persuasiveness of arguments.\footnote{\citet{LOEWEN2012212} conduct a survey experiment with pairwise comparisons and the Bradley-Terry model to determine which arguments are most persuasive. \citet{carlson_montgomery_2017}'s SentimentIt R package conducts pairwise comparisons to label political texts. \citet{hopkins_noel_2022}, a closely-related work to this study, uses pairwise comparisons among political activists to scale senators of the 114th Congress and the 117th Congress along the liberal-conservative continuum.} 

We proceed as follows. We begin by quickly reviewing large language models such as ChatGPT. We then make pairwise comparisons using ChatGPT and construct a scale. From this scale, we can determine whether ChatGPT
\begin{enumerate}
    \item hallucinates (i.e., produces incorrect or incoherent answers) in pairwise comparisons, creating an incoherent measure
    \item simply parrots answers from conventional liberal-conservative scales such as NOMINATE \citep{keith_poole_nominate,poole_2005, carroll_lewis_lo_poole_rosenthal}
    \item synthesizes the information it was trained on, leading to a unique scale
\end{enumerate}
The scale constructed using ChatGPT's pairwise comparisons aligns most with the third outcome. Although what we will call ``ChatScores'' and the first dimension of DW-NOMINATE, the most popular liberal-conservative scaling of senators, highly correlate, we also find that ChatScores depart from DW-NOMINATE scores in important ways.\footnote{We use NOMINATE and DW-NOMINATE interchangeably in this paper.} For example, the scale places more liberal Republican senators to the left of certain conservative Democratic senators in ways most political scientists would expect. The scale also solves the ``ends against the middle'' problem, in which NOMINATE estimates extreme senators to be less liberal or less conservative because they vote \textit{against} their party for ideologically extreme reasons \citep{duck-mayr_montgomery_2023}.

Because ChatGPT seems to create a unique scale, we compare ChatScores to other scales of ideology using bivariate and multivariate analyses. We find that ChatScores are also highly correlated with other liberal-conservative ideology scales. We also find that ChatScores predict human evaluations of the ideologies of senators better than other measures, including NOMINATE.

We show that ChatGPT is not hallucinating; it is not simply parroting pre-existing measures of the ideologies of senators; it responds consistently when prompted repeatedly about political ideology; and it can be prompted to evaluate pairwise comparisons that analysts can use to construct sensible and novel scales. ChatScores suggest that ChatGPT (and LLMs more generally) can be used to create scales that appear to synthesize a vast amount of information about policy positions, voting behaviors, campaign-giving patterns, and public perceptions about politicians. We expect these scales to further improve when the underlying large language models improve as well. This brief study suggests that generative large language models can return reasonable responses about American politics and have the potential to dramatically shape text-as-data methods in the social sciences in the very near future, and provides a method for doing so for one of the most important scaling questions in American politics. 

\section{A Brief Overview of ChatGPT}
\label{sec:chatgpt_overview}
ChatGPT stands for Chat \textbf{G}enerative \textbf{P}retrained \textbf{T}ransformer. Given an initial text input called the prompt, ChatGPT generates a response. ChatGPT is built on GPT-3 \citep{gpt3_paper}, a complex neural network (specifically, a decoder-only transformer) that predicts what token appears next given the set of existing tokens. It is trained on a massive corpus of text, which includes a filtered version of Common Crawl, WebText2, Books1, Books2, and Wikipedia. \citet{jay_alammar_illustrated_gpt} explains and illustrates in much greater detail how decoder-only transformer large language models are trained. In short, its ability to produce coherent textual responses comes from the massive training corpora, the sheer size of the neural network (175 billion parameters), and the self-attention mechanism. The self-attention mechanism, in effect, allows the model to dynamically upweight and downweight certain parts of the input sequence \citep{attention_is_all_you_need}. It achieves this by assigning a weight to each element of the input sequence that reflects the importance of that element relative to other elements.

ChatGPT is specifically trained to be a chatbot using a reinforcement learning technique called \textbf{r}einforcement \textbf{l}earning from \textbf{h}uman \textbf{f}eedback, or RLHF \citep[see, e.g.,][]{early_rlhf}. RLHF works by first obtaining the generated text from the model. Human annotators then rank the output from most preferred to least preferred. A reinforcement learning algorithm---specifically, proximal policy optimization \citep{ppo_algorithm}---is then used to update the large language model. For a more detailed discussion about RLHF, see \citet{lambert2022illustrating}. The addition of RLHF training enables ChatGPT to generate human-like responses. ChatGPT has garnered widespread attention across mainstream and social media. It is important to note, however, that ChatGPT can still produce incorrect responses (and sound extremely confident in doing so!), a phenomenon referred to as ``hallucination.'' It is also well-known that GPT-3 can generate text with biases, negative stereotypes, and unfair associations \citep[see, e.g.,][]{lucy-bamman-2021-gender}. Anecdotal evidence suggests that ChatGPT is just as prone to producing such output.

Social scientists have also started studying the properties and applications of large language models. \citet{ornstein_etal_stochastic_parrots} find that large language models such as GPT-3 can be effectively applied in few-shot learning contexts to text-as-data tasks in political science, including sentiment analysis, ideological scaling, and topic modeling.  \citet{argyle_busby_fulda_gubler_rytting_wingate_2023} find that biases in GPT-3 are fine-grained and demographically correlated, and can be used to emulate partisan responses from a wide variety of human subgroups. \citet{palmer_spirling_2023} find that OPT-30B \citep{opt_2022} can produce persuasive, novel arguments about politics and policies. What we are specifically interested in is seeing if ChatGPT can be used for measurement. In other words, we are interested in analyzing if ChatGPT can be used to create a coherent scale of politicians' ideologies and, if so, to provide a method for doing so accurately.

\section{Method: Using ChatGPT to Measure the Ideologies of Politicians}
\label{sec:chat_gpt_scaling}
We analyze the senators of the 116th Congress, which ended on January 3, 2021. We use this particular congress because ChatGPT is trained on information up to 2021. We do not look at previous congresses in order to prevent the newest information ChatGPT learns from leaking into assessments of ideologies of members of previous congresses. We obtain this list of senators from Voteview \citep{voteview2021}. We keep Martha McSally (R-AZ) and Kelly Loeffler (R-GA) on the list of senators.\footnote{Martha McSally was appointed to the Senate following interim Senator Jon Kyl's resignation. She then ran in Arizona's special election to finish the remainder of the Senate term, but lost to Mark Kelly. Similarly, Kelly Loeffler was appointed to the Senate following Johnny Isakson's resignation for health reasons at the end of 2019.}

To reduce the total number of pairwise comparisons that need to be made using ChatGPT and to more effectively prompt ChatGPT in pairwise comparisons (see below), we first used ChatGPT to place each senator into one of four categories: liberal Democrat, moderate Democrat, moderate Republican, or conservative Republican. For details about the prompts used, see Section \ref{sec_si:categorization_senators} in the Supplemental Information. Among the 48 Democrats (including Angus King and Bernie Sanders, two independents who caucus with the Democrats) and 54 Republicans (including Kelly Loeffler and Martha McSally), there were 30 liberal Democrats, 18 moderate Democrats, 6 moderate Republicans, and 48 conservative Republicans.

We then turn to pairwise comparisons, which we call ``matchups.'' We input the following prompt into ChatGPT for matchups between Democratic senators: 
\begin{quote}
    \singlespacing
    \sloppy \texttt{Which senator is more liberal: [senator 1] ([senator 1 party abbrev]-[senator 1 state abbrev]) or [senator 2] ([senator 2 party abbrev]-[senator 2 state abbrev])?}
\end{quote}
We use the above prompt for matchups between moderate Democrats and moderate Republicans. For matchups between Republican senators, we use a similar prompt:
\begin{quote}
    \singlespacing
    \texttt{Which senator is more conservative: [senator 1] ([senator 1 party abbrev]-[senator 1 state abbrev]) or [senator 2] ([senator 2 party abbrev]-[senator 2 state abbrev])?}
\end{quote}
We change the wording for matchups between Republican senators strictly because of a quirk of ChatGPT (and illustrative of its inability to fully ``reason'' about politics): when asked which senator in each pair is more liberal when comparing two conservative Republicans, it will often reply that neither senator is ``more liberal'' because they are both conservative Republicans. With the prompts laid out above, however, ChatGPT will make comparisons between politicians of the same general ideological category. 

We record the name of the senator that ChatGPT considers to be more conservative (liberal) in each matchup. For the following matchup types by categorization, we used the comparison prompt from above:\footnote{More specifically, we took ChatGPT's answer to the comparison prompt and used it in another prompt to extract the name of the senator out of the answer. See Section \ref{sec_si:extraction} in the Supplemental Information for more information about the prompt we used to extract the name of the more conservative senator.} 
\begin{itemize}
    \singlespacing
    \item Liberal Democrat--Liberal Democrat 
    \item Liberal Democrat--Moderate Democrat
    \item Moderate Democrat--Moderate Democrat
    \item Moderate Democrat--Moderate Republican
    \item Moderate Republican--Moderate Republican
    \item Moderate Republican--Conservative Republican
    \item Conservative Republican--Conservative Republican
\end{itemize}
Ties were allowed; this is when the model is unable to make an assessment of who is more liberal (or more conservative) between the two senators. To reduce the number of pairwise comparisons for which we had to query ChatGPT, for matchups between conservative Republicans and moderate or liberal Democrats, we record the conservative Republican to always be more conservative. Similarly, for matchups between liberal Democrats and moderate or conservative Republicans, we record the moderate or conservative Republican to always be more conservative. 

Between 102 senators, there are a total of 5,151 unique matchups. Each senator was compared to all other senators 3 times in order to study the consistency of ChatGPT's answers. In total, there were 15,453 pairwise comparisons: 8,001 of these were binary comparisons where ChatGPT was directly asked which of the two senators was more liberal or conservative, and 7,452 of these were based on ChatGPT's categorization of senators into one of the four ideology-party groups described above. The final dataset contains each matchup with the number of wins (times a senator was deemed to be more conservative in that specific matchup) and losses (times a senator was deemed to be more liberal in that specific matchup) for that particular matchup. We consider ties 0.5 wins for both senators in the matchup.\footnote{The next section explains why we treated ties in this fashion.}

\subsection{Using the Bradley-Terry Model to Estimate Ideology}
\label{subsec:bradley_terry}
The Bradley-Terry model assumes that in a contest between two players $i$ and $j$, the odds that $i$ beats $j$ in a matchup are $\alpha_i / \alpha_j$, where $\alpha_i$ and $\alpha_j$ are positive-valued parameters that indicate latent ``ability'' \citep{bradleyterry1952}. We can define $\alpha_i \equiv \exp(\lambda_i)$. Then, the log-odds of $i$ beating $j$ is 
\[\log\left[ \frac{\text{Pr}(i \text{ beats } j)}{\text{Pr}(j \text{ beats } i)} \right] = \lambda_i - \lambda_j\]
The intuition is that the larger the value of $\lambda_i$ compared to $\lambda_j$, the more likely it is for player $i$ to beat player $j$. 

We translate the above matchup into a contest between senators in terms of who is more conservative. In other words, the $\lambda$ parameters are measures of the senators' latent liberal-conservative ideology. We denote the more conservative senator in each matchup as the ``winner'' so that conservative senators have higher values, intuitively matching the liberal-conservative political spectrum and the scale of NOMINATE. For ties, we considered these 0.5 wins for both senators in the matchup. \citet{bt_turner_firth} find that this approach yields ability parameter estimates that highly correlate with more complex approaches that explicitly deal with ties. We use the bias-reduced maximum likelihood estimation approach implemented in the \texttt{BradleyTerry2} R package with ChatGPT's answers to pairwise comparisons to estimate the ideology of each senator \citep{bt_turner_firth}. We call these scores ``ChatScores.'' The $\lambda$ ideology parameters are relative to a reference senator,\footnote{Here, we use Lisa Murkowski (R-AK).} but this choice is unimportant because we rescale the ideology parameters to the unit interval.

\section{Results: ChatScores of the Senators of the 116th Congress}
\label{sec:results_ChatScores}

The steps above produce a sensible liberal-conservative scale of the senators of the 116th Congress (see Figures \ref{fig:density_plot_ChatScores} and \ref{fig:nominate_v_ChatScores} below). In the remainder of this section, we highlight interesting features of ChatScores and analyze their relationship with pre-existing measures of these senators' ideologies both within and across parties. 

\textbf{ChatScores highly correlate across repeated iterations.} Because we ran the entire set of matchups across all senators three times, we can look at the correlation of the ChatScores generated by each set of complete matchups. Table \ref{tab:correlations_iterations} shows the correlations across the iterations. It suggests that ChatGPT's responses to pairwise comparisons are quite consistent.\footnote{Although this, of course, does not prove that they are correct or based on coherent logic.}
\begin{table}[!ht]
\centering
\begin{tabular}{l|ccc}
            & Iteration 1 & Iteration 2 & Iteration 3 \\ \hline
Iteration 1 & 1.000       & 0.997       & 0.996       \\
Iteration 2 & 0.997       & 1.000       & 0.997       \\
Iteration 3 & 0.996       & 0.997       & 1.000      
\end{tabular}
\caption{Correlations across the ChatScores generated by each set of complete matchups.}
\label{tab:correlations_iterations}
\end{table}
Given the remarkably close correlations, for the rest of the analysis, we use ChatScores estimated using all matchups across all iterations. 

\textbf{ChatScores show sensible results.} ChatScores show Bernie Sanders, Elizabeth Warren, and Ed Markey as the most liberal senators. On the other end, they show Ted Cruz, Tom Cotton, and Josh Hawley as the most conservative senators. It shows Joe Manchin, Lisa Murkowski, Mitt Romney, and Susan Collins as the center-most senators (see Figure \ref{fig:nominate_v_ChatScores}). Figure \ref{fig:density_plot_ChatScores} illustrates the distribution of ChatScores by party using a density plot. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/density_ChatScores.png}
    \caption{Distribution of the ChatScores by party. Bernie Sanders (I-VT) and Angus King (I-ME) are counted as part of the Democratic senators.}
    \label{fig:density_plot_ChatScores}
\end{figure}

\textbf{ChatScores highly correlate with the first dimension of DW-NOMINATE.} DW-NOMINATE is a multidimensional scaling approach that uses roll-call voting patterns to estimate the ideological positions of legislators \citep{keith_poole_nominate,poole1997ideology,poole_2005,carroll_lewis_lo_poole_rosenthal}. They are the most widely used measure of legislator ideology \citep{caughey_schickler_2016}. The first dimension of DW-NOMINATE is typically interpreted as the liberal-conservative continuum in United States politics \citep{poole1997ideology}. The overall correlation between ChatScores and the first dimension of NOMINATE is 0.963. They are correlated at 0.828 among Democratic senators and they are correlated at 0.694 among Republican senators. Figure \ref{fig:nominate_v_ChatScores} compares the first dimension of DW-NOMINATE against ChatScores.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/chatgpt_v_nominate.png}
    \caption{First Dimension of DW-NOMINATE vs. ChatScores. Democrats are in blue, Republicans are in red, and Independents are in green.}
    \label{fig:nominate_v_ChatScores}
\end{figure}

\textbf{However, ChatScores also depart from the scores of the first dimension of DW-NOMINATE in enough ways to suggest that ChatGPT is not simply relying on actual DW-NOMINATE scores, the most popular liberal-conservative scale.}\footnote{Knowledge of NOMINATE scores is evidenced by ChatGPT correctly describing what NOMINATE scores are and (sometimes) correctly describing NOMINATE first dimension scores for a sample of senators, although it does not seem to distinguish different types of NOMINATE scores.} Notably, our method estimates Joe Manchin to be more conservative than Lisa Murkowski, Mitt Romney, and Susan Collins. This intuitively makes sense, as all four are (in)famously known to buck their respective political parties: Manchin taking a more conservative position than his Democratic colleagues and the other three taking more liberal positions than their Republican colleagues.\footnote{For example, Lisa Murkowski and Joe Manchin co-authored an op-ed in \textit{The Washington Post} in 2019, which can be found here: \url{https://www.washingtonpost.com/opinions/lisa-murkowski-and-joe-manchin-its-time-to-act-on-climate-change--responsibly/2019/03/08/2c4025f2-41d1-11e9-922c-64d6b7840b82\_story.html}. Joe Manchin endorsed Susan Collins for re-election in 2019 \citep{everett2019}, and, although this occurred during the 117th Congress, the three Republican senators were the only Republicans to vote to confirm Ketanji Brown Jackson to the Supreme Court.} In contrast, there is no overlap between senators of opposing parties in the first dimension of DW-NOMINATE. 

Looking at the extremes also indicates that ChatGPT is going beyond simply recalling NOMINATE scores for senators when making decisions in pairwise assessments. NOMINATE estimates Elizabeth Warren, Kamala Harris, and Cory Booker to be the three most liberal senators. ChatScores, on the other hand, estimates Bernie Sanders, Elizabeth Warren, and Ed Markey to be the three most liberal senators. In particular, ChatScores' ranking of Sanders over Warren comports with the rhetoric that Sanders was more progressive than Warren.\footnote{For example, it was widely reported that Sanders was a self-described ``Democratic socialist,'' while Warren was a self-described ``capitalist'' (see, e.g., \url{https://www.vox.com/policy-and-politics/2019/6/18/18678000/elizabeth-warren-bernie-sanders-2020-similarities-differences}).} Because Sanders and Markey occasionally vote against Democratic legislation, NOMINATE, which assumes a monotonic response function, may rank these senators are slightly more moderate than they potentially are \citep{duck-mayr_montgomery_2023}. Conversely, Cory Booker, ranked the 3rd most liberal senator according to NOMINATE, is the 13th most liberal senator according to ChatScores. This comports with his approach to his 2020 presidential primary campaign, which pushed unity and healing over advocating progressive policies.

Among the Republicans, ChatScores rank Ted Cruz, Tom Cotton, and Josh Hawley as the most conservative senators. They are all estimated to be more conservative than what NOMINATE would assert: NOMINATE ranks Mike Lee, Rand Paul, and Ted Cruz as the most conservative senators. In particular, Tom Cotton is ranked the 13th most conservative senator by NOMINATE but is ranked the 2nd most conservative senator by ChatScores. Cotton was widely perceived to be one of the more radical conservative senators during the George Floyd protests, penning a widely-discussed \textit{New York Times} op-ed imploring Donald Trump to send federal troops to occupy cities.\footnote{The \textit{New York Times} op-ed can be found here: \url{https://www.nytimes.com/2020/06/03/opinion/tom-cotton-protests-military.html}. The \textit{New York Times} later expressed regret for publishing this article, saying it did not meet editorial standards.} In general, their strong support for Donald Trump and frequent appearances in mainstream media stories may contribute to their high ChatScores scores.

Comparing the ordinal rankings of DW-NOMINATE and ChatScores, senators differed, on average, 8.35 positions. Some of the largest differences were Chuck Grassley (NOMINATE: 48th most conservative; ChatScores: 14th most conservative), Mitch McConnell (NOMINATE: 38th most conservative; ChatScores: 7th most conservative), and Lindsey Graham (NOMINATE: 45th most conservative; ChatScores: 17th most conservative). The top 10 biggest differences in ordinal rankings were all Republican senators. These differences seem to be shaped by their stances with respect to Donald Trump, his policies, and his nominees.\footnote{For example, Chuck Grassley very strongly supported Brett Kavanaugh's nomination to the Supreme Court of the United States.}

\textbf{ChatScores also highly correlate with alternative measures of ideology and stances on cross-cutting social issues.} These other measures of ideology go beyond roll call votes: they scale senators based on political activists' knowledge or patterns in campaign donations. Specifically, we look at the two following measures:
\begin{enumerate}
    \item Perceived ideology scores \citep{hopkins_noel_2022}: \citet{hopkins_noel_2022} employ a similar pairwise comparison approach to scaling senators using political activists, calling these ``perceived ideology scores.'' \textit{Perceived} ideology scores can be considered a separate, but related, measure of ideology: they capture the perceived ideological positions of politicians, which shape how people vote and interact with politicians. They included these pairwise comparisons of senators of the 117th Congress in a YouGov survey in April 2021. They had 1,110 activists answer these pairwise comparisons; they then scaled their answers using the Bradley-Terry model. The 11 senators who retired or did not secure a new term at the end of the 116th Congress were not included in their survey. These perceived ideology scores offer a way to compare a scale generated using ChatGPT's pairwise comparisons with a scale generated using human-labeled pairwise comparisons. 
    \item Campaign Finance Scores \citep{bonica2013}: \textbf{C}ampaign \textbf{f}inance scores (CFscores) are a measure of the ideologies of politicians, donors, and interest groups. CFscores are estimated using a network that links all individual contributors to all political candidates who received donations. It assumes that individuals choose to give to candidates close to them in a latent ideological space, and scales all actors in that space. The CFscores for the 116th Congress came from the latest data available on the Database on Ideology, Money in Politics, and Elections \citep{bonica_dime}. We look at the recipient CFScore, which is the estimated ideology of the senator based on donations received.\footnote{There is also the contributor CFscore, which is the estimated ideology of the senator based on personal donations given to other candidates. Among the senators of the 116th Congress, the two scores are correlated at 0.976, so we use the recipient CFscore.} Tammy Baldwin, Mark Kelly, and Kelly Loeffler are missing recipient CFscores. 
\end{enumerate}
We also look at the second dimension of DW-NOMINATE, which is typically interpreted as politicians' stance over cross-cutting social issues and sectional division \citep{poole1997ideology,carroll_lewis_lo_poole_rosenthal}. The second dimension has been interpreted as the stance of the major political parties over slavery, currency, nativism, civil rights, and lifestyle issues during various parts of American history. In recent Congresses, it has been interpreted as a measure capturing the anti-establishment and pro-establishment split \citep{johnson_seconddim}. Figure \ref{fig:correlation_plots} shows bivariate analyses across ChatScores, NOMINATE Dimension 1, perceived ideology scores, CFscores, and NOMINATE Dimension 2. Across all senators, ChatScores highly correlate with perceived ideology (0.929) and CFscores (0.922).

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/overall_correlation_plot.png}
        \caption{All Senators}
        \label{fig:overall_corrs_subplot}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/dems_correlation_plot.png}
        \caption{Democratic Senators}
        \label{fig:dems_corrs_subplot}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/reps_correlation_plot.png}
        \caption{Republican Senators}
        \label{fig:reps_corrs_subplot}
    \end{subfigure}
    \caption{Correlation matrices of ChatScores, the two dimensions of NOMINATE, \citet{hopkins_noel_2022}'s perceived ideology scores, and \citet{bonica2013}'s CFscores.}
    \label{fig:correlation_plots}
\end{figure}

\sloppy \textbf{The correlation between ChatScores and the second dimension of NOMINATE is greater among Democratic senators (0.578) compared to Republican senators (0.054).} If we interpret the second dimension of NOMINATE to currently measure anti-establishmentarianism \citep{johnson_seconddim}, it suggests that ChatGPT's responses correlate with the stances of Democratic senators towards the current party and government structures when evaluating Democratic senators. On the other hand, ChatGPT's responses do not seem to correlate with anti-establishment or pro-establishment views of Republican senators.

\fussy \textbf{ChatScores better predict human evaluations of senators' ideologies than NOMINATE and CFscores.} Figure \ref{fig:correlation_plots} indicates that the correlation between ChatScores and perceived ideology scores is higher than the correlations between perceived ideology and the other measures of ideology when looking at all senators and when only at senators by party.

To more formally analyze the predictive power of ChatScores on human evaluations of senators' ideologies, we compare the predictive power of ChatScores and the first dimension of NOMINATE, the second-highest correlating measure with perceived ideology scores. We use multivariate analyses to calculate how much the proportion of variance explained ($R^2$) in perceived ideology scores falls when we compare the full model, regressing perceived ideology scores on both the first dimension of NOMINATE and ChatScores, with reduced models, which only use ChatScores or NOMINATE as the predictor. Figure \ref{fig:pi_predicted_using_nominate_cgs} shows how the proportion of variance explained in perceived ideology changes as we move from the full to reduced models. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/pi_predicted_using_nominate_cgs.png}
    \caption{The full model regresses perceived ideology scores on both the first dimension of NOMINATE and ChatScores. The reduced models only use ChatScores (denoted as ``Only ChatScores'') or NOMINATE (denoted as ``Only NOMINATE'') as predictors. The proportion of variance explained, $R^2$, is always lower when ChatScores are removed as a predictor than when the first dimension of NOMINATE is removed.}
    \label{fig:pi_predicted_using_nominate_cgs}
\end{figure}

The proportion of variance explained in perceived ideology always falls more when we drop ChatScores as a predictor.\footnote{We statistically confirm these results using partial F-tests. Across all senators, the partial F-test p-value for the full model and the reduced model with only NOMINATE as a predictor is $1.319 \times 10^{-6}$, while the p-value for the full model and the reduced model with only ChatScores as a predictor is $0.167$. Across Democratic senators, the partial F-test p-value for the full model and the reduced model with only NOMINATE as a predictor is $0.012$, while the p-value for the full model and the reduced model with only ChatScores as a predictor is $0.064$. Across Republican senators, the partial F-test p-value for the full model and the reduced model with only NOMINATE as a predictor is $2.407 \times 10^{-5}$, while the p-value for the full model and the reduced model with only ChatScores as a predictor is $0.244$.} This pattern holds when we look across all senators, only Democratic senators, and only Republican senators. It suggests that ChatGPT is finding information that it can synthesize into something correlated with how activists perceive these senators' ideologies.

\section{Discussion and Conclusion}
We assessed whether ChatGPT, a generative large language model, can be potentially useful for scaling in the social sciences by estimating the latent liberal-conservative ideological scores of the senators of the 116th Congress using a pairwise comparison approach. We find that ChatGPT is not hallucinating in these pairwise comparisons, ChatGPT is not simply regurgitating a conventional liberal-conservative scale such as NOMINATE, ChatScores are stable between repeated sets of matches, and that using ChatGPT in a pairwise comparison setting yields a scale that seems to synthesize information it was trained on. In other words, we find that ChatGPT yields a sensible liberal-conservative scale of politicians. 

ChatScores correlate very highly with many liberal-conservative ideology scales, such as the first dimension of NOMINATE, \citet{hopkins_noel_2022}'s perceived ideology scores, and campaign finance scores (CFscores). We find that ChatScores among Democratic senators, but not Republican senators, are correlated with the second dimension of NOMINATE (possibly a measure of anti-establishmentarianism). Using a multivariate analysis, we find that ChatScores better predict human evaluations of senators' ideologies than other measures of ideology. In other words, ChatScores are not simply reused from a single source. Instead, our evidence is consistent with the idea that an LLM uses aggregated information about the senators to evaluate their ideologies in predictable and sensible ways.

Our work shows that generative large language models can be used for scaling problems in the social sciences. Future work should aim to apply this approach to scaling in other areas of the social sciences. It may be particularly invaluable for areas where scaling has previously been an issue because of a lack of existing appropriate data. It can also serve as a supplement to human-coded pairwise comparisons and may be particularly useful when there is a lack of representativeness among coders or if coders have significant knowledge gaps, such as a lack of recognition of the items being coded. Most of all, we are excited to see how this quickly developing technology will shape text-as-data in the social sciences in the very near future.

\newpage
\begin{singlespace}
\bibliography{refs}
\end{singlespace}

\newpage
\appendix

\section*{Supplemental Information}

\section{Prompting ChatGPT to Categorize Senators as a Liberal Democrat, Moderate Democrat, Moderate Republican, or Conservative Republican}
\label{sec_si:categorization_senators} 
We first categorize the senators as liberal Democrats, moderate Democrats, moderate Republicans, or conservative Republicans. This serves two purposes: 
\begin{enumerate}
    \item Saves on pairwise comparisons. Based on the categorizations, conservative Republicans will always be more conservative than moderate Democrats and liberal Democrats; likewise, liberal Democrats will always be more liberal than moderate Republicans and conservative Republicans. 
    \item Allows us to use a more effective prompt. As explained in Section \ref{sec:chat_gpt_scaling}, we use different prompts depending on what senators are in the matchup. 
\end{enumerate}
We input the following prompt to ChatGPT for a Republican senator: 
\begin{quote}
\singlespacing
\sloppy \texttt{Is [senator's name] (R-[senator's state abbrev.]) considered a moderate Republican or conservative Republican?}
\end{quote}
For Democratic senators, we use a similar prompt: 
\begin{quote}
\singlespacing
\sloppy \texttt{Is [senator's name] (D-[senator's state abbrev.]) considered a moderate Democrat or liberal Democrat?}
\end{quote}
To ensure that these categorizations are consistent, we run the above prompts three times and took the majority answer.

\section{Extracting the Name of the More Conservative Senator in Each Matchup}
\label{sec_si:extraction}
For each pairwise comparison prompt, ChatGPT typically returns a small paragraph explaining its choice rather than returning only the name of the senator. For example, in a comparison between Ed Markey (D-MA) and Cory Booker (D-NJ), ChatGPT returned this answer: 
\begin{quote}
    Ed Markey (D-MA) is generally considered to be more liberal than Cory Booker (D-NJ). Markey is a co-author of the Green New Deal and has been a vocal advocate for progressive policies on climate change, healthcare, and social justice. Booker, on the other hand, has centered his policy platform around criminal justice reform, economic opportunity, and affordable housing. While both senators are members of the Democratic Party and share similar values, Markey has a more progressive track record and has often been positioned as a leader of the left-wing of the party.
\end{quote}
It becomes a task to extract the name of the more liberal senator for Democratic senator matchups and the more conservative senator for Republican senator matchups. To do this, we ask ChatGPT to extract the name. Specifically, we concatenate the above answer with the following text: 
\begin{quote}
    In the above text, who is described to be the more liberal, more progressive, or less conservative senator? Return only the full name without party affiliation or state information.
\end{quote}
For matchups where we prompt ChatGPT to return the name of the more conservative senator, we concatenate that answer with the following text:
\begin{quote}
    In the above text, who is described to be the more conservative or less liberal senator? Return only the full name without party affiliation or state information.
\end{quote}
We then prompt ChatGPT with the concatenated text. ChatGPT usually returns the full name of the more liberal or more conservative senator. Punctuation and titles (such as appending ``Senator'' to the beginning of the name) were automatically removed using a Python function. Ties and answers that deviated from names are manually fixed. 

\end{document}