\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}
\usepackage{graphicx} 
\usepackage{url}
\usepackage{booktabs}
\usepackage{appendix}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{cmbright}
\usepackage{authblk}
\captionsetup[figure]{labelfont={bf},name={Fig.},labelsep=period}
\captionsetup[table]{labelfont={bf},labelsep=period}

\usepackage[backend=biber,style=science,articletitle=true,doi=false,isbn=false,url=false,eprint=false]{biblatex}
\addbibresource{refs.bib}

\title{Large Language Models Can Be Used to Scale the Ideologies of Politicians in a Zero-Shot Learning Setting}
\author[1]{Patrick Y. Wu}
\author[1,2]{Jonathan Nagler}
\author[1,2]{Joshua A. Tucker}
\author[1]{Solomon Messing}
\affil[1]{Center for Social Media and Politics, New York University}
\affil[2]{Department of Politics, New York University}
\date{\today}


\begin{document}
\maketitle 

\subsection*{Abstract}
\begin{singlespace}
    The aggregation of knowledge embedded in large language models (LLMs) holds the promise of new solutions to problems of observability and measurement in the social sciences. We examine this potential in a challenging setting: measuring latent ideology---crucial for better understanding core political functions such as democratic representation. We scale pairwise liberal-conservative comparisons between members of the 116th U.S. Senate using prompts made to ChatGPT. Our measure strongly correlates with widely used liberal-conservative scales such as DW-NOMINATE. Our scale also has interpretative advantages, such as not placing senators who vote against their party for ideologically extreme reasons towards the middle. Our measure is \textit{more} strongly associated with political activists' perceptions of senators than other measures, consistent with LLMs synthesizing vast amounts of politically relevant data from internet/book corpora rather than memorizing existing measures. LLMs will likely open new avenues for measuring latent constructs utilizing modeled information from massive text corpora.
\end{singlespace}

\newpage
\subsection*{Introduction}
We evaluate whether generative large language models can be useful for scaling and measurement problems in the social sciences. Measuring latent ideology reduces the dimensionality of complex actions and stances lawmakers take into a low dimensional scale, which, when combined with other data, allows us to assess core democratic functions: how well lawmakers represent their constituents \cite[see, e.g.,][]{ansolabehere_snyder_stewart_2001,gerber_lewis_2004,bartels_2009,thomsen_2017,caughey_warshaw_2018}, whether enacted policies have broad support or are driven by one part of the ideological spectrum \cite[see, e.g.,][]{poole1997ideology,swers_1998,Krehbiel_1998,clinton_jackman_rivers_2004,cox_mccubbins_2005}, and how position-taking occurs outside of roll call voting \cite[see, e.g.,][]{highton_rocca_2005,boudreau_etal_2019,russell_2021}. This problem of measuring ideology has been extensively studied in the U.S. national legislature, providing a widely-accepted and well-validated set of scales to which we can compare LLM-driven scaling. 

We estimate the latent ideological scores of the senators of the 116th United States Congress. We propose a method that begins with prompting ChatGPT-3.5 to pick the senator that is more liberal (or conservative) in a given pair. This is a zero-shot learning setting because we do not provide ChatGPT with \textit{any additional information} besides the senator's name, party affiliation, and state represented, and we do not include any examples of pairwise comparisons. We use the Bradley-Terry model \cite{bradleyterry1952} to estimate a one-dimensional liberal-conservative scale, which we call ChatScores.

While there is a consensus that latent ideology exists in some form, we cannot explicitly or directly measure it. Existing measures of ideology are either based on the behavior or perceptions of lawmakers. Behavior-based measures based on roll call votes measure revealed preferences constrained by a legislative agenda, which does not allow us to measure latent ideology directly. Other measures of behavior, such as news media sharing behavior \cite{eady_bonneau_tucker_nagler_2020}, are also measures of ideology based on revealed preferences. Measures based on campaign contributions \cite{bonica2013} assume ideological homophily in campaign giving and are thus based on perceptions of the contributors. 

In contrast, ChatScores are a blend of textually available information about individual lawmakers. As our Results section shows, the scale seems to reflect both the behaviors, such as votes, floor speeches, and commentary about other lawmakers, as well as perceptions of these lawmakers, such as news stories and blog posts/editorials about these lawmakers. A drawback, because of the black box nature of LLMs, is that we do not know how the LLM weighs the underlying textual information given a prompt. At the same time, the scope of this model underpinning this measure is its strength: ChatScores can make up for interpretative gaps in existing measures of ideology, such as dealing with lawmakers who vote against their own party for ideological reasons \cite{duck-mayr_montgomery_2023}.

We use pairwise comparisons because ChatGPT often does not consistently return a full list of senators ranked from most liberal to most conservative: it will sometimes only return partial lists, the most ideologically extreme senators, or simply refuse to create such a list. Pairwise comparisons have also been used extensively in social science scaling applications. For example, Loewen \textit{et al.} \cite{LOEWEN2012212} conduct a survey experiment with pairwise comparisons and the Bradley-Terry model to determine which arguments are most persuasive. Carlson and Montgomery \cite{carlson_montgomery_2017}'s SentimentIt R package conducts pairwise comparisons to label political texts. Hopkins and Noel \cite{hopkins_noel_2022}, a closely-related work to this study, use pairwise comparisons among political activists to scale senators of the 114th Congress and the 117th Congress along the liberal-conservative continuum.

We proceed as follows. We make pairwise comparisons of the senators of the 116th Congress using ChatGPT and construct a liberal-conservative ideology using the Bradley-Terry model \cite{bradleyterry1952}. From this scale, we examine whether ChatGPT
\begin{enumerate}
    \item hallucinates (i.e., produces irrelevant or incorrect answers) in pairwise comparisons, creating an incoherent measure
    \item simply parrots answers from an existing liberal-conservative scales such as DW-NOMINATE \cite{keith_poole_nominate,poole_2005,carroll_lewis_lo_poole_rosenthal}
    \item appears to use some synthesis of the information it was trained on, leading to a unique scale
\end{enumerate}

The scale constructed using ChatGPT's pairwise comparisons aligns most with the third outcome. We call this ChatGPT-predicted ideological scale ``ChatScores.'' We find that this procedure places extreme and centrist senators in sensible places on the scale. And although ChatScores and the first dimension of DW-NOMINATE, the most popular liberal-conservative scaling of senators, highly correlate, we also find that ChatScores depart from DW-NOMINATE scores in important ways. For example, the scale places some liberal Republican senators to the left of certain conservative Democratic senators. The scale also handles senators who exhibit ``ends against the middle'' behavior differently from DW-NOMINATE. ChatScores places these senators who vote \textit{against} their party for ideologically extreme reasons on extreme ends, while DW-NOMINATE places them more towards the center. 

Because using ChatGPT with pairwise comparison scaling methods creates a unique scale, we compare ChatScores to other scales of ideology using bivariate and multivariate analyses. We find that ChatScores correlate strongly with other liberal-conservative ideology scales \cite{bonica2013,hopkins_noel_2022}. Using partial correlations, we find relatively strong correlations between ChatScores and existing individual measures while controlling for other measures. We also find that ChatScores predict human evaluations of the ideologies of senators better than other measures \cite{hopkins_noel_2022}, including DW-NOMINATE.

In summary, we find that ChatGPT is not hallucinating; it consistently responds when prompted repeatedly about political ideology; it is not simply parroting pre-existing measures of the ideologies of senators; and it can be prompted to evaluate pairwise comparisons that analysts can use to construct sensible and novel scales. Further analyses of ChatScores suggest that ChatGPT (and other comparable LLMs) can be used to create a scale that correlates with an amalgamation of information about policy positions, voting behaviors, campaign-giving patterns, and public perceptions about politicians. We expect these scales to improve further when the underlying large language models improve as well. This study suggests that generative large language models can return reasonable responses about American politics and have the potential to dramatically shape text-as-data methods in the social sciences in the very near future. Our project begins this line of research by providing an LLM-driven method for one of the most important scaling questions in American politics.

\subsection*{Results}
Using ChatGPT to make pairwise comparisons about ideology and using the Bradley-Terry model to scale its answers produces a sensible liberal-conservative scale of the senators of the 116th Congress (see Figures \ref{fig:density_plot_ChatScores} and \ref{fig:nominate_v_ChatScores} below). We call these pairwise comparisons ``matchups.'' There were 5,151 total matchups across all senators in the 116th Congress. The ``winner'' of each matchup was the senator ChatGPT answered as being more conservative in its response. ``Winners'' were assigned this way to match the intuition that more conservative senators were placed on the right side of the scale and more liberal senators were placed on the left side of the scale. We highlight interesting features of ChatScores and analyze their relationship with pre-existing measures of these senators' ideologies both within and across parties. 

\subsubsection*{ChatScores highly correlate across repeated iterations}
We ran the entire set of matchups across all senators three times. We look at the correlation of the ChatScores generated by each set of complete matchups. Table \ref{tab:correlations_iterations} shows the correlations across the iterations. It suggests that ChatGPT's responses to pairwise comparisons are quite consistent. Given the remarkably close correlations, for the rest of the analysis, we use ChatScores estimated using all matchups across all iterations. 

\begin{table}[!ht]
\centering
\begin{tabular}{l|ccc}
            & Iteration 1 & Iteration 2 & Iteration 3 \\ \hline
Iteration 1 & 1.000       & 0.997       & 0.996       \\
Iteration 2 & 0.997       & 1.000       & 0.997       \\
Iteration 3 & 0.996       & 0.997       & 1.000      
\end{tabular}
\caption{\textbf{Correlations across the ChatScores generated by each set of complete matchups.}}
\label{tab:correlations_iterations}
\end{table}

\subsubsection*{ChatScores scale senators in sensible ways}
ChatScores show Bernie Sanders, Elizabeth Warren, and Ed Markey as the most liberal senators. On the other end, ChatScores show Ted Cruz, Tom Cotton, and Josh Hawley as the most conservative senators. It shows Joe Manchin, Lisa Murkowski, Mitt Romney, and Susan Collins as the center-most senators (see Figure \ref{fig:nominate_v_ChatScores}). Figure \ref{fig:density_plot_ChatScores} illustrates the distribution of ChatScores by party using a density plot. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/density_ChatScores.png}
    \caption{\textbf{Distribution of the ChatScores by party.} Bernie Sanders (I-VT) and Angus King (I-ME) are counted as part of the Democratic senators.}
    \label{fig:density_plot_ChatScores}
\end{figure}

\subsubsection*{ChatScores highly correlate with the first dimension of DW-NOMINATE} 
DW-NOMINATE (Dynamic, Weighted NOMINAl Three-Step Estimation) is a multidimensional scaling approach that uses roll call voting patterns to estimate the ideological positions of legislators \cite{keith_poole_nominate,poole1997ideology,poole_2005,carroll_lewis_lo_poole_rosenthal}. It is the most widely used measure of legislator ideology \cite{caughey_schickler_2016}. The first dimension of DW-NOMINATE is typically interpreted as the liberal-conservative continuum in United States politics \cite{poole1997ideology}. The overall correlation between ChatScores and the first dimension of DW-NOMINATE is 0.963. They are correlated at 0.828 among Democratic senators and they are correlated at 0.694 among Republican senators. Figure \ref{fig:nominate_v_ChatScores} compares the first dimension of DW-NOMINATE against ChatScores.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/chatgpt_v_nominate.png}
    \caption{\textbf{First Dimension of DW-NOMINATE vs. ChatScores.} Democrats are in blue, Republicans are in red, and Independents are in green.}
    \label{fig:nominate_v_ChatScores}
\end{figure}

\subsubsection*{ChatScores also depart from the scores of the first dimension of DW-NOMINATE in enough ways to suggest that ChatGPT is not simply parroting DW-NOMINATE scores}
Notably, our method estimates Joe Manchin to be more conservative than Lisa Murkowski, Mitt Romney, and Susan Collins. This intuitively makes sense, as all four are (in)famously known to buck their respective political parties: Manchin often takes more conservative positions than his Democratic colleagues and the other three take more liberal positions than their Republican colleagues. The four are also connected within politics and the mainstream media. For example, Lisa Murkowski and Joe Manchin co-authored an op-ed in \textit{The Washington Post} in 2019 about climate change \cite{murkowski_manchin_2019}; Joe Manchin endorsed Susan Collins for re-election in 2019 \cite{everett_2019}; and, although this occurred during the 117th Congress, the three Republican senators were the only Republicans to vote to confirm Ketanji Brown Jackson to the Supreme Court. In contrast, there is no overlap between senators of opposing parties in the first dimension of DW-NOMINATE. Co-authorship of op-eds and confirmation votes would not be used by DW-NOMINATE, which only uses roll call votes. 

Looking at the extremes also indicates that ChatGPT is going beyond simply recalling DW-NOMINATE scores for senators when responding to pairwise assessments. DW-NOMINATE estimates Elizabeth Warren, Kamala Harris, and Cory Booker to be the three most liberal senators. ChatScores, on the other hand, estimates Bernie Sanders, Elizabeth Warren, and Ed Markey to be the three most liberal senators. In particular, ChatScores' ranking of Sanders over Warren comports with the rhetoric that Sanders is more progressive than Warren. For example, it was widely reported that Sanders was a self-described ``Democratic socialist,'' while Warren was a self-described ``capitalist'' \cite{golshan_2019}. Ed Markey is also widely viewed as a strong progressive, having led the Green New Deal with Alexandria Ocasio-Cortez \cite{markey_resolution_2019}. Because Sanders and Markey occasionally vote against Democratic legislation, monotonic models such as DW-NOMINATE may rank these senators as more moderate \cite{duck-mayr_montgomery_2023}. Again, proposed legislation and self-imposed labels would not be picked up by DW-NOMINATE.

Among the Republicans, ChatScores rank Ted Cruz, Tom Cotton, and Josh Hawley as the most conservative senators. On the other hand, DW-NOMINATE ranks Mike Lee, Rand Paul, and Ted Cruz as the most conservative senators. In particular, Tom Cotton is ranked the 13th most conservative senator by DW-NOMINATE but is ranked the 2nd most conservative senator by ChatScores. Cotton was widely perceived to be one of the more radical conservative senators during the George Floyd protests, penning a widely-discussed \textit{New York Times} op-ed imploring Donald Trump to send federal troops to occupy cities \cite{cotton_2020}. This sort of public speaking behavior would not be picked up by DW-NOMINATE.

Comparing the ordinal rankings of DW-NOMINATE and ChatScores, senators differed, on average, by 8.35 positions. Some of the largest differences in ordinal rankings were Chuck Grassley (DW-NOMINATE: 48th most conservative; ChatScores: 14th most conservative), Mitch McConnell (DW-NOMINATE: 38th most conservative; ChatScores: 7th most conservative), and Lindsey Graham (DW-NOMINATE: 45th most conservative; ChatScores: 17th most conservative). The top 10 largest differences in ordinal rankings were all Republican senators. These differences seem to be shaped by their public stances with respect to Donald Trump, his policies, and his nominees, such as Chuck Grassley's strong vocal support for Brett Kavanaugh's nomination to the Supreme Court of the United States. Again, such public stances would not be captured by DW-NOMINATE.

\subsubsection*{ChatScores also highly correlate with alternative measures of ideology and stances on cross-cutting social issues} 
Next, we compare ChatScores to two alternative measures of ideology from the political science literature that are not based on roll call votes but based on the perceptions of the senators: one scales senators based on political activists' knowledge, and the other on patterns in campaign donations. Specifically, we look at the two following measures:
\begin{enumerate}
    \item Perceived ideology scores \cite{hopkins_noel_2022}: Perceived ideology scores are estimated using political activists' answers to pairwise comparisons of senators. \textit{Perceived} ideology scores can be considered a separate, but related, measure of ideology: they capture the perceived ideological positions of politicians, which shape how people vote and interact with politicians. Perceived ideologies can differ from how politicians view themselves ideologically and what their revealed preferences are. They included these pairwise comparisons of senators of the 117th Congress in a YouGov survey in April 2021. They had 1,110 activists answer these pairwise comparisons; they then scaled their answers using the Bradley-Terry model. The 11 senators who retired or did not secure a new term at the end of the 116th Congress were not included in their survey. These perceived ideology scores offer a way to compare a scale estimated using ChatGPT's pairwise comparisons with a scale estimated using human-labeled pairwise comparisons. 
    \item Campaign Finance Scores \cite{bonica2013}: \textbf{C}ampaign \textbf{f}inance scores (CFscores) are a measure of the ideologies of politicians, donors, and interest groups. CFscores are estimated using a network that links all individual contributors to all political candidates who received donations. It assumes that individuals choose to give to candidates close to them in a latent ideological space, and scales all actors in that space; in other words, it measures ideology based on the donors' perceptions of lawmakers. We used each senator's latest CFscore; the Database on Ideology, Money in Politics, and Elections have estimated CFscores up to the 2018 election cycle. \cite{bonica_dime}. We look at the recipient CFScore, which is the estimated ideology of the senator based on donations received. Tammy Baldwin, Mark Kelly, and Kelly Loeffler are missing recipient CFscores. 
\end{enumerate}
We also look at the second dimension of DW-NOMINATE, which is typically interpreted as politicians' stance over cross-cutting social issues and sectional division \cite{poole1997ideology,carroll_lewis_lo_poole_rosenthal}. The second dimension has been interpreted as the stance of the major political parties over slavery, currency, nativism, civil rights, and lifestyle issues during various parts of American history. In recent Congresses, it has been interpreted as a measure capturing the anti-establishment and pro-establishment split \cite{johnson_seconddim}. 

Figure \ref{fig:correlation_plots} shows bivariate analyses across ChatScores, DW-NOMINATE Dimension 1, perceived ideology scores, CFscores, and DW-NOMINATE Dimension 2. Across all senators, ChatScores highly correlate with perceived ideology (0.929) and CFscores (0.922). ChatScores also moderately correlate with the second dimension of DW-NOMINATE.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.485\textwidth}
        \includegraphics[width=\textwidth]{figures/overall_correlation_plot.png}
        \caption{All Senators}
        \label{fig:overall_corrs_subplot}
    \end{subfigure}
    \begin{subfigure}[b]{0.485\textwidth}
        \includegraphics[width=\textwidth]{figures/dems_correlation_plot.png}
        \caption{Democratic Senators}
        \label{fig:dems_corrs_subplot}
    \end{subfigure}
    \begin{subfigure}[b]{0.485\textwidth}
        \includegraphics[width=\textwidth]{figures/reps_correlation_plot.png}
        \caption{Republican Senators}
        \label{fig:reps_corrs_subplot}
    \end{subfigure}
    \caption{\textbf{Correlation matrices of ChatScores, the two dimensions of DW-NOMINATE, Hopkins and Noel \cite{hopkins_noel_2022}'s perceived ideology scores, and Bonica \cite{bonica2013}'s CFscores.}}
    \label{fig:correlation_plots}
\end{figure}

\subsubsection*{Partial correlations suggest that ChatScores reflect a measure of ideology based on both behaviors and perceptions of the senators}
Table \ref{tab:partial_corrs} shows the partial correlations between ChatScores and the three measures of ideology as described above. DW-NOMINATE is a measure of ideology based on the behaviors of senators, while perceived ideology and CFscores are measures of ideology based on perceptions of the senators. For each cell, the partial correlation between the ChatScores and the measure in the column title is calculated controlling for the other two measures of ideology. The partial correlations suggest that no single measure of ideology fully explains ChatScores. Instead, the results indicate that ChatScores reflect a measure of ideology based on both behaviors and perceptions of the senators. This interpretation holds when we look at the partial correlations across all senators, Democratic senators, and Republican senators, although the partial correlation between ChatScores and CFscores is not significant when we look at Democratic senators or Republican senators alone. To be clear, it does not suggest that these measures capture everything related to behaviors and perceptions about the senators. However, the three measures of ideology do capture a wide range of revealed preferences and perceptions of the senators. 

\begin{table}[!ht]
\centering
\begin{tabular}{l|ccc}
                          & DW-NOMINATE D1      & Perceived Ideology      & CFscores \\ \hline
ChatScores, All Senators  & $0.432^{***}$       & $0.527^{***}$           & $0.226^{*}$   \\
ChatScores, Dem. Senators & $0.537^{***}$       & $0.427^{**}$            & $0.286$    \\
ChatScores, Rep. Senators & $0.484^{***}$       & $0.596^{***}$           & $-0.222$   \\ \hline
\multicolumn{4}{l}{\scriptsize{Note: $^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$}}
\end{tabular}
\caption{\textbf{Partial correlations between ChatScores and the first dimension of DW-NOMINATE, perceived ideology, and CFscores.} Each cell shows the partial correlations between ChatScores and the measure in the column title, controlling for the other two measures of ideology. P-values are calculated using the t-statistic described in Kim \cite{kim2015ppcor}.} 
\label{tab:partial_corrs}
\end{table}

\subsubsection*{\sloppy The correlation between ChatScores and the second dimension of DW-NOMINATE is greater among Democratic senators (0.578) compared to Republican senators (0.054)} 
\sloppy If we interpret the second dimension of DW-NOMINATE to currently measure anti-establishmentarianism \cite{johnson_seconddim}, it suggests that ChatGPT's responses correlate with the stances of Democratic senators towards the current party and government structures when evaluating Democratic senators. On the other hand, ChatGPT's responses do not seem to correlate with anti-establishment or pro-establishment views of Republican senators.

\fussy \subsubsection*{ChatScores better predict human evaluations of senators' ideologies than DW-NOMINATE and CFscores}
Figure \ref{fig:correlation_plots} indicates that the correlation between ChatScores and perceived ideology scores is higher than the correlations between perceived ideology and the other measures of ideology when looking at all senators and when looking at senators by party.

To more formally analyze the predictive power of ChatScores on human evaluations of senators' ideologies, we compare the predictive power of ChatScores and the first dimension of DW-NOMINATE, the second-highest correlating measure with perceived ideology scores. We use multivariate analyses to calculate how much the proportion of variance explained ($R^2$) in perceived ideology scores falls when we compare the full model, regressing perceived ideology scores on both the first dimension of DW-NOMINATE and ChatScores, with reduced models, which only use ChatScores or DW-NOMINATE as the predictor. Figure \ref{fig:pi_predicted_using_nominate_cgs} shows how the proportion of variance explained in perceived ideology changes as we move from the full to reduced models. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/pi_predicted_using_nominate_cgs.png}
    \caption{\textbf{Comparing the proportion of variance, $R^2$, between the full and reduced models.} The full model regresses perceived ideology scores on both the first dimension of DW-NOMINATE and ChatScores. The reduced models only use ChatScores (denoted as ``Only ChatScores'') or DW-NOMINATE (denoted as ``Only DW-NOMINATE'') as predictors. The proportion of variance explained, $R^2$, is always lower when ChatScores are removed as a predictor than when the first dimension of DW-NOMINATE is removed.}
    \label{fig:pi_predicted_using_nominate_cgs}
\end{figure}

The proportion of variance explained in perceived ideology always falls more when we drop ChatScores as a predictor. We statistically confirm these results using partial F-tests. When looking across all senators, only Democratic senators, and only Republican senators, the partial F-test p-values for the full model and the reduced model with only DW-NOMINATE as a predictor is significant ($p<.02$ for all F-test p-values), while the p-value for the full model and the reduced model with only ChatScores as a predictor is not significant ($p>.05$ for all F-test p-values). It suggests that ChatGPT is using synthesized information that is highly correlated with how activists perceive these senators' ideologies. It also further suggests that ChatScores go beyond measuring position-taking.

\subsection*{Discussion}
Our findings suggest that generative large language models can be useful for scaling in the social sciences. We estimate the latent liberal-conservative ideological scores of the senators of the 116th Congress by prompting ChatGPT to select the more liberal (or conservative) senator in pairwise comparisons and using the Bradley-Terry model to scale ChatGPT's responses. We find that ChatGPT is not hallucinating in these pairwise comparisons, ChatScores are stable between repeated sets of matches, and ChatGPT is not simply parroting an existing liberal-conservative scale such as DW-NOMINATE. In other words, we find that ChatGPT yields a sensible liberal-conservative scale of politicians that corresponds to a unidimensional representation of the vast political information available in a corpora of internet media data and books. 

ChatScores strongly correlate with many liberal-conservative ideology scales, such as the first dimension of DW-NOMINATE, Hopkin and Noel \cite{hopkins_noel_2022}'s perceived ideology scores, and Bonica \cite{bonica2013}'s campaign finance scores (CFscores). Partial correlations suggest that ChatScores reflect a measure of ideology based on both behaviors and perceptions of the senators. We also find that ChatScores among Democratic senators, but not Republican senators, are correlated with the second dimension of DW-NOMINATE (possibly a measure of anti-establishmentarianism). Using a multivariate analysis, we find that ChatScores better predict human evaluations of senators' ideologies than other measures of ideology. In summary, ChatScores are not simply reused from a single source. Instead, our evidence is consistent with the idea that an LLM synthesizes a great deal of information about lawmakers to evaluate their ideologies in predictable and sensible ways.

Our approach to scaling speaks to a very large body of work on ideological scaling and ideal point estimation \cite[see, e.g.,][]{keith_poole_nominate,poole1997ideology,heckman_snyder_1997,martin_quinn_2002,clinton_jackman_rivers_2004,wordfish_slapin_proksch,carroll_lewis_lo_poole_rosenthal,shor_mccarty_2011,bonica2013,barbera_2015,wu_etal_2019,eady_bonneau_tucker_nagler_2020,hopkins_noel_2022,duck-mayr_montgomery_2023}. Estimation of ideology has usually focused on either the behavior of the lawmakers, such as how they vote in roll call votes; alternative measures have focused on the perceptions of lawmakers, such as through campaign donations, networks on Twitter, and perceptions of political activists. Our approach uses knowledge about lawmakers' ideologies embedded in LLMs, elicited through pairwise comparisons, to estimate their ideologies. 

There are limitations to our approach due to the black box nature of deep neural networks: we do not know how ChatGPT is formulating its response to pairwise comparison prompts. We do know what texts were used to pre-train ChatGPT-3.5 \cite{gpt3_paper}---most notably Wikipedia and the Common Crawl, a corpus created from crawling the entire web---and we know that it was almost certainly pre-trained with the existing measures discussed in this paper, as evidenced by its ability to define such measures correctly when prompted. One could attempt to ``understand'' why ChatGPT chooses a particular senator using its outputted explanation, but it may also be using the answer as its context to generate the explanation. In other words, we still would not know what factors the LLM uses, why the LLM places greater weight on certain factors, nor would we know to what degree or what direction its explanation is related to its answer. The LLM cannot ``reason'' or ``deliberate'' about ideology or politics; it is, instead, a pattern recognition model. 

What our analysis does show is that the scale produced using ChatGPT's response to pairwise comparison prompts correlates well with multiple existing measures and reflects factors that go beyond position-taking. In other words, the findings above---particularly the partial correlations and the ChatScores' strong predictive power of activists' perceptions---suggest that ChatScores reflect a blend of textual information: this ideological scale is influenced by any number of factors that exist in its training corpora such as existing measures of ideology, floor speeches, news stories about senators, and people's expressed perceptions of these senators. ChatScores can offset interpretive gaps in existing measures of ideology, such as dealing with lawmakers who vote against their own party for ideological reasons and considering perceptions of lesser-known senators. 

Despite some of its limitations, an approach similar to the one we have taken here holds open the possibility of being used for problems where measurement has been an issue because of a lack of existing appropriate data. For example, a lack of roll call votes in areas such as gun control means we cannot use roll call-based scaling methods to measure the ideological spread of legislators on this particular issue. Indeed, getting issue-specific measures of spread on just about any issue is likely to be difficult due to data limitations when relying on roll call voting; issue-based ChatScores could potentially fill this gap. For similar reasons, we cannot use roll call-based methods for estimating the ideology of legislators in parliamentary systems where members of parliament vote strictly along party lines. Furthermore, the approach we have laid out here is also not limited to simply using the politicians' names in pairwise comparisons. We could, for example, compare politicians' campaign announcements, tweets, and so on, to scale their expressed viewpoints. Thus, there is significant potential for future research contributions.

\subsection*{Materials and Methods}
\subsubsection*{A Brief Overview of ChatGPT}
\label{sec:chatgpt_overview}
ChatGPT stands for Chat \textbf{G}enerative \textbf{P}retrained \textbf{T}ransformer. Given an initial text input called the prompt, ChatGPT generates a response. ChatGPT is built on GPT-3 \cite{gpt3_paper}, a complex neural network (specifically, a decoder-only transformer) that predicts what token appears next given the set of existing tokens. It is trained on a massive corpus of text, which includes a filtered version of Common Crawl, WebText2, Books1, Books2, and Wikipedia. Alammar \cite{jay_alammar_illustrated_gpt} explains and illustrates in much greater detail how decoder-only transformer large language models are trained. In short, its ability to produce coherent textual responses comes from the massive training corpora, the sheer size of the neural network (175 billion parameters), and the self-attention mechanism. The self-attention mechanism, in effect, allows the model to dynamically upweight and downweight certain parts of the input sequence \cite{attention_is_all_you_need}. It achieves this by assigning a weight to each element of the input sequence that reflects the importance of that element relative to other elements.

ChatGPT is specifically trained to be a chatbot using a reinforcement learning technique called \textbf{r}einforcement \textbf{l}earning from \textbf{h}uman \textbf{f}eedback, or RLHF \cite[see, e.g.,][]{early_rlhf}. RLHF works by first obtaining the generated text from the model. Human annotators then rank the output from most preferred to least preferred. A reinforcement learning algorithm---specifically, proximal policy optimization \cite{ppo_algorithm}---is then used to update the large language model. For a more detailed discussion about RLHF, see Lambert \textit{et al.} \cite{lambert2022illustrating}. The addition of RLHF training enables ChatGPT to generate human-like responses. Its responses have garnered widespread attention across mainstream and social media. It is important to note that ChatGPT can still produce incorrect responses (and sound extremely confident in doing so!), a phenomenon referred to as ``hallucination.''

It is also well-known that GPT-3 can generate text with biases, negative stereotypes, and unfair associations \cite[see, e.g.,][]{lucy-bamman-2021-gender}. These biases can be potentially leveraged for social science purposes. For example, Argyle \textit{et al.} \cite{argyle_busby_fulda_gubler_rytting_wingate_2023} find that biases in GPT-3 are fine-grained and demographically correlated, and can be used to emulate partisan responses from a wide variety of human subgroups

\subsubsection*{Using ChatGPT to Measure the Ideologies of Politicians}
\label{sec:chat_gpt_scaling}
We analyze the senators of the 116th Congress, which convened on January 3, 2019 and ended on January 3, 2021. We use this particular congress because ChatGPT is trained on information up to 2021. We do not look at previous congresses in order to prevent the newest information ChatGPT learns from leaking into assessments of ideologies of members of previous congresses. We obtain this list of senators from Voteview \cite{voteview2021}. We keep Martha McSally (R-AZ) and Kelly Loeffler (R-GA) on the list of senators. Martha McSally was appointed to the Senate following interim Senator Jon Kyl's resignation. She then ran in Arizona's special election to finish the remainder of the Senate term, but lost to Mark Kelly. Similarly, Kelly Loeffler was appointed to the Senate following Johnny Isakson's resignation for health reasons at the end of 2019.

To reduce the total number of pairwise comparisons that need to be made using ChatGPT and to more effectively prompt ChatGPT in pairwise comparisons (see below), we first used ChatGPT to place each senator into one of four categories: liberal Democrat, moderate Democrat, moderate Republican, or conservative Republican. Details about the prompts used are in the Supplementary Materials. Among the 48 Democrats (including Angus King and Bernie Sanders, two independents who caucus with the Democrats) and 54 Republicans (including Kelly Loeffler and Martha McSally), there were 30 liberal Democrats, 18 moderate Democrats, 6 moderate Republicans, and 48 conservative Republicans.

We then turn to pairwise comparisons, which we call ``matchups.'' We input the following prompt into ChatGPT for matchups between Democratic senators: 
\begin{quote}
    \singlespacing
    \sloppy \texttt{Which senator is more liberal: [senator 1] ([senator 1 party abbrev]-[senator 1 state abbrev]) or [senator 2] ([senator 2 party abbrev]-[senator 2 state abbrev])?}
\end{quote}
\fussy We use the above prompt for matchups between moderate Democrats and moderate Republicans. For matchups between Republican senators, we use a similar prompt:
\begin{quote}
    \singlespacing
    \sloppy \texttt{Which senator is more conservative: [senator 1] ([senator 1 party abbrev]-[senator 1 state abbrev]) or [senator 2] ([senator 2 party abbrev]-[senator 2 state abbrev])?}
\end{quote}
\fussy We change the wording for matchups between Republican senators strictly because of a quirk of ChatGPT (and illustrative of its inability to ``reason'' about politics): when asked which senator in each pair is more liberal when comparing two conservative Republicans, it will often reply that neither senator is ``more liberal'' because they are both conservative Republicans. With the prompts laid out above, however, ChatGPT will make comparisons between politicians of the same general ideological category. We use the default temperature parameter of 1 for ChatGPT-3.5. 

We record the name of the senator that ChatGPT considers to be more conservative (liberal) in each matchup. For the following matchup types by categorization, we used the comparison prompt from above:
\begin{itemize}
    \singlespacing
    \item Liberal Democrat--Liberal Democrat 
    \item Liberal Democrat--Moderate Democrat
    \item Moderate Democrat--Moderate Democrat
    \item Moderate Democrat--Moderate Republican
    \item Moderate Republican--Moderate Republican
    \item Moderate Republican--Conservative Republican
    \item Conservative Republican--Conservative Republican
\end{itemize}
More specifically, we took ChatGPT's answer to the comparison prompt and used it in another prompt to extract the senator's name from the answer. See the Supplementary Materials for more information about the prompt we used to extract the name of the more conservative senator from ChatGPT's response.

Ties were allowed; this is when the model is unable to make an assessment of who is more liberal (or more conservative) between the two senators. To reduce the number of pairwise comparisons for which we had to query ChatGPT, for matchups between conservative Republicans and moderate or liberal Democrats, we record the conservative Republican to always be more conservative. Similarly, for matchups between liberal Democrats and moderate or conservative Republicans, we record the moderate or conservative Republican to always be more conservative. 

Between 102 senators, there are a total of 5,151 unique matchups. Each senator was compared to all other senators 3 times in order to study the consistency of ChatGPT's answers. In total, there were 15,453 pairwise comparisons: 8,001 of these were binary comparisons where ChatGPT was directly asked which of the two senators was more liberal or conservative, and 7,452 of these were based on ChatGPT's categorization of senators into one of the four ideology-party groups described above. The final comparison table contains each matchup with the number of wins (times a senator was deemed to be more conservative in that specific matchup) and losses (times a senator was deemed to be more liberal in that specific matchup) for that particular matchup. We consider ties 0.5 wins for both senators in the matchup; the next section describes why we chose this approach for ties.

\subsubsection*{Using the Bradley-Terry Model to Estimate Ideology}
\label{subsec:bradley_terry}
The Bradley-Terry model assumes that in a contest between two players $i$ and $j$, the odds that $i$ beats $j$ in a matchup are $\alpha_i / \alpha_j$, where $\alpha_i$ and $\alpha_j$ are positive-valued parameters that indicate latent ``ability'' \cite{bradleyterry1952}. We can define $\alpha_i \equiv \exp(\lambda_i)$. Then, the log-odds of $i$ beating $j$ is 
\[\log\left[ \frac{\text{Pr}(i \text{ beats } j)}{\text{Pr}(j \text{ beats } i)} \right] = \lambda_i - \lambda_j\]
The intuition is that the larger the value of $\lambda_i$ compared to $\lambda_j$, the more likely it is for player $i$ to beat player $j$. 

We translate the above matchup into a contest between senators in terms of who is more conservative. In other words, the $\lambda$ parameters are measures of the senators' latent liberal-conservative ideology. We denote the more conservative senator in each matchup as the ``winner'' so that conservative senators have higher values, intuitively matching the liberal-conservative political spectrum and the scale of DW-NOMINATE. For ties, we considered these 0.5 wins for both senators in the matchup. Turner and Firth \cite{bt_turner_firth} find that this approach yields ability parameter estimates that highly correlate with more complex approaches that explicitly deal with ties. We use the bias-reduced maximum likelihood estimation approach implemented in the \texttt{BradleyTerry2} R package with ChatGPT's answers to pairwise comparisons to estimate the ideology of each senator \cite{bt_turner_firth}. These scores are the ChatScores we describe in the previous sections. The $\lambda$ ideology parameters are relative to a reference senator (here, we use Lisa Murkowski), but this choice is unimportant because we rescale the ideology parameters to the unit interval.

\newpage
\begin{singlespace}
\printbibliography[title={\large References}]
\end{singlespace}

\newpage
\appendix

\subsection*{\centering Supplementary Materials for ``Large Language Models Can Be Used to Scale the Ideologies of Politicians in a Zero-Shot Learning Setting''}

\begin{center} Patrick Y. Wu \textit{et al.} \end{center}

\subsubsection*{Supplementary Text: Prompting ChatGPT to Categorize Senators as a Liberal Democrat, Moderate Democrat, Moderate Republican, or Conservative Republican}
\label{sec_si:categorization_senators} 
We first categorize the senators as liberal Democrats, moderate Democrats, moderate Republicans, or conservative Republicans. This serves two purposes: 
\begin{enumerate}
    \item Saves on pairwise comparisons. Based on the categorizations, conservative Republicans will always be more conservative than moderate Democrats and liberal Democrats; likewise, liberal Democrats will always be more liberal than moderate Republicans and conservative Republicans. 
    \item Allows us to use a more effective prompt. We use different prompts depending on what senators are in the matchup. 
\end{enumerate}
We input the following prompt to ChatGPT for a Republican senator: 
\begin{quote}
\singlespacing
\sloppy \texttt{Is [senator's name] (R-[senator's state abbrev.]) considered a moderate Republican or conservative Republican?}
\end{quote}
\fussy For Democratic senators, we use a similar prompt: 
\begin{quote}
\singlespacing
\sloppy \texttt{Is [senator's name] (D-[senator's state abbrev.]) considered a moderate Democrat or liberal Democrat?}
\end{quote}
\fussy To ensure that these categorizations are consistent, we run the above prompts three times and took the majority answer.

\subsubsection*{Supplementary Text: Extracting the Name of the More Conservative Senator in Each Matchup}
For each pairwise comparison prompt, ChatGPT typically returns a small paragraph explaining its choice rather than returning only the name of the senator. For example, in a comparison between Ed Markey (D-MA) and Cory Booker (D-NJ), ChatGPT returned this answer: 
\begin{quote}
    Ed Markey (D-MA) is generally considered to be more liberal than Cory Booker (D-NJ). Markey is a co-author of the Green New Deal and has been a vocal advocate for progressive policies on climate change, healthcare, and social justice. Booker, on the other hand, has centered his policy platform around criminal justice reform, economic opportunity, and affordable housing. While both senators are members of the Democratic Party and share similar values, Markey has a more progressive track record and has often been positioned as a leader of the left-wing of the party.
\end{quote}
It becomes a task to extract the name of the more liberal senator for Democratic senator matchups and the more conservative senator for Republican senator matchups. To do this, we ask ChatGPT to extract the name. Specifically, we concatenate the above answer with the following text: 
\begin{quote}
    \sloppy \texttt{In the above text, who is described to be the more liberal, more progressive, or less conservative senator? Return only the full name without party affiliation or state information.}
\end{quote}
\fussy For matchups where we prompt ChatGPT to return the name of the more conservative senator, we concatenate that answer with the following text:
\begin{quote}
    \sloppy \texttt{In the above text, who is described to be the more conservative or less liberal senator? Return only the full name without party affiliation or state information.}
\end{quote}
\fussy We then prompt ChatGPT with the concatenated text. ChatGPT usually returns the full name of the more liberal or more conservative senator. Punctuation and titles (such as appending ``Senator'' to the beginning of the name) were automatically removed using a Python function. Ties and answers that deviated from names are manually fixed. 

\end{document}