\section{Method}

\begin{figure*}
  \centering
  \includegraphics[width=1.\linewidth]{figures/overview-fig2.pdf}
     \caption{We exploit rendered depth distribution from NeRF to help supervise an approximated TSDF. We produce learned features that we feed to a small appearance network to predict RGB colors. We can extract the surface using marching cubes and store appearance features on the surface. Finally we render in real time using the mesh, appearance features and appearance network.}
   \label{fig:pipeline}
\end{figure*}

In this section we present our approach for extracting accurate 3D meshes with neural features from NeRF for subsequent real-time rendering. We present an overview of the method in \cref{fig:pipeline}. We first briefly outline the general concept of NeRFs in \cref{sec-nerfs}. In \cref{sec-surface-approx} we present our method for approximating surface from NeRF. Finally in \cref{sec-rendering} we describe mesh extraction and real-time rendering.




%----------------------
\subsection{Neural Radiance Fields} \label{sec-nerfs}
At the core, a neural radiance field is a continuous mapping from a 3D location $\mathbf{x} \in \mathbb{R}^3$ and a ray viewing direction

 $\mathbf{d} \in \mathbb{S}^2$ 
to an RGB color $\mathbf{c} \in \left[ 0, 1 \right]^3$ and volume density $\sigma \in \reals^+$. It can be formulated as,
\begin{equation}
\left[\mathbf{c}, \sigma\right] = F_{\theta} \left(\gamma_{\textit{x}}(\mathbf{x}), \gamma_{\textit{d}}({\mathbf{d}})\right) \,
\label{eq:mlp}
\end{equation}
where $F$ is modeled as an MLP with learnable parameters  $\theta$, and $\gamma: \reals^{3} \rightarrow \reals^{N}$ is a positional encoding of the input $\mathbf{x}$ required to capture high frequencies.



Given a camera pose $P_i = [R_i, t_i] \in SE(3)$, each pixel coordinate $\mathbf{p} \in \reals^{2}$  determines a ray in the world coordinate system, whose origin is the camera center of projection $\mathbf{o}_{i} = \mathbf{t}_i$ and direction is defined as $\mathbf{d}_{i, \text{p}} = R_i K_i^{-1}\bar{\mathbf{p}}$. We can express a 3D point along the viewing ray associated with $\mathbf{p}$ at depth $t$ as $\mathbf{r}_{i, \text{p}} (t) = \mathbf{o}_i + t \mathbf{d}_{i, \text{p}}$. 
To render the color $\hat{\mathbf{I}}_{i, \text{p}} \in \left[0, 1\right]^3$ at pixel $\mathbf{p}$, we sample $M$ discrete depth values $t_m$ along the ray within the near and far plane $\left[t_n, t_f \right]$, and query $F_{\theta}$ at the associated 3D points. The corresponding predicted color and volume density values $\left\{(\mathbf{c}_m, \sigma_m  ) \right\}_{m=1}^M$ are then composited as,
\begin{align}
\hat{\mathbf{I}}_{i, \text{p}} &= \hat{I}(\mathbf{p}; \theta, P_i) = \sum_{m=1}^{M} \alpha_m\mathbf{c}_m \,, \label{eq:volume_rendering} \\
\text{where} \quad \alpha_m &= T_m\left(1 - \exp(-\sigma_m\delta_m)\right) \,, \label{eq:rendering_weight} \\
T_m &= \exp \left( -\sum_{m'=1}^{m} \sigma_{m'}\delta_{m'}\right)\,. \label{eq:transmittance}
\end{align}

where $T_m$ denotes the accumulated transmittance along the ray from $t_n$ to $t_m$, and $\delta_m = t_{m+1} - t_m$ is the distance between adjacent samples. 
Finally we compute accumulated depth at percentile $k$ as follows,
\begin{equation}\label{eq:rendered-depth}
    \hat{z}_{i, \text{p}, k}  =  \sum_{m=1}^{M_k} \alpha_m t_m \,. 
\end{equation}
Where $M_k$ denotes the first index at which the accumulated transmittance $T_k > k/100$. 
% $\hat{I}$ and $\hat{z}$ denote the RGB and depth rendering functions.

% \todo{add rendering of percintile z{i,p,pr}}
\subsection{Surface approximation from NeRF} \label{sec-surface-approx}


In this section we introduce our signed surface approximation network (SSAN) module that creates a truncated signed distance field (TSDF) from NeRFs.

 
\paragraph{Signed Surface Approximation Network (SSAN). } SSAN is optimized from pretrained NeRFs to enable the extraction of a unique and accurate surface, and appearance. Relying on the pre-trained NeRF representations brings two main advantages, which we harness when training the SSAN: a rough 3D approximation of the scene geometry learnt by the NeRF, and useful priors such as rendered depths via volumetric rendering. % We fully exploit these learned priors to produce a surface approximation. 
To this end, we feed a 3D coordinate $\mathbf{x} \in \mathbb{R}^{3}$ to SSAN $\phi : \mathbb{R}^{3} \rightarrow (\mathbb{R}, \mathbb{S}^{2}, \mathbb{R}^{8})$ to predict the truncated signed distance approximation $\hat{t} \in \left[ -0.1, 0.1 \right]$, the normal $\mathbf{\hat{n}} \in \mathbb{S}^2$, used for normal smoothness regularization as well as rendering, and 8-dimensional appearance feature $\mathbf{\hat{f}} \in \left[ 0, 1 \right]^{8}$ with 
\begin{equation}
    [\hat{t}, \mathbf{\hat{n}}, \mathbf{\hat{f}}] = \phi(\mathbf{x}).
\end{equation} The outgoing features $\mathbf{\hat{f}}$ and normals $\mathbf{\hat{n}}$ are then further processed together with the viewing direction $\mathbf{d} \in \mathbb{S}^{2}$ by a small appearance network $\eta: (\mathbb{R}^8, \mathbb{S}^2, \mathbb{S}^2) \rightarrow \mathbb{R}^{3}$ to produce the view-dependant RGB color $\mathbf{\hat{c}} \in \left[ 0, 1 \right]^3$ according to
\begin{equation}
    \mathbf{\hat{c}} = \eta(\mathbf{\hat{f}}, \mathbf{\hat{n}}, \mathbf{d}).
\end{equation} 
We refer to Fig.~\ref{fig:pipeline} for an abstract overview over of SSAN.


\paragraph{TSDF.}  We aim to learn a TSDF approximation $t$ that represents the underlying surface of a scene or object. $t$ should have an accurate sign globally (positive outside the surface and negative inside).  Moreover,  $t$ should be smooth and have near constant derivatives close to the surface to enable efficient marching cubes at a later stage. Note that  we are not constrained to produce a well defined distance function. In this section, we introduce multiple loss functions to enforce these properties.


 At training time, we exploit the NeRF occupancy aggregated along rays. While the weighted average of the occupancy is often used to render an approximation of depth, we instead use the distribution of the occupancy along the ray for more accuracy as in~\cite{park2021nerfies}. We call $\hat{z}_{i, \text{p}, 50}$ the median value of depths along a ray as described in \cref{sec-nerfs}. We can estimate 3D points that are outside or inside the surface by constructing points before or after the median depth from the camera origin respectively. In practice, we choose to render  the depth of the NeRF outside at the $16^{th}$ percentile $\hat{z}_{i, \text{p}, 16}$ and inside the object surface at the $84^{th}$ percentile $\hat{z}_{i, \text{p}, 84}$. Subsequently, we compute the corresponding 3D world coordinates and feed them to SSAN to estimate the respective SDFs with:


\begin{align}
    \left(\mathbf{\hat{t}}_{50, i, p}, \mathbf{\hat{n}}_{50, i}, \mathbf{\hat{f}}_{50, i}\right)  & =  \phi(r_{i,p}(\hat{z}_{i, \text{p}, 50})), \\ 
    \left(\mathbf{\hat{t}}_{16, i, p}, \mathbf{\hat{n}}_{16, i}, \mathbf{\hat{f}}_{16, i}\right)  & =  \phi(r_{i,p}(\hat{z}_{i, \text{p}, 16})), \quad \text{and} \\ 
    \left(\mathbf{\hat{t}}_{84, i, p}, \mathbf{\hat{n}}_{84, i}, \mathbf{\hat{f}}_{84, i}\right) & = \phi(r_{i,p}(\hat{z}_{i, \text{p}, 84})).
\end{align} 

For readability, we drop the term $p$ in later references. During training, we enforce this construction by applying the loss $L_i$  at sampled projections along training rays.  

\begin{figure}
  \centering
   \includegraphics[width=0.7\linewidth]{figures/loss_description.pdf}
   \caption{We exploit occupancy aggregated over training rays to estimate the outside ($ r_{i}(\hat{z}_{i, 16})$) , the surface ($ r_{i}(\hat{z}_{i, 50})$) and the inside ($ r_{i}(\hat{z}_{i, 84})$),  of the object.}
   \label{fig:loss_description}
\end{figure}

\begin{equation}
 L_i = \sum_{i}\| \hat{t}_{16, i}-\epsilon\|^2 +  \| \hat{t}_{50, i}\|^2 + \| \hat{t}_{84, i} + \epsilon\|^2 ,
\end{equation}
We visualize these points in \cref{fig:loss_description}. We also provide an ablation in the supplementary material on the choice of the percentile values. We note that we choose to use values computed from the distribution and not a fixed constant to better account for the scale as well as uncertainties or different levels of details in one scene.
 
 
More importantly,  we also  enforce that the learned indicator function is locally smooth and has near constant derivatives close to the zero level set. Namely, the approximated TSDF function should have normals of constant norm  $n_c$ around the median depth. We recall that we are only approximating the behaviour of a SDF in order to run Marching Cubes. We enforce approximated signed distance values at positions computed from the ray distribution. However, the distribution can vary between scenes, ray direction or even regions of the same scenes. Therefore, we do not constrain the function to simulate exact euclidean distances.

\begin{equation}
 L_n = \sum_{i}  \| \| N( r_{i}( \delta))  \| - \mathbf{n}_c\| ^2 ,
\end{equation}
where $N: \mathbb{R}^3 \rightarrow \mathbb{S}^2$ is a function computing the normal at a given point $\mathbf{x}$.
$\delta$ is uniformly sampled value in $[\hat{z}_{i, 16},\hat{z}_{i, 84}]$.
To increase the speed of our method, we compute the normals using finite difference of the SDF network at nearby 3D positions. While other methods such as \cite{verbin2022ref} often compute normals using gradients from the network, their method is significantly more time consuming. 

\paragraph{Normal regularization.} To ensure a smooth surface,  we use the normal regularization used in  RefNeRF \cite{verbin2022ref} on the TSDF normals.
We use the normal smoothness loss:
 
 
 \begin{equation}
     L_s = \sum_{i} \| N(r_{i}(\hat{z}_{i, 50}))- \mathbf{\hat{n}}_{50, i} \|^2
 \end{equation}
 
 
 Enforcing that the computed normals are close to the estimated normals  produces smoother computed normals as the signals produced by the network are limited in frequency.
 We also use the normal orientation loss:
 
 \begin{equation}
 L_o = \sum_{i} max(0,  N(r_{i}(\hat{z}_{i, 50}))   \cdot \mathbf{d_i}).
 \end{equation} 
 
\paragraph{Appearance.} We train a small appearance network that takes as input shading features $\mathbf{\hat{f}}_i$ and predicted normals $\mathbf{\hat{n}}_i$ predicted by SSAN as well as input view directions $\mathbf{d}_i$ to produce the surface color at a given coordinate from a given view similar to SNeRG \cite{hedman2021snerg}. We supervise this network with the ground truth colors from the training images. We notice that, the projected median depth can be an inaccurate estimation of the surface zero-level set, especially when the angle between the view direction and the surface is small. Therefore, we supervise the color at points projected on the zero-level set. We compute the projection using $4$ steps of gradient descent along the viewing ray using the SDF approximated values, see the supplementary materials for more details. We refer to the depth of  points $r_{i}(\hat{z}_{i, 50})$  projected on the zero level set as $l$.  
 
 
 \begin{equation}
     L_{color} =\sum_{i} \| \eta(\mathbf{\hat{f}}_{l, i}, \mathbf{\hat{n}}_{l,i },\mathbf{d}_i) - \mathbf{c}_i \|^2,
 \end{equation}
 where $c_i$ is the ground truth pixel color.
 
  The different losses  are combined as a weighted average during training into a final loss $L$.
  
  
 \begin{equation}
     L =\alpha L_i + \beta L_n + \gamma L_s + \delta L_o
 \end{equation}
 
 
 
 \paragraph{Training stage.} We explore training the SSAN module in two possible ways. The training process mostly relies on having available depth maps from NeRF. Depths maps can be computed directly during training of the NeRF based method or they can also be rendered from a pretrained NeRF based model. We choose to train our method from the rendered depth percentiles of a pretrained NeRF model. We provide more information about  simultaneously training in the supplementary material. 

 \subsection{Mesh extraction and Real-time rendering}\label{sec-rendering}
 
 \paragraph{Mesh extraction.} The SSAN module converts the radiance field representation of the scene obtained from NeRF to distance field (TSDF) representation. Given the TSDF representation, we can easily reconstruct the surface by using a surface reconstruction algorithm \cite{lorensen1987marching,ju2002dual}. In this paper we use the PyMCubes\footnote{\href{https://github.com/pmneila/PyMCubes}{https://github.com/pmneila/PyMCubes}} implementation of marching cubes.
 Moreover we use either vertex features, for instance on objects from the Blender Synthetic dataset or  build a texture  using per face parametrization for unbounded scenes from \cite{barron2022mipnerf360}. We fill the texture image with appearance features sampled from the SSAN appearance features  at the interpolated face locations.


\paragraph{Rendering.} The triangle mesh geometry extracted from the SSAN can be encoded in any common format like OBJ, glTF, and others. Thus output meshes from out pipeline can be directly integrated in traditional graphics pipeline. The neural view-dependent appearance is added by rasterizing the precomputed texture and feeding the results to the appearance network that produces the final RGB color values. This strategy ensures that the network only performs $w\cdot h$ evaluations and thus can run at a high frame rate on commodity hardware. We measure average FPS of 25 on a workstation and 30 on a MacBook on the Blender Synthetic dataset objects. This approach is similar to SNeRG \cite{hedman2021snerg}, although our method only samples features at the surface boundary instead of aggregating them in a volumetric fashion, similar to MobileNeRF \cite{chen2022mobilenerf}. Contrary to these two methods however, our underlying geometry is by design significantly closer to the ground truth.


\subsection{Implementation details}


We use a backbone similar to Instant NGP \cite{mueller2022instant} for the SSAN module. We divide the module in two separate  branches with separate weights: i) the geometry branch that outputs TSDF approximation and the normal prediction. ii) The appearance branch which outputs color features. Each branch is a separate network with the same architecture.  We use Instant NGP \cite{mueller2022instant} architectures with hash table size $ 2^{19}$, coarsest resolution of 16,  highest resolution of 2048, 15 levels and a number of feature dimension per entry of 2. For the appearance network we use a network of 4 layers of MLP each with width of 32.  We use the JAX framework for our implementation.  We can train and extract a mesh end-to-end in less than an hour using 8 V100 NVIDIA GPUs.  Finally we experimentally set the  hyper-parameters  $n_c=10$ and  $\epsilon = 0.1$.
 
 
