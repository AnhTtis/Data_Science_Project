\section{Evaluation}


We  validate the effectiveness of our approach on synthetic blender scenes from~\cite{mildenhall2020nerf} and real unbounded scenes from~\cite{barron2022mipnerf360}. Our method can be easily integrated to NeRF pipelines and can help with regularizing the underlying 3D geometry to improve its accuracy. To demonstrate this we use the NeRF backbones of Mip-NeRF 360~\cite{barron2022mipnerf360} that we combine with Neural Graphics Primitives ~\cite{mueller2022instant} to speed up  training, and Ref-NeRF~\cite{verbin2022ref}. 
In addition to the results presented in this section, please see the supplementary materials for additional video results and demos.

% MipNeRF  \cite{barron2021mip} using Neural Graphics Primitives \cite{mueller2022instant} to speed up the training process. We show that our method can be extended to other methods and conditions. We evaluate our method on unbounded scenes from the \cite{barron2022mipnerf360} with Mip-NeRF 360 as backbone  and Shiny Blender scenes from
 



\subsection{Synthetic blender scenes}

% \begin{figure}
%   \centering
%   \includegraphics[width=1.\linewidth]{cvpr2023-author_kit-v1_1-1/latexfigures/blender.png}
%   \caption{Rendered novel views of our 3D meshes on the synthetic blender scenes~\cite{mildenhall2020nerf}.}
%   \label{fig:blender_qualitative}
% \end{figure}

\begin{figure}
  \centering
   \includegraphics[width=1.\linewidth]{figures/rendering.pdf}
   \caption{Rendering of the 3D meshes from test views on the Synthetic Blender dataset.}
   \label{fig:blender_qualitative}
\end{figure}

\begin{figure*}
  \centering
   %\includegraphics[width=1.\linewidth]{figures/qualitative.pdf}
   %\includegraphics[width=0.8\linewidth,trim={0 3cm 0 0},clip,left]{figures/zoom_boxes/drums_80_geometry.png}
    %\includegraphics[width=0.64\linewidth,trim={0 1cm 0 0},clip, left]{figures/zoom_boxes/drums_80_depth.png}
   %\includegraphics[width=0.8\linewidth,trim={0 3cm 0 0},clip,left]{figures/zoom_boxes/lego_180_geometry.png}
    %\includegraphics[width=0.64\linewidth,trim={0 3cm 0 0},clip,left]{figures/zoom_boxes/lego_180_depth.png}
     %  \includegraphics[width=0.8\linewidth,trim={0 3cm 0 0},clip,left]{figures/zoom_boxes/ficus_60_geometry.png}
    %\includegraphics[width=0.64\linewidth,trim={0 3cm 0 0},clip,left]{figures/zoom_boxes/ficus_60_depth.png}
   %\includegraphics[width=\linewidth,trim={0 3cm 0 0},clip]{figures/zoom_boxes/ship_20_depth.png}
    %\includegraphics[width=\linewidth,trim={0 3cm 0 0},clip]{figures/zoom_boxes/ship_20_geometry.png}
    \includegraphics[width=.85\linewidth]{figures/zoom_boxes/baseline_comparison.pdf}
   \caption{Geometry comparison on the Synthetic Blender scenes~\cite{mildenhall2020nerf}. We visualize mesh normals and the depth absolute difference with the ground truth. We notice that meshes obtained using our method have smoother, more accurate and realistic geometry.}
   \label{fig:blender_geometry_comparison}
\end{figure*}



In this section we focus on the Synthetic Blender dataset from~\cite{mildenhall2020nerf}. We compare our method to state of the art baselines that focus on real time rendering of NeRF: SNeRG \cite{hedman2021snerg}, and MobileNeRF \cite{chen2022mobilenerf}. Additionally, we compare to Mip-NeRF 360~\cite{barron2022mipnerf360} from which we reconstruct the geometry with Marching Cubes (MC) at the same grid resolution as ours and using $0.5$ as the zero-level set in the density grid. Similarly to our method we use Mip-NeRF 360 accelerated with Instant-NGP \cite{mueller2022instant}. For our method, we use vertex based feature representation and a grid of size 1024 inside the bounding box obtained with the ground truth mesh for marching cubes. For the \textit{drums} and \textit{ficus}, we use a higher resolution of 2048 since these two scenes are more detailed. We compare the geometry and appearance produced by each method. We train our method together with the Mip-Nerf 360 backbone and  Ref-NeRF backbone.   

\paragraph{Evaluation metrics.} We measure Chamfer Distance (CD) that we compute using an observability mask similar to \cite{jensen2014large}, as we want to evaluate only in observable regions to for fair comparisons. We construct an observability grid of resolution 256 constructed with rays from training views inside the mesh bounding box. Additionally, we compute the normal consistency (NC) as in \cite{Mescheder_2019_CVPR} as the  absolute value of the dot product between normals from a  point cloud sampled on the ground truth mesh  and the predicted mesh normals. Finally, we also evaluate PSNR to measure  appearance quality.

\paragraph{Comparison.} We present quantitative results on the Blender synthetic dataset in \cref{tab:quantitative_results_blender} and qualitative results in \cref{fig:blender_geometry_comparison}. We observe that our method tends to produce better geometric results in terms of Chamfer Distance and normal consistency. In \cref{fig:blender_geometry_comparison} we show the rendering of normal maps from different meshes (row 1, 3, 5) and the absolute difference with  the ground truth depth rendered from the meshes (row 2, 4, 6). We notice that MobileNeRF produces a \emph{"triangle soup"} that does not accurately reflect the geometry of the object. Computed meshes with marching cubes from pretrained NeRFs lead to more floaters and a surface that is not well aligned with the ground truth as we can notice a red region at the boundary of the depth maps as well as lighter color on the surface.  Moreover, while we are expecting lower appearance quality than other methods, due to the constraints of producing accurate geometry and having real time rendering, we note that rendering quality remains very high and visibly close to the ground truth, as can be seen in \cref{tab:quantitative_results_blender} and \cref{fig:blender_qualitative}. 

\paragraph{Effect of the backbone} We observe that using the appropriate NeRF based backbone can improve the quality of the final mesh significantly in \cref{fig:blender_backbone}. In particular, shiny objects such as the  Materials scene  highly benefit from the Ref-NeRF backbone as the method is specifically designed to deal with such objects (bottom of \cref{fig:blender_backbone}). On the other hand, the Ref-NeRF backbone can sometimes fail to approximate the right geometry due to the powerful appearance network while Mip-NeRF 360 produces more realistic geometry, for instance on the ship scene (top  of \cref{fig:blender_backbone}). This effect is also visible in \cref{tab:quantitative_results_blender} as the Mip-NeRF 360 backbone produces more accurate geometry for the Ship scene and Ref-NeRF backbone produces more accurate geometry for the Materials scene. Finally we also observe the effect of the backbones in \cref{fig:blender_geometry_comparison} as the ficus and drums scenes have shiny features that are better reconstructed when using the Ref-NeRF backbone. 


\begin{figure}
  \centering
   \includegraphics[width=1.\linewidth]{figures/backbone.pdf}
   \caption{Effect of the NeRF backbone.}
   \label{fig:blender_backbone}
\end{figure}





 
 
\begin{table*}[t!]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|cccccccccc}
        Metric & Method & Chair & Drums & Ficus & Hotdog & Lego & Mats. & Mic & Ship & Avg  \\
         \hline
      
          \multirow{4}{*}{PSNR $\uparrow$} & MobileNeRF \cite{chen2022mobilenerf} & \textbf{34.09} & \textbf{25.02} & \textbf{30.20} & \textbf{35.46} & \textbf{34.18} & 26.72 & 32.48 & \textbf{29.06} & \textbf{30.90} \\
          & SNeRG \cite{hedman2021snerg}& 33.24 & 24.57 & 29.32 & 34.33 & 33.82 & \textbf{27.21} & \textbf{32.60} & 27.97  & 30.38\\
       
         
         
       
          &\textbf{Ours}[Mip-Nerf 360] &31.44& 23.42& 26.46& 31.63& 29.01& 20.36& 27.41& 26.41 & 27.02\\
      
         &  \textbf{Ours} [Ref-NeRF] &31.93 & 23.49& 25.95& 32.38 &28.89 &23.30 &27.83& 24.70 & 27.31\\
       
        \hline
        %  \multirow{4}{*}{CD $\downarrow$} & MipNerf + Marching Cubes & 0.017 & 0.049 & 0.032& 0.029 &  0.027 &  0.020 & 0.024 & 0.047& 0.031 \\
         \multirow{4}{*}{CD $\downarrow$} &Mip-Nerf 360 + Marching Cubes&0.016&0.047&0.032&0.027&0.024&0.019&0.022&0.044&0.029\\
         &MobileNeRF & 0.015&0.02&0.018&\textbf{0.019}&0.020&0.017&0.022&0.029&0.020\\
       
         &  \textbf{Ours} [Mip-Nerf 360] &0.012&0.018&0.012&0.026&0.018&0.023&0.016&\textbf{0.019}&\textbf{0.018}\\

         & \textbf{Ours} [Ref-NeRF] &\textbf{0.010}&\textbf{0.018}&\textbf{0.008}&0.042&\textbf{0.015}&\textbf{0.014}&\textbf{0.013}&0.104&0.028\\
        \hline
        
         \multirow{4}{*}{NC $\uparrow$}  &Mip-Nerf 360 + Marching Cubes&0.808&0.750&0.779&0.815&0.645&0.836&0.763&0.696&0.762 \\
         &MobileNeRF&0.615&0.550&0.655&0.605&0.546&0.49&0.519&0.535&0.565\\
       
          &  \textbf{Ours} [Mip-Nerf 360]&0.855&0.793&0.809&0.843&0.711&0.824&0.762&\textbf{0.772}&0.796\\

         &\textbf{Ours} [Ref-NeRF] & \textbf{0.857}&\textbf{0.803}&\textbf{0.865}&0.854&\textbf{0.714}&\textbf{0.908}&\textbf{0.773}&0.611&\textbf{0.798}\\
    \end{tabular}}
    \caption{Quantitative results on the Blender Synthetic dataset \cite{martin2021nerf}. We measure appearance with PSNR and geometry with Chamfer Distance (CD) and normal consistency (NC). Our method achieves better geometric reconstruction overall while producing reasonable PSNR.}
    \label{tab:quantitative_results_blender}
\end{table*}

\subsection{Real unbounded scenes}



\begin{table*}[t!]
    \centering
    \begin{tabular}{cc|ccccccc}
        Metric & Method& Bicycle & Garden& Stump& Counter & Room & Bonsai & Kitchen \\
         \hline
         \multirow{4}{*}{PSNR $\uparrow$} & NeRF \cite{mildenhall2020nerf} & 21.76 & 23.11 & 21.73 & 25.67 & 28.56 & 26.81 & 26.31\\
          & MobileNeRF \cite{chen2022mobilenerf} &  21.70  &23.54 &23.95 & - & - & - & -\\
         & Ours & 21.15 &  22.91 & 22.66 &  20.00 &  26.13 &  25.58 & 23.59 \\
    \end{tabular}
    \caption{Novel view evaluation on real unbounded scenes from~\cite{barron2022mipnerf360}.}
    \label{tab:quantitative_results_unbounded}
\end{table*}



\begin{figure}
  \centering
   %\includegraphics[width=1.\linewidth]{figures/geometry_unbounded}
   \includegraphics[width=0.80\linewidth]{figures/geometry_unbounded.png}
   \caption{Results on scenes from the unbounded dataset \cite{barron2022mipnerf360} shown on the left. Rendered color images from our 3D meshes are shown in the right. Note that meshes from our method captures the scene geometry much better compared to MobileNeRF~\cite{chen2022mobilenerf} and \cite{barron2022mipnerf360}.}
   \label{fig:geometry_rendering}
\end{figure}

\begin{figure}
  \centering
   \includegraphics[width=0.8\linewidth]{figures/unbounded_rendering.png}

   \caption{Rendered novel views of our method on real unbounded scenes from Mip-NeRF 360~\cite{barron2022mipnerf360}. Note that rendered images are of high fidelity and captures view dependent effects while being rendered real-time on commodity hardware.}
   \label{fig:unbounded_qualitative}
\end{figure}

In this section we show results on unbounded scenes from Mip-NeRF 360 \cite{barron2022mipnerf360}. We evaluate on the publicly available scenes. We use Mip-NeRF 360 \cite{barron2022mipnerf360} accelerated with NGP from~\cite{mueller2022instant} as our NeRF backbone architecture for experiments on this dataset. Note that there is no ground truth geometry available for these scenes so we do not evaluate the geometry numerically.

When evaluating on unbounded scenes, we extract the mesh using marching cubes separately for both the scene foreground and scene background. To limit the total size of the mesh, we use a foreground box of resolution 1024 to mesh the foreground and separately use a background box of resolution 2048 to mesh the background. Due to the higher number of vertices we build a feature texture to retain image resolution. We construct a per face parametrization of the texture and map each face to a right-angled triangle of side 4 pixels in the texture image, we provide more details in the supplementary materials. 

 \cref{fig:geometry_rendering} provides a qualitative comparison of the underlying geometry obtained from our method with other methods. As shown in \cref{fig:geometry_rendering}, our method produces more accurate geometry compared to MobileNeRF meshes and Mip-Nerf 360 meshes reconstructed via marching cubes. A key contributor of this advantage is the explicit regularization of the SDF via our SSAN. Rendered novel views from our method are shown in \cref{fig:unbounded_qualitative}. We observe in  \cref{fig:unbounded_qualitative} and \cref{tab:quantitative_results_unbounded} that our method remains of good visual quality even though it is constrained by accurate geometry. 

\subsection{Physics-based applications}
Since our method produces accurate 3D meshes, several applications such as scene editing and physics simulation can be easily conducted with traditional graphics and simulation pipelines. In \cref{fig:teaser}, we show simple scene editing where we combine meshes extracted from \textit{Chair}, \textit{Hotdog}, \textit{Lego} (Blender dataset), \textit{Bonsai} and \textit{Garden} (Mip-Nerf 360 dataset) sequences using our method.  We further show a simulation of a cloth falling on objects from Blender Synthetic dataset. Please see the supplementary material for additional video results and demos.

