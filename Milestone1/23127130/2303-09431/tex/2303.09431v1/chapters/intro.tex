\section{Introduction}

\begin{figure}
  \centering
   \includegraphics[width=1.0\linewidth]{figures/teaser_draft.png}
   \caption{Our method extracts meshes with accurate geometry and view dependent appearance given a collection of posed images.  We show a composition of meshes extracted from the \textit{Chair}, \textit{Hotdog}, \textit{Lego} (Blender dataset) and \textit{Garden} (MipNeRF 360 dataset) scenes using our method (Top left: scene rendered with colors, top right: geometry visualization). Our method enables physics based simulations. We show the results of a simulation of a cloth falling on the objects from the Blender Synthetic dataset  (Bottom). }
   \label{fig:teaser}
\end{figure}

Accurate 3D scene and object reconstruction is a key problem in areas such as robotics, photogrammetry, AR/VR, where applications often rely on precise 3D geometry to perform physics-based simulations, real-time 3D visualizations, rendering and interactions. 
Moreover, the related field of novel view synthesis (NVS) has made tremendous advances in recent years.
Recently Mildenhall \textit{et al.} \cite{mildenhall2020nerf} proposed to perform NVS by means of neural radiance fields (NeRFs), a novel 3D representation where each 3D location in space can emit radiance, see Sec \ref{sec-nerfs} for more details. Novel views are  synthesized by means of differentiable volumetric rendering~\cite{mildenhall2020nerf,xie2022neural}. Due to the impressive results and simplicity of the approach, most related work has focused on improving NeRF in terms of image quality~\cite{barron2022mipnerf360}, robustness~\cite{lin2021barf,niemeyer2022regnerf,martin2021nerf}, as well as training speed~\cite{mueller2022instant} and rendering speed~\cite{hedman2021baking,chen2022mobilenerf}. 
Unfortunately, as these representations are commonly optimized for the NVS task and not explicitly for the underlying geometry \cite{chen2022mobilenerf, hedman2021snerg}, it is yet unclear how to best obtain accurate 3D meshes from radiance fields.  Indeed, while the volumetric representation of NeRF enables accurate renderings from new views, the underlying 3D geometry of each object is not uniquely defined as a level-set surface. NeRF methods often rely on layering and transparency effects to approximate complex appearance and geometry. The surface of objects is therefore approximated by dense regions of the volume instead of surfaces of zero thickness. Moreover, most related work still lacks the capability to be rendered in real-time \cite{verbin2022ref, niemeyer2022regnerf}, especially on commodity hardware. Finally, NeRFs cannot be directly integrated with most computer graphics (CG) pipelines, as they still rely on standard 3D meshes due to their compactness and physical properties.

Despite some recent work proposing alternative scene representations to re-enable real-time rendering even for NeRFs, they are again not designed to produce accurate 3D representations of the input objects or scenes to be used with standard CG pipelines~\cite{hedman2021snerg, chen2022mobilenerf}. 


To deal with these limitations, we thus introduce \emph{NeRFMeshing}, an end-to-end pipeline for efficiently extracting geometrically accurate meshes from trained NeRF-based networks, merely adding a very small overhead in time. Our method produces meshes with neural colors having accurate geometry that can be rendered in real time on commodity hardware. %We propose to regularize the underlying geometry of NeRF methods to produce a uniquely defined surface. 
Introducing a novel signed surface approximation network (SSAN), we train a post-processing NeRF pipeline, defining the underlying surface and appearance. SSAN produces an accurate 3D triangle mesh of the scene that we render using a small appearance network to produce view-dependent colors. In contrast to other works that leverage distance fields, requiring significant modifications in the used NeRF architecture~\cite{yariv2021volume, wang2021neus}, our method can be leveraged together with any NeRF, enabling to easily incorporate new advances, such as improved handling of unbounded scenes~\cite{barron2022mipnerf360} or reflective objects~\cite{verbin2022ref}. 
Essentially, SSAN estimates a Truncated Signed Distance Field (TSDF) and a feature appearance field. Harnessing the NeRF approximated geometry as well as the used training views, we distill the trained NeRF into the SSAN model. 
%
We then extract the 3D mesh from the trained SSAN which can be rendered on embedded devices at high frame-rate using rasterization and the appearance network.  Thanks to the flexibility of our method, we can generate these 3D meshes fast ~\cite{mueller2022instant}, are not tied to object-centric scenes~\cite{barron2022mipnerf360}, and can even model complex and non-lambertian surfaces~\cite{verbin2022ref}. 

To summarize, we propose NeRFMeshing, a novel method for capturing both accurate 3D meshes of the scene as well as enabling realistic view dependent rendering. The extracted meshes from our end-to-end pipeline can be integrated in graphics and simulation pipelines. Our model also preserves the high fidelity of neural radiance fields like view-dependent effects and reflections and can be used for real-time novel view synthesis.
