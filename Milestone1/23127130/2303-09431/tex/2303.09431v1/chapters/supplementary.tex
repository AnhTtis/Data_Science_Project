{\begin{flushleft}\LARGE \textbf{Appendix} \end{flushleft}}
% \section{Supplementary Material}

%%%%%%%%%%%%%Table:jointtraining%%%%%%%%%%%%%
\begin{table*}
\begin{center}
\begin{tabular}{l|l|cccccccccc|}
\hline
 Metric & Method & Chair & Drums & Ficus & Hotdog & Lego & Mats. & Mic & Ship & Avg  \\
 \hline
 PSNR &\textbf{Ours}[post-processing] &31.44& 23.42& 26.46& 31.63& 29.01& 20.36& 27.41& 26.41 &27.02 \\
 &  \textbf{Ours} [joint-training]&\textbf{31.54}&\textbf{23.83}&\textbf{26.70}&\textbf{32.16}&\textbf{29.21}&\textbf{21.54}&\textbf{27.89}&\textbf{26.63} & \textbf{27.44}\\
 \hline
 CD &  \textbf{Ours} [post-processing] &\textbf{0.012}&\textbf{0.018}&\textbf{0.012}&\textbf{0.026}&0.018&0.023&0.016&0.019&0.018\\
 &  \textbf{Ours}[joint-training]  &0.013&\textbf{0.018}&0.013&0.030&\textbf{0.017}&\textbf{0.021}&\textbf{0.015}&\textbf{0.028}&\textbf{0.019}\\
   \hline
  
  NC &  \textbf{Ours} [post-processing]&\textbf{0.855}&0.793&0.809&0.843&\textbf{0.711}&\textbf{0.824}&\textbf{0.762}&\textbf{0.772}&0.796\\
&  \textbf{Ours} [joint-training] &0.842&\textbf{0.803}&\textbf{0.844}&\textbf{0.860}&0.697&0.809&0.758&0.769&\textbf{0.798}\\
 \hline
\end{tabular}
\end{center}
\caption{Effect of training stages on the blender synthetic dataset.}
\label{tab:jointtraining}
\end{table*}

%%%%%%%%%%%%%Table:ablation_percentiles%%%%%%%%%%%%%
\begin{table}
\begin{center}
\begin{tabular}{l|c|c|c}
\hline
 & PSNR &CD & NC \\
\hline
 w/o appearance network separation& 26.74 &\textbf{0.018} & \textbf{0.802} \\
 w/ percentiles 5 \& 95& 27.01 &\textbf{0.018} & 0.796 \\
 w/ percentiles 25 \& 75& \textbf{27.02} &\textbf{0.018} & 0.796 \\
%  Ours w/o percentiles, w/ offset & 27.03 &0.018& 0.765 \\
 w/o projection & 26.82& \textbf{0.018}& 0.798\\
 Ours& \textbf{27.02} &\textbf{0.018} & 0.796 \\
\hline
\end{tabular}
\end{center}
\caption{Ablation study on the blender synthetic dataset.}
\label{tab:ablation_percentiles}
\end{table}

\section{Joint training}  \label{sec:jointtraining}
 
 We choose to train our method from a pre-trained NeRF method. In this section we measure the performance of training our method simultaneously to an un-trained Mip-NeRF 360 using instant NGP~\cite{mueller2022instant}.  We observe in  \cref{tab:jointtraining} that joint training leads to better appearance. However, due to the increased simplicity we choose to evaluate our method as post-processing from a pre-trained NeRF method. 
    
         
\section{Ablation study}   \label{sec:ablation}

We evaluate the effect of some of our design choices for our method trained with a Mip-NeRF 360 with Instant NGP on the Blender Synthetic dataset in\cref{tab:ablation_percentiles}.  
\paragraph{Effect of choice of percentiles.} We train our method on the blender synthetic dataset with different choices of percentile values. We notice that this choice only has a minor effect (\textit{c.f.}~\cref{tab:ablation_percentiles} row 2, 3, 5). 

\paragraph{Effect of separate appearance network.} We train our method with a single network to represent both the geometry and the appearance. We notice that separating the network leads to a small improvement over PSNR whilst geometric metrics remain comparable  \cref{tab:ablation_percentiles} row 1, 5. 

\paragraph{Effect of  the projection.} We train our method without the projection defined in Equation 10. We notice that it leads to a small drop in PSNR while geometric metrics are not much affected (\textit{c.f.}~\cref{tab:ablation_percentiles} row 4, 5).

\section{Additional implementation details} \label{sec:implementationdetails}
\subsection{Projection on the zero level set }
The median depth computed from a pretrained NeRF can sometimes be inaccurate, especially when the angle between the view direction and the surface is small. To get a better approximation of the zero level-set of the surface we apply 4 steps of  gradient descent along the viewing ray using the TSDF predicted value. We define one step applied to a 3D point $p$ as :

\begin{equation}
p' = p + \alpha * \hat{t},
\text{ with }
[\hat{t}, \mathbf{\hat{n}}, \mathbf{\hat{f}}] = \phi(\mathbf{p}),
\end{equation}
where $\alpha$ is a small constant. 
\subsection{Per face parametrization}
When evaluating on unbounded scenes from \cite{barron2022mipnerf360}, we represent scene features using textures with per face parametrization. We map each face in the triangle mesh to a right angled triangle of side 4 in the texture image. We show the face shape in \cref{fig:parametrization}. Each pixel feature  $\mathbf{\hat{f}}$ is computed using the SSAN at the corresponding 3D position.  

\begin{figure}
  \centering
   \includegraphics[width=0.3\linewidth]{figures/per_face_parametrization.png}
   \caption{Representation of each face of the  mesh in the texture image. Each face is mapped to a right angled triangle containing features predicted by SSAN.}
   \label{fig:parametrization}
\end{figure}

\subsection{Training times}
Our method has 4.8 minutes training time on average on the Blender Synthetic Dataset.

