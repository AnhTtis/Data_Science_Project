\section{Method}\label{sec:method}

Our method extracts a CNN fingerprint of an image generator using a small number of generated images and applies it for the detection of other images produced by the same image generator. The method relies on the properties of CNNs, as explained below with a simple experiment~(\cref{sec:induce}). Then we'll explain the fingerprint extraction~(\cref{sec:train}) and following detection process~(\cref{sec:inference}), including implementation details~(\cref{sec:implement}).

\subsection{Deep Image Fingerprint}\label{sec:induce}

Deep Image Prior~\cite{art:dip} demonstrated that CNNs incorporate an image prior in their structure. Consider an image restoration task, where given a corrupt image $X \in \RE$ the goal is to obtain the restored image $\hat{Y} \in \RE$ from a trained model $h_{\theta}$:
\begin{equation}
   \hat{Y} = \min_{\theta} E(h_{\theta},X ) + P(X)
   \label{eq:genImage}
\end{equation}
$E$ is a data term and $P$ is the image prior.

For a CNN encoder-decoder model $g_{\theta}$, the architecture itself serves as the image prior. Thus, the optimization task is simplified, as the search is focused on finding only the data term $E$ within the weights ($\theta$) space of the CNN without the need of learning $P$. Here the input of $g_{\theta}$ serves random tensor $Z \in \mathbb{R}^{C \times H \times W}$, where each element is $z_i \sim U(0,1)$.
\begin{equation}
   \hat{Y} = \min \limits_{\theta} E(g_{\theta}(Z), X)
   \label{eq:dipOpt}
\end{equation}

After the optimization, the reconstructed image is given by: 
\begin{equation}
   \hat{Y} = g_{\hat{\theta}}(Z), \;\;\; \hat{\theta} = \argmin_{\theta} E(g_{\theta}(Z), X)
   \label{eq:dipOutput}
\end{equation} 
But, following a number of observations (e.g,~\cite{art:ganfingerprint,art:ganfingerprintAE}),  \cref{eq:dipOutput} seems to be incomplete. Images produced by each CNN also contain unique fingerprint (\FI$\in \RE$), thus: 
\begin{equation}
   \hat{Y} + \mathcal{F}(g_{\hat{\theta}}) = g_{\hat{\theta}}(Z)
\end{equation}
To prove this statement, we perform a simple experiment: we optimize CNN ($g_{\theta}(Z)$) to produce a single gray image without any semantic content. It seems as a trivial task, yet after convergence, the model is still unable to reconstruct the image perfectly. Grid-like and line artifacts are observed in the image space and verified with \fft{} transform of the output image. Furthermore, if we repeat the experiment with different random seeds, artifact changes as well (\cref{fig:monochromeExp}) but retain it's grid-like structure. 
Previous works have also observed this phenomena, but only in the context of
GAN models~\cite{art:ganfingerprint, art:ganfingerprintAE}. 
\input{figs/figures/blank_exp}

Consequently, in parallel to the Deep Image Prior~\cite{art:dip}, where CNN's structure was proven to be image prior, here we have shown that CNN's structure is also a fingerprint prior. As a results the method described below will be reffered as the \emph{deep image fingerprint} (DIF).

\subsection{Fingerprint Extraction}\label{sec:train}

We can extract fingerprints of a target model by an optimization of $\theta$ given a set of generated images and a set of arbitrary real images. The optimization is similar to the denoising procedure~\cite{art:dip}, where \FI{} is acquired by \cref{eq:dipOutput}, but instead of computing mean square error loss with respect to some image we compute correlation with respect to a set of image residuals. Residual $R_i \in \RE$ is defined as $R_i = f_{HP}(X_i)$, where ($X_i$) is an image and \HP is a high-pass filter in a form of DnCNN-S~\cite{art:dncnn}.

The goal is to produce fingerprint that is highly correlated with residuals of generated images, and non correlated with residuals of real images. Pearson Correlation Coefficient is proved to be a good correlation metric between image residual and model's fingerprint~\cite{art:ganfingerprint, art:ganfingerprintAE}. In practice we transform each input into their zero-mean and unit-norm versions, preform inner product and average values. This will be referred simply as \emph{correlation} and denoted as $\rho(\cdot, \cdot)$.

The loss function is formulated similarly to contrastive loss in siamese setting~\cite{art:contrastive}. We define a similarity factor $t_{ij} \in \{0,1\}$, which is equal to 1 when two residuals are from the same class and 0 otherwise and correlation distance ($D_{ij}$) as euclidean distance between correlation values:
\begin{equation}
   D_{ij} = \sqrt{(\rho(R_i, \mathcal{F}) - \rho(R_j, \mathcal{F}))^2}
   \label{eq:corrDist}
\end{equation}
Finally the sample loss function ($\mathcal{L_{S}}$) is summarised below:
\begin{equation}
   \mathcal{L_{S}} =\frac{ t_{ij} \cdot D_{ij} + (1 - t_{ij}) \cdot \left( m - D_{ij}
    \right)}{m}
    \label{eq:lossS}
\end{equation}
$m$ is a hyper parameter. 

\subsubsection{Fingerprint Boost}
\label{subsec:fingerprint-boost}

Some models produce \emph{weak} fingerprint, specifically artifacts with low energy and/or low periodicity. To address this issue we suggest to preform a signal boost during the extraction process. Fingerprint is spatial-stationary~\cite{art:ganfingerprint, art:ganfingerprintAE}, therefore is amplified by the averaging of residuals. Consequently, averaging over residuals within a batch will provide the required boost. 

We propose a new \emph{Booster loss} ($\mathcal{L_{B}}$) as the realization of this idea. It is similar to the sample loss $\mathcal{L_{S}}$, but applied to averaged residuals within the batch per real ($\boldsymbol{r}$) and generated ($\boldsymbol{g}$) image classes. We summarize it in \cref{eq:loss}, where $M_{\boldsymbol{r}}$ and $M_{\boldsymbol{g}}$ are amount of real and generated images, respectively, within a batch.
    \begin{align}
    & R_{\boldsymbol{r}} = \frac{1}{M_{\boldsymbol{r}}} \sum_{i \in \boldsymbol{r}} R_i, \;\;\;
    R_{\boldsymbol{g}} = \frac{1}{M_{\boldsymbol{g}}} \sum_{i \in \boldsymbol{g}} R_i 
    \\ 
    & D_{\boldsymbol{r}, \boldsymbol{g}} = \sqrt{(\rho(R_{\boldsymbol{r}}, \mathcal{F}) - \rho(R_{\boldsymbol{g}}, \mathcal{F}))^2} 
    \\ 
   & \mathcal{L_{B}} = \frac{\left( m - D_{\boldsymbol{r}, \boldsymbol{g}}\right)}{m}
    \label{eq:lossB}
   \end{align}
The final loss term is an average of sample loss:
\begin{equation}
   \mathcal{L} = \frac{\mathcal{L_{B}} + \mathcal{L_{S}}}{2}
    \label{eq:loss}
\end{equation}

In addition, to diversify signal strength during the training booster loss is accompanied with augmentations that mimic low energy signal behaviour. Each residual is multiplied by an energy reduction factor $\alpha$ which is randomly sampled per each residual - $\alpha \cdot R_i, \; \alpha \sim U(a,1)$. $a$ is a hyperparameter, but in this work is constant and set to 0.5.

Despite that the booster loss and augmentations were part of experiments, they are not participate in the final results. We found that they don't impact on method performance in most cases, therefore their effect will be discussed separately. 

\subsection{Detection of Generated Images}\label{sec:inference}

We perform detection through hypothesis testing. First, we compute the means of the populations of real and fake correlations according to \cref{eq:refVals}, denoted as $\mu_{\boldsymbol{r}}$ and $\mu_{\boldsymbol{g}}$, respectively. $N_{\boldsymbol{r}}$ and $N_{\boldsymbol{g}}$ represent the number of real and generated images in the training set. Then, for each test image, we extract its residual $R_{\text{test}} = f_{HP}(X_{\text{test}})$ and then perform hypothesis testing as follows: if $|\rho( R_{test}, \mathcal{F} ) - \mu_{\boldsymbol{g}}| < |\rho(R_{test}, \mathcal{F}) - \mu_{\boldsymbol{r}}|$, the tested image is considered generated, otherwise, it is considered real or not produced by the target model. This procedure does not require any parameters, such as threshold.
\begin{equation}
   \mu_{\boldsymbol{r}} = \frac{1}{N_{\boldsymbol{r}}} 
   \sum_{i \in \boldsymbol{r}} \rho(R_{i}, \mathcal{F})
   \;,\;\;
   \mu_{\boldsymbol{g}} = \frac{1}{N_{\boldsymbol{r}}} 
   \sum_{i \in \boldsymbol{g}} \rho(R_{i}, \mathcal{F})
    \label{eq:refVals}
\end{equation}

\subsection{Implementation Details}\label{sec:implement}

In all of our experiments (\cref{sec:experiments}), we train and test the method in a similar manner. Margin is constant $m=0.01$ and $z$ has 16 channels. \HP is a pre-trained DnCNN model~\cite{art:dncnn} that is trained separately on real images with sigma range of $[5,15]$ and crop size $48 \times 48$ pixels. During the extraction procedure, \HP{'s} weights are not updated. The extraction model is a U-net~\cite{art:unet}. Optimization was carried out using Adam~\cite{art:adam}, with a constant learning rate of $5e^{-4}$. During training, the fingerprint was accumulated using exponential moving averaging. We provide additional details about the training process of \HP{} and U-Net architecture in our supplementary materials.
