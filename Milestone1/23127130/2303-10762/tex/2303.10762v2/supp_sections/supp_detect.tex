

\section{Datasets}\label{apx:data}

\subsection{GAN Datasets}

The GAN datasets that we use in this work are from the supplementary materials of Wang~\etal~\cite{art:easycnn}. Train set consists of 360k real images corresponding to 20 LSUN classes~\cite{art:lsun} and 360k images generated by 20 ProGAN~\cite{art:progan} models. Models are trained per LSUN class. For the ProGAN$_t$ dataset we randomly select 2,000 images per real and fake class, for each LSUN image class, resulting in 4,000 images per LSUN class overall. Test set of~\cite{art:easycnn} contains images generated by a number of GANs, specifically by StyleGAN~\cite{art:stylegan}, StyleGAN2~\cite{art:stylegan2}, BigGAN~\cite{art:biggan}, StarGAN~\cite{art:stargan}, GauGAN~\cite{art:gaugan} and ProGAN~\cite{art:progan}. To the latter we refer as ProGAN$_e$, which is different from ProGAN$_t$ only in the amount of images per class.

\subsection{Fined-Tuned Stable Diffusion Models}

We will outline the process of constructing datasets for the fine-tuned Stable Diffusion models discussed in \cref{sec:source}.
In all cases, fine-tuning involves the DreamBooth method~\cite{art:dreambooth}, specifically \TI{} is trained to reproduce a target object or style from a pre-defined keyword. There are three models: SD 1.4S, SD 1.5A, and SD 2.0R. The authors customly fine-tuned SD 1.4S with 10 photos. SD 1.5A\footnote{\url{https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion}}
is a publicly available model that was fine-tuned in two stages: first, SD 1.4 was fine-tuned with 680k anime images, and then, after replacing the image decoder with one from SD 1.5, it was additionally fine-tuned with~\cite{art:dreambooth} on a different set of anime images. The last model, SD 2.0R\footnote{\url{https://huggingface.co/nousr/robo-diffusion-2-base}}%
, is fine-tuned with an unspecified amount of robot images. Following above, keywords are known for each case, as SD 1.4S is trained by us, and for others, keyword is specified in the instructions for each model. To produce images with a new models a keyword is added to captions from the \laion{} dataset~\cite{misc:laion5b} in the following format: ``keyword caption''. This forces fine-tuned model to generate images with new information previously unknown to the source models. \cref{fig:sdExamples} shows an example of outputs resulting from the same caption.

\section{Additional Results}\label{apx:adresults}


\subsection{Results of Gray Image Reconstruction}

For completeness, we present additional examples of the gray image reconstruction experiment (\cref{sec:induce}).
in Figure \ref{fig:monochromeExpand}. In each case, the model produces grid-like artifacts that are densely distributed throughout the image or concentrated within a small region. For each output, we also show the Fourier transform, which has been adjusted for better visualization. We have also included the gray image and its \fft{} for reference, which have undergone the same adjustments.

\input{figs/figures/blank_expanded}

\subsection{The Effect of Train Set Size}

In \cref{sec:detection}
we present an evaluation of the proposed method as a detector of generated images. To evaluate the method under various amounts of training samples, we measure performance on images produced by both GAN and \TI{} models, as shown in \cref{fig:trainGAN,fig:trainT2I}. We observe that the accuracy remains stable across most of the models and starts to degrade drastically at $N_S = 128$, where we observe a 5\% drop for the majority of the models.

\input{figs/figures/gan_train_plot}

\subsection{Consistency for a Low Sample Amount}
We report the consistency of accuracy with a low number of samples. In this setting, each training sample has a greater impact on the results. To test this effect, we trained four models (Resnet50${F}$, Grag21${F}$, Grag21, and DIF) ten times with 128 samples according to Section 4.2. Each time we randomly sampled the training set. As shown in \cref{fig:boxes}, despite the weaker consistency of the results for DIF, statistics support the relation observed in Table 2.

\input{figs/figures/box_plots}

\subsection{The Effect of Booster Loss}

We compare the detection accuracy with and without the use of booster loss and augmentation (\cref{sec:method}). The results are presented in \cref{tab:accuracyDIFs,tab:gans} for GAN models and \TI{s}, respectively. We observe that, for most models, there is little to no variation in accuracy. However, we do observe an increase in accuracy for ProGAN$_e$ and \dalT{} models by 9.5\% and 1.9\%, respectively, which are challenging datasets for DIF. 

\input{figs/tables/gan_test}
\input{figs/tables/comparison}
\input{figs/figures/sd_examples}

\subsection{Cross-Detection for GAN Models}

We report the complete map of cross-detection for datasets corresponding to GAN and ProGAN$_t$ models in \cref{fig:crossevalGAN,fig:crossevalPROGANA} respectively. As shown in the figures, the cross-correlation is low for all the models, thus models were trained separately.

\input{figs/figures/cross_eval_gans}%

\input{figs/figures/cross_eval_progans}%


\subsection{Cross-Detection for Custom Trained ProGANs}

Models $P_A$, $P_B$, $\hat{P}_A$ and $\hat{P}_B$ were trained on centrally cropped images ($128 \times 128$ px) from AFHQ~\cite{art:stargan2}. We report the full cross-detection matrix for the four models in \cref{fig:crossevalCUSTOMF}. The cross-detection across all the models is similar, namely no symmetry between different models and high-symmetry within converged models. In addition we present the examples of images generated by $P_A$ ordered according to check point epochs at \cref{fig:catExamples}. Image quality corresponds to the convergence state of the model. Starting from epoch 40 the model produces visually appealing results, that correspond to symmetric and high cross-detection accuracy observed in \cref{fig:crossevalCUSTOMF}. Cross-detection for $\hat{P}_A$ is symmetric, but with lower cross-detection accuracy values. We suspect that in this specific case, the model did not converge properly.

\subsection{Cross-Detection vs. Cross-Correlation}
In this section, we explain why model lineage estimation is done through image cross-detection rather than fingerprint cross-correlation. Cross-correlation (Section 3) is a more intuitive process where we expect a value of 1 for the correlation of a fingerprint with itself, high absolute values for similar fingerprints and low values for unrelated fingerprints. However, during our experiments (Section 4.3), we discovered that these values are not informative. The absolute cross-correlation values of extracted fingerprints ($\mathcal{F}_E$) show (\cref{fig:crossevalCUSTOMFE}) a random pattern, whereas fingerprints extracted by residual averaging ($\mathcal{F}_A$) show (\cref{fig:crossevalCUSTOMFA}) a trend similar to cross-detection (\cref{fig:crossevalCUSTOMF}). Yet, the cross-detection of $\mathcal{F}_A$ diminishes faster with epochs, is always symmetric and not normalized. Consequently, the certainty if models are related is lower due to: a) higher sensitivity to changes during training/fine-tuning, b) symmetry is no longer a parameter and c) the correlation value alone is not informative.

\input{figs/figures/custom_gans_full}

\input{figs/figures/custom_gan_fingers}

