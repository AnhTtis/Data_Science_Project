\section{Introduction}\label{sec:intro}

Generative neural networks enable the generation of high-quality images. While this has many benefits for scientific, creative, and business purposes, it can also be used for malicious deception. As image quality continues to improve, it becomes increasingly difficult for human observers to distinguish between real and fake images without careful observation and identification of inconsistencies, especially in spite of large text-to-image-models (\TI{s})~\cite{art:faridGeo, art:faridLight}. Therefore, there is an urgent need for an automated tool that can detect generated images and keep pace with the rapid advancements. This work aims to provide such a method, which has demonstrated good performance on both novel and popular approaches.

The advancement of image generation has been made possible by the use of Deep Neural Networks (DNNs), particularly the Convolutional Neural Networks (CNNs) subclass. CNNs provide the best of both worlds by combining the image prior~\cite{art:dip} with the flexibility of DNNs~\cite{art:dnn}. This has led to the development of image generators, which are generative models trained to produce images given a sample from known distributions and can be conditioned on input. The most popular type of image generator families is the Generative Adversarial Network (GAN)~\cite{art:gan, art:dcgan}, which quickly gained popularity for its ability to produce high-quality and high-resolution images in various applications such as~\cite{art:biggan,art:progan,art:stylegan,art:stylegan2, art:cyclegan,art:stargan}. Previously these models have demonstrated state-of-the-art results in the field of image generation.

However, the advent of diffusion models~\cite{art:ddpm, art:ldm} have introduced a new paradigm that has shown the ability to generate high-quality images surpassing those produced by GANs~\cite{art:ganvsddpm}. Diffusion models are also a type of generative models and can be easily conditioned to arbitrary input. This gave rise to a new type of image generators, the \TI{s}. These models incorporate advanced image generators (e.g.~\cite{art:ddpm,art:vqgan}) and are capable of generating high-quality images from text captions combined with other input domains~\cite{art:parti, art:imagen, art:dalle2, proc:glide}. Despite all of these advancements and conceptual shifts, the fundamental basis of image generators remains unchanged, as they continue to rely on DNNs. 

To detect generated images, there are typically two approaches: data-driven and rule-based. Data-driven methods~\cite{art:easycnn, art:easygan, art:universalGan} involve training models on large datasets of images, which are expected to generalize to unseen data. However, these methods may struggle to detect images from conceptually different generators, as shown in a later study~\cite{art:diffDetect}. Rule-based methods, on the other hand, rely on identifying common patterns or characteristics seen in generated images~\cite{art:ganfingerprint,art:ganfingerprintAE,art:Joslin2020, art:ganColorspaces, art:ganclues, art:graphgan, art:forencisCNN}, and typically require less data. However, most of these rules have only been demonstrated on a selected set of GAN models and image domains, which makes it necessary to re-evaluate the rules for novel types of image generators. This can be a laborious process as the rules are human-devised.

Despite significant progress in synthetic images detection, the widespread adoption of \TI{s} created new challenges in the field. Due to the resource-intensive nature of \TI{s}' training, access to \TI{s} and the amount of available generated images for research purposes is limited. As a result, data-driven approaches and some rule-based methods may not be practical, as they require a significant amount of data for training, and in some cases the training of an image generator itself. Therefore, new detectors are expected to not only perform well, but also to be trained on a small number of images.


The proposed method is a rule-based approach that can achieve high detection accuracy using a small number of generated images (less than 512) for training. It utilizes the CNN inductive bias to extract fingerprints of image generators from the training set and applies them to detect generated images from the same model and its fine-tuned version. We tested the method on a large set of image generators and found that it achieves results that are comparable to, or better than, previously demonstrated state-of-the-art performance. In addition, we demonstrate how our method may be utilized for model lineage analysis.
