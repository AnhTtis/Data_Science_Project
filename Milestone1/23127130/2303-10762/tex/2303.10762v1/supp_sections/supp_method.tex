
\section{Implementation Details}\label{apx:method}

This section includes the implementation details of our work. The DIF models were trained and tested on an RTX 3060 GPU with 12 GB of VRAM. We used PyTorch~\cite{art:pytorch} as our deep learning framework
\footnote{\url{https://github.com/Sergo2020/DIF_pytorch_official}}.

\subsection{High-Pass Filter --- DnCNN}

The DnCNN-S~\cite{art:dncnn} model was trained separately from the residual extraction procedure. The training was performed according to the original work with minor changes. We trained the DnCNN-S model for 2,000 epochs with a learning rate of $10^{-4}$ and the Adam optimizer~\cite{art:adam}. Only real images were used during training. Random crop is set to size $(48 \times 48)$ pixels and a sigma range is set to [5, 15]. The number of training images is 1024.

During the inference of the DnCNN, we applied post-processing. First, the input to the DnCNN was padded with 10 pixels on each dimension and then reduced as post-processing according to the recommendations of the authors~\cite{art:dncnn}. Additionally, we observed a bias within the residuals, so we performed an additional post-processing step. We took the training set of the DnCNN and calculated the average of its residuals, thus estimating the fingerprint of the DnCNN ($\mathcal{F}_{DnCNN}$). During the inference of the model, this fingerprint was subtracted from the output according to:
\begin{equation}
    R_i = f_{HP}(X_i) - \mathcal{F}_{DnCNN}
\end{equation}

\subsection{U-Net Architecture}

In \cref{tab:unet}, we summarize the architecture of the U-Net model, denoted in the paper as $g_{\theta}$. Each row represents a convolution block, comprised of two convolutional layers and either an up-sampling or a down-sampling layer. The convolutional layers have a kernel size of 3, stride of 1, and padding of 1 pixels for each spatial dimension. Each convolutional layer is accompanied by Batch-Normalization~\cite{art:batchnorm} and an activation function, which are specified in \cref{tab:unet}. We use max-pooling with a kernel size of $2 \times 2$ pixels for down-sampling and a deconvolution layer with a kernel size of $2 \times 2$ pixels and stride of 2 pixels for up-sampling. The latter is suspected to be the main causes of grid-like artifacts~\cite{art:checkerboard}. The last block consists only of a single convolution layer where we do not use Batch-Normalization and rely on hyperbolic tangent (TanH) as the activation function.
\input{figs/tables/unet}
