
\section{Experiments}\label{sec:experiments}

This study includes a series of experiments that involve a varied collection of images generated by different \TI{s} and GANs. The datasets are summarized in \cref{sec:data}. The detection results are presented in \cref{sec:detection}, which is divided into two parts: the detection of images produced by \TI{s} and the detection of GAN-generated images. In \cref{sec:source}, we investigate the effect of image generator training and fine-tuning on its fingerprint, and in \cref{sec:lineage}, we analyze the relationship between selected \TI{s}. Finally, in \cref{sec:robustness}, we test the method on compressed images.

\subsection{Detection Data}\label{sec:data}

Our data includes generated images from a variety of \TI{} and GAN models. In contrast to \TI{s}, that produce multi-domain images, GANs are often limited to a specific image domain that they were trained on. Therefore, some of the GAN datasets include a number of image domains, each produced by a different model. \cref{tab:data} summarizes the data. The amount of real and generated images per set is equal. During the experiments we randomly split each dataset into equally sized train and test sets.
\input{figs/tables/data.tex}

For \TI{}, we randomly selected real images and their corresponding captions from the \laion~\cite{misc:laion5b} dataset. Then, we used the captions to generate corresponding images using publicly available Stable Diffusion models~\cite{misc:sd}, versions 1.4 (SD 1.4) and 2.1 (SD 2.1), \dalM~\cite{misc:dallemini} and GLIDE~\cite{proc:glide}. In the case of \dalT~\cite{art:dalle2} we generated images with OpenAI's official API\footnote{\url{https://platform.openai.com/docs/guides/images}}. Lastly, a large set of generated images produced by MidJourney (MJ)~\cite{misc:midJ} is publicly available on the Kaggle website~\cite{misc:midJdata}, from which we select a random subset for this work.

Datasets representing GAN models were obtained from the supplementary materials of Want~et~al.~\cite{art:easycnn}. Our supplementary document contains a detailed review of the datasets. ProGAN$_t$ and ProGAN$_e$ datasets are corresponding to ProGAN train set and test set from the materials above.

\subsection{Detection of Generated Images}\label{sec:detection}

We evaluated DIF as a generated image detector and compared it to rule-based and data-driven methods. We summarized the results of detecting images produced by \TI{s} and GANs in \cref{tab:accuracyTI} and \cref{tab:accuracyGAN}, respectively. To represent the number of training samples, we used $N_S$. Method indices corresponded to deployment methods: $D$ for pre-trained models that were only tested, $F$ for pre-trained models that were fine-tuned and evaluated, and no index indicated that the model was trained from scratch and tested. If a method performed poorly on a large number of training images, we did not further test it with fewer images.

\input{figs/tables/comparisonTI}

We compare to two rule-based methods. Marra18~\cite{art:ganfingerprint} and Joslin20~\cite{art:Joslin2020} preform \FI{} extraction with simple averaging over residuals. Marra18 classifies images with a correlation function~(\cref{sec:train}), Joslin20 preforms correlation in Fourier space. For both methods, residuals are extracted with the same \HP as in the case of DIF. 

We also compare to data-driven methods: pre-trained Resnet50~\cite{art:resnet} for image classification~\cite{art:imagenet}, Wang20~\cite{art:easycnn} is Resnet50, which is trained on ProGAN images including heavy augmentations, and Grag21~\cite{art:easygan} is similar to Wang20, but utilized modified Resnet50 and is trained on StyleGAN images. Grag21 previously demonstrated state-of-the-art results on detection of images produced by the same image generator family~\cite{art:diffDetect}. In the case where models are fined-tuned, they are frozen and only the last fully-connected layer is reinitialized and updated during training.

Following \cref{tab:accuracyTI}, the proposed method performs extremely well with generated images produced by \TI{s}. It achieves higher accuracy or is on par with pre-trained methods. A notable case is $N_S=128$, where only Grag21$_F$ outperforms DIF. However, unlike DIF, this model was pre-trained on a large set of real and fake images prior to the experiment, giving it an edge. In particular, for SD 1.4, MJ, and \dalM{}, DIF outperforms all the competitors, while for GLIDE, SD 2.1, and \dalT{}, the accuracy is lower. To understand this behavior, let us investigate the fingerprints of SD 1.4 vs. GLIDE in image space and Fourier space. Refer to \cref{fig:examplesExt}, where four images per model are shown: fingerprint extracted by averaging of image residuals ($\mathcal{F}_A$) and fingerprint extracted with DIF ($\mathcal{F}_E$) and their FFTs. In contrast to SD 1.4, GLIDE's $\mathcal{F}_A$ contains only barely visible vertical lines and two peaks in Fourier space, thus it is a weak fingerprint (\cref{sec:train}). Weak fingerprint are also found in SD 2.1, and \dalT{}, reflecting their low detection accuracy. We'll discuss the effect of Booster loss on these cases later during the section.

\input{figs/figures/ffts}

To our surprise, even though we intended DIF to be a detector of \TI{s}, it performs the best for four out of seven GAN models in terms of detection accuracy (\cref{tab:accuracyGAN}). This is surprising, given that Grag21 and Wang20 were specifically developed and trained to generalize well on images produced by GANs. The lowest detection accuracy is observed for the ProGAN$_e$ dataset, which can be explained by the large variety of models and the low number of images per model. DIF is designed to detect images from a single model, which explains the relatively low accuracy for this dataset. For completeness, we also measure detection accuracy for ProGAN$_t$, where DIF is trained on images of each model separately. In this case, the mean accuracy is 91.2\%, with the highest accuracy being 96.0\% and the lowest being 83.2\%. We present the full evaluation in our supplementary materials.

Lastly, we confirm that the proposed Booster loss (\cref{sec:train}) improves accuracy for models with a weak fingerprint and for datasets that contain a mix of modalities. Applying the Booster loss increase accuracy by 10\% for ProGAN with 512 train samples (67.1\% instead of 57.6\%) and by 2\% for \dalT{} with 64 train samples (78.0\% instead of 76.1\%). The supplementary materials contain a comprehensive evaluation of Booster loss on other datasets. 

Summarizing the experiments, DIF performs well with both novel and well-established image generation methods. We showed that in the detection of \TI{}-produced images, DIF outperforms the fine-tuned state-of-the-art method (Grag21) on four out of six test sets. Furthermore, for GAN models, the proposed method outperformed pre-trained models (Wang20 and Grag21) that were specifically designed to generalize well for images produced by the GAN model family and had previously demonstrated exceptional results on this task.

Our method has two main failure modes: some models produce weak artifacts, resulting in relatively low detection accuracy (e.g., 79.5\% for \dalT{}), and training on a mix of images from various image generators leads to poor performance (e.g., 57.7\% for ProGAN$_e$). Although the proposed booster loss (\cref{sec:train}) improves the results for both cases, further improvements for these challenging cases are a promising avenue for future work.

\input{figs/tables/comparisonGAN}

\subsection{Detection of Images From Fine-Tuned Models}\label{sec:source}

Now consider a more challenging setting, where given a trained DIF for some image generator we aim to detect images generated by it's fine-tuned version. Due to uniqueness of fingerprints (\cref{sec:related}) images generated by these new model variations might not be detected by DIF trained on images produced by the original model.

To investigate the relation between source models and their variations we conduct an experiment using varied datasets and checkpoints. First, we train four ProGAN models ($P_{A}, P_{B}, \hat{P}_{A}$, and $\hat{P}_{B}$) for 70 epochs. $P_{A}$ and $P_{B}$ are trained on 2,500 ``cat'' class images from AFHQ~\cite{art:stylegan2} with random seeds A and B respectively. $\hat{P}_{A}$ and $\hat{P}_{B}$ are trained with the same random seeds, but on 2,500 ``wild'' class images from AFHQ. Next, for each model we use five checkpoints at epochs 20,32,40,52 and 70, and generate 2,500 images for each. Finally, we construct 20 datasets by adding real images from the corresponding train set of ProGANs to each set of generated images. This results in 20 ProGAN models with 20 datasets.

\input{figs/figures/cats_cross_eval}

We preform a cross-detection on $P_{A}, P_{B}, \hat{P}_{A}$ and $\hat{P}_{B}$ and for brevity summarize results for $P_{A}$ and $P_{B}$ in \cref{fig:crossevalCATS}. The full cross-detection map is available in our supplementary materials. During cross-detection we attempt to classify images produced by some image generator with DIF which is trained on images from another image generator. Observe the symmetric relation within the same model: for checkpoints of epochs 40, 52 and 70 cross-detection accuracy is high and symmetric. Other relations include low accuracy and/or asymmetric, which is exactly what is expected for DIF trained on unique fingerprints. We may conclude that the model's fingerprint changes during training. However, as the model converges, these changes become insignificant, resulting in high cross-detection accuracy for DIF.

We conduct an additional experiment where we measure cross-detection on images from a number of fine-tuned stable diffusion models~\cite{art:dreambooth}, which involve updates of image decoder weights. The models are: our custom fine-tuned SD 1.4 with a small set of images and two downloaded models of stable diffusion v1.5 and v2.0 fine-tuned on a large set of anime images\footnote{\url{https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion}} and robot images\footnote{\url{https://huggingface.co/nousr/robo-diffusion-2-base}}. Models denoted as SD 1.4S, SD 1.5A and SD 2.0R respectively. For each model we generated 1000 images from the same caption set that was used with SD 1.4, including style/object keywords specific to each model. Then train DIF for each with 512 real and 512 generated images (\cref{sec:detection}). Upon analyzing the results presented in \cref{fig:crossevalSD}, we can conclude that the relationships observed in previous experiments are maintained even when the model is fine-tuned with new data. Thus, we can conclude that DIF will also perform well on images generated by fine-tuned variants of the models.

\input{figs/figures/cross_eval_sd}

\subsection{Model Lineage Analysis}\label{sec:lineage} 

We can use our fingerprints to detect the lineage of trained models.
In \cref{fig:crossevalTI} we show cross-detection results for \TI{s}. 
We observe that SD 1.4 and MJ produce high-cross detection accuracy, thus MJ is likely to be a fine-tuned version of SD 1.4.
Indeed, while this is not public knowledge, we found evidence that our analysis is correct\footnote{\url{https://tokenizedhq.com/midjourney-model/}}.
In contrast, SD 2.1 does not retain such a relation with both SD 1.4 and MJ, therefore we can conclude that this model was trained from scratch. Indeed, this was confirmed by the SD 2.1 developers\footnote{\url{https://github.com/Stability-AI/stablediffusion}}. 

\input{figs/figures/cross_eval_t2i} 


\subsection{Robustness}\label{sec:robustness}
 
To test our method's robustness to image compression, we use two models: SD 1.4 and GLIDE, representing strong and weak fingerprints, respectively. We created four additional datasets for each model: images compressed using JPEG at quality levels of 75 (J75) and 50 (J50), resized (R) images, and blurred (B) images. Uncompressed images are denoted as U. The resized images were down-sampled to half of their original size and then up-sampled back to their original size using nearest-neighbor interpolation. We applied blur with a sigma value of 3, resulting in heavy smoothing.

We train DIF separately on each compression set for each model, following the procedure outlined in \cref{sec:detection}, using 256 training images each time. In addition to the compression sets, we also train the model on a mixed dataset where images are randomly compressed during the training procedure.
\input{figs/figures/compression}

The detection accuracy for each compression type in reported in \cref{fig:compressSD14,fig:compressGLIDE} for SD 1.4 and GLIDE, respectively. We observe that the blur significantly reduces the extraction and detection capabilities of the method because the \FI{} signal resides on higher frequencies of the image. In other cases, the accuracy is reduced, but this depends on the characteristics of the fingerprint. 

We hypothesize that the detection of compressed images is influenced by the original pattern of \FI{}. For example, we can observe different detection behaviors for resized images in both models. DIF trained on GLIDE's resized images can easily detect uncompressed images, but in the same scenario, where DIF is trained on images produced by SD 1.4, it is unable to detect uncompressed images. Additionally, we can barely observe symmetry in cross-detection accuracy per model. This is a clear sign of changes within \FI{} that result from compression.

The above is only a brief analysis of the effect of compression on \FI{}. It appears to be a complex topic that requires a more comprehensive investigation, which we leave for future work.
