\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv_rebuttal}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\usepackage{subcaption}
\usepackage[capitalise]{cleveref}
\usepackage{svg}
\usepackage{booktabs}
\usepackage{array}

% \usepackage[skip=10pt plus1pt]{parskip}


% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\iccvPaperID{11127} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\makeatletter
\newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother

\newcommand{\TI}{LTIM}
\newcommand{\ri}{RI}
\newcommand{\gi}{GI}
\newcommand{\gm}{GM}
\newcommand{\fft}{FFT}
\newcommand{\laion}{Laion-5B}
\newcommand{\dalT}{DE-2}
\newcommand{\dalM}{DE-M}
%\newcommand{\dalT}{Dall路E 2}
%\newcommand{\dalM}{Dall路E Mini}

% \setcounter{table}{0}
% \renewcommand{\tablename}{Panel} % Set the tablename to Panel, instead of Table
\renewcommand{\thetable}{\Alph{table}}
\renewcommand{\thefigure}{\Alph{figure}}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Deep Image Fingerprint: Accurate And Low Budget Synthetic Image Detector}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}

\def\ShowNotes{}
\def\ShowNewNotes{}
\input{macros}

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

We thank the reviewers for their time and expertise. We appreciate the recognition of the novelty (R2, R3) and interest (R1) in our work.  
We corrected grammatical errors (R2, R3, R4) and added information (R1) as suggested. 
We would like to re-emphasize the following:
%Additionally, we kindly request a reconsideration of the reviewers' verdicts based on the following points: 
(a) our method achieves comparable results to the SOTA while requiring an \emph{order of magnitude fewer samples} compared to existing data-driven and rule-based approaches and (b) our method enables model lineage analysis, which has not been addressed before.
Below we answer specific comments.
\vspace{-0.5em}
\paragraph{(R1) The reason why the technique is better than other fingerprint-based techniques is not very clear.}
The fingerprint extraction techniques, Marra18 and Joslin20, do not utilize CNNs. Tables 2 and 3 demonstrate the dominance of the proposed method over them and Figure 2 exemplifies the results of the proposed extraction ($\mathcal{F}_E$) compared to average of residuals $\mathcal{F}_A$.
\vspace{-0.5em}
% \paragraph{(R1) The training procedure is based on two different concepts, but the contribution of each of them is also not clear.}
% The core of the training procedure is solely based on a contrastive-like loss (Section 3.2). The ``Booster Loss'' also employs this concept but operates on the average of residuals per class instead of individual residual pairs.
% \ohad{Did this answer R1's question? I don't think so. Consider removing.}
% \vspace{-1.5em}
\paragraph{(R1) Moreover, the authors should consider several CNNs, not only U-Net (e.g. also a StyleGAN-like architecture).}
We initially considered GAN-like architectures but they performed similarly to or worse than the proposed U-Net. \Cref{tab:accuracyBoost} has further details\footnoteref{fn}.
\vspace{-0.5em}
\paragraph{(R1,R3) The comparison with the state-of-the-art is not complete: Grag21 and Wang20 are not fine-tuned on GAN images and their cross-detection is missing.}
Grag21 and Wang20 represent SOTA methods at the time of writing this manuscript. They were trained on the GAN images dataset 
% \ohad{which dataset? This is a bit confusing since we have several}
, and the results presented in Table 3 are obtained from its validation subset. Our objective was to demonstrate that our method, trained with a smaller number of samples, achieves comparable results to the SOTA on its benchmark task. 
% As these methods are already tailored for this dataset, fine-tuning is 
%not expected to significantly improve their performance. 
% unnecessary. \ohad{I changed this to ``unnecessary''. Is this accurate?}
To further evaluate their performance, we include a cross-detection evaluation for both methods in \Cref{fig:crossCompare}\footnote{\label{fn}DE-2 and DE-M denote Dall路E 2 and Dall路E Mini respectively.}. 
% The improvement for compressed images are areas that we plan to explore in future work.
% \ohad{Last sentence is not related to the question, and also doesn't add much. Do we need it?}
\input{figs/figures/cross_compare}
\vspace{-0.5em}
\paragraph{(R1) Extraction of fingerprints with DnCNN.}
DnCNN is utilized in the same manner for both the proposed method and the other methods. The training specifications of DnCNN, including the data used (images from LAION), are provided in Appendix, Section A.1.
\vspace{-0.5em}
\paragraph{(R1) Usage of Exponential Moving Averaging (EMA).}
EMA is used for stabilization. Due to the randomness of the U-Net input, its output will have slight variations, which may affect detection accuracy by up to a few percentage points.
\vspace{-0.5em}
\paragraph{(R1,R4) Sample Loss vs. Booster Loss.}
The Booster Loss operates solely on batch averages, while our training focuses on the classification of individual images, where the signal may be weaker. 
\Cref{tab:accuracyBoost}\footnoteref{fn}
compares the performance of the two losses.
%For a comprehensive comparison of the loss performance, please refer to .
\input{figs/tables/booter_comparison}
\paragraph{(R2) Do the results presented in Table 3 and 4 have booster loss and augmentations included?}
No, as we state in Section 3.2.1, lines 315-318, they are not utilized.
\vspace{-0.5em}
\paragraph{(R3) Comparison to [44].}
Two main differences between the proposed method and [44] are as follows: (a) The training sets used in [44] are two orders of magnitude larger than those used in our proposed method and (b) in contrast to [44], we do not make any assumptions regarding the distinctiveness of fingerprints generated by the generative models. Instead, we verify this aspect as part of our work and extend it to include model lineage analysis.
\vspace{-0.5em}
\paragraph{(R4) The working hypothesis regarding the exploitation of inductive biases in CNNs is generally reasonable, however, it is not property substantiated.}
We consider the strong connection between fingerprints and CNNs, as emphasized in [12, 21]. Building upon this notion and the main hypothesis of [41], we present a few simple experiments (Figure 1) to demonstrate our hypothesis.

{\small
\bibliographystyle{ieee}
% \bibliography{egbib}
}

\end{document}
