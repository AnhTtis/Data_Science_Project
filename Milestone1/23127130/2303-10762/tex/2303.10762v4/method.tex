\section{Method}\label{sec:method}

Our method extracts a CNN fingerprint of an image generator using a small number of generated images and applies it for the detection of other images produced by the same image generator. The method relies on the properties of CNNs, as explained below with a simple experiment~(\cref{sec:induce}). Then we'll explain the fingerprint extraction~(\cref{sec:train}) and following detection process~(\cref{sec:inference}), including implementation details~(\cref{sec:implement}).

\subsection{Deep Image Fingerprint}\label{sec:induce}

Deep Image Prior~\cite{art:dip} demonstrated that CNNs incorporate an image prior in their structure. Consider an image restoration task, where given a corrupt image $X \in \RE$ the goal is to obtain the restored image $\hat{Y} \in \RE$ from a trained model $h_{\theta}$:
\begin{equation}
   \hat{Y} = \min_{\theta} E(h_{\theta},X ) + P(X)
   \label{eq:genImage}
\end{equation}
$E$ is a data term and $P$ is the image prior.

For a CNN encoder-decoder model $g_{\theta}$, the architecture itself serves as the image prior.
% \new{, i.e., it \emph{resonates with semantic information}} \ohad{I do not understand what it means to ``resonate with semantic information''}. 
Thus, the optimization task is simplified, as the search is focused on finding only the data term $E$ within the weights ($\theta$) space of the CNN without the need of learning $P$. Here the input of $g_{\theta}$ is a random tensor $Z \in \mathbb{R}^{C \times H \times W}$, where each element is $z_i \sim U(0,1)$.
\begin{equation}
   \hat{Y} = \min \limits_{\theta} E(g_{\theta}(Z), X)
   \label{eq:dipOpt}
\end{equation}

After the optimization, the reconstructed image is given by: 
\begin{equation}
   \hat{Y} = g_{\hat{\theta}}(Z), \;\;\; \hat{\theta} = \argmin_{\theta} E(g_{\theta}(Z), X)
   \label{eq:dipOutput}
\end{equation} 
But, following a number of observations, 
% (e.g,~\cite{art:ganfingerprint, art:ganfreq ,art:ganfingerprintAE}),
\cref{eq:dipOutput} seems to be incomplete. Images produced by CNNs include a unique model fingerprint (\FI$\in \RE$), thus: 
\begin{equation}
   \hat{Y} + \mathcal{F}(g_{\hat{\theta}}) = g_{\hat{\theta}}(Z)
\end{equation}
To prove this statement, we perform a simple experiment: we optimize the weights of CNN ($\theta$) U-Net~\cite{art:unet} to produce a single gray image without any semantic content. It seems as a trivial task, yet after convergence, the model is still unable to reconstruct the image perfectly (\cref{fig:monochromeExp}). 
\input{figs/figures/blank_v2}


We reveal the artifacts by simple image normalization to range [0,1]. Two main fingerprint patterns are observed: up-sampling and boundary artifacts. Previous research has primarily focused on up-sampling artifacts, resulting from interpolation and kernel overlap \cite{art:checkerboard, art:ganfreqfix, art:ganfreq, inp:fingerprintNet}. In general, an up-sampling replicates signal in spectrum domain. To simulate only these artifacts, we optimize the Up-Net model: four blocks of 1x1~\cite{art:OxO} convolutional layers, to avoid padding, followed by deconvolutional layers with kernel size of 2 and stride 2. This produces a periodic pattern in image space and dots in the spectrum domain (\cref{fig:monochromeExp}).

Boundary artifacts cause a grid-like structure in spectrum domain. When applying \fft{} to an image, it assumes periodicity, but if image is not periodic, a ``cross'' artifact will appear as a result of the image's non-periodicity~\cite{art:priodicplus}. Our target gray image, which is periodic, does not exhibit this artifact, but the reconstruction does. This phenomenon is rooted in image padding and the mechanism of convolution layers, which are known to impact CNN performance \cite{art:mindpad,art:learningpad}. 
To simulate this, we optimize the C-Net model with eight convolutional layers, each having a kernel size of 3, a stride of 1, and a padding of 1. This configuration helps preserve the spatial dimensions of the input and induces artifacts.
Addition of up-sampling replicates ``cross'' structure resulting in grid structure in spectrum domain.

Consequently, in parallel to the Deep Image Prior~\cite{art:dip}, where a CNN's structure was shown to be an image prior, here we have shown that CNN's structure is also artifact prior. As such, we term our method \emph{deep image fingerprint} (DIF), and describe it next.

\subsection{Fingerprint Extraction}\label{sec:train}

We can extract fingerprints of a target model by an optimization of $\theta$ given a set of generated images and a set of arbitrary real images. The optimization is similar to the denoising procedure~\cite{art:dip}, where \FI{} is acquired by \cref{eq:dipOutput}, but instead of computing mean square error loss with respect to some image we compute correlation with respect to a set of image residuals. Residual $R_i \in \RE$ is defined as $R_i = f_{D}(X_i)$, where ($X_i$) is an image and \HP is a denoiser filter~\cite{art:ganfingerprint}.

The goal is to produce fingerprint that is highly correlated with residuals of generated images, and non-correlated with residuals of real images. Pearson Correlation Coefficient is proved to be a good correlation metric between image residual and model's fingerprint~\cite{art:ganfingerprint, art:ganfingerprintAE}. In practice we transform each input into their zero-mean and unit-norm versions, preform inner product and average values. This will be referred simply as \emph{correlation} and denoted as $\rho(\cdot, \cdot)$.

The loss function is formulated similarly to contrastive loss in siamese setting~\cite{art:contrastive}. We define a similarity factor $t_{ij} \in \{0,1\}$, which is equal to 1 when two residuals are from the same class and 0 otherwise and correlation distance ($D_{ij}$) as euclidean distance between correlation values:
\begin{equation}
   D_{ij} = \sqrt{(\rho(R_i, \mathcal{F}) - \rho(R_j, \mathcal{F}))^2}
   \label{eq:corrDist}
\end{equation}
Finally the sample loss function ($\mathcal{L_{S}}$) is summarised below:
\begin{equation}
   \mathcal{L_{S}} =\frac{ t_{ij} \cdot D_{ij} + (1 - t_{ij}) \cdot \left( m - D_{ij}
    \right)}{m}
    \label{eq:lossS}
\end{equation}
$m$ is a hyper parameter. 

\subsection{Detection of Generated Images}\label{sec:inference}

We perform detection through hypothesis testing. 
After generating the \FI{} from the trained model $(g_{\hat{\theta}})$, the model itself (and a GPU) is no longer necessary. 
Then, we compute the means of the populations of real and fake correlations according to \cref{eq:refVals}, denoted as $\mu_{\boldsymbol{r}}$ and $\mu_{\boldsymbol{g}}$, respectively. $N_{\boldsymbol{r}}$ and $N_{\boldsymbol{g}}$ represent the number of real and generated images in the training set. For each test image, we extract its residual $R_{\text{test}} = f_{D}(X_{\text{test}})$ and then perform hypothesis testing as follows: if $|\rho( R_{test}, \mathcal{F} ) - \mu_{\boldsymbol{g}}| < |\rho(R_{test}, \mathcal{F}) - \mu_{\boldsymbol{r}}|$, the tested image is considered generated, otherwise, it is considered real or not produced by the target model. This procedure does not require any parameters, such as threshold.
\begin{equation}
   \mu_{\boldsymbol{r}} = \frac{1}{N_{\boldsymbol{r}}} 
   \sum_{i \in \boldsymbol{r}} \rho(R_{i}, \mathcal{F})
   \;,\;\;
   \mu_{\boldsymbol{g}} = \frac{1}{N_{\boldsymbol{r}}} 
   \sum_{i \in \boldsymbol{g}} \rho(R_{i}, \mathcal{F})
    \label{eq:refVals}
\end{equation}

\subsection{Implementation Details}\label{sec:implement}

In all of our experiments (\cref{sec:experiments}), we train and test the method in a similar manner. Margin is constant $m=0.01$ and $z$ has 16 channels. \HP is a pre-trained DnCNN model~\cite{art:dncnn} that is trained separately on real images (\laion{} dataset \cite{misc:laion5b}) with sigma range of $[5,15]$ and crop size $48 \times 48$ pixels. During the extraction procedure, \HP{'s} weights are not updated. The extraction model is a U-net~\cite{art:unet}. Optimization was carried out using Adam~\cite{art:adam}, with a constant learning rate of $5e^{-4}$. During training, the fingerprint was accumulated using exponential moving averaging. We provide additional details about the selection of \HP{}, the training process of DnCNN, and the U-Net architecture in our supplementary materials.
