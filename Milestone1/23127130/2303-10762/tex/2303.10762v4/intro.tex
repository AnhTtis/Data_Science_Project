\section{Introduction}\label{sec:intro}

Generative neural networks enable the generation of high-quality images. While this has many benefits for scientific, creative, and business purposes, it can also be used for malicious deception. As image quality continues to improve, it becomes increasingly difficult for human observers to distinguish between real and fake images without careful observation and identification of inconsistencies, especially in spite of large text-to-image-models (\TI{s})~\cite{art:faridGeo, art:faridLight}. Therefore, there is an urgent need for an automated tool that can detect generated images. This work aims to provide such a method, which has demonstrated good performance on both novel and popular approaches.

The advancement of image generation has been made possible by the use of Deep Neural Networks (DNNs), particularly the Convolutional Neural Networks (CNNs) subclass. CNNs provide the best of both worlds by combining the image prior~\cite{art:dip} with the flexibility of DNNs~\cite{art:dnn}. This has led to the development of image generators, which are generative models trained to produce images given a sample from known distributions and can be conditioned on input. The most popular type of image generator families is the Generative Adversarial Network (GAN)~\cite{art:gan, art:dcgan}, which quickly gained popularity for its ability to produce high-quality and high-resolution images in various applications~\cite{art:biggan,art:progan,art:stylegan,art:stylegan2, art:cyclegan,art:stargan}. Previously these models have demonstrated state-of-the-art (SOTA) results in the field of image generation.

However, the advent of diffusion models~\cite{art:ddpm, art:ldm} introduced a new paradigm that has shown the ability to generate high-quality images surpassing those produced by GANs~\cite{art:ganvsddpm}. Diffusion models are also a type of generative models and can be easily conditioned on arbitrary inputs. This gave rise to a new type of image generators, the \TI{s}. These models incorporate advanced image generators (e.g.,~\cite{art:ddpm,art:vqgan, art:ldm}) and are capable of generating high-quality images from text captions combined with other input domains~\cite{art:parti, art:imagen, art:dalle2, proc:glide, art:blended, art:blendLDM, art:spa}. 
Despite the advancements and conceptual shifts, all the mentioned methods still depend on CNNs for image generation.

To detect generated images, there are typically two approaches: data-driven and rule-based. Data-driven methods~\cite{art:easycnn, art:easygan, art:universalGan} involve training models on large datasets of images, which are expected to generalize to unseen data. However, these methods may struggle to detect images from conceptually different generators, as shown in a later study~\cite{art:diffDetect}. Rule-based methods, on the other hand, rely on identifying common patterns or characteristics seen in generated images~\cite{art:ganfingerprint,art:ganfingerprintAE,art:Joslin2020, art:ganColorspaces, art:ganclues, art:graphgan, art:forencisCNN}, and typically require less data. However, most of these rules have only been demonstrated on a selected set of GAN models and image domains, which makes it necessary to re-evaluate the rules for novel types of image generators. This can be a laborious process as the rules are human-devised.

Despite significant progress in synthetic images detection, the widespread adoption of \TI{s} created new challenges in the field. Due to the resource-intensive nature of \TI{s}' training, access to \TI{s} and the amount of available generated images for research purposes is limited. As a result, data-driven approaches and some rule-based methods may not be practical, as they require a significant amount of data for training, and in some cases the training of an image generator itself. Therefore, new detectors are expected to not only perform well, but also to be trained on a small number of images.

Detecting images generated by ``personalized'' models poses an additional challenge. These models are fine-tuned versions of \TI{s} that are trained to generate images with specific objects or in particular styles~\cite{art:dreambooth,art:textual, art:break}. Despite their widespread use, the impact of personalization and fine-tuning on model fingerprints has not been studied.

The proposed method achieves high detection accuracy with minimal training data (less than 512 images), leveraging a CNN architecture to extract fingerprints of image generators and detect images from the same generator. Other detectors typically require hundreds of thousands of images.

Our method was tested on various image generators, including established GANs and \TI{s}. It outperforms competitors trained under the same conditions and achieves comparable performance to SOTA pre-trained detectors on widely used \TI{s} such as Stable Diffusion~\cite{art:ldm} and MidJourney~\cite{misc:midJ}, with significantly fewer required training samples. Moreover, our method proves valuable for model lineage analysis (\cref{sec:lineage}). However, it is important to note that our method is a proof-of-concept work and has certain limitations, which are discussed in~ \cref{sec:conclusion}.

Recently, a new study~\cite{art:jpeg} revealed that current detection methods are heavily biased on JPEG artifacts. Following this development, we further evaluate with unbiased JPEG data, and show solid performance on par with other methods.




