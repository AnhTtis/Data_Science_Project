

\section{Datasets}\label{apx:data}

\subsection{GAN Datasets}

The GAN datasets that we use in this work are from the supplementary materials of Wang~\etal~\cite{art:easycnn}. Train set consists of 360k real images corresponding to 20 LSUN classes~\cite{art:lsun} and 360k images generated by 20 ProGAN~\cite{art:progan} models. Models are trained per LSUN class. For the ProGAN dataset we randomly select 2,000 images per real and fake class, for each LSUN image class, resulting in 4,000 images per LSUN class overall. Test set of~\cite{art:easycnn} contains images generated by a number of GANs, specifically by StyleGAN~\cite{art:stylegan}, StyleGAN2~\cite{art:stylegan2}, BigGAN~\cite{art:biggan}, StarGAN~\cite{art:stargan}, GauGAN~\cite{art:gaugan} and ProGAN~\cite{art:progan}.

\subsection{Fined-Tuned Stable Diffusion Models}

We will outline the process of constructing datasets for the fine-tuned Stable Diffusion models discussed in \cref{sec:source}.
In all cases, fine-tuning involves the DreamBooth method~\cite{art:dreambooth}, specifically \TI{} is trained to reproduce a target object or style from a pre-defined keyword. There are three models: SD 1.4S, SD 1.5A, and SD 2.0R. The authors customly fine-tuned SD 1.4S with 10 photos. SD 1.5A\footnote{\niceurl{https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion}}
is a publicly available model that was fine-tuned in two stages: first, SD 1.4 was fine-tuned with 680k anime images, and then, after replacing the image decoder with one from SD 1.5, it was additionally fine-tuned with~\cite{art:dreambooth} on a different set of anime images. The last model, SD 2.0R\footnote{\niceurl{https://huggingface.co/nousr/robo-diffusion-2-base}}%
, is fine-tuned with an unspecified amount of robot images. Following above, keywords are known for each case, as SD 1.4S is trained by us, and for others, keyword is specified in the instructions for each model. To produce images with a new models a keyword is added to captions from the \laion{} dataset~\cite{misc:laion5b} in the following format: ``keyword caption''. This forces fine-tuned model to generate images with new information previously unknown to the source models. \cref{fig:sdExamples} shows an example of outputs resulting from the same caption.

\section{Additional Results}\label{apx:adresults}

\input{figs/figures/cross_eval_gans}%

\subsection{The Effect of Train Set Size}

In Section 4 
%\cref{sec:detection}
we present an evaluation of the proposed method as a detector of generated images. To evaluate the method under various amounts of training samples, we measure performance on images produced by both GAN and \TI{} models, as shown in \cref{fig:trainGAN,fig:trainT2I}. We observe that the accuracy remains stable across most of the models and starts to degrade drastically at $N_S = 128$, where we observe a 5\% drop for the majority of the models.

\input{figs/figures/gan_train_plot}


\subsection{Consistency for a Low Sample Amount}

We report the consistency of accuracy with a low number of samples. In this setting, each training sample has a greater impact on the results. To test this effect, we trained four models (\RES, \RESMG, \RESM, and DIF) ten times with 128 samples according to Section 4.2. Each time we randomly sampled the training set. As shown in \cref{fig:boxes}, despite the weaker consistency of the results for DIF, statistics support the relation observed in Table 2.

\input{figs/figures/box_plots}
\input{figs/figures/sd_examples}%

\subsection{Cross-Detection for GAN Models}

We report the complete map of cross-detection for datasets corresponding to GAN and ProGAN$_t$ models in \cref{fig:crossevalGAN,fig:crossevalPROGANA} respectively. As shown in the figures, the cross-correlation is low for all the models, thus models were trained separately.
\input{figs/figures/cross_eval_progans}%

\subsection{Cross-Detection for Custom Trained ProGANs}

Models $P_A$, $P_B$, $\hat{P}_A$ and $\hat{P}_B$ were trained on centrally cropped images ($128 \times 128$ px) from AFHQ~\cite{art:stargan2}. We report the full cross-detection matrix for the four models in \cref{fig:crossevalCUSTOMF}. The cross-detection across all the models is similar, namely no symmetry between different models and high-symmetry within converged models. In addition we present the examples of images generated by $P_A$ ordered according to check point epochs at \cref{fig:catExamples}. Image quality corresponds to the convergence state of the model. Starting from epoch 40 the model produces visually appealing results, that correspond to symmetric and high cross-detection accuracy observed in \cref{fig:crossevalCUSTOMF}. Cross-detection for $\hat{P}_A$ is symmetric, but with lower cross-detection accuracy values. We suspect that in this specific case, the model did not converge properly.


\subsection{Cross-Detection vs. Cross-Correlation}

In this section, we clarify why model lineage estimation is performed using image cross-detection rather than fingerprint cross-correlation. Cross-correlation (Section 3) is an intuitive process where we expect a correlation value of 1 for a fingerprint matched with itself, high absolute values for similar fingerprints, and low values for unrelated fingerprints. However, in our experiments (Section 4.3), we found that these values are not informative. The absolute cross-correlation values of the extracted fingerprints ($\mathcal{F}_E$) exhibit a random pattern (\cref{fig:crossevalCUSTOMFE}), while the fingerprints obtained through residual averaging ($\mathcal{F}_A$) display a trend similar to cross-detection (\cref{fig:crossevalCUSTOMFA}). However, the cross-detection of $\mathcal{F}_A$ decreases more rapidly with epochs, lacks normalization, and is always symmetric. As a result, the certainty of model relationships is diminished due to: a) increased sensitivity to changes during training/fine-tuning, b) absence of a symmetry parameter, and c) insufficient information from the correlation value alone.

\input{figs/figures/custom_gans_full}

\input{figs/figures/custom_gan_fingers}

