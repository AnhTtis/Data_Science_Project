
\section{Experiments}\label{sec:experiments}

This study includes a series of experiments that involve a varied collection of images generated by different \TI{s} and GANs. The datasets are summarized in \cref{sec:data}. The detection results are presented in \cref{sec:detection}, which is divided into two parts: the detection of images produced by \TI{s} and the detection of GAN-generated images. In \cref{sec:source}, we investigate the effect of image generator training and fine-tuning on its fingerprint, and in \cref{sec:lineage}, we analyze the relationship between selected \TI{s}. Finally, in \cref{sec:robustness}, we test the method on compressed images.

\subsection{Detection Data}\label{sec:data}

Our data includes generated images from a variety of \TI{} and GAN models. In contrast to \TI{s}, that produce multi-domain images, GANs are often limited to a specific image domain that they were trained on. Therefore, some of the GAN datasets include a number of image domains, each produced by a different model. \cref{tab:data} summarizes the data. The amount of real and generated images per set is equal. During the experiments we randomly split each dataset into equally sized train and test sets.
\input{figs/tables/data.tex}

Datasets representing \TI{s} were generated by us and is available online. We randomly selected real images and their corresponding captions from the \laion~\cite{misc:laion5b} dataset. Then, we used the captions to generate corresponding images using publicly available Stable Diffusion models~\cite{art:ldm}, versions 1.4 (SD 1.4) and 2.1 (SD 2.1), \dalM~\cite{misc:dallemini} and GLIDE~\cite{proc:glide}. In the case of \dalT~\cite{art:dalle2} we generated images with OpenAI's official API\footnote{
\niceurl{https://platform.openai.com/docs/guides/images}}. Lastly, a large set of generated images produced by MidJourney (MJ)~\cite{misc:midJ} is publicly available on the Kaggle website~\cite{misc:midJdata}, from which we select a random subset for this work. 

Datasets representing GAN models were obtained from the supplementary materials of Wang~et~al.~\cite{art:easycnn}. Our supplementary document contains a detailed review of the datasets.

\subsection{Detection of Generated Images}\label{sec:detection}

We evaluate DIF as a generated image detector and compare it to rule-based methods and data-driven methods. We summarize the results of detecting images produced by \TI{s} and GANs in \cref{tab:accuracyTI,tab:accuracyGAN}, respectively. To represent the number of training samples and the size of the pre-training datasets, we utilize $N_S$ and $N_D$ respectively.
\input{figs/tables/comparisonTI_v2}

Rule-based methods rely on fingerprint extraction. Marra18~\cite{art:ganfingerprint} is a conventional method for extracting \FI{} that involves averaging over residuals. Classification is then performed using the extracted \FI{} as described in \cref{sec:detection}. Joslin20~\cite{art:Joslin2020} follows a similar approach to Marra18, but correlation is conducted in the Fourier domain. For these two methods and DIF, we employ the same \HP{}~(\cref{sec:implement}), as with it they demonstrate superior performance. Further details can be found in the supplementary material. Ning18~\cite{art:ganfingerprintAE} involves extracting fingerprints from images using a trainable auto-encoder model and classifying the images with an additional CNN model.


We also compare to data-driven methods: pre-trained \RES~\cite{art:resnet} on ImageNet~\cite{art:imagenet}, \RESG~\cite{art:easycnn} is \RES, which is trained on ProGAN images, \RESMG~\cite{art:easygan} and \RESML~\cite{art:diffDetect} utilize modified \RES{} architecture (\RESM{}).

The methods \RESG{}, \RESMG{}, and \RESML{} have previously demonstrated SOTA results in the detection of images produced by GANs and LDMs~\cite{art:diffDetect}.  \RESG{}, \RESMG{}, and \RESML{} are trained on 720k images each, where half of the dataset consists of real images, while the other half is composed of images generated by ProGAN~\cite{art:progan}, StyleGAN~\cite{art:stylegan} and LDM~\cite{art:ldm}, respectively. Additionally, we performed fine-tuning on all the aforementioned models using $N_S$ images. In the case of fine-tuning, the models are frozen, and only the last fully-connected layer is reinitialized and updated during training.

According to the results in \cref{tab:accuracyTI}, the proposed method achieves remarkably high performance with a low number of training samples when applied to images generated by \TI{s}. DIF demonstrates higher accuracy compared to other rule-based methods and a higher mean accuracy compared to non pre-trained data-driven methods. Furthermore, when directly comparing DIF to the best fine-tuned method (\RESML{}), DIF performs equally well in the case of SD 1.4, MJ, and \dalM{}, and outperforms all non fine-tuned models on average, even with $N_S = 128$.


For GLIDE, SD 2.1 and \dalT{}, the detection accuracy of DIF is lower. \FI{} is formed by model architecture, weight initialization, and train data~(\cref{sec:related}). Refer to \cref{fig:examplesExt}. In contrast to SD 1.4, the $\mathcal{F}_A$ of GLIDE exhibits a barely visible grid pattern that gets lost within the accumulated noise. This makes it a ``weak'' fingerprint, characterized by lower energy. Similar ``weak'' fingerprint is also observed in SD 2.1. Despite sharing architecture with SD 1.4, SD 2.1 was trained on different dataset\footnote{\label{foot:sd} \niceurl{https://github.com/Stability-AI/stablediffusion}} and possibly with different weight initialization.
\input{figs/figures/ffts}%

Similar results are obtained when evaluating images generated by GANs, as shown in \cref{tab:accuracyGAN}. DIF surpasses all rule-based methods and performs comparably to the best pre-trained model (\RESMG{}) in four out of seven GAN models.
The lowest detection accuracy is observed for the ProGAN$_e$ dataset, which is expected. As mentioned in \cref{sec:related}, each trained model produces unique fingerprints, and DIF is specifically designed to detect images from a single model. For completeness, we also measure the detection accuracy for ProGAN$_t$, where DIF is trained on images of each model separately. In this case, the mean accuracy is 91.2\%, with the highest accuracy reaching 96.0\% and the lowest accuracy being 83.2\%. Similarly to the case of SD models, we observe that not only model architecture affects it's \FI{}. A comprehensive evaluation for ProGAN$_t$ is provided in our supplementary materials. 

Summarizing the experiments, DIF exhibits strong performance with both novel and well-established image generation methods. We have demonstrated that DIF outperforms other methods trained under the same conditions on average, in the detection of images generated by both \TI{}s and GANs. Additionally, DIF performs comparably to pre-trained SOTA methods (\RESMG{} and \RESML{}). It is worth noting that DIF achieves these results with just a few hundred images, while pre-training typically requires three orders of magnitude more images and additional fine-tuning. Therefore, we consider DIF to be a formidable competitor to existing methods.
\input{figs/tables/comparisonGAN}%

\subsection{Detection of Images From Fine-Tuned Models}\label{sec:source}

Now consider a more challenging setting, where given a trained DIF for some image generator we aim to detect images generated by its fine-tuned version. Due to uniqueness of fingerprints (\cref{sec:related}) images generated by these new model variations might not be detected by DIF trained on images produced by the original model.

To investigate the relation between source models and their variations we conduct an experiment using varied datasets and checkpoints. First, we train four ProGAN models ($P_{A}, P_{B}, \hat{P}_{A}$, and $\hat{P}_{B}$) for 70 epochs. $P_{A}$ and $P_{B}$ are trained on 2,500 ``cat'' class images from AFHQ~\cite{art:stylegan2} with random seeds A and B respectively. $\hat{P}_{A}$ and $\hat{P}_{B}$ are trained with the same random seeds, but on 2,500 ``wild'' class images from AFHQ. Next, for each model we use five checkpoints at epochs 20, 32, 40, 52 and 70, and generate 2,500 images for each. Finally, we construct 20 datasets by adding real images from the corresponding train set of ProGANs to each set of generated images. This results in 20 ProGAN models with 20 datasets.
\input{figs/figures/cats_cross_eval}%

We preform a cross-detection on $P_{A}, P_{B}, \hat{P}_{A}$ and $\hat{P}_{B}$ and for brevity summarize results for $P_{A}$ and $P_{B}$ in \cref{fig:crossevalCATS}. The full cross-detection map, including comparison to cross-correlation of fingerprints, is available in our supplementary materials. During cross-detection we attempt to classify images produced by some image generator with DIF which is trained on images from another image generator. Observe the symmetric relation within the same model: for checkpoints of epochs 40, 52 and 70 cross-detection accuracy is high and symmetric. Other relations include low accuracy and/or asymmetric, which is exactly what is expected for DIF trained on unique \FI{-s}. We may conclude that the model's \FI{} changes during training. However, as the model converges, these changes become insignificant, resulting in high cross-detection accuracy for DIF.

We conduct an additional experiment where we measure cross-detection on images from a number of fine-tuned/``personalized'' stable diffusion models~\cite{art:dreambooth}, which involve updates of image decoder weights. The models are: our custom fine-tuned SD 1.4 with a small set of images and two downloaded models of stable diffusion v1.5 and v2.0 fine-tuned on a large set of anime images\footnote{\niceurl{https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion}} and robot images\footnote{\niceurl{https://huggingface.co/nousr/robo-diffusion-2-base}}. Models denoted as SD 1.4S, SD 1.5A and SD 2.0R respectively. For each model we generated 1000 images from the same caption set that was used with SD 1.4, including style/object keywords specific to each model. Then train DIF for each with 512 real and 512 generated images (\cref{sec:detection}). Upon analyzing the results presented in \cref{fig:crossevalSD}, we can conclude that the relationships observed in previous experiments are maintained even when the model is fine-tuned with new data. Consequently DIF will also perform well on images generated by fine-tuned models.
\input{figs/figures/cross_eval_sd}%

\subsection{Model Lineage Analysis}\label{sec:lineage} 

We can use our extracted fingerprints to detect the lineage of trained models. In \cref{fig:crossevalTI} we show cross-detection results for \TI{s}. We observe that SD 1.4 and MJ produce high-cross detection accuracy, thus MJ is likely to be a fine-tuned version of SD 1.4. Indeed, while this is not public knowledge, we found evidence that our analysis is correct\footnote{\niceurl{https://tokenizedhq.com/midjourney-model/}}.

In contrast, SD 2.1 does not retain such a relation with both SD 1.4 and MJ, therefore we can conclude that this model was trained from scratch. Indeed, this was confirmed by the SD 2.1 developers\footnotemark[3].
\input{figs/figures/cross_eval_t2i} 


\subsection{Robustness}\label{sec:robustness}
 
To test our method's robustness to image compression, we use two models: SD 1.4 and GLIDE, representing strong and weak \FI{-s}, respectively. We created four additional datasets for each model: images compressed using JPEG at quality levels of 75 (J75) and 50 (J50), resized (R) images, and blurred (B) images. Uncompressed images are denoted as U. The resized images were down-sampled to half of their original size and then up-sampled back to their original size using nearest-neighbor interpolation. We applied blur with a sigma value of 3, resulting in heavy smoothing.

We train DIF separately on each compression set for each model, following the procedure outlined in \cref{sec:detection}, using 256 training images each time. To emulate a more realistic scenario in which the compression type is not known, we also train the model on a mixed dataset where images are randomly compressed during the training procedure.
\input{figs/figures/compression}

The detection accuracy for each compression type in reported in \cref{fig:compressSD14,fig:compressGLIDE} for SD 1.4 and GLIDE, respectively. We observe that the blur significantly reduces the extraction and detection capabilities of the method because the \FI{} signal resides on higher frequencies of the image. In other cases, the accuracy is reduced, but this depends on the characteristics of the fingerprint. 

We hypothesize that the detection of compressed images is influenced by the original pattern of \FI{}. For example, we can observe different detection behaviors for resized images in both models. DIF trained on GLIDE's resized images can easily detect uncompressed images, but in the same scenario, where DIF is trained on images produced by SD 1.4, it is unable to detect uncompressed images. Additionally, we can barely observe symmetry in cross-detection accuracy per model. This is a clear sign of changes within \FI{} that result from compression.

The above is only a brief analysis of the effect of compression on \FI{}. It appears to be a complex topic that requires a more comprehensive investigation, which we leave for future work.

As suggested by a recent study~\cite{art:jpeg}, we compare our method to others in a more challenging setting, where generated images are also compressed. We observe that the mean and median JPEG quality of the \laion{} dataset are 0.84 and 0.85, respectively. Therefore, we compress the generated images using the median quality factor of 0.85 and perform experiments from \cref{sec:detection} for DIF, \RESM{}, and \RESML{} with 1024 samples. From~\cref{tab:jpegTI}, we conclude that despite performance degradation, DIF remains competitive.
\RESML{} dominates in this comparison due to the compression applied as part of its training augmentations. However, it requires a large pre-training dataset.

\input{figs/tables/jpeg_ti}