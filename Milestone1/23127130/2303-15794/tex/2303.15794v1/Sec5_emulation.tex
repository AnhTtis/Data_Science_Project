
\section{From simulation to emulation}
\label{sec:emulation}

Numerical simulations have played
a vital role in establishing the basic picture of how the observed large-scale structure is formed (see \cite{Angulo22} for a recent review). 
The so-called standard cosmological model 
allows us to
generate accurate initial conditions that reproduce the statistical
properties of the early Universe.
Hence, the remaining task 
in our {\it forward modeling} is to implement
appropriate physical processes and to follow the structure formation and 
evolution within a sufficiently
large volume that represents at least a good fraction of the observable universe.
This fact grants cosmological simulations
to be particularly useful for making theoretical predictions.

Theoretical models are tested by performing detailed comparisons with a broad range of observations. Bayesian statistics is the fundamental framework to derive the region of model parameters and/or select a model that are consistent with observational data. However, due to the high computational cost, direct application of simulations to statistical inference problems is not straightforward.
Obviously one cannot generate a sufficiently large number
of realizations by performing structure formation simulations to incorporate in the standard statistical procedures such as Markov chain Monte Carlo (MCMC). 

Often, summary statistics are used instead of the observed matter/galaxy distributions, which are usually composed of multivariate random variables. We can significantly reduce the volume of the data vector without much loss of information by identifying appropriate summary statistics, making a fully statistical inference feasible. In this context, numerical simulations can be used to calibrate the response of the statistics to the model parameters. One popular methodology for performing this task is emulation. Costly numerical simulations are replaced with a cheaper but accurate statistical model, which can then be used directly in the inference process. In this section, we show how this idea has been implemented and employed in recent cosmological inference problems.

\subsection{Cosmological parameter estimation}
Cosmological parameters are accurately inferred from 
a variety of observations, including weak lensing \cite{Joudaki17,Hikage19,Asgari21,Secco22} and galaxy clustering \cite{2dFGRS05,WiggleZfinal,6dFGRSRSD,FMOSRSD,VipersRSD,SDSSDR12,SDSSDR16}. 
It is becoming increasingly more popular to incorporate
ML into some elements in the cosmological parameter inference procedure (e.g., \cite{Gupta18,Ribli19a,Ravanbakhsh16,Mathuriya18,Gillet19,Hassan19}).

In modern cosmology, the basic parameters such as the
expansion rate of the present-day Universe and the matter contents are inferred based on Bayes' theorem
\begin{equation}
    P ({\bm \theta} | {\bm d}) \propto  P ({\bm d} | {\bm \theta}) P ({\bm \theta}) 
\end{equation}
where the left-hand side gives the probability distribution
of the model parameter(s) ${\bm \theta}$ conditioned on
data ${\bm d}$ (the posterior). To perform the inference, one needs to know
$P ({\bm d} | {\bm \theta})$, the distribution of ${\bm d}$ for a model specified by ${\bm \theta}$ called likelihood, and the prior model distribution $P ({\bm \theta})$.
In many physics applications, forward modeling is possible using either analytic or perturbation calculations or by
direct numerical simulations. Unfortunately, evaluating
the likelihood $P ({\bm d} | {\bm \theta})$ is often 
extremely time consuming and impractical, because the
model variation is usually large; the parameter space ${\bm \theta}$ is multidimensional and the typical curse of dimension
kills essentially all brute-force attempts.


A partial remedy for the difficulty in large-scale structure cosmology is that an unbiased estimator for reasonable summary statistics, such as the two-point correlation function or its Fourier counterpart, the power spectrum, are known and understood well. Also the respective estimator follows a normal distribution with good accuracy in most of the cases (but see, e.g., \cite{Sato11,Simon15,Sellentin18} for potential violation of Gaussianity in case of weak lensing statistics). This nice feature follows from the central limit theorem, together with the fact that such estimators are often written by a combination of many random variables. What we need in practice are the expectation values of the statistics and the covariance matrices. Unfortunately, estimating the covariance matrix is still a challenging task to do in a direct manner using numerical simulations, even for a single cosmological model \cite{Takahashi09,Blot15}. It typically requires {\it at least} a few hundreds realizations of either fully nonlinear simulations or those with some approximations. Even with the advent of modern supercomputers, 
it is challenging to perform all necessary direct simulations at all the parameter sampling points. 
Interesting discussions are found on the consequences of the cosmology-independent covariance approximation for different observational probes \cite{Eifler09,Labatie12,Carron13,Kodwani19,HarnoisDeraps19}.

In this section, we introduce an efficient emulation-based approach to circumvent this problem based on a Gaussian likelihood with a cosmology-independent covariance matrix. 
We would like to mention here briefly that there has been an impressive progress in performing
parameter inference based on the comparison between numerical simulations and observations (see \cite{Cranmer20} for a review), or based on new formulations of the likelihood function in order to deal with the observed random fields without bypassing summary statistics. 
The former includes methods designed to avoid the computationally expensive evaluation of likelihood, commonly known as likelihood-free inference.
One notable example is Approximate Bayesian Computation \cite{Rubin84}, which has already been applied in
several cosmological studies \cite{Weyant13,Ishida15,Akeret15,Lin16,Hahn17}.
The latter approach is often called as field-level inference \cite{2020JCAP...01..029E,2022JCAP...08..003T,2022MNRAS.509.3194P,2022arXiv220413216S,2023MNRAS.520.5746A,2022JCAP...08..007B,2021JCAP...04..032S,2021JCAP...01..067C,2020JCAP...11..008S,2020JCAP...04..042C,2019A&A...621A..69R},
which has the potential to maximally extract the information contained in the observed universe, by exploring a huge parameter space that includes not only cosmological parameters but also a set of Fourier modes of the random initial condition, often aided by an efficient Hamiltonian Monte Carlo sampler (\cite{HMC} and see \cite{2010MNRAS.407...29J,2010MNRAS.409..355J,2013MNRAS.432..894J} for early attempts in cosmological large-scale structure), differential programming (\cite{10.1007/978-3-319-55696-3_3,JMLR:v18:17-468,10.5555/3327546,Innes2018,2019arXiv190707587I} and \cite{Modi21,2021A&C....3600490B,2022arXiv221109815L} for cosmological simulations), or optimization in place of sampling \cite{2017JCAP...12..009S,2018JCAP...07..043F,2021JCAP...10..056M,2021arXiv210412864M}. For both the methods, developing a fast simulator is a crucial task. ML techniques are also used for this purpose, by partly replacing the gravitational force calculation in $N$-body simulations \cite{2022arXiv220705509L} or by improving the accuracy by correcting for the mistakes in approximate dynamics or by super-resolution emulation \cite{2019PNAS..11613825H,2020MNRAS.495.4227K,2021MNRAS.507.1021N,2021PNAS..11822038L,2022ApJ...930..115K}. 

The emulator approach based on summary statistics discussed in what follows is relatively conservative in the sense that it is fully based on the traditional Bayesian inference pipeline with ML elements introduced only in the prediction of the expectation values of the summaries. Nevertheless it has already been shown that such approaches achieve improvements in constraining cosmological parameters compared to traditional methods. On top of this, the advances in the inference methodology, in particular the application of ML techniques, provide a promising avenue for a more efficient cosmology analysis.




\subsection{Emulation}

A practical way to bypass time-consuming numerical simulations in Bayesian inference problems is to replace it with an emulator. Here, we introduce emulators which predict summary statistics given a set of model parameters, instead of generating fully three-dimensional cosmological density fields. The outputs from an emulator serve as accurate templates in parameter inference problems.

Let us take the density fluctuation power spectrum as a frequently used summary
statistics in cosmology. 
Conventionally, functional fits were often used
\cite{Hamilton91,Peacock94,Smith03}.
Fitting formulae have been proposed using analytic expressions of a set of functions. Although such attempts are often motivated by physical considerations, they employ empirical analytic forms that are validated and with the internal parameters calibrated against numerical simulations. However, there is a tough demand for the accuracy to match those of modern and future cosmology surveys \cite{DETF,2022JHEAp..34...49A}. It is difficult to keep up with and update the model constantly, even if a larger set of simulation data is becoming available. Although recalibration of the model parameters can be performed fairly straightforwardly \cite{Takahashi12}, the complexity of the analytical form to be used eventually limits the best achievable accuracy. In addition, generalization of an existing fitting formula to allow more input cosmological parameters is a non-trivial task \cite{Mead15}.

The concept of an emulator is fundamentally different from 
that of conventional methods that utilize interpolation 
and/or extrapolation of summary statistics. An emulator is a statistical model that deals with a regression problem. It learns the relation between inputs and outputs obtained by simulations. The task of an emulator is to generalize the relation obtained at a finite number of sampling points to a new input selected at an arbitrary point in the input parameter space of interest. Modern emulators are based on Bayesian inference of physical quantities. It is often constructed in a non-parametric manner, with the complexity automatically calibrated in a data-driven way. This is the most remarkable feature in contrast to traditional functional fitting, for which one has to manually determine the functional form. 

The advantage of constructing emulators over other simulation-based inference models is that they are {\it amortizable} in the following sense. First, one can construct an emulator without actual observational data, with which statistical inference would eventually be performed. Therefore, the emulator construction task can be started \textit{before} an observational program is started, or before its details such as the depth, area, or full specification of the target galaxies are fixed. Once an emulator is developed, it can be used for different purposes, not only for statistical inference from the observational program originally in mind,
but also for understanding, for instance, which input parameter (or which combination of different parameters) is important. The result can be used, in turn, to optimize future observations. Unlike the emulator approach, direct applications of a simulation-based statistical model in inference is non-amortizable: the generated simulation data are only for the specific inference problem. The existence of unbiased estimators of standard statistical measures in large-scale structure cosmology makes emulation-based summary statistics templates useful and generic.

The idea of emulation was first introduced to the problem of cosmological structure formation by~\cite{Heitmann06,Habib07}. Over more than ten years since then, there are more than several research groups who advance the idea with different methods for different purposes. We will discuss the key ingredients for successful emulators and their applications to actual data from the recent literature.

\subsubsection{The design of experiments}
The first important step is efficient arrangement of simulations in the input parameter space. Often, one has to deal with a multiple regression problem in which the inputs have to be sampled in a high-dimensional parameter space. The design of experiments (DoE) itself is of common research interest in different domains of science and engineering (see \cite{DoE} for a review). This is a typical exploration-exploitation trade-off problem: uniform coverage over the possible domain in the input parameter space is desirable, on the one hand, while prioritizing the parameter regions where previous observations fit better could be more suited. In the case of emulator development, one usually places more weight on exploration and prepares a database which covers a wide parameter space such that the high posterior probability region from an unseen future observation would be included. Therefore, a uniform sampling scheme over the target parameter space is useful for this problem.

There are different indicators of good DoE. The first is the space-filling property. One usually wants to avoid placing a new input parameter set close to a point where a simulation has already been performed. Also, one does not have a large gap around a certain location of the parameter space over which simulation data do not exist. The second is the projection property. One can imagine a situation where one of the input parameters has little impact on the outcome of the experiment. In this case, the lower-dimensional subspace after projection of this unimportant parameter should be filled with desirable properties.
Having these two properties together makes a DoE suitable for ``black-box'' problems, where the relation between input and output can only be known after performing simulations. A popular choice is the Latin-Hypercube Design (LHD; \cite{McKay79} and see some examples in Figure~\ref{fig:LHD}). The good space-filling property is ensured by applying conditions such as orthogonality or the maximin distance condition\footnote{To maximize the minimum distance between all the pairs of design points. This ensures that the samples tend to become more distant and fill the whole space of interest evenly.}.
Another aspect of a DoE is granularity. A one-time DoE with good space-filling and projection properties could be difficult to expand by increasing the number of sampling points while maintaining the original good property. LHD is usually not great in this regard, as there is no natural way to expand.

\begin{figure}
    \centering
    \includegraphics[width=13cm]{Figures/LHD_v2.pdf}
    \caption{Comparison of DoEs with ten points in a two dimensional space. a) Random uniform distribution, b) -- f) Latin Hypercube Design (LHD), i.e., one and only one data points in every row and column. b) Diagonal, e) Random LHD, d) Maximin distance LHD, e) Maximin distance LHD, but with an anisotropic distance metric, f) Sliced maximin distance LHD \cite{Ba15} with two slices: Each slice depicted by different symbols. The whole data points, as well as those in the same slice, form an LHD.}
    \label{fig:LHD}
\end{figure}


\subsubsection{Regression models}

Gaussian Process Regression (GPR) builds a non-parametric model
based on available data. The valuable features are that a GPR returns 
the associated errors together with the central values \cite{Rasmussen06}.

The central assumption of GPR is that the output of the unknown function, $f(\boldsymbol{x})$, follows a normal distribution. One can imagine that $\boldsymbol{x}$ stands for cosmological parameters and $f$ is the matter power spectrum at some wavenumber $k$. We introduce the probability density functional of this function, $P[f(\boldsymbol{x})]$. The Gaussianity assumption implies that this functional is fully characterized by two functions, the mean function $\langle{f(\boldsymbol{x})}\rangle$ and the covariance function $C(\boldsymbol{x}_1,\boldsymbol{x}_2) = \left\langle{(f(\boldsymbol{x}_1)-\langle{f(\boldsymbol{x}_1)}\rangle)(f(\boldsymbol{x}_2)-\langle{f(\boldsymbol{x}_2)}\rangle)}\right\rangle$, where the operation $\langle\dots\rangle$ denotes the average. The covariance function is also commonly called the kernel function. 

A key point here is that, while the relation between the cosmological parameters and the power spectrum is deterministic in principle, one introduces a statistical argument that says, due to the lack of full knowledge of the function, or sufficiently dense samples of simulation data, we are not certain about the value of the power spectrum at given cosmological parameters. We can still introduce our prior knowledge by assuming certain functions for the mean and covariance. The common practice is that the mean function is set to zero, unless one has a good idea of the expected behavior of the function $f$. Then, the covariance function sets the typical amplitude and the flexibility of the matter power spectrum as a function of cosmological parameters. The idea of GPR is to shrink the uncertainty intervals by adding simulation data. This can be done naturally considering the conditional probability $P[f(\boldsymbol{x}_{N+1})|\boldsymbol{Y}_N]$ in the presence of data $\boldsymbol{Y}_N = \{y_i|i=1,\cdots,N\}$ obtained by simulations conducted at sampling points $\boldsymbol{X}_N = \{\boldsymbol{x}_i|i=1,\cdots,N\}$. One can choose $\boldsymbol{x}_{N+1}$ at the new input point at which one would like to make a prediction in the presence of $N$ data points. Thanks to Gaussianity, the mean and variance of this conditional probability can be computed analytically: 
\begin{eqnarray}
\langle y^{(N+1)}|\boldsymbol{Y}_N\rangle =  \boldsymbol{k}^T\boldsymbol{C}_N^{-1}\boldsymbol{Y}_N,
\label{eq:GP_mean}
\end{eqnarray}
and
\begin{eqnarray}
\langle (\Delta y^{(N+1)})^2|\boldsymbol{Y}_N\rangle =
\kappa - \boldsymbol{k}^T\boldsymbol{C}_N^{-1}\boldsymbol{k},
\label{eq:GP_var}
\end{eqnarray}
respectively, where we have introduced an $N$-by-$N$ matrix $\boldsymbol{C}_{N} = \{C(\boldsymbol{x}_i,\boldsymbol{x}_j) | 1 \leq i, j \leq N\}$, which gives the covariance between the existing input points, an $N$-element vector $\boldsymbol{k} = \{C(\boldsymbol{x}_i,\boldsymbol{x}_{N+1}) | 1 \leq i \leq N\}$ between the existing and the new input, and the variance at the new input $\kappa = C(\boldsymbol{x}_{N+1},\boldsymbol{x}_{N+1})$.
It is easy to see in Eqs.~(\ref{eq:GP_mean}) and (\ref{eq:GP_var}) that without any simulation data, the mean is zero and the variance is $\kappa$ by construction. The presence of simulation data, $\boldsymbol{Y}_N$, shifts the mean and shrinks the variance according to these equations. All of these computations are written as matrix products and thus are easily evaluated. A possible bottleneck of this model is the inversion of the matrix for $\boldsymbol{C}_N^{-1}$, which can be expensive when the size of the data $N$ increases.

The remaining task for us is to specify the covariance function $C(\boldsymbol{x}_1,\boldsymbol{x}_2)$. One can design this function according to the prior knowledge of the target function $f(\boldsymbol{x})$: If one expects that this function should exhibit periodicity, for instance, one may try a periodic function for $C(\boldsymbol{x}_1,\boldsymbol{x}_2)$. A common practice, without any prior knowledge on $f(\boldsymbol{x})$ is to consider a \textit{stationary} kernel, that is, $C(\boldsymbol{x}_1,\boldsymbol{x}_2)=C(|\boldsymbol{x}_2-\boldsymbol{x}_1|)$, solely determined by the distance between the two inputs. This greatly reduces the number of free parameters that characterize the covariance function. Often, a simple function is employed, such as a Gaussian-form radial basis function, an exponential, or the Mat\'ern kernel. The free parameters for these kernels determine the amplitude and the distance metric in the multidimensional input space. A unique aspect of GP is that one can ``learn'' this function from the data $\boldsymbol{Y}_N$, by introducing free parameters in $C(\boldsymbol{x}_1,\boldsymbol{x}_2)$ and maximize the probability of having $\boldsymbol{Y}_N$. In this sense, this rather traditional Bayesian framework can be regarded as ML. Each step of GP application to a simple example problem can be found in Figure~\ref{fig:GP}.

\begin{figure}
    \centering
    \includegraphics[width=13cm]{Figures/gp_2d_v2.pdf}
    \caption{Example application of GP to a two inputs-one output regression problem. The task is to infer the 2D surface in the upper left panel from the 40 data points with random noise in the lower left panel. The two middle panels depict random draws from GP assuming the Mat\'ern 32 kernel function, before (upper) or after (lower) the parameter optimization on the data. One can see that the smoothness and the typical level of fluctuations of the surface is closer to the ground truth after the optimization. The right panels show the expectation value of the Gaussian probability density corresponding to the middle panels, now conditioned on the data points in the lower left panel. A GP with a fixed kernel function works as the prior and the final outcome can be regarded as the posterior in the standard Bayesian method.}
    \label{fig:GP}
\end{figure}


The possible drawback of a GP is that multiple output regression is not straightforward: We have focused on the simplest case of single output functions, $f(\boldsymbol{x})$. Extensions for correlated multi-output problems are usually not straightforward. In the typical applications to cosmology, such as the power spectrum emulation, it is natural to consider a multiple-output model and predict the function values at various wavenumbers when a set of cosmological parameters are specified as the input. Since the size of the full covariance function between different input-output combinations can be prohibitively large, one has to find an efficient way to decompose it or connect the final output to intermediate variables generated by a smaller model in the latent space \footnote{Multiple output Gaussian process is implemented for instance in a public PYTHON package, \textsc{Gpy} (https://sheffieldml.github.io/GPy/).}. To avoid the complexity of multiple output regression, a popular workaround is to reduce the dimensionality of the outputs by methods such as singular value decomposition or principle component analysis, to convert to a smaller number of less-correlated data vector. After this operation, each of the vector element is modeled by an independent GP, ignoring the cross correlation. Alternatives to GP in this context include fully-connected feedforward neural networks (FFNN) and the Polynomial Chaos Expansion \cite{10.2307/j.ctv7h0skv}\footnote{Polynomial Chaos Expansion (PCE) is an efficient approximator of a target (multivariate) function by using a finite set of polynomials which form an orthogonal basis with respect to the distribution of the input variables.}. Since the dependence of summary statistics on the cosmological parameters is typically smooth and only weakly nonlinear, unlike the underlying stochastic field realizations, an FFNN with rather smaller number of hidden layers or low-order polynomial expansion are known to work accurately. The optimal regression model should depend on the precise definition of the problem. Nevertheless, a thorough comparison among different models is still missing and would be useful to prepare for the next generation observational programs (see, e.g.,~\cite{Schneider11,Angulo21,Cuesta-Lazaro22} for studies along this line).


\subsection{Cosmological emulators and application to real data analysis}

\begin{table}
\caption{Structure formation simulation campaigns aimed at cosmological emulators. We summarize the cosmological parameter space (flatness assumed in all the cases), the design of experiments (acronyms: LH = Latin Hypercube, Mm = Maximin distance), the statistical model used for regression (FIT = function fitting, PC = Principal components; we do not distinguish Singular Value Decomposition from PC analysis here, GP = Gaussian Process, NN = Neural Network, PCE = Polynomial Chaos Expansion, LIN = first-order Taylor expansion around the fiducial model), the target statistics of regression, the emulated statistics, and the relevant papers, including both descriptions of simulations and regression modeling.\label{tab:emulator}}
\centering
\resizebox{\textwidth}{!}{%}
\begin{threeparttable}
 \begin{tabular}{c ||c c c c c} 
  & Cosmology & \multirow{2}{*}{DoE} & Regression & \multirow{2}{*}{Statistics} & \multirow{2}{*}{Reference}\\ 
  & (parameters) &  & model &  & \\ 
 \hline\hline
 \multirow{2}{*}{Coyote Universe} & $w$CDM & \multirow{2}{*}{symmetric LH} & \multirow{2}{*}{PC|GP} & $P_\mathrm{m}(k,z|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Heitmann10,Heitmann09,Lawrence10,Heitmann14}\\ 
  & $(\omega_\mathrm{m}, \omega_\mathrm{b}, n_\mathrm{s}, w, \sigma_8)$ & &  & $c(M|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Kwan13}\\
 \hline
 \multirow{2}{*}{PkANN} & $\nu w$CDM & \multirow{2}{*}{LH\tnote{a}} & \multirow{2}{*}{NN} & \multirow{2}{*}{$P_\mathrm{m}(k | \boldsymbol{\theta}_\mathrm{cosmo}, z)$} & \multirow{2}{*}{\cite{Agarwal12,Agarwal14}}\\
 & $(\omega_\mathrm{m}, \omega_\mathrm{b},n_\mathrm{s},w_0,\sigma_8,\sum m_\nu)$ & & & & \\
 \hline
 \multirow{2}{*}{Mira-Titan Universe} & $\nu w_0w_a$CDM & \multirow{2}{*}{nested lattice}  & \multirow{2}{*}{PC|GP} & $P_\mathrm{m}(k,z|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Heitmann16,Lawrence17,Moran22}\\
 & $(\omega_\mathrm{m}, \omega_\mathrm{b}, \sigma_8, h, n_\mathrm{s}, w_0, w_a, \omega_\nu)$ & & & $n_\mathrm{h}(M|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Bocquet20}\\
 \hline
 \multirow{6}{*}{MassiveNuS} & \multirow{5}{*}{$\nu\Lambda$CDM} & \multirow{6}{*}{LH\tnote{b}} & -- & -- & \cite{Liu18}\\
& \multirow{5}{*}{$(\sum m_\nu, \Omega_\mathrm{m}, A_\mathrm{s})$} &  & \multirow{5}{*}{GP} & $C_\ell^{(\kappa)}(\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Liu19,ZackLi19} \\
&  &  &  & PDF$^{(\kappa)}(\kappa|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Liu19} \\
&  &  & & $b^{(\kappa)}_{\ell_1,\ell_2,\ell_3}(\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Coulton19} \\
&  &  & & $N_\mathrm{peak}^{(\kappa)}(\mathrm{S/N}|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{ZackLi19}\\
&  &  & & $V_i^{(\kappa)}(\nu|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Marques19}\\
 \hline
 \multirow{4}{*}{cosmo-SLICS} & \multirow{3}{*}{$w$CDM} & \multirow{4}{*}{MmLH} & \multirow{4}{*}{PC|GP} & $\xi_{\pm}(\theta|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{HarnoisDeraps19}\\
 & \multirow{3}{*}{$(\Omega_\mathrm{m}, \sigma_8, h, w_0)$} & & & $N_\mathrm{peak}^{(\kappa)}(\mathrm{S/N}|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{HarnoisDeraps21,Davies22}\\
 &  & & & PDF$^{(\kappa)}(\mathrm{S/N}|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Martinet21}\\
 &  & & & $\xi_\mathrm{peak}^{(\kappa)}(\theta|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Davies22}\\ 
 \hline
 \multirow{4}{*}{EuclidEmulator} & $w$CDM & \multirow{4}{*}{MmLH} & \multirow{4}{*}{PC|PCE} & \multirow{4}{*}{$P_\mathrm{m}(k,z|\boldsymbol{\theta}_\mathrm{cosmo})$} & \multirow{2}{*}{\cite{EuclidEmu1}}\\ 
  & $(\omega_\mathrm{b}, \omega_\mathrm{m}, h, n_\mathrm{s}, w_0, \sigma_8)$ &  &  &  & \\ 
 & $\nu w_0w_a$CDM &  &  &  & \multirow{2}{*}{\cite{EuclidEmu2}}\\ 
  & $(\Omega_\mathrm{b}, \Omega_\mathrm{m}, \sum m_\nu, n_\mathrm{s}, h, w_0, w_a, A_\mathrm{s})$ &  &  &  & \\ 
  \hline
 \multirow{6}{*}{Aemulus} & \multirow{5}{*}{$w$CDM+$N_\mathrm{eff}$} & \multirow{6}{*}{LH\tnote{c,d}} & -- & -- & \cite{DeRose19}\\ 
  & \multirow{5}{*}{$(\omega_\mathrm{b},\omega_\mathrm{c},w_0,n_\mathrm{s},\ln(10^{10}A_\mathrm{s}),h,N_\mathrm{eff})$} &  & FIT|GP & $n_\mathrm{h}(M,z|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{McClintock19}\\  
  &  &  & GP & $w_\mathrm{p}(r_\mathrm{p}|\boldsymbol{\theta}_\mathrm{cosmo},\boldsymbol{\theta}_\mathrm{galaxy})$ & \cite{Zhai19,Zhai22}\\  
  &  &  & GP & $\xi_{0,2}(s|\boldsymbol{\theta}_\mathrm{cosmo},\boldsymbol{\theta}_\mathrm{galaxy})$ & \cite{Zhai19,Zhai22}\\  
  &  &  & FIT|GP & $b(M,z|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{McClintock19b}\\  
  &  &  & PC|PCE & $P_{ij}(k|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Kokron21}\\  
 \hline
\multirow{3}{*}{AbacusCosmos} & \multirow{2}{*}{$w$CDM} & \multirow{3}{*}{MmLH\tnote{d}} & -- & -- & \cite{Garrison18}\\
 & \multirow{2}{*}{$(\omega_\mathrm{b},\omega_\mathrm{m},h,n_\mathrm{s},\sigma_8,w_0)$} & & \multirow{2}{*}{GP} &  $\Delta\Sigma(r_\mathrm{p}|\boldsymbol{\theta}_\mathrm{galaxy},\boldsymbol{\theta}_\mathrm{galaxy})$ & \multirow{2}{*}{\cite{Wibking20}}\\
& & & & $w_\mathrm{p}(r_\mathrm{p}|\boldsymbol{\theta}_\mathrm{galaxy},\boldsymbol{\theta}_\mathrm{galaxy})$ & \\
\hline
 \multirow{3}{*}{AbacusSummit} & \multirow{2}{*}{$w_0w_a$CDM$+N_\mathrm{eff}$+running} & \multirow{2}{*}{ellipsoidal surface\tnote{e}} & -- & -- & \cite{Maksimova21}\\
& \multirow{2}{*}{$(\omega_\mathrm{b},\omega_\mathrm{c},n_\mathrm{s},\sigma_8,w_0,w_a,\alpha_\mathrm{s},N_\mathrm{eff})$} & \multirow{2}{*}{+ uniform random} & LIN & $P_{ij}(k|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Hadzhiyska21}\\ 
&  &  & GP & $\xi_\mathrm{g}(r_\mathrm{p},\pi|\boldsymbol{\theta}_\mathrm{cosmo},\boldsymbol{\theta}_\mathrm{galaxy})$ & \cite{Yuan22}\\ 
 \hline
 \multirow{2}{*}{BACCO} & $\nu w_0w_a$CDM & \multirow{2}{*}{LH\tnote{f}} & PC|GP or NN & $P_\mathrm{m}(k,z|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Angulo21}\\ 
  & $(\sigma_8,\Omega_\mathrm{m},\Omega_\mathrm{b},n_\mathrm{s},h,\sum m_\nu,w_0,w_a)$ & & NN & $P_{ij}(k|\boldsymbol{\theta}_\mathrm{cosmo})$ & \cite{Zennaro21}\\ 
 \hline
 \multirow{4}{*}{Dark Quest} & \multirow{3}{*}{$w$CDM} & \multirow{4}{*}{sliced MmLH} & \multirow{2}{*}{PC|GP or NN} & $\xi_\mathrm{hm}(x,z,M|\boldsymbol{\theta}_\mathrm{cosmo})$ & \multirow{3}{*}{\cite{Nishimichi19,Cuesta-Lazaro22}}\\ 
  & \multirow{3}{*}{$(\omega_\mathrm{b}, \omega_\mathrm{c}, \Omega_\mathrm{de}, \ln(10^{10}A_\mathrm{s}), n_\mathrm{s}, w)$} & &  & $\xi_\mathrm{hh}(x,z,M_1,M_2|\boldsymbol{\theta}_\mathrm{cosmo})$ & \\
   &  &  & FIT|PC|GP & $n_\mathrm{h}(z,M|\boldsymbol{\theta}_\mathrm{cosmo})$ & \\
 &  && NN & $P_\mathrm{h}^{(\mathrm{S})}(k,\mu|\boldsymbol{\theta}_\mathrm{cosmo},z,M_1,M_2)$ & \cite{Kobayashi20}
\end{tabular}
\begin{tablenotes}
\item[a] Near orthogonal design.
\item[b] Coulomb-like potential minimized. Each input variable then transformed to a normal or half-normal distribution.
\item[c] The sum of the distance from every point to the closest point for all two-dimensional projections is maximized.
\item[d] Axes of the hyperrectangle are rotated and rescaled to the eigenvectors and the eigenvalues of the posterior from previous CMB experiments.
\item[e] 4, 6 and the full 8D (sub)spaces each filled with points, glass configuration achieved by applying electrostatic repulsive force, antipodes discarded.
\item[f] Sample size gradually increased based on the uncertainties estimated by GP.
\end{tablenotes}
\end{threeparttable}
}
\end{table}

Emulation is now a popular approach for quick evaluation of various statistical properties of the large-scale structure of the universe. Table~\ref{tab:emulator} summarizes recent large-scale simulation campaigns for emulator building. The state-of-the-art emulators can predict not only the matter power spectrum, $P_\mathrm{m}(k)$ (or its projected version, $C_\ell$), the most fundamental quantity that characterizes the cosmological density field, but also many other quantities relevant to weak lensing observations
or those for \textit{galaxy} clustering observables. In Table 1, the quantities with superscript $\kappa$ are for the lensing convergence field\footnote{Here, convergence means the change in the observed size of an object caused by gravitational lensing effect. It is defined as the trace of the Jacobian between the unlensed and the lensed images.} in the ``Statistics'' column.
To be more precise, the cosmo-SLICS group studies the statistics of the aperture mass, which is closer to the actual observable.

For weak lensing, several non-Gaussian statistics are used to explore the information content beyond the power spectrum, including the bispectrum ($b_{\ell_1,\ell_2,\ell_3}$), the peak counts ($N_\mathrm{peak}$), the probability density function or the Minkowski functionals ($V_i$, $i=0,1,2$ for 2D lensing maps) (see \cite{Zurcher21} for a recent comparison among different statistics). For galaxy surveys, one needs to interpolate not only in the cosmological parameter space ($\boldsymbol{\theta}_\mathrm{cosmo}$), but also over the parameters specifying the properties of the galaxy sample of interest ($\boldsymbol{\theta}_\mathrm{galaxy}$), which makes the problem size even larger. To bypass this, one can rely on the so-called halo model approach (see \cite{Cooray02} for a review) while emulating the abundance and the clustering properties of dark matter halos as a function of their mass \cite{Nishimichi19,Kobayashi20}. One can also resort to perturbative bias expansion methods, such as the one in Lagrangian space \cite{Modi20}, for which various power spectra ($P_{ij}$ with $i$ and $j$ corresponding to different ``operators'') can be measured from simulation
outputs and then emulated~\cite{Kokron21,Hadzhiyska21,Zennaro21}. Some of the large simulation databases with different cosmological models, such as QUIJOTE~\cite{Paco20} and AbacusSummit~\cite{Maksimova21} are now publicly available.

We list large-scale cosmological simulation projects for emulators that interpolate summary statistics measured from simulations over the cosmological parameter space in Table~\ref{tab:emulator}. We note also that there are many other attempts to utilize emulators for different purposes, such as developing fast Boltzmann equation solvers or performing low-order perturbative calculations~\cite{Fendt07,Auld07,Auld08,Fendt09,Mootoovaloo20,Arico21b,DonaldMcCann22,DonaldMcCann22b,SpurioMancini22,Bonici22,DeRose22,Mootoovaloo22,Nygaard22,Eggemeier22}, to explore the galaxy-halo connection for fixed cosmology \cite{Kwan15}, to translate less costly, low-resolution simulations to mimic more expensive simulations. More specifically, the applications include incorporating baryonic effects to the dark-matter only simulations~\cite{Arico21}, predicting Lyman-$\alpha$ forest~\cite{Bird19,Pedersen21} or $21$-cm power spectra~\cite{Kern17,Schmit18}, extrapolating the predictions in $\Lambda$-Cold Dark Matter (CDM) cosmology
to alternative cosmological models~\cite{Giblin19}, or improving the spatial resolution of simulations~\cite{KodiRamanah20}. 
There are also promising approaches of mixed high- and low-resolution simulations as training data for summary-statistics emulators \cite{Ho22}. 
Emulators can be integrated as an essential part in Bayesian optimization by iteratively adding simulations or forward models in general to maximize the acquisition function, which quantifies the likelihood and uncertainties of the surrogate model~\cite{Leclercq18,PellejeroIbanez20,Boruah22,Neveux22}. 


Recent studies successfully integrate emulators in cosmological parameter inference. 
A model prediction tool of Ref. \cite{Eifler11} based on the Coyote Universe emulator is used to calculate the cosmic-shear two-point correlation functions in the analysis of the SDSS weak lensing maps \cite{Huff14}.
Similarly, lensing peak counts and the power spectrum
are adopted in the analysis of CFHTLenS survey \cite{Liu15}, whereas the moments and Minkowski functionals are used in Ref. \cite{Petri15}.
Emulator-based peak counts are used for the analyses of DES Y1 \cite{HarnoisDeraps21} and DES Y3 data \cite{Zurcher22}.
Joint analyses 
have been performed using the angular clustering of SDSS galaxies and galaxy-galaxy lensing signal from Subaru HSC 
 \cite{Miyatake22}, and using large- and small-scale clustering of SDSS galaxies in redshift space \cite{Kobayashi22,Zhai22,Yuan22}. Modern statistical inference
often involves dense parameter sampling in a Monte Carlo manner, 
which can be performed only if the comparison with observed data
and various model predictions can be made fast enough.
In this sense emulators are indispensable tools for 
parameter inference in the big data era.



In the future, it would be ideal to adopt some kind of automated 
learning and parameter exploration.
There are a few important quantities that the next generation cosmology surveys
will be able to measure, such as the total mass of neutrinos, non-Gaussianities in the primordial fluctuations, spatial curvature and the dark energy
equation of state (e.g., \cite{DESI16,PFS14,Euclid13,Spergel15,Ivezic19,2014arXiv1412.4872D}).
The utilization of new statistical methods would significantly enhance the performance of these programs.

In ML applications, utilizing simulations as training data poses a current limitation in that there still remains a dearth of accurate and realistic mock observations of sufficient size. Recent studies  attempted to address this issue either by performing fast simulations at the expense of accuracy or by configuring smaller simulation volumes compared to the current or future observational programs, in order to generate a large number of mock realizations for proof-of-concept. However, even with a good prospect for accumulating large simulation datasets using future computational facilities, uncertainties surrounding the numerical implementation of non-gravitational effects persist. This is largely owing to both our limited understanding of the relevant astrophysical processes and the vast disparity in the physical length scale of galaxy formation and the observed volume. Therefore, addressing the challenges will require the development of efficient statistical approaches by introducing nuisance parameters and marginalizing them in the statistical inference, or by identification of robust summary statistics that are less susceptible to these uncertainties, to ensure reliable extraction of cosmological information.
