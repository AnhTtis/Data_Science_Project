
\section{The large-scale distribution of galaxies and dark matter}
\label{sec:LSS}

Virtually all the ongoing and future cosmology surveys are aimed at probing the large-scale distribution of matter and galaxies in various manners.
In this section, we introduce three major methods and the related 
ML applications. There are different kinds of needs and technical issues in ML for the different observational methods. 

Generative models are becoming increasingly popular in many research areas from image analysis to natural language processing.
In cosmology, applications can be found, for instance, in generating fine images of galaxies and in
reconstruction of large-scale dark matter distribution. 
In this section, we introduce recent development and discuss promising uses of generative models, in addition to ML parameter inference techniques.

\subsection{Dark matter distribution probed with gravitational lensing}

Weak lensing (WL) has been established as an essential
cosmological probe of matter distribution in and around galaxies
and galaxy clusters \cite{Mandelbaum18}. Conventionally, summary statistics of lensing shear and convergence fields
are used to characterize the matter distribution and to infer cosmological parameters such as matter density $\Omega_{\rm m}$ and the density fluctuation amplitude $\sigma_8$ (see Section 5 in this review). 
There have been several proposals to use reconstructed two-dimensional \cite{Lu22} or three-dimensional density fields \cite{Lazanu21} to
study baryonic effects as well as to determine cosmological parameters from density distribution features that are not well captured by conventional summary statistics \cite{Ribli19a}. 

Convolutional neural networks (CNNs) are used for cosmological regression analysis using noisy convergence maps \cite{Fluri18,Ribli19b}. 
DeepMass of Ref.~\cite{Jeffrey20} is based on the U-Net architecture
that is originally developed for biomedical image segmentation
\cite{Ronneberger15}. The U-Net structure allows
to learn many features from a large training data set.
It is applied to reconstruct the
mass distribution from mock Dark Energy Survey science verification data,
to show better performance than Wiener filtering in terms of mean square error. 
Several studies show that CNNs outperform conventional parameter inference methods that utilize low-order summary statistics such as convergence power spectrum and two-point correlation functions \cite{Gupta18}.
CNNs are considered to have a particularly powerful
ability of capturing complex features in two-dimensional images, beyond simple summary statistics, 
and thus are able to distinguish subtle differences in 
the matter distributions in different cosmological models.
Saliency methods \cite{Simonyan13_saliency}, which evaluates the importance of individual input pixels, are used to interpret the function of neural networks \cite{Matilla20}.

CosmoGAN has been developed to generate weak lensing convergence maps \cite{Mustafa19}. It is based on generative adversarial networks (GANs)
that are trained with outputs from a set of cosmological simulations. 
The generated images (lensing convergence fields) resemble those
made directly from $N$-body simulations
(see Figure \ref{fig:CosmoGAN}), 
and also reproduce an 
array of statistics from 1-point distribution function to Minkowski functionals with reasonable accuracy. 
Properly trained generative models have a potential to be replaced with costly direct simulations for well-defined purposes, but
it is worth noting that GAN models need to be trained with a vast
number of "true" samples, either from direct numerical simulations or
from observation. This has been possible so far for one or a few,
limited cosmological models, but the overall computational cost 
is still extremely large if one aims at developing a "universal" GAN 
that is capable of generating cosmological density fields for a broad range of theoretical models.

\begin{figure}
    \centering
    \includegraphics[width=12cm]{Figures/CosmoGAN.pdf}
    \caption{Examples of weak lensing convergence maps generated from
    outputs of cosmological $N$-body simulations (top) and those generated by CosmoGAN (bottom). Each panel shows the projected density distribution in about a square degree field. Dark parts are high density regions. 
    The density distributions (maps) in the top panels are generated from ray-tracing simulations of gravitational lensing
    using an array of outputs from direct
    $N$-body simulations. We show here three realizations of such simulations. CosmoGAN learns the
    features of the cosmological density distribution from a number of the simulated maps, and becomes able to generate images (bottom panels) 
    that resemble the "true" density distribution.
    }
    \label{fig:CosmoGAN}
\end{figure} 
 
Standard techniques of density field reconstruction with WL yield noise dominated results. There are 
different kinds of noise associated with
real observations, and one of the important and intrinsic noise source
is the number density distribution of background galaxies. 
WL signals are measured
by detecting small coherent image distortion
of over tens to hundreds of galaxies within a
small area of the sky. By definition, the measurement error scales with the number of galaxies $N_{\rm gal}$ as $\propto 1/\sqrt{N_{\rm gal}}$. 
With the largest ground-based telescopes, the effective number of galaxies used for weak lensing measurement is a few tens in a 
1-${\rm arcmin}^2$
area \cite{Chang13}, and the finite number of galaxies is still a major source of noise for WL mass reconstruction. 

Several promising approaches have been proposed to denoise 
WL convergence maps using CNNs.
Ref.~\cite{Shirasaki21} use conditional GANs \cite{Isola16} 
to reconstruct convergence fields from noisy observational data.
The authors apply the GANs to real Subaru HSC data and show that the detected peak positions in the reconstructed map are matched well to the observed galaxy clusters identified by means of other observations.

\subsection{Galaxy redshift surveys}

Galaxy redshift survey \cite{Lahav04} is a straightforward way to map the three-dimensional distribution of galaxies. 
Unfortunately it requires time-consuming spectroscopic measurements of the target galaxies to determine the radial distance from the observer, and thus galaxy redshift surveys are more expensive than photometric imaging surveys. 
However, a simple statistics argument suggests that the information content extracted from a galaxy redshift survey can easily exceed that from an imaging survey. In practice, the shot noise contribution arising from the discrete nature of galaxy distribution restricts the full access to information on small scales. There is also a competition between the survey area and the depth, and the survey parameters are usually optimized to maximize the total signal-to-noise ratio. This optimization is done by considering multiple scientific goals in addition to the cosmological constraining power.

Theoretically, it is still difficult to predict accurately the distribution of galaxies using analytical or numerical methods. The relation between the underlying invisible dark matter and galaxies is called galaxy bias \cite{Desjacques18}.  Observationally, it is known that galaxies are biased with respective to dark matter differently depending on their luminosity, color or morphology. Therefore, accurate modeling of the distribution of a galaxy population can be done, if ever possible, only after the full criteria of the galaxy selection are specified. 
A common practice is to model or to parameterize the relation between dark matter and galaxies (see Sec. \ref{sec:galaxy_model} for ML-based methods), and then to compare the predicted summary statistics with those derived from observations.
A fast method to generate theoretical "templates" of the summary statistics using ML shall be discussed extensively in Sec.~5.
Such model templates can be tested by using, for instance, realistic mock galaxy catalogues generated from hydrodynamical simulations. 

For cosmology study, it is of crucial importance to select a population of galaxies whose properties are well understood. Luminous red galaxies (LRGs) or similar types of galaxies have been popular main targets of galaxy surveys in the past two decades \cite{Eisenstein01}. They are associated with massive halos, and the majority of them are located at the center of the host halo. Ongoing and forthcoming surveys such as DESI \cite{DESI16}, Subaru PFS \cite{PFS14}, and Euclid \cite{Euclid13} target emission line galaxies to map 
the galaxy and matter distribution at high redshifts. Clearly, understanding the nature of the emission line galaxies is important in order to fully utilize the data and to extract cosmological information from them in an unbiased manner. Also, novel ML-based methods would be needed to perform cosmological parameter inference beyond the traditional analyses based on 
low-order statistics.
ML approaches using three-dimensional CNNs \cite{Ntampaka20}
or graph neural networks (GNNs) have already been proposed. For the latter,
two nearby galaxies (nodes) are connected by edges \cite{Villanueva-Domingo22b}. 

\subsection{Intensity mapping}

The cosmic microwave background (CMB) radiation contains rich information on the evolution of the universe from the very early epoch just after the Big Bang to the present.
Past CMB experiments achieved accurate determination of  cosmological parameters \cite{Planck18}, and
future experiments including the Simons Observatory \cite{SimonsObs}, 
the CMB-S4 \cite{CMB-S4}, and LiteBIRD \cite{Sugai20} will perform polarization measurements
and multi-band intensity mapping with higher sensitivities. 

Intensity mapping measures everything altogether from near to far in a single band or in a wavelength pixel, and both extragalactic and Galactic foregrounds are unavoidably detected.
Refs. \cite{Troster19,Han21} propose to use generative models  
to predict the foreground components including
the kinetic and thermal Sunyaev-Zelâ€™dovich effects, cosmic infrared background, and radio galaxies.
The authors show that the intensity power spectra and other non-linear statistics are reproduced from their mock observation.
Many realizations of wide-area, high-resolution maps generated by ML methods can be used to study potential systematic errors and to evaluate covariance matrices, which are crucial for precise cosmological analysis.
The use of conditional generative models are also proposed for other purposes such as removing the foreground \cite{Wang22_CMB}, 
separation of each component \cite{Bonjean22}, reconstruction of lensing map \cite{Caldeira19} and in-painting of masked regions \cite{Yi20,Puglisi20,VafaeiSadr21,Montefalcone21}.
To utilize all-sky data, one can extend a traditional 2D CNN to be applied to images on a sphere. 
Ref. \cite{Petroff20}  demonstrate that such a spherical neural network can be used to directly remove noises from all-sky CMB data.


\subsection{Line intensity mapping}

Line intensity mapping (LIM) is a rapidly developing observational technique to probe the large-scale structure by measuring collective fluctuations 
of line emissions in a broad range of wavelength.
LIM probes three-dimensional distribution of line emitters over a larger volume with smaller observational cost than galaxy redshift surveys. 
 Ongoing experiments are aimed at measuring the fluctuations of the 21-cm spin-flip transition line from neutral hydrogen \cite{Chang10},
and several large programs such as SKA \cite{Carilli15} and CHIME \cite{Amiri17} are also underway.
LIM observations targeting at different line emission from CO, H$\rm \alpha$, and Lyman-$\rm \alpha$,  
are also being conducted and planned \cite{Kovetz19}.
NASA's SPHEREx satellite to be launched in 2024 is expected to map infrared intensities over the full sky \cite{Dore18}. 

Refs. \cite{Zamudio-Fernandez19,Wadekar21} trained GANs using the outputs of IllustrisTNG hydrodynamics simulation in order to generate neutral hydrogen distributions at  post-reionization epochs ($z$ < 6). 
The resulting power spectrum and bispectrum from their models, HIGAN and HInet, agree with those of the original simulation outputs better than an analytical method based on halo occupation distributions.
Ref. \cite{Hassan21} proposes another type of generative model, HIFlow, 
which generates neutral hydrogen maps for a given set of input cosmological parameters. The authors train HIFlow using the simulation data from CAMELS (see Sec.~\ref{sec:galaxy}), to promote learning the diversity of intensity maps. Thus generated IM catalogues can be used for statistical analysis and for cosmological parameter inference with 21-cm LIM observations. 

Observations of the 21-cm line emission from the epoch of reionization (EoR) can be a direct probe of cosmic reionization.
Because the spatial and frequency distributions of the 21-cm emission are highly non-linear
and thus difficult to interpret, 
ML could be the most effective way to extract cosmological information.
Various ML methods have been proposed to infer cosmological and astrophysical parameters 
from the intensity power spectrum \cite{Shimabukuro17,Doussot19,Choudhury22}, 
other summary statistics \cite{Jennings20,Choudhury21},
and directly from tomographic maps \cite{Gillet19,Hassan19,Neutsch22} of the 21-cm line intensity.
Ref.~\cite{Hassan19} showed that a ML model can distinguish reionization models even when 
a realistic noise level of SKA is assumed.
Other ML applications to 21-cm observations include 
identification of ionized regions \cite{Bianco21},
reconstruction of 21-cm maps from Lyman-$\rm \alpha$ emitter distributions \cite{Yoshiura21},
and reconstruction of DM distributions from 21-cm maps \cite{Villanueva-Domingo21}.
Emulation (see Section \ref{sec:emulation}) and generation \cite{List20} of the EoR 21-cm signals have also
 been explored.

 
\begin{figure}
    \centering
    \includegraphics[width=14cm]{Figures/cGAN.png}
    \caption{Schematic picture of a conditional GAN used in Ref. \cite{Moriwaki20}  
    with two CNNs called generator (left) and discriminator (right). 
    The generator reconstructs a LSS map from a noisy observational map
    while the discriminator is given either reconstructed or true map and returns a probability $p\in [0,1]$ that the input is true map.
    The CNNs consist of several convolutional and deconvolutional layers (black and gray arrows), where the input images are convolved with various filters. 
    The generator has a so-called skip connection (white arrow) \cite{Isola16}, where the outputs in the encoder layers are reused in the decoder layers.
    The training is done adversarially; the discriminator is updated to distinguish between reconstructed (generated) and true data more accurately,
    while the generator is updated to better {\it fool} the discriminator.
    Eventually the generator becomes able to reconstruct very complex features.
    }
    \label{fig:cGAN}
\end{figure} 

Foreground and background contamination is a serious problem of LIM. 
Conditional generative models can be used for information extraction and component separation from LIM data.
In 21-cm observations, emissions from our Galaxy and extragalactic radio sources are
much brighter than the high-redshift 21-cm signals.
Ref. \cite{Li19} proposes to remove foreground emissions from 21-cm observation data.
The authors utilize an autoencoder, a popular generative CNN model,
to reconstruct signals from the EoR from noisy observational data assuming SKA.
In LIM observations in other wavelengths, there are multiple bright emission lines,
and the so-called interloper lines compromise reconsruction of the LSS to be traced by the target line emission.
Removal of line interlopers can done by trained CNNs
\cite{Moriwaki20}.
The authors generate mock observational data assuming SPHEREx mission
and use conditional GANs (Figure \ref{fig:cGAN}) to separate two emission lines H$\rm \alpha$ and [O{\sc iii}]. 
Three dimensional data (angular $\times$ spectral domain) that are obtained by LIM observations 
can also be analyzed with three-dimensional CNNs \cite{Makinen21,Moriwaki21} or with recurrent neural networks \cite{Prelogovic22}.
Including the additional dimension is expected to improve the accuracy of reconstruction and regression.
