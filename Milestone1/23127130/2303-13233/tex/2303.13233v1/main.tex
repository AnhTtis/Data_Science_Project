\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{appendix}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{bm}
\usepackage[table]{xcolor}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{2480} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\yu}[1]{\textcolor{cyan}{yu:#1}}
% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World}

\author{Qifan Yu$^1$\footnotemark[1] \quad\quad Juncheng Li$^1$\footnotemark[1] \quad\quad  Yu Wu$^2$ \quad\quad  
Siliang Tang$^1$\footnotemark[2] \quad\quad Wei Ji$^3$ \quad\quad Yueting Zhuang$^1$\footnotemark[2]\\ $^1$Zhejiang University, $^2$Wuhan University, $^3$National University of Singapore\\ {\tt\small \{yuqifan, junchengli, siliang, yzhuang\}@zju.edu.cn}  \\\tt\small yu.wu-3@student.uts.edu.au, jiwei@nus.edu.sg}

\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}} %将脚注符号设置为fnsymbol类型，即特殊符号表示
\footnotetext[1]{Equal Contribution.} %对应脚注[1]
\footnotetext[2]{Corresponding Authors.}
\renewcommand{\thefootnote}{\arabic{footnote}}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
\textbf{S}cene \textbf{G}raph \textbf{G}eneration~(SGG) aims to extract~\textless subject, predicate, object\textgreater~relationships in images for vision understanding. Although recent works have made steady progress on SGG, they still suffer long-tail distribution issues that tail-predicates are more costly to train and hard to distinguish due to a small amount of annotated data compared to frequent predicates. Existing re-balancing strategies try to haddle it via prior rules but are still confined to pre-defined conditions, which are not scalable for various models and datasets. In this paper, we propose a \textbf{C}ross-mod\textbf{a}l predi\textbf{Ca}te b\textbf{o}osting~(\textbf{CaCao}) framework, where a visually-prompted language model is learned to generate diverse fine-grained predicates in a low-resource way. The proposed CaCao can be applied in a plug-and-play fashion and automatically strengthen existing SGG to tackle the long-tailed problem. Based on that, we further introduce a novel \textbf{E}ntangled cross-modal \textbf{p}rompt approach for open-world pred\textbf{i}cate s\textbf{c}ene graph generation~(\textbf{Epic}), where models can generalize to unseen predicates in a zero-shot manner. Comprehensive experiments on three benchmark datasets show that CaCao consistently boosts the performance of multiple scene graph generation models in a model-agnostic way. Moreover, our Epic achieves competitive performance on open-world predicate prediction.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Scene graph generation~(SGG) aims to detect visual relationships in real-world images, which consist of the subject, predicate, and object~(\textit{i.e.,}~\textbf{subject}:~\textit{flag},~\textbf{predicate}:~\textit{displayed on},~\textbf{object}:~\textit{screen} in Figure~\ref{sgg-example}~(a)). Since scene graphs bridge the gap between raw pixels and high-level visual semantics, SGG has been widely used in a variety of visual scene analysis and understanding tasks, such as visual question answering~\cite{jiang2020defense, han2021greedy}, image captioning~\cite{zhang2021consensus,yang2022reformer}, and 3D scene understanding~\cite{gothoskar20213dp3, zhang2021exploiting}.

\begin{figure}[ht]
   \begin{subfigure}{0.32\linewidth}
    \centering
    \includegraphics[width=1.\linewidth]{figures/input1.png}
    \caption{Detected Image}
    \label{input}
  \end{subfigure}
  \begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/sg1.png}
    \caption{Base SGG}
    \label{sg1}
  \end{subfigure}
    \begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/sg2.png}
    \caption{Enhanced SGG}
    \label{sg2}
  \end{subfigure}
  \begin{subfigure}{0.50\linewidth}

   \includegraphics[width=1.0\linewidth]{figures/long-tail-1.pdf}
    \caption{Long-tail predicate distribution}
   \label{long-tail distribution}
   \end{subfigure}
   \begin{subfigure}{0.48\linewidth}

   \includegraphics[width=1.0\linewidth]{figures/long-tail-2.pdf}
    \caption{PredCls of different predicates}
   \label{predcls-recall}
   \end{subfigure}
   \vspace{-0.1cm}
    \caption{\textbf{Illustration of handling long-tail distribution problem by cross-modal predicate boosting in Visual Genome.}~(b) and (c) show scene graphs enhanced by visual knowledge generating more informative predicates in long-tail distribution.~(d) indicates the imbalance of predicates due to the long-tailed distribution in the training set.~(e) For prediction of scene graph relationships~(PredCls), our CaCao framework can obtain consistent improvement on both head predicates and tail predicates.} 
    %\yu{is it possible to enlarge the font size~(especially for d and e)? It is hard to read when printed.}}
    \label{sgg-example}
    
\end{figure}


Recently, various methods~\cite{chen2019knowledge,zellers2018neural,zareian2020weakly,tang2019learning,yao2021visual,yan2020pcpl,desai2021learning} have been proposed to improve the SGG performance, but still tend to predict frequent but uninformative predicates due to the long-tailed distribution of predicates in SGG datasets~\cite{xu2017scene,li2021bipartite,yu2020cogtree}. In a way, those approaches degenerate into a trivial solution, which undermines the application of SGG. As shown in Figure~\ref{sgg-example}~(d), in the Visual Genome~\cite{xu2017scene}, the top 20\% of predicate categories account for almost 90\% of samples, while other tail fine-grained predicates lack sufficient training data. Accordingly, the PredCls recalls of SGG models on those tail predicates are remarkably lower than head predicates, as demonstrated in Figure~\ref{sgg-example}~(e).

Prior works have been proposed in recent years to alleviate the bias caused by the long-tail distribution based on causal rules~\cite{tang2020unbiased, li2022devil}, reweighting~\cite{tang2019learning, wang2021seesaw,yu2020cogtree} and resampling strategy~\cite{burnaev2015influence, yan2020pcpl,li2021bipartite} gradually. Nevertheless, these methods still require careful tuning of additional hyper-parameters, such as sampling frequency and category weight. They are sensitive to different architectures and data distributions, which are not flexible for real-world situations. Another alternative way is to increase the number of tail predicates in training. IETrans~\cite{zhang2022fine} uses internal relation correlation to enhance the existing dataset. However, these methods rely on the prior distribution of source data and only work in specific pre-defined conditions. Such a manner based on hand-designed rules covers only limited categories, which is time-consuming and unscalable.
%But the existing data extension methods of SGG are all based on the existing knowledge base or data distribution which is costly to maintain sufficient labeled data. 

%In order to expand the application for current SGG tasks to real worlds, we need to deal with the long-tail distribution problem in a way that is low-cost and easily scalable. 
In this paper, we propose a \textbf{C}ross-mod\textbf{a}l predi\textbf{Ca}te b\textbf{o}osting~(\textbf{CaCao}) framework, which leverages the extensive knowledge from the pre-trained language models to enrich the tail predicates of scene graphs in a low-cost and easily scalable way. Our fundamental intuition is that language models gain extensive knowledge about informative relationships from massive text corpus during general sentence pre-training~(\textit{i.e. Large silver \textcolor{blue}{{airp}lane} \textbf{parked outside} an \textcolor{red}{airport} with a \textcolor{blue}{pilot} \textbf{sitting in} it that has \textbf{come back from} a \textcolor{red}{mission}, while the pilot gets some rest.})~\cite{schick2021s, shin2020autoprompt}. 
%\textcolor{red}{The pre-trained language model is more suitable for predicate boosting since they focus more on knowledge of semantic relationships than they do on text matching, as visual-language pre-training models do.}
While the pre-trained language models contain diverse relational knowledge, it is non-trivial to elicit this knowledge from them to scene graph generation. First, there is a significant modality gap in migrating extensively linguistic knowledge into scene graph predicate prediction since such large-scale language models are `blind’ to visual regions. An alternative way is to use vision-language pre-training~(VLP) models. However, VLP models are mainly trained by image-text contrastive learning, lacking the delicate language ability to generate fine-grained predicate category words. Second, a predicate type might correspond to many different linguistic expressions~(\textit{e.g.}, he ``walks through''/``is passing through''/``passed by'' a street may correspond to same predicate). Without considering such semantic co-reference phenomenon, the adapted language model for predicate generation can easily collapse to monotonic predictions.

To address the above challenges, we first introduce a novel cross-modal prompt tuning approach, which enables the language model to subtly capture visual context and predict informative predicates as masked language modeling, called the visually-prompted language model. As for semantic co-reference, we further present an adaptive semantic cluster loss for prompt tuning, which models the semantic structures of diverse predicate expressions and adaptively adjusts the distribution to inhibit excessive enhancement of specific predicates during boosting process, thus rendering a diverse and balanced distribution. Moreover, we introduce a fine-grained predicate-boosting strategy to extend the existing dataset with the informative predicates generated by our visually prompted language model. From the comprehensive view of Figure~\ref{sgg-example}~(e), our CaCao can greatly improve the SOTA models' performance in a plug-and-play way, where \textbf{PredCls} of most predicates are consistently increased by 30\% in the purple bar than the blue.

From a more general perspective, our CaCao can not only effectively alleviate the long-tail distribution problem even in large-scale SGG but also generalize to open-world predicates by leveraging the generalizability of human language. Inspired by the impressive zero-shot performance of vision-language pre-training models~\cite{li2021supervision,radford2021learning, kim2021vilt}, which utilize the generalizability of human language for zero-shot transfer, we replace the traditional fixed predicate classification layer with category-name embedding and use the diverse predicates generated by our CaCao to learn general and transferable predicate embeddings. Specifically, we propose a novel \textbf{E}ntangled cross-modal \textbf{p}rompt approach for open-world pred\textbf{i}cate s\textbf{c}ene graph generation~(\textbf{Epic}), where the entangled cross-modal prompt alternately tinker with the predicate representation, making the scene graph model aware of the abstract interactive semantics.

Surprisingly, without using any ground-truth annotations and only with the informative relations generated by our CaCao framework, our Epic achieves competitive performance on the open-world predicate learning problem.

Our main contributions are summarized as follows: 
 \begin{itemize}
     \item We propose a novel \textbf{C}ross-mod\textbf{a}l predi\textbf{Ca}te b\textbf{o}ost-ing~(\textbf{CaCao}) framework, where a visually-prompted language model is learned to enrich the existing dataset with fine-grained predicates in a low-resource and scalable way.

     \item Our CaCao can be applied to SOTA models in a plug-and-play fashion. Experiments over three datasets show steady improvement in standard SGG tasks, demonstrating a promising direction to automatically boosting data by large-scale pre-trained language models rather than time-consuming manual annotation. 
     \item In addition, we introduce \textbf{E}ntangled cross-modal \textbf{p}rompt approach for open-world pred\textbf{i}cate s\textbf{c}ene graph generation~(\textbf{Epic}) to explore the expansibility of CaCao for unseen predicates, and validate its effectiveness with comprehensive experiments.

\end{itemize}

 \begin{figure*}[ht]
\begin{subfigure}{0.61\linewidth}
\centering
   \includegraphics[width=1.0\linewidth]{figures/framework-total.pdf}
    \caption{The overall framework of CaCao}
   \label{architecture-total}
   \end{subfigure}
   \begin{subfigure}{0.39\linewidth}
\centering
   \includegraphics[width=1.0\linewidth]{figures/framework-vplm.pdf}
    \caption{Visually-Prompted Language Model}
   \label{vplm}
   \end{subfigure}
   \caption{Illustration of our proposed Cross-Modal Predicate Boosting framework. \textbf{Visually-Prompted Language Model} is designed to extract specific visually relevant knowledge underlying pre-trained language models for informative predicates enhancement. The right subfigure shows the detail of the visually-prompted language model. \textbf{Fine-Grained Predicate Boosting} uses informative fine-grained predicates to boost the existing scene graph dataset for standard SGG and open-world predicate SGG in a model-agnostic way.}
      \vspace{-0.4cm}
   \label{architecturel}
\end{figure*}

\section{Related Work}
\noindent\textbf{Scene Graph Generation.} Current scene graph generation is still far from practical since it suffers from long-tail distribution of predicates~\cite{zellers2018neural, tang2019learning,chen2019knowledge}. Recently, resampling~\cite{zou2018unsupervised,burnaev2015influence,li2021bipartite} and reweighting~\cite{wang2021seesaw,yu2020cogtree} and causal rule-based methods have been proposed to alleviate the biased prediction in the training stage. On the other hand, some approaches aim to balance long-tailed distribution classification following class distribution~\cite{wang2020devil,zhang2021distribution, desai2021learning}. Since the predicates in scene graphs are highly relevant to the context, the direct enhancement methods based on class distribution are inapplicable for the balanced scene graph generation. Thus, some methods in SGG~\cite{yao2021visual,zhang2022fine} explore visually relevant relationships from the external knowledge base to enhance the existing dataset. However, previous approaches require additional hyper-parameters or hand-designed enhancement rules limited to pre-defined scene graphs. Therefore, in this work, we propose a cross-modal predicate boosting framework that can flexibly enhance current datasets and generalize to the open world by diverse informative predicates.

\noindent\textbf{Language Model Prompting.} Recently, researchers have found that large-scale pre-training models contain rich knowledge~\cite{talmor2020olmpics,elazar2021measuring} and are equipped with the ability to generalize to downstream tasks, which can achieve comparable performance with only little parameter-tuning~\cite{liang2022modular,gao2020making,shin2020autoprompt,jiang2020can,jia2021scaling}. We are also immensely motivated by recent PET work~\cite{schick2021s}, even though it primarily focuses on a semi-supervised situation with many unlabeled instances. FROZEN~\cite{tsimpoukelli2021multimodal} firstly explore few-shot learning in the multi-modal setting with a frozen language model since vision and language can be attended by a unified attention map~\cite{kim2021vilt}. 
Unlike the above works, we focus on using visual cues to constrain linguistic knowledge of language models,
% instead of individual modality, 
thereby enriching semantic relationships in scene graphs.

\noindent\textbf{Zero-shot Scene Graph Generation.} Current zero-shot SGG mainly focuses on the generalization of relation combination, ignoring to predict of novel predicates in the wild world~\cite{lu2016visual,tang2020unbiased,kan2021zero,goel2022not}. He~\textit{et al.}~\cite{he2022towards} first introduce open-vocabulary scene graph generation and attempt to predict unseen objects through representation-encoding. But it still cannot transfer to other SGG tasks well because of the enormous cost of dense-caption pre-training. Our approach aims to explore the extensibility of CaCao in open-world predicate prediction. Here we introduce a novel entangled cross-modal prompt for open-world predicate scene graph generation based on CaCao without costly pre-training.
% Given a scene graph with limited annotations, the \textit{visually-prompted language model} thoroughly exploits linguistic knowledge from pre-trained language models to generate open-world informative predicates and address the semantic ambiguity between these predicates by \textit{adaptive semantic cluster loss}. 2)~Then, \textit{fine-grained predicate boosting} maps abundant fine-grained predicates to existing scene graphs, automatically alleviating the predicate-biased problem in a model-agnostic way. Notably, CaCao can provide diverse predicates for Epic to achieve open-world SGG. We will elaborate on Epic in Section~\ref{Epic}.
\section{Cross-Modal Predicate Boosting}
As illustrated in Figure~\ref{architecturel}, our \textbf{C}ross-mod\textbf{a}l predi\textbf{Ca}te b\textbf{o}osting~(\textbf{CaCao}) framework mainly consists of three components: 1) First, the \textit{visually-prompted language model} thoroughly exploits linguistic knowledge from pre-trained language models and migrates it into fine-grained predicates generation. 2) Then, \textit{adaptive semantic cluster loss} is proposed to address the semantic co-reference problem in visually-prompted language model by diverse predicate expression modelling and adaptive adjustment for predicate enhancement. 3) Finally, \textit{fine-grained predicate boosting} uses these enhanced predicates to alleviate the long-tailed problem of SGG in a model-agnostic way. Furthermore, CaCao can provide various predicates for Epic to achieve open-world SGG. We will elaborate on Epic in Section~\ref{Epic}.
\subsection{Preliminaries}
\noindent\textbf{Scene Graph Generation.} In SGG, we try to locate objects in the image and identify visual predicates between subjects and objects. Concretely, given an image $I$, a scene graph $\mathcal{G=(O,R)}$ corresponding to $I$ has a set of objects $\mathcal{O}={(o_i)}_{i=1}^{N_o}$, bounding boxes $\mathcal{B}=(b_i\in{\mathbb{R}^4})$and a set of relationship relationships $\mathcal{R}=(s_i,p_i,o_i)_{i=1}^{N_r}, s_i,o_i\in{\mathcal{O}}$, where $N_o$ and $N_r$ are the number of all objects and relationships. For each image, its scene graph generation typically predicts all the above information in the picture.
\subsection{Visually-Prompted Language Model}
\label{VLP-tuning}
 Although several weakly-supervised approaches could improve visual relation modeling through different knowledge bases~\cite{zareian2020weakly, shi2021simple, yao2021visual,zhang2022fine}, they require hand-designed rules and thus have limited generalization ability.
 Thus, we attempt to utilize the linguistic knowledge of pre-trained language models to boost fine-grained predicates in a low-resource way and make language models flexibly aware of scenes through visual prompts, as shown in the \textit{visually-prompted language model} module of CaCao in Figure~\ref{architecturel}~(a).
 % Thus, we attempt to make language models acutely aware of scenes through visual prompts and utilize the underlying knowledge of pre-trained language models to adapt predicates. The critical insight of our CaCao framework is to generate informative fine-grained predicates through the visually-prompted language model, as shown in Figure~\ref{architecturel}.


\noindent\textbf{Visually-Prompted Templates.} Due to the modality gap between linguistic knowledge and visual content, language models cannot directly perceive the visual relationships in the scene graph. To better utilize visual semantics, we propose the visually-prompted template containing both visual and textual information, which is designed as \textbf{X}=``[\textit{visually-prefixed prompts}] [P] [SUB][MASK][OBJ]", where [\textit{visually-prefixed prompts}] is an image-conditioned token generated by a transformation layer $h_\theta$ from specific visual features and [P] indicates learnable textual prompt for efficient text prompt engineering.
% For each image $I$, we extract image features by vision encoders and obtain \textit{visually-prefixed prompts} by linear projection. We then concatenate \textit{visually-prefixed prompts} and the masked relation [SUB][MASK][OBJ] with continuous soft prompts [P] to construct a series of masked templates $X$.
During training, we feed our visually-prompted templates into frozen language models to predict correct predicates at the masked position and only update the textual prompt [P] together with the parameters $\theta$ in the visual projection layer $h_\theta$.
% parse the captions of the image and gather all the informative relationships as training data to tuning our visually prompted template. 

\noindent\textbf{Cross-Modal Prompt Tuning} aims to predict correct fine-grained predicates at the masked position based on cross-modal contexts from \textbf{X} by optimizing visually-prompted templates. We randomly collect 80k image-caption pairs from the web (\textit{i.e.}, CC3M, COCO caption), which contain nearly 2k categories of predicates but with much noise of simple predicates. We further design heuristic rules (\textit{e.g.}, corpus co-occurrence frequency) to filter out uninformative~(\textit{on, near}) and infrequent~(\textit{kneeling by}) predicates \textbf{automatically} instead of handling them \textbf{manually}. We finally obtain 585 categories of diverse informative predicates, nearly covering most of the common situations in the real world.
% Considering the predicate labels might consist of multiple words,
 During training, we use a softmax classifier to predict the predicate tokens for each masked position. Formally, we define the predicate category set $Y=[Y^{(1)},Y^{(2)},...,Y^{(C_p)}]$ and each predicate label $y=[y^{(1)},y^{(2)},...,y^{(C_p)}]$ is a one-hot vector, where $Y^{(C_p)}$ denotes each category word and $C_p$ denotes the number of predicate categories. Given the probability distribution $q(y_i|X_i)$ at the masked position for each input $X_i$ and the correct predicate token $p(y_i)$, we can optimize visually-prompted templates as well as the predicate classifier by the Cross-Entropy Loss as follows:
\vspace*{-0.5\baselineskip} 
\begin{equation}
\mathcal{L} = -\sum_{i=1}^{N_p} {p(y_i)log(q(y_i|X_i))}~,
\label{virtual}
\vspace*{-0.5\baselineskip} 
\end{equation}
where $N_p$ represents the number of predicates for prompt tuning. Note that we only update the parameters of visual-linguistic projection for \textit{visually-prefix prompts} and optimize textual prompts [P], as shown in Figure~\ref{architecturel}~(b). More prompt-tuning details are provided in the \textbf{Appendix C}. 

\subsection{Adaptive Semantic Cluster Loss} 
Although visually-prompted templates partially alleviate semantic ambiguity through instance-conditioned hints, it still suffers from semantic co-reference among predicates, where the same predicate semantic might have multiple linguistic expressions shown in \textbf{Appendix C}. 
% promoting more diverse predicates in CaCao.
Therefore, we further design an adaptive semantic cluster loss, which refines diverse semantic expressions based on cluster structures and adaptively suppresses excessively boosted categories based on the dynamic distribution of predicates, thus facilitating more various predicate distributions in CaCao.

Specifically, we first represent predicates as the sum of GloVe vectors\cite{pennington2014glove} corresponding to their subject, predicate, and object due to the strong interdependence between triplets in our task. We then cluster these predicates using K-means and initialize the number of centroids based on the similarity threshold between each predicate.
% We reduce the softmax punishment \yu{???} for predicates in the same cluster to prevent highly correlated predicates from over-suppressing. 
During training, we reduce the penalty for those predicates that are incorrect but in the same cluster to prevent highly correlated predicates from over-suppressing.
% by ensuring that related predicates are not excessively penalized.}
% \textcolor{red}{Since the high dependence between triplets, we characterize predicates by the sum of GloVe vectors\cite{pennington2014glove} in (s,p,o). We initialize the number of centroids by the similarity threshold between each predicate and then cluster predicates by K-means.}
% % We represent each predicate by the sum of GloVe vectors\cite{pennington2014glove} corresponding to the label words and aggregate predicates with similar semantics into the same cluster. 
% We reduce the softmax punishment for those in-cluster predicates to avoid over-suppressing correlated predicates.
We redefine each predicate label to be $y=[\phi(y^{(1)}),\phi(y^{(2)}),...,\phi(y^{(C_p)})]$ based on the clustering results, where each $\phi(y^{(j)})$ is defined as,
% As shown in Equation~(\ref{label_refinement}), we adjust each item $\phi(y^{(j)})$ in the vector based on the clustering results and refine the predicate label to be $y=[\phi(y^{(1)}),\phi(y^{(2)}),...,\phi(y^{(C_p)})]$ for training. 
\begin{eqnarray}
    \phi~(y^{(j)})=\left\{
	\begin{array}{lll}
			1      &  ,   & {j=j^*}\\
			\frac{1}{C_{j^*}}    &  ,   & {Y^{~(j)}\in{cluster(Y^{(j^*)})}}\\ 
			0    & ,    & {Y^{(j)}\notin{cluster(Y^{(j^*)})}}\\
	\end{array}\right.~.
 \label{label_refinement}
\end{eqnarray}
where $j^*$ denotes the index of the ground truth predicate in all predicates, and $Y^{(j)}$ represents the predicate category name of index $j$. $C_{j^*}$ denotes the number of predicate classes in the predicate cluster of $Y^{(j^*)}$. Instead of roughly giving negative gradients, we set the vector value of the correlated predicate in the same cluster to $\frac{1}{C_{j^*}}$. 

Furthermore, we observed that assigned predicate augmentation fails to accommodate the dynamic distribution of predicates, which leads to an excessive boost of some specific predicates that destroys diversity.
Accordingly, we set the adaptively re-weighting factor to dynamically adjust the boosting ratio of each predicate based on its proportion during training. We adjust the Cross-Entropy Loss of each input $X_i$ by semantic cluster vectors as follows:
\begin{align}
\label{e1}
\mathcal{L}_i = -\sum_{j=0}^{C_p}\phi(y^{(j)})log(\hat{\psi_j})~,
% =-(log(\hat{y_t}) + \sum_{c=1}^c\frac{1}{c}log~(\hat{y_c}))
\end{align}
\vspace{-0.3cm}
\begin{align}
\label{e2}
\hat{\psi_j} = \frac{e^{z_j}}{\sum_{k=1}^{C_p} \omega_j^ke^{z_k}},~\omega_j^k = \min(1,\frac{z_k}{z_j}+\delta)\frac{m_k}{m_j}~,
\end{align}
% \begin{align}
% \label{e3}
% \omega_{ij} = \max(0,z_j-z_i+\delta)\frac{m_i}{m_j}
% \end{align}
where $z=[z_1,z_2,...,z_{C_p}]$ and $\hat{\psi}=[\hat{\psi_1},\hat{\psi_2},...,\hat{\psi_{C_p})}]$ denote predicted logits and probabilities with softmax for each predicate class. $\omega_j^k$ denotes the adaptively re-weighting factor concerning dynamic distribution between the boosted predicate of index $j$ and other predicates of index $k$. $\frac{z_k}{z_j}$ denotes the trend of predicate prediction and $\frac{m_k}{m_j}$ indicates the initial ratio of the number of predicates. The $\delta$ is a hyper-parameter representing prediction margins for predicates. When boosting one predicate enough, we will restrain its enhancement by reducing $\omega_j^k$, guaranteeing the distribution of generated predicates to be balanced and diverse.
\begin{eqnarray}
\label{gar}
    \frac{\partial{\mathcal{L}_i}}{\partial{z_j}}=\left\{
	\begin{array}{ccc}
			\hat{\psi_j}-1   & ,    & {j=j^*}\\
			\frac{1}{C_{j^*}}(\hat{\psi_j}-1)  & , &{Y^{(j)}\in{cluster~(Y^{(j^*)})}}\\
			\hat{\psi_j}   &  ,   & {Y^{(j)}\notin{cluster(Y^{(j^*)})}}\\
	\end{array} \right..
\end{eqnarray}
Eq.~\ref{gar} shows the gradient of the loss function with different parts in logit $z$ for each predicate. If negative category $j$ is relevant to the ground truth predicate, we will reduce its punishment to encourage the diversity of CaCao. Finally, it results in a diverse and balanced boosting process.

\subsection{Fine-Grained Predicate Boosting}
\label{Knowledge relabeling}
Although we obtain abundant fine-grained predicates from CaCao, we cannot directly boost predicates into the current scene graph generation without mapping since there is a distribution gap between them. To address the above limitation, we design a fine-grained predicate boosting stage to map the open-world predicates to target categories.
% , promoting the overall performance of SGG.
% unboxing tail predicates as well as preserving the overall performance.
% \noindent\textbf{Label Confidence Imputation.} 
% It is an obvious assumption that fine-grained predicates almost exist in those interacted objects.        

First, we consider a simple hierarchy structure of predicate semantics based on lexical analysis and establish mappings between CaCao and target predicates at each level by cosine similarity of triplet embeddings. We then select the most infrequent predicate from different mapping levels as the final enhanced predicate.
% We then select whether to use enhanced fine-grained data according to its confidence score from the probability distribution in Eq.~\ref{virtual}. 
To avoid destroying the original semantic information, we only boost those unannotated object pairs that have overlaps in scene graphs. We leave exploration of more complex structures for future work.
% \begin{equation}
%     p=\arg{\max_{i\in \mathcal{T}}{\frac{\exp{L_\zeta^i~(w_i)}}{\sum_{j\in \mathcal{T}}{\exp{~(\omega_{ij}L_\zeta^j~(w_i)})}}}}~,
% \end{equation}
% where $L_\zeta^i~(x)$ represents the confidence score between generated informative predicates and target predicates.% and $\omega_{ij}$ denotes the re-weighting factor concerning distribution between i and negative class j.
% To evaluate and compare with other models, we establish the mapping from diversity predicate labels to target close-set predicates based on cosine similarity of embedding and then select whether to use enhanced fine-grained data according to its confidence score.
% \noindent\textbf{Refined Scene Graph Training.} 

Given the existing scene graph dataset with $\left| \mathcal{N} \right|$ samples consisting of subject-object pair labels $(c_i,c_j)$ and their bounding box annotations $(b_i,b_j)$ along with the predicate $p$. Our CaCao can generate extra training data $\mathcal{D}$ automatically in a low-resource way and extend to the current scene graph models flexibly based on predicate mapping. Finally, we retrain scene graph generation with enhanced data  $\mathcal{\hat{N}}=(\mathcal{N},\mathcal{D})$. We formulate the learning problem as,
 \vspace*{-0.4\baselineskip}
\begin{equation}
    \min\limits_{\theta}{\frac{1}{\left| \mathcal{\hat{N}} \right|}\sum_{i=1}^{\left| \mathcal{\hat{N}} \right|}{L(N_i;\theta)}}~,
\vspace*{-0.4\baselineskip}
\end{equation}
where $L(N_i;\theta)$ denotes the loss function of learning procedure for scene graph generation. After fine-grained predicate boosting, we retrained a refined SGG model and mixed it up with the base model for a more balanced prediction.

\begin{figure}[ht]\vspace{-0.15cm}
  \centering
  
   \includegraphics[width=1.0\linewidth]{figures/entangled-prompt-1.pdf}
   \caption{Illustration of our proposed \textbf{E}ntangled cross-modal \textbf{p}rompt approach for open-world pred\textbf{i}cate s\textbf{c}ene graph generation~(\textbf{Epic}). It guides the model to learn the unified embedding of predicates by two complementary prompts in an associative way.}
   % \textcolor{red}{This figure should be improved. better organization. could be more compact.}}
 
   % , focusing on detailed region pixels and relation-conditional semantics by two complementary prompts.}
    \vspace{-0.3cm}
   \label{entangledprompt}
\end{figure}

\section{Open-World Predicate SGG}
\label{Epic}
Since CaCao can generate various fine-grained predicates, it can provide extra unseen data for open-world generalization.
% Our experiment results indicate that the predicates generated by CaCao are various and can be further generalized to the open world. 
To this end, we propose a novel \textbf{E}ntangled cross-modal \textbf{p}rompt approach for open-world pred\textbf{i}cate s\textbf{c}ene graph generation~(\textbf{Epic}). With the help of Epic, we can fully exploit the potential of \textbf{CaCao} and further extend it to open-world predicate SGG, as shown in Figure~\ref{entangledprompt}.
% , we firstly introduce the backbone for open-world predicate learning and propose entangled cross-modal prompts to better generalize to wider unseen predicates in open-world scene graph generation.

\noindent\textbf{Open-World Predicate SGG Backbone.} A straightforward way to predict unseen classes in open-world is replacing the fixed classifier with unified embeddings. Let $\bm{x}$ be region features generated by SGG models and $\bm{\left \{p_j \right \}}_{j=1}^{C_p}$ be a set of embeddings produced by the text encoder, where $\bm{p_*}$ is the embedding of the correct predicate. The loss for open-world predicate SGG with CaCao boosting is then,
\begin{equation}
\label{cl_loss}
    \mathcal{L}_{open} = -{\rm log}\frac{{\rm exp}(sim(\bm{x}, \bm{p_*})/\tau)}{\sum_{j=1}^{C_p}{{\rm exp}(sim(\bm{x},\bm{p_j})/\tau)}}~,
\end{equation}
% where $sim(\cdot,\cdot)$ denotes cosine similarity and $\tau$ is a learnable temperature parameter. 
% During inference, we predict the predicate with the highest similarity scores.
% replace the base class embedding with the novel class embedding and predict the predicate with the highest similarity scores.

\noindent\textbf{Entangled Cross-Modal Prompt.} 
% Moreover, we find that predicates in scene graphs are relation-relevant and context-sensitive
% , while directly embedding will learn the ambiguous representation of predicates. 
Moreover, we notice that predicate semantics and image regions are closely related. For example, ``man on chair" and ``shirt on chair" represent different semantics because they correspond to different image regions; the same region with the blue bounding box shown in Figure~\ref{sgg-example}~(a) may correspond to different predicates like ``manipulating" or ``looking at". Inspired by the remarkable performance of prompts on vision and language tasks~\cite{radford2021learning,liu2022p,li2021prefix,zhou2022learning}, we introduce entangled cross-modal prompts for text encoder and image encoder to alleviate the above problems. The predicate probability is computed as:
\begin{equation}
\label{cl_loss_ours}
    P~(p^*|\bm{x}) = \frac{{\rm exp}~(sim~(\bm{f}_x~(\bm{p_*}), \bm{t}_{p_*}~(\bm{x}))/\tau)}{\sum_{j=1}^{C_p}{{\rm exp}~(sim~(\bm{f}_x~(\bm{p_*}), \bm{t}_{p_j}~(\bm{x}))/\tau)}}~,
\end{equation}
where $\bm{f}_x~(\bm{p_*})$ denotes conditional region feature based on the predicate input and $\bm{t}_{p_j}~(\bm{x})$ represents conditional predicate embedding based on image regions. 
% Specifically, we train two lightweight networks to generate a conditional vector from another modality for each input concatenated with region features or category-name embeddings. 
Figure~\ref{entangledprompt} shows a sketch of the architecture. During training, we only update the text-to-vision and vision-to-text projection parameters $~(\theta_1;\theta_2)$ to preserve the pre-trained language-vision model’s capability for open-world predicate generalization.

\begin{table*}[]
\resizebox{\textwidth}{!}{
\begin{tabular}{ccp{2cm}cccccc}
\hline
\toprule
&\multicolumn{1}{c}{\multirow{2}{*}{Model Type}} & \multicolumn{1}{c}{\multirow{2}{*}{Methods}} & 
\multicolumn{2}{c}{Predicate Classification} & \multicolumn{2}{c}{Scene Graph Classification} & \multicolumn{2}{c}{Scene Graph Detection} \\ 
 & & & Tail-R@20/50/100 $\uparrow$ & mR@20/50/100 $\uparrow$&Tail-R@20/50/100 $\uparrow$ & mR@20/50/100 $\uparrow$ &Tail-R@20/50/100 $\uparrow$ & mR@20/50/100 $\uparrow$
 \\ \hline
&\multicolumn{1}{c}{\multirow{4}{*}{Specific}} & \multicolumn{1}{l}{BGNN~\cite{li2021bipartite}} & -/- & - / 30.4 / 32.9 & -/- & - / 14.3 / 16.5 &  -/- & - / 10.7 / 12.6  \\
&& \multicolumn{1}{l}{PCPL~\cite{yan2020pcpl}} & -/- & - / 35.2 / 37.8 & -/- & - / 18.6 / 19.6 & -/- & - / 9.5 / 11.7  \\ 
&& \multicolumn{1}{l}{SVRP~\cite{he2022towards}} & -/- &  - / 24.3 / 25.3 & -/- & - / 12.5 / 15.3 & -/- &  - / 10.5 / 12.8  \\
% && \multicolumn{1}{l}{DT2-ACBS~\cite{desai2021learning}} & 26.9 / 33.1 / 37.1 & 27.4 / 35.9 / 39.7 & -/- & 18.7 / 24.8 / 27.5 &  12.4 / 16.5 / 19.3 & 16.7 / 22.0 / 24.4 \\
&& \multicolumn{1}{l}{DT2-ACBS~\cite{desai2021learning}} & -/- & 27.4 / 35.9 / 39.7 & -/- & 18.7 / 24.8 / 27.5 &  -/- & 16.7 / 22.0 / 24.4 \\
\hline\hline

&\multicolumn{1}{c}{\multirow{2}{*}{One-stage}} & \multicolumn{1}{l}{SSRCNN~\cite{teng2022structured}} & -/- & -/- & -/- & -/- & 10.4 / 16.3 / 19.1 & 13.7 / 18.6 / 22.5  \\
&&\multicolumn{1}{l}{\quad\textbf{+CaCao~(ours)}} & -/- & -/-  & -/- & -/- &  \textbf{13.6 / 18.0 / 21.2} & \textbf{14.1 / 18.7 / 23.1} \\
\hline\hline

\multirow{21}{*}{\rotatebox[]{90}{Model-Agnostic strategy}}& & \multicolumn{1}{l}{Motif~\cite{zellers2018neural}} & 10.2 / 13.3 / 14.4 &  12.1 / 15.2 / 16.2 &5.8 / 6.8 / 7.3  & 7.2 / 8.7 / 9.3 & 4.8 / 6.0 / 7.3  & 5.1 / 6.5 / 7.8   \\
&\multicolumn{1}{c}{Resample} & \multicolumn{1}{l}{\quad+Resample~\cite{burnaev2015influence}} & -/- &  14.7 / 18.5 / 20.0 & -/- & 9.1 / 11.0 / 11.8 & -/- &  5.9 / 8.2 / 9.7 \\ 
&\multicolumn{1}{c}{\multirow{2}{*}{Reweight}}& \multicolumn{1}{l}{\quad+Reweight~\cite{wang2021seesaw}} & 16.7 / 26.3 / 31.0 & 18.8 / 28.1 / 33.7 & 8.9 / 11.8 / 14.1 &  10.7 / 15.6 / 18.3 & 8.6 / 12.1 / 14.6 & 7.2 / 10.5 / 13.2 \\
&&\multicolumn{1}{l}{\quad+FGPL~\cite{lyu2022fine}} & 26.7 / 33.3 / 35.7  & 24.3 / 33.0 / 37.5 &  16.8 / 19.1 / 19.9 & 17.1 / 21.3 / 22.5 & 12.4 / 16.5 / 19.3 & 11.1 / 15.4 / 18.2\\
&\multicolumn{1}{c}{Causal Rule}& \multicolumn{1}{l}{\quad+TDE~\cite{tang2020unbiased}} & -/- & 18.5 / 25.5 / 29.1 & -/-
& 9.8 / 13.1 / 14.9 & -/- & 5.8 / 8.2 / 9.8 \\ \cline{2-9}
&\multicolumn{1}{c}{\multirow{5}{*}{Data Enhancement}}& \multicolumn{1}{l}{\quad+Only Caption Relations} & 16.7 / 20.5 / 21.8  &  15.2 / 19.8 / 21.2 & 8.1 / 9.6 / 10.1 & 8.0 / 9.8 / 10.5  & 5.3 / 7.7 / 9.4 & 6.0 / 8.2 / 10.0  \\
&&\multicolumn{1}{l}{\quad+VisualDS~\cite{yao2021visual}} & 11.3 / 14.5 / 16.3  & 13.1 / 16.1 / 17.5 &  5.9 / 7.0 / 8.3  &  7.6 / 9.3 / 9.9  & 5.1 / 6.8 / 7.8 & 5.4 / 7.0 / 8.3 \\
&&\multicolumn{1}{l}{\quad+DLFE~\cite{chiou2021recovering}} & -/- & 22.1 / 26.9 / 28.8  &  -/- & 12.8 / 15.2 / 15.9  & -/- & 8.6 / 11.7 / 13.8 \\ 
&&\multicolumn{1}{l}{\quad+IETrans~\cite{zhang2022fine}} & 27.3 / 31.3 / 33.2 & 30.2 / 35.8 / 39.1 &  13.5 / 15.5 / 16.1 & 18.2 / 21.5 / 22.8 & 9.2 / 12.3 / 14.3 & 12.0 / 15.5 / 18.0 \\
&&\multicolumn{1}{l}{\quad\textbf{+CaCao~(ours)}} & \textbf{31.4 / 36.1 / 37.6} & \textbf{30.9 / 37.1 / 38.9}  & \textbf{17.3 / 19.7 / 20.5} & \textbf{20.4 / 23.3 / 24.4} &  \textbf{13.9 / 18.4 / 21.6} & \textbf{12.6 / 17.1 / 20.0}\\ 
\cline{2-9}\specialrule{0em}{1.5pt}{1.5pt}\cline{2-9}


&&\multicolumn{1}{l}{VCTree~\cite{tang2019learning}} & 9.9 / 13.0 / 14.0 & 11.7 / 14.9 / 16.1 & 6.2 / 7.4 / 7.9& 9.1 / 11.3 / 12.0 & 4.3 / 6.1 / 7.2 & 5.2 / 7.1 / 8.3\\
&\multicolumn{1}{c}{\multirow{2}{*}{Reweight}}&\multicolumn{1}{l}{\quad+Reweight~\cite{wang2021seesaw}} & 23.9 / 30.7 / 33.7 & 19.4 / 29.6 / 35.3 & 12.2 / 14.9 / 16.1 & 13.7 / 19.9 / 23.5 & 8.4 / 12.2 / 14.7 &  7.0 / 10.5 / 13.1 \\
&&\multicolumn{1}{l}{\quad+FGPL~\cite{lyu2022fine}} & 32.2 / 36.8 / 38.2 & 30.8 / 37.5 / 40.2 &  23.5 / 26.5 / 27.5 &  21.9 / 26.2 / 27.6 & 13.5 / 17.4 / 20.4 &  \textbf{11.9} / 16.2 / 19.1\\
&\multicolumn{1}{c}{Causal Rule}&\multicolumn{1}{l}{\quad+TDE~\cite{tang2020unbiased}} & -/- &  18.4 / 25.4 / 28.7 & -/-
&  8.9 / 12.2 / 14.0 & -/- &  6.9 / 9.3 / 11.1 \\ \cline{2-9}
&\multicolumn{1}{c}{\multirow{4}{*}{Data Enhancement}}&\multicolumn{1}{l}{\quad+Only Caption Relations} & 16.2 / 20.3 / 21.7 & 14.7 / 19.3 / 20.9 & 8.0 / 9.8 / 10.4 & 8.2 / 10.1 / 10.8 & 6.0 / 8.0 / 9.7 & 5.5 / 7.8 / 9.5 \\
&&\multicolumn{1}{l}{\quad+DLFE~\cite{chiou2021recovering}} & -/- & 20.8 / 25.3 / 27.1 &  -/- & 15.8 / 18.9 / 20.0  & -/- & 8.6 / 11.7 / 13.8 \\ 
&&\multicolumn{1}{l}{\quad+IETrans~\cite{zhang2022fine}} & 27.3 / 31.6 / 33.0 & 31.7 / 37.0 / 39.7 & 11.6 / 13.6 / 14.3 & 18.2 / 19.9 / 21.8  & 9.0 / 11.8 / 13.7 & 9.8 / 12.0 / 14.9 \\
&&\multicolumn{1}{l}{\quad\textbf{+CaCao~(ours)}} & \textbf{33.1 / 37.5 / 38.9} & \textbf{33.8 / 39.0 / 40.8}  & \textbf{23.8 / 27.2 / 28.2} & \textbf{23.8 / 27.5 / 28.7} &  \textbf{14.6 / 19.4 / 22.6} & 11.8 \textbf{/ 16.4 / 19.1}  \\ 
\cline{2-9}\specialrule{0em}{1.5pt}{1.5pt}\cline{2-9}

&&\multicolumn{1}{l}{Transformer~\cite{tang2020unbiased}} & 10.8 / 13.5 / 14.6 & 12.4 / 16.3 / 17.6 & 8.8 / 10.3 / 11.8 & 8.7 / 10.1 / 10.7 & 5.3 / 7.3 / 8.8 & 5.8 / 8.1 / 9.6  \\ 
&\multicolumn{1}{c}{\multirow{2}{*}{Reweight}}&\multicolumn{1}{l}{\quad+Reweight~\cite{wang2021seesaw}} & 19.9 / 26.0 / 28.4 &  19.5 / 28.6 / 34.4 & 9.5 / 12.6 / 13.4 & 11.9 / 17.2 / 20.7 & 7.0 / 10.3 / 12.4 &  8.1 / 11.5 / 14.9 \\ 
&&\multicolumn{1}{l}{\quad+FGPL~\cite{lyu2022fine}} & 26.6 / 33.6 / 36.0 & 27.5 / 36.4 / 40.3 &  17.0 / 19.9 / 20.1 &  19.2 / 22.6 / 24.0 & 13.1 / 17.0 / 19.8 &  13.2 / 17.4 / 20.3\\
\cline{2-9}
&\multicolumn{1}{c}{\multirow{3}{*}{Data Enhancement}}&\multicolumn{1}{l}{\quad+Only Caption Relations} & 16.1 / 19.4 / 20.8 & 15.0 / 19.3 / 20.9 & 8.3 / 9.9 / 10.5 & 8.6 / 10.6 / 11.2 & 6.4 / 8.9 / 10.6 & 6.0 / 8.4 / 10.4 \\
&&\multicolumn{1}{l}{\quad+IETrans~\cite{zhang2022fine}} & 27.5 / 32.0 / 33.7& 29.1 / 35.0 / 38.0 & 14.1 / 16.2 / 16.7  & 17.9 / 20.8 / 22.3  & 11.6 / 14.9 / 17.6 & 11.7 / 15.0 / 18.1 \\
&&\multicolumn{1}{l}{\quad\textbf{+CaCao~(ours)}} & \textbf{31.7 / 35.7 / 37.0} & \textbf{36.2 / 41.7 / 43.7}  & \textbf{19.0 / 22.2 / 23.3} & \textbf{21.1 / 24.0 / 25.0} &  \textbf{14.1 / 18.7 / 21.9} & \textbf{13.5 / 18.3 / 22.1} \\
\bottomrule
\end{tabular}
}
\vspace{0.5mm}
\caption{Performance~(\%) of our method \textbf{CaCao} and other baselines with different model types on the VG-50 dataset. }
\label{base-result}
\vspace{-3mm}
\end{table*}
\begin{table}[ht]
\setlength\tabcolsep{1pt}
\resizebox{0.475\textwidth}{!}{
\begin{tabular}{ccccc}
\hline
\toprule
&Model &  PredCls mR@50/100& SGCls mR@50/100& SGDet mR@50/100\\ \hline
\multirow{6}{*}{\rotatebox[]{90}{GQA-200}}&Motif~\cite{zellers2018neural} & 16.4 / 17.1 & 8.2 / 8.6 & 6.4 / 7.7\\
&Motif + GCL\cite{dong2022stacked} & 36.7 / 38.1 & 17.3 / 18.1 & 16.8 / 18.8\\
&\textbf{Motif + CaCao~(ours)} & \textbf{37.5 / 40.5} & \textbf{19.6 / 21.9} & \textbf{17.8 / 19.6}\\
&Transformer\cite{tang2020unbiased} & 17.5 / 18.7 & 8.5 / 9.0 & 6.6 / 7.8 \\
&Transformer + GCL\cite{dong2022stacked} & \textbf{35.6} / 36.7 & 17.8 / 18.3 & 16.6 / 18.1 \\
&\textbf{Transformer + CaCao~(ours)} & 34.8 / \textbf{36.9} & \textbf{19.3 / 20.1} & \textbf{18.8 / 19.1}\\ \hline
\multirow{4}{*}{\rotatebox[]{90}{VG-1800}}&BGNN~\cite{li2021bipartite} & 1.3 / 2.4 & 0.8 / 1.4 & 0.5 / 0.9 \\ 
&Motif~\cite{zellers2018neural} & 1.7 / 2.6  &  0.9 / 1.9 & 0.6 / 1.1\\
&Motif + IETrans~\cite{zhang2022fine} & 5.1 / 8.4 & 3.6 / 5.2 & 3.1 / 4.3\\
&\textbf{Motif + CaCao~(ours)} & \textbf{10.0 / 10.8} & \textbf{4.6 / 6.3} & \textbf{4.1 / 6.2}\\ 
\bottomrule
\end{tabular}
}
\vspace{0.3mm}
\caption{Comparisons with our CaCao and other baseline methods on large-scale SGG datasets.}
% Following~\cite{zhang2022fine}, we use accuracy (Acc) and mean accuracy upon all predicate classes (mAcc) on VG-1800 dataset.}
\vspace{-1mm}
\label{largescale-result}

\end{table} 

\begin{table}[ht]
\setlength\tabcolsep{1pt}
\resizebox{0.475\textwidth}{!}{
\begin{tabular}{llcccc}
\hline
\toprule
 &\multicolumn{1}{c}{\multirow{2}{*}{Methods}} &\multicolumn{1}{c}{\multirow{2}{*}{Datasets}}&\multicolumn{3}{c}{Predicate Classification}  \\
 && &base R@50/100 & novel R@50/100 & total mR@50/100
 \\ \hline\hline
& \multicolumn{1}{l}{\multirow{3}{*}{Backbone w/o Epic~\cite{ren2015faster}}} & VG & 17.6 / 21.1  & 6.4 / 8.7 & 8.5 / 9.7\\
 & & CaCao& 17.4 / 20.4 & 7.2 / 9.2 & 8.1 / 10.4 \\
 && VG+CaCao& 17.5 / 20.9 & 11.2 / 15.8 & 13.6 / 17.7  \\
\hline
 \multirow{5}{*}{\rotatebox[]{90}{Ablations}} & \multicolumn{1}{l}{\multirow{5}{*}{\textbf{Epic}}} & VG& 22.6 / 27.2 & 7.4 / 9.7 & 10.3 / 12.6\\
 && CaCao&23.1 / 30.8 & 9.7 / 12.1 & 14.2 / 18.2   \\
 && VG+CaCao & \textbf{28.3 / 31.1} & \textbf{13.9 / 18.3}  & \textbf{16.5 / 21.8}  \\
\cline{2-6}
 &\quad w/o text-aware prompt & VG+CaCao& 16.8 / 23.1 & 12.5 / 13.9 & 13.1 / 15.4\\
 &\quad w/o vision-aware prompt & VG+CaCao&18.5 / 24.9 & 10.1 / 12.7 & 11.2 / 14.1  \\
\bottomrule
\end{tabular}
}
\vspace{0.3mm}
\caption{Performance~(\%) of our \textbf{Epic} and the backbone without Epic for open-world settings on different datasets. VG denotes the VG-50 dataset with the open-world split, VG+CaCao represents the enhanced dataset with our CaCao framework and CaCao means only use CaCao's predicates for unsupervised settings.}
\label{zs-result}
\vspace{-5.8mm}
\end{table}

\section{Experiment}
% We first consider the standard SGG setting, where we validate whether our proposed CaCao improves and benefits existing methods in a model-agnostic way. Then, we consider a more general setting to explore CaCao's generalization within open-world predicate SGG. 
% \textcolor{red}{We first consider the standard SGG setting to show our proposed CaCao's effectiveness with different baselines and expand to large-scale SGG. Then, we consider a more general setting to explore CaCao's generalization within open-world predicate SGG.} 



\subsection{Dataset and Evaluation Settings}
\noindent\textbf{Datasets.} We evaluate our proposed method for scene graph generation on the popular VG-50 benchmark similar to previous works~\cite{krishna2017visual, xu2017scene,tang2020unbiased,tang2019learning,zellers2018neural}, which consists of 50 predicate classes and 150 object classe. Furthermore, we explore more challenging datasets~(\textit{i.e.} GQA-200~\cite{hudson2019gqa, knyazev2021generative}, VG-1800~\cite{zhang2022fine}) where predicates are more diverse to
validate CaCao's generalization ability in large-scale scenarios.

\noindent\textbf{Data Split.}
For the standard SGG setting, we adopt a widely used data split following previous works~\cite{tang2020unbiased, zellers2018neural, knyazev2021generative} and expand to large-scale SGG datasets. we divide the dataset into 70\% training set, 30\% testing set, and additional 5k images for parameter tuning.
% For the standard SGG setting, we adopt a widely used data split following , which uses 70\% images with annotated relations as the training set and additional 5k validation set for parameter tuning. 
For the open-world predicate SGG setting, we first establish the related dependencies from Chen \textit{et al}~\cite{chen2019scene}. We then randomly select 70\% classes from each predicate level and assign them into the base set for training and the rest 30\% classes that contain rare predicates~(\textit{e.g.}, painted on, flying in) into the novel set for evaluation similar to other zero-shot tasks~\cite{bansal2018zero, kan2021zero,he2022towards}.
% we randomly split 70\% total predicates for training and the rest into novel classes for evaluation similar to other zero-shot tasks~\cite{kan2021zero,he2022towards}. This gives us 35 training predicate classes and 15 test predicate classes in Visual Genome~\cite{krishna2017visual}. 
To avoid disclosure of the unseen predicates, we remove all relations that contain novel predicates in the training set. Moreover, we evaluate the expansibility of our method in the unsupervised setting, in which models only use diverse predicates generated by our CaCao without any ground-truth annotations to predict unseen predicates in the open world. 

\noindent\textbf{Evaluation and Metrics.} 
Following recent works~\cite{zhang2022fine, lyu2022fine}, we evaluate our model on three widely used SGG tasks: PredCls, SGCls, and SGDet. Since the Recall@K of all predicates could be easily affected by biased distribution, it cannot precisely evaluate models' performance on long-tail distribution SGG. 
% Thus, we use Mean Recall@K~(\textbf{mR@K}) to evaluate fine-grained scene graph generation models' performance for both head and tail predicates. 
Thus, we use Mean Recall@K~(\textbf{mR@K}) to evaluate the performance of SGG models on the whole category set. 
% Since tail predicates are usually informative for image understanding, we further introduce a detailed metric Tail-R@K (Recall@K among tail 50\% predicates) to better assess those tail predicates.
We further introduce a detailed metric \textbf{Tail-R@K}~(Recall@K among tail 50\% predicates) to better assess those tail predicates, as these predicates typically provide more information for image understanding.
Besides, we use Recall@K of base predicates, novel predicates, and mean Recall@K of total predicates to evaluate the generalization ability of our method on open-world predicate SGG.

\subsection{Comparison with State of the Arts}
We report the results of our CaCao and other general SGG models for the VG-50 benchmark shown in Table~\ref{base-result}. Based on the observation of experimental results, we have summarized the following conclusions:

\textbf{Our CaCao framework can be flexibly equipped to different baseline models.} We incorporate our CaCao into three backbone models for evaluation, including Motif~\cite{zellers2018neural}, VCTree~\cite{tang2019learning}, and Transformer~\cite{tang2020unbiased}. Despite the model diversity, our CaCao can consistently improve all baseline models’ mR@K performance for all tasks that Motif+CaCao~(38.9\% \textit{v.s.} 16.2\%), VCTree+CaCao~(40.8\% \textit{v.s.} 16.1\%) and Transformer+CaCao~(43.7\% \textit{v.s.} 17.6\%) for PredCls. Also, we obtain similar performance improvements for SGCls and SGDet. Besides, we compare the ablation methods which directly use raw extra relation labels~(\textit{i.e.} only Caption Relations in Tab.~\ref{base-result}). Notably, our method Transformer+CaCao significantly surpasses its ablation method by 23.8\% in mR@20 of PredCls, demonstrating that the gain power of CaCao is mainly derived from linguistic knowledge in PLM instead of extra collected data.
% and also obtain nearly double mR@K performance on PredCls tasks. For SGCls and SGDet tasks, our CaCao achieves a similar improvement as well.

\textbf{Compared with other model-agnostic methods, our CaCao outperforms all of them in both Tail-R@K and mR@K.} Specifically, CaCao exceeds the SOTA of data enhancement models IETrans~\cite{zhang2022fine} for all three backbones with consistent improvements as 3.9\%, 5.8\%, and 3.6\% on Tail-R@20 for PredCls and 0.7\%, 2.1\%, and 7.1\% on mR@20 for Predcls. It shows that our CaCao can generate high-quality informative predicates to mitigate the long-tail distribution problem, which is conducive to fine-grained scene graph generation. It is worth noting that even when compared to SOTA methods of different model types, such as FGPL~\cite{lyu2022fine}, Motif+CaCao, VCTree+CaCao, and Transformer+CaCao still achieve significant improvements by 6.6\%, 3.0\%, and 8.7\% on mR@20 for PredCls. Besides, our CaCao can also integrate with one-stage methods~(\textit{e.g.}, SSRCNN~\cite{teng2022structured}) and achieve better performance.

% \textbf{Compared with other model-agnostic methods, especially with data enhancement model type, our CaCao outperforms all of them in both Tail-R@K and mR@K.} Specifically, Motif+CaCao, VCTree+CaCao, and Transformer+CaCao exceed Motif+IETrans~\cite{zhang2022fine}, VCTree+IETrans~\cite{zhang2022fine} and Transformer+IETrans~\cite{zhang2022fine} with consistent improvements as 3.9\%, 5.8\%, and 3.6\% on Tail-R@20 for PredCls and 0.7\%, 2.1\%, and 7.1\% on mR@20 for Predcls, demonstrating informative predicates boosted by our CaCao help models focus more on those tail predicates and benefit for fine-grained scene graph generation. It is worth noting that even compared with SOTA methods with other model types, \textcolor{red}{one-stage model} like FGPL~\cite{lyu2022fine}, we also observe significant improvements in nearly all metrics, especially mR@20 of PredCls as 6.6\%, 3.0\%, and 8.7\% for three backbones. The possible reason is that our CaCao provides diverse predicates based on contexts. 

\textbf{Our method can distinguish fine-grained predicates and achieve a large margin of improvements on these predicate predictions.} 
% sota in model-agnostic
Notably, our modal-agnostic approach can also achieve competitive performance compared with strong specific baselines~(\textit{e.g.}, 43.7\% \textit{v.s.} 39.7\% on mR@100 for PredCls), demonstrating the superiority of our proposed model. For an intuitive illustration of CaCao's discriminatory power among hard-to-distinguish predicates, we visualize the PredCls results of fine-grained predicates as shown in Figure~\ref{reuslts-compare}. We observe that Transformer+CaCao obtains overall improvement on most predicates. One possible reason is that CaCao has been exposed to various informative predicates, strengthening its discriminatory power against fine-grained predicates. More qualitative studies and detailed analyses are shown in \textbf{Appendix D}.
% , such as \textit{parked on}, \textit{painted on}, and \textit{flying in}. It is mainly because our CaCao can distinguish these general predicates and map them to the corresponding fine-grained predicates~(\textit{e.g.},~on$\rightarrow$walking on, has$\rightarrow$~(with, path of)), thus we can obtain relatively unbiased results on all predicates. 

% \textbf{Our method can distinguish fine-grained predicates and achieve a large margin of improvements on tail predicates.} Considering the diversity of predicates, we give more detailed studies of tail predicates to obtain deep insights into CaCao. We compare our CaCao with the original Transformer~\cite{tang2020unbiased} on the PredCls task and observe a remarkable 22.4\% improvement of tail-R@100. For an intuitive illustration of CaCao's disambiguation for fine-grained predicates, we visualize the PredCls performance of fine-grained predicates ranked by their frequencies as shown in Figure~\ref{reuslts-compare}. We observe that Transformer+CaCao obtained overall improvements on most of the fine-grained predicates. The prediction of tail classes with few training samples, especially those fine-grained predicates, is being promoted in Transformer+CaCao, such as \textit{parked on}, \textit{walking on}, \textit{painted on} and \textit{flying in}. It is mainly because our CaCao can distinguish these general predicates and map them to the corresponding fine-grained predicates~(\textit{e.g.},~on$\rightarrow$walking on, has$\rightarrow$~(with, path of)), thus we can obtain relatively unbiased results on all predicates. Detailed prediction probability of fine-grained predicates and more various predicate distribution from CaCao are provided in Appendix.
\begin{figure}[ht]\vspace{-0.25cm}
    \centering
   \includegraphics[width=1.0\linewidth]{figures/result-2.pdf}
   \caption{Diverse fine-grained predicates performance comparison between base Transformer~\cite{tang2020unbiased} and our enhanced Transformer+CaCao on the VG-50 dataset.}
   \label{reuslts-compare}
   \vspace{-0.45cm}
\end{figure}
\begin{figure*}[ht]
    \centering
   \includegraphics[width=0.98\linewidth]{figures/visualization.pdf}
   
   \caption{Visualization of base Transformer model~\cite{tang2020unbiased}, Transformer equipped with our CaCao framework for predicate enhancement and our Epic equipped with CaCao framework for open-world predicate SGG.}
    \vspace{-0.4cm}
   \label{visualization}
\end{figure*}
\subsection{Generalization to Large-Scale SGG}
Table~\ref{largescale-result} summarizes the results of our CaCao and other baselines on large-scale datasets. Overall, our method can successfully generalize to more challenging datasets. Notably, simply resampling~(\textit{i.e.} BGNN~\cite{li2021bipartite}) can not work well in such exacerbated scenarios, where much more predicates have less than 10 samples. In contrast, our CaCao utilizes abundant corpus knowledge to balance diverse tail predicates and surpasses other baselines for almost large-scale SGG tasks. For quantitative comparison, our CaCao can obtain consistent improvement as 8.2\%, 4.4\%, and 5.1\% on mR@100 for PredCls, SGCls, and SGDet in VG-1800 and largely enhance the unbiased predictions in GQA-200~(\textit{e.g.}, 11.9\% improvement with Motif~\cite{zellers2018neural} and 11.7\% improvement with Transformer~\cite{tang2020unbiased} on SGDet mR@100).

\subsection{Expansibility to Open-World Predicate SGG}
Inspired by the abundant fine-grained predicates produced by CaCao, we also validate our CaCao with Epic on the open-world setting for the base, novel, and total PredCls tasks to show its expansibility to open-world predicate scene graph generation. Since current SGG models cannot solve this challenging task, we verify the performance of CaCao and Epic by comparing them with the naive backbone and present the comparison results fully in Table \ref{zs-result}.

% Our Epic not only achieves better results over novel classes of predicates but keeps the consistent performance of base classes in all datasets compared with the backbone without Epic. 



Empirically, the Epic and CaCao not only achieve the best results over novel predicates but also obtain incredible benefits to base predicate categories, for example, \textbf{3.9\%} improvement on the base R@100.
It reflects that our Epic improves model learning ability and reduces over-fitting to base predicates for open-world predicate SGG through entangled instance-level prompts from correlated image regions. With the help of diverse predicates from CaCao, our Epic obtained a significant margin improvement of 8.6\% on novel R@100 for PredCls, verifying that CaCao can bring out many informative predicates for better generalization. 

Surprisingly, even without any ground truth data from VG, our method performs better with only the rich predicates generated by CaCao than with VG alone. It indicates that informative predicates generated by CaCao are diverse and contain richer information to help the model learn predicate semantic features for predicate generalization.
\begin{figure}[ht]
    \centering
     \vspace{-0.1cm}
   \includegraphics[width=1.0\linewidth]{figures/mR-k-results.pdf}
   \caption{The influence of different proportions k\% of boosted predicates for different mR@K and tail-R@100~(\textbf{red line}).}
      \vspace{-0.4cm}
   \label{different proportions}
\end{figure}

% \begin{table}[ht]
% \vspace{-0.35cm}
% \resizebox{0.48\textwidth}{!}{
% \begin{tabular}{lccccc}
% \hline
% \toprule
% \multicolumn{1}{c}{\multirow{2}{*}{Methods}} & 
% \multicolumn{4}{c}{Predicate Prediction Accuracy}  \\ 
%   & ASCL& Soft/Hard Prompts& VPT & A@1/10 $\uparrow$ \\ \hline
%  Backbone & \XSolidBrush  & Hard & \XSolidBrush &   0.08 / 0.21 \\ \hline
% \quad+ASCL & \Checkmark & Hard & \XSolidBrush &  0.12 / 0.36 \\
% \quad+TPT & \XSolidBrush & Soft & \XSolidBrush & 0.22 / 0.52  \\ 
% \quad+VPT  & \XSolidBrush & Hard & \Checkmark &  0.28 / 0.56  \\ \hline
% \quad+ASCL+TPT & \Checkmark & Soft & \XSolidBrush & 0.25 / 0.68  \\
% \quad+TPT+VPT & \Checkmark & Hard & \Checkmark &  0.38 / 0.74 \\ 
% \quad+ASCL+VPT  & \XSolidBrush & Soft & \Checkmark &  0.47 / 0.80  \\ \hline
% \textbf{CaCao}  & \Checkmark & Soft & \Checkmark &  \textbf{0.74 / 0.92}  \\ \hline
% \end{tabular}}
% \vspace{0.3mm}
% \caption{Ablation study on each module of our proposed CaCao. \textbf{ASCL}, \textbf{TPT} and \textbf{VPT} denote Adaptive Semantic Cluster Loss, Textual Prompts and Visually-prefixed Prompts, respectively.}
% \label{ablation}
% \vspace{-0.3cm}
% \end{table}
\begin{table}[ht]
\vspace{-0.2cm}
\setlength\tabcolsep{11pt}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{clccccc}
\hline
\toprule
&\multicolumn{1}{c}{\multirow{2}{*}{Methods}} & 
\multicolumn{4}{c}{Predicate Prediction Accuracy}  \\ 
&  & ASCL& TPT& VPT & A@1/10 $\uparrow$ \\ \hline
 1 &Backbone & \XSolidBrush  & \XSolidBrush & \XSolidBrush &   0.08 / 0.21 \\ \hline
2&\quad w/o ASCL & \XSolidBrush & \Checkmark & \Checkmark &  0.38 / 0.74 \\ 
3&\quad w/o TPT  & \Checkmark & \XSolidBrush & \Checkmark &  0.47 / 0.80  \\
4&\quad w/o VPT & \Checkmark & \Checkmark & \XSolidBrush & 0.25 / 0.68  \\

\hline
5&\textbf{CaCao}  & \Checkmark & \Checkmark & \Checkmark &  \textbf{0.74 / 0.92}  \\ \hline
\end{tabular}}
\vspace{0.3mm}
\caption{Ablation study with metrics predicate labels prediction accuracy~(\textbf{A@1/10}) on each module of our proposed CaCao.}
% \caption{Ablation study on each module of our proposed CaCao. \textbf{ASCL}, \textbf{TPT} and \textbf{VPT} denote Adaptive Semantic Cluster Loss, Textual Prompts and Visually-prefixed Prompts, respectively.}
\label{ablation}
\vspace{-0.3cm}
\end{table}
\subsection{In-depth Analysis}
% To deeply investigate our CaCao and Epic, we further study the ablation variants of different modules. 

\noindent\textbf{Visually-Prompted Language model.} To deeply investigate our CaCao, we further study the ablation variants of different modules in Table~\ref{ablation}. Specifically, we train the following ablation models. 1) w/o ASCL: we remove the Adaptive Semantic Cluster Loss~(ASCL). 2) w/o TPT: we remove the Textual Prompt~(TPT) in prompt templates. 3) w/o VPT: we remove the visually-prefixed Prompt~(VPT). 

The results of Row 2 indicate that adaptive semantic cluster learning is crucial for diverse fine-grained predicate prediction. Also, the results of Row 3 validate the importance of learnable prompts on textual semantic understanding. Furthermore, Row 4 suggests that the main performance gain comes from these visual semantics contained in images~(0.25 / 0.68 $\rightarrow$ 0.74 / 0.92).


% As shown in Table~\ref{ablation}, neither using any component of CaCao individually nor joining two modules together can predict exact predicates well for boosting. The reason is that without context, the model cannot generate proper predicates from semantics; even if the context is introduced, it may collapse into specific categories without adaptive adjustment strategies. At last, when we combined all the components in CaCao, we observed a significant improvement, demonstrating the effectiveness of each component and joint training. By comparing ”hard/soft labels” in Tab. 3 (0.08 / 0.21 v.s. 0.28 / 0.56), we find that results with learnable prompts in better performance.

\noindent\textbf{Influence of k\% Boosting Predicates.} As shown in Figure~\ref{different proportions}, with the increase of k\% boosting predicates, the mean recall and the tail recall gradually increased in the form of overall growth. The phenomenon indicates that predicates enhanced by CaCao are all informative and consistently bring enhancements to the existing SGG models. 

\noindent\textbf{Adaptive Semantic Cluster Learning.}
Since the quality of clustering is critical for prompt tuning, we further explore different similarity thresholds for clustering and get corresponding results. We obverse that the proper threshold can aggregate close relations together~(\textit{e.g.}, see, see through) and distinguish others (\textit{e.g.}, see, playing), thus obtaining optimal 0.74 A@1 with a 0.7 predicate similarity threshold.

\noindent\textbf{Entangled Cross-Modal Prompts.} We explore the effectiveness of the text-aware prompt and the vision-aware prompt in Epic, shown in the last two lines of Table~\ref{zs-result}. We gradually removed these entangled prompts and observed a significant decrease in performance for both base and novel classes without either prompt from another modality. These findings suggest mutual hints between the two modalities are necessary to extract associated linguistic semantics and image features for open-world predicate learning.
% It indicates that only through the mutual hints of the two modalities can we obtain the associated predicate semantics and image regions from CaCao, which is helpful for us to learn the predicate features better.

\noindent\textbf{Human Evaluation.} A key element of effective SGG boosting is to obtain high quality data. Thus, we conduct human evaluation for automatic labels from CaCao and find the radio of reasonable fine-grained predicates is \textbf{73\%}. Please refer to \textbf{Appendix E} for more details.

\noindent\textbf{Visualization Results.}
In Figure~\ref{visualization}, we visualize the enhancement SGG benefits from CaCao compared with the base scene graph and further present open-world predicate SGG visualization results by CaCao+Epic, intuitively illustrating the effectiveness of our proposed CaCao and Epic.
% For the left image, Transformer\cite{tang2020unbiased} only predict those coarse-grained predicates, while our method predict fine-grained predicates such as \textit{parked on} or \textit{in front of} instead of \textit{near}. For open-world, our Epic with CaCao predicts more informative and open predicates, such as \textit{is placing in} or \textit{perched up on}.
\section{Conclusions}
In this work, we propose an automatic boosting framework CaCao that exploits linguistic knowledge from pre-trained language models to enrich existing datasets in a low-resource way.
We tackle the long-tail issue of SGG with the help of abundant informative predicates from CaCao and generalize to open-world predicate learning with the entangled cross-modal prompt design based on VL models.  
Our extensive experiments on three datasets illustrate the significant improvement of our CaCao on fine-grained scene graph generation and open-world generalization capability. 
% The proposed CaCao can generate a variety of diverse predicates with the help of the knowledge in the pre-training model. 
% The limitation for CaCao is that the quality of predicates depends on the well-pre-trained object detector. 


%------------------------------------------------------------------------
% \section{Final copy}

% You must include your signed IEEE copyright release form when you submit
% your finished paper. We MUST have this form before your paper can be
% published in the proceedings.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}
\clearpage
	
	

\appendix
\section{Overview}
In this supplementary material we present:

\begin{itemize}
	\item The detailed statistics of the current dataset and boosted data from CaCao~(Section \ref{1}).
	
	\item More detailed analysis of CaCao~(Section \ref{2}).
	
	\item More quantitative studies for predicates~(Section \ref{3}).
	
	\item Human evaluation of boosted predicates~(Section \ref{4}).

        \item Implementation details~(Section \ref{5}).
 
	\item Additional experimental results~(Section \ref{6}).
	
	\item Additional examples~(Section \ref{7}).
	
	
\end{itemize}

\section{Dataset  Statistics}\label{1}
Table~\ref{t1-1} and \ref{t1-3} show the coarse-grained predicates and fine-grained predicates with the number of training instances for each predicate in the Visual Genome dataset\cite{krishna2017visual}. Table~\ref{t1-2} and \ref{t1-4} show the coarse-grained predicates and fine-grained predicates with the number of training instances for each predicate after cross-modal boosting by our CaCao. We can observe that CaCao increases dataset scale, especially the tail predicates, which significantly alleviates the long-tail distribution problem in SGG.  

For large-scale benchmark SGG, GQA~\cite{hudson2019gqa} contains 113K images and over 3.8M relation annotations. In order to ensure the quality and representativeness of the dataset, we perform a manual cleaning process to remove annotations that had poor quality or ambiguous meanings following prior works~\cite{dong2022stacked}. We finally select the top 200 object classes and top 100 predicate classes as the GQA-200 split like VG-50 to explore the generalization ability of CaCao in large-scale SGG. 

Besides, VG-1800~\cite{zhang2022fine} is another large-scale benchmark dataset, which filters out spelling errors and unreasonable relations, ultimately preserving 70,098 object classes and 1,807 predicate classes for more challenging scenarios. For each predicate category in VG-1800, there exist over 5 samples on the test set to provide a reliable evaluation.
	\begin{table*}[ht]
	%\setlength\tabcolsep{8pt}

		\centering
		\resizebox{1.0\textwidth}{!}{
			\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
				\hline
				\textbf{Coarse-grained Predicates}&above& across& against& along& and& at& behind& between& for& from\\	\hline	
				\textbf{Number of Predicates}&47341& 1996& 3092& 3624& 3477& 9903& 41356& 3411& 9145& 2945 \\ \hline
                \textbf{Coarse-grained Predicates}& has& in& in front of& near& of& on& over& to& under& with \\ \hline
				\textbf{Number of Predicates}& 277936& 251756& 13715& 96589& 146339& 712409& 9317& 2517& 22596& 66425 \\ 
				\hline 
			\end{tabular}
		}
		
            \vspace{0.5mm}
		\caption{Statistics of \textbf{coarse-grained predicates} for the VG-50.}
		\label{t1-1}
	\end{table*}
	\begin{table*}[!htb]
	%\setlength\tabcolsep{8pt}

		\centering
		\resizebox{1.0\textwidth}{!}{
			\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
				\hline
				\textbf{Coarse-grained Predicates}&above& across& against& along& and& at& behind& between& for& from\\	\hline	
				\textbf{Number of Predicates}&47829& 60320& 88810& 3722& 10254& 38305& 43345& 94138& 10643& 17149\\ \hline
                \textbf{Coarse-grained Predicates}& has& in& in front of& near& of& on& over& to& under& with \\ \hline
				\textbf{Number of Predicates}& 300695& 296474& 24950& 141494& 197294& 787048& 12820& 8672& 43535& 93078 \\ 
				\hline 
                
			\end{tabular}
		}
            \vspace{0.5mm}
		\caption{Statistics of \textbf{coarse-grained predicates} for the boosted VG-50 from CaCao.}
		\label{t1-2}
	\end{table*}

 \begin{table*}[!htb]
	%\setlength\tabcolsep{8pt}

		\centering
		\resizebox{1.0\textwidth}{!}{
			\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
				\hline
				\textbf{Fine-grained Predicates}&attached to& belonging to& carrying& covered in& covering& eating& flying in& growing on& hanging from& holding\\	\hline	
				\textbf{Number of Predicates}&10190& 3288& 5213& 2312& 3806& 4688& 1973& 1853& 9894& 42722\\ \hline
                \textbf{Fine-grained Predicates}& laying on& looking at& lying on& made of& mounted on& on back of& painted on& parked on& part of& playing \\ \hline
				\textbf{Number of Predicates}& 3739& 3083& 1869& 2380& 2253& 1914& 3095& 2721& 2065& 3810 \\ \hline
                \textbf{Fine-grained Predicates}& riding& says& sitting on& standing on& using& walking in& walking on& watching& wearing& wears \\ \hline
				\textbf{Number of Predicates}& 8856& 2241& 18643& 14185& 1925& 1740& 4613& 3490& 136099& 15457 \\
				\hline 
			\end{tabular}
		}
            \vspace{0.5mm}
		\caption{Statistics of \textbf{fine-grained predicates} for the VG-50.}
		\label{t1-3}
	\end{table*}
 \begin{table*}[!htb]
	%\setlength\tabcolsep{8pt}

		\centering
		\resizebox{1.0\textwidth}{!}{
			\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
				\hline
				\textbf{Fine-grained Predicates}&attached to& belonging to& carrying& covered in& covering& eating& flying in& growing on& hanging from& holding\\	\hline	
				\textbf{Number of Predicates}&80066& 20858& 79148& 54015& 17879& 100241& 6752& 20290& 90025& 68378\\ \hline
                \textbf{Fine-grained Predicates}& laying on& looking at& lying on& made of& mounted on& on back of& painted on& parked on& part of& playing \\ \hline
				\textbf{Number of Predicates}& 31783& 150817& 21944& 27189& 62583& 20628& 36882& 68218& 14727& 20789\\ \hline
                \textbf{Fine-grained Predicates}& riding& says& sitting on& standing on& using& walking in& walking on& watching& wearing& wears \\ \hline
				\textbf{Number of Predicates}& 62625& 22273& 68474& 70311& 63777& 32956& 38853& 235425& 258332& 60328 \\
				\hline 
			\end{tabular}
		}
            \vspace{0.5mm}
		\caption{Statistics of \textbf{fine-grained predicates} for the VG-50.}
		\label{t1-4}
	\end{table*}
\section{Cross-Modal Predicate Boosting}\label{2}

\subsection{Data Preprocessing} We first collect as many detailed pictures as possible from the Internet~(\textit{i.e.} CC3M, COCO caption) as the original data for training and get nearly 80k images and 2k predicate categories with corresponding descriptions. Then we conduct semantic analysis of the corresponding description statement of each image through \textbf{Stanford CoreNLP} and preserve those informative chunks~(\textit{i.e.} V, P, N, NP, and VP) to extract fine-grained triplets contained in captions. and Since the raw data contains much noise, we further design heuristic rules~(\textit{i.e.} corpus co-occurrence frequency, layer depth of lexical analysis) to filter out predicates that are not informative or misspelling automatically instead of handling them manually. We finally eliminate those coarse-grained predicates and preserve 585 categories of diverse predicates to obtain informative~\textless subject, predicate, object\textgreater~relationships, which nearly cover most of the common situations in the real world, as shown in Table~\ref{t2}. Since the VG dataset also contains some fine-grained predicates, there are 27 categories of informative predicates we obtained have overlap with them.

 

 \begin{table*}[pbt]     %[pbt]：表格最优位置
\centering                % 表格居中
\vspace{0.15cm}      % 表格与表标题之间的距离
\begin{sloppypar}
\begin{tabular}{|c|c|c|}
\hline
Image & Description & Extracted Relationships \\ \hline
\multirow{5}*{\begin{minipage}[b]{0.2\columnwidth}
		\raisebox{-0.5\height}{\includegraphics[width=0.9\linewidth]{figures/COCO_train2014_000000318556.jpg}}
	\end{minipage}}  & \multirow{5}*{A clock that blends in with the wall hangs in a bathroom.} & ('clock', 'blends in with', 'wall') \\ 
                &&('clock', 'in with', 'wall') \\
                &&('clock', 'with', 'wall') \\
                &&('clock', 'hangs in', 'bathroom')\\
                &&('clock', 'in', 'bathroom')         \\ \hline               
\multirow{3}*{\begin{minipage}[b]{0.2\columnwidth}
		\raisebox{-0.5\height}{\includegraphics[width=1.1\linewidth]{figures/COCO_train2014_000000134754.jpg}}
	\end{minipage}}  & \multirow{3}*{A couple at the beach walking with their surf boards.} & ('couple', 'at', 'beach') \\ 
                &&('couple', 'walking with', 'their-surf') \\
                &&('couple', 'with', 'their-surf')      \\ \hline 
\multirow{4}*{\begin{minipage}[b]{0.2\columnwidth}
		\raisebox{-0.5\height}{\includegraphics[width=0.7\linewidth]{figures/COCO_train2014_000000158869.jpg}}
	\end{minipage}}  & \multirow{4}*{A yellow and black bird standing on and hanging with a bike rack.} & ('black-bird', 'on', 'bike-rack') \\
                    && ('yellow-bird', 'on', 'bike-rack') \\
                &&('black-bird', 'standing on', 'bike-rack') \\
                &&('black-bird', 'hanging with', 'bike-rack')      \\ \hline 
                
\end{tabular}
\end{sloppypar}
\vspace{0.5mm}
\caption{The examples of ~\textless subject, predicate, object\textgreater~ extraction from raw data for prompt tuning.}
\label{t2}
\end{table*}

\subsection{Cross-Modal Prompt Tuning}
In the visually-prompted language model module, we use Vision Transformer~\cite{dosovitskiy2020image} as the vision encoder to obtain image features and design a transformer-layer $h_\theta$ to map the vision encoder's output to sequence embeddings for \textit{visually-prefix prompts}. For language-level, we create 30 lengths of learnable prefix vectors as continuous soft prompts [\textit{P}] and optimize them during prompt tuning. Then we conduct visually-prompted templates and feed them into BERT~\cite{devlin2018bert}, which is frozen in the entire prompt tuning process. During training, we use a softmax classifier to predict the predicate tokens for each masked position and use the cross-entropy between $q(y_i|X_i)$ and the true~(one-hot) distribution of training examples as loss. Note that we update the textual context vectors together with the transformer-layer’s parameters $\theta$ for visually-prompted template optimization.  

\begin{table}[ht]
\resizebox{0.475\textwidth}{!}{
\begin{tabular}{c|cccccc}
\hline
\toprule
 Similarity threshold & w/o ASCL & 0.1 & 0.3 & 0.5 & 0.7 & 0.9  \\ \hline
 A@1 & 0.38 & 0.39 & 0.57 & 0.63 & \textbf{0.74} & 0.48 \\
\bottomrule
\end{tabular}
}
\vspace{0.5mm}
\caption{The influence of different predicate similarity threshold for cross-modal prompt tuning in CaCao.}
\label{k_similarity}
\end{table}
\begin{table}[ht]
\resizebox{0.45\textwidth}{!}{
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Predicted Predicates & Semantic Co-reference Predicates \\ \hline
`wearing'& [`wearing', `worn on', `carrying'] \\\hline 
`holding'& [`holding', `carrying', `pulling'] \\\hline
`next to'& [`next to', `sitting next to', `standing next to'] \\\hline
`standing in'& [`standing in', `standing on', `standing by']\\\hline
`below'& [`below', `beneath', `standing behind']\\\hline
`flying in'& [`flying', `flying in', `floating in']\\\hline
`sitting on'& [`sitting at', `sitting in', `is seated on']\\\hline
`hang on'& [`hang on', `hanging on', `hanging from']\\\hline
`covered in'& [`covered in', `covered with', `covered by']\\\hline
`surrounded by'& [`surrounded by', `covered by', `pulled by'] \\\hline
`walks through'&[`walks through',`is passing through',`passed by']\\\hline
    
  \end{tabular}
  }
  \vspace{2mm}
    \caption{The examples of top clustering results for semantic co-reference predicates}\label{coreference}
\end{table}
\subsection{Adaptive Semantic Cluster Loss}
We list more semantic co-reference words and some clustering results as shown in the table~\ref{coreference}, such as he ``walks through"/``is passing through"/``passed by" a street may correspond to same predicate ``walking on. 

\noindent\textbf{Further analysis.} 
Since multiple expression words may correspond to the same predicate, direct Cross-Entropy Loss will excessively inhibit the expressive power of the pre-trained language model and thus overfit into specific head predicates. Therefore, we cluster different predicates according to semantics and reduce penalties for those predicates expressions with similar semantics to promote the diversity of predicates. However, naive clustering loss optimization still leads to excessive enhancement of some specific predicates due to biased prediction. Thus, we introduce adaptive semantic cluster loss, which adaptively adjust the cluster-based CE loss during training.
% Since the high dependence of predicates on triplets, we consider the predicate category with its most common subject and object entity and represent predicates as the sum of GloVe vectors\cite{pennington2014glove} corresponding to their subject, predicate, and object. We then cluster predicates in each dataset by K-Means and initialize the number of centroids by the similarity threshold between each predicate category. We reduce the softmax punishment for those predicates in the same cluster to prevent highly correlated predicates from over-suppressing. Therefore, we finally obtain more diverse predicates from our CaCao and improve the discriminatory power among fine-grained predicates of SGG models.

\noindent\textbf{Further experiments.} 
Since the quality of clustering is critical for following representation learning in CaCao, we further explore the influence of different predicate similarity predicates on the quality of fine-grained predicate generation. We use A@1/10 as the ablation experiment metrics to show the effectiveness of fine-grained predicate generation. 

Table~\ref{k_similarity} reports the corresponding results of different similarity threshold between each predicate embedding. We obverse that too low or too high similarity threshold will reduce the accuracy of predicate prediction. The possible reason is that too low similarity arbitrarily aggregate nearly all predicates into the same cluster and too high similarity regards each predicate individually, that may lead to incorrect clusters. Thus, appropriate threshold selection obtains optimal \textbf{0.74} A@1 with \textbf{0.7} similarity threshold.

\subsection{Fine-Grained Predicate Boosting}
In Figure~\ref{original-distribution} and~\ref{open-distribution}, we show the predicate distributions of the standard SGG dataset and open-world boosted data from CaCao. To boost predicates into the current scene graph generation, we need to establish the mapping from diversity predicate labels to target predicates, as shown in Table~\ref{t5}. First, we consider a simple hierarchy structure of predicate semantics based on lexical analysis and establish mappings between CaCao and target predicates at each level by cosine similarity of triplet embeddings. We use the BERT~\cite{devlin2018bert} embedding layer and obtain triplet embeddings by the sum of embeddings in (s, p, o). We then select the most infrequent predicate from different mapping levels as the final enhanced predicate. Notably, we only boost those unannotated object pairs that have overlaps in scene graphs to avoid dropping out the base semantic information.

Moreover, we notice that there exists ambiguity and overlap between coarse-grained predicates and fine-grained predicates in fact. We further create the mapping between fine-grained predicates and coarse-grained predicates based on the semantics between the predicates, as shown in Table~\ref{t4}. We filter out those low-confidence informative predicates and map them into general predicates as predicted results
\section{More Qualitative Studies for Predicates}\label{3}
In Figure~\ref{f1}, we present the prediction distribution of our method CaCao and compare with the baseline model Transformer~\cite{tang2020unbiased}. We observe that the prediction of fine-grianed categories with few training samples is promoted in Transformer+CaCao, such as \textit{hanging from}, \textit{flying in}, \textit{standing on} and \textit{walking on}.  For Transformer~(CaCao), the proportions of correct relation prediction distribution rise from 8\% to 40\% and 3\% to 56\% compared with Transformer for `\textit{flying in}' and `\textit{walking on}', respectively. Meanwhile, Transformer\cite{tang2020unbiased} fails to distinguish among these fine-grained predicates, arbitrarily tending to predict those general head predicates. It is mainly because our CaCao has been exposed to various informative predicates, strengthening its discriminatory power against fine-grained predicates and coarse-grained predicates~(\textit{e.g.},~on$\leftrightarrow$walking on, has$\leftrightarrow$~(with, path of)). Accordingly, the results validate our CaCao’s effectiveness of discriminatory ability against fine-grained predicates to deal with the long-tail distribution problem in SGG.

\section{Human Evaluation}\label{4}
By enhancing the existing dataset through CaCao, we expand the original 591767 relationships to 3280942 relationships, obtaining more diversified fine-grained relationships. We perform the human evaluation of the enhanced fine-grained predicates in Table~\ref{t6}, verifying that the predicates generated from CaCao conform to the image description and are informative for open-world predicate scene graph generation. We randomly select 100 images with 545 base relationships and 3543 novel relationships to verify that the predicates in these enhanced relationships are correct and informative. 

\section{Implementation Details}\label{5}
Following previous works, we used a pre-trained Faster R-CNN~\cite{ren2015faster} to obtain object proposals and image features for standard SGG setting. We first train an object detector on the target dataset and fix the detector parameters, then we train SGG models combined with the boosting relationships from CaCao to speedup training convergence. Specifically, we set the image batch size is 16, and the base learning rate is 0.001 with WarmupMultiStepLR. We optimize all models with Cross-Entropy Loss by an SGD optimizer. We incorporate our CaCao into SGG baselines, which parameters are followed to previous works\cite{tang2020unbiased,tang2019learning,zellers2018neural}. The models are trained for the first 16000 batch iterations on the original dataset. After obtaining the current predicate prediction distribution, the SGG model is refined and trained on the informative predicate for another 16000 batch iterations with reweighting strategy. For GQA-200 and VG-1800, we change the number of object classes and predicate classes respectively. Moreover, we adopt 80000 batch iterations for further training in large-scale SGG and use dropout regularization to prevent overfitting to enhanced predicates.

For open-world predicate SGG, We used CLIP~\cite{radford2021learning} as the backbone to extract visual features and Glove~\cite{pennington2014glove} to obtain the embedding of predicates. In order to enable the pre-training model to perceive the scene graph information, we first fine-tune the model through the classification process on the base class predicates. Then we froze the parameters of the fine-tuned model. To get the unified representation for entangled cross-modal prompts, we design the text-to-vision and vision-to-text projections, which consist of a layer of activation functions and a linear layer. We extract matching predicates and corresponding subject-object pairs from the ground truth as positive samples and others as negative samples to construct learning procedure for open-world predicate SGG. We use the InfoNCE Loss and set the temperature $\tau$ as 0.9 to update two lightweight networks' parameters $(\theta_1;\theta_2)$. We run all experiments with three GeForce
RTX 3090 24G GPUs.

% \subsection{Evaluation Mapping}
% In this paper, we argue that there exists ambiguity and overlap between coarse-grained predicates and fine-grained predicates in fact.  Therefore, we further establish predicate mapping from information predicate to general predicate, so that the SGG model can maintain good performance for the overall predicate. We create the mapping table based on the semantics between the predicates, as shown in Table~\ref{t4}. We select all the fine-grained predicate classes whose prediction scores are higher than the ground truth label and transfer those predicates to their mapping set to get the corresponding general predicates.





	\begin{table*}[!htb]
	\setlength\tabcolsep{10pt}

		\centering
		\resizebox{1.0\textwidth}{!}{
			\begin{tabular}{ccccc}
				\hline
				&\textbf{Total Predicate} & \textbf{True Predicate} & \textbf{Fine-grained Predicate} & \textbf{Coarse-grained Predicate} \\	\hline
            Original & 545 & 545 & 119~(21.8\%) & 426~(78.2\%) \\ 
            \textbf{CaCao} & 3543 & \textbf{2427~(68.5\%)} & \textbf{1781~(73.4\%)} & \textbf{646~(26.6\%)}\\ \hline
            Overall & 4088 & \textbf{2972~(72.7\%)}& \textbf{1900~(63.9\%)} &\textbf{1072~(36.1\%)}\\
				\hline 
                
			\end{tabular}
		}
  \vspace{0.5mm}
		\caption{Human evaluation for the accuracy and variety of boosting predicates from CaCao.}
		\label{t6}
	\end{table*}
\begin{table*}[ht]
\setlength\tabcolsep{10pt}
\begin{tabular}{cccccc}
\toprule
&\multicolumn{1}{c}{\multirow{2}{*}{Model Type}} & \multirow{2}{*}{Methods}  & \multicolumn{3}{c}{Scene Graph Detection} \\ 
 & & & R@50/100 $\uparrow$ & mR@50/100 $\uparrow$ & F@50/100 $\uparrow$ \\ 
 \hline
&\multicolumn{1}{c}{\multirow{3}{*}{Specific}} & \multicolumn{1}{l}{BGNN~\cite{li2021bipartite}}  & 31.0 / 35.8& 10.7 / 12.6 &15.9 / 18.6 \\
&& \multicolumn{1}{l}{SVRP~\cite{he2022towards}} &  31.8 / 35.8 & 10.5 / 12.8 & 15.8 / 18.9 \\
&& \multicolumn{1}{l}{DT2-ACBS~\cite{desai2021learning}} &  15.0 / 16.3 & 22.0 / 24.0 & 17.8 / 19.4 \\
\hline\hline

&\multicolumn{1}{c}{\multirow{2}{*}{One-stage}} & \multicolumn{1}{l}{SSRCNN~\cite{teng2022structured}} & 23.7 / 27.3 &  18.6 / 22.5 &  20.8 / 24.7\\
&&\multicolumn{1}{l}{\quad\textbf{+CaCao~(ours)}} & 25.4 / 30.0 & \textbf{18.7 / 23.1} & \textbf{21.5 / 26.1}\\
\hline\hline

\multirow{10}{*}{\rotatebox[]{90}{Model-Agnostic strategy}}& & \multicolumn{1}{l}{Motif~\cite{zellers2018neural}} &   31.0 / 35.1 & 6.7 / 7.7 & 11.0 / 12.6   \\
&\multicolumn{1}{c}{Resample} & \multicolumn{1}{l}{\quad+Resample~\cite{burnaev2015influence}} &   30.5 / 35.4  &  8.2 / 9.7 & 12.9 / 15.2 \\ 
&\multicolumn{1}{c}{\multirow{4}{*}{Reweight}}& \multicolumn{1}{l}{\quad+Reweight~\cite{wang2021seesaw}} &  24.4 / 29.3 & 10.5 / 13.2 & 14.7 / 18.2\\
&& \multicolumn{1}{l}{\quad+CogTree~\cite{yu2020cogtree}} & 20.0 / 22.1 & 10.4 / 11.8 & 13.7 / 15.4\\
&&\multicolumn{1}{l}{\quad+FGPL~\cite{lyu2022fine}} &21.3 / 24.3 & 15.4 / 18.2 & 17.9 / 20.8\\
&&\multicolumn{1}{l}{\quad+GCL~\cite{dong2022stacked}} & 18.4 / 22.0 &16.8 / 19.3& 17.6 / 20.6\\
&\multicolumn{1}{c}{Causal Rule}& \multicolumn{1}{l}{\quad+TDE~\cite{tang2020unbiased}} &  16.9 / 20.3 & 8.2 / 9.8 & 11.0 / 13.2\\ \cline{2-6}
&\multicolumn{1}{c}{\multirow{4}{*}{Data Enhancement}}& \multicolumn{1}{l}{\quad+Only Caption Relations} & 20.3 / 25.0 & 8.2 / 10.0 & 11.7 / 14.3\\
&&\multicolumn{1}{l}{\quad+DLFE~\cite{chiou2021recovering}} &  25.4 / 29.4 & 11.7 / 13.8 & 16.0 / 18.8\\
&&\multicolumn{1}{l}{\quad+IETrans~\cite{zhang2022fine}} &\cellcolor{lightgray}23.5 / 27.2 & \cellcolor{lightgray}15.5 / 18.0 & \cellcolor{lightgray}18.7 / 21.7 \\ 
&&\multicolumn{1}{l}{\quad\textbf{+CaCao~(ours)}} & \cellcolor{lightgray}\textbf{24.4 / 29.1} & \cellcolor{lightgray}\textbf{17.1 / 20.0} & \cellcolor{lightgray}\textbf{20.5 / 23.7} \\
\bottomrule
\end{tabular}
\vspace{3mm}
\caption{Performance~(\%) of our method \textbf{CaCao} and other baselines with different model types for both \textbf{head} and \textbf{tail} categories on VG-50 dataset.}
\label{head-result}
\end{table*}
\begin{table*}[ht]
		\centering
		\begin{tabular}{cc|lccc}
			\hline
\multicolumn{1}{c}{\multirow{2}*{Models}}&&\multicolumn{1}{c}{\multirow{2}*{Method}}&\multicolumn{3}{c}{\textsl{Scene Graph Detection~(SGDet)}} \\
       & &&zR@20&zs@50&zs@100  \\
			\hline\hline
       \multicolumn{1}{c}{\multirow{3}*{Motif~\cite{zellers2018neural}}}&& Baseline & 0.26 & 0.46 & 0.93 \\
        &&TDE-Sum~\cite{tang2020unbiased}&2.06&3.05&- \\
        &&\textbf{CaCao}&\textbf{4.52}&\textbf{7.64}&\textbf{10.48}\\ \hline
        \multicolumn{1}{c}{\multirow{4}*{VCTree~\cite{tang2019learning}}}&&Baseline & 0.14 & 0.29 & 0.73 \\
        &&TDE~\cite{tang2020unbiased}&1.47&2.31&- \\
        &&Label Refine~\cite{goel2022not} & 2.24 & 3.25 & -\\
        &&\textbf{CaCao}&\textbf{5.49}&\textbf{8.44}&\textbf{11.40}\\ \hline
        \multicolumn{1}{c}{\multirow{2}*{Transformer~\cite{tang2020unbiased}}}&&Baseline & 0.76 & 0.89 & 1.06 \\
        &&\textbf{CaCao}&\textbf{5.51}&\textbf{8.94}&\textbf{12.07}\\ 
        % Epic && - & 1.21 & 1.71 & 2.19 \\
			\hline
		\end{tabular}	
        \vspace{3mm}
	\caption{Additional experimental results of zero-shot performance on the VG-50 dataset of the SGDet task.}
	\label{zero-shot results}
\end{table*}





\section{Additional Experiment Analyses}\label{6}
\noindent\textbf{Composition Generalization.} Benefit from CaCao's generalization ability in the open-world, Our CaCao can also improve the performance of the model in the zero-shot scene graph generation task, which focuses on those subject-predicted-object composition generalization instead of just novel predicates. In Table~\ref{zero-shot results}, we report the zero-shot recall performance and compare CaCao with baselines. We observe that CaCao expands the original combination between different objects and achieves improvements in the SGDet setting with different backbones~(10.48\%, 11.40\%, 12.07\% with Motif, VCTree, and Transformer, respectively). It shows that CaCao enriches the triplet combination and enhances the discriminatory power of the SGG model among novel triplet compositions.

\noindent\textbf{Further Analysis on Open-World Predicate SGG.} (\textbf{result: 28.3\% / 31.1\% v.s. 17.6\% / 21.1\%}) The CaCao and Epic not only improve the novel categories but also greatly improve the base categories on PredCls, indicating that the entangled cross-modal prompt can provide general benefit to the representation learning of predicates, rather than merely an additional hint to the unseen predicates. Empirically, the Epic and CaCao can help the SGG model increase the discriminating power of various predicates by aligning the knowledge between the visual encoder and language encoder of the VL-model, thus promoting an overall improvement of novel categories as well as base categories.

 \noindent\textbf{Further Evaluation on Head and Tail Predicates.} Since CaCao brings much extensive visual relation knowledge on various visual predicates from powerful VL-models, the CaCao may achieve better trade-off on long-tail distribution SGG. Our results on the whole category set partly give evidence that CaCao can achieve better balance in the long-tail distribution. Additionally, we inspect the performance of CaCao across non-rare head predicates to further verify its better balance between head and tail predicate categories in Table \ref{head-result} \textbf{R@K}. Following prior works~\cite{zhang2022fine}, we further use the harmonic average of R@K and mR@K to jointly evaluate R@K and mR@K, which is denotes as \textbf{F@K}. From Table \ref{head-result}, we observe that \textbf{CaCao} outperforms other SOTA model-agnostic methods and specific string baseline according to the joint metric F@K~(\textbf{20.5 / 23.7} of F@50/100 on SGDet), showing the effectiveness of CaCao on both head and tail categories.




\section{Additional Examples}\label{7}
Figure~\ref{f2} shows some more examples for qualitative visualizations of enhanced SGG based on our CaCao.
\begin{figure*}
\begin{subfigure}{1.0\linewidth}
    \centering
    \includegraphics[width=1.\linewidth]{figures/prediction_distribution_1.pdf}
    \caption{Relation prediction distribution with the predicate of `\textit{flying in}'}
    \label{p1}
  \end{subfigure}
\begin{subfigure}{1.0\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/prediction_distribution_2.pdf}
    \caption{Relation prediction distribution with the predicate of `\textit{walking on}'}
    \label{p2}
\end{subfigure}
\caption{\textbf{The effectiveness of CaCao on fine-grained predicates.} The left pie chart shows the prediction distribution of diverse predicates through original Transformer~\cite{tang2020unbiased}; the right pie chart shows the prediction distribution of various predicates through our Transformer+CaCao. We evaluate our method on hard-to-distinguish fine-grained predicates, such as `flying in' and `walking on'.}
\label{f1}
\end{figure*}


\begin{table*}[ht]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Informative Predicates & Mapping Set to General Predicates\\ \hline

    `carrying'& ['behind', 'across', 'along', 'under', 'near']\\ \hline
    `covered in'& ['in', 'in front of', 'on', 'of', 'with'] \\\hline
    `covering'& ['across', 'in front of', 'above', 'behind', 'against']\\ \hline`eating'& ['across', 'behind', 'against', 'along', 'above']\\\hline
    `flying in'& ['in', 'in front of', 'on', 'with', 'of']\\\hline
    `growing on'& ['on', 'in front of', 'in', 'at', 'with']\\\hline
    `hanging from'& ['in front of', 'of', 'with', 'in', 'on']\\\hline
    `lying on'& ['on', 'in front of', 'in', 'at', 'along']\\\hline
    `laying on'& ['on', 'in front of', 'along', 'in', 'with']\\\hline
    `mounted on'& ['on', 'in front of', 'in', 'with', 'at']\\\hline
    `painted on'& ['on', 'in front of', 'in', 'of', 'with']\\\hline
    `parked on'& ['on', 'in front of', 'against', 'across', 'along']\\\hline
    `playing'& ['in front of', 'along', 'against', 'behind', 'above']\\\hline
    `riding'& ['along', 'behind', 'in front of', 'across', 'near']\\\hline
    `says'& ['has', 'against', 'behind', 'above', 'across']\\\hline
    `sitting on'& ['on', 'in front of', 'in', 'at', 'with']\\\hline
    `standing on'& ['on', 'in front of', 'in', 'with', 'at']\\\hline
    `using'& ['with', 'against', 'in front of', 'under', 'across']\\\hline
    `walking in'& ['in', 'in front of', 'on', 'with', 'of']\\\hline
    `walking on'& ['on', 'in front of', 'in', 'with', 'along']\\\hline
    `watching'& ['behind', 'across', 'near', 'between', 'along']\\\hline
    `looking at'& ['at', 'in front of', 'in', 'with', 'on']\\\hline
    `belonging to'& ['to', 'in front of', 'of', 'with', 'for']\\\hline
    `on back of'& ['on', 'in front of', 'of', 'in', 'with']\\\hline
    `part of'& ['of', 'in front of', 'in', 'with', 'for'] \\\hline
    `wearing'& ['across', 'behind', 'against', 'under', 'between']\\\hline
    `wears'& ['has', 'across', 'behind', 'against', 'along']\\\hline
    `made of'& ['of', 'in front of', 'in', 'with', 'for']\\\hline
    `attached to'& ['to', 'in front of', 'on', 'of', 'with']\\\hline
    `from'& ['of', 'in front of', 'in', 'with', 'for']\\\hline
    `holding'& ['in front of', 'with', 'has', 'behind', 'at'] \\\hline
  \end{tabular}
  \vspace{2mm}
    \caption{The mapping from informative predicates to general predicates.}\label{t4}
\end{table*}
\clearpage
\begin{table*}
  \centering
  \begin{tabular}{|c|}
    \hline
   \textbf{Open-world predicate relationships $\rightarrow$ Target predicate relationships} \\ \hline

[`sidewalk', `\textit{in between}', `car']~$\rightarrow$~[`sidewalk', `\textit{between}', `car']\\\hline
[`sidewalk', `\textit{walking across}', `street']~$\rightarrow$~[`sidewalk', `\textit{across}', `street']\\\hline
[`tree', `\textit{hanging in}', `building']~$\rightarrow$~[`tree', `\textit{hanging from}', `building']\\\hline
[`tree', `\textit{uses}', `phone']~$\rightarrow$~[`tree', `\textit{using}', `phone']\\\hline
[`car', `\textit{are parked on}', `street']~$\rightarrow$~[`car', `\textit{parked on}', `street']\\\hline
[`street', `\textit{parked at}', `sidewalk']~$\rightarrow$~[`street', `\textit{parked on}', `sidewalk']\\\hline
[`street', `\textit{among}', `car']~$\rightarrow$~[`street', `\textit{between}', `car']\\\hline
[`phone', `\textit{hanging on}', `tree']~$\rightarrow$~[`phone', `\textit{hanging from}', `tree']\\\hline
[`motorcycle', `\textit{displaying}', `person']~$\rightarrow$~[`motorcycle', `\textit{carrying}', `person']\\\hline
[`building', `\textit{connected to}', `pole']~$\rightarrow$~[`building', `\textit{attached to}', `pole']\\\hline
[`street', `\textit{parked at}', `sidewalk']~$\rightarrow$~[`street', `\textit{parked on}', `sidewalk']\\\hline
[`shirt', `\textit{leans against}', `woman']~$\rightarrow$~[`shirt', `\textit{against}', `woman']\\\hline
[`glass', `\textit{hanging on}', `head']~$\rightarrow$~[`glass', `\textit{hanging from}', `head']\\\hline
[`chair', `\textit{to make}', `leg']~$\rightarrow$~[`chair', `\textit{made of}', `leg']\\\hline
[`man', `\textit{watch}', `woman']~$\rightarrow$~[`man', `\textit{watching}', `woman']\\\hline
[`man', `\textit{leaning up against}', `table']~$\rightarrow$~[`man', `\textit{against}', `table']\\\hline
[`screen', `\textit{laying on}', `paper']~$\rightarrow$~[`screen', `\textit{lying on}', `paper']\\\hline
[`paper', `\textit{looking up at}', `screen']~$\rightarrow$~[`paper', `\textit{looking at}', `screen']\\\hline
[`tree', `\textit{hanging over}', `trunk']~$\rightarrow$~[`tree', `\textit{hanging from}', `trunk']\\\hline
[`car', `\textit{hooked up to}', `pole']~$\rightarrow$~[`car', `\textit{attached to}', `pole']\\\hline
[`tree', `\textit{across from}', `fence']~$\rightarrow$~[`tree', `\textit{between}', `fence']\\\hline
[`sidewalk', `\textit{hanging in}', `trunk']~$\rightarrow$~[`sidewalk', `\textit{hanging from}', `trunk']\\\hline
[`sidewalk', `\textit{traveling on}', `leaf']~$\rightarrow$~[`sidewalk', `\textit{growing on}', `leaf']\\\hline
[`boy', `\textit{looking down at}', `car']~$\rightarrow$~[`boy', `\textit{looking at}', `car']\\\hline
[`woman', `\textit{is using}', `pant']~$\rightarrow$~[`woman', `\textit{using}', `pant']\\\hline
[`woman', `\textit{towing}', `shirt']~$\rightarrow$~[`woman', `\textit{carrying}', `shirt']\\\hline
[`head', `\textit{connected to}', `nose']~$\rightarrow$~[`head', `\textit{attached to}', `nose']\\\hline
[`hair', `\textit{is looking at}', `child']~$\rightarrow$~[`hair', `\textit{looking at}', `child']\\\hline
[`nose', `\textit{tied to}', `head']~$\rightarrow$~[`nose', `\textit{attached to}', `head']\\\hline
[`finger', `\textit{is parked on}', `hand']~$\rightarrow$~[`finger', `\textit{painted on}', `hand']\\\hline
[`man', `\textit{eaten}', `pizza']~$\rightarrow$~[`man', `\textit{eating}', `pizza']\\\hline
[`windshield', `\textit{towing}', `umbrella']~$\rightarrow$~[`windshield', `\textit{carrying}', `umbrella']\\\hline
[`airplane', `\textit{hanging on}', `wing']~$\rightarrow$~[`airplane', `\textit{hanging from}', `wing']\\\hline
[`airplane', `\textit{hanging on}', `wing']~$\rightarrow$~[`airplane', `\textit{hanging from}', `wing']\\\hline
[`airplane', `\textit{flying high in}', `sky']~$\rightarrow$~[`airplane', `\textit{flying in}', `sky']\\\hline
[`sign', `\textit{strapped}', `arrow']~$\rightarrow$~[`sign', `\textit{on}', `arrow']\\\hline
[`face', `\textit{connected to}', `neck']~$\rightarrow$~[`face', `\textit{above}', `neck']\\\hline
[`tree', `\textit{across from}', `building']~$\rightarrow$~[`tree', `\textit{across}', `building']\\\hline
[`roof', `\textit{across from}', `building']~$\rightarrow$~[`roof', `\textit{along}', `building']\\\hline
[`jacket', `\textit{is cluttered with}', `man']~$\rightarrow$~[`jacket', `\textit{with}', `man']\\\hline
[`sign', `\textit{are showing on}', `building']~$\rightarrow$~[`sign', `\textit{says}', `building']\\\hline
[`short', `\textit{in between}', `man']~$\rightarrow$~[`short', `\textit{with}', `man']\\\hline
[`jean', `\textit{stacked on}', `man']~$\rightarrow$~[`jean', `\textit{painted on}', `man']\\\hline
[`person', `\textit{is walking on}', `sidewalk']~$\rightarrow$~[`person', `\textit{walking on}', `sidewalk']\\\hline
[`chair', `\textit{are looking at}', `boy']~$\rightarrow$~[`chair', `\textit{in front of}', `boy']\\\hline

  \end{tabular}
    \caption{Examples of open-world predicates to target predicates mapping.}\label{t5}
\end{table*}
\begin{figure*}[ht]

    \centering
    \includegraphics[width=1.\linewidth]{figures/zs_base.pdf}
    \caption{Top-25 predicates of original distribution}
    \label{original-distribution}
  \end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/zs_expand.pdf}
    \caption{Top-25 predicates of open-world distribution}
    \label{open-distribution}
\end{figure*}
\begin{figure*}
    \begin{subfigure}{1.0\linewidth}
        
    \centering
    \includegraphics[width=1.\linewidth]{figures/additional_example.pdf}
   \end{subfigure}
    \begin{subfigure}{1.0\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/additional_example_2.pdf}
     \end{subfigure}
     \caption{Additional Qualitative Results for Transformer equipped with our CaCao framework for predicate enhancement with the ground truth relationships. The predicted triplets are from the SGDet setting.}
     \label{f2}
\end{figure*}




 

	% \clearpage
	% %%%%%%%%% REFERENCES
	% {\small
	% 	\bibliographystyle{ieee_fullname}
	% 	\bibliography{egbib}
	% }

\end{document}

