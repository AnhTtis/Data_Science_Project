
\section{Related Work}

\textbf{Domain Generalization} researches out-of-distribution generalization with knowledge only extracted from multiple source domains.
A promising direction is to diversify training domains so as to improve generalization, referring as to domain augmentation~\cite{CrossGrad,AdvAug,MixStyle,FACT,L2A-OT}. L2A-OT~\cite{L2A-OT} creates pseudo-novel domains from source data by maximizing an optimal transport-based divergence measure. CrossGrad~\cite{CrossGrad} generates samples from fictitious domains via gradient-based domain perturbation while AdvAug~\cite{AdvAug} achieves so via adversarially perturbing images. 
MixStyle~\cite{MixStyle} and FACT~\cite{FACT} mix style information of different instances to synthetic novel domains.
Instead of enriching domain diversity, another popular solution that learning domain-invariant representations by distribution alignment via kernel-based optimization \cite{DICA,SCA}, adversarial learning \cite{MMD-AAE,CCSA}, \lv{or using uncertainty modeling~\cite{DSU}}
 demonstrate effectiveness for model generalization.
Other recent DG works also explore low-rank decomposition \cite{CSD}, self-supervised signals~\cite{Jigen}, gradient-guided dropout \cite{RSC}, etc. 
Though our proposed framework builds on the domain augmentation group, we aim to guarantee and further enhance their efficacy beyond via a convex game perspective.

\textbf{Convex Game} is a highly interesting class of cooperative games introduced by~\cite{ConvexGame}. A game is called convex when it satisfies the condition that the profit obtained by the cooperation of two coalitions plus the profit obtained by their intersection will not be less than the sum of profit obtained by the two respectively (a.k.a. supermodularity)~\cite{ConvexGame,supermodularity,convexfuzzygame}.
Co-Mixup\cite{Co-Mixup} 
formulates the optimal construction of mixup augmentation data while encouraging diversity among them by introducing supermodularity. Nevertheless, it is applied to supervised learning which aims to construct salience mixed samples.
Recently, ~\cite{onlineDG} rethinks the single-round minmax setting of DG and recasts it as a repeated online game between a player minimizing risk and an adversary presenting test distributions in light of online convex optimization~\cite{onlineconvexopti}. We note that the definition of convex game exploited in our work follows~\cite{ConvexGame}, distinct from that in~\cite{onlineDG, onlineconvexopti}.
To the best of our knowledge, this work is the first to introduce convex game into DG to enhance generalization capability. 


\textbf{Meta Learning}~\cite{learning} is a long-term research exploring to learn how to train a particular model through the training of a meta-model~\cite{learntooptimize,MAML,fewshot}, and has drawn increasing attention from DG community~\cite{MetaReg,MASF,FCN,MLDG} recently. The main idea is to simulate domain shift during training by drawing virtual-train/test domains from the original source domains. 
MLDG~\cite{MLDG} originates the episode training paradigm from \cite{MAML}, back-propagating the second-order gradients from an ordinary task loss on random meta-test domains split from the source domains. 
Subsequent meta learning-based DG methods utilize a similar strategy to meta-learn a regularizer~\cite{MetaReg}, feature-critic network~\cite{FCN}, or semantic relationships~\cite{MASF}.
Different from the former paradigm that purely leverages the gradient of task objective, which may cause sub-optimal,
we utilize the ordinary task losses to construct a supermodularity regularization with more stable optimization, aiming to encourage each training domain to contribute to model generalization.
