\appendix


  \section{Social Impact}
  \label{sec:impact}
  Our work focuses on domain generalization and attempts to make each training domain contribute to model generalization, which validates and further enhances the effectiveness of domain augmentation strand.
  This method produces a positive impact on the society and  community, saves the cost and time of data annotation, boosts the reusability of knowledge across domains, and greatly improves the efficiency. Nevertheless, this work suffers from some negative influences, which is worthy of further research and exploration. Specifically, more jobs of classification or target detection for rare or variable conditions may be cancelled. Moerover, we should be cautious about the result of the failure of the system, which could render people believe that classification was unbiased. Still, it might be not, which might be misleading, e.g., when using the system in a highly variable unseen target domain.
  
  \section{Algorithm of DCG}
  \label{sec:alg}
  
  \lvv{In this work, we propose a Domain Convex Game (DCG) framework to guarantee and further
  enhance the validity of domain augmentation approaches by casting DG as a convex game between domains. Here, we summarize the training process of DCG based on the discussions in main body as Algorithm~\ref{alg:DCG}.}

  \begin{algorithm}[H]
    \vspace{-1mm}
    \caption{The Algorithm of Domain Convex Game.}
    \label{alg:DCG}
    \begin{algorithmic} [1]
    \REQUIRE $P+Q$ diversified source domains $\mathcal{D}_s \cup \mathcal{D}_s^{aug}$; Hyper-parameters: $\omega, k$.
     \STATE randomly initialize model parameters $\boldsymbol{\theta}$.
    \FOR{iter in iterations}
    \STATE Randomly sample a mini-batch of $\mathcal{D}_s$ as $B$ and a mini-batch of $\mathcal{D}_s^{aug}$ as $B^{aug}$. 
    \STATE Split: $\tilde{\mathcal{D}_s}$ and $\tilde{\mathcal{D}_t}$ $\xleftarrow{} B$, Pick out: $\tilde{\mathcal{D}}_s^{aug}$ from $B^{aug}$.
    \STATE Construct coalitions $S,T$ by randomly sampling from $\tilde{\mathcal{D}_s} \cup \tilde{\mathcal{D}}_s^{aug}$; construct coalitions $S\cup T, S\cap T$.
    \STATE Calculate supermodularity regularization loss $\mathcal{L}_{sm}$ as Eq.~\eqref{eq:l_reg}.
    \STATE Pick out low-quality samples $\mathcal{D}_{del}$ with the top-$k$ score calculated by Eq.~\eqref{eq:score}.
    \STATE Calculate supervision loss $\mathcal{L}_{sup}$ as Eq.~\eqref{eq:l_cls}.
    \STATE Update $\boldsymbol{\theta} = \arg \min_{\boldsymbol{\theta}} \mathcal{L}_{sup} + \omega\mathcal{L}_{sm}$.
    \ENDFOR
    \end{algorithmic}
  \end{algorithm}
%   \vspace{-4mm}



  \section{Experimental Details}
  \label{sec:implementation}
  For all benchmarks, we conduct the commonly used leave-one-domain-out experiments~\cite{DBA}, where we choose one domain as the unseen target domain for evaluation, and train the model on all remaining domains. 
  We adopt the standard augmentation protocol as in~\cite{Jigen}, all images are resized to 224 Ã— 224, following with random  resized cropping, horizontal flipping and color jittering. And the Fourier domain augmentation strategy utilized to diversify source domains closely follows the implementations in~\cite{FACT}. 
  \lvv{The network backbone is set to ResNet-18 or ResNet-50 pre-trained on ImageNet~\cite{resnet} following other related works.} 
  We train the network using mini-batch SGD with batch size 16, momentum 0.9 and weight decay 5e-4 for 50 epochs. The initial learning rate is 0.001 and decayed by 0.1 at 80\% of the total epochs. The meta step size $\alpha$ is set to be the same as the learning rate.
  For the hyper-parameters, i.e., the weight of regularization loss $\omega$ and the number of discarded bad samples in each iteration $k$, their values are selected on validation data following standard practice, where we use 90\% of available data as training data and 10\% as validation data. Specifically, we set $\omega = 0.1$ and $k=5$ for all experiments.
  Our framework is implemented with PyTorch on NVIDIA GeForce RTX 3090 GPUs. All results are reported based on the average accuracy over three independent runs for a fair comparison.

  

  \section{Additional Results}
  \lvv{\subsection{Time cost analysis}}
  \lvv{We conduct experiments to study the efficiency of our method in the training and inference stages respectively, and the results are shown in Table~\ref{tab:time_cost}. For the training stage, the time cost of DCG is indeed relatively high, which is due to
 the use of meta learning when constructing the regularization term and the backpropagation when calculating the score for sample filter. For substitute, we may edit the backpropagation path that computes gradients of inputs only on a smaller subnetwork to reduce time cost. Besides, we can see that for the inference stage, our DCG method is as efficient as other methods and does not incur additional time costs.  Note that this work is an innovative effort to study the relation between model generalization and domain diversity, which is in a preliminary stage. And we will further explore more efficient techniques in future research.}
  
 \begin{table}[htbp]
  \centering
    \setlength{\tabcolsep}{1.0mm}{
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc}
    \toprule
    Methods & Training & Inference\\
    \midrule
    DEEPALL~\cite{FACT} & 168 s & 5 s\\
    FACT~\cite{FACT} & 186 s & 5 s\\
    MLDG~\cite{MLDG} & 275 s & 5 s\\
    \midrule
    DCG w$/$o Filter. & 349 s & 5 s\\
    DCG & 467 s & 5 s\\
    \bottomrule
    \end{tabular}}
    \caption{Running Time per Epoch.}
  \label{tab:time_cost}
\end{table} 


  

% \begin{table*}[htp]
%   \centering
%     \setlength{\tabcolsep}{1.0mm}{
%     % \resizebox{1.0\columnwidth}{!}{
%     \begin{tabular}{l|cccc|c}
%     \toprule
%     Methods & Art & Cartoon & Photo & Sketch & Avg. \\
%     %\midrule
%     %\multicolumn{6}{c}{\textit{ResNet18}} \\
%     \midrule
%     RSC~\cite{RSC} & 87.89 & 82.16 & 97.92 & 83.35 & 87.83 \\
%     PCL \cite{PCL}& 90.20 &83.90& 98.10& 82.60 &	88.70 \\
%     \midrule
%     DeepAll\cite{DEEPALL} & 84.94$\pm$0.66 & 76.98$\pm$1.13 & 97.64$\pm$0.10 & 76.75$\pm$0.41 & 84.08 \\
%     FACT \cite{FACT}& 89.63$\pm$0.51 &81.77$\pm$0.19& 96.75$\pm$0.10& 84.46$\pm$0.78 &	88.15 \\
%     DDG \cite{DDG}& 88.90$\pm$0.60 &85.00$\pm$1.90& 97.20$\pm$1.20& 84.30$\pm$0.70 &	88.90 \\
%     STNP \cite{STNP} & 90.35$\pm$0.62 & 84.20$\pm$1.43 & 96.73$\pm$0.46 & 85.18$\pm$0.46 & 89.11\\
%     \midrule
%     DCG (\textit{ours})  & \textbf{85.94$\pm$0.21} &	\textbf{80.76$\pm$0.36}	& \textbf{96.41$\pm$0.17} &	\textbf{82.08$\pm$0.44}	& \textbf{86.30} \\
   
%     \bottomrule
%     \end{tabular}}
%     \caption{Leave-one-domain-out results on PACS (ResNet-50).}
%   \label{tab:pacs_res50}
% \end{table*}


% \begin{table*}[htp]
%   \centering
%     \setlength{\tabcolsep}{1.0mm}{
%     % \resizebox{\columnwidth}{!}{
%     \begin{tabular}{l|ccc|cc}
%     \toprule
%     Stage & DEEPALL~\cite{FACT} & FACT~\cite{FACT} & MLDG~\cite{MLDG} & DCG w$/$o Filter. & DCG\\
%     \midrule
%     Training &  168 s &	186 s &	275 s &349 s& 467 s\\
%     Inference & 5 s &5 s&5 s& 5 s&5 s\\
%     \bottomrule
%     \end{tabular}}
%     \caption{Running Time per Epoch.}
%   \label{tab:time_cost}
% \end{table*}

  
\begin{table*}[htpb]
  \centering
    \setlength{\tabcolsep}{4.0mm}{
    % \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{l|cccc|c}
    \toprule
    Methods & Art & Cartoon & Photo & Sketch & Avg. \\
    \midrule
    \multicolumn{6}{c}{\textit{ResNet18}} \\
    \midrule\
    MLDG~\cite{MLDG} & 78.70 & 73.30 & 94.00 & 65.10 & 80.70 \\
    L2A-OT~\cite{L2A-OT} & 83.30 & 78.20 & 96.20 & 73.60 & 82.80 \\
    RSC~\cite{RSC} & 83.43 & 80.31 & 95.99 & 80.85 & 85.15 \\
      DSU \cite{DSU} & 83.60 & 79.60 & 95.80 & 77.60 & 84.10\\
    \midrule
    DeepAll\cite{DEEPALL} & 77.63$\pm$0.84 & 76.77$\pm$0.33 & 95.85$\pm$0.20 & 69.50$\pm$1.26 & 79.94 \\
     MASF~\cite{MASF} & 80.29$\pm$0.18& 77.17$\pm$0.08& 94.99$\pm$0.09& 71.69$\pm$0.22& 81.04 \\
    DDAIG~\cite{DEEPALL} & 84.20$\pm$0.30& 78.10$\pm$0.60& 95.30$\pm$0.40& 74.70$\pm$0.80 & 83.10 \\
    MixStyle ~\cite{MixStyle} & 84.10$\pm$0.40 &  78.80$\pm$0.40 & 96.10$\pm$0.30 &  75.90$\pm$0.90 & 83.70 \\
    FACT \cite{FACT}& 85.37$\pm$0.29 &78.38$\pm$0.29& 95.15$\pm$0.26& 79.15$\pm$0.69 &	84.51 \\
    STNP \cite{STNP} & 84.41$\pm$0.62 & 79.25$\pm$0.98 & 94.93$\pm$0.07 & \textbf{83.27$\pm$2.03} & 85.47\\
    \midrule
    DCG (\textit{ours})  & \textbf{85.94$\pm$0.21} &	\textbf{80.76$\pm$0.36}	& \textbf{96.41$\pm$0.17} &	82.08$\pm$0.44	& \textbf{86.30} \\
  \midrule
  \multicolumn{6}{c}{\textit{ResNet50}} \\
  \midrule
  RSC~\cite{RSC} & 87.89 & 82.16 & 97.92 & 83.35 & 87.83 \\
  PCL \cite{PCL}& 90.20 &83.90& \textbf{98.10}& 82.60 &	88.70 \\
  \midrule
  DeepAll\cite{DEEPALL} & 84.94$\pm$0.66 & 76.98$\pm$1.13 & 97.64$\pm$0.10 & 76.75$\pm$0.41 & 84.08 \\
  FACT \cite{FACT}& 89.63$\pm$0.51 &81.77$\pm$0.19& 96.75$\pm$0.10& 84.46$\pm$0.78 &	88.15 \\
  DDG \cite{DDG}& 88.90$\pm$0.60 &85.00$\pm$1.90& 97.20$\pm$1.20& 84.30$\pm$0.70 &	88.90 \\
  STNP \cite{STNP} & \textbf{90.35$\pm$0.62} & 84.20$\pm$1.43 & 96.73$\pm$0.46 & 85.18$\pm$0.46 & 89.11\\
  \midrule
  DCG (\textit{ours})  & 90.24$\pm$0.48 &	\textbf{85.12$\pm$0.79}	& 97.76$\pm$0.13 &	\textbf{86.31$\pm$0.64}	& \textbf{89.84} \\
 
    \bottomrule
    \end{tabular}}
    \caption{Leave-one-domain-out results on PACS.}
  \label{tab:pacs_all}
\end{table*}


\begin{table*}[htpb]
  % \setlength{\abovecaptionskip}{0.cm}
  % \setlength{\belowcaptionskip}{0.cm}
  \centering
   \setlength{\tabcolsep}{4.0mm}{
    % \resizebox{0.95\columnwidth}{!}{
    \begin{tabular}{l|cccc|c}
    \toprule
    Methods & Art & Clipart & Product & Real & Avg. \\
    \midrule
    MLDG~\cite{MLDG} &52.88 & 45.72 & 69.90 & 72.68 & 60.30 \\
    SagNet \cite{SagNet} & 60.20 & 45.38 & 70.42 & 73.38 & 62.34\\
  %   Jigen~\cite{Jigen} & 53.04 & 47.51 & 71.47 & 72.79 & 61.20 \\
    RSC~\cite{RSC}   & 58.42 & 47.90 & 71.63 & 74.54 & 63.12 \\
    L2A-OT~\cite{L2A-OT} & 60.60 & 50.10 & 74.80 & \textbf{77.00} & 65.60 \\
    DSU \cite{DSU} & 60.20 & 54.80 & 74.10 & 75.10 & 66.10\\
    \midrule
    DeepAll \cite{DEEPALL} & 57.88$\pm$0.20 & 52.72$\pm$0.50& 73.50$\pm$0.30& 74.80$\pm$0.10 & 64.72 \\
  %   CCSA~\cite{CCSA}  & 59.90$\pm$0.30 &49.90$\pm$0.40& 74.10$\pm$0.20 &75.70$\pm$0.20 & 64.90 \\
  %   MMD-AAE~\cite{MMD-AAE} & 56.50$\pm$0.40& 47.30$\pm$0.30& 72.10$\pm$0.30& 74.80$\pm$0.20& 62.70 \\
  %   CrossGrad~\cite{CrossGrad} & 58.40$\pm$0.70& 49.40$\pm$0.40 &73.90$\pm$0.20 &75.80$\pm$0.10 & 64.40 \\
    DDAIG~\cite{DEEPALL} & 59.20$\pm$0.10& 52.30$\pm$0.30& 74.60$\pm$0.30& 76.00$\pm$0.10 & 65.50 \\
    MixStyle \cite{MixStyle} & 58.70$\pm$0.30 & 53.40$\pm$0.20 & 74.20$\pm$0.10 & 75.90$\pm$0.10 & 65.50\\
    FACT \cite{FACT}& 60.34$\pm$0.11& 54.85$\pm$0.37& 74.48$\pm$0.13& 76.55$\pm$0.10 & 66.56 \\
    STNP \cite{STNP} & 59.55$\pm$0.21 & 55.01$\pm$0.29 & 73.57$\pm$0.28 & 75.52$\pm$0.21 & 65.89\\
    \midrule
     DCG (\textit{ours}) & \textbf{60.67$\pm$0.14} &	\textbf{55.46$\pm$0.32} &	\textbf{75.26$\pm$0.18}	& 76.82$\pm$0.09 &	\textbf{67.05}  \\
    \bottomrule
    \end{tabular}}
    \caption{Leave-one-domain-out results on Office-Home.
    %   with ResNet-18. The best and second-best results are bold and underlined.
    }
  \label{tab:home_all}
\end{table*}




\subsection{Experimental Results with Error Bars}
\label{sec:additional_res} 
For the sake of objective, we run all the experiments multiple times with random seed. We report the average results in the main body of paper for elegant, and show the complete results with error bars in the form of mean$\pm$std below (Table.~\ref{tab:pacs_all},~\ref{tab:home_all},~\ref{tab:domainnet_all}).



\begin{table*}[b]
  % \setlength{\abovecaptionskip}{0.cm}
  % \setlength{\belowcaptionskip}{0.cm}
  \centering
   \setlength{\tabcolsep}{4.0mm}{
    % \resizebox{0.95\columnwidth}{!}{
    \begin{tabular}{l|cccc|c}
    \toprule
    Methods & Clipart & Painting & Real & Sketch & Avg. \\
    \midrule
    DeepAll \cite{DEEPALL} & 65.30 &	58.40	&64.70&	59.00&	61.86  \\
    \midrule
    ERM~\cite{ERM}  & 65.50 $\pm$ 0.3& 57.10 $\pm$ 0.5& 62.30 $\pm$ 0.2& 57.10 $\pm$ 0.1 & 60.50 \\
     MLDG~\cite{MLDG} & 65.70 $\pm$ 0.2& 57.00 $\pm$ 0.2& 63.70 $\pm$ 0.3& 58.10 $\pm$ 0.1 & 61.12 \\
     Mixup~\cite{mixup} & 67.10 $\pm$ 0.2& 59.10 $\pm$ 0.5& 64.30 $\pm$ 0.3& 59.20 $\pm$ 0.3 & 62.42 \\
    MMD~\cite{MMD} & 65.00 $\pm$ 0.5& 58.00 $\pm$ 0.2& 63.80 $\pm$ 0.2& 58.40 $\pm$ 0.7& 61.30 \\
    SagNet~\cite{SagNet} & 65.00 $\pm$ 0.4& 58.10 $\pm$ 0.2 &64.20 $\pm$ 0.3& 58.10 $\pm$ 0.4 & 61.35 \\
     CORAL~\cite{CORAL} & 66.50 $\pm$ 0.2& 59.50 $\pm$ 0.4 &66.00 $\pm$ 0.6& 59.50 $\pm$ 0.1 & 62.87 \\
     MTL~\cite{MTL} & 65.30 $\pm$ 0.5 &59.00 $\pm$ 0.4& 65.60 $\pm$ 0.4& 58.50 $\pm$ 0.2 & 62.10 \\
    \midrule
     DCG (\textit{ours}) & \textbf{69.38$\pm$0.19} &	\textbf{61.79$\pm$0.22}&	\textbf{66.34$\pm$0.27}&	\textbf{63.21$\pm$0.09}&	\textbf{65.18}  \\
    \bottomrule
    \end{tabular}}
    \caption{Leave-one-domain-out results on Mini-DomainNet.
%   with ResNet-18. The best and second-best results are bold and underlined.
}
  \label{tab:domainnet_all}
\end{table*}