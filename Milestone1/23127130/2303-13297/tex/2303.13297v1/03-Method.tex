\section{Domain Convex Game}
\label{sec:method}
Motivated by such an observation in Section~\ref{sec:intro}, we propose Domain Convex Game (DCG) framework to train models that can best utilize domain diversity, as illustrated in Fig.~\ref{fig:framework}.
First, we cast DG as a convex game between domains and design a novel regularization term employing the supermodularity, which encourages each domain to benefit model generalization. Further, we construct a sample filter based on the regularization to exclude bad samples that may cause negative effect on generalization. 
In this section, we define the problem setup and present the general form of DCG.


\subsection{Preliminary}
Assuming that there are $P$ source domains of data $\mathcal{D}_s = \cup_{k=1}^PD_k$ with $n_k$ labelled samples $\{(\boldsymbol{x}^k_i, y^k_i)\}_{i=1}^{n_k}$ in the $k$-th domain $D_k$, where $\boldsymbol{x}^k_i$ and $y^k_i \in \{1,2,\cdots,C\}$ denote the samples and corresponding labels. DG aims to train a domain-agnostic model $f(\cdot,\boldsymbol{\theta})$ parametrized with $\boldsymbol{\theta}$ on source domains that can generalize well on unseen target domain(s) $\mathcal{D}_t$.
As an effective solution for DG, domain augmentation aims to enrich the diversity of source domains generally by synthesizing novel domains via mixing domain-related information, hence boosting model generalization~\cite{L2A-OT,MixStyle,FACT}.
\lvv{Our work builds on this strand, and the key insight is to ensure and further improve its efficacy by better leveraging the domain diversity. For concision, in this paper, we adopt a simple Fourier-based augmentation technique~\cite{FACT,FDA} to prepare our diversified source domains. Note that the augmentation strategy is substitutable.}

% In this paper, we exploit the Fourier-based augmentation technique [48,49] to prepare our diversified source domains for its simplicity . Also, other alternatives can be used.

\lvv{Technically}, owing to the property that the phase component of Fourier spectrum preserves high-level semantics of the original signal, while the amplitude component contains low-level statistics~\cite{1981fourier,1982fourier}, we augment the source data by distorting the amplitude information while keeping the phase information unchanged. Specifically, we mix the amplitude spectrum of an instance with that of another arbitrary instance by a linear interpolation strategy to synthesize augmented instances from novel domains. We refer readers to~\cite{FDA,FACT} for implementation details.
Since each augmented sample is generated by mixing domain information of sample pairs from random source domains in a random proportion, \lv{it has statistics distinct from the others so that can be regarded as drawn from a novel augmented domain}. Thus, we possess another $Q$ augmented source domains 
of data $\mathcal{D}_s^{aug} = \cup_{k=1}^QD_{P+k}$
with only one sample $\{(\boldsymbol{x}^{P+k}_i, y^{P+k}_i)\}_{i=1}^{1}$ in the $(P+k)$-th domain $D_{P+k}$, where $\boldsymbol{x}^{P+k}_i$ and $y^{P+k}_i$ denote the augmented samples and corresponding labels.
\lv{Note that the number of augmented domains generated this way is equivalent to the total number of all the original samples since each original sample pair will generate a pair of augmented samples.}
The goal of DCG is to train a generalizable model $f(\cdot,\boldsymbol{\theta})$ for unseen target domain(s) $\mathcal{D}_t$ with the aid of 
all $P+Q$ diversified source domains $\mathcal{D}_s\cup \mathcal{D}_s^{aug}$.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{Figures/Fig_framework.pdf}
  \caption{The pipeline of DCG. We first randomly split the diversified training domains into meta-train and meta-test domains, and generate four coalitions from the former according to the definition of convex game. Then we conduct meta learning on the four coalitions respectively and construct our regularization loss utilizing the meta-test losses of them based on the supermodularity. Meanwhile, we eliminate the low-quality samples by a sample filter and calculate supervision loss on the retained samples. }
  \label{fig:framework}
  \vspace{-4mm}
\end{figure}

\subsection{Supermodularity Regularization Term}
\label{sec:sm_reg}
Let $M = \{1,2,\cdots,m\}$ be a finite set of players and $2^M$ is the family of $2^{|M|}$ subsets of $M$. A cooperative game with player set $M$ is a map $v:2^M \xrightarrow{} \mathbb{R}$. For coalition $S\in2^M$, $v(S)$ is called the worth of $S$, and is interpreted as the total profit that $S$ can obtain when the players in $S$ cooperate. 
A game is called convex if it satisfies the
\textit{supermodularity property}~\cite{convexfuzzygame,ConvexGame,supermodularity}, i.e., for each $S,T\in2^M$:
\begin{equation}
  v(S \cup T) + v(S \cap T) \ge v(S) + v(T).
  \label{eq:convex_sm}
\end{equation}
According to this definition, we can obtain: 
\begin{equation}
  v(S \cup \{i\} \cup \{j\}) - v(S \cup \{i\}) \ge v(S \cup \{j\}) - v(S),
\end{equation}
where $S\in2^M\verb|\|\{\varnothing\}$ and $i,j$ are two players not in $S$. 
We can see that convex game requires each player to contribute to the coalition, which is consistent with our key insight, that is, each training domain is expected to benefit model generalization. More than this, convex game also possesses \textit{increasing marginal contribution property} for players, which may not hold in DG. However, this property does not hinder our goal, but can further alleviate the \textit{decreasing marginal contribution} for domains, as discussed in Section~\ref{sec:intro}.

Thus, we first cast DG as a convex game between domains.
To achieve this, at each training iteration, we randomly split the original source data $\mathcal{D}_s$ into $P-V$ meta-train domains of data $\Tilde{\mathcal{D}_s}$ and $V$ meta-test domains of data $\tilde{\mathcal{D}}_t$, where $\Tilde{\mathcal{D}_s}$ and $\tilde{\mathcal{D}}_t$ share no domain. Then we pick out the augmented domains generated by data in $\Tilde{\mathcal{D}_s}$, denoted as $\tilde{\mathcal{D}}_s^{aug}$, and incorporate them into the meta-train domains. 
This strategy to conduct meta-train/test domains is to mimic the real train-test domain shift in domain augmentation strand, which is discussed in Section~\ref{sec:discussion_main}. 
Then, since one domain may contain multiple samples, we specifically consider involving a specific convex game: \emph{convex fuzzy game}~\cite{convexfuzzygame} where each player (i.e., each domain) can be partitioned into multiple parts (each part represents a sample in DG).
Now we have a finite set of partitioned players 
$\tilde{M} = \tilde{\mathcal{D}_s} \cup\tilde{\mathcal{D}}_s^{aug}$.
We can obtain coalitions $S,T \in 2^{\tilde{M}}$ by randomly sampling two sets of data from meta-train data $\tilde{\mathcal{D}_s} \cup\tilde{\mathcal{D}}_s^{aug}$, respectively. And $S\cup T, S\cap T$ can be naturally constructed by the union and intersection of $S$ and $T$.
As for the profit $v(O), O\in\{S,T,S\cup T, S\cap T\}$, we take the generalization performance evaluated on virtual-test domains $\tilde{\mathcal{D}_t}$ after the meta-training on each coalition $O$ as the value of profit $v(O)$.

Specifically, assuming a loss function $\ell(f(\boldsymbol{x},\boldsymbol{\theta}),y)$ for a sample between its output and label, e.g., cross-entropy loss for classification task, we first conduct virtual training on the four coalitions $\{S,T,S\cup T, S\cap T\}$, respectively, with the optimization objective:
\begin{equation}
  \small
  \mathcal{F}(O) := \sum_{x\in O}\ell(f(\boldsymbol{x},\boldsymbol{\theta}),y)), O \in \{S,T,S\cup T, S\cap T\}.
\end{equation}
Then the updated virtual parameters $\boldsymbol{\theta}'$ can be computed using one step of gradient descent:
\begin{equation}
  \boldsymbol{\theta}' = \boldsymbol{\theta} - \alpha \nabla_{\boldsymbol{\theta}}\mathcal{F}(O),
\end{equation}
where $\alpha$ is the virtual step size and is empirically set to be the same as the learning rate in our experiments. Thus, we can have the corresponding meta-test loss evaluated on the virtual-test domains $\tilde{\mathcal{D}_t}$ as below:
\begin{equation}
  \mathcal{G}(\boldsymbol{\theta}') := \mathbb{E}_{\boldsymbol{x} \in \tilde{\mathcal{D}_t}} \ell(f(\boldsymbol{x},\boldsymbol{\theta}'),y).
\end{equation}
This objective simulates test on unseen domains, thus can measure the model generalization obtained by training with one coalition, i.e., $v(O)=-\mathcal{G}(\boldsymbol{\theta}')$.
Hence, the supermodularity regularization can be constructed naturally utilizing the meta-test losses of the four coalitions based on Eq.~\eqref{eq:convex_sm}:
\begin{equation}
\small
\begin{split}
  \mathcal{L}_{sm} = \max\{0,&\mathcal{G}(\boldsymbol{\theta} - \alpha\nabla_{\boldsymbol{\theta}}\mathcal{F}(S\cup T)) + \mathcal{G}(\boldsymbol{\theta} - \alpha\nabla_{\boldsymbol{\theta}}\mathcal{F}(S\cap T))\\ - &\mathcal{G}(\boldsymbol{\theta} - \alpha\nabla_{\boldsymbol{\theta}}\mathcal{F}(S)) - \mathcal{G}(\boldsymbol{\theta} - \alpha\nabla_{\boldsymbol{\theta}}\mathcal{F}(T))\}.
\end{split}
\label{eq:l_reg}
\end{equation}
\lv{Here we exploit a $max(0,\cdot)$ function combined with the pure supermodularity to construct our regularization. In this way, $\mathcal{L}_{sm}>0$ only when the inequality in Eq.~\eqref{eq:convex_sm} is violated, i.e., the domain marginal contribution is decreasing. Thus, the limit of our regularization optimization corresponds to constant marginal contribution, not the inappropriate increasing marginal contribution.} Therefore, this regularization term can not only encourage each training domain to contribute to model generalization, but also alleviate the decrease of marginal contributions to some extent, enabling the model to fully leverage the rich information in diversified domains.


\subsection{Sample Filter}
Through the optimization of the regularization term, the model will be trained to better utilize the rich information of diversified source domains. However, what we cannot avoid is that there may exist some low-quality samples with harmful information to model generalization.
For instance, noisy samples will disturb model to learn generalizable knowledge; while redundant samples 
may lead to overfitting that hinder the model from learning more diverse patterns.

In this view, we further conduct a sample filter  to avoid the negative impact of low-quality samples.
Considering that the proposed regularization aims to penalize the decreasing marginal contribution of domains and then better utilize the diverse information, the samples that contribute more to the regularization loss (i.e., cause larger increase) are more unfavorable to our goal, hindering the improvement of model generalization.
Thus, we try to measure the contribution of each input to our regularization loss and define the contribution as its score.
Inspired by \cite{LRP} which defines the contribution of each input to the prediction by introducing layer-wise relevance propagation, we formulate the score of each input as the elementwise product between the input and its gradient to regularization loss, i.e., Input $\times$ Gradient:

\begin{equation}
  score = \boldsymbol{x}^T\nabla_{\boldsymbol{x}}\mathcal{L}_{sm},x\in \tilde{\mathcal{D}_s}\cup\tilde{\mathcal{D}}_s^{aug}.
  \label{eq:score}
\end{equation}
The higher the score of the sample, the greater the regularization loss will be increased caused by it, and the more it will hinder model from benefiting from diversified domains.
Therefore, we pick out the samples with the top-$k$ score, denoted as $\mathcal{D}_{del}$, and cast them away when calculating the supervision loss for diversified source domains \lv{to eliminate the negative effect of low quality samples}:
\begin{equation}
  \mathcal{L}_{sup} = \mathbb{E}_{\boldsymbol{x}\in\mathcal{D}_s\cup\mathcal{D}_s^{aug}\verb|\| \mathcal{D}_{del}} \ell(f(\boldsymbol{x},\boldsymbol{\theta}),y).
  \label{eq:l_cls}
\end{equation}
Thus, we optimize the regularization loss to enable model to better utilize the rich information within diversified domains. In the meanwhile, we eliminate the low-quality samples \lv{(e.g, noisy samples, redundant samples, etc)} by the sample filter to avoid their negative effects. \lv{Moreover, it is found that different types of low-quality samples are more likely to be discarded in different training stages, as discussed in Section~\ref{sec:theory_proof}. And we have explored out that low quality sample filtering is necessary for both original and augmented samples in Section~\ref{sec:discussion_main}.}

The overall optimization objective is:
\begin{equation}
  \arg \min_{\boldsymbol{\theta}} \mathcal{L}_{sup} + \omega \mathcal{L}_{sm},
  \label{eq:overall}
\end{equation}
where $\omega$ weights the supervision loss and the regularization term.
The overall methodological flow is illustrated schematically in Fig.~\ref{fig:framework} and summarized in Appendix~\ref{sec:alg}.
% \vspace{-1mm}
%   \begin{algorithm}[H]
%     \vspace{-1mm}
%     \caption{The Algorithm of Domain Convex Game.}
%     \label{alg:DCG}
%     \begin{algorithmic} [1]
%     \REQUIRE $P+Q$ diversified source domains $\mathcal{D}_s \cup \mathcal{D}_s^{aug}$; Hyper-parameters: $\omega, k$.
%      \STATE randomly initialize model parameters $\boldsymbol{\theta}$.
%     \FOR{iter in iterations}
%     \STATE Randomly sample a mini-batch of $\mathcal{D}_s$ as $B$ and a mini-batch of $\mathcal{D}_s^{aug}$ as $B^{aug}$. 
%     \STATE Split: $\tilde{\mathcal{D}_s}$ and $\tilde{\mathcal{D}_t}$ $\xleftarrow{} B$, Pick out: $\tilde{\mathcal{D}}_s^{aug}$ from $B^{aug}$.
%     \STATE Construct coalitions $S,T$ by randomly sampling from $\tilde{\mathcal{D}_s} \cup \tilde{\mathcal{D}}_s^{aug}$; construct coalitions $S\cup T, S\cap T$.
%     \STATE Calculate supermodularity regularization loss $\mathcal{L}_{sm}$ as Eq.~\eqref{eq:l_reg}.
%     \STATE Pick out low-quality samples $\mathcal{D}_{del}$ with the top-$k$ score calculated by Eq.~\eqref{eq:score}.
%     \STATE Calculate supervision loss $\mathcal{L}_{sup}$ as Eq.~\eqref{eq:l_cls}
%     \STATE Update $\boldsymbol{\theta} = \arg \min_{\boldsymbol{\theta}} \mathcal{L}_{sup} + \omega\mathcal{L}_{sm}$.
%     \ENDFOR
%     \end{algorithmic}
%   \end{algorithm}
%   \vspace{-4mm}