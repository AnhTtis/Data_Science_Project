


% \begin{table*}[]
%     \centering
%     \caption{Experimental results on the nuScenes validation set. The input resolution is $704\times 256$ using ResNet50 as the backbone.
%     % *denotes the method that runs with 16 past frames.
%     * denotes the re-implementation based on the provided code.}
%     \resizebox{0.75\linewidth}{!}{
%     \begin{tabular}{l|c|c|cccccc} \hline
%     \textbf{Methods}  & \# frame & \textbf{mAP}$\uparrow$  &\textbf{NDS}$\uparrow$  & \textbf{mATE}$\downarrow$ & \textbf{mASE}$\downarrow$   &\textbf{mAOE}$\downarrow$   &\textbf{mAVE}$\downarrow$   &\textbf{mAAE}$\downarrow$  \\ \hline
%     % CenterNet~\cite{zhou2019objects} & 
%     PGD*~\cite{wang2021PGD} & \multirow{4}*{1}   & 28.8 & 37.0 & 0.75 & 0.27 & 0.52 & 1.13 & 0.18   \\
%     BEVDet~\cite{huang2021bevdet} & & 29.8 & 37.9 & 0.73 & 0.28 & 0.59 & 0.86 & 0.24   \\
%     PETR~\cite{liu2022petr}& & 31.3 & 38.1 & 0.77 & 0.28 & 0.56 & 0.92 & 0.23    \\
%     DETR3D~\cite{detr3d} & & 34.9 & 43.4 & 0.72 & 0.27 & 0.38 & 0.84 & 0.20 \\ \hline
%     BEVDet4D~\cite{huang2021bevdet4d}  & \multirow{3}*{2} & 32.2 & 45.7 & 0.70 & 0.28 & 0.50 & 0.35 & 0.21 \\
%     BEVDepth~\cite{li2022bevdepth} & & 35.1 & 47.5 & 0.64 & 0.27 & 0.48 & 0.43 & 0.20 \\
%     % BEVDet4
%     DORT (Ours) & & \textbf{37.9} & \textbf{52.1} & 0.62 & 0.27 & 0.35 & \textbf{0.31} & 0.20  \\ \hline
%     % Mvtrans~\cite{} & \\
%     BEVDepth*~\cite{huang2021bevdet4d} &  \multirow{2}*{8} & 39.8 & 52.3 & 0.57 & 0.27 & 0.49 &0.27  & 0.18  \\
%     DORT (Ours)& & \textbf{41.8} & \textbf{53.4} & 0.57 & 0.26 & 0.43 & \textbf{0.25} & 0.19  \\ \hline
%     SOLOFusion~\cite{park2023time}& \multirow{2}*{16}  & 42.7 & 53.4  & 0.57 & 0.27  & 0.41  & 0.25 &  0.18 \\
%     DORT (Ours) & &  \textbf{43.6} & \textbf{54.0} & 0.56 & 0.26 & 0.41 & \textbf{0.24} & 0.18 \\ \hline
%     \end{tabular}}
%     \label{tab:nusc_val}
%     \end{table*}






\begin{table*}[t]
    \centering
    % \vspace{-3mm}

    \caption{Experimental results on the nuScenes validation set. The input resolution is $704\times 256$ using ResNet-50 as the backbone.
    % *denotes the method that runs with 16 past frames.
    * denotes the re-implementation based on the provided code.}
    \vspace{-2mm}

    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{l|c|ccccccc} \hline
    \textbf{Methods}  & \# frame & \textbf{mAP}$\uparrow$  & \textbf{mATE}$\downarrow$ & \textbf{mASE}$\downarrow$   &\textbf{mAOE}$\downarrow$   &\textbf{mAVE}$\downarrow$   &\textbf{mAAE}$\downarrow$ & \textbf{NDS}$\uparrow$  \\ \hline
    % CenterNet~\cite{zhou2019objects} & 
    PGD*~\cite{wang2021PGD} & \multirow{4}*{1}   & 28.8  & 0.75 & 0.27 & 0.52 & 1.13 & 0.18 & 37.0   \\
    BEVDet~\cite{huang2021bevdet} & & 29.8& 0.73 & 0.28 & 0.59 & 0.86 & 0.24  & 37.9   \\
    PETR~\cite{liu2022petr}& & 31.3  & 0.77 & 0.28 & 0.56 & 0.92 & 0.23 & 38.1    \\
    DETR3D~\cite{detr3d} & & 34.9  & 0.72 & 0.27 & 0.38 & 0.84 & 0.20 & 43.4\\ \hline
    BEVDet4D~\cite{huang2021bevdet4d}  & \multirow{3}*{2} & 32.2 & 0.70 & 0.28 & 0.50 & 0.35 & 0.21 & 45.7 \\
    BEVDepth~\cite{li2022bevdepth} & & 35.1  & 0.64 & 0.27 & 0.48 & 0.43 & 0.20 & 47.5\\
    % BEVDet4
    DORT (Ours) & & \textbf{37.9}& 0.62 & 0.27 & 0.35 & \textbf{0.31} & 0.20 & \textbf{52.1}   \\ \hline
    % Mvtrans~\cite{} & \\
    BEVDepth*~\cite{huang2021bevdet4d} &  \multirow{2}*{8} & 39.8 & 0.57 & 0.27 & 0.49 &0.27  & 0.18  & 52.3 \\
    DORT (Ours)& & \textbf{41.8}  & 0.57 & 0.26 & 0.43 & \textbf{0.25} & 0.19 & \textbf{53.4} \\ \hline
    SOLOFusion~\cite{park2023time}& \multirow{2}*{16}  & 42.7 & 0.57 & 0.27  & 0.41  & 0.25 &  0.18  & 53.4 \\
    DORT (Ours) & &  \textbf{43.6}  & 0.56 & 0.26 & 0.41 & \textbf{0.24} & 0.18 & \textbf{54.0} \\ \hline
    \end{tabular}}
    \label{tab:nusc_val}
    \vspace{-3mm}
    \end{table*}


% \begin{figure}[h] \centering \begin{subfigure}{0.45\textwidth} % adjust this to set the width of the left-hand table \centering \begin{tabular}{|c|c|} \hline Column 1 & Column 2 \\ \hline Data 1 & Data 2 \\ Data 3 & Data 4 \\ \hline \end{tabular} \caption{Left-hand table} \label{fig:left} \end{subfigure} \hfill % this inserts some horizontal space between the tables \begin{subfigure}{0.45\textwidth} % adjust this to set the width of the right-hand table \centering \begin{tabular}{|c|c|} \hline Column A & Column B \\ \hline Data A & Data B \\ Data C & Data D \\ \hline \end{tabular} \caption{Right-hand table} \label{fig:right} \end{subfigure} \caption{Two tables side-by-side} \label{fig:both} \end{figure} 





\begin{table}[htb]
    \centering
    % \vspace{-3mm}
    \caption{3D object tracking results on the nuScenes validation set. We adopt ResNet-50 as the backbone and set the input resolution as $704\times 256$.}
    \vspace{-3mm}
\resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|ccc} \hline
         Method & AMOTA$\uparrow$ & AMOTP$\downarrow$ &   Recall$\uparrow$  \\ \hline
         QD-Track3D~\cite{Hu2021QD3DT} & 24.2 & 1.518 & 39.9 \\
         Time3D~\cite{Li_2022_time3d} &  21.4 & 1.360 & N/A \\ 
         TripletTrack~\cite{Marinello2022CVPR} & 28.5 & 1.485 & N/A \\ 
         MUTR3D~\cite{zhang2022mutr3d} & 29.4 & 1.498 & 42.7 \\ 
         QTTrack~\cite{Yang2022QualityTrack} & 34.7 & 1.347 & 46.2 \\ \hline
         DORT & \textbf{42.4} & \textbf{1.264} & 49.2 \\  \hline
    \end{tabular}}
    \label{tab:nusc_track_val}
    \vspace{-5mm}
\end{table}





\section{Experiments}
We validate the effectiveness of our method on the large-scale nuScenes detection and tracking benchmark~\cite{nuscene}.
% on the large-scale nuScenes training dataset. 
% We evaluate the performance on both detection and tracking benchmarks.
% We mainly report the experimental results on the nuScenes 



\subsection{Experimental Setup}
In this section, we describe the used dataset, the evaluation metrics, and the implementation details.

\noindent{\textbf{Dataset}}\quad
NuScenes~\cite{nuscene} is a large-scale autonomous driving dataset, which contains 1,000 video sequences. 
The official protocol splits the video sequences into 700 for training, 150 for validation, and 150 for testing. 
Each sequence is annotated with 3D bounding boxes, object velocity, and the tracking id to connect the objects across the frame. 
The camera calibration information, timestamps, and ego-motion in both training and test sets are provided.

\noindent{\textbf{Detection Metrics}}\quad
We adopt the official evaluation protocol provided by nuScenes benchmark~\cite{nuscene}. 
The official protocol evaluates 3D detection performance by the metrics of average translation error (ATE), average scale error (ASE), average orientation error (AOE), average velocity error (AVE), and average attribute error (AAE).
Besides, it also measures the mean average precision (mAP) with considering different recall thresholds. Instead of using 3D Intersection over Union (IoU) as the criterion, nuScenes defines the match by 2D center distance $d$ on the ground plane with thresholds $\{0.5, 1, 2, 4\}m$.
The above metrics are finally combined into a nuScenes Detection Score (NDS).

\noindent{\textbf{Tracking Metrics}}
Regarding the tracking metrics, the nuScenes benchmark mainly measures the average multi-object tracking accuracy (AMOTA), average multi-object tracking precision (AMOTP), and tracking recall. In particular, AMOTA and AMOTP are the averages of multi-object tracking accuracy (MOTA) and multi-object tracking precision (MOTP) under different recall thresholds. 
% For the nuScenes dataset, we adopt the official AP (average precision with threshold of 0.5m, 1.0m, 2m, and 4m) and ATE (average translation error) to evaluate the localization accuracy of the trained detectors.

\noindent{\textbf{Network Details}}
As discussed in Sec~\ref{sec:recurrent}, the recurrent module requires a proposal detector to generate candidate foreground regions as the $1^{st}$ stage input. We adopt the popular monocular 3D detector PGD~\cite{wang2021PGD} due to its high 2D object detection recall.
Following~\cite{huang2021bevdet, huang2021bevdet4d, li2022bevdepth}, we adopt the ResNet-50~\cite{he2016deep} with FPN as the 2D feature extractor and mainly conduct experiments on this setting. The 2D feature extractors in PGD and the recurrent module are shared to save computation time. The grid size of the 3D volume is set as $0.8m$ with the range of $[-5m, 5m]$ in the X and Z (depth) axis and $[-4m, 2m]$ in the Y (height) axis. 
During 2D to 3D features transformation, we follow~\cite{li2022bevdepth} and adopt the depth distribution guided 2D to 3D features lifting. 
% feature lifting with
Regarding the test set submission, we follow~\cite{li2022bevdepth, li2022bevstereo} and adopt the ConvNeXt-Base~\cite{liu2022convnet} as the image backbone. 
The image backbone is initialized with ImageNet pre-trained weights, and no other external data is used.
We provide more details about the network architecture of the recurrent module and the training configuration in the supplementary materials.

% \begin{figure*}[htb]
%     \centering
%     \includegraphics[width=0.9\textwidth]{cvpr23/figures/fig4_qualitative.pdf}
%     % \vspace{-4mm}
%     \caption{Visualization of our detection results before and after refinement. The blue, green, and red colors denote the bounding box before refinement, after refinement and the ground truth. The bird-eye-view images are cropped to better highlight the foreground objects. }
%     % \vspace{-5mm}
%     \label{fig:fig4_qualitative}
% \end{figure*}



% We use AdamW~\cite{} to train the model with 24 epcosh and CBGS data sample strategies. The weight decay is set as $10^{-2}$ 
\noindent{\textbf{Training Configurations}}
The model is optimized by AdamW optimizer with weight decay $10^{-2}$.
We first follow~\cite{wang2021PGD} to train the proposal detector and refine the recurrent module with 24 epochs, where the initial learning rate is set as $2\times 10^{-4}$ and decreases to $2\times 10^{-5}$ and $2\times 10^{-6}$ at the $18^{th}$ and $22^{th}$ epochs. Following~\cite{huang2021bevdet, li2022bevdepth}, we use the class balance sampling strategy (CBGS) to alleviate the class imbalance problem. 
We adopt the commonly used 2D data augmentation that randomly flips the image, resizes the image with the range of $[0.36, 0.55]$, and crops the image to the resolution of $704 \times 256$.
Regarding the input video sequences, we follow~\cite{huang2021bevdet4d, li2022bevdepth} and sample the preceding keyframes to obtain the past video sequences.
Regarding the test set submission, we enlarge the input resolution to $1600 \times 640$ and reduce the volume size to $0.4m$.
% For fair comparison, we also conduct experiments using a larger 2D backbone (ResNet101~\cite{he2016deep}) and larger input resolution ($1504\times 640$).
% More experimental results with larger backbones and higher resolution inputs can be found in supplementary materials.
% For the experimental results with larger backbone, higher resolution, we provide them in the Appendix. 

% the intiial bounding box proposal and 

% \begin{align*}
%     \frac{P_{T - \Delta T \rightarrow T}}{\Delta T} =  \frac{P_{T - 2\cdot \Delta T \rightarrow T}}{2\cdot \Delta T} =  ... = \frac{P_{T - n\cdot \Delta T \rightarrow T}}{n\cdot \Delta T}
% \end{align*}


% \begin{table}[]
%     \centering
%     \caption{Experimental results of DORT with different numbers of frames  on the nuScenes validation set.
%     ResNet50 is adopted as the backbone and input resolution is s $704\times 256$.}
%     % *denotes the method that runs with 16 past frames.
%     % * denotes the re-implementation based on the provided code.}
%     \setlength{\tabcolsep}{1pt}

%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c|c|cccccc} \hline
%     \# frame & mAP$\uparrow$  &NDS$\uparrow$  & mATE$\downarrow$ & mASE$\downarrow$   &mAOE$\downarrow$   &mAVE$\downarrow$   &mAAE$\downarrow$  \\ \hline
%     % CenterNet~\cite{zhou2019objects} & 
%     % PGD*~\cite{wang2021PGD} & \multirow{4}*{1}   & 28.8 & 37.0 & 0.75 & 0.27 & 0.52 & 1.13 & 0.18   \\
%     % BEVDet~\cite{huang2021bevdet} & & 29.8 & 37.9 & 0.73 & 0.28 & 0.59 & 0.86 & 0.24   \\
%     % PETR~\cite{liu2022petr}& & 31.3 & 38.1 & 0.77 & 0.28 & 0.56 & 0.92 & 0.23    \\
%     % DETR3D~\cite{detr3d} & & 34.9 & 43.4 & 0.72 & 0.27 & 0.38 & 0.84 & 0.20 \\ \hline
%     % BEVDet4D~\cite{huang2021bevdet4d}  & \multirow{3}*{2} & 32.2 & 45.7 & 0.70 & 0.28 & 0.50 & 0.35 & 0.21 \\
%     % BEVDepth~\cite{li2022bevdepth} & & 35.1 & 47.5 & 0.64 & 0.27 & 0.48 & 0.43 & 0.20 \\
%     % % BEVDet4
%     2 &37.9 & 52.1 & 0.62 & 0.27 & 0.35 & 0.31 & 0.20  \\ 
%     % Mvtrans~\cite{} & \\
%     % BEVDepth*~\cite{huang2021bevdet4d} &  \multirow{2}*{8} & 39.8 & 52.3 & 0.57 & 0.27 & 0.49 &0.27  & 0.18  \\
%     4 & 40.2 & 52.9 & 0.59 & 0.27 & 0.35 & 0.29 & 0.20 \\
%     8 & 41.8 & 53.4 & 0.57 & 0.26 & 0.35 & 0.25 & 0.19  \\ 
%     % SOLOFusion~\cite{park2023time}& \multirow{2}*{16}  & 42.7 & 53.4  & 0.57 & 0.27  & 0.41  & 0.25 &  0.18 \\
%     16 &  43.6 & 54.0 & 0.56 & 0.26 & 0.35  & 0.24 & 0.18 \\ \hline
%     \end{tabular}}
%     \label{tab:nusc_val}
%     \end{table}

\subsection{Main Results}
In Table~\ref{tab:main_test}, we first provide the comparison of our framework with existing state-of-the-art methods on the nuScenes test benchmarks. 
We draw the following observations: 
(i) Benefiting from dynamic objects modeling, our method displays a significant improvement in both object detection (mAP) and motion estimation (mAVE),  and 0.5\%  and relatively 7.4\% better than the previous best method. These localization and motion estimation improvements also contribute to state-of-the-art results in terms of the nuScenes detection metric (NDS).
(ii) With strong localization and motion estimation results, our tracking module can better associate the detected objects in different timestamps, resulting in superior performance over all the other trackers with different metrics.
Specifically, we improve the second best tracker~\cite{li2022uvtr}, another distance-based tracker with 10.9\% and 7.5\% relative improvements on the AMOTA and AMOTP metrics.  Compared with the joint detection and tracking methods QDTrack3D~\cite{Hu2021QD3DT} and Time3D~\cite{Li_2022_time3d}, the performance gain of our method demonstrates the effectiveness of our dynamic objects modeling framework in jointly modeling object motion and location. 
(iii) In Table~\ref{tab:nusc_val} and ~\ref{tab:nusc_track_val}, we also report our method on the nuScenes validation set with different settings. For the detection performance, we can draw the same observation as in the test set that our method can outperform previous temporal-based methods~\cite{huang2021bevdet4d, li2022bevdepth, park2023time} in terms of mAP and mAVE. 
Note that our method is also compatible with the components designed in the current BEV-based frameworks, such as the training techniques in BEVDepth~\cite{li2022bevdepth} and the depth estimation module in SOLOFusion~\cite{park2023time}. Furthermore, the local 3D volume is more friendly to practical applications, which can handle objects with arbitrary depth in the image.
% (ii) The performance gain of birds-eye-view based methods (\textit{e.g.} BEVDepth~\cite{li2022bevdepth}, BEVDet4D~\cite{li2022bevformer} over perspective-view based method (\textit{e.g.}, PGD~\cite{wang2021PGD}) mainly comes from the depth supervision in 2D-to-3D transformation in BEVDepth~\cite{li2022bevdepth} and the temporal features aggregation in BEVDet4D~\cite{huang2021bevdet4d} %~\cite{huang2021bevdet4d, li2022bevformer}.
% (ii) Based on the strong object localization and motion estimation performance, our tracking module further 


% Compared with the temporal-based methods, such as BEVDet4D~\cite{huang2021bevdet4d}, BEVDepth~\cite{li2022bevdepth}, our framework still shows significant superiority, even just maintaining a local 3D volume.
\begin{figure}[!htb]
    \centering
    % \vspace{-5mm}
    \subfloat[mAP in each iteration]{
    \includegraphics[width=0.45\columnwidth, angle=0,trim= 15 0 15 0, clip]{figures/map_loop.pdf}
    \label{fig:fig1a}
    }
    \quad
    \subfloat[mAVE in each iteration]{%
    \label{fig:fig1b}%
    \includegraphics[width=0.45\columnwidth, angle=0,trim= 15 0 15 0, clip]{figures/mave_loop.pdf}
    }%    
    \caption{Detection results of each iteration in the recurrent pipeline. 1 past frame is used in temporal modeling.}
    \label{fig:iteration}
    \vspace{-5mm}
\end{figure}
% In Table~\ref{tab:main_test_set_det}, we also display the detection performance of our method and recent state-of-the-art method on the nuScenes test set. We can draw the same observation that DORT can improve the previous methods in terms of mAP and mAVE for location and velocity prediction.
% % detection range would not be limited during training. 

% In addition to the detection performance, we also display the 3D object tracking results of our method and recent state-of-the-art approaches in Table~\ref{tab:main_test_set_track}. 
% Benefiting from the recurrently dynamic object modeling, our simple distance-based tracker achieves superior performance over all the other trackers with different metrics. Specifically, .






% We first compare our framework with existing state-of-the-art methods on the nuScenes validation benchmark 

% introduce the experimental results on the nuScenes validation set;
    % list the results of resnet50, comparied with bevdet, bevdepth, bevstereo, bevdepth4D, bevdet4D.
    % list the results of resnet101 compared with pgd, detr3d, petrv2, and bevformer.

% list the final experimental results on the nuScenes test set. may need to outperform time when still.

\begin{table}[tb]
    \vspace{-3mm}
    \centering
    \caption{Ablation study of the local volumes on the nuScenes validation set.
     * denotes the single frame version without using EMA for fair comparisons. 
     The first term and the second term in FLOPS denote the operation in perspective view and bird-eye-view, respectively.}
\resizebox{0.85\linewidth}{!}{    \begin{tabular}{l|ccc} \hline
    Method & mAP & NDS & FLOPS \\ \hline
    PGD~\cite{wang2021PGD} & 28.8 & 37.0 & 238.2 + 0 \\ 
    BEVDepth~\cite{li2022bevdepth} & 33.3 & 40.6 & 120.4 + 74.4 \\ 
    PGD + Local Volume & 33.1 & 40.5 & 238.2 + 25.2 \\\hline
    \end{tabular}}
    \label{tab:local}
    \vspace{-5mm}
\end{table}



\subsection{Ablation Study}

\noindent{\textbf{Effectiveness of Object-wise Local Volume}}\quad
We first show the effectiveness of the object-wise local volume in the single-frame setting. Since there is no motion modeling and temporal volume construction, the experiments are conducted with one iteration in the recurrent step. As shown in Table~\ref{tab:local}, the object-wise local volume can improve the proposal detector PGD with a large margin and achieve comparable performance with BEVDepth, which exploits a global volume. In addition, the computation cost for the local volume is relatively small compared to the global 3D volume, enabling us to achieve multiple iterations and use more preceding frames to boost the performance. Note that the proposal detector also can be replaced by a more efficient one.% We choose PGD due to its fast converage and 
% We can observe that the object-wise local volume still can 

\noindent{\textbf{Ablation Study of Object Motion}}\quad
We further validate the influence of different dynamic object modeling strategies on the detection performance The first experiment compares the assumed static case with that of using the ground truth object motion. As shown in Table~\ref{tab:motion}, the model with ground truth object motion outperforms the assumed static with 4.3\% mAP, demonstrating the necessity of object motion for obtaining accurate temporal correspondence features. When we replace the ground truth object motion with an estimated one, it still can bring 2.9\% mAP improvements, illustrating the usefulness of our dynamic objects modeling module.

\begin{table}[tb]
    \centering
    \caption{Experimental results of different motion modeling strategies on the nuScenes validation set. The GT motion is obtained from the ground truth velocity. 1 past frame is used in temporal modeling.}
    \resizebox{0.7\linewidth}{!}{\begin{tabular}{c|ccc}\hline
    Setting & mAP & NDS & mAVE \\ \hline
   Assumed static & 35.0 & 47.1 & 0.37 \\
   GT motion & 39.3 & - & - \\ 
   Pred motion  & 37.9 & 52.1 & 0.31 \\ \hline
    \end{tabular}}
    \label{tab:motion}
    \vspace{-5mm}
\end{table}


\noindent{\textbf{Experiments with Different Iterations}}\quad
In Figure~\ref{fig:iteration}, we provide the comparison of modeling object motion and assumed static with different recurrent iterations. Benefiting from the BEV features modeling, the two configurations display almost 2\% mAP improvements in the first iterations. In the later iterations, the improvement in assumed static stops, mainly due to the lack of accurate temporal features. While with more and more accurate temporal features, the model with modeling object motion can progressively improve the detection and motion estimation results. 
% In the first iteration, the results with and without modeling object motion first achieves almost 2\% improvement, where the performance gain mainly comes from the bev features~\cite{li2022bevdepth}. 
% of using two frames is relative small over the 

% is replace the 

% \subsection{Qualitative results}
% Fig~\ref{fig:fig4_qualitative} shows some qualitative results before and after recurrent refinement. By comparing the results before refinement (blue) and after refinement (green), we can observe that our refinement module can effectively reduce the localization error. 
% % More qualitative results can be seen in the supplementary.
% % As shown in the BEV images, the recurrent module localization after refinement (green) 

