
\section{Methodology}
This section describes the details of DORT. 
% \jiangmiao{be specific, can be based on any framework? if not, which did you use?}
% DORT is based on the current temporal-based framework (\textit{e.g.} DfM~\cite{wang2022dfm}, BEVDet4D~\cite{huang2021bevdet4d}) and extends to handle both static and moving objects. 
% Note that our work mainly focuses on designing a general joint detection and motion optimization head, and it can be used as a plug-in module in most 3D detectors~\cite{park2023time, li2022bevformer, huang2021bevdet4d}.
% Hence our detection performance also can benefit from the components designed in the current BEV-based frameworks, such as the training techniques in BEVDepth~\cite{li2022bevdepth} and the depth estimation module in SOLOFusion~\cite{park2023time}. What's more, the designed local 3D volume is also more friendly to practical applications, which can handle objects with arbitrary depth in the image.
DORT is a general joint detection and motion prediction module that can estimate coupled object location and motion results across frames. Based on the tightly coupled location and motion results, DORT is also capable of simultaneously 3D object detection and tracking. Basically, it can be based on most temporal 3D detectors~\cite{huang2021bevdet4d, park2023time, li2022bevformer}. In this work, we select the popular temporal detector BEVDepth~\cite{li2022bevdepth} as the base detector and extend it to handle both static and moving objects in temporal modeling.
We first present an overview of temporal-based frameworks in Sec~\ref{sec:dfm} and then introduce our modifications: 
the local volume for object-wise representation in Sec~\ref{sec:local_volume},
the key recurrent dynamic objects modeling in Sec.~\ref{sec:recurrent},
%the cost volume construction and recurrent refinement framework in Sec~\ref{sec:cost_volume}, 
%the multiple estimation fusion module in Sec~\ref{sec:fusion},
and the object association for monocular 4D object detection in Sec~\ref{sec:track}.

\subsection{Overview of Temporal-Based Frameworks}
\label{sec:dfm}
Previous temporal-based camera-only 3D detection methods contain three stages: 
(1) The 2D features extraction stage extracts the features from the input images.
(2) The view transformation and stereo matching process that first lift the 2D features to a global 3D volume and then warp the features in each frame to an aligned canonical space for stereo matching. Depending on the model design, the view transformation and stereo matching order may reverse.
(3) The detection stage fuses the 3D features from different observations (monocular and stereo) and estimates 3D bounding boxes based on a BEV-based detection head.

In this work, we follow previous methods~\cite{wang2022dfm, huang2021bevdet, li2022bevdepth} and adopt the widely-used 2D backbone (\textit{e.g.} ResNet~\cite{he2016deep}) to do the features extraction.
For the view transformation stage, we design an \textbf{object-wise local volume} that leverages the candidate 3D boxes to obtain the potential foreground regions and only models them with local object-wise 3D volumes.
For the stereo matching and detection stages, we propose a \textbf{recurrent dynamic objects modeling} module. Specifically, we adopt a recurrent paradigm that progressively refines the detection and motion results by concurrently updating the corresponding 3D volume features thereon.


\begin{figure}
    \centering
    \includegraphics[width=0.44\textwidth]{figures/fig4_iccvv5.pdf}
    \vspace{-2mm}
    \caption{The process of extracting local volumes in the current and past frames according to predicted bounding boxes and object motion.}
    \vspace{-5mm}
    \label{fig:local}
\end{figure}


\subsection{Object-wise Local Volume}
\label{sec:local_volume}
In previous work, the 2D-3D transformation considers each candidate 3D grid point and constructs a global volume for 3D detection. However, there are several limitations:
(1) the global 3D volume contains lots of background regions, which is not vital for 3D detection but largely increases the computation burden.
(2) Modeling a global volume needs to pre-define a detection range during training, making the detectors fail to detect objects with arbitrary depths.
(3) It is inconvenient to maintain and manipulate a global 3D volume with various object-wise operations.

Hence, we replace the global 3D volume with an object-wise local volume. Specifically, we leverage the candidate bounding boxes to determine the 3D region of interest (RoI) and set the local volume center as the bounding box center.
To keep the object ratio and achieve cross-view warping, we assign each 3D RoI volume $V \in \mathcal{R}^{W\times H\times L\times C}$ with the same 3D dimension $(W, H, L)$ and channel size $C$.  
Different from 2D detection, the objects' dimension in 3D space has less variance and empirically relies less on the RoI-Align~\cite{he2017mask} operation.
We display the construction of object-wise local volumes in Fig~\ref{fig:local}.
For the 2D to 3D transformation, we first follow LiftSplat~\cite{philion2020lift} and lift the images to a 2.5D frustum by weighting with depth probability. Then we utilize the grid sample operation to warp the features from the 2.5D frustum to each 3D local volume. 
Benefiting from the accurate 2D detection performance of perspective-view detection, the local volume features sampled from the 2.5D frustum would have a large overlap with the foreground objects. Hence, if the proposal 3D location is inaccurate, the later refinement module still has the ability to use the features to do the refinement.
% Besides, since the features is generated from a 
% warp each 3D point in the volume to the 2.5D frustum to obtain the sampling location. Based on the sampling location, the obtain the projection of each 3D point in the 2.5D frustum by  utilize the camera intrinsic to warp the 3D point in the volume and weigh the 3D features by the corresponding depth probability.
% Given a grid point $p \in R^{3}$ in the local volume, its features can be obtained via:
% \begin{align}
%     V(p) = D\\left(\pi^{-1}(p, K)\right) \cdot I\left(\pi^{-1}(p, K)\right),
% \end{align}
% where $\pi^{-1}$ denotes the projection from the camera coordinate to the image coordinate, $D$ and $I$ denote the depth probability and 2D feature map, respectively. 

\subsection{Recurrent Dynamic Objects Modeling}
\label{sec:recurrent}

The pipeline of the recurrent framework is illustrated in Fig.~\ref{fig:pipeline}. 
% As illustra
% It takes candidate 3D bounding boxes and motion estimation in the current frame as input, constructs temporal cost volumes thereon, and aggregates these cues to predict the refined boxes and their motion.
% In particular, we adopt a perspective-view based 3D detector (\textit{i.e.} PGD) to generate the initialized candidate 3D boxes and motion and only predict their residuals for refinement in the subsequent recurrent updates. Next, we elaborate on these steps in detail.
Given the candidate 3D bounding boxes and motion as input, each iteration first constructs the temporal cost volumes thereon, and aggregates these cues to refine the proposal boxes and motion. 
% Each iteration takes the candidate 3D bounding boxes and motion as input, constructs temporal cost volumes thereon, and aggregates these cues to refine the input boxes and their motion.
% the refined boxes and their motion.
% For the input of the first iteration, we adopt a perspective-view based 3D detector (\textit{i.e.} PGD) to generate the initialized candidate 3D boxes and motion. The refinement module only predict the residuals of input for refinement in the subsequent recurrent updates. Next, we elaborate on these steps in detail.
In particular, we adopt a perspective-view based 3D detector (\textit{i.e.} PGD) to generate the initialized candidate 3D boxes and motion and only predict their residuals for refinement in the subsequent recurrent updates. Next, we elaborate on these steps in detail.
% The input of each iteration is the candidate 3D bounding box and object motion. For the first iteration, we adopt the front-of-view 3D detector (\textit{i.e.} PGD) to generate candidate 3D boxes and motion. For the other iterations, we take the output from the last iteration as the input. Then we leverage the input box and motion to obtain the object-wise cost volumes by comparing the features between the source frame and the reference frame. Based on the 3D volume, a refinement network takes the cost volume as input and estimates the residual between the candidate 3D box and object motion. Finally, we obtain more stable results by voting the estimation from each paired frame. In the recurrent module, we represent the object motion as the translation velocity with time intervals.

\begin{table*}
\centering
\caption{Experimental results of monocular 3D object detection and tracking on the nuScenes test set. The input resolution is $1600\times 900$ with using ConvNeXt-Base~\cite{liu2022convnet} as the backbone.}
\label{tab:main_test}
\begin{subfigure}{0.56\textwidth}
\makeatletter\def\@captype{table}
    
% \begin{table}[!htb]
% \label{tab:nus_test}
% \vspace{-5mm}
\centering
\caption{3D detection results on the nuScenes test set.}

\vspace{-2mm}
\label{tab:main_test_set_det}
% \tiny

\setlength{\tabcolsep}{1pt}
\resizebox{0.95\linewidth}{!}{\begin{tabular}{l|ccccccccc} 
\hline
Method    & mAP$\uparrow$   & mATE$\downarrow$ & mASE$\downarrow$   &mAOE$\downarrow$   &mAVE$\downarrow$   &mAAE$\downarrow$   &NDS$\uparrow$\\\hline
% \midrule
% BEVDet4D & 45.1 & 56.9 & 0.51 & 0.24 & 0.38 & 0.30 & 0.12 \\
Ego3RT~\cite{lu2022ego3rt} & 42.5  & 0.55 & 0.26 & 0.43 & 1.01 & 0.14 & 47.3 \\
UVTR~\cite{li2022uvtr} & 47.2 & 0.57 & 0.25 & 0.39 & 0.51 & 0.12 & 55.1  \\ 
% BEVFormer     &  44.5 &53.5 & 0.63 & 0.26 & 0.41 & 0.44 & 0.14 \\
BEVFormer~\cite{li2022bevformer} & 48.1 & 0.58 & 0.25 & 0.37 & 0.37 & 0.12 & 56.9  \\
PETRv2~\cite{liu2022petrv2} & 51.2  & 0.55 & 0.25 & 0.36 & 0.40 & 0.13 & 58.6 \\
BEVDepth~\cite{li2022bevdepth} & 52.0  & 0.45 & 0.24 & 0.35 & 0.35 & 0.13 & 60.9  \\
BEVStereo~\cite{li2022bevstereo} & 52.5  & 0.43 & 0.24 & 0.36 & 0.35 & 0.14 & 61.0\\ 
SOLOFusion~\cite{park2023time} & 54.0  & 0.45 & 0.26 & 0.37 & 0.27 & 0.14 & 61.9\\ \hline
DORT (Ours) & \textbf{54.5} & 0.44 & 0.26 & 0.37 & \textbf{0.25} & 0.14 & \textbf{62.5}\\  \hline
\end{tabular}}
% \end{table}

% \end{center}

\end{subfigure}
 \hspace{0.1cm}
 \begin{subfigure}{0.37\textwidth}
\makeatletter\def\@captype{table}
    
% \begin{table}[!htb]
% \vspace{-6mm}
% \centering
% \caption{3D tracking results on the nuScenes test set.}
% \vspace{-3mm}
% \label{tab:main_test_set_det}

    
\caption{3D  tracking results on the nuScenes test set.}
% \vspace{-3mm}
\label{tab:main_test_set_track}
% \tiny
\centering
\setlength{\tabcolsep}{2pt}
\resizebox{0.95\linewidth}{!}{\begin{tabular}{l|cccc} 
\hline
Method     & AMOTA$\uparrow$  &AMOTP$\downarrow$  & MOTAR $\uparrow$  \\\hline
% \midrule
QD-3DT~\cite{Hu2021QD3DT} & 21.7 & 1.550 & 56.3 \\
Time3D~\cite{Li_2022_time3d} & 21.4 & 1.360 & - \\
PolarDETR~\cite{Chen2022PolarDETR}         & 27.3 & 1.185 & 60.7 \\ 
MUTR3D~\cite{zhang2022mutr3d}           & 27.0 & 1.494 & 64.3 \\ 
SRCN3D~\cite{Shi2022SRCN3D} & 39.8 & 1.317 & 70.2 \\
QTTrack~\cite{Yang2022QualityTrack} & 48.0 & 1.100 & 74.7 \\
UVTR~\cite{li2022uvtr} & 51.9 & 1.125 & 76.4 \\ \hline
DORT(Ours) & \textbf{57.6} & \textbf{1.040} & \textbf{78.0} \\  \hline

% \hline
\end{tabular}}
\end{subfigure}
\vspace{-3mm}

\end{table*}

\subsubsection{Cross-Frame Cost Volumes Construction}
\label{sec:cost_volume}
% Instead of assuming objects are static, our work different from previous work~\cite{wang2022dfm, huang2021bevdet4d} that further models the object motion in constructing the cost volume.
Given the initial predictions of 3D boxes and their motion, we first obtain object-wise volume features following Sec.~\ref{sec:local_volume}. 
Then we can construct the temporal cost volumes by warping features from past frames to the current frame coordinates based on \emph{ego-motion}. 
In contrast to previous work~\cite{wang2022dfm,huang2021bevdet4d} assuming objects are static, we further involve the \emph{object motion} into the warping procedure.
Specifically, for each point $p \in \mathcal{R}^{3}$ in the object-wise local volume $V$, we query the corresponding features in previous frame $t - \Delta t$ with the consideration of ego-motion $T^{ego}$ and the object motion $T^{obj}$ and construct the cost as $\left[V(p), V_{t - \Delta t}(T^{obj}T^{ego}p) \right]$.
Note that we simplify the point motion as the object motion with a rigid-body assumption, which can approximate most of the cases in driving scenarios, especially for vehicles~\cite{li2020jst, zachary2021raft3d}.

\subsubsection{3D Boxes and Motion Residual Estimation}
Given the object-wise temporal features built from input 3D boxes and motion, we leverage a refinement network to estimate the residual between the input 3D boxes and motion with the ground truth. 
The refinement network contains several 2D/3D residual-based convolutional layers to extract the 3D volumes and 2D BEV features. 
The detailed architecture is presented in Supplementary. 
Formally, the refinement is formulated as the regression of 3D attribute residuals $\mathcal{B}$, including the object's 3D center $x, y, z$, 3D size $w, h, l$, rotation $\theta$ and velocity $v_x, v_y$.
Since we use the object velocity in the current frame to represent the object motion and assume constant velocity across frame, the supervision for some frames may contain noise(\textit{e.g.} inaccurate labels, violation of rigid-body assumption, etc). Hence, we model the residual as a Laplacian distribution and design the loss function as:
\begin{align}
    \label{eq:depth}
    \mathcal{L}_{refine} = \sum_{b\in\mathcal{B}}(\frac{\sqrt{2}}{\sigma_b} \|\Delta\hat{b} - \Delta b\| + \log{\sigma_b}).
\end{align}
Here, $\Delta b$, $\Delta\hat{b}$, and $\sigma_b$ are all the network outputs, and represent the ground truth residual, the estimated residual, and the estimated standard deviation of residual for each 3D attribute, respectively.

\subsubsection{Multiple Estimation Fusion}
\label{sec:fusion}
Given $n$ frame as inputs, we can obtain $n$ 3D volumes ($1$ for the local volumes from the referenced single view and $n-1$ for the paired cross-view cost volumes) and obtain $n$ estimated residuals from the above residual estimation module. Then we weigh the importance of each residual by the estimated deviation and fuse them to obtain an ensemble result.
The fusion formulation can be represented as:
\begin{align}
    \hat{b}_{fused} = \sum_{i=1}^{n}\frac{e^{\sigma_{b_i}} b_i}{\sum_{i=1}^{n}e^{\sigma_{b_i}}},
\end{align}
where $i$ denotes the volume index. 
% \LQ{Need a fusion formulation}
% For the prediction $b_i$ in from each volume $i$, 

For simplicity, here we only estimate the velocity measurement for the referenced frame, \emph{i.e.}, the fluctuation of object velocity across different frames would not be considered explicitly. The mechanism for multi-frame fusion is expected to handle this problem adaptively. At the same time, this constraint also provides additional cues when simultaneously estimating object location and motion from more than two frames.
% In this place, we model the object with constant velocity to alleviate the ill-posed problem of motion estimation.

\subsubsection{Recurrent Location and Motion Update}
After each iteration, we can obtain the refined bounding boxes with their motion and thus can derive the updated bounding boxes in different frames. With these updated locations and RoIs, we can further update the volume features and proceed to the next-round refinement. Note that any complex or learnable motion modeling can be integrated into this procedure. Here, to be consistent with the multiple estimation fusion designs, we still keep the constant velocity prediction for simplicity to derive the bounding boxes of previous frames. 

In the training stage, we follow the recurrent methods~\cite{zachary2020raft, zachary2021raft3d} in other tasks and set the loss weight for each iteration as the same.
% As for the training loss design, we also use a learnable mechanism to balance the supervision from different rounds of refinement:
The overall loss is represented as:
\begin{align}
    \mathcal{L} = \mathcal{L}_{pv} + \sum_{i=1}^{k} \mathcal{L}^{i}_{refine},
\end{align}
where $ \mathcal{L}_{pv}$ is the loss in the perspective-view detector~\cite{wang2021PGD}, $\mathcal{L}^i_{refine}$ is the refinement loss in each iteration and $k=3$ is the number of the iterations.

In the inference stage, we first take the perspective-view detector (\textit{i.e.} PGD~\cite{wang2021PGD}) to generate the initial 3D bounding boxes and their motion and then progressively refine them with three iterations. During each iteration, we first construct the volume features as discussed in Sec~\ref{sec:cost_volume} and feed them into the recurrent refinement module to estimate the 3D boxes and motion residuals for each paired frame input. Then we utilize the multiple estimation fusion module in Sec~\ref{sec:fusion} to fuse the estimated results and obtain the refined 3D boxes and motion as the next stage input. 
\subsection{Monocular 4D Object Detection}
\label{sec:track}
So far, we have introduced our recurrent framework for 3D detection from monocular videos.
Based on progressive refinement, our model can estimate tightly coupled object location and motion results and thus can easily associate the object detection results across frames, leading to joint 3D detection and tracking.
% With modeling object motion and location, our framework can easily associate the object detection results across frames, leading to joint 3d detection and tracking. 
Specifically, we follow~\cite{Bewley2016_sort, zhou2020tracking, yin2021center, pang2021simpletrack} and associate the detection results by warping current detection to the past frames with object motion. 
Based on the ego-motion, we first convert the predicted object location to the past frame coordinate and then warp with the estimated object velocity. Then we follow the popular distance-based tracker in 2D task~\cite{Bewley2016_sort, yin2021center} and associate the objects by the closest distance matching. We also utilize the Kalman filter to maintain the trackers' location and velocity.
We provide more details of the tracking pipeline in the supplemental materials.
