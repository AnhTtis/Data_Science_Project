


\section{Object Motion in Temporal Modeling}
% \jiangmiao{add a summary of this section.}
% In this section, we first provide a theoretical and empirical analysis to show the necessity of modeling object motion in current temporal-based methods~\cite{wang2022dfm, huang2021bevdet4d}, and then briefly discuss the ill-posed problem in 3D motion modeling.

In this section, we first provide a theoretical and empirical analysis to demystify the adverse effects of neglecting object motion in temporal modeling~\cite{wang2022dfm, huang2021bevdet4d}, and then briefly discuss the challenges of modeling 3D motion in the monocular setting.

\subsection{Localization Bias from the Static Assumption}
% In previous temporal-based methods~\cite{wang2022dfm, huang2021bevdet4d, li2022bevformer}, the object motion is ignored by assuming objects are static across frames. We first show that the static assumption would derive a biased depth in temporal modeling.
% \jiangmiao{to be more specific, what is ``the same coordinate"? The most critical factor is not this?}
In previous temporal-based methods~\cite{wang2022dfm, huang2021bevdet4d, li2022bevformer}, the object motion is ignored by assuming objects are static across frames and the temporal features are directly aggregated after converting the past frames to the current frame. We first show that the static assumption would derive a biased depth in temporal modeling.
% In previous temporal-based methods~\cite{wang2022dfm, huang2021bevdet4d, li2022bevformer}, the temporal aggregation modules ignore the object motion and directly aggregate the temporal features after converting the past frames into the current frame coordinate.
% We first show that the static assumption would derive a biased depth for the moving object.
Without loss of generality, we consider the two-view case here, and the derived analysis can be naturally extended to more than two views.
We denote the camera intrinsic as $K$ and the ego-motion from frame $t_0$ to frame $t_1$ as $T_{t_0\rightarrow t_1}^{ego}$:
\begin{align}
    K = \left[\begin{array}{cccc}
        f & 0 & c_u  \\
        0 & f & c_v \\
        0 & 0 & 1  \\
    \end{array}\right], 
    T_{t_0\rightarrow t_1}^{ego} = 
    \left[
    \begin{array}{cccc}
    1 & 0 & 0 & x^{ego} \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & z^{ego} \\
    \end{array}
    \right].
\end{align}
Here, $f$ is the camera's focal length, and $(c_u, c_v)$ is the camera center coordinates in the image. For simplicity, we assume the ego-motion only contains the translation $(x^{ego}, 0, z^{ego})$ on the horizontal plane. The analysis also can be easily extended to a more complicated case that the motion contains rotation.
% We provide a more complicated case that the motion contains rotation in supplementary.

Given the multiple-view images, temporal-based methods can utilize photometric or featuremetric similarity to find the correspondence of pixel $p_{t_0} = (u_{t_0}, v_{t_0})$ in the past frame $t_0$ and the pixel $p_{t_1} = (u_{t_1}, v_{t_1})$ in the current frame $t_1$. The depth $z_{t_1}$ can be recovered by solving the correspondence:
\begin{align}
    \label{eq:nomotion}
    & T_{t_0\rightarrow t_1}^{ego}\cdot \pi(p_{t_0}, K) = \pi(p_{t_1}, K), \\
    & z_{t_1} = \frac{z^{ego}(u_{t_0} - c_u) - fx^{ego}}{u_{t_0} - u_{t_1}}, 
\end{align}
where $\pi$ denotes the projection from 2D image coordinate to 3D camera coordinate.
The above derivation assumes that the object is static across two frames. However, in the driving scenario, objects can move with a corresponding object motion $T^{obj}_{t_0 \rightarrow t_1}$.
Suppose the object motion only contains the translation on the horizontal plane in a short time interval, it can be represented as:
\begin{align}
    T^{obj}_{i \rightarrow j} = \left[
        \begin{array}{cccc}
            1 & 0 & 0 & x^{obj} \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & z^{obj} \\
        \end{array}
    \right].
\end{align}
Then the corresponding equation to recover depth can be revised as follows:
\begin{align}
    \label{eq:motion}
    & T^{obj}_{t_0 \rightarrow t_1}T_{t_0\rightarrow t_1}^{ego} \cdot \pi(p_{t_0}, K) = \pi(p_{t_1}, K),\\
    & \hat{z}_{t_1} = \frac{(z^{ego} + z^{obj})(u_{t_0} - c_u) - f(x^{ego} + x^{obj})}{u_{t_0} - u_{t_1}}.
\end{align} 
Based on Eq~\eqref{eq:nomotion} and \eqref{eq:motion}, we can obtain the depth gap:
\begin{align}
    \Delta z = \frac{z^{obj} (u_{t_0} - c_u) - fx^{ego}}{u_{t_0} - u_{t_1}}.
    \label{eq:bias_depth}
\end{align}



% \begin{figure}
% \centering
% \begin{subfigure}
%   \includegraphics[width=0.38\textwidth]{cvpr23/figures/fig2_analysis.pdf}
%   \label{fig:sfig1_stat}
% \end{subfigure}%
% \begin{subfigure}
%   \includegraphics[width=0.08\textwidth]{figures/fig6_vis.pdf}
%     \caption{Empirical analysis of the depth bias from assumed static on the nuScenes dataset. The left figure collects the average depth error with different object motion on the nuScenes dataset. The right figure is the histogram of object velocity.}
%   \label{fig:sfig1_stat}
% \end{subfigure}%
% \caption{(a). Empirical analysis of the depth bias from assumed static on the nuScenes dataset. The left figure collects the average depth error with different object motion on the nuScenes dataset. The right figure is the histogram of object velocity. (b). }
% \label{fig:fig}
% \end{figure}

% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.38\textwidth}
%          \centering
%   \includegraphics[width=\textwidth]{cvpr23/figures/fig2_analysis.pdf}
%          % \caption{$y=x$}
%          \label{fig:y equals x}
%      \end{subfigure}
%      \hfill
%      \begin{subfigure}[b]{0.08\textwidth}
%          \centering
%   \includegraphics[width=\textwidth]{figures/fig6_vis.pdf}
%          % \caption{$y=x$}
%          \label{fig:y equals x}
%      \end{subfigure}

%         \caption{Three simple graphs}
%         \label{fig:three graphs}
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig6_vis_v2.pdf}
    % \vspace{-3mm}
    \caption{Empirical analysis of the depth bias on the nuScenes dataset if objects are assumed static.}
    \vspace{-4mm}
    \label{fig:analysis}
\end{figure}


\begin{figure*}[htb]
    \centering
    \includegraphics[width=\textwidth]{figures/overview.png}
    % \vspace{-6mm}
    \caption{Pipeline overview.  Given a video sequence, we first extract the features from 2D images and generate the candidate boxes and their motion by a single-frame detector. Then the boxes and motion are progressively refined from the concurrently updated corresponding 3D volume features. A fusion process in the recurrent module finally combines the estimation from each pair of frames. Based on the tightly coupled modeling of object location and motion, the framework can achieve joint 3D detection and tracking during inference. }
    \label{fig:pipeline}
    \vspace{-4mm}
\end{figure*}

% We can observe that the first term in the denominator is relatively small, the second and the third terms are near to 0 and 1 when the ego-motion only contains the translation. Therefore, this result reveals that the depth error is almost equal to the motion error in the $z$ direction. 
% % that the depth gap is near linearly related to the object motion and hence increase when the time interval enlarges.
% In Fig~\ref{fig:analysis}, we display the empirical statistics collected from the object annotations of nuScenes dataset. We first computed the object depth error from assumed static with different time intervals. Similar to the theoretical analysis, the empirical depth error is linearly related to the object velocity and increases as the time interval enlarges. Besides, the right part in Fig.~\ref{fig:analysis} also shows that there are almost 53\% objects would moving across frames, demonstrating the necessity of modeling object motion in the temporal-based framework.
% % distribution of object velocity and the corresponding depth error caused by the static assumption on the nuScenes dataset. We can observe that there is still a large ratio of moving objects and the corresponding depth error would increase as the time interval enlarges.

% \subsection{Ill-Posed Problem in Motion Modeling}
% Besides demonstrating the necessity of modeling the object motion in temporal-based frameworks, we can observe that the 3D object motion estimation is also an ill-posed problem in the monocular setting, especially when we only have two frames, \emph{i.e.}, a single pair of correspondence observation. In that case, the correspondence of two points in the multi-view stereo can be solved by infinite combinations of object motion and location.
% To alleviate this issue, our work leverages more than two frames to constraint the object motion. More details can be referred to Sec~\ref{sec:fusion}.

From Eq~\eqref{eq:bias_depth}, we can observe that the depth bias is linearly correlated with the object motion. In Fig~\ref{fig:analysis}, we also display the empirical statistics of object motion and the corresponding depth bias from the nuScenes dataset. We can observe that the empirical depth error is also linearly correlated to the object velocity and increases as the time interval enlarges. Besides, the right part in Fig.~\ref{fig:analysis} also shows that almost 51\% of objects are moving across frames, demonstrating the necessity of modeling object motion in the temporal-based framework.


\subsection{Ill-Posed Problem in Motion Modeling}
Except for demonstrating the necessity of modeling object motion in temporal-based frameworks, we also want to mention that simultaneously estimating object location and motion is a non-trivial problem, especially in the two-frame case. As shown in Fig~\ref{fig:fig1}, the correspondence of two points from the temporal frames can come from infinite combinations of object location and motion (We provide more examples and the visualization in the supplementary).
% (The object locations can be the points in the ray in each frame, and the object t motion is the vector that connects these two points.)
%. point motion to xxx;
% more frame to constraini
This illustrates that joint estimation of location and motion from only one correspondence is an ill-posed problem. 
To alleviate this issue, we first simplify the object motion as a right-body movement so that multiple correspondences from the points in the object can be used to solve a shared motion.
Furthermore, we also leverage more than two frames to constrain the flexibility of object motion. More details can be referred to Sec~\ref{sec:fusion}.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.15\textwidth]{figures/fig6_vis.pdf}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure}