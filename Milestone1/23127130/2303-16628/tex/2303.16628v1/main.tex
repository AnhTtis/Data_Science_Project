\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{graphicx,mathtools,kantlipsum}
\usepackage{longtable}
\usepackage{tablefootnote}

\usepackage{caption}
\usepackage{subcaption}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{5987} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\setlength{\arraycolsep}{2.5pt}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\newcommand{\jiangmiao}[1]{{\color{red}{(Jiangmiao: #1)}}}
\newcommand{\LQ}[1]{{\color{cyan}{(#1)}}}

\renewcommand{\paragraph}[1]{\noindent\textbf{#1}~~}

\usepackage[misc]{ifsym}
\newcommand\blfootnote[1]{\begingroup\renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{R-DOM: Recurrent Dynamic Objects Modeling  for \\ Monocular 4D Object Detection}
% \title{Recurrent Dynamic Objects Modeling for Monocular 4D Object Detection}
% \title{Monocular 4D Object Detection by Modeling Dynamic Objects in Recurrent}
\title{DORT: Modeling Dynamic Objects in Recurrent for Multi-Camera\\ 3D Object Detection and Tracking}

\author{Qing Lian\textsuperscript{1,2}
~~~~~Tai Wang\textsuperscript{1,3}
~~~~~Dahua Lin\textsuperscript{1,3}
~~~~~Jiangmiao Pang\textsuperscript{1\textrm{\Letter}} \\
\textsuperscript{1}Shanghai AI Laboratory
~~~\textsuperscript{2}The Hong Kong University of Science and Technology \\
\textsuperscript{3}The Chinese University of Hong Kong
 \\
{\tt\small qlianab@connect.ust.hk, \{wt019,dhlin\}@ie.cuhk.edu.hk, pangjiangmiao@gmail.com}}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}

Recent multi-camera 3D object detectors usually leverage temporal information to construct multi-view stereo that alleviates the ill-posed depth estimation.
However, they typically assume all the objects are static and directly aggregate features across frames. 
This work begins with a theoretical and empirical analysis to reveal that ignoring the motion of moving objects can result in serious localization bias.
Therefore, we propose to model Dynamic Objects in RecurrenT (DORT) to tackle this problem. 
In contrast to previous global Bird-Eye-View (BEV) methods, DORT extracts object-wise local volumes for motion estimation that also alleviates the heavy computational burden. 
By iteratively refining the estimated object motion and location, the preceding features can be precisely aggregated to the current frame to mitigate the aforementioned adverse effects. 
The simple framework has two significant appealing properties.
It is flexible and practical that can be plugged into most camera-based 3D object detectors.  
As there are predictions of object motion in the loop, it can easily track objects across frames according to their nearest center distances. 
Without bells and whistles, DORT outperforms all the previous methods on the nuScenes detection and tracking benchmarks with 62.5\% NDS and 57.6\% AMOTA, respectively. The source code is available at \url{https://github.com/SmartBot-PJLab/DORT}. 

\blfootnote{\textrm{\Letter} Corresponding author.}

\end{abstract}



\input{sections/intro}
\input{sections/related}
\input{sections/analysis}
\input{sections/method}
\input{sections/exp}
\input{sections/conclusion}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage



\input{sections/supp}

% \newpage


\end{document}