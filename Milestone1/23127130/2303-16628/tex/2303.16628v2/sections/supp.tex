
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\begin{center}
    \Large
    \textbf{Appendix}
\end{center}}]
\renewcommand\thesection{\Alph{section}}
% \renewcommand\thesubsection{\arabic{subsection}}
\setcounter{section}{0}

% \begin{center}
    
% \begin{table*}[!htb]
% \centering
% \caption{Comparison on the nuScenes test set. Extra data is depth pretraining for the image backbone V2-99~\cite{dd3d}}
% \label{tab:main_test_set}
% \tiny
% \resizebox{\textwidth}{!}{
% % \setlength{\tabcolsep}{1pt}
% \begin{tabular}{l|c|c|cccccccc} 
% % \begin{tabular}{ p{25mm}<{\centering}| p{10mm}<{\centering} | p{12mm}<{\centering}  p{10mm}<{\centering}  p{10mm}<{\centering} p{10mm}<{\centering} p{10mm}<{\centering} p{12mm}<{\centering} | p{10mm}<{\centering}}
% \hline
% Method & Backbone  & Extra Data  & mAP$\uparrow$  &NDS$\uparrow$  & mATE$\downarrow$ & mASE$\downarrow$   &mAOE$\downarrow$   &mAVE$\downarrow$   &mAAE$\downarrow$  \\\hline
% % \midrule
% FCOS3D         & ResNet-101   &  \xmark  & 0.358 & 0.428 & 0.690 & 0.249 & 0.452 & 1.434 & 0.124 \\
% PGD            & ResNet-101 & \xmark & 0.386 & 0.448 & 0.626 & 0.245 & 0.451 & 1.509 & 0.127 \\
% BEVFormer      & ResNet-101 & \xmark &  0.445 & 0.535 & 0.631 & 0.257 & 0.405 & 0.435 & 0.143 \\
% PolarFormer    & ResNet-101 & \xmark &  0.456 & 0.543 & 0.610 & 0.258 & 0.391 & 0.458 & 0.129 \\ 
% DORT (Ours)     & ResNet-101 & \xmark & 0.465 & 0.566  & 0.503 & 0.249 & 0.413 & 0.369 & 0.129 \\ \hline

% DETR3D         & V2-99       & \cmark  & 0.412 & 0.479 & 0.641 & 0.255 & 0.394 & 0.845 & 0.133 \\
% UVTR           & V2-99      & \cmark  & 0.472 & 0.551 & 0.577 & 0.253 & 0.391 & 0.508 & 0.123 \\
% BEVFormer      & V2-99       & \cmark  & 0.481 & 0.569 & 0.582 & 0.256 & 0.375 & 0.378 & 0.126 \\
% BEVDepth       & V2-99      & \cmark   & 0.503 & 0.600 & 0.445 & 0.245 & 0.378 & 0.320 & 0.126 \\
% PolarFormer    & V2-99       & \cmark  & 0.493 & 0.572 & 0.556 & 0.256 & 0.364 & 0.439 & 0.127 \\
% % BEVDet4D       & Swin-B     & \xmark  & 0.451 & 0.569 & 0.511 & \textbf{0.241} & 0.386 & 0.301 & \textbf{0.121} \\
% % PETRv2         & GLOM-like  & \xmark  & 0.512 & 0.592 & 0.547 & 0.242 & 0.360 & 0.367 & 0.126 \\
% BEVDepth       & ConvNeXt-B & \xmark  & 0.520 & 0.609 & 0.445 & 0.243 & 0.352 & 0.347 & 0.127 \\
% DORT (Ours)    &  ConvNext-B & \xmark & 0.495 & 0.586 & 0.485 & 0.249 & 0.413 & 0.336 & 0.129  \\
% % BEVStereo      & V2-99      & 640 $\times$ 1600 & \cmark & \xmark & 0.525 & 0.610 & \textbf{0.431} & 0.246 & 0.358 & 0.357 & 0.138 \\

% \hline
% \end{tabular}}
% \end{table*}
% \end{center}


% The supplementary material is organized as follows:
% \begin{itemize}
%     \item In Sec~\ref{sec:analysis}, we provide the details of how ignoring object motion introduces biased depth.
%     \item In Sec~\ref{sec:track}, we introduce the used tracking algorithm in DORT
%     \item In Sec~\ref{sec:track_exp}, we present the related ablation study of the tracking algorithm.
%     \item In Sec~\ref{sec:refine}, we present the network architecture of RefineNet.
%     \item In Sec~\ref{sec:det_exp}, we display the experimental results of DORT as a plug in module for the BEV-based detector.
% \end{itemize}



\section{Implementation Details}
In the main paper, we have introduced our overall multi-camera 3D object detection and tracking framework and the details of the proposed components. In this supplemental section, we present the details of the other basic modules.
\subsection{Network Architecture}
Our framework is built based on BEVDet and BEVDepth, and we follow them to design the basic modules.

\noindent{\textbf{2D Feature Extraction.}}
Given $N$ multi-view images $I \in \mathcal{R}^{N\times W \times H\times 3}$in each frame, we use a shared 2D backbone to extract the corresponding features. We adopt the standard ResNet-50~\cite{he2016deep} as the backbone and initialize it with ImageNet pre-trained weights. Then we adopt a modified Feature Pyramid Network (FPN)~\cite{yan2018second} to extract the multiple-level features and the output 2D features are downsampled with the ratio of $\frac{1}{16}$ with channel size $256$: $F_{pv} \in \mathcal{R}^{\frac{W}{16}\times \frac{H}{16}\times 256}$.

\noindent{\textbf{View Transformation.}}
Our work is the same as BEVDet and BEVDepth that contains a 2D to 3D view transformation module. Specifically, we first leverage a depth prediction head to predict the depth probability for each pixel. Then we lift the 2D features to a 2.5D frustum space via out-product it with the depth probability. The depth probability range is set as $[0m, 60m]$ with grid size $0.5m$. With the 2.5D frustum features, the 3D features for each local volume are obtained via utilizing the camera intrinsic to project the 3D grid back to the frustum and bi-linear sample the corresponding features. As mentioned in the main paper, we aggregate the 3D volume features along the height dimension and obtain the corresponding object-wise BEV features $F^{obj}_{bev} \in \mathcal{R}^{N\times W^{obj} \times H^{obj} \times 256}$, where $W^{obj}$ and $H^{obj}$ are the object features dimension and set as 28 in the main setting.

\noindent{\textbf{RefineNet.}}
Given the object-wise features extracted based on the proposal 3D box and motion, RefineNet takes several convolutional neural networks to extract the object-wise features and estimate the bounding box and motion residual. Specifically, we first adopt an average pooling layer to aggregate the 3D features along the height dimension and obtain the BEV features. Then we filter each object-wise BEV features with 6 basic 2D residual blocks, where each residual block consists of two 2D convolution layers and a skip connection module as in ResNet. The channel size of the residual blocks in the first three layers is 256 and decreases to 64 in the last three layers. Then we aggregate the features along the spatial dimension via average pooling and take 4 layers MLP network to estimate the bounding box and motion residuals.

\subsection{The Tracking Module}
\label{sec:track_supp}
In this section, we provide the details of the tracking module that omit in the paper.
Since DORT can estimate tightly coupled object location and motion, object tracking can be easily achieved via nearest center distances association~\cite{Bewley2016_sort, yin2021center, pang2021simpletrack}. 
Hence, our tracking module is mainly adapted from the previous distance-based object tracker~\cite{Bewley2016_sort, yin2021center, pang2021simpletrack}.  Specifically, the tracking module contains four parts: Pre-processing, Association, Status Update and Life-cycle Management.

\noindent{\textbf{Pre-processing.}}
Given the detection results, the pre-processing stages mainly focus on filtering false negative objects. In our work, we first adopt Non-maximum Suppression to remove the duplicated bounding boxes with the threshold of 0.1 in terms of 3D IoU. Then we filter out the bounding boxes that the confidence threshold is lower than 0.25.

\noindent{\textbf{Association.}}
This stage associates the detection results in the current frame with tracklets in the past frame. Specifically, we first utilize the estimated object motion (velocity) to warp the detection results back to the past frame and then utilize the L2 distances of object centers to compute the similarity between the detected objects and the tracklets. Then we utilize the linear greedy matching strategy to achieve multi-object matching. 

\noindent{\textbf{Status Update.}}
This stage updates the status of the tracklets. For the tracklets that do not match with any bounding boxes, we replace it object center location with the corresponding detection results. For the unmatched objects, we utilize the estimated object velocity to update its object center location. 

\noindent{\textbf{Life-cycle Management.}}
The life-cycle management module controls the ``birth'' and ``depth'' of the tracklets (\textit{i.e.} birth, depth). Specifically, for the unmatched bounding boxes, they will be initialized as new tracklets. For the unmatched tracklets, we remove them when they are consecutive unmatched more than 2 times.

\section{Ablation Studies}



% of the Tracking Module}
\label{sec:track_exp_supp}
In this section, we provide the additional ablation studies that omit in the main paper.
We will release the code afterward for providing the details of the methods and reproducing the experimental results.
\noindent\textbf{DORT with Different Proposal Detector.}
We first show that DORT is agnostic with different proposal detectors (\textit{e.g.} PGD~\cite{wang2021PGD}, BEVDepth~\cite{li2022bevdepth}). In Table~\ref{tab:dort_proposal_det}, we display the experimental results of DORT with using PGD and BEVDepth as the proposal detectors. 
We can observe that the DORT is insensitive to the proposal detector and can consistently improve BEVDepth. We
Benefiting from the low computation overhead of BEVDepth in the perspective part and the designed local volume, DORT also can achieve a more lightweight pipeline for dynamic object modeling.

\begin{table}[htb]
    \centering
    \caption{Experimental results on the nuScenes validation set. 1 past frame is adopted in the temporal modeling. $^*$ denotes the BEV FLOPS from the proposal detector.}
    \vspace{2mm}
    \tabcolsep2pt
    \resizebox{0.95\linewidth}{!}{\begin{tabular}{c|cc|cc} \hline
\multirow{2}{*}{Method} & \multirow{2}{*}{mAP} & \multirow{2}{*}{NDS} & \multicolumn{2}{c}{Flops} \\
                        &                      &                      & PV          & BEV         \\ \hline
        BEVDepth &35.1 & 47.5 & 120.4  & 94.5 \\
        DORT with PGD & 37.9 & 52.1 &  238.2 & 40.2\\
        DORT with BEVDepth & \textbf{38.1} & \textbf{52.1} & 120.4 & 74.4$^*$+40.2\\ \hline
    \end{tabular}}
    \label{tab:dort_proposal_det}
\end{table}

\noindent\textbf{Tracking with Semantic Embedding or Geometry Distance.}
In this work, DORT achieves 3D object tracking via the nearest centerness association. To have a more comprehensive comparison of the tracking pipeline designed, we further provide the comparison of DORT with using semantic embedding to associate objects. Specifically, we follow previous methods~\cite{Hu2021QD3DT} and adopt the widely-used quasi-dense similarity learning~\cite{qdtrack_conf} to learn the tracking embedding. We extract two kinds of embedding features, one is from the perspective-view (PV) and another is from the bird-eye-view (BEV).  In Table~\ref{tab:track_abla}, we display the tracking results on the nuScenes tracking set. We can observe that DORT with geometry distance association can outperform the embedding-based methods by a large margin. Furthermore, it is also much simpler and more efficient that does not need to maintain an extra object embedding. Besides, the PV embedding is worse than the BEV-based embedding, which may be due to the view change in different cameras. 
\label{sec:track_exp}
\begin{table}[htb]
    \centering
    \caption{Experimental results on the nuScenes validation set. 1 past frame is adopted in the temporal modeling.}
    \vspace{2mm}
    \tabcolsep1pt
   \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c|ccc} \hline
        Method & AMOTA$\uparrow$ & AMOTP$\downarrow$ & MOTAR$\uparrow$ \\ \hline
        PV-Embedding & 36.8 & 1.412& 44.2 \\
        BEV-Embedding &40.1 &1.356 &46.7 \\
        DORT (Geometry Distance) &\textbf{42.4} &\textbf{1.264} &\textbf{49.2} \\ \hline
    \end{tabular}}
    \label{tab:track_abla}
\end{table}


\section{Theoretical Analysis of Ignoring Object Motion}
\label{sec:analysis}
In the main paper, we have shown that when ignoring object motion, the temporal correspondence would derive a biased depth. In this supplementary, we provide the full details of how ignoring object motion introduces a biased depth.
We denote the camera intrinsic as $K$ and the ego-motion from frame $t_0$ to frame $t_1$ as $T_{t_0\rightarrow t_1}^{ego}$:
\begin{align}
    K = \left[\begin{array}{cccc}
        f & 0 & c_u  \\
        0 & f & c_v \\
        0 & 0 & 1  \\
    \end{array}\right], 
    T_{t_0\rightarrow t_1}^{ego} = 
    \left[
    \begin{array}{cccc}
    1 & 0 & 0 & x^{ego} \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & z^{ego} \\
    \end{array}
    \right].
\end{align}
Here, $f$ is the camera's focal length, and $(c_u, c_v)$ is the camera center coordinates in the image. For simplicity, we assume the ego-motion only contains the translation $(x^{ego}, 0, z^{ego})$ on the horizontal plane. The analysis also can be easily extended to a more complicated case that the motion contains rotation.
% We provide a more complicated case that the motion contains rotation in supplementary.
% The analysis also can be easily extended to a more complicated case that the motion contains rotation.
Given the multiple-view images, temporal-based methods can utilize photometric or featuremetric similarity to find the correspondence of pixel $p_{t_0} = (u_{t_0}, v_{t_0})$ in the past frame $t_0$ and the pixel $p_{t_1} = (u_{t_1}, v_{t_1})$ in the current frame $t_1$. 

When we ignore the object motion, the depth $z_{t_1}$ of pixel $p_{t_1}$ can be recovered as:
\begin{align}
    \label{eq:nomotion_supp}
    & T_{t_0\rightarrow t_1}^{ego}\cdot \pi(p_{t_0}, K) = \pi(p_{t_1}, K), \nonumber \\
    & z_{t_1}\frac{u_{t_1} + c_u}{f} - x^{ego} = \frac{u_{t_0} + c_u}{f}(z_{t_1} - z^{ego}), \nonumber \\
    & z_{t_1} = \frac{z^{ego}(u_{t_0} - c_u) - fx^{ego}}{u_{t_0} - u_{t_1}}, 
\end{align}
where $\pi$ denotes the projection from 2D image coordinate to 3D camera coordinate.

But as we showed in the main paper, the moving objects occupy large ratios in the driving scenarios. For example, when the object contains the translation $(x^{obj}, 0, z^{obj})$ in the horizontal plane, the object's motion can be represented as 
\begin{align}
    T^{obj}_{i \rightarrow j} = \left[
        \begin{array}{cccc}
            1 & 0 & 0 & x^{obj} \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & z^{obj} \\
        \end{array}
    \right].
\end{align}
With the object motion, the depth $z_{t_1}$ of pixel $p_{t_1}$ is recovered as:
\begin{align}
    \label{eq:motion_supp}
    & T^{obj}_{t_0 \rightarrow t_1}T_{t_0\rightarrow t_1}^{ego} \cdot \pi(p_{t_0}, K) = \pi(p_{t_1}, K), \nonumber \\
    &  z_{t_1}\frac{u_{t_1} + c_u}{f} - x^{ego} - x^{obj} = \frac{u_{t_0} + c_u}{f}(z_{t_1} - z^{ego} - z^{obj}) \nonumber \\
    & \hat{z}_{t_1} = \frac{(z^{ego} + z^{obj})(u_{t_0} - c_u) - f(x^{ego} + x^{obj})}{u_{t_0} - u_{t_1}}.
\end{align} 

From Eq~\eqref{eq:nomotion_supp} and Eq~\eqref{eq:motion_supp}, we can obtain the depth gap for the temporal correspondence with and without considering object motion:
\begin{align}
    \Delta z = \frac{z^{obj} (u_{t_0} - c_u) - fx^{ego}}{u_{t_0} - u_{t_1}}.
    \label{eq:bias_depth_supp}
\end{align}


In Figure~\ref{fig:supp_motion}, we also provide a toy example to illustrate that
one temporal correspondence can come from multiple combinations of object depth and motion (\textit{i.e.} inaccurate depth with zero motion and accurate depth and GT motion).
This means that if we inaccurately assume that objects are static across frames, the temporal correspondence would derive a misleading depth.
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/supp_fig1.pdf}
    \caption{Different object motion can make the same temporal correspondence derive different depth.}
    \label{fig:supp_motion}
\end{figure}
\subsection{Ill-posed Problem of Simultaneously Estimating 3D Location and Motion}

% In Figure~\ref{fig:}
Although object motion plays a critical role in temporal correspondence, however, it is non-trivial to estimate it from the monocular images.
As shown in Figure~\ref{fig:supp_motion}, the one correspondence can come from infinite combinations of location and motion (the location can be the point in the ray $
 \overrightarrow{O_{t_0}P_{t_0}}$ and $\overrightarrow{O_{t_1}P_{t_1}}$, and the motion can be the line that connects the points.)
Hence, it is an ill-posed problem that simultaneously estimates the 3D location and motion from the monocular images. To alleviate this issue, we leverage the rigid-body assumption for the objects in the driving scenarios and elaborate more temporal frames with constant velocity regularization to further constrain the motion. 


