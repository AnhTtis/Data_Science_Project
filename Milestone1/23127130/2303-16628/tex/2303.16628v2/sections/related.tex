\section{Related work}
% \jiangmiao{1. Please use noindent+emph for sub-paragraph. 2. ``based" should add a ``-" in before. If you capital all words in the note, they should be ``**-Based" and ``Multi-View". This should be always consistent across the whole paper. I renewcommand the paragraph in the main.tex so that the layout can be more beautiful.}

\paragraph{Monocular 3D Object Detection}
% Benefit from the cheap and easy-to-maintain properties, camera-based sensing solution draws increasing attention from both academia and industry.
Monocular-based 3D object detection was first approached from single-frame based methods and evolved into multi-frame based to alleviate the ill-posed depth estimation problem.

\noindent{\emph{(a) Methods with A Single Frame}}\quad
The single-frame based methods~\cite{zhou2019objects, MonoDLE, MonoFlex, MonoEF, brazil2019m3d} first extend 2D object detectors and insert several parallel regression heads (\textit{i.e.} 3D size, depth, and orientation) to predict 3D bounding boxes.
To alleviate the ill-posed depth recovery problem, several methods improve the model from the perspectives of loss module~\cite{MonoDLE}, network architecture~\cite{Brazil2020kinematic, wang2021fcos3d}, regression objective~\cite{MonoFlex, wang2021PGD}, etc.
Besides directly regressing object depth, later approaches~\cite{RTM3D, autoshape_liu, lian2022monojsg} further design 2D-3D geometry constraints to better extract visual cues for object depth estimation.
To align the detection features with output space, another line of methods~\cite{roddick2018orthographic} design several transformation modules to lift 2D inputs into 3D space. 
Pseudo-lidar based methods~\cite{pseudo_lidar, you2019pseudo, wang2019pseudo, wang2019pseudo} first predict the per-pixel depth and convert the raw pixel into point cloud for 3D detection.
BEV-based methods~\cite{roddick2018orthographic, philion2020lift, huang2021bevdet, li2022bevdepth} propose orthographic feature transformation (OFT) to transform the image features into a 3D voxel and then adopt a LiDAR-based head to localize objects.
Later work improves the OFT from the perspectives of explicit depth distribution modeling~\cite{philion2020lift, huang2021bevdet, li2022bevdepth}, incorporating deformable attention module~\cite{li2022bevformer} or designing 3D-based position encoding for attention~\cite{detr3d, liu2022petr}.

\noindent{\emph{(b) Methods with Multiple Frames}}\quad
Although many techniques are designed in single-frame based methods, they still suffer from ill-posed depth recovery problems, leading to unsatisfactory performance for deployment.
% In driving scenario, 
To augment the single-view observation, recent work~\cite{li2020jst, Brazil2020kinematic, wang2022dfm, huang2021bevdet4d, li2022bevformer, li2022bevstereo} leverages previous frames as additional observations for geometry modeling and features augmentation.
Kinematic3D~\cite{Brazil2020kinematic} leverages 3D Kalman Filter to associate objects across frames and refines the estimated 3D box. Later studies~\cite{wang2022dfm, huang2021bevdet4d, li2022bevformer} construct cross-frame cost-volumes as another visual cue for 3D detection. 
The cost volumes is based on the multi-view stereo, which assumes objects are static across frames. However, this assumption does not align with the driving scenario, where the objects can move. 
More critically, our analysis demonstrates that the inaccurate object motion would introduce misleading visual cues for object localization. 

\paragraph{Monocular 3D Object Tracking}
% introduce tracking
% previous method -> using features association and tracking by detection
% introduce the tracking for detection and the associated work that centertrack
% list our method that joint detection and tracking. 
3D object tracking associates the objects across frames and generates a set of trajectories for motion prediction and planning. Traditional methods adopt a tracking-by-detection paradigm that first detects objects in each frame and then associates them by the appearance features~\cite{li2020jst} or objects' displacement with Kalman filter~\cite{Bewley2016_sort,yin2021center, Chen2022PolarDETR,Shi2022SRCN3D,Yang2022QualityTrack,fischer2022ccdt}. Besides the above paradigm, several methods~\cite{Hu2021QD3DT, Li_2022_time3d} design a two-stage paradigm that first associates objects based on appearance features and then utilizes the temporal motion to improve the object detection performance. 
In this work, we utilize temporal cost volumes to bridge the spatial location and temporal motion and derive a recurrent paradigm that iteratively updates them to obtain tightly coupled results for joint 3D detection and tracking.
%However, Quasi-track still requires a feature association modeul to provide  to provide a tracking  on the objec

% associating the objects across different
\paragraph{Multi-View 3D Perception}
Leveraging multi-view images to recover 3D information is a fundamental topic, such as structure from motion~\cite{mvdepthnet}, multi-view stereo~\cite{yao2018mvsnet},  simultaneous localization and mapping~\cite{slam_review}, etc. 
One line of methods develop neural-network-based cost volumes~\cite{yao2018mvsnet, yao2019recurrent,sun2017pwc,zachary2020raft,zachary2021raft3d, sun2020disprcnn} to construct cross-frame visual cues for 3D perception. 
% Specifically, MVS-Net~\cite{yao2018mvsnet, yao2019recurrent,sun2017pwc} and its follow-ups leverage the neural network to model the cost volume features and predict the corresponding 3D attributes.
% RAFT-based methods~\cite{zachary2020raft,zachary2021raft3d} develop a recurrent paradigm that iteratively updates the predicted depth by querying the corresponding cost volume features. 
Another line of methods~\cite{Wang2019NOCS, tang2018banet,li2019multisensor} constructs geometry constraints and leverage optimization techniques to obtain a tight-coupled 3D structure. 
However, most of the work assumes the scene and objects are static, making them fail to handle the moving objects in driving scenarios.
% Inspired from previous methods~\cite{yang2009cubeslam, li2020jst, sun2020disprcnn, lian2022monojsg}, our work designs an object-wise representation and derive a recurrent pipeline to progressively refine the features for obtaining accurate location and motion results
% alleivate the unsolved temporal modeling problems for moving objects. 

%To handle the moving objects, 

% To alleviate this issue, our work first s
