\section{Introduction v5}

%\bryan{Our everyday audiovisual world is composed of many different visible objects making sounds, often with multiple sources sounding simultaneously. For example, consider the video of the guitar and cello musicians playing together in Figure 1. The depicted cello and guitar have distinct timbres, and the musicians play non-unison, but complementary melodies. We seek to allow a user to select a sounding object given a natural language query, such as ``person playing a guitar'', and for a system to automatically separate the sound source from the input audio waveform and to localize the sound source in the input video. Achieving this goal would enable important downstream applications including holistic video understanding [], embodied AI [], and bidirectional audio-to-video retrieval [].}

%Moreover, the visual appearance and motion of the objects give clues about the different sources' sounds. An important problem (known as {\em sound source separation}) is to separate the different sound sources given a sound mixture.

%Traditionally, this task is formulated without knowledge of the sound sources' names []. Natural language provides an intuitive and flexible interface for selecting the different sound sources. For example, an ideal system could take the natural language query ``person playing a guitar'' and return the separated guitar sound.

\ks{Mention the weakness of prior work here, ie supervision. "Bad guy" first, then "hero"!}
Recent vision-and-language pretraining approaches [] have focused on learning strong transferable visual and text representations from web-scale image and text collections, that can generalize to downstream tasks with little to no finetuning on the target datasets. Their success has demonstrated the much higher quality \ks{strength?} of supervision present in noisy large-scale corpuses sourced from the web as compared to smaller and human-annotated datasets. Since natural language has the generality to express a large and varied range of visual concepts, it provides an intuitive and flexible interface for separating sounds in a mixed audio. To this end, we aim to learn to separate audio source components in videos based on natural language queries without any supervision. This is motivated by our observation that the co-occurrence of a sounding object and its depiction in a video provides useful cues for helping a model learn to identify and separate the sound from the background. Consequently, we seek to learn these co-occurrences from unlabeled audio-video pairs.

By itself, separating audio sources in videos or based on natural language queries is already an inherently difficult problem since the input audio is usually a mixture of sounds from different objects and there is a lack of ground-truth source annotations. This is greatly compounded by our key challenge of how to jointly localize sounding objects in both a video and an audio waveform as well as associate both with natural language in a completely self-supervised way. To ease the difficulty, \ks{Start 2nd par. with this:} state-of-the-art audio-visual separation approaches often rely on training object detectors [] with strong supervision using video labels and learning to associate separated audio sources with bounding boxes that are detected for a set of fixed-vocabulary categories (Figure~\ref{fig:latent_motiv} top). However, this may not always be feasible due to insufficient bounding box annotations for objects in the long-tailed distribution. More importantly, this severely curtails their flexibility to scale up training to a larger and more diverse set of visual concepts.

With this in mind, our objective is to address the above-mentioned limitations by learning to separate audio sources based on any combination \ks{what does 'any combination' mean?} of video and natural language queries without strong supervision, illustrated in Figure~\ref{fig:latent_motiv} (bottom). We believe that alleviating the need for human-annotated captions and bounding box supervision during training will open up the possibility for learning to perform multimodal audio source separation from the plethora of unlabeled videos online, allowing approaches to generalize well to a large variety of audio sources and visual concepts.
\uri{Maybe you can talk about how self-supervised approaches have lately surpassed models trained on strongly labeled datasets due to the differences in the amount of training data (e.g., cite CLIP and Whisper).}
Similar to how humans often leverage object sounds to understand their states and interactions with other objects, multimodal audio source separation is integral to improving machine perception of our environment. It has important implications in downstream applications including holistic video understanding [], embodied AI [] and bidirectional audio-to-video retrieval [].

We propose the first completely self-supervised approach for learning to separate audio source components with text and/or video queries from unlabeled videos without object labels or text captions at all \ks{well, technically CLIP is trained on text captions, maybe just say 'unlabeled videos'}. Our proposed \modelname approach uses a Multiple Instance Learning (MIL) formulation to learn to perform source separation at the \emph{region} level, without making assumptions on the presence of objects in the videos, by leveraging the recent success of vision-and-language foundation models [] (\eg CLIP and ALIGN) to overcome the above-mentioned challenges. The absence of text captions or object labels in these videos raises the question of how we can learn to separate audio components with natural language queries despite not training with them. To  this end, we adapt pretrained vision-and-language foundation models to extract latent concepts \ks{define this term} and leverage them for pseudo-target supervision in the form of two novel loss functions. \ks{the following is hard to understand, maybe just cut or replace with high level intuition} These loss formulations stem from our key intuition that a model has to learn a strong alignment between the visual, language and audio representations such that text and videos can be used interchangeably or in combination to perform source separation. We impose cyclic consistency losses where the spatiotemporal region features in the video are first used to separate the corresponding audio source before maximizing the similarity between the representation of the final predicted audio source and the latent concept in two different ways. Intuitively, this allows the model to identify regions with the most relevant visual concepts for source separation, facilitating its ability to learn from multi-source videos without prior knowledge of possible objects or their positions. \ks{what is the key novel idea here? just replace object detectors with CLIP? Or something more?}

Despite not relying on trained object detectors, our proposed approach outperforms strongly-supervised state-of-the art-approaches by a significant margin across multiple datasets. More significantly, we demonstrate that our trained model can also separate sound sources based on natural language queries despite not having access to ground-truth object labels or using text representations during training. In summary, our contributions are listed as follows:
\begin{enumerate}
    \item We propose the first fully self-supervised approach to learn to separate audio source components with video and/or natural language queries from unlabeled videos.
    \item We introduce two novel loss functions that leverage latent concepts to learn a strong alignment between the audio, text and video representations. 
    \item We demonstrate that our approach can generalize to natural language queries despite not training with human-annotated captions.\ks{our approach uses NL queries as input? so what does it mean to 'generalize' to them? Maybe say 'use'?} 
    \item We conduct extensive source sound separation experiments across 4 different datasets including SOLOS, MUSIC, UMRP and AudioSet, where we outperform state-of-the-art strongly-supervised approaches by a significant margin.\ks{this seems kind of too good to be true, I would at least mention the power of V&L pretraining for this, and how transferring that power to this problem gives us nice results, rather than keep saying we use no supervision at all.}
\end{enumerate}
