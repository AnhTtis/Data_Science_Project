\section{Video-Audio Separation through Text}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\textwidth]{Figures/approach_overview_v7.pdf}
    \caption{\textbf{Training overview of our proposed \modelabb approach. %using three objective functions.
    } We introduce tri-modal (Section~\ref{sec:alignment_losses}) and audio-language (Section~\ref{sec:alignment_losses}) consistency objectives to learn the latent alignment between the audio, video and language modalities. Furthermore, we adopt a prior mask prediction loss~\cite{zhao2018sound} (Section~\ref{sec:model_arch}) to guide the training of our mask prediction decoder. }
    \label{fig:model_overview}
    \vspace{-10pt}
\end{figure}

\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.43\textwidth}
         \raisebox{5mm}{
         \centering
         \includegraphics[width=\textwidth]{Figures/consistency_loss_v6.pdf}}
         \caption{Audio-language consistency loss.}
         \label{fig:cons_loss}
     \end{subfigure}
     \hspace{3mm}
     \begin{subfigure}[b]{0.51\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/kl_loss_v6.pdf}
         \caption{Tri-modal consistency loss.}
         \label{fig:kl_loss}
     \end{subfigure}
     \vspace{-5pt}
        \caption{\textbf{Audio-language and tri-modal consistency alignment objectives.} (a) The audio-language objective aims to maximize the semantic similarity between the predicted audio and its latent caption. (b) In the tri-modal consistency objective, we compute normalized probability distributions over the spatiotemporal regions based on cosine similarity scores (colored frame patches) with respect to the predicted audio and latent captions. $K$ and $Q$ denote the key and query terminology that is used in self-attention. Intuitively, this objective encourages agreement between where the latent caption and predicted audio is occuring in the video.}
        %\caption{We introduce the audio-language and tri-modal consistency alignment objectives to facilitate our model in learning a \emph{transitive alignment} between the audio and language modalities by using videos as an intermediary modality. In the tri-modal consistency objective, we compute normalized probability distributions over the spatiotemporal regions (colored frame patches) with respect to the predicted audio and latent captions.}
        \label{fig:alignment_objs}
        \vspace{-10pt}
\end{figure*}

Given a set of unlabeled videos and their audio tracks, we strive to learn a self-supervised model that separates an audio source given a natural language query and, optionally, an accompanying video. This goal requires a model to learn a strong alignment between the audio, vision, and language modalities. To this end, we propose a novel \modelname (\modelabb) approach that learns an effective representation for audio-visual source separation, as well as a strong alignment between the audio and language modalities for audio-text source separation (see Figure~\ref{fig:model_overview} for an overview). To learn this representation, we exploit the strong joint visual-semantic alignment embodied in pretrained vision-language foundation models by encouraging our model to learn a direct projection function for audio inputs into their joint embedding space (Section~\ref{sec:alignment_losses}). Our key insight is to use videos as an intermediary modality to learn a \emph{transitive} alignment between the audio and language modalities. Since the visual and language modalities are already aligned, learning a strong correspondence between audio and vision should also provide an alignment between audio and language through videos via transitivity. %This concept is integral to learning to reason across all three modalities from unlabeled videos. %The idea behind using videos as an intermediary modality is that the visual information in the video can help identify when different sources of audio are active.

Despite the intuitiveness of this idea, our experiments will show that using the visual representations of existing audio-visual foundation models does not help them learn the above-mentioned transitive alignment. To address this limitation, we introduce our novel tri-modal and audio-language consistency  alignment objectives (Figure~\ref{fig:model_overview}). The tri-modal objective encourages the model to maximize the semantic consistency between all three modalities by using videos as the intermediary modality and the audio-language objective helps to further improve the transitive alignment between audio and natural language. We leverage vision-and-language foundation models to infer latent captions that correspond to the audio components without text annotations, thus providing pseudo-language supervision (Section~\ref{sec:alignment_losses}). In addition, we adopt the ``mix-and-separate'' strategy \cite{zhao2018sound} to train our audio separation model, where the audio from multiple videos are mixed and then separated using only the video's visual information. For an input audio waveform $A$, we apply a Short-Time Fourier Transform (STFT) to obtain its magnitude spectrogram $A^S$ and phase $A^{\text{phase}}$.
Our learning objective only makes use of $A^S$, which encodes how the magnitude of the audio frequencies changes over time.
We reconstruct the predicted waveform by applying the inverse STFT on the predicted spectrogram using the original phase $A^{\text{phase}}$. We describe our \modelabb audio separation model $\mathcal{M}$ in Section~\ref{sec:model_arch}.

%\subsection{Tri-modal alignment}\label{sec:alignment_losses}
\subsection{Alignment between audio, language and video}\label{sec:alignment_losses}
A standard method to learn a strong alignment between the audio, language and vision modalities is to maximize the pairwise similarities of their input representations using a contrastive learning formulation \cite{akbari2021vatt,miech2019howto100m}. However, learning this alignment necessitates the presence of labels that indicate ground-truth correspondences between these modalities. In our self-supervised setting, the unlabeled videos only provide noisy labels of audio-visual pairings but do not contain text labels. To circumvent this limitation, we propose to extract \emph{latent captions}, which are \emph{pseudo-words} that encode the visual concepts in videos with language semantics. Prior work \cite{cohen2022my,gal2022image} has demonstrated that the rich visual-semantic alignment in vision-language foundation models can be adapted to extract latent word representations that convey the semantics of their conditioning images. Inspired by this insight, we introduce the novel idea of using latent captions to identify sound-emitting object candidates in unlabeled videos for training, thereby allowing us to train without prior knowledge of existing objects in the videos.

\noindent\textbf{Latent captions.} We perform ``textual inversion" \cite{gal2022textual,gal2022image,cohen2022my} where we adapt the CLIP language encoder to express the visual concepts of a video in the form of an encoded learnable parameter.  Instead of introducing new concepts into the vocabulary of these models which require their parameters to be updated, we search for their latent language representations through a visual reconstruction objective. This objective is based on the key insight that the final learnt latent captions should be expressive enough to convey the visual semantics of the video. While it is also possible to use the visual frame representations as latent captions, we reason that learning them as outputs of the language encoder will help a model generalize better to language queries during inference. For a given video $V$, we encode its center frame $V_{center}$ as: $f^V_{center} = g^V(V_{center})$, where $g^V$ is the embedding function of the CLIP visual encoder. Then, we replace the token embeddings used for language queries with a learnable parameter $p$ and encode it as: $g^L(p)$, where $g^L$ is the embedding function of the CLIP language  encoder. We optimize the weights of $p$ by maximizing the cosine similarity between $f^V_{center}$ and $g^L(p)$:
\begin{equation} \label{eq:latent_caption_opt}
    p^* \in \argmax_{p} sim\left(f^V_{center}, g^L(p)\right)
    \vspace{-5pt}
\end{equation}
where $sim(x,y)=x^T y/(\|x\| \|y\|)$ and $||.||$ denotes the $L_2$ norm operator. Let $C^*=g^L(p^*)$ be the latent caption. We illustrate this operation in the supplemental.
In theory, $C^*$ is analogous to a representation of a description of the frame and may replace ground-truth annotations effectively.

Our audio-language and tri-modal consistency alignment objectives are based on our reasoning that well-separated audio sources should be semantically consistent with the visual concepts that guide their separation as well as their text labels. Given an input video $V$ of $T$ frames $V = \{V_1, \cdot \cdot \cdot V_T \}$ and an audio spectrogram $A^S \in \mathbb{R}^{F \text{x} N}$ with $F$ frequency bins and $N$ STFT time frames, our model predicts a separated audio spectrogram $\hat{A}^S \in \mathbb{R}^{F \text{x} N}$. We extract a latent representation for the predicted spectrogram: $\hat{f}^A = \mathcal{M}_\theta(\hat{A}^S) \in \mathbb{R}^{D}$ where $\mathcal{M}$ denotes our audio separation model. For each video, we use its encoded predicted spectrogram and latent caption to provide pseudo-language supervision in our alignment objectives detailed below.

\smallskip
\noindent\textbf{Audio-language consistency loss.} To encourage our audio encoder to learn audio representations that are effective for separating sound components when conditioned on either text or video queries, we aim to maximize the pairwise similarities between all three modalities. The key insight is that the audio sources that are well-separated by the visual concepts in a video should have a strong semantic relevance to its latent caption which express the same concepts in natural language (Figure~\ref{fig:cons_loss}). Theoretically, this is similar to the self-supervised multimodal learning objective of maximizing the similarity between the representations of an image and its corresponding caption as well as the dissimilarity of non-corresponding captions. In lieu of ground-truth object labels, we can maximize the alignment between the predicted audio representations and the latent captions over the entire vocabulary of captions $\mathcal{X}$:
\begin{equation}
    \mathcal{L}_{\text{Audio-language}} =
-\log \left(\frac{\exp(\hat{f}^A \cdot C^*/\tau)}{\sum_{x\in\mathcal{X}} \exp(\hat{f}^A  \cdot x / \tau)} \right)
\end{equation}
where $\tau$ is the temperature. 

However, the problem of false negatives has been well-documented in image-text datasets, where captions for some images may be relevant to other images but are treated erroneously as negatives. Since we are training on unlabeled videos, we account for the high likelihood that some videos may contain similar sounding objects by using a lower weight for this objective. Intuitively, this weighting helps to alleviate the effect of false negatives and prevent it from introducing a lot of noise during training.

\noindent\textbf{Tri-modal consistency loss.} While the audio-language consistency objective facilitates improving the alignment between audio sources and their corresponding latent captions, it does not provide a straightforward solution to disregard false negatives in its contrastive formulation. To address this issue, we further introduce a softer tri-modal alignment constraint which exploits the implicit localization capability of vision-language foundation models for supervision without requiring any negative samples. Specifically, we propose to use the intermediary visual modality to encourage well-separated audio components to be grounded correctly in the relevant spatiotemporal regions of a video. Prior work \cite{zhou2021denseclip,rao2022denseclip} has demonstrated that the CLIP model can be engineered to extract a segmentation map based on natural language queries. Inspired by this finding, we use the latent captions to provide pseudo-target attention maps.

Our intuition is that enforcing a soft constraint on the predicted spectrograms such that they can be mapped to similar relevant regions as determined by the latent captions will encourage the model to implicitly learn the transitive alignment between the audio and language modalities.
Let $P_{Att}(A, b) = \sigma\left(A \cdot b\right)$ to
be the attention operation where $\sigma(z)_i = z_i/\sum_j z_j$ is the softmax function. For a given video $V$, we extract a set of spatiotemporal region representations by encoding each frame separately. Specifically, we encode the $t$-th frame as: $f^V_{t} = g_\theta^V(V_t)$, where $f^V_t \in \mathbb{R}^{HW \text{x} D}$, where $H$ and $W$ are the downsampled height and width of the input video. Then, we compute a probability distribution of similarity scores over the set of spatial region features with respect to its latent caption $C^*$ for the $t$-th frame: $P_{VC}^t = P_{Att}(f^V_t, C^*) \in \mathbb{R}^{HW\times 1}$.
Next, we compute a similar distribution over the region features of the $t$-th frame with respect to the encoded audio representation of the masked spectrogram: $P_{VA}^t = P_{Att}(f^V_t, \hat{f}^A) \in \mathbb{R}^{HW\times 1}$. We encourage the predicted audio representation to be similar to that of the latent caption by minimizing the Kullback-Leibler (KL) divergence between both attention distributions over the set of all time steps in a video $\mathcal{T}$:
\begin{equation}
    \mathcal{L}_{\text{Trimodal}} = \mathbb{E}_{t\sim\mathcal{T}}\left[\sum_{x=1}^{HW} P_{VC}^t(x)\log\frac{P_{VC}^t(x)}{P_{VA}^t(x)}\right],
\end{equation}
where $P_{VA}^t(x)$ denotes the audio-video attention distribution over the $t$-frame of the $x$-th spatial region.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/forward_model_figure_v12.pdf}
    \caption{\textbf{Spectrogram mask prediction.} During inference, our model is able to separate audio sources using video or language queries despite training without ground-truth text annotations. We note that we only use unlabeled videos during training. We illustrate the audio-visual separation mode $\mathcal{M}_{AV}$ in the supplemental.}
    \label{fig:forward_pass}
    \vspace{-5pt}
\end{figure}

\begin{table*}[t]
\centering
\begin{tabular}{|l|c|c|ccc|ccc|}
\hline
& Object   & \# Region &   &  Solos  &   &    &  Duets  &  \\
Method   &  detectors & proposals & NSDR $\uparrow$  & SIR $\uparrow$ & SAR $\uparrow$ &  NSDR $\uparrow$ & SIR $\uparrow$ & SAR $\uparrow$\\
\hline
Co-Separation \cite{gao2019co} & Yes & 2 & 7.38 & 13.70 & 10.82 & 7.42 & 13.81 & 10.60 \\
AVSGS \cite{chatterjee2021visual} & Yes & 42 & \textbf{9.04} & \textbf{14.45} & \textbf{12.24} & \textbf{10.25} & \textbf{15.60} & \textbf{12.82} \\
\hline
Sound-of-Pixels \cite{zhao2018sound} & No & None & 7.30 & 11.90 & 11.90 & 6.05 & 9.81 & 10.61\\
AV-Mix-and-Separate~\cite{gao2019co} & No & None & 3.16 & 6.74 & 8.89 & 3.23 & 7.01 & 9.14\\
NMF-MFCC \cite{spiertz2009source} & No & None & 0.92 & 5.68 & 6.84 & 0.92 & 5.68 & 6.84\\
\modelabb (Ours) & No & None & \textbf{7.98} & \textbf{13.92} & \textbf{12.35} & \textbf{8.08} & \textbf{13.97} & \textbf{11.33} \\
\hline 
\end{tabular}
\caption{\textbf{Audio-visual source separation on MUSIC.} We report results on videos that contain one instrument (solos) and two instruments (duets). Despite not training with object bounding boxes, \modelabb performs competitvely with state-of-the-art detection-based approaches.}
\label{tab:music_eval}
\vspace{-10pt}
\end{table*} 

\subsection{Audio Separation Model}\label{sec:model_arch}
We introduce two audio separation models $\mathcal{M}_{AL}$ and $\mathcal{M}_{AV}$ which share the same audio model parameters for separating audio sources. $\mathcal{M}_{AL}$ (Figure~\ref{fig:forward_pass}) infers a spectrogram mask based on a language query while $\mathcal{M}_{AV}$ (shown in supplemental) conditions its prediction on the input video. 
%Our \modelname (\modelabb) approach comprises an audio model and visual-language encoders. 
We adopt the U-Net \cite{ronneberger2015u} model 
 as the audio component in $\mathcal{M}_{AL}$ and $\mathcal{M}_{AV}$, which has been demonstrated by prior work \cite{zhao2018sound, gao2019co} to be effective at audio source separation, and the CLIP visual and language encoders to exploit its learnt visual-semantic alignment. The U-Net model comprises an encoder $\mathcal{E}$ and a decoder $\mathcal{D}$. Given an input audio spectrogram $A^S$, the encoder $\mathcal{E}$ applies a series of downsampling convolutional layers to output a set of bottleneck features: $f^A = \mathcal{E}(A^S), f^A \in \mathbb{R}^{H^A \times W^A \times D}$, where $H^A$ and $W^A$ denote the height and width of the downsampled spectrogram, respectively. %We enforce that the U-Net encoder learns a mapping of its audio representations directly into CLIP visual-semantic embedding space (Figure~\ref{fig:forward_pass}) so that it will be able to use both visual and language queries for audio separation.
%Our model (Figure~\ref{fig:forward_pass}) leverages the audio encoder and a CLIP encoding of the visual and language inputs for audio separation. 

Given a natural language query $L \in \mathbb{R}^{N_w}$ where $N_w$ is the number of words, we extract its representation $f^L \in \mathbb{R}^{1 \text{x} D}$ using the text encoder: $f^L = g^L(L)$. We stress that language queries with ground-truth object labels are only used during inference. Then, we tile the language representation by the factor $H^A W^A$ and concatenate with the audio bottleneck representations along the channel dimension: $f^{AL} = concat(f^A, tile(f^L))$, where $f^{AL}$ has the dimensions $\mathbb{R}^{H^A \times W^A \times 2D}$. We pass the concatenated representations into the decoder $\mathcal{D}$ consisting of a series of upsampling convolutional layers to generate a real-valued ratio mask: $\hat{M} = \mathcal{D}(f^{AL}) \in \mathbb{R}^{F \times N}$. To predict the separated audio source, each element of the mask is multiplied with the corresponding location in the input spectrogram: $\hat{A}^S = \hat{M} \odot A^S$, where $\odot$ denotes the Hadamard product.  

%Given an input video $V$, we use the CLIP visual encoder to extract a set of language-grounded spatiotemporal region representations $f^V \in \mathbb{R}^{T \times H \times W \times D}$ (see supplementary for more details). For the $j$-th spatiotemporal region, we tile its visual representation by the factor $H^A W^A$ and concatenate them with the audio bottleneck representations along the channel dimension: $f^{AV}_j = concat(f^A, tile(f^V_j))$, where $f^{AV}_j$ has the dimensions $\mathbb{R}^{H^A \times W^A \times 2D}$. We pass the concatenated representations into the decoder $\mathcal{D}$ consisting of a series of upsampling convolutional layers to generate a real-valued ratio mask: $\hat{M}_j = \mathcal{D}(f^{AV}_j) \in \mathbb{R}^{F \times N}$. To predict the separated audio source, each element of the mask is multiplied with the corresponding location in the input spectrogram: $\hat{A}^S_j = \hat{M}_j \odot A^S$, where $\odot$ denotes the Hadamard product.
%The mask is then applied to the input spectrogram to predict the audio component corresponding to the video: $\hat{A}^S_j = \hat{M}_j \odot A^S$.

%Similarly, we can perform the same decoding operation using text representations of language queries.
%Similarly, we can obtain an output mask given an input text query and audio spectrogram. Given a natural language query $L \in \mathbb{R}^{N_w}$ where $N_w$ is the number of words, we extract its representation $f^L \in \mathbb{R}^{1 \text{x} D}$ using the text encoder: $f^L = g^L(L)$. We stress that language queries are only used during inference. While language annotations are not used during training, the language representations of queries can be used interchangeably in place of the spatiotemporal region representations for the mask inference process since the learnt audio representations are projected into the joint visual-semantic space of CLIP. 
\smallskip
\noindent\textbf{Video and Multiple Instance Learning for mask prediction.} Since language annotations are not used during training, we train only with unlabeled videos. During training, as well as inference on audio-visual separation, we use the CLIP visual encoder to extract a set of language-grounded spatiotemporal region representations $f^V \in \mathbb{R}^{T \times H \times W \times D}$. Similarly, we can perform the same mask inference operation using videos by using the representation of each spatiotemporal region representation interchangeably in place of the language representation. This interchangeability is due to the fact that the CLIP representations project the video to the joint vision-language embedding space.

Prior work \cite{gao2019co} has demonstrated the advantages of performing audio-visual separation at the object level for videos with multiple objects. In the absence of object detectors and labels, we propose an MIL formulation, where all spatiotemporal regions in the input video are treated as positive candidates. Specifically, given a spatiotemporal grid of region representations extracted from the visual encoder as defined in this section,  we predict a spectrogram mask $\hat{M}_{j}$ for the $j$-th region. %Finally, we compute an aggregated spectrogram mask for the entire video:
Finally, we aggregate them over all regions to obtain a spectrogram mask for the entire video: %$\hat{M} = \frac{1}{T}\sum\nolimits_{j=1}^{THW}\hat{M}_{j}.$
\begin{equation}
    \hat{M} = \frac{1}{T}\sum\nolimits_{j=1}^{THW}\hat{M}_{j}.
\end{equation}
Intuitively, this formulation encourages the model to learn to identify the salient regions with sounding objects for audio separation. We obtain the predicted mask for each frame by computing the sum of the predicted masks over all spatial regions and average the predicted masks over the temporal dimension since we reason that a sounding object should have similar prediction masks across frames. We adopt the self-supervised `mix-and-separate' learning objective \cite{zhao2018sound,gao2019co} to train our model for predicting spectrogram masks. In this formulation, audio from two or more videos are mixed and the goal is to use the visual information of a video to separate its audio component. This setting allows us to generate pseudo-target masks for training supervision. We compute $\mathcal{L}_{mask}$ as the L1 loss between the pseudo-target and predicted ratio masks \cite{zhao2018sound}. Please refer to the supplemental for details of our spatiotemporal region representation, audio-visual separation model and $\mathcal{L}_{mask}$. The final objective function is computed as:
\begin{equation}
    \mathcal{L} = \lambda_1 \mathcal{L}_{mask} + \lambda_2 \mathcal{L}_{\text{Trimodal}} + \lambda_3 \mathcal{L}_{\text{Audio-language}}
\end{equation}
where $\lambda_1$, $\lambda_2$ and $\lambda_3$ denote the weights for the mask, tri-modal and audio-language consistency losses, respectively.

