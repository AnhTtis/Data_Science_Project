\section{Related Work}

\noindent\textbf{Audio-only source separation.} The goal of audio-only source separation is to use the aural cues present in the input audio waveform to separate the individual components. Conventional audio signal processing techniques rely on strong assumptions such as the number of sources in the audio waveforms to compute non-negative matrix factorization~\cite{wang2005musical,canadas2014percussive,fitzgerald2005shifted} of audio spectrograms. Deep learning methods  commonly adopt the self-supervised `mix-and-separate' strategy where multiple audio sources are combined into a synthetic mixture and then predict a spectrogram mask to retrieve queried audio components \cite{zhao2018sound, gao2019co, chatterjee2021visual}.

\noindent\textbf{Multimodal source separation.} Recent work in audio-text \cite{audiocaps,kilgour2022text} and audio-visual \cite{zhao2018sound, gao2019co, zhao2019sound, chatterjee2021visual,chowdhury2021listen} separation also use the `mix-and-separate' strategy to train a decoder to predict a spectrogram mask based on natural language and video queries, respectively. In the case of the former, state-of-the-art approaches can either accept a single object label \cite{zhao2018sound,gao2019co,chatterjee2021visual} or free-form natural language queries \cite{audiocaps,kilgour2022text}. Existing audio-visual source separation approaches often rely on training object detectors with strong supervision from bounding box annotations to localize objects of interest before learning to perform source separation at the object-level. In contrast, our proposed approach does not rely on pretrained object detectors or object labels in the training videos. The Sound of Pixels (SOP) model is the most similar in spirit to our proposed approach since it does not rely on object detectors. Instead, it learns to perform sound separation based on a video-level representation during training. Our \modelabb approach is also similar to the Voiceformer \cite{rahimi2022reading} approach which aims to separate speech from multiple speakers using a combination of audio, visual and language inputs. In contrast to our approach, their approach requires ground-truth text transcripts for training. Finally, the task is also similar in nature to that of sound localization in videos~\cite{feng2022sslnet,hu2022mix}.

\noindent\textbf{Multimodal representation learning.} One key challenge is grounding information from the text and/or audio modalities in the video frames \cite{tan2021look,gao2019co,chatterjee2021visual,anne2017localizing}. Since annotating video datasets is a costly process, prior work has investigated self-supervised pretraining methods on unlabeled videos~\cite{han2020memory, tong2022videomae}. This often involves utilizing complementary information from the audio, text and visual modalities~\cite{zellers2022merlot} to learn robust representations that can generalize to downstream tasks such as action recognition \cite{rouditchenko2020avlnet,piergiovanni2020evolving,akbari2021vatt} and text-to-video retrieval \cite{anne2017localizing, miech2019howto100m}. These approaches often use large-scale pretraining datasets such as Kinetics-400 / 600~\cite{carreira2018short} and HowTo100M~\cite{miech2019howto100m} before finetuning linear probes for target tasks and datasets. Recent work has also focused on prompting multimodal foundation models such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling} to adapt them to new tasks ranging from open-vocabulary object detection \cite{gu2021open} to domain adaptation \cite{gal2022stylegan} without modifying their parameters.

%With the increasing prevalence of multimodal foundation models such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}, recent work has also focused on prompting these models to adapt them to new tasks ranging from open-vocabulary object detection \cite{gu2021open} to domain adaptation \cite{gal2022stylegan} without modifying their parameters. 

%While prompting has mainly been popularized by the field of natural language processing~\cite{scao2021many}, adapting pretrained vision-language models~\cite{gal2022textual} has also attracted attention from recent approaches that address problems ranging from open-vocabulary object detection \cite{gu2021open} to domain adaptation \cite{gal2022stylegan}. %This is similar in spirit to our proposed approach, which adapts the pretrained CLIP model to extract latent captions from video frames for pseudo-target supervision. 