\section{Experiments}\label{sec:experiments}

We evaluate the effectiveness of our proposed \modelabb approach on audio-text and audio-visual source separation in videos. To perform fair comparison to prior work, we conduct experiments on the SOLOS \cite{montesinos2020solos}, MUSIC \cite{zhao2018sound} and AudioSet \cite{gemmeke2017audio} datasets. We refer readers to the supplemental for additional implementation and dataset details. Given a language query containing a prompt template and an object, the goal of \textbf{audio-text source separation} is to extract its corresponding sound component from an audio input, which is often comprised of a mixture of sounds emitted by various objects. Similarly, in \textbf{audio-visual source separation}, a model has to leverage the visual information in the input video to extract its corresponding sound component and align it with the relevant video pixels.

\noindent\textbf{Evaluation metrics.} We adopt the Normalized Signal-to-Distortion Ratio (NSDR), Signal-to-Interference Ratio (SIR) and Signal-to-Artifact Ratio (SAR) \cite{raffel2014mir_eval,rix2001perceptual}. NSDR computes the difference between the SDR of the predicted and mixed audio with respect to the ground-truth waveforms, which measures the sound quality of the separated audios. SIR measures the amount of contamination from other audio sources in the predicted audio. Finally, SAR measures the amount of artifacts in the predicted audio.

%\subsection{Implementation details}
%Consistent with prior work \cite{zhao2018sound,gao2019co}, we sample the audio clips at 11 kHz and use a Hann window size of 1022 and a hop length of 256 in the STFT operation. This step results in an audio spectrogram of dimensions 512 x 256, which is re-sampled on a log-frequency scale to compute a final spectrogram of dimensions 256 x 256. We use the CLIP Resnet50 model \cite{radford2021learning} and its language encoder to extract a latent caption for each video as well as encode visual and language representations for audio separation. We set the dimension of the audio U-Net bottleneck features $D$ to be the same as that of CLIP embedding space, which is 1024. We freeze the CLIP encoders during training and train the audio U-Net from scratch using a base learning rate of 4e-3. We train all models for 100 epochs with the SGD optimizer as well as using a linear warmup of 1000 steps and anneal the learning rate using a cosine decay schedule. 

%\begin{table}[t]
%\begin{center}
%\begin{tabular}{|c|c|ccc|}
%\hline
%Model & Visual encoder & NSDR  & SIR   & SAR  \\
%\hline 
%SOP \cite{zhao2018sound} & ImageNet & 6.05 & 9.81  & 10.61 \\
%SOP \cite{zhao2018sound} & CLIP & 3.01 & 8.77 & 8.18 \\
%\hline 
%\end{tabular}
%\caption{\textbf{Using CLIP visual features in Sound of Pixels model on the MUSIC dataset.}}
%\label{tab:sop_visual_comparison}
%\end{center}
%\label{solos_sop_loss_weights_ablation}
%\end{table}

\subsection{Quantitative results}\label{sec:results}

We compare our approach to the following baselines. Similar to our approach, \textbf{Sound of Pixels (SOP)} \cite{zhao2018sound} does not require object detectors but learns to perform video-level separation. We also implement the \textbf{AV-Mix-and-separate} baseline from \cite{gao2019co} with the same audio-visual separation model as ours but is trained to perform video-level separation and without our alignment objectives. The off-the-shelf \textbf{NMF-MFCC} \cite{spiertz2009source} baseline performs audio separation using non-negative matrix factorization (NMF) and Mel frequency cepstrum coefficients (MFCC). Next, we also compare to \textbf{Co-Separation} \cite{gao2019co} that learns to perform audio separation at the object-level by relying on object detectors. Last but not least, we include \textbf{AVSGS} \cite{chatterjee2021visual} that also relies on a high number of object proposals to construct a spatiotemporal scene graph to reason about context.

\noindent\textbf{Audio-visual source separation on MUSIC.} We report the results of our evaluation under two settings in Table~\ref{tab:music_eval}. In the first setting, each video contains only a single instrument while the `duet' setting includes videos that contain two different instruments. To begin, our \modelabb approach outperforms SOP by a large margin despite not relying on object detectors too. As shown in Table~\ref{tab:sop_visual_comparison}, we show that this performance gain is not due to just replacing ImageNet-pretrained Resnet18 \cite{he2016deep} visual representations with those of the CLIP model since it leads to a large drop in performance. We hypothesize that SOP does not learn a direct projection of the audio inputs used in source separation into the CLIP embedding space. In contrast, \modelabb facilitates a better adaptation of the learnt audio representations to the CLIP embedding. We also found it important to modify CLIP's self-attention (see supp.). We observe that \modelabb outperforms AV-Mix-and-Separate by significantly, particularly under the duet setting. This suggests that performing region-level audio separation provides more capacity for the model to reason about multiple sounding objects in videos.

%Finally, the significant performance gap between \modelabb and AV-Mix-and-Separate, particularly under the duet setting, suggests that performing region-level audio separation provides more capacity for the model to reason about multiple sounding objects in videos.

More significantly, we observe that our approach outperforms the Co-Separation approach \cite{gao2019co}, which relies on object labels in the videos. Since Co-Separation uses a high confidence region proposal for each instrument to localize relevant regions, our improvements over it suggest that our latent captions are able to express multiple visual concepts that are present in the video. Last but not least, our approach is also comparable to AVSGS even without scene graph reasoning modules. The latter constructs a spatiotemporal visual scene graph over a large number of region proposals to reason about context between detected objects. We note that their audio-visual scene graph component can be combined with our \modelabb approach to possibly improve performance but is beyond the scope of this work.

\begin{table}[t]
\centering
\begin{tabular}{|l|c|ccc|}
\hline
& \small{Object}   &    &    &   \\
\small{Method}   &  \small{detectors}  & \small{NSDR} $\uparrow$ & \small{SIR} $\uparrow$ & \small{SAR} $\uparrow$ \\
\hline
\small{Co-Separation \cite{gao2019co}} & Yes & 7.11 & 12.09 & 10.05 \\
\small{AVSGS \cite{chatterjee2021visual}} & Yes & \textbf{9.20} & \textbf{14.05} & \textbf{12.17} \\
\hline
\small{Sound-of-Pixels \cite{zhao2018sound}} & No & 6.28 & 10.84 & 10.13 \\
\small{AV-Mix-and-Sep~\cite{gao2019co}} & No & 2.94 & 5.81 & 8.33 \\
\small{NMF-MFCC} \cite{spiertz2009source} & No & 0.68 & 4.75 & 5.12 \\
\small{\modelabb (Ours)} & No & \textbf{8.58} & \textbf{14.16} & \textbf{12.35} \\
\hline 
\end{tabular}
\caption{\textbf{Audio-visual source separation results on the SOLOS dataset.} Similar to the results in Table~\ref{tab:music_eval}, we see that \modelabb significantly closes the gap with object detection-based approaches.}
\label{tab:solos_eval}
\vspace{-5pt}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|ccc|}
\hline
& \small{Object} &    &    &   \\
\small{Method}   &  \small{detectors}  & \small{NSDR} $\uparrow$  & \small{SIR} $\uparrow$  & \small{SAR} $\uparrow$\\
\hline
\small{Co-Separation \cite{gao2019co}} & Yes & 4.26 & 7.07 & 13.00 \\
\small{AVSGS \cite{chatterjee2021visual}} & Yes & \textbf{5.28} & \textbf{8.27} & \textbf{13.04} \\
\hline
\small{Sound-of-Pixels \cite{zhao2018sound}} & No & 1.66 & 3.58 & 11.50 \\
\small{AV-Mix-and-Sep~\cite{gao2019co}} & No & 1.68 & 3.30 & 12.20 \\
\small{NMF-MFCC} \cite{spiertz2009source} & No & 0.25 & 4.19 & 5.78 \\
\small{\modelabb (Ours)} & No & \textbf{4.15} & \textbf{7.62} & \textbf{13.20} \\
\hline 
\end{tabular}
\caption{\textbf{Audio-visual source separation results on AudioSet.} \modelabb generalizes much better to the noisier AudioSet dataset than Sound-of-Pixels which also does not use object detections.}
\label{tab:audioset_eval}
\end{table} 

\begin{table}
\begin{center}
\begin{tabular}{|l|c|ccc|}
\hline
Model & Visual encoder & NSDR $\uparrow$ & SIR  $\uparrow$ & SAR $\uparrow$ \\
\hline 
SOP \cite{zhao2018sound} & ImageNet  & 6.28 & 10.84  & 10.13 \\
SOP \cite{zhao2018sound} & CLIP & 4.42 & 8.36 & 8.21 \\
Ours & CLIP & \textbf{8.58} & \textbf{14.16} & \textbf{12.35} \\
\hline 
\end{tabular}
\caption{\textbf{Using CLIP visual features in SOP model on the SOLOS dataset.} Using CLIP visual features naively in existing methods results in a drop in performance.}
\label{tab:sop_visual_comparison}
\end{center}
\vspace{-20pt}
\end{table}

\noindent\textbf{Audio-visual source separation on SOLOS.} While the videos in the SOLOS dataset generally have less background noise than those of the MUSIC dataset, we see in Table~\ref{tab:solos_eval} that AV-Mix-and-Separate and NMF-MFCC are still unable to generalize to the cleaner audio signals.  Similar to the reported performance on the MUSIC dataset, we also observe that \modelabb is comparable to the strongly supervised AVSGS approach on the SOLOS dataset. One possible reason behind the lower performance of AVSGS on videos with single instrument is that it is less critical to reason about context for videos with a single sounding object. Our better performance as compared to Co-Separation suggests that learning to perform audio separation at the coarse video region level under an MIL formulation can serve as an effective replacement for training object detectors. These results also indicate the great promise of latent captions in replacing ground-truth object annotations for supervision.

\noindent\textbf{Audio-visual source separation on AudioSet.} Finally, we report the results of our evaluation on the AudioSet dataset in Table~\ref{tab:audioset_eval}. AudioSet has been documented by prior work \cite{gao2019co} to be much noisier than the videos in the other two datasets, which explains the lower performance across all approaches. Unlike the SOP model, we observe that our training approach is more robust to noise in the dataset where the sounding object may not always be visible.

\begin{table}
\centering
\begin{tabular}{|c|c|ccc|}
\hline
$\mathcal{L}_{\text{Audio-language}}$ & $\mathcal{L}_{\text{Trimodal}}$ & NSDR $\uparrow$ & SIR $\uparrow$  & SAR  $\uparrow$\\
\hline \xmark & \xmark & 5.47 & 10.55 & 10.95 \\
\cmark & \xmark & 8.08 & 13.74 & 12.18 \\
\xmark & \cmark & 8.10 & 13.84 & 11.79 \\
\cmark & \cmark & \textbf{8.58} & \textbf{14.16} & \textbf{12.35} \\
\hline 
\end{tabular}
\caption{\textbf{Ablation of our audio-language and tri-modal consistency alignment objectives on SOLOs.} The alignment objectives help to improve audio-visual separation performance.}
\label{solos_sop_loss_weights_ablation}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c|ccc|}
\hline
\# tokens  & NSDR $\uparrow$ & SIR $\uparrow$ & SAR $\uparrow$ \\
\hline 
1  & 7.31 & 11.34 & 11.71\\
2  & 7.67 & 13.07 & 11.45\\
3  & \textbf{8.58} & \textbf{14.16} & \textbf{12.35}\\
4 & 8.02 & 13.39 & 11.53\\
\hline 
\end{tabular}
\caption{\textbf{Ablation over number of learnable tokens on SOLOS.} Using more learnable tokens generally improves performance.
%Using more learnable tokens generally allow the language encoder to express more visual concepts.
}
\label{solos_num_learnable_tokens_ablation}
\vspace{-15pt}
\end{table}

\begin{table*}[t]
\centering
\begin{tabular}{|c|c|ccc|ccc|ccc|}
\hline
Query & Alignment  &    & SOLOS &   &    & MUSIC & &    & Audioset & \\
Modality   &  objectives  & NSDR $\uparrow$ & SIR $\uparrow$ & SAR $\uparrow$ & NSDR $\uparrow$ & SIR $\uparrow$ & SAR $\uparrow$ & NSDR $\uparrow$ & SIR $\uparrow$ & SAR $\uparrow$ \\
\hline
Language & No & -3.05 & 2.79 & 3.77 & -3.67 & 2.51 & 3.41 & -5.02 & 1.98 & 14.94\\
Language & Yes & \textbf{6.92} & \textbf{12.07} & \textbf{10.41} & \textbf{6.45} & \textbf{11.18} & \textbf{10.77} & \textbf{2.36} & \textbf{4.71} & \textbf{10.28} \\
%\hline
%Visual & Yes & 8.58 & 14.16 & 12.35 & 8.08 & 13.97 & 11.33 & 4.15 & 7.62 & 13.20\\
\hline 
\end{tabular}
\caption{\textbf{Audio-text separation with natural language queries.} We evaluate our model, that is trained only on unlabeled videos, on the task of audio-text separation. Note that we do not compare to other audio-visual separation baselines since there is no straightforward way to adapt them for language queries. We compare our full \modelabb model to a variant that is only trained with the mask prediction loss.}
\label{tab:language_queries_eval}
\vspace{-10pt}
\end{table*}
\vspace{-10pt}
\subsubsection{Model Analysis}
\noindent\textbf{Ablation of $\mathcal{L}_{\text{Trimodal}}$ and $\mathcal{L}_{\text{Audio-language}}$. } Table~\ref{solos_sop_loss_weights_ablation} provides an ablation over our audio-language and tri-modal consistency losses. Despite being trained on a larger and more diverse set of visual concepts, we find the CLIP visual representations alone do not encourage the audio encoder to learn a strong alignment between the audio and vision modalities (row 1). We observe the importance of our proposed alignment objectives where adding one or the other leads to a significant improvement in audio separation performance. Due to multiple training videos containing similar instruments, it is likely that we are treating some latent captions as false negative samples for each video in the audio-language consistency objective. However, we see that the consistency objective is still beneficial towards audio separation.

\noindent\textbf{Number of learnable token embeddings in latent captions.} We report the results of our ablation over the number of learnable token parameters used in the extraction of latent captions in Table~\ref{solos_num_learnable_tokens_ablation}. We observe that increasing the number of learnable tokens generally helps to improve the performance, although using 4 tokens hurts performance slightly. We hypothesize that a higher number of learnable tokens provide more capacity for the CLIP language model to express multiple visual concepts that are present in the videos. This finding suggests that using more tokens may be beneficial for videos with more complex visual scenes. 
%However, we also see that using 4 learnable tokens actually results in a slight drop in performance. 
%This result suggests that the number of learnable tokens may be highly-specific to the target datasets.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.5\textwidth]{Figures/positive_learnable_concept_attn_v2.pdf}
%    \caption{\textbf{Visual attention of latent captions.} We see that the latent captions tend to focus on salient foreground objects.}
%\label{fig:latent_concept_attn_map}
%\end{figure}

\subsection{Audio-language source separation}
%\noindent\textbf{Audio-text source separation with descriptive natural language queries.} 
Finally, we evaluate the capability of our trained model to separate audio components based on natural language queries in Table~\ref{tab:language_queries_eval}. We construct each query using the template `person playing a \{instrument\}'. We stress that we only train on unlabeled videos without text supervision. To begin, we compare our \modelabb model to a variant that is trained without our alignment objectives. Note that we do not compare to other audio-visual approaches since there is no straightforward way to adapt them for language queries. As evidenced by the significant performance gap between both variants, our alignment losses are integral to learning a strong transitive alignment between the audio and language modalities. This finding suggests that just learning an alignment between the audio and visual modalities does not result in a \emph{transitive} relationship between the audio and natural language modalities. 

%Additionally, we hypothesize that it may be critical for the audio encoder $\mathcal{E}$ to learn an encoding function that is well-aligned with the visual-semantic embedding space of CLIP for the audio separation model to generalize to both video and language queries. 
Additionally, for effective sound source separation given either video or text inputs, we hypothesize that it is critical to learn an audio encoder E that is well-aligned with CLIP's vision-language embedding. Note that the same shared audio encoder $\mathcal{E}$ encodes both mixed and predicted audio sources. Thus, it can also be used as a general audio encoder and the decoder $\mathcal{D}$ can condition its separation process on either language or video queries. Lastly, we observe that our alignment objectives are still insufficient for closing the performance gap between audio-text and audio-visual separation (Tables~\ref{tab:music_eval}, \ref{tab:solos_eval} and \ref{tab:audioset_eval}) completely. This observation indicates that future work is needed for further improving the latent alignment between all three modalities.

\subsection{Qualitative results}
\noindent\textbf{Predicted audio-to-video attention visualizations.} To evaluate the semantic consistency between the predicted audio sources and the visual cues that guide the separation process, we plot the attention map between the encoded predicted spectrograms and their corresponding video frames (Section~\ref{sec:alignment_losses}) in Figure~\ref{fig:audio-to-video-attn}. We observe that \modelabb helps the model to learn to ground the audio source in the video pixels despite not requiring bounding box supervision.

\begin{figure}[h]
    \centering
\includegraphics[width=0.5\textwidth]{Figures/audio_attn_visualization_v3.pdf}
    \caption{\textbf{Predicted audio attention in videos.}
    Our consistency objectives help the model to localize the relevant regions more accurately without prior knowledge of sounding object locations.
    %Despite training without prior knowledge of sounding object labels and their locations, our consistency objectives help the model to localize the relevant regions more accurately.
    }
    \label{fig:audio-to-video-attn}
    \vspace{-10pt}
\end{figure}

\begin{figure}[h]
    \centering
\includegraphics[width=0.5\textwidth]{Figures/language_predictions_visualization_v5.pdf}
    \vspace{-20pt}
    \caption{\textbf{Predicted audio spectrograms with language queries.} Interestingly, we see that including visually grounded adjectives in the queries affects the quality of the separated audio sources.
    }
    \label{fig:language_spec_outputs}
    \vspace{-15pt}
\end{figure}

\noindent\textbf{Predicted outputs of language-based audio separation.} Finally, we present an example of how different language queries affect the separation performance for the same audio input in Figure~\ref{fig:language_spec_outputs}. Significantly, despite not relying on text annotations for training supervision, our model can separate audio components based on free-form language queries. This demonstrates the effectiveness of our proposed idea to substitute ground-truth text annotations with \emph{latent captions} during training. Additionally, we also observe that providing more context in the language query (\eg, ``violin" versus ``person playing a violin") can lead to improved separations.

\vspace{-5pt}

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.5\textwidth]{Figures/positive_learnable_concept_attn_v2.pdf}
%    \caption{\textbf{Visual attention of latent captions.} We see that the latent captions tend to focus on salient foreground objects.}
%\label{fig:latent_concept_attn_map}
%\end{figure}

%\begin{figure}[h]
%    \centering
%\includegraphics[width=0.5\textwidth]{Figures/audio_attn_visualization_v3.pdf}
%    \caption{\textbf{Predicted audio attention in videos.}
%    our consistency objectives help the model to localize the relevant regions more accurately without prior knowledge of sounding object locations.
    %Despite training without prior knowledge of sounding object labels and their locations, our consistency objectives help the model to localize the relevant regions more accurately.
%    }
%    \label{fig:audio-to-video-attn}
%    \vspace{-10pt}
%\end{figure}

%\begin{figure}[h]
%    \centering
%\includegraphics[width=0.5\textwidth]{Figures/language_predictions_visualization_v5.pdf}
%    \caption{\textbf{Predicted audio spectrograms with natural language queries.} Interestingly, we observe that leveraging large vision-language foundation model causes the audio separation quality to be affected by the inclusion of visually grounded adjectives.}
%    \label{fig:language_spec_outputs}
%    \vspace{-15pt}
%\end{figure}

 %\noindent\textbf{Limitations.} While we have demonstrated that our proposed \modelabb approach is able to generalize well to free-form natural language queries for source separation, we observe that it is only able to handle visually descriptive adjectives such as \emph{person playing a small trumpet} instead of \emph{a loud trumpet}. We hypothesize that this limitation is due to a higher likelihood of visually descriptive adjectives appearing in the alt text of the pretraining dataset used by CLIP. Additionally, we only focus on separating sounds of different object classes. Our approach does not generalize well to discriminating between sounds from multiple instances of the same class (\cf, Fig {\color{red} 5} middle showing that we can detect the clarinets but not distinguish the different instances). An example of such a challenging task is audio-visual speech separation, where there are two or more people speaking simultaneously and the goal is to separate for the speech for each person. Similar to existing audio-visual speech separation approaches~\cite{ephrat2018looking,rahimi2022reading}, future work can aim to address this limitation by leveraging representations of different instances and additional information in the form of object labels and speech narrations.