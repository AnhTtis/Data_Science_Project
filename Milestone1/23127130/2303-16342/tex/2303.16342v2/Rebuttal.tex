\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{wrapfig}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
\setcounter{table}{10}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}

\newcommand{\Rone}{{\color{orange}{R1}}}
\newcommand{\Rtwo}{{\color{blue}{R2}}}
\newcommand{\Rthree}{{\color{teal}{R3}}}
\newcommand{\Rall}{{\color{cyan}{All}}}

\newcommand{\reuben}[1]{\emph{\textcolor{red}{reuben: #1}}}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6792} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Language-Guided Audio-Visual Source Separation via Trimodal Consistency}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
%\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\noindent(\Rall) We thank the reviewers for their feedback. We define Reviewers S81Q$\rightarrow$\Rone, vkgr$\rightarrow$\Rtwo, and hNU9$\rightarrow$\Rthree. Reviewers agree that our work proposes an interesting approach that reduces the need for object detectors in audio-visual separation. In particular, \Rtwo\ mentions that our work is ``likely to be the first to introduce language" to this task. 
%In particular, \Rone\ states that our analysis on learning a transitive alignment between the three modalities is insightful while \Rtwo\ mentions that our work is ``likely to be the first to introduce language" to this task. 
%We will also incorporate all feedback into our paper.
%We thank the reviewers for their feedback. We define Reviewers S81Q$\rightarrow$\Rone, vkgr$\rightarrow$\Rtwo, and hNU9$\rightarrow$\Rthree. Reviewers agree that our work proposes an interesting approach that alleviates the dependence on object detectors for the task of audio-visual separation. In particular, \Rtwo\ mentions that our work is likely to be the first to introduce ``language to the task of audio-visual source separation". %We will also incorporate all feedback into our paper.
%In particular, \Rone\ states that our analysis on learning a transitive alignment between the video, language and audio modalities is insightful while \Rtwo\ mentions that our work is likely to introduce ``language to the task of audio-visual source separation". %We will incorporate all the feedback in the next version of the paper. 

\noindent(\Rone) \textbf{1. Audiovisual speech source separation.} This task is different from ours -- we seek to separate sounds of different object classes. Our approach does not generalize well to discriminating between sounds from multiple instances of the same class (\cf, Fig {\color{red} 5} middle showing that we can detect the clarinets but not distinguish the different instances). We will discuss this limitation whereby future work can aim to address it by including additional information, such as representing different instances and speech narrations [{\color{green} 26}]. 

\noindent(\Rone) \textbf{2. Combining with an object detector.} 
We evaluate our trained model by replacing spatiotemporal region representations with those of bounding boxes during inference. We encode each bounding box as an image representation separately.
\begin{wraptable}{l}{50mm}
    \vspace{-15pt}
    \begin{tabular}{|c|ccc|}
    \hline
      &  NSDR  & SIR  & SAR  \\
      \hline
      Regions & 8.58  & 14.16 & 12.35 \\
Boxes & 8.32 & 13.63 & 12.22 \\
     \hline
    \end{tabular}
    \vspace{-10pt}
    \caption{\textbf{Evaluation on SOLOS.}}
    \label{tab:bbox_eval}
    \vspace{-20pt}
\end{wraptable}
Note that this is different from the region representations that are extracted from the modified self-attention operation in CLIP visual encoder (Section {\color{red} A.2} supp.). Consequently, our trained models may not generalize well to the different visual representations used during training and inference. We report our results in Table~\ref{tab:bbox_eval}, where we observe that using bounding box representations in our trained models leads to a slightly lower performance in audio-visual separation. %It is likely that our trained models do not generalize well to bounding box representations since they are not used during training. 
%We hypothesize that we will achieve better performance when we train the model with these bounding box representations from the beginning. 

%\begin{table}[t]
%\centering
%\begin{tabular}{|c|ccc|}
%\hline
%&  NSDR  & SIR  & SAR  \\
%\hline
%Regions & 8.58  & 14.16 & 12.35 \\
%Bounding boxes & 8.32 & 13.63 & 12.22 \\ 
%\hline 
%\end{tabular}
%\caption{\textbf{Evaluation with bounding boxes on SOLOS.}
%}
%\label{tab:bbox_eval}
%\vspace{-15pt}
%\end{table}

\noindent(\Rtwo) \textbf{1.\ Global video embeddings.} Note that ViViT is a video classification model. Our work relies on a strong vision-language foundation model. There is a lack of models trained on videos and text that go significantly beyond training on images and text. We leave to future work to explore effectively leveraging a video-language model. 

\noindent(\Rtwo) \textbf{2.\ Outperforming strong supervision claim.} We will revise the writing to state that our approach achieves performance that is competitive with those methods.

\noindent(\Rtwo) \textbf{3.\ Visually descriptive adjectives.} We hypothesize that the alt-text captions used during CLIP training contain visually descriptive adjectives, and our approach inherits this bias. Since we are not imposing any constraints on the extracted latent captions (Sec.\ {\color{red} 3.1}), it is probable that they may encode visual adjectives to describe the video frames.

\noindent(\Rtwo) \textbf{4.\ Search time for parameter $p$ in Eq.\ (1)?} $\sim$148 seconds per video on a RTX 2080 GPU for 5k iterations.

\noindent(\Rtwo) \textbf{5.\ Inter-object interaction sounds.} %Loss (2) cannot generalize well to sounds of inter-object interactions. This is a current limitation of our proposed approach. To some degree, the prompt template of our language queries containing ``a person playing a \emph{instrument}’’ specifies the interactions between a person and the instrument. However, we hypothesize that an approach will require significantly more information beyond recognizing objects, such as information about motion, to be able to separate these sounds effectively. For example, a person strumming a guitar will produce a very different sound as compared to tapping it.
%Loss (2) cannot generalize well to inter-object interaction sounds since it aims to learn to discriminate between sounds of different object classes. By learning to identify these sounds, a model may not be able to separate sounds of individual objects.
Loss ({\color{red} 2}) aims to learn to associate a separated sound with the latent caption, which often corresponds to a single object source. In theory, if the latent caption captures inter-object 
interactions, Loss ({\color{red} 2}) can also learn to associate them with the separated sound. We leave it to future work to study these phenomena. %with more complex training/evaluation datasets.

\noindent(\Rtwo) \textbf{6.\ Off-screen sounds.} For evaluation, we perform separation on the linearly combined audio sources from two or more videos, which simulates off-screen sounds. %We will include a real-world off-screen sound separation qualitative result.

\noindent(\Rtwo) \textbf{7.\ Baseline in supp.\ video.} The baseline uses only the mask prediction loss (no multimodal consistency losses).

\noindent(\Rthree) \textbf{1.\ Videos as an intermediary modality.} %Consider an example video of a dog barking. The visual concept of the dog should have high correspondences with the phrase ``a dog" as well as the sound of barking. Our proposed approach is premised on exploiting the existing strong vision-language alignment embodied in CLIP. Since we are training on unlabeled videos without text annotations, we use CLIP to extract latent captions (Section 3.1) for the videos and introduce our multimodal consistency losses that use the latent captions to improve the alignment between the learnt audio and CLIP visual and language representations.
%In an example video of a guitar performance, the visual information should have high correspondences with the phrase ``a guitar" and the sound of one. Thus, our approach is premised on exploiting the strong vision-language alignment embodied in CLIP. Since our training videos do not have text annotations, we use CLIP to extract latent captions (Section {\color{red} 3.1}) for the videos and introduce our multimodal consistency losses that use the latent captions to improve the alignment between the learnt audio and CLIP visual and language representations.
Given a video of a guitar performance, we aim to learn the correspondences between the sounds and the visual concept of a guitar as well as relevant phrases like 'playing the guitar'. During training, we use video-audio clip pairs to learn a strong audio-visual alignment with CLIP visual representations for separation. Since CLIP has already learnt a vision-language alignment, our introduced model and losses facilitate learning the transitive alignment between the audio and language modalities. Intuitively, this can be formulated as: $A \approx V \And V \approx L \rightarrow A \approx L$, where $A$, $V$ and $L$ denote the audio, video and language modalities, respectively.
%During training, we use video and audio clip pairs to learn a strong audio-visual alignment for separation. Since the unlabeled videos do not contain text annotations, we leverage CLIP which has already learnt a vision-language alignment to learn the transitive alignment between the audio and language modalities. Our introduced model architecture and losses facilitate joint reasoning across the three modalities.

\noindent(\Rthree) \textbf{2.\ Foundation models and transitive alignment.} Given the learnt alignment between the audio and CLIP visual representations, the audio representations should also have high similarities with Clip language representations of relevant phrases. However, we observe empirically (Table {\color{red} 4}) that an audio-visual separation model trained with CLIP visual representations does not generalize well to its text representations of queries. We will clarify this in the paper.
%Our task requires learning a strong alignment between the audio and CLIP visual representations. Given the existing visual-language alignment learnt by CLIP, the learnt audio representations should also have high similarities with those of relevant phrases. However, we observe empirically (Table {\color{red} 4}) that an audio-visual separation model trained with CLIP visual representations does not generalize well to its text representations of queries. We will clarify this in the paper.

\noindent(\Rthree) \textbf{3.\ Outperforming some strongly-supervised models.} One possible reason is that our losses help distill knowledge from CLIP into the audio model. Due to CLIP's large-scale pretraining, it has been shown to learn robust visual representations. We would also like to point out it is common for self-supervised approaches to outperform strongly-supervised models in different domains [{\color{green} A}, {\color{green} B},{\color{green} C}, {\color{green} D}, {\color{green} 15}, {\color{green} 24}].
%the audio [{\color{green} B},{\color{green} C}]., visual [{\color{green} 15},{\color{green} 24}] and text [{\color{green} A},{\color{green} D}] domains.

\noindent(\Rthree) \textbf{4.\ Comparison to works on similar tasks.} 
 These are mostly focused on sound localization in videos instead of separation. Also, their code is not publicly available. %We will discuss them in the paper.

\noindent(\Rthree) \textbf{5.\ Contributions and ablations.} Our major contributions include a self-supervised approach that uses foundation models to replace object detectors and two novel losses that facilitate learning without text annotations. The combination of these losses gives $12-56\%$  relative improvements (Table {\color{red} 5}). As \Rtwo\ points out, our work is likely to be the first to unify all three modalities for this task. Besides audio-visual separation (Tables {\color{red} 1},{\color{red} 2},{\color{red} 3}), we also ablate our model by separating audio sources with text queries (Table {\color{red} 7}).
%the effectiveness of using language queries by evaluating on audio-text separation (Table {\color{red} 7}). 
%Our primary contribution is to propose a self-supervised approach that leverages vision-language foundation models to alleviate the need for object detectors. As \Rtwo\ points out, our work is likely to be the first to unify all three modalities for this task. We also introduce two novel multimodal alignment losses that make use of \emph{latent captions} to enable us to learn from unlabeled videos. Last not least, we also introduce a MIL formulation to separate audio sources at the region level. In addition to audio-visual separation, we also evaluate our approach on separating audio sources using language queries (Table 7). %Note that latent captions are used during training while language queries consisting of the ground-truth objects are used during this evaluation. 
%If you have any suggestions on additional ablation experiments, we are more than happy to include them in the next version of the paper.
%We will be happy to include suggested experiments in the next version of the paper.

\noindent(\Rthree) \textbf{6.\ Code.} We will publicly release our code \& models.
\vspace{-7pt}
\hrule
\vspace{3pt}
\noindent \scriptsize \textbf{References.} [{\color{green} A}] Devlin et al. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019. [{\color{green} B}] Gong et al. Ssast: Self-supervised audio spectrogram transformer. AAAI, 2022. [{\color{green} C}] McCallum et al. Supervised and unsupervised learning of audio representations for music understanding. Arxiv, 2022. [{\color{green} D}] Radford et al. Language models are unsupervised multitask learners. OpenAI,2019.



%%%%%%%%% REFERENCES
%{\small
%\bibliographystyle{ieee_fullname}
%\bibliography{egbib}
%}

\end{document}
