% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{xcolor}         % colors
\usepackage{enumitem}
%\usepackage{subfig}
\usepackage{pifont}
\usepackage{tablefootnote}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\reuben}[1]{\emph{\textcolor{blue}{reuben: #1}}}
\newcommand{\bryan}[1]{\emph{\textcolor{orange}{Bryan: #1}}}
\newcommand{\ks}[1]{\emph{\textcolor{red}{KS: #1}}}
\newcommand{\uri}[1]{\emph{\textcolor{magenta}{ON: #1}}}
\newcommand{\ar}[1]{\emph{\textcolor{olive}{AR: #1}}}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6792} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\newcommand*{\modelname}{Video-Audio Separation through Text\xspace}

\newcommand*{\modelabb}{VAST\xspace}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
%
%\title{Sight and Sound of Language: Self-Supervised Natural Language Audio Source Separation in Videos}
\title{Language-Guided Audio-Visual Source Separation via Trimodal Consistency}
%\title{Adapting Vision-Language Models for Video Sound Source Separation}
%\title{Language-Guided Audio-Visual Source Separation via Transitive Alignment}

%Kate: some more title ideas
% Sound of Objects: Separating Audio Sources in Video by Aligning Them to Latent Captions
% Sound of Objects: Audio-Visual Source Separation without Labels
% Sight-and-Sound: Audio-Visual Language Grounding in Unlabeled Video
% Text2Sound: Mapping Language to Sight and Sound without Labels
% Caption2Sound:..
% AudibleObjects: Self-Supervised Caption Grounding in Audio-Visual Inputs
% AudibleCaptions:..

\author{Reuben Tan$^{1}$ \ \ \ \ Arijit Ray$^{1}$ \ \ \ \ Andrea Burns$^{1}$ \ \ \ \ Bryan A. Plummer$^{1}$ \ \ \ \  Justin Salamon $^{2}$ \ \ \ \  \\
Oriol Nieto$^{2}$ \ \ \ \ Bryan Russell$^{2}$ \ \ \ \ Kate Saenko$^{1,3}$ \\
$^{1}$Boston University, $^{2}$Adobe Research, $^{3}$MIT-IBM Watson AI Lab, IBM Research \\
{\tt \small \{rxtan, aburns4, array, bplum, saenko\}@bu.edu}, {\tt \small \{salamon, onieto, brussell\}@adobe.com} \\
} 
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   We propose a self-supervised approach for learning to perform audio source separation in videos based on natural language queries, using only unlabeled video and audio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities.  
   During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised approaches despite not using object detectors or text labels during training. Our project page including publicly available code can be found at \href{https://cs-people.bu.edu/rxtan/projects/VAST}{https://cs-people.bu.edu/rxtan/projects/VAST}. 
   
   %We provide a demo video that includes examples of audio-visual and audio-language separation on videos by our trained model in the wild.
\end{abstract}

\input{1_introduction.tex}
\input{2_related.tex}
\input{3_approach.tex}
\input{4_experiments.tex}
\input{5_conclusion.tex}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\input{6_appendix.tex}

\end{document}
