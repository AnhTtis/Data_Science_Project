

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/motivational_figure_v13.pdf}
    \caption{In this work, we address the task of self-supervised audio source separation in unlabeled videos by extracting latent concepts from large-scale pretrained visual-and-language model.}
    \label{fig:latent_motiv}
\end{figure}

\section{Introduction v5}

\begin{enumerate}
    \item \textbf{BR:} add a paragraph at the beginning to motivate the importance of natural language and the overall task (and maybe point to/cite the success of other language-vision tasks). For example, natural language provides an intuitive and flexible interface for selecting sounds. You could also make the point that the co-occurrence of a sounding object and its depiction in video helps a model learn to identify and separate the sound from the background, and that we want to learn this co-occurrence from audio-video pairs without human supervision.
    \item \textbf{BR:} I think it would be good to articulate clearly the key challenge that this paper addresses: how to jointly localize sounding objects in both a video and an audio waveform, and to associate both with natural language, all in a self-supervised way. You can then point out that prior work learns to associate separated audio with bounding boxes detected for a set of fixed-vocabulary categories. Your approach and key contribution (and I think you can make this point stronger) leverages the recent success of vision-language foundation models (e.g., CLIP) for overcoming this challenge. You show how to incorporate CLIP into a prior mix-and-separate (Sound of Pixels) approach for source separation. Finally, you introduce two novel losses for self-supervised learning.
    \item \textbf{BP:} When you talk about what removing the bounding boxes would enable ("We believe that"), you could also bring up the fact that this restricts what categories of objects you can separate the sounds from (which you don't explicitly state, at least so far in my reading)
    \item \textbf{BP:} In the paragraph starting with "To the best of our knowledge" you make it seem like you are doing the same thing as prior work
    \item \textbf{BP:} You make some reference to the assumptions of prior work, but again, don't explicitly state them when you bring up the "mix and separate" part of the discussion.  It makes it hard for me to follow how you are doing something different
\end{enumerate}

Separating audio sources in videos or based on natural language queries is an inherently challenging problem since the input audio is usually a mixture of sounds from different objects and there is a lack of ground-truth source annotations. To ease the difficulty of this task, state-of-the-art multimodal source separation approaches often rely on strong supervision in the form of human-annotated captions [] and trained object detectors [] for audio-text and audio-visual separation, respectively. However, training object detectors to identify possible regions of interest (Figure~\ref{fig:latent_motiv} top) may not always be feasible due to insufficient bounding box annotations for objects in the long-tailed distribution. More importantly, this severely limits their capability to learn from an abundance of unlabeled data videos online. In contrast, we, as humans, have an innate ability to separate the mixed audio input into its individual components and attribute each one to a potential object, visible or otherwise, when we perceive the environment around them.

Inspired by this capability, we endeavor to address the above-mentioned limitations by learning to separate audio sources based on any combination of video and natural language queries without strong supervision, illustrated in Figure~\ref{fig:latent_motiv} (bottom). We believe that alleviating the need for human-annotated captions and bounding box supervision during training will open up the possibility for learning to perform multimodal audio source separation from the plethora of unlabeled videos online, allowing approaches to generalize well to a large variety of audio sources. Similar to how humans often leverage object sounds to understand their states and interactions with other objects, multimodal audio source separation is integral to improving machine perception of our environment. It has important implications in downstream applications including holistic video understanding [], embodied AI [] and bidirectional audio-to-video retrieval [].

To the best of our knowledge, we propose the first completely self-supervised approach for learning to separate audio source components with text and/or video queries from unlabeled videos without object labels or text captions at all. Due to a lack of ground-truth source annotations in videos, existing approaches often adopt the 'mix-and-separate' strategy where audios from multiple video clips are combined and the learning objective is to separate the audio components based on visual cues of the individual videos. Our approach adopts the same strategy but uses a Multiple Instance Learning (MIL) formulation to learn to perform source separation at the \emph{region} level without making assumptions on the presence of objects in the videos. The absence of text annotations in these videos raises the question of how our approach can learn to separate audio components with natural language queries despite not training with them. To this end, we adapt large-scale pretrained models to extract latent concepts and leverage them for pseudo-target supervision in the form of two novel loss functions. 

These loss formulations stem from our key intuition that a model has to learn a strong alignment between the visual, language and audio representations such that text and videos can be used interchangeably or in combination to perform source separation. To this end, we impose cyclic consistency losses where the spatiotemporal region features in the video are first used to separate the corresponding audio source before maximizing the similarity between the representation of the final predicted audio source and the latent concept in two different ways. Intuitively, this allows the model to identify regions with the most relevant visual concepts for source separation, facilitating its ability to learn from multi-source videos without prior knowledge of possible objects or their positions.

Despite not relying on trained object detectors, our proposed approach outperforms strongly-supervised state-of-the art-approaches by a significant margin across multiple datasets. More significantly, we demonstrate that our trained model can also separate sound sources based on natural language queries despite not having access to ground-truth object labels or using text representations during training. In summary, our contributions are listed as follows:
\begin{enumerate}
    \item To the best of our knowledge, we propose the first fully self-supervised approach to learn to separate audio source components with video and/or natural language queries from unlabeled videos.
    \item We introduce two novel loss functions that leverage latent concepts to learn a strong alignment between the audio, text and video representations. 
    \item We demonstrate that our approach can generalize to natural language queries despite not training with human-annotated captions. 
    \item We conduct extensive source sound separation experiments across 4 different datasets including SOLOS, MUSIC, UMRP and AudioSet, where we outperform state-of-the-art strongly-supervised approaches by a significant margin.
\end{enumerate}
