\section{Approach}

\begin{enumerate}
    \item After reading through Section 3 and thinking over the contributions, I think you should make the two new training losses the hero of the paper. My suggestion is to reorder the subsections as follows: (1) Section 3.3, (2) Section 3.2. You can put the current Section 3.1 into the current Section 3.2 (see comment below). The current Section 3.3 needs a lot more motivation/intuition, so this reordering will force you to write more to make it clear. 
    \item Section 3.1 primarily includes low-level model training details. I would create a "Training" paragraph in the current Section 3.2 and move most of the current Section 3.1 content there. 
\end{enumerate}

Given a set of unlabeled videos and their accompanying audios, our goal is to learn to separate audio sources based on multimodal queries in a self-supervised manner. To this end, we propose a novel \modelname approach that learns effective audio representations for audio-visual source separation as well as a strong alignment between the audio and natural language modalities. Our approach consists of an audio encoder that extracts latent representations from an input audio spectrogram and a decoder that predicts a real-valued ratio spectrogram mask that corresponds to a query. 

To facilitate the model to learn to separate audio sources based on video or natural language queries, our approach encourages it to learn a projection of the audio representations to the joint visual-semantic embedding space learned by large-scale pretrained vision-language foundation models. Our key insight is to use videos as the intermediary modality to learn a \emph{transitive} relationship between the audio and natural language modalities. Intuitively, this helps the model learn to separate audio components with language queries by training only on unlabeled videos. However, we observe in our initial experiments that using CLIP visual representations in SOTA audio-visual separation approaches results in learnt audio representations that do not have a strong transitive alignment with the natural language modality. We circumvent this by introducing the Kullback-Leibler (KL) divergence and cyclic consistency loss functions that encourages the audio encoder to learn a mapping that is well-aligned with both visual and natural language modalities. We begin by defining the problem formulation in Section~\ref{sec:problem_definition} before describing our \modelname in Section~\ref{sec:model_arch}. Finally, we present the novel KL divergence and cyclic consistency losses in Section~\ref{sec:alignment_losses}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/latent_concept_motivational_figure_v1.pdf}
    \caption{Extraction of latent concepts for pseudo-supervision. }
    \label{fig:latent_extraction}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/combined_model_figure_v2.pdf}
    \caption{Combined approach. }
    \label{fig:latent_extraction}
\end{figure*}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.5\textwidth]{Figures/model_figure.pdf}
%    \caption{In this work, we endeavor to determine the effectiveness of leveraging powerful pretrained image-and-text foundation models such CLIP and ALIGN for multimodal sound source separation in videos under the open-vocabulary setting. }
%    \label{fig:motiv}
%\end{figure}

\subsection{Problem formulation}\label{sec:problem_definition}
We define an audio mixture $A$ as a combination of multiple audio sources $A = \{a_1, \cdot\cdot\cdot, a_N \}$, where $N$ is the number of audio source components in the mixture. We assume that each potential audio source is emitted by an object, visible or otherwise. Consequently, we can formalize the audio mixture mathematically as $A = \sum_{i=1}^N a_i$, where $a_i$ is the linear audio waveform of the $i$-th source component. This allows us to also consider the presence of ambient sounds by treating them as originating from an unseen object in the video.

Consistent with state-of-the-art audio-visual separation approaches [], we adopt the 'mix-and-separate' self-supervised training strategy. Specifically, we select a pair of videos $V_1$ and $V_2$ and combine their corresponding audio components $A_1$ and $A_2$ to compute a mixed audio waveform $A = A_1 + A_2$. We apply Short-time Fourier transform (STFT) on the mixed waveform to compute its magnitude spectrogram $A^S$ and phase $A^{\text{phase}}$, which encodes how the magnitude of the waveform frequency change over time. Our learning algorithm uses only the magnitude spectrogram $A^S \in \mathbb{R} ^ {FxT}$, where F and T are the number of frequency bins and STFT frames, respectively.

Given the input audio spectrogram and any combination of natural language and video queries, the objective is to compute a real-valued ratio spectrogram mask $\hat{M}$ for the input mixed audio spectrogram that corresponds to the query. The predicted mask is applied to the mixed audio spectrogram using the Hadamard product to compute the final predicted spectrogram. Finally, we can apply inverse STFT on the predicted magnitude spectrogram with its phase value to reconstruct the predicted audio waveform. 

\subsection{\modelname}\label{sec:model_arch}
Our Video-Audio-Text Separation (VATS) model comprises an audio model as well as visual and language encoders. We use the pretrained CLIP model as our visual and language encoders to exploit its learnt visual-semantic alignment. Given a natural language query $L \in \mathbb{R}^{N_w}$ where $N_w$ is the number of words, we extract its representation $f_L \in \mathbb{R}^{1 \text{x} D}$ using the text encoder: $f^L = g_\phi^L(L)$, where $D$ is the dimension of CLIP joint visual-semantic space and $g_\phi^L(L)$ denotes its language encoding function that is parameterized by $\phi$. For a video consisting of $T_v$ frames $V = \{V_1, \cdot\cdot\cdot, V_{T_v} \}$, we extract a spatial grid of region representations $f^V \in \mathbb{R}^{T_v x H' x W' x D}$, where $H'$ and  $W'$ denote the downsampled height and width of the input video clip, respectively. We encode each frame separately:
\begin{equation}
    f^V_{t} = g_\theta^V(V_t), 
\end{equation}
where $g^V_\theta$ is CLIP visual encoding function that is parameterized by $\theta$.  We note that the language queries are only used during inference. 

Additionally, we adopt the U-Net \cite{ronneberger2015u} model, which has been demonstrated by prior work \cite{zhao2018sound} to be effective at audio source separation. The audio U-Net model comprises an encoder and decoder components. Given an input audio spectrogram $A^S$, the encoder component $\mathcal{E}$ passes it through a series of downsampling convolutional layers to output a set of bottleneck features $f^A \in \mathbb{R}^{H^A \text{x} W^A \text{x} D}$, where $H^A$ and $W^A$ are the height and width of the downsampled spectrogram, respectively. This operation is identical for language queries. We enforce that the audio U-Net encoder learns a mapping of its representations directly into CLIP visual-semantic embedding space so that it will be able to use both visual and language queries for audio separation. Given a visual representation $f^V$ as query , we tile the visual representation by the factor $H^A W^A$ and concatenate them with the audio bottleneck representations along the channel dimension: 
\begin{equation} \label{eq:tile_concat_op}
    f^{AV} = concat(f^A, tile(f^V)),
\end{equation}
where $f^{AV}$ has the dimensions $\mathbb{R}^{H^A \text{x} W^A \text{x} D}$. Finally, we pass the concatenated representations into the decoder component $\mathcal{D}$ consisting of a series of upsampling convolutional layers to generate a real-valued ratio mask $\hat{M} \in \mathbb{R}^{F \text{x} T}$ that can be multiplied with the input spectrogram via elementwise product. This operation is formulated mathematically as: $\hat{M} = \mathcal{D}(f^{AV})$.

In the mix-and-separate learning strategy, we generate pseudo-target ratio masks for training supervision. Given a pair of ground-truth audio spectrograms $A_1^S$ and $A_2^S$, we compute their ratio masks as follows:
\begin{equation}
    M_1 = \frac{A_1^S}{A_1^S + A_2^S} \quad\text{and}\quad 
    M_2 = \frac{A_2^S}{A_1^S + A_2^S}
\end{equation} 
We adopt the mask prediction loss \cite{zhao2018sound} to train the audio U-Net model for audio separation. Given the pair of predicted masks $\hat{M_1}$ and $\hat{M_2}$, we compute the mask prediction loss as:
\begin{equation}
    \mathcal{L}_{mask} = || \hat{M}_1 - M_1 ||_1 + || \hat{M}_2 - M_2 ||_1
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/sample_clip_attn_1.pdf}
    \caption{\textbf{CLIP text-visual attention heatmaps for different objects / instruments.}}
    \label{fig:motiv}
\end{figure}

\noindent \textbf{Multiple Instance Learning for video region mask prediction. } We explore two different granularity of audio source separation - \emph{video-level} and \emph{region-level}. In \emph{video-level} separation, we can compute a video-level representation by performing the maxpooling operation over the spatiotemporal grid of region features as follows: $f^V_{\text{video}} = \text{maxpool}\limits_{r \in T_v H'W'}(f^V_r)$. This can be fused with the audio bottleneck representations in Equation~\ref{eq:tile_concat_op}. 

While video-level source separation works well for videos with single object, prior work [] has demonstrated that object-level representations result in significantly improved performance in source separation for videos with multiple objects. In the absence of object detectors, we adopt a Multiple Instance Learning (MIL) framework, where all visual regions are treated as positive candidates. Specifically, given a spatiotemporal grid of region representations extracted from the visual encoder as defined in Section~\ref{sec:model_arch},  we predict a spectrogram mask $\hat{M}_{t,r}$ for the $r$-th region in the $t$-th frame. Finally, we compute an aggregated spectrogram mask for the entire video:
\begin{equation}
    \hat{M} = \frac{1}{T_v}\sum\limits_{t=1}^{T_v}\sum\limits_{r=1}^{H'W'}\hat{M}_{t,r}.
\end{equation}
Intuitively, this formulation encourages the model to learn to identify the salient regions with sounding objects for audio separation. We only enforce this constraint within frames and average the predicted masks over temporal dimension due to our reasoning that the same sounding object should have similar prediction masks across frames.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/cyclic_text_loss.pdf}
    \caption{\textbf{Cyclic loss with CLIP text representations in audio-visual source sound separation.}}
    \label{fig:motiv}
\end{figure}

While using videos as an intermediary modality sounds like a straightforward way to learn a \emph{transitive} alignment between the audio and natural language modalities, our initial experiments demonstrate SOTA audio-visual separation approaches do not generalize well to language queries despite using CLIP visual representations. Consequently, we introduce the Kullbackâ€“Leibler (KL) divergence $\mathcal{L}_{KL}$ and cyclic consistency $\mathcal{L}_{cons}$ losses to facilitate the audio U-Net learning audio representations that are well-aligned to both visual and natural language modalities, which we describe in the following section. The final objective function is computed as the sum of all three loss components:
\begin{equation*}
    \mathcal{L}_{final} = \lambda_1 \mathcal{L}_{mask} + \lambda_2 \mathcal{L}_{KL} + \lambda_3 \mathcal{L}_{cons}
\end{equation*}
where $\lambda_1$, $\lambda_2$ and $\lambda_3$ denote the weights for the mask prediction, KL divergence and cyclic consistency loss components, respectively.

\subsection{Aligning audio representations with visual and natural language concepts}\label{sec:alignment_losses}
As mentioned above, using CLIP visual representations in SOTA audio-visual separation approaches does not help them learn to separate audio sources with language queries. This suggests that simply learning an alignment between the audio and visual modalities does not preserve the \emph{transitive} relationship between the latter and natural language. More interestingly, these approaches also perform significantly worse with CLIP visual representations as compared to those of ImageNet-pretrained visual encoders []. We hypothesize that this is due to information about written words and their visual concepts being disentangled in CLIP visual embedding space \cite{Materzynska_2022_CVPR}. 

Materzynska \etal \cite{Materzynska_2022_CVPR} show that it is possible to disentangle the visual concepts by learning linear projection functions with supervision from the language representations of the object class labels. In the absence of object labels in the unlabeled videos, we adapt the pretrained CLIP model to extract a latent caption $V*$ for each video $V$ (Figure~\ref{fig:latent_extraction}). We begin by encoding the center frame of a video $V_{center}$ as: $f^V_{center} = g_\theta^V(V_{center})$. Then, we replace the token embeddings used for language queries with a learnable parameter $p$ and encode it using the language encoder: $V* = g_\phi^L(p)$. We optimize the weights of the learnable parameter $p$ by maximizing the cosine similarity between $f^V_{center}$ and $V*$:
\begin{equation}
    \max_{p} sim(\frac{f^V_{center}}{||f^V_{center}||_2}, \frac{V*}{||V*||_2}).
\end{equation}
Intuitively, a latent caption should embody the visual concepts present in its corresponding video. These latent captions are used for pseudo-text supervision in place of ground-truth object labels. We begin by extracting the audio representation of a predicted spectrogram $\hat{A}$ in Section~\ref{sec:model_arch}: $\hat{f}^A = \mathcal{E}(\hat{A})$ and averaging over the spatial regions.

\subsubsection{Attention KL divergence loss}\label{sec:kl_loss}
To learn a transitive alignment between the learnt audio and CLIP text representations via the intermediary visual modality, we enforce that the learnt audio representation of the predicted spectrogram is grounded correctly in the relevant spatiotemporal regions. Prior work [] have demonstrated that the CLIP model can be engineered to extract a segmentation map. Inspired by this, we use the latent concepts to provide pseudo-target attention maps. Specifically, for a given video $V$, we compute a probability distribution of similarity scores over the set of spatial region features with respect to its latent concept $V^*$:
\begin{equation}
    P_{Att}(VL) = softmax(f^V V^{*T})
\end{equation}
Next, we compute a similar distribution over the region features with respect to the encoded audio representation of the masked spectrogram:
\begin{equation}
    P_{Att}(AV) = softmax(f^V \hat{f}^A^T)
\end{equation}
We encourage the predicted audio representation to learn a similar mapping to that of the latent concept by minimizing the Kullback-Leibler (KL) divergence between the two attention distributions:
\begin{equation}
    \mathcal{L}_{KL}(P_{Att}(VL) || P_{Att}(AV)) = \mathbb{E}_{V \sim P_{AV}}[\log \frac{P_{Att}(VL)}{P_{Att}(AV)}]
\end{equation}

\subsubsection{Latent caption consistency loss}\label{sec:cons_loss}

Additionally, we further improve the alignment between the predicted audio representations and CLIP visual-semantic embedding space by encouraging them to be consistent with their latent captions. Intuitively, the audio sources that are well-separated by the visual cues should have a strong semantic relevance to the latent captions that describe the visual cues.
In lieu of the object labels, we maximize the similarity between the predicted audio representations and the latent concepts. Mathematically, we can formulate this as:
\begin{equation*}
    \mathcal{L}_{cons} =
-\sum_{i=1}^{n} \log (\frac{\exp(\hat{f}^A \cdot V^*)}{\hat{f}^A  \cdot V^* + \sum_{V^{*'}} \exp(\hat{f}^A  \cdot V^{*'})} )
\end{equation*}
where $V^{*'}$ is a negative latent caption that does not correspond to the predicted audio.
In theory, this is similar to the self-supervised multimodal learning objective of maximizing the similarity between the representations of an image and its corresponding caption as well as the dissimilarity of non-corresponding captions. We hypothesize that using a low weight for this loss component is likely to be more beneficial since some videos may contain similar sounding objects. In that case, maximizing the dissimilarity between the predicted audio and latent captions with similar sounding objects may make the effect of false negatives more pronounced. 