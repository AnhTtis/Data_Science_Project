

\begin{figure}
    \centering
    \includegraphics[width=0.47\textwidth]{Figures/motivational_figure_v13.pdf}
    \caption{We propose to separate and localize audio sources based on a natural language query, by learning to align the modalities on completely unlabeled videos.
    %alone or in combination with an input video. 
    In comparison, prior audio-visual sound separation approaches require object label supervision.}
    \label{fig:latent_motiv}
    \vspace{-10pt}
\end{figure}

\section{Introduction}
Our everyday audiovisual world is composed of many visible sound sources, often with multiple sources layering on top of one another. For example, consider the video of the guitar and cello musicians playing together in Fig.~\ref{fig:latent_motiv}. The two instruments have distinct timbres, and the musicians play non-unison, but complementary melodies. Despite hearing both instruments simultaneously, humans have an innate ability to 
identify and isolate the melody of a single source object.
%Automating this task is hard and requires learning to associate 
%audio inputs to the visual and natural language modalities. 
%is highly desirable for audio separation since information in our world often comes in different modalities. 
In this paper, we define the corresponding machine task as follows: given a natural language query that selects a sounding object, such as ``person playing a guitar'', separate its sound source from the input audio waveform and localize it in the input video, without any supervision. 
%Given a video with corresponding audio and a description of the target sound source, the task is to output the isolated sounds of that source and localize it in the video. 

% The majority of multimodal audio separation approaches \cite{gao2019co,chatterjee2021visual}  
% %address this task with context from either natural language or videos separately. More importantly, they often 
% rely on strong supervision to train object detectors for audio-visual separation, or crowd-annotated captions for audio-text separation (Figure~\ref{fig:latent_motiv} top). 

% TODO explain the self-supervised `mix-and-separate' strategy where multiple audio sources are combined into a synthetic mixture and the goal is to predict a spectrogram mask to retrieve the component that correspond to the ground-truth source.

This task is challenging. First, there is no approach for associating the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform without access to annotations during training. Existing audio-visual methods \cite{zhao2018sound,gao2019co,chatterjee2021visual} do not generalize to natural language queries due to their dependence on discrete object class labels. Second, an ideal solution would jointly identify and localize sound-emitting objects in videos as well as separate the corresponding components in the audio waveform without strong supervision. Although prior audio-visual work has demonstrated the benefits of aligning relevant object regions in the video with their corresponding sounds  \cite{gao2019co,chatterjee2021visual}, these approaches require strong supervision including object label and bounding box annotations (see Fig.~\ref{fig:latent_motiv} top). Overcoming these challenges would enable important downstream applications including holistic video understanding \cite{seo2022end}, embodied AI \cite{chen2019audio}, and bidirectional audio-to-video retrieval \cite{wu2022wav2clip}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To address these challenges, we make the following contributions. First, we propose \modelname (\modelabb), a self-supervised approach that leverages large vision-language ``foundation'' models~\cite{radford2021learning,jia2021scaling} to provide pseudo-supervision for learning the alignment between the three modalities: audio, video and natural language. 
Our key insight is to learn a strong \emph{transitive} relation from audio to natural language using vision as an intermediary modality, while preserving the alignment between the visual and natural language modalities embodied by the foundation models.  However, just using the visual representations of these foundation models in existing AV separation approaches does not preserve the transitive relationships between the three modalities (Sec.~\ref{sec:results}).

Our second contribution introduces two novel multimodal alignment objectives that encourage the learnt audio representations to encode the semantics of captions and infer the latent transitive relation between the three modalities.  While natural language can express a large and varied range of visual concepts for audio separation in videos, the absence of captions in unlabeled videos during training poses a significant challenge in our self-supervised formulation. To learn the transitive alignment, we adapt a foundation model to extract \textit{latent captions} from unlabeled videos.
Intuitively, the latent captions are representations that express the visual concepts present in the videos.  Third, we introduce a Multiple Instance Learning formulation to learn to perform audio separation at the video \emph{region} level since we do not have prior information on relevant objects or their locations in the videos during training.

Finally, we demonstrate the effectiveness of our proposed \modelabb approach through extensive evaluations on the audio source separation task on the SOLOS~\cite{montesinos2020solos}, MUSIC~\cite{zhao2018sound}, and AudioSet~\cite{gemmeke2017audio} datasets. We show that our self-supervised approach outperforms strongly-supervised state-of-the-art approaches without using labels during training by leveraging the capability of vision-language foundation models. More importantly, we demonstrate that \modelabb learns to use language queries for audio separation despite not training with ground-truth language supervision.

%Without training on labels, our self-supervised approach outperforms strongly-supervised state-of-the art-approaches across multiple datasets. More significantly, we demonstrate that our trained model can also separate audio sources with natural language queries despite not having captions for training supervision. In summary, our contributions are as follows:
%\begin{enumerate}[nosep,leftmargin=*]
%    \item We propose \modelname, the first fully self-supervised approach to learn to separate audio source components with video and/or natural language queries from unlabeled videos.
%    \item We introduce two novel loss functions that leverage latent captions to learn a strong transitive alignment between the audio, text and video representations. 
%    \item We show that our approach learns to use natural language queries for audio separation despite not training with human-annotated captions.
%    \item We conduct extensive audio source separation experiments across 3 different datasets including SOLOS, MUSIC, and AudioSet, where we outperform state-of-the-art strongly-supervised approaches by leveraging the capability of vision-language foundation models.
%\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%We propose \modelname (AVSeT), a self-supervised approach that leverages large vision-language ``foundation'' models~\cite{radford2021learning,jia2021scaling} to provide pseudo-supervision for learning the alignment between the three modalities: audio, video and natural language. Our key insight is to preserve the strong alignment between the visual and language modalities embodied by the foundation models, such that we can learn audio representations that are not only well-aligned with the visual concepts in the videos but also use vision as an intermediary modality to learn a strong \emph{transitive} relation from audio to language.  

%Despite the simplicity of this insight, just using the visual representations of these foundation models in existing audio-visual separation approaches does not preserve the transitive relationships between the three modalities, as we will demonstrate empirically later on. Consequently, we propose \modelname, the first fully self-supervised approach to learn to separate audio source components with video and/or natural language queries from unlabeled videos. While natural language provides the generality to express a large and varied range of visual concepts for audio separation in videos, the absence of captions in unlabeled videos during training poses a significant challenge in our self-supervised formulation. To infer this transitive alignment, we use the foundation model to extract \textit{latent captions} from unlabeled videos. Intuitively, the latent captions are representations that should express the visual concepts present in the videos.  

%As our second contribution, we further introduce two novel alignment objectives that encourage the learnt audio representations to encode the semantics of the latent captions and learn the transitive relation between the three modalities by doing so. Since we do not have prior information on relevant objects or their locations in the videos during training, we propose a Multiple Instance Learning formulation to learn to perform audio separation at the video \emph{region} level. Last but not least, we conduct extensive audio separation experiments across 3 different datasets including SOLOS, MUSIC, and AudioSet, where we outperform state-of-the-art strongly-supervised approaches by leveraging the capability of vision-language foundation models. Our experiments demonstrate that our trained model can separate audio sources based on natural language queries despite training without any text supervision.

%Without training on labels, our self-supervised approach outperforms strongly-supervised state-of-the art-approaches across multiple datasets. More significantly, we demonstrate that our trained model can also separate audio sources with natural language queries despite not having captions for training supervision. In summary, our contributions are as follows:
%\begin{enumerate}[nosep,leftmargin=*]
%    \item We propose \modelname, the first fully self-supervised approach to learn to separate audio source components with video and/or natural language queries from unlabeled videos.
%    \item We introduce two novel loss functions that leverage latent captions to learn a strong transitive alignment between the audio, text and video representations. 
%    \item We demonstrate that our approach can use natural language queries despite not training with human-annotated captions.
%    \item 
%\end{enumerate}