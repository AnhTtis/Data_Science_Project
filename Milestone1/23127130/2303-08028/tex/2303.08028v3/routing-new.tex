\section{Lazy Data Routing}\label{sec:lazy}
The message broker system consists of a leader that orchestrates the entire message flow and multiple producers/consumers as message endpoints.
Data streams as producers publish data to the leader, and models as consumers consume data from the leader.
With this architecture, the leader can quickly become a point of contention since it has to process all the messages from/to all the different nodes.
Furthermore, large message payloads (e.g., images) can lead to a crucial networking bottleneck at the leader, as message broker systems are not designed to handle large messages.

\sys uses a novel messaging protocol to efficiently transfer data between nodes without placing an undue burden on the leader.
A message sent to the leader only contains message headers: a timestamp and a global source path.
The actual message payloads are not transferred; instead, they are kept and indexed on the node that collected the data.
A model subscribes to the topic and reads the headers as they come in.
If it wants a particular data payload, it retrieves that data lazily from the source node.

Figure~\ref{fig:lazy} illustrates this protocol. When collecting data, every data item added to a \texttt{DataStream} is annotated with a header (Figure \ref{fig:lazy}-1). We can think of this as a stream of $(h,d)$ tuples (header and data, respectively). After the tuple is created, the node locally writes the data to a time-indexed log (Figure \ref{fig:lazy}-2).
After this data is durably written, the header is published to the message broker on the leader (Figure \ref{fig:lazy}-3).
Nodes on the network can subscribe to streams of these headers.
Model inference requires the data payload, and that can be requested from the headers (Figure \ref{fig:lazy}-4). This data is transferred in a peer-to-peer fashion, and inferences happen over these streams (Figure \ref{fig:lazy}-5).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/lazy.png}
    \caption{A figure illustrating the order of operations in the lazy data routing system used by \sys.}
    \label{fig:lazy}
\end{figure}

Lazy retrieval has a number of essential benefits for typical model-serving tasks. In general, these benefits are analogous to that of lazy computation.
First, many models predict at rates much slower than the rate of data collection. For example, a model that takes 30ms to evaluate can only process one example every 30ms. If the data collection rate is significantly faster than that, the model often has to downsample the input data. Lazy data routing allows us to avoid transferring the data payload to the leader in these cases. 
Next, this strategy reduces the size of the messages processed by the message broker reducing overheads in checkpointing and serialization/de-serialization.
As a result, we also find that it can enable increased parallelism as well.
Both of these benefits can be tied back to the traits of the machine learning setting mentioned above. (See \S\ref{sec:exp-lazy} for relevant experiments)

In certain cases, we allow users to force \sys to have eager message passing. Small messages, such as 1D arrays, can be transferred from data source nodes to worker nodes via the leader node. Essentially, this embeds the payload in the message headers.
In some networks, peer-to-peer communication is not available or not efficient.
We can default to eager message passing when needed to support these cases.

\iffalse
\subsection{Other Considerations}
Practically, every edge node has a limited amount of local storage.
% \sys handles this by having a timeout for the data payloads, where a node on the network can only send a retrieval request within that timeout.
\sys handles this by offloading older data to the cloud when running out of space.
The headers will also automatically redirect to the cloud, so users still have access to their lazily routed data as if it was still on the local node.
This allows the edge node to free up that space periodically.

\subsection{Example: Multimodal Serving}
\todo{I feel this is actually wrong after I read it again. I don't need to send things repeatedly as long as I cache locally, even in eager data routing.}
Lazy data routing can be extremely useful when multiple data streams have different arrival frequencies but predictions are expected at the higher frequency.
Consider an example where an image stream A at 5 frames per second is combined with a sensor stream B at 20 Hz to issue real-time predictions. Each image is 1MB and each sensor reading is 1KB in size.
Data-triggered joins are used, where a prediction is issued whenever a new sensor reading is received.
With lazy data routing, the image stream A at lower frequency can be reused when combined with sensor readings to save communication.
During any second, only one copy of each frame from stream A is transferred from the data source to the model, resulting in 5 MB/s with lazy data routing.
On the other hand, eager data routing would transfer 4 copies of each frame from the joiner to the model, causing 20 MB/s bandwidth.
\fi