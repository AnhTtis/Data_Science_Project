\section{Experiments}\label{sec:exp}

\iffalse
To summarize our experiments:

\begin{itemize}
    \item Data-triggered joins lead to faster reaction time and queueing time in real-time decision-making than time-triggered joins (\S\ref{sec:exp-joins}, \S\ref{sec:exp-nuscenes}).
    \item Lazy data routing significantly reduces communication time when data payloads are large (\S\ref{sec:exp-lazy-tradeoff}). It naturally supports parallelism and maintains its performance even in the presence of network contention (\S\ref{sec:exp-lazy-parallelism}, \S\ref{sec:exp-congestion}). Lazy data routing also benefits from reduced latency when certain data is skipped, as the rate of data arrival exceeds model throughput (\S\ref{sec:exp-lazy-data-skipping}).
    \item In real-world settings, decentralized model placement with effective data skipping gives us higher real-time accuracy and lower backlog latency (\S\ref{sec:exp-opportunity}).
    % \item Effective downsampling ensures the timeliness of incoming data, and thus significantly improves real-time accuracy while preserving near-zero backlog. (\S\ref{sec:exp-opportunity})
    \item \sys's lightweight queuing system offers more flexible data routing, supports more network topologies, and has much lower system overhead than Ray Serve (\S\ref{sec:exp-system-overhead}, \S\ref{sec:exp-network}).
\end{itemize}
\fi


\subsection{Experimental Setup}\label{sec:exp-setup}
All of our experiments are performed on a private ``edge cluster''.
Our hardware setup consists of 5 NVIDIA Jetson Nano Developer Kits, 4 Intel Skylake NUC computers, and a desktop PC. Each NUC is equipped with an Intel Core i3-6100U CPU, 16 GB RAM, and M.2 SSD.
The desktop PC features an Intel Xeon CPU E5-2603 v4 CPU, NVIDIA Quadro P6000 GPU, 64 GB RAM, and HDD.
Direct peer-to-peer connection is available between all nodes via 1Gbps Ethernet.
Throughout these experiments, we vary the network topology to test various scenarios.
In some experiments, only partial nodes are used.
These variations will be explained in respective sections, but one NUC is always used as the leader node.
% Generally, NUCs are used as the leader node and 3 data source nodes; Jetson Nanos are used as 1 other data source node and all prediction nodes.


As a primary baseline, we have configured PyTorch distributed~\cite{pytorch-distributed} on our edge cluster, with Gloo as the distributed communication backend.
% We run PyTorch in both a centralized mode (traditional model serving) as well as a decentralized mode when possible.
We have also implemented an eager data routing architecture similar to ROS~\cite{quigley2009ros} within our framework to understand the key design decisions.
ROS is widely used in the sensor and robotics communities and provides a centralized message broker service.
However, ROS does not support lazy data routing, distributed stream synchronization, and adaptive rate control.
Additionally, we implemented a time-triggered join strategy similar to Apache Flink~\cite{flink}.
% We excluded Tensorflow from this evaluation because we found that TensorFlow distributed did not offer fine-grained control over communication needed for fair experiments.
We also set up a local NTP server to make sure all nodes share a global wall clock time.

We borrow evaluation metrics from the streaming literature and a detailed description of these metrics is in Appendix~\ref{sec:exp-metrics}.


\subsection{Application: Human Activity Recognition}\label{sec:exp-opportunity}
\noindent \textbf{Description. } We use the Opportunity dataset for human activity recognition~\cite{opportunity-dataset,opportunity-challenge} as an example.
Data from multiple motion sensors were collected about every 33ms while users were executing typical daily activities. For each subject, there are five activity of daily living (ADL) runs, and each run lasts 15-30 minutes.
We take the first subject's first four ADL runs as the training set and the last ADL run as the test set. When played at 2x speed, the last ADL run takes 8 minutes and 22 seconds.
We partition the first 134 columns vertically into four disjoint subsets, each placed on one of four nodes (3 NUCs and 1 Jetson Nano) as data sources.
The subsets are distributed as follows: columns 1-37 (accelerometers), 38-76 (IMU back and right arm), 77-102 (IMU left arm), and 103-134 (IMU shoes).
We train an aggregated random forest model with scikit-learn~\cite{scikit-learn} for all 134 features as an early fusion baseline, and also four separate random forest models for each subset of features to evaluate an ensemble-based late fusion method.
We primarily evaluate \sys with the late fusion deployment: one RF model at each data source node and we ensemble local predictions at another node.
This simulates a scenario where there is a small amount of compute on each wearable sensor and that compute is used to reduce the data communicated to make a global prediction across those sensors.
Due to space limits, we defer the early vs. late fusion discussion to Appendix~\ref{sec:appendix-late-fusion}.

In our best-effort PyTorch implementation, we use the \texttt{gather()} API to aggregate data from multiple data source nodes.
PyTorch distributed requires all tensors to be the same size to be gathered, so we have to pad each local tensor to the maximum size with zeros.
% Since there is no message queue in PyTorch, it is not possible to compare the parallel (topology 2) strategy between \sys and PyTorch. However, we can still compare centralized and decentralized strategies across these two frameworks. 
The individual streams are fast enough that misalignments can occur due to queueing delays.
However, PyTorch enforces that multiple data sources are always perfectly synchronized, as it does not begin the actual computation until data from all data sources has been gathered, and it only gathers new data after finishing previous predictions.
Such strict requirement does not exist in \sys, as we are able to set a reasonable skew (\S\ref{sec:message-skew}) in \sys.



\vspace{0.25em} \noindent \textbf{Queueing in \sys is better suited for real-time applications. }
\begin{figure}[t]
    \centering
    \begin{minipage}{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp-opportunity-latency-only-decentralized.pdf}
        \caption{Measure of backlog in the activity recognition task. More frequent predictions are on the left side.}
        \label{fig:exp-opportunity-latency}
    \end{minipage}\hfill
    \begin{minipage}{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp-opportunity-stdev.pdf}
        \caption{Standard deviation of actual prediction frequency, where \sys maintains a lower variability.}
        \label{fig:exp-opportunity-stdev}
    \end{minipage}
\end{figure}
First, we evaluate the ability of the system to even issue real-time predictions by measuring the \textit{backlog} in the system, or the accumulated queuing time as defined in Appendix~\ref{sec:exp-metrics-backlog}.
Unlike PyTorch, \sys deployments have a prediction frequency target and can use this target to automatically downsample data to meet real-time requirements.
We illustrate the improvements in Figure~\ref{fig:exp-opportunity-latency}. The x-axis is the target prediction frequency (\S\ref{sec:target-pred-freq}) designated by the end-user, where a larger number means a lower frequency; the y-axis is the \textit{backlog} for each of the serving systems over this dataset.
The compute part of the task itself takes about 23ms to complete, and a near-zero number in backlog means the inference is processed in real time.
% When target prediction frequency $\geq 27$ms per prediction, \sys has near-zero backlog.
\sys offers a no-backlog queue for a wider range of prediction frequency targets ($\geq 27$ms/pred).
However, a long queue of unprocessed examples is quickly developed without proper rate control (e.g. when target prediction frequency $\leq 26$ms per prediction).
Since PyTorch lacks a message queue and rate control, it has to process each example individually and trigger joins in a strictly synchronous manner, leading to an unsatisfactory backlog.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp-opportunity-accuracy-only-decentralized.pdf}
        \caption{Overall real-time accuracy for human activity recognition task measured in F-1 score. }
        \label{fig:exp-opportunity-accuracy}
    \end{minipage}\hfill
    \begin{minipage}{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/nuscenes-e2e-latency-cdf.pdf}
        \caption{CDF of end-to-end latency for eager data routing vs. lazy data routing.}
        \label{fig:exp-nuscenes-e2e-latency}
    \end{minipage}
\end{figure}

Even if PyTorch could meet real-time prediction targets, we find that the variability in prediction latencies is quite high.
In Figure~\ref{fig:exp-opportunity-stdev}, we see a much higher variability in actual prediction frequencies for PyTorch than \sys across all user-defined rates.
This is because PyTorch communicates in a synchronous fashion, and has to account for the variability of all 4 nodes making local predictions with local data streams.

\vspace{0.25em} \noindent \textbf{Queueing Delays Reduce ``Real-Time'' Accuracy. }
In real-time serving scenarios, the timeliness of predictions becomes a key concern. For latency-sensitive tasks, a delayed prediction equates to an incorrect one. To evaluate the timeliness of predictions, we introduce \textit{real-time accuracy} as a measure, which evaluates the accuracy of predictions against the most recent label at the time of prediction. For instance, if a prediction is made between two consecutive labels at times $t_1$ and $t_2$ ($t_1 \leq t_2$), its accuracy is compared with the label at $t_1$.
Since we assume adjacent examples are likely similar, we expect roughly correct prediction results when the examples arrive slightly late. However, if the examples arrive significantly late, they are likely outdated and yield incorrect predictions.

% define \textit{real-time accuracy} to be the accuracy of predictions compared with the last known label at prediction time.
% For example, suppose a prediction is made temporally between two consecutive labels with timestamps $t_1$ and $t_2$ ($t_1 \leq t_2$). In that case, we compare the prediction result to the ground truth at $t_1$ to calculate the real-time accuracy.


Figure~\ref{fig:exp-opportunity-accuracy} shows the real-time accuracy of \sys and PyTorch under various target prediction frequencies.
PyTorch distributed is not able to issue accurate predictions because data is communicated in a synchronous manner. It is unable to downsample the input stream even if the node is overloaded, making most of its predictions outdated.
In contrast, \sys, at a target prediction frequency of 25ms, experiences a greater backlog compared to PyTorch but achieves superior real-time accuracy. This advantage is primarily due to the experiment setup of 3 NUCs and 1 Jetson Nano for local model inference. The NUCs process the CPU model more efficiently than the Jetson Nano, resulting in a significant portion of the backlog being attributed to the Jetson Nano, as it completes local inference later than the NUCs. This situation leads to a notable message skew. To mitigate this, \sys selectively skips data that exceeds the maximum tolerable skew (\S\ref{sec:message-skew}). This strategy of skipping mostly inaccurate data significantly boosts \sys's real-time accuracy.
Furthermore, when the target prediction frequency is set above 26ms/pred, \sys sees less backlog and achieves even higher real-time accuracy.
This improvement results from \sys's capability to instantly process fresher data that, while not perfectly synchronized, falls within an acceptable time skew.


\subsection{Application: Autonomous Driving}\label{sec:exp-nuscenes}
We use a subset of the nuScenes self-driving dataset for autonomous driving~\cite{nuscenes} consisting of 6 cameras and a lidar sensor.
All cameras generate 10 frames per second and the lidar sensor emits at 2 Hz.
Each camera is connected to a separate NVIDIA Jetson Nano running pre-trained YOLOv5n model~\cite{yolov5} on GPU.
The lidar sensor is connected to a NUC node, which preprocesses the data and then transfers it to our desktop PC equipped with NVIDIA Quadro P6000 GPU running pre-trained CenterPoint model~\cite{yin2021center}.
Communication incurs considerable cost here as preprocessed lidar data is very large.
All predictions are sent to another NUC node, which triggers the join and yields synchronized predictions.
% There is also a separate NUC node acting as the message broker.




\iffalse
% Notes
A naive, centralized solution would be to aggregate all data to a single node and run a large model with all sources of data.
(1) model too large to deploy
(2) still have to time-align between sources
(3) single point of failure

Queueing time (at each node)
End-to-end time
Data-triggered/Time-triggered, Lazy/Eager
https://docs.google.com/spreadsheets/d/1NFnzf8NBrQ-yGY9WqbCMc7UKzbOlh1JVOLIg2MwEJwU/edit#gid=1795317306

Eager vs. Lazy (both data-triggered), end-to-end latency
Eager: Average 37808.74183ms, Median 37691.8551ms, p95 70834.40801ms
Lazy: Average 2102.902788ms, Median 2231.150146ms, p95 2481.405029ms

Time-triggered vs. Data-triggered (both lazy), queueing time

\begin{table}[]
\small
\begin{tabular}{ll|lll}

\end{tabular}
\caption{End-to-end latency for eager vs. lazy data routing.}
\label{tab:exp-nuscenes-e2e-latency}
\end{table}
\fi


\begin{table}[]
\small
\begin{tabular}{ll|lll}
\multicolumn{2}{l|}{\makecell{Queueing time\\ (ms)}} & \makecell{Time-triggered\\ (Flink-like)} & \makecell{Data-triggered\\ (Ours)} & Speedup \\ \hline
\multirow{2}{*}{Lidar}       & Med.    & 463.60         & 16.56          & 28.00x  \\
                             & P95       & 963.20         & 48.35          & 19.92x  \\ \hline
\multirow{2}{*}{Cam 1}    & Med.    & 51.04          & 5.64           & 9.06x   \\
                             & P95       & 126.06         & 13.56          & 9.30x   \\ \hline
\multirow{2}{*}{Cam 2}    & Med.    & 64.05          & 9.59           & 6.68x   \\
                             & P95       & 124.08         & 18.27          & 6.79x   \\ \hline
\multirow{2}{*}{Cam 3}    & Med.    & 56.62          & 9.76           & 5.80x   \\
                             & P95       & 143.45         & 17.84          & 8.04x   \\ \hline
\multirow{2}{*}{Cam 4}    & Med.    & 74.30          & 5.23           & 14.19x  \\
                             & P95       & 117.08         & 13.08          & 8.95x   \\ \hline
\multirow{2}{*}{Cam 5}    & Med.    & 45.32          & 7.63           & 5.94x   \\
                             & P95       & 88.87          & 15.90          & 5.59x   \\ \hline
\multirow{2}{*}{Cam 6}    & Med.    & 57.33          & 5.38           & 10.66x  \\
                             & P95       & 109.61         & 13.20          & 8.31x   \\
\end{tabular}
\caption{Queueing time for time- vs. data-triggered joins.}
\label{tab:exp-nuscenes-joins}
\end{table}

First, we compare the end-to-end latency between eager and lazy data routing, applying data-triggered join in both scenarios.
In the lazy data routing approach, we implemented a freshness threshold SLO (\S\ref{sec:freshness-threshold}) that discards data older than 500ms.
As depicted in Figure~\ref{fig:exp-nuscenes-e2e-latency}, the CDF of end-to-end latency demonstrates that lazy data routing significantly reduces latency by only pulling data with recent timestamps.
This reduction is particularly notable since communication is the primary bottleneck in this task.
Notably, lazy data routing led to the skipping of 72.5\% of predictions that failed to meet our freshness threshold SLO compared to eager data routing.


Second, we compare the queueing time (as defined in~\ref{sec:exp-metrics}) between time-triggered and data-triggered joins, applying lazy data routing in both scenarios.
For time-triggered join, we set the time interval of joins to be every 1 second.
For data-triggered join, we issue a join as soon as a new example that meets our freshness threshold SLO comes in.
Table~\ref{tab:exp-nuscenes-joins} shows the median and 95th percentile of queueing time for each data source.
Data-triggered join reduces the queueing time by up to 28x as it does not have to wait for fixed intervals.



\subsection{Application: Network Intrusion Detection}\label{sec:exp-network}
\sys natively allows multiple producers and multiple consumers to operate on the shared message queue at the same time, which is an essential communication paradigm in decentralized prediction but not currently supported by PyTorch or TensorFlow.
We use a public Network Intrusion Detection dataset from Canadian Institute for Cybersecurity (CIC-IDS2017)~\cite{cic-ids2017} and an existing model~\cite{kostas2018} to differentiate malicious traffic from benign network traffic.
Specifically, we partition the data horizontally into four disjoint subsets by ``Source IP'' for our four data source nodes. The underlying assumption is that network traffic from different source IP addresses may be collected separately.

If a web attack is detected, the related network packet needs to be sent to a specific destination node, but the actual computation can be done anywhere. 
We show that \sys can support three deployment strategies: (Early fusion, topology 1) transfer all data to the prediction node that does all computations in a centralized way; (Early fusion with parallelism, topology 2) transfer all data from data source nodes to an intermediate shared queue, where four prediction nodes can pull data from when they become available, and they need to inform the destination node if an attack is detected; or (Late fusion, topology 3) data source nodes do computations locally and only transfer data to the destination node if an attack is detected.  

\iffalse
\begin{table}[]
\centering
\begin{tabular}{l|l|l|l}
Method   & Nodes & \begin{tabular}[c]{@{}l@{}}PyTorch\\ samples/sec\end{tabular} & \begin{tabular}[c]{@{}l@{}}\sys\\ samples/sec\end{tabular} \\ \hline
a) Centralized           & 4+1+1  & 41.94    & 47.58 (+13.4\%)      \\
b) Parallel              & 4+1+4  & - & 182.57 (3.84x)      \\
c) Decentralized         & 4+1+0 & 181.33 (4.32x)   & 197.30 (+8.8\%, 4.15x)
\end{tabular}
\caption{Throughput for network intrusion detection task. (a)-(c) involve the same task with different model placement methods. The nodes column shows the number of nodes involved, with $x+y+z$ denoting $x$ number of data source nodes, $y$ number of leader nodes, and $z$ number of prediction nodes. Both parallel (b) and decentralized (c) methods utilize four prediction nodes, whereas centralized (a) utilizes only one. Percentages in parentheses are relative to PyTorch distributed; speedups in parentheses are relative to centralized (a).\todo{this experiment is really confusing because we partition the dataset horizontally, with no aggregation required. In the later experiment we partition the dataset vertically, so aggregation is required.}}
\label{tab:exp-network}
\end{table}
\fi

In an early fusion setting, PyTorch distributed is able to process 41.94 examples per second, while \sys can process 47.58 examples per second.
This is the baseline setting of both systems, and the performances of both systems are comparable.
In an early fusion with parallelism setting that is only supported by \sys, thanks to its queuing design, 182.57 examples are processed per second, which is almost a linear (3.84x) speedup compared to a centralized setting given that we now have 4 prediction nodes.
In a late fusion setting, we make all 4 data source nodes also local prediction nodes, and PyTorch achieves 181.33 examples per second while \sys takes 197.30 examples per second. For both systems, superlinear speedup (4.32x and 4.15x compared to centralized, respectively) is achieved by making the most of local computational resources and communicating only local prediction results instead of the entire dataset.
Since we use the same model/sub-models for both \sys and PyTorch without synchronization issues, the accuracies of predictions between both systems are the same.


\subsection{Micro-benchmarks}


\subsubsection{Data-triggered Joins Are More Responsive}\label{sec:exp-joins}
\iffalse
Ted's notes:
Join experiments: time window / data triggered
Bursty data. Suddenly a lot of data coming in and we need immediate reactions. 
Metric: average/min/max/95 percentile reaction time, communication in bytes
Baseline: fixed time window.
1. Transfer the latest raw data at the end of each time window. 
2. Only transfer if thereâ€™s an update (not the same as previous) at the end of each time window. Need state management at local node.

2 streams. One constant every 5 seconds. The other either none or very frequent (10Hz)
Also with lazy data routing, we don't have to resend duplicated lower-frequency stream.

Low frequency stream A: 5MB each file, arrives every 5 seconds. Lazy data routing.
High frequency stream B: 1Byte each file, arrives at 10 Hz.
% But very bursty. Only arrives during the first 5 seconds of the minute.
Ours: Data-triggered join with LDR.
Baseline 1: Data-triggered join without LDR.
Baseline 2: Time-triggered join with LDR. (time window == 1s, only transfer the raw data when an update is detected).
Baseline 3: Time-triggered join without LDR (time window == 1s, transfer the raw data regardless if it's the same as previous data).
Metrics: median/min/max/95 percentile reaction time / end-to-end latency; total communication bytes.
\fi
\begin{table}[t]
\begin{tabular}{l| l|l}
\multicolumn{1}{c|}{\multirow{2}{*}{Join strategy}}
    & \multicolumn{2}{c}{Reaction time} \\
    & Median &P95\\ \hline
Data-triggered & 9.02ms & 10.28ms\\
Time-triggered (time window: 1s) & 0.5s & 0.9s\\
Time-triggered (time window: 5s) & 2.5s & 4.7s\\
\end{tabular}
\caption{Reaction time for data- and time-triggered joins.}
\label{tab:exp-reaction-time}
\end{table}
This micro-benchmark evaluates the responsiveness between time-triggered join and data-triggered join.
We use two NUC nodes as data sources, one NUC node as the message broker, and one more NUC node performing the join.
We show how data-triggered joins significantly reduce reaction time for latency-sensitive applications.
We have a steady stream that arrives every 5 seconds (5 MB each) and a bursty stream that arrives at 10 Hz (1 Byte each) for a minute.
Real-time decision-making requires the latest information from both streams.
% We measure the time between the latest data generation and joint data arrival at another node as \textit{reaction time}.
The \textit{reaction time} (as defined in Appendix \ref{sec:exp-metrics-latency}) for both join strategies during that one minute is shown in Table~\ref{tab:exp-reaction-time}.
Data-triggered join achieves a much better reaction time without the difficulty of setting a reasonable time window.

\subsubsection{Benefits of Lazy Data Routing}\label{sec:exp-lazy}
In \S\ref{sec:lazy}, we described our lazy data routing model as an alternative to a ROS-like system that eagerly transfers raw data through a centralized broker.
Now, we evaluate the pros and cons of the lazy data routing model.
We employ one NUC node as the data source and one NUC node as the receiver in this subsection, except for the parallelism experiment where the number of receiver nodes is varied.

% \subsubsection{Evaluation Metrics}
% In this subsection, we use the following three metrics to measure the performance of data routing:

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/exp-lazy-data-routing/comm_latency_breakdown.pdf}
    \caption{Lazy data routing reduces latency on producer side but has fixed overhead on consumer side. Both axes are log-scaled.}
    \label{fig:exp-comm-latency-breakdown}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/exp-lazy-data-routing/comm_latency_eager_lazy.pdf}
    \caption{Lazy data routing reduces latency on producer side but has fixed overhead on consumer side. Both axes are log-scaled.}
    \label{fig:exp-comm-latency-eager-lazy}
\end{figure}

First, we send a series of messages of different sizes from a data source node to a receiver node, through the leader node. No actual computation is performed. We compare the communication latency between eager and lazy data routing.

\vspace{0.25em} \noindent \textbf{Lazy Data Routing Reduces Latency on Producer but Has Fixed Overhead on Consumer.}\label{sec:exp-lazy-tradeoff}
Since we only need to transfer the headers instead of raw data in our lazy data routing model, the latency on producer side remains a negligible number even if the message is huge.
As shown in Figure~\ref{fig:exp-comm-latency-breakdown}a and~\ref{fig:exp-comm-latency-eager-lazy}a, a ROS-like eager data routing model could result in very high latency when sending large messages, which itself could force subsequent messages to queue up and become outdated when they arrive.
In contrast, our lazy data routing model makes sure that message headers are sent in milliseconds, which never blocks the rest of messages. The consumers may choose to downsample some data and only fetch necessary data.

% \subsubsection{Lazy Data Routing Has Fixed Overhead on Consumer Side}\label{sec:exp-lazy-consumer}
Whenever the consumer needs to fetch raw data, there is a fixed overhead to establish P2P connections even if the actual data is just a few bytes. This fixed overhead can be amortized when the actual data is larger, as depicted in Figure~\ref{fig:exp-comm-latency-breakdown}b and~\ref{fig:exp-comm-latency-eager-lazy}b.

% \subsubsection{Break-Even Point Between Lazy and Eager Data Routing}
In summary, lazy data routing is more performant when the messages transferred are larger in size.
As shown in Figure~\ref{fig:exp-comm-latency-eager-lazy}c, eager data routing actually has a lower total communication latency when the messages are smaller than 512KB in size, and lazy data routing performs better when the messages are larger than 512KB in size.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp-lazy-data-routing/speedup.pdf}
        \caption{Lazy data routing scales out well while eager data routing does not.}
        \label{fig:exp-speedup}
    \end{minipage}\hfill
    \begin{minipage}{0.48\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/exp-lazy-data-routing/skipped.pdf}
        \caption{Lazy data routing saves communication when some data is skipped.}
        \label{fig:exp-lazy-skipped}
    \end{minipage}\hfill
\end{figure}

\vspace{0.25em} \noindent \textbf{Lazy Data Routing Naturally Supports Parallelism.}\label{sec:exp-lazy-parallelism}
It is very common for multiple consumers to fetch data from one or more producers at the same time.
In our lazy data routing model, since messages are transferred in a peer-to-peer fashion, the leader node only has a very light workload to process tiny headers simultaneously, saving precious bandwidth at the leader node.
However, in the eager data routing model, the leader node can be blocked when a piece of large message is going through the leader node from a producer to a consumer. As a result, other producers and/or consumers running in parallel cannot send or receive messages at the same time.
% This is essentially a temporary downtime at the leader node, which could cause the entire system to be delayed.

To compare the scalability of communication between eager and lazy data routing models, we have one producer continuously sending the same 512KB message for a total of 100 times to a shared queue. We gradually increase the number of consumer nodes from 1 to 4 and see how it scales out. While no actual computation is done, we measure the total working duration and use the single-node setup for both the eager and lazy data routing as the 1.0x baseline.
Figure~\ref{fig:exp-speedup} shows how the eager data routing model fails to scale out with more consumer nodes, while our lazy data routing model achieves reasonable speedup. The line shadows represent the lower and upper bounds of repeated experiments.
% In this aspect, our lazy data routing model is also a way to alleviate the single point of failure in case of a centralized message broker system.

\vspace{0.25em} \noindent \textbf{Lazy Data Routing Performs Better with Network Contention.}\label{sec:exp-congestion}
Our lazy data routing model is especially beneficial when the leader node is busy with network requests.
% This experiment shows how valuable this contribution can be with the network topology 1 described in \S\ref{sec:exp-setup}.
We specifically construct a task where the message payload is large: real-time inference over video streams.
% In the previous set of experiments, we largely evaluated \sys in terms of its end-to-end benefits in model serving. In this experiment, we focus specifically on the queuing and messaging system and use ROS as the primary baseline.
In this experiment, two webcams capture the same moving QR code from different positions. 
Both videos are 150 frames long at 1920x1080 resolution.
Each camera is connected to a unique data source node on the network.
For multi-camera tracking, the QR code has to be detected in both streams and corresponded in time-aligned frames from both cameras.
So these two data streams need to be joined at the prediction node.
We simulate a congestion scenario where the network bandwidth at the leader node is limited.
Note that the rest of the network retains its full speed; the only congestion is at the leader node.
% We measure the total working duration of our system versus a ROS-like system that transfers raw data through a centralized broker (without lazy routing). 

Table~\ref{tab:congestion} shows the results.
With no congestion, the system can process roughly 0.8 frames per second in both lazy and eager data routing. With congestion, the story is very different. Our lazy data routing is tolerant, while transferring raw frames in a ROS-style eager communication pattern can be extremely slow when the network is congested. The total working duration increases by a factor of 7 simply due to congestion. Without care, distributed, multi-sensor deployments can easily lose real-time processing capabilities if the broker becomes a point of contention. These experiments illustrate the value of \sys in a controlled scenario, where we can isolate performance differences.

\begin{table}[t]
\centering
\begin{tabular}{l|l|l}
Data routing strategy & Rate limit (up/down) & Time    \\ \hline
Lazy (ours) & No limit                       & 3m 10s  \\
Lazy (ours) & 1 Mbps / 1 Mbps                & 3m 12s  \\
Eager (similar to ROS)       & No limit                       & 3m 16s  \\
Eager (similar to ROS)         & 20 Mbps / 20 Mbps              & 21m 32s
\end{tabular}
\caption{Total working duration with network bandwidth limits.}
\label{tab:congestion}
\end{table}

\vspace{0.25em} \noindent \textbf{Lazy Data Routing Performs Better with Data Skipping.}\label{sec:exp-lazy-data-skipping}
Apart from network congestion, lazy data routing is also valuable when data skipping is employed to ensure the timeliness of prediction results.
We take one of the two 150-frame videos mentioned earlier and transfer these frames from one node to another, via the leader node. Each frame is about 6 MB in its uncompressed form. No actual computation is performed as we are only interested in the communication cost.
% We measure the \textit{total working duration} from when the first frame is sent out until the last frame is delivered.
In Figure~\ref{fig:exp-lazy-skipped}, we illustrate how much communication cost can be saved by lazy data routing.
On the x-axis, we have a variable percentage of frames skipped due to adaptive rate control described in \S\ref{sec:rate-control}; on the y-axis, we measure the total working duration defined in \S\ref{sec:exp-metrics-latency}.
Even when no frames are skipped, our lazy data routing model performs better than the eager data routing model due to the eliminated overhead of transferring a large amount of data through the leader node.
When more frames are skipped, our lazy data routing saves communication time almost linearly to the number of frames skipped by the downstream node.
% That means the overhead of transferring headers through the leader node is negligible.
On the other hand, the ROS-style eager routing pattern spends roughly the same time on communication even if most of the frames are skipped by the downstream model, because it would transfer the entire data payload upfront anyway.















\iffalse
\begin{table}[ht]
\centering
\begin{tabular}{l|l|l}
\begin{tabular}[c]{@{}l@{}}\sys parameter\\ \texttt{predition\_freq}\end{tabular} & \begin{tabular}[c]{@{}l@{}}End-to-end\\ latency\end{tabular}  & \begin{tabular}[c]{@{}l@{}}F-1 score\\ (\# of samples)\end{tabular} \\ \hline
30ms             & 1x (real-time)      & 0.90 (7920, 26\%)      \\
10ms             & 1x (real-time)      & 0.90 (11857, 39\%)     \\
5ms              & 1.84x   & 0.22 (31631, 105\%)     \\
1ms              & 6.98x   & 0.20 (120490, 400\%)    \\
\end{tabular}
\caption{End-to-end latency and overall accuracy with the number of samples for the same human activity recognition task. Latency numbers are relative to the actual data incoming rate, where 1x means real-time processing of streaming data. The number of samples is relative to the PyTorch baseline without downsampling/upsampling. Different \texttt{predition\_freq} parameters are applied to demonstrate the effect of adaptive SLO control.}
\label{tab:exp-rate-matching}
\end{table}
\fi




% Numbers are reported as fractions of real-time. \sys is able to serve real-time predictions in both centralized and decentralized settings. This is a ceiling on performance as it does not need to predict any faster than the data arrival rate. On the other hand, PyTorch is unable to serve real-time predictions for this example (running at about 75\% of real-time). Note that decentralized prediction can still save communication costs to reduce latency for PyTorch distributed. This shows the value of such decentralized architectures.

\iffalse
\begin{table}[]
\centering
\begin{tabular}{l|l|l}
End-to-end latency           & PyTorch  & \sys \\ \hline
Centralized             & 1.46x    & 1x (real-time)      \\
Decentralized        & 1.35x    & 1x (real-time)
\end{tabular}
\caption{End-to-end latency for human activity recognition task, from generating the first piece of data until finishing the entire workload. \sys is able to process incoming data in real time while PyTorch does computation based on stale data. Both involve the same task with different model placement methods.}
\label{tab:exp-opportunity-latency}
\end{table}
\fi

\iffalse
\subsubsection{Compute Utilization}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/exp-opportunity-compute-percentage.pdf}
    \caption{Compute utilization for different model placement strategies and user requested output frequencies. Higher target frequencies are on the left side and lower frequencies are on the right side.}
    \label{fig:exp-opportunity-compute-percentage}
\end{figure}
We have defined how we measure compute utilization in \S\ref{sec:exp-metrics}, and figure~\ref{fig:exp-opportunity-compute-percentage} shows how compute utilization changes with different user-defined output rates.
Unlike most training workloads, decentralized prediction tasks are usually not compute-bound, at least not on all nodes. A certain amount of time is always spent on communication, time-synchronization or aggregation of different data sources, rather than mostly on computation.
In the case of \sys centralized, the prediction node is compute-bound when user-defined output rate is too high, but gradually becomes relaxed when the user asks for a lower output rate.
Decentralized model placements usually have a higher compute utilization because local data source nodes also do actual computations.
Note that higher compute utilization is not necessarily a good thing, because we might be working more than we need to. Remember our goal is to give accurate predictions in real time, and that usually means a certain amount of buffer is required to ensure timeliness. In the case of PyTorch decentralized, it does a lot of computational work but still fails to achieve our requirements for end-to-end latency and real-time accuracy.
\fi


% Now, it is true that this is not a completely fair comparison between PyTorch and \sys, since message misalignments can affect accuracy in \sys.
% However, we find this effect negligible. Figure \ref{fig:exp-opportunity-accuracy} shows the F1-score for \sys and Pytorch in both centralized and decentralized deployments. It is worth noting that there is a subtle issue of ``temporal'' accuracy because PyTorch is slower than real-time.  We ignore the fact that most predictions made with PyTorch distributed are outdated and compare them with the ground truths sequentially anyway. In \sys, however, we do care about the timeliness of predictions and treat outdated results as inaccurate results. Even under this strict definition of accuracy, we find that the communication optimizations in \sys are highly beneficial with a negligible impact on results.

\iffalse
\begin{table}[]
\centering
\begin{tabular}{l|l|l}
F-1 score   & PyTorch  & \sys \\ \hline
Centralized             & 0.90   & 0.90     \\
Decentralized         & 0.91   & 0.91
\end{tabular}
\caption{Overall accuracy for human activity recognition task measured in F-1 score. Despite message misalignments and rate-matching, \sys achieves the same accuracy as a fully-synchronized deployment.}
\label{tab:exp-opportunity-accuracy}
\end{table}
\fi





% We set different \texttt{predition\_freq} parameters for the same task with (a) full transfer method. Table~\ref{tab:exp-rate-matching} shows the end-to-end latency and overall accuracy with the number of samples processed by \sys with different parameter settings. With a higher prediction frequency like 1ms, \sys automatically upsamples incoming data in order to make more frequent predictions. However, since the prediction process takes much longer than 1ms, upsampled data queues up and makes the end-to-end latency even worse. The prediction accuracy also gets worse because the data we use for predictions is outdated. With a lower prediction frequency closer to the actual model throughput, \sys automatically downsamples incoming data to match the given frequency. Since the queue is clear, we always make predictions based on the latest data to achieve better accuracy with lower end-to-end latency, only with a smaller number of samples.

\iffalse
\begin{table}[]
\centering
\begin{tabular}{l|l|l}
\# of samples processed    & PyTorch  & \sys \\ \hline
Centralized             & 30127   & 7920 (26\%)     \\
Decentralized         & 30127   & 11671 (39\%)
\end{tabular}
\caption{Overall number of samples for human activity recognition task. \sys is able to automatically downsample incoming data in order to catch up with data incoming rate.}
\label{tab:exp-opportunity-matching}
\end{table}
\fi

% \subsection{Micro-Benchmarks}
% First, we evaluate key design decisions of \sys as micro-benchmarks. Specifically, we show the benefits of (1) lazy data routing and (2) flexible data routing provided by a message broker.










% \subsection{Message Queue vs. RESTful API}
% First, we compare \sys with state-of-the-art model serving systems with RESTful APIs.


% We evaluate \sys based on two use cases: (1) network intrusion detection and (2) human activity recognition.
% For use case 1, we demonstrate how \sys is able to handle distributed prediction in a traditional workload that maximizes throughput as its goal.
% In this case, we ignore the complexity of data movement and assume the entire dataset is readily available upon request. Experiments show that \sys outperforms PyTorch distributed in such scenarios by 4x with a message broker and lazy data routing.
% For use case 2, we evaluate stream alignment and automatic rate matching offered by \sys. Data continuously comes in real time, and we need to aggregate data from multiple sources in order to make predictions in real time. \sys is able to align multiple streams of data in a time-synchronized way and produce predictions at the rate requested by the user.
