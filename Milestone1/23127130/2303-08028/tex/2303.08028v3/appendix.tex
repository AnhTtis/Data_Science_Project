\section*{Appendix}
\renewcommand{\thesubsection}{\Alph{subsection}}


\begin{figure*}[t]
    \captionsetup[subfigure]{justification=centering}
    \centering
    \begin{subfigure}{0.29\textwidth}
        \includegraphics[width=\columnwidth]{figures/network-topology-1.pdf}
        \caption{Topology 1\\ (Early fusion)}
        \label{fig:network-topology-1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.38\textwidth}
        \includegraphics[width=\columnwidth]{figures/network-topology-2.pdf}
        \caption{Topology 2\\ (Early fusion with parallelism)}
        \label{fig:network-topology-2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.29\textwidth}
        \includegraphics[width=\columnwidth]{figures/network-topology-3.pdf}
        \caption{Topology 3\\ (Late fusion)}
        \label{fig:network-topology-3}
    \end{subfigure}
    \caption{Network topologies used in our experimental edge cluster. Topology 2 takes advantage of multiple prediction nodes consuming a shared message queue at the same time, and is only used in ``Parallel'' experiments.}
    \label{fig:network-topology}
\end{figure*}



\subsection{Model Decomposition}\label{sec:model-decomposition}
Since models are the unit of placement and computation in \sys, the goal of model decomposition is to increase opportunities for optimizing placement. The idea is to approximate a single model with an ensemble or mixture of smaller local models.
Obviously, not all models can be decomposed into smaller parts. However, many real-world models can be partitioned.


\vspace{0.5em} \noindent \textbf{Strategy 1. Ensemble Models}
Ensemble machine learning models are techniques that combine multiple models to improve the accuracy and robustness of predictions. 
Let's imagine that we have $p$ features and $n$ examples with an example matrix $X$ and a label vector $Y$.
Different subsets of these features are constructed on $m$ data sources on the network. Each source generates a partition of features $f_i$, i.e., $X[:, f_i]$ is the source-specific projection of training data. 
Stacking is an ensembling technique where multiple models are trained, and their predictions are used as inputs to a final model. The final model learns to weigh the predictions of each model and make a final prediction based on the weighted inputs. This helps capture the strengths of each individual model and produce a more accurate prediction.

We can train the following models. For each feature subset $f_i$, we train a model (from any model family) that uses only the subset of features to predict the label.
\[
g_i \leftarrow \textsf{train}(X[:, f_i], Y)
\]
After training each of these models over the $m$ subset, we train a stacking model that combines the prediction. This is a learned function of the outputs of each $g_i$ that predicts a single final label:
\[
h \leftarrow \textsf{train}([g_1,...,g_m], Y)
\]
Stacking models are well-studied in literature and are not new~\cite{sagi2018ensemble}. For multi-modal prediction tasks, prior work has found that such models do not sacrifice accuracy and sometimes actually improve accuracy~\cite{shaowang2023amir}. 


\vspace{0.5em} \noindent \textbf{Strategy 2. Mixture of Experts Models}
Similarly, there are neural network architectures that can be trained end-to-end to take advantage of \sys. Mixture of Experts (MoE) is a deep learning architecture that combines multiple models or ``experts' to make predictions on a given task. The basic idea of the MoE architecture is to divide the input space into regions and assign an expert to each region. The gating network takes the input, decides which region it belongs to, and then selects the corresponding expert to make the prediction. The gating network then weights the output of each expert, and the final prediction is the weighted sum of the expert predictions. MoE architectures have been applied to a wide range of tasks, including language modeling, image classification, and speech recognition~\cite{eigen2013learning}.
After training, each expert can be placed independently once trained. 

\iffalse
\noindent \textbf{Related Work: Parallelism in machine learning.}
\todo{maybe remove this paragraph entirely? I think this line of work is largely orthogonal to our system.}
\sys is orthogonal to the work on pipeline-parallelism in model training~\cite{gpipe,narayanan2019pipedream}, as \sys is latency-optimized rather than throughput-optimized.
\cite{alpaserve} investigates statistical multiplexing of multiple devices when serving multiple models.
It is possible to integrate the methods introduced in~\cite{alpaserve} in \sys.
Our experiments show that when PyTorch is deployed in a setting similar to that in pipeline-parallel training, \sys has a significantly lower end-to-end latency.

\noindent \textbf{Related Work: Multimodal models.}
Prior work~\cite{DBLP:conf/icml/NgiamKKNLN11} has shown that multimodal models are able to capture correlations across modalities.
More recent work \cite{wayformer} compares the latency and quality of early fusion (centralized) models, late fusion (decentralized) models and a combination of these two (hierarchical fusion). Late fusion (decentralized) models are able to gain about the same level of quality at much lower latency.
\fi


\subsection{Evaluation Metrics}\label{sec:exp-metrics}
We borrow the following common metrics used in streaming systems~\cite{DBLP:conf/icde/KarimovRKSHM18} to measure the system performance of \sys.

\subsubsection{Types of Latency}\label{sec:exp-metrics-latency}

\noindent \textbf{Producer Sending Latency.} We define \textit{producer sending latency} to be the interval between the time the producer begins sending an example to the leader node and the time the producer finishes sending the same example to the leader node.

\noindent  \textbf{Consumer Receiving Latency.} We define \textit{consumer receiving latency} to be the interval between the time the producer finishes sending an example to the leader node and the time the consumer finishes receiving the same example from the leader node. That means, if there is any queuing backlog at the leader node, it is counted as part of \textit{consumer receiving latency}.

\noindent \textbf{Total Communication Latency.} We define \textit{total communication latency} to be the sum of \textit{producer sending latency} and \textit{consumer receiving latency}. It means the interval between the time the producer begins sending an example to the leader node and the time the consumer finishes receiving the same example from the leader node.
    % \item \textbf{Synchronization Latency.} We define \textit{synchronization latency} to be the interval between the time we have one example from the first available data stream ready and the time we have at least one example from each necessary data stream ready in order to make an aggregation. It is possible to reduce synchronization latency by upsampling infrequent streams.

\noindent \textbf{Reaction Time.} We define \textit{reaction time} to be the interval between the time the producer begins sending the latest example involved in a join and the time at which the joined data tuple arrives at the consumer node. It measures how timely the system reacts to the newest information available.

\noindent  \textbf{Processing Latency.} We define \textit{processing latency} to be the interval between the time a prediction node starts processing an example (or a joined set of examples) and the time it finishes processing the same example. The processing latency is used to measure the actual computation time of an example (or a joined set of examples).

\noindent  \textbf{End-to-end Latency.} We measure the interval between the time an example is collected by \sys and the time the last prediction node finishes processing the same example as \textit{end-to-end latency}. The end-to-end latency includes but not limited to total communication latency and processing latency. 

\noindent \textbf{Queueing Time.} We define \textit{queueing time} to be the interval between the time a node finishes its local inference of an example and the time such local prediction is joined with other local outputs. It measures how long it has to wait for other local predictions to be joined together.

 \noindent \textbf{Total Working Duration.} We define \textit{total working duration} to be the interval between the time the producer begins sending the first example of a task to the leader node and the time the last prediction node finishes processing the last example of the task. This is a task-level measurement rather than a per-example measurement, and it includes both communication time and computation time (if any).



\subsubsection{Backlog}\label{sec:exp-metrics-backlog}
% We will refer to a metric called ``backlog'' measured in time. Intuitively, this metric captures whether a particular deployment can serve a certain rate of data arrival in a timely manner.
We define the \textit{end-to-end latency} of the last example of a task as \textit{backlog}.
Backlog is an important metric because all kinds of delays can easily accumulate, which causes outdated predictions for later examples in a real-time inference scenario.
% Over all these pathways and different data streams, we use the total wall clock time spent on a task, from the first example gets collected until the last example gets predicted, as the proxy of end-to-end latency.
The lower bound of the backlog is near-zero, when there is no delay along the path from the data source to prediction nodes.
Ideally, such lower bound is achievable if data arrive slower than the rate our computational power can serve, or we might have to skip some data points to keep the predictions in time.
% If data were to arrive significantly faster than this metric, we would not be able to serve predictions to every data point.
% Admittedly, this metric does conflate communication and computation.
% But it is the easiest to understand across different use cases.

\iffalse
% Comment out because it's already defined in experiments section
\subsubsection{Real-time Accuracy.}\label{sec:exp-metrics-real-time-accuracy}
When it comes to real-time prediction tasks, the timeliness of predictions becomes a key concern of user experience. For latency-sensitive tasks, late prediction is incorrect prediction. To evaluate the timeliness of predictions, we define \textit{real-time accuracy} to be the accuracy of predictions compared with the last known label at prediction time.
For example, suppose a prediction is made temporally between two consecutive labels with timestamps $t_1$ and $t_2$ ($t_1 \leq t_2$). In that case, we compare the prediction result to the ground truth at $t_1$ to calculate the real-time accuracy.
Since we assume adjacent examples are likely similar, we expect roughly correct prediction results when the examples arrive slightly late. However, if the examples arrive significantly late, they are likely outdated and yield incorrect predictions.
\fi


\subsection{Benefits of Late Fusion Models}\label{sec:appendix-late-fusion}
There has been recent work discussing the latency vs. accuracy tradeoff between early fusion and late fusion models~\cite{wayformer}.
Early fusion models, while potentially capturing more cross-modal correlations, tend to require more communication as they combine raw data from various sources.
In contrast, late fusion models reduce communication by combining locally inferred results rather than raw data.
Late fusion models also naturally support parallelism since most inference is performed locally.
For latency-sensitive tasks, the communication efficiencies offered by late fusion models are extremely valuable.
% The saved communication not only reduces latency, but also improves real-time accuracy due to fresher input data.
We conduct experiments to demonstrate the benefits of late fusion models in \sys.

\subsubsection{Network Topology Setup}
Figure~\ref{fig:network-topology} shows the network topologies of our experiment setup.
Network topology 1 in Figure~\ref{fig:network-topology-1} uses only one prediction node, which is supported by both \sys and PyTorch.
% , and is the default network topology in our micro-benchmarks (\S\ref{sec:exp-lazy}).
Network topology 2 in Figure~\ref{fig:network-topology-2} has 3 additional prediction nodes, and all these 4 prediction nodes consume a shared queue at the same time in parallel experiments thanks to \sys. This cannot be done in PyTorch due to the lack of a shared queue.
Network topology 3 in Figure~\ref{fig:network-topology-3} takes advantage of model decomposition described in \S\ref{sec:model-decomposition} and uses local data source nodes as local prediction nodes, too. The node that was making prediction in topology 1 and 2 now only has to gather local predictions and take a majority vote.
All topologies have 4 data source nodes, from which we use 3 NUCs (in yellow) and 1 Jetson Nano (in black) to reflect the heterogeneity of real-world edge devices. The other NUC is set up as the leader node, or the ``master'' node in terms of PyTorch distributed. The rest of Jetson Nanos are prediction nodes where the actual computation is done, and one of them is designated as the destination node where final results should be sent.


\subsubsection{Late Fusion Models Reduce Backlog}
Figure~\ref{fig:exp-opportunity-latency-full} is an extension to Figure~\ref{fig:exp-opportunity-latency}, showing the backlog for all network topologies.
For both \sys and PyTorch, we see a lower backlog for late fusion models (Topology 3) because we are able to make the most of local data source nodes and save communication costs.
\sys early fusion with parallelism (Topology 2) also helps reduce the backlog over early fusion (Topology 1), when the model is not able to catch up with incoming data rate.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/exp-opportunity-latency.pdf}
    \caption{Measure of backlog in the activity recognition task for all network topologies.}
    \label{fig:exp-opportunity-latency-full}
\end{figure}

% \subsubsection{Late Fusion Models Improve Real-time Accuracy}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/exp-opportunity-accuracy.pdf}
%     \caption{Overall real-time accuracy for human activity recognition task measured in F-1 score. Higher target frequencies are on the left side and lower frequencies are on the right side.}
%     \label{fig:exp-opportunity-accuracy-full}
% \end{figure}


\subsubsection{Late Fusion Models Are More Tolerant To Delays}
\begin{table}[]
\centering
\small
\begin{tabular}{l|l|l}
Real-time accuracy (F-1 score)   & No delay  & 25ms delay \\ \hline
\sys Early Fusion           & 0.90   & 0.55     \\
\sys Early Fusion w/ Parallelism              & 0.90   & 0.55     \\
\sys Late Fusion         & 0.91   & 0.85
\end{tabular}
\caption{Real-time accuracy measured in F-1 score when one of the four data streams has a constant delay. Late fusion models are more accurate even if there is a delay from one data source.}
\label{tab:exp-opportunity-delayed-accuracy}
\end{table}
We next evaluate the fail-soft benefits of \sys, by measuring real-time accuracy of predictions when one of the four data streams have a constant 25ms delay. Table~\ref{tab:exp-opportunity-delayed-accuracy} shows that, while early fusion models (with or without parallelism) suffer from considerable degradation in accuracy due to the delay, late fusion models achieves a much higher accuracy even if there is a constant delay. This is because local predictions from other data streams are unaffected and the ``unpopular vote'' -- local prediction of the delayed data stream is likely dropped by the ensemble method.


\iffalse
\subsection{Queuing Delays}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/exp-queuing-duration-centralized.pdf}
    \caption{Consumer receiving latency per example for \sys centralized. The queue grows significantly faster when the user requests a higher prediction frequency target (smaller number of ms/pred).}
    \label{fig:exp-queuing-duration-centralized}
\end{figure}
Figure~\ref{fig:exp-queuing-duration-centralized} gives us a closer look into the queuing delay.
The x-axis is the sequence number of examples sent and the y-axis is the \textit{consumer receiving latency} of that specific example.
Only the first 40000 examples are shown due to brevity.
\sys centralized is the model placement strategy used in this experiment.
An increasing line means a longer time to wait for later examples to be processed, due to the growing queue.
Figure~\ref{fig:exp-queuing-duration-centralized} corresponds to Figure~\ref{fig:exp-opportunity-latency} that \sys centralized has near-zero backlog when target prediction frequency $\geq 29$ms per prediction.
\fi



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/exp-opportunity-examples.pdf}
    \caption{Number of excess examples processed for different strategies and target prediction frequencies.}
    \label{fig:exp-opportunity-examples}
\end{figure}
\subsubsection{Late Fusion Models Reduce Excess Work.}
Now, we look at the number of excess examples that are processed to better understand how \sys automatically downsamples the incoming data stream in response to the target prediction frequency.
As can be seen from Figure~\ref{fig:exp-opportunity-examples}, PyTorch (either early or late fusion), as a baseline, is marked as zero on the y-axis because it always processes a fixed number of examples equal to the input size.
In early fusion (with or without parallelism) settings, \sys is very sensitive to target prediction frequency because it can downsample incoming data stream when such target is relaxed. Therefore, we see a rapidly decreasing excess work from left to right as the target prediction frequency becomes less frequent.
On the other hand, in a late fusion setting, \sys is not as sensitive to such change in target prediction frequency for the same reason why \sys late fusion maintains a high real-time accuracy discussed in \S\ref{sec:exp-opportunity}. After faster NUCs finish local predictions, the ensemble model simply skips further local predictions made by the Jetson Nano as they fall outside the acceptable time skew range.
As a result, late fusion models only process a small number of examples, even when the target prediction frequency is high enough.

\subsection{Comparison with Federated learning.}
Our work on decentralized prediction might seem similar to federated learning~\cite{DBLP:journals/corr/KonecnyMR15, MLSYS2019_bd686fd6}, but there are several key differences. First, our goal is not to collaboratively train a shared model, but to make combined predictions based on multiple streams of data. Second, we optimize for millisecond-level end-to-end timeliness from the point of data collection to the point where prediction is delivered. Federated learning tasks usually assume a much longer end-to-end latency, and they have other optimization goals, such as communication cost. Third, we have to take care of time-synchronization between data streams, while federated learning systems usually treat those data as the same batch.