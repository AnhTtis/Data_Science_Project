
\iffalse
\subsection{Model Decomposition}\label{sec:model-decomposition}
Since models are the unit of placement and computation in \sys, the goal of model decomposition is to increase opportunities for optimizing placement. The idea is to approximate a single model with an ensemble or mixture of smaller local models.
Obviously, not all models can be decomposed into smaller parts. However, many real-world models can be partitioned.


\vspace{0.5em} \noindent \textbf{Strategy 1. Ensemble Models}
Ensemble machine learning models are techniques that combine multiple models to improve the accuracy and robustness of predictions. 
Let's imagine that we have $p$ features and $n$ examples with an example matrix $X$ and a label vector $Y$.
Different subsets of these features are constructed on $m$ data sources on the network. Each source generates a partition of features $f_i$, i.e., $X[:, f_i]$ is the source-specific projection of training data. 
Stacking is an ensembling technique where multiple models are trained, and their predictions are used as inputs to a final model. The final model learns to weigh the predictions of each model and make a final prediction based on the weighted inputs. This helps capture the strengths of each individual model and produce a more accurate prediction.

We can train the following models. For each feature subset $f_i$, we train a model (from any model family) that uses only the subset of features to predict the label.
\[
g_i \leftarrow \textsf{train}(X[:, f_i], Y)
\]
After training each of these models over the $m$ subset, we train a stacking model that combines the prediction. This is a learned function of the outputs of each $g_i$ that predicts a single final label:
\[
h \leftarrow \textsf{train}([g_1,...,g_m], Y)
\]
Stacking models are well-studied in literature and are not new~\cite{sagi2018ensemble}. For multi-modal prediction tasks, prior work has found that such models do not sacrifice accuracy and sometimes actually improve accuracy~\cite{shaowang2023amir}. 


\vspace{0.5em} \noindent \textbf{Strategy 2. Mixture of Experts Models}
Similarly, there are neural network architectures that can be trained end-to-end to take advantage of \sys. Mixture of Experts (MoE) is a deep learning architecture that combines multiple models or ``experts' to make predictions on a given task. The basic idea of the MoE architecture is to divide the input space into regions and assign an expert to each region. The gating network takes the input, decides which region it belongs to, and then selects the corresponding expert to make the prediction. The gating network then weights the output of each expert, and the final prediction is the weighted sum of the expert predictions. MoE architectures have been applied to a wide range of tasks, including language modeling, image classification, and speech recognition~\cite{eigen2013learning}.
After training, each expert can be placed independently once trained. 
\fi