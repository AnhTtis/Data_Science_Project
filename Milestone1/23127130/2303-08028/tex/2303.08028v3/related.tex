\section{Related Work}
\noindent \textbf{Model serving.}
Current machine learning model serving systems, including Clipper~\cite{clipper}, TensorFlow Serving~\cite{tfserving}, and InferLine~\cite{inferline}, all assume that the user has manually programmed all necessary data movement.
Recent systems have begun to realize the underappreciated problem of data movement and communication-intensive aspects of modern AI applications.
For example, Hoplite~\cite{hoplite} generates data transfer schedules specifically for asynchronous collective communication operations (e.g., broadcast, reduce) in a task-based framework, such as Ray~\cite{ray} and Dask~\cite{dask}.
However, they have yet to address the trade-offs in time-synchronization between different data sources when they do not arrive at the same time.


\begin{table}[t]
    \centering
    \small
    \begin{tabular}{c|c|c}
         & Ray Serve & \sys (ours) \\ \hline
        Communication & HTTP POST & Message queue \\ \hline
        Data routing & Eager & Eager / Lazy \\ \hline
        Downsampling & Not supported & Supported \\ \hline
        Result delivery & Back to caller & Can route to any topic \\ \hline
        In-out relationship & 1:1 request-response & 1:n/n:1 with joins \\
    \end{tabular}
    \caption{Design decisions: Ray Serve vs. \sys (ours)}
    \label{tab:rayserve-comp}
\end{table}

\noindent \textbf{Edge computing.}
On the other hand, there has been a steady trend towards moving model serving to resources closer to the point of data collection, or the ``edge''. 
The primary focus of model serving on the edge has been to design reduced-size models that can efficiently be deployed on lower-powered devices~\cite{han2015deep, edgedrive, videoEdge, distream}.
Simply reducing the computational footprint of each prediction served is only part of the problem, and these tools do not support data routing when the relevant features might be generated on different edge nodes.


\noindent \textbf{Distributed communication.}
The closest existing tools are those designed for distributed training of ML models. TensorFlow Distributed~\cite{TensorFlow}, for example, allows both all-reduce (synchronous) and parameter server (asynchronous) strategies to train a model with multiple compute nodes.
Another popular framework, PyTorch distributed~\cite{pytorch-distributed}, supports additional collective communication operations such as gather and all-gather with Gloo, MPI, and NCCL backends. One might ask, can we perform distributed inference using these existing distributed training frameworks? Technically it is possible, but as we have shown in \S\ref{sec:exp-opportunity}, the performance is unsatisfactory because such frameworks are optimized for maximum throughput but not end-to-end latency.


\noindent \textbf{Message queues.}
An alternative to distributed communication is message queues.
Existing message queues typically expose a publish-subscribe model that moves messages between services.
Compared with transient queues like RabbitMQ~\cite{rabbitmq} and direct TCP connections as used in Naiad~\cite{naiad}, log-based queues, such as Kafka~\cite{kafka} and Pulsar~\cite{apachepulsar}, ensure the order of messages and are persistent in nature. Users can always replay the logs for debugging purposes. 
Modern message services offered by cloud providers, such as Google PubSub and Amazon SQS, generally have higher latencies in the hundreds of milliseconds due to synchronous data replication across multiple zones.

% \sys chooses Pulsar as its underlying message broker because of its great performance.

\noindent \textbf{Dataflow systems.}
Existing batch processing and stream processing systems support dataflow computations over a dataflow graph~\cite{goog_data,spark,spark-structured-streaming,naiad,apache-samza,apachestorm,twitter-heron,TensorFlow,pytorch,flink}.
These systems are optimized for windowed operations over unbounded data, because a fixed frequency is preferred in typical data analysis workloads.
In contrast, model-serving applications have a much finer granularity of data input and more sensitivity in decision making. Fixed time-windowing is usually not suitable for bursty data. Decision making is based on up-to-date predictions as every single new event arrives.


\noindent \textbf{Temporal synchronization.}
Similarly, this problem is more than just a stream processing problem.
Traditional relational stream processing systems, e.g.,~\cite{chandrasekaran2003psoup}, have stringent requirements for temporal synchronization where they model such an operation as a temporal join. 
These systems will buffer data, indefinitely if needed, to ensure that corresponding observations are properly linked.
While desirable for relation query processing, this approach is excessive in machine learning applications which have to tolerate some level of inaccuracy anyway.
In addition, existing streaming join algorithms look for values from different streams with the same key~\cite{celljoin, splitjoin, DBLP:conf/sigmod/TeubnerM11, DBLP:journals/pvldb/RoyTG14}.
Multi-modal machine learning inference, however, pays more attention to the time skew between streams than the join predicate, as data sources might come at different rates.
In this setting, a looser level of synchronization would benefit the system and improve performance.

In the context of sensing, ROS (Robot Operating System)~\cite{quigley2009ros} is an open-source framework designed for robotics research. It incorporates an algorithm called \texttt{ApproximateTime} that tries to join messages coming on different topics at different timestamps. This algorithm can drop messages on certain topics if they arrive too frequently, but does not use any message more than once. In other words, if one sensor sends data very infrequently, the algorithm will have to wait and drop messages from all other sensors until it sees a new message from the low-frequency sensor to issue a join. The frequency of combined prediction is thus upper-bounded by the most infrequent sensor.
Such a wait can harm both end-to-end latency and accuracy due to the loss of high-frequency information.
