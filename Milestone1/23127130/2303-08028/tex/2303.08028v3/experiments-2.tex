\subsection{Comparison with Ray Serve}\label{sec:exp-system-overhead}
We conduct an object detection task with another serving system, Ray Serve~\cite{ray}, with a sample of nuScenes~\cite{nuscenes} camera data and pre-trained YOLOv5n model~\cite{yolov5} on a single NVIDIA Jetson Nano.

Single-node performance between Ray Serve, \sys, and an ideal case where the job runs locally without any communication is presented in Table~\ref{tab:exp-nuscenes-overhead}.
First, we enforce the freshness threshold SLO (\S\ref{sec:freshness-threshold}) of 1 second and see how many examples must be skipped in order to hit the SLO.
Since the data comes faster than the model's inference speed, 19.0\% of incoming data has to be skipped even in an ideal case.
\sys skips a bit more examples than ideal, but the overhead is reasonably small.
Ray Serve, however, skips 89.4\% of incoming data, which means the system consumes more computational resources than the task itself.
Second, we drop the SLO requirement and see how long it takes for each system to complete the task without downsampling.
In an ideal scenario, the model runs for 25 seconds to finish the dataset.
\sys spends 26 seconds, which presents negligible system overhead.
Ray Serve, on the other hand, spends 2m40s finishing the task, which is 6.4x slower compared to ideal due to its complex design.
In addition, we compare key design decisions between Ray Serve and \sys in Table~\ref{tab:rayserve-comp}.



\begin{table}[]
\small
\centering
\begin{tabular}{l|l|l|l}
    & Ray Serve  & \sys & Ideal \\ \hline
\makecell{\% examples skipped\\ (w/ SLO enforced)}             & 89.4\%   & 22.2\% & 19.0\%     \\ \hline
\makecell{Total working duration\\ (w/o SLO enforced)}         & 2m40s    & 26s    & 25s
\end{tabular}
\caption{Single-node performance of Ray Serve, \sys, and an ideal case.}
\label{tab:exp-nuscenes-overhead}
\end{table}








\iffalse

\section{Experiments: Multi-Camera Tracking}
% Full experiments: https://hackmd.io/@swjz/rJ1nfwi3i
% Spreadsheet and figures: https://docs.google.com/spreadsheets/d/1ebVHpCJEicbJ0IQzw15sqvJAuKCvkWpHMAvcoJAiSeQ/edit?usp=sharing
Our goal for this paper is to simulate a synchronization-sensitive task and demonstrate the trade-off between latency, accuracy, and communication. In order to achieve this goal, we set up a QR code detector where two webcams capture the same QR code from different positions. We move the physical position of the QR code along a horizontal axis and observe the QR code positions detected by both cameras. We compare the trajectory of positions to a centralized baseline where both cameras collect data on the same node to evaluate accuracy. 

\iffalse
\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{figures/offset.pdf}
    \caption{Illustration of QR code offset.}
    \label{fig:offset}
\end{figure}
\fi


\subsection{Hardware Setup}\label{sec:exp-setup}
Our hardware setup consists of two 1080p webcams and four Intel Skylake NUC computers, each equipped with an Intel Core i3-6100U CPU, 16 GB memory, and M.2 SSD. In a centralized setting (Sec.~\ref{sec:centralized}), only one NUC computer is used and it is connected to both webcams. In a distributed setting (Sec.~\ref{sec:distributed}), all four NUC computers are used: two of them are connected to two webcams respectively serving as data source nodes, one of them serves as a message broker and the other NUC serves as the compute node taking input from data source nodes. All four NUCs are interconnected via 100Mbps Ethernet.

\subsection{Software Setup}\label{sec:software}
We use Apache Pulsar~\cite{apachepulsar} as the message broker to transfer messages between NUC computers. For small messages such as a 2D array, we transfer them directly via Pulsar. For larger files such as images, we create FTP paths for them and transfer those paths in messages for the compute node to download, saving traffic on the message broker side. For QR code detection, we use an OpenCV resolution with two CNN-based Caffe models: an object detection model to detect the QR code with a bounding box and a super-resolution model to zoom in the QR code when it is small.
Videos are collected in advance to ensure reproducibility and we simulate real-time streaming of these videos.
All results present the average values of 3 experiments.
All videos used in the following experiments are of 1920x1080 resolution, 5 seconds long at 30 FPS unless otherwise specified. The size of QR code is about 200x200.

\subsection{Metrics and Ground Truth}
In this experiment, we define accuracy as pixel-level `error', which is the difference between ground truth trajectory and the experiments in both x and y axes in the unit of absolute pixels, averaged over all frames. The smaller the absolute number of `error', the more accurate the target experiment is. The ground truth of such offset is defined as the offset between two videos in a centralized setup without compression (Table~\ref{tab:centralized}(a)-(d)). If there is no QR code detected from a certain frame in the target experiment, we use the last known QR code position for that camera. For down-sample experiments, we up-sample the missing frames with the last known frame when measuring accuracy as well.
We also define latency as the time period from the timestamp when the first piece of data is transferred until the timestamp when the prediction for the last piece of data is issued. Since all of our videos have the same length of 5 seconds, this metric is a proxy for ``timeliness'' defined before.

\subsection{Centralized Compute}
\label{sec:centralized}
As a baseline, we consider a scenario where the computation is centralized. There is no communication or synchronization issue in this case. Therefore, we treat the result from this run as ground truth and running time as a baseline. We demonstrate the intrinsic characteristics between fast and slow movements of the QR code and explore the latency component of disk I/O to get a better understanding of the task.

\begin{table}[]
\centering
\begin{tabular}{l|l|l}
                 & Fast movement & Slow movement \\ \hline
a) Memory       & 9.92s                 & 10.52s        \\
%b) Write as BMP     & 11.60s (+1.68s, 17\%)       & 12.04s (+1.52s, 14\%)        \\
%c) Read from BMP    & 12.07s (+2.15s, 22\%)       & 12.42s (+1.90s, 18\%)        \\
b) Disk (uncompressed)  & 13.53s (+3.61s, 36\%)       & 13.75s (+3.23s, 31\%)       \\
c) Disk (jpeg) & 29.76s (+19.84s, 200\%)       & 31.16s (+20.64s, 196\%)
\end{tabular}
\caption{Latency from a centralized compute: (a)-(c) involve the same task with different level of disk access. Numbers and percentages in parentheses are relative to (a).}
\label{tab:centralized}
\end{table}

Table~\ref{tab:centralized} shows the latency from centralized multi-camera tracking example: (a) both streams are captured and passed to \sys in memory; (b) the camera streams are stored to disk in an uncompressed format and incrementally retrieved by \sys; and (c) the camera streams are stored to disk in a JPEG format and incrementally retrieved by \sys.

First, we look at the differences between fast movement and slow movement columns. When we move the QR code too fast, quite a few frames are too blurry for the detector to recognize anything so the decoding step is skipped. Therefore, it takes a little shorter time to finish the computation in fast movement cases. Second, we compare Table~\ref{tab:centralized}(b) against (a) to measure the latency of disk I/O. Specifically, reading and writing image files from/to disk takes about the same amount of time and they add up to about 30-40\% of compute time. We see from Table~\ref{tab:centralized}(c) that disk I/O with JPEG runs significantly longer because extra time is spent on compression and decompression.

It should be noted that images in BMP format are lossless and the size of each BMP file is about 6 MB. JPEG compression is lossy but could significantly reduce file size to 200-300 KB. The accuracy of JPEG compression is shown in Table~\ref{tab:jpeg-accuracy}. In both axes, we see an average error of less than 1 pixel, which is nearly perfect. Fast movement is worse because some blurry frames that are recognizable as BMP files are no longer recognizable after JPEG compression.

\begin{table}[]
\centering
\begin{tabular}{l|l|l}
JPEG accuracy & Fast movement & Slow movement \\ \hline
x-axis        & 0.5931px      & 0.0050px     \\
y-axis        & 0.3868px      & 0.0022px     
\end{tabular}
\caption{Errors introduced by JPEG compression.}
\label{tab:jpeg-accuracy}
\end{table}

\subsection{Distributed Edge Cluster}\label{sec:distributed}
As described in Sec.~\ref{sec:exp-setup}, we construct an Edge cluster and do the same QR code detection task, where data is collected on different nodes.
% Both data sources need to arrive at the compute node in an aligned manner.
We build a queue for each data source at the message broker and they are aggregated to make a prediction as soon as there is new data coming in from any data source. A data point may be reused to make a joint prediction if it is still the latest from an infrequent data source.
Jitter in the network, variability in processing times, and queuing delays can introduce extra errors. We compare these errors to the sub-pixel errors introduced by lossy compression above.

% Slow movement results
% \begin{table}[]
% \centering
% \begin{tabular}{l|l|l|l|l}
%               & Size   & Time                                                      & x-axis error & y-axis error \\ \hline
% BMP (30 FPS)  & 1.7 GB & \begin{tabular}[c]{@{}l@{}}3m 10s\\ (2m 41s)\end{tabular} & -31.216px    & 1.4522px     \\
% BMP (10 FPS)  & 593 MB & \begin{tabular}[c]{@{}l@{}}1m 4s\\ (53.8s)\end{tabular}   & -14.6671px   & 0.6375px     \\
% JPEG (30 FPS) & 72 MB  & \begin{tabular}[c]{@{}l@{}}52.7s\\ (8.35s)\end{tabular}   & 4.7356px     & -0.0940px    \\
% JPEG (10 FPS) & 27 MB  & \begin{tabular}[c]{@{}l@{}}17.9s\\ (3.2s)\end{tabular}    & 0.4415px     & -0.0150px   
% \end{tabular}
% \caption{BMP, JPEG and downsampling comparison. Numbers in parentheses are time spent purely on data download, measured by \texttt{wget}.}
% \label{tab:distributed}
% \end{table}

% Fast movement results
\begin{table}[]
\centering
\begin{tabular}{l|l|l|l|l}
              & Size   & Time                                                      & x-axis error & y-axis error \\ \hline
BMP (30 FPS)  & 1.7 GB & \begin{tabular}[c]{@{}l@{}}3m 11s\\ (2m 41s)\end{tabular} & 22.8402px   & 4.1812px     \\
BMP (10 FPS)  & 593 MB & \begin{tabular}[c]{@{}l@{}}1m 4s\\ (53.8s)\end{tabular}   & 7.3501px    & 1.7942px     \\
BMP (5 FPS)   & 297 MB & \begin{tabular}[c]{@{}l@{}}32.1s\\ (27.1s)\end{tabular}   & 29.4458px    & 1.5753px     \\
JPEG (30 FPS) & 72 MB  & \begin{tabular}[c]{@{}l@{}}49.8s\\ (8.3s)\end{tabular}    & 4.0714px     & 0.4754px     \\
JPEG (10 FPS) & 24 MB  & \begin{tabular}[c]{@{}l@{}}16.6s\\ (2.9s)\end{tabular}    & 6.6651px    & 2.7245px     \\
JPEG (5 FPS)  & 12 MB  & \begin{tabular}[c]{@{}l@{}}8.2s\\ (1.6s)\end{tabular}    & 29.4797px    & 1.6021px    
\end{tabular}
\caption{BMP, JPEG and down-sampling comparison. Numbers in parentheses are time spent purely on data download, measured by \texttt{wget}.}
\label{tab:distributed}
\end{table}

% \vspace{0.25em} \noindent \emph{Effect of Down-Sampling and Compression on Accuracy.}
\subsubsection{Effect of Down-sampling and Compression on Accuracy.}\label{sec:exp-downsample}
Counter-intuitively, degrading the quality of the data collected from the sensors can lead to better results in the distributed setting.
We compare compression and effects of down-sampling in Table~\ref{tab:distributed}. We find that down-sampling can almost linearly improve latency because it directly reduces data transfer. It also forces synchronization on the data source side to improve accuracy until it reaches a sweet spot, after which we lose so much information between frames that accuracy starts to decrease. JPEG compression has a similar effect of reducing data transfer significantly, which also improves latency compared to BMP counterparts. Since JPEG compression takes a while (as shown in Table~\ref{tab:centralized}), it acts as a rate limit at the data source, similar to the down-sampling method, which also forces synchronization. The accuracy `sweet spot' for JPEG is reached at the original sampling rate and further down-sampling would only decrease accuracy.
% Alternatively, we could also manually set a rate limit at compute node, which buffers input data but does not issue predictions until all data sources are fully synchronized.

% Slow movement results
% \begin{table}[]
% \centering
% \begin{tabular}{l|l|l|l}
% Ensemble method      & Time  & x-axis error & y-axis error \\ \hline
% Original 30 FPS   & 6.55s & 58.911px     & 0.2834px     \\
% Downsampled to 10 FPS & 6.35s & -11.273px    & 0.8657px
% \end{tabular}
% \caption{Latency and accuracy for Ensemble method.}
% \label{tab:ensemble}
% \end{table}

% Fast movement results
\begin{table}[]
\centering
\begin{tabular}{l|l|l|l}
Endpoint-Placement & Time  & x-axis error & y-axis error \\ \hline
Original 30 FPS       & 5.79s & 22.4691px    & 2.7398px    \\
Down-sampled to 10 FPS & 5.93s & 24.7646px   & 5.6231px     \\
Down-sampled to 5 FPS  & 5.55s & 29.5273px   & 1.7011px
\end{tabular}
\caption{Latency and accuracy for Endpoint-Placement.}
\label{tab:ensemble}
\end{table}

% \vspace{0.25em} \noindent \emph{Compute Placement and Synchronization Errors.}
\subsubsection{Compute Placement and Synchronization Errors.}\label{sec:exp-placement}
It is a natural thought that moving inference closer to the point of data collection is desirable in edge computing; this is not always the case in multimodal prediction.
Instead of transferring frames over the network, we could also run the model at data source nodes and only transfer coordinates over the network. This method makes use of model parallelism and spare resources on data source nodes to save communication between data source nodes and compute node(s). Such an ``Endpoint-Placement'' method is perfectly suitable for our use case because we have light models but heavy communication. Table~\ref{tab:ensemble} shows that this method improves latency by 33x compared to BMP (30 FPS) in Table~\ref{tab:distributed}.

However, Endpoint-Placement methods can be a source of misalignment as well, especially when a model runs faster for some data points but slower for other data points. The variability in model inference latency across nodes can add up to become a synchronization problem and reduces accuracy. In Table~\ref{tab:ensemble}, we see higher synchronization errors because of this variability.


% \vspace{0.25em} \noindent \emph{Effect of Queuing Strategy.}
\subsubsection{Effect of Lazy Data Routing.}\label{sec:exp-queuing}
In Sec.~\ref{sec:software} we described how we use FTP to transfer large data directly from data source to compute node and only pass pointers to data over the message broker. This is especially beneficial when the message broker is busy with network requests. We simulate a congestion scenario where the network bandwidth at the message broker is limited and measure the end-to-end latency of our system versus a ROS-like system that transfers raw data through a centralized broker. Table~\ref{tab:congestion} shows that our system is tolerant to slow network bandwidth while transferring raw frames can be extremely slow when the network is congested.

\begin{table}[]
\centering
\begin{tabular}{l|l|l}
                          & Rate limit (up /down) & Time    \\ \hline
Lazy (ours) & No limit                       & 3m 10s  \\
Lazy (ours) & 1 Mbps / 1 Mbps                & 3m 12s  \\
Eager (similar to ROS)       & No limit                       & 3m 16s  \\
Eager (similar to ROS)         & 20 Mbps / 20 Mbps              & 21m 32s
\end{tabular}
\caption{Latency with network bandwidth limits.}
\label{tab:congestion}
\end{table}

\fi