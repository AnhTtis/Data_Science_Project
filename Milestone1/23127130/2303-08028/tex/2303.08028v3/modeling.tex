\section{Prediction Rate Control}\label{sec:rate-control}
Next, we show how to ensure this execution layer can meet particular model-serving service level objectives (SLOs). We leverage statistical approximations that exploit temporal similarity in typical data streams.
Every model in \sys is annotated with three timing parameters: (1) if it consumes multiple streams, a maximum tolerable \emph{skew}, (2)  a \emph{target prediction frequency}, which is an output rate limiter, and (3) a \emph{freshness threshold}, designed to discard stale messages originating from an earlier time.

\subsection{Message Skew}\label{sec:message-skew}
\sys gives the programmer an illusion of stream alignment, namely, streams associated with the same topic can be thought of as synchronized from the perspective of machine learning modeling. 
The consuming models receive tuples of headers corresponding to data from each of the sources.

\iffalse
Figure \ref{fig:align} shows this point: from a programmer's perspective, the entire topic can be treated as a single stream.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/align.png}
    \caption{A model subscribes to a single topic which might include multiple streams of data. \sys ensures that multiple streams of data are time-aligned as tuples in order to make predictions.}
    \label{fig:align}
\end{figure}
\fi

Under the hood, \sys has to buffer streams locally to keep up this illusion.
The different data streams will arrive at different rates and have different system delays that cause misalignment.
We use a time interval-based interface for specifying alignment criteria.
Every topic has a maximum allowed time-skew (\S\ref{sec:streaming-inference}) between headers that can be produced.
Locally, the buffer retains a header until it receives matching header messages from other streams or the time-skew expires.
Thus, we can enforce a bounded-skew synchronization on the model side.
It is up to the user to set a reasonable time-skew limit for her specific task.
If the allowed time-skew is overly long, there is a risk of her encountering messages that lack proper synchronization.
Conversely, setting the time-skew limit too restrictively may result in the loss of some actually synchronized predictions owing to this stringent threshold.
% This heavily depends on a reasonable time interval requested between predictions. If such time interval is too short, such synchronization can be ineffective without improving accuracy; if the time interval is too long, potential long waits can stretch end-to-end timeliness while harming accuracy due to lost high-frequency information in between. 


\subsection{Hybrid Time- and Data-triggered Join}\label{sec:target-pred-freq}
The hybrid join in \sys is an innovative approach that combines the principles of both data-triggered and time-triggered joins. This method is designed to efficiently handle the challenges posed by high-velocity data streams and the processing capacity of models.

In essence, the hybrid join operates on two fundamental principles: rapid response to new data (inherited from the data-triggered join) and effective data management to avoid overloading the downstream model (inspired by the time-triggered join and backpressure mechanism~\cite{tassiulas1990stability}). When new data arrives from any stream, the hybrid join promptly triggers a joining operation, similar to a data-triggered join. This ensures that the system remains responsive to incoming data, allowing for timely processing and analysis.

However, to address the issue of data arriving faster than the downstream model can handle, the hybrid join incorporates a critical feature from the time-triggered approach: setting a minimum interval between consecutive processing instances, which we call \textit{target prediction frequency}. This interval acts as a throttle, ensuring that the downstream model is not overwhelmed by a continuous influx of data. If data arrives more rapidly than this set interval, the hybrid join mechanism will simply drop earlier data in the queue. This decision is based on the understanding that data delayed excessively in the queue may no longer be accurate or relevant for real-time decision-making.

By integrating these two approaches, the hybrid join offers a balanced solution that maximizes responsiveness to new data while maintaining a manageable processing load for the downstream model.
% This method ensures that \sys is not only fast and efficient, but also adaptable to varying data flow rates.

\iffalse
\subsection{Target Prediction Frequency}
\todo{This is actually the same as time-triggered join? In \S\ref{sec:exp-opportunity} we basically use a combination of data-triggered and time-triggered joins!}
In classical data streaming systems, ``back pressure'' is a mechanism used to control the rate at which data is processed by the system in order to prevent overload and ensure stability (e.g., as in Apache Flink~\cite{carbone2015apache}). Back pressure is usually applied when there is a mismatch between the rate at which data is being produced by the data source and the rate at which it can be processed by the downstream components of the system. In such cases, the system may experience a backlog of data that has yet to be processed, leading to increased latency and decreased throughput.

Similarly, model-serving systems are rate-limited by the decision processes that consume their predictions. This might be a monitoring dashboard subject to visualization refresh rates, or an SLO describing a desired reaction time. In \sys, users can annotate models with a \emph{target prediction frequency}, which is a minimum time interval between two consecutive predictions. This prediction frequency downsamples data if the data arrival rate exceeds what is attainable (i.e., like back-pressure).
% Conversely, if some data arrive slower than this target frequency, it provides a timeout for how long we have to wait for asynchronous messages.

To see how this works, let's work with a simple single-model example. Assume the following data stream arrives in the system:
\[
(\textsf{time}, \text{data}) = (1,x_1),(3,x_3),(4,x_4),(6,x_6)
\]
Without a target frequency, the system would yield the following predictions for each arriving example at the corresponding times:
\[
(\textsf{time}, \text{res}) = (1,f(x_1)),(3,f(x_3)),(4,f(x_4)),(6,f(x_6))
\]

Instead of synchronizing predictions on data arrival, a prediction frequency target synchronizes predictions based on a timer. It yields a prediction from the last known observation at that time.
Let us consider the case where a user wants to rate-limit the system. For example, a frequency of $2$ in the example above would yield predictions at (2,4,6).
The latest data is always used to produce this prediction.
\[
(\textsf{time}, \text{res}) = (2, f(x_1)),(4,f(x_4)),(6,f(x_6)))
\]

As we can see in this example, if the prediction frequency is slower than the data arrival rate, substantial data skipping is possible.
It gives us an additional knob to optimize data transfer, where the system need not yield predictions faster than the desired target.
For the example above, time-step $3$ is never consumed by the inference node.
This means that certain header messages are ignored, and thus lazy data routing saves us from transferring those data payloads.
A significant amount of communication can be saved if some data streams arrive faster than the desired prediction frequency.
However, there is a natural trade-off of staleness that arises with this approach. 
\fi

\subsection{Freshness Threshold}\label{sec:freshness-threshold}
As a streaming system, \sys prioritizes the timeliness of incoming data.
Stale data is essentially inaccurate data for latency-sensitive tasks.
Making use of such outdated information in real-time decision-making can lead to disastrous outcomes.
Our freshness threshold ensures the recency of data that the model can take.
It also acts as a rate limit when data arrives faster than the model inference rate.

\iffalse
\subsection{Tolerating Incomplete Messages}
\todo{same with 6.2, too much overlap with time-triggered join.}
Target frequencies and skews can also be used to tolerate variable or fault-prone data streams.
Suppose one sets the target frequency to be at the P95 data arrival rate; it can be used to generate a timeout on the message broker.
If new data from one source has not arrived before the timeout, the timer will have to short-circuit with a partial message only containing data from a subset of the sources.
Such anomalies can happen if there is a temporary failure on one of the data source nodes, or a large network delay.

In these cases, we do not want the system to fail.
\sys provides a number of fail-soft mechanisms to rectify the issue, such as dropping the tuple and imputing the missing values with a last known good observation.
In our experiments, we use last-known-good-data as our primary fail-soft strategy.
In the example above with a target of $2$, suppose the following stream of data had a failure at example $t=3$:
\[
(\textsf{time}, \text{data}) = (1,f(x_1)),(3,\blacksquare),(4,f(x_4)),(6,f(x_6))
\]
The system would yield the following predictions:
\[
(\textsf{time}, \text{result}) = (2, f(x_1)),(4,f(x_4)),(6,f(x_6))
\]
Now, suppose the target was $1$, the result would be:
\[
(\textsf{time}, \text{result}) = (1,f(x_1)),(2,f(x_1)),(3,f(x_1)),...,(6,f(x_6))
\]
In dense streams of data, such a strategy leverages temporal correlations where the last known observation is likely similar to the missing data.
This allows the prediction stream to fail soft in the presence of jitter and temporary failures.
\fi
% \subsubsection{Horizontal Rate Matching}
% Horizontal rate matching describes a problem where within a single topic, the constituent data streams arrive at different rates.
% We take a simple approach where we match the rate of the fastest incoming stream.
% Headers from slower streams can be matched to multiple headers of faster streams as long as those matches respect time ordering.

% \subsubsection{Vertical Rate Matching}
% There is further a rate-matching problem where a model consuming the data might not be able to consume the data as fast as it is produced. 
% We encourage users to set the \texttt{prediction\_freq} parameter, so \sys is able to automatically downsample the data to make real-time predictions. 
% This basically means that certain header messages are ignored, and thus lazy data routing saves us from transferring those data payloads.




% \subsection{Prediction Frequency Synchronization}
