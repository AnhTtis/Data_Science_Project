\section{Background and Existing Model Serving Frameworks}
This section motivates \sys and describes the performance of existing model serving frameworks.

\begin{example}
To understand how existing cloud-based systems work, we construct a simplified scenario where a single stream of data is fed into a model-serving framework. Each data item is a 134-dimensional feature vector, the model-serving framework must issue a prediction for each item. The items are streamed into a message broker and dequeued in timestamp order. The goal of this experiment is to illustrate that queueing and communication far outweigh the actual model inference time for typical sensing workloads.
\end{example}

Based on blog posts and tutorials that describe best practices, we developed a few different models serving pipelines on AWS and GCE \cite{gce}. We experimented with two different models, a Random Forest and a 3-layer MLP. We used roughly comparable inference hardware on both cloud providers (on AWS SageMaker EC2 P3 and GCE a VertexAI 2.10 Container), and note that this inference hardware is GPU-accelerated.

\begin{itemize}
    \item \textbf{AWS. } This model-serving pipeline uses AWS SQS to queue messages and AWS SageMaker to perform the inferences over each queued message. 
    \item \textbf{GCE. } This model-serving pipeline uses GCE Pub/Sub to queue messages and GCE VertexAI to perform the inferences over each queued message.
    \item \textbf{Inf Only. } We run both the AWS and GCE pipelines above in an inference-only mode which only measures the latency of AWS SageMaker and GCE VertexAI repsectively.
\end{itemize}

We evaluate these two baselines in terms of their end-to-end latency, which is the elapsed time since the execution of the ``publish'' message to the message queue and the delivered prediction. 
\begin{table}[ht!]
\begin{tabular}{|l| l|l||l|l|}
\hline
    & \multicolumn{2}{c||}{Random Forest}          & \multicolumn{2}{c|}{MLP}                                                                \\
    & Med. &P99 & Med. &P99 \\ \hline\hline
AWS & 141ms & 400ms & 116ms & 391ms \\
AWS (inf only) & 20ms & 25ms & 18ms & 20ms \\
GCE & 88ms & 94ms    & 74ms & 82ms \\ 
GCE (inf only) & 18ms & 22ms    & 13ms & 15ms \\  \hline
\end{tabular}
\caption{End-to-end latency for model inference over a 134-dimension sensor stream}
\end{table}

In terms of end-to-end latency, existing frameworks are not satisfactory for emerging ``real-time'' machine learning applications, where the typical latency tolerance is measured in tens of milliseconds.
With default cloud tools, one can expect hundreds of milliseconds of latency. Even worse, this is often highly unpredictable.
Interestingly enough, the primary source of end-to-end latency is not the model inference itself, but delays in message queuing. 
Streaming the data to the model becomes a bottleneck, incurring copying costs, queuing costs, and checkpointing/replication costs at the message broker.
Cloud-based messaging services were designed to be highly available and reliable, but not particularly aimed for low-latency or time-synchronized applications.
These queuing overheads can be made arbitrarily more significant if multiple streams of data are required. 
Then, there is additional waiting time in the system to align observations across streams.

\subsection{What is Streaming Inference?} \label{sec:streaming-inference}
These numbers indicate the need for a new model serving framework that tightly \textbf{integrates streaming with model inference.} 
We can build a serving framework that is more suited for streaming data, and we can build a streaming system that is more suited for the typical workloads seen in machine learning serving.
% First, we describe this problem in the abstract.

\noindent \textbf{Inference over a Single Stream. } Consider a supervised learning inference task. Let $x$ be a feature vector in $\mathcal{R}^p$ and $f_\theta$ be a model with parameters $\theta$. $f_\theta$ evaluates at $x$ and returns a corresponding prediction $y$, which is in the set of labels $\mathcal{Y}$. A prediction over a stream of such feature vectors can be thus summarized as:
\[ y_t = f_\theta(x_t) \]
where $t$ denotes a timestamp for the feature vector. In such a prediction problem, the user must ensure that the featurized data is at ``the right place at the right time'': $f_\theta$ has to be hosted somewhere in a network and $x_t$ has to be appropriately generated and sent to $f_\theta$.

\noindent \textbf{Inference over Multiple Streams. } Now, let's imagine that $x_t$ is constructed from multiple different streams of data. Each $x_t$ (the original features) can be treated as a concatenation of $d$ individual streams:
\[
x_t = \begin{bmatrix}
x^{(1)}_t &
... &
x^{(i)}_t &
... &
x^{(d)}_t  
\end{bmatrix} \
\]
Each of these streams of data $x^{(i)}_{1},...,x^{(i)}_t$ might be produced on a different node in a network.
Consider the network intrusion detection example (Example 1). Each $x^{(i)}$ corresponds to one of the streams of data (packets from node 1, packets from node 2, packets from node 3).
In this case, we have different streams of data $x^{(1)},x^{(2)},...$ coming in, and we need to aggregate them so that the final prediction arrives in our desired destination node.

If the streams of sub-features are collected independently, they will likely not be time-synchronized. This means, at any given instant, the data at the prediction node comes from a slightly different timestamp:
\[
x_t = \begin{bmatrix}
x^{(1)}_{t+\epsilon_1} &
... &
x^{(1)}_{t+\epsilon_i} &
... &
x^{(d)}_{t+\epsilon_d}  
\end{bmatrix} \
\]
Each $\epsilon_i$ denotes a positive or negative offset. The overall time-skew of the prediction problem is $\epsilon = \text{max}_{i} \epsilon_i - \text{min}_{j} \epsilon_j$. In other words, to issue a perfectly synchronized prediction at time $t$, the earliest stream has to wait for $\epsilon$ steps to ensure all features are available.
This can be even more complicated if different data streams are collected at different frequencies.
\sys provides an API for controlling synchronization errors in decentralized prediction deployments (\S\ref{sec:rate-control}).


\noindent \textbf{\sys: Our Contribution. }
Today's model serving systems lack the support for flexibly deploying models (or partial models) across a network and routing data and predictions to/from them.
Users with such problems today have to design bespoke solutions, which can result in brittle design decisions that are not robust to changes in the network or data.
While it is true that prior work has considered decomposing models across a network to optimize throughput~\cite{narayanan2019pipedream}, this work does not consider latency-sensitive applications nor does it consider disaggregated input data streams. 
\sys significantly reduces latency in queuing and communication leading to large improvements in end-to-end responsiveness. 
To motivate the contributions, if we run the same experiment on comparable hardware in AWS as above in \sys, we get the following results:

\begin{table}[ht!]
\begin{tabular}{|l| l|l||l|l|}
\hline
    & \multicolumn{2}{c||}{Random Forest}          & \multicolumn{2}{c|}{MLP}                                                                \\
    & Med. &P99 & Med. &P99 \\ \hline\hline
\sys & 21ms & 31ms & 20ms & 29ms \\ \hline        
\end{tabular}
\caption{End-to-end latency For model inference over a 134-dimension sensor stream using \sys}
\end{table}



\iffalse
\subsubsection{Stream in, Stream out.}
The key difference between \sys and model serving systems is that \sys takes streams as the unit of operations.
A model is always applied to a data stream instead of an individual data item.
The inference output is also a continuous stream.
We decouple the 1:1 relationship between input requests and output responses to allow for asynchronous and multi-modal serving. This architecture allows for new machine-learning opportunities.
Consider the following approach to the network prediction example.

\begin{example}
Rather than a single network intrusion detection model that integrates all the data, one decomposes this model into an ensemble of small local models. Each packet capture node of the network has a node-specific model that makes a prediction. Instead of communicating the features, the predictions from these local models are communicated to the desired location. These predictions can be ensembled with a lightweight model to make a single final decision. Such a deployment reduces communication and more effectively utilizes parallelism.
\end{example}

\sys provides a single system that can express both the centralized solution (e.g., all packet capture data streamed to a single model-serving framework) or the decentralized solution (e.g., the predictions of local models ensembled together).
In \sys, predictions from a model are simply another data stream that can be consumed by other models.
We believe that it is a step towards the class of machine learning systems that are needed for a future of ubiquitous sensing, AR/VR, and eventually autonomous vehicles.

% \subsubsection{Serve the most up-to-date data.}
% In latency-sensitive tasks, the system should react to any new data immediately.
% \sys aims to reduce the communication time to the minimum and eliminate wait time 
%     - Data is pushed to the model as soon as it arrives. The system should immediately react to new data.
%     - Late predictions are wrong predictions. Earlier data can be dropped when not catching up.

\subsubsection{Join Eagerly, Route Lazily.}
Existing streaming systems often rely on a pre-defined time window to issue joins between multiple streams, and engineers have to manually adjust the time window for bursty data.
\sys issues a join for every single data arrival from any stream.
Joined multi-modal data is immediately passed to the model without having to wait for a time interval.
In this way, downstream operators perceive incoming data as soon as they arrive.
To reduce communication cost, we only transfer headers of data instead of the raw payload.
Models may choose to either skip an individual data item or fetch the raw payload from data source according to the header.

In short, rather than adapting the model to a fixed data processing pipeline, one should adapt the data processing pipeline to models that might change over time. Engineers will likely change models as ML advances and these changes often have latency implications (e.g., bigger models) or buffering implications (e.g., larger input history into the model).
% When these changes happen the  

\subsubsection{Data can come from anywhere and go anywhere.}
Existing model serving services all follow the request-response API design, which requires that the prediction results be sent back to the caller.
This might not be suitable for streaming use cases, as users likely prefer results elsewhere.
\sys simply routes results to another message topic that could be consumed by any node.
The prediction does not have to go back to the data source.
In addition, \sys relaxes the strict 1:1 relationship between inputs and outputs.
We allow both n:1 (e.g. multi-modal data) and 1:n (e.g. multiple models consuming the same input) input:output relationships.

\subsubsection{Non-deterministic but reproducible.}
Unlike fixed time windows, data-triggered joins can be non-deterministic due to unpredictable network environments.
\sys logs down all join decisions with input headers, so they could always be replayed for debugging purposes.
We also periodically offload local data to the cloud, while maintaining an edge-cloud path map. With this map, users can always find the original raw payload by its header.
\fi

