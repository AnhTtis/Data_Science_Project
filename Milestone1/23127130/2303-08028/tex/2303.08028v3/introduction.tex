\section{Introduction}
The broader computing community has long understood the importance of telemetry in both physical and digital systems.
% These data streams give engineers insight into how complex systems behave, the ability to detect anomalies, and data for process optimization.
The growing maturity of AI has created new opportunities for such data, where models can be built to predict future behavior and/or automatically react to current trends -- to ``close the loop''.
This paper presents \sys, a new system that allows for low-latency feedback systems over distributed streams of data. 

Efficiently serving predictions from machine learning models is already a crucial part of modern software applications ranging from automatic fraud detection to predictive medicine~\cite{tfcasestudies}. 
Accordingly, a number of \emph{model serving frameworks} have been developed, including Clipper~\cite{clipper}, TensorFlow Serving~\cite{tfserving}, and InferLine~\cite{inferline}. 
These frameworks simplify the deployment and interfacing of trained machine-learning models with a service-oriented interface.
Typically, they provide a RESTful API that accepts features as inputs (i.e., a prediction ``request''), and responds to these requests with predicted labels (i.e., a prediction ``response'').
These frameworks provide a number of crucial optimizations such as containerizing inference code~\cite{clipper}, autoscaling~\cite{tfserving}, and model ensembling~\cite{inferline}.

Existing model serving frameworks were envisioned as components in cloud-based deployments.
Implicit to this design, there are several key assumptions: (1) prediction requests arrive asynchronously through the RESTful interface, (2) the request is self-contained with all of the features necessary to issue a prediction, (3) the design prioritizes scalability over the latency of an individual request, (4) and the response is delivered back to the requester.
We find that streaming settings challenge this design paradigm.
Consider a simple example of a model where the time-ordering of predictions matters (e.g., sensor fusion or forecasting).
If such a model is served with a RESTful model-serving framework, there is no inherent message ordering guarantee which is crucial for accurate forecasting.
The data processor needs to block processing until a prediction is returned by the framework, and this negates any pipelining or scale-out optimizations present in these frameworks. 
For such use cases, it is more convenient for developers to think of a machine learning model as an operator applied to one or more continuous streams of data with synchronization, rate limit, and freshness constraints.


To the best of our knowledge, the academic literature on this topic is relatively sparse with most existing work in video analytics~\cite{zhao2023streaming, kang2017noscope, arulraj2022accelerating, horchidan2022evaluating,flinkml, tfkafka}. 
There is also a significant amount of work in real-time systems~\cite{flink, spark-structured-streaming, apache-samza, apachestorm}, but few systems focus on model serving.
In particular, significant technical challenges arise when the relevant features for a machine learning model are generated on different network nodes than where the model is served.
The data has to get to ``the right place at the right time'' before any prediction can be made, and this communication quickly becomes the primary bottleneck.
The problem is further complicated where there are multiple data streams: the data streams have to be time-synchronized and integrated before any predictions can take place.
Prior work has shown that placement and synchronization decisions affect both performance and accuracy in nuanced ways~\cite{shaowang2021declarative, shaowang2022bidede}.
Thus, for low-latency model-serving over distributed streams of data, one has to jointly optimize for communication, rate control, and the fact that minor time alignment deviations typically have a negligible impact on accuracy in model-serving scenarios.

To better understand the tradeoffs, consider the following running example.

\begin{example}
In network intrusion detection, machine learning models applied to packet capture data are used to infer anomalous or malicious traffic patterns. Most organizations have geo-distributed private networks spanning multiple clouds and regions. The relevant features for a particular intrusion detection model may be sourced from different packet capture streams at different points in the network.
These streams will have to be synchronized and integrated to make any global prediction.
\end{example}

With existing tools, building such applications requires significant developer effort in the design of (1) communication between nodes collecting the streams, (2) the time-alignment strategy for the streams, and (3) the rate control of incoming data.  Challenge (1,2,3) create a complex tradeoff space that leads to bespoke solutions~\cite{IntelPress2020, tfcasestudies, tfkafka, lyft}.
This paper describes a first step towards such a system, called \sys, that addresses this need.
Instead of a RESTful service that handles each prediction request asynchronously, \sys routes synchronized streams of data to models that are flexibly placed anywhere in a network.
We call such an architecture \emph{synchronized prediction} to differentiate it from classical model serving, where a collection of model-serving nodes work together to serve predictions over one or more data streams in a temporally coherent way.

Practically, \sys provides a lightweight inference service that can be installed on every node of the network.
\sys employs a message broker to route data around different nodes, allowing multiple producers and consumers to operate on the same message queue simultaneously. Users can define data movements and model placements by pointing models to named streams of data rather than their physical locations. Furthermore, the user can program her model and featurization as if there was all-to-all communication in the network, and the actual data routing over the actual network topology is handled seamlessly by \sys.
These data streams can be time-synchronized so that inferences that need to look at a particular snapshot in time can appropriately construct features that join data from different sources.
Furthermore, the data can be derived from primary sources (e.g., sensors, user data streams, etc.) or can be results of computation (e.g., features/predictions computed from pre-trained models).
This flexibility allows users to build complex but robust predictive applications in networks with heterogeneous and disaggregated resources. 

While \sys resembles other streaming and data flow systems~\cite{goog_data,spark,spark-structured-streaming,naiad,apache-samza,apachestorm,twitter-heron,TensorFlow,pytorch,flink}, there are three key novel architectural features due to the model-serving focus.
\begin{itemize}
    \item (How to trigger computations?) \emph{Data-Triggered Stream Joins.} \sys employs a novel temporal join strategy for combining multiple streams of data based on data arrivals (\S\ref{sec:join}). % ``frontier completeness''

    \item (What are the communication primitives?) \emph{Lazy Data Routing.} For large data payloads (e.g., high-dimensional data streams), \sys applies an innovative communication protocol called ``lazy data routing'' where only references to data are sent through the message broker. (\S\ref{sec:lazy})

    \item (How to ensure reliable behavior?) \emph{Prediction Rate Control.} \sys presents strategies that can ensure that timely decisions still get made even in the presence of dropped, delayed messages or overloaded models. (\S\ref{sec:rate-control})
\end{itemize}
