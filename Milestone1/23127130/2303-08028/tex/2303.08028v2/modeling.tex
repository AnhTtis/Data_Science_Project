\section{Ensuring Prediction Timeliness}\label{sec:rate-control}
Next, we show how to ensure this execution layer can meet particular model-serving SLOs. We leverage statistical approximations that exploit temporal similarity in typical data streams.
Every model in \sys is annotated with two timing parameters:
\begin{itemize}
    \item If it consumes multiple streams, a maximum tolerable \emph{skew}.
    \item A \emph{target prediction frequency}., which is the desired rate of output.
\end{itemize}


\subsection{Message Skew}
\sys gives the programmer an illusion of stream alignment, namely, streams associated with the same topic can be thought of as synchronized from the perspective of machine learning modeling. 
The consuming models receive tuples of headers corresponding to data from each of the sources.
Figure \ref{fig:align} shows this point: from a programmer's perspective, the entire topic can be treated as a single stream.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/align.png}
    \caption{A model subscribes to a single topic which might include multiple streams of data. \sys ensures that multiple streams of data are time-aligned as tuples in order to make predictions.}
    \label{fig:align}
\end{figure}

Under the hood, \sys has to buffer streams locally to keep up this illusion.
The different data streams will arrive at different rates and have different systems delays that cause misalignment.
We use a time interval-based interface for specifying alignment criteria.
Thus, every topic has a maximum allowed time-skew between headers that can be produced.
Locally, the buffer retains a header until it receives matching header messages from other sensors. 
Thus, we can enforce a bounded-skew synchronization on the model side. 
This heavily depends on a reasonable time interval requested between predictions. If such time interval is too short, such synchronization can be ineffective without improving accuracy; if the time interval is too long, potential long waits can stretch end-to-end timeliness while harming accuracy due to lost high-frequency information in between. 


\subsection{Target Prediction Frequency}
In classical data streaming systems, ``back pressure'' is a mechanism used to control the rate at which data is processed by the system in order to prevent overload and ensure stability (e.g., as in Apache Flink~\cite{carbone2015apache}). Back pressure is usually applied when there is a mismatch between the rate at which data is being produced by the data source and the rate at which it can be processed by the downstream components of the system. In such cases, the system may experience a backlog of data that has yet to be processed, leading to increased latency and decreased throughput.

Similarly, model-serving systems are rate-limited by the decision processes that consume their predictions. This might be a monitoring dashboard subject to visualization refresh rates, or an SLO describing a desired reaction time. In \sys, users can annotate models with a \emph{target prediction frequency}. This prediction frequency downsamples data if the data arrival rate exceeds what is attainable (i.e., like back-pressure). 
Conversely, if some data arrive slower than this target frequency, it provides a timeout for how long we have to wait for asynchronous messages.

To see how this works, let's work with a simple single-model example. Assume the following data stream arrives in the system:
\[
(\textsf{time}, \text{data}) = (1,x_1),(3,x_3),(4,x_4),(6,x_6)
\]
Without a target frequency, the system would yield the following predictions for each arriving example at the corresponding times:
\[
(\textsf{time}, \text{res}) = (1,f(x_1)),(3,f(x_3)),(4,f(x_4)),(6,f(x_6))
\]

Instead of synchronizing predictions on data arrival, a prediction frequency target synchronizes predictions based on a timer. It yields a prediction from the last known observation at that time.
Let us consider the case where a user wants to rate-limit the system. For example, a frequency of $2$ in the example above would yield predictions at (2,4,6).
The latest data is always used to produce this prediction.
\[
(\textsf{time}, \text{res}) = (2, f(x_1)),(4,f(x_4)),(6,f(x_6)))
\]

As we can see in this example, if the prediction frequency is slower than the data arrival rate, substantial data skipping is possible.
It gives us an additional knob to optimize data transfer, where the system need not yield predictions faster than the desired target.
For the example above, time-step $3$ is never consumed by the inference node.
This means that certain header messages are ignored, and thus lazy data routing saves us from transferring those data payloads.
A significant amount of communication can be saved if some data streams arrive faster than the desired prediction frequency.
However, there is a natural trade-off of staleness that arises with this approach. 

\subsection{Tolerating Incomplete Messages}
Target frequencies and skews can also be used to tolerate variable or fault-prone data streams.
Suppose one sets the target frequency to be at the P95 data arrival rate; it can be used to generate a timeout on the message broker.
If new data from one source has not arrived before the timeout, the timer will have to short-circuit with a partial message only containing data from a subset of the sources.
Such anomalies can happen if there is a temporary failure on one of the data source nodes, or a large network delay.

In these cases, we do not want the system to fail.
\sys provides a number of fail-soft mechanisms to rectify the issue, such as dropping the tuple and imputing the missing values with a last known good observation.
In our experiments, we use last-known-good-data as our primary fail-soft strategy.
In the example above with a target of $2$, suppose the following stream of data had a failure at example $t=3$:
\[
(\textsf{time}, \text{data}) = (1,f(x_1)),(3,\blacksquare),(4,f(x_4)),(6,f(x_6))
\]
The system would yield the following predictions:
\[
(\textsf{time}, \text{result}) = (2, f(x_1)),(4,f(x_4)),(6,f(x_6))
\]
Now, suppose the target was $1$, the result would be:
\[
(\textsf{time}, \text{result}) = (1,f(x_1)),(2,f(x_1)),(3,f(x_1)),...,(6,f(x_6))
\]
In dense streams of data, such a strategy leverages temporal correlations where the last known observation is likely similar to the missing data.
This allows the prediction stream to fail soft in the presence of jitter and temporary failures.

% \subsubsection{Horizontal Rate Matching}
% Horizontal rate matching describes a problem where within a single topic, the constituent data streams arrive at different rates.
% We take a simple approach where we match the rate of the fastest incoming stream.
% Headers from slower streams can be matched to multiple headers of faster streams as long as those matches respect time ordering.

% \subsubsection{Vertical Rate Matching}
% There is further a rate-matching problem where a model consuming the data might not be able to consume the data as fast as it is produced. 
% We encourage users to set the \texttt{prediction\_freq} parameter, so \sys is able to automatically downsample the data to make real-time predictions. 
% This basically means that certain header messages are ignored, and thus lazy data routing saves us from transferring those data payloads.




% \subsection{Prediction Frequency Synchronization}
