\section{Introduction}
Serving predictions from machine learning models is a crucial part of modern software applications ranging from automatic fraud detection to predictive medicine~\cite{tfcasestudies}. 
Accordingly, a number of \emph{model serving frameworks} have been developed, including Clipper~\cite{clipper}, TensorFlow Serving~\cite{tfserving}, and InferLine~\cite{inferline}. 
These frameworks simplify the deployment and interfacing of trained machine learning models with a service-oriented interface.
Typically, they provide a RESTful API that accepts features as inputs (i.e., a prediction ``request''), and responds to these requests with predicted labels (i.e., a prediction ``response'').
These frameworks provide a number of crucial optimizations such as containerizing inference code~\cite{clipper}, autoscaling~\cite{tfserving}, and model ensembling~\cite{inferline}.

Existing model serving frameworks were envisioned as components in cloud-based deployments.
Implicit to this design, there are several key assumptions: (1) prediction requests arrive asynchronously through the RESTful interface, (2) the request is self-contained with all of the features necessary to issue a prediction, (3) and the design prioritizes scalability over the latency of an individual request.
We find that a number of emerging latency-sensitive use cases challenge this paradigm, especially in the fields of sensing, quantitative finance, and network security.
For these use cases, it is more convenient to think of a machine learning model as an operator applied to one or more continuous streams of data with timeliness and synchronization constraints. 

To the best of our knowledge, the academic literature on this topic is relatively sparse with most existing work in video analytics~\cite{zhao2023streaming, kang2017noscope, arulraj2022accelerating, horchidan2022evaluating,flinkml, tfkafka}. 
In particular, significant technical challenges arise when the relevant features for a machine learning model are generated on different network nodes than where the model is served.
The data has to get to ``the right place at the right time'' before any prediction can be made, and this communication quickly becomes the primary bottleneck.
The problem is further complicated where there are multiple data streams: the data streams have to be time-synchronized and integrated before any predictions can take place.
Prior work has shown that placement and synchronization decisions affect both performance and accuracy in nuanced ways~\cite{shaowang2021declarative, shaowang2022bidede}.

To better understand the complexities of model serving in such settings, consider the following example.

\begin{example}
In network intrusion detection, machine learning models applied to packet capture data are used to infer anomalous or malicious traffic patterns. Most organizations have geo-distributed private networks spanning multiple clouds and regions. The relevant features for a particular intrusion detection model may be sourced from different packet capture streams at different points in the network.
These streams will have to be synchronized and integrated to make any global prediction.
\end{example}

With existing tools, building such applications requires significant developer effort in the design of (1) communication between nodes, (2) compliance with timing constraints, and (3) computation placement.  
(1) In most networks, all-to-all communication is infeasible or prohibited (e.g., by firewall policy), hand-designed data routing strategies have to account for network topology which potentially changes. 
(2) Even if the developer can engineer a way to aggregate multiple streams of features in a given network, these streams are produced and communicated at different rates. The developer further has to design a protocol to match the streams in a time-aligned way with appropriate message and rate matching.
(3) Finally, machine learning models rarely consume raw data, and often data have to pass through one or more computational steps before features are produced. The developer has to decide where in the network to compute the features -- before transmission or after transmission.
In short, we believe that a number of crucial ML applications are simply infeasible today due to the engineering effort in model deployment. 
\textbf{These challenges point to a missing machine learning system that can flexibly (and dynamically) place model-serving tasks on a network and route these input data streams accordingly while conforming to any data movement constraints.}

This paper describes a first step towards such a system, called \sys, that addresses this need.
Instead of a RESTful service that handles each prediction request asynchronously, \sys routes synchronized streams of data to models that are flexibly placed anywhere in a network.
We call such an architecture \emph{decentralized prediction} to differentiate it from classical model serving, where a collection of independent model-serving nodes work together to serve predictions over data streams.
\sys provides a lightweight inference service that can be installed on every node of the network.
It further provides a low-latency data router that can connect streams with these models.
These data streams can be time-synchronized so that inferences that need to look at a particular snapshot in time can appropriately construct features that join data from different sources.
Furthermore, the data can be derived from primary sources (e.g., sensors, user data streams, etc.) or can be results of computation (e.g., features/predictions computed from pre-trained models).
This flexibility allows users to build complex but robust predictive applications in networks with heterogeneous and disaggregated resources. 

Not only does \sys simplifies the deployment process of models over data streams, it actually allows for new machine learning opportunities.
Consider the following approach to the example above.

\begin{example}
Rather than a single network intrusion detection model that integrates all the data, one decomposes this model into an ensemble of small models. Each packet capture node of the network has a node-specific model that makes a prediction. Instead of communicating the features, the predictions from these local models are communicated to the desired location. These predictions can be ensembled with a lightweight model to make a single final decision. Such a deployment reduces communication and more effectively utilizes parallelism.
\end{example}

\sys provides a single system that can express both the centralized solution (e.g., all packet capture data communicated to a single model-serving framework) or the decentralized solution (e.g., the predictions of local models ensembled together).
This paper describes the core systems primitives needed to implement such a framework in a rigorous and reliable way. We believe that it is a step towards the class of machine learning systems that are needed for a future of ubiquitous sensing, AR/VR, and eventually autonomous vehicles.


To summarize the core technical contributions:

\begin{itemize}
    \item \emph{Declarative Communication. } 
    \sys employs a message broker to route data around different nodes, allowing multiple producers and consumers to operate on the same message queue simultaneously. Users can define data movements and model placements by pointing models to named streams of data rather than their physical locations. 
    Furthermore, the user can program her model and featurization as if there was all-to-all communication in the network, and the actual data routing is handled seamlessly by \sys.


    \item \emph{Lazy Data Routing. }  \sys applies an innovative communication protocol called ``lazy data routing''. In this protocol, nodes that produce data send headers to a message broker. Nodes that receive data observe these headers and can choose to transfer the payload in a peer-to-peer fashion. 


    \item \emph{Optimization Based on Timing Hints. } \sys provides an API to allow users to specify two timing constraints: (1) a target prediction rate of each model, and (2) a maximum synchronization skew between streams. \sys is able to automatically meet these rates by adaptively downsampling and buffering incoming streams. 
\end{itemize}


%There is, of course, much work to be done, but the goal of this paper is to highlight an interesting problem in multimodal inference problems.

