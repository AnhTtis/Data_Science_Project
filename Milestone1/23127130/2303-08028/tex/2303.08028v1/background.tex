\section{Background}
In summary, existing model serving frameworks struggle with \emph{decentralized} and \emph{streaming} inference.

\subsection{What is Decentralized and Streaming Inference?}

\noindent \textbf{Inference over a Single Stream. } Consider a supervised learning inference task. Let $x$ be a feature vector in $\mathcal{R}^p$ and $f_\theta$ be a model with parameters $\theta$. $f_\theta$ evaluates at $x$ and returns a corresponding prediction $y$, which is in the set of labels $\mathcal{Y}$. A prediction over a stream of such feature vectors can be thus summarized as:
\[ y_t = f_\theta(x_t) \]
where $t$ denotes a timestamp for the feature vector. In such a prediction problem, the user must ensure that the featurized data is at ``the right place at the right time'': $f_\theta$ has to be hosted somewhere in a network and $x_t$ has to be appropriately generated and sent to $f_\theta$.

\noindent \textbf{Inference over Multiple Streams. } Now, let's imagine that $x_t$ is constructed from multiple different streams of data. Each $x_t$ (the original features) can be treated as a concatenation of $d$ individual streams:
\[
x_t = \begin{bmatrix}
x^{(1)}_t &
... &
x^{(i)}_t &
... &
x^{(d)}_t  
\end{bmatrix} \
\]
Each of these streams of data $x^{(i)}_{1},...,x^{(i)}_t$ might be produced on a different node in a network.
Consider the activity recognition running example. Each $x^{(i)}$ corresponds to one of the streams of data (packets from node 1, packets from node 2, packets from node 3).
In this case, we have different streams of data $x^{(1)},x^{(2)},...$ coming in, and we need to aggregate them so that the final prediction arrives in our desired destination node.

\noindent \textbf{Decentralized Inference. } In decentralized inference, the goal is to leverage computational resources of the entire network for making a prediction, not just the resources of the node hosting the model-serving service. Imagine that some of the computation involved in $f_\theta$ can be approximated in the following way. For each data stream, there is a $g_i$ that can be computed locally without information about any other source:
\[
\hat{x} = \begin{bmatrix}
g_1(x^{(1)}) &
... &
g_i(x^{(i)}) &
... &
g_d(x^{(d)})  
\end{bmatrix} \
\]
While local, each $g_i$ should still be computing important higher-level features that can be used to reduce the complexity of the integrated prediction.
For example, each $g_i$ could be a pre-trained model that computes features on a high-dimensional data source.
Each $g_i$ could even be an entire model, and then the integration prediction simply has to ensemble the local predictions together. 
Whatever each $g_i$ represents, these transformed streams can be combined into a single final prediction with a simpler combination model $h$:
\[ \hat{y} = h(\hat{x}) \]

\emph{The premise of decentralized prediction is to design such approximations to the centralized prediction problems by distributing some of the computation throughout the network via these $g_i$ functions.}
This strategy will approximate the true centralized solution:
\[ y = f_\theta(x)  ~ \approx ~  \hat{y} = h(\hat{x}) \]
While $\hat{y}$ may not match $y$, it might have lower latency and/or lower network costs. 
The more approximation that can be tolerated, the lower end-to-end latency we can possibly offer, up to the rate of incoming data.

Decomposing a model into compartmentalized units that run on different subsets of features can offer a number of important systems benefits.
\begin{enumerate}
    \item Communication reduction. The outputs of each $g_i$ can be much lower-dimensional than the original features leading to significant reductions in communication.
    \item Parallelism. The decomposed model gives the system more placement flexibility, thus improving the utilization of the whole distributed network.
    \item Pipelining. Since each sub-model can execute independently over data streams, their execution can be effectively pipelined to improve utilization.
\end{enumerate}

Let us see how this formalism works in the network intrusion detection example. 
First, we partition the training dataset into packet captures from each node.
Then, we train a model on each individual stream of data to predict the overall label. 
Each source-specific model will be less accurate, but after training, each node producing data can independently run the source-specific model.
With this architecture, rather than streaming the features to a central location, we can stream the local predictions.
At the destination, we finally apply a lightweight ensembling technique to produce a final intrusion detection classification.
That ensembling method can be a simple majority vote, or it could be learned from data as well.

\textbf{\sys provides a simple API where such decentralized prediction behavior is easy to express.}
While such a prediction scheme seems intuitive, today's model serving systems lack the support for flexibly deploying models (or partial models) across a network and routing data and predictions to/from them.
Users with such problems today have to design bespoke solutions, which can result in brittle design decisions that are not robust to changes in the network or data.
This paper proposes a system to help users deploy models in such decentralized data environments where the relevant features are collected on different nodes in the network.
We further talk about the different types of $g_i$ decompositions that can be used to effectively approximate centralized solutions for a fraction of the cost.

\subsection{Time-Synchronization}
While it is true that prior work has considered decomposing models across a network to optimize throughput~\cite{narayanan2019pipedream}, the multiple streams of data considered in this work creates a novel challenge.
If the streams of sub-features are collected independently, they will likely not be time-synchronized. This means, at any given instant, the data at the prediction node comes from a slightly different timestamp:
\[
x_t = \begin{bmatrix}
x^{(1)}_{t+\epsilon_1} &
... &
x^{(1)}_{t+\epsilon_i} &
... &
x^{(d)}_{t+\epsilon_d}  
\end{bmatrix} \
\]
Each $\epsilon_i$ denotes a positive or negative offset. The overall time-skew of the prediction problem is $\epsilon = max_{i} |\epsilon_i|$. In other words, to issue a perfectly synchronized prediction at time $t$, one has to wait for at least $\epsilon$ steps to ensure all the right data is available.
This can be even more complicated if different data streams are collected at different frequencies.
\sys provides an API for controlling synchronization errors in decentralized prediction deployments.

\subsection{Scenarios}
There are a number of scenarios where machine learning on such decentralized streams arises.

\begin{enumerate}
    \item \emph{Network Intrusion Detection.} As in our example, in network intrusion detection, features from packets that are captured on different nodes in a network have to be integrated to make a prediction about security. This topology creates multiple streams of data that must be coordinated for a combined decision. 
    
    \item \emph{Sensor Fusion.} Distributed streams also arise in sensor fusion tasks. Consider a model that identifies ongoing activities in an office space. It classifies these activities with three streams of data: audio, video, and network traffic. Audio and video are collected on one device and network traffic is collected on another. We have a neural network model that requires all three data sources to recognize ongoing activities in a building.
    
    \item \emph{Quantitative Finance.} In systematic trading, multiple independent streams of data, such as stock prices, options data, and news events data are combined to predict the movement of a stock price. Since these data formats are very different and their associated pre-processing steps are different, each data stream might be processed in its own computational container.
\end{enumerate}

\subsection{Related Work}
Current machine learning model serving systems, including Clipper~\cite{clipper}, TensorFlow Serving~\cite{tfserving}, and InferLine~\cite{inferline}, all assume that the user has manually programmed all necessary data movement.
Recent systems have begun to realize the underappreciated problem of data movement and communication-intensive aspects of modern AI applications. However, they have yet to address the trade-offs in time-synchronization between different data sources when they do not arrive at the same time. For example, Hoplite~\cite{hoplite} generates data transfer schedules specifically for asynchronous collective communication operations (e.g., broadcast, reduce) in a task-based framework, such as Ray~\cite{ray} and Dask~\cite{dask}.

The closest existing tools are those designed for distributed training of ML models. TensorFlow Distributed~\cite{TensorFlow}, for example, allows both all-reduce (synchronous) and parameter server (asynchronous) strategies to train a model with multiple compute nodes.
Another popular framework, PyTorch distributed~\cite{pytorch-distributed}, supports additional collective communication operations such as gather and all-gather with Gloo, MPI and NCCL backends. One might ask, can we perform distributed inference using these existing distributed training frameworks? Technically it is possible, but as we will show in Section~\ref{sec:exp-opportunity}, the performance is unsatisfactory because such frameworks are optimized for maximum throughput but not end-to-end timeliness. 

Our work of decentralized prediction might seem similar to federated learning~\cite{DBLP:journals/corr/KonecnyMR15, MLSYS2019_bd686fd6}, but there are several key differences. First, our goal is not to collaboratively train a shared model, but to make combined predictions based on multiple streams of data. Second, we optimize for millisecond-level end-to-end timeliness from the point of data collection to the point where prediction is delivered. Federated learning tasks usually assume a much longer end-to-end latency, and they have other optimization goals, such as communication cost. Third, we have to take care of time-synchronization between data streams, while federated learning systems usually treat those data as the same batch.

On the other hand, there has been a steady trend towards moving model serving to resources closer to the point of data collection, or the ``edge''. 
The primary focus of model serving on the edge has been to design reduced-size models that can efficiently be deployed on lower-powered devices~\cite{han2015deep, edgedrive, videoEdge, distream}.
Simply reducing the computational footprint of each prediction served is only part of the problem, and these tools do not support data routing when the relevant features might be generated on different edge nodes.

Similarly, this problem is more than just a stream processing problem.
Traditional relational stream processing systems, e.g., ~\cite{chandrasekaran2003psoup}, have stringent requirements for temporal synchronization where they model such an operation as a temporal join. 
These systems will buffer data, indefinitely if needed, to ensure that corresponding observations are properly linked.
While desirable for relation query processing, this approach is excessive in machine learning applications which have to tolerate some level of inaccuracy anyway. Moreover, multi-modal machine learning inference usually involves data sources generated at different rates.
In this setting, a looser level of synchronization would benefit the system and improve performance.

In the context of sensing, ROS (Robot Operating System)~\cite{quigley2009ros} is an open-source framework designed for robotics research. It incorporates an algorithm called \texttt{ApproximateTime} that tries to match messages coming on different topics at different timestamps. This algorithm can drop messages on certain topics if they arrive too frequently (downsampling), but does not use any message more than once (upsampling). In other words, if one sensor sends data very infrequently, the algorithm will have to wait and drop messages from all other sensors until it sees a new message from the low-frequency sensor to issue a match. The frequency of combined prediction is thus upper-bounded by the most infrequent sensor.
On top of that, such a wait can harm end-to-end timeliness and accuracy, especially in a synchronization-sensitive scenario where high-frequency information is lost.

There is further a connection to ``pipeline'' parallel machine learning model training~\cite{narayanan2019pipedream}. However, as an inference system, \sys is latency-optimized rather than throughput-optimized.
Our experiments show that when PyTorch is deployed in a setting similar to that in pipeline-parallel training, \sys has a significantly lower end-to-end latency.