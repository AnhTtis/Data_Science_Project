\iffalse
\section{Future Work}
\subsection{Placement Optimization}
Beyond the execution layer, there are a number of higher-level optimization challenges. A natural extension to our system is to optimize placement and find the best ``role'' for each node given its hardware specification, an idea that has been considered in prior work~\cite{shaowang2021declarative}. For example, GPU-equipped Jetson Nanos can be better candidates for GPU-enabled models than our NUCs; devices with faster I/O can better serve as data source nodes.

\subsection{Storage Distribution}
This paper also assumes infinite storage on each node. There can be another optimization problem if we have an asymmetric distribution of storage on the network because of our lazy messaging design. Our design requires that the raw data stays in the data source node until after either (1) the prediction node has fetched the raw data using its header or (2) the prediction node has skipped the header corresponding to the raw data. When and how to free the storage at data source nodes remains an unaddressed problem.

\subsection{Parameter Estimation}
While we try to achieve real-time processing of streaming data by applying the \texttt{predition\_freq} parameter, users still need to estimate the running time of their model themselves and set this parameter carefully by trial and error. If the prediction process takes much longer than this parameter, upsampled data could queue up and make the end-to-end latency even worse. This problem is harder for complex models whose running time depends on the actual input data, for example, neural network models with early exits. AutoML techniques may be used to offer a good estimation.

\subsection{Reproducibility}
Although \sys is able to aggregate multiple data streams with different rates, the result of such alignment is indeterministic and heavily depends on how the data would come in.
Users might want to debug certain errors involving \sys by replaying the decisions made by \sys. As a first step, we should log down all necessary information needed to reproduce \sys decisions, along with raw data collected from data source nodes. Some optimization can be applied as we can safely delete the data skipped by vertical rate matching. \todo{maybe cite some DB recovery papers and explain the differences?}

\subsection{Models for Decentralized Prediction}
There are also a number of interesting machine learning implications to these results. Decentralized prediction networks are particularly well-suited for mixture-of-expert and meta-learning models. We hope to explore these directions in future work.
\fi


\section{Conclusion}
There is a lot of community interest in distributed and federated problems in machine learning~\cite{DBLP:journals/corr/KonecnyMR15}.
We believe that the inference setting is under-studied despite a clear need in a number of different domains.
This paper focuses its discussion on the execution layer for decentralized prediction systems. 
The paper contributes the communication primitives needed to make such systems work in practice. We find that decentralized prediction can give system designers new knobs to enable real-time predictions.
Our results show: (1) a clear value for decentralization, (2) that \sys is an effective architecture for supporting decentralization, and (3) core techniques in \sys, such as lazy data routing, are broadly applicable to modern stream processing.
 

