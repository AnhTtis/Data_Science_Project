\section{Related Work}
Current machine learning model serving systems including Clipper~\cite{clipper}, TensorFlow Serving~\cite{tfserving}, and InferLine~\cite{inferline} all assume that the user has manually programmed all necessary data movement. Recent systems have begun to realize the underappreciated problem of data movement and communication-intensive aspects of modern AI applications, but have yet to address the trade-offs in temporal synchronization between different data sources when they do not arrive at the same time. For example, Hoplite~\cite{hoplite} generates data transfer schedules specifically for asynchronous collective communication (e.g., broadcast, reduce) operations in a task-based framework.~\cite{ray, dask}

Traditional relational stream processing systems, e.g., ~\cite{chandrasekaran2003psoup}, have very strict requirements for temporal synchronization where they model such an operation as a temporal join. 
These systems will buffer data, indefinitely if needed, to ensure that corresponding observations are properly linked.
While desirable for relation query processing, this approach is excessive in machine learning applications which have to tolerate some level of inaccuracy anyways. Moreover, multi-modal machine learning inference usually involves data sources generated at different rates.
In this setting, a looser level of synchronization would be beneficial to the system and improve performance.

In the context of sensing, ROS (Robot Operating System)~\cite{quigley2009ros} is an open-source framework designed for robotics research. It incorporates an algorithm called \texttt{ApproximateTime} that tries to match messages coming on different topics at different timestamps. This algorithm can drop messages on certain topics if they arrive too frequently, but does not use any message more than once. In other words, if one sensor sends data very infrequently, the algorithm will have to wait and drop messages from all other sensors until it sees a new message from the low-frequency sensor to issue a match.
On top of that, such a wait can be harmful to end-to-end timeliness and accuracy, especially in a synchronization-sensitive scenario where high-frequency information is lost.
If we assume that temporal correlation exists in sensor data, an alternative to this algorithm would be to reuse the last known value from the low-frequency sensor and match it with other sensors when they are ready.

\section{Trade-offs in Temporal Synchronization}\label{sec:tradeoffs}
In this section, we evaluate variable control of temporal synchronization in \sys by illustrating how different knobs might affect timeliness and synchronization errors.
Our key question here is, how do we loose our synchronization constraints to an extent that we are still able to make accurate and timely inferences?
\subsection{Knob 1. Time Interval for Message Matching}\label{sec:diss-time-interval}
The message broker has the privilege of retaining a message until it receives matching messages from other sensors, leaving room for a matching algorithm like the one in ROS. The effect of such forced synchronization on the message broker side heavily depends on a reasonable time interval between predictions. If the time interval is too short, such synchronization can be ineffective without improving accuracy; if the time interval is too long, potential long waits can stretch end-to-end timeliness while harming accuracy due to lost high-frequency information in between. Finding such a task-specific time interval can be challenging and tedious for humans and we need a system to automate this process.
In order for such a system like ROS to implement a matching algorithm, it also has to eagerly queue up messages on the message broker side, which can face contention when such messages are large in size. Experiments in Sec.~\ref{sec:exp-queuing} show how such contention affects latency. We show that a lazy method of message passing can significantly mitigate the effects of contention.

\subsection{Knob 2. Down-sampling}\label{sec:diss-downsample}
Down-sampling, or temporally sampling the data from each sensor, can drastically improve end-to-end timeliness by reducing data transfer when communication is costly for the task.
In down-sampling, there is a rate-limit on how much data each data source produces.
The consumer can still predict at a faster rate by using a last known observation from a source if needed.
Not only does down-sampling reduce computation and communication, it also increases the time between messages which allows the system to tolerate more variability (and thus better temporal synchronization).
So, the basic tradeoff is whether the loss in accuracy due to down-sampling is worth the better synchronization.
Interestingly enough, the answer seems to be ``sometimes''. Experiments in Sec.~\ref{sec:exp-downsample} show how down-sampling affects latency and accuracy.


\subsection{Knob 3. Compute Placement}\label{sec:diss-compute-placement}
It seems a good idea to place computation closer to the point of data collection, at least from the perspective of improving end-to-end timeliness for communication-intensive tasks. However, in real-world use cases, the latency of such computation varies across nodes and can be very hard to predict in advance. Such variability in latency can become a new source of synchronization problem and result in lower accuracy. Experiments in Sec.~\ref{sec:exp-placement} show how compute placement affects latency and accuracy.

\subsection{Knob 4. Lossy Compression}\label{sec:diss-lossy-compression}
Similar to down-sampling, lossy compression also affects synchronization. Compression can also significantly reduce file size and ease the burden of communication between nodes, improving end-to-end timeliness for communication-intensive tasks.
Smaller network payloads are less susceptible to network jitter. 
Furthermore, compression requires some non-trivial computation and acts as a rate-limit on the data source side, much in the same way down-sampling does. Again, if the consumer wishes to predict faster than this rate-limited source the last-known observation can be used. Experiments in Sec.~\ref{sec:exp-downsample} show how lossy compression affects latency and accuracy.
