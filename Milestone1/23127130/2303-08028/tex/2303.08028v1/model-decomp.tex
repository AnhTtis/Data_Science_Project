\subsection{Model Decomposition}\label{sec:model-decomposition}
While this paper focuses on system optimizations, it is worth devoting some time to understanding what types of machine learning models can effectively use a decentralized serving system. We will show it is relatively straightforward to do these decompositions in a generic way, and thus, is not a primary contribution of this paper.

Since models are the unit of placement and computation in \sys, the goal of model decomposition is to increase opportunities for optimizing placement. The idea is to approximate a single model with an ensemble or mixture of smaller local models.

\subsubsection{Strategy 1. Ensemble Models}
Ensemble machine learning models are techniques that combine multiple models to improve the accuracy and robustness of predictions. The idea behind ensemble modeling is that by combining the predictions of multiple models, we can reduce the risk of individual models making incorrect predictions and improve the overall performance of the model.

\emph{We can use an ensemble of models over different feature partitions to create decomposed models that can effectively take advantage of \sys.} Let's imagine that we have $p$ features and $n$ examples with an example matrix $X$ and a label vector $Y$.
Different subsets of these features are constructed on $m$ data sources on the network. Each source generates a partition of features $f_i$, i.e., $X[:, f_i]$ is the source-specific projection of training data. 

Stacking is an ensembling technique where multiple models are trained, and their predictions are used as inputs to a final model. The final model learns to weigh the predictions of each model and make a final prediction based on the weighted inputs. This helps capture the strengths of each individual model and produce a more accurate prediction.

We can train the following models. For each feature subset $f_i$, we train a model (from any model family) that uses only the subset of features to predict the label.
\[
g_i \leftarrow \textsf{train}(X[:, f_i], Y)
\]
After training each of these models over the $m$ subset, we train a stacking model that combines the prediction. This is a learned function of the outputs of each $g_i$ that predicts a single final label:
\[
h \leftarrow \textsf{train}([g_1,...,g_m], Y)
\]

Stacking models are well-studied in literature and are not new~\cite{sagi2018ensemble}. For multi-modal prediction tasks, our prior work has found that such models do not sacrifice accuracy and sometimes actually improve accuracy~\cite{shaowang2023amir}. A stacked model is a form of regularization that does not learn complex interactions across sources. In cases where some individual sources are unreliable or noisy, a stacked model is often more robust than a monolithic one. Regardless, stacking gives a generic training tool to decompose a single model into smaller source-specific models.  


\subsection{Strategy 2. Mixture of Experts Models}
Similarly, there are neural network architectures that can be trained end-to-end to take advantage of \sys. Mixture of Experts (MoE) is a deep learning architecture that combines multiple models or ``experts' to make predictions on a given task. Each expert is a specialized neural network that excels at solving a particular aspect of the problem. The MoE architecture uses a gating network to determine which expert should be used for a particular input.

The basic idea of the MoE architecture is to divide the input space into regions and assign an expert to each region. The gating network takes the input, decides which region it belongs to, and then selects the corresponding expert to make the prediction. The gating network then weights the output of each expert, and the final prediction is the weighted sum of the expert predictions. MoE architectures have been applied to a wide range of tasks, including language modeling, image classification, and speech recognition~\cite{eigen2013learning}.

In the context of \sys, each expert can be placed independently once trained. Like the stacked model above, we can align the experts to feature subsets generated on different source nodes. However, unlike the stacked model, there is no restriction that the local models output interpretable predictions. These local models might simply output a vector to be combined on a different node.




