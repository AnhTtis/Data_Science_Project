\section{\sys Architecture and API}
\label{engineering}
\sys is a system that facilitates decentralized prediction applications.

\subsection{\sys Overall Workflow}
In \sys, models are functions that are repeatedly applied to streams of data.
Models can consume multiple streams of data, whereas data streams from multiple nodes may have to be aggregated in a central place.
A prediction task consists of utilizing the results of one or more models.
Each prediction task in \sys has \emph{locality constraints}, which describe where the final results have to be delivered.
Conceptually, there are constraints on where data are created and constraints on where the final predictions must be delivered. 
Within these constraints, the system has a significant amount of flexibility to optimize for different objectives.
For example, model inference need not be placed on the delivery node, and might be better suited for another node on the network that supports hardware acceleration.

While there are many interesting aspects to this architecture, we focus this paper on the data routing challenges such a system faces. They are a small part of a bigger workflow:
\begin{enumerate}
    \item (Specification) The user specifies the location of data sources (streams) on the network, the models that use these streams, and the location where results should be delivered. \textbf{Addressed in prior work~\cite{shaowang2021declarative}}.
    \item (Optimization) The system turns this high-level specification into a placement plan where models are assigned to nodes in the network. \textbf{Addressed in prior work~\cite{shaowang2021declarative}}.
    \item \textbf{(Execution, Focus of This Paper) The data streams are efficiently routed from sources to models to prediction destinations}.
\end{enumerate}
While data routing for stream processing is well-studied, there are key nuances that arise in the machine learning context (explained in the next section).

\subsection{Execution Layer API}
\sys uses the high-level network specification described above to generate low-level code that can stream data and predictions.
\sys runs as a process on every node in the network. 
It assumes that these nodes are connected via TCP/IP, and that all-to-all communication is possible.
Every node running \sys can potentially create and consume data streams, and run model inference.
One of the nodes is designated as the \emph{leader node} running our message broker backend.
\sys extends Apache Pulsar~\cite{apachepulsar} to build a low-latency message broker backend to transfer messages between nodes.
This is the node that coordinates message routing and maintains a canonical clock for the network.
This leader can be selected through a leader election algorithm (e.g., ~\cite{malpani2000leader}), or can simply be selected by the user.
The leader is also responsible for dispatching user-written code to the other nodes on the network.
Alternatively, the user can also choose to deploy custom node-specific code on each node herself.
In the following text, we describe the API using Python syntax.

\subsubsection{Data Streams API}
Any node on the network, including the leader, can register globally-visible data streams to the network.
All data in \sys are represented as infinite streams of data. 
These streams can be of any serializable data type and leverage Python iterator syntax.
\texttt{DataStream} objects are registered with a unique identifier describing the stream.
Other nodes on the network can read from this stream of data by accessing an iterator-like interface.

\texttt{DataStreams} are further grouped into ``topics'', which describe joint predictive tasks.
For example, the streams from ``sensor 1'' and ``sensor 2'' might be used for a particular model.
Grouping streams into a list of topics gives the system information on which streams have to be synchronized.
\begin{lstlisting}
DataStream( iterator,\ #fetches new data 
            leader,\ #the leader node 
            stream_id,\ #key
            topics,\ #topics
          )
\end{lstlisting}

In this example, the user has multiple sensors, and each of them is constantly generating a stream of data. We need to combine these streams of data together in order to make a prediction. To invoke \sys, the user simply needs to wrap each data stream as a Python generator and specify a \texttt{stream\_id} for each data stream.
For example, the following code registers a stream of sensor readings:
\begin{lstlisting}
node = 'leader-node:6650'

def stream():
    while True:
        yield sensor.read()

with DataStream(stream(), 
                node, 
                stream_id='sensor1', 
                topics=['my_model']) as ds:
    #do something here
\end{lstlisting}

From a user-interface perspective, the Data Stream API is node-specific code. 
Users write and deploy such code on every node on the network that is actively collecting data from some primary source.
This code registers these streams of data with the leader and allows other nodes to read from these streams.

\subsubsection{Models}
Over these streams of data, we would like to compute different machine learning inferences. A \texttt{Model} object encapsulates such computation. A \texttt{Model} consumes one or more input data streams, and outputs another data stream.

A \texttt{Model} is similar to the definition of a \texttt{DataStream}.  \sys iterates through the data stream over the user-defined task, and the outputs are automatically sent to another topic defined by \texttt{stream\_out}. This stream of predictions can be further combined into topics that other models consume. The \texttt{Model} object also contains prediction frequency information -- a target frequency of predictions to serve.
\begin{lstlisting}
Model( model,\ #inference function
       leader,\ #the leader node 
       topic_in,\ #the input streams
       stream_out,\ #the output stream
       topics_out,\ #grouping outputs
       prediction_freq\ #how fast predictions are needed
     )
\end{lstlisting}
Our \texttt{Model} API is specifically designed to simplify how ensembling methods work in practice. We treat ensembling just like another model, which takes other models' predictions as inputs, and our system is able to combine them together in a time-synchronized way. Users only need to focus on the actual ensembling algorithm and leave the synchronization details to our system.

As an example, the following code applies a model to a combined data stream of two sensors. The model here can be an ensembling model, too.
\begin{lstlisting}
node = 'leader-node:6650'

def task(sensor1, sensor2):
    combined_data = np.hstack((sensor1, sensor2))
    return model.predict(combined_data)

with Model(task,
           node, 
           topic_in='my_model', 
           stream_out='my_model_out'
           topic_out=[], 
           prediction_freq=30) as model:
    #do something
\end{lstlisting}

