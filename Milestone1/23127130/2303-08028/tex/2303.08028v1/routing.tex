\section{Execution Layer for Model Serving}
In this section, we describe how these disaggregated data streams are routed and time-synchronized.

\subsection{Lazy Data Routing}\label{sec:lazy}
In its simplest form, the leader acts as a message-broker system. 
Data streams publish data to the leader, and models consume data from the leader.
With this architecture, the message broker can quickly become a point of contention since it has to process all of the messages from all of the different nodes.
Furthermore, large message payloads (e.g., images) can lead to a crucial networking bottleneck at this leader (Section 5.2).

\sys uses a novel messaging protocol to efficiently transfer data between nodes without placing an undue burden on the message broker.
A message sent to the leader only contains message headers: a timestamp and a global source path.
The actual message payloads are not transferred; instead, they are kept and indexed on the node that collected the data.
A model subscribes to the message broker, reading the headers as they come in.
If it wants a particular data payload, it retrieves that data lazily from the source node.

Figure \ref{fig:lazy} illustrate this protocol. On a data collection, every data item added to a \texttt{DataStream} is annotated with a header (Figure \ref{fig:lazy}-1). We can think of this as a stream of $(h,d)$ tuples (header and data, respectively). After the tuple is created, the node locally writes the data to a time-indexed log (Figure \ref{fig:lazy}-2).
After this data is durably written, the header is published to the message broker on the leader (Figure \ref{fig:lazy}-3). 
Nodes on the network can subscribe to streams of these headers.
Model inference requires the data payload, and that can be requested from the headers (Figure \ref{fig:lazy}-4). This data is transferred in a peer-to-peer fashion, and inferences happen over these streams (Figure \ref{fig:lazy}-5).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/lazy.png}
    \caption{A figure illustrating the order of operations in the lazy data routing system used by \sys.\todo{make wording consistent: data source node / data node / source node? prediction node / inference node?}}
    \label{fig:lazy}
\end{figure}

Lazy retrieval has a number of important benefits for typical model-serving tasks. In general, these benefits are analogous to that of lazy computation. First, many models predict at rates much slower than the rate of data collection. For example, a model that takes 30ms to evaluate can only process one example every 30ms. If the data collection rate is significantly faster than that, the model often has to down-sample the input data. Lazy data routing allows us to avoid transferring the data payload to the leader in these cases. Second, the primary data transfer happens without coordination through the leader, and thus, gives us placement flexibility of placing model inference close to the data source while placing the leader far away.

Practically, every edge node has a limited amount of local storage. \sys handles by having a timeout for the data payloads, where a node on the network can only send a retrieval request within that timeout.
This allows the edge node to overwrite/free up that space periodically.
Depending on the network, we allow users to force \sys to have eager message passing. Small messages, such as 1D arrays, can be transferred from data source nodes to worker nodes via the leader node. Essentially, this embeds the payload in the message headers.

\subsection{Stream Alignment}
A key feature in \sys is to allow models to predict on multiple streams of data. 
For this to work, the streams of data must be time-aligned (or roughly so). 
As we saw in the last section, multiple disparate data streams can be organized into a single topic that a model can subscribe to.
\sys gives the programmer an illusion of stream alignment, namely, streams associated with the same topic can the thought of as synchronized from the perspective of machine learning modeling. 
The consuming models receive tuples of headers corresponding to data from each of the sources.
Figure \ref{fig:align} shows this point: from a programmer's perspective, the entire topic can be treated as a single stream.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/align.png}
    \caption{A model subscribes to a single topic which might include multiple streams of data. \sys ensures that multiple streams of data are time-aligned as tuples in order to make predictions.\todo{make wording consistent: data source node / data node / source node? prediction node / inference node?}}
    \label{fig:align}
\end{figure}

Under-the-hood, \sys has to buffer streams locally to keep up this illusion.
The different data streams will arrive at different rates and have different systems delays that cause misalignment.
We use a time interval-based interface for specifying alignment criteria.
Thus, every topic has a maximum allowed time-skew between headers that can be produced.
Locally, the buffer retains a header until it receives matching header messages from other sensors. 
Thus, we can enforce a bounded-skew synchronization on the model side. 
This heavily depends on a reasonable time interval between predictions. If the time interval is too short, such synchronization can be ineffective without improving accuracy; if the time interval is too long, potential long waits can stretch end-to-end timeliness while harming accuracy due to lost high-frequency information in between. 
Crucially, all of this synchronization can happen without the data payloads.

