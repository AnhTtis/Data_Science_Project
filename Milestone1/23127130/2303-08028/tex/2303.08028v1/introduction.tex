\section{Introduction}
Serving predictions from machine learning models is a crucial part of modern software applications ranging from automatic fraud detection to predictive medicine~\cite{tfcasestudies}. 
Accordingly, a number of \emph{model serving frameworks} have been developed, including Clipper~\cite{clipper}, TensorFlow Serving~\cite{tfserving}, and InferLine~\cite{inferline}. 
These frameworks simplify the deployment and interfacing of trained machine learning models with a service-oriented interface.
Typically, they provide a RESTful API that accepts features as inputs (i.e., a prediction ``request''), and responds to these requests with predicted labels (i.e., a prediction ``response'').
These frameworks provide a number of crucial optimizations such as containerizing inference code~\cite{clipper}, autoscaling~\cite{tfserving}, and model ensembling~\cite{inferline}.

Existing model serving frameworks were envisioned as components in cloud-based deployments.
Implicit to this design, there are several key assumptions: (1) prediction requests arrive asynchronously through the RESTful interface, (2) the request is self-contained with all of the features necessary to issue a prediction, (3) and the design prioritizes scalability over the latency of an individual request.
We find that a number of emerging latency-sensitive use cases challenge this paradigm, especially in the fields of quantitative finance, sensing, and network security.
For these use cases, it is more convenient to think of a machine learning model as a function applied to one or more continuous streams of data. 

Models as stream-processing components are only supported in a few experimental open-source projects, in part due to hidden complexities of serving inferences over streams of data~\cite{flinkml, tfkafka}.
Challenges arise when the relevant features for a machine learning model are aggregated from different streams created on different nodes in a network.
In this scenario, the data streams have to be time-synchronized and integrated somewhere first before any predictions can take place.
Where computation is placed and the degree to which the streams are time-synchronized can affect both performance and accuracy in nuanced ways~\cite{shaowang2021declarative, shaowang2022bidede}.
To make this concrete, consider the following example.

\begin{example}
In smart network intrusion detection, machine learning models applied to packet capture data are used to infer anomalous or malicious traffic patterns. Most organizations have highly decentralized private networks spanning multiple clouds and regions, so centralized packet capture is impractical. Thus, the relevant features for intrusion detection are sourced from data collected and aggregated on different nodes in the network. In current model serving paradigms, this ``feature-parallel'' design pattern requires that all the features are communicated to some central service and have to be time-synchronized on this service.
\end{example}

With existing tools, the reasoning about (1) data routing, (2) time synchronization, and (3) task disaggregation lies with the developer.
(1) In a scenario like the example above, the developer has to define how data are communicated from their sources to the model serving service.
Hand-designed data routing strategies are brittle when infrastructure or costs change.
(2) Even if the developer can engineer a way to aggregate multiple streams of features, these streams are produced and communicated at different rates. The developer further has to design a protocol to match the streams in a time-aligned way with appropriate message and rate matching.
(3) Finally, machine learning models rarely consume raw data, and often data have to pass through one or more computational steps before features are produced. Today, this featurization logic is usually encapsulated in the model-serving service, whereas it might be beneficial to disaggregate steps and flexibly place such computation.
These challenges point to a missing machine learning system that can flexibly (and dynamically) place model-serving tasks on a network and route these input data streams accordingly while conforming to any data movement constraints.


This paper describes a novel model-serving architecture that addresses this need.
Instead of a centralized service that handles each prediction request, we propose a decentralized service that treats every node in a network as a potential model-serving host.
We already have seen that the data can be decentralized -- they can be collected on different nodes in a network. 
By allowing model serving to be decentralized as well, the system allows for flexible placement strategies that align model placement with data locality and the available computational resources.
We call such an architecture \emph{decentralized prediction} to differentiate it from classical model serving, where a collection of independent model-serving nodes work together to serve predictions over data streams.

At first glance, this seems like a trivial parallelization of existing architectures, but the implications of flexible data routing and computation placement are significant for machine learning inference. Consider the following approach to example above.

\begin{example}
Rather than a single network intrusion detection model that integrates all the data, one decomposes this model into an ensemble of small models. Each packet capture node of the network has a node-specific model that makes a prediction. Instead of communicating the features, the predictions from these local models are communicated to the desired location. These predictions can be ensembled with a lightweight model to make a single final decision. Such a deployment reduces communication and more effectively utilizes parallelism.
\end{example}

This type of ensembled deployment is relatively straightforward from a machine learning perspective and is related to a number of well-known ideas such as mixture-of-experts models~\cite{eigen2013learning}. However, the systems support for such deployments is lacking and requires multiple model-serving nodes that can communicate with each other and time-synchronize.
This paper presents \sys, a prototype model serving system for decentralized predictions. 
The focus on the paper are the core systems primitives needed to service such deployments, and we show that techniques developed in the database community to handle distributed streams are highly relevant~\cite{franklin2005design}.

From an API perspective, \sys provides a lightweight inference service that can be installed on every node of the network.
It further provides a low-latency data router that can connect streams with these models.
These data streams can be time-synchronized so that inferences that need to look at a particular snapshot in time can appropriately construct features that join data from different sources.
Furthermore, the data can be derived from primary sources (e.g., sensors, user data streams, etc.) or can be results of computation (e.g., features/predictions computed from pre-trained models).
This flexibility allows users to build complex but robust predictive applications in networks with heterogeneous and disaggregated resources. 

To summarize the core technical contributions:

\begin{itemize}
    \item \emph{Flexible Data Movement and Model Placement. } \sys employs a message broker to route data around different nodes, allowing multiple producers and consumers to operate on the same message queue simultaneously. Users can define data movements and model placements in a asynchronous manner by designating a logical queue as the destination of an operator, instead of a physical node to send output data to. In this way, each node is agnostic of other parts of \sys and the sender does not need to wait for an available receiver in case of contention.
    \textbf{We show that \sys, even with an additional message broker, performs better than the distributed communication API provided by PyTorch distributed.}

    \item \emph{Lazy Data Routing. }  \sys applies an innovative communication protocol called ``lazy data routing''. In this protocol, nodes that produce data send headers to a message broker. Nodes that receive data observe these headers and can choose to transfer the payload in a peer-to-peer fashion. This data routing approach allows the receiver to ``gate'' data transmission based on its needs, e.g., if a model can only predict at 10 predictions per second, but data arrives at 100 data items per second.
    \textbf{We show that lazy data routing ensures data movement unaffected by network congestion at the leader node.}
    \textbf{We also show that lazy data routing significantly reduces data communication time when some data is skipped by our automatic rate matching described below. }

    \item \emph{Stream Alignment and Adaptive Rate Control. } Time synchronization of the disaggregated data streams is a significant bottleneck. \sys provides an API to allow users to tolerate a bounded synchronization delay in exchange for improved latency. Note that such synchronization does not need the payload but only the message headers, which further motivates the design above. \sys is able to automatically match the rate of different data streams with a user-defined prediction rate by adaptively upsampling/downsampling incoming streams. \textbf{We show that \sys can meet different prediction rate targets with synchronization and/or frequency constraints with lower end-to-end latency than existing systems while preserving accuracy.}

    % \item \emph{Pipelined Execution for Model Serving. } With the flexible data routing and stream alignment, \sys can execute model inference steps in a pipelined fashion leading to better overall system utilization. \textbf{We show that \sys has a significantly higher compute utilization compared to existing systems.}
\end{itemize}


%There is, of course, much work to be done, but the goal of this paper is to highlight an interesting problem in multimodal inference problems.

