

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/sync1.png}
    \includegraphics[width=\columnwidth]{figures/sync2.png}
    \includegraphics[width=\columnwidth]{figures/sync3.png}
    \caption{(A) Temporal synchronization means that observations created at the same (or roughly the same) time are paired together during multimodal inference. (B) The same idea can be extended to windows of data if different sensors produce data at different rates. (C) An example of synchronization breaking due to an irregular data processing schedule.}
    \label{fig:sync}
\end{figure}

\vspace{0.25em}\noindent \textbf{Sensor Fusion and Multi-modal Prediction. } A machine learning model is \emph{multi-modal} if it requires data from more than one data source. For example, a model that leverages multiple cameras for tracking a box in a warehouse. Or, a model that integrates information from multiple sensors to detect room occupancy.

\vspace{0.25em}
Fundamentally, real-time sensor fusion requires temporally corresponding data to be ``in the same place at the same time'' somewhere in an edge network. \sys is a system designed to address this constraint by ensuring that right data is routed to each model. Figure \ref{fig:sync} illustrates the concept of temporal synchronization. Ideally, sensor measurements from exactly the same time point should be paired during multi-modal inference (Figure \ref{fig:sync}A-B). Since in any real distributed sensing system this is impossible, there are only degrees of synchronization that can be achieved (i.e., how similar are the timestamps of the observations from different sources). 


\subsection{Basics and Workflow}
We assume that each edge \emph{node} is connected to others on a TCP/IP network (either directly or via a switched network). A subset of these nodes are physically connected to data sources (e.g. video cameras, sensors, and other data streams).
Every node maintains a globally-synchronized catalog of data streams that are locally collected.
In \sys, \emph{models} are functions that are repeatedly applied to fixed windows of data.
Every model in \sys has \emph{locality constraints}, which describe where a model's prediction results have to be delivered.
For example, one could require that all predictions are delivered to \textsf{node1}.
Or, we could require that either \textsf{node1} or \textsf{node2} has the required output.

Unlike existing model-serving systems that work asynchronously, \sys works in a push-based model, where the arrival of each new data batch (defined by the user's inference task) triggers re-evaluation. 
These tasks subscribe to a message-broker service, which informs each node about new data. Each model can consume one or more sources of data and yield a new stream (a prediction). 
Since there is a global message-broker service, the output of models can be streamed to other models as well.
Models that consume multiple streams of data induce additional locality constraints, where data streams from multiple nodes may have to be aggregated in a central place.
For example, an activity recognition model that requires video, audio, and network data must aggregate all of the data in a single place somewhere in the network.
At a high level, \sys combines a publication-subscription system to facilitate communication between multiple model-serving nodes on a network. To the best of our knowledge, such a system does not exist in part due to the challenges in routing, scheduling, and placement. The key system goal of \sys is to provide a centralized control plane to find placement, routing decisions, and model partitioning decisions that satisfy locality and hardware constraints.

\subsection{Metrics}\label{sec:obj}
The main performance metric that we care about is \emph{timeliness}, which is the time delay between data arrival and the prediction results arriving at the appropriate node in the edge network (independent of exactly where and how \sys chose to execute that process).

\vspace{0.25em} \emph{Time-to-Decision (Timeliness). } Unlike existing model serving systems where predictions are triggered by client requests, \sys will continuously process predictions over streams of incoming data. Therefore, the concept of ``latency'' is a little more complicated in this setting. Accordingly, we define a new metric called timeliness, which is the gap between the time at which the data arrived and when a prediction was issued. The start time is defined as the time point at which all of the relevant data for a particular prediction is available somewhere on the network, and the end time is the time at which the prediction is issued and communicated to the appropriate edge node that can use the prediction. 
\vspace{0.25em}

The focus on model-serving makes the design of \sys particularly interesting, because optimization decisions that improve the timeliness of predictions may affect their accuracy: 
\begin{enumerate}
\item \emph{Prediction Accuracy (Accuracy). } We also care about the accuracy of the predictions that are made, or the gap between the prediction of a class or a continuous label and (hypothetical) ground-truth.

\item \emph{Robustness to Failure (Robustness). } Finally, it is also important to consider robustness to network and node failures. These failures can affect both the placement of computation and the availability of source data. Robustness is measured in terms of the number and type of edge nodes that can be lost while still issuing a prediction.
\end{enumerate}
All three of these metrics have both systems and machine learning implications.
For example, there are systems solutions to improving timeliness through batching and locality, but there are also machine learning solutions where different model types have latency characteristics.
Similarly, systems techniques like replication can help tolerate failures, but robust machine learning techniques can also allow for issuing predictions even if some of the features are lost.


% you can find the update-able diagram here
% https://whimsical.com/edgeservesysdesign-45wUqVe9RPDrcNeeHPyMQ3 

\subsection{Implementation}\label{sec:arch}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\columnwidth]{figures/EdgeServeSysDesign.png}
    \caption{Architecture of the \sys system.}
    \label{fig:sysdesign}
\end{figure}

To better address the aforementioned challenges in routing, scheduling, and placement, we design our system with the following principles in mind.
\subsubsection{Declarative specification of the system}
We provide a user interface for the user to configure the system in a declarative manner. Users can specify the topology of the network, e.g., which nodes are data sources, which nodes are edge nodes and how are they connected; the tasks that are to be processed, e.g., which data sources should be combined and which model should be used for processing; and the objectives of the system, e.g., should the system aim for better timeliness or better accuracy.
\subsubsection{Declarative serving and processing}
The challenging decision-making for the underlying routing, scheduling, and placement decisions should be hidden from the user and automatically determined by an optimizer given user-specified network topology, task specification, and objective.
\subsubsection{Efficient communication}
A central message broker is used to relay messages among the optimizer, the data source, and the edge nodes. Unlike other systems that transfer the entire data payload using a message broker, in our system, only the header of the data is distributed to the edge nodes and the payload is lazily accessed on demand by the edge nodes that are responsible for processing the payload.  
\subsubsection{Extensible tasks}
A task generator is built to use some of the user input to generate tasks that contain a unified interface used by the edge nodes. This can decouple the optimizer from the user configuration which makes it easy to support new types of tasks. A code manager is also integrated to serve the same purpose, by exposing APIs for edge nodes to access the right code or model used by a task, the code can be white-boxed (Python code) or even black-boxed (e.g., a Docker container).
 
\subsection{Example Execution}
Let's consider the warehouse asset tracking example in the introduction. 
There are three \emph{streams} of data: audio, video, and network traffic.
Audio and video are collected on \textsf{node1} (an Intel Video Processing Embedded System) and network traffic is collected on \textsf{node2} (a programmable wireless access point).
We have a \emph{model} which is a neural network that requires all three data sources to predict ongoing activities in the warehouse.
To issue such predictions, the system could create a data flow (via publication and subscription) that repeatedly transfers raw data from \textsf{node2} to \textsf{node1}, and host a comprehensive model on \textsf{node1}. Alternatively, it could also featurize the network traffic data locally on \textsf{node2} and only transfer pre-processed features to \textsf{node1}. \textsf{node1} applies a pooling method to issue a prediction based on features from multiple sources. This allows the user to combine the sources of data for a richer prediction, as well as leverage the specialized prediction hardware on both nodes.