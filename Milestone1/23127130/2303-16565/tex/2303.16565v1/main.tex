% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[pagenumbers]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{color}
\usepackage{multirow}
\usepackage{float}
\usepackage{colortbl}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-Performance Cloud Removal from Multi-temporal Satellite Imagery}

\author{Xuechao Zou$^{1, }$\footnotemark[1], Kai Li$^{2, }$\footnotemark[1], Junliang Xing$^{2}$, Pin Tao$^{1,2, }$\footnotemark[2], Yachao Cui$^{1}$\\
$^1$Department of Computer Technology and Applications, Qinghai University, Xining, China\\
$^2$Department of Computer Science and Technology, Tsinghua University, Beijing, China \\
{\tt\small xuechaozou@foxmail.com, lk21@mails.tsinghua.edu.cn}, \\ 
{\tt\small \{jlxing,taopin\}@tsinghua.edu.cn, yachaocui@163.com}}
\maketitle

\footnotetext[1]{These authors have contributed equally to this work.}
\footnotetext[2]{Corresponding author.}
%%%%%%%%% ABSTRACT
\begin{abstract}
    % Satellite imagery analysis is an important area of study in remote sensing, but the information loss caused by cloud cover significantly hinders the application of satellite imagery. Therefore, cloud removal is the critical initial step in most satellite analytic pipelines. However, existing methods have restricted receptive fields and only focus on local information, leading to suboptimal outcomes. In this study, we present an efficient autoencoder network for cloud removal that uniquely considers both global and local information, named Progressive Multi-Scale Attention Autoencoder (PMAA). It mainly comprises the Multi-scale Attention Module (MAM) and Local Interaction Module (LIM). PMAA establishes the long-range dependency of multi-scale features using MAM and modulates the reconstruction of fine structures using LIM, allowing for the simultaneous representation of fine- and coarse-grained features at the same level. With the help of diverse and multi-scale feature representation, PMAA outperforms the previous state-of-the-art model CTGAN on the Sen2\_MTC\_Old and \emph{Sen2\_MTC\_New} datasets. Furthermore, PMAA has a clear efficiency advantage, with 0.5\% and 14.6\% of the parameters and computational complexity of CTGAN, respectively. These extensive results highlight the potential of PMAA as a lightweight cloud removal network suitable for deployment on edge devices.

% abstract v2: xing
    Satellite imagery analysis plays a vital role in remote sensing, but the information loss caused by cloud cover seriously hinders its application. This study presents a high-performance cloud removal architecture called Progressive Multi-scale Attention Autoencoder (PMAA), which simultaneously leverages global and local information. It mainly consists of a cloud detection backbone and a cloud removal module. The cloud detection backbone uses cloud masks to reinforce cloudy areas to prompt the cloud removal module. The cloud removal module mainly comprises a novel Multi-scale Attention Module (MAM) and a Local Interaction Module (LIM). PMAA establishes the long-range dependency of multi-scale features using MAM and modulates the reconstruction of the fine-grained details using LIM, allowing for the simultaneous representation of fine- and coarse-grained features at the same level. With the help of diverse and multi-scale feature representation, PMAA outperforms the previous state-of-the-art model CTGAN consistently on the \emph{Sen2\_MTC\_Old} and \emph{Sen2\_MTC\_New} datasets. Furthermore, PMAA has a considerable efficiency advantage, with only 0.5\% and 14.6\% of the parameters and computational complexity of CTGAN, respectively. These extensive results highlight the potential of PMAA as a lightweight cloud removal network suitable for deployment on edge devices. We will release the code and trained models to facilitate the study in this direction.
\end{abstract}

\section{Introduction}
\label{intro}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/result_v3.pdf}
\caption{Performance comparison on the \emph{Sen2\_MTC\_New} dataset. Compared to existing methods, PMAA achieves SOTA performance while being computationally efficient. The area of the circle indicates the number of model parameters. The complete results are shown in Table~\ref{tab:pcr}.}
\label{sota}
\end{figure}

With the rapid development of remote sensing technology, satellite imagery has been widely applied in various research fields, such as grassland monitoring~\cite{grasslandmonitoring}, ground target detection~\cite{objectdetection1,objectdetection2}, land cover classification~\cite{landcover1,landcover2}, etc. Nonetheless, due to weather conditions, clouds often disrupt the imaging process of optical sensors carried on satellites, resulting in information loss and image quality degradation. Cloud removal from satellite imagery attempts to reconstruct the original information in cloud-covered areas to solve the above problems. It is a critical preprocessing step that significantly affects the effective use of satellite imagery.

Recently, convolutional neural networks (CNNs~\cite{deeplearning}) and generative adversarial networks (GANs~\cite{gans}) have shown significant improvements in cloud removal performance for satellite imagery. Among these, mono-temporal cloud removal methods~\cite{mcgan,8803666,rice,spa-gan} generate a corresponding cloud-free image using a single cloudy image. These methods exhibit stable performance in cloud removal from satellite imagery. However, when cloud coverage is extensive, sufficient information cannot be effectively obtained, making it challenging to generate cloud-free satellite imagery and often impossible.

As remote sensing technology advances, satellite revisitation periods to the same location become increasingly shorter. We can quickly obtain multiple images of the same area captured by satellites at different times. Recently, some researchers have explored utilizing multi-temporal satellite images and successfully applied them to cloud removal to enhance performance. The multi-temporal cloud removal from satellite imagery method uses multiple cloudy images at the same location as inputs and generates a cloud-free image using spatial and temporal information. Among these, \cite{ae} proposed a CNN-based autoencoder using multi-temporal satellite imagery to remove clouds. \cite{stnet} combined cloud detection network to improve performance. \cite{stgan} treated the cloud removal problem as a challenge of conditional image synthesis and proposed a spatio-temporal generative network for cloud removal. \cite{ctgan} proposed a Transformer-based GAN for cloud removal, significantly increasing the accuracy of generated cloud-free images.

However, cloud removal methods suffer from one or more issues. Firstly, almost all methods need to consider the model complexity, potentially resulting in additional time consumption in practical applications, especially for large-resolution satellite imagery. Secondly, existing methods do not consider progressively generating cloud-free images but only perform a single stage of reconstruction, which may affect the reconstruction of spatial details in the image. Thirdly, global and local information have different impacts on cloud removal. Previous methods generally only consider the impact of local information on generating cloud-free imageswithout considering the combination of global and local information. Therefore, the challenge remains to efficiently exploit global and local information to better serve the cloud removal task.

In this work, we propose a high-performance Progressive Multi-scale Attention Autoencoder (PMAA) for considering both fine- and coarse-grained features within different scales, as shown in Figure~\ref{pmaa}. Firstly, multi-temporal satellite images use a cloud detection backbone to extract and enhance cloudy regions as input to the cloud removal module. Secondly, the encoder in the cloud removal module downsamples through convolutional operations, producing features with different spatial resolutions, and then they are aggregated to obtain fine- and coarse-grained representations. Thirdly, aggregated multi-scale features are fed into a multi-scale attention module (MAM) to obtain global attention and modulate multi-scale features. We introduce a local interaction module (LIM) in the decoder that connects local and global features to reconstruct fine-grained image structures. Finally, we use a novel progressive learning method to cycle PMAA's cloud removal module to generate cloud-free images.

In essence, this work makes the following main contributions:
\begin{itemize}
    \item We propose a high-performance Progressive Multi-scale Attention Autoencoder (PMAA) for removing clouds, featuring the cloud detection backbone and cloud removal module. 
    \item We introduce the cloud detection backbone, which is designed to enhance the cloudy region to improve the performance of the cloud removal module.
    \item We introduce the cloud removal module, which consists of the multi-scale attention module (MAM) and local interaction module (LIM). MAM utilizes multi-scale features to represent both fine- and coarse-grained features. LIM fully leverages global attention and local multi-scale features to improve the precision of generated images.
    \item We conduct extensive ablation experiments and quantitative analyses to examine the effect of various components of PMAA.
\end{itemize}

Based on the above technical contributions, we obtain a high-performance cloud removal model from satellite imagery. Compared to previous methods, PMAA consistently achieves state-of-the-art performance on the \emph{Sen2\_MTC\_Old}~\cite{stgan} and \emph{Sen2\_MTC\_New}~\cite{ctgan} datasets. Moreover, we also demonstrate the efficiency of PMAA. Compared to the SOTA model CTGAN~\cite{ctgan}, PMAA saves approximately 99.5\% of parameters and 85.4\% of the computational cost, achieving the optimal performance-efficiency trade-off, as shown in Figure~\ref{sota}. We will release the source code and trained model to facilitate further studies in this direction.

\section{Related Work}
\subsection{Cloud Removal from Satellite Imagery}
With the development of deep learning, cloud removal methods~\cite{mcgan,ae,8803666,stnet,spa-gan,stgan,spa-gan,ctgan} for satellite imagery have increasingly become the focus of current research. Existing methods are mainly classified into two types: mono-temporal and multi-temporal. The mono-temporal-based methods~\cite{mcgan,8803666,rice,spa-gan} will have a much faster inference time because its input uses only a single cloudy image. However, when many clouds cover the satellite image, the mono-temporal-based methods may not obtain precise results, which limits its application in practical scenarios. Multi-temporal-based methods~\cite{ae,stnet,stgan,ctgan} obtain better results than mono-temporal-based methods using multi-temporal cloudy images to reconstruct a single cloud-free image. Although they recover high-quality cloud-free images, their inference process takes up more time in the application. In contrast, our proposed PMAA focuses on multi-temporal satellite images' global and local features and efficiently reconstructs cloud-free images. Furthermore, PMAA dramatically reduces the computational cost of the model, making practical applications possible.

\subsection{Attention Mechanism}
Attention mechanism originates from natural language processing (NLP), such as language modeling~\cite{lan2019albert,liu2019roberta,lan2019albert}, machine translation~\cite{transformer}, and generative tasks~\cite{afrcnn,tdanet}. They estimate the relationship between current and global features via the attention mechanism. Recently, the computer vision field has also successfully used attention mechanism to improve model performance, such as image inpainting~\cite{zits,Liu_2022_CVPR} and restoration~\cite{restormer, li2020survey}. However, these methods always transfer the attention map to deeper layers, which can lead shallow features to be less affected by attention and bring limited performance improvements. In addition, existing methods~\cite{vit,swin} tend to divide the image into non-overlapping patches to reduce the computational cost, which is unreliable for cloud removal. This is because the distribution of clouds in the image is not uniform, which will lead to inconsistent cloud occupancy in different patches.

\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/pipeline_v2.pdf}
% \caption{A pipeline for cloud removal using multi-temporal satellite imagery. We assume that the input is three cloudy images of the same location and adjacent moments, and the output is a single cloud-free image of the corresponding location. The cloud removal module is repeated $S$ times, where $S=3$.}
\caption{A pipeline for cloud removal task using multi-temporal satellite imagery. We assume that the input is three cloudy images of the same location and adjacent moments, and the output is a single cloud-free image of the corresponding location.}
\label{pipeline}
\end{figure}

\subsection{Progressive Learning}
Progressive learning aims to split complex processes (e.g., direct reconstruction of cloud-free images) into multiple easier and smaller stages (e.g., multi-step reconstruction of cloud-free images) to improve model performance and is widely considered in visual tasks and speech tasks, such as image synthesis~\cite{ddpm}, image super-resolution~\cite{wang2018fully}, and speech separation \cite{afrcnn,tdanet}. In addition, progressive learning is more in line with how humans perceive images because the human visual system does not process the whole scene simultaneously. Instead, it gradually focuses its attention on the part of the interest of the image and ignores the irrelevant details. It can combine information from different regions over time to reconstruct the complete scene in the brain \cite{miyawaki2008visual,naselaris2009bayesian,nishimoto2011reconstructing}.

\section{Method}
\subsection{Overall Pipeline}

The algorithm of multi-temporal cloud removal is expected to generate a cloud-free satellite image from three temporal cloudy satellite images (same location, adjacent time), as shown in Figure~\ref{pipeline}. We denote three cloudy satellite images as $\{\mathbf{X}_{i} \in \mathbb{R}^{4 \times H \times W}|i=1,2,3\}$, where $H$ and $W$ are the image's height and width, respectively, and ``4" denotes the four Spectral channels (RGB and infra-red). And we denote a cloud-free image at the current location as $\mathbf{y} \in \mathbb{R}^{4 \times H \times W}$. We assume that for an arbitrary location, $\mathbf{X}_{i}$ changes slowly over time and that the cloud cover position in the image varies. 

Our proposed PMAA consists of cloud detection backbone and removal module (Figure~\ref{pmaa}), first detecting the location of clouds and then using pixel reconstruction to remove cloud obscuration. We will describe these two components in detail in Sections \ref{sec:cdb} and \ref{sec:crm}.

\subsection{Cloud Detection Backbone} \label{sec:cdb}
From the perspective of human visual perception, the ability to detect and recognize clouds plays a vital role in removing them, and to some extent, enhances the interpretability of the model. The cloud detection backbone in PMAA adopts a structure similar to DeepLabv3 \cite{chen2017rethinking}, which emphasizes the weight of cloudy regions to assist the cloud removal module in recovering obscured content. Specifically, for each input cloudy image $\mathbf{X}_{i}$, the cloud detection backbone generates the corresponding cloud mask $\{\mathbf{M}_{i} \in \mathbb{R}^{1 \times H \times W}|i=1,2,3\}$. Then, we use the cloud masks to increase the weight of the cloud-affected regions 
%
\begin{equation}
    \mathbf{U}_{i} = \mathbf{M}_{i} \odot \mathbf{X}_{i} + \mathbf{X}_{i}, 
\end{equation}
%
where $\mathbf{U}_{i}\in \mathbb{R}^{4 \times H \times W}$ represents the cloudy satellite image after feature selection and weight amplification, and $\odot$ represents element-wise multiplication. 

% The cloud removal module (Figure~\ref{pmaa}) receives the spatio-temporal features $\mathbf{U}_{c} \in \mathbb{R}^{12 \times H \times W}$ as input, which are obtained by concatenating $\{\mathbf{U}_i \in \mathbb{R}^{4 \times H \times W}|i=1,2,3\}$ on the channel dimension. After progressive refinement, the cloud removal module ultimately yields a cloud-free image $\mathbf{y} \in \mathbb{R}^{4 \times H \times W}$ at the current location. The cloud removal module is inspired by the autoencoder architecture, which is described in Section~\ref{sec:autoencoder}.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/pmaa_v1.pdf}
\caption{Overview of the PMAA with cloud detection backbone and removal module. In the encoder, we downsample the input image $N$ times, where $N=3$. Then, the multi-scale features are fused by averaging pooling and summation operations. The fused features are processed by a simplified transformer layer to obtain global attention, which modulates the multi-scale features. In the image reconstruction process (decoder), PMAA uses the local interaction module to recover image details.}
\label{pmaa}
\end{figure*}

\subsection{Cloud Removal Module} \label{sec:crm}
\label{sec:autoencoder}

The cloud removal module (Figure~\ref{pmaa}) receives the spatio-temporal features $\mathbf{U}_{c} \in \mathbb{R}^{12 \times H \times W}$ as input, which are obtained by concatenating $\{\mathbf{U}_i|i=1,2,3\}$ on the channel dimension. After progressive refinement, the cloud removal module ultimately yields a cloud-free image $\mathbf{y}$ at the current location. The cloud removal module is inspired by the autoencoder architecture, which is described in the following sections.

% \subsubsection{Efficient Multi-Scale Attention Autoencoder}
\subsubsection{Encoder}

To consider data dimensionality reduction and mimicking human cognitive processes, we commonly deepen image features through downsampling while preserving as much relevant information as possible. To this end, we introduce an encoder (Figure~\ref{pmaa}(a)) capable of extracting multi-scale features. To achieve this, we employ multiple depth-wise separable convolutional layers~\cite{mobilenets} with a kernel size of $3 \times 3$ and stride of $2 \times 2$ for reducing spatial image scale and increasing receptive field. After downsampling $N$ times, we obtain $N+1$ multi-scale features $\mathbf{F}_{i} \in \mathbb{R}^{C \times {\frac{H}{2^{i}}} \times {\frac{W}{2^{i}}} }|i=0,...,N\}$ with the incremental receptive field, where $C$ denotes the features channels. All convolutional layers are followed by instance normalization~\cite{in} and ReLU activation function~\cite{relu}. Finally, the multi-scale features $\mathbf{F}_{i}$ are fed into the following Multi-scale Attention Module.

\subsubsection{Multi-scale Attention Module}

To endow the model with the capability of perceiving both fine- and coarse-grained visual features, we introduce a multi-scale attention module (MAM, Figure~\ref{pmaa}(b)) composed of three components: 1) Multi-scale Fusion; 2) Transformer Layer; 3) Selective Attention. The MAM works as follows.

\paragraph{Multi-scale Fusion.} First, we craft a multi-scale fusion approach without any parameters and with insignificant additional computation cost. It compresses all features $\{\mathbf{F}_{i}|i=0,...,N\}$ with the scale $(\frac{H}{2^{i}} \times {\frac{W}{2^{i}}})$ to the uniform scale $(\frac{H}{2^N},\frac{W}{2^N})$ using an adaptive average pooling and then fuse them by a summation operation to obtain a multi-scale representation 
%
\begin{equation}
    \mathbf{F}_\text{ms}=\sum_{i-0}^N H(\mathbf{F}_{i}),
\end{equation}
%
where $H(\cdot)$ represents adaptive average pooling. Finally, we use the $\mathbf{F}_\text{ms}$ as the input to the transformer layer in the next stage.

\paragraph{Transformer Layer.} Limited by the receptive fields of CNNs, some methods~\cite{pix2pix,mcgan,stgan} struggle to acquire enough contextual information for cloud removal, particularly when cloud cover is extensive. This is because an individual pixel and its receptive field may all fall in the cloudy region, preventing it from noticing information outside the cloud area, and severely hindering the recovery of the cloud-free image. Therefore, we introduce self-attention~\cite{transformer} to encode spatial information to establish long-range dependency. So far, there are many vision transformers based on self-attention, such as ViT~\cite{vit}, Swin Transformer~\cite{swin} and Conv2Former~\cite{conv2former}, etc. To balance performance and efficiency, we adopt a simple self-attention implemented by using a convolutional modulation operation that uses large kernel convolution to avoid the problem of time-consuming and complex computation of the attention matrix
%
\begin{equation}
    \mathbf{F}_{a}=\mathbf{F}_\text{ms}+\alpha \mathbf{W}_{3}(\text{DConv}_{k \times k}(\mathbf{W}_{1}\mathbf{F}_\text{ms})\odot \mathbf{W}_{2}\mathbf{F}_\text{ms}),
\end{equation}
%
where $\mathbf{W}_{1},\mathbf{W}_{2},\mathbf{W}_{3}$ are linear layers and $\alpha$ is a learnable parameter, and $\text{DConv}_{k \times k}$ denotes depth-wise separable convolutional layer with kernel size $k \times k$ and stride $1 \times 1$. Then, a residual connection~\cite{resnet} is added after self-attention to reduce information loss. The self-attention is immediately followed by a multilayer perceptron (MLP), which consists of one depth-separable convolution layer and two linear layers
%
\begin{equation}
    \mathbf{F}_{g}=\mathbf{F}_{a}+\beta \mathbf{V}_{2}(\mathbf{V}_{1}\mathbf{F}_{a}+\text{DConv}_{k \times k}(\mathbf{V}_{1}\mathbf{F}_{a})),
\end{equation}
%
where $\mathbf{V}_{1},\mathbf{V}_{2}$ are linear layers and $\beta$ is a learnable parameter. In summary, through the processing of a transformer layer, we obtain a feature $\mathbf{F}_{g}$ with global information.
  
\paragraph{Selective Attention.} We use $\mathbf{F}_{g}$ as a global attention to perform adaptive feature recalibration on $\mathbf{F}_{i}$ before integrating it into the decoder. This is because the earlier layers of the neural network possess rich low-level texture features, while the deeper layers have high-level semantic information. Specifically, we upsample $\mathbf{F}_{g}$ through nearest-neighbor interpolation to obtain the same spatial dimension as $\mathbf{F}_{i}$. Then, we obtain the modulated feature $\{\mathbf{F}_{i}' \in \mathbb{R}^{c \times {\frac{H}{2^{i}}}  \times {\frac{W}{2^{i}}} }|i=0,...,N\}$ through affine transformation
%
\begin{equation}
    \mathbf{F}_{i}'=\phi(\sigma(\mathbf{Z}_{1}(\mathbf{F}_{g}))) \odot \mathbf{Z}_{2}(\mathbf{F}_{i})+\phi(\mathbf{Z}_{3}(\mathbf{F}_{g})),
\end{equation}
%
where $\mathbf{Z}_{1},\mathbf{Z}_{2},\mathbf{Z}_{3}$ are linear layers, $\sigma$ represents the sigmoid activation function, and $\phi$ represents the nearest-neighbor interpolation.

\subsubsection{Decoder}

The local interaction module is the core component in the decoder (Figure~\ref{pmaa}(c)), which gradually restores image resolution through the previously modulated features $\mathbf{F}_{i}'$

%
\begin{equation}
    \mathbf{O}_{i+1 }=\phi(\sigma(\mathbf{D}_{1}(\mathbf{O}_{i}))) \odot \mathbf{D}_{2}(\mathbf{F}_{i+1}')+\phi(\mathbf{D}_{3}(\mathbf{O}_{i})),
\end{equation}
%
where $\mathbf{D}_{1},\mathbf{D}_{2},\mathbf{D}_{3}$ are three linear layers. Finally, we shall take the feature map with the highest resolution as the output of the current cloud removal module.

\subsection{Progressive Learning by PMAA}

Progressive learning (PL) has been introduced to image inpainting~\cite{repaint,eii} and image restoration~\cite{mprnet,restormer} tasks and has achieved superior performance. Multi-temporal cloud removal is a complex task that falls between image inpainting and restoration. Instead of attempting a direct transformation from multi-temporal cloudy images to a single cloud-free image, we adopt a strategy of breaking down the cloud removal process into several smaller, more tractable steps. These preliminary stages assist in laying the foundation for the latter stages of the training process
%
\begin{equation}
    \mathbf{Q}_{t+1}=S(\mathbf{Q}_{t} \oplus \varphi(\mathbf{U_{c}})),
\end{equation}
%
where $\mathbf{Q}_{t}$ denotes the output of the cloud removal module at time $t$,  $S$ represents a cloud removal module, and $\varphi$ represents a convolutional layer with kernel size of $1 \times 1$ to do the work of channel transformation. Although too many stages may cause the network to be too deep to converge, and also increase the model's parameters and computational cost. We explored the effects of different stages on the model performance in the ablation experiments (see Figure~\ref{fig:pls}) and chose the optimal number of stages to strike a balance between performance and efficiency.

\subsection{Loss Function} 

The cloud detection backbone and cloud removal module are jointly optimized to improve the model's ability to eliminate clouds and achieve high-fidelity cloud-free image generation. In assessing the cloud detection backbone, we calculate the discrepancy between the cloud mask $\hat{\mathbf{M}}$ and ground-truth mask $\mathbf{M}$ as the cloud detection loss

\begin{equation}
    \mathcal{L}_\text{Mask}=||\mathbf{M}-\hat{\mathbf{M}}||_{2}.
\end{equation}

To align the divergence between the generated cloud-free image $\mathbf{y}$ and the ground truth, we calculate the L1 loss between them
%
\begin{equation}
    \mathcal{L}_\text{L1}(G)=||\mathbf{y}-G(\mathbf{x})||_{1},
\end{equation}
%
where $x$ represents multi-temporal cloudy images, and $G(\cdot)$ denotes the generator (PMAA) within the generative adversarial network.

Also, we compute the adversarial loss for the conditional generative adversarial network, which helps to smooth the generator's output and alleviate the blur caused by excessive cloud coverage that makes it hard to recover

\begin{equation}
    \mathcal{L}_\text{cGAN}(G, D)=E_{(\mathbf{x},\mathbf{y})}[\log D(\mathbf{x}, \mathbf{y})]+E_{\mathbf{x}}[\log(1-D(\mathbf{x},G(\mathbf{x})))].
\end{equation}

Finally, we calculate the auxiliary loss to accelerate the convergence of the model
%
\begin{equation}
    \mathcal{L}_\text{Aux}=||\mathbf{y}-G_\text{Aux}(B(\mathbf{x}))||_{1},
\end{equation}
%
where $B(\mathbf{x})$ represents the feature map by the input $\mathbf{x}$ through the cloud detection backbone without the sigmoid activation function, $G_{Aux}$ represents an auxiliary generator, which composes of a standard convolutional layer with a kernel size of $3 \times 3$ followed by a tanh activation function.

The overall objective function of PMAA is
%
\begin{align}
    \mathcal{L}_\text{Final}=&\min_{G}\max_{D}\mathcal{L}_\text{cGAN}(G,D)+\lambda_\text{L1}\mathcal{L}_\text{L1}(G)+ \nonumber \\
    & \mathcal{L}_\text{Mask}+\lambda_\text{Aux}\mathcal{L}_\text{Aux}.
\end{align}
%
% where $\lambda_\text{L1}$ and $\lambda_\text{Aux}$ are set to 100 and 50, respectively. We tried this configuration briefly and experimentally and got a good result.
The weight parameter is decided based on the importance of the task in the overall loss. We choose ($\lambda_\text{L1}=100$, $\lambda_\text{Aux}=50$) for our experiments. Higher weights are assigned to main and auxiliary cloud removal tasks as they need spatial accuracy. We tried this configuration briefly and experimentally and got a good result.

\section{Experiments}

\subsection{Datasets}
\paragraph{Sen2\_MTC\_Old.} This dataset~\cite{stgan} is created from publicly-available Sentinel-2 images. It contains 945 different tiles with a total of 3130 image pairs. Every three cloudy images correspond to one cloud-free image, where each image has size $(w, h) = (256, 256)$, number of channels $C = 4$ (RGB and infra-red), and pixel values in the range $[0, 255]$. The pixel values of each image are normalized to $[0, 1]$ before input and then transformed to $[-1, 1]$ by a mean and variance of 0.5. This dataset divides into a training set, a validation set, and a test set in the ratio of $8:1:1$.

\paragraph{Sen2\_MTC\_New.} This dataset~\cite{ctgan} is also created from the Sentinel-2 images different from the \emph{Sen2\_MTC\_Old} dataset, which contains about 50 non-overlapping tiles with about 70 pairs of images per tile. Its settings are consistent with the \emph{Sen2\_MTC\_Old} dataset, except that the pixel value range is $[0, 10000]$. Compared to the \emph{Sen2\_MTC\_Old} dataset, it has higher resolution and annotation quality. This dataset divides into training, validation, and test sets in the ratio of $7:1:2$.

\subsection{Implementation Details}

\paragraph{Training Settings.}

Our proposed PMAA comprises two components: cloud detection backbone and cloud removal module. The convolutional layers of the cloud detection backbone employ 32 channels, a kernel size of $3 \times 3$ and a stride of $1 \times 1$. In contrast the cloud removal module utilizes depth-wise separable convolutional layers with the same settings. The cloud removal module number of downsampling steps is set to 4, and the number of progressive learning stages is 3. To avoid overfitting, we set the probability of all dropout layers to 0.1. During the training phase, we use AdamW~\cite{adamw} optimizer with an initial learning rate of $5\times 10^{-4}$ and a cosine decay~\cite{cosine} learning rate schedule. We train all models for 100 epochs, with a batch size of 4. For all experiments, we utilize one NVIDIA Tesla A100 80G for training and testing. This project is implemented using PyTorch.

\paragraph{Evaluation Metrics.}

In all experiments, we report the Peak Signal-to-Noise Ratio (PSNR, dB) and Structual Similarity Index Measure (SSIM~\cite{ssim}) of the test set to evaluate the precision of the generated cloud-free images. For the evaluation of model efficiency, we report the number of parameters and multiplyâ€“accumulate operations (MACs, G) for all models. The result is calculated via the ptflops\footnote{https://github.com/sovrasov/flops-counter.pytorch}.

\subsection{Ablation Studies}\label{sec:ablation}

% \begin{table}[htbp]
% \centering
% \begin{tabular}{ccccc}
% \toprule
% Fusion Strategies & PSNR   & SSIM  & Params & MACs  \\
% \midrule
% No fusion       & 18.254 & 0.577 & 3.45   & \textbf{92.31} \\
% Concatenation     & 17.865 & 0.579 & 3.69   & 92.40 \\
% Summation             & \textbf{18.497} &\textbf{0.579} & \textbf{3.45}   & 92.34 \\
% \bottomrule
% \end{tabular}
% \caption{Quantitative comparison of the cloud removal performance of different multi-scale feature fusion strategies.}
% \label{tab:mfs}
% \end{table}

% \begin{table}[htbp]
% \centering
% \begin{tabular}{ccccc}
% \toprule
% Cloud Detection & PSNR   & SSIM  & Params & MACs  \\
% \midrule
% $\times$                      & 18.117 & \textbf{0.584} & 3.44   & \textbf{88.04} \\
% $\checkmark$                  & \textbf{18.497} & 0.579 & \textbf{3.45}   & 92.34 \\
% \midrule
% \end{tabular}
% \caption{Quantitatively verify the importance of the cloud detection module. ``$\checkmark$" indicates that the cloud detection module is used, while ``$\times$" denotes the absence of its use.}
% \label{tab:cdm}
% \end{table}

% \begin{table}[htbp]
% \centering
% \begin{tabular}{ccccc}
% \toprule
% Attention Strategies & PSNR   & SSIM  & Params & MACs  \\
% \midrule
% $\times$          & 17.845 & 0.567 & \textbf{2.84}   & \textbf{92.18} \\
% Patch-based       & 17.554 & 0.556 & 3.25   & 92.25 \\
% Nonpatch-based    & \textbf{18.497} & \textbf{0.579} & 3.45   & 92.34 \\
% \bottomrule
% \end{tabular}
% \caption{Quantitatively compare the effectiveness of various self-attention implementations, where ``$\times$" denotes the absence of self-attention.}
% \label{tab:si}
% \end{table}

% \begin{table}[ht]
% \centering
% \begin{tabular}{ccccc}
% \toprule
% Number of Stages & PSNR   & SSIM  & Params & MACs   \\
% \midrule
% 1                      & 17.504 & 0.563 & \textbf{1.30}  & \textbf{61.52}  \\
% 2                      & 17.263 & 0.554 & 2.38   & 76.93  \\
% 3                      & \textbf{18.497} & 0.579 & 3.45   & 92.34  \\
% 4                      & 17.989 & 0.581 & 4.52   & 107.75 \\
% 5                      & 18.027 & \textbf{0.583} & 5.59   & 123.16 \\
% \bottomrule
% \end{tabular}
% \caption{Quantitatively compare the impact of the number of progressive learning stages.}
% \label{tab:pls}
% \end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[tb]
\small
\centering
\begin{tabular}{ccccccccc}
\toprule
\multicolumn{2}{c}{Fusion Strategies} & \multirow{2}{*}{Cloud Detection} & \multicolumn{2}{c}{Attention Strategies} & \multirow{2}{*}{PSNR $\uparrow$} & \multirow{2}{*}{SSIM $\uparrow$} & \multirow{2}{*}{Params (M) $\downarrow$} & \multirow{2}{*}{MACs (G) $\downarrow$} \\ \cmidrule(r){1-2} \cmidrule(r){4-5}
Concat       & Sum       &                                  & Patch-based       & Nonpatch-based       &                       &                       &                         &                       \\ \midrule
$\times$                   & $\times$               & $\checkmark$                               & $\times$                 & $\checkmark$                   & 18.254                & 0.577                 & 3.45                    & 92.31                 \\
$\checkmark$                  & $\times$               & $\checkmark$                               & $\times$                 & $\checkmark$                   & 17.865                & 0.579                 & 3.69                    & 92.40                 \\
$\times$                   & $\checkmark$              & $\times$                                & $\times$                 & $\checkmark$                   & 18.117                & \textbf{0.584}             & 3.44                    & \textbf{88.04}             \\
$\times$                   & $\checkmark$              & $\checkmark$                               & $\times$                 & $\times$                    & 17.845                & 0.567                 & \textbf{2.84}                & 92.18                 \\
$\times$                   & $\checkmark$              & $\checkmark$                               & $\checkmark$                & $\times$                    & 17.554                & 0.556                 & 3.25                    & 92.25                 \\
\rowcolor[RGB]{176,224,230} $\times$                   & $\checkmark$              & $\checkmark$                               & $\times$                 & $\checkmark$                   & \textbf{18.497}            & 0.579                 & 3.45                    & 92.34                 \\ \bottomrule
\end{tabular}
\caption{Ablation studies on the \emph{Sen2\_MTC\_New} dataset. ``$\checkmark$" indicates that this part is used, while ``$\times$" denotes the absence of its use.}
\label{ablation}
\end{table*}

\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{figures/stages.pdf}
\caption{Quantitatively compare the impact of the number of progressive learning stages. The area of the circle indicates the number of model parameters, where larger circles indicate more parameters. The numbers in circles indicate the number of different stages.}
\label{fig:pls}
\end{figure}

\begin{figure*}[tb]
\centering
\includegraphics[width=0.96\linewidth]{figures/visualization_v2.pdf}
\caption{Visualization results on \emph{Sen2\_MTC\_Old} (a) and \emph{Sen2\_MTC\_New} (b) test sets. The left and right sides of ``$|$" indicate the values of PSNR and SSIM, respectively.}
\label{fig:visualization}
\end{figure*}

\begin{table*}[]
\centering
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{2}{c}{Sen2\_MTC\_Old} & \multicolumn{2}{c}{Sen2\_MTC\_New} & \multirow{2}{*}{Params (M) $\downarrow$} & \multirow{2}{*}{MACs (G) $\downarrow$} \\
\cmidrule(r){2-3} \cmidrule(r){4-5} 
                         & PSNR $\uparrow$                   & SSIM $\uparrow$                 & PSNR $\uparrow$              & SSIM $\uparrow$            &                         &                       \\
                         \midrule
MCGAN~\cite{mcgan}                    & 21.146                 & 0.481                & 17.448             & 0.513            & 54.42                   & 71.56                 \\
Pix2Pix~\cite{pix2pix}                  & 22.894                 & 0.437                & 16.985             & 0.455            & 11.41                   & 58.94                 \\
AE~\cite{ae}                       & 12.204                 & 0.644                & 15.251             & 0.412            & 6.53                    & \textbf{35.72}                 \\
ST\_net~\cite{stnet}                  & 26.321                 & 0.834                & 16.206             & 0.427            & 4.64                    & 304.31                \\
STGAN~\cite{stgan}             & 26.186                 & 0.734                & 18.152             & 0.587            & 231.93                  & 1094.94               \\
CTGAN~\cite{ctgan}                    & 26.264                 & 0.808                & 18.308             & \textbf{0.609}            & 642.92                  & 632.05                \\
\rowcolor[RGB]{176,224,230} PMAA (Ours)                     & \textbf{27.434}                 & \textbf{0.855}                & \textbf{18.497}             & 0.579            & \textbf{3.45}                    & 92.34                 \\
\bottomrule
\end{tabular}
\caption{Quantitative comparison between our and existing models on \emph{Sen2\_MTC\_Old} and \emph{Sen2\_MTC\_New} datasets.}
\label{tab:pcr}
\end{table*}

To quantitatively analyze the impact of each component, we conduct abundant ablation experiments on the \emph{Sen2\_MTC\_New} dataset which has higher annotation quality.

\paragraph{Multi-scale Fusion Strategy.} We examine the effect of different multi-scale feature fusion methods on the performance of cloud removal, as shown in Table~\ref{ablation}. Compared to not performing feature fusion (using the feature map with the smallest resolution as input of transformer layer), the summation operation is the optimal feature fusion strategy, resulting in a PSNR improvement of 0.234. Notably, improper feature fusion strategies (such as concatenation operation on the channel dimension) can lead to worse results and increase computational complexity. 

\paragraph{Cloud Detection Module.} From the perspective of prior knowledge, cloud detection has strong guidance significance for cloud removal, so we study the effect of the cloud detection module on cloud removal performance. The results in Table~\ref{ablation} show that the presence of the cloud detection module greatly improves the model's performance, with PSNR on the test set improving by 0.380. The decrease in SSIM caused by the cloud detection module might be attributed to the fact that the module focuses on identifying and localizing cloud regions, which potentially impacts the reconstruction of the underlying image features.

\paragraph{Self-attention Implementation.} As outlined in Table~\ref{ablation}, we examine the impact of various self-attention strategies on cloud removal performance. Compared to the transformer layer based on patch (like Swin-Transformer), the nonpatch-based transformer layer (PMAA's transformer layer) obtains better performance, with PSNR on the test set rising by 0.943. One possible explanation for this substantial advancement could be that the patch operation may lead to some patches lacking cloud occlusion while others being entirely cloud occlusion, thus rendering PMAA incapable of learning efficiently.

\paragraph{Progressive Learning Stage.} Increasing the number of cloud removal module stages will prolong the progressive learning process, thus increasing the expressive capacity of the model. However, it will also bring additional computational cost. In Figure~\ref{fig:pls}, we study the effect of the number of PMAA's stages on the performance and efficiency of the model. When the number of PMAA's stages is 3, it reaches a balance between performance and efficiency. If the number of stages continues to increase, the model is too complex to converge, and the transfer across stages also leads to information loss, resulting in bad results.

\paragraph{Loss Function.} Firstly, $\mathcal{L}_{\text{L1}}$ is the primary loss for the supervised cloud removal task. Secondly, we utilize $\mathcal{L}_{\text{cGAN}}$ to obtain more realistic and detailed reconstructed images. Thirdly, we introduce $\mathcal{L}_{\text{Mask}}$ to force the model to focus on cloud regions, thus removing clouds more accurately. Finally, we incorporate $\mathcal{L}_{\text{Aux}}$ to accelerate the model's convergence rate and improve performance. Experimental results (see Table~\ref{tab: loss}) indicate that each loss function is effective, and accumulating these functions enables our model to learn from various aspects, thereby achieving better results.

\begin{table}[h]
\centering
\begin{tabular}{cccccc}
\hline
L1    & cGAN    & Mask    & Aux    & PSNR $\uparrow$ & SSIM $\uparrow$                       \\ \hline
$\checkmark$     & $\times$       & $\times$       & $\times$      & 17.424                      & 0.564                      \\
$\checkmark$     & $\checkmark$       & $\times$       & $\times$      & 17.439                      & 0.573                      \\
$\checkmark$     & $\checkmark$       & $\checkmark$       & $\times$      & 17.456                      & 0.578                      \\
\rowcolor[RGB]{176,224,230}$\checkmark$     & $\checkmark$       & $\checkmark$       & $\checkmark$      & \textbf{18.497}                & \textbf{0.579}                 \\ \hline
\end{tabular}
\caption{Ablation studies on the loss function.}
\label{tab: loss}
\end{table}

\subsection{Comparison with the State-of-the-Arts}

% We conducted extensive experiments to quantitatively compare the performance and efficiency of our proposed PMAA with existing models on Sen2\_MTC\_Old and \emph{Sen2\_MTC\_New} datasets. Overall, our model achieves SOTA cloud removal performance using a very small number of parameters.

\paragraph{Performance of Cloud Removal.} We compare the quantitative results (see Table~\ref{tab:pcr}) of our method with existing methods on the \emph{Sen2\_MTC\_Old} and \emph{Sen2\_MTC\_New} datasets. On both datasets, PMAA achieves consistent SOTA performance on PSNR, clearly demonstrating the superiority of our method. 1) On \emph{Sen2\_MTC\_New}: PMAA obtain competitive results but with only 0.5\% of the number of parameters compared to CTGAN. Although, PMAA has a slightly lower SSIM score compared to CTGAN, the difference is marginal and the PSNR score is better, which achieves a significant improvement of 1.170. 2) On \emph{Sen2\_MTC\_Old}: PMAA performs much better than CTGAN and other methods. The visualization results are shown in Figure~\ref{fig:visualization}, which presents a qualitative comparison of our method against existing approaches on representative examples from two datasets. We observe that PMAA can generate more detailed structures and demonstrates improved robustness in cloud removal (as highlighted in the rectangular bounding boxes).

\paragraph{Efficiency of Cloud Removal.} We compare with existing methods in terms of the number of model parameters and MACs (reflecting the computational complexity). In the Table~\ref{tab:pcr}, we observe that our proposed PMAA has an amazing efficiency with 0.5\% and 14.6\% of the previous SOTA model CTGAN's number of parameters and MACs, respectively. Our efficiency gain is attributed to two components. First, the cloud removal module samples and fuses multi-scale features using parameter-free pooling layers and additive operations, respectively. This is more efficient than CTGAN, which distributes the computational cost in each layer. Second, our transformer layer does not compute the similarity matrix and uses only the large convolutional kernel to extract features since it calculates at the minimum resolution scale, which is sufficient to extract global features. The extremely low number of parameters and computational complexity suggest that our model can be easily deployed to low-resource devices.

\section{Conclusion}

This paper presents a lightweight network (PMAA) for high-performance cloud removal from multi-temporal satellite imagery. PMAA first uses a cloud detection backbone to reinforce the cloudy areas to improve the performance of cloud removal. Then it leverages the autoencoder with an efficient multi-scale attention module to capture global contextual information and guide the local interaction module in the decoder to reconstruct cloud-free images. Meanwhile, PMAA uses progressive learning to improve the model performance further. Extensive experiments have demonstrated better performance and efficiency of PMAA on two datasets. In addition, PMAA has important implications for the grounding of the cloud removal model in practical deployments and is expected to achieve superior performance in large-scale and batch-processing cloud removal tasks from satellite imagery.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ijcai}
}

\end{document}
