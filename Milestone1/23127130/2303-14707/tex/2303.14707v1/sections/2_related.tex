\section{Related Works}
\begin{figure}[t]
\begin{center}
    \includegraphics[width=1\linewidth]{figures/toaster1.pdf}
\end{center}
\vspace{-0.1in}
\caption{\textbf{Appearance Decomposition.} \textbf{(a, d)} For object surfaces with strong view-dependent effects, vanilla NeRF often gets disrupted, resulting in poor reconstruction quality. 
%
We propose decomposing appearance into \textbf{(b)} view-independent and \textbf{(c)} view-dependent components during NeRF training and incorporating geometry-related priors to improve reconstruction quality. Our approach shows the ability to \textbf{(e)} render the view-independent component of the scene and \textbf{(f)} more accurate recovery of the scene. 
}
\label{fig:toaster}
\vspace{-0.15in}
\end{figure}


\paragraph{Neural Radiance Fields.} 
NeRFs debut in~\cite{mildenhall2020nerf}  achieved unprecedented novel view synthesis effects, by modeling the underlying 3D scene as a continuous volumetric field of color and density via layers of MLP. The input to a NeRF consists of a 5D vector containing a 3D location $(x, y, z)$, and a 2D viewing direction $(\theta, \phi)$. 
%
Since then, follow-up works have addressed the limitations and improved the performance, such as 
%
enabling dynamic modeling~\cite{zhang2021stnerf, park2021nerfies, dnerf, tretschk2021non},
%
efficient inference~\cite{yu2021plenoctrees, yu2021plenoxels, chen2022tensorf, lombardi2021mixture, muller2022instant, fpo_cvpr}, 
%
editing~\cite{liu2021editing, zhang2021stnerf, wang2021clip, jang2021codenerf,kobayashi2022decomposing}, and
%
multi-task learning~\cite{zhi2021place,liu2022unsupervised,fan2022nerf}.

Much  effort has been made to improve the accuracy of NeRF's rendered novel views and reconstructed geometry.
%
Mip-NeRF~\cite{barron2021mip} mitigates unsightly aliasing artifacts in NeRF in representing coarse and fine details.
%
Mip-NeRF 360~\cite{barron2022mip} uses a non-linear scene parameterization and online distillation to tackle unbounded scenes.
%
Aug-NeRF~\cite{chen2022aug} leverages worst-case perturbations to enable NeRF in smoothness-aware geometry reconstruction.
%
RegNeRF~\cite{Niemeyer2021Regnerf}  renders patches from unobserved viewpoints for a given radiance field and regularizes appearance and geometry.
%
AligNeRF~\cite{jiang2022alignerf}  addresses misalignment problems caused by moving objects or small camera calibration errors.
%
DS-NeRF~\cite{deng2022depth}  takes advantage of readily-available depth supervision from SfM.
%
NerfingMVS~\cite{wei2021nerfingmvs} employs  adapted depth priors from a monocular depth network to guide the sampling process of volume rendering.









\paragraph{Reflectance Decomposition.} To acquire reflectance data, sophisticated devices have traditionally been necessary to sample the light-view space~\cite{kang2018efficient, matusik2003data, nielsen2015optimal}.
%
Subsequent research has proposed practical techniques for acquiring spatially varying BRDFs, such as those presented in~\cite{kang2018efficient, matusik2003data, nielsen2015optimal, nam2018practical}.
%
More recently, deep learning methods have made it possible to acquire BRDF information from a single flash image~\cite{li2018learning,li2018materials,deschaintre2018single}.

In the context of NeRF, highly reflective objects can pose challenges in the reconstruction and relighting process. Previous works have attempted to address this issue by decomposing appearance into scene lighting and materials, but these methods assume known lighting~\cite{bi2020b, srinivasan2021nerv} or no self-occlusion~\cite{boss2021nerd, boss2021neural, zhang2021physg}.
%
Ref-NeRF~\cite{verbin2022ref} uses a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties to reproduce the appearance of glossy surfaces.
%
Despite these advances, Ref-NeRF requires accurate normal vectors and outgoing radiance estimation, which is difficult to obtain for sparse input views.
In addition, effectively addressing the view-dependent appearance problem in the context of large scenes and sparse observations remains a challenge. To address this issue, we propose a simple yet effective decomposition method to eliminate its interference without the need to estimate surface normals or outgoing radiance.





















\paragraph{Intrinsic Image Decomposition.}
Barrow and Tenenbaum introduced intrinsic images as a valuable intermediate representation for scenes~\cite{barrow1978recovering}, assuming that an image can be expressed as the pointwise product of the object's true colors or reflectance and the shading on that object. This can be represented as
$%\begin{equation}
I = R \cdot S,
$%\end %{equation}
where $I$, $R$, and $S$ denote the image, the reflectance, and the shading, respectively.

%
Early optimization-based works addressed the problem of separating an image into its reflectance and illumination components by assuming that large image gradients correspond to reflectance changes and small gradients to lighting changes~\cite{land1971lightness, horn1974determining}.
Incorporation of additional priors improves the accuracy and robustness, such as 
%
reflectance sparsity~\cite{Rother2011RecoveringIntrinsic, Shen2011IntrinsicImages}, 
%
low-rank reflectance~\cite{Bousseau2015User_assisted} and  distribution difference in gradient domain~\cite{Bi2015IntrinsicDecompositionTOG, Li2014Single}.
%
Deep learning methods~\cite{fan2018revisiting,yu2019inverserendernet, zhu2022irisformer, li2018cgintrinsics, li2018learning_intrinsic} have emerged to perform intrinsic image decomposition, estimating the reflectance and shading on labeled training data.
%
Notably and differently, in intrinsic image decomposition, where shadows and highlights are separated as high-frequency components, these components {\em may} still be separated as view-independent in our \ournerf as long as they are {\em consistent} across all input views, e.g., a static shadow is consistently observed across all views. Thus, intrinsic image decomposition is inappropriate (both overkill and inadequate) 
to the ``vi-vd" decomposition of \ournerf.

IntrinsicNeRF~\cite{ye2022intrinsicnerf} introduces intrinsic decomposition to the NeRF-based neural rendering method, which allows for editable novel view synthesis in room-scale scenes. Compared with our simple and effective appearance decomposition, IntrinsicNeRF requires dense inputs (900 images for their indoor Replica scene), which  assumes the NeRF reconstruction is accurate.



