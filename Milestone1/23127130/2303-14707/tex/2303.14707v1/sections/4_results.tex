

\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.95\linewidth]{figures/ab_app.pdf}
\end{center}
\vspace{-0.1in}
\caption{\textbf{Qualitative evaluation on appearance decomposition.} Without appearance decomposition, our model fails to recover the glossy objects such as the glass, the floor, and the plant.
}
\label{fig:ab_app}
\vspace{-0.1in}
\end{figure*}

\section{Results}
\label{sec:results}
























In this section, we provide comparisons with previous state-of-the-art NeRF-based methods and  evaluation of our main technical components, both qualitatively and quantitatively.
%
We first evaluate our proposed \ournerf on Shiny Blender dataset~\cite{verbin2022ref} containing different glossy objects rendered in Blender under conditions similar to typical NeRF's datasets, to verify our model's ability to handle challenging material properties by proper decomposition of observed appearance into the corresponding view-independent and view-dependent components.
%
We then run our proposed \ournerf on Hypersim dataset~\cite{roberts2021hypersim}, a challenging synthetic dataset of photo-realistic indoor scenes.
%
We use the given accurate camera poses for Shiny Blender and use colmap~\cite{schonberger2016structure} to estimate camera poses for Hypersim.
%
We train our \ournerf for 500K iterations to guarantee convergence on a single NVIDIA GeForce RTX 3090 Ti GPU. 
%
For Shiny Blender, we use 100 input views for training. For Hypersim, we use around 50 to 80 input views for training, depending on the scale of the scene. All the shown cases and reported metrics are from held-out views.
%
We report three error metrics, including peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM)~\cite{wang2004image}, mean absolute error (MAE), and learned perceptual image patch similarity (LPIPS)~\cite{zhang2018unreasonable}. 
%
We  include more experiment details and results in the supplemental material, including videos, and encourage the readers to check them.

\paragraph{Appearance Decomposition on Glossy Objects}
To verify \ournerf's ability to decompose object appearances into the corresponding view-independent and view-dependent components, we evaluate \ournerf on Shiny Blender dataset and render the view-independent component image and  color image with both view-independent and view-dependent components (Fig.~\ref{fig:results_ref}).

\paragraph{Comparison on Challenging Indoor Scenes}
We compare \ournerf with NeRF~\cite{mildenhall2020nerf}, mip-NeRF 360~\cite{barron2022mip} and DS-NeRF~\cite{deng2022depth}, which are representative NeRF-based methods and strong baselines for large-scale scenes. As  shown in Fig.~\ref{fig:results_indoor} and Tab~\ref{table:eval2}, our method recovers intricate details of objects in the indoor scene, especially objects with glossy surfaces, and has few observations.
%
Note that DS-NeRF uses sparse depth maps for supervision, while our method and other baselines do not.


\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.95\linewidth]{figures/ab_geo.pdf}
\end{center}
\vspace{-0.1in}
\caption{\textbf{Qualitative evaluation on geometry correction.} Without geometry correction NeRF tends to generate floaters.
}
\label{fig:ab_geo}
\vspace{-0.1in}
\end{figure*}

\paragraph{Ablation  Study on our Architecture Design}
We qualitatively and quantitatively evaluate the main components of \ournerf.

%\textbf{Appearance Decomposition.} 
%As  shown in 
Fig.~\ref{fig:ab_app} shows that without appearance decomposition, NeRF struggles to recover the glossy floor and plants.
%\textbf{Geometry Correction.} 
%As  shown in 
Fig.~\ref{fig:ab_geo} shows that  without geometry correction, the NeRF needs to generate floaters in order to explain the view-dependent observations.
%
Quantitative evaluations are given in Tab.~\ref{table:eval2}.

\begin{table}[ht]
    
	\centering
	 % YANSHUN: scale tabular to fit linewidth
    	\begin{tabular}{l|cccc}
    	    \multicolumn{5}{c}{ \colorbox{best1}{best} \colorbox{best2}{second-best} } \\ \hline
            Method                    & PSNR$\uparrow$          & SSIM$\uparrow$          & MAE$\downarrow$         & LPIPS$\downarrow$ \\ \hline
            NeRF         & 20.90                  & 0.84                & 0.052                  & 0.2509 \\
            Mip-NeRF 360 &21.32  & 0.82 & 0.048& 0.3399 \\
            DS-NeRF &\cellcolor{best2}26.27  & \cellcolor{best2}0.89 & \cellcolor{best1}0.03 & 0.2275 \\\hline
            w/o app. dec. &22.93  & 0.85 & 0.041 &\cellcolor{best2}0.21 \\
            w/o dec. geo. &25.88  & 0.86 & 0.0059 & 0.2299 \\\hline
        Ours      & \cellcolor{best1}27.29 & \cellcolor{best1}0.89 &  \cellcolor{best2}0.041& \cellcolor{best1}0.1975 \\\hline
	    % 	10-min fine-tuning & 31.63  & \textbf{0.9771} & 0.0060 &   0.0360 \\
        % 	longer fine-tuning  & \textbf{35.44} & 0.9749  & \textbf{0.0033}& \textbf{0.0324}\\
        \end{tabular}
        
%    }
\rule{0pt}{0.05pt}
\caption{\textbf{Quantitative comparison and evaluation.} We compare our proposed \ournerf with representative NeRF-based methods and their variants.  }
\label{table:eval2}
\vspace{-5mm}
\end{table}


\section{Discussion and Limitations}
Our method assumes fixed lighting condition and no semi-transparent objects in a scene. In addition, we observe that, when we deal with sparse inputs and the specular highlights of a point appear in most of the inputs, such highlights may be regarded as view-independent colors, since our method does not make any assumption about the surface properties and colors. Below, we discuss some important questions related to our work:

\vspace{2mm}
\noindent\textit{Why are there floaters in sparse but not in dense inputs?} In vanilla NeRF, observation errors are backpropagated according to Eqn.~\ref{eqn:vanillanerf}, which are backpropagated equally to density and color along a given ray without any prior. With dense inputs, the strong geometry constraint from other view points can correct the density errors along a ray, and thus the view dependent observations will be correctly backpropagated to the color component. In contrast, when the number of inputs is limited, the network cannot resolve the ambiguity that the view dependent observations are caused by change of colors, or by the semi-transparent occluders, i.e., floaters. Since errors are backpropagated equally to both density and color along a ray, generating \textit{floaters} %solution 
is more preferable by Eqn.~\ref{eqn:vanillanerf}.

\vspace{2mm}
\noindent\textit{What are the benefits of vi- and vd- color decomposition?}
%The vi- and vd- decomposition of colors 
Such decomposition can stabilize the solution by reducing the ambiguity in handling view-dependent observations as residual errors in ${\bf c}_\text{vd}$, while keeping the ${\bf c}_\text{vi}$ stable across multiple views, thus  leading to a reconstruction of higher quality. Additionally, in downstream tasks such as NeRF object detection~\cite{hu2022nerf} and segmentation~\cite{ren2022neural}, one may have to estimate the color of voxel features that is independent of viewpoints. Our ${\bf c}_\text{vi}$ can provide such voxel feature extraction for free without additional computations.

\vspace{2mm}
\noindent\textit{Why is the decomposition in Eqn.~\ref{eqn:blend} correct?} 
Eqn.~\ref{eqn:blend} can be considered as a simplified BRDF model, e.g., a simplified Phong model with diffuse and specular components but without normal and light. Although not entirely physically correct, this formulation can handle most view-dependent observations in the real world without resorting to estimating surface normals and incoming lighting conditions, thus providing a fast and easy way to optimize. According to our experiments, this formulation is generally applicable, and the resulting decomposition is reasonably accurate. 




%Our estimation is totally data-driven. Such data-driven approaches make our method very general and does not require any pre-processing or additional input while achieving state-of-the-art or even better reconstruction of NeRF with sparse inputs.


% # ablation
% ## vivd: sh vs mean
% ## geo: monotone
