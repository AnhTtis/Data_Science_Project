\section{Method}
In this section, we present \ournerf to effectively %an extension of NeRF that aims to 
address the floater artifacts caused by current inaccurate NeRF reconstruction.
%discussed in 
%Sec. \ref{sec:intro}.
%
%\ournerf is intended to be 
\ournerf can be applied as an easy plug-in component %that can be incorporated 
into any existing NeRF-like models, including MLP-based and voxel grid-based approaches, without resorting to  extra knowledge such as normal maps, depth maps, or lighting conditions.
%
In Sec.~\ref{sec:overall}, we introduce the overall architecture of \ournerf compared to the vanilla NeRF.
%
Sec.~\ref{sec:vivd} describes our principled implementation for decomposing an appearance into view-dependent (``vd") and view-independent (``vi") components to achieve better novel view synthesis and 3D geometry reconstruction.
%
Finally, we present our geometry correction strategy  in Sec.~\ref{sec:geo}. 
%


\begin{figure}[t]
\begin{center}
    \includegraphics[width=1\linewidth]{figures/method.pdf}
\end{center}
\vspace{-0.1in}
\caption{\textbf{Vanilla NeRF and \ournerf architectures. }
%
Similar to vanilla NeRF, we estimate density $\sigma_0$ and color $\mathbf{c}_0$ from the spatial MLP and the directional MLP.
%
On the other hand, we propose ``vi-vd" appearance decomposition based on spherical harmonics (SHs) and obtain a final color estimation $\mathbf{c}$ by summing the view-independent color component $\mathbf{c}_{\text{vi}}$ produced by the spatial MLP, with the view-dependent color component $\mathbf{c}_{\text{vd}}$ produced by the directional MLP.
%
Then a geometry correction procedure refines the initial estimation and obtains the final density estimation $\sigma$.
}
\label{fig:method}
\vspace{-0.15in}
\end{figure}


\subsection{Overall Architecture}
\label{sec:overall}
Fig.~\ref{fig:method} compares  vanilla NeRF~\cite{mildenhall2020nerf} and our model, both taking a sampled spatial coordinate point $\mathbf{x} = (x, y, z)$ and direction $\mathbf{d} = (\theta, \phi)$ as input and output the volume density and color.
%

Vanilla NeRF uses a spatial MLP to estimate volume density $\sigma$ at position $\mathbf{x}$.
%
Then the directional MLP takes as input the direction $\mathbf{d}$ as well as spatial feature $\mathbf{b}$ to estimate a view-depenent output color $\mathbf{c}$.
%
In \ournerf architecture, we also use the spatial MLP and the directional MLP to output an initial estimation of density $\sigma_0$ and color $\mathbf{c}_0$, similar to the vanilla NeRF. 
%
The initial estimation of color corresponding to ray $\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}$ can be evaluated from  $\sigma_0$ and $\mathbf{c}_0$:
%
\begin{equation}
\label{eqn:vanillanerf}
    \hat{\mathbf{C}}_0 = \sum_{k=1}^K \hat{T}_0(t_k)\alpha(\sigma_0(t_k)\delta_k)\mathbf{c}_0(t_k), 
\end{equation}
where $\hat{T}_0(t_k)=\operatorname{exp}\left(-\sum_{k^{'}=1}^{k-1}\sigma_0(t_k)\delta(t_k)\right)$, $\alpha \left({x}\right) = 1-\exp(-x)$, and $\delta_p = t_{k+1} - t_k$.
%
Without requiring additional inputs, \ournerf differs from vanilla NeRF in that while making the initial estimation, we also predict the view-independent color component $\mathbf{c}_{\text{vi}}$ with the spatial MLP and the view-dependent color component $\mathbf{c}_{\text{vd}}$  with the directional MLP.
%
We supervise these estimations by performing an SH-based decomposition of the initial color estimation $\mathbf{c}_0$, as described in Sec.~\ref{sec:vivd}.
%
The directional MLP  also estimates a view-dependent factor $\gamma$ and we obtain a final color estimation $\mathbf{c}$ by:
\begin{equation}
\label{eqn:blend}
    \mathbf{c} = \gamma\mathbf{c}_{\text{vi}} + (1-\gamma)\mathbf{c}_{\text{vd}}.
\end{equation}
%
Note that $\mathbf{c}_{\text{vi}}$ captures the overall scene color while $\mathbf{c}_{\text{vd}}$ captures the color variations due to changes in viewing angle.
%
By blending these two components with the factor $\gamma$, we can synthesize a faithful color of the underlying 3D scene, even from a limited number of input views.
%


%
To better eliminate the floating artifacts, we propose a geometry correction procedure on $\sigma_0$ to correct the initial density estimation and obtain the final density estimation $\sigma$, as described in Sec.~\ref{sec:geo}.
%
Such procedure is based on practical assumptions and is easy to implement.

%
The final estimation of color corresponding to ray $\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}$ is then computed as
%
\begin{equation}
\label{vol_rend}
    \hat{\mathbf{C}} = \sum_{k=1}^K \hat{T}(t_k)\alpha(\sigma(t_k)\delta_k)\mathbf{c}(t_k), 
\end{equation}
where $\hat{T}(t_k)=\operatorname{exp}\left(-\sum_{k^{'}=1}^{k-1}\sigma(t_k)\delta(t_k)\right)$, $\alpha \left({x}\right) = 1-\exp(-x)$, and $\delta_p = t_{k+1} - t_k$.
%
We train the network using photometric loss based on both the initial and the final estimations
\begin{equation}
    \mathcal{L}_{\text{pho}} =\sum_{\mathbf{r}\in\mathcal{R} } \left\|\hat{\mathbf{C}}_0 (\mathbf{r}) - \mathbf{C}(\mathbf{r}) \right\|_2^2 + \left\|\hat{\mathbf{C}}(\mathbf{r}) - \mathbf{C}(\mathbf{r}) \right\|_2^2.
\end{equation}


\subsection{Appearance Decomposition}
\label{sec:vivd}
To guide our vi-vd decomposition, we utilize Spherical harmonics (SHs) which are widely used as a low-dimensional representation for spherical functions, and have been used to model Lambertian surfaces \cite{ramamoorthi2001relationship, basri2003lambertian} as well as glossy surfaces \cite{sloan2002precomputed}. 
%
To use SH functions to model a given function, we query the SH functions $Y_{\ell}^m:\mathbb{S}^2\mapsto\mathbb{R}$ at a viewing angle $\mathbf{d}$ and then fit the estimation $\mathbf{c}_0$ by finding the corresponding coefficients. 
%
We use low-degree SH functions to compute ideal values of view-independent color components, and high-degree SH functions for view-dependent components.
%
In this subsection, we will perform all of our calculations at an arbitrary position $\mathbf{x}$ in space, and therefore we will omit the symbol $\mathbf{x}$ from our notation.

%
We use  $\mathbf{y}(\mathbf{d})\in\mathbb{R}^{L}$  to represent the set of SH function values at the viewing angle $\mathbf{d}$:
\begin{equation}
    \mathbf{y}(\mathbf{d}) = \left[
        Y_{0}^0(\mathbf{d}),Y_{1}^{-1}(\mathbf{d}), Y_{1}^{0}(\mathbf{d}),Y_{1}^{1}(\mathbf{d}), \dots, Y_{\ell_{\text{max}}}^{\ell_{\text{max}}}(\mathbf{d})\right]^\top,
\end{equation}
where $L=(\ell_{\text{max}}+1)^2$. 
%
To ensure clarity, we will use $c:\mathbb{S}^2\mapsto\mathbb{R}$ to represent one of the three channels of $\mathbf{c}_0$ at a given position $\mathbf{x}$ (also $c_{\text{vi}}$ and $c_{\text{vd}}$), noting the derivation should be readily extended to all three channels. 
%
We begin by sampling a set of $N$ viewing angles ${\mathbf{d_i}}, {1\leq i\leq N}\subset\mathbb{S}^2$.
%
The colors of all the sample directions are represented using a vector $\mathbf{s}\in\mathbb{R}^{N}$:
\begin{equation}
    \mathbf{s} =\begin{bmatrix}c(\mathbf{d}_1)&c(\mathbf{d}_2)&\dots& c(\mathbf{d}_{N-1})&c(\mathbf{d}_N)\end{bmatrix}^\top
\end{equation}
The coefficients to be determined are represented by a vector $\mathbf{k}\in\mathbb{R}^{L}$. To find the optimal coefficients that fit the view-dependent color estimation, we solve the following optimization problem:
\begin{equation} 
    \min_{\mathbf{k}\in\mathbb{R}^L} \|\mathbf{s}-\mathbf{Y}\mathbf{k}\|_2^2,
\end{equation}
where 
\begin{equation}
    \mathbf{Y} = \begin{bmatrix}
        \mathbf{y}(\mathbf{d}_1) & \mathbf{y}(\mathbf{d}_2)&\dots&\mathbf{y}(\mathbf{d}_{N-1})&\mathbf{y}(\mathbf{d}_N)
    \end{bmatrix}.
\end{equation}
This is a standard linear regression problem, where we seek to find the values of the coefficient vector $\mathbf{k}$ which minimizes the least squares error between the vector $\mathbf{s}$ and the linear combination of the columns of $\mathbf{Y}$, weighted by the coefficients in $\mathbf{k}$. Using the normal equation, the solution is given by:
\begin{equation}
    \mathbf{k}^* = (\mathbf{Y}^\top\mathbf{Y})^{-1}\mathbf{Y}^\top\mathbf{s}
\end{equation}
We can use the solution coefficients $\mathbf{k}^*$ as weights to linearly combine SH functions. Retaining the low-degree SH functions allows us to capture the view-independent appearance of the scene. Conversely, including high-degree SH functions leads to a high-frequency view-dependent residue. To differentiate between the two, we denote the low-degree and high-degree functions as $L_{\text{low}}$ and $L_{\text{high}}$, respectively.
%
To compute ideal values for the view-independent component, we can use the solution coefficients and apply the following equation:
\begin{equation}
\tilde{c}_\text{vi}=\sum_{i=1}^{L_\text{low}} \frac{1}{4\pi r^2} \iint_{\mathbb{S}^2} k^*_{i}y_i(\mathbf{d}) \sin\theta d\theta d\phi,
\end{equation}
where we take the mean value around the $\mathbb{S}^2$ surface. 
%
In our implementation, we approximate it by 
\begin{equation}
\tilde{c}_\text{vi}=\sum_{i=1}^{L_\text{low}} \sum_{i=1}^N k^*_{i}y_i(\mathbf{d_i}),
\end{equation}
%
This value is then used to guide the output of the view-independent color component $\mathbf{c}_{\text{vi}}$ from the spatial MLP using a regularizer. Specifically, we use the following equation to compute the vi-regularizer loss $\mathcal{L}_{\text{vi}}$:
\begin{equation}
\label{eqn:lvi}
\mathcal{L}_{\text{vi}} = (\mathbf{c}_{\text{vi}}- \tilde{\mathbf{c}}_{\text{vi}})^2.
\end{equation}
We apply the following equation to compute optimal values for the view-dependent component:
\begin{equation}
\tilde{c}_\text{vd}(\mathbf{d})=\sum_{i=L_\text{high}}^{L} k^*_{i}y_i(\mathbf{d}).
\end{equation} 
Incorporating the computed value to guide the output of the view-dependent color residue $\mathbf{c}_{\text{vd}}$ from the directional MLP using the vd-regularizer loss:
\begin{equation}
\label{eqn:lvd}
    \mathcal{L}_{\text{vd}} = \left\|
    \begin{bmatrix}
        \mathbf{c}_{\text{vd}}(\mathbf{d_1}) \\ \vdots \\ \mathbf{c}_{\text{vd}}(\mathbf{d_N})
    \end{bmatrix} - 
    \begin{bmatrix}
        \tilde{\mathbf{c}}_{\text{vd}}(\mathbf{d_1}) \\ \vdots \\\tilde{\mathbf{c}}_{\text{vd}}(\mathbf{d_N})
    \end{bmatrix}\right\|_2^2.
\end{equation}
As aforementioned we consider a given position $\mathbf{x}$ in the space, while in actual implementation we take the $\ell_2$-norm among all the positions in a sampled batch for Eqn. \ref{eqn:lvi} and Eqn. \ref{eqn:lvd}.




\subsection{Geometry Correction}
\label{sec:geo}
\begin{figure}[t]
\vspace{-0.2in}
\begin{center}
    \includegraphics[width=1\linewidth]{figures/density.pdf}
\end{center}
\vspace{-0.25in}
\caption{\textbf{An example traced ray's density profile. }
%
Peaks in region (a) and region (e) appear as floating artifacts and would interfere with the rendering process.
%
}
\label{fig:density}
\vspace{-0.2in}
\end{figure}
\ournerf  is consistent with standard NeRF in that  an initial density estimation, denoted as $\sigma_0$, is generated for volume rendering with the initial color estimation $\mathbf{c}_0$. 
%
We propose a geometry correction strategy for the final rendering that simultaneously refine the density estimation while better handling  unsightly floater artifacts.
%
% We decompose the density field into a view-independent component $\sigma_{\text{vi}}$ and a view-dependent component $\sigma_{\text{vd}}$.
%

To understand our working principle, without losing generality, consider a given ray intersecting an opaque 3D scene consisting of a single sphere. In the absence of noise, there exist at most two relevant  intersections, corresponding to the front and back  surface visible from the camera at both ray directions. This is in line with traditional ray tracing in computer graphics, where only the ray-object intersection closest to the camera should be returned.  Figure~\ref{fig:density} shows a density profile along a given ray, where the two salient peaks respectively correspond to the closest front and back surface visible along the ray in both camera view directions.
%, or in other words, the view-dependent geometry $\sigma_{\text{vi}}$. 
Other peaks along the same ray %, or $\sigma_{\text{vd}}$, 
will either manifest as foggy floaters hovering in the density volume or correspond to surfaces in the scene that are hidden by the front and back surfaces and, thus should not be visible or rendered at all.

Thus, our geometry correction strategy given by Alg.~\ref{alg:geo} begins the density decomposition process by performing a forward ray tracing to identify the first salient peak $k_{\text{peak}}^{\text{front}}$ from the front-facing direction, followed by a backward ray tracing pass to identify the first salient peak $k_{\text{peak}}^{\text{back}}$ from the back-facing direction.
%
We then retain the density component within and between the neighborhoods of $k_{\text{peak}}^{\text{front}}$ and $k_{\text{peak}}^{\text{back}}$ and zero the rest\footnote{With abuse of language, salient plateaus are also referred to as ``peaks'', which are also detected using Alg.~\ref{alg:geo} if the pertinent ray hits a solid (sphere).}.

\begin{algorithm}[ht]
\caption{Geometry Correction} 
\hspace*{0.02in} {\bf Input:} 
 $\{\sigma_0(t_k)\}_{k=1,...,K}$, $\sigma_{\text{thres}}, m$ \\ %Sampled density along a traced ray
\hspace*{0.02in} {\bf Output:} 
 $\{\sigma(t_k)\}_{k=1,...,K}$
\begin{algorithmic}[1]

\For{$k\leftarrow1 \text{ to } K$} \hfill // Forward pass ray tracing.
\If{$\sigma_0(t_k)>\sigma_{\text{thres}}$}
\State $k_{\text{peak}}^{\text{front}} \leftarrow k$
\State \textbf{break}
\EndIf
\EndFor

\For{$k\leftarrow K \text{ to } 1$}\hfill // Backward pass ray tracing.
\If{$\sigma_0(t_k)>\sigma_{\text{thres}}$}
\State $k_{\text{peak}}^{\text{back}} \leftarrow k$
\State \textbf{break}
\EndIf
\EndFor

\For{$k\leftarrow 1 \text{ to } K$} 
\If{$k<k_{\text{peak}}^{\text{front}}-m$ \textbf{or} %$k_{\text{peak}}^{\text{front}}+m<k<k_{\text{peak}}^{\text{back}}-m$ \textbf{or} 
$k>k_{\text{peak}}^{\text{back}}+m$  }
\State $\sigma(t_k)\leftarrow 0$
\Else 
\State $\sigma(t_k)\leftarrow \sigma_0(t_k)$
\EndIf
\EndFor
\Return  $\{\sigma(t_k)\}_{k=1,...,K}$
\end{algorithmic}
\label{alg:geo}
\end{algorithm}


\begin{figure*}[t]
\begin{center}
    \includegraphics[width=\linewidth]{figures/ref_results.pdf}
\end{center}
\vspace{-0.15in}
\caption{\textbf{Qualitative evaluation of \ournerf on Shiny Blender dataset.} We render the view-independent component image, and the final color image combining both view-independent and view-dependent components to compare with the ground truth.
}
\label{fig:results_ref}
\vspace{-0.1in}
\end{figure*}

\begin{figure*}[ht]
\begin{center}
    \includegraphics[width=\linewidth]{figures/indoor_res.pdf}
\end{center}
\vspace{-0.15in}
\caption{\textbf{Qualitative Comparison between \ournerf and baselines.} \ournerf recovers intricate object details while removing annoying ``floaters" even when observations are sparse. 
}
\label{fig:results_indoor}
\vspace{-0.15in}
\end{figure*}

\begin{comment}
The density geometry is based on the assumption that, in the forward pass ray tracing, only one prominent peak $k_{\text{peak}}^{\text{front}}$ should affect the rendering color.
%
This front, prominent peak in the density field corresponds to the first surface the traced ray hits. 
%
This assumption is in line with traditional ray tracing in computer graphics, where in the absence of noise, only one intersection per ray should be returned to indicate the closest ray-object intersection.
%
On the other hand, in the backward pass ray tracing, there should only be one prominent peak $k_{\text{peak}}^{\text{front}}$.
%
NeRF model does not involve backward pass ray tracing, but this peak would affect the rendering of some rays originating from the opposite side of the scene as it corresponds to the first surface it hits.
%
We view these two peaks' neighborhoods as view-independent components of the density field.


Other peaks along the same ray will either manifest as foggy floaters hovering in the density volume or correspond to surfaces in the scene that are hidden by the front and back surfaces and, thus will not be visible along that particular ray.
%
The former would influence the final color of the rendered pixel, while the latter is insignificant since the weight assigned to it by the volume rendering formula would be close to zero, according to Eqn.~\ref{vol_rend}
%

\end{comment}



% \begin{algorithm}[ht]
% \caption{Geometry Correction} 
% \hspace*{0.02in} {\bf Input:} 
%  $\{\sigma_0(t_k)\}_{k=1,...,K}$, $\sigma_{\text{thres}}, m$ \\ %Sampled density along a traced ray
% \hspace*{0.02in} {\bf Output:} 
%  $\{\sigma(t_k)\}_{k=1,...,K}$
% \begin{algorithmic}[1]
% \State $k_{\text{peak}} \leftarrow -1$
% \For{$k\leftarrow1 \text{ to } K$} 
% \State $\sigma(t_k) \leftarrow \sigma_0(t_k)$

% \If{$\sigma(t_k)>\sigma_{\text{thres}}$ \textbf{and} $k_{\text{peak}} = -1$}
% \State $k_{\text{peak}} \leftarrow k$
% \EndIf
% \EndFor

% \For{$k\leftarrow 1 \text{ to } K$} 
% \If{$k<k_{\text{peak}}-m$ \textbf{or} $k>k_{\text{peak}}+m$ }
% \State $\sigma(t_k)\leftarrow 0$
% \EndIf
% \EndFor
% \Return  $\{\sigma(t_k)\}_{k=1,...,K}$
% \end{algorithmic}
% \label{alg:geo}
% \end{algorithm}


\begin{comment}
To justify the assumption that the very first salient peak from the ray origin is the only valid peak along each traced ray during NeRF inference, we provide an example in Fig.~\ref{fig:density}, where we plot the density profile along a traced ray.
% 
We keep density in region (b) and region (d), corresponding to the first significant peaks from the front and back-facing view, respectively.
\end{comment}

Refer to Fig.~\ref{fig:density} again: peaks in region (a) and region (e) appear as floating artifacts and would interfere with the rendering process, and so they are discarded.
%
Notably, multiple salient peaks may exist corresponding to other surface points along the ray, such as region (d),  e.g., corresponding to the two slices of toast intersected by the pertinent ray in the previous figure.
%
If the salient peak (d) is further from the ray origin but lower than (b), as Fig.~\ref{fig:density} shows, the corresponding surface point is occluded by (b), which can be safely detected by backward pass while occlusion-correct rendering is unaffected, as peak (d) is lower. 
%moved to $\sigma_{\text{vd}}$ while noting that it may be visible from other input views. 
%
Otherwise, suppose the peak (d) is higher (e.g., the toast further back is higher), then the peak in (b) should have been clamped to reveal the true geometry (d) as the first salient peak as seen from the ray origin.
%

\begin{comment}
\TODO{
To conclude this section, we revisit Eqn.~\ref{eqn:blend}, where we incorporate a view-dependent factor, $\gamma$, to blend $\mathbf{c}_{\text{vi}}$ and $\mathbf{c}_{\text{vd}}$.
%
In hindsight, NeRF's success is partly attributed to its ability to simulate specular reflections using floaters. 
%
However, clamping the density at the peaks corresponding to floaters,  as is done in the geometry correction,   reduces the model's ability to faithfully reflect these observations. 
%
To compensate for this and absorb the gap caused by clamping, we rely on the $\gamma$ factor to allocate weights between the view-independent and view-dependent components at any position.
}
\end{comment}

%in the redistributed region, in this case, region (c).
%
%Such peaks are not floaters but explain other input views.
%
%The surface or object corresponding to the first peak occludes such peaks and  the weight of this part of the region in volume rendering is close to $0$ according to Eqn. \ref{vol_rend}.
%
%Therefore, discarding them will not undermine the rendering result. 
%






% %
% Imposing smoothness onto NeRF can be done in many explicit ways, such as regularizing total variation xxx, Laplacian of surface xxx, etc. 
% %

% %
% Also, their computation also usually operates on discretized volumetric representations and needs extra differentiation steps to be added in NeRF.



% dreamfields dreamfusion's geometry prior
% Geometry regularizers. The mip-NeRF 360 model we build upon contains many other details that
% we omit for brevity. We include a regularization penalty on the opacity along each ray similar to
% Jain et al. (2022) to prevent unneccesarily filling in of empty space. To prevent pathologies in the
% density field where normal vectors face backwards away from the camera we use a modified version
% of the orientation loss proposed in Ref-NeRF (Verbin et al., 2022). This penalty is important when
% including textureless shading as the density field will otherwise attempt to orient normals away from
% the camera so that the shading becomes darker. Full details on these regularizers and additional
% hyperparameters of NeRF are presented in the Appendix A.2.



