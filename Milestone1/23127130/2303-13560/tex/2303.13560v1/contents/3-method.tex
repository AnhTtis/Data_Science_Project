\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{Figs/CollaDepth_v2.png}
  \vspace{-3mm}
  \caption{System overview. \texttt{CoCa3D} is a camera-only 3D detector integrated with two collaboration modules. Collaborative depth estimation (Co-Depth) enhances the single-agent estimated depth to achieve more accurate 3D feature. Collaborative detection feature learning (Co-FL) complements the single-agent 3D feature to achieve more holistic 3D detection.}
  \vspace{-5mm}
  \label{fig:framework}
\end{figure*}

\vspace{-2mm}
\section{Collaborative Camera-Only 3D Detection}
\vspace{-1mm}

This section presents~\texttt{CoCa3D}, 
collaborative camera-only 3D detection framework, 
which enables multiple agents to share visual information with each other, promoting more holistic 3D detection.


\subsection{Problem Formulation}
\vspace{-1mm}
Consider $N$ agents in the scene, 
let $\mathcal{X}_i$ be the RGB image collected by the $i$th agent and $\mathcal{O}^0_i$ be the corresponding ground-truth detection. The objective of~\texttt{CoCa3D} is to maximize detection performances of all agents given certain communication budget $B$; that is,
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}
\begin{small}
\begin{eqnarray*}
    \underset{\theta,\mathcal{P}}{\max}~\sum_{i} 
     g \left(\Phi_{\theta} \left(\mathcal{X}_i,\{\mathcal{P}_{i\rightarrow j} \}_{j=1}^N \right), \mathcal{O}^0_i  \right),
    {\rm s.t.~} \sum_{i} |\mathcal{P}_{i\rightarrow j} | \leq B,
\end{eqnarray*}
\end{small}
where $g(\cdot,\cdot)$ is the detection evaluation metric, $\Phi(\cdot)$ is a detection model with trainable parameter $\theta$, and $\mathcal{P}_{i\rightarrow j}$ is the message transmitted from the $i$th agent to the $j$th agent. The key challenge is to determine the messages $\mathcal{P}_{i\rightarrow j}$, which should be both informative and compact. 
Our design rational comes from two aspects: First, since the major gap between camera and LiDAR is the depth, the message should include depth information. This would allow different viewpoints from multiple agents to disambiguate the infinite depth possibilities and localize the correct depth candidate. Second, the message should include detection clues to provide complementary detection information, which can fundamentally overcome inevitable limitations of single-agent detection, such as occlusion and long-range issues.

% The motivation is that single-agent camera-only 3D object detection is an ill-posed task as each image pixel corresponds to a projected ray in the physical world, that is, there are infinite depth possibilities in the 3D space. Furthermore, single-agent encounters inevitable physical limitations, such as limited field of view, occlusion, and long-range issues. Multi-agent collaboration can fundamentally address these physical limitations: a) different viewpoints from multiple agents can disambiguate the infinite depth possibilities and localize the correct depth candidate; b) multi-view compensation enables more accurate and holistic 3D representations.

Based on the above intuition, the proposed~\texttt{CoCa3D} includes a) single-agent camera-only 3D detection, which achieves basic  depth estimation and detection ability, and b) multi-agent collaboration, which shares both estimated depth information and detection features to improve 3D representation and detection performances; see Fig.~\ref{fig:framework}.



% \texttt{CoCa3D} includes three parts: a) single-agent camera-only 3D object detection, which follows the monocular 3D object detector~\cite{CaDDN}. It learns to project image features into 3D space by estimating depth distribution;
% b) collaborative depth estimation, which improves the single-agent depth estimation with multi-view geometry to generate more accurate 3D feature;
% and c) collaborative detection feature learning, which improves the single-agent 3D feature with the shared compact complementary messages.

\vspace{-1mm}
\subsection{Single-agent camera-only 3D detection}
\vspace{-1mm}
Single-agent camera-only 3D object detection network learns to detect 3D objects in the physical space based on the 2D camera inputs. Here we adopt CaDNN~\cite{CaDDN}, an off-the-shelf architecture. The main idea is to project the flat image feature into all the possible depths in the 3D space, and then attentively aggregate the 3D voxel feature and collapse them into the BEV feature. To mention that, the BEV feature is used because it largely reduces the computation cost, while performing similarly to 3D voxel features~\cite{lang2019pointpillars}. 
The architecture consists of five modules as detailed below.

 % {\HC  The \textbf{encoder} extracts features from the observed images. Based on the image features, the \textbf{depth estimation} module estimates the depth distribution and the uncertainty for each spatial region and the \textbf{voxel transformation} module lifts the flat image feature to the 3D voxel space via the camera calibration parameters. Then the voxel features are weighted based on the depth distribution and collapsed to BEV feature in the \textbf{collapse} module. The \textbf{decoder} decodes the BEV feature into object detections.}\Note{SC: not your contribution, too long-winded}

\vspace{2pt}
\noindent \textbf{Encoder.} The encoder extracts features from the input images. For agent $i$, given its input $\mathcal{X}_i$, the output feature map of encoder is 
$ \mathcal{F}_i = \Phi_{\rm enc}(\mathcal{X}_i) \in \mathbb{R}^{H \times W \times C},$ where $H,W,C$ are the height, width and channel of the image feature. 


\vspace{2pt}
\noindent \textbf{Depth estimation.} The depth estimation module predicts pixel-wise categorical depth distribution over a set of predefined depth bins to accurately locate image information in 3D space. Treating the estimation as a classification problem can capture the inherent depth estimation uncertainty to reduce the impact of erroneous depth estimates, as stated in CaDNN~\cite{CaDDN}.
To achieve this, a parametric depth estimation network $\Phi_{\rm Depth}(\cdot)$ is used, and the pixel-wise categorical depth distribution is obtained as
% \vspace{-2mm}
\begin{equation}
\label{eq:depth}
    \mathcal{D}_i =\Phi_{\rm Depth}(\mathcal{F}_i)\in\mathbb{R}^{H \times W \times D},    
\end{equation}
whose $(h,w)$th element $\mathcal{D}_i(h,w) \in \mathbb{R}^{D}$ refers to the estimated depth distribution of the $(h,w)$th image pixel with $D$ as the number of discretized depth bins.

\vspace{2pt}
\noindent \textbf{Voxel transformation.} Voxel transformation projects the image feature to the 3D space, given all the possible depth candidates and the known image calibration matrix, resulting in a 3D voxel representation $\widehat{\mathcal{V}}_i\in\mathbb{R}^{X\times Y\times Z \times C}$. The camera projection matrix $\mathbf{P}\in\mathbb{R}^{3\times 4}$ defines the mapping between the 3D voxel coordinate to the image pixel coordinate, this is, $[u,v,d]=\mathbf{P}\cdot[x,y,z,1]$. Similarly, the depth probabilities can be transformed to the 3D voxel space, resulting in $\widehat{\mathcal{D}}_i\in\mathbb{R}^{X \times Y \times Z}$, where each element indicates the confidence that the feature pixel belongs to the voxel.


\vspace{2pt}
\noindent \textbf{Collapse.} The voxel feature is collapsed to a single height plane to generate the bird's-eye-view (BEV) feature as $\overline{\mathcal{B}}_i=\Phi_{\rm clp}( \widehat{\mathcal{D}}_i \odot \widehat{\mathcal{V}}_i)\in\mathbb{R}^{X\times Y \times C}$, where $\odot$ is the element-wise multiplication. Symbol $\cdot$ / $\widehat{\cdot}$ / $\overline{\cdot}$ reflect 2D image / 3D voxel / BEV spaces, respectively. The collapse network $\Phi_{\rm clp}(\cdot)$ flattens the 3D voxel feature along the $Z$-axis and then applies $1\times 1$ convolution to reduce the channel dimension.


\noindent \textbf{Decoder.} 
The decoder takes the BEV feature as input, and outputs the objects including class and regression. Here, we adopt CenterNet~\cite{zhou2019objects}, an off-the-shelf detector. 
Given the BEV feature $\overline{\mathcal{B}}_i$, the detection decoder $\Phi_{\rm dec}(\cdot)$ generate the dense heatmap of $i$th agent by
$
     \overline{\mathcal{O}}_i  =  \Phi_{\rm dec}(\overline{\mathcal{B}}_i) \in \mathbb{R}^{X \times Y \times 7},
$
where each location of $\overline{\mathcal{O}}_i$ represents a rotated box with class $(c,x,y,h,w, \cos\alpha, \sin\alpha)$, denoting class confidence, position residual, size and angle. Non-maximum suppression (NMS) is applied to the dense predictions and generate the final sparse output of the 3D detection system. 

\vspace{2pt}
\noindent \textbf{Discussion.}
% \weidi{add a small discussion section, on what is the problems of the single-agent 3D detection, and build a link for your next section, say, which of this pipeline our collaborative approach can improve significantly, for example, depth estimation~($\hat{\mathcal{D}}$)......}
Admittedly, single-agent camera-only 3D detection makes localizing objects in the 3D space given 2D image input possible through numerous ingenious algorithm designs. The 3D detection performance is still far from perfect due to the ambiguous single-agent depth estimation and limited visual range. Tackling these two issues are challenging for a single agent, but from a perspective of multi-agent collaboration, sharing complementary visual clues makes depth estimation and visual range enlargement much more natural. This motivates our following designs.

\vspace{-1mm}
\subsection{Multi-agent collaboration} 
\vspace{-1mm}
% The proposed multi-agent collaboration module targets to improve the single-agent 3D representation and achieve more accurate and holistic 3D detection through information sharing between agents. 
Multi-agent collaboration includes two parts: i) collaborative depth estimation, which enables the sharing of depth information estimated by individual agents to produce more accurate 3D representation and less aliased BEV feature; and ii) collaborative detection feature learning, which enables the sharing of detection features of individual agents to achieve more holistic 3D detection.  

\vspace{-4mm}
\subsubsection{Collaborative depth estimation}
\vspace{-1mm}
Collaborative depth estimation (\textbf{Co-Depth}) targets to disambiguate infinite depth possibilities in single-agent camera-only depth estimation and localize the correct depth candidate through multi-view consistency.
The intuition is, for a correct depth candidate, its corresponding 3D location should be spatially consistent from multiple agents' viewpoints.
To achieve this, each agent can exchange the depth information through communication. Meanwhile, we promote communication efficiency by selecting the most critical and unambiguous depth information.
% , and take advantage of multi-view consistency to refine the single-agent estimated depth.
% However, communication systems in real-world scenarios are always constrained and they can hardly afford huge communication consumption in real time. 
% Therefore, we should exchange the most critical and unambiguous depth information to make the best use of the precious communication bandwidth. 
Accordingly, Co-Depth includes: a) depth uncertainty aware message packing, which packs compact messages with unambiguous depth information; and b) depth message fusion, which enhances depth estimation with the received depth messages.

\vspace{2pt}
\noindent \textbf{Depth uncertainty aware message packing.}
Depth uncertainty-aware (DUA) message packing packs the most critical depth information used for multi-view consistency into the to-be-sent message based on the depth uncertainty.  
The depth message includes: i) the voxel feature, which is used for the multi-view visual similarity measurement; and ii) the depth probability, which indicates the confidence that the feature pixel belongs to the voxel and is used for the multi-view candidate selection.

The spatially sparse features of $i$th agent are selected based on its spatial depth uncertainty map in the 2D image space $\mathbf{U}_{i}\in \mathbb{R}^{H\times W}$, that can be represented by the pixel-wise entropy from corresponding depth distribution,
%where each element at the image pixel $(h,w)$ is obtained through the entropy of the corresponding depth distribution, this is, 
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{equation*}
    \mathbf{U}_{i}(h,w)={\rm H}(\mathcal{D}_i(h,w)) \in \mathbb{R},
\end{equation*}
where ${\rm H}(\cdot)$ is the entropy function and $\mathcal{D}_i(h,w)$ is the depth distribution at the $(h,w)$th image pixel, following from~\eqref{eq:depth}. The intuition is, for the locations with low uncertainty score, an agent is confident about which depth bin the feature pixel belongs to. Thus, sharing information at these locations can help with depth estimation accuracy. On the contrary, for the locations with high uncertainty score, an agent is hard to tell which depth bin the feature pixel belongs to. Transmitting these spatial features would introduce ambiguity, and even degrade the single-agent depth estimation.


Specifically, a binary selection matrix is used to represent whether each location is selected or not, where $1$ denotes selected, and $0$ elsewhere. Let $\mathbf{M}^d_{i\rightarrow j} \in \{0, 1\}^{H\times W}$ be a selection matrix to determine the depth message sent from agent $i$  to $j$. Its $(h,w)$th element is
\begin{equation*}
\label{eq:selector}
\mathbf{M}^d_{i\rightarrow j}(h,w)= I(\mathbf{U}_{i} (h,w)<u_{\rm thre}),
\end{equation*}
where $I(\cdot)$ is an indicator function and $u_{\rm thre}$ is a predefined threshold. This reflects that a specific spatial area will be selected when its depth uncertainty is below a threshold. 


The selection matrix can be projected in the 3D voxel space, resulting in $\widehat{\mathbf{M}}^d_{i\rightarrow j}\in\mathbb{R}^{X\times Y \times Z}$.  Then, the selected voxel feature map $\widehat{\mathcal{Z}}_{i\rightarrow j}$ and  the selected depth distribution $\widehat{\mathcal{E}}_{i\rightarrow j}$ sent from the $i$th agent to the $j$th agent are obtained as
\begin{align*}
\widehat{\mathcal{Z}}_{i\rightarrow j}&=\widehat{\mathbf{M}}^d_{i\rightarrow j}\odot \widehat{\mathcal{V}}_i \in \mathbb{R}^{X\times Y \times Z \times C},\\
\widehat{\mathcal{E}}_{i\rightarrow j}&=\widehat{\mathbf{M}}^d_{i\rightarrow j}\odot \widehat{\mathcal{D}}_i \in \mathbb{R}^{X\times Y \times Z},    
\end{align*}
where $\widehat{\mathcal{V}}_i$ and $\widehat{\mathcal{D}}_i$ are the full 3D voxel feature map and depth distribution. Overall, the depth message sent from the $i$th agent to the $j$th agent is $\mathcal{P}^d_{i\rightarrow j}=(\widehat{\mathcal{E}}_{i\rightarrow j}, \widehat{\mathcal{Z}}_{i\rightarrow j})$, where both entries are spatially sparse and communication efficient.

% Note that i) $\mathcal{E}_{i\rightarrow j}$ provides spatial priors to request complementary information for the $i$th agent's need in the next round; the feature map $\mathcal{Z}_{i\rightarrow j}$ provides supportive information for the $i$th agent's need in the this round. They together enable mutually beneficial collaboration; ii) since  $\mathcal{Z}_{i\rightarrow j}^{(k)}$ is sparse, we only transmit non-zero features and corresponding indices, leading to low communication cost; and iii) the sparsity of $\mathcal{Z}_{i\rightarrow j}^{(k)}$ is determined by the binary selection matrix, which dynamically allocates the communication budget at various spatial areas based on their perceptual critical level, adapting to various communication conditions. 

\vspace{2pt}
\noindent \textbf{Depth message fusion.} Depth message fusion targets to enhance the depth estimation given the received depth messages through different  viewpoints of multiple agents. The intuition is that for a correct depth candidate, the visual features at the same 3D point observed by multiple agents should be similar. To achieve this, we introduce multi-view depth consistency weighting. Let $\widehat{\mathcal{S}}_i(x,y,z) \in \mathbb{R}$ be the matching score between agent $i$ and its neighbors $\mathcal{N}_i$, those who share messages with current agent, at coordinate $(x,y,z)$. The multi-view matching score is obtained as
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
\begin{small}
\begin{align*}
& \widehat{\mathcal{W}}_{j\rightarrow i}(x,y,z) = I(\widehat{\mathcal{E}}_{j\rightarrow i}(x,y,z)>p_{\rm thre})\in \{0, 1\}, \\
& \widehat{\mathcal{S}}_i(x,y,z) = \sum_{j\in \mathcal{N}_i} \widehat{\mathcal{W}}_{j\rightarrow i}(x,y,z)\left<\widehat{\mathcal{V}}_i(x,y,z), \widehat{\mathcal{Z}}_{j\rightarrow i}(x,y,z)\right>, 
\end{align*}    
\end{small}
where $\widehat{\mathcal{W}}_{j\rightarrow i}(x,y,z)$ is a binary weight used to filter out the neighboring view that has a low depth confidence, below a threshold $p_{\rm thre}$ and $\left<\cdot,\cdot\right>$ is the inner product that measures the visual feature consistency across two views. This means that either the depth candidate is wrong or it is not visible in that view (e.g. due to occlusion), where the multi-view matching score should not be considered. 
% Note that depth consistency weighting discards the candidates with low single-view depth probability. Such weighting is useful especially when the multi-view matching is ambiguous or unreliable. For example, if the pixel is within a texture-less surface, a wide range of depth candidates will lead to similar matching scores. If the scene contains reflective surfaces, the matching score will be computed between the reflections, resulting in over-estimated depth. In both cases, we can make robust prediction by favoring the depth candidates with high single-view depth probability.


Given the multi-view matching score, the collaborative depth distribution is obtained as
\begin{equation*}
    \widehat{\mathcal{D}}'_i = \Phi_{\rm CoDepth}\left(\left[\widehat{\mathcal{D}}_i; \widehat{\mathcal{S}}_i\right]\right)\in \mathbb{R}^{X \times Y \times Z},
\end{equation*}
where the $[;]$ denotes concatenation, $\Phi_{\rm CoDepth}(\cdot)$ is the collaborative depth estimation network implemented with $1\times1$ convolutions followed by a sigmoid activation. Each element of the collaborative depth distribution $\widehat{\mathcal{D}}'_i$ reflects the confidence of the feature locates in the corresponding voxel. Note that i) the collaboratively updated depth probability is decided on both the single-view confidence and the multi-view consistency; ii) the depth consistency weighting discards the candidates with low single-view depth probability to avoid ambiguity and make the estimation more robust.

Given this collaborative depth distribution, each agent can weigh the 3D voxel feature and then collapse into a BEV feature with a collapse network $\Phi_{\rm clp}(\cdot)$, this is, $\overline{B}_i=\Phi_{\rm clp}(\widehat{\mathcal{D}}'_i \odot \widehat{\mathcal{V}}_i)\in\mathbb{R}^{X\times Y \times C}$. The collapse network is implemented with $1\times 1$ convolution to reduce channel dimension.

\vspace{-4mm}
\subsubsection{Collaborative detection feature learning}
\vspace{-2mm}
Collaborative depth estimation carefully refines the depth and promotes more accurate 3D representations for each single agent. However, the physical limitations of single agent, such as, limited field of view, occlusion, and long-range issues still remain. To achieve more holistic 3D detection, each agent should be able to exchange 3D detection feature and make use of the complementary information. 
Meanwhile, we promote communication efficiency by selecting the most perceptually critical information.
Therefore, collaborative detection feature learning (\textbf{Co-FL}) includes: a) detection confidence aware message packing, which packs spatially sparse yet perceptually critical 3D features with the guidance of the detection confidence; and ii) detection message fusion, which enhances the 3D feature with the received detection messages.

\vspace{2pt}
\noindent \textbf{Detection confidence aware message packing.} The detection confidence aware (DCA) message packing targets to pack the complementary perceptual information into a compact message. The core idea is to explore the spatial heterogeneity of perceptual information. The intuition is that the areas that contain objects are more critical than background areas. During collaboration, areas with objects could help recover the miss-detected objects due to the limited view; and background areas could be omitted to save the precious bandwidth. To achieve this, we implement the spatial confidence map with the detection confidence map by
\begin{equation*}
    \overline{\mathbf{C}}_i=\Phi_{\rm cls}(\overline{\mathcal{B}}_i)\in\mathbb{R}^{X\times Y},
\end{equation*}
where $\Phi_{\rm cls}(\cdot)$ denotes the standard classification network in the single-agent decoder $\Phi_{\rm det}(\cdot)$, $\overline{\mathcal{B}}_i$ is the BEV feature generated with the collaboratively estimated depth. Based on this map, agents decide which spatial area to communicate; that is, each agent offers spatially sparse, yet critical features to support other agents. 

Specifically, a binary selection matrix is used to represent each location is selected or not. Let $\overline{\mathbf{M}}^f_{i\rightarrow j} \in \{0, 1\}^{X \times Y}$ be a selection matrix to determine the detection message sent from agent $i$ to $j$. Its $(x,y)$th element is
\begin{equation*}
\label{eq:selector}
\overline{\mathbf{M}}^f_{i\rightarrow j}(x,y)= I( \overline{\mathbf{C}}_i (x, y) > c_{\rm thre}),
\end{equation*}
where $I(\cdot)$ is an indicator function and $c_{\rm thre}$ is a predefined threshold. This reflects that a specific spatial area will be selected when its detection confidence is above a threshold.  Overall, the detection message sent from the $i$th agent to the $j$th agent is $\mathcal{P}^f_{i\rightarrow j}= \overline{\mathbf{M}}^f_{i\rightarrow j} \odot \overline{\mathcal{B}_i}$. Only selected features and their indices are packed, and they can be recovered at the receiver. This greatly reduces communication cost. 
% Agents exchange these compact detection messages with the partners.

\vspace{2pt}
\noindent \textbf{Detection message fusion.} Here we augment the detection feature of each agent by aggregating the received detection messages from other agents. We implement this with simple but effective non-parametric point-wise maximum fusion.
Specifically, for the $i$th agent, after receiving the $j$th agent's message $\mathcal{P}^f_{j\rightarrow i}$. We also include the ego feature map in fusion and denote $\mathcal{P}^f_{i\rightarrow i} = \overline{\mathcal{B}}_{i}$ to make the formulation simple and consistent, where $\mathcal{P}^f_{i\rightarrow i}$ might not be sparse. The fused BEV feature is obtained as
\begin{equation*}
\overline{\mathcal{B}}'_i=\underset{j\in\mathcal{N}_i\cup\{i\}}{\rm max}(\mathcal{P}^f_{j\rightarrow i})\in\mathbb{R}^{X\times Y \times C},
\end{equation*}
where ${\rm max}(\cdot)$ maximizes the corresponding features from multiple agents at each individual spatial location. Note that attention fusion is not permutation invariant, as attention weights vary with the ordering of key and query. Here we simply use the max operator to avoid this permutation variant issue. The fused BEV feature is output to the decoder to generate the final detection.
% \Note{sc: how to obtain the final detection after collaboration?}

% {\HC Note that we apply max fusion instead of attention fusion because with attention fusion, even with the same multi-view observation, the representation of the same region varies when choosing a different ego agent, which is not in line with the actual situation. ???} To avoid this issue, we apply the ego-invariant and more computation-efficient max fusion. 
\vspace{-2mm}
\subsection{Losses}
\vspace{-2mm}
To train the overall system, we supervise two tasks: categorical depth estimation and 3D object detection. The overall loss is
$
    L = L_{\rm dep} \left(\mathcal{D}_i,\mathcal{D}^0_i \right) + L_{\rm det} \left(\overline{\mathcal{O}}_i,\mathcal{O}^0_i \right),
$
where $\mathcal{D}^0_i$ and $\mathcal{O}^0_i$ is the $i$th agent's ground-truth depth category and objects, respectively, $L_{\rm dep}$ and $L_{\rm det}$ is the depth classification loss~\cite{CaDDN} and detection loss~\cite{zhou2019objects}, respectively.