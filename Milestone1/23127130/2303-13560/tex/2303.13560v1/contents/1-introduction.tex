\vspace{-3mm}
\section{Introduction}
\vspace{-1mm}

As a fundamental task of computer vision, 3D object detection aims to localize objects in the 3D physical space given an agent's real-time sensor inputs. It is crucial in a wide range of applications, including autonomous driving~\cite{lang2019pointpillars,zhou2022cross,Wu2020MotionNetJP}, surveillance systems~\cite{Yin2021Centerbased3O}, robotics~\cite{Li2022Dualview3O} and unmanned aerial vehicles~\cite{Hu2022AM3D}. Depending on sensor setups, there are multiple technical solutions to realize 3D object detection. 
In this spectrum, one extreme emphasizes raising the upper bound of detection performance, which uses high-end LiDAR sensor~\cite{zhou2018voxelnet,lang2019pointpillars,shi2020pv,deng2021voxel,zheng2021cia,mao2021pyramid,Chen20213DPC} to collect precise 3D measurements. However, this approach is too expensive to scale up. The other extreme solution emphasizes cost effectiveness, which tries to use thrifty sensor setups, e.g. only using cameras to detect 3D objects in real-time~\cite{wang2022detr3d,philion2020lift, zhang2022beverse,huang2021bevdet, wang2022dfm,roddick2018orthographic,carion2020end,zhou2022cross,huang2022bevdet4d,CaDDN}. 
However, camera-only 3D detection is significantly and consistently worse than LiDAR-based detection in most scenarios~\cite{Liu2022BEVFusionMM}. 


\begin{figure}
\vspace{-2mm}
    \centering
    \includegraphics[width=0.99\linewidth]{Figs/Intro.png}
  \vspace{-4mm}
  \caption{Collaborative camera-only 3D detection can disambiguate the single-view estimated depth, address the long-range and occlusion issues, and approach LiDAR in 3D detection.}
  \vspace{-6mm}
  \label{fig:intro}
\end{figure}

In this paper, we propose an orthogonal direction for improving camera-only 3D detection performances by introducing multi-agent collaborations. 
Hypothetically, empowered by advanced communication systems, multiple agents equipped only with cameras could share visual information with each other. 
% \weidi{would it be helpful to give some high-level intuition on why multi-agent can help, objects that are far away or invisible from one agent might be evident for another agent at other spatial location, ....}
This would bring three outstanding benefits. 
First, different viewpoints from multiple agents can largely resolve the depth ambiguity issue in camera-only 3D detection, bridging the gap with expensive LiDARs on depth estimation.
Second, multi-agent collaboration avoids inevitable limitations in single-agent 3D detection, such as occlusion and long-range issues, and potentially enables more holistic 3D detection; that is, detecting all the objects existed in the 3D scene, including those beyond visual range. Since LiDAR also suffers from limited field of view, this potentially enables collaborative cameras to outperform LiDAR. Third, the total expense of a large fleet of vehicles is significantly reduced as cameras are much cheaper than LiDAR.  However, multi-agent collaboration also brings new challenges. Different from many multi-view geometry problems, here we also have to concern communication bandwidth constraints. Thus, each agent needs to select the most informative cues to share.




Following this design rationale, 
we propose a novel collaborative camera-only 3D detection framework~\texttt{CoCa3D}. It includes three parts:
i) single-agent camera-only 3D detection, which achieves basic depth estimation and 3D detection for each agent; 
ii) collaborative depth estimation, which disambiguates the estimated depths by promoting spatial consistency across multiple agents’ viewpoints; and iii) collaborative detection feature learning, which complements detection features by sharing key detection messages with each other.
Compared to recent collaborative perception methods~\cite{li2021disconet,lei2022SyncNet} that are dealing with LiDAR,~\texttt{CoCa3D} specifically designs novel collaborative depth estimation to customize the task of camera-only 3D detection.  


To evaluate~\texttt{CoCa3D}, we conduct comprehensive experiments on one real-world dataset, DAIR-V2X~\cite{yu2022dairv2x}, and two new simulation datasets, OPV2V+ and CoPerception-UAVs+, which are extended based on original OPV2V~\cite{xu2022opv2v} and CoPerception-UAVs~\cite{hu2022where2comm} with more collaborative agents that cover three types of agents (cars, infrastructures and drones). 
Our results show that i) with 10 collaborative agents, \texttt{CoCa3D} enables camera-only detectors to overtake LiDAR-based detectors on OPV2V+; and ii) \texttt{CoCa3D} consistently outperforms previous works in the performance-bandwidth trade-off across multiple datasets by a large margin, improving the previous SOTA performances by 30.60\% on OPV2V+, 12.59\% on CoPerception-UAVs+, 44.21\% on DAIR-V2X for AP@70.
To sum up, our contributions are:

$\bullet$ We propose a novel collaborative camera-only 3D detection framework~\texttt{CoCa3D}, which improves the detection ability of cameras with multi-agent collaboration, promoting more holistic 3D detection.

$\bullet$ We propose core communication-efficient collaboration techniques, which explore the spatially sparse yet critical depth messages and tackle the depth ambiguity, occlusion, and long-range issues by fusing complementary information from different viewpoints, achieving more accurate and complete 3D representation.
% i) collaborative depth estimation, which improves depth estimation by leveraging multi-view consistency and reduces the communication cost by focusing on critical areas, ii) collaborative detection feature learning, which improves detection features by leveraging multi-view complementary and reduces the communication cost by focusing on critical areas.

$\bullet$ We expand two previous collaborative datasets with more agents, and conduct extensive experiments, validating that i)~\texttt{CoCa3D} significantly bridges the performance gap between camera and LiDAR on OPV2V+ and DAIR-V2X; and ii)~\texttt{CoCa3D} achieves the state-of-the-art performance-bandwidth trade-off.

% We extend the previous collaborative perception datasets (OPV2V and CoPerception-UAVs), with much more agents ($\geq$ 10) to thoroughly explore the benefits of collaboration.

% \clearpage


% With the fast development of communication, it is possible to allow multiple agents to share information. Meanwhile, it has one drawback. Sharing information would cost communication bandwidth, the essence is to exploit limited communication resources. Therefore, we need to carefully select what to collaborate. 

% Following this design rationale, we propose~\texttt{CoCa3D}, which includes two parts: a) single-agent camera-only 3D detection, which achieves basic depth estimation and detection ability, and b) multi-agent collaboration, which shares the most critical and unambiguous depth information estimated by individual agents to produce more accurate 3D representation and the most critical and confident detection features to achieve holistic 3D detection.

% Autonomous systems are leading a burgeoning new industrial revolution with a potential billions market. 3D sensing technology makes autonomous systems possible.
%Historically, 3D detection has relied heavily on LiDAR sensor~\cite{zhou2018voxelnet,lang2019pointpillars,shi2020pv,deng2021voxel,zheng2021cia,mao2021pyramid} for its excellent and encouraging performance.
%However, the expensive cost ($\$75,000$) makes LiDAR not scalable and prevents . 
%Recently, low-cost and easy-to-deploy cameras have a wide impact and ubiquity.
%Tesla is one of the most notable companies that has placed a big bet on cameras.
%Perceiving objects in 3D space is the foundation of automation and has attracted substantial scientific interest.
%LiDAR and camera~\cite{wang2022detr3d,philion2020lift, zhang2022beverse,huang2021bevdet, wang2022dfm,roddick2018orthographic,carion2020end,zhou2022cross,huang2022bevdet4d,CaDDN} sensors have a long history of use for 3D detection.
%LiDAR has shown excellent and encouraging performance in the large-scale 3D object detection benchmark~\cite{Caesar2020nuScenesAM} and significantly outperforms camera by 33.7\%~\cite{Liu2022BEVFusionMM} due to its ability to generate precise 3D measurements. 
%However, LiDAR could not scale due to the expensive cost and heavy weight. 
%Instead, low-cost and easy-to-deploy cameras have a wide impact and ubiquity. 
%Performance on camera-only 3D detection lags significantly relative to LiDAR due to the inherent ambiguity in depth when 3D scene is projected to the image plane. 

% To compact this effect, camera-only 3D detection methods~\cite{roddick2018orthographic,philion2020lift,CaDDN,huang2021bevdet,huang2022bevdet4d} attentively scatter 2D features along the camera ray considering all the depth candidates to generate high-quality 3D representation.
% However, localizing the correct depth candidate from a single agent's viewpoint is physically infeasible.
%Besides, the inevitable limitations in single-agent camera-only 3D detection, such as limited field of view, occlusion, and long-range issues, make it physically infeasible to accurately and holistically represent the 3D space.

%To disambiguate the depth and generate accurate and holistic 3D representation, we introduce multi-agent collaboration.
%Advances in collaborative perception~\cite{liu2020when2com,who2com,li2021disconet,zhou2022multi,ArnoldDT:22,lei2022SyncNet,Li2022mrsc,xu2022CoBEVT,li2022UQ4CP,wang2020v2vnet,V2XSim,xu2022opv2v,yu2022dairv2x} have shown the remarkable improvement achieved by enabling multiple agents to share the individual perceptual information with each other.
%However, due to the previously mentioned depth ambiguity issue, camera-only single agent has difficulties in individually producing accurate 3D representation. The low-quality single-agent 3D representation sharing may degrade the collaborative performance by introducing noisy and inaccurate information.
%Hence, we enable collaboration more thoroughly: i) enabling the sharing of depth information estimated by individual agents to produce more accurate 3D representation, The intuition is that for a correct depth candidate, its corresponding 3D location should be spatially consistent from multiple agents’ viewpoints, and ii) enabling the sharing of detection features of individual agents to achieve more holistic 3D detection. The intuition is that the invisible occluded or long-range blurred spatial regions may be visible or nearby for other agents.
%Meanwhile, we optimize the trade-off between detection performance and communication bandwidth and exchange the most critical information in collaboration.

%Following this design, we propose~\texttt{CoCa3D}, a novel communication-efficient collaborative camera-only 3D detection framework; see Fig.~\ref{fig:framework}. \texttt{CoCa3D} includes three parts: 
%i) single-agent camera-only 3D detection, which achieves 3D detection based on RGB images collected from an individual agent, 
%ii) collaborative depth estimation, which improves depth estimation through spatial consistency from multiple viewpoints of multi-agent based on the transmitted spatially sparse yet critical depth messages,
%and iii) collaborative detection feature learning, which improves detection feature through spatial complementary from multiple viewpoints of multi-agent based on the transmitted spatially sparse yet critical detection messages.
%The multi-agent collaborations work together to mitigate the depth ambiguity effect, break single-agent physical limitations, and achieve accurate and holistic 3D detection.

% This debate about which solution is more promising is not only business competition, but also significantly influences future directions for research.

% For example, many leading autonomous driving companies, such as Waymo, Cruise and Aurora, fuses data collected from multiple sensors, such as LiDAR, RADAR and camera, to achieve accurate 3D detection~\cite{zhou2018voxelnet,lang2019pointpillars,shi2020pv,deng2021voxel,zheng2021cia,mao2021pyramid}. Among those sensors, LiDAR is particularly important as its provides precise 3D measurements. However, this approach is terribly expensive. Solely a Velodyne 64-beam LiDAR (HDL-64E) costs about $\$75,000$, more than the prices of most vehicles. The other extreme solution emphasizes the fundamental feasibility, which tries to use thrifty sensor setups. For example, based on well-designed data pipeline and powerful learning models, Tesla vehicles only use cameras to detect 3D objects in real-time~\cite{wang2022detr3d,philion2020lift, zhang2022beverse,huang2021bevdet, wang2022dfm,roddick2018orthographic,carion2020end,zhou2022cross,huang2022bevdet4d,CaDDN}. However, a large amount of research experiments reflect that  camera-only 3D detection is significantly and consistently worse than LiDAR-based detection in most scenarios. This debate about which solution is more promising is not only business competition, but also significantly influences future directions for research.
% \weidi{is it too much on this paragraph, from my understanding, you only need to set up the motivation on why you do multi-agent collaboration, so all you need to say is, 
% existing research or ideas only consider to push the limit of 3D detection from the perspective of single agent, either by using LiDAR, however, it is extremely expensive, .... blabla, or algorithms to improve the camera-only 3D detection, however likely to suffer from depth ambiguity....}

% In particular, practical communication systems can hardly afford huge communication consumption in real-time. 
% \weidi{I won't say this, prefer something positive, for example,
% how to select the most informative cues from multiple agents, 
% that can be communicated with minimum bandwidth.....}
% Therefore, different from many multi-view problems, which do not take communication into account and can share a large amount of redundant information, we have to consider communication bandwidth and well select the most critical and beneficial information for sharing.
% \weidi{do you want to mention, our research is actually orthogonal to the improvement from single-agent 3D detection, and they are indeed our system baseline.}
% \weidi{the first two bullet points are kind of repetitive, what is our contribution in short ? correct me if I'm wrong,
% 1. we propose multi-agent collaboration for depth estimation, 
% by fusing complementary information from different viewpoints, bridging the gap with LiDAR;
% 2. multi-agent collaboration enables to tackle the occlusion and long-range issue, surpassing the traditional single-agent 3D detection based on camera or LiDAR prediction;
% 3. novel efficient feature fusion, that enables to extract complementary information from other agents, and guarantees communication with minimal bandwidth requirements, 
% 4. Experimentally, we validate the proposed idea on three different datasets, covering a wide span of ....., achieving state-of-the-art performance-bandwidth trade-off.....}