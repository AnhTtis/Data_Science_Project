\vspace{-3mm}
\section{Related Work}
\vspace{-1mm}

\noindent\textbf{Camera-only 3D object detection.}
Camera-only 3D detection aims to detect objects in the 3D space given the 2D image through explicit or implicit depth estimation~\cite{roddick2018orthographic, philion2020lift, zhang2022beverse,huang2021bevdet, wang2022dfm,Hu2022AM3D}. Recently, birdâ€™s-eye-view (BEV) representations are widely used for their computation efficiency and comparable performance compared to the 3D voxel features~\cite{lang2019pointpillars}. 
To get the BEV features, there are two types of methods. The depth-based methods~\cite{philion2020lift,huang2022bevdet4d, CaDDN} first estimate the depth distribution, and then attentively project the 2D image features along the projection ray to get the 3D voxel features and collapse it to BEV feature. The query-based methods~\cite{peng2022bevsegformer, li2022bevformer,zhou2022cross} first initialize queries for each BEV grid and then leverage transformer-architecture-based cross-attention to query image features with camera-aware positional embedding. Here, our single-agent camera-only detector follows the simple-yet-effective CaDDN~\cite{CaDDN}.


\noindent\textbf{LiDAR 3D object detection.}
LiDAR-based 3D detection achieves excellent performance due to the precise 3D measurements of the input data. Two well-known ways to encode LiDAR points include voxel-based~\cite{zhou2018voxelnet,lang2019pointpillars,zheng2021cia} and point-based~\cite{shi2020pv, shi2021pv,yang20203dssd}. Voxel-based methods divide the 3D space into regular voxels~\cite{zhou2018voxelnet} or pillars~\cite{lang2019pointpillars}, and encode the point inside into feature representations. Point-based methods are usually based on the PointNet~\cite{qi2017pointnet} series to aggregate the feature of points. Then the point features will be used for the proposal refinement. 
LiDAR-based 3D detection performs well, but high-quality LiDAR is hard to be adopted in a large scale due to the expensive cost. Here we propose an economic camera-only solution by introducing multi-agent collaboration, whose performance can catch up with LiDAR given a sufficient number of agents.
% Voxel-based representations are easier to parallelize, which motivates our use. Here, we project the 2D image features given the estimated depth into 3D voxels and take advantage of the voxel representation learning skills.


\noindent\textbf{Collaborative perception.}
Collaborative perception is an emerging application of multi-agent systems, which promotes perception performance by enabling agents to share information with other agents through communication. In this emerging research field, a surge of high-quality datasets, (V2X-SIM~\cite{Li_2021_RAL}, OPV2V~\cite{xu2022opv2v}, DAIR-V2X~\cite{yu2022dairv2x}, CoPerception-UAVs~\cite{hu2022where2comm}), and collaborative methods~\cite{who2com,liu2020when2com,wang2020v2vnet,xu2022opv2v,li2021disconet,Li_2021_RAL,xu2022v2xvit,yu2022dairv2x,xu2022CoBEVT,hu2022where2comm,lu2022robust,Li2022MultiRobotSC,Gao2020RegularizedGM} aimed for better performance-bandwidth trade-off have been proposed. 
Collaborative methods have three types: early, late and intermediate fusion. Compared with early fusion(i.e. transmitting raw data) and late fusion (i.e. transmitting outputs), intermediate fusion(i.e. transmitting and fusing features encoded by deep networks) achieves a better performance-bandwidth trade-off. DiscoNet~\cite{li2021disconet} adopts knowledge distillation to take the advantage of both early and intermediate collaboration. 
% V2X-ViT ~\cite{xu2022v2xvit} introduces a novel heterogeneous multi-agent attention module to fuse information across vehicles and infrastructures. 
Where2comm~\cite{hu2022where2comm} optimizes communication efficiency by sharing spatially sparse yet perceptually critical information. However, previous works mainly focus on LiDAR-based 3D object detection, while we investigate collaborative camera-only detection and specifically enhance depth estimation via multi-agent collaboration.


% LiDAR-based 3D object detection reaches high accuracy, but not all cars can be equipped with lidar due to its high price.
% It mainly has two types. The single-stage detectors~\cite{zhou2018voxelnet, lang2019pointpillars, zheng2021cia} have faster inference speed owing to the simple pipeline. The two-stage detectors~\cite{shi2020pv,deng2021voxel,mao2021pyramid} have higher accuracy owing to the repeated feature refinement. Both types tend to have voxel representation 

% VoxelNet~\cite{zhou2018voxelnet} divides point cloud into regular voxels, and transforms points within voxel into feature representation. PointPillars ~\cite{lang2019pointpillars} encodes point cloud into standing pillars and apply 2D convolution on BEV view feature map. CIA-SSD\cite{zheng2021cia} designs novel SSFA module and IoU-aware confidence rectification module to improve detection accuracy. For two-stage model, carefully designed features  will be used for box refinement. PV-RCNN~\cite{shi2020pv} utilizes voxel set abstraction module to encode the scene by keypoints, and refine proposals by RoI-grid pooling. Voxel-RCNN~\cite{deng2021voxel} proposes voxel RoI pooling for aggregating features within proposals. \Note{sc: relation to us?}