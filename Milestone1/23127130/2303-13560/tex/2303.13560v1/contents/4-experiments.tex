\begin{figure*}[!t]
  \centering
    \begin{subfigure}{0.32\linewidth}
    % \includegraphics[width=0.84\linewidth]{Figs/OPV2V/Ablation_AgentNumber.png}
    \includegraphics[width=0.85\linewidth]{Figs/AgentNumber/Ablation_AgentNumber_OPV2V.png}
    % \vspace{-2mm}
    \caption{OPV2V+}
    \label{fig:opv2v_agentnum}
  \end{subfigure}
  \begin{subfigure}{0.32\linewidth}
    % \includegraphics[width=0.85\linewidth]{Figs/DAIRV2X/Ablation_AgentNumber.png}
    \includegraphics[width=0.85\linewidth]{Figs/AgentNumber/Ablation_AgentNumber_DAIRV2X.png}
    % \vspace{-2mm}
    \caption{DAIR-V2X}
    \label{fig:dair_agentnum}
  \end{subfigure}
  \begin{subfigure}{0.32\linewidth}
    % \includegraphics[width=0.85\linewidth]{Figs/UAVs/Ablation_AgentNumber.png}
    \includegraphics[width=0.85\linewidth]{Figs/AgentNumber/Ablation_AgentNumber_UAV.png}
    % \vspace{-2mm}
    \caption{CoPerception-UAVs+}
    \label{fig:uav_agentnum}
  \end{subfigure}
  \vspace{-3mm}
  \caption{CoCa3D steadily improves 3D detection performance as the number of agents grows. In OPV2V+, around $10$ collaborative agents enable collaborative cameras to catch up with LiDAR.}
  \vspace{-4mm}
  \label{Fig:abl_agentnumber}
\end{figure*}

\vspace{-2mm}
\section{Experimental Results}
\vspace{-1mm}

\begin{table*}[!t]
\centering
% \setlength\tabcolsep{2pt}
\caption{\texttt{CoCa3D} significantly outperforms the previous SOTAs, e.g. improve by 30.60\% on OPV2V, 12.59\% on CoPerception-UAVs, 44.21\% on DAIR-V2X for metric AP@70. }
% $*$ denotes the method with limited bandwidth. Comm is short for communication cost.
\label{tab:SOTAs}
\vspace{-3mm}
\begin{tabular}{l|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{OPV2V+} & \multicolumn{3}{c|}{CoPerception-UAVs+} & \multicolumn{3}{c}{DAIR-V2X}\\
       & AP@30 & AP@50  & AP@70 & AP@50  & AP@70  & AP@80 & AP@30 & AP@50  & AP@70\\ \hline
% LiDAR &   No Collaboration    & 0.8014 &  0.7839 & 0.6512 &- &- &- &- &- &- \\ \hline
% \multirow{8}{*}{Camera} 
No Collaboration     & 0.2748 & 0.2041 & 0.0853 & 0.6956 & 0.4900 & 0.2309 & 0.0977 & 0.0524 & 0.0305 \\
Late Fusion          & 0.6501 & 0.6198 & 0.5109 & 0.7206 & 0.5372 & 0.2597 & 0.2060 & 0.1078 & 0.0455 \\
When2com~\color{blue}{(\scriptsize{CVPR'20})}             & 0.4853 & 0.4211 & 0.3737 & 0.8219 & 0.6705 & 0.4102 &0.1957 &0.0984 &0.0459 \\
V2VNet~\color{blue}{(\scriptsize{ECCV'20})}                & 0.6246 & 0.5042 & 0.3852 & 0.9093 & 0.7177 & 0.3804 &0.1640 &0.0847 &0.0512 \\
DiscoNet~\color{blue}{(\scriptsize{NeurIPS'21})}              & 0.7300 & 0.6009 & 0.4179 & 0.9054 & 0.7079 & 0.3564 &0.1836 &0.1262 &0.0683 \\
V2X-ViT~\color{blue}{(\scriptsize{ECCV'22})}               & 0.8346 & 0.6659 & 0.3946 & 0.9094 & 0.7143 & 0.3525 &0.1862 &0.1075 &0.0490 \\
% $\text{Where2comm}^*$           & 0.8006 & 0.6783 & 0.4453 & -    & 0.8725 & 0.6091 & 0.2716 & 23.13\\
Where2comm~\color{blue}{(\scriptsize{NeurIPS'22})}            & 0.8191 & 0.7089 & 0.4741  & 0.9102 & 0.7383 & 0.3676 & 0.1754 & 0.1025 & 0.0547 \\
% Ours                 & - & - & - & -    & 0.8896 & 0.6149 & 0.2892 & 22.98 \\
% $\text{CoCa3D}^*$                & 0.8496 & 0.8081 & 0.6527  & 0.9342 & 0.7551 & 0.4375 & - & - & -  \\
CoCa3D                 & \textbf{0.8642} &  \textbf{0.8260} & \textbf{0.6675}   & \textbf{0.9497} & \textbf{0.8502} & \textbf{0.5835} & \textbf{0.3522} & \textbf{0.2260} & \textbf{0.0985} \\ 
\hline
\end{tabular}
\vspace{-6mm}
\end{table*}


\subsection{Datasets and experimental settings}
\vspace{-1mm}
We cover three datasets, both real-world and simulation scenarios, and multiple agent types. Metric Average Precision (AP) at Intersection-over-Union (IoU) threshold of $0.30$, $0.50$, $0.70$, and $0.80$ are used. Metric multi-class classification accuracy is used for depth accuracy.
The communication volume follows the standard setting as ~\cite{xu2022opv2v,xu2022v2xvit,hu2022where2comm} that counts the message size by byte in log scale with base $2$.
To compare communication results straightforwardly and fair, we do not consider any extra compression.

\noindent\textbf{OPV2V+.}
The original OPV2V~\cite{xu2022opv2v} is a large-scale vehicle-to-vehicle collaborative perception dataset, co-simulated by OpenCDA~\cite{xu2021opencda} and CARLA~\cite{dosovitskiy2017carla}. Here we introduce an extended version, OPV2V+, which  includes more collaborative agents ($\approx 10$). Each agent is equipt with 4 cameras and 4 depth sensors with resolution 600 $\times$ 800. The detection range is 281.6m $\times$ 80m. Our single-agent camera-only detector follows CaDDN~\cite{CaDDN} with 50 depth categories in linear-increasing spacing mode.

\noindent
\textbf{DAIR-V2X.} DAIR-V2X~\cite{yu2022dairv2x} is the only public \textbf{real-world} collaborative perception dataset. It contains two agents: a vehicle and a road-side-unit with image resolution 1080 $\times$ 1920. The perception range is 100m$\times$79m. Our single-agent camera-only detector also follows CaDDN~\cite{CaDDN}.

\noindent\textbf{CoPerception-UAVs+.}
The original CoPerception-UAVs~\cite{hu2022where2comm} is a large-scale UAV-based collaborative perception dataset, co-simulated by AirSim ~\cite{Airsim} and CARLA~\cite{dosovitskiy2017carla}. Here we introduce an extended version, CoPerception-UAVs+, which includes more collaborative agents ($\approx 10$). Each agent equips with 1 camera and 1 depth sensor with resolution 450 $\times$ 800. The detection range is 192m $\times$ 96m. Our single-agent camera-only detector follows DVDET~\cite{DVDET} with 10 depth categories in uniform spacing mode.


% We represent the field of view into a BEV map with size $(200, 504, 64)$ and the resolution is $0.4$m/pixel in length and width. 
\vspace{-2mm}
\subsection{Quantitative evaluation}
\vspace{-1mm}

\noindent\textbf{Collaborative camera-only 3D detection overtakes LiDAR.} 
Fig.~\ref{Fig:abl_agentnumber} investigates how the collaborative camera-only 3D detection performance changes with the number of collaborative agents on multiple datasets. To set up a goal, we consider a LiDAR-based detector implemented by the widely used PointPillar~\cite{lang2019pointpillars}.
Fig.~\ref{Fig:abl_agentnumber} (a) shows that: i) with $10$ collaborative agents, CoCa3D outperforms the LiDAR 3D detection at both AP@0.5/0.7 on OPV2V+! ii) the detection performance positively increases as the number of collaborative agents. And we then use the slope of OPV2V+ to fit the function of collaborative detection performance with agent number on DAIR-V2X as it only has two agents available. Fig.~\ref{Fig:abl_agentnumber} (b) shows that camera-only 3D detection is expected to outperform LiDAR 3D detection with $7$ collaborative vehicles in the real scenario.
As LiDAR is not a commonly available equipment for drones, we set a high-quality RGB-D camera instead.  
Fig.~\ref{Fig:abl_agentnumber} (c) shows that with $3$ collaborative drones, camera-only 3D detection outperforms ground-truth depth-based 3D detection. Furthermore, the steadily increasing detection performance with the number of collaborative agents encourages the agents to actively collaborate and achieve consistent improvement.  


\noindent\textbf{Benchmark comparison.} Tab.~\ref{tab:SOTAs} compares the proposed \texttt{CoCa3D} with previous collaborative methods. We consider single-agent detection without collaboration (No collaboration), When2com~\cite{liu2020when2com}, V2VNet~\cite{wang2020v2vnet}, DiscoNet~\cite{li2021disconet}, V2X-ViT~\cite{xu2022v2xvit}, Where2comm~\cite{hu2022where2comm} and late fusion, where agents directly exchange the detected 3D boxes. We see that the proposed \texttt{CoCa3D} significantly outperforms previous state-of-the-arts, improves the SOTA performance by 30.60\% on OPV2V+, 12.59\% on CoPerception-UAVs+, 44.21\% on DAIR-V2X for AP@70. The reason is that collaborative depth estimation promotes more accurate 3D features per single agent, and the enhanced single agent features further facilitate collaborative detection feature learning, while previous collaborative methods do not specifically consider depth ambiguity and collaborate over single-agent features.



\begin{table}[!t]
\centering
\setlength\tabcolsep{2pt}
\caption{\texttt{CoCa3D} outperforms single-agent camera-only 3D detection with GT depth on OPV2V+ and CoPerception-UAVs+.}
\label{tab:OPV2V_upperbound}
\vspace{-2mm}
\begin{small}
\begin{tabular}{cc|ccc|ccc}
\hline
Co & Co & \multicolumn{3}{c|}{OPV2V+} & \multicolumn{3}{c}{CoPerception-UAVs+} \\
-Depth  & -FL& AP@30 & AP@50  & AP@70 & AP@50  & AP@70  & AP@80 \\\hline
% LiDAR        & - & - & 0.8014 &  0.7839 & 0.6512 & - & -& - \\ \hline
- & - & 0.2748 & 0.2041 & 0.0853 & 0.7213 & 0.5421 & 0.2846\\ 
 GT & - & 0.3454 & 0.2553 & 0.0973 & 0.8347 & 0.6764 & 0.4120\\
- & \checkmark & 0.8201 &  0.7191 & 0.4756 & 0.9084 & 0.7256 & 0.4028\\  % 0.82/0.71/0.47
GT & \checkmark & \textbf{0.9120} & \textbf{0.8805} & \textbf{0.7434} & \textbf{0.9505} & 0.8398 & 0.5504\\
\checkmark & \checkmark & 0.8642 & 0.8260 & 0.6675 & 0.9495 & \textbf{0.8518} & \textbf{0.5849} \\
\hline
\end{tabular}
\end{small}
\vspace{-3mm}
\end{table}



\noindent\textbf{Multi-agent collaboration evaluation.}
Tab.~\ref{tab:OPV2V_upperbound} assess the effectiveness of the proposed multi-agent collaboration on OPV2V+ and CoPerception-UAVs+ datasets. Ground-truth (GT) depth is included to provide the upper bound of collaborative camera-only 3D detection. We see that: i) both collaborations can consistently improve the performance; ii) camera-only 3D detection with collaboratively estimated depth significantly outperforms that with single-agent estimated depth and even approaches the upper bound with ground truth depth.

% \begin{table}[!ht]
% \centering
% \setlength\tabcolsep{2pt}
% \caption{\texttt{CoCa3D} outperforms single-agent camera-only 3D detection with ground-truth depth on CoPerception-UAVs.}
% \label{tab:UAV_upperbound}
% \vspace{-2mm}
% \begin{tabular}{cccccc}
% \hline
% Co-FL & Co-Depth & AP@50 & AP@70 & AP@80 & Acc \\ \hline
% % & - & N-Est & 0.6956 & 0.4900 & 0.2309 \\ 
% - & - & 0.7213 & 0.5421 & 0.2846 & 0.7586\\ 
% - & GT & 0.8347 & 0.6764 & 0.4120 & -\\
% % & \checkmark & N-Est & 0.8734 & 0.6686 & 0.3375 \\
% \checkmark & - & 0.9084 & 0.7256 & 0.4028 & 0.7586\\ 
% \checkmark & GT & \textbf{0.9505} & 0.8398 & 0.5504 &-\\
% \checkmark & \checkmark & 0.9495 & \textbf{0.8518} & \textbf{0.5849} & 0.8133\\\hline
% \end{tabular}
% \vspace{-3mm}
% \end{table}


\subsection{Qualitative evaluation}

\noindent\textbf{Visualization of depth and uncertainty.}
Fig.~\ref{fig:depth_u} shows that collaborative depth estimation outperforms single-agent depth estimation and approaches the ground truth depth. We see that: i) the single-agent depth estimation can estimate the relative depth while not able to precisely localize the depth candidate, for example, the vehicles are higher than the plane it sits while this plane is not grounded to the right category; ii) by introducing multi-view geometry, the collaboratively estimated depth can smoothly and accurately ground the plane; iii) the depth uncertainty is larger for the long-range and background areas. The reason is that the distant regions are hard to localize as they occupy too few image pixels, and the background regions are hard to localize due to the texture-less surfaces.

\noindent\textbf{Visualization of detection results.}
Fig.~\ref{fig:detections} shows that compared to single LiDAR 3D detection, cameras with collaborations (Co-Depth and Co-FL) are able to achieve holistic and accurate detection results. The reason is that the single agent has some fundamental physical limitations such as long-range and occlusion issues. Fig.~\ref{fig:detections}(a-c) shows that augmenting a single agent sensor going from just a camera to including a depth sensor to LiDAR can improve the detection but still not able to achieve holistic detection. Fig.~\ref{fig:detections}(d)/(b) and (a)/(e) compares the detection results with and without collaborative feature learning, we see that collaboration can help detect lots of the missed objects in the single-agent detection. Fig.~\ref{fig:detections}(d,e,f) show that: i) both collaborations, Co-FL and Co-Depth, consistently improve the detection performance; ii) with collaboration, collaborative camera-only agents can outperform LiDAR in 3D detection.



\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=0.99\linewidth]{Figs/OPV2V/Ablation_Uncertainty_OPV2V.png}
    \vspace{-4mm}
    \caption{OPV2V+}
    \label{fig:opv2v_graph}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=0.99\linewidth]{Figs/UAVs/Ablation_Uncertainty_UAV.png}
    \vspace{-4mm}
    \caption{CoPerception-UAVs+}
    \label{fig:uav_graph}
  \end{subfigure}
  \vspace{-3mm}
  \caption{\texttt{CoCa3D} achieves superior detection performance and communication cost trade-off over various bandwidths.}
  \label{Fig:abl_tradeoff}
  \vspace{-3mm}
\end{figure}

\begin{table}[!t]
\setlength\tabcolsep{2pt}
\centering
\caption{Co-Depth significantly improves depth accuracy.}
\vspace{-2mm}
\label{tab:abl_depthacc}
\begin{small}
\begin{tabular}{ccc|ccc}
\hline
\multicolumn{3}{c|}{Full plane} & \multicolumn{3}{c}{Foreground} \\
Single & Collaboration & Gain & Single & Collaboration & Gain\\  \hline
0.6167 & 0.7781 & 26.17\% $\uparrow$ & 0.7586 &0.8133 & 7.21\% $\uparrow$ \\ \hline
\end{tabular}
\end{small}
\vspace{-2mm}
\end{table}


\begin{table}[!t]
\setlength\tabcolsep{2pt}
\centering
\caption{Ablation of collaborative depth estimation on CoPerception-UAVs+. Uniform/Linear denotes uniform/linear-increasing spacing. Dense and sparse supervision denote applying depth supervision over the full image plane and the object regions.}
\vspace{-2mm}
\label{tab:abl_codepth}
\begin{tabular}{cccccc}
\hline
Spacing & Supervision & AP@50  & AP@70 & AP@80 \\ \hline
Uniform & Dense & \textbf{0.9502} & 0.8444 & 0.5746 \\
Uniform & Sparse & 0.9330 & 0.8042 & 0.4829  \\
Linear & Dense  & 0.9484 &  0.8365 & 0.5580 \\
Linear & Sparse  & 0.9495 & \textbf{0.8518} & \textbf{0.5849} \\ \hline
\end{tabular}
\vspace{-3mm}
\end{table}

\vspace{-1mm}
\subsection{Ablation studies}
\vspace{-1mm}



\begin{figure*}[!t]
    \centering
    \vspace{2mm}
    % origin
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/RV.png}
        \caption{Image (RV)}
        \label{fig:uav_img}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/RV_depth_est.png}
        \caption{Single (RV)}
        \label{fig:uav_est}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/RV_depth_colla_est.png}
        \caption{Collaboration (RV)}
        \label{fig:uav_colla_est}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/RV_depth_gt.png}
        \caption{Ground-truth (RV)}
        \label{fig:uav_gt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/RV_uncertainty.png}
        \caption{Uncertainty (RV)}
        \label{fig:uav_uncertainty}
    \end{subfigure}
    
    \centering
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/BEV.png}
        \caption{Image (BEV)}
        \label{fig:bev_img}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/BEV_depth_est.png}
        \caption{Single (BEV)}
        \label{fig:bev_est}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/BEV_depth_colla_est.png}
        \caption{Collaboration (BEV)}
        \label{fig:bev_colla_est}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/BEV_depth_gt.png}
        \caption{Ground-truth (BEV)}
        \label{fig:bev_gt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.19\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/UAVs/DepthVis/BEV_uncertainty.png}
        \caption{Uncertainty (BEV)}
        \label{fig:bev_uncertainty}
    \end{subfigure}
\vspace{-2mm}
\caption{Visualization of the depth and uncertainty in image range-view (RV) and bird's eye view (BEV) on CoPerception-UAVs+. Collaboratively estimated depth improves the single-agent estimated depth and approaches ground-truth depth.}
\label{fig:depth_u}
\vspace{-3mm}
\end{figure*}



\begin{figure*}[!ht]
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/OPV2V/Ablation_00020/Single_Est.jpg}
        \caption{Camera}
        \label{fig:det_single_est}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/OPV2V/Ablation_00020/Single_GT.jpg}
        \caption{RGB-D}
        \label{fig:det_single_GT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/OPV2V/Ablation_00020/LiDAR.jpg}
        \caption{LiDAR}
        \label{fig:det_LiDAR}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/OPV2V/Ablation_00020/Colla_Est.jpg}
        \caption{Camera with Co-FL}
        \label{fig:det_multi_est}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/OPV2V/Ablation_00020/Colla_GT.jpg}
        \caption{RGB-D with Co-FL}
        \label{fig:det_multi_gt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/OPV2V/Ablation_00020/Colla_Depth.jpg}
        \caption{Camera with Co-Depth and Co-FL (\texttt{CoCa3D})}
        \label{fig:det_multi_coest}
    \end{subfigure}
\vspace{-2mm}
\caption{\texttt{CoCa3D} outperforms LiDAR detection on OPV2V+ with 10 agents. \textcolor{green}{Green} and \textcolor{red}{red} boxes denote GT and detection respectively.}
\label{fig:detections}
\vspace{-6mm}
\end{figure*}




\noindent\textbf{Trade-off between detection performance and communication cost.}
Fig.~\ref{Fig:abl_tradeoff} compares the proposed \texttt{CoCa3D} with the previous communication-efficient solution Where2comm~\cite{hu2022where2comm} in terms of the trade-off between detection performance (AP@IoU=50/70) and communication bandwidth. We see that: i) \texttt{CoCa3D} can adapt to varying bandwidths by adjusting the depth uncertainty threshold and the detection confidence threshold; ii) \texttt{CoCa3D} achieves superior detection performance and communication cost trade-off to Where2comm over varying communication bandwidth. The gain mainly comes from Co-Depth's improvement in depth estimation, which improves single-agent BEV features. With enhanced single BEV features, the Co-FL can generate better-augmented BEV features, resulting in improved detection performance.



\noindent\textbf{Effect of components in collaborative depth estimation.} Tab.~\ref{tab:abl_depthacc} assesses the gain of collaborative depth estimation (Co-Depth) over single-agent depth estimation (S-Depth) in the depth accuracy metric. We see that: i) Co-Depth steadily improves S-Depth over the full plane; ii) the depth accuracy of the foreground objects is higher than the full plane, as the texture-less backgrounds have fewer cues to localize. Tab.~\ref{tab:abl_codepth} assesses the effects of the depth spacing and supervision choices. We see that: i) overall, the Co-Depth is robust to the different spacing and supervision choices on AP@50/70; ii) linear-increasing spacing is robust to the various supervision choices that uniform spacing which fails the stricter metric AP@80 without dense supervision. The reason is that the linearly increasing spacing takes into account the prior depth distribution and assigns more depth candidates to frequently occurring depth ranges, which contributes to more balanced and easier depth learning.




% \begin{table*}[]
% \caption{Ablation of depth estimation on CoPerception-UAVs.}
% \begin{tabular}{lllllll}
% \hline
% Agents        & Depth & Supervision & Depth Range   & AP@50  & AP@75  & Comm\\ \hline
% Single        & Flat  & -   & - & 0.6956 & 0.3728 & 0\\
% Single        & Est   & Object & 10   & 0.7213 & 0.4275 & 0\\
% Single        & GT    & -    & 10 & 0.8347 & 0.5742 & 0\\ \hline
% Collaboration & Flat  & -    & 10 & 0.8734 & 0.5364 & -\\
% Collaboration & Est   & Object  & -  & 0.9084 & 0.5890 & -\\
% Collaboration & \textbf{Colla-Est} & Object & 10 & 0.9495 & 0.7526 & -\\
% Collaboration & \textbf{Colla-Est} & Object + Single Feat & 10 & 0.9317 & 0.6987 & -\\
% Collaboration & \textbf{Colla-Est} & Plane & 10 & 0.9393 & 0.6784 & -\\
% Collaboration & \textbf{Colla-Est} & Plane & 20 & 0.9502 & 0.7449 & -\\
% Collaboration & \textbf{Colla-Est} & Plane & 20 & - & - & -\\
% Collaboration & \textbf{Colla-Est} & Plane + Single Feat & 10 & 0.9350 & 0.6784 & -\\\hline
% Collaboration & GT   & -    & 10 & 0.9505 & 0.7336 & -\\ 
% Collaboration & GT   & -    & 20 & - & - & -\\ \hline
% \end{tabular}
% \end{table*}




% \begin{table}[]
% \setlength\tabcolsep{2pt}
% \centering
% \caption{Ablation of depth supervision on OPV2V.}
% \begin{tabular}{cccccccc}
% \hline
% Colla & Fg Weight & Occ Sup & AP@30  & AP@50 & AP@70 & RMSE\\ \hline
% - & - & - & - &  - & - & - \\
% - & \checkmark & - & - &  - & - & - \\ 
% - & \checkmark & \checkmark & - &  - & - & - \\ 
% \checkmark & - & - & - &  - & - & - \\
% \checkmark & \checkmark & - & - &  - & - & - \\ 
% \checkmark & \checkmark & \checkmark & - &  - & - & - \\ \hline
% \end{tabular}
% \end{table}






