\subsection{Preliminaries and notation}
We consider the typical case of a 2D-convolution layer with padding and a square input tensor $X$ of dimensions of $H_x\times H_x\times C_x$ with $H_x$ the spatial width and $C_x$ the number of channels. The convolution layer produces an output tensor $Y$ of dimensions $H_y\times H_y\times C_y$ with $H_y$ the spatial width (equal to $H_x$) and $C_y$ the number of channels. The convolution is performed thanks to convolutional kernels represented by a weight tensor $W$ of size $H_k\times H_k\times C_x\times C_y$ with $H_k$ the spatial dimension of a kernel (assumed to be square), $C_x$ the number of input channels and $C_y$ the number of output channels (i.e. the number of filters) as defined previously. The output for standard convolution is as follows:
\begin{equation}
Y_{k,l,n}=\sum_{m=1}^{C_x}\sum_{i=1}^{H_k}\sum_{j=1}^{H_k}W_{i,j,m,n}\cdot X_{k+i-1,l+j-1,m}\quad\forall k,l \in [1,H_y],\quad\forall n \in [1,C_y]
\end{equation}

On modern CNN architectures, convolution layers are often coupled with batch-normalization layers that normalize (recentering and rescaling) the inputs of layers to make training faster and improve stability.

\subsection{Convolution primitives}

We detail the different convolution primitives evaluated in this work. Table~\ref{Primtive_resume} sums up performance features compared to the standard convolution.

\input{table/table}
\subsubsection{Grouped convolution}

 was first introduced in the AlexNet paper from Krizhevsky \etal~\cite{krizhevsky2012imagenet} for practical issues, then several works such as Ioannou~\etal~\cite{ioannou2017deep} have studied its effect on the performance of a neural network model. For the standard convolution, all input channels are used to compute an output channel. For a grouped convolution with G groups, each channel of the input and output are associated with a group $G_i$. Then, to compute an output channel of the group $G_i$, only the corresponding input channels are processed, as depicted in Fig.~\ref{groupedconvolution}. Thus, grouped convolutions (also referred as \textit{filter groups}) reduce the number of parameters and MAC operations of the layer by a factor G.

\begin{figure}
\centering
\includegraphics[width=0.85\textwidth]{figure/groupedconvolution.eps}
\caption{From \cite{ioannou2017deep}, standard vs. grouped convolutions: the grouped convolution with 2 groups applies half of the filters to each half of the input channels in order to compute each half of the output channels.} \label{groupedconvolution}
\end{figure}
\vspace{-10pt}
\subsubsection{Depthwise separable convolution}
Szegedy~\etal~\cite{szegedy2015going} introduce depthwise separable convolutions with the \textit{Inception} architecture. Depthwise separable convolution replaces the standard convolution by two convolutions: \textit{depthwise} and \textit{pointwise}.
Depthwise convolution is an extreme version of grouped convolution where $G = C_x = C_y$. The problem is that each filter only handles information passed down from one input channel. Pointwise convolution is applied to linearly combine the output channels of the depthwise convolution thanks to $1\times1$ kernels. It also acts as a reduction of the depth of the output tensor $Y$.
\vspace{-10pt}
\subsubsection{Shift convolution}

Even though pointwise convolution is more computationally expensive than depthwise convolution in theory, Jeon~\etal~\cite{jeon2018constructing} notice, with a hardware implementation, that depthwise convolution is more time-consuming than point convolution. They replace depthwise convolution by a shift operation which requires extremely few parameters and less computational power to produce the intermediate feature map $I$:

\begin{equation}
I_{k,l,m}=X_{k+\alpha_m,l+\beta_m,m} \text{  } \\\forall k,l \in [1,H_x],\quad\forall m \in [1,C_x]
\end{equation}
where $\alpha_m$ and $\beta_m$ denote the horizontal and vertical shift assigned to the $m^{th}$ channel of the input feature map.\\
\vspace{-10pt}
\subsubsection{Add convolution}

Multiplication operation consumes, in most cases, more energy than addition operation. Chen~\etal~\cite{chen2020addernet} exploit the fact that convolutions in deep neural networks are cross-correlation measuring the similarity between input and convolution kernel. They propose to replace cross-correlation by $L1$-norm as a similarity measure to perform an \textit{add convolution} as in Eq.\ref{add convolution}.
\begin{equation}\label{add convolution}
Y_{k,l,n}=-\sum_{m=1}^{C_x}\sum_{i=1}^{H_k}\sum_{j=1}^{H_k}\vert W_{i,j,m,n} - X_{k+i-1,l+j-1,m}\vert \quad\forall k,l \in [1,H_y],\quad\forall n \in [1,C_y]
\end{equation}
The output of an add convolution is always negative. Thus, in order to make add convolution compatible with standard activation functions like ReLu, a batch normalization layer following the add convolution layer is needed.

\subsection{Neural network library for Cortex-M MCU}

The challenge of porting neural networks to constrained platforms such as microcontrollers has led to the creation of embedding tools (e.g. TFLM\footnote{\url{https://www.tensorflow.org/lite/microcontrollers}}, N2D2\footnote{\url{https://github.com/CEA-LIST/N2D2}}, STM32Cube MX-AI\footnote{\url{https://www.st.com/en/embedded-software/x-cube-ai.html}} or NNoM\footnote{\url{https://github.com/majianjia/nnom}}). Those tools support standard convolution as well as depthwise separable convolutions layers. TFLM and STM32Cube MX-AI support floating point operations, 16 and 8 bits integer operations while NNoM supports only 8 bits integer operations. Furthermore, for Cortex-M4 and Cortex-M7 MCUs (with Digital Signal Processing extensions), SIMD instructions can be used for the computation of different primitives by integrating the middleware CMSIS-NN~\cite{lai2018cmsis} to those tools. For our study, the open source NNoM library was chosen due to its good performance and its ease of customization.