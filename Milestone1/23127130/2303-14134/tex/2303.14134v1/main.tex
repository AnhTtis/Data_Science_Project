\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib,final]{neurips_2020_ml4ps}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage{mwe}

% \title{The Problem with Mass: Anomaly Detection in Jet Physics}
% \title{Dissecting the Mass Problem: Anomaly Detection in Jet Physics}
\title{The Mass-ive Issue: Anomaly Detection in Jet Physics}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
    Tobias Golling, Takuya Nobe, Dimitrios Proios, John Andrew Raine\\ \textbf{Debajyoti Sengupta, Slava Voloshynovskiy}\\
    Universit\'e de Gen\`eve\\
    \texttt{tobias.golling@unige.ch, tnobe@cern.ch, dimitrios.proios@etu.unige.ch}\\ \texttt{jraine@cern.ch, desengup@cern.ch, svolos@unige.ch}
    % \And
    % John Andrew Raine, Debajyoti Sengupta\\
    % Universit\'e de Gen\`eve\\
    % \texttt{\{jraine,desengup\}@cern.ch}
    \AND
    Jean-Francois Arguin, Julien Leissner Martin, Jacinthe Pilette\\
    Universit\'e de Montreal \\
    \texttt{arguin@lps.montreal.ca, julien.leissner-martin@umontreal.ca}\\ \texttt{jacinthe.pilette@umontreal.ca}
    % \And
    % Julien Leissner Martin, Jacinthe Pilette\\
    % Universit\'e de Montreal \\
    % \texttt{julienleissnermartin@gmail.com,jacinthe.pillete@umontreal.ca}
    \AND
    Debottam Bakshi Gupta, Amir Farbin\\
    University of Texas, Arlington\\
    \texttt{dbakshig@cern.ch, afarbin@cern.ch}
}


% \author{%
%     Tobias Golling, Slava Voloshynovskiy, Takuya Nobe, John Andrew Raine\\
%     Universit\'e de Gen\`eve\\
%     \texttt{\{tobias.golling,svolos\}@unige.ch, \{tnobe,jraine\}@cern.ch}
%     \AND
%     Debajyoti Sengupta, Dimitrios Proios\\
%     Universit\'e de Gen\`eve\\
%     \texttt{\{debajyoti.sengupta,dimitrios.proios\}@etu.unige.ch}
%     \AND
%     Jean-Francois Arguin\\
%     Universit\'e de Montreal \\
%     \texttt{arguin@lps.montreal.ca}
%     \AND
%     Julien Leissner Martin, Jacinthe Pilette\\
%     Universit\'e de Montreal \\
%     \texttt{julienleissnermartin@gmail.com,jacinthe.pillete@umontreal.ca}
%     \AND
%     Amir Farbin, Debottam Bakshi Gupta\\
%     University of Texas, Arlington\\
%     \texttt{\{afarbin,debottam.bakshi.gupta\}@cern.ch}
% }

% \author{%
%   Tobias Golling\\
%   Universit\'e de Gen\`eve\\
%   \texttt{tobias.golling@unige.ch} \\
%   % examples of more authors
%   \And
%   Slava Voloshynovskiy \\
%   Universit\'e de Gen\`eve \\
%   % Address \\
%   \texttt{svolos@unige.ch} \\
%   \And
%   John Andrew Raine \\
%   Universit\'e de Gen\`eve \\
%   % Address \\
%   \texttt{johnny.raine@cern.ch} \\
%   \And
%   Takuya Nobe \\
%   Universit\'e de Gen\`eve \\
%   % Address \\
%   \texttt{takyua.nobe@cern.ch} \\
%   \And
%   Debajyoti Sengupta \\
%   Universit\'e de Gen\`eve \\
%   % Address \\
%   \texttt{debajyoti.sengupta@etu.unige.ch} \\
%   \And
%   Dimitrios Proios \\
%   Universit\'e de Gen\`eve \\
%   % Address \\
%   \texttt{dimitrios.proios@etu.unige.ch} \\
%   \And
%   Jean-Francois Arguin \\
%   Universit\'e de Montreal \\
%   % Address \\
%   \texttt{arguin@lps.montreal.ca} \\
%   \And
%   Jacinthe Pilette \\
%   Universit\'e de Montreal \\
%   % Address \\
%   \texttt{jacinthe.pilette@umontreal.ca} \\
%   \And
%   Julien Leissner Martin \\
%   Universit\'e de Montreal \\
%   % Address \\
%   % \texttt{email} \\
%   \And
%   Amir Farbin \\
%   University of Texas, Arlington \\
%   % Address \\
%   \texttt{afarbin@cern.ch} \\
%   \And
%   Debottam Bakshi Gupta \\
%   University of Texas, Arlington \\
%   % Address \\
%   \texttt{debottam.bakshi.gupta@cern.ch} \\
% }

\begin{document}

\maketitle

%%%%Introduce that Mass dependence is known but we are trying to focus and formalise the problem to provide a context for discussion and development

\begin{abstract}
    % Abstract about jets and anomaly detection and mass.
    In the hunt for new and unobserved phenomena in particle physics, attention has turned in recent years to using advanced machine learning techniques for model independent searches. In this paper we highlight the main challenge of applying anomaly detection to jet physics, where preserving an unbiased estimator of the jet mass remains a critical piece of any model independent search. Using Variational Autoencoders and multiple industry-standard anomaly detection metrics, we demonstrate the unavoidable nature of this problem.
\end{abstract}

\section{Introduction}

An area of extreme importance in particle physics is the hunt for new physics, observing phenomena outside of the predictions of the Standard Model (SM) of particle physics. The SM has been the established theory of fundamental particles and their interactions for over forty years, however, in order to completely describe nature we know that there are areas in which it is not a complete model. Attention therefore turns to the hunt for new processes that could explain these discrepancies and manifest in the existence of new fundamental particles.

%As these particles would likely only be produced at high energies,
The Large Hadron Collider (LHC) provides a perfect window to search for signatures left behind by these particles, that are expected to be produced in collisions. A standard signature that can be expected to be observed in the general purpose detectors at the LHC is the presence of jets, a collimated shower of energy deposited by particles in the detector, initiated by a single particle and its decay processes and subsequent shower.
% The average signature of these jets is shown in Fig.\ref{fig:jetimg} for two known processes, and two hypothetical particles. It should be noted that in contrast to the average images, an individual jet is stochastic and sparse, which presents additional challenges.
% A key property in observing new physics is the mass of the jet, which is strongly tied to the mass of the initiating particle.


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/AvgJetImage_input.pdf}
%     % \includegraphics[width=0.22\textwidth]{example-image-1x1}
%     % \includegraphics[width=0.22\textwidth]{example-image-1x1}
%     % \includegraphics[width=0.22\textwidth]{example-image-1x1}
%     % \includegraphics[width=0.22\textwidth]{example-image-1x1}

%     \caption{Average distribution of momentum within a jet in the $\eta-\phi$ plane for the four particles considered in this paper after applying preprocessing. From left to right: QCD, Top quarks, heavy Stealth bosons (400 GeV) and light Stealth bosons (80 GeV). The intensity corresponds to $p_{\mathrm{T}}$ fraction of the jet. Because only the leading 20 constituents are considered, $\sum p_{\mathrm{T}}^{\mathrm{frac}} < 1.0$ .}
%     % \caption{Input Jets. QCD, Top, HighM SB, LowM SB}
%     \label{fig:jetimg}
% \end{figure}


The major background in model independent searches comes from jets initiated by quarks and gluons (QCD jets), fundamental particles with negligible mass. The observed mass of QCD jets is a falling spectrum across all energies, in comparison to a peak centred around the particle mass observed for jets initiated by massive particles. This background is incredibly dominant, and being able to identify a potential signal from the mass alone is almost impossible.
Therefore, we employ anomaly detection techniques to identify jets which are unlike QCD jets, enhancing the presence of these signals and making their mass peaks visible, without making strong assumptions on the underlying physics that would need to be made in a supervised setting.

% However, it is critical that in doing so, artificial peaks are not introduced through sculpting of the mass distribution of the QCD jet background.
% This is an effect which has qualitatively been observed in other studies, and which we aim to formalise in the context of this work.

\section{Inputs, Architecture and Metrics}
% \label{sec}

In this paper we employ Variational Autoencoders (VAEs)~\cite{VAE,betaVAE} motivated by the Information Bottleneck theory~\cite{tishby2015dee} extended to unsupervised setting \cite{Voloshynovskiy2019NeurIPS} to reconstruct QCD jets. The architecture of the trained model is shown in Fig.~\ref{fig:VAE}.
We simulate jets from proton-proton collisions at a centre of mass energy $\sqrt{s}=13$~TeV using MadGraph+Pythia8~\cite{Madgraph,Pythia} interfaced with Delphes~\cite{Delphes} to simulate the interactions in the ATLAS detector~\cite{ATLAS} at the LHC. Jets are clustered using the anti-$k_{\mathrm{T}}$ algorithm~\cite{antikt} with a radius parameter $\Delta R=1.0$ using particle flow objects and without applying any trimming. Events were simulated
without the inclusion of pile-up interactions. A minimum jet $p_\mathrm{T}$ cut of 450~GeV is applied on the leading jet of a simulated event, emulating the large jet trigger threshold of the ATLAS detector.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.66\textwidth]{figures/New_Fig_general_diagram_without_notations_large.pdf}
    \caption{Model architecture used for jet reconstruction and anomaly detection.
    The autoencoder networks are highlighted in the blue dashed box, with the input and reconstructed output, $\mathrm{x}$ and $\mathrm{\hat{x}}$, shown in light red. The latent space distribution, $\mathrm{z}$, is represented by a green rectangle for both the true distribution $p_\theta (\mathrm{z})$ and the distribution sampled from the encoded mean and variance from the encoder $q_\phi (\mathrm{z})$. The connections between the input, output and latent space to the three constraints, which are used as loss terms in the training, are depicted by arrows.}
    %The encoder and decoder (mirrored) consist of fully connected layers of 256, 128 and 64 nodes. The latent space is 10 dimensions, with $Z$ sampled from the encoded mean and variance, with the standard VAE Gaussian prior constraint, with a loss weight of 0.25. The two observation space constraints are the mean square error between the input and output, and an adversarial discriminator trained to identify the input from reconstructed outputs. The discriminator is connected to the output of the network a gradient reversal layer~\cite{GradReversal} with a gradient penalty of 10 with the loss term assigned a weight of 0.1, to lower the effective learning rate of the discriminator in comparison to the encoder and decoder.}
    \label{fig:VAE}
\end{figure}

In this study we use simulated QCD jets to train the VAE. To test its performance for anomaly detection we simulate jets initiated by top quarks decaying hadronically, and two hypothetical new physics models based on Stealth Bosons~\cite{StealthBosonJA}, one with a mass of 80 GeV (\textit{StealthB(80)}), and another with a mass of 400 GeV (\textit{StealthB(400)}). Each Stealth Boson decays in a four prong process via two pseudo-scalars of mass 30~GeV and 80~GeV respectively.
% The two stealth boson samples are produced with higher transverse momentum, which has an inverse relationship with jet mass on substructure variables.
% A key property in observing new physics is the mass of the jet, which is strongly tied to the mass of the initiating particle.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.66\textwidth]{figures/New_Fig_general_diagram_without_notations.pdf}
%     \caption{Model architecture used for jet reconstruction and anomaly detection.
%     The autoencoder networks are highlighted in the blue dashed box, with the input and reconstructed output, $\mathrm{x}$ and $\mathrm{\hat{x}}$, shown in light red. The latent space distribution, $\mathrm{z}$, is represented by a green rectangle for both the true distribution $p_\theta (\mathrm{z})$ and the distribution sampled from the encoded mean and variance from the encoder $q_\phi (\mathrm{z})$. The connections between the input, output and latent space to the three constraints, which are used as loss terms in the training, are depicted by arrows.}
%     %The encoder and decoder (mirrored) consist of fully connected layers of 256, 128 and 64 nodes. The latent space is 10 dimensions, with $Z$ sampled from the encoded mean and variance, with the standard VAE Gaussian prior constraint, with a loss weight of 0.25. The two observation space constraints are the mean square error between the input and output, and an adversarial discriminator trained to identify the input from reconstructed outputs. The discriminator is connected to the output of the network a gradient reversal layer~\cite{GradReversal} with a gradient penalty of 10 with the loss term assigned a weight of 0.1, to lower the effective learning rate of the discriminator in comparison to the encoder and decoder.}
%     \label{fig:VAE}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.95\textwidth]{figures/AvgJetImage_input_onaxes.pdf}
%     \caption{Average distribution of momentum within a jet in the $\eta-\phi$ plane for the four particles considered in this paper after applying preprocessing. From left to right: QCD, Top, StealthB(80) and StealthB(400). The intensity corresponds to $p_{\mathrm{T}}$ fraction of the jet.}
%     \label{fig:jetimg}
% \end{figure}

As an input to the VAE we use up to the 20 most energetic constituents of each jet, with zero padding in the case of fewer constituents.
It was found that for a higher number of constituents the reconstruction performance and ability to detect anomalies did not improve.
We represent these constituents as a four-momenta point cloud, with each point in the form $(p_x,p_y,p_z,E)$. As each constituent is treated as massless, the $E$ term is the quadratic sum of the three momenta. A jet is reconstructed as the linear sum of all constituents.
In order to reduce ambiguity from degrees of freedom, a preprocessing involving centering and rotating the jet constituents is applied, following the procedure proposed in Ref.~\cite{JetImages}. Furthermore, each component is normalised by the transverse momentum of the jet $(p_{\mathrm{T}})$. Because only the leading 20 constituents are considered, the sum of constituent $p_\mathrm{T}$ can be less than the jet $p_\mathrm{T}$, and the reconstructed mass can be lower than predicted.
The average signature of these jets is shown in Fig.\ref{fig:jetimg} for two known processes, and two hypothetical particles. It should be noted that in contrast to the average images, an individual jet is stochastic and sparse, which presents additional challenges.
The constituents are flattened into an 80-dimensional vector with a linear scaling applied to all input dimensions, using the median and interquartile range in each of the input dimensions separately.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/AvgJetImage_input_onaxes-max.pdf}
    \caption{Average distribution of momentum within a jet in the $\eta-\phi$ plane for the four particles considered in this paper after applying preprocessing. From left to right: QCD, Top, StealthB(80) and StealthB(400). The intensity corresponds to $p_{\mathrm{T}}$ fraction of the jet.}
    \label{fig:jetimg}
\end{figure}




The model used for the studies in this paper is shown in Fig.~\ref{fig:VAE}. 
The model is based on the variational decomposition of unsupervised information bottleneck considered in Ref.~\cite{Voloshynovskiy2019NeurIPS}. The first term of this decomposition corresponds to the mutual information minimization between the input and latent space. It is represented by an upper bound, which also used in the traditional VAE, and is denoted as $\mathcal{D}_{\mathrm{z}|\mathrm{x}}$. The second term is responsible for the maximisation of the mutual information between the latent space and the reconstruction space. It has an upper bound set by a conditional entropy term $\mathcal{D}_{x\hat{x}}$, which ensures the accurate reconstruction on a sample level, and the Kullback-Leibler divergence term $\mathcal{D}_\mathrm{x}$, which ensures a correspondence between the total distributions of reconstructed and input data.
On the data set presented in this paper, the addition of $\mathcal{D}_\mathrm{x}$ dramatically improves the reconstruction fidelity of the average jet shower, visible when looking at the representation shown in Fig.~\ref{fig:jetimg} for reconstructed jets. When considering only the $\mathcal{D}_{x\hat{x}}$ term in the training, the relationship between the average $p_\mathrm{T}$ fraction of constituents, which span several orders of magnitude, and the location of a constituent is not accurately preserved, and a smeared central region is seen in the average representation.




The encoder and decoder (mirrored) consist of fully connected layers of 256, 128 and 64 nodes. The latent space has 10 dimensions, with $\mathrm{z}$ sampled from the encoded mean and variance. This architecture is chosen to match the network presented Ref.~\cite{oevae}, which is seen to have good reconstruction performance.
A Gaussian prior constraint on  $\mathrm{z}$ is enforced by the Kullback-Leibler divergence term, denoted as $\mathcal{D}_{\mathrm{z}|\mathrm{x}}$, with a loss weight of 0.25. The two observation space constraints are the mean square error (MSE, here denoted as $\mathcal{D}_{x\hat{x}}$) between the input and output, and an adversarial discriminator ($\mathcal{D}_\mathrm{x}$) trained to identify the input from reconstructed outputs. $\mathcal{D}_\mathrm{x}$ is connected to the output of the network via a gradient reversal layer~\cite{GradReversal}. A gradient penalty of 10 with the loss term multiplied by 0.1 is used to lower the effective learning rate of the discriminator in comparison to the variational autoencoder.
% As shown in Fig.~\ref{fig:VAE}, the model consists of a symmetrical encoder and decoder, which embed the input jets into a 10 dimensional latent space. Furthermore, an additional discriminator~($\mathcal{D}_\mathrm{x}$) is employed as a density ratio estimator between the input and output of the autoencoder. This is included as an adversarial loss term during the training with a gradient reversal layer used to allow for a simultaneous training of all networks in the model \cite{GradReversal}.
The network is trained without $\mathcal{D}_\mathrm{x}$ for an initial 50 epochs, at a learning rate of $10^{-3}$. This is followed by 10 epochs with the adversarial network included in the training, with all three loss terms and a learning rate of $10^{-5}$. Finally, the network is trained for a further 90 epochs with the weights in $\mathcal{D}_\mathrm{x}$ frozen, using all three loss terms and a learning rate of $10^{-3}$.
The model is implemented and trained using TensorFlow~2.2~\cite{Tensorflow}.
The discriminator network is frozen to ensure a weak discriminator which was found to perform better for anomaly detection.


More complicated architectures were tested in these studies, for example including the use of a recursive input instead of a flat vector, and an adversarial discriminant for the latent space prior. However, all networks presented the same behaviour as the model presented in this paper when applied to anomaly detection. The final choice of the hyperparameters used in training was made based on the reconstruction of the jet mass and the ability of the $\mathcal{D}_\mathrm{x}$ network to separate QCD jets from top jets for both the input and reconstructed jets.
% Paragraph about VAE training procedure, hyperparameters and loss. Need to redo this section to incorporate BIB motivated architecture.

In this paper five metrics to detect anomalous jets are compared. Firstly, the \textit{MSE} of a jet passed through the network is considered. A second observation space metric called the \textit{Energy Movers Distance}~(EMD)~\cite{EMD}, motivated by optimal transport problems, is employed between an input jet and its reconstructed output using the momenta and energy as a fraction of jet $p_\mathrm{T}$ for the leading 20 constituents.
Two approaches are employed in the latent representation of a jet, using the encoded means. Firstly, a \textit{$k$-Nearest Neighbour} (kNN) algorithm~\cite{sklearn} is trained on the encoded latent space with the sum distance of the nearest neighbours to the encoded point of a jet used as a metric. Secondly, a \textit{One Class Support Vector Machine}~(OCSVM)~\cite{sklearn} is trained on the encoded latent representation. In both cases, only the class of jets used for training the network are used to train the latent space metrics.
Finally, the output of $\mathcal{D}_\mathrm{x}$ for the jet in the input space~(Dx(in)) is studied.

\section{Performance}

In the training three loss terms are being balanced and jointly minimised, however one of the key measures of performance is the ability to reconstruct the masses of jets.
In Fig.~\ref{fig:performance} the input and reconstructed jet mass are compared for QCD jets, top jets, and the two Stealth Boson jets. The network is able to reconstruct the mass of QCD jets to a high degree, with the signal samples all being reasonably well modelled. This demonstrates that the network has learned some of the underlying physics, though it is not able to perfectly capture the resonances. 

Furthermore, as the aim is to use the VAE for anomaly detection, the ability to separate the background, QCD, from different signals is therefore also important to measure. The ROC curves for the different anomaly metrics for separating each of the three signal samples from QCD are shown in Fig.~\ref{fig:performance}. Here it can be seen that only the EMD metric can correctly identify StealthB(80) as anomalous, with the other metrics assigning this signal to be more nominal-like than QCD.

In the case of Stealth(80), the jets are from a much narrower resonance than top and StealthB(400) jets. They also have a much higher $p_{\mathrm{T}}$ distribution compared to QCD and top jets. This boosting leads to a very collimated jet, with the opening angle correlated to the ratio of the jet mass and the jet $p_\mathrm{T}$. This effect can be seen in the average jet images in Fig.~\ref{fig:jetimg}, where the StealthB(80) jets have less identifiable substructure, and where the distribution of constituent momentum is strongly concentrated in the centre of the jet. As a result, it is much harder for the current metrics to identify the jets as anomalous.
Excluding StealthB(80), the other metrics offer a range of performance, with the observation space metrics and kNN performing particularly well at identifying top and StealthB(400) jets from QCD.

% Trained networks look at performance of metrics.
% MSE is the best, inclusively, better on high mass SB, worse on low mass SB. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/JetMassReco.pdf} \hskip4ex
    \includegraphics[width=0.45\textwidth]{figures/ROC_vae_dx_qcd_all_signals_compressed.pdf}
    \caption{Jet mass of the input and reconstructed jets (left) and ROC curves calculated from the metrics for separating top jets (orange), low mass (red) and high mass (green) Stealth Bosons from QCD jets.}
    %Reconstructed jet masses are lower than predicted from theory due to selecting only the leading 20 constituents.}
    \label{fig:performance}
\end{figure}

\section{The Question of Mass}

Just as the jet mass is a key property to study how well known jets are reconstructed, and which is used to validate the autoencoder performance, it is also hugely important in the context of anomaly detection.
When searching for new phenomena in particle physics, the most quantifying property of any observation is the observed mass of a resonance. As such, it is crucial that we are able to identify anomalous jets for a range of mass values, and that when applying a cut on an anomaly metric we do not artificially create a signal from sculpting of the mass distribution.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/sculpting.pdf}\hskip4ex
    \includegraphics[width=0.45\textwidth]{figures/summaryscatter.pdf}
    % \includegraphics[width=0.32\textwidth]{example-image-1x1}
    \caption{QCD jet mass sculpting after applying cuts on MSE and EMD (left).
    Comparing signal efficiency versus mass sculpting (right) of the metrics for top jets (solid) and StealthB(400) (hollow).
    % Mass sculpting measured from the relative entropy of the QCD jet mass distribution after applying the cut to the inclusive distribution.
    %The optimal case is a high signal efficiency with low mass sculpting, and no increase in sculpting for higher background rejections.
    }
    \label{fig:mass_sculpt}
\end{figure}

As has been seen in the previous section, it is more difficult for the anomaly metrics to identify the lower mass StealthB(80) correctly. Furthermore, when applying a cut on the MSE distribution, we can see that the QCD background is indeed sculpted and biased towards higher mass jets, as shown in Fig.~\ref{fig:mass_sculpt}. To quantify the performance of all the metrics, the amount of mass sculpting is compared to the rejection efficiency of QCD for each of the metrics, alongside the signal acceptance, for top and StealthB(400) jets.
Mass sculpting is measured from the relative entropy of the QCD jet mass distribution after applying the cut to the inclusive distribution.
The ideal metric would have a flat response in sculpting as a function of background rejection, whilst maintaining high signal efficiency, for both signals. Of the metrics studied, kNN is particularly bad for mass sculpting, whereas EMD has the least impact on selected jet masses. 

% Looking at performance in another way, jet mass, key property, bump hunt.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/sculpting.pdf}\hskip4ex
%     \includegraphics[width=0.45\textwidth]{figures/summaryscatter.pdf}
%     % \includegraphics[width=0.32\textwidth]{example-image-1x1}
%     \caption{QCD jet mass sculpting after applying cuts on MSE and EMD (left).
%     Comparing signal efficiency versus mass sculpting (right) of the metrics for top jets (solid) and StealthB(400) (hollow).
%     % Mass sculpting measured from the relative entropy of the QCD jet mass distribution after applying the cut to the inclusive distribution.
%     %The optimal case is a high signal efficiency with low mass sculpting, and no increase in sculpting for higher background rejections.
%     }
%     \label{fig:mass_sculpt}
% \end{figure}

Another robustness test of anomaly metrics is to change the definition of what is nominal, and to train the network using a more massive signal than QCD, for example top jets.
In Fig.~\ref{fig:invert} this comparison has been performed for MSE on the model trained on top and QCD jets, as well as for kNN in the latent space.
In both observation space and latent space metrics, higher mass signals are seen as more anomalous, even when used as the nominal dataset for training. In particular, MSE is almost unchanged for top jets and QCD, regardless of which sample is used for training.
% For StealthB(400) the distribution is broader but the general trend is the unchanged.
This metric is highly correlated to jet mass, and anti correlated to jet $p_{\mathrm{T}}$. These are two underlying properties which are dominated by the relative position and momenta of the two leading constituents of the jet.
% Of all the metrics studied, only EMD has a positive correlation with $p_\mathrm{T}$.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/MSESBscores.pdf}\hskip4ex
    \includegraphics[width=0.45\textwidth]{figures/kNNSBscores.pdf}
    % \includegraphics[width=0.32\textwidth]{example-image-b}
    % \includegraphics[width=0.45\textwidth]{example-image-c}
    \caption{Comparing MSE score (left) and kNN (right) for QCD, top jets, and StealthB(400) when training the model and metrics with QCD (solid) or Top (dashed) as nominal.}
    % \caption{QCD Top Normal-Invert training for MSE, LS, Stealth Bosons}
    \label{fig:invert}
\end{figure}

\section{Conclusions}
% \label{others}

As can be seen from the studies presented in this paper, particular care needs to be taken when designing anomaly metrics for jets in high energy physics.
Jet mass is a key property in assessing reconstruction performance, but also for the eventual application of anomaly detection in an analysis.
% Of the five metrics studied, all of them su
As such, the performance of multiple approaches should be evaluated with a variety of signals at different masses, in order to choose an approach not with the best absolute performance, but which is most robust to signals of all masses and protection against artificially introducing a signal through mass sculpting.

Outlier Exposure presents one avenue of study with promising results, as investigated in Ref.~\cite{oevae}. Furthermore, attempts to decorrelate the anomaly metrics or components of the architecture are another direction of interest. Using techniques to directly remove the correlation to the jet mass from the latent space are under investigation, and making all  constraints $\mathcal{D}_{\mathrm{x}\mathrm{\hat{x}}}$, $\mathcal{D}_{\mathrm{\hat{x}}}$ and $\mathcal{D}_{\mathrm{z}}$ conditionl on the jet mass provides an additional avenue of study.

Further studies should also test the generalisability of techniques to identify anomalous jets from the nominal sample, regardless of what is used in training, ensuring that anomalies are detected using jet substructure, not simply identifying the mass of the jets.
We believe that in all cases, similar benchmark metrics to those presented in this paper will form a necessary component in the performance evaluation.

% As can be seen from the results presented in this paper, particular care needs to be taken to avoid an intrinsic bias towards higher mass jets when employing standard anomaly detection techniques. The generalisability of the approaches for what is considered as anomalous is also non trivial and prevents an opportunity for exciting new developments.

% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare 
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work). 
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2020/PaperInformation/FundingDisclosure}.


% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
% \end{ack}
\clearpage
% \section*{References}

\section*{Broader Impact}

The proposed approach is based on the information bottleneck formulation applied to anomaly detection in high energy physics. In particular we focus on one of the major complexities involved with applying this family of approaches to jet physics, namely the large and non-trivial dependence on the jet mass.
The approach is generic in nature and subsequently can be used in a wide range of practical applications, and the lessons learned from understanding the relationship between what is seen as anomalous and latent properties is equally transferable to other domains.
In this paper we do not address any ethical aspects or societal consequences, as the focus is on understanding the fundamental nature of the universe.
A positive outcome of this work is in establishing a series of checks and considerations to enable a fair comparison of approaches and benchmarks. No negative outcomes are foreseen as this paper seeks to promote engagement in the community and help improve theoretical and empirical understanding of this burgeoning field of research. Though we cannot at this time present the holy grail that is a solution to this problem in a fully unsupervised setting, this is the subject of our future work. We also plan to incorporate into a full statistical analysis, searching for signs of new elementary particles in proton collisions, and devise a generic approach that would have the potential to be applied to other domains.

\small




\begin{thebibliography}{15}

\bibitem{VAE} 
D. P. Kingma and M. Welling, \textit{Auto-Encoding Variational Bayes}. 
arXiv:1312.6114, (2013).

\bibitem{betaVAE} 
I. Higgins et. al., \textit{beta-VAE: Learning Basic Concepts with a Constrained Variational Framework}. (2017). 
arXiv:1312.6114, 2013.

\bibitem{tishby2015dee}
N. Tishby and N. Zaslavsky, \textit{Deep learning and the information bottleneck principle}.  2015 IEEE Information Theory Workshop, (2015).

\bibitem{Voloshynovskiy2019NeurIPS}
S. Voloshynovskiy et. al., \textit{Information bottleneck through variational glasses}. NeurIPS Workshop on Bayesian Deep Learning, (2019).

\bibitem{Madgraph}
J. Alwall et. al. \textit{The automated computation of tree-level and next-to-leading order differential cross sections, and their matching to parton shower simulations}. arXiv:1405.0301, (2014).

\bibitem{Pythia}
T. Sjostrand, S. Mrenna, and P. Z. Skands, \textit{A Brief Introduction to PYTHIA 8.1}. arXiv:0710.3820, (2008).

\bibitem{Delphes}
J. de Favereau et. al., \textit{DELPHES 3, A modular framework for fast simulation of a generic collider experiment}. arXiv:1307.6346, (2013).

\bibitem{ATLAS}
ATLAS Collaboration, \textit{The ATLAS Experiment at the CERN Large Hadron Collider}. JINST 3 S08003, (2008).

\bibitem{antikt}
M. Cacciari et. al., \textit{The anti-k$_\mathrm{t}$ jet clustering algorithm}. JHEP 04, (2008). 

\bibitem{StealthBosonJA}
J. A. Aguilar-Saavedra and F. R. Joaquim, \textit{The minimal stealth boson: models and benchmarks}. arXiv:1905.12651, (2019).

\bibitem{JetImages}
J. Pearkes et. al., \textit{Jet Constituents for Deep Neural Network Based Top Quark Tagging}. arXiv:1704.02124, (2017).

\bibitem{oevae}
T. Cheng et. al., \textit{Variational Autoencoders for Anomalous Jet Tagging}. arXiv:2007.01850, (2020).

\bibitem{GradReversal}
Y Ganin and V Lempitsky, \textit{Unsupervised Domain Adaptation by Backpropagation}. arXiv:1409.7495, (2014).

\bibitem{EMD}
P. T. Komiske, E. M. Metodiev, and J. Thaler, \textit{The Metric Space of Collider Events}. arXiv:1902.02346, (2019).

\bibitem{Tensorflow}
M. Abadi et. al., \textit{TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems}. \textsc{url}: \url{https://www.tensorflow.org}, (2015).

\bibitem{sklearn}
F. Pedregosa et. al., \textit{Scikit-learn: Machine Learning in {P}ython}. JMLR 12, (2011).



%@inproceedings{tishby2015deep,
%  title={Deep learning and the information bottleneck principle},
%  author={Tishby, Naftali and Zaslavsky, Noga},
%  booktitle={2015 IEEE Information Theory Workshop (ITW)},
%  pages={1--5},
%  year={2015},
%  organization={IEEE}
%}
%@inproceedings { Voloshynovskiy2019NeurIPS,
%    author = { Voloshynovskiy, Slava and Kondah, Mouad and Rezaeifar, Shideh and Taran, Olga and Hotolyak, Taras and Rezende, Danilo Jimenez },
%    booktitle = { NeurIPS Workshop on Bayesian Deep Learning },
%    title = { Information bottleneck through variational glasses },
%    address = { Vancouver, Canada },
%    pages = { },
%    month = { December },
%    year = { 2019 }
%}


\end{thebibliography}
% References follow the acknowledgments. Use unnumbered first-level heading for
% the references. Any choice of citation style is acceptable as long as you are
% consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
% when listing the references.
% {\bf Note that the Reference section does not count towards the eight pages of content that are allowed.}
% \medskip



% [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
% connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
% (eds.), {\it Advances in Neural Information Processing Systems 7},
% pp.\ 609--616. Cambridge, MA: MIT Press.

% [2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
%   Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
% TELOS/Springer--Verlag.

% [3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
% recall at excitatory recurrent synapses and cholinergic modulation in rat
% hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
