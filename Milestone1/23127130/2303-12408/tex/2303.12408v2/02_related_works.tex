\section{Related Works}

\paragraph{Visualizing Omnidirectional View of Scenes}
% Panoramic images are widely used in many applications for remote experiences for foreign streets or vacation homes.
Panoramic images are widely used in many applications for remote experiences. %for foreign streets or vacation homes.
After captured by photo-stitching apps or dedicated hardware, they allow users to rotate around the captured position.
However, we need additional information to allow the full 6 DoF movement in the scene.
Prior works propose sophisticated camera rigs to capture spherical light fields~\cite{broxton2019low, broxton2020deepview, broxton2020immersive, pozo2019integrated, overbeck2018system}.
% Given multiple viewpoints of captured images, they extend existing works for novel view synthesis methods and reconstruct 3D mesh or multi-sphere images instead of multi-plane images in ordinary images.
Given multi-view images, they enable synthesizing images at novel viewpoints by reconstructing 3D mesh or multi-sphere images instead of multi-plane images in ordinary images.
With additional depth information, recent works demonstrate novel view synthesis with a single panoramic image~\cite{hara2022enhancement, hsu2021moving}.
The depth channel is acquired from RGBD camera or approximated coarse planar facades.

In contrast, we assume more casual input, using commodity 360$^\circ$ camera with two fish-eye lenses to capture a short video clip of the large-scale scene.
A few works also explored the same setup~\cite{bertel2020omniphotos, jang2022egocentric} and represented the scene with a deformed proxy mesh with texture maps using pre-trained neural networks for optical flow and depth estimation.
Our pipeline is simpler as we train a neural network with the captured sequence of images without any pre-trained network.
We combine the visualization pipeline for large-scale scenes with NeRF formulation and can capture complex view-dependent effects and fine structures, unlike reconstructed textured mesh.



\if 0
\paragraph{Scene Reconstruction from Outward-looking Images}
To reconstruct a scene for 6DoF view synthesis, the most accessible method is to capture images from different viewpoints.
For large-scale environments, however, it is costly to scan every nook and cranny of the scene with a small-fov camera.
Therefore, prior works~\cite{broxton2019low, broxton2020deepview, broxton2020immersive, pozo2019integrated, overbeck2018system} capture the spherical light fields for efficient scene acquisition and achieve free-viewpoint rendering by reconstructing mesh or multi-sphere images.
But they need a specialized system consisting of a multi-camera rig to capture, which is not available to end users.
Recent works~\cite{hara2022enhancement, hsu2021moving} propose neural rendering methods to synthesize novel view images from a single omnidirectional image.
However, they need an additional depth channel from an RGB-D camera or an inaccurate facade approximation.
With the most casual setup,~\cite{bertel2020omniphotos, jang2022egocentric} use an input of short omnidirectional video taken from commercial 360$^\circ$ camera with two fish-eye lenses, which is identical to us.
Their reconstructed deformed proxy mesh and explicit mesh with texture map are not capable of representing view-dependent effects and fine structures.
Also, they need complicated multi-stage processes and pretrained networks for optical flow and depth estimation.
In contrast, we propose a simple yet efficient method to synthesize high-quality images from casually captured omnidirectional video combined with a recent neural rendering technique.
\fi

\paragraph{Practical Variants of NeRF}
NeRF~\cite{mildenhall2021nerf} flourished in the field of novel view synthesis, showing photorealistic quality with its simple formulation.
However, the original NeRF formulation exhibits clear drawbacks, such as lengthy training and rendering time, and the difficulty of deformation or scene edits.
Many follow-up works exploded, overcoming the limitations in various aspects~\cite{Barron_2021_ICCV, Barron_2022_CVPR, Martin-Brualla_2021_CVPR, Niemeyer_2021_CVPR, Park_2021_ICCV, Srinivasan_2021_CVPR, choi2022ibl}. % and proposed additional applications.
Here we specifically focus on practical extension for fast rendering and training.
% NeRF represents a scene as a single MLP that maps 5D coordinates (position and viewing direction) into color and volume density. 
NeRF represents a scene as a single MLP that maps coordinates into color and volume density. 
It is slow in rendering and optimization as the volume rendering requires multiple forward passes of the MLP.


To accelerate the rendering speed, radiance is represented with an explicit voxel grid storing features~\cite{liu2020neural, yu2021plenoctrees, hedman2021baking}.
However, they train the network by distilling information from pre-trained NeRF, which even lengthens the training time.
More recent works exploit various data structures to directly optimize the feature grid~\cite{chen2022tensorf, Sun_2022_CVPR, muller2022instant, Fridovich-Keil_2022_CVPR}.
They have shown that employing an explicit feature grid achieves fast optimization without sacrificing quality.
The feature grids are defined on the Cartesian coordinate system, which assumes a scene within a bounding box.
These are not suitable for representing large-scale scenes whose viewpoints observe outside of the captured locations.

The na\"ive strategy to choose ray samples wastes most samples and it leads to slow convergence since many regions are either free spaces or occluded by other objects in the real world.
To increase the sample efficiency, the original NeRF~\cite{mildenhall2021nerf} employs a hierarchical sampling strategy for the volume density and maintains two density MLPs for coarse and fine resolution, respectively.
In the same context, M\"uller et al.~\cite{muller2022instant} maintain additional multi-scale occupancy grids to skip ray marching steps.
Hu et al.~\cite{Hu_2022_CVPR} allocate dense momentum voxels for valid sampling, and Sun et al.~\cite{Sun_2022_CVPR} also use an extra coarse density voxel grid.
Maintaining separate coarse feature grids or neural networks requires additional memory and increases computational burdens. %, leading to slower convergence.
We propose an efficient sampling strategy and quickly train a volume that represents a large-scale environment.
\if 0
Another key factor for slow training in NeRF is the sparsity of the volume.
Many regions are either free spaces or occluded by other objects.
The original NeRF~\cite{mildenhall2021nerf} employs a hierarchical sampling strategy for the volume density and maintains two density MLPs for coarse and fine resolution, respectively.
In the same context, M\"uller et al.~\cite{muller2022instant} maintain additional multi-scale occupancy grids to skip ray marching steps.
Hu et al.~\cite{Hu_2022_CVPR} allocate dense momentum voxels for valid sampling, and Sun et al.~\cite{Sun_2022_CVPR} also use an extra coarse density voxel grid.
Maintaining separate coarse feature grids or neural networks requires additional memory and increases computational burdens. %, leading to slower convergence.
We propose an efficient sampling strategy and quickly train a volume that represents a large-scale environment.
\fi

\if 0
\paragraph{Neural Radiance Fields with Feature Grids} NeRF~\cite{mildenhall2021nerf} flourished in the field of novel view synthesis, showing photorealistic quality with its simple formulation.
NeRF represents a whole scene as a single MLP that maps 5D coordinates (position and viewing direction) into color and volume density.
Many follow-ups~\cite{Barron_2021_ICCV, Barron_2022_CVPR, Martin-Brualla_2021_CVPR, Niemeyer_2021_CVPR, Park_2021_ICCV, Srinivasan_2021_CVPR} adopt NeRF's MLP-based representation to tackle various problems.
However, the MLP-based representation suffers from slow optimization and rendering.
To accelerate the rendering speed, prior works~\cite{liu2020neural, yu2021plenoctrees, hedman2021baking}, model radiance with explicit feature voxels.
But they need to pretrain NeRF to distill information, which leads to lengthy reconstruction time.
Therefore recent works~\cite{chen2022tensorf, Sun_2022_CVPR, muller2022instant, Fridovich-Keil_2022_CVPR} exploit various data structures to directly optimize the feature grid.
They have shown that employing an explicit feature grid achieves fast optimization without sacrificing quality.
Nonetheless, all of the data structures proposed are based on the Cartesian coordinate, which is not suitable for outward-looking scenarios.
\fi