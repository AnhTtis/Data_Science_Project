% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage[accsupp]{axessibility}
\usepackage{listings}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{subcaption}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\Crefname{figure}{Figure}{Figures}
\crefname{figure}{Fig.}{Figs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{361} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\usepackage{colortbl}
\usepackage[table,dvipsnames]{xcolor}
\usepackage{xcolor}
\def\todo#1{{\color{purple}{\small\bf\sf#1}}}
\def\diff#1{{\color{magenta}{\small\bf\sf#1}}}

\definecolor{tab_yellow}{rgb}{1, 1, 0.7}
\definecolor{tab_orange}{rgb}{1, 0.85, 0.7}
\definecolor{tab_red}{rgb}{1, 0.7, 0.7}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Balanced Spherical Grid for Egocentric View Synthesis}

\author{Changwoon Choi$^1$, Sang Min Kim$^1$, Young Min Kim$^{1,2}$\\
{\small $^1$Dept. of Electrical and Computer Engineering, Seoul National University, Korea}\\
{\small $^2$Interdisciplinary Program in Artificial Intelligence and INMC, Seoul National University}\\
% Dept. of Electrical and Computer Engineering, Seoul National University, Korea\\
% {\tt\small changwoon.choi00@gmail.com, \{tkdals9082, youngmin.kim\}@snu.ac.kr}
}


\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \centering
        \captionsetup{type=figure}
        % \includegraphics[width=\linewidth]{figures/egonerf_teaser.pdf}
        \includegraphics[width=\linewidth]{figures/egonerf_teaser_lowres.pdf}
        \captionof{figure}{We propose a practical solution to reconstruct large-scale scenes from a short egocentric  video. (a) Our scalable capturing setup observes the holistic environment by casually swiping a selfie stick with an omnidirectional camera attached.
        %We take a casually captured omnidirectional video as our input. 
        (b) Then we optimize our balanced spherical feature grids which are tailored for the outward-looking setup. 
        (c) EgoNeRF can quickly train and render high-quality images at nearby positions.
        Project page: \url{https://changwoon.info/publications/EgoNeRF}
        %(c) We demonstrate the rendered images of our approach from a novel viewpoint.
        } 
        \label{fig:teaser}

\end{center}
}]


\maketitle

%%%%%%%%% ABSTRACT %%%%%%%%%
\begin{abstract}
\vspace{-0.5em}
We present EgoNeRF, a practical solution to reconstruct large-scale real-world environments for VR assets.
% Given a few seconds of casually captured 360 video, EgoNeRF can efficiently build neural radiance fields which enable high-quality rendering from novel viewpoints.
Given a few seconds of casually captured 360 video, EgoNeRF can efficiently build neural radiance fields.
Motivated by the recent acceleration of NeRF using feature grids, we adopt spherical coordinate instead of conventional Cartesian coordinate.
Cartesian feature grid is inefficient to represent large-scale unbounded scenes because it has a spatially uniform resolution, regardless of distance from viewers.
%Cartesian feature grid which has uniform resolution ignorant of distance from viewers' position is inefficient to represent large-scale or unbounded scenes.
The spherical parameterization better aligns with the rays of egocentric images, and yet enables factorization for performance enhancement.
However, the na\"ive spherical grid suffers from singularities at two poles, and also cannot represent unbounded scenes. % or points far from the origin.
To avoid singularities near poles, we combine two balanced grids, which results in a quasi-uniform angular grid.
% We also adapt to sample density by partitioning the radial grid exponentially to the radius and eventually placing an environment map at infinity. 
We also partition the radial grid exponentially and place an environment map at infinity to represent unbounded scenes. 
% The sparse density fields can stably converge by deploying resamping techniques.
% Together, we increase the number of valid samples to train NeRF volume for the omnidirectional set-up.
Furthermore, with our resampling technique for grid-based methods, we can increase the number of valid samples to train NeRF volume.
We extensively evaluate our method in our newly introduced synthetic and real-world egocentric 360 video datasets, and it consistently achieves state-of-the-art performance.
\vspace{-0.5em}
\end{abstract}

%%%%%%%%% BODY TEXT %%%%%%%%%
\input{01_introduction}
\input{02_related_works}
\input{03_balanced_spherical_grid.tex}
\input{04_EgoNeRF.tex}
%\input{05_experiments}
\input{05_exp_v2}
\input{06_conclusion}

\paragraph{Acknowledgements}
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2023-00208197) and Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) [NO.2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)].
Young Min Kim is the corresponding author.

%%%%%%%%% REFERENCES %%%%%%%%%
{\small
\bibliographystyle{ieee_fullname}
\bibliography{EgoNeRF_ref}
}

\input{supp}

\end{document}
