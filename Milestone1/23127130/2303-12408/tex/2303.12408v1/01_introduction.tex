\section{Introduction}

% VR and virtual environment, what is needed, and what are current limitations
With the recent advance in VR technology, there exists an increasing need to create immersive virtual environments.
While a synthetic environment can be created by expert designers, various applications also require transferring a real-world environment.
Spherical light fields~\cite{broxton2019low,broxton2020deepview,broxton2020immersive, pozo2019integrated,overbeck2018system} can visualize photorealistic rendering of the real-world environment with the help of dedicated hardware with carefully calibrated multiple cameras.
A few works~\cite{jang2022egocentric,bertel2020omniphotos} also attempt to synthesize novel view images by reconstructing an explicit mesh from an egocentric omnidirectional video.
However, their methods consist of complicated multi-stage pipelines and require pretraining for optical flow and depth estimation networks.


% video input, large scale capture; practicality system combined with NeRF
In this paper, we build a system that can visualize a large-scale scene without sophisticated hardware or neural networks trained with general scenes.
We utilize panoramic images, as suggested in spherical light fields.
However, we acquire input with a commodity omnidirectional camera with two fish-eye lenses instead of dedicated hardware.
As shown in~\cref{fig:teaser} (a), the environment can be captured with the omnidirectional camera attached to a selfie stick within less than five seconds.
Then the collected images observe a large-scale scene that surrounds the viewpoints. 
We introduce new synthetic and real-world datasets of omnidirectional videos acquired from both indoor and outdoor scenes.
Combined with Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf}, the images can train a neural volume that can render fine details or view-dependent effects without explicit 3D models.

\begin{figure}
    \includegraphics[width=\linewidth]{figures/grid_comparison.pdf}
    \caption{(a) When the camera trajectory is short relative to the scene size, the proposed balanced spherical grid (left) exhibits uniform hitting rate for grid cells whereas the conventional Cartesian grid (right) suffers from non-uniform ray-grid hits. The orange shade indicates the relative density of hit count of the grid cells. (b) Experiments show that spherical coordinates achieve nearly uniform ray-grid hit distribution, while Cartesian coordinate is biased to the center especially when we use a fine-resolution grid.}
    \if 0
    \caption{(a) 2D visualization of ray-grid intersections in outward-looking scenario, where the orange shade indicates the relative density of hit count of the grid cells. When the camera trajectory is short relative to the scene size, the proposed balanced spherical grid (left) exhibits uniform hitting rate for grid cells whereas the conventional Cartesian grid (right) suffers from non-uniform ray-grid hits, showing bright orange near the camera location. (b) Experiments show that spherical coordinates achieve nearly uniform ray-grid hit distribution, while Cartesian coordinates are biased to the center especially when we use a fine-resolution grid.}
    \fi
    \label{fig:grid_comparison}
\end{figure}



% Our main claim
\if 0
To this end, we present Egocentric Neural Radiance Fields, or EgoNeRF, which is the neural volume representation that is tailored to egocentric omnidirectional visual input.
The original set-up of NeRF volume is not optimal to model large-scale scenes.
In contrast to the original NeRF's outside-in viewing assumption, egocentric omnidirectional videos of large scenes mostly contain inside-out views.
% The neural volume is assumed to be within a bounding box, and the quality of images quickly degrades when NeRF tries to synthesize viewpoints far from those acquired during training.
To better align the captured viewing directions with the data structure, EgoNeRF models the volume using a spherical coordinate system centered at the current location instead of the conventional Cartesian coordinate system as depicted in~\cref{fig:grid_comparison}.
We store geometric features on the spherical grid and also factorize them, which leads to faster convergence without sacrificing performance~\cite{chen2022tensorf, Sun_2022_CVPR}.
\fi

To this end, we present Egocentric Neural Radiance Fields, or EgoNeRF, which is the neural volume representation tailored to egocentric omnidirectional visual input.
Although NeRF and its variants with MLP-based methods show remarkable performance in view synthesis, they suffer from lengthy training and rendering time.
The recent Cartesian feature grids can lead to faster convergence~\cite{chen2022tensorf, Sun_2022_CVPR} for rendering a bounded scene with an isolated object, but they have several limitations for our datasets which mostly contain inside-out views of large scenes:
%To overcome the slow speed of MLP-based methods, recent methods~\cite{chen2022tensorf, Sun_2022_CVPR} optimize Cartesian feature grids which leads to faster convergence without sacrificing performance.
%However, in contrast to the original NeRF's outside-in viewing assumption, egocentric omnidirectional videos of large scenes mostly contain inside-out views.
%Cartesian feature grid has several limitations in the outward-looking scenario.
% (1) The uniform grid size, regardless of distance from the camera, makes it inefficient to represent fine details of near objects and coarse integrated information from far objects.
(1) The uniform grid size, regardless of distance from the camera, is insufficient to represent fine details of near objects and extravagant for coarse integrated information from far objects.
%(2) Cartesian grid suffers from non-uniform ray-grid hits in the egocentric scenario, which leads to slow convergence and blurry results as demonstrated in~\cref{fig:grid_comparison}.
% (2) Cartesian grid suffers from non-uniform ray-grid hits in the egocentric scenario as demonstrated in~\cref{fig:grid_comparison}, thus prior arts need careful training strategies such as coarse-to-fine grid upsampling~\cite{chen2022tensorf,Sun_2022_CVPR}.
(2) Cartesian grid suffers from non-uniform ray-grid hits in the egocentric scenario as demonstrated in~\cref{fig:grid_comparison}, thus, as pointed in~\cite{Sun_2022_CVPR}, prior arts need careful training strategies such as progressive scaling~\cite{chen2022tensorf,Sun_2022_CVPR} or view-count-adaptive per-voxel learning rate~\cite{Sun_2022_CVPR}.
% These limitations cause blurry artifacts and suboptimal results for Cartesian grid-based approaches.
% Furthermore, prior works are limited to representing a bounded scene with feature grids.
EgoNeRF models the volume using a spherical coordinate system to cope with the aforementioned limitations.
\Cref{fig:time-PSNR} shows that EgoNeRF converges faster compared to MLP-based methods (NeRF~\cite{mildenhall2021nerf} and mip-NeRF 360~\cite{Barron_2022_CVPR}) and has higher performance compared to Cartesian grid methods (TensoRF~\cite{chen2022tensorf} and DVGO~\cite{Sun_2022_CVPR}).
% ~\Cref{fig:time-PSNR} shows that our approach achieves faster convergence compared to MLP-based methods and higher performance compared to Cartesian feature grid methods.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/time-PSNR_graph_v1.pdf}
    \caption{Training curve comparison in \textit{OmniBlender} scenes.}
    \label{fig:time-PSNR}
\end{figure}

% balanced grid & training techniques
% \todo{Our spherical grid is designed to be balanced with respect to the egocentric omnidirectional image inputs, leading to a more efficient data structure for the large-scale environment.}
Our spherical grid is designed to be balanced in any direction, which leads to a more efficient data structure for the large-scale environment.
The na\"ive spherical grid contains high valence vertices at two poles, and, when adapted as a feature grid for neural volume rendering, the polar regions suffer from undesirable artifacts.
We exploit a quasi-uniform angular grid by combining two spherical grids~\cite{kageyama2004yin}.
% In the radial direction, the grid intervals increase quadratically at far fields such that the volumes occupied by spherical grid cells are proportional to the projected area covered by omnidirectional images.
In the radial direction, the grid intervals increase exponentially, which not only allows our representation to cover large spaces but also makes the spherical frustum have a similar length in the angular and radial directions.
We add an environment map at infinite depth, which is especially useful for outdoor environments with distant backgrounds such as skies.
% Last but not least, we propose an efficient resampling method along camera rays exploiting our density feature grid.
Last but not least, we propose an efficient hierarchical sampling method exploiting our density feature grid without maintaining an additional coarse density grid.


\if 0
%% contribution: efficient memory usage
To summarize, the technical contributions of the paper are highlighted as follows:
\begin{itemize}
    \item We present EgoNeRF, a practical approach to represent the radiance field of the large-scale scene from casually captured egocentric images/video.
    \vspace{-0.5em}
    \item We design a balanced spherical feature grid as an efficient data structure to train a neural volume for outward-looking images.
    \vspace{-0.5em}
    \item \todo{We propose a training strategy tailored to large-scale environment capture with NeRF.}
    %\item We introduce new synthetic and real-world datasets of omnidirectional videos which are egocentrically captured from both indoor and outdoor scenes.
\end{itemize}
\fi
We demonstrate that our proposed approach can lead to faster convergence and high-quality rendering with a small memory footprint in various scenarios for large-scale environments.
EgoNeRF is expected to create a virtual rendering of large scenes from data captured by non-expert users, which cannot be easily modeled with 3D assets or conventional NeRF. 








\if 0
% 1) VR device and problems of previous real-world capturing setup
The advent of commercial VR devices enables users to experience immersive content while navigating with 6 DoF in a virtual environment.
Most virtual scenes are synthetic 3D models created with extensive effort by professional designers, yet not up to the photorealistic level.
To display photorealistic scenes in VR headsets, recent works ~\cite{broxton2019low,broxton2020deepview,broxton2020immersive, pozo2019integrated,overbeck2018system} capture spherical light fields from real-world environment.
Notwithstanding previous approaches can generate high-quality free views nearby the captured position, they require specially built systems of dozens of cameras, which is difficult for end users to access.


% 2) Our problem setup (Egocentric omnidirectional video) & previous works with a similar setup
In this paper, we deal with a much more casual capturing condition using an omnidirectional camera (e.g. Ricoh Theta, Insta360 One) with two fisheye lenses which is cost-effective to capture environment scenes.
We aim to reconstruct an entire 3D scene from an egocentric omnidirectional video with a short trajectory.
By rotating an omnidirectional camera with a selfie stick as shown in ~\cref{fig:teaser}, one can casually record a short egocentric video less than 5 seconds, which enables to capture the surrounding scene while remaining nearly static.
% new paragraph?
By virtue of the advantage of the simple acquisition process, a few works~\cite{jang2022egocentric,bertel2020omniphotos} attempt to synthesize novel view images by reconstructing an explicit mesh from an egocentric omnidirectional video.
However, their methods consist of complicated multi-stage pipelines and require pretraining for optical flow and depth estimation networks.
Moreover, the explicit mesh representation is limited in expressing fine structures.

% 3) NeRF and NeRF with Voxel feature grids
Neural Radiance Fields (NeRF)~\cite{mildenhall2021nerf} has achieved great success in view synthesis of challenging scenes, including fine objects and various materials by representing a scene as coordinate-based MLP.
Furthermore, recent literature~\cite{chen2022tensorf, Sun_2022_CVPR} have shown that optimizing explicit Cartesian voxel grids containing appearance and geometry features ensures much faster convergence of NeRF without sacrificing performance.
Therefore, we propose an approach for egocentric view synthesis from an omnidirectional video in the manner of optimizing explicit feature grids with a tiny decoding MLP.


% 4) Limitation of existing methods
However, representing a large-scale scene from egocentric captured images with Cartesian feature grids has several limitations.
% 4-1) inefficacy 
First, the Cartesian grid has a uniform grid size regardless of distance from the camera. % , which leads to inefficient memory usage.
Since the image pixels obtained from the distant regions contain integrated information from the bigger volume, far-away regions do not need such a fine resolution.
In contrast, the closed regions need a finer resolution.
Therefore, the data structure ignorant of the distance between the camera center and the scene leads to inefficient memory usage.
% 4-2) non-robust optimization
Second, Cartesian grid is prone to suboptimal solution in egocentric setup.
In contrast to the original NeRF's outside-in viewing assumption, egocentric omnidirectional video of large scene mostly contain inside-out views.
As shown in~\cref{fig:grid_comparison}, grids nearby the viewer's position hit much more frequently than distant grids.
The nonuniform ray-grid intersection results in the wrong estimation of depth at far points from the camera. (i.e. suboptimal cloudy geometry near camera trajectory)

% 5) Our method
To this end, we present EgoNeRF (Egocentric Neural Radiance Fields) to relieve the aforementioned limitations with our novel representation tailored for egocentric omnidirectional video input.
We adopt spherical coordinate that handles the first issue by its inherent adaptive grid size according to distance from the grid center.
Furthermore, the spherical grid representation makes a uniform ray-grid hit rate which resolves the second limitation.
However, a na\"ive spherical grid (i.e. latitude-longitude grid) has a grid convergence problem near the two poles.
The grid convergence makes radial artifacts as shown in \todo{Fig.X}.
To cope with the problem, we use Yin-Yang grid~\cite{kageyama2004yin} which is a quasi-uniform overset grid in spherical geometry.
To maintain spherical frustum balanced, we exponentially increase the grid interval along $r$ direction.
Also, inspired by TensoRF~\cite{chen2022tensorf}, we decompose our feature grid tensor into compact vector and matrix factors, which efficiently reduces the space complexity.
Although our exponential stepping grid can cover large-scale scenes, it is not sufficient to cover infinite-far regions. (e.g. sky)
Therefore we additionally optimize an environment map to represent objects outside the grid.
Last but not least, we propose an efficient resampling method along camera rays exploiting our density feature grid.
The extensive experiments on our newly introduced synthetic and real-world dataset show that EgoNeRF is able to synthesize high-quality novel view images from egocentric images.
To summarize, the technical contributions of the paper are highlighted as follows:
\begin{itemize}
    \item We present EgoNeRF, an efficient and scalable approach to represent the radiance field of the large-scale scene from casually captured egocentric images/video.
    \item Our novel balanced spherical feature grid is the first attempt to design explicit grids for neural rendering suited to egocentric capturing scenario.
    % the first가 조금 위험하면
    % \item We propose a novel balanced spherical feature grid representation for neural rendering which is an efficient data structure for the egocentric capturing scenario.
    \item We introduce new synthetic and real-world datasets of omnidirectional videos which are egocentrically captured from both indoor and outdoor scenes.
\end{itemize}
\fi