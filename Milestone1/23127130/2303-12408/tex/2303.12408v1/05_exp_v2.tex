\input{tables/quantitative_all.tex}

\section{Experiments}
\label{sec:experiments}

We demonstrate that EgoNeRF can quickly capture and synthesize novel views of large-scale scenes. 
We describe full implementation details including hyperparameter setup in the supplementary material.

\if 0
\paragraph{Implementation Details}
We implement EgoNeRF with PyTorch~\cite{paszke2019pytorch} without any customized CUDA kernels for optimization.
For all scenes, we use $300^3$ voxels for both $\mathcal{G}_\sigma$ and $\mathcal{G}_a$ with $N_r^y:N_\theta^y:N_\phi^y=1:\frac{2\sqrt{3}}{3}:2\sqrt{3}$.
The dimension of appearance feature $C$ is 27 and we use two-layer MLP of 128 hidden units for decoding network $f_{\text{MLP}}$.
We use the same $r_0=0.005$ for all scenes and the size of convolution kernel $K$ for obtaining a coarse grid is 2.
We describe full implementation details in the supplementary material.
\fi

\paragraph{Datasets}
Since many of the existing datasets for NeRF are dedicated to a setup where a bounded object is captured from outside-in viewpoints, we propose new synthetic and real datasets of large-scale environments captured with omnidirectional videos.
\textit{OmniBlender} is a realistic synthetic dataset of 11 large-scale scenes with detailed textures and sophisticated geometries in both indoor and outdoor environments, 25 images for both train and test, respectively.
It consists of omnidirectional images along a relatively small circular camera path.
The spherical images are rendered using Blender's Cycles path tracing renderer~\cite{blender} with 2000$\times$1000 resolution.
\textit{Ricoh360} is a real-world 360$^\circ$ video dataset captured with a Ricoh Theta V camera with 1920$\times$960 resolution.
We record video on the circular path by rotating an omnidirectional camera fixed with a selfie stick as shown in \cref{fig:teaser} (a).
The dataset consists of 11 diverse indoor and outdoor scenes, 50 images for train and test, respectively.
With the benefit of the simple procedure, the whole data acquisition is finished in less than 5 seconds, which enables capturing the surrounding scene while it remains nearly static.
We obtain camera poses using SfM library OpenMVG~\cite{moulon2016openmvg}.
A detailed description of our dataset can be found in the supplementary material.


\paragraph{Baselines}
EgoNeRF is a variant of NeRF~\cite{mildenhall2021nerf}, which synthesizes novel views of the scene using the neural volume trained with multi-view images.
However, the original NeRF utilizes an MLP to represent the scene volume.
There also exists a recent variant called mip-NeRF 360~\cite{Barron_2022_CVPR}, which combines many techniques to increase the quality of the results, including the adaptation to unbounded scenes by warping space farther than a certain radius.
EgoNeRF employs feature grids and vector-matrix factorization in the balanced spherical grid. DVGO~\cite{Sun_2022_CVPR} exploits feature grids in a Cartesian coordinate with great acceleration, whereas TensoRF~\cite{chen2022tensorf} deploys factorization, also in Cartesian coordinate.
For all the methods, we train with the same ray batch size and the same number of feature grids (for DVGO and TensoRF) with one RTX-3090 GPU for a fair comparison.



\subsection{Quantitative Results}
The quantitative results are reported in mean PSNR, SSIM~\cite{wang2004image}, and LPIPS~\cite{zhang2018unreasonable} across test images in \textit{OmniBlender} and \textit{Ricoh360} dataset in \cref{tab:quantitative}.
Since equirectangular images in our datasets have distortion near poles, we additionally measure weighted-to-spherically uniform PSNR and SSIM~\cite{sun2017weighted} ($\text{PSNR}^{\text{WS}}$ and $\text{SSIM}^{\text{WS}}$), which place smaller weights near the poles when evaluating the metrics.

\Cref{tab:quantitative} shows that EgoNeRF outperforms all compared methods across all error metrics in the \textit{OmniBlender} dataset. 
With the efficient grid structure of EgoNeRF, the difference is more significant in earlier iterations.
Considering the time for each iteration, the efficiency gap is even more significant, which is also visualized in ~\cref{fig:time-PSNR}.
% Our approach shows high performance even at the early 5k steps, which takes 10 minutes of wall-clock time, in contrast to mip-NeRF 360 needs 22.6 hours to train 100k steps to outperform our results at 5k steps.
Our approach shows high performance even at the early 5k steps, which takes 10 minutes of wall-clock time. In contrast, mip-NeRF 360 needs approximately 8 hours to outperform our results at 5k steps.
%% discussion about artifacts with camera poses -- added below
In \textit{Ricoh360}, EgoNeRF surpasses other methods in 5k and 10k training steps, and shows comparable results in 100k steps.
However, our approach sometimes produces spotty artifacts in real-world datasets because the camera pose estimates can be erroneous.
On the other hand, MLP-based methods show blurry rendering, which sporadically achieves better scores in error metrics.
%However, our approach sometimes shows slight noisy artifacts which lower error metrics for some scenes in the real-world dataset when the camera poses are wrong, while compared methods show blurry artifacts for those scenes.
Such a phenomenon is prominent when the error in the camera pose makes the rays hit neighboring cells in the feature grid, which is further discussed in the supplementary material.
% Further discussion regarding the effects of grid resolution and camera pose errors on rendering results can be found in the supplementary material.

More importantly, the feature grid using the Cartesian coordinate system (TensoRF and DVGO) results in inferior performance, especially in outdoor scenes.
This supports our main claim that the Cartesian grid is inadequate to represent large-scale scenes captured from egocentric viewpoints.
In contrast, MLP-based methods (NeRF and mip-NeRF 360) achieve moderate results.

\if 0
\input{tables/inward_facing}
Although our approach is not designed to reconstruct small bounded objects from inward-facing images, we also report results from widely-used datasets for novel view synthesis in \cref{tab:inward_facing}.
EgoNeRF shows comparable results in the Synthetic-NeRF dataset~\cite{mildenhall2021nerf}, which contains 8 synthetic objects.
In mip-NeRF 360 dataset~\cite{Barron_2022_CVPR}, which contains inward-facing objects but has unbounded background scenes, EgoNeRF outperforms other baselines except mip-NeRF 360.
\fi


\input{figures/qualitative_results.tex}



\if 0
\subsection{Dataset \& Implementation Details}
\label{subsec:dataset_implementation_details}
% \input{tables/OmniBlender}
% \input{tables/Ricoh}
\input{tables/quantitative_all.tex}
\paragraph{Outward-Looking 360$^\circ$ Video Data}
Most of the existing widely used datasets for novel view synthesis capture the interest object around the outside-in viewpoints.
In contrast, we aim to reconstruct the large-scale environmental scene.
To this end, we evaluate our model on novel datasets of outward-looking omnidirectional videos.
\textit{OmniBlender} is a realistic synthetic dataset consisting of omnidirectional images along a relatively small circular camera path compared to the entire scene.
The spherical images are rendered using Blender's Cycles path tracing renderer~\cite{blender} with 2000$\times$1000 resolution.
OmniBlender contains 11 large-scale scenes with detailed textures and sophisticated geometries in both indoor and outdoor environments.
\textit{Ricoh360} is a real-world 360$^\circ$ video dataset captured with conventional Ricoh Theta V camera with 1920$\times$960 resolution.
We record video on the circular path by rotating an omnidirectional camera fixed with a selfie stick.
With the benefit of the simple procedure, the whole data acquisition is finished in less than 5 seconds, which enables capturing the surrounding scene while remaining nearly static.
The dataset is consist of 11 diverse indoor and outdoor scenes.
We obtain camera pose using structure-from-motion library OpenMVG~\cite{moulon2016openmvg}.
A detailed description of our dataset can be found in the supplementary material.

\paragraph{Inward-facing Data}
Although EgoNeRF is not best designed to represent small objects from inward-facing images, we also report results from widely-used outside-in viewing datasets.
We evaluate our approach on Synthetic-NeRF~\cite{mildenhall2021nerf}, which contains 8 synthetic objects and real-world data from mip-NeRF 360~\cite{Barron_2022_CVPR}.

\paragraph{Implementation Details}
We implement EgoNeRF with PyTorch~\cite{paszke2019pytorch} without any customized CUDA kernels for optimization.
For all scenes, we use $300^3$ voxels for both $\mathcal{G}_\sigma$ and $\mathcal{G}_a$ with $N_r^y:N_\theta^y:N_\phi^y=1:\frac{2\sqrt{3}}{3}:2\sqrt{3}$.
The dimension of appearance feature $C$ is 27 and we use two-layer MLP of 128 hidden units for decoding network $f_{\text{MLP}}$.
We use the same $r_0=0.005$ for all scenes and the size of convolution kernel $K$ for obtaining a coarse grid is 2.
We describe full implementation details in the supplementary material.

\input{figures/qualitative_results.tex}
\subsection{Results}
\label{subsec:results}
\paragraph{Compared methods}
We compare EgoNeRF with state-of-the-art feature grid-based novel-view synthesis algorithms DVGO~\cite{Sun_2022_CVPR} and TensoRF~\cite{chen2022tensorf}.
Both methods accelerate the training speed of NeRF by exploiting feature grids defined in the Cartesian coordinate system with voxel and vector-matrix decomposed representation.
We evaluate our method against MLP-based methods NeRF~\cite{mildenhall2021nerf}.
We also evaluate against mip-NeRF 360~\cite{Barron_2022_CVPR} which is able to model unbounded scenes by warping space farther than a certain radius.
For all the methods, we train with the same ray batch size and the same number of feature grids (for DVGO and TensoRF) for a fair comparison.


\paragraph{Evaluation}
We report mean PSNR, SSIM~\cite{wang2004image}, and LPIPS~\cite{zhang2018unreasonable} across test images in \textit{OmniBlender} and \textit{Ricoh360} dataset in~\cref{tab:quantitative}.
Since the equirectangular images in our datasets have distortion near poles, we additionally measure weighted-to-spherically uniform PSNR and SSIM~\cite{sun2017weighted} ($\text{PSNR}^{\text{WS}}$ and $\text{SSIM}^{\text{WS}}$).
EgoNeRF outperforms all compared methods across all error metrics in the OmniBlender dataset.
Also, EgoNeRF surpasses other methods in 5k, 10k training steps, and shows comparable results in 100k steps in Ricoh360.
Both feature grid-based methods with the Cartesian coordinate system show inferior performance, especially in outdoor scenes.
This supports our main claim that the Cartesian grid is inadequate to represent large-scale scenes captured from egocentric viewpoints.
In contrast, MLP-based methods NeRF and mip-NeRF 360 achieve moderate results.
It is notable that our approach shows high performance even at the early 5k steps, which takes 10 minutes of wall-clock time, in contrast to mip-NeRF 360 needs 22.6 hours to train 100k steps to outperform our results at 5k steps.
Although our approach is not best designed to reconstruct small bounded objects from inward-facing images, EgoNeRF shows comparable results in the Synthetic-NeRF dataset.
In mip-NeRF 360 dataset, which contains inward-facing objects but has unbounded background scenes, EgoNeRF outperforms other baselines except mip-NeRF 360.
\input{tables/inward_facing}

\fi


\subsection{Qualitative Results}


The qualitative results in \textit{OmniBlender} and \textit{Ricoh360} datasets are demonstrated in~\cref{fig:qualitative_comparison}.
Our method reconstructs fine details in both close-by objects and far-away regions.
% However, when we deploy Cartesian grids with the same number of cells, many cells are wasted without being trained in far objects, while center cells might not have a sufficient resolution as depicted in \cref{fig:grid_comparison}.
However, for Cartesian grid-based methods (TensoRF and DVGO), many cells are wasted without being trained in far objects, while center cells might not have a sufficient resolution as depicted in \cref{fig:grid_comparison}.
% However, prior works with Cartesian feature grids waste many cells without being trained in far objects, while center cells might not have a sufficient resolution as depicted in \cref{fig:grid_comparison}.
It results in visible artifacts in the picture in \textit{BarberShop}, bike in \textit{BistroBike}, bricks in \textit{bricks}, and posters in \textit{poster}.
This phenomenon is predominant in large scenes, while EgoNeRF gives consistently faithful results regardless of the size of the scenes.

MLP-based approaches show better visual results than Cartesian feature grid-based methods with much longer training and rendering time.
However, mip-NeRF 360 often misses fine structures: e.g., stick of broom, thin handle and support fixture in tray, headrest attachment in chair in \textit{Barbershop}, street lamp, side mirror of bike, small colorful lightbulbs and thin wire in \textit{BistroBike}.
Some of them are also indicated with white dotted circles in \cref{fig:qualitative_comparison}. 
% This may be due to mip-NeRF 360 resample ray samples from the additional proposal MLP and they do not apply rendering loss for the proposal MLP to relieve lengthy training time to optimize large MLPs, thus the weight distribution is strongly determined by the initial guess of the proposal MLP.
This may be due to mip-NeRF 360 resample ray samples from the proposal MLP and do not apply rendering loss for the proposal MLP to relieve lengthy training time to optimize large MLPs, thus the weight distribution is strongly determined by the initial guess of the proposal MLP.
In contrast, since our approach shares the same density grid $\mathcal{G}_\sigma$ to query volume density at coarse samples and fine samples, EgoNeRF shows superior rendering results on fine details.
Also, MLP-based approaches show smoothed rendering results across all the scenes (e.g., windows are blurred, cannot see the sky through the gap between bricks, the boundaries between stepping blocks are blurred in \textit{Bricks}, two reflected lights are merged in \textit{poster}. Some of them are also highlighted with white dotted circles in \cref{fig:qualitative_comparison}.), while EgoNeRF shows high-quality images similar to ground-truth images.



\subsection{Ablation study}
\label{subsec:ablation}
\input{tables/Ablation.tex}
\input{figures/ablation.tex}
We analyze the effects of important components of EgoNeRF with ablated versions.
\Cref{tab:ablation} shows that removing any of the components in our model degrades the performance across all metrics.
The first three rows are related to the balanced spherical grid.
Using the uniform radial partition deteriorates the performance, especially in outdoor scenes.
% Without Yin-Yang grids, the angular partition exhibits in high valence grid points on two poles and coarse cells near the equator.
Without Yin-Yang grids, the angular partition exhibits high valence grid points on two poles and degrades the error metrics consequently. 
Removing both radial and angular balanced grids, which is identical to uniform spherical grids, causes the biggest drop in performance except PSNR in indoor scenes.
As shown in the first row of~\cref{fig:ablation}, the spherical feature grid has radial direction artifacts (red box) and shows blurrier rendered results for nearby objects compared to our full model.
Also, not employing resampling techniques and using a double number of ray samples reduces performance.
Lastly, removing the environment map in outdoor scenes shows blurry artifacts in infinitely far regions as shown in the second row of~\cref{fig:ablation} and reduces the performance consequently.

We provide additional analysis on the impact of hyperparameters, scene depths, and out-of-distribution testing in the supplementary material.
