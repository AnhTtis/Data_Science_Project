\section{Training EgoNeRF}
\label{sec:EgoNeRF}

We utilize the balanced spherical grids to represent the volume density $\sigma$ and color $c$, which are stored in $\mathcal{G}_\sigma$ and $\mathcal{G}_a$, respectively.
In this chapter, we describe the technical details of the optimization process of our proposed method.


\if 0
Given a set of egocentric captured images/video with corresponding camera parameters, EgoNeRF aims to reconstruct 3D scene representation and synthesize novel view images. %% moved to 3.2
Further elaborate, we obtain pixel values through volume rendering by querying volume density $\sigma$ and color $c$, which are obtained from $\mathcal{G}_\sigma$ and $\mathcal{G}_a$ along camera rays.
In this chapter, we describe technical details in the optimization process of our proposed method.
\fi

\subsection{Hierarchical Density Adaptation}
%\subsection{Importance Sampling for Grid-Based Models}
\label{subsec:importance_sampling}

\if 0
We first train the density volume $\mathcal{G}_\sigma$.
As the scenes typically contain sparse occupied regions, we adapt the coarse-to-fine strategy of the original NeRF~\cite{mildenhall2021nerf}.
While other recent variants using feature grid~\cite{muller2022instant,Hu_2022_CVPR,Sun_2022_CVPR} maintain a dedicated data structure for the coarse grid, we store the values of coarse sample in the same structure as we store the fine grid.
When we update the grid with fine samples, we effectively use filtered values of initial coarse estimate.
This approach efficiently saves memory footprint and shortens the optimization time.

The coarse-to-fine strategy first samples coarse $N_c$ points along the ray to train a density estimate $\sigma$ from which we can sample fine $N_f$ points with importance sampling.
To share the estimates from coarse sample, we distill the value into the neighborhood of the fine grid with the convolution kernel $K$:
\begin{equation}
    \sigma(\mathbf{x_{\text{coarse}}}) = \mathcal{T}(\mathcal{G}_\sigma^{c}(\mathbf{x}_{\text{coarse}})) = \mathcal{T}(K * \mathcal{G}_\sigma(\mathbf{x}_{\text{fine}})).
\end{equation}
We use average pooling kernel as $K$.
It is reasonable to define a coarse grid by convolving the dense grid because our density grid $\mathcal{G}_\sigma$ stores the volume density itself, which has physical meaning, not neural features.

From the volume density values of coarse sampled points, we calculate weights for importance sampling by
\begin{equation}
    w_i = \tau_i (1 - e^{-\sigma_i \delta_i}), i\in [1, N_c],
\end{equation}
where $\delta_i$ is the distance between coarse samples, $\tau_i =\  e^{-\sum_{j=1}^{i-1}{\sigma_j\delta_j}}$ is transmittance.
Then the fine $N_f$ locations are sampled from the probability distribution. %, which is obtained by normalizing the coarse weights, using inverse transform sampling following original NeRF.
The volume density values of fine samples are queried from $\mathcal{G}_\sigma$ as described in Eq.~\ref{eq:querying}.
Finally, the volume density $\sigma$ and color $c$ at $N_c + N_f$ samples are used to render pixel.
\fi

As the scenes typically contain sparse occupied regions, we adapt the hierarchical sampling strategy of the original NeRF~\cite{mildenhall2021nerf} for feature grids.
While other recent variants using feature grid~\cite{muller2022instant,Hu_2022_CVPR,Sun_2022_CVPR} maintain a dedicated data structure for the coarse grid, we exploit our dense geometry feature grid $\mathcal{G}_\sigma$ for the first coarse sampling stage without allocating additional memory for the coarse grid.
% Then the positions of fine ray samples are sampled from the distribution obtained from coarse ray samples.

The hierarchical sampling strategy first samples coarse $N_c$ points along the ray to obtain a density estimate $\sigma$ from which we can sample fine $N_f$ points with importance sampling.
However, evaluating $\sigma$ with dense $\mathcal{G}_\sigma$ at the coarsely sampled points might skip the important surface regions.
Therefore, we obtain $\sigma$ value from a coarser density feature grid which can be obtained on the fly by applying a non-learnable convolution kernel $K$:
\begin{equation}
    \sigma(\mathbf{x_{\text{coarse}}}) = \mathcal{T}(\mathcal{G}_\sigma^{c}, \mathbf{x}_{\text{coarse}}) = \mathcal{T}(K * \mathcal{G}_\sigma, \mathbf{x}_{\text{coarse}}).
\end{equation}
We use the average pooling kernel as $K$.
It is reasonable to define a coarse grid by convolving the dense grid because our density grid $\mathcal{G}_\sigma$ stores the volume density itself, which has a physical meaning, not neural features.

From the volume density values of coarsely sampled points, we calculate weights for importance sampling by
\begin{equation}
    w_i = \tau_i (1 - e^{-\sigma_i \delta_i}), i\in [1, N_c],
\end{equation}
where $\delta_i$ is the distance between coarse samples, $\tau_i =\  e^{-\sum_{j=1}^{i-1}{\sigma_j\delta_j}}$ is the accumulated transmittance.
Then the fine $N_f$ locations are sampled from the filtered probability distribution. %, which is obtained by normalizing the coarse weights, using inverse transform sampling following original NeRF.
% The volume density values of fine samples are queried from $\mathcal{G}_\sigma$ as described in ~\cref{eq:querying}.
Finally, the volume density $\sigma$ and color $c$ at $N_c + N_f$ samples are used to render pixels.



\if 0
Most of the real-world scenes are dominated by unoccupied regions and occluded regions are frequently occur.
To exploit the sparsity and avoid inefficient samples in free space and occluded region, NeRF~\cite{mildenhall2021nerf} takes hierarchical sampling strategy.
They first sample coarse $N_c$ points along ray and query volume density $\sigma$ from network.
Then they sample fine $N_f$ points with importance sampling technique from the distribution of weights obtained from coarse volume densities.
To evaluate volume density values of fine points, NeRF needs another network.
In the same context, instant-NGP~\cite{muller2022instant} maintains additional multiscale occupancy grids to skip ray marching steps, EfficientNeRF~\cite{Hu_2022_CVPR} allocates dense momentum $\sigma$ voxels for valid sampling, and DVGO~\cite{Sun_2022_CVPR} also uses extra coarse density voxel grid.
Maintaining additional coarse feature grids or neural networks not only requires additional memory, but also increases computational burdens which leads slower convergence.
However, we propose an efficient way to draw samples proportional to their contribution in the volume rendering process.


We exploit our dense geometry feature grid $\mathcal{G}_\sigma$ for the first coarse sampling stage without allocating additional memory for coarse grid and avoiding optimizing features.
However, evaluating $\sigma$ with dense $\mathcal{G}_\sigma$ at the coarse sampled points might skip the important surface regions.
Therefore, we obtain $\sigma$ value from a wide range of density feature grid by applying a non-learnable convolution kernel $K$:
\begin{equation}
    \sigma(\mathbf{x_{\text{coarse}}}) = \mathcal{T}(\mathcal{G}_\sigma^{c}(\mathbf{x}_{\text{coarse}})) = \mathcal{T}(K * \mathcal{G}_\sigma(\mathbf{x}_{\text{coarse}})).
\end{equation}
We use average pooling kernel as $K$.
Obtaining coarse grid by convolving a kernel to dense grid is a reasonable approach since our density feature grid $\mathcal{G}_\sigma$ stores the volume density itself, which has physical meaning, not a neural latent feature.
This approach efficiently saves memory footprint and shortens the optimization time.

From the volume density values of coarse sampled points, we calculate weights of each sample by
\begin{equation}
    % w_i = \tau_i (1 - \exp{(-\sigma_i \delta_i)}, i\in [1, N_c],
    w_i = \tau_i (1 - e^{-\sigma_i \delta_i}), i\in [1, N_c],
\end{equation}
where $\delta_i$ is distance between coarse sample, $\tau_i = \exp{(-\sum_{j=1}^{i-1}{\sigma_j\delta_j})}$ is transmittance.
Then the fine $N_f$ locations are sampled from the probability distribution, which is obtained by normalizing the coarse weights, using inverse transform sampling following original NeRF.
The volume density values of fine samples are queried from $\mathcal{G}_\sigma$ as described in Eq.~\ref{eq:querying}.
Finally, the volume density $\sigma$ and color $c$ at $N_c + N_f$ samples are used to render pixel.
\fi

% \subsection{Combining Appearance and Environment Map}
\subsection{Optimization}
%\subsection{Training EgoNeRF}
\label{subsec:training}

The images of EgoNeRF are synthesized by applying the volume rendering equation along the camera ray~\cite{mildenhall2021nerf} and the optional environment map.
Specifically, the points $\mathbf{x}_i = \mathbf{o} + t_i\mathbf{d}$ along the camera ray from camera position $\mathbf{o}$ and ray direction $\mathbf{d}$ are accumulated to find the pixel value by
\begin{equation}
    % C=\sum_{i=1}^{N_c + N_f}\tau_i (1-\exp{(-\sigma_i \delta_i)}c_i, \tau_i = \exp{(-\sum_{j=1}^{i-1}\sigma_j\delta_j)}
    \hat{C}=\sum_{i=1}^{N}\tau_i (1-e^{-\sigma(\mathbf{x}_i) \delta_i})c(\mathbf{x}_i,\mathbf{d}) + \tau_{N+1}c_{\text{env}}(\mathbf{d}).
    \label{eq:rendering}
\end{equation}
% $\tau_i = e^{-\sum_{j=1}^{i-1}\sigma(\mathbf{x}_j)\delta_j}$ is the transmittance, and $\delta_i = t_{i+1}-t_i$ represents  the interval between adjacent samples. $N=N_c + N_f$ is the number of samples as described in~\cref{subsec:importance_sampling}.
$N=N_c + N_f$ is the number of samples as described in~\cref{subsec:importance_sampling}.
$\sigma(\mathbf{x})$ and $c(\mathbf{x}, \mathbf{d})$ are obtained from our balanced feature grids in~\cref{eq:querying}.
Since the size of our feature grid is exponentially increasing along the $r$ direction, we distribute $N_c$ coarse samples exponentially rather than uniformly.
%
The second term in~\cref{eq:rendering} is fetched from the environment map
\begin{equation}
    c_{\text{env}}(\mathbf{d}) = \mathcal{E}(u, v; \mathbf{d}),
\end{equation}
where the sampling position $(u,v)$ is only dependent on the viewing direction $\mathbf{d}$.
% Environment map is jointly optimized with our rendering pipeline as described in Eq.~\ref{eq:rendering}
The effect of the environment map is further discussed in~\cref{subsec:ablation}.

Finally, we optimize the photometric loss between rendered images and training images
\begin{equation}
    \mathcal{L} = \frac{1}{|\mathcal{R}|}\sum_{\mathbf{r} \in \mathcal{R}} \left\lVert \hat{C}(\mathbf{r}) - C(\mathbf{r}) \right\rVert_2^2,
\end{equation}
where $\mathcal{R}$ is a randomly sampled ray batch, $\hat{C}(\mathbf{r}), C(\mathbf{r})$ are rendered and the ground-truth color of the pixel corresponding to ray $\mathbf{r}$.
With the simple photometric loss, our feature grids $\mathcal{G}_\sigma, \mathcal{G}_a$, decoding MLP $f_{\text{MLP}}$, and environment map $\mathcal{E}$ are jointly optimized.
For real-world datasets, in which camera poses are not perfect, we additionally optimize a TV loss~\cite{rudin1994total} at our feature grid to reduce noise.
Furthermore, since our balanced feature grid guarantees a nearly uniform ray-grid hitting rate, EgoNeRF does not need a coarse-to-fine reconstruction approach for robust optimization used in other feature grid-based methods~\cite{Sun_2022_CVPR, chen2022tensorf}.

\if 0
%\paragraph{Rendering Pipeline}
To synthesize images from arbitrary viewpoints, we volume render through camera rays following NeRF~\cite{mildenhall2021nerf}.
Specifically, from camera position $\mathbf{o}$ and ray direction $\mathbf{d}$, one can obtain pixel value by:
\begin{equation}
    % C=\sum_{i=1}^{N_c + N_f}\tau_i (1-\exp{(-\sigma_i \delta_i)}c_i, \tau_i = \exp{(-\sum_{j=1}^{i-1}\sigma_j\delta_j)}
    \hat{C}=\sum_{i=0}^{N - 1}\tau_i (1-e^{-\sigma(\mathbf{x}_i) \delta_i})c_i(\mathbf{x}_i,\mathbf{d}) + \tau_{N}c_{\text{env}}(\mathbf{d}),
    \label{eq:rendering}
\end{equation}
\if 0
\begin{equation}
    \tau_i = e^{-\sum_{j=1}^{i-1}\sigma_j(\mathbf{x}_j)\delta_j}, \delta_i = t_{i+1}-t_i, \mathbf{x}_i = \mathbf{o} + t_i\mathbf{d}
\end{equation}
\fi
where transmittance $\tau_i = e^{-\sum_{j=1}^{i-1}\sigma_j(\mathbf{x}_j)\delta_j}$, interval between adjacent samples $\delta_i = t_{i+1}-t_i$, and querying position $\mathbf{x}_i = \mathbf{o} + t_i\mathbf{d}$, and number of samples $N=N_c + N_f$ which are obtained from~\cref{subsec:importance_sampling}.
$\sigma(\mathbf{x})$ and $c(\mathbf{x}, \mathbf{d})$ is obtained from our balanced feature grid in Eq.~\ref{eq:querying}.
Since the size of our feature grid is exponentially increasing along $r$ direction, we distribute $N_c$ coarse samples exponentially rather than uniformly.

%\paragraph{Environment Map}
%Although our balanced spherical feature grid is able to cover large-scale scenes with efficient memory, we use an additional environment map to be capable of modeling infinite-distant objects.
%The environment map $\mathcal{E}$ is a simple equirectangular image, or equivalent to $\mathbb{R}^{H\times 2H \times 3}$ tensor.
One can fetch color from environment map with arbitrary viewing direction $\mathbf{d}$:
\begin{equation}
    c_{\text{env}} = \text{Sample}(\mathcal{E}(u, v; \mathbf{d})),
\end{equation}
where the sampling position $(u,v)$ is only dependent to the viewing direction $\mathbf{d}$.
% Environment map is jointly optimized with our rendering pipeline as described in Eq.~\ref{eq:rendering}
The effect of environment map is further discussed in~\cref{subsec:results}.

%\paragraph{Optimization}
Finally, we optimize the photometric loss between rendered images and training images
\begin{equation}
    \mathcal{L} = \frac{1}{|\mathcal{R}|}\sum_{\mathbf{r} \in \mathcal{R}} \left\lVert \hat{C}(\mathbf{r}) - C(\mathbf{r}) \right\rVert_2^2,
\end{equation}
where $\mathcal{R}$ is a randomly sampled ray batch, $\hat{C}(\mathbf{r}), C(\mathbf{r})$ are rendered and ground-truth color of pixel corresponding to ray $\mathbf{r}$.
With the simple photometric loss, our feature grids $\mathcal{G}_\sigma, \mathcal{G}_a$, decoding MLP $f_{\text{MLP}}$, and environment map $\mathcal{E}$ are jointly optimized.
For real-world dataset, in which camera poses are not perfect, we additionally optimize a TV loss~\cite{rudin1994total} at our feature grid to reduce noise.
Furthermore, since our balanced feature grid guarantees nearly uniform ray-grid hitting rate, EgoNeRF do not need coarse-to-fine reconstruction approach for robust optimization used in other feature grid base methods~\cite{Sun_2022_CVPR, chen2022tensorf}.
\fi