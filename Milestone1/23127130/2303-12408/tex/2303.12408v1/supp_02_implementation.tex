\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{supp_figures/omniblender.pdf}
    \caption{Samples from our synthetic \textit{OmniBlender} dataset.}
    \label{fig:dataset_omniblender}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{supp_figures/ricoh360.pdf}
    \caption{Samples from our real-world \textit{Ricoh360} dataset.}
    \label{fig:dataset_ricoh}
\end{figure*}

\section{Implementation Details}
\label{sec:implementastion}

EgoNeRF is implemented with PyTorch~\cite{paszke2019pytorch} without using any customized CUDA kernels. We will release the code and the dataset publicly upon publication.

\subsection{Hyperparameter Setup}
In this section, we report the hyperparameter setup used in experiments for EgoNeRF.
We use Adam optimizer~\cite{KingmaB14} with a learning rate of 0.02 following~\cite{chen2022tensorf}, and use default values of other hyperparameters of Adam optimizer ($\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-7}$).
For all scenes, we use $300^3$ voxels for both $\mathcal{G}_\sigma$ and $\mathcal{G}_a$ with 
$N_r^y:N_\theta^y:N_\phi^y=1:\frac{2\sqrt{3}}{3}:2\sqrt{3}$.
The dimension of appearance feature $C$ is 27 and we use two-layer MLP of 128 hidden units for decoding network $f_{\text{MLP}}$.
The number of decomposed components $N_\sigma=16$ and $N_a=48$.
We use the $r_0=0.03, 0.05$, and $R_{\text{max}}=15, 300$ for the indoor and outdoor scenes, respectively.
And the size of convolution kernel $K$ for obtaining a coarse grid is 2.


\subsection{Dataset Information}
\label{subsec:data}
\paragraph{OmniBlender}


\textit{OmniBlender} is our newly introduced synthetic dataset.
OmniBlender contains outward-looking images of 11 challenging large-scale indoor/outdoor environments with various objects and materials.
We render equirectangular images along short helix camera trajectory with Blender's Cycles path tracer~\cite{blender}.
All the images have 2000$\times$1000 resolution and we use 25 images for the training set and test set respectively.
Sample images from our dataset are presented in \cref{fig:dataset_omniblender}.
We slightly modify the publicly available 3D models from various sources below:

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstdefinestyle{egonerf}{
backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen}\itshape,
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=none,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    emph={BarberShop, Classroom, ItalianFlat, Restroom, Bistro, FisherHut, LoneMonk, LOU, Pavilion},
    emphstyle=\bfseries
}
\lstset{style=egonerf}
\lstinputlisting[language=python]{license.txt}

\paragraph{Ricoh360}
\textit{Ricoh360} is our newly introduced real-world dataset.
The dataset contains short omnidirectional videos captured by rotating a commercial Ricoh Theta V camera attached to a selfie stick.
We collect 11 large-scale scenes from various indoor/outdoor environments.
After capturing the videos, we estimate camera parameters corresponding to each images by structure from motion library~\cite{moulon2016openmvg}.
We use 50 images for training set and test set respectively.
Sample images from Ricoh360 dataset are demonstrated in \cref{fig:dataset_ricoh}.