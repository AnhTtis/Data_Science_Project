
\subsection{Gradient boosted regression trees}

\label{sec:gbrt}
Gradient boosting regression trees are a particularly prominent and
empirically successful prediction method used in many applications. It starts
with regression trees and then refines them iteratively by focusing on the
errors in the previous iteration. This idea of refinement by focusing on error
correction is known as boosting. Gradually an entire forest of trees is
constructed. Then an average across the set of trees is used as the model's
prediction.

Start with a regression tree. It is the basic building block for the method. A
regression tree uses numerical cutoffs to assign an observation to a branch. In
the simplest case there is a threshold number $\overline{x}$. If the
observation has $x>\overline{x}$ the data is assigned to the upper branch.  If
the observation has $x\leq \overline{x}$ the data is assigned to the lower
branch. Then within each branch there are subsequent partitions constructed.
Eventually an entire set of branches are constructed. The final set is called a
leaf, and it defines the set that the particular observation belongs to. A
range of rules can be used to define the number of branches, their order of
consideration and so on...

\[\text{[INSERT \autoref{fig:TreeExample} AROUND HERE]}\]

Figure \ref{fig:TreeExample} shows an example of a simple tree. It predicts
expected profit using current profit, and book to market. If current
profit is greater or equal to $0.5$, the decision tree predicts the
future profitability to be $0.8$. If current profit is less than $0.5$
and log value of book to market is less than $1$, predicted future
profit is $0.1$. Otherwise, the predicted profit is $0.5$.
The tree in Figure \ref{fig:TreeExample} can also be represented as,
\begin{equation}
\hat{y} = h(x) = \sum_{m = 1}^{3} c_m I \{ (x_1,x_2) \in R_m \}
\end{equation}
where $R_m$ is the partition of the input variables, and $c_m$ is the
predicted value assigned to the terminal leaf.

In the example it takes at most two ``branches'' to reach the
final terminal leaves. This is called the number of layers of the decision
tree. The more layers the tree has, the more complex the model is.
When a regression tree model is deep (more layers), the model tends to have low
bias but larger variance. While when the model is shallow (fewer layers), the
model becomes too simple, with low variance but large bias.

Regression trees are easy to interpret, but commonly do not predict well.
Ensembles of trees have been found to predict better, see \citet{hastie2009elements}. Random forest constructs many trees using independent bootstrap samples from the data to generate a forest. Boosting creates trees to improve on past prediction mistake instead of bootstrap samples from the original data. Boosting is more complex than bootstrapping, but it tends to improve final performance, see \citet{efron2016computer}.

Gradient boosting is the most widely adopted version of boosted forests. It
starts by estimating decision trees with fixed shallow depth. Then it computes
the residuals for the trees. At the next iteration more weight is devoted to
the cases in which the model fit poorly. In the end an ensemble of trees are
used to `vote' on the appropriate results. This generally reduces the bias in a
simple tree model while maintaining the low variance. The main drawback
relative to a simple tree as in Figure \ref{fig:TreeExample}, is that forests
do not have such simple depictions that show how each variable affects the
final result.

A more formal representation is,
\begin{equation}
\hat{y} = F_M(x) = \sum_{m = 1}^{M} h_m(x)
\end{equation}
where $h_m$ is decision tree regressor with depth of $d$, and $M$ is the
number of trees in the forest. $F_M(x)$ is solved by using a greedy algorithm
framework,
\begin{equation}
F_k(x) = F_{k-1}(x) + \gamma h_k
\end{equation}
where $\gamma$ is the learning rate. The learning rate shrinks the contribution
of each additional tree. $h_k$ is the newly added tree solved by minimize a loss
function $L$ given $F_{k-1}(x)$
\begin{equation}
h_k = \arg \min_h \sum_{i }(L(y_i,F_{k-1} (x_i) + \gamma h(x_i) )
\end{equation}


There are three important hyperparameters in gradient boosting: the depth of the
tree $d$ (max$\_$depth), the number of trees in the forest $M$ (n$\_$%
estimators), and learning rate $\gamma$ (learning$\_$rate). The default
parameters have the following values: depth of the tree is $3$, number of
trees in the forest is $100$, and learning rate is $0.1$. We have
systematically carried out the analysis with the default hyperparameters as
well as with hyperparameters optimized using cross-validation. The results are
very similar. Except where noted, the Tables use the default hyperparameters.

As in the Fama-MacBeth estimation, to predict profit at time $\tau+1$,
$\hat{\pi}_{\tau+1}$, we train the model using information before $\tau+1$,
$\{X_{t-1},\pi_{t} \}_{t\leq \tau}$. We then apply the model using factors
available at time $\tau$, $X_\tau$.

Gradient boosting estimation was done using the software
GradientBoostingRegressor from
https://scikit-learn.org/stable/modules/ensemble.html, and using XGBoost from
\citet{chen2016xgboost}. The results are very similar. The reported results in
the Tables use the algorithm GradientBoostingRegressor from scikit-learn.org.
For greater detail see the algorithm \ref{alg:GBRTPredict}.


\begin{algorithm}
\caption{GBRT Predictions}
\label{alg:GBRTPredict}
\begin{algorithmic}
\Procedure{GBRT}{$X_{i,t},\pi_{i,t+1}$}\Comment{Where $t$ - time year,
$i$ - firm, $X_{i,t}$ - predictors at time $t$, $\pi_{i,t+1}$ -
profitability at $t+1$ to predict}
\For{$ 1975 \leq T \leq 2014$}
\State Training sample are observations for all firms and $ 1964 \leq t 
\leq T-1$
\State Fit the following GBRT model using training sample data
\State $\pi_{i,t+1}  = f(X_{i,t}) $
\State Get estimated model  $\hat{f}_T$
\State Use information at time $T$ $X_{i,T}$ and fitted model 
$\hat{f}_T$ to predict profitability at $T+1$
\State $\hat{\pi}_{i,T+1}  = \hat{f}_T(X_{i,T})$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}




\section{Methods to evaluate predictions}

There are many ways to evaluate predictions that have distinct justifications.
We report the results using in-sample $R^2$, out of sample $R^2$,
\citet{diebold2002comparing} t-tests, and estimation with cross-validation.

\subsection{In-sample $R^2$}

An in-sample $R^2$ is perhaps the best known method to assess the ability of a
model to account for the variation in the data. If a model is correctly
specified, then maximum likelihood estimated parameters will be optimal. As
argued by \citet{inoue2005sample} it is then appropriate to evaluate those
parameters within the sample.

Commonly we have less than full confidence that the structure of the model
being estimated is the actual true data generating process. Any model may be
reasonable, but it is almost certainly misspecified relative the true data
generating process. Accordingly, as is standard in the machine learning
literature \citep{efron2016computer}, we place much greater weight on the
out-of-sample performance as discussed in the next section.

Even if we are not confident that the model is correctly specified, an
in-sample $R^2$ is widely reported as a familiar diagnostic tool. In-sample
explanatory power is given by
\begin{equation}
R^2  =1 - \frac{\sum (\hat{\pi}_{i,t+1}  - \pi_{i,t+1} )^2}{\sum (\pi_{i,t+1} -\bar{\pi}_{i,t+1} )^2}.
\end{equation}
where $\bar{\pi}_{i,t+1}$ is the sample average profitability. Because this is in-sample, it is bounded below by 0. The
calculation steps are given in greater detail as Algorithm \ref{alg:InSampleR2}.

\begin{algorithm}
\caption{In-Sample $R^2$}
\label{alg:InSampleR2}
\begin{algorithmic}
\Procedure{IS}{$X_{i,t},\pi_{i,t+1}$}\Comment{Where $t$ - time year, 
$i$ - firm, $X_{i,t}$ - predictors at time $t$, $\pi_{i,t+1}$ - 
profitability at $t+1$ to predict}
\State Use observations such that $1975 \leq t \leq 2014$ as the whole 
sample, ($1976 \leq t+1 \leq 2015$)
\If {evaluate FM}
\ForAll{firm-year observations in the whole sample}
\State for all time period t, run cross-sectional regression for all 
firms at time t
\State $\pi_{i,t+1}  = \lambda_{0,t} + \lambda_{1,t} X_{i,t} +  
\varepsilon_{i,t}$
\State get estimated coefficients for each time period t $ 
\{\hat{\lambda}_{0,t} ,\hat{\lambda}_{1,t} \}$
\State compute the average coefficients $ \{\hat{\lambda}_{0} 
,\hat{\lambda}_{1} \}$
\EndFor
\ForAll{firm-year observations in the whole sample}
\State make prediction of profitability in the whole sample
\State $\hat{\pi}_{i,t+1}  = \hat{\lambda}_{0} + \hat{\lambda}_{1} 
X_{i,t}$
\State calculate the in-sample $R^2$ in the whole sam
\State $R^2_{IS} = 1 - \frac{\sum (\hat{\pi}_{i,t+1}  - \pi_{i,t+1} 
)^2}{\sum (\pi_{i,t+1} -\bar{\pi}_{i,t+1} )^2}$ where 
$\bar{\pi}_{i,t+1}$ is the average of profitability $\pi_{i,t+1}$ for 
the whole sample
\EndFor
\EndIf
\If {evaluate GBRT}
\ForAll{firm-year observations in the whole sample }
\State Fit the following GBRT model using the whole sample
\State $\pi_{i,t+1}  = f(X_{i,t}) $
\State Get estimated model  $\hat{f}$
\EndFor
\ForAll{firm-year observations in the whole sample}
\State Use fitted model $\hat{f}$ to predict profitability in the whole 
sample
\State $\hat{\pi}_{i,t+1}  = \hat{f}(X_{i,t})$
\State calculate the in-sample $R^2$ in the whole sam
\State $R^2_{IS} = 1 - \frac{\sum (\hat{\pi}_{i,t+1}  - \pi_{i,t+1} 
)^2}{\sum (\pi_{i,t+1} -\bar{\pi}_{i,t+1} )^2}$ where 
$\bar{\pi}_{i,t+1}$ is the average of profitability $\pi_{i,t+1}$ for 
the whole sample
\EndFor
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Out-of-sample $R^2$}
Out-of-sample explanatory power is given by
\begin{equation}
R^2  =1 - \frac{\sum (\hat{\pi}_{i,t+1}  - \pi_{i,t+1} )^2}{\sum (\pi_{i,t+1} -\pi_{i,t} )^2}.
\end{equation}
If $\hat{\pi}_{i,t+1}$ is very far from $\pi_{i,t+1}$ then the numerator may be
much larger than the denominator and so the term being subtracted may be larger
than one. So the out-of-sample $R^2$ is not bounded below by zero. When that
happens the model is clearly not doing a good job of explaining the data.

\subsection{\citet{diebold2002comparing}}

A popular and useful method of evaluating model performance is the
\citet{diebold2002comparing} t-test, see also \citet{diebold2015comparing}.
Following \citet{gu2020empirical}, the test statistics DM is calculated as
\begin{eqnarray}
DM &=& \bar{d}^{1,2}_{t}/\hat{\sigma}(\bar{d}^{1,2}_{t}), \text{where} \\
d^{1,2}_{t+1} &=& \frac{1}{n} \sum_{i=1}^{n} ((e^2_{i,t+1})^2 - (e^1_{i,t+1})^2), \notag
\end{eqnarray}
$e^1_{i,t+1}, e^2_{i,t+1}$ is the prediction error for firm $i$ profitability at time $t+1$ using each method, and $\bar{d}^{1,2}_{t}$ and $\hat{\sigma}(\bar{d}^{1,2}_{t})$ are the mean and Newey-West standard error of the time series $d^{1,2}_{t}$, respectively.

\subsection{Cross-validation}
Cross-validation is a standard machine learning method intended to reduce
overfitting, see \citep{hastie2009elements, efron2016computer, bates2021cross}.
With a panel of data there is again an issue of how to deal with the time
dimension. We also want to ensure data comparability to the out-of-sample $R^2$
calculation.

Accordingly, firm-year observations from 1975 to 2014 are used as the data. The
data is randomly partitioned into 10-folds using subsamples that are stratified
by year. This ensures proper balance of years in each fold. No other factors
were used for further stratification.

The model is estimated 10 times and then an average is reported. In the first
round, the first fold is held out for validation and the remaining 9 folds are
used for parameter estimation.  In the second round, the second fold is held
out for validation and the remaining 9 folds are used for parameter estimation.
The process continues in this way until 10 estimates have been computed.

The reported out-of-sample $R^2$ for cross-validation is the average
out-of-sample $R^2$ across all 10 estimations. The calculation steps are given
in detail as Algorithm \ref{alg:KFoldCV}.

\begin{algorithm}
\caption{K-Fold Stratified Cross Validation}
\label{alg:KFoldCV}
\begin{algorithmic}
\Procedure{CV}{$k,X_{i,t},\pi_{i,t+1}$}\Comment{Where $t$ - time year, 
$i$ - firm, $X_{i,t}$ - predictors at time $t$, $\pi_{i,t+1}$ - 
profitability at $t+1$ to predict, K-fold, stratified by time $t$}
\State $K = 10$
\State Use observations such that $1975 \leq t \leq 2014$ as the full 
sample, ($1976 \leq t+1 \leq 2015$)
\State The sample is randomly split into K groups, $\{S_j\}_{j}^K$, 
stratified by year, meaning that each subsets contains roughly the same 
proportions of observation in each year. The K-Fold Stratified 
splitting is implemented using 
\href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html}{sklearn.model\_selection.stratifiedkfold}
(scikit-learn 0.24.2).
\For{$ 1 \leq k \leq K$}
\State the training set $T_k$ is $\bigcup\limits_{j = 1, j \neq k }^{K} 
S_{j} $
\State the validation set  $V_k$ is $S_{j = k} $
\If {evaluate FM}
\ForAll{firm-year observations in the training set $T_k$}
\State for all time period t, run cross-sectional regression for all 
firms at time t
\State $\pi_{i,t+1}  = \lambda_{0,t} + \lambda_{1,t} X_{i,t} + 
\varepsilon_{i,t}$
\State get estimated coefficients for each time period t $ 
\{\hat{\lambda}_{0,t,k} ,\hat{\lambda}_{1,t,k} \}$
\State compute the average coefficients $ \{\hat{\lambda}_{0,k} 
,\hat{\lambda}_{1,k} \}$
\EndFor
\ForAll{firm-year observations in the validation set $V_k$}
\State make prediction of profitability in the validation set $V_k$
\State $\hat{\pi}_{i,t+1}  = \hat{\lambda}_{0,k} + \hat{\lambda}_{1,k} 
X_{i,t}$
\State calculate the out-of-sample $R^2$ at the validation set $V_k$
\State $R^2_{OOS,k} = 1 - \frac{\sum (\hat{\pi}_{i,t+1}  - \pi_{i,t+1} 
)^2}{\sum (\pi_{i,t+1} -\pi_{i,t} )^2}$ where ${i,t}\in V_k$
\EndFor
\EndIf
\If {evaluate GBRT}
\ForAll{firm-year observations in the training set $T_k$}
\State Fit the following GBRT model using training set $T_k$
\State $\pi_{i,t+1}  = f(X_{i,t}) $
\State Get estimated model  $\hat{f}_k$
\EndFor
\ForAll{firm-year observations in the validation set $V_k$}
\State Use fitted model $\hat{f}_k$ to predict profitability in the 
validation set $V_k$
\State $\hat{\pi}_{i,t+1}  = \hat{f}_k(X_{i,T})$
\State calculate the out-of-sample $R^2$ at the validation set $V_k$
\State $R^2_{OOS,k} = 1 - \frac{\sum (\hat{\pi}_{i,t+1}  - \pi_{i,t+1} 
)^2}{\sum (\pi_{i,t+1} -\pi_{i,t} )^2}$ where ${i,t}\in V_k$
\EndFor
\EndIf
\State Compute the average out-of-sample $R^2$ for the model being 
evaluated
\State $R^2_{\text{K-Fold}} = \frac{1}{K}\sum_{k = 1}^{K} R^2_{OOS,k}  $
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
