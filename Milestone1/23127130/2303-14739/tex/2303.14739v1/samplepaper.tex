% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage[
colorlinks,
linkcolor=blue,
anchorcolor=blue,
filecolor=blue,
urlcolor=blue,
citecolor=blue]{hyperref}

\usepackage{bm}
\usepackage{bbm}
\usepackage{amssymb}
\usepackage{marvosym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{xcolor}
\newcommand{\ZM}[1]{{\textcolor{blue}{[\textbf{ZM:} #1]}}}
\newcommand{\CJ}[1]{{\textcolor{magenta}{[\textbf{CJ:} #1]}}}
\newcommand{\ZT}[1]{{\textcolor{orange}{[\textbf{ZT:} #1]}}}

\begin{document}
%
% \title{Geometric-Aware Attenuation Field Learning for Sparse-View CBCT Reconstruction}
% \title{Sparse-View CBCT Reconstruction with Geometry-aware Attenuation Feature Field}
% \title{Sparse-View CBCT Reconstruction via Geometry-aware Attenuation Feature Field Learning}
\title{Geometry-Aware Attenuation Field Learning for Sparse-View CBCT Reconstruction}

%
% \titlerunning{Attenuation Field Learning for CBCT Reconstruction}
% \titlerunning{Sparse-View CBCT Reconstruction with Attenuation Feature Field}
\titlerunning{Geometry-Aware Attenuation Field Learning}
% https://www.overleaf.com/project/63e30203505a0f66d63dfbf2
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{
Zhentao Liu\inst{1} \and
Yu Fang\inst{1} \and
Changjian Li\inst{2} \and
Han Wu\inst{1} \and
Yuan Liu\inst{3} \and
Zhiming Cui \inst{1}\textsuperscript{(\Letter)} \and
Dinggang Shen\inst{1,4,5}\textsuperscript{(\Letter)}
% Paper ID: 542
}

%
\authorrunning{
  Z. Liu et al.
  % Paper ID: 542
}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{
% ShanghaiTech University, Shanghai, China 
School of Biomedical Engineering, ShanghaiTech Univerisity, Shanghai, China\\
\email{cuizm.neu.edu@gmail.com, dgshen@shanghaitech.edu.cn}\and
School of Informatics, The University of Edinburgh, Edinburgh, UK\and
Department of Computer Science, The University of Hong Kong, Hong Kong, China\and
Shanghai United Imaging Intelligence Co. Ltd., Shanghai, China\and
Shanghai Clinical Research and Trial Center, Shanghai, China
}
%
\maketitle           % typeset the header of the contribution
%
\begin{abstract}
% Cone Beam Computed Tomography (CBCT) is the most widely used imaging method in dentistry.
% As hundreds of X-ray projections are needed to reconstruct high quality CBCT image in traditional algorithms, sparse-view CBCT reconstruction has become a main focus to reduce radiation dose.
% Several attempts have been made to solve it while still suffer from insufficient data or ignore geometry relationships of X-ray projections.
% This work proposes a novel framework for sparse-view CBCT reconstruction, which solves sparse input problem and enjoys 3D geometry awareness across different views.
% We first adopt \emph{CNN feature extraction} to extract feature representations of input views.
% Then, we sample 3D query points on sparse voxel grid, and utilize \emph{geometry-aware feature querying} to acquire spatial consistent feature vectors from different viewpoints.
% The queried multi-view feature vectors will be aggregated together through \emph{adaptive feature fusing} considering the importance of different views.
% After all voxel points have been queried, a low-resolution 3D attenuation feature field is formed with geometry awareness, which will be decoded to desired resolution as reconstructed CBCT image through \emph{attenuation field decoding}.
% The experiments have demonstrated our method's superior reconstruction quality, which is further verified by tooth segmentation.

Cone Beam Computed Tomography (CBCT) is the most widely used imaging method in dentistry. As hundreds of X-ray projections are needed to reconstruct a high-quality CBCT image (i.e., the attenuation field) in traditional algorithms, sparse-view CBCT reconstruction has become a main focus to reduce radiation dose.
Several attempts have been made to solve it while still suffering from insufficient data or poor generalization ability for novel patients.
% \ZT{TBD}
This paper proposes a novel attenuation field \emph{encoder-decoder} framework by first \emph{encoding} the volumetric feature from multi-view X-ray projections, then \emph{decoding} it into the desired attenuation field. The key insight is when building the volumetric feature, we comply with the multi-view CBCT reconstruction nature and emphasize the view consistency property by geometry-aware spatial feature querying and adaptive feature fusing. 
Moreover, the prior knowledge information learned from data population guarantees our generalization ability when dealing with sparse view input. 
Comprehensive evaluations have demonstrated the superiority in terms of reconstruction quality, and the downstream application further validates the feasibility of our method in real-world clinics.

% \keywords{Geometry-Aware  \and Sparse-View \and CBCT Reconstruction}
\end{abstract}
%
%
%
\section{Introduction}
Cone Beam Computed Tomography (CBCT), a variant of CT scanning, is the most widely used imaging technique in dentistry since it provides 3D structure information with higher spatial resolution using shorter scanning time. 
The standard imaging scheme of CBCT is illustrated in Fig.~\ref{fig0}. 
% During CBCT scanning, an X-ray source moves around the interested area and emits cone-shaped beams, capturing multiple 2D projections. 
% CBCT reconstruction aims to solve a 3D attenuation field from such 2D projections.
During CBCT scanning, an X-ray source uniformly moves along an arc-shaped orbit and emits a cone-shaped beam towards the interested organ (e.g., oral cavity) for each angular step. And, the detector on the opposite side of the patient captures a 2D perspective projection. CBCT reconstruction aims to recover a 3D attenuation field (i.e., CBCT image) from such 2D projections in an inverse manner. This is primarily achieved based on the Filtered Back Projection (FBP) algorithm~\cite{FDK}, which, however, typically requires hundreds of projections and results in a high radiation exposure.
% Therefore, sparse-view CBCT reconstruction by reducing the number of projection views has received widespread attention in the research field.
Thus, sparse-view (e.g., 5, or 10 views) CBCT reconstruction by reducing the number of projection views has received widespread attention in the research field.

\begin{figure}[t!]
\centering
    \includegraphics[width=\textwidth]{cbctscan4.pdf}
    \vspace{-6mm}
    \caption{CBCT Imaging: (a) CBCT scanning would produce a series of (b) X-ray projections, which will be used to solve (c) a 3D CBCT image through CBCT reconstruction.} 
    \label{fig0}
    \vspace{-4mm}
\end{figure}


Sparse-view CBCT reconstruction is a challenging task due to data insufficiency.
To address this problem, many traditional methods exploited the consistency between the acquired projections and the reconstructed image.
For example, SART~\cite{SART} proposes an iterative strategy that minimizes the difference between 2D projections and their estimates while updating the 3D attenuation field.
It is effective under noisy and insufficient data, but still compromises image quality while more computationally demanding.
With the advance of deep learning, some learning-based methods are designed to learn the common knowledge from data population~\cite{bi-recon,Single-recon,X2CTGAN}, benefiting from the generalization ability of convolution networks.
% These frameworks are based on an encoder-decoder structure. Take \cite{Single-recon} for example, the 2D encoder learns the feature representations of given projections, which is then combined and transferred into 3D by reshaping, followed by a 3D decoder to generate the volumetric image.
These frameworks are based on an encoder-decoder structure, where the 2D encoder learns the feature representations of given projections, which are then combined and transferred into 3D by simply reshaping, following a 3D decoder to generate the volumetric image.
While these methods can provide decent reconstructed images, they often lack important fine details and tend to be over-smooth.
This is mainly due to the brute-force concatenation of multiple fix-posed projections that completely ignores their spatial view and geometric relationships.
Recently, another line of methods, neural rendering~\cite{NeRF}, appears to be an emerging technique for novel view synthesis, which also targets an inverse rendering problem similar to CBCT reconstruction.
However, CBCT reconstruction aims for the entire volumetric attenuation field (See Fig. \ref{fig0}), while neural rendering in computer vision society merely approximates the surface of interested objects.
NAF~\cite{NAF} first adapts this technique for sparse-view CBCT reconstruction, and leverages multi-resolution hash encoding~\cite{Instant-ngp} to achieve promising performance with only 50 projections.
It benefits from 3D geometry awareness and multi-view consistency of neural rendering, preserving fine details despite the input being less sufficient.
Unfortunately, without generalization ability, NAF relies on time-consuming per-scene optimization, typically taking tens of minutes for one subject, and does not tackle the sparse-view problem thoroughly, making it struggle to produce reliable quality when giving very limited projections (e.g., 5 or 10 views).
% Unfortunately, although generalizing from similar scenes is crucial in medical image analysis, NAF relies on time-consuming per-scene optimization, typically taking tens of minutes for one subject, and does not tackle the sparse-view problem properly, making it struggle to produce reliable quality when giving very limited projections (e.g., 5, 10 views).

In this paper, we propose a novel encoder-decoder framework of geometry-aware attenuation field learning for sparse-view CBCT reconstruction, enjoying both the generalization ability of learning-based methods and the geometry-aware view consistency of multi-view reconstruction (e.g., neural rendering).
% \ZT{How to build relationship with neural rendering? Are we independent from that?}
Specifically, we first adopt \emph{CNN feature extraction} to encode the X-ray projections.
% And then, we sample sparse 3D query points in the target space, and utilize \emph{geometry-aware feature querying} to acquire multi-view features from different projections. 
And then, when building volumetric features, we emphasize view consistency by \emph{geometry-aware feature querying} from different projections. 
Furthermore, to consider the importance of different views, we utilize \emph{adaptive feature fusing} to aggregate multi-view features into point-wise attenuation feature vectors.
In this way, a 3D attenuation feature field is formed,
% with geometry awareness, low-resolution 
and finally, decoded through \emph{attenuation field decoding} to the desired resolution of the CBCT image.
% Our framework is flexible for both input poses and the number of views, and allows learning ahead from the data population, which enables efficient high-quality reconstruction under very limited input views (e.g., 5, 10 views).
Our framework is flexible for both input poses and the number of views. And thanks to the prior knowledge learned from the data population, our method can naturally generalize to other patients without further training and efficiently produce high-quality reconstruction given very limited input views (e.g., 5 or 10 views).
Experiments have demonstrated the superior reconstruction quality of our method, which is further verified by potential downstream applications, e.g., tooth segmentation.



% Cone Beam Computed Tomography (CBCT) is a variation of traditional Computed Tomography (CT), which provides 3D structure information with higher spatial resolution and shorter scanning time. 
% It is becoming a much more popular non-invasive clinical examination, especially in dentistry. 
% During CBCT scanning, an X-ray source uniformly moves around the region of interest and emits cone-shaped X-ray beams while the detector captures 2D projections at equal angular intervals. 
% However, as hundreds of projection views are required to reconstruct high-quality CBCT image, the radiation doses of X-rays have been a long-standing problem. 
% In this paper, we aim to explore sparse-view CBCT reconstruction in order to reduce the radiation dose.

% Sparse-view CBCT reconstruction tries to decrease the number of projections while maintaining high CBCT imaging quality.
% Several attempts have been made to tackle this problem.
% The analytical FDK algorithm \cite{FDK} uses Radon transform and its inverse to estimate the attenuation coefficients.
% It can produce decent reconstruction results under enough input views but still suffer from streaking artifacts introduced by the ill-posedness of insufficient data.
% The iterative SART \cite{SART} algorithm formulates reconstruction as a minimization process paired with regularization terms and performs well in ill-posed situations but demands considerable computational time and memory.
% Nowadays, some learning-based methods have been proposed to solve the sparse-view problem by making use of the generalization ability of convolution networks. 
% Shen et al. \cite{Single-recon}, Kasten et al. \cite{bi-recon} and X2CT-GAN \cite{X2CTGAN} are representatives. They could be basically formulated into an encoder-decoder framework, where the encoder extracts information from projections and the decoder reconstructs the CT volume. 
% One of their main drawbacks is that they transform 2D projection into 3D CT via convolution rudely which ignore X-ray perspective projection geometry.
% As a result, they could only handle specific direction projection views since their specific model designs. 
% Recently, neural radiance field (NeRF \cite{NeRF}) has emerged to achieve view synthesis and geometric reconstruction tasks.
% Meanwhile, its generalizable variants such as PixelNeRF \cite{pixelnerf} and IBRNet \cite{IBRNet} are designed to utilize shared priors across scenes extracted by convolution networks under sparse input scenarios. 
% As a pioneer neural rendering work designed for CBCT reconstruction, NAF \cite{NAF} leverages neural implicit field equipped with hash encoder \cite{Instant-ngp} to achieve state-of-the-art performance in sparse-view CBCT reconstruction (50 views). 
% Its differentiable volume rendering process follows X-ray perspective projection which inspires our model design.
% However, unlike NeRF and its variants that use volume rendering to approximate objects surfaces, every voxel point in CBCT image is informative which extremely improves computational complexity. 
% Thus, NAF is still a time-consuming per-scene optimization process. 
% And it also suffers from poor image quality under very limiting input views (e.g., 10, 5 views). 

% In order to solve the ill-posedness of insufficient data and take X-ray perspective projection geometry into account, we propose a novel framework to tackle sparse-view CBCT reconstruction. 
% Our method first adopts convolutional encoding to construct a low-resolution 3D geometric-aware attenuation feature map from 2D X-ray projections as the latent representation of the target CBCT scan. 
% Then it will be decoded as the reconstructed CBCT image with the target size.
% After training across different scenes, our method allows for reconstructing CBCT images with quick speed without the need for online training like NAF, resulting in a significant reduction in computational time. 
% Due to the learned prior knowledge across scenes, our method can handle very limiting input scenarios (e.g., 10, 5 views) that previous methods always fail. 
% Besides, our model can cope with flexible direction projection views due to geometric-awareness.
% The experiments have demonstrated our method’s superiority in both performance and efficiency. 
% What’s more, our reconstructed results also perform well on the downstream task such as tooth segmentation. 

% Our main contributions can be summarized as follows: 
% 1) A novel framework to tackle the sparse-view CBCT reconstruction problem.
% 2) Our method can cope with flexible direction projection views due to geometric-awareness.
% 3) Our experiments demonstrate our superiority in performance and efficiency. And the downstream task like tooth segmentation further verifies our reconstruction quality.



\section{Method}
% The overview of our framework for sparse-view CBCT reconstruction is illustrated in Fig.~\ref{fig1}. 
% The key insight of our framework is generalizable attenuation feature field learning with geometry awareness.
% Given a set of 2D X-ray projections (Sec.~\ref{sec:preliminary}), we first adopt \emph{CNN feature extraction} to extract their feature representations, then construct the attenuation feature field by \emph{geometry-aware feature querying} and \emph{adaptive feature fusing}, and finally reconstruct the CBCT image through \emph{attenuation field decoding} (Sec.~\ref{sec:field}).
% Our framework is trained with both 3D and 2D supervision (Sec.~\ref{sec:opt}). \CJ{Can greatly simplify with only one or two sentences.}

% TBD

% Our proposed method's framework has been illustrated in Fig. \ref{fig1}. 
% Given a set of X-ray projections, we first construct a low-resolution 3D geometric-aware attenuation feature grid as the latent representation of the target CBCT scan. 
% And then it will be decoded as the reconstructed CBCT image with the target size. 
% In the following subsections, we will illustrate our proposed method in detail.



% \subsection{Preliminary}
% \label{sec:preliminary}

% \subsubsection{CBCT Scanning and Reconstruction}
% \ZT{Delete this section} \CJ{True, can remove of there is no space.} The general scheme is presented in Fig.~\ref{fig0}. During CBCT scanning, an X-ray source moves along an arc-shaped orbit around the interested area. 
% For each angular step, following perspective projection, the X-ray source emits a cone-shaped beam towards the interested organ (e.g., oral cavity) and the detector on opposite side of the patient captures a 2D projection. 
% These 2D projections are then used to compute the 3D attenuation coefficients, i.e., voxel values on the 3D CBCT image.
% Therefore, CBCT reconstruction can be formulated as an inverse problem that aims to recover a 3D attenuation field from a set of 2D X-ray projections.

% \subsubsection{Data Preparation}
% In clinical practice, paired 2D X-ray projections and 3D CBCT images are very scarce. 
% Thus, in this paper, we address this challenge by generating multiple X-ray projections from the collected CBCT image by Digitally Reconstructed Radiography (DRR) technique.
% Specifically, we perform the perspective projection uniformly on an arc-shaped orbit around the CBCT volume, and utilize Beer's Law to simulate the attenuation of X-ray during CBCT scanning.
% This enables us to train and evaluate our proposed method on a diverse and extensive dataset, which faithfully reflects the real world scenarios encountered in clinics.

\begin{figure}[t!]
\centering
    \includegraphics[width=0.98\textwidth]{pipeline.PNG}
    \caption{Illustration of our proposed method to achieve 3D CBCT image reconstruction via 2D X-ray projections.} 
    \label{fig1}
    \vspace{-4mm}
\end{figure}

% \subsection{Preliminary}
% \label{sec:preliminary}

% \subsubsection{CBCT Scanning and Reconstruction}
% During CBCT scanning, an X-ray source moves around a rotation axis following a predefined track with a constant angular marching step. 
% X-ray source would emit cone-shaped X-ray beams toward the interested organ (e.g., teeth) and the detector would capture 2D projections. 
% Each 2D X-ray image is a perspective projection. 
% Given a set of X-ray projections as input, our expected output is the volumetric attenuation field, i.e., the reconstructed CBCT image, where each voxel point on it is represented as an attenuation value. 
% In CBCT imaging, the attenuation coefficient describes the materials absorption factor of the X-ray beam, dependent on their densities. 

% \subsubsection{Data Preparation}
%  Note that, in the real clinics, paired CBCT scans and X-ray projections are rare. 
%  Thus, in our work, we use Digitally Reconstructed Radiography (DRR) to simulate X-ray projections obtained from CBCT scans.
% Assuming a casting ray $\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}$ emitted from X-ray source $\mathbf{o}$ for arbitrary pixel in any X-ray projection, it will intersect CBCT volume twice, i.e., the entry point $\mathbf{o}+t_{min}\mathbf{d}$ and exit point $\mathbf{o}+t_{max}\mathbf{d}$, where $\mathbf{d}$ denotes ray direction.
%  And then, we sample points between the entry point and exit point with uniform marching steps, e.g., half of the volume spacing. 
%  This dense uniform sampling ensures that every CBCT grid cell will be traversed and sampled.
%  Every sample point will be assigned an attenuation value by trilinear interpolation. 
%  According to Beer's Law, the X-ray attenuation process could be formulated as follows.

% \begin{equation}\label{eq1}
% I = I_0\mathrm{exp}(-\sum_{i}\mu_i\delta_i),
% \end{equation}
% where $I_0$ is the initial intensity, $\mu_{i}$ denotes sample point's attenuation value, $\delta_i$ denotes marching step, and $I$ is the simulated X-ray projection intensity. 

% \begin{figure}[t!]
% \includegraphics[width=\textwidth]{pipeline.PNG}
% \caption{Illustration of our proposed method to achieve 3D CBCT image reconstruction via 2D X-ray projections.} \label{fig1}
% \end{figure}



\subsection{Geometry-Aware Attenuation Field Learning}
\label{sec:field}

\subsubsection{CNN Feature Extraction} 
According to Fig. \ref{fig0}, the 3D attenuation field is solved with 2D X-ray projections in an inverse rendering manner.
An intuitive idea is to extract the feature representations of these projections, and use them to learn the mapping to the attenuation field.
Specifically, given $N$ projections $\{\mathbf{I}_{i}\}^N_{i=1}$, we exploit a 2D CNN encoder $\mathbf{E}$ (ResNet34~\cite{resnet} in our work) to extract 2D feature representations, denoted as $\{\mathbf{F}_i=\mathbf{E}(\mathbf{I}_i)\}^N_{i=1}$.

\subsubsection{Geometry-Aware Feature Querying} 
The key insight of our framework is the generalizable attenuation field learning with geometry awareness.
As illustrated in Fig.~\ref{fig1}, we aim to fetch the attenuation feature field in the world coordinate, by querying the 2D feature representations in the pixel coordinate. 
In this step, we sample 3D query points on a sparse voxel grid, then project each query point onto all feature representations $\{\mathbf{F}_i\}_{i=1}^N$ respectively, by utilizing their camera pose information to make a transformation between the world and pixel coordinates.
For $\mathbf{F}_i$, we denote its extrinsic camera matrix as $\mathbf{M}^{(i)}=[\mathbf{R}^{(i)}\ \mathbf{t}^{(i)}]$ and intrinsic camera matrix as $\mathbf{K}^{(i)}$, where $\mathbf{R}^{(i)}$ denotes rotation and $\mathbf{t}^{(i)}$ denotes translation. 
Then, for each 3D query point $\mathbf{x}$ in the world coordinate, we transform it into the corresponding pixel coordinate $\mathbf{x}^{(i)}_{uv}$ of $\mathbf{F}_i$ as follows:
% \CJ{not clear, check: x means the per-voxel world coordinates of the downsampled feature voxel?} \ZT{Yes}
\begin{equation}
\mathbf{x}^{(i)}_{uv} = \mathbf{K}^{(i)} \mathbf{M}^{(i)} \mathbf{x},
\end{equation}
we can then get the feature vector of $\mathbf{x}$ from $\mathbf{F}_i$ by bilinear interpolation:
\begin{equation}
    \mathbf{f}_i=\mathbf{F}_i(\mathbf{x}^{(i)}_{uv}), i=1,2,...,N
\end{equation}
where $\mathbf{f}_i \in \mathbb{R}^C$.
In this way, we can obtain spatial consistent feature vectors $\{\mathbf{f}_i\}_{i=1}^N$ of a query point $\mathbf{x}$ from all 2D feature representations $\{\mathbf{F}_i\}_{i=1}^N$.


\subsubsection{Adaptive Feature Fusing}
After acquiring multi-view feature vectors $\{\mathbf{f}_i\}_{i=1}^N$ of a query point $\mathbf{x}$ by feature querying, we aim to fuse them into an attenuation feature vector $\mathbf{f}$.
However, because of the diverse spatial positioning of query points, a specific query point $\mathbf{x}$ may receive different attenuation information from different views. 
% However, the visibility and occlusions of the projection may make different views contribute different attenuation information for each query point $\mathbf{x}$. 
% However, the visibility and occlusions of the projection may cause different views to contribute to the attenuation information of each query point $\mathbf{x}$.
Therefore, inspired by~\cite{IBRNet}, we design an adaptive feature fusing strategy to aggregate these feature vectors. 
Specifically, for $\{\mathbf{f}_i\}_{i=1}^N$ of the query point $\mathbf{x}$, we compute an element-wise mean vector $\mathbf{f}_\mu \in \mathbb{R}^C$ and variance vector $\mathbf{f}_\sigma \in \mathbb{R}^C$ to capture global information.
We consider each $\mathbf{f}_i$ as local information from the $i$-th view, and integrate it with both $\mathbf{f}_\mu$ and $\mathbf{f}_\sigma$ as global information by concatenation.
The concatenated feature is fed into the first MLP to aggregate both local and global information, producing an aggregated global-aware feature vector $\mathbf{f}^{'}_i\in \mathbb{R}^C$ and a normalized pooling weight $w_{i}\in[0,1]$ for each view, which are weighted summed together and sent into the second MLP to obtain the final fused feature $\mathbf{f} \in \mathbb{R}^C$.
Notice that the pooling weight $w_i$ can be considered as the contributing factor of the $i$-th view. 

\subsubsection{Attenuation Field Decoding}
After obtaining the attenuation feature vector $\mathbf{f}$ of each query point, we can then build the attenuation feature field by combining all query points together.
% {To ease memory overhead and accelerate computation, the attenuation feature field is built upon a low-resolution voxel grid with a downsampled size of $D/S \times\! H/S \times\! W/S \times\! C$.}
Limited by the memory size of hardware devices, we build a low-resolution attenuation feature voxel grid with a downsampled size of $D/S \times\! H/S \times\! W/S \times\! C$, which greatly accelerates computation.
This attenuation feature field can be considered as the feature representation of the target CBCT image.
Thus, we feed it into the attenuation field decoder to obtain the target volumetric attenuation field (i.e., a CBCT image) in the desired resolution $D \times\! H \times\! W$. %The decoding module is 3D implementation of SRGAN~\cite{SRGAN,3DSRGAN}.


\subsection{Model Optimization}
\label{sec:opt}
We mainly use the ground truth volumetric attenuation values to supervise the training of our framework.
We first define the reconstruction loss $L_{recon}$ as the L1 loss between ground truth $\mathbf{V}_{gt}$ and the prediction $\mathbf{V}_{pred}$ to enforce voxel-wise similarity. 
We also apply patch discrimination~\cite{CycleGAN} to improve reconstruction quality, with a least-squares adversarial loss $L_{adv}$~\cite{LSGAN}.
To recover fine-grained details, we further introduce gradient loss $L_{grad}$ as L1 loss between the first order differential of ground truth $\nabla\mathbf{V}_{gt}$ and the one of prediction $\nabla\mathbf{V}_{pred}$.
Particularly, similar to neural rendering based methods~\cite{NeRF,NAF}, we also introduce a self-supervised 2D projection loss $L_{proj}$, by minimizing the L1 loss between a random batch $\mathbf{B}$ of DRR-rendered pixels and their ground truth during the training stage.
Therefore, our final objective function is defined as:
\begin{equation}
    L = L_{recon} + \lambda_{adv}L_{adv} + \lambda_{grad}L_{grad} + \lambda_{proj}L_{proj},
\end{equation}
where $\lambda_{adv}$, $\lambda_{grad}$, and $\lambda_{proj}$ are used to control importance of different terms. 



% \subsection{Geometric-Aware Attenuation Field Learning}
% \label{sec:field}
% Our key idea is to construct a 3D geometric-aware attenuation feature map to bridge the gap between 2D X-ray projections and 3D CBCT image. 
% Every voxel point on it is assigned a geometric-aware feature vector, which is perspectively queried from different views and adaptively fused together. 
% Then it will be decoded as the reconstructed CBCT image with the target size.
% We will introduce the details in following sections.

% \subsubsection{CNN Feature Extraction} 
% According to Eq. \ref{eq1}, X-ray projection intensity is correlated with voxel points attenuation coefficients on CBCT grid that it traverses. 
% A naive idea is to leverage some kind of latent representation of X-ray projections to represent the attenuation value of CBCT voxel grid. 
% Here, we apply a 2D CNN encoder, ResNet34 \cite{resnet} in our work, to extract feature maps from $N$ number input projections $\{\mathbf{I}_{i}\}^N_i$ as latent representation.
% We denote the extracted feature maps as $\{\mathbf{F}_i=\mathbf{E}(\mathbf{I}_i)\}^N_i$.
% Convolution networks have the potential to generalize, therefore our model would not require online training like NAF, significantly reducing computation time. Additionally, the extracted shared priors across scenes would assist us in solving the sparse input problem. 

% \subsubsection{Geometric-aware Feature Querying} 
% One of our insights is that during CBCT scanning, X-ray projection formulation follows the perspective projection process which \cite{bi-recon,Single-recon,X2CTGAN} ignore. 
% Mimicking this physical process, we project the query point of CBCT voxel grid onto the receptor planar perspectively and then interpolated feature vectors from the extracted feature maps. 
% Assuming that the camera poses of each input view are known, we denote $i^{th}$ input view associated extrinsic matrix as $\mathbf{M}^{(i)}=[\mathbf{R}^{(i)}\ \mathbf{t}^{(i)}]$ and intrinsic matrix as $\mathbf{K}^{(i)}$, where $\mathbf{R}^{(i)}$ denotes rotation and $\mathbf{t}^{(i)}$ denotes translation. 
% For a query voxel point $\mathbf{x}$ in the world coordinate, we transform it into the pixel coordinate of $i^{th}$ input view as follows. 

% \begin{equation}
% \mathbf{x}^{(i)}_{uv} = \mathbf{K}^{(i)} \mathbf{M}^{(i)} \mathbf{x},
% \end{equation}
% This projection process is clearly illustrated in Fig. \ref{fig1}. 
% And then, we could get the corresponding bilinearly interpolated feature vectors from extracted feature maps $\{\mathbf{f}_i=\mathbf{F}_i(\mathbf{x}^{(i)}_{uv})\}^N_i$, and $\mathbf{f}_i \in \mathbb{R}^C$. 
% This geometric-aware feature querying enables us to handle flexible direction projection views where \cite{bi-recon,Single-recon,X2CTGAN} fail and helps us to capture X-ray projection information more correctly. 

% \subsubsection{Adaptive Feature Fusing} 
% Inspired by \cite{IBRNet}, we apply an adaptive feature fusing strategy to aggregate the interpolated feature vectors from different views as illustrated in Fig. \ref{fig1}. 
% The element-wise mean vector $\mathbf{f}_\mu \in \mathbb{R}^C$ and variance vector $\mathbf{f}_\sigma \in \mathbb{R}^C$ among feature vectors $\mathbf{f}_i$ are calculated to capture global information.
% And then $\mathbf{f}_i$ concatenated with $\mathbf{f}_\mu$ and $\mathbf{f}_\sigma$ will be sent to the first shallow MLP to aggregate both local and global information, bringing out global-aware feature vector $\mathbf{f}^{'}_i\in \mathbb{R}^C$ along with pooling weight $w_{i}\in[0,1]$. 
% For the query voxel point $\mathbf{x}$, each view may contribute different information and the pooling weight $w_i$ could be considered as the corresponding contributing factor. 
% And then, we weight sum $\mathbf{f}^{'}_i$ using $w_i$ and send it to the second shallow MLP to obtain the final aggregated feature $\mathbf{f} \in \mathbb{R}^C$ as the latent representation of the query voxel point attenuation value. To reduce computation load and accelerate sampling speed, we build a low-resolution voxel grid with a size of $D/S\times\!H/S\times\!W/S$, which is a $S$ times downsampled version of the target CBCT size $D\times\!H\times\!W$. 
% After all the voxel points on the grid have been queried, we could obtain the attenuation feature map with a size of $D/S\times\!H/S\times\!W/S\times\!C$ as the latent representation of target CBCT scan.

% \subsubsection{Attenuation Field Decoding} 
% After obtaining the attenuation feature map, we decode the feature vectors on it to our target attenuation values. 
% Note that, the resolution of attenuation feature map is $S$ times smaller than the original CBCT size. 
% Thus, we would super-resolve it to our target size during the decoding process. 
% The decoding module is the 3D implementation of SRGAN \cite{SRGAN,3DSRGAN}. 

% \subsection{Model Optimization}

% We define the reconstruction loss $L_{recon}$ as the L1 loss between ground truth CBCT image $\mathbf{V}_{gt}$ and the reconstructed one $\mathbf{V}_{pred}$ to enforce voxel-wise similarity. And apply patch discrimination \cite{CycleGAN} to pay more attention to reconstruction details. Here we adopt LSGAN loss \cite{LSGAN} to stabilize the adversarial training process which we denote as $L_{adv}$. After obtaining the reconstructed CBCT image, we could synthesize projections referring to the DRR simulation process. 
% We introduce self-supervised 2D projection loss $L_{proj}$ as geometry regularization.
% It could be defined as L1 loss between ground truth projection $I_{gt}$ and synthesized projection $I_{pred}$.

% \begin{equation}
% L_{proj}= \sum_{\mathbf{r}\in\mathbf{B}}\Vert I_{gt}(\mathbf{r}) - I_{pred}(\mathbf{r}) \Vert_1,
% \end{equation}
% where $\mathbf{B}$ denotes the set of ray batch. In order to recover more sharp details, we further introduce gradient loss $L_{grad}$ as L1 loss between the first order differential of ground truth CBCT image $\nabla\mathbf{V}_{gt}$ and the reconstructed one $\nabla\mathbf{V}_{pred}$.

% Combining the above loss functions, our final total objective function could be defined as follows.

% \begin{equation}
%     L = L_{recon} + \lambda_{adv}L_{adv} + \lambda_{proj}L_{proj} + L_{grad}
% \end{equation}
% where $\lambda_{adv}$, $\lambda_{proj}$ are used to control relative importance of adversarial loss and projection loss. 



\section{Experiments}
\subsection{Experimental Settings}
\subsubsection{Dataset} 
In clinical practice, paired 2D X-ray projections and 3D CBCT images are very scarce. 
Thus, we address this challenge by generating multiple X-ray projections from the collected CBCT image using the Digitally Reconstructed Radiography (DRR) technique, i.e., we conduct the process as shown in Fig. \ref{fig0}
% perspective projection uniformly on an arc-shaped orbit around the CBCT volume, 
and utilize Beer's Law to simulate the attenuation of X-ray during scanning.
In total, our dataset consists of 130 dental CBCT images of different patients with a resolution of $256\times256\times256$.
We split it into 100 images for training, 10 for validating, and 20 for testing.
For each CBCT image, the corresponding X-ray projections are produced as described above.
% Notice that our framework is flexible for both input poses and the number of views.
In our experiments, we generate $N$ X-ray projections for every $360/N$ degree around the center of the CBCT image, where each X-ray projection has a resolution of $256\times256$.
We select $N=5,~10,~\text{and}~20$ throughout this paper. 
%\CJ{Importance view sampling? Why average view angles?}


\subsubsection{Implementation Details} 
In experiments, we empirically set $\lambda_{adv}=0.001$, $\lambda_{proj}=0.01$, $\lambda_{grad}=1$, the downsampling rate $S=4$, the channel size $C=256$, and the DRR ray batch size $|\mathbf{B}|=1024$. 
We use Adam optimizer with a learning rate of 1$\times10^{-4}$, which decays by 0.5 for every 50 epochs, and the training process ends after 150 epochs.
The decoder and discriminator are 3D implementation of SRGAN \cite{SRGAN,3DSRGAN}.
All experiments are conducted on a single A100 GPU.


\subsubsection{Comparison Methods and Evaluation Metrics} 
Our proposed framework is compared with four typical methods, i.e., FBP, SART, NAF, and PixelNeRF.
FBP~\cite{FDK} is the classical CBCT reconstruction algorithm widely used in industries.
SART~\cite{SART} is a traditional algorithm that addresses the sparse-view problem by iterative minimization and regularization.
NAF~\cite{NAF} provides the state-of-the-art performance of CBCT reconstruction with per-scene optimization, based on the adaptation of neural rendering~\cite{NeRF} and multi-resolution hash encoding~\cite{Instant-ngp}.
As neural rendering aims to solve inverse rendering problem that is highly related to our work, we also compare our method with PixelNeRF~\cite{pixelnerf}, a representative framework in computer vision society to tackle the sparse-view problem by leveraging the generalization ability of CNNs.
Notably, we do not compare with~\cite{bi-recon,Single-recon,X2CTGAN}, since they cannot handle flexible input poses nor the number of views.
We utilize two commonly used metrics to evaluate the reconstruction performance, i.e., Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM)~\cite{SSIM}.
We also report the reconstruction time to measure the efficiency of different methods.


% \section{Experiments}

% \subsection{Experimental Settings}

% \subsubsection{Data} 
% We gather 130 dental CBCT images with a resolution of 256$\times$256$\times$256, of which 100 serve as training samples, 10 serve as validation samples, and the remaining 20 serve as testing samples.
% For each CBCT image, we simulate X-ray projections via DRR following the standard CBCT scanning procedure that we have mentioned before.
% Take 20 views input scenario for example, the X-ray source moves with an angular step $18^{\circ}$ within the range of $[0,360^{\circ})$. For each angular step, it will produce a X-ray projection, resulting in 20 projections totally.
% Each X-ray projection has a resolution of 256$\times$256.
% 10 views and 5 views configurations could be obtained similarly with different angular marching step 
% $36^{\circ}$, $72^{\circ}$ respectively.

% \subsubsection{Comparison Methods and Metrics} 
% We compare our model with three baseline methods. 
% FDK: the classical analytical algorithm. 
% NAF: the state-of-the-art neural rendering work designed for CBCT reconstruction. 
% PixelNeRF: the representative generalizable neural rendering method. 
% Here we do not add \cite{bi-recon,Single-recon,X2CTGAN} into comparison due to their model designs could not handle the input views settings in our experiments. 
% We compute Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) \cite{SSIM} to evaluate the reconstruction performance and count the computation time to evaluate the reconstruction efficiency. 

% \subsubsection{Implementation Details} 
% In our experiment, we empirically set $\lambda_{adv}=0.001$, $\lambda_{proj}=0.01$, the downsampling rate $S=4$, the ray batch size $|\mathbf{B}|=1024$. 
% We use Adam optimizer with an initial learning rate 1$\times10^{-4}$ and decay 0.5 times every 50 epochs, and the training process will end after 150 epochs. 
% More network details could be found in supplementary materials. 
% All experiments are conducted on a single A100 GPU.

%  \begin{table}[t]
% \centering
% \scriptsize
% \caption{Quantitative comparison with different methods.}
% \label{tab1}
% \setlength{\tabcolsep}{2pt}
% \renewcommand\arraystretch{1.6}
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{2}{*}{Method} & \multicolumn{3}{|c|}{5 views} & \multicolumn{3}{|c|}{10 views} & \multicolumn{3}{|c}{20 views} \\ \cline{2-10}
%                         & PSNR & SSIM & Time(s) & PSNR & SSIM & Time(s) & PSNR & SSIM & Time(s)  \\  \hline
% % FBP \cite{FDK}	& 13.24 & 0.23 & \textbf{0.14} & 17.58 & 0.37 & \textbf{0.17} & 20.94 & 0.51 & \textbf{0.18}   \\  \hline
% % NAF \cite{NAF}	 & 22.39 & 0.73 & 306.68 & 25.08 & 0.81 & 538.31 & 28.68 & 0.88 & 1182.01   \\  \hline
% % PixelNeRF \cite{pixelnerf}	& 22.12 & 0.77 & 5.38 & 24.03 & 0.82 & 7.84 & 26.85 & 0.87 & 12.77  \\   \hline
% % Ours	& \textbf{27.38} & \textbf{0.89} & 0.79 & \textbf{28.81} & \textbf{0.92} & 0.86 & \textbf{31.34} & \textbf{0.94} & 0.98  \\   \hline
% % FBP \cite{FDK}	& 13.2383 & 0.2308 & \textbf{0.1412} & 17.5791 & 0.3704 & \textbf{0.1695} & 20.9406 & 0.5107 & \textbf{0.1847}   \\  \hline
% % NAF \cite{NAF}	 & 22.3876 & 0.7317 & 306.68 & 25.0835 & 0.8054 & 538.31 & 28.6837 & 0.8758 & 1182.01   \\  \hline
% % PixelNeRF \cite{pixelnerf}	& 22.1231 & 0.7674 & 5.3797 & 24.0323 & 0.8217 & 7.8385 & 26.8490 & 0.8740 & 12.7668  \\   \hline
% % Ours	& \textbf{27.3768} & \textbf{0.8937} & 0.7933 & \textbf{28.8100} & \textbf{0.9154} & 0.8640 & \textbf{31.3428} & \textbf{0.9429} & 0.9786  \\   \hline
% % FBP \cite{FDK}	& 13.2383 & 0.2308 & \textbf{0.14} & 17.5791 & 0.3704 & \textbf{0.17} & 20.9406 & 0.5107 & \textbf{0.18}   \\  \hline
% % SART \cite{SART} & 20.6180 & 0.6704 & 25.72 & 24.6664 & 0.7973 & 54.76 & 27.5152 & 0.8680 & 114.95  \\  \hline
% % NAF \cite{NAF}	 & 22.3876 & 0.7317 & 306.68 & 25.0835 & 0.8054 & 538.31 & 28.6837 & 0.8758 & 1182.01   \\  \hline
% % PixelNeRF \cite{pixelnerf}	& 22.1231 & 0.7674 & 5.38 & 24.0323 & 0.8217 & 7.84 & 26.8490 & 0.8740 & 12.77  \\   \hline
% % Ours	& \textbf{27.3768} & \textbf{0.8937} & 0.79 & \textbf{28.8100} & \textbf{0.9154} & 0.86 & \textbf{31.3428} & \textbf{0.9429} & 0.98  \\   \hline
% FBP \cite{FDK}	& 13.24 & 0.23 & \textbf{0.14} & 17.58 & 0.37 & \textbf{0.17} & 20.94 & 0.51 & \textbf{0.18}   \\  \hline
% SART \cite{SART} & 20.62 & 0.67 & 25.72 & 24.67 & 0.80 & 54.76 & 27.52 & 0.87 & 114.95  \\  \hline
% NAF \cite{NAF}	 & 22.39 & 0.73 & 306.68 & 25.08 & 0.80 & 538.31 & 28.68 & 0.88 & 1182.01   \\  \hline
% PixelNeRF \cite{pixelnerf}	& 22.12 & 0.77 & 5.38 & 24.03 & 0.82 & 7.84 & 26.85 & 0.87 & 12.77  \\   \hline
% Ours	& \textbf{27.38} & \textbf{0.89} & 0.79 & \textbf{28.81} & \textbf{0.92} & 0.86 & \textbf{31.34} & \textbf{0.94} & 0.98  \\   \hline
% \end{tabular}
% \end{table}


\begin{table}[t]
\centering
\caption{Quantitative comparison with different methods.}
\label{tab1}
\setlength{\tabcolsep}{1.3pt}
\renewcommand\arraystretch{1.1}
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{5 views}                                          & \multicolumn{3}{c|}{10 views}                                         & \multicolumn{3}{c}{20 views}                                          \\ \cline{2-10} 
                        & \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{SSIM} & Time(s)       & \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{SSIM} & Time(s)       & \multicolumn{1}{c}{PSNR} & \multicolumn{1}{c}{SSIM} & Time(s)       \\ \hline
FBP~\cite{FDK}                    & 13.24                     & 0.23                      & \textbf{0.14} & 17.58                     & 0.37                      & \textbf{0.17} & 20.94                     & 0.51                      & \textbf{0.18} \\
SART~\cite{SART}                    & 20.62                     & 0.67                      & 25.72         & 24.67                     & 0.80                      & 54.76         & 27.52                     & 0.87                      & 114.95        \\
NAF~\cite{NAF}                     & 22.39                     & 0.73                      & 306.68        & 25.08                     & 0.80                      & 538.31        & 28.68                     & 0.88                      & 1182.01       \\
PixelNeRF~\cite{pixelnerf}               & 22.12                     & 0.77                      & 5.38          & 24.03                     & 0.82                      & 7.84          & 26.85                     & 0.87                      & 12.77         \\ \hline
Ours                    & \textbf{27.38}            & \textbf{0.89}             & 0.79          & \textbf{28.81}            & \textbf{0.92}             & 0.86          & \textbf{31.34}            & \textbf{0.94}             & 0.98          \\ \hline
\end{tabular}
\end{table}

\begin{figure}[t!]
\centering
    \includegraphics[width=0.97\textwidth]{ReconResult-5method.pdf}
    \caption{Qualitative comparison with different methods.} 
    \label{fig2}
    \vspace{-4mm}
\end{figure}


\subsection{Results}
\subsubsection{Quantitative and Qualitative Results}
Tab.~\ref{tab1} presents the quantitative comparisons of different methods.
Our method outperforms all other methods by a significant margin in both PSNR and SSIM.
Notably, our method achieves the highest performance with only 5 input views (27.38 dB of PSNR), and a PSNR above 30 dB for 20 input views, dramatically surpassing the current state-of-the-art method (i.e., NAF).
% \ZT{Notably, our method with 5 views outperforms SART and NAF with 20 views in visual quality but not in PSNR metric. One possible reason could be that our method with 5 views produces a little smooth reconstruction result which lacks fine-grained details and noises in the original CBCT image. While PSNR pursues voxel-wise similarity and mistakes the noises in SART and NAF as details which indicates it deviates from human vision perception and is not a good visual quality metric. SSIM keeps a relatively correct forecast trend and is in line with intuitive feeling of human eye.}
Moreover, our reconstruction time is less than a second, which is much faster than other sparse-view methods (i.e., SART, NAF, and PixelNeRF).
% Moreover, our reconstruction time is less than a second, which is at least an order of magnitude faster than other sparse-view methods (i.e., SART, NAF, and PixelNeRF).
% Because NAF suffers from per-scene optimization for tens of minutes, 
SART and NAF suffer from time-consuming iterative computation and per-scene optimization respectively,
% And we benefit from low-resolution attenuation feature field for feature querying when comparing to PixelNeRF.
and we benefit from low-resolution feature querying compared to PixelNeRF.

In Fig.~\ref{fig2}, we present a visual comparison of the reconstructed 3D CBCT images in axial slices.
It can be seen that, FBP fails to handle sparse-view input, leading to severe streaking artifacts due to insufficient views.
SART can significantly improve the quality by reducing those artifacts, but that trades off the important fine details.
NAF is capable of high-quality reconstruction by adapting neural rendering with hash encoder.
However, its quality drops drastically with extremely limited number of input views (e.g., 5, 10 views), as it is a per-scene optimized method without generalizable ability learned from a diverse data population.
PixelNeRF also enjoys generalizable ability as we do, but it lacks volumetric supervision to ensure 3D quality, which is the crucial difference between medical imaging and natural scene.
% PixelNeRF shares similar strategies with our method, but it lacks volumetric supervision to ensure 3D quality, which is the crucial difference between medical imaging and natural scene.
Notably, our method outperforms all other methods and is the only one that provides comparable quality with the ground truth, even with only 5 input views.

 \begin{table}[t!]
\centering
\caption{Ablation study on different components.}
\label{tab2}
\setlength{\tabcolsep}{2.6pt}
\renewcommand\arraystretch{1.15}
\begin{tabular}{cccc|cc|cc|cc}
\hline
\multicolumn{4}{c|}{Components}                                           & \multicolumn{2}{c|}{5 views}              & \multicolumn{2}{c|}{10 views}             & \multicolumn{2}{c}{20 views}              \\ \hline
\multicolumn{1}{l}{Baseline} & ada.         &$L_{proj}$  &$L_{grad}$         & \multicolumn{1}{c}{PSNR} & SSIM          & \multicolumn{1}{c}{PSNR} & SSIM          & \multicolumn{1}{c}{PSNR} & SSIM          \\ \hline
$\checkmark$                 &              &              & \textbf{}    & 26.47                     & 0.88          & 27.86                     & 0.90          & 29.97                     & 0.93          \\
$\checkmark$                 & $\checkmark$ &              &              & 26.98                     & 0.89          & 28.57                     & 0.91          & 30.85                     & 0.94          \\
$\checkmark$                 & $\checkmark$ & $\checkmark$ &              & 27.18                     & 0.89          & 28.70                     & 0.91          & 31.02                     & 0.94          \\ \hline
$\checkmark$                 & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{27.38}            & \textbf{0.89} & \textbf{28.81}            & \textbf{0.92} & \textbf{31.34}            & \textbf{0.94} \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}


% \begin{table}[t!]
% \centering
% \caption{Ablation study on different components.}
% \label{tab2}
% \setlength{\tabcolsep}{2.6pt}
% \renewcommand\arraystretch{1.15}
% \begin{tabular}{cccc|cc|cc|cc}
% \hline
% \multicolumn{4}{c|}{Components}                                           & \multicolumn{2}{c|}{5 views}   & \multicolumn{2}{c|}{10 views}  & \multicolumn{2}{c}{20 views}   \\ \hline
% \multicolumn{1}{l}{Baseline} & ada.         & proj         & grad         & PSNR           & SSIM          & PSNR           & SSIM          & PSNR           & SSIM          \\ \hline
% $\checkmark$                 &              &              & \textbf{}    & 26.47          & 0.88          & 27.86          & 0.90          & 29.97          & 0.93          \\
% $\checkmark$                 & $\checkmark$ &              &              & 26.98          & 0.89          & 28.57          & 0.91          & 30.85          & 0.94          \\
% $\checkmark$                 & $\checkmark$ & $\checkmark$ &              & 27.18          & 0.89          & 28.70          & 0.91          & 31.02          & 0.94          \\
% $\checkmark$                 & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{27.38} & \textbf{0.89} & \textbf{28.81} & \textbf{0.92} & \textbf{31.34} & \textbf{0.94} \\ \hline
% \end{tabular}
% \vspace{-4mm}
% \end{table}



\subsubsection{Ablation Study}
The quantitative ablation study results are listed in Table \ref{tab2}, where the baseline model simply makes use of average pooling to aggregate feature vectors from different input views and only adopts reconstruction and adversarial losses during training. 
Every $\checkmark$ in the table means adding the corresponding component into the baseline model as a new alternative solution. 
Note here, 'ada.' in the table means adaptive feature fusing strategy.
% All the variants are computed around 1 second.
As can be seen, compared with other SOTA methods, our baseline already performs best. For example, in terms of PSNR, we gain remarkable $4.35$ dB, $3.83$ dB, and $3.12$ dB improvement for 5, 10, and 20 views against PixelNeRF. Because our strong baseline has equipped with geometry-aware view consistency in feature learning and voxel-wised supervision in training, which lay the foundation and are the keys to the success of our method. 
Furthermore, with the addition of other components, the PSNR and SSIM values increase gradually, which demonstrates the effectiveness of our technical designs. 
For example, adaptive feature fusing provides a more flexible and accurate integration of information from different views than average pooling, thus brings a relatively considerable improvement.
While the projection loss boosts geometry-aware view consistency and the gradient loss enhances the sharpness, respectively.

\subsubsection{Per-Scene Fine-Tuning}
After across-scenes training, our model could provide a decent reconstructed CBCT image from sparse X-ray views.
We could further fine-tune our result scene-wise by only making use of projection loss with the same input views. 
The PSNR value of reconstructed results would improve by 0.66 dB, 0.74 dB, and 0.75 dB on average for 5, 10, and 20 views after optimizing for about 4-15 minutes. 
% 4 minutes for 5 views, 8 minutes for 10 views, 15 minutes for 20 views.

\subsubsection{Application}
We evaluate the quality of the reconstructed CBCT images by performing tooth segmentation as a downstream application. 
We first get expert manual annotations for each CBCT image in our testing set and use the pre-trained SOTA network~\cite{tooth-seg} to segment teeth. As a reference, the average Dice score of the teeth segmentation from ground truth CBCT images (compared with human annotation) in our testing set is $0.94$. Thus, whichever method whose Dice score is closer to this value achieves a higher reconstruction quality.
In Fig.~\ref{fig3}, we report the Dice scores (mean and standard deviation) of all methods tested on CBCT images reconstructed with different numbers of input views.
Note that, we omit some results in the figure (e.g., those from NAF with 5 and 10 views) that failed for tooth segmentation (i.e., Dice score less than 0.6) to ensure clear comparison. 
As can be seen, our method clearly outperforms all competitors indicating the usability and superior image quality of our reconstruction.
% Obviously, the Dice score is positively correlated with reconstruction quality, and our method consistently achieves the best and most stable performance.
Furthermore, we provide a visual example in Fig.~\ref{fig4}, where our Dice scores are $0.88$, $0.90$, and $0.92$ for 5, 10, and 20 views, against $0.95$ of the ground truth CBCT segmentation for this specific case. 
Although our Dice scores are not as high as the ground truth, they are comparable indicating the great potential of our method for downstream applications and real clinical use. 
% highlighting our potential for downstream applications and real clinical use.
% \CJ{better manually label all the results?} \ZT{Note: Dice score of GT Segmentation is 0.95 in Fig. 5 case, and 0.94 on average over test set. We are very close to it.}

% \subsection{Results}
% \subsubsection{Quantitative and Qualitative Results}
% Table \ref{tab1} lists the quantitative comparison results of four competing methods.
% Both the PSNR and SSIM values of our method are significantly higher than other methods in each input view scenario. 
% What's more, our method only takes around 1 second to finish reconstruction, which is much faster than NAF and PixelNeRF. 
% Fig. \ref{fig2} give a visual example of the reconstructed results of four competing methods. 
% Our method visually outperforms other methods by a large margin and achieves excellent reconstruction results which look very closely to the ground truth even under only 5 input views. 

% Obviously, FBP suffers from severe streaky artifacts due to the ill-posedness of insufficient data. 
%  NAF is capable of learning a comprehensive scene representation with the help of hash encoding. 
%  However, it also fails to reconstruct a high-quality CBCT image under very limiting input views (e.g., 10, 5 views) due to data limitation and under-explored hash encoding for CBCT reconstruction. 
%  What's more, It's a time-consuming per-scene optimization process which leads to a much longer training time. 
%  PixelNeRF also makes use of shared priors across scenes which may improve performance and does not need online training like NAF. 
%  However, only supervised by projection loss also leads to poor image quality like streaky artifacts. 
%  And it needs to query every voxel point on the target CBCT size voxel grid. 
%  While our method only needs to query voxel points on a low-resolution voxel grid which is 4 times smaller than the original size, resulting in a much faster sampling and computation speed. 

% \subsubsection{Ablation Study} 
% The quantitative ablation study results have been listed in Table \ref{tab2}. 
% The baseline model makes use of average pooling to aggregate different views image feature vectors, and only adopts reconstruction loss and adversarial loss during training. 
% Every $\checkmark$ in the table means adding the corresponding component into the baseline model and making up a new comparison counterpart.
% 'ada.' in the table means adaptive feature fusing strategy.
% All the variants are computed around 1 second. 
% As illustrated in Table \ref{tab2}, with the addition of different components, the PSNR and SSIM values increase gradually, which demonstrates the effectivity of our loss terms and adaptive feature fusing strategy.
% The projection loss term and the gradient loss term improves geometric accuracy and reconstruction sharpness respectively.
% While adaptive feature fusing provides a more flexible and adequate integration of information from different views than average pooling. 

% \subsubsection{Per-scene Fine-tuning}
% After across-scenes training, our model could provide a decent reconstructed CBCT image from sparse X-ray views.
% We could further fine-tune our result scene-wise by only making use of projection loss with the same input views. 
% Take 10 input views scenario for example, the PSNR value of reconstructed results would improve by 0.7440 dB on average after optimizing for about 8 minutes. 

% \subsubsection{Downstream Application} 
% We compare the tooth segmentation \cite{tooth-seg} outcomes between the original CBCT images and the reconstructed CBCT images to validate our potential for downstream applications.
% We calculate the DICE coefficient between the original CBCT image segmentation outcomes and manual labels as $DICE_{ori}$ and calculate the DICE coefficient between the reconstructed CBCT image segmentation outcomes and manual labels as $DICE_{pred}$. 
% It indicates that the reconstructed CBCT image is similar to the originals if $DICE_{pred}$ matches with $DICE_{ori}$. 
% The statistic is illustrated in Fig \ref{fig3}. 
% The performance increases with the rise of input views. 
% Our method achieves better segmentation results than others under all input view scenarios which means our reconstructed CBCT images are most closely to the originals. 
% Note that, FBP under 5 input views, NAF, and PixelNeRF under 5, 10 input views have all failed for tooth segmentation task (i.e., $DICE_{pred}<0.5$), thus we do not draw the corresponding data in Fig. \ref{fig3}.
% Besides, we provide a visualization example of tooth segmentation in Fig. \ref{fig4}. 
% In this case, $DICE_{ori}=0.9475$ and $DICE_{pred}=0.9308$. 
% Most of the teeth can be segmented accurately, only parts of the molar teeth roots are imperfect.



% \begin{table}[t!]
% \centering
% \scriptsize
% \caption{Ablation study on different components.}
% \label{tab2}
% \setlength{\tabcolsep}{2.5pt}
% \renewcommand\arraystretch{1.6}
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{2}{*}{Method}& \multicolumn{3}{|c|}{Components} & \multicolumn{2}{|c|}{5 views} & \multicolumn{2}{|c|}{10 views} & \multicolumn{2}{|c}{20 views} \\ \cline{2-10}
%                         &$L_{proj}$ & $L_{grad}$ &ada. & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM   \\  \hline
% Baseline	&             &             &             & 27.0523 & 0.8876 & 28.5877 & 0.9125 & 31.0354 & 0.9393 \\  \hline
% Compare1	&$\checkmark$ &             &             & 27.2121 & 0.8903 & 28.6212 & 0.9125 & 31.1822 & 0.9409 \\  \hline
% Compare2 	&$\checkmark$ &$\checkmark$ &             & 27.3301 & 0.8933 & 28.7749 & 0.9150 & 31.2137 & 0.9413 \\  \hline
% Ours	    &$\checkmark$ &$\checkmark$ &$\checkmark$ & \textbf{27.3768} & \textbf{0.8937}  & \textbf{28.8100} & \textbf{0.9154}  & \textbf{31.3428} & \textbf{0.9429} \\  \hline
% \end{tabular}
% \end{table}

% \begin{table}[t!]
% \centering
% \scriptsize
% \caption{Ablation study on different components.}
% \label{tab2}
% \setlength{\tabcolsep}{2.5pt}
% \renewcommand\arraystretch{1.6}
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{2}{*}{Method}& \multicolumn{3}{|c|}{Components} & \multicolumn{2}{|c|}{5 views} & \multicolumn{2}{|c|}{10 views} & \multicolumn{2}{|c}{20 views} \\ \cline{2-10}
%                         &ada. &$L_{proj}$  &$L_{grad}$ & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM   \\  \hline
% Baseline	&             &             &             & 27.05 & 0.89 & 28.59 & 0.91 & 31.04 & 0.94 \\  \hline
% Compare1	&$\checkmark$ &             &             &  &  &  &  & 30.95  & 0.94  \\  \hline
% Compare2 	&$\checkmark$ &$\checkmark$ &             &  &  &  &  &  &  \\  \hline
% Ours	    &$\checkmark$ &$\checkmark$ &$\checkmark$ & \textbf{27.38} & \textbf{0.89}  & \textbf{28.81} & \textbf{0.92}  & \textbf{31.34} & \textbf{0.94} \\  \hline
% \end{tabular}
% \end{table}




\begin{figure}[t!]
	\centering
	\begin{minipage}[t]{0.41\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{DICE3.pdf}
        \vspace{-3mm}
		\caption{Quantitative comparison of tooth segmentation performance.}
        \label{fig3}
	\end{minipage}
        \hspace{.1in}
	\begin{minipage}[t]{0.41\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{segment5.PNG}
        \vspace{-3mm}
		\caption{Qualitative results of tooth segmentation.}
        \label{fig4}
	\end{minipage}
 \vspace{-4mm}
\end{figure}

\section{Conclusion}
% This paper proposes a novel framework of geometry-aware attenuation feature field learning for sparse-view CBCT reconstruction, which enjoys both the generalization ability across data population and 3D geometry awareness across different views.
% Our model is flexible for both input poses and the number of views due to geometry awareness. 
% And generalization ability enables efficient high-quality reconstruction under sparse input views.
% Our approach has achieved good reconstruction quality, and tooth segmentation further verifies our superiority. 

% The feature querying process obey perspective projection geometric constraint and our adaptive feature fusing strategy further improves the representation ability. Besides, the low-resolution sampling reduces computation load, and increase computation speed. 

This paper proposes a novel attenuation field learning framework for sparse-view CBCT reconstruction. By learning the prior knowledge from the data population, our method can generalize to other patients without further training and efficiently solve the sparse-view input problem with high-quality reconstructed CBCT images. Extensive evaluations validate our superiority and the downstream application demonstrates the applicability of our method in real-world clinics.

%
% ---- Bibliography ---- 
\bibliographystyle{splncs04}  
% \bibliography{reference}  
\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{SART}
Andersen, A., Kak, A.: Simultaneous algebraic reconstruction technique (sart):
  A superior implementation of the art algorithm. Ultrasonic imaging
  \textbf{6},  81--94 (02 1984). \doi{10.1016/0161-7346(84)90008-7}

\bibitem{tooth-seg}
Cui, Z., Fang, Y., Mei, L., Zhang, B., Yu, B., Liu, J., Jiang, C., Sun, Y., Ma,
  L., Jiawei, H., Liu, Y., Zhao, Y., Lian, C., Ding, Z., Zhu, M.: A fully
  automatic ai system for tooth and alveolar bone segmentation from cone-beam
  ct images. Nature Communications  \textbf{13}, ~2096 (04 2022).
  \doi{10.1038/s41467-022-29637-2}

\bibitem{FDK}
Feldkamp, L., Davis, L.C., Kress, J.: Practical cone-beam algorithm. J. Opt.
  Soc. Am  \textbf{1},  612--619 (01 1984)

\bibitem{resnet}
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
  recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) pp. 770--778 (2015)

\bibitem{bi-recon}
Kasten, Y., Doktofsky, D., Kovler, I.: End-To-End Convolutional Neural Network
  for 3D Reconstruction of Knee Bones from Bi-planar X-Ray Images, pp. 123--133
  (10 2020). \doi{10.1007/978-3-030-61598-7_12}

\bibitem{SRGAN}
Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A.,
  Aitken, A., Tejani, A., Totz, J., Wang, Z., Shi, W.: Photo-realistic single
  image super-resolution using a generative adversarial network. In: 2017 IEEE
  Conference on Computer Vision and Pattern Recognition (CVPR). pp. 105--114
  (2017). \doi{10.1109/CVPR.2017.19}

\bibitem{LSGAN}
Mao, X., Li, Q., Xie, H., Lau, R.Y., Wang, Z., Smolley, S.P.: Least squares
  generative adversarial networks. In: 2017 IEEE International Conference on
  Computer Vision (ICCV). pp. 2813--2821 (2017). \doi{10.1109/ICCV.2017.304}

\bibitem{NeRF}
Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R.,
  Ng, R.: Nerf: Representing scenes as neural radiance fields for view
  synthesis. In: European Conference on Computer Vision (2020)

\bibitem{Instant-ngp}
M{\"u}ller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics
  primitives with a multiresolution hash encoding. ACM Transactions on Graphics
  (TOG)  \textbf{41},  1 -- 15 (2022)

\bibitem{3DSRGAN}
Sanchez, I., Vilaplana, V.: Brain mri super-resolution using 3d generative
  adversarial networks (04 2018)

\bibitem{Single-recon}
Shen, L., Zhao, W., Xing, L.: Patient-specific reconstruction of volumetric
  computed tomography images from a single projection view via deep learning.
  Nature biomedical engineering  \textbf{3}(11),  880—888 (November 2019).
  \doi{10.1038/s41551-019-0466-4}

\bibitem{IBRNet}
Wang, Q., Wang, Z., Genova, K., Srinivasan, P., Zhou, H., Barron, J.T.,
  Martin-Brualla, R., Snavely, N., Funkhouser, T.: Ibrnet: Learning multi-view
  image-based rendering. In: 2021 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR). pp. 4688--4697 (2021).
  \doi{10.1109/CVPR46437.2021.00466}

\bibitem{SSIM}
Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: from
  error visibility to structural similarity. IEEE Transactions on Image
  Processing  \textbf{13}(4),  600--612 (2004). \doi{10.1109/TIP.2003.819861}

\bibitem{X2CTGAN}
Ying, X., Guo, H., Ma, K., Wu, J., Weng, Z., Zheng, Y.: X2ct-gan:
  Reconstructing ct from biplanar x-rays with generative adversarial networks.
  In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR). pp. 10611--10620 (2019). \doi{10.1109/CVPR.2019.01087}

\bibitem{pixelnerf}
Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields
  from one or few images. 2021 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR) pp. 4576--4585 (2020)

\bibitem{NAF}
Zha, R., Zhang, Y., Li, H.: NAF: Neural Attenuation Fields for Sparse-View CBCT
  Reconstruction, pp. 442--452 (09 2022). \doi{10.1007/978-3-031-16446-0_42}

\bibitem{CycleGAN}
Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image
  translation using cycle-consistent adversarial networks. In: 2017 IEEE
  International Conference on Computer Vision (ICCV). pp. 2242--2251 (2017).
  \doi{10.1109/ICCV.2017.244}

\end{thebibliography}

\end{document}
