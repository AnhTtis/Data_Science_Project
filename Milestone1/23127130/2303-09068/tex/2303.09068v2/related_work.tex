\section{Related Work} \label{sec: related work}
\subsection{Machine Learning Techniques for Tabular Data} \label{subsec: MLFT}
Traditional ML methods for analyzing tabular data include Gradient Boosting~\cite{friedman2002stochastic}, XGBoost~\cite{chen2016xgboost}, LightGBM~\cite{ke2017lightgbm}, and CatBoost~\cite{prokhorenkova2018catboost}, all based on decision tree algorithms~\cite{quinlan1986induction}. 
These techniques are widely used and remain dominant over CNNs when handling tabular data~\cite{shwartz2022tabular}. They mitigate overfitting in regression or classification tasks by employing ensemble methods such as boosting and bagging~\cite{friedman2002stochastic}. 
However, the performance of traditional ML techniques decreases as the number of attributes increases. The increased number of attributes negatively impacts testing performance, as the models struggle to generalize well on unseen data due to high dimensionality and complexity~\cite{prokhorenkova2018catboost}. Additionally, the training speed decreases when the attributes consist of numerous real values, as seen in industrial sensor data~\cite{wang2018industrial}. This decline is due to the exponential growth in the number of branches in the tree, leading to slow training speeds and high computational costs~\cite{ke2017lightgbm}. LightGBM was introduced with the leaf-wise algorithm to improve training speed. Still, it generally does not outperform XGBoost and CatBoost because it cannot capture the detailed feature information available in the level-wise algorithm~\cite{prokhorenkova2018catboost}. Therefore, finding a new method to handle tabular data effectively with numerous real attributes is necessary.

% \subsection{Convolutional Neural Networks in the Context of Tabular Data}
% CNNs, a deep learning technique, employ convolution layers with multiple kernels to identify intricate patterns in feature maps composed of real values~\cite{simonyan2014very}. The introduction of skip connections has significantly improved CNN performance~\cite{he2016identity}. However, applying 1-D CNNs to analyze 1-D tabular data is generally less effective than traditional ML techniques~\cite{shwartz2022tabular}. Therefore, to utilize 2-D convolution operations (i.e., 2-D CNNs), it is necessary to convert tabular data into 2-D images.
% Despite their strengths, CNNs are generally slower than ML techniques for tabular data due to the excessive number of parameters. While some studies have trained images using ML techniques for tabular data, converting images to tabular data can slow down ML techniques due to the large number of attributes (i.e., pixels) in an image~\cite{samat2020catboost}. Images typically consist of R, G, and B channels with many pixels, which increases the number of tree branches and slows training speed. Techniques such as Fitnets~\cite{romero2014fitnets} and neural architecture search methods~\cite{li2023zico} have been developed to optimize CNN layers using only essential weight parameters, maintaining or improving performance. When tabular data contains many real attributes, CNNs can train complex patterns according to correlations between attributes~\cite{zeiler2014visualizing}.

\subsection{Converting Tabular Data into Images for CNNs} \label{subsec: converting tabular into images}
Industries such as semiconductor manufacturing generate extensive tabular data characterized by real-number attributes, which differ from integer-based categorical attributes. As the number of attributes increases, this characteristic can hinder the performance of traditional ML techniques~\cite{wang2018industrial, borisov2021deep}.

CNNs can handle complex patterns in feature maps with real values, but their effectiveness is primarily limited to 2-D spatial data, such as images~\cite{zeiler2014visualizing}, rather than 1-D convolution operations~\cite{borisov2021deep, shwartz2022tabular}. Therefore, researchers have proposed various methods to convert tabular data into images for CNNs.
Methods such as DeepInsight~\cite{sharma2019deepinsight}, REFINED~\cite{bazgir2020representation}, IGTD~\cite{zhu2021converting}, and SuperTML~\cite{sun2019supertml} have demonstrated superior performance compared to traditional ML approaches, especially when there are many attributes in tabular data.
However, these methods may result in overfitting or the oversight of global patterns~\cite{borisov2021deep}. A key aspect of these methods is that each attribute is assigned a fixed position within a 2D (or 3D) matrix by grouping similar attributes. However, grouping similar attributes could hinder learning complex patterns across various attributes, as convolutional operations might overly focus on patterns of adjacent, similar features~\cite{dumoulin2016guide, borisov2021deep}. This strategy contrasts sharply with traditional ML practices that recommend dropping highly correlated attributes to prevent overfitting~\cite{yu2003feature, borisov2021deep}.

SuperTML~\cite{sun2019supertml} engraves features of tabular data onto an empty black image (i.e., a zero 2-D matrix), with each feature engraved in varying sizes based on its importance. Although SuperTML performed better than traditional ML techniques in some datasets, it faces challenges in determining the extent to increase image size with a growing number of attributes. It also requires prioritizing attributes due to considerations of font size when the choice of font type influences engraving features and its performance. These constraints highlight the necessity for a more generalized method.

Despite the enhanced performance achieved by DeepInsight, REFINED, IGTD, and SuperTML, it remains to be seen whether the improvement stems from CNNs' ability to detect complex patterns through 2-D convolution operations or primarily from their feature positioning methods. To address this uncertainty, we introduce eight different feature positioning scenarios in CNNs in Section~\ref{sec: voltex positioning}. These scenarios aim to demonstrate that the effectiveness of CNNs in detecting complex patterns is not solely responsible for improved test performance; how features are positioned also plays a critical role. Based on these insights, we introduce a new method for positioning features called Vortex Feature Positioning (VFP). VFP directly transforms all attributes of tabular data into images based on their correlations, reducing the risk of overfitting by considering the varying degrees of correlations.
Additionally, VFP adjusts the image size based on the number of attributes, directly converting all tabular data attributes into images based on their correlations and accounting for their correlation degrees, unlike prior methods with fixed image sizes.

