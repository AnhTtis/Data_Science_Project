\documentclass{article}

\input{math_commands.tex}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage[ruled]{algorithm2e} % For algorithms
\usepackage{algpseudocode}
\usepackage{textcomp}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{makecell}


\title{VFP: Converting Tabular Data for IIoT into Images Considering Correlations of Attributes for Convolutional Neural Networks}

\author{Jong-Ik~Park\\
Chung-Ang University\\
Seoul, Korea\\
\texttt{bellwing95@cau.ac.kr} \\
  \And
Cheol-Ho~Hong\\
Chung-Ang University\\
Seoul, Korea\\
\texttt{cheolhohong@cau.ac.kr} \\
}


\begin{document}
\maketitle
\begin{abstract}
For tabular data generated from IIoT devices, traditional machine learning (ML) techniques based on the decision tree algorithm have been employed. However, these methods have limitations in processing tabular data where real number attributes dominate. To address this issue, DeepInsight, REFINED, and IGTD were proposed to convert tabular data into images for utilizing convolutional neural networks (CNNs). They gather similar features in some specific spots of an image to make the converted image look like an actual image. Gathering similar features contrasts with traditional ML techniques for tabular data, which drops some highly correlated attributes to avoid overfitting. Also, previous converting methods fixed the image size, and there are wasted or insufficient pixels according to the number of attributes of tabular data. Therefore, this paper proposes a new converting method, Vortex Feature Positioning (VFP). VFP considers the correlation of features and places similar features far away from each. Features are positioned in the vortex shape from the center of an image, and the number of attributes determines the image size. VFP shows better test performance than traditional ML techniques for tabular data and previous converting methods in five datasets: Iris, Wine, Dry Bean, Epileptic Seizure, and SECOM, which have differences in the number of attributes.
\end{abstract}

\section{Introduction} \label{introduction}
The industrial internet of things (IIoT) collects vast amounts of sensor data that are commonly presented in a tabular form~\cite{aggarwal2013managing, wang2018industrial}. These industry data are utilized by machine learning (ML) techniques to identify faulty goods and detect anomalies. However, the increasing number of attributes in the data fields and the need for high resolution data often make the traditional ML techniques inadequate. The traditional ML methods are based on decision tree algorithms~\cite{quinlan1986induction} combined with voting algorithms~\cite{dietterich2000ensemble} and ensemble methods~\cite{bauer1999empirical} (e.g. Gradient Boosting~\cite{friedman2002stochastic}, XGboost~\cite{chen2016xgboost}, LightGBM~\cite{ke2017lightgbm}, and CatBoost~\cite{prokhorenkova2018catboost}). They have high test accuracy with large amounts of training tabular data, but the performance decreases with the increase in the number of attributes due to fewer weight parameters. This performance drop becomes more severe when the data are primarily real numbers. 

To address this issue, researchers are exploring the use of CNNs~\cite{borisov2021deep} by converting tabular data into an image. CNNs are a deep learning technique with many weight parameters and stand out for image data that show distinct patterns within pixels~\cite{zeiler2014visualizing}. The challenging issue of using CNNs for tabular data is that CNNs employ filters or convolution kernels to detect a particular pattern in a 2-D image. However, a sample of tabular data is generally a vector, and even if we convert it into 2-D array by using a conventional shape method, the 2-D image does not have meaningful patterns inside the converted image. Therefore, CNNs tend not to work properly without placing the attributes of tabular data into proper places in the converted image that form a spatial pattern~\cite{borisov2021deep, grinsztajn2022tree}. 

Previous studies, such as DeepInsight~\cite{sharma2019deepinsight}, REFINED~\cite{bazgir2020representation}, IGTD~\cite{zhu2021converting}, and SuperTML~\cite{sun2019supertml}, have proposed methods to convert 1-D tabular data into 2-D image data, taking into account the attribute positions in the tabular data. Specifically, DeepInsight, REFINED, and IGTD focused on grouping similar attributes with high correlations in one or more specific locations in a 2-D matrix to enable a CNNs model to learn the patterns within similar attributes of the tabular data. However, this approach may lead to overfitting if similar attributes are intensively grouped in a certain location, as the model may not be able to train universal patterns between dissimilar attributes effectively. Moreover, these methods convert tabular data into a fixed-size image, regardless of the volume or scale of the attributes. This can result in wasted pixels if the volume is small or if there is insufficient space to accommodate all attribute values if the attributes are large-scale. SuperTML, on the other hand, carves features (or attributes) of a sample (or row) on an empty black image by setting features in different font sizes depending on their importance. While SuperTML showed improved performance in some datasets, it has two limitations: it may not be applicable if there are too many attributes to carve on an image with a reasonable size, and its performance varies depending on the font type used.

This paper presents Vortex Feature Positioning (VFP), a novel approach for converting tabular data in IIoT into images for CNNs while accounting for attribute correlations. As the number of sensors in IIoT continues to rise, there is a corresponding increase in attributes with real sensor data values~\cite{abramovici2015smart, krishnamurthi2020overview}. To address this, VFP converts tabular data into images for use with CNNs employing 2-D convolutional operations. To avoid overfitting, VFP arranges the attributes in a spatial space that considers their similarities with high correlations, while also allowing for the formation of an image with a flexible size. VFP arranges the attributes of the tabular data in a vortex shape, rotating from the center, based on their Pearson correlation coefficient (PCC) to optimally perform convolution operations and extract essential patterns. The PCC determines the order of attribute placement. VFP outperforms previous methods, such as the ML techniques for tabular data and IGTD, on various datasets, including Iris, Wine, Dry Bean, Epileptic Seizure, and SECOM, which have different numbers of attributes. The selection of these datasets highlights VFP's robustness to varying attribute counts.


Machine learning and deep learning approaches are aimed at discovering complex patterns in training data effectively. In this context, VFP makes three significant contributions:

\begin{itemize}
    \item VFP outperforms previous ML techniques for tabular data and previous methods for CNNs that convert tabular data into images.
    \item VFP can convert tabular data with varying numbers of attributes into images with an optimized number of pixels.
    \item VFP can utilize any state-of-the-art CNNs and training methods.
\end{itemize}

By making these contributions, VFP presents a valuable and versatile tool for transforming tabular data into images suitable for CNNs.

The rest of the paper is structured as follows: In Section~\ref{sec: related work}, we discuss related work. In Section~\ref{sec: voltex positioning}, we present our proposed VFP method. Section~\ref{sec: evaluation} describes the experimental evaluation, and in Section~\ref{sec: conclusion}, we conclude the paper and discuss future work.

\section{Related Work} \label{sec: related work}
\subsection{Machine Learning Techniques for Tabular Data} \label{subsec: MLFT}

Traditional ML methods for analyzing tabular data include Gradient Boosting~\cite{friedman2002stochastic}, XGBoost~\cite{chen2016xgboost}, LightGBM~\cite{ke2017lightgbm}, and CatBoost~\cite{prokhorenkova2018catboost}, which are all based on the decision tree algorithm~\cite{quinlan1986induction}. They have been widely used and are still dominant over CNNs when the data is in tabular format~\cite{shwartz2022tabular}. These techniques address overfitting issues resulting from limited attributes in tabular data for regression or classification tasks by utilizing ensemble methods known as boosting and bagging~\cite{bauer1999empirical, friedman2002stochastic}.
%Also, as introduced from Gradient Boosting and XGBoost to CatBoost, the performance has gradually increased.
However, the traditional ML techniques show slow training speed when the number of attributes increases because the number of branches of the tree becomes exponential. When the characteristics of attributes are composed of many real values, such as in industrial sensor data~\cite{aggarwal2013managing, wang2018industrial}, the training speed decreases further, and the performance is poor~\cite{ke2017lightgbm}. LightGBM was introduced with the leaf-wise algorithm to improve training speed, but it does not outperform XGBoost and CatBoost because it cannot obtain the detailed feature information shown in the level-wise algorithm~\cite{prokhorenkova2018catboost}. Therefore, it is necessary to find a new method to handle tabular data with numerous real attributes.

CNNs, a ML technique, utilize convolution layers with multiple kernels to identify intricate patterns in feature maps composed of real values~\cite{simonyan2014very}. The introduction of skip connection has also led to a significant improvement in the performance of CNNs~\cite{he2016deep}. However, these improvements are only applicable to CNNs that utilize 2-D convolution operations and not to 1-D convolution operations, which are generally considered inferior to traditional ML techniques for analyzing tabular data~\cite{shwartz2022tabular}. To utilize 2-D convolution operations, we need to convert tabular data into images. This study proposes a new method for converting tabular data into images, which will be discussed further in Section~\ref{subsec: converting tabular into images}.

CNNs are generally considered to be slower than ML techniques for tabular data due to excessive weight parameters. There were a few cases that trained images utilizing ML techniques for tabular data. Suppose we convert images to tabular data for employing ML techniques for tabular data. ML techniques for tabular data might slow down due to the many converted attributes (i.e., pixels) of an image~\cite{samat2020catboost, bentejac2021comparative}. An image comprises R, G, and B 3-channel with many pixels. 
%(i.e., $the\;number\;of\;pixels \times the\;number\;of\;channels$).
Many attributes decrease the training speed of ML techniques for tabular data because of the increasing number of branches of trees. In addition, various techniques have been developed for CNNs to optimize the number of layers using only essential weight parameters, such as Fitnets~\cite{romero2014fitnets}, while maintaining or improving performance. Therefore, there are many chances to improve training speed in CNNs. In other words, if tabular data contains many real attributes, there is no reason not to use CNNs, which train complex patterns according to correlations between attributes~\cite{zeiler2014visualizing}.

\subsection{Converting Tabular Data into Images for CNNs} \label{subsec: converting tabular into images}
Industries, such as semiconductor manufacturing, generate vast amounts of tabular data containing real attributes, which can limit the performance of the traditional ML techniques as the number of attributes increases~\cite{aggarwal2013managing, mourtzis2016industrial, wang2018industrial, borisov2021deep}. CNNs can handle complex patterns in feature maps with real values, but their effectiveness is limited to 2D spatial data, such as images~\cite{zeiler2014visualizing}. However, CNNs with 2-D convolution operations can be better than the ML techniques for tabular data, not with 1-D convolution operations~\cite{borisov2021deep, shwartz2022tabular}, and CNNs with 2-D convolution operations only handle 2-D spatial data: images. Therefore, researchers have proposed various methods to convert tabular data into images for employing CNNs.

Previous methods, such as DeepInsight~\cite{sharma2019deepinsight}, REFINED~\cite{bazgir2020representation}, IGTD~\cite{zhu2021converting}, and SuperTML~\cite{sun2019supertml}, have shown better performance than the traditional ML techniques. However, DeepInsight, REFINED, and IGTD require gathering similar attributes to form images, which may lead to overfitting or neglecting important global patterns~\cite{borisov2021deep}. Also, the idea of gathering similar features of tabular data contrasts with dropping attributes showing high correlations with other attributes to avoid overfitting in ML techniques for tabular data~\cite{yu2003feature, borisov2021deep} and prevent training universal complex patterns within attributes because convolution operations simultaneously process several features according to the given kernel size and excessively strengthen the patterns of similar features if they are adjacent~\cite{dumoulin2016guide, borisov2021deep}. 
SuperTML~\cite{sun2019supertml} engraves features of tabular data in an empty black image (i.e., a zero 2-d matrix), and each feature is engraved with a different size according to the degree of importance. SuperTML showed better performance than the traditional ML techniques in some datasets. However, SuperTML needs to consider how much to increase the image size as the number of attributes increases, and it is always necessary to count the priority of attributes because of the font size for engraving features. Furthermore, the performance of SuperTML is also affected by the font type. These limitations highlight the need for a more generalized method.

Although DeepInsight, REFINED, IGTD, and SuperTML improved test performance from ML techniques for tabular data, we deduce that the increase in performance is due to the exceptional ability to find complex patterns well of CNNs with 2-D convolution operations, not primarily due to their feature positioning methods.
To overcome these limitations, this paper proposes a new feature positioning method, Vortex Feature Positioning (VFP), that directly converts all attributes of tabular data into images using their correlations. VFP optimizes the image size depending on the number of attributes and avoids overfitting by considering the degree of correlations.

\section{Vortex Feature Positioning} \label{sec: voltex positioning}
\begin{figure*}[h]
    \centering
    \includegraphics[width=8.5cm]{figs/figure_featuremap.pdf}
    \caption{Three cases of feature positioning considering the number of features per a convolution operation. Zero padding of size 1 and 2, and distancing.}
    \label{fig:2D Embedding}
\end{figure*}
As the number of real value attributes in tabular data increases, the traditional ML techniques exhibit lower performance and slower training speeds~\cite{borisov2021deep}. CNNs can overcome these limitations by converting tabular data into images and exploiting the benefits of 2-D convolution operations to capture complex patterns in tabular data~\cite{sharma2019deepinsight, zhu2021converting}. 

Previous methods of converting tabular data into images gather similar features in a specific spot. However, since tabular data is heterogeneous data and features of tabular data are not pixels of an actual image, embedded features may not behave like pixel values of an actual image, and gathering similar features may lead to overfitting that often occurs in ML techniques for tabular data when dealing with highly correlated features~\cite{caruana2000overfitting, yu2003feature, borisov2021deep}. In this section, we explain our new method for converting tabular data into images, Vortex Feature Positioning (VFP), which considers two critical conditions:
\begin{itemize}
    \item Tabular data is heterogeneous data, and features of tabular data are not pixels of an actual image. Therefore, we should assume the results of 2-D convolution operations as formed patterns, not extracted patterns.
    \item Highly correlated features of tabular data should be positioned far away from each other.
\end{itemize}

We first explain how to embed features into a 2-D matrix while considering convolution operations in Section~\ref{subsec: embed feature}. Then, we describe how to arrange features according to the correlation of attributes in an image in Section~\ref{subsec: arranging features}.

\subsection{Embedding Features Considering Convolution Operations}
\label{subsec: embed feature}
CNNs (e.g., InceptionNet~\cite{szegedy2015going}, ResNet~\cite{he2016deep}, and DenseNet~\cite{huang2017densely}) typically use $3\times3$ kernels in the convolution layer. The results of convolution operations with $3\times3$ kernels and feature maps in the first layer affect every layer's kernels and final inferences~\cite{zeiler2014visualizing}. Therefore, in the first layer, we need to consider the number of features per convolution operation to decide the degree of complexity of patterns. To embed features into an empty 2-D matrix, we consider three cases, as shown in Figure~\ref{fig:2D Embedding}: zero padding of size 1 and 2, and distancing.

$3\times3$ kernels of convolution layers can handle up to nine features. The numbers of convolution operations per the number of features handled at once are calculated in Eq.s~\ref{eqn:pad1}, \ref{eqn:pad2}, and \ref{eqn:distancing}. $N_{i}^{case}$ is the number of convolution operations using $i$ features in each case, and $m$ and $n$ are the numbers of rows and columns each. $m$ and $n$ have the same values due to square images often used in CNNs~\cite{he2016deep, huang2017densely}. However, we distinguished them for convenience. \\ \noindent
\textbf{Zero Padding of Size 1}
\begin{equation}
\label{eqn:pad1}
\begin{aligned}
    &{N_{4}^{Pad1}=4} \\            
    &{N_{6}^{Pad1}=2\times{(m-2)}+2\times{(n-2)}=2m+2m-8} \\
    &{N_{9}^{Pad1}=(m-2)\times{(n-2)}=mn-2m-2n+4} 
\end{aligned}
\end{equation}
\textbf{Zero Padding of Size 2}
\begin{equation}
\label{eqn:pad2}
\begin{aligned}
    &{N_{1}^{Pad2}=4}\;\;\;\;\;\;\;\;\;\;{N_{2}^{Pad2}=8}\;\;\;\;\;\;\;\;\;\;{N_{4}^{Pad2}=4} \\ 
    &{N_{3}^{Pad2}=2\times{(m-2)}+2\times(n-2)=2m+2m-8} \\            
    &{N_{6}^{Pad2}=2\times{(m-2)}+2\times(n-2)=2m+2m-8} \\
    &{N_{9}^{Pad2}=(m-2)\times{(n-2)}=mn-2m-2n+4} 
\end{aligned}
\end{equation}
\textbf{Distancing}
\begin{equation}
\label{eqn:distancing}
\begin{aligned}
    &{N_{1}^{Dist}=m\times{n}=mn} \\            
    &{N_{2}^{Dist}=n\times{(m-1)}+m\times{(n-1)}=2mn-m-n} \\
    &{N_{4}^{Dist}=(m-1)\times{(n-1)}=mn-m-n+1} 
\end{aligned}
\end{equation} 

The number of convolution operations per the number of features is shown in Table~\ref{table:convoltion_number}.
\begin{table*}[h]
    \scriptsize
    \centering 
    \caption{The number of convolution operations per the number of features.}
    \label{table:convoltion_number}
    \begin{tabular}{c|c|c|c}
    \noalign{\smallskip}\noalign{\smallskip}\hline\hline
    \multirow{2}{*}{\textbf{Cases}} & \textbf{Zero Padding} & \textbf{Zero Padding} & \multirow{2}{*}{\textbf{Distancing}}   \\ 
    \multicolumn{1}{c|}{\textbf{}}& \textbf{of Size 1}  & \textbf{of Size 2} &   \\ \hline
    \multicolumn{1}{c|}{\textbf{Using 1 feature}}  & N/A & 4 & $mn$ \\ \hline
    \multicolumn{1}{c|}{\textbf{Using 2 features}} & N/A & 8 & $2mn-m-n$ \\ \hline
    \multicolumn{1}{c|}{\textbf{Using 3 features}} & N/A & $2m+2n-8$  & N/A \\ \hline
    \multicolumn{1}{c|}{\textbf{Using 4 features}} &  4  & 4 & $mn-m-n+1$ \\ \hline            
    \multicolumn{1}{c|}{\textbf{Using 6 features}} & $2m+2n-8$ & $2m+2n-8$ & N/A \\ \hline
    \multicolumn{1}{c|}{\textbf{Using 9 features}} & $mn-2m-2n+4$ & $mn-2m-2n+4$ & N/A \\ 
    \Xhline{3\arrayrulewidth}
    \multicolumn{1}{c|}{\textbf{Sum of all cases}} & $mn$ & $mn+2m+2n+4$ & $4mn-2m-2n+1$ \\ \hline
    \hline
    \end{tabular}
\end{table*}
ZPOS1 uses three cases of the number of features for convolution operations totaling $mn$. ZPOS2 uses six cases of the number of features for convolution operations totaling $mn+2m+2n+4$. Distancing uses three cases of the number of features for convolution operations totaling $4mn-2m-2n+1$. The number of convolution operations denotes how many patterns CNNs have and the number of features used at once related to the complexity of patterns. For example, if there are nine features ($m=3$ and $n=3$), ZPOS2 and distancing perform twenty-five convolution operations identically. 
However, if there are more than nine features, distancing performs much more convolution operations and uses relatively fewer features than ZPOS2. Therefore, distancing considers many rough patterns between features, and ZPOS2 considers detailed patterns as many combinations of various features. Since ZPOS1 uses only four, six, and nine features and performs $mn$ convolution operations, it is beneficial to avoid overfitting problems than ZPOS2, which finds excessively detailed patterns. We also embed features of attributes in an image with no padding or distancing. However, convolution operations only use nine features at once, and the number of convolution operations is $(m-2)(n-2)$, which is the smallest among others. Overall, the number of features used in a single convolution operation and the degree of complexity of patterns should be considered when embedding features into a 2-D matrix.

\subsection{Arranging Features Considering Correlations of Attributes} \label{subsec: arranging features}
The traditional ML techniques may exhibit overfitting when they heavily utilize highly correlated attributes without dropping them during training~\cite{yu2003feature, grabczewski2005feature}.
Previous methods of converting tabular data into images have mainly focused on placing similar features together, resulting in better performance~\cite{sharma2019deepinsight, borisov2021deep}.
The high ability of CNNs to form critical patterns increases the performance. However, we cannot conclude that gathering similar features increases performance, because each attribute in tabular data, heterogeneous data, is not the same as pixels of the actual image, homogeneous data~\cite{borisov2021deep}.

VFP performs convolution operations on low-correlated attributes by placing low-correlated features at the center of a blank image. This is because 2-D convolution operations more use center-located features than edge-located features.

\begin{equation}
\label{eqn:pearson_coefficient}
    r(\va,\vb) = \frac{\sum\limits^{n}(\va_{i} - \overline{\va})(\vb_{i} - \overline{\vb})}{\sqrt{
        (\sum\limits^{n}(\va_{i} - \overline{\va})^2)(\sum\limits^{n}(\vb_{i} - \overline{\vb})^2)
    }},
\end{equation}
Pearson correlation coefficient (PCC) in Equation~\ref{eqn:pearson_coefficient} is used to investigate the relationship between attributes, and attributes are rearranged based on PCCs.
\begin{algorithm}[h]
\caption{Arranging attributes based on the sum of Pearson correlation coefficients}
\label{algorithm:reorder}
    \textbf{Input:} $\sX$ = \{$\vx_{i}|i=1,2...,k$\} \\
    \textbf{Output:} $\sX_{orderred}$ \\              
    $\vc = $\{$c_i|i=1,2,...,k$\} \\
    \For {$i \in 1,2,...,k$} {
        $\vc_{i} \gets 0$ \\
        \For {$j \in 1,2,...,k$} {
            $\vc_{i} \gets \vc_{i}+r(\vx_i,\vx_j)$
        }          
    }
    $\vc \gets rank_{ascending}(\vc)$ or $\vc \gets rank_{descending}(\vc)$  \\
    \For {$i \in 1,2,...,k$} {
        ${\sX_{orderred}}\;_{\sC_{i}} \gets \sX_{i}$
    }          
\end{algorithm}
Algorithm~\ref{algorithm:reorder} outlines the arranging process, which sorts attributes in ascending or descending order according to the sum of PCCs to investigate whether gathering similar values improves performance more than spreading them out.
$\sX$ is a tabular dataset composed of attributes $\vx_{i}$. $\vc$ is a vector composed of $c_i$, which is the sum of PCCs of $\vx_{i}$ and other attributes. $rank_{ascending}(\vc)$ and $rank_{descending}(\vc)$ are functions that arrange $\vc$ according to $c_i$ size in ascending or descending order each. We arrange $\sX$ based on the order stored in $\vc$.
\begin{figure*}[t]
    \centering
    \includegraphics[width=11cm]{figs/figure_featurechannel.pdf}
    \caption{Vortex Feature Positioning and forming a 3-ch image by copying a 2-d matrix.}
    \label{fig:channel Positioning}
\end{figure*}
After arranging attributes based on the PCC, features of attributes are embedded in an image. CNNs perform more convolution operations on features at the center of the image than on exterior features.
Therefore, placing features in a vortex shape from the center of the image exploits desired features for many convolution operations, while undesired features are the opposite.
For example, arranging features in ascending order according to the sum of PCCs performs many convolution operations with low-correlated features such as $x_{1}^{c}$, $x_{2}^{c}$, $x_{3}^{c}$, and $x_{4}^{c}$, which are similar to feature selection in ML techniques for tabular data~\cite{grabczewski2005feature}. In contrast, features $x_{n}^{c}$ and $x_{n-1}^{c}$, which have high PCCs and are located near the edge, are used for relatively fewer convolution operations.

We use a single-channel image (i.e., a grayscale image) in VFP. However, state-of-the-art CNNs (i.e., ResNet and DenseNet) require 3-channel images. This paper employs ResNets. Therefore (R), green (G), and blue (B) channels all have the same feature map, as shown in Figure~\ref{fig:channel Positioning}.

\section{Experimental Evaluation} \label{sec: evaluation}
\subsection{Dataset} \label{subsec: dataset}
\begin{figure*}[t]
    \centering
    \includegraphics[width=16cm]{figs/figure_imagedata.pdf}
    \caption{Converted images employing IGTD and VFP (with distancing). Datasets are IRIS and SECOM.}
    \label{fig:image data}
\end{figure*}
The evaluation considers five tabular datasets according to the number of attributes: Iris, Wine, Dry Bean, Epileptic Seizure, and SECOM datasets from the UCL Machine Learning Repository~\cite{Dua:2019}. Iris, Wine, Dry Bean, and SECOM are commonly used industrial tabular datasets, while Epileptic Seizure is a medical tabular dataset known to be challenging to train effectively.
\begin{table}[h]
    \scriptsize
    \centering 
    \caption{Dataset information}
    \label{table:data_information}
    \begin{tabular}{c|c|c|c|c|c|c}
    \noalign{\smallskip}\noalign{\smallskip}\hline\hline
    & \multirow{2}{*}{\textbf{Iris}} & \multirow{2}{*}{\textbf{Wine}} & \textbf{Dry} &  \textbf{Epileptic} & \multirow{2}{*}{\textbf{SECOM}} \\ 
    \multicolumn{1}{c|}{\textbf{}}&  &  & \textbf{Bean} & \textbf{Seizure} & \\ \hline
    \multicolumn{1}{c|}{\textbf{Classes}} & 3 & 3 & 7 & 5 & 2 \\ \hline
    \multicolumn{1}{c|}{\textbf{Number of Attributes}} & 4 & 13 & 16 & 178 & 591 \\ \hline
    \multicolumn{1}{c|}{\textbf{Number of Intances}} & 150 & 178 & 13,611 & 11,500 & 1,567 \\ \hline
    %\multicolumn{1}{c|}{\textbf{Attributes}} & \multirow{2}{*}{Real} & \multirow{2}{*}{Real} & \multirow{2}{*}{Real} & \multirow{2}{*}{Real} & \multirow{2}{*}{Real} \\      
    %\multicolumn{1}{c|}{\textbf{ Characteristics}}&  &  & & &  \\ \hline
    \multicolumn{1}{c|}{\textbf{Missing Values}} & No & No & No & No & Yes \\ \hline \hline
    \end{tabular}
\end{table}
Each dataset's information is summarized in Table~\ref{table:data_information}. Iris, Wine, and Dry Bean have relatively small 4, 13, and 16 attributes, respectively, while Epileptic Seizure and SECOM have 178 and 591 attributes, respectively. All datasets contain real-valued attributes, which are known to be difficult to train using ML techniques for tabular data. Each dataset is randomly split into training and testing sets in a 0.8:0.2 ratio, and a Min-Max scaler is applied to normalize the data.

Figure~\ref{fig:image data} shows example images for the IRIS and SECOM datasets generated using IGTD and VFP with distancing. IGTD~\cite{zhu2021converting} is a recent method for converting tabular data into images that showed better performance and smaller image sizes than other methods such as DeepInsight~\cite{sharma2019deepinsight} and REFINED~\cite{bazgir2020representation}. The image size generated by IGTD is fixed at $50\times50$ pixels, regardless of the number of attributes in the dataset. In contrast, VFP with distancing automatically adjusts the image size based on the number of attributes, resulting in image sizes of $5\times5$ and $45\times45$ pixels for the Iris and SECOM datasets, respectively. Although Y. Zhu et al.~\cite{zhu2021converting} mention that the image size can be changed according to the number of attributes, there is no proposed generalized rule for image sizes across different datasets.

\subsection{Models} \label{subsec: model}
\begin{table*}[t]\centering
\caption{Architecture of ResNet-18 and modified ResNet-18. Building blocks are in square brackets, with the stacked number of blocks. N in the linear section is the number of classes of each dataset.}
\includegraphics[width=10cm]{figs/table_resnetarch.pdf}
\label{table:resnet}
\end{table*}
This paper employs ResNets for the evaluation which are constructed by stacking residual blocks. He et al. used four types of residual blocks~\cite{he2016deep} to construct ResNets, and we use two ResNets in our experiments with different kernel sizes in the first layer (Conv1) to compare their performance. The structures of two ResNets are shown in Table~\ref{table:resnet}. The structures of two ResNets are shown in Table~\ref{table:resnet}, and the kernel size of the first layer (Conv1) is different to compare the performance according to the number of features handled at once. Not modified ResNet-18 (original) uses a $7\times7$ kernel size and stride of 2 for the first layer to downsize large images. The $7\times7$ kernel can use up to 49 features at one convolution operation. We modified the size of kernels in the first layer to $3\times3$ and stride of 1 to be consistent with VFP's convolution operations. We compared the test results of these two models to determine whether the kernel size in the first layer is critical to the performance.

\subsection{Evaluation} \label{subsec: evalutaion}
We evaluate experiments on an NVIDIA Tesla V100 PCIe 32GB. We use a cosine annealing warm-up restarts optimizer with learning rates range of from 0.1 to 0.001, and learning rates are repeated every ten epochs. The mini-batch size is 128, and we train Iris, Wine, Epileptic Seizure, and SECOM datasets for 150 epochs and Dry Bean dataset for 600 epochs. We repeat each experiment eight times with different random seeds ranging from 1000 to 8000 with a 1000 increment to initialize the weight parameters of CNNs~\cite{he2015delving}.

\begin{table*}[hbt!]
    \scriptsize
    \centering 
    \caption{Test results of employing Vortex Feature Positioning on converted spatial data with Modified ResNet-18. (Ascending Order according to the size of the sum of Pearson correlation coefficient)}
    \label{table:test_result_corr_ascending}
    \begin{tabular}{cc|c|c|c|c|c}
    \noalign{\smallskip}\noalign{\smallskip}\hline\hline
    && \multirow{2}{*}{\textbf{Iris}} & \multirow{2}{*}{\textbf{Wine}} & \textbf{Dry} &  \textbf{Epileptic} &  \multirow{2}{*}{\textbf{SECOM}} \\ 
    \multicolumn{1}{c}{\textbf{}}&  &  & & \textbf{Bean} &  \textbf{Seizure} & \\ \Xhline{3\arrayrulewidth}

    \multicolumn{1}{c|}{\textbf{VFP}} & \textbf{Train Acc. (\%)}
    &N/A&100.0&95.2&100.0&100.0
 \\ \cline{2-7}
    \multicolumn{1}{c|}{\textbf{(No Padding)}}& \textbf{Test Acc. (\%)} &\textbf{N/A}&\textbf{100.0}&\textbf{94.2}&\textbf{76.9}&\textbf{94.3}
 \\ \Xhline{3\arrayrulewidth}
    \multicolumn{1}{c|}{\textbf{VFP}} & \textbf{Train Acc. (\%)} 
    &100.0&100.0&95.2&100.0&100.0
 \\ \cline{2-7} 
    \multicolumn{1}{c|}{\textbf{(ZPOS1)}}& \textbf{Test Acc. (\%)} &\textbf{100.0}&\textbf{100.0}&\textbf{94.2}&\textbf{77.0}&\textbf{93.9}
 \\ \Xhline{3\arrayrulewidth}
    
    \multicolumn{1}{c|}{\textbf{VFP}} & \textbf{Train Acc. (\%)} 
    &100.0&100.0&94.8&100.0&100.0
 \\ \cline{2-7} 
    \multicolumn{1}{c|}{\textbf{(ZPOS2)}}& \textbf{Test Acc. (\%)} &\textbf{100.0}&\textbf{100.0}&\textbf{94.1}&\textbf{77.7}&\textbf{94.3}
 \\ \Xhline{3\arrayrulewidth}
    
    \multicolumn{1}{c|}{\textbf{VFP}}& \textbf{Train Acc. (\%)}
    &100.0&100.0&94.6&100.0&100.0
 \\ \cline{2-7}
    \multicolumn{1}{c|}{\textbf{(Distancing)}}& \textbf{Test Acc. (\%)} &\textbf{100.0}&\textbf{100.0}&\textbf{94.3}&\textbf{78.3}&\textbf{94.6}
 \\ \hline\hline
    \end{tabular}
\end{table*}
\begin{table*}[hbt!]
    \scriptsize
    \centering 
    \caption{Test results of employing Vortex Feature Positioning on converted spatial data with Modified ResNet-18. (Descending Order according to The Size of The Sum of Pearson Correlation Coefficient)}
    \label{table:test_result_corr_descending}
    \begin{tabular}{cc|c|c|c|c|c}
    \noalign{\smallskip}\noalign{\smallskip}\hline\hline
    && \multirow{2}{*}{\textbf{Iris}} & \multirow{2}{*}{\textbf{Wine}} & \textbf{Dry} &  \textbf{Epileptic} &  \multirow{2}{*}{\textbf{SECOM}} \\ 
    \multicolumn{1}{c}{\textbf{}}&  &  & & \textbf{Bean} &  \textbf{Seizure} & \\ \Xhline{3\arrayrulewidth}

    \multicolumn{1}{c|}{\textbf{VFP}} & \textbf{Train Acc. (\%)}
    &N/A&100.0&95.3&100.0&100.0
 \\ \cline{2-7}
    \multicolumn{1}{c|}{\textbf{(No Padding)}}& \textbf{Test Acc. (\%)} &\textbf{N/A}&\textbf{100.0}&\textbf{94.1}&\textbf{77.3}&\textbf{93.9}
 \\ \Xhline{3\arrayrulewidth}
    \multicolumn{1}{c|}{\textbf{VFP}} & \textbf{Train Acc. (\%)} 
    &100.0&100.0&94.6&100.0&94.7
 \\ \cline{2-7} 
    \multicolumn{1}{c|}{\textbf{(ZPOS1)}}& \textbf{Test Acc. (\%)} &\textbf{100.0}&\textbf{100.0}&\textbf{94.2}&\textbf{77.3}&\textbf{93.9}
 \\ \Xhline{3\arrayrulewidth}
    
    \multicolumn{1}{c|}{\textbf{VFP}} & \textbf{Train Acc. (\%)} 
    &100.0&100.0&94.8&100.0&100.0
 \\ \cline{2-7} 
    \multicolumn{1}{c|}{\textbf{(ZPOS2)}}& \textbf{Test Acc. (\%)} &\textbf{100.0}&\textbf{100.0}&\textbf{94.0}&\textbf{76.6}&\textbf{93.9}
 \\ \Xhline{3\arrayrulewidth}
    
    \multicolumn{1}{c|}{\textbf{VFP}}& \textbf{Train Acc. (\%)}
    &100.0&100.0&94.2&100.0&100.0
 \\ \cline{2-7}
    \multicolumn{1}{c|}{\textbf{(Distancing)}}& \textbf{Test Acc. (\%)} &\textbf{100.0}&\textbf{100.0}&\textbf{94.2}&\textbf{77.3}&\textbf{93.9}
 \\ \hline\hline
    \end{tabular}
\end{table*}
\begin{table*}[hbt!]
    \scriptsize
    \centering 
    \caption{Comparison Test results of converting tabular data into image (IGTD), ML techniques for tabular data (XGBoost and CatBoost) and Vortex Feature Positioning (ascending order) with distancing).}
    \label{table:test_result_method}
    \begin{tabular}{cc|c|c|c|c|c}
    \noalign{\smallskip}\noalign{\smallskip}\hline\hline
    && \multirow{2}{*}{\textbf{Iris}} & \multirow{2}{*}{\textbf{Wine}} & \textbf{Dry} &  \textbf{Epileptic} &  \multirow{2}{*}{\textbf{SECOM}} \\ 
    \multicolumn{1}{c}{\textbf{}}&  &  & & \textbf{Bean} &  \textbf{Seizure} & \\ \Xhline{3\arrayrulewidth}

%    \multicolumn{1}{c|}{\textbf{Logistic}}& \textbf{Train Acc. (\%)} %&98.3&100.0&92.7&31.0&100.0
% \\ \cline{2-7} 
%    \multicolumn{1}{c|}{\textbf{Regression}}& \textbf{Test Acc. (\%)} &\textbf{100.0}&\textbf{97.2}&\textbf{92.7}&\textbf{24.7}&\textbf{84.1}
% \\ \Xhline{3\arrayrulewidth}

%    \multicolumn{1}{c|}{\textbf{Gradient}}& \textbf{Train Acc. (\%)} &99.8&100.0&91.6&58.3&79.0
% \\ \cline{2-7} 
%    \multicolumn{1}{c|}{\textbf{Boosting}}& \textbf{Test Acc. (\%)} &\textbf{97.0}&\textbf{94.2}&\textbf{90.1}&\textbf{53.5}&\textbf{4.0}
% \\ \Xhline{3\arrayrulewidth}

    \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{XGBoost}}}& \textbf{Train Acc. (\%)} &100.0&100.0&100.0&99.7&100.0
 \\ \cline{2-7} 
    \multicolumn{1}{c|}{\textbf{}}& \textbf{Test Acc. (\%)} &\textbf{96.7}&\textbf{100.0}&\textbf{93.2}&\textbf{70.0}&\textbf{93.9}
 \\ \Xhline{3\arrayrulewidth}

%    \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{LightGBM}}}& \textbf{Train Acc. (\%)} &97.3&99.1&95.7&83.8&
% \\ \cline{2-7} 
%    \multicolumn{1}{c|}{\textbf{}}& \textbf{Test Acc. (\%)} &\textbf{96.9}&\textbf{94.0}&\textbf{91.1}&\textbf{67.1}&\textbf{}
% \\ \Xhline{3\arrayrulewidth}

    \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{CatBoost}}}& \textbf{Train Acc. (\%)} &100.0&100.0&99.0&100.0&99.7
 \\ \cline{2-7} 
    \multicolumn{1}{c|}{}& \textbf{Test Acc. (\%)} &\textbf{96.7}&\textbf{100.0}&\textbf{93.3}&\textbf{72.3}&\textbf{94.3}
 \\ \Xhline{3\arrayrulewidth}

    \multicolumn{1}{c|}{\textbf{IGTD}}& \textbf{Train Acc. (\%)}
    &90.0&100.0&94.3&74.7&100.0 \\ \cline{2-7}
    \multicolumn{1}{c|}{\textbf{(ResNet-18)}}& \textbf{Test Acc. (\%)} &\textbf{86.7}&\textbf{100.0}&\textbf{94.0}&\textbf{68.1}&\textbf{94.3} \\
    \Xhline{3\arrayrulewidth}

    \multicolumn{1}{c|}{\textbf{IGTD}}& \textbf{Train Acc. (\%)}
    &82.5&100.0&94.9&94.0&97.4 \\ \cline{2-7}
    \multicolumn{1}{c|}{\textbf{(Modified ResNet-18)}}& \textbf{Test Acc. (\%)} &\textbf{80.0}&\textbf{100.0}&\textbf{94.2}&\textbf{75.3}&\textbf{94.3} \\
    \Xhline{3\arrayrulewidth}

    \multicolumn{1}{c|}{\textbf{VFP}}& \textbf{Train Acc. (\%)}
    &99.2&100.0&94.7&100.0&100.0 \\ \cline{2-7}
    \multicolumn{1}{c|}{\textbf{(Ascending, Distancing, ResNet-18)}}& \textbf{Test Acc. (\%)} &\textbf{100.0}&\textbf{100.0}&\textbf{93.7}&\textbf{77.1}&\textbf{94.6} \\
    \Xhline{3\arrayrulewidth}
    
    \multicolumn{1}{c|}{\textbf{VFP}}& \textbf{Train Acc. (\%)}
    &100.0&100.0&94.6&100.0&100.0 \\ \cline{2-7}
    \multicolumn{1}{c|}{\textbf{(Ascending, Distancing, Modified ResNet-18)}}& \textbf{Test Acc. (\%)} &\textbf{100.0}&\textbf{100.0}&\textbf{94.3}&\textbf{78.3}&\textbf{94.6} 
     \\ \hline \hline

\end{tabular}
\end{table*}
\subsubsection{Converting Tabular Data into Images}
To convert tabular data into images, we first embed one sample of each tabular dataset into a 2-D matrix with different types of zero padding and distancing: zero padding of size 1 (ZPOS1), zero padding of size 2 (ZPOS2), and distancing. We then convert the resulting 2-D matrix into a 3-channel image by copying itself. In the Iris dataset with no padding, $3\times3$ kernels cannot perform convolution operations since it has only four attributes that are embedded into $2\times2$ matrices.

\subsubsection{Employing Vortex Feature Positioning}
VFP performs convolution operations on desired features to improve performance. The results show that using ZPOS1, ZPOS2, and distancing outperforms the no padding state, which performs $(m-2)(n-2)$ convolution operations (Table~\ref{table:test_result_corr_ascending} and \ref{table:test_result_corr_descending}). Ascending orders generally show the same or higher performance, with the exception of 2 out of 20 cases in Tables~\ref{table:test_result_corr_ascending} and \ref{table:test_result_corr_descending}: (Dry Bean-No padding) and (Dry Bean-ZPOS1). Among the ascending orders, distancing shows the best result in all cases. Although distancing uses only 1, 2, and 4 features, which results in fewer complex patterns based on many features, excessive primary rough patterns can help avoid overfitting and improve performance. Therefore, it is expected that an embedded space that utilizes the desired relationships of features can form more adequate patterns and further improve performance.

\subsubsection{Comparison Performance with Other Machine Learning Techniques}
The performance of VFP is compared to the traditional ML techniques (XGBoost and CatBoost) in Table~\ref{table:test_result_method}. The experiments for the ML techniques are repeated eight times per tree depth, and the tree depth varies from one to sixteen. VFP (ascending order) with distance outperforms ML techniques for tabular data for all datasets. Particularly in the case of Epileptic Seizure, VFP has a test accuracy $6.0-8.3\%$ higher than traditional ML techniques. Furthermore, in the Kaggle competition held in 2021 using Dry Bean, many contestants employed attribute dropping and traditional ML techniques, with their best test accuracy being 94.049\%, lower than the VFP record of 94.308\% without feature selection~\cite{yu2003feature, dry-beans-classification-iti-ai-pro-intake01}. Therefore, it can be expected that even higher performance can be achieved if VFP adopts feature selection before training.

\subsubsection{Comparison Performance with various ResNets} \label{subsubsection:various_resnet_test}
\begin{figure}[t]
    \centering
    \includegraphics[width=7.5 cm]{figs/graph_resnet_comp.pdf}
    \caption{Test results of ResNet-18, and modified ResNet-18 employing Vortex Feature Positioning (ascending order) with distancing in Epileptic Seizure dataset.}
    \label{graph:resnet_comp}
\end{figure}
In this paper, ResNet-18 and a modified version of ResNet-18 are used to train tabular datasets to investigate whether kernel size in the first layer significantly impacts performance. The results, shown in Figure \ref{graph:resnet_comp}, indicate that the modified ResNet-18 achieved higher test accuracy than the original ResNet-18. It is possible that modifying the size of kernels in the first layer can improve performance because kernel size affects the number of features that can be handled in one convolution operation, and the number of features in turn determines the complexity of patterns that can be detected.

\section{Conclusion} \label{sec: conclusion}
This paper introduces Vortex Feature Positioning (VFP), a novel method for converting tabular data into images based on attribute correlations. By utilizing ascending orders and distancing strategies, VFP achieves superior performance compared to the traditional ML techniques across various datasets.

Compared to existing methods for converting tabular data into images, VFP is optimized to avoid overfitting by taking into account the correlations among attributes in the dataset. Furthermore, VFP can dynamically adjust the size of the converted images to accommodate different numbers of attributes, providing a flexible and adaptable solution for IIoT tabular data.

\bibliographystyle{unsrt}  

\bibliography{references}

\end{document}
