\vspace{-2mm}
\section{Blind Estimation System}
\vspace{-2mm}
Our blind estimation system first encodes the reference $y$ into latent vectors $z$, which should contain the necessary information to estimate the graph. Then, from the latent $z$,  we reconstruct the graph in two stages, which resemble the synthetic data generation procedure; we first decode the prototype $\smash{\hat{G}_0}$ autoregressively, then estimate the remaining parameters $\hat{p}$ (see Figure \ref{fig:framework}).
\input{figures/3-blind_estimation_framework}

\noindent \textbf{Reference Encoder.} 
We first apply $7$ two-dimensional convolutional layers to the reference Mel spectrogram.
Then, we flatten and project the channel and frequency axes of the output feature map into $512$-dimensional vectors. 
Finally, we perform an attentive pooling across the time axis with learnable queries to obtain latent vectors $z$. 

\noindent \textbf{Prototype Decoder.}
Most autoregressive models \cite{faez2021deep, li2018learning, liao2019efficient} generate graphs \emph{node-by-node}; for each step, they observe a partially decoded subgraph, then estimate the next node and its edges.
Instead, we present an alternative method called \emph{token-by-token} generation with Tokenized Graph Transformer (TokenGT, see Figure \ref{fig:tokengt}) \cite{kim2022pure}. TokenGT uses a vanilla transformer \cite{vaswani2017attention} and treats both nodes and edges as input tokens. 
It introduces token type embeddings that distinguish the nodes and edges and node id embeddings to describe the connectivity (we additionally have node/edge type embeddings).
This model architecture allows us to (i) alleviate the potential information bottleneck problem due to the sparse audio processing graph and (ii) frame the prototype decoding as a canonical autoregressive sequence generation task.
To decode the graph, we first add a start-of-graph token $\mathrm{x_S}$ to the empty sequence. Then, we estimate the following graph tokens $\mathrm{x}_1, \cdots, \mathrm{x}_N$ one by one with prediction heads for the token type, node id, node type, and edge type. 
The decoding starts with the \texttt{[out]} node, then follows the breadth-first search (BFS) order. For the drum graphs, we decode the mixing subgraph first; then, we decode each individual source track subgraph one by one in BFS order. We finish the decoding when the token type estimator outputs an end-of-graph token $\mathrm{x_E}$. To condition the reference latent $z$, we concatenate it with the other input tokens. 

\noindent \textbf{Parameter Estimator.} 
We reuse the prototype decoder for the parameter estimation; we add another projection head, append a task token $\mathrm{x}_\mathrm{T}$ to differentiate the two tasks, and remove the causal attention mask. Since each parameter has a different range and scale, we translate and rescale the ground-truth value to fit into $[0, 1]$ range.

\noindent \textbf{Architecture Details.} 
The FFT size, hop length, and the number of Mel filter banks of the reference Mel spectrogram are $1536$, $384$, and $256$, respectively. The convolutional backbone is a VGGish model \cite{hershey2017cnn} with the following modifications: (i) depthwise separable convolutions \cite{chollet2017xception}, (ii) channels divided into four groups with dilations of $[1$, $2$, $4$, $8]$, (iii) and the use of layer normalization \cite{ba2016layer}.
We used a transformer decoder layer with $1$ (singing) and $6$ (drum) queries for the pooling.
For the graph decoder, we use the $6$-layer transformer encoder with the pre-layer normalization \cite{xiong2020layer} and $16$ heads. While the original paper used eigenvectors of the normalized Laplacian matrix as node id encoding, we use the sinusoidal embeddings since the eigenvectors are intractable during the decoding.

\noindent \textbf{Training.} We train the prototype decoding task in a teacher-forcing manner using the cross-entropy losses with label-smoothing of $0.1$. At the same time, we train the parameter estimation task by feeding an oracle prototype $G_0$ as input and using the $l_1$ distance as an objective. We use AdamW \cite{loshchilov2017decoupled} optimizer, a linear learning rate scheduler with \texttt{5e-4} peak learning rate, $50\si{k}$ warmup steps, $200\si{k}$ total training steps, and batch size of $32$.

\input{figures/4-tokenized_graph_transformer}