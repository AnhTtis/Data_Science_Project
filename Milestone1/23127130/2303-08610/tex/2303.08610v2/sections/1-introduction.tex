\vspace{-2.5mm}
\section{Introduction}
\vspace{-1.5mm}
A dry audio signal is rarely delivered to the end listeners ``as is"--- it goes through various processing steps to achieve desired auditory effects.  
Recent deep learning approaches have improved, replaced, and automated some parts of such processing pipelines with neural networks. Some focused on estimating the parameters of known conventional DSP systems \cite{ramirez2020blackbox, Mitcheltree_2021, engel2020ddsp, colonel2021reverse}, while others mimicked, augmented, or integrated the existing processors with neural networks \cite{ramirez2019general, neuralbiquads, steinmetz2020diffmixconsole}.  
These prior works consider one or a few types of processors with fixed signal routings. 
However, domain practitioners, e.g., musicians and audio engineers, typically connect multiple small audio processors in a flexible way to achieve the desired processing. 
If we consider these processors as nodes and the connections as edges, we can represent this procedure as an \emph{audio processing graph} $G$.
See Figure \ref{fig:graph} for an example. First, we feed an input dry signal $x$, e.g., singing voice, into the \texttt{[in]} node. Then, the \texttt{[crossover]} splits the signal into two, one with low-frequency components and the other with high-frequency components (written as \texttt{low} and \texttt{high}, respectively). The two signals are independently processed with their respective \texttt{[distortion]} modules, summed together with \texttt{[mix]}, and passed to the \texttt{[out]} node, resulting in an output signal $y=G(x)$. This graph $G$ performs so-called ``multiband distortion," and we can extend its functionality with additional processors and connections. For example, we can add \texttt{[stereo\_lfo]} to modulate the crossover frequency, making the distortion effect time-varying. This flexibility makes expressive processing possible while retaining full interpretability and controllability.

In this paper, we conduct a preliminary study on integrating the audio processing graph structure with neural networks. Specifically, we tackle a \emph{blind estimation} problem; from the reference audio $y$, we aim to reconstruct the original graph $G$. 
Our motivation for choosing this task is threefold. First, automated reverse engineering itself is a well-established task and has practical values \cite{colonel2021reverse}. Second, it provides concrete objective metrics, allowing us to evaluate baseline systems reliably. Finally, it could serve as an appropriate first step towards applying and extending the graph-based methods to other applications, which include automatic processing \cite{martinez2022automatic, koo2022remaster, stasis2017audio} and style transfer \cite{steinmetz2022styletransfer}.

The blind graph estimation task poses several challenges to overcome. First, there is no publicly available graph-audio paired dataset. Therefore, we first build a \emph{synthetic dataset} on our own. Second, audio processing graphs are heterogeneous; each node and edge have a type and parameters as attributes, making them nontrivial to decode. To mitigate this, we propose a two-stage decoding method consisting of (i) \emph{prototype graph decoding}, which estimates graph connectivity and node/edge types, and (ii) \emph{parameter estimation}, which decides the remaining parameters. We achieve this with a convolutional reference encoder \cite{hershey2017cnn} and a transformer-based graph decoder \cite{vaswani2017attention, kim2022pure}. 

Finally, we apply our method to singing voice effects and drum mixing estimation tasks. The evaluation results show that, while our model fails to perfectly reconstruct the original graph in most cases, it preserves essential local/global graph structures, and rendered audio from the estimated graph can be perceptually similar to the reference. We also show that the proposed graph decoding strategy is preferable to other possible alternatives.

\input{figures/1-audio_processing_graph_example}

