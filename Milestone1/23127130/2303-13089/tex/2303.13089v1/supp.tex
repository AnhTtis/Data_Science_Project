\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\vspace{-3em}
\begin{center}
    \centering
    \Large{\textbf{Box-Level Active Detection -- Supplementary Material}}
\end{center}%
  \begin{figure}[H]
  \setlength{\linewidth}{\textwidth}
  \setlength{\hsize}{\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{figures/vis_score_voc.pdf}
  \caption{Reference generated by the chairman model (1st column with boxes in green and box-level acquisition scores), and corresponding member predictions (2nd-5th columns with boxes in blue). Experiments are performed at the first active learning cycle under the VOC-sup setting. We only show top-ranking member predictions for ease of visualization.}
  \label{fig:ref_voc}
  \end{figure}
\vspace{1em}
}]

{\hypersetup{linkcolor=blue}
\tableofcontents}

\begin{figure*}[ht]
\centering
\includegraphics[width=1\textwidth]{figures/vis_score_coco.pdf}
\caption{Reference generated by the chairman model (1st column with boxes in green and box-level acquisition scores), and corresponding member predictions (2nd-5th columns with boxes in blue). Experiments are performed at the first active learning cycle under the COCO-sup setting. We only show top-ranking member predictions for ease of visualization.}
\label{fig:ref_coco}
\end{figure*}
  
\begin{table*}[htp]
\centering
\resizebox{1\linewidth}{!}{ 
\begin{tabular}{c|ccccccccccc}
\toprule
sComPAS Ablations & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & Inference Time (s) \\
\midrule
Members2Members & 68.00$\pm$0.20 & 73.15$\pm$0.41 & 75.85$\pm$1.26 & 78.00$\pm$0.20 & 78.60$\pm$0.20 & 79.50$\pm$0.34 & 80.00$\pm$0.06 & 80.55$\pm$0.27 & 81.10$\pm$0.20 & 81.20$\pm$0.20 & 0.6739 \\
DetectorRef2Members & 68.00$\pm$0.20 & 73.40$\pm$0.34 & 76.35$\pm$0.55 & 77.70$\pm$0.34 & 78.75$\pm$0.41 & 79.50$\pm$0.20 & 79.75$\pm$0.13 & 79.95$\pm$0.27 & 80.10$\pm$0.14 & 80.70$\pm$0.28 & 0.1656 \\
ChairmanRef2Members (ours) & 68.00$\pm$0.20 & \textbf{73.57$\pm$0.57} & \textbf{76.40$\pm$0.30} & \textbf{78.23$\pm$0.23} & \textbf{79.43$\pm$0.06} & \textbf{80.30$\pm$0.17} & \textbf{80.97$\pm$0.06} & \textbf{81.37$\pm$0.15} & \textbf{81.73$\pm$0.12} & \textbf{82.13$\pm$0.25} & 0.1667 \\
\bottomrule
\end{tabular}}
\caption{Ablation study on the existence and the source of the acquisition reference under the VOC-sup setting. The accuracy (\%) and time consumption are averaged over 3 runs. Inference Time in seconds denotes the average forward time per image during the acquisition stage.}
\label{tab:ref}
\end{table*}




\section{Efficacy of the Acquisition Reference}
In this section, we validate the use of acquisition reference in terms of both accuracy and efficiency. The comparison is conducted under the VOC-sup setting on the same server with 4 NVIDIA RTX 3090. We set the committee size as 10 in accordance with our main results.

\textbf{The existence of the reference}. 
Without the reference, the \textit{Members2Members} variant constructs a committee for each instance by traversing every prediction from all other members, as done in WhiteBoxQBC~\citesupp{suproy2018qbc}.
In contrast, with a reference incorporated, the assignment procedure can be reduced to comparing between it and other member hypotheses.
As shown in Tab.~\ref{tab:ref}, if facilitated by the reference, such as in our \textit{ChairmanRef2Members}, the inference for acquisition takes about 0.1667 seconds per image in practice. It is approximately $4\times$ faster than the Members2Members ablation, which indicates that the reference is essential when scaling up to larger datasets.
Besides, since the augmentations used to construct the committee is diverse and relatively strong, basing the informativeness estimation solely on member outputs is susceptible to noise and randomness. As the detection accuracy shows, with a robust and reliable reference, the quality of active sampling consistently improves over cycles.

\textbf{The source of the reference}. 
To obtain reliable references as query candidates, the original image is fed into the chairman model, whose predictions are adopted as reference in our proposed ComPAS design. For comparison with the chairman, we experiment with the \textit{DetectorRef2Members} variant, where the reference is obtained from the detector itself instead of its temporal ensemble. As the results in Tab.~\ref{tab:ref} show, with almost equal computational cost, the quality of chairman-generated references beat the non-EMA alternative by a large margin.


\textbf{Visual inspection of our reference}. In Fig.~\ref{fig:ref_voc} and Fig.~\ref{fig:ref_coco}, we visualize top-ranked reference hypotheses according to our disagreement scores, as well as member predictions assigned to them. The experiment is performed at the 1st cycle under the VOC-sup and COCO-sup settings respectively. For ease of visualization, we trim member predictions based on their scores, \ie, contributions to the acquisition score of the reference box, so that only top-ranking box predictions are shown. 
We observe that the chairman model can recognize well-learned targets and locate some salient objects as reference in a stable manner, whereas those perturbed members can bring considerable variations and randomness. Member predictions challenge the judgments of the chairman or supplement with potential candidates, so that controversial regions of the input space can be found. 
For example, in Fig.~\ref{fig:ref_voc}, the bird in the first row obtains a consensus, whereas the branch next to it is mistakenly recognized by the chairman as a bird with spread wings, which is disputed by the committee. Similar observations can also be made from the camera held by the girl shown in Fig.~\ref{fig:ref_coco}.
Besides, the proposed metric also prioritizes targets that are challenging to localize. Take the 3rd row in Fig.~\ref{fig:ref_coco} for example: the committee has a disagreement over the train body, which leads to a higher acquisition score.
The qualitative results further indicate that the proposed disagreement quantification under strong variations well identifies the input space where the current model neglects. Once actively annotated, they can effectively provide informativeness and guarantee consistent improvements in later model updates. 

\begin{table}[!t]\small
    \centering
    \resizebox{0.7\linewidth}{!}{%
    \begin{tabular}{c|ccc}
        \hline
        \multirow{2}{*}{Cycle} & \multicolumn{3}{c}{M} \\ \cline{2-4} 
        & 1 & 4 & 10 \\ \hline
        0 & 68.00$\pm$0.20 & 68.00$\pm$0.20 & 68.00$\pm$0.20 \\
        1 & 72.97$\pm$0.35 & \textbf{73.83$\pm$0.47} & 73.57$\pm$0.57 \\
        2 & 76.37$\pm$0.21 & \textbf{76.83$\pm$0.32} & 76.40$\pm$0.30 \\
        3 & 78.10$\pm$0.46 & 78.17$\pm$0.38 & \textbf{78.23$\pm$0.23} \\
        4 & 78.93$\pm$0.31 & 79.20$\pm$0.10 & \textbf{79.43$\pm$0.06} \\
        5 & 79.67$\pm$0.31 & 79.90$\pm$0.35 & \textbf{80.30$\pm$0.17} \\
        6 & 80.33$\pm$0.12 & 80.50$\pm$0.35 & \textbf{80.97$\pm$0.06} \\
        7 & 80.87$\pm$0.47 & 80.83$\pm$0.25 & \textbf{81.37$\pm$0.15} \\
        8 & 81.37$\pm$0.12 & 81.33$\pm$0.23 & \textbf{81.73$\pm$0.12} \\
        9 & 81.70$\pm$0.26 & 81.83$\pm$0.15 & \textbf{82.13$\pm$0.25} \\
        \hline
    \end{tabular}%
    }
    \caption{Sensitivity to the input-end committee size under the VOC-sup setting.}
    \label{table:size}
\end{table}

\section{Sensitivity to the Input-end Committee Size}
As there is no consensus on the appropriate committee size to use~\citesupp{supsettles2012active}, we experiment under the VOC-sup setting with a varying number of members $M$. As shown in Tab.~\ref{table:size}, although one member prediction can work well under the proposed pipeline, providing more data variations on the input-end helps the model identify more informative and representative samples and provides robustness, which guarantees further improvements and stability in a cheap but effective way.


 

\section{Comparison with Ensemble-based Methods}

We further compare the proposed ComPAS with well-performed ensemble-based methods, including Ensemble~\citesupp{supbeluch2018ensemble} and MCDropout~\citesupp{supgal2017mcdropout}. Following \citesupp{supchoi2021gmm}, those multi-model methods are implemented based on MeanEntropy, which is also the best single-model baseline in our experiments.
The detection accuracy has been presented in Fig.~1, Fig.~3 and Tab.~1 of the main paper, and the numerical results of figures are reported in Tab.~\ref{tab:voc} and Tab.~\ref{tab:coco} respectively. In Tab.~\ref{tab:efficiency}, we detail the size of the ensemble/committee, 
the required training time and the inference time per image for informativeness estimation. The experiments are conducted under the VOC-sup setting on the same server with 4 NVIDIA RTX 3090. 

As can be seen, even when there is only one member, \ie $M=1$, our method retains its overall supremacy in both effectiveness and efficiency.
Built upon it, we provide flexibility in the committee size to suit the needs of different end applications. With more members incorporated, better detection performance can be further pursued via more input variations. Since the committee construction only happens during the acquisition stage, and image perturbations can be processed in one feed-forward pass in practice, the extra costs incurred are marginal in contrast to ensemble~\citesupp{supbeluch2018ensemble} and MCDropout~\citesupp{supgal2017mcdropout}.



\begin{table}[tp]
\centering
\resizebox{1\linewidth}{!}{ 
\begin{tabular}{c|ccc}
\toprule
Methods & M & \#Trainable Parameters (M) & Inference Time (s)\\
\midrule
MeanEntropy-Ensemble    & 3  & 123.51 & 0.0310 \\
MeanEntropy-MCDropout   & 25 & 41.17  & 0.2400 \\
\midrule
\multirow{4}{*}{sComPAS (ours)}  & 1  & 41.17  & 0.0277 \\
            & 4  & 41.17  & 0.0741 \\
            & 10 & 41.17  & 0.1667 \\
            & 15 & 41.17  & 0.2582 \\
\bottomrule
\end{tabular}}
\caption[]{Comparison with the ensemble-based methods under the VOC-sup setting. $M$ represents the size of the ensemble/committee following the implementation of \citesupp{supchoi2021gmm,supbeluch2018ensemble}. The number of trainable parameters are reported in millions. Inference Time in seconds denotes the average forward time per image during the acquisition stage.}\label{tab:efficiency}
\end{table}

\begin{figure}[tp]
\centering
\includegraphics[width=0.35\textwidth]{figures/voc_weights.pdf}
\caption{Analysis of the loss weight ratio among different levels of supervision (full-labeled: partial-labeled(: unlabeled)) under the VOC-sup and VOC-semi settings.}
\label{fig:weights}
\end{figure}




\section{Analysis of Overall Loss Weights}
Our overall objective functions for both training settings give the same relevance to different levels of supervision. To analyze the importance of the fully labeled subset versus others, we specifically finetune the weight of them under both VOC-sup and VOC-semi settings.

Results in Fig.~\ref{fig:weights} show that fully labeled images are more informative when they dominate the data pool, but increasing their relevance cannot guarantee consistent improvement as the distribution changes along learning cycles. In contrast, re-weighting losses by the sample ratio keeps the method simple but effective.

\section{Performance under Different Active Learning Settings}
As active learning is known to be sensitive to settings~\citesupp{supmunjal2022RobustReproducibleActive}, in this section, we validate our observations under different scenarios.


\begin{figure}[tp]
\centering
\hspace*{-0.5cm}\includegraphics[width=.35\textwidth, trim={0 0.2cm 0 0.2cm}, clip]{figures/voc_lowdata.pdf}
\caption{Weaker initialization (cycle0) and the started state (cycle1) under lower-data regimes under the VOC-sup setting.}
\label{fig:lowdata}
\end{figure}

\textbf{Performance with weaker starting points}. 
In Fig.~\ref{fig:lowdata}, we start the detector under lower-data regimes without overfitting. Results of the initialization with 3K, 2K and 1K boxes and of the corresponding started states demonstrate that our active sampling strategy is robust to weaker initialization and outperforms competitors. 

\textbf{Performance with a larger acquisition batch size}.
We conduct experiments on the COCO dataset under the COCO-sup setting with a larger acquisition batch size. The iterations are initialized with 50K boxes, the same as the experiments shown in Fig.~3M of the main paper. In each active learning cycle, we append 40K boxes based on respective query and annotation strategies. 
Results of three independent runs are plotted in Fig.~\ref{fig:coco54} and numerically detailed in Tab.~\ref{tab:coco54} respectively, which show that the proposed method is superior regardless of the acquisition batch size.

\begin{figure}[tp]
\centering
\hspace*{-0.5cm}\includegraphics[width=.35\textwidth, trim={0 0.2cm 0 0.2cm}, clip]{figures/coco_labeled_54.pdf}
\caption{Box-level comparative results on COCO-sup with a 40K per-cycle budget.}
\label{fig:coco54}
\end{figure}

\begin{figure*}[p]
\centering
\includegraphics[width=1\textwidth]{figures/vis_cycle.pdf}
\caption{Iteration of pseudo labels across active learning cycles accompanied by actively annotated targets. For each set of images, we highlight the image in red frame if the active annotation happens in that step.}
\label{fig:PseudoEvolve}
\end{figure*}

\begin{table*}[t]
\centering
\resizebox{1\linewidth}{!}{ 
\begin{tabular}{c|cccccccccc}
\toprule
\multirow{2}{*}{Methods} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
\cmidrule{2-11}
 & \multicolumn{10}{c}{VOC-sup}\\
\midrule
Random & 68.00$\pm$0.20 & 71.10$\pm$0.56 & 73.03$\pm$0.60 & 74.23$\pm$0.38 & 75.93$\pm$0.31 & 77.07$\pm$0.23 & 77.77$\pm$0.21 & 78.60$\pm$0.10 & 79.23$\pm$0.15 & 79.80$\pm$0.10 \\
MeanEntropy & 68.00$\pm$0.20 & 72.60$\pm$0.44 & 74.87$\pm$0.50 & 76.33$\pm$0.45 & 77.83$\pm$0.31 & 78.77$\pm$0.31 & 79.37$\pm$0.15 & 80.30$\pm$0.20 & 80.73$\pm$0.32 & 81.00$\pm$0.20 \\
WhiteBoxQBC & 68.00$\pm$0.20 & 71.37$\pm$0.85 & 73.23$\pm$0.31 & 74.33$\pm$0.47 & 75.30$\pm$0.78 & 76.73$\pm$0.74 & 77.17$\pm$0.68 & 78.13$\pm$0.45 & 78.67$\pm$0.65 & 79.20$\pm$0.26 \\
CoreSet & 68.00$\pm$0.20 & 71.20$\pm$0.50 & 73.23$\pm$0.32 & 75.00$\pm$0.53 & 76.03$\pm$0.35 & 77.17$\pm$0.47 & 77.97$\pm$0.40 & 78.93$\pm$0.42 & 79.47$\pm$0.45 & 79.87$\pm$0.31 \\
LearningLoss* & 66.70$\pm$0.89 & 68.10$\pm$1.25 & 69.77$\pm$0.87 & 70.43$\pm$0.81 & 71.33$\pm$0.87 & 72.47$\pm$0.59 & 72.97$\pm$0.45 & 73.57$\pm$0.51 & 74.57$\pm$0.40 & 74.73$\pm$0.91 \\
ALMDN* & 67.50$\pm$0.71 & 70.00$\pm$0.28 & 70.85$\pm$0.07 & 72.05$\pm$0.64 & 73.10$\pm$0.71 & 73.65$\pm$0.92 & 74.35$\pm$0.92 & 75.25$\pm$0.92 & 75.45$\pm$1.06 & 76.35$\pm$0.78 \\
MeanEntropy-MCDropout* & \textbf{68.87$\pm$0.59} & 73.17$\pm$0.25 & 75.27$\pm$0.15 & 76.90$\pm$0.20 & 78.07$\pm$0.35 & 79.00$\pm$0.10 & 79.70$\pm$0.00 & 80.40$\pm$0.35 & 80.77$\pm$0.15 & 81.40$\pm$0.35 \\
MeanEntropy-Ensemble* & 67.87$\pm$0.23 & 72.07$\pm$0.25 & 74.70$\pm$0.20 & 75.80$\pm$0.44 & 77.30$\pm$0.10 & 78.43$\pm$0.31 & 79.30$\pm$0.17 & 79.80$\pm$0.17 & 80.70$\pm$0.26 & 81.13$\pm$0.15 \\
BoxCnt & 68.00$\pm$0.20 & 69.87$\pm$0.32 & 71.20$\pm$0.56 & 72.27$\pm$0.45 & 73.20$\pm$0.56 & 74.00$\pm$0.17 & 74.83$\pm$0.32 & 75.53$\pm$0.49 & 76.50$\pm$0.46 & 77.17$\pm$0.32 \\
sCOMPAS (ours) & 68.00$\pm$0.20 & \textbf{73.57$\pm$0.57} & \textbf{76.40$\pm$0.30} & \textbf{78.23$\pm$0.23} & \textbf{79.43$\pm$0.06} & \textbf{80.30$\pm$0.17} & \textbf{80.97$\pm$0.06} & \textbf{81.37$\pm$0.15} & \textbf{81.73$\pm$0.12} & \textbf{82.13$\pm$0.25} \\
\midrule
 & \multicolumn{10}{c}{VOC-semi}\\
\midrule
ActiveTeacher & 77.80$\pm$1.15 & {78.84$\pm$0.27} & {79.24$\pm$0.31} & {80.02$\pm$0.52} & {80.27$\pm$0.28} & {80.62$\pm$0.39} & {80.90$\pm$0.40} & {81.26$\pm$0.12} & {81.56$\pm$0.40} & {82.01$\pm$0.10} \\
mCOMPAS (ours) & \textbf{79.23$\pm$0.51} & \textbf{79.97$\pm$0.21} & \textbf{80.63$\pm$0.61} & \textbf{81.37$\pm$0.31} & \textbf{81.97$\pm$0.31}  & - &  - & -  & -  & - \\
\bottomrule
\end{tabular}}
\caption{MAP50 and standard deviation (\%) under VOC-sup and VOC-semi settings, with 3K boxes for initialization and 1K boxes for each active learning cycle. Results reported are averaged over 3 independent runs. We stop the learning of mCOMPAS after it far exceeds the fully supervised performance (81\% mAP50). The best result of each cycle is highlighted in bold. }
\label{tab:voc}
\end{table*}


\section{Pseudo-Active Synergy Visualization over Active Learning Cycles}
In Fig.~\ref{fig:PseudoEvolve}, We present the iteration of pseudo-labels predicted by the chairman model accompanied by active human annotations across the learning cycles. 
Images are highlighted in red frames if the active annotation happens in those steps.
We find that the proposed acquisition function prioritizes challenging targets, such as small, occluded (\eg cars in the 1st and 4th images) or deviant (\eg the cow in the last images mistakenly recognized as a horse) ones. Meantime, most unlabeled targets can be covered by pseudo-label generation, which gets better in both classification and localization across active learning cycles.
Through iterative knowledge gain and self-supervision, the synergy between them is effectively exploited.



\section{Implementation Details}
In addition to the general settings of our unified codebase introduced in the main paper, we detail the re-implementation of compared methods in this section.

\textbf{FullSup}. For reference, we also report the fully supervised (FS) performance, denoted by FullSup, given the same model, augmentations and runtime settings as in AL experiments. Thus $r$\% Sup refers to $r$\% of the FS performance instead of a data split.

\textbf{MeanEntropy}. 
After NMS (Non-maximal suppression), we calculate the entropy of a box candidate based on the predictive probability of its most confident class. Then the uncertainty scores are averaged over the image.

\textbf{WhiteBoxQBC}~\citesupp{suproy2018qbc}. Following the Algorithm. 1 provided in the paper, we take the NMS outputs of the multiple scales of Faster R-CNN to construct the `committee', among which all classes and pair-wise bounding box predictions are traversed to estimate the image uncertainty based on predictive \textit{margin}.

\textbf{CoreSet}~\citesupp{supsener2018coreset}. We apply global average pooling on the multi-level features extracted by the Feature Pyramid Network(FPN) of Faster R-CNN, which are then concatenated as the latent space representation of an image. We implement the k-Center-Greedy algorithm to select unlabeled images during the acquisition stage.

\textbf{LearningLoss}~\citesupp{supyoo2019learningloss}. The multi-level features extracted by the Feature Pyramid Network (FPN) of Faster R-CNN are fed into the loss prediction module. The gradient from the loss prediction module is stopped at $0.8\times$ total iterations following the implementation of the paper. We finetune the weight of the learned loss and use 0.3 in consideration of convergence and better performance.

\begin{table*}[ht]
\centering
\resizebox{1\linewidth}{!}{ 
\begin{tabular}{c|cccccccccc}
\toprule
\multirow{2}{*}{Methods} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
\cmidrule{2-11}
 & \multicolumn{10}{c}{COCO-sup}\\
\midrule
Random & 22.50$\pm$0.10 & 24.13$\pm$0.12 & 24.73$\pm$0.06 & 25.37$\pm$0.21 & 25.90$\pm$0.17 & 26.37$\pm$0.15 & 26.67$\pm$0.06 & 27.23$\pm$0.06 & 27.67$\pm$0.15 & 27.87$\pm$0.23 \\
MeanEntropy & 22.50$\pm$0.10 & 25.07$\pm$0.29 & 25.70$\pm$0.26 & 26.40$\pm$0.26 & 26.93$\pm$0.25 & 27.30$\pm$0.30 & 27.77$\pm$0.38 & 28.17$\pm$0.25 & 28.33$\pm$0.32 & 28.73$\pm$0.35 \\
WhiteBoxQBC & 22.50$\pm$0.10 & 24.03$\pm$0.15 & 24.93$\pm$0.15 & 25.50$\pm$0.10 & 26.10$\pm$0.26 & 26.47$\pm$0.06 & 27.03$\pm$0.32 & 27.53$\pm$0.40 & 27.97$\pm$0.42 & 28.33$\pm$0.47 \\
CoreSet & 22.50$\pm$0.10 & 24.23$\pm$0.25 & 25.00$\pm$0.44 & 25.40$\pm$0.17 & 25.93$\pm$0.25 & 26.47$\pm$0.31 & 26.90$\pm$0.17 & 27.30$\pm$0.10 & 27.63$\pm$0.29 & 28.03$\pm$0.15 \\
LearningLoss* & \textbf{23.27$\pm$0.25} & 23.63$\pm$0.23 & 23.90$\pm$0.26 & 24.03$\pm$0.23 & 24.20$\pm$0.20 & 24.33$\pm$0.06 & 24.53$\pm$0.06 & 24.80$\pm$0.20 & 24.90$\pm$0.20 & 25.13$\pm$0.21 \\
MIAOD* & 20.70$\pm$0.28 & 22.90$\pm$0.14 & 23.70$\pm$0.57 & 24.70$\pm$0.42 & 25.00$\pm$0.42 & 25.45$\pm$0.78 & 25.85$\pm$0.64 & 26.30$\pm$0.57 & 26.85$\pm$0.35 & 27.30$\pm$0.28 \\
ALMDN* & 22.93$\pm$0.21 & 23.83$\pm$0.67 & 23.90$\pm$0.35 & 23.97$\pm$0.25 & 24.23$\pm$0.38 & 24.47$\pm$0.38 & 24.60$\pm$0.26 & 24.90$\pm$0.17 & 25.23$\pm$0.35 & 25.73$\pm$0.35 \\
MeanEntropy-MCDropout* & 23.00$\pm$0.17 & 25.60$\pm$0.36 & 26.37$\pm$0.31 & 26.90$\pm$0.00 & 27.53$\pm$0.06 & 27.93$\pm$0.12 & 28.23$\pm$0.15 & 28.67$\pm$0.06 & 29.23$\pm$0.25 & 29.37$\pm$0.21 \\
MeanEntropy-Ensemble* & 22.53$\pm$0.12 & 24.67$\pm$0.15 & 25.27$\pm$0.25 & 25.80$\pm$0.10 & 26.37$\pm$0.12 & 26.73$\pm$0.06 & 27.10$\pm$0.10 & 27.27$\pm$0.06 & 27.63$\pm$0.15 & 27.97$\pm$0.25 \\
sCOMPAS (ours) & 22.50$\pm$0.10 & \textbf{26.25$\pm$0.35} & \textbf{28.30$\pm$0.14} & \textbf{29.75$\pm$0.35} & \textbf{30.75$\pm$0.35} & \textbf{31.45$\pm$0.21} & \textbf{32.20$\pm$0.14} & \textbf{32.65$\pm$0.21} & \textbf{33.15$\pm$0.07} & \textbf{33.50$\pm$0.00} \\
\midrule
 & \multicolumn{10}{c}{COCO-semi}\\
\midrule
ActiveTeacher & 29.16$\pm$0.10 & 30.57$\pm$0.04 & 31.09$\pm$0.01 & 31.61$\pm$0.07 & 31.90$\pm$0.01 & 32.21$\pm$0.13 & 32.44$\pm$0.02 & 32.66$\pm$0.05 & 32.81$\pm$0.08 & 33.01$\pm$0.16 \\
mCOMPAS (ours) & \textbf{32.30$\pm$0.10} & \textbf{33.07$\pm$0.06} & \textbf{33.60$\pm$0.00} & \textbf{33.83$\pm$0.06} & \textbf{34.30$\pm$0.20} & \textbf{34.70$\pm$0.10} & \textbf{35.00$\pm$0.10} & \textbf{35.30$\pm$0.17} & \textbf{35.50$\pm$0.10} & \textbf{35.73$\pm$0.12} \\
\bottomrule
\end{tabular}}
\caption{MAP and standard deviation (\%) under COCO-sup and COCO-semi settings, with 50K boxes for initialization and 20K boxes for each active learning cycle. Results reported are averaged over 3 independent runs. The best result of each cycle is highlighted in bold. }
\label{tab:coco}
\end{table*}

\textbf{ALMDN}~\citesupp{supchoi2021gmm} Following the code published by the authors, we turn the detection heads of Faster R-CNN into four Gaussian mixture models (GMMs), and take the maximum over epistemic and aleatoric uncertainty for classification and localization as the score of an unlabeled image.

\textbf{MIAOD}~\citesupp{supyuan2021miaod} Following the code published by the authors, the second detection head and an additional multi-instance learning (MIL) head are built upon Faster R-CNN, and the disagreement between the two classification branches is used as an uncertainty indicator. The weight of MIL is finetuned as 0.1 in consideration of convergence and better performance. 
The results of MIAOD obtained from our re-implementation significantly surpass those reported by the authors on the COCO dataset.
However, under the VOC setting, consistent improvement cannot be assured over the active learning cycles after parameter search, among which the best performance we can get is no more than 72\% mAP50. Experiments with the code provided by the authors under the same budget also confirm our observation, where we only get around 70\% mAP50 in the last cycle. This might suggest that MIAOD is less applicable to the low-data regime. Thus, the results of MIAOD on the VOC dataset are not reported in the main paper.

\textbf{MCDropout}~\citesupp{supgal2017mcdropout}. Following the practice in \citesupp{supchoi2021gmm}, the adaption of MCDropout to the detection task is achieved by image-level estimation followed by averaging the results of 25 forward passes. Dropout layers with $p=0.1$ are inserted at the last two stages after each bottleneck module of the ResNet backbone.
The image-level informativeness can be estimated by different acquisition functions. Here we also follow the implementation in \citesupp{supchoi2021gmm} to use entropy, which performs the best under our experimental settings.

\textbf{Ensemble}~\citesupp{supbeluch2018ensemble}. Similarly, we establish an ensemble of three independent detectors following \citesupp{supchoi2021gmm}. The informativeness estimation and result ensemble are the same as the MCDropout implementation.

\textbf{BoxCnt}  BoxCnt is devised by us to attack the image-level estimation for active detection. This hack is achieved by naively prioritizing unlabeled images with the most number of box predictions after NMS.


\begin{table*}[ht]
\centering
\resizebox{0.9\linewidth}{!}{ 
\begin{tabular}{c|cccccccccc}
\toprule
\multirow{2}{*}{Methods} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7  \\
\cmidrule{2-9}
 & \multicolumn{8}{c}{COCO-sup}\\
\midrule
    Random & 22.50$\pm$0.10 & 25.97$\pm$0.15 & 27.37$\pm$0.21 & 28.23$\pm$0.15 & 29.17$\pm$0.21 & 29.87$\pm$0.06 & 30.53$\pm$0.15 & 31.10$\pm$0.10 \\
    MeanEntropy & 22.50$\pm$0.10 & 26.80$\pm$0.20 & 27.93$\pm$0.23 & 28.80$\pm$0.26 & 29.70$\pm$0.30 & 30.37$\pm$0.15 & 30.93$\pm$0.15 & 31.37$\pm$0.06 \\
    WhiteBoxQBC & 22.50$\pm$0.10 & 26.00$\pm$0.20 & 27.43$\pm$0.23 & 28.30$\pm$0.20 & 29.50$\pm$0.46 & 30.13$\pm$0.49 & 30.87$\pm$0.38 & 31.43$\pm$0.32 \\
    CoreSet & 22.50$\pm$0.10 & 26.03$\pm$0.35 & 27.47$\pm$0.40 & 28.33$\pm$0.23 & 29.33$\pm$0.25 & 30.03$\pm$0.23 & 30.63$\pm$0.21 & 31.27$\pm$0.12 \\
    LearningLoss* & \textbf{23.20$\pm$0.20} & 24.07$\pm$0.21 & 24.87$\pm$0.12 & 25.60$\pm$0.17 & 26.33$\pm$0.21 & 27.27$\pm$0.21 & 27.80$\pm$0.20 & 28.47$\pm$0.40 \\
    MIAOD* & 19.10$\pm$0.36 & 22.37$\pm$1.18 & 23.80$\pm$0.98 & 24.83$\pm$0.83 & 25.43$\pm$0.81 & 26.43$\pm$0.38 & 27.00$\pm$0.56 & 27.47$\pm$0.60 \\
    ALMDN* & 22.93$\pm$0.21 & 24.63$\pm$0.45 & 25.23$\pm$0.38 & 25.73$\pm$0.25 & 26.20$\pm$0.44 & 26.80$\pm$0.36 & 27.60$\pm$0.30 & 28.13$\pm$0.35 \\
    MeanEntropy-MCDropout* & 23.13$\pm$0.06 & 27.20$\pm$0.20 & 28.27$\pm$0.06 & 29.23$\pm$0.12 & 29.97$\pm$0.21 & 30.80$\pm$0.10 & 31.50$\pm$0.20 & 31.90$\pm$0.10 \\
    MeanEntropy-Ensemble* & 22.33$\pm$0.06 & 26.33$\pm$0.23 & 27.23$\pm$0.15 & 28.07$\pm$0.46 & 28.80$\pm$0.26 & 29.57$\pm$0.12 & 29.97$\pm$0.06 & 30.57$\pm$0.06 \\
    sCOMPAS (ours) & 22.50$\pm$0.10 & \textbf{28.23$\pm$0.21} & \textbf{30.50$\pm$0.20} & \textbf{32.03$\pm$0.25} & \textbf{33.20$\pm$0.10} & \textbf{33.93$\pm$0.15} & \textbf{34.60$\pm$0.36} & \textbf{34.93$\pm$0.23} \\
\bottomrule
\end{tabular}}
\caption{MAP and standard deviation (\%) under COCO-sup and COCO-semi settings, with 50K boxes for initialization and 20K boxes for each active learning cycle. Results reported are averaged over 3 independent runs. The best result of each cycle is highlighted in bold. }
\label{tab:coco54}
\end{table*}


\section{Related Work Beyond Active Learning}
The main paper discusses widely used and most recent acquisition functions, ensemble models and evaluation methods for active learning. In addition to it, we give a more extensive survey of literature related to the proposed framework and method in this section.

\textbf{Consistency regularization}. 
Active learning estimates predictive consistency as an uncertainty indicator to sample unlabeled candidates, based on which images with the most inconsistent hypotheses are queried for human annotation. To this end, previous methods measure the \textit{disagreement} between multiple models or heads~\citesupp{supseung1992qbc,supbeluch2018ensemble,supyuan2021miaod,supchoi2021gmm,suproy2018qbc}, and the \textit{robustness} of the output after noise perturbation~\citesupp{supkao2018localizationaware} or horizontal flip~\citesupp{supelezi2022NotAllLabels} of the image.

While the consistency estimation of active learning mainly happens in the acquisition stage, quite similarly, semi-supervised learning (SSL)~\citesupp{supoliver2018realisticconsistency,supsohn2020simple,supjeong2019consistency,supxu2021softteacher} encourages consistency in the outputs of realistic image perturbations during training. The shared motivation behind them is to find a smooth manifold for the dataset so that the version space of the model is minimized~\citesupp{supsettles2012active,supbelkin2006manifold,supoliver2018realisticconsistency}.
Their goals are further aligned in \citesupp{supgao2020consistencyeccv} for active classification, where under the semi-supervised learning framework, Gao \etal sample images with inconsistent predictions that the model has difficulty self-learning via consistency regularization.
Most recent active detection methods also draw on advanced semi-supervised learning (SSL) techniques. For example, Mi \etal~\citesupp{supmi2022ActiveTeacherSemiSupervised} achieve active sampling under the framework of the unbiased teacher~\citesupp{supliu2021unbiased}, a state-of-the-art semi-supervised detection method to exploit all available data. But their informativeness estimation is based on entropy and class diversity, which is not aligned with the training objective. 
Elezi \etal~\citesupp{supelezi2022NotAllLabels} measures the Kullback-Leibler divergence between the predictive distributions of flipped images. They also provide offline pseudo-labels for unlabeled images with confident predictions, so that human-labeled, pseudo-labeled and unlabeled images are all involved in consistency-based model optimization. 

In comparison to previous studies, the proposed ComPAS method provides sufficient perturbations as the testing ground for disagreement estimation, aligns the consistency-oriented goal in model training and active acquisition, and supports both labeled-only and mixed-supervision learning.

\textbf{Sparse annotation for object detection}. Sparsely annotated object detection (SAOD)~\citesupp{supwu2018soft,supniitani2019partaware,supzhang2020BRL,supwang2021comining,suprambhatla2022sparsely}, deals with the interference of unlabeled labels, which will be considered as hard negatives during the training of detectors. The methods in this field mainly fall into two classes: loss re-weighting and pseudo-labeling. Soft sampling~\citesupp{supwu2018soft} and Background Re-calibration~\citesupp{supzhang2020BRL} trust the judgements of the detector and accordingly adjust the weights of negative proposals. More recent methods draw on a Siamese network~\citesupp{supwang2021comining} or consistency regularization~\citesupp{suprambhatla2022sparsely} to generate pseudo-labels for unlabeled regions.

The proposed box-level active detection framework inevitably incurs the similar challenge. While SAOD methods are validated on randomly down-sampled benchmark datasets, our setting distinguishes itself in prioritizing the annotation of challenging targets while requiring remedies for the easier ones. 
Without specific handling, the performance of detection would be severely interfered. But meantime we can benefit from the prior knowledge of the unlabeled targets: pseudo-labeling that accepts confident model predictions can exactly supplement easier targets with self-supervision. 

\textbf{Mixed types of supervision for object detection}. 
Besides the box-level supervision we focus on in this paper, some recent work also includes additional image-level labels~\citesupp{supbiffi2020many,supzhong2022mixed,supfang2020ehsod} and more types of supervision (\eg scribbles in \citesupp{supren2020omniufo}) to facilitate the detection performance. 
For example, \citesupp{supbiffi2020many,supzhong2022mixed,supfang2020ehsod} utilize weakly labeled datasets and a proportion of fully labeled images.
In comparison, our box-only setting is simpler and more effective.
Take the VOC dataset for example, as presented in Tab.~\ref{tab:voc}, sCOMPAS obtains 73.57\% mAP50 with merely 8.5\% (4K) \emph{boxes}, and then achieves 76.4\% mAP50 with 10.6\% (5K) \emph{boxes}. 
Given access to all \emph{unlabeled images}, mCOMPAS reaches 79.97\%-80.63\% mAP50. 
It clearly surpasses the state-of-the-art 69.4\% mAP50 in \citesupp{supbiffi2020many,supzhong2022mixed,supfang2020ehsod}, where \emph{all image-level labels} and 10\% \emph{fully labeled images} are required.

Mixed types of supervision can also be sampled via active learning.
Based on a pre-trained weakly supervised detector, BiB~\citesupp{supBiB_eccv22} proposes to actively provide full box annotations for images. 
BAOD~\citesupp{suppardo2021baod} progressively adds image-level supervision or full box annotations in each active learning cycle. 
In contrast to their mixed types of supervision and exhaustive annotation protocol, the proposed pipeline is simple and effective in de-redundancy, and is superior in detection performance.


\section{Numerical Results}
Last, we present the exact numerical results used to plot Fig.~1 and Fig.~3 of the main paper and the results of Fig.~\ref{fig:coco54} in this supplementary file.
Tab.~\ref{tab:voc} reports mAP50 and standard deviation (\%) under the VOC-sup and VOC-semi settings. Tab.~\ref{tab:coco} presents results under COCO-sup and COCO-semi settings with the 20K box-level annotation batch size, and Tab.~\ref{tab:coco54} presents the results with a larger batch size of 40K boxes.




\newpage
{\small
\bibliographystylesupp{ieee_fullname}
\bibliographysupp{egbib_sup}
}
