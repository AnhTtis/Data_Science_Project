\documentclass[11pt]{article}

%\usepackage{setspace}
%\doublespacing
\usepackage{cite}
\usepackage{url}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{stfloats}
\usepackage{amsfonts,amssymb,amsmath,bm,paralist,theorem,cite,ifthen,color,wrapfig}
%\usepackage{amsthm}

\usepackage{array}
\usepackage{caption}
\usepackage{calc}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{fullpage}
\captionsetup[figure]{labelfont={small,rm,bf},labelsep=period,font={small,rm}}
\captionsetup[table]{labelfont={rm,bf},labelsep=period,belowskip=3pt}
\theorembodyfont{\rmfamily}
%\newcommand{\figurename}{\textbf{Figure}}
%

\usepackage{pifont} 
\newcommand{\cmark}{\ding{51}} 
\newcommand{\xmark}{\ding{55}} 

\newenvironment{Ventry}[1]%
{\begin{list}{}{
    \renewcommand{\makelabel}[1]{\mbox{\textnormal{##1}}\hfil}%
    \settowidth{\labelwidth}{\mbox{\textnormal{#1}}}%
    %\setlength{\itemsep}{-.4\baselineskip}%
    \setlength{\leftmargin}{\labelwidth+\labelsep}}}%
    %\setlength
{\end{list}}

\newtheorem{Lemma}{Lemma}
\newtheorem{Prop}{Proposition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Def}{Definition}
\newtheorem{Corollary}{Corollary}
\newtheorem{Property}{Property}
\theorembodyfont{\rmfamily}
\newtheorem{Exa}{Example}
\newtheorem{Rmk}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{Example}{Example}


\begin{document}

\bibliographystyle{IEEEtran}
%-------------
\begin{center}
{\large {\bf 
Learning An Active Inference Model of Driver Perception and Control:
Application to Vehicle Car-Following}} \\
\medskip by \\

\medskip
by Ran Wei,
Alfredo Garcia,
Anthony McDonald,
Gustav Markkula,
Johan Engstrom,
Matthew O'Kelly
\\
\medskip
\today
\end{center}


We thank the reviewers and the editorial team for their careful reading of our manuscript and
for the valuable comments and suggestions. In this document we provide our detailed
point-to-point responses. The main changes made are summarized below:

\begin{itemize}
\item We have provided the link to our implementation source code on Github and an appendix with implementation details. 
\item We have clarified the nature and extent of our claims in terms of performance comparisons to baseline models.
\item We have added additional plots of the predicted actions by different models and their trajectory deviations over time to illustrate more detailed model behavior.
\item We have provided a comparison between simple average of evaluation metrics vs. interquartile mean (IQM) to validate and provide further insight into our choice of IQM as the main metric aggregation method.
\item We have added additional text to clarify our model visualization and interpretability method.
\end{itemize}

\newpage



\section{Response to Reviewer 1's Comments}

\medskip
{\bf General Comments}
\medskip
\begin{enumerate}
\item { \color{blue} I have several concerns. The resulting model and its estimation methods are not specified in sufficient detail, and the reader can't - at least easily - tell what the concrete resulting model is and how it is exactly fitted to data. Several crucial details for reproducing the results and implementing the model are missing. This both hinders readers' understanding of the model and also assessment of the empirical model comparison results. I've detailed what is missing below. I strongly suggest that source code for reproducing the results would be made available.
 
Presentation of the simulation and validation results is somewhat hard to interpret, and the resulting model behavior is left somewhat unclear. I have also concerns about the methodology used to empirically assess and compare the model. Some of the conclusions don't seem to be unambiguously supported by the data, and some findings are interpreted a bit too favorably for the proposed model. I elaborate on these and suggest improvements below.
 
There is some non-standard terminology and definitions, detailed below.
 
The theorem proofs seem reasonable, but I can't say I've verified they are correct.}

{\bf Response}. We appreciate your meticulous review of our paper. We have made the source code available on Github (\url{https://github.com/ran-weii/interactive_inference}). In the revised version, we have added an appendix with details on the implementation.

Regarding the claims on model performance we have added text to better describe the nature and extent of our claims (see text in blue in the revised Introduction). Briefly, the naturalistic dataset used in the paper only includes relatively few extreme observations (e.g. extremely low relative distance and/or high relative velocity) and no collisions. Thus, the learned model from the dataset provides an account of human drivers' perception and control in {\em average} conditions. For this reason, our testing of model performance focuses on {\em aggregate} measures. We report the higher rate of collisions exhibited by our model and argue that this is due to ``distribution shift", i.e. the mismatch between the data used to train the model and the data used in the testing scenario. In other words, the learned representation of the world is not accurate in extreme conditions (e.g. extremely low relative distance and/or high relative velocity) and therefore induces collisions. This is a common problem for offline reinforcement learning models as it is often not possible to learn state dynamics in operating conditions that are not represented in the data \cite{Wei_2024}.

\end{enumerate}

\medskip
\noindent
{\bf Specific Comments}
\medskip

\begin{enumerate}
    \item {\color{blue} The abstract states ``According to active inference, the agent acts upon the world so as to minimize surprise defined as the sum of a measure of mismatch between perceived vs preferred states of the world and a measure of uncertainty in expected outcomes".
 
I think this is too specific to characterize the overall active inference concept. I think the surprise definition``how much an agent’s current sensory observations differ from its preferred sensory observations" by Parr et al [21] is the most commonly accepted.}

\medskip
{\bf Response}. We have changed the abstract to reflect the definition of surprise in Parr et al [21].

\item {\color{blue} ``The Bayesian brain hypothesis [3], [4] posits that the human brain represents sensory information in the form of probability distributions over a lower dimensional representation of the environment."
 
The Bayesian brain hypothesis does not state about the dimensionalities of the states (and the dimensionalities are not discussed in [3] nor [4]). The dimensionality asymmetry between sensory information and environment representation probably holds in most real-world cases, but usually does not in experiments studying the Bayesian brain mechanisms. In these the sensory input models are usually quite simple. In many cases the internal representation is more complicated than the sensory input, to account for e.g. incorporating prior information.
I'd say the Bayesian Brain hypothesis is best described along the lines of [4]: ``The fundamental concept behind the Bayesian approach to perceptual computations is that the information provided by a set of sensory data about the world is represented by a conditional probability density function over the set of unknown variables – the posterior density function." and ``More generally, the component computations that underlay Bayesian inferences [that give rise to p(Z/I)] are ideally performed on representations of conditional probability density functions rather than on unitary estimates of parameter values."}

\medskip
{\bf Response:} We thank the reviewer for this observation. We have changed the text to accurately describe the Bayesian brain hypothesis without any allusion to the lower dimensional representations described in \cite{Badre_2021, DeBeek_2002}.

\item {\color{blue} "There is a significant literature on the structural estimation of human control when the state is observable [13]–[15]. In contrast, structural estimation when the state is only partially observable (as in models accounting for human perception) has received less attention. Notable exceptions include [16]–[19]. However, the environments considered in these papers are either low dimensional as in [16], [17], restricted to a linear-quadratic control [18] or customized for a specific control task [19].
 
I think in control theoretical literature ``system identification" or "model identification" are more standard terms for this (and the cited papers mostly use something like this). "Structural estimation" is fine too, but please help the reader by mentioning that this is what is meant.}

\medskip
{\bf Response:} In response to your comment, we have changed the text to refer to {\em model identification and estimation} instead of {\em structural} estimation.

\item {\color{blue} Figure 1 is rather difficult to interpret, especially panels c-d before the reader understands the proposed model. A lot more interpretable would be to show model-produced time-series trajectories and human driven trajectories with matching initial conditions from the Online Evaluation (i.e. the trajectories the model tries to predict). And perhaps scatterplots of model-produced vs human-produced accelerations from the Offline Evaluation.}

{\bf Response:} We have followed your suggestion and moved the figure to section {\small V.D} so that the readers can interpret the plot after they have understood our model. Furthermore, we have added additional figures to compare human and model generated predictions and trajectories in both the offline and online settings. We have copied these figures below in Figures \ref{fig:offline_pred} and \ref{fig:online_pred}.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs_rebuttal/offline_pred_dense.png}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs_rebuttal/offline_pred_sparse.png}
    \end{subfigure}
    \caption{Example offline predictions in the dense (same)-lane (left) and sparse (new)-lane (right) settings. Shading represents standard deviation of predictions.}
    \label{fig:offline_pred}
\end{figure}

\begin{figure}[!h]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs_rebuttal/online_pred_dense.png}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs_rebuttal/online_pred_sparse.png}
    \end{subfigure}
    \caption{ADE for each time step averaged over all online testing episodes for the dense (same)-lane (left) and sparse (new)-lane (right) settings.}
    \label{fig:online_pred}
\end{figure}

\item {\color{blue} Page 18, line 43: ``The substantial overlap between Figures 1a and 1b indicates that the learned model of perception and control produces observed features that are similar to those in the data.''
 I find this a rather strong interpretation of Figure 1a and 1b. The distributions of the values are qualitatively similar, but they are so for any car following model that does anything reasonable. For me, the first impression comparing panels a and b is that the fit is not great (especially the tails of the $\tau^{-1}$ distributions).}

{\bf Response:} Our goal is to provide an {\em interpretable} model (in terms of a cognitive model of perception and action) that exhibits a statistical performance that is similar or arguably superior to other black-box models. The naturalistic dataset does not include collisions and only includes relatively few extreme observations (e.g. extremely low relative distance or high relative velocity). From such a dataset, it is difficult to learn an accurate model for these tail conditions and impossible to learn the kinematics of a collision. Thus, the learned model is meant to provide an account of human drivers' perception and control in {\em average} conditions. 

% Hence, our testing of model performance focuses on {\em aggregate} measures (e.g. Average Deviation Error (ADE) and the corresponding interquartile mean (IQM) values). 
 
 %As recognized in the {\em offline} reinforcement learning literature \cite{Wei_2024}, with such limited data coverage it is likely that the learned model of perception of the world is quite inaccurate in extreme scenarios that are poorly covered in the dataset \cite{Zeng_2023}.  %Based on these measure, we show the active inference-based model performs well when compared to imitation learning-based models from the machine learning literature. 
 % However, the active inference model does exhibit higher collision rates than IQM when tested offline. This is expected because of the limitations of the dataset which poorly covers extreme scenarios.

 \item {\color{blue} Page 19, line 48: What ``the POMDP framework'' means is somewhat ambiguous. If you mean to refer to your specific model formulation, please use e.g. ``our POMDP framework'' or ``the proposed POMDP framewor'' to disambiguating from POMDP in general. Also page 20, line 10, (equation II.1) is not the general POMPD.
 
Lower dimensional hidden state is not a requirement in POMDP. In many cases it is of higher dimension and the POMDP (or HMM in simpler cases) is used to estimate the the dimensions that can't be directly observed.}

\medskip
{\bf Response:} In the revised version, we refer to the {\em proposed} POMDP framework of perception and action to emphasize its particular nature. We agree that lower dimensional representations are not a requirement in general POMDPs. However, we believe that in light of \cite{Badre_2021, DeBeek_2002} it is of interest to study lower dimensional representations in sensorimotor control tasks.

\item {\color{blue} Page 20, line 7: ``We consider randomized policies $\pi...$"
 The "randomized policy" here refers to the action selection being drawn from a distribution, instead of e.g. a greedy policy? I think the standard term for this is "stochastic policy", please use standard terminology when feasible.}

 {\bf Response:} In the jargon of Markov Decision Processes (MDPs) it is customary to equivalently refer to {\em randomized} or {\em stochastic} policies. We have added text in the revised version in this sense.

\item {\color{blue} Page 20, line 18: ``and $c(\pi(·|h_t ))$ is the per-period information processing cost...'' This cost function is not defined or mentioned beforehand. Please refer to it as "a per-perioid information processing cost" to indicate the reader that they are not expected to understand it yet.}

{\bf Response:} We have followed your suggestion in the revised version.

\item {\color{blue} Page 20, line 34: ``...where $\Delta S$ is the simplex in $R^M$''.
 It's not clear what "the simplex" means here (may be standard terminology I'm not aware of). Is $b_t$ of the same format than $\pi$, meaning $b_t(s)\in[0, 1]$ and $\sum_{s\in S} b_t(s) = 1$? I'd find this definition easier to read.}

{\bf Response:} We have followed your suggestion in the revised version.

\item {\color{blue} Equation II.7 is missing a parenthesis.}

{\bf Response:} Thanks. We have added the parenthesis.

\item {\color{blue} Algorithm 1 could be more specific. $\theta_1$ and $\theta_2$ should be step-indexed as e.g. $\theta_{1,k}$ and $\theta{2,k}$, and that initial values $\theta_{1,0}$ and $\theta_{2,0}$ are required. Refer to the equation for computation of $P(\theta,D)$.}

{\bf Response:} We have followed your suggestion in the revised version.

\item {\color{blue} In general, the concrete specifications of AIDA, how it is estimated and what the estimation results were, are insufficient. The reader can not reproduce or understand the model fit based on what's currently documented. The specification can be either documented in the paper, or (preferably) given as program source code that reproduces the fit. I would like to know:
\begin{itemize}
\item What is the exact equation for the log P in Algorithm 1?
\item What planning horizon was used, or is it even a free parameter?
\item What was the gradient stepping method used (a simple semi-gradient step? Stepsize?)?
\item How many outer loop steps K of Algorithm 1 were taken?
\item What was the convergence criterion for the value iteration?
\item Did the algorithm converge? At what rate (given by e.g. learning curves)?
\item What were the initial parameter values?
\item What was the value of the hyperparameter $\lambda$?
\item What was the resulting discretization for the control action described in page 27, line 8?
\item How were/are the state values discretized/categorized?
\item What are the specifications for the perception model (tabular probabilities?)
\item What are the resulting perception and preference models?
\item What is the resulting control law like (e.g. show (marginal) maximum preference acceleration for the different belief states)?
\end{itemize}
 
Most of these can be remedied by supplying the source code, and the manuscript can refer to that for details.}

{\bf Response:} As in our response to comment 1, the source code is available on Github. In addition, we have added an appendix with implementation details. Below we address these questions point by point:
\begin{itemize}
    \item The equation for $\log P(\theta|\mathcal{D})$ in algorithm 1 is given in equation III.5 in the manuscript.
    \item We used a fixed planning horizon of 30 time steps. We have added this to the appendix.
    \item We trained all models using the Adam optimizer. The learning rates for different models are given in the appendix.
    \item We trained AIDA for 500 epochs. The number of epochs for other models are provided in the appendix.
    \item Our model uses a finite horizon. Hence, we use a finite number of backward recursion steps to compute the value function.
    \item As described above, we use a fixed number of training epochs. The number was chosen based on visual inspection of a few experimental run learning curves, i.e., until the loss value no longer decreases significantly.
    \item All initial AIDA parameters were drawn randomly from a standard normal distribution.
    \item We selected $\lambda = 1$ based on a few experimental training runs with $\lambda \in [0.2, 1, 4]$. $\lambda = 1$ was chosen because it best trade off fitting dataset actions against maintaining online performance.
    \item As described in the manuscript, discretization for control actions were obtained by fitting a Gaussian mixture model to dataset actions. The resulting action bins, i.e., the means of the Gaussian components, are roughly symmetrically distributed with respect to zero actions and the distance between adjacent action bins increased from lower to higher magnitude actions. The Gaussian mixture components are shown in Fig. \ref{fig:gmm}.
    \item The states were not discretized but learned from data via the mixture of normalizing flow observation model which is trained together with the preference model.
    \item The perception model is a mixture of normalizing flow with Gaussian base distributions. We have updated section V.A to describe model parameterization and added more implementation details in the appendix. The final learned observation and preference models are illustrated in Figure 8, previously Figure 1, where the samples represent the observation densities of the learned discrete states.
    \item The resulting control law can be seen from Fig. 1 (b) where the policy tends to decelerate when the relative speed and $\tau^{-1}$ are negative and relative distance is small. 
\end{itemize}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.4\textwidth, height=0.3\textheight]{figs_rebuttal/action_gmm_components.png}
    \caption{Component density of Gaussian mixture model used to discretize continuous longitudinal control actions (dds).}
    \label{fig:gmm}
\end{figure}

\item {\color{blue} Also, currently the final equations used for the model are interspersed within the derivation, which makes them a bit hard to spot. I'd like to see the resulting model equations (re)presented clearly after the derivation.}

{\bf Response:} In response to your comment we have added a table that summarizes all the components of the model. 
\begin{center}
\begin{tabular}{ |p{4cm}||p{11cm}| }
 \hline
 \multicolumn{2}{|c|}{Structure of Model of Perception and Control} \\
 \hline
Perception &  \\
 \hline
 Observations   & $\mathbb{O}_{\widehat{\theta}_1}(o_t|s_t)$    \\
 Transitions & $\mathbb{T}_{\widehat{\theta}_1}(s_{t+1}|s_t,a_t)$       \\ 
 Generative Model &
$\sigma_{\widehat{\theta}_1}(o_{t+1}|b_{t},a_{t}) = \sum_{s_{t+1}}\sum_{s_t}\mathbb{O}_{\widehat{\theta}_1}(o_{t+1}|s_{t+1}) \mathbb{T}_{\widehat{\theta}_1}(s_{t+1}|s_t,a_t)b_{t}(s_t)$
 \\
 \hline
 Control & \\ \hline
 Preferences (reward) & $r_{\widehat{\theta}_2}(s_t,a_t)$ \\
 Control Policy   &  $\pi_{\widehat{\theta}}(a_t|b_t) =\frac{\exp Q^*_{\widehat{\theta}}(b_t,a_t)}{\sum_{a' \in A}\exp Q^*_{\widehat{\theta}}(b_t,a')}$ \\
 \hline
\end{tabular}
\end{center}


\item {\color{blue} Page 25, line 4:
In [19], the modelled agent's estimate is also a belief of the (continuous) state, represented using a particle approximation for the internal state distribution, but the beliefs are formed using a transformation from optical variables to the cartesian variables. Continuous vs discrete state and (explicit) leading vehicle prediction are indeed major differences. Also, [19] uses deterministic policy and does not employ a POMPD or other reward optimizing formulation.}

{\bf Response:} Thanks. We have added additional text describing the differences to [19].

\item {\color{blue} 
I don't think [45] applies well to continuous control tasks like car following. Contemporary theories, e.g. Bayesian psychophysics, typically use continuous formulations, and I'm not aware of findings or claims that humans to use categorical perception for such tasks.
Categorization/discretization of the states and actions greatly simplify POMDP models, and continuous models are typically intractable. I find discretization more a limitation than a benefit, but doing it for technical reasons is perfectly fine, although it does cause some issues (e.g. currently the range of state values the model operates in is likely limited by the discretization?). Please discuss these briefly.}

{\bf Response:} Thanks. We have modified the footnote to clarify that the experiments in [45] took place in simpler environments. Categorical representation is indeed a limitation when fine-grained control is required, as illustrated in our model diagnostics in section C.3. However, it is advantageous for analyzing decision making over high-level discrete choices, such as whether braking at a small set of intensity levels are needed. They also afford better interpretability due to their resemblance to clustering and learning ``prototypical" observations.

\item {\color{blue}
Page 25, line 41: The description of the BC models is insufficient. What were the layer specifications for the MLP and the RNN, how were they trained, did the training converge etc? Without these details, it's impossible to assess whether the results reflect BC performance in general, or just some arbitrary BC formulations. Providing source code replicating the results is sufficient (and preferred) for the specification.
} 
 
{\bf Response:} We have added an appendix describing details of the MLP and RNN implementations and reference to the Github source code. In brief, we used standard architectures for MLP and RNN (similar to \cite{zhou2017recurrent}) and trained these models for a fixed number of epochs. The number of epochs were manually set based on visual inspection of the validation loss over training epochs.

\item {\color{blue}
Page 26, "Dataset". Please plot some sample trajectories selected with the criteria (or refer to figs 7 and 8). (Having these on the plot with human-vs-model trajectories I proposed earlier is fine).}

{\bf Response:} We have added plots for human vs model predicted control actions or behavior in both offline and online settings. See Figures \ref{fig:offline_pred} and \ref{fig:online_pred}.

\item{\color{blue}  
Page 26, line 40: 
A car following model not incorporating v is a major shortcoming. It prevents the model from handling e.g. speed limits. Please elaborate on the spurious correlation problem. It's not clear to me how the referenced articles apply in this case.}

{\bf Response:} The issue of spurious correlation or causal confusion is a well documented phenomenon in learning-to-control or learning-based driving simulations from offline data and closely related to distribution shift \cite{kuefler2017imitating, bhattacharyya2020modeling, de2019causal, codevilla2019exploring}. In this setting, the model is prone to learning associations that are not causal or shortcuts and still achieve high training accuracy, e.g., computing predictions for accelerations from past velocities. Since our dataset and scenarios were chosen for modeling behavior of maintaining relative speed and distance rather than speed limits, we decided to remove features from model inputs that will likely lead the model to learning spurious correlations. We have added text in the manuscript to explain this phenomenon.

\item {\color{blue} Page 26, line 43: 
Including $\tau^{-1}$ seems a bit spurious, as it is just $(\Delta v)/d$. For discretized observations, depending on the discretization method, it may be purely spurious (doesn't effect the binning).}

{\bf Response:}  The observations are not discretized in our model. Including $\tau^{-1}$ in addition to $\Delta v$ and $d$ is standard practice in a recent line of car following models, e.g., \cite{markkula2016farewell}.

\item {\color{blue}Page 26, line 46: 
$\tau$ is invariant to the width of the vehicle (i.e. it has the same value regardless of the lead vehicle width).}

{\bf Response:} As explained in text, $\tau$ is not time-to-collision but a quantity referred to as ``looming" by a recent line of car following models (e.g., \cite{markkula2016farewell}) which computes the rate of change of the visual angle divided by the visual angle itself. Since the visual angle depends on the width of the lead vehicle, it is not invariant to it. However, lead vehicle width does have little effect on this quantity compared to time-to-collision. As explained in the response to comment 19, this feature was chosen mainly to be consistent with recent related work on car following models.

\item {\color{blue} Page 27, line 24: 
Please elaborate this selection. In the "Dataset" section you mention having 1254 trajectories. How are these split between new-lane and same-lane? Most importantly: how many (same-lane) trajectories does this leave in the training set?}

{\bf Response:} The 1254 trajectories are all in the same-lane condition. Among these, 878 trajectories were used to train models while the remaining were used to perform offline and online evaluations. There are 290 trajectories in the new-lane condition in addition to the 1254 same-lane trajectories. None of them were included in the training set. We have updated the manuscript and appendix to reflect these choices. 
 
\item  {\color{blue} Page 27, "C. Model Evaluation and Comparison"
 Assessing the models' Offline performance would greatly benefit from scatter plots with model-predicted and observed accelerations. These would be a lot more transparent than the central tendency aggregates. To level the playing field with the deterministic IDM, you could for example sample multiple predictions for each observed sample and use an average. Or even better, compute the analytical mean or mode action for the models (e.g. weighted average control based on the stochastic policy probability outputs).}

{\bf Response:} We have added a figure that shows the mean predicted accelerations and standard deviation for different models. See Figures \ref{fig:offline_pred} and \ref{fig:online_pred}. 

\item {\color{blue} Page 27, line 34:
What is the Gaussian policy for the IDM? It's a deterministic model. Does this mean noise was added to the IDM predictions?}

{\bf Response:} Our fitting method treats all models (e.g., AIDA, MLP, RNN) as parameterized policies and maximize the log likelihood of dataset actions. Since IDM is a deterministic model, in order to evaluate a log likelihood loss, we introduced a state-independent standard deviation parameter $\sigma$ and let the control action predicted by the IDM rule to be the mean, i.e., $\mu = IDM(o)$ where $o$ is the environment observation. $\mu, \sigma$ together define a Gaussian likelihood for each dataset observation-action pair. In our experiments, the fitted standard deviation averaged over all seeds was 0.46. There was previously an implementation mistake in sampling from the fitted Gaussian policy. We have corrected the mistake by discarding the variance during testing and only use the mean prediction so the resulting control rule is deterministic. 

\item {\color{blue} 
Page 27, 49: 
How were the stochastic policies sampled for these? Was noise added to the IDM outputs?}

{\bf Response:} For IDM, we compute the actions predicted by the IDM rule and discard the variance parameter used for fitting at test time. For BC and AIDA which have stochastic policies, we first sample a discrete action and then sample the mean of the Gaussian mixture component selected by the discrete action. For these models, we draw 30 samples for each prediction. We have updated the manuscript to make this clearer.

\item {\color{blue} Page 28, line 3: 
I assume this is the deviation error means absolute deviation, i.e. it is Mean Absolute Deviation (MAD)? }

{\bf Response:} Yes. We use the term ADE from [25].

\item {\color{blue} 
Page 28, line 9: "3) Statistical Evaluation": I'm a bit uneasy with using the IQM here, especially with seeing similar IQMs causing wildly different collision rates. For many/most purposes the error tails of car following models are very important. At least I'd like to see how the errors are distributed.
 
Further, it is not entirely clear to me what is randomized for the different samples (used in the plots and tests later). There are at least 2 sources of random variability if I understand correctly: the action sampling for the stochastic policies and random initial values used for optimization. Which of these is of interest for the distribution of the errors?
 
I'm not also sure why statistical tests are used for these. With enough samples all tests become significant, and with simulation they are "freely" available. }

{\bf Response:} We have further investigated the relationship between using versus not using IQM in our results. Figure \ref{fig:online_ade_iqm_comparison} compares online ADE with and without IQM. It shows that IQM does compress the upper tail of ADE, especially for BC-RNN and AIDA and make most AIDA models better than BC-RNN. However, it also exposes the fact that IDM performs substantially worse in the dense (new)-lane conditions which is consistent with the offline evaluation results but would have been unobservable without IQM. It should be noted that not using IQM only increased the ADE of the worst AIDA model by 1 m and did not alter the performance ranking between compared models in either testing conditions. And arguably IQM made the difference between model behavior more salient. We acknowledge these nuances and have added the results without IQM to the appendix.

\begin{figure}[!h]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs_rebuttal/online_mae.png}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figs_rebuttal/online_mae_iqm.png}
    \end{subfigure}
    \caption{Comparison of online ADE by model using IQM (right) and not using IQM (left).}
    \label{fig:online_ade_iqm_comparison}
\end{figure}

Our choice of using IQM is mainly driven by prior work \cite{agarwal2021deep} in evaluating learned controllers in stochastic environments using a finite number of testing runs. In our settings, the environment (i.e., traffic condition) is not necessarily stochastic since we play back dataset trajectories of the lead vehicle, and the only source of randomness is due to sampling of the learned stochastic policies. However, the dataset trajectories being played back are also not uni-modal so that a finite sample average of performance can serve a good measure for a model's central tendency. Furthermore, in machine learning-based driving models (e.g., \cite{bhattacharyya2020modeling}), a few mistakes by the model tend to compound and cause the controlled vehicle to deviate significantly from the dataset trajectory. These trajectories distort the performance measure substantially and arguably the measure becomes meaningless after a certain amount of deviation, as we do not stop a testing episode immediately after a collision but rather continue until the dataset trajectory finishes replay. Thus, the combination of IQM and the raw collision rates provide two views of the evaluation results to give readers more insight into the behavior of compared models. 

Lastly, the purpose of statistical testing was to clarify the difference between model performance that cannot be visually distinguished from the figures.

\item {\color{blue} Page 28, line 30: 
Were the models re-trained for each of these samples? How many samples are there (15?)?}

{\bf Response:} This plot illustrates the perception model learned by the \emph{single}, best performing AIDA model where in column c, each color represents samples from the observation distribution of the corresponding discrete state $P(o|s)$. These samples together help us visualize the density and geometry of the learned distributions, which represent the ``prototypical" observations a driver expects to see in a state. As described in the figure caption, for each state, we drew 200 samples as it was sufficient to visually represent the learned distribution.

\item {\color{blue} 
Page 28, line 34: 
I'm quite surprised to see so much variation in the IDM results. Is the variation because of different optimization results from different intial values? As the simplest model, I would assume the IDM is least prone to local optima, and hence variation between training runs. Did the IDM optimization converge properly? }

{\bf Response:} It is true that the variation in IDM results is larger in the sparse (new)-lane setting than the dense (same)-lane setting. However, it should be noted that these variations are not large in practice. For example, the distance between the largest and smallest values are about 0.5 m. In our experiments, all IDM models converged to similar parameter values. However, their predictions are not necessarily the same. These differences become amplified in the sparse (new)-lane setting which has a different distribution from the dense (same)-lane setting. 

\item {\color{blue} 
Page 30: Fig. 5 Why are the new-lane errors so much lower than same-lane errors? One would assume that generalizing to new setting would cause higher errors. The new-lane data was somehow such that it's easier for the models to predict, or is there less variation in it? This merits some discussion. }

{\bf Response:} As discussed, the new lane errors were lower because there was less congestion and stop-and-go in the dataset, thus ``easier" to predict. To make this clearer to the reader, we have renamed same-lane to dense-lane and new-lane to sparse-lane. We believe this is a valid scenario to test model generalization given dataset limitations. Furthermore, the reverse is impossible: training on sparse lanes and generalize to dense lanes, since the model wouldn't have experienced enough stop-and-go to learn the environment kinematics and desired states. 

\item {\color{blue}  
Page 30: Fig 6 The collision rates for especially BC-RNN and AIDA seem quite disasterous to me (I assume the ground-truth collision rate is 0\%). Why do they crash so much, even though the ADE-IQM is very similar to the others? Is IQM hiding some very bad runs that lead into crashes?}

{\bf Response:} It is acknowledged in machine learning-based driving simulation literature that significant deviation from dataset trajectories and collision are common issues, especially under distribution shift (e.g., \cite{bhattacharyya2020modeling}). Furthermore, as explained earlier, when learning from an offline dataset, it is not possible in general for the model to learn the kinematics of emergency or near crash conditions that are never contained in the dataset. Thus, the models will not know how to react in these conditions as illustrated in figures 7 and 8 of our analysis. 

It is true that higher collision rates tend to be correlated with larger ADE. Figure \ref{fig:ade_vs_crash} shows that for AIDA, there is a positive correlation between ADE and collision rate. However, the correlation is not obvious for BC-RNN or when both models are evaluated under ADE-IQM. Also notice that the difference in raw ADE for AIDA is only 1 m. 

In our experiments, the reason why BC-RNN and AIDA had much higher collision rates despite small differences in ADE is that we define collision as whenever the ego and lead vehicle had overlapping bounding boxes. This does not mean the ego vehicle has to incur significant deviation from the dataset trajectory to be counted as a collision (even though there are a few trajectories for which it did). Collision can result from insufficient braking which puts the ego vehicle's stopping position slightly ahead of the dataset position. 
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\textwidth]{figs_rebuttal/ade_vs_crash.png}
    \caption{Scatter plot of online ADE and LV collision rate with linear regression and 95\% confidence interval.}
    \label{fig:ade_vs_crash}
\end{figure}

\item {\color{blue} 
Page 30, line 52: 
Is such clear interpretation merited? AIDA clearly captures driver collision behavior worse than the IDM.}

{\bf Response:} In the original manuscript, we refer to ``capture driving behavior" as making better offline predictions and maintaining competitve online ADE. We have revised this statement as "AIDA can capture driver car following behavior comparably if not better than baseline models".

\item {\color{blue}  
Page 31, line 47:
 It's not clear to me from the manuscript how AIDA learned the categorization of the states. Is this part of the AIDA itself, or was it done as preprocessing using e.g. Gaussian mixture estimation as for the actions? }

 {\bf Response:} Categorization of states was not a part of preprocessing but learned from data. As explained in the manuscript, both the perception and preference models were optimized to fit the observed control actions in the dataset. In addition, an observation log likelihood loss was added to the objective function. This encourages the model to find a categorization of states that best explain the dataset actions while not deviating too much from dataset observations. 
 
\item {\color{blue} Figures 7 and 8: It's not very clear to me why the same belief states lead to such different predicted accelerations. I don't fully grasp how the predicted accelerations are computed in the model. Does predicted acceleration mean the distribution of the accelerations of the policy for a given belief state (equation III.3)?}

{\bf Response:} Yes, predicted acceleration is the distribution of accelerations given a belief state. Such a distribution can be multimodal, which can result from two causes. The first cause is that drivers in the dataset took different actions given similar observations. The second cause is that our perception model consisting of discrete states and mixture of normalizing flow observations are not flexible enough such that distinct observation sequences would appear as though they have the same belief states. We believe both of these causes are at play in the present scenario and these figures were specifically used to illustrate the latter. In this context, despite the representational limitation of our perception model, it provides interpretability which allows us to understand failures caused by its limitations.

\item {\color{blue} I can't clearly tell from the derivations what the Q* in the equations is. I'm guessing it's the optimized RL Q-function. }

{\bf Response:} Yes it is the optimal RL Q-function. We have added a statement in Theorem 1 to make this clear.
 
\item {\color{blue} Page 34, line 33: 
This conclusion is clearly false. AIDA failed spectacularly in predicting the same-lane collisions, whereas IDM was perfect.}

{\bf Response:} We have modified this sentence as "AIDA performed comparably and in certain cases better than the rule-based IDM and data-driven neural network benchmarks."
 
\end{enumerate}


\newpage
\section{Response to Reviewer 2’s Comments}
\begin{enumerate}
\item {\color{blue} The paper is generally well-organized and well-written. I feel the Abstract is too long and then should be split into two or three paragraphs (context + research gap + proposed work + results).}

{\bf Response:} In response to your comment, in the revised version, we have shortened the abstract.

\item {\color{blue} If possible, I would suggest a table summarizing the used features, along with a small description. At some point on page 7, the authors have a section named "Feature Computation", which got me confused: are these new computed features or those already present in the Interaction data set? }

{\bf Response:} These are features computed based on the raw trajectory data in the Interaction dataset. Due to space constraint, we added a table for features used in the Appendix.

\item {\color{blue} 
I missed a part detailing how the proposed model was trained: how many runs were done? i.e., how many random training/testing data splits were done? (Meanwhile, I guess I got my answer; the authors used different seeds to induce different splits, right?) }

{\bf Response:} Yes. We trained each model with 15 seeds. We used a 7-3 train-test split on the same-lane dataset. In the revised version, we have included an Appendix for additional training and implementation detail. In addition, the source code is available on Github at: \url{https://github.com/ran-weii/interactive_inference} 

\item {\color{blue} Because the results are based on simulation, it wouldn't hurt to provide better details of the underlying simulation model.}

{\bf Response:} In the revised version, we have added an appendix that describes the specifics of the simulation model (e.g. the vehicle dynamics model). 

\item {\color{blue} It is interesting to see that on the online evaluation ADE-IQM, AIDA's performance is more on par with the remaining models than it is on the offline one. The advantages of AIDA are more striking in the in the offline evaluation.}

{\bf Response:} Yes, AIDA's main advantage is in predicting driver actions from observation-action histories drawn from distributions that are similar to the dataset. Online evaluation introduces distribution shift for which AIDA does not provide significant advantage over other machine learning models. This is partially due to the discrete state representation as we show in section C.5 on model diagnostics and likely also due to heterogeneous driving behavior in naturalistic dataset making fitting a single model of perception and action suboptimal.

\item {\color{blue} Regarding AIDA's interpretability, did the authors ever think of using shap values analysis (or similar approaches)? }

{\bf Response:} Our goal with AIDA is to provide an {\em interpretable} model --in terms of a cognitive model of perception and action-- that exhibits a statistical performance that is similar (or arguably superior) to other black-box models.
As we illustrate in the paper, a researcher can directly interpret each model component (i.e. perception and control) and trace driving behaviors back to these components. Hence, we do not consider techniques for post-hoc interpretability methods like SHAP value.  

\item {\color{blue} In Figure 6, high collision rates (same lane) are not good, right? However, AIDA attained the highest ones, also exhibiting more variance across runs. For the new lane, the models seem equivalent. }

{\bf Response:} In the revised version, we have added text to better describe the extent of our claims. In particular, we argue that the high collision rates observed (see text in blue in the revised Introduction) is due to `distribution shift', i.e. the mismatch between the data used to train the model and the data used in the testing scenario. Briefly, the naturalistic dataset used in the paper only includes relatively few extreme observations (e.g. extremely low relative distance and/or high relative velocity) and no actual collisions. Thus, the learned model from the dataset provides an account of human drivers' perception and control in {\em average} conditions and the learned representation is not accurate in extreme conditions (e.g. extremely low relative distance and/or high relative velocity). This is a common problem for offline reinforcement learning models as it is often not possible to learn state dynamics in operating conditions that are not represented in the data \cite{Wei_2024, levine2020offline}. 

\end{enumerate}

\newpage

\bibliographystyle{ieeetr}
%\bibliography{manifold}
\bibliography{PaperBIB,ref,main,example}
\end{document}