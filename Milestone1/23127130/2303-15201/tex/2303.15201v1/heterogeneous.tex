% Repository:  https://github.com/chiehrosswang/TRB_LaTeX_tex
%
% Transportation Research Board conference paper template
% version 4.0 Lite (updates made to be compatible in Overleaf and ShareLaTeX)
% 
% 
% When numbered option is activated, lines are numbered.
\documentclass[numbered]{trbunofficial}
\usepackage{graphicx}
\usepackage{cmbright}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{color}
\usepackage{subcaption}

% \usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
% For TRB version hide links
\usepackage[hidelinks]{hyperref}

% Put here what will go to headers as author
\AuthorHeaders{Wei, McDonald, Garcia, Markkula, and Engstr\"{o}m}
\title{An active inference model of car-following: Advantages and applications}

% TODO: add macros for easier formatting of \author.

\author{%
  \textbf{Ran Wei}\\
  \hfill\break
  \textbf{Anthony McDonald, Corresponding Author}\\
  \hfill\break%
  \textbf{Alfredo Garcia}\\
  \hfill\break%
  \textbf{Gustav Markkula}\\
  \hfill\break%
  \textbf{Johan Engstr\"{o}m}\\
  \hfill\break%
  \textbf{Additional Waymo authors by contribution}
}

% \author{%
%   \textbf{Ran Wei}\\
% %   Department of Industrial and Systems Engineering, Texas A\&M University, College Station, TX, USA, rw422@tamu.edu\\
%   \hfill\break% this is a way to add line numbering on empty line
%   \textbf{Anthony McDonald, Corresponding Author}\\
% %   Department of Industrial and Systems Engineering, University of Wisconsin-Madison, 1513 University Ave., Madison, WI, USA, 53706, admcdonald@wisc.edu\\
%   \hfill\break%
%   \textbf{Alfredo Garcia}\\
% %   Department of Industrial and Systems Engineering, Texas A\&M University, College Station, TX, USA, alfredo.garcia@exchange.tamu.edu
% %   \textbf{Johan Engstr\"{o}m}\\
% %   Waymo LLC, Mountain View, CA, USA\\
% %   \hfill\break%
% %   \textbf{Gustav Markkula}\\
% %   Institute for Transport Studies, Leeds University, Leeds, UK\\
% %   \hfill\break%
  
  
% }

% If necessary modify the number of words per table or figure default is set to
% 250 words per table
% \WordsPerTable{250}

% If words are counted manually, put that number here. This does not include
% figures and tables. This can also be used to avoid problems with texcount
% program i.e. if one does not have it installed.
% \TotalWords{200}

\begin{document}
\maketitle

\begin{center}\textbf{Abstract}\end{center}
Driver process models play a central role in the testing, verification, and development of automated and autonomous vehicle technologies.  Prior models developed from control theory and physics-based rules are limited in automated vehicle applications due to their lack of flexibility and generalizability. In contrast, recent data-driven machine learning models can imitate nuanced driving behavior but are limited in practical applications due to the need for large training datasets and their lack of interpretability. To address these limitations, we propose a novel car-following modeling approach using the Bayesian principles of human cognition---AIDM. The AIDM is learned from data but retains interpretability due to a fixed model structure which aligns with human reasoning. In this work, we use a naturalistic dataset to demonstrate the flexibility and generalizability of the AIDM approach compared to rule-based and data-driven benchmarks. We also extend the AIDM to model heterogeneous driving styles. The results illustrate that AIDM has comparable flexibility to Behavior Cloning neural network benchmarks and comparable interpretability to the Intelligent Driver Model.


\hfill\break%
\noindent\textit{Keywords}: Driver behavior modeling, Active Inference, Behavior Cloning, Intelligent Driver Model, Car-Following
\newpage

\section{Introduction}
The rapid development of automated and connected vehicle technologies has created a corresponding demand for models of driver behavior that can be used to calibrate design parameters \cite{scheel2022urban, scanlon2021waymo}, evaluate technologies \cite{bargman_counterfactual_2017, roesener2017safe}, and refine real-time decision making. To be effective in these tasks, driver models must be flexible, scalable, and interpretable. Model flexibility is the ability of the model to reflect nuanced social behavior of human drivers \cite{brown2017social}. Scalability is the ability of the model to extend to novel environments with minimal modeler intervention. Interpretability refers to both a clear connection between predicted behavior and model mechanics (i.e., cause and effect) and a grounding of the model in psychological and psycho-motor principles. Few existing driver behavior models meet these criteria and car following models are particularly limited \cite{mcdonald2019toward}. It is important to develop flexible, scalable, and interpretable car following models for automated vehicles and future transportation systems as car following represents a large portion of current driving time and crashes involving automated vehicles \cite{Alambeigi_trb,novakazi2021levels}. Furthermore, car following involves a complex expression of social behaviors through physical vehicle positioning \cite{brown2017social}, e.g., speeding up to prevent a vehicle from merging. 

Existing car following models can be partitioned into rule-based models and data-driven models. Rule-based models generate acceleration behavior based on a function specified by the modeler. Typically, this function is grounded in known observations or driver behavior theory \cite{saifuzzaman2014incorporating}. For example, the Intelligent Driver Model (IDM) predicts driver acceleration based on deviations from a desired speed and space headway \cite{kesting2009agents}. While rule-based models have a clear connection between predicted behavior and model mechanics, they are limited in their flexibility and scalability. Analysis have shown that rule-based models struggle to capture commonly observed driver behaviors such as satisficing \cite{hancock1999car}, and that they fail to generalize to near-crash scenarios \cite{hamdar2008existing}. Despite these limits, rule-based models are still widely used for automated vehicle analyses \cite{talebpour2016influence} and thus offer a valid benchmark for new models.

In contrast to rule-based models, data-driven models learn a function mapping observations or features to acceleration behaviors using an algorithm. Recent works have used neural networks \cite{zhou2017recurrent}, hybrid-neural network algorithms with physics constraints \cite{mo2021physics}, reinforcement learning \cite{zhu2018human}, and generalized adversarial imitation learning \cite{bhattacharyya2020modeling}, to model car following behavior. These approaches have shown considerable flexibility in replicating human behavior, however, data-driven models still struggle to reproduce well-known traffic phenomena such as stop-and-go oscillation and they do not generalize well \cite{zhou2017recurrent, spencer2021feedback}. Furthermore, the complexity of existing data-driven models prohibits interpretability both in the connection between input and output and in their grounding in human psychology. Despite these shortcomings, data-driven models are more scalable to complex scenarios which are difficult for manual model specification. In particular, Behavior Cloning (BC) is a class of data-driven models known for its simplicity and general effectiveness \cite{pomerleau1988alvinn, codevilla2019exploring, kumar2021should}. BC models have been widely adopted for developing and evaluating automated vehicle algorithms and are a common benchmark for evaluating novel data-driven models \cite{igl2022symphony, suo2021trafficsim}.

Across the relative strengths of rule-based and data-driven car following models, it is clear that there is a role for model structure (to aid in interpretability)--especially structure grounded in psychological theory \cite{markkula2022explaining}--and learning from data (to aid in flexibility). Recent work suggests that active inference, and more generally, Bayesian theories of cognition are a promising approach to incorporate data and model structure \cite{wei2022modeling, engstrom2022modeling}. The central tenants of Bayesian model of human cognition are: 1) humans have internal probabilistic generative models of the environment and 2) humans leverage their model of the environment to make inference about action courses that lead to their desired state of the environment \cite{doya2007bayesian, gershman2015computational}. Under this framework, flexible and nuanced human behavior results from subjective beliefs about the environment and the need to cope with uncertainty while making action choices \cite{engstrom2018great}. Models developed from this framework are interpretable in that they explicitly model beliefs--defined by probability distributions across roadway states--and observations--defined by vehicle kinematics--and their relationship to driver actions. As a result, model decisions can be examined without complex post-hoc explanation methods, such as those described in \cite{rudin2019stop}. However, the generalizability of these approaches, particularly in driving, is a substantial open question. 

Our goal in this article is to introduce a Bayesian Active Inference Driver Model (AIDM), evaluate the models flexibility and generalizability relative to rule-based and data-driven benchmarks, and  illustrate the interpretability of the model and the resulting insights that intrpretability provides into car following behavior.

\section{Materials and Methods}
In this section, we introduce a formulation of the benchmarks -- IDM and Behavior Cloning -- then describe our AIDM formulation. We then describe the dataset used for model fitting and the model comparison approach. To simplify notation, we adopt a unifying view of car following models as longitudinal driving control policies which map input signals observed by drivers to a control signal, i.e., the instantaneous longitudinal acceleration. We denote the driver observations (or features in machine learning terminology) at discrete time step $t$ by $o_t$ and the control signal by $a_t$. Using this nomenclature, the most generic class of driver control policies can be described as a probabilistic mapping from the entire history of inputs and controls, denoted by $h_t = \{o_{1:t}, a_{1:t-1}\}$ to the next control signal, i.e., $\pi(a_t|h_t)$. However, the control policy may only depend on the most recent observation as $\pi(a_t|o_t)$. The definition of the control policy differentiates the IDM, Behavior Cloning, and AIDM models.

\subsection{Intelligent Driver Model}
The IDM \cite{treiber2000congested} implements a control policy based on drivers' instantaneous observation of their own vehicle's speed $v$, relative speed to the lead vehicle $\Delta v$, and relative distance to the lead vehicle $d$, i.e., $\pi(a_t|o_{t}=\{v_t, \Delta v_t, d_t\})$. At each time step, the IDM model computes a longitudinal acceleration to regulate the ego vehicle towards a desired speed $\Tilde{v}$ and desired space headway $\Tilde{d}$ using the following control rule:
\begin{align}\label{eq:idm1}
   a_t = a_{max}\left[1 - \left(\frac{v_t}{\Tilde{v}}\right)^{4} - \left(\frac{\Tilde{d}}{d_t}\right)^2\right]
\end{align}
where the desired space headway is defined as:
\begin{align}\label{eq:idm2}
    \Tilde{d} = d_{0} + v_{t}\tau - \frac{v_t\Delta v_t}{2\sqrt{a_{max}b}}
\end{align}

The IDM has the following parameters: $a_{max}$ the maximum acceleration rate which can be implemented by the driver, $d_0$ the minimum allowable headway distance, $\tau$ the desired headway time, and $b$ the maximum deceleration rate. While these parameters can be set manually by human designers, they usually depend on the road condition and vary with individual driver characteristics, e.g., the desired velocity and minimum headway distance. Thus, various methods have been proposed to calibrate model parameters from traffic data \cite{papathanasopoulou2015towards, treiber2013microscopic}.

A major limitation of IDM is that it cannot express certain types of behavior as a result of the control rule defined in (\ref{eq:idm1}) and (\ref{eq:idm2}). For example, it cannot express behavior such as increase in desired time headway after periods of dense traffic and other behaviors that depends non-trivially on the observation history \cite{treiber2003memory}. Incorporation of such behavior requires significant intervention from the model designers in adapting the control rule, e.g, modifying (\ref{eq:idm1}) and (\ref{eq:idm2}) to depend on a ``memory" component in the previous example \cite{kesting2009agents}.

\subsection{Behavior Cloning}
BC refers to methods that train flexible classes of policies from datasets of human car following behavior. The dataset, denoted with $\mathcal{D}$, is usually organized in the form of observation-action trajectories, i.e., $\mathcal{D} = \{o_{1:T}^{(i)}, a_{1:T}^{(i)}\}_{i=1}^{N}$, where $N$ is the total number of trajectories and $T$ is the length of each trajectory. BC mostly uses neural network parameterized policies that depend on either the entire history $h_t$ or the most recent observation $o_t$.
Let us denote the policy parameters with $\theta$, BC trains policies to maximize the expected log likelihood of the dataset trajectories:
\begin{align}\label{eq:bc_objective}
    \max_{\theta} \mathcal{L}(\theta) = \mathbb{E}_{o_{1:t}, a_{1:t} \sim \mathcal{D}}\left[\sum_{t=1}^{T} \log \pi_{\theta}(a_t|h_t)\right]
\end{align}
Compared with reinforcement learning and online imitation learning, BC has the advantage of being simple to implement and time efficient. In addition, BC does not require a high fidelity traffic simulation environment for training. In contrast with rule-based policies, BC policies are more flexible and can express a much larger class of behavior. 

However, BC has known disadvantages of being sensitive to the quantity and quality of training data and input features. The covariate-shift between the training dataset and testing environment and the neural network models' lack of generalizability to unseen inputs often cause BC models to overfit to the training dataset while producing poor control behavior during closed-loop testing (defined in section \ref{sec:online_test}) \cite{spencer2021feedback}. Furthermore, several studies have found that BC can be highly sensitive to input features \cite{bhattacharyya2020modeling, de2019causal, zhou2017recurrent}. Specifically, when a driver's past control inputs are used as input features to the trained policy, it is likely that the policy merely repeats previous controls in closed-loop testing. This has been interpreted as a form of learning spurious correlations or causal confusion in machine learning, since driver controls at adjacent time steps are usually so similar that predicting previous control can quickly minimize training error. 

Despite these shortcomings, BC, or variations of it, is a widely applied approach in developing automated vehicle algorithms and building simulated agents and environments for training them \cite{suo2021trafficsim, igl2022symphony}. It can produce high quality simulated behavior in practice when the training dataset is large and diverse, appropriate features are selected, and the policy class is expressive enough \cite{codevilla2019exploring, zhou2017recurrent, kumar2021should} and thus it represents a valid data-driven modeling benchmark. 

\subsection{Active Inference Driver Model}
Active inference is an emerging modeling paradigm of human behavior which has been increasingly adopted in cognitive neuroscience and machine learning in recent years \cite{friston2010free, mazzaglia2022free}. It rests on the assumption that human agents do not have direct access to the state of the environment but receive information about it through sensory signals \cite{friston2010free}. In order to behave autonomously, human agents build an internal generative model of how their own actions influence the state of the world through sensory signals, which then enables them to select actions that bring about desired state and sensory signals in the actual world. Since the world is vast and humans can only build models based on their own limited experience, the internal models are subjective and vary between individuals. 

An active inference model is defined by the agent's internal generative model, implemented as a Partially Observable Markov Decision Process (POMDP). A POMDP describes a dynamics process in which the state of the environment $s_t \in \mathcal{S}$ evolves with driver actions, $a_t \in \mathcal{A}$, according to a probability distribution, $P(s_{t+1}|s_t, a_t)$, and, generates observation signal, $o_{t+1} \in \mathcal{O}$, according to, $P(o_{t+1}|s_{t+1})$. In this work, we assume the environment states are discrete and the observation signals are multivariate continuous variables. Using this structure, the active inference agent can make inferences about the hidden state of the environment upon receiving observation using the Bayes rule:
\begin{align}\label{eq:aif_belief}
    b(s_t) = \frac{P(o_t|s_t)\sum_{s_{t-1}}P(s_t|s_{t-1}, a_{t-1})b(s_{t-1})}{\sum_{s_t}P(o_t|s_t)\sum_{s_{t-1}}P(s_t|s_{t-1}, a_{t-1})b(s_{t-1})}
\end{align}
where $b(s_t) = P(s_t|h_t)$ denotes the agent's belief about the environment state given the observation-action history $h_t$.

The active inference model, following the theory, selects control actions to minimize a criterion known as the (cumulative) expected free energy (EFE):
\begin{align}
    \mathcal{G}^{*}(b_t) \triangleq \min_{\pi} \mathbb{E}\left[\sum_{t}^{t+H} EFE(b_t, a_t) + \log \pi(a_t|b_t)\right]
\end{align}
where $H \leq \infty$ is a finite planning horizon. The EFE is defined as:
\begin{align}\label{eq:efe}
    EFE(b_t, a_t) = \mathbb{E}[D_{KL}\left(b_{t+1}||\Tilde{P}\right)] + \mathbb{E}[\mathcal{H}(o_{t+1})]
\end{align}
where $\Tilde{P} := \Tilde{P}(s_{t+1})$ defines the agent's preferred state distribution, $D_{KL}(\cdot || \cdot)$ denotes the Kullback-Leibler divergence and $\mathcal{H}(\cdot)$ denotes Shannon entropy. The first expectation is taken with respect to:
\begin{align}
\begin{split}
    P(o_{t+1}|b_t, a_t) &= \sum_{s_{t+1}}P(o_{t+1}|s_{t+1})P(s_{t+1}|b_t, a_t) \\
    &= \sum_{s_{t+1}}P(o_{t+1}|s_{t+1})\sum_{s_t}P(s_{t+1}|s_t, a_t)b(s_t)
\end{split}
\end{align}
and the second expectation is taken with respect to $P(s_{t+1}|b_t, a_t)$ defined above. Thus, the first term in EFE is a measure of discrepancy between the current belief and the desired state distribution. The second terms measures the agent's predictive uncertainty about future observations. These terms represent goal-seeking and epistemic behavior respectively \cite{tschantz2020learning}.

Let $\mathcal{G}^{*}(b_t, a_t)$ be defined as:
\begin{align}\label{eq:cumulative_efe}
    \mathcal{G}^{*}(b_t, a_t) := EFE(b_t, a_t) + \log \pi(a_t|b_t) + \int_{}P(o_{t+1}|b_t, a_t)\mathcal{G}^{*}(b_{t+1})d_{o_{t+1}}
\end{align}
Then the optimal policy has a closed-form expression \cite{haarnoja2018soft}:
\begin{align}\label{eq:aif_policy}
    \pi(a_t|b_t) = \frac{e^{-\mathcal{G}^{*}(b_t, a_t)}}{\sum_{\Tilde{a} \in \mathcal{A}}e^{-\mathcal{G}^{*}(b_t, \Tilde{a})}}
\end{align}

Active inference has two important differences from the traditional notion of POMDP in operations research and reinforcement learning. First, both the generative model and the control objective are internal to the agent, meaning they can differ in substantial ways from the true environment generative model or a canonical notion of desired behavior, .e.g., a good driver should always be centered in the lane. This has important implications as many human driving behaviors can be explained as inference in subjective or sub-optimal models \cite{victor2018automation}. Second, active inference makes an explicit distinction between goal directed and epistemic behavior in its reward function formulation according to the first and second term in (\ref{eq:efe}). This distinction supports adaptation  \cite{tschantz2020learning} and provides the model significant explanatory power. 

\subsection{Dataset}
We performed our analysis of IDM, BC, and AIDM using the INTERACTION dataset \cite{zhan2019interaction}, a publicly available driving dataset recorded using drones on fixed road segments in the USA, Germany, and China. The dataset provides a lanelet2 format map \cite{poggenhans2018lanelet2} and a set of time-indexed trajectories of the positions, velocities, and headings of each vehicle in the scene in the map's coordinate system at a sampling frequency of 10 Hz, and the vehicle's length and width for each road segment. The dataset contains a variety of traffic behaviors, including car following, free-flow traffic, and merges. 

Due to our emphasis on car-following behavior, we subset the data to include car-following data from a two-way, seven-lane highway segment in China with a total distance of 175 m. We focused on vehicles in the middle two west-bound lanes shown in Fig. \ref{fig:map}. We further filtered the remaining vehicles according to two criteria: 1) there was a lead vehicle with a maximum headway distance of 60 m, and 2) the ego vehicle was not performing a merge or lane change. We identified merging and lane change behavior using an automated logistic regression based approach and validated the classifications with a manual review of a subset of trajectories. We also removed all trajectories with length shorter than 5 seconds, leaving a total of 1254 trajectories with an average length of 14 seconds.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/map.png}
    \caption{Dataset map. West bound lanes containing the training vehicle trajectories are highlighted in blue. East bound lanes containing the testing vehicle trajectories are highlighted in orange.}
    \label{fig:map}
\end{figure}

%%from Tony: I agree we should move this to the appendix or for now just keep it out.
% automatic labeling strategy. First, we divided the trajectory of each vehicle into multiple driving segments based on changes in ego lane or the lead vehicle. For each driving segment, if the ego vehicle in the previous or the next segment belong to a different lane, we labeled the first or the last 30\% of the time steps in the current segment as merging and the rest as non-merging. We then trained two logistic regression classifiers to separately classify merging in the beginning and the end of a driving segment. The classifiers used as features the squared values of the ego vehicle's lane position, lateral speed, lateral acceleration, and heading error. Since there are much more non-merging than merging time steps, the classifiers were trained with a cost ratio of $2:1$ for mis-classifying a merging time step. We then visually examined the trained classifiers' labeling quality on a subset of the trajectories.  \textcolor{red}{[might move this to appendix.]}

\subsubsection{Feature Computation}
The input features to the IDM model are defined in (\ref{eq:idm1}) and (\ref{eq:idm2}). For BC and AIDM, we included an additional feature $\tau^{-1}$ defined as the rate of change of the visual angle of the lead vehicle from the ego driver's seat position divided by the angle itself. $\tau^{-1}$ can be considered as a perceptual-control analog of inverse time-to-collision, a feature commonly used in driver modeling \cite{bhattacharyya2020modeling, svard2021computational,engstrom2018simulating}, with the difference of incorporating the width of the lead vehicle into feature computation. This is consistent with recent findings on the impact of optimal expansion of the lead vehicle's image on driver longitudinal control behavior \cite{markkula2016farewell}. Furthermore, the inclusion of this feature puts the information contained in the inputs to BC and AIDM on a similar level to IDM, as IDM implicitly accounts for time-to-collision in its desired space headway computation in (\ref{eq:idm2}).

We computed all features in the Frenet frame (i.e., lane-centric coordinates \cite{werling2010optimal}), by first transforming vehicle positions, velocities, and headings into the Frenet frame using its current lane center line as the reference path and then computing the features from the transformed positions and velocities. We obtained drivers' instantaneous longitudinal control inputs by differentiating the Frenet frame longitudinal velocities. For BC and AIDM, we discretized the continuous control inputs into discrete action probability distributions using a Gaussian mixture model of 15 Gaussian components with mean and variance parameters chosen using the Bayesian Information Criteria \cite{murphy2012machine}.

\subsection{Model implementation}
In this section, we describe our approach of parameterizing the IDM, BC, and AIDM models. Our software implementation is publicly available \textcolor{red}{add github url as citation \footnote{Github url}}.

\textbf{IDM.} Following \cite{jung2022gray}, we parameterized the IDM model by treating the IDM policy as a conditional Gaussian distribution: $\pi(a_t|o_t=\{v_t, \Delta v_t, d_t\}) = \mathcal{N}(a_t|\mu_t, \sigma^2)$ with mean action $\mu_t$ and variance $\sigma^2$. The mean action $\mu_t$ is computed from the IDM rule defined in (\ref{eq:idm1}) and (\ref{eq:idm2}) by making the desired speed $\Tilde{v}$, minimum headway distance $d0$, desired headway time $\tau$, maximum acceleration rate $a_{max}$ and maximum deceleration rate $b$ adjustable. The action variance $\sigma^2$ is assumed to be independent of the input features. 

\textbf{BC.} Following \cite{bhattacharyya2020modeling}, we implemented the BC model with two types of neural network policies: standard Multi-Layer Perceptron (MLP) networks and recurrent neural networks (RNN). The MLP network takes as input the observation vector (normalized by training set statistics) and outputs a probability distribution over the discrete control actions. The RNN addresses the possibility that driver behavior may be influenced by the observation history rather than just the most recent observation. For the RNN, we combined a Gated Recurrent Unit (GRU) network \cite{chung2014empirical} with a MLP network, where the GRU network compresses the observation history into a fixed length vector, which is then transformed into the action distribution by the MLP network. In order to alleviate the causal confusion problem reported in prior works \cite{de2019causal, codevilla2019exploring, zhou2017recurrent}, we excluded ego speed and previous actions in the GRU network's input.

\textbf{AIDM.} We parameterized the discrete state transition probabilities $P(s_{t+1}|s_t, a_t)$ and the desired state distribution $\Tilde{P}(s_t)$ of AIDM using categorical distributions parameterized by their logits. We parameterized the observation distributions $P(o_t|s_t)$ using Normalizing Flow, a flexible class of neural network-based density estimator \cite{rezende2015variational, papamakarios2021normalizing}. Normalizing Flow uses invertible neural networks to transform simple distributions, e.g., Gaussian distributions, into complex and correlated distributions while maintaining the tractability of likelihood evaluation and sampling. In this work we used a single, shared Inverse Autoregressive Flow \cite{kingma2016improved} to transform a set of conditional Gaussians with mean vector $\mu(s_t)$ and covariance matrix $\Sigma(s_t)$. This provides the AIDM with adequate flexibility in modeling complex and nonlinear observation sequences and associating observed actions with agent beliefs. We parameterized a distribution over the agent's planning horizon using a Poisson rate parameter and used the QMDP method \cite{littman1995learning, karkus2017qmdp} as a closed-form approximation of the cumulative expected free energy in (\ref{eq:cumulative_efe}). Due to the intractability of entropy estimation of Normalizing Flows, we calculated EFE using entropy of the base observation distributions and ignored additional effect introduced by the Normalizing Flow. In subsequent sections, we refer to the transition and observation parameters with $\theta_1$, the desired state distribution and planning horizon parameters with $\theta_2$, and the combined parameters with $\theta = \{\theta_1, \theta_2\}$.

\textbf{Hyper-AIDM.} A distinct advantage of the AIDM model is the ability to visualize and interpret each individual component, i.e., the transition, observation, and preference probabilities. To demonstrate how such a structure can benefit the understanding of behavioral variations between individual drivers in the dataset, we formulated a model of a population of active inference agents, i.e., the drivers, using a hierarchical Bayesian model. Although it is possible to formulate a hierarchical Bayesian version of the IDM or BC model, we omit that analysis in this paper because the addition does not provide additional interpetablity associated with driver beliefs and cognitive dynamics.

Given each active inference agent can be represented by its parameters $\theta$, we assume there is a notion of an average agent representing the canonical behavior of all drivers in the dataset, i.e., the driver performs car following. The observed individual differences are created by deviating from the average agent. Let us denote the parameters of the average agent with $\mu_{\theta}$, we assume the individual agent parameters are created by a linear transformation:
\begin{align}\label{eq:factor_model}
    \theta = Az + \mu_{\theta}, z \sim \mathcal{N}(0, \mathbb{I})
\end{align}
where $z \in \mathbb{R}^{d}, d \ll |\theta|$ is a $d$ dimensional continuous variable sampled from a standard multivariate Gaussian distribution representing the individual drivers and $A$ is a transformation weight matrix. The vector $z$ can be interpreted as driver attributes that influences parameters $\theta$ and the resulting behavior. The average driver is one that is not influenced by $z$, i.e., $z = 0$. 

Specifically, we let $z$ control the following parameters of an active inference agent: 1) the transition matrix, 2) preference distribution, 3) means and covariance matrices of the base observation distribution, and 4) the planning horizon. Given this model generates the parameters of an active inference agent using another model, i.e., (\ref{eq:factor_model}), similar to Hypernetworks \cite{ha2016hypernetworks, krueger2017bayesian}, we refer to it as Hyper-AIDM. 

\subsection{Parameter Estimation}
\subsubsection{IDM, BC, and AIDM}
We estimated the parameters of IDM, BC, and AIDM models by maximizing the expected log likelihood of driver control inputs from the dataset under the corresponding control policy, i.e., (\ref{eq:bc_objective}). This procedure for the AIDM model differs slightly from IDM and BC by requiring a nested step. Between each parameter update in the nested procedure, we first computed the sequence of beliefs given the observation-action history using (\ref{eq:aif_belief}) and the optimal belief-action policy using (\ref{eq:aif_policy}). We then evaluated the log likelihood as a function of the computed beliefs:
\begin{align}\label{eq:aif_objective}
    \max_{\theta} \mathbb{E}_{o_{1:T}, a_{1:T} \sim \mathcal{D}}\left[ \sum_{t=1}^{T-1}\log \pi_{\theta}(a_{t}|b_{t,\theta_1})\right] \quad \text{{\small s.t.}} \quad  \pi_{\theta}(a_t|b_t) = \frac{e^{-\mathcal{G}_{t, \theta}^*(b_t, a_t)}}{\sum_{\Tilde{a}_t \in \mathcal{A}}e^{-\mathcal{G}_{t, \theta}^*(b_t, \Tilde{a}_t)}}
\end{align}

While (\ref{eq:aif_objective}) allows us to learn task-relevant beliefs about active inference agents as it depends on both $\theta_1$ and $\theta_2$, the parameters are fundamentally unidentifiable since there are potentially infinite sets of $\theta$ with the same likelihood \cite{reddy2018you, armstrong2017impossibility, bouchard2004tradeoff}. The consequence of this is learning an environment model that deviates significantly from the reality, which leads to a large number of infractions as a result of the agent not being able to recognize the actual environment state \cite{wei2022world}.

In order to constrain the hypothesis space and avoid configurations of $\theta$ that are incompatible with real-world constraints, we designed a hierarchical Bayesian model with a prior distribution $P(\theta)$ encoding likely configurations of $\theta$. Specifically, we designed the prior as $P(\theta) = P(\theta_1)P(\theta_2|\theta_1)$, where:
\begin{align}
    P(\theta_1) \propto \exp\left(\lambda \mathbb{E}_{o_{1:T}, a_{1:T} \sim \mathcal{D}}\left[\sum_{t=1}^{T}\log P_{\theta_1}(o_t|h_{t-1}, a_{t-1})\right]\right)
\end{align}
with hyperparameter $\lambda$ controlling how much the prior distribution prefers model accuracy, measured by expected log likelihood of observations. We let $P(\theta_2|\theta_1)$ be a uniform distribution. In our experiments, we only compute the Maximum A posteriori (MAP) estimate of the Bayesian model by converting the prior into the following loss function added to the objective in (\ref{eq:aif_objective}):
\begin{align}
    \mathcal{L}(\theta_1) = \lambda \mathbb{E}_{o_{1:T}, a_{1:T} \sim \mathcal{D}}\left[\sum_{t=1}^{T}\log P_{\theta_1}(o_t|h_{t-1}, a_{t-1})\right]
\end{align}
To prevent learning unreasonably large observation variance as a result of the observation entropy term in (\ref{eq:efe}), another symptom previously reported in \cite{wei2022world}, we applied a penalty on the $l2$ norm of the observation covariance parameters. 

\subsubsection{Hyper-AIDM}
Given that the individual driver $z$ vectors are not observed, we estimated the Hyper-AIDM model parameters $\mu_{\theta}$ and $A$ by maximizing the log marginal likelihood:
\begin{align}
    \max_{\mu_{\theta}, A} \mathbb{E}_{o_{1:T}, a_{1:T} \sim \mathcal{D}}\left[\log \int_{z}\prod_{t=1}^{T-1}\pi(a_t|b_t, z)P(z)\right]
\end{align}
Specifically, we maximized the evidence lower bound (ELBO) \cite{blei2017variational}, a tractable lower bound of the log marginal likelihood, under the auto-encoding variational Bayes framework \cite{kingma2013auto}:
\begin{align}
    ELBO = \mathbb{E}_{o_{1:T}, a_{1:T} \sim \mathcal{D}}\left[\mathbb{E}_{Q(z)}\left[\sum_{t=1}^{T-1}\log \pi(a_t|b_t, z)\right] + D_{KL}(Q(z) || P(z))\right]
\end{align}
where $Q(z)$ is a variational approximation of the posterior distribution $P(z|o_{1:T}, a_{1:T})$ modeled as a multivariate Gaussiain distribution with parameters generated by a GRU-MLP inference network. We evaluated the second expectation using a single sample from $Q(z)$.

Similar to the AIDM model, the Hyper-AIDM model is unidentifiable. We thus extended the penalty method in the previous section by formulating the following prior:
\begin{align}
    P(\mu_{\theta}, A) \propto \exp\left(R_1(\mu_{\theta}, A) + R_2(\mu_{\theta}, A) + R_3(\mu_{\theta}, A) \right)
\end{align}
where
\begin{align}
\begin{split}
    R_1(\mu_{\theta}, A) &= \lambda_1\mathbb{E}_{o_{1:T}, a_{1:T} \sim \mathcal{D}}\left[\log\int_z\prod_{t=1}^{T} \pi(a_t|b_t, z)P(z)\right] \\
    R_2(\mu_{\theta}, A) &= \lambda_2\mathbb{E}_{o_{1:T}, a_{1:T} \sim \mathcal{D}}\left[\log\int_z\prod_{t=1}^{T} P(o_t|h_{t-1}, a_{t-1}, z)P(z)\right] \\
    R_3(\mu_{\theta}, A) &= \lambda_3\mathbb{E}_{o_{1:T}, a_{1:T} \sim \mathcal{D}}\left[\log\int_z\prod_{t=1}^{T} P(o_t|h_{t-1}, a_{t-1}, z)Q(z)\right]
\end{split}
\end{align}
Here, $R_1$ and $R_2$ encode the prior belief that the average agent should achieve high marginal action and observation likelihood. $R_3$ encodes the prior belief that the individual agents should also achieve high observation likelihood with respect to their individual trajectories in addition to just the action likelihood. Furthermore, we constrained the matrix $A$ to have orthonormal columns, a standard practice to avoid unidentifiability of latent factor models \cite{murphy2012machine}. We evaluated the integrals using a single sample from the corresponding distributions. 

\subsubsection{Model selection}
We trained each model with 15 random seeds controlling model parameter initialization and dataset mini batch iteration orders. To select the hyperparameters for AIDM, we first trained the model for $\lambda = [0.2, 1, 4]$ and and then selected $\lambda = 1$ as it best trades off environment model accuracy and agent behavior predictive performance (with criteria described in the next section). We then set $\lambda_1 = 1$ and $\lambda_2 = \lambda_3 = 0.5$. 

\subsection{Model Evaluation and Comparison}
We evaluated and compared our models' ability to generate behavior similar to the human drivers in the dataset using both open-loop offline predictions and closed-loop online simulations. In both cases, we evaluated the models on two different held-out testing datasets. The first dataset includes vehicles from the same lanes as the training dataset. This dataset tests whether the models can generalize to unseen vehicles in the same traffic condition. We obtained this dataset by dividing all selected trajectories in the west-bound lanes using a 7-3 train-test ratio. The second dataset includes vehicles from the top two east-bound lanes in Fig. \ref{fig:map}. This dataset tests whether the models can generalize to unseen vehicles in novel traffic conditions, since the traffic in the east-bound lanes have on average higher speed and less density. We refer to these two datasets as \textit{in-distribution} and \textit{out-of-distribution}, respectively. We randomly selected 100 trajectories with a minimum length of 10 seconds from the in-distribution dataset and 75 trajectories with a minimum length of 5 seconds from the out-of-distribution dataset for testing.

\subsubsection{Offline Evaluation}
The goal of the offline evaluation was to assess each model's ability to predict a driver's next action based on the observation-action history contained in the held-out testing dataset. This task evaluates the models' ability to be used as a short-horizon predictor of other vehicles' behavior in an on-board trajectory planner \cite{sadigh2016planning}. We measured a model's predictive accuracy using Mean Absolute Error (MAE) of the predicted control inputs on the held-out testing datasets. For the IDM model, we calculated the predicted control inputs by sampling from the Gaussian policy. For BC and AIDM models, we first sampled a discrete action from the action distribution predicted by the models and then sampled from the corresponding Gaussian component in the Gaussian mixture model used to perform action discretization. For the Hyper-AIDM model, we evaluated both the prior and posterior agent's predictive accuracy by fixing the $z$ values on the prior and posterior means. Action prediction then proceeded in the same way as the AIDM model. 

\subsubsection{Online Evaluation}\label{sec:online_test}
Rather than prediction instantaneous actions, the goal of the online evaluation was to assess the models' ability to generate trajectories similar to human drivers such that they can be used as simulated agents in automated vehicle training and testing environments \cite{igl2022symphony}. This is fundamentally different from offline predictions because the models need to choose actions based on observation-action history generated by its own actions rather than those stored in the fixed, offline dataset. This can introduce significant covariate shift \cite{spencer2021feedback} sometimes resulting in situations outside of the model's training data, leading to poor action selection. 

We built a single-agent simulator where the ego vehicle's longitudinal acceleration is controlled by the trained models and its lateral acceleration is controlled by a feedback controller for lane centering. The lead vehicle simply plays back the trajectory recorded in the dataset. Other vehicles do not have any effect on the ego vehicle, given our observation space does not contain other vehicle related features. For the Hyper-AIDM model, we first used the inference network to generate the $z$ vector for the target vehicle. We then simulated the active inference agent with parameters generated from this $z$ vector. 

Following \cite{suo2021trafficsim}, We measured the similarity between the generated trajectories and the true trajectories using the following metrics:
\begin{enumerate}
    \item Average deviation error (ADE): deviation of the Frenet Frame longitudinal position from the dataset averaged over all time steps in the trajectory.
    \item Collision indicator: whether a collision with the lead vehicle occurred along the trajectory. 
\end{enumerate}


\subsubsection{Statistical Evaluation}
Following the recommendations made in \cite{colas2019hitchhiker, agarwal2021deep} for evaluating learned control policies, we represented the central tendency of a model's offline prediction and online control performance using the interquartile mean (IQM) of the respective performance metric. The IQMs are computed as the average performance metric of trajectories belonging to the middle 50\% of all tested trajectories. To compare the central performance difference between the AIDM and baseline models, we performed two-sided Welch's t-tests with 95 percent confidence level on the MAE-IQM values computed from different random seeds with the assumption that the performance distributions between two models may have different variances.

\section{Results and Discussion}
% \subsection{Model Accuracy and Behavior Prediction Trade Off}
% Fig. \ref{fig:map} shows the average per-step step observation log likelihood, MAE, and ADE values of the AIDM models for a range of different $\lambda$ values controlling the strength of prior belief on environment model accuracy. In particular, higher values of $\lambda$ led to higher observation likelihood and thus the model's ability to predict the observation signal in the next time step. However, this is met with the model's decreased ability to predict driver actions. Given the limited coverage of the dataset, e.g., the dataset rarely contains observation signals corresponding to near-crash scenarios, a faithful model will use most of its representational capacity on the dataset observations and thus preclude the possibility of explaining driver behavior as avoiding unseen, near-crash-like observations. This echo's the model-policy objective mismatch problem in model-based reinforcement learning, where a model trained to faithfully reconstruct the seen data can lead to inferior agent behavior \cite{lambert2020objective}. 

% Despite the inferiority in predicting actions, higher values of $\lambda$ however did not lead to significantly different online control behavior as measured by ADE. This was expected as online performance depends more strongly on the long-term trajectory distribution rather than instantaneous action predictions, ignoring the additional effect from the difference between simulation and real world physics. In order to balance behavior explanation and generation, we chose a fixed value of $\lambda = x$ in the following analysis. 

\subsection{Offline Performance Comparison}
Fig. \ref{fig:eval_offline} shows the offline evaluation results for each model with the model type plotted on the X-axis. The Y-axis shows the IQM of average MAE values (thereafter abbreviated as MAE-IQM) of action predictions along each testing trajectory. The color of the points in the figure represents the testing condition and each point for each model corresponds to a different random seed's result. AIDM achieved the lowest MAE-IQM in in-distribution tests, followed by RNN, MLP, and IDM. The corresponding Welch's t-test results in Table \ref{tab:offline_eval} suggest AIDM's in-distribution predictive performance advantage over each baseline model is significant (P < 0.001). The stronger predictive performance in AIDM and RNN is likely attributed to the fact that driver decisions depend on the history of past observations rather than just the most recent observation, which can be modeled by the recurrent structure of AIDM and RNN. Compared to baseline models, AIDM has the largest performance variance across models trained with different seeds. Empirically, these models converged to different training action and observation likelihoods, suggesting AIDM optimization might be prone to local optima with qualitatively different performance compared to baseline models.

\begin{figure}[!htb]
\centering
    \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=1\linewidth]{fig/offline_mae.png}
    \caption{}
    \label{fig:eval_offline}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=1\linewidth]{fig/online_mae.png}
    \caption{}
    \label{fig:eval_online}
    \end{subfigure}%
\caption{\textbf{(a)}: Offline evaluation results. Each column plots the interquartile mean of mean absolute action prediction error of the corresponding model. \textbf{(b)}: Online evaluation results. Each column plots the interquartile mean of mean average position deviation error from the true vehicle trajectory of the corresponding model. In both plots, each point corresponds to a random seed used to initialize model training and the color corresponds to the testing condition of either in-distribution or out-of-distribution.}
\end{figure}

\begin{table}[!htb]
\caption{Two-sided Welch's t-test results of MAE-IQM against baseline models. \textit{ID = in-distribution, OOD = out-of-distribution, * = statistically significant with 95 percent confidence interval.}}
\begin{subtable}{.5\linewidth}
\centering
\caption{Offline evaluation}
\label{tab:offline_eval}
\begin{tabular}{ccc}
    \hline
    Baseline & ID P-value & OOD P-value \\
    \hline
    IDM & $< 0.001^{*}$ & $< 0.001^{*}$ \\
    BC-MLP & $< 0.001^{*}$ & $0.73$ \\
    BC-RNN & $< 0.001^{*}$ & $0.90$ \\
    \hline
\end{tabular}
\end{subtable}%
\begin{subtable}{.5\linewidth}
\centering
\caption{Online evaluation}
\label{tab:online_eval}
\begin{tabular}{ccc}
    \hline
    Baseline & ID P-value & OOD P-value \\
    \hline
    IDM & $0.005^{*}$ & $< 0.001^{*}$ \\
    BC-MLP & $< 0.001^{*}$ & $< 0.001^{*}$ \\
    BC-RNN & $< 0.001^{*}$ & $< 0.001^{*}$ \\
    \hline
\end{tabular}
\end{subtable} 
\end{table}

The distances between the orange and blue points in Fig. \ref{fig:eval_offline} show that MLP and RNN achieved lower MAE-IQM in the out-of-distribution tests, with MLP's decrements being larger. This was expected since the out-of-distribution condition, i.e., the faster condition, requires less braking and accelerating, resulting in simpler predictions, albeit different observation statistics. In contrast, IDM and AIDM's MAE-IQM values in the out-of-distribution tests increased compared to the in-distribution tests, with the gap in IDM being substantially higher, although the Welch's t-tests in Table \ref{tab:offline_eval} suggest that AIDM's out-of-distribution predictive performance is not significantly different from those of MLP (P = 0.73) and RNN (P = 0.90). This suggests that IDM and AIDM's action prediction performance is sensitive to the training data distribution. This was expected for AIDM as it was trained to predict observation signals in the training dataset. Although IDM was not trained to explicitly predict observation signals, its control rule, as defined in (\ref{eq:idm2}), is based on a desired time gap, which depends on statistics of the traffic condition. \textcolor{red}{These observations point to an interesting connection between IDM and AIDM, i.e., both models make predictions based on expected outcome of the future.}

\subsection{Online Performance Comparison}
Fig. \ref{fig:eval_online} shows the online evaluation results for each model using the same format as the offline evaluation results. However, each point in Fig. \ref{fig:eval_online} represents the IQM of the ADE values along each testing trajectory, with blue and orange colors representing testing results from the in-distribution and out-of-distribution tests, respectively. In the in-distribution testing condition, all models achieved an IQM of ADE values between 1.8 and 2.8, which is approximately the length of a vehicle. This suggests all models can adequately imitate driver behavior in the dataset even when predicting control actions based on its own generated behavior rather than a static dataset. Among all models, MLP achieved the lowest ADE values, followed by AIDM, IDM, and RNN. The Welch's t-test results in Table \ref{tab:online_eval} show that AIDM's in-distribution online tests performance is significantly different from all baseline models (P $\leq$ 0.005). The lack of performance by RNN is not surprising. Although we tried to prevent learning causal confusion by excluding the ego speed from the RNN's observations, it might still have picked up other spurious correlations based on relative speed and distance. 

The distances between the orange and blue points in Fig. \ref{fig:eval_online} shows that similar to the offline evaluation results, MLP and RNN achieve lower MAE-IQM in the out-of-distribution tests than the in-distribution tests, while IDM achieved higher MAE-IQM in the out-of-distribution tests. In contrast to the offline evaluation results, AIDM also achieved lower MAE-IQM in the out-of-distribution tests. Welch's t-test results show that AIDM's out-of-distribution online test performance is significantly different from all baseline models (P < 0.001). Given the mechanistic similarity between IDM and AIDM predictions, this result suggests the AIDM's decisions are likely based on expectations different from the desired time gap in IDM. 

\subsection{Driver Decision Explanation}
Given that AIDM's decisions are emitted from a two-step process, i.e., forming beliefs about the environment state and selecting control actions that minimize free energy, the model's success at the car following task depends on the two sub-processes both independently and jointly. To assess the model components, namely the observation, transition, and preference distributions, independently, we developed visualizations of each component. To understand how the two sub-processes interact, we conducted simulations of agent beliefs (similar to the concept of imagination in model-based reinforcement learning \cite{hafner2019dream}). We verified both the individual components and agent belief simulation against expectations guided by driving theory \cite{fuller2005towards, engstrom2018great,summala1997hierarchical}.

\subsubsection{Independent Component Evaluation}
We used the observation distributions learned by the AIDM model to facilitate the understanding of its preference, prediction of state transitions, and hypothetical reactive behavior. The results are shown in Fig. \ref{fig:obs_scatter_data} - \ref{fig:obs_scatter_policy}.

Fig. \ref{fig:obs_scatter_preference} shows scatter plots of samples from the AIDM's state-conditioned observation distribution $P(o|s)$, where we sampled 200 observations from each state and plotted the samples for each pair of observation features. Each point in the plot is colored by the log of the preference probability of the associated state, where higher preference probability corresponds to brighter color. A comparison with Fig. \ref{fig:obs_scatter_data}, which plots observations sampled uniformly from the dataset, shows that the geometry of the scatters in Fig. \ref{fig:obs_scatter_preference} aligns with the empirical dataset. Furthermore, the highest preference probability corresponds to observations of zero $\tau^{-1}$, zero relative velocity, and relative distance near 15 m. This aligns with the task-difficulty homeostasis hypothesis that drivers prefer states in which the crash risk is manageable \cite{fuller2005towards}.

\begin{figure}[!htb]
\centering
    \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[height=0.15\textheight, width=1\linewidth]{fig/scatter_preference.png}
    \caption{}
    \label{fig:obs_scatter_preference}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[height=0.15\textheight, width=1\linewidth]{fig/scatter_stationary.png}
    \caption{}
    \label{fig:obs_scatter_stationary}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[height=0.15\textheight, width=1\linewidth]{fig/scatter_policy.png}
    \caption{}
    \label{fig:obs_scatter_policy}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[height=0.15\textheight, width=1\linewidth]{fig/scatter_data.png}
    \caption{}
    \label{fig:obs_scatter_data}
    \end{subfigure}
\caption{Visualization of AIDM model configuration. In panels (a), (b), and (c), we sampled from AIDM's state conditioned observation distributions and plotted the sampled points for each pair of observation feature combinations. In panel (d), we instead plotted observations sampled from the dataset. The points in each panel are colored by: (a) log probabilities of the preference distribution, (b) log probabilities of the stationary distribution of the controlled transition matrix, (c) AIDM's predicted accelerations upon observing the sampled signals from a uniform prior belief, and (d) accelerations from the dataset.}
\end{figure}

Fig. \ref{fig:obs_scatter_stationary} and \ref{fig:obs_scatter_policy} follow similar format. However, in Fig. \ref{fig:obs_scatter_stationary} the points are colored by the stationary distribution of the Markov chain parameterized by the AIDM's transition probabilities and controlled by the AIDM's policy. The controlled Markov chain transition probabilities were computed as $P(s'|s, \pi) = \sum_{a}P(s'|s, a)\pi(a|b=\delta(s))$, where $\delta(s)$ denotes a belief distribution with probability mass only on state $s$. The colors in this figure show that the model expects to visit states with zero $\tau^{-1}$, zero relative velocity, and relative distance of 15 m. The model also expects to visit states with higher magnitude $\tau^{-1}$, relative velocity, and relative distance with decreasing probabilities. This pattern is consistent with Fig. \ref{fig:obs_scatter_preference} and shows that the model not only prefers risk-neutral states but also expects to maintain risk-homeostasis in a dynamic environment.

In Fig. \ref{fig:obs_scatter_policy}, the points are colored by the model's chosen control actions, assuming the model has a uniform prior belief about the environment state before receiving the sampled observation. This figure shows that the model tends to choose negative accelerations when the relative speed and $\tau^{-1}$ are negative and relative distance is small, and positive accelerations in the opposite case. The brightness gradient of different positions in Fig. \ref{fig:obs_scatter_policy} is consistent with that in Fig. \ref{fig:obs_scatter_data}, where the points are colored by accelerations in the empirical dataset. This shows that the model has learned driver acceleration tendency in the dataset.

\subsubsection{Agent Belief Simulation}
The visualizations in the previous section show that the AIDM had learned a preference distribution that align with known driver behavior theory, and it correctly expects to match the preference distribution given its control policy. However, they do not show how uncertainty in the belief updating process in a dynamic scenario interacts AIDM's action selection. 

To this end, we simulated a set of 100-second (1000-time step) sequences of observations and actions in the AIDM's imagination space, with each sequence starting from a different belief state. Specifically, we created two copies of the trained AIDM, using the first copy as the traffic environment which receives agent control action and generates observations and the second copy as the agent which receives observations and generates control actions. Importantly, the first copy knows the true state, whereas the second copy does not. 

An example of the simulation is shown in Fig. \ref{fig:belief_sim}. The first three plots on the left show time series of simulated relative distance, relative speed, and $\tau^{-1}$, respectively. The last plot on the left shows the agent's control actions in response to the simulated observations. The heat maps on the right, also indexed by time plotted on the X-axes, show the true imagined states, the inferred belief states, and the predicted actions, respectively, where the state and actions indices are respectively ordered by the average $\tau^{-1}$ and acceleration magnitude of the corresponding state. These figures show that the inferred belief states closely tracked the true states, and the agent selected large braking or accelerating actions whenever the relative speed and $\tau^{-1}$ were substantially negative or positive. This aligns with the observation in Fig \ref{fig:obs_scatter_policy} but verifies it against a dynamic scenario. 

\begin{figure}[!htb]
\centering
    \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[height=0.4\textheight, width=1\linewidth]{fig/sim_obs.png}
    \caption{}
    \label{fig:belief_sim_obs}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[height=0.4\textheight, width=1\linewidth]{fig/sim_state.png}
    \caption{}
    \label{fig:belief_sim_state}
    \end{subfigure}%
\caption{Example of agent belief simulation, where we simulated an 100-second (1000-time step) interactive process between two copies of a trained AIDM model, one generates/imagines observation while the other one reacts with control actions. In the left panel, the first three plots show the imagined observations in time series and the last plot shows the agent's acceleration responses. In the right panel, the first two heat maps show the true and the inferred belief states, with state indices sorted by the magnitude of the corresponding mean $\tau^{-1}$. The last heat map shows the agent's predicted discrete action distributions with action indices sorted by the corresponding mean acceleration magnitudes.}
\label{fig:belief_sim}
\end{figure}

Fig. \ref{fig:belief_sim_corr} shows the correlations between each observation feature and acceleration in the belief simulation (top row) and the empirical dataset (bottom row). It shows that correlations in the belief simulation are consistent with the empirical association in that acceleration is proportional to both relative speed and $\tau^{-1}$. This provides partial evidence that the model beliefs are not only consistent with the empirical dataset and known driver behavior theory independently and in static settings, as shown in the previous section, but they are also temporally consistent and internally coherent.

\begin{figure}[!htb]
    \centering
    \includegraphics[height=0.3\textheight, width=1\linewidth]{fig/sim_corr.png}
    \caption{Observation-action correlations in belief simulation (top row) and empirical dataset (bottom row). Each plot shows the correlation between an observation variable (X-axis) and acceleration (Y-axis) chosen by either the AIDM agent or drivers in the dataset.}
    \label{fig:belief_sim_corr}
\end{figure}

\subsubsection{Trajectory Diagnostics}
This section analyzes AIDM's diagnostic ability by visualizing driver beliefs about the environment states and actions to be taken on trajectories where it makes less accurate predictions. A sequence of beliefs is calculated by applying (\ref{eq:aif_belief}) to each trajectory in the dataset using the ground truth action.

An example is shown in Fig. \ref{fig:traj_diagnostics}. The three plots on the left show the relative distance, speed, and $\tau^{-1}$ observed by the driver along the trajectory, respectively. The three plots on the right show the ground truth discretized action, action predicted by the ADIM model, and beliefs computed by the AIDM model, respective. In the right plots, action indices are sorted by their corresponding value and state indices are sorted by the corresponding mean $\tau^{-1}$. This figure shows that driver belief closely track the observed relative speed and $\tau^{-1}$. However, the model makes highly uncertain predictions for the majority of the trajectory. At $t=10$ and $t=120$, AIDM's predictions are multi-model where one of the modes coincides with the true actions. The multi-modality suggests substantial inter-driver variability in the dataset that cannot be fully explained by a single model. 

\begin{figure}[!htb]
    \centering
    \includegraphics[height=0.3\textheight, width=1\linewidth]{fig/trajectory_belief.png}
    \caption{Trajectory diagnostics. }
    \label{fig:traj_diagnostics}
\end{figure}

To understand how the multi-modality may be a result of driver beliefs, we summarized driver beliefs by computing the MAE between posterior prediction of observation and the true observations along each trajectory. We then plotted the observation MAE against action MAE in Fig. \ref{fig:o_mae}, where each column corresponds to a different observation modality. This figure shows a strong correlation between relative speed and $\int^{-1}$ prediction accuracy and action prediction accuracy, suggesting these to observation modalities are the most crucial for predicting driver longitudinal control behavior in car following. Furthermore, it suggests a possibility to improve action prediction accuracy by aligning driver beliefs with the actual observation, highlighting the importance of belief calibration in driving.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\linewidth]{fig/o_mae_vs_u_mae.png}
    \caption{}
    \label{fig:o_mae}
\end{figure}

\subsection{Driver Characteristics Analysis}
Driver characteristics analysis reveals the relationships between the driver vector $z$, driver parameters $\theta$, action predictions, and the overall behavior of the respective trajectories. Given that driver vector $z$ is modeled to have orthogonal and additive impact on driver parameters, similar to the Principal Component Analysis \cite{murphy2012machine}, we selected elements of $z$ with the highest impact on driver parameters and visualized their effects on driver parameters by interpreting the corresponding rows in the transformation matrix $A$ and observing their effects on driver belief dynamics. 

First, we verified that improvements in driver action prediction accuracy is correlated with increasing magnitude of $z$. This was expected as $z$ provides the model with extra degrees of freedom to explain nuanced individual behavior not captured by the average agent. Two principle components explained 80 \% of the variance in driver parameters. We will thus focus on these two components in subsequent parameters interpretations. 

The first row in Fig. \ref{fig:umap} shows a UMAP clustering of driver trajectories in the dataset, where the position of each point corresponds to the location of the trajectory in the UMAP embedding space. In each sub plot, the color of the points correspond to the average observation feature along the trajectory indicated by the color bar legend. This figure shows that trajectories in the dataset mainly differ with respect to their average relative distances with minor variations in average relative velocities. 

\begin{figure}[!htb]
    \centering
    \includegraphics[height=0.4\textheight, width=1\linewidth]{fig/umap_clustering.png}
    \caption{UMAP clustering of trajectory-averaged observation features (row 1), action prediction MAE reductions (row 2 column 1), and principle component values (row 2 columns 2 and 2). }
    \label{fig:umap}
\end{figure}

To see how dataset heterogeneity correlates with model predictions, we used the same plotting format but instead colored the point by reduction in action prediction MAE from the prior to posterior and the 2 PCs, shown in the second row of Fig \ref{fig:umap}. The trends in these plots match with the trends in empirical features, namely, increasing average relative distance correlates with increasing reduction of action prediction MAE, increasing values of PC0 and decreasing values of PC1. 

To understand how the PCs impact driver parameters and lead to action prediction improvements, we first visualized driver observation model when conditioning on different z values, where we saw substantial variations in the observation sampled compared to the prior, i.e. $z = 0$. To understand how z correlate with driver observation predictions and subsequently lead to changes in action prediction, we computed the average observation reconstruction error when conditioning on z of the respective trajectory and when z is zero. Fig. \ref{fig:delta_rec_mae} shows the correlations between trajectory averaged observation features and changes in observation reconstruction MAE, both normalized by the empirical variance of each observation feature. Each point in the plot represents a trajectory and is colored by the reduction in action prediction MAE. The figure shows that the model makes worse prediction of relative distance when conditioning on z than when not, however, there is no substantial different between the posterior and prior models' abilities to predict relative speed and $\tau^{-1}$. Fig \ref{rec_error_pc}. plots the correlation between the correlation between trajectory averaged observation features and observation reconstruction error with points colored by their associated $z$ values. This figure shows a quadratic relationship between trajectory averaged relative distance and relative distance reconstruction error, both associated with a strong correlation with PC1. This suggests that for trajectories with high average relative distance, where the corresponding PC is low, there is a tendency to \emph{underestimate} the value of relative distance. The second columns shows a visible negative correlation between trajectory averaged relative velocity and model reconstruction error, although not strongly correlated with PC values. This suggests a tendency to also underestimate relative velocity in drivers. There is a much weaker correlation between PC0 and the empirical and predicted relative distance. The correlations between the empirical and predicted $\tau^{-1}$ in both PCs are less pronounced. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1\linewidth]{fig/delta_rec_mae.png}
    \caption{Observation reconstruction MAE change from prior to posterior vs. trajectory-averaged observation features colored by action prediction MAE reduction.}
    \label{fig:delta_rec_mae}
\end{figure}

\begin{figure}[!htb]
\centering
    \begin{subfigure}[b]{1\linewidth}
    \includegraphics[width=1\linewidth]{fig/rec_error_pc_0.png}
    \end{subfigure}
    %
    \begin{subfigure}[b]{1\linewidth}
    \includegraphics[width=1\linewidth]{fig/rec_error_pc_1.png}
    \end{subfigure}%
\caption{Observation reconstruction error vs. trajectory-averaged observation features colored by principle component values.}\label{rec_error_pc}
\end{figure}

To understand the direct effects of the principle components on driver behavior, we visualized the transformation $A$. We found no substantial weight values in the initial belief, transition, and planning horizon parameters. However, we found substantial weight values in the observation distribution parameters, leading to observable changes in visualization of the observation samples. The weights corresponding to the observation means for PC0 and PC1 are shown in Fig \ref{fig:pc_obs}, where the columns represent observation modality and each row correspond to a different state, with state index sorted by $\tau^{-1}$. PC1 has substantial weight values on the expected relative distances for states with low expected $\tau^{-1}$, and PC0 has substantial weight values on the expected relative speeds for states with high expected $\tau^{-1}$. To measure the effects of observation modification of agent beliefs and subsequent action selection, we plotted the correlation between the average KL divergence of adjacent beliefs along each trajectory and the associated action prediction MAE reduction in Fig \ref{fig:belief_kl}. This figure shows a positive correlation between belief divergence and reduction in action prediction error. This suggests improvements in action prediction performance can be attributed to more dynamic beliefs. 

\begin{figure}[!htb]
\centering
    \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=1\linewidth]{fig/pc_obs.png}
    \caption{}\label{fig:pc_obs}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=1\linewidth]{fig/delta_belief_kl.png}
    \caption{}\label{fig:belief_kl}
    \end{subfigure}%
\caption{(a) Principle component weights on driver observation expectations. (b) Correlations between average belief Kl divergence and reduction in action prediction MAE.}
\end{figure}

In summary, we have shown that the Hyper-AIDM captured the variations in the dataset using a 2-principle component model driver parameters. Relating the principle components to dataset attributes and changes in driver observation and action predictions, our results show that the heterogeneous behavior of driver who generated trajectories of high relative distance can be explained by their underestimation of the actual relative distance. This leads to more dynamic changes in their beliefs and thus more responsive behavior. 

\section{General Discussion}
In this paper, we proposed active inference as a framework for modeling driver control behavior in car following. Compared to IDM, an instance of rule-based driver model, and Behavior Cloning, an instance of data-drive driver model, the proposed AIDM shows comparable performance in replicating naturalistic driver behavior in both offline prediction and online control settings. The main advantage of active inference is the modeling of driver beliefs, which affords it superior transparency to BC. By visualizing the observation, transition, and preference components of the AIDM and simulating its beliefs, we saw that AIDM can learn knowledge about car following recorded in prior theories of driver behavior. Introspection of driver beliefs on specific trajectories show that dynamic and adaptive beliefs are a crucial component to nuanced human driving behavior. 

\section{Conclusions}

\section{Acknowledgements}

\newpage

\bibliographystyle{trb} %Change this to APA 7th.
\bibliography{ref.bib}
\end{document}
