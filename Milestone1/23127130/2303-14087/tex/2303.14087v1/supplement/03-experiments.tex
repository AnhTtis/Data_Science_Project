\section{Additional results}
\label{sec:supp:results}

In this section, we provide information about the size of our models (\Cref{sec:supp-model-size}), additional quantitative results on depth, RGB-D images, and the test set (\Cref{sec:supp-quant-results}), breakdown of \opdformerp performace on different parts and comparisons of different training strategies for \opdformerp (\Cref{sec:supp-analysis}).  In addition, we provide additional qualitative examples (\Cref{sec:supp-qual-results}), visualization of the transformer attention maps (\Cref{sec:supp-attention}), and a discussion of object part consistency (\Cref{sec:supp-part-consistency}).

\input{sections/tables/tab-count-para-flop}


\input{supplement/figtables/tab-results-depth-mini.tex}
\input{supplement/figtables/tab-results-rgbd-mini.tex}

\input{supplement/figtables/tab-results-opdall-test-mini.tex}

\input{sections/tables/tab-analysis-part-cat.tex}


\subsection{Model size}
\label{sec:supp-model-size}

In \Cref{tab:model-param-count}, we show the number of parameters and FLOPs for each of our models.  We see from \Cref{tab:model-param-count} that our \opdformerbaseline models have slightly less parameters than the \opdrcnn models.  The \opdformerp model with Swin-L backbone is considerably larger (5x). 

\subsection{Additional quantitative results}
\label{sec:supp-quant-results}

We present additional quantitative results comparing our proposed \opdformerbaseline with \opdrcnn, including results on depth and RGBD inputs.
We also present results on the test set.
Results consistently show that \opdformerp outperforms other variants. 

\mypara{Results for depth and RGBD inputs.}
\Cref{tab:results-OPDAll-val-depth-mini,tab:results-OPDAll-val-rgbd-mini} report the results using depth and RGBD as input with val set from three different datasets.
We see that the \opdformerbaseline variants perform better than \opdrcnn methods across input image formats.
Compared with using RGB input (see main paper Tab. 3), for part motion prediction, using D input only gives slightly worse results while using RGBD input gives slightly better results. 

\mypara{Results on the test set.}
\Cref{tab:results-OPDAll-test} evaluates the different methods on RGB images from the test set of the three datasets.  The performance on the test set largely follows that of the validation sets, with \opdformerbaseline variants outperforming \opdrcnn and per-part object pose (\opdformerp) providing the best performance.

\input{supplement/figtables/tab-results-mask2former-compare}

\subsection{Additional analysis}
\label{sec:supp-analysis}

\mypara{What part types are more challenging?}
\Cref{tab:analysis-part-cat} analyzes performance on three part categories: \drawer, \door, \lid.
We find \lid to be the most challenging to detect on all three datasets.
On \ourdatamulti, all three part types are much more challenging with considerably lower \partdet.

\mypara{Comparison between Mask2Former and \opdformerbaseline}. 
We compare the performance of our \opdformerbaseline with the original Mask2Former, without any additional motion losses, to check whether having extra losses would impact the detection performance. 
We report results on \ourdatamulti val set with RGB input in \Cref{tab:results-mask2former-comp}.  We start with the Mask2Former weights pretrained on the COCO dataset~\cite{lin2014microsoft} and then consider different training strategies. For all experiments except for (2), we pretrain on OPDReal before we train on \ourdatamulti. For Mask2Former, we initialize the model with weights pretrained on OPDReal.  For training \opdformerbaseline (row 1), we compare training from pretrained model just on COCO dataset (and not further pretrained on OPDReal, row 2), vs starting with the weights from the Mask2Former model pretrained on \ourdatamulti (row 3) vs training \opdformerbaseline directly from a pretrained model on \ourdatareal (row 4). 
From \Cref{tab:results-mask2former-comp}, we see that \opdformerbaseline performance without pretraining (row 2) on \ourdatareal is lower than models with pretraining (row 1,3,4).  
For models with pretraining, they have very similar part detection performance (\partdet) but pretraining the motion parameters on \ourdatareal (row 4) results in better motion prediction, especially for motion axis and origin (+\mtype{}\axis, +\mtype{}\axis{}\orig).  

\subsection{Additional qualitative results}
\label{sec:supp-qual-results}

We provide additional qualitative results on both the \ourdatacad and \ourdatareal datasets.
\Cref{fig:vis-compare-real-synth-single} shows the qualitative results for selected models on the \ourdatacad and \ourdatareal datasets.
Overall, we again see that our proposed \opdformerbaseline variants have better performance than \opdrcnn methods and can predict more consistent motion parameters.

We also present qualitative visualizations of how the different variants of \opdformerbaseline performs on \ourdatamulti dataset (see   \Cref{fig:vis-compare-opdformer}). 
From the first two columns, we see that \opdformerp can perform better even when the detected mask is similar to other variants.
Overall, \opdformerp has better prediction ability when there are multiple object present, illustrating the importance of predicting the object pose on a per-part basis.


\subsection{Visualizing the transformer attention masks}
\label{sec:supp-attention}

We visualize the attention maps of the transformer architecture following the same layer selection for the masked attention as Mask2Former~\cite{cheng2021masked}.
Figure \ref{fig:vis-attention} shows the visualizations.
We choose the attention maps of the last three masked attention layers, which use image features with different resolutions.
From the visualization, we see that the masked attention assigns high weight on the openable parts.

\input{supplement/figtables/fig-vis-compare-real-synthetic-single}
\input{supplement/figtables/fig-vis-compare-opdformer-multi}


\input{supplement/figtables/fig-vis-attention.tex}


