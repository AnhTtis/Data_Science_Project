\section{Introduction}


Detecting the openable parts of real-world objects and predicting how the parts can move is useful in developing intelligent agents that can assist us with everyday household tasks.
Consider the simple task of `getting a spoon from the cabinet drawer'.
To achieve this, we need to identify what part of the cabinet is the drawer, that the drawer is openable, and that it opens with a translational motion.
Interest in tackling this problem has led to recent work focusing on mobility prediction of articulated object parts.

Prior work on mobility prediction aims to identify moving parts of an object, and predict the motion type and parameters of each moving part from a complete mesh~\cite{hu2018functionality} or 3D point cloud~\cite{wang2019shape2motion,hu2018functionality,yan2019rpm,shi2021self,shi2022p3}.
Recent work also considered mobility prediction from partial point clouds~\cite{li2020category} or depth images~\cite{jain2020screwnet,jain2022distributional}.
These methods rely mainly on depth information as input.
Another common limitation is strong category-specific assumptions or reliance on prior knowledge.
For instance, \citet{li2020category} assume a fixed kinematic chain (i.e. a separate model is trained for three-drawer cabinets vs two-drawer cabinets).

\input{sections/figs/teaser}

Recently, \citet{jiang2022opd} introduced the task of Openable Part Detection (OPD) where the openable parts and their corresponding motion parameters are predicted for a single articulated object from a single-view image (RGB, depth, or RGB-D).
This approach is object category agnostic, as it detects an arbitrary number of openable parts using Mask R-CNN~\cite{he2017mask} and predicts motion parameters for each part independently.
However, this work focus on single-object image and 
does not handle real-world scene layouts with potentially multiple objects, each with potentially multiple openable parts (e.g., real-world kitchens contain several cabinetry and drawer units).
To study OPD in real-world scenes with multiple objects, we introduce \ourtask, a challenging dataset of images with annotated part masks and motion parameters from real-world scenes containing multiple objects.
We create this dataset by leveraging recent work on articulated 3D scenes by \citet{mao2022multiscan}.

As noted in prior work~\cite{li2020category,jiang2022opd}, the motion parameters of openable parts (e.g., the direction in which a drawer slides open) is strongly correlated.
Since we have multiple objects in real scenes, we also need to model object pose for each part (in contrast to \citet{jiang2022opd} who only handle single objects).
We observe that parts in a object inform the movement of other parts in the same object, and that the pose of one object can inform the pose of another.
For instance, given the cabinets shown in \Cref{fig:teaser}, all the drawers move along the same axis, while the rotation axis of the doors is perpendicular to the translation axis of the drawers.  Objects are also likely to be placed either parallel or perpendicular to each other.
Thus, by leveraging features of other parts and the pose we can better detect and predict articulation parameters for each part.


We propose \opdformerbaseline, a part-informed transformer architecture leveraging self-attention to produce more globally consistent motion predictions.
The self-attention of this architecture better leverages the above observations of strong correlation between part positions, part mobility parameters, and object pose.
We compare three variants of our model: predicting directly in camera coordinates, predicting parts with a na\"ive single global pose, and with object pose predicted per-part.
We benchmark \opdformerbaseline against prior work and show that with the stronger architecture
and with per-part object pose prediction, we outperform prior methods by up to $10\%$ on openable part detection \& motion prediction with the same R50 backbone. Performance is further improved relative to baselines using a Swin-L backbone.

In summary, we make the following contributions: i) we construct a more realistic image-dataset for OPD with multiple objects ii) we propose a part-informed transformer architecture that leverages part--part and part--object pose correlations; iii) we systematically evaluate our approach and show it achieves state-of-the-art performance on the OPD task for both the single and multi-object setting. 

