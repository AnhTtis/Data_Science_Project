\begin{figure*}[t]
\includegraphics[width=\linewidth]{figure/structure/OPDFormer-figure-V4.2.pdf}
\vspace{-15pt}
\caption{
Our \opdformerbaseline architecture is based on the Mask2Former~\cite{cheng2021masked} architecture.
The left side shows the overall network while the right shows the mask module in detail.  The Mask2Former employs an image backbone and pixel decoder to obtain pixel-level embeddings, which are passed to a transformer decoder with masked attention together with learnable part queries to learn embeddings that are used to predict the part type and mask (by the mask module).
To obtain a high-resolution mask, Mask2Former uses a multi-scale strategy with visual feature maps at increasing resolutions, each of which are fed into the transformer decoder.  The transformer decoder unit is then stacked for $L$ layers.
We enhance the mask module to predict the part motion parameters (motion type, origin, axis) in addition to the part type and mask (see green boxes). The part bounding box is computed directly from the mask.
We investigate three variants of the architecture that predict motion parameters either directly in camera coordinates (-C), or in object coordinates which are then transformed to camera coordinates via a global pose or a per-part object pose.
The pose prediction variants are indicated in \textcolor{orange}{orange} dashed boxes: global pose (`-O' at middle bottom), and per-part object pose (`-P' top right).
The detected parts in the center correspond to the part queries.
}
\label{fig:network-structure}
\end{figure*}

\section{Approach}

We adopt the detect-and-predict strategy for openable part detection and motion parameter estimation, following \citet{jiang2022opd}.
Our architecture replaces the Mask R-CNN~\cite{he2017mask} detection component with Mask2Former~\cite{cheng2021masked}. 
Mask2Former uses the transformer decoder to predict instance masks and classes, matching against ground truth using the Hungarian algorithm during training.
We extend the Mask2Former architecture to predict part motion parameters in the mask module, and create three variants of the architecture that predict the motion parameters using different coordinate frames.
Our key differences from \citet{jiang2022opd} are that: 1) we replace the MaskRCNN segmentation architecture with Mask2Former; and 2) we introduce a per-part object pose prediction (instead of a global object pose prediction).


\subsection{Model variants}

All models eventually predict motion parameters in camera coordinates (\textbf{C}).
However, as noted in prior work~\cite{li2020category,jiang2022opd}, it is useful to predict motion parameters in the object coordinate frame as the motion axes are often parallel to one of the main axes of the object (see supplement).
The object pose is used as a bridge to transform between the object coordinate frame and the camera coordinate frame.


\citet{jiang2022opd} used the entire image to predict a single object pose, ignoring the fact that there could be multiple objects with different poses. 
To alleviate this limitation, we develop two variants of our architecture for pose prediction, predicting a single \emph{global} pose vs predicting a different object pose for each \emph{part}.
By predicting the object pose per part, we can handle multiple objects without explicitly detecting each object.
This allows us to have an object-agnostic method that can generalize across object categories.
In addition, we consider a base variant that predicts directly in the camera coordinates.

\mypara{Camera coordinates.} 
The base variant \opdformerc does not predict the object pose, and predict the motion parameters in the camera coordinate directly (see \Cref{fig:network-structure}, without orange dashed boxes).  Note that it is the direct analogue of the Mask R-CNN based baseline \opdrcnn-C from \citet{jiang2022opd}.

\mypara{Single global pose.}
In this variant, we predict a single global pose for all objects and parts in the input image.
This is predicted directly from the image features of the entire image (see \Cref{fig:network-structure} dashed box with label `-O').
We call this variant \opdformero as it is the direct analogue of the OPDRCNN-O introduced in OPD~\cite{jiang2022opd}.
For \ourdatamulti, we train with the scene coordinates defining a global pose, and transform relevant motion parameters from camera coordinates to these scene coordinates.


\mypara{Per-part object pose.}
When there are multiple objects in an image, each of the objects can have a different pose and its openable parts would have motion parameters strongly correlated with that object's pose.
To account for this, we add an additional head for each part that predicts the object's pose (see \Cref{fig:network-structure} dashed box with `-P' label).
We call this variant \opdformerp and compare it against \opdrcnnop, an extension of the MaskRCNN based model from \citet{jiang2022opd} to predict per-part object pose.
For \ourdatamulti, we leverage the object oriented bounding boxes in MultiScan~\cite{mao2022multiscan} to obtain object poses and transform motion parameters to object coordinates for training.

\mypara{Parameterization.}
In all variants, the motion parameters and object pose are parameterized in the same way as \citet{jiang2022opd}'s \opdrcnn: motion axis and motion origin are 3-dim vectors and object pose is a 12-dim vector (9 for rotation and 3 for translation).
Motion type prediction is trained with a cross-entropy loss and other motion parameters and object pose use a smooth L1 loss with $\beta=1$.

\subsection{Network Architecture and Losses}

The overall architecture and mask module with per-part prediction heads are shown in the left and right sides of \Cref{fig:network-structure}.
Our architecture uses a Mask2Former module for part segmentation and self-attention over parts.
We use the same multiscale pixel decoder and transformer decoder (with 100 queries) as in \citet{cheng2021masked}, and an R50 backbone for fair comparison with \citet{jiang2022opd}'s \opdnet.

For the segmentation and motion losses, we add the auxiliary loss after each transformer decoder.
The object pose loss is determined by the specific architecture variant and is either a single loss term or one loss term per part.

\mypara{Segmentation losses.}
We use the same set of losses as Mask2Former~\cite{cheng2021masked}, including the binary cross-entropy loss ($L_\text{ce}$) and the dice loss ($L_\text{dice}$)~\cite{milletari2016v} for the mask segmentation, and cross-entropy loss ($L_\text{cls}$) for the mask classification: $L_\text{seg} = \lambda_{ce} L_\text{ce} + \lambda_{dice} L_\text{dice} + \lambda_{cls} L_\text{cls}$.
We adopt the loss weights proposed in Mask2Former, $\lambda_{ce} = 5, \lambda_{dice} = 5$ and $\lambda_{cls} = 2$ for matched predictions and $0.1$ for unmatched.

\mypara{Motion losses.}
Motion prediction losses are based on \opdrcnn~\cite{jiang2022opd}.
We use a cross entropy loss for the motion type ($L_\text{c}$), combined with smooth L1 losses for the motion axis ($L_\text{a}$) and motion origin ($L_\text{o}$): $L_\text{mot} = \lambda_c L_c + \lambda_a L_a + \lambda_o L_o$.
We also use the same loss weight ratios.
Specifically, we set $\lambda_c = 2, \lambda_a = 16, \lambda_o = 16$ for our experiments.

\mypara{Object pose loss.}
Object pose prediction is trained under the smooth L1 loss ($L_\text{pose}$) with $\lambda_\text{pose} = 30$.

We sum all of the above losses to obtain the overall loss used during training: $L = L_\text{seg} + L_\text{mot} + \lambda_\text{pose} L_\text{pose}$.
