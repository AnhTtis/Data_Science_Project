\section{Experiments}

We compare our proposed architecture against baselines on both single object datasets (\ourdatacad, \ourdatareal from \citet{jiang2022opd}) and the new multiple object dataset we created (\ourdatamulti).
We also conduct an analysis of part consistency on single objects and the challenges of handling multiple objects.
In the main paper, we present experiments for RGB input images.
See the supplement for results with depth only (D) and RGBD, and additional analysis.

\subsection{Implementation details} 
Our architecture is based on Mask2Former~\cite{cheng2021masked} as implemented in Detectron2~\cite{wu2019detectron2}.
We use the R-50 backbone Mask2Former model pretrained on COCO~\cite{lin2014microsoft} instance segmentation to initialize our weights, and train with AdamW~\cite{loshchilov2017decoupled}.
The learning rate and other hyperparameters match those used by Mask2Former.
Our experiments are carried out on a machine with 64GB RAM and an RTX 2080Ti GPU.
We train each model end-to-end for $60000$ steps and pick the best checkpoint based on val set performance (+\mtype{}\axis{}\orig).
Models evaluated on \ourtask are first pretrained on \ourdatareal and then finetuned on the \ourdatamulti train split.
The \opdrcnn baselines are first pretrained on \ourdatacad, then \ourdatareal, and finally \ourdatamulti.

We note that the predicted object pose rotation matrix is not guaranteed to be a valid rotation matrix. \citet{jiang2022opd} did not address this issue.
We convert the predicted rotation matrix into a unit quaternion and back using PyTorch3D~\cite{ravi2020accelerating} to ensure a valid rotation.
The results for \opdnet are approximately the same as without such post-processing.
For \ourdatamulti, we use a confidence threshold of $0.8$ to determine whether a predicted part is valid.


\subsection{Experimental setup}

For single objects, we evaluate our method on two datasets introduced in OPD \cite{jiang2022opd}, \ourdatacad and \ourdatareal.
For multiple objects, we evaluate on \ourdatamulti.

\mypara{Metrics.}
We use the evaluation metrics for part detection and motion prediction from \citet{jiang2022opd}. 
The metrics extend the traditional mAP metric for detection to the motion prediction task, including two main metrics: mAP@IoU=0.5 for the predicted part label and 2D bounding box (\partdet).
For each metric, the detection is further constrained by whether: motion type is matched (+\mtype{}), motion type and motion axis are matched (+\mtype{}\axis{}), and whether motion type, axis and origin are all matched (+\mtype{}\axis{}\orig), within predefined error thresholds.
Note that \citet{jiang2022opd}'s metrics were only defined for inputs with openable parts.
Since we have frames with no openable parts, we measure the percentage of those we correctly predicted as having no openable parts.

\mypara{Methods.}
We compare variants of our \opdformerbaseline with the MaskRCNN-based \opdrcnn~\cite{jiang2022opd}.
We compare the following variants: predicting directly in camera coordinates (\textsc{-C}), vs predicting a single global pose (\textsc{-O}) vs predicting per-part object poses (\textsc{-P}). 

\input{sections/tables/tab-results-opdall-val-mini}

\input{sections/figs/fig-vis-compare-real-multi}

\subsection{Results}

\Cref{tab:results-OPDAll-val-mini} evaluates the different methods on RGB input images from the val set of the three datasets.  We report the mean and standard error across three runs with different seeds. 
See the supplement for depth and RGB-D input results, motion averaged metrics, and for performance on the test set.
\Cref{fig:vis-compare-real-multi} shows example predictions on \ourdatamulti, and the supplement provides qualitative results on \ourdatacad and \ourdatareal.


Our \opdformerbaseline variants outperform the \opdrcnn baselines on all metrics.
One reason is the stronger part detection (\partdet) provided by the Mask2Former backbone.
We note that the \opdformerbaseline variants with the R50 backbone actually have fewer parameters than \opdrcnn methods, indicating that the performance gains are not due to increased parameters.
For example, \opdrcnnop has 46.1M parameters whereas \opdformerp has 42.0M parameters.
This observation is similar for other \opdformerbaseline variants and corresponding \opdrcnn baselines (see supplement).   


\mypara{Are camera coordinates useful?}
As observed in \citet{jiang2022opd}, predicting motion parameters in object coordinates and predicting the object pose (\opdrcnno) outperform prediction in camera coordinates (\opdrcnnc).
This is true for the single object case (\ourdatacad and \ourdatareal), but not for \ourdatamulti where the assumption of one global object coordinate does not hold.

\mypara{Is having per-part object pose prediction important?}
When we take motion parameters into account, we see the advantage of per-part object pose predictions with the transformer-based architecture (\opdformerp).
On the main metric (+\mtype{}\axis{}\orig), our per-part \opdformerp consistently outperforms the global \opdformero, which in turn outperforms \opdrcnno by \citet{jiang2022opd}.
Interestingly, \opdrcnnop does not help over \opdrcnno for the single object scenario.

\input{sections/tables/tab-results-split-case.tex}


\input{sections/tables/tab-results-pose-error-mini}


\mypara{How challenging is \ourdatamulti?}
\ourdatamulti is much more challenging than the single object \ourdatareal data.
As expected, the best performing model (\opdformerp) for \ourdatamulti makes use of the per-part object pose prediction.  
There is a significant difference in performance between \opdformerp and \opdformero  for \ourdatamulti, but less for single objects.
This is because in the single object scenario having one global pose is sufficient.


\mypara{Analysis by number of openable objects.}
To better understand performance on \ourdatamulti we evaluate on all images in \ourdatamulti grouping into images with zero, one, or multiple openable (articulated) objects (AO).
\Cref{tab:results-OPDMulti-split} shows that \opdrcnn-based methods are better at avoiding false predictions on images without any openable parts.
For images with one or more openable objects, \opdformerp makes the most accurate predictions (highest +\mtype{}\axis{}\orig).
We also see that multiple AO is more challenging with both the part detection (\partdet) and motion parameter predictions (+\mtype{}\axis{}\orig) being much lower than the single AO case.



\mypara{What part types are more challenging?}
We find that \lid is the most challenging to detect on \ourdatacad.
We suspect this is due to limited data and variability of the \lid shape.
See the supplement for a detailed analysis.

\input{sections/figs/failure-case.tex}

\mypara{How good are the predicted object poses?}
To check whether \opdformerbaseline provides improved object pose predictions, we evaluate the object pose directly by measuring the rotation error (angle between two rotation matrices) and translation error (Euclidean distance normalized by the object diagonal length).
Following prior work~\cite{li2021leveraging}, we report the median error and accuracy at different thresholds.
For rotation accuracy, we use thresholds of $5^\circ$ degree, and for translation accuracy, we use a threshold of $0.1$.
\Cref{tab:results-pose-error-mini} shows the results of above object pose evaluation metric on the val sets of \ourdatacad and \ourdatamulti. We can see that \opdformerbaseline variants all  outperform \opdrcnno, indicating that the transformer structure can give better pose predictions.  For \ourdatamulti, the rotation and translation error are much higher than for \ourdatacad, illustrating the challenge of our \ourdatamulti scenario. 
Furthermore, our \opdformerp has better object pose prediction than \opdformero, indicating the importance of having per-part object pose prediction.  
In the single setting (\ourdatacad), the part-weight-average global pose gives the best object pose prediction. 



\input{sections/tables/tab-results-backbone-compare}
\mypara{Effect of backbone.}
Most of our experiments use the R50 backbone as it is smaller and requires fewer resources to train.
We check performance with Swin-L, a more powerful backbone compared to R50.
\Cref{tab:results-backbone} shows that with the Swin-L backbone, \opdformerp outperforms the R50 backbone in all cases.
Even when the part detection performance is roughly the same for OPDSynth dataset, the motion prediction is considerably higher (by 4.5\%).
Note that \opdformerp with Swin-L backbone (with 200 queries for the transformer decoder) has 205.6M parameters, which is around $5\times$ larger than the R50 backbone.


\mypara{Failure case analysis.}
\Cref{fig:fail} shows some failure cases.
Many errors occur due to the limited field-of-view and significant cropping of openable parts (see first column).
In the second and third column unclear edges lead to part detection failures.
In the fourth column motion type prediction fails due to a rotating door with drawer-like features.
The last column is an incorrect prediction of a wall as a door.
