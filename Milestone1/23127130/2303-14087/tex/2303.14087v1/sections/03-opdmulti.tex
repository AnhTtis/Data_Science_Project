\section{\ourtask Task}

The OPD task seeks to identify all openable parts and their motion parameters from a single-view image $I$.
Specifically, to output a set of openable parts $P = \{p_1 \ldots p_k\}$ where an openable part is defined to be a drawer, door, or lid.
The output for each part is a segmentation mask $m_i$, 2D bounding box $b_i$, semantic label $l_i \in \{\drawer, \door, \lid\}$, and motion parameters $\phi_i$ specifying motion type $c_i \in \{\mttrans, \mtrot\}$, motion axis direction $a_i \in R^3$ and motion origin $o_i \in R^3$ (for revolute joints only).
\citet{jiang2022opd} focused on single-object images, which feature one object with at least one openable part.  
Here, we generalize the task and create a new dataset of real-world scenes with multiple objects, each possessing potentially multiple openable parts.
We call this new task and associated dataset \ourtask, reflecting the multi-object setting.

\subsection{\ourdatamulti dataset construction}
\label{sec:dataset}

\input{sections/figs/fig-vis-data-compare}
\input{sections/tables/tab-data-opdmulti-stats-compare.tex}
\input{sections/figs/stats_frames.tex}
\input{sections/tables/tab-data-opdmulti-stats.tex}

To create \ourtask image dataset, we leverage MultiScan~\cite{mao2022multiscan}, a dataset of RGB-D reconstructions of real indoor scenes providing object and part-level annotations.
We use the RGB-D video frames in this dataset along with part and part articulation annotations to create our \ourdatamulti dataset.

Specifically, we sample frames from RGB videos in the MultiScan dataset and project object and part segmentation masks to the image plane.
We also process the annotated motion parameters and object poses to the same format as the \ourdatacad and \ourdatareal datasets~\cite{jiang2022opd}.
Unlike prior work, we keep the full image resolution instead of center-cropping to avoid dropping objects that appear on the sides.
Since some frames may contain small or partial parts that are cropped and hard to detect, we ignore openable part annotations that cover less than $5\%$ of the image pixels.
Overall, we use 273 scans from 116 MultiScan scenes to create our image dataset, following the MultiScan train/val/test set split.  

We find that some of the projected annotations are noisy and inaccurate.
To ensure that our evaluation dataset is of high quality, we manually inspect all frames in the val and test splits and indicate whether they have \emph{mask} or \emph{motion} errors.
Mask errors are typically caused by reconstruction issues (e.g., a door with glass panes is not fully reconstructed so when projected onto the image the annotated mask is incomplete).
We also observe shifts in the mask for some frames if the estimated camera poses are not consistent with the final reconstruction. 
We find that 512 val set and 1749 test set openable part mask annotations are noisy (out of a total of 6077 val and 4704 test openable parts).
For mask error cases, we manually correct the mask using the Toronto Annotation Suite~\cite{torontoannotsuite}.
See the supplement for details.

\subsection{\ourdatamulti dataset statistics}

Following \ourdatacad and \ourdatareal~\cite{jiang2022opd}, we focus on three openable part types (drawer, door, lid) that are common across many object categories.
\Cref{tab:data-opdmulti-stats-compare,tab:data-opdmulti-stats} provides dataset statistics.
Our \ourdatamulti contains 33 object categories with at least a door, drawer, or lid (23, 15, 8 categories respectively).
Example categories include cabinets, refrigerators, wardrobes, microwaves, washing machines, nightstands, toilets, printers, and rice cookers, with a long-tailed distribution from frequent (182 cabinets) to infrequent (7 rice cookers).
Since our focus is on rigid openable objects, we do not include non-rigid object such as bags in our dataset.

In \Cref{fig:data-compare} we compare the images in \ourdatamulti vs prior datasets.
Note that images from \ourdatamulti are more varied with frames showing a variable number of openable objects including multiple openable objects, single openable object with natural background clutter, and frames with no openable objects.
\Cref{fig:frame-distribution} shows the distribution over number of objects and location of part pixels for the images in the resulting dataset.
From the inset, which shows the distribution of openable part pixels aggregated across the frames, we can see that \ourdatamulti has a broader part pixel distribution than \ourdatareal.
For \ourdatareal, most of the openable parts are in the center while in \ourdatamulti the openable parts are spread more evenly across the frame.
Overall, the images in \ourdatamulti are more diverse with both distant and close-up views and views from different angles.