%------------------------------------------------------------------------
\section{Method}
\label{sec:method}
In this section, we describe the detailed architecture of the proposed network, MAIR. Let $K$ denote the number of viewpoints; then, the inputs to the network are $K$ triples, where each triple is composed of an RGB image with $H$,$W$ size ($\mathrm{I} \in \mathbb{R}^{3 \times H \times W}$), depth map ($\Tilde{\mathrm{D}} \in \mathbb{R}^{H \times W}$), and its confidence map ($\Tilde{\mathrm{C}} \in \mathbb{R}^{H \times W}$). $\Tilde{\mathrm{D}}$ and $\Tilde{\mathrm{C}}$ are obtained using a state-of-the-art MVS model\cite{giang2021curvature}. We designed a three-stage structure that progressively estimates the normal, direct lighting, material, and spatially-varying lighting. The entire MAIR pipeline is summarized in Fig.~\ref{fig:whole}.

\subsection{Stage 1 - Target View Analysis Stage}
Stage 1 of MAIR comprises three estimation networks: Normal map (NormalNet), \textbf{In}cident \textbf{D}irect \textbf{L}ighting (InDLNet), and \textbf{Ex}itant \textbf{D}irect \textbf{L}ighting  (ExDLNet). Inspired by recent studies \cite{cis2020, vsg, lighthouse}, we adopt spatially-varying spherical Gaussians (SVSGs)~\cite{cis2020} and volumetric spherical Gaussian (VSG)~\cite{vsg} for the representation of incident lighting and exitant lighting, respectively. 

\noindent{\bf Normal map estimation.} 
Unlike single-view-based methods\cite{cis2020, irisformer2022, Li22, vsg}, where normal information should be inferred from the scene context, the normal map ($\Tilde{\mathrm{N}}$) can be directly derived from the depth map. Thus, NormalNet can show robust performance especially for real-world images, where the distribution of image contents and geometry largely differs from the training data. Still, use of other available information, including the RGB, depth gradient map ($\nabla\Tilde{\mathrm{D}} \in \mathbb{R}^{H \times W}$), and confidence map, can help NormalNet better handle unreliable depth predictions. Specifically, NormalNet is formulated as follows:
\vspace{-2mm} 
\begin{equation}\label{eqn:eq_normal}
\Tilde{\mathrm{N}} = \text{NormalNet}(\mathrm{I},\Tilde{\mathrm{D}}, \nabla\Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}), \Tilde{\mathrm{N}} \in \mathbb{R}^{3 \times H \times W}.
\end{equation}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/mvanet.pdf}
   \caption{An illustration of MVANet when $K$=3. MVANet creates a value vector by encoding color, context feature, and specular feature, and uses multi-view weights as attention to create multi-view aggregated features. Since our goal is to obtain the BRDF of target-view($1$-view), in level-2, only the value vector of target view is processed.}
   \label{fig:mvanet}
\end{figure*}

\noindent{\bf Incident direct lighting estimation.} 
Given $\mathrm{I}$, $\Tilde{\mathrm{D}}$, $\Tilde{\mathrm{C}}$, and $\Tilde{\mathrm{N}}$ obtained using NormalNet, we estimate SVSGs as a lighting representation of incident direct lighting, which is proven to be effective in modeling environment map~\cite{cis2020}. The proposed InDLNet is formulated as follows: 

\vspace{-2mm} 
\begin{equation}
\{\boldsymbol{\xi}_{s}\}, \{\lambda_{s} \}, \{\boldsymbol{\eta}_{s} \} = \text{InDLNet}(\mathrm{I}, \Tilde{\mathrm{N}}, \Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}),
\label{eqn:incident_SGs}
\end{equation}

where $\boldsymbol{\xi}_s \in \mathbb{R}^2 $ is the direction vector outward from the center of the unit sphere, $\lambda_s \in \mathbb{R}$ is sharpness, and $\boldsymbol{\eta}_s \in \mathbb{R}^3$ is intensity. The environment map is then parameterized with $S_D$ SG lobes ${\{\boldsymbol{\xi}_s, \lambda_s, \boldsymbol{\eta}_s\}_{s=1}^{S_D}}$.  For the $s$-th SG, its radiance $\mathcal{G}(\boldsymbol{l})$ in the direction $\boldsymbol{l} \in \mathbb{R}^2$ can be obtained as

\vspace{-2mm} 
\begin{equation}
\mathcal{G}(\boldsymbol{l};\boldsymbol{\eta}_s,\lambda_s,\boldsymbol{\xi}_s)=\boldsymbol{\eta}e^{\lambda_s(\boldsymbol{l}\cdot \boldsymbol{\xi}_s -1)},
\end{equation}
Using all $S_D$ SG lobes, the incident radiance $\mathcal{R}_{i}(\boldsymbol{l})$ in the direction $\boldsymbol{l}$  is expressed as

\vspace{-2mm} 
\begin{equation}
\mathcal{R}_{i}(\boldsymbol{l}) = \displaystyle\sum_{s=1}^{S_D} \mathcal{G}(\boldsymbol{l};\boldsymbol{\eta}_{s},\lambda_{s},\boldsymbol{\xi}_{s}),
\end{equation}
Li \etal\cite{cis2020} used $S_D=12$ to represent complex spatially-varying lighting; however, we found $S_D=3$ to be sufficient to model much simpler direct lighting. Also we used global intensity to make the SVSGs spatially coherent.

\noindent{\bf Exitant direct lighting estimation.} 
Although effective, the above environment map alone is insufficient to model lighting in a 3D space. Thus, we adopt a voxel-based representation called VSG~\cite{vsg} to further model exitant direct lighting. ExDLNet estimates exitant direct lighting volume $\Tilde{\mathrm{V}}_\text{DL}$ as

\vspace{-4mm} 
\begin{equation}
\Tilde{\mathrm{V}}_\text{DL} = \text{ExDLNet}(\mathrm{I}, \Tilde{\mathrm{N}}, \Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}), \Tilde{\mathrm{V}}_\text{DL} \in \mathbb{R}^{8 \times X \times Y \times Z},
\end{equation}
where $X$, $Y$, and $Z$ are the sizes of the volume. Each voxel in $\Tilde{\mathrm{V}}_\text{DL}$ contains opacity $\alpha$ and SG parameters ($\boldsymbol{\eta}, \boldsymbol{\xi}, \lambda$). From VSG, alpha compositing in the direction $\boldsymbol{l}$  allows us to calculate the incident radiance $\mathcal{R}_{e}(\boldsymbol{l})$ as follows:
\vspace{-2mm} 
\begin{equation}
\mathcal{R}_{e}(\boldsymbol{l}) = \displaystyle\sum_{n=1}^{N_R}\prod_{m=1}^{n-1} (1-\alpha_m)\alpha_n \mathcal{G}(-\boldsymbol{l};\boldsymbol{\eta}_{n},\lambda_{n},\boldsymbol{\xi}_{n}),
\end{equation}
where $N_R$ is the number of ray samples, and $\boldsymbol{\eta}_{n}$,$\lambda_{n}$, and $\boldsymbol{\xi}_{n}$ are the SG parameters of the sample. 

The intensity at which light converges at a point, \ie, incident radiance, serves as guidance for the network to infer diffuse and specular reflections. Meanwhile, information on how light is present in 3D space, \ie, exitant radiance, helps the network infer indirect lighting. Fig.~\ref{fig:DL_explain} is an explanation of the incident/exitant direct lighting. Note that these two representations are not convertible and have different physical-meanings. Please see supplementary for a detailed description of direct lighting.

\vspace{-2mm} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/DL_explain_main.pdf}
  \caption{Explanation of the incident/exitant direct lighting. E means per-pixel direct lighting environment map.}
  \label{fig:DL_explain}
\end{figure}
\vspace{-2mm} 

\subsection{Stage 2 - Material Estimation Stage}
For BRDF estimation, the specular radiance must be considered. To obtain a specular radiance feature $\boldsymbol{f}_{\text{spec}}$, we propose a SpecNet. The diffuse radiance $\boldsymbol{\kappa}$ and specular radiance $\boldsymbol{\gamma}$ of the microfacet BRDF model~\cite{microfacet} are as follows:

\vspace{-3mm} 
\begin{equation}
\boldsymbol{\kappa} = \frac{\boldsymbol{a}}{\pi}\int_{\boldsymbol{l}} L(\boldsymbol{l})\boldsymbol{n}\cdot\boldsymbol{l} \,d\boldsymbol{l}, \boldsymbol{\kappa} \in \mathbb{R}^3
\label{eqn:diffuse_brdf}
\end{equation}

\vspace{-2mm} 
\begin{equation}
\boldsymbol{\gamma} = \int_{\boldsymbol{l}} L(\boldsymbol{l})\mathcal{B}_s(\boldsymbol{v}, \boldsymbol{l}, \boldsymbol{n}, r) \boldsymbol{n}\cdot\boldsymbol{l} \,d\boldsymbol{l}, \boldsymbol{\gamma} \in \mathbb{R}^3
\label{eqn:specular_brdf}
\end{equation}

where $\boldsymbol{a}$ is diffuse albedo, $\boldsymbol{l}$ is lighting direction, $L(\boldsymbol{l})$ is lighting intensity, $\mathcal{B}_s$ is specular BRDF, $\boldsymbol{n}$ is normal, $r$ is roughness, and $\boldsymbol{v}$ is viewing direction, respectively. Since \cref{eqn:specular_brdf} is highly complicated, it is necessary to efficiently encode inputs to make it easier for the network to learn. To this intent, we first rewrite the arguments of $\mathcal{B}_s$ as following:

\vspace{-2mm} 
\begin{equation}
[\mathcal{F}(\boldsymbol{v},\boldsymbol{h}), (\boldsymbol{n}\cdot \boldsymbol{h})^2, \boldsymbol{n}\cdot \boldsymbol{l}, \boldsymbol{n}\cdot \boldsymbol{v}, r],
\end{equation}

where $\mathcal{F}$ is the Fresnel equation, and $\boldsymbol{h}$ is the half vector. Then, we approximate the lighting of \cref{eqn:specular_brdf} with the SVSGs of \cref{eqn:incident_SGs}. 
%Conceptually, 
Since each SG lobe ${\{\boldsymbol{\xi}, \lambda, \boldsymbol{\eta} \}}$ can be thought of as an individual light source, $\boldsymbol{\xi}$, $\boldsymbol{\eta}$, and $\lambda$ can be regarded as $\boldsymbol{l}$, $L(\boldsymbol{l})$, and a parameter to approximate the integral, respectively. Consequently, $\boldsymbol{\gamma}$ can be read as follows:

\vspace{-3mm} 
\small
\begin{equation}
\boldsymbol{\gamma} = {\sum\limits_{s=1}^{S_D} g(\mathcal{F}(\boldsymbol{v},\boldsymbol{h}_s), (\boldsymbol{n}\cdot \boldsymbol{h}_s)^2, \boldsymbol{n}\cdot \boldsymbol{\xi}_s, \boldsymbol{n}\cdot \boldsymbol{v}, \boldsymbol{\eta}_s, \lambda_s, r)},
\label{eqn:specular_approx}
\end{equation}
\normalsize
where $g$ is a newly defined function from our reparameterization. Using \cref{eqn:specular_approx}, we define SpecNet as follows:

\vspace{-5mm} 
\begin{multline}\label{eqn:fspec}
\boldsymbol{f}_{\text{spec}}^{k} = \displaystyle\sum_{s=1}^{S_D} m_s\text{SpecNet}(\mathcal{F}(\boldsymbol{v}_k,\boldsymbol{h}_{s,k}), (\Tilde{\boldsymbol{n}}\cdot \boldsymbol{h}_{s,k})^2, \\
\Tilde{\boldsymbol{n}} \cdot \boldsymbol{\xi}_s, \Tilde{\boldsymbol{n}} \cdot \boldsymbol{v}_k, \boldsymbol{\eta}_s, \lambda_s), 
\end{multline}
\vspace{-4mm} 
\begin{equation}\label{eqn:eq_mask}
m_s = \begin{cases}
   1 &\text{if } \lVert \boldsymbol{\eta}_s \rVert_1\Tilde{\boldsymbol{n}} \cdot \boldsymbol{\xi}_s > 0, \\
   0 &\text{else, } 
\end{cases}
\end{equation}
where $k$ is $k$-th view. A binary indicator $m_s$ is used to exclude SG lobes from $\boldsymbol{\gamma}$ if the intensity of the light source ($\lVert \boldsymbol{\eta}_s \rVert_1$) is 0 or the dot product of the normal and light axis ($\Tilde{\boldsymbol{n}} \cdot \boldsymbol{\xi}_s$) is less than 0. Since SpecNet approximates \cref{eqn:specular_brdf} with this physically-motivated encoding, $\boldsymbol{f}_{\text{spec}}$ can include feature for specular radiance information.
In addition to SpecNet, we use ContextNet to obtain a context feature map $\boldsymbol{f}_{\text{context}} = \text{ContextNet}(\mathrm{I},\Tilde{\mathrm{D}},\Tilde{\mathrm{C}},\Tilde{\mathrm{N}})$ that contains the local context of the scene. All views share $\boldsymbol{f}_{\text{context}}$ of the target view. 

Next, a \textbf{M}ulti-\textbf{V}iew \textbf{A}ggregation network (MVANet) is used to aggregate $\boldsymbol{f}_\text{spec}$, $\boldsymbol{f}_\text{context}$, and RGB across the pixels from all $K$ views, which corresponds to the target view pixel considering MVS depths. However, some of these pixel values might have negative effect if they are from the wrong surfaces due to occlusion. 
To consider occlusion, the depth projection error in $k$-view, denoted as $e_k = \max(-\log(\mid \Tilde{d}_k - z_k \mid), 0)$, is calculated. $\Tilde{d}_k$ is the depth at the pixel position obtained by projecting a point seen from the target view onto $k$-view, and $z_k$ is the distance between the point and the camera center of $k$-view. The depth projection error $\boldsymbol{e} \in \mathbb{R}^K$ is obtained by aggregating $e_k$ from all $K$ views. We use multi-view weight $\boldsymbol{w} = \frac {\boldsymbol{e}} {||\boldsymbol{e}||_1}, \boldsymbol{w} \in \mathbb{R}^K$ as attention weights during the multi-view feature aggregation in MVANet. Our intuition for material estimation is to consider the mean and variance of RGB. MVANet first encodes the input for each view to produce a value vector $\boldsymbol{q}$, and produces a mean and variance of $\boldsymbol{q}$ according to $\boldsymbol{w}$\cite{ibrnet}. It is encoded again and produces a multi-view aggregated feature $\boldsymbol{m}$. Since $\boldsymbol{m}$ is created from weighted means and variances, it has multi-view information considering occlusion. This process is repeated once again for the target view. See Fig.~\ref{fig:mvanet} for detailed structure of MVANet. 

Since MVANet exploits only local features, long-range interactions within the image need to be further considered for inverse rendering\cite{irisformer2022}. Thus, we propose RefineNet for albedo ($\Tilde{\mathrm{A}} \in \mathbb{R}^{3 \times H \times W}$), roughness($\Tilde{\mathrm{R}} \in \mathbb{R}^{H \times W}$) estimation using $\boldsymbol{f}_{\text{BRDF}}$ from MVANet.

\vspace{-5mm} 
\begin{equation}\label{eqn:eq_refine}
\Tilde{\mathrm{A}}, \Tilde{\mathrm{R}} = \text{RefineNet}(\mathrm{I},\Tilde{\mathrm{D}},\Tilde{\mathrm{C}},\Tilde{\mathrm{N}}, \boldsymbol{f}_{\text{BRDF}}, \boldsymbol{f}_{\text{context}}).
\end{equation}


\subsection{Stage 3 - Lighting Estimation Stage}
In stage 3, \textbf{S}patially \textbf{V}arying \textbf{L}ighting Estimation Network(SVLNet) infers 3D lighting with direct lighting, geometry, and material. To this end, we create a visible surface volume ($\mathrm{T} \in \mathbb{R}^{10 \times X \times Y \times Z}$). Although Wang \etal\cite{vsg} used a similar representation, they used a Lambertian reflectance model, which cannot represent complex lighting due to specularity. In contrast, we initialize $\mathrm{T}$ by reprojecting $\mathrm{I}, \Tilde{\mathrm{N}}, \Tilde{\mathrm{A}}, \Tilde{\mathrm{R}}$, which can model specularity. For each voxel, let $(u, v)$ and $d$ denote the projected coordinate of the center point and the depth, respectively. Then, the local feature $\boldsymbol{t} \in \mathbb{R}^{10}$ for each voxel is initialized as follows.
\begin{equation}
\boldsymbol{t} =\left [\rho \mathrm{I}(u,v), ~\rho \Tilde{\mathrm{N}}(u,v), ~\rho \Tilde{\mathrm{A}}(u,v), ~\rho \Tilde{\mathrm{R}}(u,v)\right ], 
\end{equation}
where $\rho = e^{-\Tilde{\mathrm{C}}(u,v)(d-\Tilde{\mathrm{D}}(u,v))^2}$. Note that the confidence ($\Tilde{\mathrm{C}}$) is used to reflect the accuracy of the depth. $\mathrm{T}$ and $\Tilde{\mathrm{V}}_\text{DL}$ are fed to SVLNet, producing outputs $\Tilde{\mathrm{V}}_\text{SVL}$ representing 3D spatially-varying lighting volume, as follows:
\vspace{-1mm} 
\begin{equation}
\Tilde{\mathrm{V}}_\text{SVL} = \text{SVLNet}(\Tilde{\mathrm{V}}_\text{DL}, \mathrm{T}), \Tilde{\mathrm{V}}_\text{SVL} \in \mathbb{R}^{8 \times X \times Y \times Z}.
\end{equation}
In previous work\cite{vsg}, they used implicit global feature volume, which is unclear what information it contains, but we specified $\Tilde{\mathrm{V}}_\text{DL}$ explicitly so that the network can learn the interaction of light source, material, and geometry. 