
\section{OpenRooms FF dataset}\label{B}
We created a dataset for multi-view inverse rendering called OpenRooms Forward Facing (Openrooms FF) dataset. Openrooms FF is an extension of the existing single-view inverse rendering dataset, OpenRooms~\cite{openrooms2021}, and most of resources to build the dataset are provided by the authors of OpenRooms~\cite{openrooms2021}, including data sources and creation tools. The materials, however, were unavailable due to the licensing issue, so we had to purchase materials from Adobe Stock~\cite{adobestock} except for 200 materials that were not found from Adobe Stock; instead, we replace them with other similar materials. We selected 23,618 images from the OpenRooms dataset by filtering out the images in which the camera looks at a wall or window, lacks textures in the scene, or object is too close to the camera. Then, we rendered forward facing multi-view images of 3 $\times$ 3 arrays by moving camera in eight directions: up, right up, right, right down, down, left down, and left, left up using the OptiX-based renderer~\cite{optixrenderer}. The baseline was set proportionally to the average depth of the scene to observe the change in the specular radiance. See Fig.~\ref{fig:33} for a multi-view images sample. As a result, a total of 212,562 (9 $\times$ 23,618) images were created and 27,000 (9 $\times$ 3000) images were separated into test dataset. OpenRooms FF consists of HDR RGB images, diffuse albedo images, roughness images, normal maps, binary masks, depth maps, per-pixel environment maps. We rendered images at 640 $\times$ 480 resolution but resized to 320 $\times$ 240 with bilinear interpolation for the training/test. The OpenRooms FF is summarized in Tab.~\ref{tab:openroomsFF}.


\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/supp_33.png}
   \caption{Sample of forward facing multi-view images in OpenRooms FF.}
   \label{fig:33}
\end{figure}

\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{|l|c|c|}
\hline
& Dataset & Training / Test \\
\hline
HDR RGB & 640 $\times$ 480  & 320 $\times$ 240 \\
Diffuse Albedo & 640 $\times$ 480  & 320 $\times$ 240 \\
Roughness & 640 $\times$ 480  & 320 $\times$ 240 \\
Normal & 640 $\times$ 480  & 320 $\times$ 240 \\
Mask & 640 $\times$ 480  & 320 $\times$ 240 \\
Depth & 640 $\times$ 480  & Not used \\
per-pixel DL & 40 $\times$ 30 $\times$ 32 $\times$ 16  & 40 $\times$ 30 $\times$ 16 $\times$ 8 \\
per-pixel SVL & 160 $\times$ 120 $\times$ 32 $\times$ 16  & 160 $\times$ 120 $\times$ 16 $\times$ 8 \\
\hline
\end{tabular}
\caption{Data type and resolution of OpenRooms FF. Spatially-varying lighting (SVL) has a spatial resolution of 160 $\times$ 120 and an angular resolution of 32 $\times$ 16.}
\label{tab:openroomsFF}
\end{table}


\section{Direct Lighting Details}\label{C}
Since the intensity($\boldsymbol{\eta}_s$) of incident direct lighting is the intensity of the light source, it is unrelated to pixel location. Thus we use global intensities ${\boldsymbol{\eta}_s}$ rather than per-pixel intensities. Instead, per-pixel visibility ${\mu_s \in \mathbb{R}}$ was used to account for occlusion. To enhance the dynamic range of the SG lobes, we use the non-linear transformation~\cite{cis2020}. The ablation study results for $S_D$ in SVSGs of incident direct lighting are shown in Tab.~\ref{tab:abl_SD}. Please see \cref{eqn:eq_lossDL} for $\mathcal{L}_{\text{reg}}$. Direct lighting performance improved as $S_D$ increased, but GPU Memory also increased. We chose $S_D=3$ considering its performance and GPU usage. Fig.~\ref{fig:DL_example} shows the incident(SVSGs) / exitant($\Tilde{\mathrm{V}}_\text{DL}$) direct lighting estimation results. SVSGs generally performed better because $\Tilde{\mathrm{V}}_\text{DL}$ estimates 3D volume, while SVSGs directly estimates 2D per-pixel environment map(E). Also, even though the consistency between them is not considered, since they are trained with the same ground truth(GT), they are consistent enough as shown in the Fig.~\ref{fig:DL_example}.



\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{|c|c|c|c|}
        \hline
         $S_D$ & si-MSE & {$\mathcal{L}_{\text{reg}}$} & GPU Memory(GB). \\
         \hline
        1 & 0.106 & 0.136 & 10.8\\ 
        \hline
        2 & 0.103 &0.127 & 11.26\\
        \hline
        \textbf{3} & \textbf{0.101} & \textbf{0.092} & \textbf{12.72}\\
        \hline
        4 & 0.101 & 0.081 & 13.43\\
        \hline
        6 & 0.100 & 0.061 & 14.94\\
        \hline
\end{tabular}
\caption{The ablation study results for $S_D$ in SVSGs.}
\label{tab:abl_SD}
\end{table}


\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/DL_example.pdf}
   \caption{Direct lighting environment map~($16\times8\times3$) estimation results for OpenRooms FF.} 
   \label{fig:DL_example}
\end{figure*}


\section{Analysis of Lighting Estimation Results}\label{D}
We have analyzed spatially-varying lighting quality in detail. Since the SVLNet implementation is quite memory-hungry, the resolution of our $\Tilde{\mathrm{V}}_\text{SVL}$ is $128^3$, which is low compared to the image resolution ($320\times240$ ). Also, because the field-of-view of our camera setup is limited, the lighting of the out-of-view area must rely on context inference about the dataset. Fig.~\ref{fig:L_analysis} shows the per-pixel lighting estimation results for the OpenRooms FF test scene. In the Fig.~\ref{fig:L_analysis}, our estimation approximates the overall outline of the GT better than Li~\etal\cite{cis2020} , but fails to mimic the high frequency details of the GT due to limitations in resolution and field-of-view.


\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/L_analysis.pdf}
   \caption{Per-pixel environment map~($32\times16\times3$) estimation results for OpenRooms FF.}
   \label{fig:L_analysis}
\end{figure*}



\section{View Synthesis}\label{E}
While image-based rendering(IBR) can perform view interpolation excellently, the view-dependent effect of highly specular objects, such as chrome spheres, is difficult to reproduce using IBR. Physically-based rendering(PBR) can handle this view-dependent effect realistically, but PBR requires scene material, geometry, and spatially-varying lighting that is difficult to obtain in the real-world. Because MAIR can perform accurate inverse rendering in real-world scenes, and can be easily applied to existing view synthesis methods with multi-view images, we can take advantage of IBR and PBR. The view synthesis result of the scene with chrome sphere inserted is in the accompanied video. This application consists of two steps: (1) background rendering with NeRF~\cite{nerf}, and (2) object and mask rendering with our renderer. We render the shadow of an object in all images and we train NeRF with these images. Background including shadow in novel view is rendered with NeRF, and chrome sphere in novel view is rendered with our lighting and renderer. Among the variants of NeRF, we use DirectVoxGO~\cite{dvgo} for fast training.

 
\section{Implementation details}\label{F}
\noindent{\bf Training and architecture details.} Our experiments were conducted with 8 NVIDIA RTX A5000 (24GB). In training, we use Adam optimizer, and the binary mask image $(\mathrm{M}_o, \mathrm{M}_l)$. $\mathrm{M}_o \in \mathbb{R}^{H \times W}$ is mask on pixels of valid materials, and $\mathrm{M}_l \in \mathbb{R}^{H \times W}$ is mask on pixels of valid materials and area lighting. The binary mask image is included in the OpenRooms FF and is used only for training. First, we define masked L1 angular error function ($g_1$), masked MSE function ($g_2$), masked scale invariant MSE function ($g_3$), masked scale invariant $\log$ space MSE function ($g_4$), and regularization function ($g_5$) as follows.

\small
\begin{align}
g_1(A, B, M) = ||(\cos^{-1}(A \odot B)) \otimes M||_1, \\
g_2(A, B, M) = ||{(A - B) \otimes M}||_2^2, \\
g_3(A, B, M) = ||{(A - \tau B) \otimes M}||_2^2, \\
g_4(A, B, M) = ||(\text{log}(A+1) - \text{log}(\tau B+1)) \otimes M) ||_2^2, \\
g_5(A) = -A\log(A),
\end{align}
\normalsize
where $\odot$ is element-wise dot product, $\otimes$ is element-wise multiplication, and $\tau$ is the scale obtained by least square regression between A and B.

In stage 1, the loss function of NormalNet is as follows:
\small
\begin{equation}
\mathcal{L}_{\text{normal}} = \beta_1 g_1(\mathrm{N}, \Tilde{\mathrm{N}}, \mathrm{M}_l)
+ \beta_2 g_2(\mathrm{N}, \Tilde{\mathrm{N}}, \mathrm{M}_l).
\end{equation}
\normalsize
NormalNet has a U-Net\cite{unet} structure with 6 down-up convolution blocks. 

Since the light source is not transparent, we use a regularization $g_5$ so that the visibility $\mu_s$ of InDLNet and the opacity $\alpha$ of ExDLNet can be 0 or 1. the loss function of InDLNet and ExDLNet is as follows:
\small
\begin{align} \label{eqn:eq_lossDL}
\mathcal{L}_{\text{InDL}} = \beta_1 g_4(\mathrm{E}_{DL}, \Tilde{\mathrm{E}}_{DL}, \mathrm{M}_o) +\beta_2 g_5(\mu_s), \\
\mathcal{L}_{\text{ExDL}} = \beta_1 g_4(\mathrm{E}_{DL}, \Tilde{\mathrm{E}}_{DL}, \mathrm{M}_o) +\beta_2 g_5(\alpha),
\end{align} 
\normalsize
where $\mathrm{E}_{DL}$ is the per-pixel direct lighting environment map. InDLNet also has a U-Net structure that encoder is shared, and decoders are separated by $\lambda_s, \xi_s, \mu_s$. The light source intensity $\eta_s$ was decoded using MLP. ExDLNet follows structure of OccNet\cite{occupancy} and uses MLP with conditional batch normalization (CBN) \cite{de2017modulating}. All convolution blocks use batch normalization(BN).

In stage2, the loss function is as follows. 

\small
\begin{equation}
\mathcal{L}_{\text{BRDF}} = \beta_1 g_3(\mathrm{A}, \Tilde{\mathrm{A}}, \mathrm{M}_o) + \beta_2 g_2(\mathrm{R}, \Tilde{\mathrm{R}}, \mathrm{M}_o).
\end{equation}
\normalsize

ContextNet uses U-Net with ResNet18\cite{resnet}, SpecNet uses MLP with 3 layers, MVANet uses layer normalization (LN), and RefineNet uses U-Net with group normalization(GN).

In stage3, the loss function is as follows. 
\small
\begin{multline} 
    \mathcal{L}_{\text{SVL}} = \beta_1 g_4(\mathrm{E}_{SVL}, \Tilde{\mathrm{E}}_{SVL}, \mathrm{M}_o) + \beta_2 g_5(\alpha) \\
    + \beta_3\displaystyle\sum_{k=1}^K ||w_k{(\mathrm{I}^k- \tau_{diff}\Tilde{\mathrm{I}}_{diff} - \tau_{spec}\Tilde{\mathrm{I}}^k_{spec}) \otimes \mathrm{M}_o}||_2^2,
\end{multline} 
\normalsize
% \!\!\!\!

where $\mathrm{E}_{SVL}$ is the per-pixel lighting environment map, $\tau_{diff}$ and $\tau_{spec}$ are the scale obtained by least square regression with target image. $\mathrm{I}^k, \Tilde{\mathrm{I}}_{diff}, \Tilde{\mathrm{I}}^k_{spec}$ are $k$-view image, diffuse image, $k$-view specular image, respectively, and $w_k$ is multi-view weight. In SVLNet, visible surface volume ($\mathrm{T}$) is concatenated with $\Tilde{\mathrm{V}}_\text{DL}$ after 2 downsampling and processed with 3D U-Net. The resolution of the $\Tilde{\mathrm{V}}_\text{DL}$ is $32^3$, and the resolution of the $\mathrm{T}$ and $\Tilde{\mathrm{V}}_\text{SVL}$ is $128^3$. SVLNet uses instance normalization(IN). SVLNet needs a lot of memory when training, so we render environment map with a spatial resolution of 60$\times$80. A summary of training, number of GPUs, hyperparameter and network architecture is provided in Tab.~\ref{tab:arch}. Rendering includes the time to obtain a 60$\times$80$\times$8$\times$16 environment map from VSG and the time to re-render the input image.

\begin{table*}[htb!]
\footnotesize
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|l|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Stage &Network &input & Arch & norm &batch &epoch &$\beta_1$ &$\beta_2$ &$\beta_3$ &lr &training / GPUs &inference &output(channels)\\
\hline
\multirow{3}{*}{1}&NormalNet &$\mathrm{I},\Tilde{\mathrm{D}}, \nabla\Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}$,&U-Net  & BN & 96 & 60 & 1.0 & 1.0 & - &2e-3 &7h / 4 & 3ms &$\Tilde{\mathrm{N}}(3)$  \\

&InDLNet &$\mathrm{I}, \Tilde{\mathrm{N}}, \Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}$& U-Net, MLP &BN &384 &80 &1.0 &1e-3 & - &2e-4 &10h / 4 &5ms &$\boldsymbol{\xi}_s, \lambda_s, \mu_s, {\boldsymbol{\eta}_s}(8)$ \\

&ExDLNet &$\mathrm{I}, \Tilde{\mathrm{N}}, \Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}$& U-Net, MLP &BN &96 &80 &1.0 &1e-4 & - & 1e-4 &1d / 8 & 6ms &$\Tilde{\text{V}}_\text{DL}(8)$\\

\hline
\multirow{4}{*}{2} &ContextNet &$\mathrm{I}, \Tilde{\mathrm{N}}, \Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}$ & Res U-Net  & BN &\multirow{4}{*}{64} &\multirow{4}{*}{40} &\multirow{4}{*}{3.0} &\multirow{4}{*}{1.0} &\multirow{4}{*}{-} &\multirow{4}{*}{1e-4} &\multirow{4}{*}{1d 20h / 8} &\multirow{4}{*}{54ms} &$\boldsymbol{f}_\text{context}(32)$\\
&SpecNet &${\boldsymbol{\xi}_s}, {\lambda_s}, {\mu_s}, {\boldsymbol{\eta}_s}, \boldsymbol{v}, \Tilde{\mathrm{N}}$ & MLP & -  &&&&&&&&&$\boldsymbol{f}_\text{spec}(8)$\\
&MVANet &$\mathrm{I}, \boldsymbol{f}_\text{context}, \boldsymbol{f}_\text{spec}, \boldsymbol{w}$ & - &LN &&&&&&&&&$\boldsymbol{f}_\text{BRDF}(16)$\\
&RefineNet &$\mathrm{I}, \Tilde{\mathrm{N}}, \Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}, \boldsymbol{f}_\text{context}, \boldsymbol{f}_\text{BRDF}$ & U-Net &GN &&&&&&&&&$\Tilde{\mathrm{A}}(3), \Tilde{\mathrm{R}}(1)$\\
\hline
3&SVLNet &$\mathrm{I}, \Tilde{\mathrm{N}}, \Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}, \Tilde{\mathrm{A}}, \Tilde{\mathrm{R}}, \Tilde{\text{V}}_\text{DL}$ &3D U-net &IN &8 &10 &10.0 &1e-2 &1.0 &1e-4 &3d 8h / 8 &11ms &$\Tilde{\text{V}}_\text{SVL}(8)$ \\
\hline
- &Rendering & $\Tilde{\mathrm{N}}, \Tilde{\mathrm{D}}, \Tilde{\mathrm{C}}, \Tilde{\mathrm{A}}, \Tilde{\mathrm{R}}, \Tilde{\text{V}}_\text{SVL}$, &-&-&-&-&-&-&-&- &- &834ms &$\Tilde{\mathrm{I}}(3)$\\
\hline

\end{tabular}
}
\caption{The details of the network architecture, and training. Please refer to the main paper for the architecture of MVANet.}
\label{tab:arch}
\end{table*}

\noindent{\bf Test details.} 
Li \etal\cite{cis2020} and we both used an environment map with an angular resolution of 16 $\times$ 8 during training, but we created an environment map with 32 $\times$ 16 during testing because our VSG was not restricted by resolution. In training, all views are rendered for re-rendering loss, but in testing, only the target view was rendered.


\section{Additional Experimental Results}\label{G}
\subsection{Indoor Synthetic Scenes}
We provide additional inverse rendering results for OpenRooms FF test scene in Fig.~\ref{fig:supp_ir_indoor}. Our method leverage multi-view and incident direct lighting to provide more accurate material estimation results for highly specular regions. (\eg table in sample 2, chair in sample 3) Furthermore, the proposed method yields better normal estimation results especially for more complicated structures by utilizing MVS depth. As a result, our lighting is more realistic and we can re-render input image more accurately.

\subsection{Real-World Scenes}
The performance gaps between MAIR and the single-view-based methods are more distinct in the unseen real-world scene. Fig.~\ref{fig:supp_ir_real} shows that our method robustly produces reasonable normal maps even for complex scene structures, and this naturally affects the subsequent material, lighting estimation. MAIR shows better material estimation results for shadowed regions(\eg table, wall in sample 2, floor in sample 3) or specular regions(\eg drawer in sample 4). Although there are no ground truths for materials, from our experience, we know that the stones, bushes in sample 1, and the dolls in sample 5 should show high roughness, which are consistent with our high roughness estimation results.

\subsection{Object Insertion}
Inverse rendering performance of three competing methods, lighthouse~\cite{lighthouse}, Li \etal\cite{cis2020}, and MAIR, are tested by comparing the quality of object insertion. We implemented a simple renderer for object insertion by referring to Wang \etal\cite{vsg} and used it for rendering results of MAIR and lighthouse~\cite{lighthouse}. As the public implementation of Li \etal\cite{cis2020} includes a renderer of their own, results of Li \etal\cite{cis2020} were rendered using this renderer, except for the results of the chrome sphere insertion; the renderer from Li \etal\cite{cis2020} does not support the chrome sphere rendering directly, so we used our renderer for this case. It should be also noted that all results of lighthouse~\cite{lighthouse} were produced by using our scene geometries because scene geometry results from lighthouse~\cite{lighthouse} were not accurate enough to render.

We conducted a user study to evaluate the quality of object insertion from the three methods. Given a background image and an object of a particular material, users selected the most natural image among the three different results in a random order. 100 users evaluated 25 different scenes. Fig.~\ref{fig:supp_oi_chrome1}, \ref{fig:supp_oi_chrome2}, \ref{fig:supp_oi_indoor}, \ref{fig:supp_oi_real1}, and \ref{fig:supp_oi_real2} show all the scenes used in our user study. Our 3D lighting not only clearly expresses HDR lighting, but also fully reflects real-world scene geometry and material. This allowed the object to be realistically inserted into the scene, acquiring the highest score among the competing methods. 

We also provide additional object insertion results. In the accompanied video, the object can be located not only on the plane but also on any geometry, and the shadow of the object realistically appears to match the scene illumination.

% Therefore, for fairness, only the results of us and Li \etal\cite{cis2020} were compared quantitatively.
%\noindent{\bf User study.}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.97\linewidth]{figure/supp_ir_indoor_1.pdf}
  \includegraphics[width=0.97\linewidth]{figure/supp_ir_indoor_2.pdf}
  \includegraphics[width=0.97\linewidth]{figure/supp_ir_indoor_3.pdf}
  \includegraphics[width=0.97\linewidth]{figure/supp_ir_indoor_4.pdf}
   \caption{Additional inverse rendering results on OpenRooms FF. Small insets are the estimations without bilateral solver (BS).}
   \label{fig:supp_ir_indoor}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.99\linewidth]{figure/supp_ir_real_1.pdf}
  \includegraphics[width=0.99\linewidth]{figure/supp_ir_real_2.pdf}
  \includegraphics[width=0.99\linewidth]{figure/supp_ir_real_3.pdf}
   \caption{Additional inverse rendering results on IBRNet dataset\cite{ibrnet}. Small insets are the estimations without BS.}
   \label{fig:supp_ir_real}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.99\linewidth]{figure/supp_oi_chrome_1.pdf}
  \includegraphics[width=0.99\linewidth]{figure/supp_oi_chrome_2.pdf}
  \includegraphics[width=0.99\linewidth]{figure/supp_oi_chrome_3.pdf}
   \caption{Additional chrome sphere insertion results on IBRNet dataset\cite{ibrnet}. The number under the image is the result of user study.}
   \label{fig:supp_oi_chrome1}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/supp_oi_chrome_4.pdf}
   \caption{Additional chrome sphere insertion results on IBRNet dataset\cite{ibrnet}. The number under the image is the result of user study.} 
   \label{fig:supp_oi_chrome2}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/supp_oi_indoor_1.pdf}
  \includegraphics[width=\linewidth]{figure/supp_oi_indoor_2.pdf}
   \caption{Additional white sphere insertion results on OpenRooms FF. The number under the image is the result of user study.} 
   \label{fig:supp_oi_indoor}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/supp_oi_real_1.pdf}
  \includegraphics[width=\linewidth]{figure/supp_oi_real_2.pdf}
  \includegraphics[width=\linewidth]{figure/supp_oi_real_3.pdf}
   \caption{Additional virtual object~\cite{stanford} insertion results on IBRNet dataset\cite{ibrnet}. The number under the image is the result of user study.}
   \label{fig:supp_oi_real1}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/supp_oi_real_4.pdf}
  \includegraphics[width=\linewidth]{figure/supp_oi_real_5.pdf}
   \caption{Additional virtual object~\cite{stanford} insertion results on IBRNet dataset\cite{ibrnet}. The number under the image is the result of user study.}
   \label{fig:supp_oi_real2}
\end{figure*}