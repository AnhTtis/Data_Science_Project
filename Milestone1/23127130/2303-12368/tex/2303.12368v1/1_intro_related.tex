
%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Inverse rendering is a technology used to estimate material, lighting, and geometry from RGB color images. Decomposing a scene through inverse rendering enables various applications such as object insertion, relighting, and material editing in VR and AR. However, since inverse rendering is an ill-posed problem, previous studies have focused only on a part of the inverse rendering, such as intrinsic image decomposition~\cite{id1,id2,id3,id4}, shape from shading~\cite{sfs1,sfs2,sfs3}, and material estimation~\cite{sime1, sime2, sime3, sime4, sime5}. 

Recent advances in GPU-accelerated physically-based rendering algorithms have made constructing large-scale photorealistic indoor high dynamic range (HDR) image dataset that include geometries, materials, and spatially-varying lighting~\cite{openrooms2021}. The availability of such dataset and the recent success of deep learning technology have enabled seminal works on single-view-based inverse rendering~\cite{cis2020, irisformer2022, vsg, phyir, Li22, zhu2022montecarlo}. These methods have fundamental limitations in that they are prone to bias in training dataset despite having shown promising results. Specifically, single-view-based inverse rendering must refer to specular reflectance from the contextual information of the image, making it less reliable for predicting complex SVBRDF(spatially-varying bidirectional reflectance distribution function) in the real-world. Fig.~\ref{fig:main} shows such an example, where decomposition has severely failed owing to the complexity of the real-world scene. In addition, depth-scale ambiguity makes it challenging to employ these methods to 3D applications such as object insertion. 

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{figure/main.pdf}
  \caption{The result of inverse rendering and floating chrome sphere insertion in the unseen real-world scene. Since the single-view-based method\cite{cis2020} relies only on contextual information, it has difficulty estimating the complex material, geometry of the real-world. Notice the spatially-consistent albedo of apples, eloborated normal, and realistic lighting reflected on the inserted object. In previous work\cite{cis2020}, they have limitations on floating object insertion because they use per-pixel lighting.}
  \label{fig:main}
  \vspace{-2mm}
\end{figure}

In this paper, we introduce \textbf{MAIR}, the scene-level \textbf{M}ulti-view \textbf{A}ttention \textbf{I}nverse \textbf{R}endering pipeline. MAIR exploits multiple RGB observations of the same scene point from multiple cameras and, more importantly, it utilizes multi-view stereo (MVS) depth as well as scene context to estimate SVBRDF. As a result, the method becomes less dependent on the implicit scene context, and shows better performance on unseen real-world images. However, the processing of multi-view images inherently requires a high computational cost to handle multiple observations with occlusions and brightness mismatches. To remedy this, we design a three-stage training pipeline for MAIR that can significantly increase training efficiency and reduce memory consumption. Spatially-varying lighting consists of direct lighting and indirect lighting. Indirect lighting affected by the surrounding environment makes inverse rendering difficult. Therefore, in Stage 1, we first estimate the direct lighting and geometry, which reflect the amount of light entering each point and in which direction the specular reflection appears. We estimate the material in Stage 2 using the estimates of the direct lighting, geometry, and multi-view color images. In Stage 3, we collect all the material, geometry, and direct lighting information and finally estimate 3D spatially-varying lighting, including indirect lighting. The MAIR pipeline is shown in Fig.~\ref{fig:whole}. We created the OpenRooms Forward Facing(OpenRooms FF) dataset as an extension of OpenRooms~\cite{openrooms2021} to train the proposed network.

Our contribution can be summarized as follows:
\begin{itemize}
\item As summarized in Tab.~\ref{tab:compare_prior}, we believe this is the first demonstration of using multi-view images to decompose the scene into geometry, complex material, and 3D spatially-varying lighting without test-time optimization. Also, we release OpenRooms FF dataset.
% Our method can robustly perform inverse rendering compared with the single-view context-based method and we release OpenRooms FF dataset.

\item We propose a framework that can efficiently train multi-view inverse rendering networks. Our framework increases the training efficiency by decomposing lighting and separating the scene components by stage.
\item Our method achieves better inverse rendering performance than the existing single-view-based method, and realistic object insertion in real-world is possible by reproducing 3D lighting of the real-world.
\vspace{-2mm}
\end{itemize}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{figure/entire.pdf}
   \caption{MAIR's entire pipeline. Our method has reduced the difficulty of inverse rendering by splitting the scene components as small as possible, and progressively estimating the scene components.}
   \label{fig:whole}
\end{figure*}


\section{Related Works}
\noindent\textbf{Inverse rendering.}
Research on inverse rendering has received significant attention in recent years owing to the development of deep learning technology. Yu \etal\cite{outdoorinv} performed outdoor inverse rendering with multi-view self-supervision, but their lighting is simple distant lighting. A pioneering work by Li \etal \cite{cis2020} conducted inverse rendering on a single image, and IRISformer\cite{irisformer2022} further improved the performance by replacing convolutional neural networks (CNNs) with a Transformer\cite{vit}. PhyIR \cite{phyir} addressed this problem using panoramic image. However, the lighting representation in~\cite{cis2020,irisformer2022, phyir} is a 2D per-pixel environment map, which is insufficient for modeling 3D lighting. Li \etal \cite{Li22} adopted a parametric 3D lighting representation; however, it fixed with two types of indoor light sources. The recently introduced Zhu \etal\cite{zhu2022montecarlo} demonstrated realistic 3D volumetric lighting based on ray tracing. But, since these prior works\cite{outdoorinv, cis2020,irisformer2022,Li22, phyir, zhu2022montecarlo} are all single-view-based, they inherently rely on the scene context, making these methods less reliable for unseen images. In contrast, the proposed method can estimate BRDF and geometry more accurately by utilizing the multi-view correspondences as additional cues for inverse rendering. 

\begin{table}[t!]
\footnotesize
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|c|c|c|c|}
\hline
Method & \multicolumn{1}{c|}{Input} & \multicolumn{1}{c|}{Material} & \multicolumn{1}{c|}{Lighting} & \multicolumn{1}{c|}{Insertion} \\
\hline
VSG\cite{vsg} &single &diffuse &volume &any\\
Li~\etal\cite{cis2020} &single &microfacet &per-pixel &surface\\
IRISformer\cite{irisformer2022} &single  &microfacet &per-pixel &surface\\
PBE\cite{Li22} &single &microfacet &parametric &any\\
SOLD \cite{SOLD-Net} & single &\xmark &per-pixel &surface\\
Zhu~\etal\cite{zhu2022montecarlo} &single &microfacet(metalic) &volume &any\\
lighthouse~\cite{lighthouse} &stereo &\xmark &volume &any\\
FreeView~\cite{philip2021} &multi &glossy &irradiance & \xmark\\
Zhang~\etal \cite{invrender2022} &multi &microfacet &object & \xmark\\
PhotoScene~\cite{photoscene} &any &microfacet &parameteric &any\\
Intrinsic3D~\cite{maier2017intrinsic3d} &multi RGBD &diffuse &volume &any\\
Zhang \etal\cite{zhang2016emptying} &multi RGBD &diffuse &parameteric &any\\
\hline
MAIR (Ours) &multi &microfacet &volume &any\\
\hline
\end{tabular}%
}
\caption{Compared to previous works, our work is the first demonstration to perform multi-view scene-level inverse rendering.}
\vspace{-3mm}
\label{tab:compare_prior}
\end{table}

\noindent\textbf{Lighting estimation.}
Lighting estimation has been studied not only as a sub-task of the inverse rendering but also an important research topic\cite{le1,le2,le3,Song19}. In Lighthouse \cite{lighthouse}, 3D spatially-varying lighting was obtained by estimating the RGBA lighting volume. Wang \etal\cite{vsg} estimated a more sophisticated 3D lighting volume by replacing RGB with a spherical Gaussian in lighting volume. However, because the methods in~\cite{lighthouse,vsg} assume Lambertian reflectance, they cannot represent complex indirect lighting and have limitations in expressing HDR lighting due to weak-supervision with LDR dataset\cite{interiornet}. On the other hand, the proposed method can handle complex SVBRDF and HDR lighting well because we trained our model on the large indoor HDR dataset\cite{openrooms2021}. Recently, a study on spatially-varying lighting estimation in the outdoor scenes\cite{wang2022neural,SOLD-Net} was introduced. Because they focus on outdoor street scenes, they cannot clearly reproduce the indirect lighting by the scene material. 

\noindent\textbf{Multi-view inverse rendering and neural rendering.}
Earlier works such as Intrinsic3D~\cite{maier2017intrinsic3d} and Zhang~\etal \cite{zhang2016emptying} perform multi-view inverse rendering without deep learning, but require additional equipment to obtain RGB-D images. 3D geometry-based methods~\cite{cis-texture, invpath, kim2016multi} require additional computation to generate mesh. PhotoScene~\cite{photoscene}, in particular, requires external CAD geometry, which should be manually aligned for each object. Moreover, all of these methods require test-time optimization. In contrast, we only need per-view depth maps and we does not require 3D geometry or test-time optimization, making it more computationally efficient and much easier to apply to more general scenes. Philip \etal\cite{philip2019, philip2021} demonstrated a successful relighting with multi-view images; however, these methods cannot be used for applications such as object insertion because they use trained neural renderers. The advent of NeRF\cite{nerf, mipnerf, mip360} has led to a breakthrough in the field of neural rendering research. Several recent methods\cite{nerd, nerv, physg, zhang2021nerfactor, refnerf, invrender2022} have successfully performed inverse rendering using multi-view images; however, they are difficult to apply to scene-level inverse rendering because they are only trained and tested on object-centric images.

% Various research results\cite{nerfeditable, nerfcomposite, nerfobjcen, giraffe}, such as scene editing or object insertion have been also introduced. However, they did not consider the materials and lighting of the scene. In contrast, our work focuses on scene-level inverse rendering, which enables physically-meaningful object insertion.