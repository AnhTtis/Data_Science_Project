\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{float}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{hyperref}
\hypersetup{colorlinks,allcolors=black}
\usepackage{caption}  % for small gap between table and caption 
\captionsetup[table]{skip=3pt}
\captionsetup[figure]{skip=3pt}
\usepackage[capitalize]{cleveref}
\usepackage{pifont}%for xmark
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\setlength {\marginparwidth }{2cm}
\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\ MAIR: Multi-view Attention Inverse Rendering with 3D Spatially-Varying Lighting Estimation}

\author{
{JunYong Choi$^{1,2}$}\quad 
{SeokYeong Lee$^{1,2}$}\quad
{Haesol Park$^{1}$}\quad 
{Seung-Won Jung$^{2}$}\quad
{Ig-Jae Kim$^{1,3,4}$}\quad
{Junghyun Cho$^{1,3,4}$} 
\\[2mm]
{$^{1}$Korea Institute of Science and Technology(KIST)}\quad 
{$^{2}$Korea University} \\
{$^{3}$AI-Robotics, KIST School, University of Science and Technology} \\
{$^{4}$Yonsei-KIST Convergence Research Institute, Yonsei University} \\
\vspace{-3mm}
{\tt\small \{happily,shapin94,haesol,drjay,jhcho\}@kist.re.kr}\quad
{\tt\small swjung83@korea.ac.kr}
\renewcommand\footnotemark{}
\renewcommand\footnoterule{}
\thanks{This work was partly supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government(MSIT)(No.2020-0-00457, 50\%) and KIST Institutional Program(Project No.2E32301, 50\%).}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
We propose a scene-level inverse rendering framework that uses multi-view images to decompose the scene into geometry, a SVBRDF, and 3D spatially-varying lighting. Because multi-view images provide a variety of information about the scene, multi-view images in object-level inverse rendering have been taken for granted. However, owing to the absence of multi-view HDR synthetic dataset, scene-level inverse rendering has mainly been studied using single-view image. We were able to successfully perform scene-level inverse rendering using multi-view images by expanding OpenRooms dataset and designing efficient pipelines to handle multi-view images, and splitting spatially-varying lighting. Our experiments show that the proposed method not only achieves better performance than single-view-based methods, but also achieves robust performance on unseen real-world scene. Also, our sophisticated 3D spatially-varying lighting volume allows for photorealistic object insertion in any 3D location.


\end{abstract}

%------------------------------------------------------------------------

\input{1_intro_related}
\input{2_method}
\input{3_experiments}

\textbf{{\huge Appendix}}

\appendix

\section{Appendix Outline}
These appendices provide details about the OpenRooms FF dataset (Appendix~\ref{B}), details about direct lighting (Appendix~\ref{C}) and analysis of lighting estimation results (Appendix~\ref{D}), view synthesis applications (Appendix~\ref{E}), additional implementation details (Appendix~\ref{F}), and additional experimental results (Appendix~\ref{G}).
\input{4_supp_content}


%%%%%%%%% REFERENCES

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


\end{document}
