%------------------------------------------------------------------------

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/ir_indoor.pdf}
   \caption{Material, geometry, lighting estimation on OpenRooms FF test data. The small insets in the first row are the material estimation processed by Bilateral Solvers (BS). Even for scenes where inverse rendering is difficult due to strong specular radiance, our method can obtain material, geometry, and lighting more accurately. Please see the lighting and material of the desk.}
   \label{fig:ir_result_synthetic}
\end{figure*}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/ir_real.pdf}
   \caption{Inverse rendering results for unseen real-world data. Li \etal\cite{cis2020} failed the geometry estimation and predicted the material almost exactly as the input image, while we can remove shadow and disentangle the material and lighting from the image.}
   \label{fig:ir_result_real}
\end{figure*}


\section{Implementation Details}
\label{sec:implementation}
\noindent{\bf Dataset.} 
OpenRooms\cite{openrooms2021} provides many HDR images and various ground truths for indoor scenes; however, the distribution of camera poses is too random to be used in multi-view applications. Therefore, we created OpenRooms FF. Among OpenRooms\cite{openrooms2021} images, 23,618 images were selected to be appropriate for multi-view rendering based on the camera position and minimum depth, and eight neighboring frames were rendered by translating the camera for each image. For a detailed description of OpenRooms FF, please see supplementary material.


\noindent{\bf Training and loss.} 
We used nine images ($K$=9) in our experiments. We trained each stage separately since the ground truth required for each stage is available in OpenRooms FF. We trained lighting with only 2D per-pixel ground truth lighting, but our VSG can represent lighting well in any 3D space (See (b) of Fig.~\ref{fig:oi_real}.). A brief description of the loss for each stage is provided in Tab.~\ref{tab:loss}. For a detailed description of training, loss function, and network architecture, please see supplementary material.

\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{|c|l|l|}
\hline
\multicolumn{1}{|c|}{Stage} & \multicolumn{1}{c|}{Network} & \multicolumn{1}{c|}{Loss}\\
\hline
\multirow{3}{*}{1} & NormalNet & MSE($\mathrm{N}$) + L1 angular($\mathrm{N})$\\
& InDLNet & si-MSE(Lighting)  \\ 
& ExDLNet & si-MSE(Lighting) \\ 
\hline
2 & All & si-MSE($\mathrm{A}$) + MSE($\mathrm{R}$) \\
\hline
3 & All & si-MSE(Lighting) + MSE(Re-rendering)\\
\hline
\end{tabular}
\caption{Training loss for MAIR. si- means scale invariant.}
\vspace{-3mm}
\label{tab:loss}
\end{table}


%-------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}
Since no previous works, to the best of our best knowledge, addressed multi-view scene-level inverse rendering, we compare our method qualitatively and quantitatively with a single-view-based method\cite{cis2020}. We also conduct a qualitative performance evaluation on the real-world dataset~\cite{ibrnet}. Last, we demonstrate that our realistic 3D spatially-varying lighting through virtual object insertion.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/oi_indoor.pdf}
   \caption{Object insertion comparison for OpenRooms FF test scene. Our method can insert the sphere to match the scene shadow, reproduce color bleeding, and put the sphere in the air.}
   \label{fig:oi_result_synthetic}
    % \vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figure/oi_real.pdf}
   \caption{Object~\cite{stanford} and floating chrome sphere insertion comparison for unseen real-world data from IBRNet dataset\cite{ibrnet}. Our method estimate lighting robustly for unseen images, and we can insert objects more realistically than previous methods.}
   \label{fig:oi_real}
\end{figure*}

\subsection{Evaluation on Synthetic Data}
For quantitative comparison, we trained Li \etal\cite{cis2020} on our OpenRooms FF. Tab.~\ref{tab:compare} shows the performance comparison for all test images in OpenRooms FF. Material and geometry errors were measured by MSE and lighting error was measured by log-space MSE with ground truth per-pixel environment map. Our method outperformed the previous method in geometry and material estimation. Moreover, we achieved better lighting performance even though the previous method directly estimates per-pixel lighting but we obtain per-pixel lighting from the 3d lighting volume. For the re-rendering, the previous method showed better performance on average because Li \etal\cite{cis2020}'s cascade structure self-supervises re-rendering through the rendering layer. However, in scenes where specular radiance appears strongly, we found that our method robustly and accurately estimates the details of lighting and materials, thereby yielding more realistic result, as shown in Fig.~\ref{fig:ir_result_synthetic}. 

\begin{table}[ht]
\newcommand{\B}{\fontseries{b}\selectfont} % local definition
\footnotesize
\centering
\begin{tabular}{|l|S[detect-weight]|S[detect-weight]S[explicit-sign=+]|} 
\hline
MSE ($\times 10^{-2}$)& \multicolumn{1}{c|}{Li \etal \cite{cis2020}} & \multicolumn{2}{c|}{MAIR (Ours)}\\
\hline
Albedo $\downarrow$ & 0.569 & \B 0.368 &{(}-0.201{)}\\
Normal $\downarrow$ & 2.71 & \B 1.36 &{(}-1.35{)}\\
Roughness $\downarrow$ & 3.66 & \B 2.70 &{(}-0.96{)}\\
Lighting $\downarrow$ & 13.74 & \B 12.04 &{(}-1.70{)}\\
Re-rendering $\downarrow$ & \B 0.554 & 0.633 &{(}+0.079{)}\\
% Re-rendering(with $\Tilde{C}$) $\downarrow$ & \B 0.552 & 0.554 &{(}+0.002{)}\\
\hline
\end{tabular}
\caption{Quantitative comparison of material, geometry, and lighting in OpenRooms FF. Albedo and roughness of Li \etal\cite{cis2020} are processed with a Bilateral Solver (BS).}
% Our 3D lighting volume represents distorted lighting in the depth estimation failure region, which is vulnerable to re-rendering.
\label{tab:compare}
\vspace{-3mm}
\end{table}

\subsection{Evaluation on Real-world Data}
We evaluate the performance of inverse rendering for the real-world scenes in the IBRNet dataset\cite{ibrnet} in which the scene context is difficult to grasp. As shown in Fig.~\ref{fig:ir_result_real}, the single-view-based method fails in geometry estimation due to the lack of the scene context, leading to the inaccurate disentanglement of the material and illumination. Our method, on the other hand, obtains a reasonable geometry, removes shadows, and predicts spatially-consistent albedo. More results can be found in the supplementary material.

\subsection{Evaluation on Object Insertion}
We further demonstrate the effectiveness of our robust inverse rendering on the object insertion task. In Fig.~\ref{fig:oi_result_synthetic},~\ref{fig:oi_real} we provide a comparison with the single-view method~\cite{cis2020} and the stereo method\cite{lighthouse}. It should be noted that, we use the normal from MAIR for Lighthouse\cite{lighthouse} results because Lighthouse\cite{lighthouse} does not provide scene geometry. In Li \etal\cite{cis2020}'s object insertion application, the user must specify the plane on which the object is located. Therefore, object cannot be placed on another object or floated in the air, and shadows are only cast on the plane. In our method, it is possible to insert objects freely into the 3D space without restrictions on the shadow. We also conducted a user study on the object insertion, in which respondents chose the most realistic images rendered by competing methods for 25 scenes. As shown in the Tab.~\ref{tab:oi_userstudy}, our method shows the best performance in both synthetic and real-world scenes. 


\begin{table}[ht]
\footnotesize
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|c|c|c|}
\hline
Scene & \multicolumn{1}{|c|}{Lighthouse~\cite{lighthouse}} & \multicolumn{1}{|c|}{Li \etal\cite{cis2020}} & \multicolumn{1}{|c|}{MAIR}\\
\hline
OpenRooms FF & - &0.220  &\textbf{0.780}\\
IBRNet~\cite{ibrnet}& 0.120  &0.180  & \textbf{0.700}\\
IBRNet~\cite{ibrnet}(chrome)  & 0.186  & 0.025 & \textbf{0.789} \\
\hline
\end{tabular}
}
\caption{User study results on virtual object insertion. The design and details are described in supplementary material.}
\label{tab:oi_userstudy}
\vspace{-2mm}
\end{table}

\noindent{\bf Object insertion in indoor test dataset.} In (a) of Fig.~\ref{fig:oi_result_synthetic}, the sphere on the floor is inserted to match the geometry and lighting of the scene, and the sphere in the air is inserted to express the appropriate shadows for the scene geometry. In (b) of Fig.~\ref{fig:oi_result_synthetic}, the sphere inserted on the floor represents the color bleeding, showing that our method is capable of indirect lighting estimation.

% because Li \etal\cite{cis2020} uses only pixel lighting on the floor
\noindent{\bf Object insertion in real-world unseen dataset.} To show the generalization ability of the proposed model trained only on the synthetic indoor images, we tested object insertion on the real-world unseen dataset\cite{ibrnet}. In (a) of Fig.~\ref{fig:oi_real}, Li \etal\cite{cis2020} failed to separate lighting from the material, and as a result, the appearance of the object was colored with the material on the floor. Our method successfully separate material and lighting from the image, and realistic lighting is represented on the inserted object. Lighthouse\cite{lighthouse} was more vulnerable to unseen real-world data because it did not consider scene geometry and materials, even though they use stereo images. Their objects have a similar appearance regardless of the scene. In (b) Fig.~\ref{fig:oi_real}, we inserted a chrome sphere to validate the indirect lighting estimation accuracy of the proposed method. Because lighthouse\cite{lighthouse} was trained with an indoor LDR dataset, it always tended to create an environment map with an indoor scene on the sphere, and failed to reproduce HDR lighting. Li \etal\cite{cis2020}'s pixel lighting was not sufficient to express indirect lighting of the scene reflected in chrome sphere. In contrast, our chrome sphere shows realistic lighting that reflects the surrounding environment. Please refer to the supplementary material for more results.

\subsection{Ablation Study}
\noindent{\bf Design of stage 2.} 
Tab.~\ref{tab:abl_DL_stage2} shows experimental results for network design choices in stage 2. It shows that MVANet requires a local context of ContextNet for material estimation. In addition, RefineNet is necessary to compensate for the weakness of the pixel-wise operation in MVANet. We also validate the effect of $\boldsymbol{f}_{\text{spec}}$.``w/o $\boldsymbol{f}_{\text{spec}}$'' is the result of the model trained without $\boldsymbol{f}_{\text{spec}}$, and ``w/o reparameterize'' is the result of the model trained without physically-motivated encoding as follows:

\vspace{-2mm} 
\begin{equation}\label{eqn:fspec_abl}
\boldsymbol{f}_{\text{spec}}^{k} = \displaystyle\sum_{s=1}^{S_D} \text{SpecNet}(\boldsymbol{v}_k, \Tilde{\boldsymbol{n}}, \boldsymbol{\xi}_s, \boldsymbol{\eta}_s, \lambda_s).
\end{equation}
Improvement in roughness estimation implies that our physically-motivated encoding considering the microfacet BRDF~\cite{microfacet} model helps in estimating specular radiance. 

\vspace{-2mm}
\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{|l|c|c|}
\hline
MSE ($\times 10^{-2}$)& \multicolumn{1}{|c|}{Albedo$\downarrow$} & \multicolumn{1}{|c|}{Roughness$\downarrow$} \\
\hline
w/o ContextNet &0.498 &4.064  \\
w/o RefineNet &0.661  &3.971  \\
w/o $\boldsymbol{f}_{\text{spec}}$ & 0.436  & 3.056 \\
w/o reparameterize & 0.428 & 3.085 \\
Ours & \textbf{0.423} & \textbf{2.739} \\
\hline
\end{tabular}
\caption{ablation studies in stage 2.}
\label{tab:abl_DL_stage2}
\vspace{-4mm}
\end{table}

\noindent{\bf Attention with multi-view weight.} We also validate the effect of multi-view weight($\boldsymbol{w}$) in MVANet. Fig.~\ref{fig:graph_numview} shows the material estimation accuracy according to the number of views. When training without $\boldsymbol{w}$, increasing the number of views seems ineffective, possibly due to the negative effect of noises in multi-view features introduced by occlusion. On the other hand, MVANet can selectively focus on the information required for material estimation with $\boldsymbol{w}$, making the accuracy increase with the number of views.

\vspace{-2mm}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/graph_numview.pdf}
  \caption{material evaluation depending on the number of views.}
  \label{fig:graph_numview}
\end{figure}
\vspace{-2mm}

\noindent{\bf The necessity of exitant direct lighting.}
Fig.~\ref{fig:graph_vdl} shows that $\Tilde{\text{V}}_\text{DL}$ is helpful for indirect lighting estimation. The result of training with a global lighting feature as in \cite{vsg}, instead of $\Tilde{\text{V}}_\text{DL}$, is also compared (with GLF). The $\Tilde{\text{V}}_\text{DL}$ helps network to converge quickly, which indicates that $\Tilde{\text{V}}_\text{DL}$ helps the network infers indirect lighting.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/graph_vdl.pdf}
  \caption{lighting evaluation depending on the input source.}
  \label{fig:graph_vdl}
\end{figure}
\vspace{-2mm}

\section{Discussion}
\noindent{\bf Comparisons with 3D geometry-based methods.}
Fig.~\ref{fig:comparisons} shows the comparison results with PhotoScene~\cite{photoscene} and MVIR~\cite{kim2016multi} in 2 views. The inference time is 2s for ours, 9m 52s for MVIR~\cite{kim2016multi} and 10m 40s for PhotoScene~\cite{photoscene} on RTX 2080 Ti. MVIR~\cite{kim2016multi} fails to generate geometry, showing severe artifacts. The PhotoScene~\cite{photoscene} shows a complete scene reconstruction thanks to the CAD geometry and material graph, but the synthesized images largely differ from the original scene. 

\noindent{\bf Inter-view consistency.}
Unlike 3D geometry-based methods~\cite{photoscene, kim2016multi}, which leverage global 3D geometry to achieve consistency, Our MAIR operates in pixel-space and therefore does not explicitly guarantee inter-view consistency. Still, we did not observe significant inconsistencies during our experiment, presumably due to the role of MVANet, which can be seen in Fig.~\ref{fig:comparisons}. For object insertion, we used lighting of the center-view.

\vspace{-2mm}
\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/3dcompare_2view.pdf}
  \caption{Comparisons with PhotoScene~\cite{photoscene} and MVIR~\cite{kim2016multi}.}
  \label{fig:comparisons}
\end{figure}
\vspace{-2mm}

\noindent{\bf Limitation.}
One possible limitation comes from the cascaded nature of the pipeline. If depth estimation fails due to, for example, presence of dynamic objects or large textureless region, our MAIR will not work properly (See Fig.~\ref{fig:failure_case}.). Another possible limitation comes from the VSG representation. Although VSG can express 3D lighting effectively, it cannot be applied to applications such as light source editing because it is non-parametric. 
% BRDF, geometry, and lighting prediction can help improve each other~\cite{cis2020}. According to this, our stage-wise pipeline can be sub-optimal because it does not estimate jointly.


\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figure/failure_case.pdf}
  \caption{Failure case when depth prediction fails.}
  \label{fig:failure_case}
\end{figure}

\noindent{\bf Conclusion.}
We presented the first practical multi-view scene-level inverse rendering method by creating an multi-view HDR synthetic dataset. Compared to the single-view based methods, we found that our method is more robust for unseen real-world scenes, providing high-quality virtual object insertion results. We believe that our work can elevate image-based rendering and physically-based rendering together, so realizing a higher level of inverse rendering and scene reconstruction.