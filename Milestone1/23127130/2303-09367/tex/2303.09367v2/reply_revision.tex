\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=2.5cm,right=2.5cm,top=1.5cm,bottom=2.5cm,footnotesep=0.5cm]{geometry}

\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage{parskip}
\usepackage{xcolor}
\usepackage{array}
\usepackage{subcaption}

\newenvironment{bluewords}{\color{blue}}{}


\begin{document}


\begin{center}
\Large \textbf{Goal-conditioned Offline Reinforcement Learning \\ through State Space Partitioning} \\
\vspace{10pt}
\large \textbf{(Paper ID: MACH-D-23-00139)} \\
\vspace{20pt}
\Large \textbf{Response Letter 2}
\vspace{20pt}

\large Mianchu Wang$^1$, Yue Jin$^1$, Giovanni Montana$^{1, 2}$ \\
\vspace{10pt}
\large $^1$University of Warwick \\
\large $^2$Alan Turing Institute \\
\end{center}

\hspace{10pt}

Dear editor and reviewers,

\vspace{10pt}

We sincerely appreciate the time and effort you devoted to reviewing our paper. Your constructive feedback has been invaluable in helping us refine our manuscript. Here, we address each of your comments individually.

\vspace{10pt}

\textcolor{blue}{\textbf{Reviewer \#1}}

We appreciate your insightful suggestions and have revised the manuscript accordingly. The additional experiments, previously in the Appendix, are now seamlessly integrated into the main body of the paper for better coherence and structure. We also expanded the discussion on entropy regularization to highlight its benefits and limitations in comparison to DAWOG. The study suggests that while entropy regularization marginally enhances the performance of the GEAW baselines, DAWOG maintains a superior edge. Regarding the statistical representation of our results, we have clarified in the figures and tables that all experiments were conducted with $4$ independent runs. We understand your point about using bootstrapped confidence intervals. However, for this study, we opted for standard deviation, a widely accepted metric in offline RL, to demonstrate the stability and convergence of our algorithm. The figures clearly show that the performance of entropy regularization consistently falls outside the range of $\mu \pm \sigma$ or even $\mu \pm 2\sigma$, reinforcing our confidence in DAWOG's robustness.

\textcolor{blue}{\textbf{Reviewer \#2}} 

We are pleased to hear that our responses in the previous revision addressed your concerns. In the latest version of our manuscript, we have integrated the additional findings and experiments into the main paper, following your valuable suggestion. This integration has enhanced the structure and coherence of our work, making the benefits and applications of the DAWOG algorithm clearer to our readers.

\vspace{10pt}

\vspace{20pt}

Sincerely,

The Authors of Paper MACH-D-23-00139


\end{document}