\documentclass[sn-mathphys]{sn-jnl}% Math and Physical Sciences Reference Style

\usepackage{pdflscape}
\usepackage{subcaption}

\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
\newtheorem{proposition}[theorem]{Proposition}% 
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%
\usepackage{comment}
%\excludecomment{confidential}

\raggedbottom


\begin{document}

%\title[Dual-advantage weighted supervised lea for goal-conditioned policy learning in off-line settings]{Dual-advantage weighting for goal-conditioned policy learning in off-line settings}

\title[Goal-conditioned Offline RL through State Space Partitioning]{Goal-conditioned Offline Reinforcement Learning through State Space Partitioning}


\author[1]{\fnm{Mianchu} \sur{Wang}}\email{mianchu.wang@warwick.ac.uk}
\author[1]{\fnm{Yue} \sur{Jin}}\email{yue.jin.3@warwick.ac.uk}
\author[1,2]{\fnm{Giovanni} \sur{Montana}}\email{g.montana@warwick.ac.uk\footnote{Corresponding author.} }

\affil[1]{\orgname{University of Warwick}, \orgaddress{\city{Coventry}, \country{UK}}}
\affil[2]{\orgname{Alan Turning Institute}, \orgaddress{\city{London}, \country{UK}}}

\abstract{

Offline reinforcement learning (RL) seeks to develop sequential decision-making policies using only offline datasets. This presents a significant challenge, especially when attempting to accomplish multiple distinct goals or outcomes within a given scenario while receiving sparse rewards. Though previous research has demonstrated that using advantage-weighted log-likelihood loss for offline learning of goal-conditioned policies via supervised learning ensures monotonic policy improvement, it is not sufficient to fully address distribution shift and multi-modality issues. The latter is particularly problematic in long-horizon tasks where identifying a unique and optimal policy that transitions from a state to the desired goal is difficult due to the presence of multiple, potentially conflicting solutions. To address these challenges, we introduce a complementary advantage-based weighting scheme that incorporates an additional source of inductive bias. Given a value-based partitioning of the state space, the contribution of actions expected to lead to target regions that are easier to reach, compared to the final goal, is further increased. Our proposed approach, Dual-Advantage Weighted Offline Goal-conditioned RL (DAWOG), outperforms several competing offline algorithms in widely used benchmarks. Furthermore, we provide an analytical guarantee that the learned policy will not be inferior to the underlying behavior policy.}

\keywords{Goal-conditioned RL, Offline RL, Imitation Learning}

\maketitle

\section{Introduction}

Goal-conditioned reinforcement learning (GCRL) aims to learn policies capable of reaching a wide range of distinct goals, effectively creating a vast repertoire of skills \cite{liu2022goal, plappert2018multi, andrychowicz2017hindsight}. When extensive historical training datasets are available, it becomes possible to infer decision policies that surpass the unknown behavior policy (i.e., the policy that generated the data) in an offline manner, without necessitating further interactions with the environment \cite{eysenbach2022contrastive, mezghani2022learning, chebotar2021actionable}. A primary challenge in GCRL lies in the reward signal's sparsity: an agent only receives a reward when it achieves the goal, providing a weak learning signal. This becomes especially challenging in long-horizon problems where reaching the goals by chance alone is difficult.

In an offline setting, the challenge of learning with sparse rewards becomes even more complex due to the inability to explore beyond the already observed states and actions. When the historical data comprises expert demonstrations, imitation learning presents a straightforward approach to offline GCRL \cite{ghosh2021learning,emmons2022rvs}: in goal-conditioned supervised learning (GCSL), offline trajectories are iteratively relabeled, and a policy learns to imitate them directly. Furthermore, GCSL's objective lower bounds a function of the original GCRL objective \cite{yang2022rethinking}. However, in practice, the available demonstrations can often contain sub-optimal examples, leading to inferior policies. A simple yet effective solution involves re-weighting the actions during policy training within a likelihood maximization framework. A parameterized advantage function is employed to estimate the expected quality of an action conditioned on a target goal, so that higher-quality actions receive higher weights (Yang et al., 2022). This approach is referred to as weighted goal-conditioned supervised learning (WGCSL).

Despite its effectiveness, in this paper we argue that WGCSL still faces the well-known multi-modality issue, particularly in long-horizon tasks: identifying an optimal policy that reaches any desired goal is challenging due to the presence of multiple and potentially conflicting trajectories leading to it. While a goal-conditioned advantage function ensures that actions expected to lead to the goal receive heightened focus during training, we hypothesize that incorporating an additional source of inductive bias can provide shorter-horizon, more attainable targets, thereby assisting the policy in learning how to make incremental progress towards any ultimate goal.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/exp_fig/trajectories/full_trajs.png}
    \caption{Visualisation of trajectories (in blue) in different maze environments. The trajectories have been generated by policies trained via supervised learning with different action weighting schemes:  no action weighting (left),  goal-conditioned advantage weighting (middle) and dual-advantage weighting (right). In this task, an agent (ant) needs to reach the end goal (red circle) from a starting position (orange circle). The proposed dual-advantage weighting scheme can remarkably alleviate the multi-modality issue.}
    \label{fig:traj_visual_intro}
\end{figure}

In this study, we explore a complementary advantage-based action weighting scheme that further capitalizes on the goal-conditioned value function. During training, the state space is iteratively divided into a fixed number of regions, ensuring that all states within the same region have approximately the same goal-conditioned value. These regions are then ranked from the lowest to the highest value. Given the current state, the policy is encouraged to reach the immediately higher-ranking region, relative to the state's present region, in the fewest steps possible. This "target region" offers a state-dependent, short-horizon objective that is easier to achieve compared to the final goal, leading to generally shorter successful trajectories. Our proposed algorithm, Dual-Advantage Weighted Offline GCRL (DAWOG), seamlessly integrates the original goal-conditioned advantage weight with the new target-based advantage to effectively address the multi-modality issue.

A motivating example is illustrated in Figure \ref{fig:traj_visual_intro}, which presents the behavior of three pre-trained policies in maze-based navigation environments. A quadruped robot has learned to navigate through the maze. The robot is given previously unseen goals (red circles) that must be reached from a starting position (orange circles). The policies have been trained using supervised learning with no action weighting as a baseline (left), goal-conditioned advantage weighting (middle), and the proposed dual-advantage weighting (right). While goal-conditioned advantage weighting generally improves upon the baseline, the policy can sometimes lead the robot to sub-optimal regions, necessitating additional time steps before returning on track towards the goal. Remarkably, the proposed dual-advantage weighting scheme can alleviate the multi-modality issue and produce policies that take fewer detours, reaching the goal through smoother and shorter trajectories.

The main contributions of this paper are as follows. First, we introduce DAWOG, a simple-to-train and stable algorithm for offline GCRL. Second, we provide theoretical guarantees that DAWOG never performs worse than the underlying behavior policy. Third, we extensively evaluate DAWOG on benchmark datasets \cite{fu2020d4rl, plappert2018multi, yang2022rethinking} in comparison with several competing offline learning algorithms, including methods adaptable for the goal-conditioned setting. Our empirical results demonstrate that DAWOG improves upon the current state-of-the-art performance, particularly in the AntMaze environments  \cite{fu2020d4rl}  even in challenging settings where goals are uniformly sampled across the entire maze. Lastly, we present a series of additional studies to illustrate DAWOG's properties and its low sensitivity to hyperparameters.

\section{Related Work}

In this section, we offer a brief overview of methodologically related approaches. In goal-conditioned RL (GCRL) In \textbf{goal-conditioned RL (GCRL)}, one of the main challenges is the sparsity of the reward signal. An effective solution is hindsight experience replay (HER) \cite{andrychowicz2017hindsight}, which relabels failed rollouts that have not been able to reach the original goals and treats them as successful examples for different goals thus effectively learning from failures. HER has been extended to solve different challenging tasks in synergy with other learning techniques, such as curriculum learning \cite{fang2019curriculum}, model-based goal generation \cite{yang2021mher, jurgenson2020sub, nasiriany2019planning, nair2018visual}, and generative adversarial learning \cite{durugkar2021adversarial,charlesworth2020plangan}. In the offline setting, GCRL aims to learn goal-conditioned policies using only a fixed dataset. The simplest solution has been to adapt standard offline reinforcement learning algorithms \cite{kumar2020conservative,fujimoto2021minimalist} by simply concatenating the state and the goal as a new state. Chebotar \textit{et al.} \cite{chebotar2021actionable} propose goal-conditioned conservative Q-learning and goal-chaining to prevent value over-estimation and increase the diversity of the goal. Some of the previous works design offline GCRL algorithms from the perspective of state-occupancy matching \cite{eysenbach2022contrastive}. Mezghani \textit{et al.} \cite{mezghani2022learning} propose a self-supervised reward shaping method to facilitate offline GCRL.

Our work is most related to \textbf{goal-conditioned imitation learning (GCIL)}.   Emmons \textit{et al.} \cite{emmons2022rvs} study the importance of concatenating goals with states showing its effectiveness in various environments. 
Ding \textit{et al.}  \cite{ding2019goal} extend generative adversarial imitation learning \cite{ho2016generative} to goal-conditioned settings. 
 Ghosh \textit{et al.} \cite{ghosh2021learning} extend behaviour cloning \cite{bain1995framework} to goal-conditioned settings and propose goal-conditioned supervised learning (GCSL) to imitate relabelled offline trajectories. 
Yang \textit{et al.} \cite{yang2022rethinking} connect GCSL to offline GCRL, and show that the objective function in GCSL is a lower bound of a function of the original GCRL objective. They propose the WGCSL algorithm, which re-weights the offline data based on advantage function similarly to \cite{peng19advantage,wang2018exponentially}. We addresses the offline GCRL problem using a similar approach, but extend the idea further by designing a new advantage-based action re-weighting scheme. 

Some connections can also be found with \textbf{goal-based hierarchical reinforcement learning} methods\cite{li2022hierarchical,chane2021goal,kim2021landmarkguided,zhang2021worldmodel,nasiriany2019planning}. These works feature a high-level model capable of predicting a sequence of intermediate sub-goals and learn low-level policies to achieve them. Instead of learning to reach a specific sub-goals, our policy learns to reach an entire sub-region of the state space containing states that are equally valuable and provide an incremental improvement towards the final goal. 
%In a way, our approach  inherits the advantage of hierarchical reinforcement learning in long horizon tasks while avoiding the out-of-distribution goals generated from the high-level model.

Lastly, there have been other applications of \textbf{state space partitioning} in reinforcement learning, such as facilitating exploration and accelerating policy learning in online settings \cite{ma2020clustered,wei2018learning,karimpanal2017identification,mannor2004dynamic}. Ghosh \textit{et al.} \cite{ghosh2018divide} demonstrate that learning a policy confined to a state partition instead of the whole space can lead to low-variance gradient estimates for learning value functions.  In their work, states are partitioned using K-means to learn an ensemble of locally optimal policies, which are then progressively merged into a single, better-performing policy. Instead of partitioning states based on their geometric proximity, we partition states according to the proximity of their corresponding goal-conditioned values. We then use this information to define an auxiliary reward function and, consequently, a region-based advantage function.

\section{Preliminaries} \label{sec:preliminaries}
\textbf{Goal-conditioned MDPs.} Goal-conditioned tasks are usually modelled as Goal-Conditioned Markov Decision Processes (GCMDP), denoted by a tuple $<\mathcal{S}, \mathcal{A}, \mathcal{G}, P, R>$ where $\mathcal{S}$, $\mathcal{A}$, and $\mathcal{G}$ are the state, action and goal space, respectively. For each state $s \in \mathcal{S}$, there is a corresponding achieved goal, $\phi(s) \in \mathcal{G}$, where $\phi:\mathcal{S} \rightarrow \mathcal{G}$ \cite{liu2022goal}. At a given state $s_t$, an action $a_t$  taken towards a desired goal $g$ results in a visited next state $s_{t+1}$ according to the environment's transition dynamics, $P(s_{t+1} \mid s_t, a_t)$. The environment then provides a reward, $r_t = R(s_{t+1}, g)$, which is non-zero  only when the goal has been reached, i.e.,
\begin{equation} \label{reward_function}
    R(s, g) =
    \begin{cases}
      1, & \text{if $\mid \mid \phi(s) - g \mid \mid ^2_2 \leq \text{threshold}$,}\\
      0, & \text{otherwise.}\\
    \end{cases}       
\end{equation}

\textbf{Offline Goal-conditioned RL.} In offline GCRL, the agent aims to learn a goal-conditioned policy, $\pi: \mathcal{S} \times \mathcal{G} \rightarrow \mathcal{A}$, using an offline dataset containing previously logged trajectories that might be generated by any number of unknown behaviour policies. 
% The historical data might have been collected by any number of unknown behaviour policies. 
The objective is to maximise the expected and discounted cumulative returns, 
\begin{equation} \label{rl_objective}
    J_{GCRL}(\pi)=\mathbb{E}_{\substack{g \sim P_g, s_0 \sim P_0, \\a_t \sim \pi(\cdot \mid s_t, g), \\s_{t+1} \sim P(\cdot \mid s_t, a_t)}} \left [ \sum^T_{t=0} \gamma^t r_t \right ],
\end{equation}
where $\gamma \in (0, 1]$ is a discount factor, $P_g$ is the distribution of the goals, $P_0$ is the distribution of the initial state, and $T$ 
% is the terminal time step; 
corresponds to the time step at which an episode ends, i.e., either the goal has been achieved or timeout has been reached. 

\textbf{Goal-conditioned Supervised Learning (GCSL).} GCSL \cite{ghosh2021learning} relabels the desired goal in each data tuple $(s_t, a_t, g)$ with the goal achieved henceforth in the trajectory to increase the diversity and quality of the data \cite{andrychowicz2017hindsight, kaelbling1993learning}. The relabelled dataset is denoted as 
$\mathcal{D}_R=\{(s_t, a_t, g=\phi(s_i)) \mid T \geq i > t \geq 0\}$. GCSL 
% trains decision policies 
learns a policy that mimics the relabelled transitions by maximizing
\begin{equation}
    J_{GCSL}(\pi) = \mathbb{E}_{(s_t, a_t, g) \sim \mathcal{D}_R} \left [ \pi(a_t \mid s_t, g) \right ].
\end{equation}
Yang \textit{et al.} \cite{yang2022rethinking} have connected GCSL to GCRL and demonstrated that $J_{GCSL}$ lower bounds $\frac{1}{T}\log J_{GCRL}$.

\textbf{Goal-conditioned Value Functions.} A goal-conditioned state-action value function \cite{schaul2015universal} quantifies the value of an action $a$ taken from a state $s$ conditioned on a goal $g$ using the sparse rewards of Eq. \ref{reward_function},
\begin{equation} \label{eq:goal_q}
    Q^\pi(s, a, g)=\mathbb{E}_{\pi} \left [ \sum^T_{t=0} \gamma^t r_t \mid s_0=s, a_0=a \right ]
\end{equation}
where $\mathbb{E}_{\pi}[\cdot]$ denotes the expectation taken with respect to $a_t \sim \pi(\cdot \mid s_t, g)$ and $s_{t+1} \sim P(\cdot \mid s_t, a_t)$. Analogously, the goal-conditioned state value function quantifies the value of a state $s$ when trying to reach $g$,
\begin{equation} \label{pre:goal_v}
    V^\pi(s, g)=\mathbb{E}_{\pi} \left [ \sum^T_{t=0} \gamma^t r_t \mid s_0=s\right ].
\end{equation}
The goal-conditioned advantage function, 
\begin{equation} \label{eq:goal_adv}
    A^\pi(s, a, g)=Q^\pi(s, a, g)-V^\pi(s, g),
\end{equation}
then quantifies how advantageous it is to take a specific action $a$ in state $s$ towards $g$ over taking the actions sampled from $\pi(\cdot \mid s, g)$ \cite{yang2022rethinking}. 

\section{Methods} \label{sec:methods}

In this section, we  formally present the proposed methodology and analytical results. First, we introduce a notion of target region advantage function in Section \ref{function}, which we use to develop the learning algorithm in Section \ref{algo}.  In Section \ref{theory} we provide a theoretical analysis offering guarantees that  DAWOG  learns a policy that is never worse than the underlying behaviour policy.

%\subsection{Overview}\label{overview}
 

%We propose dual-advantage weighted offline GCRL (DAWOG) algorithm to solve GCRL problems. DAWOG utilises an exponential weighting factor that combines the goal-conditioned advantage function and a novel region-based advantage function to incentivize actions to reach the goal  within as few time steps as possible. 

% In the reminder of this section, we properly define the target region, define the target region-conditioned value functions and the advantage function, and present the full algorithm. Finally, we demonstrate that the policy learnt by the algorithm is not worse than the behavioural policy that generates the relabelled data. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/intro.png}
    \caption{Illustration of the two advantage functions used by DAWOG for a simple navigation task. First, a goal-conditioned advantage is learnt using only relabelled offline data. Then, a target-region advantage is obtained by partitioning the states according to their goal-conditioned value function, identifying a target region, and rewarding actions leading to this region in the smallest possible number of steps. DAWOG updates the policy to imitate the offline data through an exponential weighting factor that depends on both advantages.}
   \label{fig:algo_diagram}
\end{figure}

\subsection{Target region advantage function}\label{function}

For any state $s \in \mathcal{S}$ and goal $g \in \mathcal{G}$, the domain of the goal-conditioned value function in Eq. \ref{pre:goal_v} is the unit interval due to the binary nature of the reward function in Eq. \ref{reward_function}. Given a positive integer $K$, we partition $[0, 1]$ into $K$ equally sized intervals, $\{ \beta_i \}_{i=1,\ldots,K}$. For any goal $g$, this partition induces a corresponding partition of the state space. 

\begin{definition} \label{def:bin}
(Goal-conditioned State Space Partition)
For a fixed desired goal $g \in \mathcal{G}$, the state space is partitioned into $K$ equally sized regions according to $V^\pi(\cdot, g)$. The $k^{th}$ region, notated as $B_{k}(g)$, contains  all states whose goal-conditioned values are within $\beta_k$, i.e.,
\begin{equation}
B_{k} (g) = \{s \in \mathcal{S} \mid  V^\pi(s, g) \in \beta_k \}. 
\end{equation}
\end{definition}

Our ultimate objective is to up-weight actions taken in a state $s_t \in B_{k}(g)$ that are likely to lead to a region only marginally better (but never worse) than $B_{k}(g)$ as rapidly as possible. 

\begin{definition} \label{def:target_region}
(Target Region) For $s \in B_{k}(g)$, the mapping $b(s, g): \mathcal{S} \times \mathcal{G} \rightarrow \{1, \ldots, K\}$ returns the correct index $k$. The goal-conditioned target region is defined as 
\begin{equation}
    G(s, g) = B_{\min\{b(s, g)+1, K\}}(g),
\end{equation}
which is the set of states whose goal-conditioned value is not less than the states in the current region. For $s \in B_k(g)$, $G(s, g)$ is the current region $B_k(g)$ if and only if $k=K$. 
\end{definition}

We now introduce two target region value functions. 

\begin{definition} (Target Region Value Functions) For a state $s$, action $a \in \mathcal{A}$, and the target region $G(s, g)$, we define a target region V-function and a target region Q-function based on an auxiliary reward function that returns a non-zero reward only when the next state belongs to the target region, i.e., 
\begin{equation}
    \tilde{r}_t = \tilde{R}(s_t, s_{t+1}, G(s_t, g)) = 
    \begin{cases}
      1, & \text{if } s_{t+1} \in G(s_t,g)\\
      0, & \text{otherwise.} \\
    \end{cases}
\end{equation}
The target region Q-value function is 
\begin{equation}
    \Tilde{Q}^\pi(s, a, G(s, g)) = \mathbb{E}_{\pi} \left [ \sum^{\tilde{T}}_{t=0} \gamma^t \tilde{r}_t \mid s_0=s, a_0=a \right ],
\end{equation}
where $\tilde{T}$ corresponds to the time step at which the target region is achieved or timeout is reached, $\mathbb{E}_{\pi}[\cdot]$ denotes the expectation taken with respect to the policy $a_t \sim \pi(\cdot \mid s_t, g)$ and the transition dynamics $s_{t+1} \sim P(\cdot \mid s_t, a_t)$. The target region Q-function estimates the expected cumulative return when starting in $s_t$, taking an action $a_t$, and then following the policy $\pi$, based on the auxiliary reward. The discount factor $\gamma$ reduces the contribution of delayed target achievements. Analogously, the target region value function is defined as
\begin{equation}
    \Tilde{V}^\pi(s, G(s, g)) = \mathbb{E}_{\pi} \left [ \sum^{\tilde{T}}_{t=0} \gamma^t \tilde{r}_t \mid s_0=s \right ]
\end{equation}
and quantifies the quality of a state $s$ according to the same criterion. 
\end{definition}

Using the above value functions, we are in a position to introduce the corresponding target region advantage function. 
\begin{definition} (Target Region Advantage Function)
    The target region-based advantage function is defined as 
    \begin{equation} \label{eq:region_adv} 
    \tilde{A}^\pi(s, a, G(s,g)) =  \tilde{Q}^\pi(s, a, G(s, g))  - \tilde{V}^\pi(s, G(s, g)).
    \end{equation}
    It estimates the advantage of action $a$ towards the target region in terms of the cumulative return by taking $a$ in state $s$ and following the policy $\pi$ thereafter, compared to taking actions sampled from the policy.
\end{definition}


\subsection{The DAWOG algorithm}\label{algo}

The proposed DAWOG belongs to the family of WGCSL algorithms, i.e. it is designed to optimise the following objective function 
\begin{equation} \label{eq:policy}
    J_{DAWOG}(\pi) = \mathbb{E}_{(s_t, a_t, g) \sim \mathcal{D}_R} \left [ w_t \log \pi(a_t \mid s_t, g) \right ]
\end{equation}
where the role of $w_t$ is to re-weight each action's contribution to the loss. In DAWOG, $w_t$ is an exponential weight of form 
\begin{equation}\label{eq:weight}
    w_t = \exp_{clip} (\beta A^{\pi_b}(s_t, a_t, g) + \Tilde{\beta} \Tilde{A}^{\pi_b}(s_t, a_t, G(s_t, g)),
\end{equation}
where $\pi_b$ is the underlying behaviour policy that generate the relabelled dataset $\mathcal{D}_R$. The contribution of the two advantage functions, $\Tilde{A}^{\pi_b}(s_t, a_t, g)$ and $\Tilde{A}^{\pi_b}(s_t, a_t, G(s_t,g))$, is controlled by positive scalars, $\beta$ and $\Tilde{\beta}$, respectively. However, empirically, we have found that using a single shared parameter generally performs well across the tasks we have considered (see Section \ref{further_studies}). The clipped exponential, $\exp_{clip}(\cdot)$, is used for numerical stability and keeps the values within the $(0, M]$ range, for a given $M>0$  threshold.

The algorithm combines the originally proposed goal-conditioned advantage \cite{yang2022rethinking} with the novel target region advantage. The former ensures that actions likely to lead to the goal are up-weighted. However, when the goal is still far, there may still be several possible ways to reach it, resulting in a wide variety of favourable actions. The target region advantage function provides additional guidance by further increasing the contribution of actions expected to lead to a higher-valued sub-region of the state space as rapidly as possible. Both $A^{\pi_b}(s_t, a_t, g)$ and $\Tilde{A}^{\pi_b}(s_t, a_t, G(s_t,g))$ are beneficial in a complementary fashion: whereas the former is more concerned with long-term gains, which are more difficult and uncertain, the latter is more concerned with short-term gains, which are easier to achieve. As such, these two factors are complementary and their combined effect plays an important role in the algorithm's final performance (see Section \ref{further_studies}).  An illustration of the dual-advantage weighting scheme is shown in Fig.\ref{fig:algo_diagram}. 

In the remainder, we explain the entire training procedure. The advantage $A^{\pi_b}(s_t, a_t, g)$ is estimated via 
\begin{equation}\label{goal-advantage}
A^{\pi_b}(s_t, a_t, g)= r_t + \gamma V^{\pi_b}(s_{t+1}, g) - V^{\pi_b}(s_t, g).
\end{equation} 

In practice, the goal-conditioned V-function is approximated by a deep neural network with parameter $\psi_1$, which is learnt by minimising the temporal difference (TD) error \cite{sutton2018reinforcement}:
\begin{equation} \label{eq:goal_value}
    \mathcal{L}(\psi_1) = \mathbb{E}_{(s_t, s_{t+1}, g) \sim \mathcal{D}_R} \left [ (V_{\psi_1}(s_t, g) - y_t)^2 \right ],
\end{equation}
where $y_t$ is the target value given by
\begin{equation}
    y_t=r_t + \gamma (1 - d(s_{t+1}, g)) V_{\psi^-_1}(s_{t+1}, g).
\end{equation}
Here $d(s_{t+1}, g)$) indicates whether the state $s_{t+1}$ has reached the goal $g$. The parameter vector $\psi^-_1$ is a slowly moving average of $\psi_1$ to stabilise training \cite{mnih2015human}. Analogously, the target region advantage function is estimated by 
\begin{equation}\label{region-advantage}
    \tilde{A}^{\pi_b}(s_t, a_t, G(s_t, g))= \tilde{r}_t + \gamma \tilde{V}^{\pi_b}(s_{t+1}, G(s_t, g)) - \tilde{V}^{\pi_b}(s_t, G(s_t, g)),
\end{equation}
where the target region V-function is approximated with a deep neural network parameterised with $\psi_2$. The relevant loss function is
\begin{equation} \label{eq:region_value}
    \mathcal{L}(\psi_2) = \mathbb{E}_{(s_t, s_{t+1}, g) \sim \mathcal{D}_R} \left [ (\tilde{V}_{\psi_2}(s_t, G(s_t, g)) - \tilde{y}_t)^2 \right ],
\end{equation}
where the target value is 
\begin{equation}
    \tilde{y}_t=\tilde{r}_t + \gamma (1 - \tilde{d}(s_{t+1}, G(s_t, g))) \tilde{V}_{\psi^-_2}(s_{t+1}, G(s_t, g)).
\end{equation}
and $\tilde{d}(s_{t+1}, G(s_t, g))$) indicates whether the state $s_{t+1}$ has reached the target region $G(s_t, g)$. $\psi^-_2$ is a slowly moving average of $\psi_2$.  The full procedure is presented in Algorithm \ref{pseudocode} where the two value functions are jointly optimised and contribute to optimising Eq. \ref{eq:policy}. 

\begin{algorithm}[t]
\caption{Dual-Advantage Weighted Offline GCRL (DAWOG)} \label{pseudocode}
\begin{algorithmic}[1]
% \textbf{Input:} Relabelled dataset $\mathcal{D}_R$
\renewcommand{\algorithmicrequire}{\textbf{Initialise:}}
\Require parameters $\theta$, $\psi_1$, $\psi^-_1$, $\psi_2$, and $\psi^-_2$ for policy network, goal-conditioned value network and its target network, partition-conditioned value network and its target network, respectively; relabelled dataset $\mathcal{D}_R$
\While{\textit{not converged}}
    \For {$i=1, \ldots,I$}
        \State Sample a minibatch $\{(s, a, s', g)\}^B_{b=1}$ from $D_R$.
        \State Update $\psi_1$ to minimise Eq. \ref{eq:goal_value}.
        \State Update $\psi_2$ to minimise Eq. \ref{eq:region_value}.
        \State Comp. $A(s, a, g) = r_t + \gamma V_{\psi_1}(s', g) - V_{\psi_2}(s, g)$.
        \State Comp. $\tilde{A}(s, a, G(s, g)) = \tilde{r}_t + \gamma \tilde{V}_{\psi_2}(s', G(s, g)) - \tilde{V}_{\psi_2}(s, G(s, g))$.
        \State Update the policy by minimising Eq. \ref{eq:policy}.
    \EndFor
    \State $\psi^-_1 \leftarrow \rho \psi_1 + (1-\rho) \psi^-_1$.
    \State $\psi^-_2 \leftarrow \rho \psi_2 + (1-\rho) \psi^-_2$.
\EndWhile
\State \Return $\pi_\theta$.
\end{algorithmic}
\end{algorithm}

\subsection{Policy improvement guarantees}\label{theory}
In this section, we demonstrate that our learnt policy is never worse than the underlying behaviour policy $\pi_b$ that generates the relabelled data. First,  we express the policy learnt by our algorithm in an equivalent form, as follows. 

\begin{proposition}
    DAWOG learns a policy $\pi_\theta$ to minimise the KL-divergence from 
    \begin{equation} \label{eq:dual_imitate_policy}
        \tilde{\pi}_{dual}(a \mid s, g) = \exp (w + N(s, g)) \pi_b(a \mid s, g), 
    \end{equation}
    where $w = \beta A^{\pi_b}(s, a, g) + \tilde{\beta} \tilde{A}^{\pi_b}(s, a, G(s,g))$, $G(s,g)$ is the target region, and $N(s, g)$ is a normalising factor to ensuring that $\sum_{a \in \mathcal{A}} \tilde{\pi}_{dual}(a \mid s, g)=1$. 
\end{proposition}
\begin{proof}
    According to Eq. \ref{eq:policy}, DAWOG maximises the following objective with the policy parameterised by $\theta$:
    \begin{align}
    \begin{split}
        \arg \max_\theta J(\theta) = & \arg \max_\theta \mathbb{E}_{(s, a, g) \sim \mathcal{D}_R} \left [ \exp (w + N(s, g)) \log \pi_\theta(a \mid s, g)\right ] \\
        = & \arg \max_\theta \mathbb{E}_{(s, g) \sim \mathcal{D}_R} \left [ \sum_{a} \exp(w + N(s, g)) \pi_b(a \mid s, g) \log \pi_\theta(a \mid s, g) \right ] \\
        = & \arg \max_\theta \mathbb{E}_{(s, g) \sim \mathcal{D}_R} \left [  \sum_{a} \tilde{\pi}_{dual}(a \mid s, g) \log \pi_\theta(a \mid s, g) \right ] \\
        = & \arg \max_\theta \mathbb{E}_{(s, g) \sim \mathcal{D}_R} \left [  \sum_{a} \tilde{\pi}_{dual}(a \mid s, g) \log \frac{\pi_\theta(a \mid s, g)}{\tilde{\pi}_{dual}(a \mid s, g)} \right ]\\
        = & \arg \min_\theta \mathbb{E}_{(s, g) \sim \mathcal{D}_R} \left [  D_{KL} (\tilde{\pi}_{dual}(\cdot \mid s, g) \mid \mid \pi_\theta(\cdot \mid s, g)) \right ]
    \end{split}
    \end{align}
    $J(\theta)$ reaches its maximum when
    \begin{equation}
        D_{KL} (\tilde{\pi}_{dual}(\cdot \mid s, g) \mid \mid \pi_{\theta}(\cdot \mid s, g)) = 0, \forall s \in \mathcal{S}, g \in \mathcal{G}.
    \end{equation}
\end{proof}

Then, we propose Proposition \ref{prop:mono_inc} to show the condition for policy improvement.
\begin{proposition} \label{prop:mono_inc}
\textup{\cite{wang2018exponentially,yang2022rethinking}} Suppose two policies $\pi_1$ and $\pi_2$ satisfy
\begin{equation} \label{eq:mono_inc}
    h_1(\pi_2(a \mid s, g)) = h_1(\pi_1(a \mid s, g)) + h_2(s, g, A^{\pi_1}(s, a, g))
\end{equation}
where $h_1(\cdot)$ is a monotonically increasing function, and $h_2(s, g, \cdot)$ is monotonically increasing for any fixed $s$ and $g$. Then we have 
\begin{equation*}
    V^{\pi_2}(s, g) \geq V^{\pi_1}(s, g), \forall s \in \mathcal{S} \text{ and } g \in \mathcal{G}.
\end{equation*}
That is, $\pi_2$ is uniformly as good as or better than $\pi_1$.
\end{proposition}

We want to leverage this result to demonstrate that  $V^{\tilde{\pi}_{dual}}(s, g) \geq V^{\pi_b}(s, g)$ for any state $s$ and goal $g$. Firstly, we need to obtain a monotonically increasing function  $h_1(\cdot)$. This is achieved by taking the logarithm of the both sides of Eq. \ref{eq:dual_imitate_policy}, i.e.,
\begin{align}
\begin{split}
    \log \tilde{\pi}_{dual}(a \mid s, g) =& \log \pi_b(a \mid s, g) + \beta A^{\pi_b}(s, a, g) \\& + \tilde{\beta} \tilde{A}^{\pi_b}(s, a, G(s,g)) + N(s, g).
\end{split}
\end{align}
so that $h_1(\cdot)= \log(\cdot)$. The following proposition establishes that we also have a function $h_2(s, g, A^{\pi_b}(s, a, g)) =  \beta A^{\pi_b}(s, a, g) + \tilde{\beta} \tilde{A}^{\pi_b}(s, a, G(s,g)) + N(s, g)$, which is monotonically increasing for any fixed $s$ and $g$. Since $\beta, \tilde{\beta} \geq 0$ and $N(s, g)$ is independent of the action, it is equivalent to prove that for any fixed $s$ and $g$, there exists a monotonically increasing function $l$ satisfying 
\begin{equation}
l(s, g, \tilde{A}^{\pi_b}(s, a, G(s,g))) = A^{{\pi_b}}(s, a, g).
\end{equation}
% the fact can be written in the form of Proposition \ref{prop: mono_proof}.

\begin{proposition} \label{prop: mono_proof}
    Given fixed $s$, $g$ and the target region $G(s,g)$, the goal-conditioned advantage function $A^\pi$ and the target region-conditioned advantage function $\tilde{A}^\pi$ satisfy 
    $l(s, g, A^\pi(s, a, g)) = \tilde{A}^\pi(s, a, G(s,g))$, where $l(s, g,\cdot)$ is monotonically increasing for any fixed $s$ and $g$.
\end{proposition}

\begin{proof}
    By the definition of monotonically increasing function, if for all $a', a'' \in \mathcal{A}$ such that $A^\pi(s, a', g) \geq A^\pi(s, a'', g)$ and we can reach $\tilde{A}^\pi(s, a', G(s,g)) \geq \tilde{A}^\pi(s, a'', G(s,g))$, then the proposition can be proved.
    
    We start by having any two actions $a', a'' \in \mathcal{A}$ such that 
    \begin{equation}
        A^\pi(s, a', g) \geq A^\pi(s, a'', g).
    \end{equation}
    By adding $V^\pi(s, g)$ on both sides, the inequality becomes
    \begin{equation} \label{Q_ineq}
        Q^\pi(s, a', g) \geq Q^\pi(s, a'', g). 
    \end{equation}
    By Definition \ref{eq:goal_q}, the goal-conditioned Q-function can be written as
    \begin{equation} \label{Q_traj}
        Q^\pi(s, a, g)=\mathbb{E}_{\pi} \left [ R_{t,\tau_i} \mid s_t=s, a_t=a \right ],
    \end{equation}
    where $\tau_i$ represents a trajectory: $s_t, a_t, r_t^i, s_{t+1}^i, a_{t+1}^i, r_{t+1}^i, \ldots, s_{T}^i$
    \begin{equation} \label{}
        R_{t,\tau_i}=r_t^i + \gamma r_{t+1}^i + \ldots + \gamma^{t_{tar}^i}V(s_{tar}^i, g),
    \end{equation}
    $s_{tar}^i$ corresponds to the state where $\tau_i$ gets into the target region, $t_{tar}^i$ is the corresponding time step.
    Because the reward is zero until the desired goal is reached, Eq. \ref{Q_traj} can  be written as 
    \begin{equation} \label{Q_traj_}
        Q^\pi(s, a, g)=\mathbb{E}_{\pi} \left [\gamma^{t_{tar}^i}V(s_{tar}^i, g) \mid s_t=s, a_t=a \right ].
    \end{equation}
    Similarly, 
    \begin{equation} \label{our_Q_traj}
    \begin{aligned}
        \tilde{Q}^\pi(s, a, G(s,g))&=\mathbb{E}_{\pi} \left [\gamma^{t_{tar}^i}\tilde{V}(s_{tar}^i, G(s,g)) \mid s_t=s, a_t=a \right ]\\
        &=\mathbb{E}_{\pi} \left [\gamma^{t_{tar}^i} \mid s_t=s, a_t=a \right ].
    \end{aligned}
    \end{equation}
    According to Eq.\ref{Q_ineq} and Eq.\ref{Q_traj_}, we have
    \begin{equation} \label{Q_ineq_}
        \mathbb{E}_{\pi} \left [\gamma^{t_{tar}^{i'}}V(s_{tar}^{i'}, g) \mid s_t=s, a_t=a' \right ] \geq \mathbb{E}_{\pi} \left [\gamma^{t_{tar}^{i''}}V(s_{tar}^{i''}, g) \mid s_t=s, a_t=a'' \right ].
    \end{equation}
    Given the valued-based partitioning of the state space, we assume that the goal-conditioned values of states in the target region are sufficiently close such that $\forall i', i'', V(s_{tar}^{i'}, g) \approx v,  V(s_{tar}^{i''}, g) \approx v $. Then, Eq.\ref{Q_ineq_} can be approximated as 
    \begin{equation} \label{Q_ineq_final}
        v \cdot \mathbb{E}_{\pi} \left [\gamma^{t_{tar}^{i'}} \mid s_t=s, a_t=a' \right ] \geq  v \cdot \mathbb{E}_{\pi} \left [\gamma^{t_{tar}^{i''}} \mid s_t=s, a_t=a'' \right ].
    \end{equation}
    Removing $v$ on both sides of Eq.\ref{Q_ineq_final} and according to Eq.\ref{our_Q_traj}, we have
    \begin{equation}
        \tilde{Q}^\pi(s, a', G(s, g)) \geq \tilde{Q}^\pi(s, a'', G(s, g)).
    \end{equation}
    Then,
    \begin{equation}
        \tilde{A}^\pi(s, a', G(s, g)) \geq \tilde{A}^\pi(s, a'', G(s, g)).
    \end{equation}
\end{proof}

\section{Experimental results}

In this section, we examine DAWOG's performance relative to existing state-of-the-art algorithms using environments of increasing complexity. The remainder of this section is organized as follows. The benchmark tasks and datasets are presented in Section \ref{tasks}. The implementation details are provided in Section \ref{implementation}. A list of competing methods is presented in Section \ref{competing_methods}, and the comparative performance results are found in Section \ref{performance}. Here, we also qualitatively inspect the policies learnt by DAWOG in an attempt to characterise the improvements that can be achieved over other methods. Section \ref{further_studies} presents extensive ablation studies to appreciate the relative contribution of the different advantage weighting factors. Finally, in Section \ref{sensitivity}, we study how the dual-advantage weight depends on its hyperparameters.  

\subsection{Tasks and datasets} \label{tasks}

\subsubsection{Grid World} 
We designed two $16 \times 16$ grid worlds to assess the performance on a simple navigation task. From its starting position on the grid, an agent needs to reach a goal that has been randomly placed in one of the available cells. Only four actions are available to move left, right, up, and down. The agent accrues a positive reward when it reaches the cell containing the goal. To generate the benchmark dataset, we trained a Deep Q-learning algorithm \cite{mnih2015human}, whose replay buffer, containing $4,000$ trajectories of $50$ time steps, was used as the benchmark dataset.

\subsubsection{AntMaze Navigation}

The AntMaze suite used in our experiment is obtained from the D4RL benchmark \cite{fu2020d4rl}, which has been widely adopted by offline GCRL studies \cite{eysenbach2022contrastive,emmons2022rvs,li2022hierarchical}. The task requires to control an 8-DoF quadruped robot that moves in a maze and aims to reach a target location within an allowed maximum of $1,000$ steps. The suite contains three kinds of different maze layouts:  \texttt{umaze} (a U-shape wall in the middle), \texttt{medium} and \texttt{large}, and provides three training datasets. The datasets differ in the way the starting and goal positions of each trajectory were generated: in \texttt{umaze} the starting position is fixed and the goal position is sampled within a fixed-position small region; in \texttt{diverse} the starting and goal positions are randomly sampled in the whole environment; finally, in \texttt{play}, the starting and goal positions are randomly sampled within hand-picked regions. In the sparse-reward environment, the agent obtains a reward only when it reaches the target goal. We use a normalised score as originally proposed in \cite{fu2020d4rl}, i.e.,
\begin{equation*}
s_n = 100 \cdot \frac{s - s_r} { s_e - s_r}
\end{equation*}
where $s$ is the unnormalised score, $s_r$ is a score obtained using a random policy and $s_e$ is the score obtained using an expert policy. 

In our evaluation phase, the policy is tested online. The agent's starting position is always fixed, and the goal position is generated using one of the following methods: 
\begin{itemize}
    \item  \textit{fixed goal}:  the goal position is sampled within a small and fixed region in a corner of the maze, as in previous work \cite{eysenbach2022contrastive,emmons2022rvs,li2022hierarchical}; 
    \item  \textit{diverse goal}: the goal position is uniformly sampled over the entire region. This evaluation scheme has not been adopted in previous works, but helps assess the policy's generalisation ability in goal-conditioned settings.  
\end{itemize}

\subsubsection{Gym Robotics} 
Gym Robotics \cite{plappert2018multi} is a popular robotic suite used in both online and offline GCRL studies \cite{yang2022rethinking, eysenbach2022contrastive}.
The agent to be controlled is a 7-DoF robotic arm, and several tasks are available: in \texttt{FetchReach}, the arm needs to touch a desired location; in \texttt{FetchPush}, the arm needs to move a cube to a desired location;  in \texttt{FetchPickAndPlace} a cube needs to be picked up and moved to a desired location; finally, in \texttt{FetchSlide}, the arm needs to slide a cube to a desired location. Each environment returns a reward of one when the task has been completed within an allowed time horizon of $50$ time steps.  For this suite, we use the expert offline dataset provided by \cite{yang2022rethinking}. The dataset for FetchReach contains  $1 \times 10^5$ time steps whereas all the other datasets contain $2 \times 10^6$ steps. The datasets are collected using a pre-trained policy using DDPG and hindsight relabelling \cite{lillicrap2016continuous, andrychowicz2017hindsight}; the actions from the policy were perturbed by adding Gaussian noise with zero mean and $0.2$ standard deviation.

\subsection{Implementation details} \label{implementation}

DAWOG's training procedure is shown in Algorithm \ref{pseudocode}. In our implementation, for continuous control tasks, we use a Gaussian policy following previous recommendations \cite{raffin2021stablebaselines}. When interacting with the environment, the actions are sampled from the above distribution.  All the neural networks used in DAWOG are 3-layer multi-layer perceptrons with $512$ units in each layer and ReLU activation functions. The parameters are trained using the Adam optimiser \cite{kingma2014adam} with a learning rate $1 \times 10^{-3}$. The training batch size is $512$ across all networks. To represent $G(s, g)$ we use a $K$-dimensional one-hot encoding vector where the $i^{th}$ position is non-zero for the target region and zero everywhere else along with the goal $g$. Four hyper-parameters need to be chosen: the state partition size, $K$, the two coefficients controlling the relative contribution of the two advantage functions, $\beta$ and $\tilde{\beta}$, and the clipping bound, $M$. In our experiments, we use $K=20$ for umaze and medium maze, $K=50$ for large maze, and $K=10$ for all other tasks. In all our experiments, we use fixed values of $\beta = \tilde{\beta} = 10$. The clipping bound is always kept at $M=10$. 

\subsection{Competing methods} \label{competing_methods}

Several competing algorithms have been selected for comparison with DAWOG, including offline DRL methods that were not originally proposed for goal-conditioned tasks and required some minor adaptation. In the remainder of this Section, the nomenclature 'g-' indicates that the original algorithm has been implemented to operate in a goal-conditioned setting by concatenating the state and the goal as a new state and with hindsight relabelling.

The first category of algorithms includes regression-based methods that imitate the relabelled offline dataset using different weighting strategies: 
\begin{itemize}
    \item GCSL \cite{ghosh2021learning} imitates the relabelled transitions without any weighting strategies;
    \item g-MARWIL\cite{wang2018exponentially} incorporates goal-conditioned advantage to weight the actions' contribution in the offline data;
    \item WGCSL \cite{yang2022rethinking} incorporates goal-conditioned advantage to weight the actions' contribution using multiple criteria.
\end{itemize}

We also include three actor-critic methods: 
\begin{itemize}
    \item Contrastive RL \cite{eysenbach2022contrastive} estimates a Q-function by contrastive learning;
    \item g-CQL \cite{kumar2020conservative} learns a conservative Q-function;
    \item g-BCQ \cite{fujimoto2019off} learns a Q-function by clipped double Q-learning with a restricted policy;
    \item g-TD3-BC \cite{fujimoto2021minimalist}  combines TD3 algorithm \cite{fujimoto2018addressing} with a behavior cloning regularizer. 
\end{itemize}
Finally, we include a hierarchical learning method, IRIS \cite{mandlekar2020iris}, which employs a low-level imitation learning policy to reach sub-goals commanded by a high-level goal planner. 

\subsection{Performance comparisons and analysis} \label{performance}

\begin{table}
  \caption{Experiment results for the two Grid World navigation environments.}
  \label{exp:grid_res}
  \small
  \centering
  \begin{tabular}{l|rrr}
    \toprule
    Environment     & DAWOG & GCSL & g-MAR. \\
    \midrule
    grid-wall       & $\textbf{87.2}_{\pm 1.9}$   
                    & $70.6_{\pm 4.8}$  
                    & $79.8_{\pm 4.0}$ \\
    grid-umaze      & $\textbf{82.4}_{\pm 2.6}$
                    & $68.0_{\pm 3.8}$
                    & $77.2_{\pm 3.5}$\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{sidewaystable}
\sidewaystablefn%

\begin{minipage}{\textheight}
\centering
\caption{Experiment results in Gym Robotics.}
\label{exp:gym_res}
\begin{tabular}{l|rrrrrrrrr}
\toprule
Environment        & DAWOG   & GCSL    & WGCSL & g-MAR. &CRL & g-CQL  & g-TD3. & g-BCQ \\
\midrule
FetchReach        & $\textbf{46.7}_{\pm 0.1}$ & $41.7_{\pm 0.3}$  &$46.3_{\pm 0.1}$& $45.0_{\pm 0.1}$& $46.1_{\pm 0.1}$ &$1.0_{\pm 0.2}$& $45.5_{\pm 0.3}$ & $35.1_{\pm 3.1}$\\
FetchPush         & $\textbf{39.3}_{\pm 0.2}$          & $28.5_{\pm 0.9}$  &$39.1_{\pm 0.2}$ &$37.4_{\pm 0.2}$   & $36.5_{\pm 0.6}$ & $5.7_{\pm 0.8}$ & $30.8_{\pm 0.6}$        & $3.6_{\pm 0.9}$    \\
FetchPickAndPlace &$\textbf{37.9}_{\pm 0.4}$  & $25.2_{\pm 0.8}$  & $34.3_{\pm 0.5}$& $34.5_{\pm 0.5}$& $35.7_{\pm 0.2}$ & $3.2_{\pm 2.5}$& $36.5_{\pm 0.5}$& $1.4_{\pm 0.2}$ \\
FetchSlide        &$9.9_{\pm 0.8}$            & $3.05_{\pm 0.6}$  & $\textbf{10.7}_{\pm 1.0}$ & $4.5_{\pm 1.7}$& $9.9_{\pm 0.2}$& $0.8_{\pm 0.3}$ & $5.8_{\pm 0.6}$& $0.1_{\pm 0.1}$ \\
\bottomrule
\end{tabular}
\end{minipage}

\bigskip 

\begin{minipage}{\textheight}
\centering
\caption{Experiment results in AntMaze environments. The results are normalised by the expert score from D4RL paper.}
\label{exp:antmaze_res}
\begin{tabular}{l|l|rrrrrrrr}
\toprule
            & Environment        & DAWOG   & GCSL    & WGCSL & g-MAR. & CRL   & g-CQL  & g-TD3. & IRIS\\
\midrule 
            & umaze           & $\textbf{92.8}_{\pm 3.3}$  & $64.4_{\pm 5.0}$  & $85.8_{\pm 2.9}$ & $83.2_{\pm 5.0}$  & $81.9_{\pm 1.7}$  & $66.0_{\pm 2.6}$   & $83.2_{\pm 9.6}$ & $82.6_{\pm 4.7}$\\
            & umaze-diverse    & $\textbf{90.4}_{\pm 3.3}$ & $65.4_{\pm 3.7}$  & $81.6_{\pm 7.8}$ & $70.4_{\pm 5.6}$  & $75.4_{\pm 3.5}$  & $58.4_{\pm 2.4}$   & $77.8_{\pm 8.8}$ & $89.4_{\pm 2.4}$\\
Fixed Goal  & medium-play     & $\textbf{86.6}_{\pm 6.4}$  & $61.8_{\pm 9.2}$  & $50.4_{\pm 7.0}$ & $64.4_{\pm 5.2}$  & $71.5_{\pm 5.2}$  & $27.0_{\pm 4.5}$   & $66.0_{\pm 8.0}$ & $73.1_{\pm 4.5}$\\
            & medium-diverse   & $\textbf{87.4}_{\pm 5.7}$ & $64.2_{\pm 3.7}$  & $46.4_{\pm 6.9}$ & $63.0_{\pm 7.8}$  & $72.5_{\pm 2.8}$  & $32.4_{\pm 4.1}$   & $52.6_{\pm 9.2}$ & $64.8_{\pm 2.6}$\\
            & large-play      & $\textbf{58.6}_{\pm 6.2}$  & $30.6_{\pm 7.6}$  & $30.4_{\pm 6.2}$ & $20.4_{\pm 5.8}$  & $41.6_{\pm 6.0}$  & $8.6_{\pm 4.2}$    & $30.6_{\pm 6.0}$ & $57.9_{\pm 3.6}$\\
            & large-diverse    & $\textbf{52.0}_{\pm 8.6}$ & $32.6_{\pm 8.5}$  & $29.8_{\pm 5.8}$  & $25.2_{\pm 6.7}$  & $49.3_{\pm 6.3}$  & $8.8_{\pm 3.6}$    & $39.8_{\pm 8.9}$ & $43.7_{\pm 1.3}$\\
\midrule
            &umaze           &$\textbf{75.6}_{\pm 5.1}$& $61.2_{\pm 3.1}$&$74.4_{\pm 7.5}$& $67.4_{\pm 3.0}$&$60.4_{\pm 3.2}$&$58.3_{\pm 2.5}$& $63.0_{\pm 5.5}$& -\phantom{---} \\
            &umaze-diverse   &$\textbf{75.2}_{\pm 4.6}$&$58.2_{\pm 5.1}$&$74.0_{\pm 6.3}$&$60.2_{\pm 3.6}$&$49.6_{\pm 4.6}$&$54.6_{\pm 5.2}$&$54.8_{\pm 2.5}$& -\phantom{---} \\
Diverse Goal&medium-play     & $\textbf{70.6}_{\pm 2.6}$ &$45.4_{\pm 5.4}$&$39.2_{\pm 5.2}$&$43.6_{\pm 6.6}$&$26.6_{\pm 2.4}$&$33.6_{\pm 2.6}$& $38.0_{\pm 2.4}$&-\phantom{---} \\
            &medium-diverse  &$\textbf{72.8}_{\pm 3.1}$&$43.2_{\pm 4.3}$&$38.4_{\pm 5.7}$&$49.6_{\pm 5.4}$&$28.6_{\pm 2.8}$&$34.4_{\pm 2.0}$&$45.6_{\pm 5.2}$&-\phantom{---} \\
            &large-play      & $\textbf{44.2}_{\pm 4.3}$ &$35.0_{\pm 6.4}$&$31.4_{\pm 2.4}$&$41.2_{\pm 2.5}$&$24.6_{\pm 5.3}$&$18.3_{\pm 7.5}$&$31.4_{\pm 3.2}$&-\phantom{---} \\
            &large-diverse   & $\textbf{43.8}_{\pm 4.6}$&$35.4_{\pm 2.8}$&$28.3_{\pm 2.8}$&$40.4_{\pm 5.6}$&$26.0_{\pm 2.4}$&$24.6_{\pm 4.1}$&$33.6_{\pm 5.9}$&-\phantom{---} \\
\bottomrule
\end{tabular}
\end{minipage}
\end{sidewaystable}

To appreciate how state space partitioning works, we provide examples of valued-based partition for the grid worlds environments in Figure \ref{exp:grid_plot}. In these cases, the enviromental states simply correspond to locations in the grid. Here, the state space is divided with darker colours indicating higher values. As expected, these figures clearly show that states can be ordered based on the estimated value function, and that higher-valued states are those close to the goal. We also report the average return across five runs in Table \ref{exp:grid_res} where we compare DAWOG against GCSL and g-MAR. - two algorithms that are easily adapted for discrete action spaces.  
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp_fig/grid_partition/umaze_value88.png} \subcaption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp_fig/grid_partition/umaze_value102.png} \subcaption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp_fig/grid_partition/wall_value126.png} \subcaption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp_fig/grid_partition/wall_value613.png} \subcaption{}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp_fig/grid_partition/umaze_partition88.png} \subcaption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp_fig/grid_partition/umaze_partition102.png} \subcaption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp_fig/grid_partition/wall_partition126.png} \subcaption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{figures/exp_fig/grid_partition/wall_partition613.png} \subcaption{}
    \end{subfigure}
    \caption{An illustration of goal-conditioned state space partitions for two simple Grid World navigation tasks. In each instance, the desired goal is represented by a red circle. In these environments, each state simply corresponds to a position on the grid and, in the top row, is colour-coded according to its goal-conditional value. In the lower row, states sharing similar values have been merged to form a partition. For any given state, the proposed target region advantage up-weights actions that move the agent directly towards a neighboring region with higher-value.}
    \label{exp:grid_plot}
\end{figure}

% comparison of performance results 
The results for Gym Robotics suite are shown in Table \ref{exp:gym_res}. For each algorithm, we report the average return and the standard deviation obtained from $5$ independent runs, each one using a different seed. As can be seen from the results, most of the competing algorithms reach a comparable performance with DAWOG. However, DAWOG generally achieves higher scores and the most stable performance in different tasks.

Analogous results for the AntMaze suite are shown in Table \ref{exp:antmaze_res}. Here, DAWOG outperforms all baselines as these are long-horizon and significantly more challenging environments. In diverse goal setting all algorithms under-perform, but DAWOG still achieves the highest average score. This setup  requires better generalisation performance given that the test goals are sampled from every position within the maze. 
% qualitative analysis of the trajectories 
To gain an appreciation for the benefits introduced by the target region approach, in Figure \ref{fig:traj_visual_intro} we visualise $100$ trajectories realised by three different policies for AntMaze tasks: dual-advantage weighting  (DAWOG), equally-weighting and goal-conditioned advantage weighting. The trajectories generated by equally-weighting occasionally lead to regions in the maze that should have been avoided, which results in sub-optimal solutions. The policy from goal-conditioned advantage weighting is occasionally less prone to making the same mistakes, although it still suffers from the multi-modality problem. This can be appreciated, for instance, by observing the \texttt{antmaze-medium} case. In contrast, DAWOG is generally able to reach the goal with fewer detours, hence in a shorter amount of time.   

%The state and the goal are two-dimensional coordinates, so we can visualise the state space partition in Figure \ref{exp:grid_plot}. The experiment results on accumulative returns are shown in Table \ref{exp:grid_res}. 

\subsection{Ablation studies} \label{further_studies}

In this Section we take a closer look at how the two advantage-based weights featuring in Eq. \ref{eq:weight}  perform, both separately and jointly taken, when used in the loss of Eq. \ref{eq:policy}. We compare learning curves, region occupancy times (i.e. time spent in each region of the state space whilst reaching the goal), and potential overestimation biases.  

\subsubsection{Learning curves} \label{learning_curves}

\begin{figure}
    \centering
    \includegraphics[width=0.96\textwidth]{figures/exp_fig/weighting_strategy.png}
    \caption{Training curves for different tasks using different algorithms, each one implementing a different weighting scheme: dual-advantage, no advantage, only goal-conditioned advantage, and only the target region advantage.} 
    \label{exp:ablation}
\end{figure}

In the AntMaze environments, we train DAWOG using no advantage ($\beta=\tilde{\beta}=0$), only the goal-conditioned advantage ($\beta=10, \tilde{\beta}=0$), only the target region advantage ($\beta=0, \tilde{\beta}=10$), and the proposed dual-advantage ($\beta=\tilde{\beta}=10$). 
Figure \ref{exp:ablation} presents the corresponding learning curves over $50,000$ gradient updates. Both the goal-advantage and region-based advantage perform better than using no advantage, and their performance is generally comparable, with the latter often achieving higher normalised returns. Combining the two advantages leads to significantly higher returns than any advantage weight individually taken. 

\subsubsection{Region occupancy times}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/exp_fig/region_steps/med_play_bar.png}
    \includegraphics[width=0.45\textwidth]{figures/exp_fig/region_steps/med_div_bar.png}
    \includegraphics[width=0.45\textwidth]{figures/exp_fig/region_steps/large_play_bar.png}
    \includegraphics[width=0.45\textwidth]{figures/exp_fig/region_steps/large_div_bar.png}
    \caption{ Average time spent in a region of the state space before moving on to the higher-ranking region ($K=50$) using a goal-conditioned value function for state partitioning. The $y$-axis indicates the average number of time steps (in log scale) spent in a region. The dual-advantage weighting scheme allows the agent to reach each subsequent target region more rapidly compared to the goal-conditioned advantage alone, which results in overall shorter time spent to reach the final goal.}
    \label{fig:steps_to_target_region}
\end{figure}

In this study, we set out to confirm that the dual-advantage weighting scheme results in a policy favouring actions leading to the next higher ranking target region rapidly, i.e. by reducing the occupancy time in each region.  Using the AntMaze environments, Figure \ref{fig:steps_to_target_region} shows the average time spent in a region of the state space partitioned with $K=50$ regions.  As shown here, the dual-advantage weighting allows the agent to reach the target (next) region in fewer time steps compared to the goal-conditioned advantage alone. As the ant interacts with the environment within an episode, the available time to complete the task decreases. Hence, as the ant progressively moves to higher ranking regions closer to the goal, the occupancy times decrease.  

\subsubsection{Over-estimation bias}  \label{bias}

\begin{figure} 
    \centering
    \includegraphics[width=0.9\textwidth]{figures/exp_fig/value_functions.png}
    \caption{ Estimation error of goal-conditioned and target region value functions in Grid World tasks.}
    \label{exp:estimation_bias}
\end{figure}

We assess the extent of potential overestimation errors affecting the two advantage weighting factors used in our method (see Eq. \ref{eq:weight}). This is done by studying the error that occurred in the estimation of the corresponding V-functions (see Eq. \ref{goal-advantage} and Eq. \ref{region-advantage}). Given a state $s$ and goal $g$, we compute the goal-conditioned V-value estimation error as $V_{\psi_1}(s, g) - V^{\pi}(s, g)$, where $V_{\psi_1}(s, g)$ is the parameterised function learnt by our algorithm and  $V^{\pi}(s, g)$ is an unbiased Monte-Carlo estimate of the goal-conditioned V-function's true value \cite{sutton2018reinforcement}. Since $V^{\pi}(s, g)$ represents the expected discounted return obtained by the underlying behaviour policy that generates the relabelled data, we use a policy pre-trained with the GCSL algorithm to generate $1,000$ trajectories to calculate the Monte-Carlo estimate (i.e. the average discounted return). Analogously, the target region V-value estimation error is $\tilde{V}_{\psi_2}(s, g, G(s, g)) - \tilde{V}^{\pi}(s, g, G(s,g))$. We use the learnt target region V-value function to calculate $\tilde{V}_{\psi_2}(s, g, G(s, g))$, and Monte-Carlo estimation to approximate $\tilde{V}^{\pi}(s, g, G(s,g))$. 

We present experimental results for the Grid World environment, which contains two layouts, i.e., grid-umaze and grid-wall.
For each layout, we randomly sample $s$ and $g$ uniformly within the entire maze and ensure that the number of regions separating them is uniformly distributed in $\{1, \ldots, K\}$. Then, for each $k$ in that range: 1) $1,000$ goal positions are sampled randomly within the whole layout; 2) for each goal position, the state space is partitioned according to $V_{\psi}(\cdot,g)$; and 3) a state is sampled randomly within the corresponding region. Since there may exist regions without any states, the observed total number of regions is smaller than $K=10$. The resulting estimation errors are shown in Fig. \ref{exp:estimation_bias}. As can be seen here, both the mean and standard deviation of the $\tilde{V}$-value errors are consistently smaller than those corresponding to the $V$-value errors. This indicates the target region value function is more robust against over-estimation bias, which may help improve the generalisation performance in out-of-distribution settings. 

\subsection{Sensitivity to hyperparameters} \label{sensitivity}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.60\linewidth]{figures/exp_fig/diff_k.png}
    \bigskip
    \caption{DAWOG's performance, evaluated on the AntMaze dataset, as a function of $K$, the number of state space partitions required to define the target regions. The box plot of the normalised return in AntMaze task is achieved by DAWOG in four settings when the target region size decreases ($K$ increases). We used five runs with different seeds. Best performance (highest average returns and lowest variability) was consistently achieved across all settings with around $K=50$ equally sized target regions.}
    \label{exp:diff_k}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/exp_fig/abl_betas.png}
    \bigskip
    \caption{DAWOG's performance, evaluated on six datasets, as a function of its two hyperparameters, $\beta$ and $\tilde{\beta}$, controlling the goal-conditioned and target-based exponential weights featuring in Equation \ref{eq:weight}. The performance metric is the average return across 5 runs. To produce the results presented in Table \ref{exp:antmaze_res}, we used a (potentially sub-optimal) fixed parameter combination: $\beta=10$ and $\tilde{\beta}=10$.}
    \label{exp:beta_study}
\end{figure}

Lastly, we examine the impact of the number of partitions ($K$) and the coefficients $\beta$ and $\tilde{\beta}$, which control the relative contribution of the two advantage functions on DAWOG's overall performance. In the AntMaze task, we report the distribution of normalized returns as $K$ increases. Figure \ref{exp:diff_k} reveals that an optimal parameter yielding high average returns with low variance often depends on the specific task and is likely influenced by the environment's complexity.

Figure \ref{exp:beta_study} illustrates DAWOG's performance as a function of $\beta$ and $\tilde{\beta}$. The plot demonstrates some minimal sensitivity to various parameter combinations but also exhibits a good degree of symmetry. In all our experiments, including those in Tables \ref{exp:antmaze_res} and \ref{exp:gym_res}, we opted for a shared value, $\beta=\tilde{\beta}=10$, rather than optimizing each parameter combination for each task. This choice suggests that strong performance can be achieved even without extensive hyperparameter optimization.

\section{Discussion and conclusions}

We propose a dual-advantage weighting scheme for supervised learning to address multi-modality and distribution shift challenges in goal-conditioned offline reinforcement learning (GCRL). The corresponding algorithm, DAWOG (Dual-Advantage Weighting for Offline Goal-conditioned learning), prioritizes actions that lead to higher-reward regions, introducing an additional source of inductive bias and enhancing the ability to generalize learned skills to novel goals. Theoretical support is provided by demonstrating that the derived policy is never inferior to the underlying behavior policy. Empirical evidence shows that DAWOG learns highly competitive policies and surpasses several existing offline algorithms on demanding goal-conditioned tasks. Furthermore, DAWOG is easy to implement and train, making it a valuable contribution to advancing our understanding of offline GCRL and its relationship with goal-conditioned supervised learning (GCSL).

Future developments can explore various aspects of the proposed approach. Firstly, our current method partitions states into equally-sized bins for the value function. Implementing an adaptive partitioning technique that does not assume equal bin sizes could provide finer control over state partition shapes (e.g., merging smaller regions into larger ones), potentially leading to further performance improvements.

Secondly, considering DAWOG's effectiveness in alleviating the multi-modality problem in offline GCRL, it may also benefit other GCRL approaches beyond advantage-weighted GCSL. Specifically, our method could extend to actor-critic-based offline GCRL, such as TD3-BC \cite{fujimoto2021minimalist} , which introduces a behavior cloning-based regularizer into the TD3 algorithm \cite{fujimoto2018addressing} to keep the policy closer to actions experienced in historical data. The dual-advantage weighting scheme could offer an alternative direction for developing a TD3-based algorithm for offline GCRL.

Lastly, given our method's ability to accurately weight actions, it might also facilitate exploration in online GCRL, potentially in combination with self-imitation learning  \cite{oh2018self, ferret2021self, li2022phasic}. For example, a recent study demonstrated that advantage-weighted supervised learning is a competitive method for learning from good experiences in GCRL settings \cite{li2022phasic}. These promising directions warrant further exploration.

\section*{Acknowledgements}

Giovanni Montana acknowledges support from a UKRI AI Turing Acceleration Fellowship (EPSRC EP/V024868/1).

% \bibliography{references}% common bib file

% \backmatter

% \begin{confidential}

% \newpage

% \section*{Statements and Declarations}

% \subsection*{Funding}
% Giovanni Montana acknowledges support from a UKRI AI Turing Acceleration Fellowship (EPSRC EP/V024868/1).

% \subsection*{Conflict of interest/Competing interests}
% No competing and financial interests to disclose.

% \subsection*{Ethics approval}
% Not applicable.

% \subsection*{Consent to participate}
% The authors give their consent to participate.

% \subsection*{Consent for publication}
% The authors give their consent for publication.

% \subsection*{Availability of data and materials}

% All the data will be made available upon paper publication.

% \subsection*{Availability of data and materials}

% All code will be made available upon paper publication.

% \subsection*{Authors' contributions}

% Authors' contributions follow the authors' order convention.

% \end{confidential}

\begin{thebibliography}{46}
% BibTex style file: bmc-mathphys.bst (version 2.1), 2014-07-24
\ifx \bisbn   \undefined \def \bisbn  #1{ISBN #1}\fi
\ifx \binits  \undefined \def \binits#1{#1}\fi
\ifx \bauthor  \undefined \def \bauthor#1{#1}\fi
\ifx \batitle  \undefined \def \batitle#1{#1}\fi
\ifx \bjtitle  \undefined \def \bjtitle#1{#1}\fi
\ifx \bvolume  \undefined \def \bvolume#1{\textbf{#1}}\fi
\ifx \byear  \undefined \def \byear#1{#1}\fi
\ifx \bissue  \undefined \def \bissue#1{#1}\fi
\ifx \bfpage  \undefined \def \bfpage#1{#1}\fi
\ifx \blpage  \undefined \def \blpage #1{#1}\fi
\ifx \burl  \undefined \def \burl#1{\textsf{#1}}\fi
\ifx \doiurl  \undefined \def \doiurl#1{\url{https://doi.org/#1}}\fi
\ifx \betal  \undefined \def \betal{\textit{et al.}}\fi
\ifx \binstitute  \undefined \def \binstitute#1{#1}\fi
\ifx \binstitutionaled  \undefined \def \binstitutionaled#1{#1}\fi
\ifx \bctitle  \undefined \def \bctitle#1{#1}\fi
\ifx \beditor  \undefined \def \beditor#1{#1}\fi
\ifx \bpublisher  \undefined \def \bpublisher#1{#1}\fi
\ifx \bbtitle  \undefined \def \bbtitle#1{#1}\fi
\ifx \bedition  \undefined \def \bedition#1{#1}\fi
\ifx \bseriesno  \undefined \def \bseriesno#1{#1}\fi
\ifx \blocation  \undefined \def \blocation#1{#1}\fi
\ifx \bsertitle  \undefined \def \bsertitle#1{#1}\fi
\ifx \bsnm \undefined \def \bsnm#1{#1}\fi
\ifx \bsuffix \undefined \def \bsuffix#1{#1}\fi
\ifx \bparticle \undefined \def \bparticle#1{#1}\fi
\ifx \barticle \undefined \def \barticle#1{#1}\fi
\bibcommenthead
\ifx \bconfdate \undefined \def \bconfdate #1{#1}\fi
\ifx \botherref \undefined \def \botherref #1{#1}\fi
\ifx \url \undefined \def \url#1{\textsf{#1}}\fi
\ifx \bchapter \undefined \def \bchapter#1{#1}\fi
\ifx \bbook \undefined \def \bbook#1{#1}\fi
\ifx \bcomment \undefined \def \bcomment#1{#1}\fi
\ifx \oauthor \undefined \def \oauthor#1{#1}\fi
\ifx \citeauthoryear \undefined \def \citeauthoryear#1{#1}\fi
\ifx \endbibitem  \undefined \def \endbibitem {}\fi
\ifx \bconflocation  \undefined \def \bconflocation#1{#1}\fi
\ifx \arxivurl  \undefined \def \arxivurl#1{\textsf{#1}}\fi
\csname PreBibitemsHook\endcsname

%%% 1
\bibitem{liu2022goal}
\begin{bchapter}
\bauthor{\bsnm{Liu}, \binits{M.}},
\bauthor{\bsnm{Zhu}, \binits{M.}},
\bauthor{\bsnm{Zhang}, \binits{W.}}:
\bctitle{Goal-conditioned Reinforcement Learning: Problems and Solutions}.
In: \bbtitle{International Joint Conference on Artificial Intelligence}
(\byear{2022})
\end{bchapter}
\endbibitem

%%% 2
\bibitem{plappert2018multi}
\begin{botherref}
\oauthor{\bsnm{Plappert}, \binits{M.}},
\oauthor{\bsnm{Andrychowicz}, \binits{M.}},
\oauthor{\bsnm{Ray}, \binits{A.}},
\oauthor{\bsnm{McGrew}, \binits{B.}},
\oauthor{\bsnm{Baker}, \binits{B.}},
\oauthor{\bsnm{Powell}, \binits{G.}},
\oauthor{\bsnm{Schneider}, \binits{J.}},
\oauthor{\bsnm{Tobin}, \binits{J.}},
\oauthor{\bsnm{Chociej}, \binits{M.}},
\oauthor{\bsnm{Welinder}, \binits{P.}},
\oauthor{\bsnm{Kumar}, \binits{V.}},
\oauthor{\bsnm{Zaremba}, \binits{W.}}:
Multi-goal Reinforcement Learning: Challenging Robotics Environments and
  Request for Research
(2018)
\end{botherref}
\endbibitem

%%% 3
\bibitem{andrychowicz2017hindsight}
\begin{bchapter}
\bauthor{\bsnm{Andrychowicz}, \binits{M.}},
\bauthor{\bsnm{Wolski}, \binits{F.}},
\bauthor{\bsnm{Ray}, \binits{A.}},
\bauthor{\bsnm{Schneider}, \binits{J.}},
\bauthor{\bsnm{Fong}, \binits{R.}},
\bauthor{\bsnm{Welinder}, \binits{P.}},
\bauthor{\bsnm{McGrew}, \binits{B.}},
\bauthor{\bsnm{Tobin}, \binits{J.}},
\bauthor{\bsnm{Abbeel}, \binits{P.}},
\bauthor{\bsnm{Zaremba}, \binits{W.}}:
\bctitle{Hindsight Experience Replay}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2017})
\end{bchapter}
\endbibitem

%%% 4
\bibitem{eysenbach2022contrastive}
\begin{bchapter}
\bauthor{\bsnm{Eysenbach}, \binits{B.}},
\bauthor{\bsnm{Zhang}, \binits{T.}},
\bauthor{\bsnm{Levine}, \binits{S.}},
\bauthor{\bsnm{Salakhutdinov}, \binits{R.}}:
\bctitle{Contrastive Learning as Goal-conditioned Reinforcement Learning}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2022})
\end{bchapter}
\endbibitem

%%% 5
\bibitem{mezghani2022learning}
\begin{bchapter}
\bauthor{\bsnm{Mezghani}, \binits{L.}},
\bauthor{\bsnm{Sukhbaatar}, \binits{S.}},
\bauthor{\bsnm{Bojanowski}, \binits{P.}},
\bauthor{\bsnm{Lazaric}, \binits{A.}},
\bauthor{\bsnm{Alahari}, \binits{K.}}:
\bctitle{Learning Goal-conditioned Policies Offline with Self-supervised Reward
  Shaping}.
In: \bbtitle{Conference on Robot Learning}
(\byear{2022})
\end{bchapter}
\endbibitem

%%% 6
\bibitem{chebotar2021actionable}
\begin{bchapter}
\bauthor{\bsnm{Chebotar}, \binits{Y.}},
\bauthor{\bsnm{Hausman}, \binits{K.}},
\bauthor{\bsnm{Lu}, \binits{Y.}},
\bauthor{\bsnm{Xiao}, \binits{T.}},
\bauthor{\bsnm{Kalashnikov}, \binits{D.}},
\bauthor{\bsnm{Varley}, \binits{J.}},
\bauthor{\bsnm{Irpan}, \binits{A.}},
\bauthor{\bsnm{Eysenbach}, \binits{B.}},
\bauthor{\bsnm{Julian}, \binits{R.}},
\bauthor{\bsnm{Finn}, \binits{C.}},
\bauthor{\bsnm{Levine}, \binits{S.}}:
\bctitle{Actionable Models: Unsupervised Offline Reinforcement Learning of
  Robotic Skills}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2021})
\end{bchapter}
\endbibitem

%%% 7
\bibitem{ghosh2021learning}
\begin{bchapter}
\bauthor{\bsnm{Ghosh}, \binits{D.}},
\bauthor{\bsnm{Gupta}, \binits{A.}},
\bauthor{\bsnm{Reddy}, \binits{A.}},
\bauthor{\bsnm{Fu}, \binits{J.}},
\bauthor{\bsnm{Devin}, \binits{C.M.}},
\bauthor{\bsnm{Eysenbach}, \binits{B.}},
\bauthor{\bsnm{Levine}, \binits{S.}}:
\bctitle{Learning to Reach Goals via Iterated Supervised Learning}.
In: \bbtitle{International Conference on Learning Representations}
(\byear{2021})
\end{bchapter}
\endbibitem

%%% 8
\bibitem{emmons2022rvs}
\begin{bchapter}
\bauthor{\bsnm{Emmons}, \binits{S.}},
\bauthor{\bsnm{Eysenbach}, \binits{B.}},
\bauthor{\bsnm{Kostrikov}, \binits{I.}},
\bauthor{\bsnm{Levine}, \binits{S.}}:
\bctitle{RvS: What is Essential for Offline RL via Supervised Learning?}
In: \bbtitle{International Conference on Learning Representations}
(\byear{2022})
\end{bchapter}
\endbibitem

%%% 9
\bibitem{yang2022rethinking}
\begin{bchapter}
\bauthor{\bsnm{Yang}, \binits{R.}},
\bauthor{\bsnm{Lu}, \binits{Y.}},
\bauthor{\bsnm{Li}, \binits{W.}},
\bauthor{\bsnm{Sun}, \binits{H.}},
\bauthor{\bsnm{Fang}, \binits{M.}},
\bauthor{\bsnm{Du}, \binits{Y.}},
\bauthor{\bsnm{Li}, \binits{X.}},
\bauthor{\bsnm{Han}, \binits{L.}},
\bauthor{\bsnm{Zhang}, \binits{C.}}:
\bctitle{Rethinking Goal-conditioned Supervised Learning and its Connection to
  Offline {RL}}.
In: \bbtitle{International Conference on Learning Representations}
(\byear{2022})
\end{bchapter}
\endbibitem

%%% 10
\bibitem{fu2020d4rl}
\begin{botherref}
\oauthor{\bsnm{Fu}, \binits{J.}},
\oauthor{\bsnm{Kumar}, \binits{A.}},
\oauthor{\bsnm{Nachum}, \binits{O.}},
\oauthor{\bsnm{Tucker}, \binits{G.}},
\oauthor{\bsnm{Levine}, \binits{S.}}:
D4RL: Datasets for Deep Data-Driven Reinforcement Learning
(2020)
\end{botherref}
\endbibitem

%%% 11
\bibitem{fang2019curriculum}
\begin{bchapter}
\bauthor{\bsnm{Fang}, \binits{M.}},
\bauthor{\bsnm{Zhou}, \binits{T.}},
\bauthor{\bsnm{Du}, \binits{Y.}},
\bauthor{\bsnm{Han}, \binits{L.}},
\bauthor{\bsnm{Zhang}, \binits{Z.}}:
\bctitle{Curriculum-guided Hindsight Experience Replay}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2019})
\end{bchapter}
\endbibitem

%%% 12
\bibitem{yang2021mher}
\begin{bchapter}
\bauthor{\bsnm{Yang}, \binits{R.}},
\bauthor{\bsnm{Fang}, \binits{M.}},
\bauthor{\bsnm{Han}, \binits{L.}},
\bauthor{\bsnm{Du}, \binits{Y.}},
\bauthor{\bsnm{Luo}, \binits{F.}},
\bauthor{\bsnm{Li}, \binits{X.}}:
\bctitle{MHER: Model-based Hindsight Experience Replay}.
In: \bbtitle{Deep RL Workshop NeurIPS 2021}
(\byear{2021})
\end{bchapter}
\endbibitem

%%% 13
\bibitem{jurgenson2020sub}
\begin{bchapter}
\bauthor{\bsnm{Jurgenson}, \binits{T.}},
\bauthor{\bsnm{Avner}, \binits{O.}},
\bauthor{\bsnm{Groshev}, \binits{E.}},
\bauthor{\bsnm{Tamar}, \binits{A.}}:
\bctitle{Sub-goal Trees: a Framework for Goal-based Reinforcement Learning}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2020})
\end{bchapter}
\endbibitem

%%% 14
\bibitem{nasiriany2019planning}
\begin{bchapter}
\bauthor{\bsnm{Nasiriany}, \binits{S.}},
\bauthor{\bsnm{Pong}, \binits{V.}},
\bauthor{\bsnm{Lin}, \binits{S.}},
\bauthor{\bsnm{Levine}, \binits{S.}}:
\bctitle{Planning with Goal-conditioned Policies}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2019})
\end{bchapter}
\endbibitem

%%% 15
\bibitem{nair2018visual}
\begin{bchapter}
\bauthor{\bsnm{Nair}, \binits{A.V.}},
\bauthor{\bsnm{Pong}, \binits{V.}},
\bauthor{\bsnm{Dalal}, \binits{M.}},
\bauthor{\bsnm{Bahl}, \binits{S.}},
\bauthor{\bsnm{Lin}, \binits{S.}},
\bauthor{\bsnm{Levine}, \binits{S.}}:
\bctitle{Visual Reinforcement Learning with Imagined Goals}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2018})
\end{bchapter}
\endbibitem

%%% 16
\bibitem{durugkar2021adversarial}
\begin{bchapter}
\bauthor{\bsnm{Durugkar}, \binits{I.}},
\bauthor{\bsnm{Tec}, \binits{M.}},
\bauthor{\bsnm{Niekum}, \binits{S.}},
\bauthor{\bsnm{Stone}, \binits{P.}}:
\bctitle{Adversarial Intrinsic Motivation for Reinforcement Learning}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2021})
\end{bchapter}
\endbibitem

%%% 17
\bibitem{charlesworth2020plangan}
\begin{bchapter}
\bauthor{\bsnm{Charlesworth}, \binits{H.}},
\bauthor{\bsnm{Montana}, \binits{G.}}:
\bctitle{PlanGAN: Model-based Planning with Sparse Rewards and Multiple Goals}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2020})
\end{bchapter}
\endbibitem

%%% 18
\bibitem{kumar2020conservative}
\begin{bchapter}
\bauthor{\bsnm{Kumar}, \binits{A.}},
\bauthor{\bsnm{Zhou}, \binits{A.}},
\bauthor{\bsnm{Tucker}, \binits{G.}},
\bauthor{\bsnm{Levine}, \binits{S.}}:
\bctitle{Conservative Q-learning for Offline Reinforcement Learning}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2020})
\end{bchapter}
\endbibitem

%%% 19
\bibitem{fujimoto2021minimalist}
\begin{bchapter}
\bauthor{\bsnm{Fujimoto}, \binits{S.}},
\bauthor{\bsnm{Gu}, \binits{S.}}:
\bctitle{A Minimalist Approach to Offline Reinforcement Learning}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2021})
\end{bchapter}
\endbibitem

%%% 20
\bibitem{ding2019goal}
\begin{bchapter}
\bauthor{\bsnm{Ding}, \binits{Y.}},
\bauthor{\bsnm{Florensa}, \binits{C.}},
\bauthor{\bsnm{Abbeel}, \binits{P.}},
\bauthor{\bsnm{Phielipp}, \binits{M.}}:
\bctitle{Goal-conditioned Imitation Learning}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2019})
\end{bchapter}
\endbibitem

%%% 21
\bibitem{ho2016generative}
\begin{bchapter}
\bauthor{\bsnm{Ho}, \binits{J.}},
\bauthor{\bsnm{Ermon}, \binits{S.}}:
\bctitle{Generative Adversarial Imitation Learning}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2016})
\end{bchapter}
\endbibitem

%%% 22
\bibitem{bain1995framework}
\begin{bchapter}
\bauthor{\bsnm{Bain}, \binits{M.}},
\bauthor{\bsnm{Sammut}, \binits{C.}}:
\bctitle{A Framework for Behavioural Cloning.}
In: \bbtitle{Machine Intelligence 15},
pp. \bfpage{103}--\blpage{129}
(\byear{1995})
\end{bchapter}
\endbibitem

%%% 23
\bibitem{peng19advantage}
\begin{botherref}
\oauthor{\bsnm{Peng}, \binits{X.B.}},
\oauthor{\bsnm{Kumar}, \binits{A.}},
\oauthor{\bsnm{Zhang}, \binits{G.}},
\oauthor{\bsnm{Levine}, \binits{S.}}:
Advantage-weighted Regression: Simple and Scalable Off-policy Reinforcement
  Learning
(2019)
\end{botherref}
\endbibitem

%%% 24
\bibitem{wang2018exponentially}
\begin{bchapter}
\bauthor{\bsnm{Wang}, \binits{Q.}},
\bauthor{\bsnm{Xiong}, \binits{J.}},
\bauthor{\bsnm{Han}, \binits{L.}},
\bauthor{\bsnm{sun}, \binits{p.}},
\bauthor{\bsnm{Liu}, \binits{H.}},
\bauthor{\bsnm{Zhang}, \binits{T.}}:
\bctitle{Exponentially Weighted Imitation Learning for Batched Historical
  Data}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2018})
\end{bchapter}
\endbibitem

%%% 25
\bibitem{li2022hierarchical}
\begin{botherref}
\oauthor{\bsnm{Li}, \binits{J.}},
\oauthor{\bsnm{Tang}, \binits{C.}},
\oauthor{\bsnm{Tomizuka}, \binits{M.}},
\oauthor{\bsnm{Zhan}, \binits{W.}}:
Hierarchical Planning through Goal-conditioned Offline Reinforcement Learning
(2022)
\end{botherref}
\endbibitem

%%% 26
\bibitem{chane2021goal}
\begin{bchapter}
\bauthor{\bsnm{Chane-Sane}, \binits{E.}},
\bauthor{\bsnm{Schmid}, \binits{C.}},
\bauthor{\bsnm{Laptev}, \binits{I.}}:
\bctitle{Goal-conditioned Reinforcement Learning with Imagined Subgoals}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2021})
\end{bchapter}
\endbibitem

%%% 27
\bibitem{kim2021landmarkguided}
\begin{bchapter}
\bauthor{\bsnm{Kim}, \binits{J.}},
\bauthor{\bsnm{Seo}, \binits{Y.}},
\bauthor{\bsnm{Shin}, \binits{J.}}:
\bctitle{Landmark-guided Subgoal Generation in Hierarchical Reinforcement
  Learning}.
In: \bbtitle{Advances in Neural Information Processing Systems}
(\byear{2021})
\end{bchapter}
\endbibitem

%%% 28
\bibitem{zhang2021worldmodel}
\begin{bchapter}
\bauthor{\bsnm{Zhang}, \binits{L.}},
\bauthor{\bsnm{Yang}, \binits{G.}},
\bauthor{\bsnm{Stadie}, \binits{B.C.}}:
\bctitle{World Model as a Graph: Learning Latent Landmarks for Planning}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2021})
\end{bchapter}
\endbibitem

%%% 29
\bibitem{ma2020clustered}
\begin{botherref}
\oauthor{\bsnm{Ma}, \binits{X.}},
\oauthor{\bsnm{Zhao}, \binits{S.-Y.}},
\oauthor{\bsnm{Yin}, \binits{Z.-H.}},
\oauthor{\bsnm{Li}, \binits{W.-J.}}:
Clustered Reinforcement Learning
(2020)
\end{botherref}
\endbibitem

%%% 30
\bibitem{wei2018learning}
\begin{bchapter}
\bauthor{\bsnm{Wei}, \binits{H.}},
\bauthor{\bsnm{Corder}, \binits{K.}},
\bauthor{\bsnm{Decker}, \binits{K.}}:
\bctitle{Q-learning Acceleration via State-space Partitioning}.
In: \bbtitle{International Conference on Machine Learning and Applications}
(\byear{2018})
\end{bchapter}
\endbibitem

%%% 31
\bibitem{karimpanal2017identification}
\begin{barticle}
\bauthor{\bsnm{Karimpanal}, \binits{T.G.}},
\bauthor{\bsnm{Wilhelm}, \binits{E.}}:
\batitle{Identification and Off-policy Learning of Multiple Objectives using
  Adaptive Clustering}.
\bjtitle{Neurocomputing}
\bvolume{263},
\bfpage{39}--\blpage{47}
(\byear{2017})
\end{barticle}
\endbibitem

%%% 32
\bibitem{mannor2004dynamic}
\begin{bchapter}
\bauthor{\bsnm{Mannor}, \binits{S.}},
\bauthor{\bsnm{Menache}, \binits{I.}},
\bauthor{\bsnm{Hoze}, \binits{A.}},
\bauthor{\bsnm{Klein}, \binits{U.}}:
\bctitle{Dynamic Abstraction in Reinforcement Learning via Clustering}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2004})
\end{bchapter}
\endbibitem

%%% 33
\bibitem{ghosh2018divide}
\begin{bchapter}
\bauthor{\bsnm{Ghosh}, \binits{D.}},
\bauthor{\bsnm{Singh}, \binits{A.}},
\bauthor{\bsnm{Rajeswaran}, \binits{A.}},
\bauthor{\bsnm{Kumar}, \binits{V.}},
\bauthor{\bsnm{Levine}, \binits{S.}}:
\bctitle{Divide-and-conquer Reinforcement Learning}.
In: \bbtitle{International Conference on Learning Representations}
(\byear{2018})
\end{bchapter}
\endbibitem

%%% 34
\bibitem{kaelbling1993learning}
\begin{bchapter}
\bauthor{\bsnm{Kaelbling}, \binits{L.P.}}:
\bctitle{Learning to Achieve Goals}.
In: \bbtitle{International Joint Conference on Artificial Intelligence}
(\byear{1993})
\end{bchapter}
\endbibitem

%%% 35
\bibitem{schaul2015universal}
\begin{bchapter}
\bauthor{\bsnm{Schaul}, \binits{T.}},
\bauthor{\bsnm{Horgan}, \binits{D.}},
\bauthor{\bsnm{Gregor}, \binits{K.}},
\bauthor{\bsnm{Silver}, \binits{D.}}:
\bctitle{Universal Value Function Approximators}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2015})
\end{bchapter}
\endbibitem

%%% 36
\bibitem{sutton2018reinforcement}
\begin{bbook}
\bauthor{\bsnm{Sutton}, \binits{R.S.}},
\bauthor{\bsnm{Barto}, \binits{A.G.}}:
\bbtitle{Reinforcement Learning: An Introduction}.
\bpublisher{MIT press},
\blocation{Cambridge, MA, USA}
(\byear{2018})
\end{bbook}
\endbibitem

%%% 37
\bibitem{mnih2015human}
\begin{barticle}
\bauthor{\bsnm{Mnih}, \binits{V.}},
\bauthor{\bsnm{Kavukcuoglu}, \binits{K.}},
\bauthor{\bsnm{Silver}, \binits{D.}},
\bauthor{\bsnm{Rusu}, \binits{A.A.}},
\bauthor{\bsnm{Veness}, \binits{J.}},
\bauthor{\bsnm{Bellemare}, \binits{M.G.}},
\bauthor{\bsnm{Graves}, \binits{A.}},
\bauthor{\bsnm{Riedmiller}, \binits{M.}},
\bauthor{\bsnm{Fidjeland}, \binits{A.K.}},
\bauthor{\bsnm{Ostrovski}, \binits{G.}}, \betal:
\batitle{Human-level Control through Deep Reinforcement Learning}.
\bjtitle{nature}
\bvolume{518}(\bissue{7540}),
\bfpage{529}--\blpage{533}
(\byear{2015})
\end{barticle}
\endbibitem

%%% 38
\bibitem{lillicrap2016continuous}
\begin{bchapter}
\bauthor{\bsnm{Lillicrap}, \binits{T.P.}},
\bauthor{\bsnm{Hunt}, \binits{J.J.}},
\bauthor{\bsnm{Pritzel}, \binits{A.}},
\bauthor{\bsnm{Heess}, \binits{N.M.O.}},
\bauthor{\bsnm{Erez}, \binits{T.}},
\bauthor{\bsnm{Tassa}, \binits{Y.}},
\bauthor{\bsnm{Silver}, \binits{D.}},
\bauthor{\bsnm{Wierstra}, \binits{D.}}:
\bctitle{Continuous Control with Deep Reinforcement Learning}.
In: \bbtitle{International Conference on Learning Representations}
(\byear{2016})
\end{bchapter}
\endbibitem

%%% 39
\bibitem{raffin2021stablebaselines}
\begin{barticle}
\bauthor{\bsnm{Raffin}, \binits{A.}},
\bauthor{\bsnm{Hill}, \binits{A.}},
\bauthor{\bsnm{Gleave}, \binits{A.}},
\bauthor{\bsnm{Kanervisto}, \binits{A.}},
\bauthor{\bsnm{Ernestus}, \binits{M.}},
\bauthor{\bsnm{Dormann}, \binits{N.}}:
\batitle{Stable-baselines 3: Reliable Reinforcement Learning Implementations}.
\bjtitle{Journal of Machine Learning Research}
\bvolume{22}(\bissue{268}),
\bfpage{1}--\blpage{8}
(\byear{2021})
\end{barticle}
\endbibitem

%%% 40
\bibitem{kingma2014adam}
\begin{botherref}
\oauthor{\bsnm{Kingma}, \binits{D.P.}},
\oauthor{\bsnm{Ba}, \binits{J.}}:
Adam: A Method for Stochastic Optimization
(2014)
\end{botherref}
\endbibitem

%%% 41
\bibitem{fujimoto2019off}
\begin{bchapter}
\bauthor{\bsnm{Fujimoto}, \binits{S.}},
\bauthor{\bsnm{Meger}, \binits{D.}},
\bauthor{\bsnm{Precup}, \binits{D.}}:
\bctitle{Off-policy Deep Reinforcement Learning without Exploration}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2019})
\end{bchapter}
\endbibitem

%%% 42
\bibitem{fujimoto2018addressing}
\begin{bchapter}
\bauthor{\bsnm{Fujimoto}, \binits{S.}},
\bauthor{\bsnm{Hoof}, \binits{H.}},
\bauthor{\bsnm{Meger}, \binits{D.}}:
\bctitle{Addressing Function Approximation Error in Actor-critic Methods}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2018})
\end{bchapter}
\endbibitem

%%% 43
\bibitem{mandlekar2020iris}
\begin{bchapter}
\bauthor{\bsnm{Mandlekar}, \binits{A.}},
\bauthor{\bsnm{Ramos}, \binits{F.}},
\bauthor{\bsnm{Boots}, \binits{B.}},
\bauthor{\bsnm{Savarese}, \binits{S.}},
\bauthor{\bsnm{Fei-Fei}, \binits{L.}},
\bauthor{\bsnm{Garg}, \binits{A.}},
\bauthor{\bsnm{Fox}, \binits{D.}}:
\bctitle{IRIS: Implicit Reinforcement without Interaction at Scale for Learning
  Control from Offline Robot Manipulation Data}.
In: \bbtitle{International Conference on Robotics and Automation}
(\byear{2020})
\end{bchapter}
\endbibitem

%%% 44
\bibitem{oh2018self}
\begin{bchapter}
\bauthor{\bsnm{Oh}, \binits{J.}},
\bauthor{\bsnm{Guo}, \binits{Y.}},
\bauthor{\bsnm{Singh}, \binits{S.}},
\bauthor{\bsnm{Lee}, \binits{H.}}:
\bctitle{Self-imitation Learning}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2018})
\end{bchapter}
\endbibitem

%%% 45
\bibitem{ferret2021self}
\begin{bchapter}
\bauthor{\bsnm{Ferret}, \binits{J.}},
\bauthor{\bsnm{Pietquin}, \binits{O.}},
\bauthor{\bsnm{Geist}, \binits{M.}}:
\bctitle{Self-imitation Advantage Learning}.
In: \bbtitle{International Conference on Autonomous Agents and Multiagent
  Systems}
(\byear{2021})
\end{bchapter}
\endbibitem

%%% 46
\bibitem{li2022phasic}
\begin{bchapter}
\bauthor{\bsnm{Li}, \binits{Y.}},
\bauthor{\bsnm{Gao}, \binits{T.}},
\bauthor{\bsnm{Yang}, \binits{J.}},
\bauthor{\bsnm{Xu}, \binits{H.}},
\bauthor{\bsnm{Wu}, \binits{Y.}}:
\bctitle{Phasic Self-imitative Reduction for Sparse-reward Goal-conditioned
  Reinforcement Learning}.
In: \bbtitle{International Conference on Machine Learning}
(\byear{2022})
\end{bchapter}
\endbibitem

\end{thebibliography}

\end{document}



