%%% Feature Sets 

\paragraph{Deep XGBoost ensembles with different feature sampling methods}

50 XGBoost models with 10 different feature sampling schemes. In each sampling scheme, we randomly sample $50\%$ features globally and then train 5 models with different random seeds. The train size is fixed at 585 and the models are retrained every 50 eras. 

Other hyper-parameters of the XGBoost models are set as follows. 

\begin{itemize}
    \item Grow policy: Depth-wise
    %\item Number of boosting rounds: 5000
    %\item Early Stopping 250
    %\item Learning rate: 0.01
    \item Max Depth: 4 
    %\item Max Leaves: 128 
    \item Min Samples per node: 10 
    \item Data subsample: 0.75
    \item Feature subsample by tree: 0.75
    \item Feature subsample by level/node: 1
    \item L1 regularisation: 0
    \item L2 regularisation: 0 
\end{itemize}

A deep XGBoost model is created by combining the 50 models incrementally. For every 10th era, a shallow XGBoost model with the following hyper-parameters is trained using model predictions from the most recent 185 eras (with suitable data lag). Monotonic constraints are imposed so that we do not assign negative weights to the model predictions. This is repeated over 5 times over different random seeds.

%% Layer 2 XGBoost 
\begin{itemize}
    \item Grow policy: Depth-wise
    \item Number of boosting rounds: 20
    \item Early Stopping 20
    \item Learning rate: 0.1
    \item Max Depth: 4 
    %\item Max Leaves: 128 
    \item Min Samples per node: 10 
    \item Data subsample: 0.75
    \item Feature subsample by tree: 0.75
    \item Feature subsample by level/node: 1
    \item L1 regularisation: 0
    \item L2 regularisation: 0 
\end{itemize}


XGBoost models are trained with early-stopping for different number of boosting rounds and learning rates for 5 different random seeds. Three scenarios are considered: 
\begin{itemize}
    \item Small:    XGBoost models with 500 boosting rounds and learning rate 0.1, early-stopping at 25 rounds
    \item Standard: XGBoost models with 5000 boosting rounds and learning rate 0.01, early-stopping at 250 rounds
    \item Large:    XGBoost models with 50000 boosting rounds and learning rate 0.001, early-stopping at 2500 rounds
\end{itemize}
The 10 features sets are fixed across the scenarios.


In tables \ref{table:XGBDeep500}, \ref{table:XGBDeep5000} and \ref{table:XGBDeep50000}, the performance of XGBoost models in Layer 1 of each sampling method are reported with the average over all the sampling methods (Average). The test period reported is Era 801 to Era 1050. We compare the deep XGBoost models in Layer 2 (Deep Incremental) with dynamic model selection from our previous study \cite{wong2023online,} (Dynamic Best), which selected the top 10 models (out of 50) based on recent performances. The criteria to select models is based on the highest Exponential Weighted Moving Averages (EWMA) Mean Corr with a weight decay of 0.001. Each selected top models have equal contribution. 


Studying the joint effect of random feature selection and random seeds in training, we found that for some feature sets (Feature Set 3 for standard and large XGBoost models) have a much higher variance across random seeds than other feature sets. This effect is more readily observed for large XGBoost models, where all feature sets except Feature Set 3 have a variance smaller than 0.002 with the variance from Feature Set 3 being an outlier (0.0015). Therefore, it is important to repeat experiments using different random seeds over different steps of model learning. 

The feature sets with good performances for XGBoost models from smaller sizes does not generalise towards XGBoost models of larger sizes. The top 3 sets of small XGBoost models are from Feature Set 3,8,9. The top 3 sets of standard XGBoost models are from Feature Set 1,4,9. The top 3 sets of large XGBoost models are from Feature Set 1,4,9. The alignment of ranking of feature sets between standard and large XGBoost models are higher due to convergence of training process, as demonstrated in Figure \ref{figure/chapter3/XGBSnapshotPlot.pdf}. It demonstrated the joint effects between feature selection and training process. Variance between models with different feature sampling schemes decreases as the size of XGBoost models increases as expected. The small XGBoost models have the highest variance between feature sets (0.001), where the standard and large XGBoost models have a lower variance between feature sets (0.007,???). 

Dynamic model selection created ensembles with a better Sharpe and Calmar ratio with Mean Corr not much significant different from the average over all feature sets. Dynamic model selection provided good risk adjusted return but being over conservative in the test period.

Deep incremental learning improves all three performances measures (Mean Corr, Sharpe and Calmar ratio) in the three scenarios. Deep incremental learning models have a higher Mean Corr than dynamic model selection in all three scenarios. There are no significant differences of the Sharpe and Calmar ratios between deep incremental learning models and dynamically selected models. 

Overall, deep incremental learning achieves a well-rounded improvement in both absolute and risk-adjusted(relative) performance measures. 


\begin{table}[!ht]
    \centering
    \caption{Small XGBoost models with different feature sampling schemes (500 boosting rounds) }
    \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
        Feature Set & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                    & mean      & std       & mean   & std    & mean   & std    \\ \hline
        1           & 0.0163    & 0.0009    & 0.8036 & 0.0735 & 0.1151 & 0.0392 \\ \hline
        2           & 0.0164    & 0.0009    & 0.8154 & 0.0331 & 0.114  & 0.0063 \\ \hline
        3           & 0.0167    & 0.0016    & 0.86   & 0.0998 & 0.1451 & 0.0509 \\ \hline
        4           & 0.019     & 0.001     & 0.9832 & 0.0992 & 0.1643 & 0.0449 \\ \hline
        5           & 0.0164    & 0.0019    & 0.8274 & 0.1067 & 0.1403 & 0.0396 \\ \hline
        6           & 0.0165    & 0.0011    & 0.8424 & 0.0654 & 0.1045 & 0.0351 \\ \hline
        7           & 0.0158    & 0.0014    & 0.8308 & 0.0988 & 0.1348 & 0.0528 \\ \hline
        8           & 0.0178    & 0.0015    & 0.9206 & 0.1097 & 0.1546 & 0.0451 \\ \hline
        9           & 0.0181    & 0.0012    & 0.9086 & 0.083  & 0.1977 & 0.1146 \\ \hline
        10          & 0.0162    & 0.0011    & 0.843  & 0.0533 & 0.1218 & 0.0235 \\ \hline
        Average          & 0.0169    & 0.0010    & 0.8635 & 0.0565 & 0.1392 & 0.0280 \\ \hline
        Deep Incremental & 0.0207    & 0.0003    & 1.0302 & 0.0076 & 0.165  & 0.0227 \\ \hline
        Dynamic Best     & 0.0171    & N.A.      & 1.0054 & N.A.   & 0.1918 & N.A.   \\ \hline
    \end{tabular}
    \label{table:XGBDeep500}
\end{table}


\begin{table}[!ht]
    \centering
    \caption{Standard XGBoost models with different feature sampling schemes (5000 boosting rounds) }
    \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
        Feature Set      & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                         & mean      & std       & mean   & std    & mean   & std    \\ \hline
        1                & 0.0227    & 0.0006    & 1.1156 & 0.0278 & 0.2247 & 0.017  \\ \hline
        2                & 0.0212    & 0.0009    & 1.0424 & 0.0658 & 0.211  & 0.0608 \\ \hline
        3                & 0.021     & 0.0017    & 1.0429 & 0.1162 & 0.2307 & 0.0537 \\ \hline
        4                & 0.0226    & 0.0004    & 1.1225 & 0.0254 & 0.1907 & 0.0222 \\ \hline
        5                & 0.0216    & 0.0004    & 1.078  & 0.0177 & 0.2884 & 0.0322 \\ \hline
        6                & 0.0217    & 0.0005    & 1.0912 & 0.0242 & 0.183  & 0.0189 \\ \hline
        7                & 0.0219    & 0.0006    & 1.1209 & 0.0413 & 0.2915 & 0.0742 \\ \hline
        8                & 0.0214    & 0.0002    & 1.0644 & 0.0177 & 0.2275 & 0.0276 \\ \hline
        9                & 0.0224    & 0.0007    & 1.1005 & 0.0388 & 0.2192 & 0.0459 \\ \hline
        10               & 0.0207    & 0.0005    & 1.0346 & 0.0403 & 0.2181 & 0.0449 \\ \hline
        Average          & 0.0217    & 0.0007    & 1.0813 & 0.0339 & 0.2285 & 0.0359 \\ \hline
        Deep Incremental & 0.0224    & 0.0002    & 1.1198 & 0.0090 & 0.2445 & 0.0113 \\ \hline
        Dynamic Best     & 0.0216    & N.A.      & 1.1272 & N.A.   & 0.2646 & N.A.   \\ \hline
    \end{tabular}
    \label{table:XGBDeep5000}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{Large XGBoost models with different feature sampling schemes (50000 boosting rounds) }
    \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
        Feature Set      & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                         & mean      & std       & mean   & std    & mean   & std    \\ \hline
        1                & 0.0233    & 0.0001    & 1.1449 & 0.0111 & 0.2751 & 0.0084 \\ \hline
        2                & 0.0216    & 0.0002    & 1.0725 & 0.0097 & 0.2696 & 0.0176 \\ \hline
        3                & 0.0219    & 0.0015    & 1.0778 & 0.1061 & 0.2692 & 0.0669 \\ \hline
        4                & 0.0236    & 0.0002    & 1.1762 & 0.0071 & 0.2598 & 0.009  \\ \hline
        5                & 0.0226    & 0.0002    & 1.1428 & 0.0073 & 0.2983 & 0.0143 \\ \hline
        6                & 0.0225    & 0.0002    & 1.1437 & 0.0121 & 0.2498 & 0.0258 \\ \hline
        7                & 0.0223    & 0.0001    & 1.1354 & 0.0093 & 0.3216 & 0.0143 \\ \hline
        8                & 0.022     & 0.0002    & 1.1019 & 0.0063 & 0.2541 & 0.0064 \\ \hline
        9                & 0.0234    & 0.0002    & 1.1601 & 0.0161 & 0.3194 & 0.0439 \\ \hline
        10               & 0.0213    & 0.0002    & 1.0768 & 0.0171 & 0.2857 & 0.0414 \\ \hline
        Average          & 0.0224    & 0.0008    & 1.1232 & 0.0378 & 0.2803 & 0.0256 \\ \hline
        Deep Incremental & 0.0228    & 0.0002    & 1.1608 & 0.0207 & 0.3492 & 0.0199 \\ \hline
        Dynamic Best     & 0.0225    & N.A.      & 1.1503 & N.A.   & 0.3173 & N.A.   \\ \hline
    \end{tabular}
    \label{table:XGBDeep50000}
\end{table}










\paragraph{Deep XGBoost ensembles with different model complexity} 


XGBoost models are trained without early-stopping for different number of boosting rounds and learning rates for 5 different random seeds. Three scenarios are considered: 
\begin{itemize}
    \item Small:    XGBoost models with 500 boosting rounds and learning rate 0.1
    \item Standard: XGBoost models with 5000 boosting rounds and learning rate 0.01
    \item Large:    XGBoost models with 50000 boosting rounds and learning rate 0.001 
\end{itemize}

As we are not using early-stopping in model training, it is not necessary to set $25\%$ of data as validation as above. For the given training set size of 585, we use the most recent $100\%$(all), $75\%$ and $50\%$ of the 585 eras of data to train XGBoost models. For each XGBoost model, 10 model snapshots are collected at regular intervals of the training process as above. The 150 predictions are then combined in the second layer of the deep incremental model. For every 10th era, a shallow XGBoost model with the following hyper-parameters is trained using model predictions from the most recent 185 eras (with suitable data lag). Monotonic constraints are imposed so that we do not assign negative weights to the model predictions. The hyper-parameters of the XGBoost models are as above. 

In tables \ref{table:XGBComplexity500},  \ref{table:XGBComplexity5000} and \ref{table:XGBComplexity50000}, the performance from the deep XGBoost model in Layer 2 (Deep Incremental) is compared with different XGBoost snapshots trained using different amount of training data ($50\%$,$75\%$,$100\%$) in Layer 1 (Base) in Era 801 to Era 1050. 


Using deep model ensemble can improve model performances for both small and standard XGBoost models. Improvement for small XGBoost models is more significant than standard XGBoost models. Variance of model performances are also significantly reduced. For standard XGBoost models, improvement on Mean Corr compared to models trained with all data in Layer 1 is not significant but the improvement on Calmar ratios are significant (p-value of t-test is 0.0039). The improvement from standard to large XGBoost models is not significant.  Figure \ref{fig:XGBSamplingPlot} shows that model performances converges after $80\%$ of boosting rounds are added for XGBoost models of different sizes (small,standard and large). Detailed model performance can be found in tables \ref{table:XGBComplexity500Full},  \ref{table:XGBComplexity5000Full} and \ref{table:XGBComplexity50000Full}.




\begin{table}[!ht]
    \centering
    \caption{Small XGBoost models with different model complexity (500 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        $50\%$   & $0.0126 \pm 0.0013$   & $0.6526 \pm 0.0714$  & $0.0999 \pm 0.0308$ \\ \hline
        $75\%$   & $0.0160 \pm 0.0016$   & $0.7795 \pm 0.0940$  & $0.1352 \pm 0.0452$ \\ \hline
        $100\%$  & $0.0199 \pm 0.0018$   & $0.9817 \pm 0.1212$  & $0.2110 \pm 0.0839$ \\ \hline
        Deep Incremental    & $0.0224 \pm 0.0002$   & $1.0531 \pm 0.0268$  & $0.3112 \pm 0.0089$ \\ \hline
    \end{tabular}
    \label{table:XGBComplexity500}
\end{table}


\begin{table}[!ht]
    \centering
    \caption{Standard XGBoost models with different model complexity (5000 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        $50\%$   & $0.0164 \pm 0.0011$   & $0.7498 \pm 0.0585$  & $0.1361 \pm 0.0363$ \\ \hline
        $75\%$   & $0.0191 \pm 0.0017$   & $0.8501 \pm 0.1011$  & $0.1642 \pm 0.0484$ \\ \hline
        $100\%$  & $0.0232 \pm 0.0016$   & $1.0280 \pm 0.0943$  & $0.2297 \pm 0.0848$ \\ \hline
        Deep Incremental     & $0.0235 \pm 0.0002$   & $1.0623 \pm 0.0103$  & $0.3455 \pm 0.0173$ \\ \hline
    \end{tabular}
    \label{table:XGBComplexity5000}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{Large XGBoost models with different model complexity (50000 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        $50\%$   & $0.0164 \pm 0.0008$   & $0.7460 \pm 0.0524$  & $0.1408 \pm 0.0307$ \\ \hline
        $75\%$   & $0.0195 \pm 0.0017$   & $0.8614 \pm 0.0976$  & $0.1619 \pm 0.0390$ \\ \hline
        $100\%$  & $0.0236 \pm 0.0015$   & $1.0410 \pm 0.0932$  & $0.2342 \pm 0.0761$ \\ \hline
        Deep Incremental     & $0.0238 \pm 0.0002$   & $1.0788 \pm 0.0076$  & $0.3422 \pm 0.0303$ \\ \hline
    \end{tabular}
    \label{table:XGBComplexity50000}
\end{table}


\begin{figure}
    \centering
    \includegraphics{figure/chapter3/XGBSamplingPlot.pdf}
    \caption{Mean Corr of XGBoost models trained with all 585 eras of data with regular updates every 50 eras during test period (Era 801 to Era 1050). 10 different model snapshots are taken for XGBoost models of different number of boosting rounds (500,5000,50000). Model are trained with 5 different random seeds. 95\% confidence intervals of Mean Corr for each model are shown in the plot.}
    \label{fig:XGBSamplingPlot}
\end{figure}
