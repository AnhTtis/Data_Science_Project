
\documentclass{article}
\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[a4paper, margin=2cm]{geometry}
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder


% Figures and Tables
\usepackage{graphicx}
\usepackage[most]{tcolorbox}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{longtable}


% Mathematics 
\usepackage{amsmath,amssymb,amsfonts}

\usepackage{textcomp}

\usepackage{verbatim}

\usepackage{xcolor}
\usepackage{url}
\usepackage{xr-hyper}
\usepackage{hyperref}
\usepackage{pdfpages} 
\usepackage{bm}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{array}
\newcolumntype{L}{>{\arraybackslash}m{3cm}}

\usepackage{fullpage}
\usepackage{rotating}
\usepackage{stmaryrd}
\usepackage{proof}


%% Tikz 
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{decorations.pathmorphing} % noisy shapes
\usetikzlibrary{fit}% fitting shapes to coordinates
\usetikzlibrary{backgrounds}	

% Theorem 
\usepackage{amsthm}
\usepackage{amsmath}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}


% Algorithms 
\usepackage[ruled]{algorithm2e}
\usepackage{algorithmic} %% Used in Chapter 1 


% References 
\usepackage[style=numeric,sorting=none,firstinits=true]{biblatex}
\addbibresource{references.bib}



%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{THOR Paper Series 2}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}

  
%% Title
\title{Deep incremental learning models for financial temporal tabular datasets with distribution shifts
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Citation}}: \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Thomas Wong \\
  Imperial College London \\
  London\\
  \texttt{mw4315@ic.ac.uk} \\
  %% examples of more authors
   \And
  Mauricio Barahona \\
  Imperial College London \\
  London\\
  \texttt{m.barahona@imperial.ac.uk} \\
}


\begin{document}
\maketitle


\begin{abstract}
We present a robust deep incremental learning framework for regression tasks on financial temporal tabular datasets which is built upon the incremental use of commonly available tabular and time series prediction models to adapt to distributional shifts typical of financial datasets.
%
The framework uses a simple basic building block (decision trees) to build self-similar models of any required complexity to deliver robust performance under adverse situations such as regime changes, fat-tailed distributions, and low signal-to-noise ratios. 
%
As a detailed study, we demonstrate our scheme using XGBoost models trained on the Numerai dataset and show that a two layer deep ensemble of XGBoost models over different model snapshots delivers high quality predictions under different market regimes. We also show that the performance  of XGBoost models with different number of boosting rounds in three scenarios (small, standard and large) is monotonically increasing with respect to model size and converges towards the generalisation upper bound. We also evaluate the robustness of the model under variability of different hyperparameters, such as model complexity and data sampling settings. Our model has low hardware requirements as no specialised neural architectures are used and each base model can be independently trained in parallel.
\end{abstract}



\keywords{Machine Learning, time series Prediction, Deep Learning, }



\section{Introduction} 
\label{section:overview}

%% At the request of reviwers, need to add more references for background
%\subsection{Background on incremental learning}


Many important applications of machine learning, such as the Internet of Things (IoT) \cite{song2018situ} and cyber-security \cite{buczak2015survey}, involve streams of data, where data (features) are continuously updated and predictions are made \textit{point-in-time}. Incremental learning \cite{belouadah2021comprehensive,wu2019large,van2022three} is used to adapt deployed machine learning systems to changes in data streams. For example, in image classification systems, class incremental learning \cite{zhu2021class,NEURIPS2022_c8ac22c0,NEURIPS2022_ae817e85} is used where the categories of images cannot be known in advance. A key challenge in incremental learning is therefore the presence of distributional shifts in data (or concept drifts) \cite{Gama14,Jie19} which results in model degradation \cite{bayram2022concept,} during inference, i.e., the drop of out-of-sample performance when the model learns relationships from the training set that significantly differ from those in the test set. 

Reinforcement learning \cite{arulkumaran2017deep,NEURIPS2022_d112fdd3,NEURIPS2022_eb4898d6,} is a related modelling approach where a model (agent) continuously learns a policy to optimise rewards by interacting with the environment. Reinforcement learning is particularly useful when the actions of the model can influence the environment and there are multiple agents interacting with each other \cite{NEURIPS2020_77441296,}. Under the assumption that the actions of models have no influence on the data stream (environment), reinforcement learning reduces to incremental learning when the reward of agents is not used as input features. In other words, the input features (environment) are \textbf{independent} to the actions of the agent(predictions of the model) as there is no complete feedback loop between the agent and environment. Applying trained reinforcement learning agents into unknown situations, such as trading \cite{deng2016deep}, perception in self-driving cars \cite{NEURIPS2021_0d5bd023} and robotics \cite{kober2013reinforcement} remains a great challenge. Domain adaption for reinforcement learning algorithms \cite{arndt2020meta,higgins2017darla,} aims to bridge the gap between controlled environments and real-life situations using different advanced and complex algorithms. Key challenges such as robustness of agent behaviour \cite{ma2018improved} and interpretability \cite{mott2019towards,} remains unsolved. Another limitation in reinforcement learning is the much larger amount of computational resources required, compared to other machine learning approaches. 

Deep incremental learning introduced in this paper, is a hybrid approach which allows predictions from base learner models to be reused in future predictions for prediction tasks on data streams. Unlike reinforcement learning which allows flow of information between agent and environment in both directions, deep incremental learning only allows a single directional flow of information from one layer to the next one as a waterfall. The \textbf{point-in-time} nature of predictions are preserved so that no look-ahead bias is introduced. Deep incremental learning can be considered as an extension of model stacking \cite{naimi2018stacked,} which taken into account the stream nature of data . 

The incremental problem studied here is a ranking task based on data streams of tabular features and targets, defined as temporal tabular datasets, which is first introduced in \cite{wong2023dynamic}. While not explored in detail in this study, the incremental learning framework we proposed can be applied to other kinds of supervised learning tasks on data streams with little modifications required. 


%% The beginning of mathematics 

\paragraph{Definitions}

\begin{definition}[Temporal Tabular Datasets]
\label{def:temptable}
A temporal tabular dataset is a collection of matrices $\{ X_i, y_i \}_{1 \leq i \leq T}$ collected over time eras 1 to $T$. Each matrix $X_i$ represents data available at era $i$ with shape $N_i \times M$, where $N_i$ is the number of data samples in era $i$ and $M$ is the number of features describing the samples. $y_i$ are the targets corresponding to the features $X_i$, which can be single-dimensional or multi-dimensional. Note that the definition of the features are fixed throughout the eras, in the sense that the same computational formula is used to compute the features at each era. On the other hand, the number of data samples $N_i$ does not have to be constant across time. In practical applications, the number of data samples in each era is assumed to be globally bounded.
%% Data Lag
Unlike standard online learning problems, where the data arrived can be used to update machine learning models \text{right after} prediction is made, there is a \textbf{fixed} and known time lag for the targets from an era to be known, defined as \textbf{data embargo}. For example, for a dataset with a data embargo of $5$, at era $X$ the features of Era $X$ would be known and then a prediction of the ranking of items at that time could be made. The targets of era $X$ are only known at era $X+5$, which can then be used to calculate the quality of predictions according to a suitably chosen metric. 
%For many applications, the temporal tabular dataset can grow to \textbf{infinite} size. 
\end{definition}

For tabular datasets in general, the features can be in different formats, such as numerical, ordinal or categorical. With suitable pre-processing techniques, features can be transformed into equal-sized or Gaussian-binned numerical(ordinal) values. The datasets used in this study are already standardised into discrete bins. 

%Researchers might be tempted to use one-hot encoding to create "categorical" features out of these discrete bins, as they might want to capture the non-linearity effects of features. However, this procedure violates the assumptions made in the data creation process, where the raw features before standardisation are continuous measures \footnote{The datasets considered in this paper consists of factors which capture known economic effects in predicting stock prices, such as momentum, these factors are continuous measures with natural ordering.}. 

There are two approaches to model temporal tabular datasets. The first is to apply standard machine learning algorithms for tabular datasets as usual, taking into account the temporal order of data during data sampling. The second is to model the \textbf{derived} time series defined as follows.

\begin{definition}[Derived Time Series]
\label{def:derivedts}
Time series can be derived from the temporal tabular dataset with different transformations. Transformations considered here are restricted to those that can be applied to each data era \textbf{independently}. 
%A major limitation of this assumption is that it precludes the use of feature engineering methods such as Auto-Encoders which are applied across eras. However this assumption can make sure there is no look-ahead bias in the model as these derived time series would be used to build time series models. 
Dimension reduction operations which transforms the matrix of a single slice of tabular features at era $i$ with its targets into a one-dimensional tensor can be used: formally defined as $f(X,y): (\mathbb{R}^{N_i \times M}, \mathbb{R}^{N_i \times 1}) \mapsto \mathbb{R}^{M})$ where $X$ and $y$ are the features and targets, $N_i$ is the number of observations at era $i$, $M$ is the number of features. This procedure is defined as deriving the \textbf{feature performances} of the temporal tabular dataset.
\end{definition}

Most machine learning models can be expressed as a sequence of transformations between different tensors. Restricting to transformations between tabular and time series data only, 4 different basic transformations are obtained as follows:

\begin{enumerate}
    \item Transformation from tabular data to tabular 
    \begin{itemize}
        \item Standard tabular machine learning models which transform the given feature target pair $(X,y)$ into $y'$ where the first(data) dimension of $X,y,y'$ are equal dimensions and $y$ and $y'$ matches all dimensions. The transformations is performed \textbf{point-wise}, where at inference each item can be predicted independently. There are many examples for this class of machine learning models, including (and not limited to) gradient-boosting decision trees, and multi-layer perceptron networks. 
        \item List-wise tabular machine learning models which transform the \textbf{whole} list of items at a time. For a temporal tabular dataset, this means the item rankings are predicted \textbf{all at once}. Examples include various list-wise models for learn-to-rank problems \cite{li2020learning}. 
    \end{itemize}
    \item Transformation from tabular data to time series
    \begin{itemize}
        \item Deriving time series using the procedure defined in \ref{def:derivedts} or other suitable transformations. 
    \end{itemize}   
    \item Transformation from time series to tabular data 
    \begin{itemize}
        \item Feature Engineering methods for time series data such as Signature transforms \cite{Terry22} and Random Fourier transforms \cite{Sutherland15} which transforms a slice of time series into a single dimensional tensor which captures the characteristics of the time series. 
    \end{itemize}       
    \item Transformation from time series to time series 
    \begin{itemize}
        \item Sequence models in deep learning, such as LSTM \cite{HochSchm97} and Transformers \cite{Bryan19}. 
    \end{itemize}
\end{enumerate}


The above transformations are the building blocks of the deep incremental learning model. For tabular prediction tasks, any chain of transformations starting with tabular features and ending with tabular targets can be used. This expands the class of models from standard tabular models to other multi-step models such as \textbf{factor-timing} models described in \ref{section:ML-TS}. In factor-timing models, the features are first transformed into a time series, capturing the history of feature performances and then using the ranking of predicted feature performances to formulate a factor-timing portfolio. The time series model for predicting feature performances could be any of the above-mentioned methods. 


\paragraph{How incremental learning is different from traditional machine learning} 

The incremental nature of prediction tasks on data streams challenges the traditional assumptions of machine learning problems. In particular, a (single-pass) cross-validation which splits the data into \textbf{fixed} training, validation and test periods is not the most suitable framework. Instead, an incremental learning framework should be used to retrain and/or update model parameters. Under this framework, a model is represented by a continuous stream of parameters instead of a single set of parameters. The procedure also adds new hyper-parameters, such as training size and retrain period to the machine learning model. In practice, these hyper-parameters could be selected based on computational resources available rather than optimised. The size of memory will limit the maximum training size and the amount of GPU or other processing units available will limit how often the models are retrained/updated. 

When retrain period is greater than 1, there is an extra degree of freedom in when to start the incremental learning procedure. For example, if models are retrained every 10th era, then starting the incremental learning procedure at Era 1, Era 2, ... to Era 10 would give 10 different training procedures using different training sets. This choice could have a non-negligible impact on prediction performances for non-stationary datasets \cite{hoffstein2020rebalance} . 

The clear distinction between features, targets and predictions is also blurred in the incremental learning setting, as predictions from each model can be used as additional features in building other models. New targets can also be created by subtracting against the predictions made. This suggests model training should be considered as a \textbf{multi-step} problem instead of a \text{single-step} problem. 

As an example, consider an incremental learning problem with a data lag of 1, then in figure \ref{fig:gantt},  three models are trained under the incremental learning framework. The training period of each model is fixed at 4. At Era 6, information up to Week 4 (where both the features and targets are known) is used to train Model 1 and then obtain predictions from Era 6 on-wards. At Era 11, information up to Week 9 is used for model training. Features between Week 6 and Week 9 are combined with predictions from Model 1 to  train a new model (Model 2). Similarly, at Era 16,  Model 3 is trained using data from Era 11 to Era 14, which consists of the original given features, and predictions from Model 1 and Model 2. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figure/chapter3/gantt.png}
    \caption{Example of reusing model predictions in an incremental learning model}
    \label{fig:gantt}
\end{figure}

There are various benefits of reusing model predictions within an incremental learning model. It provides a hierarchical structure of models, in which each model can be interpreted as an improvement of the previous ones to distributional shifts in data and the prediction quality of each model can be inspected independently. Moreover, reusing model predictions introduced a feedback learning loop where model predictions correct themselves in an incremental manner.  

%Finally, it allows machine learning models to process an infinite stream of data with a finite memory usage. By limiting to training each model using the original features and predictions from (a fixed number of) previous models \textbf{only} for a training set with fixed window size, the size of the training set is capped as both the number of features and observations are bounded. Information from previous eras can be passed indirectly to the latest machine learning models. 


The above incremental learning model can be applied to not just a \textbf{single} machine learning model, but a \textbf{collection} of machine learning models in parallel, which is defined as \textbf{layers} of models. The notion is borrowed from MLP, where each node within a layer is now an independently trained model. 

A major difference between our incremental learning model from MLP is that training is done in a single forward pass rather than by back-propagation. This allows us to train very complicated models even with little resources, as there is no need to put the whole model in (distributed) memory to pass gradients between layers. As each model within a layer can be trained independently, training models \textbf{within} a layer becomes an embarrassingly parallel task when multiple GPUs can be used. The program code of the model is also easier to maintain as there is no need to use specialised software packages to distribute data between GPUs.

Recent work in deep learning research suggests back-propagation is not strictly necessary for model training \cite{Hinton22}. Deep Regression Ensemble (DRE) \cite{Kelly22deep} is an example where back-propagation is not used in training. DRE is built by training multiple layers of ridge regression models with random feature projections. DRE can be considered as a multi-layer perceptron network where some layers have frozen weights. The deep incremental model presented here is significantly different from DRE as no restrictions are made on the machine learning models used. 

There are many other possibilities in designing incremental learning models to deal with concept drifts in data, see \cite{Gama14,Jie19,} for survey on recent methods in modelling data streams with different change detection and adaptive learning techniques. 


\subsection{Practical considerations} 
Unlike standard machine learning problems where model training is done offline, where memory and computational time is usually not a major consideration in model design, incremental learning models deployed to live data streams would be limited by both the memory and computational time during inference and online retrain/update of model parameters. The exact requirements varied between problems and therefore it is not possible to offer a one-size-fits-all solution.

All models in this paper are trained on a single CUDA-enabled GPU of 10GB memory for model training. While this assumption precludes the use of very advanced deep learning methods, we can still build very complicated models using commonly used tabular and time series models as building blocks. The model can be designed to process an infinite data stream of temporal tabular data effectively without ever-growing memory consumption, by using only the latest values in the incremental learning process. 



\section{Machine Learning Models for Time Series Datasets} 
\label{section:ML-TS}

In this section, we introduce machine learning models that are commonly used for time series datasets and how to create \textbf{factor-timing} models based on predictions from time series models.


% Extract features from TS models
\paragraph{Multivariate time series}

A multivariate time series $X$ of $T$ steps and $N$ channels can be represented as $X = (\bm{x}_1, \bm{x}_2, \dots, \bm{x}_i, \dots, \bm{x}_T)$, with $1 \leq i \leq T$ and each vector $\bm{x}_i \in \mathbb{R}^N$ represents the values of the $N$ channels at time $i$. The number of channels of the time series is assumed to be fixed throughout time with regular and synchronous sampling, i.e. the values in each vector from multiple channels arrive at the same time at a fixed frequency. 

\begin{definition}[Time Series Model] 
Given a time series $X_T \in \mathbb{R^{T\times M}} $ where $T$ is the size of the time dimension and $M$ is the number of features. A (one-step) ahead time series model is a function $f: \mathbb{R^{T\times M}} \mapsto \mathbb{R}^M$. 
\end{definition}

In practice, the function $f$ is often learned by training statistical/machine learning models using future values of $X_T$, which is obtained by shifting the values of $X_T$ across the time dimension. 


\subsection{Factor Timing Models}

Factor-timing models are created based on predicted rankings of features from the time series models as shown in Algorithm \ref{alg:factor-timing}. The raw predicted values from the time series models are converted into normalised rankings which can be used as weights of a linear factor-timing model. Within an incremental learning framework, a new factor-timing model is trained at each time step using the latest historical values of the derived feature importance time series. 
 
\begin{algorithm}[hbt!]
\caption{Factor Timing Model}
\label{alg:factor-timing}
\KwIn{At prediction era $t$, predicted values from time series model $\hat{y}_t \in \mathbb{R}^d$, temporal tabular dataset $X_t \in \mathbb{R}^{N_t \times d}$ where $d$ is the number of features}
\KwOut{factor timing model $\hat{z}_t \in \mathbb{R}^{N_t}$ }
Calculate the normalised ranking of features $\hat{r}_t$ from predictions of the time series model 
\begin{equation*}
    \hat{r}_t = \text{rank}(\hat{y}_t) - 0.5
\end{equation*}
where rank is the function which calculates the percentile rank of a value within a vector.  $\hat{r}_t$ are ranged between $-0.5$ and $0.5$ \\
Apply truncation to the normalised ranking of features if needed, given upper bound $u$ and lower bound $l$ on the normalised rankings, $-0.5 < l < u <0.5$
\begin{equation*}
    r_t = \max(\min(\hat{r}_t,u),l)
\end{equation*} 
Calculate linear factor-timing predictions $\hat{z}_t = X_t r_t $ 
\end{algorithm}


\subsection{Statistical Rules}
\label{section:stats}

Simple statistical rules can be applied on each feature time series \textbf{independently} to summarise the history of the time series. Moving averages are commonly used to capture trends in time series. In this paper, rather than fixing different look-back sizes to compute the moving averages of time series, exponential moving averages (EMA) with varying weight decays are used. This approach avoids the need for a warm-up period in computing traditional window-based moving averages. The formula to compute EMA is given as follows. 

\begin{definition}[Exponential Moving Average]
\label{equation:EMA}
Given an univariate time series $X = (X_1,X_2,\dots,X_t,\dots)$, the exponential moving average $y_t$ of the time series at time $t$ with weight decay $\alpha$ is defined as 
\begin{align*}
    y_1 &= X_1 \\
    y_t &= (1-\alpha) y_{t-1} + \alpha X_t
\end{align*}
\end{definition}


\subsection{Feature Engineering Models}
\label{section:feature-eng-multi}

Feature engineering methods can be applied to multivariate time series so that tabular features are obtained.  

\paragraph{Feature extraction}
Feature extraction methods are defined as functions that map the two-dimensional time series $X \in \mathbb{R}^{T \times N}$ to a one-dimensional feature space $f(X) \in \mathbb{R}^K$ where $K$ is the number of features. Feature extraction methods reduce the dimension and noise in time series data. With feature extraction methods, traditional machine learning models such as ridge regression can be used without relying on advanced neural network architectures such as Recurrent Neural Networks (RNN), Long-Short-Term-Memory (LSTM) Networks or Transformers \cite{Bryan19}. 

%% Look-back windows
\paragraph{Look-back windows}
For time series which can potentially grow in infinity, a look-back window is needed to restrict the data size when calculating features from time series. To avoid look-ahead bias features that represent the state of time series at time $i$ can only be calculated using values obtained up to time $i$, which is $(\bm{x}_1, \bm{x}_2, \dots, \bm{x}_i)$. For most incremental learning tasks, an assumption is made such that data collected more recently are more important than data collected from the past. Therefore, analysis is often restricted to use the most recent $k$ data points only, which are $(\bm{x}_{i-k}, \bm{x}_{i+1-k}, \dots, \bm{x}_i)$. This represents the state of the time series at time $i$ with a look-back window of size $k$. Feature extraction methods are applied to data within the look-back window only. Multiple look-back windows can be used to extract features corresponding to short-term and long-term trends of the time series. At each time $i$, features extracted with different look-back windows are concatenated to represent the state of the time series.  

%To model multivariate time series effectively, better methods which can be applied on multiple time series in parallel are needed. In this study, both deterministic and random transformations are considered to create tabular features for the prediction task. 

% Signatures
\subsubsection{Signature Transforms} 
Signature transforms \cite{Lyons07, Chevyrev16, Terry22} are \textbf{deterministic} transformations which can be used to extract features from multivariate time series. Signature transforms are applied on continuous paths. A path $X$ is defined as a continuous function from a finite interval $[a,b]$ to $\mathbb{R}^d$ with $d$ the dimension of the path. $X$ can be parameterised in coordinate form as $X_t = (X_t^1,X_t^2,\dots,X_t^d)$ with each $X_t^i$ being a single dimensional path. 

% Iterated Integrals 
For each index $ 1 \leq i \leq d$, the increment of $i$-th coordinate of path at time $t \in [a,b]$, $S(X)_{a,t}^i$, is defined as 
\begin{equation*}
    S(X)_{a,t}^i = \int_{a<s<t} \mathrm{d}X_s^i = X_t^i - X_a^i
\end{equation*}
As $S(X)_{a,\cdot}^i$ is also a real-valued path, the integrals can be calculated iteratively. A $k$-fold iterated integral of $X$ along the indices $i_1,\dots,i_k$ is defined as 
\begin{equation*}
    S(X)_{a,t}^{i_1,\dots,i_k} = \int_{a<t_k<t} \dots \int_{a<t_1<t_2}   \mathrm{d}X_{t_1}^{i_1}  \dots \mathrm{d}X_{t_k}^{i_k} 
\end{equation*}

% Definition of Signature 
The Signature of a path $X: [a,b] \mapsto \mathbb{R}^d$, denoted by $S(X)_{a,b}$, is defined as the infinite series of all iterated integrals of $X$, which can be represented as follows 
\begin{align*}
    S(X)_{a,b} &= (1, S(X)_{a,b}^1, \dots, S(X)_{a,b}^d,  S(X)_{a,b}^{1,1}, \dots ) \\
               &= \bigoplus_{n=1}^{\infty} S(X)_{a,b}^n
\end{align*}

An alternative definition of signature as the response of an exponential nonlinear system is given in \cite{Terry22}. 

% Log Signature 
Log Signature can be computed by taking the logarithm on the formal power series of Signature. No information is lost as it is possible to recover the (original) Signature from Log Signature by taking the exponential \cite{Chevyrev16,Terry22}. Log Signature provides a more compact representation of the time series than Signature. 
\begin{equation*}
    log S(X)_{a,b} =  \bigoplus_{n=1}^{\infty}  \frac{(-1)^{(n-1)}}{n} S(X)_{a,b}^{\bigotimes n} 
\end{equation*}


%%% Theoretical properties of signatures 
%%% Multiplicative Functional 
%%% Universal Property of Signature in predictions (flexible) 
Signatures can be computed efficiently using the Python package signatory \cite{kidger2021signatory}. The signature is a multiplicative functional in which Chen's identity holds. This allows quick computation of signatures on overlapping slices in a path. Signatures provide a unique representation of a path which is invariant under reparameterisation \cite{Chevyrev16, Terry22}. Rough Path Theory suggests the signature of a path is a good candidate set of linear functionals which captures the aspects of the data necessary for forecasting. In particular, continuous functions of paths are approximately linear on signatures \cite{pmlr-v130-lemercier21a}. This can be considered as a version of the universal approximation theorem \cite{cybenko1989approximation} for signature transforms. 


%% Interpretation of Signatures
% Level 1 Signature corresponds to the difference of two series (tail-head). When log price series are given as input, it corresponds to log return 
% Basic Statistical features can be recovered from signatures 

\paragraph{Limitations for signature transforms in high dimensional datasets}
The number of signatures and log-signatures increases exponentially with the number of channels. For time series with a large number of channels, random sampling can be applied to select a small number ($5 < N < 20$) of time series with replacement from the original time series on which signature transforms are applied. Random sampling can be repeated a given number of times to generate representative features of the whole multivariate time series. Similar ideas are considered in \cite{James20}, in which random projections on the high dimensional time series are used to reduce dimensionality before applying signature transforms.  

%% Lookback window
Let $\tilde{X}$ be a multivariate time series with $T$ time-steps and $d$ dimensional features, denote $\tilde{X}_s \in \mathbb{R}^d$ be the observation of the time series at timestep $s$. Procedure \ref{alg:lookback} can be used to obtain paths, which are slices of time series with different lookback windows. Random Signature transforms \ref{alg:randomsig} can then be used to compute the signature of the path, which summarises the information of the time series. 

\begin{algorithm}[hbt!]
\caption{Lookback Window Slicing}\label{alg:lookback}
\KwIn{time series $\tilde{X} \in \mathbb{R}^{T \times d}$, lookback $\delta$}
\KwOut{paths $X_t \in \mathbb{R}^{t \times d}$}
\For{$1 \leq t \leq T$}{
    Set start of slice $s_1 = \max(1, t - \delta)$ \;
    Set end of slice $s_2 = t$ \;
    $X_t = (\tilde{X}_{s_1},\tilde{X}_{s_1+1}, \dots, \tilde{X}_{s_2}) $ \;
}
\end{algorithm}


%% Random Signature Transform Algorithms 
\begin{algorithm}[hbt!]
\caption{Random Signature Transform}\label{alg:randomsig}
\KwIn{path $X_t \in \mathbb{R}^{t \times d}$, level of signature $L$, number of channels $C$, number of feature sets $p$,} 
where $d > C$ \;
\KwOut{log signatures $s_t \in \mathbb{R}^{pN}$ }
Define $N= \text{Number of Log Signatures of a path with } C \text{ channels up to level }  L $ \;
\For{$1 \leq i \leq p$}{
    Sample with replacement $C$ Columns from $X_t$, defined as $\tilde{X}^i_t$ \;
    Compute the Log Signatures $s_t^i \in \mathbb{R}^N$ of $\tilde{X}^i_t$ \;
}
Combine all log signatures $s_t = (s_t^1, \dots, s_t^p)$ 
\end{algorithm}


\subsubsection{Random Fourier Transforms} 
Random Fourier Transforms are used in \cite{kelly2022virtue} to model the return of financial price time series. They can be applied to the feature performance time series at each time step as in Algorithm \ref{alg:rft}. The key idea is to approximate a mixture model of Gaussian kernels with trigonometric functions \cite{Sutherland15}.  

\begin{comment}
A price time series $P_t \in \mathbb{R}^T$ is first transformed into a return series $X_t \in \mathbb{R}^{T \times d} $ by taking the percentage change of price at different lookback intervals. Let $\delta_1, \dots, \delta_d \in \mathbb{N}$ be a given a list of lookback intervals, 
\begin{equation*}
    X_{t,\delta_i} = \frac{P_t - P_{t-\delta_i}}{P_{t-\delta_i}}
\end{equation*}
where $1 \leq t \leq T$ and $1 \leq i \leq d$     
\end{comment}


%% Random Fourier Transform Algorithms 
\begin{algorithm}[hbt!]
\caption{Random Fourier Transform \cite{kelly2022virtue}}\label{alg:rft}
\KwIn{signal vector $x_t \in \mathbb{R}^d$, number of features sets $p$, }
\KwOut{transformed vector $s_t \in \mathbb{R}^{14p}$ }
\For{$1 \leq i \leq p$}{
    Sample $w_i \sim \mathcal{N}(0, I_{d\times d})$ \;
    Set grid $(\gamma_i)_{i=1}^14 = (0.1, 0.5, 1, 2, 4, 8, 16, 0.1, 0.5, 1, 2, 4, 8, 16)$ \;
    \For{$1 \leq j \leq 7$}{
        Set $ s_{t,14i+j} = \frac{1}{\sqrt{7p}} \sin(\gamma_j w_i^T x_t)$
    }
    \For{$8 \leq j \leq 14$}{
        Set $ s_{t,14i+j} = \frac{1}{\sqrt{7p}} \cos(\gamma_j w_i^T x_t)$
    }
}
\end{algorithm}


\subsubsection{Ridge Regression}
Applying the above feature engineering methods, tabular features that have sizes greater than the number of observations is created. This results in an over-parameterised model where regularisation is required as there are multiple models that can perfectly fit the data. In this paper, ridge regression, which is a linear regression model with L2-regularisation is used. Ridge regression has closed-form solutions and there is an efficient implementation which can calculate ridge regression models with different L2-regularisation on the same dataset. 


\subsubsection{Selection of Look-back Window} 
For all the feature engineering methods mentioned above, a look-back window needs to be selected. Choosing different sizes of look-back windows corresponds to extracting the dynamics of feature performances at different time scales in the data stream. The lookback can be set to a fixed size or be "infinite", which means using all the available history of the time series at the time of prediction. 


\section{Machine Learning Models for Tabular Datasets} 
\label{section:tabular-inc}

There is rich literature comparing different machine learning approaches on tabular datasets \cite{mcelfresh2023neural,Shwartz21,Leo22,Arlind21,}. A key limitation of these studies is that most datasets chosen are static and small in size (with less than 200 features and less than 10k data rows). Here, we are working with dynamic datasets which are large in size (with more than 1000 numerical features and more than 200k data rows). While researchers hold different views on which of GBDT and MLP models are more suitable for (general purpose) tabular regression/classification tasks, many bench-marking studies \cite{mcelfresh2023neural,Shwartz21,Leo22,} demonstrated that advanced deep learning methods such as transformers \cite{Arik_Pfister_2021,} under-performed traditional approaches such as gradient boosting decision trees and multi-layer perceptron networks. These complex deep learning methods often require a huge amount of computational resources, limiting their use in large-scale real-time incremental learning systems. Some research \cite{mcelfresh2023neural,} even suggests it is not necessary to choose between GBDT and neural network models. A GBDT model with a little amount of hyper-parameter tuning will perform very closely to the optimal model. 

Some research \cite{mcelfresh2023neural,} has demonstrated for larger datasets, Gradient Boosting Decision Trees performed better than 11 neural-network-based approaches and 5 baseline approaches such as Support Vector Machines (SVM). GBDT models also outperform when feature distributions are skewed or heavy-tailed. It remains an open question, whether within an incremental learning setting for data streams with distribution shifts, GBDT still outperforms other deep learning approaches. 

In this paper, Gradient Boosting Decision Trees (GBDT) and multi-layer perceptron networks (MLP) are studied in detail as both demonstrated strong performances in many bench-marking studies \cite{mcelfresh2023neural,Shwartz21,Leo22,} and efficient implementations exist for these algorithms which allows scalable model training and inference. 


\subsection{Gradient Boosting Models} 

Gradient Boosting Decision Trees is a standard tool for tabular data modelling. There are many efficient implementations, for example, XGBoost \cite{XGBoost}, LightGBM \cite{LightGBM} and CatBoost \cite{CatBoost}. In this paper, XGBoost is used as it offers good GPU acceleration and flexible feature sub-sampling at the node level. 

\paragraph{Gradient Boosting Algorithm} 

Gradient Boosting is a generic algorithm for combining base learners in a sequential manner to obtain better predictions. A common choice of base learners would be decision trees. The aim of (gradient) boosting is to reduce \textbf{bias} of the ensemble learner. The aim is different to that of bagging, which fits multiple independent base learners at the same time, and the ensemble learner has a lower \textbf{variance} than each base learner. For practical implementations, bagging and boosting can often be used together. For example, in XGBoost, multiple trees (forests) can be fit in each boosting round. However empirical experience (as shown in textbooks like \cite{pml1Book}) suggests boosting works better than bagging for most datasets. Therefore, only a single tree is trained in each boosting round for the gradient boosting models used in this study. Algorithm \ref{alg:gradient-boosting} shows the pseudo-code for the general gradient boosting for a regression problem. 

\begin{algorithm}[htb!]
Given $N$ data samples $(\mathbf{x_i}, y_i), 1 
\leq i \leq N$ with the aim to find an increasing better estimate $\hat{f}(\mathbf{x})$ of the minimising function $f(x)$ which minimise the loss $\mathcal{L}(f)$ between targets and predicted values. $\mathcal{L}(f) = \sum_i l(y_i,f(\mathbf{x_i})) $ where $l$ is a given loss function such as mean square losses for regression problems. Function $f$ is restricted to the class of additive models $f(\mathbf{x}) = \sum_{k=1}^K w_k h(\mathbf{x},	\bm{\alpha_k})$ where $h(\cdot,\bm{\alpha})$ is a weak learner with parameters $\bm{\alpha}$ and $w_k$ are the weights. \\

Initialise  $f_0(\mathbf{x}) = \arg \min_{\bm{\alpha_0}} \sum_{i=1}^N l(y_i, h(\mathbf{x_i}, 	\bm{\alpha_0}))$  \\

\For{k = 1 : K}{
    Compute the gradient residual using $g_{ik} = - \left [ \frac{\partial l(y_i, f_{k-1}(\mathbf{x_i})) }{\partial f_{k-1}(\mathbf{x_i}) } \right ]  $ \\
    Use the weak learner to compute $\bm{\alpha_k}$ which minimises $ \sum_{i=1}^N (g_{ik} - h(\mathbf{x_i},	\bm{\alpha_k}))^2 $  \\
    Update with learning rate $\lambda$ $f_k(\mathbf{x}) = f_{k-1}(\mathbf{x}) + \lambda h(\mathbf{x}, \bm{\alpha_k}) $ \\ 
}
\textbf{Return} $f(\mathbf{x}) = f_K(\mathbf{x})$ \\
\caption{Gradient boosting algorithm \cite{FriedmanJeromeH.2001GfaA, B_hlmann_2007} }
\label{alg:gradient-boosting}
\end{algorithm} 


\paragraph{XGBoost Implementation}

XGBoost \cite{XGBoost} modifies the above "standard" gradient boosting algorithms with approximation algorithms in split finding. Instead of finding the best(exact) split by searching over all possible split points on all the features, a histogram is constructed where splitting is based on percentiles of features. XGBoost supports two different growth policies for the leaf nodes, where nodes closest to the root are split (depth-wise) or the nodes with the highest change of loss function are split (loss-guide). The default tree-growing policy is depth-wise and performs better in most benchmark studies. XGBoost also supports L1 and L2 regularisation of model weights. Other standard model regularisation techniques such as limiting the maximum depth of trees and the minimum number of data samples in a leaf node are also supported. 


\paragraph{Model Snapshots}

For GBDT models, it is easy to extract model snapshots, defined as the model parameters captured at the different parts of the training process. This can be done without any additional memory costs at inference.

Model snapshots of a GBDT model can be obtained as follows. The snapshots start with the first tree and the number of trees to be used is set to be $10\%,20\%,\dots,100\%$ of the number of boosting rounds. This trivially gives 10 different GBDT models representing different model complexities from a \textbf{single} model. 

%To avoid early convergence of the learning process, the learning rate can be set to a small value. 


\subsection{Deep Learning Models} 


\paragraph{Training process}

PyTorch Lightning \cite{Falcon_PyTorch_Lightning_2019} is used to build neural network models as it supports modular design and allows rapid prototyping. %The learning rate of neural networks is found by the Learning Rate Finder over a parameter grid of $(1e-3,0.1)$. 
Early stopping is applied based on the validation set based on a given number of rounds (patience). The batch size of the neural network is set to be the size of each era. The Adam optimiser in PyTorch with the default settings for the learning rate schedule is used. L2-regularisation on the model weights is also applied. Gradient clipping is also be applied to prevent the gradient explosion problem for correlation-based loss functions. 

\paragraph{Architecture}

The network architecture is a sequential neural network with two parts, firstly a "Feature Engineering" part which consists of multiple feature engineering blocks and then the "funnel" part which is a standard MLP with decreasing layer sizes. 

Each feature engineering block has an Auto-Encoder-like structure, where the number of features is unchanged after passing each block. Setting a neuron scale ratio of less than 1 corresponds to the case of introducing a bottleneck to the network architecture so as to learn a latent representation of data in a lower dimensional space. %Setting a neuron scale ratio greater than 1 corresponds to the case of introducing random combinations of features which are then refined during the model training process.
Algorithm \ref{alg:encoding} shows how to create the feature engineering part of the network.

Funnel architecture, as used in \cite{Zimmer_Auto-PyTorch_Tabular_Multi-Fidelity_2021} is an effective way to define the neuron sizes in a network for different input feature sizes. Algorithm \ref{alg:funnel} shows how to create the funnel part of the network. 

Each Linear layer is followed by a ReLU activation layer and dropout layer where $10\%$ of weights are randomly zeroed. %Batch Normalisation is not used. 

\begin{definition}[Linear Layer] ~\\
    A Linear Layer $(M_1,M_2)$ within a sequential neural network is a transformation $X_2 = f(X_1)$ with input tensor $X_1 \in \mathbb{R}^{N \times M_1}$ and output tensor $X_2 \in \mathbb{R}^{N \times M_2}$ where $N$ is the batch size of data. For a given non-linear activation function $\sigma(\cdot)$ such as ReLU, let $W \in \mathbb{R}^{M_2 \times M_1}$ be the weight tensor and $b \in \mathbb{R}^{M_2}$ be the bias tensor to be learnt in the training process, the Linear layer is defined as 
    \begin{equation*}
        f(X_1) = \sigma(X_1 W^T + b)
    \end{equation*}
    % where the addition of bias tensor is performed by broadcast multiplication 
\end{definition}


\begin{algorithm}[hbt!]
\caption{Feature Engineering network architecture}
\label{alg:encoding}
\KwIn{Input feature size $M$, Number of encoding layers $L$, neuron scale ratio $r$}
\KwOut{Sequential Feature Engineering Network Architecture}
\For{$1 \leq l \leq L$}{
    Encoding Layer $l$: Linear layer $(M,M*r)$ \\
    Decoding Layer $l$: Linear layer $(M*r,M)$ 
}
\end{algorithm}

\begin{algorithm}[hbt!]
\caption{Funnel network architecture}
\label{alg:funnel}
\KwIn{Input feature size $M$, Output feature size $K$, Number of intermediate layers $L$, neuron scale ratio $r$}
\KwOut{Sequential Funnel Network Architecture}
Input Layer: Linear layer (M, $M*r$) \\
\For{$1 \leq l \leq L$}{
    Intermediate Layer $l$: Linear layer $(M*r^{l}, M*r^{l+1})$ 
}
Output Layer: Linear layer $(M*r^{L+1},K)$ \\
\end{algorithm}


\paragraph{Feature projection and Loss Function} 

Pearson correlation calculated on the whole \textbf{era} of target and predictions is used as the loss function at each training epoch. Feature projection, if needed, can be applied from the outputs of network architecture. The neutralised predictions are further standardised to zero mean and unit norm. The negative Pearson correlation of the standardised predictions and targets is then used as the loss function to train the network parameters. 



\section{Deep incremental learning model}

A deep incremental learning model can be built using the tabular and factor-timing models as components. Algorithm \ref{alg:incremental-stack} outlines the overall structure of the model. Each layer in the deep incremental learning model is trained sequentially. At the start of training in a layer, features and targets that are shared by each component model are prepared. %Targets can be adjusted by gradient boosting formula if needed.

Each factor-timing and tabular model within the layer is trained in an incremental manner as described in Section \ref{section:overview}, in which model parameters are updated as new data arrives. Each component model within a layer can be trained in parallel. 


\begin{algorithm}[hbt!]
\caption{Deep Incremental learning model with model stacking}
\label{alg:incremental-stack}

\KwIn{Temporal Tabular Dataset $\{ X_i, y_i \}_{1 \leq i \leq T}$, number of layers $L$, the number of models within each layer $(K_1, \dots, K_L)$, training size $a_l$, data embargo $b$, }

%Build the feature importance time series using the given features and targets $\{ X_i, y_i \}$ for each era \\
\For{$1 \leq l \leq L$}{
    Calculate the start of model predictions for layer $l$ as $1+\sum_{i=1}^l (a_i+b)$ \\ 
    Prepare Features $\{ X_j^l \}$ where $ \sum_{i=1}^{l-1} (a_i+b) \le j \leq \sum_{i=1}^{l-1} (a_i+b) + a_l $ where $X_j^l$ consists of predictions from previous layers. \\
    \For{$1 \leq k \leq K_L$}{
    Perform Data and Feature Sub-sampling for each component model \\
    Train component model using features $\{ X_j^l, y_j \}$ \\
    Obtain predictions of the model from era $1+\sum_{i=1}^l (a_i+b)$ onward \\
    }
    %Build the feature importance time series for the next layer using predictions from the models trained in the current layer starting at era $1+l(a+b)$ 
}
\end{algorithm}


\paragraph{Self-Similarity nature of model}

The incremental learning model demonstrates self-similarity with a fractal-like structure. In other words, the overall model shared a similar structure with each of its components. 

The incremental learning model can be interpreted as a stacked model with $\sum_{i=1}^L K_i$ base learners trained with $L$ iterations, where $K_i$ base learners are trained in the $i$-th iteration. Ideas from bagging and boosting are integrated within the model. Each layer consists of multiple models trained in parallel as in bagging so that variance is reduced by combining predictions from different models within a layer. The stacked model here can be considered as a degenerate case of boosting where the learning rate of the target is set to zero, such that the target is not adjusted based on predictions from previous layers between layers. The above architecture can be modified to allow for target adjustment (boosting) between layers if needed. 

The incremental learning model can also be interpreted as a neural network model where each node is now a machine learning model instead of a parameter. As predictions from previous layers can be used as features for the following layers, this simulates the residual connections in some neural network architecture. Each layer in the model will refine the prediction as neural networks. 

The self-similarity structure stems from the fact that the component models within each layer can be chosen to be Gradient Boosting Decision Trees (GBDT) and Multi-layer Perceptron Neural Networks (MLP) so that the learning mechanism of each individual component is similar to the overall model. The self-similarity structure can be extended repeatedly by interpreting the incremental learning model as a base learner for another incremental learning model. Therefore, the complexity of the incremental learning model is not constrained. 



\paragraph{Trees and Neural Networks are alike}

Gradient Boosting Decision Trees (GBDT) and neural networks are usually considered two distinct classes of machine learning models. However recent researches such as soft decision trees \cite{NODE} and transformers \cite{Transformer17} suggests it is possible to make neural network models more tree-like. Similarly, replacing the base learner in GBDT models with weakly trained shallow nets can make GBDT models more neural-like. 

A better way to understand machine learning models is to place each model in a \textbf{continuous} spectrum of model density, from sparse to dense representations/structures, instead of assigning binary labels of tree or network. Under this framework, decision trees are examples of sparse models and neural networks are examples of dense models. Most machine learning models employ decision rules and regression with non-linear activation functions as the basic operations of learning. Depending on the exact algorithm used, a machine learning model will consist of a mix of dense and sparse structures, and thus a binary classification of the nature of the model is not suitable. 

In most applications, the choice of model depends on the nature of the input features. In general, sparse models are good for unstructured data or categorical features. Dense models are good for structured data (For example images, text) or numerical features. %The binned ordinal features for the temporal ranking task are somewhere in between, making both trees and networks good choices of models. 



\paragraph{Adaptive nature of model} 

The incremental learning model supports \textbf{dynamic} model training, as parameters of each component model are updated regularly to adapt to distributional shifts in data. Under the traditional machine learning framework, hyper-parameters of machine learning models are selected by cross-validations. A big limitation of using cross-validations for incremental learning problems is that the optimal hyper-parameters based on a \textbf{single} test period might not work in future. This is replaced by \textbf{dynamic} hyper-parameter optimisation in the incremental learning model. Predictions from previous layers based on different model hyper-parameters are combined in the next layer. The model parameters in the next layer can then be interpreted as the dynamic soft selection of hyper-parameters. Soft hyper-parameter selection is also related to Bayesian methods of learning the regularisation hyper-parameter of regression models. Instead of attempting to derive the posterior distributions of the model hyper-parameters, which is difficult when there are no closed-form solutions, the weight parameters can be considered as an approximation to the posterior distribution. 

Model stacking over deep learning models with different random seeds \cite{lakshminarayanan2017simple}, different hyper-parameters \cite{Florian20} and different architectures \cite{zaidi2021neural} are shown to demonstrate robust performances for \textbf{static} datasets. The deep incremental learning model presented here can be considered as an extension of these techniques to \textbf{stream} datasets and other machine learning models in general. In particular, models incrementally trained with data streams with different sizes of training datasets and timesteps between model updates can be stacked to obtain more robust predictions. 



\paragraph{Universal Approximation} 

It is well-known that MLP and GBDT models have the universal function approximation property \cite{cybenko1989approximation}. Deep Learning models for sequences, such as LSTM \cite{schafer2006recurrent} are also shown to have the universal function approximation property for any dynamical systems. The above incremental learning model is a composition of models that each has the universal function approximation property, therefore, it also has the universal approximation property for the underlying stochastic processes that drive the data generation of the temporal tabular datasets. 

%The universal approximation provides a theoretical guarantee that if there is an infinite amount of computational resources then the above model can be used for any modelling tasks of temporal tabular datasets. In reality, a wide range of heuristics is applied to simplify the model design with our finite amount of computational resources so that the approximated model can be as close to the theoretical optimal as possible. 


\subsection{Connections with common machine learning tasks}

The incremental learning model presented above is a complete end-to-end autoML tool which transforms the given features into predictions. Different machine learning tasks, such as feature engineering and model stacking are integrated within the model. A key insight is that tabular models such as GBDT or MLP can be used repeatedly in different layers of the model to achieve the purpose of different machine learning tasks, which were previously considered separate tasks. Using just two building blocks, GBDT and MLP models, very complex and effective incremental learning models can be built. 


\paragraph{Feature Engineering/Selection} 

Feature Engineering is an important part of modelling tabular data. A wide range of standard feature engineering methods, including random non-linear transforms \cite{Horn19} and data-based methods such as feature tools \cite{James15} are proposed to learn higher-order feature relationships. Standard tabular models are then trained on the original features with the newly created features. More recent methods would use deep learning methods, in particular sequential attention for feature selection and engineering, examples include TabNet\cite{Arik_Pfister_2021} and TabTransformer \cite{TabTransformer}. For these models, feature engineering and tabular data modelling are performed in a single model. A major limitation of these models is high (active) memory consumption during model training. Another limitation is that many feature engineering methods are developed for \textbf{static} datasets which assumes a set of relationships can be learnt on \textbf{fixed} training set and then generalises to the test set. Unless these feature engineering components are retrained regularly as the downstream tabular models, the learnt relationships might not hold over distribution shifts of data. Most feature engineering methods applied to tabular data are a transformation from tabular features to another set of tabular features and are not much different than a collection of weakly trained tabular models. Under the incremental learning framework, predictions from shallow trees and networks in the early layers can be considered as new features generated for downstream layers. It is not strictly necessary to make a distinction between feature engineering and tabular modelling. Training models in a multi-step layered structure avoids the look-ahead bias issue naturally.  Feature selection can be considered a special case of feature engineering where the feature engineering transformation to be learnt is a Boolean mask.


\paragraph{Feature Projection} 

Feature projection and its dynamic variant (Dynamic Feature Projection) introduced in the previous Numerai study \cite{wong2023dynamic} can also be reformulated under the incremental learning framework as follows. 

The need of feature projection is based on the assumption that unconstrained model training methods will result in model predictions that are explained by a small subset of features. While these models work well under data regimes that are similar to the training set, these models will suffer when there are distributional shifts in data. In investment terminology, unconstrained models are like investment portfolios concentrated in a small amount of assets, and therefore carry high risks under changes of market regimes. Feature projection, which can be used to limit both the maximal feature concentration risks and (linear)-dependencies of the model on the selected features can mitigate the impact of data distributional shifts by both spreading the model risk across a wider range of features and focusing more on the non-linear relationships between features. Dynamic feature projection suggests for data streams with distributional shifts, the optimal subset of features for projection is \textbf{dynamic} rather than \textbf{fixed}.  

A key issue that remained unresolved in the previous study \cite{wong2023dynamic} is that the degree of feature projection and the size of the subset of features to be projected is given by useful heuristics, rather than being optimised based on data available. Clearly, these parameters can be learned from the dataset and adjusted according to data distribution shifts. Under the incremental learning model, model predictions undergoing different feature projection processes can be combined in later layers. The optimal degree of feature projection depends on both the feature correlation structure and the target construction process. 


\paragraph{Advanced Architectures of neural networks} 

Many advanced architectures of neural networks outperform basic multi-layer perceptron networks \cite{Arik_Pfister_2021,NODE,TabTransformer,} on a small amount of datasets presented in the research, which all of the datasets studied are \textbf{static} machine learning problems. However, some research suggest the contrary when running the models using different hyper-parameters or on \textbf{different} datasets \cite{Arlind21,Leo22,Shwartz21}. As a result, the decision of whether advanced architectures are better than standard MLP for incremental learning tasks on tabular datasets remains an open problem. 

For most incremental learning problems, the only purpose is to generate high-quality and robust forecasts on \textbf{future} observations using models trained on \textbf{past} data only. A good performance based on \textbf{historical} data is not relevant if the backtest performances cannot carry on into the future. An inherent limitation in interpreting research results from machine learning on temporal-based datasets is that results reported in a published journal article are only a \textbf{snapshot} on the underlying data stream. When there are significant distribution shifts in data, conclusions can become invalid on new data arrived. This is a particular concern for neural networks models, which demonstrated high sensitivity to random seeds and training hyper-parameters. 

Another limitation of deep learning methods with advanced architectures is that they are inherently difficult to interpret. Compared with the incremental learning framework presented here, our model combines standalone machine learning models in a \textbf{component-wise} manner, such that each component model can be evaluated independently and then inspected to monitor the changes in prediction quality in different layers. 


\paragraph{Model Stacking/Selection}

Stacking is a simple but highly effective technique to combine different machine learning model predictions. The concept of stacking is not limited to machine learning. In finance, portfolio optimisation are studied in detail to improve investment returns, where a convex optimisation is solved at each time step to find the linear combination of assets or strategies that maximise risk-adjusted return.

Under the incremental learning framework, model stacking can be performed dynamically, which combined predicted ranking from different machine learning models at each era with different weights. Instead of considering model stacking as a \textbf{separate} step to model training, model stacking can be incorporated as an extra layer in the incremental learning model.


\section{Case Study with Numerai competition}

\subsection{Numerai Dataset} 
\label{section:numerai-sunshine-dataset} 


\paragraph{Features and Targets}

The v4.1 of the Numerai dataset \cite{numerai-datav4.1}, which consists of a total of 1586 features is used for analysis in this paper. The dataset is in the format of a Temporal Tabular dataset. The dataset contains multiple targets, which represent normalised stock returns by different statistical methods. A new target, 'target-cyrus-v4-20' is used for scoring the trained models. Common risk factors in stock trading are removed during the target construction process. As a result, feature projection \cite{wong2023dynamic} is not necessary for models trained with this target and will not be used in the following analysis.

To reduce computational time for grid searches, two different methods are used for feature selection. The first one is by random sampling and the second one is by taking the median over correlated features, making use of the specific data structure of the dataset to reduce dimensionality of the dataset. 

In the first method, a random sample of features can be drawn from the 1586 features in the dataset. In the second method, we reduce the dimensionality of data making use of the feature importance's time series. This method makes use of the observed feature correlation structure in the Numerai dataset. The features that demonstrate high multicollinearity are grouped and replaced with their Median (called` Group X Median'). A reason to use Median instead of Mean is to preserve the binned data structures (as each feature is binned values between $-2$ to $2$ after making the mean zero). 

The features from Numerai can be separated into two different sets. One of the sets consists of 1445 features which can be grouped in 289 groups by a multicollinearity criteria (Pearson correlation $>0.9$) consistently in different eras. Each group consists of an equal number of 5 features. This set of 1445 features is called \textbf{Set A}. The other set consists of 141 features that cannot be grouped by the above multicollinearity criteria. This set of 141 features is called \textbf{Set B}. For features in \textbf{Set A}, the median of each group is computed. This group of 289 Median features and the \textbf{Set B} features, in total 430 features are then used to calculate the \textbf{feature performances} time series, which are then used to build a factor timing portfolio described above. The feature performance time series is calculated based on the Pearson correlation of each feature with the target in each era. 

\paragraph{Data Lag}

Setting the data lag for predictions would depend on the robustness of the data pipeline from Numerai. The theoretical minimum for the scoring target to resolve is 5 weeks (4 weeks of market data and 1 week for data processing). To take account into both the data lag for the data generation process from Numerai and the time to train models, a conservative data lag of 15 weeks is used for training the models. 

\paragraph{Scoring Function} 

Numerai calculates a variant of Pearson correlation score for \textbf{all} predictions in a single era with the following formula \cite{numerai-corr}.

Let $y_p$ be the predictions ranked between 0 and 1, $y_t$ be the targets centred between -0.5 and 0.5, $\Phi(\cdot)$ be the Distribution function of standard Gaussian, $\textbf{sgn}(\cdot)$ and $\textbf{abs}(\cdot)$ be the element-wise sign and absolute value function, then Numerai Corr $\rho_n$ is given by 
\begin{align*}
    y_g &= \Phi^{-1}(y_p) \\
    y_{g15} &= \textbf{sgn}(y_g) \cdot \textbf{abs}(y_g)^{1.5} \\
    y_{t15} &= \textbf{sgn}(y_t) \cdot \textbf{abs}(y_t)^{1.5} \\
    \rho_n &= \textbf{Corr}(y_{g15},y_{t15})
\end{align*}

Corr defined above is the Pearson correlation function. The purpose of applying a power transformation is to emphasise the contribution from the highest and lowest predictions. 

The Pearson correlation score in each era is collected over the test period to calculate the following portfolio metrics. 
\begin{itemize}
    \item Mean Corr: The average of correlation scores over all the eras in the test period
    \item Maximum Drawdown: The maximum difference between the cumulative peak (High watermark) and the cumulative sum of the correlation scores in the test period 
    \item Sharpe ratio: The ratio of Mean corr and standard deviation of correlation scores over all the eras in the test period
    \item Calmar ratio: The ratio of Mean Corr and Maximum Drawdown of the correlation scores over all the eras in the test period
\end{itemize}



\section{Training incremental learning models with Numerai data}

A two-step process is used to train incremental learning models with the Numerai dataset. The first step is to use data before Era 800 for the hyper-parameter optimisation of tabular and factor-timing models. After that, the incremental learning model is run to get online predictions from Era 800 onward. 


\subsection{Hyper-parameter optimisation for tabular models} 

For different tabular models introduced in section \ref{section:tabular-inc}, hyper-parameter optimisation is performed using data before Era 800. The training and validation set is data between Era 1 and Era 600, with $25\%$ of data as the validation set. Due to memory constraints, data sub-sampling is applied during model training. $25\%$ of the eras in the training period is used with sampling performed at regular intervals. The performance of the models between Era 601 and Era 800 (evaluation period) is then used to select hyper-parameters for the tabular models. The Mean Corr and Sharpe Ratio of the prediction ranking correlation in the evaluation period is reported. Due to memory issues for training neural network models, a global feature selection process is used to select $50\%$ of the 1586 features at the start of each model process by random. 


\paragraph{Factor Timing Models}

Different factor timing models are trained as described in Section \ref{section:ML-TS}. 

Signature transforms and random Fourier transforms models with different model complexities, defined as the ratio of the number of time series features computed to the number of data points in the time series are trained. In order to study models from different complexity regimes, the model complexity is varied between 0.1 and 10 to obtain models from under-parameterised regime to over-parameterised regime. 

Table \ref{table:stats} reported the model performances in the evaluation period for factor timing models based on exponential moving averages. Tables \ref{table:signatures}, \ref{table:fourier} reported the model performances in the evaluation period for signatures transforms, random Fourier transforms models, averaged over different ridge regularisation parameters (0.01,0.1,1.0,10.0,100.0) and 5 different random seeds for all total of 25 combinations. The lookback period of calculating the signatures and random Fourier transforms is set to using all the available data %or the most recent 400,200,100 eras. 

Exponential Moving Averages models with weight decay between 0.0025 and 0.002 performed better than signatures and random Fourier transforms models in general. Weight decay of 0.005 is the best weight decay parameter between eras 601 and 800. For signature transforms models, higher complexity gives slightly better performances but is not significant. For random Fourier transforms models, higher complexity gives poorer performances when the model complexity ratio is higher than 2.5. 

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
        Weight Decay & Mean Corr & Sharpe & Calmar \\ \hline
        0.000625 & 0.0047  & 0.3181 & 0.0273 \\ \hline
        0.00125  & 0.0057  & 0.3590 & 0.0378 \\ \hline
        0.0025   & 0.0082  & 0.4791 & 0.0598 \\ \hline
        0.005    & 0.0091  & 0.5488 & 0.0850 \\ \hline
        0.01     & 0.0088  & 0.5396 & 0.0757 \\ \hline
        0.02     & 0.0085  & 0.5161 & 0.0741 \\ \hline
        0.04     & 0.0068  & 0.4157 & 0.0360 \\ \hline
        0.08     & 0.0058  & 0.3614 & 0.0260 \\ \hline
        0.16     & 0.0046  & 0.2930 & 0.0203 \\ \hline
    \end{tabular}
    \caption{Exponential Moving Averages models between Era 601 and 800}
    \label{table:stats}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        Lookback & Model Complexity & Mean Corr & Sharpe & Calmar\\ \hline
        All & 0.1 & $0.0074 \pm 0.0008$   & $0.4392 \pm 0.0503$  & $0.0582 \pm 0.0071$ \\ \hline
        All & 0.25 & $0.0076 \pm 0.0009$   & $0.4517 \pm 0.0571$  & $0.0605 \pm 0.0102$ \\ \hline
        All & 0.5 & $0.0077 \pm 0.0009$   & $0.4637 \pm 0.0630$  & $0.0635 \pm 0.0129$ \\ \hline
        All & 0.75 & $0.0079 \pm 0.0010$   & $0.4721 \pm 0.0653$  & $0.0658 \pm 0.0140$ \\ \hline
        All & 1 & $0.0080 \pm 0.0010$   & $0.4800 \pm 0.0702$  & $0.0688 \pm 0.0171$ \\ \hline
        All & 2.5 & $0.0082 \pm 0.0010$   & $0.4923 \pm 0.0671$  & $0.0685 \pm 0.0123$ \\ \hline
        All & 5 & $0.0083 \pm 0.0009$   & $0.5005 \pm 0.0660$  & $0.0701 \pm 0.0142$ \\ \hline
        All & 7.5 & $0.0083 \pm 0.0008$   & $0.5038 \pm 0.0628$  & $0.0720 \pm 0.0162$ \\ \hline
        All & 10 & $0.0084 \pm 0.0008$   & $0.5056 \pm 0.0607$  & $0.0727 \pm 0.0184$ \\ \hline
    \end{tabular}
    \caption{Signature transform models between Era 601 and 800}
    \label{table:signatures}
\end{table}
	
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        Lookback & Model Complexity & Mean Corr & Sharpe & Calmar\\ \hline
        All & 0.1 & $0.0077 \pm 0.0008$   & $0.4684 \pm 0.0513$  & $0.0585 \pm 0.0116$ \\ \hline
        All & 0.25 & $0.0082 \pm 0.0008$   & $0.4954 \pm 0.0496$  & $0.0632 \pm 0.0120$ \\ \hline
        All & 0.5 & $0.0075 \pm 0.0008$   & $0.4551 \pm 0.0441$  & $0.0570 \pm 0.0117$ \\ \hline
        All & 0.75 & $0.0075 \pm 0.0010$   & $0.4499 \pm 0.0540$  & $0.0525 \pm 0.0063$ \\ \hline
        All & 1 & $0.0067 \pm 0.0005$   & $0.3975 \pm 0.0298$  & $0.0502 \pm 0.0075$ \\ \hline
        All & 2.5 & $0.0065 \pm 0.0011$   & $0.3900 \pm 0.0651$  & $0.0530 \pm 0.0107$ \\ \hline
        All & 5 & $0.0029 \pm 0.0025$   & $0.1443 \pm 0.1316$  & $0.0207 \pm 0.0194$ \\ \hline
        All & 7.5 & $0.0019 \pm 0.0020$   & $0.1284 \pm 0.1304$  & $0.0174 \pm 0.0182$ \\ \hline
        All & 10 & $0.0024 \pm 0.0021$   & $0.1523 \pm 0.1309$  & $0.0182 \pm 0.0191$ \\ \hline
    \end{tabular}
    \caption{Fourier transform models between Era 601 and 800}
    \label{table:fourier}
\end{table}


Repeating the above analysis for data between Eras 601 and 1050, tables \ref{table:statsall}, \ref{table:signaturesall} and \ref{table:fourierall} show performances of statistical rule-based models, signature and random Fourier transforms models. Exponential Moving Averages models with weight decay 0.005 has the highest Sharpe ratio and the second highest Mean Corr. However, models with weight decay 0.01 and 0.02 have a higher Calmar ratio. Signature transforms models have similar performances over different model complexities. Signature transforms models performed better than random Fourier transforms models over different model complexities. Exponential Moving Average models with suitable weight decay parameters performed the best. 

Comparing the partial results between Eras 601 and 800 and the full results between Eras 601 and 1050 demonstrates why selecting model hyper-parameters for incremental learning tasks on non-stationary data streams based on \textbf{finite} snapshots of data might not be robust over longer horizons. Some hyper-parameters might perform better than others over a short amount of time but not over longer observations. 

%For models that depend on randomness, calculating model performances over different random seeds and then performing hypothesis tests can identify situations where the improvement of performances is not significant. Hypothesis tests can only be used to check if we can/cannot \textbf{reject} the null hypothesis that performances between different hyper-parameters are equal on average (equal with respect to different measures such as mean, and distribution depending on what hypothesis tests are used). 


\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
        Weight Decay & Mean Corr & Sharpe & Calmar \\ \hline
        0.000625 & 0.0044  & 0.2540 & 0.0090 \\ \hline
        0.00125 & 0.0058  & 0.3099 & 0.0111 \\ \hline
        0.0025 & 0.0087  & 0.4220 & 0.0152 \\ \hline
        0.005 & 0.0086  & 0.4992 & 0.0248 \\ \hline
        0.01  & 0.0077  & 0.4977 & 0.0454 \\ \hline
        0.02  & 0.0075  & 0.4759 & 0.0436 \\ \hline
        0.04   & 0.0059  & 0.3594 & 0.0211 \\ \hline
        0.08   & 0.0045  & 0.2494 & 0.0108 \\ \hline
        0.16   & 0.0029  & 0.1598 & 0.0049 \\ \hline
    \end{tabular}
    \caption{Exponential Moving Averages models between Era 601 and 1050}
    \label{table:statsall}
\end{table}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        Lookback & Model Complexity & Mean Corr & Sharpe & Calmar\\ \hline
        All & 0.1 & $0.0079 \pm 0.0006$   & $0.4269 \pm 0.0370$  & $0.0196 \pm 0.0030$ \\ \hline
        All & 0.25 & $0.0079 \pm 0.0005$   & $0.4334 \pm 0.0373$  & $0.0216 \pm 0.0062$ \\ \hline
        All & 0.5 & $0.0080 \pm 0.0005$   & $0.4420 \pm 0.0433$  & $0.0236 \pm 0.0093$ \\ \hline
        All & 0.75 & $0.0080 \pm 0.0005$   & $0.4468 \pm 0.0451$  & $0.0243 \pm 0.0097$ \\ \hline
        All & 1 & $0.0081 \pm 0.0005$   & $0.4516 \pm 0.0468$  & $0.0250 \pm 0.0096$ \\ \hline
        All & 2.5 & $0.0081 \pm 0.0005$   & $0.4557 \pm 0.0428$  & $0.0268 \pm 0.0089$ \\ \hline
        All & 5 & $0.0080 \pm 0.0007$   & $0.4486 \pm 0.0498$  & $0.0267 \pm 0.0095$ \\ \hline
        All & 7.5 & $0.0079 \pm 0.0009$   & $0.4430 \pm 0.0622$  & $0.0265 \pm 0.0121$ \\ \hline
        All & 10 & $0.0078 \pm 0.0011$   & $0.4384 \pm 0.0737$  & $0.0260 \pm 0.0129$ \\ \hline
    \end{tabular}
    \caption{Signature transform models between Era 601 and 1050}
    \label{table:signaturesall}
\end{table}
 	
\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        Lookback & Model Complexity & Mean Corr & Sharpe & Calmar\\ \hline
        All & 0.1 & $0.0071 \pm 0.0005$   & $0.3834 \pm 0.0235$  & $0.0163 \pm 0.0028$ \\ \hline
        All & 0.25 & $0.0070 \pm 0.0010$   & $0.3737 \pm 0.0452$  & $0.0152 \pm 0.0025$ \\ \hline
        All & 0.5 & $0.0064 \pm 0.0008$   & $0.3525 \pm 0.0364$  & $0.0151 \pm 0.0038$ \\ \hline
        All & 0.75 & $0.0064 \pm 0.0006$   & $0.3416 \pm 0.0270$  & $0.0123 \pm 0.0018$ \\ \hline
        All & 1 & $0.0063 \pm 0.0006$   & $0.3364 \pm 0.0246$  & $0.0136 \pm 0.0026$ \\ \hline
        All & 2.5 & $0.0057 \pm 0.0012$   & $0.3141 \pm 0.0561$  & $0.0146 \pm 0.0031$ \\ \hline
        All & 5 & $0.0019 \pm 0.0018$   & $0.0937 \pm 0.0940$  & $0.0070 \pm 0.0069$ \\ \hline
        All & 7.5 & $0.0016 \pm 0.0010$   & $0.0997 \pm 0.0632$  & $0.0051 \pm 0.0037$ \\ \hline
        All & 10 & $0.0014 \pm 0.0014$   & $0.0893 \pm 0.0870$  & $0.0059 \pm 0.0060$ \\ \hline
    \end{tabular}
    \caption{Fourier transform models between Eras 601 and 1050}
    \label{table:fourierall}
\end{table}


For models hyper-parameters that do not depend on randomness, such as exponential moving averages or other rule-based trend indicators, hypothesis testing cannot be applied directly to select hyper-parameters directly. Instead, soft hyper-parameter selection within an incremental learning framework are used to find an optimal \textbf{dynamic} combination of predictions. 

A deep XGBoost model is created by combining the 9 exponential moving averages (EMA) models incrementally. For every 10th era, a shallow XGBoost model with the following hyper-parameters is trained using model predictions from the most recent 185 eras (with suitable data lag). Monotonic constraints are imposed so that we do not assign negative weights to the model predictions. 

%% Layer 2 XGBoost 
\begin{itemize}
    \item Grow policy: Depth-wise
    \item Number of boosting rounds: 20
    \item Early Stopping 20
    \item Learning rate: 0.1
    \item Max Depth: 8 
    \item Max Leaves: 128 
    \item Min Samples per node: 10 
    \item Data subsample: 0.75
    \item Feature subsample by tree: 0.75
    \item Feature subsample by level/node: 1
    \item L1 regularisation: 0.001
    \item L2 regularisation: 0.001 
\end{itemize}


In table \ref{table:EMADeep}, the performance from the deep EMA models (Deep), repeated over 5 different random seeds is compared with each of the EMA models with different weight decays (Base) in Era 601 to Era 1050. Deep incremental learning can create better predictions by combining EMA models with different weight decays. Variance between performances are also significantly reduced. 

\begin{table}[!ht]
    \centering
    \caption{Deep EMA models between Era 601 and 1050}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        Base  & $0.0062 \pm 0.0021$  & $0.3586 \pm 0.1233$  & $0.0206 \pm 0.0148 $ \\ \hline
        Deep  & $0.0087 \pm 0.0001$  & $0.4773 \pm 0.0062$  & $0.0282 \pm 0.0014 $ \\ \hline
    \end{tabular}
    \label{table:EMADeep}
\end{table}




\paragraph{Neural Networks}

Neural Networks models \textbf{without} feature projection are trained with different number of encoding and funnel layers using the architecture described in Section \ref{section:tabular-inc}. 

\begin{itemize}
    \item Number of Feature Eng Layers: 0,1,2,3,4
    \item Number of Funnel Layers: 1,2,3 
\end{itemize}

Other hyper-parameters of the neural network models are fixed in the grid search as follows. 
\begin{itemize}
    \item Degree of Feature projection: 0.0
    \item Loss Function: Pearson Corr
    \item Number of epochs: 100 
    \item Early Stopping: 10
    \item Learning Rate: 0.001
    \item Dropout: 0.1
    \item Encoding Neuron Scale: 0.8
    \item Funnel Neuron Scale: 0.8 
    \item Gradient Clip: 0.5 
\end{itemize}


In Table \ref{table:MLP1} shows the performances of neural network models with different network architectures over 5 different random seeds.

The architecture with the highest Mean Corr is the model without feature engineering layers and a standard MLP model with 2 linear layers. When the number of funnel layers equals to 1, the MLP model is equivalent to a (regularised) linear model and has the worst performance. Increasing the number of feature engineering layers does not significantly improve Mean Corr. As model complexity increases, model performances are more varied over different random seeds, suggesting the lack of robustness of deep neural network models. 


\begin{table}[!ht]
    \centering
    \caption{Neural Network models between Era 601 and 800}
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
        Feature Eng Layers  & Funnel Layers  & Mean Corr & Sharpe  & Calmar \\ \hline
        0  & 1  & $0.0159 \pm 0.0016$   & $0.8042 \pm 0.0631$  & $0.0826 \pm 0.0087$ \\ \hline
        0  & 2  & $0.0235 \pm 0.0001$   & $1.1344 \pm 0.0119$  & $0.2692 \pm 0.0201$ \\ \hline
        0  & 3  & $0.0223 \pm 0.0005$   & $1.0478 \pm 0.0372$  & $0.2117 \pm 0.0072$ \\ \hline
        1  & 1  & $0.0222 \pm 0.0003$   & $1.0509 \pm 0.0112$  & $0.2118 \pm 0.0068$ \\ \hline
        1  & 2  & $0.0216 \pm 0.0003$   & $1.0061 \pm 0.0377$  & $0.2021 \pm 0.0128$ \\ \hline
        1  & 3  & $0.0224 \pm 0.0003$   & $1.0575 \pm 0.0121$  & $0.2212 \pm 0.0269$ \\ \hline
        2  & 1  & $0.0217 \pm 0.0004$   & $1.0176 \pm 0.0357$  & $0.2104 \pm 0.0178$ \\ \hline
        2  & 2  & $0.0218 \pm 0.0009$   & $1.0346 \pm 0.0571$  & $0.2005 \pm 0.0076$ \\ \hline
        2  & 3  & $0.0226 \pm 0.0006$   & $1.0754 \pm 0.0352$  & $0.2348 \pm 0.0242$ \\ \hline
        3  & 1  & $0.0224 \pm 0.0006$   & $1.0467 \pm 0.0402$  & $0.2226 \pm 0.0281$ \\ \hline
        3  & 2  & $0.0221 \pm 0.0009$   & $1.0564 \pm 0.0441$  & $0.2332 \pm 0.0291$ \\ \hline
        3  & 3  & $0.0217 \pm 0.0007$   & $1.0245 \pm 0.0414$  & $0.2049 \pm 0.0156$ \\ \hline
        4  & 1  & $0.0215 \pm 0.0006$   & $1.0131 \pm 0.0192$  & $0.1980 \pm 0.0146$ \\ \hline
        4  & 2  & $0.0219 \pm 0.0010$   & $1.0490 \pm 0.0673$  & $0.2229 \pm 0.0309$ \\ \hline
        4  & 3  & $0.0218 \pm 0.0017$   & $1.0459 \pm 0.0880$  & $0.2513 \pm 0.0229$ \\ \hline
    \end{tabular}
    \label{table:MLP1}
\end{table}


\paragraph{XGBoost}

Root Mean Square Error (RMSE), the standard loss function for regression problems is used to train the XGBoost models. Early-stopping based on \textbf{Pearson} correlation in the validation set is applied to control the model complexity if needed. A grid search is performed to select the data sub-sample and feature sub-sample ratios of the XGBoost models. 

\begin{itemize}
    \item Max Depth: 4,6,8 
    \item Data subsample: 0.25,0.5,0.75
    \item Feature subsample by tree: 0.25,0.5,0.75
    \item L1 regularisation: 0, 0.001, 0.01
    \item L2 regularisation: 0, 0.001, 0.01
\end{itemize}


Other hyper-parameters of the XGBoost models are fixed as follows. 
%% Sensible Defaults for XGBoost 
\begin{itemize}
    \item Grow policy: Depth-wise
    \item Number of boosting rounds: 5000
    \item Learning rate: 0.01
    \item Early stopping: 250
    %\item Max Leaves: 128 
    \item Min Samples per node: 10 
    \item Feature subsample by level/node: 1
\end{itemize}


%% Comparison of XGBoost performances over different parameters 
Table \ref{table:XGB1} compares performances of XGBoost models by different data subsample ratios, feature subsample ratios and max depth, mean and standard deviation over 45 models of the 9 combinations of L1 and L2 regularisation each with 5 different random seeds are reported.

Calmar ratio is the performance metric with the most variance, suggesting selecting models based on Calmar ratio is not robust. Mean Corr is the least varied metric between random seeds and therefore we use it for hyper-parameter selection. 

Models with data sub-sampling ratio of $75\%$ performed better than models with data sub-sampling ratio of $50\%$ and $25\%$, with a lower variance between model performances over different random seeds also. Models with feature sub-sampling ratio of $75\%$ also performed better. XGBoost models with max depth of 4 performed better than models with max depth of 6 and 8 for each fixed data and feature sub-sampling ratios. 

Table \ref{table:XGB2} compares performances of XGBoost models by different L1 and L2 regularisation and max depth with fixed data and feature sub-sampling ratio of $75\%$. Mean and standard deviation over 5 models with different random seeds are reported. There are no significant difference between model performances over different L1 and L2 regularisation when other model hyper-parameters are fixed. Therefore, we set the L1 and L2 regularisation penalty to be zero when training XGBoost models in the next section. 

In the next section which study the impact of different data sampling schemes of incremental learning models, the XGBoost parameters are set as follows. Selection is based on having the highest Mean Corr. 

\begin{itemize}
    \item Grow policy: Depth-wise
    \item Number of boosting rounds: 5000
    \item Learning rate: 0.01
    \item Early stopping: 250
    \item Min Samples per node: 10 
    \item Feature subsample by level/node: 1
    \item Max Depth: 4 
    \item Data subsample: 0.75
    \item Feature subsample by tree: 0.75
    \item L1 regularisation: 0
    \item L2 regularisation: 0
\end{itemize}



\begin{table}[!ht]
    \centering
    \caption{XGBoost models with different data subsample ratios, feature subsample ratios and max depths between Era 601 and 800}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
    Data Sample & Feature Sample & Depth & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                &                &       & mean      & std       & mean   & std    & mean   & std    \\ \hline
    0.25        & 0.25           & 4     & 0.0242    & 0.0014    & 1.2126 & 0.0992 & 0.3451 & 0.1237 \\ \hline
    0.25        & 0.25           & 6     & 0.0225    & 0.0018    & 1.1502 & 0.1034 & 0.3275 & 0.0727 \\ \hline
    0.25        & 0.25           & 8     & 0.0187    & 0.0015    & 1.0045 & 0.0904 & 0.2227 & 0.0858 \\ \hline
    0.25        & 0.5            & 4     & 0.0236    & 0.0014    & 1.1929 & 0.0706 & 0.2804 & 0.0413 \\ \hline
    0.25        & 0.5            & 6     & 0.0222    & 0.0014    & 1.1193 & 0.0825 & 0.2495 & 0.075  \\ \hline
    0.25        & 0.5            & 8     & 0.0189    & 0.0012    & 0.9999 & 0.066  & 0.23   & 0.0791 \\ \hline
    0.25        & 0.75           & 4     & 0.0249    & 0.0016    & 1.258  & 0.1066 & 0.3501 & 0.1275 \\ \hline
    0.25        & 0.75           & 6     & 0.0228    & 0.0013    & 1.1414 & 0.0666 & 0.2974 & 0.0864 \\ \hline
    0.25        & 0.75           & 8     & 0.0188    & 0.0023    & 0.9734 & 0.1425 & 0.1848 & 0.075  \\ \hline
    0.5         & 0.25           & 4     & 0.0259    & 0.0009    & 1.2751 & 0.055  & 0.3641 & 0.0809 \\ \hline
    0.5         & 0.25           & 6     & 0.0248    & 0.0012    & 1.2453 & 0.0768 & 0.3862 & 0.1135 \\ \hline
    0.5         & 0.25           & 8     & 0.0217    & 0.0018    & 1.1244 & 0.1076 & 0.3684 & 0.143  \\ \hline
    0.5         & 0.5            & 4     & 0.0267    & 0.001     & 1.3394 & 0.0908 & 0.4423 & 0.1279 \\ \hline
    0.5         & 0.5            & 6     & 0.0255    & 0.001     & 1.2733 & 0.0603 & 0.4521 & 0.1521 \\ \hline
    0.5         & 0.5            & 8     & 0.0224    & 0.0011    & 1.1622 & 0.063  & 0.4375 & 0.1299 \\ \hline
    0.5         & 0.75           & 4     & 0.0268    & 0.0011    & 1.3173 & 0.0842 & 0.413  & 0.0998 \\ \hline
    0.5         & 0.75           & 6     & 0.0255    & 0.0011    & 1.2716 & 0.075  & 0.4429 & 0.1468 \\ \hline
    0.5         & 0.75           & 8     & 0.0226    & 0.0014    & 1.1566 & 0.1021 & 0.4315 & 0.146  \\ \hline
    0.75        & 0.25           & 4     & 0.0265    & 0.0009    & 1.3146 & 0.0605 & 0.4388 & 0.0731 \\ \hline
    0.75        & 0.25           & 6     & 0.0268    & 0.0009    & 1.3439 & 0.0778 & 0.6006 & 0.2169 \\ \hline
    0.75        & 0.25           & 8     & 0.0235    & 0.0005    & 1.2071 & 0.048  & 0.5044 & 0.1404 \\ \hline
    0.75        & 0.5            & 4     & 0.027     & 0.0007    & 1.3345 & 0.0477 & 0.4345 & 0.0665 \\ \hline
    0.75        & 0.5            & 6     & 0.0271    & 0.0007    & 1.3469 & 0.0479 & 0.63   & 0.1526 \\ \hline
    0.75        & 0.5            & 8     & 0.0241    & 0.0012    & 1.234  & 0.0702 & 0.4843 & 0.2099 \\ \hline
    0.75        & 0.75           & 4     & 0.0273    & 0.0006    & 1.3624 & 0.0485 & 0.4885 & 0.1032 \\ \hline
    0.75        & 0.75           & 6     & 0.0267    & 0.0009    & 1.3373 & 0.0566 & 0.5509 & 0.1314 \\ \hline
    0.75        & 0.75           & 8     & 0.0237    & 0.0005    & 1.2369 & 0.065  & 0.5501 & 0.1776 \\ \hline
    \end{tabular}
    \label{table:XGB1}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{XGBoost models with different L1 and L2 regularisation with fixed data and feature sub-sampling ratios of $75\%$ between Era 601 and 800}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
    Max Depth & L2-reg & L1-reg & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
              &        &        & mean      & std       & mean   & std    & mean   & std    \\ \hline
    4         & 0.0    & 0.0    & 0.0276    & 0.0007    & 1.3829 & 0.0515 & 0.5305 & 0.1077 \\ \hline
    4         & 0.0    & 0.001  & 0.0274    & 0.0006    & 1.3764 & 0.0485 & 0.5161 & 0.1213 \\ \hline
    4         & 0.0    & 0.1    & 0.0268    & 0.0008    & 1.3283 & 0.0627 & 0.4224 & 0.0937 \\ \hline
    4         & 0.001  & 0.0    & 0.0276    & 0.0007    & 1.3829 & 0.0515 & 0.5305 & 0.1077 \\ \hline
    4         & 0.001  & 0.001  & 0.0274    & 0.0006    & 1.3764 & 0.0485 & 0.5161 & 0.1213 \\ \hline
    4         & 0.001  & 0.1    & 0.0268    & 0.0008    & 1.3283 & 0.0627 & 0.4224 & 0.0938 \\ \hline
    4         & 0.1    & 0.0    & 0.0271    & 0.0002    & 1.3489 & 0.0253 & 0.4771 & 0.0824 \\ \hline
    4         & 0.1    & 0.001  & 0.0274    & 0.0004    & 1.368  & 0.0358 & 0.4977 & 0.1186 \\ \hline
    4         & 0.1    & 0.1    & 0.0274    & 0.0004    & 1.3694 & 0.0362 & 0.4833 & 0.0918 \\ \hline
    6         & 0.0    & 0.0    & 0.0266    & 0.0008    & 1.3354 & 0.0453 & 0.5574 & 0.1282 \\ \hline
    6         & 0.0    & 0.001  & 0.0267    & 0.0011    & 1.3353 & 0.061  & 0.5422 & 0.1697 \\ \hline
    6         & 0.0    & 0.1    & 0.0267    & 0.0011    & 1.3201 & 0.0638 & 0.5665 & 0.1578 \\ \hline
    6         & 0.001  & 0.0    & 0.0265    & 0.0007    & 1.3347 & 0.0441 & 0.5711 & 0.1244 \\ \hline
    6         & 0.001  & 0.001  & 0.0268    & 0.0011    & 1.3403 & 0.0657 & 0.5184 & 0.1226 \\ \hline
    6         & 0.001  & 0.1    & 0.0267    & 0.0011    & 1.3201 & 0.0638 & 0.5665 & 0.1578 \\ \hline
    6         & 0.1    & 0.0    & 0.0269    & 0.0009    & 1.3551 & 0.0559 & 0.6187 & 0.1823 \\ \hline
    6         & 0.1    & 0.001  & 0.0267    & 0.0013    & 1.3307 & 0.0818 & 0.4871 & 0.1164 \\ \hline
    6         & 0.1    & 0.1    & 0.0269    & 0.0009    & 1.3638 & 0.0563 & 0.5304 & 0.0598 \\ \hline
    8         & 0.0    & 0.0    & 0.0237    & 0.0008    & 1.2226 & 0.0592 & 0.5127 & 0.0875 \\ \hline
    8         & 0.0    & 0.001  & 0.0238    & 0.0003    & 1.2445 & 0.0623 & 0.5269 & 0.1949 \\ \hline
    8         & 0.0    & 0.1    & 0.0237    & 0.0004    & 1.2408 & 0.0633 & 0.5298 & 0.0905 \\ \hline
    8         & 0.001  & 0.0    & 0.0238    & 0.0007    & 1.243  & 0.0942 & 0.5657 & 0.2615 \\ \hline
    8         & 0.001  & 0.001  & 0.0238    & 0.0004    & 1.2383 & 0.0254 & 0.4905 & 0.1505 \\ \hline
    8         & 0.001  & 0.1    & 0.0238    & 0.0003    & 1.2429 & 0.0748 & 0.6487 & 0.1551 \\ \hline
    8         & 0.1    & 0.0    & 0.0235    & 0.0006    & 1.2102 & 0.0655 & 0.6036 & 0.2693 \\ \hline
    8         & 0.1    & 0.001  & 0.024     & 0.0003    & 1.2461 & 0.0552 & 0.53   & 0.1934 \\ \hline
    8         & 0.1    & 0.1    & 0.0235    & 0.0008    & 1.2438 & 0.1056 & 0.5426 & 0.2102 \\ \hline
    \end{tabular}
    \label{table:XGB2}
\end{table}




\paragraph{XGBoost model snapshots}

Traditional machine learning research suggests there exists an optimal model complexity where the trade-off of bias and variance is optimal (for a loss function that behaves like the Mean-Squared error). However, modern machine learning research suggests using an over-parameterised model might improve performance in a test set even when model training loss cannot be further improved. The improvement is significant in cases where the model specification is incomplete and low signal-to-noise ratio in the given features. This counter-intuitive phenomenon is explored in different research papers \cite{Hastie19,NakkiranPreetum2021Dddw,Teresa22,chen2023learning,} from both theoretical and empirical perspectives. 

The two viewpoints are summarised as follows. 
\begin{itemize}
    \item Viewpoint 1: (Classical Approach): There exists an optimal model complexity which can be found by performing bootstrapping-like procedures, such as cross-validation. 
    \item Viewpoint 2: (Modern Approach): Over-parameterised model will outperform the optimal model under the classical approach. 
\end{itemize}


To validate the above viewpoints for our dataset, XGBoost models are trained \textbf{without} early-stopping for different number of boosting rounds and learning rates for 5 different random seeds. Three scenarios are considered: 
\begin{itemize}
    \item Small:    XGBoost models with 500 boosting rounds and learning rate 0.1
    \item Standard: XGBoost models with 5000 boosting rounds and learning rate 0.01
    \item Large:    XGBoost models with 50000 boosting rounds and learning rate 0.001 
\end{itemize}

%Note that in all three scenarios, the product of number of boosting rounds and learning rate is constant. In other words, we are studying the relationship between learning speed in the model training and its convergence. 

Other hyper-parameters of the XGBoost models are listed below. 

\begin{itemize}
    \item Grow policy: Depth-wise
    %\item Number of boosting rounds: 5000
    %\item Learning rate: 0.01
    \item Max Depth: 4 
    \item Min Samples per node: 10 
    \item Data subsample: 0.75
    \item Feature subsample by tree: 0.75
    \item Feature subsample by level/node: 1
    \item L1 regularisation: 0
    \item L2 regularisation: 0
\end{itemize}

Model snapshots are created by running the first $10\%,20\%,\dots,100\%$ of boosting rounds during inference. In figure \ref{fig:XGBSnapshotPlot}, the Mean Corr of XGBoost models over different stages of training process between Era 601 and Era 800 are reported. Detailed performances of model snapshots of the three scenarios are reported in tables \ref{table:XGBSnapshot1},\ref{table:XGBSnapshot2},\ref{table:XGBSnapshot3}. 

In all three scenarios, increasing the number of boosting rounds beyond $60\%$ of the training process does not significantly improve Mean Corr. Similarly, Sharpe and Calmar ratios improves as we continue adding new trees to the models but it is not significant after $60\%$ of the trees are added. It suggests that over-parameterised models might be able to reduce the Max Drawdown and other downside risks in predictions. However, it cannot be ruled out whether over-parameterised models performed better due to chance. 

The best model performance is achieved by the large XGBoost models which has the smallest learning rate, which means the slowest learning process. Improvement from small to standard XGBoost models is significant but the improvement from standard to large XGBoost models is not significant.


\begin{figure}[!ht]
    \centering
    \includegraphics{figure/chapter3/XGBSnapshotPlot.pdf}
    \caption{Mean Corr of XGBoost models between Era 601 and Era 800. 10 different model snapshots are taken for XGBoost models of different number of boosting rounds (500,5000,50000). Model are trained with 5 different random seeds. 95\% confidence intervals of Mean Corr for each model are shown in the plot.}
    \label{fig:XGBSnapshotPlot}
\end{figure}



\begin{table}[!ht]
    \centering
    \caption{Model Snapshots of small XGBoost Models (Max Boosting rounds is 500, Learning rate is 0.1) between Era 601 and 800}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Complexity & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
               & mean      & std       & mean   & std    & mean   & std    \\ \hline
    0.1        & 0.0189    & 0.0012    & 0.9611 & 0.0567 & 0.1951 & 0.0486 \\ \hline
    0.2        & 0.0215    & 0.0005    & 1.0681 & 0.0201 & 0.2311 & 0.0583 \\ \hline
    0.3        & 0.0222    & 0.0009    & 1.1115 & 0.0471 & 0.3132 & 0.151  \\ \hline
    0.4        & 0.0229    & 0.001     & 1.1852 & 0.0659 & 0.4139 & 0.1599 \\ \hline
    0.5        & 0.0235    & 0.0004    & 1.2405 & 0.0264 & 0.4275 & 0.0795 \\ \hline
    0.6        & 0.0237    & 0.0008    & 1.2458 & 0.0582 & 0.4707 & 0.1168 \\ \hline
    0.7        & 0.0236    & 0.0009    & 1.2608 & 0.053  & 0.5006 & 0.1628 \\ \hline
    0.8        & 0.0235    & 0.0014    & 1.2741 & 0.108  & 0.634  & 0.246  \\ \hline
    0.9        & 0.0238    & 0.0012    & 1.2884 & 0.0872 & 0.6521 & 0.266  \\ \hline
    1.0        & 0.0238    & 0.0011    & 1.3169 & 0.0874 & 0.6336 & 0.239  \\ \hline
    \end{tabular}
    \label{table:XGBSnapshot1}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{Model Snapshots of standard XGBoost Models (Max Boosting rounds is 5000, Learning rate is 0.01) between Era 601 and 800}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Complexity & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
               & mean      & std       & mean   & std    & mean   & std    \\ \hline
    0.1        & 0.0217    & 0.0001    & 1.0521 & 0.0075 & 0.2257 & 0.0193 \\ \hline
    0.2        & 0.0245    & 0.0002    & 1.1831 & 0.0175 & 0.3117 & 0.0371 \\ \hline
    0.3        & 0.0259    & 0.0003    & 1.2688 & 0.0212 & 0.3901 & 0.0621 \\ \hline
    0.4        & 0.0268    & 0.0002    & 1.3311 & 0.0253 & 0.4423 & 0.0674 \\ \hline
    0.5        & 0.0276    & 0.0003    & 1.3807 & 0.0345 & 0.52   & 0.1134 \\ \hline
    0.6        & 0.0281    & 0.0002    & 1.4149 & 0.0286 & 0.6229 & 0.1546 \\ \hline
    0.7        & 0.0284    & 0.0003    & 1.4409 & 0.0391 & 0.7143 & 0.1777 \\ \hline
    0.8        & 0.0285    & 0.0004    & 1.4536 & 0.0418 & 0.7891 & 0.1685 \\ \hline
    0.9        & 0.0285    & 0.0005    & 1.4656 & 0.0512 & 0.8137 & 0.1077 \\ \hline
    1.0        & 0.0285    & 0.0004    & 1.4695 & 0.0439 & 0.8141 & 0.0882 \\ \hline
    \end{tabular}
    \label{table:XGBSnapshot2}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{Model Snapshots of large XGBoost Models (Max Boosting rounds is 50000, Learning rate is 0.001) between Era 601 and 800}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Complexity & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
               & mean      & std       & mean   & std    & mean   & std    \\ \hline
    0.1        & 0.0221    & 0.0001    & 1.0685 & 0.0067 & 0.2424 & 0.0067 \\ \hline
    0.2        & 0.0249    & 0.0001    & 1.1926 & 0.0079 & 0.327  & 0.0148 \\ \hline
    0.3        & 0.0262    & 0.0001    & 1.2743 & 0.0082 & 0.3928 & 0.0173 \\ \hline
    0.4        & 0.0271    & 0.0001    & 1.3334 & 0.0072 & 0.4445 & 0.0304 \\ \hline
    0.5        & 0.0277    & 0.0001    & 1.3776 & 0.0089 & 0.51   & 0.0335 \\ \hline
    0.6        & 0.0282    & 0.0001    & 1.4151 & 0.0091 & 0.5884 & 0.0468 \\ \hline
    0.7        & 0.0285    & 0.0001    & 1.4422 & 0.0107 & 0.6638 & 0.0531 \\ \hline
    0.8        & 0.0288    & 0.0001    & 1.462  & 0.0092 & 0.7726 & 0.0825 \\ \hline
    0.9        & 0.0288    & 0.0001    & 1.4747 & 0.0063 & 0.8925 & 0.1111 \\ \hline
    1.0        & 0.0289    & 0.0001    & 1.4839 & 0.0084 & 1.0072 & 0.1164 \\ \hline
    \end{tabular}
    \label{table:XGBSnapshot3}
\end{table}


\paragraph{Conclusion} 

%% Factor Timing models under-performed. 
Factor timing models, which are based on learning the dynamics of weights from the feature correlations on each era under-performed other tabular models such as XGBoost and MLP. In fact, feature engineering models do not perform better than simple rule-based models when evaluated over a long enough timeframe. Exponential Moving Averages models have a strong and robust performance compared to other factor timing models despite their simplicity. The key hyper-parameter, weight decay can be selected using a deep incremental learning framework which adjusts dynamically over time. %All the factor-timing models considered above are highly correlated in both performances and predictions. 

The Numerai dataset demonstrates a very strong non-stationary and non-linear nature such that the linear weights of the regression model trained on each era cannot be effectively used to predict future weights. In other words, \textbf{linear} relationships between features and target is not stable such that forecast cannot be made accurately using historical values. It suggests the dataset would be better modelled with traditional tabular machine learning models, with regular updates on model weights, rather than using time series methods to learn the regression weights of a linear model for each era. 


%% Complex MLP models are not helpful 
Increasing complexity of MLP models cannot improve model performances as expected by the Modern ML viewpoint. The best performance is achieved by a standard MLP with two layers, which provides the minimal amount of non-linearity required so that the model does not degenerate to a ridge regression model. As suggested in \cite{Teresa22}, the performance of over-parameterised models are affected by a myriad of factors including model architecture and training process. It cannot be ruled out that there are other model architectures that can make deep learning models performing better than XGBoost. Only the most basic neural network architectures are considered here due to computational resources constraints. However, as suggested from research on bench-marking of tabular ML models \cite{Leo22}, recent deep learning models for tabular data such as TabNet does not always perform better than MLP models. It is unlikely there are advance neural network architectures that are efficient and performed better than MLP.  

%% XGBoost models performed better than MLP 
XGBoost models performed better than MLP models over a wide range of hyper-parameters. The binned nature of features favours the use of decision trees over neural networks. Therefore, we do not consider the use of MLP in building deep model ensemble for the Numerai dataset below. It is possible in other applications MLP models performed on par or even better than GBDT models, and therefore MLP should still be included in the general framework, unless there are significant evidence for their under-performance or larger computational costs. Other advanced neural architectures are not explored here given their high variation of performances over random seeds \cite{gundersen2023sources,} and difficulty of hyper-parameters tuning.


%% Early Stopping for XGBoost 
A key design choice for XGBoost models is whether to apply early-stopping, as it reflects two different viewpoints (Classical vs Modern) towards optimal model complexity. In the above example, over-parameterised XGBoost models did not lead to a drop of model performances as expected by classical ML viewpoint. However, over-parameterised models also did not lead to a significant improvement of model performances. Model performances converges once a certain complexity threshold is reached, which is $60\%$ of the training process in this case. Therefore, the benefit of applying early-stopping to XGBoost models here is mainly for reducing computational time, instead of regularising model complexity. Models snapshots for XGBoost models can be obtained without any additional memory costs, unlike snapshots for deep learning models \cite{huang2017snapshot,}.  Therefore, combining model snapshots taken in the training process where performances starts to converge can create better predictions with little additional costs.




\subsection{Incremental Learning model for XGBoost models} 

The XGBoost models with the optimised design choices (based on data up to Era 800) are used in incremental learning models with different training sizes and retrain periods. Train size is how many eras of data are used to create the training and validation set. retrain period represents how often the model parameters are updated with the latest data. A retrain period of 100 means the model is retrained every 100th era. 

\subsubsection{XGBoost models with different retrain periods (model update frequencies) and train sizes} 

The train size of models is set to be 585,685,785. Numerai recommended a train set of size 574 \cite{numerai-datav4.1} and therefore we do not study in detailed train size smaller than 585. The retrain period is set to be 200,100,50,25 respectively, which represents updating model every 4,3,2,1 years. 


\paragraph{Validation}

Tables \ref{table:XGBInc585validate},\ref{table:XGBInc685validate},\ref{table:XGBInc785validate} list the performances of the XGBoost models trained with the same optimised hyper-parameters between validation period Era 801 and Era 900 over 5 different random seeds. For XGBoost models of a fixed training set size, retraining models more frequently improves model performances but the effect is not significant when the retrain period is shorter than 50. The best model performance is achieved using a train size of 585, suggesting more data does not always improve model performances. For a given train size, the best performances are obtained by setting the retrain period to be the smallest (25). 


\begin{table}[!ht]
    \centering
    \caption{XGBoost Models with different retrain periods, train size is fixed to 585 in validation period (Era 801 to 900)}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Retrain Period & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                   & mean      & std       & mean   & std    & mean   & std    \\ \hline
    25             & 0.0213    & 0.0009    & 1.1748 & 0.0591 & 0.222  & 0.0284 \\ \hline
    50             & 0.021     & 0.0005    & 1.1698 & 0.0564 & 0.2176 & 0.0293 \\ \hline
    100            & 0.0196    & 0.001     & 1.0235 & 0.061  & 0.2035 & 0.0318 \\ \hline
    200            & 0.0196    & 0.001     & 1.0235 & 0.061  & 0.2035 & 0.0318 \\ \hline
    %400            & 0.0147    & 0.0008    & 0.7149 & 0.0385 & 0.1147 & 0.0143 \\ \hline
    \end{tabular}
    \label{table:XGBInc585validate}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{XGBoost Models with different retrain periods, train size is fixed to 685 in validation period (Era 801 to 900)}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Retrain Period & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                   & mean   & std    & mean   & std    & mean   & std    \\ \hline
    25             & 0.0206 & 0.0007 & 1.0936 & 0.0397 & 0.2426 & 0.0228 \\ \hline
    50             & 0.02   & 0.0006 & 1.0358 & 0.0358 & 0.2409 & 0.0269 \\ \hline
    100            & 0.0196 & 0.0008 & 1.0049 & 0.0299 & 0.2406 & 0.0256 \\ \hline
    200            & 0.0168 & 0.001  & 0.8366 & 0.0559 & 0.1184 & 0.0217 \\ \hline
    %400            & 0.0168 & 0.001  & 0.8366 & 0.0559 & 0.1184 & 0.0217 \\ \hline
    \end{tabular}
    \label{table:XGBInc685validate}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{XGBoost Models with different retrain periods, train size is fixed to 785 in validation period (Era 801 to 900)}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Retrain Period & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                   & mean   & std    & mean   & std    & mean   & std    \\ \hline
    25             & 0.0197 & 0.0007 & 1.0339 & 0.0554 & 0.1778 & 0.0321 \\ \hline
    50             & 0.0192 & 0.0007 & 0.9576 & 0.0593 & 0.1695 & 0.0322 \\ \hline
    100            & 0.0189 & 0.001  & 0.9435 & 0.0711 & 0.1677 & 0.0342 \\ \hline
    200            & 0.0189 & 0.001  & 0.9435 & 0.0711 & 0.1677 & 0.0342 \\ \hline
    %400            & 0.0189 & 0.001  & 0.9435 & 0.0711 & 0.1677 & 0.0342 \\ \hline
    \end{tabular}
    \label{table:XGBInc785validate}
\end{table}

\paragraph{Test}

Tables \ref{table:XGBInc585test},\ref{table:XGBInc685test},\ref{table:XGBInc785test} list the performances of the XGBoost models trained with the same optimised hyper-parameters between test period Era 901 and Era 1050 over 5 different random seeds. XGBoost models with a train size 585 are still performing the better than those with train sizes 685 and 785. There are no significant difference in performances between XGBoost models of train size 585 with a retrain period of 25 or 50. Setting the retrain period to be the smallest obtain the highest Mean Corr (for a given train size) when train size is 585 or 685, but not when train size is 785. 


\begin{table}[!ht]
    \centering
    \caption{XGBoost Models with different retrain periods, train size is fixed to 585 in test period (Era 901 to 1050)}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Retrain Period & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                   & mean   & std    & mean   & std    & mean   & std    \\ \hline
    25             & 0.024  & 0.0004 & 1.0812 & 0.025  & 0.285  & 0.0464 \\ \hline
    50             & 0.0238 & 0.0008 & 1.0978 & 0.0323 & 0.2418 & 0.017  \\ \hline
    100            & 0.0222 & 0.0006 & 1.0058 & 0.0282 & 0.2256 & 0.017  \\ \hline
    200            & 0.0226 & 0.0006 & 1.0607 & 0.0094 & 0.3432 & 0.0221 \\ \hline
    %400            & 0.0231 & 0.0006 & 1.2016 & 0.0152 & 0.3766 & 0.045  \\ \hline
    \end{tabular}
    \label{table:XGBInc585test}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{XGBoost Models with different retrain periods, train size is fixed to 685 in test period (Era 901 to 1050)}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Retrain Period & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                   & mean   & std    & mean   & std    & mean   & std    \\ \hline
    25             & 0.0225 & 0.0003 & 1.054  & 0.0386 & 0.2576 & 0.0104 \\ \hline
    50             & 0.0219 & 0.0003 & 1.0192 & 0.0236 & 0.2558 & 0.0376 \\ \hline
    100            & 0.0211 & 0.0006 & 0.9465 & 0.0196 & 0.2467 & 0.0378 \\ \hline
    200            & 0.0213 & 0.0008 & 0.99   & 0.04   & 0.2591 & 0.0577 \\ \hline
    %400            & 0.021  & 0.0004 & 1.0184 & 0.0231 & 0.1656 & 0.0291 \\ \hline
    \end{tabular}
    \label{table:XGBInc685test}
\end{table}


\begin{table}[!ht]
    \centering
    \caption{XGBoost Models with different retrain periods, train size is fixed to 785 in test period (Era 901 to 1050)}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Retrain Period & Mean Corr & Mean Corr & Sharpe & Sharpe & Calmar & Calmar \\ \hline
                   & mean   & std    & mean   & std    & mean   & std    \\ \hline
    25             & 0.0219 & 0.0006 & 1.0134 & 0.0263 & 0.2487 & 0.0133 \\ \hline
    50             & 0.0231 & 0.0004 & 1.1224 & 0.0294 & 0.3144 & 0.0411 \\ \hline
    100            & 0.0225 & 0.0007 & 1.0799 & 0.039  & 0.3063 & 0.0376 \\ \hline
    200            & 0.0238 & 0.0005 & 1.2507 & 0.0301 & 0.5173 & 0.0799 \\ \hline
    % 400            & 0.0223 & 0.0003 & 1.1397 & 0.0219 & 0.2294 & 0.0138 \\ \hline
    \end{tabular}
    \label{table:XGBInc785test}
\end{table}



\paragraph{Discussion} 

For incremental learning tasks, data sampling schemes has a significant impact on model performances. The above XGBoost models demonstrated a wide range of performances, with some sampling schemes significantly better than others. All the XGBoost models have the \textbf{same} hyper-parameters except changing how the training data is sampled and how frequent the models are retrained. This suggests when bench-marking machine learning models for incremental learning tasks, it is important to report model performances over different data sampling settings to give a more complete picture of model performances under different use cases. 

A useful empirical rule to select training size is to ensure the training set is large enough to cover different data regimes that are possible to come up in future. Once the training set includes enough data samples from different regimes, further increase in train size will have little improvement to model performance. Setting a small retrain period (updating models more frequently) can improve model performances in most settings. This does not hold when train size is the largest (785). The optimal data sampling scheme depends on the speed of distribution shifts in the data. For fast-moving drifts, sampling schemes that uses a shorter training size or more frequent model retrains/updates will give a better performances. For slow-moving or recurrent drifts \cite{Gama14}, sampling schemes that uses a longer training size or less frequent model retrains/updates will give more robust performances. 


\section{Deep Incremental Learning Models}
\label{section:paradigm-ML} 

\begin{comment}
    Compare the model peformances 
    1. Dynamic Model Selection based on recent performances of Top N Model
    2. Deep Incremental Learning based on ensemble of model 
\end{comment}



Both stock market prediction and weather forecast shares similar challenges of working with complex systems, which are inherently unpredictable over long enough horizons due to their chaotic nature. Ensemble forecasting \cite{Thoppil2021,} is widely used in weather forecast to improve robustness of predictions. Instead of creating predictions based on a single set of data/parameters, multiple sets of data/parameters are used to create a range of scenarios which represents possible trajectories of the weather. Here, we introduce deep incremental learning models, which combine models trained with different data sampling methods and hyper-parameters to obtain more robust predictions. We study two methods to ensemble XGBoost models. One is to combine XGBoost models with different feature sampling schemes. The other is to combine XGBoost model snapshots with different model complexity. 


\paragraph{Deep XGBoost ensembles with different feature sampling methods}

20 XGBoost models with different feature sampling schemes. In each sampling scheme, we randomly sample $50\%$ features globally and then train 5 models with different random seeds. The train size is fixed at 585 and the models are retrained every 50 eras. 

Other hyper-parameters of the XGBoost models are set as follows. 

\begin{itemize}
    \item Grow policy: Depth-wise
    %\item Number of boosting rounds: 5000
    %\item Early Stopping 250
    %\item Learning rate: 0.01
    \item Max Depth: 4 
    %\item Max Leaves: 128 
    \item Min Samples per node: 10 
    \item Data subsample: 0.75
    \item Feature subsample by tree: 0.75
    \item Feature subsample by level/node: 1
    \item L1 regularisation: 0
    \item L2 regularisation: 0 
\end{itemize}

A deep XGBoost model is created by combining the 20 models incrementally. For every 10th era, a shallow XGBoost model with the following hyper-parameters is trained using model predictions from the most recent 185 eras (with suitable data lag). Monotonic constraints are imposed so that we do not assign negative weights to the model predictions. This is repeated over 5 times over different random seeds.

%% Layer 2 XGBoost 
\begin{itemize}
    \item Grow policy: Depth-wise
    \item Number of boosting rounds: 20
    \item Early Stopping 20
    \item Learning rate: 0.1
    \item Max Depth: 4 
    %\item Max Leaves: 128 
    \item Min Samples per node: 10 
    \item Data subsample: 0.75
    \item Feature subsample by tree: 0.75
    \item Feature subsample by level/node: 1
    \item L1 regularisation: 0
    \item L2 regularisation: 0 
\end{itemize}


XGBoost models are trained with early-stopping for different number of boosting rounds and learning rates for 5 different random seeds. Three scenarios are considered: 
\begin{itemize}
    \item Small:    XGBoost models with 500 boosting rounds and learning rate 0.1, early-stopping at 25 rounds
    \item Standard: XGBoost models with 5000 boosting rounds and learning rate 0.01, early-stopping at 250 rounds
    \item Large:    XGBoost models with 50000 boosting rounds and learning rate 0.001, early-stopping at 2500 rounds
\end{itemize}


In tables \ref{table:XGBDeep500}, \ref{table:XGBDeep5000} and \ref{table:XGBDeep50000}, the performance from the deep XGBoost models in Layer 2 (Deep) in Era 801 to Era 1050 are reported. The performance of XGBoost models in Layer 1 averaged over each sampling method(Sampling 1, Sampling 2, Sampling 3, Sampling 4) are also reported. 

For small XGBoost models, performances between models trained with different sampling schemes are not significantly different. However for standard XGBoost models, performances between models trained with different sampling schemes are significantly different (p-value of t-test comparing Sampling 1 and Sampling 3 is 0.001). For large XGBoost models, performances between models trained with different feature sampling schemes are even more significantly different. As variance between model performances reduces with more boosting rounds, most of the randomness in training process in large XGBoost models are removed. The difference of model performances in large XGBoost models can be attributed to the difference in training data used, rather than model hyper-parameters or the training process. This demonstrated the importance of using consistent data sampling schemes when bench-marking machine learning models.

Deep model ensemble can improves model performances of small XGBoost models significantly but not for standard XGBoost models. The findings are similar to the above where we trained deep XGBoost models with different model snapshots. 

\begin{table}[!ht]
    \centering
    \caption{Small XGBoost models with different feature sampling schemes (500 boosting rounds) }
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        Sampling 1  & $0.0163 \pm 0.0009$   & $0.8036 \pm 0.0735$  & $0.1151 \pm 0.0392$ \\ \hline
        Sampling 2  & $0.0166 \pm 0.0009$   & $0.8377 \pm 0.0528$  & $0.1155 \pm 0.0338$ \\ \hline
        Sampling 3  & $0.0168 \pm 0.0016$   & $0.8174 \pm 0.0762$  & $0.1344 \pm 0.0346$ \\ \hline
        Sampling 4  & $0.0168 \pm 0.0019$   & $0.8252 \pm 0.1104$  & $0.1215 \pm 0.0284$ \\ \hline
        Deep        & $0.0212 \pm 0.0002$   & $0.9813 \pm 0.0206$  & $0.1460 \pm 0.0103$ \\ \hline
    \end{tabular}
    \label{table:XGBDeep500}
\end{table}


\begin{table}[!ht]
    \centering
    \caption{Standard XGBoost models with different feature sampling schemes (5000 boosting rounds) }
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        Sampling 1  & $0.0227 \pm 0.0006$   & $1.1156 \pm 0.0278$  & $0.2247 \pm 0.0170$ \\ \hline
        Sampling 2  & $0.0216 \pm 0.0003$   & $1.0877 \pm 0.0321$  & $0.2957 \pm 0.0646$ \\ \hline
        Sampling 3  & $0.0208 \pm 0.0006$   & $0.9997 \pm 0.0411$  & $0.2330 \pm 0.0671$ \\ \hline
        Sampling 4  & $0.0213 \pm 0.0005$   & $1.0763 \pm 0.0379$  & $0.2295 \pm 0.0339$ \\ \hline
        Deep        & $0.0232 \pm 0.0002$   & $1.1339 \pm 0.0054$  & $0.2454 \pm 0.0046$ \\ \hline
    \end{tabular}
    \label{table:XGBDeep5000}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{Large XGBoost models with different feature sampling schemes (50000 boosting rounds) }
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        Sampling 1  & $0.0233 \pm 0.0001$   & $1.1449 \pm 0.0111$  & $0.2751 \pm 0.0084$ \\ \hline
        Sampling 2  & $0.0221 \pm 0.0002$   & $1.1084 \pm 0.0121$  & $0.3313 \pm 0.0144$ \\ \hline
        Sampling 3  & $0.0219 \pm 0.0001$   & $1.0582 \pm 0.0106$  & $0.2906 \pm 0.0232$ \\ \hline
        Sampling 4  & $0.0214 \pm 0.0001$   & $1.1025 \pm 0.0092$  & $0.2950 \pm 0.0233$ \\ \hline
        Deep        & $0.0235 \pm 0.0001$   & $1.1339 \pm 0.0088$  & $0.3369 \pm 0.0122$ \\ \hline
    \end{tabular}
    \label{table:XGBDeep50000}
\end{table}




\paragraph{Deep XGBoost ensembles with different model complexity} 


XGBoost models are trained without early-stopping for different number of boosting rounds and learning rates for 5 different random seeds. Three scenarios are considered: 
\begin{itemize}
    \item Small:    XGBoost models with 500 boosting rounds and learning rate 0.1
    \item Standard: XGBoost models with 5000 boosting rounds and learning rate 0.01
    \item Large:    XGBoost models with 50000 boosting rounds and learning rate 0.001 
\end{itemize}

As we are not using early-stopping in model training, it is not necessary to set $25\%$ of data as validation as above. For the given training set size of 585, we use the most recent $100\%$(all), $75\%$ and $50\%$ of the 585 eras of data to train XGBoost models. For each XGBoost model, 10 model snapshots are collected at regular intervals of the training process as above. The 150 predictions are then combined in the second layer of the deep incremental model. For every 10th era, a shallow XGBoost model with the following hyper-parameters is trained using model predictions from the most recent 185 eras (with suitable data lag). Monotonic constraints are imposed so that we do not assign negative weights to the model predictions. The hyper-parameters of the XGBoost models are as above. 

In tables \ref{table:XGBComplexity500},  \ref{table:XGBComplexity5000} and \ref{table:XGBComplexity50000}, the performance from the deep XGBoost model in Layer 2 (Deep) is compared with different XGBoost snapshots trained using different amount of training data ($50\%$,$75\%$,$100\%$) in Layer 1 (Base) in Era 801 to Era 1050. 


Using deep model ensemble can improve model performances for both small and standard XGBoost models. Improvement for small XGBoost models is more significant than standard XGBoost models. Variance of model performances are also significantly reduced. For standard XGBoost models, improvement on Mean Corr compared to models trained with all data in Layer 1 is not significant but the improvement on Calmar ratios are significant (p-value of t-test is 0.0039). The improvement from standard to large XGBoost models is not significant.  Figure \ref{fig:XGBSamplingPlot} shows that model performances converges after $80\%$ of boosting rounds are added for XGBoost models of different sizes (small,standard and large). Detailed model performance can be found in tables \ref{table:XGBComplexity500Full},  \ref{table:XGBComplexity5000Full} and \ref{table:XGBComplexity50000Full}.




\begin{table}[!ht]
    \centering
    \caption{Small XGBoost models with different model complexity (500 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        $50\%$   & $0.0126 \pm 0.0013$   & $0.6526 \pm 0.0714$  & $0.0999 \pm 0.0308$ \\ \hline
        $75\%$   & $0.0160 \pm 0.0016$   & $0.7795 \pm 0.0940$  & $0.1352 \pm 0.0452$ \\ \hline
        $100\%$  & $0.0199 \pm 0.0018$   & $0.9817 \pm 0.1212$  & $0.2110 \pm 0.0839$ \\ \hline
        Deep     & $0.0224 \pm 0.0002$   & $1.0531 \pm 0.0268$  & $0.3112 \pm 0.0089$ \\ \hline
    \end{tabular}
    \label{table:XGBComplexity500}
\end{table}


\begin{table}[!ht]
    \centering
    \caption{Standard XGBoost models with different model complexity (5000 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        $50\%$   & $0.0164 \pm 0.0011$   & $0.7498 \pm 0.0585$  & $0.1361 \pm 0.0363$ \\ \hline
        $75\%$   & $0.0191 \pm 0.0017$   & $0.8501 \pm 0.1011$  & $0.1642 \pm 0.0484$ \\ \hline
        $100\%$  & $0.0232 \pm 0.0016$   & $1.0280 \pm 0.0943$  & $0.2297 \pm 0.0848$ \\ \hline
        Deep     & $0.0235 \pm 0.0002$   & $1.0623 \pm 0.0103$  & $0.3455 \pm 0.0173$ \\ \hline
    \end{tabular}
    \label{table:XGBComplexity5000}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{Large XGBoost models with different model complexity (50000 boosting rounds)}
    \begin{tabular}{|l|l|l|l|}
    \hline
        Method  & Mean Corr  & Sharpe  & Calmar  \\ \hline
        $50\%$   & $0.0164 \pm 0.0008$   & $0.7460 \pm 0.0524$  & $0.1408 \pm 0.0307$ \\ \hline
        $75\%$   & $0.0195 \pm 0.0017$   & $0.8614 \pm 0.0976$  & $0.1619 \pm 0.0390$ \\ \hline
        $100\%$  & $0.0236 \pm 0.0015$   & $1.0410 \pm 0.0932$  & $0.2342 \pm 0.0761$ \\ \hline
        Deep     & $0.0238 \pm 0.0002$   & $1.0788 \pm 0.0076$  & $0.3422 \pm 0.0303$ \\ \hline
    \end{tabular}
    \label{table:XGBComplexity50000}
\end{table}


\begin{figure}
    \centering
    \includegraphics{figure/chapter3/XGBSamplingPlot.pdf}
    \caption{Mean Corr of XGBoost models trained with all 585 eras of data with regular updates every 50 eras during test period (Era 801 to Era 1050). 10 different model snapshots are taken for XGBoost models of different number of boosting rounds (500,5000,50000). Model are trained with 5 different random seeds. 95\% confidence intervals of Mean Corr for each model are shown in the plot.}
    \label{fig:XGBSamplingPlot}
\end{figure}




\paragraph{Summary} 

%% Improvement by deep model ensemble
Model ensemble is an effective way to improve the robustness of model predictions, in particular when the base learners are not strong predictors on their own, such as the case with small XGBoost models. For both methods of model ensemble (over feature sampling methods and over model snapshots), different performance metrics (Mean Corr, Sharpe, Calmar) improved with variance reduced significantly also. As expected, improvement is more significant on small XGBoost models than standard and large XGBoost models.  

%% Size of models 
XGBoost and other GBDT models are robust towards the number of boosting rounds. Unlike neural network models, where over-parameterised networks are difficult to train and leads to performance deterioration. XGBoost models are monotonic increasing and converging with respect to the number of boosting rounds in the three scenarios studied above. Therefore, when choosing the number of boosting rounds for GBDT models, it is better to pick a large number that is within the computational budget and then apply a small learning rate to prevent early convergence. 

%% Soft Hyper-parameter selection 
Deep model ensemble over model snapshots performed better than deep model ensemble over different feature sampling schemes. Improvement in Calmar ratio is significant for small and standard XGBoost models (p-value for small $<0.0001$, p-value for standard $<0.0001$). Mean Corr and Sharpe ratios of different deep model ensemble is not significantly different. As early-stopping is not required when using deep model ensemble over model snapshots, we can use more data to train models in Layer 1, this leads to an improvement of model performances in both Layer 1 and Layer 2. Under the deep incremental learning framework, model complexity is optimised dynamically in Layer 2, rather than through early-stopping of model training in Layer 1. This is an example of soft hyper-parameter selection, where model weights are assigned across models trained with small differences in hyper-parameters (In this example, the number of boosting rounds are varied while other hyper-parameters are unchanged). Soft hyper-parameter selection are more flexible than traditional hard hyper-parameter selection based on train/validation/test splits and this is why it can improve model performances. 

%% Diversified incremental learning models 
For incremental learning tasks, diversified models can be created using the degrees of freedom that is unrelated to a particular machine learning methods(random seeds, retrain period, global feature and data sampling) and model designs that are applicable to a wide range of models (model snapshots over different complexities). For a fair comparison of machine learning methods, it is not enough to simply compare the methods with the same set of data cross-validations but also under different data sampling schemes and model complexity regimes. Reporting the aggregated performances over different training settings would provide more useful insights into the real-life performances of prediction systems. 


\section{Discussion} 

%% Summary
In this study, both traditional tabular and factor-timing models are studied for the incremental learning problem on the temporal tabular dataset from Numerai. Traditional tabular models, if retrained regularly can adapt to distribution shifts in data which cannot be modelled by factor-timing models effectively. 

%% GBDT is a robust choice
We found that GBDT models is the best candidate of machine learning methods for the Numerai datasets, agreeing with the findings of \cite{mcelfresh2023neural,}, which demonstrate the robust and superior performances of GBDT models on large datasets. This is also partly due to the nature of features, being binned values from continuous underlying measures, which favours models based on decision rules rather than regression. Some hyper-parameters of XGBoost models, such as data and feature sub-sampling in tree building have significant effects on model performances. However, changing the number of boosting rounds used in model snapshots does not change model performances much, which is contrary to the mainstream belief that model complexity and regularisation are key to out-of-sample performances. With suitable designs of the training process, such as a slow learning rate with a large number of boosting rounds, we can train XGBoost models with good performances over a large part of the training process, slowly converging to the theoretical optimal. 


%% Data sampling 
Data management and forgetting mechanism is an integrated part of an incremental learning pipeline \cite{Gama14} to build robust prediction models on a data stream. Data sampling methods, such as training sizes and retrain periods can have significant effects on model performances. The impact of data sampling methods are usually over-looked in most quantitative finance research and even in hedge funds \cite{hoffstein2020rebalance}. This is an example of the stability-plasticity dilemma \cite{carpenter1993normal,} in incremental problems, which is the trade-off between the ability of machine learning models to adapt to new patterns and preserve existing knowledge. It is not known in advance which data sampling method will have the optimal performance and therefore it is better to select those dynamically. Combining models trained with different data sampling methods can improve the robustness of model performances even without changing any hyper-parameters of the machine learning models. When researchers are reporting model performances for prediction tasks on temporal tabular datasets, it is better to report the ensemble performances over different data sampling methods and random seeds to remove unwanted variances due to randomness in data. 

%% Random Feature Sampling as boostrap
We use random feature sampling to study the variance in data generation process with little computational costs. Compared to recent approaches which use deep generative methods to create simulated data, bootstrapping like sampling methods have the distinct advantage of not introducing any assumptions or bias on the data generation process, as the process is purely data driven. 


%% Model Complexity
The Numerai datasets used in this paper showed that \textbf{both} viewpoints on model complexity are incomplete pictures of the reality of data modelling. The classical viewpoint expects over-fitting of the model after a certain number of boosting rounds. Mean Corr drops slightly as the number of boosting rounds increases but the Sharpe ratio is not changed significantly. The robustness of models improves as the Calmar ratio increases when models become larger. This suggests rather than being an over-fitted model, larger models are more robust towards downside risks. Modern ML viewpoint is also not correct as both deep factor-timing models based on Signature and random Fourier transforms under-performed simple trend-based models. Complex MLP architectures also under-performed a simple MLP model with 2 layers. This observation is unexpected since these methods are used as examples in research papers advocating the modern ML viewpoint \cite{kelly2022virtue,belkin2019reconciling,}. 

For XGBoost models of different numbers of boosting rounds, snapshots taken after $20\%$ of boosting rounds are all useful candidates to be used in an ensemble. It is not possible to reject the null hypothesis that model snapshots with at least $20\%$ of boosting rounds have equal performances based on results from 5 different random seeds. 


Not all methods of increasing model complexities can improve model performances. For factor-timing models, increasing the size of random feature sets or increasing the number of layers in transformers layers are inefficient ways of data learning. For transformer models, there is no clear relationship between model complexities and performances, results from hyper-parameter optimisation are not robust and might not be useful for future predictions. For MLP models, increasing the number of layers does not always improve model performance. The optimal model complexity depends on both the architecture and training process. For XGBoost models, increasing the complexity of a \textbf{single} model by disabling early-stopping does not lead to significant improvement or deterioration of model performances. There is not enough evidence supporting or against the use of early-stopping based on model performances alone. However, increasing model complexity horizontally through different model ensemble methods such as bagging is more robust than training a single large model as there is fewer risks in over-fitting the data over different models. Not all approaches to building complex models are equal. Some approaches are more robust than others based on both theoretical and empirical findings. 

There is no simple rule to select whether to use classic or over-parameterised machine learning models. The optimal model complexity depends on the training process and datasets for a \textbf{static} machine learning problem. Within an incremental learning framework, training multiple models with different complexities and then applying soft hyper-parameter selection within a deep model framework is better than using cross-validation to select a \textbf{single} set of 'optimal' hyper-parameters. 

Incremental learning is an effective way to adapt to distribution shifts in data. Both the training size and model retrain/update period should be selected based on walk-forward prediction performances of the early parts of the data stream. The training size needs to be large enough to cover different data regimes in history and not too large to include old data that are no longer relevant. In general, increasing the model retrain period can improve model performances but the requirements on computational resources also increase. Therefore, trade-offs between computational costs and the marginal gain in model performances are made for practical incremental learning systems. For incremental learning problems, more recent data can be used in model training if early-stopping is not used. 


%% Further Work 
In most practical applications, \textbf{multiple} machine learning methods are used together to create an ensemble prediction. The incremental learning model presented in this paper provides a comprehensive way to integrate different machine learning models in a consistent and systematic way to create point-in-time predictions. With a multi-layer structure and modularised design within each layer, the deep incremental learning model can flexibly model datasets with different complexities and structures. Further work can be done by integrating different deep tabular models into the model and bench-marking different machine learning methods under the incremental learning framework. 

%% Crowd-sourcing 
For crowd-sourcing scientific projects in which submissions from the public are combined to form a meta-model, the deep incremental learning model presented here provided a mechanism to combine many weak models into a strong ensemble. As each component model considered in this study has a very low hardware requirement (only a single GPU is required), this is accessible to most data scientists on Kaggle or other data science competitions. Indeed, the promising results shown in this paper using only standard approaches (XGBoost, MLP) on a dataset with real use cases suggests even if the data scientists in the crowd-sourcing projects might not be aware of the latest advances in machine learning, they can still contribute towards different projects in a meaningful way. 





\section{Acknowledgements}
This was supported in part by the Wellcome Trust under Grant 108908/B/15/Z and by the EPSRC under grant EP/N014529/1 funding the EPSRC Centre for Mathematics of Precision Healthcare at Imperial. We thank Numerai GP, LLC for providing the datasets used in the study.  


\section{Data and Code Availability} 
The data and code used in this paper are available at \url{https://github.com/barahona-research-group/THOR-2}.



%Bibliography
\newpage
\printbibliography



\newpage 
\section{Supplementary Information}


\subsection{Detailed results of deep XGBoost models with different model snapshots} 

\begin{table}[!ht]
    \centering
    \caption{Small XGBoost models with different model complexity (500 boosting rounds) over different sampling and model snapshots}
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
    Sampling & Complexity & Mean Corr   & Mean Corr   & Sharpe & Sharpe & Calmar & Calmar \\ \hline
             &            & mean   & std    & mean   & std    & mean   & std    \\ \hline
    0.5      & 0.1        & 0.0104 & 0.0011 & 0.5422 & 0.0715 & 0.0504 & 0.007  \\ \hline
    0.5      & 0.2        & 0.0113 & 0.0007 & 0.5689 & 0.041  & 0.071  & 0.0188 \\ \hline
    0.5      & 0.3        & 0.0121 & 0.0004 & 0.6272 & 0.0321 & 0.0906 & 0.0192 \\ \hline
    0.5      & 0.4        & 0.0126 & 0.0007 & 0.6536 & 0.0333 & 0.0993 & 0.0191 \\ \hline
    0.5      & 0.5        & 0.013  & 0.0009 & 0.6712 & 0.0447 & 0.1053 & 0.0134 \\ \hline
    0.5      & 0.6        & 0.0133 & 0.0009 & 0.6918 & 0.039  & 0.1094 & 0.009  \\ \hline
    0.5      & 0.7        & 0.0135 & 0.0009 & 0.695  & 0.0456 & 0.1076 & 0.0163 \\ \hline
    0.5      & 0.8        & 0.0134 & 0.0011 & 0.6899 & 0.0578 & 0.1181 & 0.0308 \\ \hline
    0.5      & 0.9        & 0.0134 & 0.0012 & 0.6916 & 0.0683 & 0.1275 & 0.0386 \\ \hline
    0.5      & 1.0        & 0.0134 & 0.0012 & 0.6943 & 0.0689 & 0.1198 & 0.035  \\ \hline
    0.75     & 0.1        & 0.0124 & 0.0017 & 0.581  & 0.0846 & 0.0604 & 0.0174 \\ \hline
    0.75     & 0.2        & 0.0145 & 0.0013 & 0.6942 & 0.0765 & 0.1027 & 0.0282 \\ \hline
    0.75     & 0.3        & 0.0156 & 0.0008 & 0.745  & 0.0527 & 0.1286 & 0.0427 \\ \hline
    0.75     & 0.4        & 0.0162 & 0.0004 & 0.7736 & 0.0216 & 0.1264 & 0.0396 \\ \hline
    0.75     & 0.5        & 0.0166 & 0.0005 & 0.7979 & 0.0221 & 0.1393 & 0.0421 \\ \hline
    0.75     & 0.6        & 0.0169 & 0.0004 & 0.8283 & 0.0237 & 0.154  & 0.0249 \\ \hline
    0.75     & 0.7        & 0.0169 & 0.0005 & 0.831  & 0.0264 & 0.1442 & 0.0298 \\ \hline
    0.75     & 0.8        & 0.0169 & 0.0005 & 0.8432 & 0.0308 & 0.165  & 0.0347 \\ \hline
    0.75     & 0.9        & 0.0168 & 0.0005 & 0.8421 & 0.0446 & 0.1639 & 0.0387 \\ \hline
    0.75     & 1.0        & 0.0171 & 0.0006 & 0.8582 & 0.0553 & 0.1673 & 0.0458 \\ \hline
    1.0      & 0.1        & 0.0159 & 0.0009 & 0.7504 & 0.0441 & 0.096  & 0.0179 \\ \hline
    1.0      & 0.2        & 0.0183 & 0.001  & 0.8615 & 0.0749 & 0.127  & 0.0144 \\ \hline
    1.0      & 0.3        & 0.0198 & 0.0008 & 0.9512 & 0.049  & 0.1777 & 0.0433 \\ \hline
    1.0      & 0.4        & 0.0204 & 0.0006 & 0.9913 & 0.0736 & 0.2059 & 0.0555 \\ \hline
    1.0      & 0.5        & 0.0205 & 0.0009 & 1.0113 & 0.0817 & 0.215  & 0.0687 \\ \hline
    1.0      & 0.6        & 0.0208 & 0.001  & 1.0188 & 0.0786 & 0.241  & 0.0707 \\ \hline
    1.0      & 0.7        & 0.0208 & 0.0011 & 1.0289 & 0.0813 & 0.2393 & 0.0677 \\ \hline
    1.0      & 0.8        & 0.021  & 0.0009 & 1.0582 & 0.0873 & 0.2551 & 0.0831 \\ \hline
    1.0      & 0.9        & 0.021  & 0.0011 & 1.0696 & 0.1024 & 0.2472 & 0.0635 \\ \hline
    1.0      & 1.0        & 0.0208 & 0.0008 & 1.076  & 0.0837 & 0.3057 & 0.1061 \\ \hline
    \end{tabular}
    \label{table:XGBComplexity500Full}
\end{table}



\begin{table}[!ht]
    \centering
    \caption{Standard XGBoost models with different model complexity (5000 boosting rounds) over different sampling and model snapshots}
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
    Sampling & Complexity & Mean Corr   & Mean Corr   & Sharpe & Sharpe & Calmar & Calmar \\ \hline
             &            & mean   & std    & mean   & std    & mean   & std    \\ \hline
    0.5      & 0.1        & 0.0138 & 0.0002 & 0.6183 & 0.0098 & 0.062  & 0.0098 \\ \hline
    0.5      & 0.2        & 0.0154 & 0.0001 & 0.6839 & 0.0079 & 0.1003 & 0.0118 \\ \hline
    0.5      & 0.3        & 0.0162 & 0.0002 & 0.7258 & 0.0156 & 0.1273 & 0.016  \\ \hline
    0.5      & 0.4        & 0.0165 & 0.0003 & 0.7525 & 0.0155 & 0.1359 & 0.0214 \\ \hline
    0.5      & 0.5        & 0.0169 & 0.0003 & 0.7741 & 0.0204 & 0.1508 & 0.0149 \\ \hline
    0.5      & 0.6        & 0.017  & 0.0003 & 0.7782 & 0.0268 & 0.1541 & 0.02   \\ \hline
    0.5      & 0.7        & 0.0171 & 0.0003 & 0.7849 & 0.0267 & 0.1602 & 0.0257 \\ \hline
    0.5      & 0.8        & 0.0172 & 0.0002 & 0.7933 & 0.0224 & 0.1577 & 0.0257 \\ \hline
    0.5      & 0.9        & 0.0171 & 0.0003 & 0.7944 & 0.0202 & 0.159  & 0.0323 \\ \hline
    0.5      & 1.0        & 0.017  & 0.0003 & 0.7923 & 0.0192 & 0.1536 & 0.0249 \\ \hline
    0.75     & 0.1        & 0.0149 & 0.0002 & 0.6195 & 0.0106 & 0.0589 & 0.0042 \\ \hline
    0.75     & 0.2        & 0.0172 & 0.0003 & 0.7384 & 0.0194 & 0.1189 & 0.0164 \\ \hline
    0.75     & 0.3        & 0.0185 & 0.0004 & 0.8097 & 0.0217 & 0.1604 & 0.0194 \\ \hline
    0.75     & 0.4        & 0.0192 & 0.0001 & 0.8469 & 0.0205 & 0.1723 & 0.0127 \\ \hline
    0.75     & 0.5        & 0.0196 & 0.0003 & 0.8792 & 0.0288 & 0.1894 & 0.0204 \\ \hline
    0.75     & 0.6        & 0.0201 & 0.0004 & 0.9053 & 0.0312 & 0.1993 & 0.044  \\ \hline
    0.75     & 0.7        & 0.0202 & 0.0005 & 0.9165 & 0.0359 & 0.1924 & 0.0346 \\ \hline
    0.75     & 0.8        & 0.0202 & 0.0004 & 0.9199 & 0.0367 & 0.1866 & 0.0268 \\ \hline
    0.75     & 0.9        & 0.0203 & 0.0003 & 0.9288 & 0.0294 & 0.1872 & 0.0383 \\ \hline
    0.75     & 1.0        & 0.0203 & 0.0003 & 0.9369 & 0.0266 & 0.1761 & 0.0283 \\ \hline
    1.0      & 0.1        & 0.0191 & 0.0003 & 0.8123 & 0.0191 & 0.0908 & 0.0065 \\ \hline
    1.0      & 0.2        & 0.0216 & 0.0004 & 0.9261 & 0.0162 & 0.1366 & 0.0083 \\ \hline
    1.0      & 0.3        & 0.0229 & 0.0004 & 0.9894 & 0.0191 & 0.1705 & 0.0217 \\ \hline
    1.0      & 0.4        & 0.0235 & 0.0005 & 1.0282 & 0.0126 & 0.1926 & 0.0233 \\ \hline
    1.0      & 0.5        & 0.0238 & 0.0004 & 1.0506 & 0.0168 & 0.2179 & 0.0244 \\ \hline
    1.0      & 0.6        & 0.024  & 0.0003 & 1.0711 & 0.022  & 0.2504 & 0.032  \\ \hline
    1.0      & 0.7        & 0.0242 & 0.0004 & 1.0841 & 0.0284 & 0.275  & 0.0396 \\ \hline
    1.0      & 0.8        & 0.0242 & 0.0005 & 1.0999 & 0.0303 & 0.3019 & 0.0445 \\ \hline
    1.0      & 0.9        & 0.0242 & 0.0004 & 1.1059 & 0.0333 & 0.3208 & 0.0528 \\ \hline
    1.0      & 1.0        & 0.0242 & 0.0005 & 1.1127 & 0.0359 & 0.3406 & 0.0491 \\ \hline
    \end{tabular}
    \label{table:XGBComplexity5000Full}
\end{table}


\begin{table}[!ht]
    \centering
    \caption{Large XGBoost models with different model complexity (50000 boosting rounds) over different sampling and model snapshots}
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
    Sampling & Complexity & Mean Corr   & Mean Corr   & Sharpe & Sharpe & Calmar & Calmar \\ \hline
             &            & mean   & std    & mean   & std    & mean   & std    \\ \hline
    0.5      & 0.1        & 0.0143 & 0.0002 & 0.6275 & 0.0119 & 0.0661 & 0.002  \\ \hline
    0.5      & 0.2        & 0.0156 & 0.0002 & 0.6875 & 0.0103 & 0.1064 & 0.0066 \\ \hline
    0.5      & 0.3        & 0.0162 & 0.0002 & 0.7226 & 0.0113 & 0.1368 & 0.0062 \\ \hline
    0.5      & 0.4        & 0.0165 & 0.0002 & 0.742  & 0.0108 & 0.1453 & 0.0064 \\ \hline
    0.5      & 0.5        & 0.0167 & 0.0002 & 0.757  & 0.0096 & 0.1507 & 0.0077 \\ \hline
    0.5      & 0.6        & 0.0168 & 0.0002 & 0.769  & 0.01   & 0.1541 & 0.0041 \\ \hline
    0.5      & 0.7        & 0.0169 & 0.0002 & 0.7791 & 0.0109 & 0.1584 & 0.006  \\ \hline
    0.5      & 0.8        & 0.017  & 0.0002 & 0.7883 & 0.0092 & 0.1622 & 0.0063 \\ \hline
    0.5      & 0.9        & 0.017  & 0.0002 & 0.7915 & 0.0089 & 0.1632 & 0.0076 \\ \hline
    0.5      & 1.0        & 0.0169 & 0.0002 & 0.795  & 0.009  & 0.1647 & 0.0091 \\ \hline
    0.75     & 0.1        & 0.0154 & 0.0001 & 0.6332 & 0.0042 & 0.0633 & 0.0016 \\ \hline
    0.75     & 0.2        & 0.0178 & 0.0001 & 0.7512 & 0.0064 & 0.1205 & 0.0039 \\ \hline
    0.75     & 0.3        & 0.019  & 0.0001 & 0.8164 & 0.0066 & 0.16   & 0.0043 \\ \hline
    0.75     & 0.4        & 0.0197 & 0.0001 & 0.8604 & 0.0036 & 0.1808 & 0.0047 \\ \hline
    0.75     & 0.5        & 0.0202 & 0.0001 & 0.8914 & 0.0047 & 0.1951 & 0.0053 \\ \hline
    0.75     & 0.6        & 0.0205 & 0.0001 & 0.9104 & 0.0072 & 0.1872 & 0.0098 \\ \hline
    0.75     & 0.7        & 0.0206 & 0.0001 & 0.9254 & 0.0051 & 0.1826 & 0.0081 \\ \hline
    0.75     & 0.8        & 0.0207 & 0.0001 & 0.936  & 0.0061 & 0.1792 & 0.0057 \\ \hline
    0.75     & 0.9        & 0.0207 & 0.0001 & 0.9427 & 0.0075 & 0.1766 & 0.0037 \\ \hline
    0.75     & 1.0        & 0.0207 & 0.0001 & 0.947  & 0.0063 & 0.174  & 0.0052 \\ \hline
    1.0      & 0.1        & 0.0197 & 0.0001 & 0.8248 & 0.0043 & 0.094  & 0.0026 \\ \hline
    1.0      & 0.2        & 0.0221 & 0.0001 & 0.9396 & 0.0084 & 0.1415 & 0.006  \\ \hline
    1.0      & 0.3        & 0.0232 & 0.0001 & 0.9963 & 0.0078 & 0.1805 & 0.0063 \\ \hline
    1.0      & 0.4        & 0.0238 & 0.0001 & 1.0359 & 0.0081 & 0.2059 & 0.0071 \\ \hline
    1.0      & 0.5        & 0.0242 & 0.0001 & 1.064  & 0.0061 & 0.2325 & 0.0062 \\ \hline
    1.0      & 0.6        & 0.0244 & 0.0001 & 1.0842 & 0.0062 & 0.2551 & 0.0102 \\ \hline
    1.0      & 0.7        & 0.0245 & 0.0001 & 1.1011 & 0.0068 & 0.2806 & 0.015  \\ \hline
    1.0      & 0.8        & 0.0246 & 0.0001 & 1.1144 & 0.0099 & 0.2983 & 0.0146 \\ \hline
    1.0      & 0.9        & 0.0246 & 0.0001 & 1.1213 & 0.0136 & 0.3161 & 0.0135 \\ \hline
    1.0      & 1.0        & 0.0246 & 0.0001 & 1.1283 & 0.0136 & 0.3371 & 0.0125 \\ \hline
    \end{tabular}
    \label{table:XGBComplexity50000Full}
\end{table}




\end{document}



