% !TEX root = ../PaperForReview.tex

\section{Related Work}
\label{sec:related_work}

\noindent\textbf{Scene Graph Prediction in Point Cloud.}
% %
% Scene Graph Generation (SGG) on images has become a popular task ~\cite{zellers2018neural, yang2018graph, xu2017scene, tang2020unbiased, suhail2021energy, woo2018linknet, chen2019knowledge, tang2019learning} in the last few years, which aims to detect objects and relationships between objects from images. 
% %
% However, because of the lack of spatial structure information, SGG on images has some limitations for the understanding of the positional relationship along the depth between objects.
% %
% Recently, Scene Graph Datasets on 3D gets attention. 
% %
% Armeni \etal first present a 3D scene graph dataset, which maps a space into the form of hierarchical structure, \eg building, room, object, camera.
% %
% But it just predicts the relationships derived from detected object instances on images.
% %
% 3DSSG~\cite{wald2020learning} is another Scene Graph dataset on Point Cloud, and it predicts the Scene Graph directly from point clouds and class-agnostic instance segmentation mask.
% %
% SGFN~\cite{wu2021scenegraphfusion} extends the dataset based on 3DSSG, which aims to generate scene graphs incrementally from the local point clouds restructured by RGB-D sequences.
% %
% While textual information is lost on point clouds, predicting a scene graph directly from it will miss some important cues.
% %
% Our methods follow the task in SGPN~\cite{wald2020learning}, which solves the above problem by involving 2D semantics.
Image-based semantic scene graph prediction has been extensively studied~\cite{zellers2018neural,yang2018graph,xu2017scene,tang2020unbiased,suhail2021energy,woo2018linknet,chen2019knowledge,tang2019learning}in recent years, but only a few works try to predict 3D semantic scene graph in the point cloud.
%
% Scene graph prediction by 2D images is a popular research area~\cite{zellers2018neural,yang2018graph,xu2017scene,tang2020unbiased,suhail2021energy,woo2018linknet,chen2019knowledge,tang2019learning} in the past few years, which aims to detect objects and relationships among objects.
% %
% However, previous researches focuses on 2D images, and relationships in 3D scene are less explored.
% However, limited by 2D images, it is difficult to model many relationships in the real 3D world.
% 
% Recently, scene graph prediction from point clouds draws more attention.
%
Armeni~\etal~\cite{armeni20193d} presented the first 3D scene graph dataset, which maps 3D buildings into hierarchical structures.
%
Wald~\etal~\cite{wald2020learning} constructed a point cloud-based semantic scene graph dataset, namely 3DSSG, with a GNN-based baseline model named SGPN. 
%
The follow-up work SGFN~\cite{wu2021scenegraphfusion} predicted 3DSSG incrementally from RGB-D sequences.
%
In recent years, a few methods were proposed to improve the GNN-based baseline.
% 
% \revise{SGG\_point intro.}
% 
\sggpoint~\cite{zhang2021exploiting} used an edge-oriented graph convolution network to exploit multi-dimensional edge features for relation modeling.
%
Zhang~\etal~\cite{zhang2021knowledge} proposed a graph auto-encoder network to automatically learn a group of embeddings for each class in advance, and then perform the 3DSSG prediction to recognize credible relation triplets from pre-learned knowledge.
% 
% However, these methods do 2D projections, where external knowledge from rich pretraining data can be introduced.

\vspace{+1mm}
\noindent\textbf{3D Scene Understanding with 2D Semantics.}
%
% In the past few years, multi-modality methods have caught considerable attention.
%
% To increase the attribute information of objects in Point Clouds, various tasks utilize 2D images to complement 3D.
%
A list of methods have employed 2D semantics to help 3D instance-level tasks, such as 3D object detection, segmentation, visual grounding and dense captioning~\cite{qi2018frustum,qi2020imvotenet,bai2022transfusion,sindagi2019mvx,vora2020pointpainting,yin2021multimodal,zhi2021place}. They can be coarsely divided into two categories, \ie, concatenating image features with each 3D point~\cite{chen2020scanrefer, zhao20213dvg, cai20223djcg, bai2022transfusion,vora2020pointpainting,yin2021multimodal,qi2020imvotenet}, and projecting object detection results into 3D space~\cite{pang2020clocs, yang2021sat, yuan2022x, qi2018frustum, xu2018pointfusion, lahoud20172d}.
%
Most methods require 2D semantics both in the training and inference stages.
%
Recently, SAT~\cite{yang2021sat} and $\cX$-Trans2Cap~\cite{yuan2022x} explore using 2D semantics only in training to assist 3D visual grounding and dense captioning. Both of them can learn enhanced models that only use 3D inputs in inference.
%
But these methods are restrained to instance-level tasks and the networks have to be carefully designed.
%
% There are mainly two types of approaches that align 3D and 2D semantics.
% % todo
% One is to project 3D detected or Ground-Truth boxes to 2D frames, like ~\cite{yang2021sat}, which prefers to focus on object-level tasks.
% %
% This method has a better representation because of the high accuracy of projection results.
% %
% The other is just the opposite, \ie project 2D pixel-level feature to 3D points, \eg 3DVG~\cite{zhao20213dvg} and 3DJCG~\cite{cai20223djcg}.
% %
% This method needs to save features on a larger receptive field, which is space-consuming.
% %
% Since 3D Scene Graph Prediction on 3DSSG obtains instance segmentation during training, we just use the first type method to generate object-level features.
% %
% Previous studies have added 2D as an additional input to 3D Perception tasks, using  various fusion techniques.
% %
% Yang \etal \cite{yang2021sat} propose that extra 2D inputs potentially limit the application scenarios since the 2D information either does not exist in inference.
%
We follow similar ideas as~\cite{yang2021sat,yuan2022x} and use 2D semantics only in training, but we would like to enhance the 3DSSG prediction that requires structural understanding rather than instance-level perception.

\vspace{+1mm}
\noindent\textbf{Knowledge-inserted Methods in Scene Graph Prediction.}
%
% According to the knowledge source, we can divide the previous studies into two directions.
% %
% The first is to use the prior knowledge from the training dataset.
%
Zellers~\etal~\cite{zellers2018neural} and Chen~\etal~\cite{chen2019knowledge} indicated that the statistical co-occurrences between object pairs and relationships are useful for relation prediction.
%
%  also use the same statistical priors and incorporates them into graph propagation networks to solve the long-tail problem in Scene Graph Generation.
%
Besides, \cite{sharifzadeh2021classification, zhang2021knowledge} generated class-level prototypical representations from all previous perceptual outputs as the prior knowledge.
%
These methods explicitly encoded the data priors into the model.
%
% The other is to utilize the knowledge from the extra database.
%
% To dismiss the huge variations in the visual appearances between the same predicate, which has a different object/subject, 
%
\cite{li2017vip, lu2016visual, liao2019natural, abdelkarim2021exploring} attempted to combine language priors with scene graph prediction.
%
% Furthermore, commonsense knowledge is also utilized to assist the task.
%
Zareian~\etal~\cite{zareian2020bridging} proposed a Graph Bridging Network to propagate messages between scene graphs and knowledge graphs.
%
% They encode the commonsense using language models, which ignores the commonsense hidden in vision.
%
Our VL-SAT scheme uses CLIP to encode the linguistic semantics, which is thus better aligned with 2D semantics, and even the required 3D structural semantics during the training stage.
%