% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{mmstyles}

\newcommand{\method}{\textcolor{red}{VL-SAT}}
\newcommand{\module}{\textcolor{red}{Cross-modal Heterogeneous Collaboration}~}

% \newcommand{\basetwo}{\textcolor{black}{$\text{}_{\text{2d}}$ }}
\newcommand{\basetwo}{Oracle}
% \newcommand{\basethree}{\textcolor{black}{$\mathrm{Base_{3d}}$ }}
\newcommand{\sggpoint}{$\text{SGG}_{\text{point}}$}
\newcommand{\basethree}{\textcolor{black}{$\text{Base}_{\text{3d}}$ }}
\newcommand{\baseclip}{\textcolor{black}{$\text{Base}_{\text{CLIP}}$ }}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{1977} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud \\ (Supplementary Material)}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\maketitle

%%%%%%%%% BODY TEXT
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{picture/supp.pdf}
\caption{
%
\textbf{The Teacher-Student Model based on the Knowledge Distillation Scheme.}
%
% In training, the student model gets knowledge by feature mimic.
%
During training, the teacher model transfers its knowledge to the student model via feature mimic.
%
Besides, we also add KL loss between teacher logits and student logits on both object and predicate classifiers to advance the knowledge transfer process.
%
During inference, the student model takes the same inputs as the 3D model in our VL-SAT scheme.
}
\label{fig:network_kd}
\end{figure*}

\begin{table*}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c}
\hline
Split & Predicate \\
\hline
Head & left, right, front, behind, close by, same as, attached to, standing on \\
Body & bigger than, smaller than, higher than, lower than, lying on, hanging on \\
Tail & supported by, inside, same symmetry as, connected to, leaning against, part of, belonging to, build in, standing in, cover, lying in, hanging in \\
\hline
\end{tabular}}
\caption{Splits of predicates.}
\label{tab:splits}
\end{table*}

\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\hline 
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{Predicate} & \multicolumn{2}{c}{Triplet} \\ 
\cline{2-6}
& mA@1 & mA@3 & mA@5 & mA@50 & mA@100 \\ 
\hline
SGFN                             & 41.89 & 70.82 & 81.44 & 58.37 & 67.61  \\
\hline
KD (Teacher)  & 53.57 & 72.37 & 86.18 & 73.31 & 81.08  \\
VL-SAT (Oracle)                   & 55.66 & 76.28 & 86.45 & 74.10 & 81.38  \\
\hline
KD (Student)  & 52.22 & 72.50 & 83.18 & 62.92 & 71.75  \\
VL-SAT (Ours)                     & 54.03 & 77.67 & 87.65 & 65.09 & 73.59  \\
\hline
\end{tabular}}
\caption{
%
Results of different knowledge transfer methods. 
%
We refer to the multi-modal teacher-student model as Knowledge Distillation (KD) scheme, and then we compare the results with our VL-SAT scheme.}
\label{tab:ablation_kd}
\end{table}

\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c}
\hline 
\multirow{2}{*}{Model}  & \multicolumn{2}{c|}{Object}  & \multicolumn{2}{c|}{Predicate} & \multicolumn{2}{c}{Triplet} \\
\cline{2-7}
& A@5 & A@10 & mA@3 & mA@5 & mA@50 & mA@100 \\ 
\hline
\baseclip (XYZ)            & 79.03 & 86.81 & 72.50 & 83.59 & 60.65 & 69.71  \\
\baseclip (XYZ+RGB)        & 76.35 & 84.19 & 71.45 & 79.10 & 58.76 & 67.67  \\
VL-SAT(ours)            & 78.66 & 85.91 & 77.67 & 87.65 & 65.09 & 73.59   \\
\hline
\end{tabular}}
\caption{
%
Results of different inputs. 
%
% We want to figure out whether adding the RGB information directly into the input performs better than the VL-SAT schema which only uses XYZ information. 
%
We figure out whether adding RGB information directly into the 3D point cloud (XYZ) input can boost 3DSSG prediction performance as our VL-SAT scheme does.
%
\baseclip shares the same network architecture as non-VL-SAT but leverages CLIP-initialized object classifier.
% CI means CLIP-initialized object classifier.
}
\label{tab:ablation_input}
\end{table}

\begin{table}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
\hline 
\multirow{2}{*}{Backbone} & \multicolumn{3}{c|}{Predicate} & \multicolumn{2}{c}{Triplet} \\ 
\cline{2-6}
& mA@1 & mA@3 & mA@5 & mA@50 & mA@100 \\ 
\hline
non-VL-SAT & 41.99 & 70.88	& 81.67 & 59.58 & 67.75  \\
CLIP Pretrained & 48.65 & 76.12 & 87.09 & 62.85 & 71.60  \\
ImageNet21k Pretrained & 47.43 & 74.47 & 85.71 & 61.36 & 70.07 \\
\hline
\end{tabular}}
\caption{
%
Results of different visual encoders. 
%
% We want to figure out the assistance of vision, along with the influence of vision encoder pretrained by different methods. 
%
We figure out the influence of visual assistance and the influence of visual encoder pretrained using different datasets.
%
We conduct the experiments with a variant of the VL-SAT scheme, which discards all the linguistic assistance, \ie CLIP-based object classifier initialization, and CLIP-based triplet-level regularization.
}
\label{tab:ablation_encoder}
\end{table}

% \begin{table}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c}
% \hline 
% \multirow{2}{*}{Stage} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Predicate} & \multicolumn{2}{c}{Triplet} \\ 
% \cline{3-7}
% & & mA@1 & mA@3 & mA@5 & mA@50 & mA@100 \\ 
% \hline
% None & None & 50.56 & 76.99 & 86.43 & 63.24 & 72.74 \\
% After GCN & L1          & 53.19 & 76.12	& 86.67 & 63.07 & 72.34  \\
% After GCN & Cosine      & 53.31 & 76.15	& 86.31	& 63.85	& 72.45  \\
% After Node Encoder & L1     & 53.79 & 77.12 & 87.57	& 64.13	& 72.82 \\
% After Node Encoder & Cosine & \textbf{54.03} & \textbf{77.67} & \textbf{87.65} & \textbf{65.09} & \textbf{73.59}  \\
% \hline
% \end{tabular}}
% \caption{
% %
% Results of different mimic stages and methods. 
% %
% The $1^\text{st}$ row indicates our VL-SAT scheme without mimic loss.
% %
% The last row indicates VL-SAT (ours) model.
% }
% \label{tab:ablation_mimic}
% \end{table}


\section{Implementation Details}
%
\vspace{+1mm}
\noindent\textbf{2D Data Preparation.}
%
Since each 3D scan in the 3DSSG dataset~\cite{wald2020learning} is associated with RGB sequences with known camera poses, thus it is possible to extract 2D image patches associated with each point cloud instance $\mP_{i}$.
%
We first project the 3D points in $\mP_{i}$ to each RGB frame according to the given camera pose, and then calculate the area of the enlarged bounding box surrounded by the projected points. Since then, we rank the frames in the descending order of these areas and select the image patches in the bounding boxes in the top-$N$ frames as the N-view image patches of the instance $\mP_i$.
%
The visual features $\vo_i$ corresponding to $\mP_i$ are thus generated by mean pooling the visual features of N-view image patches through a fixed CLIP vision encoder that has been finetuned on 3DSSG~\cite{wald2020learning,gao2021clip}.

% To generate the 2D visual features corresponding to a point cloud instance $\mP_{i}$, we first project the 3D points of the object $\mP_{i}$ to 2D frames using a pinhole camera model. 
% %
% We rank the frames according to the rate of 3D points that can be projected onto the image, and then we select the top-N frames as the N-view 2D representation of the instance $\mP_{i}$.
% %
% To generate the image feature $\vo^{2d}_i$ of the instance $\mP_i$, we encode the N-view 2D images with a CLIP Adapter~\cite{gao2021clip} which is finetuned on 3DSSG~\cite{wald2020learning}, along with a following mean pooling layer.
%
% What should be noted is that we utilize the visual encode of the CLIP equipped with VIT-Based-Patch32 Vision Encoder to be our Adapter's backbone.
%
% What should be noted is that the CLIP adapter's backbone is implemented with ViT architecture~\cite{dosovitskiy2021an}.
%
% The CLIP adapter's backbone is implemented with ViT-B-32 architecture~\cite{dosovitskiy2021an}.
%

\vspace{+1mm}
\noindent\textbf{Architecture Details.}
%
We adopt a simple PointNet~\cite{qi2017pointnet} as the 3D node encoder.
%
% Besides, in order to match the edge feature better, we append the box size and volume to the node feature before passing it to GNN.
%
% As for the 2D node encoder, we use the structure mentioned in 2D Data Preparation.
%
As for the 2D node encoder, we use Vit-B-32 architecture~\cite{dosovitskiy2021an} as the backbone of the CLIP image encoder.
%
% All the node features and edge features in the oracle model or 3d model utilize $512$ feature dim, which is aligned to the output size of the Clip Adapter.
%
The feature dimension of all the node and edge features in the oracle and 3D model is set to be $512$.
%
% , which is aligned to the output size of the CLIP Adapter.
%
The structure of GNN is borrowed from SGFN\cite{wu2021scenegraphfusion}, which uses a FAT mechanism to combine neighboring features.
%
All the multi-head self attention (MHSA) or multi-head cross attention (MHCA) structures in our method use $8$ heads, with a hidden feature size of $512$.
%
According to our experiments, $\rho(\cdot,\cdot)$ in $L_\text{tri-emb}$ is implemented with $\ell_1$ norm, and the $\rho(\cdot,\cdot)$ in $L_\text{node-init}$ is implemented with negative cosine distance.

\vspace{+1mm}
\noindent\textbf{Splits of Predicates.} 
%
We split the $26$ predicate classes into three parts: \textit{head}, \textit{body}, \textit{tail}.
%
In detail, we sort the predicates according to their frequencies in the training set in descending order and select the top $8$ categories as head classes, the last $12$ categories as tail classes, and the remaining $6$ categories as body classes.
%
You can refer to~\cref{tab:splits}.

%
\section{More Experiments}
%
\subsection{Comparison with Knowledge Distillation Scheme.}
%
To prove the superiority of our proposed VL-SAT scheme, we design a knowledge distillation (KD) scheme as in ~\cref{fig:network_kd}, which adheres to a teacher-student paradigm.
%
The teacher is a multi-modal model, which fuses visual and geometrical information using bi-directional cross-attention.
%
Besides, to compare with our VL-SAT scheme in a fair manner, we also leverage linguistic assistance in the KD scheme.
%
The student model is the same as our non-VL-SAT model.
%
The knowledge transfer process from teacher to student is implemented with traditional mimic loss and KL loss.
%
As shown in ~\cref{tab:ablation_kd}, since our oracle model trained with VL-SAT scheme can combine multi-modal knowledge more effectively, the performance is better than the teacher model of KD scheme among all metrics, \eg $2.1\%$ gains on predicate mA@$1$.
%
% Besides, our method has a better knowledge transfer ability, with $2.1\%$ gains on mA@$50$.
%
Besides, VL-SAT (ours) outperforms KD (student) with $2.1\%$ gains on triplet mA@$50$.
%
We think the performance degradation of KD scheme is because
%
% The teacher model has a different structure compared with the student model, and the mimic feature is somehow different in the stage, which is hard to transfer knowledge effectively.
%
the teacher model has a different network structure compared with the student model, and the heterogeneous network structures may hinder the knowledge transfer process as indicated in ~\cite{wang2022head}.

%
\subsection{Can RGB Information on Point Cloud Boost 3DSSG Prediction As Well?}
%
% A straightforward thought is that,  whether the performance is comparable when we just use the RGB information of the point cloud to enhance the object feature instead of utilizing the assistance of 2D images.
%
% So we also conduct the experiment in which the input contains XYZ along with RGB information.
%
% It is worth to noted that, since we want to figure that whether the RGB input can replace the function of the oracle model, we just utilize a CLIP-initialized object classifier (CI), which is independent of the oracle model in the VL-SAT Scheme.
%
% In table~\ref{tab:ablation_input}, we can find that the performance dropped greatly comparing lines 2 with lines 1 and lines 3, which means adding RGB performs even worse on both object and predicate than the model with only XYZ input.
%
% We think this is resulting from the inherent feature of PointNet\cite{qi2017pointnet}.
%
% It is proved that the structure of Max-Pooling in PointNet will ignore the RGB information in \cite{qi2020imvotenet}.
%
% Besides, the network is overfitting during the object training, which results in a performance drop on both the object and predicate on the validation set.
%
Since the VL-SAT scheme boosts 3DSSG prediction significantly, it is intuitive to think about whether adding RGB information directly into 3D point cloud could also do well.
%
We conduct such experiments (namely, $\text{Base}_\text{CLIP}$ since we employ ClIP-initialized object classifier in this baseline) in \cref{tab:ablation_input} and find that simply concatenating RGB values to point cloud's XYZ coordinates (as $\text{Base}_\text{CLIP}$ (XYZ+RGB)) brings moderate performance drop (as $\text{Base}_\text{CLIP}$ (XYZ)) in 3DSSG prediction task.
%
We doubt it is due to over-fitting on RGB values as indicated in ~\cite{qi2020imvotenet}.
%
The experiment results also validate the necessity of our VL-SAT scheme.

\subsection{Influence of Visual Assistance.}
%
% To figure out the influence of visual assistance, we conduct the experiment without language input during training.
%
% Comparing the results with line 1 and line 2 in table~\cref{tab:ablation_encoder}, we can find that by collaborating with 2D vision feature, the model still improves $6.6
% \%$ gains on predicate mA@$1$ and $3.3\%$ gains on triplet mA@$50$.
%
% Besides, we also try the image encoder pretrained by ImageNet21K~\cite{deng2009imagenet}, which has the same structure as our Clip Vision Encoder.
%
% We aim to figure out the influence of different image representation abilities.
%
% The results between line 2 and line 3 denote that the CLIP encoder performs slightly better, with $1.2\%$ and $1.5\%$ gains on predicate mA@$1$ and triplet mA@$50$ respectively.
%
% Besides, we can also find that the representation ability of the CLIP-Pretrained Vision Encoder is slightly better than the ImageNet21k-Pretrained.
%
% But what needs to be noted is that we utilize CLIP not only because of its representation ability but also its ability to match vision features and linguistic features in our VL-SAT scheme.
%

To investigate the influence of visual assistance, we conduct experiments without linguistic assistance, \ie CLIP-based object classifier initialization, CLIP-based triplet-level regularization, during training.
%
As shown in ~\cref{tab:ablation_encoder}, with only visual assistance, our method still obtains $6.66\%$ gain on predicate mA@$1$ and $3.27\%$ gain on triplet mA@$50$.
%
Furthermore, we try the visual encoder pretrained on ImageNet21K~\cite{deng2009imagenet} dataset, which shares the same network structure as the CLIP pretrained visual encoder used in our VL-SAT.
%
The ImageNet21K pretrained visual encoder also shows performance gains over non-VL-SAT model, but is inferior to our CLIP pretrained visual encoder.
%
The result shows that the CLIP pretrained visual encoder possesses a stronger representation ability over the ImageNet21K pretrain visual encoder.

% \subsection{Different regularization distance $\rho$.}
% We also conduct the experiment on different mimic stages and different mimic functions.
%
% As to the stage of mimic, we find that feature mimic after backbone is slightly better than the latter.
%
% We think the reason is earlier mimic stage can transform knowledge better, and the knowledge then can be involved in GNN effectively, while the After GNN mimic stage can not reach a similar effect. 
%
% Besides, the cosine similarity performs better compared with L1 Norm on different modality alignments.
%


% We apply a mimic loss $L_\text{node-init}$ between the 3D node features $\vo_i^{3d}$ and the 2D node features $\vo_i^{2d}$ in the VL-SAT scheme, and we provide a comprehensive ablation study of the mimic loss in ~\cref{tab:ablation_mimic}.
% %
% As shown in the $1^\text{st}$ row of ~\cref{tab:ablation_mimic}, the performance of our method drops significantly when the mimic loss is abandoned, which validates the necessity of the mimic loss in our method.
% %
% We further investigate the influence of different mimic stages and loss functions and find that applying the mimic loss after the node encoder with negative cosine distance loss achieves the best performance.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
