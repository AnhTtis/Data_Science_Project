\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\ColA}[1]{\textcolor[RGB]{160,32,240}{#1}}
\newcommand{\ColB}[1]{\textcolor[RGB]{205,102,29}{#1}}
\newcommand{\ColC}[1]{\textcolor{blue}{#1}}
\newcommand{\ColD}[1]{\textcolor{red}{#1}}
\newcommand{\sggpoint}{$\text{SGG}_{\text{point}}$}
\renewcommand\thetable{R\arabic{table}}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{1977} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud \vspace{-2mm}}  % **** Enter the paper title here
% \title{}

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
%
% We thank all reviewers for their constructive comments and for recognizing the novelty and effectiveness of our VL-SAT training scheme.
% %
% We are also encouraged by the comments ``Using powerful ... is certainly a powerful, but not yet well researched idea visual-linguistic guided training paradigm'' \& ``the motivation is convincing and the paper is well written'' (\ColA{RsFZJ}), and ``The proposed method is technically reasonable'' (\ColC{R2Rx7}).
% %
% We will include more experiments and discussions, fix the minor issues in the next version.

\noindent\textbf{\ColA{sFZJ-1}: Comparison with feature distillation scheme.}
%
We have compared with the feature distillation scheme in Tab. S2 in the supplementary. VL-SAT outperforms this scheme with $+2.17\%$ gain on triplet mA@$50$ metric, $5.17\%$ gain on predicate mA@$3$, which validates the effectiveness by using the back-propagated gradient flows in regularizing the 3D features. Note that our method is more flexible than direct feature alignment, since the benefits from the oracle model are implicitly embedded into the 3D features, and it will not bring significant bias when training the 3D model.

\begin{table}[h]
\vspace{-2mm}
\footnotesize
\centering
\caption{Results of different VL-SAT variants}
\vspace{-2mm}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{ID} & \multirow{2}{*}{Model} & Object & \multicolumn{2}{c|}{Predicate} & \multicolumn{2}{c}{Triplet} \\
\cline{3-7}
& & A@1 & mA@1 & mA@3 & mA@50 & mA@100 \\
\hline
A & non-VL-SAT & 54.79 & 41.99 & 70.88 & 59.58 & 67.75 \\
B & VL-SAT (ours) & 55.66 & 54.03 & 77.67 & 65.09 & 73.59 \\
\hline
C & VL-SAT (oracle 2D) & 66.39 & 55.66 & 76.28 & 74.10 & 81.38 \\
D & VL-SAT (oracle 3D) & 66.89 & 56.78 & 78.86 & 75.15 & 82.24 \\
E & oracle obj. \& 3D pred. & 66.38 & 54.03 & 77.67 & 73.79 & 82.18 \\
% F & w/ feature align. & 55.32 & 43.02 & 72.06 & 61.54 & 69.16 \\
\hline
\end{tabular}}
\label{tab:r-exp1}
\vspace{-2mm}
\end{table}

\noindent\textbf{\ColA{sFZJ-2}: VL-SAT (oracle 3D).}
%
In \cref{tab:r-exp1} (ID C \& D), VL-SAT (oracle 3D) slightly outperforms VL-SAT (oracle 2D).

% We report the results of VL-SAT (oracle 3D) in \cref{tab:r-exp1} (ID C), which slightly outperform those of VL-SAT (oracle 2D).
%
% But both oracle models take in 2D and 3D data during inference, which doesn't fit the setting of using only 3D data in inference.

\noindent\textbf{\ColA{sFZJ-3}: Upper bound performance.}
%
We reported this variant in \cref{tab:r-exp1} (ID E), whose performance is comparable to the oracle models (refer to \cref{tab:r-exp1} ID C \& D).
%
% We combine the object classification from the oracle model and the predicate prediction from the 3D model (Tab.~\ref{tab:r-exp1} model E), and it shows an upper bound of the performance.

\noindent\textbf{\ColA{sFZJ-4}: View-dependent spatial relational predicates.} 
%
Following the setting of SGPN and SGFN, in both training and testing stages, 3D scenes are placed in the same 3D coordinate, and each predicate is defined from one fixed camera view. In this case, the view-dependent spatial relational predicates are not ambiguous.
%
Will explore how to generally tackle view-dependent ambiguity in the future.

% View-dependent spatial relational predicates are evaluated using the ground-truth predicate annotations in inference time, which is a common practice in SGPN and SGFN. 
% %
% A GT view-dependent spatial relational predicate annotation implicitly defines the potential camera view of a scene. 
% %
% One possible solution to further tackle the ambiguity brought by camera view is to transform the 3D scene into a canonical pose for different camera views.

% \noindent\textbf{\ColA{sFZJ-5}: Direct feature alignment.}
% %
% Actually, we have adopted feature alignment (\ie $L_{\text{node-init}}$ in L442) at the very beginning of the network.
% % Additionally, we have adopted feature alignment in our method, \ie $L_{\text{node-init}}$ in L442.
% %
% % This loss aligns the 3D node features with 2D node features at the very beginning of the model.
% %
% Moreover, the variant (ID F) in \cref{tab:r-exp1} uses feature alignment (\ie, $\ell_1$ loss) in node/edge-level collaborations instead of the cross-attention modules, whose performance is slightly better than the baseline (non-VL-SAT), but is significantly inferior to our VL-SAT.
% %


\noindent\textbf{\ColA{sFZJ-5}: Weakly supervised settings.} It's possible to extend our method in a weakly-supervised training scheme, by following the strategy from caption-supervised 2DSSG prediction methods. Here, object and predicate classifiers are learned by graph matching/grounding techniques with the parsed linguistic structures of the extracted captions.

% The model can be extended to the weakly supervised setting and is a promising direction we are working on.

\noindent\textbf{\ColB{1Cnk-1}: Novelty.}
%
(1) The main contribution is the Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme rather than a new network architecture or loss terms.
%
To our best knowledge, VL-SAT is the first end-to-end optimized visual-linguistic knowledge transfer work that empowers 3DSSG prediction models with special attention paid to long-tail and ambiguous relations, which is less explored in the literature.
%
% Thus it is valid to apply an off-the-shelf GNN-based architecture as the baseline. 
%
This scheme is well-motivated, simple and effective, where the new gradient flow-based structural knowledge transfer strategy outperforms the common feature distillation strategy (see \ColA{sFZJ-1}).
%
% Even though the added loss terms/components in this scheme are not new individually, applying them to our training scheme is well motivated, and the way we incorporate them is not trivial.
%
VL-SAT can significantly boosts our baseline and other 3DSSG prediction methods~[36, 46] (in Tab. 7), only with 3D inputs in the inference stage.
%
Our contribution was recognized by reviewers \ColA{sFZJ} and \ColC{2Rx7}, who commented ``training strategy is interesting, and most importantly powerful even for generalization to new datasets'', ``...strong benefits in prediction of under-represented predicate classes...'', \etc.
%
% (2) Oversampling and unbiased training, as suggested by \ColB{1Cnk}, just improves the baseline marginally (triplet mA@50: 59.58$\rightarrow$60.68), but is tremendously inferior to our VL-SAT (triplet mA@50: 60.68$\rightarrow$65.09). This comparison also indicates the effectiveness of VL-SAT.
(2) As suggested by \ColB{1Cnk}, oversampling and unbiased training, just improves the baseline marginally (tail predicate mA@3: $40.01\rightarrow42.02$), but is tremendously inferior to our VL-SAT (tail predicate mA@3: $42.02\rightarrow52.38$). This comparison also indicates the effectiveness of our VL-SAT.

%
% Our method does not aim at designing better networks, but on effectively using visual-linguistic information to assist the training.
% %
% Our main contribution is the Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme, which can empower the point cloud-based 3DSSG prediction model with sufficient discrimination about long-tailed, zero-shot, and ambiguous semantic relation triplets.
% %
% The scheme can be applied to various base 3D models (\eg SGFN[36], \sggpoint[46]), as shown in Tab.7.
%
% Besides, our method doesn't need extra 2D inputs during inference, which potentially limits the application scenarios since the 2D information either does not exist in inference or requires tedious pre-processing, such as 2D-3D matching and 2D object detection.

\noindent\textbf{\ColB{R1Cnk-2}: Lower performance in head predicates.} 
%
% It is a long-lasting issue that improving the performance in tail classes will inevitably harm the discrimination of the head classes.
%
The performance in head predicates has become saturated (see Tab. 4), thus it makes more sense to enhance the overall performance by improving the performance in tail predicates, as what our VL-SAT does.
%
Since our VL-SAT boosts the overall performance by a large margin, such a slight performance degradation in head predicates is acceptable.

%
% It's a common phenomenon that the performance improvement of tail classes is accompanied by performance decline of the head classes, as shown in ~\cite{zhang2021knowledge,tang2019learning,tang2020unbiased}. 
%
% In comparison to significant improvement of tail predicates, such slight degradation 
% %
% Our method outperforms SGFN by a large margin, \ie $+10.01\%$ on body predicate mA@$3$, $+13.71\%$ on tail predicate mA@$3$. 
% %
% The prediction results of our method and SGFN on the head predicates are comparable, \ie $96.31\%$ vs.\ $95.08\%$ on head predicate mA@3, $99.21\%$ vs.\ $99.38\%$ on head predicate mA@5. 
% %
% The performances on head predicates are all over $95\%$, which indicates the model performance is close to saturation on these predicates, and slight performance fluctuations are normal and acceptable. 

\noindent\textbf{\ColB{1Cnk-3}: Applicable scope.}
%
Yes, we need 2D RGB data at the training stage. In fact, 2D RGB data can be conveniently collected since point clouds are usually captured together with 2D RGB images by RGB-D sensors or directly reconstructed from 2D RGB image collections.

% 2D image assisted training for 3D scene understanding is a common setting as in ~\cite{yang2021sat,yuan2022x,xu2022image2point}. 
% %
% We use 2D image data in the training process, firstly because 2D images contain stronger texture and color information than 3D point clouds, and secondly, because it is easy to obtain 2D image data paired with 3D point cloud using commodity RGB-D sensors. 
% %
% We discard 2D images in the inference stage because extra 2D information is usually computationally intensive and may be unavailable in some cases.

\noindent\textbf{\ColB{1Cnk-4}: Performance degradation on object prediction.}
%
Regularizing the training of predicates and triplets may bring bias to the representation of objects. Thanks to the NC/CI modules, this degradation is not severe, which validates that our whole VL-SAT training scheme is effective.

% %
% The main goal of scene graph prediction is to correctly predict triplets, which requires simultaneous prediction of objects and predicates. 
% %
% Edge-level collaboration and triplet-level regularization do cause a small drop in object classification performance but bring about significant performance gain in predicate prediction and finally improve the performance of triplet prediction. 
% %
% Such a trade-off between object and predicate prediction is reasonable and acceptable.

% \noindent\textbf{\ColB{R1Cnk-5}: Multi-modal.} Our multi-modal oracle combines RGB, point cloud, and language information.
%
% We use text features from the CLIP encoder in the object classifier and triplet-level regularization.

% \begin{table}[h]
% \vspace{-2mm}
% \footnotesize
% \centering
% \caption{Effect of tail oversampling with unbiased training.}
% \vspace{-2mm}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{l|c|c|c|c|c|c}
% \hline
% \multirow{3}{*}{Model} & Object & \multicolumn{4}{c|}{Predicate} & Triplet \\
% \cline{2-7}
% & \multirow{2}{*}{A@1} & Head & Body & Tail & All & \multirow{2}{*}{mA@50} \\
% \cline{3-6}
% & & mA@3 & mA@3 & mA@3 & mA@1 & \\
% \hline
% non-VL-SAT & 54.79 & 95.32 & 71.88 & 40.01 & 41.99 & 59.58\\
% +Tail-Oversample & 54.48 & 95.17 & 72.51 & 42.02 & 43.87 & 60.68 \\
% \hline
% VL-SAT (ours) & 55.66 & 96.31 & 80.03 & 52.38 & 54.03 & 65.09 \\
% \hline
% \end{tabular}}
% \label{tab:r-exp2}
% \vspace{-2mm}
% \end{table}

% \noindent\textbf{\ColB{R1Cnk-6}: Oversampling and unbiased training.}
% In \cref{tab:r-exp2}, simply oversampling the tail predicates with unbiased training does improve the performance on body and tail predicates, but slightly hurts the result on head predicates, as stated in \ColB{R2-Q2}.
% %
% Furthermore, the simple oversampling method is still inferior to our VL-SAT, with a $10.36\%$ gap on the tail predicate mA@3 metric.

\noindent\textbf{\ColB{1Cnk-5}: Losses are too complex.}
%
In addition to losses for conventional 3DSSG prediction methods, we only introduce two losses $L_{\text{tri-emb}}$ and $L_{\text{node-init}}$, each of which is well motivated and brings significant gain.
%
Note that the overall performance gain is not marginal (see the results in Tab. 1 $\sim$ 4).
%
Will polish the paper to make it easier to follow. 

% There are $2$ auxiliary loss in our method, \ie $L_{\text{tri-emb}}$, $L_{\text{node-init}}$. 
% %
% $L_{\text{tri-emb}}$ brings $+1.09\%$ gain on triplet mA@$50$ metric. 
% %
% $L_{\text{node-init}}$ brings $+1.85\%$ gain on triplet mA@50 metric. 
% %
% Both losses play an important role in the performance of the VL-SAT method. 

\noindent\textbf{\ColB{1Cnk-6}: Cross-attention vs.\ FAT.} 
Cross-attention is different from FAT. Will add more implementation details.

% Cross-attention in edge-level collaboration is the same as multi-head cross-attention in Transformer~\cite{vaswani2017attention}.
% %
% Cross-attention in node-level collaboration adds a distance-aware masking matrix onto the attention matrix before the softmax function, which can remove unnecessary attention between instances that are far apart without valid relations.

\noindent\textbf{\ColC{2Rx7-1}: Accuracy in Tab.1.} 
%
In both papers of SGFN and SFPN, the claimed top-k recall is actually top-k accuracy, according to their source codes (see footnote in page 5).
%
For a fair comparison, we use accuracy in Tab. 1, in which the re-implemented results of SGFN and SFPN are aligned with results in the original papers. 
%
Note that we also evaluate our method using the recall metric in Tab. 2 and Tab. 3.

\noindent\textbf{\ColC{2Rx7-2}: PredCls metric.}
%
% PredCls doesn't need object labels as network input and just evaluates the performance in predicting predicates (set object score to $1$ in PredCls). Will modify the statement of PredCls.
%
Sorry for the misleading sentence. In fact, we mean ``\textit{evaluate} the predicate'' rather than ``\textit{predict} the predicate''.
%
% PredCls is a metric to evaluate triplets.
%
% When calculating the recall of one GT triplet, PredCls will rank the predicted triplets according to the predicate scores, and by default set the object scores as 1.
% PredCls doesn't need object labels as network input and just evaluates the performance of predicate prediction. 
%
In SGPN, we choose a pair of point cloud segments whose object labels match one GT triplet, and calculate the recall of this triplet by ranking the predicted predicate scores between the pair of segments.
%

% The statement about PredCls may be ambiguous. 
% %
% PredCls doesn't take object class labels as network input, instead it's just an evaluation method. 
% %
% For PredCls, when we calculate the triplet recall, we will set the object score to $1$, which means we don't consider the object classification result.

\noindent\textbf{\ColC{2Rx7-3}: Difference between non-VL-SAT and SGFN.} 
%
The only difference between them is that non-VL-SAT adds a multi-head self-attention (MHSA) module before each GNN module in SGFN (please refer to L287).
%
% Compared with SGFN, the MHSA module slightly improves the model's performance.

\noindent\textbf{\ColC{2Rx7-4}: Object accuracy degradation.}
%
% We will add more discussion about this issue in the next version.
%
Please refer to \ColB{1Cnk-4}, we will add more discussions in the next version.

% \noindent\textbf{\ColC{2Rx7-5}: L362.} Will cite Transformers in the next version.

%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
