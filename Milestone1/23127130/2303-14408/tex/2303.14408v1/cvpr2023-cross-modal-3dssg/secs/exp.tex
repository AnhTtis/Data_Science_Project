% !TEX root = ../PaperForReview.tex

\begin{table*}
\centering
\caption{
%
Quantitative Results of 3D semantic scene graph prediction on the 3DSSG validation set~\cite{wald2020learning}. 
%
Evaluations are conducted in terms of object, predicate, and triplet. 
%
The results of SGPN, \sggpoint, and SGFN are based on our reproduced model with point cloud-only inputs, since they don't compute the mA@$k$ metric in their papers.
}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{Model} & \multicolumn{3}{c|}{Object} & \multicolumn{6}{c|}{Predicate} & \multicolumn{4}{c}{Triplet} \\
 \cline{2-14}
& A@$1$ & A@$5$ & A@$10$ & A@$1$ & A@$3$ & A@$5$ & mA@$1$ & mA@$3$ & mA@$5$ & A@$50$ & A@$100$ & mA@$50$ & mA@$100$ \\
 \hline
 SGPN~\cite{wald2020learning} & 48.28 & 72.94 & 82.74 & \textbf{91.32} & 98.09 & 99.15 & 32.01 & 55.22 & 69.44 & 87.55 & 90.66 & 41.52 & 51.92 \\
\sggpoint~\cite{zhang2021exploiting} & 51.42 & 74.56 & 84.15 & 92.4 & 97.78 & 98.92 & 27.95 & 49.98 & 63.15 & 87.89 & 90.16 & 45.02 & 56.03 \\
 SGFN~\cite{wu2021scenegraphfusion} & 53.67 & 77.18 & 85.14 & 90.19 & 98.17 & 99.33 & 41.89 & 70.82 & 81.44 & 89.02 & 91.71 & 58.37 & 67.61 \\
 \hline
%  \basetwo & \textbf{65.56} & \textbf{86.24} & \textbf{91.51} & 89.34 & 97.89 & 99.20 & 41.77 & 61.83 & 76.11 & \textbf{92.79} & \textbf{95.06} & 64.18 & \textbf{77.23} \\
 non-VL-SAT & 54.79 & 77.62 & 85.84 & 89.59 & 97.63 & 99.08 & 41.99 & 70.88 & 81.67 & 88.96 & 91.37 & 59.58 & 67.75 \\
%  \hline
%  Ours & 3D + 2D  & 65.88 & 86.53 & 91.46 & 90.78 & 55.66 & 98.45 & 78.61 & 99.56 & 89.82 & 93.09 & 74.10 & 95.22 & 81.38 \\
%  Ours & 3D + RGB & 51.87 & 76.35 & 84.19 & 91.28 & 48.52 & 98.49 & 70.45 & 99.42 & 78.10 & 88.74 & 56.76 & 91.46 & 65.67 \\
 VL-SAT (ours) & \textbf{55.66} & \textbf{78.66} & \textbf{85.91} & 89.81 & \textbf{98.45} & \textbf{99.53} & \textbf{54.03} & \textbf{77.67} & \textbf{87.65} & \textbf{90.35} & \textbf{92.89} & \textbf{65.09}  & \textbf{73.59} \\
 \hline
 VL-SAT (oracle) & 66.39 & 86.53 & 91.46 & 90.66 & 98.37 & 99.40 & 55.66 & 76.28 & 86.45 & 92.67 & 95.02 & 74.10 & 81.38 \\
 \hline
\end{tabular}}
\label{tab:exp-results}
\end{table*}

% \begin{table*}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{2}{*}{Model} & \multicolumn{3}{c|}{Object} & \multicolumn{6}{c|}{Predicate} & \multicolumn{4}{c}{Triplet} \\
%  \cline{2-14}
%  & A@$1$ & A@$5$ & A@$10$ & A@$1$ & A@$3$ & A@$5$ & mA@$1$ & mA@$3$ & mA@$5$ & A@$50$ & A@$100$ & mA@$50$ & mA@$100$ \\
%  \hline
%  SGPN~\cite{wald2020learning} & 48.28 & 72.94 & 82.74 & \textbf{91.32} & 98.09 & 99.15 & 32.01 & 55.22 & 69.44 & 87.55 & 90.66 & 41.52 & 51.92 \\
% $\mathrm{\mathbf{SGG_{point}}}$~\cite{zhang2021exploiting} & 51.44 & 74.56 & 84.15 & 90.69 & 97.09 & 98.51 & 27.95 & 46.27 & 58.85 & 87.89 & 90.16 & 39.28 & 52.09 \\
%  SGFN~\cite{wu2021scenegraphfusion} & 53.67 & 77.18 & 85.14 & 90.19 & 98.17 & 99.33 & 41.89 & 70.82 & 81.44 & 89.02 & 91.71 & 58.37 & 67.61 \\
%  \hline
%  \basetwo & \textbf{65.56} & \textbf{86.24} & \textbf{91.51} & 89.34 & 97.89 & 99.20 & 41.77 & 61.83 & 76.11 & \textbf{92.79} & \textbf{95.06} & 64.18 & \textbf{77.23} \\
%  \basethree & 54.79 & 77.62 & 85.84 & 89.59 & 97.63 & 99.08 & 41.99 & 70.88 & 81.67 & 88.96 & 91.37 & 59.58 & 67.75 \\
%  \hline
% %  Ours & 3D + 2D  & 65.88 & 86.53 & 91.46 & 90.78 & 55.66 & 98.45 & 78.61 & 99.56 & 89.82 & 93.09 & 74.10 & 95.22 & 81.38 \\
% %  Ours & 3D + RGB & 51.87 & 76.35 & 84.19 & 91.28 & 48.52 & 98.49 & 70.45 & 99.42 & 78.10 & 88.74 & 56.76 & 91.46 & 65.67 \\
%  Ours & 55.66 & 78.66 & 85.91 & 89.81 & \textbf{98.45} & \textbf{99.53} & \textbf{54.03} & \textbf{77.67} & \textbf{87.65} & 90.35 & 92.89 & \textbf{65.09}  & 73.59 \\
%  \hline
% \end{tabular}}
% \caption{
% %
% 3D scene graph prediction performance on 3DSSG validation set. 
% %
% Evaluation is conducted in terms of object, predicate, and triplet. 
% %
% The results of SGPN, $\mathrm{\mathbf{SGG_{point}}}$, and SGFN are based on our reproduced model, since they don't compute the mA@$k$ metric in the paper.
% }
% \label{tab:exp-results-old}
% \end{table*}

\section{Experiments and Discussions}
\label{sec:experiments_and_discussions}

\subsection{Setups and Implementation Details}
\label{subsec:exp-setup}

\noindent\textbf{Datasets.} 
%
We conduct experiments on 3DSSG~\cite{wald2020learning}. It is a 3D semantic scene graph dataset drawn from the 3RScan dataset~\cite{wald2019rio}, with rich annotations about instance segmentation masks and relation triplets.
%
It has $1553$ 3D reconstructed indoor scenes, $160$ classes of objects, and $26$ types of predicates.
%
% We sample $128$ points from a segmented instances to represent a node entity. 
%
% As for relationship initialization, we use the relative location information of two objects.
%
In the experiments, we use the same data preparation and training/validation split as in 3DSSG~\cite{wald2020learning}.

\vspace{+1mm}
\noindent\textbf{Metrics and Tasks.} 
%
We follow the experiment settings in 3DSSG~\cite{wald2020learning} . 
%
In both training and testing stages, 3D scenes are placed in the same 3D coordinate. The view-dependent spatial relation predicates are not ambiguous.
% To evaluate our method comprehensively, we follow the scheme in \cite{wald2020learning}, \eg we compute the top-k accuracy (A@$k$) to evaluate object and predicate prediction performance, which is referred to as R@$k$ in \cite{wald2020learning}.
% 
To evaluate the prediction of the object and predicate, we use the top-k accuracy (A@$k$) metric.
%
As for the triplets, we first multiply the subject, predicate, and object scores to get triplet scores, and then compute the top-k accuracy (A@$k$) as the evaluation metric.
% 
The triplet is considered correct only if the subject, predicate, and object are all correct\footnote{However, the metric top-k accuracy is written as the top-k recall or R@k in 3DSSG~\cite{wald2020learning} and SGFN~\cite{wu2021scenegraphfusion}.}.
%
% To alleviate the effects of long-tail distribution on test results, we also computed the average of the predicate and triplet accuracy on each predicate class as the mean top-k accuracy (mA@$k$) metric, which is a common practice in 2D scene graph prediction task \cite{tang2019learning}.
To fairly evaluate the performance of long-tailed predicate distribution, we also compute the average top-k accuracy of the predicate across all predicate classes, denoted as the mean top-k accuracy (mA@$k$).
%

% To compare with the method in Zhang~\etal~\cite{zhang2021knowledge}, 
%
We also conduct two 2D scene graph tasks proposed in \cite{xu2017scene} in the 3D scenario, as what Zhang~\etal~\cite{zhang2021knowledge} did, \ie,
%
(1) Scene Graph Classification (SGCls) that \textcolor{black}{evaluates} the triplet together.
%
(2) Predicate Classification (PredCls) that only \textcolor{black}{evaluates} the predicate \textcolor{black}{with the ground-truth labels of object entities.}
% given that the subject and object are correct.
%
Following Zhang~\etal~\cite{zhang2021knowledge}, we compute the recall at the top-k (R@$k$) triplets.
%
The triplet is considered correct when the subject, predicate, and object are all valid.
%
% As defined in ~\cite{lu2016visual}, the recall is the fraction of the correct top-k triplets against the ground truth.
%
Additionally, we also adopt mean recall (mR@$k$) to evaluate the performance on the unevenly sampled relations using a similar strategy as mA@$k$.

\vspace{+1mm}
% \noindent\textbf{Network Train and Inference Details.}
\noindent\textbf{Implementation Details.} 
%
Our network is end-to-end optimized using AdamW optimizer~\cite{DBLP:journals/corr/KingmaB14,DBLP:journals/corr/abs-1711-05101} with the batch size as $8$.
%
We train the network for $100$ epochs, and the base learning rate is set as $0.001$ with a cosine annealing learning rate decay strategy~\cite{loshchilov2016sgdr}.
%
$N_\text{obj}=160$ and $N_\text{rel}=26$ in our experiments.
%
GNN modules are repeated for $T=2$ times in both 3D and oracle multi-modal models.
%
$\lambda_{\text{obj}}=\lambda_{\text{aux}}=0.1$, $\lambda_{\text{pred}}=1$ in \cref{eq:loss_total}.
%
All experiments are carried out on the PyTorch platform equipped with one NVIDIA GeForce RTX 2080 Ti GPU card, and each experiment takes about $48$ hours until model convergence.
% What should be noted is that the 2D inputs are only used during the training stage.
Note that 2D inputs are only used during the training stage.
% 
During the inference stage, we follow the same strategy in  ~\cite{xu2017scene}, which selects the top@$1$ class of both object and predicate while giving an object instance index tuple. 
% %
% Then we multiply the object score, subject score and predicate score as the triplet score.
% %
% In the end, we sorted all triplet scores and select top@$k$ as our model's output.
% TODO 模型类型
%
Please refer to the supplementary for the details of the network structures.
%


\subsection{Comparison with the State-of-the-art Methods}
\label{subsec:exp-results}
%
We compare our method with a list of reference methods, \ie SGPN~\cite{wald2020learning}, \sggpoint~\cite{zhang2021exploiting}, SGFN~\cite{wu2021scenegraphfusion}, Co-Occurrence~\cite{zhang2021knowledge}, KERN~\cite{chen2019knowledge}, Schemata~\cite{sharifzadeh2021classification}, Zhang~\etal~\cite{zhang2021knowledge}. 
%
% SGPN~\cite{wald2020learning} is the pioneer in 3D scene graph prediction tasks, which uses a simple PointNet~\cite{qi2017pointnet} with GNN to generate scene graph predictions.
% %
% The following work SGFN~\cite{wu2021scenegraphfusion} leverages an attention-based node feature updating strategy in GNN and incrementally builds up scene graphs from RGB-D sequences.
% %
% In this comparison, the online updating strategy of SGFN is blocked.
% %
% Zhang~\etal~\cite{zhang2021knowledge} uses a graph auto-encoder to automatically extract class-wise representations as prior knowledge to enhance relationship prediction.
% %
% $\text{SGG}_\text{point}$~\cite{zhang2021exploiting} builds two associated twin interaction mechanisms between nodes and edges to effectively bridge perception and reasoning.
%
In addition, to gain a deeper understanding of our approach, we also report the performances of the oracle multi-modal prediction model (termed as VL-SAT (oracle)), as well as the baseline performance of the 3D prediction model that is trained purely by 3D data (term as non-VL-SAT). The proposed method is term as VL-SAT.

% we design some variants of our method. \basetwo uses a similar network structure as the teacher network except for multi-level feature enhancements and takes in 2D image patches of objects and 3D bounding box positions in the train and validation process. \basethree adopts the same network as the student network and takes in 3D point clouds in both train and validation 
% period.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{picture/freq.pdf}
\caption{
%
The line chart shows the predicate frequency in the train set of 3DSSG~\cite{wald2020learning}.
%
The bar chart shows the results on mA@$1$ of the predicate prediction of SGPN~\cite{wald2020learning} and our VL-SAT.
}
\label{fig:predicate_dis}
\end{figure}

\vspace{+1mm}
\noindent\textbf{Quantitative Results.}
%
The comparison results are summarized in \cref{tab:exp-results}.
%
The baseline ``non-VL-SAT'' has a similar performance as SGFN.
%
\textcolor{black}{The only difference between them is that ``non-VL-SAT'' adds a multi-head self-attention (MHSA) module~\cite{vaswani2017attention} before each GNN module in SGFN.}
%
Thanks to the delicate visual-linguistic assisted training scheme, our ``VL-SAT'' tremendously improves the baseline, according to the evaluation with respect to the predicate, and the triplet.
%
Moreover, according to the less biased mA@k metrics with respect to long-tailed distribution, when evaluating the predicate, the proposed ``VL-SAT'' outperforms the baseline ``non-VL-SAT'' with around $12.0\%$, $6.8\%$ and $6.0\%$ gains at mA@$1$, mA@$3$, and mA@$5$ respectively.
%
Our method reaches new state-of-the-art results on triplet prediction, with $6.8\%$ gain on mA@50 and $5.9\%$ gain on mA@100 over SGFN~\cite{wu2021scenegraphfusion}.
%
Note that the results of object classification just have a marginal improvement, which means that a simple PointNet-based 3D encoder may not be able to convey similar instance-level representative power as the 2D vision encoder.
%
% (TODO) It is interesting to find that our oracle model ``VL-SAT (oracle)'' further boosts the overall performance, but its gains mainly come from a better object classifier rather than better modeling of the predicate.

% Due to sufficient object texture and color information in 2D images, \basetwo performs well on object classification and brings about $12\%$ gains on A@$1$ compared with SGFN.
% %
% The shortcoming of \basetwo is also obvious. 
% %
% Because of lacking spatial information, \basetwo performs poorly on predicate prediction.
% %
% Our method gains $2\%$ improvement on object A@$1$ metric over SGFN~\cite{wu2021scenegraphfusion}, benefiting from the multi-level enhancement strategy.
% %
% As for the evaluation of predicate prediction, SGFN, \basethree, and our method all achieve remarkable performance on A@$k$ metric.
% %
% Regarding the mA@$k$ metric, which is less influenced by the long-tail distribution effect, our method outperforms SGFN by $12.2\%$ on the predicate mA@1 metric.
% %
% Thanks to the strong performance on the object and predicate prediction, our method reaches a new state-of-the-art result on triplet prediction, with $6.8\%$ gain on mA@50 and $5.9\%$ gain on mA@100 over SGFN\cite{wu2021scenegraphfusion}.


As illustrated in \cref{tab:exp-buaa1} and \cref{tab:exp-buaa2}, we also compare our ``VL-SAT'' with the reference methods, with respect to two tasks named SGCls and PredCls, according to the settings introduced by Zhang~\etal~\cite{zhang2021knowledge}.
%
Our method outperforms Zhang~\etal~\cite{zhang2021knowledge} by a large margin.
%
For example, with graph constraint~\cite{xu2017scene} (as a more rigorous testing scenario~\cite{zareian2020bridging}), ``VL-SAT'' has $2.5\%$ gain on R@$20$ in SGCls, $8.5\%$ gain on R@$20$ in PredCls.
%
Moreover, with respect to the less biased metrics in \cref{tab:exp-buaa2}, ``VL-SAT'' even achieves $6.6\%$ gains on mR@$20$ in SGCls than Zhang~\etal~\cite{zhang2021knowledge}.

% \begin{table*}
% \centering
% \resizebox{\linewidth}{!}{
%         \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c}
%         \hline
% \multirow{2}{*}{Model} & \multicolumn{3}{c|}{Object} & \multicolumn{6}{c|}{Predicate} & \multicolumn{4}{c}{Triplet} \\
%  \cline{2-14}
%  & R@$1$ & R@$5$ & R@$10$ & R@$1$ & mR@$1$ & R@$3$ & mR@$3$ & R@$5$ & mR@$5$ & R@$50$ & mR@$50$ & R@$100$ & mR@$100$ \\
%  \hline
%  SGPN~\cite{wald2020learning} & 48.28 & 72.94 & 82.74 & 91.32 & 32.01 & 98.09 & 55.22 & 99.15 & 69.44 & 87.55 & 41.52 & 90.66 & 51.92 \\
%  SGFN~\cite{wu2021scenegraphfusion} & 53.67 & 77.18 & 85.14 & 90.19 & 41.89 & 98.17 & 70.82 & 99.33 & 81.44 & 89.02 & 58.37 & 91.71 & 67.61 \\
%  \hline
%  $\mathrm{Base_{2d}}$ & \textbf{65.56} & \textbf{86.24} & \textbf{91.51} & 89.34 & 41.77 & 97.89 & 61.83 & 99.20 & 76.11 & \textbf{92.79} & 64.18 & \textbf{95.06} & \textbf{77.23} \\
%  \revise{$\mathrm{Base_{3d}}$} & 55.43 & 79.09 & 86.51 & \textbf{91.81} & 52.30 & \textbf{98.70} & 71.48 & \textbf{99.64} & 82.75 & 89.71 & 60.74 & 92.96 & 71.56 \\
%  \hline
% %  Ours & 3D + 2D  & 65.88 & 86.53 & 91.46 & 90.78 & 55.66 & 98.45 & 78.61 & 99.56 & 89.82 & 93.09 & 74.10 & 95.22 & 81.38 \\
% %  Ours & 3D + RGB & 51.87 & 76.35 & 84.19 & 91.28 & 48.52 & 98.49 & 70.45 & 99.42 & 78.10 & 88.74 & 56.76 & 91.46 & 65.67 \\
%  Ours & 55.66 & 78.66 & 85.91 & 89.81 & \textbf{54.03} & 98.45 & \textbf{77.67} & 99.53 & \textbf{87.65} & 90.35 & \textbf{65.09} & 92.89  & 73.59 \\
%  \hline
% \end{tabular}}
% \caption{
% %
% 3D scene graph prediction performance on 3DSSG validation set. 
% %
% Evaluation is conducted in terms of object, predicate, and triplet. 
% %
% The results of SGFN and SGPN are based on our reproduced model.
% %
% }
% \label{tab:exp-results}
% \end{table*}

\begin{table}
\caption{
%
Quantitative results of the compared methods with respect to the SGCls and PredCls tasks, with and without graph constraint. The evaluation metric is top-$k$ recall.
}
\label{tab:exp-buaa1}
\centering
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{c|c|c}
\hline
\multicolumn{1}{c|}{}& SGCls & PredCls \\
\cline{2-3}
Model & R@$20$/$50$/$100$ & R@$20$/$50$/$100$ \\
\midrule
\multicolumn{3}{c}{with Graph Constraints} \\
\hline
Co-Occurrence~\cite{zhang2021knowledge} & 14.8/19.7/19.9 & 34.7/47.4/47.9 \\
KERN~\cite{chen2019knowledge} & 20.3/22.4/22.7 & 46.8/55.7/56.5 \\
SGPN~\cite{wald2020learning} & 27.0/28.8/29.0 & 51.9/58.0/58.5 \\
Schemata~\cite{sharifzadeh2021classification} & 27.4/29.2/29.4 & 48.7/58.2/59.1 \\
Zhang~\etal~\cite{zhang2021knowledge} & 28.5/30.0/30.1 & 59.3/65.0/65.3 \\
SGFN~\cite{wu2021scenegraphfusion} & 29.5/31.2/31.2 & 65.9/78.8/79.6 \\
VL-SAT (ours) & \textbf{32.0}/\textbf{33.5}/\textbf{33.7} & \textbf{67.8}/\textbf{79.9}/\textbf{80.8} \\
\midrule
\multicolumn{3}{c}{without Graph Constraints} \\
\hline
Co-Occurrence~\cite{zhang2021knowledge} & 14.1/20.2/25.8 & 35.1/55.6/70.6 \\
KERN~\cite{chen2019knowledge} & 20.8/24.7/27.6 & 48.3/64.8/77.2 \\
SGPN~\cite{wald2020learning} & 28.2/32.6/35.3 & 54.5/70.1/82.4 \\
Schemata~\cite{sharifzadeh2021classification} & 28.8/33.5/36.3 & 49.6/67.1/80.2 \\
Zhang~\etal~\cite{zhang2021knowledge} & 29.8/34.3/37.0 & 62.2/78.4/88.3 \\
SGFN~\cite{wu2021scenegraphfusion} & 31.9/39.3/45.0 & 68.9/82.8/91.2 \\
VL-SAT (ours) & \textbf{33.8}/\textbf{41.3}/\textbf{47.0} & \textbf{70.5}/\textbf{85.0}/\textbf{92.5} \\
\hline
\end{tabular}}
\end{table}

% \begin{table}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{cc|c|c|c|c|c|c}
% \hline
% \multicolumn{2}{c|}{}&\multicolumn{3}{|c|}{SGCls} & \multicolumn{3}{|c}{PredCls} \\
% \cline{3-8}
% & Model & R@$20$ & R@$50$ & R@$100$ & R@$20$ & R@$50$ & R@$100$ \\
% \hline
% \multirow{7}{*}{w/ GC} & Co-Occurrence~\cite{zhang2021knowledge} & 14.8 & 19.7 & 19.9 & 34.7 & 47.4 & 47.9 \\
% & KERN~\cite{chen2019knowledge} & 20.3 & 22.4 & 22.7 & 46.8 & 55.7 & 56.5 \\
% & SGPN~\cite{wald2020learning} & 27.0 & 28.8 & 29.0 & 51.9 & 58.0 & 58.5 \\
% & Schemata~\cite{sharifzadeh2021classification} & 27.4 & 29.2 & 29.4 & 48.7 & 58.2 & 59.1 \\
% & Zhang2021~\cite{zhang2021knowledge} & 28.5 & 30.0 & 30.1 & 59.3 & 65.0 & 65.3 \\
% & SGFN~\cite{wu2021scenegraphfusion} & 29.5 & 31.2 & 31.2 & 65.9 & 78.8 & 79.6 \\
% & Ours & \textbf{32.0} & \textbf{33.5} & \textbf{33.7} & \textbf{67.8} & \textbf{79.9} & \textbf{80.8} \\
% \hline 
% \multirow{7}{*}{w/o GC} & Co-Occurrence~\cite{zhang2021knowledge} & 14.1 & 20.2 & 25.8 & 35.1 & 55.6 & 70.6 \\
% & KERN~\cite{chen2019knowledge} & 20.8 & 24.7 & 27.6 & 48.3 & 64.8 & 77.2 \\
% & SGPN~\cite{wald2020learning} & 28.2 & 32.6 & 35.3 & 54.5 & 70.1 & 82.4 \\
% & Schemata~\cite{sharifzadeh2021classification} & 28.8 & 33.5 & 36.3 & 49.6 & 67.1 & 80.2 \\
% & Zhang2021~\cite{zhang2021knowledge} & 29.8 & 34.3 & 37.0 & 62.2 & 78.4 & 88.3 \\
% & SGFN~\cite{wu2021scenegraphfusion} & 31.9 & 39.3 & 45.0 & 68.9 & 82.8 & 91.2 \\
% & Ours & \textbf{33.8} & \textbf{41.3} & \textbf{47.0} & \textbf{70.5} & \textbf{85.0} & \textbf{92.5} \\
% \hline
% \end{tabular}}
% \caption{
% %
% Quantitative results of the evaluated methods in SGCls and PredCls tasks in recall, with and without graph constraint (GC).
% }
% \label{tab:exp-buaa1}
% \end{table}

\begin{table}
\centering
\caption{
%
Quantitative results of the compared methods with respect to the SGCls and PredCls tasks, with graph constraint. The evaluation metric is top-$k$ mean recall.
}
\label{tab:exp-buaa2}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{c|c|c}
\hline
&\multicolumn{1}{|c|}{SGCls} & \multicolumn{1}{|c}{PredCls} \\
\cline{2-3}
Model & mR@$20$/$50$/$100$ & mR@$20$/$50$/$100$ \\
\hline
Co-Occurrence~\cite{zhang2021knowledge} & 8.8/12.7/12.9 & 33.8/47.4/47.9 \\
KERN~\cite{chen2019knowledge} & 9.5/11.5/11.9 & 18.8/25.6/26.5 \\
SGPN~\cite{wald2020learning} & 19.7/22.6/23.1 & 32.1/38.4/38.9 \\
Schemata~\cite{sharifzadeh2021classification} & 23.8/27.0/27.2 & 35.2/42.6/43.3 \\
Zhang~\etal~\cite{zhang2021knowledge} & 24.4/28.6/28.8 & 56.6/63.5/63.8 \\
SGFN~\cite{wu2021scenegraphfusion} & 20.5/23.1/23.1 & 46.1/54.8/55.1 \\
VL-SAT(ours) & \textbf{31.0}/\textbf{32.6}/\textbf{32.7} & \textbf{57.8}/\textbf{64.2}/\textbf{64.3} \\
\hline 
\end{tabular}}
\end{table}

\begin{figure*}
\centering
\includegraphics[width=0.99\linewidth]{picture/3dssg.pdf}
\caption{
%
Qualitative results from SGFN~\cite{wu2021scenegraphfusion} and our method on the 3DSSG~\cite{wald2020learning} dataset.
%
\textit{Red edge}: miss-classified edges from SGFN, \textit{green edge}: edges corrected by our method, \textit{red node}: miss-classified node.
}
\label{fig:qualitative}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.95\linewidth]{picture/scannet.pdf}
\caption{
%
Qualitative results from SGFN~\cite{wu2021scenegraphfusion} and our method on ScanNet~\cite{dai2017scannet} dataset. 
%
Note that there are no annotations of scene graphs on ScanNet~\cite{dai2017scannet}, so we utilize the descriptions from ScanRefer~\cite{chen2020scanrefer} to parse the relationships between different objects manually.
%
\textit{Red edge}: miss-classified edges from SGFN, \textit{green edge}: edges corrected by our method, \textit{red node}: miss-classified node.
}
\label{fig:qualitative_scannet}
\end{figure*}

%
\begin{table*}
\centering
\caption{
%
Based on the distribution of the predicates in the train set of the 3DSSG dataset~\cite{wald2020learning}, we split the $26$ predicate classes into the head, body, and tail classes, and then compute mA@3 and mA@5 metrics on each split.
%
% Besides, we also split the predicate classes into spatial predicate and semantic predicate, based on whether the predicate describes a spatial relation.
%
Moreover, we test several methods on unseen and seen triplets in the validation set to evaluate the generalization ability of these methods.
}
\label{tab:long-tail}
\resizebox{0.8\linewidth}{!}{
        \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{3}{*}{Model}& \multicolumn{6}{|c|}{Predicate} & \multicolumn{4}{|c}{Triplet} \\
\cline{2-11}
&\multicolumn{2}{c|}{Head} & \multicolumn{2}{|c|}{Body} & \multicolumn{2}{|c}{Tail} & \multicolumn{2}{|c|}{Unseen} & \multicolumn{2}{|c}{Seen} \\
 \cline{2-11}
& mA@$3$ & mA@$5$ &mA@$3$ & mA@$5$ & mA@$3$ & mA@$5$ & A@50 & A@100 & A@50 & A@100 \\
 \hline
 SGPN~\cite{wald2020learning} & \textbf{96.66} & 99.17 & 66.19 & 85.73 & 10.18 & 28.41 & 15.78 & 29.60 & 66.60 & 77.03  \\
 SGFN~\cite{wu2021scenegraphfusion} & 95.08 & \textbf{99.38} & 70.02 & 87.81 & 38.67 & 58.21 & 22.59 & 35.68 & 71.44 & 80.11 \\
 \hline
 non-VL-SAT & 95.32 & 99.01 & 71.88 & 88.64 & 40.01 & 58.33 & 21.99 & 35.44 & 71.52 & 80.34  \\
 VL-SAT (ours) & 96.31 & 99.21 & \textbf{80.03} & \textbf{93.64}  & \textbf{52.38} & \textbf{66.13} & \textbf{31.28} & \textbf{47.26} & \textbf{75.09} & \textbf{82.25} \\
 \hline
\end{tabular}}
\end{table*}

\vspace{+1mm}
\noindent\textbf{Qualitative Results.}
%
We provide some qualitative results between SGFN and our ``VL-SAT'' in \cref{fig:qualitative}.
%
These results demonstrate that our method can predict more reliable scene graphs with more accurate edges and nodes.
%
For example, our method successfully distinguishes some similar predicate, like \textit{standing on} versus \textit{supported by} and further disambiguates related instances, such as \textit{shower curtain} versus \textit{bath cabinet}.
%
The results conducted on ScanNet~\cite{dai2017scannet} in \cref{fig:qualitative_scannet} validate that VL-SAT is generalizable to more datasets.

\subsection{More Evaluations about Predicate and Triplet}
\label{subsec:exp-long-tail}

\noindent\textbf{Tail Predicates.} 
%
In \cref{fig:predicate_dis}, we visualize the frequency of the predicates in the train set as the line chart, which shows the long-tail distribution.
%
We also show the per-class predicate prediction performances of ``VL-SAT'' and SGFN~\cite{wald2020learning} in the bar chart.
%
Compared with SGFN, our method gets a significant improvement in the tail categories.
%
To further explore the improvements brought by ``VL-SAT'', we split the $26$ predicate classes into three parts: \textit{head}, \textit{body}, and \textit{tail} according to their frequencies in the train set, and calculate mA@$k$ metric.
%
In \cref{tab:long-tail}, we obtain $13.71\%$ improvement on mA@$3$ when predicting the predicate on tail categories.
%
\textcolor{black}{Compared with SGFN, our method slightly drops on some head classes but significantly increases on tail classes.
%
Since our VL-SAT boosts the overall performance by a large margin, such a slight performance degradation in head predicates is acceptable.}
%
We also provide some examples of tail predicates in \cref{fig:qualitative}.
%
In the top row, our method correctly predicts tail predicates like \textit{supported by}.
%
In the bottom row, our method corrects the relation between the kitchen counter and the kitchen cabinet from \textit{attached to} to \textit{part of}.

% Definition of tail predicates can be found in supplementary.

% \noindent\textbf{Semantic predicate analysis.}
% %
% \revise{To show the gains on the predicate which contains semantic information, we also split the $26$ predicates into two parts, \eg predicates about the spatial structure like behind, and the semantic predicates like same as.
% %
% In table \ref{tab:long-tail}, we also obtain about $12\%$ and $7\%$ improvements on mR@$3$ and mR@$5$ of predicate evaluation on the semantic predicates, respectively.
% %
% The above result convinces us that, our methods not only ease the long-tail distribution on the training dataset but also introduce the semantic information on the spatial-only input.}

\vspace{+1mm}
\noindent\textbf{Unseen Triplets.} 
%
We consider relation triplets that do not appear in the train set as unseen triplets. 
%
In \cref{tab:long-tail}, our method gains about $8.69\%$ on A@$50$ on unseen triplets compared with SGFN~\cite{wald2020learning}.
%
The results validate that thanks to the VL-SAT scheme, our model can convey more robust feature representations based on the 3D point cloud, which leads to a better generalization ability on unseen triplets.


%
% The results show that our method performs well on tail predicates.

% \begin{table*}
% \centering
% \resizebox{\linewidth}{!}{
%         \begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{3}{*}{Model}& \multicolumn{6}{|c|}{Predicate} & \multicolumn{4}{|c|}{Predicate} & \multicolumn{4}{|c}{Triplet} \\
% \cline{2-15}
% &\multicolumn{2}{c|}{Head} & \multicolumn{2}{|c|}{Body} & \multicolumn{2}{|c}{Tail} & \multicolumn{2}{|c}{Spatial} & \multicolumn{2}{|c}{Semantic} & \multicolumn{2}{|c|}{Unseen} & \multicolumn{2}{|c}{Seen} \\
%  \cline{2-15}
% & mA@$3$ & mA@$5$ &mA@$3$ & mA@$5$ & mA@$3$ & mA@$5$  & mA@$3$ & mA@$5$ & mA@$3$ & mA@$5$ & A@50 & A@100 & A@50 & A@100 \\
%  \hline
%  SGPN & \textbf{96.66} & 99.17 & 66.19 & 85.73 & 10.18 & 28.41 & \textbf{82.76} & 85.97 & 39.53 & 57.22 & 15.78 & 29.60 & 66.60 & 77.03  \\
%  SGFN & 95.08 & \textbf{99.38} & 70.02 & 87.81 & 38.67 & 58.21 & 81.98 & \textbf{86.87} & 58.07 & 75.89 & 22.59 & 35.68 & 71.44 & 80.11 \\
%  \hline
%  \basethree & 95.32 & 99.01 & 71.88 & 88.64 & 40.01 & 58.33 & 81.41 & 86.17 & 60.19 & 75.33 & 21.99 & 35.44 & 71.52 & 80.34  \\
%  Ours & 96.31 & 99.21 & \textbf{80.03} & \textbf{93.64}  & \textbf{52.38} & \textbf{66.13} & 82.57 & 86.66 & \textbf{70.14} & \textbf{82.76} & \textbf{31.28} & \textbf{47.26} & \textbf{75.09} & \textbf{82.25} \\
%  \hline
% \end{tabular}}
% \caption{
% %
% Based on the frequency of the predicates in the train set, we split the $26$ classes of predicates into the head, body, and tail classes, and then compute mA@3 and mA@5 on each split.
% %
% Besides, we also split the predicate classes into spatial predicate and semantic predicate, based on whether the predicate describes a spatial relation.
% %
% Moreover, we test several methods on unseen and seen triplets in the validation set to evaluate the generalization ability of these methods.
% }
% \label{tab:long-tail}
% \end{table*}

\begin{table}
\centering
\caption{
%
Results of our method when different modules are ablated.
%
CI means CLIP-initialized object classifier.
% 
NC means node-level collaboration.
%
EC means edge-level collaboration.
%
TR means triplet-level CLIP-based regularization.
}
\label{tab:ablation_exp}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{CI} & \multirow{2}{*}{NC} & \multirow{2}{*}{EC} & \multirow{2}{*}{TR} & \multicolumn{2}{|c|}{Object} & \multicolumn{2}{|c|}{Predicate} & \multicolumn{2}{|c}{Triplet} \\
\cline{5-10}
& & & & A@$5$ & A@$10$ & mA@$3$ & mA@$5$ & mA@$50$ & mA@$100$ \\
\hline
% & & & & 77.62 & 85.84 & 70.88 & 81.67 & 59.58 & 67.75 \\
% \checkmark & & & & 79.03 & 86.81 & 72.50 & 83.59 & 60.65\color{red}{$\uparrow$1.07} & 69.71 \\
% \checkmark & \checkmark & & & \textbf{79.28} & \textbf{86.82} & 73.92 & 84.78 & 62.88\color{red}{$\uparrow$2.23} & 71.84 \\
% \checkmark & \checkmark & \checkmark & & 78.71 & 86.17 & 76.92 & 87.08 & 64.00\color{red}{$\uparrow$1.12} & 72.42 \\
% \checkmark & \checkmark & \checkmark & \checkmark & 78.66 & 85.91  & \textbf{77.67} & \textbf{87.65} & \textbf{65.09}\color{red}{$\uparrow$1.09} & \textbf{73.59} \\
& & & & 77.62 & 85.84 & 70.88 & 81.67 & 59.58 & 67.75 \\
\checkmark & & & & 79.03 & 86.81 & 72.50 & 83.59 & 60.65 & 69.71 \\
\checkmark & \checkmark & & & \textbf{79.28} & \textbf{86.82} & 73.92 & 84.78 & 62.88 & 71.84 \\
\checkmark & \checkmark & \checkmark & & 78.71 & 86.17 & 76.92 & 87.08 & 64.00 & 72.42 \\
\checkmark & \checkmark & \checkmark & \checkmark & 78.66 & 85.91  & \textbf{77.67} & \textbf{87.65} & \textbf{65.09} & \textbf{73.59} \\
\hline
\end{tabular}}
\end{table}

% \begin{table}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{2}{*}{CI} & \multirow{2}{*}{OE} & \multirow{2}{*}{RE} & \multirow{2}{*}{TE} & \multicolumn{3}{|c|}{Object} & \multicolumn{3}{|c|}{Predicate} & \multicolumn{2}{|c}{Triplet} \\
% \cline{5-12}
% & & & & A@$1$ & A@$5$ & A@$10$ & mA@$1$ & mA@$3$ & mA@$5$ & mA@$50$ & mA@$100$ \\
% \hline
% & & & & 54.79 & 77.62 & 85.84 & 41.99 & 70.88 & 81.67 & 59.58 & 67.75 \\
% \checkmark & & & & 56.31 & 79.03 & 86.81 & 52.01\color{red}{$\uparrow$10.02} & 72.50\color{red}{$\uparrow$1.62} & 83.59 & 60.65\color{red}{$\uparrow$1.07} & 69.71 \\
% \checkmark & \checkmark & & & 55.89 & \textbf{79.28} & \textbf{86.82} & 52.52\color{red}{$\uparrow$0.51} & 73.92\color{red}{$\uparrow$1.42} & 84.78 & 62.88\color{red}{$\uparrow$2.23} & 71.84 \\
% \checkmark & \checkmark & \checkmark & & \textbf{56.40} & 78.71 & 86.17 & 53.55\color{red}{$\uparrow$1.03} & 76.92\color{red}{$\uparrow$3.00} & 87.08 & 64.00\color{red}{$\uparrow$1.12} & 72.42 \\
% \checkmark & \checkmark & \checkmark & \checkmark & 55.66 & 78.66 & 85.91 & \textbf{54.03}\color{red}{$\uparrow$0.48} & \textbf{77.67}\color{red}{$\uparrow$0.75} & \textbf{87.65} & \textbf{65.09}\color{red}{$\uparrow$1.09} & \textbf{73.59} \\
% \hline
% \end{tabular}}
% \caption{
% %
% Results of our method when different modules are ablated.
% %
% CI means CLIP-initialized object classifier.
% % 
% OE means object-level feature enhancement.
% %
% RE means relation-level feature enhancement.
% %
% TE means triplet-level feature enhancement.
% }
% \label{tab:ablation_exp_old}
% \end{table}

% \begin{table}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{2}{*}{CI} & \multirow{2}{*}{OE} & \multirow{2}{*}{RE} & \multirow{2}{*}{TE} & \multicolumn{3}{|c|}{Object} & \multicolumn{3}{|c|}{Predicate} & \multicolumn{2}{|c}{Triplet} \\
% \cline{5-12}
% & & & & A@$1$ & A@$5$ & A@$10$ & mA@$1$ & mA@$3$ & mA@$5$ & mA@$50$ & mA@$100$ \\
% \hline
% % & & & & 54.79 & 77.62 & 85.84 & 41.99 & 70.88 & 81.67 & 59.58 & 67.75 \\
% $\times$ & \checkmark & \checkmark & $\times$ & 55.53\color{green}{$\downarrow$0.13} & 78.36 & 86.24 & 49.51\color{green}{$\downarrow$4.52} & 73.70 & 85.12 & 63.56\color{green}{$\downarrow$1.53} & 72.10 \\
% $\times$ & \checkmark & \checkmark & \checkmark & - & - & - & - & - & - & - & - \\
% \checkmark & $\times$ & \checkmark & \checkmark & 55.28\color{green}{$\downarrow$0.38} & 78.9 & 86.41 & 50.04\color{green}{$\downarrow$3.99} & 71.59 & 81.37 & 61.56\color{green}{$\downarrow$3.53} & 71.05 \\
% \checkmark & \checkmark & $\times$ & \checkmark & 56.59\color{red}{$\uparrow$0.93} & 78.38 & 86.66 & 51.54\color{green}{$\downarrow$2.49} & 77.32 & 87.09 & 64.16\color{green}{$\downarrow$0.93} & 72.93 \\
% \checkmark & \checkmark & \checkmark & $\times$ & 56.40\color{red}{$\uparrow$0.74} & 78.71 & 86.71 & 53.55\color{green}{$\downarrow$0.48} & 76.92 & 87.08 & 64.00\color{green}{$\downarrow$1.09} & 72.42 \\
% \checkmark & \checkmark & \checkmark & \checkmark & 55.66 & 78.66 & 85.91 & 54.03 & 77.67 & 87.65 & 65.09 & 73.59 \\
% \hline
% \end{tabular}}
% \caption{
% %
% Results of our method when different modules are ablated.
% %
% CI means CLIP-initialized object classifier.
% % 
% OE means object-level feature enhancement.
% %
% RE means relation-level feature enhancement.
% %
% TE means triplet-level feature enhancement.
% }
% \label{tab:ablation_exp1}
% \end{table}

\begin{table}
\centering
\caption{
%
Our method with different cross-modal collaboration operations.
% 
NC means node-level collaboration.
%
EC means edge-level collaboration.
%
CT means concatenation.
%
% MC means mimic.
%
% KNN means k-nearest neighbor.
%
CA means cross-attention in our method.
}
\label{tab:fusion}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c}
\hline
\multirow{2}{*}{NC} & \multirow{2}{*}{EC} & \multicolumn{2}{|c|}{Object} & \multicolumn{2}{|c|}{Predicate} & \multicolumn{2}{|c}{Triplet} \\
\cline{3-8}
& & A@$1$ & A@$5$ & mA@$1$ & mA@$3$ & mA@$50$ & mA@$100$ \\
\hline
% MC & MC & - & -& -& -& -& -& -& - \\
% MC & CA & - & -& -& -& -& -& -& - \\
% CA & MC & - & -& -& -& -& -& -& - \\
% \hline
% CT & CT & 55.78 & 77.58 & 85.23 & 51.64 & 74.13 & 83.49 & 60.37\color{green}{$\downarrow$4.72} & 72.66 \\
% CT & CA & 56.14 & 78.38 & 86.60 & 52.28 & 75.04 & 84.52 & 61.50\color{green}{$\downarrow$3.59} & 73.80 \\
% CA & CT & 56.00 & 77.68 & 85.86 & 52.14 & 73.54 & 87.55 & 63.92\color{green}{$\downarrow$1.17} & 73.10 \\
CT & CT & 55.78 & 77.58 & 51.64 & 74.13 & 60.37 & 72.66 \\
CT & CA & 56.14 & 78.38 & 52.28 & 75.04 & 61.50 & 73.80 \\
CA & CT & 56.00 & 77.68 & 52.14 & 73.54 & 63.92 & 73.10 \\
\hline
% KNN & KNN & - & -& -& -& -& -& -& - \\
% KNN & CA & 55.51 & 78.12 & 85.71 & 52.88\color{green}{$\downarrow$1.15} & 75.83 & 87.02 & 63.67\color{green}{$\downarrow$1.42} & 73.26 \\
% CA & KNN & 55.47 & 78.10 & 86.15 & 52.27\color{green}{$\downarrow$1.76} & 73.92 & 87.98 & 64.14\color{green}{$\downarrow$0.95} & 73.39 \\
% \hline
CA & CA & 55.66 & 78.66 & 54.03 & 77.67 & 65.09 & 73.59 \\
\hline
\end{tabular}}
\end{table}

% \begin{table}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{2}{*}{OE} & \multirow{2}{*}{RE} & \multicolumn{3}{|c|}{Object} & \multicolumn{3}{|c|}{Predicate} & \multicolumn{2}{|c}{Triplet} \\
% \cline{3-10}
% & & A@$1$ & A@$5$ & A@$10$ & mA@$1$ & mA@$3$ & mA@$5$ & mA@$50$ & mA@$100$ \\
% \hline
% % MC & MC & - & -& -& -& -& -& -& - \\
% % MC & CA & - & -& -& -& -& -& -& - \\
% % CA & MC & - & -& -& -& -& -& -& - \\
% % \hline
% % CT & CT & - & -& -& -& -& -& -& - \\
% CT & CA & 56.14 & 78.38 & 86.60 & 52.28\color{green}{$\downarrow$1.75} & 75.04 & 84.52 & 61.50\color{green}{$\downarrow$3.59} & 73.80 \\
% CA & CT & 56.00 & 77.68 & 85.86 & 52.14\color{green}{$\downarrow$1.89} & 73.54 & 87.55 & 63.92\color{green}{$\downarrow$1.17} & 73.10 \\
% \hline
% % KNN & KNN & - & -& -& -& -& -& -& - \\
% % KNN & CA & 55.51 & 78.12 & 85.71 & 52.88\color{green}{$\downarrow$1.15} & 75.83 & 87.02 & 63.67\color{green}{$\downarrow$1.42} & 73.26 \\
% % CA & KNN & 55.47 & 78.10 & 86.15 & 52.27\color{green}{$\downarrow$1.76} & 73.92 & 87.98 & 64.14\color{green}{$\downarrow$0.95} & 73.39 \\
% % \hline
% CA & CA & 55.66 & 78.66 & 85.91 & 54.03 & 77.67 & 87.65 & 65.09 & 73.59 \\
% \hline
% \end{tabular}}
% \caption{
% %
% Our method with different cross-modal enhancement strategies.
% % 
% OE means object-level feature enhancement.
% %
% RE means relation-level feature enhancement.
% %
% CT means concatenation.
% %
% % MC means mimic.
% %
% % KNN means k-nearest neighbour.
% %
% CA means cross-attention in our method.
% }
% \label{tab:fusion}
% \end{table}

% \begin{table}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c|c|c}
% \hline
% \multirow{2}{*}{\#GNN} & \multicolumn{3}{|c|}{Object} & \multicolumn{3}{|c|}{Predicate} & \multicolumn{2}{|c}{Triplet} \\
% \cline{2-9}
% & A@$1$ & A@$5$ & A@$10$ & mA@$1$ & mA@$3$ & mA@$5$ & mA@$50$ & mA@$100$ \\
% \hline
% $1$ & \textbf{55.76} & 78.40 & \textbf{86.79} & 48.54 & 68.45 & 77.60 & 57.10 & 70.91 \\
% $2$ & 55.66 & \textbf{78.66} & 85.91 & \textbf{54.03} & \textbf{77.67} & \textbf{87.65} & \textbf{65.09} & \textbf{73.59} \\
% $3$ & 55.09 & 77.39 & 85.33 & 42.24 & 67.84 & 83.93 & 62.54 & 71.13 \\
% $4$ & 53.95 & 76.99 & 84.78 & 40.10 & 63.64 & 79.47 & 61.37 & 71.08 \\
% \hline
% \end{tabular}}
% \caption{
% Performance of our method when different number of GNN layers are used.
% }
% \label{tab:gnn}
% \end{table}

\begin{table}
\centering
\caption{
%
Performance gains brought by our VL-SAT scheme with two reference 3DSSG prediction models.
}
\label{tab:sgg}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c}
\hline
& \multicolumn{2}{|c|}{Object} & \multicolumn{2}{|c|}{Predicate} & \multicolumn{2}{|c}{Triplet} \\
\cline{2-7}
& A@$1$ & A@$5$ & mA@$1$ & mA@$3$ & mA@$50$ & mA@$100$ \\
\hline
\sggpoint~\cite{zhang2021exploiting} & 51.42 & 74.56 & 27.95 & 49.98 & 45.02 & 56.03 \\
+VL-SAT & 52.08 & 75.76 & 38.04 & 60.36 & 52.51 & 64.31 \\
\hline
%  non-VL-SAT & 54.79 & 77.62 & 41.99 & 70.88 & 59.58 & 67.75 \\
%  VL-SAT (ours) & 55.66 & 78.66 & 54.03 & 77.67 & 65.09 & 73.59 \\
SGFN~\cite{wu2021scenegraphfusion} & 53.67 & 77.18 & 41.89 & 70.82 & 58.37 & 67.61 \\
+VL-SAT & 55.43 & 78.88 & 52.91 & 72.37 & 63.57 & 72.02 \\
\hline
\end{tabular}}
\end{table}




\subsection{Ablation Study and Analysis}
\label{subsec:ablation}

\noindent\textbf{Ablation Study.} 
%
In \cref{tab:ablation_exp}, we conduct a comprehensive ablation study.
%
The first row denotes the baseline method ``no-VL-SAT''.
% 
From \cref{tab:ablation_exp}, we could observe that the
%
% CLIP-initialized object classifier brings about $1.41\%$ gains on object A@$5$ and $1.62\%$ gains on predicate mA@$3$.
%
CLIP-initialized object classifier brings about $1.41\%$ gains on object A@$5$.
%
Node and edge-level collaboration and triplet-level CLIP-based regularization steadily bring gains on triplet prediction, with $2.23\%$, $1.12\%$, and $1.09\%$ boost on mA@$50$ metric. 
%
% The ablation study well demonstrates the effectiveness of each designed component.
%
\textcolor{black}{It is worth noting that regularizing the training of predicates and triplets may bring bias to the representation of objects, which leads to a slight drop in object prediction when EC/TR is employed.
%
Thanks to the NC/CI modules, this degradation is not severe, which validates the effectiveness of our VL-SAT training scheme.}

\vspace{+1mm}
\noindent\textbf{Different Cross-modal Collaboration Strategies.} 
%
We investigate the effects of different cross-modal collaboration operations in~\cref{tab:fusion}.
%
We compare a simple operation named CT, in which we just concatenate corresponding features between two models.
%
% CT can be applied to either node or edge-level collaboration.
%
When CT is applied, the mA@$50$ metric of the triplet prediction drops significantly.
%
% By employing our multi-head cross-attention (termed as CA) in both collaborations, significant gains in predicate and triplet predication can be observed.
%
By employing our multi-head cross-attention (termed as CA) in both collaborations, significant gains can be observed.
%
% The results show the effectiveness of  modules used in node and edge-level collaboration.

% \vspace{+1mm}
% \noindent\textbf{Number of GNN layers.} 
% %
% With more GNN layers, the performance of our method first increases and then drops, and the watershed is $2$.
% %
% So, we choose to use $2$ layers of GNN in our method.

\vspace{+1mm}
\noindent\textbf{Generalization Ability.}
%
In \cref{tab:sgg}, we also show the performance gains brought by our VL-SAT scheme with two reference 3DSSG prediction models, namely \sggpoint~\cite{zhang2021exploiting} and SGFN~\cite{wu2021scenegraphfusion}.
%
VL-SAT shows consistent performance gains with different 3D prediction models, especially with respect to the evaluation of predicate and triplet.
%
% The results demonstrate the effectiveness of VL-SAT when a different model is adopted.
% The results demonstrate the VL-SAT scheme is generalizable to common 3DSSG prediction models.