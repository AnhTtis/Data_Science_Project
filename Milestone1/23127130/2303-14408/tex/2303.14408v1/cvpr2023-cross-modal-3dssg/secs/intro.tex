% !TEX root = ../PaperForReview.tex


\section{Introduction}
\label{sec:intro}

Structurally understanding 3D geometric scenes is particularly important for tasks that require interaction with real-world environments, such as AR/VR~\cite{qi2019deep,zhao2021transformer3d,cheng2021back,shi2020points,shi2019pointrcnn,shi2020pv,zhang2020h3dnet} and navigation~\cite{cai20223djcg,he2021transrefer3d,chen2020scanrefer}.
%
As one vital topic in this field, predicting 3D semantic scene graph (3DSSG) in point cloud~\cite{wald2020learning} has received emerging attention in recent years.
%
% Specifically, given the point cloud of a 3D scene that is associated with class-agnostic 3D instance masks, the 3DSSG prediction task would like to construct a directed graph whose nodes are semantic labels of the 3D instances and the edges recognize the directional semantic or geometrical relations (termed as \emph{predicate}) between connected 3D instances (termed as \emph{subject} and \emph{object}), respectively.
%
Specifically, given the point cloud of a 3D scene that is associated with class-agnostic 3D instance masks, the 3DSSG prediction task would like to construct a directed graph whose nodes are semantic labels of the 3D instances and the edges recognize the directional semantic or geometrical relations between connected 3D instances.

% Thus, each relation in the scene graph can be represented as a triplet $\langle \emph{subject}, \emph{predicate}, \emph{object} \rangle$.
%
% The core problem of this task is to explore how to associate the subject and object pairs and understand their relations, in the form of relation triplets $\langle \emph{subject}, \emph{predicate}, \emph{object} \rangle$.

% Unlike 2D semantic scene graph (2DSSG) prediction studies the semantics can be captured in images 

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{picture/teaser.pdf}
\caption{
\textbf{Comparison between previous method and our VL-SAT.}
%
(a) SGPN~\cite{wald2020learning}, as the 3D model, fails to find capture predicates such as \emph{build in}.
%
(b) VL-SAT creates an oracle model by heterogeneously fusing 2D semantics, and language knowledge along with the geometrical features, and the 3D model receives benefits from the oracle model during training.
%
During inference, the enhanced 3D model can correctly detect the tail predicates.}
\label{fig:teaser}
\end{figure}


However, in addition to common difficulties faced by scene graph prediction, there are several challenges specified to the 3DSSG prediction task. (1) 3D data such as point clouds only capture the geometric structures of each instance and may superficially define the relations by relative orientations or distances.
%
(2) Recent 3DSSG predication datasets~\cite{wald2020learning,zhang2021exploiting} are quite small and suffer from long-tailed predicate distributions, where semantic predicates are often rarer than geometrical predicates.
%
For example, as shown in \cref{fig:teaser}, the pioneering work SGPN~\cite{wald2020learning} usually prefers a simple and common geometric predicate \emph{standing on} between \emph{sink} and \emph{bath cabinet}, while the ground-truth relation triplet $\langle \emph{sink}, \emph{build in}, \emph{bath cabinet}\rangle$ cares more about the semantics, and the frequency of \emph{build in} in the training dataset is quite low, as shown in \cref{fig:predicate_dis}(a).
%
Even though some attempts~\cite{zhang2021knowledge,wu2021scenegraphfusion,zhang2021exploiting} have been proposed thereafter, the inherent limitations of the point cloud data to some extent hinder the effectiveness of these methods.

%
Since 2D images provide rich and meaningful semantics, and the scene graph prediction task is in nature aligned with natural languages, we explore using visual-linguistic semantics to assist the training, as another pathway to essentially enhance the capability of common 3DSSG prediction models with the aforementioned challenges.

How to assist 3D structural understanding with visual-linguistic semantics remains an open problem.
%
Previous studies mainly focus on employing 2D semantics to enhance instance-level tasks, such as object detection~\cite{qi2018frustum,qi2020imvotenet,bai2022transfusion,sindagi2019mvx}, visual grounding and dense captioning~\cite{zhao20213dvg,cai20223djcg,chen2020scanrefer,yuan2022x}.
%
Most of them require visual data both in training and inference, but a few of them, such as SAT~\cite{yang2021sat} and $\cX$-Trans2Cap~\cite{yuan2022x} treat 2D semantics as auxiliary training signals and thus offer more practical inference only with 3D data.
%
But these methods are specified to instance-level tasks and require delicately designed networks for effective assistance, thus they are less desirable to our structural prediction problem. 
%
Thanks to the recent success of large-scale cross-modal pretraining like CLIP~\cite{radford2021learning}, 2D semantics in images can be well aligned with linguistic semantics in natural languages, and the visual-linguistic semantics have been applied for alleviating long-tailed issue in tasks related to 2D scene graphs~\cite{zareian2020bridging, abdelkarim2021exploring, tang2019learning, tang2020unbiased} and human-object interaction~\cite{liao2022gen}.
%
But how to adapt similar assistance of visual-linguistic semantics to the 3D scenario remains unclear.


In this study, we propose the Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme to empower the point cloud-based 3DSSG prediction model (termed as the 3D model) with sufficient discrimination about long-tailed and ambiguous semantic relation triplets.
%
In this scheme, we simultaneously train a powerful multi-modal prediction model as the oracle (termed as oracle model) that is heterogeneously aligned with the 3D model, which captures reliable structural semantics by extra data from vision, extra training signals from language, as well as the geometrical features from the 3D model. These introduced visual-linguistic semantics have been aligned by CLIP.
%
Consequently, the benefits of the oracle model, especially the multi-modal structural semantics, can be efficiently embedded into the 3D model through the back-propagated gradient flows.
%
In the inference stage, the 3D model can perform superior 3DSSG prediction performance with only 3D inputs. For example, in \cref{fig:teaser}(b), the predicate \emph{build in} can be reliably detected.
%
To our best knowledge, VL-SAT is the first visual-linguistic knowledge transfer work that is applied to 3DSSG prediction in the point cloud.
%
Moreover, VL-SAT can successfully enhance  SGFN~\cite{wu2021scenegraphfusion} and $\text{SGG}_\text{point}$~\cite{zhang2021exploiting}, validating that this scheme is generalizable to common 3DSSG prediction models.


We benchmark VL-SAT on the 3DSSG dataset~\cite{wald2020learning}. Quantitative and qualitative evaluations, as well as comprehensive ablation studies, validate that the proposed training scheme leads to significant performance gains, especially for tail relations, as discussed in \cref{sec:experiments_and_discussions}.

% As one of the fundamental tasks that aim at understanding the 3D visual world, 3D Scene Graph Prediction ...
% % insert figure
% The contribution of our paper can be summarized as: (1) to our best knowledge, it is the first time ... (2) (3)
% %
% You can refer it in \href{https://docs.qq.com/doc/DTEZxSU95cGdVWmVi}{introduction}

% As one of the fundamental tasks that aim at understanding the visual world better, scene graph prediction[2d sgg refs] aims to build a common structural representation that encodes the semantic elements and the complex relationships between them.
% % 
% Recently, some works~\cite{wald2020learning} have tried to generate scene graphs directly from 3D point clouds to restore the spatial information lost in 2D images, which can benefit the VR/AR and robot navigation tasks more.
% % 
% Specifically, given point clouds of a scene along with the object segmentation mask, 3D scene graph prediction aims to predict the object semantic class along with the relationships between different objects.
%

%
% However, there are still some problems in 3D scene graph prediction tasks, which hinder the performance of existing methods.
% %
% 1) The lack of texture information in 3D point clouds restrains a better understanding of the scene, which leads to a spatial-only relation prediction.
%
% As shown in Figure~\ref{fig:teaser}, the model prefers to predict a spatial-related relationship \textit{standing on} between sink and cabinet, while the ground-truth relationship is \textit{build in}.
%
% Utilizing the 2D semantic information by projecting 3D objects to 2D images to enrich the point clouds is a common practice to alleviate the problem in other 3D tasks, like 3D detection~\cite{qi2018frustum,qi2020imvotenet,bai2022transfusion,sindagi2019mvx} and 3D Visual Grounding~\cite{zhao20213dvg,cai20223djcg,chen2020scanrefer}.
% %
% However, these methods require extra 2D inputs in both train and inference stages, which brings a computation burden.
% %
% SAT~\cite{yang2021sat} proposes a method with an attention mask to prevent the network from utilizing 2D knowledge directly during training, which alleviates performance dropping only using 3d during inference.
% %
% But it fuses the multi-modal tokens with a union transformer encoder with the object-level only knowledge enhancement, which has a great limitation on the relation inference with a larger computation burden.
% %
% Similarly, X-Trans2Cap~\cite{yuan2022x} utilizes a similar idea to assist 3D dense caption tasks only during training with two modality transformer encoders.
% %
% But it only focuses on the same instance between two modalities, which ignores the hard negative instances in the scene.
% %
% 2) 3D scene graph prediction task suffers from a relatively small dataset size and long-tail distribution of predicates, which makes it difficult to do well on some tail categories. 
% %
% As shown in Figure xxx, the data distribution is very uneven, and in some categories, the previous method cannot distinguish them at all.
% %
% Zhang2021~\cite{zhang2021knowledge} generates a meta embedding from the training dataset to ease the long-tail problem on 3DSSG, which can be seen as a re-balance method.
% %
% But it can not bring new extra knowledge essentially.
% %
% Recently, thanks to a large amount of 2D data, CLIP connects the two modalities, image, and text, to achieve a better representation extraction effect and solve the long-tail distribution problem to some extent. 
% %
% So we hope that we can assist 3D learning with this generic multi-modal representation information during training and expect to achieve better results.
%

% To address the above problems, we propose a multi-modal knowledge-assisted model while ensuring that the original 3D scene graph structure remains largely unchanged with only training assistance. 
% %
% We first use the 2D sequence of reconstructed point clouds to generate 2D multi-view features corresponding to all objects to supplement the texture attribute features of objects in 3D point clouds.
% %
% Then we encode the 2D images with CLIP Vision Encoder to obtain the 2D vision tokens corresponding to the 3D objects.
% %
% To transfer the knowledge effectively, we propose a new training schema Visual-Linguistic Semantics Assisted Training (VL-SAT) in 3D Scene Graph Prediction.
% %
% We transfer the knowledge ...

%
% The contribution of our paper can be summarized as (need to be modified):
% %
% 1) We propose a Visual-Linguistic Semantic Assisted Training framework with node and edge-level collaboration and triplet-level regularization to collaboratively boost both oracle model and 3D model.
%  to predict reliable 3D scene graphs
% 1). We use 2D CLIP to fuse with 3D data by mapping method, which complements the color, texture attributes, and semantic features of objects that are difficult to obtain from 3D data through multi-modal fusion, and better extracts the relationship information between 3D objects, which is the first attempt of the pre-trained semantic big model in 3D vision+language task.
%
% 2) We further migrate 2D and language semantics to generate a strong oracle model. To our best knowledge, it is the first time to study the contribution of 2D pretrained model on 3D-VL tasks.
% 2) We propose multiple knowledge migration schemes between different modalities, which can achieve the effect of comprehensive model performance improvement without adding extra computation at the time of inference.
% 3) Experiments show that our method effectively solves the problems of single object relationships and poor tail performance brought by insufficient data volume of 3D Scene Graph.