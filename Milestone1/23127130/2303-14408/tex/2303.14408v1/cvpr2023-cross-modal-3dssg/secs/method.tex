% !TEX root = ../PaperForReview.tex

\section{Method}
\label{sec:method}

%-------------------------------------------------------------------------
\begin{figure*}[t]
\centering
\includegraphics[width=1.0\linewidth]{picture/arch.pdf}
\caption{
%
\textbf{The proposed Visual-Linguistic Semantics Assisted Training (VL-SAT) for 3D scene graph prediction.}
%
In training, VL-SAT takes 2D and language semantics as extra inputs and helps 3D scene graph prediction with node and edge-level collaboration and triplet-level regularization.
%
In inference, VL-SAT only takes the 3D point cloud to predict reliable 3D scene graphs.
% The network takes in both 3D and 2D inputs: 3D inputs are point clouds of segmented objects and 2D inputs are image patches projected from corresponding 3D objects.
%
% The framework adheres to a teacher-student architecture, in which both teacher and student networks consist of several GNN-based blocks.
%
% The teacher network is further enhanced in three levels: object-level, relation-level and triplet-level.
%
% The student and teacher networks are jointly trained, but only the student network and 3D inputs are used during inference (see the orange arrow).
}
\label{fig:network}
\end{figure*}

We first overview the formulation of 3D semantic scene graph (3DSSG) prediction in point cloud (\cref{sub:problem_formulation}) and then elaborate on a GNN-based network that we experiment on as our 3D prediction model (\cref{sub:baseline}).
%
Since then we highlight how our Visual-Linguistic Semantics Assisted Training (VL-SAT) scheme comprehensively transfers the benefits of an oracle multi-modal prediction model to the 3D prediction model in discriminating challenging relations (\cref{sub:heterogeneous_collaborative_learning}).
%
Finally, we depict the training objective in \cref{sub:training_and_inference}.

\subsection{Problem Formulation}
\label{sub:problem_formulation}

Suppose we have a point cloud $\mP \in \Rbb^{N\times3}$ with $N$ 3D points, and a set of \emph{class-agnostic} instance masks $\cM = \{\mM_1,...,\mM_K\}$ that associate the point cloud $\mP$ with $K$ semantic instances, as indicated by SGPN~\cite{wald2020learning}, we aim at predicting a 3D semantic scene graph as a directed graph $\cG = \{ \cO, \cR \}$.
%
The set of objects $\cO = \{ o_i \}_{i=1}^K$ are all \emph{named} object instances that are specified by instance masks $\cM$. Each edge $r_{ij}$ in $\cR$ depicts the \emph{predicate} in a relation triplet $\langle \emph{subject}, \emph{predicate}, \emph{object} \rangle$, where the head node $o_i$ of this edge is the \emph{subject} and the tail node $o_j$ is the \emph{object}.
%
To be specific, $o_i$ indicates an object label from $N_\text{obj}$ semantic classes.
%
$r_{ij}$ is a predicate label from $N_\text{rel}$ predicate classes.
%
% Note that the number of edges in $\cR$ is $K(K-1)$ since the graph is assumed fully connected.
%


\subsection{3D Prediction Model}
\label{sub:baseline}

As depicted in \cref{fig:network}, our employed 3D prediction model shares a similar network structure as those GNN-based scene graph prediction methods, such as SGFN~\cite{wu2021scenegraphfusion} and $\text{SGG}_\text{point}$~\cite{zhang2021exploiting}, which mainly consists of node encoder, edge encoder, and scene graph reasoning modules.

\vspace{+1mm}
\noindent\textbf{Node Encoder.}
%
Based on one class-agnostic instance mask $\mM_i$ along with the input point cloud $\mP$, we can extract the set of points $\mP_i$ that correspond to one semantic instance. 
%
% Note that the number of points in each instance is different from each other.
%
We employ a simple PointNet~\cite{qi2017pointnet} to extract instance-level features.
%
The node features $\vo_i^\text{3d} \in \Rbb^{D}$ before the GNN-based scene graph reasoning are thus given by these instance-level features.

\vspace{+1mm}
\noindent\textbf{Edge Encoder.}
%
We follow the same practice as in SGFN~\cite{wu2021scenegraphfusion} to encode the edge features for the GNN-based scene graph reasoning.
%
It requires calculating the differences between several attributes between the linked instances. 
%
For each instance, these attributes include the mean $\vmu$ and standard deviation $\vsigma$ of the 3D points, the size $\vb = (b_x, b_y, b_z)$, the volumn $v = b_xb_yb_z$, and the maximum side length $l=\max(b_x, b_y, b_z)$ of the bounding box.
%
Thus the edge features $\vr_{ij}^\text{3d}\in\Rbb^D$ are encoded by projecting the concatenated differences of these attributes between two instances, via multi-layer perceptron (MLP) layers, \ie,
%
\begin{equation}
    \vr_{ij}^\text{3d} = \mathtt{MLP}(\mathtt{cat}(\vmu_i-\vmu_j, \vsigma_i-\vsigma_j,\vb_i-\vb_j,\ln\frac{l_i}{l_j},\ln \frac{v_i}{v_j})),
\end{equation}
where the subscript $i$ indicates the instance $\mP_i$ in the head node, and the $j$ means the instance $\mP_j$ in the tail node.

\vspace{+1mm}
\noindent\textbf{Scene Graph Reasoning.}
%
In our experiment, we apply a similar GNN structure as in SGFN~\cite{wu2021scenegraphfusion}, which utilizes a Feature-wise Attention (FAT) module~\cite{wu2021scenegraphfusion} to pass messages between nodes and edges, and then gets the updated node and edge features.
%
Each GNN module is paired with a multi-head self-attention (MHSA) module, and they are repeated for $T$ times to extract the final node and edge features $\{\tilde{\vo}_i^\text{3d}\}_{i=1,\ldots,K}$ and $\{\tilde{\vr}_{ij}^\text{3d}\}_{i\neq j, i,j=1,\ldots,K}$.
%
% The GNN modules are repeated for $T$ times to extract the final node and edge features $\{\tilde{\vo}_i^\text{3d}\}_{i=1,\ldots,K}$ and $\{\tilde{\vr}_{ij}^\text{3d}\}_{i\neq j, i,j=1,\ldots,K}$.
%
Since then, an object classifier and a predicate classifier are to predict the elements $\{ o_i, r_{ij}, o_j \}$ of each possible relation triplet from the triplet features $\{ \tilde{\vo}_i^\text{3d}, \tilde{\vr}_{ij}^\text{3d}, \tilde{\vo}_j^\text{3d} \}$.
%
These relation triplets finally construct the semantic scene graph $\cG = \{ \cQ, \cR \}$.
%

\vspace{+1mm}
Note that the 3D prediction model does not have to strictly follow the network of SGFN~\cite{wu2021scenegraphfusion}. More recent GNN-based models, such as~$\text{SGG}_\text{point}$~\cite{zhang2021exploiting} can also be applied. In \cref{subsec:ablation}, we show that the proposed VL-SAT scheme can also enhance both baselines with significant gains, validating that our method is generalizable to common 3DSSG prediction models.

\subsection{Visual-Linguistic Semantics Assisted Training}
\label{sub:heterogeneous_collaborative_learning}

In this subsection, we elaborate on how the visual-linguistic semantics assisted training (VL-SAT) scheme can empower the 3D prediction model with sufficient discrimination about long-tailed and ambiguous semantic relation triplets.
%
The key idea is that this discriminative power comes from auxiliarily learning a powerful multi-modal prediction model that receives structural semantics from vision, and language, as well as the 3D geometry from the 3D prediction model.
%
The multi-modal semantics are expected to be heterogeneously aligned with the 3D semantics at the node and edge levels, and the benefits from the oracle model can be efficiently absorbed by the 3D prediction model during the training process.
%
To be specific, we first introduce the applied multi-modal prediction model that has heterogeneous collaboration with the 3D prediction model, and then the auxiliary training strategies that boost the performance of the oracle model and eventually enhance the 3D prediction model.
%
We present the pipeline of VL-SAT in \cref{fig:network}.

\vspace{+1mm}
\noindent\textbf{Multi-modal Prediction Model as the Oracle.}
%
This multi-modal prediction model acts as the oracle to our 3D prediction model. It copies the 3D prediction network in~\cref{sub:baseline} and also learns to predict 3D semantic scene graphs, but its node features are represented by visual features. 
%
These visual features are extracted by a fixed 2D instance encoder, describing RGB image patches that are associated with each point cloud instance $\mP_i$\footnote{Please refer to the supplementary for the details about how to gather associated image patches with each point cloud instance.}. 
%
The edge features of the multi-modal prediction model are encoded in the same way in \cref{sub:baseline}, thus still capturing 3D spatial structures in understanding relations.
%
 
The features of this oracle model heterogeneously collaborate with those in the 3D prediction model at the node and edge levels, which are conducted before and after each GNN layer in the scene graph reasoning module.
%
The former is a node-level collaboration, and the latter is an edge-level collaboration.
%
To be specific, these collaboration operations are implemented by multi-head cross-attention (MHCA) modules~\cite{vaswani2017attention}, where the keys and values are node/edge features from the 3D model, and the queries are their counterparts from the multi-modal model.
%
The node-level collaboration has a distance-aware masking strategy to remove unnecessary attention between instances that are far apart without valid relations.
%
The mask value between two instances $\mP_i$ and $\mP_j$ are learned by
%
\begin{equation}
    D_{ij}^\text{node} = \mathtt{MLP}(\mathtt{cat}(\vmu_i-\vmu_j, \| \vmu_i-\vmu_j\|_2)),
\end{equation}
with respect to the mean coordinates $\vmu_i$ and $\vmu_j$ of the point cloud instances $\mP_i$ and $\mP_j$.
%
The edge-level collaboration does not use a distance-aware masking strategy since the distance between edges is hard to define, thus it is safer to incorporate all edges into attention calculation. 

%
Note that the heterogeneous collaboration is \emph{unidirectional} from the 3D model to the oracle model, while the benefits of the oracle model are passed to the 3D model through the back-propagated gradient flows.
%
It favors that in the inference stage, predicting 3D semantic scene graphs will not need extra data from other modalities.
%


\vspace{+1mm}
\noindent\textbf{Auxiliary Training Strategies.}
%
% Apart from losses for predicting scene graphs, since the oracle multi-modal model would like to perceive scene graphs also from the visual perspective, it is in nature able to enhanced by visual-linguistic knowledge captured by CLIP~\cite{radford2021learning}.
%
Since the oracle multi-modal model would like to perceive scene graphs from both the visual and linguistic perspective, it is natural to enhance the oracle model using visual-linguistic knowledge captured by CLIP~\cite{gao2021clip}.
%
Specifically, we can generate CLIP text embedding $\ve^{\text{text}}_{ij}$ for each groundtruth relation triplet, and regularize the corresponding triplet features $\{\tilde{\vo}^{\text{oracle}}_i, \tilde{\vr}^{\text{oracle}}_{ij}, \tilde{\vo}^{\text{oracle}}_j\}$ at the end of each GNN layer of the scene graph reasoning module.
%
The CLIP text embeddings are offline extracted by the template ``A scene of a/an [subject][predicate] a/an [object]'' for each GT relation 
%
Thus, the regularization becomes to minimize the embedding distances between the text embeddings $\ve_{ij}^\text{text}$ and the fused triplet features $\vt_{ij}^\text{oracle}$, \ie,
%
\begin{equation}
    L_\text{tri-emb} = \sum_{i=1}^K \sum_{j=1,j\neq i}^K \rho(\vt_{ij}^\text{oracle}, \ve_{ij}^\text{text}) \cdot \Ibb_{[\ve_{ij}^\text{text}~\text{is from GT triplet}]} \label{eq:triplet_embedding_loss}
\end{equation}
%
where $\vt_{ij}^\text{oracle} = \mathtt{MLP}(\mathtt{cat}(\tilde{\vo}_i^\text{oracle}, \tilde{\vr}_{ij}^\text{oracle}  ,\tilde{\vo}_j^\text{oracle}))$ is the fused embedding of the concatenated features $\tilde{\vo}_i^\text{oracle}, \tilde{\vr}_{ij}^\text{oracle}$, and  $\tilde{\vo}_j^\text{oracle}$.
%
$\rho(\cdot, \cdot)$ is a distance metric, we can apply $\ell_1$ norm or negative cosine distance.
%
$\Ibb_{[\cdot]}$ is an indicator function that equals to $1$ when the argument is true, and $0$ otherwise.
%
Thus \cref{eq:triplet_embedding_loss} only regularizes the node and edge features whose triplets have ground-truth relations. 

In addition, before being put into the scene graph reasoning modules, the 3D node features $\vo_i^\text{3d}$ from the 3D model and the 2D node features $\vo_i^\text{2d}$ from the oracle model can be aligned. 
%
We apply a same distance measurement as \cref{eq:triplet_embedding_loss},
%
\begin{equation}
    L_\text{node-init} = \sum_{i=1}^K \rho(\vo_i^\text{3d}, \vo_i^\text{2d}).
\end{equation}
%
To enhance the representation ability of the initialized 2D node features, the 2D instance encoder is a fixed CLIP-pretrained vision encoder.
%
Moreover, to enhance the object classifiers of both models, we use the CLIP object embeddings to initialize the weights of the object classifiers both at the 3D prediction model and the oracle multi-modal prediction model, as in~\cite{radford2021learning,liao2022gen}.

\subsection{The Training Objective}
\label{sub:training_and_inference}
% In this subsection, we discuss the processes of network training and inference.
%
% \noindent\textbf{Training Objective.}
%
% \textcolor{blue}{To be revised.}
The training objective of the entire network is defined as:
\begin{multline}
L = \lambda_{\text{obj}}(L_{\text{obj}}^{\text{3d}}+L_{\text{obj}}^{\text{oracle}}) + \lambda_{\text{pred}}(L_{\text{pred}}^{\text{3d}}+L_{\text{pred}}^{\text{oracle}}) + \\ \lambda_{\text{aux}}(L_{\text{tri-emb}}+L_{\text{node-init}})
\label{eq:loss_total}
\end{multline}
%
$L_{\text{obj}}$ indicates object classification loss and is implemented with cross-entropy loss.
%
$L_{\text{obj}}^{\text{3d/oracle}}$ is applied on 3D/oracle object classifier.
%
$L_{\text{pred}}$ indicates predicate classification loss and is formulated as per-class binary cross-entropy loss as in ~\cite{wald2020learning}.
%
$L_{\text{pred}}^{\text{3d/oracle}}$ is applied on 3D/oracle predicate classifier.
%
$\lambda_{\text{node}}$, $\lambda_{\text{edge}}$, $\lambda_{\text{aux}}$ are hyper-parameters to balance each loss in the same scale.

%
% In train stage, the 3D prediction model and oracle multi-modal model are jointly trained.
% %
% During inference time, only the 3D prediction model is utilized with point cloud-only input.