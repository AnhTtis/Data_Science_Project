% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}            % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{mmstyles}

\newcommand{\method}{\textcolor{red}{VL-SAT}}
\newcommand{\module}{\textcolor{red}{Cross-modal Heterogeneous Collaboration}~}

% \newcommand{\basetwo}{\textcolor{black}{$\text{}_{\text{2d}}$ }}
\newcommand{\basetwo}{Oracle}
% \newcommand{\basethree}{\textcolor{black}{$\mathrm{Base_{3d}}$ }}
\newcommand{\sggpoint}{$\text{SGG}_{\text{point}}$}
\newcommand{\basethree}{\textcolor{black}{$\text{Base}_{\text{3d}}$ }}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{1977} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud}

\author{
    Ziqin Wang$^1$,~~
    Bowen Cheng$^1$,~~
    Lichen Zhao$^1$,~~
    Dong Xu$^2$,~~
    Yang Tang$^{3*}$,~~
    Lu Sheng$^1$\thanks{Lu Sheng and Yang Tang are the corresponding authors.}~~ \\
    $^1$School of Software, Beihang University\\
    $^2$The University of Hong Kong, ~~ $^3$East China University of Science and Technology \\
    \small
    \texttt{\{wzqin,chengbowen052,zlc1114,lsheng\}@buaa.edu.cn} \hspace{10pt}
    \texttt{dongxu@cs.hku.hk} \hspace{10pt}
    \texttt{yangtang@ecust.edu.cn}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}

The task of 3D semantic scene graph (3DSSG) prediction in the point cloud is challenging since (1) the 3D point cloud only captures geometric structures with limited semantics compared to 2D images, and (2) long-tailed relation distribution inherently hinders the learning of unbiased prediction.
%
Since 2D images provide rich semantics and scene graphs are in nature coped with languages, in this study, we propose \textbf{V}isual-\textbf{L}inguistic \textbf{S}emantics \textbf{A}ssisted \textbf{T}raining (\textbf{VL-SAT}) scheme that can significantly empower 3DSSG prediction models with discrimination about long-tailed and ambiguous semantic relations.
%
The key idea is to train a powerful multi-modal oracle model to assist the 3D model. This oracle learns reliable structural representations based on semantics from vision, language, and 3D geometry, and its benefits can be heterogeneously passed to the 3D model during the training stage.
%
By effectively utilizing visual-linguistic semantics in training, our VL-SAT can significantly boost common 3DSSG prediction models, such as SGFN and $\text{SGG}_\text{point}$, only with 3D inputs in the inference stage, especially when dealing with tail relation triplets.
%
Comprehensive evaluations and ablation studies on the 3DSSG dataset have validated the effectiveness of the proposed scheme.
%
% Code will be made available \href{https://github.com/wz7in/CVPR2023-VLSAT}{here}. 
%
Code is available at \href{https://github.com/wz7in/CVPR2023-VLSAT}{https://github.com/wz7in/CVPR2023-VLSAT}.
%
\end{abstract}

%%%%%%%%% BODY TEXT
\input{secs/intro}
\input{secs/rel}
\input{secs/method}
\input{secs/exp}

\section{Conclusions}
%
We have introduced a visual-linguistic semantics assisted training (VL-SAT) scheme to boost 3D semantic scene graph prediction in the point cloud.
%
We build a strong oracle multi-modal model, which captures structural semantics using extra input data from vision, auxiliary training signals from language, and geometric features from the 3D model.
%
The oracle multi-modal model enhances the 3D prediction model via back-propagated gradient flows.
%
Consequently, the 3D prediction model can predict reliable scene graphs with only a 3D point cloud as input.
%
% Comprehensive ablation studies show the importance and effectiveness of the proposed VL-SAT scheme.
%
Qualitative and quantitative results demonstrate that our method remarkably outperforms the existing methods.
\label{sec:conclusion}

\vspace{+1mm}
\noindent\textbf{Acknowledgements.} This work was supported by National Key Research and Development Program of China (2021YFB1714300), and National Natural Science Foundation of China (62132001, 62233005).

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
