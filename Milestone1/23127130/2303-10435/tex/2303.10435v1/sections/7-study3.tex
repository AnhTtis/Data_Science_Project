\section{Effect of Resolution on Machine's Recognition Performance}
\label{sec:study3}

In this section, we explore the effect of image resolution on machine's recognition performance of ADLs and visual privacy features. We adopted the open-access cutting-edge deep learning methods as the machine recognizer. 
% We describe the machine learning approaches and the major results. 

\subsection{ADLs Recognition}
\label{sub:adl_machine}

\subsubsection{Training and Evaluation Dataset}
We applied data augmentation approaches to the training dataset in section~\ref{sec:dataset}, including horizontal flip, and Gaussian Noise, enlarging the dataset by four times. To fairly compare the recognition performance of the machine and the human, we utilized the same evaluation dataset in section~\ref{sec:study2}. 

\subsubsection{Training and Evaluation Procedure}
We utilized both convolutional neural networks and transformer-based models as our ADLs classifiers, including \textbf{ResNet50}~\cite{he2015deep}, \textbf{Efficient Net}~\cite{Mingxing2019EfficientNet} and \textbf{Vision Transformer (ViT)}~\cite{dosovitskiy2020vit}. All the models used here were pretrained with ImageNet dataset~\cite{imagenet} that output 1000 probabilistic values. In this experiment, we took every fame of the video clips in our dataset as the model input during our training, validating, and testing procedure. We first scaled the image of low resolution to $512\times 512$ pixels to standardize the input of the model. Then, we fine-tuned the pretrained network using the training dataset with a certain resolution ($r$) in which the images were all at the resolution of $r\times r$. To transfer the pretrained network model to our application, we added an additional five-node fully connected layer at the end of the network. We used sigmoid as the activation function. Once we finished the training procedure, we evaluated the fine-tuned model using the evaluation dataset under the same image resolution ($r$).
As we have described in section~\ref{sec:dataset}, we use the randomly chosen 5\% of the total dataset as the validation dataset in our implementation. In order to avoid the over-fitting problem, we used the early stopping method. In other words, we will stop our training procedure when the accuracy on the validation dataset does not rise anymore for 5 successive epochs. 

\subsubsection{Result}
Table~\ref{tab:us3_result} shows the effect of image resolution on machines' performance of the ADLs recognition task. Results indicate that \textbf{the machine outperforms the human regarding the ADLs recognition task on low-resolution images}. Vision Transformer can maintain an accuracy of 84.4\% even when the image resolution is as low as $20\times 20$. However, such a resolution is far from enough for humans to recognize ADLs at an ideal accuracy level. For resolutions above $100 \times 100$, both humans and machines can achieve a high accuracy above 90\%. Such results show the possibility of constructing a range of image resolutions to preserve visual privacy without bearing great loss in ADLs recognition simultaneously.

\input{tables/us3_result}
\input{figures/machine_performance}

\subsection{Privacy Features Recognition}

\subsubsection{Facial Identification}
\label{subsec:machine_face}
We adopted \textbf{InsightFace}~\cite{deng2018arcface} for facial identification by testing whether the model can recognize human faces in certain areas of the frames. We used the pretrained \textbf{ArcFace} model for facial identification provided by InsightFace. Also, we checked every frame of the video clips in this experiment. The result is shown in Figure~\ref{fig:machine_result} as the teal line. Results from the ArcFace model indicate that even the state-of-art models cannot detect any human faces below $50\times 50$ pixels. However, as the resolution increases from $100 \times 100$ to $240\times 240$ pixels, machine's facial identification performance significantly increases from 71.0\% to 100.0\%. Such results imply that identifiable faces can be preserved well against the machine attacker when the image resolution is below $50 \times 50$ pixels.

\subsubsection{Nudity Recognition}
\label{subsec:machine_nudity}
We adopted the pretrained \textbf{NudeNet}~\footnote{Software DOI: 10.5281/zenodo.3584720} for binary nudity recognition. This model was trained to detect nude parts of the human body in images. Here we utilized the classifier model to help us make a distinction between safe and unsafe images. We report the result of NudeNet as the orange line in Figure~\ref{fig:machine_result}. The precision and recall of NudeNet also reveal that it cannot identify any nude parts below the resolution of $30 \times 30$ pixels. Under the resolution of $100 \times 100$ pixels, NudeNet can recognize frames containing nude parts with an accuracy of 88.0\%. Therefore, we conclude that resolutions below $30 \times 30$ pixels can effectively preserve the nudity privacy feature.

\subsubsection{Property and Object Detection}
\label{subsec:machine_value}
We adopted \textbf{DETR}~\cite{Nicolas2020DETR} pretrained on the COCO dataset~\footnote{https://cocodataset.org/} for property and object detection. 
Considering the availability of pretrained object detection models, we used the detection performance of DETR on COCO objects as an estimation of machine's recognition performance on valuable properties.
We manually annotated the objects which belong to the COCO classes in each frame as ground truth. 
In our implementation, we first resized videos of different resolutions up to $240\times 240$ pixels. Then we kept bounding boxes with a confidence level above a pre-set threshold (e.g., 0.75) as a result of the model. 
To evaluate the model performance under different resolutions, we compared the objects detected by the model and the ground truth of each frame one by one to calculate the recognition accuracy. 
The purple line in Figure~\ref{fig:machine_result} shows the recognition accuracy of DETR. Results show that below the resolution of $50 \times 50$, DETR fails to detect any object. On images with a resolution of $100 \times 100$, DETR can achieve an accuracy of 72.0\%. Under the resolution of $160 \times 160$, large objects such as the main character can be detected precisely with an overall accuracy of 77.0\%. Under the resolution of $240\times 240$, the object detection results are more accurate, and small targets such as bottles and cups can be detected, too. Therefore, DETR can finally achieve an accuracy of 82.0\%.

\subsubsection{Relationship Classification}
\label{subsec:machine_relationship}

We adopted a pretrained cutting-edge social relationship classification model \textbf{GRM}~\cite{Wang2018Deep} on the evaluation dataset with different image resolutions. This model utilized a node message propagation mechanism and a graph attention mechanism to explore the interaction between the person pair of interest and contextual objects. The prerequisite to inferring the relationship between people is to obtain the context information using the object detection model. In our implementation, we resized the raw video of different resolutions to $240 \times 240$ pixels. Then, we annotated the bounding boxes and classes of different objects using DETR and labeled the bounding boxes of the person pair whose social relationship we wanted to examine. The model took every frame of the raw videos and objects list as input and generated the classification result as output.

The accuracy result we reported as the magenta line in Figure~\ref{fig:machine_result} describes the performance of the GRM model on the four-classes social relationship recognition task including \textit{intimate relationship}, \textit{non-intimate relationship}, \textit{no relationship}, and \textit{no person}. As is shown, the GRM model can detect nothing and will classify any input image as the \textit{no person} type under resolutions below $30 \times 30$ pixels. Our results here also proved that a low resolution below $30 \times 30$ is sufficient to preserve the privacy of social relationships against the cutting-edge machine recognition method. When the resolution is $100\times 100$ pixels, GRM can recognize social relationships in the video with an accuracy of 34.1\%. For resolutions of $160\times 160$ pixels and $240 \times 240$ pixels, GRM can achieve an accuracy of 60.9\% and 80.5\%, respectively.