\section{ADLs Dataset with Visual Privacy Features}
\label{sec:dataset}

This section describes the dataset we used to explore the effect of image resolution on humans' and machines' performance on activity recognition and visual privacy awareness tasks.

\subsection{Constructing the ADLs Dataset}

In order to evaluate the model in realistic environments, we used the publicly-available PA-HMDB51 dataset for privacy-preserving activity recognition~\cite{wu2019framework}. This dataset consists of about 355 minutes and 51 types of human activity videos collected from realistic environments with various visual privacy features annotated. 

In this paper, we mainly focus on activities of daily living (ADLs) in a smart home scenario. Therefore, three of our authors selected the qualified videos from the PA-HMDB51 dataset together with the following requirements. 1) The video represents a home environment. 2) All authors agreed that the main character conducted the same kind of activities. 3) All authors felt comfortable to publish the video online. For instance, due to the internet policy, we only chose men's or kids' topless videos in this study. Then, we divided the human activities in the PA-HMDB51 dataset into five basic kinds of activities of daily living (ADLs) including \textit{functional mobility}, \textit{feeding}, \textit{intimacy}, \textit{entertainment}, and \textit{personal hygiene}. Finally, we obtained 46, 30, 22, 37, and 16 minutes of videos for functional mobility, feeding, intimacy, entertainment, and personal hygiene, respectively.

We randomly split the PA-HMDB51 dataset into a training dataset, a validation dataset, and an evaluation dataset, which accounts for 90\%, 5\%, and 5\%, respectively. Considering the difference of the video duration in the PA-HMDB51 dataset, we divided all the videos into 2-second clips for later training and evaluation without affecting the judgment of the video content. Therefore, there are 226 clips of the videos in the evaluation dataset, with 69, 45, 33, 55, and 24 clips for functional mobility, feeding, intimacy, entertainment, and personal hygiene, respectively.

\subsection{Labeling the Privacy Features}

Based on the user study results presented in section~\ref{sec:study1}, we annotated each frame and each clip in our dataset with privacy features including \textit{nudity}, \textit{identifiable face}, \textit{valuable property}, and \textit{relationship}. 
Since privacy features may vary during the video clip, for example, even in the same video clip, the visibility of a person's face may be different, we provided both \textit{frame-level} and \textit{clip-level} labels of for each video in our dataset. First of all, we annotated all of the privacy attributes on each frame of different clips. Then, we annotated each clip according to the frames in the clip for later user studies and machine experiments. The detailed description of both frame-level and clip-level labels are listed below.
\input{figures/annotation_example.tex}
\begin{itemize}
    \item \textbf{Nudity}. The nudity label of each frame included three types that are \textit{naked or semi-naked (topless or bottomless)}, \textit{fully clothed}, and \textit{no person}. A clip is labeled as \textit{naked or semi-naked (topless or bottomless)} if at least one frame of the clip is labeled as \textit{naked or semi-naked (topless or bottomless)}. Otherwise, the clip is labeled as \textit{fully clothed} in a similar way. If every frame is labeled as \textit{no person}, we will finally label the clip as \textit{no person}.
    \item \textbf{Identifiable face}. If more than 70\% of a human face is visible, we consider the frame to contain an identifiable face. Therefore, each frame is labeled as \textit{yes}, \textit{no}, and \textit{no person}. A clip with more than one frame labeled as \textit{yes} is labeled as \textit{yes}, otherwise \textit{no}. A clip with every frame labeled as \textit{no person} is then labeled as \textit{no person}. 
    \item \textbf{Valuable property}. We only consider safe box, jewelry, watch, ring, and cash as valuable properties. Each frame is labeled as \textit{yes}, \textit{no}, and \textit{no person}. We label clips with at least one frame labeled \textit{yes} as \textit{yes}, otherwise \textit{no}. Clips with no person on any frame are labeled as \textit{no person}.
    \item \textbf{Relationship}. We consider the relationship of all the people presented in the video. There are four types of labels for each frame: \textit{intimate relationship}, \textit{non-intimate relationship}, \textit{only one person}, and \textit{no person}. A video clip is labeled as \textit{intimate relationship} if at least one frame of the clip is labeled as \textit{intimate relationship} and the frames labeled as \textit{intimate relationship} are no less than those labeled as \textit{non-intimate relationship}. Otherwise, a clip is labeled as \textit{non-intimate relationship} in a similar way. A clip with only one person presented is labeled as \textit{only one person} and labeled as \textit{no person} if there is no person existing in the clip.
\end{itemize}

Examples of the annotated frames in the dataset are demonstrated in Figure~\ref{fig:annotation_example}. Each frame was annotated by at least three of our authors and then cross-checked.