\section{Problem Definition and Implementation Pipeline}
%The trade-off between image utility and privacy preserving has been widely discussed in the field of both computer vision and human-computer interaction. The general idea of various existing models is to preserve the users' privacy as much as possible without losing too much of the image utility. Here, we will define this trade-off problem in a more quantitative way.
This section offers the mathematical definition of privacy-preserving machine recognition using low-resolution images.

\subsection{Problem Definition}
Assume $\mathcal{X}$ to be the raw image set in a realistic environment that could be captured by the image sensor, for example, single-frame pictures or multi-frame videos. 
$f_r(\mathcal{X})$ represents the captured image set from the image sensor at a resolution of $r\times r$.
% $f_r(\cdot)$ denotes the function that re-samples the image set to a targeted image resolution at $r \times r$. 
% Thus, $f_r(\mathcal{X})$ represents that all images in the $\mathcal{X}$ set have the same resolution at $r \times r$.
Assume $T$ to be the main recognition task associated with $\mathcal{X}$, in this paper, ADLs recognition. $P$ is the visual privacy awareness task associated with $\mathcal{X}$. There are three main components in our model.
\begin{itemize}
    \item \textbf{Recognition Function}. We define the recognition function of the main recognition task $T$ as $f_T(\cdot)$ and the privacy detection function designed for the privacy feature $P$ as $f_P(\cdot)$. Both $f_T(\cdot)$ and $f_P(\cdot)$ can generate the recognition results given the captured image set $f_r(\mathcal{X})$. 
    % Our model will explore both human and machines' recognition performance on the image set. 
    To give an example of machine recognition, $f_T(\cdot)$ and $f_P(\cdot)$ can be computer vision models such as artificial neural networks.
    \item \textbf{Evaluation Function}. We define the evaluation function $L_T(\cdot)$ and $L_P(\cdot)$ which take both the outcome of the recognition function $f_T(f_r(\mathcal{X}))$ and $f_P(f_r(\mathcal{X}))$ as input and evaluate the performance of the recognizers according to the ground truth labels $g_T(\mathcal{X})$ and $g_P(\mathcal{X})$.
    \item \textbf{Importance Weights}. Considering the variety of privacy features contained in the captured image set, there may be differences in humans' perceived importance of different privacy features. For a given type of privacy feature $P_i$ in the privacy feature set $\mathcal{P}$, we define a weight coefficient $\omega_i$ to denote humans' perceived importance of $P_i$.
\end{itemize}

% \subsection{Optimization Objective}
% Empirically, when we set the resolution of the image sensor to a extremely low value, the performance of the privacy recognizer will drop significantly while the performance of the activity recognizer will not be affected too much. 
% Such an observation is one of the key assumptions of realizing the trade-off between action recognition and privacy preserving, which has been illustrated intuitively in Figure~\ref{fig:intro}. 
% To put it in a mathematical way, under the resolution of $r_1$ and $r_2$ where $r_1 \ll r_2$, then we define
% \begin{align*}
% \Delta L_T&=\left\vert L_T\left(f_T(f_{r_2}(\mathcal{X})), g_T(\mathcal{X})\right) - L_T\left(f_T(f_{r_1}(\mathcal{X})), g_T(\mathcal{X})\right)\right\vert\\
% \Delta L_P&= \left\vert L_P\left(f_P(f_{r_2}(\mathcal{X})), g_P(\mathcal{X})\right) - L_P\left(f_P(f_{r_1}(\mathcal{X})), g_P(\mathcal{X})\right)\right\vert 
% \end{align*}
% Our assumption is that the camera system can obtaining little detailed visual information while still capture as much necessary non-private information as possible under $r_1$ to maintain the performance of target image recognition task under $r_2$. In other words,
% \begin{align}
%     \Delta L_T \ll \Delta L_P
%     \label{eq:finding}
% \end{align}

\input{figures/framework.tex}

% It is challenging to define such a trade-off that considers both activity recognition and privacy preservation. 
To optimize the trade-off between privacy preservation and activity recognition empirically, many prior works in computer vision have focused on finding suitable measurement metrics and objective functions mathematically~\cite{jihun2016minimax, jure2017closed, raval2017adversarial, wu2018pilot, wu2019framework}. However, most of these aforementioned works ignored humans' mental evaluation and recognition abilities of privacy features, and were thus insufficient. Based on prior works, we regard our research problem as mathematically optimizing the objective function $S(r)$ shown in Equation~\ref{eq:problem}, to reveal the trade-off between privacy preservation and machine recognition with both machine and human factors taken into consideration. 
\begin{equation}
    S(r)=L_T\left(f_T(f_r(\mathcal{X})), g_T(\mathcal{X})\right) - \lambda \sum_{i=1}^{n} \omega_i L_{P_i}\left(f_{P_i}(f_r(\mathcal{X})), g_{P_i}(\mathcal{X})\right)
    \label{eq:problem}
\end{equation}
Here, $\lambda > 0$ is a scaling factor representing the sensitivity ratio of visual privacy preservation over activity recognition performance.
The goal of formulating our research problem in the form of Equation~\ref{eq:problem} is to find out optimal resolution ranges where (1) cameras are limited from obtaining detailed visual information to preserve as much privacy information as possible and (2) cameras are able to capture as much detailed non-private information as possible to improve recognition performance.
 
\subsection{Implementation Pipeline}
The implementation pipeline of solving the optimization problem in Equation~\ref{eq:problem} has been depicted in Figure~\ref{fig:framework}.
% This section summarizes our core logic for solving this problem and describe the relationship between the different sections below.
In this paper, we choose the activities of daily living (ADLs) recognition task in a home environment as our target task $T$. 
% Nevertheless, it is still unclear which privacy feature $P$ we need to preserve preferentially given such a target task $T$. 
First of all, we conducted a user study to obtain humans' perceived importance of various privacy features ($\omega$ in our formulation), with  
% Also, it is crucial to know how human valuation of privacy features changes under different resolutions. 
the main results presented in section~\ref{sec:study1}. To evaluate our model in realistic environments, we utilized the publicly available video dataset PA-HMDB51 which is described in section~\ref{sec:dataset}.

The critical step of the whole implementation procedure is to model the recognition abilities of humans and machines under different resolutions. In order to preserve privacy comprehensively, we consider the recognition performance of both the human and the machine for each specific privacy feature $P$ to get an estimation of the evaluation results ($L_P$ in our formulation). The processing logic is similar for the activity recognition task to obtain the evaluation results ($L_T$ on our formulation). 
% Mathematically speaking, the recognition function in Equation~\ref{eq:problem} should be rewritten as
% \begin{align}
%     \widehat{f}_T&={\arg\max}_{f_T\in \mathcal{F}_T} L_T\left(f_T(f_r(\mathcal{X})), g_T(\mathcal{X})\right)\\
%     \widehat{f}_P&={\arg\max}_{f_P\in \mathcal{F}_P} L_P\left(f_P(f_r(\mathcal{X})), g_P(\mathcal{X})\right)
% \end{align}
% Take these results into Equation~\ref{eq:problem}, we can simplify our objective function as
% \begin{equation}
%     \widehat{S}(r) =L_T\left(\widehat{f}_T(f_r(\mathcal{X})), g_T(\mathcal{X})\right) - \lambda \sum_{i=1}^{n} \omega_i L_P^i\left(\widehat{f}_{P^i}(f_r(\mathcal{X})), g_P(\mathcal{X})\right)
%     \label{eq:implement}
% \end{equation}
We conducted a user study to model human recognition performance on those tasks in section~\ref{sec:study2}. Then, we utilized state-of-art computer vision models to finish those recognition tasks under different resolutions in section~\ref{sec:study3}. 
% However, with the development of super-resolution techniques, it is doubtful whether our estimation is robust against these state-of-art techniques. To handle this problem, we conducted additional surveys to check the robustness of our modeling results in section~\ref{sec:super}. 
Also, we provided additional analysis in section~\ref{sec:super} to check whether our modeling results are robust against currently state-of-art super-resolution techniques.
In the end, we proposed the calculating procedure of our objective function in section~\ref{sec:discussion} to model the trade-off between privacy preservation and activity recognition.

% \subsection{Threat Model}

% Threat modeling is an integrated process that helps to design secure and private systems. Typically, a threat modeling process consists of two steps: 1) listing assets of the system; and 2) identifying possible security and privacy threats on those assets \cite{myagmar2005threat, de2005threat, desmet2005threat}. 

% \begin{figure}[hbtp]
%     \centering
%         \includegraphics[width=0.75\columnwidth]{figures/threat_model.png}
%     \caption{Threat model of the ADLs recognition in the home environment.}
%     \label{fig:threat_model}
% \end{figure}

% Figure~\ref{fig:threat_model} illustrates the threat model of the ADLs recognition vision system. The image sensor keeps capturing raw images of the user's activities at home. Then, the images are processed on an embedded system and/or transferred to a remote server through a wire or wireless network to perform ADLs recognition. Both the embedded system or the server can store the images for later revision or analysis. According to these assets, we identify three privacy threats.

%  \emph{\textbf{Hardware Attack (T1)}}: the attacker invades the embedded system through the network and fetches the raw data from the image sensor.
 
%  \emph{\textbf{Network Spoofing (T2)}}: the attacker monitors the data transmission between the embedded system and the server, spoofing the system to obtain the transmitted data (either raw data or processed data).
 
%  \emph{\textbf{Server Attack (T3)}}: the attacker hacks the server and fetches the data (either raw data or processed data).
 
% By analyzing these threats, we found that to protect the visual privacy against the attacker fundamentally. We can optimize the appropriate specifications of the image sensor. Therefore, the raw image data does not contain much private information. One promising solution is the anonymized image~\cite{Ryoo2017, Ryoo2018,Xu-fully-coupled, Chen2016,Ryoo-2017, Ryoo-2016}, which adopts ultra-low-resolution image sensors to preserve visual privacy.

% %\xin{This part should be added.}

%In this paper, we explore the following research questions. 

%1) How image resolution affects human's perception ability on the ADLs recognition and visual privacy awareness?

%2) How image resolution affects the cutting-edge machine learning methods on ADLs recognition and visual privacy awareness?

%3) How to calculate the image resolution boundary that machine can recognize daily activities while the visual privacy can be safeguarded?
