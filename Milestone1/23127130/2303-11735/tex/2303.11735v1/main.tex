\documentclass[fleqn,10pt]{wlscirep}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bm}
\usepackage{tikz}
\usepackage{todonotes}
\usepackage{xcolor}
\usepackage{pgffor}
\usepackage{ifthen,xstring}
\usepackage{bbold}

%% Packages not needed in published version
\usepackage[normalem]{ulem}


\usetikzlibrary{calc}
\usetikzlibrary{external}
%\tikzexternalize[prefix=tikz/] % Don't recalculate unchanged Tikz

\newcommand{\doiurl}[1]{\href{https://dx.doi.org/#1}{#1}}
\newcommand{\BB}[1]{{{\color{magenta}{BB: #1}}}}
\input{tikzstuff} % TN primitives

\title{Tensor Networks for Quantum Machine Learning}% (with Applications to Physics)}
\author[1,*]{Hans-Martin Rieser}
\author[1]{Frank Köster}
\author[1,+]{Arne Peter Raulf}
\affil[1]{Deutsches Zentrum für Luft- und Raumfahrt, Institute for AI safety and security, Ulm / St. Augustin, Germany}

\affil[*]{e-mail: hans-martin.rieser@dlr.de, https://orcid.org/0000-0002-1921-1436}
\affil[+]{https://orcid.org/0009-0003-8672-3014}

\begin{abstract}
Once developed for quantum theory, tensor networks have been established as a successful machine learning paradigm. Now, they have been ported back to the quantum realm in the emerging field of quantum machine learning to assess problems that classical computers are unable to solve efficiently. Their nature at the interface between physics and machine learning makes tensor networks easily deployable on quantum computers. In this review article, we shed light on one of the major architectures considered to be predestined for variational quantum machine learning. In particular, we discuss how layouts like MPS, PEPS, TTNs and MERA can be mapped to a quantum computer, how they can be used for machine learning and data encoding and which implementation techniques improve their performance.

\end{abstract}
\begin{document}

\flushbottom
\maketitle

\thispagestyle{empty}

\section{Introduction}

Quantum computation is widely believed to set a new paradigm in computation. Utilizing quantum phenomena allows to solve certain problems% like prime factorization
\cite{Shor.1994} far more efficient than classical binary algorithms. This raises hope that quantum implementations of other tasks also may provide quantum advantages.

One of the applications that could benefit from the access to the high dimensional Hilbert spaces of quantum computers is machine learning (ML). ML is a data driven approach for solving complex problems. An ML algorithm generates a model from training data that can be used to make predictions against previously unseen data. Quantum machine learning (QML) could advance learning by improved generalization to unknown data~\cite{Caro.2022}, higher noise robustness and the need for less training data~\cite{Abbas.2021}, and provide a more natural approach to quantum data analysis circumventing intermediate measurements~\cite{Perrier.15.08.2021} or generally a better computational complexity scaling~\cite{Boixo.2018}. 

Promising candidates for QML architectures are tensor networks (TN). They provide a structured approach for handling large objects with tensor structure which carry high amounts of correlated information like quantum states. Initially developed to store and process physical states of many-body quantum systems in numerical simulations\cite{White.1992,StellanOstlund.1995}, TNs also turned out to be useful for ML applications.
Their approach to realize learning architectures is complementary to neural networks. 
As the TN description uses a (quantum) state and operator formulation, the transfer to a quantum computer can be done naturally. 

In this review, we focus on the application of TNs for QML. We will begin with a short introduction to the classical TN theory including optimization and ML approaches in Section~\ref{sec:class}. Then, we will discuss how to apply these concepts to a quantum computer in Section~\ref{sec:qtn} and the encoding of data to quantum states for ML applications in Section~\ref{sec:Encoding}.

We will not cover many aspects of classical TNs in detail. For a deeper technical dive into TNs, the reader may refer to a general introduction~\cite{Bridgeman.2017} and the reviews on specific layouts~\cite{Cirac.2021,Evenbly.2009} or decomposition and optimization techniques~\cite{Schollwock.2011,Cichocki.2016,Vanderstraeten.2019}. Applications are many-body quantum systems~\cite{Orus.2019}, nonlinear system identification~\cite{Batselier.2022} and classical ML~\cite{Cichocki.2016b,Levine.2023}.

The field of TN-QML is just developing, and notations and terminology vary throughout the community. Due to their origin in quantum theory, some authors call even ML with classical TNs "quantum machine learning"\cite{Liu.15.05.2020}. In our opinion, a more suitable term would be \emph{quantum-inspired} here. Furthermore, one can argue that variational quantum circuits (VQC)~\cite{Schuld.2021} require classical optimization and therefore are hybrid. In this article however, we will use the following convention: Methods fully evaluated without a quantum computer will be called \emph{classical}. Methods developed for quantum computers that only require classical optimization of weights will be called \emph{quantum} TNs (QTN), as full quantum computation is still far out of reach. The term \emph{hybrid} will be used for methods that combine QML with a classical data processing structure, e.g. pre-training or data pre- and post-processing.

\section{Classical Tensor Networks}\label{sec:class}
    
\subsection{Introduction on Tensors and Tensor Networks}\label{ssec:tensor}

TNs are a decomposition of large tensorial structures into several connected low rank tensors (see Figure \ref{fig:structure}~(a)). Tensors are multidimensional arrays and therefore generalizations of vectors and matrices. While a matrix has two indices a tensor may have an arbitrary amount of indices. Technically, tensors describe objects from and maps between tangent and cotangent spaces. A tensor may have regular (lower) and dual (upper) indices depending on whether this index refers to objects from a tangent space or from a cotangent space. Each of these spaces may have different dimensions. A single tensor may have both types of indices and therefore connect to both tangent and cotangent spaces. The rank of a tensor corresponds to its number of free indices.

	\begin{figure}
	    \centering
	    \input{structure}
	
	\caption[Examples for common tensor network layouts]{Examples for common tensor network layouts. (a) a general irregular tensor network. One may use any tensor network structure to express the large tensor $A^{abcd}$ or the wave function $\langle\Psi|$. However, regular tensor networks provide benefits in terms of interpretability and universality. Both (b) matrix product states (MPS) and (c) projected entangled pair states (PEPS) share the same grid structure with different dimensionality. (d) tree tensor networks (TTN) and (e) multiscale entanglement renormalization ansatz (MERA) have a hierarchical structure, where MERA entangle between individual branches in contrast to TNN. %This figure is inspired by\cite{Orus.2019}
    }
	    \label{fig:structure}
	\end{figure}

The electromagnetic field tensor $F_{ab}$ from relativistic physics for example is a rank two tensor with four dimensional space time indices each and the Riemann curvature tensor from general relativity ${R^a}_{bcd}$ is a rank four object, with one dual ($a$) and three regular ($b$, $c$, $d$) indices. Free regular indices can be contracted with free dual indices by summing over all dimensions of this index. Einstein sum convention is a convenient form to express this contraction: having the same index twice automatically implies a summation

    \begin{equation}
        -\frac{1}{4\mu_0}F_{ab}F^{ab}=-\frac{1}{4\mu_0}\sum_{a,b=0}^{3}F_{ab}F^{ab}.
    \end{equation}

As writing these tensors with indices can be very complex for larger problems, graphical notations like the Penrose diagrams have been developed to simplify the handling of tensor equations\cite{Schollwock.2011}. The tensors from before correspond to the diagrams 

    \begin{tikzpicture}[scale=.8]
        \draw[line width = 1pt] (0,-1)--++(0,2);
        \draw[line width = 1pt] (-.5,1)--(0,0)--(.5,1);
        \draw[line width = 1pt, fill=white] (0,0)circle(.5);
        
        \draw[line width = 1pt] (1.75,-.5)--++(0,1.5);
        \draw[line width = 1pt] (2.25,-.5)--++(0,1.5);
        \draw[line width = 1pt, fill=white] (2,1)circle(.5);
        \draw[line width = 1pt, fill=white] (2,-.5)circle(.5);
        \node at (0,0){$R$};
        \node at (2,1){$F^{ab}$};
        \node at (2,-.5){$F_{ab}$};
    \end{tikzpicture}

The graphical notation actually is one strength of the TN paradigm, as it provides accessibility to high dimensional states: Each symbol is a tensor, its rank is given by the number of legs it has and the type of index determines the direction of the associated leg. 

Figure \ref{fig:structure}~(a) illustrates the idea behind the TN approach: A large tensor $A^{abcd}$ which may represent some quantum state $\langle\Psi|$ usually is hard to handle computationally. It requires large storage space and the manipulation of a large number of entries for each operation. Breaking down $A$ into a network of smaller connected tensors improves computability when the internal structure of $A$ matches the TN's layout. This requires a third kind of tensor index called \emph{internal} or virtual index that connects the constituents of the TN. We will denote this kind of index in greek letters. The dimension of internal indices is called bond dimension $\chi$. It determines how strongly the constituent tensors are coupled and how much information is shared between them.

TNs allow to apply local operations individually on each tensor node instead of having to evaluate the whole tensor at once. Tensors can be joined by contracting over connected indices or decomposed into several connected tensors. The most common technique for decompositions along a single direction is singular value decomposition (SVD), a generalization of diagonalization for arbitrary shaped tensors. Polar decomposition is faster than SVD, but does not allow for reducing bond dimensions easily. Tucker decomposition can be used for decomposing nodes within several directions at once\cite{TamaraG.KoldaandBrettW.BaderSandiaNationalLaboratories.2018}. 

The general idea behind tensor decomposition methods is to represent an arbitrary tensor with a specific set of constituent tensors. In SVD for instance, a tensor $A^\alpha_{a\delta}$ is decomposed into a unitary matrix $U^\alpha_\beta$, a diagonal singular value matrix $\Sigma^\beta_\gamma$ and an isometric matrix $V^\gamma_{a\delta}$

\begin{equation}
    \begin{tikzpicture}[baseline=(current  bounding  box.center)]
        \node[left] at (-.6,0){$A^\alpha_{a\delta} =$};
        \draw[line width=1pt](-.5,0)--++(1,0);
        \draw[line width=1pt](0,0)--++(0,.45);
        \TNode{}{0,0}{white}{};
        \node at (.75,0) {$=$};
        \node at (.75,.25) {\tiny SVD};
        \draw[line width =1pt](1,0)--(3.35,0);
        \draw[line width =1pt](3,0)--++(0,.45);
        \TNode{C}{1.5,0}{white}{$U$};
        \TNode{}{2.25,0}{white}{$\Sigma$};
        \TNode{2}{3,0}{white}{$V$};
        \node[right]at(3.5,0){$=U^\alpha_\beta\Sigma^\beta_\gamma V^\gamma_{a\delta}$};
    \end{tikzpicture}
\end{equation}

where isometric tensors with known direction are given by triangles, unitaries and isometries with unknown orientation by squares and any other kind of tensor by a circle. Having access to the singular values in the diagonal matrix $\Sigma$ allows for reducing bond dimensions by removing zero singular values. This also can be used for approximation removing the lowest singular values having the least contribution to the bond.

Since tensor decompositions can be done in any direction on each bond and contracted to each side at any time, TNs are not unique but contain a gauge degree of freedom. One can make use of this property to bring the TN to a canonical form where the bonds form orthonormal Hilbert spaces\cite{Orus.2019} and the tensors are isometric or even unitary\cite{Bridgeman.2017}. In many cases, it makes sense to bring the TN to such a canonical form where all tensor nodes are isometric. This has several advantages. First of all, isometric tensors automatically fulfil a normalization condition $A^{a\mu} A^\dag_{a\Tilde{\mu}} = \delta^\mu_{\Tilde{\mu}}$ which enables the application of optimization schemes (see Section \ref{ssec:opt}). Second, it is mandatory for techniques that require directionality \cite{Zaletel.2020} or make use of the properties of isometries \cite{Geng.2022}. In particular, mapping a TN to a quantum circuit requires the tensors to be at least isometric (see Section \ref{sec:qtn}). %Some layouts like MERA are defined only in terms of isometries and unitaries, the other layouts can be brought to their canonical form using singular value (SVD) or Tucker decomposition.

%Contracting tensors scales with their rank and the indices' dimensions. As the tensors usually grow with each contraction step and the contraction order is not unique, one has to optimize the path taken, e.g. by counting dimensions or via ML methods\cite{Meirom.2022} \todo[size=\scriptsize]{ML: citation does not work}. Expectation values for operators from a unitary TN can be sampled using Monte Carlo methods  instead of performing a costly contraction. Due to the unitary constraint, only nodes connected to the operator have to be evaluated in this scheme\cite{Ferris.2012}.

\paragraph{Applications in Quantum Computing} are based on the original appliction of TNs: reducing the computational cost of storing and evaluating lowly entangled multi-particle quantum states. This comes in handy for quantum computer simulations both for execution\cite{Zhou.2020,Nguyen.21.04.2021} and validation\cite{McCaskey.2018} of circuits as well as the estimation of errors\cite{Guo.2020}. Especially for short NISQ era algorithms, entanglement between many qubits usually is not too high and therefore circuit sizes well beyond the power of other simulation methods can be evaluated using TNs\cite{Pednault.16.10.2017}. %A structured search within a TTN to identify highly entangled circuit parts may reduce the computational effort further \cite{Seitz.02.06.2022}.

Additionally, TNs have been proposed to parallelize quantum simulations by cutting the system into several weakly entangled pieces and approximating the state of all but one piece by TNs\cite{Barratt.2021}. Simulating a quantum computer may indeed be more resource efficient than using quantum hardware itself for a lot of low-entanglement applications\cite{Jaschke.24.05.2022}. This idea has been used already to develop quantum inspired algorithms executed on classical hardware, e.g. for optimizing stock market portfolios\cite{Alcazar.15.01.2021, Mugel.2022} or radiotherapy plans\cite{Cavinato.2021} with quantum algorithms compressed to a classical TN approximation.

\subsection{Tensor Network Layouts}\label{ssec:layout}

Technically, the TN may have any shape but using regular TNs provides many benefits like simpler optimization, simpler control and transferability to problems with different structure. Such TNs are also more interpretable than arbitrary networks. The most common layouts either are grid (Fig. \ref{fig:structure} b and c) or hierarchical (Fig. \ref{fig:structure} d and e) states. Promoting state layouts to operators is either done by allowing every individual grid tensor node to have regular and dual indices or by connecting a complete hierarchical network with its dual on their topmost layers. 

\paragraph{Grid Layouts} are the most natural TN description of physical lattices as the layout has a similar structure to the system. These layouts can be seen as derivatives of Projected Entangled Pair States (PEPS)\cite{Sierra.1998}. In quantum applications, PEPS nodes are constructed as composite objects consisting of coupled internal spins. Each spin connects to a neighboring site via an edge and at each node the constituent spins are entangled and truncated, thus the name PEPS. The number of spin tuples depends on the the dimensionality of the network\cite{Cirac.2021}, typically a hypercube or hexagonal.

The constituent spin construction is very useful when employing PEPS for the description of quantum systems as this allows for spin constraints on the bonds. For ML applications however, ansaetze for the nodes reflect computational approximations or inductive biases. 

Although PEPS are defined for arbitrary dimensions, usually low dimensional layouts are used. One dimensional PEPS are called Matrix Product States (MPS) or tensor trains. %Due to the possible confusion with TTNs, we will avoid using the latter name. 
These are the simplest and most studied TN layouts\cite{Cirac.2021}. In index notation, the MPS from Fig.~\ref{fig:structure}~(b) will look like

    \begin{equation}
    	A^{abcde} = \tilde{A}^{a(1)}_{\alpha}\;\tilde{A}^{\alpha b(2)}_\beta\;\tilde{A}^{\beta c(3)}_\gamma\;\tilde{A}^{\gamma d(4)}_\delta\;\tilde{A}^{\delta e(5)}
    \end{equation}
with constituent tensors $A^{(k)}$. Common gauges for MPS are called left, right and site canonical forms depending on the orientation of the isometric tensor nodes\cite{Cirac.2021}.

Brickwall or checkerboard TNs used in some quantum computing applications\cite{Uvarov.2020, Lubasch.2020, Lazzarin.2022} are another variety of two dimensional grid layouts equivalent to a hexagonal PEPS. The brickwall layout is a superposition of MPS up to a certain bond dimension\cite{Lubasch.2020} as it allows for the realization of MPS of different gauges overlapping at the same time.

\paragraph{Hierarchical Layouts} have input or output tensor nodes that are not coupled directly but are pooled on several internal layers. The simplest hierarchical structure is a tree tensor network (TTN) where two or more child nodes are connected to a parent node in the next layer until only a single node is left on the top. This layout is also called hierarchical Tucker decomposition. TTNs are able to catch both local entanglement and long range entanglement between groups of nodes, but not long range entanglement between individual tensor nodes. %This leads to a logarithmic scaling of the maximum entanglement entropy a TTN can carry. When optimizing a TTN for a state with long range individual interaction, this still will lead to a growth of the bond dimensions necessary to fully represent the state. 
A TTN may have variable depth on different branches when the considered system is not homogeneous\cite{Murg.2015}.

The Multi Scale Entanglement Renormalization Ansatz (MERA) is an isometric TTN derivative with better entropy scaling \cite{Vidal.}. The main idea is to enhance the hierarchy with layers of unitary nodes connecting neighboring branches. These so called \emph{disentanglers} reduce entanglement passed on to the next level (see figure \ref{fig:structure} e).
MERA has a higher computational cost than other layouts due to the loops, but it can capture symmetry and far higher entanglement\cite{Cirac.2021,Araz.21.02.2022} while still being efficiently storable\cite{Bridgeman.2017}. Varieties of MERA %like bMERA 
offer even better entropy scaling \cite{Orus.2019}. Both TTN and MERA can be generalized to higher dimensions by considering unit cells of the respective dimension at each node\cite{Tagliacozzo.2009,Cincio.2008}. 

\vspace{1em}
    
The layout of a TN determines the maximal entanglement or internal correlation it can  support. This gives a bound on the system type the TN can approximate without having a bond dimension scaling exponentially with the system size. For MPS and PEPS entanglement fulfils an area law which means, that the amount of entanglement between a sub-network and its surroundings scales with
its boundary\cite{Wolf.2008}. This means, the entanglement for an 1-D MPS is constant \cite{Cirac.2021}, for a 2-D PEPS it scales linearily. For MERA based layouts, the entaglement scales up to a volume law, where a sub-networks entanglement with the surroundings depends on the number of nodes within the sub-network.

In practice, the choice for a specific layout usually is a trade-off between the possible entanglement and the computational cost: MPS and TTN can be contracted efficiently, MERA and PEPS usually are costly.

Further refinements can be made by applying symmetries to the TN\cite{Cirac.2021}. Relevant symmetric systems are homogeneous or periodic grids or layers in hierarchical networks\cite{Bridgeman.2017}. For ML, this reduces the complexity of the TN and makes it easier to train. 

\subsection{Optimization Methods}\label{ssec:opt}

The term 'optimizing TNs' can refer to two things. The size of a TN representation can be reduced by iterative executions of tensor decompositions along the internal bonds. This allows for the local adaption of bond dimensions to relevant degrees of freedom, e.g. by defining a threshold for relevant singular values. 

More often however, one seeks to optimize the value of some function of the TN. In quantum physics for example, this means to maximize the overlap between some given state and a TN approximation or minimizing the energy expectation value with respect to some Hamiltonian to find its TN ground state. This corresponds to minimizing a loss function of a TN based ML approach. The optimization can be achieved via several well established methods. In particular, general global gradient methods are available as well as TN specific techniques which make use of the network's locality and the tensorial nature of the nodes.

\paragraph{Renormalization methods} make use of the gauge ambiguity in TNs. They exploit the locality of operators to optimize the TN site by site. Density matrix renormalization group (DMRG), the first method of this kind, was developed to optimize spin chain Hamiltonians efficiently \cite{White.1992}. Soon, it was understood that restricting the maximum entanglement at each site reduces computational resources while describing lowly-entangled chains very well\cite{Vidal.2004} and further renormalization techniques were developed\cite{Daley.2004}. These provide powerful tools for optimizing MPS. 
Renormalization methods for TNs have been reviewed extensively before\cite{Schollwock.2011,Bridgeman.2017,Orus.2019}. Hence, we will only sketch the basic idea of DMRG for a finite MPS here.

DMRG can be applied to Hamiltonians $H$ that consist of independent blocks connecting neighboring MPS nodes. First, initialize a state randomly and consider the expectation value $\langle\Psi_0 | H |\Psi_0\rangle$. Start with a block at one end of the chain and contract all other nodes to an environment tensor generating an effective Hamiltonian for the first site. Diagonalize the effective Hamiltonian and truncate its Hilbert space to the lowest (effective) eigenvalues. Subsequently iterating this procedure at each site, will deterministically evolve the MPS towards the Hamiltonian's groundstate.

Renormalization methods provide a local and fast way of optimization adapted to the structure of TNs but also have some disadvantages. First, DMRG is hard to implement in standard ML frameworks, especially when combining TNs and neuronal layers\cite{Barratt.2022}. The algorithm has to be handcrafted for each problem\cite{Geng.2022}. Second, generalization to higher dimensions is possible \cite{Zaletel.2020,Wall.2021} but not as efficient as for MPS due to entropy scaling\cite{Cirac.2021}.

\paragraph{Global gradient methods} are standard optimization techniques that also apply to TNs. While using an overall global gradient usually is outperformed by renormalization methods, global methods make sense in special cases. In particular, renormalization methods have not been established yet for QTNs. Currently, stochastic gradient approximation methods\cite{Spall.1992} are employed in QML to circumvent the need for costly calculations of total gradients in high dimensional parameter spaces \cite{Grant.2018}.

Global gradients have the downside that the gradient may vanish for random initial conditions in high dimensional parameter spaces. In QML, this is usually referred to as the \emph{barren plateau} phenomenon \cite{McClean.2018} and is similar to the vanishing gradient problem known from classical ML~\cite{Hochreiter.1991}.

The performance of gradient methods can be boosted by considering the special structure of TNs, e.g. with adapted initialization schemes \cite{Barratt.2022}. Introducing locality either on the optimization routine or the loss can also mitigate barren plateaus (see Section \ref{sec:qtn}).

\paragraph{Geometric methods} make use of the network's underlying tensorial geometry. Tools from differential geometry can be used for analyzing the TN on the space of entanglement patterns \cite{Swingle.2012} and optimizing on loss manifolds \cite{Rohwedder.2013}. This kind of optimization performs well on high dimensional parameter spaces, especially in combination with stochastic gradient descent \cite{Novikov.12.05.2016} and auto-differentiation on individual nodes\cite{Luchnikov.2021,Hauru.2021} or whole layers\cite{Geng.2022}.

More advanced geometric methods reuse previous update steps. For this, their gradient vectors have to be transported along the optimization manifold \cite{Cichocki.2016b}. However, they have to be applied in practice yet.

\subsection{Classical Machine Learning with Tensor Networks}\label{ssec:cml}

We already discussed in Section \ref{ssec:generalSt} that TNs are able to approximate high dimensional states within a regular, less complex structure. In ML, such states arise as maps of data features and as weight tensors that connect the data features to the desired result, e.g. a label in classification\cite{EdwinStoudenmire.}. 

\begin{figure}
    \centering
    \input{Class2QTN.tex}
    \caption[From a classical classifier to an efficient quantum tensor network.]{From a classical classifier to an efficient quantum tensor network. (a) Formally, the task of classification is performed by some function $f_l(x)$ which is an object, that accepts input data $x$ and outputs some label $l$. (b) In machine learning, one realizes the classification function $f_l(x)=\langle W_l|\Phi(x)\rangle$ with a weight tensor $W^l$ with trainable parameters where the (possibly transformed) input $\Phi(x)$ is fed into. The classification function is constructed as an overlap between both tensors. (c) In a tensor network approach, one decomposes the large tensor $W^l$ into a network of smaller tensors, e.g. by restricting the structure to a matrix product state (MPS) layout. In this case, the bond dimension is $\chi=4$. (d) By identifying isometric tensor nodes with unitary quantum gates (grey boxes), the MPS classifier can be mapped to a quantum computer. Higher bond dimensions between the tensor nodes require multi qubit gates. In this case, the resulting circuit needs $\log \chi = 2$ internal qubits and three qubit gates. (e) The multi qubit gates can be expressed by a repetition of the MPS two qubit gate structure. Each additional internal qubit requires another layer of two qubit gates. (f) If the quantum hardware supports resetting qubits during execution, a qubit efficient approach can be implemented reusing discarded qubits. The efficient circuit is a trade-off between qubit number and circuit length.}
    \label{fig:Class2QTN}
\end{figure}

In principle, an ML algorithm seeks to find a function $f_l(x):\mathcal{D}\to\mathcal{S}$ of some datum $x$ within the space of all possible inputs $\mathcal{D}$ that is mapped to a space of possible results $\mathcal{S}$, for instance a set of labels $l$. This function is called the model. Usually, the model is a composition of a data embedding $\Phi(x)$ and a trainable weight tensor $W_l$ connecting the embedded data to the output, as shown in Fig.~\ref{fig:Class2QTN}~(a)-(b). The weight tensor $W_l$ can be approximated as a TN whose output represents the choice of labels (see Fig.~\ref{fig:Class2QTN}, c). We get 

     \begin{equation}
         f_l(x)=W_l\circ\Phi(x)\approx\langle W_{l,TN}|\Phi(x)\rangle
     \end{equation}

where $\langle W^l_{TN}|$ is the TN approximation of the weight tensor. The dimensions of the weight tensor's index $l$ store the probabilities $P^{l_i}$ of the corresponding labels $l_i$ 

\begin{equation}
    W^\chi_{l_i}\circ \Phi_\chi(x)=P_{l_i}(x).
\end{equation}

Multi-class classifications are either done by training a single TN with large outgoing bond dimension or a set of networks with a single outgoing label bond each (one versus all). The data is embedded with a feature map that can transform the data before mapping it to the network\cite{Wall.2021b}. This approach is very similar to encoding maps for QML\cite{Schuld.2021} and can be approximated as TN as well.

A second way of embedding data into a feature space is using a density matrix $|\Phi(x)\rangle\langle\Phi(x)|$ and contracting it with a label dependent weight state $|W_l\rangle$. In this construction, the bond dimension $\chi$ is given directly by the non-vanishing eigenvalues of the covariance matrix\cite{Wall.2021} and the decision function is realized as the maximum overlap
	
	\begin{equation}
	    f_l(x)=\text{argmax}_l \langle W_l|\Phi(x)\rangle\langle\Phi(x)|W_l\rangle.
	\end{equation}

This construction has the advantage of being able to process incomplete data by contracting over missing bonds and can represent specific probability distributions based on the data sets\cite{Wall.2021b}. 

Building generative TN models is also straightforward. The goal of a generative ML model is to learn the distribution of its training data and to generate additional samples from this distribution. The simplest possibility is to use the dual of a trained classifier or a regressor obtained by adjoining all tensor nodes within the network. %This leads to a network similar to a prepared state with some additional input. Adding noise channels leads to novel data.

Due to their quantum inspired construction, TNs have the issue of not being able to copy information within their structure. This means, that information cannot be distributed to different branches of a TN in a way a neural network uses information to activate its neurons for example. If for instance an operation in image analysis needs to use the value of adjacent pixels, one has to pass the same data into several input nodes by using overlapping observation windows\cite{Glasser.15.06.2018}. However, this approach does not allow copying connected tensors to different locations.

Often, it makes sense to combine different layouts to use advantages of both. As an example, hierarchical layouts coarse grain the data and grid layers can be used to efficiently combine the information from different branches of the hierarchical TN \cite{Reyes.22.01.2020}. The hierarchical part can be optimized with unsupervised ML methods where the ideal weight tensor is derived from the data covariance matrix\cite{Stoudenmire.2018}. It is even possible to add a TN layer to a neural network architecture e.g. for complexity reduction in the input layer with MPS\cite{Chen.2018}, MERA convolutional layers \cite{Kong.} or approximating a fully connected layer \cite{Novikov.2015}.

TN architectures are closely related to neural networks. Restricted\cite{Chen.2018,Glasser.2019} and deep\cite{Li.2021} Boltzmann machines can be mapped to a two dimensional TN consisting of MPS and Matrix Product Operators (MPO), an operator valued version of MPS. Boltzmann machines therefore may be simulated using an MPS which allows for adjusting accuracy and execution time via the bond dimension allowing for a compression of neural network representations\cite{Cichocki.2016b}.

The map between both architectures has been exploited in both ways to compare specific network layouts. On the one hand, node numbers in an MPS representation of a Boltzmann machine will scale exponentially with the number of neurons\cite{Collura.2021} and recurrent neural networks can simulate MPS with reduced computational effort for certain cases\cite{Wu.24.06.2022}. On the other hand, hierarchical TNs efficiently implement convolutional or recurrent neural networks\cite{Levine.2019}. 

\paragraph{Applications in ML} can been found for a wide variety of tasks. In image analysis, TN based ML models are used for classification\cite{Wall.2021,Araz.2021,Felser.2021}, compression\cite{Selvan.13.11.2020} or feature extraction \cite{Kong.,Liu.2021}. TN based regressors have been successfully applied to nonlinear system identification\cite{Batselier.2022} where the task is to generate a model of a nonlinear system from its behaviour. 

Generative TN structures have been employed in anomaly detection\cite{Wang.03.06.2020} and as classifiers when reversing the generative TN structure\cite{Sun.2020}. A TN can be used to learn a probability distribution from data and a simulated "measurement" of the TN state will generate a new instance from the distribution\cite{Han.2018,Cheng.2019}. Generative TNs have also been applied to unsupervised feature identification in images\cite{Bai.2022}.

\section{Quantum Tensor Network Machine Learning} \label{sec:qtn}
\subsection{Mapping to Quantum Circuits}\label{ssec:generalSt}
	
	\begin{figure}
	    \centering
    \input{TN2QTN}

	\caption[Mapping from isometric tensors to unitary quantum gates]{Mapping from isometric tensors to unitary quantum gates. 
	(a) Whether a bond $\alpha,\beta$ is mapped to an incoming or outgoing qubit bundle, depends on whether the tensor is given in right or left isometric form. The free bonds $i$ are represented by outgoing qubits. Adjoining a tensor flips its directions. Qubit preservation is taken into account by adding additional ancilla qubits or discarding left over ones. 	
	(b) Mapping the normalization condition for isometric tensors illustrates that discarding qubits actually has to fulfil a condition: For an exact representation of the classical network, discarded qubits have to be post-selected to $|0\rangle $ to be the dual of the ancilla state.}
	    \label{fig:TNtoQTN}
	\end{figure}

The quantum-inspired construction of TNs makes it straightforward to translate the concept to quantum computations. Tensor nodes are realized by multi-qubit gates with incoming and outgoing qubits carrying the bonds of the node. 

The procedure for mapping the classical TN to a QTN is shown in Fig. \ref{fig:Class2QTN} (c)-(d). Quantum gates are unitary therefore the corresponding TN has to be in canonical form with at least isometric nodes\cite{Liu.2019} (see Section \ref{ssec:layout}). Fig. \ref{fig:TNtoQTN} (a) shows how isometric tensors are mapped to gates. 
The bond dimension $\chi$ is determined by the number of qubits $n$ transferred between connected gates, i.e., $\chi=2^n$. These qubits are called internal or virtual qubits. 
The qubits carrying the free (or physical) bonds are either forward or backwards directed, depending on whether the node has a vector or dual valued index. To preserve the number of qubits, the sum of all incoming qubits (free and internal) must equal the sum of the outgoing qubits at each gate. Therefore, one prepares necessary additional incoming qubits in a dummy state $|0\rangle$ or discards left over outgoing qubits. 

Discarded qubits usually are carried on unobserved, but a direct correspondence to classical TNs requires post-selection to a reference state $\langle 0|$ on these qubits\cite{Wall.2022}. From the normalization condition in Fig.~\ref{fig:TNtoQTN} (b)

\begin{equation}
    	\delta_{\beta'}^\beta=R^{\beta(k)}_{i\alpha}{R^\dag}^{i\alpha(k)}_{\beta'}\quad\quad
    	\leftrightarrow\quad\quad\langle\beta' *| {U^\dag}^{(k)}_RU^{(k)}_R | 0\beta \rangle = \langle *|0\rangle \langle\beta'|\beta\rangle
\end{equation}

it follows that a post-selection measurement on $\langle*|$ is the counterpart of the ancilla $|0\rangle$ initialization. This is caused by the fact that an isometry is mapped to a unitary and classical dimensional reduction or information loss has to be accounted for. Instead, one also can perform an uncomputation operation for each gate used\cite{Grant.2018}. For a network fully optimized on the quantum machine, the post selection requirement can be released which allows for hybrid methods \cite{Wall.2022,Dborin.2022} or efficient layouts where discarded qubits can be reset and reused \cite{Huggins.2019}. 

Using this recipe, one can map the TN layouts known from Section~\ref{ssec:layout} to quantum circuits. Fig. \ref{fig:MPSLayouts} shows a central gauge MPS and a TTN. 
More advanced networks like a brickwall, MERA and a square PEPS are shown in Fig. \ref{fig:TNLayouts}. Mapping these networks to a quantum computer gets more and more involved with growing bond dimension and requires larger circuits with high connectivity (or many swap gates) between the qubits.

	\begin{figure}
	    \centering
	    \input{MPSlayouts}

    \caption[Simple tensor networks and their quantum counterparts]{Simple tensor networks and their quantum counterparts. Single qubit unitary gates are omitted as they can be absorbed into an adjacent two qubit unitary. Each bond may be realized by one or more qubits.
    (a) shows a matrix product state (MPS) in site canonical form and its quantum implementation. Choosing a central gauge halves the circuit depth compared to the left canonical MPS from Fig. \ref{fig:Class2QTN}. However, the central node is not isometric in general and can only be mapped to the unitary quantum gate approximately. If the quantum computer supports resetting qubits during execution, a qubit efficient approach can be implemented reusing discarded qubits with constant qubit number. (b) A tree tensor network offers higher entanglement than a matrix product state, but its qubit efficient quantum representation will need a total of $\log n$ qubits for $n$ inputs.
}
	    \label{fig:MPSLayouts}
	\end{figure}
	
	\begin{figure}
	    \centering
        \input{TNLayouts}

	    \caption[Higher dimensional quantum tensor network structures]{Higher dimensional quantum tensor network structures. (a) The brickwall architecture offers a higher amount of entanglement than matrix product states (MPS) and can be seen as a derivative of hexagonal projected entangled pair states. A brickwall allows for the representation of every MPS gauge up to a bond dimension given by the depth of the circuit. (b) The multiscale renormalization ansatz (MERA) quantum network requires gates between qubits further apart which may be realized by introducing swap gates in between on current hardware. Both brickwall and MERA do not allow for qubit efficient implementations. (c) The quantum circuit of pair entangled product states (PEPS) heavily depends on the order in which the PEPS nodes are evaluated. The realization will feature coupled staircase structures similar to MPS. Here, a qubit efficient approach scales linearily with the length of the diagonal.}
	    \label{fig:TNLayouts}
	\end{figure}

\subsection{Efficiently implementing Quantum Tensor Networks}

Large circuits, multi-qubit gates and gates not using the standard gate set are hard to implement on near-term noisy intermediate scale quantum (NISQ) computers. To reduce circuit complexity, several approaches exist. A major step in bringing TNs to quantum computers was the development of a breakdown method for multi-qubit nodes to two-qubit unitaries with high fidelity~\cite{Ran.2020, Lin.2021}, which can be implemented on NISQ devices efficiently. This approach has been based on a classical procedure for photonic qudits\cite{Schon.2005}. The approach is shown for an MPS in Fig.~\ref{fig:Class2QTN}~(d-e): This MPS has left canonical gauge and therefore has a quantum gate equivalent looking like a staircaise of multi-qubit gates. The size of the gates is given by the number of internal qubits $n=\log\chi$ that have to be passed on to the next gate (d). The three qubit gates in this example may be replaced by two layers of two qubit gates which provide the same connectivity between adjacent incoming free qubits (e). Each additional internal qubit would add another layer of two-qubit gates to the circuit. 

The most general ansatz for the gates within a tensor node is a full unitary gate\cite{Dilip.24.04.2022,Huggins.2019}. Representing these gates with simple gates available on a NISQ device however results in long circuits that are prone to noise. Therefore, simplified ansaetze for the two-qubit gates are commonly used\cite{Guala.2022, Fastovets.01.10.201805.10.2018,Araz.21.02.2022, Uvarov.2020,Dborin.2022}. For quantum input, the performance of these simplified ansaetze can yield comparable maximum performance to a general unitary ansatz, but they seem to be harder to train. For classical data, the performance was much lower using simple nodes. This holds for both grid \cite{Lazzarin.2022} and hierarchical layouts\cite{Grant.2018}.

To reduce the total qubit count, the structure of many TNs allows for an efficient reordering of its blocks, such that discarded qubits can be reset and reused for the input of new information (see Fig.~\ref{fig:Class2QTN}~f). A qubit efficient MPS only requires a constant amount of qubits determined by the dimension of the inputs and the desired bond dimension. For a TTN, the qubit number scales logarithmically with the size of the input.

Using the qubit efficient approach may not have an effect on the optimized model parameters because the circuit is trained to carry on the label information and the 'no signalling' principle therefore forbids an influence of these discarded qubits on the result\cite{Huggins.2019}. Instead of simply resetting, the information in the discarded qubits can be used for quantum error correction within the nodes\cite{Cong.2019} which improves performance on NISQ devices. In general, the influence of the qubit efficient procedure e.g. on trainability is still not clear. When combined with a local loss however, no barren plateaus arise in the error correcting ansatz\cite{Pesah.2021}.

Combining these simplifications reduces both qubit number and gate complexity \cite{Ran.2020}. The overall circuit depth is harder to reduce. Choosing a central gauge for MPS at least halves circuit depth compared to left or right gauges\cite{Dborin.2022} (see~Fig.\ref{fig:MPSLayouts}).

\subsection{Variational Machine Learning with Quantum Tensor Networks}\label{ssec:vqc}

Recently, a wide variety of ML architectures employing variational quantum circuits (VQC) have been developed. A VQC is a quantum circuit whose gates have tunable parameters. General unitaries can be constructed from a combination of rotation and entangling gates like the CNOT gate. A common architecture for QML is a layered VQC. Here, the circuit consists of encoding blocks that map the data to the circuit and parametrized variational blocks which entangle the qubits. 
To increase the expressivity of the quantum circuit, these blocks can be repeated before the measurement \cite{Schuld.2021}. 

A QTN with tunable gates is also a variety of VQC with an internal TN layout. The structure of TNs provides several advantages for QML. First of all, insights from the available theory on classical TNs also apply to their quantum counterparts. Due to the direct correspondence, data and models from classical TNs can be translated to QTNs and vice versa. This can be used to better initialize quantum models (see Section~\ref{ssec:hybrid}). Furthermore, the choice of a specific TN layout allows for the introduction of inductive bias, e.g. knowledge about the type of data and therefore the construction of a QML structure that will fit the data well. Finally unlike for general VQC algorithms, the space of possible weights in TN based ML can be adjusted easily by varying the bond dimension. This allows for tuning the expressivity of the circuit to mitigate under- and overfitting \cite{Huggins.2019}. It is not clear yet how the expressivity of a QTN scales or compares to layered VQC approaches but both architectures can be mapped onto each other\cite{Du.2020}.

Until now, the development of QTN ML approaches focuses on supervised classificators and generators. Supervised learning with QTNs works similar to the classical approach shown in Section~\ref{ssec:cml}. Examples for QML circuits based on different layouts are shown in Figures \ref{fig:MPSLayouts} and \ref{fig:TNLayouts}. The data is mapped to the quantum computer using some feature map $\Phi(x)$ which is shown as orange dots in the images. The feature map may be a tensor network itself (see Section \ref{sec:Encoding}). The weight tensor $W_l$ is represented as the blue quantum tensor network. In the end, a measurement on the remaining qubits yields a result, e.g. a classification. If a multiclass output is needed, introducing an exit node (see Fig.~\ref{fig:EncStrat}~b) will improve the fraction of correct classifications\cite{Dilip.24.04.2022}. 

Generative TNs can be realized by reversing the TN structure. The inputs of the generative network are given by some reference computational basis state which are entangled by the TN (see Fig.~\ref{fig:EncStrat}~a). These generative networks can be trained either by sampling the generative QTN and comparing the results to a given training set\cite{Huggins.2019} or by training a classifier and adjoining every gate as noted in Section~\ref{ssec:cml}.

Some studies already include an investigation of the influence of noise on the QTN circuit. Numerical results indicate, that low level noise is not a problem for classification \cite{Huggins.2019,Grant.2018}. It even may be used to enhance the performance of the algorithm by adding ancilla qubits initialized with noise to the circuit. This effectively generates a probabilistic model which is easier to train. However, if the noise is too high this also leads to decoherence rendering the circuit unfunctional\cite{Liao.02.09.2022}.

Optimizing the parameters of these QTNs relies on some variety of global gradient descent for the majority of literature. Geometric \cite{Wall.2022} or genetic methods \cite{Chen.2022} are used only rarely. Renormalization methods like for classical TNs have not been adapted to QTNs but may be employed in hybrid methods \cite{Lubasch.2020}. Some proposals even consider employing TNs for optimizing parameters or hyperparameters of QML algorithms\cite{Sagingalieva.10.05.2022}.
For specific implementations, first evidence exists that the locality of TNs can overcome barren plateaus \cite{Zhang.12.11.2020, Qi.08.06.2022}. Especially the use of local loss functions, which can be implemented using local Hamiltonians, provides a favourable loss landscape without gradients vanishing exponentially fast\cite{Pesah.2021,Liu.2022}. The same approach may also reduce the amount of training data needed\cite{Araz.21.02.2022}. 

\subsection{Hybrid training}\label{ssec:hybrid}

Hybrid QTN architectures combine quantum and classical elements to use the advantages of both worlds. Compared to NISQ devices, classical computers are able to perform computations on far larger datasets and their use is very cheap. The quantum part of the algorithm may introduce some qualitative quantum advantage like higher maximum performance or generalization of the model. At the moment, two hybrid strategies make use of these characteristics. First, the classical reduction of the input data's dimensionality with pre-processing like PCA\cite{Dborin.2022}, auto-encoders or TN based encodings discussed in Section \ref{sec:Encoding}. If the classical part is trainable, it may be optimized together with the subsequent QTN. Second, the direct maps between TNs and their quantum counterparts allow for classical pre-training of the quantum model's initial values. Even when more powerful quantum computers are available, the execution of quantum circuits will still be expensive and pre-training methods to reduce the number of quantum circuit executions will stay relevant. In this section, we will focus on hybrid pre-training methods. 

As discussed in Section~\ref{ssec:generalSt}, a TN in canonical form can be mapped exactly to a quantum computer. This allows to train a coarse classical TN model which can be refined and expanded after mapping it to a quantum computer. Any standard QTN layout may be prepared with classically prepared initial conditions\cite{Wall.2021,Wall.2021b} and for providing efficient initial values, no post-selection on the quantum computer is required \cite{Wall.2022}. Using these initial values for the QTN's parameters makes the training of larger quantum circuits far more efficient in comparison with random or identity initialization schemes. The main benefit is that the initial training phase, where the gradients decrease exponentially with the qubit number already has been performed classically and therefore the training on the quantum device starts in a favourable spot of the parameter space \cite{Dborin.2022}.

    \begin{figure}
	\centering
	\input{QTNWall}

        \caption[Hybrid training methods for quantum tensor networks.]{Hybrid training methods for quantum tensor networks. A pre-trained classical tensor network provides suitable initial values for further optimization on a quantum computer. The direct approach may be refined to make training easier or to have access to a larger part of the multi-qubit Hilbert space. Method (a)\cite{Dborin.2022} maps a classically optimized MPS to the diagonal gates $U_c$ of a brickwall ansatz; all off-diagonal gates are initialized as identities. Then a second optimization step on the quantum computer is conducted. While the optimized diagonal gates $U_o$ have to be full unitary gates to enable the transfer from the classical network, the off-diagonal gates $W$ can use a simpler ansatz with fewer parameters. Approach (b)\cite{Wall.2022} maps two classical MPS to the quantum circuit. A homogeneous MPS ($N_b$-times $U_G$) truncated to an appropriate boundary condition $U_R$ prepares an initial state. In the second MPS, the nodes $U_D^{(i)}$ upload and process the $N_x$ elements of the datum $x$. Finally, the classification is performed on the exit node $U_C$. Method (a) is shown as a generator, method (b) as an efficient classifier.
        }
	\label{fig:QTNWall}
    \end{figure}

Modifications to the basic procedure of classically pre-training a QTN have been developed to lower the requirements on the classical preparation and to make the quantum part easier to train. For training a brickwall layout (see Fig. \ref{fig:TNLayouts}~a), it is sufficient to prepare an initial MPS state that is embedded within the brickwall, e.g. the diagonal, and the remaining gates start as identity gates\cite{Dborin.2022} as shown in the centre panel of Fig.~\ref{fig:QTNWall}~(a). To make quantum training easier, one does not have to use full unitary gates $U$ on the whole circuit, but can restrict the off-diagonal gates to some simple ansatz $W$ as shown in the right panel of this figure. This approach can be seen as a quantum version of the copy node initialization for classical TNs, where most of the tensor nodes are initialized with identity tensors \cite{Barratt.2022}.

Another modification considers preparing a prior distribution within the feature space before uploading the data. This reduces the bond dimensions and gate complexity needed \cite{Wall.2022}. Fig.~\ref{fig:QTNWall}~(b) shows the approach for an efficient MPS classifier. At the beginning of the circuit, a homogeneous MPS $U_G^{N_b}$ of length $N_b$ prepares the prior distribution. Setting a trainable boundary condition $U_R$ reduces the number of nodes the MPS needs to represent an effective prior. The second part $U_D^{(i)}$ is a standard efficient MPS similar to Fig. \ref{fig:Class2QTN}~(f), where the $N_x$ data features are introduced into the QML and an output node $U_C$ prepares the classification result in the end. The circuit technically can be optimized without classical pre-training. But for higher bond dimensions, this construction is far easier to train having initial values obtained with classical TN-specific methods like DMRG \cite{Wall.2022}.

\subsection{Case Studies and Implementations}\label{ssec:TN4QML}

The application of QTN ML methods has been limited to demonstrative feasibility studies up to now. Most authors focus on classification tasks for image recognition either with binary classes\cite{Araz.21.02.2022} or multiclass setups\cite{Dilip.24.04.2022}. One implementation of binary image classification\cite{Huggins.2019} has been performed on real photonic hardware\cite{Wang.2021}. Other uses are classifications on parameterized classical data\cite{Bhatia.04.05.2019} and on quantum simulation results \cite{Uvarov.2020, Lazzarin.2022}. Besides the proof of concept, these studies demonstrate that QTN approaches already can process relatively high dimensional input data like grayscale images of up to $37^2$ pixels. They show that QTNs can achieve accuracies for the classification of both classical and quantum datasets in the range of $0.85$ to $0.95$ with only a small amount of parameters and internal qubits. 

The application of QTNs for a regression of continuous properties has not been discussed widely yet. One proposition for this application is to approximate eigenvectors of unitary matrices\cite{Kardashin.2021}, but finding the right bond dimension is crucial to find an approximate state having sufficient overlap with the real eigenvector without using huge circuits.

QTN generators have been implemented by various authors to provide quantum state samples from learned distributions\cite{Huggins.2019, Wall.2021b, Dborin.2022} as a feasibility study.

Most case studies that use publicly avaiable frameworks rely on the qiskit \cite{Qiskit}, as it supports resets in the middle of an execution, which are necessary for efficient TNs. For ML, Qiskit is compatible with the pytorch framework. Cirq \cite{CirqDevelopers.2022} also provides a reset functionality and integrates with the tensor flow ML suite. Pennylane\cite{Bergholm.12.11.2018}, which focuses on QML applications, currently cannot implement mid-circuit measurements for efficient QTNs, but provides methods for both basic MPS and TTN based quantum classifiers. The implementation allows for varying virtual qubit bonds and connects the quantum circuits to most common ML frameworks in Python.

\section{Tensor Networks for Data Encoding}\label{sec:Encoding}

For the performance of data driven quantum algorithms and QML algorithms in particular, encoding of data plays a crucial role. Current quantum computing hardware provides neither a sufficient amount of qubits nor gate depth to encode high dimensional data sets in a straightforward fashion. However, this does not necessarily mean that the problem size to be tackled with current quantum algorithms has to be small. Instead, one relies on classical and quantum pre-processing steps that reduce the data to its essential features. 

By adjusting the bond dimension, TNs provide a direct way of compressing data both lossless by discarding dimensions with singular values equal to zero and approximate by setting upper bounds on the bond dimension. The input QTN state can be prepared in at least five different ways. First, by maximizing the overlap between a classical representation of a quantum state and a TN. Second, by encoding classical data in a TN and reducing its bond dimensions by tensor decompositions. In both approaches, the network will then be mapped to quantum circuits as shown in Fig.~\ref{fig:EncStrat}~a for MPS and TTN. Efficient mapping methods are also available for PEPS\cite{Schwarz.2012}. A third classical method is training a TN to compress the data into a latent representation with fixed bond dimension and encode the latent vectors using some direct strategy. On a quantum computer, one can directly maximize the overlap between an existing quantum state and a QTN. This requires preparing the reference quantum state multiple times until convergence which may be very costly. Finally, one can train a generative network to output a state from some distribution (see Section~\ref{ssec:vqc}). Therefore we focus on classical pre-processing in this section. 

	\begin{figure}
	    \centering
	\input{EncStrat}
    \caption[Encoding strategies for machine learning using tensor networks]{Encoding strategies for machine learning using tensor networks. (a) Encoding classical or compressing quantum data using matrix product states (MPS, left) or a tree tensor network (TTN, right). Each gate $U^\dag_i$ is a direct mapping from an isometric node of a classical tensor network. A generative quantum tensor network has the same structure as an input state, but with one or more $|0\rangle$ input qubits replaced by a label or noise encoded input. (b) Data may be encoded in several independent MPS (blue, green) and fed into the circuit to reduce information loss. A quantum machine learning algorithm can make use of the same MPS structure (red) and directly connect to the encoding MPS. An additional output gate (yellow) improves classification accuracy for multi-class tasks. (c) To encode two-dimensional data like images into MPS, one needs to choose a one dimensional path either at the cost of losing parts of the information or introducing high bond dimensions. Cutting the area into patches improves the encoding result as this reduces the maximum distances on the MPS between neighbouring sites in the original data. %\cite{Dilip.24.04.2022}.
    }
	    \label{fig:EncStrat}
	\end{figure}

Encoding data in TN based quantum layouts promises some benefits over classical encoding. Especially, the access to a large state space even when using few qubits could boost efficiency of information storage. For example, the number of parameters needed to represent certain time evolutions of quantum states is exponentially reduced with QTNs compared to classical ones\cite{Lin.2021}.

Depending on the type of data, different layouts of TNs provide the most efficient storage because the scaling of their mutual information has to match the scaling behaviour of entanglement in the TN. As discussed in section~\ref{fig:MPSLayouts}, 
MPS suit 1D-Data like time series and logarithmic TTNs or 2D TNs like MERA or PEPS are better suited for images depending on the amount of local correlation. For text, the information scales even steeper\cite{Lu.11.03.2021}, which requires 3D-PEPS or high dimensional MERA variants which have not been implemented on a quantum computer yet. Exploiting symmetries reduces the need for complexity within the structure, e.g. by using wavelet transform techniques in images \cite{McCord.04.03.2022}.  

Nevertheless, MPS and TTN can be implemented and optimized easily and still provide an improvement over direct encoding methods. They are therefore widely used in encoding for QML. The performance of MPS and TTN can be improved by combining them with other methods. When encoding images, one can split the whole image into patches and encode each patch into an MPS (see Fig.~\ref{fig:EncStrat}~c) which will catch local entanglement better but requires more storage. For a fixed bond dimension, the number of qubits is proportional to the number of patches encoded. The pixels in each patch are addressed by a method that is known as flexible representation of quantum images (FRQI)\cite{Le.2011}. The method was developed as a classical compression method \cite{Latorre.2005} and has recently been transferred to quantum computers\cite{Chen.2018,Ran.2020}. Patchwise MPS encoding can be easily combined with MPS QML methods (see Fig.~\ref{fig:EncStrat}~b)\cite{Dilip.24.04.2022}.

Trainable TN encoding using a latent space representation from the outgoing bond dimensions usually is optimized together with the parameters of the QML circuit \cite{Araz.21.02.2022, Chen.2021}. A theoretical study on the error performance of function regression models finds upper bounds when certain continuity requirements on the loss and the network are met\cite{Qi.08.06.2022}. Particularly, they find that the optimization error connected to barren plateaus will be negligible if the loss on the TN parameters is Lipschitz and satisfies a Polyak-Lojasiewicz condition.
However, they do not develop a method to set up a TN that actually fulfils these conditions. Trainable encoding can be improved by a patchwise approach, too. Applying trainable MPS approximators on small regions of the image yields a linear model of the image where the spatial information is stored in the feature space\cite{Selvan.13.11.2020}. Due to the independence of the various layers, this method also could be realized with a hybrid circuit, where the initial layers are classical and the final layers are quantum.
	
TN encoding pairs well with TN based ML but it is applicable to any other QML approach. For layered VQC approaches, first results imply that TN pre-processing trained together with the VQC classifier performs better than regular PCA on image data\cite{Chen.2021} and can be used as an estimator for the Q-value function of a reinforcement learning ansatz\cite{Chen.2022}.

TN encoding is not only relevant for QML, but can be used to provide states for any other quantum application that requires complex input. For example, overlaps of QTN generated basis functions can be used to approximate non-linear functions\cite{Lubasch.2020}. This approach may reduce the number of grid points needed in quantum simulations with nonlinear PDEs as couplings compared to the classical approach.

\section{Conclusion}

TNs have proven themselves useful for storing and processing quantum states as well as for classical ML applications. Combining both aspects makes them a suitable tool for QML as well. We have seen in Sections \ref{sec:qtn} and \ref{sec:Encoding}, that TNs can be employed for various tasks within the QML pipeline, from pre-processing and encoding to the variational part and the optimizers\cite{Sagingalieva.10.05.2022}. They have a very flexible representation as they allow for both pure quantum algorithms and classical-quantum hybrids while a wide range of optimization methods can be applied. 

Bringing TNs to a quantum computer has advantages considering architecture design. The representation of a quantum state with TNs on a quantum computer reduces the necessary amount of qubits compared to other encoding methods \cite{Barratt.2021}. Thereby TNs provide an efficient way of mapping classical data to quantum applications. The tensors of a QTN do not have to be contracted costly as on classical hardware since the contraction happens as part of the execution of the quantum circuit. When using hybrid approaches, TNs allow for a seamless connection between classical and quantum methods enabling pre-training and gradual tuning of the border between both systems -- which will become important when the power of NISQ devices scales up significantly. Having the possibility of choosing a qubit efficient implementation is also a very important feature although its effects on trainability are not yet fully understood and require further investigation.

In comparison to classical TNs, QTNs are expected to provide several benefits for the algorithms themselves. As quantum algorithms naturally implement entanglement, QTNs will have access to a  Hilbert space that grows exponentially with the number of qubits. This enlarges storage capacity and the available parameter space for QML algorithms. While classical TNs are able to represent only low-entangled, low-complexity states, QTN have access also to low-complexity states that can be generated by Hamiltonian time evolution \cite{Lin.2021} independent of the amount of entanglement. However, this may need very large circuits depths. Additionally, QTNs provide a natural way of using complex numbers instead of real ones which reduces the number of parameters necessary greatly in certain architectures \cite{Glasser.2019}. It is yet unclear this is a general advantage of QTNs.

Regardless of that, QTNs seem to be easier to train than other QML methods. For example, utilizing local optimization routines that make use of the localized TN structure can help to overcome problems like barren plateaus and reduce the amount of training data needed. However, these results have been obtained using specific implementations, some combined with special features like error correction. The results are therefore not generalizable to all QTN layouts yet. Choosing a layout that fits the data structure well also can reduce the need for large general circuits that are hard to train due to their large amount of parameters. 

The mentioned actual and possible benefits come with downsides compared to classical networks. Gates are directed and reshaping the network cannot be performed in a straightforward way. The usual difficulties with quantum computations like encoding classical data and the need to perform non-reversible measurements to obtain a result still apply. Moreover, compared to more general QML methods like layered VQCs, the strict structure of a TN layout may render it an architecture which cannot be applied on general problems but has to be handcrafted each time. Therefore it is unclear at the moment, whether the benefits of TNs really can be translated to a relevant quantum advantage outside the lab.

Although QTNs have the potential to be a successful framework for QML, their development has just begun and further research is needed in many directions. Modifications to the basic layouts like variable bond dimensions which can be used to reduce computational costs have not been adapted to QTNs yet. In particular, a quantum version of TN-specific local optimization methods is interesting for building algorithms that can be trained more easily. 

However, most important is a more fundamental insight into the capabilities of QTNs -- especially in comparison with classical or other VQC based methods. This includes methods to assess ML performance theoretically on the layout level and not just for specific implementations. Having general measures e.g. for expressivity or trainability would enable us to identify the range of application where it makes sense to use QTN architectures and concentrate future development on these areas.

\bibliography{main}

\section*{Acknowledgements}
The authors thank 
Bogusz Bujnowski, 
Lautaro Hickmann, 
Markus Lange 
and Pia Siegl for our discussions on classical TNs, QTNs and ML and their very helpful remarks on this review.

\section*{Author contributions}
The main contribution was done by Hans-Martin Rieser. 

\section*{Competing interests}
The authors declare no competing interests.

\end{document}