% This file was created with Citavi 6.14.0.0

@misc{Milsted.31.05.2018,
 abstract = {Tensor networks are often used to accurately represent ground states of quantum spin chains. Two popular choices of such tensor network representations can be seen to implement linear maps that correspond, respectively, to euclidean time evolution and to global scale transformations. In this paper, by exploiting the local structure of the tensor networks, we explain how to also implement local or non-uniform versions of both euclidean time evolution and scale transformations. We demonstrate our proposal with a critical quantum spin chain on a finite circle, where the low energy physics is described by a conformal field theory (CFT), and where non-uniform euclidean time evolution and local scale transformations are conformal transformations acting on the Hilbert space of the CFT. We numerically show, for the critical quantum Ising chain, that the proposed tensor networks indeed transform the low energy states of the periodic spin chain in the same way as the corresponding conformal transformations do in the CFT.},
 author = {Milsted, Ashley and Vidal, Guifre},
 date = {31.05.2018},
 title = {Tensor networks as conformal transformations},
 url = {http://arxiv.org/pdf/1805.12524v1},
 keywords = {High Energy Physics - Lattice;Physics - Strongly Correlated Electrons},
 file = {}
}

@article{Orus.2019,
 abstract = {Nature Reviews Physics, doi:10.1038/s42254-019-0086-7        },
 author = {Or{\'u}s, Rom{\'a}n},
 year = {2019},
 title = {Tensor networks for complex quantum systems},
 pages = {538--550},
 volume = {1},
 number = {9},
 journal = {Nature Reviews Physics},
 doi = {10.1038/s42254-019-0086-7},
 file = {}
}


@book{Moitra.2018,
 year = {2018},
 title = {Algorithmic Aspects of Machine Learning},
 publisher = {{Cambridge University Press}},
 isbn = {9781316882177},
 editor = {Moitra, Ankur},
 doi = {10.1017/9781316882177}
}


@article{Mugel.2022,
 abstract = {In this paper we tackle the problem of dynamic portfolio optimization, i.e., determining the optimal trading trajectory for an investment portfolio of assets over a period of time, taking into account transaction costs and other possible constraints. This problem is central to quantitative finance. After a detailed introduction to the problem, we implement a number of quantum and quantum-inspired algorithms on different hardware platforms to solve its discrete formulation using real data from daily prices over 8 years of 52 assets, and do a detailed comparison of the obtained Sharpe ratios, profits and computing times. In particular, we implement classical solvers (Gekko, exhaustive), D-Wave Hybrid quantum annealing, two different approaches based on Variational Quantum Eigensolvers on IBM-Q (one of them brand-new and tailored to the problem), and for the first time in this context also a quantum-inspired optimizer based on Tensor Networks. In order to fit the data into each specific hardware platform, we also consider doing a preprocessing based on clustering of assets. From our comparison, we conclude that D-Wave Hybrid and Tensor Networks are able to handle the largest systems, where we do calculations up to 1272 fully-connected qubits for demonstrative purposes. Finally, we also discuss how to mathematically implement other possible real-life constraints, as well as several ideas to further improve the performance of the studied methods.},
 author = {Mugel, Samuel and Kuchkovsky, Carlos and Sanchez, Escolastico and Fernandez-Lorenzo, Samuel and Luis-Hita, Jorge and Lizaso, Enrique and Orus, Roman},
 year = {2022},
 title = {Dynamic Portfolio Optimization with Real Datasets Using Quantum  Processors and Quantum-Inspired Tensor Networks},
 url = {http://arxiv.org/pdf/2007.00017v2},
 keywords = {Computer Science - Computational Engineering Finance and Science;Quantum Physics},
 pages = {77},
 volume = {4},
 number = {1},
 issn = {2643-1564},
 journal = {Physical Review Research},
 doi = {10.1103/PhysRevResearch.4.013006},
 file = {}
}


@article{Murg.2015,
 abstract = {We study the tree-tensor-network-state (TTNS) method with variable tensor orders for quantum chemistry. TTNS is a variational method to efficiently approximate complete active space (CAS) configuration interaction (CI) wave functions in a tensor product form. TTNS can be considered as a higher order generalization of the matrix product state (MPS) method. The MPS wave function is formulated as products of matrices in a multiparticle basis spanning a truncated Hilbert space of the original CAS-CI problem. These matrices belong to active orbitals organized in a one-dimensional array, while tensors in TTNS are defined upon a tree-like arrangement of the same orbitals. The tree-structure is advantageous since the distance between two arbitrary orbitals in the tree scales only logarithmically with the number of orbitals N, whereas the scaling is linear in the MPS array. It is found to be beneficial from the computational costs point of view to keep strongly correlated orbitals in close vicinity in both arrangements; therefore, the TTNS ansatz is better suited for multireference problems with numerous highly correlated orbitals. To exploit the advantages of TTNS a novel algorithm is designed to optimize the tree tensor network topology based on quantum information theory and entanglement. The superior performance of the TTNS method is illustrated on the ionic-neutral avoided crossing of LiF. It is also shown that the avoided crossing of LiF can be localized using only ground state properties, namely one-orbital entanglement.},
 author = {Murg, V. and Verstraete, F. and Schneider, R. and Nagy, P. R. and Legeza, {\"O}.},
 year = {2015},
 title = {Tree Tensor Network State with Variable Tensor Order: An Efficient Multireference Method for Strongly Correlated Systems},
 pages = {1027--1036},
 volume = {11},
 number = {3},
 journal = {Journal of chemical theory and computation},
 doi = {10.1021/ct501187j},
 file = {}
}


@misc{Nguyen.21.04.2021,
 abstract = {The numerical simulation of quantum circuits is an indispensable tool for development, verification and validation of hybrid quantum-classical algorithms on near-term quantum co-processors. The emergence of exascale high-performance computing (HPC) platforms presents new opportunities for pushing the boundaries of quantum circuit simulation. We present a modernized version of the Tensor Network Quantum Virtual Machine (TNQVM) which serves as a quantum circuit simulation backend in the eXtreme-scale ACCelerator (XACC) framework. The new version is based on the general purpose, scalable tensor network processing library, ExaTN, and provides multiple configurable quantum circuit simulators enabling either exact quantum circuit simulation via the full tensor network contraction, or approximate quantum state representations via suitable tensor factorizations. Upon necessity, stochastic noise modeling from real quantum processors is incorporated into the simulations by modeling quantum channels with Kraus tensors. By combining the portable XACC quantum programming frontend and the scalable ExaTN numerical backend we introduce an end-to-end virtual quantum development environment which can scale from laptops to future exascale platforms. We report initial benchmarks of our framework which include a demonstration of the distributed execution, incorporation of quantum decoherence models, and simulation of the random quantum circuits used for the certification of quantum supremacy on the Google Sycamore superconducting architecture.},
 author = {Nguyen, Thien and Lyakh, Dmitry and Dumitrescu, Eugene and Clark, David and Larkin, Jeff and McCaskey, Alexander},
 date = {21.04.2021},
 title = {Tensor Network Quantum Virtual Machine for Simulating Quantum Circuits  at Exascale},
 url = {http://arxiv.org/pdf/2104.10523v1},
 keywords = {Quantum Physics},
 file = {}
}


@inproceedings{Nichting.2021,
 author = {Nichting, Matthias and Lobig, Thomas and Koster, Frank},
 title = {Case Study on Gap Selection for Automated Vehicles Based on Deep Q-Learning},
 pages = {252--257},
 publisher = {IEEE},
 isbn = {978-1-6654-2404-2},
 booktitle = {2021 International Conference on Artificial Intelligence and Computer Science Technology (ICAICST)},
 year = {2021},
 doi = {10.1109/ICAICST53116.2021.9497818},
 file = {}
}


@misc{Milsted.03.05.2019,
 abstract = {TensorNetwork is an open source library for implementing tensor network algorithms in TensorFlow. We describe a tree tensor network (TTN) algorithm for approximating the ground state of either a periodic quantum spin chain (1D) or a lattice model on a thin torus (2D), and implement the algorithm using TensorNetwork. We use a standard energy minimization procedure over a TTN ansatz with bond dimension {\$}$\backslash$chi{\$}, with a computational cost that scales as {\$}O($\backslash$chi{\^{}}4){\$}. Using bond dimension {\$}$\backslash$chi $\backslash$in [32,256]{\$} we compare the use of CPUs with GPUs and observe significant computational speed-ups, up to a factor of $100$, using a GPU and the TensorNetwork library.},
 author = {Milsted, Ashley and Ganahl, Martin and Leichenauer, Stefan and Hidary, Jack and Vidal, Guifre},
 date = {03.05.2019},
 title = {TensorNetwork on TensorFlow: A Spin Chain Application Using Tree Tensor  Networks},
 url = {http://arxiv.org/pdf/1905.01331v1},
 keywords = {Computer Science - Learning;High Energy Physics - Theory;Physics - Computational Physics;Physics - Strongly Correlated Electrons;Statistics - Machine Learning},
 file = {}
}


@article{Nishino.2000,
 author = {Nishino, T. and Okunishi, K. and Hieida, Y. and Maeshima, N. and Akutsu, Y.},
 year = {2000},
 title = {Self-consistent tensor product variational approximation for 3D classical models},
 pages = {504--512},
 volume = {575},
 number = {3},
 issn = {05503213},
 journal = {Nuclear Physics B},
 doi = {10.1016/S0550-3213(00)00133-4},
 file = {}
}


@article{Orus.2008,
 author = {Or{\'u}s, R. and Vidal, G.},
 year = {2008},
 title = {Infinite time-evolving block decimation algorithm beyond unitary evolution},
 volume = {78},
 number = {15},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.78.155117},
 file = {}
}


@article{Orus.2014,
 abstract = {Annals of Physics, 349 (2014) 117-158. doi:10.1016/j       .aop.2014.06.013  },
 author = {Or{\'u}s, Rom{\'a}n},
 year = {2014},
 title = {A practical introduction to tensor networks: Matrix product states and projected entangled pair states},
 keywords = {Entanglement;MPS;PEPS;tensor networks},
 pages = {117--158},
 volume = {349},
 issn = {00034916},
 journal = {Annals of Physics},
 doi = {10.1016/j.aop.2014.06.013},
 file = {}
}


@article{Oseledets.2010,
 abstract = {Linear Algebra and Its Applications, 432 (2010) 70-88. doi:10.1016/j     .laa.2009.07.024  },
 author = {Oseledets, Ivan and Tyrtyshnikov, Eugene},
 year = {2010},
 title = {TT-cross approximation for multidimensional arrays},
 keywords = {Cross approximation;Curse of dimensionality;Interpolation;Low-rank matrices;Multidimensional integration;Multi-way arrays;Singular value decomposition;Tensor decompositions;Tensor trains;TT decomposition},
 pages = {70--88},
 volume = {432},
 number = {1},
 issn = {00243795},
 journal = {Linear Algebra and its Applications},
 doi = {10.1016/j.laa.2009.07.024},
 file = {}
}


@misc{Pednault.16.10.2017,
 abstract = {With the current rate of progress in quantum computing technologies, systems with more than 50 qubits will soon become reality. Computing ideal quantum state amplitudes for circuits of such and larger sizes is a fundamental step to assess both the correctness, performance, and scaling behavior of quantum algorithms and the fidelities of quantum devices. However, resource requirements for such calculations on classical computers grow exponentially. We show that deferring tensor contractions can extend the boundaries of what can be computed on classical systems. To demonstrate this technique, we present results obtained from a calculation of the complete set of output amplitudes of a universal random circuit with depth 27 in a 2D lattice of {\$}7 $\backslash $times 7{\$} qubits, and an arbitrarily selected slice of {\$}2{\^{}}{37}{\$} amplitudes of a universal random circuit with depth 23 in a 2D lattice of {\$}8 $\backslash $times 7{\$} qubits. Combining our methodology with other decomposition approaches found in the literature, we show that we can simulate {\$}7 $\backslash$times 7{\$}-qubit random circuits to arbitrary depth by leveraging secondary storage. These calculations were thought to be impossible due to resource requirements.},
 author = {Pednault, Edwin and Gunnels, John A. and Nannicini, Giacomo and Horesh, Lior and Magerlein, Thomas and Solomonik, Edgar and Draeger, Erik W. and Holland, Eric T. and Wisnieff, Robert},
 date = {16.10.2017},
 title = {Pareto-Efficient Quantum Circuit Simulation Using Tensor Contraction  Deferral},
 url = {http://arxiv.org/pdf/1710.05867v4},
 keywords = {Quantum Physics},
 file = {}
}


@article{PerezGarcia.2007,
 author = {Perez-Garcia, D. and Verstraete, F. and Wolf, M. M. and Cirac, J. I.},
 year = {2007},
 title = {Matrix product state representations},
 pages = {401--430},
 volume = {7},
 number = {5{\&}6},
 issn = {15337146},
 journal = {Quantum Information and Computation},
 doi = {10.26421/QIC7.5-6-1},
 file = {}
}


@misc{Perrier.15.08.2021,
 abstract = {The availability of large-scale datasets on which to train, benchmark and test algorithms has been central to the rapid development of machine learning as a discipline and its maturity as a research discipline. Despite considerable advancements in recent years, the field of quantum machine learning (QML) has thus far lacked a set of comprehensive large-scale datasets upon which to benchmark the development of algorithms for use in applied and theoretical quantum settings. In this paper, we introduce such a dataset, the QDataSet, a quantum dataset designed specifically to facilitate the training and development of QML algorithms. The QDataSet comprises 52 high-quality publicly available datasets derived from simulations of one- and two-qubit systems evolving in the presence and/or absence of noise. The datasets are structured to provide a wealth of information to enable machine learning practitioners to use the QDataSet to solve problems in applied quantum computation, such as quantum control, quantum spectroscopy and tomography. Accompanying the datasets on the associated GitHub repository are a set of workbooks demonstrating the use of the QDataSet in a range of optimisation contexts.},
 author = {Perrier, Elija and Youssry, Akram and Ferrie, Chris},
 date = {15.08.2021},
 title = {QDataset: Quantum Datasets for Machine Learning},
 url = {http://arxiv.org/pdf/2108.06661v1},
 keywords = {Quantum Physics},
 file = {}
}


@misc{Novikov.12.05.2016,
 abstract = {Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2{\^{}}160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.},
 author = {Novikov, Alexander and Trofimov, Mikhail and Oseledets, Ivan},
 date = {12.05.2016},
 title = {Exponential Machines},
 url = {http://arxiv.org/pdf/1605.03795v3},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 file = {}
}


@misc{Meirom.18.04.2022,
 abstract = {Quantum Computing (QC) stands to revolutionize computing, but is currently still limited. To develop and test quantum algorithms today, quantum circuits are often simulated on classical computers. Simulating a complex quantum circuit requires computing the contraction of a large network of tensors. The order (path) of contraction can have a drastic effect on the computing cost, but finding an efficient order is a challenging combinatorial optimization problem.  We propose a Reinforcement Learning (RL) approach combined with Graph Neural Networks (GNN) to address the contraction ordering problem. The problem is extremely challenging due to the huge search space, the heavy-tailed reward distribution, and the challenging credit assignment. We show how a carefully implemented RL-agent that uses a GNN as the basic policy construct can address these challenges and obtain significant improvements over state-of-the-art techniques in three varieties of circuits, including the largest scale networks used in contemporary QC.},
 author = {Meirom, Eli A. and Maron, Haggai and Mannor, Shie and Chechik, Gal},
 date = {18.04.2022},
 title = {Optimizing Tensor Network Contraction Using Reinforcement Learning},
 url = {http://arxiv.org/pdf/2204.09052v1},
 keywords = {Computer Science - Learning;Quantum Physics},
 file = {}
}


@misc{McCord.04.03.2022,
 abstract = {We benchmark the efficacy of several novel orthogonal, symmetric, dilation-3 wavelets, derived from a unitary circuit based construction, towards image compression. The performance of these wavelets is compared across several photo databases against the CDF-9/7 wavelets in terms of the minimum number of non-zero wavelet coefficients needed to obtain a specified image quality, as measured by the multi-scale structural similarity index (MS-SSIM). The new wavelets are found to consistently offer better compression efficiency than the CDF-9/7 wavelets across a broad range of image resolutions and quality requirements, averaging 7-8{\%} improved compression efficiency on high-resolution photo images when high-quality (MS-SSIM = 0.99) is required.},
 author = {McCord, James C. and Evenbly, Glen},
 date = {04.03.2022},
 title = {Improved Wavelets for Image Compression from Unitary Circuits},
 url = {http://arxiv.org/pdf/2203.02556v1},
 keywords = {Quantum Physics},
 file = {}
}


@article{McCaskey.2018,
 abstract = {The exploration of hybrid quantum-classical algorithms and programming models on noisy near-term quantum hardware has begun. As hybrid programs scale towards classical intractability, validation and benchmarking are critical to understanding the utility of the hybrid computational model. In this paper, we demonstrate a newly developed quantum circuit simulator based on tensor network theory that enables intermediate-scale verification and validation of hybrid quantum-classical computing frameworks and programming models. We present our tensor-network quantum virtual machine (TNQVM) simulator which stores a multi-qubit wavefunction in a compressed (factorized) form as a matrix product state, thus enabling single-node simulations of larger qubit registers, as compared to brute-force state-vector simulators. Our simulator is designed to be extensible in both the tensor network form and the classical hardware used to run the simulation (multicore, GPU, distributed). The extensibility of the TNQVM simulator with respect to the simulation hardware type is achieved via a pluggable interface for different numerical backends (e.g., ITensor and ExaTENSOR numerical libraries). We demonstrate the utility of our TNQVM quantum circuit simulator through the verification of randomized quantum circuits and the variational quantum eigensolver algorithm, both expressed within the eXtreme-scale ACCelerator (XACC) programming model.},
 author = {McCaskey, Alexander and Dumitrescu, Eugene and Chen, Mengsu and Lyakh, Dmitry and Humble, Travis},
 year = {2018},
 title = {Validating quantum-classical programming models with tensor network simulations},
 keywords = {Models, Theoretical;Neural Networks, Computer;Programming Languages},
 pages = {e0206704},
 volume = {13},
 number = {12},
 journal = {PloS one},
 doi = {10.1371/journal.pone.0206704},
 file = {}
}


@article{Lazzarin.2022,
 abstract = {Hybrid quantum-classical algorithms based on variational circuits are a promising approach to quantum machine learning problems for near-term devices, but the selection of the variational ansatz is an open issue. Recently, tensor network-inspired circuits have been proposed as a natural choice for such ansatz. Their employment on binary classification tasks provided encouraging results. However, their effectiveness on more difficult tasks is still unknown. Here, we present numerical experiments on multi-class classifiers based on tree tensor network and multiscale entanglement renormalization ansatz circuits. We conducted experiments on image classification with the MNIST dataset and on quantum phase recognition with the XXZ model by Cirq and TensorFlow Quantum. In the former case, we reduced the number of classes to four to match the aimed output based on 2 qubits. The quantum data of the XXZ model consist of three classes of ground states prepared by a checkerboard circuit used for the ansatz of the variational quantum eigensolver, corresponding to three distinct quantum phases. Test accuracy turned out to be 59{\%}-93{\%} and 82{\%}-96{\%} respectively, depending on the model architecture and on the type of preprocessing.},
 author = {Lazzarin, Marco and Galli, Davide Emilio and Prati, Enrico},
 year = {2022},
 title = {Multi-class quantum classifiers with tensor network circuits for quantum  phase recognition},
 url = {http://arxiv.org/pdf/2110.08386v1},
 keywords = {Quantum Physics},
 pages = {128056},
 volume = {434},
 number = {1},
 issn = {03759601},
 journal = {Physics Letters A},
 doi = {10.1016/j.physleta.2022.128056},
 file = {}
}


@article{Leong.2022,
 abstract = {Variational quantum algorithms offer a promising new paradigm for solving partial differential equations on near-term quantum computers. Here, we propose a variational quantum algorithm for solving a general evolution equation through implicit time-stepping of the Laplacian operator. The use of encoded source states informed by preceding solution vectors results in faster convergence compared to random re-initialization. Through statevector simulations of the heat equation, we demonstrate how the time complexity of our algorithm scales with the Ansatz volume for gradient estimation and how the time-to-solution scales with the diffusion parameter. Our proposed algorithm extends economically to higher-order time-stepping schemes, such as the Crank-Nicolson method. We present a semi-implicit scheme for solving systems of evolution equations with non-linear terms, such as the reaction-diffusion and the incompressible Navier-Stokes equations, and demonstrate its validity by proof-of-concept results.},
 author = {Leong, Fong Yew and Ewe, Wei-Bin and Koh, Dax Enshan},
 year = {2022},
 title = {Variational quantum evolution equation solver},
 pages = {10817},
 volume = {12},
 number = {1},
 journal = {Scientific reports},
 doi = {10.1038/s41598-022-14906-3},
 file = {}
}


@article{Levine.2019,
 abstract = {Modern deep learning has enabled unprecedented achievements in various domains. Nonetheless, employment of machine learning for wave function representations is focused on more traditional architectures such as restricted Boltzmann machines (RBMs) and fully-connected neural networks. In this letter, we establish that contemporary deep learning architectures, in the form of deep convolutional and recurrent networks, can efficiently represent highly entangled quantum systems. By constructing Tensor Network equivalents of these architectures, we identify an inherent reuse of information in the network operation as a key trait which distinguishes them from standard Tensor Network based representations, and which enhances their entanglement capacity. Our results show that such architectures can support volume-law entanglement scaling, polynomially more efficiently than presently employed RBMs. Thus, beyond a quantification of the entanglement capacity of leading deep learning architectures, our analysis formally motivates a shift of trending neural-network based wave function representations closer to the state-of-the-art in machine learning.},
 author = {Levine, Yoav and Sharir, Or and Cohen, Nadav and Shashua, Amnon},
 year = {2019},
 title = {Quantum Entanglement in Deep Learning Architectures},
 url = {http://arxiv.org/pdf/1803.09780v3},
 keywords = {Computer Science - Learning;Quantum Physics},
 pages = {401},
 volume = {122},
 number = {6},
 issn = {0031-9007},
 journal = {Physical Review Letters},
 doi = {10.1103/PhysRevLett.122.065301},
 file = {}
}


@incollection{Levine.2023,
 author = {Levine, Yoav and Sharir, Or and Cohen, Nadav and Shashua, Amnon},
 title = {Bridging Many-Body Quantum Physics and Deep Learning via Tensor Networks},
 pages = {439--474},
 publisher = {{Cambridge University Press}},
 isbn = {9781009025096},
 editor = {Grohs, Philipp and Kutyniok, Gitta},
 booktitle = {Mathematical aspects of deep learning},
 year = {2023},
 address = {Cambridge},
 doi = {10.1017/9781009025096.012}
}


@misc{Levine.05.04.2017,
 abstract = {Deep convolutional networks have witnessed unprecedented success in various machine learning applications. Formal understanding on what makes these networks so successful is gradually unfolding, but for the most part there are still significant mysteries to unravel. The inductive bias, which reflects prior knowledge embedded in the network architecture, is one of them. In this work, we establish a fundamental connection between the fields of quantum physics and deep learning. We use this connection for asserting novel theoretical observations regarding the role that the number of channels in each layer of the convolutional network fulfills in the overall inductive bias. Specifically, we show an equivalence between the function realized by a deep convolutional arithmetic circuit (ConvAC) and a quantum many-body wave function, which relies on their common underlying tensorial structure. This facilitates the use of quantum entanglement measures as well-defined quantifiers of a deep network's expressive ability to model intricate correlation structures of its inputs. Most importantly, the construction of a deep ConvAC in terms of a Tensor Network is made available. This description enables us to carry a graph-theoretic analysis of a convolutional network, with which we demonstrate a direct control over the inductive bias of the deep network via its channel numbers, that are related to the min-cut in the underlying graph. This result is relevant to any practitioner designing a network for a specific task. We theoretically analyze ConvACs, and empirically validate our findings on more common ConvNets which involve ReLU activations and max pooling. Beyond the results described above, the description of a deep convolutional network in well-defined graph-theoretic tools and the formal connection to quantum entanglement, are two interdisciplinary bridges that are brought forth by this work.},
 author = {Levine, Yoav and Yakira, David and Cohen, Nadav and Shashua, Amnon},
 date = {05.04.2017},
 title = {Deep Learning and Quantum Entanglement: Fundamental Connections with  Implications to Network Design},
 url = {http://arxiv.org/pdf/1704.01552v2},
 keywords = {Computer Science - Learning;Computer Science - Neural and Evolutionary Computing;Quantum Physics},
 file = {}
}


@article{Li.2021,
 abstract = {Restricted Boltzmann machines (RBM) and deep Boltzmann machines (DBM) are important models in machine learning, and recently found numerous applications in quantum many-body physics. We show that there are fundamental connections between them and tensor networks. In particular, we demonstrate that any RBM and DBM can be exactly represented as a two-dimensional tensor network. This representation gives an understanding of the expressive power of RBM and DBM using entanglement structures of the tensor networks, also provides an efficient tensor network contraction algorithm for the computing partition function of RBM and DBM. Using numerical experiments, we demonstrate that the proposed algorithm is much more accurate than the state-of-the-art machine learning methods in estimating the partition function of restricted Boltzmann machines and deep Boltzmann machines, and have potential applications in training deep Boltzmann machines for general machine learning tasks.},
 author = {Li, Sujie and Pan, Feng and Zhou, Pengfei and Zhang, Pan},
 year = {2021},
 title = {Boltzmann machines as two-dimensional tensor networks},
 url = {http://arxiv.org/pdf/2105.04130v1},
 keywords = {Computer Science - Learning;Physics - Computational Physics;Physics - Statistical Mechanics;Quantum Physics;Statistics - Machine Learning},
 pages = {21},
 volume = {104},
 number = {7},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.104.075154},
 file = {}
}


@misc{Liao.02.09.2022,
 abstract = {Tensor network quantum machine learning (QML) models are promising applications on near-term quantum hardware. While decoherence of qubits is expected to decrease the performance of QML models, it is unclear to what extent the diminished performance can be compensated for by adding ancillas to the models and accordingly increasing the virtual bond dimension of the models. We investigate here the competition between decoherence and adding ancillas on the classification performance of two models, with an analysis of the decoherence effect from the perspective of regression. We present numerical evidence that the fully-decohered unitary tree tensor network (TTN) with two ancillas performs at least as well as the non-decohered unitary TTN, suggesting that it is beneficial to add at least two ancillas to the unitary TTN regardless of the amount of decoherence may be consequently introduced.},
 author = {Liao, Haoran and Convy, Ian and Yang, Zhibo and Whaley, K. Birgitta},
 date = {02.09.2022},
 title = {Decohering Tensor Network Quantum Machine Learning Models},
 url = {http://arxiv.org/pdf/2209.01195v1},
 keywords = {Quantum Physics},
 file = {}
}


@article{Lin.2021,
 abstract = {doi:10.1103/PRXQuantum.2.010342  url:https://doi.org/10.1103/PRXQuantum.2.010342}, 
 author = {Lin, Sheng-Hsuan and Dilip, Rohit and Green, Andrew G. and Smith, Adam and Pollmann, Frank},
 year = {2021},
 title = {Real- and Imaginary-Time Evolution with Compressed Quantum Circuits},
 keywords = {doi:10.1103/PRXQuantum.2.010342  url:https://doi.org/10.1103/PRXQuantum.2.010342}, 
 volume = {2},
 number = {1},
 journal = {PRX Quantum},
 doi = {10.1103/PRXQuantum.2.010342},
 file = {}
}


@article{Liu.2019,
 abstract = {New Journal of Physics, 21(2019) 073059. doi:10.1088/1367-2630/ab31ef}, 
 author = {Liu, Ding and Ran, Shi-Ju and Wittek, Peter and Peng, Cheng and Garc{\'i}a, Raul Bl{\'a}zquez and Su, Gang and Lewenstein, Maciej},
 year = {2019},
 title = {Machine learning by unitary tensor network of hierarchical tree structure},
 keywords = {quantum machine learning;quantum many-body;tensor networks},
 pages = {073059},
 volume = {21},
 number = {7},
 journal = {New Journal of Physics},
 doi = {10.1088/1367-2630/ab31ef},
 file = {}
}


@misc{Liu.15.05.2020,
 abstract = {Tensor networks (TN) have found a wide use in machine learning, and in particular, TN and deep learning bear striking similarities. In this work, we propose the quantum-classical hybrid tensor networks (HTN) which combine tensor networks with classical neural networks in a uniform deep learning framework to overcome the limitations of regular tensor networks in machine learning. We first analyze the limitations of regular tensor networks in the applications of machine learning involving the representation power and architecture scalability. We conclude that in fact the regular tensor networks are not competent to be the basic building blocks of deep learning. Then, we discuss the performance of HTN which overcome all the deficiency of regular tensor networks for machine learning. In this sense, we are able to train HTN in the deep learning way which is the standard combination of algorithms such as Back Propagation and Stochastic Gradient Descent. We finally provide two applicable cases to show the potential applications of HTN, including quantum states classification and quantum-classical autoencoder. These cases also demonstrate the great potentiality to design various HTN in deep learning way.},
 author = {Liu, Ding and Yao, Zekun and Zhang, Quan},
 date = {15.05.2020},
 title = {Quantum-Classical Machine learning by Hybrid Tensor Networks},
 url = {http://arxiv.org/pdf/2005.09428v1},
 keywords = {Computer Science - Learning;Quantum Physics;Statistics - Machine Learning},
 file = {}
}


@article{Liu.2021,
 author = {Liu, Yuhan and Li, Wen-Jun and Zhang, Xiao and Lewenstein, Maciej and Su, Gang and Ran, Shi-Ju},
 year = {2021},
 title = {Entanglement-Based Feature Extraction by Tensor Network Machine Learning},
 volume = {7},
 issn = {2297-4687},
 journal = {Frontiers in Applied Mathematics and Statistics},
 doi = {10.3389/fams.2021.716044},
 file = {}
}


@article{Liu.2022,
 abstract = {Tensor networks are efficient representations of high-dimensional tensors with widespread applications in quantum many-body physics. Recently, they have been adapted to the field of machine learning, giving rise to an emergent research frontier that has attracted considerable attention. Here, we study the trainability of tensor-network based machine learning models by exploring the landscapes of different loss functions, with a focus on the matrix product states (also called tensor trains) architecture. In particular, we rigorously prove that barren plateaus (i.e., exponentially vanishing gradients) prevail in the training process of the machine learning algorithms with global loss functions. Whereas, for local loss functions the gradients with respect to variational parameters near the local observables do not vanish as the system size increases. Therefore, the barren plateaus are absent in this case and the corresponding models could be efficiently trainable. Our results reveal a crucial aspect of tensor-network based machine learning in a rigorous fashion, which provide a valuable guide for both practical applications and theoretical studies in the future.},
 author = {Liu, Zidu and Yu, Li-Wei and Duan, L. -M and Deng, Dong-Ling},
 year = {2022},
 title = {The Presence and Absence of Barren Plateaus in Tensor-network Based  Machine Learning},
 url = {http://arxiv.org/pdf/2108.08312v1},
 keywords = {Physics - Disordered Systems and Neural Networks;Quantum Physics},
 pages = {177},
 volume = {129},
 number = {27},
 issn = {0031-9007},
 journal = {Physical Review Letters},
 doi = {10.1103/PhysRevLett.129.270501},
 file = {}
}


@misc{Lu.11.03.2021,
 abstract = {We investigate the potential of tensor network based machine learning methods to scale to large image and text data sets. For that, we study how the mutual information between a subregion and its complement scales with the subsystem size $L$, similarly to how it is done in quantum many-body physics. We find that for text, the mutual information scales as a power law {\$}L{\^{}}$\backslash$nu{\$} with a close to volume law exponent, indicating that text cannot be efficiently described by 1D tensor networks. For images, the scaling is close to an area law, hinting at 2D tensor networks such as PEPS could have an adequate expressibility. For the numerical analysis, we introduce a mutual information estimator based on autoregressive networks, and we also use convolutional neural networks in a neural estimator method.},
 author = {Lu, Sirui and Kan{\'a}sz-Nagy, M{\'a}rton and Kukuljan, Ivan and Cirac, J. Ignacio},
 date = {11.03.2021},
 title = {Tensor networks and efficient descriptions of classical data},
 url = {http://arxiv.org/pdf/2103.06872v1},
 keywords = {Computer Science - Learning;Physics - Strongly Correlated Electrons;Quantum Physics;Statistics - Machine Learning},
 file = {}
}


@article{Lu.2021,
 abstract = {Neuroevolution, a field that draws inspiration from the evolution of brains in nature, harnesses evolutionary algorithms to construct artificial neural networks. It bears a number of intriguing capabilities that are typically inaccessible to gradient-based approaches, including optimizing neural-network architectures, hyperparameters, and even learning the training rules. In this paper, we introduce a quantum neuroevolution algorithm that autonomously finds near-optimal quantum neural networks for different machine-learning tasks. In particular, we establish a one-to-one mapping between quantum circuits and directed graphs, and reduce the problem of finding the appropriate gate sequences to a task of searching suitable paths in the corresponding graph as a Markovian process. We benchmark the effectiveness of the introduced algorithm through concrete examples including classifications of real-life images and symmetry-protected topological states. Our results showcase the vast potential of neuroevolution algorithms in quantum architecture search, which would boost the exploration towards quantum-learning advantage with noisy intermediate-scale quantum devices.},
 author = {Lu, Zhide and Shen, Pei-Xin and Deng, Dong-Ling},
 year = {2021},
 title = {Markovian Quantum Neuroevolution for Machine Learning},
 url = {http://arxiv.org/pdf/2012.15131v2},
 keywords = {Physics - Disordered Systems and Neural Networks;Quantum Physics},
 volume = {16},
 number = {4},
 issn = {2331-7019},
 journal = {Physical Review Applied},
 doi = {10.1103/PhysRevApplied.16.044039},
 file = {}
}


@article{Lubasch.2020,
 abstract = {We show that nonlinear problems including nonlinear partial differential equations can be efficiently solved by variational quantum computing. We achieve this by utilizing multiple copies of variational quantum states to treat nonlinearities efficiently and by introducing tensor networks as a programming paradigm. The key concepts of the algorithm are demonstrated for the nonlinear Schr\{\textquotedbl}{o}dinger equation as a canonical example. We numerically show that the variational quantum ansatz can be exponentially more efficient than matrix product states and present experimental proof-of-principle results obtained on an IBM Q device.},
 author = {Lubasch, Michael and Joo, Jaewoo and Moinier, Pierre and Kiffner, Martin and Jaksch, Dieter},
 year = {2020},
 title = {Variational quantum algorithms for nonlinear problems},
 url = {http://arxiv.org/pdf/1907.09032v3},
 keywords = {Quantum Physics},
 pages = {451},
 volume = {101},
 number = {1},
 issn = {2469-9926},
 journal = {Physical Review A},
 doi = {10.1103/PhysRevA.101.010301},
 file = {}
}


@article{Luchnikov.2021,
 abstract = {Optimization with constraints is a typical problem in quantum physics and quantum information science that becomes especially challenging for high-dimensional systems and complex architectures like tensor networks. Here we use ideas of Riemannian geometry to perform optimization on manifolds of unitary and isometric matrices as well as the cone of positive-definite matrices. Combining this approach with the up-to-date computational methods of automatic differentiation, we demonstrate the efficacy of the Riemannian optimization in the study of the low-energy spectrum and eigenstates of multipartite Hamiltonians, variational search of a tensor network in the form of the multiscale entanglement-renormalization ansatz, preparation of arbitrary states (including highly entangled ones) in the circuit implementation of quantum computation, decomposition of quantum gates, and tomography of quantum states. Universality of the developed approach together with the provided open source software enable one to apply the Riemannian optimization to complex quantum architectures well beyond the listed problems, for instance, to the optimal control of noisy quantum systems.},
 author = {Luchnikov, Ilia A. and Krechetov, Mikhail E. and Filippov, Sergey N.},
 year = {2021},
 title = {Riemannian geometry and automatic differentiation for optimization  problems of quantum physics and quantum technologies},
 url = {http://arxiv.org/pdf/2007.01287v4},
 keywords = {Physics - Computational Physics;Quantum Physics},
 pages = {073006},
 volume = {23},
 number = {7},
 journal = {New Journal of Physics},
 doi = {10.1088/1367-2630/ac0b02},
 file = {}
}


@proceedings{Lukichev.01.10.201805.10.2018,
 year = {01.10.2018 - 05.10.2018},
 title = {International Conference on Micro- and Nano-Electronics 2018},
 publisher = {SPIE},
 isbn = {9781510627093},
 editor = {Lukichev, Vladimir F. and Rudenko, Konstantin V.}
}


@misc{Qi.06.10.2021,
 abstract = {The advent of noisy intermediate-scale quantum (NISQ) computers raises a crucial challenge to design quantum neural networks for fully quantum learning tasks. To bridge the gap, this work proposes an end-to-end learning framework named QTN-VQC, by introducing a trainable quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN is composed of a parametric tensor-train network for feature extraction and a tensor product encoding for quantum embedding. We highlight the QTN for quantum embedding in terms of two perspectives: (1) we theoretically characterize QTN by analyzing its representation power of input features; (2) QTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the generation of quantum embedding to the output measurement. Our experiments on the MNIST dataset demonstrate the advantages of QTN for quantum embedding over other quantum embedding approaches.},
 author = {Qi, Jun and Yang, Chao-Han Huck and Chen, Pin-Yu},
 date = {06.10.2021},
 title = {QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks},
 url = {http://arxiv.org/pdf/2110.03861v3},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computation and Language;Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning;Computer Science - Neural and Evolutionary Computing;Quantum Physics},
 file = {}
}


@misc{Latorre.2005,
 abstract = {The pixel values of an image can be casted into a real ket of a Hilbert space using an appropriate block structured addressing. The resulting state can then be rewritten in terms of its matrix product state representation in such a way that quantum entanglement corresponds to classical correlations between different coarse-grained textures. A truncation of the MPS representation is tantamount to a compression of the original image. The resulting algorithm can be improved adding a discrete Fourier transform preprocessing and a further entropic lossless compression.

4 pages, 5 figures},
 author = {Latorre, Jose I.},
 date = {2005},
 title = {Image compression and entanglement},
 publisher = {arXiv},
 doi = {10.48550/ARXIV.QUANT-PH/0510031}
}


@misc{Qi.08.06.2022,
 abstract = {The noisy intermediate-scale quantum (NISQ) devices enable the implementation of the variational quantum circuit (VQC) for quantum neural networks (QNN). Although the VQC-based QNN has succeeded in many machine learning tasks, the representation and generalization powers of VQC still require further investigation, particularly when the dimensionality reduction of classical inputs is concerned. In this work, we first put forth an end-to-end quantum neural network, namely, TTN-VQC, which consists of a quantum tensor network based on a tensor-train network (TTN) for dimensionality reduction and a VQC for functional regression. Then, we aim at the error performance analysis for the TTN-VQC in terms of representation and generalization powers. We also characterize the optimization properties of TTN-VQC by leveraging the Polyak-Lojasiewicz (PL) condition. Moreover, we conduct the experiments of functional regression on a handwritten digit classification dataset to justify our theoretical analysis.},
 author = {Qi, Jun and Yang, Chao-Han Huck and Chen, Pin-Yu and Hsieh, Min-Hsiu},
 date = {08.06.2022},
 title = {Theoretical Error Performance Analysis for Variational Quantum Circuit  Based Functional Regression},
 url = {http://arxiv.org/pdf/2206.04804v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Computer Science - Neural and Evolutionary Computing;Quantum Physics},
 file = {}
}


@misc{Reyes.22.01.2020,
 abstract = {We present an algorithm for supervised learning using tensor networks, employing a step of preprocessing the data by coarse-graining through a sequence of wavelet transformations. We represent these transformations as a set of tensor network layers identical to those in a multi-scale entanglement renormalization ansatz (MERA) tensor network, and perform supervised learning and regression tasks through a model based on a matrix product state (MPS) tensor network acting on the coarse-grained data. Because the entire model consists of tensor contractions (apart from the initial non-linear feature map), we can adaptively fine-grain the optimized MPS model backwards through the layers with essentially no loss in performance. The MPS itself is trained using an adaptive algorithm based on the density matrix renormalization group (DMRG) algorithm. We test our methods by performing a classification task on audio data and a regression task on temperature time-series data, studying the dependence of training accuracy on the number of coarse-graining layers and showing how fine-graining through the network may be used to initialize models with access to finer-scale features.},
 author = {Reyes, Justin and Stoudenmire, Miles},
 date = {22.01.2020},
 title = {A Multi-Scale Tensor Network Architecture for Classification and  Regression},
 url = {http://arxiv.org/pdf/2001.08286v1},
 keywords = {Computer Science - Learning;Quantum Physics;Statistics - Machine Learning},
 file = {}
}


@incollection{TamaraG.KoldaandBrettW.BaderSandiaNationalLaboratories.2018,
 author = {{Tamara G. Kolda and Brett W. Bader, Sandia National Laboratories}},
 title = {Tensor Decompositions: Applications},
 keywords = {canonical decomposition (CANDECOMP);higher-order principal components analysis (Tucker);higher-order singular value decomposition (HOSVD);multilinear algebra;multiway arrays;parallel factors (PARAFAC);Tensor decompositions},
 pages = {48--70},
 publisher = {{Cambridge University Press}},
 isbn = {9781316882177},
 editor = {Moitra, Ankur},
 booktitle = {Algorithmic Aspects of Machine Learning},
 year = {2018},
 doi = {10.1017/9781316882177.005},
 file = {}
}


@inproceedings{Tieleman.2008,
 author = {Tieleman, Tijmen},
 title = {Training restricted Boltzmann machines using approximations to the likelihood gradient},
 pages = {1064--1071},
 publisher = {{ACM Press}},
 isbn = {9781605582054},
 editor = {Cohen, William and McCallum, Andrew and Roweis, Sam},
 booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
 year = {2008},
 address = {New York, New York, USA},
 doi = {10.1145/1390156.1390290},
 file = {}
}


@article{Uvarov.2020,
 abstract = {Machine learning has emerged as a promising approach to study the properties of many-body systems. Recently proposed as a tool to classify phases of matter, the approach relies on classical simulation methods$-$such as Monte Carlo$-$which are known to experience an exponential slowdown when simulating certain quantum systems. To overcome this slowdown while still leveraging machine learning, we propose a variational quantum algorithm which merges quantum simulation and quantum machine learning to classify phases of matter. Our classifier is directly fed labeled states recovered by the variational quantum eigensolver algorithm, thereby avoiding the data reading slowdown experienced in many applications of quantum enhanced machine learning. We propose families of variational ansatz states that are inspired directly by tensor networks. This allows us to use tools from tensor network theory to explain properties of the phase diagrams the presented method recovers. Finally, we propose a nearest-neighbour (checkerboard) quantum neural network. This majority vote quantum classifier is successfully trained to recognize phases of matter with {\$}99\%$ accuracy for the transverse field Ising model and {\$}94\%$ accuracy for the XXZ model. These findings suggest that our merger between quantum simulation and quantum enhanced machine learning offers a fertile ground to develop computational insights into quantum systems.},
 author = {Uvarov, Alexey and Kardashin, Andrey and Biamonte, Jacob},
 year = {2020},
 title = {Machine Learning Phase Transitions with a Quantum Processor},
 url = {http://arxiv.org/pdf/1906.10155v2},
 keywords = {Computer Science - Learning;Physics - Disordered Systems and Neural Networks;Physics - Strongly Correlated Electrons;Quantum Physics},
 volume = {102},
 number = {1},
 issn = {2469-9926},
 journal = {Physical Review A},
 doi = {10.1103/PhysRevA.102.012415},
 file = {}
}


@article{V.MurgF.VerstraeteJ.I.Cirac.,
 author = {{V. Murg, F. Verstraete, J. I. Cirac}},
 title = {Variational study of hard-core bosons in a 2--D optical lattice using Projected Entangled Pair States (PEPS)},
 url = {arxiv:cond-mat/0611522v1},
 file = {}
}


@article{Vanderstraeten.2019,
 abstract = {In these lecture notes we give a technical overview of tangent-space methods for matrix product states in the thermodynamic limit. We introduce the manifold of uniform matrix product states, show how to compute different types of observables, and discuss the concept of a tangent space. We explain how to variationally optimize ground-state approximations, implement real-time evolution and describe elementary excitations for a given model Hamiltonian. Also, we explain how matrix product states approximate fixed points of one-dimensional transfer matrices. We show how all these methods can be translated to the language of continuous matrix product states for one-dimensional field theories. We conclude with some extensions of the tangent-space formalism and with an outlook to new applications.},
 author = {Vanderstraeten, Laurens and Haegeman, Jutho and Verstraete, Frank},
 year = {2019},
 title = {Tangent-space methods for uniform matrix product states},
 url = {http://arxiv.org/pdf/1810.07006v3},
 keywords = {Physics - Statistical Mechanics;Physics - Strongly Correlated Electrons;Quantum Physics},
 issn = {2590-1990},
 journal = {SciPost Physics Lecture Notes},
 doi = {10.21468/SciPostPhysLectNotes.7},
 file = {}
}


@misc{Vidal.,
 author = {Vidal, Guifr{\'e}},
 date = {2006},
 title = {Entanglement renormalization},
 url = {https://arxiv.org/pdf/cond-mat/0512165v2},
 file = {}
}


@article{Vidal.2003,
 abstract = {We present a classical protocol to efficiently simulate any pure-state quantum computation that involves only a restricted amount of entanglement. More generally, we show how to classically simulate pure-state quantum computations on n qubits by using computational resources that grow linearly in n and exponentially in the amount of entanglement in the quantum computer. Our results imply that a necessary condition for an exponential computational speedup (with respect to classical computations) is that the amount of entanglement increases with the size n of the computation, and provide an explicit lower bound on the required growth.},
 author = {Vidal, Guifr{\'e}},
 year = {2003},
 title = {Efficient classical simulation of slightly entangled quantum computations},
 pages = {147902},
 volume = {91},
 number = {14},
 issn = {0031-9007},
 journal = {Physical review letters},
 doi = {10.1103/PhysRevLett.91.147902},
 file = {}
}


@article{Tagliacozzo.2009,
 author = {Tagliacozzo, L. and Evenbly, G. and Vidal, G.},
 year = {2009},
 title = {Simulation of two-dimensional quantum systems using a tree tensor network that exploits the entropic area law},
 volume = {80},
 number = {23},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.80.235127},
 file = {}
}


@article{Vidal.2004,
 abstract = {We present a numerical method to simulate the time evolution, according to a generic Hamiltonian made of local interactions, of quantum spin chains and systems alike. The efficiency of the scheme depends on the amount of entanglement involved in the simulated evolution. Numerical analysis indicates that this method can be used, for instance, to efficiently compute time-dependent properties of low-energy dynamics in sufficiently regular but otherwise arbitrary one-dimensional quantum many-body systems. As by-products, we describe two alternatives to the density matrix renormalization group method.},
 author = {Vidal, Guifr{\'e}},
 year = {2004},
 title = {Efficient simulation of one-dimensional quantum many-body systems},
 pages = {040502},
 volume = {93},
 number = {4},
 issn = {0031-9007},
 journal = {Physical review letters},
 doi = {10.1103/PhysRevLett.93.040502},
 file = {}
}


@article{Wall.2021,
 abstract = {We describe a quantum-assisted machine learning (QAML) method in which multivariate data is encoded into quantum states in a Hilbert space whose dimension is exponentially large in the length of the data vector. Learning in this space occurs through applying a low-depth quantum circuit with a tree tensor network (TTN) topology, which acts as an unsupervised feature extractor to identify the most relevant quantum states in a data-driven fashion. This unsupervised feature extractor then feeds a supervised linear classifier and encodes the output in a small-dimensional quantum register. In contrast to previous work on \emph{quantum-inspired} TTN classifiers, in which the embedding map and class decision weights did not map the data to well-defined quantum states, we present an approach that can be implemented on gate-based quantum computing devices. In particular, we identify an embedding map with accuracy similar to exponential machines (Novikov \emph{et al.}, arXiv:1605.03795) , but which produces valid quantum states from classical data vectors, and utilize manifold-based gradient optimization schemes to produce isometric operations mapping quantum states to a register of qubits defining a class decision. We detail methods for efficiently obtaining one- and two-point correlation functions of the decision boundary vectors of the quantum model, which can be used for model interpretability, as well as methods for obtaining classifications from partial data vectors. Further, we show that the use of isometric tensors can significantly aid in the human interpretability of the correlation functions extracted from the decision weights, and may produce models that are less susceptible to adversarial perturbations. We demonstrate our methodologies in applications utilizing the MNIST handwritten digit dataset and a multivariate timeseries dataset of human activity recognition.},
 author = {Wall, Michael L. and D'Aguanno, Giuseppe},
 year = {2021},
 title = {Tree tensor network classifiers for machine learning: from  quantum-inspired to quantum-assisted},
 url = {http://arxiv.org/pdf/2104.02249v1},
 pages = {1498},
 volume = {104},
 number = {4},
 issn = {2469-9926},
 journal = {Physical Review A},
 doi = {10.1103/PhysRevA.104.042408}
}


@article{Wall.2022,
 abstract = {We demonstrate the use of matrix product state (MPS) models for discriminating quantum data on quantum computers using holographic algorithms, focusing on classifying a translationally invariant quantum state based on $L$ qubits of quantum data extracted from it. We detail a process in which data from single-shot experimental measurements are used to optimize an isometric tensor network, the tensors are compiled into unitary quantum operations using greedy compilation heuristics, parameter optimization on the resulting quantum circuit model removes the post-selection requirements of the isometric tensor model, and the resulting quantum model is inferenced on either product state or entangled quantum data. We demonstrate our training and inference architecture on a synthetic dataset of six-site single-shot measurements from the bulk of a one-dimensional transverse field Ising model (TFIM) deep in its antiferromagnetic and paramagnetic phases. We experimentally evaluate models on Quantinuum's H1-2 trapped ion quantum computer using entangled input data modeled as translationally invariant, bond dimension 4 MPSs across the known quantum phase transition of the TFIM. Using linear regression on the experimental data near the transition point, we find predictions for the critical transverse field of {\$}h=0.962{\$} and {\$}0.994{\$} for tensor network discriminators of bond dimension {\$}$\backslash$chi=2{\$} and {\$}$\backslash$chi=4{\$}, respectively. These predictions compare favorably with the known transition location of {\$}h=1{\$} despite training on data far from the transition point. Our techniques identify families of short-depth variational quantum circuits in a data-driven and hardware-aware fashion and robust classical techniques to precondition the model parameters, and can be adapted beyond machine learning to myriad applications of tensor networks on quantum computers, such as quantum simulation and error correction.},
 author = {Wall, Michael L. and Titum, Paraj and Quiroz, Gregory and Foss-Feig, Michael and Hazzard, Kaden R. A.},
 year = {2022},
 title = {A tensor network discriminator architecture for classification of  quantum data on quantum computers},
 url = {http://arxiv.org/pdf/2202.10911v1},
 keywords = {Physics - Statistical Mechanics;Quantum Physics},
 pages = {520},
 volume = {105},
 number = {6},
 issn = {2469-9926},
 journal = {Physical Review A},
 doi = {10.1103/PhysRevA.105.062439},
 file = {}
}


@misc{Wang.03.06.2020,
 abstract = {Originating from condensed matter physics, tensor networks are compact representations of high-dimensional tensors. In this paper, the prowess of tensor networks is demonstrated on the particular task of one-class anomaly detection. We exploit the memory and computational efficiency of tensor networks to learn a linear transformation over a space with dimension exponential in the number of original features. The linearity of our model enables us to ensure a tight fit around training instances by penalizing the model's global tendency to a predict normality via its Frobenius norm---a task that is infeasible for most deep learning models. Our method outperforms deep and classical algorithms on tabular datasets and produces competitive results on image datasets, despite not exploiting the locality of images.},
 author = {Wang, Jinhui and Roberts, Chase and Vidal, Guifre and Leichenauer, Stefan},
 date = {03.06.2020},
 title = {Anomaly Detection with Tensor Networks},
 url = {http://arxiv.org/pdf/2006.02516v2},
 keywords = {Computer Science - Learning;Quantum Physics;Statistics - Machine Learning},
 file = {}
}


@article{Wang.2021,
 author = {Wang, Kunkun and Xiao, Lei and Yi, Wei and Ran, Shi-Ju and Xue, Peng},
 year = {2021},
 title = {Experimental realization of a quantum image classifier via tensor-network-based machine learning},
 pages = {2332},
 volume = {9},
 number = {12},
 journal = {Photonics Research},
 doi = {10.1364/PRJ.434217},
 file = {}
}


@article{White.1992,
 author = {White, Steven R.},
 year = {1992},
 title = {Density matrix formulation for quantum renormalization groups},
 keywords = {doi:10.1103/PhysRevLett.69.2863  url:http://dx.doi.org/10.1103/PhysRevLett.69.2863},
 volume = {69},
 number = {19},
 issn = {0031-9007},
 journal = {Physical Review Letters},
 file = {}
}

@misc{Wu.24.06.2022,
 abstract = {We show that any matrix product state (MPS) can be exactly represented by a recurrent neural network (RNN) with a linear memory update. We generalize this RNN architecture to 2D lattices using a multilinear memory update. It supports perfect sampling and wave function evaluation in polynomial time, and can represent an area law of entanglement entropy. Numerical evidence shows that it can encode the wave function using a bond dimension lower by orders of magnitude when compared to MPS, with an accuracy that can be systematically improved by increasing the bond dimension.},
 author = {Wu, Dian and Rossi, Riccardo and Vicentini, Filippo and Carleo, Giuseppe},
 date = {24.06.2022},
 title = {From Tensor Network Quantum States to Tensorial Recurrent Neural  Networks},
 url = {http://arxiv.org/pdf/2206.12363v1},
 keywords = {Computer Science - Learning;Physics - Computational Physics;Physics - Strongly Correlated Electrons;Quantum Physics;Statistics - Machine Learning},
 file = {}
}


@misc{Ye.07.02.2020,
 abstract = {Lane-change maneuvers are commonly executed by drivers to follow a certain routing plan, overtake a slower vehicle, adapt to a merging lane ahead, etc. However, improper lane change behaviors can be a major cause of traffic flow disruptions and even crashes. While many rule-based methods have been proposed to solve lane change problems for autonomous driving, they tend to exhibit limited performance due to the uncertainty and complexity of the driving environment. Machine learning-based methods offer an alternative approach, as Deep reinforcement learning (DRL) has shown promising success in many application domains including robotic manipulation, navigation, and playing video games. However, applying DRL to autonomous driving still faces many practical challenges in terms of slow learning rates, sample inefficiency, and safety concerns. In this study, we propose an automated lane change strategy using proximal policy optimization-based deep reinforcement learning, which shows great advantages in learning efficiency while still maintaining stable performance. The trained agent is able to learn a smooth, safe, and efficient driving policy to make lane-change decisions (i.e. when and how) in a challenging situation such as dense traffic scenarios. The effectiveness of the proposed policy is validated by using metrics of task success rate and collision rate. The simulation results demonstrate the lane change maneuvers can be efficiently learned and executed in a safe, smooth, and efficient manner.},
 author = {Ye, Fei and Cheng, Xuxin and Wang, Pin and Chan, Ching-Yao and Zhang, Jiucai},
 date = {07.02.2020},
 title = {Automated Lane Change Strategy using Proximal Policy Optimization-based  Deep Reinforcement Learning},
 url = {http://arxiv.org/pdf/2002.02667v2},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Computer Science - Robotics},
 file = {}
}


@article{Wall.2021b,
 abstract = {Noisy, intermediate-scale quantum (NISQ) computing devices have become an industrial reality in the last few years, and cloud-based interfaces to these devices are enabling exploration of near-term quantum computing on a range of problems. As NISQ devices are too noisy for many of the algorithms with a known quantum advantage, discovering impactful applications for near-term devices is the subject of intense research interest. We explore quantum-assisted machine learning (QAML) on NISQ devices through the perspective of tensor networks (TNs), which offer a robust platform for designing resource-efficient and expressive machine learning models to be dispatched on quantum devices. In particular, we lay out a framework for designing and optimizing TN-based QAML models using classical techniques, and then compiling these models to be run on quantum hardware, with demonstrations for generative matrix product state (MPS) models. We put forth a generalized canonical form for MPS models that aids in compilation to quantum devices, and demonstrate greedy heuristics for compiling with a given topology and gate set that outperforms known generic methods in terms of the number of entangling gates, e.g., CNOTs, in some cases by an order of magnitude. We present an exactly solvable benchmark problem for assessing the performance of MPS QAML models, and also present an application for the canonical MNIST handwritten digit dataset. The impacts of hardware topology and day-to-day experimental noise fluctuations on model performance are explored by analyzing both raw experimental counts and statistical divergences of inferred distributions. We also present parametric studies of depolarization and readout noise impacts on model performance using hardware simulators.},
 author = {Wall, Michael L. and Abernathy, Matthew R. and Quiroz, Gregory},
 year = {2021},
 title = {Generative machine learning with tensor networks: benchmarks on  near-term quantum computers},
 url = {http://arxiv.org/pdf/2010.03641v1},
 keywords = {Quantum Physics},
 volume = {3},
 number = {2},
 issn = {2643-1564},
 journal = {Physical Review Research},
 doi = {10.1103/PhysRevResearch.3.023010},
 file = {}
}


@inproceedings{Szegedy.2016,
 abstract = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR);2016; ; ;10.1109/CVPR.2016.308},
 author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
 title = {Rethinking the Inception Architecture for Computer Vision},
 pages = {2818--2826},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2016},
 doi = {10.1109/CVPR.2016.308},
 file = {}
}


@article{Swingle.2012,
 abstract = {I show how recent progress in real space renormalization group methods can be used to define a generalized notion of holography inspired by holographic dualities in quantum gravity. The generalization is based upon organizing information in a quantum state in terms of scale and defining a higher dimensional geometry from this structure. While states with a finite correlation length typically give simple geometries, the state at a quantum critical point gives a discrete version of anti de Sitter space. Some finite temperature quantum states include black hole-like objects. The gross features of equal time correlation functions are also reproduced in this geometric framework. The relationship between this framework and better understood versions of holography is discussed.},
 author = {Swingle, Brian},
 year = {2012},
 title = {Entanglement Renormalization and Holography},
 url = {http://arxiv.org/pdf/0905.1317v1},
 keywords = {High Energy Physics - Theory;Physics - Strongly Correlated Electrons},
 pages = {231},
 volume = {86},
 number = {6},
 issn = {1550-7998},
 journal = {Physical Review D},
 doi = {10.1103/PhysRevD.86.065007},
 file = {}
}


@article{Sun.2020,
 abstract = {Phys. Rev. B 101, 075135 (2020). doi:10.1103/PhysRevB.101.075135 },
 author = {Sun, Zheng-Zhi and Peng, Cheng and Liu, Ding and Ran, Shi-Ju and Su, Gang},
 year = {2020},
 title = {Generative tensor network classification model for supervised machine learning},
 keywords = {doi:10.1103/PhysRevB.101.075135 url:https://doi.org/10.1103/PhysRevB.101.075135},
 volume = {101},
 number = {7},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.101.075135},
 file = {}
}


@misc{Roberts.03.05.2019,
 abstract = {TensorNetwork is an open source library for implementing tensor network algorithms. Tensor networks are sparse data structures originally designed for simulating quantum many-body physics, but are currently also applied in a number of other research areas, including machine learning. We demonstrate the use of the API with applications both physics and machine learning, with details appearing in companion papers.},
 author = {Roberts, Chase and Milsted, Ashley and Ganahl, Martin and Zalcman, Adam and Fontaine, Bruce and Zou, Yijian and Hidary, Jack and Vidal, Guifre and Leichenauer, Stefan},
 date = {03.05.2019},
 title = {TensorNetwork: A Library for Physics and Machine Learning},
 url = {http://arxiv.org/pdf/1905.01330v1},
 keywords = {Computer Science - Learning;High Energy Physics - Theory;Physics - Computational Physics;Physics - Strongly Correlated Electrons;Statistics - Machine Learning},
 file = {}
}


@article{Rohwedder.2013,
 author = {Rohwedder, Thorsten and Uschmajew, Andr{\'e}},
 year = {2013},
 title = {On Local Convergence of Alternating Schemes for Optimization of Convex Problems in the Tensor Train Format},
 pages = {1134--1162},
 volume = {51},
 number = {2},
 issn = {0036-1429},
 journal = {SIAM Journal on Numerical Analysis},
 doi = {10.1137/110857520},
 file = {}
}


@misc{Sagingalieva.10.05.2022,
 abstract = {Image recognition is one of the primary applications of machine learning algorithms. Nevertheless, machine learning models used in modern image recognition systems consist of millions of parameters that usually require significant computational time to be adjusted. Moreover, adjustment of model hyperparameters leads to additional overhead. Because of this, new developments in machine learning models and hyperparameter optimization techniques are required. This paper presents a quantum-inspired hyperparameter optimization technique and a hybrid quantum-classical machine learning model for supervised learning. We benchmark our hyperparameter optimization method over standard black-box objective functions and observe performance improvements in the form of reduced expected run times and fitness in response to the growth in the size of the search space. We test our approaches in a car image classification task, and demonstrate a full-scale implementation of the hybrid quantum neural network model with the tensor train hyperparameter optimization. Our tests show a qualitative and quantitative advantage over the corresponding standard classical tabular grid search approach used with a deep neural network ResNet34. A classification accuracy of 0.97 was obtained by the hybrid model after 18 iterations, whereas the classical model achieved an accuracy of 0.92 after 75 iterations.},
 author = {Sagingalieva, Asel and Kurkin, Andrii and Melnikov, Artem and Kuhmistrov, Daniil and Perelshtein, Michael and Melnikov, Alexey and Skolik, Andrea and Dollen, David Von},
 date = {10.05.2022},
 title = {Hyperparameter optimization of hybrid quantum neural networks for car  classification},
 url = {http://arxiv.org/pdf/2205.04878v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning;Quantum Physics},
 file = {}
}


@article{Schollwock.2011,
 author = {Schollw{\"o}ck, Ulrich},
 year = {2011},
 title = {The density-matrix renormalization group in the age of matrix product states},
 pages = {96--192},
 volume = {326},
 number = {1},
 issn = {00034916},
 journal = {Annals of Physics},
 doi = {10.1016/j.aop.2010.09.012},
 file = {}
}


@article{Schon.2005,
 abstract = {We consider the deterministic generation of entangled multiqubit states by the sequential coupling of an ancillary system to initially uncorrelated qubits. We characterize all achievable states in terms of classes of matrix-product states and give a recipe for the generation on demand of any multiqubit state. The proposed methods are suitable for any sequential generation scheme, though we focus on streams of single-photon time-bin qubits emitted by an atom coupled to an optical cavity. We show, in particular, how to generate familiar quantum information states such as W, Greenberger-Horne-Zeilinger, and cluster states within such a framework.},
 author = {Sch{\"o}n, C. and Solano, E. and Verstraete, F. and Cirac, J. I. and Wolf, M. M.},
 year = {2005},
 title = {Sequential generation of entangled multiqubit states},
 pages = {110503},
 volume = {95},
 number = {11},
 issn = {0031-9007},
 journal = {Physical review letters},
 doi = {10.1103/PhysRevLett.95.110503},
 file = {}
}


@article{Schuch.2007,
 abstract = {We determine the computational power of preparing projected entangled pair states (PEPS), as well as the complexity of classically simulating them, and generally the complexity of contracting tensor networks. While creating PEPS allows us to solve PP problems, the latter two tasks are both proven to be {\#}P-complete. We further show how PEPS can be used to approximate ground states of gapped Hamiltonians and that creating them is easier than creating arbitrary PEPS. The main tool for our proofs is a duality between PEPS and postselection which allows us to use existing results from quantum complexity.},
 author = {Schuch, Norbert and Wolf, Michael M. and Verstraete, Frank and Cirac, J. Ignacio},
 year = {2007},
 title = {Computational complexity of projected entangled pair states},
 pages = {140506},
 volume = {98},
 number = {14},
 issn = {0031-9007},
 journal = {Physical review letters},
 doi = {10.1103/PhysRevLett.98.140506},
 file = {}
}


@article{Schuld.2021,
 author = {Schuld, Maria and Sweke, Ryan and Meyer, Johannes Jakob},
 year = {2021},
 title = {Effect of data encoding on the expressive power of variational quantum-machine-learning models},
 volume = {103},
 number = {3},
 issn = {2469-9926},
 journal = {Physical Review A},
 doi = {10.1103/PhysRevA.103.032430}
}


@article{Schwarz.2012,
 abstract = {We present a quantum algorithm to prepare injective projected entangled pair states (PEPS) on a quantum computer, a class of open tensor networks representing quantum states. The run time of our algorithm scales polynomially with the inverse of the minimum condition number of the PEPS projectors and, essentially, with the inverse of the spectral gap of the PEPS's parent Hamiltonian.},
 author = {Schwarz, Martin and Temme, Kristan and Verstraete, Frank},
 year = {2012},
 title = {Preparing projected entangled pair states on a quantum computer},
 pages = {110502},
 volume = {108},
 number = {11},
 issn = {0031-9007},
 journal = {Physical review letters},
 doi = {10.1103/PhysRevLett.108.110502},
 file = {}
}


@misc{Seitz.02.06.2022,
 abstract = {We develop and analyze a method for simulating quantum circuits on classical computers by representing quantum states as rooted tree tensor networks. Our algorithm first determines a suitable, fixed tree structure adapted to the expected entanglement generated by the quantum circuit. The gates are sequentially applied to the tree by absorbing single-qubit gates into leaf nodes, and splitting two-qubit gates via singular value decomposition and threading the resulting virtual bond through the tree. We theoretically analyze the applicability of the method as well as its computational cost and memory requirements, and identify advantageous scenarios in terms of required bond dimensions as compared to a matrix product state representation. The study is complemented by numerical experiments for different quantum circuit layouts up to 37 qubits.},
 author = {Seitz, Philipp and Medina, Ismael and Cruz, Esther and Huang, Qunsheng and Mendl, Christian B.},
 date = {02.06.2022},
 title = {Simulating quantum circuits using tree tensor networks},
 url = {http://arxiv.org/pdf/2206.01000v2},
 keywords = {Physics - Computational Physics;Quantum Physics},
 file = {}
}


@misc{Selvan.13.11.2020,
 abstract = {The recently introduced locally orderless tensor network (LoTeNet) for supervised image classification uses matrix product state (MPS) operations on grids of transformed image patches. The resulting patch representations are combined back together into the image space and aggregated hierarchically using multiple MPS blocks per layer to obtain the final decision rules. In this work, we propose a non-patch based modification to LoTeNet that performs one MPS operation per layer, instead of several patch-level operations. The spatial information in the input images to MPS blocks at each layer is squeezed into the feature dimension, similar to LoTeNet, to maximise retained spatial correlation between pixels when images are flattened into 1D vectors. The proposed multi-layered tensor network (MLTN) is capable of learning linear decision boundaries in high dimensional spaces in a multi-layered setting, which results in a reduction in the computation cost compared to LoTeNet without any degradation in performance.},
 author = {Selvan, Raghavendra and {\O}rting, Silas and Dam, Erik B.},
 date = {13.11.2020},
 title = {Multi-layered tensor networks for image classification},
 url = {http://arxiv.org/pdf/2011.06982v2},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning;Statistics - Machine Learning},
 file = {}
}


@misc{Selvan.13.11.2020b,
 abstract = {The recently introduced locally orderless tensor network (LoTeNet) for supervised image classification uses matrix product state (MPS) operations on grids of transformed image patches. The resulting patch representations are combined back together into the image space and aggregated hierarchically using multiple MPS blocks per layer to obtain the final decision rules. In this work, we propose a non-patch based modification to LoTeNet that performs one MPS operation per layer, instead of several patch-level operations. The spatial information in the input images to MPS blocks at each layer is squeezed into the feature dimension, similar to LoTeNet, to maximise retained spatial correlation between pixels when images are flattened into 1D vectors. The proposed multi-layered tensor network (MLTN) is capable of learning linear decision boundaries in high dimensional spaces in a multi-layered setting, which results in a reduction in the computation cost compared to LoTeNet without any degradation in performance.},
 author = {Selvan, Raghavendra and {\O}rting, Silas and Dam, Erik B.},
 date = {13.11.2020},
 title = {Multi-layered tensor networks for image classification},
 url = {http://arxiv.org/pdf/2011.06982v2},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning;Statistics - Machine Learning},
 file = {}
}


@misc{Sengupta.06.07.2022,
 abstract = {A tensor network is a type of decomposition used to express and approximate large arrays of data. A given data-set, quantum state or higher dimensional multi-linear map is factored and approximated by a composition of smaller multi-linear maps. This is reminiscent to how a Boolean function might be decomposed into a gate array: this represents a special case of tensor decomposition, in which the tensor entries are replaced by 0, 1 and the factorisation becomes exact. The collection of associated techniques are called, tensor network methods: the subject developed independently in several distinct fields of study, which have more recently become interrelated through the language of tensor networks. The tantamount questions in the field relate to expressability of tensor networks and the reduction of computational overheads. A merger of tensor networks with machine learning is natural. On the one hand, machine learning can aid in determining a factorization of a tensor network approximating a data set. On the other hand, a given tensor network structure can be viewed as a machine learning model. Herein the tensor network parameters are adjusted to learn or classify a data-set. In this survey we recover the basics of tensor networks and explain the ongoing effort to develop the theory of tensor networks in machine learning.},
 author = {Sengupta, Richik and Adhikary, Soumik and Oseledets, Ivan and Biamonte, Jacob},
 date = {06.07.2022},
 title = {Tensor networks in machine learning},
 url = {http://arxiv.org/pdf/2207.02851v1},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Learning;Physics - Disordered Systems and Neural Networks;Quantum Physics},
 file = {}
}


@misc{Sierra.1998,
 abstract = {We present an overview of the Density Matrix Renormalization Group and its connections to Quantum Groups, Matrix Products and Conformal Field Theory. We emphasize some common formal structures in all these theories. We also propose two-dimensional extensions of the variational matrix product ansatzs.

REVTEX file, 19 pages. Proceedings of the Workshop on the Exact Renormalization Group, Faro (Portugal), 10-12 September 1998, to be published by World Scientific. One reference added},
 author = {Sierra, G. and Martin-Delgado, M. A.},
 date = {1998},
 title = {The Density Matrix Renormalization Group, Quantum Groups and Conformal Field Theory},
 publisher = {arXiv},
 doi = {10.48550/ARXIV.COND-MAT/9811170}
}


@article{Silvi.2010,
 author = {Silvi, P. and Giovannetti, V. and Montangero, S. and Rizzi, M. and Cirac, J. I. and Fazio, R.},
 year = {2010},
 title = {Homogeneous binary trees as ground states of quantum critical Hamiltonians},
 volume = {81},
 number = {6},
 issn = {2469-9926},
 journal = {Physical Review A},
 doi = {10.1103/PhysRevA.81.062335},
 file = {}
}


@misc{Srinivasan.21.10.2020,
 abstract = {Modeling joint probability distributions over sequences has been studied from many perspectives. The physics community developed matrix product states, a tensor-train decomposition for probabilistic modeling, motivated by the need to tractably model many-body systems. But similar models have also been studied in the stochastic processes and weighted automata literature, with little work on how these bodies of work relate to each other. We address this gap by showing how stationary or uniform versions of popular quantum tensor network models have equivalent representations in the stochastic processes and weighted automata literature, in the limit of infinitely long sequences. We demonstrate several equivalence results between models used in these three communities: (i) uniform variants of matrix product states, Born machines and locally purified states from the quantum tensor networks literature, (ii) predictive state representations, hidden Markov models, norm-observable operator models and hidden quantum Markov models from the stochastic process literature,and (iii) stochastic weighted automata, probabilistic automata and quadratic automata from the formal languages literature. Such connections may open the door for results and methods developed in one area to be applied in another.},
 author = {Srinivasan, Siddarth and Adhikary, Sandesh and Miller, Jacob and Rabusseau, Guillaume and Boots, Byron},
 date = {21.10.2020},
 title = {Quantum Tensor Networks, Stochastic Processes, and Weighted Automata},
 url = {http://arxiv.org/pdf/2010.10653v1},
 keywords = {Computer Science - Learning;Quantum Physics},
 file = {}
}

@article{StellanOstlund.1995,
 author = {{Stellan Ostlund} and {Stefan Rommer}},
 year = {1995},
 title = {Thermodynamic Limit of Density Matrix Renormalization},
 keywords = {doi:10.1103/PhysRevLett.75.3537    url:http://dx.doi.org/10.1103/PhysRevLett.75.3537}, 
 volume = {75},
 number = {19},
 issn = {0031-9007},
 journal = {Physical Review Letters},
 file = {}
}

@article{Stoudenmire.2018,
 abstract = {Quantum Science and Technology, 3(2018) 034003. doi:10.1088/2058-9565/aaba1a    },
 author = {Stoudenmire, E. Miles},
 year = {2018},
 title = {Learning relevant features of data with multi-scale tensor networks},
 keywords = {machine learning;supervised learning;tensor networks;unsupervised learning},
 pages = {034003},
 volume = {3},
 number = {3},
 issn = {2058-9565},
 journal = {Quantum Science and Technology},
 doi = {10.1088/2058-9565/aaba1a},
 file = {}
}


@article{Ran.2020,
 abstract = {The matrix product state (MPS) belongs to the most important mathematical models in, for example, condensed matter physics and quantum information sciences. However, to realize an $N$-qubit MPS with large $N$ and large entanglement on a quantum platform is extremely challenging, since it requires high-level qudits or multi-body gates of two-level qubits to carry the entanglement. In this work, an efficient method that accurately encodes a given MPS into a quantum circuit with only one- and two-qubit gates is proposed. The idea is to construct the unitary matrix product operators that optimally disentangle the MPS to a product state. These matrix product operators form the quantum circuit that evolves a product state to the targeted MPS with a high fidelity. Our benchmark on the ground-state MPS's of the strongly-correlated spin models show that the constructed quantum circuits can encode the MPS's with much fewer qubits than the sizes of the MPS's themselves. This method paves a feasible and efficient path to realizing quantum many-body states and other MPS-based models as quantum circuits on the near-term quantum platforms.},
 author = {Ran, Shi-Ju},
 year = {2020},
 title = {Encoding of Matrix Product States into Quantum Circuits of One- and  Two-Qubit Gates},
 url = {http://arxiv.org/pdf/1908.07958v2},
 keywords = {Physics - Strongly Correlated Electrons;Quantum Physics},
 pages = {401},
 volume = {101},
 number = {3},
 issn = {2469-9926},
 journal = {Physical Review A},
 doi = {10.1103/PhysRevA.101.032310},
 file = {}
}


@article{LaRose.2020,
 abstract = {Phys. Rev. A 102, 032420 (2020). doi:10.1103/PhysRevA.102.032420},
 author = {LaRose, Ryan and Coyle, Brian},
 year = {2020},
 title = {Robust data encodings for quantum classifiers},
 keywords = {doi:10.1103/PhysRevA.102.032420 url:https://doi.org/10.1103/PhysRevA.102.032420},
 volume = {102},
 number = {3},
 issn = {2469-9926},
 journal = {Physical Review A},
 doi = {10.1103/PhysRevA.102.032420},
 file = {}
}


@misc{Larocca.24.09.2021,
 abstract = {The prospect of achieving quantum advantage with Quantum Neural Networks (QNNs) is exciting. Understanding how QNN properties (e.g., the number of parameters $M$) affect the loss landscape is crucial to the design of scalable QNN architectures. Here, we rigorously analyze the overparametrization phenomenon in QNNs with periodic structure. We define overparametrization as the regime where the QNN has more than a critical number of parameters $M_c$ that allows it to explore all relevant directions in state space. Our main results show that the dimension of the Lie algebra obtained from the generators of the QNN is an upper bound for $M_c$, and for the maximal rank that the quantum Fisher information and Hessian matrices can reach. Underparametrized QNNs have spurious local minima in the loss landscape that start disappearing when {\$}M$\backslash$geq M{\_}c{\$}. Thus, the overparametrization onset corresponds to a computational phase transition where the QNN trainability is greatly improved by a more favorable landscape. We then connect the notion of overparametrization to the QNN capacity, so that when a QNN is overparametrized, its capacity achieves its maximum possible value. We run numerical simulations for eigensolver, compilation, and autoencoding applications to showcase the overparametrization computational phase transition. We note that our results also apply to variational quantum algorithms and quantum optimal control.},
 author = {Larocca, Martin and Ju, Nathan and Garc{\'i}a-Mart{\'i}n, Diego and Coles, Patrick J. and Cerezo, M.},
 date = {24.09.2021},
 title = {Theory of overparametrization in quantum neural networks},
 url = {http://arxiv.org/pdf/2109.11676v1},
 keywords = {Computer Science - Learning;Quantum Physics;Statistics - Machine Learning},
 file = {}
}


@inproceedings{Krasowski.2020,
 author = {Krasowski, Hanna and Wang, Xiao and Althoff, Matthias},
 title = {Safe Reinforcement Learning for Autonomous Lane Changing Using Set-Based Prediction},
 pages = {1--7},
 publisher = {IEEE},
 isbn = {978-1-7281-4149-7},
 booktitle = {2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)},
 year = {2020},
 doi = {10.1109/ITSC45102.2020.9294259},
 file = {}
}


@article{Bittel.2021,
 abstract = {Variational quantum algorithms are proposed to solve relevant computational problems on near term quantum devices. Popular versions are variational quantum eigensolvers and quantum ap- proximate optimization algorithms that solve ground state problems from quantum chemistry and binary optimization problems, respectively. They are based on the idea of using a classical computer to train a parameterized quantum circuit. We show that the corresponding classical optimization problems are NP-hard. Moreover, the hardness is robust in the sense that, for every polynomial time algorithm, there are instances for which the relative error resulting from the classical optimization problem can be arbitrarily large assuming P {\$}$\backslash$neq{\$} NP. Even for classically tractable systems composed of only logarithmically many qubits or free fermions, we show the optimization to be NP-hard. This elucidates that the classical optimization is intrinsically hard and does not merely inherit the hardness from the ground state problem. Our analysis shows that the training landscape can have many far from optimal persistent local minima. This means that gradient and higher order descent algorithms will generally converge to far from optimal solutions.},
 author = {Bittel, Lennart and Kliesch, Martin},
 year = {2021},
 title = {Training variational quantum algorithms is NP-hard},
 url = {http://arxiv.org/pdf/2101.07267v2},
 keywords = {algorithm;circuit;hard;hybrid;maximization;minimization;NP;optimization;QAOA;quantum;quantum approximate optimization algorithm;Quantum Physics;time evolution;variational;variational quantum algorithms;variational quantum eigensolver;VQA;VQE},
 volume = {127},
 number = {12},
 issn = {0031-9007},
 journal = {Physical Review Letters},
 doi = {10.1103/PhysRevLett.127.120502},
 file = {}
}


@article{Boixo.2018,
 abstract = {A critical question for the field of quantum computing in the near future is whether quantum devices without error correction can perform a well-defined computational task beyond the capabilities of state-of-the-art classical computers, achieving so-called quantum supremacy. We study the task of sampling from the output distributions of (pseudo-)random quantum circuits, a natural task for benchmarking quantum computers. Crucially, sampling this distribution classically requires a direct numerical simulation of the circuit, with computational cost exponential in the number of qubits. This requirement is typical of chaotic systems. We extend previous results in computational complexity to argue more formally that this sampling task must take exponential time in a classical computer. We study the convergence to the chaotic regime using extensive supercomputer simulations, modeling circuits with up to 42 qubits - the largest quantum circuits simulated to date for a computational task that approaches quantum supremacy. We argue that while chaotic states are extremely sensitive to errors, quantum supremacy can be achieved in the near-term with approximately fifty superconducting qubits. We introduce cross entropy as a useful benchmark of quantum circuits which approximates the circuit fidelity. We show that the cross entropy can be efficiently measured when circuit simulations are available. Beyond the classically tractable regime, the cross entropy can be extrapolated and compared with theoretical estimates of circuit fidelity to define a practical quantum supremacy test.},
 author = {Boixo, Sergio and Isakov, Sergei V. and Smelyanskiy, Vadim N. and Babbush, Ryan and Ding, Nan and Jiang, Zhang and Bremner, Michael J. and Martinis, John M. and Neven, Hartmut},
 year = {2018},
 title = {Characterizing Quantum Supremacy in Near-Term Devices},
 url = {http://arxiv.org/pdf/1608.00263v3},
 keywords = {Quantum Physics},
 pages = {595--600},
 volume = {14},
 number = {6},
 issn = {1745-2473},
 journal = {Nature Physics},
 doi = {10.1038/s41567-018-0124-x},
 file = {}
}


@article{Bridgeman.2017,
 abstract = {The curse of dimensionality associated with the Hilbert space of spin systems provides a significant obstruction to the study of condensed matter systems. Tensor networks have proven an important tool in attempting to overcome this difficulty in both the numerical and analytic regimes.  These notes form the basis for a seven lecture course, introducing the basics of a range of common tensor networks and algorithms. In particular, we cover: introductory tensor network notation, applications to quantum information, basic properties of matrix product states, a classification of quantum phases using tensor networks, algorithms for finding matrix product states, basic properties of projected entangled pair states, and multiscale entanglement renormalisation ansatz states.  The lectures are intended to be generally accessible, although the relevance of many of the examples may be lost on students without a background in many-body physics/quantum information. For each lecture, several problems are given, with worked solutions in an ancillary file.},
 author = {Bridgeman, Jacob C. and Chubb, Christopher T.},
 year = {2017},
 title = {Hand-waving and Interpretive Dance: An Introductory Course on Tensor  Networks},
 url = {http://arxiv.org/pdf/1603.03039v4},
 keywords = {High Energy Physics - Theory;Physics - Statistical Mechanics;Physics - Strongly Correlated Electrons;Quantum Physics},
 pages = {223001},
 volume = {50},
 number = {22},
 issn = {1751-8113},
 journal = {Journal of Physics A: Mathematical and Theoretical},
 doi = {10.1088/1751-8121/aa6dc3},
 file = {}
}


@article{Caro.2022,
 abstract = {Modern quantum machine learning (QML) methods involve variationally optimizing a parameterized quantum circuit on a training data set, and subsequently making predictions on a testing data set (i.e., generalizing). In this work, we provide a comprehensive study of generalization performance in QML after training on a limited number N of training data points. We show that the generalization error of a quantum machine learning model with T trainable gates scales at worst as [Formula: see text]. When only K$\ll$T gates have undergone substantial change in the optimization process, we prove that the generalization error improves to [Formula: see text]. Our results imply that the compiling of unitaries into a polynomial number of native gates, a crucial application for the quantum computing industry that typically uses exponential-size training data, can be sped up significantly. We also show that classification of quantum states across a phase transition with a quantum convolutional neural network requires only a very small training data set. Other potential applications include learning quantum error correcting codes or quantum dynamical simulation. Our work injects new hope into the field of QML, as good generalization is guaranteed from few training data.},
 author = {Caro, Matthias C. and Huang, Hsin-Yuan and Cerezo, M. and Sharma, Kunal and Sornborger, Andrew and Cincio, Lukasz and Coles, Patrick J.},
 year = {2022},
 title = {Generalization in quantum machine learning from few training data},
 pages = {4919},
 volume = {13},
 number = {1},
 journal = {Nature communications},
 doi = {10.1038/s41467-022-32550-3},
 file = {}
}


@article{Cavinato.2021,
 abstract = {We present a novel application of Tensor Network methods in cancer treatment as a potential tool to solve the dose optimization problem in radiotherapy. In particular, the intensity-modulated radiation therapy technique-that allows treating irregular and inhomogeneous tumors while reducing the radiation toxicity on healthy organs-is based on the optimization problem of the beamlets intensities that shall result in a maximal delivery of the therapy dose to cancer while avoiding the organs at risk of being damaged by the radiation. The resulting optimization problem is expressed as a cost function to be optimized. Here, we map the cost function into an Ising-like Hamiltonian, describing a system of long-range interacting qubits. Finally, we solve the dose optimization problem by finding the ground-state of the Hamiltonian using a Tree Tensor Network algorithm. In particular, we present an anatomical scenario exemplifying a prostate cancer treatment. A similar approach can be applied to future hybrid classical-quantum algorithms, paving the way for the use of quantum technologies in future medical treatments.},
 author = {Cavinato, Samuele and Felser, Timo and Fusella, Marco and Paiusco, Marta and Montangero, Simone},
 year = {2021},
 title = {Optimizing radiotherapy plans for cancer treatment with Tensor Networks},
 keywords = {Algorithms;cancer treatment;Humans;hybrid quantum--classical technology;Male;medical physics;Prostatic Neoplasms/radiotherapy;quantum information;quantum spin glass;Radiation Injuries;radiotherapy;Radiotherapy Planning, Computer-Assisted;Radiotherapy, Intensity-Modulated;tensor network},
 volume = {66},
 number = {12},
 journal = {Physics in medicine and biology},
 doi = {10.1088/1361-6560/ac01f2},
 file = {}
}


@article{Chen.2018,
 abstract = {Phys. Rev. B 97, 085104 (2018). doi:10.1103/PhysRevB.97.085104},
 author = {Chen, Jing and Cheng, Song and Xie, Haidong and Wang, Lei and Xiang, Tao},
 year = {2018},
 title = {Equivalence of restricted Boltzmann machines and tensor network states},
 keywords = {doi:10.1103/PhysRevB.97.085104   url:https://doi.org/10.1103/PhysRevB.97.085104  
 },
 volume = {97},
 number = {8},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.97.085104},
 file = {}
}


@article{Chen.2022,
 abstract = {Machine Learning: Science and Technology, 3 (2022) 015025 doi: 10.1088/2632-2153/ac4559     },
 author = {Chen, Samuel Yen-Chi and Huang, Chih-Min and Hsing, Chia-Wei and Goan, Hsi-Sheng and Kao, Ying-Jer},
 year = {2022},
 title = {Variational quantum reinforcement learning via evolutionary optimization},
 keywords = {artificial intelligence;evolutionary optimization;quantum machine learning;quantum neural networks;Reinforcement Learning;variational quantum circuits},
 pages = {015025},
 volume = {3},
 number = {1},
 journal = {Machine Learning: Science and Technology},
 doi = {10.1088/2632-2153/ac4559},
 file = {}
}


@misc{Bhatia.04.05.2019,
 abstract = {In recent years, interest in expressing the success of neural networks to the quantum computing has increased significantly. Tensor network theory has become increasingly popular and widely used to simulate strongly entangled correlated systems. Matrix product state (MPS) is the well-designed class of tensor network states, which plays an important role in processing of quantum information. In this paper, we have shown that matrix product state as one-dimensional array of tensors can be used to classify classical and quantum data. We have performed binary classification of classical machine learning dataset Iris encoded in a quantum state. Further, we have investigated the performance by considering different parameters on the ibmqx4 quantum computer and proved that MPS circuits can be used to attain better accuracy. Further, the learning ability of MPS quantum classifier is tested to classify evapotranspiration ({\$}ET{\_}{o}{\$}) for Patiala meteorological station located in Northern Punjab (India), using three years of historical dataset (Agri). Furthermore, we have used different performance metrics of classification to measure its capability. Finally, the results are plotted and degree of correspondence among values of each sample is shown.},
 author = {Bhatia, Amandeep Singh and Saggi, Mandeep Kaur and Kumar, Ajay and Jain, Sushma},
 date = {04.05.2019},
 title = {Matrix Product State Based Quantum Classifier},
 url = {http://arxiv.org/pdf/1905.01426v1},
 keywords = {Computer Science - Learning;Quantum Physics},
 file = {}
}


@article{Chen.2021,
 abstract = {Machine Learning: Science and Technology, 2 (2021) 045021 doi: 10.1088/2632-2153/ac104d      },
 author = {Chen, Samuel Yen-Chi and Huang, Chih-Min and Hsing, Chia-Wei and Kao, Ying-Jer},
 year = {2021},
 title = {An end-to-end trainable hybrid classical-quantum classifier},
 keywords = {principal component analysis;quantum machine learning;tensor network;variational quantum circuit},
 pages = {045021},
 volume = {2},
 number = {4},
 journal = {Machine Learning: Science and Technology},
 doi = {10.1088/2632-2153/ac104d},
 file = {}
}


@inproceedings{Chen.2018b,
 abstract = {2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC);2018; ; ;10.1109/YAC.2018.8406391    },
 author = {Chen, Y. W. and Guo, K. and Pan, Y.},
 title = {Robust supervised learning based on tensor network method},
 keywords = {Matrix product state;supervised learning;tensor network;tensor train},
 pages = {311--315},
 publisher = {IEEE},
 isbn = {978-1-5386-7255-6},
 booktitle = {2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC)},
 year = {2018},
 doi = {10.1109/YAC.2018.8406391},
 file = {}
}


@misc{Chen.13.01.2023,
 abstract = {TeD-Q is an open-source software framework for quantum machine learning, variational quantum algorithm (VQA), and simulation of quantum computing. It seamlessly integrates classical machine learning libraries with quantum simulators, giving users the ability to leverage the power of classical machine learning while training quantum machine learning models. TeD-Q supports auto-differentiation that provides backpropagation, parameters shift, and finite difference methods to obtain gradients. With tensor contraction, simulation of quantum circuits with large number of qubits is possible. TeD-Q also provides a graphical mode in which the quantum circuit and the training progress can be visualized in real-time.},
 author = {Chen, Yaocheng and Wu, Xingyao and Kuo, Chung-Yun and {Du Yuxuan} and Tao, Dacheng},
 date = {13.01.2023},
 title = {TeD-Q: a tensor network enhanced distributed hybrid quantum machine  learning framework},
 url = {http://arxiv.org/pdf/2301.05451v1},
 keywords = {Physics - Computational Physics;Quantum Physics},
 file = {}
}


@article{Cheng.2019,
 abstract = {Phys. Rev. B 99, 155131 (2019). doi:10.1103/PhysRevB.99.155131         },
 author = {Cheng, Song and Wang, Lei and Xiang, Tao and Zhang, Pan},
 year = {2019},
 title = {Tree tensor networks for generative modeling},
 keywords = {doi:10.1103/PhysRevB.99.155131          url:https://doi.org/10.1103/PhysRevB.99.155131},
 volume = {99},
 number = {15},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.99.155131},
 file = {}
}


@article{Cichocki.2016,
 abstract = {Machine learning and data mining algorithms are becoming increasingly important in analyzing large volume, multi-relational and multi--modal datasets, which are often conveniently represented as multiway arrays or tensors. It is therefore timely and valuable for the multidisciplinary research community to review tensor decompositions and tensor networks as emerging tools for large-scale data analysis and data mining. We provide the mathematical and graphical representations and interpretation of tensor networks, with the main focus on the Tucker and Tensor Train (TT) decompositions and their extensions or generalizations.  Keywords: Tensor networks, Function-related tensors, CP decomposition, Tucker models, tensor train (TT) decompositions, matrix product states (MPS), matrix product operators (MPO), basic tensor operations, multiway component analysis, multilinear blind source separation, tensor completion, linear/multilinear dimensionality reduction, large-scale optimization problems, symmetric eigenvalue decomposition (EVD), PCA/SVD, huge systems of linear equations, pseudo-inverse of very large matrices, Lasso and Canonical Correlation Analysis (CCA) (This is Part 1)},
 author = {Cichocki, A. and Lee, N. and Oseledets, I. V. and Phan, A. -H and Zhao, Q. and Mandic, D.},
 year = {2016},
 title = {Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale  Optimization Problems: Perspectives and Challenges PART 1},
 url = {http://arxiv.org/pdf/1609.00893v3},
 keywords = {Computer Science - Numerical Analysis},
 pages = {249--429},
 volume = {9},
 number = {4-5},
 issn = {1935-8237},
 journal = {Foundations and Trends in Machine Learning},
 doi = {10.1561/2200000059},
 file = {}
}

@book{Hochreiter.1991,
 author = {Hochreiter, Josef},
 year = {1991},
 title = {Untersuchungen zu dynamischen neuronalen Netzen},
 address = {M{\"u}nchen},
 institution = {{Technische Universitat Munchen}},
 file = {},
 publisher = {thesis}
}

@article{Pesah.2021,
 abstract = {doi:10.1103/PhysRevX.11.041011   url:https://doi.org/10.1103/PhysRevX.11.041011 },
 author = {Pesah, Arthur and Cerezo, M. and Wang, Samson and Volkoff, Tyler and Sornborger, Andrew T. and Coles, Patrick J.},
 year = {2021},
 title = {Absence of Barren Plateaus in Quantum Convolutional Neural Networks},
 keywords = {doi:10.1103/PhysRevX.11.041011   url:https://doi.org/10.1103/PhysRevX.11.041011 },
 volume = {11},
 number = {4},
 journal = {Physical Review X},
 doi = {10.1103/PhysRevX.11.041011},
 file = {}
}

@misc{Zhang.12.11.2020,
 abstract = {Quantum Neural Networks (QNNs) have been recently proposed as generalizations of classical neural networks to achieve the quantum speed-up. Despite the potential to outperform classical models, serious bottlenecks exist for training QNNs; namely, QNNs with random structures have poor trainability due to the vanishing gradient with rate exponential to the input qubit number. The vanishing gradient could seriously influence the applications of large-size QNNs. In this work, we provide a viable solution with theoretical guarantees. Specifically, we prove that QNNs with tree tensor and step controlled architectures have gradients that vanish at most polynomially with the qubit number. We numerically demonstrate QNNs with tree tensor and step controlled structures for the application of binary classification. Simulations show faster convergent rates and better accuracy compared to QNNs with random structures.},
 author = {Zhang, Kaining and Hsieh, Min-Hsiu and Liu, Liu and Tao, Dacheng},
 date = {12.11.2020},
 title = {Toward Trainability of Quantum Neural Networks},
 url = {http://arxiv.org/pdf/2011.06258v2},
 keywords = {Quantum Physics},
 file = {}
}

@article{McClean.2018,
 abstract = {Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design characteristic of random circuits, and that solutions to this problem must be studied.},
 author = {McClean, Jarrod R. and Boixo, Sergio and Smelyanskiy, Vadim N. and Babbush, Ryan and Neven, Hartmut},
 year = {2018},
 title = {Barren plateaus in quantum neural network training landscapes},
 pages = {4812},
 volume = {9},
 number = {1},
 journal = {Nature communications},
 doi = {10.1038/s41467-018-07090-4},
 file = {}
}

@article{Cichocki.2016b,
 abstract = {Part 2 of this monograph builds on the introduction to tensor networks and their operations presented in Part 1. It focuses on tensor network models for super-compressed higher-order representation of data/parameters and related cost functions, while providing an outline of their applications in machine learning and data analytics. A particular emphasis is on the tensor train (TT) and Hierarchical Tucker (HT) decompositions, and their physically meaningful interpretations which reflect the scalability of the tensor network approach. Through a graphical approach, we also elucidate how, by virtue of the underlying low-rank tensor approximations and sophisticated contractions of core tensors, tensor networks have the ability to perform distributed computations on otherwise prohibitively large volumes of data/parameters, thereby alleviating or even eliminating the curse of dimensionality. The usefulness of this concept is illustrated over a number of applied areas, including generalized regression and classification (support tensor machines, canonical correlation analysis, higher order partial least squares), generalized eigenvalue decomposition, Riemannian optimization, and in the optimization of deep neural networks. Part 1 and Part 2 of this work can be used either as stand-alone separate texts, or indeed as a conjoint comprehensive review of the exciting field of low-rank tensor networks and tensor decompositions.},
 author = {Cichocki, A. and Phan, A-H and Zhao, Q. and Lee, N. and Oseledets, I. V. and Sugiyama, M. and Mandic, D.},
 year = {2016},
 title = {Tensor Networks for Dimensionality Reduction and Large-Scale  Optimizations. Part 2 Applications and Future Perspectives},
 url = {http://arxiv.org/pdf/1708.09165v1},
 keywords = {Computer Science - Learning;Computer Science - Numerical Analysis},
 pages = {249--429},
 volume = {9},
 number = {6},
 issn = {1935-8237},
 journal = {Foundations and Trends in Machine Learning},
 doi = {10.1561/2200000067},
 file = {}
}


@article{Cincio.2008,
 abstract = {We propose a symmetric version of the multiscale entanglement renormalization ansatz in two spatial dimensions (2D) and use this ansatz to find an unknown ground state of a 2D quantum system. Results in the simple 2D quantum Ising model on the 8x8 square lattice are found to be very accurate even with the smallest nontrivial truncation parameter.},
 author = {Cincio, Lukasz and Dziarmaga, Jacek and Rams, Marek M.},
 year = {2008},
 title = {Multiscale entanglement renormalization ansatz in two dimensions: quantum Ising model},
 pages = {240603},
 volume = {100},
 number = {24},
 issn = {0031-9007},
 journal = {Physical review letters},
 doi = {10.1103/PhysRevLett.100.240603},
 file = {}
}


@article{Cirac.2021,
 abstract = {The theory of entanglement provides a fundamentally new language for describing interactions and correlations in many body systems. Its vocabulary consists of qubits and entangled pairs, and the syntax is provided by tensor networks. We review how matrix product states and projected entangled pair states describe many-body wavefunctions in terms of local tensors. These tensors express how the entanglement is routed, act as a novel type of non-local order parameter, and we describe how their symmetries are reflections of the global entanglement patterns in the full system. We will discuss how tensor networks enable the construction of real-space renormalization group flows and fixed points, and examine the entanglement structure of states exhibiting topological quantum order. Finally, we provide a summary of the mathematical results of matrix product states and projected entangled pair states, highlighting the fundamental theorem of matrix product vectors and its applications.},
 author = {Cirac, Ignacio and Perez-Garcia, David and Schuch, Norbert and Verstraete, Frank},
 year = {2021},
 title = {Matrix Product States and Projected Entangled Pair States: Concepts,  Symmetries, and Theorems},
 url = {http://arxiv.org/pdf/2011.12127v2},
 keywords = {High Energy Physics - Theory;Physics - Statistical Mechanics;Physics - Strongly Correlated Electrons;Quantum Physics},
 pages = {959},
 volume = {93},
 number = {4},
 issn = {0034-6861},
 journal = {Reviews of Modern Physics},
 doi = {10.1103/RevModPhys.93.045003},
 file = {}
}


@article{Chen.2021b,
 abstract = {Machine Learning: Science and Technology, 2 (2021) 045021 doi: 10.1088/2632-2153/ac104d },
 author = {Chen, Samuel Yen-Chi and Huang, Chih-Min and Hsing, Chia-Wei and Kao, Ying-Jer},
 year = {2021},
 title = {An end-to-end trainable hybrid classical-quantum classifier},
 keywords = {principal component analysis;quantum machine learning;tensor network;variational quantum circuit},
 pages = {045021},
 volume = {2},
 number = {4},
 journal = {Machine Learning: Science and Technology},
 doi = {10.1088/2632-2153/ac104d},
 file = {}
}


@article{Bhatia.2018,
 abstract = {Quantum Information Processing, https://doi.org/10.1007/s11128-018-2053-0 },
 author = {Bhatia, Amandeep Singh and Kumar, Ajay},
 year = {2018},
 title = {Neurocomputing approach to matrix product state using quantum dynamics},
 keywords = {Matrix product state;Quantum dynamics;Quantum probabilistic logic node;Quantum RAM-based node;Quantum weightless neural networks},
 volume = {17},
 number = {10},
 issn = {1570-0755},
 journal = {Quantum Information Processing},
 doi = {10.1007/s11128-018-2053-0},
 file = {}
}


@misc{Beny.14.01.2013,
 abstract = {Renormalization group (RG) methods, which model the way in which the effective behavior of a system depends on the scale at which it is observed, are key to modern condensed-matter theory and particle physics. We compare the ideas behind the RG on the one hand and deep machine learning on the other, where depth and scale play a similar role. In order to illustrate this connection, we review a recent numerical method based on the RG---the multiscale entanglement renormalization ansatz (MERA)---and show how it can be converted into a learning algorithm based on a generative hierarchical Bayesian network model. Under the assumption---common in physics---that the distribution to be learned is fully characterized by local correlations, this algorithm involves only explicit evaluation of probabilities, hence doing away with sampling.},
 author = {B{\'e}ny, C{\'e}dric},
 date = {14.01.2013},
 title = {Deep learning and the renormalization group},
 url = {http://arxiv.org/pdf/1301.3124v4},
 keywords = {Quantum Physics},
 file = {}
}


@article{Batselier.2018,
 author = {Batselier, Kim and Ko, Ching-Yun and Phan, Anh-Huy and Cichocki, Andrzej and Wong, Ngai},
 year = {2018},
 title = {Multilinear state space system identification with matrix product operators},
 keywords = {Matrix product operator;Multilinear systems;Nonlinear system identification;Polynomial state space models;Tensors},
 pages = {640--645},
 volume = {51},
 number = {15},
 issn = {24058963},
 journal = {IFAC-PapersOnLine},
 doi = {10.1016/j.ifacol.2018.09.219},
 file = {}
}


@proceedings{.2016,
 year = {2016},
 title = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1}
}


@proceedings{.2018,
 year = {2018},
 title = {2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC)},
 publisher = {IEEE},
 isbn = {978-1-5386-7255-6}
}


@proceedings{.2020,
 year = {2020},
 title = {2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)},
 publisher = {IEEE},
 isbn = {978-1-7281-4149-7},
 doi = {10.1109/ITSC45102.2020}
}


@proceedings{.2021,
 year = {2021},
 title = {2021 International Conference on Artificial Intelligence and Computer Science Technology (ICAICST)},
 publisher = {IEEE},
 isbn = {978-1-6654-2404-2}
}


@book{AashikChandramohan.2019,
 abstract = {2019 Wireless Days (WD);2019; ; ;},
 author = {{Aashik Chandramohan} and {Mannes Poel} and {Bernd Meijerink} and {Geert Heijenk}},
 year = {2019},
 title = {2019 Wireless Days (WD)},
 url = {http://ieeexplore.ieee.org/servlet/opac?punumber=8731789},
 keywords = {Autonomous driving;Cooperative driving;Highway environment;Q learning;Reinforcement Learning;SUMO},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {9781728101170},
 file = {}
}


@article{Abbas.2021,
 abstract = {Nature Computational Science, doi:10.1038/s43588-021-00084-1  }, 
 author = {Abbas, Amira and Sutter, David and Zoufal, Christa and Lucchi, Aurelien and Figalli, Alessio and Woerner, Stefan},
 year = {2021},
 title = {The power of quantum neural networks},
 pages = {403--409},
 volume = {1},
 number = {6},
 journal = {Nature Computational Science},
 doi = {10.1038/s43588-021-00084-1},
 file = {}
}


@misc{Alcazar.15.01.2021,
 abstract = {We introduce a new framework that leverages machine learning models known as generative models to solve optimization problems. Our Generator-Enhanced Optimization (GEO) strategy is flexible to adopt any generative model, from quantum to quantum-inspired or classical, such as Generative Adversarial Networks, Variational Autoencoders, or Quantum Circuit Born Machines, to name a few. Here, we focus on a quantum-inspired version of GEO relying on tensor-network Born machines and referred to hereafter as TN-GEO. We present two prominent strategies for using TN-GEO. The first uses data points previously evaluated by any quantum or classical optimizer, and we show how TN-GEO improves the performance of the classical solver as a standalone strategy in hard-to-solve instances. The second strategy uses TN-GEO as a standalone solver, i.e., when no previous observations are available. Here, we show its superior performance when the goal is to find the best minimum given a fixed budget for the number of function calls. This might be ideal in situations where the cost function evaluation can be very expensive. To illustrate our results, we run these benchmarks in the context of the portfolio optimization problem by constructing instances from the S\&P 500 and several other financial stock indexes. We also comprehensively compare state-of-the-art algorithms in a generalized version of the portfolio optimization problem. The results show that TN-GEO is among the best compared to these state-of-the-art algorithms; a remarkable outcome given the solvers used in the comparison have been fine-tuned for decades in this real-world industrial application. We see this as an important step toward a practical advantage with quantum-inspired models and, subsequently, with quantum generative models.},
 author = {Alcazar, Javier and Vakili, Mohammad Ghazi and Kalayci, Can B. and Perdomo-Ortiz, Alejandro},
 date = {15.01.2021},
 title = {GEO: Enhancing Combinatorial Optimization with Classical and Quantum  Generative Models},
 url = {http://arxiv.org/pdf/2101.06250v2},
 keywords = {Quantum Physics},
 file = {}
}

@inproceedings{Novikov.2015,
 author = {Novikov, Alexander and Podoprikhin, Dmitrii and Osokin, Anton and Vetrov, Dmitry P.},
 title = {Tensorizing Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2015/file/6855456e2fe46a9d49d3d3af4f57443d-Paper.pdf},
 volume = {28},
 publisher = {{Curran Associates, Inc}},
 editor = {{C. Cortes} and {N. Lawrence} and {D. Lee} and {M. Sugiyama} and {R. Garnett}},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2015}
}





@article{Anand.2021,
 abstract = {Natural evolutionary strategies (NES) are a family of gradient-free black-box optimization algorithms. This study illustrates their use for the optimization of randomly-initialized parametrized quantum circuits (PQCs) in the region of vanishing gradients. We show that using the NES gradient estimator the exponential decrease in variance can be alleviated. We implement two specific approaches, the exponential and separable natural evolutionary strategies, for parameter optimization of PQCs and compare them against standard gradient descent. We apply them to two different problems of ground state energy estimation using variational quantum eigensolver (VQE) and state preparation with circuits of varying depth and length. We also introduce batch optimization for circuits with larger depth to extend the use of evolutionary strategies to a larger number of parameters. We achieve accuracy comparable to state-of-the-art optimization techniques in all the above cases with a lower number of circuit evaluations. Our empirical results indicate that one can use NES as a hybrid tool in tandem with other gradient-based methods for optimization of deep quantum circuits in regions with vanishing gradients.},
 author = {Anand, Abhinav and Degroote, Matthias and Aspuru-Guzik, Al{\'a}n},
 year = {2021},
 title = {Natural Evolutionary Strategies for Variational Quantum Computation},
 url = {http://arxiv.org/pdf/2012.00101v2},
 keywords = {Computer Science - Learning;Computer Science - Neural and Evolutionary Computing;Quantum Physics},
 pages = {045012},
 volume = {2},
 number = {4},
 journal = {Machine Learning: Science and Technology},
 doi = {10.1088/2632-2153/abf3ac},
 file = {}
}


@article{Araz.2021,
 author = {Araz, Jack Y. and Spannowsky, Michael},
 year = {2021},
 title = {Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States},
 volume = {2021},
 number = {8},
 journal = {Journal of High Energy Physics},
 doi = {10.1007/JHEP08(2021)112},
 file = {}
}


@misc{Araz.21.02.2022,
 abstract = {Tensor Networks (TN) are approximations of high-dimensional tensors designed to represent locally entangled quantum many-body systems efficiently. This study provides a comprehensive comparison between classical TNs and TN-inspired quantum circuits in the context of Machine Learning on highly complex, simulated LHC data. We show that classical TNs require exponentially large bond dimensions and higher Hilbert-space mapping to perform comparably to their quantum counterparts. While such an expansion in the dimensionality allows better performance, we observe that, with increased dimensionality, classical TNs lead to a highly flat loss landscape, rendering the usage of gradient-based optimization methods highly challenging. Furthermore, by employing quantitative metrics, such as the Fisher information and effective dimensions, we show that classical TNs require a more extensive training sample to represent the data as efficiently as TN-inspired quantum circuits. We also engage with the idea of hybrid classical-quantum TNs and show possible architectures to employ a larger phase-space from the data. We offer our results using three main TN ansatz: Tree Tensor Networks, Matrix Product States, and Multi-scale Entanglement Renormalisation Ansatz.},
 author = {Araz, Jack Y. and Spannowsky, Michael},
 date = {21.02.2022},
 title = {Classical versus Quantum: comparing Tensor Network-based Quantum  Circuits on LHC data},
 url = {http://arxiv.org/pdf/2202.10471v1},
 keywords = {Computer Science - Learning;High Energy Physics - Experiment;High Energy Physics - Phenomenology;Quantum Physics},
 file = {}
}


@article{Bai.2020,
 abstract = {New Journal of Physics, 22(2020) 043015. doi:10.1088/1367-2630/ab7a34 },
 author = {Bai, Ge and Yang, Yuxiang and Chiribella, Giulio},
 year = {2020},
 title = {Quantum compression of tensor network states},
 keywords = {matrix product states;quantum data compression;quantum machine learning;quantum many-body systems;tensor networks},
 pages = {043015},
 volume = {22},
 number = {4},
 journal = {New Journal of Physics},
 doi = {10.1088/1367-2630/ab7a34},
 file = {}
}


@article{Bai.2022,
 abstract = {Given an image of a white shoe drawn on a blackboard, how are the white pixels deemed (say by human minds) to be informative for recognizing the shoe without any labeling information on the pixels? Here we investigate such a ``white shoe'' recognition problem from the perspective of tensor network (TN) machine learning and quantum entanglement. Utilizing a generative TN that captures the probability distribution of the features as quantum amplitudes, we propose an unsupervised recognition scheme of informative features with the variations of entanglement entropy (EE) caused by designed measurements. In this way, a given sample, where the values of its features are statistically meaningless, is mapped to the variations of EE that statistically characterize the gain of information. We show that the EE variations identify the features that are critical to recognize this specific sample, and the EE itself reveals the information distribution of the probabilities represented by the TN model. The signs of the variations further reveal the entanglement structures among the features. We test the validity of our scheme on a toy dataset of strip images, the MNIST dataset of hand-drawn digits, the fashion-MNIST dataset of the pictures of fashion articles, and the images of brain cells. Our scheme opens the avenue to the quantum-inspired and interpreted unsupervised learning, which can be applied to, e.g., image segmentation and object detection.},
 author = {Bai, Sheng-Chen and Tang, Yi-Cheng and Ran, Shi-Ju},
 year = {2022},
 title = {Unsupervised Recognition of Informative Features via Tensor Network  Machine Learning and Quantum Entanglement Variations},
 url = {http://arxiv.org/pdf/2207.06031v2},
 keywords = {Computer Science - Learning;Physics - Data Analysis;Physics - Strongly Correlated Electrons;Quantum Physics;Statistics and Probability},
 pages = {100701},
 volume = {39},
 number = {10},
 issn = {0256-307X},
 journal = {Chinese Physics Letters},
 doi = {10.1088/0256-307X/39/10/100701},
 file = {}
}


@article{Barratt.2021,
 abstract = {npj Quantum Information, doi:10.1038/s41534-021-00420-3 },  
 author = {Barratt, F. and Dborin, James and Bal, Matthias and Stojevic, Vid and Pollmann, Frank and Green, A. G.},
 year = {2021},
 title = {Parallel quantum simulation of large systems on small NISQ computers},
 volume = {7},
 number = {1},
 journal = {npj Quantum Information},
 doi = {10.1038/s41534-021-00420-3},
 file = {}
}

@article{Barratt.2022,
 abstract = {Tensor networks have demonstrated significant value for machine learning in a myriad of different applications. However, optimizing tensor networks using standard gradient descent has proven to be difficult in practice. Tensor networks suffer from initialization problems resulting in exploding or vanishing gradients and require extensive hyperparameter tuning. Efforts to overcome these problems usually depend on specific network architectures, or ad hoc prescriptions. In this paper we address the problems of initialization and hyperparameter tuning, making it possible to train tensor networks using established machine learning techniques. We introduce a `copy node' method that successfully initializes arbitrary tensor networks, in addition to a gradient based regularization technique for bond dimensions. We present numerical results that show that the combination of techniques presented here produces quantum inspired tensor network models with far fewer parameters, while improving generalization performance.},
 author = {Barratt, Fergus and Dborin, James and Wright, Lewis},
 year = {2022},
 title = {Improvements to Gradient Descent Methods for Quantum Tensor Network  Machine Learning},
 url = {http://arxiv.org/pdf/2203.03366v1},
 keywords = {Computer Science - Learning;Physics - Strongly Correlated Electrons;Quantum Physics},
 journal = {Second Workshop on Quantum Tensor Networks in Machine Learning},
 file = {}
}

@article{Batselier.2022,
 abstract = {IEEE Control Systems;2022;42;1;10.1109/MCS.2021.3122268   },
 author = {Batselier, Kim},
 year = {2022},
 title = {Low-Rank Tensor Decompositions for Nonlinear System Identification: A Tutorial with Examples},
 pages = {54--74},
 volume = {42},
 number = {1},
 issn = {1066-033X},
 journal = {IEEE Control Systems},
 doi = {10.1109/MCS.2021.3122268},
 file = {}
}


@article{Batselier.2017,
 abstract = {Automatica, 84 (2017) 26-35. doi:10.1016/j .automatica.2017.06.033     },
 author = {Batselier, Kim and Chen, Zhongming and Wong, Ngai},
 year = {2017},
 title = {Tensor Network alternating linear scheme for MIMO Volterra system identification},
 pages = {26--35},
 volume = {84},
 issn = {00051098},
 journal = {Automatica},
 doi = {10.1016/j.automatica.2017.06.033},
 file = {}
}


@proceedings{Cohen.2008,
 year = {2008},
 title = {Proceedings of the 25th international conference on Machine learning - ICML '08},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781605582054},
 editor = {Cohen, William and McCallum, Andrew and Roweis, Sam},
 doi = {10.1145/1390156}
}


@article{Collura.2021,
 author = {Collura, Mario and Dell'Anna, Luca and Felser, Timo and Montangero, Simone},
 year = {2021},
 title = {On the descriptive power of Neural-Networks as constrained Tensor Networks with exponentially large bond dimension},
 volume = {4},
 number = {1},
 journal = {SciPost Physics Core},
 doi = {10.21468/SciPostPhysCore.4.1.001},
 file = {}
}


@article{Convy.2022,
 abstract = {Tensor networks have emerged as promising tools for machine learning, inspired by their widespread use as variational ansatze in quantum many-body physics. It is well known that the success of a given tensor network ansatz depends in part on how well it can reproduce the underlying entanglement structure of the target state, with different network designs favoring different scaling patterns. We demonstrate here how a related correlation analysis can be applied to tensor network machine learning, and explore whether classical data possess correlation scaling patterns similar to those found in quantum states which might indicate the best network to use for a given dataset. We utilize mutual information as measure of correlations in classical data, and show that it can serve as a lower-bound on the entanglement needed for a probabilistic tensor network classifier. We then develop a logistic regression algorithm to estimate the mutual information between bipartitions of data features, and verify its accuracy on a set of Gaussian distributions designed to mimic different correlation patterns. Using this algorithm, we characterize the scaling patterns in the MNIST and Tiny Images datasets, and find clear evidence of boundary-law scaling in the latter. This quantum-inspired classical analysis offers insight into the design of tensor networks which are best suited for specific learning tasks.},
 author = {Convy, Ian and Huggins, William and Liao, Haoran and Whaley, K. Birgitta},
 year = {2022},
 title = {Mutual Information Scaling for Tensor Network Machine Learning},
 keywords = {area law;mutual information;tensor network machine learning},
 volume = {3},
 number = {1},
 journal = {Machine learning: science and technology},
 doi = {10.1088/2632-2153/ac44a9},
 file = {}
}


@article{Daley.2004,
 author = {Daley, A. J. and Kollath, C. and Schollw{\"o}ck, U. and Vidal, G.},
 year = {2004},
 title = {Time-dependent density-matrix renormalization-group using adaptive effective Hilbert spaces},
 pages = {P04005},
 volume = {2004},
 number = {04},
 journal = {Journal of Statistical Mechanics: Theory and Experiment},
 doi = {10.1088/1742-5468/2004/04/P04005}
}


@book{Grohs.2023,
 abstract = {In recent years the development of new classification and regression algorithms based on deep learning has led to a revolution in the fields of artificial intelligence, machine learning, and data analysis. The development of a theoretical foundation to guarantee the success of these algorithms constitutes one of the most active and exciting research topics in applied mathematics. This book presents the current mathematical understanding of deep learning methods from the point of view of the leading experts in the field. It serves both as a starting point for researchers and graduate students in computer science, mathematics, and statistics trying to get into the field and as an invaluable reference for future research.},
 year = {2023},
 title = {Mathematical aspects of deep learning},
 address = {Cambridge},
 publisher = {{Cambridge University Press}},
 isbn = {9781009025096},
 editor = {Grohs, Philipp and Kutyniok, Gitta},
 doi = {10.1017/9781009025096}
}


@misc{Guala.2022,
 author = {Guala, Diego and Cruz-Rico, Esther and Zhang, Shaoming and Arrazola, Juan Miguel},
 year = {2022},
 title = {Tensor-Network Quantum Circuits},
 url = {https://pennylane.ai/qml/demos/tutorial_tn_circuits.html},
 urldate = {25.8.2022}
}


@article{Guo.2020,
 abstract = {We show how to learn structures of generic, non-Markovian, quantum stochastic processes using a tensor network based machine learning algorithm. We do this by representing the process as a matrix product operator (MPO) and train it with a database of local input states at different times and the corresponding time-nonlocal output state. In particular, we analyze a qubit coupled to an environment and predict output state of the system at different time, as well as reconstruct the full system process. We show how the bond dimension of the MPO, a measure of non-Markovianity, depends on the properties of the system, of the environment and of their interaction. Hence, this study opens the way to a possible experimental investigation into the process tensor and its properties.},
 author = {Guo, Chu and Modi, Kavan and Poletti, Dario},
 year = {2020},
 title = {Tensor network based machine learning of non-Markovian quantum processes},
 url = {http://arxiv.org/pdf/2004.11038v1},
 keywords = {Quantum Physics},
 volume = {102},
 number = {6},
 issn = {2469-9926},
 journal = {Physical Review A},
 doi = {10.1103/PhysRevA.102.062414},
 file = {}
}


@article{H.Tagare.,
 author = {{H. Tagare}},
 title = {Notes on Optimization on Stiefel Manifolds},
 file = {}
}


@article{Haegeman.2013,
 author = {Haegeman, Jutho and Osborne, Tobias J. and Verstraete, Frank},
 year = {2013},
 title = {Post-matrix product state methods: To tangent space and beyond},
 volume = {88},
 number = {7},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.88.075133},
 file = {}
}


@article{Han.2018,
 abstract = {doi:10.1103/PhysRevX.8.031012  url:https://doi.org/10.1103/PhysRevX.8.031012 },
 author = {Han, Zhao-Yu and Wang, Jun and Fan, Heng and Wang, Lei and Zhang, Pan},
 year = {2018},
 title = {Unsupervised Generative Modeling Using Matrix Product States},
 keywords = {doi:10.1103/PhysRevX.8.031012  url:https://doi.org/10.1103/PhysRevX.8.031012 },
 volume = {8},
 number = {3},
 journal = {Physical Review X},
 doi = {10.1103/PhysRevX.8.031012},
 file = {}
}


@article{Hauru.2021,
 author = {Hauru, Markus and {van Damme}, Maarten and Haegeman, Jutho},
 year = {2021},
 title = {Riemannian optimization of isometric tensor networks},
 volume = {10},
 number = {2},
 journal = {SciPost Physics},
 doi = {10.21468/SciPostPhys.10.2.040},
 file = {}
}


@article{Hayden.2016,
 author = {Hayden, Patrick and Nezami, Sepehr and Qi, Xiao-Liang and Thomas, Nathaniel and Walter, Michael and Yang, Zhao},
 year = {2016},
 title = {Holographic duality from random tensor networks},
 volume = {2016},
 number = {11},
 journal = {Journal of High Energy Physics},
 doi = {10.1007/JHEP11(2016)009},
 file = {}
}


@article{Huang.2021,
 abstract = {The use of quantum computing for machine learning is among the most exciting prospective applications of quantum technologies. However, machine learning tasks where data is provided can be considerably different than commonly studied computational tasks. In this work, we show that some problems that are classically hard to compute can be easily predicted by classical machines learning from data. Using rigorous prediction error bounds as a foundation, we develop a methodology for assessing potential quantum advantage in learning tasks. The bounds are tight asymptotically and empirically predictive for a wide range of learning models. These constructions explain numerical results showing that with the help of data, classical machine learning models can be competitive with quantum models even if they are tailored to quantum problems. We then propose a projected quantum model that provides a simple and rigorous quantum speed-up for a learning problem in the fault-tolerant regime. For near-term implementations, we demonstrate a significant prediction advantage over some classical models on engineered data sets designed to demonstrate a maximal quantum advantage in one of the largest numerical tests for gate-based quantum machine learning to date, up to 30 qubits.},
 author = {Huang, Hsin-Yuan and Broughton, Michael and Mohseni, Masoud and Babbush, Ryan and Boixo, Sergio and Neven, Hartmut and McClean, Jarrod R.},
 year = {2021},
 title = {Power of data in quantum machine learning},
 pages = {2631},
 volume = {12},
 number = {1},
 journal = {Nature communications},
 doi = {10.1038/s41467-021-22539-9},
 file = {}
}


@article{Huggins.2019,
 abstract = {Quantum Science and Technology, 4(2019) 024001. doi:10.1088/2058-9565/aaea94   },
 author = {Huggins, William and Patil, Piyush and Mitchell, Bradley and Whaley, K. Birgitta and Stoudenmire, E. Miles},
 year = {2019},
 title = {Towards quantum machine learning with tensor networks},
 keywords = {machine learning;quantum computing;tensor networks},
 pages = {024001},
 volume = {4},
 number = {2},
 issn = {2058-9565},
 journal = {Quantum Science and Technology},
 doi = {10.1088/2058-9565/aaea94},
 file = {}
}


@article{IgnacioCirac.2017,
 author = {{Ignacio Cirac}, J. and Perez-Garcia, David and Schuch, Norbert and Verstraete, Frank},
 year = {2017},
 title = {Matrix product unitaries: structure, symmetries, and topological invariants},
 pages = {083105},
 volume = {2017},
 number = {8},
 journal = {Journal of Statistical Mechanics: Theory and Experiment},
 doi = {10.1088/1742-5468/aa7e55},
 file = {}
}


@misc{Jaschke.24.05.2022,
 abstract = {The quantum advantage threshold determines when a quantum processing unit (QPU) is more efficient with respect to classical computing hardware in terms of algorithmic complexity. The {\textquotedbl}green{\textquotedbl} quantum advantage threshold $-$ based on a comparison of energetic efficiency between the two $-$ is going to play a fundamental role in the comparison between quantum and classical hardware. Indeed, its characterization would enable better decisions on energy-saving strategies, e.g. for distributing the workload in hybrid quantum-classical algorithms. Here, we show that the green quantum advantage threshold crucially depends on (i) the quality of the experimental quantum gates and (ii) the entanglement generated in the QPU. Indeed, for NISQ hardware and algorithms requiring a moderate amount of entanglement, a classical tensor network emulation can be more energy-efficient at equal final state fidelity than quantum computation. We compute the green quantum advantage threshold for a few paradigmatic examples in terms of algorithms and hardware platforms, and identify algorithms with a power-law decay of singular values of bipartitions $-$ with power-law exponent {\$}$\backslash$alpha \lesssim 1{\$} $-$ as the green quantum advantage threshold in the near future.},
 author = {Jaschke, Daniel and Montangero, Simone},
 date = {24.05.2022},
 title = {Is quantum computing green? An estimate for an energy-efficiency quantum  advantage},
 url = {http://arxiv.org/pdf/2205.12092v2},
 keywords = {Quantum Physics},
 file = {}
}


@article{Karagoz.2020,
 abstract = {Automatica, 122 (2020) 109300. doi:10.1016/j.automatica.2020.109300  },
 author = {Karagoz, Ridvan and Batselier, Kim},
 year = {2020},
 title = {Nonlinear system identification with regularized Tensor Network B-splines},
 pages = {109300},
 volume = {122},
 issn = {00051098},
 journal = {Automatica},
 doi = {10.1016/j.automatica.2020.109300},
 file = {}
}


@article{Kardashin.2021,
 author = {Kardashin, Andrey and Uvarov, Alexey and Biamonte, Jacob},
 year = {2021},
 title = {Quantum Machine Learning Tensor Network States},
 volume = {8},
 journal = {Frontiers in Physics},
 doi = {10.3389/fphy.2020.586374},
 file = {}
}


@article{Kiran.2022,
 author = {Kiran, B. Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Sallab, Ahmad A. Al and Yogamani, Senthil and Perez, Patrick},
 year = {2022},
 title = {Deep Reinforcement Learning for Autonomous Driving: A Survey},
 pages = {4909--4926},
 volume = {23},
 number = {6},
 issn = {1524-9050},
 journal = {IEEE Transactions on Intelligent Transportation Systems},
 doi = {10.1109/TITS.2021.3054625},
 file = {}
}

@article{Kong.,
 abstract = {Tiny object classification problem exists in many machine learning applications like medical imaging or remote sensing, where the object of interest usually occupies a small region of the whole image. It is challenging to design an efficient machine learning model with respect to tiny object of interest. Current neural network structures are unable to deal with tiny object efficiently because they are mainly developed for images featured by large scale objects. However, in quantum physics, there is a great theoretical foundation guiding us to analyze the target function for image classification regarding to specific objects size ratio. In our work, we apply Tensor Networks to solve this arising tough machine learning problem. First, we summarize the previous work that connects quantum spin model to image classification and bring the theory into the scenario of tiny object classification. Second, we propose using 2D multi-scale entanglement renormalization ansatz (MERA) to classify tiny objects in image. In the end, our experimental results indicate that tensor network models are effective for tiny object classification problem and potentially will beat state-of-the-art. Our codes will be available online https://github.com/timqqt/MERA{\_}Image{\_}Classification.},
 author = {Kong, Fanjie and Liu, Xiao-yang and Henao, Ricardo},
 title = {Quantum Tensor Network in Machine Learning: An Application to Tiny  Object Classification},
 url = {http://arxiv.org/pdf/2101.03154v1},
 keywords = {Computer Science - Computer Vision and Pattern Recognition;Computer Science - Learning},
 journal = {34th Conference on Neural Information},
 file = {},
 year={2020}
}



@misc{Kottmann.20.10.2022,
 abstract = {We perform quantum simulation on classical and quantum computers and set up a machine learning framework in which we can map out phase diagrams of known and unknown quantum many-body systems in an unsupervised fashion. The classical simulations are done with state-of-the-art tensor network methods in one and two spatial dimensions. For one dimensional systems, we utilize matrix product states (MPS) that have many practical advantages and can be optimized using the efficient density matrix renormalization group (DMRG) algorithm. The data for two dimensional systems is obtained from entangled projected pair states (PEPS) optimized via imaginary time evolution. Data in form of observables, entanglement spectra, or parts of the state vectors from these simulations, is then fed into a deep learning (DL) pipeline where we perform anomaly detection to map out the phase diagram. We extend this notion to quantum computers and introduce quantum variational anomaly detection. Here, we first simulate the ground state and then process it in a quantum machine learning (QML) manner. Both simulation and QML routines are performed on the same device, which we demonstrate both in classical simulation and on a physical quantum computer hosted by IBM.},
 author = {Kottmann, Korbinian},
 date = {20.10.2022},
 title = {Investigating Quantum Many-Body Systems with Tensor Networks, Machine  Learning and Quantum Computers},
 url = {http://arxiv.org/pdf/2210.11130v1},
 keywords = {Quantum Physics},
 file = {}
}


@article{Grant.2018,
 author = {Grant, Edward and Benedetti, Marcello and Cao, Shuxiang and Hallam, Andrew and Lockhart, Joshua and Stojevic, Vid and Green, Andrew G. and Severini, Simone},
 year = {2018},
 title = {Hierarchical quantum classifiers},
 volume = {4},
 number = {1},
 journal = {npj Quantum Information},
 doi = {10.1038/s41534-018-0116-9},
 file = {}
}


@article{Zaletel.2020,
 abstract = {Tensor-network states (TNS) are a promising but numerically challenging tool for simulating two-dimensional (2D) quantum many-body problems. We introduce an isometric restriction of the TNS ansatz that allows for highly efficient contraction of the network. We consider two concrete applications using this ansatz. First, we show that a matrix-product state representation of a 2D quantum state can be iteratively transformed into an isometric 2D TNS. Second, we introduce a 2D version of the time-evolving block decimation algorithm for approximating of the ground state of a Hamiltonian as an isometric TNS-which we demonstrate for the 2D transverse field Ising model.},
 author = {Zaletel, Michael P. and Pollmann, Frank},
 year = {2020},
 title = {Isometric Tensor Network States in Two Dimensions},
 keywords = {doi:10.1103/PhysRevLett.124.037201      url:https://doi.org/10.1103/PhysRevLett.124.037201 },
 pages = {037201},
 volume = {124},
 number = {3},
 issn = {0031-9007},
 journal = {Physical review letters},
 doi = {10.1103/PhysRevLett.124.037201},
 file = {}
}


@article{Gopalakrishnan.2019,
 abstract = {Phys. Rev. B 100, 064309 (2019). doi:10.1103/PhysRevB.100.064309    },
 author = {Gopalakrishnan, Sarang and Lamacraft, Austen},
 year = {2019},
 title = {Unitary circuits of finite depth and infinite width from quantum channels},
 keywords = {doi:10.1103/PhysRevB.100.064309     url:https://doi.org/10.1103/PhysRevB.100.064309  },
 volume = {100},
 number = {6},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.100.064309},
 file = {}
}


@misc{Glasser.15.06.2018,
 abstract = {Tensor networks have found a wide use in a variety of applications in physics and computer science, recently leading to both theoretical insights as well as practical algorithms in machine learning. In this work we explore the connection between tensor networks and probabilistic graphical models, and show that it motivates the definition of generalized tensor networks where information from a tensor can be copied and reused in other parts of the network. We discuss the relationship between generalized tensor network architectures used in quantum physics, such as string-bond states, and architectures commonly used in machine learning. We provide an algorithm to train these networks in a supervised-learning context and show that they overcome the limitations of regular tensor networks in higher dimensions, while keeping the computation efficient. A method to combine neural networks and tensor networks as part of a common deep learning architecture is also introduced. We benchmark our algorithm for several generalized tensor network architectures on the task of classifying images and sounds, and show that they outperform previously introduced tensor-network algorithms. The models we consider also have a natural implementation on a quantum computer and may guide the development of near-term quantum machine learning architectures.},
 author = {Glasser, Ivan and Pancotti, Nicola and Cirac, J. Ignacio},
 date = {15.06.2018},
 title = {From probabilistic graphical models to generalized tensor networks for  supervised learning},
 url = {http://arxiv.org/pdf/1806.05964v2},
 keywords = {Computer Science - Learning;Physics - Strongly Correlated Electrons;Quantum Physics;Statistics - Machine Learning},
 file = {}
}


@article{Dborin.2022,
 author = {Dborin, James and Barratt, Fergus and Wimalaweera, Vinul and Wright, Lewis and Green, Andrew G.},
 year = {2022},
 title = {Matrix product state pre-training for quantum machine learning},
 pages = {035014},
 volume = {7},
 number = {3},
 issn = {2058-9565},
 journal = {Quantum Science and Technology},
 doi = {10.1088/2058-9565/ac7073},
 file = {}
}


@misc{Dilip.24.04.2022,
 abstract = {The advent of noisy-intermediate scale quantum computers has introduced the exciting possibility of achieving quantum speedups in machine learning tasks. These devices, however, are composed of a small number of qubits, and can faithfully run only short circuits. This puts many proposed approaches for quantum machine learning beyond currently available devices. We address the problem of efficiently compressing and loading classical data for use on a quantum computer. Our proposed methods allow both the required number of qubits and depth of the quantum circuit to be tuned. We achieve this by using a correspondence between matrix-product states and quantum circuits, and further propose a hardware-efficient quantum circuit approach, which we benchmark on the Fashion-MNIST dataset. Finally, we demonstrate that a quantum circuit based classifier can achieve competitive accuracy with current tensor learning methods using only 11 qubits.},
 author = {Dilip, Rohit and Liu, Yu-Jie and Smith, Adam and Pollmann, Frank},
 date = {24.04.2022},
 title = {Data compression for quantum machine learning},
 url = {http://arxiv.org/pdf/2204.11170v2},
 keywords = {Physics - Disordered Systems and Neural Networks;Quantum Physics},
 file = {}
}


@article{Du.2020,
 abstract = {Phys. Rev. Res. 2, 033125 (2020). doi:10.1103/PhysRevResearch.2.033125  },
 author = {Du, Yuxuan and Hsieh, Min-Hsiu and Liu, Tongliang and Tao, Dacheng},
 year = {2020},
 title = {Expressive power of parametrized quantum circuits},
 keywords = {doi:10.1103/PhysRevResearch.2.033125   url:https://doi.org/10.1103/PhysRevResearch.2.033125  
 },
 volume = {2},
 number = {3},
 issn = {2643-1564},
 journal = {Physical Review Research},
 doi = {10.1103/PhysRevResearch.2.033125},
 file = {}
}


@article{Dukelsky.1998,
 author = {Dukelsky, J. and Mart{\'i}n-Delgado, M. A. and Nishino, T. and Sierra, G.},
 year = {1998},
 title = {Equivalence of the variational matrix product method and the density matrix renormalization group applied to spin chains},
 pages = {457--462},
 volume = {43},
 number = {4},
 issn = {0295-5075},
 journal = {Europhysics Letters (EPL)},
 doi = {10.1209/epl/i1998-00381-x},
 file = {}
}

@article{EdwinStoudenmire.,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {{Edwin Stoudenmire} and {David J. Schwab}},
 title = {Supervised Learning with Tensor Networks},
 year = {2016},
 journal = {30th Conference on Neural Information Processing Systems},
 file = {}
}

@article{Evenbly.2009,
 author = {Evenbly, G. and Vidal, G.},
 year = {2009},
 title = {Algorithms for entanglement renormalization},
 volume = {79},
 number = {14},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.79.144108},
 file = {}
}


@article{Evenbly.2010,
 author = {Evenbly, G. and Vidal, G.},
 year = {2010},
 title = {Entanglement renormalization in noninteracting fermionic systems},
 volume = {81},
 number = {23},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.81.235102},
 file = {}
}


@article{Evenbly.2011,
 abstract = {Tensor network states are used to approximate ground states of local Hamiltonians on a lattice in D spatial dimensions. Different types of tensor network states can be seen to generate different geometries. Matrix product states (MPS) in D=1 dimensions, as well as projected entangled pair states (PEPS) in D{\textgreater}1 dimensions, reproduce the D-dimensional physical geometry of the lattice model; in contrast, the multi-scale entanglement renormalization ansatz (MERA) generates a (D+1)-dimensional holographic geometry. Here we focus on homogeneous tensor networks, where all the tensors in the network are copies of the same tensor, and argue that certain structural properties of the resulting many-body states are preconditioned by the geometry of the tensor network and are therefore largely independent of the choice of variational parameters. Indeed, the asymptotic decay of correlations in homogeneous MPS and MERA for D=1 systems is seen to be determined by the structure of geodesics in the physical and holographic geometries, respectively; whereas the asymptotic scaling of entanglement entropy is seen to always obey a simple boundary law -- that is, again in the relevant geometry. This geometrical interpretation offers a simple and unifying framework to understand the structural properties of, and helps clarify the relation between, different tensor network states. In addition, it has recently motivated the branching MERA, a generalization of the MERA capable of reproducing violations of the entropic boundary law in D{\textgreater}1 dimensions.},
 author = {Evenbly, G. and Vidal, G.},
 year = {2011},
 title = {Tensor network states and geometry},
 url = {http://arxiv.org/pdf/1106.1082v1},
 keywords = {Quantum Physics},
 pages = {891--918},
 volume = {145},
 number = {4},
 issn = {0022-4715},
 journal = {Journal of Statistical Physics},
 doi = {10.1007/s10955-011-0237-4},
 file = {}
}


@inproceedings{Fastovets.01.10.201805.10.2018,
 author = {Fastovets, D. V. and Bogdanov, Yu.I. and Bantysh, B. I. and Lukichev, V. F.},
 title = {Machine learning methods in quantum computing theory},
 url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11022/2522427/Machine-learning-methods-in-quantum-computing-theory/10.1117/12.2522427.full   },
 pages = {85},
 publisher = {SPIE},
 isbn = {9781510627093},
 editor = {Lukichev, Vladimir F. and Rudenko, Konstantin V.},
 booktitle = {International Conference on Micro- and Nano-Electronics 2018},
 year = {01.10.2018 - 05.10.2018},
 doi = {10.1117/12.2522427},
 file = {}
}


@article{Felser.2021,
 abstract = {npj Quantum Information, doi:10.1038/s41534-021-00443-w  },
 author = {Felser, Timo and Trenti, Marco and Sestini, Lorenzo and Gianelle, Alessio and Zuliani, Davide and Lucchesi, Donatella and Montangero, Simone},
 year = {2021},
 title = {Quantum-inspired machine learning on high-energy physics data},
 volume = {7},
 number = {1},
 journal = {npj Quantum Information},
 doi = {10.1038/s41534-021-00443-w},
 file = {}
}


@article{Ferris.2012,
 abstract = {Tensor network states are powerful variational ans\{\textquotedbl}atze for many-body ground states of quantum lattice models. The use of Monte Carlo sampling techniques in tensor network approaches significantly reduces the cost of tensor contractions, potentially leading to a substantial increase in computational efficiency. Previous proposals are based on a Markov chain Monte Carlo scheme generated by locally updating configurations and, as such, must deal with equilibration and autocorrelation times, which result in a reduction of efficiency. Here we propose a perfect sampling scheme, with vanishing equilibration and autocorrelation times, for unitary tensor networks -- namely tensor networks based on efficiently contractible, unitary quantum circuits, such as unitary versions of the matrix product state (MPS) and tree tensor network (TTN), and the multi-scale entanglement renormalization ansatz (MERA). Configurations are directly sampled according to their probabilities in the wavefunction, without resorting to a Markov chain process. We also describe a partial sampling scheme that can result in a dramatic (basis-dependent) reduction of sampling error.},
 author = {Ferris, Andrew J. and Vidal, Guifre},
 year = {2012},
 title = {Perfect Sampling with Unitary Tensor Networks},
 url = {http://arxiv.org/pdf/1201.3974v3},
 keywords = {Physics - Strongly Correlated Electrons;Quantum Physics},
 pages = {401},
 volume = {85},
 number = {16},
 issn = {1098-0121},
 journal = {Physical Review B},
 doi = {10.1103/PhysRevB.85.165146},
 file = {}
}


@misc{Fishman.28.07.2020,
 abstract = {ITensor is a system for programming tensor network calculations with an interface modeled on tensor diagram notation, which allows users to focus on the connectivity of a tensor network without manually bookkeeping tensor indices. The ITensor interface rules out common programming errors and enables rapid prototyping of tensor network algorithms. After discussing the philosophy behind the ITensor approach, we show examples of each part of the interface including Index objects, the ITensor product operator, tensor factorizations, tensor storage types, algorithms for matrix product state (MPS) and matrix product operator (MPO) tensor networks, quantum number conserving block-sparse tensors, and the NDTensors library. We also review publications that have used ITensor for quantum many-body physics and for other areas where tensor networks are increasingly applied. To conclude we discuss promising features and optimizations to be added in the future.},
 author = {Fishman, Matthew and White, Steven R. and Stoudenmire, E. Miles},
 date = {28.07.2020},
 title = {The ITensor Software Library for Tensor Network Calculations},
 url = {http://arxiv.org/pdf/2007.14822v2},
 keywords = {Computer Science - Mathematical Software;Physics - Computational Physics;Physics - Strongly Correlated Electrons},
 file = {}
}


@misc{Franken.23.12.2020,
 abstract = {Variational quantum circuits build the foundation for various classes of quantum algorithms. In a nutshell, the weights of a parametrized quantum circuit are varied until the empirical sampling distribution of the circuit is sufficiently close to a desired outcome. Numerical first-order methods are applied frequently to fit the parameters of the circuit, but most of the time, the circuit itself, that is, the actual composition of gates, is fixed. Methods for optimizing the circuit design jointly with the weights have been proposed, but empirical results are rather scarce. Here, we consider a simple evolutionary strategy that addresses the trade-off between finding appropriate circuit architectures and parameter tuning. We evaluate our method both via simulation and on actual quantum hardware. Our benchmark problems include the transverse field Ising Hamiltonian and the Sherrington-Kirkpatrick spin model. Despite the shortcomings of current noisy intermediate-scale quantum hardware, we find only a minor slowdown on actual quantum machines compared to simulations. Moreover, we investigate which mutation operations most significantly contribute to the optimization. The results provide intuition on how randomized search heuristics behave on actual quantum hardware and lay out a path for further refinement of evolutionary quantum gate circuits.},
 author = {Franken, Lukas and Georgiev, Bogdan and M{\"u}cke, Sascha and Wolter, Moritz and Heese, Raoul and Bauckhage, Christian and Piatkowski, Nico},
 date = {23.12.2020},
 title = {Quantum Circuit Evolution on NISQ Devices},
 url = {http://arxiv.org/pdf/2012.13453v3},
 keywords = {Computer Science - Learning;Quantum Physics;Statistics - Machine Learning},
 file = {}
}


@misc{Ganahl.28.06.2019,
 abstract = {We use TensorNetwork [C. Roberts et al., arXiv: 1905.01330], a recently developed API for performing tensor network contractions using accelerated backends such as TensorFlow, to implement an optimization algorithm for the Multi-scale Entanglement Renormalization Ansatz (MERA). We use the MERA to approximate the ground state wave function of the infinite, one-dimensional transverse field Ising model at criticality, and extract conformal data from the optimized ansatz. Comparing run times of the optimization on CPUs vs. GPU, we report a very significant speed-up, up to a factor of 200, of the optimization algorithm when run on a GPU.},
 author = {Ganahl, Martin and Milsted, Ashley and Leichenauer, Stefan and Hidary, Jack and Vidal, Guifre},
 date = {28.06.2019},
 title = {TensorNetwork on TensorFlow: Entanglement Renormalization for quantum  critical lattice models},
 url = {http://arxiv.org/pdf/1906.12030v1},
 keywords = {Physics - Computational Physics},
 file = {}
}


@article{Gel.2019,
 author = {Gel{\ss}, Patrick and Klus, Stefan and Eisert, Jens and Sch{\"u}tte, Christof},
 year = {2019},
 title = {Multidimensional Approximation of Nonlinear Dynamical Systems},
 volume = {14},
 number = {6},
 issn = {1555-1415},
 journal = {Journal of Computational and Nonlinear Dynamics},
 doi = {10.1115/1.4043148},
 file = {}
}


@article{Geng.2022,
 abstract = {Machine Learning: Science and Technology, 3 (2022) 015020 doi: 10.1088/2632-2153/ac48a2   },
 author = {Geng, Chenhua and Hu, Hong-Ye and Zou, Yijian},
 year = {2022},
 title = {Differentiable programming of isometric tensor networks},
 keywords = {auto-differentiation;condensed matter physics;machine learning;tensor network},
 pages = {015020},
 volume = {3},
 number = {1},
 journal = {Machine Learning: Science and Technology},
 doi = {10.1088/2632-2153/ac48a2},
 file = {}
}


@misc{Gili.21.01.2022,
 abstract = {Defining and accurately measuring generalization in generative models remains an ongoing challenge and a topic of active research within the machine learning community. This is in contrast to discriminative models, where there is a clear definition of generalization, i.e., the model's classification accuracy when faced with unseen data. In this work, we construct a simple and unambiguous approach to evaluate the generalization capabilities of generative models. Using the sample-based generalization metrics proposed here, any generative model, from state-of-the-art classical generative models such as GANs to quantum models such as Quantum Circuit Born Machines, can be evaluated on the same ground on a concrete well-defined framework. In contrast to other sample-based metrics for probing generalization, we leverage constrained optimization problems (e.g., cardinality constrained problems) and use these discrete datasets to define specific metrics capable of unambiguously measuring the quality of the samples and the model's generalization capabilities for generating data beyond the training set but still within the valid solution space. Additionally, our metrics can diagnose trainability issues such as mode collapse and overfitting, as we illustrate when comparing GANs to quantum-inspired models built out of tensor networks. Our simulation results show that our quantum-inspired models have up to a {\$}68 $\backslash$times{\$} enhancement in generating unseen unique and valid samples compared to GANs, and a ratio of 61:2 for generating samples with better quality than those observed in the training set. We foresee these metrics as valuable tools for rigorously defining practical quantum advantage in the domain of generative modeling.},
 author = {Gili, Kaitlin and Mauri, Marta and Perdomo-Ortiz, Alejandro},
 date = {21.01.2022},
 title = {Evaluating Generalization in Classical and Quantum Generative Models},
 url = {http://arxiv.org/pdf/2201.08770v2},
 keywords = {Computer Science - Learning;Quantum Physics},
 file = {}
}

@article{Glasser.2019,
 abstract = {Tensor-network techniques have enjoyed outstanding success in physics, and have recently attracted attention in machine learning, both as a tool for the formulation of new learning algorithms and for enhancing the mathematical understanding of existing methods. Inspired by these developments, and the natural correspondence between tensor networks and probabilistic graphical models, we provide a rigorous analysis of the expressive power of various tensor-network factorizations of discrete multivariate probability distributions. These factorizations include non-negative tensor-trains/MPS, which are in correspondence with hidden Markov models, and Born machines, which are naturally related to local quantum circuits. When used to model probability distributions, they exhibit tractable likelihoods and admit efficient learning algorithms. Interestingly, we prove that there exist probability distributions for which there are unbounded separations between the resource requirements of some of these tensor-network factorizations. Particularly surprising is the fact that using complex instead of real tensors can lead to an arbitrarily large reduction in the number of parameters of the network. Additionally, we introduce locally purified states (LPS), a new factorization inspired by techniques for the simulation of quantum systems, with provably better expressive power than all other representations considered. The ramifications of this result are explored through numerical experiments. Our findings imply that LPS should be considered over hidden Markov models, and furthermore provide guidelines for the design of local quantum circuits for probabilistic modeling.},
 author = {Glasser, Ivan and Sweke, Ryan and Pancotti, Nicola and Eisert, Jens and Cirac, J. Ignacio},
 year = {2019},
 title = {Expressive power of tensor-network factorizations for probabilistic  modeling, with applications from hidden Markov models to quantum machine  learning},
 url = {http://arxiv.org/pdf/1907.03741v2},
 keywords = {Computer Science - Learning;Mathematics - Optimization and Control;Physics - Strongly Correlated Electrons;Quantum Physics;Statistics - Machine Learning},
 journal = {Advances in Neural Information Processing Systems 32},
 file = {}
}

@article{Zhou.2020,
 abstract = {doi:10.1103/PhysRevX.10.041038             url:https://doi.org/10.1103/PhysRevX.10.041038 },
 author = {Zhou, Yiqing and Stoudenmire, E. Miles and Waintal, Xavier},
 year = {2020},
 title = {What Limits the Simulation of Quantum Computers?},
 keywords = {doi:10.1103/PhysRevX.10.041038              url:https://doi.org/10.1103/PhysRevX.10.041038},
 volume = {10},
 number = {4},
 journal = {Physical Review X},
 doi = {10.1103/PhysRevX.10.041038},
 file = {}
}

@proceedings{IEEE.1994,
 year = {1994},
 title = {Proceedings 35th Annual Symposium on Foundations of Computer Science},
 publisher = {{IEEE Comput. Soc. Press}},
 isbn = {0-8186-6580-7}
}


@inproceedings{Shor.1994,
 abstract = {Proceedings 35th Annual Symposium on Foundations of Computer Science;1994; ; ;10.1109/SFCS.1994.365700          },
 author = {Shor, P. W.},
 title = {Algorithms for quantum computation: discrete logarithms and factoring},
 pages = {124--134},
 publisher = {{IEEE Comput. Soc. Press}},
 isbn = {0-8186-6580-7},
 booktitle = {Proceedings 35th Annual Symposium on Foundations of Computer Science},
 year = {1994},
 doi = {10.1109/SFCS.1994.365700},
 file = {}
}

@book{Schuld.2021b,
 author = {Schuld, Maria and Petruccione, Francesco},
 year = {2021},
 title = {Machine Learning with Quantum Computers},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-83097-7},
 doi = {10.1007/978-3-030-83098-4},
 file = {Schuld2021{\_}ML4QC:Attachments/Schuld2021{\_}ML4QC.pdf:application/pdf}
}

@article{Cong.2019,
 abstract = {We introduce and analyze a novel quantum machine learning model motivated by convolutional neural networks. Our quantum convolutional neural network (QCNN) makes use of only {\$}O(\log(N)){\$} variational parameters for input sizes of $N$ qubits, allowing for its efficient training and implementation on realistic, near-term quantum devices. The QCNN architecture combines the multi-scale entanglement renormalization ansatz and quantum error correction. We explicitly illustrate its potential with two examples. First, QCNN is used to accurately recognize quantum states associated with 1D symmetry-protected topological phases. We numerically demonstrate that a QCNN trained on a small set of exactly solvable points can reproduce the phase diagram over the entire parameter regime and also provide an exact, analytical QCNN solution. As a second application, we utilize QCNNs to devise a quantum error correction scheme optimized for a given error model. We provide a generic framework to simultaneously optimize both encoding and decoding procedures and find that the resultant scheme significantly outperforms known quantum codes of comparable complexity. Finally, potential experimental realization and generalizations of QCNNs are discussed.},
 author = {Cong, Iris and Choi, Soonwon and Lukin, Mikhail D.},
 year = {2019},
 title = {Quantum Convolutional Neural Networks},
 url = {http://arxiv.org/pdf/1810.03787v2},
 keywords = {Physics - Strongly Correlated Electrons;Quantum Physics},
 pages = {1273--1278},
 volume = {15},
 number = {12},
 issn = {1745-2473},
 journal = {Nature Physics},
 doi = {10.1038/s41567-019-0648-8},
 file = {}
}

@article{Le.2011,
 author = {Le, Phuc Q. and Dong, Fangyan and Hirota, Kaoru},
 year = {2011},
 title = {A flexible representation of quantum images for polynomial preparation, image compression, and processing operations},
 pages = {63--84},
 volume = {10},
 number = {1},
 issn = {1570-0755},
 journal = {Quantum Information Processing},
 doi = {10.1007/s11128-010-0177-y},
 file = {}
}

@article{Wolf.2008,
 abstract = {The holographic principle states that on a fundamental level the information content of a region should depend on its surface area rather than on its volume. In this Letter we show that this phenomenon not only emerges in the search for new Planck-scale laws but also in lattice models of classical and quantum physics: the information contained in part of a system in thermal equilibrium obeys an area law. While the maximal information per unit area depends classically only on the number of degrees of freedom, it may diverge as the inverse temperature in quantum systems. It is shown that an area law is generally implied by a finite correlation length when measured in terms of the mutual information.},
 author = {Wolf, Michael M. and Verstraete, Frank and Hastings, Matthew B. and Cirac, J. Ignacio},
 year = {2008},
 title = {Area laws in quantum systems: mutual information and correlations},
 pages = {070502},
 volume = {100},
 number = {7},
 issn = {0031-9007},
 journal = {Physical review letters},
 doi = {10.1103/PhysRevLett.100.070502},
 file = {}
}

@article{Spall.1992,
 abstract = {IEEE Transactions on Automatic Control;1992;37;3;10.1109/9.119632},
 author = {Spall, J. C.},
 year = {1992},
 title = {Multivariate stochastic approximation using a simultaneous perturbation gradient approximation},
 pages = {332--341},
 volume = {37},
 number = {3},
 issn = {00189286},
 journal = {IEEE Transactions on Automatic Control},
 doi = {10.1109/9.119632},
 file = {}
}

@misc{Bergholm.12.11.2018,
 abstract = {PennyLane is a Python 3 software framework for differentiable programming of quantum computers. The library provides a unified architecture for near-term quantum computing devices, supporting both qubit and continuous-variable paradigms. PennyLane's core feature is the ability to compute gradients of variational quantum circuits in a way that is compatible with classical techniques such as backpropagation. PennyLane thus extends the automatic differentiation algorithms common in optimization and machine learning to include quantum and hybrid computations. A plugin system makes the framework compatible with any gate-based quantum simulator or hardware. We provide plugins for hardware providers including the Xanadu Cloud, Amazon Braket, and IBM Quantum, allowing PennyLane optimizations to be run on publicly accessible quantum devices. On the classical front, PennyLane interfaces with accelerated machine learning libraries such as TensorFlow, PyTorch, JAX, and Autograd. PennyLane can be used for the optimization of variational quantum eigensolvers, quantum approximate optimization, quantum machine learning models, and many other applications.},
 author = {Bergholm, Ville and Izaac, Josh and Schuld, Maria and Gogolin, Christian and Ahmed, Shahnawaz and Ajith, Vishnu and Alam, M. Sohaib and Alonso-Linaje, Guillermo and AkashNarayanan, B. and Asadi, Ali and Arrazola, Juan Miguel and Azad, Utkarsh and Banning, Sam and Blank, Carsten and Bromley, Thomas R. and Cordier, Benjamin A. and Ceroni, Jack and Delgado, Alain and {Di Matteo}, Olivia and Dusko, Amintor and Garg, Tanya and Guala, Diego and Hayes, Anthony and Hill, Ryan and Ijaz, Aroosa and Isacsson, Theodor and Ittah, David and Jahangiri, Soran and Jain, Prateek and Jiang, Edward and Khandelwal, Ankit and Kottmann, Korbinian and Lang, Robert A. and Lee, Christina and Loke, Thomas and Lowe, Angus and McKiernan, Keri and Meyer, Johannes Jakob and Monta{\~n}ez-Barrera, J. A. and Moyard, Romain and Niu, Zeyue and O'Riordan, Lee James and Oud, Steven and Panigrahi, Ashish and Park, Chae-Yeun and Polatajko, Daniel and Quesada, Nicol{\'a}s and Roberts, Chase and S{\'a}, Nahum and Schoch, Isidor and Shi, Borun and Shu, Shuli and Sim, Sukin and Singh, Arshpreet and Strandberg, Ingrid and Soni, Jay and Sz{\'a}va, Antal and Thabet, Slimane and Vargas-Hern{\'a}ndez, Rodrigo A. and Vincent, Trevor and Vitucci, Nicola and Weber, Maurice and Wierichs, David and Wiersema, Roeland and Willmann, Moritz and Wong, Vincent and Zhang, Shaoming and Killoran, Nathan},
 date = {12.11.2018},
 title = {PennyLane: Automatic differentiation of hybrid quantum-classical  computations},
 url = {http://arxiv.org/pdf/1811.04968v4}
}

@misc{CirqDevelopers.2022,
 abstract = {Cirq is a Python library for writing, manipulating, and optimizing quantum circuits and running them against quantum computers and simulators.

See full list of authors on Github: https://github.com/quantumlib/Cirq/graphs/contributors},
 author = {{Cirq Developers}},
 year = {2022},
 title = {Cirq},
 publisher = {Zenodo},
 doi = {10.5281/ZENODO.7465577}
}

@misc{ Qiskit,
       author = {A-tA-v and MD SAJID ANIS and Abby-Mitchell and H{\'e}ctor Abraham and AduOffei and Rochisha Agarwal and Gabriele Agliardi and Merav Aharoni and Vishnu Ajith and Ismail Yunus Akhalwaya and Gadi Aleksandrowicz and Thomas Alexander and Matthew Amy and Sashwat Anagolum and Andr{\'e} and Anthony-Gandon and Israel F. Araujo and Eli Arbel and Eric Arellano and Abraham Asfaw and Ikko Eltociear Ashimine and Anish Athalye and Artur Avkhadiev and Carlos Azaustre and PRATHAMESH BHOLE and Vishal Bajpe and Abhik Banerjee and Santanu Banerjee and Will Bang and Aman Bansal and Panagiotis Barkoutsos and Ashish Barnawal and George Barron and George S. Barron and Luciano Bello and Yael Ben-Haim and M. Chandler Bennett and Daniel Bevenius and Dhruv Bhatnagar and Prakhar Bhatnagar and Arjun Bhobe and Paolo Bianchini and Lev S. Bishop and Carsten Blank and Sorin Bolos and Soham Bopardikar and Samuel Bosch and Sebastian Brandhofer and Brandon and Sergey Bravyi and Bryce-Fuller and David Bucher and Lukas Burgholzer and Artemiy Burov and Fran Cabrera and Padraic Calpin and Lauren Capelluto and Jorge Carballo and Gin{\'e}s Carrascal and Adam Carriker and Ivan Carvalho and Rishabh Chakrabarti and Adrian Chen and Chun-Fu Chen and Edward Chen and Jielun (Chris) Chen and Richard Chen and Franck Chevallier and Kartik Chinda and Rathish Cholarajan and Jerry M. Chow and Spencer Churchill and CisterMoke and Christian Claus and Christian Clauss and Caleb Clothier and Romilly Cocking and Ryan Cocuzzo and Jordan Connor and Filipe Correa and Zachary Crockett and Abigail J. Cross and Andrew W. Cross and Simon Cross and Juan Cruz-Benito and Chris Culver and Antonio D. C{\'o}rcoles-Gonzales and Navaneeth D and Sean Dague and Tareq El Dandachi and Animesh N Dangwal and Jonathan Daniel and DanielAja and Marcus Daniels and Matthieu Dartiailh and Abd{\'o}n Rodr{\'\i}guez Davila and Faisal Debouni and Anton Dekusar and Amol Deshmukh and Mohit Deshpande and Delton Ding and Jun Doi and Eli M. Dow and Patrick Downing and Eric Drechsler and Marc Sanz Drudis and Eugene Dumitrescu and Karel Dumon and Ivan Duran and Kareem EL-Safty and Eric Eastman and Grant Eberle and Amir Ebrahimi and Pieter Eendebak and Daniel Egger and EgrettaThula and ElePT and Iman Elsayed and Emilio and Alberto Espiricueta and Mark Everitt and Davide Facoetti and Farida and Paco Mart{\'\i}n Fern{\'a}ndez and Samuele Ferracin and Davide Ferrari and Axel Hern{\'a}ndez Ferrera and Romain Fouilland and Albert Frisch and Andreas Fuhrer and Bryce Fuller and MELVIN GEORGE and Julien Gacon and Borja Godoy Gago and Claudio Gambella and Jay M. Gambetta and Adhisha Gammanpila and Luis Garcia and Tanya Garg and Shelly Garion and James R. Garrison and Jim Garrison and Tim Gates and Nir Gavrielov and Gian Gentinetta and Hristo Georgiev and Leron Gil and Austin Gilliam and Aditya Giridharan and Glen and Juan Gomez-Mosquera and Gonzalo and Salvador de la Puente Gonz{\'a}lez and Jesse Gorzinski and Ian Gould and Donny Greenberg and Dmitry Grinko and Andr{\'e} Gro{\ss}ardt and Wen Guan and Dani Guijo and Guillermo-Mijares-Vilarino and John A. Gunnels and Harshit Gupta and Naman Gupta and Jakob M. G{\"u}nther and Mikael Haglund and Isabel Haide and Ikko Hamamura and Omar Costa Hamido and Frank Harkins and Kevin Hartman and Areeq Hasan and Vojtech Havlicek and Joe Hellmers and {\L}ukasz Herok and Ryan Hill and Stefan Hillmich and Ian Hincks and Colin Hong and Hiroshi Horii and Connor Howington and Shaohan Hu and Wei Hu and Chih-Han Huang and Junye Huang and Rolf Huisman and Haruki Imai and Takashi Imamichi and Kazuaki Ishizaki and Ishwor and Raban Iten and Toshinari Itoko and Alexander Ivrii and Adrin Jalali and Ali Javadi and Ali Javadi-Abhari and Wahaj Javed and Qian Jianhua and Madhav Jivrajani and Kiran Johns and Scott Johnstun and Jonathan-Shoemaker and JosDenmark and JoshDumo and John Judge and Tal Kachmann and Akshay Kale and Naoki Kanazawa and Jessica Kane and Kang-Bae and Annanay Kapila and Anton Karazeev and Paul Kassebaum and Takumi Kato and Tobias Kehrer and Josh Kelso and Scott Kelso and Hugo van Kemenade and Vismai Khanderao and Spencer King and Yuri Kobayashi and Kovi11Day and Arseny Kovyrshin and Jeevesh Krishna and Rajiv Krishnakumar and Pradeep Krishnamurthy and Vivek Krishnan and Kevin Krsulich and Prasad Kumkar and Gawel Kus and LNoorl and Ryan LaRose and Enrique Lacal and Rapha{\"e}l Lambert and Haggai Landa and John Lapeyre and Dariusz Lasecki and Joe Latone and Scott Lawrence and Christina Lee and Gushu Li and Tan Jun Liang and Jake Lishman and Dennis Liu and Peng Liu and Lolcroc and Abhishek K M and Liam Madden and Yunho Maeng and Saurav Maheshkar and Kahan Majmudar and Aleksei Malyshev and Mohamed El Mandouh and Joshua Manela and Manjula and Jakub Marecek and Manoel Marques and Kunal Marwaha and Dmitri Maslov and Pawe{\l} Maszota and Dolph Mathews and Atsushi Matsuo and Farai Mazhandu and Doug McClure and Maureen McElaney and Joseph McElroy and Cameron McGarry and David McKay and Dan McPherson and Srujan Meesala and Dekel Meirom and Corey Mendell and Thomas Metcalfe and Martin Mevissen and Andrew Meyer and Antonio Mezzacapo and Rohit Midha and Alexander Miessen and Declan Millar and Daniel Miller and Hannah Miller and Zlatko Minev and Abby Mitchell and Ansah Mohammad and Nikolaj Moll and Alejandro Montanez and Gabriel Monteiro and Michael Duane Mooring and Renier Morales and Niall Moran and David Morcuende and Seif Mostafa and Mario Motta and Romain Moyard and Prakash Murali and Daiki Murata and Jan M{\"u}ggenburg and Tristan NEMOZ and David Nadlinger and Ken Nakanishi and Giacomo Nannicini and Paul Nation and Edwin Navarro and Yehuda Naveh and Scott Wyman Neagle and Patrick Neuweiler and Aziz Ngoueya and Thien Nguyen and Johan Nicander and Nick-Singstock and Pradeep Niroula and Hassi Norlen and NuoWenLei and Lee James O'Riordan and Oluwatobi Ogunbayo and Pauline Ollitrault and Tamiya Onodera and Raul Otaolea and Steven Oud and Dan Padilha and Hanhee Paik and Soham Pal and Yuchen Pang and Ashish Panigrahi and Vincent R. Pascuzzi and Simone Perriello and Eric Peterson and Anna Phan and Kuba Pilch and Francesco Piro and Marco Pistoia and Christophe Piveteau and Julia Plewa and Pierre Pocreau and Clemens Possel and Alejandro Pozas-Kerstjens and Rafa{\l} Pracht and Milos Prokop and Viktor Prutyanov and Sumit Puri and Daniel Puzzuoli and Pythonix and Jes{\'u}s P{\'e}rez and Quant02 and Quintiii and Rafey Iqbal Rahman and Arun Raja and Roshan Rajeev and Isha Rajput and Nipun Ramagiri and Anirudh Rao and Rudy Raymond and Oliver Reardon-Smith and Rafael Mart{\'\i}n-Cuevas Redondo and Max Reuter and Julia Rice and Matt Riedemann and Rietesh and Drew Risinger and Pedro Rivero and Marcello La Rocca and Diego M. Rodr{\'\i}guez and RohithKarur and Ben Rosand and Max Rossmannek and Mingi Ryu and Tharrmashastha SAPV and Nahum Rosa Cruz Sa and Arijit Saha and Abdullah Ash- Saki and Arfat Salman and Sankalp Sanand and Martin Sandberg and Hirmay Sandesara and Ritvik Sapra and Hayk Sargsyan and Aniruddha Sarkar and Ninad Sathaye and Niko Savola and Bruno Schmitt and Chris Schnabel and Zachary Schoenfeld and Travis L. Scholten and Eddie Schoute and Julian Schuhmacher and Mark Schulterbrandt and Joachim Schwarm and Paul Schweigert and James Seaward and Sergi and Diego Emilio Serrano and Ismael Faro Sertage and Kanav Setia and Freya Shah and Priti Ashvin Shah and Nathan Shammah and Will Shanks and Rohan Sharma and Polly Shaw and Yunong Shi and Jonathan Shoemaker and Adenilton Silva and Andrea Simonetto and Deeksha Singh and Divyanshu Singh and Parmeet Singh and Phattharaporn Singkanipa and Yukio Siraichi and Siri and Jesus Sistos and Jes{\'u}s Sistos and Iskandar Sitdikov and Seyon Sivarajah and Slavikmew and Magnus Berg Sletfjerding and John A. Smolin and Mathias Soeken and Igor Olegovich Sokolov and Igor Sokolov and Vicente P. Soloviev and SooluThomas and M St and Starfish and Dominik Steenken and Matt Stypulkoski and Adrien Suau and Shaojun Sun and Kevin J. Sung and Makoto Suwama and Oskar S{\l}owik and Rohit Taeja and Hitomi Takahashi and Tanvesh Takawale and Ivano Tavernelli and Charles Taylor and Pete Taylour and Soolu Thomas and Kevin Tian and Mathieu Tillet and Maddy Tod and Miroslav Tomasik and Caroline Tornow and Enrique de la Torre and Juan Luis S{\'a}nchez Toural and Kenso Trabing and Matthew Treinish and Dimitar Trenev and TrishaPe and Felix Truger and TsafrirA and Georgios Tsilimigkounakis and Kazuki Tsuoka and Davindra Tulsi and Do{\u{g}}ukan Tuna and Wes Turner and Kento Ueda and Yotam Vaknin and Carmen Recio Valcarce and Francois Varchon and Adish Vartak and Almudena Carrera Vazquez and Prajjwal Vijaywargiya and Victor Villar and Bhargav Vishnu and Desiree Vogt-Lee and Christophe Vuillot and WQ and James Weaver and Johannes Weidenfeller and Rafal Wieczorek and Jonathan A. Wildstrom and Jessica Wilson and Erick Winston and WinterSoldier and Etienne Wodey and Jack J. Woehr and Stefan Woerner and Ryan Woo and Christopher J. Wood and Ryan Wood and Steve Wood and James Wootton and Matt Wright and Lucy Xing and Jintao YU and Yaiza and Bo Yang and Unchun Yang and Jimmy Yao and Daniyar Yeralin and Ryota Yonekura and David Yonge-Mallo and Ryuhei Yoshida and Richard Young and Jessie Yu and Lebin Yu and Yuma-Nakamura and Christopher Zachow and Laura Zdanski and Helena Zhang and Evgenii Zheltonozhskii and Iulia Zidaru and Bastian Zimmermann and Ben Zindorf and Christa Zoufal and a-matsuo and acastanedam and aeddins-ibm and alexzhang13 and b63 and bartek-bartlomiej and bcamorrison and brandhsn and nick bronn and chetmurthy and choerst-ibm and comet and dalin27 and deeplokhande and dekel.meirom and derwind and dime10 and ehchen and ewinston and fanizzamarco and fs1132429 and gadial and galeinston and georgezhou20 and georgios-ts and gruu and hhorii and hhyap and hykavitha and itoko and jeppevinkel and jessica-angel7 and jezerjojo14 and jliu45 and johannesgreiner and jscott2 and kUmezawa and klinvill and krutik2966 and luciacuervovalor and ma5x and merav-aharoni and michelle4654 and msuwama and nico-lgrs and nrhawkins and ntgiwsvp and ordmoj and sagar pahwa and pritamsinha2304 and rickyzcode and rithikaadiga and ryancocuzzo and saktar-unr and saswati-qiskit and sebastian-mair and septembrr and sethmerkel and sg495 and shaashwat and smturro2 and sternparky and strickroman and tigerjack and tsura-crisaldo and upsideon and vadebayo49 and welien and willhbang and wmurphy-collabstar and yang.luh and yuri@FreeBSD and Mantas {\v{C}}epulkovskis},
       title = {Qiskit: An Open-source Framework for Quantum Computing},
       year = {2021},
       doi = {10.5281/zenodo.2573505}
}