\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{caption}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}
\usepackage[dvipsnames]{xcolor}

\usepackage[pagebackref=true, breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy

\begin{document}

\title{TriPlaneNet: An Encoder for EG3D Inversion}

\author{Ananta R. Bhattarai \and Matthias Nie{\ss}ner \and Artem Sevastopolsky \and \\
Technical University of Munich (TUM)
}

\maketitle

\begin{abstract}
Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. 
At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. 
Despite the success of universal optimization-based methods for 2D GAN inversion, those, applied to 3D GANs, may fail to produce 3D-consistent renderings. 
Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. 
In our work, we introduce a real-time method that bridges the gap between the two approaches by directly utilizing the tri-plane representation introduced for EG3D generative model.
In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets.
As shown in our work, the renderings are similar in quality to optimization-based techniques and significantly outperform the baselines for novel view.
As we empirically prove, this is a consequence of directly operating in the tri-plane space, not in the GAN parameter space, while making use of an encoder-based trainable approach. \end{abstract}
\vspace{-0.5cm}

\section{Introduction}
Generative adversarial networks (GANs) have gained popularity due to their ability to generate images of high resolution and variation. In recent years, numerous works~\cite{chan2021pi,chan2022efficient} have tackled the problem of multi-view consistent image synthesis with 3D-aware GANs. Typically, nowadays these include a neural rendering engine and volumetrically integrate the internal 3D presentations. Such methods make generators aware of a 3D structure by modeling it with explicit voxel grids~\cite{henzler2019escaping, sitzmann2019deepvoxels, nguyen2020blockgan} or neural implicit representations~\cite{chan2021pi,or2022stylesdf}. 
Most notably, EG3D~\cite{chan2022efficient} introduced a 3D GAN framework based on a tri-plane 3D representation that is both efficient and expressive to enable high-resolution 3D-aware image synthesis. Moreover, they demonstrate state-of-the-art results for unconditional geometry-aware image synthesis.

Main applications of 3D GANs include human face inversion including head tracking, reenactment, facial manipulation, and novel view synthesis of a given image or video. 
Oftentimes, the classical GAN formulation does not support trivial inversion, i.e.~finding the appropriate code in the learned GAN space for a given sample. A straightforward way to achieve this is by obtaining the latent code of the input image via optimization-based or encoder-based approaches, i.e.~applying 2D GAN inversion techniques. An existing branch of research studies 2D GAN inversion in high detail~\cite{abdal2019image2stylegan,richardson2021encoding,alaluf2021restyle,abdal2020image2stylegan++,zhu2020domain,tewari2020pie}, but nevertheless, the problem remains underexplored in 3D.

\begin{table}[t!]
\centering
\setlength\tabcolsep{0pt}
\begin{tabular}{cc@{\hskip 0.1cm}ccc}
    \multirow{4}{*}[-5em]{
    \begin{subfigure}[h]{0.11\textwidth}
        \includegraphics[width=\textwidth]{images/1_introduction/1086/original/1086.jpg}
        \caption*{Input}
    \end{subfigure}
    \hspace{0.1cm}
    } & 
    \raisebox{1\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ \cite{karras2020analyzing}} }
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/w_plus/1086.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/w_plus/1086_-0.3.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/w_plus/1086_0.3.jpg} \\
& 
    \raisebox{1.2\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}} 
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/PTI/1086.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/PTI/1086_-0.3.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/PTI/1086_0.3.jpg} \\
& 
    \raisebox{1.2\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}} 
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/pSp/1086.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/pSp/1086_-0.3.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/pSp/1086_0.3.jpg} \\
&
    \raisebox{1.8\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}} 
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/triplanenet/1086.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/triplanenet/1086_-0.3.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/triplanenet/1086_0.3.jpg} \\
\end{tabular}
\vspace{-0.1cm}
\captionof{figure}{
For a given picture, our method predicts the appropriate latent code and the tri-plane offsets for EG3D generator in a feed-forward manner. This way, both the frontal view and the novel view rendering can be obtained with high fidelity and in real-time.
}
\label{first_page}
\vspace{-0.4cm}
\end{table}

\begin{figure*}[t!]
    \centering
    \includegraphics[trim={0 5cm 0 0},width=\textwidth]{images/3_method/method.pdf}
    \caption{Overview of the proposed method. TriPlaneNet consists of two branches. The first branch (above) comprises the predictor $\hat{w} = \boldsymbol{\phi}(x)$ of the pivotal latent code $\hat{w} \in \mathcal{W}+$, which results in an RGB image $\hat{y} = \mathcal{R}(\boldsymbol{G}(\hat{w}))$ after passing it through the EG3D tri-plane generator $\boldsymbol{G}(\cdot)$ and renderer block $\mathcal{R}(\cdot)$ containing super-resolution module. The second branch (below) uses the first-stage approximation $\hat{y}$ and its difference with the target $(x - \hat{y})$ to predict the numerical offsets to the tri-planes $\Delta \boldsymbol{T}$ by a convolutional autoencoder $\Delta \boldsymbol{T} = \boldsymbol{\psi}(\hat{y}, x - \hat{y})$, which yields the final prediction $y = \mathcal{R}(\boldsymbol{G}(\hat{w}) + \Delta \boldsymbol{T})$.}
    \label{method:fig}
\end{figure*}

Optimization-based inversion methods are often superior to encoder-based approaches in terms of reconstruction quality. However, encoder-based techniques are orders of magnitude faster as they map a given image to the latent space of GAN in a single forward pass. Compared to 2D GAN inversion, 3D GAN inversion is a more challenging task as the inversion needs to both preserve the identity of an input image and satisfy the 3D consistency constraint in generated novel views. In particular, optimization-based GAN inversion methods that have no knowledge of the specific GAN architecture make sure to yield a high-quality rendering of the desired image from the same camera view, but the lack of any geometry information in the image may produce broken or flattened geometry when rendered from a novel camera. We improve these shortcomings in two separate ways. First, by predicting an input latent code for the EG3D generator with a convolutional encoder, we observe that the geometry is preserved better. This can be attributed to the fact that the encoder, trained for the inversion task, is exposed to thousands of multi-view images and this way learns to be more 3D-aware. Second, we utilize the knowledge about the model and improve the fidelity and consistency by predicting offsets to the tri-planes that constitute the 3D representation in EG3D. Unlike voxel grids or implicit representations, tri-planes can be naturally estimated by 2D convnets and, as demonstrated by our experiments, can realistically express object features beyond the capabilities of an input latent code, e.g. hands and long hair, without breaking 3D consistency (see Fig.~\ref{first_page}). This advantage is achieved by recovering the object representation directly in the world space. Since the tri-plane offsets are fully predicted by convolutional layers, our inversion can run in real-time on modern GPUs.

We propose the EG3D-specific inversion scheme in two stages. In the first stage, the initial inversion is obtained using the pSp encoder that directly embeds the input image into the $\mathcal{W}+$ space of EG3D. In the second stage, we introduce an encoder, TriPlaneNet, that learns to refine the reconstruction given the original image and initial inversion. Our second stage encoder takes the initial inversion, and the difference between initial inversion and the original image as the inputs and predicts a numerical offset to the initial tri-plane representation. Some recent works \cite{alaluf2022hyperstyle,dinh2022hyperinverter,roich2022pivotal} for 2D inversion have attempted to improve the reconstruction quality in two stages. First, they obtain the latent code for the input image. Then, they fine-tune generator weights with respect to the initial inversion. However, we demonstrate that TriPlaneNet preserves identity and ensures 3D consistency in novel views better compared to other approaches.

To summarize, our contributions are the following:
\begin{itemize}
    \item We propose a novel, real-time inversion framework for EG3D that enables high-quality reconstruction while maintaining multi-view consistency by directly utilizing the tri-plane representation.
    \item We demonstrate that our method achieves on par reconstruction quality compared to optimization-based inversion methods in the original view, while strongly outperforming them for novel view rendering. Our method is also more resilient towards harder cases such as when a hat or accessories are featured.
\end{itemize}

\section{Related Work}
\noindent \textbf{3D Generative Models for Human Faces.}
Representing and generating diverse 3D human faces and heads attracted increasing attention over the last decade \cite{nguyen2019hologan,chen2016infogan,huang2017beyond}, while the appearance of NeRF~\cite{mildenhall2021nerf} has sparked additional interest in that topic. 
The first generative models built upon NeRF-style volumetric integration \cite{schwarz2020graf,niemeyer2021giraffe} achieved generalization by conditioning the multi-layer perceptron on latent codes, representing the object's shape and appearance. 
More novel $\pi$-GAN \cite{chan2021pi} and StyleNeRF \cite{gu2021stylenerf} condition the generative network on the output of the StyleGAN-like generator~\cite{karras2020analyzing}, which amounted to the higher-quality rendering of faces and arbitrary objects with subtle details. 
As a next major improvement step, authors of EG3D~\cite{chan2022efficient} propose a tri-plane 3D representation that serves as a bridge between expressive implicit representations and spatially-restricting explicit representations. 
As a byproduct, methods such as EG3D and StyleSDF~\cite{or2022stylesdf} allow for the extraction of explicit, highly detailed geometry of the human faces, while being trained without any volumetric supervision.
Some modifications of NeRFs~\cite{gafni2021dynamic,park2021nerfies,athar2022rignerf} and NeRF-based generative models~\cite{bergman2022generative,hong2022headnerf} allows for the explicit expression and appearance control of the rendered faces.
Further, recently demonstrated abilities of diffusion models to generate highly accurate 2D images are currently being transferred onto 3D objects~\cite{zeng2022lion,muller2022diffrf} and 3D human heads~\cite{wang2022rodin}.

\noindent \textbf{GAN Inversion.} Unlike other kinds of generative models, such as VAE or normalizing flows, inverting a GAN (finding the appropriate latent code for a given image) is oftentimes a more tricky and computationally-demanding task. 
Early attempts focused on the tuning of the latent code with the optimization-based approaches~\cite{creswell2018inverting,lipton2017precise,karras2020analyzing}. 
Various approaches exploited the idea of predicting latent representation by an encoder~\cite{guan2020collaborative,luo2017learning,perarnau2016invertible,zhu2016generative,park2019semantic}.
In~\cite{roich2022pivotal}, a universal PTI method is introduced which comprises the optimization of a latent code and, consequently, fine-tuning parameters of the generator.
A recent survey on GAN inversion~\cite{xia2022gan} compares multiple generic techniques introduced since the appearance of GANs.

\noindent \textbf{Inversion of 2D GANs} For StyleGAN, an important observation was made by the authors of~\cite{abdal2019image2stylegan} that operating in the extended $\mathcal{W}+$ space is significantly more expressive than in the restrictive $\mathcal{W}$ generator input space. 
The latter idea has been strengthened and better adapted for face editing with the appearance of pSp~\cite{richardson2021encoding} and e4e~\cite{tov2021designing}, as well as of their cascaded variant ReStyle~\cite{alaluf2021restyle} and other works~\cite{abdal2020image2stylegan++,zhu2020domain,tewari2020pie}.
Similarly to PTI but in an encoder-based setting, HyperStyle~\cite{alaluf2022hyperstyle} and HyperInverter~\cite{dinh2022hyperinverter} predict offsets to the StyleGAN generator weights in a lightweight manner in order to represent the target picture in a broader space of parameters.

\noindent \textbf{Inversion of 3D GANs}. Unlike the 2D case, the inversion of a 3D GAN is a significantly more advanced problem due to the arising ambiguity: the latent code must be both compliant with the target image and correspond to its plausible 3D representation. 
While PTI remains a universal method that solves this problem for an arbitrary generator, recent art demonstrates the quality rapidly declines when the PTI inversion result is rendered from a novel view.
The suggested ways of resolving this fidelity-consistency tradeoff for an arbitrary 3D GAN include incorporating multi-view consistency regularizers~\cite{li20223d}, augmenting training with surrogate mirrored images~\cite{yin20223d}, or using depth information when available~\cite{ko20233d}.
Our work employs a convolutional encoder and utilizes the EG3D tri-plane structure to improve the 3D consistency of the inverted rendering without additional multi-view constraints required during training. 

\noindent \textbf{Applications of GAN encoders.} While being developed for inversion, convolutional encoders for GANs can compress a lot of information about the domain distribution and be used as a proxy.
For instance, some of the recent works employ them for pre-training for semantic segmentation~\cite{baranchuk2021label}, face recognition~\cite{sevastopolsky2022boost}, and as a generic prior~\cite{you2023unified,nitzan2022mystyle}. 

\section{Method}
\subsection{Preliminaries}

\noindent \textbf{GAN inversion.}
Given a target image $x$, the goal of GAN inversion is to find a latent code that minimizes the reconstruction loss between the synthesized image and the target image:
\begin{equation}
    \hat{w} = \operatorname*{argmin}_{w}\mathcal{L}(x, G(w; \theta))
    \label{gan-obj}
\end{equation}
where $G(w; \theta)$ is the image generated by pre-trained generator $G$ parameterized by weights $\theta$, over the latent $w$. The loss objective $\mathcal{L}$ usually employs $L_2$ or LPIPS \cite{Zhang2018TheUE}. The problem in (\ref{gan-obj}) can be solved via optimization or encoder-based approaches. Encoder-based approaches utilize an encoder network $E$ to map real images into a latent code. The training of an encoder network is performed over a large set of images $\{x^{i}\}_{i=1}^{N}$ to minimize:
\begin{equation}
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{3pt}
    \min_E \sum_{i=1}^N\mathcal{L}(x^i, G(E(x^i); \theta))
\end{equation}
During inference, an input image is inverted by $G(E(x); \theta)$. In the recent works~\cite{roich2022pivotal, alaluf2022hyperstyle, dinh2022hyperinverter}, a number of approaches are proposed to additionally estimate image-specific generator parameters $\theta(x)$ by a convolutional network.\newline

\begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{c@{\hskip 0.3cm}ccccccc}
    \raisebox{2.5\height}{\rotatebox[origin=c]{90}{Input}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/3207.jpg}  \\[0.3cm]
\raisebox{1.2\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/3207.jpg}  \\
\raisebox{1.7\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/3207.jpg}  \\
\raisebox{1.7\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/3207.jpg}  \\
\raisebox{2.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/3207.jpg} 
\end{tabular}
\captionof{figure}{Qualitative comparison for image reconstruction. Compared to other approaches, our method can reconstruct a face in the same view in more detail, especially introducing more detail for features such as hats, hair, and background.}
\label{visualcomp}
\vspace{-0.3cm}
\end{table*}

\noindent \textbf{EG3D.}
EG3D \cite{chan2022efficient} uses tri-plane 3D representation for geometry-aware image synthesis from 2D images. EG3D image generation pipeline consists of several modules: a StyleGAN2-based feature generator, a tri-plane representation, a lightweight neural decoder, a volume renderer, and a super-resolution module. To synthesize an image, a random latent code $z \in \mathbb{R}^{D}$ (typically, $D=512$) and camera parameters are first mapped to a pivotal latent code $w \in \mathcal{W}+$ using a mapping network. Then, $w$ is fed into the StyleGAN2 CNN generator $\boldsymbol{G}(\cdot)$ to generate a $H \times W \times 96$ feature map. This feature map is reshaped to form three 32-channel planes, thus forming a tri-plane feature representation $\boldsymbol{T}$ of the corresponding object. To sample from the tri-plane features, a position $p \in \mathbb{R}^3$ is first projected onto the three feature planes. Then, corresponding feature vectors ($F_{xy}(p), F_{xz}(p), F_{yz}(p)$) are retrieved using bilinear interpolation and aggregated. These aggregated features are processed by a lightweight neural decoder to transform the feature into the estimated color and density at the location $p$. Volume rendering is then performed to project 3D feature volume into a feature image. Finally, a super-resolution module is utilized to upsample the feature image to the final image size. For simplicity, we will later refer to the lightweight neural decoder, renderer, and the super-resolution block, all combined, as the rendering block $\mathcal{R}(\cdot)$. The high efficiency and expressiveness of EG3D, as well as the ability to work with tri-planes directly, motivates the development of our model-specific inversion algorithm.\newline

\noindent \textbf{pSp.}
Richardson \textit{et al}.~\cite{richardson2021encoding} proposed a pSp framework based on an encoder that can directly map real images into $\mathcal{W}+$ latent space of StyleGAN. In pSp, an encoder backbone with a feature pyramid generates three levels of feature maps. The extracted feature maps are processed by a map2style network to extract styles. The styles are then fed into the generator network to synthesize an image $\hat{y}$:

\begin{equation}
\hat{y} = G(E(x) + \bar{w}),
\end{equation}
where $G(\cdot)$ and $E(\cdot)$ denote the generator and encoder networks respectively and $\bar{w}$ is the average style vector of the pretrained generator.


\begin{table*}[h!]
    \setlength{\tabcolsep}{0cm}
    \renewcommand{\arraystretch}{0}
    \begin{tabular}{ccccc@{\hskip 0.2cm}ccccc}
    Input & $\mathcal{W}+$~\cite{karras2020analyzing} & PTI~\cite{roich2022pivotal} & pSp~\cite{richardson2021encoding} & \textbf{Ours} & Input & $\mathcal{W}+$~\cite{karras2020analyzing} & PTI~\cite{roich2022pivotal} & pSp~\cite{richardson2021encoding} & \textbf{Ours} \\[0.2cm]
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/original/10008.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/10008_-0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/10008_-0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/10008_-0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/10008_-0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/original/5358.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/5358_-0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/5358_-0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/5358_-0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/5358_-0.6.jpg}   \\
\includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/original/10008.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/10008_-0.3.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/10008_-0.3.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/10008_-0.3.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/10008_-0.3.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/original/5358.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/5358_-0.3.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/5358_-0.3.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/5358_-0.3.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/5358_-0.3.jpg}   \\
\includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/original/10008.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/10008_0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/10008_0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/10008_0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/10008_0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/original/5358.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/5358_0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/5358_0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/5358_0.6.jpg} & 
    \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/5358_0.6.jpg}   \\[0.1cm]
\includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/original/10008.jpg} & 
    \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/w_plus/10008_0.6.jpg} & 
    \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/PTI/10008_0.6.jpg} & 
    \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/pSp/10008_0.6.jpg} & 
    \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/10008_0.6.jpg} & 
    \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/original/5358.jpg} & 
    \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/w_plus/5358_0.6.jpg} & 
    \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/PTI/5358_0.6.jpg} & 
    \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/pSp/5358_0.6.jpg} & 
    \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/5358_0.6.jpg}   \\
    \end{tabular}
    \vspace{-0.1cm}
    \captionof{figure}{Qualitative evaluation on novel view rendering of yaw angle -0.6, -0.3, and 0.6 radians (full and zoom-in). In comparison to others, our method preserves identity and multi-view consistency better when rendered from a novel view.}
    \label{novelview}
    \vspace{-0.3cm}
\end{table*}

\subsection{TriPlaneNet}
Our TriPlaneNet inversion framework comprises two branches
(see Figure \ref{method:fig} for the overview).
The first branch employs a pSp encoder to embed an input image into $\mathcal{W}+$ space of EG3D. Specifically, given an input image $x$, we train pSp encoder $\phi$ to predict the pivotal latent $\hat{w} \in \mathcal{W}+$:
\begin{equation}
    \hat{w} = \phi(x) + \bar{w}
\end{equation}
where the dimension of $\hat{w}$ is $K \times D$ (for the output image resolution of 128, $K=14$, and $D=512$). The pivotal code is then fed into StyleGAN2 generator $\boldsymbol{G}(\cdot)$ in the EG3D pipeline to obtain initial tri-plane features $\boldsymbol{T}$. Then, the tri-plane representation is processed by the rendering block $\mathcal{R}(\cdot)$ to generate initial reconstruction $\hat{y}$:
\begin{equation}
    \setlength{\abovedisplayskip}{5pt}
    \setlength{\belowdisplayskip}{5pt}
    \hat{y} = \mathcal{R}(\boldsymbol{G}(\hat{w}))
\end{equation}

The second branch consists of a convolutional auto-encoder $\psi$ that learns to predict numerical offsets to the initial tri-plane features. The input to the autoencoder network is the channel-wise concatenation of initial reconstruction $\hat{y}$ and the difference between an input image and initial reconstruction ($x - \hat{y}$). Given this extended input, the autoencoder is tasked with computing tri-plane offsets $\Delta \boldsymbol{T}$ with respect to tri-plane features obtained in the first branch:
\begin{equation}
    \Delta \boldsymbol{T} = \boldsymbol{\psi}(\hat{y}, x - \hat{y})
\end{equation}
The new tri-plane features corresponding to the inversion of the input image $x$ are then computed as an element-wise addition of tri-plane offsets $\Delta \boldsymbol{T}$ with initial tri-plane features $\boldsymbol{T} = G(\hat{w})$. This new tri-plane representation is similarly processed by the rendering block $\mathcal{R}(\cdot)$ to obtain the final reconstructed image:
\begin{equation}
    y = \mathcal{R}(\boldsymbol{T} + \Delta \boldsymbol{T})
\end{equation}

The autoencoder follows the typical U-Net \cite{Ronneberger2015UNetCN} architecture, consisting of a contracting path and an expansive path. For our encoder backbone, we use the pre-trained IR-SE-50 architecture from \cite{Deng2018ArcFaceAA}. In the decoder network, instead of using typical nearest neighbor upsampling, we use sub-pixel convolutional layers \cite{Shi2016RealTimeSI} in order to efficiently upscale the extracted features. The decoder architecture is similar to that of RUNet \cite{Hu2019RUNet} with some minor modifications. Detailed overview of the architecture is presented in Appendix \ref{supp:arch-details}.  

\subsection{Loss Functions}
The pipeline is trained by minimizing the loss function that decomposes into the separate loss expressions for two branches:
\begin{equation}
    \mathcal{L}_{\phi, \psi}(x, y, \hat{y}) = \mathcal{L}_\phi(x, \hat{y}) + \mathcal{L}_\psi(x, y)
\end{equation}

For training the encoder $\phi(\cdot)$ in the first branch, we employ pixel-wise $\mathcal{L}_2$ loss, LPIPS loss \cite{Zhang2018TheUE}, and ID loss \cite{Deng2018ArcFaceAA}, closely following the proposed loss for pSp \cite{richardson2021encoding}. Therefore, the total loss formulation is given by
\begin{equation}
    \mathcal{L}_{\phi}(x, \hat{y}) = \lambda_1\mathcal{L}_2(x, \hat{y}) + \lambda_2\mathcal{L}_\textrm{LPIPS}(x, \hat{y}) + \lambda_3\mathcal{L}_{id}(x, \hat{y})
    \label{encoder-loss}
\end{equation} 
In order to train the auto-encoder $\psi(\cdot)$ in the second branch, we replace $\mathcal{L}_2$ in (\ref{encoder-loss}) with $\mathcal{L}_1$ smooth loss. Then, the loss objective becomes:
\begin{equation}
    \mathcal{L}_{\psi}(x, y) = \lambda_4\mathcal{L}_1(x, y) + \lambda_5\mathcal{L}_\textrm{LPIPS}(x, y) + \lambda_6\mathcal{L}_{id}(x, y)
    \label{ae-loss}
\end{equation}

\section{Experiments}
\setlength{\tabcolsep}{4.5pt}

\subsection{Training procedure}
\noindent \textbf{Datasets.} Since our focus is on the human facial domain, we use FFHQ \cite{Karras2018ASG} dataset and 100,000 synthesized images from EG3D pre-trained on FFHQ for training and perform the evaluation on the 2824 CelebA-HQ ~\cite{Liu2014DeepLF, Karras2017ProgressiveGO} test set. We extract the camera pose and pre-process the data in the same way as in \cite{chan2022efficient}. Since the pre-processing technique used couldn't identify the camera poses of 4 images, we skip the quantitative evaluation of 4 images for all the methods presented in the paper. Instead of using ground truth camera poses for synthesized images, we extract the camera pose following the same procedure as for other datasets to ensure consistency. We also augment the FFHQ dataset by mirroring it.  

\noindent \textbf{Training details.}
We conduct all our experiments in the human facial domain. Therefore, our pre-trained EG3D generator is also trained on the FFHQ dataset \cite{Karras2018ASG}. For training our models, we adopt the same training configuration from \cite{richardson2021encoding} except for some minor modifications, which will be explicitly mentioned here. We start the training of the second branch only after 20K steps and train both branches until 500K steps. Afterwards, we freeze the first branch and fine-tune the second branch until 1M steps. 
The model and the ablations are trained with a batch size of 3. We operate in the resolution of $256 \times 256$ and set the loss weights as follows: $\lambda_1=\lambda_2=1$, $\lambda_3=0.1$, $\lambda_4=1$, $\lambda_5=\lambda_6=0.1$.

\noindent \textbf{Baselines.} We compare our approach with both optimization- and encoder-based inversion methods. For optimization-based methods, we select $\mathcal{W}+$ optimization from \cite{karras2020analyzing} and PTI from \cite{roich2022pivotal}. For encoder-based methods, we choose pSp from \cite{richardson2021encoding}. For $\mathcal{W}+$ optimization, we optimize the latent code for 1K steps. For PTI, we first optimize the latent code $\hat{w} \in \mathcal{W}+$ for 1K steps and then fine-tune the generator for 1K steps. For pSp, we employ the original training configuration from \cite{richardson2021encoding} with a batch size of 4. In addition to FFHQ and mirrored FFHQ, we include 100K EG3D synthesized examples for training the pSp encoder. 

\setlength{\tabcolsep}{1.5pt}
\begin{table}
\caption{Quantitative comparison on image reconstruction. The inference time including the EG3D pass is given for a single RTX A100 Ti GPU. Our method is on par with PTI for frontal face inversion while being considerably faster.}
\vspace{-0.4cm}
\begin{center}
    \begin{tabular}{ l | c c c c | c}
        \hline
        Method\, & L$_2$ $\downarrow$ & LPIPS $\downarrow$ & ID $\uparrow$ & MS-SSIM $\uparrow$ & Infer. time $\downarrow$\\
        \hline
        $\mathcal{W}+$ \cite{karras2020analyzing} & \,0.08 & 0.18 & 0.72 & 0.78 & 77.07 s\\  
        PTI \cite{roich2022pivotal} & \,\textbf{0.011} & \textbf{0.07} & 0.85 & \textbf{0.90} & 42.27 s\\
        pSp \cite{richardson2021encoding} & \,0.049 & 0.18 & 0.69 & 0.70 & \textbf{0.04 s}\\
        \hline
        Ours & 0.016 & 0.09 & \textbf{0.88} & 0.89 & 0.07 s\\
    \end{tabular}
\end{center}
\label{quant-same-view-table}
\vspace{-0.75cm}
\end{table}

\subsection{Results}

\noindent \textbf{Comparison to the state-of-the-art.} We present the evaluation of our approach w.r.t. the baselines in Fig.~\ref{visualcomp} and Table~\ref{quant-same-view-table}. 
Commonly used metrics L$_2$, LPIPS \cite{Zhang2018TheUE}, MS-SSIM \cite{ms-ssim}, and ID similarity \cite{Deng2018ArcFaceAA} have been selected to analyze various aspects of perceptual similarity between inputs and corresponding reconstructions. 
Among others, PTI achieves the best results according to L$_2$, LPIPS, and MS-SSIM when the input image is re-rendered from the same view. 
However, our method outperforms the baselines according to the ID similarity, while being an order of magnitude times faster than optimization-based approaches. 
From a visual standpoint, our approach preserves finer details of the input image such as background objects, hair, and hat better than the other methods. 

Furthermore, we demonstrate the identity preservation quality of input image re-rendering from a novel view in Table~\ref{novel-view-table} and in Fig.~\ref{novelview}.
Our method significantly outperforms other inversion approaches w.r.t. ID similarity and extrapolates fine details to novel views in a more precise way.
The ID score declines faster above a certain value of the yaw angle due to the limitation of EG3D to represent angles broader than in its training distribution.

\begin{table}[t!]
\caption{Quantitative comparison on novel-view rendering. We significantly outperform the baselines by the ID score. L$_2$ and others not suitable here due to spatial misalignment.}
\vspace{-0.5cm}
\begin{center}
    \begin{tabular}{l|c c c c c c|c}
        \hline
        \multirow{3}{*}{Method} & \multicolumn{7}{c}{ID $\uparrow$}\\
        \cline{2-8}
        & \multicolumn{6}{c|}{Novel View (Yaw angle in rad)} & \multirow{2}{*}{Avg.}\\
& \,\,-0.8\,\, & \,\,-0.6\,\, & \,\,-0.3\,\, & \,\,0.3\,\, & \,\,0.6\,\, & \,\,0.8\,\,\, & \\
        \hline
        $\mathcal{W}+$ \cite{karras2020analyzing} & 0.61 & 0.63 & 0.67 & 0.67 & 0.63 & 0.61 & 0.64\\ 
        PTI \cite{roich2022pivotal} & 0.65 & 0.69 & 0.76 & 0.76 & 0.69 & 0.66 & 0.70\\
        pSp \cite{richardson2021encoding} & 0.62 & 0.64 & 0.67 & 0.68 & 0.65 & 0.62 & 0.65\\
        \hline
        Ours & \textbf{0.70} & \textbf{0.74} & \textbf{0.83} & \textbf{0.83} & \textbf{0.76} & \textbf{0.72} & \textbf{0.76}\\
    \end{tabular}
\end{center}
\label{novel-view-table}
\vspace{-0.75cm}
\end{table}

\begin{table}[b!]
    \vspace{-0.3cm}
    \centering
    \setlength\tabcolsep{1pt}
    \begin{tabular}{ccc}
    \includegraphics[width=0.15\textwidth]{images/4_experiments/ablation/1715/original/1715.jpg} & 
    \includegraphics[width=0.15\textwidth]{images/4_experiments/ablation/1715/lamda1/1715.jpg} & 
    \includegraphics[width=0.15\textwidth]{images/4_experiments/ablation/1715/psp-eg3d/1715.jpg}
    \\
    Input & $\lambda_5=1$ (LPIPS) & No $2^{nd}$ branch  \\
    \includegraphics[width=0.15\textwidth]{images/4_experiments/ablation/1715/triplane_ffhq/1715.jpg} & 
    \includegraphics[width=0.15\textwidth]{images/4_experiments/ablation/1715/wdis/1715.jpg} & 
    \includegraphics[width=0.15\textwidth]{images/4_experiments/ablation/1715/triplanenet/1715.jpg} \\
    w/o EG3D data & w/ $\mathcal{D}$ & \textbf{Ours} \\
    \end{tabular}
    \captionof{figure}{
        Qualitative ablation study for the loss, dataset, and architecture changes. $\mathcal{D}$ stands for the pre-trained EG3D dual discriminator~\cite{chan2022efficient} \textit{Electronic zoom-in recommended.}
    }
    \label{fig:ablation}
    \vspace{-0.3cm}
\end{table}

\begin{table*}
\caption{Quantitative ablation study for the loss, dataset, and architecture changes.}
\vspace{-0.4cm}
\begin{center}
    \begin{tabular}{l|c c c |c| c c c c c c |c}
        \hline
        \multirow{3}{*}{Method} & \multirow{3}{*}{\,\,\,\,L$_2$ $\downarrow$\,\,\,\,} & \multirow{3}{*}{LPIPS $\downarrow$} & \multirow{3}{*}{MS-SSIM $\uparrow$} &  \multicolumn{7}{c}{ID $\uparrow$}\\
        \cline{5-12}
        & & & & \multirow{2}{*}{Same View} & \multicolumn{7}{c}{Novel View (Yaw angle in radians)} \\
        \cline{6-12}
        & & & & & \,\,-0.8\,\, & \,\,-0.6\,\, & \,\,-0.3\,\, & \,\,0.3\,\, & \,\,0.6\,\, & \,\,0.8\,\, & \,\,Avg.\,\,\\
        \hline
        Ours & \textbf{0.018} & 0.11 & \textbf{0.87} & \textbf{0.84} & \textbf{0.68} & \textbf{0.72} & \textbf{0.79} & \textbf{0.80} & \textbf{0.73} & \textbf{0.69} & \textbf{0.73}\\
        \dots\, no sub-pixel layers & 0.019 & 0.11 & 0.87 & \textbf{0.84} & \textbf{0.68} & \textbf{0.72} & \textbf{0.79} & \textbf{0.80} & \textbf{0.73} & \textbf{0.69} & \textbf{0.73}\\
        \dots\, w/ discriminator & 0.019 & 0.12 & 0.86 & 0.83 & \textbf{0.68} & \textbf{0.72} & \textbf{0.79} & 0.79 & \textbf{0.73} & \textbf{0.69} & \textbf{0.73}\\
        \dots\, ${\lambda_5=1}$ (LPIPS) & 0.020 & \textbf{0.09} & 0.86 & 0.79 & 0.66 & 0.70 & 0.76 & 0.76 & 0.71 & 0.67 & 0.71\\
        \dots\, w/o EG3D samples\,\, & 0.021 & 0.13 & 0.86 & 0.83 & 0.67 & 0.71 & 0.78 & 0.78 & 0.72 & 0.67 & 0.72\\
pSp & 0.076 & 0.22 & 0.59 & 0.70 & 0.63 & 0.65 & 0.68 & 0.69 & 0.66 & 0.63 & 0.66\\
        \dots\, w/ EG3D samples & 0.049 & 0.18 & 0.70 & 0.69 & 0.62 & 0.64 & 0.67 & 0.68 & 0.65 & 0.62 & 0.65\\
        \hline
    \end{tabular}
\end{center}
\label{ablation-table}
\vspace{-0.5cm}
\end{table*}

\begin{table}[h!]
    \centering
    \setlength\tabcolsep{2pt}
    \begin{tabular}{cccccc}
        \multicolumn{2}{c}{\includegraphics[width=0.15\textwidth]{images/4_experiments/multiview/0061_org.jpg}} & 
        \multicolumn{2}{c}{\includegraphics[width=0.15\textwidth]{images/4_experiments/multiview/0061_-0.3.jpg}} & 
        \multicolumn{2}{c}{\includegraphics[width=0.15\textwidth]{images/4_experiments/multiview/0080_org.jpg}} \\
        \multicolumn{2}{c}{(a) video frame} & 
        \multicolumn{2}{c}{(b) novel view} & 
        \multicolumn{2}{c}{(c) nearest for \textit{(b)}} \\
        \multicolumn{3}{c}{\includegraphics[trim={3cm 0 0 0},clip,width=0.24\textwidth]{images/4_experiments/multiview/061_vs_080_colorbar.pdf}} &
        \multicolumn{3}{c}{\includegraphics[trim={3cm 0 0 0},clip,width=0.24\textwidth]{images/4_experiments/multiview/061_vs_sfm.pdf}}
        \\
        \multicolumn{3}{c}{(d) Recon. from view \textit{(a)}} & 
        \multicolumn{3}{c}{(e) Recon. from view \textit{(a)}}
        \\
        \multicolumn{3}{c}{vs. recon. from view \textit{(c)}} & 
        \multicolumn{3}{c}{vs. 3D model from SfM}
    \end{tabular}
    \captionof{figure}{
        For a sample frame \textit{(a)} of a multi-view sequence, we encode it by TriPlaneNet and render from a novel view \textit{(b)}, while \textit{(c)} is the nearest to \textit{(b)} frame in the sequence. In \textit{(d)}, we compare the 3D reconstructions obtained by marching cubes from EG3D after encoding the frames \textit{(a)} and \textit{(c)} by nearest neighbor distance from \textit{(a)} recon. to \textit{(c)} recon. (in \% of interocular distance). In \textit{(e)}, the recon. from \textit{(a)} view is compared to the 3D model estimated by SfM for the whole sequence, aligned to each other by face landmarks.
    }
    \label{multiview}
    \vspace{-0.3cm}
\end{table}

\begin{table}[h!]
\setlength{\tabcolsep}{0cm}
\renewcommand{\arraystretch}{0}
\begin{tabular}{c@{\hskip 0.2cm}cccc}
    \raisebox{2\height}{\rotatebox[origin=c]{90}{Input}} & 
    \includegraphics[width=0.113\textwidth]{images/4_experiments/video_frames/frame0_org.jpg} & 
    \includegraphics[width=0.113\textwidth]{images/4_experiments/video_frames/frame30_org.jpg} & 
    \includegraphics[width=0.113\textwidth]{images/4_experiments/video_frames/frame43_org.jpg} & 
    \includegraphics[width=0.113\textwidth]{images/4_experiments/video_frames/frame94_org.jpg}
    \\
    \raisebox{1.0\height}{\rotatebox[origin=c]{90}{Novel view}}  & 
    \includegraphics[width=0.113\textwidth]{images/4_experiments/video_frames/frame0_-0.3.jpg} & 
    \includegraphics[width=0.113\textwidth]{images/4_experiments/video_frames/frame30_-0.3.jpg} & 
    \includegraphics[width=0.113\textwidth]{images/4_experiments/video_frames/frame43_-0.3.jpg} & 
    \includegraphics[width=0.113\textwidth]{images/4_experiments/video_frames/frame94_-0.3.jpg} \\
    \raisebox{0.75\height}{\rotatebox[origin=c]{90}{Reconstruction}}  & 
    \includegraphics[trim={3cm 3cm 3cm 3cm},clip,width=0.113\textwidth]{images/4_experiments/video_frames/frame0_render_left.jpg}& 
    \includegraphics[trim={3cm 3cm 3cm 3cm},clip,width=0.113\textwidth]{images/4_experiments/video_frames/frame30_render_left.jpg} & 
    \includegraphics[trim={3cm 3cm 3cm 3cm},clip,width=0.113\textwidth]{images/4_experiments/video_frames/frame43_render_left.jpg} & 
    \includegraphics[trim={3cm 3cm 3cm 3cm},clip,width=0.113\textwidth]{images/4_experiments/video_frames/frame94_render_left.jpg}     
\end{tabular}
\vspace{-0.25cm}
\captionof{figure}{Novel view synthesis of frames extracted from a talking head video. \textit{Electronic zoom-in recommended.}}
\label{video:novel view}
\vspace{-0.4cm}
\end{table}

\noindent \textbf{Ablation study.}
In Fig.~\ref{fig:ablation} and Table~\ref{ablation-table}, we ablate over the possible differences in our model design, such as loss functions and changes introduced to the pSp~\cite{richardson2021encoding} model.
As some of those were introduced to mitigate checkerboard-style artifacts, we demonstrate visually how the LPIPS loss weight and incorporated pre-trained dual discriminator for EG3D affect the visual quality of the renderings. 
All models in this ablation are trained for 500K steps. We observed that setting LPIPS loss weight $\lambda_5$ to 1 introduces artifacts.
As can be seen in Table \ref{ablation-table}, synthesized examples from EG3D for training improve both pSp and TriPlaneNet. As demonstrated qualitatively and quantitatively, the best result is achieved by setting LPIPS loss weight to 0.1, including EG3D generated samples in the training set, using sub-pixel convolutional layers and disabling the discriminator.

\noindent \textbf{Evaluation for a multi-view sequence.} In Fig.~\ref{multiview}, we evaluate the multi-view consistency of the reconstruction on a short smartphone-captured multi-view video of a person asked to stand still. The video consists of 106 frames and includes left-to-right sweeps approx. from $-90^\circ$ to $90^\circ$ yaw angle. We evaluate the novel view rendering capability for a single video frame by comparing it to the nearest neighbor video frame and the consistency of the marching cubes reconstructions from EG3D. Similarly, we measure the per-point distance to the 3D model estimated by SfM software~\cite{agisoft} for the whole sequence. Despite that the shoulders and invisible head parts are hard to reconstruct for the method, we notice that the facial part and frontal hair are highly view-consistent and close to the ground truth.

\noindent \textbf{Novel view synthesis for a talking head video.}
We demonstrate an application of our method to render in-the-wild videos from a novel view.
In Fig.~\ref{video:novel view}, frames of a video with a person talking and their rendering from a fixed novel view in the EG3D space are presented. The background in the video was removed by a matting network~\cite{ke2022modnet}.
 
The encoder is capable of representing tiny details of in-the-wild portrait imagery in 3D and supports complex facial expressions.

\subsection{PTI and tri-plane offsets behavior}

\begin{table*}[t!]
\setlength{\tabcolsep}{0pt}
\begin{tabular}{clllclll}
                     & \multicolumn{3}{c}{\textcolor{RawSienna}{$\mathcal{W}+$ opt.} + \dots} & \multicolumn{4}{c}{\textcolor{blue}{$\mathcal{W}+$ pred.} + \dots}                              \\
\vspace{0.5cm}       & \multicolumn{3}{c}{\multirow{8}{*}{\includegraphics[trim={1cm 0cm 13.5cm 0.2cm},clip,height=8.7cm]{images/4_experiments/analysis/analysis_w_opt_combined.pdf}}}                          & \multicolumn{4}{c}{\multirow{8}{*}{\includegraphics[trim={1cm 0cm 10cm 0.2cm},clip,height=8.7cm]{images/4_experiments/analysis/analysis_w_pred_combined.pdf}}}                                                        \\
\includegraphics[width=0.13\textwidth]{images/4_experiments/analysis/Original/16989.jpg}
                    & \multicolumn{3}{c}{}                                           & \multicolumn{4}{c}{}                                                                         \\
Input                & \multicolumn{3}{c}{}                                           & \multicolumn{4}{c}{}                                                                         \\
                     & \multicolumn{3}{c}{}                                           & \multicolumn{4}{c}{}                                                                         \\
\vspace{0.5cm}       & \multicolumn{3}{c}{}                                           & \multicolumn{4}{c}{}                                                                         \\
\includegraphics[width=0.13\textwidth]{images/4_experiments/analysis/Original/28640.jpg}
                     & \multicolumn{3}{c}{}                                           & \multicolumn{4}{c}{}                                                                         \\
Input                & \multicolumn{3}{c}{}                                           & \multicolumn{4}{c}{}                                                                         \\
\vspace{0.5cm}       & \multicolumn{3}{c}{}                                           & \multicolumn{4}{c}{}                                                                         \\[-0.2cm]
                     &  \hspace{2.3cm}                      & {\small + \textcolor{RawSienna}{EG3D params}\,\,\,\,\,}   & {\small \textcolor{blue}{+ tri-plane}}   & \hspace{0.3cm}{\small (pSp)} & {\small \textcolor{RawSienna}{+ EG3D params}\,\,\,\,} & {\small \textcolor{RawSienna}{+ tri-plane}\,\,\,\,\,\,\,\,} & {\small + \textcolor{blue}{tri-plane pred.}} \\
\multicolumn{1}{l}{} & \multicolumn{1}{l}{}   & {\small \textcolor{RawSienna}{opt. (PTI)}}      &  {\small \textcolor{blue}{pred.}}                   & \hspace{2.5cm}                       & {\small \textcolor{RawSienna}{opt.}}          & {\small \textcolor{RawSienna}{opt.}}        & {\small \textbf{(Ours)} }          
\end{tabular}
\captionof{figure}{Comparison of hybrid approaches on CelebA-HQ test dataset. We refer to the computation done via optimization as \textcolor{RawSienna}{opt.} and via an encoder (pSp or TriPlaneNet) as \textcolor{blue}{pred.} We observe that the methods starting from \textcolor{RawSienna}{$\mathcal{W}+$ opt.} have issues with consistency, whereas subsequent \textcolor{blue}{tri-plane pred.} can partially alleviate it. Experiments starting from \textcolor{blue}{$\mathcal{W}+$ pred.} demonstrate that the tri-plane space is more spatially restrictive than the EG3D parameters space. \textit{Electronic zoom-in recommended.}}
\label{analysis:fig}
\end{table*}

Both our method and optimization- and encoder-based baselines can be decomposed into two stages: estimating the latent code and the delta for the generator parameters. In Fig.~\ref{analysis:fig}, we show how combining these steps, each performed either by optimization (\textcolor{RawSienna}{opt.}) or an encoder (\textcolor{blue}{pred.}), influences the inversion behavior.

\textcolor{RawSienna}{$\mathcal{W}+$ opt.} is performed per single image and, while it is capable of reconstructing the face in the same view, it doesn't account for 3D geometry due to the lack of the supervision from other views. As a result, we observe skewed rendering from a novel view. Accordingly, this is also the main reason why the consistency breaks with the PTI = (\textcolor{RawSienna}{$\mathcal{W}+$ opt.} + \textcolor{RawSienna}{EG3D params opt.}) method. Interestingly, tri-plane prediction, performed on top of \textcolor{RawSienna}{$\mathcal{W}+$ opt.}, can alleviate the damage to the geometry caused by \textcolor{RawSienna}{$\mathcal{W}+$ opt}.

\textcolor{blue}{$\mathcal{W}+$ pred.} by a pSp encoder, on the contrary, introduces more consistent geometry, since it was trained to reconstruct images from arbitrary views. At the same time, the rendering quality in the same view is marginally worse than PTI. Applying the PTI's second step (\textcolor{RawSienna}{EG3D params opt.}) helps improve it significantly, however, it incorrectly modifies head proportions, similar to the \textcolor{RawSienna}{$\mathcal{W}+$ opt.} behavior. To investigate this effect further, instead of optimizing EG3D parameters after \textcolor{blue}{$\mathcal{W}+$ pred.}, we try optimizing the tri-plane offsets directly, and this fully cancels the deterioration of geometry while preserving high fidelity in the same view. Since both \textcolor{RawSienna}{EG3D params opt.} and \textcolor{RawSienna}{tri-plane opt.} are performed for a single image (i.e.~without multi-view supervision during training), this may indicate that offsetting the tri-planes is more spatially restrictive and thus stable. Our method reduces the artifacts observed with \textcolor{blue}{$\mathcal{W}+$ pred.} + \textcolor{RawSienna}{tri-plane opt.} by utilizing an encoder for tri-plane offsets.

\section{Conclusion}
\vspace{-0.15cm}
We present a novel approach for EG3D inversion that achieves high-quality reconstructions with view consistency and can be run in real time on modern GPUs. 
We also show that directly utilizing tri-plane representation better preserves 3D structure and identity in novel views compared to other approaches. 
Although our method achieves compelling results and is on par with optimization-based approaches for frontal face inversion, both visually and quantitatively, it has certain limitations.
For instance, it is limited by the range of yaw angles shown to EG3D during training and cannot model the background depth.
In addition, similarly to PTI and others, our approach may introduce artifacts for challenging examples in CelebA-HQ where the prediction of the first branch differs significantly from the input. 
We show a detailed analysis of such cases in Appendix \ref{CCTR} and propose a \textit{cascaded test-time refinement} (CTTR) approach for improving the method's robustness.

\section*{Acknowledgments}
This work was supported by the ERC Starting Grant Scan2CAD (804724) and the German Research Foundation (DFG) Research Unit "Learning and Simulation in Visual Computing." We also thank Yawar Siddiqui, Guy Gafni, Shivangi Aneja, and Simon Giebenhain for participating in the sample videos and helpful discussions.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\appendix
\section{Training details}

We operate in the resolution of $256 \times 256$ except for the calculation of $\mathcal{L}_{id}$. Specifically, the region around the face is cropped and resized to $112 \times 112$ before feeding into the face recognition network \cite{Deng2018ArcFaceAA} to calculate $\mathcal{L}_{id}$. To train our models, we use the Ranger optimizer that combines Rectified Adam \cite{Liu2019OnTV} with the Lookahead technique \cite{Zhang2019LookaheadOK} and set a learning rate to 0.0001. The model is trained using a single NVIDIA GeForce RTX 3090 GPU.

\subsection{EG3D samples added to the training data set}

Our model is trained on a combination of real images from FFHQ and generated samples from EG3D. As shown in the ablation study in the main text, adding EG3D samples helps both pSp~\cite{roich2022pivotal} and TriPlaneNet. The synthetic training samples are generated via sampling latent codes $z$ for EG3D with no truncation ($\psi = 1$) often applied for large-scale GANs~\cite{brock2018large}, thus including the hard samples. Figure \ref{fig:psi} shows the comparison between FFHQ and EG3D samples train losses for $\psi=1$ and $\psi=0.7$. We attribute the difference between the curves with $\psi=0.7$ to the domain gap between the generated samples from EG3D and real ones from FFHQ. In order to match the camera pose distribution in the FFHQ, we randomly sample FFHQ camera poses including mirrored while generating EG3D samples.

\begin{table*}
\caption{Quantitative ablation study over the number of synthesized EG3D samples in the training set. We do not highlight any specific numbers in bold to demonstrate that the difference between \{10K, \dots, 100K\} is relatively negligible. Due to that, the number of EG3D samples was selected according to the least observed overfitting (see Fig.~\ref{fig:test_loss_vs_EG3D_samples}).}
\vspace{-0.4cm}
\begin{center}
\resizebox{\textwidth}{!}{
    \begin{tabular}{l|c c c |c| c c c c c c |c}
        \hline
        \multirow{3}{*}{Our Method} & \multirow{3}{*}{\,\,\,\,L$_2$ $\downarrow$\,\,\,\,} & \multirow{3}{*}{LPIPS $\downarrow$} & \multirow{3}{*}{MS-SSIM $\uparrow$} &  \multicolumn{7}{c}{ID $\uparrow$}\\
        \cline{5-12}
        & & & & \multirow{2}{*}{Same View} & \multicolumn{7}{c}{Novel View (Yaw angle in radians)} \\
        \cline{6-12}
        & & & & & \,\,-0.8\,\, & \,\,-0.6\,\, & \,\,-0.3\,\, & \,\,0.3\,\, & \,\,0.6\,\, & \,\,0.8\,\, & \,\,Avg.\,\,\\
        \hline
        \dots\, w/ 0 EG3D samples\,\, & 0.021 & 0.13 & 0.86 & 0.83 & 0.67 & 0.71 & 0.78 & 0.78 & 0.72 & 0.67 & 0.72\\
        \dots\, w/ 10K EG3D samples & 0.019 & 0.11 & 0.87 & 0.85 & 0.68 & 0.72 & 0.80 & 0.80 & 0.74 & 0.69 & 0.74\\
        \dots\, w/ 25K EG3D samples & 0.018 & 0.11 & 0.87 & 0.84 & 0.68 & 0.72 & 0.79 & 0.80 & 0.74 & 0.69 & 0.74\\
        \dots\, w/ 50K EG3D samples & 0.019 & 0.12 & 0.87 & 0.85 & 0.69 & 0.73 & 0.80 & 0.81 & 0.74 & 0.70 & 0.74\\
        \dots\, w/ 75K EG3D samples & 0.018 & 0.12 & 0.87 & 0.83 & 0.67 & 0.71 & 0.78 & 0.79 & 0.73 & 0.69 & 0.73\\
        \dots\, w/ 100K EG3D samples & 0.018 & 0.11 & 0.87 & 0.84 & 0.68 & 0.72 & 0.79 & 0.80 & 0.73 & 0.69 & 0.73\\
        \hline
    \end{tabular}
}
\end{center}
\label{eg3d-samples-table}
\vspace{-0.5cm}
\end{table*} \begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/supplement/number_eg3d_samples/test_loss_vs_EG3D_samples.pdf}
    \caption{Test loss (calculated over the CelebA-HQ) during training for experiments with varying number of EG3D samples. The choice of 100K EG3D samples turns out to be optimal w.r.t. the test loss closer to the end of training.}
    \label{fig:test_loss_vs_EG3D_samples}
\end{figure} \begin{figure*}[!h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/supplement/truncation_psi/ffhqvseg3d_0.7.pdf}
        \caption{$\psi=0.7$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/supplement/truncation_psi/ffhqvseg3d_1.0.pdf}
        \caption{$\psi=1.0$ (Ours)}
    \end{subfigure}
    \caption{Training curves for two values of the truncation multiplier $\psi$ defining the difficulty of the synthetic samples. The loss for the batches consisting fully of FFHQ and fully of EG3D samples is reported separately to demonstrate the consequences of selecting $\psi$ on the observed domain gap between the real and synthetic data.}
    \label{fig:psi}
\end{figure*} 
In Table \ref{eg3d-samples-table}, we demonstrate the dependence of the reconstruction quality on CelebA-HQ on the number of synthetic samples, created by EG3D in advance added to the dataset. Although the change in metrics is marginal with respect to the increasing number of EG3D samples, test loss shown in Figure \ref{fig:test_loss_vs_EG3D_samples} demonstrates that the amount of 100K samples, used for the final model, is close to optimal. 
 
\section{Architecture details}
\label{supp:arch-details}
\noindent \textbf{First Branch} To implement the encoder, we adopt the design of the pSp encoder from ~\cite{richardson2021encoding}. As the EG3D generator expects 14 styles vector for the selected resolution, we modify the pSp architecture to output 14 styles vector instead of 18. For the backbone network, we employ IR-SE-50 \cite{Deng2018ArcFaceAA} pretrained for face recognition.\\

\noindent \textbf{Second Branch (TriPlaneNet)} Our TriPlaneNet consists of an encoder and a decoder network, a typical U-Net \cite{Ronneberger2015UNetCN} architecture. The encoder backbone is an IR-SE-50 \cite{Deng2018ArcFaceAA} pretrained on face recognition, which accelerates convergence. We adopt the design of the RUNet \cite{Hu2019RUNet} for the decoder with some minor modifications. Instead of using ReLU as in RUNet, we use PReLU \cite{He2015DelvingDI} with a separate $\alpha$ for each input channel and an initial value of 0.25. Similar to RUNet, every step in the decoder path consists of upsampling, concatenation, and convolution operations. Upsamping of the feature map is performed with a sub-pixel convolutional layer. Then, it is followed by a concatenation with the intermediate feature maps from the encoder path. The intermediate features are extracted from the 2nd, 6th, 20th, and 21st layers of the encoder. Finally, batch normalization, $3 \times 3$ convolution, PReLU, $3 \times 3$ convolution, and PReLU are applied sequentially. The last step in the decoder path comprises sub-pixel convolutional, $3 \times 3$ convolution, PReLU,  $3 \times 3$ convolution, PReLU, and $1 \times 1$ convolution operations, while result in $256 \times 256 \times 96$ tri-plane offsets in the end.

\section{Cascaded Test-Time Refinement (CTTR)}
\label{CCTR}
We propose a Cascaded Test-Time Refinement (CTTR) procedure to refine the reconstructions for harder cases (see Figure \ref{method:cttr} for the overview).
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{images/supplement/CTTR/cttr.pdf}
    \caption{Overview of the Cascaded Test-Time Refinement (CTTR). In the CCTR step, the input image $x$ and reconstructed image $y$ is passed once again through the already trained second branch of TriPlaneNet to obtain the refined tri-plane offsets $\Delta \boldsymbol{T_r}$. The refined tri-offsets are added to the initial tri-plane features $\boldsymbol{T}$, which are then processed by the renderer block $\mathcal{R}(\cdot)$ to yield the refined reconstruction $y_r = \mathcal{R}(\boldsymbol{G}(\hat{w}) + \Delta \boldsymbol{T_r})$.}.
    \label{method:cttr}
\end{figure*} At the inference time, given a hard input image $x$ and final image $y$ reconstructed by the second branch of TriPlaneNet, we reuse the second branch by passing $y$ as a replacement for the previous $\hat{y}$ in the second branch input. This provides us the refined tri-plane offsets $\Delta \boldsymbol{T_r}$:
\begin{equation}
    \Delta \boldsymbol{T_r} = \boldsymbol{\psi}(y, x - y)
\end{equation}
This $\Delta \boldsymbol{T_r}$ is summed with the initial tri-plane features $\boldsymbol{T} = G(\hat{w})$ and passed through the rendering block $\mathcal{R}(\cdot)$ to obtain the new refined image:
\begin{equation}
    y_r = \mathcal{R}(\boldsymbol{T} + \Delta \boldsymbol{T_r})
\end{equation}
The same procedure can be applied $N$ times in a cascaded way by reusing the image $y_r$ as a new input to the second branch. However, we found that applying it only once is a reasonable trade-off between quality and the proportionally increased inference time. We show how each additional step of CCTR affects the reconstruction quality and increases the inference time in Figure \ref{fig:refinement:iterations}. \begin{table*}[h!]
    \centering
    \setlength\tabcolsep{1pt}
    \begin{tabular}{cccccc}
    Input & No CCTR & CCTRx1  & CCTRx2 & CCTRx3 & CCTRx4 \\
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/original/12229.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/0/12229.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/1/12229.jpg}
    &
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/2/12229.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/3/12229.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/4/12229.jpg} \\
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/original/879.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/0/879.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/1/879.jpg}
    &
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/2/879.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/3/879.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/refinement/iterations/4/879.jpg} \\
     Infer. Time $\rightarrow$ & 0.07 s & 0.10 s & 0.13 s & 0.16 s & 0.19 s \\
    \end{tabular}
    \captionof{figure}{Comparison of reconstruction quality and inference time without and with each additional step of CCTR. 
    % As the inputs to the TriPlaneNet remain almost consistent for each additional step of CCTR, the results are similar for the odd and even numbers of cascades.
    Since single CCTR step brings the image closer to the pSp (first branch) output in terms of the plausibility, CTTRx\{1,3\} and CTTRx\{0,2,4\} look similar.
    }
    \label{fig:refinement:iterations}
    \vspace{-0.3cm}
\end{table*} 
Figure \ref{refinement-easy:fig} and \ref{refinement-hard:fig} shows the visual comparison before and after the refinement for two groups of examples: \textit{easy samples,} the reconstruction before refinement is plausible and close to the input, and \textit{hard samples,} where some regions, e.g. eyes, feature significant artifacts prior to the refinement. For easy samples, we observe that refinement removes very fine details and does not help with such cases. However, for hard samples, even though refinement does not fully preserve the input image, it creates more plausible images and removes artifacts. Therefore, we recommend applying the refinement approach in more challenging cases.

\section{Novel view rendering of videos}
We demonstrate additional results in Figure \ref{video:novel-view-supp} for novel view rendering of in-the-wild videos. 

\section{Discussion of the baselines design}
\begin{figure*}[!t]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/supplement/baselines/w+opt.pdf}
        \caption{\textcolor{RawSienna}{$\mathcal{W}+$ opt.}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/supplement/baselines/triplaneopt.pdf}
        \caption{\textcolor{RawSienna}{Tri-plane opt.}}
    \end{subfigure}

    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/supplement/baselines/params_opt.pdf}
        \caption{\textcolor{RawSienna}{EG3D params opt.}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/supplement/baselines/w+pred.pdf}
        \caption{\textcolor{blue}{$\mathcal{W}+$ pred.} (pSp~\cite{richardson2021encoding})}
    \end{subfigure}
    \caption{Overview of our baseline approaches. $\boldsymbol{G}(\cdot)$ and $\mathcal{R}(\cdot)$ stand for EG3D generator and renderer blocks respectively. Hybrid approaches described in the main text constitute the combination of the techniques shown above, applied sequentially one after another. For instance, PTI~\cite{roich2022pivotal} sequentially performs (a) and then (c)., and \textcolor{blue}{$\mathcal{W}+$ pred.} + \textcolor{RawSienna}{tri-plane opt.} sequentially performs (d) and then (b).}
    \label{fig:baselines}
\end{figure*} In Figure \ref{fig:baselines}, we provide visual overview of the baseline designs used for the analysis of PTI \cite{roich2022pivotal} and tri-plane offsets behavior. One-stage inversion techniques can be divided into two approaches: optimization-based and based on an encoder prediction. Two-stage inversion techniques involve inference followed by fine-tuning  of some of the generator parameters. Similarly, these parameters can be fine-tuned via optimization or by encoder prediction. Since our method involves prediction of the tri-plane offsets and avoid fine-tuning of the generator parameters, we also consider the baseline where the tri-plane offsets in the second stage are optimized. To furthermore understand PTI \cite{roich2022pivotal} and the tri-plane offsets behavior, we design different hybrid two-stage inversion approaches that combine both optimization and prediction. 

For \textcolor{RawSienna}{$\mathcal{W}+$ opt.}, we optimize the latent code $w \in \mathcal{W}+$ for 1K steps following \cite{karras2020analyzing}. The \textcolor{blue}{$\mathcal{W}+$ pred.} constitutes the baseline with the latent code $w \in \mathcal{W}+$ predicted by pSp encoder~\cite{richardson2021encoding}. For \textcolor{RawSienna}{EG3D params opt.}, we apply the second stage of PTI from \cite{roich2022pivotal} and optimize for 1K steps. To optimize for the tri-plane offsets (\textcolor{RawSienna}{tri-plane opt.}), we use L-BFGS~\cite{liu1989limited} as the optimizer and employ combination of $L_2$ or LPIPS~\cite{Zhang2018TheUE} with regularization term ($L_2$ and LPIPS discrepancy with the first branch prediction) as a loss objective. We run the optimization for 50 steps. As L-BFGS approximates the Hessian by calculating several estimates in a single step, 50 steps is equivalent to optimizing EG3D params for 1K steps.

\section{Additional Qualitative Results}
In this section, we present additional qualitative results on same view inversion, novel view rendering and ablation for the loss, dataset, and architecture changes.

\begin{itemize}
    \item Figures \ref{supp:visualcomp1} and \ref{supp:visualcomp2} provide qualitative comparison of our approach with existing state-of-the-art inversion techniques for image reconstruction.
    
    \item Figures \ref{supp:visualcomp3}, \ref{supp:visualcomp4}, \ref{supp:visualcomp5}, \ref{supp:visualcomp6}, and \ref{supp:visualcomp7} demonstrate qualitative comparison of our approach with existing state-of-the-art inversion techniques on novel-view rendering.
    \item Figure \ref{fig:ablation-supp} contains extensive qualitative ablation study for the loss, dataset, and architecture changes.
\end{itemize}

\begin{table*}[h!]
\setlength{\tabcolsep}{0cm}
\renewcommand{\arraystretch}{0}
\begin{tabular}{c@{\hskip 0.2cm}cccccccc}
    \raisebox{2\height}{\rotatebox[origin=c]{90}{Input}}
    & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/original/0.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/original/13.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/original/54.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/original/79.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/original/143.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/original/166.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/original/181.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/original/207.jpg}
    \\
    \raisebox{1.0\height}{\rotatebox[origin=c]{90}{Novel view}}  & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/rotated/0_-0.3.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/rotated/13_-0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/rotated/54_-0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/rotated/79_-0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/rotated/143_-0.3.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/rotated/166_-0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/rotated/181_-0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video2/rotated/207_-0.3.jpg}
    
    \\[0.3cm]
    
    \raisebox{2\height}{\rotatebox[origin=c]{90}{Input}}
    & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/original/0.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/original/10.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/original/61.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/original/85.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/original/121.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/original/134.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/original/148.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/original/217.jpg}
    \\
    \raisebox{1.0\height}{\rotatebox[origin=c]{90}{Novel view}}  & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/rotated/0_0.6.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/rotated/10_0.6.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/rotated/61_0.6.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/rotated/85_0.6.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/rotated/121_0.6.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/rotated/134_0.6.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/rotated/148_0.6.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video3/rotated/217_0.6.jpg}

    \\[0.3cm]
    
    \raisebox{2\height}{\rotatebox[origin=c]{90}{Input}}
    & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/original/0.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/original/4.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/original/23.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/original/166.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/original/208.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/original/218.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/original/201.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/original/213.jpg}
    \\
    \raisebox{1.0\height}{\rotatebox[origin=c]{90}{Novel view}}  & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/rotated/0_0.3.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/rotated/4_0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/rotated/23_0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/rotated/166_0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/rotated/208_0.3.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/rotated/218_0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/rotated/201_0.3.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video5/rotated/213_0.3.jpg}

    \\[0.3cm]
    
    \raisebox{2\height}{\rotatebox[origin=c]{90}{Input}}
    & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/original/0.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/original/4.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/original/35.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/original/46.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/original/75.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/original/113.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/original/170.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/original/230.jpg}
    \\
    \raisebox{1.0\height}{\rotatebox[origin=c]{90}{Novel view}}  & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/rotated/0_0.8.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/rotated/4_0.8.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/rotated/35_0.8.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/rotated/46_0.8.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/rotated/75_0.8.jpg} & 
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/rotated/113_0.8.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/rotated/170_0.8.jpg} &
    \includegraphics[width=0.12\textwidth]{images/supplement/video_frames/video6/rotated/230_0.8.jpg}
\end{tabular}
\vspace{-0.25cm}
\captionof{figure}{More novel view synthesis of frames extracted from a talking head video. The novel view yaw angles from the frontal are as follows: second row (0.3 radians), fourth row (0.6 radians), sixth row (0.3 radians), and last row (0.8 radians). \textit{Electronic zoom-in recommended.}}
\label{video:novel-view-supp}
\vspace{-0.4cm}
\end{table*} \begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{c@{\hskip 0.3cm}ccccc}
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/1052.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/26640.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/16160.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/24296.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/1379.jpg}
    \\[0.3cm]
    \raisebox{1.5\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/1052.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/26640.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/16160.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/24296.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/1379.jpg}
    \\
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/1052.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/26640.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/16160.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/24296.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/1379.jpg}
    \\
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/1052.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/26640.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/16160.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/24296.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/1379.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/1052.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/26640.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/16160.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/24296.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/1379.jpg}
    \\
    \raisebox{1.1\height}{\rotatebox[origin=c]{90}{\textbf{Ours (Refinement)}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/1052.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/26640.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/16160.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/24296.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/1379.jpg}
    \\
\end{tabular}
\captionof{figure}{Additional qualitative comparison for image reconstruction. The last row additionally demonstrates the results with the proposed refinement procedure (CTTR) which is only required for the challenging cases (see Fig.~\ref{refinement-hard:fig}).}
\label{supp:visualcomp1}
\vspace{-0.3cm}
\end{table*}

\begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{c@{\hskip 0.3cm}ccccc}
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/1523.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/1842.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/3532.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/4141.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/original/3944.jpg}
    \\[0.3cm]
    \raisebox{1.5\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/1523.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/1842.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/3532.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/4141.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/w_plus/3944.jpg}
    \\
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/1523.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/1842.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/3532.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/4141.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/PTI/3944.jpg}
    \\
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/1523.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/1842.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/3532.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/4141.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/pSp/3944.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/1523.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/1842.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/3532.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/4141.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/triplanenet/3944.jpg}
    \\
    \raisebox{1.1\height}{\rotatebox[origin=c]{90}{\textbf{Ours (Refinement)}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/1523.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/1842.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/3532.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/4141.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/inversion/refinement/3944.jpg}
\end{tabular}
\captionof{figure}{Additional qualitative comparison for image reconstruction. The last row additionally demonstrates the results with the proposed refinement procedure (CTTR) which is only required for the challenging cases (see Fig.~\ref{refinement-hard:fig}).}
\label{supp:visualcomp2}
\vspace{-0.3cm}
\end{table*} \begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{cc@{\hskip 0.3cm}ccccc}
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/12253.jpg}
    &
    \raisebox{1.5\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/12253_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/12253_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/12253_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/12253_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/12253.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/12253_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/12253_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/12253_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/12253_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/12253.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/12253_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/12253_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/12253_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/12253_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/12253.jpg}
    &
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/12253_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/12253_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/12253_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/12253_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/12253.jpg}
    &
    \raisebox{1.1\height}{\rotatebox[origin=c]{90}{\textbf{Ours (Refinement)}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/12253_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/12253_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/12253_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/12253_0.6.jpg}
\end{tabular}
\captionof{figure}{Additional qualitative evaluation on novel view rendering of yaw angle -0.6, -0.3, 0.3, and 0.6 radians.}
\label{supp:visualcomp3}
\vspace{-0.3cm}
\end{table*}


\begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{cc@{\hskip 0.3cm}ccccc}
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/28451.jpg}
    &
    \raisebox{1.5\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/28451_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/28451_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/28451_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/28451_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/28451.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/28451_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/28451_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/28451_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/28451_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/28451.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/28451_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/28451_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/28451_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/28451_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/28451.jpg}
    &
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/28451_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/28451_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/28451_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/28451_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/28451.jpg}
    &
    \raisebox{1.1\height}{\rotatebox[origin=c]{90}{\textbf{Ours (Refinement)}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/28451_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/28451_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/28451_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/28451_0.6.jpg}
\end{tabular}
\captionof{figure}{Additional qualitative evaluation on novel view rendering of yaw angle -0.6, -0.3, 0.3, and 0.6 radians.}
\label{supp:visualcomp4}
\vspace{-0.3cm}
\end{table*}

\begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{cc@{\hskip 0.3cm}ccccc}
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/16531.jpg}
    &
    \raisebox{1.5\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/16531_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/16531_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/16531_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/16531_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/16531.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/16531_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/16531_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/16531_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/16531_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/16531.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/16531_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/16531_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/16531_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/16531_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/16531.jpg}
    &
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/16531_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/16531_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/16531_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/16531_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/16531.jpg}
    &
    \raisebox{1.1\height}{\rotatebox[origin=c]{90}{\textbf{Ours (Refinement)}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/16531_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/16531_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/16531_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/16531_0.6.jpg}
\end{tabular}
\captionof{figure}{Additional qualitative evaluation on novel view rendering of yaw angle -0.6, -0.3, 0.3, and 0.6 radians.}
\label{supp:visualcomp5}
\vspace{-0.3cm}
\end{table*}

\begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{cc@{\hskip 0.3cm}ccccc}
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/18703.jpg}
    &
    \raisebox{1.5\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/18703_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/18703_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/18703_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/18703_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/18703.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/18703_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/18703_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/18703_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/18703_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/18703.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/18703_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/18703_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/18703_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/18703_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/18703.jpg}
    &
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/18703_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/18703_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/18703_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/18703_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/18703.jpg}
    &
    \raisebox{1.1\height}{\rotatebox[origin=c]{90}{\textbf{Ours (Refinement)}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/18703_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/18703_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/18703_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/18703_0.6.jpg}
\end{tabular}
\captionof{figure}{Additional qualitative evaluation on novel view rendering of yaw angle -0.6, -0.3, 0.3, and 0.6 radians.}
\label{supp:visualcomp6}
\vspace{-0.3cm}
\end{table*}

\begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{cc@{\hskip 0.3cm}ccccc}
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/26015.jpg}
    &
    \raisebox{1.5\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/26015_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/26015_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/26015_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/w_plus/26015_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/26015.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/26015_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/26015_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/26015_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/PTI/26015_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/26015.jpg}
    &
    \raisebox{3.0\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/26015_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/26015_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/26015_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/pSp/26015_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/26015.jpg}
    &
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/26015_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/26015_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/26015_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/triplanenet/26015_0.6.jpg}
    \\
    \raisebox{3.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/original/26015.jpg}
    &
    \raisebox{1.1\height}{\rotatebox[origin=c]{90}{\textbf{Ours (Refinement)}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/26015_-0.6.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/26015_-0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/26015_0.3.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/rotation/refinement/26015_0.6.jpg}
\end{tabular}
\captionof{figure}{Additional qualitative evaluation on novel view rendering of yaw angle -0.6, -0.3, 0.3, and 0.6 radians.}
\label{supp:visualcomp7}
\vspace{-0.3cm}
\end{table*}

 \begin{table*}[h!]
    \centering
    \setlength\tabcolsep{1pt}
    \begin{tabular}{cccccc}
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/original/6506.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/lambda_1/6506.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/pSp/6506.jpg}
    &
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplane_ffhq/6506.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/discriminator/6506.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplanenet/6506.jpg} \\
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/original/7531.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/lambda_1/7531.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/pSp/7531.jpg}
    &
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplane_ffhq/7531.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/discriminator/7531.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplanenet/7531.jpg} \\
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/original/9413.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/lambda_1/9413.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/pSp/9413.jpg}
    &
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplane_ffhq/9413.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/discriminator/9413.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplanenet/9413.jpg} \\
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/original/16504.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/lambda_1/16504.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/pSp/16504.jpg}
    &
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplane_ffhq/16504.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/discriminator/16504.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplanenet/16504.jpg} \\
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/original/16976.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/lambda_1/16976.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/pSp/16976.jpg}
    &
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplane_ffhq/16976.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/discriminator/16976.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplanenet/16976.jpg} \\
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/original/18430.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/lambda_1/18430.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/pSp/18430.jpg}
    &
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplane_ffhq/18430.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/discriminator/18430.jpg}
    & 
    \includegraphics[width=0.15\textwidth]{images/supplement/ablation/triplanenet/18430.jpg} \\
    Input & $\lambda_5=1$ (LPIPS) & No $2^{nd}$ branch  & w/o EG3D data & w/ $\mathcal{D}$ & \textbf{Ours}
    \end{tabular}
    \captionof{figure}{Additional qualitative ablation study for the loss, dataset, and architecture changes. $\mathcal{D}$ stands for the pre-trained EG3D dual discriminator \cite{chan2022efficient}. 
    }
    \label{fig:ablation-supp}
    \vspace{-0.3cm}
\end{table*} \begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{c@{\hskip 0.3cm}ccccc}
    \raisebox{2.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/original/15507.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/original/15929.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/original/15933.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/original/12783.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/original/12025.jpg}
    \\[0.3cm]
    \raisebox{1.2\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/w_plus/15507.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/w_plus/15929.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/w_plus/15933.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/w_plus/12783.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/w_plus/12025.jpg}
    \\
    \raisebox{1.7\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/PTI/15507.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/PTI/15929.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/PTI/15933.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/PTI/12783.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/PTI/12025.jpg}
    \\
    \raisebox{1.7\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/pSp/15507.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/pSp/15929.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/pSp/15933.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/pSp/12783.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/pSp/12025.jpg}
    \\
    \raisebox{2.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/triplanenet/15507.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/triplanenet/15929.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/triplanenet/15933.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/triplanenet/12783.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/triplanenet/12025.jpg}
    \\
    \raisebox{1\height}{\rotatebox[origin=c]{90}{\textbf{Ours (Refinement)}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/refinement/15507.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/refinement/15929.jpg}
    & 
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/refinement/15933.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/refinement/12783.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/easy/refinement/12025.jpg}
\end{tabular}
\captionof{figure}{Qualitative comparison for image reconstruction on easy cases. Our proposed CTTR approach (last row) does not have much effect for well-reconstructed examples with our inversion framework (TriPlaneNet).}
\label{refinement-easy:fig}
\vspace{-0.3cm}
\end{table*}

\begin{table*}[]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{c@{\hskip 0.3cm}ccccc}
    \raisebox{2.5\height}{\rotatebox[origin=c]{90}{Input}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/original/14816.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/original/1070.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/original/4368.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/original/84.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/original/7963.jpg}
    \\[0.3cm]
    \raisebox{1.2\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/w_plus/14816.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/w_plus/1070.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/w_plus/4368.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/w_plus/84.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/w_plus/7963.jpg}
    \\
    \raisebox{1.7\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/PTI/14816.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/PTI/1070.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/PTI/4368.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/PTI/84.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/PTI/7963.jpg}
    \\
    \raisebox{1.7\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/pSp/14816.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/pSp/1070.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/pSp/4368.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/pSp/84.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/pSp/7963.jpg}
    \\
    \raisebox{2.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/triplanenet/14816.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/triplanenet/1070.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/triplanenet/4368.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/triplanenet/84.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/triplanenet/7963.jpg}
    \\
    \raisebox{1\height}{\rotatebox[origin=c]{90}{\textbf{Ours (Refinement)}}}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/refinement/14816.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/refinement/1070.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/refinement/4368.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/refinement/84.jpg}
    &
    \includegraphics[width=0.19\textwidth]{images/supplement/refinement/hard/refinement/7963.jpg}   
\end{tabular}
\captionof{figure}{Qualitative comparison for image reconstruction on challenging cases. We observe that some of the face regions, e.g. eyes, look more plausible with the CTTR procedure (last row) in these cases, at the cost of slightly less preserved identity and increased inference time.}
\label{refinement-hard:fig}
\vspace{-0.3cm}
\end{table*}  
\end{document}
