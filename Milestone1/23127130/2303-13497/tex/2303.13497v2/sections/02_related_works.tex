\section{Related Work}\label{relatedwork}
\noindent \textbf{3D Generative Models for Human Faces.}
Representing and generating diverse 3D human faces and heads attracted increasing attention over the last decade \cite{nguyen2019hologan,chen2016infogan,huang2017beyond}, while the appearance of NeRF~\cite{mildenhall2021nerf} has sparked additional interest in that topic. 
The first generative models built upon NeRF-style volumetric integration \cite{schwarz2020graf,niemeyer2021giraffe} achieved generalization by conditioning the multi-layer perceptron on latent codes, representing the object's shape and appearance. 
Later introduced $\pi$-GAN \cite{chan2021pi} and StyleNeRF \cite{gu2021stylenerf} condition the generative network on the output of the StyleGAN-like generator~\cite{karras2020analyzing}, which amounted to the higher-quality rendering of faces and arbitrary objects with subtle details. 
As a next major improvement step, authors of EG3D~\cite{chan2022efficient} propose a tri-plane 3D representation that serves as a bridge between expressive implicit representations and spatially-restricting explicit representations. 
As a byproduct, methods such as EG3D and StyleSDF~\cite{or2022stylesdf} allow for the extraction of explicit, highly detailed geometry of the human faces, despite the fact that they are trained without any volumetric supervision.
Further, recently demonstrated abilities of diffusion models to generate highly accurate 2D images are currently being transferred onto 3D objects~\cite{zeng2022lion,muller2022diffrf} and 3D human heads~\cite{wang2022rodin,pan2023avatarstudio}.

\noindent \textbf{GAN Inversion.} Unlike other kinds of generative models, such as VAE or normalizing flows, inverting a GAN (finding the appropriate latent code for a given image) is oftentimes a tricky and computationally demanding task. 
Early attempts focused on the tuning of the latent code with the optimization-based approaches~\cite{creswell2018inverting,lipton2017precise,karras2020analyzing}. 
Various approaches exploited the idea of predicting latent representation by an encoder~\cite{guan2020collaborative,luo2017learning,perarnau2016invertible,zhu2016generative,park2019semantic}.
In~\cite{roich2022pivotal}, a universal PTI method is introduced, which comprises the optimization of a latent code and, consequently, fine-tuning parameters of the generator.
A recent survey on GAN inversion~\cite{xia2022gan} compares multiple generic techniques introduced since the appearance of GANs.

\noindent \textbf{Inversion of 2D GAN.} For StyleGAN, an important observation was made by the authors of~\cite{abdal2019image2stylegan} that operating in the extended $\mathcal{W}+$ space is significantly more expressive than in the restrictive $\mathcal{W}$ generator input space. 
The latter idea has been strengthened and better adapted for face editing with the appearance of pSp~\cite{richardson2021encoding} and e4e~\cite{tov2021designing}, as well as of their cascaded variant ReStyle~\cite{alaluf2021restyle} and other works~\cite{abdal2020image2stylegan++,zhu2020domain,tewari2020pie}.
Similarly to PTI but in an encoder-based setting, HyperStyle~\cite{alaluf2022hyperstyle} and HyperInverter~\cite{dinh2022hyperinverter} predict offsets to the StyleGAN generator weights in a lightweight manner in order to represent the target picture in a broader space of parameters.

\noindent \textbf{Inversion of 3D GAN.} 
Unlike the 2D case, the inversion of a 3D GAN is a significantly more advanced problem due to the arising ambiguity: the latent code must be both compliant with the target image and correspond to its plausible 3D representation. 
While PTI remains a universal method that solves this problem for an arbitrary generator, recent art demonstrates that the quality rapidly declines when the PTI inversion result is rendered from a novel view.
The suggested ways of resolving this fidelity-consistency trade-off for an arbitrary 3D GAN include incorporating multi-view consistency or geometry regularizers~\cite{li20223d,xie2023high}, augmenting training with surrogate mirrored images~\cite{yin20223d}, introducing local features~\cite{lan2023self}, or optimizing camera parameters and latent code simultaneously~\cite{ko20233d}. 
All of these approaches are still optimization-based and require at least a few minutes of inference time per image.
A concurrent encoder-based work Live 3D Portrait~\cite{Trevithick2023RealTimeRF} also leverages the tri-plane representation for high-fidelity reconstruction while relying on a self-constructed generator instead of EG3D and skipping the latent code prediction part.
Our work solves the inversion for the pretrained, frozen EG3D generator and addresses face manipulation due to the use of the latent space. 
Additionally, in~\cite{Trevithick2023RealTimeRF}, the training pipeline is reversed compared to ours. Starting from the random latent code, they generate synthetic images from EG3D to train the encoder. In contrast, we pass real and synthetic images through the encoder to generate latent codes.     
Another work, EG3D-GOAE~\cite{yuan2023make}, concurrent to ours, modifies the internal features of EG3D instead of tri-planes directly.


\begin{table*}[t]
\setlength{\tabcolsep}{0pt}
\renewcommand{\arraystretch}{0}
\begin{tabular}{c@{\hskip 0.3cm}ccccccc}
    \raisebox{2.5\height}{\rotatebox[origin=c]{90}{Input}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/original/3207.jpg}  \\[0.3cm]
\raisebox{1.2\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ opt. \cite{karras2020analyzing}}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/w_plus/3207.jpg}  \\
\raisebox{1.7\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/PTI/3207.jpg}  \\
\raisebox{1.7\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/pSp/3207.jpg}  \\
\raisebox{2.5\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/142.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/381.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/394.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/451.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/622.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/1030.jpg} & 
    \includegraphics[width=0.138\textwidth]{images/4_experiments/inversion/triplanenet/3207.jpg} 
\end{tabular}
\captionof{figure}{Qualitative comparison for image reconstruction. Compared to other approaches, our method can reconstruct a face in the same view in more detail, especially introducing more detail for features such as hats, hair, and background.}
\label{visualcomp}
\vspace{-0.3cm}
\end{table*}