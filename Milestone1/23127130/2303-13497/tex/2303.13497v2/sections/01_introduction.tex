\section{Introduction}\label{introduction}
In recent years, numerous works~\cite{chan2021pi,chan2022efficient} have tackled the problem of multi-view consistent image synthesis with 3D-aware GANs. 
Such methods make generators aware of a 3D structure by modeling it with explicit voxel grids~\cite{henzler2019escaping, sitzmann2019deepvoxels, nguyen2020blockgan} or neural implicit representations~\cite{chan2021pi,or2022stylesdf}. 
Most notably, EG3D~\cite{chan2022efficient} introduced a 3D GAN framework based on a tri-plane 3D representation that is both efficient and expressive to enable high-resolution 3D-aware image synthesis. Moreover, they demonstrate state-of-the-art results for unconditional geometry-aware image synthesis.

\begin{table}[t!]
\centering
\setlength\tabcolsep{0pt}
\begin{tabular}{cc@{\hskip 0.1cm}ccc}
    \multirow{4}{*}[-5em]{
    \begin{subfigure}[h]{0.11\textwidth}
        \includegraphics[width=\textwidth]{images/1_introduction/1086/original/1086.jpg}
        \caption*{Input}
    \end{subfigure}
    \hspace{0.1cm}
    } & 
    \raisebox{1\height}{\rotatebox[origin=c]{90}{$\mathcal{W}+$ \cite{karras2020analyzing}} }
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/w_plus/1086.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/w_plus/1086_-0.3.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/w_plus/1086_0.3.jpg} \\
& 
    \raisebox{1.2\height}{\rotatebox[origin=c]{90}{PTI~\cite{roich2022pivotal}}} 
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/PTI/1086.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/PTI/1086_-0.3.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/PTI/1086_0.3.jpg} \\
& 
    \raisebox{1.3\height}{\rotatebox[origin=c]{90}{SPI~\cite{yin20223d}}} 
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/SPI/1086_same-view.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/SPI/1086_-0.3.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/SPI/1086_0.3.jpg} \\
&
    \raisebox{1.2\height}{\rotatebox[origin=c]{90}{pSp~\cite{richardson2021encoding}}} 
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/pSp/1086.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/pSp/1086_-0.3.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/pSp/1086_0.3.jpg} \\
&
    \raisebox{1.8\height}{\rotatebox[origin=c]{90}{\textbf{Ours}}} 
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/triplanenet/1086.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/triplanenet/1086_-0.3.jpg}
    & \includegraphics[width=0.105\textwidth]{images/1_introduction/1086/triplanenet/1086_0.3.jpg} \\
\end{tabular}
\vspace{-0.1cm}
\captionof{figure}{
For a given picture, our method predicts the appropriate latent code and the tri-plane offsets for the EG3D generator in a feed-forward manner. This way, both the frontal view and the novel view rendering can be obtained with high fidelity and in close to real time.
\label{fig:introduction}
}
\vspace{-0.4cm}
\end{table}
\begin{figure*}[t!]
    \centering
    \includegraphics[trim={0 2.5cm 0 0},width=\textwidth]{images/3_method/method.pdf}
    \caption{Overview of the proposed method. TriPlaneNet consists of two branches. The first branch (above) comprises the predictor $\hat{w} = \boldsymbol{\phi}(x)$ of the pivotal latent code $\hat{w} \in \mathcal{W}+$, which results in an input-view RGB image $\hat{y} = \mathcal{R}(\boldsymbol{G}(\hat{w}), \pi)$ and a mirror-view RGB image $\hat{y}_m = \mathcal{R}(\boldsymbol{G}(\hat{w}), \pi_m)$ with corresponding camera matrices $\pi$ and $\pi_m$ respectively after passing it through the EG3D tri-plane generator $\boldsymbol{G}(\cdot)$ and renderer block $\mathcal{R}(\cdot, \cdot)$ containing super-resolution module. The second branch (below) uses the first-stage approximation $\hat{y}$ its difference with the target $(x - \hat{y})$, difference between mirror image and rendered mirror view image $(x_m - \hat{y}_m)$ and the tri-planes features $\boldsymbol{G}(\hat{w})$ to predict the numerical offsets to the tri-planes $\Delta \boldsymbol{T}$ by a convolutional autoencoder $\Delta \boldsymbol{T} = \boldsymbol{\psi}(\hat{y}, x - \hat{y})$, which yields the final prediction $y = \mathcal{R}(\boldsymbol{G}(\hat{w}) + \Delta \boldsymbol{T}, \pi)$.}
    \label{method:fig}
\end{figure*}

The main applications of 3D GANs include human face inversion, including head tracking, reenactment, facial manipulation, and novel view synthesis of a given image or video. 
Oftentimes, the classical GAN formulation does not support trivial inversion, i.e.~finding the appropriate code in the learned GAN space for a given sample. A straightforward way to achieve this is by obtaining the latent code of the input image via optimization-based or encoder-based approaches, i.e.~applying 2D GAN inversion techniques. An existing branch of research studies 2D GAN inversion in high detail~\cite{abdal2019image2stylegan,richardson2021encoding,alaluf2021restyle,abdal2020image2stylegan++,zhu2020domain,tewari2020pie}, but nevertheless, the problem remains underexplored in 3D.

Optimization-based inversion methods are often superior to encoder-based approaches in terms of reconstruction quality. However, encoder-based techniques are orders of magnitude faster as they map a given image to the latent space of GAN in a single forward pass. 
Compared to 2D GAN inversion, 3D GAN inversion is a more challenging task as the inversion needs to both preserve the identity of an input image and plausibly embed the head in 3D space. 
In particular, optimization-based 2D GAN inversion methods that have no knowledge of the specific GAN architecture make sure to yield a high-quality rendering of the desired image from the same camera view, but the lack of any geometry information in the image may produce broken or stretched geometry when rendered from a novel camera. Optimization-based 3D GAN inversion techniques improve these shortcomings by adding 3D constraints in the optimization process. Even though these techniques prevent geometry collapse and offer high-fidelity reconstruction, they are slow and time-consuming. We improve the above-mentioned shortcomings in two separate ways. First, by predicting an input latent code for the EG3D generator with a convolutional encoder, we observe that the geometry is preserved better than by optimizing it. This can be attributed to the fact that the encoder, trained for the inversion task, is exposed to thousands of images under different poses and, in this way, learns to be 3D-aware. 
Second, we utilize the knowledge about the model and improve the details and consistency by predicting offsets to the tri-planes that constitute the 3D representation in EG3D. Unlike voxel grids or implicit representations, tri-planes can be naturally estimated by 2D convnets and, as demonstrated by our experiments, can realistically express object features beyond the capabilities of an input latent code, e.g., hands and long hair (see Fig.~\ref{fig:introduction}). This advantage is attained by recovering the object representation directly in the world space. Since the tri-plane offsets are fully predicted by convolutional layers, our inversion can run in close to real time on modern GPUs.

We propose the EG3D-specific inversion scheme in two stages. In the first stage, the initial inversion is obtained using the latent encoder that directly embeds the input image into the $\mathcal{W}+$ space of EG3D. In the second stage, we introduce another encoder, TriPlaneNet, that learns to refine the initial reconstruction. 
Conditioned on the input image and corresponding tri-plane features, it predicts a numerical offset for them. 
The system is trained with a combination of perceptual and photometric losses. In addition, we make use of the soft constraint based on the mirror image -- \emph{probably symmetric prior} inspired by~\cite{Wu2019UnsupervisedLO} -- that makes the encoder even more 3D-aware.

To summarize, our contributions are the following:
\begin{itemize}
    \item We propose a novel and fast inversion framework for EG3D that enables high-quality reconstruction and plausible geometric embedding of a head in 3D space by directly utilizing the tri-plane representation and a soft symmetry constraint.
    \item We demonstrate that our method achieves on-par reconstruction quality compared to optimization-based inversion methods and is an order of magnitude time faster. Our method is also more resilient towards harder cases, such as when a hat or accessories are featured.
\end{itemize}