\section{Experiments}


\begin{table}[b!]
{\footnotesize
    \vspace{-0.3cm}
    \centering
    \setlength\tabcolsep{0pt}
        \begin{tabular}{cccccc}
            &
            Input
            &
            w/o $\mathcal{L}_m$
            &
            No $2^{nd}$ branch
            &
            $\mathcal{L}_m=0.5$
            &
            Ours
            \\
            \raisebox{1.0\height}{\rotatebox[origin=c]{90}{Same View}}\,\,
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/original/942_original.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/wo_mirror_loss/942.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/no_second_branch/942.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/wo_mirror_0.5_lambda/942.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/triplanenet/942.jpg}
            \\
            &
            \multicolumn{1}{r}{\raisebox{3.5\height}{\small Novel View}}\,\,
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/wo_mirror_loss/942_0.3.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/no_second_branch/942_0.3.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/wo_mirror_0.5_lambda/942_0.3.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/triplanenet/942_0.3.jpg}
            \\
            &
            \multicolumn{1}{r}{\raisebox{3.5\height}{\small Geometry}}\,\,
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/wo_mirror_loss/942_wo_mirror_loss_geometry.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/no_second_branch/942_no_second_branch_geometry.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/wo_mirror_0.5_lambda/942_mirror_loss_0.5_geometry.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/ablation/triplanenet/942_triplanenet_geometry.jpg}
            \\
        \end{tabular}%
}
\captionof{figure}{
        Qualitative ablation study for the loss, dataset, and architecture changes. \textit{Electronic zoom-in recommended.}
    }
\label{fig:ablation}
\vspace{-0.3cm}
\end{table}


\subsection{Training procedure}
\noindent \textbf{Datasets.} Since our focus is on the human facial domain, we use FFHQ \cite{Karras2018ASG} dataset and 100K generated images from EG3D pre-trained on FFHQ for training and perform the evaluation on the CelebA-HQ ~\cite{Liu2014DeepLF, Karras2017ProgressiveGO} test set. We extract the camera pose and pre-process the FFHQ and synthetic data in the same way as in \cite{chan2022efficient}. Since the pre-processing technique could not identify the camera poses of 4 images, we skipped the quantitative evaluation of 4 images for all the methods presented in the paper. We also augment the training dataset by mirroring it.  

\begin{table*}[h!]
\setlength{\tabcolsep}{0pt}
{\footnotesize
\begin{tabular}{clllcllll}
                     & \multicolumn{3}{c}{\textcolor{RawSienna}{$\mathcal{W}+$ opt.} + \dots} & \multicolumn{5}{c}{\textcolor{blue}{$\mathcal{W}+$ pred.} + \dots}                              \\
\vspace{0.5cm}       & \multicolumn{3}{c}{\multirow{8}{*}{\includegraphics[trim={6.5cm 5.5cm 6.5cm 0.2cm},clip,height=10cm]{images/4_experiments/analysis/analysis_left.pdf}}}                          & \multicolumn{5}{c}{\multirow{8}{*}{\includegraphics[trim={3.5cm 5.5cm 3cm 0.2cm},clip,height=10cm]{images/4_experiments/analysis/analysis_right.pdf}}}                                                        \\
\includegraphics[width=0.1\textwidth]{images/4_experiments/analysis/Original/16989.jpg}
                    & \multicolumn{3}{c}{}                                           & \multicolumn{5}{c}{}                                                                         \\
Input                & \multicolumn{3}{c}{}                                           & \multicolumn{5}{c}{}                                                                         \\
                     & \multicolumn{3}{c}{}                                           & \multicolumn{5}{c}{}                                                                         \\
\vspace{1.5cm}       & \multicolumn{3}{c}{}                                           & \multicolumn{5}{c}{}                                                                         \\
\includegraphics[width=0.1\textwidth]{images/4_experiments/analysis/Original/28640.jpg}
                     & \multicolumn{3}{c}{}                                           & \multicolumn{5}{c}{}                                                                         \\
Input                & \multicolumn{3}{c}{}                                           & \multicolumn{5}{c}{}                                                                         \\
\vspace{1.8cm}       & \multicolumn{3}{c}{}                                           & \multicolumn{5}{c}{}                                                                         \\
                     &  \hspace{2.2cm}                      & {\footnotesize + \textcolor{RawSienna}{EG3D params}\,\,\,}   & {\footnotesize \textcolor{blue}{+ tri-plane}}   & \hspace{0.3cm}{\footnotesize (pSp)} & {\footnotesize \textcolor{RawSienna}{+ EG3D}\,\,\,\, \hspace{0.6cm}} & {\footnotesize \textcolor{RawSienna}{\,\,\,\,+ tri-plane}\,\,\,\,\,\,\,\,} & {\footnotesize + \textcolor{blue}{tri-plane pred.}} & \,\,{\footnotesize + \textcolor{blue}{tri-plane pred.}}
                     \\
\multicolumn{1}{l}{} & \multicolumn{1}{l}{}   & {\footnotesize \textcolor{RawSienna}{opt. (PTI)}}      &  {\footnotesize \textcolor{blue}{pred.}}                   & \hspace{2.1cm}                       & {\footnotesize \textcolor{RawSienna}{params opt.}}          & {\,\,\,\,\footnotesize \textcolor{RawSienna}{opt.}}        &  & \,\,{\footnotesize + symm. prior}
\vspace{-0.2cm}
\end{tabular}
}
\captionof{figure}{Comparison of hybrid approaches on CelebA-HQ test dataset. We refer to the computation done via optimization as \textcolor{RawSienna}{opt.} and via an encoder as \textcolor{blue}{pred.} We observe that the methods starting from \textcolor{RawSienna}{$\mathcal{W}+$ opt.} yield elongated head geometry, whereas subsequent \textcolor{blue}{tri-plane pred.} can partially alleviate it. Experiments starting from \textcolor{blue}{$\mathcal{W}+$ pred.} demonstrate that the tri-plane space is more spatially restrictive than the EG3D parameters space. \textit{Ours} = \textcolor{blue}{$\mathcal{W}+$ pred.} + \textcolor{blue}{tri-plane pred.} + symmetry prior. \textit{Electronic zoom-in recommended.}}
\vspace{-0.3cm}
\label{analysis:fig}
\end{table*}

\noindent \textbf{Training details.}
Our pre-trained EG3D generator is also trained on the FFHQ dataset \cite{Karras2018ASG}. We train two versions of the same model: \textit{Ours}, trained on FFHQ and synthetic data, and $\textit{Ours (FFHQ)}$, trained only on FFHQ data. We discuss the motivation to use synthetic samples in Sec.~\ref{results} and describe the training procedure details in Appendix~\ref{supp:impdetails}. 


\noindent \textbf{Baselines.} We compare our approach with both optimization- and encoder-based inversion methods. Among optimization-based methods, we compare to universal $\mathcal{W}+$ optimization~\cite{karras2020analyzing} and PTI~\cite{roich2022pivotal}, as well as to Pose Opt.~ \cite{ko20233d} and SPI~\cite{yin20223d}, recently introduced for 3D GANs. Among encoder-based methods, we compare to e4e~\cite{tov2021designing}, pSp~\cite{richardson2021encoding} and EG3D-GOAE~\cite{yuan2023make}. 
For $\mathcal{W}+$ optimization, we optimize the latent code for 1K steps. For PTI, we first optimize the latent code $\hat{w} \in \mathcal{W}+$ for 1K steps and then fine-tune the generator for 1K steps. For Pose Opt. and SPI, we re-run their official implementation. For pSp, we employ the original training configuration from \cite{richardson2021encoding} with a batch size of 3. We train the pSp encoder on both FFHQ and synthetic data, similarly to our method. For EG3D-GOAE, we take the released checkpoint and run the inference on our dataset. 



\begin{table}[h!]
    {\footnotesize
    \vspace{0cm}
    \centering
    \setlength\tabcolsep{0pt}
        \begin{tabular}{ccccc}
            Input
            &
            PTI
            &
            SPI
            &
            \textbf{Ours}
            &
            SfM
            \\
            
            &
            (119.34 s)
            &
            (258.84 s)
            &
            \textbf{(0.12 s)}
            &
            reconstruction
            \\
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/original/IMG_8340_original.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/PTI/PTI.ply_3.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/SPI/SPI.ply_3.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/triplanenet/triplanenet.ply_3.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/SfM/IMG_8340.jpg}
            \\
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/original/IMG_8342_original.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/PTI/PTI.ply_9.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/SPI/SPI.ply_9.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/triplanenet/triplanenet.ply_9.jpg}
            &
            \includegraphics[width=0.09\textwidth]{images/4_experiments/geometry/SfM/IMG_8342.jpg}
            \\
        \end{tabular}%
    }
\captionof{figure}{
Comparison of the estimated 3D geometry w.r.t. the "ground-truth" reconstruction by Structure-from-Motion (SfM). Our method estimates the view-consistent embedding of a head in 3D from a single image. \textit{Electronic zoom-in recommended.}
}
\label{fig:ablation-geometry}
\vspace{-0.3cm}
\end{table}

\subsection{Results}\label{results}

\noindent \textbf{Comparison to the state-of-the-art.} We present the evaluation of our approach w.r.t.~the baselines in Fig.~\ref{visualcomp} and Table~\ref{quant-same-view-table}. 
Commonly used metrics MSE, LPIPS \cite{Zhang2018TheUE}, MS-SSIM \cite{ms-ssim}, and ID similarity \cite{Huang2020CurricularFaceAC} (measured by the pre-trained face recognition network not used in training) have been selected to analyze various aspects of perceptual similarity between inputs and corresponding reconstructions. To assess the quality of 3D geometry, we measure Depth MSE following a similar protocol from~\cite{Shi2020Lifting2S}. 
Among optimization-based techniques, SPI achieves the best results for same-view reconstruction. We attribute this to the almost three times longer optimization time than PTI and Pose Opt. 
However, our method outperforms all the baselines according to the Depth metric while being an order of magnitude times faster than optimization-based approaches. 
Effectively, this means that the head shape is closer to the one estimated by a parametric face prior model than for the other methods.

Furthermore, we demonstrate the identity preservation quality of input image re-rendering from a novel view in Table~\ref{novel-view-table} and in Fig.~\ref{novelview}.
We outperform all the baselines on extreme novel view yaw angles, and our method embeds the head in 3D space in a much more plausible way while preserving fine details and not relying on any explicit face or head priors.
For all methods in general, the ID score declines faster above a certain value of the yaw angle due to the uneven angle distribution in the FFHQ dataset.




\begin{table*}[h!]
    \begin{adjustbox}{minipage=0.63\linewidth,scale=1.0}
    {\fontsize{8pt}{0.35cm}
    \selectfont
    \begin{center}
        \caption{Quantitative ablation study for the loss, dataset, and architecture changes.}
        \vspace{-0.2cm}
        \begin{tabular}{l|c c c c |c| c c c | c c c}
            \multirow{3}{*}{Method} & \multirow{3}{*}{\,\,MSE $\downarrow$\,\,} & \multirow{3}{*}{LPIPS $\downarrow$} & \multirow{3}{*}{MS-SSIM $\uparrow$} &  \multirow{3}{*}{Depth $\downarrow$} & \multicolumn{6}{c}{ID $\uparrow$}\\
            \cline{6-12}
            & & & & & \,Same\, & \multicolumn{6}{c}{Novel View (Yaw angle in radians)} \\
            \cline{7-12}
            & & & & & View &\,\,-0.8\,\, & \,\,-0.6\,\, & \,\,-0.3\,\, & \,\,0.3\,\, & \,\,0.6\,\, & \,\,0.8\,\, \\ %
            \hline \hline
            Ours & 0.019 & \textbf{0.08} &  0.87 & 0.051 & 0.68 & 0.39 & 0.47 & 0.58 & 0.59 & 0.48 & 0.40 \\ %
            \dots\, \textit{(FFHQ)} & 0.022 & 0.09 & 0.86 & \textbf{0.044} & \textbf{0.70} & \textbf{0.41} & \textbf{0.48} & \textbf{0.60} & \textbf{0.60} & \textbf{0.50} & \textbf{0.42} \\ %
            \dots\, w/o $\mathcal{L}_m$ & \textbf{0.017} & \textbf{0.08} & \textbf{0.88} & 0.082 & 0.69 & 0.39 & 0.47 & 0.59 & 0.59 & 0.48 & 0.40 \\ %
            \dots\, $\mathcal{L}_m = .005$ & 0.018 & \textbf{0.08} & 0.87 & 0.068 & 0.69 & 0.38 & 0.46 & 0.59 & 0.60 &  0.48 & 0.40 \\ %
            \dots\, $\mathcal{L}_m = 0.5$ & 0.028 & 0.11 & 0.83 & 0.049 & 0.64 & 0.37 & 0.44 & 0.55 & 0.56 & 0.45 & 0.37 \\ %
            No $2^{nd}$ branch &  0.047 &  0.18 &  0.73 & 0.056 &  0.41 &  0.25 & 0.30 & 0.36 & 0.37 &  0.31 & 0.26 \\ %
            \dots\, \textit{(FFHQ)} & 0.047 & 0.18 & 0.74 & 0.051 & 0.44 & 0.28 & 0.33 & 0.39 & 0.40 & 0.34 & 0.29 \\ %
            \dots\, w/o $\mathcal{L}_m$ & 0.045 & 0.18 & 0.73 & 0.076 & 0.40 & 0.24 & 0.28 & 0.35 & 0.36 & 0.29 & 0.25 \\ %
            \dots\, $\mathcal{L}_m = .005$ & 0.045 & 0.18 & 0.73 & 0.068 & 0.39 & 0.24 & 0.28 & 0.34 & 0.36 & 0.30 & 0.25 \\ %
            \dots\, $\mathcal{L}_m = 0.5$ & 0.057 & 0.20 & 0.70 & 0.053 & 0.41 & 0.25 & 0.29 & 0.37 & 0.37 & 0.30 & 0.25 \\ %
            \hline
        \end{tabular}
        \label{ablation-table}
    \end{center}
    }
    \end{adjustbox}
    \hfill
    \hskip 0.5cm
    \begin{adjustbox}{minipage=0.30\linewidth,scale=1.0}
        \setlength{\tabcolsep}{0pt}
        \renewcommand{\arraystretch}{0}
        \begin{tabularx}{\columnwidth}{cccc}
            & {\footnotesize Input}   & {\footnotesize Ours (FFHQ)}  & {\footnotesize Ours} \\
            \raisebox{1.5\height}{\rotatebox[origin=c]{90}{\footnotesize Original}}\, & \includegraphics[height=1.7cm]{images/4_experiments/ablation-dataset/original/235_aligned.jpg} &
            \includegraphics[height=1.7cm]{images/4_experiments/ablation-dataset/wo_eg3d_samples/235_aligned.jpg} &
            \includegraphics[height=1.7cm]{images/4_experiments/ablation-dataset/triplanenet/235_aligned.jpg} \\
            \raisebox{1.2\height}{\rotatebox[origin=c]{90}{\footnotesize Shifted up}}\, & \includegraphics[height=1.7cm]{images/4_experiments/ablation-dataset/original/235_misaligned.jpg} &
            \includegraphics[height=1.7cm]{images/4_experiments/ablation-dataset/wo_eg3d_samples/235_misaligned.jpg} &
            \includegraphics[height=1.7cm]{images/4_experiments/ablation-dataset/triplanenet/235_misaligned.jpg} \\
        \end{tabularx}
        \captionof{figure}{Adding synthetic samples to training (\textit{Ours}) makes the model robust to the input image shifts compared to trained on FFHQ only (\textit{Ours (FFHQ)}).}%
        \label{fig:ablation-dataset}
    \end{adjustbox}
\end{table*}

\noindent \textbf{Ablation study.}
In Fig.~\ref{fig:ablation} and Table~\ref{ablation-table}, we ablate over the possible differences in our model design, such as loss functions %
weights and the presence of the second branch.
As some of those were introduced to handle occluded regions in the input view, we demonstrate visually how the incorporated symmetric prior affects the novel view and 3D geometry quality. 
All models in this ablation except the first branch encoder are trained for 600K steps. We observed that including symmetric prior significantly improves novel-view quality and geometric consistency. Despite the model trained on FFHQ achieving a higher ID score, in Fig.~\ref{fig:ablation-dataset}, we show that adding synthetic EG3D samples in training allows us to correctly canonicalize face for a slightly shifted input, while our model trained with only real images is very sensitive to minor image misalignment. 


\noindent \textbf{Geometry evaluation for a multi-view sequence.} In Fig.~\ref{fig:ablation-geometry}, we present a qualitative evaluation of the geometry obtained using our method against PTI~\cite{roich2022pivotal}, SPI~\cite{yin20223d}, and ground truth geometry obtained using 
a third-party SfM software~\cite{agisoft}. We notice that the estimated geometry is highly view-consistent and is closer to the ground truth.

We present more evaluations and visual results in the Appendix, such as novel view synthesis for a talking head video and face manipulation capabilities.


\subsection{PTI and tri-plane offsets behavior}

Both our method and optimization- and encoder-based baselines can be decomposed into two stages: estimating the latent code and the delta for the generator parameters. In Fig.~\ref{analysis:fig}, we show how combining these steps, each performed either by optimization (\textcolor{RawSienna}{opt.}) or an encoder (\textcolor{blue}{pred.}), influences the inversion behavior. 

\textcolor{RawSienna}{$\mathcal{W}+$ opt.} inverts a single image and cannot account for 3D geometry due to the lack of supervision from other views, which results in incorrectly stretched geometry.
Accordingly, the same happens with PTI = (\textcolor{RawSienna}{$\mathcal{W}+$ opt.} + \textcolor{RawSienna}{EG3D params opt.}) method. Interestingly, tri-plane prediction, applied on top of \textcolor{RawSienna}{$\mathcal{W}+$ opt.}, can alleviate the damage to the geometry caused by \textcolor{RawSienna}{$\mathcal{W}+$ opt}.

\textcolor{blue}{$\mathcal{W}+$ pred.} by a pSp encoder, on the contrary, embeds the head in 3D more plausibly due to the supervision from images under different poses during training. At the same time, the same-view quality is marginally worse than PTI. Applying the PTI's second step (\textcolor{RawSienna}{EG3D params opt.}) helps improve it significantly; however, it incorrectly modifies head proportions, similar to the \textcolor{RawSienna}{$\mathcal{W}+$ opt.} behavior. To investigate this effect further, instead of optimizing EG3D parameters after \textcolor{blue}{$\mathcal{W}+$ pred.}, we try optimizing the tri-plane offsets directly, and this fully cancels the incorrect stretching of geometry while preserving high fidelity in the same view. Since both \textcolor{RawSienna}{EG3D params opt.} and \textcolor{RawSienna}{tri-plane opt.} are performed for a single image (i.e.~without multi-pose supervision during training), this may indicate that offsetting the tri-planes is more spatially restrictive and thus stable. 
Therefore, we base our method on directly leveraging the tri-plane representation. 


We further improve the checkerboard artifacts in novel view, noticeable for \textcolor{RawSienna}{tri-plane opt.}, by \textcolor{blue}{tri-plane prediction}, and improve the embedding in 3D space by a symmetric prior. 