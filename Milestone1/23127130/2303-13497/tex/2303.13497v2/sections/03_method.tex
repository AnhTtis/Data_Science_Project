\section{Method}\label{method}

\begin{table*}[h!]
    \centering
    \setlength{\tabcolsep}{0cm}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lllllllllll}
        \multicolumn{5}{l}{\hskip 4cm Novel Views} &
        \hskip 0.5cm Input
        & 
        \multicolumn{5}{l}{\hskip 2.5cm Novel Views}
        \\
        \raisebox{3.0\height}{$\mathcal{W}+$~\cite{karras2020analyzing}}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/10008_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/10008_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/10008_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/w_plus/10008_0.6.jpg}
        & 
        \multirow{3}{*}{
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/original/10008.jpg}
        }
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/5358_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/5358_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/w_plus/5358_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/w_plus/5358_0.6.jpg}
        &
        \raisebox{3.0\height}{\hskip 0.1cm $\mathcal{W}+$~\cite{karras2020analyzing}}
        \\
        \raisebox{3.0\height}{PTI~\cite{roich2022pivotal}}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/10008_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/10008_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/10008_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/PTI/10008_0.6.jpg}
        &
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/5358_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/5358_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/PTI/5358_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/PTI/5358_0.6.jpg}
        &
        \raisebox{3.0\height}{\hskip 0.1cm PTI~\cite{roich2022pivotal}}
        \\
        \raisebox{3.0\height}{Pose Opt.~\cite{ko20233d}\,}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/poseopt/10008_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/poseopt/10008_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/poseopt/10008_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/poseopt/10008_0.6.jpg}
        &
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/poseopt/5358_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/poseopt/5358_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/poseopt/5358_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/poseopt/5358_0.6.jpg}
        &
        \raisebox{3.0\height}{\hskip 0.1cm Pose Opt.~\cite{ko20233d}}
        \\
        \raisebox{3.0\height}{SPI~\cite{yin20223d}}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/SPI/10008_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/SPI/10008_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/SPI/10008_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/SPI/10008_0.6.jpg}
        & 
        \multirow{3}{*}{
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/original/5358.jpg}
        }
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/SPI/5358_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/SPI/5358_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/SPI/5358_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/SPI/5358_0.6.jpg}
        &
        \raisebox{3.0\height}{\hskip 0.1cm SPI~\cite{yin20223d}}
        \\
        \raisebox{3.0\height}{pSp~\cite{richardson2021encoding}}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/10008_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/10008_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/10008_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/pSp/10008_0.6.jpg}
        &
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/5358_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/5358_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/pSp/5358_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/pSp/5358_0.6.jpg}
        &
        \raisebox{3.0\height}{\hskip 0.1cm pSp~\cite{richardson2021encoding}}
        \\
        \raisebox{3.0\height}{\textbf{Ours}}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/10008_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/10008_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/10008_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/10008_0.6.jpg}
        &
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/5358_-0.6.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/5358_-0.3.jpg}
        &
        \includegraphics[width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/5358_0.6.jpg}
        &
        \includegraphics[trim={2.2cm 2.2cm 2.2cm 2.2cm},clip,width=0.099\textwidth]{images/4_experiments/rotation/triplanenet/5358_0.6.jpg}
        &
        \raisebox{3.0\height}{\hskip 0.1cm \textbf{Ours}}
        \\
    \end{tabular}%
    }
    \vspace{-0.1cm}
    \captionof{figure}{Qualitative evaluation on novel view rendering of yaw angle -0.6, -0.3, and 0.6 radians (full and zoom-in). In comparison to others, our method preserves identity and multi-view consistency better when rendered from a novel view.}
    \label{novelview}
    \vspace{-0.3cm}
\end{table*}

\subsection{Preliminaries}

\noindent \textbf{GAN inversion.}
Given a target image $x$, the goal of GAN inversion is to find a latent code that minimizes the reconstruction loss between the synthesized image and the target image:
\begin{equation}
    \hat{w} = \operatorname*{argmin}_{w}\mathcal{L}(x, G(w; \theta))
    \label{gan-obj}
\end{equation}
where $G(w; \theta)$ is the image generated by a pre-trained generator $G$ parameterized by weights $\theta$, over the latent $w$. 
The problem in (\ref{gan-obj}) can be solved via optimization or encoder-based approaches. Encoder-based approaches utilize an encoder network $E$ to map real images into a latent code. The training of an encoder network is performed over a large set of images $\{x^{i}\}_{i=1}^{N}$ to minimize:
\begin{equation}
    \setlength{\abovedisplayskip}{3pt}
    \setlength{\belowdisplayskip}{3pt}
    \min_E \sum_{i=1}^N\mathcal{L}(x^i, G(E(x^i); \theta))
\end{equation}
During inference, an input image is inverted by $G(E(x); \theta)$. In the recent works~\cite{roich2022pivotal, alaluf2022hyperstyle, dinh2022hyperinverter}, a number of approaches are proposed to additionally estimate image-specific generator parameters $\theta(x)$ by a convolutional network.\newline

\noindent \textbf{EG3D.}
EG3D \cite{chan2022efficient} uses tri-plane 3D representation for geometry-aware image synthesis from 2D images. EG3D image generation pipeline consists of several modules: a StyleGAN2-based feature generator, a tri-plane representation, a lightweight neural decoder, a volume renderer, and a super-resolution module. To synthesize an image, a random latent code $z \in \mathbb{R}^{D}$ (typically, $D=512$) and camera parameters are first mapped to a pivotal latent code $w \in \mathcal{W}+$ using a mapping network. Then, $w$ is fed into the StyleGAN2 CNN generator $\boldsymbol{G}(\cdot)$ to generate a $H \times W \times 96$ feature map. This feature map is reshaped to form three 32-channel planes, thus forming a tri-plane feature representation $\boldsymbol{T}$ of the corresponding object. To sample from the tri-plane features, a position $p \in \mathbb{R}^3$ is first projected onto the three feature planes. Then, corresponding feature vectors ($F_{xy}(p), F_{xz}(p), F_{yz}(p)$) are retrieved using bilinear interpolation and aggregated. These aggregated features are processed by a lightweight neural decoder to transform the feature into the estimated color and density at the location $p$. Volume rendering is then performed to project 3D feature volume into a feature image. Finally, a super-resolution module is utilized to upsample the feature image to the final image size. For simplicity, we will later refer to the lightweight neural decoder, renderer, and the super-resolution block, all combined, as the rendering block $\mathcal{R}(\cdot, \cdot)$. The high efficiency and expressiveness of EG3D, as well as the ability to work with tri-planes directly, motivates the development of our model-specific inversion algorithm.\newline

\noindent \textbf{pSp.}
Richardson \textit{et al}.~\cite{richardson2021encoding} proposed a pSp framework based on an encoder that can directly map real images into $\mathcal{W}+$ latent space of StyleGAN. In pSp, an encoder backbone with a feature pyramid generates three levels of feature maps. The extracted feature maps are processed by a map2style network to extract styles. The styles are then fed into the generator network to synthesize an image $\hat{y}$:

\begin{equation}
\hat{y} = G(E(x) + \bar{w}),
\end{equation}
where $G(\cdot)$ and $E(\cdot)$ denote the generator and encoder networks respectively and $\bar{w}$ is the average style vector of the pretrained generator.

\subsection{TriPlaneNet}
Our TriPlaneNet inversion framework comprises two branches
(see Fig.~\ref{method:fig} for the overview).
The first branch employs a latent encoder following a design of pSp to embed an input image into $\mathcal{W}+$ space of EG3D. Specifically, given an input image $x$, we train a encoder $\phi$ to predict the pivotal latent $\hat{w} \in \mathcal{W}+$:
\begin{equation}
    \hat{w} = \phi(x) + \bar{w}
\end{equation}
where the dimension of $\hat{w}$ is $K \times D$ (for the output image resolution of 128, $K=14$, and $D=512$). The pivotal code is then fed into StyleGAN2 generator $\boldsymbol{G}(\cdot)$ in the EG3D pipeline to obtain initial tri-plane features $\boldsymbol{T}$. Then, the tri-plane representation is processed by the rendering block $\mathcal{R}(\cdot, \cdot)$ to generate initial reconstruction $\hat{y}$:
\begin{equation}
    \setlength{\abovedisplayskip}{5pt}
    \setlength{\belowdisplayskip}{5pt}
    \hat{y} = \mathcal{R}(\boldsymbol{G}(\hat{w}), \pi)
\end{equation}
where $\pi$ is the input-view camera matrix.

The second branch consists of a convolutional auto-encoder $\psi$ that learns to predict numerical offsets to the initial tri-plane features. The input to the encoder module of the autoencoder network is the channel-wise concatenation of initial reconstruction $\hat{y}$, the difference between an input image and initial input-view reconstruction ($x - \hat{y}$), and the difference between a mirrored input image and initial mirror-view reconstruction ($x_m - \hat{y}_m$). The decoder takes input from the encoder and first branch tri-plane features. Given these inputs, the autoencoder is tasked with computing tri-plane offsets $\Delta \boldsymbol{T}$ with respect to tri-plane features obtained in the first branch:
\begin{equation}
    \Delta \boldsymbol{T} = \boldsymbol{\psi}(\hat{y},\, x - \hat{y},\, x_m - \hat{y}_m,\, G(\hat{w}))
\end{equation}
The new tri-plane features corresponding to the inversion of the input image $x$ are then computed as an element-wise addition of tri-plane offsets $\Delta \boldsymbol{T}$ with initial tri-plane features $\boldsymbol{T} = G(\hat{w})$. This new tri-plane representation is similarly processed by the rendering block $\mathcal{R}(\cdot, \cdot)$ to obtain the final reconstructed image:
\begin{equation}
    y = \mathcal{R}(\boldsymbol{T} + \Delta \boldsymbol{T}, \pi)
\end{equation}

The autoencoder follows the typical U-Net \cite{Ronneberger2015UNetCN} architecture, consisting of a contracting path and an expansive path.
The decoder architecture is similar to that of RUNet \cite{Hu2019RUNet} with some minor modifications.
A detailed view of the architecture is presented in Appendix~\ref{supp:impdetails}.  

\subsection{Loss Functions}
The pipeline is trained by minimizing the loss function that decomposes into the separate loss expressions for two branches:
\begin{equation}
    \mathcal{L}_{\phi, \psi}(x, y, \hat{y}, y_m, \hat{y}_m) = \mathcal{L}_\phi(x, \hat{y}, \hat{y}_m) + \mathcal{L}_\psi(x, y, y_m)
\end{equation}

For training the encoder $\phi(\cdot)$ in the first branch, we employ pixel-wise $\mathcal{L}_2$ loss, LPIPS loss \cite{Zhang2018TheUE}, and ID loss \cite{Deng2018ArcFaceAA}. Therefore, the total loss formulation is given by
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{\phi}(x, \hat{y}, \hat{y}_m) &= \lambda_1\mathcal{L}_2(x, \hat{y}) + \lambda_2\mathcal{L}_\textrm{LPIPS}(x, \hat{y}) 
        \\ & + \lambda_3\mathcal{L}_\textrm{id}(x, \hat{y}) + \lambda_m\mathcal{L}_{m}(x_m, \hat{y}_m)
    \label{encoder-loss}
    \end{aligned}
\end{equation}
where $x_m = \textrm{flip}(x)$, and $\mathcal{L}_{m}(x_m, \hat{y}_m)$ is a \textit{probably symmetric prior} defined as
\begin{equation}
    \vspace{-0.1cm}
    \begin{multlined}
        \mathcal{L}_{m}(x_m, \hat{y}_m) = \lambda_4\mathcal{L}_\textrm{symm}(x_m, \hat{y}_m, \sigma(x_m)) \\ + \lambda_5\mathcal{L}_\textrm{LPIPS}(x_m, \hat{y}_m) + \lambda_6\mathcal{L}_\textrm{id}(x_m, \hat{y}_m),
    \end{multlined}
    \label{encoder-loss1}
\end{equation}
where $\mathcal{L}_\textrm{symm}$ is a symmetric term inspired by~\cite{Wu2019UnsupervisedLO}. Since human faces are not perfectly symmetric, the symmetric term is based on a per-pixel Gaussian density with the pixel-wise uncertainty map $\sigma(x_m)$ that assigns lower confidence to the region in the mirrored image where the symmetry assumption fails.%

\setlength{\tabcolsep}{1.5pt}
\begin{table}[b!]
\caption{Quantitative comparison on save-view inversion. The inference time, including the EG3D pass, is given for a single RTX A100 Ti GPU. \textit{Ours} exceeds other encoder-based methods by photometric scores and embeds the head in 3D space significantly better than all the other methods (see \textit{Depth $\downarrow$}).}
\vspace{-0.4cm}
\begin{center}
\resizebox{0.49\textwidth}{!}{
    \begin{tabular}{ l l | c c c c | c | c}
        \hline
        \multicolumn{2}{l|}{Method} & MSE$\downarrow$ & LPIPS$\downarrow$ & ID $\uparrow$ & MS-SSIM$\uparrow$ & \,Depth$\downarrow$\, & Infer. time $\downarrow$\\
        \hline \hline
        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\textcolor{RawSienna}{Optim.}}}}\,\,\ldelim\{{3.8}{*} &  $\mathcal{W}+$ \cite{karras2020analyzing} & \,0.071 & 0.17 & 0.55 & 0.80 & 0.086 & \textbf{77.07} s\\  
        & PTI \cite{roich2022pivotal} & \,0.013 & 0.07 & 0.76 & 0.89 & 0.087 & 119.34 s\\
        & P. Opt.~\cite{ko20233d} & \,0.014 & 0.08 & 0.67 & 0.88 & 0.119 & 110.86 s\\
        & SPI~\cite{yin20223d} & \,\underline{\textbf{0.005}} & \underline{\textbf{0.05}} & \underline{\textbf{0.94}} & \underline{\textbf{0.95}} & \textbf{0.078} & 258.84 s\\
        \hline
         \parbox[t]{2mm}{\multirow{4.5}{*}{\rotatebox[origin=c]{90}{\textcolor{blue}{Encoder}}}}\,\,\ldelim\{{4.7}{*}
         & e4e \cite{tov2021designing} & \,0.060 & 0.21 & 0.33 & 0.70 & 0.061 & \underline{\textbf{0.04 s}}\\
         & pSp \cite{richardson2021encoding} & \,0.045 & 0.18 & 0.40 & 0.73 & 0.076 & \underline{\textbf{0.04 s}}\\
        & EG3D-GOAE \cite{yuan2023make} & \,0.026 & 0.11 & 0.67 & 0.84 & 0.053 & 0.18 s \\
        & Ours \textit{(FFHQ)} & 0.016 & 0.07 & \textbf{0.78} & 0.89 & \underline{\textbf{0.042}} & 0.12 s\\
        & Ours & \textbf{0.015} & \textbf{0.06} & 0.77 & \textbf{0.90} & 0.047 & 0.12 s\\
        \hline
    \end{tabular}
}
\end{center}
\label{quant-same-view-table}
\vspace{-0.7cm}
\end{table}

The loss for the second branch $\mathcal{L}_\psi$ is constructed the same way as $\mathcal{L}_\phi$ by replacing $\mathcal{L}_2$ with $\mathcal{L}_1$ smooth loss in (\ref{encoder-loss}), inside $\mathcal{L}_{symm}$ in (\ref{encoder-loss1}) and first branch output $\hat{y}$ with the second branch output $y$. %
Appendix~\ref{supp:impdetails} contains more details about the loss functions.



\begin{table}[t!]
\caption{Quantitative comparison on novel view rendering of the inverted representation. We outperform all the other baselines on extreme novel view yaw angles. MSE and others are not suitable here due to spatial misalignment.}
\vspace{-0.5cm}
\begin{center}
\resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l l|c c c c c c|c}
        \hline
        \multicolumn{2}{l|}{\multirow{2}{*}{Method}} & %
        \multicolumn{6}{c|}{ID $\uparrow$ for novel yaw angle (rad):} & \,ID $\uparrow$\,\\
        \cline{3-9}
& & \,\,\,\,-0.8\,\,\,\, & \,\,\,\,-0.6\,\,\,\, & \,\,\,\,-0.3\,\,\,\, & \,\,\,\,0.3\,\,\,\, & \,\,\,\,0.6\,\,\,\, & \,\,\,\,0.8\,\,\,\,\, & Avg \\
        \hline \hline
        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\textcolor{RawSienna}{Optim.}}}}\,\,\ldelim\{{3.8}{*} & $\mathcal{W}+$ \cite{karras2020analyzing} & 0.26 & 0.32 & 0.42 & 0.42 & 0.33 & 0.28 & 0.33\\ 
        & PTI \cite{roich2022pivotal} & 0.34 & 0.41 & 0.55 & 0.56 & 0.43 & 0.35 & 0.44\\
        & P. Opt.~\cite{ko20233d} & 0.30 & 0.36 & 0.50 & 0.50 & 0.38 & 0.31 & 0.39\\
        & SPI~\cite{yin20223d} & \textbf{0.43} & \textbf{0.52} & \underline{\textbf{0.70}} & \underline{\textbf{0.71}} & \textbf{0.54} & \textbf{0.44} & \underline{\textbf{0.55}}\\
        \hline
        \parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{\textcolor{blue}{Encoder}}}}\,\,\ldelim\{{4.7}{*}
        & e4e \cite{tov2021designing} & 0.19 & 0.23 & 0.28 & 0.28 & 0.24 & 0.21 & 0.23\\
        & pSp \cite{richardson2021encoding} & 0.24 & 0.28 & 0.35 & 0.36 & 0.29 & 0.25 & 0.29\\
        & EG3D-GOAE \cite{yuan2023make} & 0.38 & 0.46 & 0.57 & 0.57 & 0.48 & 0.40 & 0.47\\
        & Ours \textit{(FFHQ)} & \underline{\textbf{0.45}} & \underline{\textbf{0.54}} & \textbf{0.66} & \textbf{0.67} & \underline{\textbf{0.56}} & \underline{\textbf{0.47}} & \underline{\textbf{0.55}}\\
        & Ours & 0.44 & 0.53 & \textbf{0.66} & \textbf{0.67} & 0.55 & 0.46 & \underline{\textbf{0.55}}\\
        \hline
    \end{tabular}
}
\end{center}
\label{novel-view-table}
\vspace{-0.7cm}
\end{table}