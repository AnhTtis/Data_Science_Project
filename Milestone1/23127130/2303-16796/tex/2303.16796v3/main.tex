\documentclass{article}
\usepackage[utf8]{inputenc}
%\documentclass[a4paper, 8pt]{article}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amssymb, graphicx, amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}
%\usepackage{overpic}
%\usepackage{subfigure}
%\usepackage{subcaption}
%\usepackage{caption}
\addtolength{\hoffset}{-1cm}
\addtolength{\voffset}{-2cm}
\addtolength{\textwidth}{2cm}
\addtolength{\textheight}{3cm}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

%\title{Module-based regularization improves the graphical lasso when observing noisy data}
\title{Module-based regularization improves Gaussian graphical models when observing noisy data}
% Use "Gaussian graphical models" instead in title?
% Module-based regularization improves Gaussian graphical models when observing noisy data
%\author{Magnus Neuman, Joaquín Calatayud, Viktor Tasselius, Martin Rosvall}
\author{Magnus Neuman$^1$, Joaquín Calatayud$^2$, Viktor Tasselius$^{1,3}$,  Martin Rosvall$^1$}
\date{}

\begin{document}

\maketitle
\begin{small}
    \noindent 1. Integrated Science Lab, Department of Physics, Umeå University, Umeå, Sweden\\
    \noindent 2. Department of Biology, Geology, Physics and Inorganic Chemistry, King Juan Carlos University, Madrid, Spain\\
    \noindent 3. School of Public Health and Community Medicine, University of Gothenburg, Gothenburg, Sweden
    
\end{small}

\section*{Abstract}
Inferring relations from correlational data allows researchers across the sciences to uncover complex connections between variables for insights into the underlying mechanisms. The researchers often represent inferred relations using Gaussian graphical models, requiring regularization to sparsify the models. Acknowledging that the modular structure of the inferred network is often studied, we suggest module-based regularization to balance under- and overfitting. Compared with the graphical lasso, a standard approach using the Gaussian log-likelihood for estimating the regularization strength, this approach better recovers and infers modular structure in noisy synthetic and real data. The module-based regularization technique improves the usefulness of Gaussian graphical models in the many applications where they are employed.

%\section*{Abstract}
%Relations in multi-variate correlational data are often inferred using the graphical lasso, but this method can perform poorly when data are noisy and when the number of observed features is large. %compared to the number of samples. 
%Acknowledging that modular structure in the network representing the inferred relations is often studied downstream, we suggest a way of integrating modular structure in the selection of the regularization strength in the graphical lasso by balancing under- and overfitting of modular structure to data. %Modular structure is often studied downstream in the network representing the inferred relations, and in this work we suggest a way of integrating modular structure in the selection of the regularization strength in the graphical lasso by balancing under- and overfitting of modular structure to data.
%We show using synthetic and real data that this approach allows us to better recover and infer modular structure in noisy data, and with fewer samples compared to the standard graphical lasso.%, which uses the log-likelihood when selecting the regularization strength. % adjust according to latest results 230120


\section*{Introduction}
Inferring relations between observed features from correlational data is a foundational approach to exploring underlying mechanisms in, for example,  ecological, genetic and neural systems \cite{Barberan, Wang, Bullmore}. The resulting relations are often represented as a network where the features are nodes and their respective relations are links. These networks are dense, making it difficult to discern relevant structures. Field-specific methods to sparsify them suggest soft thresholding \cite{Horvath}, but hard thresholding is often applied in practice \cite{Barberan, deVries, Neuman}. %Gaussian graphical models are also commonly used to represent correlational data by encoding relations between features through partial correlations.
Gaussian graphical models provide an alternative way of representing correlational data by encoding relations between features through partial correlations. %, which is zero when two observed features are conditionally independent, given all the other features.
A popular approach to infer a Gaussian graphical model is the graphical lasso (GLASSO) \cite{Friedman2007, Yuan}, which estimates the precision matrix while ensuring sparsity through $l_1$-regularization. This method and related methods, such as neighborhood selection \cite{Meinshausen}, elastic net \cite{Zou_elasticnet} and Markov networks \cite{Murphy}, are widely used in many disciplines \cite{Harris, Epskamp_psycho, Cao_cElegans,Severson_batteries}. Despite its widespread application, GLASSO struggles to tackle noise and the high dimensionality that comes with many observed features, often exceeding the number of available samples \cite{Raskutti2008, Wainwright2009, Ravikumar2011, Liu2012}. 

Representing the inferred relations as networks enables studying structures in the data with standard tools from network science. Network modules -- groups of tightly connected nodes -- are studied across scientific disciplines because they reveal significant patterns and functional relationships in diverse systems, ranging from ecological \cite{calatayud2020positive} to metabolic networks \cite{guimera2005functional}. However, the GLASSO is agnostic to modular structure in the inferred networks, which can obscure network structure and subsequent interpretation and understanding of the studied systems. Simultaneously inferring the network and its modular structure can alleviate this problem, but requires prior knowledge about the dynamical processes on the network \cite{Peixoto}. Attempts at integrating modular structure with the GLASSO use a varying penalization term that depends on the underlying modular structure, with a predetermined number of modules \cite{Ambroise2009, Steeg2019}. This approach limits the usability since the number of modules is in general unknown. Manually setting the number of modules risks over- or underfitting the modular structure, and nodes in data with high noise rarely have a significant module membership.

Here we integrate the two steps from relational data to network modules -- network inference and community detection -- in an extension of the GLASSO method. We use the network's modular structure to select the regularization strength, which allows us to balance over- and underfitting the modular structure to the data. Using synthetic data, we show that this approach allows us to recover more modular structures in noisy data compared with the standard GLASSO. Applied to country-level daily incidence during the Covid-19 pandemic and gene co-expression data from the plant {\it Arabidopsis thaliana}, we find that the module-based GLASSO can identify more modular structure in these data compared to the standard GLASSO -- highly relevant for researchers studying these systems. %To further illustrate the advantages of the suggested approach we apply it to daily incidence on country level during the Covid-19 pandemic and to gene co-expression data from the plant {\it Arabidopsis thaliana}.

%It is commonplace in many fields to represent the inferred relations between observed features as a network where the nodes are the studied features and the links between them are the respective relations. However, GLASSO is blind to derived networks, with the potential to obscure network structure and subsequent interpretation and understanding of the studied systems. 

 %[A modification of the GLASSO acknowledges that some relations cannot exist given how the data were collected \cite{Grechkin}]

%... However, problems with its applicability are reported ... Related methods include neighborhood selection ... elastic net .... Related approaches to infer networks from time series rely on knowledge of the underlying dynamics \cite{Han, Peixoto}.

%It is commonplace in many fields to represent the inferred relations between observed features as a network where the nodes are the studied features and the links between them are the respective relations. 

%Correlational networks are models of the underlying data and standard tools from network science are often used to study structures in the data. Network modules are arguably the most important network structure, being studied in diverse fields across the sciences, from ecological \cite{calatayud2020positive} to metabolic networks \cite{guimera2005functional}. Attempts at integrating modular structure with the graphical lasso use a varying penalization term that depends on the underlying modular structure, with a predetermined number of modules \cite{Ambroise2009, Steeg2019}. This limits the usability since the number of modules is in general unknown -- selecting a value risks over- or underfitting modular structure -- and since in particular with noisy data nodes do not necessarily have a significant module membership. % and can then be disconnected.
%The first step in the workflow when going from data to network modules - inference of relations, meaning links in the network - is generally viewed as separated from the second step in the workflow - studying modular structure in the inferred network. In this work we integrate these two steps in the GLASSO method and use the modular structure in the network to select the regularization strength, allowing us to find the regularization that has the best balance between over- and underfitting modular structure to the data. We show using synthetic data that this approach allows us to recover more modular structure when the data are noisy compared to the standard GLASSO using log-likelihood to determine the regularization strength. To further illustrate the advantages of the suggested approach we apply it to X published datasets. %To achieve this we employ the map equation framework together with its search algorithm Infomap to find the regularization that has the best balance between over- and underfitting modular structure to the data.

\section*{Results}
Gaussian graphical models describe relations between observed features. They are derived from the precision matrix $\Theta$ that encodes conditional independence between variables, meaning that two observations $X_i$ and $X_j$ are independent, given all other observations, if the corresponding $ij$:th element in $\Theta$ is zero. The GLASSO aims at maximizing the Gaussian log-likelihood of the precision matrix given the data while ensuring a sparse solution by imposing an $l_1$-regularization term $\lambda ||\Theta ||_1$, with the regularization parameter $\lambda$. The best precision matrix $\Theta^{\lambda}$ for a specific value of $\lambda$ is thus 
\begin{equation}
    \Theta^{\lambda} = \argmax_{\Theta}\left( \log \det (\Theta) -\mathrm{tr} (\Theta \hat{\Sigma}) - \lambda ||\Theta ||_1 \right),
\end{equation}
where $\hat{\Sigma}$ is the covariance matrix calculated from the observed data. The parameter $\lambda$ determines the regularization strength and thereby the sparsity of the inferred precision matrix. The regularization parameter $\lambda$ is often determined using cross-validation where the best value $\lambda^*$ is the one that has the largest log-likelihood of the test data $\hat{\Sigma}^{test}$ given the model $\Theta^{\lambda, train}$ inferred from the training data $\hat{\Sigma}^{train}$ such that
\begin{equation}
    \lambda^* = \argmax_{\lambda}\left( \log \det (\Theta^{\lambda, train}) -\mathrm{tr} (\Theta^{\lambda, train} \hat{\Sigma}^{test})\right).
\label{eq:loglik}
\end{equation}
The resulting regularization strength conserves relations with support in both the training and test data, without considering any conserved structures in the data.

To take the modular structure into account when selecting the regularization strength, we suggest using the map equation framework and its search algorithm Infomap \cite{RosvallPNAS2008, Rosvall2, Edler1}. The map equation encodes a random walk on a network and measures the codelength $L(M)$ of the random walk given a partition $M$ of the network into modules. Infomap uses a greedy approach to find the partition $M^*$ that minimizes the codelength, 
\begin{equation}
    M^* = \argmin_M L(M),
\end{equation}
such that $M^*$ is the best partition of the network according to the minimum description length principle. This popular approach is widely recognized as one of the best methods for detecting network communities \cite{Lancichinetti,Aldecoa2013}. To connect Infomap with the GLASSO regularization, we suggest maximizing the signal of modular structure present in both the training and test sets when cross-validating the regularization parameter $\lambda$. We measure this signal using the codelength savings in the test data given the optimal partition of the training data, such that
\begin{equation}
    \lambda^* = \argmax_{\lambda}  \frac{L^{test}(1)-L^{test}(M^{\lambda, train})}{L^{test}(1)},
\label{eq:clsav}
\end{equation}
where $M^{\lambda, train}$ is the optimal partition of the training data and $L^{test}(1)$ is the one-level codelength of the test data with all nodes in the same module. The %fraction in Eq.~\ref{eq:clsav} is called the codelength savings and is larger than zero 
codelength savings are positive if the modular structure in the training data is present also in the test data and has its maximum when this shared modular structure is most prominent. This peak is associated with the $\lambda$ that best captures modular structure in the data without over- or underfitting, analogous to the log-likelihood in Eq.~\ref{eq:loglik}. In previous work \cite{Neuman}, we explored this module-based approach for hard thresholding of correlation networks and showed that a too low threshold gives a highly connected network with little modular structure in both the training and test networks, while a too high threshold gives a highly modular structure in the training network that is not present in the test network. The same reasoning applies to the GLASSO when selecting regularization strength. The approach we suggest finds the best compromise between these two extremes.

To derive the network $\mathcal{G}(\Theta^*)$ that balances under- and overfitting, we use GLASSO to estimate the precision matrix $\Theta^*$  corresponding to $\lambda^*$. We use the relation between a partition matrix element and the partial correlation so that the link $e_{ij}$ between nodes $i$ and $j$ is given by 
\begin{equation}
    e_{ij} = |- \theta_{ij}/ \sqrt{\theta_{ii}\theta_{jj}}|,
\end{equation}
where $\theta_{ij}$ is elements of $\Theta^*$, and the link weight is thus the absolute value of the partial correlation.

\subsection*{Synthetic data}
To test the module-based regularization, we generate synthetic data by sampling a covariance matrix $S$ from a Wishart distribution such that
\begin{equation}
    S\sim W_p(n, \Sigma),
\end{equation}
where $\Sigma$ is the block-diagonal covariance matrix of the planted (oracle) modular structure, $p$ is the dimension (number of features or nodes) and $n$ is the number of degrees of freedom. We plant a modular structure by imposing a block-diagonal structure:
\begin{equation}
    \Sigma_{i,j} = 
    \begin{cases}
        1, & i=j \\
        c, & M(p_i)=M(p_j)\\
        0, & M(p_i)\neq M(p_j),
    \end{cases}
\end{equation}
where $M(p_i)$ denotes the module of node $p_i$. In this way, both the planted covariance matrix $\Sigma$ and the sampled matrix $S$ are positive definite. To change the signal-to-noise ratio, we can vary the the planted within-module covariance $c$ and the degrees of freedom $n$ in the Wishart distribution. Using this setup, we sample the observed data $X$ from a $p$-variate normal distribution such that $X_i\sim N_p(0, S)$ and $X\in \mathbb{R}^{p\times q}$, where $q$ denotes the number of samples. The objective is to infer the planted modular structure using these data.

We use ten planted modules with ten nodes in each module to illustrate our approach, as shown in Fig.~\ref{fig:1x4}. The sampled covariance matrix is shown in Fig.~\ref{fig:1x4}a, where the number of degrees of freedom is $n=100$ and the planted covariance is $c=0.4$, and we see that the matrix is noisy but with discernible modular structure. Using this covariance matrix we draw $q=100$ samples to obtain the synthetic data. We see that the log-likelihood-based GLASSO (hereafter Standard GLASSO) gives a lower optimal $\lambda$ value and hence regularizes less than the module-based GLASSO (hereafter Modular GLASSO), since their respective quality functions peak at different $\lambda$-values (Fig.~\ref{fig:1x4}b). This leads to the Standard GLASSO including a lot of noisy links, as the network representation shows (Fig.~\ref{fig:1x4}c). In contrast, Modular GLASSO increases the regularization to maximize the modular structure common to the test and training data, enabling the method to correctly recover the planted modular structure (Fig.~\ref{fig:1x4}d).

%One can assume that, being agnostic to modular structure, the Standard GLASSO sees relations between all observed features in the noisy data, thus failing to identify modular structure.

\begin{figure}[tb]
\centering
\includegraphics[scale=0.6]{Figs/Fig11all.pdf}
\caption{\textbf{Comparing Standard and Modular GLASSO methods in detecting planted modular structure}. The covariance matrix sampled from the Wishart distribution is noisy but with modular structure (a). With data sampled using this matrix, the GLASSO based on log-likelihood (Standard GLASSO) regularizes less than the GLASSO based on modular structure through Infomap's codelength (Modular GLASSO) (b), which leads to the Standard GLASSO's failure to identify any modular structure (c) while the Modular GLASSO successfully recovers the planted modular structure (d).}   
\label{fig:1x4}
\end{figure}

To explore this result, we vary the covariance $c$ and the number of samples $q$. We quantify how well the methods recover the planted partition by calculating the adjusted mutual information (AMI) between the planted partition and the recovered partition, which is the partition found by Infomap given the network $\mathcal{G}(\Theta^*)$ (Fig.~\ref{fig:2}). The Standard GLASSO recovers the planted partition when the number of samples is small and the covariance is large, but not for many samples (Fig.~\ref{fig:2}a). This tendency to recover more modular structure with fewer samples exemplifies the ``blessing of dimensionality'' \cite{Steeg2019}. In contrast, the Modular GLASSO recovers the modular structure when the number of samples and the covariance are large -- increasing the number of samples is always beneficial until all modular structure is recovered (Fig.~\ref{fig:2}b).

When we decrease the noise level by using $n=1000$ degrees of freedom in the Wishart distribution, the methods show similar performance, with a slight advantage for the Standard GLASSO, and recover the modular structure for sufficiently large covariance and number of samples (Fig.~\ref{fig:2}cd). This result indicates that Standard GLASSO's performance is sensitive to the presence of noise in the data. %deteriorates when the data are noisy.

To compare the methods more closely, we plot the optimal $\lambda$-value as a function of the number of samples for a fixed value $c=0.6$ of the within-module covariance (Fig.~\ref{fig:3}). The Standard GLASSO's optimal $\lambda$ decreases for more samples, while it increases for the Modular GLASSO. The AMI approaches zero for the Standard GLASSO for more samples because it regularizes less. The Modular GLASSO's regularization increases with the number of samples. The AMI reaches 1 since the method captures the signal of the modular structure and adapts the regularization. In contrast, the Standard GLASSO does not regularize at all when there are many samples, retaining all spurious relations between the observed features and obscuring the modular structure. 


%The recovered partition is derived from the precision matrix $\Theta^*$ found by the GLASSO for both methods 


\begin{figure}[tb]
\centering
\includegraphics[scale=0.8]{Figs/Fig2labels.pdf}
%\caption{\textbf{Performance comparison of the Standard and the Modular GLASSO in detecting planted partitions under low and high noise conditions}. The adjusted mutual information (AMI) between recovered and planted partitions shows that the Standard GLASSO finds the planted partition only if the samples are few when the noise level is high but when samples and within-module covariance are sufficient for low noise. In contrast, the Modular GLASSO finds the planted partition also in high noise when samples and within-module covariance are sufficient. This result suggests that the Standard GLASSO is sensitive to noise. The AMI values are averaged over 10 runs.}
\caption{\textbf{Performance comparison of the Standard and the Modular GLASSO in detecting planted partitions under low and high noise conditions}. The adjusted mutual information (AMI) between recovered and planted partitions shows that the Standard GLASSO finds the planted partition only if the samples are few when the noise level is high, but when samples and within-module covariance are sufficient for low noise. In contrast, the Modular GLASSO finds the planted partition also in high noise when samples and covariance are sufficient.}
\label{fig:2}
\end{figure}

%\emph{Upper two figs averaged over 10 runs, lower figs one run and lower res to speed up and get a prel fig.} The adjusted mutual information between planted and inferred partition when varying the number of samples and the within-module covariance. Upper figures show synthetic data drawn from a Wishart distribution with degrees of freedom equal to the number of nodes. Second from top shows $df=10*N$. With less noise the graphical lasso using log-likelihood performs better, but when noise level is higher (top figures) it is beneficial to take the modular structure into account when cross validating the regularization parameter.

\begin{figure}[tb]
\centering
\includegraphics[scale=0.8]{Figs/Fig3.pdf}
\caption{\textbf{The optimal regularization strength as a function of the number of samples for Standard and Modular GLASSO}. The Standard GLASSO regularizes less, resulting in the inclusion of many noisy correlations. The Modular GLASSO regularizes based on the modular structure, leading to a stronger regularization as the number of samples increases and the recovery of the planted modular structure. The large points represent averages over ten runs, with individual runs shown as small points. The AMI between recovered and planted partitions is displayed as a number next to each point.}   
\label{fig:3}
\end{figure}

 \subsection*{Real-world data}
\paragraph{Covid-19 data.} We analyze the global Covid-19 data \cite{covid} with the daily incidence of Covid-19 in 192 countries over 777 days, from 2020/01/01 to 2022/02/15. The observed features are the world's countries and the samples are the 777 days with Covid-19 incidence, making it a sample-rich data set. The signal-to-noise ratio is high because the distribution of correlations significantly deviates from what would be expected from spurious correlations (the Kolmogorov-Smirnov statistic is 0.72). For these data, Standard and Modular GLASSO suggest vastly different $\lambda$-values (Fig.~\ref{fig:4}a).
For the Standard GLASSO, $\lambda^* \sim 0.001$ and $\lambda \lesssim 0.1$ results in only one module in the corresponding network, providing no information about modular structure in the Covid-19 data. Excluding the edge-case peak for a disintegrated network with many singletons, $\lambda^* \sim 0.36$ for the Modular GLASSO resulting in 14 modules (Fig.~\ref{fig:4}b). The modules spread on the world map exhibit a geographic signal, with neighboring countries often belonging to the same module, as in Eastern Europe and parts of Central America, for example. China, however, forms its own module. In some cases, the connection between countries within the same module is less obvious, such as between the United States and the Iberian Peninsula, leaving it unclear whether a causal connection exists.

While the Standard GLASSO provides no information about modular structure in the global Covid-19 data, the Modular GLASSO unveils intriguing modular patterns. This situation resembles cases with high noise levels and many samples in the analysis of synthetic data when the Standard GLASSO retains many spurious relations, resulting in a dense, module-free network.


\begin{figure}[tb]
\centering
\includegraphics[scale=0.48]{Figs/Fig4w_inset.pdf}
\caption{\textbf{Application of Standard and Modular GLASSO to Covid-19 incidence data}. The Standard GLASSO (log-likelihood) and Modular GLASSO (codelength savings) suggest vastly different regularization strengths for the Covid-19 data (a). The Standard GLASSO reveals no modular structure in the resulting network, while the Modular GLASSO uncovers the 14 modules represented by different colors on the world map (b). The modules exhibit a geographical signal as adjacent countries tend to belong to the same module, with some interesting exceptions.}
\label{fig:4}
\end{figure}

\paragraph{Gene co-expression data.} We analyze gene co-expression data obtained from the plant {\it Arabidopsis thaliana} under cold stress with included control samples (see Methods for details). We select the 1,000 genes with the highest variance across the 209 samples. Similar to the Covid-19 data, the correlations deviate significantly from what would be expected from pure noise (the Kolmogorov-Smirnov statistic is 0.39). In this case, however, the number of features exceeds the number of samples.

Cross-validating using the codelength savings to maximize the modular structure common to training and test data regularizes more. The Standard GLASSO applies minimal regularization ($\lambda^* \sim 0.002$) and finds seven modules in the data (Fig.~\ref{fig:5}a). In contrast, the Modular GLASSO suggests strong regularization ($\lambda^* \sim 0.76$) and disconnects nodes, resulting in distinct network representations of the data (Fig.~\ref{fig:5}bc).  The stronger regularization can reveal additional structure in the underlying data, offering valuable insights into the gene regulation patterns.


\begin{figure}[tb]
\centering
\includegraphics[scale=0.5]{Figs/Fig5all.pdf}
\caption{\textbf{Application of Standard and Modular GLASSO to gene co-expression data}. The Standard GLASSO (log-likelihood) and Modular GLASSO (codelength savings) suggest vastly different regularization strengths also for the gene co-expression data (a). The Standard GLASSO's minimal regularization leads to a network with little modular structure (b). In contrast, Modular GLASSO disconnects nodes to maximize the modular structure during cross-validation, revealing more regularities in the underlying system (c).}
\label{fig:5}
\end{figure}

\section*{Discussion and conclusions}
Regularizing Gaussian graphical models is challenging due to the presence of noise and the complexity of high-dimensional data.
To tackle this issue, we introduce a regularization method that capitalizes on the modular structure inherent in the data. The Modular GLASSO outperforms standard regularization approaches by applying stronger regularization to retain only the connections that contribute significantly to the modular structure. In contrast, the Standard GLASSO, which maximizes the Gaussian log-likelihood, regularizes less when dealing with noisy data, retaining many noisy links and failing to detect modular structure. The differences between these two methods are crucial when analyzing real-world data sets. For example, when analyzing Covid-19 incidence data and gene co-expression data, the Modular GLASSO uncovered more modular structure in the data, providing deeper insights about the underlying system such as identifying groups of countries with similar epidemic patterns and gene clusters with similar functions. %The Standard GLASSO fails to see these structures in the data. 

Constructing a network from correlational data requires many samples. When cross-validating modules, both the training and test networks must contain modular structure present in the complete data set. Two-fold splitting offers a reliable approach but requires relatively many samples, leading to suboptimal results with Modular GLASSO for low-noise data (Fig.~\ref{fig:2}) and relatively large spread (Fig.~\ref{fig:3}). A potential solution to this data-splitting issue is to eliminate the need for splitting altogether by employing Bayesian methods as an alternative to cross-validation. A Bayesian approach would make Modular GLASSO less data demanding.

Using codelength savings for model selection may result in selecting an overly sparse model when some modules have much larger link weights than others. In such cases, the codelength savings in the test network can be larger if the modules with smaller link weights are completely disintegrated into disconnected nodes. Partly washing out modular structure by excessive regularization in this way can, however, reveal potentially interesting structure through the remaining modules that can be difficult to discern with less regularization, as for the gene co-expression data (Fig.~\ref{fig:5}c). Since the disconnected nodes are weakly connected in the unregularized network, disconnecting them is supported in the data and in line with the module-based regularization.

In summary, we find that regularization based on modules effectively uncovers more structure in relational data sets. Because many downstream analysis tasks rely on identifying modular structure, including studying groups, communities, and clusters of observed features, many researchers may find it appealing and intuitive to base also their model selection criterion on modular structure. As we show, this approach is essential for detecting underlying modular structure in correlational data that would otherwise remain obscured by noise.

\section*{Methods}
%two-fold cross val. Averaging over 10 runs. R CVGLASSO and GLASSO settings.
%Countries: 10:th percentile with lowest variance left out. Data standardized.

\subsection*{Optimization and randomness in the map equation}
The relative codelength savings in the test network, $L^{test}(M^{\lambda,train})$, which depend on the regularization parameter $\lambda$, are stochastic for two reasons: the two-fold splitting in the cross-validation and the inherent randomness in the search algorithm Infomap optimizing the non-convex map equation objective function \cite{Calatayud}. To overcome this stochasticity, we perform two-fold splitting ten times and average the results. To calculate the AMI in Fig.~\ref{fig:2}, we also perform a sample average approximation with ten runs. For simple partition comparisons, we only look for two-level solutions with Infomap. %, such that $L^{test}(M^{\lambda,train}) = \frac{1}{10}\sum_{i=1}^{10} L^{test_i}(M^{\lambda,train_i})$.
When selecting the optimal $\lambda$ to calculate the AMI, we test a set of $\lambda \in \{0.01, 0.06, \ldots, 0.96\}$, and choose the $\lambda$ corresponding to the first maximum in the codelength savings. To avoid noise around zero at low $\lambda$ values, we use the additional condition that the codelength savings must exceed 0.01.

\subsection*{Analysis and real-world data}
We use the R packages {\tt GLASSO} and {\tt CVGLASSO} throughout the tests. To find the maximum log-likelihood, we increase or decrease the function parameters {\tt nlam} and {\tt lam.min.ratio} if necessary.

Gene co-expression data come from the Sequence Read Archive (SRA), where we identified all available RNA-Seq samples relating to cold stress in the leaf tissue of {\it Arabidopsis thaliana} ecotype Columbia-0. The selected data include both control and treated samples and were retrieved in April 2021. %A full list of included samples can be found in the supplementary material.
The data were quantified using salmon version 1.2.1 \cite{patro2017salmon} against the Araport 11 release of the {\it Arabidopsis thaliana} genome. Pre-processing and normalization were done in R using the variance stabilizing transform available in DESeq2 \cite{love2014moderated}.

To avoid constant values when analyzing the world Covid-19 data, we leave out countries belonging to the 10:th percentile with the lowest variance in daily incidence. %The reason for this is , which can happen when splitting even if the full data have different values.%, which leads to the standard deviation being undefined.

All data are standardized before analysis, and have zero mean and unit standard deviation.

\subsection*{Modular GLASSO algorithm}
Algorithm 1 shows the pseudo code for Modular GLASSO. The code uses the R function {\tt GLASSO} to estimate the precision matrix $\Theta$ with a given value of the the regularization parameter $\lambda$. The code uses the Infomap algorithm to infer the modules $M$ in a network $\mathcal{G}(\Theta)$ and to calculate the codelength savings $l$ when a network is partitioned into a given modular structure.
\begin{algorithm}[hbt!]
\caption{Modular GLASSO}\label{alg:cap}
\begin{algorithmic}
\State Input: $X\in \mathbb{R}^{p\times q}$, $p$ - number of features (nodes), $q$ - number of samples
\State Output: $\mathcal{G}(\Theta^*)$, best model

\State $\Lambda \gets \{0.01, 0.06, \ldots, 0.96\}$ // set of $\lambda$ values to test
\State $\lambda^* \gets \mathrm{min}(\Lambda)$ // optimal $\lambda$
\State $l^* \gets 0$ // optimal codelength savings
\For{$\lambda \in \Lambda$} // These steps are repeated 10 times and averaged
    %\For{$i \in [1, 2, \ldots, 10]$}
    %\Comment{Split the data}
        \State $X^{train} \gets \mathrm{sample}(X, \mathrm{fraction} = 0.5, \mathrm{axis} = 1)$
        \State $X^{test} \gets X \setminus X^{train}$
        \State $\mathcal{G}(\Theta^{train}) \gets \mathrm{GLASSO}(X^{train}, \lambda)$
        \State $M^{train} \gets \mathrm{Infomap}(\mathcal{G}(\Theta^{train}))$
        \State $\mathcal{G}(\Theta^{test}) \gets \mathrm{GLASSO}(X^{test}, \lambda)$
        \State $l \gets \mathrm{Infomap}(\mathcal{G}(\Theta^{test}), M^{train})$
    \If{$l > l^*$}
        \State $l^* \gets l$
        \State $\lambda^* \gets \lambda$
    \EndIf
    %\EndFor
\EndFor
\State $\mathcal{G}(\Theta^*) \gets \mathrm{GLASSO}(X, \lambda^*)$
\end{algorithmic}
\end{algorithm}
%%\bibliographystyle{plain}
%\bibliographystyle{unsrt}
%\bibliography{refs}
%\end{document}
\begin{thebibliography}{10}

\bibitem{Barberan}
Albert Barber{\'a}n, Scott~T Bates, Emilio~O Casamayor, and Noah Fierer.
\newblock Using network analysis to explore co-occurrence patterns in soil
  microbial communities.
\newblock {\em The ISME Journal}, 6(2):343--351, 2012.

\bibitem{Wang}
Y~X~Rachel Wang and Haiyan Huang.
\newblock Review on statistical methods for gene network reconstruction using
  expression data.
\newblock {\em J Theor Biol}, 362:53--61, Dec 2014.

\bibitem{Bullmore}
Ed~Bullmore and Olaf Sporns.
\newblock Complex brain networks: graph theoretical analysis of structural and
  functional systems.
\newblock {\em Nature Reviews Neuroscience}, 10(3):186--198, 2009.

\bibitem{Horvath}
Bin Zhang and Steve Horvath.
\newblock A general framework for weighted gene co-expression network analysis.
\newblock {\em Stat Appl Genet Mol Biol}, 4:Article17, 2005.

\bibitem{deVries}
Franciska~T. de~Vries, Rob~I. Griffiths, Mark Bailey, Hayley Craig, Mariangela
  Girlanda, Hyun~Soon Gweon, Sara Hallin, Aurore Kaisermann, Aidan~M. Keith,
  Marina Kretzschmar, Philippe Lemanceau, Erica Lumini, Kelly~E. Mason, Anna
  Oliver, Nick Ostle, James~I. Prosser, Cecile Thion, Bruce Thomson, and
  Richard~D. Bardgett.
\newblock Soil bacterial networks are less stable under drought than fungal
  networks.
\newblock {\em Nature Communications}, 9(1):3033, 2018.

\bibitem{Neuman}
Magnus Neuman, Viktor Jonsson, Joaqu{\'\i}n Calatayud, and Martin Rosvall.
\newblock Cross-validation of correlation networks using modular structure.
\newblock {\em Applied Network Science}, 7(1):75, 2022.

\bibitem{Friedman2007}
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
\newblock {Sparse inverse covariance estimation with the graphical lasso}.
\newblock {\em Biostatistics}, 9(3):432--441, 12 2007.

\bibitem{Yuan}
Ming Yuan and Yi~Lin.
\newblock {Model selection and estimation in the Gaussian graphical model}.
\newblock {\em Biometrika}, 94(1):19--35, 03 2007.

\bibitem{Meinshausen}
Nicolai Meinshausen and Peter B{\"u}hlmann.
\newblock {High-dimensional graphs and variable selection with the Lasso}.
\newblock {\em The Annals of Statistics}, 34(3):1436 -- 1462, 2006.

\bibitem{Zou_elasticnet}
Hui Zou and Trevor Hastie.
\newblock Regularization and variable selection via the elastic net.
\newblock {\em Journal of the Royal Statistical Society. Series B (Statistical
  Methodology)}, 67(2):301--320, 2005.

\bibitem{Murphy}
K.~P. Murphy.
\newblock {\em Machine learning: A probabilistic perspective}.
\newblock The MIT Press, Cambridge, Massachusetts, USA, 2012.

\bibitem{Harris}
David~J. Harris.
\newblock Inferring species interactions from co-occurrence data with markov
  networks.
\newblock {\em Ecology}, 97(12):3308--3314, 2016.

\bibitem{Epskamp_psycho}
Sacha Epskamp, Denny Borsboom, and Eiko~I. Fried.
\newblock Estimating psychological networks and their accuracy: A tutorial
  paper.
\newblock {\em Behavior Research Methods}, 50(1):195--212, 2018.

\bibitem{Cao_cElegans}
Junyue Cao, Jonathan~S. Packer, Vijay Ramani, Darren~A. Cusanovich, Chau Huynh,
  Riza Daza, Xiaojie Qiu, Choli Lee, Scott~N. Furlan, Frank~J. Steemers, Andrew
  Adey, Robert~H. Waterston, Cole Trapnell, and Jay Shendure.
\newblock Comprehensive single-cell transcriptional profiling of a
  multicellular organism.
\newblock {\em Science}, 357(6352):661--667, 2017.

\bibitem{Severson_batteries}
Kristen~A. Severson, Peter~M. Attia, Norman Jin, Nicholas Perkins, Benben
  Jiang, Zi~Yang, Michael~H. Chen, Muratahan Aykol, Patrick~K. Herring,
  Dimitrios Fraggedakis, Martin~Z. Bazant, Stephen~J. Harris, William~C. Chueh,
  and Richard~D. Braatz.
\newblock Data-driven prediction of battery cycle life before capacity
  degradation.
\newblock {\em Nature Energy}, 4(5):383--391, 2019.

\bibitem{Raskutti2008}
Pradeep Ravikumar, Garvesh Raskutti, Bin Yu, and Martin~J Wainwright.
\newblock Model selection in gaussian graphical models: High-dimensional
  consistency of $\ell _{1}$-regularized mle.
\newblock In D.~Koller, D.~Schuurmans, Y.~Bengio, and L.~Bottou, editors, {\em
  Advances in Neural Information Processing Systems}, volume~21. Curran
  Associates, Inc., 2008.

\bibitem{Wainwright2009}
Martin~J. Wainwright.
\newblock Sharp thresholds for high-dimensional and noisy sparsity recovery
  using $\ell _{1}$ -constrained quadratic programming (lasso).
\newblock {\em IEEE Transactions on Information Theory}, 55(5):2183--2202,
  2009.

\bibitem{Ravikumar2011}
Pradeep Ravikumar, Martin~J. Wainwright, Garvesh Raskutti, and Bin Yu.
\newblock {High-dimensional covariance estimation by minimizing $l_1$-penalized
  log-determinant divergence}.
\newblock {\em Electronic Journal of Statistics}, 5(none):935 -- 980, 2011.

\bibitem{Liu2012}
Han Liu, Fang Han, Ming Yuan, John Lafferty, and Larry Wasserman.
\newblock {High-dimensional semiparametric Gaussian copula graphical models}.
\newblock {\em The Annals of Statistics}, 40(4):2293 -- 2326, 2012.

\bibitem{calatayud2020positive}
Joaqu{\'\i}n Calatayud, Enrique Andivia, Adri{\'a}n Escudero, Carlos~J
  Meli{\'a}n, Rub{\'e}n Bernardo-Madrid, Markus Stoffel, Cristina Aponte,
  Nagore~G Medina, Rafael Molina-Venegas, Xavier Arnan, et~al.
\newblock Positive associations among rare species and their persistence in
  ecological assemblages.
\newblock {\em Nature ecology \& evolution}, 4(1):40--45, 2020.

\bibitem{guimera2005functional}
Roger Guimera and Lu{\'\i}s~A Nunes~Amaral.
\newblock Functional cartography of complex metabolic networks.
\newblock {\em Nature}, 433(7028):895--900, 2005.

\bibitem{Peixoto}
Tiago~P. Peixoto.
\newblock Network reconstruction and community detection from dynamics.
\newblock {\em Phys. Rev. Lett.}, 123:128301, Sep 2019.

\bibitem{Ambroise2009}
Christophe Ambroise, Julien Chiquet, and Catherine Matias.
\newblock {Inferring sparse Gaussian graphical models with latent structure}.
\newblock {\em Electronic Journal of Statistics}, 3(none):205 -- 238, 2009.

\bibitem{Steeg2019}
Greg Ver~Steeg, Hrayr Harutyunyan, Daniel Moyer, and Aram Galstyan.
\newblock Fast structure learning with modular regularization.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{RosvallPNAS2008}
Martin Rosvall and Carl~T. Bergstrom.
\newblock Maps of random walks on complex networks reveal community structure.
\newblock {\em Proceedings of the National Academy of Sciences},
  105(4):1118--1123, 2008.

\bibitem{Rosvall2}
Martin Rosvall, Daniel Axelsson, and Carl T.~Bergstrom.
\newblock The map equation.
\newblock {\em The European Physical Journal Special Topics}, 178(1):13--23,
  2009.

\bibitem{Edler1}
Daniel Edler, Ludvig Bohlin, and Martin Rosvall.
\newblock Mapping higher-order network flows in memory and multilayer networks
  with infomap.
\newblock {\em Algorithms}, 10(112), 2017.

\bibitem{Lancichinetti}
Andrea Lancichinetti and Santo Fortunato.
\newblock Community detection algorithms: A comparative analysis.
\newblock {\em Phys. Rev. E}, 80:056117, Nov 2009.

\bibitem{Aldecoa2013}
Rodrigo Aldecoa and Ignacio Mar\'in.
\newblock {Exploring the limits of community detection strategies in complex
  networks.}
\newblock {\em Sci. Rep.}, 3:2216, 2013.

\bibitem{covid}
{Our World in Data}.
\newblock Data on COVID-19 (coronavirus), https://github.com/owid/covid-19-data/tree/master/public/data, accessed 16 Feb 2022.

\bibitem{Calatayud}
Joaqu\'{\i}n Calatayud, Rub\'en Bernardo-Madrid, Magnus Neuman, Alexis Rojas,
  and Martin Rosvall.
\newblock Exploring the solution landscape enables more reliable network
  community detection.
\newblock {\em Phys. Rev. E}, 100:052308, Nov 2019.

\bibitem{patro2017salmon}
Rob Patro, Geet Duggal, Michael~I Love, Rafael~A Irizarry, and Carl Kingsford.
\newblock Salmon provides fast and bias-aware quantification of transcript
  expression.
\newblock {\em Nature methods}, 14(4):417--419, 2017.

\bibitem{love2014moderated}
Michael~I Love, Wolfgang Huber, and Simon Anders.
\newblock Moderated estimation of fold change and dispersion for rna-seq data
  with deseq2.
\newblock {\em Genome biology}, 15(12):1--21, 2014.

\end{thebibliography}
\end{document}

\section*{Outline}
\begin{itemize}
 \item Cross-validation in GL is based on log-likelihood
 \item Downstream tasks often include studying the modular structure in the estimated correlation network. Log-likelihood disregards modular structure when selecting the regularization parameter.
 \item Log-likelihood works when observing data with low noise levels
 \item We sample multivariate normal data with covariance sampled from the Wishart distribution
 \item This adds noise to the observed modular structure
 \item We suggest cross-validation based on modular structure using Infomap
 \item This leads to less regularization and allows us to recover modular structure from noisy data
 \item However, few samples leads to GL regularizing less, since few samples give less support for the many relations encoded in the precision matrix. This leads to GL recovering modular structure even better than Infomap, which has problems with few samples due to train-test splitting. But when GL is given "enough" samples it won't see anny modules since it sees all the spurious relations as supported.
 \item Demonstrate using synthetic data
 \item Do we have a real dataset that we can test this on? Use sp100, climate data, and test cold-stress gene data with subset of 4000 largest variance genes
 \item In practice, when having a rich modular structure and noise the graphical lasso performs poorly when the cross validation to select on lambda is based on log-likelihood. Using codelength improves performance in these cases.
 \item Show that larger modules improves all methods - argument to use small modules in benchmark since more difficult.
 \item Does standard GL recovery success depend on number of samples as log(number of nodes) - no
\end{itemize}
Possible journals: Biometrika (IF 3.028, 2582 pounds), Journal of Machine Learning Research (IF 4.091, free+open access), Machine Learning (IF 5.414, free through umu?), PRX (IF 14.42, free through Bibsam agreement), PRX Life, new journal starting March 20th.