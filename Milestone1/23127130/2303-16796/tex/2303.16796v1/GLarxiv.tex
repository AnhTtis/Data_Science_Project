\documentclass{article}
\usepackage[utf8]{inputenc}
%\documentclass[a4paper, 8pt]{article}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amssymb, graphicx, amsmath}
%\usepackage{subfigure}
%\usepackage{subcaption}
%\usepackage{caption}
\addtolength{\hoffset}{-1cm}
\addtolength{\voffset}{-2cm}
\addtolength{\textwidth}{2cm}
\addtolength{\textheight}{3cm}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\title{Module-based regularization improves Gaussian graphical models when observing noisy data}

%\author{Magnus Neuman, Joaquín Calatayud, Viktor Tasselius, Martin Rosvall}
\author{Magnus Neuman$^1$, Joaquín Calatayud$^3$, Viktor Tasselius$^{1,2}$,  Martin Rosvall$^1$}
\date{}

\begin{document}

\maketitle
\begin{small}
    \noindent 1. Integrated Science Lab, Department of Physics, Umeå University, Umeå, Sweden\\
    \noindent 2. School of Public Health and Community Medicine, University of Gothenburg, Gothenburg, Sweden\\
    \noindent 3. Departamento de Biología, Geología, Física y Química inorgánica, Universidad Rey Juan Carlos, Madrid, Spain
\end{small}

\section*{Abstract}
Researchers often represent relations in multi-variate correlational data using Gaussian graphical models, which require regularization to sparsify the models. Acknowledging that they often study the modular structure of the inferred network, we suggest integrating it in the cross-validation of the regularization strength to balance under- and overfitting. Using synthetic and real data, we show that this approach allows us to better recover and infer modular structure in noisy data %, and with fewer samples
compared with the graphical lasso, a standard approach using the Gaussian log-likelihood when cross-validating the regularization strength. % adjust according to latest results 230120


\section*{Introduction}
Inferring relations between observed features from correlational data is an important problem in many scientific disciplines, including ecology, genomics and neuroscience \cite{Barberan, Wang, Bullmore}. The resulting relations are often represented as a network where the features are nodes and their respective relations are links. These networks are typically dense, making it difficult to discern structure. Field-specific methods to make them sparser suggest soft thresholding \cite{Horvath}, but hard thresholding is often applied in practice \cite{Barberan, deVries, Neuman}. Gaussian graphical models are also commonly used to represent correlational data by encoding relations between features through partial correlation. %, which is zero when two observed features are conditionally independent, given all the other features.
A common approach to infer a Gaussian graphical model is the graphical lasso (GLASSO) \cite{Friedman2007, Yuan}, which estimates the precision matrix while ensuring sparsity through $l_1$-regularization. This method and related methods, such as neighborhood selection \cite{Meinshausen}, elastic net \cite{Zou_elasticnet} and Markov networks \cite{Murphy}, are widely used in many disciplines \cite{Harris, Epskamp_psycho, Cao_cElegans,Severson_batteries}. Problems reported with the GLASSO include the high dimensionality that comes with many observed features, often exceeding the number of available samples, and the presence of noise in the data \cite{Raskutti2008, Wainwright2009}. 

Representing the inferred relations as networks enables studying structures in the data with standard tools from network science. Network modules -- groups of tightly connected nodes -- are arguably the most important network structure, studied in diverse fields across the sciences, from ecological \cite{calatayud2020positive} to metabolic networks \cite{guimera2005functional}. However, the GLASSO is agnostic to modular structure in the inferred networks, which can obscure network structure and subsequent interpretation and understanding of the studied systems. Attempts at integrating modular structure with the GLASSO use a varying penalization term that depends on the underlying modular structure, with a predetermined number of modules \cite{Ambroise2009, Steeg2019}. This tweak limits the usability since the number of modules is in general unknown. Manually setting the number of modules risks over- or underfitting the modular structure, and nodes in data with high noise rarely have a significant module membership.

We integrate the two steps from relational data to network modules -- network inference and community detection -- in the GLASSO method. Using the network's modular structure to select the regularization strength allows us to balance over- and underfitting the modular structure to the data. Using synthetic data, we show that this approach allows us to recover more modular structures in noisy data compared with the standard GLASSO.  Applied to country level daily incidence during the Covid-19 pandemic and gene co-expression data from the plant {\it Arabidopsis thaliana}, we find that the module-based GLASSO can identify more modular structure in these data compared to the standard GLASSO. %To further illustrate the advantages of the suggested approach we apply it to daily incidence on country level during the Covid-19 pandemic and to gene co-expression data from the plant {\it Arabidopsis thaliana}.

\section*{Results}
Gaussian graphical models describe relations between observed features and are derived from the precision matrix $\Theta$ that encodes conditional independence between variables, meaning that two observations $X_i$ and $X_j$ are independent, given all other observations, if the corresponding $ij$:th element in $\Theta$ is zero. The GLASSO aims at maximizing the Gaussian log-likelihood of the precision matrix given the data while ensuring a sparse solution by imposing an $l_1$-regularization term $\lambda ||\Theta ||_1$ with the regularization parameter $\lambda$. The best precision matrix $\Theta^{\lambda}$ for a specific value of $\lambda$ is thus 
\begin{equation}
    \Theta^{\lambda} = \argmax_{\Theta}\left( \log \det (\Theta) -\mathrm{tr} (\Theta \hat{\Sigma}) - \lambda ||\Theta ||_1 \right)
\end{equation}
where $\hat{\Sigma}$ is the covariance matrix calculated from the observed data. The parameter $\lambda$ determines the regularization strength and thereby the sparsity of the inferred precision matrix. The regularization parameter $\lambda$ is often determined using cross-validation where the best value $\lambda^*$ is the one that has the largest log-likelihood of the test data $\hat{\Sigma}^{test}$ given the model $\Theta^{\lambda, train}$ inferred from the training data $\hat{\Sigma}^{train}$, such that
\begin{equation}
    \lambda^* = \argmax_{\lambda}\left( \log \det (\Theta^{\lambda, train}) -\mathrm{tr} (\Theta^{\lambda, train} \hat{\Sigma}^{test})\right).
\label{eq:loglik}
\end{equation}
This leads to a regularization strength that conserves relations with support in both the training and test data, without considering any conserved structures in the data.

As a way of taking modular structure into account when selecting the regularization strength we suggest using the map equation framework and its search algorithm Infomap \cite{RosvallPNAS2008, Rosvall2, Edler1}. The map equation encodes a random walk on a network and gives the code length $L(M)$ of the random walk given a partition $M$ of the network into modules. Building on the principle of minimum description length, Infomap uses a greedy approach to find the partition $M^*$ that minimizes the code length, such that 
\begin{equation}
    M^* = \argmin_M L(M),
\end{equation}
and $M^*$ is in this sense the best partition of the network. This approach is widely used and recognized as one of the best methods for detecting network communities \cite{Lancichinetti} \emph{More?}. To connect Infomap with the GLASSO and thereby integrating modular structure with the GLASSO regularization we suggest using a cross-validation procedure to find the best regularization parameter $\lambda^*$ that maximizes the signal of modular structure present in both the training and test sets. We measure this signal using the code length savings in the test data given the optimal partition of the training data, such that
\begin{equation}
    \lambda^* = \argmax_{\lambda}  \frac{L^{test}(1)-L^{test}(M^{\lambda, train})}{L^{test}(1)},
\label{eq:clsav}
\end{equation}
where $M^{\lambda, train}$ is the optimal partition of the training data and $L^{test}(1)$ is the one-level code length of the training data, with all nodes in the same module. The fraction in Eq.\ \ref{eq:clsav} is called the code length savings and is larger than zero if the modular structure in the training data is present also in the test data and has its maximum when this common modular structure peaks. This maximum value corresponds to the $\lambda$ that best captures modular structure in the data without over- or underfitting, analogous to the log-likelihood in Eq.\ \ref{eq:loglik}. In a previous work \cite{Neuman} we explored this approach for hard thresholding of correlation networks and showed that a too low threshold gives a highly connected network with little modular structure in both the training and test networks, while a too high threshold gives a highly modular structure in the training network that is not present in the test network. The same kind of reasoning applies to the GLASSO when selecting regularization strength and the approach we suggest can find the best compromise between these two extremes.

The network $\mathcal{G}(\Theta^*)$ is derived from the precision matrix $\Theta^*$ given by GLASSO, where $\Theta^*$ denotes the precision matrix corresponding to $\lambda^*$. We use the relation between a partition matrix element and the partial correlation so that the link $e_{ij}$ between nodes $i$ and $j$ is given by 
\begin{equation}
    e_{ij} = |- \theta_{ij}/ \sqrt{\theta_{ii}\theta_{jj}}|,
\end{equation}
where $\theta_{ij}$ is elements of $\Theta^*$, and the link weight is thus the absolute value of the partial correlation.

\section*{Synthetic data}
To test the suggested method we use synthetic data where a covariance matrix $S$ is sampled from a Wishart distribution such that
\begin{equation}
    S\sim W_p(n, \Sigma),
\end{equation}
where $\Sigma$ is the block-diagonal covariance matrix of the planted (oracle) modular structure, $p$ is the dimension (number of features or nodes) and $n$ is the number of degrees of freedom. We plant a modular structure by imposing a block-diagonal structure as such:
\begin{equation}
    \Sigma_{i,j} = 
    \begin{cases}
        1, & i=j \\
        c, & M(p_i)=M(p_j)\\
        0, & M(p_i)\neq M(p_j),
    \end{cases}
\end{equation}
where $M(p_i)$ denotes the module of node $p_i$. In this way both the planted covariance matrix $\Sigma$ and the sampled matrix $S$ are positive definite. The planted within-module covariance $c$ can be varied to increase the signal-to-noise ratio as can the degrees of freedom $n$ in the Wishart distribution. The observed data $X$ using this setup are then sampled from a $p$-variate normal distribution such that $X_i\sim N_p(0, S)$ and $X\in \mathbb{R}^{p\times q}$ where $q$ denotes the number of samples. The objective is to infer the planted modular structure using these data.

We use 10 planted modules with 10 nodes in each module to illustrate our approach, as shown in Fig.\ \ref{fig:1x4}. The sampled covariance matrix is shown in Fig.\ \ref{fig:1x4}a, where the number of degrees of freedom is $n=100$ and the planted covariance is $c=0.4$, and we see that the matrix is noisy but with discernible modular structure. Using this covariance matrix we draw $q=100$ samples to obtain the synthetic data. We see that the log-likelihood-based GLASSO (hereafter Standard GLASSO) gives a lower optimal $\lambda$ value and hence regularizes less than the module-based GLASSO (hereafter Modular GLASSO), since their respective quality functions peak at different $\lambda$-values (Fig.\ \ref{fig:1x4}b). This leads to the Standard GLASSO including a lot of noisy links, as the network representation shows (Fig.\ \ref{fig:1x4}c). Modular GLASSO on the other hand increases the regularization to maximize the modular structure common in the test and training data, which leads the method to correctly recovering the planted modular structure, as seen in Fig.\ \ref{fig:1x4}d.

%One can assume that, being agnostic to modular structure, the Standard GLASSO sees relations between all observed features in the noisy data, thus failing to identify modular structure.

\begin{figure}[tb]
\begin{center}
\includegraphics[scale=0.6]{Figs/Fig11all.pdf}
\caption{The covariance matrix sampled from the Wishart distribution is noisy but with modular structure (a). With data sampled using this matrix, the GLASSO based on log-likelihood (Standard GLASSO) regularizes less than the GLASSO based on modular structure through Infomap's code length (Modular GLASSO) (b), which leads to the Standard GLASSO failing to identify modular structure (c) while the Modular GLASSO recovers the planted modular structure (d).}   
\label{fig:1x4}
\end{center}
\end{figure}

To explore this further we vary the covariance $c$ and the number of samples $q$. We quantify how well the methods recover the planted partition by calculating the adjusted mutual information (AMI) between the planted partition and the recovered partition, which is the partition found by Infomap given the network $\mathcal{G}(\Theta^*)$. The results are shown in Fig.\ \ref{fig:2} (upper). We see that the Standard GLASSO indeed recovers the planted partition when the number of samples is small and the covariance is large enough, but when the number of samples is larger the AMI is zero and the Standard GLASSO then recovers nothing of the planted modules. The Modular GLASSO recovers the modular structure when the number of samples and the covariance are large enough. Contrary to the Standard GLASSO it is beneficial to increase the number of samples in order to recover more modular structure, but only up to a certain point where all of the modular structure is recovered. Applying the Standard GLASSO in a region of the $cq$-space where it recovers more of the modular structure as the number of samples decreases can lead to observations of the ``blessing of dimensionality'', which have been reported \cite{Steeg2019}.

If we decrease the noise level by using $n=1000$ degrees of freedom in the Wishart distribution the methods behave similarly, with a slight advantage for the Standard GLASSO, recovering the modular structure for large enough covariance and number of samples (Fig.\ \ref{fig:2}, lower). The performance of the Standard GLASSO thus seems sensitive to the presence of noise in the data. %deteriorates when the data are noisy.

To further understand the differences between the methods we plot the optimal $\lambda$-value as a function of the number of samples for a fixed value $c=0.6$ of the within-module covariance (Fig.\ \ref{fig:3}). We can see that the Standard GLASSO optimal $\lambda$ decreases as the number of samples increases, while it increases for the Modular GLASSO. The AMI is also included and we see that it approaches zero for the Standard GLASSO as the number of samples increases and as the method regularizes less. The regularization based on modular structure that is built into Modular GLASSO leads to a stronger regularization as the number of samples increases, and an AMI equal to one, since the method captures the signal of the modular structure and adapts it level of regularization to that signal. The opposite seems to be the case for Standard GLASSO since it regularizes nothing when there are many samples, in this way retaining all relations between the observed features, which are the noisy correlations between features in this case. This obscures modular structure. 


%The recovered partition is derived from the precision matrix $\Theta^*$ found by the GLASSO for both methods  

\begin{figure}[tb]
\begin{center}
    

\includegraphics[scale=0.8]{Figs/Fig2.pdf}

\caption{The adjusted mutual information (AMI) between recovered and planted partition shows that the Standard GLASSO finds the planted partition only if the samples are few when the noise level is high, but when samples and within-module covariance are sufficient for low noise. Modular GLASSO on the other hand finds the planted partition also in high noise when samples and within-module covariance are sufficient. This suggests that the Standard GLASSO is sensitive to noise. The AMI is averaged over 10 runs.}
\label{fig:2}
\end{center}
\end{figure}

%\emph{Upper two figs averaged over 10 runs, lower figs one run and lower res to speed up and get a prel fig.} The adjusted mutual information between planted and inferred partition when varying the number of samples and the within-module covariance. Upper figures show synthetic data drawn from a Wishart distribution with degrees of freedom equal to the number of nodes. Second from top shows $df=10*N$. With less noise the graphical lasso using log-likelihood performs better, but when noise level is higher (top figures) it is beneficial to take the modular structure into account when cross validating the regularization parameter.

\begin{figure}[tb]
\begin{center}
\includegraphics[scale=0.8]{Figs/Fig3.pdf}
\caption{The optimal $\lambda$-values for the two methods behave differently when the number of samples increases. The Standard GLASSO regularizes less and then includes many of the noisy correlations. The Modular GLASSO regularizes based on modular structure, which leads to a stronger regularization as the number of samples increases, in this way recovering the planted modular structure. The digits are the AMI between recovered and planted partition. The large dots correspond to an average over 10 runs, and the individual runs are included as small dots.}   
\label{fig:3}
\end{center}
\end{figure}

 \section*{Real-world data}
We apply the Modular GLASSO to real-world data sets in order to further illustrate its behavior. Firstly, we analyze the global Covid-19 data \cite{covid} where the daily incidence of Covid-19 is observed in 192 countries over 777 days. The observed features are thus the world's countries and the samples are the 777 days with Covid-19 incidence, making this a sample-rich data set. The signal-to-noise ratio is high, since the distribution of correlations is far from what is expected from purely spurious correlations (Kolmogorov-Smirnov 0.72). Figure \ref{fig:4}a shows the Standard GLASSO log-likelihood and the Modular GLASSO code length savings of the trained model given the test data, and we see that the methods suggest vastly different $\lambda$-values. %In fact, the Standard GLASSO $\lambda^*$ is so small ($~0.001$) that it is difficult to discern on the linear scale in Fig. \ref{fig:4}a.
All $\lambda$-values up to approximately 0.1 will result in only one module in the corresponding network, thus giving no information about modular structure in the Covid-19 data. The Modular GLASSO has a local maximum of the code length savings at $\lambda = 0.36$. For high $\lambda$-values the network disintegrates and has many singleton nodes, giving large code length savings. Care has to be taken as to not consider a maximum where this is the case, but the first maximum as $\lambda$ increases. The optimal $\lambda$ as given by the Modular GLASSO results in a network with 14 modules, indicated on the world map with different colors (Fig.\ \ref{fig:4}b). By inspecting the modules visually we see that some seem to have a geographic signal, with adjacent countries belonging to the same module, such as Eastern Europe and parts of Central America. China is in its own module and there seems to be a geographic signal on all continents. In some cases the connection between countries in the same module is less obvious, such as between the US and the Iberian peninsula, and it remains an open question whether there is a causal connection or not. We hence see that the Standard GLASSO gives no information about modular structure in the world Covid-19 data while Modular GLASSO reveals seemingly interesting modular structure in these data. Comparing with the analysis above using synthetic data this is similar to the case when the noise level is high and there are many samples, which forces the Standard GLASSO to retain many spurious relations which results in a dense and module-free network.


\begin{figure}[tb]
\begin{center}
    

\includegraphics[scale=0.48]{Figs/Fig4w_inset.pdf}

\caption{The Standard GLASSO (log-likelihood) and Modular GLASSO (code length savings) suggest vastly different $\lambda$-values for regularizing the Covid-19 data (a). Using Standard GLASSO there is no modular structure in the resulting network while when using Modular GLASSO there are 14 modules, indicated on the world map by different colors (b). The modules have a geographical signal in the sense that adjacent countries belong to the same module, with some notable and potentially interesting exceptions.}
\label{fig:4}
\end{center}
\end{figure}

Secondly, we analyze gene co-expression data obtained from the plant {\it Arabidopsis thaliana} under cold stress but with included control samples (see Methods for details). We select the 1000 genes with the highest variance across the 209 samples. This is thus a data set where the number of features exceeds the number of samples, in contrast to the Covid-19 data. Also the correlations in these data are far from what would be expected in pure noise (Kolmogorov-Smirnov 0.39).The Standard GLASSO regularizes less ($\lambda^* \approx 0.002$) and finds seven modules in the data (Fig. \ref{fig:5}a), while the Modular GLASSO suggests strong regularization ($\lambda^* \approx 0.8$) and disconnects nodes, which results in different network representations of the data (Fig. \ref{fig:5}bc). Hence, we see that when cross-validating using the code length savings, as in Modular GLASSO, it is favorable to regularize more, and disconnect nodes, in order to maximize the modular structure common to training and test data. This can potentially reveal more structure in the underlying data and in the studied gene regulation.


\begin{figure}[tb]
\begin{center}
    

\includegraphics[scale=0.5]{Figs/Fig5all.pdf}

\caption{Also when analyzing gene co-expression data the Standard GLASSO (log-likelihood) and Modular GLASSO (code length savings) give different regularization strengths (a). This leads to Standard GLASSO detecting less modular structure (b). Using the code length savings, Modular GLASSO disconnects nodes to maximize the modular structure when cross-validating (c), which can reveal more about the underlying system.}
\label{fig:5}
\end{center}
\end{figure}

\section*{Discussion}
We have suggested a general method to base the regularization of Gaussian graphical models on modular structure in the data. This method (Modular GLASSO) improves the ability to find modular structure in noisy data by regularizing more to keep only the links that give significant modular structure. On the contrary, the Standard GLASSO, which maximizes the Gaussian log-likelihood, regularizes less when the data are noisy, in this way keeping many noisy links and failing to detect modular structure. The differences between these two methods can be crucial when analyzing real-world data sets, as we saw when analyzing the Covid-19 incidence data and the gene co-expression data, since Modular GLASSO can reveal more about the data and the underlying system, such as groups of countries with similar epidemic patterns or groups of genes with similar function. The standard GLASSO fails to see these structures in the data. Many downstream analysis tasks deal with modular structure in the data, studying groups, communities, clusters, etc.\ of observed features, and it should therefore be appealing, and intuitive, to many researchers to base also their model selection criterion on modular structure.

Constructing a network from correlational data is data-demanding in the sense that we need many samples. Both the training and test networks have to contain modular structure present in the complete data set. The best way to achieve this is to do 2-fold splitting, which however requires relatively many samples. This is the main reason we get somewhat inferior results with Modular GLASSO in Fig.\ \ref{fig:2} for low-noise data, and a large spread in Modular GLASSO results in Fig.\ \ref{fig:3}. A possible way of improving this and making Modular GLASSO less data demanding is to explore Bayesian methods as an alternative to cross-validation. The Standard GLASSO is less sensitive to the choice of splitting strategy.

A potential drawback of using code length savings for model selection is that it can lead to selecting an overly sparse model if there are modules in the network that are strongly connected, with large link weights, compared to other modules. In this case the code length savings in the test network can be larger if the modules with smaller link weights are completely disintegrated, resulting in disconnected nodes, in this way failing to capture modular structure by excessive regularization. However, as we saw when analyzing the gene co-expression data, this can reveal potentially interesting structure through the modules that remain (Fig. \ref{fig:5}c) that can be difficult to discern with less regularization. The disconnected nodes are weakly connected in the unregularized network and disconnecting them is therefore supported in the data, which is what the module-based regularization suggests.

\section*{Methods}
%2-fold cross val. Averaging over 10 runs. R CVGLASSO and GLASSO settings.
%Countries: 10:th percentile with lowest variance left out. Data standardized.

\subsection*{Optimization and randomness in the map equation}
The relative code length savings in the test network, $L^{test}(M^{\lambda,train})$, that depend on the regularization parameter $\lambda$, are stochastic for two reasons. The first reason is the two-fold splitting in the cross-validation. The second reason is the inherent randomness in the optimization algorithm Infomap and the non-convex properties of the map equation \cite{Calatayud}. To overcome this we do 2-fold splitting 10 times and average the results, and in each split we do a sample average approximation with 10 runs, such that $L^{test}(M^{\lambda,train}) = \frac{1}{10}\sum_{i=1}^{10} L^{test_i}(M^{\lambda,train_i})$. When selecting the optimal $\lambda$ to calculate the AMI in Fig. \ref{fig:2} we test a set of $\lambda \in [0.01, 0.06, \ldots, 0.96]$, and choose the $\lambda$ corresponding to the first maximum in the code length savings. To avoid noise around zero at low $\lambda$ values we use the additional condition that the code length savings have to be larger than 0.01.

\subsection*{Analysis and real-world data}
The R packages {\tt GLASSO} and {\tt CVGLASSO} are used throughout this work. The function parameters {\tt nlam} and {\tt lam.min.ratio} are changed (increased and decreased resp.) if necessary to find the maximum of the log-likelihood. 

Gene co-expression data were retrieved from the Sequence Read Archive (SRA) where we identified all available RNA-Seq samples relating to cold stress in {\it Arabidopsis thaliana} ecotype Columbia-0, but limited ourselves to leaf tissue and excluded any genetic variants. The selected data include both control and treated samples and were retrieved in April 2021. %A full list of included samples can be found in the supplementary material.
The data were quantified using salmon version 1.2.1 \cite{patro2017salmon} against the Araport 11 release of the {\it Arabidopsis thaliana} genome. Pre-processing and normalization were done in R using the variance stabilizing transform available in DESeq2 \cite{love2014moderated}.

The world Covid-19 data were analyzed leaving out the countries belonging to the 10:th percentile with the lowest variance. The reason for this is to avoid constant values, which can happen when splitting even if the full data have different values, which leads to the standard deviation being undefined.

All data are standardized before analysis, and they thus have zero mean and unit standard deviation.



%\bibliographystyle{plain}
%\bibliographystyle{unsrt}
%\bibliography{refs}
\begin{thebibliography}{2}
    \bibitem{Barberan} Barberán, A., Bates, S.T., Casamayor, E.O., Fierer, N.: Using network analysis to explore co-occurrence patterns in soil microbial communities. The ISME Journal 6(2), 343–351 (2012). doi:10.1038/ismej.2011.119
    \bibitem{Wang} Wang, Y.X.R., Huang, H.: Review on statistical methods for gene net- work reconstruction using expression data. J Theor Biol 362, 53–61 (2014). doi:10.1016/j.jtbi.2014.03.040
    \bibitem{Bullmore} Bullmore, E., Sporns, O.: Complex brain networks: graph theoretical analysis of struc- tural and functional systems. Nature Reviews Neuroscience 10(3), 186–198 (2009). doi:10.1038/nrn2575
    \bibitem{Horvath} Zhang, B., Horvath, S.: A general framework for weighted gene co-expression network analysis. Stat Appl Genet Mol Biol 4, 17 (2005). doi:10.2202/1544-6115.1128
    \bibitem{deVries} de Vries, F.T., et al.: Soil bacterial networks are less stable under drought than fungal networks. Nature Commu- nications 9(1), 3033 (2018). doi:10.1038/s41467-018-05516-7
    %\bibitem{Marbach} Marbach, D., et al.: Wisdom of crowds for robust gene network inference. Nature Methods 9(8), 796–804 (2012). doi:10.1038/nmeth.2016
    \bibitem{Neuman} Neuman, M, Jonsson, V., Calatayud, J. Rosvall, M.: Crossvalidation of correlation networks using modular structure. Applied Network Science, 7(1):75, 2022.
    \bibitem{Friedman2007} Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432–441, 12 2007.
    \bibitem{Yuan} Ming Yuan and Yi Lin. Model selection and estimation in the Gaussian graphical model. Biometrika, 94(1):19–35, 03 2007.
    \bibitem{Meinshausen} Meinshausen, N., Bühlmann, P.: High-dimensional graphs and variable selection with the Lasso. The Annals of Statistics 34(3), 1436–1462 (2006).
    \bibitem{Zou_elasticnet} Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 67(2):301–320, 2005.
    \bibitem{Murphy} K. P. Murphy. Machine learning: A probabilistic perspective. The MIT Press, Cambridge, Massachusetts, USA, 2012.
    \bibitem{Harris} David J. Harris. Inferring species interactions from co-occurrence data with Markov networks. Ecology, 97(12):3308–3314, 2016.
    \bibitem{Epskamp_psycho} Sacha Epskamp, Denny Borsboom, and Eiko I. Fried. Estimating psychological networks and their accuracy: A tutorial paper. Behavior Research Methods, 50(1):195–212, 2018.
    \bibitem{Cao_cElegans}  Junyue Cao, et al. Comprehensive single-cell transcriptional profiling of a multicellular organism. Science, 357(6352):661–667, 2017.
    \bibitem{Severson_batteries}  Kristen A. Severson, Peter M. Attia, Norman Jin, Nicholas Perkins, Benben Jiang, Zi Yang, Michael H. Chen, Muratahan Aykol, Patrick K. Herring, Dimitrios Fraggedakis, Martin Z. Bazant, Stephen J. Harris, William C. Chueh, and Richard D. Braatz. Datadriven prediction of battery cycle life before capacity degradation. Nature Energy, 4(5):383–391, 2019.
    \bibitem{Raskutti2008} Pradeep Ravikumar, Garvesh Raskutti, Bin Yu, and Martin J Wainwright. Model selection in gaussian graphical models: High-dimensional consistency of l1-regularized mle. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2008.
    \bibitem{Wainwright2009} Martin J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using  l1-constrained quadratic programming (lasso). IEEE Transactions on Information Theory, 55(5):2183–2202, 2009.
    \bibitem{calatayud2020positive} Calatayud, J., Andivia, E., Escudero, A., Melián, C.J., Bernardo-Madrid, R., Stoffel, M., Aponte, C., Medina, N.G., Molina-Venegas, R., Arnan, X., et al.: Positive associations among rare species and their persistence in ecological assemblages. Nature ecology \& evolution 4(1), 40–45 (2020)
    \bibitem{guimera2005functional} Guimera, R., Nunes Amaral, L.A.: Functional cartography of complex metabolic net- works. nature 433(7028), 895–900 (2005)
    \bibitem{Ambroise2009} Christophe Ambroise, Julien Chiquet, and Catherine Matias. Inferring sparse Gaussian graphical models with latent structure. Electronic Journal of Statistics, 3(none):205–238, 2009.
    \bibitem{Steeg2019} Greg Ver Steeg, Hrayr Harutyunyan, Daniel Moyer, and Aram Galstyan. Fast structure learning with modular regularization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
    \bibitem{RosvallPNAS2008} Rosvall, M., Bergstrom, C.T.: Maps of random walks on complex net- works reveal community structure. Proceedings of the National Academy of Sciences 105(4), 1118–1123 (2008).
    \bibitem{Rosvall2} Rosvall, M., Axelsson, D., Bergstrom, C.T.: The map equation. The European Physical Journal Special Topics 178(1), 13–23 (2009).
    \bibitem{Edler1} Edler, D., Bohlin, L., Rosvall, M.: Mapping higher-order network flows in memory and multilayer networks with infomap. Algorithms 10(112) (2017)
    \bibitem{Lancichinetti} Andrea Lancichinetti and Santo Fortunato. Community detection algorithms: A comparative analysis. Phys. Rev. E, 80:056117, 2009.
    \bibitem{covid} Our World in Data. Data on Covid-19 (coronavirus). https://github.com/owid/covid-19-data/tree/master/public/data, Accessed 16 Feb 2022.
    \bibitem{Calatayud} Calatayud, J., Bernardo-Madrid, R., Neuman, M., Rojas, A., Rosvall, M.: Exploring the solution landscape enables more reliable network community detection. Phys. Rev. E 100, 052308 (2019). doi:10.1103/PhysRevE.100.052308
    \bibitem{patro2017salmon} Patro, R., Duggal, G., Love, M.I., Irizarry, R.A., Kingsford, C.: Salmon provides fast and bias-aware quantification of transcript expression. Nature methods 14(4), 417–419 (2017)
    \bibitem{love2014moderated} Love, M.I., Huber, W., Anders, S.: Moderated estimation of fold change and dispersion for rna-seq data with deseq2. Genome biology 15(12), 1–21 (2014)

\end{thebibliography}
\end{document}