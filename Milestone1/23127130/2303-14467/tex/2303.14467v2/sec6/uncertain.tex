\subsection{Uncertain graphs}
Zou~\cite{zou2013polynomial} studied the problem of extracting a \rev{vertex subset that} maximizes the \emph{expected density} from  an uncertain graph, i.e., $G=(V,E,p)$ and $p: E\rightarrow [0,1]$,
where $p(e)$ represents the probability of existence for each $e\in E$ \cite{PotamiasBGK10,BonchiGKV14}. 
\rev{The author} showed that this problem can be reduced to DSP on edge-weighted graphs,
and designed a polynomial-time exact algorithm based on the reduction.
\rev{Recently, Sun et al. \cite{sun2021efficient} applied a probabilistic truss indexing framework to the triangle DSP (see Section~\ref{subsec:numerator}) in uncertain graphs.}

Miyauchi and Takeda~\cite{miyauchi2018robust} considered the uncertainty of edge weights rather than the existence of edges.
To model that, they assumed that there is an edge-weight space $I=\times_{e\in E}[l_e,r_e]\subseteq \times_{e\in E}[0,\infty)$
that contains the unknown true edge weight $w$.
To evaluate the performance of $S\subseteq V$ without any concrete edge weight,
they employed a well-known measure in the field of robust optimization, called the robust ratio.
Intuitively, $S\subseteq V$ with a large robust ratio has a density close to the optimal value even on $G$ with the edge weight selected adversarially from $I$.
Using this, they introduced the robust densest subgraph problem:
given $G=(V,E)$ and $I=\times_{e\in E}[l_e,r_e]\subseteq \times_{e\in E}[0,\infty)$,
we are asked to find $S\subseteq V$ that maximizes the robust ratio under $I$.
They designed an algorithm that returns $S\subseteq V$ with the best possible robust ratio (except for the constant factor), under some mild condition. 
In addition, they also introduced the robust densest subgraph problem with sampling oracle,
where we have access to an oracle that accepts $e\in E$ and outputs a value drawn from a distribution on $[l_e, r_e]$
in which the expected value is equal to the unknown true edge weight, and designed a pseudo-polynomial-time algorithm with a strong quality guarantee.

Tsourakakis et al.~\cite{tsourakakis2019novel} introduced an optimization problem called the risk-averse dense subgraph discovery problem.
Here an uncertain graph is defined as a pair of $G=(V,E)$ and $(g_e(\theta_e))_{e\in E}$,
where the weight $w(e)$ of each edge $e\in E$ is drawn independently from the rest
according to some probability distribution $g_e$ with parameter $\theta_e$.
Each probability distribution $g_e$ is assumed to have finite mean $\mu_e$ and variance $\theta^2_e$.
Intuitively speaking, their risk-averse variant aims to find $S\subseteq V$
that maximizes $\frac{\sum_{e\in E[S]}\mu_e}{|S|}$ but minimizes $\frac{\sum_{e\in E[S]}\sigma^2_e}{|S|}$.
Tsourakakis et al.~\cite{tsourakakis2019novel} showed that this problem reduces to DSP on negatively-weighted graphs,
and designed an efficient approximation algorithm based on the reduction.

Recently, Kuroki et al.~\cite{Kuroki+20} pointed out that
the sampling procedure used in Miyauchi and Takeda~\cite{miyauchi2018robust},
where all edges are repeatedly queried by a sampling oracle that returns an individual edge weight,
is often quite costly or sometimes impossible.
To overcome this issue, \rev{the authors} introduced a novel framework called the densest subgraph bandits (DS bandits),
by incorporating the concept of stochastic combinatorial bandits~\cite{chen2013combinatorial,chen2014combinatorial} into DSP.
In DS bandits, a learner is given $G=(V,E)$ 
whose edge-weights are associated with unknown probability distributions.
During the exploration period, the learner chooses a subset of edges (rather than only single edge, unlike Miyauchi and Takeda~\cite{miyauchi2018robust}) to sample,
and observes the sum of noisy edge weights in a queried subset.
They investigated DS bandits with the objective of best arm identification;
that is, the learner must report one subgraph that (s)he believes to be optimal after the exploration period.
Their first algorithm has an upper bound on the number of samples required to identify a $(1+\epsilon)$-approximate solution
with probability at least $1-\delta$ for $\epsilon>0$ and $\delta \in (0,1)$.
Their second algorithm, based on the greedy peeling algorithm, is scalable and parameter free, 
which has a non-trivial upper bound on the probability that the density of the output is less than half of the optimal value.


The most recent contribution is due to Saha et al. \cite{Saha2022most}, 
that defined DSP in uncertain graphs with an alternative notion of density, called Densest Subgraph Probability (DSPr).
DSPr of $U \subseteq V$ is the summation of the probabilities of all \rev{realizations} in which $U$ represents the densest subgraph (with any \rev{notion of density}).
They designed the Most Probable Densest Subgraph (MPDS) problem considering edge density, clique density, pattern density, and their top-$k$ variants.
Their principal contribution is the edge density-based MPDS algorithm, that is built on independent sampling of possible worlds (e.g., via \rev{a} Monte-Carlo sampling) and \rev{an} efficient enumeration of all edge-densest subgraphs \rev{in each of them}.


