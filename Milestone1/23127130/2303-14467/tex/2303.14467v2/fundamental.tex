In this section, we review the fundamental results for DSP,
starting from classical exact and approximation algorithms
and arriving to very recent breakthrough results.

\subsection{Exact algorithms for DSP}
There is a long history of polynomial-time exact algorithms for DSP.
In 1979, Picard and Queyranne~\cite{Picard82} presented the first exact algorithm for DSP in their technical report (officially published in 1982).
The algorithm is designed based on a series of maximum-flow computations.
In 1984, Goldberg~\cite{goldberg1984finding} gave an improved version of the above algorithm in terms of time complexity.
More than 15 years later, Charikar~\cite{Charikar2000} designed an LP-based exact algorithm for DSP.
As maximum-flow computation can be done by solving LP, the existence of an LP-based exact algorithm is trivial.
However, Charikar's \rev{LP-based algorithm} \rev{exploits a different idea}  \rev{and it is of independent interest, because it allows
to see the renowned greedy-peeling approximation algorithm (described later in Section \ref{subsec:approx})
as a primal-dual algorithm for DSP.}


\subsubsection{Goldberg's maximum-flow-based algorithm}
We next review the maximum-flow-based exact algorithm for DSP, designed by Goldberg~\cite{goldberg1984finding}.
Strictly speaking, the algorithm we describe is slightly different from Goldberg's one, but the difference is not essential and it is just for the sake of simplicity of presentation. The algorithm maintains upper and lower bounds on the (unknown) optimal value of the problem,
and tightens the bounds step-by-step using binary search, until the current lower bound is guaranteed to be the optimal value of DSP.
To update upper and lower bounds, the algorithm utilizes maximum-flow computation.
As initial upper and lower bounds, $m/2$ and $0$ can be employed, respectively.
Let $\beta \geq 0$ be the midpoint of the upper and lower bounds kept in the current iteration.
For $G=(V,E)$ and $\beta \geq 0$, the algorithm constructs the following edge-weighted directed graph $(U,A,w_\beta)$: $U=V\cup \{s,t\}$, $A=A_s\cup A_E\cup A_t$, where
$A_s=\{(s,v)\mid v\in V\}$, $A_E=\{(u,v), (v,u)\mid \{u,v\}\in E\}$, and $A_t=\{(v,t)\mid v\in V\}$, \rev{and} $w_\beta \colon A\rightarrow \mathbb{R}_+$ \rev{such that}
\begin{align*}
w_\beta(e) =
\begin{cases}
\frac{\deg(v)}{2} &\text{if } e=(s,v)\in A_s,\\
\frac{1}{2} &\text{if } e\in A_E,\\
\beta &\text{if } e\in A_t. 
\end{cases}
\end{align*}
For $v\in V$, $\deg(v)$ is the degree of $v$ in $G$. 
This graph is illustrated in Figure~\ref{fig:digraph}. 
Here we introduce some terminology and notation.
An $s$--$t$ cut of $(U,A,w_\beta)$ is a partition $(X,Y)$ of $U$ (i.e., $X\cup Y=U$ and $X\cap Y=\emptyset$) such that $s\in X$ and $t\in Y$.
The cost of an $s$--$t$ cut $(X,Y)$, denoted by $\text{cost}(X,Y)$, is defined as the sum of weights of edges going from $X$ to $Y$,
i.e., $\mathrm{cost}(X,Y)=\sum_{(u,v)\in A: u\in X,v\in Y}w_\beta(u,v)$.
An $s$--$t$ cut having the minimum cost is called a minimum $s$--$t$ cut.
The following lemma is useful for updating the upper and lower bounds using a minimum $s$--$t$ cut of $(U,A,w_\beta)$:
\begin{figure}[t]
\centering
\includegraphics[scale=1.15]{./digraph.pdf}
\caption{An edge-weighted directed graph $(U,A,w_\beta)$ constructed from $G$ and $\beta$.}
\label{fig:digraph}
\end{figure}



\begin{lemma}\label{lemma1}
Let $(X,Y)$ be an $s$--$t$ cut of the edge-weighted directed graph $(U,A,w_\beta)$,
and $S=X\setminus \{s\}$.
Then it holds that
$\mathrm{cost}(X,Y)=m+\beta |S|-e[S].$
In particular, when $X=\{s\}$, $\mathrm{cost}(X,Y)=m$ holds.
\end{lemma}
\begin{proof}
Any edge from $X$ to $Y$ is contained in exactly one of the following sets:
$\{(s,v)\in A_s\mid v\in V\setminus S\},\
\{(u,v)\in A_E\mid u\in S,\, v\in V\setminus S\},\
\{(v,t)\in A_t\mid v\in S\}$.
Therefore, the cost of $(X,Y)$ can be evaluated as follows:
\begin{align*}
\text{cost}(X,Y)
&=\sum_{(s,v)\in A_s:\, v\in V\setminus S}w_\beta(s,v)+\sum_{(u,v)\in A_E:\, u\in S,v\in V\setminus S}w_\beta(u,v)
+\sum_{(v,t)\in A_t:\, v\in S}w_\beta(v,t)\\
&= \sum_{v\in V\setminus S} \frac{\text{deg}(v)}{2}+\frac{|\{{\{u,v\}\in E\mid u\in S,\, v\in V\setminus S}\}|}{2}+\beta|S|\\
&=e[V\setminus S]+|\{{\{u,v\}\in E\mid u\in S,\, v\in V\setminus S}\}|+\beta|S|\\
&=m+\beta|S|-e[S].
\end{align*}
\end{proof}
Let $(X,Y)$ be the minimum $s$--$t$ cut of $(U,A,w_\beta)$ computed by the algorithm.
If $X= \{s\}$ does not hold,
then $S=X\setminus \{s\}$ is a vertex subset that satisfies $\beta|S|-e[S]\leq 0$, i.e., $e[S]/|S|\geq \beta$;
hence, the lower bound on the optimal value can be replaced by $\beta$.
On the other hand, if $X=\{s\}$, then we see that there is no $S\subseteq V$ that satisfies $e[S]/|S|> \beta$;
hence, the upper bound can be replaced by $\beta$.
The following lemma is useful for specifying the termination condition of the algorithm.
\begin{lemma}
Let $G=(V,E)$ be an undirected graph.
For any $S_1,S_2\subseteq V$, if $d(S_1)\neq d(S_2)$, then
$|d(S_1)-d(S_2)|\geq \frac{1}{n(n-1)}$.
\end{lemma}
\begin{proof}
Defining $\Delta=\left| d(S_1)-d(S_2)\right|$, we have
$\Delta=\left| \frac{e[S_1]|S_2|-e[S_2]|S_1|}{|S_1||S_2|}\right|$.
If $|S_1|=|S_2|$ holds, then
$\Delta=\left| \frac{e[S_1]-e[S_2]}{|S_1|}\right|$, implying that $\Delta \geq \frac{1}{n}$.
On the other hand, if $|S_1|\neq |S_2|$, then $|S_1||S_2|\leq n(n-1)$; therefore, $\Delta \geq \frac{1}{n(n-1)}$.
\end{proof}
From this lemma, we see that once the difference between the upper and lower bounds becomes less than $\frac{1}{n(n-1)}$,
the current lower bound attains the optimal value of DSP.
The entire procedure is summarized in Algorithm~\ref{alg:flow}.
The following theorem guarantees the solution quality and time complexity.
\begin{algorithm}[t]
\caption{Maximum-flow-based algorithm}\label{alg:flow}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{\ $G=(V,E)$}
\Output{\ $S\subseteq V$}
$\beta^{(0)}_\text{ub}\leftarrow m/2$, $\beta^{(0)}_\text{lb}\leftarrow 0$, $i\leftarrow 0$\;
\While{$\beta_\mathrm{ub}^{(i)} - \beta_\mathrm{lb}^{(i)}\geq \frac{1}{n(n-1)}$}{
  $\beta^{(i)}\leftarrow \frac{\beta_\text{lb}^{(i)}+\beta_\text{ub}^{(i)}}{2}$\;
  Compute the minimum $s$--$t$ cut $(X^{(i)},Y^{(i)})$ of $(U,A,w_{\beta^{(i)}})$ using maximum-flow computation\;
  \If{$X^{(i)}\neq \{s\}$}{
    $\beta_\text{lb}^{(i+1)}\leftarrow \beta^{(i)}$, $\beta_\text{ub}^{(i+1)}\leftarrow \beta_\text{ub}^{(i)}$\;
  }
  \Else{
    $\beta_\text{lb}^{(i+1)}\leftarrow \beta_\text{lb}^{(i)}$, $\beta_\text{ub}^{(i+1)}\leftarrow \beta^{(i)}$\;
  }
  $i\leftarrow i+1$\;
}
Compute the minimum $s$--$t$ cut $(X,Y)$ of $(U,A,w_{\beta_\text{lb}^{(i)}})$ using maximum-flow computation\;
\Return{$X\setminus \{s\}$}
\end{algorithm}
\begin{theorem}
Algorithm~\ref{alg:flow} returns an optimal solution to DSP in $O(T_\mathrm{Flow}\log n)$ time, where $T_\mathrm{Flow}$ is the time complexity required to compute a minimum $s$--$t$ cut $(X^{(i)},Y^{(i)})$ using maximum-flow computation ($i=0,1,\dots$).
\end{theorem}
\begin{proof}
From the above discussion, we know that the algorithm returns an optimal solution to DSP.
In what follows, we show that the time complexity of the algorithm is given by $O(T_\text{Flow}\log n)$.
The number of iterations $\hat{i}$ of the while-loop is the minimum (nonnegative) integer among $i$'s that satisfy
\begin{align*}
\frac{1}{2^i}(\beta^{(0)}_\text{ub}-\beta^{(0)}_\text{lb})< \frac{1}{n(n-1)}.
\end{align*}
Therefore, we see that
\begin{align*}
\hat{i}=O(\log(mn(n-1)))=O(\log n).
\end{align*}
Obviously, the time complexity of each iteration of the while-loop is dominated by $T_\text{Flow}$.
Thus, we have the theorem.
\end{proof}

For example, employing the maximum-flow computation algorithm by \rev{Cheriyan et} al.~\cite{Cheriyan96},
we can compute a minimum $s$--$t$ cut of $(U,A,w_{\beta^{(i)}})$ in $O(|U|^3/\log|U|)=O(n^3/\log n)$ time.
Therefore, in that case, the time complexity of Algorithm~\ref{alg:flow} becomes $O(n^3)$.

In the context of maximum-flow-based exact algorithms for DSP, we remark that \rev{Gallo et} al.~\cite{gallo1989fast} showed how a single parametric maximum flow computation provides an optimal solution to DSP.
To the best of our knowledge, the fastest algorithm for parametric maximum flow computation is the one given by Hochbaum~\cite{hochbaum2008pseudoflow} and solves DSP in $\bigO\left(mn \log n\right)$ time, which is better than $O(n^3)$ above, when the input graph is sparse, i.e., $m=O(n)$.



\subsubsection{Charikar's LP-based algorithm}
The algorithm first solves an LP and then constructs a vertex subset, using the information of the optimal solution to the LP,
which is guaranteed to be an optimal solution to DSP.
Let us introduce a variable $x_e$ for each $e\in E$ and a variable $y_v$ for each $v\in V$.
The LP used in the algorithm is as follows:
\begin{alignat*}{4}
&\text{maximize} &\quad &\sum_{e\in E}x_e \\
&\text{subject to} &    &x_e\leq y_u,\ x_e\leq y_v  &\quad &(\forall e=\{u,v\}\in E),\\
&                  &    &\sum_{v\in V}y_v=1, \\
&                  &    &x_e\geq 0, \ y_v\geq 0     &      &(\forall e\in E,\, \forall v\in V).
\end{alignat*}
Roughly speaking, the first constraints stipulate that if we take edge $e=\{u,v\}$, we have to take both of the endpoints $u$ and $v$.
The second constraint just standardizes the objective function of DSP.
The following lemma implies that the LP is a continuous relaxation of DSP.
\begin{lemma}\label{lem:OPT_LP}
Let $\mathrm{OPT}_\mathrm{LP}$ be the optimal value of the LP.
For any $S\subseteq V$, it holds that
$\mathrm{OPT}_\mathrm{LP}\geq d(S).$
In particular, letting $S^*\subseteq V$ be an optimal solution to DSP, we have $\mathrm{OPT}_\mathrm{LP}\geq d(S^*)$.
\end{lemma}
\begin{proof}
Take an arbitrary $S\subseteq V$. Construct a solution $(\bm{x},\bm{y})$ of the LP as follows:
\begin{align*}
x_e=
\begin{cases}
\frac{1}{|S|}  &\text{if } e\in E[S],\\
0              &\text{otherwise},
\end{cases}
\quad
y_v=
\begin{cases}
\frac{1}{|S|}  &\text{if } v\in S,\\
0              &\text{otherwise}. 
\end{cases}
\end{align*}
Then it is easy to see that this solution is feasible for the LP.
Moreover, the objective value of $(\bm{x},\bm{y})$ can be evaluated as
\begin{align*}
\sum_{e\in E}x_e=\sum_{e\in E[S]}\frac{1}{|S|}=\frac{e[S]}{|S|}=d(S).
\end{align*}
Therefore, we have $\text{OPT}_\text{LP}\geq d(S)$.
As we took $S\subseteq V$ arbitrarily, this inequality holds even for $S=S^*$.
\end{proof}

The algorithm first solves the LP to obtain an optimal solution $(\bm{x}^*,\bm{y}^*)$.
For $(\bm{x}^*,\bm{y}^*)$ and $r\geq 0$, let $S(r)=\{v\in V\mid y^*_v\geq r\}$.
Also, let $y^*_\text{max} =\max_{v\in V}y^*_v$.
Then the algorithm computes $r^*\in \argmax\{d(S(r))\mid r\in [0,\,y^*_\text{max}]\}$ and just outputs $S(r^*)$.
To find such $r^*$, it suffices to check the value of $d(S(r))$ at $r=y^*_v$ for every $v\in V$,
because $S(r)$ may change only at those points.
The entire procedure is summarized in Algorithm~\ref{alg:LP}.
Clearly, the algorithm runs in polynomial time.
\begin{algorithm}[t]
\caption{LP-based algorithm}\label{alg:LP}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{\ $G=(V,E)$}
\Output{\ $S\subseteq V$}
Compute an optimal solution $(\bm{x}^*,\bm{y}^*)$ to the LP\;
$r^*\in \argmax\{d(S(r))\mid r\in [0,\,y^*_\text{max}]\}$\;
\Return{$S(r^*)$}
\end{algorithm}
\begin{theorem}\label{thm:LP}
Algorithm~\ref{alg:LP} is a polynomial-time exact algorithm for DSP.
\end{theorem}
\begin{proof}
Let $S^*\subseteq V$ be an optimal solution to DSP.
By Lemma~\ref{lem:OPT_LP}, we know that $\text{OPT}_\text{LP}\geq d(S^*)$.
Noting the choice of parameter $r^*$ in Algorithm~\ref{alg:LP},
it suffices to show that there exits $r\in [0,\,y^*_\text{max}]$ such that
\begin{align*}
d(S(r))=\frac{e[S(r)]}{|S(r)|}\geq \text{OPT}_\text{LP}.
\end{align*}

Suppose for contradiction that for any $r\in [0,\,y^*_\text{max}]$, it holds that
\begin{align*}
d(S(r))=\frac{e[S(r)]}{|S(r)|}< \text{OPT}_\text{LP}.
\end{align*}
Then we have
\begin{align}\label{ineq:dense_suppose_bar}
\int_0^{y^*_\text{max}}e[S(r)]\,\text{d}r<\text{OPT}_\text{LP}\int_0^{y^*_\text{max}}|S(r)|\,\text{d}r.
\end{align}
Define indicator functions $X_e\colon [0,\,y^*_\text{max}]\rightarrow \{0,1\}$ ($e=\{u,v\}\in E$)
and $Y_v\colon [0,\,y^*_\text{max}]\rightarrow \{0,1\}$ ($v\in V$) as follows:
\begin{align*}
X_e(r)=
\begin{cases}
1  &\text{if } r\leq y^*_u \text{ and } r\leq y^*_v,\\
0  &\text{otherwise},
\end{cases}\quad
Y_v(r)=
\begin{cases}
1  &\text{if } r\leq y^*_v,\\
0  &\text{otherwise}.
\end{cases}
\end{align*}
From the optimality of $(\bm{x}^*,\bm{y}^*)$, we have $x^*_e=\min\{y^*_u,\, y^*_v\}$ for every $e=\{u,v\}\in E$,
and therefore
\begin{align}\label{eq:dense_indicator_x}
\int_0^{y^*_\text{max}}e[S(r)]\,\text{d}r&=\int_0^{y^*_\text{max}}\left(\sum_{e\in E}X_e(r)\right)\text{d}r\nonumber \\
&=\sum_{e\in E}\int_0^{y^*_\text{max}} X_e(r)\,\text{d}r
=\sum_{e\in E}\min\{y^*_u,\,y^*_v\}
=\sum_{e\in E}x^*_e
=\text{OPT}_\text{LP}.
\end{align}
Similarly we have
\begin{align}\label{eq:dense_indicator_y}
\int_0^{y^*_\text{max}}|S(r)|\,\text{d}r&=\int_0^{y^*_\text{max}}\left(\sum_{v\in V}Y_v(r)\right)\text{d}r
=\sum_{v\in V}\int_0^{y^*_\text{max}} Y_v(r)\,\text{d}r
=\sum_{v\in V}y^*_v=1.
\end{align}
By Inequality~\eqref{ineq:dense_suppose_bar} and Equalities~\eqref{eq:dense_indicator_x} and \eqref{eq:dense_indicator_y}, we obtain
\begin{align*}
\text{OPT}_\text{LP}=
\int_0^{y^*_\text{max}}e[S(r)]\,\text{d}r
<\text{OPT}_\text{LP}\int_0^{y^*_\text{max}}|S(r)|\,\text{d}r
=\text{OPT}_\text{LP},
\end{align*}
a contradiction. This concludes the proof.
\end{proof}

It is worth noting that Balalau et al.~\cite{balalau2015topkoverlapping} proved that $\{v\in V\mid y^*_v >0\}$ is a densest subgraph, based on a more sophisticated analysis. This simplifies the above rounding procedure and reduces its time complexity from $O(m+n\log n)$ to $O(n)$, while the time complexity of Algorithm~\ref{alg:LP} is dominated by that for solving the LP.


\subsubsection{An exact algorithm based on submodular function minimization}
The objective function of DSP  has a strong connection with submodularity property of set functions.
Based on this observation, an exact algorithm for DSP can be designed using submodular function minimization. Let $V$ be a finite set.
A function $f\colon 2^V\rightarrow \mathbb{R}$ is said to be submodular
if $f(X)+f(Y)\geq f(X\cup Y)+f(X\cap Y)$ holds for any $X,Y\subseteq V$.
There is a well-known equivalent definition of the submodularity:
a function $f$ is submodular
if and only if $f$ satisfies $f(X\cup \{v\})-f(X)\geq f(Y\cup \{v\})-f(Y)$ for any $X\subseteq Y$ and $v\in V\setminus Y$,
which is called the diminishing marginal return property.
A function $f\colon 2^V\rightarrow \mathbb{R}$ is said to be supermodular if $-f$ is submodular.
A function $f\colon 2^V\rightarrow \mathbb{R}$ is said to be modular if $f$ is submodular and supermodular.
In the submodular function minimization problem (without any constraint),
given a finite set $V$ and a submodular function $f\colon 2^V\rightarrow \mathbb{R}$,
we are asked to find $S\subseteq V$ that minimizes $f(S)$.
Note that the function $f$ is given as a value oracle, which returns $f(S)$ for a given $S\subseteq V$.
The submodular function minimization problem can be solved exactly in polynomial time~\cite{fujishige2005submodular}.
Strictly speaking, there exist algorithms that solve the problem, using the polynomial number of calls of the value oracle in terms of the size of $V$. 
Indeed, Gr\"otschel et al.~\cite{groetschel1981ellipsoid} gave the first polynomial-time algorithm based on the ellipsoid method.
Later, Iwata et al.~\cite{iwata2001combinatorial} \rev{and Schrijver~\cite{Schrijver00}} designed a combinatorial, strongly-polynomial-time algorithm.
Today some faster algorithms are known, e.g., \cite{orlin2009faster,lee2015faster}.

Let us consider the following problem:
$\text{minimize}\ \ \beta |S| - e[S] \, \text{subject to}\ \ S\subseteq V,$
where $\beta \geq 0$ is a constant.
As the function $\beta |S|$ is modular and the function $-e[S]$ is submodular~\cite{fujishige2005submodular}, the function $\beta |S| - e[S]$ is submodular.
Therefore, this problem is a special case of the submodular function minimization problem.
Let us recall the $i$th iteration of the while-loop of Algorithm~\ref{alg:flow},
where either the upper bound $\beta_\text{ub}^{(i)}$ or the lower bound $\beta_\text{lb}^{(i)}$ is updated
using a minimum $s$--$t$ cut of the edge-weighted directed graph $(U,A,w_\beta^{(i)})$,
where $\beta^{(i)}=(\beta_\text{ub}^{(i)}+\beta_\text{ub}^{(i)})/2$.
We can see that the update can be done through solving the above problem.
Let $S_\text{out}$ be an exact solution to the problem with $\beta=\beta^{(i)}$.
If $S_\text{out}\neq \emptyset$ holds,
then $S_\text{out}$ is a vertex subset that satisfies $\beta^{(i)}|S_\text{out}|-e[S_\text{out}]\leq 0$, i.e., $e[S_\text{out}]/|S_\text{out}|\geq \beta^{(i)}$;
hence, the lower bound $\beta_\text{lb}^{(i)}$ can be replaced by $\beta^{(i)}$.
On the other hand, if $S_\text{out}=\emptyset$, then for any $S\subseteq V$,
$e[S]/|S|\leq \beta^{(i)}$ holds; hence, $\beta_\text{ub}^{(i)}$ can be replaced by $\beta^{(i)}$.


\subsection{Approximation algorithms for DSP}\label{subsec:approx}
Despite the polynomial-time solvability of DSP,
there exists a wide literature studying faster approximation algorithms for DSP.
The first approximation algorithm was identified by Kortsarz and Peleg~\cite{Kortsarz-Peleg94}, \rev{based on the concept of $k$-core}.
\rev{For $G=(V,E)$ and $k\in \mathbb{Z}_{+}$, the $k$-core is the (unique) maximal subgraph in which every vertex has degree at least $k$ \cite{malliaros2020core}.
The authors proved that the $k$-core with the maximum $k$ is a $2$-approximate solution for DSP (straightforward after proving Theorem~\ref{thm:peeling} about the greedy peeling algorithm, which is given below).}
Charikar~\cite{Charikar2000} then studied a greedy algorithm for DSP, later to be known as the greedy peeling algorithm,
and proved that the algorithm is also a $2$-approximation algorithm but runs in $O(m+n\log n)$ time.
It is now widely known that the algorithm can be implemented to run in linear time, \rev{i.e., $O(m+n)$ time,} for unweighted graphs.
About 20 years later, Boob et al.~\cite{boob2020flowless} designed an iterative version of the greedy peeling algorithm,
inspired by the multiplicative weights update method~\cite{arora2012multiplicative}.
Later, Chekuri et al.~\cite{Chekuri2022supermod} proved that the iterative greedy peeling algorithm converges to an optimal solution to DSP.
\rev{In the rest of this section, we review these fundamental results.}

\subsubsection{Greedy peeling algorithm}

Here we review the $2$-approximation algorithm for DSP, presented by Charikar~\cite{Charikar2000}.
\rev{The algorithm works by repeatedly removing from the graph the vertex with the smallest degree, and saving the remaining vertices as a potential solution. The process continues until only one node remains in the graph. Finally, it outputs the subset of vertices with the highest density among the recorded candidate solutions.}
The pseudocode is given in Algorithm~\ref{alg:peeling}, where for $S\subseteq V$ and $v\in S$, $\deg_S(v)$ denotes the degree of $v$ in $G[S]$.
\begin{algorithm}[t]
\caption{Greedy peeling algorithm}\label{alg:peeling}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{\ $G=(V,E)$}
\Output{\ $S\subseteq V$}
$S_{n}\leftarrow V$, $i\leftarrow n$\;
\While{$i>1$}{
  $v_\text{min}\in \argmin\{\text{deg}_{S_i}(v)\mid v\in S_i\}$\;
  $S_{i-1}\leftarrow S_i\setminus \{v_\text{min}\}$\;
  $i\leftarrow i-1$\;
}
$S_\mathrm{max} \in \argmax\{d(S)\mid S\in \{S_1,\dots, S_{n}\}\}$\;
\Return{$S_\mathrm{max}$}
\end{algorithm}

\begin{theorem}\label{thm:peeling}
Algorithm~\ref{alg:peeling} is a $2$-approximation algorithm for DSP. Moreover, the algorithm can be implemented to run in linear time.
\end{theorem}
\begin{proof}
Let $S^*\subseteq V$ be an optimal solution to DSP.
From the optimality of $S^*$, for any $v\in S^*$, we have
\begin{align*}
d(S^*)=\frac{e[S^*]}{|S^*|}\geq \frac{e[S^*\setminus \{v\}]}{|S^*|-1}=d(S^*\setminus \{v\}).
\end{align*}
Transforming the above inequality using $e[S^*\setminus \{v\}]=e[S^*]-\deg_{S^*}(v)$, we have that for any $v\in S^*$,
\begin{align}\label{ineq:dense_optimality}
\deg_{S^*}(v)\geq d(S^*).
\end{align}

Let $v^*$ be the vertex that is contained in $S^*$ and removed first by Algorithm~\ref{alg:peeling}.
Let $S'\subseteq V$ be the vertex subset kept just before removing $v^*$ in Algorithm~\ref{alg:peeling}.
Then the density of $S'$ can be evaluated as
\begin{align}\label{ineq:peeling_LB}
d(S')=\frac{\frac{1}{2}\sum_{v\in S'}\deg_{S'}(v)}{|S'|}
\geq \frac{\frac{1}{2}|S'|\deg_{S'}(v^*)}{|S'|}
\geq \frac{1}{2}\deg_{S^*}(v^*)
\geq \frac{1}{2}d(S^*),
\end{align}
where the first inequality follows from the greedy choice of $v^*\in S'$,
the second inequality follows from $S'\supseteq S^*$,
and the last inequality follows from Inequality~\eqref{ineq:dense_optimality}.
Noticing that $S'$ is one of the candidate subsets of the output, we have the approximation ratio of $2$, as desired.

Next we show that the algorithm can be implemented to run in linear time.
For each (possible) degree $d=0,\dots, n-1$, the vertices having degree $d$ are kept in a doubly-linked list.
In the very first iteration, the algorithm scans the lists with increasing order of $d$ until it finds a vertex.
Once a vertex is found, the algorithm removes the vertex from the list, and moves each neighbor of the vertex to the one lower list.
Owing to the structure of doubly-linked lists, this entire operation can be done in $O(\deg(v))$ time.
Moving to the next iteration, it suffices to go back to the one lower list because no vertex can exist in lower levels than that,
implying that in the entire algorithm, the number of moves from one list to another is bounded by $O(n)$.
Therefore, the above operations can be conducted in $O(m+n)$ time.
Note that we do not need to compute the density of a vertex subset from scratch in each iteration.
It suffices to compute $d(V)$ in the very first iteration and then compute the density of a current vertex subset based on the difference:
in each iteration, the numerator decreases by the degree of the removed vertex in the graph at hand and the denominator decreases by 1.
This concludes the proof.
\end{proof}


Here we demonstrate that the $k$-core with the maximum $k$ is also a $2$-approximate solution for DSP. 
This can be shown easily using Inequality~\eqref{ineq:dense_optimality}.
Let $k^*$ be the maximum value among $k$'s such that there exists a nonempty $k$-core. 
Let $S^*_\text{core}$ be (the vertex subset of) the $k^*$-core.
By the definition of $k^*$-core, we have $\min_{v\in S^*_\text{core}}\deg_{S^*_\text{core}}(v)=k^* \geq \deg_{S^*}(v^*)$,
where $v^*$ is that defined in the proof of Theorem~\ref{thm:peeling}.
Then, using Inequality~\eqref{ineq:dense_optimality}, we have
\begin{align*}
d(S^*_\text{core})
\geq \frac{1}{2}k^*
\geq \frac{1}{2}\deg_{S^*}(v^*)
\geq \frac{1}{2}d(S^*),
\end{align*}
as desired. 


In practice, Algorithm~\ref{alg:peeling} rarely outputs a vertex subset that has an objective value of almost half the optimal value,
which means that the algorithm has a better empirical approximation ratio.
However, Gudapati et al.~\cite{gudapati2021greedy} proved that the approximation ratio of $2$ is tight,
that is, there exists no constant $\alpha < 2$ such that the algorithm is an $\alpha$-approximation algorithm.

It should be remarked that Algorithm~\ref{alg:peeling} can be seen as a primal-dual algorithm for DSP, which also gives a proof of $2$-approximation.
Let us consider the dual of the LP (used in Algorithm~\ref{alg:LP}):
\begin{alignat*}{4}
&\text{minimize}   &\quad &t \\
&\text{subject to} &    &t\geq \sum_{e=\{u,v\}\in E} z_{e,v}      &\quad      &(\forall v\in V),\\
&                  &    &z_{e,u} + z_{e,v} \geq 1 &      &(\forall e=\{u,v\}\in E),\\
&                  &    &z_{e,u}, z_{e,v}\geq 0    &      &(\forall e=\{u,v\}\in E).
\end{alignat*}
The above dual LP can be understood as follows:
each edge $e=\{u,v\}$ has cost $1$ and we need to distribute it to the endpoints $u,v$ so as to minimize the maximum cost over vertices.
Recall that in each iteration of Algorithm~\ref{alg:peeling}, we specify the minimum degree vertex $v_\text{min}$ in the graph at hand and then remove it.
If we assign all the costs of the incident edges of $v_\text{min}$ to $v_\text{min}$ when removing it, we can get a feasible solution for the dual LP.
Let $t^*$ be the objective value of the solution (in terms of the dual LP).
Let us focus on the iteration where the assignment of the maximum cost $t^*$ is carried out.
Then the density of the vertex subset kept just before the assignment is lower bounded by $t^*/2$, because all vertices have degree at least $t^*$.
By the LP's (weak) duality theorem, $t^*$ is lower bounded by the optimal value of the primal LP, thus by the optimal value of DSP.
Therefore, we again see that Algorithm~\ref{alg:peeling} is a $2$-approximation algorithm for DSP.





\subsubsection{Iterative greedy peeling algorithm and beyond}\label{subsubsec:iterative_peeling}

Boob et al.~\cite{boob2020flowless} proposed {\sc Greedy++} for DSP,
an algorithm inspired by the multiplicative weights update~\cite{arora2012multiplicative}.
Let $T\in \mathbb{Z}_{+}$.
The algorithm runs Charikar's greedy peeling algorithm for $T$ times
while updating the priority of each vertex using the information of the past iterations.
The pseudocode is given in \refalg{peeling++}.
Note that each iteration of {\sc Greedy++} requires $\bigO(m+n\log n)$ time. 
The authors empirically showed that the algorithm tends to converge to the optimum
and conjectured its convergence to optimality.
\begin{algorithm}[t]
\caption{\textsf{Greedy++}}\label{alg:peeling++}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{\ $G=(V,E)$, $T\in \mathbb{Z}_{>0}$}
\Output{\ $S\subseteq V$}
\lForEach{$v \in V$} {$\ell_v \leftarrow 0$}
\For{$t =1,2,\dots,T$}{
  $S^t_{n}\leftarrow V$, $i\leftarrow n$\;
  \While{$i> 1$}{
    $v_\text{min}\in \argmin\{\text{deg}_{S^t_i}(v) + \ell_v \mid v\in S^t_i\}$\;
    $\ell_{v_\text{min}} \leftarrow \ell_{v_\text{min}} + \text{deg}_{S^t_i}(v_\text{min})$\;
    $S^t_{i-1}\leftarrow S^t_i\setminus \{v_\text{min}\}$\;
    $i\leftarrow i-1$\;
  }
  $t\leftarrow t+1$\;
}
$S_\mathrm{max} \in \argmax\{d(S)\mid S\in \{S^1_n,\dots, S^1_1,S^2_n,\dots, S^2_1,\dots, S^T_n,\dots, S^T_1\}\}$\;
\Return{$S_\mathrm{max}$}
\end{algorithm}

Chekuri et al.~\cite{Chekuri2022supermod} proved the conjecture of Boob et al.~\cite{boob2020flowless},
showing that {\sc Greedy++} converges to a solution with an approximation ratio arbitrarily close to $1$
and that it naturally extends to a broad class of supermodular functions (i.e., normalized, non-negative, and monotone supermodular functions) in the numerator of the objective function.
More in detail, the authors proved that for any $\epsilon >0$,
{\sc Greedy++} provides a
$(1+\epsilon)$-approximate solution for DSP
after
$\bigO\left(\frac{\Delta \log n}{\text{OPT} \epsilon^2} \right)$ iterations,
where $\Delta$ is the maximum degree of a vertex in the graph and $\text{OPT}$ is the optimal value of DSP. 
\rev{Harb et al.~\cite{harb2023convergence} recently established the convergence of {\sc Greedy++} to the so-called optimal dense decomposition vector.}
Fazzone et al.~\cite{Fazzone2022} modified {\sc Greedy++} to have a quantitative certificate of the solution quality provided by the algorithm at each iteration.
Thanks to this, the authors equipped {\sc Greedy++} with a practical device
that allows termination whenever a solution with a user-specified approximation ratio is found, making the algorithm suited for practical purposes.
Prior to Chekuri et al.~\cite{Chekuri2022supermod},
Boob et al.~\cite{boob2019faster} provided a $(1+\epsilon)$-approximation algorithm for DSP
by reducing DSP to solving $\bigO\left(\log n\right)$ instances of the mixed packing and covering problem.
This algorithm runs in $\tilde\bigO\left(\frac{m \Delta}{\epsilon} \right)$ time\footnote{$\poly\log n$ factors are hidden in the $\tilde\bigO$ notation.}.
Chekuri et al.~\cite{Chekuri2022supermod} designed another $(1+\epsilon)$-approximation algorithm running in
$\tilde\bigO\left(\frac{m}{\epsilon} \right)$ time by approximating maximum flow.
Very recently, Harb et al.~\cite{harb2022faster} proposed a new iterative algorithm,
which provides an $\epsilon$-additive approximate solution for DSP in
$\bigO\left(\frac{\sqrt{m \Delta}}{\epsilon} \right)$ iterations,
where each iteration requires $\bigO\left(m\right)$ time and admits some level of parallelization.
The authors also provided a different peeling technique called fractional peeling, with theoretical guarantees and good empirical performance.



