
Urban-level human motion capture is attracting more and more attention, which targets at acquiring consecutive fine-grained human pose representations, such as 3D skeletons and parametric mesh models, with accurate global locations in the physical world. It is essential for human action recognition, social-behavioral analysis, and scene perception and further benefits many downstream applications, including Augmented/Virtual Reality, simulation, autonomous driving, smart city, sociology, etc. However, capturing extra large-scale dynamic scenes and annotating detailed 3D representations for humans with diverse poses is not trivial. 







Over the past decades, a large number of datasets and benchmarks have been proposed and have greatly promoted the research in 3D human pose estimation (HPE). They can be divided into two main categories according to the capture environment. The first class usually leverages marker-based systems~\cite{ionescu2013human3, sigal2010humaneva,mahmood2019amass}, cameras~\cite{yi2022human, huang2022capturing,yi2022mime}, or RGB-D sensors~\cite{hassan2019prox, SiweiZhang2021EgoBodyHB} to capture human local poses in constrained environments. However, the optical system is sensitive to light and lacks depth information, making it unstable in outdoor scenes and difficult to provide global translations, and the RGB-D sensor has limited range and could not work outdoors. The second class~\cite{Marcard_2018_ECCV, roetenberg2009xsens} attempts to take advantage of body-mounted IMUs to capture occlusion-free 3D poses in free environments. However, IMUs suffer from severe drift for long-term capturing, resulting in misalignments with the human body. Then, some methods exploit additional sensors, such as RGB camera~\cite{kaichi2020resolving}, RGB-D camera~\cite{xu2017flycap, TrumbleBMVC17,zherong2018}, or LiDAR~\cite{li2022lidarcap} to alleviate the problem and make obvious improvement. However, they all focus on HPE without considering the scene constraints, which are limited for reconstructing human-scene integrated digital urban and human-scene natural interactions.







To capture human pose and related static scenes simultaneously, some studies use wearable IMUs and body-mounted camera~\cite{guzov2021human} or LiDAR~\cite{Dai_2022_CVPR} to register the human in large real scenarios and they are promising for capturing human-involved real-world scenes. However, human pose and scene are decoupled in these works due to the ego view, where auxiliary visual sensors are used for collecting the scene data while IMUs are utilized for obtaining the 3D pose. Different from them, we propose a novel setting for human-scene capture with wearable IMUs and global-view LiDAR and camera, which can provide multi-modal data for more accurate 3D HPE.



In this paper, we propose a huge scene-aware dataset for sequential human pose estimation in urban environments, named SLOPER4D. To our knowledge, it is the first urban-level 3D HPE dataset with multi-modal capture data, including calibrated and synchronized IMU measurements, LiDAR point clouds, and images for each subject. Moreover, the dataset provides rich annotations, including 3D poses, SMPL~\cite{smpl2015loper} models and locations in the world coordinate system, 2D poses and bounding boxes in the image coordinate system, and reconstructed 3D scene mesh. In particular, we propose a joint optimization method for obtaining accurate and natural human motion representations by utilizing multi-sensor complementation and scene constraints, which also benefit global localization and camera calibration in the dynamic acquisition process. Furthermore, SLOPER4D consists of over \numberseq sequences in \numberscene scenes, including library, commercial street, coastal runway, football field, landscape garden, etc., with 2k$\sim$13k $m^2$ area size and $200\sim1,300m$ trajectory length for each sequence. By providing multi-modal capture data and diverse human-scene-related annotations, \TITLE~opens a new door to benchmark urban-level HPE.








{We conduct extensive experiments to show the superiority of our joint optimization approach for acquiring high-quality 3D pose annotations.} Additionally, based on our proposed new dataset, we benchmark two critical tasks: camera-based 3D HPE and LiDAR-based 3D HPE, as well as provide benchmarks for GHPE. 




Our contributions are summarized as follows: 
\vspace{-2mm}
\begin{itemize}

    \item We propose the first large-scale urban-level human pose dataset with multi-modal capture data and rich human-scene annotations.



    \item We propose an effective joint optimization method for acquiring accurate human motions in both local and global by integrating LiDAR SLAM results, IMU poses, and scene constraints.


    \item We benchmark two HPE tasks as well as a GHPE task on SLOPER4D, demonstrating its potential of promoting urban-level 3D HPE research.




\end{itemize}