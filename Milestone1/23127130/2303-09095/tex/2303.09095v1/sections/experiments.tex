
 
\begin{figure}[!htb]
    \centering
     \includegraphics[width=0.98\linewidth]{figures/eval_opt.pdf}
     \vspace{-1mm}
     \caption{
     Comparison between our optimization results (\textbf{\textcolor[rgb]{0.937,0.686,0.69}{red SMPL}}) and the ICP results (\textcolor[RGB]{141,202,108}{\textbf{green SMPL)}}. It shows the red SMPL aligns better with the \textcolor[rgb]{0.133, 0.792, 0.796}{\textbf{cyan human points}} than the green SMPL.}
     \label{fig:eval_opt}

    \vspace{-3mm}
\end{figure}


\begin{figure}[!htb]
    \centering
     \includegraphics[width=0.98\linewidth]{figures/eval_cali.pdf}
     \vspace{-3mm}
     \caption{The comparison before (\textbf{left}) and after (\textbf{right}) extrinsic calibration optimization. The point clouds (upper) and SMPL (lower) are projected onto the image.}
     \label{fig:eval_cali}

    \vspace{-4mm}
\end{figure}

 In this section, we first evaluate \TITLE~Dataset qualitatively, indicating that our dataset is solid enough to benchmark new tasks. Then we perform a cross-dataset evaluation to further assess our dataset's novelty on two tasks: LiDAR-based 3D HPE and camera-based 3D HPE. Finally, we introduce the new benchmark, GHPE, and perform experiments on GLAMR. More quantitative evaluations and experiments are in the supplementary material.

\PAR{Training/Test splits.} We split our data into training and test sets for LiDAR/Camera-based pose estimation. The training set of \TITLE~ contains eleven sequences of data with a total of 80k LiDAR frames and corresponding RGB frames. The test set has four sequences of data with around 20k LiDAR frames and corresponding RGB frames.

For global pose estimation, we select three challenging scenarios for evaluation. The first one is a single-person football training scenario with highly dynamic motions. The second one is running along a coastal runway. The third one is a garden tour involving daily motions.

\PAR{Evaluation metrics.}
For 3D HPE, we employ Mean per joint position error (MPJPE) and Procrustes-aligned MPJPE (PA-MPJPE) for evaluation. MPJPE is the mean euclidean distance between the ground-truth and predicted joints. PA-MPJPE first aligns the predicted joints to the ground-truth joints by carrying out rigid transformation based on Procrustes analysis and then calculates MPJPE. 
For global trajectory evaluation, we utilize Absolute Trajectory Error (ATE) and the Relative Pose (the pose refers to orientation here) Error (RPE) in visual SLAM systems \cite{grupp2017evo}, where the ATE is well-suited for measuring the global localization and, in contrast, the RPE is suitable for measuring the system's drift, for example, the drift per second. 
Global MPJPE (G-MPJPE) is MPJPE calculated by placing the SMPL model in the global coordinates.

\PAR{Qualitative evaluation.} For the human pose qualitative evaluation, we project the SMPL to the image and visualize the 3D human with corresponding LiDAR points in 3D space (shown in \cref{fig:dataset}). The results demonstrate that the 3D human mesh aligns well with 3D environments and 2D images. As a large-scale urban-level human pose dataset, \TITLE~ provides multi-modal capture data and rich human-scene annotations, as well as diverse challenging human activities in large scenes.
To evaluate our optimization method, we first compare our method with the results from ICP. As shown in \cref{fig:eval_opt}, the scene-aware constraints and human mesh-to-points constraint efficiently optimize the local poses, global translation, and even the orientation error from IMU.
To show the effectiveness of the camera extrinsic optimization,  we report the results in \cref{fig:eval_cali}. The 2D projecting error was visually lowered after optimization.

