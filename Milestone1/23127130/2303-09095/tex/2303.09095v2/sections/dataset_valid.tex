\subsection{Cross-Dataset Evaluation}

\begin{table}[!htb]
    \centering
    \footnotesize

\begin{tabular}{l|rr|rr}
    \hline
    \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\diagbox{Train\quad}{Test}\end{tabular}} & \multicolumn{2}{c|}{LH26M } & \multicolumn{2}{c}{Ours} \\ 

        & MPJPE  & PA-MPJPE & MPJPE  & PA-MPJPE \\ 
        \hline
    LH26M      & \textbf{79.3} & \textbf{67.0} & 228.7 & 149.9 \\
    Ours       & 212.3 & 128.3  & 86.1  & 65.1 \\
    LH26M + Ours & 85.5  & 72.0   & \textbf{79.2}  & \textbf{60.1} \\
    \hline

    \multicolumn{5}{c}{(a) LiDAR-based 3D pose estimation with LiDARCap~\cite{li2022lidarcap}.} \\
    \multicolumn{5}{c}{} \\

    \hline
    \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\diagbox{Train\quad}{Test}\end{tabular}} & \multicolumn{2}{c|}{3DPW} & \multicolumn{2}{c}{Ours} \\ 

        & MPJPE & PA-MPJPE & MPJPE  & PA-MPJPE \\ \hline
    VIBE~\cite{kocabas2020vibe} & 93.5  & 56.5 & 102.5 & 66.2 \\
    Ours + AMASS & 124.3 & 66.8 & \textbf{86.6}  & \textbf{52.4} \\
     w. 3DPW     & \textbf{83.0} & \textbf{52.0} & 90.0  & 58.3 \\

    \hline
    HbryIK~\cite{li2021hybrik}  & 88.7  & 49.3  & 104.9 & 57.0          \\
    w. Ours   & 87.3 & 49.2 & 67.6 & 44.2    \\
    w. 3DPW   & \textbf{71.3} & \textbf{41.8} & 75.8 & 50.0 \\
    w. 3DPW + Ours & 76.4 & 46.7 & \textbf{66.2} & \textbf{42.8}  \\


    \hline
    \multicolumn{5}{c}{(b)  Camera-based 3D pose estimation.} \\
    \end{tabular}%
    \vspace{-1mm}
    \caption{Cross-dataset evaluation results with different modalities. The LH26M in (a) refers to LiDARHuman26M dataset from LiDARcap. VIBE is pre-trained on AMASS~\cite{mahmood2019amass}, MPI-INF-3DHP~\cite{mono-3dhp2017} InstaVariety~\cite{humanMotionKZFM19}, PoseTrack~\cite{andriluka2018posetrack}, PennAction~\cite{zhang2013actemes}. HbryIK is pre-trained on H36M, MPI-INF-3DHP, MSCOCO~\cite{lin2014microsoft}}
    \label{tab:cross}
    \vspace{-3mm}
 \end{table}


 
We evaluate root-relative 3D human pose estimation with different modalities, namely the LiDAR and the camera. 3DPW is an in-the-wild human motion dataset that is most related to us. With VIBE, we cross-evaluated our dataset's camera modal by using 3DPW.
LiDARHuman26M is a lidar-based dataset for long-range human pose estimation. We can cross-evaluate our dataset's LiDAR modal with it.
\cref{tab:cross}(a) shows the evaluation results on LiDAR-based 3D pose estimation task and \cref{tab:cross}(b) shows the results on camera-based 3D pose estimation. Taking the results from \cref{tab:cross}(a), for example, when the model is trained from another dataset only,  the errors are the largest.
But the error will be further reduced by around 60\% when training on LiDARHuman26M and our dataset together. 
It suggests a domain gap exists between different LiDAR sensors, and both datasets complement each other.
The results of another task show that the pre-trained VIBE model generalizes better on 3DPW than on our dataset. But the error on 3DPW increases when finetuned on our dataset, while the error decreases on our dataset. This suggests that the pre-trained model complements \TITLE~better than the opposite. Comparing the results across different modalities, the error on our dataset from the method trained on mixed LiDAR point cloud datasets is 13\% lower than the method trained on the images.

\subsection{Benchmark on Global Human Pose Estimation}

\begin{table}[htb]
    \centering
    \footnotesize
    \vspace{-3mm}
\begin{tabular}{llrrrr}
    \toprule
    Scene & Metric & \multicolumn{1}{l}{RMSE $\downarrow$} & \multicolumn{1}{l}{$mean$} & \multicolumn{1}{l}{$std.$} & \multicolumn{1}{l}{$max$} \\
    \midrule
    Football & ATE & 3.26 & 2.85 & 1.58 & 11.83 \\
    Running001 & ATE & 29.48 & 25.55 & 14.72 & 56.07 \\
    Garden001 & ATE & \textbf{2.86} & 2.57 & 1.26 & 6.55 \\
    \midrule
    Football & RPE & 0.08 & 0.06 & 0.05 & 1.34 \\
    Running001 & RPE & 0.40 & 0.35 & 0.19 & 1.04 \\
    Garden001 & RPE & \textbf{0.06} & 0.04 & 0.04 & 0.71 \\
    \bottomrule
    \end{tabular}%
    \vspace{-2mm}

    \caption{Global trajectory evaluation of GLAMR. Unit: $m$.}
    \vspace{-3mm}

    \label{tab:ghpe}
\end{table}

\begin{table}[htb]
    \centering
    \footnotesize
    \vspace{-3mm}
\begin{tabular}{lrrrr}
    \toprule
    Scene & \multicolumn{1}{l}{Scale} & \multicolumn{1}{l}{MPJPE $\downarrow$} & \multicolumn{1}{l}{PA-MPJPE $\downarrow$} & \multicolumn{1}{l}{G-MPJPE $\downarrow$} \\
    \midrule
    Football & 11.83 & 264.6 & 118.5 & 5268.7 \\
    Running001 & 56.07 & 652.1 & 119.6 & 32329.3 \\
    Garden001 & 6.55 & \textbf{139.4} & \textbf{86.3} & \textbf{4407.0} \\
    \bottomrule
    \end{tabular}%
    \vspace{-2mm}
    
    \caption{GHPE results from GLAMR. Unit: $mm$.}
    \vspace{-3mm}
    \label{tab:ghpe2}
\end{table}


\begin{figure}[!htb]
    \centering
     \includegraphics[width=0.98\linewidth]{figures/glamr.pdf}
     \vspace{-2mm}
     \caption{The ATE error mapped on the GT trajectory. The color represents the error according to the color bar.}
     \label{fig:glamr}

    \vspace{-4mm}
\end{figure}












In this subsection, we benchmark the GHPE task of GLAMR~\cite{yuan2022glamr} on \TITLE. GLAMR is a global occlusion-aware method for 3D global human mesh recovery from dynamic monocular cameras. For the scale uncertainty of the monocular camera, we compute the affine matrix from the estimated trajectory to the ground truth trajectory and rotate, translate and scale the estimated trajectory before error computation.

\cref{tab:ghpe} reports the global trajectory error with ATE and RPE, \cref{tab:ghpe2} reports the global human pose metric, and \cref{fig:glamr} shows the ATE error mapped on GT trajectory.
Comparing the results on the three scenes, the \textit{football} and \textit{Garden001} have a significantly lower RPE in the global scene.
In comparison, GLAMR performs the worst on the running scene, with an ATE's RMSE of 29.48 m. This scene has the largest area size and the highest human pace. GLAMR achieves a low PA-MPJPE of 86.3mm on \textit{Garden001}, a sequence with daily walking and visiting motions. It's the first time that we have tested the GPHE on such large outdoor scenes. GLAMR achieves relatively better results on daily human motion while performing worse on high-dynamic activities in the wild. The interesting point is that the trajectory tendency is pretty similar to the reference, even in dynamic football training motions, which demonstrates the ability of GLAMR to be a baseline. It is expected that more research will focus on GHPE in real-world interactive scenarios, and the experiments show our \TITLE's potential to promote urban-level GHPE research.
















