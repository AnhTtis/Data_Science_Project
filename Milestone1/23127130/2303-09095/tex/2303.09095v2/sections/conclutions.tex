
\PAR{Limitations.} 
Firstly, SLOPER4D is limited to single-person capture though it perceives multiple-person data. Secondly, the camera and LiDAR are not synchronized online, causing tedious offline work if the camera loses frames even with a low time offset (\textless50 $ms$). Finally, texture information from the camera is not fully exploited for color and texture reconstruction of scenes and humans.
In our future work, we will propose an online synchronization algorithm and extend our work to multiple-person capturing. 



\PAR{Conclusions.} 
We propose the first large-scale urban-level human pose dataset with multi-modal capture data and rich human-scene annotations. Based on our proposed new dataset, we benchmark two critical tasks, camera-based 3D HPE and LiDAR-based 3D HPE. SLOPER4D also benchmarks the GHPE task. The results demonstrate the potential of SLOPER4D in boosting the development of these areas. 

Our work contributes to extending motion capture to large global scenes based on the current methods and datasets. We hope this work will foster future creation and interaction in urban environments. 

{Acknowledgements.} We thank Zhiyong Wang for helping us incorporate FAST-LIO2 into our mapping system.
This work was supported in part by the National Natural Science Foundation of China (No.62171393, No.62206173), 
the Fundamental Research Funds for the Central Universities (No.20720220064), 
the open fund of PDL (WDZC20215250113, 2022-KJWPDL-12),
and FuXiaQuan National Independent Innovation Demonstration Zone Collaborative Innovation Platform (No.3502ZCQXT2021003).
 We also acknowledge support from Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI). 