






















\begin{figure*}[!htb]
    \centering

    \includegraphics[width=0.99\linewidth]{figures/dataset_short.pdf}  
    \vspace{-1mm}
    \caption{The diverse scenes and activities of our dataset. The images in the left column are our reconstructed scenes with human trajectories overlaid on them. The right images are the SMPL meshes overlaid on images / point clouds / scenes.
    }

    \vspace{-3mm}
    \label{fig:dataset}
 \end{figure*}

\subsection{Dataset Comparison}
\label{subsec:data_comapre}

\TITLE~is the first large-scale urban-level human pose dataset with multi-modal capture data and rich human-scene annotations for GHPE.
The head-mounted LiDAR and camera are utilized to simultaneously record the IMU-wearer's activities, including running outside, playing football, visiting, reading, climbing/descending stairs, discussing, borrowing a book, greeting, etc.

The dataset consists of \numberseq sequences from \numberperson human subjects in \numberscene locations. 
There are a total of \framelidar LiDAR frames, \framevideo video frames, and \framemocap IMU-based motion frames captured over a total distance of more than 8 $km$ and an area of up to 13,000 $m^2$. The results of our dataset are shown in \cref{fig:dataset}.
For the captured person, we provide the segmentation of 3D points from LiDAR frames and 2D bounding boxes from images synchronized with LiDAR. We also provide 3D pose annotations with SMPL format. Compared to other datasets~\cref{tab:data_compare}, it is worth mentioning that \TITLE~ provides the 3D scene reconstructions and accurate global translation annotations, allowing us to quantitatively study the scene-aware global pose estimation from both LiDAR and monocular videos. In addition to the dense 3D point cloud map reconstructed from the LiDAR, \TITLE~ provides the high-precision colorful point cloud map from a Terrestrial Laser Scanner (Trimble TX5) for better visualization and map comparison.

