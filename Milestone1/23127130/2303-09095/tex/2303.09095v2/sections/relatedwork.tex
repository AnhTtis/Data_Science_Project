
\input{tables/dataset_compare.tex}

\subsection{3D Human Motion Datasets}

Many datasets have been proposed with different sensors and setups to facilitate the research on 3D human pose estimation.
The H3.6M\cite{ionescu2013human3} is a large-size dataset providing synchronized video with optical-based MoCap in studio environments. To perform markerless capture in different indoor scenes, PROX~\cite{hassan2019prox} uses an RGB-D sensor to scan a single person. EgoBody~\cite{SiweiZhang2021EgoBodyHB} uses multiple RGB-D sensors to pre-scan the room and scan the interacting persons.
LiDARHuman26M~\cite{li2022lidarcap} can capture long-range human motions with static LiDAR and IMUs.
However, they are limited to static environments, human activities, and interactions. 3DPW~\cite{Marcard_2018_ECCV} is the first dataset providing 3D annotations in the wild which uses a single hand-held RGB camera to optimize human pose from IMUs for a certain period of frames. It doesn't provide accurate global translation and 3D scenes. 
HPS~\cite{guzov2021human} reconstructs the human body pose using IMUs and self-localizes it with a head-mounted camera in large 3D scenes, but it heavily relies on the pre-built map. HSC4D~\cite{Dai_2022_CVPR} removes the reliance on the pre-built map and achieves global human motion capture in large scenes. 
However, the camera in HPS and the LiDAR in HSC4D are only used to perceive the environment rather than capture human data. 
With the scene-aware dataset we proposed for global human pose estimation, we can benchmark the 3D HPE in the wild with the LiDAR or camera modalities.

\subsection{Human Localization and Scene Mapping}
Human self-localization aims at estimating the 6-DoF of the human subject in global coordinates. The image-based methods~\cite{kendall2015posenet, radwan2018vlocnet++, wang2020atloc} regress locations directly from a single image with a pre-built map. The scene-specific property makes them hard to generalize to unseen scenes. 
LiDAR is widely used in Simultaneous Localization and Mapping (SLAM)\cite{zhang2014loam, shan2018lego, bosse2012zebedee,lin2020loam} due to its robustness and low drift. To address the drift problem and improve robustness in dynamic motions, RGB cameras~\cite{zhang2015visual, shin2020dvl, seo2019tight}, IMU~\cite{shan2020lio, geneva2018lips, opromolla2016lidar}, or both~\cite{deilamsalehy2016sensor, zuo2019lic, shan2021lvi}, have been integrated with the mapping task.
Most attention has been paid to autonomous driving \cite{kitti} \cite{gskim-2020-mulran} or robotics from the third-person view and they usually do not focus on humans. To achieve self-localization, LiDAR is designed as backpacked \cite{Liu2010IndoorLA, 8736839, Karam2019DesignCA} and hand-held\cite{bauwens2016forest}. To efficiently capture human motions and reconstruct urban scenes, we utilize LiDAR with a built-in IMU (different from the IMUs for motion capture) and propose a pipeline for constructing multi-modal data.
This approach provides accurate information on human motions at both local and global levels, as well as enables mapping in large outdoor environments. 

\subsection{Global 3D Human Pose Estimation}
Most studies recover human meshes in camera coordinate~\cite{zhen2020smap, li2021hybrik} or root-relative poses\cite{kanazawaHMR18,kolotouros2019spin, kocabas2020vibe}.
Recovering global human motions in unconstrained scenes is a challenging topic in computer vision and has gained more and more research interest in recent years.
IMU sensors are widely used in commercial~\cite{roetenberg2007moven, roetenberg2009xsens} and research activities~\cite{vlasic2007practical, SIP, DIP:SIGGRAPHAsia:2018}, and are attached to body limbs to capture human motions in studio-environments.  But it suffers severe drift in the wild. 
Some methods rely on additional RGB~\cite{Marcard2016HumanPE, Marcard_2018_ECCV, dou2016fusion4d, Malleson3DV17} or pre-scan maps~\cite{guzov2021human}, or LiDAR~\cite{Dai_2022_CVPR} to complement the IMUs in large-scale scenes.
Based on human-scene interaction, some work proposed scene-aware solutions using static cameras~\cite{yi2022human, huang2022capturing} to obtain accurate and scene-natural human motions. 4DCapture\cite{MiaoLiu20204DHB} uses a dynamic head-mounted camera to self-localize and reconstruct the scene with the Struct From Motion method. However, it often fails when the illumination changes in the wild. MOVER~\cite{yi2022human} uses a single camera to optimize the 3D objects in a static scene, resulting in better 3D scene reconstruction and human motions.
GLAMR~\cite{yuan2022glamr} uses global trajectory predictions to constrain both human motions and dynamic camera poses, achieving state-of-the-art results on in-the-wild videos.
However, it lacks a benchmark for quantitatively comparing different HPE methods on a global level. To deal with this limitation, we propose \TITLE, the first large-scale urban-level human pose dataset with rich 2D/3D annotations. 


