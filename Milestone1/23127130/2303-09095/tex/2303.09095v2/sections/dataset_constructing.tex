\TITLE~collects scene-aware 4D human data with our body-worn capturing system in urban scenes. In this section, we first introduce the data acquisition in \cref{subsec:data_collecting}, second, we detail the data construction and annotation process in \cref{subsec:data_anno}, then we introduce the global optimization-based \cref{subsec:optimization} method to obtain high quality both 3D/2D data, finally, we compare our dataset in \cref{subsec:data_comapre} with the existing datasets and highlight our novelty.

\subsection{Data Acquisition}
\label{subsec:data_collecting}

\begin{figure}[!htb]
    \centering
     \includegraphics[width=0.98\linewidth]{figures/hardware.pdf}
     \vspace{-1mm}
     \caption{\textbf{Our capturing system's hardware details.} The sensor module includes a LiDAR, a camera, and 17 body-attached IMU sensors. The storage module consists of a NUC11, a receiver, and a battery in the backpack.}
     \label{fig:system_design}
    %  \vspace{-\baselineskip}
    \vspace{-5mm}
\end{figure}

\PAR{Hardware setup.}
As shown in \cref{fig:teaser}, during the data collection procedure, the scanning person follows the performer (IMUs wearer) and scans him with a LiDAR and a camera on the helmet. Additionally,  \cref{fig:system_design} shows the hardware details of our capturing system. Regarding the sensor module, the 128-beams Ouster-os1 LiDAR and the DJI-Action2 wide-range camera are rigidly installed on the helmet. To capture raw human motions, we use Noitom's inertial MoCap product, PN Studio, to attach 17 wireless IMUs to the IMU wearer's body limbs, torso, and head. 
The camera's field of view (FOV) is 116°$\times$84° and the LiDAR's FOV is 360°$\times$45°. To make the performer within the LiDAR's FOV as much as possible, we tilt the LiDAR down around 45°. 
Regarding the storage module, the scanning person's backpack places a wireless IMU data receiver, a 24V battery, and an Intel NUC11. The mini-computer NUC11 stores IMU data from the wireless receiver and point clouds from LiDAR in real time. Videos are stored locally in the camera. The LiDAR and NUC11 are both powered by the battery.

\PAR{Coordinate systems.}
Let's define three coordinate systems: 1) IMU coordinate system \{$I$\}: the origin is at LiDAR wearer's spine base at the starting time, and the $X/Y/Z$ axis is pointing left/upward/forward of the human. 2) LiDAR Coordinate system \{$L$\}: the origin is at the center of the LiDAR, and the $X/Y/Z$ axis is pointing right/forward/upward of the LiDAR. 3) Global/World coordinate system \{$W$\}: the origin is on the floor of the LiDAR wearer's starting position, and the $X/Y/Z$ axis is pointing right/forward/upward of the LiDAR wearer.

\PAR{Calibration.} 
Following the setup in \cite{wen2019toward}, we use a chessboard to calibrate the camera intrinsic $K_{in}$ and introduces a terrestrial laser scanner (TLS) to obtain accurate camera extrinsic parameter, $K_{ex}$. Due to the LiDAR point cloud being too sparse, we manually choose the corresponding points both on the 2D image and the TLS map registered to the point cloud, and then we solve the perspective-n-point (PnP) problem to obtain $K_{in}$.
For every 3D scene, the calibration $R_{WL}$, which transforms \{$L$\} to \{$W$\} is manually set to make the ground's $z$-axis upward and height to zero for the starting position.
By using singular value decomposition, the calibration $R_{WI}$, which transforms \{$I$\} to \{$W$\}, is calculated through the similarity between IMU trajectory and LiDAR trajectory on the XY plane.
\PAR{Synchronization.}
The synchronization of data from multiple sensors in human subject data is achieved through peak detection.
Before and after the capture, the subject is asked to perform jumps. Then the peak height time in IMU is automatically detected and the peak times in the LiDAR and camera data are manually identified. Finally, all modalities are aligned by the peaks and downsampled to match the LiDAR frame rate of 20 Hz.  

\subsection{Data Processing}
\label{subsec:data_anno}
\PAR{2D pose detection.} 
We use Detectron~\cite{wu2019detectron2} to detect and Deepsort~\cite{wojke2017simple} to track humans in videos. However, the tracking often fails due to the IMUs wearer entering/exiting the field of view or occlusions. To solve this problem, we manually assign the same ID for the tracked person in a video sequence. As for 3D point cloud reference, we project them on images according to the $K_{ex}$. However, due to the jitter brought by dynamic motions, the camera and the LiDAR are not perfectly rigidly connected. 
Thus, $K_{ex}$ will be further optimized in \cref{subsec:optimization}. 

\PAR{LiDAR-inertial localization and mapping.}
The LiDAR-only method often fails in mapping because of the dynamic head rotation and crowded urban environments.
Incorporating an IMU can compensate for motion distortion in a LiDAR scan $p^{L}$ and provide an accurate initial pose. 
Using a LiDAR with an integrated IMU, and by combining Kalman filter-based lidar-inertial odometry\cite{Xu2022FASTLIO2FD} with factor graph-based loop closure optimization\cite{gtsam}\cite{Kim2018ScanCE}, we successfully estimate the ego-motion of LiDAR and build the global consistency 3D scene map with n frame point clouds $P_{1:n}^L=\{p_{1}^{L},\ldots, p_{n}^{L}\}$. 
To provide accurate scene constrain in \cref{subsec:optimization}, we utilize the VDB-Fusion~\cite{vizzo2022sensors} to generate a clean scene mesh $\bm{S}$ that excludes moving objects.

\PAR{IMUs pose estimation.}
We use SMPL~\cite{smpl2015loper} to represent the human body motion ${M^I} = \varPhi (\theta^I, t^I, \beta) \in \mathbb{R}^{6890}$ in IMU coordinate space\{$I$\}, 
where pose parameter $\Theta_{1:n}^I = \{\theta_{1}^{I},\ldots, \theta_{n}^{I}\} \in \mathbb{R}^{72 \times n}$ is composed of pelvis joint's orientation $R_{1:n}^I = \{r_{1}^{I},\ldots, r_{n}^{I}\} \in \mathbb{R}^{3 \times n}$ and the other 23 joints' rotation relative to their parent joint. 
The $T_{1:n}^I = \{t_{1}^{I},\ldots, r_{n}^{I}\} \in \mathbb{R}^{3 \times n}$ is the pelvis joint's translation and $\beta \in \mathbb{R}^{10}$ is a constant value representing a person's body shape. 
$T$ and $\Theta$ are estimated by the commercial MoCap product, while $\beta$ is obtained by using IPNet~\cite{bhatnagar2020ipnet} to fit the scanned model captured by an iPhone13 Promax.
Since the IMU are accurate locally but drift globally, $T^I$ is used for raw calibration of the \{$I$\} to \{$W$\}, and the initial global motion $M = M^W = R^{WI}M^I$ will be further optimized.

\begin{figure*}[!htb]
    \centering
     \includegraphics[width=0.98\linewidth]{figures/method.pdf}
     \vspace{-1mm}
     \caption{\textbf{The pipeline of the dataset construction.} The capturing system simultaneously collects multimodal data, including LiDAR, camera, and IMU data. Then they are further processed. A joint optimization approach with multiple loss terms is then employed to optimize motion locally and globally. As a result, we obtain rich 2D/3D annotations with accurate global motion and scene information.
     }
     \vspace{-3mm}

     \label{fig:method}
\end{figure*}

\subsection{Data Optimization}
\label{subsec:optimization}
\input{sections/jointopt.tex}
