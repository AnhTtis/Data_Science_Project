\section{Introduction}
\label{sec:introduction}

A \emph{stochastic bandit} \citep{lai85asymptotically,auer02finitetime,lattimore19bandit} is an online learning problem where a \emph{learning agent} sequentially interacts with an environment over $n$ rounds. In each round, the agent pulls an \emph{arm} and receives a \emph{stochastic reward}. The mean rewards of the arms are initially unknown and the agent learns them by pulling the arms. Therefore, the agent faces an \emph{exploration-exploitation dilemma} when pulling the arms: \emph{explore}, and learn more about the arms; or \emph{exploit}, and pull the arm with the highest estimated reward. An example of this setting is a recommender system, where the arm is a recommendation and the reward is a click.

Most bandit algorithms assume that the reward variance or it upper bound is known. For instance, the confidence intervals in \ucb \citep{auer02finitetime} are derived under the assumption that the rewards are $[0, 1]$, and hence $\sigma^2$-sub-Gaussian for $\sigma = 0.5$. In Bernoulli \klucb \citep{garivier11klucb} and Thompson sampling (TS) \citep{agrawal12analysis}, tighter confidence intervals are derived for Bernoulli rewards. Specifically, a Bernoulli random variable wither either a low or high mean also has a low variance. In general, the reward variance is hard to specify \citep{audibert09}. While overestimating it is typically safe, this decreases the learning rate of the bandit algorithm and increases regret. On the other hand, when the variance is underestimated, this may lead to linear regret because the algorithm commits to an arm without sufficient evidence.

Since variance misspecification is a major concern, it has been studied extensively. In the cumulative regret setting, \citet{audibert09} proposed an algorithm based on upper confidence bounds (UCBs) and \citet{mukherjee18} proposed an elimination algorithm. In best-arm identification (BAI) \citep{audibert09exploration,bubeck09pure}, several papers studied the fixed-budget \citep{gabillon11multibandit,faella20kernel,SGV20} and fixed-confidence \citep{lu21variancedependent,zhou22approximate,jourdan22dealing} settings with unknown reward variances. All above works studied frequentist algorithms. On the other hand, Bayesian algorithms based on posterior sampling \citep{thompson33likelihood,chapelle11empirical,agrawal12analysis,russo14learning,russo18tutorial,kveton21metathompson,hong22hierarchical} perform well in practice, but learning of reward variances in these algorithms is understudied. The only comprehensive work that we are aware of is \citet{honda14optimality}. While they study TS with unknown reward variances, the algorithm is less general than needed (Exponential posterior on precision rather than Gamma), the regret bound is frequentist and does not show the effect of the prior, and the work is purely theoretical. Our paper departs from this work in all aforementioned aspects.

This paper makes the following contributions. \textbf{(1)} First, we present Thompson sampling for a $K$-armed Gaussian bandit with \emph{known heterogeneous reward variances} and bound its Bayes regret (\cref{sec:bayes_known}). While this is not difficult given prior works, it is a stepping stone for the unknown variances. \textbf{(2)} Second, we propose a Thompson sampling algorithm \varts for a $K$-armed Gaussian bandit with \emph{unknown heterogeneous reward variances} (\cref{alg:bayes_unknown} in \cref{sec:bayes_unknown2}). \varts maintains a joint Gaussian-Gamma posterior for the mean and precision of the rewards of all arms and samples from them in each round. \textbf{(3)} Third, we derive a Bayes regret bound for \varts (\cref{thm:bayes_unknown2} in \cref{sec:bayes_unknown2}). The bound captures the uncertainty of both unknown reward means and variances, and is the first Bayesian regret bound of this kind. The novelty in the analysis is in handling random confidence interval widths due to random reward variances (\cref{sec:unknown_reg}). The result is a regret bound that resembles that with known reward variances, replaced by the corresponding prior-dependent quantities. \textbf{(4)} Finally, we comprehensively evaluate \varts, from Bernoulli to Gaussian bandits. In the Bernoulli bandit, our algorithm is competitive with Bernoulli TS; while it outperforms all baselines in all other settings (\cref{sec:experiments}).


\subsection{Related Work}
\label{sec:rel}

The beginnings of variance-aware adaptive algorithms can be traced to \citet{auer02finitetime}. \citet{auer02finitetime} proposed a variance-aware \ucb, called \ucbnormal, for Gaussian bandits where the reward distribution of arm $i$ is $\cN(\mu_i, \sigma_i^2)$ and $\sigma_i > 0$ is assumed to be known. The $n$-round regret of this algorithm is $O\bigg(\sum_{i: \mu_i < \mu_{a^*}} \frac{\sigma_i^2}{\Delta_i} \log n\bigg)$, where $a^*$ is the arm with the highest mean reward $\mu_i$. The first UCB algorithm for unknown reward variances with an analysis was \ucbv by \citet{audibert09}. The key idea in the algorithm is to design high-probability confidence intervals based on empirical Bernstein bounds. The $n$-round regret of \ucbv is $O\bigg(\sum_{i: \mu_i < \mu_{a^*}} \bigg(\frac{\sigma_i^2}{\Delta_i} + b\bigg) \log n\bigg)$, where $b$ is an upper bound on the absolute value of the rewards. In summary, variance adaptation in \ucbv incurs only a small penalty of $O(b K \log n)$. \citet{mukherjee18} proposed an elimination-based variant of $\ucbv$ that attains the optimal gap-free regret of $O(\sqrt{K n})$, as opposing to the original $O(\sqrt{K n \log n})$. While empirical Bernstein bounds are general, they tend to be conservative in practice. This was observed before by \citet{garivier11klucb} and we observe the same trend in our experiments (\cref{sec:experiments}). Our work can be viewed as a similar development with Thompson sampling. We show that Thompson sampling with unknown reward variances (\cref{sec:bayes_unknown2}) incurs only a slightly higher regret than the one with the known variances (\cref{sec:bayes_known}), by a multiplicative factor. Comparing to \ucbv, the algorithm is highly practical.

A recent work by \citet{zhang21} extended the above results to $d$-dimensional linear bandits \cite{abbasi-yadkori11improved,dani08stochastic,dani08price}, where the arm set can be infinite. Their regret bound is $\tilde O(\text{poly}(d)\sqrt{1 + \sum_{s = 1}^n \sigma_s^2})$, where $\sigma_s^2$ is the unknown reward variance in round $s$. They also extended their result to linear mixture MDPs.

Another related work on variance-sensitive multi-armed bandits is \citet{MV20}. They derive risk-aware regret bounds, where the mean reward is a difference between the weighted mean reward of the arm and its variance. The bounds are frequentist and asymptotic ($n \to \infty$).
