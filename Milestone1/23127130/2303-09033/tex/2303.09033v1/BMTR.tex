\section{Bayesian Multi-Task Regression}
\label{sec:bayesian multi-task regression}

Multi-task Bayesian regression was proposed by \citet{lindley72bayes}. In their work, there is only one prior for all the regression tasks. In other words, there is no meta-prior and the posterior is calculated only for $2$ levels. In our work, we extended this formulation by adding a meta-prior. Now we need to compute the posterior across $3$ levels, which we work out in this section. Recall that all variances in our hierarchical data generation process are known. Formally, we have the following generative model
\begin{equation}
\begin{aligned}
  \theta_0 \;
  &\sim \; \mathcal{N}(\mu_0, \Lambda_0^{-1}) \\
  \theta_{t}|\theta_0 \;
  &\sim \; \mathcal{N}(\theta_0, \Sigma) \qquad\;\, \text{ for } t=1,2,\dots,T \\
  y_{t,i}|\theta_t, x_{t,i} \;
  &\sim \; \mathcal{N}(x_{t,i}^T \theta_t, \sigma^2) \quad \text{ for } i=1,2,\dots,N
\end{aligned}
\end{equation}
For convenience, we group all observations in a matrix-vector form as $X_t = (x_{t,1},\dots,x_{t,n})$ and $y_t = (y_{t,1},\dots,y_{t,n})$. Our goal is to compute $p\left(\theta_0 \middle| X_{1:T}, y_{1:T}\right)$. We begin by trying to recursively compute
\begin{equation}
  p(\theta_0|X_{1:t}, y_{1:t})
  \propto p(\theta_0|X_{1:t-1}, y_{1:t-1}) p(y_{t} | \theta_0, X_{t})
  \label{eq:recurse}
\end{equation}


\subsection{Computing $p(y_{t} | \theta_0, X_{t})$}

Note that, given $\theta_t$ and $X_t$, we have
\begin{equation}
    y_t = X_t\theta_t + \epsilon \qquad \text{ where } \epsilon \sim \mathcal{N}(0, \sigma^2 I)
\end{equation}
Similarly, given $\theta_0$, we have
\begin{equation}
    \theta_t = \theta_0 + \nu \qquad \text{ where } \nu \sim \mathcal{N}(0, \Sigma)
\end{equation}
Combining the above two equations
\begin{equation}
    y_t = X_t\theta_0 + X_t \nu + \epsilon
\end{equation}
From the above, it is clear that $y_t|\theta_0, X_{t}$ is a Gaussian. We can find the parameters of this distribution easily as
\begin{equation}
\begin{aligned}
    \mathbb{E}[y_t|\theta_0, X_t] &= X_t\theta_0\\
    \text{Cov}(y_t|\theta_0, X_t) &= \mathbb{E}[(y_t-X_t\theta_0)(y_t-X_t\theta_0)^T] \\
    &= \mathbb{E}[(X_t \nu + \epsilon)(X_t \nu + \epsilon)^T] \\
    &= \mathbb{E}[(X_t \nu\nu^TX_t^T + \epsilon \nu^T X_t^T + X_t \nu \epsilon^T + \epsilon\epsilon^T)] \\
    &= X_t \underbrace{\mathbb{E}[\nu\nu^T]}_{=\Sigma}X_t^T + \underbrace{\mathbb{E}[\epsilon \nu^T]}_{=0 \text{ as } \epsilon \perp \nu} X_t + X_t \underbrace{\mathbb{E}[\nu \epsilon^T]}_{=0 \text{ as } \epsilon \perp \nu} + \underbrace{\mathbb{E}[\epsilon\epsilon^T]}_{=\sigma^2 I} \\
    &= X_t\Sigma X_t^T + \sigma^2I
\end{aligned}
\end{equation}
Thus we get
\begin{equation}
    y_{t} | \theta_0, X_{t} \; \sim \; \mathcal{N}(X_t\theta_0, \  \sigma^2I + X_t\Sigma X_t^T)
    \label{eq:marginal_final}
\end{equation}


\subsection{Computing $p(\theta_0|X_{1:t}, y_{1:t})$ by induction}

\paragraph{Induction hypothesis:} $\forall t: \theta_0|X_{1:t}, y_{1:t} \; \sim \; \mathcal{N}(\mu_t, \Lambda_t^{-1}) $

\paragraph{Base case $t=0$:} This corresponds to prior as there is no data and thus by definition $\theta_0 \; \sim \; \mathcal{N}(\mu_0, \Lambda_0^{-1})$.

\paragraph{Inductive step} We derive the distribution of $\theta_0|X_{1:t}, y_{1:t}$ given that $\theta_0|X_{1:t-1}, y_{1:t-1} \; \sim \; \mathcal{N}(\mu_{t-1}, \Lambda_{t-1}^{-1})$. Then by using the recurrence from \eqref{eq:recurse} and marginal distribution from \eqref{eq:marginal_final}, we obtain
\begin{equation}
\begin{aligned}
    p(\theta_0|X_{1:t}, y_{1:t}) &\propto p(\theta_0|X_{1:t-1}, y_{1:t-1}) p(y_{t} | \theta_0, X_{t}) \\
    &\propto \exp\left\{ -\frac{1}{2} (\theta_0-\mu_{t-1})^T\Lambda_{t-1}(\theta_0-\mu_{t-1})  \right\}  \exp\left\{ -\frac{1}{2} (y_t-X_t\theta_0)^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}(y_t-X_t\theta_0)  \right\} \\
    &\propto \exp\left\{ -\frac{1}{2} (\theta_0-\mu_{t-1})^T\Lambda_{t-1}(\theta_0-\mu_{t-1})   -\frac{1}{2} (y_t-X_t\theta_0)^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}(y_t-X_t\theta_0)  \right\} \\
    &\propto \exp\left\{ -\frac{1}{2}\theta_0^T\Lambda_{t-1}\theta_0 +\theta_0^T\Lambda_{t-1}\mu_{t-1} - \frac{1}{2} \theta_0^TX_t^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}X_t\theta_0 + \theta_0^TX_t^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}y_t \right\} \\
    &\propto \exp\left\{ -\frac{1}{2}\theta_0^T\underbrace{\left(\Lambda_{t-1} + X_t^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}X_t\right)}_{=\Lambda_t}\theta_0 +\theta_0^T\underbrace{\left(\Lambda_{t-1}\mu_{t-1} + X_t^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}y_t \right)}_{=\Lambda_t\mu_t} \right\} \\
    &\propto \exp\left\{ -\frac{1}{2}\theta_0^T\Lambda_t\theta_0 +\theta_0^T\Lambda_t\mu_t \right\} \\
    &\propto \exp\left\{ -\frac{1}{2}\theta_0^T\Lambda_t\theta_0 +\theta_0^T\Lambda_t\mu_t -\frac{1}{2}\mu_t^T\Lambda_t\mu_t  \right\} \\
    &\propto \exp\left\{ -\frac{1}{2}(\theta_0-\mu_t)^T\Lambda_t(\theta_0-\mu_t) \right\} \\
\end{aligned}
\end{equation}
This completes our proof by induction, as now we have shown that $\theta_0|X_{1:t}, y_{1:t} \; \sim \; \mathcal{N}(\mu_t, \Lambda_t^{-1})$, where
\begin{equation}
\begin{aligned}
    \Lambda_t &= \Lambda_{t-1} + X_t^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}X_t\\
    \mu_t &= \Lambda_t^{-1} \left(\Lambda_{t-1}\mu_{t-1} + X_t^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}y_t \right)
\end{aligned}
\end{equation}
Using these recurrences, the posterior after $T$ tasks is a Gaussian with parameters
\begin{equation}
\begin{aligned}
    \Lambda_T &= \Lambda_0 + \sum_{t=1}^T X_t^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}X_t\\
    \mu_T &= \Lambda_T^{-1} \left( \Lambda_{0}\mu_{0} + \sum_{t=1}^T X_t^T(\sigma^2I + X_t\Sigma X_t^T)^{-1}y_t \right)
\end{aligned}
\end{equation}
In a high-dimensional setting, where $N<K$, the above equation is efficient as it handles matrices of size $N \times N$ at cost $O(N^{\omega})$. In a large-sample setting, where $K<N$, using $N \times N$ matrices can be costly. To reduce the computational cost to $O(K^{\omega})$, we apply the Woodbury matrix identity
\begin{equation}
  \left(A+UCV\right)^{-1}
  = A^{-1}-A^{-1}U\left(C^{-1}+VA^{-1}U\right)^{-1}VA^{-1}
\end{equation}
to $(\sigma^2I + X_t\Sigma X_t^T)^{-1}$. This yields new recurrences
\begin{equation}
\begin{aligned}
    \Lambda_t &= \Lambda_{t-1} + \frac{S_t}{\sigma^{2}} - \frac{S_t}{\sigma^{2}} \left(\Sigma^{-1}+ \frac{S_t}{\sigma^{2}}\right)^{-1} \frac{S_t}{\sigma^{2}}\\
    \mu_t &= \Lambda_t^{-1} \left(\Lambda_{0}\mu_{0} + \sum_{t=1}^T \frac{c_t}{\sigma^{2}} - \frac{S_t}{\sigma^{2}} \left(\Sigma^{-1}+ \frac{S_t}{\sigma^{2}}\right)^{-1} \frac{c_t}{\sigma^{2}} \right)
\end{aligned}
\label{eq:linear bandit posterior}
\end{equation}
where
\begin{equation}
\begin{aligned}
    S_t &= X_t^TX_t\\
    c_t &= X_t^Ty_t
\end{aligned}
\end{equation}
Then final posterior parameters turn out to be
\begin{equation}
\begin{aligned}
    \Lambda_T &= \Lambda_0 + \sum_{t=1}^T \frac{S_t}{\sigma^{2}} - \frac{S_t}{\sigma^{2}} \left(\Sigma^{-1}+ \frac{S_t}{\sigma^{2}}\right)^{-1} \frac{S_t}{\sigma^{2}}\\
    \mu_T &= \Lambda_T^{-1} \left( \Lambda_{0}\mu_{0} + \sum_{t=1}^T \frac{c_t}{\sigma^{2}} - \frac{S_t}{\sigma^{2}} \left(\Sigma^{-1}+ \frac{S_t}{\sigma^{2}}\right)^{-1} \frac{c_t}{\sigma^{2}} \right)
\end{aligned}
\end{equation}
This concludes the proof.
