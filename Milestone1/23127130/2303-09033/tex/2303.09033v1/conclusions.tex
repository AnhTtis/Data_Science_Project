\section{Conclusions and Future Work}
\label{sec:concl}

We study the problem of learning to act in a multi-armed Bayesian bandit with Gaussian rewards and heterogeneous reward variances. As a first step, we present a Thompson sampling algorithm for the setting of known reward variances and bound its regret (\cref{thm:bayes_known}). The bound scales as $\sqrt{n\log n} \sqrt{\sum_{i =1}^K\sigma_i^2 \log(1 + n\frac{\sigma_{0,i}^{2}}{\sigma_i^{2}})}$. Therefore, it goes to zero as the reward variances $\sigma_i^2$ or the prior variances of the mean arm rewards $\sigma_{0, i}^2$ decrease. Our main contribution is \varts, a variance-aware TS algorithm for Gaussian bandits with unknown heterogeneous reward variances. The algorithmic novelty lies in maintaining a joint Gaussian-Gamma posterior for the mean and variance of rewards of each arm. We derive a Bayes regret bound for \varts (\cref{thm:bayes_unknown2}), which scales similarly to the known variance bound. More specifically, it is $\sqrt{n \log n}\sqrt{\sum_{i = 1}^K \frac{\beta_{0,i}}{\alpha_{0,i}-1} \log\Big( 1+\frac{n}{\kappa_{0,i}}\Big)}$, where $\frac{\beta_{0, i}}{\alpha_{0, i} - 1}$ represents a proxy for the reward variance $\sigma_i^2$ and $\kappa_{0, i}^{-1}$ plays the role $\sigma_{0, i}^2/\sigma_i^2$. This bound captures the effect of the prior on learning reward variances and is the first of its kind.

\textbf{Future work.} Our work is in the direction of designing practical and analyzable variance-aware algorithms. It can extended in multiple ways. First, variance-aware bounds for TS in the frequentist setting would be of interest. Our work starts with the Bayesian setting since Bayesian analyses of TS do not require an anti-concentration argument \citep{agrawal12analysis} and show the effect of the prior \citep{kveton21metathompson,hong22hierarchical}. In this sense, they are simpler and more insightful. Another natural direction would be extending our work to contextual problems, such as a linear bandit \citep{dani08stochastic,abbasi-yadkori11improved}. Finally, and more generally for all modern analyses of Bayesian bandit algorithms, it is desirable to have matching lower bounds.
