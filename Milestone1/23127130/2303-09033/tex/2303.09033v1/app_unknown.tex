\section{Additional proofs for \cref{sec:bayes_unknown2}}
\label{app:unknown}

\bayesunknown*

\begin{proof}[Proof of \cref{thm:bayes_unknown2}]
We start by noting that the posterior updates (from results of \cite{murphy2007conjugate}): 

\begin{align*}
    & \mu_{t,i} = \frac{\kappa_{0,i}\mu_{0,i} + N_t(i)\bar{x}_{t,i}}{\kappa_{0,i}+N_t(i)}
    \\
    & \kappa_{t,i} = \kappa_{0,i} + N_t(i)
    \\
    & \alpha_{t,i} = \alpha_{0,i} + N_t(i)/2
    \\
    & \beta_{t,i} = \beta_{0,i} 
    +
    \frac{1}{2}\sum_{t \in [T]}\I{A_t = i}(x_{t,i} - \bar{x}_{t,i})^2
    +
    \frac{\kappa_{0,i} N_t(i) (\bar{x}_{t,i} - \mu_{0,i})^2 }{2(\kappa_{0,i} + N_t(i))},
\end{align*}
where $N_t(i)= \sum_{s = 1}^{t-1} \I{A_t = i}$ denotes the number of pulls of arm $i$ up to round $t$, and $\bar x_{t,i}= \frac{1}{N_t(i)}\sum_{s = 1}^{t-1} \I{A_t = i}x_{t,i}$ being the averaged empirical mean reward of arm-$i$ at round $t$. 
%
Thus we have: 

\[
\mu_{i} - \hat \mu_{t,i}  \mid (\kappa_t,\lambda_t) \sim \cN(0 ,(\kappa_{t,i} \lambda_{t,i})^{-1},
\]
where $\lambda_{t,i} \sim \text{Gamma}(\alpha_{t,i},\beta_{t,i})$. 

Same as proof of \cref{thm:bayes_known}, we can break the regret in round $t$ as before:
\begin{align}
\label{eq:term5}
  \E{\mu_{A^*} - \mu_{A_t}}
  & = \E{\condE{\mu_{A^*} - \mu_{A_t}}{H_t}} \nonumber \\
  & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} +
  \E{\condE{{{\hat{\mu}_{t,A_t}}}- \mu_{A_t}}{H_t}}\,,
  \nonumber \\
  & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} +
  \mathbb E\Bigsn{\mathbb E_{\bsigma}\bigsn{\mathbb E [\hat{\mu}_{t,A_t}- \mu_{A_t} \mid \bsigma ] \mid H_t} }\,, \nonumber
  \\
  & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} 
\end{align}

where $\sigma_{t,i}^2$ is defined as $\sigma_{t,i}^2= \frac{1}{{\kappa_{t,i}\lambda_{t,i}}}$ which is a `sampled posterior variance', (where recall $\lambda_{t,i} \sim \text{Gam}(\alpha_{t,i},\beta_{t,i})$). The last equality holds since given $H_t$ and $\bsigma$,  $E[\mu_{i} \mid H_t] 
 = \hat{\mu}_{t,i}$ for any $i \in [K]$ by definition of the posterior update. %\red{check the argument}.


Now given $H_t$ and $\bsigma_t$, let us define the high-probability confidence interval of each arm $i$ as $C_t(i) = \sqrt{2 \sigma_{t, i}^2 \log(1 / \delta)}$, and the ``good event"
$
  E_t
  = \set{\forall i \in [K]: \abs{\mu_i -  \hat{\mu}_{t,i}} \leq C_t(i)}
$,
same as what we introduced in the proof of \cref{thm:bayes_known}. 
%\todob{Refer to earlier defined concepts instead of redefining them, especially if you want to make a point that this proof is built on another one.}
%which essentially indicates the ``good event"' %\todob{Use \say{this} for quotes.} when all the confidence intervals at round $t$ hold good. 
%
Further note, given $H_t$ and $\sigma_{t,i}^2$ (or equivalently $\lambda_{t,i})$),  $\mu_i - \hat{\mu}_{t,i} \mid \sigma_{t,i}^2, H_t \sim \cN(\mathbf{0}, \sigma_{t,i}^2)$, 
since given $H_t$ and $\sigma_t$, $\mu_i$ has the posterior $\mu_i \sim \cN(\hat \mu_{t,i}, \sigma_{t,i}^{2})$, where recall we defined $\sigma_{t,i}^2 = \frac{1}{\kappa_{t,i}\lambda_{t,i}}$ and $\lambda_{t,i} = \frac{1}{\sigma_{t,i}^2} \sim \text{Gam}(\alpha_{t,i},\beta_{t,i})$.
 %\red{I am confused a bit}
Thus following same analysis as in \eqref{eq:thm11}, we get: 
%So we w.h.p. $(1-\delta')$, we have \red{checK!}:
%
\begin{align}
  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{\bsigma_t, H_t}
  \leq \sum_{i = 1}^K \sqrt{\frac{\sigma_{t,i}^2}{2 \pi}} \delta.
  \label{eq:bayes_regret_scale22}
\end{align} %\todob{This seems incorrect. $\sigma_i^2$ should be $\sigma_{t, i}^2$?}
%


Further taking expectation over $H_n, \bsigma, \bmu$, and summing over $t = 1,2, \ldots n$ we get:
\begin{align*}
  &\E{\sum_{t = 1}^n(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}
  \\	
  & \leq 
  \delta(2\pi)^{-1/2}\mathbb{E}_{H_n, \bsigma, \bmu}\biggsn{\sum_{t = 1}^n \sum_{i = 1}^K \mathbb{E}_{\bsigma_{t}}[\sigma_{t,i} \mid \bmu,\bsigma, H_n]}
  \\
  & = 
    \delta(2\pi)^{-1/2}\mathbb{E}_{H_n, \bsigma, \bmu}\biggsn{\sum_{t = 1}^n \sum_{i = 1}^K \mathbb E_{\lambda_{t,i} \sim \text{Gam}(\alpha_{t,i},\beta_{t,i})}\Bigsn{ \sqrt{\frac{1}{2 \kappa_{t,i}\lambda_{t,i}}} \mid \bmu,\bsigma, H_n } }
  \\
  & \leq  
  \delta(2\pi)^{-1/2}\mathbb{E}_{H_n, \bsigma, \bmu}\biggsn{\sum_{t = 1}^n \sum_{i = 1}^K \biggsn {\sqrt{\mathbb E_{\lambda_{t,i}}\bigsn{ \frac{1}{2 \kappa_{t,i}\lambda_{t,i}} \mid \bmu,\bsigma, H_n  } }}  }
  \\
    & \overset{(1)}{=}
  \delta(2\pi)^{-1/2}\mathbb{E}_{H_n, \bsigma, \bmu}\biggsn{ \sum_{i = 1}^K \sum_{t = 1}^n \biggsn {\sqrt{ \bigsn{ \frac{\beta_{t,i}}{2 \kappa_{t,i}(\alpha_{t,i}-1)}} }} \mid \bmu,\bsigma, H_n  }
  \\
  & \overset{(2)}{\leq}  
  \delta(2\pi)^{-1/2}\biggsn{ \biggsn {\sqrt{ nK   \mathbb{E}_{H_n, \bsigma, \bmu}\bigsn{ \sum_{t = 1}^n \sum_{i = 1}^K  \frac{\beta_{t,i}}{2 \kappa_{t,i}(\alpha_{t,i}-1)} \mid \bmu,\bsigma, H_n  } }}  } =
  \\
  &\hspace{-0.1in}   
  \frac{\delta}{\sqrt{2\pi}}\biggsn{ \biggsn {\sqrt{ nK   \mathbb{E}_{H_n, \bsigma, \bmu}\bigsn{ \sum_{i = 1}^K \sum_{\tau = 1}^n  \frac{2\beta_{0,i} 
    +
    \sum_{t \in [\tau]}\I{A_t = i}(x_\tau(i) - \bar{x}_{\tau,i})^2
    +
    \kappa_{0,i} [(\bar{x}_{\tau,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]}{2 \kappa_{t,i}(\alpha_{0,i} + N_{\tau}(i)/2 -1)}  \mid \bmu,\bsigma, H_n  } }}},
\end{align*}
where $(1)$ holds since since $1/\lambda_{t,i}$ follows Inverse-Gamma$(\alpha_{t,i},\beta_{t,i})$ and thus we can apply \cref{lem:inv_gam} (note we assumed $\alpha_{0,i}> 1$, which implies $\alpha_{t,i}> 1, ~\forall t \in [T]$).
$(2)$ applies by successive application of Cauchy-Schwarz and pushing the expectation under square-root by Jensen's inequality.


Further, applying the posterior update forms from \cref{alg:bayes_unknown}, we get: 

\begin{align*}
  &\frac{\sqrt{2\pi}}{\delta}\E{\sum_{t = 1}^n(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}
  \\	
  & \leq   
  \biggsn{ \biggsn {\sqrt{ nK \mathbb{E}_{H_n, \bsigma, \bmu}\bigsn{   \sum_{i = 1}^K \sum_{\tau = 1}^n  \frac{2\beta_{0,i} 
    +
    \sum_{t \in [\tau]}\I{A_t = i}(x_\tau(i) - \bar{x}_{\tau,i})^2
    +
    \kappa_{0,i} [(\bar{x}_{\tau,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]}{2 (\kappa_{0,i}+N_\tau(i))(\alpha_{0,i} + N_{\tau}(i)/2 -1)} \mid \bmu,\bsigma, H_n } }}   }
    \\
    & =  
    \biggsn{ \biggsn {\sqrt{ nK \mathbb{E}_{H_n, \bsigma, \bmu}\bigsn{ \sum_{i = 1}^K \mathbb E_{N_n(i)}\Bigsn{\sum_{s = 0}^{N_n(i)}  \frac{2\beta_{0,i} 
    +
    \sum_{t = 1}^s(x_s(i) - \bar x_{s,i} )^2
    +
    \kappa_{0,i} [(\bar{x}_{s,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]}{ (\kappa_{0,i}+s)(2\alpha_{0,i} + s -2)}} \mid \bmu,\bsigma, H_n  }}}  },
\end{align*}
where we we slightly abused the notation above to denote $\bar x_{s,i}= \frac{1}{s}\sum_{t' = 1}^s x_{t',i}$ for all $s > 1$. However, further to get rid of the randomness of $N_n(i)$, we can further upper bound the above expression as:
\begin{align}
\label{eq:long0}
  &\frac{\sqrt{2\pi}}{\delta}\E{\sum_{t = 1}^n(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}
  \\	
  & \leq   \nonumber
    \biggsn{ \biggsn {\sqrt{ nK   \mathbb{E}_{H_n, \bsigma, \bmu}\bigsn{ \sum_{i = 1}^K \sum_{s = 0}^{n}  \frac{2\beta_{0,i} 
    +
    \sum_{t = 1}^s(x_s(i) - \bar x_{s,i} )^2
    +
    \kappa_{0,i} [(\bar{x}_{s,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]}{(\kappa_{0,i}+s)(2\alpha_{0,i} + s -2)}\mid \bmu,\bsigma, H_n }}}   },
\end{align}
which after applying \cref{lem:var_bdd} we get:

\begin{align}
\label{eq:long1}
  &\E{\sum_{t = 1}^n(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t} }
  \\	
  & \leq   \nonumber
    \frac{\delta \sqrt{nK}}{\sqrt{2\pi}}\biggsn{ \biggsn {\sqrt{ \mathbb{E}_{H_n, \bsigma, \bmu} \bigsn{ \underbrace{\sum_{i = 1}^K \frac{2 \beta_{0,i} + \kappa_{0,i}(\mu_i-\mu_{0,i})^2}{2\kappa_{0,i}(\alpha_{0,1}-1)}}_{(A)} + \underbrace{ \sum_{i = 1}^K \sum_{s = 1}^{n}  \frac{ 2\beta_{0,i} 
    +
    \sigma_i^2 (s-1)
    +
    \kappa_{0,i} [\frac{\sigma_{i}^2}{s} + (\mu_{i} - \mu_{0,i})^2  ]  }{(\kappa_{0,i} + s)(2\alpha_{0,i} + s - 2)}}_{(B)} \mid \bmu,\bsigma, H_n }}}   },
\end{align}

For the term $(A)$, since $\mu \sim \cN(\mu_{0,i},\frac{1}{\kappa_{0,i}\lambda_{i}})$, where $\lambda_{i} \sim \text{Gam}(\alpha_{0,i},\beta_{0,i})$, taking conditional expectation over $\mu_i \mid \lambda_{0,i}$ and further over $\lambda_{0,i}$ we get:

\[
\mathbb E_{\lambda_{i}}\biggsn{ \mathbb E_{\mu_{i}}\Bigsn{ \bigsn{\sum_{i = 1}^K \frac{2 \beta_{0,i} + \kappa_{0,i}(\mu_i-\mu_{0,i})^2}{2\kappa_{0,i}(\alpha_{0,1}-1)}} \mid \lambda_i } }
= 
\sum_{i = 1}^K \frac{2 \beta_{0,i} + \mathbb E_{\lambda_{i}}\biggsn{ \lambda_i^{-1}}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
=   \sum_{i = 1}^K \frac{2 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)},
\]
where note the last equality holds since since $1/\lambda_{i}$ follows Inverse-Gamma$(\alpha_{0,i},\beta_{0,i})$ and thus we can apply \cref{lem:inv_gam}, given we assumed $\alpha_{0,i}> 1$ by assumption.
  
To control term $(B)$, 
\begin{align*}
& \mathbb E_{H_n,\bsigma,\bmu} \biggsn{ \sum_{i = 1}^K \sum_{s = 1}^{n}  \frac{ 2\beta_{0,i} 
    +
    \sigma_i^2 (s-1)
    +
    \kappa_{0,i} [\frac{\sigma_{i}^2}{s} + (\mu_{i} - \mu_{0,i})^2  ]  }{(\kappa_{0,i} + s)(2\alpha_{0,i} + s - 2)}}
    \\
    & \leq
    \mathbb E_{H_n,\bsigma,\bmu} \biggsn{ \sum_{i = 1}^K \sum_{s = 1}^{n}  \frac{ 2\beta_{0,i} 
    +
    \sigma_i^2 s
    +
    \kappa_{0,i} [\frac{\sigma_{i}^2}{s} + (\mu_{i} - \mu_{0,i})^2  ]  }{(\kappa_{0,i} + s)(2\alpha_{0,i} + s - 2)}}
    \\
      & \overset{(1)}{\leq}
      \sum_{i = 1}^K \frac{\beta_{0,i}}{(\kappa_{0,i}+1)(\alpha_{0,i}-0.5)} + \sum_{i = 1}^K \sum_{s = 1}^{n} \frac{\beta_{0,i}/(\alpha_{0,i}-1)}{\kappa_{0,i}+s}
      + 
    \mathbb E_{\bsigma} \biggsn{ \sum_{i = 1}^K \sum_{s = 1}^{n} \frac{\sigma_i^2}{\kappa_{0,i} + s} }   
    \\
    & \hspace{2in} + \mathbb E_{\bsigma} \biggsn{ \sum_{i = 1}^K \sum_{s = 1}^{n} \frac{
     \sigma_{i}^2  }{s^2}} 
    +
    \mathbb E_{\bsigma} \biggsn{ \sum_{i = 1}^K \sum_{s = 1}^{n}\frac{
     \mathbb E_{\bmu}[(\mu_{i} - \mu_{0,i})^2 \mid \sigma  ]  }{2(\alpha_{0,i}-1) + s}} 
    \\
    & \overset{(2)}{\leq}
      \sum_{i = 1}^K \frac{\beta_{0,i}}{(\kappa_{0,i}+1)(\alpha_{0,i}-0.5)} + \sum_{i = 1}^K \frac{\beta_{0,i}}{\alpha_{0,i}-1}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)
      + 
    \biggsn{ \sum_{i = 1}^K  \frac{\beta_{0,i}}{\alpha_{0,i}-1}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big) }   
    \\
    & \hspace{2in} + \biggsn{ \sum_{i = 1}^K  \frac{
    \beta_{0,i}  }{(\alpha_{0,i}-1)}\frac{\pi^2}{6}} 
    +
    \mathbb E_{\bsigma} \biggsn{ \sum_{i = 1}^K 
    \kappa_{0,i}\frac{\sigma_i^2}{\kappa_{0,i}} \log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)   } 
    \\
    & \leq 
    \sum_{i = 1}^K \frac{\beta_{0,i}}{\kappa_{0,i}(\alpha_{0,i}-1)}
    +
   \sum_{i = 1}^K \frac{(3+\frac{\pi^2}{6})\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)
   \\
   & \leq 
   \sum_{i = 1}^K \frac{\beta_{0,i}}{\kappa_{0,i}(\alpha_{0,i}-1)}
    +
   \sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)
    %
\end{align*} 
where Inequality $(1)$ several times uses the fact that $2(\alpha_{0,i} - 1)>0$, and Inequality $(2)$ comes from repeated application of \cref{lem:reciprocal sum} and the fact that $\sum_{x = 1}^\infty 1/x^2 \leq \frac{\pi^2}{6}$.

Then combining the above two derived upper bounds of term (A) and (B) to \eqref{eq:long1}, we further have: 

\begin{align}
\label{eq:long2}
  &\E{\sum_{t = 1}^n(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}} \nonumber
  \\	
  & \leq   
    \frac{\delta \sqrt{nK}}{\sqrt{2\pi}} \sqrt{\sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
+ 
\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)}
    \,.
\end{align}


Coming back to the original bayesian regret expression, note that the regret expression from \eqref{eq:term5} can be actually decomposed based on $E_t$ and $\bar E_t$ as:
\begin{align}
\label{eq:term2}
  &\condE{{\hat{\mu}_{t,A^*}}  - \mu_{A^*}}{H_t} \nonumber
  \\  
  &\leq \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{E_t}}{H_t} +
  \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t} \nonumber
  \\
  & \leq \condE{C_t(A^*)}{H_t} \nonumber
  +\condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t}
  \\
  & = \condE{C_t(A_t)}{H_t} 
  +\condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t}
  \,,
\end{align}
where the last equality follows from the fact that given $H_t$, $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions, as both $\tilde \mu_{t,i}$ and $\mu_{t,i}$ can be seen as independent posterior samples from a NG$(\hat \mu_{t,i},\kappa_{t,i},\alpha_{t,i},\beta_{t,i})$ distribution marginalized over $\lambda$; more precisely for any realization of $\tilde \mu_{t,i}$ and $\mu_i$:
\[
P(\tilde \mu_{t,i}) = \int_{\lambda} P\big( (\tilde \mu_{t,i}, \lambda_{t,i} \sim \text{NG}(\hat \mu_{t,i},\kappa_{t,i},\alpha_{t,i},\beta_{t,i}) ) \big)d\lambda  ~~~\text{and}
\]
\[
P(\mu_{i}) = \int_{\lambda} P\big( (\tilde \mu_{t,i}, \lambda_{t,i} \sim \text{NG}(\hat \mu_{t,i},\kappa_{t,i},\alpha_{t,i},\beta_{t,i}) ) \big)d\lambda,
\]
they follow the same distribution. 


Bounding the second term in \eqref{eq:term2} follows as in \eqref{eq:long2}. 
%
The only remaining task is to bound the first term in \eqref{eq:term2}, which can be upper bounded as follows:

\begin{align*}
  &\E{\sum_{t = 1}^n C_t(A_t)}
  = \E{\sum_{t = 1}^n \sum_{i = 1}^K \I{A_t = i} C_t(i)} 
  = \E{\sum_{i = 1}^K \bigg[ \sum_{t = 1}^n \I{A_t = i} C_t(i) \bigg]}
  \\
  &\overset{(1)}{\leq} \E{\sum_{i = 1}^K \bigg[ \sqrt{N_n(i)} \sqrt{\sum_{t = 1}^n C_t^2(i)\I{A_t = i} }\bigg]}
  \\
  &\overset{(2)}{\leq} \E{ \bigg[ \sqrt{\sum_{i = 1}^K N_n(i)} \sqrt{\sum_{i = 1}^K \sum_{t = 1}^n C_t^2(i)\I{A_t = i} }\bigg]}
  \\
  &\overset{}{=}  \bigg[ \sqrt{n} \sqrt{\E{ \sum_{i = 1}^K \sum_{t = 1}^n {2 \sigma_{t, i}^2 \log(1 / \delta)}\I{A_t = i} }\bigg]}
  \\
  &\overset{}{=}  \bigg[ \sqrt{n} \sqrt{\E{ \sum_{i = 1}^K \sum_{s = 0}^{N_n(i)} \bign{2 \sigma_{t, i}^2 \log(1 / \delta)}\I{A_t = i} }\bigg]}
  \\
  & \overset{}{\leq}  \bigg[ \sqrt{n} \sqrt{\E{ \sum_{i = 1}^K \sum_{s = 0}^{n} \bign{2 \sigma_{t, i}^2 \log(1 / \delta)}\I{A_t = i} }\bigg]}
  \\
  & \overset{(4)}{\leq} \bigg[ \sqrt{n\log \frac{1}{\delta}} \sqrt{{ \sum_{i = 1}^K \sum_{s = 0}^n   \mathbb{E}_{H_n, \bsigma, \bmu}\bigsn{ \sum_{i = 1}^K \sum_{s = 0}^{n}  \frac{2\beta_{0,i} 
    +
    \sum_{t = 1}^s(x_s(i) - \bar x_{s,i} )^2
    +
    \kappa_{0,i} [(\bar{x}_{s,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]}{(\kappa_{0,i}+s)(2\alpha_{0,i} + s -2)}\mid \bmu,\bsigma, H_n } }\bigg]}
  \\
  & \leq \sqrt{n\log \frac{1}{\delta}}\sqrt{\sum_{i = 1}^K \sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
+ 
\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)}
 \,,
\end{align*}
where $(1)$ and $(2)$ uses Cauchy-Schwarz, $(2)$ applies Jensen's inequality, the Inequality $(4)$ above follows similar to the derivation of \eqref{eq:long0}, and the inequality follows from a similar derivation as shown for \eqref{eq:long2} from \eqref{eq:long0}.

Finally, chaining the above inequalities on the summation of the instantaneous regret over $n$ rounds (see \eqref{eq:term2})

\begin{align*}
\label{eq:term2}
  &\mathbb{E}_{H_n,\bmu,\bsigma}\biggsn{\sum_{t = 1}^n\condE{{\hat{\mu}_{t,A^*}}  - \mu_{A^*}}{H_t}} \nonumber
  \\  
  &\leq
  \biggn{\frac{\delta \sqrt{nK}}{\sqrt{2\pi}} + \sqrt{n\log \frac{1}{\delta}}}\sqrt{\sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
+ 
\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)}  
  \,,
\end{align*}
which proves the claim, noting $C= \sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
+ 
\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)$. 
\end{proof}

\subsection{Additional Claims used in the Proof of \cref{thm:bayes_unknown2}} 

\begin{restatable}[\cite{murphy2007conjugate}]{lem}{gaussgam}
\label{lem:gauss_gam}
The joint distribution of $(\mu,\lambda) \sim NG(\mu_0,\kappa_0,\alpha_0,\beta_0)$ is equivalent to the consecutive draw of $\lambda \sim \text{Gam}(\alpha_0,\beta_0)$ followed by $\mu \sim \cN(\mu_0,(\kappa_0\lambda)^{-1})$, $(\mu_0,\kappa_0,\beta_0,\alpha_0)$ being the paraeters of the Gaussian-Gamma distribution. In other words, 
\[
NG(\mu,\lambda \mid \mu_0,\kappa_0,\alpha_0,\beta_0) = \cN(\mu \mid \mu_0,(\kappa_0\lambda)^{-1})\text{Gam}(\gamma \mid \alpha_0,\beta_0) 
\] 
\end{restatable}



\begin{restatable}[\cite{cook08}]{lem}{invgam}
\label{lem:inv_gam}
Suppose $X$ follows a Gamma$(\alpha,\beta)$ distribution with shape parameter $\alpha>0$ and rate parameter $\beta > 0$, then $1/X$ follows an Inverse-Gamma$(\alpha,1/\beta)$ distribution. 
\\
Moreover if $\alpha>1$, $E[1/X] = \frac{\beta}{\alpha-1}$. 
\end{restatable}


\begin{restatable}[\cite{cook08}]{lem}{avgchi}
\label{lem:avg_chi}
Let ${\displaystyle X_{1},...,X_{n}}$ are i.i.d. ${\displaystyle N(\mu ,\sigma ^{2})}$ random variables, (a). then 
\[
{\displaystyle \sum _{i=1}^{n}\frac{(X_{i}-{\bar {X}_n})^{2}}{\sigma ^{2}}\sim \chi _{n-1}^{2}},
\]
where ${\displaystyle {\bar {X}_n}={\frac {1}{n}}\sum _{i=1}^{n}X_{i}}$ is the sample mean of $n$ random draws, and $\chi_{d}^2$ is the chi-squared random variable with $d$-degrees of freedom (for any $d \in \N_+$). 
\\
(b). Further if $X \sim \chi_d^2$, then $E[X] = d$.
\end{restatable}

\begin{restatable}[]{lem}{varbdd}
\label{lem:var_bdd}
For any $i \in [K]$, and round $\tau \in [T]$,
\begin{align*}
  E_{H_\tau,\bmu,\bsigma}&\biggsn{\frac{\beta_{\tau,i}}{ (\kappa_{\tau,i})(\alpha_{\tau,i} -1)} \mid N_\tau(i)} 
    \leq  
        E_{H_\tau,\bmu,\bsigma}\biggsn{
    \frac{\Bigsn{2\beta_{0,i} 
    +
    \sigma_i^2 (N_{\tau}(i)-1)
    +
    \kappa_{0,i} [\frac{\sigma_{i}^2}{N_{t,i}} + (\mu_{i} - \mu_{0,i})^2  ]  }}
    {\kappa_{\tau,i}(2\alpha_{0,i} + 2N_{\tau}(i) - 2)} \mid N_\tau(i)}
\end{align*}
\end{restatable}

\begin{proof}
We start by noting that for any $i \in [K]$, 
\begin{align*}
 \mathbb{E}_{H_\tau,\bsigma}&\Bigsn{ \mathbb E_{\bsigma_{\tau}}[ \sigma_{\tau,i} \mid H_{\tau},\bsigma ]}
	\\
	& = 
 \mathbb{E}_{H_{\tau},\bsigma}\Bigsn{ \mathbb E_{\lambda_{\tau,i} \sim \text{Gam}(\alpha_{\tau,i},\beta_{\tau,i})}{ \sqrt{\frac{1}{ \kappa_{\tau,i}\lambda_{\tau,i}}}} \mid H_{\tau},\bsigma}
    \\
    &
    \leq \mathbb{E}_{H_{\tau},\bsigma}\Bigsn{ \sqrt{\frac{1}{ \kappa_{\tau,i}}}  \sqrt{\mathbb E_{\lambda_{\tau,i}} \biggsn{\frac{1}{\lambda_{\tau,i}} } } \mid H_{\tau},\bsigma} 
    \\
    &
    = \mathbb{E}_{H_{\tau},\bsigma}\Bigsn{ \sqrt{\frac{1}{ \kappa_{\tau,i}}}
    \sqrt{\frac{\beta_{\tau,i}}{ (\alpha_{\tau,i} -1)}   } \mid H_{\tau},\bsigma},
\end{align*}
where the last equality holds by the fact that given $H_{\tau}$, $\frac{1}{\lambda_{t,i}}$ follows the \textit{Inverse-Gamma}$(\alpha_{\tau,i},\beta_{\tau,i}^{-1})$ distribution and \cref{lem:inv_gam} \cite{cook08}. Now from the posterior update rules, given $H_{\tau}$ we have:
\begin{align*}
\frac{\beta_{\tau,i}}{ (\alpha_{\tau,i} -1)} 
    & \leq 
    \frac{\beta_{0,i} 
    +
    \frac{1}{2}\sum_{t \in [\tau]}\I{A_t = i}(x_{t,i} - \bar{x}_{\tau,i})^2
    +
    \frac{\kappa_{0,i} N_t(i) (\bar{x}_{\tau,i} - \mu_{0,i})^2 }{2(\kappa_{0,i} + N_{\tau}(i))} }{(\alpha_{0,i} + N_{\tau}(i)/2 -1)}
    \\
    & \leq 
    \frac{2\beta_{0,i} 
    +
    \sum_{t \in [\tau]}\I{A_t = i}(x_\tau(i) - \bar{x}_{\tau,i})^2
    +
    \kappa_{0,i} [(\bar{x}_{\tau,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]  }
    {2(\alpha_{0,i} + N_{\tau}(i)/2 -1)}
\end{align*}

Now given any fixed reward-precision $\lambda_i \sim \text{Gam}(\alpha_{0,i},\beta_{0,i})$, $\sigma_{i}^2= \frac{1}{\lambda_{i}}$, mean-reward $\mu_i \sim \cN(\mu_{0,i},\sigma_{0,i}^2)$, and fixed number of pulls $N_\tau(i) = s$ (say), note $x_{t,i} \overset{iid}{\sim} \cN(\mu_{i}, \sigma_{i}^2)$ can be seen as $N_\tau(i) = s$ independent and identical draws from $\cN(\mu_i,\sigma_i^2)$. Thus
$\sum_{t = 1}^\tau\I{A_t = i} \frac{(x_{t,i} - \bar x_{\tau,i})^2}{\sigma_{i}^2} = \sum_{t = 1}^s\frac{(x_{t,i} - \bar x_{\tau,i})^2}{\sigma_{i}^2}$ follows $\chi^2(s-1)$, i.e. chi-squared distribution with $s-1$ degrees of freedom (see \cref{lem:avg_chi}). Further using \cref{lem:avg_chi}, part-(b) again, we know 
\[
E_{x_{t,i} \sim \cN(\mu_i,\sigma_i^2)} \biggsn{\sum_{t = 1}^s\frac{(x_{t,i} - \bar x_{\tau,i})^2}{\sigma_{i}^2}} = (s-1).
\]

Applying this to above chain of equations, note we get:

\begin{align*}
& E_{H_\tau,\bmu,\bsigma}\biggsn{\frac{\beta_{\tau,i}}{ (\kappa_{\tau,i})(\alpha_{\tau,i} -1)} \mid N_{\tau}(i)}
	\\    
    & = 
    E_{H_\tau,\bmu,\bsigma}\biggsn{
    \frac{{2\beta_{0,i} 
    +
    E_{x_{t,i}\sim \cN(\mu_i,\sigma_i^2)}\biggsn{\sigma_i^2 \sum_{t = 1}^{N_{\tau}(i)}(\frac{x_{t,i} - \bar{x}_{\tau,i}}{\sigma_i})^2 \mid \mu_i, \sigma_{i}}
    +
    \kappa_{0,i} [(\bar{x}_{\tau,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]  }}
    {2(\kappa_{\tau,i})(\alpha_{0,i} + N_{\tau}(i)/2 -1)} \mid N_\tau(i)}
    \\
    & = 
    E_{H_\tau,\bmu,\bsigma}\biggsn{
    \frac{\Bigsn{2\beta_{0,i} 
    +
    \sigma_i^2 (N_{\tau}(i)-1)
    +
    \kappa_{0,i} \big[E_{\bar x_{\tau,i} \overset{\text{iid}}{\sim} \cN(\mu,\sigma^2/N_\tau(i))}[(\bar{x}_{\tau,i} - \mu_{i})^2 \mid \mu_i,\sigma_i] + (\mu_{i} - \mu_{0,i})^2  \big]  }}
    {2(\kappa_{\tau,i})(\alpha_{0,i} + N_{\tau}(i)/2 -1)} \mid N_\tau(i)}
    \\
    & \overset{(1)}{=} 
    E_{H_\tau,\bmu,\bsigma}\biggsn{
    \frac{\Bigsn{2\beta_{0,i} 
    +
    \sigma_i^2 (N_{\tau}(i)-1)
    +
    \kappa_{0,i} [\frac{\sigma_{i}^2}{N_{t,i}} + (\mu_{i} - \mu_{0,i})^2  ]  }}
    {2(\kappa_{\tau,i})(\alpha_{0,i} + N_{\tau}(i)/2 -1)} \mid N_\tau(i)}
\end{align*}
where (1) again follows since given $\mu_i, \sigma_i$ and $N_\tau(i)$ as before, $x_{t,i} \overset{\text{iid}}{\sim} \cN(\mu,\sigma^2)$, and thus by definition 
$\bar x_{\tau, i} = \frac{1}{N_{\tau}(i)} \sum_{t = 1}^\tau \I{A_t = i} x_{t,i} \sim \cN\big(\mu, \frac{\sigma^2}{N_{\tau}(i)}\big)$> This leads to $E_{\bar x_{\tau,i} \overset{\text{iid}}{\sim} \cN(\mu,\sigma^2/N_\tau(i))}[\bar x_{\tau, i} - \mu_i]^2 = \frac{\sigma_i^2}{N_{\tau}(i)}$. Thus the claim is concluded. 
\end{proof}
%Also further note that, given any fixed $\bsigma$ s.t. $\sigma_i^2= 1/\lambda_i$ where $\lambda_i \sim \text{Gam}(\alpha_{0,i},\beta_{0,i})$.

\iffalse%%%%%%%%%%%%%%%%%%

\begin{restatable}[\red{chi-sqr confidence, not used} Lemma 1, \cite{massart}]{lem}{chiconc}
\label{lem:chi_conc}
Suppose $X$ is a chi-squared distribution with $D$ degrees of freedom, for some $D \in \N_+$, then for any $\delta \in (0,1)$, with probability at least $1-\delta$,
\begin{align*}
   X \leq D + 2\sqrt{{D\log (1/\delta)} } + 2\log (1/\delta)
\end{align*}
\end{restatable}


\begin{restatable}[\red{not used, needed if use precision lower bound}]{lem}{preclb}
\label{lem:prec_lb}
Let $X \sim Gamma(\alpha,\beta)$. Then given any $\delta \geq \frac{1}{\exp(\alpha)}$, w.h.p. at least $(1-\delta)$,
\[
X > \frac{\alpha}{e \beta}
\]
\end{restatable}


\begin{proof}
We start by noting that, for any $x_0 \in R_+$ and some $t>0$, applying Markov's Inequality we get:

\begin{align*}
\P(X < x_0) & = \P(-tX > -tx_0) = \P(\exp(-tX) > \exp(-x_0))
\\
& = \frac{\E{\exp(-tX)}}{\exp(-tx_0)}.
\end{align*}

Moreover, using Gamma-pdf, we get:
\begin{align*}
\E{\exp(-tX)} & = \frac{\beta^\alpha}{\Gamma(\alpha)}\int_{x = 0}^{\infty}\exp(-tX)\exp(-\beta X)x^{\alpha -1}dx 
= \frac{\beta^\alpha}{\Gamma(\alpha)}\int_{x = 0}^{\infty}\exp(-(t+\beta)x)x^{\alpha -1}dx 
\\
& = \frac{\beta^\alpha}{\Gamma(\alpha)}\int_{u = 0}^{\infty}\exp(-u)\biggn{\frac{u}{t+\beta}}^{\alpha -1}\frac{du}{t+\beta}
\\
& = \frac{\beta^\alpha}{\Gamma(\alpha) (t+\beta)^\alpha}\int_{u = 0}^{\infty}\exp(-u)u^{\alpha-1}du = \frac{\beta^\alpha}{(t+\beta)^\alpha}.
\end{align*}

Since we want $\P(X < x_0) < \delta$, the goal is to find the threshold $x_0$  such that: 
\begin{align*}
\min_{t} \frac{\beta^\alpha \exp(tx_0)}{(t+\beta)^\alpha} < \delta.
\end{align*}

It is easy to check the minimum of the above expression is attained at $t = \frac{\alpha}{x_0} - \beta$, given $x_0 < \frac{\alpha}{\beta}$ since by definition we need $t>0$.

Combining above two equations, we get:
\begin{align*}
\bigg(\frac{\beta}{\alpha}\bigg)^{\alpha} \frac{e^{\alpha - \beta x_0}}{x_0^\alpha} < \delta,
\end{align*}

 which implies to choose $x_{0}$ such that:

\[
x_0 \geq \frac{\alpha}{\beta} + \frac{\alpha}{\beta}\log\Big( x_{0} \frac{\beta}{\alpha} \Big) + \frac{\log (1/\delta)}{\beta} \text{ s.t. } x_{0} > \frac{\alpha}{\beta}.
\]

Finally, the claim holds noting that above implies we can choose $x_0$ to be $\frac{\alpha}{e\beta}$, which satisfies the above inequality as long as $\log (1/\delta) \leq \alpha$.

\iffalse%%%%%%%%%%%%%%%%
Or more loosely, it enough to choose $x_0$ such that:

\[
x_0 \geq 1 + \frac{\log (1/\delta)}{\beta} + \frac{\alpha}{\beta}\log x_0 \text{ s.t. } x_{0} > \frac{\alpha}{\beta}.
\]

Finally note that above can be satisfied for any $x_0 > 2\bign{ 1 + \frac{\log 1/\delta}{\beta} + \big(\frac{\alpha}{\beta}\big)^2 }$
by \cref{lem:tvslogt} where in our case $A = 1 + \frac{\log 1/\delta}{\beta}$ and $B = \frac{\alpha}{\beta}$.
\fi %%%%%%%%%%%%%%%%%%%%%%%%
%We finally note that above can be satisfied setting $x_{0} = \frac{\alpha}{e \beta}$, which satisfies the above relation assuming $\alpha/e \geq \log(1/\delta)$. 
\end{proof}

\begin{restatable}[(\red{Ignore - Not Used})]{lem}{tvslogt}
\label{lem:tvslogt}
For any $x > 0$ and $A> 0, B > 1$, $x > A + B\log x$ for any $x \ge 2A + 2B^2$.
\end{restatable}

\begin{proof}
This follows form the calculation as shown below:
\begin{align*}
	A + B\log x & = A + B\log (2A + 2B^2 )\\
	& < A + B\log\big(2B^2 (1 + A/B^2)\big)\\
	& < A + B\log 2B^2 + A/B ~~~~\text{as } \log(1+x) \leq x\\	
	& = A + A/B + 2B^2\\
	& \le 2A + 2B^2 = x,
\end{align*}
where the last inequality follows since $B > 1$.
\end{proof}

\fi%%%%%%%%%%%%%%%%%%%%%%%