\section{Experiments}
\label{sec:experiments}

We also study the empirical performance of our proposed algorithms. Since \varts does not assume that the reward variances are known, and thus is more realistic than \cref{alg:bayes_known}, we focus on \varts. We conduct four experiments. First, we evaluate \varts in a Bernoulli bandit, which is a standard bandit benchmark. Second, we experiment with beta reward distributions. Their support is $[0, 1]$, similarly to Bernoulli distributions, but their variances are not fully determined by their means. Third, we experiment with a Gaussian bandit. Finally, we vary the number of arms and the variances of reward distributions, to show the robustness of \varts.


\subsection{Experimental Setup}

All problems are Bayesian bandits, where the mean arm rewards are sampled from some prior distribution. For a Gaussian bandit with unknown reward variances, \varts is run with the true $(\bmu_0, \bkappa_0, \balpha_0, \bbeta_0)$. For other problems, the hyper-parameters of \varts are set using the method of moments from samples from the prior. In particular, for a given Bayesian bandit, let $\bar{\mu}$ and $v_\mu$ be the estimated mean and variance of mean arm rewards sampled from its prior, respectively. Moreover, let $\bar{\lambda}$ and $v_\lambda$ be the estimated mean and variance of the precision (reciprocal of the variances) of reward distributions sampled from its prior, respectively. Then $\mu_{0, i} = \bar{\mu}$, $\beta_{0, i} = \bar{\lambda} / v_\lambda$, $\alpha_{0, i} = \beta_{0, i} / \bar{\lambda}$, and $\kappa_{0, i} = \beta_{0, i} / (\alpha_{0, i} v_\mu)$.

We compare \varts to several baselines. \ucb \citep{auer02finitetime} is arguably the most popular algorithm for stochastic $K$-armed bandits with $[0, 1]$ rewards. It does not adapt to the reward variances and is expected to be conservative. We also consider its two variants that adapt to the variances: \ucbtuned \citep{auer02finitetime} and \ucbv \citep{audibert09exploration}. \ucbtuned is a heuristic that performs well in practice. \ucbv uses empirical Bernstein confidence intervals and has theoretical guarantees. We implement both algorithms for $[0, 1]$ rewards. The last two algorithms are based on posterior sampling: Bernoulli and Gaussian TS. Bernoulli TS is implemented with $\mathrm{Beta}(1, 1)$ prior. When the rewards $Y_{t, i}$ are not binary, we clip them to $[0, 1]$ and then apply Bernoulli rounding: a reward $Y_{t, i} \in [0, 1]$ is replaced with $1$ with probability $Y_{t, i}$ and with $0$ otherwise. The last baseline is Gaussian TS. Similarly to \varts, we fit its hyper-parameters as $\mu_{0, i} = \bar{\mu}$ and $\sigma_{0, i}^2 = v_\mu$, where $\bar{\mu}$ and $v_\mu$ denote the estimated mean and variance of mean arm rewards, respectively. We set $\sigma_i^2$ to the $95$-th percentile of sampled reward variances from the prior. This means that Gaussian TS is sound on most arms, because their reward variances are overestimated. All simulations are averaged over $1\,000$ randomly initialized runs.

\begin{figure*}[h]
  \centering
  \includegraphics[width=6.6in]{Figures/Synthetic.pdf}
  \vspace{-0.2in}
  \caption{\varts compared to five baselines. The results are averaged over $1\,000$ randomly initialized runs. The plots share legends.}
  \label{fig:synthetic}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=6.6in]{Figures/Scaling.pdf}
  \vspace{-0.2in}
  \caption{\varts compared to five baselines. In the first two plots, we vary the number of arms $K$. In the last plot, we decrease the reward variances by scaling them with $1 / \gamma$. The results are averaged over $1\,000$ randomly initialized runs. The plots share legends.}
  \label{fig:scaling}
\end{figure*}


\subsection{Bernoulli Bandit}
\label{sec:bernoulli bandit}

We start with a Bernoulli bandit with $K = 10$ arms and horizon $n = 5\,000$. The mean reward of each arm $i$ is sampled i.i.d.\ from $\mathrm{Beta}(1, 1)$. Our results are reported in \cref{fig:synthetic}a. As expected, the frequentist methods (\ucb and \ucbv) have the highest regret. \varts performs comparably to Gaussian TS. Not surprisingly, Bernoulli TS, which is designed for this problem class, performs the best and has about $25\%$ lower regret than \varts. The second best method is \ucbtuned, which is a heuristic without regret guarantees.


\subsection{Beta Bandit}
\label{sec:beta bandit}

Our beta bandit problem is a variant of \cref{sec:bernoulli bandit} where the reward distribution of arm $i$ is $\mathrm{Beta}(s_i \mu_i, s_i (1 - \mu_i))$ for $s_i \geq 1$. When $s_i = 1$, the beta distribution coincides with the Bernoulli distribution. When $s_i > 1$, it has about $s_i$ lower variance. We set $s_i \sim \mathrm{Exp}(0.1)$. Thus $\E{s_i} = 10$ and the variance of the reward distribution is $10$ times lower on average than in \cref{sec:bernoulli bandit}. Our results are reported in \cref{fig:synthetic}b. As in \cref{fig:synthetic}a, the frequentist methods (\ucb and \ucbv) have the highest regret. \varts has the lowest regret. The regret of Gaussian TS is about $35\%$ higher and that of Bernoulli TS is about $70\%$ higher. This experiment clearly demonstrates the value of variance adaptation in a highly variable environment.


\subsection{Gaussian Bandit}
\label{sec:gaussian bandit}

The third experiment is with a Gaussian bandit where both the means and variances of rewards are sampled i.i.d.\ from a prior parameterized by $\mu_{0, i} = 0.5$, $\kappa_{0, i} = 1$, $\alpha_{0, i} = 1$, and $\beta_{0, i} = 1 / 4$. On average, this corresponds to an arm with mean reward $0.5$ and reward variance $0.25$. Therefore, the algorithms for $[0, 1]$ rewards (\ucb, \ucbtuned, \ucbv, and Bernoulli TS) should perform well. Unlike in the first two experiments, \varts is correctly specified in this case. Our results are reported in \cref{fig:synthetic}c. \varts outperforms all baselines by a large margin. The closest baseline is \ucb, with at least twice the regret.


\subsection{Robustness}
\label{sec:robustness}

In the last experiment, we show the robustness of our approach by varying the parameters in the earlier problems. In \cref{fig:scaling}a, we vary the number of arms $K$ in the beta bandit (\cref{sec:beta bandit}). In \cref{fig:scaling}b, we vary the number of arms $K$ in the Gaussian bandit (\cref{sec:gaussian bandit}). Finally, in \cref{fig:scaling}c, we decrease the reward variances in the Gaussian bandit by setting $\beta_{0, i} = \gamma$ for $\gamma \in [1, 100]$. Our results show that \varts robustly outperforms all baselines. The closest baseline is Gaussian TS. But even that one can have a multiple times higher regret (\cref{fig:scaling}c).
