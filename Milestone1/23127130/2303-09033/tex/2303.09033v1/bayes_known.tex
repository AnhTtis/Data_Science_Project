\section{Gaussian Bandit with Known Variances}
\label{sec:bayes_known}

We start with the Bayesian setting with Gaussian rewards and known heterogeneous reward variances. In \cref{sec:bayes_known_algo}, we introduce a Thompson sampling algorithm \citep{thompson33likelihood,chapelle11empirical,agrawal12analysis,russo14learning,gopalan14thompson} for this setting. Gaussian TS is straightforward and appeared in many prior works, starting with \citet{agrawal13further}. We state the regret bound and discuss it in \cref{sec:known_reg}, and sketch its proof in \cref{sec:known_reg_prf}. The regret bound scales roughly as: 
%\todob{Make sure that the parentheses are tall enough for the math inside them. See below. This is at multiple places.} 
%\todob{Do we do colons before display math? Choose one style and stick to it. This is at multiple places.}
\begin{align}
  \textstyle
  \sqrt{n\log n} \sqrt{\sum_{i =1}^K\sigma_i^2
  \log\Big(1 + n\frac{\sigma_{0,i}^{2}}{\sigma_i^{2}}\Big)}\,.
  \label{eq:rough bound known}
\end{align}
One notable property of the bound is that it goes to zero as either the reward variances $\sigma_i^2$ or the prior variances of the mean arm rewards $\sigma_{0, i}^2$ do. Although the bound is novel, its proof mostly follows \citet{kveton21metathompson}. The main reason for stating the bound is to contrast it with the main result in \cref{sec:bayes_unknown2}.

\input{algo_bayes_known.tex}


\subsection{Regret Analysis}
\label{sec:known_reg}

Before we analyze \cref{alg:bayes_known}, we recall the problem setting. The mean arm rewards are sampled from a Gaussian prior, $\bmu \sim P_0 = \cN(\bmu_0, \diag{\bsigma_0^2})$, where $\bmu_0 \in \realset^K$ and $\bsigma_0^2 \in \R_{\geq 0}^K$ are the prior means and variances of $\bmu$, respectively. Both $\bmu_0$ and $\bsigma_0^2$ are known by the agent. The reward of arm $i$ in round $t$ is sampled as $x_{t,i} \sim \cN(\mu_i,\sigma_i^2)$. The reward variances $\bsigma^2$ are fixed and known. Our regret bound is presented below.

\begin{restatable}[Variance-dependent regret bound for known variances]{thm}{bayesknown}
\label{thm:bayes_known} Consider the above setting. Then for any $\delta > 0$, the Bayes regret of Gaussian TS is bounded as:
\begin{align*}
  & R_n
  \leq \sum_{i = 1}^K\sqrt{\frac{2 \sigma_{0,i}^2}{\pi}} n \delta + {} \\
  & \sqrt{2n} \sqrt{\sum_{i =1}^K\sigma_i^2\Big(\log(1 + n\sigma_{0,i}^{2}\sigma_i^{-2}) + \sigma_{0,i}^{2}\sigma_i^{-2}\Big)\log(1/\delta)}\,.
\end{align*}
\end{restatable}

\textbf{Discussion.} For $\delta = 1 / n$, the bound in \cref{thm:bayes_known} scales roughly as:
\begin{align*}
  \tilde{O}\left(\sqrt{n \sum_{i = 1}^K\sigma_i^2 \log\Bign{1 + n\sigma_{0,i}^{2}\sigma_i^{-2} }} + \sqrt{n \sum_{i = 1}^K \sigma_{0,i}^2}\right)\,.
\end{align*} %\todob{This is written in a funny way. We decide to ignore $\log n$ from $\log(1 / \delta)$ because of $\tilde{O}$. But we keep the others. One way out is to present this result similarly to Theorem 2.}
Note here we ignore the first term in \cref{thm:bayes_known}, which is order-wise dominated by the second term when $\delta = 1 / n$. 
% 
Our bound has several properties that we discuss next. First, it matches the usual $\sqrt{n}$ dependence of all classic Bayes regret bounds \citep{russo14learning,russo16information,lu19informationtheoretic}. Second, it increases with variances $\sigma_i^2$ of individual arm rewards, which is expected because higher reward variances make learning harder. Third, the bound can be viewed as a generalization of existing bounds that assume homogeneous reward variance. Specifically, \citet{kveton21metathompson} derive a $\tilde{O}(\sqrt{\sigma^2 K n})$ Bayes regret bound in Lemma 4 under the assumption that the reward distribution of arm $i$ is $\cN(\mu_i, \sigma^2)$. We match it when $\sigma_i = \sigma$ for all arms $i$. Fourth, the bound approaches zero as $\sigma_{0, i} \to 0$. In this setting, the mean arm reward $\mu_i$ is almost certain because its prior variance $\sigma_{0, i}^2$ is low, and no exploration is necessary. This is a unique property of Bayes regret bounds that is not captured by similar frequentist analyses, such as that of \ucbnormal \citep{auer02finitetime}.

We do not provide a matching lower bound and this is a general issue in all recent works on Bayes regret minimization. Starting with seminal works \citep{russo14learning,russo16information}, all Bayes regret bounds are $O(\sqrt{n})$ and no matching lower bounds are derived. The only Bayesian lower bound that we are aware of is $O(\log^2 n)$ for a $K$-armed bandit (Theorem 3 of \citet{lai87adaptive}). In summary, matching Bayes regret upper and lower bounds are an open research problem. Therefore, to validate the shape of our bound, we discuss its behavior in all parameters of interest and relate it to prior regret bounds accepted by the community.


\subsection{Proof Sketch of \cref{thm:bayes_known}}
\label{sec:known_reg_prf}

We sketch the proof of \cref{thm:bayes_known} below. The complete proof can be found in \cref{app:known}.

Let $\bmu_t \in \realset^K$ denote the vector of unknown mean arm rewards at round $t$, sampled as $\mu_{t,i} \sim \cN(\hat\mu_{t,i},\sigma_{t,i}^2)$ for all $i \in [K]$. 
%\todob{$\bmu_{t,i}$ should not be bold.} \todob{Say that $\bmu_t$ are the true unknown mean arm rewards. This is to distinguish it from $\tilde{\bmu}_t$.} 
Let $H_t$ denote the history in round $t$. We also define a high-probability confidence interval for arm $i$ in round $t$ as $C_t(i) = \sqrt{2 \sigma_{t, i}^2 \log(1 / \delta)}$, where $\delta > 0$ is the confidence level, and event:
%
\begin{align*}
  E_t
  = \set{\forall i \in [K]: \abs{\mu_i -  \hat{\mu}_{t,i}} \leq C_t(i)}
\end{align*}
%
that all confidence intervals in round $t$ hold. Now let us fix round $t$. We start by noting that the instantaneous regret at round $t$ can be bounded as:
%
\begin{align*}
  & \E{\mu_{A^*} - \mu_{A_t}}
  = \E{\condE{\mu_{A^*} - \mu_{A_t}}{H_t}} \\
  & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} +
  \E{\condE{{{\hat{\mu}_{t,A_t}}} - \mu_{A_t}}{H_t}}
  \\ & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}}\,.
\end{align*} %w\todob{We also need to say that "The second equality holds because $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions, and $\hat{\mu}_t$ is deterministic given history $H_t$" - \red{this is discussed later}}
%
%The first equality is an application of the tower rule. The second equality holds because $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions, and $\hat{\mu}_t$ is deterministic given history $H_t$. 
The last equality holds since given $H_t$, clearly $E[\mu_{i}\mid H_t] 
 = \hat{\mu}_{t,i}$ for any $i \in [K]$.
% 
Then we introduce event $E_t$ to deal with the remaining term as:
\begin{align*}
  & \condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}} }{H_t} =  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar E_t}}{H_t}
  \\
  & \hspace{1.2in}
  + \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{E_t}}{H_t}
  \\
  & \leq \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{H_t} + \condE{C_t(A_t)}{H_t}\,,
\end{align*}
where the inequality follows from the observation that
$
  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{E_t}}{H_t}
  \leq \condE{C_t(A^*)}{H_t}\,,
$
and further $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions given $H_t$. 
%
Also given history $H_t$, since $\hat{\mu}_t$ is deterministic, $\bmu - \hat{\bmu}_t \mid H_t \sim \cN(\mathbf{0}, \Sigma_t)$, we get:
%\red{above should have the scale factor $b \geq (\mu^* - \mu_i)$?, what is a suitable $b$?}
\begin{align}
\label{eq:thm11}
  & \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{H_t} \nonumber
  \\
  & \leq \sum_{i = 1}^K \frac{1}{\sqrt{2 \pi \sigma_{t, i}^2}}
  \int_{x = C_t(i)}^\infty x
  \exp\left[- \frac{x^2}{2 \sigma_{t, i}^2}\right] \dif x
  %\nonumber \\
  %& = \sum_{i = 1}^K - \sqrt{\frac{\sigma_{t, i}^2}{2 \pi}}
  %\int_{x = C_t(i)}^\infty \frac{\partial}{\partial x}
  %\left(\exp\left[- \frac{x^2}{2 \sigma_{t, i}^2}\right]\right) \dif x
  \nonumber \\
  & = \sum_{i = 1}^K \sqrt{\frac{\sigma_{t, i}^2}{2 \pi}} \delta
  \leq \sum_{i = 1}^K \sqrt{\frac{\sigma_{0,i}^2}{2 \pi}}  \delta\,.
\end{align}
%
\iffalse%%%%%%%%%%%%%%%%
Now we chain all inequalities for the regret in round $t$ and get
\begin{align*}
  \E{\mu_{A^*} - \mu_{A_t}}
  \leq \E{C_t(A_t)} + \sum_{i = 1}^K\sqrt{\frac{2 \sigma_{0,i}^2}{\pi}}  \delta\,.
\end{align*}
\fi%%%%%%%%%%%%%%%%%%%
%
Having established the above bounds, summing the instantaneous regret over $n$ rounds, the Bayes regret of \cref{alg:bayes_known} can be bounded as:
\begin{align*}
  \E{\sum_{t = 1}^n \mu_{A^*} - \mu_{A_t}}
  \leq \E{\sum_{t = 1}^n C_t(A_t)} +
  \sum_{i = 1}^K\sqrt{\frac{2 \sigma_{0,i}^2}{\pi}} n \delta\,.
\end{align*}
%
The last part is to bound $\E{\sum_{t = 1}^n C_t(A_t)}$ from above. We apply a series of Cauchy-Schwarz inequalities, carefully handle the form of the confidence terms $C_t(i)$, and apply \cref{lem:reciprocal sum}, which yields: %\todob{Two indicators below are written differently from earlier. Choose one notation for indicators and stick to it.}
%
\begin{align*}
  & \E{\sum_{t = 1}^n C_t(A_t)}
  = \E{\sum_{t = 1}^n \sum_{i = 1}^K \I{A_t = i} C_t(i)} 
  \\
  &\leq \E{\sum_{i = 1}^K \bigg[ \sqrt{N_t(i)} \sqrt{\sum_{t = 1}^n C_t^2(i)\I{A_t = i} }\bigg]}
  \\
  & \leq  \sqrt{2n} \sqrt{\sum_{i =1}^K\sigma_i^2\log(1 + n\sigma_{0,i}^{2}\sigma_i^{-2})\log(1/\delta)} 
  \\
  & \hspace{1.2in} + \sqrt{2n\sum_{i = 1}^K\sigma_{0,i}^{2}\log(1/\delta)}
  \,,
\end{align*} 
where recall from \cref{alg:bayes_known} that $N_{t+1}(i) = N_t(i)+\I{A_t=i}$ for all $i \in [K]$ denotes the number of times arm $i$ is pulled in $t$ rounds.
%
%\todob{$N_t(i)$ is not properly quantified.} 
%\todob{A simpler application of Cauchy-Schwarz would be $\sum_{t = 1}^n C_t(A_t) \leq \sqrt{n} \sqrt{\sum_{t = 1}^n C_t^2(A_t)} = \sqrt{n} \sqrt{\sum_{i = 1}^K \sum_{t = 1}^n \I{A_t = i} C_t^2(A_t)}$. Note that there is no need for the expectation. This is all worst case.}
Chaining of all inequalities concludes the proof.

% While this section presents a clean variance-sensitive regret bound for Gaussian bandits (see \cref{thm:bayes_known} and its following discussion), our primary goal is to design a general TS algorithm with variance aware regret bounds even when the reward variances are unknown. We present this algorithm next, in \cref{alg:bayes_unknown}, and analyze its regret in \cref{thm:bayes_unknown2}.
