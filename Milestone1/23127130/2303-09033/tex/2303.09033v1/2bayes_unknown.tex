\section{Bayesian Setting with Unknown Variance}
\label{sec:bayes_unknown2}
%See \cref{sec:ng-prior} for the setting and problem parameterization.
In this section, we deal with the problem setup with unknown variance, as described in \cref{sec:prob}. We again propose a  thompson sampling (TS) based algorithm to obtain variance-adaptive bounds for this case as well, however the assumptions and posterior updates are non-trivial owning to the nature of unknown variance. The complete algorithm is described in \cref{sec: bayes_known_algo} below. Further our bayesian regret analysis for this case again shows a regret bound similar to roughly $\tilde O(\sqrt{2n} \sqrt{\sum_{i =1}^K\sigma_i^2 \log(1/\delta)})$, which again scales according to individual reward-variance of the $K$-arms (see \cref{thm:bayes_unknown2} for a more formal bound). 



\input{algo_bayes_unknown.tex}

\begin{restatable}[Regret Analysis: Adaptive-Variance Bounds for Unknown Variance]{thm}{bayesunknown}
\label{thm:bayes_unknown2}
%\red{Incomplete, don't read this}
Let $\bmu$ and $\bsigma^2$ respectively denotes the vector of arm means and variance in a $K$-armed Gaussian bandit that are generated as $(\mu_{i},1/\sigma_i^2) \sim P_* = NG(\mu_{0,i}(i), \kappa_{0,i}, \alpha_{0,i}, \beta_{0,i})$ for all $i \in [K]$ and the reward of arm-$i$, $Y_{i,s}$ is sampled from $\cN(\mu_{i},\sigma_i^2)$ ($\mu_{0,i},\kappa_{0,i},\alpha_{0,i},\beta_{0,i}$ being the known prior parameters). Then assuming $A^*$ be the optimal arm under $\bmu$ and $A_t$ be the pulled arm in round $t$ by TS, for any $\delta > 0$,

\begin{align*}
  &\E{\sum_{t = 1}^n \mu_{A^*} - \mu_{A_t}}
  \leq
  \\
  & \bigg[ \sqrt{n}  \sqrt{\E{\sum_{i = 1}^K  
  \biggsn{ 
  (1+2\beta_{0,i} 
    + 2\sigma_i^2\sqrt{\ln \frac{1}{\delta'}} + 2\ln \frac{1}{\delta'} + \kappa_{0,i} [(\bar{x}_{t,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2 ])\ln \bigg(1 + \frac{n}{2\alpha_{0,i}\kappa_{0,i}} \bigg)
    } \ln \frac{1}{\delta}}}\bigg]
  \,,
\end{align*}

where $c(\delta) = 2 \sqrt{2 (\kappa_{0}\sigma_0)^{-2} \log(1 / \delta)} K + \sqrt{2 \sigma_0^2 / \pi} K n \delta$ %\red{needs some correction}.
\end{restatable}

\begin{proof}
We start by noing that the posterior updates (from results of \cite{murphy2007conjugate}): 

\begin{align*}
    & \mu_{t,i} = \frac{\kappa_{0,i}\mu_{0,i} + N_t(i)\bar{x}_{t,i}}{\kappa_{0,i}+N_t(i)}
    \\
    & \kappa_{t,i} = \kappa_{0,i} + N_t(i)
    \\
    & \alpha_{t,i} = \alpha_{0,i} + N_t(i)/2
    \\
    & \beta_{t,i} = \beta_{0,i} 
    +
    \frac{1}{2}\sum_{t \in [T]}\1(A_t = i)(x_{t,i} - \bar{x}_{t,i})^2
    +
    \frac{\kappa_{0,i} N_t(i) (\bar{x}_{t,i} - \mu_{0,i})^2 }{2(\kappa_{0,i} + N_t(i))},
\end{align*}
where $N_t(i):= \sum_{s = 1}^{t-1} \1(A_t = i)$ denotes the number of pulls of arm $i$ up to round $t$, and $\bar x_{t,i}:= \frac{1}{N_t(i)}\sum_{s = 1}^{t-1} \1(A_t = i)Y_{t,i}$ being the averaged empirical mean reward of arm-$i$ at round $t$. 
%
Thus we have: 

\[
\mu_{i} - \hat \mu_{t,i}  \mid (\kappa_t,\lambda_t) \sim \cN(0 ,(\kappa_{t,i} \lambda_{t,i})^{-1},
\]
where $\lambda_{t,i} \sim \text{Gamma}(\alpha_{t,i},\beta_{t,i})$. 

Same as proof of \cref{thm:bayes_known}, we can break the regret in round $t$ as before:
\begin{align}
\label{eq:term5}
  \E{\mu_{A^*} - \mu_{A_t}}
  & = \E{\condE{\mu_{A^*} - \mu_{A_t}}{H_t}} \nonumber \\
  & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} +
  \E{\condE{{{\hat{\mu}_{t,A_t}}}- \mu_{A_t}}{H_t}}\,,
  \nonumber \\
  & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} +
  \mathbb E\Bigsn{\mathbb E_{\bsigma}\bigsn{\mathbb E [\hat{\mu}_{t,A_t}- \mu_{A_t} \mid \bsigma ] \mid H_t} }\,, \nonumber
  \\
  & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} 
\end{align}

where $\sigma_{t,i}^2$ is defined as $\sigma_{t,i}^2:= \frac{1}{{\kappa_{t,i}\lambda_{t,i}}}$ which is a `sampled posterior variance', (where recall $\lambda_{t,i} \sim \text{Gam}(\alpha_{t,i},\beta_{t,i})$). The last equality holds since given $H_t$ and $\bsigma$,  $E[\mu_{i} \mid H_t] 
 = \hat{\mu}_{t,i}$ for any $i \in [K]$ by definition of the posterior update. %\red{check the argument}.

Now given $H_t$ and $\bsigma_t$, let us define $C_t(i) = \sqrt{2 \sigma_{t, i}^2 \log(1 / \delta)}$ -- the high-probability confidence interval of arm $i$ in round $t \in [K], ~\forall i \in [K]$. Now consider the event $E_t$:
\begin{align*}
  E_t
  = \set{\forall i \in [K]: \abs{\mu(i) -  \hat{\mu}_t(i)} \leq C_t(i)},
\end{align*}
which essentially indicates the `good event' when all the confidence intervals at round $t$ hold good. 
%

Further note, given $\cH_t$ and $\sigma_{i}^2$ (or equivalently $\lambda_{t,i})$),  $\mu_i - \hat{\mu}_{t,i} \mid \sigma_{t,i}^2, H_t \sim \cN(\mathbf{0}, \sigma_{t,i}^2)$, since $\sigma_t$ and $\sigma$ has same posterior. 
%\red{I am confused a bit}
Thus following same analysis as \eqref{eq:bayes regret scale}, we get: 
%So we w.h.p. $(1-\delta')$, we have \red{checK!}:

\begin{align}
  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{\bsigma_t, H_t}
  & \leq \sum_{i = 1}^K \frac{1}{\sqrt{2 \pi \sigma_{t, i}^2}}
  \int_{x = C_t(i)}^\infty x
  \exp\left[- \frac{x^2}{2 \sigma_{t, i}^2}\right] \dif x \nonumber \\
  & = \sum_{i = 1}^K - \sqrt{\frac{\sigma_{t, i}^2}{2 \pi}}  \int_{x = C_t(i)}^\infty \frac{\partial}{\partial x}
  \left(\exp\left[- \frac{x^2}{2 \sigma_{t, i}^2}\right]\right) \dif x  \nonumber \\
  & = \sum_{i = 1}^K \sqrt{\frac{\sigma_{t, i}^2}{2 \pi}} \delta.
  \label{eq:bayes_regret_scale2}
\end{align}

Now note, using \eqref{eq:bayes_regret_scale2}, we get:

\begin{align}
 \label{eq:bayes_regret_scale3}
\mathbb{E}_{H_t,\bsigma}&\Bigsn{\mathbb{E}_{\bsigma_t}\bigsn{\E{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}\mid \bsigma_t} \mid H_t}} 
	\leq \nonumber 
    \sum_{i = 1}^K \mathbb{E}_{H_t,\bsigma}\Bigsn{ \mathbb E_{\bsigma_{t}}[ \sqrt{\frac{\sigma_{t,i}^2}{2 \pi}} \mid H_t,\bsigma } ]\delta
    \\
    & = \frac{1}{\sqrt{2 \pi}}\sum_{i = 1}^K \mathbb{E}_{H_t,\bsigma}\Bigsn{ \mathbb E_{\bsigma_{t}}[ \sigma_{t,i} \mid H_t,\bsigma ]}\delta
    \\
    & = \sum_{i = 1}^K \mathbb{E}_{H_t,\bsigma}\Bigsn{ \mathbb E_{\lambda_{t,i} \sim \text{Gam}(\alpha_{t,i},\beta_{t,i})}{ \sqrt{\frac{1}{2 \kappa_{t,i}\lambda_{t,i}\pi}}} }\delta \nonumber
    \\
    &
    \leq \sum_{i = 1}^K \frac{\delta}{\sqrt{2\pi}}\mathbb{E}_{H_t,\bsigma}\Bigsn{ \sqrt{\frac{1}{ \kappa_{t,i}}}  \sqrt{\mathbb E_{\lambda_{t,i}} \biggsn{\frac{1}{\lambda_{t,i}} } } } \nonumber
    \\
    &
    = \sum_{i = 1}^K \frac{\delta}{\sqrt{2\pi}} \mathbb{E}_{H_t,\bsigma}\Bigsn{ \sqrt{\frac{1}{ \kappa_{t,i}}}
    \sqrt{\frac{\beta_{t,i}}{ (\alpha_{t,i} -1)}   }},
\end{align}
where the last equality holds by the fact that given $H_t$, $\frac{1}{\lambda_{t,i}}$ follows the \textit{Inverse-Gamma}$(\alpha_{t,i},\beta_{t,i}^{-1})$ distribution and \cref{lem:inv_gam} \cite{cook08}. 
%

Now given any fixed $\bsigma$ and $\delta' \in (0,1)$, for any $t$, let us define an event, 
\[
\cG_{t} := \bigg\{ \forall i \in [K]:  \frac{1}{N_\tau(i)-1}\sum_{t = 1}^\tau\I{A_t = i} \frac{(x_{t,i} - \bar x_{\tau, i})^2}{\sigma_{i}^2} \leq 1 + 2\sqrt{\frac{\ln (1/\delta')}{N_\tau(i)-1}} + \frac{2\ln (1/\delta')}{N_\tau(i)-1} \bigg \}.
\]

Then the regret expression from \eqref{eq:term5} can be further decomposed as:
\begin{align}
\label{eq:term2}
  &\condE{{\hat{\mu}_{t,A^*}}  - \mu_{A^*}}{H_t} \nonumber
  \\  
  &\leq \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{E_t}}{H_t} +
  \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t} \nonumber
  \\
  & = \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{E_t} \I{\cG_t}}{H_t} + \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{E_t} \I{\bar \cG_t}}{H_t}
  +\condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t}\nonumber
  \\
  & \leq \condE{C_t(A^*)\I{\cG_t}}{H_t} + \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar \cG_t}}{H_t}
  +\condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t}
  \,.
\end{align}

where the inequality follows from the observation that
\begin{align*}
  \condE{({\hat{\mu}_{t,A_t}} - \mu_{A_t}) \I{E_t}\I{\cG_t}}{H_t}
  \leq \condE{C_t(A_t)\I{\cG_t}}{H_t}\,.
\end{align*}

The remaining proof is to bounds the three terms of \eqref{eq:term2}.

{i. Bounding the third term in \eqref{eq:term2}:} This term can be bounded as in \eqref{eq:bayes_regret_scale3}. 

{ii. Bounding the second term in \eqref{eq:term2}:} For this term note that using \cref{lem:chi_conc}:
\begin{align*}
\condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar \cG_t}}{H_t} \leq \delta'
\end{align*}
%\red{need the max-bound}

{iii. Bounding the first term in \eqref{eq:term2}:} This case, we start by noting that 
\begin{align*}
\mathbb E_{\bmu, \bsigma, \bsigma_t}[C_t(A^*)\I{\cG_t} \mid H_t] =
\mathbb E_{\bsigma_t}[ \mathbb E_{\bmu, \bsigma} [C_t(A^*)\I{\cG_t} \mid \bsigma_t] \mid H_t] = \mathbb E_{\bsigma_t}[ \mathbb E_{\bmu, \bsigma} [C_t(A_t)\I{\cG_t} \mid \bsigma_t] \mid H_t], 
\end{align*}

where the last part follows from the fact that given $H_t$ and $\bsigma_t$, $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions. %\red{check, possibly conditioning on $\bsigma_t$ is redundant -- removed it}. 
%
Now we chain all inequalities for the regret in round $t$ and get:

\begin{align*}
  \E{\mu_{A^*} - \mu_{A_t}}
  \leq 2 \mathbb E_{H_t,\bmu,\bsigma}[C_t(A_t)] 
  + 
  \sum_{i = 1}^K \frac{\delta}{\sqrt{2\pi}} \mathbb{E}_{H_t,\bmu,\bsigma}\Bigsn{ \sqrt{\frac{1}{ \kappa_{t,i}}}
    \sqrt{\frac{\beta_{t,i}}{ (\alpha_{t,i} -1)}   }}\delta\,.
\end{align*}

Therefore, the $n$-round Bayes regret is bounded as
\begin{align*}
  \E{\sum_{t = 1}^n \mu_{A^*} - \mu_{A_t}}
  \leq 2 \mathbb E_{H_T,\bmu,\bsigma}[\sum_{t = 1}^n C_t(A_t)] 
  + 
  \sum_{t = 1}^n \sum_{i = 1}^K \frac{\delta}{\sqrt{2\pi}} \mathbb{E}_{H_T,\bmu,\bsigma}\Bigsn{ \sqrt{\frac{1}{ \kappa_{t,i}}}
    \sqrt{\frac{\beta_{t,i}}{ (\alpha_{t,i} -1)}   }}\delta\,.
\end{align*}


\begin{align*}
  &\E{\sum_{t = 1}^n C_t(A_t)}
  = \E{\sum_{t = 1}^n \sum_{i = 1}^K \1(A_t = i) C_t(i)} 
  = \E{\sum_{i = 1}^K \bigg[ \sum_{t = 1}^n \1(A_t = i) C_t(i) \bigg]}
  \\
  &\overset{(1)}{\leq} \E{\sum_{i = 1}^K \bigg[ \sqrt{N_n(i)} \sqrt{\sum_{t = 1}^n C_t^2(i)\1(A_t = i) }\bigg]}
  \\
  &\overset{(2)}{\leq} \E{ \bigg[ \sqrt{\sum_{i = 1}^K N_n(i)} \sqrt{\sum_{i = 1}^K \sum_{t = 1}^n C_t^2(i)\1(A_t = i) }\bigg]}
  \\
  &\overset{(3)}{\leq}  \bigg[ \sqrt{\sum_{i = 1}^K N_n(i)} \sqrt{\E{ \sum_{i = 1}^K \sum_{t = 1}^n C_t^2(i)\1(A_t = i) }\bigg]}
  \\
  &\overset{}{=}  \bigg[ \sqrt{n} \sqrt{\E{ \sum_{i = 1}^K \sum_{t = 1}^n C_t^2(i)\1(A_t = i) }\bigg]}
  \\
  & \overset{(4)}{\leq} \bigg[ \sqrt{n} \sqrt{{ \sum_{i = 1}^K \sum_{s = 1}^n E_{H_\tau,\bmu,\bsigma}\biggsn{
    \frac{\Bigsn{2\beta_{0,i} 
    +
    \sigma_i^2 (s-1)
    +
    \kappa_{0,i} [\frac{\sigma_{i}^2}{s} + (\mu_{i} - \mu_{0,i})^2  ]  }}
    {2(\kappa_{0,i} + s)(\alpha_{0,i} + s/2 -1)} \mid N_\tau(i)} }\bigg]}
  \\
  & \leq \bigg[ \sqrt{n} \sqrt{\E{ \sum_{i = 1}^K \sum_{t = 1}^n C_t^2(i)\1(A_t = i) }\bigg]}
  \\
  %%
\end{align*}

where $(1)$ and $(2)$ uses Cauchy-Schwarz, $(2)$ applies Jensen's inequality, the Inequality $(4)$ above follows from \cref{lem:gam_conc}, and the second last inequality follows from \cref{lem:reciprocal sum}.

\iffalse %%%%%%%%%%%%%%%

&\overset{(4)}{\leq}  \E{\sum_{i = 1}^K \bigg[ \sqrt{N_n(i)}  \sqrt{\sum_{s = 1}^n  
  \biggsn{ 
  \frac{2\beta_{0,i} 
    +
    \sigma_i^2(s-1 + 2\sqrt{\ln \frac{1}{\delta'}(s-1)} + 2\ln \frac{1}{\delta'})
    +
    \kappa_{0,i} [(\bar{x}_{t,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]  }{(\kappa_{0,i} + s)(2\alpha_{0,i} + s - 2)}
  } \ln \frac{1}{\delta}\1(A_t = i)}}\bigg]
  \\ 
  &\leq \E{\sum_{i = 1}^K \bigg[ \sqrt{N_n(i)}  \sqrt{  
  \biggsn{ 
  (1+2\beta_{0,i} 
    + 2\sqrt{\ln \frac{1}{\delta'}} + 2\ln \frac{1}{\delta'} + \kappa_{0,i} [(\bar{x}_{t,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2 ])\ln \bigg(1 + \frac{n}{2\alpha_{0,i}\kappa_{0,i}} \bigg)
    } \ln \frac{1}{\delta}}}\bigg]
    \\
    &\leq \bigg[ \sqrt{n}  \sqrt{\E{\sum_{i = 1}^K  
  \biggsn{ 
  (1+2\beta_{0,i} 
    + 2\sqrt{\ln \frac{1}{\delta'}} + 2\ln \frac{1}{\delta'} + \kappa_{0,i} [(\bar{x}_{t,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2 ])\ln \bigg(1 + \frac{n}{2\alpha_{0,i}\kappa_{0,i}} \bigg)
    } \ln \frac{1}{\delta}}}\bigg]

\red{old bound with precision lower bounds - reject below!}

\begin{align*}
  \E{\sum_{t = 1}^n C_t(A_t)}
  & = \E{\sum_{t = 1}^n \sum_{i = 1}^K \1(A_t = i) C_t(i)} 
  = \E{\sum_{i = 1}^K \bigg[ \sum_{t = 1}^n \1(A_t = i) C_t(i) \bigg]}
  \\
  &\leq \E{\sum_{i = 1}^K \bigg[ \sqrt{N_t(i)} \sqrt{\sum_{t = 1}^n C_t^2(i)\1(A_t = i) }\bigg]}
  \\
  &=  \E{\sum_{i = 1}^K \bigg[ \sqrt{N_t(i)}  \sqrt{\sum_{t = 1}^n \frac{1}{\lambda_{0,i}\big(\kappa_{0,i} + n\big)} \log(1 / \delta)\1(A_t = i)}\bigg]}
  \\
  &=  \E{\sum_{i = 1}^K \bigg[ \sqrt{2N_t(i)\lambda_{0,i}^{-1}}  \sqrt{\sum_{s = 1}^{N_n(i)} \frac{1}{\kappa_{0,i} + s} \log(1 / \delta) + \frac{1}{\kappa_{0,i}}\log(1/\delta)}\bigg]}
  \\
  &\leq \E{\sum_{i = 1}^K \bigg[ \sqrt{2N_t(i)(\lambda_{0,i}^{-1})}  \sqrt{\sum_{s = 1}^{n} \frac{1}{\kappa_{0,i} + s} \log(1 / \delta) + \frac{1}{\kappa_{0,i}}\log(1/\delta)}\bigg]}
  \\	  
  & \overset{(a)}{\leq} \E{\sum_{i = 1}^K \bigg[ \sqrt{2N_t(i)\lambda_{0,i}^{-1}}  \sqrt{\Big(\log(1 + n\kappa_{0,i}) + \frac{1}{\kappa_{0,i}}\Big)\log(1/\delta)}\bigg]}
  \\
  &= \E{\sum_{i = 1}^K \bigg[ \sqrt{2N_t(i)}  \sqrt{\lambda_{0,i}^{-1}\Big(\log(1 + n\kappa_{0,i}) + \frac{1}{\kappa_{0,i}}\Big)\log(1/\delta)}\bigg]}
  \\
  & \leq  \sqrt{2n} \sqrt{\sum_{i =1}^K\lambda_{0,i}^{-1}\Big(\log(1 + n\kappa_{0,i}) + \frac{1}{\kappa_{0,i}}\Big)\log(1/\delta)}
  \,.
\end{align*}
\fi %%%%%%%%%%%%%%%%%%%%%

\end{proof}




\iffalse %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Since $\lambda \sim Gamma(\alpha_{0,i},\beta_{0,i})$, we have:

\red{not true if $\lambda_{0,i} \geq $ (mode)}

\begin{align*}
	P(\I{\bar F}) \leq \lambda_{0,i} f(\lambda_{0,i}) & \leq \delta' 
	\\
	\implies &
	\frac{\beta_{0,i}^{\alpha_{0,i}}}{\Gamma(\alpha_{0,i}) }\lambda_{0,i}^{\alpha_{0,i}} e^{-\beta_{0,i} \lambda_{0,i}} \leq \delta'
	\\
	\implies & 
	\log\big( \frac{\beta_{0,i}^{\alpha_{0,i}}}{\Gamma(\alpha_{0,i}) }\big) + \alpha_{0,i}\log \lambda_{0,i} - \lambda_{0,i}\beta_0 \leq \log (\delta')
	\\
	\implies &
	\lambda_{0,i} \geq \frac{1}{\beta_{0,i}}\log \bigg( \frac{\beta_{0,i}^{\alpha_{0,i}}}{\Gamma(\alpha_{0,i}) \delta'} \bigg) + \frac{\alpha_{0,i}}{\beta_{0,i}} \log \lambda_{0,i} 
\end{align*}

We can now choose $\lambda_{0,i}$ based on Lem 18 of \cite{SKM21}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Additional Claims used in the Proof of \cref{thm:bayes_unknown2}} 

\begin{restatable}[\cite{cook08}]{lem}{invgam}
\label{lem:inv_gam}
Suppose $X$ follows a Gamma$(\alpha,\beta)$ distribution with shape parameter $\alpha>0$ and rate parameter $\beta > 0$, then $1/X$ follows an Inverse-Gamma$(\alpha,1/\beta)$ distribution. 
\\
Moreover if $\alpha>1$, $E[1/X] = \frac{\beta}{\alpha-1}$. 
\end{restatable}


\begin{restatable}[\cite{cook08}]{lem}{avgchi}
\label{lem:avg_chi}
Let ${\displaystyle X_{1},...,X_{n}}$ are i.i.d. ${\displaystyle N(\mu ,\sigma ^{2})}$ random variables, (a). then 
\[
{\displaystyle \sum _{i=1}^{n}\frac{(X_{i}-{\bar {X}_n})^{2}}{\sigma ^{2}}\sim \chi _{n-1}^{2}},
\]
where ${\displaystyle {\bar {X}_n}={\frac {1}{n}}\sum _{i=1}^{n}X_{i}}$ is the sample mean of $n$ random draws, and $\chi_{d}^2$ is the chi-squared random variable with $d$-degrees of freedom (for any $d \in \N_+$). 
\\
(b). Further if $X \sim \chi_d^2$, then $E[X] = d$.
\end{restatable}

\begin{restatable}[]{lem}{varbdd}
\label{lem:var_bdd}
For any $i \in [K]$, and round $\tau \in [T]$,
\begin{align*}
  E_{H_\tau,\bmu,\bsigma}&\biggsn{\frac{\beta_{\tau,i}}{ (\kappa_{\tau,i})(\alpha_{\tau,i} -1)} \mid N_{\tau,i}} 
    \leq  
        E_{H_\tau,\bmu,\bsigma}\biggsn{
    \frac{\Bigsn{2\beta_{0,i} 
    +
    \sigma_i^2 (N_{\tau}(i)-1)
    +
    \kappa_{0,i} [\frac{\sigma_{i}^2}{N_{t,i}} + (\mu_{i} - \mu_{0,i})^2  ]  }}
    {2(\kappa_{\tau,i})(\alpha_{0,i} + N_{\tau}(i)/2 -1)} \mid N_\tau(i)}
\end{align*}
\end{restatable}

\begin{proof}
We start by noting that for any $i \in [K]$, 
\begin{align*}
 \mathbb{E}_{H_\tau,\bsigma}&\Bigsn{ \mathbb E_{\bsigma_{\tau}}[ \sigma_{\tau,i} \mid H_{\tau},\bsigma ]}
	\\
	& = 
 \mathbb{E}_{H_{\tau},\bsigma}\Bigsn{ \mathbb E_{\lambda_{\tau,i} \sim \text{Gam}(\alpha_{\tau,i},\beta_{\tau,i})}{ \sqrt{\frac{1}{ \kappa_{\tau,i}\lambda_{\tau,i}}}} \mid H_{\tau},\bsigma}
    \\
    &
    \leq \mathbb{E}_{H_{\tau},\bsigma}\Bigsn{ \sqrt{\frac{1}{ \kappa_{\tau,i}}}  \sqrt{\mathbb E_{\lambda_{\tau,i}} \biggsn{\frac{1}{\lambda_{\tau,i}} } } \mid H_{\tau},\bsigma} 
    \\
    &
    = \mathbb{E}_{H_{\tau},\bsigma}\Bigsn{ \sqrt{\frac{1}{ \kappa_{\tau,i}}}
    \sqrt{\frac{\beta_{\tau,i}}{ (\alpha_{\tau,i} -1)}   } \mid H_{\tau},\bsigma},
\end{align*}
where the last equality holds by the fact that given $H_{\tau}$, $\frac{1}{\lambda_{t,i}}$ follows the \textit{Inverse-Gamma}$(\alpha_{\tau,i},\beta_{\tau,i}^{-1})$ distribution and \cref{lem:inv_gam} \cite{cook08}. Now from the posterior update rules, given $H_{\tau}$ we have:
\begin{align*}
\frac{\beta_{\tau,i}}{ (\alpha_{\tau,i} -1)} 
    & \leq 
    \frac{\beta_{0,i} 
    +
    \frac{1}{2}\sum_{t \in [\tau]}\1(A_t = i)(x_{t,i} - \bar{x}_{\tau,i})^2
    +
    \frac{\kappa_{0,i} N_t(i) (\bar{x}_{\tau,i} - \mu_{0,i})^2 }{2(\kappa_{0,i} + N_{\tau}(i))} }{(\alpha_{0,i} + N_{\tau}(i)/2 -1)}
    \\
    & \leq 
    \frac{2\beta_{0,i} 
    +
    \sum_{t \in [\tau]}\1(A_t = i)(x_\tau(i) - \bar{x}_{\tau,i})^2
    +
    \kappa_{0,i} [(\bar{x}_{\tau,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]  }
    {2(\alpha_{0,i} + N_{\tau}(i)/2 -1)}
\end{align*}

Now given any fixed reward-precision $\lambda_i \sim \text{Gam}(\alpha_{0,i},\beta_{0,i})$, $\sigma_{i}^2:= \frac{1}{\lambda_{i}}$, mean-reward $\mu_i \sim \cN(\mu_{0,i},\sigma_{0,i}^2)$, and fixed number of pulls $N_\tau(i) = s$ (say), note $x_{t,i} \overset{iid}{\sim} \cN(\mu_{i}, \sigma_{i}^2)$ can be seen as $N_\tau(i) = s$ independent and identical draws from $\cN(\mu_i,\sigma_i^2)$. Thus
$\sum_{t = 1}^\tau\I{A_t = i} \frac{(x_{t,i} - \bar x_{\tau,i})^2}{\sigma_{i}^2} = \sum_{t = 1}^s\frac{(x_{t,i} - \bar x_{\tau,i})^2}{\sigma_{i}^2}$ follows $\chi^2(s-1)$, i.e. chi-squared distribution with $s-1$ degrees of freedom (see \cref{lem:avg_chi}). Further using \cref{lem:avg_chi}, part-(b) again, we know 
\[
E_{x_{t,i} \sim \cN(\mu_i,\sigma_i^2)} \biggsn{\sum_{t = 1}^s\frac{(x_{t,i} - \bar x_{\tau,i})^2}{\sigma_{i}^2}} = (s-1).
\]

Applying this to above chain of equations, note we get:

\begin{align*}
& E_{H_\tau,\bmu,\bsigma}\biggsn{\frac{\beta_{\tau,i}}{ (\kappa_{\tau,i})(\alpha_{\tau,i} -1)} \mid N_{\tau}(i)}
	\\    
    & = 
    E_{H_\tau,\bmu,\bsigma}\biggsn{
    \frac{{2\beta_{0,i} 
    +
    E_{x_{t,i}\sim \cN(\mu_i,\sigma_i^2)}\biggsn{\sigma_i^2 \sum_{t = 1}^{N_{\tau}(i)}(\frac{x_{t,i} - \bar{x}_{\tau,i}}{\sigma_i})^2 \mid \mu_i, \sigma_{i}}
    +
    \kappa_{0,i} [(\bar{x}_{\tau,i} - \mu_{i})^2 + (\mu_{i} - \mu_{0,i})^2  ]  }}
    {2(\kappa_{\tau,i})(\alpha_{0,i} + N_{\tau}(i)/2 -1)} \mid N_\tau(i)}
    \\
    & = 
    E_{H_\tau,\bmu,\bsigma}\biggsn{
    \frac{\Bigsn{2\beta_{0,i} 
    +
    \sigma_i^2 (N_{\tau}(i)-1)
    +
    \kappa_{0,i} \big[E_{\bar x_{\tau,i} \overset{\text{iid}}{\sim} \cN(\mu,\sigma^2/N_\tau(i))}[(\bar{x}_{\tau,i} - \mu_{i})^2 \mid \mu_i,\sigma_i] + (\mu_{i} - \mu_{0,i})^2  \big]  }}
    {2(\kappa_{\tau,i})(\alpha_{0,i} + N_{\tau}(i)/2 -1)} \mid N_\tau(i)}
    \\
    & \overset{(1)}{=} 
    E_{H_\tau,\bmu,\bsigma}\biggsn{
    \frac{\Bigsn{2\beta_{0,i} 
    +
    \sigma_i^2 (N_{\tau}(i)-1)
    +
    \kappa_{0,i} [\frac{\sigma_{i}^2}{N_{t,i}} + (\mu_{i} - \mu_{0,i})^2  ]  }}
    {2(\kappa_{\tau,i})(\alpha_{0,i} + N_{\tau}(i)/2 -1)} \mid N_\tau(i)}
\end{align*}
where (1) again follows since given $\mu_i, \sigma_i$ and $N_\tau(i)$ as before, $x_{t,i} \overset{\text{iid}}{\sim} \cN(\mu,\sigma^2)$, and thus by definition 
$\bar x_{\tau, i} = \frac{1}{N_{\tau}(i)} \sum_{t = 1}^\tau \I{A_t = i} x_{t,i} \sim \cN\big(\mu, \frac{\sigma^2}{N_{\tau}(i)}\big)$> This leads to $E_{\bar x_{\tau,i} \overset{\text{iid}}{\sim} \cN(\mu,\sigma^2/N_\tau(i))}[\bar x_{\tau, i} - \mu_i]^2 = \frac{\sigma_i^2}{N_{\tau}(i)}$. Thus the claim is concluded. 
\end{proof}
%Also further note that, given any fixed $\bsigma$ s.t. $\sigma_i^2:= 1/\lambda_i$ where $\lambda_i \sim \text{Gam}(\alpha_{0,i},\beta_{0,i})$.

\iffalse%%%%%%%%%%%%%%%%%%

\begin{restatable}[\red{chi-sqr confidence, not used} Lemma 1, \cite{massart}]{lem}{chiconc}
\label{lem:chi_conc}
Suppose $X$ is a chi-squared distribution with $D$ degrees of freedom, for some $D \in \N_+$, then for any $\delta \in (0,1)$, with probability at least $1-\delta$,
\begin{align*}
   X \leq D + 2\sqrt{{D\ln (1/\delta)} } + 2\ln (1/\delta)
\end{align*}
\end{restatable}


\begin{restatable}[\red{not used, needed if use precision lower bound}]{lem}{preclb}
\label{lem:prec_lb}
Let $X \sim Gamma(\alpha,\beta)$. Then given any $\delta \geq \frac{1}{\exp(\alpha)}$, w.h.p. at least $(1-\delta)$,
\[
X > \frac{\alpha}{e \beta}
\]
\end{restatable}


\begin{proof}
We start by noting that, for any $x_0 \in R_+$ and some $t>0$, applying Markov's Inequality we get:

\begin{align*}
\P(X < x_0) & = \P(-tX > -tx_0) = \P(\exp(-tX) > \exp(-x_0))
\\
& = \frac{\E{\exp(-tX)}}{\exp(-tx_0)}.
\end{align*}

Moreover, using gamma-pdf, we get:
\begin{align*}
\E{\exp(-tX)} & = \frac{\beta^\alpha}{\Gamma(\alpha)}\int_{x = 0}^{\infty}\exp(-tX)\exp(-\beta X)x^{\alpha -1}dx 
= \frac{\beta^\alpha}{\Gamma(\alpha)}\int_{x = 0}^{\infty}\exp(-(t+\beta)x)x^{\alpha -1}dx 
\\
& = \frac{\beta^\alpha}{\Gamma(\alpha)}\int_{u = 0}^{\infty}\exp(-u)\biggn{\frac{u}{t+\beta}}^{\alpha -1}\frac{du}{t+\beta}
\\
& = \frac{\beta^\alpha}{\Gamma(\alpha) (t+\beta)^\alpha}\int_{u = 0}^{\infty}\exp(-u)u^{\alpha-1}du = \frac{\beta^\alpha}{(t+\beta)^\alpha}.
\end{align*}

Since we want $\P(X < x_0) < \delta$, the goal is to find the threshold $x_0$  such that: 
\begin{align*}
\min_{t} \frac{\beta^\alpha \exp(tx_0)}{(t+\beta)^\alpha} < \delta.
\end{align*}

It is easy to check the minimum of the above expression is attained at $t = \frac{\alpha}{x_0} - \beta$, given $x_0 < \frac{\alpha}{\beta}$ since by definition we need $t>0$.

Combining the above two equations, we get:
\begin{align*}
\bigg(\frac{\beta}{\alpha}\bigg)^{\alpha} \frac{e^{\alpha - \beta x_0}}{x_0^\alpha} < \delta,
\end{align*}

 which implies to choose $x_{0}$ such that:

\[
x_0 \geq \frac{\alpha}{\beta} + \frac{\alpha}{\beta}\ln\Big( x_{0} \frac{\beta}{\alpha} \Big) + \frac{\ln (1/\delta)}{\beta} \text{ s.t. } x_{0} > \frac{\alpha}{\beta}.
\]

Finally, the claim holds noting that the above implies we can choose $x_0$ to be $\frac{\alpha}{e\beta}$, which satisfies the above inequality as long as $\ln (1/\delta) \leq \alpha$.

\iffalse%%%%%%%%%%%%%%%%
Or more loosely, it is enough to choose $x_0$ such that:

\[
x_0 \geq 1 + \frac{\ln (1/\delta)}{\beta} + \frac{\alpha}{\beta}\ln x_0 \text{ s.t. } x_{0} > \frac{\alpha}{\beta}.
\]

Finally note that above can be satisfied for any $x_0 > 2\bign{ 1 + \frac{\ln 1/\delta}{\beta} + \big(\frac{\alpha}{\beta}\big)^2 }$
by \cref{lem:tvslogt} where in our case $A = 1 + \frac{\ln 1/\delta}{\beta}$ and $B = \frac{\alpha}{\beta}$.
\fi %%%%%%%%%%%%%%%%%%%%%%%%
%We finally note that above can be satisfied setting $x_{0} = \frac{\alpha}{e \beta}$, which satisfies the above relation assuming $\alpha/e \geq \ln(1/\delta)$. 
\end{proof}

\begin{restatable}[(\red{Ignore - Not Used})]{lem}{tvslogt}
\label{lem:tvslogt}
For any $x > 0$ and $A> 0, B > 1$, $x > A + B\ln x$ for any $x \ge 2A + 2B^2$.
\end{restatable}

\begin{proof}
This follows from the calculation as shown below:
\begin{align*}
	A + B\ln x & = A + B\ln (2A + 2B^2 )\\
	& < A + B\ln\big(2B^2 (1 + A/B^2)\big)\\
	& < A + B\ln 2B^2 + A/B ~~~~\text{as } \ln(1+x) \leq x\\	
	& = A + A/B + 2B^2\\
	& \le 2A + 2B^2 = x,
\end{align*}
where the last inequality follows since $B > 1$.
\end{proof}

\fi%%%%%%%%%%%%%%%%%%%%%%%