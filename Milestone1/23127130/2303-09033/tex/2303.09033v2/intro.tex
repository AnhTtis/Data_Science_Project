%\vspace{-3pt}
\section{Introduction}
\label{sec:introduction}

A \emph{stochastic bandit} \citep{lai85asymptotically,auer02finitetime,lattimore19bandit} is an online learning problem where a \emph{learning agent} sequentially interacts with an environment over $n$ rounds. In each round, the agent pulls an \emph{arm} and receives a \emph{stochastic reward}. The mean rewards of the arms are initially unknown and the agent learns them by pulling the arms. Therefore, the agent faces an \emph{exploration-exploitation dilemma} when pulling the arms: \emph{explore}, and learns more about the arms; or \emph{exploit}, and pull the arm with the highest estimated reward. An example of this setting is a recommender system, where the arm is a recommendation and the reward is a click.

Most bandit algorithms assume that the reward variance or it is upper bound is known. For instance, the confidence intervals in \ucb \citep{auer02finitetime} are derived under the assumption that the rewards are $[0, 1]$, and hence $\sigma^2$-sub-Gaussian for $\sigma = 0.5$. In Bernoulli \klucb \citep{garivier11klucb} and Thompson sampling (TS) \citep{agrawal12analysis}, tighter confidence intervals are derived for Bernoulli rewards. Specifically, a Bernoulli random variable wither either a low or high mean also has a low variance. In general, the reward variance is hard to specify \citep{audibert09}. \emph{While overestimating it is typically safe, this decreases the learning rate of the bandit algorithm and increases regret. On the other hand, when the variance is underestimated, this may lead to linear regret because the algorithm commits to an arm without sufficient evidence.}

We motivate learning of reward variances by the following example. Take a movie recommender that learns to recommend highest rated movies in a \say{Trending Now} carousel. The movies are rated on scale $[1, 5]$. Some movies, such as The Godfather, are classics. Therefore, their ratings are high on average and have low variance. On the other hand, ratings of low-budget movies are often low on average and have low variance, due to the quality of the presentation. Finally, most movies are made for a specific audience, such as Star Wars, and thus have a high variance in ratings. Clearly, any sensible learning algorithm would require a lot less queries to estimate the mean ratings of movies with low variances. Since the variance is unknown a priori, adaptation is necessary. This would reduce the overall query complexity and improve statistical efficiency--as one should--because we only pay for what is uncertain. This example is not limited to movies and applies to other domains, such as online shopping. Our work answers the following questions in affirmative:

\begin{center}
\emph{
Can we quickly learn the right representation of the reward distribution for efficient learning? What is the right trade-off of the learner's performance (regret) versus the prior parameters and reward variances? Can we design an algorithm to achieve that rate? Does the regret decrease with lower reward variances and more informative priors on them?}
\end{center}

Unknown reward variances are a major concern and thus have been studied extensively. In the cumulative regret setting, \citet{audibert09} proposed an algorithm based on upper confidence bounds (UCBs) and \citet{mukherjee18} proposed an elimination algorithm. In best-arm identification (BAI) \citep{audibert09exploration,bubeck09pure}, several papers studied the fixed-budget \citep{gabillon11multibandit,faella20kernel,SGV20,lalitha2023fixed} and fixed-confidence \citep{lu21variancedependent,zhou22approximate,jourdan22dealing} settings with unknown reward variances. All above works studied frequentist algorithms. On the other hand, Bayesian algorithms based on posterior sampling \citep{thompson33likelihood,chapelle11empirical,agrawal12analysis,russo14learning,russo18tutorial,kveton21metathompson,hong22hierarchical} perform well in practice, but learning of reward variances in these algorithms is understudied. Our problem setup and Bayesian objective are presented in \cref{sec:prob}. Our contributions are summarized below.

\textbf{Contributions:} \textbf{(1)} To warm up, we study Thompson sampling in a $K$-armed Gaussian bandit with \emph{known heterogeneous reward variances} and bound its Bayes regret (\cref{sec:bayes_known}). Our regret bound (\cref{thm:bayes_known}) decreases as reward variances decrease. It also approaches zero as the prior variances of mean arm rewards go to zero. In this case, a Bayesian learning agent knows the bandit instance with certainty. \textbf{(2)} We propose a Thompson sampling algorithm \varts for a $K$-armed Gaussian bandit with \emph{unknown heterogeneous reward variances} (\cref{sec:bayes_unknown2}). \varts maintains a joint Gaussian-Gamma posterior for the mean and precision of the rewards of all arms and samples from them in each round. 
\textbf{(3)} We prove a Bayes regret bound for \varts (\cref{thm:bayes_unknown2}), which decreases with lower reward variances and more informative priors on them. This is the first regret bound of its kind. The novelty in our analysis is in handling random confidence interval widths due to random reward variances. The resulting regret bound captures the same trade-offs as if the variance was known, replaced by the corresponding prior-dependent quantities. \textbf{(4)} We comprehensively evaluate \varts on various types of reward distributions, from Bernoulli to beta to Gaussian. Our evaluation shows that \varts outperforms all existing baselines, even with an estimated prior (\cref{sec:experiments}). This showcases the generality and robustness of our method.

%\vspace{-4pt}
\section{Related Work}
\label{sec:rel}
%\vspace{-4pt}

The beginnings of variance-adaptive algorithms can be traced to \citet{auer02finitetime}. \citet{auer02finitetime} proposed a variance-adaptive \ucb, called \ucbnormal, for Gaussian bandits where the reward distribution of arm $i$ is $\cN(\mu_i, \sigma_i^2)$ and $\sigma_i > 0$ is assumed to be known. The $n$-round regret of this algorithm is $O\left(\sum_{i: \mu_i < \mu_{a^*}} \frac{\sigma_i^2}{\Delta_i} \log n\right)$, where $a^*$ is the arm with the highest mean reward $\mu_i$. The first UCB algorithm for unknown reward variances with an analysis was \ucbv by \citet{audibert09}. The key idea in the algorithm is to design high-probability confidence intervals based on empirical Bernstein bounds. The $n$-round regret of \ucbv is $O\left(\sum_{i: \mu_i < \mu_{a^*}} \left(\frac{\sigma_i^2}{\Delta_i} + b\right) \log n\right)$, where $b$ is an upper bound on the absolute value of the rewards. In summary, variance adaptation in \ucbv incurs only a small penalty of $O(b K \log n)$. \citet{mukherjee18} proposed an elimination-based variant of $\ucbv$ that attains the optimal gap-free regret of $O(\sqrt{K n})$, as opposing to the original $O(\sqrt{K n \log n})$. While empirical Bernstein bounds are general, they tend to be conservative in practice. This was observed before by \citet{garivier11klucb} and we observe the same trend in our experiments (\cref{sec:experiments}). Our work can be viewed as a similar development with Thompson sampling. We show that Thompson sampling with unknown reward variances (\cref{sec:bayes_unknown2}) incurs only a slightly higher regret than the one with known variances (\cref{sec:bayes_known}), by a multiplicative factor. Compared to \ucbv, the algorithm is highly practical.

Two closest papers to our work are \citet{honda14optimality,MV20}. Both papers propose variance-adaptive Thompson sampling and bound its regret. There are three key differences from our work. First, the algorithms of \citet{honda14optimality,MV20} are designed for the frequentist setting. Specifically, they have a fixed sufficiently-wide prior, and enjoy a per-instance regret bound under this prior. While this is a strong guarantee, the algorithms can perform poorly when priors are narrower and thus more informative. Truly Bayesian algorithm designs, as proposed in our work, can be analyzed for any informative prior. Second, the analyses of \citet{honda14optimality,MV20} are frequentist. Therefore, they cannot justify the use of more informative priors. In contrast, we prove regret bounds that decrease with lower reward variances and more informative priors on them. Finally, the regret bounds of \citet{honda14optimality,MV20} are asymptotic. We provide strong finite-time guarantees. We discuss these difference in more detail after \cref{thm:bayes_unknown2} and demonstrate them empirically in \cref{sec:experiments}.

Another related line of works are variance-dependent regret bounds for $d$-dimensional linear contextual bandits \citep{kim22,quan22,quan23,zhang21}. These works address the problem of time-dependent variance adaptivity. They derive frequentist regret bounds that scale as $\tilde O(\text{poly}(d)\sqrt{1 + \sum_{s = 1}^n \sigma_s^2})$, where $\sigma_s^2$ is an unknown reward variance in round $s$. This setting is different from ours in two aspects. First, they study changing reward variances over time but keep them fixed across the arms. We do the opposite in our work. Second, their algorithm designs and analyses are frequentist, and thus cannot exploit prior knowledge. On the other hand, we focus only on $K$-armed bandits, which is a special case of linear bandits.
