\section{Gaussian Bandit with Unknown Variances}
\label{sec:bayes_unknown2}

Our main contribution is that we consider the Bayesian setting with Gaussian rewards and unknown heterogeneous reward variances. Similarly to \cref{sec:bayes_known}, we propose a Thompson sampling algorithm for this setting in \cref{sec:bayes_unknown_algo}. 
%The algorithm is novel and generalizes even that of \citet{honda14optimality}, which is the closest related work in the frequentist setting. \red{Add \cite{MV20,quan23}...}. 
We discuss the regret bound in \cref{sec:unknown_reg}. 
%, and sketch its proof in \cref{sec:unknown_reg_prf}
The regret bound scales roughly as:
$%\begin{align*}
  \textstyle
  \sqrt{n \log n}\sqrt{\sum_{i = 1}^K \frac{\beta_{0,i}}{\alpha_{0,i}-1}
  \log\Big( 1+\frac{n}{\kappa_{0,i}}\Big)}\,,
$%\end{align*}
where $\frac{\beta_{0, i}}{\alpha_{0, i} - 1}$ represents a proxy for the reward variance $\sigma_i^2$ in \eqref{eq:rough bound known} and $\kappa_{0, i}^{-1}$ plays the role $\sigma_{0, i}^2/\sigma_i^2$. Since the dependencies are analogous, the bound captures the structure of the problem similarly to \eqref{eq:rough bound known}. \emph{Our main novelty lies in handling the uncertainty of reward variances $\bsigma^2$, which is unique among all existing TS proofs.}

\input{algo_bayes_unknown.tex}

\vspace{-5pt}
\subsection{Regret Analysis}
\label{sec:unknown_reg}

Recall from \cref{sec:prob}, here the bandit instance $(\bmu, \bsigma)$ is sampled from a Gaussian-Gamma distribution: For any arm $i$, the mean and variance of its rewards are sampled as $(\mu_i, \sigma_i^{-2}) \sim NG(\mu_{0, i}, \kappa_{0, i}, \alpha_{0, i}, \beta_{0, i})$, where $(\bmu_0, \bkappa_0, \balpha_0, \bbeta_0)$ are known prior parameters. This can also be seen as first sampling $\sigma_i^{-2} \sim \text{Gam}(\alpha_{0,i},\beta_{0,i})$ and then $\mu_i \sim \cN(\mu_{0,i}, \frac{\sigma_i^2}{\kappa_{0,i}})$. Our regret bound shows:% presented below.

\begin{restatable}[Variance-dependent regret bound for unknown variances]{thm}{bayesunknown}
\label{thm:bayes_unknown2} Consider the above setting and let $\alpha_{0,i }\geq 1$ for all arms $i \in [K]$. Then for any $\delta > 0$, the Bayes regret of \varts is bounded as:
$%\begin{align*}
  R_n
  \leq C\sqrt{n \log(1 / \delta)} + \delta C \sqrt{\frac{nK}{2\pi}}\,,
$%\end{align*}
where $C^2 = \sum_{i = 1}^K \frac{\beta_{0,i}}{\alpha_{0,i} - 1} \biggn{ \frac{2}{\kappa_{0,i}} + \frac{0.5}{\kappa_{0,i}(\alpha_{0,i}-1)} + 5\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big) }$ is a constant dependent on prior parameters.
\end{restatable}

\textbf{Proof of \cref{thm:bayes_unknown2}.} The difficulty lies in tightly bounding confidence intervals of random reward means with unknown reward variances, to get the right dependence on reward variances $\frac{\beta_{0, i}}{\alpha_{0, i} - 1}$ and mean reward variances $\frac{1}{\kappa_{0, i}}$. This is algebraically challenging due to the complicated nature of the posterior updates of Gaussian-Gamma distributions, as presented in \cref{alg:bayes_unknown}. To overcome these difficulties, we carefully condition random variables on each other together with appropriate histories, and combine these using Jensen's and Cauchy-Schwarz inequalities. The key lemmas along with the complete proof of \cref{thm:bayes_unknown2} are in \cref{app:unknown}.

\textbf{Dependence on all parameters of interest and prior.} For $\delta = 1 / n$, the bound in \cref{thm:bayes_unknown2} is $\tilde{O}(\sqrt{C n})$. The dependence on $\sqrt{n}$ is the same as in \cref{thm:bayes_known}. Further, a closer examination of $C$ reveals many similarities: 

First, $\beta_{0, i} / (\alpha_{0, i} - 1)$ is the mean of an Inverse-Gamma distribution with parameters $(\alpha_{0, i}, \beta_{0, i})$. Since the precision of the reward distribution of arm $i$, $\lambda_i$, is sampled from $\text{Gam}(\alpha_{0, i}, \beta_{0, i})$, we have that $\beta_{0, i} / (\alpha_{0, i} - 1)$ is the mean of the reward variance distribution of arm $i$. Thus $\beta_{0, i} / (\alpha_{0, i} - 1)$ in \cref{thm:bayes_unknown2} plays the role of $\sigma_i^2$ in \cref{thm:bayes_known}, which represents the \emph{effective reward variance}.

Second, $\kappa_{0, i}$ in the Gaussian-Gamma prior plays the role of $\sigma_i^2 / \sigma_{0, i}^2$ in the known variance setting \citep{murphy2007conjugate}. Therefore, as $\kappa_{0, i} \to \infty$, the bound in \cref{thm:bayes_unknown2} should go to zero, similarly to \cref{thm:bayes_known}. This is indeed the case and a very unique property of Bayes regret bounds, which is not captured by \citet{honda14optimality,zhou22approximate}.

Finally, we take $\alpha_{0, i}, \beta_{0, i} \to \infty$ while keeping $\beta_{0, i} / (\alpha_{0, i} - 1)$ constant. As the mean and variance of the Inverse-Gamma distribution are $\beta_{0, i} / (\alpha_{0, i} - 1)$ and $\beta_{0, i}^2 / ((\alpha_{0, i} - 1)^2 (\alpha_{0, i} - 2))$, respectively, this correspond to holding the mean of the variance prior fixed while narrowing its width. In this case, we expect the bound in \cref{thm:bayes_unknown2} to approach that in \cref{thm:bayes_known}, which happens because the term $0.5 / (\kappa_{0, i} (\alpha_{0, i} - 1))$ vanishes. After that, the bounds are similar up a multiplicative factor of $5$.

\textbf{Existing frequentist regret bounds for variance-adaptive Thomson sampling.} \citet{honda14optimality,MV20} proposed variance-adaptive Thompson sampling and bounded its regret. These works differ from us in three aspects. First, the algorithms of \citet{honda14optimality,MV20} are designed for the frequentist setting. Specifically, they have a fixed sufficiently-wide prior, and enjoy a per-instance regret bound under this prior. As an example, the algorithm of \citet{MV20} for $\rho \to \infty$ (Remark 4) is essentially \varts with $\bmu_{0, i} = 0$, $\bkappa_{0, i} = 0$, $\balpha_{0, i} = 0.5$, and $\bbeta_{0, i} = 0.5$. While per-instance regret bounds are strong, the algorithms of \citet{honda14optimality,MV20} can perform poorly when priors are narrower and thus more informative. Truly Bayesian algorithm designs, as proposed in our work, can be analyzed for any informative prior. Second, the analyses of \citet{honda14optimality,MV20} are frequentist. This means that they cannot justify the use of more informative priors and are essentially similar to those of frequentist upper confidence bound algorithms. As an example, in Remark 4 of \citet{MV20}, the authors derive a $O\left(\sum_{i: \mu_i < \mu_{a^*}} \frac{1}{\Delta_i} \log n\right)$ regret bound, where $\Delta_i = \mu_{a^*} - \mu_i$ and $a^*$ is the arm with the highest mean reward $\mu_i$. This bound clearly does not depend on prior parameters, which we incorporate in our bounds. Specifically, our bound in \cref{thm:bayes_unknown2} decreases with lower reward variances and more informative priors on them. Finally, the regret bounds of \citet{honda14optimality,MV20} are asymptotic. We provide strong finite-time guarantees.

\iffalse%%%%%%%%%%%%%%%%%
\subsection{Proof Sketch of \cref{thm:bayes_unknown2}}
\label{sec:unknown_reg_prf}
xsasa
We sketch the proof of \cref{thm:bayes_unknown2} below. The complete proof can be found in \cref{app:unknown}.

The overall structure of the proof is inspired by the analysis of the known variance case (\cref{sec:known_reg_prf}). However, this proof is significantly more involved due to the unknown reward variance and maintaining Gaussian-Gamma posteriors. As in the proof of \cref{thm:bayes_known}, we can decompose the regret in round $t$ as:
\begin{align}
\label{eq:term55}
  & \E{\mu_{A^*} - \mu_{A_t}}
  = \E{\condE{\mu_{A^*} - \mu_{A_t}}{H_t}} \nonumber
  \\
  & 
  \leq 
  \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} +
  \mathbb E\Bigsn{\mathbb E_{\bsigma}\bigsn{\mathbb E [\hat{\mu}_{t,A_t}- \mu_{A_t} \mid \bsigma ] \mid H_t} }
  = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} 
\end{align}
where $\sigma_{t,i}^2= \frac{1}{{\kappa_{t,i}\lambda_{t,i}}}$ 
%\todob{There is no $\sigma_{t, i}^2$ above.} 
is a sampled posterior variance and $\lambda_{t,i} \sim \text{Gam}(\alpha_{t,i},\beta_{t,i})$. 
%\todob{We need to have clearer terminology (also in the algorithm). $\lambda_{t, i}^{-2}$ is the posterior sampled variance of rewards. $\sigma_{t,i}^2$ is the posterior variance of the mean reward.} 
The last equality holds since given $H_t$ and $\bsigma$,  $E[\mu_{i} \mid H_t] = \hat{\mu}_{t,i}$ for any $i \in [K]$ by definition of the posterior update.

Now given $H_t$ and $\bsigma_t$, let us define the high-probability confidence interval of each arm $i$ as $C_t(i) = \sqrt{2 \sigma_{t, i}^2 \log(1 / \delta)}$, and the ``good event"
$
  E_t
  = \set{\forall i \in [K]: \abs{\mu_i -  \hat{\mu}_{t,i}} \leq C_t(i)}
$,
same as what we introduced in the proof of \cref{thm:bayes_known}. 
%\todob{Refer to earlier defined concepts instead of redefining them, especially if you want to make a point that this proof is built on another one.}
%which essentially indicates the ``good event"' %\todob{Use \say{this} for quotes.} when all the confidence intervals at round $t$ hold good. 
%
Further note, given $H_t$ and $\sigma_{t,i}^2$ (or equivalently $\lambda_{t,i})$),  $\mu_i - \hat{\mu}_{t,i} \mid \sigma_{t,i}^2, H_t \sim \cN(\mathbf{0}, \sigma_{t,i}^2)$, 
since given $H_t$ and $\sigma_t$, $\mu_i$ has the posterior $\mu_i \sim \cN(\hat \mu_{t,i}, \sigma_{t,i}^{2})$, where recall we defined $\sigma_{t,i}^2 = \frac{1}{\kappa_{t,i}\lambda_{t,i}}$ and $\lambda_{t,i} = \frac{1}{\sigma_{t,i}^2} \sim \text{Gam}(\alpha_{t,i},\beta_{t,i})$.
 %\red{I am confused a bit}
Thus following same analysis as in \eqref{eq:thm11}, we get: 
%So we w.h.p. $(1-\delta')$, we have \red{checK!}:
%
\begin{align}
  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{\bsigma_t, H_t}
  \leq \sum_{i = 1}^K \sqrt{\frac{\sigma_{t,i}^2}{2 \pi}} \delta.
  \label{eq:bayes_regret_scale22}
\end{align} %\todob{This seems incorrect. $\sigma_i^2$ should be $\sigma_{t, i}^2$?}
%
Further taking expectation over $H_n, \bsigma, \bmu$, and summing over all $t$ we get:
\begin{align}
\label{eq:long22}
  &\E{\sum_{t = 1}^n(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}
  \leq  
  \delta(2\pi)^{-1/2}\mathbb{E}_{H_n, \bsigma, \bmu}\biggsn{\sum_{t = 1}^n \sum_{i = 1}^K \mathbb{E}_{\bsigma_{t}}[\sigma_{t,i} \mid \bmu,\bsigma, H_t]}
\end{align}
%
Now one of the main novelty of this proof lies in showing:
\begin{align*}
& \mathbb{E}_{H_n, \bsigma, \bmu}\biggsn{\sum_{t = 1}^n \sum_{i = 1}^K \mathbb{E}_{\bsigma_{t}}[\sigma_{t,i} \mid \bmu,\bsigma, H_t]}
\leq 
\sqrt{nK} \sqrt{\sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
+ 
\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)}
\end{align*} 
%\todob{$H_n$ in the inner expectation should be $H_t$. We should not condition on the future.} 
%\todob{Why conditioning on $\bmu$? - \red{used later in the detailed proof, note $\sigma_{t,i}$ depends on $\mu$, etc.}} 
which involves a series of careful applications of tower rule of expectations and \cref{lem:reciprocal sum} (in \cref{app:unknown}) -- this is the most novel part of this proof which shows an elegant way to upper bound $\E{\sigma_{t, i}}$ for any $i$ and $t$. While the analysis is non-trivial and lengthy, the complete proof can be found in \cref{app:unknown}.
%
Now coming back to the original Bayesian regret expression, note that the regret expression from \eqref{eq:term5} 
%\todob{A forward reference to Appendix. Fix if it as an error or say "in Appendix" - \red{I wrote its in Appendix B? Not sure of the issue}} 
can be actually decomposed based on $E_t$ and $\bar E_t$ as:
\begin{align}
\label{eq:term22}
  &\condE{{\hat{\mu}_{t,A^*}}  - \mu_{A^*}}{H_t} 
  \leq \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{E_t}}{H_t} 
  + \nonumber
  \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t} 
  \\
  & \leq \condE{C_t(A^*)}{H_t} \nonumber
  +\condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t}
  \\
  & = \condE{C_t(A_t)}{H_t} 
  +\condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t},
\end{align}
where the last equality follows from the fact that $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions, as both $\tilde \mu_{t,i}$ and $\mu_{t,i}$ can be seen as independent posterior samples from a NG$(\hat \mu_{t,i},\kappa_{t,i},\alpha_{t,i},\beta_{t,i})$.% distribution marginalized over $\lambda$. %\todob{It is unclear what "marginalized over $\lambda$" means here.}

\iffalse%%%%%%%%%%%%%%%%%%
More precisely for any realization of $\tilde \mu_{t,i}$ and $\mu_i$:
\[
P(\tilde \mu_{t,i}) = \int_{\lambda} P\big( (\tilde \mu_{t,i}, \lambda_{t,i} \sim \text{NG}(\hat \mu_{t,i},\kappa_{t,i},\alpha_{t,i},\beta_{t,i}) ) \big)d\lambda  ~~~\text{and}
\]
\[
P(\mu_{i}) = \int_{\lambda} P\big( (\tilde \mu_{t,i}, \lambda_{t,i} \sim \text{NG}(\hat \mu_{t,i},\kappa_{t,i},\alpha_{t,i},\beta_{t,i}) ) \big)d\lambda,
\]
they follow the same distribution. 
\fi%%%%%%%%%%%%%%%%%%%%%%%

The second term in \eqref{eq:term22} is bounded similarly to \eqref{eq:long22}. 
%
The only remaining task is to bound the first term in \eqref{eq:term22}, which again involves some non-trivial chain of inequalities involving tower rule of expectations, Cauchy-Schwarz, Jensen's and \cref{lem:reciprocal root sum}, detailed in \cref{app:unknown}. As a result, the final expression can be bounded as:% \todob{Indicators below are written differently from earlier. Choose one notation for indicators and stick to it.}
%
\begin{align*}
  &\E{\sum_{t = 1}^n C_t(A_t)}
  \leq \E{ \bigg[ \sqrt{\sum_{i = 1}^K N_n(i)} \sqrt{\sum_{i = 1}^K \sum_{t = 1}^n C_t^2(i)\I{A_t = i} }\bigg]}
  \\
  &\overset{}{=}  \bigg[ \sqrt{n} \sqrt{\E{ \sum_{i = 1}^K \sum_{t = 1}^n {2 \sigma_{t, i}^2 \log(1 / \delta)}\I{A_t = i} }\bigg]}
  =  \bigg[ \sqrt{n} \sqrt{\E{ \sum_{i = 1}^K \sum_{s = 0}^{N_n(i)} \bign{2 \sigma_{t, i}^2 \log(1 / \delta)}\I{A_t = i} }\bigg]}
  \\
  & \leq \sqrt{n\log \frac{1}{\delta}} \times 
  \sqrt{\sum_{i = 1}^K \sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
+ 
\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)}
 \,.
\end{align*}
Finally, chaining of the above inequalities on the summation of the instantaneous regret over $n$ rounds, from \eqref{eq:term22} we get:
%\todob{Why the reference to \eqref{eq:term22}?}
%
\begin{align*}
%\label{eq:term2}
  &\mathbb{E}_{H_n,\bmu,\bsigma}\biggsn{\sum_{t = 1}^n\condE{{\hat{\mu}_{t,A^*}}  - \mu_{A^*}}{H_t}} \nonumber
  \leq 
  C \biggn{\frac{\delta \sqrt{nK}}{\sqrt{2\pi}} + \sqrt{n\log \frac{1}{\delta}}}
  \,,
\end{align*}
which proves the claim.

\fi%%%%%%%%%%%%%%%%%%%%

%\todob{The structure of the current proof is not good because the connection to the known variance proof seems weak. I would do the following:

%1) Start with (4).

%2) Apply (7).

%3) Bound the second term in (7) using (5). Leave $\sigma_{t, i}^2$ the way it is.

%4) Bound the first term in (7). Tie it to how we bound an analogous term in Theorem 1.

%5) Massage the final bound to get $\E{\sigma_{t, i}^2}$. Finally, apply the main trick, that $\E{\sigma_{t, i}^2}$ has a nice upper bound for any $i$ and $t$. - \red{done!}}

% noting $C:= \sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
%+ 
%\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)$. 

% %As described in \cref{rem:unknown}, \cref{thm:bayes_unknown2} gives the first variance-sensitive regret bounds with TS, even for the case of unknown arm variances. 
% We also performed empirical studies to evaluate the efficacy
% of \cref{alg:bayes_unknown}. As reported in the next section, \varts\, significantly overperforms the existing baselines, \ucb \citep{auer02finitetime}, \ucbtuned \citep{auer02finitetime} and \ucbv \citep{audibert09exploration} even in the misspecified setting (i.e. when arm rewards $\bmu$ are not necessarily Gaussian). %, which seconds the practicality of our proposed algorithm, beyond its theoretical regret bounds. 
% \todob{I do not understand this discussion. Simply say that experiments come next. We can summarize the experimental results at the beginning of the experimental section if you want to. - \red{shortened, done!}}
