\section{Frequentist Setting}
\label{sec:freq}

\blue{Few sents to overview the section...}

\red{Refer to the Algorithm}

\subsection{Known Variance}
\label{sec:freq_known}

\textbf{Setting. } A \emph{multi-armed bandit} \cite{lai85asymptotically,auer02finitetime,lattimore19bandit} is an online learning problem where the learning agent pulls $K$ arms in $n$ rounds. In round $t \in [n]$, the agent pulls arm $I_t \in [K]$ and receives its reward. The reward of arm $i \in [K]$ in round $t$, $Y_{i, t}$, is drawn i.i.d.\ from a distribution of arm $i$, $P_i$, with mean $\mu_i$ and support $[0, 1]$. The means are unknown to the learning agent. The objective of the agent is to maximize its expected cumulative reward in $n$ rounds.

We assume that arm $1$ is optimal, that is $\mu_1 > \max_{i > 1} \mu_i$. Let $\Delta_i = \mu_1 - \mu_i$ be the \emph{gap} of arm $i$, the difference in the expected rewards of arms $1$ and $i$. Then maximization of the expected $n$-round reward can be viewed as minimizing the \emph{expected $n$-round regret}, which we define as
\begin{align}
  R(n)
  = \sum_{i = 2}^K \Delta_i \E{\sum_{t = 1}^n \I{I_t = i}}\,.
  \label{eq:regret_freq}
\end{align}

\subsection*{Regret Analysis}
\label{sec:freq_known_analysis}

\citep{giro} proved the following regret upper bound of their algorithm `General Randomized Exploration' (Alg. 1) in a multi-armed bandit with $K$ arms. The distribution $p$ in line \ref{alg:gre:estimate} of \cref{alg:gre} is a function of the history of the arm. For $s \in [n] \cup \set{0}$, let
\begin{align}
  Q_{i, s}(\tau)
  = \condprob{\tilde{\mu}_{i,s} \geq \tau}{\cH_{i, s}}
  \label{eq:optimism}
\end{align} 
be the tail probability that $\tilde{\mu}$ conditioned on history $\cH_{i, s}$ is at least $\tau$ for some tunable parameter $\tau$.

\begin{thm}[\cite{giro}]
\label{thm:gre upper bound} 
For any tunable parameters $(\tau_i)_{i = 2}^K \in \realset^{K - 1}$, the expected $n$-round regret of \cref{alg:gre} can be bounded from above as $R(n) \leq \sum_{i = 2}^K \Delta_i (a_i + b_i)$, where
\begin{align*}
  a_i
  & = \sum_{s = 0}^{n - 1} \E{\min \set{1 / Q_{1, s}(\tau_i) - 1, n}}\,, \\
  b_i
  & = \sum_{s = 0}^{n - 1} \prob{Q_{i, s}(\tau_i) > 1 / n} + 1\,.
\end{align*}
\end{thm}

\begin{restatable}[Variance Adaptive Frequentist Regret for Known Variance]{thm}{freqknown}
\label{thm:freq_known}
%\red{Incomplete, don't read this}
Let $\bmu$ be the expected reward vector of the $K$-armed MAB with their respective sub-Gaussianity parameter vector $\bsigma^2$. Then assuming $A_*$ being the optimal arm under $\bmu$ and $A_t$ be the pulled arm in round $t$, the expected $n$-round regret of TS is bounded above as:
\[
\E{\sum_{t = 1}^n \mu(A_*) - \mu(A_t)} \leq \sum_{i = 2}^K \Bign{\frac{62 \sigma_i^2}{\Delta_i} (2\log (n) + 1) + 4\Delta_i}.
\]
\iffalse %%%%%%%%%%%%%
\red{To be updated}...
 For any $a > 1 / \sqrt{2}$, the expected $n$-round regret of $\giro$ is bounded from above as
\begin{align}
  R(n)
  \leq \sum_{i = 2}^K \Delta_i
  \Bigg[& \underbrace{\left(\frac{16 (2 a + 1) c}{\Delta_i^2} \log n + 2\right)}_
  {\text{\emph{Upper bound on $a_i$ in \cref{thm:gre upper bound}}}} + {}
  \label{eq:upper bound} \underbrace{\left(\frac{8 (2 a + 1)}{\Delta_i^2} \log n + 2\right)}_
  {\text{\emph{Upper bound on $b_i$ in \cref{thm:gre upper bound}}}}\Bigg]\,,
  \nonumber
\end{align}
where $b = (2 a + 1) / [a (a + 1)]$ and
\begin{align*}
  c
  = \frac{2 e^2 \sqrt{2 a + 1}}{\sqrt{2 \pi}} \exp\left[\frac{8 b}{2 - b}\right]
  \left(1 + \sqrt{\frac{2 \pi}{4 - 2 b}}\right)
  \nonumber
\end{align*}
is an upper bound on the expected inverse probability of being optimistic, which is derived in \cref{sec:optimism upper bound}.
\fi %%%%%%%%%%%%%%%%%%
\end{restatable}

\begin{proof}
\textbf{Algorithmic Notations. } Let $\mu_i \in [0,1]$ be the true mean reward of arm $i$ (with known reward variance $\sigma_i$), for any $i \in [K]$. Assume the algorithm draws the arm according to the posterior samples $\tmu_{i,s} \sim \cN(\hmu_{i,s},\sigma_i)$, 
%
where $\hmu_{i,s}$ being the posterior mean of arm $i$ after the arm is pulled for $s$ times. %We also assume that $\hmu_{i,0} = 0$ and the posterior variance is $\sigma_{i,0} = 1$. %\red{check with BKV}. 
%

\noindent
Given the result of \cref{thm:gre upper bound} our proof has two parts:

\noindent
\textbf{(i) Bounding $b_i$:}

\noindent
Let us assume $\tau_i = \mu_i + \Delta_i/2$. And let us define the event $\cE_{i,s}: = \{\hmu_{i,s} - \mu_i < \Delta_i/4 \}$.

\begin{align*}
\P  (Q_{i,s} & (\tau_i) > 1/n)
	\\
	& =  
	\P(Q_{i,s}(\tau_i) > 1/n \cap \cE_{i,s}) + \P(Q_{i,s}(\tau_i) > 1/n \cap \cE_{i,s}^c)
	\\	
	& \overset{(a)}{\leq} 
	0 + \P(\cE_{i,s}^c) = \P(\hmu_{i,s} - \mu_i \geq \Delta_i/4)
	\leq 
	 1/n
\end{align*} 

where the last inequality follows by \cref{lem:concs} for any $s > \frac{32\sigma_i^2 (\ln n + 1)}{\Delta_i^2}$. Further the inequality $(a)$ follows by noting \[\P\big(Q_{i,s}(\tau_i) > 1/n \cap \cE_{i,s} \big) = 0,
\]
since by \cref{lem:concs}, we can show for any $s > \frac{32 \sigma_i^2 \log n}{\Delta_i^2} $, under the event $\cE_{i,s}$:
\begin{align}
\label{eq:1}
Q_{i,s}(\tau_i)& = \condprob{\tilde{\mu}_{i,s} - \mu_i \geq \Delta_i/2 \cap \hmu_{i,s} - \mu_i < \Delta_i/4}{\cH_{i, s}}  \nonumber\\
& = \P\Bign{(\tmu_{i,s} - \mu_i + \hmu_{i,s} -\hmu_{i,s}) \geq \Delta_i/2 \cap \hmu_{i,s} - \mu_i < \Delta_i/4 \mid \cH_{i, s}} \nonumber
	\\
	& \leq \P(\tmu_{i,s} -\hmu_{i,s} \geq \Delta_i/4)
	\leq 1/n,
\end{align} \todob{The derivation is confusing. Write out $Q_{i,s}(\tau_i)$ and then apply $\cE_{i,s}$.}
where the last inequality follows from the second part of \cref{lem:concs} and since we assumed $s > \frac{32 \sigma_i^2}{\Delta_i^2} \log n$.

\noindent
\textbf{(ii) Bounding $a_i$:} 

\noindent
Let us assume $\tau_i = \mu_i - \Delta_i/2$ and define the event $\cE_{i,s}: = \{\mu_1 - \hmu_{1,s} < \Delta_i/4 \}$. %\todob{This is later used as $\cE_{1,s}$.}

We start by noting that $\cE_{i,s} \implies 1 - Q_{1,s}(\tau_i) \leq 1/n$ (i.e. $Q_{1,s}(\tau_i) \geq 1-1/n$). Let us denote the event $\cF_{i,s}:= Q_{1,s}(\tau_i) \geq 1 - 1/n$. The claim follows as when $\cF_{i,s}$ holds, we have: 

\begin{align}
\label{eq:2}
1 - Q_{1,s}(\tau_i)  & = \condprob{\tilde{\mu}_{1,s} - \mu_1 < -\Delta_i/2 \cap \mu_1 -\hmu_{1,s} < \Delta_i/4}{\cH_{1, s}}  \nonumber
\\
& = 
\P\Bign{(\tmu_{1,s} - \mu_1 + \hmu_{1,s} -\hmu_{1,s}) < -\Delta_i/2 \cap \mu_1 -\hmu_{1,s} < \Delta_i/4 \mid \cH_{1, s}} \nonumber
\\
& = 
\P\Bign{(\mu_1 - \tmu_{1,s} + \hmu_{1,s} -\hmu_{1,s}) > \Delta_i/2 \cap \mu_1 -\hmu_{1,s} < \Delta_i/4 \mid \cH_{1, s}} \nonumber\\
& \leq 
\P(\hmu_{1,s} -\tmu_{1,s} \geq \Delta_i/4)
	\leq 1/n^2,
\end{align} 
%\todob{Extra parentheses on the RHS.} 
\todob{The derivation is confusing. Write out $Q_{1,s}(\tau_i)$ and then apply $\cE_{1,s}$.}
where the last inequality follows from the second part of \cref{lem:concs} and since we assumed $s > \frac{64 \sigma_i^2}{\Delta_i^2} \log n$.

Having established this, now we note that for any $s > \frac{32 \sigma_i^2}{\Delta_i^2} (2\log (n) + 1)$: %\todob{Something missing.}:

\begin{align*}
& \E{\min \bigg \{ \frac{1}{Q_{1,s}(\tau_i)}-1,n \bigg \} } 
\\
	& \leq  
	\E{\min\{1/Q_{1,s}(\tau_i)-1,n\}1(\cE_{i,s})} + \E{\min\{1/Q_{1,s}(\tau_i)-1,n\}1(\cE^c_{i,s})}\\
	& \leq
	\E{\min\{\frac{1}{Q_{1,s}(\tau_i)}-1,n\}1(\cE_{i,s})1(\cF_{1,s})} + 
	\E{\min\{\frac{1}{Q_{1,s}(\tau_i)}-1,n\}1(\cE_{i,s})1(\cF^c_{1,s})}	\\
	& \quad\quad\quad\quad\quad\quad\quad\quad
	+ \E{\min\{\frac{1}{Q_{1,s}(\tau_i)}-1,n\}1(\cE^c_{i,s})}\\
	& \leq 
	\frac{1}{n}\E{1(\cE_{1,s})1(\cF_{1,s})} + 
	n\E{1(\cE_{i,s})1(\cF^c_{1,s})}	+ n\E{1(\cE^c_{i,s})}\\
	& \leq 
	\frac{1}{n}\P{(\cE_{i,s} \cap \cF_{1,s})} + 
	n\P{(\cE_{i,s} \cap \cF^c_{1,s})}	+ n\P{(\cE^c_{i,s})}	\\
	& \overset{(a)}{\leq} 
	1/n(1 - 1/n) + n 1/n^2 + n 1/n^2 \leq 3/n,
\end{align*} \todob{I do not understand the need for $\cF_{1,s}$.}
where inequality (a) follows from \cref{eq:2} and \cref{lem:concs} implies $\P(\cE^c_{i,s}) < \frac{1}{n^2}$ for any $s > \frac{32 \sigma_i^2}{\Delta_i^2} (2\log (n) + 1)$. 

Suppose $s_0 = \frac{32 \sigma_i^2}{\Delta_i^2} (2\log (n) + 1)$. The final bound now follows by combining the main regret upper bound of \cref{thm:gre upper bound} (from \cite{giro}) with the above derived upper bounds $a_i$ and $b_i$ as:

\begin{align*}
R(n) & \leq \sum_{i = 2}^K \Delta_i (a_i + b_i)
\\
& \leq 
	\sum_{i = 2}^K \Delta_i \biggn{\sum_{s = 0}^{n - 1} \E{\min \set{1 / Q_{1, s}(\tau_i) - 1, n}} + \sum_{s = 0}^{n - 1} \prob{Q_{i, s}(\tau_i) > 1 / n} + 1}
\\
& \leq
	\sum_{i = 2}^K \Delta_i \biggn{2s_0 + \sum_{s = s_0+1}^{n - 1} \E{\min \set{1 / Q_{1, s}(\tau_i) - 1, n}} + \sum_{s = s_0}^{n - 1} \prob{Q_{i, s}(\tau_i) > 1 / n} + 1}
\\
& \leq 
\sum_{i = 2}^K \Bign{\frac{62 \sigma_i^2}{\Delta_i} (2\log (n) + 1) + 4\Delta_i}
\end{align*}

\red{some problem, needs discussion} - $\tau_i$s are different for $a_i, b_i$ + how to bound $a_i$ for small $s$? \todob{I do not understand the issue.}

\end{proof}


\subsubsection{Concentration Results used in the Proof of \cref{thm:freq_known}} 

\todob{We need plain English to explain the reasoning behind the choices of the prior mean and variance.}

\begin{lem}
\label{lem:concs}
For every arm $i \in [K]$ and time step $s \in [T]$, let us define: 

\begin{align*}
\hat \mu_{i,s} = \frac{1}{\sigma_i^{2} + s}\sum_{\ell = 1}^s Y_{i,\ell},
\end{align*}

where $Y_{i,s}$s are iid draws from any $\sigma_i$-subGaussian random variable with mean $\mu_i >0$. Then for any $\epsilon > 0$, 

\begin{align*}
\P(\hmu_{i,s} & -\mu_{i} \geq \epsilon) \leq \exp\Bign{-\frac{s\epsilon^2}{2\sigma_i^2}}~~ \forall i \in [K]\sm\{1\} \text{ and }\\
& \P(\hmu_{1,s}-\mu_{1} \leq -\epsilon) \leq e \exp\Bign{-\frac{s\epsilon^2}{2\sigma_i^2}} 
\end{align*}

Further for any $i\in [K]$ we always have: 
\begin{align*}
\P(\tmu_{i,s} & -\hmu_{i} \geq \epsilon)
= 
\P(\tmu_{i,s} -\hmu_{i} \leq -\epsilon)
\leq \exp\Bign{-\frac{s\epsilon^2}{2\sigma_i^2}} 
~~ \forall i \in [K]
\end{align*}

\end{lem}

\begin{proof}
Note we get:

\begin{align*}
\P(\hat \mu_{i,s}  - \mu_i \geq \epsilon)
 & = \P\biggn{\frac{1}{s + \sigma_i^2} \sum_{\ell = 1}^s Y_{i,\ell} - \mu_i \leq \epsilon }\\
 & = \P\biggn{\sum_{\ell = 1}^s Y_{i,\ell} - s\mu_i \geq (s+\sigma_i^2) \epsilon + \sigma_i^2 \mu_i }\\
 & \leq \P\biggn{\sum_{\ell = 1}^s Y_{i,\ell} - s\mu_i \geq s \epsilon } \hfill \text{ since } (\mu_i + \epsilon)\sigma_i^2 > 0\\
 & = \P\biggn{\frac{1}{s}\sum_{\ell = 1}^s Y_{i,\ell} - \mu_i \geq \epsilon } \leq \exp\Bign{-\frac{s\epsilon^2}{2\sigma_i^2}}, \end{align*}
where the last inequality follows from subGaussian concentration.

On the other hand, 

\begin{align*}
\P(\hat \mu_{1,s}  - \mu_1 \leq -\epsilon) & = \P(\mu_1 - \hat \mu_{1,s}   \geq \epsilon)
 = \P\biggn{\mu_1 - \frac{1}{s + \sigma_1^2} \sum_{\ell = 1}^s Y_{1,\ell}  \geq \epsilon }\\
 & = \P\biggn{s\mu_1 - \sum_{\ell = 1}^s Y_{1,\ell}  \geq (s+\sigma_1^2) \epsilon - \sigma_1^2 \mu_1 }\\
 & = \P\biggn{\mu_1 - \frac{1}{s}\sum_{\ell = 1}^s Y_{1,\ell}  \geq \Big(1+\frac{\sigma_1^2}{s}\Big) \epsilon - \frac{\sigma_1^2 \mu_1}{s} }\\
 & \leq \P\biggn{\sum_{\ell = 1}^s Y_{i,\ell} - s\mu_i \geq s \epsilon } \\
& \overset{(a)}{\leq} \exp\Bign{-\frac{s\biggn{\Big(1+\frac{\sigma_1^2}{s}\Big) \epsilon - \frac{\sigma_1^2 \mu_1}{s}}^2}{2\sigma_i^2}}\\
 %
 & \leq \exp\biggn{-\Bign{\frac{s\epsilon^2}{2\sigma_1^2} - \mu_1\epsilon}} \hfill \text{ since } \frac{\sigma_1^2\mu_1^2}{2s} > 0 ~\forall s\\
 & \leq \exp\biggn{-\Bign{\frac{s\epsilon^2}{2\sigma_1^2} - 1}}  \hfill \text{ since } (\mu_i\epsilon) < 1, \text{ as } \epsilon > 0 \text{ and } \mu_i \in [0,1] \\
 & = e\exp\Bign{-\frac{s\epsilon^2}{2\sigma_1^2}}, 
 \end{align*}
where the inequality (a) again follows from subGaussian concentration. This concludes the first part of the proof.

For the second part, firstly, it is easy to follow that since $\tilde \mu_{i,s} \sim \cN(\hat \mu_{i,s}, \sigma_i^2)$, $\sigma_i$ being the (known) variance of arm $i$, from the subGaussian concentration, we have: 

\begin{align*}
\P \big( & \hat \mu_{i,s} - \tilde \mu_{i,s} \geq \epsilon \big) = \P \big( \hat \mu_{i,s} - \tilde \mu_{i,s} \leq -\epsilon \big)
 \leq \exp\Bign{-\frac{s\epsilon^2}{2\sigma_i^2}}.
\end{align*}
\end{proof}

