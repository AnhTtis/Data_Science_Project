\section{Gaussian Bandit with Unknown Variances}
\label{sec:bayes_unknown2}

The main contribution of this work is that we consider the Bayesian setting with Gaussian rewards and unknown heterogeneous reward variances. Similarly to \cref{sec:bayes_known}, we propose a Thompson sampling algorithm for this setting in \cref{sec:bayes_unknown_algo}. The algorithm is novel and generalizes even that of \citet{honda14optimality}, which is the closest related work in the frequentist setting. We state the regret bound and discuss it in \cref{sec:unknown_reg}, and sketch its proof in \cref{sec:unknown_reg_prf}. The regret bound scales roughly as:
\begin{align*}
  \textstyle
  \sqrt{n \log n}\sqrt{\sum_{i = 1}^K \frac{\beta_{0,i}}{\alpha_{0,i}-1}
  \log\Big( 1+\frac{n}{\kappa_{0,i}}\Big)}\,,
\end{align*}
where $\frac{\beta_{0, i}}{\alpha_{0, i} - 1}$ represents a proxy for the reward variance $\sigma_i^2$ in \eqref{eq:rough bound known} and $\kappa_{0, i}^{-1}$ plays the role $\sigma_{0, i}^2/\sigma_i^2$. Since the dependencies are analogous, the bound captures the structure of the problem similarly to \eqref{eq:rough bound known}. \emph{The main novelty in the proof is in handling the uncertainty of reward variances $\bsigma^2$, which is very unique among all existing TS proofs.}

\input{algo_bayes_unknown.tex}


\subsection{Regret Analysis}
\label{sec:unknown_reg}

Before we analyze \cref{alg:bayes_unknown}, we recall the problem setting. We assume that the bandit instance $(\bmu, \bsigma)$ is sampled jointly from a Gaussian-Gamma distribution. More specifically, for any arm $i$, the mean and variance of its rewards are sampled as $(\mu_i, \sigma_i^{-2}) \sim NG(\mu_{0, i}, \kappa_{0, i}, \alpha_{0, i}, \beta_{0, i})$, where $(\bmu_0, \bkappa_0, \balpha_0, \bbeta_0)$ are known prior parameters. This can also be seen as first sampling $\sigma_i^{-2} \sim \text{Gam}(\alpha_{0,i},\beta_{0,i})$ and then $\mu_i \sim \cN(\mu_{0,i}, \frac{\sigma_i^2}{\kappa_{0,i}})$. Our regret bound is presented below.

\begin{restatable}[Variance-dependent regret bound for unknown variances]{thm}{bayesunknown}
\label{thm:bayes_unknown2} Consider the above setting and assume that  $\alpha_{0,i }\geq 1$ for all arms $i \in [K]$. Then for any $\delta > 0$, the Bayes regret of \varts is bounded as:
\begin{align*}
  R_n
  \leq C\sqrt{n \log(1 / \delta)} + \delta C \sqrt{\frac{nK}{2\pi}}\,,
\end{align*}
where $C^2 = \sum_{i = 1}^K \frac{\beta_{0,i}}{\alpha_{0,i} - 1} \biggn{ \frac{2}{\kappa_{0,i}} + \frac{0.5}{\kappa_{0,i}(\alpha_{0,i}-1)} + 5\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big) }$ is a constant dependent on prior parameters.
\end{restatable}

\textbf{Discussion.} For $\delta = 1 / n$, the bound in \cref{thm:bayes_unknown2} is $\tilde{O}(\sqrt{C n})$. The dependence on $\sqrt{n}$ is the same as in \cref{thm:bayes_known}. A closer examination of $C$ reveals many similarities.

First, $\beta_{0, i} / (\alpha_{0, i} - 1)$ is the mean of an Inverse-Gamma distribution with parameters $(\alpha_{0, i}, \beta_{0, i})$. Since $\lambda_i$, the precision of the reward distribution of arm $i$, is sampled from $\text{Gam}(\alpha_{0, i}, \beta_{0, i})$, we have that $\beta_{0, i} / (\alpha_{0, i} - 1)$ is the mean of the variance distribution of arm $i$. That is, $\beta_{0, i} / (\alpha_{0, i} - 1)$ in \cref{thm:bayes_unknown2} plays the role of $\sigma_i^2$ in \cref{thm:bayes_known}.

Second, $\kappa_{0, i}$ in the Gaussian-Gamma prior in the unknown variance case plays the role of $\sigma_i^2 / \sigma_{0, i}^2$ in the known variance case \citep{murphy2007conjugate}. Therefore, as $\kappa_{0, i} \to \infty$, we would expect the bound in \cref{thm:bayes_unknown2} to approach zero, similarly to \cref{thm:bayes_known}. This is clear upon inspecting $C$.

Finally, take $\alpha_{0, i}, \beta_{0, i} \to \infty$ while $\beta_{0, i} / (\alpha_{0, i} - 1)$ is kept constant. In this setting, the term $0.5 / (\kappa_{0, i} (\alpha_{0, i} - 1))$ vanishes and we recover the same algebraic form as in \cref{thm:bayes_known}. Therefore, $0.5 / (\kappa_{0, i} (\alpha_{0, i} - 1))$ can be viewed as the cost for not knowing the variance $\sigma_i^2$. In addition, the variance of the Inverse-Gamma distribution of $\sigma_i^2$ approaches zero because it is $O(\beta_{0, i}^2 \alpha_{0, i}^{-3})$. Therefore, our setting approaches that of the known variance, and so does our bound, up to a small multiplicative factor of $5$.

In summary, the known variance bound in \cref{thm:bayes_known} is closely related to the unknown variance bound in \cref{thm:bayes_unknown2}. The additive penalty for not knowing the variance mimics the frequentist setting \citep{audibert09exploration}, although the specific penalty, $0.5 / (\kappa_{0, i} (\alpha_{0, i} - 1))$, differs.


\subsection{Proof Sketch of \cref{thm:bayes_unknown2}}
\label{sec:unknown_reg_prf}

We sketch the proof of \cref{thm:bayes_unknown2} below. The complete proof can be found in \cref{app:unknown}.

The overall structure of the proof is inspired by the analysis of the known variance case (\cref{sec:known_reg_prf}). However, this proof is significantly more involved due to the unknown reward variance and maintaining Gaussian-Gamma posteriors. As in the proof of \cref{thm:bayes_known}, we can decompose the regret in round $t$ as:
\begin{align}
\label{eq:term55}
  & \E{\mu_{A^*} - \mu_{A_t}}
  = \E{\condE{\mu_{A^*} - \mu_{A_t}}{H_t}} = \nonumber \\
  & 
  \hspace{-3pt}
  \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} +
  \mathbb E\Bigsn{\mathbb E_{\bsigma}\bigsn{\mathbb E [\hat{\mu}_{t,A_t}- \mu_{A_t} \mid \bsigma ] \mid H_t} }\nonumber
  \\
  & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} 
\end{align}
where $\sigma_{t,i}^2= \frac{1}{{\kappa_{t,i}\lambda_{t,i}}}$ %\todob{There is no $\sigma_{t, i}^2$ above.} 
is a sampled posterior variance and $\lambda_{t,i} \sim \text{Gam}(\alpha_{t,i},\beta_{t,i})$. 
%\todob{We need to have clearer terminology (also in the algorithm). $\lambda_{t, i}^{-2}$ is the posterior sampled variance of rewards. $\sigma_{t,i}^2$ is the posterior variance of the mean reward.} 
The last equality holds since given $H_t$ and $\bsigma$,  $E[\mu_{i} \mid H_t] = \hat{\mu}_{t,i}$ for any $i \in [K]$ by definition of the posterior update.

Now given $H_t$ and $\bsigma_t$, let us define the high-probability confidence interval of each arm $i$ as $C_t(i) = \sqrt{2 \sigma_{t, i}^2 \log(1 / \delta)}$, and the ``good event"
$
  E_t
  = \set{\forall i \in [K]: \abs{\mu_i -  \hat{\mu}_{t,i}} \leq C_t(i)}
$,
same as what we introduced in the proof of \cref{thm:bayes_known}. 
%\todob{Refer to earlier defined concepts instead of redefining them, especially if you want to make a point that this proof is built on another one.}
%which essentially indicates the ``good event"' %\todob{Use \say{this} for quotes.} when all the confidence intervals at round $t$ hold good. 
%
Further note, given $H_t$ and $\sigma_{t,i}^2$ (or equivalently $\lambda_{t,i})$),  $\mu_i - \hat{\mu}_{t,i} \mid \sigma_{t,i}^2, H_t \sim \cN(\mathbf{0}, \sigma_{t,i}^2)$, 
since given $H_t$ and $\sigma_t$, $\mu_i$ has the posterior $\mu_i \sim \cN(\hat \mu_{t,i}, \sigma_{t,i}^{2})$, where recall we defined $\sigma_{t,i}^2 = \frac{1}{\kappa_{t,i}\lambda_{t,i}}$ and $\lambda_{t,i} = \frac{1}{\sigma_{t,i}^2} \sim \text{Gam}(\alpha_{t,i},\beta_{t,i})$.
 %\red{I am confused a bit}
Thus following same analysis as in \eqref{eq:thm11}, we get: 
%So we w.h.p. $(1-\delta')$, we have \red{checK!}:
%
\begin{align}
  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{\bsigma_t, H_t}
  \leq \sum_{i = 1}^K \sqrt{\frac{\sigma_{t,i}^2}{2 \pi}} \delta.
  \label{eq:bayes_regret_scale22}
\end{align} %\todob{This seems incorrect. $\sigma_i^2$ should be $\sigma_{t, i}^2$?}
%
Further taking expectation over $H_n, \bsigma, \bmu$, and summing over all $t$ we get:
\begin{align}
\label{eq:long22}
  &\E{\sum_{t = 1}^n(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}
  \leq 
  \delta(2\pi)^{-1/2}\mathbb{E}_{H_n, \bsigma, \bmu}\biggsn{\sum_{t = 1}^n \sum_{i = 1}^K \mathbb{E}_{\bsigma_{t}}[\sigma_{t,i} \mid \bmu,\bsigma, H_t]}
\end{align}
%
Now one of the main novelty of this proof lies in showing:
\begin{align*}
& \mathbb{E}_{H_n, \bsigma, \bmu}\biggsn{\sum_{t = 1}^n \sum_{i = 1}^K \mathbb{E}_{\bsigma_{t}}[\sigma_{t,i} \mid \bmu,\bsigma, H_t]}
\leq 
\sqrt{nK} \sqrt{\sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
+ 
\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)}
\end{align*} 
%\todob{$H_n$ in the inner expectation should be $H_t$. We should not condition on the future.} 
%\todob{Why conditioning on $\bmu$? - \red{used later in the detailed proof, note $\sigma_{t,i}$ depends on $\mu$, etc.}} 
which involves a series of careful applications of tower rule of expectations and \cref{lem:reciprocal sum} (in \cref{app:unknown}) -- this is the most novel part of this proof which shows an elegant way to upper bound $\E{\sigma_{t, i}}$ for any $i$ and $t$. While the analysis is non-trivial and lengthy, the complete proof can be found in \cref{app:unknown}.
%
Now coming back to the original Bayesian regret expression, note that the regret expression from \eqref{eq:term5} 
%\todob{A forward reference to Appendix. Fix if it as an error or say "in Appendix" - \red{I wrote its in Appendix B? Not sure of the issue}} 
can be actually decomposed based on $E_t$ and $\bar E_t$ as:
\begin{align}
\label{eq:term22}
  \condE{{\hat{\mu}_{t,A^*}}  - \mu_{A^*}}{H_t} 
  & \leq \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{E_t}}{H_t} 
  +
  \condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t} \nonumber
  \\
  & \leq \condE{C_t(A^*)}{H_t} \nonumber
  +\condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t}
  \\
  & = \condE{C_t(A_t)}{H_t} 
  +\condE{({\hat{\mu}_{t,A^*}} - \mu_{A^*}) \I{\bar{E}_t}}{H_t},
\end{align}
where the last equality follows from the fact that $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions, as both $\tilde \mu_{t,i}$ and $\mu_{t,i}$ can be seen as independent posterior samples from a NG$(\hat \mu_{t,i},\kappa_{t,i},\alpha_{t,i},\beta_{t,i})$.% distribution marginalized over $\lambda$. %\todob{It is unclear what "marginalized over $\lambda$" means here.}

\iffalse%%%%%%%%%%%%%%%%%%
More precisely for any realization of $\tilde \mu_{t,i}$ and $\mu_i$:
\[
P(\tilde \mu_{t,i}) = \int_{\lambda} P\big( (\tilde \mu_{t,i}, \lambda_{t,i} \sim \text{NG}(\hat \mu_{t,i},\kappa_{t,i},\alpha_{t,i},\beta_{t,i}) ) \big)d\lambda  ~~~\text{and}
\]
\[
P(\mu_{i}) = \int_{\lambda} P\big( (\tilde \mu_{t,i}, \lambda_{t,i} \sim \text{NG}(\hat \mu_{t,i},\kappa_{t,i},\alpha_{t,i},\beta_{t,i}) ) \big)d\lambda,
\]
they follow the same distribution. 
\fi%%%%%%%%%%%%%%%%%%%%%%%

The second term in \eqref{eq:term22} is bounded similarly to \eqref{eq:long22}. 
%
The only remaining task is to bound the first term in \eqref{eq:term22}, which again involves some non-trivial chain of inequalities involving tower rule of expectations, Cauchy-Schwarz, Jensen's and \cref{lem:reciprocal root sum}, detailed in \cref{app:unknown}. As a result, the final expression can be bounded as:% \todob{Indicators below are written differently from earlier. Choose one notation for indicators and stick to it.}
%
\begin{align*}
  &\E{\sum_{t = 1}^n C_t(A_t)}
  \\
  &\overset{}{\leq} \E{ \bigg[ \sqrt{\sum_{i = 1}^K N_n(i)} \sqrt{\sum_{i = 1}^K \sum_{t = 1}^n C_t^2(i)\I{A_t = i} }\bigg]}
  \\
  &\overset{}{=}  \bigg[ \sqrt{n} \sqrt{\E{ \sum_{i = 1}^K \sum_{t = 1}^n {2 \sigma_{t, i}^2 \log(1 / \delta)}\I{A_t = i} }\bigg]}
  \\
  &\overset{}{=}  \bigg[ \sqrt{n} \sqrt{\E{ \sum_{i = 1}^K \sum_{s = 0}^{N_n(i)} \bign{2 \sigma_{t, i}^2 \log(1 / \delta)}\I{A_t = i} }\bigg]}
  \\
  & \leq \sqrt{n\log \frac{1}{\delta}} \sqrt{\sum_{i = 1}^K \sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
+ 
\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)}
 \,.
\end{align*}
Finally, chaining of the above inequalities on the summation of the instantaneous regret over $n$ rounds, from \eqref{eq:term22} we get:
%\todob{Why the reference to \eqref{eq:term22}?}
%
\begin{align*}
%\label{eq:term2}
  &\mathbb{E}_{H_n,\bmu,\bsigma}\biggsn{\sum_{t = 1}^n\condE{{\hat{\mu}_{t,A^*}}  - \mu_{A^*}}{H_t}} \nonumber
  \leq
  C \biggn{\frac{\delta \sqrt{nK}}{\sqrt{2\pi}} + \sqrt{n\log \frac{1}{\delta}}}
  \,,
\end{align*}
which proves the claim.

%\todob{The structure of the current proof is not good because the connection to the known variance proof seems weak. I would do the following:

%1) Start with (4).

%2) Apply (7).

%3) Bound the second term in (7) using (5). Leave $\sigma_{t, i}^2$ the way it is.

%4) Bound the first term in (7). Tie it to how we bound an analogous term in Theorem 1.

%5) Massage the final bound to get $\E{\sigma_{t, i}^2}$. Finally, apply the main trick, that $\E{\sigma_{t, i}^2}$ has a nice upper bound for any $i$ and $t$. - \red{done!}}

% noting $C:= \sum_{i = 1}^K \frac{4 \beta_{0,i} + \frac{\beta_{0,i}}{\alpha_{0,i}-1}}{2\kappa_{0,i}(\alpha_{0,1}-1)}
%+ 
%\sum_{i = 1}^K \frac{5\beta_{0,i}}{(\alpha_{0,i}-1)}\log\Big(1 + \frac{n}{\kappa_{0,i}}\Big)$. 

% %As described in \cref{rem:unknown}, \cref{thm:bayes_unknown2} gives the first variance-sensitive regret bounds with TS, even for the case of unknown arm variances. 
% We also performed empirical studies to evaluate the efficacy
% of \cref{alg:bayes_unknown}. As reported in the next section, \varts\, significantly overperforms the existing baselines, \ucb \citep{auer02finitetime}, \ucbtuned \citep{auer02finitetime} and \ucbv \citep{audibert09exploration} even in the misspecified setting (i.e. when arm rewards $\bmu$ are not necessarily Gaussian). %, which seconds the practicality of our proposed algorithm, beyond its theoretical regret bounds. 
% \todob{I do not understand this discussion. Simply say that experiments come next. We can summarize the experimental results at the beginning of the experimental section if you want to. - \red{shortened, done!}}
