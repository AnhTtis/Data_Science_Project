%\vspace{-4pt}
\section{Problem Setup}
\label{sec:prob}
%\vspace{-4pt}

\textbf{Notation.} The set $\set{1, \dots, n}$ is denoted by $[n]$. The indicator $\I{E}$ denotes that event $E$ occurs. We use boldface letters to denote vectors. For any vector $\v \in \R^d$, we denote by $v_i$ its $i$-th entry, or sometimes simply $v(i)$. We denote the entry-wise square of $\v$ by $\v^2$. A diagonal matrix with entries $\v$ is denoted by $\diag{\v}$. $\tilde{O}$ denotes big O notation up to polylogarithmic factors. Gaussian, Gamma, and Gaussian-Gamma distributions are denoted by $\cN$, $\text{Gam}$, and $NG$, respectively.

\textbf{Setting.} A bandit \emph{instance} is a pair of mean arm rewards and reward variances, $(\bmu, \bsigma^2)$, where $\bmu \in \R^K$ is a vector of mean arm rewards, $\bsigma^2 \in \R_{\geq 0}^K$ are the reward variances, and $K$ is the number of arms. We propose algorithms and analyze them for both when the reward variances $\bsigma^2$ are known (\cref{sec:bayes_known}) and unknown (\cref{sec:bayes_unknown2}).

\textbf{Feedback model.} The agent interacts with the bandit instance $(\bmu, \bsigma^2)$ for $n$ rounds. In round $t \in [n]$, it pulls one arm and observes a stochastic realization of its reward. We denote the pulled arm in round $t$ by $A_t \in [K]$, a stochastic reward vector for all arms in round $t$ by $\boldsymbol{x}_t \in \realset^K$, and the reward of arm $i \in [K]$ by $x_{t, i} \in \realset$. The rewards are sampled from a Gaussian distribution, $x_{t, i} \sim \cN(\mu_i, \sigma_i^2)$. The interactions of the agent up to round $t$ are summarized by a \emph{history} $H_s = \big( A_{1}, x_{1,A_1}, \dots, A_{t}, x_{t, A_t}\big)$.

\textbf{Bayesian bandits setting.} We consider a \emph{Bayesian multi-armed bandit} \citep{russo14learning,russo18tutorial,kveton21metathompson,hong22hierarchical} where the bandit instance is either fully or partially random:
%
\textbf{(i).} When \emph{reward variances are known} (\cref{sec:bayes_known}): The bandit instance $(\bmu,\bsigma)$ is generated as follows. The mean arm rewards are sampled from a Gaussian distribution, $\bmu \sim P_0 = \cN(\bmu_0, \diag{\bsigma_0^2})$, where $\bmu_0 \in \realset^K$ and $\bsigma_0^2 \in \R_{\geq 0}^K$ are the prior means and variances of $\bmu$, respectively. Both $\bmu_0$ and $\bsigma_0^2$ are assumed to be known by the agent. The reward variances $\bsigma^2$ are also known.
%
\textbf{(ii).} When \emph{reward variances are unknown} (\cref{sec:bayes_unknown2}): We assume that the bandit instance $(\bmu, \bsigma)$ is sampled from a Gaussian-Gamma prior distribution. More specifically, for any arm $i$, the mean and variance of its rewards are sampled as $(\mu_i, \sigma_i^{-2}) \sim NG(\mu_{0, i}, \kappa_{0, i}, \alpha_{0, i}, \beta_{0, i})$, where $(\bmu_0, \bkappa_0, \balpha_0, \bbeta_0)$ are known prior parameters. This can also be seen as first sampling $\sigma_i^{-2} \sim \text{Gam}(\alpha_{0,i},\beta_{0,i})$ followed by $\mu_i \sim \cN(\mu_{0,i}, \frac{\sigma_i^2}{\kappa_{0,i}})$. This equivalence follows from the basic properties of the Gaussian-Gamma distribution, as shown in \cref{lem:gauss_gam} in Appendix.

\textbf{Regret.} We measure the $n$-round \emph{Bayes regret} of a learning agent with instance prior $P_0$ as:
\begin{align}
  \textstyle
  R_n
  =  \E{{\sum_{t = 1}^n
  \mu_{A^*} - n\mu_{A_t}}}\,,
  \label{eq:reg_bayes}
\end{align} 
where $A^* = \argmax_{i \in [K]} \mu_{i}$ denotes the \emph{optimal arm}. The above expectation is over the mean arm rewards $\bmu \sim P_0$, unlike in the frequentist setting where $\bmu$ would be unknown but fixed \citep{lattimore19bandit}. The randomness in the above expectation also includes how the algorithm chooses $A_t$ and the randomness in the observed bandit feedback $x_{t,A_t} \sim \cN(\mu_{A_t},\sigma_{A_t}^2)$.

We depart from the classic bandit setting \citep{auer02finitetime,abbasi-yadkori11improved,lattimore2020bandit} in two major ways. First, we consider Gaussian reward noise, as opposing to more general sub-Gaussian noise. The Gaussian noise and corresponding conjugate priors lead to closed-form posteriors in our algorithms and analyses, which simplifies them. This is why this choice has been popular in recent Bayesian analyses \citep{lu19informationtheoretic,kveton21metathompson,wan21metadatabased,hong22hierarchical,hong22deep}. Second, our regret is Bayesian, on average over bandit instances. An alternative would be the frequentist regret, which holds for any bounded bandit instance. We choose the Bayes regret because it can be used to capture the relation between the bandit instance and its prior, and thus show benefits of informative priors. We discuss this in depth throughout the paper, and especially after \cref{thm:bayes_known} and \cref{thm:bayes_unknown2}. To alleviate concerns about Gaussian posteriors in the algorithm design, we experiment with a variety of other bandit problems in \cref{sec:experiments}.
