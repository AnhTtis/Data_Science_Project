%\vspace{-4pt}
\section{Gaussian Bandit with Known Variances}
\label{sec:bayes_known}
%\vspace{-4pt}

We start with the Bayesian setting with Gaussian rewards and known heterogeneous reward variances. In \cref{sec:bayes_known_algo}, we introduce a Thompson sampling algorithm \citep{thompson33likelihood,chapelle11empirical,agrawal12analysis,russo14learning,gopalan14thompson} for this setting. Gaussian TS is straightforward and appeared in many prior works, starting with \citet{agrawal13further}. We state the regret bound and discuss it in \cref{sec:known_reg}. The regret bound scales roughly as: 
%\todob{Make sure that the parentheses are tall enough for the math inside them. See below. This is at multiple places.} 
%\todob{Do we do colons before display math? Choose one style and stick to it. This is at multiple places.}
$%\begin{align}
  \textstyle
  \sqrt{n\log n} \sqrt{\sum_{i =1}^K\sigma_i^2
  \log\Big(1 + n\frac{\sigma_{0,i}^{2}}{\sigma_i^{2}}\Big)}\,.
  \label{eq:rough bound known}
$%\end{align}
One notable property of the bound is that it goes to zero when the reward variances $\sigma_i^2$ or the prior variances of the mean arm rewards $\sigma_{0, i}^2$ do. Although the bound is novel, its proof mostly follows \citet{kveton21metathompson}. The main reason for stating the bound is to contrast it with the main result in \cref{sec:bayes_unknown2}.

\input{algo_bayes_known.tex}


\subsection{Regret Analysis}
\label{sec:known_reg}

Before analyzing \cref{alg:bayes_known}, let us recall the setting again: The mean arm rewards are sampled from a Gaussian prior, $\bmu \sim P_0 = \cN(\bmu_0, \diag{\bsigma_0^2})$, where $\bmu_0 \in \realset^K$ and $\bsigma_0^2 \in \R_{\geq 0}^K$ are the prior means and variances of $\bmu$, respectively. The reward of arm $i$ in round $t$ is sampled as $x_{t,i} \sim \cN(\mu_i,\sigma_i^2)$. Both $\bmu_0$ and $\bsigma_0^2$ and reward variances $\bsigma^2$ are fixed and known. Our regret bound is presented below.

\begin{restatable}[Variance-dependent regret bound for known variances]{thm}{bayesknown}
\label{thm:bayes_known} Consider the above setting. Then for any $\delta > 0$, the Bayes regret of Gaussian TS is bounded as:
\begin{align*}
  & R_n
  \leq \sum_{i = 1}^K\sqrt{\frac{2 \sigma_{0,i}^2}{\pi}} n \delta + 
  \sqrt{2n} \sqrt{\sum_{i =1}^K\sigma_i^2\Big(\log(1 + n\sigma_{0,i}^{2}\sigma_i^{-2}) + \sigma_{0,i}^{2}\sigma_i^{-2}\Big)\log(1/\delta)}\,.
\end{align*}
\end{restatable}

The complete proof of \cref{thm:bayes_known} is in \cref{app:known}. We discuss the bound below.

\textbf{Dependence on all parameters of interest and prior.} For $\delta = 1 / n$, the bound in \cref{thm:bayes_known} scales roughly as
$
  \tilde{O}\left(\sqrt{n \sum_{i = 1}^K\sigma_i^2 \log\Bign{1 + n\sigma_{0,i}^{2}\sigma_i^{-2} }} + \sqrt{n \sum_{i = 1}^K \sigma_{0,i}^2}\right)
$.
Note that we ignore the first term in \cref{thm:bayes_known}, which is order-wise dominated by the second term when $\delta = 1 / n$. Our bound has several properties that we discuss next. First, it matches the usual $\sqrt{n}$ dependence of all classic Bayes regret bounds \citep{russo14learning,russo16information,lu19informationtheoretic}. Second, it increases with variances $\sigma_i^2$ of individual arm rewards, which is expected because higher reward variances make learning harder. Third, the bound can be viewed as a generalization of existing bounds that assume homogeneous reward variance. Specifically, \citet{kveton21metathompson} derived a $\tilde{O}(\sqrt{\sigma^2 K n})$ Bayes regret bound in Lemma 4 under the assumption that the reward distribution of arm $i$ is $\cN(\mu_i, \sigma^2)$. We match it when $\sigma_i = \sigma$ for all arms $i$. Fourth, the bound approaches zero as $\sigma_{0, i} \to 0$. In this setting, Gaussian TS knows the mean reward $\mu_i$ almost with certainty because its prior variance $\sigma_{0, i}^2$ is low, and no exploration is necessary. This is a unique property of Bayes regret bounds that is not captured by any frequentist analysis, such as that of \ucbnormal \citep{auer02finitetime}.

\textbf{Regret optimality.} Starting with the seminal works of \citet{russo14learning,russo16information}, all Bayes regret bounds are $\tilde{O}(\sqrt{n})$ and do not have finite-time instance-dependent lower bounds. \citet{lattimore2020bandit} derived a $\Omega(\sqrt{K n})$ asymptotic lower bound for a $K$-armed bandit as $n \to \infty$ (Theorem 35.1). Our regret bound (\cref{thm:bayes_known}) matches this rate when the prior and reward variances are the same for all arms, such as $\sigma_{0, i} = \sigma_i = 1$ for any $i \in [K]$. In addition, it gives an improved dependence on lower reward variances and more informative priors, which implies faster learning rates in these regimes. In fact, when the prior variances of all mean arm rewards go to zero, $\sigma_{0, i} \to 0$ for any $i \in [K]$, our bound goes to zero; as expected. Thus we conjecture that our regret bound is worst-case optimal. The only other lower bound that we are aware of is $\Omega(\log^2 n)$ for a $K$-armed bandit (Theorem 3 in \citet{lai87adaptive}). This lower bound is asymptotic and applies only to exponential-family reward distributions with a single parameter, which excludes Gaussian distributions because they have two parameters. To conclude, we believe that deriving a tight finite-time $\Omega({\sqrt{K n}})$ lower bound for our setting is an important problem, but this may require new techniques and should be of independent interest to the Bayesian community itself.

\iffalse%%%%%%%%%%%% 
\todob{
The above paragraph is a potential minefield. I would rephrase it as:

* All recent Bayes regret analyses are $\tilde{O}(\sqrt{n})$.

* There is a $\Omega(\sqrt{K n})$ lower bound that we match for some setting.

* There is a recent Bayes regret analysis of BayesUCB (Logarithmic Bayes Regret Bounds) with a $O(\log^2 n)$ regret bound that matches the $\Omega(\log^2 n)$ lower bound of \citet{lai87adaptive} in Theorem 3. Both bounds trade off $\sqrt{n}$ for $\log n$ and an additional prior-dependent constant. We do not have a dependence on the additional prior-dependent constant. It is also unclear if a $O(\log^2 n)$ regret bound can be proved for TS.
}
\fi%%%%%%%%%%%% 

%\todob{This is a good place to compare to existing TS bounds. Check Theorem 3 in \citet{agrawal13further}. This is a gap-free regret bound of Gaussian TS for any bandit instance with $[0, 1]$ rewards. Note that the bound does not depend on the actual prior. The prior in the algorithm is $\cN(0, 1)$ and the reward variance is $1$. Therefore, the regret does not go to zero as the width of the actual prior decreases. - \red{there is no Thm-$3$ in the paper? I added a discussion after Thm 4}}

%\todob{We need more discussion here. \citet{lai87adaptive} has two main results: a $O(\log^2 n)$ asymptotic upper bound in Theorem 3, claim (i); and a $\Omega(\log^2 n)$ asymptotic lower bound in Theorem 3, claim (ii). These two match. The issue is that the upper bound is for a frequentist algorithm. Therefore, the match shows that the prior does not matter. Thus this is not the right lower bound for \cref{thm:bayes_known}, where the analyzed Bayesian algorithm can be arbitrarily better than \ucb.

%There is only one logarithmic Bayes regret analysis that is finite-time and shows the value of the prior. This is BayesUCB in "Logarithmic Bayes Regret Bounds". While it is plausible that the analysis can be extended to TS, this is orthogonal to the direction of this work, to show the value of variance adaptation.}

\iffalse%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof Sketch of \cref{thm:bayes_known}}
\label{sec:known_reg_prf}

We sketch the proof of \cref{thm:bayes_known} below. The complete proof can be found in \cref{app:known}.

Let $\bmu_t \in \realset^K$ denote the vector of unknown mean arm rewards at round $t$, sampled as $\mu_{t,i} \sim \cN(\hat\mu_{t,i},\sigma_{t,i}^2)$ for all $i \in [K]$. 
%\todob{$\bmu_{t,i}$ should not be bold.} \todob{Say that $\bmu_t$ are the true unknown mean arm rewards. This is to distinguish it from $\tilde{\bmu}_t$.} 
Let $H_t$ denote the history in round $t$. We also define a high-probability confidence interval for arm $i$ in round $t$ as $C_t(i) = \sqrt{2 \sigma_{t, i}^2 \log(1 / \delta)}$, where $\delta > 0$ is the confidence level, and event:
%
\begin{align*}
  E_t
  = \set{\forall i \in [K]: \abs{\mu_i -  \hat{\mu}_{t,i}} \leq C_t(i)}
\end{align*}
%
that all confidence intervals in round $t$ hold. Now let us fix round $t$. We start by noting that the instantaneous regret at round $t$ can be bounded as:
%
\begin{align*}
  & \E{\mu_{A^*} - \mu_{A_t}}
  = \E{\condE{\mu_{A^*} - \mu_{A_t}}{H_t}}  
  \leq 
  \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} +
  \E{\condE{{{\hat{\mu}_{t,A_t}}} - \mu_{A_t}}{H_t}}
  \\ & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}}\,.
\end{align*} 

%\todob{We also need to say that "The second equality holds because $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions, and $\hat{\mu}_t$ is deterministic given history $H_t$" - \red{this is discussed later}}
%
%The first equality is an application of the tower rule. The second equality holds because $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions, and $\hat{\mu}_t$ is deterministic given history $H_t$. 
The last equality holds since given $H_t$, clearly $E[\mu_{i}\mid H_t] 
 = \hat{\mu}_{t,i}$ for any $i \in [K]$.
% 
Then we introduce event $E_t$ to deal with the remaining term as:
\begin{align*}
  & \condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}} }{H_t} =  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar E_t}}{H_t}
  + \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{E_t}}{H_t}
  \\
  & \leq \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{H_t} + \condE{C_t(A_t)}{H_t}\,,
\end{align*}
where the inequality follows from the observation that
$
  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{E_t}}{H_t}
  \leq \condE{C_t(A^*)}{H_t}\,,
$
and further $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions given $H_t$. 
%
Also given history $H_t$, since $\hat{\mu}_t$ is deterministic, $\bmu - \hat{\bmu}_t \mid H_t \sim \cN(\mathbf{0}, \Sigma_t)$, we get:
%\red{above should have the scale factor $b \geq (\mu^* - \mu_i)$?, what is a suitable $b$?}
\begin{align}
\label{eq:thm11}
  & \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{H_t} 
  \leq \sum_{i = 1}^K \frac{1}{\sqrt{2 \pi \sigma_{t, i}^2}}
  \int_{x = C_t(i)}^\infty x
  \exp\left[- \frac{x^2}{2 \sigma_{t, i}^2}\right] \dif x
   = \sum_{i = 1}^K \sqrt{\frac{\sigma_{t, i}^2}{2 \pi}} \delta
  \leq \sum_{i = 1}^K \sqrt{\frac{\sigma_{0,i}^2}{2 \pi}}  \delta\,.
\end{align}
%
\iffalse%%%%%%%%%%%%%%%%
Now we chain all inequalities for the regret in round $t$ and get
\begin{align*}
  \E{\mu_{A^*} - \mu_{A_t}}
  \leq \E{C_t(A_t)} + \sum_{i = 1}^K\sqrt{\frac{2 \sigma_{0,i}^2}{\pi}}  \delta\,.
\end{align*}
\fi%%%%%%%%%%%%%%%%%%%
%
Having established the above bounds, summing the instantaneous regret over $n$ rounds, the Bayes regret of \cref{alg:bayes_known} can be bounded as:
\begin{align*}
  \E{\sum_{t = 1}^n \mu_{A^*} - \mu_{A_t}}
  \leq \E{\sum_{t = 1}^n C_t(A_t)} +
  \sum_{i = 1}^K\sqrt{\frac{2 \sigma_{0,i}^2}{\pi}} n \delta\,.
\end{align*}
%
The last part is to bound $\E{\sum_{t = 1}^n C_t(A_t)}$ from above. We apply a series of Cauchy-Schwarz inequalities, carefully handle the form of the confidence terms $C_t(i)$, and apply \cref{lem:reciprocal sum}, which yields: %\todob{Two indicators below are written differently from earlier. Choose one notation for indicators and stick to it.}
%
\begin{align*}
  & \E{\sum_{t = 1}^n C_t(A_t)}
  = \E{\sum_{t = 1}^n \sum_{i = 1}^K \I{A_t = i} C_t(i)} 
  \leq \E{\sum_{i = 1}^K \bigg[ \sqrt{N_t(i)} \sqrt{\sum_{t = 1}^n C_t^2(i)\I{A_t = i} }\bigg]}
  \\
  & \leq  \sqrt{2n} \sqrt{\sum_{i =1}^K\sigma_i^2\log(1 + n\sigma_{0,i}^{2}\sigma_i^{-2})\log(1/\delta)} + \sqrt{2n\sum_{i = 1}^K\sigma_{0,i}^{2}\log(1/\delta)}
  \,,
\end{align*} 
where recall from \cref{alg:bayes_known} that $N_{t+1}(i) = N_t(i)+\I{A_t=i}$ for all $i \in [K]$ denotes the number of times arm $i$ is pulled in $t$ rounds.
%
%\todob{$N_t(i)$ is not properly quantified.} 
%\todob{A simpler application of Cauchy-Schwarz would be $\sum_{t = 1}^n C_t(A_t) \leq \sqrt{n} \sqrt{\sum_{t = 1}^n C_t^2(A_t)} = \sqrt{n} \sqrt{\sum_{i = 1}^K \sum_{t = 1}^n \I{A_t = i} C_t^2(A_t)}$. Note that there is no need for the expectation. This is all worst case.}
Chaining of all inequalities concludes the proof.

% While this section presents a clean variance-sensitive regret bound for Gaussian bandits (see \cref{thm:bayes_known} and its following discussion), our primary goal is to design a general TS algorithm with variance adaptive regret bounds even when the reward variances are unknown. We present this algorithm next, in \cref{alg:bayes_unknown}, and analyze its regret in \cref{thm:bayes_unknown2}.
\fi%%%%%%%%%%%%%%%%%%%%%%