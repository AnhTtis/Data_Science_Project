%\vspace{-4pt}
\section{Experiments}
\label{sec:experiments}
%\vspace{-4pt}

We also study the empirical performance of our proposed algorithms. Since \varts does not assume that the reward variances are known, and thus is more realistic than \cref{alg:bayes_known}, we focus on \varts. We conduct four experiments. First, we evaluate \varts in a Bernoulli bandit, which is a standard bandit benchmark. Second, we experiment with beta reward distributions. Their support is $[0, 1]$, similarly to Bernoulli distributions, but their variances are not fully determined by their means. Since \varts is designed for Gaussian bandits, the first two experiments also evaluate the robustness of \varts to model misspecification. Third, we experiment with a Gaussian bandit. Finally, we vary the number of arms and observe how the performance of \varts scales with problem size.

%\vspace{-5pt}
\subsection{Experimental Setup}
%\vspace{-3pt}

All problems are Bayesian bandits, where the mean arm rewards are sampled from some prior distribution. In a Gaussian bandit, \varts is run with the true $(\bmu_0, \bkappa_0, \balpha_0, \bbeta_0)$. In other problems, the hyper-parameters of \varts are set using the method of moments from samples from the prior. In particular, for a given Bayesian bandit, let $\bar{\mu}_i$ and $v_i$ be the estimated mean and variance of the mean reward of arm $i$ sampled from its prior, respectively. Moreover, let $\bar{\lambda}_i$ and $\nu_i$ be the estimated mean and variance of the precision of reward distribution of arm $i$ sampled from its prior, respectively. Then, based on these statistics, we estimate the unknown hyper-parameters as $\mu_{0, i} = \bar{\mu}_i$, $\beta_{0, i} = \bar{\lambda}_i / \nu_i$, $\alpha_{0, i} = \beta_{0, i} / \bar{\lambda}_i$, and $\kappa_{0, i} = \beta_{0, i} / (\alpha_{0, i} v_i)$ using their empirical estimates \citep{pearson36method}.

We compare \varts to several baselines. \ucb \citep{auer02finitetime} is the most popular algorithm for stochastic $K$-armed bandits with $[0, 1]$ rewards. It does not adapt to the reward variances and is expected to be conservative. We also consider its two variants that adapt to the variances: \ucbtuned \citep{auer02finitetime} and \ucbv \citep{audibert09exploration}. \ucbtuned is a heuristic that performs well in practice. \ucbv uses empirical Bernstein confidence intervals and has theoretical guarantees. We implement both algorithms for $[0, 1]$ rewards. The next two baselines are Thompson sampling algorithms: Bernoulli and Gaussian TS \citep{agrawal13further}. Bernoulli TS has a $\mathrm{Beta}(1, 1)$ prior, as analyzed in \citet{agrawal13further}. When the rewards $Y_{t, i}$ are not binary, we clip them to $[0, 1]$ and then apply Bernoulli rounding: a reward $Y_{t, i} \in [0, 1]$ is replaced with $1$ with probability $Y_{t, i}$ and with $0$ otherwise. Gaussian TS has a $\cN(0, 1)$ prior and unit reward variances, as analyzed in \citet{agrawal13further}. The last two baselines are Thompson sampling with unknown reward variances \citep{honda14optimality,MV20}. We implement Algorithm 1 in \citet{honda14optimality} and call it \htts, and Algorithm 3 in \citet{MV20} for $\rho \to \infty$ and call it \ztts. Note that \ztts is \varts where $\mu_{0, i} = 0$, $\kappa_{0, i} = 0$, $\alpha_{0, i} = 0.5$, and $\beta_{0, i} = 0.5$. The shortcoming of all TS baselines is that they are designed to have frequentist per-instance guarantees. Therefore, their priors are set too conservatively to compete with \varts, which takes the true prior or its estimate as an input. All simulations consider $n = 2\,000$ and are averaged over $1\,000$ randomly initialized runs.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=5.5in]{Figures/Synthetic.pdf}
  \vspace{-0.35in}
  \caption{\varts compared to $7$ baselines. The plots share legends.}
  \label{fig:synthetic}
\end{figure*}

\begin{figure*}[t!]
  \centering
  \includegraphics[width=5.5in]{Figures/Scaling.pdf}
  \vspace{-0.35in}
  \caption{\varts with $7$ baselines as we vary the number of arms $K$. The plots share legends.}
  \label{fig:scaling}
\end{figure*}

%\vspace{-3pt}
\subsection{Bernoulli Bandit}
\label{sec:bernoulli bandit}

We start with a Bernoulli bandit with $K = 10$ arms. The mean reward of arm $i \in [K]$, $\mu_i$, is sampled i.i.d.\ from prior $\mathrm{Beta}(i, K + 1 - i)$. Since $\E{\mu_i} = i / (K + 1)$ and $\std{\mu_i} \approx 1 / \sqrt{K + 1}$, higher prior means indicate higher $\mu_i$, but it is unlikely that arm $K$ has the highest mean reward.

Our results are reported in \cref{fig:synthetic}a. We observe that \varts and Bernoulli TS have the lowest regret. The latter is not surprising since Bernoulli TS is designed specifically for this problem class. The fact that we match its performance is a testament to adapting to reward variances and using priors. The next three best-performing algorithms (\ucbtuned, \htts, and \ztts) adapt to reward variances but do not use informative priors. The frequentist algorithms with regret bounds (\ucb and \ucbv) have the highest regret because they are too conservative.

%\vspace{-3pt}
\subsection{Beta Bandit}
\label{sec:beta bandit}

The bandit problem in the second experiment is a variant of \cref{sec:bernoulli bandit} where the reward distribution of arm $i$ is $\mathrm{Beta}(s \mu_i, s (1 - \mu_i))$ for $s = 10$. Roughly speaking, this means that the reward variance of arm $i$ is $10$ lower than in \cref{sec:bernoulli bandit}. The rest of the experimental setup is the same as in \cref{sec:bernoulli bandit}.

Our results are reported in \cref{fig:synthetic}b. We observe only two differences from \cref{fig:synthetic}a. First, \varts outperforms Bernoulli TS, because it learns that the arms have $10$ times lower reward variances than in \cref{fig:synthetic}a. Therefore, it can be more aggressive in pulling the optimal arm. Second, both \htts and \ztts outperform \ucbtuned, supposedly due to more principled learning of reward variances.

%\vspace{-3pt}
\subsection{Gaussian Bandit}
\label{sec:gaussian bandit}

The third experiment is with a Gaussian bandit where both the means and variances of rewards are sampled i.i.d.\ from a prior with parameters $\mu_{0, i} = i / (K + 1)$, $\kappa_{0, i} = K$, $\alpha_{0, i} = 4$, and $\beta_{0, i} = 1$. For this setting, $\E{\mu_i} = i / (K + 1)$ and $\std{\mu_i} \approx 1 / \sqrt{K + 1}$. Therefore, higher prior means indicate higher $\mu_i$, but it is unlikely that arm $K$ has the highest mean reward. Moreover, the average reward variance is $0.25$. Therefore, bandit algorithms for $[0, 1]$ rewards are expected to work well.

Our results are reported in \cref{fig:synthetic}c. We observe that \ucbtuned has the lowest regret and \varts performs similarly. This shows the practicality of our design, which is analyzable and comparable to a well-known heuristic without guarantees. All other algorithms have at least $50\%$ higher regret. As before, the frequentist algorithms with regret bounds (\ucb and \ucbv) are overly conservative and among the worst performing baselines.

%\vspace{-4pt}
\subsection{Scalability}
\label{sec:scalability}

We vary the number of arms $K$ and observe how the performance of \varts scales with problem size. This experiment is done in Bernoulli (\cref{sec:bernoulli bandit}), beta (\cref{sec:beta bandit}), and Gaussian (\cref{sec:gaussian bandit}) bandits. Our results in \cref{fig:scaling} shows that the gap between \varts and the baselines increases with $K$. For $K = 32$ and Bernoulli bandit, \varts has at least $3$ times lower regret than any baseline. For $K = 32$ and beta bandit, \varts has at least $5$ times lower regret than any baseline. For $K = 32$ and Gaussian bandit, \varts has at least $2$ times lower regret than any baseline. These gains are driven by adapting to reward variances and using priors, on both the mean reward and its variance.
