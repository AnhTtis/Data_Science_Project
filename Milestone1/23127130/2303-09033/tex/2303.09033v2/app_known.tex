\section{Additional proofs for \cref{sec:bayes_known}}
\label{app:known}

\bayesknown*

\begin{proof}[Proof of \cref{thm:bayes_known}]
Recall we denote by $\hat{\bmu}_t \in \realset^K$ the MAP estimate of $\bmu$ at round $t$, and let $\bmu_t \in \realset^K$ be a random posterior sample in round $t$ such that $\bmu_{t,i} \sim \cN(\hat\mu_{t,i},\sigma_{t,i}^2)$ for all $i \in [K]$.  $H_t$ denote the history in round $t$. Note that in posterior sampling, for any vector $\bnu \in \realset^K$, $\condprob{\bmu_t = \bnu}{H_t} = \condprob{\bmu = \bnu}{H_t}$. Let $A^*$ be the optimal arm under the realized reward vector $\bmu$ and $A_t$ be the optimal arm under $\bmu_t$ (and hence pulled by the TS algorithm at round $t$).

We rely on several properties of Gaussian posterior sampling with a diagonal prior covariance matrix. More specifically, following the results from \cref{lem:gaussian posterior update} or \cite{murphy2007conjugate}, the posterior distribution in round $t$ is $\cN(\hat{\bmu}_t, \Sigma_t)$, where $\Sigma_t = \diag{(\sigma_{t, i}^2)_{i = 1}^K}$ is a diagonal covariance matrix with non-zero entries:
\begin{align*}
  \sigma_{t, i}^2
  = \frac{1}{\sigma_{0,i}^{-2} + N_t(i) \sigma_i^{-2}}
  = \frac{\sigma_i^2}{\sigma_i^2 \sigma_{0,i}^{-2} + N_t(i)}\,,
\end{align*}
and $N_t(i)= \sum_{s = 1}^{t-1} \I{A_t = i}$ denotes the number of pulls of arm $i$ up to round $t$. Accordingly, a high-probability confidence interval of arm $i$ in round $t$ is $C_t(i) = \sqrt{2 \sigma_{t, i}^2 \log(1 / \delta)}$, where $\delta > 0$ is the confidence level. Let us define an event:
\begin{align*}
  E_t
  = \set{\forall i \in [K]: \abs{\mu_i -  \hat{\mu}_{t,i}} \leq C_t(i)},
\end{align*}
that implies all confidence intervals in round $t$ hold.

Fix round $t$. Now we bound the regret in round $t$ as follows.  
We start by noting that the regret can be decomposed as:
\begin{align*}
  \E{\mu_{A^*} - \mu_{A_t}}
  & = \E{\condE{\mu_{A^*} - \mu_{A_t}}{H_t}} \\
  & \leq \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}} +
  \E{\condE{{{\hat{\mu}_{t,A_t}}} - \mu_{A_t}}{H_t}}
  \\ & = \E{\condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}}}{H_t}}\,.
\end{align*} 

%The first equality is an application of the tower rule. The second equality holds because $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions, and $\hat{\mu}_t$ is deterministic given history $H_t$. 
The last equality holds since given $H_t$, clearly $E[\mu_{i}\mid H_t] 
 = \hat{\mu}_{t,i}$ for any $i \in [K]$.

Now let us deal with the remaining term in the decomposition. Fix history $H_t$. Then we introduce event $E_t$ and get
\begin{align*}
  & \condE{\mu_{A^*} - {\hat{\mu}_{t,A^*}} }{H_t}
  \\
  & = \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar E_t}}{H_t}
  + \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{E_t}}{H_t}
  \\
  & \leq \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{H_t} + \condE{C_t(A_t)}{H_t}\,,
\end{align*}
where the inequality follows from the observation that

\begin{align*}
  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{E_t}}{H_t}
  \leq \condE{C_t(A^*)}{H_t}\,,
\end{align*}
and further $A_t \mid H_t$ and $A^* \mid H_t$ have the same distributions given $H_t$. 
%
Now note since $\bmu - \hat{\bmu}_t \mid H_t \sim \cN(\mathbf{0}, \Sigma_t)$, we further have
%\red{above should have the scale factor $b \geq (\mu^* - \mu_i)$?, what is a suitable $b$?}
\begin{align}
  \condE{(\mu_{A^*} - {\hat{\mu}_{t,A^*}}) \I{\bar{E}_t}}{H_t}
  & \leq \sum_{i = 1}^K \frac{1}{\sqrt{2 \pi \sigma_{t, i}^2}}
  \int_{x = C_t(i)}^\infty x
  \exp\left[- \frac{x^2}{2 \sigma_{t, i}^2}\right] \dif x
  \nonumber \\
  & = \sum_{i = 1}^K - \sqrt{\frac{\sigma_{t, i}^2}{2 \pi}}
  \int_{x = C_t(i)}^\infty \frac{\partial}{\partial x}
  \left(\exp\left[- \frac{x^2}{2 \sigma_{t, i}^2}\right]\right) \dif x
  \nonumber \\
  & = \sum_{i = 1}^K \sqrt{\frac{\sigma_{t, i}^2}{2 \pi}} \delta
  \leq \sqrt{\frac{\sigma_{0,i}^2}{2 \pi}}  \delta\,.
  \label{eq:bayes regret scale}
\end{align}

Now we chain all inequalities for the regret in round $t$ and get
\begin{align*}
  \E{\mu_{A^*} - \mu_{A_t}}
  \leq \E{C_t(A_t)} + \sum_{i = 1}^K\sqrt{\frac{2 \sigma_{0,i}^2}{\pi}}  \delta\,.
\end{align*}
Therefore, the $n$-round Bayes regret is bounded as
\begin{align*}
  \E{\sum_{t = 1}^n \mu_{A^*} - \mu_{A_t}}
  \leq \E{\sum_{t = 1}^n C_t(A_t)} +
  \sum_{i = 1}^K\sqrt{\frac{2 \sigma_{0,i}^2}{\pi}} n \delta\,.
\end{align*}

The last part is to bound $\E{\sum_{t = 1}^n C_t(A_t)}$ from above. Since the confidence interval $C_t(i)$ decreases with each pull of arm $i$, $\sum_{t = 1}^n C_t(A_t)$ is bounded for any $\mu$ by pulling arms in a round robin \citep{russo14learning}, which yields

\iffalse%%%%%%%%%%%%%%%%%%
\red{Bound-1: Round-Robin [WORSE BOUND - CAN IGNORE THIS PART]}

\begin{align*}
  \E{\sum_{t = 1}^n C_t(A_t)}
  &\leq \E{\sum_{t = 1}^n \sum_{i = 1}^K \sqrt{2 \sigma_{t, i}^2 \log(1 / \delta)} }
  =  \E{ \sum_{i = 1}^K \sum_{t = 1}^n \sqrt{\frac{2\sigma_i^2}{\sigma_i^2 \sigma_{0,i}^{-2} + N_t(i)} \log(1 / \delta)}}
  \\	  
  & \leq \sum_{i = 1}^K \sqrt{2 \sigma_i^2 \log(1 / \delta)}
  \sum_{s = 0}^{n} \sqrt{\frac{1}{s + \sigma_i^2 \sigma_{0,i}^{-2}}} 
  \\
  & = \sum_{i = 1}^K \sqrt{2 \sigma_i^2 \log(1 / \delta)} 
  \left(\sum_{s = 1}^{n} \sqrt{\frac{1}{s + \sigma_i^2 \sigma_{0,i}^{-2}}} +  \frac{\sigma_{0,i}}{\sigma_i}\right) 
  \\
  & \leq 2 \sum_{i = 1}^K \sqrt{2 \sigma_i^2 \log(1 / \delta)} 
  \left(\sqrt{n + \sigma_i^2 \sigma_{0,i}^{-2}} -
  \sqrt{\sigma_i^2 \sigma_{0,i}^{-2}}\right) +
  \sqrt{2 \sigma_{0,i}^2 \log(1 / \delta)}K  
  \,.
\end{align*}
The second inequality follows from \cref{lem:reciprocal root sum}. 

\red{Bound-2: Cauchy-Schwarz [TIGHTER BOUND]}
\fi %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{align*}
  \E{\sum_{t = 1}^n C_t(A_t)}
  & = \E{\sum_{t = 1}^n \sum_{i = 1}^K \I{A_t = i} C_t(i)} 
  = \E{\sum_{i = 1}^K \bigg[ \sum_{t = 1}^n \I{A_t = i} C_t(i) \bigg]}
  \\
  &\leq \E{\sum_{i = 1}^K \bigg[ \sqrt{N_t(i)} \sqrt{\sum_{t = 1}^n C_t^2(i)\I{A_t = i} }\bigg]}
  \\
  &=  \E{\sum_{i = 1}^K \bigg[ \sqrt{N_t(i)}  \sqrt{\sum_{t = 1}^n \frac{2\sigma_i^2}{\sigma_i^2 \sigma_{0,i}^{-2} + N_t(i)} \log(1 / \delta)\I{A_t = i}}\bigg]}
  \\
  &=  \E{\sum_{i = 1}^K \bigg[ \sqrt{2N_t(i)\sigma_i^2}  \sqrt{\sum_{s = 1}^{N_n(i)} \frac{1}{\sigma_i^2 \sigma_{0,i}^{-2} + s} \log(1 / \delta) + \frac{\sigma_{0,i}^2}{\sigma_i^2}\log(1/\delta)}\bigg]}
  \\
  &\leq \E{\sum_{i = 1}^K \bigg[ \sqrt{2N_t(i)\sigma_i^2}  \sqrt{\sum_{s = 1}^{n} \frac{1}{\sigma_i^2 \sigma_{0,i}^{-2} + s} \log(1 / \delta) + \frac{\sigma_{0,i}^2}{\sigma_i^2}\log(1/\delta)}\bigg]}
  \\	  
  & \overset{(a)}{\leq} \E{\sum_{i = 1}^K \bigg[ \sqrt{2N_t(i)\sigma_i^2}  \sqrt{\Big(\log(1 + n\sigma_{0,i}^{2}\sigma_i^{-2}) + \sigma_{0,i}^{2}\sigma_i^{-2}\Big)\log(1/\delta)}\bigg]}
  \\
  &= \E{\sum_{i = 1}^K \bigg[ \sqrt{2N_t(i)}  \sqrt{\sigma_i^2\Big(\log(1 + n\sigma_{0,i}^{2}\sigma_i^{-2}) + \sigma_{0,i}^{2}\sigma_i^{-2}\Big)\log(1/\delta)}\bigg]}
  \\
  & \leq  \sqrt{2n} \sqrt{\sum_{i =1}^K\sigma_i^2\Big(\log(1 + n\sigma_{0,i}^{2}\sigma_i^{-2}) + \sigma_{0,i}^{2}\sigma_i^{-2}\Big)\log(1/\delta)}\\
  & \leq  \sqrt{2n} \sqrt{\sum_{i =1}^K\sigma_i^2\log(1 + n\sigma_{0,i}^{2}\sigma_i^{-2})\log(1/\delta)} + \sqrt{2n\sum_{i = 1}^K\sigma_{0,i}^{2}\log(1/\delta)}
  \,,
\end{align*}
where the first and the last inequality follows due to Cauchy-Schwarz, and Inequality $(a)$ is due to \cref{lem:reciprocal sum}. 
%
Chaining all inequalities completes the proof.
\end{proof}

