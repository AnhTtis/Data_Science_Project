%\vspace{-10pt}
\begin{algorithm}[h]
  \caption{Gaussian TS for known reward variances.}
  \label{alg:bayes_known}
  \begin{algorithmic}[1]
    \State \textbf{Inputs:} Prior means $\bmu_0$, prior variances $\bsigma_0^2$, and reward variances $\bsigma^2$
    \State \textbf{Init:} $\forall i \in [K]: N_1(i):=0, \ \hmu_{1,i}:= \mu_{0,i}, \ \sigma_{1,i}:= \sigma_{0,i}, \bar{x}_{1,i}:= 0$ 
    \Statex \vspace{-0.05in}
    \For{$t = 1, \dots, n$} %\todob{Up to $n$. Not up to $T$.}
    		\State Posterior sampling: $\forall i \in [K]: \tilde \mu_{t,i} \sim \cN(\hmu_{t,i},\sigma_{t,i}^2)$
        \State Pull: $A_t:= \arg\max_{i \in [K]}\tilde \mu_{t,i}$
    		\State Reward feedback: $x_{t,A_t} \sim \cN(\mu_{A_t},\sigma_{A_t}^2)$
    		\State Posterior update: 
			%%%%%%    
    		\For{$i = 1, \dots, K$}
			\State $N_{t+1}(i):= N_{t}(i) + \I{A_t = i}$, \ $\sigma_{t+1, i}^2:= \frac{1}{\sigma_0^{-2} + N_{t+1}(i) \sigma_i^{-2}}$
    		%	\State Update posterior variance: 
  		%	\State Update posterior mean: 
     \State $\hmu_{t+1, i}
  := \sigma_{t+1,i}^2\Big( \frac{\mu_{0,i}}{\sigma_{0,i}^2} + \frac{N_{t+1}(i)\bar x_{t+1,i}}{\sigma_{i}^2} \Big)$, s.t. $\bar x_{t+1,i}:= \frac{1}{N_{t+1}(i)}\sum_{s=1}^{t} \I{A_{s}=i} x_{s,i}$
    		\EndFor
	\EndFor
   % \vspace{-5pt}
  \end{algorithmic}
\end{algorithm}


\vspace{-5pt}
\subsection{Gaussian Thompson Sampling}
\label{sec:bayes_known_algo}
\vspace{-2pt}
The key idea in our algorithm is to maintain a posterior distribution over the unknown mean arm rewards $\bmu$ and act optimistically with respect to samples from it. Since $\bmu$ and its rewards are sampled from Gaussian distributions, the posterior is also Gaussian. Specifically, the posterior distribution of arm $i$ in round $t$ is $\cN(\hmu_{t,i},\sigma_{t,i}^2)$, where $\hat \mu_{t, i}$ and $\sigma_{t, i}^2$ are the posterior mean and variance, respectively, of arm $i$ in round $t$. These quantities are initialized as $\hat \mu_{1,i}:= \mu_{0,i}$ and $\sigma_{1,i}:= \sigma_{0,i}$.


Our algorithm is presented in \cref{alg:bayes_known} and we call it \emph{Gaussian TS} due to Gaussian rewards. The algorithm works as follows. In round $t$, it samples the mean reward of each arm $i$ from its posterior, $\tilde \mu_{t, i} \sim \cN(\hmu_{t, i}, \sigma_{t, i}^2)$. After that, the arm with the highest posterior-sampled mean reward is pulled, $A_t:= \arg\max_{i \in [K]}\tilde \mu_{t,i}$. Finally, the algorithm observes a stochastic reward of arm $A_t$, $x_{t,A_t} \sim \cN(\mu_{A_t},\sigma_{A_t}^2)$, and updates its posteriors (\cref{lem:gaussian posterior update} in Appendix) as:
\begin{align*}
  \sigma_{t+1, i}^2
  := \frac{1}{\sigma_0^{-2} + N_{t+1}(i) \sigma_i^{-2}}\,, \quad
  \hmu_{t+1, i}
  := \sigma_{t,i}^2\bigg( \frac{\mu_{0,i}}{\sigma_{0,i}^2} +
  \frac{N_{t+1}(i)\bar x_{t+1,i}}{\sigma_{i}^2} \bigg)\,,
\end{align*}
where $\bar x_{t+1,i}:= \frac{1}{N_{t+1}(i)}\sum_{s=1}^{t} \I{A_{s}=i} x_{s,i}$ %\todob{$x_{s,i}$} 
is the empirical mean reward of arm $i$ at the beginning of round $t+1$ and $N_{t+1}(i)$ is the number of its pulls.

