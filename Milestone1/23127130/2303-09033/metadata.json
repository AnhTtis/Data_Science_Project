{
    "arxiv_id": "2303.09033",
    "paper_title": "Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling",
    "authors": [
        "Aadirupa Saha",
        "Branislav Kveton"
    ],
    "submission_date": "2023-03-16",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG"
    ],
    "abstract": "Most bandit algorithms assume that the reward variance or its upper bound is known. While variance overestimation is usually safe and sound, it increases regret. On the other hand, an underestimated variance may lead to linear regret due to committing early to a suboptimal arm. This motivated prior works on variance-aware frequentist algorithms. We lay foundations for the Bayesian setting. In particular, we study multi-armed bandits with known and \\emph{unknown heterogeneous reward variances}, and develop Thompson sampling algorithms for both and bound their Bayes regret. Our regret bounds decrease with lower reward variances, which make learning easier. The bound for unknown reward variances captures the effect of the prior on learning reward variances and is the first of its kind. Our experiments show the superiority of variance-aware Bayesian algorithms and also highlight their robustness.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09033v1"
    ],
    "publication_venue": null
}