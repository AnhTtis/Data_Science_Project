% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{float}
% Camera ready - 3/13 due
\usepackage{algorithm2e, comment}

\RestyleAlgo{ruled}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Adaptive Endpointing with Deep Contextual Multi$-$armed Bandits}
%
% Single address.
% ---------------
% \name{Do June Min\thanks{*This work was done during the author's Amazon internship.}${^1}{^*}$,  Andreas Stolcke${^2}{^*}$,  Anirudh Raju${^2}{^*}$, Colin Vaz${^2}{^*}$, \\ Di He${^2}{^*}$, Venkatesh Ravichandran${^2}{^*}$ and  Viet Anh Trinh${^2}{^*}$}
% \author{Do June Min\thanks{*This work was done during the author's Amazon internship.}${^1}{^*}$,  Andreas Stolcke${^2}$,  Anirudh Raju${^2}$, Colin Vaz${^2}$ \\ Di He${^2}$, Venkatesh Ravichandran${^2}$ and  Viet Anh Trinh${^2}$}
% \address{${^1}$University of Michigan $\quad$ ${^2}$Amazon Alexa AI, USA \\
% dojmin@umich.edu $\quad$  \{stolcke, ranirudh, vazcoli, deehe, veravic, trinhvie\}@amazon.com}
\name{\parbox{\textwidth}{\centering
Do June Min\thanks{*This work was done during the author's Amazon internship.}${^1}{^*}$,  Andreas Stolcke${^2}$,  Anirudh Raju${^2}$, Colin Vaz${^2}$, \\
Di He${^2}$, Venkatesh Ravichandran${^2}$ and  Viet Anh Trinh${^2}$}}
\address{${^1}$University of Michigan \quad ${^2}$Amazon Alexa AI, USA \\
dojmin@umich.edu \quad  \{stolcke, ranirudh, vazcoli, deehe, veravic, trinhvie\}@amazon.com}

%\newcommand{\squeezeup}{\vspace{-2.5mm}}
\newcommand{\squeezeup}{\vspace{0mm}}
\begin{document}
\ninept
%
\maketitle{}
%
\begin{abstract}
Current endpointing (EP) solutions learn in a supervised framework, which does not allow the model to incorporate feedback and improve in an online setting. Also, it is a common practice to utilize costly grid-search to find the best configuration for an endpointing model. 
%Even when manually fine-tuned models are deployed, the models remain unchanged during deployment. 
In this paper, we aim to provide a solution for adaptive endpointing by proposing an efficient method for choosing an optimal endpointing configuration given utterance-level audio features in an online setting, while avoiding hyperparameter grid-search.
Our method does not require ground truth labels, and only uses online learning from reward signals without requiring annotated labels.
%To that end, we first build a supervised classifier that reduces early cutoff rate without latency degradation.
Specifically, we propose a deep contextual multi-armed bandit-based approach, which combines the representational power of neural networks with the action exploration behavior of Thompson modeling algorithms.
We compare our approach to several baselines, and show that our deep bandit models also succeed in reducing early cutoff errors while maintaining low latency.
\end{abstract}
%
\begin{keywords}
endpointing, multi-armed bandits, automatic speech recognition, %endpointing, end-of-query detection,
turn taking, dialog modeling. 
\end{keywords}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

In modern spoken language AI assistants and dialog systems, endpointing is a key step in the system pipeline, determining when a speaker has finished an utterance \cite{Ferrer2003, Arsikere2014ComputationallyefficientEF, Li2022, Chang2022}.
%While the task of endpointing is similar to turn-taking in human conversations, in many cases the agent lacks the ability to 
Similar to turn-taking in human-human conversations, smooth endpointing that avoids early cutoffs of speaker utterances or excessive latency before an agent response is key to efficient conversational interaction \cite{Schlangen2006FromRT}.
For instance, speech disfluencies in the form of pauses can lead to poor endpointing, and require attention to prosodic properties to avoid mistaking them for utterance-final pauses \cite{Arsikere2014ComputationallyefficientEF}.
%without considering such cases \cite{Stolcke1996}.
Regardless of the modeling used, endpointing hyperparameters need to be carefully calibrated, e.g., to find a good balance between early cutoffs and latency \cite{Zhao2021, Lu2022, Huang2022E2ESJ}.

% Linear Thompson
% \cite{Moerchen2020}

To simplify the problem for learning purposes, in this paper we investigate learning the choice 
between just two endpointing configurations, ``standard'' and ``relaxed'', using features that are extracted for each speaker or utterance (i.e., a sequence of utterances).
Whereas the ``standard'' configuration leads to endpointing behavior suitable for most speakers, the ``relaxed'' configuration is suited for utterances with slow speaking rate and more mid-utterance pausing.
Thus, the task of endpointing adaptation is formulated as finding the better configuration for each utterance.
Although considerable work has been done on different endpointing models and algorithms, there have been few studies on how endpointing hyperparameters can be optimized at a personal or contextual level \cite{Maas2018, Jayasimha2021PersonalizingSS, Ding2022PersonalV2}.
Furthermore, adaptive decision making in acoustic modeling has been studied mostly in the context of ASR \cite{Munkhdalai2022, Sathyendra2022}, rather than endpointing.

Specifically, our goal is to address the following questions:
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/bandit_diagram.png}
  \caption{An overview of our adaptive endpointing with deep contextual multi-armed bandit (CMAB).}
  \label{fig:bandit_diagram}
\end{figure}


\noindent\textbf{Which features are most useful?} 
As inputs to the adaptive endpointing model, different types of features can be considered.
Features that can be extracted directly from the target audio are good candidates.
Although it is not realistic to assume the full target audio would be available in time for adaptive endpointing, it is reasonable to assume that features derived from the initial parts of an utterance can be fed to an online model.
We compare these features and how they impact the performance of our models.

\noindent\textbf{How much information do we need?}
Since an endpointing agent needs to choose a configuration before the entire input has been consumed, the task is to predict in advance whether an early cutoff is likely, rather than detecting an event that has already happened.
Clearly the more of the input the agent sees, the more accurate the predictions will be.
In simulations we investigate the effects of varying amounts of prior data for making a decision about the endpointing configuration.
% However, access to the full target audio is unrealistic, so one parameter of decision making is the fraction of the utterance-derived features and simulate an online situation where only the first parts of the utterance are available.

\noindent\textbf{Can an online contextual bandit model be used in place of an offline-trained model?}
Finally, we note that a supervised learning framework is not suitable for online learning, and investigate whether an online model may be used instead.
Specifically, we adopt the contextual multi-armed bandit (CMAB) framework so that models can learn from reward signals based on latency and cutoff results, instead of ground-truth annotations.

In summary, we find that 
(1) target audio and partial ASR hypotheses based on the starts of utterances are most important;
(2) the more target audio data, the better the performance up to a point---with only about an initial 20\% of the data, an agent can reduce early cutoff without degrading latency---; 
and 
(3) online models such as deep CMAB are applicable to the endpointing task, reducing cutoffs while maintaining latency performance.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Task \& Methodology}

\subsection{Task: Adaptive endpointing}
\label{subsec:p_ep}
Our task is to predict the optimal endpointing configuration for the speaker.
While there could be an unlimited number of configurations for each hyperparameter set, we limit our attention to this binary setting, where the focus is to reliably predict when the target utterance is better endpointed with the ``relaxed'' configuration, as opposed to the default or ``standard'' configuration.


%Correctly identifying when a utterance is better endpointed with the ``relaxed`` configuration will lead to reduced early cutoff of utterances, while incurring similar or slightly increased latency. Thus, we also set an operational objective of \textit{reducing cutoff rate by 5\% relative over the default baseline, while maintaining latency}.


\label{sec:task_methodology}

\subsection{Dataset}
\label{subsec:dataset}
For our study, we use de-identified data sampled from a voice-enabled assistant.
Using this collection of utterances, we then annotate each utterance with ground truth information using the following logic:
\begin{itemize}
    \item If an utterance is cut off early with the ``standard`` configuration, label the utterance as Class 1, meaning that the optimal configuration for the utterance is ``relaxed`` (positive class).
    \item Conversely, if an utterance is not cut off early, then the utterance is labeled ``standard``, or Class 0.
\end{itemize}
We split our collection of about 610 hours into training, development, and test splits with a ratio of 8:1:1, each with about 2.5\% positive labels (early cut-off with ``standard`` configuration). 
Overall, only about 0.02\% of the utterances are cut off early in both of the configurations.
The audio data is in English. %, and in the following format: data types, channels.
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

\begin{comment}

\begin{table}[tb]
\caption{Dataset statistics}
\label{tab:dataset}
\centering
\resizebox{0.8\linewidth}{!}{%
\begin{tabular}{ccc}
\hline
Split & \# Hours & \% of Early EPs \\ \hline\hline
Train & 500 & 2.46 \\
Dev & 60 & 2.52 \\
Test & 50 & 2.54 \\\hline
\end{tabular}%
}
\end{table}
\end{comment}


\subsection{System Architecture \& Features}
\label{subsec:architecture_features}
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/rnnt.png}
  \caption{Shared model architecture for the supervised classifier and deep contextual bandit models. For the bandit models, the classification layer is replaced by a reward predictor for each action.}
  \label{fig:model}
\end{figure}

%Our model architecture, which is the same for both supervised classifier and bandit models, shown in Figure~\ref{fig:model}, is similar to the acoustic model proposed in \cite{Maas2018}.
Our model architecture is an LSTM-based model proposed by Maas et al, which uses acoustic features and is pretrained to predict both end-of-utterance and voice activity \cite{Maas2018}.
We make necessary modifications by adding a last-frame pooling step, and add an MLM-based text encoder to embed partial text hypotheses.
The audio and hypothesis features are concatenated with additional (``Other'') features and fed to the classification layer for prediction.


\begin{table}[t]
  \caption{Input Features}
  %\vspace{-5mm}
    \label{tab:features}
  \centering
  \begin{tabular}{ll}
    \hline
    \textbf{Name}      & \textbf{Description}                \\
    \hline\hline
    Audio                    &    extracted audio features                                    \\
    Hypothesis                   &     best-1 hypothesis from decoding                            \\
    Pause Duration                   &   time between wakeword and intent                               \\
    Wakeword Duration                   &  duration of wakeword                                \\
    Pitch Features                   &      paralinguistic features for intonation                            \\
    Intent Domain                   &      domain of the intentful utterance                            \\
    \hline
  \end{tabular}
\end{table}

The full set of features we experimented with is listed in Table~\ref{tab:features}.
Audio and hypothesis features represent the acoustic and semantic content of the utterance, respectively.
Language model features model the syntactic and semantic completeness of utterances and have been shown to boost the performance of endpointing models \cite{Ferrer2003}.
While transcription hypotheses are distinct from language model posteriors typically used in such studies, we take partial or complete transcriptions of the target audio as a proxy for language model predictions.
In addition, we also include some hand-crafted features that are relevant to endpointing based on prior work.
Wakeword and pause duration could be indicative of initial speaking rate or hesitation by the speaker, while past research shows that prosodic and paralinguistic features, such as pitch, are important for endpointing \cite{Ferrer2003,Maas2017, Ishimoto2017EndofUtterancePB, Liu2017TurnTakingEM, Thomas2012AcousticAD, Maier2017TowardsDE}.
%Finally, we also found preliminary evidence that different intent domain is correlated with varying amounts of early cutoff.
Also, we assume oracle access to the intent domain, which refers to the category of the user's command. %(for example ).
We included this feature based on our intuition that certain intents or commands are more likely to induce slower or more disfluent speech production, such as web search or question answering.

\subsection{Deep contextual multi-armed bandit (Deep CMAB) algorithm}
\label{subsec:dcmab}
A key disadvantage of the supervised approach is that it cannot be trained in an online manner, since training requires knowing the decoding result with the ``standard`` configuration, regardless of which decision it made during the prediction step, making it necessary to prepare the training set in advance.
On the other hand, online learning frameworks such as CMAB only require that the agent receive a reward signal, for the chosen action \cite{Agrawal2013ThompsonSF}.


Online bandit models such as Linear Thompson sampling have been successfully applied in online prediction settings such as recommendation systems \cite{Moerchen2020}, and recent studies show that moving from offline models trained on custom datasets to online models trained using implicit signal can provide significant improvement in performance and cost \cite{Yu2021}.


Thus, we adopt the CMAB approach for our online agents.
Specifically, we adopt the recent deep implementation of the CMAB, instead of popular linear algorithms such as Linear Thompson sampling \cite{Russo2018}.
We find that deep bandits are better suited to our task both for their representational power and for handling audio and their batched training capability.
While there are different algorithms for deep bandits, such as proposed in \cite{Zhang2021neural, Riquelme2018}, we adopt the relatively simple framework proposed in \cite{Collier2018}.
One difference between \cite{Collier2018} and our approach is that instead of updating the model periodically, we update our model after seeing each batch of examples.
Specifically, we adopt concrete dropout in place of conventional dropout weights for neural network model training.
Gal et al.\ show that concrete dropout allows the model to calibrate the amount of exploration naturally, as training progresses \cite{Gal2017}. 


\begin{comment}

We also experiment with different action exploration strategy by trying different \texttt{choose\_action} function:
\begin{itemize}
    \item Epsilon Greedy (EGreedy) uses a fixed parameter of $p=0.1$ for taking random action. Greedy action is taken for the rest of cases.
    \item Stochastic Gradient Descent (SGD) always takes the greedy action. The idea is that the noise from SGD step will provide a natural measure for exploration.
    \item Concrete Dropout (Dropout) is similar to SGD, but instead uses a special dropout implementation, called Concrete Dropout \cite{Gal2017}.
    The authors show that concrete dropout allows the model to calibrate the amount of exploration naturally, as training progresses. 
\end{itemize}
\end{comment}


\SetKwComment{Comment}{/* }{ */}
\begin{algorithm}[t]
%\caption{Deep Contextual Bandit Pseudocode}\label{alg:deep_bandit}
\KwData{Set of utterances $S$, Neural Network $f$}
\For{$\texttt{utterances} \in S$}{
    \For{$a_i \in \texttt{Actions}$}{
        $\texttt{rewards}_i \gets f(s, a_i))$
    }
    $\texttt{chosen} \gets \textbf{\texttt{choose\_action}}\textbf{(rewards)}$\\
    $\texttt{real\_reward} \gets \texttt{decode}\texttt{(s, chosen)}$\\
    $f \gets \textbf{update}\texttt{($f$, real\_reward, rewards)}$\\
    \If{terminate}{break}
}
\caption{Deep Contextual Bandit Pseudocode}\label{alg:deep_bandit}
\end{algorithm}



\begin{table*}[t]
  \caption{Endpointing metrics obtained with different features in an idealized setting where the features are computed from the whole target audio. Relative results (indicated by $\pm$) use the Standard Only as a baseline.
  TM95 refers to a trimmed mean (lower 95th percentile). DTM95:99 refers to a doubly-trimmed mean (95th to 99th percentile).} 
  \label{tab:compare_features_idealized}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccc||cccccc}
\hline
% Metrics / Model & Standard Only & Relaxed Only & Oracle Result &
% \begin{tabular}[c]{@{}c@{}}Target \\ Audio\end{tabular} & \begin{tabular}[c]{@{}c@{}}Target\\ Hypothesis\end{tabular} & Intent Domain & \begin{tabular}[c]{@{}c@{}}Wakeword\\ Duration\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pause\\ duration\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pitch\\ Features\end{tabular} \\
% \hline\hline
% Accuracy (\%) & 97.46 & 2.75 & 100 & 87.48 & 97.68 & 21.11 & 95.23 & 67.43 & 94.4 \\
% Precision & - & 2.75 & 100 & 17.19 & 53.5 & 2.29 & 0.4 & 2.46 & 3.66 \\
% Recall & - & 100 & 100 & 91.57 & 84.83 & 68.41 & 0.29 & 28.67 & 4.07 \\
% F1 score & - & 5.36 & 100 & 28.95 & 65.61 & 4.44 & 0.34 & 4.53 & 3.86 \\
% Early EP rate & 2.54 & 0.07 & 0.05 & 0.29 & 0.43 & 0.89 & 2.77 & 1.94 & 2.65 \\
% TM95 & 363 & 1537 & 369 & 471 & 374 & 1298 & 364 & 710 & 374 \\
% DTM95:99 & 830 & 1734 & 1216 & 1624 & 1390 & 1718 & 1059 & 1662 & 1364
% Metrics / Model & Standard Only & Relaxed Only & Oracle Result & \multicolumn{1}{l}{Target Audio} & \begin{tabular}[c]{@{}c@{}}Target\\ Hypothesis\end{tabular} & Intent Domain & \begin{tabular}[c]{@{}c@{}}Wakeword\\ Duration\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pitch\\ Features\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pause\\ duration\end{tabular} \\ \hline\hline
% Accuracy  & 97.46 & 2.75 & 100 & 87.48 & 97.68 & 21.11 & 95.23 & 94.40 & 67.43 \\
% Precision & NA & 2.75 & 100 & 17.19 & 53.5 & 2.29 & 0.4 & 3.66 & 2.46 \\
% Recall & NA & 100 & 100 & 91.57 & 84.83 & 68.41 & 0.29 & 4.07 & 28.67 \\
% F1 score & NA & 5.36 & 100 & 28.95 & 65.61 & 4.44 & 0.34 & 3.86 & 4.53 \\
% Early EP rate & - & -97.24\% & -98.03\% & -88.58\% & -83.07\% & -64.96\% & +9.06\% & +4.33\% & -23.62\% \\
% TM95 (Latency) & - & +323.42\% & +1.65\% & +29.75\% & +3.03\% & +257.58\% & +0.28\% & +3.03\% & +95.59\% \\
% DTM95:99 (Latency) & - & +108.92\% & +46.51\% & +95.66\% & +67.47\% & +106.99\% & +27.59\% & +64.34\% & +100.24\%
% \\ \hline
Metrics / Model & Standard Only & Relaxed Only & Oracle Result & \begin{tabular}[c]{@{}c@{}}Target\\ Audio \end{tabular}& \begin{tabular}[c]{@{}c@{}}Target\\ Hypothesis\end{tabular} & \begin{tabular}[c]{@{}c@{}}Intent\\ Domain \end{tabular} & \begin{tabular}[c]{@{}c@{}}Wakeword\\ Duration\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pitch\\ Features\end{tabular} & \begin{tabular}[c]{@{}c@{}}Pause\\ duration\end{tabular} \\ \hline\hline
Accuracy (\%) & 97.46 & 2.75 & 100 & 87.48 & 97.68 & 21.11 & 95.23 & 94.40 & 67.43 \\
Precision (\%) & NA & 2.75 & 100 & 17.19 & 53.50 & 2.29 & 0.40 & 3.66 & 2.46 \\
Recall (\%) & NA & 100 & 100 & 91.57 & 84.83 & 68.41 & 0.29 & 4.07 & 28.67 \\
F1 score & NA & 5.36 & 100 & 28.95 & 65.61 & 4.44 & 0.34 & 3.86 & 4.53 \\
Early EP rate & - & 0.07 & 0.05 & 0.29  & 0.43 & 0.89  & 2.77  & 2.65  & 1.94  \\
Early EP rate change & - & -97.24\% & -98.03\% & -88.58\% & -83.07\% & -64.96\% & +9.06\% & +4.33\% & -23.62\% \\
% (combined)Early EP rate & - & 0.07 (-97.24\%) & 0.05 (-98.03\%) & 0.29 (-88.58\%) & 0.43 (-83.07\%) & 0.89 (-64.96\%) & 2.77 (+9.06\%) & 2.65 (+4.33\%) & 1.94 (-23.62\%) \\
% TM95 & - & 1537 (+323.42\%) & 369 (+1.65\%) & 471 (+29.75\%) & 374 (+3.03\%) & 1298 (+257.58\%) & 364 (+0.28\%) & 374 (+3.03\%) & 710 (+95.59\%) \\
% DTM95:99 & - & 1734 (+108.92\%) & 1216 (+46.51\%) & 1624 (+95.66\%) & 1390 (+67.47\%) & 1718 (+106.99\%) & 1059 (+27.59\%) & 1364 (+64.34\%) & 1662 (+100.24\%)
Latency (TM95) & - & +323.42\% & +1.65\% & +29.75\% & +3.03\% & +257.58\% & +0.28\% & +3.03\% & +95.59\% \\
Latency (DTM95:99) & - & +108.92\% & +46.51\% & +95.66\% & +67.47\% & +106.99\% & +27.59\% & +64.34\% & +100.24\%
\\ \hline
\end{tabular}%
}

\end{table*}

The pseudocode for our algorithm is given below as Algorithm~\ref{alg:deep_bandit}. The neural network model uses the architecture shown in Figure~\ref{fig:model}, while for \textbf{\texttt{choose\_action}}, we take the greedy action with argmax, and \textbf{\texttt{update}} the model through stochastic gradient reward.
In the implementation, our model predicts rewards for both actions simultaneously, rather than predicting a reward given an action.
For the bandit model, the reward signal is computed as a linear combination of latency (in ms) and cutoff (indicator variable), which is then used in the reward prediction loss calculation (mean squared error loss).
%We finetune the mixing term as a hyperparamter.
The mixing weights are hyperparameters that we find using experiments on held-out development data.
Intuitively, the bandit model tries to predict the expected reward of each configuration, and chooses a configuration based on the reward, while the supervised classifier directly outputs the predicted optimal configuration. 


% Also, explain we create one model per EP\_mode.

% \subsection{Inference: Ensemble}
% \label{subsec:dcmab}
% This part is about how we found that decomposing the task into ep-model wise decision resulted in better performance.
% However, 

%\RestyleAlgo{ruled}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Experiments}
\label{sec:experiments}


\subsection{Experiment setup}
\label{subsec:ex_setup}
Beyond accuracy measures, we also evaluate the performance of our models using the following metrics:
(1) Early endpointing rate (Early EP rate), which measures the fraction of times the endpointer triggers before the end of the last utterance is reached.
(2) Trimmed mean 95 (TM95) of latency (ms), average of the lower 95th percentile of the data
(3) Double-trimmed mean 95 (DTM95:99) of latency (ms), average of the interval (inclusive) between the 95th and 99th percentiles of the data.

To contextualize and better compare the performance of our models, we also measure the results of several baselines.
\textbf{Standard Only}, the default baseline model, always chooses the ``standard'' configuration, while \textbf{Relaxed Only} always chooses the ``relaxed'' configuration. 
Lastly, we also consider the \textbf{Oracle Model}, which always outputs the optimal configuration choice, giving an upper bound of achievable performance.

%\vspace{-0.25cm}

\subsection{Which features are most useful?}
\label{subsec:result1}

To study which features are important for adaptive endpointing, we conduct utterance-wise endpointing experiments in an idealized setting.
That is, we assumed that the adaptive endpointing model will have the full length of the target audio and the features derived from the full audio as inputs.
In Table~\ref{tab:compare_features_idealized}, we note that target audio and hypothesis features achieve largest cutoff reduction, followed by intent domain, which incurs significant latency degradation.
%Moreover, we observe that only a subset of features (target audio, target hypothesis, intent domain, and pause duration)result in reasonably high recall levels, indicating that certain features, such as wakeword duration or pitch features, are not useful for identifying when 

%Finally, to better compare the features, we adjust the threshold for prediction features so that we can compare the models at a fixed early EP rate target point.
%We note that 
%can leads to models that achieve 80\% (hypothesis) and 20\% (audio) cutoff reduction, with no TM95 degradation, and Oracle-level DTM95:99 latency.

%These solutions are too complicated IMHO, you don't need to install any new packages.

\begin{comment}
\begin{figure*}[!t]
\centering
\begin{tabular}{ll}
\includegraphics[width=0.45\textwidth]{figures/new_5_tradeoff_tm.png}
&
\includegraphics[width=0.45\textwidth]{figures/new_5_tradeoff_dtm.png}
\end{tabular}
\caption{Comparing audio, hypothesis, and intent features on cutoff-latency tradeoff curves.}
\label{fig:compare_features_idealized}
\end{figure*}
\end{comment}


We note that the top-performing features (target audio, target hypothesis, and intent domain) require processing of the target audio utterance. 
On the other hand, features that can be obtained without the intent-carrying portion of an utterance (wakeword duration, pitch features) perform poorly, showing that it is difficult to reliably predict the overall utterance pattern just from the paralinguistic features derived from the initial parts of the utterance. 
%This result motivates our next question, that is, if intent utterance features are necessary for better prediction of optimal EP configuration, how much 

\textbf{Conclusion}: Hypothesis and audio features are most informative. In an idealized setting where the whole target utterance is used for prediction, hypothesis and audio features lead to 80\% and 20\% relative cutoff rate reduction, respectively, with no TM95 degradation and Oracle-level DTM95:99 latency. 

\vspace{-0.25cm}


\begin{figure}[t]
\centering
\begin{tabular}{ll}
\includegraphics[width=0.45\textwidth]{figures/new_2_tradeoff_tm.png}
\\
\includegraphics[width=0.45\textwidth]{figures/new_2_tradeoff_dtm.png}
\end{tabular}
\caption{Comparing hypothesis features with varying portions on cutoff-latency tradeoff curves.}
\label{fig:compare_features_hypothesis}
\end{figure}

\subsection{How much information do we need?}
\label{subsec:result2}

%Table~\ref{tab:varying_anoynt}
In practice, consuming the whole target audio is unrealistic.
Thus, we relax this assumption by assuming our models now see data corresponding to the first $X$\% of the utterance.
To derive the input features for this setup, we first process the full target utterance to derive corresponding audio and hypothesis features. 
Then, we take the first $X$\% of the resulting features and feed them to our model.
%While this processing allows us to simulate feeding only partial audio utterance to the model, it 
We conducted experiments to compare bandit models trained with all types of features, but we chose to include only the two features with the best performance, for brevity and legibility.

Figure~\ref{fig:compare_features_hypothesis} shows the latency vs.\ early EP trade-off curves plotted for supervised classifiers with target hypothesis features with varying amount (20\%, 60\%, 100\%) of tokens.
First, we observe that the cutoff vs.\ DTM95:99 curve has a ``worse`` trade-off curve since more latency degradation is required to achieve the same amount of early cutoff reduction. 
This is because by definition of DTM95:99 is more sensitive to changes in the tail of the latency distribution.
However, we note that even the 20\% model achieves no TM95 latency degradation and only $\sim$20\% DTM95:99 latency degradation, as indicated by the intersection between the blue curves and the dotted red line (target cutoff rate).

Moreover, we confirm that using a larger portion of the target features improves the performance.
In Figure~\ref{fig:compare_features_hypothesis}, the 100\% model achieves significantly better trade-offs for both TM95 and DTM95:99.
This is a confirmation of the intuition that the latter parts of the audio provide more information about which endpointing configuration is optimal for the audio.
%Note that our task is not endpointing itself, but choosing a better 
%We also note that 

\begin{comment}
    

\begin{table}[!h]
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccccc}
\hline
Metrics / Model & \begin{tabular}[c]{@{}c@{}}Audio\\ 20\%\end{tabular} & \begin{tabular}[c]{@{}c@{}}Audio\\ 60\%\end{tabular} & \begin{tabular}[c]{@{}c@{}}Audio\\ 100\%\end{tabular} & \begin{tabular}[c]{@{}c@{}}Hypothesis\\ 20\%\end{tabular} & \begin{tabular}[c]{@{}c@{}}Hypothesis\\ 60\%\end{tabular} & \begin{tabular}[c]{@{}c@{}}Hypothesis\\ 100\%\end{tabular} \\ \hline \hline
Accuracy (\%) & 82.47 & 84.85 & 87.48 & 90.34 & 87.55 & 97.68 \\
Precision & 9.97 & 12.77 & 17.19 & 14.74 & 14.55 & 53.5 \\
Recall & 69.5 & 82.46 & 91.57 & 52.31 & 74.66 & 84.83 \\
F1 score & 17.44 & 22.11 & 28.95 & 23 & 24.36 & 65.61 \\
Early EP rate & 0.85 & 0.5 & 0.29 & 1.35 & 0.7 & 0.43 \\
TM95 & 535 & 503 & 471 & 444 & 488 & 374 \\
DTM95:99 & 1643 & 1636 & 1624 & 1602 & 1619 & 1390 \\ \hline
\end{tabular}%
}
\caption{Comparison of Models, varying amount of audio and hypothesis portions in the input.}
\label{tab:varying_anoynt}
\end{table}
\end{comment}

\begin{comment}

\begin{figure*}[h]
\centering
\begin{tabular}{ll}
\includegraphics[width=0.45\textwidth]{figures/new_1_tradeoff_tm.png}
&
\includegraphics[width=0.45\textwidth]{figures/new_1_tradeoff_dtm.png}
\end{tabular}
\caption{Comparing audio features with varying portions on cutoff-latency tradeoff curves.}
\label{fig:compare_features_audio}
\end{figure*}
\end{comment}

\textbf{Conclusion:} By using only the initial 20\% of the data, we can reduce Early-EP rate by 5\%, with no degradation in TM95 and 20\% relative DTM95:99 degradation over the baseline.
However, we also find that the latter parts of the contain valuable information about optimal endpointing configurations.
\vspace{-0.25cm}

\subsection{Can an online contextual bandit model be used in place of an offline-trained model?}
\label{subsec:result_3}

% \begin{table*}[]
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{ccccccccccccc}
% \hline
% Metrics / Model & Classifier 20\% & Classifier 60\% & Classifier 100\% & Dropout 20\% & Dropout 60\% & Dropout 100\% & SGD 20\% & SGD 60\% & SGD 100\% & EGreedy 20\% & Egreedy 60\% & Egreedy 100\% \\ \hline\hline
% Accuracy (\%) & 78.48 & 93.39 & 98.02 & 97.06 & 97.39 & 97.62 & 97.1 & 97.38 & 97.61 & 96.12 & 97.12 & 97.65 \\
% Precision & 9.11 & 26.3 & 59.18 & 35.7 & 51.85 & 53.53 & 37.2 & 51.13 & 53.85 & 21.61 & 46.96 & 54.4 \\
% Recall & 76.47 & 78.5 & 86.93 & 10.44 & 39.79 & 83.05 & 7.96 & 40.84 & 84.13 & 16.57 & 47.34 & 84.1 \\
% F1 score & 16.28 & 39.4 & 70.42 & 16.15 & 45.03 & 65.1 & 13.11 & 45.41 & 65.67 & 18.76 & 47.15 & 66.07 \\
% Early EP rate & 0.69 & 0.6 & 0.38 & 2.43 & 1.63 & 0.48 & 2.54 & 1.59 & 0.45 & 2.26 & 1.45 & 0.47 \\
% TM95 & 592 & 415 & 374 & 365 & 369 & 374 & 364 & 368 & 375 & 368 & 371 & 374 \\
% DTM95:99 & 1655 & 1586 & 1382 & 924 & 1126 & 1395 & 890 & 1129 & 1399 & 1088 & 1243 & 1398
% \\ \hline
% \end{tabular}%
% }
% \end{table*}


\begin{figure}[!h]
\centering
\begin{tabular}{ll}
\includegraphics[width=0.45\textwidth]{figures/enew_3_tradeoff_tm.png}
\\
\includegraphics[width=0.45\textwidth]{figures/enew_3_tradeoff_dtm.png}
%\vspace{-10mm}
\vspace{-0.25cm}
\end{tabular}

\caption{Comparing supervised and bandit models on first segment input features.}
\label{fig:compare_models_first}
\end{figure}

\begin{comment}
    
\begin{table*}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccccc}
\hline
Metrics / Model & Classifier 20\% & Classifier 60\% & Classifier 100\% & Dropout 20\% & Dropout 60\% & Dropout 100\% \\ \hline \hline
Accuracy (\%)   & 97.46         & 78.48           & 93.39           & 98.02            & 97.06        & 97.39        & 97.62         \\
Precision       & 0             & 9.11            & 26.3            & 59.18            & 35.7         & 51.85        & 53.53         \\
Recall          & 0             & 76.47           & 78.5            & 86.93            & 10.44        & 39.79        & 83.05         \\
F1 score        & 0             & 16.28           & 39.4            & 70.42            & 16.15        & 45.03        & 65.1          \\
Early EP rate   & 2.54          & 0.69            & 0.6             & 0.38             & 2.43         & 1.63         & 0.48          \\
TM95            & 363           & 592             & 415             & 374              & 365          & 369          & 374           \\
DTM95:99        & 830           & 1655            & 1586            & 1382             & 924          & 1126         & 1395     
\\ \hline
\end{tabular}%
}
\caption{Comparing different bandit exploration strategies, with varying input portions.}
\end{table*}
\end{comment}

Since the offline setup required by the supervised model is not ideal, as discussed in Section~\ref{subsec:dcmab}, we investigate if a deep CMAB model can also meet the objective of reducing early cutoff without degrading latency, by comparing the supervised and bandit models.
%\subsection{Results for First Segment Experiment}
Furthermore, we compare these predictive models in a more realistic setting where the total length of the target audio is not known beforehand.
(In previous experiments, we either assumed an idealized setting, with access to the full target utterance, or knowing the utterance length in advance.)
Hence, we extracted the initial segment of an utterance containing the wake word, and use the audio and hypothesis corresponding only to that segment. 
%For example, 
%\begin{itemize}
%    \item “[0] alexa [1] play the music “ $\rightarrow$ “ alexa “
%    \item “[0] echo what time does [1] the sun set tonight” $\rightarrow$ “echo what time does”
%\end{itemize}
%where [0] and [1] denote the beginning of the first and second segments, respectively.
%
On average, the time fraction of that initial segment is $\sim$ 30\% of the full utterance.
We consider this setup a simulation of when a speculative listener is first activated, and retrieves a partial utterance and decoding result (hypothesis) to the endpointing model.

Figure~\ref{fig:compare_models_first} shows the trade-off curves for the supervised and bandit models.
While the bandit model (dts\_both\_first) achieves slightly worse trade-offs for both TM95 and DTM95:99, both models achieve target cutoff reductions with little (DTM95:99) to no (TM95) sacrifice for latency measures.
We note that both supervised and bandit models achieve a significant (2.5 $\rightarrow$ 1.8, $\sim$30\%) Early-EP rate reduction without any TM95 latency degradation, while a small degradation in DM95:99 latency is observed.
%However, for DTM95:99 both models have to sacrifice early ep rate performance, in which case it would be advisable to stop at the second elbow of the curves.


\textbf{Conclusion:} Deep contextual bandits can reduce cutoff rate by 5\% without TM95 latency degradation, and $\sim$20\% DTM95:99 degradation. The more target data is available, the more the gap between supervised classifier and bandit model narrows.


%\vspace{-10mm}
\begin{comment}

\subsection{Comparing Bandit Exploration Strategies}
\label{subsec:comparing_bandits}


\begin{figure*}[h]
\centering
\begin{tabular}{ll}
\includegraphics[width=0.45\textwidth]{figures/new_4_tradeoff_tm.png}
&
\includegraphics[width=0.45\textwidth]{figures/cumreg.png}
\end{tabular}
\caption{Comparing audio, hypothesis, and intent features on cutoff-latency tradeoff curves.}
\label{fig:compare_features_idealized}
\end{figure*}



\subsection{Discussion}
\label{subsec:discuss_results}
% discuss results and do shilling
Partial hypothesis is the most useful feature, even more so than the audio features.
Motivates using LM posterior or other decoder-level information

The more online information the model has, the better the performance.
However, even with portions of the data we can achieve our goal.

The supervised classifier model performs better than the bandit model. 
However, having more target data can bridge the gap.



Mention internal goal: 5\% reduction and no latency degradation..
mention how our models achieve that
\end{comment}
\vspace{-0.25cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusion}
We have proposed adaptive endpointing as a framework for dynamically choosing an optimal endpointing configuration, based on features derived from each input utterance.
By implementing a static supervised classifier for endpointing configuration, we show that utterance-level selection of the locally best endpointing configuration leads to reduction in early cutoff rate, while keeping latency degradation small.
We also show that an online model can be trained without having access to ground truth data. 
For this purpose, a deep contextual multi-armed bandit (CMAB) model combines the efficiency of Bayesian exploration with the representational power of neural networks,
does not require ground truth annotation, and can be adapted to utilize a variety of reward signals that may be available in an online deployment setting.
%combines the efficiency of Bayesian exploration with the representational power of neural networks.
%We implement and test different features, and conclude that deep bandit models can achieve comparable performances against supervised baselines trained on a large amount of ground-truth annotated data.
We find that audio and text features derived from the target utterance are most important for endpointing, and online-trained deep CMAB models can be used in place of impractical offline supervised classifiers, while still reducing early cutoff without latency degradation.
% Next steps
%As future work, we suggest improving and extending the deep bandit framework can be promising.
%For example, practitioners can design new reward signals based on online user responses, or 

%\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
%\newpage
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}



\end{document}
