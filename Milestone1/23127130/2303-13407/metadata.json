{
    "arxiv_id": "2303.13407",
    "paper_title": "Adaptive Endpointing with Deep Contextual Multi-armed Bandits",
    "authors": [
        "Do June Min",
        "Andreas Stolcke",
        "Anirudh Raju",
        "Colin Vaz",
        "Di He",
        "Venkatesh Ravichandran",
        "Viet Anh Trinh"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2024-02-09"
    ],
    "latest_version": 1,
    "categories": [
        "eess.AS",
        "cs.LG"
    ],
    "abstract": "Current endpointing (EP) solutions learn in a supervised framework, which does not allow the model to incorporate feedback and improve in an online setting. Also, it is a common practice to utilize costly grid-search to find the best configuration for an endpointing model. In this paper, we aim to provide a solution for adaptive endpointing by proposing an efficient method for choosing an optimal endpointing configuration given utterance-level audio features in an online setting, while avoiding hyperparameter grid-search. Our method does not require ground truth labels, and only uses online learning from reward signals without requiring annotated labels. Specifically, we propose a deep contextual multi-armed bandit-based approach, which combines the representational power of neural networks with the action exploration behavior of Thompson modeling algorithms. We compare our approach to several baselines, and show that our deep bandit models also succeed in reducing early cutoff errors while maintaining low latency.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13407v1"
    ],
    "publication_venue": null,
    "doi": "10.1109/ICASSP49357.2023.10097142"
}