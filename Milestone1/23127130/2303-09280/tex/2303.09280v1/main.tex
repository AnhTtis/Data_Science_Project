\documentclass[reprint,superscriptaddress]{revtex4-2}

\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb,amsthm,color}
\usepackage{multirow}
\usepackage{booktabs} % enables toprule, etc
\allowdisplaybreaks
%\renewcommand\Affilfont{\itshape\small}
%\usepackage{epstopdf,epsfig}
%\usepackage{newtxtext}
%\usepackage{newtxmath}
%\usepackage[square,numbers]{natbib}

\renewcommand{\figurename}{Fig.}
\renewcommand{\tablename}{Tab.}

\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    urlcolor   = blue,
    citecolor  = black,
}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newcommand{\RomanNumeralCaps}[1]

%\usepackage{float}
%\usepackage{caption}
%\captionsetup{justification=justified,width=\textwidth}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\begin{document}

\preprint{APS/123-QED}

\title{Topology optimization with physics-informed neural networks: \\ application to noninvasive detection of hidden geometries}

\author{Saviz Mowlavi}
 \email{mowlavi@merl.com}
\affiliation{Department of Mechanical Engineering, MIT, Cambridge, MA 02139, USA}
\affiliation{Mitsubishi Electric Research Laboratories, Cambridge, MA 02139, USA}
\author{Ken Kamrin}%
 \email{kkamrin@mit.edu}
\affiliation{Department of Mechanical Engineering, MIT, Cambridge, MA 02139, USA}


\begin{abstract}
Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws.
%A large class of inverse problems in engineering target geometry identification, where the goal is to discover hidden geometric properties such as internal defects through noninvasive measurement techniques. 
Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology.  %to geometry detection has so far
%required prior knowledge on the expected number and types of shapes. 
Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. 
We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusions in linear and nonlinear elastic bodies using measurements of outer surface displacement from a single mechanical loading experiment.
%Applying our framework to the detection of hidden voids or inclusions in a body using measurements of surface displacement under a prescribed mechanical loading, we successfully discover the number, locations, and shapes of the hidden structures in a variety of cases involving both linear and nonlinear elastic materials. 
Our methodology opens a pathway for PINNs to solve various engineering problems targeting geometry optimization.
\end{abstract}

\maketitle

%\section{Introduction}
%\label{sec:Introduction}

Noninvasive detection of hidden geometries is desirable in countless applications including medical imaging and diagnosis \citep{cherepenin2001}, nondestructive evaluation of materials \citep{hellier2013}, and mine detection \cite{robledo2009}. The goal is to infer the locations, and shapes of structures hidden inside a matrix from surface measurements of the response to an applied load such as a magnetic field \cite{dorn2000,griffiths2001}, an electric current \citep{adler2021}, or a mechanical traction \citep{bonnet2005}. 
Identifying these internal boundaries from the measured data constitutes a challenging inverse problem due to the unknown topology, the large number of parameters required to describe arbitrary geometries \citep{haslinger2003}, the potential sparsity of the data, and the complexity of the underlying physical laws which usually take the form of linear or nonlinear partial differential equations (PDEs) \citep{dorn2006}.

In recent years, physics-informed neural networks (PINNs) have emerged as a robust tool for problem inversion across disciplines and over a range of model complexity \citep{dissanayake1994,lagaris1998,raissi2019}. PINNs' ability to seamlessly blend measurement data or design objectives with governing PDEs in nontrivial geometries has enabled practitioners to solve easily a range of inverse problems involving identification or design of unknown properties in fields ranging from mechanics to optics and medicine \citep{raissi2020,sahli2020,lu2021,haghighat2021,chen2022,mowlavi2023}. Encouraged by these early successes, we introduce in this paper a general topology optimization (TO) framework to solve noninvasive geometry detection problems using PINNs, leveraging both the measurements and the governing PDEs. Building on the strength of PINNs, our approach is straightforward to implement regardless of the complexity of the physical model, produces accurate results using measurement data from a single experiment, and does not require a training dataset. To the best of our knowledge, the present work marks the first time that PINNs have been applied to problems involving \textit{a priori} unknown topology and geometry.

Classical approaches to solve geometry identification inverse problems tend to be complex as they combine traditional numerical solvers such as the finite-element or boundary-element method, adjoint techniques to evaluate the sensitivity of the error residual with respect to the shape or the topology, and gradient descent-based optimization algorithms to update the geometry at every iteration \citep{bendsoe1989,eschenauer1994,allaire2004,dorn2006,guo2014}. %This complexity prevents widespread adoption by practitioners and hinders further progress in the field. 
Furthermore, these techniques call for carefully chosen regularization schemes and do not always yield satisfactory results, especially in the presence of sparse measurements acquired by only one or a few sets of experiments. For example, in the mechanical loading case where 
%an elastic body with inclusions is being pulled from the sides,
voids and inclusions in an elastic body are to be identified from measurements of surface displacement in response to a prescribed traction, past studies limit themselves to simple shapes like squares and circles or fail to find the right number of shapes \citep{lee2000,ameur2004,bonnet2005,mei2016,mei2021}.
Attempts to apply PINNs to geometry detection are in their infancy, so far restricted to cases where the number and shape-type of hidden voids or inclusions in an elastic structure are provided in advance \citep{zhang2022}.
%In inverse problems, the goal is to find physical or geometric properties of a system that optimize a desired objective function under the constraint of the governing physical laws \citep{aster2018,martins2021}. Such problems concern either the identification of unobserved properties of an actual system given a set of measurement data, or the design of properties of a hypothetical system to achieve a target functionality, and they arise in various fields of engineering including solid mechanics \citep{bendsoe2003,bonnet2005,doyley2012}, fluid mechanics \citep{borrvall2003,dbouk2017}, optics \citep{jensen2011,molesky2018}, and acoustics \cite{duhring2008,garcia2011}. Inverse problems involving identification or design of \textit{geometric} properties are particularly hard to solve due to the very large number of parameters involved in describing arbitrary geometries \citep{haslinger2003}, in addition to the complexity of the governing physical laws which usually take the form of partial differential equations (PDEs). Methods for these problems are categorized into shape optimization methods, which restrict the search space to geometries with a predefined number of structures, or topology optimization (TO) methods, which do not make such restriction and therefore need to be able to merge or split structures during the solution process \citep{sigmund2013}.

%For geometry design problems, there exist various TO methods in large part motivated by the design of structures that exhibit minimum compliance under load \citep{bendsoe2003}. These methods, however, are very complex as they combine traditional numerical solvers such as the finite-element or boundary-element method, adjoint techniques to evaluate the sensitivity of the objective function with respect to the shape or the topology, and gradient descent-based optimization algorithms to update the geometry at every iteration \citep{bendsoe1989,eschenauer1994,allaire2004,guo2014}. For geometry identification problems, on the other hand, most methods are restricted to shape optimization since they require the number of shapes to be specified in advance \citep{mellings1995,lee1999,waisman2010,karageorghis2012,jung2014}. Other approaches that can find the right number of shapes require a priori knowledge of the geometry of each shape \citep{burczynski2001,sun2013}. Only a few studies perform TO by letting both the number and the shape of structures to be identified as part of the problem \citep{lee2000,ameur2004,mei2016,mei2021}. These studies, however, either limit themselves to simple shapes like squares and circles or fail to find the right number of shapes.

%PINNs' ability to seamlessly blend measurement data or design objectives with governing PDEs in complex geometries has enabled practitioners to solve easily a range of inverse problems involving identification or design of unknown properties in fields ranging from mechanics to optics and medicine \citep{raissi2020,sahli2020,lu2021,haghighat2021,chen2022,mowlavi2023}. However, their application to noninvasive geometry detection problems has to date been more limited due to the difficulty in representing shapes with \textit{a priori} unknown boundaries. 
%A notable exception is a recent study that applied PINNs to the problem of detecting hidden voids and inclusions in elastic bodies using mechanical loading \citep{zhang2022}. The authors predefine both the number and the type (circle, ellipse, etc) of shapes to be identified, leaving the PINNs with the task of inferring the scalar parameter values defining these shapes. The practical applicability of that method is therefore restricted by the need for prior knowledge on the topology of the solution and the types of structures to expect.

Our PINN-based TO framework does not require any prior knowledge on the number and types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field equal to 0 in one phase and 1 in the other. The material density is parameterized through a neural network, which needs to be regularized in order to push the material density towards 0 or 1 values. Thus, one key ingredient in our framework is a novel eikonal regularization, inspired from fast-marching level-set methods \citep{adalsteinsson1995,osher2004} and neural signed distance functions \citep{gropp2020}, that promotes a constant thickness of the interface region where the material density transitions between 0 and 1, leading to well-defined boundaries throughout the domain. 
This eikonal regularization enters as an additional term in the standard PINN loss, which is then used to train the neural networks underlying the material density and physical quantities to yield a solution to the geometry detection problem.
% [SM: alternative to the above sentence] Following standard PINN methodology, the neural networks underlying the material distribution and the physical quantities are then trained to minimize a loss that enforces the governing equations as well as the measurement data to be satisfied.
As an illustration, we apply our framework to cases involving an elastic body under mechanical loading, and discover the topology, locations, and shapes of hidden structures for a variety of geometries and materials.
%Our work creates a pathway for PINNs to be applied to a range of geometry identification and design problems in engineering and science \textcolor{blue}{[include this sentence? already present at the end of intro and conclusion]}.
%To the best of our knowledge, our framework constitutes the first time that PINNs have been applied to problems involving \textit{a priori} unknown topology and geometry.

\section*{Results} 

\subsection*{Problem formulation}

We consider noninvasive geometry detection problems of the following form. Suppose we have a continuous body $\mathcal{B}$ containing an unknown number of hidden voids or inclusions, with unknown shapes and at unknown locations within the body. The material properties are assumed to be known and homogeneous within the body and the inclusions. We then apply a certain type of loading (e.g.~mechanical, thermal, acoustic, etc) on the body's external boundary $\partial \mathcal{B}^\mathrm{ext}$, which produces a response within the body that can be described by a set of $n$ physical quantities (e.g.~displacements, stresses, temperature, etc). These physical quantities can be lumped into a vector field $\boldsymbol{\psi} : \mathcal{B} \rightarrow \mathbb{R}^n$ and satisfy a known set of governing PDEs.
%with known boundary conditions on $\mathcal{B}^\mathrm{ext}$ characterizing the applied loading, as well as boundary conditions on unknown internal boundaries $\partial \mathcal{B}^\mathrm{int}$ reflecting the presence of hidden voids or inclusions.
%\textcolor{blue}{These physical quantities satisfy a known set of governing PDEs expressed in residual form as $\mathbf{r}(\boldsymbol{\psi}(\mathbf{x})) = 0$, $\mathbf{x} \in \mathcal{B}$, 
%with partially unknown boundary conditions on $\partial \mathcal{B} = \partial \mathcal{B}^\mathrm{ext} \cup \partial \mathcal{B}^\mathrm{int}$ characterizing both the loading applied to the body's external boundary $\partial \mathcal{B}^\mathrm{ext}$ as well as the presence of unknown internal boundaries $\partial \mathcal{B}^\mathrm{int}$ due to the hidden voids or inclusions.
 %with boundary conditions $\mathbf{b}^\mathrm{ext}(\boldsymbol{\psi}(\mathbf{x})) = 0$, $\mathbf{x} \in \partial \mathcal{B}^\mathrm{ext}$, characterizing the loading applied to the body's known external boundary, as well as $\mathbf{b}^\mathrm{int}(\boldsymbol{\psi}(\mathbf{x})) = 0, \mathbf{x} \in \partial \mathcal{B}^\mathrm{int}$, reflecting the presence of unknown internal boundaries $\partial \mathcal{B}^\mathrm{int}$ due to the hidden voids or inclusions.} 
 The goal of the inverse problem is to identify the number, locations, and shapes of the voids or inclusions based on measurements along $\partial \mathcal{B}^\mathrm{ext}$ of some of the physical quantities contained in $\boldsymbol{\psi}$.

As a concrete example, we consider two prototypical plane-strain elasticity inverse problems.
%%%%
\begin{figure}[tb]
\centering
\includegraphics[width=0.5\textwidth]{Figures/NCS_Geometry}
\caption{\textbf{Setup of two geometry identification problems in elastic bodies under mechanical loading.} \textbf{a}, A square elastic matrix with hidden voids or inclusions is pulled by a known uniform traction on two opposite sides. The goal is to identify the number, locations, and shapes of the voids or inclusions within using measurements of the displacement occurring along the outer boundary of the matrix. \textbf{b}, An elastic layer on top of a hidden rigid substrate is compressed from the top by a uniform pressure. The goal is to identify the shape of the substrate using measurements of the displacement of the top surface.}
\label{fig:Geometry}
\end{figure}
%%%%
In the first case, a square elastic matrix with hidden voids or inclusions is pulled by a uniform traction $P_o$ on two sides (Fig.~\ref{fig:Geometry}a). The goal of the inverse problem is to identify the number, locations, and shapes of the voids or inclusions using discrete measurements of the displacement of the outer boundary of the matrix. In the second case, an elastic layer on top of a hidden rigid substrate is compressed from the top by a uniform pressure $P_o$, with periodic lateral boundary conditions (Fig.~\ref{fig:Geometry}b). The goal is to identify the shape of the substrate using discrete measurements of the displacement of the top surface. For both cases, the constitutive properties of all materials are assumed to be known. We will consider two different types of constitutive laws: compressible linear elasticity, which characterizes the small deformation of any compressible elastic material, and incompressible nonlinear hyperelasticity, which models the large deformation of rubber-like materials. In the linear elastic case, there exists a unique solution to the inverse problem (see proof in Supplementary Information), making it well-suited to evaluating the accuracy of our TO framework.

Following density-based TO methods \citep{sigmund2013}, we avoid any restriction on the number and shapes of hidden structures by parameterizing the geometry of the elastic body $\mathcal{B}$ through a discrete-valued material density function $\rho : \Omega \rightarrow \{0,1\}$, where $\Omega$ is a global domain comprising both $\mathcal{B}$ and the hidden voids or inclusions. The material density is defined to be equal to 1 in the elastic body $\mathcal{B}$ and 0 in the voids or inclusions. 
The physical quantities $\boldsymbol{\psi}$ can then be extended to the global domain $\Omega$ by introducing an explicit $\rho$-dependence in their governing PDEs, leading to equations of the form 
\begin{subequations}
\begin{align}
\mathbf{r}(\boldsymbol{\psi}(\mathbf{x}),\rho(\mathbf{x})) &= 0, \quad \mathbf{x} \in \Omega, \label{eq:GoverningPDEs} \\
\mathbf{b}(\boldsymbol{\psi}(\mathbf{x})) &= 0, \quad \mathbf{x} \in \partial \Omega, \label{eq:AppliedBCs}
\end{align}
\end{subequations}
with known boundary conditions defined solely on the external boundary $\partial \Omega = \partial \mathcal{B}^\mathrm{ext}$. Note that the residual functions $\mathbf{r}$ and $\mathbf{b}$ may contain partial derivatives of $\boldsymbol{\psi}$ and $\rho$. The inverse problem is now to find the distribution of material density $\rho$ in $\Omega$ so that the corresponding solution for $\boldsymbol{\psi}$ matches surface measurements $\boldsymbol{\psi}_i^m$ at discrete locations $\mathbf{x}_i \in \partial \Omega^m \subset \partial \Omega$, that is,
\begin{equation}
\boldsymbol{\psi}(\mathbf{x}_i) = \boldsymbol{\psi}_i^m, \quad \mathbf{x}_i \in \partial \Omega^m.
\label{eq:Measurements}
\end{equation}
In practice, we might only measure select quantities in $\boldsymbol{\psi}$ at some of the locations, but we do not write so explicitly to avoid overloading the notation.

For the linear elasticity problem that we consider as an example, $\boldsymbol{\psi} = (\mathbf{u},\boldsymbol{\sigma})$ where $\mathbf{u}(\mathbf{x})$ and $\boldsymbol{\sigma}(\mathbf{x})$ are displacement and stress fields, respectively, and the governing equations comprise equilibrium relations $\sum_j\partial \sigma_{ij}/\partial x_j = 0$ and a constitutive law $F(\boldsymbol{\sigma}, \nabla \mathbf{u}, \rho) = 0$, both defined over $\Omega$. The presence of $\rho$ in the constitutive law specifies different material behaviors for the elastic solid phase and the void or rigid inclusion phase. The applied boundary conditions take the form $\mathbf{u} = \bar{\mathbf{u}}$ on $\partial \Omega_u$ and $\boldsymbol{\sigma} \mathbf{n} = \bar{\mathbf{t}}$ on $\partial \Omega_t$, where $\partial \Omega_u$ and $\partial \Omega_t$ are partitions of the external boundary with applied displacements and applied tractions, respectively, and $\mathbf{n}$ is the outward unit normal. In the case of the elastic layer, the outer boundary also comprises a portion $\partial \Omega_p$ with periodic boundary conditions on the displacement and traction. Finally, the requirement that the predictions for $\mathbf{u}$ at the surface match the measurement data is expressed as $\mathbf{u}(\mathbf{x}_i) = \mathbf{u}_i^m$, $\mathbf{x}_i \in \partial \Omega^m$. See Methods for a detailed formulation of the governing equations and boundary conditions for all considered cases.

Similar to density-based TO methods \citep{sigmund2013}, we relax the binary constraint on the material density by allowing intermediate values of $\rho$ between 0 and 1. This renders the problem amenable to gradient-based optimization, which underpins the PINN-based TO framework that we introduce in the next section. However, the challenge is to find an appropriate regularization mechanism that drives the optimized material distribution towards 0 and 1 rather than intermediate values devoid of physical meaning. As we will show in the discussion, common strategies employed in TO \citep{dorn2006,sigmund2013} do not yield satisfactory results in our PINN-based framework for geometry detection problems. Thus, we have developed a novel eikonal regularization scheme inspired from level-set methods and signed distance functions, which we will describe after presenting the general framework.
%A common strategy in TO is to penalize implicitly these intermediate values through a suitably chosen interpolation function between material properties and material density \citep{bendsoe1989,bendsoe1999,borrvall2003}. This method, however, only works in the presence of a solid fraction constraint \cite{sigmund2013}, whose absence from the geometry identification problem calls for another type of regularization. 
%Ref.~\cite{mei2016} proposed to employ a total variation diminishing regularization that penalizes the gradient norm $|\nabla \rho|$ throughout the domain. When combined with PINNs, we have found this approach to produce solutions with sharp material density transitions but containing large regions of intermediate $\rho$ values between 0 and 1. 

\subsection*{General framework}
\label{sec:GeneralFramework}

%%%%
\begin{figure*}
\centering
\includegraphics[width=\textwidth]{Figures/NCS_Framework}
\caption{\textbf{TO framework for noninvasive detection of hidden geometries.} The geometry of the system, which is initially unknown, is parameterized by a material density field given through a level-set function and equal to 1 in the elastic body and 0 in the voids or inclusions. The level-set function and the physical quantities describing the problem are approximated with deep neural networks designed to inherently satisfy the applied boundary conditions. These neural networks are then trained to minimize a loss function that drives the material density and physical quantities towards satisfying the governing equation of the problem while matching discrete surface measurements. A crucial eikonal regularization term in the loss function ensures that the material density transitions between 0 and 1 over a prescribed length scale and avoids settling on intermediate values. By the end of the optimization, the converged material distribution reveals the location and shapes of the hidden structures.}
\label{fig:Framework}
\end{figure*}
%%%%

We propose a TO framework based on PINNs for solving noninvasive geometry detection problems (Fig.~\ref{fig:Framework}). 
%We present the methodology in a general setting in the next two sections, and describe in Appendix \ref{app:AdditionalInformationSolutionMethodology} its particular application to the two problems formulated in the introduction. 
At the core of the framework are several deep neural networks that approximate the physical quantities $\boldsymbol{\psi}(\mathbf{x})$ describing the problem and the material density $\rho(\mathbf{x})$. For the physical quantities, each neural network maps the spatial location $\mathbf{x} = (x_1,x_2)$ to one of the variables in $\boldsymbol{\psi} = (\psi_1, \cdots, \psi_n)$; this can be expressed as $\psi_i = \bar{\psi}_i(\mathbf{x}; \boldsymbol{\theta}_i)$ where $\bar{\psi}_i$ is the map defined by the $i$th neural network and its trainable parameters $\boldsymbol{\theta}_i$ (see Methods).
%This can be expressed as $\mathbf{u} = \mathbf{N}(x,y; \boldsymbol{\theta})$, where $\mathbf{N}$ is the map defined by all $n$ neural networks their trainable parameters $\boldsymbol{\theta}$.
%For the material distribution, we let $\rho = \mathrm{sigmoid}(\phi/\delta)$ where $\phi = N_\phi(\mathbf{x}; \boldsymbol{\theta}_\phi)$ is a level set field defined by a neural network with trainable parameters $\boldsymbol{\theta}_\phi$. The sigmoid function ensures that the material distribution remains between 0 and 1, and $\delta$ is a transition length scale that we will comment on later.  
For the material distribution, we first define a neural network with trainable parameters $\boldsymbol{\theta}_\phi$ that maps $\mathbf{x}$ to a scalar variable $\phi = \bar{\phi}(\mathbf{x}; \boldsymbol{\theta}_\phi)$. A sigmoid function is then applied to $\phi$ to yield $\rho = \mathrm{sigmoid}(\phi/\delta) = \mathrm{sigmoid}(\bar{\phi}(\mathbf{x}; \boldsymbol{\theta}_\phi)/\delta)$, which we simply write as $\rho = \bar{\rho}(\mathbf{x},\boldsymbol{\theta}_\phi)$. This construction ensures that the material density $\rho$ remains between 0 and 1, and $\delta$ is a transition length scale that we will comment on later. We define the phase transition to occur at $\rho = 0.5$ so that the zero level-set of $\phi$ delineates the boundary between the two material phases, hence $\phi$ is hereafter referred to as a level-set function \citep{osher1988,osher2004}.
%For the material distribution, we first define a neural network with output $\phi = N_\phi(\mathbf{x}; \boldsymbol{\theta}_\phi)$ and trainable parameters $\boldsymbol{\theta}_\phi$; a sigmoid function is then applied to $\phi$ to yield $\rho = \mathrm{sigmoid}(\phi/\delta)$. This ensures that the material distribution $\rho$ remains between 0 and 1, and $\delta$ is a transition length scale that we will comment on later.  

We now seek the parameters $\boldsymbol{\theta}_{\boldsymbol{\psi}} = \{\boldsymbol{\theta}_1, \dots, \boldsymbol{\theta}_n\}$ and $\boldsymbol{\theta}_\phi$ so that the neural network approximations for $\boldsymbol{\psi}(\mathbf{x})$ and $\rho(\mathbf{x})$ satisfy the governing equations \eqref{eq:GoverningPDEs} and applied boundary conditions \eqref{eq:AppliedBCs} while matching the surface measurements \eqref{eq:Measurements}. This is achieved by constructing a loss function of the form
\begin{align}
\mathcal{L}(\boldsymbol{\theta}_{\boldsymbol{\psi}}, \boldsymbol{\theta}_\phi) &= \lambda_\mathrm{meas} \mathcal{L}_\mathrm{meas}(\boldsymbol{\theta}_{\boldsymbol{\psi}}) + \lambda_\mathrm{gov} \mathcal{L}_\mathrm{gov}(\boldsymbol{\theta}_{\boldsymbol{\psi}}, \boldsymbol{\theta}_\phi) \nonumber \\ 
&\quad + \lambda_\mathrm{reg} \mathcal{L}_\mathrm{eik}(\boldsymbol{\theta}_\phi), \label{eq:TotalLoss}
\end{align}
where $\mathcal{L}_\mathrm{meas}$ and $\mathcal{L}_\mathrm{gov}$ measure the degree to which the neural network approximations do not satisfy the measurements and governing equations, respectively, $\mathcal{L}_\mathrm{eik}$ is a crucial regularization term that drives $\rho$ towards 0 or 1 values and that we will explain below, and the $\lambda$'s are scalar weights. The measurement loss takes the form
\begin{equation}
\mathcal{L}_\mathrm{meas}(\boldsymbol{\theta}_{\boldsymbol{\psi}}) = \frac{1}{|\partial \Omega^m|} \sum_{\mathbf{x}_i \in \partial \Omega^m} |\bar{\boldsymbol{\psi}}(\mathbf{x}_i;\boldsymbol{\theta}_{\boldsymbol{\psi}})-\boldsymbol{\psi}_i^m|^2,
\end{equation}
where $|\partial \Omega^m|$ denotes the size of the set $\partial \Omega^m$. A trivial modification of this expression is necessary in the case where only select quantities in $\boldsymbol{\psi}$ are measured. The governing equations loss takes the form
\begin{equation}
\mathcal{L}_\mathrm{gov}(\boldsymbol{\theta}_{\boldsymbol{\psi}}, \boldsymbol{\theta}_\phi) = \frac{1}{|\Omega^d|} \sum_{\mathbf{x}_i \in \Omega^d} |\mathbf{r}(\bar{\boldsymbol{\psi}}(\mathbf{x}_i;\boldsymbol{\theta}_{\boldsymbol{\psi}}),\bar{\rho}(\mathbf{x}_i;\boldsymbol{\theta}_\phi))|^2,
\label{eq:LossGov}
\end{equation}
where $\Omega^d$ is a set of collocation points in $\Omega$, and we use automatic differentiation to calculate in a mesh-free fashion the spatial derivatives contained in $\mathbf{r}$. We design the architecture of our neural networks in such a way that they inherently satisfy the boundary conditions (see Methods, section ``Detailed PINNs formulation'').
%\begin{subequations}
%\begin{align}
%\mathcal{L}_F(\boldsymbol{\theta}_{\boldsymbol{\psi}}, \boldsymbol{\theta}_\phi) &= \frac{1}{|I|} \sum_{i \in I} | F(\mathbf{u}(\mathbf{x}_i,\boldsymbol{\theta}_{\boldsymbol{\psi}}), \rho(\mathbf{x}_i,\boldsymbol{\theta}_\phi)) |^2, \\
%\mathcal{L}_G(\boldsymbol{\theta}_{\boldsymbol{\psi}}, \boldsymbol{\theta}_\phi) &= \frac{1}{|I|} \sum_{i \in I} \mathbf{1}_{\{ G > 0 \}} | G(\mathbf{u}(\mathbf{x}_i,\boldsymbol{\theta}_{\boldsymbol{\psi}}), \rho(\mathbf{x}_i,\boldsymbol{\theta}_\phi)) |^2.
%\end{align}
%\end{subequations}
%We note that these expressions are readily generalized to the case of $F$ and $G$ containing multiple constraints over different domains (for instance, PDEs and boundary conditions). 

Finally, the optimal parameters $\boldsymbol{\theta}_{\boldsymbol{\psi}}^*$ and $\boldsymbol{\theta}_\phi^*$ that solve the problem can be obtained by training the neural networks to minimize the loss \eqref{eq:TotalLoss} using stochastic gradient descent-based optimization. The corresponding physical quantities $\bar{\boldsymbol{\psi}}(\mathbf{x};\boldsymbol{\theta}_{\boldsymbol{\psi}}^*)$ will match the discrete surface measurements while satisfying the governing equations of the problem, while the corresponding material density $\bar{\rho}(\mathbf{x};\boldsymbol{\theta}_\phi^*)$ will reveal the number, locations, and shapes of the hidden voids or inclusions.

\subsection*{Material density regularization}
\label{sec:MaterialDensityRegularization}

We now describe the key ingredient that ensures the success of our framework. As mentioned above, the main challenge is to promote the material density $\rho(\mathbf{x})$ to converge towards 0 or 1 away from the material phase boundaries, given by the zero level-set $\phi = 0$. Moreover, we desire the thickness of the transition region along these boundaries, where $\rho$ goes from 0 to 1, to be uniform everywhere in order to ensure consistency of physical laws across the interface (e.g.~stress jumps). 

To visualize what happens in the absence of regularization, consider a random instance of the neural network $\phi = \bar{\phi}(\mathbf{x},\boldsymbol{\theta}_\phi)$ (Fig.~\ref{fig:LevelSet}a, left) and the corresponding material distribution $\rho = \mathrm{sigmoid}(\phi/\delta)$ with $\delta = 0.01$ (Fig.~\ref{fig:LevelSet}a, center).
%%%%
\begin{figure}[tb]
\centering
\includegraphics[width=0.5\textwidth]{Figures/NCS_LevelSet}
\caption{\textbf{Eikonal regularization of the material density.} $\mathbf{a}$, A random level-set function $\phi$ yields a material density $\rho = \mathrm{sigmoid}(\phi/\delta)$ with large regions of values between 0 and 1, due to the nonuniformity of the gradient $|\nabla \phi|$ along the material boundaries defined by the zero level-set of $\phi$ (black lines). $\mathbf{b}$, Constraining $\phi$ to solve the eikonal equation $\nabla \phi = 1$ in a narrow band $\Omega_\mathrm{eik}$ of thickness $w$ (edges depicted by dashed lines) along the material boundaries results in a uniform transition thickness of $\rho$ from 0 to 1, without large regions of intermediate density values. $\mathbf{c}$, The loss $\mathcal{L}_\mathrm{eik}$ implements the eikonal regularization in the PINN-based TO framework by penalizing deviations away from the constraint $|\nabla \phi| = 1$ on a subset of collocation points $\Omega_\mathrm{eik}^d \subset \Omega_d$ that approximates the true narrow band $\Omega_\mathrm{eik}$.}
\label{fig:LevelSet}
\end{figure}
%%%%
The sigmoid transformation ensures that $\rho$ never drops below 0 or exceeds 1, leading to large regions corresponding to one phase or the other. However, the thickness of the transition region where $\rho$ goes from 0 to 1 is not everywhere uniform, resulting in large zones where $\rho$ assumes nonphysical values between 0 and 1 (Fig.~\ref{fig:LevelSet}a, center). This behavior stems from the non-uniformity of the gradient norm $|\nabla \phi|$ along the material boundaries $\phi = 0$, with small and large values of $|\nabla \phi|$ leading to wide and narrow transition regions, respectively (Fig.~\ref{fig:LevelSet}a, right).

We propose to regularize the material density by forcing the gradient norm $|\nabla \phi|$ to be unity in a narrow band $\Omega_\mathrm{eik}$ of width $w$ along the material boundaries defined by the zero level-set $\phi = 0$. In this way, $\phi$ becomes a signed distance function to the material boundary in the narrow band, thereby constraining the gradient of $\rho$ to be constant along the interface. To ensure that the narrow band covers the near-entirety of the transition region where $\rho$ goes from 0 to 1, we choose $w = 10 \delta$ so that $\rho = \mathrm{sigmoid}(\pm w/2\delta) = \mathrm{sigmoid}(\pm 5) \simeq 0$ or $1$ along the edge of the narrow band. To illustrate the effect of such regularization, we consider the previous random instance of the neural network $\phi = \bar{\phi}(\mathbf{x},\boldsymbol{\theta}_\phi)$ and enforce the constraint $|\nabla \phi| = 1$ in the narrow band $\Omega_\mathrm{eik}$ along its zero level-set (Fig.~\ref{fig:LevelSet}b, left and right). The zero level-set is kept fixed to facilitate comparison with the unregularized case (Fig.~\ref{fig:LevelSet}a). With $\phi$ now behaving like a signed-distance function in the narrow band, a uniform transition thickness for $\rho$ along all material boundaries is achieved, without large regions of intermediate density values (Fig.~\ref{fig:LevelSet}b, center).

In practice, we implement this regularization into our PINN-based TO framework by including an `eikonal' loss term $\mathcal{L}_\mathrm{eik}$ in \eqref{eq:TotalLoss}, which takes the form
\begin{equation}
\mathcal{L}_\mathrm{eik}(\boldsymbol{\theta}_\phi) = \frac{1}{|\Omega_\mathrm{eik}^d|} \sum_{\mathbf{x}_i \in \Omega_\mathrm{eik}^d} \left(| \nabla \phi (\mathbf{x}_i)| - 1 \right)^2,
\label{eq:EikonalLoss}
\end{equation}
where $\Omega_\mathrm{eik}^d = \{\mathbf{x}_i \in \Omega^d : |\phi(\mathbf{x}_i)| < w/2\}$. The aim of this term is to penalize deviations away from the constraint $|\nabla \phi| = 1$ in the narrow band $\Omega_\mathrm{eik}$ of width $w$ along the interface defined by the zero level-set $\phi = 0$. Because finding the subset of collocation points $\mathbf{x}_i$ in $\Omega^d$ belonging to the true narrow band of width $w$ at every step of the training process would be too expensive, we instead relax the domain over which the constraint $|\nabla \phi| = 1$ is active by utilizing the subset $\Omega_\mathrm{eik}^d$ of collocation points that satisfy $|\phi(\mathbf{x}_i)| < w/2$. As the constraint $|\nabla \phi| = 1$ is progressively better satisfied during the training process, $\Omega_\mathrm{eik}^d$ will eventually overlap the true narrow band of width $w$ along the zero level-set of $\phi$ (Fig.~\ref{fig:LevelSet}c). 

Since the constraint $|\nabla \phi| = 1$ in the narrow band takes the form of an eikonal equation, we call this approach eikonal regularization. We emphasize that in contrast to recent works training neural networks to solve the eikonal equation \citep{gropp2020}, our eikonal regularization does not force $\phi$ to vanish on a predefined boundary. Rather, the zero level-set of $\phi$ evolves freely during the training process in such a way that the corresponding material distribution $\rho = \mathrm{sigmoid}(\phi/\delta)$ and physical quantities $\boldsymbol{\psi}$ minimize the total loss \eqref{eq:TotalLoss}, eventually revealing the material boundaries delineating the hidden voids or inclusions. %Finally, we note that fast marching algorithms in traditional level-set methods also solve the eikonal equation in a narrow band around the zero level-set \citep{adalsteinsson1995}, but they require an explicit velocity to update the position of the zero level-set, which is usually calculated using complex adjoint-based methods.

\subsection*{Setup of numerical experiments}
\label{sec:SetupNumericalExperiments}

We evaluate our TO framework on a range of challenging test cases involving different numbers and shapes of hidden structures and various materials (Methods, Tabs.~\ref{tab:MatrixCases} and \ref{tab:LayerCases}). As a substitute for real experiments, we use the finite-element method (FEM) software Abaqus to compute the deformed shape of the boundary of the elastic structure and generate the measurement data for each case (Methods, section ``FEM simulations''). Using this measurement data, we run our TO framework to discover the number, locations, and shapes of the hidden voids or rigid inclusions (for implementation and training details, see Methods, section ``Architecture and training details''). We then compare the obtained results with the ground truth --- the voids or inclusions originally fed into Abaqus --- to assess the efficacy of our framework.

\subsection*{Elastic matrix experiments}
\label{sec:ElasticMatrix}

%%%%
\begin{figure*}
\centering
\includegraphics[width=\textwidth]{Figures/NCS_Ela.pdf}
\caption{\textbf{Identification of voids and inclusions in elastic matrices.} A linear elastic matrix containing voids (\textbf{a-d}): \textbf{a}, The various loss components that enforce the solution to match the surface measurement data, satisfy the governing equations, and obey the eikonal regularization, are being minimized during the training process. \textbf{b},\textbf{c}, The final level-set function $\phi$ and its gradient magnitude $|\nabla \phi|$ show the effect of the eikonal regularization, making $\phi$ a signed distance function in narrow band along the interface. \textbf{d}, The final material density $\rho$ reveals the number, locations, and shapes of the hidden voids, which are compared with the ground truth shown in dotted white lines. \textbf{e}, The final material density predictions in the case of a linear elastic matrix containing soft, stiff or rigid inclusions. \textbf{f}, The final material density predictions in the case of a nonlinear hyperelastic matrix containing voids subject to large stretches.}
\label{fig:NCS_Ela}
\end{figure*}
%%%%

We first apply our framework to cases involving a linear elastic matrix  (Fig.~\ref{fig:Geometry}a) containing voids (cases 1, 3, 8, 10, 15, 17 in Methods, Tab.~\ref{tab:MatrixCases}). As the various loss components are minimized during training (Fig.~\ref{fig:NCS_Ela}a), the material density $\rho$ evolves and splits in a way that progressively reveals the number, locations, and shapes of the hidden voids (Extended Data Fig.~\ref{fig:NCS_ElaVoid_Iters}), without advance knowledge of their topology. By the end of the training, the transition regions where the material density goes from 0 to 1 have uniform thickness along all internal boundaries (Fig.~\ref{fig:NCS_Ela}d), thanks to the eikonal regularization that encourages the level-set gradient $\nabla \phi$ to have unit norm in a band along the material boundaries $\phi = 0$ (Fig.~\ref{fig:NCS_Ela}b,c). The agreement between the final inferred shapes and the ground truth is remarkable, with our framework able to recover intricate details such as the three lobes and the concave surfaces of the star-shaped void (Fig.~\ref{fig:NCS_Ela}d, second from left), or the exact aspect ratio and location of a thin slit (Fig.~\ref{fig:NCS_Ela}d, third from left). The stress and strain fields of the deformed matrix are also obtained as a byproduct of the solution process (Extended Data Fig.~\ref{fig:NCS_ElaVoid_Stress}).
%designed after that considered in Ref.~\cite{zhang2022} In Ref.~\cite{zhang2022}, the location of the slit could only be identified using 10 measurement points located inside the matrix, even though its shape was known in advance. Conversely, our framework is able to detect both the location and shape of the slit with remarkable accuracy, despite only relying on surface measurements. 
The only case that is not completely identified is the U-shaped void (Fig.~\ref{fig:NCS_Ela}d, first from right), a result of the miniscule influence of the inner lobe on the outer surface displacements due to its low level of strain and stress (Extended Data Fig.~\ref{fig:NCS_AbaqusStress}). Finally, our framework maintains accurate results when reducing the number of surface measurement points or restricting measurements to a few surfaces (Extended Data Figs.~\ref{fig:NCS_SparseMeasCircle}-\ref{fig:NCS_SparseMeasSlit}).

Next, we consider cases involving linear elastic and rigid inclusions in the linear elastic matrix (cases 4, 5, 6, 11, 12, 13 in Methods, Tab.~\ref{tab:MatrixCases}). Our framework successfully identifies the inclusions in almost all cases (Fig.~\ref{fig:NCS_Ela}e). Inferred displacements and stresses of the deformed matrix (Extended Data Fig.~\ref{fig:NCS_ElaInclusion_Stress}) confirm the intuition that voids or soft inclusions soften the matrix while stiff or rigid inclusions harden the matrix. The U-shaped soft and stiff elastic inclusions (Fig.~\ref{fig:NCS_Ela}e, second and third from right) are better detected by the framework than their void or rigid counterparts (Fig.~\ref{fig:NCS_Ela}d, first from right and Fig.~\ref{fig:NCS_Ela}e, first from right), since an elastic inclusion induces some strain and stress on the inner lobe (Extended Data Fig.~\ref{fig:NCS_AbaqusStress}).

Finally, we consider cases involving a soft, incompressible Neo-Hookean hyperelastic matrix with the same void shapes considered previously (cases 2, 7, 9, 14, 16, 18 in Methods, Tab.~\ref{tab:MatrixCases}). 
The geometries are identified equally well (Fig.~\ref{fig:NCS_Ela}f) in this large deformation regime (Extended Data Fig.~\ref{fig:NCS_HyperElaVoid_Stress}) as with linear elastic materials, which illustrates the ability of the framework to cope with nonlinear governing equations without any added complexity in the formulation or the implementation.

\subsection*{Elastic layer experiments}
\label{sec:ElasticLayer}

We finally apply our framework to the periodic elastic layer (Fig.~\ref{fig:Geometry}b), where a linear elastic material covers a hidden rigid substrate (cases 20, 21, 22 in Methods, Tab.~\ref{tab:LayerCases}).
%%%%
\begin{figure}[htbp!]
\centering
\includegraphics[width=0.49\textwidth]{Figures/NCS_PeriodicEla.pdf}
\caption{\textbf{Identification of substrate shape underneath a periodic linear elastic layer.} \textbf{a}, The various loss components that enforce the solution to match the surface measurement data, satisfy the governing equations, and obey the eikonal regularization, are being minimized during the training process. \textbf{b},\textbf{c}, The final level-set function $\phi$ and its gradient magnitude $|\nabla \phi|$ show the effect of the eikonal regularization, which makes $\phi$ a signed distance function in narrow band along the material boundary. \textbf{d}, The final material density $\rho$ reveals the shape of the buried rigid substrate.}
\label{fig:NCS_PeriodicEla}
\end{figure}
%%%%
Contrary to the matrix problem, this setup only provides access to measurements on the top surface, and the hidden geometry to be discovered is not completely surrounded by the elastic material. Our TO framework is nevertheless able to detect the correct depths and shapes of the hidden substrates (Fig.~\ref{fig:NCS_PeriodicEla}). This example demonstrates the versatility of the framework in adapting to various problem setups.

%%%%
\begin{figure*}
\centering
\includegraphics[width=\textwidth]{Figures/NCS_Reg.pdf}
\caption{\textbf{Comparison between eikonal regularization and alternative regularizations.} \textbf{a}, The eikonal regularization achieves a high IoU (intersection over union, a geometry detection accuracy metric equal to 1 in the perfect case) above 0.95 for any value of the regularization weight $\lambda_\mathrm{reg}$ within a range spanning three orders of magnitude. The results are consistent over 4 random initializations of the neural networks parameters, with the circles reporting the average value and the shade reporting the highest and lowest values. By contrast, the total variation diminishing (TVD), explicit penalization, and Solid Isotropic Material with Penalization (SIMP) regularizations never exceed IoU values above 0.93, with larger variability among realizations.  \textbf{b}, The final material density $\rho$ obtained with each regularization mechanism for various values of the regularization weight $\lambda_\mathrm{reg}$ or exponent $p$ (shown in \textbf{a} by the shaded areas) demonstrates the efficacy of the eikonal regularization.}
\label{fig:NCS_Reg}
\end{figure*}
%%%%

\section*{Discussion}

As with any TO method relying on a material density field to parameterize the geometry, the success of our PINN-based framework hinges on the presence of an appropriate regularization mechanism to penalize intermediate density values. Although we have shown that our novel eikonal regularization leads to consistently accurate results, other regularization approaches have been employed in classical adjoint-based TO methods \citep{dorn2006,sigmund2013}. These include the total variation dimishing (TVD) regularization \citep{chan2004,mei2016} that penalizes the $L_1$ norm of the density gradient $\nabla \rho$, the explicit penalization regularization \citep{allaire1993} that penalizes the integral over the domain of $\rho (1-\rho)$, and the Solid Isotropic Material with Penalization (SIMP) approach \citep{bendsoe1989} that relates material properties such as the shear modulus and the material density through a power-law with exponent $p$. The latter is the most popular regularization mechanism in structural optimization \citep{bendsoe2003}. However, when implemented in our PINN-based framework for the detection of hidden geometries, these methods yield inferior results to the eikonal regularization (Fig.~\ref{fig:NCS_Reg}). Indeed, we compare all four approaches on a challenging test case involving a linear elastic rectangular matrix pulled from the top and bottom and containing soft inclusions in the shape of the letters M, I, and T (case 19, Tab.~\ref{tab:MatrixCases}). {The measurements consist of the displacement along the outer boundary, similar to the previous square matrix examples.} We consider different values of the regularization weight $\lambda_\mathrm{reg}$ (for the eikonal, TVD and explicit penalization regularizations) and the exponent $p$ (for the SIMP regularization), and solve the inverse problem using four random initializations of the neural networks in each case. Not only was the eikonal regularization the only one to find the right shapes, it did so over three orders of magnitude of $\lambda_\mathrm{reg}$, demonstrating a desirable robustness with respect to $\lambda_\mathrm{reg}$ (Fig.~\ref{fig:NCS_Reg}).

%We have presented a TO framework based on PINNs, with an eikonal regularization inspired by level-set methods and signed distance functions. Thanks to the parametrization of the geometry through a material density field that can adapt to any topology, our framework is able to discover both the number and the shape of hidden structures, without any prior knowledge required regarding the number or the type of shape to expect. In other words, it is a true TO framework since it can discover the right topology of the solution beyond merely discovering the right shape(s) given a known topology. We note that this makes our approach stand out amongst other approaches in the literature that treat the same geometry identification problem, for which one needs to specify in advance either the number of structures to identify, the type of geometries to identify (e.g.~circle, slit, etc), or both. Notable exceptions are the studies in Ref.~\cite{lee2000} which parametrizes the geometry using a curve defined through a set of control nodes that can represent multiple structures at the same time, and Ref.~\cite{mei2016} which parametrizes the geometry through a material distribution field. Ref.~\cite{lee2000} demonstrates the detection of three circle-shaped inclusions but does not present results involving more complicated shapes, while Ref.~\cite{mei2016} only considers examples with a single circle-shaped inclusion. By contrast, our framework is, to our knowledge, the first one to successfully detect multiple structures with each very different shapes. Moreover, thanks to the representation of the geometry through a material distribution field and the flexibility of PINNs, extending our framework to the three-dimensional case should be straightforward.

%Since we parameterize the geometry implicitly through a material density field, a major challenge is to drive the latter towards 0 or 1 and avoid intermediate values that do not have physical meaning. In this work, we have proposed to solve this problem by requiring that the level-set function underlying the material distribution satisfies the eikonal equation in a narrow band of finite width along its zero contour level. Since the zero contour level defines the location of the interface between the two phases, this ensures that the level-set function becomes a signed distance function to the interface. As a result, the thickness of the interface zone where the material density transitions between 0 and 1 is everywhere uniform, and there cannot be large zones of intermediate material density values. This eikonal constraint on the level-set function is naturally implemented in the PINN framework through the inclusion of an additional loss function, which we call eikonal regularization. 
%The eikonal regularization bears resemblance to recent computer graphics works showing that neural networks can be trained to learn signed distance functions by solving the eikonal equation \citep{gropp2020}. However, in their case the neural network is also forced to vanish at a set of points sampled on a predefined surface of interest, while in our case the neural network approximation of the level-set function is allowed to evolve freely in such a way that the interface defined by its zero level set solves the geometry identification problem. Another difference is that we only impose the eikonal constraint in a narrow band around the zero level set; outside the band the material density is nearly equal to 0 or 1 so the specific values of the level-set function do not matter. Interestingly, restricting the level-set function to solve the eikonal equation only in a narrow band around its zero level set is similar in spirit to narrow-band algorithms in traditional level-set methods \citep{adalsteinsson1995}. These algorithms nevertheless require an explicit velocity vector with which to update in a separate step the position of the zero level set, which is usually calculated using complex adjoint-based methods.

Thanks to the flexibility of the PINN framework, adapting our approach to various scenarios involving partial information, three-dimensional geometries, or other types of noninvasive imaging experiments (e.g.~using thermal \citep{banks1990}, acoustic \citep{colton2000}, electric \citep{cheney1999}, or magnetic \citep{ma2017} loading) should be straightforward. As an illustration, we identify a hidden inclusion in a nonlinearly conducting matrix using partially unknown thermal loading (Supplementary Information), mimicking an inaccessible surface. This experiment reveals our method's ability to generate good results without further modifications even when the forward problem is ill-posed, which would require including the unknown boundary condition as an additional optimization variable in a classical adjoint-based approach.

In conclusion, we have presented a PINN-based TO framework with a novel eikonal regularization, which we have applied to the noninvasive detection of hidden inclusions. By representing the geometry through a material density field combined with a novel eikonal regularization, our framework is able to discover the number, shapes and locations of hidden structures, without any prior knowledge required regarding the number or the types of shapes to expect.
%Although we have applied our framework to the problem of identifying hidden shapes and inclusions in elastic bodies, we emphasize that the methodology we propose is very general. Specifically, 
Finally, the idea of parameterizing geometries of arbitrary topologies with a material density field regularized with the eikonal constraint opens a pathway for PINNs to be applied to a wide range of design optimization problems constrained by physical governing equations. These include, for instance, the design of lenses that achieve targeted optical properties \citep{molesky2018,ma2021} or the design of structures and metamaterials that exhibit desirable mechanical, acoustic, or thermal properties \citep{bendsoe2003,kadic2019,kollmann2020}.

%The main requirement is that the same governing equations describe, through an explicit dependence on the material density variable, both phases of the problem under consideration. Within the realm of solid mechanics, such problems include TO of structures \citep{bendsoe2003} and metamaterials \citep{kadic2019,kollmann2020}, where the goal is to design structures that exhibit minimum compliance under load or other desirable mechanical, acoustic or thermal properties. In fluid mechanics, shape or TO of structures is also a subject of interest, this time with the objective of minimizing the drag or maximizing the heat exchange of the fluid flowing through or around the structure \citep{borrvall2003,mohammadi2004,dbouk2017}. Finally, one can also envision applications in photonic design, where the goal is to design lenses that demonstrate specified optical properties \citep{molesky2018,ma2021}.

\section*{Methods}

\subsection*{Governing equations}
\label{app:ProblemFormulation}

The two plane-strain elasticity inverse problems considered in this study (Fig.~\ref{fig:Geometry}) are defined in a two-dimensional domain $\Omega \subset \mathbb{R}^2$ formed by the union of the elastic body $\mathcal{B}$ and the voids or inclusions. Denoting with $\mathbf{x} = (x_1,x_2) \in \Omega$ the planar spatial coordinates, the hidden geometrical layout of voids or inclusions is characterized by a material density $\rho(\mathbf{x})$ equal to 1 in the body $\mathcal{B}$ and 0 in the voids or inclusions.

\subsubsection*{Small-deformation linear elasticity} 
\label{app:SmallDeformationLinearElasticity}

We first consider the case where the elastic body and inclusions consist of linear elastic materials, with Young's modulus $E$ and Poisson's ratio $\nu$ for the body, and Young's modulus $\bar{E}$ and Poisson's ratio $\bar{\nu}$ for the inclusions. Voids and rigid inclusions correspond to the limits $\bar{E} \rightarrow 0$ and $\bar{E} \rightarrow \infty$, respectively. The deformation of the elastic body containing the inclusions is described by a vector field $\boldsymbol{\psi}(\mathbf{x}) = (\mathbf{u}(\mathbf{x}), \boldsymbol{\sigma}(\mathbf{x}))$, where $\mathbf{u}(\mathbf{x})$ is a planar displacement field with components $u_i(\mathbf{x})$ and $\boldsymbol{\sigma}(\mathbf{x})$ is a Cauchy stress tensor with components $\sigma_{ij}(\mathbf{x})$. Indices $i$ and $j$ will hereafter always range from 1 to 2. 

The governing PDEs comprise the equilibrium equations
\begin{equation}
\sum_{j}\frac{\partial \sigma_{ij}}{\partial x_j} = 0, \quad \mathbf{x} \in \Omega,
\label{eq:LinearEquilibrium}
\end{equation}
as well as a linear elastic constitutive law $F(\boldsymbol{\sigma}, \nabla \mathbf{u}, \rho) = 0$ that we will express in two different but equivalent ways, depending on whether the inclusions are softer or stiffer than the matrix. For voids and soft inclusions, we consider the constitutive law in stress-strain form,
\begin{align}
\boldsymbol{\sigma} &= \rho \left[ \lambda \, \mathrm{tr} (\boldsymbol{\epsilon}) \, \mathbf{I} + 2 \mu \, \boldsymbol{\epsilon} \right], \nonumber \\
&\quad \ (1 - \rho) \left[ \bar{\lambda} \, \mathrm{tr} (\boldsymbol{\epsilon}) \, \mathbf{I} + 2 \bar{\mu} \, \boldsymbol{\epsilon} \right], \quad \mathbf{x} \in \Omega,
\label{eq:LinearStressStrainVoid}
\end{align}
where $\boldsymbol{\epsilon} = (\nabla \mathbf{u} + \nabla \mathbf{u}^T)/2$ is the infinitesimal strain tensor, $\mathrm{tr}(\boldsymbol{\epsilon})$ denotes its trace, $\lambda = E \nu/(1+\nu)(1-2\nu)$ and $\mu = E/2(1+\nu)$ are the Lam\'e constants of the body, and $\bar{\lambda} = \bar{E} \bar{\nu}/(1+\bar{\nu})(1-2\bar{\nu})$ and $\bar{\mu} = \bar{E}/2(1+\bar{\nu})$ are the Lam\'e constants of the inclusions. Notice that the case of voids, the stress vanishes in the $\rho = 0$ regions. For stiff and rigid inclusions, 
%the strain must vanish in the $\rho = 0$ regions, 
we consider the constitutive law in the inverted strain-stress form
\begin{align}
\boldsymbol{\epsilon} &= \rho \left[ \frac{1+\nu}{E} \boldsymbol{\sigma} - \frac{\nu(1+\nu)}{E} \, \mathrm{tr} (\boldsymbol{\sigma}) \, \mathbf{I} \right] \nonumber \\
&\quad \ (1-\rho) \left[ \frac{1+\bar{\nu}}{\bar{E}} \boldsymbol{\sigma} - \frac{\bar{\nu}(1+\bar{\nu})}{\bar{E}} \, \mathrm{tr} (\boldsymbol{\sigma}) \, \mathbf{I} \right], \quad \mathbf{x} \in \Omega,
\label{eq:LinearStressStrainInclusion}
\end{align}
where $\mathrm{tr}(\boldsymbol{\sigma})$ is the trace of the stress tensor. This relation differs from the three-dimensional one due to the plane strain assumption. Notice that the case of rigid inclusions, the strain vanishes in the $\rho = 0$ regions.

%A particular loading is prescribed on the outer boundary $\partial \Omega$ of the domain, which can be decomposed into a portion $\partial \Omega_t$ with prescribed traction and a portion $\partial \Omega_u$ with prescribed displacement. The boundary conditions are
%\begin{subequations}
%\begin{alignat}{2}
%\mathbf{u}(\mathbf{x}) &= \bar{\mathbf{u}}(\mathbf{x}), &\quad &\mathbf{x} \in \partial \Omega_u, \\
%\boldsymbol{\sigma}(\mathbf{x}) \mathbf{n}(\mathbf{x}) &= \bar{\mathbf{t}}(\mathbf{x}), &&\mathbf{x} \in \partial \Omega_t,
%\end{alignat} \label{eq:LinearBCs}%
%\end{subequations}
%where $\partial \Omega_t$ and $\partial \Omega_u$ are partitions of the external boundary with applied traction $\bar{\mathbf{t}}(\mathbf{x})$ and applied displacement $\bar{\mathbf{u}}(\mathbf{x})$, respectively, and $\mathbf{n}$ denotes the outward unit normal vector. In the case of the elastic layer, the outer boundary also comprises a portion $\partial \Omega_p$ with periodic boundary conditions on the displacement and traction. 
%Upon application of the loading, the elastic body deforms in a way that satisfies the equilibrium equation \eqref{eq:LinearEquilibrium}, the constitutive relation \eqref{eq:LinearStressStrainVoid} or \eqref{eq:LinearStressStrainInclusion}, and the boundary conditions \eqref{eq:LinearBCs}. We then measure the surface displacements $\mathbf{u}_i^m$ at discrete locations $\mathbf{x}_i$ forming a subset $\partial \Omega^m$ of $\partial \Omega_t$.

The boundary conditions on $\partial \Omega$ and surface displacement measurement locations $\partial \Omega^m$ are different in the two problems. For the elastic matrix (Fig.~\ref{fig:Geometry}a), the domain is $\Omega = [-0.5,0.5] \times [-0.5,0.5]$ and the boundary conditions are
\begin{subequations}
\begin{alignat}{2}
\boldsymbol{\sigma}(\mathbf{x}) \mathbf{n}(\mathbf{x}) &= -P_o \mathbf{e}_1, &\quad &\mathbf{x} \in \{-0.5,0.5\} \times [-0.5,0.5], \\
\boldsymbol{\sigma}(\mathbf{x}) \mathbf{n}(\mathbf{x}) &= \mathbf{0}, \quad &&\mathbf{x} \in [-0.5,0.5] \times \{-0.5,0.5\}.
\end{alignat} \label{eq:MatrixBCs}%
\end{subequations}
The measurement locations $\partial \Omega^m$ are distributed along the entire external boundary $\partial \Omega$. In the case of the M, I, T inclusions (case 19, Tab.~\ref{tab:MatrixCases}), the boundary conditions \eqref{eq:MatrixBCs} are changed to account for the fact that the matrix is pulled from the top and bottom boundaries and covers the domain $\Omega = [-1,1] \times [-0.5,0.5]$. For the elastic layer (Fig.~\ref{fig:Geometry}b), the domain is $\Omega = [0,1] \times [-0.5,0]$ and the boundary conditions are
\begin{subequations}
\begin{alignat}{2}
\boldsymbol{\sigma}(\mathbf{x}) \mathbf{n}(\mathbf{x}) &= -P_o \mathbf{e}_2, &\quad& \mathbf{x} \in [0,1] \times \{0\}, \\
\mathbf{u} &= \mathbf{0}, && \mathbf{x} \in [0,1] \times \{-0.5\},
\end{alignat} \label{eq:LayerBCs}%
\end{subequations}
as well as periodic for the displacement and traction on $\mathbf{x} \in \{0,1\} \times [-0.5,0]$. The measurement locations $\partial \Omega^m$ are distributed along the top surface $\partial \Omega_t = [0,1] \times \{0\}$.

The geometry identification problem that we solve can then be stated as follows. Given surface displacement measurements $\mathbf{u}_i^m$ at locations $\mathbf{x}_i \in \partial \Omega^m$, find the distribution of material density $\rho$ in $\Omega$ such that the difference between the predicted and measured surface displacements vanish, that is,
\begin{equation}
\mathbf{u}(\mathbf{x}_i) = \mathbf{u}_i^m, \quad \mathbf{x}_i \in \partial \Omega^m.
\end{equation}
The predicted displacement field must satisfy the equilibrium equation \eqref{eq:LinearEquilibrium}, the constitutive relation \eqref{eq:LinearStressStrainVoid} or \eqref{eq:LinearStressStrainInclusion}, and the boundary conditions \eqref{eq:MatrixBCs} or \eqref{eq:LayerBCs}.

\subsubsection*{Large-deformation nonlinear hyperelasticity} 
\label{app:LargeDeformationNonlinearHyperelasticity}

Next, we consider the case where the elastic body consists of an incompressible Neo-Hookean hyperelastic material with shear modulus $\mu$.
We now have to distinguish between the reference (undeformed) and current (deformed) configurations. 
We denote by $\mathbf{x} = (x_1,x_2) \in \Omega$ and $\mathbf{y} = (y_1,y_2) \in \Omega^*$ the coordinates in the reference and deformed configurations, respectively, with $\Omega^*$ the deformed image of $\Omega$. 
The displacement field $\mathbf{u}(\mathbf{x})$ with components $u_i(\mathbf{x})$ moves an initial position $\mathbf{x} \in \Omega$ into its current location $\mathbf{y} = \mathbf{x} + \mathbf{u}(\mathbf{x}) \in \Omega^*$. 
In order to formulate the governing equations and boundary conditions in the reference configuration $\Omega$, we need to introduce the first Piola-Kirchhoff stress tensor $\mathbf{S}(\mathbf{x})$ with components $S_{ij}(\mathbf{x})$. Unlike the Cauchy stress tensor, the first Piola-Kirchhoff stress tensor is defined in $\Omega$ and is not symmetric. The deformation of the elastic body is then described by the vector field $\boldsymbol{\psi}(\mathbf{x}) = (\mathbf{u}(\mathbf{x}), \mathbf{S}(\mathbf{x}),p(\mathbf{x}))$ defined over $\Omega$, where $p(\mathbf{x})$ is a pressure field that serves to enforce the incompressibility constraint.

The equilibrium equations are
\begin{equation}
\sum_{j}\frac{\partial S_{ij}}{\partial x_j} = 0, \quad \mathbf{x} \in \Omega,
\label{eq:NonlinearEquilibrium}
\end{equation}
where the derivatives in $\nabla_\mathbf{x}$ are taken with respect to the reference coordinates $\mathbf{x}$. We only consider the presence of voids so that the nonlinear constitutive law $F(\mathbf{S}, \nabla_\mathbf{x} \mathbf{u}, p, \rho) = 0$ is simply expressed as
\begin{equation}
\mathbf{S} = \rho \left[ -p \mathbf{F}^{-T} + \mu \mathbf{F} \right], \quad \mathbf{x} \in \Omega,
\label{eq:NonlinearStressStrain}
\end{equation}
where $\mathbf{F}(\mathbf{x}) = \mathbf{I} + \nabla_\mathbf{x} \mathbf{u}(\mathbf{x})$ is the deformation gradient tensor. Notice that the stress vanishes in the $\rho = 0$ regions. Finally, we have the incompressibility constraint
\begin{equation}
\rho \left[ \det(\mathbf{F}) - 1 \right] = 0, \quad \mathbf{x} \in \Omega,
\label{eq:NonlinearIncompressibility}
\end{equation}
which turns itself off in the $\rho = 0$ regions since voids do not deform in a way that preserves volume.

We only treat the matrix problem (Fig.~\ref{fig:Geometry}a) in this hyperelastic case. The domain is $\Omega = [-0.5,0.5] \times [-0.5,0.5]$ and the boundary conditions are
\begin{subequations}
\begin{alignat}{2}
\mathbf{S}(\mathbf{x}) \mathbf{n}_0(\mathbf{x}) &= -P_o \mathbf{e}_1, &\quad &\mathbf{x} \in \{-0.5,0.5\} \times [-0.5,0.5], \\
\mathbf{S}(\mathbf{x}) \mathbf{n}_0(\mathbf{x}) &= \mathbf{0}, \quad &&\mathbf{x} \in [-0.5,0.5] \times \{-0.5,0.5\}.
\end{alignat} \label{eq:MatrixBCsHyperEla}%
\end{subequations}
As in the linear elastic case, the measurement locations $\partial \Omega^m$ are distributed along the entire external boundary $\partial \Omega$.

The geometry identification problem can then be stated identically as in the linear elastic case. This time, the predicted displacement field must satisfy the equilibrium equation \eqref{eq:NonlinearEquilibrium}, the constitutive relation \eqref{eq:NonlinearStressStrain}, the incompressibility condition \eqref{eq:NonlinearIncompressibility}, and the boundary conditions \eqref{eq:MatrixBCsHyperEla}.

%Similar to the small-deformation case, the outer boundary $\partial \Omega$ of the domain can be decomposed into a portion $\partial \Omega_t$ with prescribed traction and a portion $\partial \Omega_u$ with prescribed displacement. The corresponding boundary conditions are
%\begin{subequations}
%\begin{alignat}{2}
%\mathbf{u}(\mathbf{x}) &= \bar{\mathbf{u}}(\mathbf{x}), &\quad &\mathbf{x} \in \partial \Omega_u, \\
%\mathbf{S}(\mathbf{x}) \mathbf{n}_0(\mathbf{x}) &= \bar{\mathbf{s}}(\mathbf{x}), &&\mathbf{x} \in \partial \Omega_t,
%\end{alignat} \label{eq:NonlinearBCs}%
%\end{subequations}
%where $\bar{\mathbf{u}}(\mathbf{x})$ and $\bar{\mathbf{s}}(\mathbf{x})$ are, respectively, the prescribed boundary displacement and traction in the reference configuration, and $\mathbf{n}_0$ denotes the outward unit normal vector in the reference configuration. In the case of the elastic layer, the outer boundary also comprises a portion $\partial \Omega_p$ with periodic boundary conditions on the displacement and traction. Upon application of the loading, the elastic body deforms in a way that satisfies the equilibrium equation \eqref{eq:NonlinearEquilibrium}, the constitutive relation \eqref{eq:NonlinearStressStrain}, and the boundary conditions \eqref{eq:NonlinearBCs}. We then measure the surface displacements $\mathbf{u}_i^m$ at discrete locations $\mathbf{x}_i$ forming a subset $\partial \Omega^m$ of $\partial \Omega_t$.

\subsubsection*{Rescaling} 
\label{app:Rescaling}

The various physical quantities involved in the elasticity inverse problem span a wide range of scales; for instance, displacements may be orders of magnitude smaller than the length scale associated with the geometry. Thus, we rescale all physical quantities into nondimensional values of order one, as also done in \cite{henkes2022}. Lengths are rescaled with the width $L$ of the elastic matrix or elastic layer, tractions and stresses with the magnitude $P_o$ of the applied traction at the boundaries, and displacements with the ratio $L P_o / E$, where $E$ is the Young's modulus of the elastic material (in the hyperelastic case, we use the equivalent Young's modulus $E = 3 \mu$, where $\mu$ is the shear modulus of the hyperelastic material). This rescaling is critical to enable the neural networks underlying our framework to handle elasticity problems across a wide range of material moduli and applied loads.

\subsection*{Solution methodology}
\label{app:AdditionalInformationSolutionMethodology}

Here, we describe in detail the application of our TO framework to the solution of the two plane-strain elasticity inverse problems formulated in the introduction. We will treat separately the small-deformation linear elasticity case and the large-deformation hyperelasticity case.

\subsubsection*{Small-deformation linear elasticity}

Since the problem is described by the physical quantities $\boldsymbol{\psi} = (u_1,u_2,\sigma_{11},\sigma_{22},\sigma_{12})$, we introduce the neural network approximations
\begin{subequations}
\begin{align}
u_1(\mathbf{x}) &= \bar{u}_1(\mathbf{x}; \boldsymbol{\theta}_1), \\
u_2(\mathbf{x}) &= \bar{u}_2(\mathbf{x}; \boldsymbol{\theta}_2), \\
\sigma_{11}(\mathbf{x}) &= \bar{\sigma}_{11}(\mathbf{x}; \boldsymbol{\theta}_3), \\
\sigma_{22}(\mathbf{x}) &= \bar{\sigma}_{22}(\mathbf{x}; \boldsymbol{\theta}_4), \\
\sigma_{12}(\mathbf{x}) &= \bar{\sigma}_{12}(\mathbf{x}; \boldsymbol{\theta}_5), \\
\phi(\mathbf{x}) &= \bar{\phi}(\mathbf{x}; \boldsymbol{\theta}_\phi).
\end{align} \label{eq:LinearNN}%
\end{subequations}
The last equation represents the level-set neural network, which defines the material density as $\rho(\mathbf{x}) = \bar{\rho}(\mathbf{x};\boldsymbol{\theta}_\phi) = \mathrm{sigmoid}(\bar{\phi}(\mathbf{x};\boldsymbol{\theta}_\phi)/\delta)$. We then formulate the loss function \eqref{eq:TotalLoss} by specializing the loss term expressions presented in results section to the linear elasticity problem. Omitting the $\boldsymbol{\theta}$'s for notational simplicity, we obtain
\begin{subequations}
\begin{align}
\mathcal{L}_\mathrm{meas}(\boldsymbol{\theta}_{\boldsymbol{\psi}}) &= \frac{1}{|\partial \Omega^m|} \sum_{\mathbf{x}_i \in \partial \Omega^m}  |\bar{\mathbf{u}}(\mathbf{x}_i) - \mathbf{u}_i^m|^2, \label{eq:LinearLmeas} \\
\mathcal{L}_\mathrm{gov}(\boldsymbol{\theta}_{\boldsymbol{\psi}}, \boldsymbol{\theta}_\phi) &= \frac{1}{|\Omega^d|} \sum_{\mathbf{x}_i \in \Omega^d} |\mathbf{r}_\mathrm{eq}(\bar{\boldsymbol{\sigma}}(\mathbf{x}_i))|^2 \nonumber \\
&\quad + \frac{1}{|\Omega^d|} \sum_{\mathbf{x}_i \in \Omega^d} |\mathbf{r}_\mathrm{cr}(\bar{\mathbf{u}}(\mathbf{x}_i),\bar{\boldsymbol{\sigma}}(\mathbf{x}_i),\bar{\rho}(\mathbf{x}_i))|^2, \label{eq:LinearLF} \\
\mathcal{L}_\mathrm{eik}(\boldsymbol{\theta}_\phi) &= \frac{1}{|\Omega_\mathrm{eik}^d|} \sum_{\mathbf{x}_i \in \Omega_\mathrm{eik}^d} \left(| \nabla \bar{\phi} (\mathbf{x}_i)| - 1 \right)^2,
\end{align}
\end{subequations}
where $\bar{\mathbf{u}} = (\bar{u}_1,\bar{u}_2)$ and $\bar{\boldsymbol{\sigma}}$ has components $\bar{\sigma}_{i,j}$, $i,j=1,2$. In \eqref{eq:LinearLF}, the terms $\mathbf{r}_\mathrm{eq}$ and $\mathbf{r}_\mathrm{cr}$ refer to the residuals of the equilibrium equation \eqref{eq:LinearEquilibrium} and the constitutive relation \eqref{eq:LinearStressStrainVoid} or \eqref{eq:LinearStressStrainInclusion}. The eikonal loss term is problem-independent and therefore identical to \eqref{eq:EikonalLoss}. 

We note that instead of defining neural network approximations for the displacements and the stresses, we could define neural network approximations solely for the displacements, that is, $\boldsymbol{\psi} = (u_1,u_2)$. In this case, the loss term \eqref{eq:LinearLF} would only include the residual of the equilibrium equation \eqref{eq:LinearEquilibrium}, in which the stress components would be directly expressed in terms of the displacements and the material distribution using the constitutive relation \eqref{eq:LinearStressStrainVoid}. However, several recent studies \citep{rao2021,haghighat2021,henkes2022,rezaei2022,gladstone2022,harandi2023} have shown that the mixed formulation adopted in the present work results in superior accuracy and training performance, which could partly be explained by the fact that only first-order derivatives of the neural network outputs are involved since the displacements and stresses are only differentiated to first oder in \eqref{eq:LinearEquilibrium} and \eqref{eq:LinearStressStrainVoid}. In our case, the mixed formulation holds the additional advantage that it enables us to treat stiff and rigid inclusions using the inverted constitutive relation \eqref{eq:LinearStressStrainInclusion} instead of \eqref{eq:LinearStressStrainVoid}. Finally, the mixed formulation allows us to directly integrate both displacement and traction boundary conditions into the output of the neural network approximations, as we describe in the next paragraph.

We design the architecture of the neural networks in such a way that they inherently satisfy the boundary conditions, treating the latter as hard constraints \citep{dong2021,sukumar2022}. For the elastic matrix, we do this through the transformations
\begin{subequations}
\begin{align}
\bar{u}_1(\mathbf{x}; \boldsymbol{\theta}_1) &= \bar{u}_1'(\mathbf{x}; \boldsymbol{\theta}_1), \\
\bar{u}_2(\mathbf{x}; \boldsymbol{\theta}_2) &= \bar{u}_2'(\mathbf{x}; \boldsymbol{\theta}_2), \\
\bar{\sigma}_{11}(\mathbf{x}; \boldsymbol{\theta}_3) &= (x-0.5)(x+0.5) \, \bar{\sigma}_{11}'(\mathbf{x}; \boldsymbol{\theta}_3) + P_o, \\
\bar{\sigma}_{22}(\mathbf{x}; \boldsymbol{\theta}_4) &= (y-0.5)(y+0.5) \, \bar{\sigma}_{22}'(\mathbf{x}; \boldsymbol{\theta}_4), \\
\bar{\sigma}_{12}(\mathbf{x}; \boldsymbol{\theta}_5) &= (x-0.5)(x+0.5) \cdot \nonumber \\
&\quad \ (y-0.5)(y+0.5) \, \bar{\sigma}_{12}'(\mathbf{x}; \boldsymbol{\theta}_5), \\
\bar{\phi}(\mathbf{x}; \boldsymbol{\theta}_\phi) &= (x-0.5)(x+0.5) \cdot \nonumber \\
&\quad \ (y-0.5)(y+0.5) \, \bar{\phi}'(\mathbf{x}; \boldsymbol{\theta}_\phi) + w,
\end{align}
\end{subequations}
where the quantities with a prime denote the raw output of the neural network. In this way, the neural network approximations defined in \eqref{eq:LinearNN} obey by construction the boundary conditions \eqref{eq:MatrixBCs}. Further, since we know that the elastic material is present all along the outer surface $\partial \Omega$, we define $\bar{\phi}$ so that $\phi = w$ on $\partial \Omega$, which ensures that $\rho = \mathrm{sigmoid}(\phi/\delta) \simeq 1$ on $\partial \Omega$ (recall that $w$ is such that $\mathrm{sigmoid}(w/2\delta) \simeq 1$). In the case of the M, I, T inclusions, these transformations are changed to reflect the fact that the matrix is wider and pulled from the top and bottom. For the periodic elastic layer, we introduce the transformations
\begin{subequations}
\begin{align}
\bar{u}_1(\mathbf{x}; \boldsymbol{\theta}_1) &= (y+0.5) \, \bar{u}_1'(\cos x, \sin x, y; \boldsymbol{\theta}_1), \\
\bar{u}_2(\mathbf{x}; \boldsymbol{\theta}_2) &= (y+0.5) \, \bar{u}_2'(\cos x, \sin x, y; \boldsymbol{\theta}_2), \\
\bar{\sigma}_{11}(\mathbf{x}; \boldsymbol{\theta}_3) &= \bar{\sigma}_{11}'(\cos x, \sin x, y; \boldsymbol{\theta}_3), \\
\bar{\sigma}_{22}(\mathbf{x}; \boldsymbol{\theta}_4) &= y \, \bar{\sigma}_{22}'(\cos x, \sin x, y; \boldsymbol{\theta}_4) - P_o, \\
\bar{\sigma}_{12}(\mathbf{x}; \boldsymbol{\theta}_5) &= y \, \bar{\sigma}_{12}'(\cos x, \sin x, y; \boldsymbol{\theta}_5), \\
\bar{\phi}(\mathbf{x}; \boldsymbol{\theta}_\phi) &= y(y+0.5) \, \bar{\phi}'(\cos x, \sin x, y; \boldsymbol{\theta}_\phi) \nonumber \\
&\quad + w(4y+1),
\end{align}
\end{subequations}
so that the neural network approximations defined in \eqref{eq:LinearNN} obey by construction the boundary conditions \eqref{eq:LayerBCs} and are periodic along the $x$ direction. Further, since we know that the elastic material is present all along the top surface $y = 0$ and the rigid substrate is present all along the bottom surface $y = -0.5$, we define $\bar{\phi}$ so that $\phi = w$ for $y = 0$ and $\phi = -w$ for $y=-0.5$, which ensures that $\rho = \mathrm{sigmoid}(\phi/\delta) \simeq 1$ for $y = 0$ and $\rho \simeq 0$ for $y=-0.5$.

\subsubsection*{Large-deformation hyperelasticity}

The problem is now described by the physical quantities $\boldsymbol{\psi} = (u_1,u_2,S_{11},S_{22},S_{12},S_{21},p)$. We therefore introduce the neural network approximations
\begin{subequations}
\begin{align}
u_1(\mathbf{x}) &= \bar{u}_1(\mathbf{x}; \boldsymbol{\theta}_1), \\
u_2(\mathbf{x}) &= \bar{u}_2(\mathbf{x}; \boldsymbol{\theta}_2), \\
S_{11}(\mathbf{x}) &= \bar{S}_{11}(\mathbf{x}; \boldsymbol{\theta}_3), \\
S_{22}(\mathbf{x}) &= \bar{S}_{22}(\mathbf{x}; \boldsymbol{\theta}_4), \\
S_{12}(\mathbf{x}) &= \bar{S}_{12}(\mathbf{x}; \boldsymbol{\theta}_5), \\
S_{21}(\mathbf{x}) &= \bar{S}_{21}(\mathbf{x}; \boldsymbol{\theta}_6), \\
p(\mathbf{x}) &= \bar{p}(\mathbf{x}; \boldsymbol{\theta}_7), \\
\phi(\mathbf{x}) &= \bar{\phi}(\mathbf{x}; \boldsymbol{\theta}_\phi),
\end{align} \label{eq:NonlinearNN}%
\end{subequations}
and the material distribution is given by $\rho(\mathbf{x}) = \bar{\rho}(\mathbf{x};\boldsymbol{\theta}_\phi) = \mathrm{sigmoid}(\bar{\phi}(\mathbf{x};\boldsymbol{\theta}_\phi)/\delta)$. We then formulate the loss function \eqref{eq:TotalLoss} by specializing the loss term expressions presented in Section \ref{sec:GeneralFramework} to the linear elasticity problem, using the governing equations given in Appendix \ref{app:SmallDeformationLinearElasticity}. Omitting the $\boldsymbol{\theta}$'s for notational simplicity, we obtain
\begin{subequations}
\begin{align}
\mathcal{L}_\mathrm{meas}(\boldsymbol{\theta}_{\boldsymbol{\psi}}) &= \frac{1}{|\partial \Omega^m|} \sum_{\mathbf{x}_i \in \partial \Omega^m}  |\bar{\mathbf{u}}(\mathbf{x}_i) - \mathbf{u}_i^m|^2, \label{eq:NonlinearLmeas} \\
\mathcal{L}_\mathrm{gov}(\boldsymbol{\theta}_{\boldsymbol{\psi}}, \boldsymbol{\theta}_\phi) &= \frac{1}{|\Omega^d|} \sum_{\mathbf{x}_i \in \Omega^d} |\mathbf{r}_\mathrm{eq}(\bar{\mathbf{S}}(\mathbf{x}_i))|^2 \nonumber \\
&\quad+ \frac{1}{|\Omega^d|} \sum_{\mathbf{x}_i \in \Omega^d} |\mathbf{r}_\mathrm{cr}(\bar{\mathbf{u}}(\mathbf{x}_i),\bar{\mathbf{S}}(\mathbf{x}_i),\bar{p}(\mathbf{x}_i),\bar{\rho}(\mathbf{x}_i))|^2 \nonumber \\
&\quad+ \frac{1}{|\Omega^d|} \sum_{\mathbf{x}_i \in \Omega^d} |\mathbf{r}_\mathrm{inc}(\bar{\mathbf{u}}(\mathbf{x}_i),\bar{\rho}(\mathbf{x}_i))|^2, \label{eq:NonlinearLF} \\
\mathcal{L}_\mathrm{eik}(\boldsymbol{\theta}_\phi) &= \frac{1}{|\Omega_\mathrm{eik}^d|} \sum_{\mathbf{x}_i \in \Omega_\mathrm{eik}^d} \left(| \nabla \bar{\phi} (\mathbf{x}_i)| - 1 \right)^2,
\end{align}
\end{subequations}
where $\bar{\mathbf{u}} = (\bar{u}_1,\bar{u}_2)$ and $\bar{\mathbf{S}}$ has components $\bar{S}_{i,j}$, $i,j=1,2$. In \eqref{eq:NonlinearLF}, the terms $\mathbf{r}_\mathrm{eq}$, $\mathbf{r}_\mathrm{cr}$, and $\mathbf{r}_\mathrm{inc}$ refer to the residuals of the equilibrium equation \eqref{eq:NonlinearEquilibrium}, the constitutive relation \eqref{eq:NonlinearStressStrain}, and the incompressibility constraint \eqref{eq:NonlinearIncompressibility}. The eikonal loss term is problem-independent and therefore identical to \eqref{eq:EikonalLoss}. 

As in the linear elasticity case, we design the architecture of the neural networks in such a way that they inherently satisfy the boundary conditions. For the elastic matrix problem,
\begin{subequations}
\begin{align}
\bar{u}_1(\mathbf{x}; \boldsymbol{\theta}_1) &= \bar{u}_1'(\mathbf{x}; \boldsymbol{\theta}_1), \\
\bar{u}_2(\mathbf{x}; \boldsymbol{\theta}_2) &= \bar{u}_2'(\mathbf{x}; \boldsymbol{\theta}_2), \\
\bar{S}_{11}(\mathbf{x}; \boldsymbol{\theta}_3) &= (x-0.5)(x+0.5) \, \bar{S}_{11}'(\mathbf{x}; \boldsymbol{\theta}_3) + P_o, \\
\bar{S}_{22}(\mathbf{x}; \boldsymbol{\theta}_4) &= (y-0.5)(y+0.5) \, \bar{S}_{22}'(\mathbf{x}; \boldsymbol{\theta}_4), \\
\bar{S}_{12}(\mathbf{x}; \boldsymbol{\theta}_5) &= (y-0.5)(y+0.5) \, \bar{S}_{12}'(\mathbf{x}; \boldsymbol{\theta}_5), \\
\bar{S}_{21}(\mathbf{x}; \boldsymbol{\theta}_6) &= (x-0.5)(x+0.5) \, \bar{S}_{21}'(\mathbf{x}; \boldsymbol{\theta}_6), \\
\bar{p}(\mathbf{x}; \boldsymbol{\theta}_7) &= \bar{p}'(\mathbf{x}; \boldsymbol{\theta}_7), \\
\bar{\phi}(\mathbf{x}; \boldsymbol{\theta}_\phi) &= (x-0.5)(x+0.5) \cdot \nonumber \\
&\quad \ (y-0.5)(y+0.5) \, \bar{\phi}'(\mathbf{x}; \boldsymbol{\theta}_\phi) + w,
\end{align}
\end{subequations}
where the quantities with a prime denote the raw output of the neural network. In this way, the neural network approximations defined in \eqref{eq:NonlinearNN} obey by construction the boundary conditions of the problem. As before, since we know that the elastic material is present all along the outer surface $\partial \Omega$, we define $\bar{\phi}$ so that $\phi = w$ on $\partial \Omega$, which ensures that $\rho = \mathrm{sigmoid}(\phi/\delta) \simeq 1$ on $\partial \Omega$.

\subsection*{Architecture and training details}
\label{app:ImplementationDetails}

Here, we provide implementation details regarding the architecture of the deep neural networks, the training procedure and corresponding parameter values.

\subsubsection*{Neural network architecture}
\label{app:NeuralNetworkFormulation}

State variable fields of the form $\psi(\mathbf{x})$ are approximated using deep fully-connected neural networks that map the location $\mathbf{x}$ to the corresponding value of $\psi$ at that location. This map can be expressed as $\psi(\mathbf{x}) = \bar{\psi}(\mathbf{x};\boldsymbol{\theta})$, and is defined by the sequence of operations
\begin{subequations}
\begin{align}
\mathbf{z}^0 &= \mathbf{x}, \label{eq:NNInput} \\
\mathbf{z}^k &= \sigma(\mathbf{W}^k \mathbf{z}^{k-1} + \mathbf{b}^k), \quad 1 \le k \le \ell-1, \label{eq:NNMiddleLayers} \\
\psi = \mathbf{z}^\ell &= \mathbf{W}^\ell \mathbf{z}^{\ell-1} + \mathbf{b}^\ell.
\end{align}
\end{subequations}
The input $\mathbf{x}$ is propagated through $\ell$ layers, all of which (except the last) take the form of a linear operation composed with a nonlinear transformation. Each layer outputs a vector $\mathbf{z}^k \in \mathbb{R}^{q_k}$, where $q_k$ is the number of `neurons', and is defined by a weight matrix $\mathbf{W}^k \in \mathbb{R}^{q_k \times q_{k-1}}$, a bias vector $\mathbf{b}^k \in \mathbb{R}^{q_k}$, and a nonlinear activation function $\sigma(\cdot)$. Finally, the output of the last layer is assigned to $\psi$. The weight matrices and bias vectors, which parametrize the map from $\mathbf{x}$ to $\psi$, form a set of trainable parameters $\boldsymbol{\theta} = \{\mathbf{W}^k,\mathbf{b}^k\}_{k=1}^\ell$.

The choice of the nonlinear activation function $\sigma(\cdot)$ and the initialization procedure for the trainable parameters $\boldsymbol{\theta}$ are both important factors in determining the performance of neural networks. While the tanh function has been a popular candidate in the context of PINNs \citep{lu2021a}, recent works by Refs.~\cite{sitzmann2020,wong2021} have shown that using sinusoidal activation functions can lead to improved training performance by promoting the emergence of small-scale features. In this work, we select the sinusoidal representation network (SIREN) architecture from Ref.~\cite{sitzmann2020}, which combines the use of the sine as an activation function with a specific way to initialize the trainable parameters $\boldsymbol{\theta}$ that ensures that the distribution of the input to each sine activation function remains unchanged over successive layers. Specifically, each component of  $\mathbf{W}^k$ is uniformly distributed between $- \sqrt{6/q_k}$ and $\sqrt{6/q_k}$ where $q_k$ is the number of neurons in layer $k$, and $\mathbf{b}^k = \mathbf{0}$, for $k=1, \dots, \ell$. Further, the first layer of the SIREN architecture is $\mathbf{z}^1 = \sigma(\omega_0 \mathbf{W}^1 \mathbf{z}^0 + \mathbf{b}^1)$ instead of \eqref{eq:NNMiddleLayers}, with the extra scalar $\omega_0$ promoting higher-frequency content in the output.

\subsubsection*{Training procedure}

We construct the total loss function \eqref{eq:TotalLoss} and train the neural networks in TensorFlow 2. The training is performed using ADAM, a first-order gradient-descent-based algorithm with adaptive step size \cite{kingma2014}. In each case, we repeat the training over four random initializations of the neural networks parameters and report the best results. Three tricks resulted in noticeably improved training performance and consistency:
\begin{itemize}
\item First, we found that pretraining the level-set neural network $\phi(\mathbf{x}) = \bar{\phi}(\mathbf{x}; \boldsymbol{\theta}_\phi)$ in a standard supervised setting leads to much more consistent results over different initializations of the neural networks. During this pretraining step, carried out before the main optimization step in which all neural networks are trained to minimize the loss \eqref{eq:TotalLoss}, we minimize the mean-square error 
\begin{equation}
\mathcal{L}_\mathrm{sup}(\boldsymbol{\theta}_\phi) = \frac{1}{|\Omega^d|} \sum_{\mathbf{x}_i \in \Omega^d} |\bar{\phi}(\mathbf{x}_i; \boldsymbol{\theta}_\phi) - \phi_i |,
\end{equation}
where $\Omega^d$ is the same set of collocation points as in \eqref{eq:LossGov}, the supervised labels $\phi_i = |\mathbf{x}_i| - 0.25$ for the elastic matrix, and $\phi_i = y_i + 0.25$ for the elastic layer. The material density $\bar{\rho}(\mathbf{x};\boldsymbol{\theta}_\phi) = \mathrm{sigmoid}(\bar{\phi}(\mathbf{x};\boldsymbol{\theta}_\phi)/\delta)$ obtained at the end of this pretraining step is one outside a circle of radius 0.25 centered at the origin for the elastic matrix, and it is one above the horizontal line $y = -0.25$ for the elastic layer. This choice for the supervised labels is justified by the fact that $\rho$ is known to be one along the outer boundary of the domain $\Omega$ for the elastic matrix, and it is known to be one (zero) along the top (bottom) boundary of $\Omega$ for the elastic layer. 
\item Second, during the main optimization in which all neural networks are trained to minimize the loss \eqref{eq:TotalLoss}, we evaluate the loss component $\mathcal{L}_\mathrm{gov}$ in \eqref{eq:LossGov} using a different subset, or mini-batch, of residual points from $\Omega^d$ at every iteration. Such a mini-batching approach has been reported to improve the convergence of the PINN training process \citep{wight2021,daw2022}, corroborating our own observations. In our case, we choose to divide the set $\Omega^d$ into 10 different mini-batches of size $|\Omega^d|/10$, which are then employed sequentially to evaluate $\mathcal{L}_\mathrm{gov}$ during each subsequent gradient update
\begin{subequations}
\begin{align}
\boldsymbol{\theta}_{\boldsymbol{\psi}}^{k+1} &= \boldsymbol{\theta}_{\boldsymbol{\psi}}^k - \alpha_{\boldsymbol{\psi}}(k) \nabla_{\boldsymbol{\theta}_{\boldsymbol{\psi}}} \mathcal{L}(\boldsymbol{\theta}_{\boldsymbol{\psi}}^k, \boldsymbol{\theta}_\phi^k), \\
\boldsymbol{\theta}_\phi^{k+1} &= \boldsymbol{\theta}_\phi^k - \alpha_\phi(k) \nabla_{\boldsymbol{\theta}_\phi} \mathcal{L}(\boldsymbol{\theta}_{\boldsymbol{\psi}}^k, \boldsymbol{\theta}_\phi^k).
\end{align} \label{eq:GradientUpdate}%
\end{subequations}
An epoch of training, which is defined as one complete pass through the whole set $\Omega^d$, therefore consists of 10 gradient updates.
\item Third, the initial nominal step size $\alpha_{\boldsymbol{\psi}}$ governing the learning rate of the physical quantities neural networks is set to be 10 times larger than its counterpart $\alpha_\phi$ governing the learning rate of the level-set neural network. This results in a separation of time scales between the rate of change of the physical quantities neural networks and that of the level-set neural network, which is motivated by the idea that physical quantities should be given time to adapt to a given geometry before the geometry itself changes.
\end{itemize} 

\subsubsection*{Parameter values}

The parameter values described below apply to all results presented in this paper.

\begin{itemize}
\item \textbf{Neural network architecture.} For all cases except the M, I, T inclusions (case 19, Tab.~\ref{tab:MatrixCases}), we opted for neural networks with 4 hidden layers of 50 neurons each, which we found to be a good compromise between expressivity and training time. For the M, I, T inclusions, we used 6 hidden layers with 100 neurons each. Further, we choose $\omega_0 = 10$ as the scalar appearing in the first layer of the SIREN architecture.
\item \textbf{Collocation and measurements points.} In the square and rectangle elastic matrix problems, we consider that the boundary displacement is measured along each of the four external boundaries at 100 equally-spaced points, which amounts to $|\partial \Omega^m| = 400$. In the elastic layer problem, we consider that the boundary displacement is measured along the top boundary at 100 equally-spaced points, which amounts to $|\partial \Omega^m| = 100$. For both geometries except the M, I, T inclusions, the set of collocation points $\Omega^d$ consists of 10000 points distributed in $\Omega$ with a Latin Hypercube Sampling (LHS) strategy, yielding 10 mini-batches containing 1000 points each. For the M, I, T inclusions, $\Omega^d$ consists of 50000 points, yielding 50 mini-batches containing 1000 points each.
\item \textbf{Training parameters.} The pretraining of the level-set neural network is carried out using the ADAM optimizer with nominal step size $10^{-3}$ over 800 training epochs, employing the whole set $\Omega^d$ to compute the gradient of $\mathcal{L}_\mathrm{sup}$ at each update step. The main optimization, during which all neural networks are trained to minimize the total loss \eqref{eq:TotalLoss}, is carried out using the ADAM optimizer. For the matrix cases except the M, I, T inclusions, we use a total of 150k training epochs starting from a nominal step size $10^{-4}$ for the level-set neural network and $10^{-3}$ for the other neural networks. This step size is reduced to $10^{-4}$ for all neural networks at 60k epochs, and again to $10^{-5}$ at 120k epochs. The schedule is the same for the elastic layer cases, with the difference that we use a total of 200k training epochs. For the matrix case with the M, I, T inclusions, we use a total of 50k epochs (note that each epoch contains 5 times as many mini-batches as in the other cases) starting from a nominal step size $10^{-4}$ for the level-set neural network and $10^{-3}$ for the other neural networks. This step size is reduced to $10^{-4}$ for all neural networks at 16k epochs, and again to $10^{-5}$ at 40k epochs. Finally, the scalar weights in the loss \eqref{eq:TotalLoss} are assigned the values $\lambda_\mathrm{meas} = 10$, $\lambda_\mathrm{gov} = 1$, and $\lambda_\mathrm{reg} = 1$ for all cases. We also multiply the second term of $\mathcal{L}_\mathrm{gov}$ in \eqref{eq:LinearLF} and \eqref{eq:NonlinearLF} with a scalar weight $\lambda_\mathrm{cr} = 10$.
\end{itemize}

\subsection*{FEM simulations}
\label{app:FEMSimulations}

The FEM simulations that provide the boundary displacement data and the ground truth are performed in the software Abaqus, using its Standard (implicit) solver. The list of all cases considered in provided in Tab.~\ref{tab:MatrixCases} for the elastic matrix setup (Fig.~\ref{fig:Geometry}a) and in Tab.~\ref{tab:LayerCases} for the periodic elastic layer setup (Fig.~\ref{fig:Geometry}b). Every case is meshed using a linear density of 200 elements per unit length along each boundary, corresponding to between 25k to 80k total elements depending on domain size as well as number and shapes of voids or inclusions. We employ bilinear quadrilateral CPE4 plain-strain elements for the cases involving a linear elastic material, and their hybrid constant-pressure counterpart CPE4H for the cases involving a hyperelastic material. We apply a load $P_o/E = 0.01$ for the cases involving a linear elastic material, and a load $P_o/E = 0.173$ for the cases involving a hyperelastic material.

\begin{table}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{ c c c c }

\toprule
Case & Geometry & Matrix & Inclusion \\ 
\toprule
1 & \multirow{2}{2cm}{Two circles} & LE & V \\
2 & & HE & V \\
\midrule
3 & \multirow{5}{2cm}{One star and one rectangle}  & LE & V \\
4 & & LE & LE-soft \\
5 & & LE & LE-stiff \\
6 & & LE & R \\
7 & & HE & V \\
\midrule
8 & \multirow{2}{2cm}{One slit} & LE & V \\
9 & & HE & V \\
\midrule
10 & \multirow{5}{2cm}{One U} & LE & V \\
11 & & LE & LE-soft \\
12 & & LE & LE-stiff \\
13 & & LE & R \\
14 & & HE & V \\
\midrule
15 & \multirow{2}{2cm}{One T} & LE & V \\
16 & & HE & V \\
\midrule
17 & \multirow{2}{2cm}{Four circles} & LE & V \\
18 & & HE & V \\
\midrule
\multirow{2}{*}{19} & \multirow{2}{2cm}{One M, one I and one T} &  \multirow{2}{*}{LE} & \multirow{2}{*}{LE-soft} \\
  & & \\
\bottomrule
\end{tabular}
\caption{List of all elastic matrix (Fig.~\ref{fig:Geometry}a) cases, classified according to the geometry of the inclusions and the elastic properties of the matrix and inclusions. LE: linear elastic with $E = 1$ and $\nu = 0.3$; HE: incompressible Neo-Hookoean hyperelastic with $\mu = 0.38$; LE-soft: linear elastic with $E = 0.2$, $\nu = 0.3$; LE-stiff: linear elastic with $E = 5$, $\nu = 0.3$; R: rigid; V: void. \label{tab:MatrixCases}}
\end{table}

\begin{table}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{ c c c c }
\toprule
Case & Geometry & Layer & Substrate \\ 
\toprule
20 & \multirow{1}{2cm}{Sinusoidal} & LE & R \\
\midrule
21 & \multirow{1}{2cm}{Pulse} & LE & R \\
\midrule
22 & \multirow{1}{2cm}{Random wave} & LE & R \\
\bottomrule
\end{tabular}
\caption{List of all periodic elastic layer (Fig.~\ref{fig:Geometry}b) cases, classified according to the shape of the substrate. LE: linear elastic with $E = 1$ and $\nu = 0.3$. \label{tab:LayerCases}}
\end{table}

\bibliographystyle{abbrvnat}
\bibliography{bibliography}
%Use of the above commands will create a bibliography using the .bib file. Shown below is a bibliography built from individual items.

\setcounter{figure}{0}
\renewcommand{\figurename}{Extended Data Fig.}
%\renewcommand{\thefigure}{\arabic{figure}}

%%%%
\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figures/NCS_ElaVoid_Iters.pdf}
\caption{\textbf{Identification of voids in a linear elastic matrix.} Evolution of the material density during the training process for the cases reported in Fig.~\ref{fig:NCS_Ela}a-d.}
\label{fig:NCS_ElaVoid_Iters}
\end{figure*}
%%%%

%%%%
\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figures/NCS_ElaVoid_Stress.pdf}
\caption{\textbf{Identification of voids in a linear elastic matrix.} Final Cauchy stress components $\sigma_{xx}$ (\textbf{a}), $\sigma_{yy}$ (\textbf{b}), and $\sigma_{xy}$ (\textbf{c}), displayed in the deformed configuration obtained from the final displacement components $u_1$ and $u_2$, for the cases reported in Fig.~\ref{fig:NCS_Ela}a-d. The grey dotted lines show the outline of the matrix surface in the reference configuration.}
\label{fig:NCS_ElaVoid_Stress}
\end{figure*}
%%%%

%%%%
\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figures/NCS_ElaInclusion_Stress.pdf}
\caption{\textbf{Identification of inclusions in a linear elastic matrix.} Final Cauchy stress components $\sigma_{xx}$ (\textbf{a}), $\sigma_{yy}$ (\textbf{b}), and $\sigma_{xy}$ (\textbf{c}), displayed in the deformed configuration obtained from the final displacement components $u_1$ and $u_2$, for the cases reported in Fig.~\ref{fig:NCS_Ela}e. The grey dotted lines show the outline of the matrix surface in the reference configuration.}
\label{fig:NCS_ElaInclusion_Stress}
\end{figure*}
%%%%

%%%%
\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figures/NCS_HyperElaVoid_Stress.pdf}
\caption{\textbf{Identification of voids in a nonlinear hyperelastic matrix.} Final Cauchy stress components $\sigma_{xx}$ (\textbf{a}), $\sigma_{yy}$ (\textbf{b}), and $\sigma_{xy}$ (\textbf{c}), displayed in the deformed configuration obtained from the final displacement components $u_1$ and $u_2$, for the cases reported in Fig.~\ref{fig:NCS_Ela}f. The grey dotted lines show the outline of the matrix surface in the reference configuration.}
\label{fig:NCS_HyperElaVoid_Stress}
\end{figure*}
%%%%

%%%%
\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figures/NCS_SparseMeasCircle.pdf}
\caption{\textbf{Effect of sparse measurements on the identification of voids in a linear elastic matrix with one circle-shaped void.} Final material density obtained in our framework when using fewer measurement locations and restricting the number of measurements to a subset of the outer surfaces.}
\label{fig:NCS_SparseMeasCircle}
\end{figure*}
%%%%

%%%%
\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figures/NCS_SparseMeasTwo.pdf}
\caption{\textbf{Effect of sparse measurements on the identification of voids in a linear elastic matrix with one star-shaped and one rectangle-shaped void.} Final material density obtained in our framework when using fewer measurement locations and restricting the number of measurements to a subset of the outer surfaces.}
\label{fig:NCS_SparseMeasTwo}
\end{figure*}
%%%%

%%%%
\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figures/NCS_SparseMeasSlit.pdf}
\caption{\textbf{Effect of sparse measurements on the identification of voids in a linear elastic matrix with one slit-shaped void.} Final material density obtained in our framework when using fewer measurement locations and restricting the number of measurements to a subset of the outer surfaces.}
\label{fig:NCS_SparseMeasSlit}
\end{figure*}
%%%%

%%%%
\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{Figures/NCS_AbaqusStress.pdf}
\caption{\textbf{Stress distribution in a linear elastic matrix for different types of U-shaped inclusions.} The von Mises stress obtained in Abaqus for U-shaped inclusions with different constitutive properties reveals that the concave part of the matrix is subject to very little stress in the case of a void or rigid inclusion. Note also that stiff and rigid inclusions `strengthen' the matrix, as opposed to the void and soft inclusion that `soften' the matrix.}
\label{fig:NCS_AbaqusStress}
\end{figure*}
%%%%

\end{document}
