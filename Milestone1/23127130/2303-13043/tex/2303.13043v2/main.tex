
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              %

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{xspace}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[dvipsnames]{xcolor}
\usepackage{diagbox}
\usepackage{pifont}
\usepackage{svg}
\usepackage{amsmath}
\usepackage[numbers]{natbib}
\usepackage{enumitem}
\usepackage{physics}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{gensymb}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{footmisc}
\usepackage{empheq}
\usepackage{cases}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{tabu}
\usepackage{flushend}
\usepackage{multirow}
\usepackage{bm}
\usepackage{color, colortbl}


\newcommand\todo[1]{\textcolor{Peach}{[TODO: #1]}}
\newcommand\xin[1]{\textcolor{blue}{[xin: #1]}}
\newcommand\baifeng[1]{\textcolor{OliveGreen}{[baifeng: #1]}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\def\cvprPaperID{233} %
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}



\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g.}\xspace} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e.}\xspace} \def\Ie{\emph{I.e}\onedot}
\def\vs{\emph{vs.}\xspace}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\def\viz{\emph{viz}\onedot}
\makeatother


\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfh}{\mathbf{h}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfZ}{\mathbf{Z}}
\newcommand{\tbfz}{\widetilde{\mathbf{z}}}
\newcommand{\tbfZ}{\widetilde{\mathbf{Z}}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\tbfu}{\widetilde{\mathbf{u}}}
\newcommand{\tbfU}{\widetilde{\mathbf{U}}}
\newcommand{\bfQ}{\mathbf{Q}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfP}{\mathbf{P}}
\newcommand{\bfJ}{\mathbf{J}}


\newcommand{\model}{{AbSViT}\xspace} 
\newcommand\minisection[1]{\vspace{1.3mm}\noindent \textbf{#1}}

\definecolor{Gray}{gray}{0.9}





\title{Top-Down Visual Attention from Analysis by Synthesis}

\author{Baifeng Shi\\
UC Berkeley\\
\and
Trevor Darrell\\
UC Berkeley\\
\and
Xin Wang\\
Microsoft Research\\
}
\maketitle
\begin{abstract}


Current attention algorithms (e.g., self-attention) are stimulus-driven and highlight all the salient objects in an image.
However, intelligent agents like humans often guide their attention based on the high-level task at hand, focusing only on task-related objects. 
This ability of task-guided top-down attention provides task-adaptive representation and helps the model generalize to various tasks. 
In this paper, we consider top-down attention from a classic Analysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a functional equivalence between visual attention and sparse reconstruction; we show that an AbS visual system that optimizes a similar sparse reconstruction objective modulated by a goal-directed top-down signal naturally simulates top-down attention. We further propose Analysis-by-Synthesis Vision Transformer (\model), which is a top-down modulated ViT model that variationally approximates AbS, and achieves controllable top-down attention. For real-world applications, \model consistently improves over baselines on Vision-Language tasks such as VQA and zero-shot retrieval where language guides the top-down attention. \model can also serve as a general backbone, improving performance on classification, semantic segmentation, and model robustness.
Project page: \url{https://sites.google.com/view/absvit}.
   
\end{abstract}

\input{1_intro}
\input{2_relwork}
\input{3_prelim}
\input{4_derivation}
\input{5_implementation}
\input{6_exp}
\input{7_limitation}
\input{8_conclusion}

\minisection{Acknowledgement}.
The authors would like to thank Tianyuan Zhang and Amir Bar for their valuable suggestions. Baifeng Shi and Trevor Darrell are supported by DARPA and/or the BAIR Commons program.


{\small
\bibliographystyle{plainnat}
\bibliography{egbib}
}

\newpage
\appendix
\onecolumn
\input{9_appendix}

\end{document}
