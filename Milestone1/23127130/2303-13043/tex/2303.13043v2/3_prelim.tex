\section{Preliminaries: Attention as Sparse Reconstruction}
\label{sec:att_emerge}

\citet{shi2022visual} show that a sparse reconstruction (SR) module functionally resembles visual attention. An SR module takes an input $\bfx \in \bbR^d$ %
and outputs $\bfz = \bfP\tbfu^\ast$ where $\bfP \in \bbR^{d \times d^\prime}$ is the dictionary and $\tbfu^\ast$ is the sparse code, \ie, 
\begin{equation}\small
\label{eq:sr}
    \tbfu^\ast = \argmin_{\tbfu \in \bbR^{d^\prime}}  \frac{1}{2} ||\bfP \tbfu - \bfx||^2_2 + \lambda ||\tbfu||_1.
\end{equation}
Each atom (column) of $\bfP$ contains a template pattern and each element in $\tbfu$ is the activation of the corresponding template. The objective is to reconstruct the input using as few templates as possible. To solve \cref{eq:sr}, one may adopt a first-order optimization \cite{rozell2008sparse,shi2022visual} with dynamics at time $t$ of
\begin{equation}\small
\label{eq:sr_dyn}
    \dv{\bfu}{t} \propto -\bfu - (\bfP^T \bfP - \mathbf{I}) \tbfu  + \bfP^T \bfx,
\end{equation}
where the optimization is over an auxiliary variable $\bfu$ and $\tbfu = \mathit{g}_\lambda (\bfu) = \mathit{sgn}(\bfu) (|\bfu| - \lambda)_+$ with $\mathit{sgn}(\cdot)$ as the sign function and $(\cdot)_+$ as ReLU %
Here $\bfu$ is activated by the template matching $\bfP^T \bfx$ between the dictionary and the input, and different elements in $\bfu$ inhibit each other through $- (\bfP^T \bfP - \mathbf{I}) \tbfu$ to promote sparsity.

To see the connection between visual attention and sparse reconstruction, recall that attention in the human visual system is achieved via two steps~\cite{desimone1995neural}: (i) \textit{grouping} features into separate objects or regions, and (ii) \textit{selecting} the most salient objects or regions while repressing the distracting ones. A similar process is also happening in SR, \ie, if each atom in $\bfP$ is a template of every single object, then each element in $\bfu$ groups the input features belonging to that object through $\bfP^T \bfx$, while the sparsity constraint promoted by the lateral inhibition $- (\bfP^T \bfP - \mathbf{I}) \tbfu$ selects the object that is most activated. As shown in~\cite{shi2022visual}, SR modules achieve similar attention effects as self-attention (SA)~\cite{vaswani2017attention} while being more robust against image corruptions.

Interestingly, it is also pointed out in~\cite{shi2022visual} that under certain constraints (\eg, the key and query transform is the same), SA can be viewed as solving a similar SR problem but without sparsity. After adding the sparsity back, SA is an approximation of 

\vspace{-0.5em}
\begin{footnotesize}
\begin{empheq}[left=\empheqlbrace]{align}
\label{eq:sa_sr}
    \tbfU^\ast &= \argmin_{\tbfU} \frac{1}{2} ||\Phi(\bfK)\tbfU - \bfV||_2^2 + \lambda||\tbfU||_1, \\
    \bfZ &= \Phi(\bfQ)\tbfU^\ast,
\end{empheq}
\end{footnotesize}\noindent
where $\bfQ, \bfK, \bfV \in \bbR^{(hw)\times c}$ are the query, key, and value matrices, $\Phi(\bfQ), \Phi(\bfK) \in \bbR^{(hw) \times d^\prime}$ are the random features~\cite{choromanski2020rethinking} that approximate the softmax kernel $\Phi(\bfQ)_i \Phi(\bfK)_j^T \approx e^{\bfQ_i \bfK_j^T}$, $\tbfU^\ast \in \bbR^{d^\prime \times c}$ is the sparse code and $\bfZ$ is the output. This provides a novel perspective on the mechanism of SA, \ie, it is solving a channel-wise sparse reconstruction of the value matrix $\bfV$ using an input-dependent dictionary $\Phi(\bfK)$. Visualization of $\Phi(\bfK)$ shows each atom contains a mask for one single object or region, which means that SA is trying to reconstruct the input with as few masks as possible, thus only the salient objects are selected and highlighted (\cref{fig:dict} (a)).