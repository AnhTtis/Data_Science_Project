
































\section{Derivation of Eq. (10)}


From Eq. (6-7) we have
\begin{equation}\small
    p(\tbfu_\ell | \tbfu_{\ell+1}) \propto \exp\{-\frac{1}{2}||\bfP_\ell \tbfu_\ell - \mathit{g}_\ell(\bfP_{\ell+1} \tbfu_{\ell+1})||_2^2 - \lambda||\tbfu_\ell||_1\}.
\end{equation}
Then Eq. (10) is derived by
\begin{equation}\small
\begin{split}
    \dv{\tbfu_\ell}{t} &\propto \nabla_{\tbfu_\ell} \log p(\tbfu_{\ell-1} | \tbfu\ell) + \nabla_{\tbfu_\ell} \log p(\tbfu_\ell | \tbfu_{\ell-1}) \\
    &= -\nabla_{\tbfu_\ell}\frac{1}{2} ||\bfP_{\ell-1} \tbfu_{\ell-1} - \mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)||_2^2 -\nabla_{\tbfu_\ell}\frac{1}{2} ||\bfP_\ell \tbfu_\ell - \mathit{g}_\ell (\bfP_{\ell+1} \tbfu_{\ell+1})||_2^2 - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1 \\
    &= \bfP_\ell^T \bfJ_{\ell-1}^T \left(\bfP_{\ell-1} \tbfu_{\ell-1} - \mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)\right) - \bfP_\ell^T \left(\bfP_\ell \tbfu_\ell - \mathit{g}_\ell (\bfP_{\ell+1} \tbfu_{\ell+1}) \right) - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1\\
    &= - \bfP_\ell^T \left(\bfP_\ell \tbfu_\ell - \mathit{g}_\ell (\bfP_{\ell+1} \tbfu_{\ell+1}) - \bfJ_{\ell-1}^T \bfP_{\ell-1} \tbfu_{\ell-1} \right) - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1 - \bfP_\ell^T \bfJ_{\ell-1}^T \mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)\\
    &= -\nabla_{\tbfu_\ell}\frac{1}{2} ||\bfP_\ell \tbfu_\ell - \mathit{g}_\ell (\bfz_{\ell+1}) - \bfJ_{\ell-1}^T \bfz_{\ell-1}||_2^2 - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1 - \nabla_{\tbfu_\ell} \frac{1}{2} ||\mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)||^2_2 \\
    &= -\nabla_{\tbfu_\ell}\frac{1}{2} ||\bfP_\ell \tbfu_\ell - (\bfx_\ell^{td} + \bfx_\ell^{bu})||_2^2 - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1 - \nabla_{\tbfu_\ell} \frac{1}{2} ||\mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)||^2_2.
\end{split}
\end{equation}
We informally use $\nabla$ for subgradients as well.


\section{Additional Results on Natural Images}

In Fig.\ref{fig:spatial_bistable}-\ref{fig:att_compare}, we show examples of top-down attention on artificial images. Here we show more results on natural images containing multiple objects. We borrow the LVIS dataset and collect images that contain object categories that also appear in ImageNet. We demonstrate that given different prior, \model is able to focus on different objects in the same image (\cref{fig:natural_spatial_bistable}). We also compare \model's top-down attention with several baseline methods (\cref{fig:natural_compare}) and observe that \model has cleaner attention maps than other methods.

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=0.45\columnwidth]{figures/spatial_bistable_real.png}}
\caption{Visualization of top-down attention on natural images. From left to right, we show the original images, the bottom-up attention, as well as the top-down attention regarding to different objects in each image.}
\label{fig:natural_spatial_bistable}
\end{center}
\vskip -0.3in
\end{figure}

\begin{figure}[H]
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figures/att_compare_real.png}}
\caption{Comparison of top-down attention map between \model and different baselines. }
\label{fig:natural_compare}
\end{center}
\end{figure}



\section{Additional Implementation Details}

\minisection{ImageNet Pretraining}. The ViT and RVT baselines as well as our \model model are trained using the recipe in \cite{mao2022towards}, and FAN is trained using the recipe in its original paper~\cite{zhou2022understanding}. Specifically, we use AdamW optimizer to train \model for 300 epochs, with a batch size of 512, a base learning rate of 5e-4, and 5 warm-up epochs. One may use different batch-size and adjust the learning rate by the linear scaling rule. We use a cosine learning rate scheduling and weight decay of 0.05. We use the default setting of data augmentation, which includes Mixup, Cutmix, ColorJittering, AutoAugmentation, and Random Erasing. For \model, the weights of supervised loss and variational loss are set as 1 and 0.1.

\minisection{Robustness against Image Corruptions}. We evaluate model robustness against image corruption on ImageNet-C, which contains a total of 19 corruption types. We follow \cite{mao2022towards} and evaluate 15 types of corruption including Brightness, Contrast, Defocus Blur, Elastic Transform, Fog, Frost, Gaussian Noise, Glass Blur, Impulse Noise, JPEG Compression, Motion Blur, Pixelate, Shot Noise, Snow, and Zoom Blur. Note that other work (e.g. \cite{zhou2022understanding}) tests on a different subset of corruption types. To make a fair comparison, all the models are tested under the aforementioned 15 corruption types.

\minisection{Semantic Segmentation}. We use MMSegmentation~\cite{mmseg2020} as our test bed. We take the ImageNet pretrained ViT-B and \model-B and finetune them on semantic segmentation on PASCAL VOC, Cityscapes, and ADE20K. For all the experiments, we use UperNet~\cite{xiao2018unified} as the decoder head and FCNHead as the auxiliary head. We train on 2 GPUs with a total batch size of 16, using AdamW optimizer, a learning rate of $0.00006$, and weight decay of $0.01$. We train for 20k, 40k, and 160k iterations for three datasets, respectively. We use image resolution of 512x512 for PASCAL VOC and ADE20K, and 512x1024 for Cityscapes.

\minisection{V\&L Finetuning}. Following \cite{dou2022empirical}, the whole model contains a pretrained visual encoder, a pretrained text encoder, and a multimodal encoder to merge vision and language. We use the ImageNet pretrained ViT or \model for the visual encoder, a pretrained RoBERTa for the text encoder, and the multimodal encoder is trained from scratch.  We use a learning rate of $1e-5$ for visual and text encoders and $5e-5$ for the multimodal encoder. For top-down attention, we use the \texttt{[cls]} token as the prior $\xi$. Since the text and visual tokens are not aligned initially, we train a linear transform to project the text tokens into the same space as the visual tokens. This is trained by the prior loss, which is set as a CLIP-style loss (\cref{eq:clip_loss}) to align the text and visual tokens.








