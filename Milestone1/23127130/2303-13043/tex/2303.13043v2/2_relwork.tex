\section{Related Work}
\label{sec:relwork}


\minisection{Top-down visual attention}
endows us with the crucial ability to selectively collect information related to the behavioral goal. Several attempts have been made towards understanding the mechanism of top-down attention from experimental observations such as multiplicative tuning~\cite{mcadams1999effects} and contrast responses~\cite{martinez2002attentional,reynolds2009normalization} in V4, and extra-classical receptive fields in V1~\cite{angelucci2002circuits,sceniak2001visual,cavanaugh2002nature}. Other work tries to build a principled computational model for top-down attention~\cite{yu2004inference,chikkerur2010and,mirza2019introducing}.

Top-down attention has also found numerous applications in computer vision tasks where additional guidance (\eg, language) is available aside from the image. Previous work employs top-down attention for object detection~\cite{oliva2003top}, image captioning~\cite{xu2015show}, and visual question answering~\cite{xu2016ask,anderson2018bottom}. However, these algorithms are either incompatible with current self-attention-based models or show inferior performance, as indicated by our experiments. Other work ~\cite{lu2017knowing,lu2016hierarchical,yang2016stacked,dosovitskiy2020image} uses a feedforward model that takes both image and the high-level guidance (\eg, text tokens or \texttt{[cls]} token) as input, which we show is suboptimal compared to our top-down model design. 
Dou \etal~\cite{dou2022empirical}  propose to extract image and text features with separate encoders and combine them with a multi-modal fusion module during vision-language pretraining, which works better than using a single multi-modal feedforward model on vision language tasks. However, in this way, the visual encoder is still bottom-up. We show that augmenting it with the proposed top-down attention further improves model performance on standard benchmarks. 


\minisection{Top-down attention explained as Analysis by Synthesis}.
Analysis by Synthesis (AbS) is hypothesized as a potential computational model behind top-down attention. \citet{lee2002top} starts from a Bayesian inference perspective and explains the top-down modulation in examples such as illusionary contours and shapes from shading. \citet{yu2004inference} focus on the top-down attention in Ponser's task~\cite{posner1980orienting} and build a hierarchical model where each layer corresponds to a computational step of Bayesian inference. Subsequent work~\cite{rao2005bayesian,chikkerur2010and} assumes each object is generated by an appearance variable and a location variable and uses Bayesian inference to perform spatial attention and feature attention. \citet{borji2012object} adopt a Dynamic Bayesian Network to simulate eye fixation in top-down attention. However, these models do not apply to practical designs in modern deep learning. 


\minisection{Generative model for discriminative learning}.
It has been widely explored in using generative models to assist discriminative learning. Specifically, the belief that representation with strong generative capability can better capture the structure of visual signals has inspired numerous unsupervised learning algorithms, from the early Restricted Boltzmann Machine~\cite{hinton2006fast,hinton2006reducing} and Helmholtz Machine~\cite{dayan1995helmholtz}, to the following auto-encoder models such as DAE~\cite{vincent2010stacked} and VAE~\cite{kingma2013auto}. Recent work~\cite{he2022masked,tong2022unsupervised} has shown impressive results on generative unsupervised learning. Generative models can also help with supervised learning, \eg, by refining object detection~\cite{lin2017feature} or detecting errors in semantic segmentation~\cite{xia2020synthesize}. Feedforward models with generative feedback are also more robust to input corruptions~\cite{huang2020neural}. In our work, \model also contains a generative feedback path that is able to refine the intermediate representation and attention and thus improves the performance.


