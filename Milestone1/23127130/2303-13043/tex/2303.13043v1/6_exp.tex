\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.85\columnwidth]{figures/spatial_bistable.pdf}}
\caption{Controllable top-down attention in multi-object images. For each image, bottom-up attention will highlight both objects. In contrast, we can use different class prototypes as the prior to control the top-down attention to focus on different objects, and the classification result also changes accordingly.}
\label{fig:spatial_bistable}
\end{center}
\vskip -0.3in
\end{figure}

\section{Experiments}
\label{sec:exp}

In this section, we first show that \model achieves controllable top-down attention in multi-object scenes (\cref{sec:exp_td_att}). Then we test \model on Vision-Language tasks such as VQA and zero-shot image retrieval (\cref{sec:exp_vl}), and also on ImageNet classification and model robustness (\cref{sec:exp_imagenet}), as well as semantic segmentation (\cref{sec:exp_segmentation}). Finally, we analyze specific designs of \model in \cref{sec:exp_model_design}.

\minisection{Datasets}. For VQA, we use VQAv2~\cite{goyal2017making} for training and testing and compare the attention map with human attention collected by VQA-HAT~\cite{das2017human}. For zero-shot image retrieval, we use Flickr30K~\cite{plummer2015flickr30k}. For image classification, we train and test on ImageNet-1K (IN)~\cite{deng2009imagenet}, and also test on corrupted images from IN-C~\cite{hendrycks2019benchmarking}, adversarial images from IN-A~\cite{hendrycks2021natural}, and out-of-distribution images from IN-R~\cite{hendrycks2021many} and IN-SK~\cite{wang2019learning}. For semantic segmentation, we test on PASCAL VOC~\cite{pascal-voc-2012}, Cityscapes~\cite{Cordts2016Cityscapes}, and ADE20K~\cite{zhou2017scene}.

\minisection{Experimental setup}. We compare several baselines for goal-directed attention: (i) \textbf{PerceiverIO}~\cite{jaegle2021perceiver} uses $e_\xi(\cdot)$ to reweight the tokens from feedforward output just like in \model, but directly outputs the reweighted tokens without any feedback, (ii) \textbf{MaskAtt} uses the same soft mask for reweighting the output tokens to reweight the value tokens in intermediate self-attention modules, instead of adding the top-down tokens on them, (iii) \textbf{Feedback} directly feeds back the output tokens without reweighting. For V\&L tasks, we use the METER~\cite{dou2022empirical} framework, which contains a vision backbone, a language backbone, and a multimodal fusion module. We use ViT~\cite{dosovitskiy2020image} as the vision backbone and replace it with \model or the baseline models. For image classification, we try the backbones of ViT, RVT~\cite{mao2022towards}, and FAN~\cite{zhou2022understanding}, which is state of the art on ImageNet and robustness benchmarks. The scaling factor $\alpha$ is set as $1$ during ImageNet pretraining and evaluation and set as $10$ for finetuning on V\&L tasks because we find \model pretrained on supervised single-object classification only learns weak top-down attention in multi-object scenes (\cref{sec:lim_weak_td}). See the Appendix for additional implementation details.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Controllable Top-Down Attention of \model}
\label{sec:exp_td_att}

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.85\columnwidth]{figures/att_compare.pdf}}
\caption{Comparison between different top-down attention algorithms. Prior corresponds to the left image. \model has cleaner attention map than other baselines.}
\label{fig:att_compare}
\end{center}
\vskip -0.3in
\end{figure}

To test the top-down attention in multi-object images, we take a \model pretrained on ImageNet (\cref{sec:exp_imagenet}) and create multi-object images by randomly sampling two images from ImageNet and concatenating them side by side. To control the top-down attention, we use the class prototype (from the last linear layer) of the two classes as $\xi$. Since in regular ViT, the class prototypes only align with the \texttt{[cls]} token but not with other output tokens, here we use a ViT with global average pooling. We set $\alpha=10$.

To compare the bottom-up and top-down attention, we visualize the norm of output tokens from ViT and \model for each class. As shown in \cref{fig:spatial_bistable}, bottom-up attention highlights both objects while only the target object is selected by top-down attention. Consequently, the classification result, which has a tie between two classes when no prior is available, is biased towards the target class when we turn on the prior. This indicates \model has the ability to control its attention on different objects given different priors. We also compare the top-down attention of \model with several baselines (\cref{fig:att_compare}). We can see that the attention of PerceiverIO focuses coarsely on the target object but is noisy, possibly because it lacks a feedback mechanism. MaskAtt, on the other hand, tends to miss parts of the object, implying that masking attention is less suitable for ViTs.



%Note that in this experiment we use a large scaling factor of $\alpha=5$ (and $\alpha=1$ for other experiments) for the top-down signal to get a clearer top-down attention. In fact, as shown in Appendix, the top-down bias is weak when $\alpha=1$, and gets stronger as we use larger $\alpha$. The weak top-down attention when $\alpha=1$ is probably because \model is only trained for image-level recognition under a supervised setting, and we may get a better top-down attention if we use an unsupervised learning algorithm for object/region-level recognition~\cite{henaff2021efficient,henaff2022object,wen2022self}. We leave this for future exploration.
%It is also noticeable that the bottom-up attention is still noisy while the top-down mechanism can further refine the attention map, which is consistent to our observation in \cref{sec:exp_imagenet}.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\model for Vision-Language Tasks}
\label{sec:exp_vl}

\begin{table}[t]
  \centering
  \begin{footnotesize}
  \begin{tabular}{@{}lccccc@{}}
    {\multirow{2}{*}{Model}} & \multicolumn{2}{c}{VQAv2} & \multicolumn{3}{c}{Flickr-Zero-Shot} \\
     &  test-dev & test-std & IR@1 & IR@5 & IR@10 \\
    \midrule    
    BEiT-B-16~\cite{bao2021beit} & 68.45 & - & 32.24 & - & - \\
    CLIP-B-32~\cite{radford2021learning} & 69.69 & - & 49.86 & - & - \\
    \midrule
    ViT-B & 67.89 & 67.92 & 42.40 & 77.18 & 86.82 \\
    - PerceiverIO & 67.87 & 67.93 & 42.52 & 76.92 & 86.73 \\
    - Feedback & 67.99 & 68.13 & 42.04 & 77.38 & 86.90 \\
    - MaskAtt & 67.53 & 67.51 & 41.89 & 76.53 & 86.78 \\
    - \model & \textbf{68.72} & \textbf{68.78} & \textbf{45.28} & \textbf{77.98} & \textbf{87.52} \\
\bottomrule
  \end{tabular}
  \caption{Comparison of different top-down attention algorithms on VQA and zero-shot image retrieval. \model achieves consistent improvements on both tasks.}
  \label{tab:vl}
  \end{footnotesize}
\end{table}


\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{figures/vqa_hat.pdf}}
\caption{Comparison of attention map from \model and human attention on VQA. \model's attention is adjustable to different questions and is consistent with human attention. }
\label{fig:vqa_hat}
\end{center}
\vskip -0.3in
\end{figure}


We test AbSViT on two V\&L tasks, VQA, and zero-shot image retrieval. We use the METER framework and replace the vision backbone with ViT-B, AbSViT-B, and other baselines. All the vision backbones are pretrained on ImageNet (\cref{sec:exp_imagenet}). Results are shown in \cref{tab:vl}. 

On VQAv2, \model surpasses the baselines on both test splits and reaches the same performance as the unsupervised model (BEiT-B). At the same time, PerceiverIO has no improvement over ViT, probably because the multimodal fusion in METER can already perform token reweighting. The pure feedback network helps a little, mainly due to the feature refinement during the feedback loop. It is worth noticing that MaskAtt, a strategy frequently used in previous work, actually hurts performance when added to the vision transformer. On zero-shot image retrieval, \model also has higher performance than all other baselines. Especially, it has an improvement of $\sim3\%$ over bottom-up ViT on IR@1.

We also visualize the attention map of \model on VQA and compare it to human attention. As shown in \cref{fig:vqa_hat}, \model can adjust its attention to the objects related to the question. The attention map is also consistent with human attention.%\footnote{Note that the human attention here is oversmoothed due to the data collection strategy in VQA-HAT.} 
Nevertheless, the attention map of \model is still not precise enough. For example, in the last example, when the question is ``What is the person holding?'', the top-down attention highlights both the person and the dogs. Since the model is only pretrained on ImageNet, it may be further improved by CLIP~\cite{radford2021learning} pretraining. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Image Classification and Robustness}
\label{sec:exp_imagenet}


\begin{table}[t]
  \centering
  \begin{scriptsize}
  \begin{tabular}{lcccccc}
    Model & P/F & Clean & IN-C ($\downarrow$) & IN-A & IN-SK & IN-R \\
    \midrule
    
    PiT-Ti~\cite{heo2021rethinking} & 5/0.7 & 72.9 & 69.1 & 6.2 & 34.6 & 21.6 \\
    ConViT-Ti~\cite{d2021convit} & 6/1.4 & 73.3 & 68.4 & 8.9 & 35.2 & 22.4 \\
    PVT-Ti~\cite{wang2021pyramid} & 13/1.9 & 75.0 & 79.6 & 7.9 & 33.9 & 21.5 \\
    GFNet-Ti~\cite{rao2021global} & 8/1.3 & 74.6 & 65.9 & 6.3 & 40.4 & 27.0 \\
    \midrule
    ViT-Ti~\cite{dosovitskiy2020image} & 6/1.3 & 72.5 & 71.1 & 7.5 & 33.0 & 20.1 \\
    \rowcolor{Gray}
    - AbS & 7/2.6 & \textbf{74.1} & \textbf{66.7} & \textbf{10.1} & \textbf{34.9} & \textbf{22.6} \\
    \midrule
    RVT-Ti~\cite{mao2022towards} & 9/1.3 & 78.1 & 58.8 & 13.9 & 42.5 & 29.1 \\
    \rowcolor{Gray}
    - AbS & 11/2.7 & \textbf{78.6} & \textbf{55.9} & \textbf{17.3} & \textbf{43.2} & \textbf{29.9} \\
    \midrule
    FAN-Ti~\cite{zhou2022understanding} & 7/1.3 & 77.5 & 59.8 & 13.1 & 42.6 & 29.9 \\
    \rowcolor{Gray}
    - AbS & 9/2.9 & \textbf{78.3} & \textbf{57.4} & \textbf{16.5} & \textbf{42.8} & \textbf{31.2} \\
    \midrule

    PiT-S~\cite{heo2021rethinking} & 24/2.9 & 80.9 & 52.5 & 21.7 & 43.6 & 30.8 \\
    PVT-S~\cite{wang2021pyramid} & 25/3.8 & 79.9 & 66.9 & 18.0 & 40.1 & 27.2 \\
    Swin-T~\cite{liu2021swin} & 28/4.5 & 81.2 & 62.0 & 21.6 & 41.3 & 29.1 \\
    ConvNext-T~\cite{liu2022convnet} & 29/4.5 & 82.1 & 53.2 & 24.2 & 47.2 & 33.8 \\
    \midrule
    ViT-S~\cite{dosovitskiy2020image} & 22/4.2 & 80.1 & 54.6 & 19.2 & 41.9 & 28.9 \\
    \rowcolor{Gray}
    - AbS & 26/9.8 & \textbf{80.7} & \textbf{51.6} & \textbf{24.3} & \textbf{43.1} & \textbf{30.2} \\
    \midrule
    RVT-S~\cite{mao2022towards} & 22/4.3 & 81.9 & 50.5 & 26.0 & 47.0 & 34.5 \\
    \rowcolor{Gray}
    - AbS & 26/10.4 & 81.9 & \textbf{48.7} & \textbf{31.1} & \textbf{48.5} & \textbf{35.6} \\
    \midrule
    FAN-S~\cite{zhou2022understanding} & 28/5.3 & 82.8 & 49.1 & 29.3 & 47.4 & 35.6 \\
    \rowcolor{Gray}
    - AbS & 32/11.4 & \textbf{83.0} & \textbf{47.4} & \textbf{34.0} & \textbf{48.3} & \textbf{36.4}\\
    \midrule

    PiT-B~\cite{heo2021rethinking} & 74/12.5 & 82.4 & 48.2 & 33.9 & 43.7 & 32.3 \\
    PVT-L~\cite{wang2021pyramid} & 61/9.8 & 81.7 & 59.8 & 26.6 & 42.7 & 30.2 \\
    Swin-B~\cite{liu2021swin} & 88/15.4 & 83.4 & 54.4 & 35.8 & 46.6 & 32.4 \\
    ConvNext-B~\cite{liu2022convnet} & 89/15.4 & 83.8 & 46.8 & 36.7 & 51.3 & 38.2 \\
    \midrule
    ViT-B~\cite{dosovitskiy2020image} & 87/17.2 & 80.8 & 49.3 & 25.2 & \textbf{43.3} & 31.6 \\
    \rowcolor{Gray}
    - AbS & 99/38.9 & \textbf{81.0} & \textbf{48.3} & \textbf{28.2} & 42.9 & 31.7 \\
    \midrule
    RVT-B~\cite{mao2022towards} & 86/17.7 & 80.9 & 52.1 & 26.6 & \textbf{39.6} & 26.1 \\
    \rowcolor{Gray}
    - AbS & 100/39.5 & 80.9 & \textbf{51.7} & \textbf{28.5} & 39.3 & 26.0 \\
    \midrule
    FAN-B~\cite{zhou2022understanding} & 54/10.4 & 83.5 & 45.0 & 33.2 & 51.4 & 39.3 \\
    \rowcolor{Gray}
    - AbS & 62/21.8 & \textbf{83.7} & \textbf{44.1} & \textbf{38.4} & \textbf{52.0} & \textbf{39.8} \\
    
    \bottomrule
  \end{tabular}
  \caption{Results on ImageNet classification and robustness benchmarks. \model improves performance across different benchmarks and backbones. P/F: \# of parameters and FLOPs. $\downarrow$: lower is better.}
  \label{tab:imagenet}
  \end{scriptsize}
\end{table}


\begin{table}[t]
  \centering
  \begin{footnotesize}
  \begin{tabular}{@{}lccccc@{}}
    Model & Clean & IN-C ($\downarrow$) & IN-A & IN-R & IN-SK \\
    \midrule    
    ViT-Ti & 72.5 & 71.1 & 7.5 & 33.0 & 20.1 \\
    - PerceiverIO & 72.8 & 70.4 & 8.0 & 32.8 & 20.5 \\
    - Feedback & 73.4 & 67.8 & 9.7 & 34.6 & 22.4 \\
    - MaskAtt & 72.5 & 70.6 & 8.3 & 33.4 & 20.5 \\
    - AbS & \textbf{74.1} & \textbf{66.7} & \textbf{10.1} & \textbf{34.9} & \textbf{22.6} \\   
    \midrule
    RVT-Ti & 78.1 & 58.8 & 13.9 & 42.5 & 29.1 \\
    - PerceiverIO & 78.3 & 57.8 & 13.7 & 42.8 & 29.8 \\
    - Feedback & 79.1 & 55.7 & 18.2 & 44.1 & 31.3 \\
    - MaskAtt & 77.9 & 59.0 & 13.5 & 43.0 & 29.7 \\
    - AbS & \textbf{79.5} & \textbf{54.8} & \textbf{18.7} & \textbf{44.5} & \textbf{32.5} \\
\bottomrule
  \end{tabular}
  \caption{Comparison of different top-down attention algorithms on ImageNet classification and robustness.}
  \label{tab:ablation_att}
  \end{footnotesize}
  % \vskip -0.2in
\end{table}



\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{figures/image_classification.pdf}}
\caption{Visualization of the bottom-up attention, token weights, and the top-down attention in \model. The bottom-up attention is noisy and fails to detect the complete foreground object. In \model, the query mask can coarsely detect the foreground object and reweight tokens fed back to direct the top-down attention to better extract the foreground object. }
\label{fig:image_classification}
\end{center}
\vskip -0.3in
\end{figure}


We test \model on ImageNet classification and robustness benchmarks (\cref{tab:imagenet}). %We apply our method on three backbones (ViT, RVT, and FAN) and across three model scales (Tiny, Small, and Base). 
We report mCE (lower the better)~\cite{hendrycks2019benchmarking} for IN-C and accuracy for other datasets. On clean images, AbSViT consistently improves over baselines, with a similar number of parameters although higher FLOPs. The clean accuracy on FAN-B is improved to 83.7\%, reaching the same level as ConvNext-B with fewer parameters. On corrupted (IN-C) and adversarial (IN-A) images, \model boosts the performance by about $1$-$5\%$ across all the scales. Especially, the performance on FAN-B is raised by $1\%$ and $5\%$ for IN-C and IN-A, reaching a new state-of-the-art result. On out-of-distribution images, \model also improves by $3\%$ on Tiny and Small models and $0.5\%$ on FAN-B.

\cref{fig:image_classification} visualizes the attention map of ViT and \model, as well as token weights generated in $e_\xi(\cdot)$. The bottom-up attention in ViT is often noisy and only partly detects the foreground object. On the other hand, the query $\xi$ in \model learns to coarsely detect the foreground and reweight the feedforward output tokens, which are fed back and generate top-down attention that better detects the foreground object.

We compare \model with several baseline algorithms for goal-directed attention in \cref{tab:ablation_att}. One may see that a pure feedback model already improves the clean accuracy and robustness, and \model further boosts the performance by better extracting the foreground object. Due to a similar reason, PerceiverIO without feedback also slightly improves the performance. On the other hand, MaskAtt is sometimes harmful (on Clean, IN-C, and IN-A for RVT), implying that a mask attention design is unsuitable for vision transformers. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Semantic Segmentation}
\label{sec:exp_segmentation}

\begin{table}[t]
  \centering
  \begin{footnotesize}
  \begin{tabular}{@{}lccc@{}}
    Model & PASCAL VOC & Cityscapes & ADE20K \\
    \midrule
    ResNet-101~\cite{mmseg2020} & 77.1 & \textbf{78.7} & 42.9 \\
    \midrule    
     ViT-B & 80.1 & 75.3 & 45.2 \\
     AbSViT-B & \textbf{81.3} \scriptsize(\textcolor{LimeGreen}{+1.2}) & 76.8 \scriptsize(\textcolor{LimeGreen}{+1.5}) &  \textbf{47.2} \scriptsize(\textcolor{LimeGreen}{+2.0}) \\
     \bottomrule
  \end{tabular}
  \caption{Semantic segmentation results on three datasets.}
  \label{tab:seg}
  \end{footnotesize}
  \vskip -0.1in
\end{table}

We evaluate the performance of \model as a backbone for semantic segmentation on three datasets (PASCAL VOC, Cityscapes, and ADE20K). We compare with two baseline backbones, regular ViT and ResNet-101. We use UperNet~\cite{xiao2018unified} as the segmentation head for all the backbones. Results are shown in \cref{tab:seg}. We can see that when using \model as the backbone, we can achieve $1.2$-$2.0\%$ improvements over the ViT baseline with approximately the same number of parameters. This indicates that \model can be used as a general backbone for different vision tasks.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Justification of Model Design}
\label{sec:exp_model_design}





The design of \model follows the principle of AbS. For example, \model adds the top-down signal only to the value matrix considering the analogy between self-attention and sparse reconstruction (\cref{sec:design}). At the same time, an arbitrary design may also add it to the query and key. We also optimize the variational loss to approximate AbS instead of just building a top-down model and training with the supervised loss. In this section, we show the advantage of these ``destined'' designs compared with an arbitrary design, which also justifies the proposed guiding principle of AbS.

We first try an arbitrary design of self-attention with top-down input by adding the top-down signal on the query, key, and value instead of only on the value. We name this design as \model-QKV. We compare \model and \model-QKV on image classification and robustness (\cref{tab:ablation_qkv}), and we can see that \model is superior to \model-QKV on every benchmark. This is consistent with our analysis in \cref{sec:td_from_abs} that the sparse reconstruction AbS is optimizing has an additional top-down input (corresponding to V), while the dictionary (corresponding to Q and K), which contains templates for separate objects, is fixed. 

We also test the effect of the variational loss $\mathcal{L}_{var}$, which ensures the model is approximating AbS. We compare \model with its counterpart without $\mathcal{L}_{var}$, \ie, a top-down model trained with only supervised loss. As shown in \cref{tab:ablation_var}, adding $\mathcal{L}_{var}$ largely improves the clean accuracy and robustness. Note that, as discussed in \cref{sec:design}, we do not have a prior loss $-\log p(\bfz_L)$ for image classification, which means the improvement completely comes from the reconstruction loss $\frac{1}{2} \sum_{\ell=1}^L ||\mathit{sg}(\bfz_\ell) - \mathit{g}_\ell(\mathit{sg}(\bfz_{\ell+1}))||_2^2$ which forces the decoder to reconstruct $\bfz_\ell$ from $\bfz_{\ell+1}$. This implies that a generative model (``synthesis'') is important to high-quality top-down attention in visual recognition (``analysis'').


\begin{table}[t]
  \centering
  \begin{footnotesize}
  \begin{tabular}{@{}lccccc@{}}
    Model & Clean & IN-C ($\downarrow$) & IN-A & IN-R & IN-SK \\
    \midrule    
    \model-QKV & 73.3 & 68.0 & 9.4 & 33.8 & 21.2 \\
    \model & \textbf{74.1} & \textbf{66.7} & \textbf{10.1} & \textbf{34.9} & \textbf{22.6} \\   
\bottomrule
  \end{tabular}
  \caption{The predicted design of top-down self-attention (\model) is better than an arbitrary design (\model-QKV).}
  \label{tab:ablation_qkv}
  \end{footnotesize}
\end{table}


\begin{table}[t]
  \centering
  \begin{footnotesize}
  \begin{tabular}{@{}lcccccc@{}}
     & $\mathcal{L}_{var}$ & Clean & IN-C ($\downarrow$) & IN-A & IN-R & IN-SK \\
    \midrule    
    \model & \xmark & 73.1 & 69.0 & 9.5 & 33.5 & 20.8 \\
    \model & \cmark & \textbf{74.1} & \textbf{66.7} & \textbf{10.1} & \textbf{34.9} & \textbf{22.6} \\   
\bottomrule
  \end{tabular}
  \caption{Ablation on the variational loss $\mathcal{L}_{var}$.}
  \label{tab:ablation_var}
  \end{footnotesize}
  \vskip -0.1in
\end{table}