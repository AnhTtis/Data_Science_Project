% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{xspace}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[dvipsnames]{xcolor}
\usepackage{diagbox}
\usepackage{pifont}
\usepackage{svg}
\usepackage{amsmath}
\usepackage[numbers]{natbib}
\usepackage{enumitem}
\usepackage{physics}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{gensymb}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{footmisc}
\usepackage{empheq}
\usepackage{cases}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{tabu}
\usepackage{flushend}
\usepackage{multirow}
\usepackage{bm}
\usepackage{color, colortbl}


\newcommand\todo[1]{\textcolor{Peach}{[TODO: #1]}}
\newcommand\xin[1]{\textcolor{blue}{[xin: #1]}}
\newcommand\baifeng[1]{\textcolor{OliveGreen}{[baifeng: #1]}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
% \crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
% \crefname{table}{Table}{Tables}
% \crefname{figure}{Figure}{Figures}
% \crefname{equation}{Eq.}{Eqs.}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{233} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% definition of common abbreviation

% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g.}\xspace} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e.}\xspace} \def\Ie{\emph{I.e}\onedot}
\def\vs{\emph{vs.}\xspace}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\def\viz{\emph{viz}\onedot}
\makeatother

% definition of tick and cross

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfh}{\mathbf{h}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfZ}{\mathbf{Z}}
\newcommand{\tbfz}{\widetilde{\mathbf{z}}}
\newcommand{\tbfZ}{\widetilde{\mathbf{Z}}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\tbfu}{\widetilde{\mathbf{u}}}
\newcommand{\tbfU}{\widetilde{\mathbf{U}}}
\newcommand{\bfQ}{\mathbf{Q}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfP}{\mathbf{P}}
\newcommand{\bfJ}{\mathbf{J}}


\newcommand{\model}{{AbSViT}\xspace} 
\newcommand\minisection[1]{\vspace{1.3mm}\noindent \textbf{#1}}

\definecolor{Gray}{gray}{0.9}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%% TITLE - PLEASE UPDATE
\title{Top-Down Visual Attention from Analysis by Synthesis}

\author{Baifeng Shi\\
UC Berkeley\\
% {\tt\small baifeng_shi@berkeleu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Trevor Darrell\\
UC Berkeley\\
% {\tt\small trevor@eecs.berkeley.edu}
\and
Xin Wang\\
Microsoft Research\\
% {\tt\small wanxin@microsoft.com}
}
\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}

% Stimulus-driven bottom-up attention (e.g., self-attention) has been widely adopted in vision transformer models. Top-down attention, which conversely adjusts the attention to different objects or regions given a high-level top-down goal, is appealing but lacks a principled design.

Current attention algorithms (e.g., self-attention) are stimulus-driven and highlight all the salient objects in an image.
However, intelligent agents like humans often guide their attention based on the high-level task at hand, focusing only on task-related objects. 
This ability of task-guided top-down attention provides task-adaptive representation and helps the model generalize to various tasks. 
%Unfortunately, this critical feature is missing in current vision transformers.
In this paper, we consider top-down attention from a classic Analysis-by-Synthesis (AbS) perspective of vision. Prior work indicates a functional equivalence between visual attention and sparse reconstruction; we show that an AbS visual system that optimizes a similar sparse reconstruction objective modulated by a goal-directed top-down signal naturally simulates top-down attention. We further propose Analysis-by-Synthesis Vision Transformer (\model), which is a top-down modulated ViT model that variationally approximates AbS, and achieves controllable top-down attention. For real-world applications, \model consistently improves over baselines on Vision-Language tasks such as VQA and zero-shot retrieval where language guides the top-down attention. \model can also serve as a general backbone, improving performance on classification, semantic segmentation, and model robustness.
Project page: \url{https://sites.google.com/view/absvit}.
   
\end{abstract}

%%%%%%%%% BODY TEXT
\input{1_intro}
\input{2_relwork}
\input{3_prelim}
\input{4_derivation}
\input{5_implementation}
\input{6_exp}
\input{7_limitation}
\input{8_conclusion}

\minisection{Acknowledgement}.
The authors would like to thank Tianyuan Zhang and Amir Bar for their valuable suggestions. Baifeng Shi and Trevor Darrell are supported by DARPA and/or the BAIR Commons program.


%%%%%%%%% REFERENCES
{\small
% \bibliographystyle{ieee_fullname}
\bibliographystyle{plainnat}
\bibliography{egbib}
}

\newpage
\appendix
\onecolumn
\input{9_appendix}

\end{document}
