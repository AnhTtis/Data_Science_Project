% % CVPR 2023 Paper Template
% % based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% % modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

% \documentclass[10pt,twocolumn,letterpaper]{article}

% %%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% % \usepackage{cvpr}              % To produce the CAMERA-READY version
% %\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% % Include other packages here, before hyperref.
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{booktabs}
% \usepackage{microtype}
% \usepackage{xspace}
% \usepackage{amsmath,amsfonts,amssymb,amsthm}
% \usepackage[dvipsnames]{xcolor}
% \usepackage{diagbox}
% \usepackage{pifont}
% \usepackage{svg}
% \usepackage{amsmath}
% \usepackage[numbers]{natbib}
% \usepackage{enumitem}
% \usepackage{physics}
% \usepackage{mathtools}
% \usepackage{algorithm}
% \usepackage{algorithmic}
% \usepackage{gensymb}
% \usepackage{wrapfig}
% \usepackage{subcaption}
% \usepackage{multirow}
% \usepackage{footmisc}
% \usepackage{empheq}
% \usepackage{cases}
% \usepackage{adjustbox}
% \usepackage{array}
% \usepackage{tabu}
% \usepackage{flushend}
% \usepackage{multirow}
% \usepackage{bm}
% \usepackage{color, colortbl}


% \newcommand\todo[1]{\textcolor{Peach}{[TODO: #1]}}
% \newcommand\xin[1]{\textcolor{blue}{[xin: #1]}}
% \newcommand\baifeng[1]{\textcolor{OliveGreen}{[baifeng: #1]}}

% % It is strongly recommended to use hyperref, especially for the review version.
% % hyperref with option pagebackref eases the reviewers' job.
% % Please disable hyperref *only* if you encounter grave issues, e.g. with the
% % file validation for the camera-ready version.
% %
% % If you comment hyperref and then uncomment it, you should delete
% % ReviewTempalte.aux before re-running LaTeX.
% % (Or just hit 'q' on the first LaTeX run, let it finish, and you
% %  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% % Support for easy cross-referencing
% \usepackage[capitalize]{cleveref}
% \crefname{section}{Sec.}{Secs.}
% \Crefname{section}{Section}{Sections}
% \Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}

% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}

% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}


% %%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{233} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}


% \begin{document}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % definition of common abbreviation

% % Add a period to the end of an abbreviation unless there's one
% % already, then \xspace.
% \makeatletter
% \DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
% \def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

% \def\eg{\emph{e.g.}\xspace} \def\Eg{\emph{E.g}\onedot}
% \def\ie{\emph{i.e.}\xspace} \def\Ie{\emph{I.e}\onedot}
% \def\vs{\emph{vs.}\xspace}
% \def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
% \def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
% \def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
% \def\etal{\emph{et al}\onedot}
% \def\viz{\emph{viz}\onedot}
% \makeatother

% % definition of tick and cross

% \newcommand{\cmark}{\ding{51}}%
% \newcommand{\xmark}{\ding{55}}%

% \newcommand{\bbR}{\mathbb{R}}
% \newcommand{\bfx}{\mathbf{x}}
% \newcommand{\bfh}{\mathbf{h}}
% \newcommand{\bfz}{\mathbf{z}}
% \newcommand{\bfZ}{\mathbf{Z}}
% \newcommand{\tbfz}{\widetilde{\mathbf{z}}}
% \newcommand{\tbfZ}{\widetilde{\mathbf{Z}}}
% \newcommand{\bfu}{\mathbf{u}}
% \newcommand{\bfU}{\mathbf{U}}
% \newcommand{\tbfu}{\widetilde{\mathbf{u}}}
% \newcommand{\tbfU}{\widetilde{\mathbf{U}}}
% \newcommand{\bfQ}{\mathbf{Q}}
% \newcommand{\bfK}{\mathbf{K}}
% \newcommand{\bfV}{\mathbf{V}}
% \newcommand{\bfP}{\mathbf{P}}
% \newcommand{\bfJ}{\mathbf{J}}


% \newcommand{\model}{{AbSViT}\xspace} 
% \newcommand\minisection[1]{\vspace{1.3mm}\noindent \textbf{#1}}

% \definecolor{Gray}{gray}{0.9}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% %%%%%%%%% TITLE - PLEASE UPDATE
% \title{Top-Down Visual Attention from Analysis by Synthesis: Appendix}

% \author{Baifeng Shi\\
% UC Berkeley\\
% % {\tt\small baifeng_shi@berkeleu.edu}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Trevor Darrell\\
% UC Berkeley\\
% % {\tt\small trevor@eecs.berkeley.edu}
% \and
% Xin Wang\\
% Microsoft Research\\
% % {\tt\small wanxin@microsoft.com}
% }
% \onecolumn
% \maketitle


%%%%%%%%% BODY TEXT

\section{Derivation of Eq. (10)}


From Eq. (6-7) we have
\begin{equation}\small
    p(\tbfu_\ell | \tbfu_{\ell+1}) \propto \exp\{-\frac{1}{2}||\bfP_\ell \tbfu_\ell - \mathit{g}_\ell(\bfP_{\ell+1} \tbfu_{\ell+1})||_2^2 - \lambda||\tbfu_\ell||_1\}.
\end{equation}
Then Eq. (10) is derived by
\begin{equation}\small
\begin{split}
    \dv{\tbfu_\ell}{t} &\propto \nabla_{\tbfu_\ell} \log p(\tbfu_{\ell-1} | \tbfu\ell) + \nabla_{\tbfu_\ell} \log p(\tbfu_\ell | \tbfu_{\ell-1}) \\
    &= -\nabla_{\tbfu_\ell}\frac{1}{2} ||\bfP_{\ell-1} \tbfu_{\ell-1} - \mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)||_2^2 -\nabla_{\tbfu_\ell}\frac{1}{2} ||\bfP_\ell \tbfu_\ell - \mathit{g}_\ell (\bfP_{\ell+1} \tbfu_{\ell+1})||_2^2 - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1 \\
    &= \bfP_\ell^T \bfJ_{\ell-1}^T \left(\bfP_{\ell-1} \tbfu_{\ell-1} - \mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)\right) - \bfP_\ell^T \left(\bfP_\ell \tbfu_\ell - \mathit{g}_\ell (\bfP_{\ell+1} \tbfu_{\ell+1}) \right) - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1\\
    &= - \bfP_\ell^T \left(\bfP_\ell \tbfu_\ell - \mathit{g}_\ell (\bfP_{\ell+1} \tbfu_{\ell+1}) - \bfJ_{\ell-1}^T \bfP_{\ell-1} \tbfu_{\ell-1} \right) - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1 - \bfP_\ell^T \bfJ_{\ell-1}^T \mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)\\
    &= -\nabla_{\tbfu_\ell}\frac{1}{2} ||\bfP_\ell \tbfu_\ell - \mathit{g}_\ell (\bfz_{\ell+1}) - \bfJ_{\ell-1}^T \bfz_{\ell-1}||_2^2 - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1 - \nabla_{\tbfu_\ell} \frac{1}{2} ||\mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)||^2_2 \\
    &= -\nabla_{\tbfu_\ell}\frac{1}{2} ||\bfP_\ell \tbfu_\ell - (\bfx_\ell^{td} + \bfx_\ell^{bu})||_2^2 - \nabla_{\tbfu_\ell} \lambda ||\tbfu_\ell||_1 - \nabla_{\tbfu_\ell} \frac{1}{2} ||\mathit{g}_{\ell-1} (\bfP_\ell \tbfu_\ell)||^2_2.
\end{split}
\end{equation}
We informally use $\nabla$ for subgradients as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Results on Natural Images}

In Fig.\ref{fig:spatial_bistable}-\ref{fig:att_compare}, we show examples of top-down attention on artificial images. Here we show more results on natural images containing multiple objects. We borrow the LVIS dataset and collect images that contain object categories that also appear in ImageNet. We demonstrate that given different prior, \model is able to focus on different objects in the same image (\cref{fig:natural_spatial_bistable}). We also compare \model's top-down attention with several baseline methods (\cref{fig:natural_compare}) and observe that \model has cleaner attention maps than other methods.

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=0.45\columnwidth]{figures/spatial_bistable_real.png}}
\caption{Visualization of top-down attention on natural images. From left to right, we show the original images, the bottom-up attention, as well as the top-down attention regarding to different objects in each image.}
\label{fig:natural_spatial_bistable}
\end{center}
\vskip -0.3in
\end{figure}

\begin{figure}[H]
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{figures/att_compare_real.png}}
\caption{Comparison of top-down attention map between \model and different baselines. }
\label{fig:natural_compare}
\end{center}
% \vskip -0.3in
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Implementation Details}

\minisection{ImageNet Pretraining}. The ViT and RVT baselines as well as our \model model are trained using the recipe in \cite{mao2022towards}, and FAN is trained using the recipe in its original paper~\cite{zhou2022understanding}. Specifically, we use AdamW optimizer to train \model for 300 epochs, with a batch size of 512, a base learning rate of 5e-4, and 5 warm-up epochs. One may use different batch-size and adjust the learning rate by the linear scaling rule. We use a cosine learning rate scheduling and weight decay of 0.05. We use the default setting of data augmentation, which includes Mixup, Cutmix, ColorJittering, AutoAugmentation, and Random Erasing. For \model, the weights of supervised loss and variational loss are set as 1 and 0.1.

\minisection{Robustness against Image Corruptions}. We evaluate model robustness against image corruption on ImageNet-C, which contains a total of 19 corruption types. We follow \cite{mao2022towards} and evaluate 15 types of corruption including Brightness, Contrast, Defocus Blur, Elastic Transform, Fog, Frost, Gaussian Noise, Glass Blur, Impulse Noise, JPEG Compression, Motion Blur, Pixelate, Shot Noise, Snow, and Zoom Blur. Note that other work (e.g. \cite{zhou2022understanding}) tests on a different subset of corruption types. To make a fair comparison, all the models are tested under the aforementioned 15 corruption types.

\minisection{Semantic Segmentation}. We use MMSegmentation~\cite{mmseg2020} as our test bed. We take the ImageNet pretrained ViT-B and \model-B and finetune them on semantic segmentation on PASCAL VOC, Cityscapes, and ADE20K. For all the experiments, we use UperNet~\cite{xiao2018unified} as the decoder head and FCNHead as the auxiliary head. We train on 2 GPUs with a total batch size of 16, using AdamW optimizer, a learning rate of $0.00006$, and weight decay of $0.01$. We train for 20k, 40k, and 160k iterations for three datasets, respectively. We use image resolution of 512x512 for PASCAL VOC and ADE20K, and 512x1024 for Cityscapes.

\minisection{V\&L Finetuning}. Following \cite{dou2022empirical}, the whole model contains a pretrained visual encoder, a pretrained text encoder, and a multimodal encoder to merge vision and language. We use the ImageNet pretrained ViT or \model for the visual encoder, a pretrained RoBERTa for the text encoder, and the multimodal encoder is trained from scratch.  We use a learning rate of $1e-5$ for visual and text encoders and $5e-5$ for the multimodal encoder. For top-down attention, we use the \texttt{[cls]} token as the prior $\xi$. Since the text and visual tokens are not aligned initially, we train a linear transform to project the text tokens into the same space as the visual tokens. This is trained by the prior loss, which is set as a CLIP-style loss (\cref{eq:clip_loss}) to align the text and visual tokens.



% \section{The Top-Down Attention Learned from Supervised Single-Object Recognition is Weak}

% \begin{figure}[h]
% \begin{center}
% \centerline{\includegraphics[width=0.65\columnwidth]{figures/weak_attention.png}}
% \caption{Visualization of top-down attention with different scaling factor $\alpha$. Prior corresponds to the bird. We pretrain the model with $\alpha=1$ and tune the $\alpha$ during test time. We can see that $\alpha=1$ barely has any top-down effect on the attention and we need to choose a larger $\alpha$ in order to amplify the top-down attention.}
% \label{fig:weak_attention}
% \end{center}
% \vskip -0.2in
% \end{figure}

% In experiments we find that \model trained on supervised single-object recognition only has a weak top-down attention. For example, we pretrain \model on ImageNet classification with the scaling factor set as $\alpha=1$, and then we visualize the top-down attention on multi-object scene, as shown in~\cref{fig:weak_attention}. We can see that, even though we add a prior corresponding to the bird, the attention when $\alpha=1$ still highlights both the bird and the dog. However, \cref{fig:weak_attention} shows that the top-down attention is more and more biased towards the bird as we increase $\alpha$. This indicates a simple solution that we can use a larger $\alpha$ during test time or finetuning in downstream tasks (\eg, VQA), although this is not consistent to the training setting where $\alpha=1$. We think the reason for the waek top-down attention is that \model is only trained under supervision of single-object recognition, and it might learn a better top-down attention under a unsupervised setting or trained with multi-object images.


% %%%%%%%%% REFERENCES
% {\small
% % \bibliographystyle{ieee_fullname}
% \bibliographystyle{plainnat}
% \bibliography{egbib}
% }

% \end{document}
