\section{Limitations and Future Work}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{ImageNet Classification Is a Poor Teacher of Top-Down Attention}
\label{sec:lim_weak_td}

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{figures/weak_attention.png}}
\caption{Visualization of top-down attention with different scaling factor $\alpha$. Prior corresponds to the bird. The top-down attention gets more and more biased on the bird when increasing $\alpha$.}
\label{fig:weak_attention}
\end{center}
\vskip -0.3in
\end{figure}

\model is trained to focus on different objects given different priors in multi-object images. However, ImageNet classification targets single object classification without any prior, making it unsuitable for pretraining top-down attention.
We find that the ImageNet-supervised \model only learns weak top-down attention. A simple trick to augment the top-down attention for downstream tasks such as VQA is manually setting a larger scaling factor $\alpha$ (\eg, $\alpha = 10$). In \cref{fig:weak_attention}, we visualize the top-down attention with different $\alpha$. We can see that, with a prior corresponding to the bird, the attention under $\alpha=1$ still highlights both the bird and the dog but is more and more biased towards the bird as we increase $\alpha$. For future exploration, we may learn stronger top-down attention through object-level unsupervised learning~\cite{xiao2021region,henaff2022object} or vision-language pretraining~\cite{xu2022groupvit,mukhoti2022open}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{How Many Syntheses Do We Need for Analysis?}
\label{sec:lim_weak_syn}


\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{figures/weak_synthesis.png}}
\caption{Examples of images decoded from the bottom-up, top-down, or the combination of bottom-up and top-down signals. The decoder can reconstruct the whole image from the bottom-up signal while failing to generate anything recognizable from the top-down signal alone. When decoding from the combination of bottom-up and top-down signals, only the foreground object is reconstructed.   }
\label{fig:weak_synthesis}
\end{center}
\vskip -0.4in
\end{figure}

In \cref{sec:implem}, we mention that enforcing strong generative capability on the features $\bfz_\ell$ will downgrade the discriminative power regarding classification accuracy. There is a similar observation in recent self-supervised learning work~\cite{he2022masked}, where reconstruction-based algorithms have worse linear-probing performance~\cite{caron2021emerging}. However, the empirical results in \cref{tab:ablation_var} indicate that at least some degree of generative power is still helpful. This echoes the classical debate of how much generative capability (``synthesis'') we need for visual discrimination (``analysis''). As a starting point, we measure the generative power of the ImageNet-pretrained \model (\cref{fig:weak_synthesis}). Specifically, we train a linear decoder that projects the bottom-up input $\bfx_0^{bu}$ of the first layer to the original image and then visualize the image decoded from the bottom-up signal $\bfx_0^{bu}$, the top-down signal $\bfx_0^{td}$, or their combination $\bfx_0^{bu} + \bfx_0^{td}$. We can see that the bottom-up signal contains full information about the original image and gives a perfect reconstruction. On the other hand, the top-down signal has lost most of the information, which is reasonable considering that $\bfx_0^{td}$ itself is decoded from the last layer's feature. Intriguingly, when we combine the bottom-up and the top-down signals, it can reconstruct only the foreground object, implying \model can selectively preserve partial information in the image, and the selection process is adaptive to different priors. This leaves the question of whether a \textit{selective} generation process is the best companion of the discriminative model and how to control the selective process under different priors adaptively.