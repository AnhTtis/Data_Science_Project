\section{Top-Down Attention from AbS}
\label{sec:deriv}

We consider top-down visual attention from an Analysis by Synthesis (AbS) view of vision. We start from the hierarchical AbS formulation of visual perception (\cref{sec:abs}) and show that it is equivalently optimizing a sparse reconstruction objective that is modulated by a top-down signal, thus entailing top-down attention (\cref{sec:td_from_abs}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hierarchical AbS}
\label{sec:abs}

AbS formulates visual perception as a Bayesian inference process. Given the image generation process $p(\bfh | \bfz)$ and a prior $p(\bfz)$, where $\bfh$ is the image and $\bfz$ is the latent code, AbS finds $\bfz^\ast = \argmax_{\bfz} p(\bfh | \bfz) p(\bfz)$.
%a plausible latent that well explains the image, say, by Maximum a Posteriori (MAP): $\bfz^\ast = \argmax_{\bfz} p(\bfh | \bfz) p(\bfz)$.

In this work, we assume the generation is hierarchical, \ie, $\bfz_L \to \bfz_{L-1} \to  \cdots \to \bfz_1 \to \bfh$, where $\bfz_\ell$ is the latent at $\ell$-th layer. The MAP estimation is
\begin{equation}\small
\label{eq:abs}
    \bfz_L^\ast, \cdots, \bfz_1^\ast = \argmax_{\bfz_L, \cdots, \bfz_1} p(\bfh | \bfz_1) \cdots p(\bfz_{L-1} | \bfz_L) p(\bfz_L).
\end{equation}

For each generation process $\bfz_{\ell+1} \to \bfz_\ell$ between layer $\ell$ and $\ell+1$, we further assume that $\bfz_\ell$ is constructed by a sparse code $\tbfu_\ell$ which is generated from $\bfz_{\ell+1}$ via a non-linear function $\mathit{g}_\ell(\cdot)$, \ie,

\vspace{-0.8em}
\begin{footnotesize}
\begin{align}
    \tbfu_\ell &\sim p(\tbfu_\ell | \bfz_{\ell+1}) \propto \exp\{-\frac{1}{2}||\bfP_\ell \tbfu_\ell - \mathit{g}_\ell(\bfz_{\ell+1})||_2^2 - \lambda||\tbfu_\ell||_1\} \label{eq:gen_1}\\
    \bfz_\ell &= \bfP_\ell \tbfu_\ell \label{eq:gen_2},
\end{align}
\end{footnotesize}\noindent
where $\bfP_\ell$ is the dictionary. Intuitively, it first generates $\mathit{g}_\ell(\bfz_{\ell+1})$ as a blurry and noisy version of $\bfz_\ell$, then find the sparse code $\tbfu_\ell$ to construct a sharper and cleaner version.

Since $\bfz_\ell$ is decided by $\tbfu_\ell$, it suffices to optimize the MAP estimation over $\{\tbfu_\ell\}_{\ell=1}^L$, \ie,
\begin{equation}\small
\label{eq:abs_2}
    \tbfu_L^\ast, \cdots, \tbfu_1^\ast = \argmax_{\tbfu_L, \cdots, \tbfu_1} p(\bfh | \tbfu_1) \cdots p(\tbfu_{L-1} | \tbfu_L) p(\tbfu_L).
\end{equation}
Solving \cref{eq:abs_2} by simple gradient ascent (of the logarithm) gives the dynamics
\begin{equation}\small
\label{eq:abs_dyn}
    \dv{\tbfu_\ell}{t} \propto \nabla_{\tbfu_\ell} \log p(\tbfu_{\ell-1} | \tbfu_\ell) + \nabla_{\tbfu_\ell} \log p(\tbfu_\ell | \tbfu_{\ell+1})
\end{equation} 
where $\tbfu_\ell$ is affected by both $\tbfu_{\ell-1}$ and $\tbfu_{\ell+1}$. 
    




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Top-Down Attention from AbS}
\label{sec:td_from_abs}

From AbS (Eq. (\ref{eq:gen_1}-\ref{eq:abs_dyn})) we can derive the dynamics of $\tbfu_\ell$ as 
\begin{equation}\footnotesize
\label{eq:abs_dyn2}
    \dv{\tbfu_\ell}{t} \propto \nabla_{\tbfu_\ell} \left( -\frac{1}{2} ||\bfP_\ell \tbfu_\ell - (\bfx_\ell^{bu} + \bfx_\ell^{td}) ||_2^2  - \lambda ||\tbfu_\ell||_1 - \mathit{r}_\ell(\tbfu_\ell) \right)
\end{equation}
where $\bfx_\ell^{td} = \mathit{g}_\ell (\bfz_{\ell+1})$ is the top-down signal and $\bfx_\ell^{bu} = \mathit{f}_\ell (\bfz_{\ell-1}) = \bfJ_{\mathit{g}_{\ell-1}}^T \bfz_{\ell-1} $ is the bottom-up signal where $\bfJ_{\mathit{g}_{\ell-1}}$ is the jacobian of $\mathit{g}_{\ell-1}(\bfP_\ell \tbfu_\ell)$, and $\mathit{r}_\ell(\tbfu_\ell) = ||\mathit{g}_{\ell-1}(\bfP_\ell \tbfu_\ell)||_2^2$ is an additional regularization. Details of the derivation are pushed back to Appendix. One may notice from \cref{eq:abs_dyn2} that, in AbS  each layer is solving a similar sparse reconstruction problem as in \cref{eq:sr} but with the input of $\bfx_\ell^{bu} + \bfx_\ell^{td}$, thus simulating attention that is modulated by both bottom-up and top-down signals. This can also be observed by turning \cref{eq:abs_dyn2} into
\begin{equation}\small
    \dv{\tbfu_\ell}{t} \propto -\bfu_\ell  - (\bfP_\ell^T \bfP_\ell - \mathbf{I})\tbfu_\ell + \bfP_\ell^T \bfx_\ell^{bu} + \bfP_\ell^T \bfx_\ell^{td} - \nabla \mathit{r}_\ell(\tbfu_\ell).
\end{equation}
Comparing with \cref{eq:sr_dyn}, here $\tbfu_\ell$ is steered by an additional term $\bfP_\ell^T \bfx_\ell^{td}$ that acts as a bias on which atom in $\bfP_\ell$ to choose. For example, if atoms in $\bfP_\ell$ are templates of separate objects (like in self-attention), then $\bfP_\ell^T \bfx_\ell^{td}$ highlights the objects that are consistent with the top-down signal (\cref{fig:dict} (b)).

This implies an AbS system naturally entails top-down attention. Intuitively, the prior reflects which objects the output $\bfz_L$ should highlight. Then the affected $\bfz_L$ is fed back to layer $L-1$ through $\mathit{g}_{L-1}$, as a top-down signal to direct which objects to select in layer $L-1$. The same process repeats until the first layer. Different priors will direct the intermediate layers to select different objects, achieving top-down attention. 
%\todo{maybe use bistable image as an example here!} In the following we will build a ViT-based model with top-down modulation to approximate Bayesian inference (\cref{sec:implem}), and show that it indeed achieves top-down attention (\cref{sec:exp}).

Interestingly, if we consider the analogy between self-attention and sparse reconstruction, \cref{eq:abs_dyn2} leads to a smooth way of building a top-down version of self-attention, \ie, we only need to add a top-down signal to the value $\bfV$, while keeping other parts such as $\bfQ$ and $\bfK$ (which decides the dictionary) untouched. We will make it clearer in \cref{sec:implem}.




\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{figures/dictionary.pdf}}
\caption{(a) Each atom in the dictionary contains masks for separate objects or regions. The sparse reconstruction tries to use as few masks as possible to reconstruct the input feature map, thus only the salient objects are highlighted. (b) The top-down signal $\bfx^{td}_\ell$ puts a bias on the weights of the atoms so that only the objects that agree with $\bfx^{td}_\ell$ are selected.}
\label{fig:dict}
\end{center}
\vskip -0.3in
\end{figure}