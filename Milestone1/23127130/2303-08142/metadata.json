{
    "arxiv_id": "2303.08142",
    "paper_title": "Performance Embeddings: A Similarity-based Approach to Automatic Performance Optimization",
    "authors": [
        "Lukas Tr√ºmper",
        "Tal Ben-Nun",
        "Philipp Schaad",
        "Alexandru Calotoiu",
        "Torsten Hoefler"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [
        "2023-03-16"
    ],
    "latest_version": 1,
    "categories": [
        "cs.SE",
        "cs.DC",
        "cs.LG"
    ],
    "abstract": "Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit clear correspondences between program characteristics and optimizations, outperforming prior specialized state-of-the-art approaches and generalizing beyond their capabilities.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08142v1"
    ],
    "publication_venue": null
}