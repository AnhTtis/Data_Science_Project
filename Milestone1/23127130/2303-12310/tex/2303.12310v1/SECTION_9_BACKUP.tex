\clearpage
\section{Backup}

\begin{figure*}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{Latex/Fig_1_Paper_Concept.pdf}	
	\caption{Closed loop analysis for system level, device level, and run time optimization}
	\label{fig:STCO}	
\end{figure*}

\textbf{From key contribution}
\begin{itemize}
    \item Guided by the operational intensity of the workload and its compute-memory demands, we further optimize the power consumption of the system at iso-performance through dynamic resource allocation and voltage/frequency scaling of the compute and memory units. For example, based on the mode of DNN workload operation (i.e., inference or training), mode of parallelism for large models and data batches (i.e., model, data, or hybrid parallelism), the memory (i.e., BW, capacity) and compute resources are optimized at runtime.
\end{itemize}

\textbf{From MTJ physical structure} MTJ is the building block of all MRAMs. It is a three-layer structure consisting of an insulating oxide layer (e.g., MgO) sandwiched between two independent ferromagnetic layers (e.g., CoFeB). The bit value is stored inside the MTJ as its resistive state. The resistive status of MTJ depends on the relative magnetic orientation of the two ferromagnetic layers. One ferromagnetic layer, with the fixed magnetic field, is known as the $fixed/reference$ layer.The other ferromagnetic layer that can change its magnetic orientation in the presence of an external source (i.e., flowing a spin-polarized current trough MTJ) is known as the $free$ layer. When both layers have the magnetic field pointed in the same direction, the parallel orientation ($R_{P}$) exhibiting low resistance represents a low logic value (0). On the other hand, when two layers' magnetic fields are pointed toward opposite directions, the antiparallel orientation ($R_{AP}$) reflecting high resistive state represents storing the high logic value 1 (Fig. \ref{fig:sot_bitcell}) \cite{stt_eqn1}, \cite{stt_eqn2}.

\textbf{From MTJ read operation}
During the read operation of SOT-MRAM, upon the activation of the word line, a small amount of current is passed through the read line and the grounded source line. MTJ resistance is measured by sensing the voltage across the MTJ and comparing it with a reference value \cite{dual-port_field-free_fab2}. Low resistance ($R_{P}$) represents stored bit is 0 and high resistance ($R_{AP}$) represents stored bit is 1. 

\textbf{From MTJ write operation}
The underlying physical explanation of the current-induced switching mechanism of the free layer under $spin\ orbit\ torque$ effect is complex \cite{tahoori_1}\cite{manchon2019current} and outside the scope of this paper. In simple terms, a damping-like torque, attributed to $spin\ hall\ effect\ (\tau_{SH})$, triggers the switching and a field-like torque, attributed to $Rashba\ effect (\tau_{FL})$ accelerates the switching \cite{ultrafast_embedded_mem_fab4}\cite{tahoori_1}. 

\begin{figure*}
    \centering
    \includegraphics[width = \textwidth]{Latex/Fig_8_BW_Calc_new.pdf}
    \caption{Computation visualization inside systolic array \textcolor{red}{This figure can be removed}}
    \label{bw_calc_all}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{Latex/Fig_9_Bandwidth_cases.pdf}
    \caption{Possible scenarios of operand matrix and systolic array dimensions \textcolor{red}{This figure can be removed as this is redundant information, same as table \ref{table:bw_expression}}}
    \label{bw_calc_cases}
\end{figure}

In \emph{rows 6-8 \& 17}, we list the training-dedicated ephemeral entities, such as Upstream gradients, the gradient of loss and activation functions, optimizer states, with their dimension, memory access count, and reuse status.

In Table \ref{Transformer_analysis}, we summarize the memory accesses requirement and dataflow of the transformer workload during inference and training regardless of the model architecture and hardware platform. The input sequence propagates through the embedding layer and different sublayers of the encoder stacks to extract different linguistic features and inter-token dependency of the input sequence. The decoder stacks then generate the output sequence by taking the encoded input sequence from the encoder stack and the output sequence generated by itself in the previous timesteps (Fig \ref{fig:transformer}). The input sequence multiplied by different layer weights takes different activation names and shapes (Table \ref{Transformer_analysis} col. 1 \& 2) throughout the model operation. The minimum memory accesses requirement of each layerâ€™s (sub-layer) activations and weights in different levels of memory hierarchy during inference and training are listed in col. 3 \& 4 of Table \ref{Transformer_analysis}, respectively. In this work, we consider a memory hierarchy of HBM3 DRAM (off-chip memory), Unified Buffer (on-chip memory), and SRAM scratchpad (a small SRAM inside PE array to hold weights and partial sums). During inference, the read-only weights are  directly loaded from HBM3 DRAM to register file of each PE unit bypassing the UB. However, we use double buffered SRAM to hide the off-chip access latency. While the array is computing with loaded weights,the next set of weights are temporarily written to SRAM buffer to hide the off-chip access latency behind the PE array computation latency. \emph{Read: once} under heading \textbf{HBM3 DRAM} means the whole chunk of data, for example, \emph{Input sequence}, is read once from DRAM given that the Unified Buffer (UB) is large enough to hold all samples in the minibatch. \emph{UB $\rightarrow$ PE core} means the entity is read from $UB$ and written into or operated inside \emph{PE core}. We consider the memory access counts in terms of how many times the data entities are in operation throughout the workflow. However, the exact access count depends on the model architecture, hyperparameters, and hardware platforms such as input sequence length, weight matrix, word embedding size, batch size, UB size, SRAM size, and PE core dimension.
We also provide a high-level dataflow and reuse property for each entity during inference and training (\emph{col. 5 \& 6} of table \ref{Transformer_analysis}, respectively). To illustrate, the last column of the second row indicates the dataflow and reuse status of Embedded input during training. During forward pass, the embedded inputs are generated inside the PE core, partial sums are accumulated in the SRAM, and finally, the output is written to Unified buffer. For weight gradient calculation, the embedded inputs read from UB to PE core during backpropagation. \emph{Reuse inside PE core} represents at forward pass, the embedded input can be reused trice as input to Key, Query, and Value linear layer. It can also be reused thrice during backpropagation to calculate the weight gradient of the Key, Query, and Value linear layer. In \emph{rows 6-8 \& 17}, we list the training-dedicated ephemeral entities, such as Upstream gradients, the gradient of loss and activation functions, optimizer states, with their dimension, memory access count, and reuse status.


\begin{figure}[h]
  \centering
    \includegraphics[scale=0.75]{Latex/Fig_7_BW_systolic_array.pdf}
    \caption{Read-Write bandwidth of GEMM operation in Systolic array. \textcolor{red}{(1) Symbols in figure not matching to that in text. (2) How does activation buffer relate to unified buffer ? Separate ? If separate what are the BWs ? }}
    \label{fig:bw_fc}
\end{figure}

\begin{table}[]
\caption{HBM specifications \cite{HBM3}, \cite{HBM2_energy}}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Memory Tech & \begin{tabular}[c]{@{}c@{}}Data rate\\ per-pin\end{tabular} & \begin{tabular}[c]{@{}c@{}}I/O\\ interface\end{tabular} & \begin{tabular}[c]{@{}c@{}}Bandwidth\\ per-device\end{tabular} & \begin{tabular}[c]{@{}c@{}}Energy\\ (pJ/bit)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Latency\\ (ps/access)\end{tabular} \\ \hline
HBM2 & 3.2 GB/s & 1024-bit & 410 GBPs & 3.9 & 290 \\ \hline
HMB3 & 6.4 GB/s & 1024-bit & 819 GBPs & 3.9 & 145 \\ \hline
\end{tabular}

\label{hbm_spec}
\end{table}

-----------SOT vs SRAM bit cell level comparison--------
\begin{table}[]

\centering
\caption{Comparison of SOT-MRAM \& SRAM in bit-cell level at 7nm CMOS technology}
\begin{tabular}{|c|c|c|}
\hline
                  & SOT-MRAM (1/0)      & SRAM \\ \hline
Read latency (ns)  & 0.9          & 0.25 \\ \hline
Write latency (ns) & 1           & 0.25 \\ \hline
Read current (uA)  & 65/41          &      \\ \hline
Write current (uA) & 163/159        &      \\ \hline
Read power (uW)    & 31/16         & 169  \\ \hline
Write power (uW)   & 112/109          & 83   \\ \hline
Leakage power (nW) & 0.065         & 18   \\ \hline
\end{tabular}
\label{sot_vs_sram_7nm}
\end{table}
