\section{Background}
\label{background}
\subsection{AI/DL Applications}
\subsubsection{Computer Vision (CV) and Pattern Recognition}
CV is one of the earliest applications of DL. CV models are the stacks of convolution layers to extract the objectsâ€™ features and a few FC layers at the end to classify them (Fig. \ref{fig:conv_op}). Image classification, object detection, object/instance segmentation, image reconstruction, and captioning are the scopes of CV models. Image captioning combines both CV and NLP tasks.

\subsubsection{Natural Language Processing (NLP)}
Language modeling deals with processing sequential data. Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) have been used in language modeling until the state-of-the-art Transformer \cite{vaswani2017attention} model is introduced. NLP models are used in machine translation, text summarization, speech recognition, syntactic and semantic parsing, question answering, dialog system etc.

\subsection{AI/DL Accelerators}
At the core of AI/DLs is the matrix-matrix/vector multiplication (GEMM) with massive parallelism. Exploiting this parallelism, SIMD and Systolic Array (SA) based architecture have been used to accelerate the computations. Different dataflows, such as row stationary, output stationary, weight stationary, have been evolved to maximize the reuse and reduce the data movement. Off-chip DRAM access being 100-200 times more energy and latency expensive than any ALU operation or on-chip access \cite{eyeriss} plays a crucial role in determining the overall system performance. In this work, we focus on reducing the off-chip memory access by increasing the on-chip Global Buffer (GLB) size with SOT-MRAM. 
\subsection{SOT-MRAM}
\subsubsection{Physical Structure} 

With MTJ \cite{stt_eqn1} as storing element, the SOT-MRAM is a four terminal device: (i) \emph{Read Wordline (RWL)}, (ii) \emph{Write Wordline (WWL)}, (iii) \emph{Bit Line (BL)}, and (iv) \emph{Bit Linebar (BLB)} \cite{sot_model_kazemi} (Fig. \ref{fig:sot_bitcell}). The MTJ stack, with its free layer at the interface, is placed on top of a SOT layer (also known as the channel) to ensure SOT induced switching. The SOT layer is composed of heavy metals or topological insulators \cite{manchon2019current}. Two access transistors are used to access the separate read and write path.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.8]{Fig_3_SOT_bitcell_new.pdf}
	%\vspace{-5pt}
	\caption{Physical structure of a SOT-MRAM bit cell highlighting separate read (along blue line) and write (along red line) path}
	\label{fig:sot_bitcell}	
	%\vspace{-5pt}
\end{figure}

\subsubsection{Read-Write Operation}
Upon the activation of RWL, a small amount of current is passed through BL and grounded BLB. The resistive state of the MTJ is captured by sensing the voltage across it and comparing the voltage with a reference value \cite{dualport_fieldfree_fab2}. Low resistive state ($R_{P}$) and high resistive state ($R_{AP}$) represents bit 0 and 1 respectively.



%\subsubsection{Write operation} 
The write operation of MTJ-based MRAM involves switching the resistive status of MTJ. In SOT-MRAM, switching occurs due to Spin Orbit Torque (SOT) effect. Unlike STT-MRAM, a current is passed through the SOT layer to change the MTJ resistive state by switching the magnetic orientation of the free layer. 
In the write operation, a bidirectional write current flows through BL and BLB. The potential of BL and BLB changes depending on the bit value written in the cell. For example, to write 1, current flows from BL to BLB and vice versa to write 0 \cite{dualport_fieldfree_fab2} \cite{tahoori_1}.


