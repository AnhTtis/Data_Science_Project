\section{Results and Analysis}
\label{result_analysis}
% We developed a MATLAB-based framework to implement our analytical \emph {Memory and Compute Model}. Unlike ScaleSim \cite{samajdar2020systematic} and Timelooop \cite{parashar2019timeloop} simulator, which only support profiling DNN workloads in inference mode to date, our model captures both training and inference behavior of CV and NLP models. In this section, we provide the result and analysis of the CV and NLP workloads during inference and training from our analytical model and present the optimum Power, Performance, and Area results by performing the DTCO of SOT-MRAM.

In this section, we provide the result and analysis of the STCO on the CV and NLP workloads during inference and training and present the optimum Power, Performance, and Area results by performing the DTCO of SOT-MRAM. We developed a MATLAB-based framework to implement our analytical \emph {Memory and Compute Model}. Unlike ScaleSim \cite{samajdar2020systematic} and Timelooop \cite{parashar2019timeloop} simulator, which only support profiling DNN workloads in inference mode to date, our model captures both training and inference behavior of CV and NLP models.



\subsection{Bandwidth Demand}
\label{bw_demand}


% We implement our analytical bandwidth characterization model to calculate the bandwidth demand for 18 widely used state-of-the-art CV models for both training and inferene cases. Our analytical model differs from the 
% ScaleSim\cite{samajdar2020systematic} simulator. Given the PE array dimension, on-chip memory size, and dataflow, ScaleSim reports the DRAM bandwidth for IFMAP, OFMAP, and weight buffer for stall-free execution of the PE array, and the PE array utilization depends on the array dimension and mapping of the dataflow. However, we estimate the required on-chip bandwidth to ensure that the array utilization can reach 100\%, assuming that the mapping efficiency is 100\%.



\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{Bandwidth_CV.pdf}
    %\vspace{-10pt}
    \caption{Bandwidth requirement of CV models for different PE array size. (a) Read Bandwidth, (b) Write Bandwidth.}
    %\vspace{-15pt}
    \label{bw_cv}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{Bandwidth_NLP.pdf}
    %\vspace{-10pt}
    \caption{Bandwidth requirement of NLP models for different PE array size. (a) Read Bandwidth, (b) Write Bandwidth.}
    %\vspace{-5pt}
    \label{bw_nlp}
\end{figure}


\begin{figure*}[ht]
    \centering
    \includegraphics[width = \textwidth]{FIG_CV_Vary_GLB.pdf}
    %\vspace{-25pt}
    \caption{Impact of larger GLB memories on performance and energy efficiency for CV models at inference and training. Percentage reduction in DRAM accesses at inference (a) and training (d). Performance Speedup  from DRAM access reductions at inference (b) and training (e). Energy savings from reduced DRAM accesses at inference (c) and training (f). Both cases compare results to a baseline of 2MB GLB running 16 samples.}
    %\vspace{-10pt}
    \label{d_fig1_cv_infr}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width = \textwidth]{FIG_CV_Vary_BATCH.pdf}
    %\vspace{-25pt}
    \caption{Impact of batch size on performance and energy efficiency for CV models at inference and training. Percentage increase in DRAM accesses at inference (a), at training (d). Performance slowdown (latency increase) from extra DRAM accesses at inference (b), at training (e). Energy increase from extra DRAM accesses at inference (c), at training (f). In both cases, results are compared to a baseline of 16 samples running with 4MB GLB.}
    %\vspace{-10pt}
    \label{d_fig2_cv}
\end{figure*}

In Fig. \ref{bw_cv} (a), (b), we plot the read-write on-chip bandwidth demand in $bytes/cycle$ of 18 widely used CV models. Resenet101 and Resnet50 running on a 256$\times$256 PE array will demand the highest read bandwidth, 4017 bytes/cycle, from GLB, whereas Squeezenet will demand the lowest bandwidth, 1028 bytes/cycle. Naturally, as the PE array size increases, the computation capacity per cycle $T_{MAC}$ increases which demands more data from memory to keep all PEs active. From the workload perspective, we observe that the most contributing factor to the read bandwidth demand is its inverse relationship with the filter and ofmap size. We explain the inverse relationship of filter and ofmap size with the read bandwidth using the convolutional reuse concept. As the filter size decreases, the scope of convolutional reuse decreases. The ofmap again depends on the filter and ifmap size. With the decrease of filter size and ofmap size, the convolutional reuse decreases, giving rise to more bandwidth demand. The layer of Resnet101 that requires the most bandwidth (4017 bytes/cycle) has the ofmap dimension (7$\times$7) and filter dimension (1$\times$1). On the other hand, the most demanding (1028 bytes/cycle) layer of Squeezenet has the ofmap dimension (18$\times$18) and filter dimension (1$\times$1). Another observation is that though 1$\times$1 convolution reduces the computation complexity, it requires more bandwidth from memory, i.e., becomes memory intensive.


\begin{figure*}[ht]
    \centering
    \includegraphics[width = \textwidth]{FIG_NLP_Vary_GLB.pdf}
    %\vspace{-25pt}
    \caption{Impact of larger GLB memories on performance and energy efficiency for NLP models at inference and training. Percentage reduction in DRAM accesses at inference(a), at training (d). Performance Speedup  from DRAM access reductions at inference (b), at training (e). Energy savings from reduced DRAM accesses at inference (c), at training (e). In both cases, results are compared to a baseline of 2MB GLB running 16 samples}
    \label{d_fig1_nlp}
    %\vspace{-8pt}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width = \textwidth]{FIG_NLP_VARY_Batch.pdf}
    %\vspace{-25pt}
    \caption{Impact of batch size on performance and energy efficiency for NLP models at inference and training. Percentage increase in DRAM accesses, inference (a), and training (d). Performance slowdown (latency increase) from extra DRAM accesses at inference (b), at training (e). Energy increase from extra DRAM accesses at inference (c), at training (f). Results are compared to a baseline of 16 samples running with 4MB GLB.}
    \label{d_fig2_nlp}
\end{figure*}

The write bandwidth is also inversely proportional to the filter size. However, in 1$\times $1 convolutions, it depends on the number of outputs generated by the PE array. The write bandwidth is always smaller than the read bandwidth (Fig. \ref{bw_cv} (b)) as it takes more than one operand to generate one output. For example, in a 3$\times$3 convolution, it takes 18 operands to generate a single output; in a 1$\times$1 convolution, it takes two operands to generate one output element. 


In transformer-based NLP models, the computations are essentially the GEMM operation. The bandwidth estimation of these models is performed using the same expressions as FC layers. As the dimension of the operand matrices is larger than the PE array dimension, following case iv (Table \ref{table:bw_expression}, section \ref{bw_fc_layer}), the read bandwidth of all models depends on the PE array size (Fig. \ref{bw_nlp} (a)). The write bandwidth depends on the PE array dimension and the input sequence length. As different models are trained with different input sequence lengths\cite{hugging_face}, their write bandwidth demand is not the same across all models. The models having the highest sequence length (2048) have the lower write bandwidth demand 102 bytes/cycle running on a 256$\times$256 PE array (Fig. \ref{bw_nlp} (b)).


\subsection{Impact of on-chip memory}
\label{on-chip impact}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{dtco_fig_1.pdf}
    %\vspace{-20pt}
    \caption{Critical current vs $\theta_{SH}$(a), $w_{SOT}$(b), $t_{SOT}$(c), and $t_{FL}$(d).}
    \label{fig:dtco_ic}
    %\vspace{-10pt}
\end{figure*}

%To realize the impact of on-chip memory on the accelerator performance, we implement our analytical model of memory access counts (Table \ref{conv_analysis} for CV models and Table \ref{Transformer_analysis} for NLP models, see section \ref{mem_access}). 
%Timeloop \cite{parashar2019timeloop} also reports the scalar DRAM read-write counts for DNN models, but we did not use it in our analysis as it does not map the DRAM access counts as a function of GLB size.
%rather a function of mapping and loop nests.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.52]{dtco_fig2_ic_vs_tp_delta_t_ret.pdf}
    %\vspace{-10pt}
    \caption{(a) Switching pulse width $\tau_{p}$ vs applied switching current $I_{sw}$.  (b) Thermal stability factor $\Delta$ (left Y-axis) and retention time $t_{ret}$ (right Y-axis) vs MTJ dimension for a fixed retention failure rate, $P_{RF} = 10^{-9}$. At $\Delta = 70$, MTJ dimension = 88nm, retention time is $>$ 10 years \cite{imce2019}.}
    \label{fig:dtco_pulse_width}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.52]{dtco_fig3_tmr_t_MgO_rd_latency.pdf}
    %\vspace{-10pt}
    \caption{(a) Impact of oxide thickness on TMR. (b) Impact of TMR on read latency.}
    \label{fig:tmr_vs_oxide_rd_ltncy}
    %\vspace{-15pt}
\end{figure}

Compared to a GLB size of 2MB, the DRAM access counts for all CV models decrease significantly if we increase the GLB size. In inference, reaching the 100\% reduction in access means it only needs to read the initial inputs, weights for each layer, and write the final layer output, no DRAM access is needed for the intra and inter-layer operations. Further increase in GLB size will not positively impact the performance in these cases. 
%We did this analysis running a batch of 16 samples, which is quite a large batch size for inference. 
For 16 samples, DRAM access is reduced by 100\% for 14 models at 128MB, and most models experience a reduction of >80\% at 64MB (Fig. \ref{d_fig1_cv_infr} (a)). Fig. \ref{d_fig1_cv_infr} (b), (c) show the performance speed up and energy saving coming from these DRAM access reductions. 


We observe a slower improvement in the DRAM access reduction during training unless the GLB size is large enough, at least 256MB for most models (Fig. \ref{d_fig1_cv_infr} (d)). However, even the smaller percent reduction in DRAM access results in significant performance and energy improvement %compared to a larger percent reduction during inference% 
(Fig. \ref{d_fig1_cv_infr} (e), (f)). This is because training requires at least 2$\times$ DRAM accesses as inference. A similar trend is observed for NLP models. Transformer-based NLP models are usually larger than the CV models. This is the reason we achieve more performance speedup and energy reduction even at smaller DRAM access reduction rate (see Fig. \ref{d_fig1_nlp}).

We also observe the impact of batch size on the system performance. DNN models learn faster if we increase the batch size. Thus for a fixed GLB size, the DRAM access count increases significantly at larger batch size, causing performance slowdown and more energy consumption. Fig. \ref{d_fig2_cv} and Fig. \ref{d_fig2_nlp} (a, b, c for inference and d, e, f for training) show the increase in DRAM access count and its associated impact on performance and energy for CV and NLP models respectively at different batch sizes.  

The key takeaway from this analysis is that we can reduce the energy and latency associated with DRAM accesses if we increase the GLB size. For larger batch sizes, the energy and latency improvement is even more.

\subsection{DTCO of SOT for PPA Optimization}


From section \ref{on-chip impact} we see that the GLB size of 64MB (for inference) and 256MB (for training) offer significant energy and performance improvement. However, it is not feasible and efficient to use such large SRAMs because of its area and leakage power, even if the low-power techniques are employed. Section \ref{bw_demand} implies that we need approximately 4000bytes/cycle bandwidth between GLB and PE array for larger array size (256$\times$256). In this subsection, we provide the SOT-MRAM DTCO results and observation meeting the requirements stated in the above two subsections. We perform the DTCO in \textit{Cadence Virtuoso}  tool using the compact SOT-MRAM model from \cite{sot_model_kazemi}, and use \textit{Synopsys} 14nm library \cite{saed14} for  the CMOS transistors and peripheral circuits.

\subsubsection{$I_{C}$ optimization}

%The critical switching current depends on the device's physical and geometrical structure and its magnetic property. The critical switching current can be controlled for a given external field by adjusting the device dimension and using the appropriate material. 
To realize the impact of SOT efficiency $\theta_{SH}$ on $I_{c}$, we sweep $\theta_{SH}$ from 0.1 to 100 (Fig. \ref{fig:dtco_ic} (a)). With $\theta_{SH} \ge 100$,  $I_{c}$ goes as low as 0.5uA. Even though the widely used SOT layers are made of heavy metal alloys having smaller $\theta_{SH}$ (e.g., 0.1 to 0.5), recent advancement in material engineering demonstrates that in topological insulator $\theta_{SH}$ can go as high as 152 \cite{khang2018conductive}. We recommend using topological insulators as the SOT layer to achieve a lower switching current.


Next, we analyze the impact of SOT layer geometry on the switching current (Fig. \ref{fig:dtco_ic} (b), (c)). $I_{c}$ scales down linearly with the decrease of SOT layer width, and $w_{SOT}$ can be set to desired value based on the performance and reliability requirement (Fig. \ref{fig:dtco_ic} (b)). While $I_{c}$ scales linearly with the width of the SOT layer, the thickness of the SOT layer has an interesting effect on the switching current. The SOT layer should be relatively thin but bulk enough for heavy metal layers to experience the bulk effect to achieve high SOT efficiency. Once it crosses optimum thickness, which is ~3nm (Fig. \ref{fig:dtco_ic} (c)), many of the charges that are injected into the metal do not contribute to the switching. As a result, $I_{c}$ goes high beyond 3nm of thickness. 

The free layer thickness is a major contributor to optimizing the switching current. The smaller the thickness, the smaller the switching current (fig \ref{fig:dtco_ic} (d)). However, with the scaling down of $t_{FL}$, the thermal stability factor $\Delta$ also scales down, reducing the memory's data retention time $t_{ret}$. Non-volatility is a great feature of MRAM, but it can be compromised to achieve higher density, higher bandwidth, and lower energy when the target application is a cache. Because, in the cache even for AI workloads, the data lifetime is much shorter, typically in the seconds range \cite{stt_ai_us}. Fig. \ref{fig:dtco_pulse_width}(b) shows $\Delta$ and $t_{ret}$ as functions of free layer volume. While scaling down $t_{FL}$ to optimize $I_{c}$, we keep an eye on the reliability of the stored data. We consider a retention failure rate of $10^{-9}$ (i.e., 1 bit flip per billion) to adjust the $t_{FL}$.

\subsubsection{Bandwidth optimization}
As shown in Fig. \ref{fig:tmr_vs_oxide_rd_ltncy} (a), \cite{tsunekawa2005giant} demonstrated that TMR ratio of the MTJ device can be increased by increasing the oxide thickness. We increase the oxide thickness from 1nm to 2nm to decrease the  read latency from 260ps to 213ps (Fig. \ref{fig:tmr_vs_oxide_rd_ltncy} (b)). 
The write pulse width is inversely proportional to the applied switching current. While we want to lower the applied current to achieve low energy, the higher amplitude of the applied current is required for faster magnetization reversal. However, switching occurs at smaller pulse width at the iso-current if we scale down the SOT layer width. This is because of the smaller critical current at smaller geometry (Fig. \ref{fig:dtco_ic} (b,d)).
Fig. \ref{fig:dtco_pulse_width}(a) shows that switching pulse width is reduced significantly from 800ps to 175ps @90uA by scaling down the SOT layer width from 150nm to 100nm. Thus, we achieve higher write bandwidth by scaling down the SOT layer width to meet the high BW demand from AI workloads. The DTCO optimized parameters of SOT-MRAM used in this study are listed in Table \ref{optimized_dtco_param}.

\begin{table}[ht]
\centering
\caption{SOT-MRAM DTCO optimized parameters}
\label{optimized_dtco_param}
\vspace{-10pt}
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{Parameter} & \multicolumn{1}{c|}{Value} & \multicolumn{1}{c|}{Parameter} & \multicolumn{1}{c|}{Value} \\ \hline
Spin Hall angle $\theta_{SH}$   & 1                          & TMR                            & 240\%                      \\ \hline
Free layer thickness $t_{FL}$   & 0.2nm                      & MTJ diameter                   & 50nm                       \\ \hline
SOT width $w_{SOT}$             & 100nm                      & SOT thickness $t_{SOT}$        & 3nm                        \\ \hline
Oxide thickness $t_{MgO}$       & 2nm                        & Thermal stability factor $\Delta$                       & 40                         \\ \hline
\end{tabular}
%\vspace{-10pt}
\end{table}


With a read latency of 213ps and a write latency of 175ps, each  SOT-MRAM bit-cell can achieve a read bandwidth of 4.69Gbps and a write bandwidth of 5.71Gbps. We then dynamically allocate the memory bus width on-demand to satisfy the bandwidth requirement for different workloads and PE array size stated in section \ref{bw_demand}.





\subsection{System level performance evaluation}
\begin{figure*}[ht]
    \centering
    \includegraphics[width = \textwidth]{SYS_LVL_PRF_ALL.pdf}
    %\vspace{-25pt}
    \caption{System level energy improvement with SOT-MRAM and DTCO-optimized-SOT-MRAM over SRAM at the same size for CV (a-d) and NLP (e-h) models. The top plots show energy (a, e) and latency (b, f) for inference, and the bottom plots show energy (c, g) and latency (d,h) for training.}
    %\vspace{-10pt}
    \label{fig_sys_lvl_perf_cv}
\end{figure*}

% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width = \textwidth]{Fig_sys_lvl_perf_nlp.pdf}
%     \caption{System level energy improvement with SOT-MRAM and DTCO optimized SOT-MRAM over SRAM at the same size for NLP models: (a) Inference, (b) Training. System level latency improvement with SOT-MRAM and DTCO optimized SOT-MRAM over SRAM at the same size: (c) Inference, (d) Training.}
%     \label{fig_sys_lvl_perf_nlp}
% \end{figure*}


In this subsection, we analyze the PPA metrics in the system level on the DNN benchmarks with SRAM, SOT-MRAM, and DTCO-optimized-SOT-MRAM. We simulate Synopsys 14nm low-power SRAM \cite{saed14}, and SOT-MRAM \cite{sot_model_kazemi} in bit-cell level in Cadence Virtuoso and feed this data to Destiny \cite{destiny} to find the array-level PPA. Destiny currently does not support SOT-MRAM; we modified the STT-MRAM source files to reflect the separate read-write path and SOT switching mechanism to incorporate SOT-MRAM. Based on the array-level results from Destiny, we estimate the system-level PPA metrics using our analytical model. This analysis only incorporates the PPA metrics from the memory system (DRAM and GLB) assuming that the PPA of the compute unit is constant. 
With SOT-MRAM as GLB, we observe significant energy and latency improvement over SRAM at 64MB (for inference) and 256MB (for training) (see Fig. \ref{fig_sys_lvl_perf_cv} (a-d) for CV benchmarks and (e-h) for NLP benchmarks). On average, the 64MB SOT-MRAM offers 5$\times$ energy reduction and 2$\times$ latency reduction over 64MB SRAM across all CV models in inference mode. Our DTCO-optimized-SOT-MRAM offers further improvement, 7$\times$ energy, and 8$\times$ latency reduction over SRAM at iso-capacity. We observe that the most contributing factor in energy reduction (>50\%) is the near-zero leakage power of SOT-MRAM compared to high leakage power of SRAM. The improvement is even more in training mode; 6$\times$ (8$\times$ with SOT-opt.) energy reduction and 2$\times$ (9$\times$ with SOT-opt.) latency reduction. With 64MB SOT-MRAM, NLP models in inference mode experience 2$\times$ (3$\times$ with SOT-opt.) energy reduction and 2$\times$ (4$\times$ with SOT-opt.) latency reduction than 64MB SRAM. Like CV benchmarks, with 256MB SOT-MRAM, NLP benchmarks also experience more energy improvement, 6$\times$ (8$\times$ with SOT-opt.), and latency improvement, 2.5$\times$ (4.5$\times$ with SOT-opt.), in training mode. The more improvement in training mode is because of two reasons: (1) GLB size increases from 64MB to 256MB, and (ii) GLB access counts are significantly large (at least 5$\times$) in training. Our DTCO-opt-SOT-MRAM further adds value to PPA by its smaller silicon area, 0.54$\times$ at 64MB and 0.52$\times$ at 256MB of 14nm SRAM at iso-capacity (Fig. \ref{sys_area_comp}).

\begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.65]{Fig_Area_comparison.pdf}
    %\vspace{-10pt}
    \caption{Area improvement of SOT-MRAM and SOT-MRAM-OPT over SRAM}
    \label{sys_area_comp}
    %\vspace{-10pt}
\end{figure}




