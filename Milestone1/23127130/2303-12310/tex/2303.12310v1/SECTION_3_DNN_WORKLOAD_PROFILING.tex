%\section{WORKLOAD AWARE HYBRID MEMORY MODEL}
\section{DNN WORKLOAD PROFILING}
\label{workload_profiling}
Profiling the target workload is a prerequisite for designing an accelerator for the target workload. Assuming that we have a powerful computing system to handle the exhaustive computations of the DL workload, we focus on providing efficient data movement between the compute and memory system to ensure 100\% utilization of computing resources by introducing the workload-aware hybrid memory system. We propose the hybrid memory system by analyzing the Deep Learning model workloads from Computer Vision (CV) and Natural Language Processing (NLP) domain.
We analytically model the on-chip bandwidth requirement and memory access patterns of different parts of the workload during inference and training, \emph {Memory and Compute Model}, to develop the memory system.
%The model is developed considering the memory bandwidth requirement (
%both on-chip to DRAM and %
%compute unit to on-chip), memory access count, and memory size requirement of different parts of the workload. In this section, we explain each consideration in detail.


\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.7]{Fig_2_Accelerator_system_2.pdf}
    %\vspace{-10pt}
    \caption{Block diagram of Accelerator architecture}
    \label{fig:system_architecture}
\end{figure}


\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.9]{Fig_5_Convolution_new.pdf}
    %\vspace{-12pt}
    \caption{CV models workflow breakdown}
    %\vspace{-5pt}
    \label{fig:conv_op}
\end{figure*}


\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.7]{Fig_6_Training_comp_graph.pdf}
    %\vspace{-10pt}
    \caption{Computational graph of DNN training}
    %\vspace{-12pt}
    \label{fig:comp_graph}
\end{figure*}


\subsection{Memory Bandwidth Expression}
We express the required bandwidth (BW) as a function of compute resources and workload. $BW$ (bytes/sec)  is defined as the rate at which data needs to be transferred to/from memory by a processor to fully utilize the computation resources of the processor. Mathematically,
\begin{align}
\label{req_mem_bw}
   BW= \frac{F_p}{OI}
\end{align}
Where $F_p$ = Theoretical peak performance of accelerator (ops/sec) = number of operations the accelerator performs per sec. The $F_{p}$ of a $H_{A} \times W_{A}$  Processing Element (PE) array (Fig. \ref{fig:system_architecture}):

\begin{align}
    \label{peak_fp}
        F_{p} = H_A*W_A*F_{acc}
\end{align}
$F_{acc}$ = Operating frequency of the accelerator. $OI$ = Operational Intensity of Workload (ops/byte) = number of operations performed by the accelerator per byte. It is a workload-dependent parameter and a measure of parallelism of the workload. In the subsequent subsections, we will formulate the $OI$ of Conv. and Fully-Connected (FC) layer to find their BW, respectively. Note that the read and write bandwidth will not be the same for these workloads. 
%For determining write bandwidth, we define $OI$ as the number of outputs generated per byte.




\subsubsection{Read Bandwidth ($BW_{RD}$) of Conv. layer}

To formulate an expression for \emph{OI} of convolution workload: First, we determine the total number of MAC operations, $T_{MAC}$, performed by a $H_{A} \times W_{A}$ PE array per clock cycle
\begin{align}
    T_{MAC}\;=H_{A}\;*\;W_{A}
\end{align}
Second, we figure out how many bytes should be read from memory to utilize all PEs of the accelerator in one clock cycle. In a row stationary dataflow \cite{eyeriss}, it takes ($k_{h}*k_{w}+of_{h}*of_{w})*d_{w}$ bytes of data ($d_{w}$ = data type in bytes, i.e., FP32, BF16 etc.) and \#$(of_{h} * of_{w} * k_{h} *k_{w})$ PEs to generate the partial ofmaps corresponding to one input channel. Depending on the size of the PE array, in each iteration (one complete use of accelerator), multiple input channels can be fit. The input channels (i.e., no. of partial ofmaps) computed by the PE array in each iteration:

\begin{align}
    N_{ich\_per\_stp}\;=\frac{H_A*W_A}{of_{h}*of_{w}*k_h*k_w} 
\end{align}
Total bytes read from memory to utilize all PEs:
\begin{equation}
\begin{aligned}
    T_{byte}=\frac{H_A*W_A}{k_h*k_w*of_{h}*of_{w}}* \\
    {(k_h*k_w+if_{h}*if_{w})*d_{w}}
\end{aligned}
\end{equation}
We divide the total number of MAC operations, $T_{MAC}$, by the total bytes accessed, $T_{byte}$, to find $OI$:
\begin{equation}
    \begin{aligned}
    \label{oi}
        OI=\frac{k_h*k_w*of_{h}*of_{w}}{d_{w}*(k_h*k_w+if_{h}*if_{w})}
    \end{aligned}
\end{equation}
Substituting the expression of $OI$ in equation  \eqref{req_mem_bw} gives $BW_{RD}$ as a function of array size and workload:
\begin{equation}
    \begin{aligned}
        BW_{RD} & = \frac{(k_h*k_w\;+if_{h}*if_{w})*d_{w}}{k_w*k_h*of_{h}*of_{w}}*\\
& \;\;\;\;\;\;\;H_{A}*W_{A}*F_{acc}
    \end{aligned}
\end{equation}
For the symbol meanings please see Fig. \ref{fig:conv_op}.
\subsubsection{Write Bandwidth ($BW_{WR}$) of Conv. Layer}
Partial ofmap of a single input channel requires \#($of_{h}*of_{w}*k_{h}*k_{w}$) PEs. Therefore, $H_{A}\times W_{A}$ PEs generate $(H_{A}*W_{A})/(of_{h}*of_{w}*k_{h}*k_{w})$ ofmaps in each iteration. Each partial ofmap contains $of_{h}*of_{w}$ elements. The total output bytes generated by the PE array in one iteration is, equivalently, the write bandwidth is:
% \begin{equation}
% \begin{aligned}
%     \label{wr_bw}
%     BW_{WR} = \frac{H_{A}*W_{A}}{N_{ofmap\_rw}*N_{ofmap\_cl}*k_{h}*k_{w}}\;*\\ N_{ofmap\_rw}*N_{ofmap\_cl}*d_{width}*F_{acc}
% \end{aligned}
% \end{equation}





\begin{equation}
\begin{aligned}
    \label{wr_bw}
    BW_{WR} = \frac{H_{A}*W_{A} * F_{acc} * d_{w}}{k_{h}*k_{w}}\;
\end{aligned}
\end{equation}


\begin{table}[ht]
\setlength{\tabcolsep}{3pt} 
\centering
\caption{RD/WR bandwidth expression of FC layer for different cases}
\label{table:bw_expression}
\vspace{-8pt}
\begin{tabular}{|cc|c|c|}
\hline
\multicolumn{2}{|c|}{Cases}& $BW_{RD}$& $BW_{WR}$\\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$M<H_{A}$; $N<W_{A}$}}& \multirow{2}{*}{$K<W_{A}$}& \multirow{2}{*}{$\frac{M*N+K*M}{N+K}$}& \multirow{2}{*}{$\frac{K*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \cline{2-4} 
\multicolumn{1}{|c|}{}& \multirow{2}{*}{$K\ge W_{A}$} & \multirow{2}{*}{$\frac{M*N+W_{A}*M}{N+W_{A}}$}& \multirow{2}{*}{$\frac{W_{A}*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$M<H_{A}; N\ge W_{A}$}}  & \multirow{2}{*}{$K<W_{A}$}& \multirow{2}{*}{$\frac{M*W_{A}+K*M}{N+K}$}& \multirow{2}{*}{$\frac{K*W_{A}}{2*W_{A}+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \cline{2-4} 
\multicolumn{1}{|c|}{}& \multirow{2}{*}{$K\ge W_{A}$} & \multirow{2}{*}{$\frac{M*W_{A}+W_{A}*M}{2*W_{A}}$}& \multirow{2}{*}{$\frac{{W_{A}}^{2}}{2*W_{A}+K-1}$} \\
\multicolumn{1}{|c|}{}&&&\\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$M\ge H_{A}; N<W_{A}$}}  & \multirow{2}{*}{$K<W_{A}$}& \multirow{2}{*}{$\frac{H_{A}*N+K*H_{A}}{N+K}$}& \multirow{2}{*}{$\frac{K*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \cline{2-4} 
\multicolumn{1}{|c|}{}& \multirow{2}{*}{$K\ge W_{A}$} & \multirow{2}{*}{$\frac{H_{A}*N+W_{A}*H_{A}}{W_{A}+N}$}& \multirow{2}{*}{$\frac{W_{A}*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$M\ge H_{A};N\ge W_{A}$}} & \multirow{2}{*}{$K<W_{A}$}    & \multirow{2}{*}{$\frac{H_{A}*W_{A}+W_{A}*H_{A}}{W_{A}+K}$} & \multirow{2}{*}{$\frac{W_{A}*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \cline{2-4} 
\multicolumn{1}{|c|}{}& \multirow{2}{*}{$K\ge W_{A}$} & \multirow{2}{*}{$\frac{H_{A}*W_{A}+W_{A}*H_{A}}{2*W_{A}}$} & \multirow{2}{*}{$\frac{{W_{A}^2}}{2*W_{A}+K-1}$}   \\
\multicolumn{1}{|c|}{}&&&\\ \hline
\end{tabular}
%\vspace{-8pt}
\end{table}




%  \setlength{\tabcolsep}{-3.5pt} 
 \setlength{\tabcolsep}{3.2pt}
\begin{table*}[ht]
{\tiny
\caption{Architecture-and-platform-agnostic Memory access and Dataflow of CV model workload during inference and training}
\vspace{-10pt}
\label{conv_analysis}
\begin{tabular}{|l|l|ll|ll|}
\hline
\multicolumn{1}{|c|} {\multirow{2}{*}{\textbf{Data Entity}}} & \multicolumn{1}{c|} {\multirow{2}{*}{\textbf{Data Dimension}}} & \multicolumn{2}{c|}{\textbf{Memory Access}} & \multicolumn{2}{c|}{\textbf{Dataflow \& Reuse Status}} \\ \cline{3-6} 
 &  & \multicolumn{1}{c|}{\textbf{Inference}} & \multicolumn{1}{c|}{\textbf{Training}} & \multicolumn{1}{c|}{\textbf{Inference}} & \multicolumn{1}{c|}{\textbf{Training}} \\ \hline
\multirow{11}{*}{\begin{tabular}{@{}l@{}}{Initial Input (I)}
\end{tabular}} & \multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}{$\begin{aligned}
I_{h}\times I_{w}\times \\ N_{ich}\times N_{bt}
\end{aligned}$}\end{tabular}}
& \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read}: Once*, if UB $\ge$ I, else, multiple* \\ \underline{Write}: No write back\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ \underline{Read}:\\ UB $\rightarrow$ PE core: Once*\\ UB $\rightarrow$ HBM3: None\\ \underline{Write}:\\ PE core $\rightarrow$ UB: None\\ HBM3 $\rightarrow$ UB: Once*, if UB $\ge$ I, \\ else multiple*\end{tabular}}} & \multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read}: Once*, if UB $\ge$ I, else multiple*\\ \underline{Write}: No write back\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ \underline{Read}:\\ UB $\rightarrow$ PE core: At least twice* (one during forward \\ pass, one during backward pass to calculate gradient)
\\ UB $\rightarrow$ HBM3: None\\ \underline{Write}:\\ HBM3 $\rightarrow$ UB: Once*, if UB $\ge$ I, else multiple*\\ PE core $\rightarrow$ UB: None\end{tabular}} & \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $HBM3 \rightarrow UB \rightarrow PE\; core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ Multiple times for convolutional \\ resue and over different filters\end{tabular}}} & \multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ \underline{Forward}: \\ $HBM3 \rightarrow UB \rightarrow  PE\; core$\\ \underline{Backward}: $UB \rightarrow PE\;core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ \underline{Forward}:\\ Multiple times for convolutional \\ resue and over different filters\\ \underline{Backward}:\\ Multiple times for gradient calculation \\ of  different filters\end{tabular}} \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\ \hline
\multirow{14}{*}{\begin{tabular}[c]{@{}l@{}}Output feature\\ map (OFMAP)\end{tabular}} & \multirow{7}{*}{\begin{tabular}{@{}l@{}}Conv. Layer:\\
$\begin{aligned} of_{h} \times of_{w} \times \\ N_{och} \times N_{bt}\end{aligned}$\end{tabular}} &  \multicolumn{1}{l|}{\multirow{14}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read \& Write}: None, if UB $\ge$ OFMAP, \\ else multiple*\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ \underline{Read}:\\ UB $\rightarrow$ PE core: None\\ UB $\rightarrow$ HBM3: None, if UB $\ge$ OFMAP, \\else multiple*\\ \underline{Write}:\\ PE core $\rightarrow$ UB: Once*\\ HBM3 $\rightarrow$ UB: None\\ \textbullet{} {\textbf{SRAM}}\\ Multiple* R/W for partial OFMAP \\ storage and local reuse\end{tabular}}} & \multirow{14}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read \& Write}: None, if UB $\ge$ OFMAP, else multiple*\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ \underline{Read}:\\ UB $\rightarrow$ PE core: None\\ UB $\rightarrow$ HBM3: None, if UB $\ge$ OFMAP, else multiple*\\ \underline{Write}:\\ PE core $\rightarrow$ UB: Once*\\ HBM3 $\rightarrow$ UB: None\\ \textbullet{} {\textbf{SRAM}}\\ Multiple* R/W for partial OFMAP storage and \\ local reuse\end{tabular}} & \multicolumn{1}{l|}{\multirow{14}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $PE\; core \rightarrow SRAM \rightarrow UB$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ Multiple times for partial sum \\generation\end{tabular}}} & \multirow{14}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ \underline{Forward:} \\ $PE\; core \rightarrow SRAM \rightarrow UB$\\  \underline{Backward:} Not Applicable (N/A) \\ \textbullet{} {\textbf{Reuse in PE core}}\\ \underline{Forward:} Multiple times for partial \\sum  generation \\  \underline{Backward:} Not Applicable (N/A) \end{tabular}} \\ 
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\ \cline{2}
 & \multirow{7}{*}{\begin{tabular}{@{}l@{}}FC Layer:\\
$\begin{aligned} N_{bt} \times m_{fc} \end{aligned}$\end{tabular}} & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\ \hline
\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}Intput feature\\ map (IFMAP)\end{tabular}}& \multirow{6}{*}{\begin{tabular}{@{}l@{}}Conv. Layer: \\{$\begin{aligned}if_{h} \times if_{w} \times \\ N_{ich} \times N_{bt}\end{aligned}$} \end{tabular}}&  \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read \& Write}: None, if UB $\ge$ IFMAP,\\ else multiple*\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ \underline{Read}:\\ UB $\rightarrow$ PE core: Once*\\ UB $\rightarrow$ HBM3: None\\ \underline{Write}:\\ PE core $\rightarrow$ UB: None\\ HBM3 $\rightarrow$ UB: None, if UB $\ge$ IFMAP, \\ else multiple*\end{tabular}}} & \multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read \& Write}: None, if UB $\ge$ IFMAP, else multiple*\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ \underline{Read}: \\ UB $\rightarrow$ PE core: At least twice* (one during forward \\ pass, one during backward pass to calculate gradient)
\\ UB $\rightarrow$ HBM3: None\\ \underline{Write}:\\ PE core $\rightarrow$ UB: None \\ HBM3 $\rightarrow$ UB: None, if UB $\ge$IFMAP, else multiple*\end{tabular}} & \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $UB \rightarrow PE\;core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ Multiple times for convolutional \\ reuse and over multiple filters\end{tabular}}} & \multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ \underline{Forward \& Backward}: \\ $UB \rightarrow PE\;core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ \underline{Forward}:\\ Multiple times for convolutional reuse \\ and over multiple filters\\ \underline{Backward}:\\ Multiple times for gradient calculation \\ of multiple filters\end{tabular}} \\ 
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\ 
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\\cline{2}
 & \multirow{6}{*}{\begin{tabular}{@{}l@{}}FC Layer: \\{$\begin{aligned} N_{bt} \times n_{fc}\end{aligned}$} \end{tabular}} & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\ \hline
\multirow{11}{*}{Weights (W)} & \multirow{6}{*}{\begin{tabular}{@{}l@{}}Conv. Layer: \\ {$\begin{aligned}k_{h} \times k_{w} \times \\ N_{ich} \times N_{och}\end{aligned}$}\end{tabular}} & \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read}: Once, if $H_A \times W_A$ $\ge$ W,\\ else multiple*\\ \underline{Write}: No write back\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ \underline{Read \& Write}: None\end{tabular}}} & \multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read}: Once*, if UB $\ge$ W, else multiple*\\ \underline{Write}: Once* (to write the trained weights)\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ \underline{Read}:\\ UB $\rightarrow$ PE core: At least thrice* (for forward pass,\\input gradient calculation, and trained filter weights)
\\ UB $\rightarrow$ HBM3: Once* %(to write the trained weights)
\\ \underline{Write}:\\ PE core $\rightarrow$ UB: At least once* %(to write trained \\ weights)
\\ HBM3 $\rightarrow$ UB: Once*, if UB $\ge $W, else multiple*\end{tabular}} & \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $HBM3 \rightarrow PE\;core/SARM$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ Multiple times for convolutional reuse \\ and for different samples of minibatch\\ \\ \\ Weights are double-buffered to PE \\ core's SRAM to hide DRAM access \\ latency\end{tabular}}} & \multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ \underline{Forward}: \\ $HBM3 \rightarrow UB \rightarrow PE\;core$\\ \underline{Backward}: $UB \rightarrow PE\;core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ \underline{Forward}: \\ Multiple times for convolutional reuse \\ and for different samples of minibatch\\ \underline{Forward}: \\ Multiple times to calculate the gradient \\ of input/IFMAP\end{tabular}} \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\\cline{2}
 & \multirow{6}{*}{\begin{tabular}{@{}l@{}}FC Layer: \\ {$\begin{aligned} n_{fc} \times m_{fc}\end{aligned}$}\end{tabular}} & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\ \hline
\multirow{9}{*}{\begin{tabular}[c]{@{}l@{}}Upstream gradient,\\ gradient of loss\\ function, gradient \\ of activation function, \\ optimizer states\end{tabular}} & \multirow{9}{*}{\begin{tabular}[c]{@{}l@{}}Same as current \\ layer's IFMAP\\ and filter weights\end{tabular}} & \multicolumn{1}{l|}{\multirow{9}{*}{Not Applicable (N/A)}} & \multirow{9}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read \& Write}: None, if UB $\ge$ all grad., else multiple*\\ \textbullet{} {\textbf{Unified Buffer (UB}}\\ \underline{Read}:\\ UB $\rightarrow$ PE core: Once*\\ UB $\rightarrow$ HBM3: None, if UB $\ge$ all grad., else multiple* \\ \underline{Write}:\\ PE $\rightarrow$ UB: Once*\\ HBM3 $\rightarrow$ UB: None, if UB $\ge$ all grad., else multiple* \end{tabular}} & \multicolumn{1}{l|}{\multirow{9}{*}{Not Applicable (N/A)}} & \multirow{9}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $PE\;core \rightarrow SRAM \rightarrow UB$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ For previous layer's gradient calculation\end{tabular}} \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
%  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
  &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
   &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
      &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\
 &  & \multicolumn{1}{l|}{} &  & \multicolumn{1}{l|}{} &  \\ \hline 
\multicolumn{6}{l}{\multirow{2}{*}{*Exact access count depends on UB size, batch-size, IFMAP \& OFMAP size, PE core dimension, kernel size, number of filters, and dataflow mapping}}
%\multicolumn{6}{l}{} 
\end{tabular}}
%\vspace{-8pt}
\end{table*}





\subsubsection{$BW_{RD}$ \& $BW_{WR}$ of FC layer}
\label{bw_fc_layer}
The systolic array is a widely used architecture to perform GEMM operation \cite{tpu}. %The required Global buffer bandwidth of a systolic array-based architecture depends on the array dimensions, operand matrix dimensions, and the operational intensity of the workloads. 
Depending on the array dimension ($H_{A}\times W_{A}$) and operand matrix dimension (input matrix: $K\times M$, weight matrix: $M\times N$, and output matrix: $K\times N$), we formulate required Read and Write GLB bandwidth for four different cases: (i) Weight matrix dimensions (both) are less than the systolic array dimensions ($M<H_{A}, N<W_{A}$), (ii) Height of weight matrix is less than the height of systolic array, but the width of weight matrix is larger than or equal to the width of the systolic array ($M<H_{A}, N \ge W_{A}$), (iii) Height of weight matrix is larger than or equal to the height of systolic array, but width of the weight matrix is less than the width of the systolic array  ($M \ge H_{A}, N<W_{A}$), and (iv) Both height and width of weight matrix are larger than or equal to the height and width of systolic array respectively ($M \ge H_{A}, N\ge W_{A}$). 


%(see Fig. \ref{bw_calc_cases} for visualization).

In a weight stationary dataflow, it takes $N$ clock cycles to load the weight matrix into the systolic array. Once the weights are loaded, the input matrix is streamed from left to right and the outputs are collected downward. %It takes $K$ clock cycles to load the input matrix into the systolic array. 
The input matrix's first column reaches the weight matrix's last column at $2N$ clock cycles. The last (or $K^{th}$) column of the input matrix reaches the last column of weight matrix after $2N+K-1$ clock cycles and generates the output matrix, $K\times N$. %(fig \ref{bw_calc_all}). 
Based on the above dataflow and mapping, the peak read-write bandwidth per clock cycle for different cases is summarized in Table \ref{table:bw_expression}. The expressions are shown for weight stationary dataflow. However, the above expressions hold for the output and input stationary dataflow, except the fixed and streamed matrix getting swapped.




\subsection{Memory Access Patterns}
\label{mem_access}
Our proposed memory system consists of HMB3 (off-chip memory), a large Unified Buffer (UB) with multiple SOT-MRAM banks, a smaller double-buffered SRAM, and PE reg file specific to each PE unit (Fig. \ref{fig:system_architecture}). The banks inside SOT-MRAM are optimized through a DTCO between the SOT-MRAM parameters and the workload requirements. The double-buffered SRAM holds the weights (during inference) and partial outputs. Its size is determined by the PE array size and the largest partial output size. In this subsection, we analyze the memory access patterns of CV and NLP models for the proposed memory system. The terms GLB and UB are used interchangeably in the paper to represent the large on-chip memory.



\setlength{\tabcolsep}{1pt} 
\begin{table*}[!ht]
\tiny
\caption {Architecture-and-platform-agnostic Memory access and Dataflow of Transformer-based NLP model workload during inference and training}\vspace{-10pt}
\label{Transformer_analysis}
%\resizebox{\textwidth}{!}{%
\begin{tabular}{llllll}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{Data Entity}}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Data Dimension}}} & \multicolumn{2}{c|}{\textbf{Memory Accesses}} & \multicolumn{2}{c|}{\textbf{Dataflow \& Reuse Status}} \\ \cline{3-6} 
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\textbf{Inference}} & \multicolumn{1}{c|}{\textbf{Training}} & \multicolumn{1}{c|}{\textbf{Inference}} & \multicolumn{1}{c|}{\textbf{Training}} \\ \hline
\multicolumn{6}{c}{\emph{Encoder Layer}} \\ \hline
\multicolumn{1}{|l|}{\multirow{11}{*}{Input Sequence (I)}} & \multicolumn{1}{l|}{\multirow{11}{*}{$N_{sql}\times N_{bt}$}} & \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read}: Once*, if UB $\ge$ I, else multiple*\\ \underline{Write}: No write back\\ \textbullet{} {\textbf{Unified Buffer (UB)}:}\\ \underline{Read}:\\ UB$\rightarrow$ PE core: Once*\\ UB$\rightarrow$ HBM3: None\\ \underline{Write}:\\ PE core $\rightarrow$ UB: None\\ HBM3 $\rightarrow$ UB: Once*, if UB $\ge$ I,\\ else multiple*\end{tabular}}} & \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read}: Once*, if UB $\ge$ I, else multiple*\\ \underline{Write}: No write back\\ \textbullet{} {\textbf{Unified Buffer (UB)}:}\\ \underline{Read}:\\ UB$\rightarrow$ PE core: Twice*(one during forward pass, \\one during  backward pass to calculate gradient)\\ UB$\rightarrow$ HBM3: None\\ \underline{Write}:\\ PE core$\rightarrow$ UB: None\\ HBM3$\rightarrow$ UB: Once*, if UB $\ge$ I, else multiple\end{tabular}}} & \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $HBM3\rightarrow UB \rightarrow PE\; core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ No resue\end{tabular}}} & \multicolumn{1}{l|}{\multirow{11}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ \underline{Forward}: \\ $HBM3\rightarrow UB \rightarrow PE\; core$\\ \underline{Backward}: $UB\rightarrow PE\; core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ \underline{Forward \& backward pass}: No reuse\end{tabular}}} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% \multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \hline
\multicolumn{1}{|l|}{\multirow{6}{*}{Embedded Input}} & \multicolumn{1}{l|}{\multirow{7}{*}{\begin{tabular}[c]{@{}l@{}}{$\begin{aligned}N_{sql}\times N_{em}\times\\ N_{bt}\end{aligned}$}\end{tabular}}} & \multicolumn{1}{l|}{\multirow{19}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read \& Write}: None, if UB can hold full \\batch, else multiple*\\ \textbullet{} {\textbf{Unified Buffer (UB)}:}\\ \underline{Read}: \\ UB$\rightarrow$ PE core: Once*, if PE array can \\ produce complete output in one iteration, \\ else multiple*\\ UB$\rightarrow$ HBM3: None, if UB can hold full\\ batch,  else multiple*\\ \underline{Write}:\\ PE core $\rightarrow$ UB: Once*, if PE array can \\ produce complete output  in one iteration,\\ else multiple*\\ HBM3 $\rightarrow$ UB: None, if UB can hold full\\ batch,  else multiple*\\ \textbullet{} {\textbf{SRAM:}}\\ Multiple R/W for partial sums and local reuse\end{tabular}}} & \multicolumn{1}{l|}{\multirow{19}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read \& Write}: None, if UB can hold full batch, \\ else multiple*\\ \textbullet{} {\textbf{Unified Buffer (UB)}:}\\ \underline{Read}: \\ UB$\rightarrow$ PE core: twice* (once at forward pass \\ \& once at backward pass for local gradient \\ calculation)\\ UB$\rightarrow$ HBM3: None, if UB can hold full batch,\\ else multiple*\\ \underline{Write}:\\ PE core $\rightarrow$ UB: Once* (during forward pass \\ for next layer \& for future access in backprop)\\ HBM3 $\rightarrow$ UB: None, if UB can hold full batch,\\ else multiple*\\ \textbullet{} {\textbf{SRAM:}}\\ Multiple R/W for partial sums and local reuse\end{tabular}}} & \multicolumn{1}{l|}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $PE\;core \rightarrow SRAM/UB$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ As input to $Q$, $K$, and $V$ layer\\ for  next stage\end{tabular}}} & \multicolumn{1}{l|}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ \underline{Forward}: $PE\; core\rightarrow SRAM \rightarrow UB$\\ \underline{Backward}: $UB\rightarrow PE\; core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ \underline{Forward \& Backward pass}: To calculate $Q$, $K$, \\ and $V$ layer activation and weight gradient\end{tabular}}} \\
% \multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% \multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|c|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\cline{1-2} \cline{5-6} 
\multicolumn{1}{|l|}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Query(Q), Key(K),\\ Value(V), Output of \\ Concat. Layer (Z),\\ Output of Multi-head\\ attention Layer (E)\end{tabular}}} & \multicolumn{1}{l|}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}$\begin{aligned} N_{sql}\times N_{em}\times \\ N_{bt} \end{aligned}$\end{tabular}}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $PE\;core \rightarrow SRAM/UB$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ For next stage activation \\ calculation\end{tabular}}} & \multicolumn{1}{l|}{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ \underline{Forward}: $PE\;core\rightarrow SRAM \rightarrow UB$\\ \underline{Backward}: $UB\rightarrow PE\;core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ \underline{Forward \& Backward pass}: For next stage \\ activation \& gradient calculation\end{tabular}}} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\  \cline{1-2} \cline{5-6}
\multicolumn{1}{|l|}{\multirow{2}{*}{Attention Filter (AF)}} & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}$\begin{aligned} N_{sql}\times N_{sql}\times \\ N_{bt}\end{aligned}$ \end{tabular} }} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{{\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $PE\;core \rightarrow SRAM/UB$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ As input to $K$ \& $V$ of Encoder-\\Decoder  SelfAttention Layer\end{tabular}}} } & \multicolumn{1}{l|}{\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ \underline{Forward}: $PE\; core\rightarrow SRAM \rightarrow UB$\\ \underline{Backward}: $UB\rightarrow PE\; core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ \underline{Forward \& Backward pass}: To find $K$, $V$  and \\ weight gradient of Encoder-Decoder Self-\\Attention Layer\end{tabular}}} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ 
\multicolumn{1}{|l|}{\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Final Encoder \\ Output Layer\end{tabular}}} &  \multicolumn{1}{l|}{\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}} $\begin{aligned}  N_{sql}\times N_{em}\times \\ N_{bt}\end{aligned}$\end{tabular}}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\multirow{5}{*}{}} & \multicolumn{1}{l|}{} \\ \cline{1-2}
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% \multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% \multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% \multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \hline
\multicolumn{1}{|l|}{\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Upstream Gradient\\ ($\delta_{x}$)\\ x = 1,2,3,...,N\end{tabular}}} & \multicolumn{1}{l|}{\multirow{15}{*}{\begin{tabular}[c]{@{}l@{}}Same as current \\ layer activation\end{tabular}}}  & \multicolumn{1}{l|}{\multirow{15}{*}{Not Applicable (N/A)}} & \multicolumn{1}{l|}{\multirow{15}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read \& Write}: None, if UB $\ge$ all grad.,\\ else multiple*\\ \textbullet{} {\textbf{Unified Buffer (UB)}:}\\ \underline{Read}: \\ UB$\rightarrow$ PE core: Once*, if PE core $\ge$ all grad.,\\ else multiple*\\ UB$\rightarrow$ HBM3: None, if UB $\ge$ all grad.,\\   else multiple*\\ \underline{Write}:\\ PE core $\rightarrow$ UB: Once* (For previous layer's $\delta$ )\\ HBM3 $\rightarrow$ UB: None, if UB $\ge$ all grad.,\\  else multiple*\\ \textbullet{} {\textbf{SRAM:}}\\ Multiple R/W for partial sums and local reuse  \end{tabular}}} & \multicolumn{1}{l|}{\multirow{15}{*}{Not Applicable (N/A)}} & \multicolumn{1}{l|}{\multirow{15}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $PE\; core\rightarrow SRAM \rightarrow UB$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ For previous layer's $\delta$ calculation\end{tabular}}} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \cline{1-1}
\multicolumn{1}{|l|}{\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Gradient of Loss \\ ($\Delta$L) \\ (only for final \\ output layer)\end{tabular}}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \cline{1-1}
\multicolumn{1}{|l|}{\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Gradient of \\ activations\end{tabular}}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% \multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% \multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \hline

\multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Embedding \\ Weights ($W^{em}$)\end{tabular}}} & \multicolumn{1}{l|}{\multirow{2}{*}{$N_{vocab}\times N_{em}$}} & \multicolumn{1}{l|}{\multirow{18}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read}: \\ HBM3 $\rightarrow$ PE core: Once*, if PE array can\\ hold full weight matrix, else multiple*\\ \underline{Write}:\\ PE core $\rightarrow$ HBM3: No write back\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ No read and write\\ \end{tabular}}} & \multicolumn{1}{l|}{\multirow{18}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read}: Once*, if UB $\ge$ W, else multiple*\\ \underline{Write}: At least once* (to write back updated \\ weights after training)\\ \textbullet{} {\textbf{Unified Buffer (UB)}:}\\ \underline{Read}: \\ UB$\rightarrow$ PE core: at least thrice* (one at forward \\ pass, two at backward pass to calculate $\Delta$W \\ and updated W)\\ UB$\rightarrow$ HBM3: At least once* (to write back \\ updated weights after training)\\ \underline{Write}:\\ PE core $\rightarrow$ UB: At least once* (to update \\ weights)\\ HBM3 $\rightarrow$ UB: Once*, if UB $\ge$ W, else multiple*\\ \textbullet{} {\textbf{SRAM:}}\\ Multiple R/W for partial W, $\Delta$ W \\per minibatch iteration to facilitate reuse\end{tabular}}} & \multicolumn{1}{l|}{\multirow{18}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $HBM3 \rightarrow PE\;core /SRAM$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ Multiple times for each sample \\per minibatch \\ \\ Weights are doubled buffered to \\ PE core's SRAM to hide DRAM \\ access latency.\end{tabular}}} & \multicolumn{1}{l|}{\multirow{18}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ \underline{Forward}: $HBM3 \rightarrow UB \rightarrow PE\; core$\\ \underline{Backward}: $UB \rightarrow PE\; core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ \underline{Forward}: Multiple times for each sample\\ per minibatch\\ \underline{Backward}: For next stage gradient \\ calculation\end{tabular}}} \\
%\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \cline{1-2}
\multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}{Query Weights ($W^{Q}$)}\end{tabular}}} & \multicolumn{1}{l|}{\multirow{2}{*}{$(N_{em} \times d_{q})*h$}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}\\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \cline{1-2}
\multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}{@{}l@{}}{Key Weights ($W^{K}$)}\end{tabular}}} & \multicolumn{1}{l|}{\multirow{2}{*}{$(N_{em} \times d_{k})*h$}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}\\ 
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \cline{1-2}
\multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}{@{}l@{}}{Value Weights ($W^{V}$)}\end{tabular}}} & \multicolumn{1}{l|}{\multirow{2}{*}{$(N_{em} \times d_{v})*h$}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ 
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \cline{1-2}
\multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}{@{}l@{}}{Concat. Weights ($W^{O}$)}\end{tabular}}} & \multicolumn{1}{l|}{\multirow{2}{*}{$N_{em} \times N_{em}$}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} &  \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \cline{1-2}
\multicolumn{1}{|l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}FFN FC1 Weights \\($W^{FC1}$)\end{tabular}}} & \multicolumn{1}{l|}{\multirow{2}{*}{$N_{em} \times d_{ff}$}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ 
%\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \cline{1-2}
\multicolumn{1}{|l|}{\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}FFN FC2 Weights \\($W^{FC2}$)\end{tabular}}} & \multicolumn{1}{l|}{\multirow{3}{*}{$d_{ff} \times N_{em}$}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \cline{1-2}
\multicolumn{1}{|l|}{\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Output classifier\\ weights ($W^{cl}$)\end{tabular}}} & \multicolumn{1}{l|}{\multirow{3}{*}{\begin{tabular}{@{}l@{}}{$\begin{aligned}(N_{sql}*N_{em})\times \\ N_{vocab}\end{aligned}$}\end{tabular}}} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ \hline
\multicolumn{1}{|l|}{\multirow{9}{*}{Optimizer states}} & \multicolumn{1}{l|}{\multirow{9}{*}{}} & \multicolumn{1}{l|}{\multirow{9}{*}{Not Applicable (N/A)}} & \multicolumn{1}{l|}{\multirow{9}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{HBM3 DRAM}}\\ \underline{Read \& Write}: None\\ \textbullet{} {\textbf{Unified Buffer (UB)}}\\ \underline{Read}:\\ $UB \rightarrow PE\;core$: Once* \\ \underline{Write}:\\ $PE\;core \rightarrow UB$: Once*\\ \textbullet{} {\textbf{SRAM}}\\ \underline{Read \& Write}: None\end{tabular}}} & \multicolumn{1}{l|}{\multirow{9}{*}{Not Applicable (N/A)}} & \multicolumn{1}{l|}{\multirow{9}{*}{\begin{tabular}[c]{@{}l@{}}\textbullet{} {\textbf{Dataflow}}\\ $UB \rightarrow PE\; core$\\ \textbullet{} {\textbf{Reuse in PE core}}\\ No reuse\end{tabular}}} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
% \multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\ 
% \multicolumn{1}{|l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} \\
\hline
\multicolumn{6}{c}{\emph{Decoder Layer}} \\ \hline
\multicolumn{6}{|l|}{\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}   The decoder layer has two multi-head attention layers; (i) Decoder self-attention layer and (ii) Encoder-Decoder attention layer. Each of them has similar data entities ($Q,\;K,\;V, \;Z,\;E,\;AF,\;W^{Q},\;W^{K},\; W^{V},\; W^{O}$) \\ and \; $W^{FC1},\;W^{FC2}$ with same dimension. However, the sequence length might vary from that of the Encoder's one.\end{tabular}}} \\
\multicolumn{6}{|l|}{} \\
\multicolumn{6}{|l|}{} \\ \hline
\multicolumn{6}{l}{\multirow{2}{*}{*Exact access count depends on UB size, weight matrix, PE core dimension, and dataflow mapping}}\\
\end{tabular}
%\vspace{-10pt}
\end{table*}




\subsubsection{CV models}

%Unlike the conventional GPU-based on-chip memory hierarchy where there are multiple levels of cache memory (such as Register file, L0, L1 instruction cache, L1, L2 data cache, etc.) we propose a memory system of large unified buffer composed of multiple banks of SOT-MRAM, a smaller SRAM buffer and PE reg file specific to each PE unit (fig. \ref{fig:system_architecture}). The banks inside SOT-MRAM are again optimized through a DTCO between the SOT-MRAM parameters and the workload requirements. 
%Some operand data are directly loaded from (cite nvidia ample gpu white paper)


Deep Residual Networks, having convolutional layers at their core, dominate the Computer Vision domain. The input images are convolved with the layer-specific filter weights to produce the output feature map (\emph{OFMAP}). The \emph{OFMAP} goes through the pooling and normalization layers to act as input (\emph{IFMAP}) to the next layer. The linear and softmax layer at the end finally recognizes the image (Fig. \ref{fig:conv_op}). The dimensionality of the key components of such models depends on the model architecture (Table \ref{conv_analysis} column 1 \& 2). Depending on the UB size, activation \& weight size, the minimum memory accesses requirement of each layers activations and weights in different levels of memory hierarchy during inference and training are listed in col. 3 \& 4 of Table \ref{conv_analysis}, respectively. During inference, the read-only weights are directly loaded from HBM3 to the register file of each PE unit, bypassing the UB. However, we use double-buffered SRAM to hide the off-chip access latency. While the array is computing with loaded weights, the next set of weights is temporarily written to the SRAM buffer to hide the off-chip access latency behind the PE array computation latency. \emph{Read: Once*} under heading HBM3 DRAM means the whole chunk of data, for example, \emph{Initial Input(I)}, is read once from DRAM given that the Unified Buffer (UB) is large enough to hold all samples in the minibatch. $UB\; \rightarrow PE\;core$ means the entity is read from $UB$ and written into or operated inside \emph{PE core}. We also provide a high-level dataflow and reuse scope for each entity during inference and training (col. 5 \& 6 of Table \ref{conv_analysis}, respectively). To illustrate, the last column of the first row indicates input images' dataflow and reuse status during training. During the forward pass, input images are read from HBM3, written to UB, and read from UB to be operated inside PEs core. For weight gradient calculation, the inputs are read from UB to PE core during backpropagation, assuming that the UB is large enough to hold the input images along with the generated ofmap of the current layer, thus avoiding the DRAM accesses during backward pass. \emph{Reuse in PE core} represents at forward pass, the inputs can be reused multiple times for convolutional and filter reuse. It can also be reused multiple times during backpropagation to calculate the gradients of different filters. Row 5 of Table \ref{conv_analysis} shows the training-specific ephemeral entities with their dimension, memory access count, and reuse status.
% such as Upstream gradients, the gradient of loss and activation functions, and optimizer states with their dimension, memory access count, and reuse status. 
The training workflow is complicated and requires many more memory accesses (both off-chip and on-chip) compared to inference. For example, to calculate the weight gradients of Layer 1, it requires the current layer's activation gradient $\frac{da_1}{dz_1}$, input ($a_0$), next layer's weight ($W_{2}$) and the upstream gradient from Layer 2 ($\delta_{1}$) (as shown in  Fig. \ref{fig:comp_graph}). 



% We summarize the memory access requirements and the dataflow of the CNN-based DNN workload for both inference and training in the rest columns of Table  \ref{conv_analysis}. We follow the conventions similar to Table \ref{Transformer_analysis}. The analysis is based on the most optimized row-stationary dataflow. However, it is also applicable to all other dataflow, such as weight stationary, input, and output stationary dataflow as we generalized the analysis in the workload level. 
% %In a row stationary dataflow, the kernel rows are loaded into the PE array and kept stationary, ifmaps are shifted by stride size, and partial sums are accumulated vertically. 
% The last row of the table contains the memory access and the dataflow requirements for the training-only entities, such as upstream gradients, gradient of loss function, and gradient of activation functions.
% To start off, the input images are loaded from DRAM to UB and the weights are directly loaded from DRAM to PE array. 



\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.7]{Fig_4_Transformer.pdf}
    %\vspace{-8pt}
    \caption{Transformer model workflow breakdown}
    %\vspace{-15pt}
    \label{fig:transformer}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Natural Language Processing (NLP)} 




We analyze the state-of-the-art Transformer model \cite{vaswani2017attention} as the representative of the NLP domain. The input sequence propagates through the embedding layer and different sublayers of the encoder stacks to extract different linguistic features and inter-token dependency of the input sequence. The decoder stacks then generate the output sequence by taking the encoded input sequence from the encoder stack and the output sequence generated by itself in the previous timesteps (Fig \ref{fig:transformer}). The input sequence multiplied by different layer weights takes different activation names and shapes (Table \ref{Transformer_analysis} col. 1 \& 2) throughout the model operation. We summarize the minimum memory accesses requirement, dataflow and reuse scopes of each layers (sub-layer) activations and weights in different levels of memory hierarchy during inference and training are in Table \ref{Transformer_analysis}. We follow the same convention as Table \ref{conv_analysis}. We illustrate one cell of Table \ref{Transformer_analysis} to help the readers naviagate the table. The last column of the second row indicates the dataflow and reuse status of Embedded input during training. During forward pass, the embedded inputs are generated inside the PE core, partial sums are accumulated in the SRAM, and finally, the output is written to Unified buffer (UB). For weight gradient calculation, the embedded inputs are read from UB to PE core during backpropagation. \emph{Reuse inside PE core} represents at forward pass, the embedded input can be reused thrice as input to Key, Query, and Value linear layer. It can also be reused thrice during backpropagation to calculate the weight gradient of the Key, Query, and Value linear layer. 



We consider the memory access counts in terms of how many times the data entities operate throughout the workflow. However, the exact access count depends on the model architecture, hyperparameters, hardware platforms, dataflow \& mapping such as IFMAP, OFMAP, word embedding, input sequence length, weight size, batch size, UB size, SRAM size, PE core dimension, row-stationary dataflow, etc. 
%The analysis is based on the row-stationary dataflow. However, it is also applicable to all other dataflows, such as weight stationary, input, and output stationary dataflow as we generalized the analysis in the workload level. 
