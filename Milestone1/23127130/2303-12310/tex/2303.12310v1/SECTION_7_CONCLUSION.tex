\section{Conclusion}
\label{conclusion}
We proposed an efficient and high-performance memory system with SOT-MRAM for AI accelerators in this work. Guided by detailed target workload characterization, our memory system comprises of HBM3 DRAM, a DTCO-enabled SOT-MRAM GLB and a small SRAM buffer. Our large SOT-MRAM GLB significantly reduces the energy and latency by reducing expensive DRAM accesses while still having acceptable on-chip access energy and latency, achieving overall system-level high performance. We finally demonstrate that our memory system performs 8$\times$ and 9$\times$ better in terms of energy and latency respectively on CV benchmarks in training (7 and 8 times better in inference) and 8$\times$ and 4.5$\times$ better in terms of energy and latency respectively on NLP benchmarks in training (3 and 4 times better in inference) while consuming only around 50\% of SRAM area at iso-capacity.