\section{Introduction}
\label{intro}
%\IEEEPARstart{T}{he} proliferation of Artificial Intelligence (AI) and Deep Learning (DL) has precipitated the microprocessor industry to experience a large amount of data processing and data storage. With a significant amount of research effort on Design Specific Architecture (DSA) for Deep Learning domains, large-scale data processing is no more overwhelming for computing devices \cite{dnn_survey}\cite{tpu}\cite{amp100_gpu}. However, the lack of efficient and high-performance data communication between the computing and memory element (known as memory wall or memory bottleneck) masks the improvement coming from the efficient compute system \cite{cao2021mobile}. One promising solution to address the memory bottleneck of AI-specific workload is to increase the on-chip memory capacity\cite{park2018deep}. The area inefficiency, leaky nature, and process variation of existing CMOS-based memory (i.e., SRAM) at advanced technology nodes, combined with an increasing demand for on-chip memory capacity, have led researchers to explore potential alternatives of SRAM as on-chip memory.



\IEEEPARstart{T}{he} proliferation of Artificial Intelligence (AI) and Deep Learning (DL) has precipitated the computing hardware community to continually design innovative AI/DL accelerators with  large data processing capabilities. Research shows that the AI/DL model accuracy improves as training data set size grows \cite{DL_data1}. With increasing data set, model size also grows. Consequently, memory demand in AI/DL accelerators will also grow asymptotically linearly with model and data size \cite{DL_data1} \cite{dnn_survey}. As a result, the bottleneck for state-of-the-art AI/DL models in the accelerator hardware is now memory rather than data and compute availability, and we expect this trend to worsen in the future \cite{dnn_survey}\cite{tpu}\cite{amp100_gpu}.

The lack of efficient and high-performance data flow between the computing and memory element (i.e., the memory wall or memory bottleneck) masks the improvement coming from the efficient compute system \cite{cao2021mobile}. One promising solution to the memory bottleneck of AI-specific workload is to increase the on-chip memory capacity\cite{park2018deep}. For both training and inference, the on-chip memory capacity in the accelerator needs to be increased to ensure that the intermediate activations, as well as the weights of the current layer, can be loaded. Moreover, significantly more memory is required during training to store the gradients and optimizer states. Inadequate on-chip memory capacity will cause frequent DRAM accesses which will exacerbate energy costs, as well as stall the compute cores of AI/DL accelerator until the data is fetched. Because of this large capacity demand, an SRAM-based on-chip memory system can be detrimental due to leakage energy and area inefficiency. 



\begin{figure*}[ht]
	\centering
	\includegraphics[width=1.0\textwidth]{Fig_1_Paper_concept_new.pdf}
	%\vspace{-20pt}
	\caption{Workflow of closed-loop analysis for system and device level optimization for AI/Deep Learning Accelerator Design}
	\label{fig:paper_concept}
	%\vspace{-10pt}
\end{figure*}

% The promising features, such as high density, near-zero leakage power, immunity against radiation-induced soft errors, and CMOS compatibility of non-volatile memory (NVM) technologies, attracted researchers from academia and industry to study different NVM technologies actively \cite{memory_trend}. In addition to retaining all NVM-inherent properties, Spin Transfer Torque (STT) MRAM exhibits superiority as an embedded storage element in the cache over the other NVM variants, e.g., Phase Change Memory (PCM), and Resistive RAM (ReRAM) \cite{dac_19}\cite{memory_trend} because of its higher endurance (number of times data can be written to the memory) and recent improvement in read-write access latency and energy \cite{stt_ai_us}. As a result, STT-MRAM has already shifted its gear from the R\&D phase to commercialization as the NAND-based embedded flash replacement \cite{dac_19} \cite{recent-progress-in-SOT_fab3}.


% The promising features, such as high density, near-zero leakage power, immunity against radiation-induced soft errors, and CMOS compatibility of  emerging Spin-based non-volatile (NVM)  magnetic memory (i.e., MRAM)  technologies, attracted researchers from academia and industry \cite{memory_trend}.  However, MRAM in its regular form cannot be used in AI accelerators due to its slow write speed and high write energy \cite{ recent-progress-in-SOT_fab3}\cite{optimized_SOT_imec}.

% Spin Transfer Torque (STT) MRAM, has already shifted its gear from the R\&D phase to commercialization as the NAND-based embedded flash replacement \cite{dac_19} \cite{recent-progress-in-SOT_fab3}. STT-MRAM, a two-terminal magnetic memory with Magnetic Tunnel Junction (MTJ) as the storing element, flows a bidirectional spin-polarized current through the MTJ for read-write operation \cite{stt_eqn1}. The major challenges of STT-MRAM, such as poor write performance, reliability issues, e.g., Read Disturbance (RD), retention failure, \cite{recent-progress-in-SOT_fab3}\cite{dualport_fieldfree_fab2}, stem from two main reasons. First, the high write current flowing through the MTJ accounts for almost $10\times$ energy consumption as SRAM. Large write delay (> ns range) resulting from spin injection symmetry in switching the magnetic orientation of free layer belittles STT-MRAM's feasibility as an on-chip cache \cite{ultrafast_embedded_mem_fab4}. The stress on the dielectric oxide of the MTJ due to the large write current accelerates the time-dependent wear out of the cell \cite{tahoori_1}. Second, its shared read-write  path makes it vulnerable to RD.
%that makes it vulnerable to RD because it shares the same electrical path for read-write access. 
%Furthermore, an identical read-write path never allows it to be suitable for dual-port memory that can simultaneously afford a read-write operation, thus limiting memory bandwidth \cite{dualport_fieldfree_fab2}. 

The promising features, such as high density, near-zero leakage power, immunity against radiation-induced soft errors, and CMOS compatibility of  emerging Spin-based non-volatile (NVM)  magnetic memory (i.e., MRAM)  technologies, attracted researchers from academia and industry \cite{memory_trend}. Spin Transfer Torque (STT) MRAM, has already shifted its gear from the R\&D phase to commercialization as the NAND-based embedded flash replacement \cite{dac_19} \cite{recent-progress-in-SOT_fab3}. However, MRAM in its regular form cannot be used in AI accelerators due to its slow write speed and high write energy \cite{ recent-progress-in-SOT_fab3}\cite{optimized_SOT_imec}.

STT-MRAM, a two-terminal magnetic memory with Magnetic Tunnel Junction (MTJ) as the storing element, flows a bidirectional spin-polarized current through the MTJ for read-write operation \cite{stt_eqn1}. The major challenges of STT-MRAM, such as poor write performance, reliability issues, e.g., Read Disturbance (RD), retention failure, \cite{recent-progress-in-SOT_fab3}\cite{dualport_fieldfree_fab2}, stem from two main reasons. First, the high write current flowing through the MTJ accounts for almost $10\times$ energy consumption as SRAM. Large write delay (> ns range) resulting from spin injection symmetry in switching the magnetic orientation of free layer belittles STT-MRAM's feasibility as an on-chip cache \cite{ultrafast_embedded_mem_fab4}. The stress on the dielectric oxide of the MTJ due to the large write current accelerates the time-dependent wear out of the cell \cite{tahoori_1}. Second, its shared read-write  path makes it vulnerable to RD.



Spin-Orbit Torque (SOT) MRAM, considered the next generation of STT-MRAM, offers high performance without compromising reliability issues such as RD. SOT-MRAM is a four-terminal memory cell that uses MTJ as the storing element \cite{sot_model_kazemi}. By splitting the read-write path and using a different switching scheme, SOT-MRAM resolves all the challenges of STT-MRAM while retaining its every benefit \cite{recent-progress-in-SOT_fab3} \cite{dualport_fieldfree_fab2} \cite{ultrafast_embedded_mem_fab4} \cite{tahoori_1} \cite{size_dependent_switching_fab1}   . Isolate read and write path allows the designer to optimize the read and write path independently, decreasing the write current and increasing the read-write operating margin, thus solving the RD-induced reliability issues. %Moreover, a separate read-write path allows straightforward implementation of the dual-port memory cell, resulting in high bandwidth \cite{dualport_fieldfree_fab2}. 
%The absence of a large write current flowing through the MTJ cell ensures a longer lifetime. The non-volatility property of SOT-MRAM, along with contributing to the easy adoption of power gating, also initiates normally-off/instant-on computing facilities \cite{tahoori_2}. 
Though lacking mass-scale production from foundries due to early-stage manufacturing challenges, \cite{recent-progress-in-SOT_fab3} \cite{optimized_SOT_imec} 
\cite{dualport_fieldfree_fab2} \cite{ultrafast_embedded_mem_fab4} \cite{size_dependent_switching_fab1} \cite{sot_0.35ns_write} have demonstrated the successful fabrication of SOT-MRAM with attractive specifications. Its attractive features, such as high density, reliability and endurance, zero leakage, read-write latency comparable to SRAM, and research effort to enable mass production make it one of the best candidates for AI accelerator memory system where large on-chip memory is a must for training and inference.

The performance of an AI accelerator depends on both the compute and memory throughput of the device. While most accelerators have enough compute throughput, their performance is limited by memory throughput operating in the \emph{memory bound} region. To address the \emph{memory bound} problem of the AI hardware, in this paper, we perform a closed-loop STCO on AI workloads and DTCO on SOT-MRAM to present a hybrid memory system. To our knowledge, this is the first work that analyzes and evaluates the performance of SOT-MRAM as the on-chip memory of AI accelerators targeting both inference and training. The STCO-DTCO methodology is shown in Fig. \ref{fig:paper_concept}, and the key contributions of the paper are highlighted as follows.

%The proposed memory system can provide up to xxxGHz off-chip, xxx GHz on-chip read bandwidth, and xxxGHz on-chip write bandwidth. The key contributions of this paper are as follows.
\begin{itemize}
    \item We present a power and performance-optimized hybrid memory system for Deep Learning (DL) accelerators through a workload-aware STCO and DTCO. Comprised of off-chip HBM3 DRAM, on-chip SRAMs, and DTCO-enabled SOT-MRAM, the hybrid memory system can support the training and inference of DL workloads. We perform a closed-loop STCO and DTCO by taking into account the (i) System performance attributes (e.g., throughput and energy cost); (ii) Architectural and micro-architectural attributes (e.g., compute resources utilization, memory bandwidth) (iii) Workload attributes at both training and inference (e.g., runtime action counts, dataflow and data reuse) to reach the Pareto optimal solution.
    
    \item Using the Deep Learning modelsâ€™ execution profiles,  DTCO enables device and circuit level customization of read/write bandwidth, retention time, and capacity of SOT-MRAM memory banks to meet the bandwidth and capacity demands of DL workloads. Moreover, to achieve dynamic runtime optimization of the power and performance of the accelerator hardware for diverse workloads, memory banks are individually optimized with various bandwidths and capacities.

    \item Finally, using various DNN benchmarks,  we provide a comparative analysis of the existing SRAM-based memory system and the proposed DTCO-STCO optimized hybrid memory system for AI accelerators.
\end{itemize}


The rest of the article is organized as follows. Section \ref{background} discusses the background. In Section \ref{workload_profiling}, we present the analytical model for DNN workload profiling, followed by the DTCO of SOT-MRAM in Section \ref{dtco_MRAM}. Sections \ref{result_analysis} and \ref{rltd_work} present the results \& analysis, and related works, respectively, following the conclusion in Section \ref{conclusion}.
