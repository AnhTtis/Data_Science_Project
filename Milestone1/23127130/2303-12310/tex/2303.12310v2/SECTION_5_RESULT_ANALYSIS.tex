\section{Results and Analysis}
\label{result_analysis}
% We developed a MATLAB-based framework to implement our analytical \emph {Memory and Compute Model}. Unlike ScaleSim \cite{samajdar2020systematic} and Timelooop \cite{parashar2019timeloop} simulator, which only support profiling DNN workloads in inference mode to date, our model captures both training and inference behavior of CV and NLP models. In this section, we provide the result and analysis of the CV and NLP workloads during inference and training from our analytical model and present the optimum Power, Performance, and Area results by performing the DTCO of SOT-MRAM.

In this section, we provide the results and analysis of the STCO on the CV and NLP workloads during inference and training and present the optimum Power, Performance, and Area results by performing the DTCO of SOT-MRAM. We developed a MATLAB-based framework to implement our analytical \emph {Memory and Compute Model} to capture the relationship between the memory access counts and the memory hierarchy sizes in typical systolic array based AI accelerators. 
Unlike ScaleSim \cite{samajdar2020systematic} and Timelooop \cite{parashar2019timeloop} simulator, which only support profiling DNN workloads in inference mode to date, our model captures both training and inference behavior of CV and NLP models. We also verified our model's results with Timeloop in inference mode.



\subsection{Bandwidth Demand}
\label{bw_demand}


% We implement our analytical bandwidth characterization model to calculate the bandwidth demand for 18 widely used state-of-the-art CV models for both training and inferene cases. Our analytical model differs from the 
% ScaleSim\cite{samajdar2020systematic} simulator. Given the PE array dimension, on-chip memory size, and dataflow, ScaleSim reports the DRAM bandwidth for IFMAP, OFMAP, and weight buffer for stall-free execution of the PE array, and the PE array utilization depends on the array dimension and mapping of the dataflow. However, we estimate the required on-chip bandwidth to ensure that the array utilization can reach 100\%, assuming that the mapping efficiency is 100\%.

%%parameter table for NLP models
\begin{table*}[]
\centering
\caption{Parameters of NLP models}
\label{nlp_param}

\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Model & Enc. layer & Dec. layer & Attn. head & \begin{tabular}[c]{@{}c@{}}Word Embedding\\ ($N_{em}$)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Intermediate dimension\\ ($d_{ff}$)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Seq. length \\ ($N_{sql}$)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Vocab. size\\ ($N_{vocab}$)\end{tabular} \\ \hline
Transformer  & 12 & 6 & 8 & 512 & 2048 & 1024 & 37000 \\ \hline
BERT & 12 & - & 12 & 768 & 3072 & 512 & 30522 \\ \hline
Distil BERT  & 6 & - & 12 & 768 & 3072 & 512 & 30522 \\ \hline
Mobile BERT  & 24 & - & 4 & 128 & 512 & 512 & 30522 \\ \hline
Squeeze BERT & 12 & - & 12 & 768 & 3072 & 512 & 30522 \\ \hline
Visual BERT  & 12 & - & 12 & 512 & 3072 & 512 & 30522 \\ \hline
GPT & - & 12 & 12 & 768 & 2048 & 512 & 40478 \\ \hline
GPT-2 & - & 12 & 12 & 768 & 2048 & 1024 & 50257 \\ \hline
GPT-3 & - & 96 & 96 & 12288 & 49152 & 2048 & 50257 \\ \hline
GPT-Neo & - & 24 & 16 & 2048 & 8192 & 2048 & 50257 \\ \hline
GPT-J & - & 28 & 16 & 4096 & 16384 & 2048 & 50400 \\ \hline
\end{tabular}
\end{table*}


\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{Bandwidth_CV.pdf}

    \caption{Bandwidth requirement of CV models for different PE array size. (a) Read Bandwidth, (b) Write Bandwidth.}

    \label{bw_cv}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{Bandwidth_NLP.pdf}
  
    \caption{Bandwidth requirement of NLP models for different PE array size. (a) Read Bandwidth (for GEMM and softmax operation), (b) Write Bandwidth.}.
  
    \label{bw_nlp}
\end{figure}


\begin{figure*}[ht]
    \centering
    \includegraphics[width = \textwidth]{FIG_CV_Vary_GLB.pdf}
   
    \caption{Impact of larger GLB memories on performance and energy efficiency for CV models at inference and training. Percentage reduction in DRAM accesses at inference (a) and training (d). Performance Speedup  from DRAM access reductions at inference (b) and training (e). Energy savings from reduced DRAM accesses at inference (c) and training (f). Both cases compare results to a baseline of 2MB GLB running 16 samples.}
   
    \label{d_fig1_cv_infr}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale = 1.15]{FIG_CV_Vary_BATCH.pdf}
    
    \caption{Impact of batch size on performance and energy efficiency for CV models at inference and training. Percentage increase in DRAM accesses at inference (a), at training (d). Performance slowdown (latency increase) from extra DRAM accesses at inference (b), at training (e). Energy increase from extra DRAM accesses at inference (c), at training (f). In both cases, results are compared to a baseline of 16 samples running with 4MB GLB.}
    
    \label{d_fig2_cv}
\end{figure*}

In Fig. \ref{bw_cv} (a), (b), we plot the read-write on-chip bandwidth demand in $bytes/cycle$ of 18 widely used CV models. Resenet101 and Resnet50 running on a 256$\times$256 PE array will demand the highest read bandwidth, 4017 bytes/cycle, from GLB, whereas Squeezenet will demand the lowest bandwidth, 1028 bytes/cycle. Naturally, as the PE array size increases, the computation capacity per cycle $T_{MAC}$ increases which demands more data from memory to keep all PEs active. From the workload perspective, we observe that the most contributing factor to the read bandwidth demand is its inverse relationship with the filter and ofmap size. We explain the inverse relationship of filter and ofmap size with the read bandwidth using the convolutional reuse concept. As the filter size decreases, the scope of convolutional reuse decreases. The ofmap again depends on the filter and ifmap size. With the decrease of filter size and ofmap size, the convolutional reuse decreases, giving rise to more bandwidth demand. The layer of Resnet101 that requires the most bandwidth (4017 bytes/cycle) has the ofmap dimension (7$\times$7) and filter dimension (1$\times$1). On the other hand, the most demanding (1028 bytes/cycle) layer of Squeezenet has the ofmap dimension (18$\times$18) and filter dimension (1$\times$1). Another observation is that though 1$\times$1 convolution reduces the computation complexity, it requires more bandwidth from memory, i.e., becomes memory intensive. The write bandwidth is also inversely proportional to the filter size. However, in 1$\times $1 convolutions, it depends on the number of outputs generated by the PE array. The write bandwidth is always smaller than the read bandwidth (Fig. \ref{bw_cv} (b)) as it takes more than one operand to generate one output. For example, in a 3$\times$3 convolution, it takes 18 operands to generate a single output; in a 1$\times$1 convolution, it takes two operands. 


\begin{figure*}[ht]
    \centering
    \includegraphics[width = \textwidth]{FIG_NLP_Vary_GLB.pdf}
    
    \caption{Impact of larger GLB memories on performance and energy efficiency for NLP models at inference and training. Percentage reduction in DRAM accesses at inference(a), at training (d). Performance Speedup  from DRAM access reductions at inference (b), at training (e). Energy savings from reduced DRAM accesses at inference (c), at training (e). In both cases, results are compared to a baseline of 2MB GLB running 16 samples}
    \label{d_fig1_nlp}
    
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[scale = 1.11]{FIG_NLP_VARY_Batch.pdf}
    
    \caption{Impact of batch size on performance and energy efficiency for NLP models at inference and training. Percentage increase in DRAM accesses, inference (a), and training (d). Performance slowdown (latency increase) from extra DRAM accesses at inference (b), at training (e). Energy increase from extra DRAM accesses at inference (c), at training (f). Results are compared to a baseline of 16 samples running with 4MB GLB.}
    \label{d_fig2_nlp}
\end{figure*}




As mentioned in section \ref{bw_fc_layer}, the bandwidth requirement for transformer-basded model are calculated using the expressions of Table \ref{table:bw_expression}. The dimension of the operand matrices is larger than the PE array dimension, hence following Case IV (Table \ref{table:bw_expression}, Section \ref{bw_fc_layer}), the read bandwidth of all models depends on the PE array size (Fig. \ref{bw_nlp} (a)). The write bandwidth depends on the PE array dimension and the input sequence length. The softmax read bandwidth depends on the SFU width, and matches with the GEMM read bandwidth. 
As different models are trained with different input sequence lengths\cite{hugging_face}, their write bandwidth demand is not the same across all models. The parameter sizes and settings of the models used in this work are shown in Table \ref{nlp_param}.
The models having the highest sequence length (2048) have the lower write bandwidth demand 102 bytes/cycle running on a 256$\times$256 PE array (Fig. \ref{bw_nlp} (b)).


\subsection{Impact of on-chip memory}
\label{on-chip impact}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{dtco_fig_1.pdf}
    
    \caption{Critical current vs $\theta_{SH}$(a), $w_{SOT}$(b), $t_{SOT}$(c), and $t_{FL}$(d).}
    \label{fig:dtco_ic}
    
\end{figure*}

%To realize the impact of on-chip memory on the accelerator performance, we implement our analytical model of memory access counts (Table \ref{conv_analysis} for CV models and Table \ref{Transformer_analysis} for NLP models, see section \ref{mem_access}). 
%Timeloop \cite{parashar2019timeloop} also reports the scalar DRAM read-write counts for DNN models, but we did not use it in our analysis as it does not map the DRAM access counts as a function of GLB size.
%rather a function of mapping and loop nests.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.48]{dtco_fig2_ic_vs_tp_delta_t_ret.pdf}
   
    \caption{(a) Switching pulse width $\tau_{p}$ vs applied switching current $I_{sw}$.  (b) Thermal stability factor $\Delta$ (left Y-axis) and retention time $t_{ret}$ (right Y-axis) vs MTJ dimension for a fixed retention failure rate, $P_{RF} = 10^{-9}$. At $\Delta = 70$, MTJ dimension = 88nm, retention time is $>$ 10 years \cite{imce2019}.}
    \label{fig:dtco_pulse_width}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.48]{dtco_fig3_tmr_t_MgO_rd_latency.pdf}
    
    \caption{Impact of, (a) oxide thickness on TMR. (b) TMR on read latency.}
    \label{fig:tmr_vs_oxide_rd_ltncy}
    
\end{figure}

Compared to a GLB size of 2MB, the DRAM access counts for all CV models decrease significantly if we increase the GLB size. In inference, reaching the 100\% reduction in access means it only needs to read the initial inputs, weights for each layer, and write the final layer output, no DRAM access is needed for the intra and inter-layer operations. Further increase in GLB size will not improve the performance in these cases. 
%We did this analysis running a batch of 16 samples, which is quite a large batch size for inference. 
For 16 samples, DRAM access is reduced by 100\% for 14 models at 128MB, and most models experience a reduction of >80\% at 64MB (Fig. \ref{d_fig1_cv_infr} (a)). Fig. \ref{d_fig1_cv_infr} (b), (c) show the performance speed up and energy saving coming from these DRAM access reductions. 


We observe a slower improvement in the DRAM access reduction during training unless the GLB size is large enough, at least 256MB for most models (Fig. \ref{d_fig1_cv_infr} (d)). However, even the smaller percent reduction in DRAM access results in significant performance and energy improvement %compared to a larger percent reduction during inference% 
(Fig. \ref{d_fig1_cv_infr} (e), (f)). This is because training requires at least 2$\times$ DRAM accesses as inference. The smaller percent reduction of a large number of DRAM accesses translates to a significant energy and latency improvement. 
A similar trend is observed for NLP models. Transformer-based NLP models are usually larger than the CV models. This is the reason we achieve more performance speedup and energy reduction even at smaller DRAM access reduction rate (Fig. \ref{d_fig1_nlp}).We also observe that DNN models learn faster if we increase the batch size. However, for a fixed GLB size, the DRAM access count increases significantly at larger batch size, causing performance slowdown and more energy consumption. Fig. \ref{d_fig2_cv} and Fig. \ref{d_fig2_nlp} (a, b, c for inference and d, e, f for training) show the increase in DRAM access count and its associated impact on performance and energy for CV and NLP models respectively at different batch sizes.  

The key takeaway from this analysis is that we can reduce the energy and latency associated with DRAM accesses if we increase the GLB size. For larger batch sizes, the energy and latency improvement is even more. Because at large batch sizes, throughput increases at the cost of DRAM accesses. As we increase the GLB size, DRAM accesses reduce, and we achieve latency and energy reduction.

\subsection{DTCO of SOT for PPA Optimization}


From section \ref{on-chip impact} we see that the GLB size of 64MB (for inference) and 256MB (for training) offer significant energy and performance improvement. However, it is not feasible and efficient to use such large SRAMs because of its area and leakage power, even if the low-power techniques are employed. Section \ref{bw_demand} implies that we need approximately 4000bytes/cycle bandwidth between GLB and PE array for larger array size (256$\times$256). In this subsection, we provide the SOT-MRAM DTCO results and observation meeting the requirements stated in the above two subsections. We perform the DTCO in \textit{Cadence Virtuoso}  tool using the compact SOT-MRAM model from \cite{sot_model_kazemi}, and use \textit{Synopsys} 14nm library \cite{saed14} for  the CMOS transistors and peripheral circuits.

\subsubsection{$I_{C}$ optimization}

%The critical switching current depends on the device's physical and geometrical structure and its magnetic property. The critical switching current can be controlled for a given external field by adjusting the device dimension and using the appropriate material. 
To realize the impact of SOT efficiency $\theta_{SH}$ on $I_{c}$, we sweep $\theta_{SH}$ from 0.1 to 100 (Fig. \ref{fig:dtco_ic} (a)). With $\theta_{SH} \ge 100$,  $I_{c}$ goes as low as 0.5uA. Even though the widely used SOT layers are made of heavy metal alloys having smaller $\theta_{SH}$ (e.g., 0.1 to 0.5), recent advancement in material engineering demonstrates that in topological insulator $\theta_{SH}$ can go as high as 152 \cite{khang2018conductive}. We recommend using topological insulators as the SOT layer to achieve a lower switching current.


Next, we analyze the impact of SOT layer geometry on the switching current (Fig. \ref{fig:dtco_ic} (b), (c)). $I_{c}$ scales down linearly with the decrease of SOT layer width, and $w_{SOT}$ can be set to desired value based on the performance and reliability requirement (Fig. \ref{fig:dtco_ic} (b)). While $I_{c}$ scales linearly with the width of the SOT layer, the thickness of the SOT layer has an interesting effect on the switching current. The SOT layer should be relatively thin but bulk enough for heavy metal layers to experience the bulk effect to achieve high SOT efficiency. Once it crosses optimum thickness, which is ~3nm (Fig. \ref{fig:dtco_ic} (c)), many of the charges that are injected into the metal do not contribute to the switching, and $I_{c}$ increases. 

The smaller the free layer thickness, $t_{FL}$, the smaller the switching current (Fig. \ref{fig:dtco_ic} (d)). We also scale the diameter of MTJ, $d_{MTJ}$, to reduce the MTJ area. However, with the scaling down of $d_{MTJ}$ together with $t_{FL}$, the thermal stability factor $\Delta$ also scales down, reducing the memory's data retention time $t_{ret}$. Non-volatility is a great feature of MRAM, but it can be compromised to achieve higher density, higher bandwidth, and lower energy when the target application is a cache. Because, in the cache even for AI workloads, the data lifetime is much shorter, typically in the seconds range \cite{stt_ai_us}. Fig. \ref{fig:dtco_pulse_width}(b) shows $\Delta$ and $t_{ret}$ as functions of free layer volume. While scaling down $t_{FL}$ to optimize $I_{c}$, and $d_{MTJ}$ to optimize area, we keep an eye on the reliability of the stored data. We consider a retention failure rate of $10^{-9}$ (i.e., 1 bit flip per billion).

\subsubsection{Bandwidth optimization}
As shown in Fig. \ref{fig:tmr_vs_oxide_rd_ltncy} (a), TMR ratio of the MTJ device can be increased by increasing the oxide thickness \cite{tsunekawa2005giant}. We increase the oxide thickness to decrease the  read latency (Fig. \ref{fig:tmr_vs_oxide_rd_ltncy} (b)). 
The write pulse width is inversely proportional to the applied switching current. While we want to lower the applied current to achieve low energy, the higher amplitude of the applied current is required for faster magnetization reversal. However, switching occurs at smaller pulse width at the iso-current if we scale down the SOT layer width. This is because of the smaller critical current at smaller geometry (Fig. \ref{fig:dtco_ic} (b,d)).
Fig. \ref{fig:dtco_pulse_width}(a) shows that switching pulse width can be reduced significantly by scaling down the SOT layer width. Thus, we can achieve higher write bandwidth by scaling down the SOT layer width to meet the high BW demand from AI workloads. 

%The DTCO optimized parameters of SOT-MRAM used in this study are listed in Table \ref{optimized_dtco_param}. Since process variations can impact precise control of the device dimensions, we add 30\% guard-band to the device dimensions in Table \ref{optimized_dtco_param}.

\begin{table}[ht]
\centering
\caption{SOT-MRAM DTCO optimized parameters. 30\% guard-band are added with thickness and width for process variations.}
\label{optimized_dtco_param}

\setlength\tabcolsep{5pt}
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{Parameter} & \multicolumn{1}{c|}{Value} & \multicolumn{1}{c|}{Parameter} & \multicolumn{1}{c|}{Value} \\ \hline
Spin Hall angle $\theta_{SH}$   & 1                          & TMR                            & 240\%                      \\ \hline
Free layer thickness $t_{FL}$   & 0.5nm                      & MTJ diameter $d_{MTJ}$               & 55nm                       \\ \hline
SOT width $w_{SOT}$             & 130nm                      & SOT thickness $t_{SOT}$        & 3nm                        \\ \hline
Oxide thickness $t_{MgO}$       & 3nm                        & Thermal stability factor $\Delta$                       & 45                         \\ \hline
\end{tabular}

\end{table}


\subsection{Process \& Temperature Variation and Bitcell Simulation}
In this subsection, we perform Process and Temperature variation on the DTCO-optimized parameters, design the peripheral circuits, and test the read-write operation on the bit cell at scaled parameters.

\subsubsection{Process and Temperature variation}
To incorporate process variations, we model MTJ diameter, free layer thickness, and SOT layer width as Gaussian variables in the Verilog A model of SOT-MTJ \cite{sot_model_kazemi}. We assume standard deviations ($\sigma$) as 5\% of their respective means ($\mu$) and perform Monte Carlo simulations with 5000 samples within 4$\sigma$ variation. We also consider the temperature variations. The extreme point at the right side of the scaled target parameter is $\mu + 4\sigma, T_{cold}$ (Fig. \ref{fig:process_var}). From equations \ref{Ic3} and \ref{tau}, $I_{sw}$ and $\tau_{p}$ are independent of Temperature. As a result, the worst case for write operation (highest $I_{sw}$ and longest $\tau_{p}$) is at $\mu + 4 \sigma$. This point is, however, benign to the read operation and retention failure.
As we scale down $d_{MTJ}$ and $t_{FL}$, $\Delta$ also reduces, reducing $t_{ret}$, and $I_{data}$. $\Delta$ reduces further as temperature increases \cite{stt_eqn1}. Thus, the worst case for read operation (smallest $I_{data}$) and retention failure (smallest $t_{ret}$) is at $\mu - 4 \sigma, T_{hot}$ (see Fig. \ref{fig:process_var}). As $I_{data}$ reduces, the difference between $I_{data1}$ and $I_{data0}$ becomes even smaller and difficult to sense.
%We optimize and test our read circuitry at the worst-case variation.

To ensure the reliability of the SOT-MRAM bit cell, we add a 30\% guard band on the scaled SOT device parameters: 20\% for process variation and 10\% for temperature variation. The optimized DTCO parameters after adding the PT induced 30\% guard-band are shown in Table \ref{optimized_dtco_param}.


\begin{figure}[h]
    \centering
\includegraphics[width=0.43\textwidth]{process_variation_new.pdf}
    \caption{Impact and distribution of Process and Temperature variation on scaled parameters.}
    \label{fig:process_var}
\end{figure}

\subsubsection{Write operation}
To write SOT-MRAM bitcell, we bias BL with the data-to-be-written and SL with the complement of data-to-be-written. Assuming that the magnetization state of the Reference layer is -1, to write 1 into the MTJ bitcell, we switch the magnetic orientation of the Free layer to +1 state resulting in a high resistive state. To achieve this state, we turn on the WWL, connect BL to VDD and SL to the ground. The resultant current switches the free layer's magnetic orientation from -1 to +1. The opposite bias is applied to write 0. We do not need any additional peripheral circuits for the write operation of SOT-MTJ.

\subsubsection{Read operation} Read operation involves sensing the current passing through MTJ at P and AP states. For our SOT-MRAM bitcell, with the parameters shown in Table \ref{optimized_dtco_param}, $I_{data0} = 20uA$ and $I_{data1} = 33uA$. We design and optimize the read circuitry to sense this small differential current, as shown in Fig. \ref{fig:peripheral_circuits}. 
Our proposed read sensing circuit only contains an additional current mirror block (to amplify current), and it does not require the precharge circuits compared to SRAM. Hence, there is no additional area overhead in the periphery compared to SRAM. The dynamic power consumption are shown in Table \ref{tab:power}

To capture the stochastic nature of MTJ switching, we simulate the bit cell for 1000 bitstream. We achieve a read and write yield of 100\%, and at 250ps and 520ps, respectively.  
% With DTCO we achieve a read latency of 250ps and a write latency of 500ps for SOT-MRAM bit-cell. 
This results in  read bandwidth of 4 Gbps and a write bandwidth of 1.9 Gbps. We then dynamically allocate the memory bus width on-demand to satisfy the bandwidth requirement for different workloads and PE array size stated in section \ref{bw_demand}.
\begin{table}[h]
    \centering
    \caption{Dynamic Power consumption (in uW) of SRAM and SOT-MRAM. (1/0) means the corresponding power to access bit 1 and 0.}
    \begin{tabular}{|c|c|c|}
        \hline
         & Read(1/0) & Write(1/0) \\ \hline
        SRAM & 426 & 373 \\ \hline
        SOT-MRAM & 150/368 & 325/300 \\ \hline
        \end{tabular}
    \label{tab:power}
\end{table}

\begin{figure}
    \centering    \includegraphics[scale=0.43]{mtj-read-write.pdf}
    \caption{SOT-MTJ bitcell with read sensing circuitry.}
\label{fig:peripheral_circuits}
\end{figure}




\subsection{System level performance evaluation of SOT-MRAM based Memory}
\begin{figure*}[ht]
    \centering
    \includegraphics[width = \textwidth]{SYS_LVL_PRF_ALL.pdf}

    \caption{System level energy improvement with SOT-MRAM and DTCO-optimized-SOT-MRAM over SRAM at the same size for CV (a-d) and NLP (e-h) models. The top plots show energy (a, e) and latency (b, f) for inference, and the bottom plots show energy (c, g) and latency (d,h) for training.}

    \label{fig_sys_lvl_perf_cv}
\end{figure*}

% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width = \textwidth]{Fig_sys_lvl_perf_nlp.pdf}
%     \caption{System level energy improvement with SOT-MRAM and DTCO optimized SOT-MRAM over SRAM at the same size for NLP models: (a) Inference, (b) Training. System level latency improvement with SOT-MRAM and DTCO optimized SOT-MRAM over SRAM at the same size: (c) Inference, (d) Training.}
%     \label{fig_sys_lvl_perf_nlp}
% \end{figure*}

% \textcolor{blue}{In this subsection, we evaluate the performance at the system level on the DNN benchmarks with SRAM, SOT-MRAM, and DTCO-optimized-SOT-MRAM. }

In this subsection, we analyze the PPA (Power, Performance, and Area) metrics at the system level on the DNN/CNN benchmarks with SRAM, SOT-MRAM, and DTCO-optimized-SOT-MRAM. 
We use the Destiny \cite{destiny} memory simulator to find the array-level data for both SRAM and SOT-MRAM. We modify Destiny source code to reflect: (i) SOT switching mechanism, (ii) special read sensing circuit for SOT-MRAM, and (iii) 14nm CMOS technode. Then, we feed the extracted bitcell-level data of SOT-MRAM in the $.cell$ file to find the PPA at the desired memory capacity.

%We modify Destiny\cite{destiny} source code to reflect the SOT switching mechanism and special peripheral circuits for SOT-MRAM. With the extracted bitcell-level data (including peripheral circuits), we design the SOT-MRAM.cell in Destiny \cite{destiny} to find the array-level PPA of SOT-MRAM. We also modify the destiny Technology file to reflect the 14nm process node for CMOS technology, as we }  
% To find the bit-cell level read/write latency and energy, we simulate Synopsys 14nm low-power SRAM \cite{saed14} bit cell, and SOT-MRAM \cite{sot_model_kazemi} bit-cell in Cadence Virtuoso. We then feed this data to Destiny \cite{destiny} to find the array-level PPA. To reflect the separate read-write path and SOT switching mechanism, we designed the peripheral and sensing circuits in the Cadence Virtuoso and used the extracted latency and energy parameters in Destiny's config. file.
 Based on the array-level results from Destiny, and DRAM \& GLB access counts from Algorithms \ref{alg:dram_acc_cnt_infr}, and \ref{alg:dram_acc_cnt_trng}, we estimate the system-level power and performance. Finally, we analyze the area of the memory modules of different technologies (14nm SRAM, SOT-MRAM, and DTCO-opt-SOT-MRAM) at iso-capacity. This analysis only incorporates the PPA metrics from the memory system (DRAM and GLB), assuming that the PPA of the compute unit is constant. With SOT-MRAM as GLB, we see significant energy and latency improvement over SRAM at 64MB (for inference) and 256MB (for training) (see Fig. \ref{fig_sys_lvl_perf_cv} (a-d) for DNN benchmarks and (e-h) for NLP benchmarks). On average, the 64MB SOT-MRAM offers 5$\times$ energy reduction and 2$\times$ latency reduction over 64MB SRAM across all CNN models at inference. Our DTCO-optimized-SOT-MRAM offers further improvement, 7$\times$ energy, and 8$\times$ latency reduction over SRAM at iso-capacity. For latency improvement, the most contributing factor is the DRAM access reduction with large GLB and the smaller read/write latency of SOT-MRAM at larger capacity compared to SRAM. At smaller capacity, SRAM is way faster than SOT-MRAM \cite{tahoori_1,optimized_SOT_imec}. We observe that the most contributing factor in energy reduction (>50\%) is the near-zero leakage power of SOT-MRAM compared to high leakage power of SRAM. The improvement is even more in training mode; 6$\times$ (8$\times$ with SOT-opt.) energy reduction and 2$\times$ (9$\times$ with SOT-opt.) latency reduction. With 64MB SOT-MRAM, NLP models in inference mode experience 2$\times$ (3$\times$ with SOT-opt.) energy reduction and 2$\times$ (4$\times$ with SOT-opt.) latency reduction than 64MB SRAM. Like CV benchmarks, with 256MB SOT-MRAM, NLP benchmarks also experience more energy improvement, 6$\times$ (8$\times$ with SOT-opt.), and latency improvement, 2.5$\times$ (4.5$\times$ with SOT-opt.), in training mode. The more improvement in training mode is because of two reasons: (1) GLB size increases from 64MB to 256MB, and (ii) GLB access counts are significantly large (at least 5$\times$) in training. Our DTCO-opt-SOT-MRAM further adds value to PPA by its smaller silicon area, 0.54$\times$ at 64MB and 0.52$\times$ at 256MB of 14nm SRAM at iso-capacity (Fig. \ref{sys_area_comp}).

\begin{figure}[ht]
    \centering
    \includegraphics[scale = 0.55]{Fig_Area_comparison.pdf}

    \caption{Area improvement of SOT-MRAM and SOT-MRAM-OPT}
    \label{sys_area_comp}
   
\end{figure}




