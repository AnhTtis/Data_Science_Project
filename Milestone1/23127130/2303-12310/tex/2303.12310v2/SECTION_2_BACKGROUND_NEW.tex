\section{Background}
\label{background}
\subsection{AI/DL Applications}

\begin{figure*}[ht]
    
    \centering
    \includegraphics[scale=0.8]{Fig_5_Convolution_rebuttal.pdf}
    
    \caption{CV model (CNN/DNN) abstract architecture. Deep convolution (Conv) layers with residual/skip connection followed by fully connected (FC) layer/s. For symbol meaning please see Table \ref{sys_param}.}
    \label{fig:conv_op}
       
\end{figure*}
% \vspace{-15pt}
\subsubsection{Computer Vision (CV) and Pattern Recognition}

% CV is one of the earliest applications of DL. 
CV models, also called Convolutional/Deep Neural Networks (CNN/DNN),  are the stacks of convolution layers connected straight and/or through residual connection \cite{resnet} to extract the objectsâ€™ features, and a few Fully Connected (FC) layers at the end to classify the objects. Image classification,  captioning, reconstruction and object/instance segmentation are the scopes of CV models.  Deep Residual Networks, having convolutional layers at their core, dominate the CV domain. The input images are convolved with the filter weights to produce the output feature map (\emph{OFMAP}). The \emph{OFMAP} goes through the pooling and normalization layers to act as input (\emph{IFMAP}) to the next layer. The linear and softmax layer at the end finally recognizes the image (Fig. \ref{fig:conv_op}). The size of each data entity (IFMAP, OFMAP, and Weights) depend on the model architecture. 


\subsubsection{Natural Language Processing (NLP)}
Language modeling deals with processing sequential data. Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) have been used in language modeling until the state-of-the-art Transformer \cite{vaswani2017attention} model is introduced. NLP models are used in machine translation, text summarization, speech recognition, syntactic and semantic parsing, question answering, dialog system etc. In Transformer-based models \cite{vaswani2017attention}, the input sequence propagates through the embedding layer and different sublayers of the encoder stacks to extract different linguistic features and inter-token dependency of the input sequence. The decoder stacks then generate the output sequence by taking the encoded input sequence from the encoder stack and the output sequence generated by itself in the previous timesteps (Fig \ref{fig:transformer}). The input sequence multiplied by different layer weights takes different activation names and shapes throughout the model operation.



\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.65]{Fig_4_Transformer.pdf}
    
    \caption{Transformer model workflow breakdown}
   
    \label{fig:transformer}
\end{figure*}

\subsection{AI/DL Accelerators}

At the core of AI/DLs is the matrix-matrix/vector multiplication (GEMM) with massive parallelism. Exploiting this parallelism,  Systolic Array (SA) based architecture \cite{tpu} have been used to accelerate the computations. Different dataflows, such as row stationary, output stationary, weight stationary, have been evolved to maximize the reuse and reduce the data movement. Off-chip DRAM access being 100-200 times more energy and latency expensive than any ALU operation or on-chip access \cite{eyeriss} plays a crucial role in determining the overall system performance. Another non-conventioanl type of architecture, In-Memory Computing (IMC) \cite{imc_survey} has recently  evolved to address the data communication cost for DNN accelerators. However, in this work, we focus on reducing the off-chip memory access for conventional DNN accelerator architectures \cite{tpu}, \cite{eyeriss}\cite{nvdla} by increasing the on-chip Global Buffer (GLB) size with SOT-MRAM. 

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.72]{Fig_3_SOT_bitcell_new.pdf}
	
	\caption{Physical structure of a SOT-MRAM bit cell highlighting separate read (along blue line) and write (along red line) path}
	\label{fig:sot_bitcell}	
	
\end{figure}


\subsection{SOT-MRAM}
\subsubsection{Physical Structure} 
With MTJ \cite{stt_eqn1} as storing element, the SOT-MRAM is a 
three terminal device. Depending on the type of bit cell, there are three to four lines to control the read-write operation. In this work, we consider a two transistor one SOT (2T1SOT) bit cell architecture that requires two access transistors, (i) \emph{Read Wordline (RWL)}, (ii) \emph{Write Wordline (WWL)}, (iii) \emph{Bit Line (BL)}, and (iv) \emph{source Line (SL)} to accommodate separate read-write access path \cite{sot_model_kazemi} \cite{2T1SOT} (Fig. \ref{fig:sot_bitcell}). The MTJ stack, with its free layer at the interface, is placed on top of a SOT layer (i.e., channel) to ensure SOT-induced switching. The SOT layer is composed of heavy metals or topological insulators \cite{manchon2019current}. 



\subsubsection{Read-Write Operation}
Upon the activation of RWL, a small amount of current is passed through BL and grounded SL. The resistive state of the MTJ is captured by sensing the voltage across it and comparing the voltage with a reference value \cite{dualport_fieldfree_fab2}. Low resistive state ($R_{P}$) and high resistive state ($R_{AP}$) represents bit 0 and 1 respectively.
%\subsubsection{Write operation} 
The write operation of MTJ-based MRAM involves switching the resistive status of MTJ. In SOT-MRAM, switching occurs due to Spin Orbit Torque (SOT) effect. Unlike STT-MRAM, a current is passed through the SOT layer to change the MTJ resistive state by switching the magnetic orientation of the free layer. 
A bidirectional write current flows through BL and SL during write operation. The potential of BL and SL changes depending on the bit value written in the cell. For example, to write `1', current flows from BL to SL and vice versa to write `0' \cite{dualport_fieldfree_fab2} \cite{tahoori_1}.


