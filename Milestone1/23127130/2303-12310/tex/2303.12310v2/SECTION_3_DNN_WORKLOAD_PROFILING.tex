%\section{WORKLOAD AWARE HYBRID MEMORY MODEL}
\section{DNN WORKLOAD PROFILING}
\label{workload_profiling}
Profiling the target workload is a prerequisite for designing an accelerator for the target workload. Assuming that we have a powerful computing system to handle the exhaustive computations of the DL workload, we focus on providing efficient data movement between the compute and memory system to ensure 100\% utilization of computing resources by introducing the workload-aware hybrid memory system. We propose the hybrid memory system by analyzing the Deep Learning model workloads from CV   and NLP domain.
We analytically model the on-chip bandwidth requirement and memory access patterns of different parts of the workload during inference and training, \emph {Memory and Compute Model}, to develop the memory system for TPU-like \cite{tpu} DNN accelerators.
%The model is developed considering the memory bandwidth requirement (
%both on-chip to DRAM and %
%compute unit to on-chip), memory access count, and memory size requirement of different parts of the workload. In this section, we explain each consideration in detail.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{Fig_2_Accelerator_system_2.pdf}
    
    \caption{Block diagram of Accelerator architecture}
    \label{fig:system_architecture}
\end{figure}




\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.7]{Fig_6_Training_comp_graph.pdf}
    
    \caption{Computational graph of DNN training}
    
    \label{fig:comp_graph}
\end{figure*}


\subsection{Memory Bandwidth Expression}
We express the required bandwidth (BW) as a function of compute resources and workload. $BW$ (bytes/sec)  is defined as the rate at which data needs to be transferred to/from memory by a processor to utilize the computation resources of the processor fully. Mathematically,
\begin{align}
\label{req_mem_bw}
   BW= \frac{F_p}{OI}
\end{align}
Where $F_p$ = Theoretical peak performance  (ops/sec) = number of operations the accelerator performs per sec. The $F_{p}$ of a $H_{A} \times W_{A}$  Processing Element (PE) array (Fig. \ref{fig:system_architecture}):

\begin{align}
    \label{peak_fp}
        F_{p} = H_A*W_A*F_{acc}
\end{align}
$F_{acc}$ = Operating frequency of the accelerator. $OI$ = Operational Intensity of Workload (ops/byte) = number of operations performed per byte accessed. It is a measure of parallelism of the workload. In the subsequent subsections, we will formulate the $OI$ of Conv. and FC layer to find their BW, respectively. Note that the read and write bandwidth will not be the same for these workloads. 
%For determining write bandwidth, we define $OI$ as the number of outputs generated per byte.




\subsubsection{Read Bandwidth ($BW_{RD}$) of Conv. layer}

To formulate an expression for \emph{OI} of convolution workload: First, we determine the total number of MAC operations, $T_{MAC}$, performed by a $H_{A} \times W_{A}$ PE array per clock cycle
\begin{align}
    T_{MAC}\;=H_{A}\;*\;W_{A}
\end{align}
Second, we figure out how many bytes should be read from memory to utilize all PEs of the accelerator in one clock cycle. In a row stationary dataflow \cite{eyeriss}, it takes ($k_{h}*k_{w}+of_{h}*of_{w})*d_{w}$ bytes of data ($d_{w}$ = data type in bytes, i.e., FP32, BF16 etc.) and \#$(of_{h} * of_{w} * k_{h} *k_{w})$ PEs to generate the partial ofmaps corresponding to one input channel. Depending on the size of the PE array, in each iteration (one complete use of accelerator), multiple input channels can be fit. The input channels (i.e., no. of partial ofmaps) computed by the PE array in each iteration:

\begin{align}
    N_{ich\_per\_stp}\;=\frac{H_A*W_A}{of_{h}*of_{w}*k_h*k_w} 
\end{align}
Total bytes read from memory to utilize all PEs:
\begin{equation}
\begin{aligned}
    % T_{byte}=\frac{H_A*W_A}{k_h*k_w*of_{h}*of_{w}}* \\
    % {(k_h*k_w+if_{h}*if_{w})*d_{w}}
    T_{byte}=\frac{H_A*W_A}{k_h*k_w*of_{h}*of_{w}}* 
    {(k_h*k_w+if_{h}*if_{w})*d_{w}}
\end{aligned}
\end{equation}
We divide the total number of MAC operations, $T_{MAC}$, by the total bytes accessed, $T_{byte}$, to find $OI$:
\begin{equation}
    \begin{aligned}
    \label{oi}
        OI=\frac{k_h*k_w*of_{h}*of_{w}}{d_{w}*(k_h*k_w+if_{h}*if_{w})}
    \end{aligned}
\end{equation}
Substituting the expression of $OI$ in equation  \eqref{req_mem_bw} gives $BW_{RD}$ as a function of array size and workload:
\begin{equation}
    \begin{aligned}
%         BW_{RD} & = \frac{(k_h*k_w\;+if_{h}*if_{w})*d_{w}}{k_w*k_h*of_{h}*of_{w}}*\\
% & \;\;\;\;\;\;\;H_{A}*W_{A}*F_{acc}
        BW_{RD} & = \frac{(k_h*k_w\;+if_{h}*if_{w})*d_{w}}{k_w*k_h*of_{h}*of_{w}}* H_{A}*W_{A}*F_{acc}
    \end{aligned}
\end{equation}
For the symbol meanings, please see Fig. \ref{fig:conv_op} and Table \ref{sys_param}.


\subsubsection{Write Bandwidth ($BW_{WR}$) of Conv. Layer}
Partial ofmap of a single input channel requires \#($of_{h}*of_{w}*k_{h}*k_{w}$) PEs. Therefore, $H_{A}\times W_{A}$ PEs generate $(H_{A}*W_{A})/(of_{h}*of_{w}*k_{h}*k_{w})$ ofmaps in each iteration. Each partial ofmap contains $of_{h}*of_{w}$ elements. The total output bytes generated by the PE array in one iteration is, equivalently, the write bandwidth:
% \begin{equation}
% \begin{aligned}
%     \label{wr_bw}
%     BW_{WR} = \frac{H_{A}*W_{A}}{N_{ofmap\_rw}*N_{ofmap\_cl}*k_{h}*k_{w}}\;*\\ N_{ofmap\_rw}*N_{ofmap\_cl}*d_{width}*F_{acc}
% \end{aligned}
% \end{equation}

\begin{table}[]
\centering
\caption{CNN and systolic array parameters nomenclature}
\label{sys_param}
\begin{tabular}{|l|l|}
\hline
$W_{A}, H_{A}$   & Accelerator array width \& height (PEs) \\ \hline
% $H_{A}$   & Accelerator array height (PEs) \\ \hline
$k_{w}, k_{h}$   & Kernel width \& height \\ \hline
% $k_{h}$   & Kernel height \\ \hline
$of_{w}, of_{h}$  & Output feature map width \& height \\ \hline
% $of_{h}$  & Output feature map height \\ \hline
$if_{w}, if_{h}$   & Input feature map width \& height \\ \hline
% $if_{h}$   & Input feature map height \\ \hline
$N_{ich}, N_{och}$ & No. of input \& output channel \\ \hline
% $N_{och}$ & No. of output channel \\ \hline
$N_{bt}$  & Batch size \\ \hline
$n_{fc}, m_{fc}$  & No. of neurons in input \& output FC layer  \\ \hline
% $m_{fc}$  & No. of neurons in output FC layer \\ \hline
\end{tabular}
\end{table}



\begin{equation}
\begin{aligned}
    \label{wr_bw}
    BW_{WR} = \frac{H_{A}*W_{A} * F_{acc} * d_{w}}{k_{h}*k_{w}}\;
\end{aligned}
\end{equation}


\begin{table}[ht]
\setlength{\tabcolsep}{3pt} 
\centering
\caption{RD/WR bandwidth expression of FC layer for different cases}
\label{table:bw_expression}
\begin{tabular}{|cc|c|c|}
\hline
\multicolumn{2}{|c|}{Cases}& $BW_{RD}$& $BW_{WR}$\\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$M<H_{A}$; $N<W_{A}$}}& \multirow{2}{*}{$K<W_{A}$}& \multirow{2}{*}{$\frac{M*N+K*M}{N+K}$}& \multirow{2}{*}{$\frac{K*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \cline{2-4} 
\multicolumn{1}{|c|}{}& \multirow{2}{*}{$K\ge W_{A}$} & \multirow{2}{*}{$\frac{M*N+W_{A}*M}{N+W_{A}}$}& \multirow{2}{*}{$\frac{W_{A}*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$M<H_{A}; N\ge W_{A}$}}  & \multirow{2}{*}{$K<W_{A}$}& \multirow{2}{*}{$\frac{M*W_{A}+K*M}{N+K}$}& \multirow{2}{*}{$\frac{K*W_{A}}{2*W_{A}+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \cline{2-4} 
\multicolumn{1}{|c|}{}& \multirow{2}{*}{$K\ge W_{A}$} & \multirow{2}{*}{$\frac{M*W_{A}+W_{A}*M}{2*W_{A}}$}& \multirow{2}{*}{$\frac{{W_{A}}^{2}}{2*W_{A}+K-1}$} \\
\multicolumn{1}{|c|}{}&&&\\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$M\ge H_{A}; N<W_{A}$}}  & \multirow{2}{*}{$K<W_{A}$}& \multirow{2}{*}{$\frac{H_{A}*N+K*H_{A}}{N+K}$}& \multirow{2}{*}{$\frac{K*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \cline{2-4} 
\multicolumn{1}{|c|}{}& \multirow{2}{*}{$K\ge W_{A}$} & \multirow{2}{*}{$\frac{H_{A}*N+W_{A}*H_{A}}{W_{A}+N}$}& \multirow{2}{*}{$\frac{W_{A}*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{$M\ge H_{A};N\ge W_{A}$}} & \multirow{2}{*}{$K<W_{A}$}    & \multirow{2}{*}{$\frac{H_{A}*W_{A}+W_{A}*H_{A}}{W_{A}+K}$} & \multirow{2}{*}{$\frac{W_{A}*N}{2*N+K-1}$}\\
\multicolumn{1}{|c|}{}&&&\\ \cline{2-4} 
\multicolumn{1}{|c|}{}& \multirow{2}{*}{$K\ge W_{A}$} & \multirow{2}{*}{$\frac{H_{A}*W_{A}+W_{A}*H_{A}}{2*W_{A}}$} & \multirow{2}{*}{$\frac{{W_{A}^2}}{2*W_{A}+K-1}$}   \\
\multicolumn{1}{|c|}{}&&&\\ \hline
\end{tabular}
\end{table}


\begin{table}[]
\centering
\caption{Parameter nomenclature for Algorithm \ref{alg:dram_acc_cnt_infr} and \ref{alg:dram_acc_cnt_trng}}

\begin{tabular}{|l|l|}
\hline
$I, O, W$   & $ifmap,\; ofmap,\;weight$ size in MB \\ \hline
% $O$   & $ofmap$ size in MB \\ \hline
% $W$   & $weight$ size in MB \\ \hline
$RD_{DRAM}$   & DRAM Read access counts\\ \hline
$WR_{DRAM}$   & DRAM Write access counts \\ \hline
$RD_{GLB}$   & GLB Read access counts \\ \hline
$WR_{GLB}$   & GLB Write access counts \\ \hline
$GI, GO, GW$ & $ifmap, ofmap, weight$ Gradient size in MB\\ \hline
% $GO$ & $ofmap$ Gradient size in MB\\ \hline
% $GW$ & $weights$ Gradient size in MB\\ \hline
% $GLB$  & Global Buffer \\ \hline
$mbpa$  & MB of data fetched per memory access \\ \hline
$layer\_f$   & Layer size (MB) combining $ifmap$, $ofmap$ \& \\   & $weights$ \\ \hline
$layer\_b$   & Layer size (MB) in backprop combining upstream,\\  
&  $ofmap$ \& $weight$ gradient  \\ \hline
$cum\;layer$ & Cummulative size of layer \\ \hline
$rd\_f, rd\_b$ & DRAM read access during forward \& backward pass \\ \hline
% $rd\_b$  & DRAM read access during backward pass \\ \hline
$wr\_f, wr\_b$  & DRAM write access during forward \& backward pass  \\ \hline
% $wr\_b$  & DRAM write access during backward pass \\ \hline
\end{tabular}
\label{alg_param}
\end{table}



\RestyleAlgo{ruled}
\SetKwComment{Comment}{/* }{ */}

\begin{algorithm}[hbt!]
%\scriptsize
\footnotesize
\caption{DRAM \& GLB access count at Inference }
\label{alg:dram_acc_cnt_infr}
% \KwData{$n \geq 0$}
% \KwResult{$y = x^n$}
% $y \gets 1$\;
% $X \gets x$\;
% $N \gets n$\;

\For{$i=1$ \KwTo $no.\;of\;layers$}
{$RD_{GLB} \gets \frac{I_{i}}{mbpa_{GLB}}$ \\
% \tcp{Read accesses}

\eIf{$i=1$}
{$WR_{GLB} \gets \frac{I_{i}+O_{i}} {mbpa_{GLB}}$ \\
        \eIf{($I_{i}+W_{i})\leq GLB $} {$RD_{DRAM} \gets \frac{I_{i}+W_{i}}{mbpa_{DRAM}}$}{$RD_{DRAM} \gets \frac{I_{i}+W_{i}}{mbpa_{DRAM}}+ \frac{I_{i}+W_{i}-GLB}{mbpa_{DRAM}}$}}
        {$WR_{GLB} \gets \frac{O_{i}}{mbpa_{GLB}}$ \\
        \eIf{$O_{i-1} \leq UB$}
            {\eIf{$W_{i} \leq GLB $}{$RD_{DRAM} \gets \frac{W_{i}}{mbpa_{DRAM}}$}
                {$RD_{DRAM} \gets \frac{W_{i}}{mbpa_{DRAM}} + \frac{W_{i}-GLB}{mbpa_{DRAM}}$}}
            {$RD_{DRAM} \gets  \frac{I_{i}+W_{i}}{mbpa_{DRAM}}+\frac{(I_{i}+W_{i})-GLB}{mbpa_{DRAM}}$}}
    % \tcp{Write accesses}
    \eIf{$i=no.\;of\;layers$}{$WR_{DRAM} \gets \frac{O_{i}}{mbpa_{DRAM}}$}
    {\eIf{$O_{i}>GLB$}{$WR_{DRAM} \gets \frac{O_{i}-UB}{mbpa_{DRAM}}$}{$WR_{DRAM} \gets 0$}}
}
% \While{$N \neq 0$}{
%   \eIf{$N$ is even}{
%     $X \gets X \times X$\;
%     $N \gets \frac{N}{2} $ \Comment*[r]{This is a comment}
%   }{\If{$N$ is odd}{
%       $y \gets y \times X$\;
%       $N \gets N - 1$\;
%     }
%   }
% }

\end{algorithm}




\begin{algorithm}[hbt!]
\footnotesize
\caption{DRAM \& GLB access count at Training} 
\label{alg:dram_acc_cnt_trng}
$cum\;layer \gets 0$\;
$tmp \gets 0$\;
% $N \gets n$\;

\For{$i=1$ \KwTo $no.\;of\;layers$}
{   $layer\_f_{i} \gets I_{i} + O_{i} + W_{i}$ \;
    $layer\_b_{i} \gets GI_{i} + GO_{i} + GW_{i}$ \;
    $layer\;(i) \gets layer\_f_{i} + layer\_b_{i}$ \;
    $cum\;layer(i) \gets tmp + layer\;(i)$ \;
    $tmp \gets cum\;layer(i)$ \;

$RD_{GLB} \gets \frac{3*I_{i}+O_{i}+5*W_{i}}{mbpa_{GLB}}$ \\
$WR_{GLB} \gets \frac{2*I_{i}+2*O_{i}+3*W_{i}}{mbpa_{GLB}}$ \\ 
\eIf{$cum\;layer(i) \leq GLB$}
{\If{$i=1$}{$rd\_f(i) \gets \frac{I_{i}+W_{i}}{mbpa_{DRAM}}$}
\If{$i=no.\;of\;layers$}{$wr\_f(i) \gets \frac{O_{i}}{mbpa_{DRAM}}$ }
$rd\_f(i) \gets \frac{W_{i}}{mbpa_{DRAM}}$ \;
$rd\_b(i) \gets 0$ \;
$wr\_f(i) \gets 0$ \;
}
{
\eIf{($i\neq 1$) AND ($O_{i-1} \leq GLB$)}{$rd\_f(i) \gets \frac{W_i}{mbpa_{DRAM}}$}
{\eIf{$I_{i}+W_{i} \leq GLB$}{$rd\_f(i) \gets \frac{I_{i}+W_{i}}{mbpa_{DRAM}}$}{$rd\_f(i) \gets \frac{I_i+W_i}{mbpa_{DRAM}}+\frac{I_i+W_i-GLB}{mbpa_{DRAM}}$}}

\eIf{($GI_i+GO_i+GW_i \leq GLB$)}
{$wr\_f(i) \gets 0$ \\
$rd\_b(i) \gets 0$}
{$wr\_f(i) \gets \frac{GI_i + GO_i + GW_i}{mbpa_{DRAM}} $ \\
$rd\_b(i) \gets \frac{GI_i + GO_i + GW_i}{mbpa_{DRAM}}$}
}
$wr\_b(i) \gets \frac{W_{i}}{mb\_per\_acs}$
}


\end{algorithm}

\subsubsection{$BW_{RD}$ \& $BW_{WR}$ of FC layer}
\label{bw_fc_layer}
The systolic array is a widely used architecture to perform GEMM operation \cite{tpu}. %The required Global buffer bandwidth of a systolic array-based architecture depends on the array dimensions, operand matrix dimensions, and the operational intensity of the workloads. 
Depending on the array dimension ($H_{A}\times W_{A}$) and operand matrix dimension (input matrix: $K\times M$, weight matrix: $M\times N$, and output matrix: $K\times N$), we formulate required Read and Write GLB bandwidth for four different cases: (i) Weight matrix dimensions (both) are less than the systolic array dimensions ($M<H_{A}, N<W_{A}$), (ii) Height of weight matrix is less than the height of systolic array, but the width of weight matrix is larger than or equal to the width of the systolic array ($M<H_{A}, N \ge W_{A}$), (iii) Height of weight matrix is larger than or equal to the height of systolic array, but width of the weight matrix is less than the width of the systolic array  ($M \ge H_{A}, N<W_{A}$), and (iv) Both height and width of weight matrix are larger than or equal to the height and width of systolic array respectively ($M \ge H_{A}, N\ge W_{A}$). 


%(see Fig. \ref{bw_calc_cases} for visualization).

In a weight stationary dataflow, it takes $N$ clock cycles to load the weight matrix into the systolic array. Once the weights are loaded, the input matrix is streamed from left to right and the outputs are collected downward. %It takes $K$ clock cycles to load the input matrix into the systolic array. 
The input matrix's first column reaches the weight matrix's last column at $2N$ clock cycles. The last (or $K^{th}$) column of the input matrix reaches the last column of weight matrix after $2N+K-1$ clock cycles and generates the output matrix, $K\times N$. %(fig \ref{bw_calc_all}). 
Based on the above dataflow and mapping, the peak read-write bandwidth per clock cycle for different cases is summarized in Table \ref{table:bw_expression}. The expressions are shown for weight stationary dataflow. 
% However, the above expressions hold for the output and input stationary dataflow, except the fixed and streamed matrix getting swapped.

% \textcolor{red}{The operations of Transformer-based NLP models can be boiled down to the matrix-matrix multiplication and the dot product. }
From the transformer-based NLP model architecture (Fig. \ref{fig:transformer}), we observe that the dominant operations are the GEMM operations. As a result, we model the read-write bandwidth requirement for different layers of the transformer-based model same as the read-write bandwidth of FC layer. Another dominant operation after GEMM is softmax operation, performed mostly on the scaled attention filter matrix, AF of size $N_{sql} \times N_{sql}$. $\sigma(AF)ij=\frac{e^{AF_{ij}}}{\sum_{i=1}^{N_{sql}}e^{AF_{ij}}}$. The softmax operation is generally performed in the Special Function Unit (SFU)\cite{amp100_gpu} (Fig. \ref{fig:system_architecture}). The bandwidth requirement of the softmax operation depends on the hardware architecture, mapping, and $AF$ matrix dimension. Assuming that the SFU contains $1 \times H_{A}$ units, each capable of performing one exponential operation, followed by an accumulator for accumulating the exponentials, and a regular ALU for performing the division the bandwidth of softmax operation on SFU is estimated as $BW_{softmax} = d_{w} * H_{A}$. 
% \begin{equation}
% \label{bw_softmax}
%     BW_{softmax} = \begin{cases}
%     d_{w} * H_{A} & \text{if}\;\; N_{sql} > H_{A} \\
%     d_{w} * N_{sql} & \text{if}\;\; N_{sql} < H_{A}
%    \end{cases}
% \end{equation}}



\subsection{Memory Access Patterns}
\label{mem_access}
Our proposed memory system consists of HMB3 (off-chip DRAM memory), a large GLB with multiple SOT-MRAM banks, a smaller double-buffered SRAM, and PE reg file specific to each PE unit (Fig. \ref{fig:system_architecture}). The banks inside SOT-MRAM are optimized through a DTCO between the SOT-MRAM parameters and the workload requirements. The double-buffered SRAM holds the weights and partial outputs. In this subsection, we analyze the memory access patterns of CV and NLP models for the proposed memory system. 


%Unlike the conventional GPU-based on-chip memory hierarchy where there are multiple levels of cache memory (such as Register file, L0, L1 instruction cache, L1, L2 data cache, etc.) we propose a memory system of large unified buffer composed of multiple banks of SOT-MRAM, a smaller SRAM buffer and PE reg file specific to each PE unit (fig. \ref{fig:system_architecture}). The banks inside SOT-MRAM are again optimized through a DTCO between the SOT-MRAM parameters and the workload requirements. 
%Some operand data are directly loaded from (cite nvidia ample gpu white paper)

The required number of main memory accesses depends on the GLB size, weight, activation size, and dataflow. Assuming a fixed dataflow, weight stationary in this case, we model the memory access counts during inference and training as a function of the model's workloads and the GLB size in Algorithm \ref{alg:dram_acc_cnt_infr}, and \ref{alg:dram_acc_cnt_trng} for inference and training, respectively. During inference, inputs (e.g., images, tokens) are read from HBM3, written to GLB, and read from GLB to be operated inside PEs core. The read-only weights are directly loaded from HBM3 to the register file of each PE unit, bypassing the GLB. Using   double-buffered SRAM, while the array is computing with loaded weights, the next set of weights is temporarily written to the SRAM buffer to hide the off-chip access latency behind the PE array computation latency. 
Suppose the GLB size is large enough to hold all samples in the minibatch. In that case, the data entity can be read all at once, resulting in the memory accesses equal to the algorithmic minimum memory accesses. Algorithmic minimum memory access represents the number of elements in the data entity \cite{parashar2019timeloop}. For weight gradient calculation, during backpropagation of the training, the inputs are read from GLB to PE core, assuming that the GLB is large enough to hold the input images along with the generated ofmap of the current layer, thus avoiding the DRAM accesses during backward pass. In convolution, the inputs can be reused multiple times for convolutional and filter reuse. It can also be reused multiple times during backpropagation to calculate the gradients of different filters. In Transformer-based NLP models, the embedded input can be reused thrice as input to Key, Query, and Value linear layer. It can also be reused thrice during backpropagation to calculate the weight gradient of the Key, Query, and Value linear layer. The training workflow is complicated and requires many more memory accesses (both off-chip and on-chip) compared to inference. For example, to calculate the weight gradients of Layer 1, it requires the current layer's activation gradient $\frac{da_1}{dz_1}$, input ($a_0$), next layer's weight ($W_{2}$) and the upstream gradient from Layer 2 ($\delta_{1}$) (Fig. \ref{fig:comp_graph}).

The pseudo code of Alg. \ref{alg:dram_acc_cnt_infr} models the inference memory access patterns. The inputs and weights must be loaded from DRAM for the first layer. Depending on the combined size of input \& weight matrix size, and GLB size, it requires either algorithmic minimum read accesses or more than that (lines 3-9 of Alg. \ref{alg:dram_acc_cnt_infr}). For the rest of the layers, if the $ofmap$ of the previous layer can fit in GLB, then no read accesses are required for input activation, as the $ofmap$ of the previous layer will act as the $ifmap$ to the next layer. Only the weights are read from DRAM for such layers (lines 12-20 of Alg. \ref{alg:dram_acc_cnt_infr}). The opposite case applies to write accesses: the $ofmap$ of the last layer must be written to the DRAM. For other layers, it needs to be written to DRAM depending on its size and GLB size (lines 22-30 of Alg. \ref{alg:dram_acc_cnt_infr}). No write accesses are required for weight matrices during inference. As the weights bypass the GLB during inference, the GLB read accesses are calculated from the $ifmap$ size for each layer (line 2). The write accesses are calculated from the $ofmap$ except for the 1st layer (line 11, 4). See Table \ref{alg_param} for symbol meanings.

The pseudo code of Algorithm \ref{alg:dram_acc_cnt_trng} models the training behavior. We initialize several temporary variables: $layer\_f_{i}$ (comprising of $ifmap$, $ofmap$, and $weight$ matrices of $i^{th}$ layer), $layer\_b_{i}$ (comprising of upstream gradient, $ofmap$, and $weight$ matrix gradients of $i^{th}$ layer), and $layer_{i}$ (combining  $layer\_f_{i}$ and $layer\_b_{i}$)  as shown in lines 4-6 in  Alg. \ref{alg:dram_acc_cnt_trng}. $cum\;layer(i)$ contains all layers' all entities up to $i^{th}$ layer (line 7-8, Alg. \ref{alg:dram_acc_cnt_trng}). If the GLB is large enough to hold $cum\;layer(i)$, we just need to read the $ifmap$ of the first layer \& $weight$ of all layers from DRAM during the forward pass and write all layers' updated $weight$ during the backward pass and last layer's $ofmap$ to DRAM during the forward pass (lines 11-20 \& 39, Alg. \ref{alg:dram_acc_cnt_trng}). Otherwise, the forward pass is the same as the inference (lines 22-30, Alg. \ref{alg:dram_acc_cnt_trng}). During the backward pass, depending on the size of upstream gradients, $ofmap$, and $weight$ gradients, it accesses the gradients from DRAM (lines 31-37, Alg. \ref{alg:dram_acc_cnt_trng}). The GLB read-write accesses are shown in lines 9-10 in Alg. \ref{alg:dram_acc_cnt_trng}. The $ifmap$ of each layer needs to be read twice, once during the forward pass and once during the backward pass. The upstream gradient, equal in size as $ifmap$, must be read once during the backward pass. The $ofmap$ is read once during the backward pass to calculate the upstream gradient. The weight is read 5 times (once during the forward pass, 4 times during the backward pass). The $ifmap$ and $ofmap$ are written twice, once during the forward pass and once during the backward pass. The $weight$ is written thrice, twice during forward pass and once during backward pass.

% \textcolor{blue}{We model the memory access counts during inference and training as a function of the model's workloads and the GLB size. Algorithm \ref{alg:dram_acc_cnt_infr} and \ref{alg:dram_acc_cnt_trng} models the memory access counts for inference and training, respectively. Suppose the GLB size is large enough to hold all samples in the minibatch. In that case, the data entity can be read all at once, resulting in the memory accesses equal to the algorithmic minimum memory accesses. Algorithmic minimum memory access represents the number of elements in the data entity \cite{parashar2019timeloop}.}
 
% During inference and the forward pass of training, input images are read from HBM3, written to UB, and read from GLB to be operated inside PEs core. For weight gradient calculation, during backpropagation of the training, the inputs are read from GLB to PE core, assuming that the GLB is large enough to hold the input images along with the generated ofmap of the current layer, thus avoiding the DRAM accesses during backward pass. The inputs can be reused multiple times for convolutional and filter reuse. It can also be reused multiple times during backpropagation to calculate the gradients of different filters.
% % such as Upstream gradients, the gradient of loss and activation functions, and optimizer states with their dimension, memory access count, and reuse status. 
% The training workflow is complicated and requires many more memory accesses (both off-chip and on-chip) compared to inference. For example, to calculate the weight gradients of Layer 1, it requires the current layer's activation gradient $\frac{da_1}{dz_1}$, input ($a_0$), next layer's weight ($W_{2}$) and the upstream gradient from Layer 2 ($\delta_{1}$) (as shown in  Fig. \ref{fig:comp_graph}). 



% We summarize the memory access requirements and the dataflow of the CNN-based DNN workload for both inference and training in the rest columns of Table  \ref{conv_analysis}. We follow the conventions similar to Table \ref{Transformer_analysis}. The analysis is based on the most optimized row-stationary dataflow. However, it is also applicable to all other dataflow, such as weight stationary, input, and output stationary dataflow as we generalized the analysis in the workload level. 
% %In a row stationary dataflow, the kernel rows are loaded into the PE array and kept stationary, ifmaps are shifted by stride size, and partial sums are accumulated vertically. 
% The last row of the table contains the memory access and the dataflow requirements for the training-only entities, such as upstream gradients, gradient of loss function, and gradient of activation functions.
% To start off, the input images are loaded from DRAM to GLB and the weights are directly loaded from DRAM to PE array. 



%%%%%%%%%%%%%%%%%%%%%



% We summarize the minimum memory accesses requirement, dataflow and reuse scopes of each layerâ€™s (sub-layer) activations and weights in different levels of memory hierarchy during inference and training are in Table \ref{Transformer_analysis}. We follow the same convention as Table \ref{conv_analysis}. We illustrate one cell of Table \ref{Transformer_analysis} to help the readers naviagate the table. The last column of the second row indicates the dataflow and reuse status of Embedded input during training. During forward pass, the embedded inputs are generated inside the PE core, partial sums are accumulated in the SRAM, and finally, the output is written to Unified buffer (UB). For weight gradient calculation, the embedded inputs are read from GLB to PE core during backpropagation. \emph{Reuse inside PE core} represents at forward pass, the embedded input can be reused thrice as input to Key, Query, and Value linear layer. It can also be reused thrice during backpropagation to calculate the weight gradient of the Key, Query, and Value linear layer. 



% We consider the memory access counts in terms of how many times the data entities operate throughout the workflow. However, the exact access count depends on the model architecture, hyperparameters, hardware platforms, dataflow \& mapping such as IFMAP, OFMAP, word embedding, input sequence length, weight size, batch size, GLB size, SRAM size, PE core dimension, row-stationary dataflow, etc. 
%The analysis is based on the row-stationary dataflow. However, it is also applicable to all other dataflows, such as weight stationary, input, and output stationary dataflow as we generalized the analysis in the workload level. 
