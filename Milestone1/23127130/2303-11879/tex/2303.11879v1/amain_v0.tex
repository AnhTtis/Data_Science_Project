\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=footnotesize,textfont=footnotesize]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{xcolor}
% \usepackage{subcaption}
\def\Plus{\texttt{+}}

\newcommand{\ie}{\emph{i.e.},\xspace}
\newcommand{\eg}{\emph{e.g.},\xspace}

\def\B#1{\mathbf #1}
\def\C#1{\mathcal #1}


\begin{document}

\title{Multimodal Pre-training Framework for Sequential Recommendation via Contrastive Learning}

\author{Lingzi Zhang, Xin Zhou, Zhiqi Shen
        % <-this % stops a space
\thanks{Lingzi Zhang, Xin Zhou, and Zhiqi Shen are with the School of Computer Science and Engineering, Nanyang Technological University, Singapore, and also with the Alibaba-NTU Singapore Joint Research Institute (e-mail: lingzi001@e.ntu.edu.sg; xin.zhou@ntu.edu.sg; zqshen@ntu.edu.sg).}% <-this % stops a space
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
% Sequential recommendation systems utilize the sequential interactions of users with items as the main supervision signals for learning the user's preference.
% However, as the user behavior data is usually sparse, existing sequential recommendation methods usually generate unsatisfactory results.
Sequential recommendation systems utilize the sequential interactions of users with items as their main supervision signals in learning users' preferences.
However, existing methods usually generate unsatisfactory results due to the sparsity of user behavior data. 
To address this issue, we propose a novel pre-training framework, named \underline{M}ultimodal \underline{S}equence \underline{M}ixup for \underline{S}equential \underline{R}ecommendation (MSM4SR), which leverages both users' sequential behaviors and items' multimodal content (\ie text and images) for effectively recommendation. Specifically, MSM4SR tokenizes each item image into multiple textual keywords and uses the pre-trained BERT model to obtain initial textual and visual features of items, for eliminating the discrepancy between the text and image modalities. A novel backbone network, \ie \underline{M}ultimodal \underline{M}ixup \underline{S}equence \underline{E}ncoder (M$^2$SE), is proposed to bridge the gap between the item multimodal content and the user behavior, using a complementary sequence mixup strategy. In addition, two contrastive learning tasks are developed to assist M$^2$SE in learning generalized multimodal representations of the user behavior sequence. Extensive experiments on real-world datasets demonstrate that MSM4SR outperforms state-of-the-art recommendation methods. Moreover, we further verify the effectiveness of MSM4SR on other challenging tasks including cold-start and cross-domain recommendation.
\end{abstract}

\begin{IEEEkeywords}
Multimodal Recommendation, Sequential Recommendation, Contrastive Learning.
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\IEEEPARstart{S}{equential} 
% Sequential recommendation~(SR) systems exploit users’ sequential behaviors to predict their next actions and generate item recommendation~\cite{wang2019sequential}.
recommendation systems are designed to capture the dynamic preferences of users based on their historical behaviors, with the goal of predicting the next item that they will be interested in~\cite{wang2019sequential}.
% Generally, sequential recommendation methods use users' sequential interactions with items as the main supervision signal to learn model parameters.
The primary supervision signal utilized for learning the parameters of these models is typically derived from the sequential interactions of users with items. 
% However, as users' behavior data is often sparse, existing sequential recommendation methods that rely solely on users' sequential behaviors are susceptible to the data sparsity problem, resulting in suboptimal recommendation performance.
However, given the sparsity of user behavior data, sequential recommendation methods that rely solely on such data are susceptible to the problem of data sparsity, resulting in suboptimal performance.
% To solve this problem, previous methods seek to incorporate item attributes into the sequential recommendation framework via self-attention mechanisms~\cite{zhou2020s3,liu2021noninvasive} and self-learning objectives~\cite{xie2022decoupled}.
% To mitigate this issue, several recent studies seek to incorporate item attributes into the sequential recommendation framework through the utilization of self-attention mechanisms~\cite{zhou2020s3,liu2021noninvasive} and self-learning objectives~\cite{xie2022decoupled}.

In practice, there exists a significant amount of multimodal information associated with items (\eg images and text descriptions), which has been employed to alleviate the data sparsity problem in building conventional recommendation systems~\cite{zhang2019feature,liu2021pre,he2016vbpr,xu2018graphcar}. 
For example, \cite{he2016vbpr,xu2018graphcar} leverage item multimodal content as a regularization factor and integrate it with collaborative filtering frameworks. Recent studies~\cite{wei2019mmgcn,wei2020graph,zhang2021mining} utilize graph neural networks to uncover the hidden links between different modalities and establish an in-depth understanding of users' preferences. Although these methods have made considerable advancements, they are only applicable to non-sequential recommendation systems.


Motivated by the success of multimodal recommendation methods~\cite{he2016vbpr,xu2018graphcar,wei2019mmgcn,wei2020graph,zhang2021mining} and pre-trained models~\cite{han2021pre}, we propose to apply the pre-training and fine-tuning paradigm with contrastive learning to effectively exploit the item multimodal content~(\ie text and images) in order to enhance the sequential recommendation performance. However, there are two major challenges in integrating item multimodal content information with existing sequential recommendation frameworks.
%There exist the following two challenges in integrating item multimodal side information with existing sequential recommendation framework.
\emph{Firstly}, there exists a representation discrepancy across different types of modality content.
% \textcolor{blue}{How to effectively join information from different modalities to perform the recommendation needs further exploration.}
\emph{Secondly}, there is a domain gap between users' sequential behaviors and items' multimodal content.
% This is because, besides the item content, users' behaviors may also caused by some other factors, \eg product promotion event.
Intuitively, users' behaviors are not only driven by the contents of items, but also evolve over time.

To this end, we propose a novel pre-training framework, named \underline{M}ultimodal \underline{S}equence \underline{M}ixup for \underline{S}equential \underline{R}ecommendation (MSM4SR). To reduce the discrepancy between textual and visual modalities, 
% MSM4SR tokenizes each item image into multiple textual keywords using a language-image pre-training model~\cite{radford2021learning} and then utilizes the pre-trained Sentence-BERT model~\cite{reimers-2019-sentence-bert} to obtain the initial textual and visual embeddings of items. 
MSM4SR first tokenizes the images of items into multiple textual keywords using a language-image pre-training model~\cite{radford2021learning}. These keywords are then fed into the pre-trained Sentence-BERT model~\cite{reimers-2019-sentence-bert} to obtain items' initial visual embeddings. Analogously, initial textual embeddings of items are derived from their text descriptions using the same Sentence-BERT model.
Compared to previous methods~\cite{wei2019mmgcn,wei2020graph,zhang2021mining}, which directly utilize a pre-trained image encoder to extract image features, the use of word tokens generated from images has several advantages. Firstly, it helps to bridge the semantic gap between multimodal features, eliminating the need for additional encoders to map them into a common semantic space. Secondly, it enables the use of existing language models to process multiple images and extract meaningful information while discarding redundant information. Lastly, merchants can use word tokens to describe their products in a more comprehensive manner, allowing them to differentiate themselves from competitors and increase customer engagement.

\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{MSM4SR_Model.pdf}
\caption{(a) The multimodal feature extraction module used to obtain initial multimodal features of items. (b) The structure of the proposed multimodal mixup sequence encoder that fuses items' multimodal content with users' behavior sequence. (c) The workflow of the proposed pre-training framework, where $\C{S}$ is the input sequence and $i_{n+1}$ is the target item. }
\label{fig:framework}
\end{figure*}

To address the domain gap between user sequential behaviors and item multimodal content, 
% MSM4SR optimizes a backbone network, \ie Multimodal Mixup Sequence Encoder (M$^2$SE), with contrastive learning losses during pre-training. 
MSM4SR utilizes a pre-training strategy that optimizes a backbone network, the Multimodal Mixup Sequence Encoder (M$^2$SE), through the use of contrastive learning losses. 
M$^2$SE fuses item multimodal content with user behavior sequences by implementing a complementary sequence mixup strategy to generate mix-modality representations of a user's behavior sequence. Two contrastive learning losses, \ie 1) modality-specific next item prediction loss, and 2) cross-modality contrastive learning loss, are developed to help M$^2$SE learn more generalized sequence representations across modalities. The modality-specific next item prediction loss captures the correlation between a mix-modality sequence and the next item in each modality space, while the cross-modality contrastive learning loss is employed to calibrate discrepancies in representations from different modality spaces. 
% A novel backbone network, \ie Multimodal Mixup Sequence Encoder (M$^2$SE), is developed to effectively fuse items' multimodal content with users' behavior sequence. M$^2$SE leverages a complementary sequence mixup strategy to obtain the mix-modality representations of a user's behavior sequence.

% In the pre-training stage, two contrastive learning losses, \ie 1) modality-specific next item prediction loss, and 2) cross-modality contrastive learning loss, are developed to help M$^2$SE learn more generalized sequence representations across modalities. The modality-specific next item prediction loss is used to capture the correlation between a mix-modality sequence and the next item in each modality space, while the cross-modality contrastive learning loss is employed to calibrate discrepancies of representations from different modality space. %across modalities
% During the fine-tuning stage, we train a model for next item prediction using left-to-right fine-tuning with representations learned from each modality and ID embeddings.

We perform extensive experiments on three real-world datasets to evaluate the effectiveness of MSM4SR. Experimental results show that MSM4SR can outperform state-of-the-art approaches for sequential recommendation and multimodal recommendation tasks. 
% Moreover, the experiments under the cold-start setting also demonstrate MSM4SR has the potential to solve the cold-start recommendation and cross-domain recommendation problems.
Moreover, we explore the promising applications of MSM4SR in other two challenging tasks: cold-start recommendation and cross-domain recommendation. 
% The experimental results demonstrate MSM4SR has the potential to improve sequential recommendation performance under the cold-start and cross-domain settings.
The results suggest that MSM4SR has the potential to benefit the sequential recommendation task in both cold-start and cross-domain settings.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
% In this section, we review relevant existing works about sequential recommendation and multimodal recommendation.

\subsection{Sequential Recommendation}
Earlier works~\cite{rendle2010factorizing} on sequential recommendation adopt Markov Chain (MC) to capture the transitions over user-item interaction sequences. However, 
% it is difficult for MC-based methods to deal with complex sequence patterns. 
these methods are not well-suited to handle complex sequence patterns.
% With the fast development of deep learning techniques, different deep learning techniques, \eg Convolutional Neural Networks (CNNs)~\cite{tang2018personalized,yuan2019simple}, Recurrent Neural Networks (RNNs)~\cite{donkers2017sequential,peng2021ham}, and Graph Neural Networks (GNNs)~\cite{chang2021sequential,IJCAI-GCL4SR}, have been applied to model users' sequential behaviors. 
More recently, deep learning techniques such as Convolutional Neural Networks (CNNs)~\cite{tang2018personalized,yuan2019simple}, Recurrent Neural Networks (RNNs)~\cite{donkers2017sequential,peng2021ham}, and Graph Neural Networks (GNNs)~\cite{chang2021sequential,IJCAI-GCL4SR} are applied to model users' sequential behaviors.
Moreover, transformer architecture-based methods~\cite{kang2018self,sun2019bert4rec,zhou2020s3,liu2021augmenting} have shown strong performance in capturing long-range dependencies in a sequence. 

To improve the performance of sequential recommendation, some recent studies integrate various auxiliary information into the sequential recommendation framework. For example, in FDSA~\cite{zhang2019feature}, different item features are first aggregated using a vanilla attention layer, followed by a feature-based self-attention block to learn how features transit among items in a sequence. The $\textrm{S}^3$-Rec model~\cite{zhou2020s3} adopts a pre-training strategy to predict the correlation between an item and its attributes. Moreover, in DIF-SR~\cite{xie2022decoupled}, the auxiliary information is moved from the input to the attention layer. The attention calculation of auxiliary information and item representation is decoupled to improve the modeling capability of item representations. Despite the success of these sequential recommendation methods, they ignore the multimodal information of items.

% Lack of multimodal contexts makes it difficult for the model to learn optimal representations from user behavior sequences. Few works~\cite{hidasi2016parallel,cui2018mv} explore the possibility of utilizing multimodal information for sequential recommendation. For example, MV-RNN~\cite{cui2018mv} proposes to combine item latent embedding with multimodal features using concatenation, addition, or feature reconstruction. The user's sequential behaviors are captured by a recurrent neural network. In this case, multimodal information is fused for each item at the input of a sequence encoder without considering sequential contexts derived from different modalities. Instead, we propose to a complementary sequence mixup strategy to bridge the modality gap between multimodal sequences. Sequential representations learned from both modalities are utilized to improve the recommendation performance.

% \begin{figure*}
%      \centering
%      \begin{subfigure}[]{0.9\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{image_tokens_1_v2.pdf}
%          \caption{Pantry}
%          % \label{fig:H}
%      \end{subfigure}
%      \begin{subfigure}[]{0.9\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{image_tokens_2_v2.pdf}
%          \caption{Arts}
%          % \label{fig:H}
%      \end{subfigure}
%      \caption{An example of converting images of an item into text tokens. Images are retrieved from Amazon pantry dataset. Text tokens are generated using CLIP~\cite{radford2021learning}. Best viewed in color.}
%      \label{fig:image-tokens}
% \end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{image_tokens_1_v2.pdf}
\caption{An example of converting images of an item into text tokens. Images are retrieved from Amazon Pantry dataset. Text tokens are generated using CLIP~\cite{radford2021learning}. Best viewed in color.}
% \vspace{-10pt}
\label{fig:image-tokens}
\end{figure*}

\subsection{Multimodal Recommendation}
Multimodal recommendation methods exploit the multimodal content of items to improve recommendation performance. Previous works~\cite{he2016vbpr,kang2017visually,xu2018graphcar} incorporate the visual features of item images into the matrix factorization based recommendation framework. 
In other works~\cite{liu2019user,liu2019userdiverse,liu2021pre}, attention networks are used to combine multimodal features to enhance the representation learning of users and items. For example, UVCAN~\cite{liu2019user} learns multi-modal information of users and items using stacked attention networks. Moreover, several recent studies~\cite{wei2019mmgcn,wei2020graph,zhang2021mining,wang2021dualgnn,yi2021multi} leverage GNNs to exploit the item multimodal information.
%Recent studies~\cite{wei2019mmgcn,wei2020graph,zhang2021mining,wang2021dualgnn} have started to leverage Graph Neural Networks (GNNs) for building multimodal recommender systems.
For example, LATTICE~\cite{zhang2021mining} embeds a modality-aware graph structure learning layer, which identifies item-item graph structures using multimodal features.
% This layer injects high-order item affinities into item representations explicitly.
DualGNN~\cite{wang2021dualgnn} explicitly models the user's attention over different modalities and inductively learns her multi-modal preference. In MVGAE~\cite{yi2021multi}, a multi-modal variational graph auto-encoder is proposed to fuse modality-specific node embeddings according to the product-of-experts principle.
%MVGAE~\cite{yi2021multi} proposes
% The semantic information of each modality is weighted based on the estimation of uncertainty.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}

% This section introduces the details of the proposed framework for sequential recommendation with items' multimodal information. 
% Let $\mathcal{U}$ and $\mathcal{I}$ denote the set of users and items, respectively. For each user $u \in \mathcal{U}$, we denote her behavior sequence by $\C{S} = \{i_1, i_2, \cdots, i_n\}$, 

% \subsection{Notations}
Let $\mathcal{I}$ denote the set of items, and $\C{S} = \{i_1, i_2, \cdots, i_n\}$ denote a user behavior sequence, where $n$ items are sorted in a chronological order based on the interaction timestamp. In this work, we consider the text and image content of items to build the model. For each item $i$, it is associated with a chunk of text descriptions that is split into sentences as $\C{T}_i=\{t_1^i, t_2^i, \cdots , t^i_{|\mathcal{T}_i|}\}$, and a set of images $\C{V}_i=\{v_1^i, v^i_2, \cdots, v^i_{|\C{V}_i|}\}$, where $|\C{T}_i|$ and $|\C{V}_i|$ denote the number of sentences and images, respectively. Next, we introduce the main components of MSM4SR, including multimodal feature extraction, backbone network M$^2$SE, pre-training objectives, and fine-tuning objectives.


\subsection{Multimodal Feature Extraction}
Figure~\ref{fig:framework}(a) shows the workflow using pre-trained models to obtain the initial text and image features of items.

\subsubsection{Text Feature Extraction}
For each sentence in $\C{T}_i$, we feed it into the pre-trained Sentence-BERT~\cite{reimers-2019-sentence-bert} to obtain its latent representation. The initial text feature $\B{x}_i^t$ of item $i$ is obtained by stacking representations of all the sentences in $\C{T}_i$ as follows,
\begin{equation}
    \B{x}_i^t = stack\big[\textrm{BERT}(t^i_1), \textrm{BERT}(t^i_2), \cdots, \textrm{BERT}(t^i_{|\mathcal{T}_i|})\big],
\end{equation}
where $\B{x}_i^t\in\mathbb{R}^{|\mathcal{T}_i|\times d}$, $stack[,]$ denotes stacking multiple vectors into a matrix, and $d$ is the embedding dimension.

\subsubsection{Image Feature Extraction}~\label{sec:clip} Inspired by~\cite{lin2022towards}, we use a pre-trained language-image model, \ie CLIP~\cite{radford2021learning}, to describe each image by text tokens. This process can help eliminate the gap between text and image modality representations. To capture the key visual information of an image, $N$ most relevant text tokens are retained based on their similarities to the image. Then, we obtain the initial feature $\B{v}^i_\ell$ for an item image $v^i_\ell \in \C{V}_i$, by concatenating these text tokens as a sentence and feeding it into the same pre-trained Sentence-BERT model. The initial image feature $\B{x}_i^v$ of item $i$ can be obtained by stacking the features of all images in $\C{V}_i$ as follows,
\begin{align}
    f(w) &= \textrm{sim}(\textrm{CLIP}(v^i_\ell), \textrm{CLIP}(w)) \quad \forall w \in \C{D}, \nonumber\\
    \B{v}^i_{\ell}&=\textrm{BERT}(concat(\textrm{TopN}(\{f(w_1),\cdots,f(w_{|\mathcal{D}|})\}, N))), \nonumber\\
    \B{x}_i^v &= stack[\B{v}^i_{1}, \B{v}^i_{2}, \cdots, \B{v}^i_{|\mathcal{V}_i|}],
\label{eq:image2text}
\end{align}
where $\B{x}_i^v \in \mathbb{R}^{|\C{V}_i|\times d}$, $w$ is a text token in the word dictionary $\C{D}$, and $|\mathcal{D}|$ denotes the size of the dictionary. $\textrm{sim}(\cdot)$ is to compute the cosine similarity between the embedding of an image $v_\ell^i$ and a word $w$ obtained by the CLIP model.
% For each dataset, we create the dictionary using texts of all items, eliminating words that appear infrequently.
$\textrm{TopN}(\cdot)$ function selects $N$ words that have the highest similarities with the image. $concat(\cdot)$ is the operation of concatenating $N$ words into one sentence. Note that $\B{x}_i^t$ and $\B{x}_i^v$ are derived during data pre-processing stage. 
% More details about multimodal feature extraction are introduced in supplementary material. 


% The details about the pre-training of CLIP model and the construction of word dictionary for each dataset are discussed in the supplementary materials.


% \subsection{Tokenized Image Case Study}

In order to verify text tokens retrieved from images using the pre-trained language-image model, we select one representative item from the Amazon Pantry dataset for analysis. The details of the item are depicted in Figure~\ref{fig:image-tokens}. The item is an energy bar that aims to provide athletes with carbohydrates and protein. 
% The seller of this item provides a summary of it with relatively few words, which is not comprehensive enough to present the item to the customer. 
While the seller of the item provides a brief summary, it is not comprehensive enough to fully convey the item to customers.
% When text tokens are generated from images, they obtain the key information carried by images, such as its nutrition facts (\eg grains, vitamins, oats) and its usage modes (\eg adventure, supplement, energy). 
By generating text tokens from images, key information such as nutritional facts (\eg, grains, vitamins, oats) and usage modes (\eg, adventure, supplement, energy) are obtained. 
% A similar concept is applied to the Arts item. They are carpets with a variety of colors. In the merchant's product descriptions, there is an explicit description of how the carpet can be applied in different scenarios, but there is no description of its look. By using text tokens from images, we can describe the characteristics of the carpet based on its colors (\eg vermilion, blue, green-gold) and designs (\eg stardust, starburst, lava). 
Compared to the image feature extracted from pre-trained image encoders (\eg ResNet~\cite{wei2019mmgcn,zhang2021mining}), word tokens can discard irrelevant information to achieve better recommendation performance. Additionally, word tokens converted from images can provide merchants with a new perspective to enhance item descriptions and market items more effectively.


\subsection{Multimodal Mixup Sequence Encoder}

Next, we propose a backbone network, \ie M$^2$SE, to encode user sequences with multimodal features extracted from items. The structure of M$^2$SE is shown in Figure~\ref{fig:framework}(b). Observe that M$^2$SE includes four main components: sequence random dropout, text and image encoders, complementary sequence mixup, and transformer layers. Next, we introduce the details of each component.

\subsubsection{Sequence Random Dropout}
For a user behavior sequence $\C{S}$, M$^2$SE randomly drops a portion of items from $\C{S}$ with a drop ratio $\rho$, to help the model achieve better generalization performance. The obtained sub-sequence after the random dropout operation is denoted by $\widetilde{\C{S}}$.


\subsubsection{Text and Image Encoders}
These two encoders are used to adapt the initial modality features of items obtained from the pre-trained language model to learn users' sequential behaviors. Both encoders share the same structure, including an attention layer and a Mixture-of-Expert (MoE) architecture~\cite{shazeer2017outrageously}.


In the text encoder, each item $i \in \widetilde{\C{S}}$ is represented by its initial textual feature $\B{x}_i^t$. 
% the items in $\widetilde{\C{S}}$ are firstly represented by their initial textual features as $\widetilde{\C{S}}^t = \{\B{x}_1^{t}, \cdots, \B{x}_{n}^{t}\}$, where $n$ denotes the length of $\widetilde{\C{S}}$.
% To adapt modality features from a pre-trained language model for the sequential recommendation task, we utilize a modality encoder (\ie text/image encoder in Figure~\ref{fig:framework}) to process $\textbf{X}^m$~($m \in \{t, v\}$).
% The modality encoder has two main components, which are a self-attention layer and a Mixture-of-Expert (MoE) architecture~\cite{shazeer2017outrageously}.
The attention layer is composed of two linear transformations to fuse $i$'s sentence-level embeddings as follows,
\begin{align}
    \B{\alpha}^t &= \textrm{softmax}\big((\B{x}_i^t\B{W}^{t}_{1} + \B{b}^{t}_{1})\B{W}^{t}_{2}+b^{t}_{2}\big), \nonumber\\
    \B{e}_i^t &= \sum_{j=1}^{|\C{T}_i|}\alpha^t_j\B{x}_i^t[j,:],
\end{align}
where $\B{W}^{t}_{1}\in\mathbb{R}^{d\times d_a}$, $\B{W}^{t}_{2}\in\mathbb{R}^{d_a}$, $\B{b}^{t}_{1}\in\mathbb{R}^{d_a}$, and $b^{t}_{2}\in\mathbb{R}$ are learnable parameters. $d_a$ is the attention dimension size. $\alpha^t_j$ is the $j$-th element of $\alpha^t$, and $\B{x}_i^t[j,:]$ denotes the $j$-th row of feature matrix $\B{x}_i^t$. Then, MoE is used to increase the model’s capacity for adapting the fused modality representation $\B{e}_i^t$. Each expert in MoE consists of a linear transformation, followed with a dropout layer and a normalization layer. Let $E_k(\B{e}_i^t)\in\mathbb{R}^{d_0}$ denote the output of the $k$-th expert network, and $\B{g}^t\in\mathbb{R}^{O}$ is the output of the gating network as follows,
\begin{align}
    E_k(\B{e}_i^t) &= \textrm{LayerNorm}(\textrm{Dropout}(\B{e}_i^t\B{W}_k^t)), \nonumber\\
    \textbf{g}^t &= \textrm{softmax}(\B{e}_i^t\B{W}^t_{3}),
\end{align}
where $\B{W}^{t}_{3}\in\mathbb{R}^{d\times O}$ and $\textbf{W}_k^t\in\mathbb{R}^{d\times d_0}$ are learnable parameters, $O$ is the number of experts, and $d_0$ is the dimension of the hidden embedding. Then, the output of MoE for item $i$ is formulated as follows,
\begin{equation}
    \B{z}_i^t = \sum_{k=1}^{O}g_k^t E_{k}(\B{e}_i^t),
    \label{eq:x}
\end{equation}
where $\textbf{z}_i^t \in \mathbb{R}^{d_0}$, and $g^t_k$ is the weight derived from $k$-th gating router. Here, we omit bias terms in the equation for simplicity.
% We build another MoE network for the image modality as well to get $\widetilde{\textbf{x}}^i$.
% The output of the text encoder is the concatenation of the outputs of MoE network for all items in $\widetilde{\C{S}}$, which is denoted by $\B{Z}^t=stack[\B{z}_1^t, \B{z}_2^t, \cdots, \B{z}_{|\widetilde{\C{S}}|}^t]$.
The outputs of MoE network for all items in $\widetilde{\C{S}}$ are stacked to form the output of the text encoder, which is denoted by $\B{Z}^t=stack[\B{z}_1^t, \B{z}_2^t, \cdots, \B{z}_{|\widetilde{\C{S}}|}^t]$.

Similarly, in the image encoder, each item in $\widetilde{\C{S}}$ is represented by its image feature $\B{x}_i^v$. The output of the image encoder is denoted by $\B{Z}^v=stack[\B{z}_1^v, \B{z}_2^v, \cdots, \B{z}_{|\widetilde{\C{S}}|}^v]$, where $\B{z}_i^v$ is the output of the MoE network for the $i$-th item in $\widetilde{\C{S}}$.

\subsubsection{Complementary Sequence Mixup}

To alleviate the representation discrepancy between two different modality sequences, we propose a complementary sequence mixup method, which mixes up text representations and image representations in a complementary manner. Specifically, we define a mixup ratio $p$ between $0$ to $0.5$, which is randomly generated during model training. For each item in $\widetilde{\C{S}}$, we swap its embedding in $\B{Z}^t$ and $\B{Z}^v$ with probability $p$ and generate two mix-modality sequence embeddings $\B{M}^t$ and $\B{M}^v$. The definition of $p \le 0.5$ ensures the generated mix-modality sequence embedding dominates with information from the same modality. In this case, $\B{M}^t$ and $\B{M}^v$ complement each other in terms of the modality choice for each item in the sequence.


\subsubsection{Transformer Layers}

The Transformer structure~\cite{vaswani2017attention} is used to further encode $\B{M}^t$ and $\B{M}^v$.
% We first stack each item's embedding in $\B{M}^t$ and $\B{M}^v$ to form two new matrices $\B{H}^t_0\in\mathbb{R}^{n\times d_0}$ and $\B{H}^v_0\in\mathbb{R}^{n\times d_0}$, respectively.
We first add positional encodings to $\B{M}^t$ and $\B{M}^v$, and then feed the summed embeddings into $L$ Transformer layers. Note that each Transformer layer consists of a multi-head self-attention sub-layer and a point-wise feed-forward network. Let $\B{H}^t_{L}$ and $\B{H}^v_{L}$ denote the output of the $L$-th Transformer layer based on $\B{M}^t$ and $\B{M}^v$, respectively.
Following~\cite{zhou2020s3}, we use last rows in $\B{H}^t_{L}$ and $\B{H}^v_{L}$ as two mix-modality representations of the input sequence, which are denoted by $\B{h}^t$ and $\B{h}^v$.

% Given the $(\ell-1)$-th layer's hidden representation $\textbf{H}^t_{\ell-1}$ and $\textbf{H}^v_{\ell-1}$ from the transformer block, the $\ell$-th layer outputs are as follows,
% \begin{align}
%     \textbf{H}^t_{\ell} &= \textrm{FFN}\big(\textrm{MHSA}(\textbf{H}^t_{\ell-1})\big),\nonumber\\
%     \textbf{H}^v_{\ell} &= \textrm{FFN}\big(\textrm{MHSA}(\textbf{H}^v_{\ell-1})\big).
%     \label{eq:trm}
% \end{align}

% The last elements of $\textbf{H}^t_{\ell}$ and $\textbf{H}^v_{\ell}$ are respectively denoted by $\textbf{h}^t_{\ell,n}$ and $\textbf{h}^v_{\ell,n}$, and can be treated as the sequence representations~\cite{zhou2020s3}. $\textbf{h}^t_{\ell,n}$ and $\textbf{h}^v_{\ell,n}$ correspond to the hidden embedding of the last position in the sequence at layer $\ell$. For simplicity, we use $\textbf{m}^t$ and $\textbf{m}^v$ to denote $\textbf{h}^t_{\ell,n}$ and $\textbf{h}^v_{\ell,n}$ respectively. Note that $\textbf{H}^t_{l}$ and $\textbf{H}^v_{l}$ share the same transformer structure.


\subsection{Pre-training Objectives}

To pre-train the backbone model, we propose two optimization objectives, \ie modality-specific next item prediction, and cross modality contrastive learning, based on mix-modality sequence representations.
%$\B{h}^t$ and $\B{h}^v$.
These objectives enable the backbone model to better capture the connections between representations across different modalities. 
The workflow in the pre-training phase is shown in Figure~\ref{fig:framework}(c). 

Let $\C{B}=\{(\C{S}_j, i_j)\}_{j=1}^{|\C{B}|}$ denote a batch of pre-training data, where $\C{S}_j$ denotes a user's behavior sequence and $i_j$ is her next interaction item after $\C{S}_j$. With M$^2$SE, we can obtain two mix-modality sequence representations $\B{h}_j^{t}$ and $\B{h}_j^{v}$ for $\C{S}_j$. As $\B{h}_j^t$ and $\B{h}_j^v$ are obtained by mixing up modalities, we first use two linear transformations to map them into the text feature space and image feature space respectively,
\begin{align}
    \widehat{\B{m}}_j^{t} &= \B{h}^{t}_j\B{W}_t + \B{b}_t,
    & \widetilde{\B{m}}_j^{t} &= \B{h}_j^{v}\B{W}_t + \B{b}_t, \nonumber\\
    \widehat{\B{m}}_j^{v} &= \B{h}_j^{v}\B{W}_v + \B{b}_v,
    & \widetilde{\B{m}}_j^{v} &= \B{h}_j^{t}\B{W}_v + \B{b}_v,
\end{align}
where $\B{W}_t, \B{W}_v\in\mathbb{R}^{d_0\times d_0}$ and $\B{b}_t,\B{b}_v\in\mathbb{R}^{d_0}$ are learnable parameters.
%to transform mixed sequence representations into the text space and the image space.
Motivated by the success of contrastive learning in model pre-training, we define the pre-training objective functions in a contrastive manner. %in both the text and image feature space.


\subsubsection{Modality-specific Next Item Prediction}
Modality-specific Next Item Prediction (NIP) aims to predict the next item based on the mix-modality sequence representations.
For each $(\C{S}_j, i_j)$ pair, $\C{S}_j$ is the input sequence and $i_j$ is the target item. Thus, in the text feature space, we pair $\widehat{\B{m}}_j^t$ and $\widetilde{\B{m}}_j^t$ with the $i_j$'s text embedding $\B{z}_j^t$ obtained by the text encoder as a positive sample, and pair $\widehat{\B{m}}_j^t$ and $\widetilde{\B{m}}_j^t$ with the text embeddings of other items $\{i_{j'}|j' \neq j, 1 \leq j' \leq |\C{B}|\}$ from $\C{B}$ as negative samples. The next item prediction loss defined in the text feature space is as follows,
\begin{align}
    \mathcal{L}_\textrm{NIP}^{(t)} =
    -\sum^{|\C{B}|}_{j=1} \log\frac{
    f(\widehat{\B{m}}_{j}^t, \B{z}^{t}_{j}) +
    f(\widetilde{\B{m}}_{j}^t, \B{z}^{t}_{j})}
    {\sum_{j'=1}^{|\C{B}|} [f(\widehat{\B{m}}_{j}^t, \B{z}^{t}_{j'}) + f(\widetilde{\B{m}}_{j}^t, \B{z}^{t}_{j'})]},
\end{align}
where $ f(\B{s}, \B{z}) = \textrm{exp}(\textrm{sim}(\B{s}, \B{z})/\tau)$, and $\tau$ is a temperature hyper-parameter. Similarly, we can define the next item prediction loss $\mathcal{L}_\textrm{NIP}^{(v)}$ in the image feature space as follows,
\begin{align}
    \mathcal{L}_\textrm{NIP}^{(v)} =
    -\sum^{|\C{B}|}_{j=1} \log\frac{
    f(\widehat{\B{m}}_{j}^v, \B{z}^{v}_{j}) +
    f(\widetilde{\B{m}}_{j}^v, \B{z}^{v}_{j})}
    {\sum_{j'=1}^{|\C{B}|} [f(\widehat{\B{m}}_{j}^v, \B{z}^{v}_{j'}) + f(\widetilde{\B{m}}_{j}^v, \B{z}^{v}_{j'})]}.
\end{align}


\subsubsection{Cross Modality Contrastive Learning}
% To better capture and align the semantic relationships between different modalities,
To capture the semantic relationship between different modalities, we develop a Cross Modality Contrastive Loss (CMCL). Specifically, the complementary mix-modality sequence representations mapped to the same feature space, \eg ($\widehat{\B{m}}_j^t$, $\widetilde{\B{m}}_j^t$) and ($\widehat{\B{m}}_j^v$, $\widetilde{\B{m}}_j^v$), are paired as positive samples, while randomly-selected samples in the training batch are paired as negative samples. 
% CMCL aims to bring positive samples closer in the feature space than those negative samples.
Following~\cite{zhu2021graph}, CMCL for the text space is defined in a symmetric contrastive way as follows,
\begin{align}
    &\ell(\widehat{\B{m}}_{j}^t, \widetilde{\B{m}}_{j}^t) =
    \log\frac{
    f(\widehat{\B{m}}_{j}^t,\widetilde{\B{m}}_{j}^t)}
    {\sum\limits_{j'=1}^{|\C{B}|} f(\widehat{\B{m}}_{j}^t, \widetilde{\B{m}}_{j'}^t) +
    \sum\limits_{j'\neq j}^{|\C{B}|} f(\widehat{\B{m}}_{j}^t, \widehat{\B{m}}_{j'}^t)},\nonumber\\
    &\mathcal{L}_\textrm{CMCL}^{(t)} = -\frac{1}{2}\sum_{j=1}^{|\C{B}|}(
    \ell(\widehat{\B{m}}_{j}^t, \widetilde{\B{m}}_{j}^t) +
    \ell(\widetilde{\B{m}}_{j}^t, \widehat{\B{m}}_{j}^t)).
\end{align}
Similarly, CMCL in the image space is defined as follows,
\begin{equation}
    \mathcal{L}_\textrm{CMCL}^{(v)} = -\frac{1}{2}\sum_{j=1}^\C{B}(
    \ell(\widehat{\B{m}}_{j}^v, \widetilde{\B{m}}_{j}^v) +
    \ell(\widetilde{\B{m}}_{j}^v, \widehat{\B{m}}_{j}^v)).
\end{equation}
% In summary, we employ a multi-task learning scheme to jointly optimize the modality-wise next prediction loss and the cross-modality contrastive loss during pre-training.

The overall loss function for model pre-training is formulated as follows,
\begin{equation}
    \mathcal{L}_{\textrm{pre-train}} = \mathcal{L}_\textrm{NIP}^{(t)} + \mathcal{L}_\textrm{NIP}^{(v)} + \lambda(\mathcal{L}_\textrm{CMCL}^{(t)} + \mathcal{L}_\textrm{CMCL}^{(v)}),
\end{equation}
where $\lambda$ is a hyper-parameter used to balance these two groups of losses.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\columnwidth]{finetune_m.pdf}
%     \caption{The multimodal fine-tuning framework.}
%     \label{fig:left-to-right}
% \end{figure}

\subsection{Fine-tuning for Sequential Recommendation}

% Let $\widetilde{\C{B}}=\{(\widetilde{\C{S}}_j, \widetilde{i}_j)\}_{j=1}^{|\widetilde{\C{B}}|}$ denote a batch of fine-tuning data, where $\widetilde{i}_j$ is a user's next interaction item after her previous interaction sequence $\widetilde{\C{S}}_j$. Then, $\widetilde{i}_j$ can be treated as the label of the sequence $\widetilde{\C{S}}_j$

% Figure~\ref{fig:left-to-right} illustrates the fine-tuning framework based on multimodal item information. 
Let $\widetilde{\C{B}}$ denote a batch of fine-tuning data. For each $(\C{S}, i) \in \widetilde{\C{B}}$, $i$ is the user's next interaction item after her interaction sequence $\C{S}$. From the classification perspective, $i$ can be treated as the target label of the input sequence $\C{S}$. Following~\cite{kang2018self,zhou2020s3}, we treat sequential recommendation as a supervised classification problem and use cross-entropy loss for model fine-tuning. 

In the fine-tuning stage, we disable the sequence random dropout and complementary sequence mixup operations in the pre-trained M$^2$SE network, by setting the dropout ratio $\rho$ and sequence mix up ratio $p$ to 0. Moreover, we also incorporate the ID embeddings $\B{E}_{\C{S}}$ of items in the sequence $\C{S}$ with its text and image representations $\B{M}^t$ and $\B{M}^v$ using element-wise summation, and then feed the summed embeddings into the Transformer layers to obtain the sequence embeddings $\B{h}^t$ and $\B{h}^v$. Then, for the sequence $\C{S}$, the predicted probability distribution $\widehat{\B{y}}^{(\C{S})}$ of its potential target labels (\ie items) is defined as follows,
\begin{align}
    \widehat{\B{y}}^{(\C{S})} = \textrm{softmax}
    \big(\B{h}^{t}(\B{F}^t+\B{E})^{\top}
    +\B{h}^{v}(\B{F}^v+\B{E})^{\top}\big),
    \label{eq:ft1}
\end{align}
where $\widehat{\B{y}}^{(\C{S})}\in\mathbb{R}^{|\mathcal{I}|}$, $\B{E}\in\mathbb{R}^{|\mathcal{I}|\times d_0}$ denotes the ID embedding matrix of all items. $\B{F}^{t}, \B{F}^{v}\in\mathbb{R}^{|\mathcal{I}|\times d_0}$ denote the text and image modality embedding matrices of all items, which are obtained by the text encoder and image encoder in M$^2$SE. The loss function for model fine-tuning is defined as follows,
\begin{equation}
    \mathcal{L}_{\textrm{finetune}} = -\sum_{(\C{S}, i) \in \widetilde{\C{B}}} \log\big(\widehat{\B{y}}^{(\C{S})}(i)\big),
    \label{eq:s2}
\end{equation}
where $\widehat{\B{y}}^{(\C{S})}(i)$ denotes the predicted probability for the ground-truth label $i$ of the sequence $\C{S}$. By minimizing Eq.~\eqref{eq:s2}, we fine-tune the parameters of the pre-trained M$^2$SE network, as well as the ID embedding of items.

Alternatively, MSM4SR can be applied when only one modality is available. A comparison between unimodal and multimodal settings is discussed in the Section~\ref{sec:unimodal}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
% In this section, we perform extensive experiments on three public datasets for sequential recommendation scenarios. A comparison between the proposed method and the current state-of-the-art recommendation baselines demonstrates its effectiveness.

\subsection{Experimental Settings}

\subsubsection{Datasets}
The experiments are conducted on the Amazon review dataset~\cite{ni2019justifying}, which provides the multimodal information of items. We use three ``5-core" subsets for experimental evaluation, \ie \textit{Pantry}, \textit{Arts}, and \textit{Office}.
% three widely-used datasets: \textit{Pantry}, \textit{Arts}, and \textit{Office}, which are sub-categories of the Amazon review dataset~\cite{ni2019justifying} that provides the multimodal information of items.
Following~\cite{zhou2020s3,IJCAI-GCL4SR}, we convert each rating into an implicit feedback record.
% For all datasets, we keep ``5-core" datasets, in which users and items with less than five interactions are iteratively removed.
On each dataset, we group interactions by users and construct the interaction sequence for each user by sorting her interactions in a chronological order.
The statistics of the pre-processed experimental datasets are summarized in Table~\ref{tab:stats}.

\begin{table}
    \caption{Statistics of the experimental datasets. ``Avg. n" denotes the average length of interaction sequences.}
  \centering
   \footnotesize
  \begin{tabular}{l|cccc}
    \toprule
    Datasets & \#Users & \#Items & \#Inter. & Avg. n\\ 
    \midrule
    % Science &8822&17562&62083&7.04 \\
    Pantry & 13,614 & 7,670 &131,311 &9.65 \\
    Arts   & 32,216 & 52,557 &264,465 &8.21 \\
    Office & 68,224 & 59,705 &527,209 &7.73 \\
    \bottomrule
  \end{tabular}
  \label{tab:stats}
\end{table}
\begin{table*}
    \caption{The overall performance achieved by different methods. The best results are in \textbf{boldface}, and the second best results are \underline{underlined}. \%Imp indicates the relative improvement percentage of MSM4SR over the best baseline method.}
    \centering
    \footnotesize
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{l|l| c c c c c c c c c c c c c}
    \toprule
    Dataset & Metric & LightGCN & GRCN & DualGNN & MV-RNN & SASRec & $\textrm{S}^3$-Rec & SINE & DIF-SR & $\text{SASRecM}$  & MSM4SR & \%Imp \\
    \midrule
    \multirow{6}{*}{Pantry}
    &R@5&0.0270&0.0365&0.0321&0.0157&0.0277&0.0315&0.0297&0.0300&\underline{0.0369}&\textbf{0.0405}&9.76\%\\
    &R@10&0.0460&0.0552&0.0485&0.0276&0.0457&0.0535&0.0534&0.0473&\underline{0.0600}&\textbf{0.0673}&12.17\%\\
    &R@20&0.0774&0.0856&0.0739&0.0467&0.0722&0.0845&0.0873&0.0736&\underline{0.0934}&\textbf{0.1040}&11.35\%\\
    &N@5&0.0176&\underline{0.0229}&0.0202&0.0101&0.0147&0.0187&0.0167&0.0163&0.0223&\textbf{0.0235}&2.62\%\\
    &N@10&0.0236&0.0289&0.0254&0.0134&0.0204&0.0257&0.0243&0.0219&\underline{0.0298}&\textbf{0.0321}&7.72\%\\
    &N@20&0.0315&0.0366&0.0318&0.0184&0.0271&0.0335&0.0329&0.0284&\underline{0.0382}&\textbf{0.0414}&8.38\%\\
    \midrule
    \multirow{6}{*}{Arts}
    &R@5&0.0543&0.0546&0.0596&0.0299&0.0704&0.0715&0.0667&0.0712&\underline{0.0816}&\textbf{0.0854}&4.66\%\\
    &R@10&0.0726&0.0741&0.0788&0.0446&0.0910&0.0961&0.0935&0.0899&\underline{0.1099}&\textbf{0.1184}&7.73\%\\
    &R@20&0.0967&0.0999&0.1033&0.0661&0.1125&0.1250&0.1237&0.1126&\underline{0.1430}&\textbf{0.1570}&9.79\%\\
    &N@5&0.0381&0.0386&0.0433&0.0184&0.0442&0.0467&0.0404&0.0449&\underline{0.0525}&\textbf{0.0531}&1.14\%\\
    &N@10&0.0440&0.0448&0.0495&0.0232&0.0509&0.0546&0.0491&0.0510&\underline{0.0616}&\textbf{0.0637}&3.41\%\\
    &N@20&0.0501&0.0513&0.0557&0.0283&0.0563&0.0619&0.0567&0.0567&\underline{0.0699}&\textbf{0.0735}&5.15\%\\
    \midrule
    \multirow{6}{*}{Office}
    &R@5&0.0325&0.0556&0.0518&0.0259&0.0841&0.0823&0.0837&\underline{0.0857}&0.0850&\textbf{0.0968}&12.95\%\\
    &R@10&0.0518&0.0714&0.0661&0.0416&0.1025&0.1027&0.1059&0.1039&\underline{0.1060}&\textbf{0.1206}&13.77\%\\
    &R@20&0.0752&0.0911&0.0843&0.0641&0.1222&0.1254&0.1305&0.1241&\underline{0.1316}&\textbf{0.1480}&12.46\%\\
    &N@5&0.0219&0.0408&0.0385&0.0159&0.0558&0.0575&0.0546&0.0561&\underline{0.0584}&\textbf{0.0721}&23.46\%\\
    &N@10&0.0281&0.0460&0.0431&0.0210&0.0617&0.0641&0.0618&0.0620&\underline{0.0652}&\textbf{0.0797}&22.23\%\\
    &N@20&0.0339&0.0509&0.0477&0.0266&0.0667&0.0698&0.0680&0.0671&\underline{0.0716}&\textbf{0.0866}&20.95\%\\
    \bottomrule
    \end{tabular}
    \label{tab:overall}
\end{table*}


\subsubsection{Evaluation Settings}
Following~\cite{zhou2020s3,sun2019bert4rec}, we apply the \textit{leave-one-out} strategy to evaluate the performance of recommendation models in both pre-training and fine-tuning stages. Specifically, for each user, the last item of her interaction sequence is used for testing, the second last item is used for validation, and the remaining items are used for model training. The performance of a recommendation model is evaluated by two widely used metrics, \ie Recall@$K$ and Normalized Discounted Cumulative Gain@$K$ (respectively denoted by R@$K$ and N@$K$). $K$ is empirically set to 5, 10, and 20.
% We rank the prediction on the whole item set. The performance is reported using top-$k$ Recall (R@$k$) and top-$k$ Normalized Discounted Cumulative Gain (N@$k$) with $k$ chosen from $\{5, 10, 20\}$.
All evaluation metrics are computed on the whole candidate item set without negative sampling.


\subsubsection{Baseline Methods}
We compare the proposed model with three groups of baseline methods:

\begin{itemize}
    \item \textit{General Recommendation Model:} \textbf{LightGCN}~\cite{he2020lightgcn} is one of the most representative GNN-based recommendation methods. % a GNNs-based model for recommendation. 
    It simplifies the design of GCNs for collaborative filtering by discarding feature transformation and non-linear activation. 
        % The recommendation performance is improved while training difficulty and computation complexity is reduced.
    \item \textit{Multimodal Recommendation Models:} 1) \textbf{GRCN}~\cite{wei2020graph} is a graph-based multimodal recommendation model that refines the user-item interaction graph by identifying false-positive feedback and pruning noisy edges; 2) \textbf{DualGNN}~\cite{wang2021dualgnn} incorporates a multimodal representation learning module to explicitly model the user's attentions over different modalities.
    \item \textit{Sequential Recommendation Models:} 
    1) \textbf{MV-RNN}~\cite{cui2018mv} is a multimodal sequential recommendation model that combines multimodal features at its input and applies a recurrent structure to dynamically capture users' interests; 
    2) \textbf{SASRec}~\cite{kang2018self} is a directional self-attention method for next item prediction;
    % \textbf{BERT4Rec}~\cite{sun2019bert4rec} is a bi-directional self-attention method which introduces the Cloze task to predict the masked items from both left and right contexts.
    3) \textbf{$\textrm{S}^3$-Rec}~\cite{zhou2020s3} devises four self-supervised learning objectives for sequential recommendation based on the mutual information maximization principle; 
    4) \textbf{SINE}~\cite{tan2021sparse} introduces a sparse-interest module that adaptively infers a sparse set of concepts and outputs multiple embeddings for each user; 
    5) \textbf{DIF-SR}~\cite{xie2022decoupled} moves various attribute information from the input to the attention layer, and decouples attribute information and item representation from the calculation of attention;
    6) \textbf{$\text{SASRecM}$} integrates multimodal information of items with the SASRec architecture. Specifically, the model encodes modality features using the same text/image encoder as MSM4SR, sums them with item ID embeddings, and feeds the output into SASRec to generate recommendations.
\end{itemize}


% \subsubsection{Multimodal Feature Extraction}
% We introduce how to pre-process and extract features from textual descriptions and visual images of items in detail. 

% \textbf{Text Feature Extraction.} \label{sec:text_feat}
% % In this section, we introduce how to pre-process textual descriptions for items in detail. 
% For every item in the dataset, its ``descriptions" are collected to generate the text representation. We first split a chunk of descriptions into multiple sentences. For every sentence, we perform text pre-processing steps, including removing punctuation and stopwords, tokenization, stemming, and lemmatization using the Natural Language Toolkit (NLTK)~\cite{bird2009natural}, which is an open-source Python library for Natural Language Processing. Next, the pre-processed sentence is fed into the Sentence-BERT model~\cite{reimers-2019-sentence-bert} to obtain its latent representation. %(\textcolor{blue}{introduce the dimension of the representation}).


% \textbf{Image Feature Extraction.}
% % In this section, we introduce how to build a word dictionary specific to each dataset.
% We first build a word dictionary specific to each dataset. As different datasets contain items from different categories, there are unique words for describing items within each category. For every dataset, we collect words from ``title" for all items. Next, we perform the same text pre-processing steps on collected words, as mentioned in Section~\ref{sec:text_feat}. 
% % the previous section ``Text Feature Extraction". 
% Then, words that only appear once are removed. As a result, the size of the word dictionaries of Pantry, Arts, and Office datasets are $4,260$, $11,958$, and $10,190$, respectively. Next, CLIP~\cite{radford2021learning} is employed to convert images associated with each item into text tokens based on the word dictionary.
% Text tokens of all images of an item are combined and fed into the Sentence-BERT model~\cite{reimers-2019-sentence-bert} to obtain its latent representation. Details are explained in the Methodology Section~\ref{sec:clip}.

\subsubsection{Implementation Details}
% All evaluated methods are implemented using the open-source framework RecBole~\cite{zhao2021recbole}. 
The proposed method is implemented by Pytorch~\cite{paszke2019pytorch} and an open-source recommendation framework RecBole~\cite{zhao2021recbole}. The Adam optimizer~\cite{kingma2014adam} is used to learn model parameters. Following~\cite{zhou2020s3}, we set the maximum sequence length to $50$. Our training phase consists of two stages: pre-training and fine-tuning. The learned parameters from the pre-training stage are used to initialize the M$^2$SE network in the fine-tuning stage. Both the pre-training and fine-tuning are performed on the same dataset to obtain the final recommendation results.

In the multimodal feature extraction stage of MSM4SR, 
% descriptions and images associated with the items are collected from the datasets.
the pre-trained Sentence-BERT model maps every sentence of text descriptions or a group of word tokens extracted from an image into a $768$-dimensional dense vector, \ie $d=768$. For each item, we consider up to $10$ sentences and $10$ images. 

For pre-training MSM4SR, we set the learning rate to 0.001, batch size to 1024, and the number of experts $O$ to 8, on all datasets. In addition, we set $\rho$, $\tau$, and $\lambda$ to 0.2, 0.07, and 0.01, respectively. The attention dimension $d_a$ and embedding dimension $d_0$ are fixed to $64$. The proposed model is pre-trained for $300$ epochs. 

For fine-tuning MSM4SR and training baseline methods, 
% the embedding dimension is set to $64$. Then, 
we apply grid-search to identify the best hyper-parameter settings based on the validation data for each method. The search space is as follows: learning rate in $\{0.0001, 0.0005, 0.001\}$, batch size in $\{256, 512, 1024\}$, and weight decay in $\{0.0001, 0.0005, 0.001\}$. 
For fair comparison, the hyper-parameters of Transformer layers are kept identical for MSM4SR and transformer-based baselines (\ie SASRec, $\textrm{S}^3$-Rec, and DIF-SR). Specifically, the number of attention heads and number of self-attention blocks are set to 2. The remaining hyper-parameters for baseline methods follow the original papers. Additionally, we adopt an early stopping strategy, \ie we apply a premature stopping if R@20 on the validation data does not increase for 10 epochs.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Performance Comparison}
\begin{table*}[!ht]
    \caption{The recommendation performance of MSM4SR and its variants on Pantry and Office datasets.}
    \centering
    \footnotesize
    \begin{tabular}{l | c c c c c c| c c c c c c}
    \toprule
          \multirow{2}{*}{Methods}&
     \multicolumn{6}{c|}{Pantry} &
     \multicolumn {6}{c}{Office} \\ %\cline{2-5}
     & R@5 &R@10 & R@20 & N@5 &N@10 & N@20 & R@5 &R@10 & R@20 & N@5 &N@10& N@20\\
     \midrule
     MSM4SR &\textbf{0.0405}	&\textbf{0.0673} &\textbf{0.1040}&\textbf{0.0235}&\textbf{0.0321}&\textbf{0.0414} &\textbf{0.0968} &\textbf{0.1206}&\textbf{0.1480}&\textbf{0.0721}&\textbf{0.0797}&\textbf{0.0866}\\
     
     MSM4SR\textsubscript{ResNet} &0.0387&0.0647&0.1007&0.0234&0.0317&0.0408
     &0.0924	&0.1159&0.1435	&0.0673&0.0749	&0.0818\\
     % MSM4SR\textsubscript{w/o Seq Dropout} & 0.0381	&0.0984	& 0.0223 &0.0393 &0.0948	&0.1461&0.0706&0.0852\\
     MSM4SR\textsubscript{w/o NIP} &0.0292&0.0501&0.0816&0.0179 &0.0245	&0.0324
     &0.0863 &0.1062&0.1302&0.0573&0.0637&0.0697\\
     MSM4SR\textsubscript{w/o CMCL} &0.0390 &0.0662& 0.1030	&0.0223&0.0310& 0.0403
     &0.0875 &0.1095&0.1349&0.0578&0.0649&0.0713\\
     MSM4SR\textsubscript{w/o Mixup} &0.0380	&0.0649&0.1014	&0.0220 &0.0307&0.0399
     &0.0957&0.1192&0.1480&0.0692&0.0768&0.0841\\
     MSM4SR\textsubscript{w/o Pre-train} &0.0359&0.0595&0.0920&0.0203&0.0286&0.0369
     &0.0883 &0.1094	&0.1335&0.0597&0.0665&0.0726\\
     MSM4SR\textsubscript{E2E} &0.0355&0.0605&0.0937&0.0209&0.0290&0.0373 &0.0821&0.1013&0.1243&0.0527&0.0589&0.0647 \\
    \bottomrule
    \end{tabular}
    \label{tab:ablation}
\end{table*}
We summarize the overall performance comparison results in Table~\ref{tab:overall}, from which we have the following observations.
\textit{Firstly}, the multimodal recommendation methods (\ie GRCN and DualGNN) consistently outperform the general recommendation model (\ie LightGCN). This suggests that leveraging the multimodal information of items can effectively enhance the recommendation performance.
\textit{Secondly}, sequential recommendation models generally perform better than non-sequential recommendation models, by capturing users' sequential behavior patterns. 
% However, on Pantry dataset, the non-sequential model GRCN exploiting multimodal data achieves better performance than the sequential baseline methods. 
However, an exception to this is observed on the Pantry dataset, where the non-sequential model GRCN, which utilizes multimodal data, achieves better performance than sequential baseline methods.
One potential explanation is that the multimodal features of items are more informative for this dataset, compared with other two datasets. Meanwhile, $\textrm{S}^3$-Rec usually achieves better performance than other sequential recommendation baselines, highlighting the effectiveness of using self-supervised signals and side information (\ie item attributes) for pre-training sequential recommendation models.
\textit{Thirdly}, 
% MSM4SR and SASRecM both outperform baseline methods on most evaluation metrics. As compared to baseline methods, they can leverage multimodal data and simultaneously capture sequential behavior patterns of users to improve recommendation accuracy.
MSM4SR and SASRecM both outperform baseline methods on most evaluation metrics by leveraging multimodal data and simultaneously capturing sequential behavior patterns of users to improve recommendation accuracy. 
\textit{Lastly}, the proposed MSM4SR model consistently outperforms all baseline methods by a significant margin. 
% By leveraging two pre-training objectives to capture the multimodal data correlation with user behaviors, MSM4SR improves the generalization capabilities of sequential recommendation models, which leads to the best performance.
This is attributed to the utilization of two pre-training objectives to capture the correlation of multimodal data with user behaviors, thereby improving the generalization capabilities of sequential recommendation models and resulting in the best overall performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Study}
To study the contribution of each component of MSM4SR to sequential recommendation, we consider the following variants of MSM4SR for evaluation: 
1) \textbf{MSM4SR}\textsubscript{ResNet}: we use ResNet to extract features of item images, instead of converting an item image into keywords; 
% 2) \textbf{MSM4SR}\textsubscript{w/o Seq Dropout}: we remove the sequence random dropout module in M$^2$SE; 
2) \textbf{MSM4SR}\textsubscript{w/o NIP}: we remove the modality-wise next item prediction losses in the pre-training stage; 
3) \textbf{MSM4SR}\textsubscript{w/o CMCL}: we remove the cross-modality contrastive losses in the pre-training stage;
4) \textbf{MSM4SR}\textsubscript{w/o Mixup}: we remove the complementary sequence mixup module in M$^2$SE;
% is removed and sequences only contain unimodal information;
5) \textbf{MSM4SR}\textsubscript{w/o Pre-train}: we remove the pre-training tasks and train the proposed model from scratch based on the multimodal fine-tuning setting.
6) \textbf{MSM4SR}\textsubscript{E2E}: we optimize the proposed model in an end-to-end manner by summing up the pre-training loss~$\mathcal{L}_{\textrm{pre-train}}$ and the finetuning loss~$\mathcal{L}_{\textrm{finetune}}$.

% \begin{table}
% \small
%     \centering
%     \begin{tabular}{l | c c| c c }
%     \toprule
%      \multirow{2}{*}{Methods}&
%      \multicolumn{2}{c|}{Pantry} &
%      \multicolumn {2}{c}{Office} \\ %\cline{2-5}
%      & R@20 & N@20 & R@20 & N@20\\
%      \midrule
%      MSM4SR &\textbf{0.1040}&\textbf{0.0414} &\textbf{0.1483}&\textbf{0.0856}\\
%     %  Coarse &0.1003&0.0400     &0.1475&0.0869\\
%      MSM4SR\textsubscript{ResNet} &0.1016&0.0406     &0.1480&0.0844\\
%      MSM4SR\textsubscript{w/o Seq Dropout} &0.0984&0.0393    &0.1461&0.0852\\
%      MSM4SR\textsubscript{w/o NIP} &0.0816&0.0324     &0.1302&0.0697\\
%      MSM4SR\textsubscript{w/o CMCL} &0.1030&0.0403    &0.1349&0.0713\\
%      MSM4SR\textsubscript{w/o Mixup} &0.1015&0.0408  &0.1471&0.0851\\
%      MSM4SR\textsubscript{w/o Pre-train} &0.0920&0.0369 &0.1335&0.0726\\
%     \bottomrule
%     \end{tabular}
%     \caption{The recommendation performance of MSM4SR and its variants on Pantry and Office datasets.}
%     \label{tab:ablation}
% \end{table}


Table~\ref{tab:ablation} presents the performance of MSM4SR and its variants on Pantry and Office datasets. It shows that each proposed component of MSM4SR consistently improves recommendation performance. Moreover, the modality-wise next item prediction losses are particularly important for pre-training MSM4SR for sequential recommendation, as removing them leads to a significant decline in performance.
% This result may be explained by the fact that next item prediction is the objective that is optimized at both pre-train and fine-tuning stage.
% The potential reason is that our model optimizes the next item prediction as its main objective at both pre-training and fine-tuning stages. 
This is likely due to the fact that the model optimizes next item prediction as its main objective during both pre-training and fine-tuning stages.
Furthermore, cross modality contrastive losses are more effective in improving recommendation performance on Office dataset compared to the Pantry dataset, indicating that items in the Office dataset provide more training signals to align various modalities.
% . This indicates that items on office dataset are providing more training signals to align various modalities.
% One possible reason is that more items in the office are providing more training signals to align various modalities.
Additionally, we note that the recommendation performance decreases significantly when pre-training tasks are removed, which further validates the effectiveness of applying pre-training for multimodal sequential recommendation.
Lastly, if the model is trained end-to-end by combining the $\mathcal{L}_{\textrm{pre-train}}$ and $\mathcal{L}_{\textrm{finetune}}$, the performance becomes worse as pre-train losses aim to learn the interactions across different modalities, while finetuning losses emphasize recommendation tasks using cross entropy losses. If they are optimized together, the model is unable to converge to the optimal solution for the recommendation task.

\subsection{Unimodal vs Multimodal Performance}
\label{sec:unimodal}
To study the effectiveness of MSM4SR in exploiting different modality information, we consider the following two variants of MSM4SR for evaluation, \ie 1) \textbf{MSM4SR-V}: the text modality information is not used for model fine-tuning (\ie removing $\B{h}^{t}(\B{F}^t+\B{E})^{\top}$ from Eq.~\eqref{eq:ft1}); 2) \textbf{MSM4SR-T}: the image modality information is not used for model fine-tuning (\ie removing $\B{h}^{v}(\B{F}^v+\B{E})^{\top}$ from Eq.~\eqref{eq:ft1}). In MSM4SR-V, MSM4SR-T, and MSM4SR, the model is pre-trained with both text and image modalities. 
% Table~\ref{tab:compare} summarizes the recommendation performance achieved by MSM4SR-V, MSM4SR-T, MSM4SR, and the best baseline method on each dataset. 
Table~\ref{tab:compare} presents the recommendation performance of MSM4SR-V, MSM4SR-T, and MSM4SR, as well as the best baseline method on each dataset. 
We can note that MSM4SR-T outperforms MSM4SR-V on all datasets, indicating that text information of items contributes more to performance gain than item images. Leveraging both text and image modality information leads to the best recommendation performance for most datasets. This illustrates the effectiveness of exploiting items' multimodal information for sequential recommendation.

\begin{table}
    \caption{The recommendation performance achieved by the best baseline method and MSM4SR under unimodal and multimodal-based settings on each dataset. }
\footnotesize
    \centering
    \begin{tabular}{l|l|c c c c}
    \toprule
    Dataset & Model & R@10 & R@20 & N@10 & N@20\\
    \midrule
    \multirow{4}{*}{Pantry}
    &GRCN&0.0552&0.0856&0.0289&0.0366\\
    &MSM4SR-V&0.0596&0.0928&0.0290&0.0373\\
    &MSM4SR-T&0.0649&0.1001&0.0318&0.0407\\
    &MSM4SR&\textbf{0.0673}&\textbf{0.1040}&\textbf{0.0321}&\textbf{0.0414}\\
    \midrule
    \multirow{4}{*}{Arts}
    &S$^3$-Rec&0.0961&0.1250&0.0546&0.0619\\
    &MSM4SR-V&0.1018&0.1345&0.0581&0.0663\\
    &MSM4SR-T&0.1144&0.1523&\textbf{0.0652}&\textbf{0.0748}\\
    &MSM4SR&\textbf{0.1184}&\textbf{0.1570}&0.0637&0.0735\\
    \midrule
    \multirow{4}{*}{Office}
    &S$^3$-Rec&0.1027&0.1254&0.0641&0.0698\\
    &MSM4SR-V&0.1153&0.1415&0.0764&0.0830\\
    &MSM4SR-T&0.1186&0.1464&0.0772&0.0841\\
    &MSM4SR&\textbf{0.1206}&\textbf{0.1480}&\textbf{0.0797}&\textbf{0.0866}\\
    \bottomrule
    \end{tabular}
    \label{tab:compare}
\end{table}


\begin{figure*}[!t]
\centering
\subfloat[]{\includegraphics[width=0.245\textwidth]{lam_recall.pdf}}
% \hfil
\subfloat[]{\includegraphics[width=0.245\textwidth]{imgtoken_recall.pdf}}
\subfloat[]{\includegraphics[width=0.245\textwidth]{exps_recall.pdf}}
\subfloat[]{\includegraphics[width=0.245\textwidth]{drop_recall.pdf}}
\caption{The performance trends of MSM4SR with respect to different settings of $\lambda$, $N$, $O$, $\rho$ on Pantry and Office datasets.}
\label{fig:parameters}
\end{figure*}


\begin{figure*}[!t]
\centering
\subfloat[Pantry-Recall]{\includegraphics[width=0.245\textwidth]{user_pantry.pdf}}
% \hfil
\subfloat[Pantry-NDCG]{\includegraphics[width=0.245\textwidth]{user_arts_ndcg.pdf}}
\subfloat[Office-Recall]{\includegraphics[width=0.245\textwidth]{user_office.pdf}}
\subfloat[Office-NDCG]{\includegraphics[width=0.245\textwidth]{user_office_ndcg.pdf}}
\caption{The performance on different user groups achieved by GRCN, S$^3$-Rec, and MSM4SR on Pantry and Office datasets.}
\label{fig:perfrom-groups-1}
\end{figure*}

\subsection{Parameter Sensitivity Study}

In this experiment, we study the impact of three hyper-parameters, including the $\lambda$ to balance between modality-wise next item prediction loss and cross-modality contrastive loss, the number of tokens retrieved for each image $N$, the number of experts used in the MoE architecture $O$, and the random dropout probability of a sequence $\rho$. We conduct experiments on Pantry and Office, and report R@20 for comparison.

\subsubsection{Impact of $\lambda$}
The performance comparison using different values of $\lambda$ is shown in Figure~\ref{fig:parameters}(a). We vary $\lambda$ in $\{0.001, 0.01, 0.1, 1.0\}$. We can note that the best performance is achieved when $\lambda$ is set to $0.01$. Recommendation performance is compromised with either a large or a small $\lambda$.

\subsubsection{Impact of $N$}
The number of tokens retrieved for each image $N$ is varied in $\{5, 10, 15, 30, 50\}$. As shown in Figure~\ref{fig:parameters}(b), $15$ word tokens per image appears to be the optimal setting for both datasets. Insufficient information from images is captured when fewer tokens are used, while excessive token usage usually introduces noise. 

\subsubsection{Impact of $O$}
The number of experts used in the MoE architecture $O$ is chosen from $\{1, 2, 4, 8, 16\}$. From Figure~\ref{fig:parameters}(c), we can notice the results with respect to $O$ are consistent on both datasets. The best performance is achieved when $O$ is set to 8. However, the further increase of $O$ does not help with the performance.

\subsubsection{Impact of $\rho$}
As shown in Figure~\ref{fig:parameters}(d), we examine the model performance with different dropout probabilities of the user sequence $\rho$, which ranges from $0$ to $0.5$ with a step size of $0.1$. We can observe that the model performance is relatively stable with the change of the random dropout probability.


% % In the experiments, we study the impact of three hyper-parameters on the model performance. Specifically, we vary the parameter $\lambda$, which balances the contribution of the modality-wise next item prediction loss and cross-modality contrastive loss, in $\{0.001, 0.01, 0.1, 1.0\}$. The number of tokens retrieved for each image $N$ is varied in $\{5, 10, 15, 30, 50\}$. The number of experts used in the MoE architecture $O$ is chosen from $\{1, 2, 4, 8, 16\}$.

% Figure~\ref{fig:parameters} shows the performance trends of MSM4SR with respect to different settings of $\lambda$, $N$, and $O$. From Figure~\ref{fig:parameters} (a), we can note that the best performance is achieved when $\lambda$ is set to $0.01$. Recommendation performance will be compromised with either a large or a small $\lambda$. Moreover, as shown in Figure~\ref{fig:parameters} (b), $15$ word tokens per image appears to be the optimal setting for both datasets.
% % Only with Office dataset does NDCG perform comparably when using $15$ or $30$ tokens.
% Insufficient information from images is captured when fewer tokens are used, while excessive token usage usually introduces noise. 
% % Thus, the recommendation performance first improves with an increase in the number of image tokens, and then begins to drop when the number of image tokens increases thereafter. 
% In addition, from Figure~\ref{fig:parameters} (c), we can notice the results with respect to $O$ are consistent on both datasets. The best performance is achieved when $O$ is set to 8. However, further increases of $O$ does not help improve the performance.

% % However, it cannot be further improved when the number of experts is increased to have more trainable parameters


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Recommendation Performance on Different User Groups}

% Table 2 in the main content shows the overall performance comparison results.  
Based on the overall performance comparison results shown in Table~\ref{tab:overall}, 
we investigate the influence of data sparsity on users. Specifically, we split all users into five groups according to the length of their interaction sequences, and
% their interaction sequence length. 
models are evaluated on each group of users. Figure~\ref{fig:perfrom-groups-1} shows the performance comparison on two datasets. We have the following observations. \textit{Firstly}, the proposed MSM4SR model performs better than GRCN and $\textrm{S}^3$-Rec on all user groups for Office dataset. For Pantry dataset, both GRCN and MSM4SR outperform $\textrm{S}^3$-Rec when the user sequences are longer, indicating the usefulness of multimodal features. \textit{Secondly}, compared with GRCN and $\textrm{S}^3$-Rec, a decrease in the sequence length of user behaviors leads to greater improvements for MSM4SR. This demonstrates that MSM4SR can perform better in more sparse scenarios.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Exploration on Two Downstream Tasks}
\subsubsection{Cold-start Item Recommendation Performance}
\begin{table}
    \caption{The recommendation performance of cold-items achieved by CLCRec, MASR, MSM4SR\textsubscript{w/o Pre-train}, and MSM4SR. }
\footnotesize
    \centering
    \begin{tabular}{l|l|c c| c c}
    \toprule
    Dataset & Model & R@10& R@20 & N@10 & N@20\\
    \midrule
    \multirow{4}{*}{Pantry}
    % &SASRec & 0.1152 & 0.0489\\
    % &MASR&0.0047&0.0061&0.0077&0.0032&0.0036&0.0040\\
    &MASR&0.0111&0.0119&0.0064&0.0066 \\
    &CLCRec &0.0166&0.0251&0.0081&0.0103\\
    % &MASR & 0.0100&0.0124&0.0173&0.0072&0.0079&0.0092\\
    &MSM4SR\textsubscript{w/o Pre-train} &0.0257&0.0337&0.0119&0.0139\\
    &MSM4SR&\textbf{0.0360}&\textbf{0.0491}&\textbf{0.0176}&\textbf{0.0208} \\
    \midrule
    \multirow{4}{*}{Arts}
    % &SASRec & 0.2378 & 0.1379 \\
    &MASR&0.0136&0.0165&0.0080&0.0090 \\
    &CLCRec &0.0178&0.0239&0.0101&0.0116\\
    % &MASR &0.0202&0.0318&0.0519&0.0138&0.0175&0.0225\\
    &MSM4SR\textsubscript{w/o Pre-train} &0.0314&0.0455&0.0145&0.0181\\
    &MSM4SR&\textbf{0.0403}&\textbf{0.0552}&\textbf{0.0191}&\textbf{0.0229}\\
    \midrule
    \multirow{4}{*}{Office}
    &MASR&0.0079&0.0094&0.0050&0.0054 \\
    &CLCRec&0.0094&0.0120&0.0049&0.0056 \\
    % &MASR &0.0061&0.0103&0.0184&0.0043&0.0057&0.0077 \\
    &MSM4SR\textsubscript{w/o Pre-train} &0.0128&0.0183&0.0061&0.0074\\
    &MSM4SR &\textbf{0.0235}&\textbf{0.0312}&\textbf{0.0117}&\textbf{0.0136}\\
    \bottomrule
    \end{tabular}
    \label{tab:cold}
\end{table}
To validate the effectiveness of our model for cold-start recommendation, we consider the following methods for evaluation in addition to \textbf{MSM4SR}\textsubscript{w/o Pre-train} and \textbf{MSM4SR}: 
% 1) \textbf{SASRec}: This baseline method is based on Transformer structure and does no consider multimodal item information for recommendation; 
1) \textbf{CLCRec}~\cite{wei2021contrastive}: this method explores the mutual dependency between item multimodal features and collaborative representations to alleviate the cold-start item problem.
2) \textbf{MASR}~\cite{hu2022memory}: authors construct two memory banks to store historical user sequences and a retriever-copy network to search similar sequences to enhance the recommendation performance for cold-start items.
% 3) \textbf{MSM4SR}\textsubscript{w/o Pre-train}: we train the proposed model from scratch based on the fine-tuning setting, without considering pre-training tasks;
% follows the multimodal-based fine-tuning framework defined in Eqn.\ref{eq:s2};
% 4) \textbf{MSM4SR}: this is the proposed model based on the pre-training and fine-tuning setting.

% In the experiments, users with only three interaction items are defined as cold-users, and remaining users with more than three interaction items are defined as warm-users. 
% % denoted as cold-users and the remaining users are denoted as warm-users.
% SASRec and MSM4SR\textsubscript{w/o Pretrain} use both cold-users and warm-users for model training, and only use cold-users for model evaluation. MSM4SR is firstly pre-trained on warm-users and then fine-tuned only on cold-users. 
% % (3) pre-trains on warm users and then performs downstream fine-tuning only on cold users.
% Table~\ref{tab:cold} shows the performance achieved by SASRec, MSM4SR\textsubscript{w/o Pretrain}, and MSM4SR, on cold-users. We can note that MSM4SR\textsubscript{w/o Pretrain} and MSM4SR outperform SASRec, which demonstrate the effectiveness of leveraging multimodal information to alleviate the cold-start problem. MSM4SR performs the best in terms of most evaluation metrics. This results show that cold-users can benefit from self-supervised multimodal pre-training tasks leveraging users with more interactions.

In the experiments, counting all items in the training set, we consider those that appear less than 10 times as cold-items, and remaining items are defined as warm-items. CLCRec, MASR, and MSM4SR\textsubscript{w/o Pre-train} are trained based on the complete data including both cold-items and warm-cold items, and are evaluated based on user sequences that take cold-items as the target item for prediction. MSM4SR is firstly pre-trained on warm-items and then fine-tuned with the complete data. The model performance is evaluated in the same way as other three baselines. Since cold-items do not have sufficient interaction data, item ID embeddings are discarded at the fine-tuning stage. 

Table~\ref{tab:cold} shows the performance achieved by CLCRec, MASR, MSM4SR\textsubscript{w/o Pre-train}, and MSM4SR, on cold-items.
We can note that both MSM4SR\textsubscript{w/o Pre-train} and MSM4SR outperform two baseline methods, which demonstrate the effectiveness of leveraging multimodal information to alleviate the cold-start item problem. Overall, MSM4SR performs the best by a wide margin across all evaluation metrics. This result shows that cold-items can benefit from self-supervised multimodal pre-training tasks leveraging items with more interactions.



\subsubsection{Cross-domain Recommendation Performance}
\begin{table}
    \caption{The recommendation performance achieved by RecGURU, UniSRec, MSM4SR\textsubscript{w/o Pre-train}, and MSM4SR under cross-domain setting on Pantry and Arts datasets.}
\footnotesize
    \centering
    \begin{tabular}{l|l|  c c |c c}
    \toprule
    Dataset & Model & R@10& R@20 & N@10 & N@20\\
    \midrule
    \multirow{4}{*}{Pantry}
    % & SASRec &0.0277&0.0457&0.0722&0.0147&0.0204&0.0271\\
    & RecGURU &0.0308&0.0537&0.0152&0.0210\\
    & UniSRec &0.0582&0.0932&0.0265&0.0353\\
    & MSM4SR\textsubscript{w/o Pre-train} &0.0595&0.0920&0.0286&0.0369\\
    & MSM4SR\textsubscript{Cross} &\textbf{0.0622}&\textbf{0.0944}&\textbf{0.0294}&\textbf{0.0375}\\
    \midrule
    \multirow{4}{*}{Arts}
    % &SASRec &0.0704&0.0910&0.1125&0.0442&0.0509&0.0563\\
    &RecGURU &0.0890&0.1174&0.0569&0.0641 \\
    &UniSRec &0.0995&0.1300&0.0565&0.0642 \\
    &MSM4SR\textsubscript{w/o Pre-train} &0.1030&\textbf{0.1374}&0.0558&0.0644\\
    &MSM4SR\textsubscript{Cross} &\textbf{0.1041}&0.1367&\textbf{0.0573}&\textbf{0.0657}\\
    \bottomrule
    \end{tabular}
    \label{tab:cross}
\end{table}

We study the knowledge transfer capability of the pre-trained model under the MSM4SR framework. Specifically, we evaluate the cross-domain recommendation performance of MSM4SR, using Office dataset as the source domain, Pantry and Arts datasets as target domains. In the experiments, RecGURU, UniSRec and two variants of MSM4SR are used for comparison:
% 1) SASRec: This is the sequential recommendation model trained on each single domain.
1) \textbf{RecGURU}~\cite{li2022recguru}: this baseline model is a cross-domain sequential recommendation framework that exploits adversarial learning to construct a generalized user representation unified across different domains.
2) \textbf{UniSRec}~\cite{hou2022towards}: authors utilize item texts to learn more transferable and universal representations from multiple domains for sequential recommendation.
3) \textbf{MSM4SR}\textsubscript{w/o Pre-train}: we use the target domain data to train the proposed model from scratch based on the multimodal fine-tuning strategy. 
4) \textbf{MSM4SR}\textsubscript{Cross}: we use the source domain data to pre-train the MSM4SR framework and fine-tune it based on the target domain data. For UniSRec and MSM4SR variants, we perform parameter-efficient fine-tuning for target domains by fixing the parameters of the Transformer architecture and only fine-tuning the modality encoders.

The results of cross-domain recommendation performance are shown in Table~\ref{tab:cross}. Compared with RecGURU, UniSRec, and MSM4SR\textsubscript{w/o Pre-train}, MSM4SR\textsubscript{Cross} achieves the best performance in terms of most evaluation metrics. This indicates the proposed pre-training framework is effective in transfering knowledge from source domain to target domain. Additionally, the proposed contrastive learning tasks enable MSM4SR to learn generalized multimodal representations for user behavior sequences to benefit sequential recommendation. It is worth noting that RecGURU performs comparably with MSM4SR on the Arts dataset in terms of NDCG but performs the worst on the Pantry dataset. This can be attributed to the fact that Office and Arts have more common users~(\ie 4,068) than Office and Pantry~(\ie 1,525). As a result, RecGURU, which is designed to capture generalized user representations using adversarial learning, is more successful in knowledge transfer from Office to Arts, but fails from Office to Pantry.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
This paper proposes a novel pre-training framework, called MSM4SR (\ie Multimodal Sequence Mixup for Sequential Recommendation), for boosting the sequential recommendation performance. In MSM4SR, item images are firstly represented by textual tokens to eliminate the discrepancy between text and image modalities. Then, MSM4SR employs a novel backbone network M$^2$SE (\ie Multimodal Mixup Sequence Encoder) to integrate items' multimodal content with the user behavior sequence, with a complementary sequence mixup strategy. Two contrastive learning losses are designed to help M$^2$SE learn generalized multimodal sequence representations. The experiments on real datasets demonstrate the proposed pre-training framework can help improve sequential recommendation performance under different settings. 
% For future work, we would like to develop more effective fine-tuning strategies to improve the performance of pre-trained backbone network M$^2$SE in different downstream tasks, such as sequential recommendation, cross-domain recommendation, and user classification.


% use section* for acknowledgment
\section*{Acknowledgment}
This research is supported by Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI), Nanyang Technological University, Singapore.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% {\appendix[Proof of the Zonklar Equations]
% Use $\backslash${\tt{appendix}} if you have a single appendix:
% Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
% If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
% You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
%  starts a section numbered zero.)}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}


\bibliographystyle{IEEEtran}
\bibliography{ref}



\newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{Lingzi Zhang}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




% \vfill

\end{document}


