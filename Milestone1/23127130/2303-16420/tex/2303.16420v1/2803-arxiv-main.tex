\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{bbm}
\usepackage{bm}
\usepackage{multirow}
% \usepackage{xcolor}
%\usepackage{algorithm}
% \usepackage[noend]{algpseudocode}
%%%%%%%%%%%%%%%%%%%%%% new added
\usepackage{float}
\usepackage{makecell} 
\usepackage{boldline}
\usepackage{diagbox}
\usepackage{threeparttable}
\usepackage[dvipsnames]{xcolor}
\usepackage{adjustbox}
\usepackage{booktabs}
% \usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
%new added
\allowdisplaybreaks[4]
%%%%%%%%%%%%%%%%%%%%%%
\usepackage{caption}
\usepackage{subfigure}
\usepackage{mathabx} %%%%% bigtimes 
\usepackage{authblk}
\usepackage{float}
\usepackage[title]{appendix}
% \usepackage{makecell} 
\usepackage{boldline}
\usepackage{diagbox}
\usepackage{threeparttable}
\usepackage[dvipsnames]{xcolor}
\usepackage{adjustbox}
\usepackage{booktabs}

%new added
\RequirePackage{xcolor}[2.11]
\colorlet{inlinkcolor}{green!60!black}
\colorlet{exlinkcolor}{red!50!black}
\colorlet{reviewcolor}{black!50}


\usepackage[colorlinks=true]{hyperref}
\hypersetup{urlcolor=exlinkcolor, citecolor=inlinkcolor,linkcolor=inlinkcolor}
\usepackage[capitalise,nameinlink]{cleveref}
\crefname{equation}{}{}
\crefname{figure}{Figure}{Figures}
\crefname{assumption}{Assumption}{Assumptions}
\crefname{subappendix}{Appendix}{Appendices}
% \crefname{table}{Tab.}{Tabs.}
\newcommand{\crefrangeconjunction}{~-~}



\makeatletter
\newenvironment{breakablealgorithm}
{
% \begin{breakablealgorithm}
% \begin{center}
% \begin{minipage}{0.9\linewidth}
\refstepcounter{algorithm}% New algorithm
\hrule height1pt depth0pt \vspace{-0.2cm} \kern2pt% \@fs@pre for \@fs@ruled
\renewcommand{\caption}[2][\relax]
{% Make a new \caption
{\raggedright\textbf{\ALG@name~\thealgorithm} ##1\par}%
\ifx\relax##1\relax % #1 is \relax
\addcontentsline{loa}
{algorithm}
{\protect\numberline{\thealgorithm}##2}%
\else % #1 is not \relax
\addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
\fi
\kern2pt\hrule\kern0pt
% \vspace{-0.2em}
}
\small
% \vspace{-0.3em}
}
{% \end{breakablealgorithm}

\kern2pt\hrule \relax% \@fs@post for \@fs@ruled
% \end{center}
% \end{minipage}
}
\makeatother




\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]
% \newtheorem{algorithm}{Algorithm}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{corollary}{Corollary}[section]

\newcommand{\dd}{\mathsf {d\kern -0.07em l}} %	nested

\newcommand{\Q}[1]{{\color{Plum} #1}}
\newcommand{\ws}[1]{{\color{WildStrawberry} #1}}
\newcommand{\SN}[1]{{\color{PineGreen} #1}}
\newcommand{\HX}[1]{{\color{blue} #1}}

\newcommand{\bgeqn}{\begin{eqnarray}}
\newcommand{\edeqn}{\end{eqnarray}}
\newcommand{\bgeq}{\begin{eqnarray*}}
\newcommand{\edeq}{\end{eqnarray*}}
\newcommand{\bec}{\begin{center}}
\newcommand{\enc}{\end{center}}
\newcommand{\R}{{\rm I\!R}}
\newcommand{\ule}{\overline{\lim}_{\epsilon\rightarrow 0^+}}
\newcommand{\inmat}[1]{\mbox{\rm {#1}}}
\newcommand{\outlim}{\mathop{\overline{\lim}}}
\newcommand{\insmat}[1]{\mathop{\rm {#1}}}
\newcommand{\twoargs}[3]{
\insmat{#1}_{{\scriptstyle \rule{0cm}{.3cm}#2} \atop{\scriptstyle
\rule{0cm}{.3cm}#3}}}

\def\prob {{\rm Prob}}
\renewcommand{\Box}{\framebox{\rule{0.3em}{0.0em}}}

\newcommand{\fb}{\rule[-2pt]{4pt}{8pt}}
\newcommand{\RA}{\Rightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\half}{ \mbox{\small$\frac{1}{2}$}}
\newcommand{\quarter}{ \mbox{\small$\frac{1}{4}$}}
\newcommand{\rfp}[1]{(\ref{#1})}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\rar}{\rightarrow}
\newcommand{\Rar}{\Rightarrow}
\newcommand{\dar}{\downarrow}
\newcommand{\imp}{\Longrightarrow}
\newcommand{\rr}{\rightrightarrows}
\newcommand{\vt}{{\vartheta}}
\newcommand{\B}{{\cal B}}

\newcommand{\bdx}{\bm{x}}
\newcommand{\bdy}{\bm{y}}
\newcommand{\bdxi}{\bm{\xi}}
\newcommand{\bdzeta}{\bm{\zeta}}
\newcommand{\bdv}{\bm{v}}
\newcommand{\bdV}{\bm{V}}
\newcommand{\bdz}{\bm{z}}
\newcommand{\bdt}{\bm{t}}
\newcommand{\bda}{\bm{a}}
\newcommand{\bdb}{\bm{b}}
\newcommand{\bdf}{\bm{f}}
% \newcommand{\bdps}{\bm{G}}
\newcommand{\bdu}{\bm{U}}



\newcommand{\bb}{\mbox{\tiny\boldmath$b$}}
\newcommand{\bcc}{\mbox{\boldmath$c$}}
\newcommand{\bxi}{\mbox{\boldmath$\xi$}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section}
\numberwithin{definition}{section}
\newcommand{\hatu}{\hat{u}}
\newcommand{\hatv}{\hat{v}}
\newcommand{\barv}{\bar{v}}
\newcommand{\calu}{\mathcal{U}}
\newcommand{\tldu}{\tilde{u}}
\newcommand{\tldc}{\tilde{\bm{C}}}
\newcommand{\tldpsi}{\tilde{\bm{\psi}}}
\newcommand{\calx}{\mathcal{X}}
\newcommand{\caly}{\mathcal{Y}}
\newcommand{\bbb}{\mathbb{B}}
\newcommand{\scru}{\mathscr{U}}
\newcommand{\scrg}{\mathscr{G}}
\newcommand{\lt}{\left}
\newcommand{\rt}{\right}
\newcommand{\disp}{\displaystyle}
\newcommand{\st}{{\rm s.t.}}
\newcommand{\barf}{\bar{f}}
\newcommand{\1}{\mathds{1}}
\newcommand{\pl}{{\rm pl}}
\newcommand{\bdps}{\bm{\psi}}
\newcommand{\bdc}{\bm{C}}
\newcommand{\bdca}{\bm{A}}
\newcommand{\bdcb}{\bm{B}}
\newcommand{\bde}{\bm{e}}
\newcommand{\bdbt}{\bm{\beta}}
\newcommand{\bbp}{\mathbb{P}}
\newcommand{\bdg}{\bm{g}}
\newcommand{\bdcz}{\bm{Z}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\dist{\mathop{\rm dist}}

\def\vv{\vartheta}
\def\eps{\varepsilon}
\def\la{{\langle}}
\def\ra{{\rangle}}
\def\val{{\rm val}}
\def\lsc{{\rm lsc}}
\def\dom{{\rm dom}}
\def\intt{{\rm int}}
\def\conv{{\rm conv}}
\def\cl{{\rm cl}}
\def\gph {{\rm gph}}

\def\hF{\widetilde{F}}
\def\hQ{\widehat{Q}}
\def\hS{\widehat{S}}
\def\bbe{{\mathbb{E}}}

\setlength{\textwidth}{16cm} \setlength{\textheight}{23cm}
\setlength{\oddsidemargin}{0.1cm} \setlength{\evensidemargin}{0.1cm}
\setlength{\topmargin}{-1.5cm} \setlength{\parskip}{0.25cm}
\renewcommand{\textfraction}{0.1}
\renewcommand{\bottomfraction}{0.3}
\renewcommand{\topfraction}{0.7}
\renewcommand{\baselinestretch}{1.08}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\def\Prob{{\rm{Prob}}}


\title{Multi-Attribute Utility Preference Robust Optimization: 
A Continuous Piecewise Linear Approximation Approach\thanks{This project is supported by RGC grant 14500620.
}}
\author[1]{Qiong Wu}
\author[1]{Sainan Zhang}
\author[2]{Wei Wang}
\author[1]{Huifu Xu\thanks{Corresponding Author.}}
% \author[1]{Huifu Xu\thanks{Corresponding Author Email: \textsf{hfxu@se.cuhk.edu.hk}.}}
\affil[1]{Department of Systems Engineering and Engineering Management,The Chinese University of Hong Kong, Shatin, N.T., Hong Kong.}
\affil[2]{School of Business, University of Southampton, Southampton SO17 1BJ, UK.}
\affil[ ]{\textsf{\{qiwu,snzhang,hfxu\}@se.cuhk.edu.hk,ww1e17@soton.ac.uk}.}
\vspace{-1cm}
\date{\today}
% \vspace{0cm}
\begin{document}
\maketitle

% \vspace{-0.5cm}
\begin{abstract}
In this paper, we consider a multi-attribute decision making problem 
where the decision maker's (DM's) objective is to maximize the expected utility of
outcomes but the true utility function which captures the DM's risk preference is ambiguous. We propose a maximin multi-attribute
utility preference robust optimization (UPRO) model where the optimal decision is based on the worst-case utility function in an ambiguity set of plausible utility functions constructed 
using partially available information 
such as the DM's specific preferences between some lotteries. Specifically, we consider a
UPRO model with two attributes, where
the DM's risk attitude is multivariate 
risk-averse and the ambiguity set is defined by a linear system of inequalities 
represented by the Lebesgue–Stieltjes (LS) integrals of the DM's  utility functions.
To solve the maximin problem, 
we propose an explicit piecewise linear approximation (EPLA) scheme 
to approximate the DM's true unknown utility 
so that the inner minimization problem reduces to a linear program, 
and we solve the approximate maximin problem 
by a derivative-free (Dfree) method. 
Moreover, by introducing binary variables to locate the position of the reward function 
in a family of simplices,
we propose an implicit piecewise linear approximation (IPLA) representation 
of the approximate  UPRO and solve it using the Dfree method. Such IPLA technique prompts us to reformulate the approximate UPRO as a single mixed-integer program (MIP) and extend the tractability of the approximate UPRO to the multi-attribute case. Under some moderate conditions, we derive error bounds between the UPRO  and the approximate UPRO  in terms of the  ambiguity set, the optimal value and the optimal solution.
Furthermore, we extend the model to the expected utility maximization problem with expected utility constraints where the worst-case utility functions 
in the objective and constraints are considered simultaneously. Finally, we report the numerical results about 
performances of the proposed models and the computational schemes, and show that the schemes work efficiently 
and the UPRO model is stable against 
data perturbation.
% \keywords{Multi-attribute UPRO \and Non-additive utility \and Lebesgue–Stieltjes integral \and Preference elicitation \and Piecewise linear approximation \and Tractability \and MIP \and Error bounds \and Data perturbation}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{90C17 \and 90C29 
% %\and 90C17
% \and 90C31 
% %\and 90C11 
% \and 91B16 %\and 32B25
% }
\end{abstract}

\textbf{Keywords}: Multi-attribute UPRO, Non-additive utility, Lebesgue–Stieltjes integral, Preference elicitation, Piecewise linear approximation, Tractability, MIP, Error bounds, Data perturbation



\section{Introduction}

{\color{black}
Utility preference robust optimization (UPRO) model concerns the optimal decision making 
where the decision maker (DM) aims to maximize the expected utility but 
the true utility function which captures the DM's preference is ambiguous. 
Instead of finding an approximate utility function using partially available information as in the literature of behavioural economics (see, e.g., \cite{clemen2013making} and \cite[Chapter 10]{gonzalez2018utility}), 
% The method for choosing a utility function
% proposed in most textbooks on decision analysis
% (see, for instance, Clemen and Reilly 2001) is to make
% a set of pairwise comparisons between lotteries (often
% using the Becker-DeGroot-Marschak reference lotteries;
% Becker et al. 1964) in order to identify the value
% of the utility function at a discrete set of points. The
% utility function is then completed by naïve interpolation.
the UPRO models construct a set of plausible utility functions and
base the optimal decision on the worst-case utility function from the set to mitigate such ambiguity.
This %kind
type
of approach can be traced back to Maccheroni~\cite{maccheroni2002maxmin}
who considers the worst-case utility evaluation among 
%many
a number of available 
utilities when a conservative DM faces uncertain outcomes of lotteries.
He derives necessary and sufficient conditions %{\color{red}to guarantee 
for the existence of 
% The interpretation is simple: a conservative decision maker has an unclear evaluation of the different outcomes when facing lotteries. She then acts as if she were considering many expected utility evaluations and taking the worst one
% for 
a set of utility functions such that the {\color{black}worst-case} 
in %from 
the set can be used to characterize the conservative decision making   
%this kind of decision-making 
framework.
Armbruster and Delage \cite{AmD15}
give a comprehensive treatment 
of the problem from 
%an 
optimization perspective by formally 
proposing a maximin  UPRO paradigm.
Specifically, they consider  a class of  utility functions which 
are  concave or S-shaped and discuss how a DM's preference 
may be elicited through pairwise comparisons.  
Moreover, 
they demonstrate that solving the UPRO 
% preference robust optimization (PRO) 
model
%is 
%down to 
reduces to
solving a linear program (LP) under some mild conditions.
Over the past few years, the research of 
% preference robust optimization (PRO) 
UPRO related models 
has received increasing attentions, see for instance \cite{hu2015robust,haskell2016ambiguity,hu2018robust,GXZ21,DGX22,WuX22}.

The above UPRO models are all about single-attribute decision making problem. 
In practice, there is a multitude of {\color{black}literature} focusing on the multi-attribute case.
%decision making. %For instance, 
For instance,
in healthcare it is typical to use several
metrics rather than just one to measure the quality of life (\cite{feeny2002multiattribute,torrance1982application}).
Similar problems can be found in network management \cite{azaron2008multi,chen2010stochastic},
scheduling \cite{Zakariazadeh2014}, 
multiobjective design optimization problem 
\cite{Dino2017,tseng1990minimax}, and portfolio
optimization \cite{fliege2014robust}. Indeed, over the past few decades,
there has been significant research on multi-attribute expected utility
\cite{fishburn1992multiattribute,miyamoto1996multiattribute,tsetlin2006equivalent,tsetlin2007decision,tsetlin2009multiattribute,von1988decomposition}.
%and multi-attribute risk management \cite{burgert2006consistent,Galichon2012,hamel2010duality,jouini2004vector,hu2022distributionally, liu2021multistage}.


Zhang et al.~\cite{zhang2020preference} seem to be the first to 
%consider
propose 
a preference robust optimization (PRO) model for multi-attribute decision making. Specifically, they 
consider 
a multivariate shortfall risk minimization problem where
there is an ambiguity in an investor's true disutility function of losses and they consider
the worst-case disutility function 
%of loss from 
in an ambiguity set
%for calculating 
to calculate the risk measure.
%is considered. 
Wu et al.~\cite{wu2020preference}
 %\cite{WHHX22}
propose a general PRO model 
%preference robust optimization model 
for 
 multi-attribute decision making. Instead of 
considering expected utility,
%maximization problem, 
they consider a 
%general 
quasi-concave choice function 
%model 
to measure the DM's multi-attribute rewards
which subsumes the expected utility model as a special case,
and propose
a support function-based approach 
to solve
%for solving
the resulting 
preference robust choice problem. Since 
the model is very general,
the computational scheme does not benefit 
from the specific structure that it would do in expected utility maximization problems.
% For example, how can we design pairwise questionnaires to elicit a decision maker's utility preference? 
% %In this paper, we propose a comprehensive study 
% %Specifically,
% In single-attribute decision making problems, 
% %Guo et al.~
% \cite{GuX21}
% propose a piecewise linear approximation method for solving PRO models and deriving error bounds 
For example, it is unclear whether we can use piecewise linear utility functions to approximate the true unknown utility function
in the multi-attribute UPRO models 
as in the single-attribute case (\cite{GXZ21}).
We are interested in the piecewise linear approximation (PLA) approach for several reasons. First, a DM's utility preference is usually elicited at some discrete points. Connecting the 
utility function values at these points to form a piecewise linear utility function is the easiest way to obtain an approximate utility function.
Second, the  PLA approach works for a broader class of
UPRO models without specific requirements on convexity, S-shapedness or quasiconvexity of the true utility function.
Third, despite the PLA approach does not solve UPRO models precisely as the support function-based approach, it allows %one
us to derive an  error bound under some moderate conditions.
 
In this paper, we endeavour to carry out a comprehensive 
study on the multi-attribute UPRO from modelling to computational schemes and underlying theory. Unlike 
single-attribute case, a conservative DM's utility function is not necessarily concave which means that
Armbruster-Delage's support function-based approach 
is not applicable in this case.  
This prompts us to 
adopt the PLA approach. The extension of the 
PLA approach from single-attribute UPRO to %bi-attribute
multi-attribute
UPRO would be trivial if the utility function is additive or concave. %Here
% {\color{blue}
However, 
when we consider 
%come to 
a general multi-attribute utility function without
%the
specific independence condition,
%structure,
the construction,
representation of PLA and subsequent computation of the approximate 
UPRO require 
%significant 
a lot of new work.
% }
%inputs. 
%conditions.
One of the 
%A particular 
challenges that we have to tackle is to find an appropriate
%that
%the 
representation
%definition 
of a piecewise linear utility function which is 
easy to construct, and 
%be 
to embed
%ded 
in the objective function and in the ambiguity set.
%approximates the true utility function 
%risk averse 
%is non-trivial. First, 
The main contributions of this paper can be summarized as follows. 
% However,  
% % 
% the extension of PLA from single-attribute UPRO to multi-attribute UPRO
% %.% The extension 
% requires us to tackle a number of technical issues some of which are challenging.
% One is the construction of the ambiguity set.
% In single attribute case, \cite{GuX21} consider some moment-type conditions for constructing an ambiguity set
% of utility functions.
% %where resembles a probability measure.
% The specific structure of the ambiguity set 
% facilitates them to derive
% the error bounds arising from the PLA approach. 
% % While a number of utility elicitation approaches 
% % such as pairwise comparisons and certainty equivalents
% % can be reformulated as moment-type conditions in single 
% % attribute decision making problems, 
% % %they cannot be done 
% % it is not so 
% % in multi-attribute case. 
% % Constructing linear pieces in PLA is another challenge.

% {\color{red}TO BE WRITTEN UP}

% }






% Decision-making problems where the consequences are expressed in terms of more than one attribute have become increasingly important. 
% As a classical and important decision criterion, the expected utility theory established by \cite{von1947theory} 
% has been a dominant normative and descriptive model 
% % provides a way
% to solve such problems. 
% For example, in the investment literature, see, e.g., \cite{sabatino2015sustainability,abbas2018foundations}, an individual makes decisions to maximize his expected utility for a lifetime consumption stream. 
% In practice, however, identifying such a multiattribute utility function to present the DM's risk preference may not be easy. 
% In the literature on decision analysis and behavioural economics, 
% % the multiattribute utility function is 
% a standard method to elicit such utility function is to trade-off a tolerable amount of pairwise gambles and then to use the elicited information to construct an approximated utility function via interpolation method, see, e.g., \cite[Chapter 10]{gonzalez2018utility} and references therein. 
% % 
% % 
% In the literature of operational research and management science, 
% \cite{AmD15} and \cite{hu2015robust} take a different approach to handling the issue for the single-attribute case. 
% Instead of trying to find a piecewise linear approximation of the true utility function to guide the decision-making procedure, 
% they use the partially available information to construct an ambiguity set of all plausible utility functions that are consistent with decision makers' behavior 
% and then base the optimal decision on the worst-case utility function from the ambiguity set. 
% This approach is known as \emph{preference robust optimization} (PRO) as it follows the general philosophy of robust optimization. 

% As an interesting direction in the interdisciplinary area of robust optimization and behavior economics, 
% PRO model has received sustained attention over the past few years. 
% For instance, \cite{HFD16} consider the decision making problem when the joint ambiguity set of the risk-averse DM's utility function and the underlying probability of exogenous uncertainty 
% and derive the tractable reformulation when the joint ambiguity set is in a certain special case. 
% Based on the relation between stochastic dominance and expected utility 
% % equivalent representation of stochastic dominance 
% (see, e.g, \cite{levy1992stochastic}), 
% \cite{HuS17} propose a so-called \emph{reference-based almost stochastic dominance} (RSD) by restricting the utility class (describe the basic properties of risk aversion) to 
% % the ambiguity set of utility functions 
% a smaller ambiguity set in a neighborhood of certain reference utility function 
% and then develop an approximation model using Bernstein polynomials to solve the RSD-based decision-making problem. 
% Recently, 
% \cite{GuX21} extend the work of \cite{hu2015robust} by considering the \emph{piecewise linear approximation} (PLA) of the moment-type ambiguity set to derive the tractable reformulation of the maximin optimization problem and deriving the error bound for the PLA framework with respect to the ambiguity set, optimal solutions and optimal value. 
% \cite{liu2021multistage} extend the PRO model to the multistage case. 
% \cite{hu2022distributionally} propose the random multiattribute utility-based PRO model when the multiattribute utility function is additive. 
% The idea of PRO approach has also been widely used in risk management where the choice of risk measures is uncertain. 
% For example, \cite{DeL17} consider the law invariant convex risk measure.  
% \cite{DGX22} focus on the shortfall risk and their work has been extended to the multivariate case by \cite{zhang2020preference}. 
% \cite{WaX20,guo2022robust} analyze the spectral risk measure. 
% We also refer to \cite{wu2020preference,WaX21a,haskell2022preference} and references therein. 

% \cite{zhang2020preference} multivariate shortfall risk measure 


% In this paper, we follow the stream of PRO model and consider the bi-attribute utility preference robust optimization where the ambiguity set is constructed by the moment-type condition and the utility functions are \emph{conservative}, 
% which represents a class of \emph{multivariate risk averse} (MRA) decision makers, see, e.g., \cite{richard1975multivariate}. 
% Interestingly, such conservative utility function can induce a Lebesgue-Stieltjes measure on $\R^2$, which would facilitate us to calculate the expected utility of a continuous random profit when the utility function has piece-wise linear format. 
% Inspired by this result, we consider the piece-wise linear approximation of the ambiguity set of conservative bi-attribute utility functions and then derive the tractable reformulation of the resulting PRO model. 
% We also take a step further to derive the error bound of the approximation with respect to the ambiguity set, optimal solutions, and optimal value. 
% We also consider some other issues such as impact of data perturbation/contamination on the optimal value, 
% dependence of ambiguity set on the decision variables. 


% We consider the case that the utility function $u$ is defined on a consequence space $T=X\times Y$ 
% where $X$ and $Y$ are nondegenerate closed and bounded real intervals. 

% {\color{red}
% multivariate risk averse DM

% To finish introduction by 15 September, 2022. 
% }

% 
% 
% \begin{itemize}
%     \item How to define certainty equivalent in MA case?
%     \item Risk averse vs utility concavity in MA case
%     \item Decision dependent Ambiguity vs decision independent ambiguity
%     \item ???
% \end{itemize}
% {\color{red}
% There have already been four paper on PRO model in multiattribute decision making: Hu, Zhang, Xu and Zhang (2022) on random utility, Hu, Haskell, Huang and Xu(2022) on choice function model in act space, Haskell, Xu and Huang (2021) on choice function model in lottery space,
% Zhang, Xu and Wang (2020) on multivariate shortfall risk measure. So this paper must not repeat what these papers have done.
% 
% PLA and conjoint analysis are now new.
% 
% Two potential directions:
% (a) copula model, (b) decision dependent ambiguity set.
% 
% How far can we go in (a)? Can you work together to find out what the real challenges are? 
% }


% We propose to approximate the ambiguity set so that the resulting PRO models can be more easily
% solved when the utility functions are non-concave. We also consider other issues such as data
% perturbation/contamination in the construction of the ambiguity, dependence of ambiguity set
% on the decision variables and unboundedness of the domain of utility functions. 



First, we 
%consider a 
propose a maximin robust optimization model for
bi-attribute decision making 
%utility-based PRO model 
where the DM is
%{\color{green}
multivariate
risk-averse,
%the multivarite 
% {\color{red}where} 
there is incomplete information to identify 
the DM's true utility function, and the optimal decision is based on the worst-case utility function in an ambiguity set. We discuss in detail how the ambiguity set 
of bivariate utility functions may be constructed by 
standard preference elicitation 
%approaches
methods such as pairwise comparisons.
%and certainty equivalents. 
%does not have a clear-cut utility function to capture his/her risk preference and the ambiguity set is constructed using the so-called moment-type condition. 
To solve 
%the resulting 
the maximin problem, we propose a two-dimensional continuous 
%piecewise linear approximation 
PLA scheme
to approximate 
the true unknown utility function
%in the ambiguity set 
so that the inner minimization problem 
can be reduced to a finite-dimensional program.
%a linear programming problem.
Differing from the one-dimensional case, 
we divide the domain of the utility functions
into a set of mutually exclusive triangles
and define an approximate 
%piecewise linear 
utility function which is linear %in
over
each of the triangles. %to approxiamate the true. 
% the PLA requires us to 
% construct two linear functions over each cell (small rectangle) separated by the diagonals of the cell.
% %and preserve 
% {\color{purple}
Moreover, 
%it requires us to 
we reformulate the ambiguity
set defined by 
%{\color{purple}
the expected utility values of 
the DM's preferences 
%over
between
%certain
lotteries
% }
% {\color{red}comparing the expected utility values between certain lotteries}
% {\color{green}Please double check this sentence.}
into
the one where the expected utility values are 
represented by the Lebesgue–Stieltjes (LS) integrals with respect to (w.r.t.) the utility function. 
% }
% {\color{blue}
% Moreover, 
% we construct the ambiguity
% set by inequalities where the underlying functions are defined by the Lebesgue–Stieltjes integrals with respect to (w.r.t.) the utility functions.}
The PLA approach allows us to derive the approximate utility function explicitly using indicator functions 
{\color{black}and} characterize the Lipschitz continuity of the utility function by individual variables,   
and 
%The LS integrals 
%are easy 
{\color{black}enables} us to calculate the  LS integrals conveniently.
%when the utility function is piecewise linear.

% a linear system of inequalities where the underlying functions  

% since risk-averse utility functions
% can induce a Lebesgue–Stieltjes measure, we can reformulate the ambiguity set 
% %as moment-type  
% %the DM's expected utility in 
% where the expected utility functions are represented as 
% Lebesgue–Stieltjes with respect to the utility function.


%Second, we derive the equivalent 
% representation of %moment-type 
% the ambiguity set constructed by pairwise comparisons
% %of bi-attribute utilities 
% and find that the multivariate risk averse utility function can induce a Lebesgue–Stieltjes measure, which would help us to calculate the expected utility of any random profit when the utility function is piecewise linear. 

Second, by exploiting the piecewise linearity of the approximate utility function,  
we use the well-known polyhedral method
%to model the PLA,
%see e.g. 
(\cite{DLM10,KDN04,LeW01,VAN10,vielma2015mixed}) 
% {\color{purple}
to represent the 
%{\color{purple}vector} of
%the 
multi-attribute reward functions
%on
%in a family of 
using  a convex combination of 
the vertices of the 
simplex
%polytope 
%triangle 
containing the vector 
in the domain of 
%lying 
%in the epigraph 
%space 
the  multivariate utility function,
% }
% {\color{blue}
% to obtain the coefficients of utility values by solving a system of equalities and inequalities,
% }
and subsequently
reformulate the inner 
%piecewise linear 
approximate 
%maxmin 
utility minimization problem as a mixed-integer program (MIP). Differing from the PLA approach described above, the approximate utility function cannot be represented explicitly, rather it is determined by solving an MIP. We call this implicit PLA (IPLA) whereas the former is explicit PLA (EPLA).  
A clear benefit of IPLA is that 
%approach effectively 
it works for multidimensional cases and 
also allows us to reformulate the whole maximin problem as a single MIP. 
% {\color{red}This extends the uni-variate increasing utility case to two and three-dimensional cases
% to reformulate the UPRO as a single mixed integer problem.
% }

Third, we extend the preference robust approach to the expected utility maximization problem with expected utility constraints. Instead of considering the worst-case utility in the objective and 
the worst-case utility in the constraints separately, 
we propose a UPRO model where the optimal decision is based on the same worst-case utility function in both the objective and the constraints. We derive conditions under which the two robust formulations are equivalent and
%and 
carry out comparative analysis through numerical studies to identify 
the differences that the two models may render.


Fourth, to justify the PLA scheme, we derive {\color{black}error bounds} for the optimal value and the optimal solutions, which is built on a newly derived Hoffman's lemma for the linear system in the infinite-dimensional space under the pseudo-metric. 
We also quantify the difference between {\color{black}the ambiguity sets} before and after the PLA
%piecewise linear approximation 
and indicate the special cases when these two ambiguity sets coincide. Moreover, to facilitate the application of the UPRO model in a data-driven environment, we carry out stability analysis on the optimal value and the optimal solutions of the UPRO model against 
data 
perturbation/contamination. 
%/contamination %of
% in the data. 



Finally, we undertake extensive numerical tests on the proposed UPRO models and 
%tractable reformulation
computational schemes and obtain the following main findings. The EPLA scheme (see (\ref{eq:PRO-N-reformulate})) and 
the IPLA scheme (see (\ref{eq:PRO_MILP_eqi}))
generate the same results in terms of the convergence of the worst-case utility functions and the optimal values, 
but the former works much faster because 
the IPLA requires solving a MILP as opposed to an LP in the EPLA and the number of integer variables 
increases rapidly with the increase of scenarios of
the underlying exogenous uncertainty.
%(compared to LP in the former). 
The 
%PLA with the polyhedral method 
IPLA works also well in tri-attribute case 
although
%but
the CPU time is long. 
%This is because
For the constrained expected utility maximization problem, the two robust models 
may coincide in some cases but differ in other cases depending on 
the constraints. The approximate maximin model is stable in the presence of small perturbations arising %in
during
the preference elicitation process and
{\color{black}resulting from} exogenous uncertainty data.




The rest of the paper is organized as follows. 
Section~\ref{sec:2Bi-Attribute} introduces the multi-attribute UPRO model 
%as well as the equivalent representation 
and the definition 
of the ambiguity set. 
Section~\ref{sec:numer-methods}
%details 
gives the details of
the EPLA 
%of the multi-attribute utility function 
approach
and 
%the computational scheme for solving the approximated 
tractable formulation of 
approximate UPRO in bi-attribute case.
% 
%analyses the utility maximization problem from mixed integer linear programming (MILP) perspective and extends to multi-atrribute case.
Section~\ref{sec:multi-atrribute} discusses the IPLA approach for the UPRO %model 
in multi-attribute case.  
%the case 
%where the ambiguity set is decision-dependent. 
Section~\ref{sec-errorbound} investigates the error bound of the approximate ambiguity set as well as the impact on the optimal value and the optimal solutions to the UPRO model. 
Section~\ref{sec:constrained} extends the UPRO model to the constrained optimization problem.
% Section \ref{sec:perturbation} discusses the stability of the PRO model against data contamination and distortion. 
% Finally, 
Section~\ref{sec:numerical results} reports the numerical tests of the UPRO model. Concluding remarks are given in Section~\ref{sec:Concluding remarks}. 
%{\color{purple}I added the labels of Sections except the decision-dependent part.}
%Got it, many thanks. 

% {\bf Notation.} 
% Throughout this paper, 
% we use $\R^n$ to represent the $n$-dimensional Euclidean space. 
% The $k$th element of a vector $\bdx\in \R^n$ 
% % ($k\leq n$) 
% is denoted by $x_k$. 

% By convention, 
% vectors (matrices) are denoted by boldface lowercase (uppercase) letters 
% and scalars are denoted by lowercase letters. 
% The following notational conventions will be used throughout the paper: $u_i(y)=u(x_i,y), u_j(x)=u(x,y_j)$, and $u_{ij}=u(x_i,y_j)$. 
% $[g]_{x_1,y_1}^{x_2,y_2}=g(x_1,y_1)+g(x_2,y_2)-g(x_1,y_2)-g(y,y_1)$. 

% probability space, signed measure 
% % \Q{QW: I think $u_i$ and $u_j$ is confusing when $i=j$ and only $u(\underline{x},y)$ and $u(x,\underline{y})$ are used, they will both be $u_1$.
% % }



\section{The bi-attribute UPRO model}
\label{sec:2Bi-Attribute}


We consider the following one-stage expected bi-attribute utility maximization problem 
\bgeqn 
\label{eq:UMP-bi}
\max_{\bdz\in Z} \; \bbe_P[u(\bdf(\bdz,\bdxi))],
\edeqn
where $\bdf:\R^{n}\times \R^{m} \to \R^2$ is a continuous vector-valued function representing the rewards from two attributes,
$\bdz$ is a decision vector which is restricted to taking values over a specified feasible set $Z\subset \R^n$, 
$\bdxi$ is a random vector
representing exogenous uncertainties in the decision making problem mapping from probability space $(\Omega,\mathcal{F},\mathbb{P})$ to $\R^m$,
%and 
the expectation is taken w.r.t.~the probability of $\bdxi$, i.e., $P:=\mathbb P\circ\bdxi^{-1}$, 
and $u:\R^2\to\R$ is a real-valued 
%increasing 
componentwise non-decreasing continuous utility function, which maps each value of $\bdf$ to a utility value of the DM's interest. 
%{\color{red}
% {Let \color{purple}$T:=X\times Y=\{(x,y):x\in X,y\in Y\}$,
% where $x$ and $y$ represent the two attributes that a DM concerns 
% in decision making problems. }
%This sentence is a bit not related here, I suggest removing it if not necessary.
%}
% $T$ will be used in the following Assumption 2.1.
To facilitate our discussions, we make the following assumption throughout the paper. 


\begin{assumption}
\label{assu-original}
$\bdf$ is a continuous function 
with its range covered by
% {\color{purple}$T%=X\times Y
% =[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$,}
%{\color{blue}
$T:=X\times Y$ with $X:=[\underline{x},\bar{x}]$ and $Y:=[\underline{y},\bar{y}]$,
%}
$Z$ is a compact convex subset of $\R^n$ and the support set $\Xi$ of $\bdxi$ is compact. 
\end{assumption}


Assumption~\ref{assu-original} allows us to restrict the domain of the unknown true utility function to a rectangle $T$.
We follow \cite{hu2015robust} and the literature of behavioural economics to 
normalize the utility function with $u(\underline{x},\underline{y})=0$ and $u(\bar{x},\bar{y})=1$. 
In most of the existing research on multi-attribute decision making,  
utility functions are assumed to be known (\cite{greco2016multiple,liesio2021nonadditive}) 
or can be elicited and estimated through a tolerable amount of questions
(\cite{andre2007non}). 
In practice, however, a DM's utility function is often unknown 
either from the DM's perspective or from the modeller's perspective (\cite{AmD15}). 



In this paper, our focus %here 
is on the situation where the DM does not have complete information to identify the true utility function $u^*$, i.e., 
%utility 
risk preference, 
but it is possible to elicit partial information to construct an ambiguity set of utility functions, denoted by $\mathcal{U}$, 
such that the true utility function 
which represents the DM's preference lies within $\mathcal{U}$ with high likelihood.
Under this circumstance, it might be sensible to consider 
the following bi-attribute utility preference robust optimization model 
to mitigate the model risk arising from the ambiguity in the true utility function 
\begin{equation}
\label{eq:MAUT-robust}
    \inmat{(BUPRO)} \quad
    \vt:=\max_{\bdz\in Z} \; \min_{u\in {\cal U}} \; \bbe_P[u(\bdf(\bdz,\bdxi))].
\end{equation}
The structure of the BUPRO model is largely determined by the structure of the ambiguity set $\calu$ as well as the nature of the utility functions in this set. 
%As reviewed in the introduction, 
Various approaches have been proposed
to construct 
%for constructing 
an ambiguity set of utility functions in the literature of PRO depending on the availability of information (see \cite{AmD15, liu2021multistage,guo2022robust}). 
They are usually based on two types of information about a DM's preference: 
generic information such as risk aversion or risk taking 
%(tastes on gains and losses) 
and specific information such as preferring one prospect to another (see \cite{WaX23}).  
% {\color{red}Apart from Lipschitz continuity and increasing properties of $u$,
% the following is a common hypothesis about a DM’s utility function and an effective scheme for eliciting the utility function in a two-attribute case.
% {\em Risk averse}.


In single-attribute decision making, 
a DM is risk-averse if and only if the DM's utility function is concave (see \cite{tsanakas2003risk}).
Unfortunately, the equivalent relation 
does not hold in the multi-attribute case. 
%From the existing literature in behavioral finance, 
% There are at least three main definitions of multivariate risk averse, see. e.g, \cite{richard1975multivariate,duncan1977matrix,karni1979multivariate,levy1991arrow} and references therein. 
% In this paper, we adopt the definition 
% %from 
% in \cite{richard1975multivariate}. 
Let $x_0,x_1\in X$, $y_0,y_1\in Y$ with  $x_0< x_1$ and $y_0<y_1$. 
Consider the following two lotteries:
% for any $x_0,x_1,y_0$, and $y_1$: 
Lottery one ($L_1$) gives the DM a 0.5 chance of receiving $(x_0,y_0)$ and a 0.5 chance of receiving $(x_1,y_1)$. 
Lottery two ($L_2$) gives the DM a 0.5 chance of receiving $(x_0,y_1)$ and a 0.5 chance of receiving $(x_1,y_0)$.
The DM is said to be \emph{multivariate risk-averse} (MRA) 
if the DM prefers $L_2$ to $L_1$ 
for all $x_0,x_1,y_0$ and $y_1$ described above
(see e.g.,\cite{richard1975multivariate}).
%,duncan1977matrix,karni1979multivariate,levy1991arrow}).
% {\color{red}
This type of behaviour means that the DM prefers  taking 
a mix of the best and worst in the two respective attribute
to getting 
either the ``best'' or the ``worst'' with equal probability.
% }
%two attributes.
Using the expected utility theory, we can write down the DM's preference mathematically as
$0.5u(x_0,y_0)+0.5u(x_1,y_1) \leq 0.5u(x_0,y_1)+0.5u(x_1,y_0)$,
which is equivalent to
%Consequently, for an MRA DM, his multiattribute utility satisfies the so-called \emph{conservative property}: 
\begin{equation}
\label{eq:conservative}
u(x_0,y_1)+u(x_1,y_0)\geq u(x_0,y_0)+u(x_1,y_1)
\end{equation}
for all $x_0,x_1,y_0$ and $y_1$. 
(\ref{eq:conservative}) is known as 
\emph{conservative property}.
% Note that such multiattribute utility function can induce a Lebesgue-Stieltjes measure on $\R^2$, see, e.g., \citet[Section 1.3.3, Remark 2.3.3]{AtL06}. 
%Moreover, for
In the case when the utility function is twice continuously differentiable,
%in all variables with positive first derivatives in all variables, \citet[Theorem 1]{richard1975multivariate} shows that a necessary and sufficient condition for this utility function is 
the property is equivalent to 
$u_{x y}:=\frac{\partial^2 u}{\partial x \partial y}\leq 0$ for all $(x,y)\in T$,
%=X \times Y$,
see \cite[Theorem 1]{richard1975multivariate}.
%{\color{red}
This kind of definition is given in \cite{richard1975multivariate},  
{\color{black}and} there are some other definitions of MRA, see e.g.~\cite{ duncan1977matrix,karni1979multivariate,levy1991arrow} and references therein. 
%They are equivalent in bi-attribute decision making.
%}
%cases.




%\end{equation*}
% He is \emph{multivariate risk neutral} (MRN) if and only if he is indifferent between $L_1$ and $L_2$ for all $x_0,x_1,y_0$, and $y_1$. 
% Finally, he is \emph{multivariate risk seeking} (MRS) if he does not prefer $L_2$ to $L_1$ for all $x_0,x_1,y_0$, and $y_1$. 
%\vspace{-25pt}
%\begin{remark}
%\label{rem:averse-concavity}
%It should be noted that
From the definition, we can see immediately that a
% {\color{blue}multivariate}
risk-averse DM's utility function is not necessarily concave (e.g.~
%$u(x,y)$ is concave.
%being a concave function does not mean that $u_{xy}\leq 0$, so that $u(x,y)$ is MRA. 
%Consider,  
%for example, 
%where 
%
$u(x,y)=x+y-(x y)^{1/4}$ for $x>0$ and $y>0$). 
% It is easy to see that $u_{x y}(x,y)=-\frac{1}{16}(x y)^{-3/4}<0$
% but $u$ is strictly convex.
%concave.  
This is a fundamental difference between the multi-attribute and single-attribute utility functions.
% in multi-attribute
%decision making
% and %the utility function in
% single-attribute decision making.
%attribute utility function, where the latter is risk averse if and only if the utility function is concave. 
%\end{remark}
% Let 
% $$
% {\cal U}_{\rm RA}:=\{u:u \inmat{\;satisfies conservative property (\ref{eq:conservative})}\}.
% $$
In the forthcoming discussions, we will consider %this type of
utility functions satisfying
(\ref{eq:conservative})
since 
% {\color{blue}multivariate}
risk-averse is widely considered in the literature (\cite{abbas2005attribute,abbas2009multiattribute}),
% {\color{blue}
 e.g., $u(x,y)=\frac{1-e^{-\gamma(x+\beta y)}}{1-e^{-\gamma(1+\beta)}}$ with $\gamma>0$, $\beta>0$,
and $u(x,y)=e^x-e^{-y}-e^{-x-2y}$.
% }

% For example,
% %\[
% $
% u(x,y)=\frac{1-e^{-\gamma(x+\beta y)}}{1-e^{-\gamma(1+\beta)}},
% %\quad 
% \forall\,x\in [0,1], y\in[0,1]$, 
% satisfies the conservative property,
% (see \cite[Example 1 (b)]{abbas2009multiattribute}), %\]
% where $\beta>0$ is a parameter.
% Specifically,
% for any $x_{i+1}\geq x_i$, $y_{j+1}\geq y_j$, it holds
% % XXXXXXXXXXXXXXXXXX\\
% % %In the following example, 
% % In the literature, there are many concrete bi-variate 
% % %Next, we give
% % %we provide 
% % %some well-known 
% % %examples to illustrate such multiattribute
% % utility functions in bi-attribute decision making. Here we list a couple of well-known ones.
% % \begin{example}
% % \label{ex-MRA}
% % % Some MRA utility functions from the literature. 
% % Let $X=[0,1]$ and $Y=[0,1]$.
% % \begin{enumerate}[label=(\roman*)]
% % \item \cite[Example 1]{abbas2005attribute} 
% % %For $\beta>0$, consider 
% % \[
% % u(x,y)= \frac{x\left(1-e^{-y/(0.3+x)}\right)}{1-e^{-1/(0.3+x)}}, \quad \forall\, x\in X, y\in Y. 
% % \]
% % \item \cite[Example 1 (b)]{abbas2009multiattribute} %Consider 
% % \[
% % u(x,y)=\frac{1-e^{-\gamma(x+\beta y)}}{1-e^{-\gamma(1+\beta)}} \quad \forall\,x\in X, y\in Y,\
% % \]
% % where $\beta>0$ is a parameter.
% % \end{enumerate}
% % \end{example}
% \bgeq
% && u(x_{i+1},y_{j+1})+u(x_i,y_j)-u(x_{i+1},y_j)-u(x_i,y_{j+1})\\
% &=& \frac{1}{1-e^{-\gamma(1+\beta)}} \left(
% 1-e^{-\gamma(x_{i+1}+\beta y_{j+1})}
% +1-e^{-\gamma(x_i+\beta y_j)} 
% -(1-e^{-\gamma(x_{i+1}+\beta y_j)})
% -(1-e^{-\gamma(x_{i}+\beta y_{j+1})})
% \right)
% \\
% &=&\frac{e^{-\gamma(x_i+\beta y_j)}}{1-e^{-\gamma(1+\beta)}} \left(
% -e^{-\gamma(x_{i+1}-x_i+\beta (y_{j+1}-y_j))}
% -1
% +e^{-\gamma(x_{i+1}-x_i)}
% +e^{-\gamma\beta (y_{j+1}-y_j)}
% \right)\\
% &=&\frac{e^{-\gamma(x_i+\beta y_j)}}{1-e^{-\gamma(1+\beta)}} \left(
% e^{-\gamma(x_{i+1}-x_i)}(1-e^{-\gamma\beta (y_{j+1}-y_j))})
% -(1-e^{-\gamma \beta (y_{j+1}-y_j)})
% \right)\\
% &=&\frac{e^{-\gamma(x_i+\beta y_j)}}{1-e^{-\gamma(1+\beta)}} \left(
% (e^{-\gamma(x_{i+1}-x_i)}-1)(1-e^{-\gamma\beta (y_{j+1}-y_j))}) \right)\leq 0.
% \edeq
% }
% Here inspired by the ambiguity set of single-attribute utilities in \cite{hu2015robust, GuX21}, we consider the following moment-type  ambiguity set of bi-attribute utility functions. 



%{\em  Pairwise comparison}.
Specific information about a DM's preference is often obtained by a modeller during %in
a preference elicitation process.
The most widely used elicitation method is pairwise comparison
(\cite{AmD15}).
For instance, a DM is given a pair of lotteries ${\bm A}$ and ${\bm B}$ defined over $(\Omega,{\cal F},\mathbb{P})$ with different outcomes and asked for preference.
If the DM prefers ${\bm A}$, then we can use the expected utility theory to characterize the preference,
i.e., 
% {\color{blue}  
% \bbe[u(\bdcb)]\leq\bbe[u(\bdca)]
% }
\begin{equation*}
\bbe_{\mathbb{P}}[u(\bdcb(\omega))] = \int_{T} u(x,y) d F_{\bdcb}(x,y)\leq \int_{T} u(x,y) d F_{\bdca}(x,y) = \bbe_{\mathbb{P}}[u(\bdca(\omega))]
\end{equation*}
or equivalently
\begin{equation}
\label{eq:ambi-U-ex}
\int_{T} u(x,y) d \psi(x,y):=\int_{T} u(x,y) d (F_{\bdcb}(x,y)-F_{\bdca}(x,y))\leq 0, 
\end{equation} 
where
$F_{{\bm A}}$ and $F_{{\bm B}}$ are the cumulative distribution functions of ${\bm A}$ and $\bdcb$, 
$u$ is the true utility function which represents the DM's preference but is unknown. 
The outcomes of the pairwise comparisons enable us to narrow down the scope of the utility function by inequalities. 
As more and more questions are asked,   
we 
%will be able to 
can derive more inequalities
as such {\color{black}which 
%hopefully will 
lead to} a smaller ambiguity set.  
% Note that the function $\psi(x,y):=F_{\bdcb}(x,y)-F_{\bdca}(x,y)$ has bounded variation. 
% This 
% %motivates 
% prompts
% us to 
%introduce 
To facilitate discussions, we give a formal definition of the ambiguity set constructed as such.

\begin{definition}
%[Ambiguity set {\color{red}based on pairwise comparisons}{\color{blue}linear system}]
\label{defi:ambguity-set}
Let $\scru$ be the set of continuous, componentwise non-decreasing, and 
normalized utility functions mapping from $T$
%=X\times Y$ 
to $[0,1]$ 
% with a finite number of non-differentiable points, 
%which satisfies normalized conditions and
satisfying conservative property (\ref{eq:conservative}). 
Define the ambiguity set of utility functions  %is defined as 
as
\begin{equation}
\label{eq:ambiguity_set}
    \mathcal{U}:=\left\{ u\in \scru 
    %\,\lt|\,
    \,:\,\int_{T} u(x,y)d\psi_l(x,y)\leq c_l, l=1,\ldots,M  \right\},
\end{equation}
where $\psi_l:T\rightarrow \R$ is a real-valued function and
%with bounded {\color{red}Hardy-Krause} variation,
$c_l$ is a given constant  for $l=1,\ldots, M$, and the integrals are in the sense of Lebesgue-Stieltjes 
% {\color{red}LS} 
integration.
%integrals.
% (in other words, $\psi_l$ is a real-valued function with bounded variation, see, e.g., \citet[P. 129]{hildebrandt1963introduction}, \cite{Mcs47})
\end{definition}
In this definition, we make a blanket assumption that 
%are using 
%implicitly assume 
the LS
%Lebesgue-Stieltjes
integrals are well-defined,
% which implicitly 
% assume
% %require 
% $\psi_l$ to have a bounded {\color{red}Hardy-Krause} variation,
we refer readers to \cite{clarkson1933definitions}, 
\cite[page 129]{hildebrandt1963introduction}
and \cite{Mcs47} for the concept and properties of the integration. 
% {\color{red}
% Wei: the bounded variation should be bounded Hardy–Krause variation because every bounded Hardy–Krause variation function is Borel measurable, see, e.g., \citet[Theorem 3.1]{aistleitner_pausinger_svane_tichy_2017}.
% } 
%HX: we don't need these details as it is not the focus of the paper
%{\color{red}
${\cal U}$ in (\ref{eq:ambiguity_set}) is defined by a system of inequalities 
which are linear in both $u$ and $\psi_l$. 
Thus the ambiguity set ${\cal U}$ defined as such is a convex set. 
Moreover, we assume that the DM's preferences shown during the elicitation process are consistent, which means that ${\cal U}$ is non-empty.
%the system of inequalities 
In practice, preferences observed over an elicitation process may be inconsistent due to observation/measurement errors, noise in data %errors
%{\color{red}contamination}
or the DM's wrong answers. We refer readers to %Bertsimas and O'Hair 
\cite{AmD15} and
%Armbruster and Delage 
\cite{BeO13} for approaches to handle the inconsistency.


% {\color{red}The following example is a bit redundant, please comparing with the discussion before Definition 2.1. 
% I suggest merging them together. Please double check. 
% }
% Three places will refer to this example.

% \begin{example} 
% \label{ex:pairwise}
% Let $\underline{x}=\underline{y}=0$ and $\bar{x}=\bar{y}=1$. 
% %then the rectangle 
% Then $T=X\times Y=[0,1]\times [0,1]$.
% Consider $T=X\times Y$
% Consider the following two lotteries $\bdca,\bdcb:\Omega\to T$. 
% games
% {\color{red} HX: should we use terminology ``lottery'' rather than ``game''?}
% {\color{blue}
% Sainan: delete it, it should be lottery.}
% {\color{red}Wei: Both are OK, for the consistency of the paper, we should use the terminology ``lottery''.}
% \begin{equation*}
% %\text{Game}\;
% \boldsymbol{A}=\left\{
% \begin{array}{ll}
% (0,0) &  \inmat{with probability } 1-p, \\
% (1,1) & \inmat{with probability } p, 
% \end{array}
% \right. 
% \inmat{\;and}\quad
% \boldsymbol{B}=(\hat{x},\hat{y})\; \inmat{with probability}\;1,
% \end{equation*}
% where
% %for some
% $(\hat{x},\hat{y})\in (X\times Y)/\{(0,0),(1,1)\}$. 
%Suppose that 
%{\color{red}If} 
% If the DM prefers $\boldsymbol{A}$ rather than $\boldsymbol{B}$,
% then we have 
% % \begin{equation}
% $\int_{T} u(x,y) d F_{\bdcb}(x,y) \leq \int_{T} u(x,y) d F_{\bdca}(x,y)$,
% % \end{equation}
% or equivalently
% \begin{equation}
% \label{eq-amb-pairwise}
% \int_{T} u(x,y) d (F_{\bdcb}(x,y)-F_{\bdca}(x,y))\leq 0. 
% \end{equation} 
% Then we can use the preference information to construct the ambiguity set
%by the expected utility theory,  the DM's utility function lies in the ambiguity set: 
% \begin{equation}
% \mathcal{U}
% =\left\{u \in \scru: \int_{T} u(x,y) d \psi(x,y)\leq 0 \right\},
% %=\Big\{u \in \scru: u(\hat{x},\hat{y}) \leq p \Big\}, 
% \label{eq:ambi-U-ex}
% \end{equation} 
% where 
% $\psi(x,y)=F_{\boldsymbol{B}}(x,y)-F_{\boldsymbol{A}}(x,y)$.
% =\1_{[\hat{x},1]\times [\hat{y},1]}(x,y)-(1-p)\1_{[0,1]\times[0,1]\setminus (1,1)}(x,y)-\1_{(1,1)}(x,y).$
% \]

% {\color{red}
% In the following bi-attribute case,  we  present  tractable  reformulation for evaluating $\min_{u\in \widetilde{\cal U}} \bbe_{P}[u({\bm f}(\bdz,\bdxi))]$
% for the set $\widetilde{\cal U}$ that  are  formed  from  intersections  of  the sets ${\cal U}_{\rm RA}$, ${\cal U}$ and Lipschitz/convex/concave properties of utility functions.}


% % \section{Setup of the UPRO models}
% %Multi-Attribute Utility Preference Robust Optimization}
% %\label{sec:2Bi-Attribute}
% %In this section, we develop the multi-attribute utility preference robust optimization model for the vector-valued profit function with $n$ attributes 
% XXXXXXXXXXXXXXXXXXXXXXXXX\\
% We begin our discussion of UPRO models by considering the two-attribute case. 
% and construct an ambiguity set of plausible multi-attribute utility functions with the DM's partially available information. 
% Specifically, we construct the ambiguity set by extending the single-attribute moment-type approach \citep{guo2022robust} to the multi-attribute case. 
% will develop utility preference robust optimization models
% for decision making problems with two attributes. 
% %To this end, we introduce the concept of risk   
% %To do so, we start with the bi-variate risk aversion preference. 
% For the completeness of the paper, we recall the expected utility theory and the risk aversion in multi-attribute setting. 
% For the simplification of notation, we will mainly focus on the decision-making problem with two attributes in this paper although our model and computational procedures can be easily extend to the case with $n$ attributes. 
% We begin by recalling the definition of multi-attribute risk aversion in this setup. 
% \subsection{Setup}
% In this subsection, we adopt some preliminary results from behavioral economics to provide the application background and theoretical foundations for the proposed preference robust model in the paper.
% % For the simplification of notation, we first consider the decision-making problem with two attributes and generalize the definitions and theorems of Sections 2-6 to utility functions for $n$ attributes in Section 7. 
% Let $x$ and $y$ denote the two attributes that a DM concerns in a decision-making problem.  
% Let $z_1, z_2, \ldots, z_n$ denote the $n$ attributes that a DM concerns for a decision-making problem. 
% For instance, a person has to select between two employment options: 
% option A gives him $\$12K$ per month and 20 days of annual vacation, while option B gives him $\$15K$ per month and 10 days of annual vacation. 
% In this example, there are two attributes, namely, the monthly salary and the length of a vacation for the job options denoted by $z_1$ and $z_2$ respectively. 
% The person has to decide between $(12K,20)$ and $(15K,10)$. 
% The sets of all possible attribute levels of $x$ and $y$ are
% % For $i=1,2,\ldots,n$, the set of all possible attribute levels of $z_i$ is 
% assumed to be a closed interval 
% %from 
% in the real
% %number 
% line $\R$ and is denoted by 
% % $Z_i$. 
% $X$ and $Y$ respectively. 
% Let $T=X\times Y$, the Cartesian product of $X$ and $Y$, be the set of all consequences $\bdt=(x,y)$ where $x\in X$ and $y\in Y$. 
% We let $Z=Z_1\times Z_2\times\cdots\times Z_n$, 
% the Cartesian product of the $Z_i$, be the set of all $n$-tuples $(z_1,z_2,\ldots,z_n)$, where $z_i\in Z_i$ for each $i$. 
% An element of $Z$ is called a \emph{consequence} and is denoted as $z=(z_1,\ldots,z_n)$. 
% An element of $T$ is called a \emph{consequence} and is denoted as $t=(x,y)$. 
% 
% \underline{The foundation of expected utility theory and Multivariate risk averse}
% 
% {\color{red}Wei:
% Throughout the paper, 
% we assume that the DM is \emph{rational} and his preference relation defined over the consequence space $Z$ satisfies four axioms: 
% completeness; transitivity; independence of irrelevant alternatives; and continuity. 
% Then such relation can be represented by a real-valued multi-attribute utility function, denoted by $u:T\rightarrow \R$, 
% which assigns values 
% % numbers (utilities) 
% to each consequence over $Z$ such that choosing the best consequence according to the preference relation amounts to choosing the one with the highest expected utility, see, e.g., \citet[Theorem 8.2]{fishburn1970utility}. 
% This result is known as \emph{von Neumann–Morgenstern utility representation theorem}. 
% % {\color{red}wei: updated on Oct 12, 2022.}
% }
% {\color{green} This comment is not well connected to the context.
% }
% {\color{blue}The main motivation of this comment is to emphasize the implicit assumption of this paper, that is, the proposed model only holds for the utility maximization DM.}
% 
% we know that the preference relation can be represented by a real-valued multi-attribute utility function, denoted by $u:Z\rightarrow \R$. 
% 
% i.e. one can assign numbers (utilities) to each outcome of the lottery such that choosing the best lottery according to the preference relation amounts to choosing the lottery with the highest expected utility.
% Let $\prec$ denote a binary relation 
% We move to the discussion about the multivariate risk averse. 

% \subsection{Bi-Variate Risk Aversion}
% \subsection{Multivariate risk aversion}

% %We consider the multivariate risk averse DM. 
% In the single-attribute decision-making problem, 
% We are particularly interested in the case that the DM is risk averse. In the literature of behavioural economics, a DM is 
% \emph{risk averse} if and only if 
% %his
% the DM's single-attribute utility function is concave.
% %, see, e.g., XXX. 
% %However, this 
% Unfortunately, the equivalent relation 
% %might not be true
% does not hold in multi-attribute case. 
% %From the existing literature in behavioral finance, 
% There are at least three main definitions of multivariate risk averse, see. e.g, \cite{richard1975multivariate,duncan1977matrix,karni1979multivariate,levy1991arrow} and references therein. 
% In this paper, we adopt the definition 
% %from 
% in \cite{richard1975multivariate}. 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In single-attribute decision making, a decision maker % is risk averse (taking) if and only if 
% the utility function presenting 
% the DM's preference is concave (taking).
% % concavity and convexity of a utility function are  
% % used to characterize risk aversion and risk taking 
% % of a DM. 
% This is no longer the case in bi-attribute 
% case. 
% % Let the two attributes of concern to the DM be denoted by $x$ and $y$. 
% % The set of all possible levels of attributes $x$ and $y$ are assumed to be closed intervals form the real number line and are denoted as $X$ and $Y$, respectively. 
% Let $x\in X$ and $y\in Y$ denote the two attributes that DM concerns, where $X\subset\R$ and $Y\subset\R$ are closed intervals 
% %which denote 
% signifying 
% the possible attribute levels.
% For example, a person has to select between two employment options: option A gives him \$12K per month and 20 days of annual vacation, while option B gives him \$15K per month and 10 days of annual vacation. 
% In this example, $x$ and $y$ represent the monthly salary and the length of a vacation for the job options. 
% The person has to decide between (12K,20) and (15K,10). 
% 
% Let $T=X\times Y$, the Cartesian product of $X$ and $Y$, be the set of all pairs $(x,y)$ where $x\in X$ and $y\in Y$. 
% The element $t=(x,y)$ of $T$ is called a consequence. 
% Throughout this paper, we assume that the DM's risk preference can be captured by a utility function $u(\bdt)=u(x,y)$ defined over $T$. 
% 
% We are now ready to define bi-variate risk aversion based on \cite{richard1975multivariate}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Throughout this paper, our focus for the bi-attribute utility function is continuous, non-decreasing, multivariate risk averse/conservative. 

% \WW{Risk preference including risk taste, risk attitude, and belief. 
% In this paper, our focus is utility, which is known as taste in behavioral finance. 
% We may consider how to reorganize this section carefully: 

% 1. Starting with bi-attribute utility function. 
% 2. Examples of bi-attribute utility function are needed. 
% 3. 
% To emphasize the properties of bi-attribute utility function: risk averse and monotonicity. 
% 4. Notation issues. 
% }
% \WW{Reorganized by Wei, please double check.}
% Decision making problems where the consequences are expressed in terms of more than one attribute have become increasingly important. 
% As a classical decision criterion, the expected utility theory has been widely adopted in practice. 

% \subsubsection{Lebesgue-Stieltjes integral}

% In this subsection, we introduce the Lebesgue-Stieltjes integral over $\R^2$ and review some preliminary results about the integral. 

% Let $F:T\rightarrow \R$ be a function of bounded Hardy–Krause variation, where $T=X\times Y\subset \R^2$. 
% Then $F$ is Borel measurable, see, e.g., \citet[Theorem 3.1]{aistleitner_pausinger_svane_tichy_2017}. 


% Since the risk-averse DM under UPRO framework is widely discussed in the literature, see, e.g., \cite{AmD15}

% bi-attribute to multi-attribute, 

% discuss different definitions in the literature,  

% S-shaped.  curvature property 


% Different from the single-attribute case, 


% \subsection{Preference robust model}


% We consider the following one-stage expected multi-attribute utility maximization problem 
% \bgeqn 
% \max_{\bdz\in Z} \bbe_P[u(\bdf(\bdz,\bdxi))],
% \edeqn
% where $\bdf:\R^{n}\times \R^{m} \to \R^2$ is a continuous vector-valued function representing a financial position with two attributes or performance of an investment in practice, 
% $\bdz$ is a decision vector which is restricted to taking values over a specified feasible set $Z\subset \R^n$, 
% $\bdxi:(\Omega,\mathcal{F},\mathbb{P}) \rightarrow \R^m$ is a 
% vector of random variables representing exogenous uncertainties in the decision making problem and the expectation is taken w.r.t. the probability of $\bdxi$, i.e., $P:=\mathbb P\circ \bdxi^{-1}$, 
% and $u:\R^2\to\R$ is a real-valued increasing utility function, which maps each value of $\bdf$ to a utility value of the DM's interest. 
% To facilitate our discussions, we make the following assumption throughout the paper. 




% models here, and discuss the compactness and attainments of the problem 

% \subsection{Lebesgue–Stieltjes 
% integral}

% Difficulty for calculation? infinite program? 
% motivation for continuous piecewise linear approximation. 

% All are ready solved and wait for summarize. 

% signed measure induced by a function 

% the definitions for bounded variations

% definition of the L-S integral, 

% In this subsection, we introduce the Lebesgue-Stieltjes integral over $\R^2$ and review some preliminary results about the integral. 

% Let $F:T\rightarrow \R$ be a function of bounded Hardy–Krause variation, where $T=X\times Y\subset \R^2$. 
% Then $F$ is Borel measurable, see, e.g., \citet[Theorem 3.1]{aistleitner_pausinger_svane_tichy_2017}. 


% For $d\in \mathbb{N}$ and for non-empty intervals $\Xi_1,\ldots,\Xi_d\subset \R$, define by $\Xi:=\times_{i=1}^d \Xi_i$ their Cartesian product and, for $I\subset\{1,\ldots,d\}$, $I\neq \emptyset$, 
% by $\Xi_I:=\times_{i\in I}\Xi_i$ the Chartesian product of $(\Xi_i)_{i\in I}$. 
% For $i\in\{1,\ldots,d\}$, we denote the lower and upper bound on $\Xi_i$ by $a_i:=\inf\{\Xi_i\}$ and $b_i:=\sup\{\Xi_i\}$, respectively. 
% We set $a:=(a_1,\ldots,a_d)$ and $b:=(b_1,\ldots,b_d)$. 

% For $x=(x_1,\ldots,x_d),y=(y_1,\ldots,y_d)\in \R^d$, we write $x\leq y$ whenever $x_i\leq y_i$ for all $i\in \{1,\ldots,d\}$. 
% Further, we denote by $x\leq y$ by $[x,y]:=\times_{i=1}^d [x_i,y_i]$ the closed cuboid spanned by $x$ and $y$ and, similarly, by $[x,y),(x,y]$, and $(x,y)$ the corresponding componentwise half-open and open cuboid, respectively. 

% Denote by $\mathscr{B}(\Xi)$ the Borel $\sigma$-algebra on $\Xi$. 
% A signed measure on $(\Xi,\mathscr{\Xi})$ is a countably additive set function $\nu:\mathscr{B}\rightarrow \R\bigcup \{\pm\infty\}$ which attains at most one of the values $+\infty$ and $-\infty$ and fulfills $\nu(\emptyset)=0$. 
% Whenever there exists $\delta=(\delta_1,\ldots,\delta_d)$, $\epsilon=(\epsilon_1,\ldots,\epsilon_d)$ with $\delta_i,\epsilon_i>0$ for all $i\in\{1,\ldots,d\}$ such that $x-\delta,y+\epsilon\in \Xi$ and $\nu([x-\delta,y+\epsilon])$ is finite, then the iterated limit
% \bgeq
% \lim_{\substack{\delta_i,\epsilon_i\downarrow 0\\ i\in\{1,\ldots,d\}}} \nu\left(\times_{i=1}^d [x_i-\delta_i,y_i+\epsilon_i]\right)
% &:=& \lim_{\delta_{i_1}\downarrow 0} \lim_{\epsilon_{j_1}\downarrow 0}\cdots \lim_{\delta_{i_d}\downarrow 0} \lim_{\epsilon_{j_d}\downarrow 0}\nu \left(\times_{i=1}^d [x_i-\delta_i,y_i+\epsilon_i] \right)\\
% &=& \nu\left( \bigcap_{\substack{\delta_i,\epsilon_i\\i=1,\ldots,d}} \times_{i=1}^d[x_i-\delta_i,y_i+\epsilon_i] \right)\\
% &=& \nu([x_1,y_1]\times\cdots\times [x_d,y_d])
% \edeq
% exists for all arrangements $(i_1,\ldots,i_d), (j_1,\ldots,j_d)\in S_d$ of the limits due to the continuity of measures, see, e.g., \citet[Theorem 2.4.3]{benedetto2009integration}, where $S_d$ denotes the set of all permutations of $\{1,\ldots, d\}$. 

% For a function $f:\Xi\rightarrow \R$, for $z=(z_1,\ldots, z_d)\in \Xi$, for $i=\{1,\ldots,d\}$, and for $\epsilon\geq 0$ such that $z_i+\epsilon\in \Xi$, define by 
% \[
% \Delta_{\epsilon}^i f(z):=f(z+\epsilon e_i)-f(z)
% \]
% the difference operator of length $\epsilon$ applied to the $i$-th component $f$, where $e_i$ denotes the $i$-th unit vector. 
% Then, for $x=(x_1,\ldots,x_d),y=(y_1,\ldots,y_d)\in \Xi$ such that $x\leq y$, the $d$-variate difference operator defined by 
% \begin{equation}
% \Delta_{x,y}[f]:=\Delta_{\epsilon_1}^1\cdots\Delta_{\epsilon_d}^d f(x), \;\epsilon_i:=y_i-x_i,\;i=\{1,\ldots,d\},
% \end{equation}
% describe the \emph{$f$-quasi-volume} $V_f(B_{x,y})$ (i.e., a possibly negative volume generated by $f$) of a box $B_{x,y}\subset \Xi$ with $(x,y)\subset B_{x,y}\subset [x,y]$, i.e., 
% \[
% V_f(B_{x,y})=\Delta_{x,y}[f],
% \]
% see, e.g., \cite{nelsen2007introduction}. 
% For $u=(u_1,\ldots,u_d),v=(v_1,\ldots,v_d)\in \R^d$, denote by $u\vee v:=(\max\{u_1,v_1\},\ldots,\max\{u_d,v_d\})$ and $u\wedge v:=(\min\{u_1,v_1\},\ldots,\min\{u_d,v_d\})$ the componentwise maximum or minimum, respectively. 
% Then define for a function $f:\Xi\rightarrow \R$ and $x,y\in \Xi$ the limit 
% \[
% D_{x,y}[f]:=\lim_{\substack{\delta_i,\epsilon_i\downarrow 0\\i\in \{1,\ldots,d\}}} \Delta_{a\vee(x-\delta),b\wedge(y+\epsilon)}[f]
% =\lim_{\delta_1\downarrow 0}\lim_{\epsilon_1\downarrow 0}\cdots\lim_{\delta_d\downarrow0}\lim_{\epsilon_d\downarrow 0} \Delta_{a\vee(x-\delta),b\wedge(y+\epsilon)}[f]
% \]
% whenever the iterated limit on the right-hand side exists and coincides for all permutations of the limits. 


% \begin{definition}[Measure inducing function]
% A function $f:\Xi\rightarrow \R$ is \emph{measure inducing} if there exists a signed measure $\nu$ such that 
% \begin{equation}
% \label{eq:measure-inducing-function}
% \nu([x,y])=D_{x,y}[f]
% \end{equation}
% for all $x,y\in \Xi$ with $x\leq y$. 
% Denote by $\nu_f$ the signed measure induced by $f$. 
% \end{definition}

% If $f:\Xi\rightarrow\R$ is continuous, then the induced signed measure $\nu_f$ is generated by 
% \[
% \nu_f(B)=\Delta_{x,y}[f]
% \]
% for all $x=(x_1,\ldots,x_d)$, $y=(y_1,\ldots,y_d)\in \R^d$ such that $a\leq x\leq y\leq b$, where $B=\times_{i=1}^d B_i$ with $B_i\in \{[x_i,y_i],(x_i,y_i],[x_i,y_i),(x_i,y_i)\}$, $1\leq i\leq d$, 
% see, e.g., \citet[Proposition 2.3]{Ans22}. 

% {\color{red}
% \begin{proposition}
% Let $\psi:[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]\rightarrow \R$ be a real-valued function with bounded Hardy-Krause variation and 
% % and 
% $F:[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]\rightarrow \R$ be a continuous piecewise linear function with only two pieces, see Figure \ref{fig-division}. 
% Then 
% % (i) The signed measure $\mu_F$ induced by $F$ is generated by 
% % \[
% % \mu_F(R)=\Delta_{x,y}[F]=F(x_{i+1},y_{j+1})-F(x_{i+1},y_{j})-F(x_{i},y_{j+1})+F(x_i,y_j)
% % \]
% % for all $R=[x_i,x_{i+1}]\times [y_j,y_{j+1}]$ with $\underline{a}\leq x_i\leq x_{i+1}\leq \overline{a}$ 
% % and $\underline{b}\leq y_j\leq y_{j+1}\leq \overline{b}$. 

% % (ii) 
% % $\mu_F(\inmat{int}(I))=\mu_F(\inmat{int}(II))=0,\;\mu_F(\partial (I\cup II))=0, \;\inmat{and}\; \mu_F(I\cap II)=\mu_F(I\cup II)$. 

% % (iii) The induced signed measure $\mu_F$ on $I\cap II$ is proportional to the Lebesgue measure $\lambda$ on $I\cap II$ with constant 
% % $\mu_F(I\cap II)/\lambda(I\cap II)$. 

% % (iv) The following equality holds
% \[\int_{[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]}\psi(x,y)d F(x,y)
% =\mu_F(I\cap II)
% % [F(\overline{a},\overline{b})-F(\overline{a},\underline{b})-F(\underline{a},\overline{b})+F(\underline{a},\underline{b})] 
% \frac{1}{\overline{a}-\underline{a}}\int_{\underline{a}}^{\overline{a}} \psi(x,y(x))d x, 
% \]
% where $y(x)$ is the linear function for segment $I\cap II$ and 
% \[
% \mu_F(I\cap II)=F(\overline{a},\overline{b})-F(\underline{a},\overline{b})-F(\overline{a},\underline{b})+F(\underline{a},\underline{b}).
% \]
% \end{proposition}

% \begin{proof}
% From \citet[Definition 2.1 and Proposition 2.3]{Ans22}, we know that the continuous function $F$ is measure inducing and the induced signed measure $\mu_F$ by $F$ is generated by 
% % The signed measure $\mu_F$ induced by $F$ is generated by 
% \[
% \mu_F(B)=\Delta_{x,y}[F]=F(x_{i+1},y_{j+1})-F(x_{i+1},y_{j})-F(x_{i},y_{j+1})+F(x_i,y_j)
% \]
% for all $B=[x_i,x_{i+1}]\times [y_j,y_{j+1}]$ or the corresponding component-wise half-open and open rectangles with $\underline{a}\leq x_i\leq x_{i+1}\leq \overline{a}$ 
% and $\underline{b}\leq y_j\leq y_{j+1}\leq \overline{b}$. 
% By the piece-wise linear property of $F$, it is straightforward that $\mu_F(B)=0$ for any $B\subset I$ or $B\subset II$ because 
% \[
% F(x_{i+1},y_{j+1})+F(x_i,y_j)=F((x_i+x_{i+1})/2,(y_j+y_{j+1})/2)=F(x_{i+1},y_{j})+F(x_{i},y_{j+1})
% \]
% for either $B\subset I$ or $B\subset II$. 
% Consequently, $\mu_F(\partial (I\cup II))=0$, where $\partial(A)$ means the boundary of the set $A$. 
% To see this, let $B=[x_i,x_i]\times [y_j,y_{j+1})$, we get $\mu_F(B)=0$. Likewise, we have that the signed measures of another three (disjoint) edges of the rectangle $I\cup II$ are all zero. 
% By the countable additivity of signed measure, we have $\mu_F(\partial I\cup II)=0$. 
% % {\color{green} How do you obtain this with signed measure?} 
% % {\color{black}Wei: This is directly from the definition, let $B=[x_i,x_i]\times [y_j,y_{j+1})$, we get $\mu_F(B)=0$. Likewise, we have that the signed measures of another three edge of the rectangle $I\cup II$ are all zero. 
% % By the countable additivity of signed measure, we have $\mu_F(\partial I\cup II)=0$. 
% % }

% \underline{Step 1.} We show that $\mu_F(\inmat{int}(I))=\mu_F(\inmat{int}(II))=0\;\inmat{and}\; \mu_F(I\cap II)=\mu_F(I\cup II)$, 
% where $\inmat{int}(A)$ means the interior of the set $A$. 
% For each $(x,y)\in \inmat{int}(I)$, 
% there exists a open ball centered at $(x,y)$ that is contained in $\inmat{int}(I)$. 
% % there exist $\delta_i,\epsilon_i>0$ for $i=1,2$ such that 
% % \[
% % R:=(x-\delta_1,x+\delta_2)\times (y-\epsilon_1, y+\epsilon_2)\in \inmat{int}(I). 
% % \]
% Now, we can construct $B_{ij}$ containing $(x,y)$ and contained in the ball of the form $(x_i,x_{i+1})\times (y_j,y_{j+1})$ 
% where $x_i,x_{i+1},y_j,y_{j+1}$ are all rational numbers, 
% and then $\inmat{int}(I)$ would be the union of these rectangles. 
% This union is at most countable (after discarding identical rectangles) 
% because $\{(x_i,x_{i+1})\times (y_j,y_{j+1}):x_i,x_{i+1},y_j,y_{j+1}\in\mathbb{Q}\}$ is countable. 
% Moreover, based on these rectangles $B_{ij}$, we can get a disjoint refinement countable rectangles $D_{l}$ such that $\inmat{int}(I)$ would be the union of these rectangles. 
% Therefore, by the countable additivity of signed measure, see, e.g., \citet[p 119]{AtL06}, we have 
% \[
% \mu_F(\inmat{int}(I))=\sum_{l}\mu_F(D_{l})=0. 
% \]
% Likewise, we have $\mu_F(\inmat{int}(II))=0$. 
% Consequently, $\mu_F(I\cap II)=\mu_F(I\cup II)$. 

% \underline{Step 2.} We show that the induced signed measure $\mu_F$ on $I\cap II$ is proportional to the Lebesgue measure $\lambda$ on $I\cap II$ with constant $\mu_F(I\cap II)/\lambda(I\cap II)$. 
% Without loss of generality, we consider the signed measure $\mu_F$ on the connected subset $L$ of $I\cap II$. 
% By the connectedness of $L$, there exists a rectangle $B=[x_i,x_{i+1}]\times [y_j,y_{j+1}]$ such that $L$ is the diagonal of this rectangle. 
% Consequently, analogous to Step 1, we have 
% \[
% \mu_F(L)=\mu_F(B)=\frac{\lambda(L)}{\lambda(I\cap II)} \mu_F(I\cup II)=\frac{\mu_F(I\cup II)}{\lambda(I\cap II)}\lambda(L). 
% \]

% \underline{Step 3.} 
% Since any function with bounded Hardy–Krause variation is Riemann integral, 
% then the Lebesgue-Stieltjes integral of a function with bounded Hardy–Krause variation with respect to a continuous piecewise linear function in two dimension is well-defined and can be calculated by  
% % Therefore, 
% \bgeq
% &&\int_{[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]}\psi(x,y)dF(x,y):=\int_{[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]} \psi(x,y) d\mu_F\\
% &=& \int_{I\cap II} \psi(x,y)d \mu_F= \mu_F(I\cap II)\frac{1}{\lambda(I\cap II)} \int_{I\cap II} \psi(x,y)d \lambda\\ 
% % &=& {\color{green}???}
% % \mu_F(I\cap II)\frac{1}{\overline{a}-\underline{a}}\int_{\underline{a}}^{\overline{a}} \psi(x,y(x))dx \\
% &=& \mu_F(I\cap II) \frac{1}{\overline{a}-\underline{a}}\int_{\underline{a}}^{\overline{a}} \psi(x,y(x))dx, 
% \edeq
% where $y(x)$ is the linear function for segment $I\cap II$. 
% % {\color{green}
% % Please explain the first and second inequalities. 
% % I find Qiong's version is clearer albeit some arguments 
% % should be improved. 
% % }
% % Note that t
% The first equality is from the definition of Lebesgue-Stieltjes integral. 
% The second is based on the basic property of integral, 
% that is, $\int_{A}fdg=\int_{A_1}fdg +\int_{A_2}fdg$ such that $A_1\cap A_2=\emptyset$ and $A_1\cup A_2=A$ for any partition of $A$. 
% the third is from the fact that 
% Lebesgue-Stieltjes integral and Lebesgue integral are both defined via the following steps: 
% First, define the Lebesgue-Stieltjes/Lebesgue integral for the simple function w.r.t. the positive measure, 
% the difference is that the former uses the Lebesgue-Stieltjes measure whereas the later uses the Lebesgue measure. 
% For the signed measure, using the Jordan's decomposition of measure, that is, any signed measure can be decomposed as the difference between two positive measures. 
% Second, from our step 2, we know that the Lebesgue-Stieltjes measure is proportional to the Lebesgue measure on $I\cup II$. 
% Thus, for the simple functions, the equity holds. 
% Third, for a general function, we use a simple function to approximate it. Then the Lebesgue-Stieltjes integral and Lebesgue integral for the approximated simple function are the same as we just proved. 
% Consequently, using the dominated convergence theorem to change the position between the $\lim$ and $\int$. 
% We get the equality holds for a general function dominated by a Lebesgue integrable function. 
% % Finally, for the non-Lebesgue integrable function, the L-S integral and L-integral are both $\infty$ (the sign is based on the sign of the value $\mu_F(I\cap II)$). 
% Therefore, the equality is straightforward. 
% The last equality is based on the fact that the Lebesgue integral for a Riemann integrable function is equal to its Riemann integral. 
% % any function with bounded Hardy–Krause variation is Riemann integral. 
% This completes the proof. 
% \hfill\Box
% \end{proof}
% }





% \begin{definition}[Outer measure]

% \end{definition}
% \begin{definition}
    
% \end{definition}


% \begin{proposition}
% \label{prop-int-pl}
% Let 
% $F:[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]
% \rightarrow \R$ be a continuous 
% %piecewise linear function with only two pieces 
% function. Assume: 
% %satisfying the following two properties:
% (a) (Monotonicity) For any $x_1, x_2 \in [\underline{a}, \overline{a}], 
% y_1, y_2\in [\underline{b}, \overline{b}]$ with $x_1\leq x_2$ and $y_1\leq y_2$, 
%     $$
%     F(x_2,y_2)-F(x_2,y_1)-F(x_1,y_2)+F(x_1,y_1)\geq0;
%     $$
% (b) %$F(x,y)=\lim_{x'\downarrow x, y'\downarrow y} F(x',y')$ for all $(x,y)\in\R^2$,and 
% %Let $I, II$ be denote the two triangle regions connecting 
% $F$ is piecewise linear with two linear pieces divided   
% by line segment connecting points $A(\underline{a}, \underline{b})$ and
% $B(\overline{a}, \overline{b})$.
% (c) $\psi:[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]\rightarrow \R$ is
% a real-valued measurable function with respect to 
% a measure induced by $F$ and has finite discontinuous points.
% \noindent
% Then 
% \begin{equation}
% \label{eq-int-pl}
%     \int_{\underline{a}, \bar{a}}^{\underline{b}, \bar{b}}\psi(x,y)d F(x,y)=
%     \frac{F(\bar{a},\bar{b})-F(\underline{a},\bar{b})-F(\bar{a},\underline{b})+F(\underline{a},\underline{b})}{\bar{a}-\underline{a}}\int_{\underline{a}}^{\bar{a}} \psi(x,y(x))d x, 
% \end{equation}
% where $y(x)$ is the linear function for
% representing the 
% segment $AB$.
% %connecting points
% %$(\underline{a}, \underline{b})$ and
% %$(\overline{a}, \overline{b})$.
% \end{proposition}



% \textbf{Proof.}
% According to the discussions in 
% Section 1.3.3 \cite{AtL06},
% $F$ generates a Lebesgue-Stieltjes (outer) measure $\mu_F$ defined as
% \begin{equation*}
%     \mu_F((\underline{a}, \bar{a}]\times(\underline{b}, \bar{b}])= F(\bar{a},\bar{b})-F(\underline{a},\bar{b})-F(\bar{a},\underline{b})+F(\underline{a},\underline{b}).
% \end{equation*}
% By the definition of Lebesgue-Stieltjes integration, \begin{equation*}
%     \int_{\underline{a}, \underline{b}}^{\bar{a}, \bar{b}}\psi(x,y)d F(x,y) = \int_{(\underline{a}, \bar{a}]\times(\underline{b}, \bar{b}]}\psi(x,y)d \mu_F(x,y).
% \end{equation*}
% %{\color{green} $\mu_F$ has not been defined.}
% Let $I$ and $II$ denote the triangle regions
% in $[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]$ above (including) and below (including) 
% $AB$ respectively. Let $R=(x_1,x_2]\times(y_1,y_2]$ be a subset of $I$ or $II$.
% Then
% % Since $F$ is piecewise linear function, we use $I$ and $II$ to denote the two pieces of $F$.
% % For any $R=(x_1,x_2]\times(y_1,y_2]$ with $\underline{a}<x_1\leq x_2\leq\bar{a}$ and $\underline{b}<y_1\leq y_2\leq\bar{b}$, 
% % we have 
% \[
% F(x_2,y_2)+F(x_1,y_1)=F((x_1+x_2)/2,(y_1+y_2)/2)=F(x_2,y_1)+F(x_1,y_2),
% \]
% which implies $\mu_F(R)=0$. By setting 
% $x_1=x_2=\bar{a}$ and $y_1=y_2=\bar{b}$, 
% then we can show that 
% $\mu_F(\partial (I\cup II))=0$ where $\partial$ denotes
% the boundary of a set.
% %by letting $x_1=x_2=\bar{a}$ or $y_1=y_2=\bar{b}$.
% Let $\{a_i\}$ and $\{b_i\}$ be two sequences of monotically increasing numbers such that 
% $R_i:=(a_i,a_{i+1}]\times(b_i,b_{i+1}]\subset \inmat{int\,} I$
% and $\cup_i R_i = I$.
% % Moreover, there exist a sequence of $R_i:=(a_i,a_{i+1}]\times(b_i,b_{i+1}]\subset \inmat{int\,} I, i=1,\ldots,N$ such that $\disp{\inmat{int\,} I\subset\cup_i^N R_i}$, where the equation holds since rational numbers are dense and countable.
% By the property of outer measure, 
% $$
% \mu_F(\inmat{int\,} I)\leq 
% \sum_i\mu_F(R_i)=0.
% $$
% This shows $\mu_F(\inmat{int\,} I)=0$.
% Likewise, $\mu_F(\inmat{int\,} II)=0$.
% Consequently we have $\mu_F(I\cup II)=\mu_F(I\cap II)$.
% Next, let $x,x'\in (\underline{a},\overline{a}]$ with $x<x'$ and consider the segment $L=(x,x']\times(y(x),y(x')]\cap(I\cap II)$,
% we have
% \begin{equation*}
%     \mu_F(L) = \frac{x-\underline{a}}{\bar{a}-\underline{a}} \, \mu_F(I\cap II),
% \end{equation*}
% where $y(x)$ is the linear function representing 
% $I\cap II$ (AB).
% By dividing $AB$ into segments $L_i= (x_{i-1},x_i]\times(y(x_{i-1}),y(x_i)]\cap(I\cap II), i=1,\ldots,N$ with $x_0=\underline{a}$ and $x_N=\bar{a}$,
% % then
% \begin{equation*}
% \begin{split}
%     \int_{[\underline{a},\overline{a}]\times [\underline{b}, \overline{b}]}\psi(x,y)d F(x,y)  &= \int_{I\cap II} \psi(x,y) d\mu_F =
%     \sum_{i=1}^N \int_{L_i} \psi(x,y)d \mu_F \\
%     &= \sum_{i=1}^N \int_{x_{i-1}}^{x_{i}} \psi(x,y(x)) d x 
%     = \frac{\mu_F(I\cap II)}{\overline{a}-\underline{a}}\int_{\underline{a}}^{\overline{a}} \psi(x,y(x))d x,
% \end{split}
% \end{equation*}
% where the third equality holds since $\psi(x,y(x))$ is Riemann integrable.
% \hfill \Box


% Notice that (\ref{eq-int-pl}) holds for any piecewise linear $F$ which generates signed measure with bounded variation, see e.g., \cite{Mcs47,Ans22}.

% {\color{red}Wei: the comment is not precise. (\ref{eq-int-pl}) holds only for some special cases. }

% {\color{black}
% \subsection{Key result 1}
% {\color{red}wei: I will summarize and update the result into a lemma finally. 
% This result is essential to the paper because it not only provides a motivation for the continuous piecewise linear approximation of utility function but also pave the way to derive the tractable reformulation of the proposed model. 
% Moreover, the result can be easily extend to multivariate case. 
% }


% In this note, we consider the Lebesgue-Stieltjes integral of a function with bounded variation with respect to a continuous piecewise linear function in two dimension. 
% Specifically, let 
% $\psi:[x_i, x_{i+1}]\times [y_j, y_{j+1}]\rightarrow \R$ be a function with bounded variation; 
% $F:[x_i, x_{i+1}]\times [y_j, y_{j+1}]\rightarrow \R$ be a continuous piecewise linear function with only two pieces, see Figure \ref{fig-division}. 
%Moreover, the step-like approximation of $\psi_l$ and the piecewise linear approximation of $u$ may have the same effect if they are set up properly. 
% I intend to calculate the Lebesgue-Stieltjes integral as follows: 
% \begin{equation}
% \label{eq:LS_general}
% 	\int_{[x_i, x_{i+1}]\times [y_j, y_{j+1}]} \psi(x, y)dF(x, y). 
% \end{equation}

% Let $I$ and $II$ represent the up and lower triangles (closed) in the domain $[x_i, x_{i+1}]\times [y_j, y_{j+1}]$ respectively. 
% Then we have 
% \[
% \mu_F(\inmat{int}(I))=\mu_F(\inmat{int}(II))=0,\;\mu_F(\partial (I\cup II))=0, \;\inmat{and}\; \mu_F(I\cap II)=\mu_F(I\cup II),
% \]
% where $\mu_F$ is the signed measure induced by $F$ and 
% \[\mu_F(I\cup II)=F(x_{i+1},y_{j+1})-F(x_{i+1,j})-F(x_{i,j+1})+F(x_i,y_j).
% \]
% This is because $F$ is a linear function in domain $I$ and domain $II$ respectively. 
% let $x_i=a_0<a_1<\cdots<a_n=x_{i+1}$ 
% and $y_j=b_0<b_1<\cdots<b_m=y_{j+1}$. 
% Consequently, 
% for any smaller rectangle denoted by $R_{ij}:=[a_i,a_{i+1}]\times [b_j,b_{j+1}]$ lies in $I$ or $II$, we have 
% \[
% \mu_F(R_{ij})=F(a_i,b_j)+F(a_{i+1},b_{j+1})-F(a_i, b_{j+1})-F(a_{i+1},b_j)=0. 
% \]
% In other words, for any $(x,y)\in \inmat{int}(I)$, there exist $\delta_i,\epsilon_i>0$ for $i=1,2$ such that 
% \[
% [x-\delta_1,x+\delta_2]\times [y-\epsilon_1, y+\epsilon_2]\in \inmat{int}(I), 
% \]
% consequently, 
% \bgeq
% \mu_F((x,y))&:=&\lim_{\delta_i\downarrow 0, \epsilon_i\downarrow 0} \mu_F([x-\delta_1,x+\delta_2]\times [y-\epsilon_1, y+\epsilon_2])\\
% &=& \mu_F\left(\bigcap_{\delta_i\downarrow 0, \epsilon_i\downarrow 0}[x-\delta_1,x+\delta_2]\times [y-\epsilon_1, y+\epsilon_2]\right)\\
% &=& 0. 
% \edeq
% Moreover, the induced signed measure $\mu_F$ on $I\cap II$ is proportional to the Lebesgue measure $\lambda$ on $I\cap II$ with constant 
% $\mu_F(I\cap II)/\lambda(I\cap II)$. 
% To see this, 
% 
% Let $R_{ij}:=[a_i,a_{i+1}]\times [b_j,b_{j+1}]$. 
% Consequently, there are $nm$ smaller rectangles in the domain $I\cup II$, where 
% 
% Since $u_N$ is linear in the domain $I$ and $II$ respectively, then 
% we have for any smaller triangle denoted by $R_{ij}$ lies in the up (lower) triangle 
% 

% The Lebesgue-Stieltjes integral of a function with bounded variation with respect to a continuous piecewise linear function in two dimension can be calculated by  
% Therefore, 
% \bgeq
% &&\int_{[x_i, x_{i+1}]\times [y_j, y_{j+1}]}\psi(x,y)dF(x,y):=\int_{[x_i, x_{i+1}]\times [y_j, y_{j+1}]} \psi(x,y) d\mu_F\\
% &=& \int_{I\cap II} \psi(x,y)d \mu_F= \mu_F(I\cap II)\frac{1}{\lambda(I\cap II)} \int_{I\cap II} \psi(x,y)d \lambda\\
% &=& \mu_F([x_i, x_{i+1}]\times [y_j, y_{j+1}])\frac{1}{x_{i+1}-x_i}\int_{x_i}^{x_{i+1}} \psi(x,y(x))dx \\
% &=& [F(x_{i+1},y_{j+1})-F(x_{i+1,j})-F(x_{i,j+1})+F(x_i,y_j)] \frac{1}{x_{i+1}-x_i}\int_{x_i}^{x_{i+1}} \psi(x,y(x))dx, 
% \edeq
% where $y(x)$ is the linear function for segment $I\cap II$. 


% \begin{proposition}
% Let $\psi:[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]\rightarrow \R$ be a {\color{PineGreen} continuous} function
% {\color{red}Wei: what you need in your proof is that $\psi$ is $\Delta$-monotone instead of componentwise monotone, see the definition in \citet[Definition 2.13]{Ans22}. Moreover, the bounded variation condition is redundant. Please double check.}
% % {\color{PineGreen}Q: $\Delta$-monotonicity is more restrictive than componentwise monotone, see the comment below \citet[Definition 2.13]{Ans22}.}
% and 
% $F:[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]\rightarrow \R$ be a continuous and positive monotone piecewise linear function with only two pieces, see Figure \ref{fig-division}. 
% {\color{red}Wei: the positive monotone is redundant.}
% Then 
% % (i) The signed measure $\mu_F$ induced by $F$ is generated by 
% % \[
% % \mu_F(R)=\Delta_{x,y}[F]=F(x_{i+1},y_{j+1})-F(x_{i+1},y_{j})-F(x_{i},y_{j+1})+F(x_i,y_j)
% % \]
% % for all $R=[x_i,x_{i+1}]\times [y_j,y_{j+1}]$ with $\underline{a}\leq x_i\leq x_{i+1}\leq \overline{a}$ 
% % and $\underline{b}\leq y_j\leq y_{j+1}\leq \overline{b}$. 

% % (ii) 
% % $\mu_F(\inmat{int}(I))=\mu_F(\inmat{int}(II))=0,\;\mu_F(\partial (I\cup II))=0, \;\inmat{and}\; \mu_F(I\cap II)=\mu_F(I\cup II)$. 

% % (iii) The induced signed measure $\mu_F$ on $I\cap II$ is proportional to the Lebesgue measure $\lambda$ on $I\cap II$ with constant 
% % $\mu_F(I\cap II)/\lambda(I\cap II)$. 

% % (iv) The following equality holds
% \[\int_{[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]}\psi(x,y)d F(x,y)
% =
% \frac{F(\overline{a},\overline{b})-F(\overline{a},\underline{b})-F(\underline{a},\overline{b})+F(\underline{a},\underline{b})}{\overline{a}-\underline{a}}\int_{\underline{a}}^{\overline{a}} \psi(x,y(x))d x, 
% \]
% where $y(x)$ is the linear mapping for the main diagonal of $[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]$. 
% \end{proposition}



% \textbf{Proof.}
% According to the definition of Lebesgue-stieltjes integration 
% \cite[Section 1.3.3, Remark 2.3.3]{AtL06}, 
% \begin{eqnarray}
% \label{eq:definition-LS-integral}
%     && \int_{\underline{a},\underline{b}}^{\bar{a},\bar{b}} \psi(x,y) d F(x,y) \nonumber \\
%     &=&
%     \int_{(\underline{a},\bar{a}]\times (\underline{b},\bar{b}]} \psi(x,y) d \mu_F(x,y) \label{defi:definition-LS-R2} \\
%     &=& \sup\left\{\sum_{i=1}^N c_i \mu_F(B_i):
%     \begin{array}{ll}
%     & \sum_{i=1}^N c_i  \1_{B_i}(x,y) \leq \psi(x,y),\forall (x,y)\in (\underline{a},\bar{a}]\times (\underline{b},\bar{b}], \\
%     & B_i\cap B_j= \emptyset, \mbox{ for } i\neq j, i,j=1,\cdots,N, \bigcup_{i=1}^N B_i =(\underline{a},\bar{a}]\times (\underline{b},\bar{b}] 
% \end{array}
% \right\}, \nonumber
% \end{eqnarray}
% {\color{red}Wei: it should be the closed rectangle instead of half-open one, please double check.}
% where $\mu_F$ is the Lebesgue-Stieltjes measure generated
% by $F$, that is, 
% \bgeqn
% \label{eq:definition-LS-measure}
% \mu_F ((\underline{a},\bar{a}]\times (\underline{b},\bar{b}]) :=F(\bar{a}+,\bar{b}+)-F(\underline{a}+,\bar{b}+)-F(\bar{a}+,\underline{b}+)+F(\underline{a}+,\underline{b}+).
% \edeqn
% {\color{red}Wei: this is not the definition of the induced L-S measure by $F$, what we needed is for any rectangle to define the measure from outside approximation.}
% Since $F$ is continuous function, then the above equation reduces to 
% \bgeqn
% \label{eq:definition-LS-measure-1}
% \mu_F ((\underline{a},\bar{a}]\times (\underline{b},\bar{b}]) =F(\bar{a},\bar{b}) -F(\underline{a},\bar{b}) -F(\bar{a},\underline{b}) +F(\underline{a},\underline{b}).
% \edeqn
% Moreover, since $F$ is positive monotone, then $\mu_F ((\underline{a},\bar{a}]\times (\underline{b},\bar{b}])\geq0$. 
% {\color{red}Wei: from the positive monotone, we cannot obtain the measure is non-negative, what we needed here is that $F$ is $\Delta$-monotone. 
% Or we may use the piecewise linear property of $F$ to obtain that either $\mu_F(B)\geq 0$ for all rectangles $B$ or $\mu_F(B)\leq 0$ depending on curvature of $F$, $F$ is not conservative or conservative, respectively. Please double check. 
% We may assume w.l.o.g. $\mu_F(B)\geq 0$ for all $B$ in the following. 
% }
% Assume that 
% $$
% F=\1_{I} (\bdt) (\bm{\alpha}^{up} \bdt+\beta^{up}) + 
% \1_{II} (\bdt) (\bm{\alpha}^{lo} \bdt+\beta^{lo}),
% $$
% where $I$ and $II$ denote the upper and lower piece of $F$ respectively.
% For any $B=(a,a']\times(b,b']\subset I/(I\cap II)$, we assert that $\mu_F(B)=0$.
% This follows from 
% $$
% \mu_F(B)=\bm{\alpha}^{up} (a',b')+\beta^{up} 
% - (\bm{\alpha}^{up} (a,b')+\beta^{up}) 
% - (\bm{\alpha}^{up} (a',b)+\beta^{up}) 
% + \bm{\alpha}^{up} (a,b)+\beta^{up} =0.
% $$
% Furthermore, for any small region $B\subset I/(I\cap II)$ lie within a single piece, we can find a sequence of rectangles $R_i\subset I/(I\cap II)$, $i=1,\ldots,N$ such that 
% \begin{equation}
%     0\leq\mu_F(B)\leq\sum_{i=1}^N\mu_F(R)=0.
% \end{equation}
% For any point $(x,y)$ on the off-diagonal boundary of $F$ such as $(x_i,b)$ for $b\in(y_j,y^{j+1})$, there exists $\epsilon>0$ such that 
% $$
% \mu_F((x,y))=\lim_{\epsilon\to 0}\mu_F((x_i-\epsilon,x_i+\epsilon]\times (b-\epsilon,b+\epsilon])=0.
% $$
% Similar results can be obtained for the vertice $(x_i,y_{j+1})$ and $(x_{i+1},y_j)$.
% Hence
% $$
% \mu_F(\inmat{ri\,}I)=\mu_F(\inmat{ri\,}I)=\mu_F(\inmat{bd\,}(I\cup II))=0
% $$ 
% {\color{red}Wei: the notation ``$\inmat{ri}$" is for relative interior instead of interior, we may use the standard one ``$\inmat{int}$";  
% from $\mu_F((x,y))=0$, you cannot obtain $\mu_F(\inmat{int}(I))=0$, see e.g., the Lebesgue measure in $\R^2$. 
% Here, you may refer to my proof.}
% and 
% $$
% \mu_F(I\cup II)=\mu_F(I\cap II).
% $$
% Consequently,
% (\ref{eq:definition-LS-integral}) becomes 
% \begin{eqnarray}
%     && \int_{\underline{a},\underline{b}}^{\bar{a},\bar{b}} \psi(x,y) d F(x,y) \nonumber \\
%     && = \sup\left\{\sum_{i=1}^N c_i \mu_F(B_i):
%     \begin{array}{ll}
%     & \sum_{i=1}^N c_i  \1_{B_i}(x,y(x)) \leq \psi(x,y(x)),x\in(\underline{a},\bar{a}], \\
%     & B_i\cap B_j= \emptyset, \mbox{ for } i\neq j, i,j=1,\cdots,N, I\cap II \subset\bigcup_{i=1}^N B_i\cap(I\cup II)
% \end{array}
% \right\}. \nonumber
% \end{eqnarray}
% {\color{red}Wei: This equation does not hold in general, please replacing $\sum_{i=1}^N c_i  \1_{B_i}(x,y(x)) \leq \psi(x,y(x))$ with $\sum_{i=1}^N c_i  \1_{B_i}(x,y) \leq \psi(x,y)$. }

% Moreover, since $\psi$ is componentwise monotone, then $c_i$ equals to the value of $\psi$ on the leftmost end of $B_i\cap (I\cap II)$.
% Moreover, any partition $B_i, i=1,\ldots,N$ can be regularized to a series of rectangular partition since measure of the extra off-diagonal part is zero.
% For any uneven rectangular partition $B_l=(a_l,a_{l+1}]\times(b_l,b_{l+1}], l=0,\ldots,N-1,$ with 
% $$
% a_0=\underline{a}, a_N=\bar{a}, b_0=\underline{b}, b_N = \bar{b}, b_l = y(a_l),
% $$
% we assert there exists an even rectangular partition $B'_l=(a'_l,a'_{l+1}]\times(b'_l,b'_{l+1}], l=0,\ldots,N'-1,$ with 
% $$
% a'_0=\underline{a}, a'_{N'}=\bar{a}, b'_0=\underline{b}, b'_{N'} = \bar{b}, b'_l = y(a_l), a'_{l+1}-a'_l \leq \min\{a_{l+1}-a_l,l=1,\ldots,N\}
% $$
% such that $\sum_{i=1}^N c_i \mu_F(B_i) \leq \sum_{i=1}^{N'} c'_i \mu_F(B'_i)$, where $c_i=\psi(a_i,b_i)$ and $c'_i=\psi(a'_i,b'_i)$, and $\{a_i,i=1,\ldots,N\}\subset \{a'_i,i=1,\ldots,N'\}$.
% Assume $a_i=a'_{l_i}$ for $i=1,\ldots,N$ with $a_0=a'_{l_0}$ and $a_N=a'_{l_N}$, then
% \begin{eqnarray*}
%     \sum_{i=1}^{N} c_i \mu_F(B_i) &=& \sum_{i=1}^N \psi(a_i,b_i) \mu_F(B_i) \\
%     &=& \sum_{i=1}^N \psi(a_i,b_i) \mu_F(\cup_{j=l_{i-1}+1}^{l_i}B_j) \\
%     &\leq& \sum_{i'=1}^{N'} \psi(a'_i,b'_i) \mu_F(B'_i)= \sum_{i=1}^{N'} c'_i \mu_F(B'_i).
% \end{eqnarray*}
% Notice that 
% % $a'_i=\underline{a}+\frac{i(\bar{a}-\underline{a})}{N}$, $b'_i=y(a'_i)=\underline{b}+\frac{i(\bar{b}-\underline{b})}{N}$ for $i=1,\ldots,N'$, then
% % \begin{eqnarray*}
% %     \mu_F(B'_i) &=& \psi(a'_{i+1},b'_{i+1})-\psi(a'_i,b'_{i+1})-\psi(a'_{i+1},b'_i)+\psi(a'_i,b'_i) \\
% %     &=& 
% % \end{eqnarray*}
% $$
% \mu_F(B'_i)=\frac{\mu_F((\underline{a},\bar{a}]\times(\underline{b},\bar{b}])}{N}=\frac{F(\bar{a},\bar{b}) -F(\underline{a},\bar{b}) -F(\bar{a},\underline{b}) +F(\underline{a},\underline{b})}{N}.
% $$
% {\color{red}Wei: why can we get this without using the piecewise linear property of $F$? Please see the argument in my proof.} 
% Hence 
% \begin{eqnarray*}
%     \int_{\underline{a},\underline{b}}^{\bar{a},\bar{b}} \psi(x,y) d F(x,y) &=& \lim_{N\to\infty} \sum_{i=1}^{N} \psi(a_i,y(a_i)) \frac{F(\bar{a},\bar{b}) -F(\underline{a},\bar{b}) -F(\bar{a},\underline{b}) +F(\underline{a},\underline{b})}{N} \\
%     &=& \frac{F(\bar{a},\bar{b}) -F(\underline{a},\bar{b}) -F(\bar{a},\underline{b}) +F(\underline{a},\underline{b})}{\bar{a}-\underline{a}} \lim_{N\to\infty} \sum_{i=1}^{N} \psi(a_i,y(a_i))\frac{\bar{a}-\underline{a}}{N} \\
%     &=& \frac{F(\bar{a},\bar{b}) -F(\underline{a},\bar{b}) -F(\bar{a},\underline{b}) +F(\underline{a},\underline{b})}{\bar{a}-\underline{a}} \int_{\underline{a}}^{\bar{a}} \psi(x,y(x)) d x,
% \end{eqnarray*}
% where the last equality holds due to $\psi$ is monotone continuous function which is integrable. 
% {\color{red}Wei: the first equality does not hold, it should be ``$\geq$'' by definition. 
% Moreover, the first equality is the definition of R-S integral not L-S integral. 
% The last equality holds not because $\psi$ is monotone continuous instead it from the definition of Riemann integral and monotonicity of $\psi$. Please double check.}
% The proof is complete. \hfill\Box

% Notice that we assume $F$ is positive monotone, that is $F$ induces a positive measure.
% Same result can be obtained for the negative measures.

% \begin{equation*}
    
% \end{equation*}





% \begin{proof}
% (i) is directly from \citet[Proposition 2.3]{Ans22}. 
% \hfill\Box
% \end{proof}

% All tractable reformulations for the general case in the paper can be updated now. 

% \subsection{Key result 2}

% About reformulation binary variables and constraints. 

% \subsection{Key result 3}

% About error bound? tightness and the rate of convergence 
% }

% \newpage




% {\color{red}Wei: the last item is without ``p'', please double check.}
% \bgeq
% \psi(x,y)
% &=&F_{\boldsymbol{B}}(x,y)-F_{\boldsymbol{A}}(x,y)\\
% &=&\1_{\{\hat{x}\leq x\leq 1,\hat{y}\leq y \leq 1\}}-(1-p)\1_{\{0\leq x<1,0\leq y<1\}\cup \{x=1,0\leq y<1\}\cup \{0\leq x<1,y=1\}} -\1_{\{x=1,y=1\}}\\
% &=&\1_{[\hat{x},1]\times [\hat{y},1]}(x,y)-(1-p)\1_{[0,1]\times[0,1]\setminus (1,1)}(x,y)-p\1_{(1,1)}(x,y). 
% \edeq
% {\color{red}wei: something is missing here because $\psi(1,1)\neq 0$, please double check with the previous version. } 
% With 3 jumps at $(0,0)$, $(\hat{x},\hat{y})$, and $(1,1)$ 
%Moreover, by (\ref{eq:parts_integral}), we have 
% Let
%\begin{eqnarray}
% By simple calculation, we have
% \begin{equation*}
% \begin{split}
%     & \hat{\psi}(x,y)= \1_{[0,\hat{x})\times [0,\hat{y})}(x,y)-p\1_{[0,1)\times [0,1)}(x,y), \\
%     & \psi_1(x) = \1_{[0,\hat{x})}(x)-p\1_{[0,1)}(x),\quad
%     \psi_2(x) = \1_{[0,\hat{y})}(y)-p\1_{[0,1)}(y).
% \end{split}
% \end{equation*}
%\nonumber 
%\end{eqnarray}
% Consequently we can express ${\cal U}$ in the form of (\ref{eq-equipres}), i.e., the right hand side of (\ref{eq:ambi-U-ex}).
% \begin{eqnarray*}
% &&\int_T \bar{\psi}(x,y) d u(x,y)+ \int_{X}\psi_{1}(x)d u(x,0)+\int_{Y} \psi_{2}(y)d u(0,y)\\
% &&= \mu_u([0,\hat{x})\times [0,\hat{y}))-p\mu_u([0,1)\times [0,1))+u(\hat{x},0)-p u(1,0)+u(0,\hat{y})-p u(0,1)\\
% &&= [u(\hat{x},\hat{y})-u(0,\hat{y})-u(\hat{x},0)]-p[1-u(0,1)-u(1,0)]+u(\hat{x},0)-p u(1,0)+u(0,\hat{y})-p u(0,1)\\
% &&= u(\hat{x},\hat{y})-p.
% \end{eqnarray*}
% Therefore, we verify the proposition \ref{prop:equiv_ambiguity} using this example. 
% \end{example}



\section{Explicit piecewise linear approximation of BUPRO}
%Piecewise linear approximation}
%Computational schemes}
\label{sec:numer-methods}

%One of the main tasks of this paper is to develop efficient computational schemes for solving the %BUPRO model 
We now move on to discuss how to solve the maximin problem (\ref{eq:MAUT-robust}).
%because 
Since the true utility function
is not necessarily concave, we cannot
adopt the 
% {\color{purple}linear envelope method}
{\color{black}support function-based approach} 
% that {\color{red}is} 
used in single-attribute UPRO models (see \cite{AmD15})  
and in multi-attribute UPRO models (see \cite{zhang2020preference}).
% The main challenge is that 
% the inner utility minimization problem is infinite-dimensional.
Instead, we use the PLA
%Here we adopt 
% the piecewise linear approximation (PLA)
approach considered in \cite{GXZ21}.
The main challenge is that 
constructing a PLA of a bivariate utility function is much more complex than that of a univariate utility function. In this section, we discuss the details.


%, and consequently is very hard to solve in general. 
% In the literature of PRO in the single-variate setting, tractable reformulations are often developed by utilization of support functions, see \cite{AmD15}. 
% Moreover, one of the most interesting findings of the reformulations is that the worst-case single-variate utility function has a piecewise linear structure, see, for example, 
% \cite{AmD15,zhang2020preference}. 
% % \cite[Theorem 1]{AmD15}, 
% % \cite[Lemma 4.1]{WaX21a}. 
% This motivates us to use a two-dimensional piecewise linear function to approximate the bi-attribute utility function and then to derive the computational scheme to solve BUPRO model.  
% consider an ambiguity set of two-dimensional piecewise utility functions directly and use it to approximate $\calu$.



% \subsection{Piecewise linear approximation of bi-attribute utility function}

% {\color{red} 
% Wei: 
% Let ${\cal X}:=
% \{\underline{x}=x_0<x_1<\ldots<x_{N_1}=\bar{x}\}\subset X$ and 
% ${\cal Y}:=\{
% \underline{y}=y_0<y_1<\ldots<y_{N_2}=\bar{y}\}\subset Y$. 
% This notation or the following?

% For the consistency of the paper, I suggest using the convex combination to represent these two pieces. and then specify the parameter weights. 
% }

% {\color{red} 
% Wei: 
% Let ${\cal X}:= 
% \{\underline{x}=x_1<\ldots<x_{N_1}<x_{N_1+1}=\bar{x}\}\subset X$ and 
% ${\cal Y}:=\{
% \underline{y}=y_1<\ldots<y_{N_2}<y_{N_2+1}=\bar{y}\}\subset Y$. 
% This notation or the following?

% For the consistency of the paper, I suggest using the convex combination to represent these two pieces. And then specify the parameter weights.
% }

Let ${\cal X}:=\{x_i,i=1,\ldots,N_1\}\subset X$ and 
${\cal Y}:=\{y_j,j=1,\ldots,N_2\}\subset Y$ with 
$\underline{x}=x_1<\ldots<x_{N_1}=\bar{x}$ and $\underline{y}=y_1<\ldots<y_{N_2}=\bar{y}$.
We define $\calx\times \caly:=\{(x_i,y_j), x_i\in {\cal X},y_j\in {\cal Y}\}$ as a set of $N_1N_2$ gridpoints. 
%{\color{red}
Let $X_1:=[x_1,x_2]$, $X_i:=(x_i,x_{i+1}]$ for $i=2,\ldots,N_1-1$ and $Y_1:=[y_1,y_2]$, $Y_j:=(y_j,y_{j+1}]$ for $j=2,\cdots,N_2-1$. We divide $T$ into $(N_1-1)(N_2-1)$ mutually exclusive 
cells $T_{i,j}:=X_i\times Y_j$, $i=1,\cdots,N_1-1$, $j=1,\cdots,N_2-1$
and
$T=\bigcup_{i=1}^{N_1-1}\bigcup_{j=1}^{N_2-1} T_{i,j}$.
% where $T_{i,j}:=X_i\times Y_j$.
%} 
%{\color{green}Please double check.}
% We call {\color{red} the following} each rectangle {\color{red}a cell:} $T_{i,j}:=X_i\times Y_j=(x_i,x_{i+1}]\times(y_j,y_{j+1}]$ for $i=2,\ldots,N_1-1$ and $j=2,\ldots,N_2-1$, {\color{purple}$T_{1,j}:=[x_1,x_2]\times(y_j,y_{j+1})$} for $j=2,\ldots,N_2-1$, {\color{purple}$T_{i,1}:=[x_i,x_{i+1}]\times(y_1,y_2)$} for $i=2,\ldots,N_1-1$, and $T_{1,1}:=[x_1,x_2]\times[y_1,y_2]$ {\color{red}a cell}. 
% Then $T=X\times Y$ are divided into $(N_1-1) (N_2-1)$ mutually exclusive 
% cells as {\color{red}$T=\bigcup_{i=1}^{N_1-1}\bigcup_{j=1}^{N_2-1} T_{i,j}$.} 
% Each cell can be divided 
% into two triangles either using the {\em main diagonal} 
% connecting $(x_i,y_j)$ and $(x_{i+1},y_{j+1})$
% or using the {\em counter diagonal}
% connecting $(x_i,y_{j+1})$ and $(x_{i+1},y_{j})$, 
% see Figure~\ref{fig-division-all} for an illustration.
%The diagonals form a grid in $X\times Y$.
% {\color{blue} 
% Let $X_i=[x_{i-1},x_i]$ and 
% $Y_j = [y_{i-1},y_i]$
% } 
% {\color{red}
% for $i=1,\ldots,N_1$ and $j=1,\ldots,N_2$.
% } 
% Consider the rectangle 
% {\color{blue} 
% $X_i\times Y_j :=
% }
% [x_{i-1},x_i]\times [y_{j-1},y_j]$. 
% Following the terminology in the literature, 
% we call the line segment connecting points $(x_i,y_j)$ and $(x_{i+1},y_{j+1})$ the {\em main diagonal} 
% {\color{blue} 
% of $X_i=[x_i,x_{i+1}]$ } 
% {\color{red}(it should be $X_i\times Y_j$)}
% and 
% $(x_{i+1},y_j)$ and $(x_{i},y_{j+1})$ 
% the {\em counter diagonal}. 

There are two ways to define a continuous piecewise linear function over a cell
$T_{i,j}$.
One is to define two linear pieces over the two triangle areas separated 
using the {\em main diagonal} (Type-1 PLA) connecting $(x_i,y_j)$ and $(x_{i+1},y_{j+1})$ and the other is using the {\em counter diagonal} (Type-2 PLA) connecting $(x_i,y_{j+1})$ and $(x_{i+1},y_{j})$, see Figure~\ref{fig-division-all} for an illustration.
%connect the grid points $(x_{i-1},y_{j-1})$ and $(x_i,y_j)$ for $i=1,\ldots,N_1, j=1,\ldots,N_2$, then we obtain diagonal lines and we call them Type-1 diagonal lines.
Consider Type-1.
For any $(x,y)\in T_{i,j}$, if $\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\leq\frac{y-y_j}{x-x_i}$, then $(x,y)$ lies in the upper triangle %area 
and the upper linear piece of the utility function is defined as
\begin{equation}
\label{eq-up}
    u^{1u}_{i,j}(x,y) := \frac{y_{j+1}-y}{y_{j+1}-y_j} u_{i,j}
    +\lt(\frac{y-y_j}{y_{j+1}-y_j}-\frac{x-x_i}{x_{i+1}-x_i}\rt) u_{i,j+1}
    +\frac{x-x_i}{x_{i+1}-x_i} u_{i+1,j+1}.
\end{equation}
%Otherwise,
If $0\leq\frac{y-y_j}{x-x_i}\leq \frac{y_{j+1}-y_j}{x_{i+1}-x_i}$,
then $(x,y)$ lies in the lower triangle %area and the lower linear piece is defined as
and
\begin{equation}
\label{eq-lo}
    u^{1l}_{i,j}(x,y) := \frac{x_{i+1}-x}{x_{i+1}-x_i} u_{i,j}
    +\lt( \frac{x-x_i}{x_{i+1}-x_i}-\frac{y-y_j}{y_{j+1}-y_j} \rt) u_{i+1,j}
    +\frac{y-y_j}{y_{j+1}-y_j} u_{i+1,j+1},
\end{equation}
where $u_{i,j} := u(x_i,y_j)$, $i=1,\cdots,N_1,j=1,\cdots,N_2$.
%{\color{blue} see \cref{fig-diagall-b,fig-diagall-a}.}


% We call $ u^{1u}_{i,j}(x,y)$
% and  $u^{1l}_{i,j}(x,y)$ active pieces over 
% the respective triangle of $T_{i,j}$.
%areas.
Note that this kind of definition is based on 
the interpolation method using the utility values 
at the three vertices of the triangles. 
% {\color{blue}
% Let $u_N$ denote the piecewise linear utility function defined over $T$ with each linear piece. 
It differs
significantly from Guo and Xu \cite{GXZ21} and Hu~et~al.~\cite{hu2022distributionally}, 
where each linear piece is defined by
%a generic linear function 
a slope intercept form.
% multiplied by the indicator function of the respective active region.
%the interval 
%active area of the linear piece. This is 
%where the linear piece is active. 
We do not
%cannot
%use 
adopt their approaches because
% in multi-attribute case
% %because 
% they would require any two neighbouring active linear pieces to meet at the common boundaries $[x_i,x_{i+1}]\times \{y_j\}$
% and 
% $\{x_i\}\times [y_j,y_{j+1}]$ 
% to represent the continuity of the PLA of utility function,
% %of the active areas, which 
% % is difficult to represent. 
% which would significantly complicate the representation of PLA.}
% %piecewise linear utility function.
% {\color{Purple}
% using their approaches 
in multi-attribute case  they %would 
require the utility values of two neighbouring active linear pieces to 
coincide on the boundary of each cell, 
%This 
which would significantly complicate the representation of PLA.
% }

We now turn to discuss the construction of Type-2 PLA.
% The other one is to connect grid points $(x_i,y_{j+1})$ and $(x_{i+1},y_j)$, 
% % {\color{red}for $i=1,\ldots,N_1, j=1,\ldots,N_2$,} 
% then we obtain the Type-2 diagonal lines.
%Thus we define t
The upper 
%right and lower left 
and lower linear 
pieces 
can be defined respectively as
\begin{equation}
\label{eq-up-2}
    u_{i,j}^{2u}(x,y) := \frac{x_{i+1}-x}{x_{i+1}-x_i} u_{i,j+1}
    +\lt( \frac{x-x_i}{x_{i+1}-x_i}-\frac{y_{j+1}-y}{y_{j+1}-y_j} \rt) u_{i+1,j+1}
    +\frac{y_{j+1}-y}{y_{j+1}-y_j} u_{i+1,j}
\end{equation}
and 
\begin{equation}
\label{eq-lo-2}
    u_{i,j}^{2l}(x,y) := \frac{y-y_j}{y_{j+1}-y_j} u_{i,j+1}
    +\lt( \frac{x_{i+1}-x}{x_{i+1}-x_i}-\frac{y-y_j}{y_{j+1}-y_j} \rt) u_{i,j}
    + \frac{x-x_i}{x_{i+1}-x_i} u_{i+1,j}.
\end{equation}
% see \cref{fig-division}~(b) $\&$ (c).
% with breakpoints at the main diagonal, that is, {\color{blue} for 
% $(x,y)\in X_i\times Y_j$}
%For any $(x,y)\in X\times Y$,
% If $0\leq 
% \frac{y-y_j}{x-x_i}\leq 
% \frac{y_{j+1}-y_j}{x_{i+1}-x_i}
% $, 
%then 
% $(x,y)$ is located below the main diagonal line in which case $  u^{lo}(x,y)$
% is active. Otherwise, it lies above 
% the main diagonal line, see Figure~\ref{fig-diagall-a}.
% Likewise, 
% if $0\leq 
% \frac{y-y_j}{x-x_i}> 
% \frac{y_{j+1}-y_j}{x_{i+1}-x_i}
% $, then $(x,y)$ is located below the main diagonal line in which case $  u^{lo}(x,y)$
% is active. 
% The other is to define two linear pieces with breakpoints at counter diagonal, 
% {\color{red}HX: write down the formula}
% see Figure~\ref{fig-diagall-b}. 
Notice that the conservative property for the utility function plays an important role, that is, 
\begin{equation}
\label{eq:consevative-condition}
u_{i,j+1}+u_{i+1,j}\geq u_{i,j}+u_{i+1,j+1} \quad \forall i=1,\cdots,N_1-1,j=1,\cdots,N_2-1.
\end{equation}
If the conservative property holds at each cell, 
then the graph of the Type-2 piecewise linear function majorizes 
that of the Type-1, see Figure~\ref{fig-diagall-b}.
In this case, the diagonal line 
connecting points $1$ and $4$ looks like a ``valley", 
%while ``valley" in division (b), 
while the segment connecting points $2$ and $3$ looks like a ``ridge".
% \vspace{-0.5cm}
%{\color{red} To be commented. }
\begin{figure}[!ht]
  \vspace{-0.4cm}
  \centering
   \hspace{-1em}
  \subfigure[\scriptsize{Main $\&$ counter diagonals}]{ %lines}]{
    \label{fig-division-all} %% label for first subfigure
    \includegraphics[width=0.3\linewidth]{division.pdf}
  }
%   \hspace{-1em}
  \subfigure[\scriptsize{Conservative conditions hold}]{
    \label{fig-diagall-b} %% label for second subfigure
    % \includegraphics[width=1.4in]{rectangle-all-2.pdf}
    % }
    \includegraphics[width=0.3\linewidth]{Rectangle-all-11.pdf}
    }
  \hspace{-1em}
  \subfigure[\scriptsize{Conservative conditions fail}]{
    \label{fig-diagall-c} %% label for first subfigure
    % \includegraphics[width=1.4in]{rectangle-all-1.pdf}
       \includegraphics[width=0.3\linewidth]{Rectangle-all-2.pdf}
    }
    \vspace{-1em}
  \captionsetup{font=footnotesize}
  \caption{ 
  (a) The red line is the main diagonal and
  the blue line is the counter diagonal.
  (b) When the conservative condition holds, 
  the graph of the 
  Type-2 piecewise linear function (PLF) (blue and green planes) lies above 
  that of the Type-1 PLF (represented by
dotted lines).
  %piecewise linear function. 
  (c) When the conservative condition fails,
  %Otherwise, the situation is the opposite.
   the graph of the 
  Type-1 
  %piecewise linear function
  PLF (orange and green planes) lies above 
  that of the Type-2 PLF (represented by dotted lines).
  %piecewise linear function. 
  }
  \vspace{-0.5cm}
  \label{fig-division} %% label for entire figure
\end{figure}
% \begin{figure}[!ht]
% \vspace{-0.5cm}
%   \centering
%   \subfigure[Conservative conditions hold]{
%     %\label{fig-division-all} %% label for first subfigure
%     \includegraphics[width=1.9in]{Rectangle-all-111.pdf}
%   }
%   \hspace{-1em}
%   \subfigure[\scriptsize{Conservative conditions fail}]{
%   % \label{fig-diagall-b} %% label for second subfigure
%     % \includegraphics[width=1.4in]{rectangle-all-2.pdf}
%     % }
%     \includegraphics[width=2.1in]{Rectangle-all-222.pdf}
%     }
%   \hspace{-2em}
% %   \subfigure[\scriptsize{Conservative conditions fail}]{
% %   %  \label{fig-diagall-c} %% label for first subfigure
% %     % \includegraphics[width=1.4in]{rectangle-all-1.pdf}
% %       \includegraphics[width=2.1in]{Rectangle-all-2.pdf}
% %     }
%     \vspace{-1em}
%   \captionsetup{font=footnotesize}
%   \caption{}
%   \end{figure}
%   {\color{blue}Sainan:I am not sure if the additional two figures are necessary.}

\begin{definition}[Ambiguity set of piecewise linear utility functions]
\label{def-ambi-N}
Let $\scru_N\subset\scru$ be the set of all Type-1 (or Type-2) piecewise linear utility functions 
%with breaking points on $\calx\times \caly$.
% with two linear pieces 
over $T_{i,j}$
for $i=1,\cdots,N_1-1, j=1,\cdots,N_2-1$.
% Let $\scru_N\subset\scru$ be a set of piecewise linear utility functions with breaking points on $\calx\times \caly$.
Define the ambiguity set of piecewise linear utility functions  as
%satisfying the moment-type conditions:
% \begin{equation}
% \begin{split}
%     \calu_{N}:= \lt\{u_{N}\in \scru_{N} \lt|\; 
%     \int_{T} \rt.\rt.
%     & \overline{\psi}_m(x,y) d u_{N}(x,y)
%      + \int_{X} \psi_{1,m}(x) d u_{N}(x,\underline{y}) \\
%     & \lt. + \int_{Y} \psi_{2,m}(y) d u_{N}(\underline{x},y) \leq c_m, 
%     \inmat{\;for\;} m=1,\ldots,M \rt\}.
% \end{split}
% \end{equation}
\begin{equation}
\label{eq:U_N-PLA}
    \mathcal{U}_N:=\lt\{ u_N\in \mathscr{U}_N \,:\,
    %\lt|\; 
    \int_{T} u_N(x,y) d\psi_l(x,y)
    \leq c_l, \; l=1,\ldots,M  \rt\}.
\end{equation}
\end{definition}


We propose to use $\calu_{N}$ to approximate $\calu$.
Since $\scru_{N}\subset\scru$, then $\calu_{N}\subset\calu$.
Conversely, for any $u\in\calu$, 
%one 
we can construct a piecewise linear utility function $u_{N}\in\scru_{N}$ by connecting the utility values at 
% some of the 
% adjacent 
gridpoints $(x_i,y_j)$, {\color{black}$i=1,\cdots,N_1$, $j=1,\cdots,N_2$}. 
% {\color{red}For the one-dimensional case the comment is fine, while for the high-dimensional case it is not. Please double check.}
In general, $u_{N}\notin\calu_{N}$ but the inclusion may hold 
% {\color{purple}under}
{\color{black}in} some special cases.
% {\color{RoyalBlue}
% such as $\psi_l$ is simple function which takes constant value over $T_{i,j}$ for $l=1,\ldots,M$.
% }
%The following proposition states this. 

\begin{proposition}
\label{prop-uti-N}
Let $\psi_l(x,y)$ be a simple function over $T$ for $l=1,\cdots,M$,
which takes constant values over cells $T_{i,j}$
for $i=1,\cdots,N_1-1, j=1,\cdots,N_2-1$.
%with possible jump points at 
%$\calx\times \caly$. 
% $x_i,y_j$ for $i=1,\ldots,N_1$, $j=1,\ldots,N_2$
% % {\color{red}It might be better to use $\calx\times \caly$}. 
Then for any $u\in\calu$, there exists a function $u_{N}\in\scru_{N}$ with $u_N(x_i,y_j)=u(x_i,y_j)$ for $i=1,\ldots,N_1$, $j=1,\ldots,N_2$ such that $u_{N}\in\calu_{N}$. 
Specifically, for $(x,y)\in T$
%[x_i,x_{i+1}]\times[y_j,y_{j+1}]
% X_i\times Y_j, i=1,\ldots,N_1-1,j=1,\ldots,N_2-1$, 
such $u_{N}$ can be constructed as Type-1 or Type-2 piecewise linear functions defined as
\begin{equation}
\label{eq-utility-N-1}
\begin{split}
    & \inmat{(Type-1)\quad} u_{N}(x,y) = \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \1_{T_{i,j}} (x,y) \times \\
    & \lt[
    u^{1u}_{i,j}(x,y) \1_{\lt(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty\rt)} \lt(\frac{y-y_j}{x-x_i}\rt) 
    + u^{1l}_{i,j}(x,y) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} \lt(\frac{y-y_j}{x-x_i}\rt) \rt]
\end{split}
\end{equation}
or 
\begin{equation}
\label{eq-utility-N-2}
\begin{split}
    & \inmat{(Type-2)\quad}  u_{N}(x,y) = \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \1_{T_{i,j}} (x,y) \times \\
    & \lt[u^{2u}_{i,j}(x,y) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} \lt(\frac{y_{j+1}-y}{x-x_i}\rt) +
    u^{2l}_{i,j}(x,y) 
    \1_{\lt(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty\rt)} \lt(\frac{y_{j+1}-y}{x-x_i}\rt) \rt],
\end{split}
\end{equation}
where $u^{1u}_{i,j}(x,y)$, $u^{1l}_{i,j}(x,y)$, $u^{2u}_{i,j}(x,y)$, and $u^{2l}_{i,j}(x,y)$ are defined as in (\ref{eq-up})-(\ref{eq-lo-2}),
and $\1_A(\cdot)$ denotes the indicator function of set $A$.

%,(\ref{eq-lo}),(\ref{eq-up-2}) and (\ref{eq-lo-2}).
\end{proposition}



% \begin{proof}
% %If
%  Since $\psi_l$ takes a constant value over $T_{i,j}$,
% %is a simple function over $T$, 
% then its induced measure $\mu_{\psi_l}^*$\footnote{{\color{blue}
% Let
% ${\cal C}_2:=\{I_1\times I_2: I_1,I_2\in {\cal C}\}$,
% where  ${\cal C}:=\{(a,b]:-\infty \leq a\leq b< \infty\}\cup \{(a,\infty): -\infty \leq a <\infty\}$.
% For $I_1=(a_1,a_2]$,
% $I_2=(b_1,b_2]$, 
% $-\infty<a_1,a_2,b_1,b_2<\infty$,
% set
% $\mu_{\psi}(I_1\times I_2):=\psi(a_2+,b_2+)-\psi(a_2+,b_1+)-\psi(a_1+,b_2+)-\psi(a_1+,b_1+)$,
% where $\psi(a+,b+):=\lim_{a'\downarrow a, b'\downarrow b} \psi(a',b')$.
% $\mu_{\psi}(I_1\times I_2)$ on unbounded sets in ${\cal C}_2$ is defined by the limiting procedure: 
% $\mu_{\psi}(I_1\times I_2)=\lim_{n\rightarrow \infty}\mu_{\psi}((I_1\times I_2)\cap J_n)$, 
% where $J_n=(-n,n]\times (-n,n]$.
% % When $\psi:[\underline{x},\bar{x}]\times [\underline{y},\bar{y}]$ is non-decreasing ans satisfies that $\psi(\bar{x},\bar{y})+\psi(\underline{x},\underline{y})\geq \psi(\bar{x},\underline{y})+\psi(\underline{x},\bar{y})$.
% The measure $\mu_{\psi}^*$ induced by $\mu_{\psi}$  is defined by
% $\mu_{\psi}^*(A):=\inf\{\sum_{n=1}^{\infty} \mu(A_n): \{A_n\}_{n\geq 1}\subset {\cal C}_2, A\subset \cup_{n\geq 1} A_n\}$
% % $\mu_{\psi}((\underline{x},\bar{x}]\times (\underline{y},\bar{y}]) = \psi(\bar{x},\bar{y})+\psi(\underline{x},\underline{y})- \psi(\bar{x},\underline{y})-\psi(\underline{x},\bar{y})$ 
% (see \cite[Section~1.3.3]{AtL06}).}
% } 
% is nonzero over the interior of $T_{i,j}$.
% % not equal to zero 
% %only in some vertices of $T_{i,j}$. 
% % This is directly from the definition of induced signed measure, see, e.g.,\citet{Ans22}. 
% % \[
% % \mu_{\psi_l}(B)=\lim_{\delta_1,\delta_2;\epsilon_1,\epsilon_2\downarrow 0} \psi_l(x_i-\delta_1,y_j-\epsilon_1)+\psi_l(x_i-\delta_1,y_j-\epsilon_1)\psi_l(x_i-\delta_1,y_j-\epsilon_1)\psi_l(x_i-\delta_1,y_j-\epsilon_1)
% % \]
% % for all $B=[x_i,x_{i+1}]\times [y_j,y_{j+1}]$ 
% % with $\underline{a}\leq x_i\leq x_{i+1}\leq \overline{a}$ 
% % and $\underline{b}\leq y_j\leq y_{j+1}\leq \overline{b}$.  
% Consequently, for $l=1,\ldots,M$,
% \begin{equation}
% \label{eq-int-u-u-N}
% \begin{split}
%     \int_{T} u_N(x,y)d\psi_l(x,y)
%     &= \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} u_N(x_i,y_j) \mu_{\psi_l}^*(x_i,y_j)\\
%     &= \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} u(x_i,y_j) \mu_{\psi_l}^*(x_i,y_j)
%     =\int_{T} u(x,y)d\psi_l(x,y) \leq c_l.
% \end{split}
% \end{equation}
% The proof is complete.
% % All three equations lead to 
% % \begin{equation}
% % \label{eq-int-u-u-N}
% %     \int_{T} u_N(x,y)d\psi_l(x,y) = \int_{T} u(x,y)d\psi_l(x,y) \leq c_m, m=1,\ldots,M.
% % \end{equation}
% \hfill\Box
% \end{proof}

The proof is deferred to Appendix~\ref{app:proof-uN}.
Using $\calu_{N}$, we propose to solve the BUPRO problem (\ref{eq:MAUT-robust}) 
%can be approximated 
by solving the following approximate problem: 
\begin{equation}
\label{eq:MAUT-robust-N}
\inmat{(BUPRO-N)} \quad
\vt_N:=\max_{\bdz\in Z}\min_{u_{N}\in\calu_{N}} \bbe_P[u_{N}(\bdf(\bdz,\bdxi))].
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \SN{
% \begin{lemma}
% Let $\Delta^{up}_{i,j}$ and $\Delta_{i,j}^{lo}$ be the upper and lower triangles from the rectangle $(x_i,x_{i+1}]\times(y_j,y_{j+1}]=\Delta^{up}_{i,j}\cup \Delta_{i,j}^{lo}$
% for $i=1,\cdots,N_1$, $j=1,\cdots,N_2$ .
% Let $u_N$ be a piecewise linear function on every piece of the triangle,
% and 
% \bgeq
% u_N(x,y)=
% \sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} 
% \lt(
% \left\langle \bda^{up}_{i,j},(x,y)^T \right\rangle +b^{up}_{i,j} \rt) \1_{\Delta_{i,j}^{up}}(x,y) 
% + \lt( \left\langle \bda^{lo}_{i,j},(x,y)^T \right\rangle +b^{lo}_{i,j}\rt) \1_{\Delta_{i,j}^{lo}}(x,y),
% \edeq
% with
% \begin{equation*}
%     \left\langle \bda^{up}_{i,j},(x_i,y_j)^T \right\rangle +b^{up}_{i,j} =
%     \left\langle \bda^{lo}_{i,j},(x_i,y_j)^T \right\rangle +b^{lo}_{i,j}
% \end{equation*}
% and 
% \begin{equation*}
%     \left\langle \bda^{up}_{i,j},(x_{i+1},y_{j+1})^T \right\rangle +b^{up}_{i,j} = \left\langle \bda^{lo}_{i,j},(x_{i+1},y_{j+1})^T \right\rangle +b^{lo}_{i,j}.
% \end{equation*}
% Then we have the induced measure $\mu_{u_N}$ associated with piecewise linear utility function $u_N$ is given by
% % \bgeq
% % \mu_{u_N}\left((x_i,x_{i+1}]\times(y_i,y_{i+1}]\right)
% % = \frac{1}{2}
% % \left(\left(a_{1,i,j}^{up}-\underline{x}^{2i j}\right)(x_{i+1}-x_i)
% % +\left(\underline{x}^{2i j}-\underline{y}^{1i j}\right)(y_{j+1}-y_j) \right).
% % \edeq
% \begin{equation*}
%     \mu_{u_N}\left((x_i,x_{i+1}]\times(y_i,y_{i+1}]\right)
%     = \frac{1}{2} \left\langle \bda^{up}_{i,j}-\bda^{lo}_{i,j},(x_{i+1},y_j)^T-(x_i,y_{j+1})^T\right\rangle.
% \end{equation*}
% \end{lemma}
% \begin{figure}[!ht]
% \begin{minipage}[t]{\linewidth}
% \centering
% \includegraphics[width=4in]{two-triangles.pdf}
% \end{minipage}%
% \hspace{0cm} 
% \caption{The two triangles of the rectangle $(x_i,x_{i+1}]\times (y_j,y_{j+1}]$}
% \end{figure}

% \noindent{\bf Proof}.
% By the definition of the induced measure $\mu_{u_N}$
% \begin{eqnarray*}
%     && \mu_{u_N}\left((x_i,x_{i+1}]\times(y_i,y_{i+1}]\right) \\
%     && = u_N(x_{i+1},y_{j+1})-u_N(x_i,y_{j+1})-u_N(x_{i+1},y_j)+u_N(x_i,y_j)\\
%     && = \lt( \left\langle \bda^{up}_{i,j},(x_{i+1},y_{j+1})^T \right\rangle +b^{up}_{i,j} \rt) 
%     -\lt( \left\langle \bda^{up}_{i,j},(x_i,y_{j+1})^T \right\rangle +b^{up}_{i,j} \rt)  \\
%     && \hspace{10em} - \lt( \left\langle \bda^{lo}_{i,j},(x_{i+1},y_j)^T \right\rangle +b^{lo}_{i,j} \rt)-\lt( \left\langle \bda^{lo}_{i,j},(x_i,y_j)^T \right\rangle +b^{lo}_{i,j} \rt) \\
%     && = \lt(\bda^{up}_{1,i,j}-\bda^{lo}_{1,i,j}\rt)(x_{i+1}-x_i),
% \end{eqnarray*}
% and likewise
% \begin{eqnarray*}
%     && \mu_{u_N}\left((x_i,x_{i+1}]\times(y_i,y_{i+1}]\right) \\
%     && = u_N(x_{i+1},y_{j+1})-u_N(x_i,y_{j+1})-u_N(x_{i+1},y_j)+u_N(x_i,y_j)\\
%     && = \lt( \left\langle \bda^{lo}_{i,j},(x_{i+1},y_{j+1})^T \right\rangle +b^{lo}_{i,j} \rt) 
%     -\lt( \left\langle \bda^{up}_{i,j},(x_i,y_{j+1})^T \right\rangle +b^{up}_{i,j} \rt)  \\
%     && \hspace{10em} - \lt( \left\langle \bda^{lo}_{i,j},(x_{i+1},y_j)^T \right\rangle +b^{lo}_{i,j} \rt)-\lt( \left\langle \bda^{up}_{i,j},(x_i,y_j)^T \right\rangle +b^{up}_{i,j} \rt) \\
%     && = \lt(\bda^{lo}_{2,i,j}-\bda^{up}_{2,i,j}\rt)(y_{j+1}-y_j).
% \end{eqnarray*}
% We obtain
% \begin{equation}
% \label{eq-measure}
%     \mu_{u_N}\left((x_i,x_{i+1}]\times(y_i,y_{i+1}]\right)
%     =\frac{1}{2}
%     \left( (\bda^{up}_{1,i,j}-\bda^{lo}_{1,i,j})(x_{i+1}-x_i)
%     +(\bda^{lo}_{2,i,j}-\bda^{up}_{2,i,j})(y_{j+1}-y_j) \right).
% \end{equation}
% The proof is complete.
% \hfill $\Box$
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Next we write the piecewise linear utility in an another linear form which will be used in constructing the tractable formulation in the concave utility case.
% Let $(x,y)$ be a point in the upper simplex of the rectangle $[x_i,x_{i+1}]\times[y_j,y_{j+1}]$ with the utility value $u(x,y)$ and $(a^{up}_{i,j},b^{up}_{i,j},c^{up}_{i,j})$ 
% % and $(a^{lo}_{i,j},b^{lo}_{i,j},c^{lo}_{i,j})$ 
% be the normal vector of this simplex.
% % in rectangle $[x_i,x_{i+1}]\times[y_j,y_{j+1}]$.
% Then we have 
% \begin{equation}
%     c^{up}_{i,j}(u(x,y)-u_{i,j})+a^{up}_{i,j}(x-x_i)+b^{up}_{i,j}(y-y_j)=0,
% \end{equation}
% Moreover, the normal vector on the upper simplex can be solved by the following equations, 
% \begin{equation}
% \begin{split}
%     & a^{up}_{i,j} (x_i-x_i)+b^{up}_{i,j} (y_{j+1}-y_j)+c^{up}_{i,j} (u_{i,j+1}-u_{i,j})=0, \\
%     & a^{up}_{i,j} (x_{i+1}-x_i)+b^{up}_{i,j} (y_{j+1}-y_{j+1})+c^{up}_{i,j} (u_{i+1,j+1}-u_{i,j+1})=0.
% \end{split}
% \end{equation}
% Note the length of normal vectors has no effect on the value of the utility function, thus we set $c^{up}_{i,j}=1$ and obtain
% $$
% a_{i,j}^{up} = \frac{u_{i,j+1}-u_{i+1,j+1}}{x_{i+1}-x_i}, \; b_{i,j}^{up} = \frac{u_{i,j}-u_{i,j+1}}{y_{j+1}-y_j}.
% $$
% Hence the upper simplex can be written as 
% \begin{equation}
%     u(x,y) = u_{i,j}-\frac{u_{i,j+1}-u_{i+1,j+1}}{x_{i+1}-x_i} (x-x_i)-\frac{u_{i,j}-u_{i,j+1}}{y_{j+1}-y_j} (y-y_j)
% \end{equation}
% or alternatively,
% \begin{equation*}
%     u^{up}(x,y) = \lt\langle \bm{\beta}_{i,j}^{up},(x-x_i,y-y_i) \rt\rangle + d_{i,j}^{up}, 
% \end{equation*}
% where 
% $$
% \bm{\beta}^{up}_{i,j} = \lt(\frac{u_{i+1,j+1}-u_{i,j+1}}{x_{i+1}-x_i}, \frac{u_{i,j+1}-u_{i,j}}{y_{j+1}-y_j} \rt), \; 
% d^{up}_{i,j} = u_{i,j}+\frac{u_{i,j+1}-u_{i+1,j+1}}{x_{i+1}-x_i} x_i+\frac{u_{i,j}-u_{i,j+1}}{y_{j+1}-y_j} y_j.
% $$ 
% Likewise, the lower simplex of rectangle $[x_i,x_{i+1}]\times[y_j,y_{j+1}]$ can be written as 
% \begin{equation*}
%     u^{lo}(x,y) = \lt\langle \bm{\beta}_{i,j}^{lo},(x-x_i,y-y_i) \rt\rangle + d_{i,j}^{lo}, 
% \end{equation*}
% where 
% $$
% \bm{\beta}_{i,j}^{lo} = \lt(\frac{u_{i+1,j}-u_{i,j}}{x_{i+1}-x_i}, \frac{u_{i+1,j+1}-u_{i+1,j}}{y_{j+1}-y_j} \rt), \;
% d^{lo}_{i,j} = u_{i,j}+\frac{u_{i,j}-u_{i+1,j}}{x_{i+1}-x_i} x_i+\frac{u_{i+1,j}-u_{i+1,j+1}}{y_{j+1}-y_j} y_j.
% $$

% {\color{blue}WW: For a rectangle in $\R^2$, there are two ways to divide it into two triangles. 
% So, a direct issue is to analyze its impact on the piecewise linear approximation and consequently the optimal solution of our decision making problem. 
% We may emphasize this point. 
% }


% \begin{figure}
%   \centering
%   \subfigure[]{
%     \label{fig-division-a} %% label for first subfigure
%     \includegraphics[width=2.8in]{division-a.pdf}
%   }
%   \subfigure[]{
%     \label{fig-division-b} %% label for second subfigure
%     \includegraphics[width=2.8in]{division-b.pdf}
%   }
%   \caption{The two possible divisions into nonoverlapping
% simplices for a rectangle}
%   \label{fig-division} %% label for entire figure
% \end{figure}



% \begin{figure}
%   \centering
%   \subfigure[]{
%     \label{fig-diagall-a} %% label for first subfigure
%     \includegraphics[width=2.8in]{diagall-conserv.pdf}
%   }
%   \subfigure[]{
%     \label{fig-diagall-b} %% label for second subfigure
%     \includegraphics[width=2.8in]{diagall-nonconserv.pdf}
%   }
%   {\color{red} HX: Please update the figure as discussed yesterday.}
%   \caption{The two possible piecewise linear approximations}
%   \label{fig-diagall} %% label for entire figure
% \end{figure}


% Note that the rectangle $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$ is divided by the upward diagonal line by connecting vertices $1$ and $4$ in the above discussion, which is shown in Figure \ref{fig-division} (a).
% We can also divide a rectangle into non-overlapping simplices by connecting vertices $2$ and $3$ in Figure \ref{fig-division} (b).
% In this case, the piecewise linear utility in the rectangle $[x_i,x_{i+1}]\times[y_j,y_{j+1}]$ is 
% \begin{equation}
% \label{eq-utility-N}
%     u_N(x,y) = 
%     u^{up}(x,y) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} \lt(\frac{y_{j+1}-y}{x-x_i}\rt) +
%     u^{lo}(x,y) \1_{\lt(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty\rt)} \lt(\frac{y_{j+1}-y}{x-x_i}\rt),
% \end{equation}
% where 
% \begin{equation}
% \label{eq-up-2}
%     u^{up}(x,y) := \frac{x_{i+1}-x}{x_{i+1}-x_i} u(x_i,y_{j+1})
%     +\lt( \frac{x-x_i}{x_{i+1}-x_i}-\frac{y_{j+1}-y}{y_{j+1}-y_j} \rt) u_{i+1,j+1}
%     +\frac{y_{j+1}-y}{y_{j+1}-y_j} u(x_{i+1},y_j)
% \end{equation}
% and 
% \begin{equation}
% \label{eq-lo-2}
%     u^{lo}(x,y) := \frac{y-y_j}{y_{j+1}-y_j} u_{i,j+1}
%     +\lt( \frac{x_{i+1}-x}{x_{i+1}-x_i}-\frac{y-y_j}{y_{j+1}-y_j} \rt) u_{i,j}
%     + \frac{x-x_i}{x_{i+1}-x_i} u_{i+1,j}.
% \end{equation}
% % Notice that the division (b) in Figure \ref{fig-division} is more complex than division (a) because the formulation of the utility function (i.e. the coefficients of the convex combination) depends on the utility order of points $(x_i,y_{j+1})$ and $(x_{i+1},y_j)$.
% % Moreover, the values of $u(x_i,y_{j+1})$ and $u(x_{i+1},y_j)$ are unknown in prior.





In the rest of the section, we discuss numerical schemes for solving the BUPRO-N problem. 
% We divide our discussion into two parts: increasing concave utility function case and increasing utility function case with concave conditional utility functions. 
To this end, we need to restrict 
our discussion to the case that $\bdxi$ is discretely distributed.
\begin{assumption}
\label{assu-discrete}
    $P$ %follows 
    is a discrete distribution with $P(\bdxi=\bdxi^k)=p_k$ for $k=1,\ldots,K$.
\end{assumption}

Under Assumption~\ref{assu-discrete},
% the above assumption, 
we can write the BUPRO-N model
% (\ref{eq:MAUT-robust-N}) 
as 
\begin{equation}
\label{eq:MAUT-robust-N-dis}
    \max_{\bdz\in Z}\min_{u_{N}\in\calu_{N}} \sum_{k=1}^K p_k u_{N}(\bdf(\bdz,\bdxi^k)).
\end{equation}
The maximin problem can be decomposed into an inner minimization problem
\begin{equation}
\label{eq:MAUT-robust-N-dis-min}
    v_{N}(\bdz):=\min_{u_{N}\in\calu_{N}} \sum_{k=1}^K p_k u_{N}(\bdf(\bdz,\bdxi^k))
\end{equation}
and an outer maximization problem $ \vt_N=\max_{\bdz\in Z} v_{N}(\bdz)$.
% \begin{equation}
%     \label{eq:MAUT-robust-N-dis-max}
% \end{equation}
Our strategy is to formulate (\ref{eq:MAUT-robust-N-dis-min}) as an LP %linear program 
and solve 
% \cref{eq:MAUT-robust-N-dis}
the outer maximization problem
by 
%the
derivative-free (Dfree) methods.
We will discuss 
%the convergence of 
the performance of PLA 
in Section~\ref{sec-errorbound}.
Note that if $\bdxi$ is continuously distributed, then we may regard (\ref{eq:MAUT-robust-N-dis}) as
a
discrete
approximation to the BUPRO-N model. 



We now move on to 
derive the tractable formulation of 
%calculating inner minimization problem 
(\ref{eq:MAUT-robust-N-dis-min}) 
%of BUPRO-N model with 
%the case that 
when $\scru$ is a class of componentwise non-decreasing and Lipschitz continuous utility functions which is concave in 
%every 
each single variate. 
%attribute.

\begin{assumption}
\label{A:concave-in-x-and-y}
For any $u\in\scru$,
% defined as in \cref{defi:ambguity-set},
the single-variate
%attribute 
utility functions $u(x,\hat{y})$ and $u(\hat{x},y)$ are concave at any instantiations $\hat{y}\in Y$ and $\hat{x}\in X$.
\end{assumption}

The 
% single-attribute 
single-variate
utility functions 
%of $u$ 
in Assumption~\ref{A:concave-in-x-and-y} 
%at all variables 
can be regarded as non-normalized
%  one-dimensional 
single-attribute utility functions.
%and t
The concavity condition is used widely  in the literature of expected utility theory, 
which implies the 
DM is risk-averse for each 
%single
individual  attribute (\cite{tsanakas2003risk}).
% We can also assume that the single-attribute utilities are convex or that each single-variate
% %attribute 
%  utility reflects 
% %the
%  different attitudes of the DM towards each attribute.
% Note that this assumption is not strictly required, even is not necessarily needed when we discuss the error bound of PLA.
%HX I delete the sentence because readers would ask why we make the assumption otherwise

\begin{assumption}
\label{assu-lip}
Each function $u\in\scru$ is Lipschitz continuous over $T$
with the modulus being bounded by $L$ in the sense that 
\begin{equation}
\label{eq-Lip-condition}
    |u(x,y)-u(x',y')|\leq L \|(x-x',y-y')\|_1 \quad \forall \, (x,y),(x',y')\in T.
\end{equation}
\end{assumption}

The normalization condition and the Lipschitz condition imply that  $L\geq 1/(\bar{x}-\underline{x}+\bar{y}-\underline{y})$. This Lipschitz condition means that the DM's utility change is not drastic at any level of the attributes. It is satisfied
when $u$ is locally Lipschitz continuous over an open set containing $T$.





% Assumption~\ref{assu-lip} is needed in Section~\ref{sec-errorbound} when we derive the error bound of the ambiguity sets resulting from the PLA in the
% %increasing
% non-decreasing
% utility case. 
% We present it here as some of the conditions 
% %that should 
% %to 
% will be included in the reformulation of the BUPRO-N model.
% In the case that the function is piecewise linear, 
% the constant $L$ gives an upper bound for the infinite norm of the tangent vector
% %in 
% over each cell.
% % and (\ref{eq-Lip-condition}) holds
% % the Lipschitz continuous condition is reduced to Lipschitz continuity of single-attribute utility.
% The assumption is satisfied by many practically interesting utility functions
% such as the MRA utility functions.} 
% {\color{blue}
% (see e.g.
% %in 
% %{\color{red}
% $u(x,y)=\frac{1-e^{-\gamma(x+\beta y)}}{1-e^{-\gamma(1+\beta)}}$ 
% %for 
% $\gamma,\beta>0$ in \cite[Example 1 (b)]{abbas2009multiattribute}).
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Notice that in the case when $\psi_l$ is not a simple function for $l=1,\cdots,M$, the LS integrals in (\ref{eq:U_N-PLA}) cannot be calculated directly. Fortunately, we can tackle the issue by swapping the positions
between $u_N$ and $\psi_l$.
% This means that
% we have to reformulate the representation of 
% the ambiguity set so that the 
% the approximate UPRO model (\ref{eq:MAUT-robust-N-dis}) is solvable. 
% Fortunately, since $u_N$ is a piecewise linear function, we are able to recast 
% (\ref{eq:U_N-PLA}) as 
% The next proposition justifies
Specifically, 
% we can use 
% %the 
% integration by parts and the PLA approach to % calculate the LS integrals.
using multivariate integration by parts for the %Lebesgue-Stieltjes 
LS
integrals (see, e.g., \cite{young1917multiple} and \cite{Ans22}), 
we can rewrite 
%the integrals in \ref{eq:ambiguity_set} may be alternatively represented as follows:
% \begin{eqnarray}
% \label{eq:parts_integral}
% \notag
% &&\int_T u(x,y)d\psi_l(x,y)\\ \notag
% && = u(\underline{x},\underline{y})[\psi_l]_{\underline{x},\underline{y}}^{\bar{x},\bar{y}}+\int_T [\psi_l]_{x,y}^{\bar{x},\bar{y}} d u(x,y)+ \int_X [\psi_l]_{x,\underline{y}}^{\bar{x},\bar{y}}d u(x,\underline{y})+\int_{Y} [\psi_l]_{\underline{x},y}^{\bar{x},\bar{y}} d u(\underline{x},y)\\ \notag
% && = \int_T [\psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},y)+\psi_l(x,y)]d u(x,y)\\ \notag
% && \quad +\int_X [\psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},\underline{y})+\psi_l(x,\underline{y})]d u(x,\underline{y})\\ 
% && \quad +\int_{Y} [\psi_l(\bar{x},\bar{y})-\psi_l(\underline{x},\bar{y})-\psi_l(\bar{x},y)+\psi_l(\underline{x},y)]d u(\underline{x},y) \nonumber \\
% &&=:
% \int_T \hat{\psi}_m(x,y) d u(x,y)+ \int_{X}\psi_{1,m}(x)d u(x,\underline{y}) +\int_{Y} \psi_{2,m}(y)d(\underline{x},y).
% \end{eqnarray}
%Then the ambiguity set defined in 
%the 
ambiguity set 
(\ref{eq:ambiguity_set}) as
\begin{equation}
\label{eq:u_N-int}
\begin{split}
    \calu_N=\left\{u_N \in \scru_N: \int_T \right.
    \hat{\psi}_l(x,y) & d u_N(x,y)  + \int_{X}\psi_{1,l}(x)d u_N(x,\underline{y})\\ 
    &\left. +\int_{Y} \psi_{2,l}(y)d u_N(\underline{x},y) \leq c_l,\;l=1,\ldots,M  \right\},
\end{split}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%can be equivalently written as 
% \begin{equation}
% \label{eq-equipres}
% \begin{split}
%     \calu:=\left\{u \in \scru: \int_T \right.
%     \hat{\psi}_l(x,y) &d u(x,y)  + \int_{X}\psi_{1,l}(x)d u(x,\underline{y})\\
%     &\left. +\int_{Y} \psi_{2,l}(y)d u(\underline{x},y) \leq c_l,\;l=1,\ldots,M  \right\},
% \end{split}
% \end{equation}
 where $\hat{\psi}_l(x,y):=\psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},y)+\psi_l(x,y)$, 
 $\psi_{1,l}(x):=\psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},\underline{y})+\psi_l(x,\underline{y})$, and 
 $\psi_{2,l}(y):=\psi_l(\bar{x},\bar{y})-\psi_l(\underline{x},\bar{y})-\psi_l(\bar{x},y)+\psi_l(\underline{x},y)$ for $l=1,\ldots,M$. 
 Likewise, we can reformulate the ambiguity set ${\cal U}$  defined in (\ref{eq:ambiguity_set})
 as 
 \begin{equation}
\label{eq-equipres}
\begin{split}
    \calu=\left\{u \in \scru: \int_T \right.
    \hat{\psi}_l(x,y) &d u(x,y)  + \int_{X}\psi_{1,l}(x)d u(x,\underline{y})\\
    &\left. +\int_{Y} \psi_{2,l}(y)d u(\underline{x},y) \leq c_l,\;l=1,\ldots,M  \right\}.
\end{split}
\end{equation}
 % This can be easily obtained by using 
% The reformulation swaps the positions between $\psi_l$ and $u$ but will facilitate us to do the computations and analysis, we will come back to this in Sections 3 and 4.
In the case that 
the decision making problem has only one variable (e.g. $y$ disappears),  the first term and the third term at the left hand side of the inequalities 
will disappear and consequently 
the two-dimensional conditions defined as in (\ref{eq:u_N-int}) reduce to the one-dimensional moment-type conditions in \cite{GXZ21}.
%Back to 
%We now return to 
% Moreover, when the DM's preference is elicited via 
%  pairwise comparison questions,
%  %defined as in (\ref{eq:ambi-U-ex}). If 
% e.g.,
% \begin{equation}
% \label{eq-def-lotterypair}
%     \bdca=\lt\{
%     \begin{array}{ll}
%     (\underline{x},\underline{y}) & \inmat{\;w.p.\;} 1-p, \\
%     (\bar{x},\bar{y}) & \inmat{\;w.p.\;} p,
%     \end{array} 
%     \rt.
%     \inmat{\quad and \quad}
%     \bdcb=(\hat{x},\hat{y}) \inmat{\; w.p.\;} 1,
% \end{equation}
% then 
% %it can be easily calculated 
% we have 
% %that 
% $\hat{\psi}(x,y)= \1_{[0,\hat{x})\times [0,\hat{y})}(x,y)-p\1_{[0,1)\times [0,1)}(x,y)$,
% $\psi_1(x) = \1_{[0,\hat{x})}(x)-p\1_{[0,1)}(x)$, and
% $\psi_2(x) = \1_{[0,\hat{y})}(y)-p\1_{[0,1)}(y)$.
% In this case, 
% %Moreover,
% %Then
% %Consequently, by the continuity of $u$ and $u(0,0)=0$, we have
% % \begin{equation*}
% % \begin{split}
% %     &\int_T \hat{\psi}(x,y) d u(x,y)+ \int_{X}\psi_{1}(x)d u(x,0)+\int_{Y} \psi_{2}(y)d u(0,y) \\
% %     %&
% %     %= \mu_u([0,\hat{x})\times [0,\hat{y}))-p\mu_u([0,1)\times [0,1))+u(\hat{x},0)-p u(1,0)+u(0,\hat{y})-p u(0,1) \\
% %     &= (u(\hat{x},\hat{y})-u(0,\hat{y})-u(\hat{x},0))-p(1-u(0,1)-u(1,0))+u(\hat{x},0)-p u(1,0)+u(0,\hat{y})-p u(0,1) \\
% %     &= u(\hat{x},\hat{y})-p,
% % \end{split}
% % \end{equation*}
% $$
% \int_T \hat{\psi}(x,y) d u_N(x,y)+ \int_{X}\psi_{1}(x)d u_N(x,0)+\int_{Y} \psi_{2}(y)d u_N(0,y) =u_N(\hat{x},\hat{y})-p.
% $$
% which 
% %illustrates 
% implies the equivalence between
% % (\ref{eq:U_N-PLA}) and (\ref{eq:u_N-int})
% (\ref{eq:ambiguity_set}) and (\ref{eq-equipres}).
% Thus the ambiguity set of the piecewise linear utility functions can be alternatively represented as  
% %constructed as
% \begin{equation}
% \label{eq:u_N-int}
% \begin{split}
%     \calu_N:=\left\{u_N \in \scru_N: \int_T \right.
%     \hat{\psi}_l(x,y) & d u_N(x,y)  + \int_{X}\psi_{1,l}(x)d u_N(x,\underline{y})\\ 
%     &\left. +\int_{Y} \psi_{2,l}(y)d u_N(\underline{x},y) \leq c_l,\;l=1,\ldots,M  \right\}.
% \end{split}
% \end{equation}
% %In the following 
% From 
% %the 
% computational perspectives,
% the main difficulty lies in 
% the first integral as the second and the third integrals are one-dimensional. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The next proposition states how
%, we discuss the calculation 
% give a reformulation 
%for the first
the two-dimensional LS integrals 
in (\ref{eq:u_N-int})
may be converted into one-dimensional 
Riemann integrals. 
The proof is deferred to Appendix~\ref{app:proof-LS}.

\begin{proposition}
\label{prop-int-pl}
Let 
$F: 
[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]
\rightarrow \R$ be a 
%bounded and 
continuous 
%piecewise linear function with only two pieces 
function.
% {\color{blue}and $\mu^*_F$ be the LS measure generated by 
% $F$, ${\cal M}_{\mu^*_F}:=\{A\subset T: A \inmat{ \it is }\mu^*_F\inmat{\it -measurable}\}$}. 
Assume: 
%satisfying the following two properties:
% (a) (Monotonicity) 
% {\color{red}We may assume $F$ is not conservative or conservative here because Only two cases exist for such PLA functions.} 
% For any $x_1, x_2 \in [\underline{a}, \overline{a}], 
% y_1, y_2\in [\underline{b}, \overline{b}]$ with $x_1\leq x_2$ and $y_1\leq y_2$, 
%     $$
%     F(x_2,y_2)-F(x_2,y_1)-F(x_1,y_2)+F(x_1,y_1)\geq0;
%     $$
% (b) %$F(x,y)=\lim_{x'\downarrow x, y'\downarrow y} F(x',y')$ for all $(x,y)\in\R^2$,and 
%Let $I, II$ denote the two triangle regions connecting 
(a) $F$ is a piecewise linear function
with two linear pieces divided   
by line segment connecting points $A(\underline{a}, \underline{b})$ and
$B(\bar{a}, \bar{b})$;
(b) $\psi$ is
a real-valued measurable function 
%with respect to 
w.r.t.~a measure induced by $F$,
% {\color{blue}
% $\psi:T
% \rightarrow \R$ is a measurable function on LS measure space $(T,{\cal M}_{\mu^*_F},\mu^*_F)$}, 
and is 
 Riemann integrable 
over the line segment connecting points $A$ and $B$. 
%$\psi:[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]\rightarrow \R$ is
%a real-valued measurable function with respect to 
%a measure induced by $F$ and has 
% {\color{red} finite discontinuous points???
% HX: Please clarify why you mean.
% }.\\ 
% {\color{red}In (\ref{eq:ambiguity_set}), $\psi$ has bounded HK variation, while here $\psi$ has finite discontinuous points. Please double check.}
% {\color{blue} Q: I don't think bounded HK variation is necessary. What we need here is Riemann integrable, and bounded variation of Vitali when we define the ambiguity set. I didn't find the content about bounded HK variation. Please enlighten me. }
Then 
\begin{equation}
\label{eq-int-pl}
    \int_{\underline{a}, \bar{a}}^{\underline{b}, \bar{b}}\psi(x,y)d F(x,y)=
    \frac{F(\bar{a},\bar{b})-F(\underline{a},\bar{b})-F(\bar{a},\underline{b})+F(\underline{a},\underline{b})}{\bar{a}-\underline{a}}\int_{\underline{a}}^{\bar{a}} \psi(x,y(x))d x, 
\end{equation}
where $y(x)$ is the linear function 
representing the 
segment $AB$.
\end{proposition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Now
With this, we are ready to
% we can 
reformulate the inner minimization problem (\ref{eq:MAUT-robust-N-dis-min})
% \begin{equation}
% \label{eq-innmin}
%     v_N(\bdz)=\min_{u_{N}\in\calu_{N}} \sum_{k=1}^K p_k u_{N}(\bdf(\bdz,\xi^k)),
% \end{equation}
as an LP.
%linear programming problem 
%and we give a simple proof.
% The proposition below states this 
% which is a convex program in the case that $\calu_{N}$ is a compact and convex set.
% Recall that each utility function $u_{N}\in\calu_{N}$ can be written as 
% \begin{equation*}
%     u_{N}(x,y)=\sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \1_{X_i\times Y_j} (\bdf(\bdz,\bdxi^k)) 
%     \lt[ u^{lo} \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} 
%     \lt( \frac{y-y_j}{x-x_i} \rt) + u^{up} \1_{\lt( \frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty \rt)} \lt(\frac{y-y_j}{x-x_i}\rt) \rt],
% \end{equation*}
% where $u^{up}(x,y)$ and $u^{lo}(x,y)$ are defined as in (\ref{eq-up}) and (\ref{eq-lo}).


\begin{proposition}
Under Assumptions \ref{assu-original}-\ref{assu-lip}, the inner minimization problem (\ref{eq:MAUT-robust-N-dis-min}) with Type-1 PLA can be reformulated as the following LP:
%linear programming problem:
% By using the Type-1 approximate utility defined in \cref{prop-uti-N}, we may reformulate problem 
% % (\ref{eq-innmin})  
% (\ref{eq:MAUT-robust-N-dis-min})
% as
% \begin{subequations}
% \label{eq:PRO-N-reformulate}
% \begin{eqnarray}
%     & \disp{ \min_{\substack{u_{i,j}, i=1,\ldots,N_1\\j=1,\ldots,N_2}} } & \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \1_{X_i\times Y_j} (\bdf^k) 
%     \lt[ u^{lo}(f_1^k,f_2^k) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} 
%     \lt( \frac{f_2^k-y_j}{f_1^k-x_i} \rt) \rt. \nonumber \\
%     && \lt. + u^{up}(f_1^k,f_2^k) \1_{\lt( \frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty \rt)} \lt(\frac{f_2^k-y_j}{f_1^k-x_i}\rt) \rt] \label{eq-traform-obj} \\
%     & \st & 
%     \sum_{i=1}^{N_1}\sum_{j=1}^{N_2} \frac{u_{i,j+1}-u_{i+1,j+1}-u_{i,j}+u_{i+1,j}}{x_{i+1}-x_i} \int_{x_i}^{x_{i+1}} \psi(x,y(x)) d x  \nonumber \\
%     && + 
%     \sum_{i=1}^{N_1} \frac{u_{i+1,1}-u_{i,1}}{x_{i+1}-x_i} \int_{x_i}^{x_{i+1}} \psi_{1,m}(x) d x \nonumber \\
%     && +
%     \sum_{j=1}^{N_2} \frac{u_{1,j+1}-u_{1,j}}{y_{j+1}-y_j} \int_{y_j}^{y_{j+1}} \psi_{2,m}(y) d y \leq c_m, m=1,\ldots,M, \label{eq-traform-paircom}\\
%     && \frac{u_{i+1,j}-u_{i,j}}{x_{i+1}-x_i} \leq \frac{u_{i,j}-u_{i-1,j}}{x_i-x_{i-1}}, i=2,\ldots,N_1-1, j=1,\ldots,N_2, \label{eq-traform-concave1} \\
%     && \frac{u_{i,j+1}-u_{i,j}}{y_{j+1}-y_j} \leq \frac{u_{i,j}-u_{i,j-1}}{y_j-y_{j-1}}, i=1,\ldots,N_1, j=2,\ldots,N_2-1, \label{eq-traform-concave2} \\
%     && u_{i+1,j}-u_{i,j}\leq L(x_{i+1}-x_i), i=1,\ldots,N_1-1, j=1,\ldots,N_2, \label{eq-traform-lip1} \\
%     && u_{i,j+1}-u_{i,j}\leq L(y_{j+1}-y_j), i=1,\ldots,N_1,j=1,\ldots,N_2-1, \label{eq-traform-lip2} \\ 
%     && u_{i+1,j}\geq u_{i,j}, i=1,\ldots,N_1-1,j=1,\ldots,N_2, \label{eq-traform-mon1} \\
%     && u_{i,j+1}\geq u_{i,j}, i=1,\ldots,N_1,j=1,\ldots,N_2-1, \label{eq-traform-mon2} \\
%     && u_{i,j}+u_{i+1,j+1} \leq
%     u_{i,j+1}+u_{i+1,j}, i=1,\ldots,N_1-1,j=1,\ldots,N_2-1, \label{eq-traform-conservative} \\
%     && u_{1,1}=0, 
%     % \label{eq-traform-norm0} \\
%     % &&
%     u_{N_1,N_2}=1, \label{eq-traform-norm1}
% \end{eqnarray}
% \end{subequations}
\begin{subequations}
\label{eq:PRO-N-reformulate}
\begin{align}
    \disp{ \min_{{\bm u}}} \quad
    & \sum_{k=1}^K p_k \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \1_{T_{i,j}} (\bdf^k) 
    \lt[ u_{i,j}^{1l}(\bdf^k) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} 
    \lt( \frac{f_2^k-y_j}{f_1^k-x_i} \rt) \rt. \nonumber \\
    & \lt. + u_{i,j}^{1u}(\bdf^k) \1_{\lt( \frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty \rt)} \lt(\frac{f_2^k-y_j}{f_1^k-x_i}\rt) \rt] \label{eq-traform-obj} \\
    \st \quad
    % \hspace{2.6em} 
    & 
    \sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \frac{u_{i,j+1}-u_{i+1,j+1}-u_{i,j}+u_{i+1,j}}{x_{i+1}-x_i} \int_{x_i}^{x_{i+1}} \hat{\psi}_l (x,y(x)) d x  \nonumber \\
    & + 
    \sum_{i=1}^{N_1-1} \frac{u_{i+1,1}-u_{i,1}}{x_{i+1}-x_i} \int_{x_i}^{x_{i+1}} \psi_{1,l}(x) d x \nonumber \\
    & +
    \sum_{j=1}^{N_2-1} \frac{u_{1,j+1}-u_{1,j}}{y_{j+1}-y_j} \int_{y_j}^{y_{j+1}} \psi_{2,l}(y) d y \leq c_l, l=1,\ldots,M, \label{eq-traform-paircom}\\
    & \frac{u_{i+1,j}-u_{i,j}}{x_{i+1}-x_i} \leq \frac{u_{i,j}-u_{i-1,j}}{x_i-x_{i-1}}, i=2,\ldots,N_1-1, j=1,\ldots,N_2, \label{eq-traform-concave1} \\
    & \frac{u_{i,j+1}-u_{i,j}}{y_{j+1}-y_j} \leq \frac{u_{i,j}-u_{i,j-1}}{y_j-y_{j-1}}, i=1,\ldots,N_1, j=2,\ldots,N_2-1, \label{eq-traform-concave2} \\
    & u_{i+1,j}-u_{i,j}\leq L(x_{i+1}-x_i), i=1,\ldots,N_1-1, j=1,\ldots,N_2, \label{eq-traform-lip1} \\
    & u_{i,j+1}-u_{i,j}\leq L(y_{j+1}-y_j), i=1,\ldots,N_1,j=1,\ldots,N_2-1, \label{eq-traform-lip2} \\ 
    & u_{i+1,j}\geq u_{i,j}, i=1,\ldots,N_1-1,j=1,\ldots,N_2, \label{eq-traform-mon1} \\
    & u_{i,j+1}\geq u_{i,j}, i=1,\ldots,N_1,j=1,\ldots,N_2-1, \label{eq-traform-mon2} \\
    & u_{i,j}+u_{i+1,j+1} \leq
    u_{i,j+1}+u_{i+1,j}, i=1,\ldots,N_1-1,j=1,\ldots,N_2-1, \label{eq-traform-conservative} \\
    & u_{1,1}=0, u_{N_1,N_2}=1, \label{eq-traform-norm1}
\end{align}
\end{subequations}
where {\color{black}${\bm u}:={\rm vec}\left((u_{i,j})_{1\leq i\leq N_1}^{1\leq j\leq N_2}\right)=(u_{1,1},\ldots,u_{N_1,1},\ldots,u_{1,N_2},\ldots,u_{N_1,N_2})^T\in \R^{N_1N_2}$,}
$\bdf^k:=\bdf(\bdz,\\ \bdxi^k)=(f_1^k,f_2^k)$  with $f_1^k:=f_1(\bdz,\bdxi^k)$, $f_2^k:=f_2(\bdz,\bdxi^k)$,
$\hat{\psi}_l$,
$\psi_{1,l}$
$\psi_{2,l}$ are defined as in (\ref{eq:u_N-int}),
$u_{i,j}^{1l}$ and $u_{i,j}^{1u}$ are defined as in (\ref{eq-up}) and (\ref{eq-lo}).
% {\color{red}Please highlight the structures of $u_{i,j}^{lo}$ and $u_{i,j}^{up}$ in the proposition. (3.1)-(3.2)?}
\end{proposition}


\noindent 
\textbf{Proof.}
Using the Type-1 PLA as defined in (\ref{eq-utility-N-1}), we may reformulate the objective as (\ref{eq-traform-obj}).
Moreover,
constraint (\ref{eq-traform-paircom})
% {\color{red}requires}
% characterizes
represents
the integral inequalities conditions defined as in (\ref{eq:u_N-int}) from
Proposition~\ref{prop-int-pl}.
Constraints (\ref{eq-traform-concave1}) and (\ref{eq-traform-concave2})
% represents the concavity of conditional utility functions. 
% comply with
%represents
characterize
%the 
concavity of single-variate utility functions
assumed in
Assumption~\ref{A:concave-in-x-and-y}.
Constraints (\ref{eq-traform-lip1}) and (\ref{eq-traform-lip2}) %require to
%characterize
capture
the Lipschitz continuity for the utility function.
Constraints (\ref{eq-traform-mon1}) and (\ref{eq-traform-mon2}) 
% require 
%characterize
reflect
componentwise monotonicity of utility functions.
Constraint (\ref{eq-traform-conservative}) %represents 
states
the conservative property.
Constraint (\ref{eq-traform-norm1})
% requires
%characterizes
is
the normalization condition for the utility function.
\hfill \Box



\begin{remark}
\label{rem:BUPRO-DF}
(i)
 Note that (\ref{eq:PRO-N-reformulate}) is reformulated based on the Type-1 PLA.
%approximate utility.
% is the linear program for linear or nonlinear $\bdf$. 
A similar formulation can be obtained for Type-2 PLA. 
%piecewise linear utility functions. 
By solving (\ref{eq:PRO-N-reformulate}),
we can 
%will be able to 
%find 
obtain the worst-case utility function $u_N^{\rm worst}$.
% for fixed $\bdz$. 
The information on $u_N^{\rm worst}$ 
gives us 
%an indication 
a guidance 
as to how the inner minimization problem 
approximates the true expected utility.
%minimization problem.
%HX: I was wrong, no change! 
The problem size depends on the number of gridpoints and is independent of 
the scenarios of $\bdxi$.



% since the objective and constraints are all linear in $u_{i,j}$ for $i=1,\ldots,N_1, j=1,\ldots,N_2$.
% This linear structure of this problem motivates us to use alternating iterative minimization method or derivative-free method for solving \cref{eq:MAUT-robust-N-dis}.
% In the case that alternating iterative minimization method is used, restricting $T$ to being a convex and compact set guarantees that the optimal $\bdz^*$ is contained in $Z$, see \cite{GuX21}.
% In the case that using derivative-free method, the maximin problem \cref{eq:MAUT-robust-N-dis} is solved as a single maximization problem,
% \cref{eq:MAUT-robust-N-dis-max}
% where each function evaluation $v_N(\bdz)$ is down to solving a linear program.
% Note also that no matter how large $K$ is, the number of the pieces of the worst-case piecewise linear utility function in \cref{eq:PRO-N-reformulate} is $2(N_1-1)(N_2-1)$.
% In the case that $\psi_l$ is defined as in (\ref{eq:ambi-U-ex}) with
% \begin{equation*}
%     \bdca_l=\lt\{
%     \begin{array}{ll}
%     (\underline{x},\underline{y}) & \inmat{with probability\;} 1-p^l, \\
%     (\bar{x},\bar{y}) & \inmat{with probability\;} p^l,
%     \end{array} 
%     \rt.
%     \inmat{\quad and \quad}
%     \bdcb_l=(x_{i_l},y_{i_l}) \inmat{\; with probability\;} 1,
% \end{equation*}
% constraint (\ref{eq-traform-paircom}) reduces to $u(x_{i_l},y_{i_l})\leq p^l$.
% , where $\bdcb_l=(x_{i_l},y_{i_l})$ corresponds to sure lottery, and $\bdca$ takes value $(\underline{x},\underline{y})$ with probability $1-p^l$ and $$.
%is a certain lottery.
Note also that the single-attribute utility functions are assumed to be concave in Assumption~\ref{A:concave-in-x-and-y}.
% this assumption facilitate us to write constraints (\ref{eq-traform-concave1}) and (\ref{eq-traform-concave2}). 
This is in accordance with single-attribute decision making in the risk-averse case.
%We add this assumption 
%to restrain the shape of the approximate utility function and this assumption facilitates us to write the tractable formulation.
Likewise, we can also assume that one (or both) single-attribute utility at any instantiation is (are) convex.
In that case, it suffices to input constraints (\ref{eq-traform-concave1}) or (and) (\ref{eq-traform-concave2}) in the reverse direction.


The Lipschitz continuity is also reflected by the Type-1 PLA  (\ref{eq-utility-N-1}) which can be formulated as
%follows:
%, we can also reformulate the Type-1 approximate utility function on each triangle as 
% By the definition of piecewise linear utility functions defined in \ref{eq-up} and \ref{eq-lo}, we can also reformulate them into
\begin{equation*}
    u^{1u}_{i,j}(x,y) = \lt( \frac{u_{i+1,j+1}-u_{i,j+1}}{x_{i+1}-x_i}, \frac{u_{i,j+1}-u_{i,j}}{y_{j+1}-y_j} \rt) (x,y)^T +b^{1u}_{i,j}
\end{equation*}
and
\begin{equation*}
    u^{1l}_{i,j}(x,y) = \lt( \frac{u_{i+1,j}-u_{i,j}}{x_{i+1}-x_i}, \frac{u_{i+1,j+1}-u_{i+1,j}}{y_{j+1}-y_j} \rt) (x,y)^T+b^{1l}_{i,j},
\end{equation*}
where $b^{1u}_{i,j}$
% =\frac{x_i(u_{i,j+1}-u_{i+1,j+1})}{x_{i+1}-x_i}-\frac{y_{j+1} u_{i,j}+y_j u_{i,j+1}}{y_{j+1}-y_j}
and $b^{1l}_{i,j}$ are constants representing the 
% the rest terms. 
intercepts respectively.
%Then t
The Lipschitz continuity defined as in Assumption~\ref{assu-lip} corresponds to 
$$
\max\lt\{ \lt\|\lt( \frac{u_{i+1,j+1}-u_{i,j+1}}{x_{i+1}-x_i}, \frac{u_{i,j+1}-u_{i,j}}{y_{j+1}-y_j} \rt)\rt\|_{\infty},\lt\|\lt( \frac{u_{i+1,j}-u_{i,j}}{x_{i+1}-x_i}, \frac{u_{i+1,j+1}-u_{i+1,j}}{y_{j+1}-y_j} \rt)\rt\|_{\infty}  \rt\}\leq L,
$$ 
{\color{black} over each cell $T_{i,j}$,} which implies constraints (\ref{eq-traform-lip1}) and (\ref{eq-traform-lip2}).


(ii)
%Notice that w
%We define t
The aforementioned PLA utility function $u_N$ is constructed either in Type-1 or in Type-2.
%diagonal lines.
%We can also
%lso define 
It is possible to allow both.
%construct the PLA utility function 
Specifically, we can define 
% without specifying the same diagonal lines in all cells, we call it mixed-type PLA which can be formulated as 
\begin{equation*}
\begin{split}
    & u_N (x,y)  =\sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \1_{T_{i,j}}(x,y) \times \\
    & \lt[
    h_{i,j}
    \lt(
    u^{1u}_{i,j}(x,y) \1_{\lt(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty\rt)} \lt(\frac{y-y_j}{x-x_i}\rt) 
    + u^{1l}_{i,j}(x,y) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} \lt(\frac{y-y_j}{x-x_i}\rt) \rt) \rt.\\
    & \lt. + (1-h_{i,j})
    \lt(u^{2u}_{i,j}(x,y) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} \lt(\frac{y_{j+1}-y}{x-x_i}\rt) +
    u^{2l}_{i,j}(x,y) 
    \1_{\lt(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty\rt)} \lt(\frac{y_{j+1}-y}{x-x_i}\rt) \rt)
    \rt],
\end{split}
\end{equation*}
where 
% $u_{i,j}^{1,up}$, 
% $u_{i,j}^{1,lo}$,
% $u_{i,j}^{2,up}$ $u_{i,j}^{2,lo}$ 
% refer to the upper and lower linear pieces of the two types,
%refers to the upper piece in Type-1 and so other correspondence, and 
$\{h_{i,j}, i=1,\ldots,N_1-1,j=1,\ldots,N_2-1\}$ is a set of binary variables 
%i.e.,
%$h_{i,j}$ can take only two values, either
taking values $0$ or $1$.
In the case that $h_{i,j}=1$, Type-1 PLA is invoked over $T_{i,j}$.
Otherwise, Type-2 PLA is active.
Obviously, this approach significantly extends 
the class of piecewise linear utility functions 
and consequently the optimal value of the inner minimization problem is smaller than that of Type-1 and Type-2.
% With this definition of $u_N$, the ambiguity set would be enlarged with many utility functions with a different diagonal line over each cell, such that the optimal value of the inner minimization problem decrease and so the outer maximization problem, and it would be smaller than the corresponding optimal value with Type-1 or Type-2 PLA utility function.
%As for 
With regard to the tractable formulation, we will have $(N_1-1)(N_2-1)$ additional binary variables 
% the difference from this reformulation is on the objective and variables, that is, $(N_1-1)(N_2-1)$ binary variables will be added in the inner minimization problem.
% Thus, 
and the inner minimization becomes an MILP.


%{\color{blue}
% Let 
% $
% I_1:=\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right],\quad 
% I_2:=\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right).
% $
% (iii) The objective function %$u_N(f(\bdz,\bdxi))$
% $\sum_{k=1}^K p_k u_{N}(\bdf(\bdz,\bdxi^k))$
(iii) %The piecewise linear utility function 
Type-1 PLA
$u_N(\bdf(\bdz,\bdxi^k))$
can be alternatively represented in the following form:
%written as 
\begin{equation*}
    u_{N}(\bdf(\bdz,\bdxi^k)) = \sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1}  \left[ \alpha_{i,j}^k(\bdz) u_{i,j}
    +\alpha_{i,j+1}^k(\bdz) u_{i,j+1}
    +\alpha_{i+1,j}^k(\bdz) u_{i+1,j}
    +\alpha_{i+1,j+1}^k(\bdz) u_{i+1,j+1}\right],
\end{equation*}
where
\begin{align*}
% \label{eq:coefficient-obj}
    &\alpha_{i,j}^k(\bdz):=\mathds{1}_{T_{i,j}}(\bdf^k) \lt[ 
     \frac{y_{j+1}-f_2^k}{y_{j+1}-y_j} \mathds{1}_{\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]}\left(\frac{f_2^k-y_j}{f_1^k-x_i} \right) 
    +  \frac{x_{i+1}-f_1^k}{x_{i+1}-x_i} \mathds{1}_{\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)}  \left(\frac{f_2^k-y_j}{f_1^k-x_i}\right) \rt], 
    \nonumber \\
    &\alpha_{i,j+1}^k(\bdz):= \left(\frac{f_2^k-y_j}{y_{j+1}-y_j}-\frac{f_1^k-x_i}{x_{i+1}-x_i}\right) \mathds{1}_{T_{i,j}}(\bdf^k) \mathds{1}_{\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]} \left(\frac{f_2^k-y_j}{f_1^k-x_i}\right) , \\
    & \alpha_{i+1,j}^k(\bdz):= \left( \frac{f_1^k-x_i}{x_{i+1}-x_i}-\frac{f_2^k-y_j}{y_{j+1}-y_j}\right) \mathds{1}_{T_{i,j}}(\bdf^k) \mathds{1}_{\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)}\left(\frac{f_2^k-y_j}{f_1^k-x_i}\right), \nonumber \\
    & \alpha_{i+1,j+1}^k(\bdz):=\mathds{1}_{T_{i,j}} (\bdf^k) \lt[  \frac{f_1^k-x_i}{x_{i+1}-x_i} \mathds{1}_{\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]}\left(\frac{f_2^k-y_j}{f_1^k-x_i}\right) 
    +  \frac{f_2^k-y_j}{y_{j+1}-y_j} \mathds{1}_{\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)}\left(\frac{f_2^k-y_j}{f_1^k-x_i}\right) \rt]. \nonumber
\end{align*}
% for $i=1,\cdots,N_1-1,j=1,\cdots,N_2-1$.
For fixed $\bdz$,
%we can see clearly that $u_N(f(\bdz,\bdxi))$
%Note that the objective function $\theta(\bdz)$ of problem \cref{eq:PRO-N-reformulate} can be reformulated as 
% \bgeq
% \theta(\bdz)= \alpha_{i,j}(\bdz) u_{i,j}
% +\alpha_{i,j+1}(\bdz) u_{i,j+1}
% +\alpha_{i+1,j}(\bdz) u_{i+1,j}
% +\alpha_{i+1,j+1}(\bdz) u_{i+1,j+1}.
% \edeq
% && \theta(\bdz):=u_{i,j} \left(\mathds{1}_{T_{i,j}}(\bdf^k) \mathds{1}_{\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]} \left(\frac{f_2^k-y_j}{f_1^k-x_i} \right) \frac{y_{j+1}-f_2^k}{y_{j+1}-y_j} 
% + \mathds{1}_{T_{i,j}}(\bdf^k) \mathds{1}_{\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)} \left(\frac{f_2^k-y_j}{f_1^k-x_i}\right) \frac{x_{i+1}-f_1^k}{x_{i+1}-x_i}\right)\nonumber \\
% && +u_{i,j+1} \left(\mathds{1}_{T_{i,j}}(\bdf^k) \mathds{1}_{\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]} \left(\frac{f_2^k-y_j}{f_1^k-x_i}\right) \left(\frac{f_2^k-y_j}{y_{j+1}-y_j}-\frac{f_1^k-x_i}{x_{i+1}-x_i}\right) \right)\\
% &&+ u_{i+1,j} \left(\mathds{1}_{T_{i,j}}(\bdf^k)\mathds{1}_{\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)}\left(\frac{f_2^k-y_j}{f_1^k-x_i}\right)\left( \frac{f_1^k-x_i}{x_{i+1}-x_i}-\frac{f_2^k-y_j}{y_{j+1}-y_j}\right)\right)\nonumber \\
% && +u_{i+1,j+1} \left(\mathds{1}_{T_{i,j}} (\bdf^k) \mathds{1}_{\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]} \left(\frac{f_2^k-y_j}{f_1^k-x_i}\right) \frac{f_1^k-x_i}{x_{i+1}-x_i} 
% + \mathds{1}_{T_{i,j}}(\bdf^k) \mathds{1}_{\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)} \left(\frac{f_2^k-y_j}{f_1^k-x_i}\right) \frac{f_2^k-y_j}{y_{j+1}-y_j}\right). \nonumber
% \edeqn
%For fixed $\bdz$,
% $f_1^k=f_1(\bdz,\bdxi^k),f_2^k=f_2(\bdz,\bdxi^k)$ are fixed,
%and
%we know 
the inner minimization problem (\ref{eq:MAUT-robust-N-dis-min}) 
% in \cref{eq:PRO-N-reformulate}
is also an LP with this $u_N$.
Let $L({\bm u},{\bm \lambda};\bdz)$ be the Lagrange function of the inner problem
%of  \cref{eq:PRO-N-reformulate},
and ${\bm \lambda}$ be the vector of Lagrange multipliers.
Then the inner problem can be reformulated as 
$\min_{{\bm u}}\max_{{\bm \lambda}} L({\bm u},{\bm \lambda};\bdz)$.
%Although we can get the dual of this LP and then 
In this way, we can reformulate the maximin problem (\ref{eq:PRO-N-reformulate}) as a single maximization problem
$
\max_{\bdz\in Z,{\bm \lambda}} \{\min_{{\bm u}}L({\bm u},{\bm \lambda};\bdz)\}.
$
% {\color{red}
Unfortunately, 
this is not helpful 
%the single problem is not solvable 
since the coefficients of
$u_{i,j}$,
$u_{i,j+1}$,
$u_{i+1,j}$,
$u_{i+1,j+1}$
are composed of  indicator functions of 
%unknown 
$\bdf^k$.
% }
In the next section,
we will propose a new approach to 
handle the issue 
%overcome this difficulty 
and extend the discussions to 
%$3$-attribute and 
the multi-attribute case.

\end{remark}



\section{Implicit piecewise linear approximation of UPRO -- from bi-attribute to multi-attribute case}
%MILP perspective and extension to multi-attribute case}
\label{sec:multi-atrribute}

% {\color{red}
% In section 3, for fixed $z$, 
% we have obtained the LP reformulation (3.16) for the inner minimization problem to the problem (3.11) and then we use the derivative-free method to solve the outer maximization problem. 
% If this section still considers the MILP reformulation for the inner minimization problem, 
% this would be a bit superficial. 
% For the fixed $z$, $f(z,\xi_k)$ is also fixed, why do we give up LP reformulation to derive 
% the MILP reformulation for the inner minimization problem if we still use the derivative free method to solve the outer maximization problem. 

% The main motivation for this section is that after introducing some binary variables to locate the position of $f(x,\xi_k)$ in the triangle partitions (Type 1 or Type 2), 
% the objective function in (3.16) has an explicit linear form not containing the indicator functions. 
% Then, we may use the dual theory for LP model to transfer the minimization problem (3.16) as a maximization LP. 
% Consequently, the maximin problem would be a single maximization problem involving some binary variables depending on $z$. 
% When $f(z,\xi)$ is linear in $z$, the final problem would be a MILP. 

% }

% In Section~\ref{sec:numer-methods}, we propose to use
% derivative-free method for solving program (\ref{eq:PRO-N-reformulate}).



% In the objective function of program \cref{eq:PRO-N-reformulate},
% we use indicator functions $\mathds{1}_{\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]}(\frac{y-y_j}{x-x_j})$ and $\mathds{1}_{\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)}(\frac{y-y_j}{x-x_j})$ to 
% %restrict the
% judge whether  $f({\bm z},\bdxi^k)$ lies in the upper or lower triangle of a particular cell. 
% This can be achieved by introducing binary variables and subsequently the inner minimization problem becomes a 
% mixed integer linear programming (MILP). {\color{red}The inner minimization problem is always a LP under the setting in the beginning of section 3. }
% Since 
% existing software  
% such as gurobi and cplex is very powerful for solving MILP, we discuss in this section how to 
% develop an MILP formulation for the inner minimization problem. Specifically, we use the polyhedral method
% %to model the PLA,
% %see e.g. 
% (\cite{DLM10,LeW01})
% %We will 
% to model PLA approach for solving 
% %reformulate the PLA of the
% expected utility maximization problem as a mixed integer program in two dimensional, three dimensional and $m$ dimensional Euclidean space in \cref{sec:two-dim-u}-\cref{sec:m-dim-u},
% respectively.
% % In subsection \ref{sec:LP-reformulate},
% % we will reformulate the PLA of the utility minimization problem as an LP with respect to increments and get the tractable formulation of the robust utility maximin problem as a mixed integer maximization problem.

{\color{black}
In this section,
we look into the PLA approach from 
a slightly different perspective: instead of deriving an explicit form of 
piecewise linear function as we discussed in
the previous section, we propose to use
the well-known polyhedral method
%will introduce implicit reformulation of PLA of UPRO
%by polyhedral method 
(see e.g. \cite{LeW01,KDN04,DLM10,VAN10,vielma2015mixed}),
where the PLA
% piecewise linear
function 
at each cell is implicitly determined by solving a minimization or a maximization program.
% Specifically,
% we 
% %where 
% introduce binary variables 
% %are used 
% to identify which triangle area
% the target point ${\bm f}(\bdz,\bdxi^k)$ locates,
% and the PLA form are determined by solving a system with linear equalities and linear inequalities.
% Specifically, we use the polyhedral method
% %to model the PLA,
% %see e.g. 
% (\cite{DLM10,LeW01})
% %We will 
% to model PLA approach 
% %for solving 
% to solve
% %reformulate the PLA of the
% expected utility maximization problem
% {\color{blue}
% and the robust problem
% } as a mixed integer program 
%  in two attributes, three attributes and $m$ attributes cases in \cref{sec:two-dim-u}-\cref{sec:m-dim-u},
% respectively.
% and we discuss in this section how to develop a mixed-integer program formulation
% %for 
% %the inner minimization problem. 
% to calculate the coefficients of $u_{i,j}$
% implicitly.
There are two 
%reasons 
advantages 
for 
%to
doing this.
%The one is that the implicit reformulation technique makes it easier to extend the implicit PLA of UPRO to three attributes case.
One is that the implicit approach allows us to
extend the PLA for UPRO problem from bi-attribute decision making problems to the multi-attribute case and
this would be extremely 
complex under the   
%for  
%to derive the 
explicit PLA framework. 
%function.
% This 
% avoids the complex explicit reformulation of $\bbe_{P_K}[u_N({\bm f}(\bdz,\bdxi))]$
% since there are six cases to discuss where the target point ${\bm f}(\bdz, \bdxi^k)\in \R^3$ locates.
%where the coefficients of $u_{i,j,l}$ are calculated through equalities and inequalities.
The other 
%reason 
is that 
%it can help us to 
the implicit approach enables us to
reformulate 
%maximin problem 
the approximate UPRO problem
into a single MILP
when ${\bm f}(\bdz,\bdxi)$ is linear in $\bdz$.
%since the derivative-free method does not use the structure of  this problem as far as possible.
}


% In the objective function of program \cref{eq:PRO-N-reformulate},
% we use indicator functions $\mathds{1}_{\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]}(\frac{y-y_j}{x-x_j})$ and $\mathds{1}_{\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)}(\frac{y-y_j}{x-x_j})$ to 
% %restrict the
% judge whether  $f({\bm z},\bdxi^k)$ lies in the upper or lower triangle of a particular cell. 
% {\color{blue}
% This can be achieved by introducing binary variables and subsequently the coefficients of $u_{i,j}$ in (\ref{eq:coefficient-obj}) can be identified by several equalities and inequalities
% to avoid identifying whether
% $\frac{y-y_j}{x-x_j}$ lies within $\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]$ or $\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)$.}
% {\color{red}HX: with Remark 3.1 (iii),
% this is no longer needed.}

% {\color{Green}
% Sainan: I try to explain how the binary variable corresponds to the indicator function.}
% {\color{red}The inner minimization problem is always a LP under the setting in the beginning of section 3. }
% Since 
% existing software  
% such as GUROBI and CPLEX is very powerful for solving mixed-integer programs, we discuss in this section how to 
% develop a mixed-integer program formulation
% %for 
% %the inner minimization problem. 
% {\color{blue}to calculate the coefficients of $u_{i,j}$}
%by solving a program.}
% Specifically, we use the polyhedral method
% %to model the PLA,
% %see e.g. 
% (\cite{DLM10,LeW01})
% %We will 
% to model PLA approach 
% %for solving 
% to solve
% %reformulate the PLA of the
% expected utility maximization problem
% {\color{blue}
% and the robust problem
% } as a mixed integer program 
%  in two dimensional, three dimensional and $m$ dimensional Euclidean space in \cref{sec:two-dim-u}-\cref{sec:m-dim-u},
% respectively.
% In subsection \ref{sec:LP-reformulate},
% we will reformulate the PLA of the utility minimization problem as an LP with respect to increments and get the tractable formulation of the robust utility maximin problem as a mixed integer maximization problem.


\subsection{Bi-attribute case}
\label{sec:two-dim-u}
%{\color{blue}
% Recall that 
%${\cal X}=
%\{\underline{x}=x_1<\ldots<x_{N_1}=\bar{x}\}\subset X$,
%${\cal Y}=\{
%\underline{y}=y_1<\ldots<y_{N_2}=\bar{y}\}\subset Y$,
% $\calx\times \caly=\{(x_i,y_j), x_i\in {\cal X},y_j\in {\cal Y}\}$ forms a set of grid points
% in $X\times Y$,
% $X_i=[x_i,x_{i+1}]$,
% $Y_j = [y_j,y_{j+1}]$
% and rectangle
% $X_i\times Y_j=
% [x_i,x_{i+1}]\times [y_j,y_{j+1}]$.
% Consider a utility function $u:[\underline{x},\bar{x}]\times [\underline{y},\bar{y}]\rightarrow \R$
% and its continuous 
% piecewise linear approximation 
% %PLA through
% $u_N$ which is linear 
% in each cell $X_i\times Y_j$, 
% for
% %$((x_i,y_j),u(x_i,y_j))$,
% $i=1,\cdots,N_1$,
% $j=1,\cdots,N_2$.
%{\color{red}HX: In a two dimensional case, we cannot define breakpoints like this. Please refer to my descriptions in Section 3.1 (page 9)}
% $u_N$ is the PLA by connecting points $\{((x_i,y_j),f(x_i,y_j))\in \R^{2+1}:\; 1\leq i\leq N_1,\; 1\leq j\leq N_2\}$.

% Observe that 
% in two dimensional case,
% we can use the following method to 
%The polyhedral method (\cite{DLM10,KDN04,LeW01,VAN10,vielma2015mixed})
%prompts us
% {\color{red}Let
% $h_{i,j,k}^u=\mathds{1}_{T_{i,j}\bigcap\left(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\infty \right)}$
% and $h_{i,j,k}^l=\mathds{1}_{T_{i,j}\bigcap\left[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\right]}$}
Inspired by the polyhedral method, we 
%to
can obtain the coefficients $\alpha_{i,j}$ of $u_N(\bdf(\bdz,\bdxi^k))$ in terms of $u_{i,j}$ 
under Type-1 PLA
in Remark~\ref{rem:BUPRO-DF}~(iii)
by solving a system of linear equalities and inequalities:
% \begin{subequations}
% \label{eq:mixed-integer-R2}
% \begin{eqnarray}
%  & & \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k}=1,
% \label{eq:mixed-integer-R2-b}\\
% && \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} x_{i}=f_1^k,\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} y_{j}= f_2^k,\;\; k=1,\cdots,K,\label{eq:mixed-integer-R2-c}\\
% && \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} h_{i,j,k}^{u}+h_{i,j,k}^{l}=1,\;\;
% k=1,\cdots,K,\label{eq:mixed-integer-R2-d}\\
% % &&  h_{i,j,k}^{u}, \; h_{i,j,k}^{l}\in \{0,1\},\;i=1,\cdots,N_1-1,\; j=1,\cdots,N_2-1,\; k=1,\cdots,K,\label{eq:mixed-integer-R2-e}\\
% &&0\leq  \alpha_{i,j}^{k}\leq 
% h_{i,j,k}^{u}
% +h_{i,j,k}^{l} 
% +h_{i,j-1,k}^{u}
% +h_{i-1,j-1,k}^{l}
% +h_{i-1,j-1,k}^{u}
% +h_{i-1,j,k}^{l},\nonumber \\
% && \qquad \qquad \qquad  \qquad \qquad 
% %\inmat{for}\; 
% i=1,\cdots,N_1,\; j=1,\cdots,N_2, \; k=1,\cdots,K,
% \label{eq:mixed-integer-R2-f}
% \end{eqnarray}
% \end{subequations}
\begin{subequations}
\begin{align}
    & \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k}=1,\; k=1,\cdots,K,
    \label{eq:mixed-integer-R2-b}\\
    & \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} x_{i}=f_1^k,\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} y_{j}= f_2^k,\;\; k=1,\cdots,K,\label{eq:mixed-integer-R2-c}\\
    & \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \lt(h_{i,j,k}^{u}+h_{i,j,k}^{l}\rt)=1,\;\;
    k=1,\cdots,K,\label{eq:mixed-integer-R2-d}\\
    & {\bm h}^u_k,{\bm h}^l_k\in \{0,1\}^{(N_1-1)(N_2-1)},\;k=1,\cdots,K,
    % &&  h_{i,j,k}^{u}, \; h_{i,j,k}^{l}\in \{0,1\},\;i=1,\cdots,N_1-1,\; j=1,\cdots,N_2-1,\; k=1,\cdots,K,
    \label{eq:mixed-integer-R2-e}\\
    &0\leq \alpha_{i,j}^{k}\leq 
    h_{i,j,k}^{u}
    +h_{i,j,k}^{l} 
    +h_{i,j-1,k}^{u}
    +h_{i-1,j-1,k}^{l}
    +h_{i-1,j-1,k}^{u}
    +h_{i-1,j,k}^{l},\nonumber \\
    & \qquad \qquad \qquad  \qquad \qquad \quad
%\inmat{for}\; 
i=1,\cdots,N_1,\; j=1,\cdots,N_2, \; k=1,\cdots,K,
\label{eq:mixed-integer-R2-f}
\end{align}
\end{subequations}
% where ${\bm f(\bdz,\bdxi^k)}=({\bm f(\bdz,\bdxi^k)}_1,{\bm f(\bdz,\bdxi^k)}_2)$.
where {\color{black} ${\bm h}^u_k:= {\rm vec}\left((h^u_{i,j,k})_{1\leq i\leq N_1}^{1\leq j\leq N_2}\right)$, ${\bm h}^l_k:= {\rm vec}\left((h^l_{i,j,k})_{1\leq i\leq N_1}^{1\leq j\leq N_2}\right)$ for $k=1,\ldots,K$,}
% \begin{align*}
%     {\bm h}^u_k & :=(h_{1,1,k}^u,\cdots,h_{N_1-1,1,k}^u,\cdots,h_{1,N_2-1,k}^u,\cdots,h_{N_1-1,N_2-1,k}^u)^T, \\
%     {\bm h}^l_k & :=(h_{1,1,k}^l,\cdots,h_{N_1-1,1,k}^l,\cdots,h_{1,N_2-1,k}^l,\cdots,h_{N_1-1,N_2-1,k}^l)^T, k=1,\cdots,K,
% \end{align*}
% for 
% $$,
% {\color{red}
% ${\bm h}^u=({\bm h}^u_1,\cdots,{\bm h}^u_{K})\in \{0,1\}^{(N_1-1)\times(N_2-1)\times K}$,
% ${\bm h}^l=({\bm h}^l_1,\cdots,{\bm h}^l_{K})\in \{0,1\}^{(N_1-1)\times(N_2-1)\times K}$,}
% $h_{i,j,k}^{u}, \; h_{i,j,k}^{l}\in \{0,1\},\;i=1,\cdots,N_1-1,\;
% j=1,\cdots,N_2-1,\; k=1,\cdots,K,
%  %\label{eq:mixed-integer-R2-e}
% $,
${\bm f}(\bdz,\bdxi^k)=(f_1^k,f_2^k)^T$ with $f_i^k:=f_i(\bdz,\bdxi^k)$ for $i=1,2$,
% ${\bm \alpha}=({\bm \alpha}^1,\cdots,{\bm \alpha}^K)\in \R^{N_1\times N_2\times K}$,
% ${\bm h}^u=({\bm h}^u_1,\cdots,{\bm h}^u_{K})\in \R^{N_1\times N_2\times K}$,
% ${\bm h}^l=({\bm h}^l_1,\cdots,{\bm h}^l_{K})\in \R^{N_1\times N_2\times K}$,
$h_{0,*,*}^*=h_{*,0,*}^*=h_{N_1,*,*}^*=h_{*,N_2,*}^*=0$.
Here $*$ represents all indexes possibly taken at the subscripts and superscripts.
Constraint (\ref{eq:mixed-integer-R2-b}) and $\alpha_{i,j}^k\geq 0$
result from the coefficients of the convex combinations of $u_{i,j}$
% convex components of $u_{i,j}$
for $u_N({\bm f}(\bdz,\bdxi^k))$.
% in a linear function $l({\bdx})$, 
% and ${\bm x}=\alpha x_1+\beta x_2 +\gamma x_3$ with $\alpha +\beta +\gamma =1$, we have
% % $u({\bm x})=u(\alpha x_1+\beta x_2 +\gamma x_3)=\alpha l(x_1)+\beta l(x_2)+\gamma l(x_3)$ which means that
% the linearity of $u_N$ over $T_{i,j}$ guarantees that
%  the convex combination coefficients of ${\bm f}(\bdz,\bdxi^k)$ and $u({\bm f}(\bdz,\bdxi^k))$ are the identical.
Constraint (\ref{eq:mixed-integer-R2-c}) 
% and $\alpha_{i,j}^k\geq 0$
arises because 
% in a linear function $l({\bdx})$, 
% and ${\bm x}=\alpha x_1+\beta x_2 +\gamma x_3$ with $\alpha +\beta +\gamma =1$, we have
% $u({\bm x})=u(\alpha x_1+\beta x_2 +\gamma x_3)=\alpha l(x_1)+\beta l(x_2)+\gamma l(x_3)$ which means that
the linearity of $u_N$ over $T_{i,j}$ guarantees that
the convex combination coefficients of ${\bm f}(\bdz,\bdxi^k)$ and $u_N({\bm f}(\bdz,\bdxi^k))$ are 
% {\color{purple}the} 
identical.
Since $h_{i,j,k}^u, h_{i,j,k}^l\in \{0,1\}$,
constraint (\ref{eq:mixed-integer-R2-d}) imposes 
a restriction 
that only one is used 
for the convex combination among all triangles.
The constraint (\ref{eq:mixed-integer-R2-f}) imposes that the
% {\color{blue}
only nonzero $\alpha_{i,j}$ 
% values different from $0$
% }
can be those associated with the
three vertices of a such triangle.
For example, if $h_{i,j,k}^l=1$, then 
$\bdf(\bdz,\bdxi^k)$ lies in the
lower triangle
of the cell $T_{i,j}$. This is %evidenced 
indicated by the fact that
$\alpha_{i,j}^k\leq h_{i,j,k}^l=1$,
$\alpha_{i+1,j+1}^k\leq h_{i,j,k}^l=1$,
$\alpha_{i+1,j}^l\leq h_{i,j,k}^l=1$,
and $\alpha_{i',j'}^k=0$ for $(i',j')\notin \{(i,j),(i+1,j+1),(i+1,j)\}$,
see Figure~\ref{fig:Type1IPLA} where
the six triangles are related to point $(x_i,y_j)$ 
and we indicate the corresponding binary variables $h_{i,j,k}^u$ and $h_{i,j,k}^l$ in each triangle to facilitate readers understanding.
% Then the 
% {\color{red}HX:
% why don't we say  that the representation of 
% $u_{N}(\bdf(\bdz,\bdxi^k)) $ 
% derived in
% Remark 3.1 (iii) 
% can be obtained by solving a program?
% We can then jump to (4.1).
% }
% {\color{Green}
% Sainan: Yes, 
% it is a more straightforward way.}
% % {\color{blue}For fixed $\bdz\in Z$,
% % to  calculate $\alpha_{ij}(\bdz)$,
% we can figure out $f_1(\bdz,\bdxi^k)$ and $f_2(\bdz,\bdxi^k)$ first,
% and
% then locate which rectangle $T_{i^*,j^*}$ contains it.
% By the form of PLA,
% we know the value of $u$ at $(f_1^k,f_2^k)$ is only related to function values at three vertices.
% % We can calculate ${\bm \alpha}$ as
% By (\ref{eq:coefficient-obj}),
% we have
% \bgeq
% && \alpha_{i^*,j^*}^k (\bdz)= \frac{f_1^k-x_{i^*+1}}{x_{i^*}-x_{i^*+1}} \inmat{ and } \alpha_{i^*+1,j^*+1}^k(\bdz) = \frac{f_2^k-y_{j^*}}{y_{j^*+1}-y_{j^*}}.
% \edeq
% If $0\leq \frac{f_2^k-y_j}{f_1^k -x_i}\leq  \frac{y_{j^*+1}-y_{j^*}}{x_{i^*+1}-x_{i^*}}$,
% then 
% $\alpha_{i^*,j^*+1}^k(\bdz) = \frac{f_2^k-y_{j^*}}{y_{j^*+1}-y_{j^*}}-\frac{f_1^k-x_{i^*}}{x_{i^*+1}-x_{i^*}}$
% and
% $\alpha_{i',j'}^k(\bdz)=0$ for $(i',j')\notin \{(i,j),(i+1,j+1),(i+1,j)\}$.
% Observe that $\alpha_{i^*,j^*+1}^k(\bdz) =1-\alpha_{i^*,j^*}^k(\bdz)-\alpha_{i^*+1,j^*+1}^k(\bdz)$
% which means
% $$
% u_N(f_1^k,f_2^k)=
% \alpha_{i^*,j^*}^k(\bdz)u_{i^*,j^*}
% +\alpha_{i^*+1,j^*+1}^k(\bdz) u_{i^*+1,j^*+1}
% + (1-\alpha_{i^*,j^*}^k(\bdz)-\alpha_{i^*+1,j^*+1}(\bdz)) u_{i^*,j^*+1}.
% $$
% If $ \frac{f_2^k-y_j}{f_1^k -x_i}>  \frac{y_{j^*+1}-y_{j^*}}{x_{i^*+1}-x_{i^*}}$,
% then 
% $\alpha_{i^*+1,j^*}^k(\bdz) =\frac{f_1^k-x_{i^*}}{x_{i^*+1}-x_{i^*}}-\frac{f_2^k-y_{j^*}}{y_{j^*+1}-y_{j^*}}$,
% and $\alpha_{i',j'}^k(\bdz)=0$ for $(i',j')\notin \{(i,j),(i+1,j+1),(i,j+1)\}$.
% We can also see that $\alpha_{i^*+1,j^*}^k(\bdz)= 1-\alpha_{i^*,j^*}^k(\bdz)-\alpha_{i^*+1,j^*+1}^k(\bdz)$,
% which means
% $$
% u_N(f_1^k,f_2^k)=
% \alpha_{i^*,j^*}^k(\bdz)u_{i^*,j^*}
% +\alpha_{i^*+1,j^*+1}^k(\bdz) u_{i^*+1,j^*+1}
% + (1-\alpha_{i^*,j^*}^k(\bdz)-\alpha_{i^*+1,j^*+1}(\bdz)) u_{i^*+1,j^*}.
% $$
% {\color{blue}
% Note that for a linear function $l({\bdx})$, 
% and ${\bm x}=\alpha x_1+\beta x_2 +\gamma x_3$ with $\alpha +\beta +\gamma =1$, we have
% $u({\bm x})=u(\alpha x_1+\beta x_2 +\gamma x_3)=\alpha l(x_1)+\beta l(x_2)+\gamma l(x_3)$ which means
% that
% %This guarantees that 
%  the convex combination coefficients of $(x,y)$ and $u(x,y)$ are 
%  the identical.
%  % For any given  point $({x},{y})\in T_{i,j}$,
% % %and the two triangles are separated by the main diagonal,
% % % {\color{blue}In this case,
% % $(x,y)$ can be characterized by three points $(x_i,y_j)$, $(x_{i+1},y_{j+1})$
% % and $(x_{i+1},y_j)$ 
% % (res.~$(x_i,y_{j+1})$) if the point lies within the lower triangle (res. upper triangle),
% % and the utility function 
% % $u(x,y)$ can be characterized 
% % by the function values at the three points, i.e.,
% % $u_{i,j}$, $u_{i+1,j+1}$
% % and $u_{i+1,j}$ (res.~$u_{i,j+1}$).
% This prompts us to use grid points $(x_i,y_j)$ and the matrix of utility values $(u_{i,j})_{N_1\times N_2}$ and  $\alpha_{i,j}^k\geq 0$ with $\sum_{i=1}^{N_1}\sum_{j=1}^{N_2}\alpha_{i,j}^k=1$ to 
% represent ${\bm f}(\bdz,\bdxi^k)$ and $u({\bm f}(\bdz,\bdxi^k))$.
% Additionally,
% we introduce binary variables $h_{i,j,k}^u$ and $h_{i,j,k}^l$ to identify whether a point ${\bm f}(\bdz,\bdxi^k)$ locates at the upper or lower triangle of a cell $T_{i,j}$.
% The connection between $\alpha_{i,j,k}^k$ and $h_{i,j,k}^l$, $h_{i,j,k}^u$ is that only the three vertices are used to represent the implicit PLA when $h_{i,j,k}^u=1$ or $h_{i,j,k}^l=1$ .
%  }
%  the identical. Using the fact, we can write
% %The function value 
% $u({x},{y})$ as
% %is characterized by convex combination of the function values evaluated at the vertices of the triangle containing $(x,y)$,
% %that is,
% \begin{equation}
% \label{eq:convex-coefficient}
% u(x,y)=\lambda u_{i,j}
% +\mu u_{i+1,j+1}+(1-\lambda-\mu) \hat{u}, \; 
% \inmat{for}\; (x,y)\in X_i\times Y_j,
% \end{equation}
% where  $\lambda, \mu\in [0,1]$ and
% $$
% \hat{u}= u_{i+1,j}\1_{ \lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i} \rt]} \lt( \frac{y-y_j}{x-x_i} \rt) + u_{i,j+1} \1_{ \lt(\frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty \rt)} \lt( \frac{y-y_j}{x-x_i} \rt).
% $$
% \bgeq
% \hat{u}=\left\{ 
% \begin{array}{ll}
% u(x_{i+1}, y_j) & \inmat{if }{y}\leq y_j+({x}-x_i)\frac{y_{j+1}-y_j}{x_{i+1}-x_i},\\
% u(x_i,y_{j+1}) & \inmat{otherwise}.
% \end{array}
% \right.
% \edeq
% Consequently,
%
Consequently,
under Assumption~\ref{assu-discrete},
%Then 
we can formulate
the bi-attribute utility maximization problem  
$\max_{\bdz\in Z}\sum_{k=1}^Kp_k[u_N({\bm f(\bdz,\bdxi^k)})]$ 
% in (\ref{eq:UMP-bi}) 
% (with $P$ being replaced by the empirical distribution
%  $P_K$) 
 as
an MIP:
%mixed-integer problem:
% {\color{red} HX:
% It sounds like this section is 
% to reformulate the inner minimization problem as a single mixed integer programming problem. Why not for Dfree? I thought it is about application of the polyhedral method. 
% }
\begin{subequations}
\label{eq:mixed-integer-R2-2}
\begin{align}
 \max\limits_{\bdz\in Z,{\bm \alpha},{\bm h^l},{\bm h}^u} \; &  \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} u_{i,j}\\
 {\rm s.t. }\quad\;\;\; & \inmat{constraints } (\ref{eq:mixed-integer-R2-b})-
 (\ref{eq:mixed-integer-R2-f}),
% &{\rm s.t.} & \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k}=1,
% \label{eq:mixed-integer-R2-b}\\
% && \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} x_{i}=f_1^k,\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} y_{j}= f_2^k,\;\; k=1,\cdots,K,\label{eq:mixed-integer-R2-c}\\
% && \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} h_{i,j,k}^{u}+h_{i,j,k}^{l}=1,\;\;
% k=1,\cdots,K,\label{eq:mixed-integer-R2-d}\\
% % &&  h_{i,j,k}^{u}, \; h_{i,j,k}^{l}\in \{0,1\},\;i=1,\cdots,N_1-1,\; j=1,\cdots,N_2-1,\; k=1,\cdots,K,\label{eq:mixed-integer-R2-e}\\
% &&0\leq  \alpha_{i,j}^{k}\leq 
% h_{i,j,k}^{u}
% +h_{i,j,k}^{l} 
% +h_{i,j-1,k}^{u}
% +h_{i-1,j-1,k}^{l}
% +h_{i-1,j-1,k}^{u}
% +h_{i-1,j,k}^{l},\nonumber \\
% && \qquad \qquad \qquad  \qquad \qquad  i=1,\cdots,N_1,\; j=1,\cdots,N_2, \; k=1,\cdots,K,
% \label{eq:mixed-integer-R2-f}
% % \\
% % && \alpha_{ij}^k\geq 0, i=1,\cdots,N_1,j=1,\cdots,N_2,k=1,\cdots,K,
\end{align}
\end{subequations}
where ${\bm \alpha}:=({\bm \alpha}^1,\cdots,{\bm \alpha}^K)\in \R^{(N_1N_2)\times K}$,
${\bm \alpha}^{k}:={\rm vec}\left((\alpha_{i,j}^k)_{1\leq i\leq N_1}^{1\leq j\leq N_2}\right)$
% ${\bm \alpha}^{k}:=(\alpha_{1,1}^k,\cdots,\alpha_{N_1,1}^k,\cdots,\alpha_{1,N_2}^k,\cdots, \alpha_{N_1,N_2}^k)^T$ 
for $k=1,\cdots,K$,
${\bm h}^l:=({\bm h}_1^l,\cdots,{\bm h}_K^l)\in \R^{(N_1-1)(N_2-1)\times K}$,
${\bm h}^u:=({\bm h}_1^u,\cdots,{\bm h}_K^u)\in \R^{(N_1-1)(N_2-1)\times K}$.
If $f(\bdz,\bdxi)$ is linear in $\bdz$, then the problem (\ref{eq:mixed-integer-R2-2}) is an MILP.
This idea can be applied to the BUPRO-N model.
To ease the exposition, we consider 
the case that the ambiguity set is constructed 
by pairwise comparisons, %i.e,  
that is,
$\psi_l=F_{{\bm B}_l}-F_{{\bm A}_l}$,
and
% {\color{blue}
% In what follows, 
% %this section,
% we concentrate on the implicit PLA reformulation of BUPRO and obtain the single mixed-integer program form of BUPRO-N.
% Hence we ignore the convex constraints (\ref{eq-traform-concave1})-(\ref{eq-traform-concave2}) and use specific ${\cal U}_N$ in 
% (\ref{eq:u_N-int}) 
% with $\psi_l(x,y)$ taking the  form
% $F_{{\bm B}_l}(x,y)-F_{{\bm A}_l}(x,y)$
% defined as in Example~\ref{eq-amb-pairwise} to highlight the new techniques,
% where questions $({\bm A}_l,{\bm B}_l)$ be random variables defined over $(\Omega,{\cal F},\mathbb{P})$.}
% We can apply the idea to problem 
% (\ref{eq:PRO-N-reformulate})
% %We can also incorporate the  robust version into the mixed integer format.
% %Specifically,
% with ambiguity set constructed by pairwise comparison 
\begin{equation*}
\begin{split}
    {\cal U}_N
    &=\left\{u_N\in \mathscr{U}_N:
    %\bbe_{\mathbb{P}}[u({\bm B}_l)]\leq \bbe_{\mathbb{P}}[u({\bm A}_l)],
    \int_{T} u_N(x,y) d (F_{\bdcb_l}(x,y)-F_{\bdca_l}(x,y))\leq 0,\;l=1,\cdots,M\right\}. 
% &=&\left\{u_N\in \mathscr{U}_N: \int_{\underline{x},\underline{y}}^{\bar{x},\bar{y}} \psi_l(x,y) d u_N(x,y)
%         + \int_{\underline{x}}^{\bar{x}} \psi_{1,m}(x)d u_N(x,\underline{y})  +\int_{\underline{y}}^{\bar{y}} \psi_{2,m} (y) d u_N(\underline{x},y) \leq 0,\;m=1,\cdot,M \right\}\\
% &=& \left\{u_N\in \mathscr{U}_N:\sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \psi_l(x_i,y_j)
% (u_{i+1j+1}-u_{i,j+1}-u_{i+1j}+u_{ij}) \right.\\
% &&  \qquad  \qquad 
% + \left. \sum_{i=1}^{N_1-1}\psi_{1,m}(x_i) (u_{i+1,1}-u_{i1})
% +\sum_{j=1}^{N_2-1} \psi_{2,m} (y_j) (u_{1,j+1}-u_{1,j}) \leq 0,\; m=1,\cdots,M\right\},
% \edeq
% where $u_{ij}=u(x_i,y_j)$,
% $i=1,\cdots,N_1$, $j=1,\cdots,N_2$.
% For simplicity,
% we consider 
% \bgeq
% {\cal U}_N
%     &=\left\{u_N\in \mathscr{U}_N:
%     \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \mathbb{P}({\bm B}_l=(x_i,y_j)- \mathbb{P}({\bm A}_l=(x_i,y_j))u_{i,j}
%   \leq 0, \;l=1,\cdots,M
% \right\}.
\end{split}
\end{equation*}
% {\color{red}It might be better to use the general form of ambiguity set instead of a specific form. Please double check.}
% Then
% we can reformulate 
% %robust utility maximization problem 
% BUPRO-N
% $\disp{\max_{\bdz\in Z} \min_{u_N\in \tilde{\cal U}_N}\bbe_{P_K}[u_N({\bm f(\bdz,\bdxi)})]}$ 
% in (\ref{eq:MAUT-robust-N})
% with 
% Let
% $
% \tilde{\cal U}_N={\cal U}_P 
% % \bigcap {\cal U}_{\rm RA}
% \bigcap \{u_N: u_N \inmat{ is Lipschtz with the modulus }L\}.
% $
Under Assumption~\ref{assu-lip},
suppose  the set of gridpoints $\{(x_i,y_j):i=1,\cdots,N_1,j=1,\cdots,N_2\}$ contains all the outcomes of lotteries $ {\bm A}_l$ and ${\bm B}_l$ for $l=1,\cdots,M$,
% Then
then we can reformulate 
%robust utility maximization problem 
BUPRO-N problem
% $\disp{\max_{\bdz\in Z} \min_{u_N\in {\cal U}_N}\bbe_{P_K}[u_N({\bm f(\bdz,\bdxi)})]}$ 
% in 
(\ref{eq:MAUT-robust-N-dis})
%with 
as: 
%a mixed integer problem:
\begin{subequations}
\label{eq:PRO_MILP_eqi}
\begin{align}
\max\limits_{\bdz\in Z,{\bm \alpha}, {\bm h}^u, {\bm h}^l} 
\min\limits_{{\bm u}} \;\;&
\sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^k u_{i,j}\\
{\rm s.t.} \;\; &
 \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} (\mathbb{P}({\bm B}_l=(x_i,y_j))- \mathbb{P}({\bm A}_l=(x_i,y_j)))u_{i,j}
   \leq 0, \nonumber \\
   & \hspace{16em} l=1,\cdots,M,
   \label{eq:PRO_MILP_mina2-c}\\
  & \inmat{constraints } (\ref{eq-traform-lip1})-(\ref{eq-traform-norm1}),  \\
%\label{eq:PRO_MILP_eqi-b}\\
%&& 
 & \inmat{constraints } (\ref{eq:mixed-integer-R2-b})
 %, (\ref{eq:mixed-integer-R2-d})
  -(\ref{eq:mixed-integer-R2-f}),
%   \label{eq:PRO_MILP_eqi-c}
 %(\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d}),\\
%   \\
% &&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} x_{i}=f_1(\bdz,\bdxi^k),\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} y_{j}= f_2(\bdz,\bdxi^k),\; k=1,\cdots,K,
\end{align}
\end{subequations}
where constraints (\ref{eq-traform-lip1})-(\ref{eq-traform-norm1}) characterize
the restrictions on ${\bm u}$
and constraints (\ref{eq:mixed-integer-R2-b})
%, (\ref{eq:mixed-integer-R2-d})
-(\ref{eq:mixed-integer-R2-f}) 
%characterize 
stipulate the coefficients of ${\bm u}$ implicitly as discussed earlier.
{\color{black}Problem (\ref{eq:PRO_MILP_eqi}) is equivalent to problem (\ref{eq:PRO-N-reformulate}) without constraints (\ref{eq-traform-concave1})-(\ref{eq-traform-concave2}) and with $\psi_l=F_{{\bm B}_l}-F_{{\bm A}_l}$, $l=1,\cdots,M$.}
  It is possible to change the maximization
  w.r.t. ${\bm \alpha}$, ${\bm h}^l$  and ${\bm h}^u$ into 
  minimization. The next proposition explains this. 
  
 
%  The following proposition discusses that the variables ${\bm \alpha}$, ${\bm h}^l$ 
% % and ${\bm h}^u$ can be taken from the outer maximization problem to the inner minimization problem.
% Problem (\ref{eq:PRO_MILP_eqi}) is equivalent to problem (\ref{eq:PRO-N-reformulate}) without constraints (\ref{eq-traform-concave1})-(\ref{eq-traform-concave2}).

\begin{proposition}
The BUPRO-N problem (\ref{eq:PRO_MILP_eqi}) is equivalent to 
\begin{subequations}
\label{eq:PRO_MILP_mina2}
\begin{align}
%v_{\rm inner}({\bm z}_1^*)
% :=
 \displaystyle \max_{\bdz\in Z}  \displaystyle \min_{{\bm \alpha},{\bm h}^u,{\bm h}^l,
 %}  \displaystyle \min_{
 {\bm u}} \;\;&
\sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} u_{i,j}\\
 \inmat{s.t.}\quad\;\; & \inmat{constraints } (\ref{eq-traform-lip1})-(\ref{eq-traform-norm1}),\; 
% \\ &{\rm s.t.}&
   (\ref{eq:PRO_MILP_mina2-c}), \label{eq:PRO_MILP_mina2-b}\\
    & \inmat{constraints } (\ref{eq:mixed-integer-R2-b})
%  ,(\ref{eq:mixed-integer-R2-d})
  -(\ref{eq:mixed-integer-R2-f}).
% &&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} x_{i}=f_1(\bdz_1,\bdxi^k),\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} y_{j}= f_2(\bdz_1,\bdxi^k),\;\; k=1,\cdots,K.~~
% \label{eq:PRO_MILP_mina2-c}
% \qquad 
\end{align}
\end{subequations}
% \begin{subequations}
% \label{eq:PRO_MILP_eqi}
% \begin{eqnarray}
% &\max\limits_{\bdz\in Z,{\bm \alpha}, {\bm h}^u, {\bm h}^l} & \min\limits_{{\bm u}}
% \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^k u_{i,j}\\
% &{\rm s.t.}&
%  (\ref{eq:mixed-integer-R2-b}),
%   (\ref{eq:mixed-integer-R2-d})-(\ref{eq:mixed-integer-R2-f}),(\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d}),\\
% &&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} x_{i}=f_1(\bdz,\bdxi^k),\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} y_{j}= f_2(\bdz,\bdxi^k),\; k=1,\cdots,K.
% \end{eqnarray}
% \end{subequations}
\end{proposition}
\noindent{\bf Proof.}
% First,
% we observe  that constraints (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d}) are only related to variable ${\bm u}$,
% constraints (\ref{eq:mixed-integer-R2-b}),
%   (\ref{eq:mixed-integer-R2-d})-(\ref{eq:mixed-integer-R2-f}) are for ${\bm \alpha}$,
%   ${\bm h}^u$, ${\bm h}^l$,
%   and $\bdz$ needs to satisfy 
%   constraint (\ref{eq:PRO-MILP-d}).
% Let ${\bm z}_1^*$ and ${\vt}_{1}^*$ be the optimal solution and  optimal value of problem (\ref{eq:PRO-MILP}).
%{\color{purple}The middle 
We begin by 
writing part of the outer maximization (w.r.t. ${\bm \alpha},{\bm h}^u,{\bm h}^l$)
and  the inner minimization
problem of
%{\color{green}Please double check.}
(\ref{eq:PRO_MILP_eqi}) as
%take
%the form
%can be
%equivalently written as
\begin{subequations}
\label{eq:PRO_MILP_mina}
\begin{align}
%v_{\rm inner}({\bm z}_1^*)
% :=
 \displaystyle \max_{{\bm \alpha},{\bm h}^u,{\bm h}^l} \; &  \min_{{\bm u}}\left\{
\sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} u_{i,j}:  (\ref{eq-traform-lip1})-(\ref{eq-traform-norm1}) \right \}\\
{\rm s.t.} \quad &
\inmat{\,constraints } (\ref{eq:mixed-integer-R2-b})
 %, (\ref{eq:mixed-integer-R2-d})
 - (\ref{eq:mixed-integer-R2-f}),
 (\ref{eq:PRO_MILP_mina2-c}). \label{eq:PRO_MILP_mina-b}
% &&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} x_{i}=f_1(\bdz_1,\bdxi^k),\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} y_{j}= f_2(\bdz_1,\bdxi^k),\;\; k=1,\cdots,K.~~
% \label{eq:PRO_MILP_mina-c}
% \qquad 
\end{align}
\end{subequations}
Since 
the representation of point $\bdf(\bdz,\bdxi^k)$ by the convex combination of the vertices of a simplex is unique, 
the feasible set of the outer 
%minimization
maximization problem (\ref{eq:PRO_MILP_mina})
(specified by
(\ref{eq:PRO_MILP_mina-b}))
%-(\ref{eq:PRO_MILP_mina-c}))
is a singleton 
for each fixed $\bdz$.
%(the representation of point $(f_1(\bdz,\bdxi^k),f_2(\bdz,\bdxi^k))$ by the convex combination of the vertices of a simplex is unique),
Thus we can replace operation ``$\max_{{\bm \alpha},{\bm h}^u,{\bm h}^l}$'' with ``$\min_{{\bm \alpha},{\bm h}^u,{\bm h}^l}$'' without affecting the optimal value and the optimal solutions of (\ref{eq:PRO_MILP_mina}).
The replacement effectively reduces 
(\ref{eq:PRO_MILP_eqi}) to (\ref{eq:PRO_MILP_mina2}). \hfill $\Box$

Note that the outer maximization problem (\ref{eq:PRO_MILP_mina2}) can be solved by a Dfree method,
where the inner problem can be seen as an MILP when ${\bm f}(\bdz,\bdxi^k)$ is linear in $\bdz$.

% \begin{subequations}
% \label{eq:PRO-MILP}
% \begin{eqnarray}
% &\max\limits_{\bdz\in Z}&  \min\limits_{{\bm \alpha}, {\bm h}^u, {\bm h}^l} \min\limits_{{\bm u}}
% \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^k u_{i,j}\\
% &{\rm s.t.}&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \Big( \mathbb{P}({\bm B}_m=(x_i,y_j))-\mathbb{P}({\bm A}_m=(x_i,y_j)) \Big) u_{i,j}, 
% \;m=1,\cdots,M, \qquad~ \label{eq:PRO-MILP-d}\\
% % \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k}=1,
% % \label{eq:mixed-integer-R2-b2}\\
% % && \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} x_{i}={\bm f(\bdz,\bdxi^k)}_1,\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} y_{j}={\bm f(\bdz,\bdxi^k)}_2,\;\; k=1,\cdots,K,\label{eq:mixed-integer-R2-c2}\\
% % % && \sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \psi_l(x_i,y_j)
% % % (u_{i+1,j+1}-u_{ij+1}-u_{i+1j}+u_{ij}) \\
% % % &&   +  \sum_{i=1}^{N_1-1}\psi_{1,m}(x_i) (u_{i+1,i}-u_{i1})
% % % +\sum_{j=1}^{N_2-1} \psi_{2,m} (y_j) (u_{1,j+1}-u_{1,j}) \leq 0,\; m=1,\cdots,M,\\
% % && \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} h_{ijk}^{u}+h_{ijk}^{l}=1,\;\;
% % k=1,\cdots,K,\label{eq:mixed-integer-R2-d2}\\
% % &&  h_{ijk}^{u}, \; h_{ijk}^{l}\in \{0,1\},\;i=1,\cdots,N_1-1,\; j=1,\cdots,N_2-1,\; k=1,\cdots,K,\label{eq:mixed-integer-R2-e2}\\
% % && \alpha_{ij}^{k}\leq 
% % h_{ijk}^{u}
% % +h_{ijk}^{l} 
% % +h_{ij-1k}^{u}
% % +h_{i-1j-1k}^{l}
% % +h_{i-1j-1k}^{u}
% % +h_{i-1jk}^{l},\nonumber \\
% % && \qquad \qquad \qquad  \qquad \qquad 
% % i=1,\cdots,N_1,\; j=1,\cdots,N_2, \; k=1,\cdots,K,\label{eq:mixed-integer-R2-f2}\\
% &&   (\ref{eq-traform-concave1})-(\ref{eq-traform-norm1}),
% \label{eq:PRO-MILP-c}\\
% && (\ref{eq:mixed-integer-R2-b})-(\ref{eq:mixed-integer-R2-f}), \label{eq:PRO-MILP-b}
% \end{eqnarray}
% \end{subequations}
% where ${\bm u}=(u_{i,j})_{N_1\times N_2}$.
\begin{remark}
\begin{itemize}
% \item[(i)]
% In the two-dimensional case,
% we can use the following method to calculate $\alpha_{ij}$.
% For $\bdz\in Z$,
% we can figure out $f_1(\bdz^k,\bdxi^k)$ and $f_2(\bdz^k,\bdxi^k)$ first,
% and
% then locate which rectangle $X_{i^*}\times Y_{j^*}$  contains it.
% We can calculate ${\bm \alpha}$ as
% \bgeq
% && \alpha_{i^*,j^*}^k = \frac{f_1(\bdz^k,\bdxi^k)-x_{i^*+1}}{x_{i^*}-x_{i^*+1}} \inmat{ and } \alpha_{i^*+1,j^*+1}^k = \frac{f_2(\bdz^k,\bdxi^k)-y_{j^*}}{y_{j^*+1}-y_{j^*}}.
% \edeq
% If $f_2(\bdz^k,\bdxi^k)\geq \frac{y_{j^*+1}-y_{j^*}}{x_{i^*+1}-x_{i^*}}(f_1(\bdz^k,\bdxi^k)-x_{i^*})$,
% then 
% $\alpha_{i^*,j^*+1}^k = 1-\alpha_{i^*j^*}^k-\alpha_{i^*+1,j^*+1}^k$ and
% $\alpha_{i',j'}^k=0$ for $(i',j')\notin \{(i,j),(i+1,j+1),(i+1,j)\}$.
% Otherwise,
% $\alpha_{i^*+1,j^*}^k = 1-\alpha_{i^*,j^*}^k-\alpha_{i^*+1,j^*+1}^k$,
% and $\alpha_{i'j'}^k=0$ for $(i',j')\notin \{(i,j),(i+1,j+1),(i,j+1)\}$.
% Using $\bdz$, $\alpha_{i,j}^k$,
% the inner minimization problem of (\ref{eq:PRO-MILP}) can be simplified as
% $
%  \min_{\bm u}  \left\{\sum_{k=1}^K p_k \sum_{i=1}^{N_1}\sum_{j=1}^{N_2} \alpha_{i,j}^k u_{i,j}:(\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d})\right\}
%  $.
%\\
% &{\rm s.t.}& 
% \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \mathbb{P}({\bm B}_m=(x_i,y_j))u_{i,j}  \leq \sum_{i=1}^{N_1} \sum_{j=1}^{N_2}  \mathbb{P}({\bm A}_m=(x_i,y_j))u_{i,j}, 
% \;m=1,\cdots,M.
% \edeq
% \item[(ii)]
% {\color{red}NEW!}
% For fixed ${\bm z}\in Z$,
% we know that there's a unique ${\bm \alpha}$ satisfies constraints (\ref{eq:mixed-integer-R2-b})-(\ref{eq:mixed-integer-R2-f}), and these constraints are independent with variable ${\bm u}$.
% Then we can regard problem (\ref{eq:PRO-MILP})
% as 
% \bgeq
% &\displaystyle \max_{{\bm z}\in Z, {\bm \alpha}}& \min_{\bm u} \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^k u_{ij}\\
% &{\rm s.t.}& (\ref{eq:PRO-MILP-b})-(\ref{eq:PRO-MILP-d}).
% \edeq
% From Table~\ref{tab-result-N-MILP},
% we see that the derivative-free method for (\ref{eq:PRO-MILP}) cost much time.
% In order to tackle this problem,
% we get the dual problem of the inner problem above,
\item[(i)] Inequality (\ref{eq:mixed-integer-R2-f}) corresponds to Type-1 PLA.
%the case that a cell is divided into two triangles by the main diagonal.
% For the case that the cell is divided by the counter diagonal
For Type-2 case
% under Type-2 PLA 
(see Figure~\ref{fig:Type2IPLA}),
we can replace (\ref{eq:mixed-integer-R2-f}) by 
\begin{equation}
\label{eq:constraint-alpha}
\begin{split}
0\leq \alpha_{i,j}^k\leq  h^u_{i-1,j,k}  + h^l_{i-1,j,k} & +  h^u_{i-1,j-1,k}+h^l_{i,j-1,k}+h^u_{i,j-1,k}+h^l_{i,j,k}, \\
    & i=1,\cdots,N_1,j=1,\cdots,N_2,k=1,\cdots,K.
\end{split}
\end{equation}
% where $[M_1]:=\{1,\cdots,M_1\}$ and $[M_2]$, $[K]$ are defined similarly.

\item[(ii)] 
% The ambiguity set of utility function can also be constructed by combing 
% %the two cases that the cell is divided by the main diagonal or counter diagonal.
% the Type-1 and Type-2 PLA.
For the mixed-type PLA,
we 
%to
can also obtain the coefficients $\alpha_{i,j}$ of $u_N(\bdf(\bdz,\bdxi^k))$ in terms of $u_{i,j}$ 
%under mixed-type PLA
by solving a system of linear equalities and inequalities:
\begin{subequations}
\begin{align}
    & \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k\tau}=1,\;\tau=1,2,\;k=1,\cdots,K,
    \label{eq:mixed-integer-Type2-b}\\
    & \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k\tau} x_{i}=f_1^k,\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k\tau} y_{j}= f_2^k,\;\; k=1,\cdots,K,\;\tau =1,2, \label{eq:mixed-integer-Type2-c}\\
    %& 
    %\sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \lt(h_{i,j,k}^{u\tau}+h_{i,j,k}^{l\tau}\rt)=1,\;\;
    %k=1,\cdots,K,
    &  \sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \lt(h_{i,j,k}^{1u}+h_{i,j,k}^{1l}+h_{i,j,k}^{2u}+h_{i,j,k}^{2l}\rt)=1,\;k=1,\cdots,K,
    \label{eq:mixed-integer-Type2-d}\\
    & {\bm h}^{\tau u}_{k},{\bm h}^{\tau l}_{k}\in \{0,1\}^{(N_1-1)(N_2-1)},\;k=1,\cdots,K,\;\tau=1,2,
    % &&  h_{i,j,k}^{u}, \; h_{i,j,k}^{l}\in \{0,1\},\;i=1,\cdots,N_1-1,\; j=1,\cdots,N_2-1,\; k=1,\cdots,K,
    \label{eq:mixed-integer-Type2-e}\\
    &0\leq \alpha_{i,j}^{k1}\leq 
    h_{i,j,k}^{1u}
    +h_{i,j,k}^{1l} 
    +h_{i,j-1,k}^{1u}
    +h_{i-1,j-1,k}^{1l }
    +h_{i-1,j-1,k}^{1u }
    +h_{i-1,j,k}^{1l},\nonumber \\
    & \qquad \qquad \qquad  \qquad
i=1,\cdots,N_1,\; j=1,\cdots,N_2, \; k=1,\cdots,K,\\
&0\leq \alpha_{i,j}^{k2}\leq  h^{2u}_{i-1,j,k}  + h^{2l}_{i-1,j,k} +  h^{2u}_{i-1,j-1,k}+h^{2l}_{i,j-1,k}+h^{2u}_{i,j-1,k}+h^{2l}_{i,j,k}, \nonumber \\
& \qquad \qquad \qquad  \qquad i=1,\cdots,N_1,j=1,\cdots,N_2,k=1,\cdots,K,
\label{eq:mixed-integer-Type2}
\end{align}
\end{subequations}
where variables $\alpha_{i,j}^{k1}$, $h_{i,j,k}^{1u}$, $h_{i,j,k}^{1l}$ represent the Type-1 PLA case,
and $\alpha_{i,j}^{k2}$, $h_{i,j,k}^{2u}$, $h_{i,j,k}^{2l}$ 
%and
 %(\ref{eq:mixed-integer-R2-c})-(\ref{eq:mixed-integer-R2-e}) where $\alpha_{i,j}^{k}$, $h_{i,j,k}^{u}$, $h_{i,j,k}^{l}$  are replaced by 
 %$\alpha_{i,j}^{k2}$, $h_{i,j,k}^{u2}$, %$h_{i,j,k}^{l2}$,
 %and (\ref{eq:constraint-alpha}) to
represent the Type-2 case.
%A new 
The constraint (\ref{eq:mixed-integer-Type2-d})
indicates that only one type partition is used for each cell.
%can be added to indicate which partition is used for each cell:
% $
% \sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \lt(h_{i,j,k}^{u1}+h_{i,j,k}^{l1}+h_{i,j,k}^{u2}+h_{i,j,k}^{l2}\rt)=1.
% $
% \item[(iii)] %{\color{blue}
% We can extend model (\ref{eq:PRO_MILP_eqi}) to the case where 
% %that the domain of $u$ is not a rectangle 
% %cell
% $T_{i,j}$ is an irregular 
% %shape
% polyhedron, see
% % is in irregular shape,
% % then we can divide it by triangles
% % and use a binary variable $h_{t}$ to identify if point $(x,y)$ lies in the $t$-th triangles,
% % see this case in 
% Figure~\ref{fig:IPLA-irregular}.
%\cref{fig:alpha_ij}~(c).
%}
% \item[(v)] {\color{purple}
% For fixed $\bdz$, ${\bm h}^u$,
% ${\bm h}^l$,
% we know the inner problem of (\ref{eq:PRO_MILP_eqi}) is an LP,
% and then we can reformulate (\ref{eq:PRO_MILP_eqi}) as a single MILP,
% please see Theorem \ref{thm:Single-MILP} in Appendix~\ref{app:Single-MILP} for the case without convex/concave constraints (\ref{eq-traform-concave1})-(\ref{eq-traform-concave2}).}
\end{itemize}
\end{remark}

\vspace{-2em}
\begin{figure}[!hbpt]
  \centering
%      \subfigure[convex coefficients of $u(x,y)$ and $x,y$]{
% %     \label{subfig-main-contuor-ptb10} %% label for second subfigure
%     \includegraphics[width=3in]{convex-1.pdf}
%   }
   \subfigure[]
   {
  \label{fig:Type1IPLA}
  %% label for second subfigure
    \includegraphics[width=0.4\linewidth]{alpha_ij.pdf}
    \hspace{0.2cm}
  }
   \subfigure[]
   {
    \label{fig:Type2IPLA} %% label for second subfigure
    \includegraphics[width=0.4\linewidth]{alpha_ij2.pdf}
  }
%      \subfigure[Irregular case]
%   {
%   \label{fig:IPLA-irregular}
%   %% label for second subfigure
%     \includegraphics[width=1.4in]{irregular.pdf}
%   }
  \vspace{-1em}
  \captionsetup{font=footnotesize}
  \caption{\footnotesize 
  (a) \& (b) represent the bi-attribute case %where the domain of $u$ is regular
  over $T_{i,j}$.
  They show the six triangles related to point $(x_i,y_j)$.
%   (c) depicts the case that the domain of $u$ is %not 
%   irregular,
%   there are five triangles related to $v_4$,
%   and the related binary variables are $h_1$, $h_2$, $h_3$,
%   $h_4$, $h_5$.
  }
  \vspace{-0.5cm}
  \label{fig:alpha_ij} %% label for entire figure
\end{figure}


% If $\bdf(\bdz,\bdxi)$ is linear in $\bdz$,
% then the inner minimization problem is an MILP.
% For the above problem,
% we use derivative-free method to solve it.
%{\color{blue}

\subsubsection{Single mixed-integer reformulation of \texorpdfstring{(\ref{eq:PRO_MILP_eqi})}{}}
%{\color{blue}

By deriving the Lagrange dual of the inner minimization problem of (\ref{eq:PRO_MILP_eqi})
which is established under Type-1 PLA, we can recast the maximin problem as a 
%Problem (\ref{eq:PRO_MILP_eqi}) can also be solved by a 
single MILP when ${\bm f}(\cdot,\bdxi)$ is linear.
%in $\bdz$.
% Note that 
% %and we present it in the following proposition.
% %Different from 
% Hu et al.~\cite{hu2022distributionally}
% also 
% %who 
% considered a single MILP reformulation for multi-attribute utility with additive form,
% we discuss the general utility function in bi-attribute case
% without this restriction.


\begin{proposition}
[Reformulation of (\ref{eq:PRO_MILP_eqi})]
\label{prop:single-MILP}
Problem (\ref{eq:PRO_MILP_eqi})
%without constraints (\ref{eq-traform-concave1})-(\ref{eq-traform-concave2}) 
can be reformulated as a single MILP when ${\bm f}(\bdz,\bdxi)$ is linear in $\bdz$,
% Then we have the PRO model (\ref{eq:PRO_MILP_eqi})
% can be reformulated as a single MILP
\begin{subequations}
\label{eq:PRO_MILP_single}
\hspace{-0.5cm}
\begin{align}
\displaystyle \max_{\substack{{\bm z}\in Z, {\bm \alpha},
{\bm h}^u, {\bm h}^l\\
{\bm \lambda}^1,
{\bm \lambda}^2,
{\bm \eta}^1\\
{\bm \eta}^2,
{\bm \tau}, {\bm \zeta}}} \; &  -\sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2}\eta_{i,j}^1L(x_{i+1}-x_i)
-\sum_{i=1}^{N_1}\sum_{j=1}^{N_2-1} \eta_{i,j}^2 L(y_{j+1}-y_j) 
+ \sum_{k=1}^Kp_k \alpha_{N_1,N_2}^k
\nonumber  \\
& 
-\lambda_{N_1-1,N_2}^1
-\lambda_{N_1,N_2-1}^2
+\eta_{N_1-1,N_2}^1
+\eta_{N_1,N_2-1}^2  +\tau_{N_1-1,N_2-1}
 +{\bm \zeta}^T {\bm Q}_{N_1,N_2} \\
{\rm s.t.} \quad\;\;\,  
&
\sum_{k=1}^Kp_k\alpha_{i,j}^k
 +\lambda_{i,j}^1-\lambda_{i-1,j}^1+\lambda_{i,j}^2-\lambda_{i,j-1}^{2}+\eta_{i-1,j}^1 -\eta_{i,j}^1+\eta_{i,j-1}^2-\eta_{i,j}^2 
 \nonumber\\
 & 
 \quad  + \tau_{i,j}+\tau_{i-1j-1}
 -\tau_{i,j-1} -\tau_{i-1,j}
+{\bm \zeta}^T{\bm Q}_{i,j}
\geq  0,
i\in {\cal I}, j\in {\cal J},\\
& \sum_{k=1}^Kp_k\alpha_{N_1,j}^k
-\lambda_{N_1-1,j}^1
+\lambda_{N_1,j}^2
-\lambda_{N_1,j-1}^2
+\eta_{N_1-1,j}^1
% -\eta_{N_1j}^1
+\eta_{N_1,j-1}^2
-\eta_{N_1,j}^2
\nonumber \\
& 
\quad  +\tau_{N_1-1,j-1}
-\tau_{N_1-1,j} +{\bm \zeta}^T {\bm Q}_{N_1,j}\geq  0, j\in {\cal J},\\
& \sum_{k=1}^Kp_k\alpha_{1,j}^k
+\lambda_{1,j}^1
+\lambda_{1,j}^2
-\lambda_{1,j-1}^2
-\eta_{1,j}^1
+\eta_{1,j-1}^2
-\eta_{1,j}^2
+\tau_{1,j}
-\tau_{1,j-1} \nonumber \\
&
\quad  +{\bm \zeta}^T {\bm Q}_{1,j} \leq  0, j\in {\cal J}, \\
& \sum_{k=1}^Kp_k \alpha_{i,N_2}^k
+ \lambda_{i,N_2}^1
-\lambda_{i-1,N_2}^1
-\lambda_{i,N_2-1}^2
+\eta_{i-1,N_2}^1
-\eta_{i,N_2}^1
+\eta_{i,N_2-1}^2 
 \nonumber \\
& 
\quad  +\tau_{i-1,N_2-1}
-\tau_{i,N_2-1}
 +{\bm \zeta}^T {\bm Q}_{i,{N_2}}
\geq  0, i\in {\cal I},\\
& \sum_{k=1}^Kp_k \alpha_{i,1}^k
+ \lambda_{i,1}^1
-\lambda_{i-1,1}^1
+\lambda_{i,1}^2
+\eta_{i-1,1}^1
-\eta_{i,1}^1
-\eta_{i,1}^2 
+\tau_{i,1}
-\tau_{i-1,1}\nonumber \\
&
\quad  +{\bm \zeta}^T {\bm Q}_{i,{N_2}}
\geq  0, i\in {\cal I}\\
& \sum_{k=1}^Kp_k \alpha_{1,1}^k
+ \lambda_{1,1}^1
+\lambda_{1,1}^2
-\eta_{1,1}^1
-\eta_{1,1}^2 
+\tau_{1,1}
 +{\bm \zeta}^T {\bm Q}_{i,{N_2}}
\geq  0,\\
& \sum_{k=1}^Kp^k\alpha_{N_1,1}^k-\lambda_{N_1-1,1}^1+\lambda_{N_1,1}^2+\eta_{N_1-1,1}^1-\eta_{N_1,1}^2-\tau_{N_1-1,1}\geq 0,\\
& \sum_{k=1}^Kp^k\alpha_{1,N_2}^k+\lambda_{1,N_2}^1-\lambda_{1,N_2-1}^2-\eta_{1,N_2}^1+\eta_{1,N_2-1}^2-\tau_{1,N_2-1}\geq 0,\\
% &&
% \sum_{k=1}^Kp_k\alpha_{1,1}^k
% -\frac{\theta_{2,1}^1}{x_2-x_1}
% -\frac{\theta_{1,2}^2}{y_2-y_1}
% +\lambda_{1,1}^1
% +\lambda_{1,1}^2
% -\eta_{1,1}^1
% -\eta_{1,1}^2
% +\tau_{1,1} 
%  +\sum_{m=1}^{M}\zeta_m Q_{1,1}^m  
%  {\color{red}\leq  0}, \\
% && +\sum_{k=1}^Kp_k\alpha_{1,2}^k
% -\frac{\theta_{2,2}^1}{x_2-x_1}
% +\frac{\theta_{1,2}^2}{y_2-y_1}
% -\frac{\theta_{1,3}^2}{y_3-y_2}
% +\frac{\theta_{1,2}^2}{y_3-y_2}
% +\lambda_{1,2}^1
% +\lambda_{1,2}^2
% -\lambda_{1,1}^2
% -\eta_{1,2}^1
% +\eta_{1,1}^2
% -\eta_{1,2}^2\nonumber \\
% && 
% +\tau_{1,2}
% -\tau_{1,1} 
%  +\sum_{m=1}^{M}\zeta_m Q_{1,2}
% {\color{red}\leq  0}, \\
% &&
% +\sum_{k=1}^Kp_k\alpha_{2,1}^k
% -\frac{\theta_{2,1}^1}{x_3-x_2}
% -\frac{\theta_{2,1}^1}{x_2-x_1}
% +\frac{\theta_{3,1}^1}{x_3-x_2} -\frac{\theta_{2,2}^2}{y_2-y_1}
% +\lambda_{2,1}^1
% -\lambda_{1,1}^1
% +\lambda_{2,1}^2
% +\eta_{1,1}^1
% -\eta_{2,1}^1
% -\eta_{2,1}^2
%  \nonumber \\
% &&
% +\tau_{2,1}
% -\tau_{1,1}  +\sum_{m=1}^{M}\zeta_m Q_{2,1}^m
% {\color{red}\leq  0},\\
% &&
% \sum_{k=1}^Kp_k\alpha_{2,2}^k
% -\frac{\theta_{2,2}^1}{x_3-x_2}
% -\frac{\theta_{2,2}^1}{x_2-x_1} +\frac{\theta_{2,2}^2}{y_2-y_1}
% -\frac{\theta_{2,3}^2}{y_3-y_2}
% +\frac{\theta_{2,2}^2}{y_3-y_2}
% +\lambda_{2,2}^1
% -\lambda_{1,2}^1
% +\lambda_{2,2}^2
% -\lambda_{2,1}^2
% \nonumber \\
% && 
% +\eta_{1,2}^1
% -\eta_{2,2}^1
% +\eta_{2,1}^2
% -\eta_{2,2}^2 
% +\tau_{2,2}
% +\tau_{1,1}
% -\tau_{2,1}
% -\tau_{1,2} 
% +\sum_{m=1}^{M}\zeta_m Q_{2,2}^m 
% {\color{red}\leq  0}, \\
% &&
% + \sum_{k=1}^Kp_k\alpha_{1,N_2}^k
% +\frac{\theta_{2N_2}^1}{x_{2}-x_1}
% -\frac{\theta_{1N_2-1}^2}{y_{N_2}-y_{N_2-1}}
% +\lambda_{1N_2}^1
% -\lambda_{1,N_2-1}^2
% -\eta_{1N_2}^1
% +\eta_{1N_2-1}^2
% -\tau_{1N_2-1} 
%  +\sum_{m=1}^{M}\zeta_m Q_{1,N_2}^m {\color{red}\leq  0}, \\
% &&
% + \sum_{k=1}^Kp_k\alpha_{1,N_2-1}^k
% +\frac{\theta_{2,N_2-1}^1}{x_{2}-x_1} +\frac{\theta_{1,N_2-1}^2}{y_{N_2-1}-y_{N_2-2}}
% -\frac{\theta_{1,N_2-2}^2}{y_{N_2-1}-y_{N_2-2}}
% +\frac{\theta_{1,N_2-1}^2}{y_{N_2}-y_{N_2-1}}
% +\lambda_{1,N_2-1}^1
% +\lambda_{1,N_2-1}^2
% \nonumber \\
% && 
% -\lambda_{1,N_2-2}^2
% -\eta_{1,N_2-1}^1
% +\eta_{1,N_2-2}^2
% -\eta_{1,N_2-1}^2
% +\tau_{1,N_2-1}
% -\tau_{1,N_2-2}
%  +\sum_{m=1}^{M}\zeta_m Q_{1,N_2-1}^m {\color{red}\leq  0}, \\
% &&
% + \sum_{k=1}^Kp_k\alpha_{2,N_2}^k
% -\frac{\theta_{2,N_2}^1}{x_3-x_2}
% -\frac{\theta_{2N_2}^1}{x_2-x_1}
% +\frac{\theta_{3N_2}^1}{x_3-x_2}
% -\frac{\theta_{2N_2-1}^2}{y_{N_2}-y_{N_2-1}}
% +\lambda_{2N_2}^1
% -\lambda_{1,N_2}^1
% -\lambda_{2,N_2-1}^2 \\
% &&
% +\eta_{1,N_2}^1
% -\eta_{2,N_2}^1
% +\eta_{2,N_2-1}^2
% +\tau_{1,N_2-1}
% -\tau_{2,N_2-1}
%  +\sum_{m=1}^{M}\zeta_m Q_{2,N_2}^m {\color{red}\leq  0},\\
% &&
% + \sum_{k=1}^Kp_k\alpha_{2,N_2-1}^k
% -\frac{\theta_{2,N_2-1}^1}{x_{3}-x_2}
% -\frac{\theta_{2N_2-1}^1}{x_2-x_1}
% +\frac{\theta_{3,N_2-1}^1}{x_3-x_2}
% +\frac{\theta_{2,N_2-1}^2}{y_{N_2-1}-y_{N_2-2}} -\frac{\theta_{2,N_2-2}^2}{y_{N_2-1}-y_{N_2-2}}
% +\frac{\theta_{2,N_2-1}^2}{y_{2N_2}-y_{N_2-1}} \nonumber \\
% &&+\lambda_{2,N_2-1}^1
% -\lambda_{1,N_2-1}^1
% +\lambda_{2,N_2-1}^2
% -\lambda_{2,N_2-2}^2
% +\eta_{1,N_2-1}^1
% -\eta_{2,N_2-1}^1
% +\eta_{2,N_2-2}^2
% -\eta_{2,N_2-1}^2 \nonumber \\
% &&
% +\tau_{2,N_2-1}
% +\tau_{1,N_2-2}
% -\tau_{2,N_2-2}
% -\tau_{1,N_2-1}
%  +\sum_{m=1}^{M}\zeta_m Q_{2,N_2-1}^m
% {\color{red}\leq  0},\\
% &&
% + \sum_{k=1}^Kp_k\alpha_{N_1-1,2}^k
% +\frac{\theta_{N_1-2,2}^1}{x_{N_1-1}-x_{N_2-2}}
% -\frac{\theta_{N_1-1,2}^1}{x_{N_1}-x_{N_1-1}}
% -\frac{\theta_{N_1-1,2}^1}{x_{N_1-1}-x_{N_1-2}}
% +\frac{\theta_{N_1-1,2}^2}{y_2-y_1}
% -\frac{\theta_{N_1-1,3}^2}{y_3-y_2}
% \\
% &&
% +\frac{\theta_{N_1-1,2}^2}{y_3-y_2}
% +\lambda_{N_1-1,2}^1
% -\lambda_{N_1-2,2}^1
% +\lambda_{N_1-1,2}^2
% -\lambda_{N_1-1,1}^2
% +\eta_{N_1-2,2}^1
% -\eta_{N_1-1,2}^1
% +\eta_{N_1-1,1}^2
% -\eta_{N_1-1,2}^2
% \nonumber \\
% &&
% +\tau_{N_1-1,2}
% +\tau_{N_1-2,1}
% -\tau_{N_1-1,1}
% -\tau_{N_1-2,2}
%  +\sum_{m=1}^{M}\zeta_m Q_{N_1-1,2}^m {\color{red}\leq  0}, \\
% &&
% + \sum_{k=1}^Kp_k\alpha_{N_1,2}^k
% +\frac{\theta_{N_1-1,2}^1}{x_{N_1}-x_{N_1-1}}
% +\frac{\theta_{N_1,2}^2}{y_{2}-y_1}
% -\frac{\theta_{N_1,3}^2}{y_3-y_2}
% +\frac{\theta_{N_1,2}^2}{y_3-y_2}
% -\lambda_{N_1-1,1}^1
% +\lambda_{N_1,2}^2
% -\lambda_{N_1,1}^2  \\
% &&
% +\eta_{N_1-1,2}^1
% +\eta_{N_1,1}^2
% -\eta_{N_1,2}^2
% +\tau_{N_1-1,1}
% -\tau_{N_1-1,2} 
% +\sum_{m=1}^{M}\zeta_m Q_{N_1,2}^m 
% {\color{red}\leq  0},\\
% &&
% +\sum_{k=1}^Kp_k \alpha_{N_1-1,1}^k
% +\frac{\theta_{N_1-2,1}^1}{x_{N_1-1}-x_{N_1-2}}
% -\frac{\theta_{N_1-1,1}^1}{x_{N_1}-x_{N_1-1}}
% -\frac{\theta_{N_1-1,1}^1}{x_{N_1-1}-x_{N_1-2}}
% -\frac{\theta_{N_1-1,2}^2}{y_2-y_1}
% +\lambda_{N_1-1,1}^1
% -\lambda_{N_1-2,1}^1\\
% && 
% +\lambda_{N_1-1,1}^2
% +\eta_{N_1-2,1}^1
% -\eta_{N_1-1,1}^1
% -\eta_{N_1-1,1}^2
% +\tau_{N_1-1,1}
% -\tau_{N_1-2,1} 
% +\sum_{m=1}^{M}\zeta_m Q_{N_1-1,1}^m {\color{red}\leq  0},\\
% &&
% + \sum_{k=1}^Kp_k\alpha_{N_1,1}^k
% +\frac{\theta_{N_1-1,1}^1}{x_{N_1}-x_{N_1-1}}
% -\frac{\theta_{N_1,2}^2}{y_2-y_1} -\lambda_{N_1-1,1}
% +\lambda_{N_1,1}^2
% +\eta_{N_1-1,1}^1
% -\eta_{N_1,1}^2
% -\tau_{N_1-1,1}
% +\sum_{m=1}^{M}\zeta_m Q_{N_1,1}^m 
% {\color{red}\leq  0},\\
% &&
% +\sum_{k=1}^Kp_k \alpha_{N_1-1,N_2-1}^k
% +\frac{\theta_{N_1-2,N_2-1}^1}{x_{N_1-1}-x_{N_1-2}}
% -\frac{\theta_{N_1-1,N_2-1}^1}{x_{N_1}-x_{N_1-1}}
% -\frac{\theta_{N_1-1,N_2-1}^1}{x_{N_1-1}-x_{N_1-2}}
% +\frac{\theta_{N_1-1,N_2-1}^2}{y_{N_1-1}-y_{N_1-2}}\\
% &&
% -\frac{\theta_{N_1-1,N_2}^1}{y_{N_2}-y_{N_2-1}} 
% +\frac{\theta_{N_1-1,N_2-1}^2}{y_{N_2}-y_{N_2-1}}
% +\lambda_{N_1-1N_2-1}^1
% -\lambda_{N_1-2N_2-1}^1
% +\lambda_{N_1-1N_2-1}^2
% -\lambda_{N_1-1N_2-2}^2\\
% &&
% +\eta_{N_1-2N_2-1}^1
% +\eta_{N_1-1N_2-1}^1
% +\eta_{N_1-1N_2-2}^2
% -\eta_{N_1-1N_2-1}^2
% +\tau_{N_1-1,N_2-1}
% +\tau_{N_1-2,N_2-2}
% \\
% && -\tau_{N_1-1,N_2-2}
% -\tau_{N_1-2,N_2-1}
% +\sum_{m=1}^{M}\zeta_m Q_{N_1-1,N_2-1}^m {\color{red}\leq  0},\\
% &&
% + \sum_{k=1}^Kp_k\alpha_{N_1-1,N_2}^k
% +\frac{\theta_{N_1-2,N_2}^1}{x_{N_1-1}-x_{N_1-2}} -\frac{\theta_{N_1-1,N_2}^1}{x_{N_1}-x_{N_1-1}}
% -\frac{\theta_{N_1-1,N_2}^1}{x_{N_1-1}-x_{N_1-2}}
% -\frac{\theta_{N_1-1,N_2-1}^2}{y_{N_2}-y_{N_2-1}} \\
% &&
% +\lambda_{N_1-1,N_2}^1
% -\lambda_{N_1-2,N_2}^1
% -\lambda_{N_1-1,N_2-1}^2
% +\eta_{N_1-2,N_2}^1
% -\eta_{N_1-1,N_2}^1
% +\eta_{N_1-1,N_2-1}^2
% \\
% && 
%  +\tau_{N_1-2,N_2-1}
% -\tau_{N_1-1,N_2-1}
% +\sum_{m=1}^{M}\zeta_m Q_{N_1-1,N_2}^m  {\color{red}\leq  0},\\
% &&
% + \sum_{k=1}^Kp_k\alpha_{N_1,N_2-1}^k
% +\frac{\theta_{N_1-1,N_2-1}^1}{x_{N_1}-x_{N_1-1}}
% +\frac{\theta_{N_1,N_2-1}^1}{y_{N_2-1}-y_{N_2-2}}
% -\frac{\theta_{N_1,N_2-2}^2}{y_{N_2}-1-y_{N_2-2}}
% -\frac{\theta_{N_1,N_2-2}^2}{y_{N_2-1}-y_{N_2-2}}
% \\
% &&  -\lambda_{N_1-1,N_2-1}^1
% +\lambda_{N_1,N_2-1}^2
% -\lambda_{N_1,N_2-2}^2
% +\eta_{N_1-1,N_2-1}^1
% +\eta_{N_1,N_2-2}^2
% -\eta_{N_1,N_2-1}^2
% +\tau_{N_1-1,N_2-2}
% \\
% && 
%  -\tau_{N_1-1,N_2-1}
%  +\sum_{m=1}^{M}\zeta_m Q_{N_1,N_2-1}^m {\color{red}\leq  0},\\
% % &&
% % + \alpha_{N_1,N_2}
% % +\frac{\theta_{N_1-1,N_2}^1}{x_{N_1}-x_{N_1-1}}
% % -\frac{\theta_{N_1,N_2-1}^2}{y_{N_2}-y_{N_2-1}}
% % -\lambda_{N_1-1,N_2}^1
% % -\lambda_{N_1,N_2-1}^2
% % +\eta_{N_1-1,N_2}^1
% % +\eta_{N_1,N_2-1}^2  \\
% % && 
% % +\tau_{N_1-1,N_2-1}-\sigma
% %  +\sum_{m=1}^{M}\zeta_m Q_{N_1,N_2}^m 
% %  {\color{red}\leq  0},
&\inmat{constraints } (\ref{eq:mixed-integer-R2-b}),
  (\ref{eq:mixed-integer-R2-d})-
  (\ref{eq:mixed-integer-R2-f}),\label{eq:PRO_MILP_single-j}\\
&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} x_{i}=f_1(\bdz,\bdxi^k),\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^{k} y_{j}= f_2(\bdz,\bdxi^k), \nonumber \\
& \hspace{16em} k=1,\cdots,K,\\
& {\bm \lambda}^1 \geq 0, 
{\bm \lambda}^2 \geq 0,
{\bm \eta}^1\geq 0,
{\bm \eta}^2 \geq 0,
{\bm \tau} \geq 0,
\end{align}
\end{subequations}
where ${\cal I}:=\{2,\cdots,N_1-1\}$,
${\cal J}:=\{2,\cdots,N_2-1\}$,
${\bm Q}_{ij}:=(Q_{ij}^1,\cdots,Q_{ij}^M)^T\in \R^M$,
$Q_{ij}^l:= \mathbb{P}({\bm B}_l=(x_i,y_j))-\mathbb{P}({\bm A}_l=(x_i,y_j))$,
% ${\bm \theta}^1\in \R^{(N_1-2)\times N_2}$,
% ${\bm \theta}^2\in \R^{N_1\times (N_2-2)}_+$,
${\bm \lambda}^1\in \R^{(N_1-1)\times N_2}_+$,
${\bm \lambda}^2\in \R^{N_1\times (N_2-1)}_+$,
${\bm \eta}^1\in \R^{(N_1-1)\times N_2}_+$,
${\bm \eta}^2 \in \R^{N_1\times (N_2-1)}_+$,
${\bm \tau}\in \R^{(N_1-1)\times (N_2-1)}_+$,
% $\sigma\in \R$,
${\bm \zeta}\in \R^M$.
\end{proposition}
We can also reformulate BUPRO-N with the Type-2 PLA as a single MIP.
We only need to
replace (\ref{eq:PRO_MILP_single-j}) with 
(\ref{eq:mixed-integer-R2-b}),
  (\ref{eq:mixed-integer-R2-d})-(\ref{eq:mixed-integer-R2-e}) and (\ref{eq:constraint-alpha}).
  Note that 
%and we present it in the following proposition.
%Different from 
Hu et al.~\cite{hu2022distributionally}
consider a distributionally robust model for 
the random utility maximization problem 
in multi-attribute decision making 
and reformulate a maximin PRO 
as a single MILP. 
The main difference is that they considered 
the true utility function to be 
in additive form (sum of the single-attribute utility functions).
%utility function 
Here we consider a general   
multivariate true utility function.
Thus, we believe this is a step forward
from computational perspective
in handling BUPRO-N. 
Note that in this formulation, we have not incorporated  
Assumption~\ref{A:concave-in-x-and-y}
%the convexity/concavity of single-variate utility functions $u_N(\cdot,\hat{y})$ and $u_N(\hat{x},\cdot)$ for all $\hat{x}\in X$ and $\hat{y}\in Y$ is not considered for the convenience of comparison of the three models (maximin EPLA, maximin IPLA and single MILP using IPLA) 
because the dual formulation of the problem with 
the convexity/concavity constraints 
would be very complex.

%also 
%who 
% considered a single MILP reformulation for multi-attribute utility with additive form,
% we discuss the general utility function in bi-attribute case
% without this restriction.
  
  
% Since the feasible set of the above problem  
% $
% \left\{({\bm \alpha}, {\bm h}^{u}, {\bm h}^{l}): (\ref{eq:PRO_MILP_mina-b})-(\ref{eq:PRO_MILP_mina-c}) \right\}
%   $ is singleton,
%   which is denoted by $({\bm \alpha}_1^*, {\bm h}_1^{u*}, {\bm h}_1^{l*})$,
%   we have $(\bdz^*_1,{\bm \alpha}_1^*, {\bm h}_1^{u*}, {\bm h}_1^{l*})$ is a feasible point of problem 
%  \bgeq
% & \displaystyle \max_{{\bm z\in Z},{\bm \alpha},{\bm h}^{u},{\bm h}^{l}} \min_{\bm u}
% & \left\{ \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} u_{ij}: (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d})\right\}.\\
% &{\rm s.t.}&
%  (\ref{eq:mixed-integer-R2-b}),
%   (\ref{eq:mixed-integer-R2-d})-(\ref{eq:mixed-integer-R2-f}),\\
% &&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} x_{i}=f_1(\bdz,\bdxi^k),\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} y_{j}= f_2(\bdz,\bdxi^k),\;\; k=1,\cdots,K.
% \edeq
% Thus we have the optimal value ${\vt}^*_2$ of problem (\ref{eq:PRO_MILP_eqi}) is greater than ${\vt}_1^*$.
% Conversely,
% Let  $(\bdz^*_2,{\bm \alpha}_2^*, {\bm h}_2^{u*}, {\bm h}_2^{l*})$ be the optimal solution of problem (\ref{eq:PRO_MILP_eqi}),
% and the corresponding solution of problem 
% $\min_{\bm u}
%  \left\{ \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k*} u_{ij}: (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d})\right\}
% $ is ${\bm u}^*$.
% We observe that ${\bm u}^*$ satisfies constraints (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d}),
% $(\bdz^*_2,{\bm \alpha}_2^*, {\bm h}_2^{u*}, {\bm h}_2^{l*})$ satisfies
% (\ref{eq:PRO_MILP_mina-b})-(\ref{eq:PRO_MILP_mina-c}),
% and thus $\bdz^*_2$ is a feasible point of problem (\ref{eq:PRO-MILP})
% and the corresponding objective value of (\ref{eq:PRO-MILP}) equals to 
% $$
% \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k*} u_{ij}^*=\vt^*_2.
% $$
% Hence $\vt_1^*\geq \vt_2^*$.
% Therefore  the optimal value and optimal solution of problems (\ref{eq:PRO-MILP}) and (\ref{eq:PRO_MILP_eqi}) are the same.
% \hfill
% \Box
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \noindent{\bf Proof.}
% First,
% we observe  that constraints (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d}) are only related to variable ${\bm u}$,
% constraints (\ref{eq:mixed-integer-R2-b}),
%   (\ref{eq:mixed-integer-R2-d})-(\ref{eq:mixed-integer-R2-f}) are for ${\bm \alpha}$,
%   ${\bm h}^u$, ${\bm h}^l$,
%   and $\bdz$ needs to satisfy 
%   constraint (\ref{eq:PRO-MILP-d}).
% Let ${\bm z}_1^*$ and ${\vt}_{1}^*$ be the optimal solution and  optimal value of problem (\ref{eq:PRO-MILP}).
% Let 
% \begin{subequations}
% \label{eq:PRO_MILP_mina}
% \begin{eqnarray}
% v_{\rm inner}({\bm z}_1^*)
%  :=& \displaystyle \min_{{\bm \alpha},{\bm h}^u,{\bm h}^l}&  \min_{{\bm u}}\left\{
% \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} u_{ij}: (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d})) \right \}\\
% &{\rm s.t.}&
%  (\ref{eq:mixed-integer-R2-b}),
%   (\ref{eq:mixed-integer-R2-d})-(\ref{eq:mixed-integer-R2-f}),\label{eq:PRO_MILP_mina-b}\\
% &&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} x_{i}=f_1(\bdz_1^*,\bdxi^k),\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} y_{j}= f_2(\bdz_1^*,\bdxi^k),\;\; k=1,\cdots,K.~~
% \label{eq:PRO_MILP_mina-c}
% \qquad 
% \end{eqnarray}
% \end{subequations}
% Since the feasible set of the above problem  
% $
% \left\{({\bm \alpha}, {\bm h}^{u}, {\bm h}^{l}): (\ref{eq:PRO_MILP_mina-b})-(\ref{eq:PRO_MILP_mina-c}) \right\}
%   $ is singleton,
%   which is denoted by $({\bm \alpha}_1^*, {\bm h}_1^{u*}, {\bm h}_1^{l*})$,
%   we have $(\bdz^*_1,{\bm \alpha}_1^*, {\bm h}_1^{u*}, {\bm h}_1^{l*})$ is a feasible point of problem 
%  \bgeq
% & \displaystyle \max_{{\bm z\in Z},{\bm \alpha},{\bm h}^{u},{\bm h}^{l}} \min_{\bm u}
% & \left\{ \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} u_{ij}: (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d})\right\}.\\
% &{\rm s.t.}&
%  (\ref{eq:mixed-integer-R2-b}),
%   (\ref{eq:mixed-integer-R2-d})-(\ref{eq:mixed-integer-R2-f}),\\
% &&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} x_{i}=f_1(\bdz,\bdxi^k),\;\; \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} y_{j}= f_2(\bdz,\bdxi^k),\;\; k=1,\cdots,K.
% \edeq
% Thus we have the optimal value ${\vt}^*_2$ of problem (\ref{eq:PRO_MILP_eqi}) is greater than ${\vt}_1^*$.
% Conversely,
% Let  $(\bdz^*_2,{\bm \alpha}_2^*, {\bm h}_2^{u*}, {\bm h}_2^{l*})$ be the optimal solution of problem (\ref{eq:PRO_MILP_eqi}),
% and the corresponding solution of problem 
% $\min_{\bm u}
%  \left\{ \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k*} u_{ij}: (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d})\right\}
% $ is ${\bm u}^*$.
% We observe that ${\bm u}^*$ satisfies constraints (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d}),
% $(\bdz^*_2,{\bm \alpha}_2^*, {\bm h}_2^{u*}, {\bm h}_2^{l*})$ satisfies
% (\ref{eq:PRO_MILP_mina-b})-(\ref{eq:PRO_MILP_mina-c}),
% and thus $\bdz^*_2$ is a feasible point of problem (\ref{eq:PRO-MILP})
% and the corresponding objective value of (\ref{eq:PRO-MILP}) equals to 
% $$
% \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k*} u_{ij}^*=\vt^*_2.
% $$
% Hence $\vt_1^*\geq \vt_2^*$.
% Therefore  the optimal value and optimal solution of problems (\ref{eq:PRO-MILP}) and (\ref{eq:PRO_MILP_eqi}) are the same.
% \hfill
% \Box
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









% \bgeq
% \displaystyle ({\bm \alpha}^*, {\bm h}^{u*}, {\bm h}^{l*})
% &=\displaystyle \arg\min_{({\bm \alpha}, {\bm h}^{u}, {\bm h}^{l})} &
% \displaystyle \left\{ \min_{{\bm u}}
% \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} u_{ij}: (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d})) \right \} \\
% &{\rm s.t.}& (\ref{eq:PRO_MILP_mina-b})-(\ref{eq:PRO_MILP_mina-c}).\\
% &= \displaystyle \arg\max_{({\bm \alpha}, {\bm h}^{u}, {\bm h}^{l})} &
%  \left\{ \min_{{\bm u}}
% \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} u_{ij}: (\ref{eq:PRO-MILP-c})-(\ref{eq:PRO-MILP-d})) \right \} \\
%  & {\rm s.t.} & (\ref{eq:PRO_MILP_mina-b})-(\ref{eq:PRO_MILP_mina-c}).
% \edeq
% Then 
% \bgeq
% \left\{\begin{array}{cl}
% \displaystyle \min_{\bm u}& \displaystyle \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k*} u_{ij}\\ 
% {\rm s.t.}& (\ref{eq:PRO-MILP-b})-(\ref{eq:PRO-MILP-d}).
% \end{array}
% \right.
% \Leftrightarrow
% \left\{\begin{array}{cl}
% \displaystyle \min_{{\bm \alpha},{\bm h}^u,{\bm h}^l}\min_{\bm u}& \displaystyle \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} u_{ij}\\ 
% {\rm s.t.}& (\ref{eq:PRO-MILP-b})-(\ref{eq:PRO-MILP-d}).
% \end{array}
% \right.
% \Leftrightarrow
% \left\{\begin{array}{cl}
% \displaystyle \max_{{\bm \alpha},{\bm h}^u,{\bm h}^l}\min_{\bm u}& \displaystyle \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} u_{ij}\\ 
% {\rm s.t.}& (\ref{eq:PRO-MILP-b})-(\ref{eq:PRO-MILP-d}).
% \end{array}
% \right.
% \edeq
% Consequently,  (\ref{eq:PRO-MILP}) can be equivalently written as 
% \bgeq
% \max_{{\bm z\in Z},{\bm \alpha}, {\bm h}^{u}, {\bm h}^{l}} \min_{\bm u} \left\{ \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{ij}^{k} u_{ij}: (\ref{eq:PRO-MILP-b})-(\ref{eq:PRO-MILP-d})\right\}.
% \edeq


% }




% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% {\color{blue}
% THIS PARAGRAPH NEEDS TO UPDATE.

% we will discuss the PLA from an  MILP perspective.
% There are two advantages to use binary variables to formulate the PLA.
% First,
% we can generate the bi-attribute robust utility maximin problem to three-attribute or general $m$-attribute cases in similar ways.
% Second,
% instead of solving the utility preference robust problem 
% %by 
% using derivative-free method,
% we will equivalently formulated it as a maximization problem 
% using
% %by
% dualization technique,
% and then introduce binary variable to obtain an MILP formulation of the utility preference robust optimization problem when $f(\bdz,\bdxi)$ is linear in $\bdz$.
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tri-attribute case}
\label{sec:three-dim-u}

We now extend our discussions 
on the implicit PLA of UPRO to the tri-attribute case.


\subsubsection{Triangulation of a cube and interpolation}

% There is are two types dividion of a rectangle into non-overlapping simplices in two-dimensional case. 

We follow the well-known triangulation method (see e.g.
% Chien and Kuh 
\cite{chien1977solving,meyer2005convex,misener2010piecewise}) to 
divide each
cube into six non-overlapping simplices. 
%(see \cite{meyer2005convex}).
There are six ways to divide, 
{\color{black}and}
here we 
use the second way (called Type B in the references).
% The three dimensional case has 
% six representative classes that divide a
% cube into six non-overlapping simplices (see \cite{meyer2005convex}).
% Next, we  choose triangulation type B,
% since 
% only the
% three planes 
% need to be considered to isolate a point in a small
% cube into a particular simplex,
% and each of the six simplices in triangulation
% type B have equal volume in the case of uniform partitioning, increasing the
% accuracy of the interpolation (see \cite{misener2010piecewise}).
Specifically, we consider $u:[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]\times [\underline{z},\bar{z}]\rightarrow \R$
% Let ${\cal X}:=\{x_i,i=1,\ldots,N_1\}{\color{red}\subset X}$ and 
% ${\cal Y}:=\{y_j,j=1,\ldots,N_2\}\subset Y$ with 
% $\underline{x}=x_1<\ldots<x_{N_1}=\bar{x}$ and $\underline{y}=y_1<\ldots<y_{N_2}=\bar{y}$.
% We define $\calx\times \caly:=\{(x_i,y_j), x_i\in {\cal X},y_j\in {\cal Y}\}$ as a set of $N_1N_2$ gridpoints. 
with %gridpoints 
$\underline{x}=x_1< x_2 < \cdots< x_{N_1}=\bar{x}$,
$\underline{y}=y_1< y_2 < \cdots< y_{N_2}=\bar{y}$
and $\underline{z}=z_1 < z_2< \cdots< z_{N_3}=\bar{z}$.
Let $X_i:=(x_i,x_{i+1}]$,
$Y_j:=(y_j,y_{j+1}]$
and $Z_l:=(z_l,z_{l+1}]$.
For any given point $(x,y,z)\in X_i\times Y_j\times Z_l$, 
%point,
%given $(\bar{x},\bar{y},\bar{z})$,
%with $x_i\leq {x}\leq x_{i+1}$,
%$y_j\leq {y} \leq y_{j+1}$ and 
%$z_l\leq {z}\leq z_{l+1}$,
consider the cube with vertices 
% (we will replace the points by corresponding numbers in the following Figures)
$1$: $(x_i,y_j,z_l)$, $2$: $(x_{i+1},y_j,z_l)$, $3$: $(x_{i},y_{j+1},z_l)$ $4$: $(x_{i+1},y_{j+1},z_l)$,
$5$: $(x_i,y_j,z_{l+1})$, $6$: $(x_{i+1},y_j,z_{l+1})$, $7$: $(x_{i},y_{j+1},z_{l+1})$, $8$: $(x_{i+1},y_{j+1},z_{l+1})$.
% By seven diagonals connecting different vertices,
% \bgeq
% &&[(x_i,y_j,z_l), (x_{i+1},y_{j+1},z_{l+1})]\;(\inmat{line: } 1\rightarrow 8),\;\; \quad
% [(x_i,y_j,z_l), (x_{i},y_{j+1},z_{l+1})]\;(\inmat{line: } 1\rightarrow 7),\\
% &&[(x_i,y_j,z_l), (x_{i+1},y_{j+1},z_{l})]\;(\inmat{line: } 1\rightarrow 4),\;\;\quad \quad 
% [(x_i,y_j,z_l), (x_{i+1},y_{j},z_{l+1})]\;(\inmat{line: } 1\rightarrow 6),\\
% &&[(x_{i+1},y_j,z_l), (x_{i+1},y_{j+1},z_{l+1})]\;(\inmat{line: } 2\rightarrow 8),\;\;
% [(x_{i},y_j,z_{l+1}), (x_{i+1},y_{j+1},z_{l+1})]\;(\inmat{line: } 5\rightarrow 8),\\
% &&[(x_{i},y_{j+1},z_{l}), (x_{i+1},y_{j+1},z_{l+1})]\;(\inmat{line: } 3\rightarrow 8),
% \edeq
We first divide a cube $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]\times [\underline{z},\bar{z}]$ in $\R^3$ into two parts,
denoted by 
Part $1$-$2$-$4$-$5$-$6$-$8$
and Part $1$-$3$-$4$-$5$-$7$-$8$.
Then we can produce six simplices by three planes,
see 
%Figures~\ref{fig-divisioin-6}~(a) (b), 
Figures~\ref{fig-divisioin-6-1} $\&$ \ref{fig-divisioin-6-2}.
% Specifically,
% in Figure~\ref{fig-divisioin-6}~(a)~(b),
% we  divide the cube into two parts by cutting through the plane of vertices $1$-$4$-$5$-$8$,
% denoted by 
% parts $1$-$2$-$4$-$5$-$6$-$8$ and 
% $1$-$3$-$4$-$5$-$7$-$8$.
% In Figure~\ref{fig-divisioin-6-1}~(a),
% we cut the part $1$-$2$-$4$-$5$-$6$-$8$ by plane of vertices $1$-$2$-$8$ (plane with red dot),
% and get the first simplex $1$-$2$-$4$-$8$ (red color).
% In Figure\ref{fig-divisioin-6-1}~(b) \& (c),
% we go on to cut the rest part by plain $1$-$2$-$8$ (plane with green dot),
% and obtain the second simplex $1$-$2$-$6$-$8$ (green color)
% and the third simplex $1$-$5$-$6$-$8$ (purple corlor).
% \vspace{-0.2cm}
% \begin{figure}[!htbp]
%   \centering
%   \subfigure[Part $1$-$2$-$4$-$5$-$6$-$8$]{
% %     \label{subfig-main-contuor-ptb10} %% label for second subfigure
%     \includegraphics[width=1.8in]{division6-01.pdf}
%   }
%   \subfigure[Part $1$-$3$-$4$-$5$-$7$-$8$]{
% %     \label{subfig-main-contuor-ptb15}
%     \includegraphics[width=1.8in]{division6-02.pdf}
%   }
%   \subfigure[]{
% %     \label{subfig-main-contuor-ptb5} %% label for second subfigure
%     \includegraphics[width=1.9in]{I-VIII.pdf}
%   }
% %   \caption{}
%   \caption{
%   \footnotesize 
%   (a)-(b) Divide a cube $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]\times [\underline{z},\bar{z}]$ in $\R^3$ into two parts.
%   (c)Divide a cube into $8$ sub-cubes in octants I, II, III, IV, V, VI, VII and VIII.}
%   \label{fig-divisioin-6} %% label for entire figure
% \end{figure}
% 
\begin{figure}[!ht]
\vspace{-0.1cm}
  \centering
   \subfigure[{\color{red}Red} simplex $1$-$2$-$4$-$8$]{
     \label{fig:3a} %% label for second subfigure
    \includegraphics[width=0.3\linewidth]{division6-1.pdf}
  }
   \subfigure[{\color{PineGreen}Green} simplex $1$-$2$-$6$-$8$]{
     \label{fig:3b} %% label for second subfigure
    \includegraphics[width=0.3\linewidth]{division6-2.pdf}
  }
   \subfigure[{\color{purple}Purple} simplex $1$-$5$-$6$-$8$]{
     \label{fig:3c}
    \includegraphics[width=0.3\linewidth]{division6-3.pdf}
  }
   \vspace{-0.3cm}
  \captionsetup{font=footnotesize}
  \caption{\footnotesize
  Divide the part $1$-$2$-$4$-$5$-$6$-$8$ into three simplices in $\R^3$.
% (a) $\&$ (b) divide the cube into two parts by cutting through the plane of vertices $1$-$4$-$5$-$8$,
% denoted by 
% parts $1$-$2$-$4$-$5$-$6$-$8$ and 
(a) cuts the part $1$-$2$-$4$-$5$-$6$-$8$ by plane with vertices $1$-$2$-$8$,
%(with red dotted lines),
and get the first simplex $1$-$2$-$4$-$8$ (red color).
(b) \& (c) go on to cut the rest part by plain $1$-$2$-$8$,
%(with green dotted lines),
and obtain the second simplex $1$-$2$-$6$-$8$ (green color)
and the third simplex $1$-$5$-$6$-$8$ (purple color).}
\label{fig-divisioin-6-1} %% label for entire figure
\end{figure}

\begin{figure}[!ht]
\vspace{-0.5cm}
  \centering
   \subfigure[{\color{red}Red} simplex $1$-$5$-$7$-$8$]{
     \label{fig:4a} %% label for second subfigure
    \includegraphics[width=0.3\linewidth]{division6-4.pdf}
  }
   \subfigure[{\color{PineGreen}Green} simplex $1$-$3$-$7$-$8$]{
     \label{fig:4b} %% label for second subfigure
    \includegraphics[width=0.3\linewidth]{division6-5.pdf}
  }
   \subfigure[{\color{purple}Purple} simplex $1$-$3$-$4$-$8$]{
     \label{fig:4c}
    \includegraphics[width=0.3\linewidth]{division6-6.pdf}
  }
  \vspace{-0.2cm}
  \captionsetup{font=footnotesize}
  \caption{\footnotesize Divide the part $1$-$3$-$4$-$5$-$7$-$8$ into three simplices in $\R^3$.
(a) cuts the part $1$-$3$-$4$-$5$-$7$-$8$ by plane of vertices $1$-$7$-$8$,
%(plane with red dot),
and get the first simplex $1$-$5$-$7$-$8$ (red color).
(b) \& (c) go on to cut the rest part by plain $1$-$3$-$8$,
%(plane with green dot),
and obtain the second simplex $1$-$3$-$7$-$8$ (green color)
and the third simplex $1$-$3$-$4$-$8$ (purple color).}
  \vspace{-0.5cm}
  \label{fig-divisioin-6-2} %% label for entire figure
\end{figure}
% Next, we present the formulation of three planes:
% \bgeq
% && \inmat{plane}\;1\inmat{-}4\inmat{-}5\inmat{-}8:\;
% (y_{j+1}-y_j)x-(x_{i+1}-x_i)y+x_{i+1}y_j-x_iy_{j+1}=0,\\
% && \inmat{plane}\;1\inmat{-}2\inmat{-}7\inmat{-}8:\;
% (z_{l+1}-z_l) y-(y_{j+1}-y_j)z+y_{j+1}z_l-y_jz_{l+1}=0,\\
% && \inmat{plane}\;1\inmat{-}3\inmat{-}6\inmat{-}8:\;
% (z_{l+1}-z_l) x -(x_{i+1}-x_i) z+x_{i+1}z_l-x_iz_{l+1}=0.
% \edeq

Let $1$-$4$-$5$-$8$$\searrow$
denote the front half subspace of  the plane constructed by points $1$-$4$-$5$-$8$,
i.e.,
\bgeq
\inmat{$1$-$4$-$5$-$8$$\searrow$}:=\{(x,y,z)^T\in \R^3:
(y_{j+1}-y_j)x-(x_{i+1}-x_i)y+x_{i+1}y_j-x_iy_{j+1}\geq 0\},
\edeq
and let
$1$-$2$-$7$-$8$ $\uparrow$ denote the upper subspace of the plane constructed by points $1$-$2$-$7$-$8$,
that is,
\bgeq
&& \inmat{ $1$-$2$-$7$-$8$} \uparrow:=\{(x,y,z)^T\in \R^3: (z_{l+1}-z_l) y-(y_{j+1}-y_j)z +y_{j+1}z_l-y_jz_{l+1} \geq 0\},\\
&& \inmat{ $1$-$3$-$6$-$8$} \uparrow:=\{(x,y,z)^T\in \R^3: 
(z_{l+1}-z_l) x -(x_{i+1}-x_i) z +x_{i+1}z_l-x_iz_{l+1}\leq 0\}.
\edeq
% $a x_i+by_j+cz_l+d=0$
% $ax_{i+1}+by_j+c z_l+d=0$
% $ax_{i+1}+by_{j+1}+cz_{l+1}+d=0$
% $a x_{i+1}+by_{j+1}+cz_{l+1}+d=0$
% $ax_i+by_j+cz_l+d=0$
% $ax_{i}+by_{j+1}+c z_l+d=0$
% $ax_{i+1}+by_{j}+cz_{l+1}+d=0$
% $a x_{i+1}+by_{j+1}+cz_{l+1}+d=0$
The function value $u({x},{y},{z})$ is approximated by a convex combination of the function values evaluated at the vertices of the simplex containing $({x},{y},{z})$,
that is,
$$
u({x},{y},{z})=\lambda u_{i,j,l}
+\mu u_{{i+1},{j+1},{l+1}}+
%(1-\lambda-\mu) 
\bar{u},
$$
where  $\lambda, \mu\in [0,1]$ and 
% $
% \bar{u}=\left\{ 
% \begin{array}{ll}
% \tilde{u} & \inmat{if }  (x,y)\in \inmat{$1$-$4$-$5$-$8$}
% \searrow\\
% \hat{u} & \inmat{otherwise}.
% \end{array}
% \right.
% $
% and 
{\small \bgeq
&& \bar{u}=\left\{\begin{array}{ll}
\eta u_{{i+1},j,l}
+(1-\lambda-\mu-\eta)u_{{i+1},{j+1},l} & \inmat{if }\;
(x,y)\in \inmat{$1$-$4$-$5$-$8$}
\searrow 
\bigcap 
\inmat{$1$-$2$-$7$-$8$ $\downarrow$ (Fig.~\ref{fig:3a})},\\
\eta u_{{i+1},j,{l+1}} +(1-\lambda-\mu-\eta) u_{{i+1},j,l} & \inmat{if }\;
(x,y)\in \inmat{$1$-$4$-$5$-$8$}
\searrow
\bigcap
\inmat{$1$-$3$-$6$-$8$} \downarrow 
\bigcap 
\inmat{$1$-$2$-$7$-$8$ $\uparrow$ (Fig.~\ref{fig:3b})},\\
\eta u_{{i+1},j,{l+1}} +(1-\lambda-\mu-\eta) u_{i,j,{l+1}} & \inmat{if }\;
(x,y)\in \inmat{$1$-$4$-$5$-$8$}
\searrow
\bigcap
\inmat{$1$-$3$-$6$-$8$} \uparrow 
\bigcap \inmat{$1$-$2$-$7$-$8$ $\uparrow$ (Fig.~\ref{fig:3c})},
\end{array}
\right.\\
&& \bar{u}=\left\{\begin{array}{ll}
\eta u_{i,j,{l+1}}
+(1-\lambda-\mu-\eta)u_{i,{j+1},{l+1}} & 
\inmat{if }\;
(x,y)\in \inmat{$1$-$4$-$5$-$8$}
\nearrow 
\bigcap
\inmat{$1$-$2$-$7$-$8$ $\uparrow$ (Fig.~\ref{fig:4a})},\\
\eta u_{{i},{j+1},l}
+(1-\lambda-\mu-\eta) u_{{i},{j+1},{l+1}} & 
\inmat{if }\;
(x,y)\in \inmat{$1$-$4$-$5$-$8$}
\nearrow 
\bigcap
\inmat{$1$-$3$-$6$-$8$} \uparrow
\bigcap
\inmat{$1$-$2$-$7$-$8$ $\downarrow$(Fig.~\ref{fig:4b})},\\
\eta u_{{i},{j+1},l}
+(1-\lambda-\mu-\eta) u_{{i+1},{j+1},l}& 
\inmat{if }\; (x,y)\in \inmat{$1$-$4$-$5$-$8$}
\nearrow 
\bigcap
\inmat{$1$-$3$-$6$-$8$} \downarrow 
\bigcap
\inmat{$1$-$2$-$7$-$8$ $\downarrow$(Fig.~\ref{fig:4c})}.
% & \inmat{if}\;
% (x,y)\in \inmat{$1$-$2$-$7$-$8$ $\downarrow$}.
\end{array}
\right.
\edeq
}
% \bgeq
% && \tilde{u}=\left\{\begin{array}{ll}
% \left\{\begin{array}{ll}
% \eta u_{{i+1},j,{l+1}} +(1-\lambda-\mu-\eta) u_{i,j,{l+1}} & \inmat{if }\;
% (x,y)\in \inmat{$1$-$3$-$6$-$8$} \uparrow,\\
% \eta u_{{i+1},j,{l+1}} +(1-\lambda-\mu-\eta) u_{{i+1},j,l} & \inmat{otherwise},
% \end{array}
% \right. & \inmat{if}\;
% (x,y)\in 1\inmat{-}2\inmat{-}7\inmat{-}8 \uparrow ,\\
% \eta u_{{i+1},j,l}
% +(1-\lambda-\mu-\eta)u_{{i+1},{j+1},l} & \inmat{if}\;
% (x,y)\in \inmat{$1$-$2$-$7$-$8$ $\downarrow$},
% \end{array}
% \right.
% \\
% && \hat{u}=\left\{\begin{array}{ll}
% \eta u_{i,j,{l+1}}
% +(1-\lambda-\mu-\eta)u_{i,{j+1},{l+1}} & 
% \inmat{if}\;
% (x,y)\in \inmat{$1$-$2$-$7$-$8$ $\uparrow$},\\
% \left\{\begin{array}{ll}
% \eta u_{{i},{j+1},l}
% +(1-\lambda-\mu-\eta) u_{{i},{j+1},{l+1}} & 
% \inmat{if }\;
% (x,y)\in \inmat{$1$-$3$-$6$-$8$} \uparrow,\\
% \eta u_{{i},{j+1},l}
% +(1-\lambda-\mu-\eta) u_{{i+1},{j+1},l} & \inmat{otherwise}.
% \end{array}
% \right. & \inmat{if}\;
% (x,y)\in \inmat{$1$-$2$-$7$-$8$ $\downarrow$}.
% \end{array}
% \right.
% \edeq
% and 
% \bgeq
% && u_1=
% \\
% && u_4=
% \edeq
% {\color{blue}Similar to the argument in the two dimensional case,
% $x,y$ and $z$ can be represented by the convex combination of $\{x_1,\cdots,x_{N_1}\}$,
% $\{y_1,\cdots,y_{N_2}\}$
% and $\{z_1,\cdots,z_N\}$, respectively.}

\subsubsection{Implicit PLA}

%{\color{blue}
As in the two-dimensional case,
since $u_N$ is linear over each simplex,
a target point ${\bm f}(\bdz,\bdxi^k)\in \R^3$ and its approximate utility value 
$u_N({\bm f}(\bdz,\bdxi^k))$ have the same 
%the 
convex combination. 
%efficients 
%of the gridpoints $(x_i,y_j,z_l)$ and $u_{i,j,l}$.
We use binary variables $h_{i,j,l,k}^{1u}$,
$h_{i,j,l,k}^{1m}$,
$h_{i,j,l,k}^{1l}$
to characterize 
whether  point 
${\bm f}(\bdz,\bdxi^k)$ lies 
in the upper or middle,
or lower simplex in Part $1\inmat{-}2\inmat{-}4\inmat{-}5\inmat{-}6\inmat{-}8$ or beyond,
see Figure~\ref{fig-divisioin-6-1}.
Likewise, we use binary variables $h_{i,j,l,k}^{2u}$,
$h_{i,j,l,k}^{2m}$,
$h_{i,j,l,k}^{2l}$
to characterize 
whether  ${\bm f}(\bdz,\bdxi^k)$ 
lies in the upper, or middle,
or lower simplex in Part $1\inmat{-}3\inmat{-}4\inmat{-}5\inmat{-}7\inmat{-}8$  or beyond,
see Figure~\ref{fig-divisioin-6-2}.
% The connection between $\alpha_{i,j,l,k}^k$ and $h_{i,j,l,k}^l$, $h_{i,j,l,k}^u$ is that only the four vertices are used to represent the implicit PLA when $h_{i,j,l,k}^*=1$.
% }
As in the bi-attribute case, we can identify the coefficients of the convex combinations 
% The polyhedral method (\cite{DLM10,LeW01,VAN10,vielma2015mixed})
%  prompts us
%  to obtain the coefficients $\alpha_{i,j}$ of $u_{i,j}$ 
%  in Remark~\ref{rem:BUPRO-DF} (iii)
by solving a system of linear equalities and inequalities:
% Then we can reformulate the utility maximization problem 
% $\max_{{\bm w}\in Z}\bbe_{P_N}[u_N^*({\bm f({\bm w},\bdxi^k)})]$ as 
% a  mixed integer problem:
\begin{subequations}
\label{eq:mixed-integer-R3}
\begin{align}
%  & \max\limits_{{\bm w}\in Z} &  \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2}\sum_{l=1}^{N_3}\alpha_{i,j,l}^k u_{i,j,l}\\
& \sum_{i=1}^{N_1} \sum_{j=1}^{N_2}
\sum_{l=1}^{N_3}
\alpha_{i,j,l}^k=1,\;\;k=1,\cdots,K, \label{eq:mixed-integer-R3-b}\\
& \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \sum_{l=1}^{N_3} \alpha_{i,j,l}^k x_{i}= f_1^k,\;
%{\bm f_1({\bm w},\bdxi^k)},\;
\sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \sum_{l=1}^{N_3}\alpha_{i,j,l}^k y_{j}=f_2^k,\;
%f_2({\bm w},\bdxi^k),
% \label{eq:mixed-integer-R3-c}
% \\ &&
\sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \sum_{l=1}^{N_3}\alpha_{i,j,l}^k z_{l}=f_3^k, \nonumber \\
%f_3({\bm w},\bdxi^k),\;\;
& \hspace{20em} k=1,\cdots,K,
\label{eq:mixed-integer-R3-d} \\
& \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1}
\sum_{l=1}^{N_3-1}
h_{i,j,l,k}^{1u}+h_{i,j,l,k}^{1m}+h_{i,j,l,k}^{1l}
+h_{i,j,l,k}^{2u}+h_{i,j,l,k}^{2m}+h_{i,j,l,k}^{2l}=1, \nonumber \\
& \hspace{20em} k=1,\cdots,K,
\label{eq:mixed-integer-R3-e} \\
 & {\bm h}_k^{\tau u},
 {\bm h}_k^{\tau m},
 {\bm h}_k^{\tau l}
 \in \{0,1\}^{(N_1-1) (N_2-1) (N_3-1)},\;
 \tau=1,2,\;k=1,\cdots,K,
 %h_{i,j,l,k}^{\tau u}, \; h_{i,j,l,k}^{\tau m}\; h_{i,j,l,k}^{\tau l}\in \{0,1\},
% \tau = 1,2,
% i=1,\cdots,N_1,
% j=1,\cdots,N_2, 
% l=1,\cdots,N_3,
% k=1,\cdots,K, \quad~~~~
 \label{eq:mixed-integer-R3-f}\\
& 0\leq \alpha_{i,j,l}^k \leq \sum_{\nu = {\rm I}}^{\rm VIII} H_{i,j,l,k}^{\nu},\;
i=1,\cdots,N_1,\; j=1,\cdots,N_2, \;
l=1,\cdots,N_3, \nonumber \\
& \hspace{20em} k=1,\cdots,K, 
\label{eq:mixed-integer-R3-g}
\end{align}
\end{subequations}
where 
${\bm h}_{k}^{\tau u}:= (h_{1,1,1,k}^{\tau u},\cdots, h_{N_1-1,N_2-1,N_3-1,k}^{\tau u})^T$,
${\bm h}_{k}^{\tau m}:=(h_{1,1,1,k}^{\tau m},\cdots, h_{N_1-1,N_2-1,N_3-1,k}^{\tau m})^T$,
${\bm h}_{k}^{\tau l}:=(h_{1,1,1,k}^{\tau l}$,
$\cdots, h_{N_1-1,N_2-1,N_3-1,k}^{\tau l})^T$
for $\tau =1,2$,
% $i=1,\cdots,N_1$,
% $j=1,\cdots,N_2$, 
% $l=1,\cdots,N_3$,
 $k=1,\cdots,K$,
and
\bgeq
&& H_{i,j,l,k}^{\rm I}:= h_{i,j,l,k}^{1u}+h_{i,j,l,k}^{1m}+h_{i,j,l,k}^{1l}
+h_{i,j,l,k}^{2u}+h_{i,j,l,k}^{2m}+h_{i,j,l,k}^{2l},  
 \\
&& H_{i,j,l,k}^{\rm II}
:=h_{i-1,j,l,k}^{1m}+h_{i-1,j,l,k}^{1l}, \qquad~~~
H_{i,j,l,k}^{\rm III}
:=h_{i-1,j-1,l,k}^{1l}+h_{i-1,j-1,l,k}^{2l},\\
&& 
H_{i,j,l,k}^{\rm IV}
:=h_{i,j-1,l,k}^{2m}
+h_{i,j-1,l,k}^{2l},
\qquad~~~ 
H_{i,j,l,k}^{\rm V}
:=h_{i,j,l-1,k}^{1u}+h_{i,j,l-1,k}^{2u},\\
&&   
H_{i,j,l,k}^{\rm VI}:
=h_{i-1,j,l-1,k}^{1u}
+h_{i-1,j,l-1,k}^{1m}, 
\quad
H_{i,j,l,k}^{\rm VIII}:=
h_{i,j-1,l-1,k}^{2u}+h_{i,j-1,l-1,k}^{2m},\\
&& 
H_{i,j,l,k}^{\rm VII}:=
h_{i-1,j-1,l-1,k}^{1u}
+h_{i-1,j-1,l-1,k}^{1m}
+h_{i-1,j-1,l-1,k}^{1l}
+h_{i-1,j-1,l-1,k}^{2u}\\
&& \qquad \qquad 
+h_{i-1,j-1,l-1,k}^{2m}
+h_{i-1,j-1,l-1,k}^{2l},
\edeq
% \label{eq:mixed-integer-R3-g}
% \end{eqnarray}
% \end{subequations}
 ${\bm f({\bm w},\bdxi^k)}=(f_1^k,f_2^k,f_3^k)^T$,
$f_i^k:=f_i({\bm w},\bdxi^k)$ for
$i=1,2,3$,
$h_{0,*,*,*}^*=h_{*,0,*,*}^*=h_{*,*,0,*}^*=h_{N_1,*,*,*}^*=h_{*,N_2,*,*}^*=h_{*,*,N_3,*}^*=0$.
Constraint (\ref{eq:mixed-integer-R3-e})
imposes the restriction that only one is active
for the convex combination among all %$24$
$6$ simplices.
Constraint (\ref{eq:mixed-integer-R3-g}) imposes that the
only nonzero $\alpha_{i,j,l}^k$ can be those associated with the
four vertices of such simplex,
see 
%\ref{fig-divisioin-6}(c) $\&$ 
Figure~\ref{fig:I-VIII}.
%{\color{blue}
$H_{i,j,l,k}^\nu$
represents the sum of $h_{i,j,l,k}^*$
with $*\in \{1u,1m,1l,2u,2m,2l\}$
in Octant $\nu$ that are related to point $(x_i,y_j,z_l)$ for $\nu={\rm I},\ldots,{\rm VIII}$.
% We can see that
% there are six, two, two, two, two, two, six, two simplices in Octant I-VIII that are related to the point $(x_i,y_j,z_l)$, respectively.
Specifically,
there are $6$ simplices in Octant I that are related to point $(x_i,y_j,z_l)$,
and the corresponding binary variables are
$h_{i,j,l,k}^{1u}$,
$h_{i,j,l,k}^{1m}$,
$h_{i,j,l,k}^{1l}$,
$h_{i,j,l,k}^{2u}$,
$h_{i,j,l,k}^{2m}$,
$h_{i,j,l,k}^{2l}$.
There are two triangles in Octant II that are related to $(x_i,y_j,z_l)$,
and
the corresponding binary variables are $h_{i-1,j,l,k}^{1m}$ and
$h_{i-1,j,l,k}^{1l}$.
The related binary variables in Octant III-VIII
can also be observed.
Such $h_{i,j,l,k}^*$ can be used to identify which vertices are used to represent 
${\bm f}(\bdz,\bdxi^k)$.
For example, if $h_{i,j,l,k}^{1l}=1$, then 
$\bdf(\bdz,\bdxi^k)$ lies in the
lower simplex in the former part of
cube $X_i\times Y_j\times Z_l$.
This is 
indicated
%evidenced 
by the fact 
that
$\alpha_{i,j,l}^k\leq h_{i,j,l,k}^{1l}=1$,
$\alpha_{i+1,j+1,l+1}^k\leq h_{i,j,l,k}^{1l}=1$,
$\alpha_{i+1,j,l}^k\leq h_{i,j,l,k}^{1l}=1$,
$\alpha_{i+1,j+1,l}^k\leq h_{i,j,l,k}^{1l}=1$,
and $\alpha_{i',j',l'}^k=0$ for $(i',j',l')\notin \{(i,j,l),(i+1,j+1,l+1),(i+1,j,l),(i+1,j+1,l)\}$,
see Figure~\ref{fig:I-VIII} for
the 24 simplices that are related to point $(x_i,y_j,z_l)$.
% 

\begin{figure}[!ht]
% \vspace{-0.55cm}
  \centering
%   \subfigure[a cube]{
% %     \label{subfig-main-contuor-ptb5} %% label for second subfigure
%     \includegraphics[width=1.6in]I-VIII.pdf}
%   }
   \subfigure[A cube]{
%     \label{subfig-main-contuor-ptb5} %% label for second subfigure
    \includegraphics[width=0.25\linewidth]{I-VIII.pdf}
    }
     \hspace{-0.3em}
   \subfigure[Octants I-IV]{
%     \label{subfig-main-contuor-ptb10} %% label for second subfigure
    \includegraphics[width=0.32\linewidth]{I-IV.pdf}
  }
  \hspace{-0.3em}
   \subfigure[Octants V-VIII]{
%     \label{subfig-main-contuor-ptb15}
    \includegraphics[width=0.32\linewidth]{V-VIII.pdf}
  }
  \vspace{-0.35cm}
  \caption{\footnotesize 
  (a) divides a cube into $8$ sub-cubes denoted by octants I, II, III, IV, V, VI, VII and VIII.
  The red point in (a)-(c) is $(x_i,y_j,z_l)$.
  %, and corresponds to the red point in Figure~\ref{fig-divisioin-6}.
  (b) $\&$ (c) illustrate all the $24$ simplices related to the point $(x_i,y_j,z_l)$,
which 
prompts
the last constraint in problem (\ref{eq:mixed-integer-R3}).
% In Figure~\ref{fig:I-VIII},
% the red point is $(x_i,y_j,z_l)$ in Figure~\ref{fig-divisioin-6}~(c).
(b) represents the cases in octants I-IV.
In octant I,
the vertex $1$ is $(x_i,y_j,z_l)$,
and there are six simplices containing the red point $(x_i,y_j,z_l)$.
In octant II,
the vertex $1$ is $(x_{i-1},y_j,z_l)$,
and there are two simplices containing the red point.
%$(x_i,y_j,z_l)$.
In octant III,
the vertex $1$ is $(x_{i-1},y_{j-1},z_l)$,
and there are two simplices containing the red point.
%$(x_i,y_j,z_l)$.
In octant IV,
the vertex $1$ is $(x_{i},y_{j-1},z_l)$,
and there are two simplices containing the red point. 
(c) represents the cases 
in octants V-VIII.
In octant V,
the vertex $1$ is $(x_i,y_j,z_{l-1})$,
and there are two simplices containing the red point.
%$(x_i,y_j,z_l)$.
In octant VI,
the vertex $1$ is $(x_{i-1},y_j,z_{l-1})$,
and there are two simplices containing the red point.
%$(x_i,y_j,z_l)$.
In octant VII,
the vertex $1$ is $(x_{i-1},y_{j-1},z_{l-1})$,
and there are six simplices containing the red point. %$(x_i,y_j,z_l)$.
In octant VIII,
the vertex $1$ is $(x_{i},y_{j-1},z_{l-1})$,
and there are two simplices containing the red point.}
%$(x_i,y_j,z_l)$.}
  \label{fig:I-VIII} %% label for entire figure
\end{figure}
% In Figure~\ref{fig:I-VIII-all},
% we present a point $(x_i,y_j,z_l)$ and the $8$ related sub-cubes in octants I, II, III, IV, V, VI, VII and VIII,
% respectively.
% In Figure~\ref{fig:I-VIII},
Consequently, we can reformulate the tri-attribute utility maximization problem \linebreak
$\max_{{\bm w}\in Z}\sum_{k=1}^K p_k[u_N({\bm f({\bm w},\bdxi^k)})]$ as:
\begin{subequations}
\label{eq:mixed-integer-R3-2}
\begin{align}
 \max\limits_{{\bm w}\in Z,{\bm \alpha},{\bm h^l},{\bm h}^m,{\bm h}^u} \; &  \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2}\sum_{l=1}^{N_3}\alpha_{i,j,l}^k u_{i,j,l}\\
% &{\rm s.t.} & \sum_{i=1}^{N_1} \sum_{j=1}^{N_2}
% \sum_{l=1}^{N_3}
% \alpha_{i,j,l}^k=1,\;\;k=1,\cdots,K, \label{eq:mixed-integer-R3-b}\\
{\rm s.t.} \qquad\;\; & \inmat{constraints } (\ref{eq:mixed-integer-R3-b})-
(\ref{eq:mixed-integer-R3-g}),
\end{align}
\end{subequations}
where 
${\bm \alpha}:=({\bm \alpha}^1,\cdots,{\bm \alpha}^K)\in \R^{(N_1N_2N_3)\times K}$,
${\bm \alpha}^k:=(\alpha_{1,1,1}^k,\cdots,\alpha_{N_1,N_2,N_3}^k)^T$ for $k=1,\cdots,K$,
${\bm h}^u:=({\bm h}^{1u}_1,\cdots,{\bm h}^{1u}_K,{\bm h}^{2u}_1,\cdots,{\bm h}^{2u}_K)\in \R^{(N_1-1)(N_2-1)(N_3-1)\times 2K}$,\;
${\bm h}^m:=({\bm h}^{1m}_1,\cdots,{\bm h}^{1m}_K,{\bm h}^{2m}_1,\cdots, \\ {\bm h}^{2m}_K)$,\;
${\bm h}^l:=({\bm h}^{1l}_1,\cdots,$
${\bm h}^{1l}_K,{\bm h}^{2l}_1,\cdots,{\bm h}^{2l}_K)$,\;
${\bm u}:=(u_{1,1,1},\cdots,$
$u_{N_1,N_2,N_3})^T\in \R^{N_1 N_2N_3}$.
If $f({\bm w},\bdxi)$ is linear in ${\bm w}$,
then (\ref{eq:mixed-integer-R3-2}) is an MILP.}
% \begin{figure}[!htbp]
%   \centering
%   \subfigure{
% %     \label{subfig-main-contuor-ptb5} %% label for second subfigure
%     \includegraphics[width=2.5in]{figures/I-VIII.pdf}
%   }
%   \caption{Divide a cube into $8$ sub-cubes in octants I, II,
% III, IV, V, VI, VII and VIII}
%   \label{fig:I-VIII-all} %% label for entire figure
% \end{figure}
%{\color{blue}
% We can also incorporate the  robust version into the mixed integer format.
% For a functions of bounded variation,
% we can define three dimensional Lebesgue–Stieltjes integral with respect to measures induced by $u$.
% Specifically,
Extending this to the UPRO model, we consider the ambiguity set ${\cal U}_N$ constructed by pairwise comparison of questions $({\bm A}_m,{\bm B}_m)$.
% For the cubic $[x_i, x_{i+1}]\times[y_j,y_{j+1}]\times [z_l,z_{l+1}]$,
% let ${\bm v}_k=\left(v_k^{(1)},v_k^{(2)},v_k^{(3)}\right)$, $k=1,\cdots,2^3$
% be the points such that each $v_k^{(m)}$ ($m=1,2,3$) is either $x_i,y_j,z_l$, $x_{i+1}$, $y_{j+1}$, $z_{l+1}$,
% that is,
% ${\bm v}_1=(x_{i+1},y_{j+1},z_{l+1})$,
% ${\bm v}_2=(x_i,y_{j+1},z_{l+1})$,
% ${\bm v}_3=(x_{i+1},y_{j},z_{l+1})$,
% ${\bm v}_4=(x_{i+1},y_{j+1},z_{l})$,
% ${\bm v}_5=(x_{i},y_{j},z_{l+1})$,
% ${\bm v}_6=(x_{i},y_{j+1},z_{l})$,
% ${\bm v}_7=(x_{i+1},y_{j},z_{l})$,
% ${\bm v}_8=(x_{i},y_{j},z_{l})$.
% Let
% $n({\bm v}_k)\in \{1,2,3\}$ be the number of lower symbols $x_i$, $y_j$ and $z_l$.
% Then the Lebesgue-Stieltjes measure induced by $u$ is
% \bgeq
% \mu_u((x_i, x_{i+1}]\times(y_j,y_{j+1}]\times (z_l,z_{l+1}])
% &=&\sum_{k=1}^{2^3}(-1)^{n({\bm v}_k)} u({\bm v}_k)\\
% &=&(-1)^0u(x_{i+1},y_{j+1},z_{l+1})
% +(-1)^{1} u(x_i,y_{j+1},z_{l+1})\\
% &&
% +(-1)^{1}
% u(x_{i+1},y_{j},z_{l+1})
% +(-1)^{1}
% u(x_{i+1},y_{j+1},z_{l})\\
% &&
% +(-1)^2
% u(x_{i},y_{j},z_{l+1})
% +(-1)^2
% u(x_{i},y_{j+1},z_{l})\\
% &&
% +(-1)^2
% u(x_{i+1},y_{j},z_{l})
% +(-1)^3
% u(x_{i},y_{j},z_{l}).
% \edeq
% Denote $\tilde{\psi}(x,y,z)=\sum_{k=1}^{8} (-1)^{n(\tilde{\bdv}_k)} \psi({\tilde{\bdv}}_k)$,
% where $\tilde{\bdv}$ corresponds to $[x,\bar{x}]\times[y,\bar{y}]\times [z,\bar{z}]$.
% Based on Lebesgue-Stieltjes integral,
% we can construct an ambiguity set by moment-type condition:
% $$
% {\cal U}_N=\left\{u_N\in \mathscr{U}_N: \int_{[\underline{x},\bar{x}]\times [\underline{y},\bar{x}]\times [\underline{z},\bar{z}]}  u_N(x,y,z) d\psi_l(x,y,z)\leq c,m=1,\cdots,M \right\}.
% $$
% By integral by parts,
% we have 
% \bgeq
% {\cal U}_N
% &=&\left\{u_N\in \mathscr{U}_N: \int_{[\underline{x},\bar{x}]\times [\underline{y},\bar{x}]\times [\underline{z},\bar{z}]}  \tilde{\psi}_m(x,y,z)du_N(x,y,z) \right. \\
% && +\int_{[\underline{x},\bar{x}]\times [\underline{y},\bar{y}]}  \tilde{\psi}_m(x,y,\underline{z})du_N(x,y,\underline{z}) 
% +\int_{[\underline{x},\bar{x}]\times [\underline{z},\bar{z}]}  \tilde{\psi}_m(x,\underline{y},z)du_N(x,\underline{y},z)
% \\
% && +\int_{[\underline{y},\bar{y}]\times [\underline{z},\bar{z}]}  \tilde{\psi}_m(\underline{x},y,z)du_N(\underline{x},y,z)
% +\int_{[\underline{x},\bar{x}]}  \tilde{\psi}_m(x,\underline{y},\underline{z})du_N(x,\underline{y},\underline{z})\\
% && 
% \left. 
% +\int_{[\underline{y},\bar{y}]}  \tilde{\psi}_m(\underline{x},y,\underline{z})du_N(\underline{x},y,\underline{z})
% +\int_{[\underline{z},\bar{z}]}  \tilde{\psi}_m(\underline{x},\underline{y},z)du_N(\underline{x},\underline{y},z)
% \leq \tilde{c} \right\}\\
% &=& 
% \left\{u_N\in \mathscr{U}_N: 
% \sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1}\sum_{l=1}^{N_3-1}  \tilde{\psi}_m(x,y,z) \mu_{u_N}((x_i, x_{i+1}]\times(y_j,y_{j+1}]\times (z_l,z_{l+1}])\right.\\
% && 
% \left.\sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \tilde{\psi}_m(x,y,\underline{z})
% \right\}????.
% \edeq
% {\color{red}Sainan: To be update.}
% \bgeq
% ${\cal U}_N
% =\{u_N\in \mathscr{U}_N: \bbe_{\mathbb{P}}[u({\bm B}_m)]\leq \bbe_{\mathbb{P}}[u({\bm A}_m)],\;m=1,\cdots,M\}$.
% &=&\left\{u_N\in \mathscr{U}_N:\sum_{i=1}^{N_1} \sum_{j=1}^{N_2}\sum_{l=1}^{N_3} ( \mathbb{P}({\bm B}_m=(x_i,y_j,z_l))-\mathbb{P}({\bm A}_m=(x_i,y_j,z_l)))u_{i,j,l} \leq 0,
% \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \sum_{l=1}^{N_3} \mathbb{P}({\bm A}_m=(x_i,y_j,z_l))u_{i,j,l}, 
% \;m=1,\cdots,M\right\}.
% &=&\left\{u_N\in \mathscr{U}_N: \int_{\underline{x},\underline{y}}^{\bar{x},\bar{y}} \psi_l(x,y) d u_N(x,y)
%         + \int_{\underline{x}}^{\bar{x}} \psi_{1,m}(x)d u_N(x,\underline{y})  +\int_{\underline{y}}^{\bar{y}} \psi_{2,m} (y) d u_N(\underline{x},y) \leq 0,\;m=1,\cdots,M \right\}\\
% &=& \left\{u_N\in \mathscr{U}_N:\sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \psi_l(x_i,y_j)
% (u_{i+1j+1}-u_{i,j+1}-u_{i+1j}+u_{ij}) \right.\\
% &&  \qquad  \qquad 
% + \left. \sum_{i=1}^{N_1-1}\psi_{1,m}(x_i) (u_{i+1,1}-u_{i1})
% +\sum_{j=1}^{N_2-1} \psi_{2,m} (y_j) (u_{1,j+1}-u_{1,j}) \leq 0,\; m=1,\cdots,M\right\},
% \edeq
% Let $u_{ij}=u(x_i,y_j)$,
% $i=1,\cdots,N_1$, $j=1,\cdots,N_2$,
% t
% Let $
% \tilde{\cal U}_N={\cal U}_N \bigcap \{u_N: u_N \inmat{ is Lipschitz with its modulus }L\}.
% $
Under Assumption~\ref{assu-lip},
suppose that the set of gridpoints $\{(x_i,y_j,z_l):i=1,\cdots,N_1,j=1,\cdots,N_2,l=1,\cdots,N_3\}$ contains all the outcomes of lotteries $ {\bm A}_m$ and ${\bm B}_m$ for $m=1,\cdots,M$,
then we can solve the tri-attribute utility preference robust optimization (TUPRO) problem by solving the approximate TUPRO-N problem 
$\max_{\bdz\in Z} \min_{u_N\in {\cal U}_N} \sum_{k=1}^Kp_k[u_N({\bm f(\bdz,\bdxi^k)})]$ 
as:
\begin{subequations}
\label{eq:PRO_MILP_3m}
\begin{align}
\max\limits_{\bdz\in Z}
\min\limits_{
\substack{{\bm \alpha}, {\bm h}^u,{\bm h}^m\\
{\bm h}^l,\bm u}}\;&
\sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2}
\sum_{l=1}^{N_3}
\alpha_{i,j,l}^k u_{i,j,l}\\
{\rm s.t.} \;\;\;\; & 
% \sum_{i=1}^{N_1} \sum_{j=1}^{N_2}
% \sum_{l=1}^{N_3}
% \alpha_{ijl}^k=1,\;\;k=1,\cdots,K, \label{eq:mixed-integer-R3-b}\\
% && \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \sum_{l=1}^{N_3} \alpha_{ijl}^k x_{i}^k={\bm f({\bm w},\bdxi^k)}_1,\;
% \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \sum_{l=1}^{N_3}\alpha_{ijl}^k y_{j}^k={\bm f({\bm w},\bdxi^k)}_2,
% \label{eq:mixed-integer-R3-c}
% \\
% &&  \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \sum_{l=1}^{N_3}\alpha_{ijl}^k z_{l}^k={\bm f({\bm w},\bdxi^k)}_3,\;\;
% k=1,\cdots,K,
% \label{eq:mixed-integer-R3-d}\\
% && \inmat{(\ref{eq:mixed-integer-R3-e})-(\ref{eq:mixed-integer-R3-f})},\\
% && \frac{u_{i+1,j,l}-u_{i,j,l}}{x_{i+1}-x_{i}}\geq \frac{u_{i,j,l}-u_{i-1,j,l}}{x_{i}-x_{i-1}}, i=2,\cdots,N_1-1,j=1,\cdots,N_2,l=1,\cdots,N_3,\label{eq:PRO_MILP_3m-c}\\
% && \frac{u_{i,j+1,l}-u_{i,j,l}}{y_{j+1}-y_{j}}\leq \frac{u_{i,j,l}-u_{i,j-1,l}}{y_{j}-y_{j-1}}, i=1,\cdots,N_1,j=2,\cdots,N_2-1,l=1,\cdots,N_3, \qquad \label{eq:PRO_MILP_3m-d}\\
% && \frac{u_{i,j,l+1}-u_{i,j,l}}{z_{l+1}-z_{l}}\leq \frac{u_{i,j,l}-u_{i,j,l-1}}{z_{l}-z_{l-1}}, i=1,\cdots,N_1,j=1,\cdots,N_2, l=2,\cdots, N_3-1,\label{eq:PRO_MILP_3m-e}\\
 u_{i+1,j,l}\geq u_{i,j,l}, i=1,\cdots,N_1-1, j=1,\cdots,N_2,l=1,\cdots,N_3,  \label{eq:PRO_MILP_3m-f}\\
& u_{i,j+1,l}\geq u_{i,j,l}, i=1,\cdots,N_1, j=1,\cdots,N_2-1,l=1,\cdots,N_3, \label{eq:PRO_MILP_3m-h}\\
& u_{i,j,l+1}\geq u_{i,j,l}, i=1,\cdots,N_1, j=1,\cdots,N_2,l=1,\cdots,N_3-1, \label{eq:PRO_MILP_3m-i}\\
& u_{i+1,j,l}-u_{i,j,l}\leq L(x_{i+1}-x_i), \nonumber \\
& \qquad \qquad i=1,\cdots,N_1-1, j=1,\cdots,N_2,l=1,\cdots,N_3,\label{eq:PRO_MILP_3m-j}\\
& u_{i,j+1,l}-u_{i,j,l}\leq L(y_{j+1}-y_j), \nonumber \\
& \qquad \qquad i=1,\cdots,N_1, j=1,\cdots, N_2-1, l=1,\cdots,N_3, \label{eq:PRO_MILP_3m-k}\\
& u_{i,j,l+1}-u_{i,j,l}\leq L(z_{l+1}-z_{l}), \nonumber \\
& \qquad \qquad  i=1,\cdots,N_1,j=1,\cdots,N_2, l=1,\cdots,N_3-1, \label{eq:PRO_MILP_3m-l} \\
& u_{1,1,1}=0, \; u_{N_1,N_2,N_3}=1,  \label{eq:PRO_MILP_3m-m}\\
&\sum_{i=1}^{N_1} \sum_{j=1}^{N_2}\sum_{l=1}^{N_3} ( \mathbb{P}({\bm B}_m=(x_i,y_j,z_l))-\mathbb{P}({\bm A}_m=(x_i,y_j,z_l)))u_{i,j,l} \leq 0,
% \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \sum_{l=1}^{N_3} \mathbb{P}({\bm A}_m=(x_i,y_j,z_l))u_{i,j,l}, 
\nonumber \\
& \hspace{16em} m=1,\cdots,M, \label{eq:PRO_MILP_3m-n}\\
& \inmat{constraints }(\ref{eq:mixed-integer-R3-b})-(\ref{eq:mixed-integer-R3-g}),\label{eq:PRO_MILP_3m-b}
\end{align}
\end{subequations}
where
% ${\bm \alpha}=({\bm \alpha}^1,\cdots,{\bm \alpha}^K)\in \R^{(N_1N_2N_3)\times K}$,
% ${\bm \alpha}^k:=(\alpha_{1,1,1}^k,\cdots,\alpha_{N_1,N_2,N_3}^k)^T$ for $k=1,\cdots,K$,
% ${\bm h}^u:=({\bm h}^{1u}_1,\cdots,{\bm h}^{1u}_K,{\bm h}^{2u}_1,\cdots,{\bm h}^{2u}_K)$,
% ${\bm h}^m:=({\bm h}^{1m}_1,\cdots,{\bm h}^{1m}_K,{\bm h}^{2m}_1,\cdots,{\bm h}^{2m}_K)$,
% ${\bm h}^l:=({\bm h}^{1l}_1,\cdots,$
% ${\bm h}^{1l}_K,{\bm h}^{2l}_1,\cdots,{\bm h}^{2l}_K)$,
% {\color{red} ${\bm u}:= {\rm vec}\left((u_{i,j,l})_{N_1,N_2,N_3}\right)\in \R^{N_1 N_2N_3}$ }.
 ${\bm u}:=(u_{1,1,1},\cdots,u_{N_1,N_2,N_3})^T\in \R^{N_1 N_2N_3}$.
% If $f(\bdz,\bdxi)\in \R^{N_1\times N_2\times N_3}$ is linear in $\bdz$,
% We can also get the dual of the inner problem w.r.t. ${\bm u}$ and then obtain a single mixed-integer program.
% Moreover,
% problem (\ref{eq:PRO_MILP_3m}) is equivalently to 
% \bgeqn
% \label{eq:PRO_MILP_3m-D}
% &\max\limits_{\bdz\in Z} \min\limits_{{\bm \alpha}^k, {\bm u}^u, {\bm u}^l,{\bm u}}&
% \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2}
% \sum_{l=1}^{N_3}
% \alpha_{ij}^k u_{ijl} \nonumber \\
% &{\rm s.t.}& 
% (\ref{eq:PRO_MILP_3m-f})-(\ref{eq:PRO_MILP_3m-b}).
% \edeqn
We can solve problem (\ref{eq:PRO_MILP_3m})
by a Dfree method,
where the  inner problem is an MILP when ${\bm f}(\bdz,\bdxi)$ is linear in $\bdz$.
%mixed-integer program.
It is also possible to reformulate 
the problem further as a single MILP, we leave this for interested readers.
% {\color{blue}
% There exists an alternative way to characterize the interpolation in three dimensional case,
% see \cite{misener2010piecewise}.}
%MILP when ${\bm f}(\bdz,\bdxi)$ is linear in $\bdz$.
% see Appendix~\ref{app:reformulation-3m-single}.
% \end{remark}
% {\bf Test results}\\
% XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\
% Questionnaire
% $$
% \bdcz_1 = \lt\{
% \begin{array}{ll}
%     (\underline{x},\underline{y},\underline{z}) & \inmat{with probability\;} 1-p, \\
%     (\bar{x},\bar{y},\bar{z}) & \inmat{with probability\;} p,
% \end{array} 
% \rt.
% \inmat{\quad and \quad}
% \bdcz_2=(x,y,z) \inmat{\; with probability\;} 1,
% $$
% where $x$ $y$ and $z$ are generated randomly over $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]\times [\underline{z},\bar{z}]$.
% Observe that 
% $$
% \bbe[u(\bdcz_1)]=(1-p) u(\underline{x},\underline{y},\underline{z})+ p u(\bar{x},\bar{y},\bar{z}) \inmat{\quad and\quad} \bbe[u(\bdcz_2)]=u(x,y,z). 
% $$
% Recall the normalized conditions, then the questionnaire is to check the following inequality
% \begin{equation}
%     p<u(x,y,z).
% \end{equation}
% Now we turn to discuss how to choose $x$, $y$ $z$ and $p$.
% Choose $M_1-2$ points from $[\underline{x},\bar{x}]$ and $M_2-2$ points from $[\underline{y},\bar{y}]$ randomly via uniform distribution; 
% sort and label them by $x_i, i=1,\ldots,M_1-2$, $y_j, j=1,\ldots,M_2-2$
% and $z_l$, $l=1,\cdots,M_3$.
% Let ${\cal X}:= \{\underline{x},x_1,\ldots,x_{M_1-2},\bar{x}\}$ and ${\cal Y}:= \{\underline{y},y_1,\ldots,y_{M_1-2},\bar{y}\}$,
% ${\cal Z}:=\{\underline{z},z_1,\cdots,z_{M_3-2},\bar{z}\}$,
% let ${\cal X}\times{\cal Y}\times {\cal Z}:=\{(x_i,y_j,z_l):x_i\in{\cal X}, y_j\in{\cal Y},z_l\in {\cal Z}\}$ be the set of all certain lotteries except points $(\underline{x},\underline{y},\underline{z})$. Let $(\bar{x},\bar{y},\bar{z})$ and $\calu_{N}^{m}$ be the set of all utility functions which are consistent to the former generated $m-1$ pairwise comparison questionnaires.
% Assume the $m$th certain outcome is $(x_{i_m},y_{j_m},z_{l_m})$.
% \begin{eqnarray}
%   I_1^m&=&\min_{{\bm u}} u_{i_m,j_m,l_m}\\
%     &\st &  h_\tau p_{\tau-1}\leq h_\tau u_{i_{\tau_1}+1,j_{\tau_2}+1,l_{\tau_3}+1}, \tau=1,\cdots,m-1, \label{eq-lottery-3} \\
%     && (\ref{eq:PRO_MILP_3m-d})-(\ref{eq:PRO_MILP_3m-i}), (\ref{eq:PRO_MILP_3m-m}).
%     % && \frac{u_{i+1,j,l}-u_{i,j,l}}{x_{i+1}-x_{i}} \geq \frac{u_{i,j,l}-u_{i-1,j,l}}{x_i-x_{i-1}}, i=2,\ldots,M_1-1, j=1,\ldots,M_2, l=1,\cdots,M_3,  \qquad~~ \\
%     % && \frac{u_{i,j+1,l}-u_{i,j,l}}{y_{j+1}-y_j} \leq \frac{u_{i,j,l}-u_{i,j-1,l}}{y_{j}-y_{j-1}}, i=1,\ldots,M_1, j=2,\ldots,M_2-1,
%     % l=1,\cdots,M_3,\\
%     % && \frac{u_{i,j,l+1}-u_{i,j,l}}{z_{l+1}-z_{l}} \leq \frac{u_{i,j,l}-u_{i,j,l-1}}{z_l-z_{l-1}}, i=1,\ldots,M_1, j=1,\ldots,M_2,
%     % l=2,\cdots,M_3-1,\\
%     % && u_{i+1,j,l}\geq u_{i,j,l}, i=1,\ldots,M_1-1,j=1,\ldots,M_2, l=1,\cdots,M_3,  \\
%     % && u_{i,j+1,l}\geq u_{i,j,l}, i=1,\ldots,M_1,j=1,\ldots,M_2-1,  l=1,\cdots,M_3,\\
%     % && u_{i,j,l+1}\geq u_{i,j,l}, i=1,\ldots,M_1,j=1,\ldots,M_2,  l=1,\cdots,M_3-1,\\
%     % && u_{1,1,1}=0,\; u_{M_1,M_2,M_3}=1. 
% \end{eqnarray}
% Moreover,
% \begin{eqnarray}
%   I_2^m&=&\max_{{\bm u}} u_{i_m,j_m,l_m}\\
%     &\st &  h_\tau p_{\tau-1}\leq h_\tau u_{i_{\tau_1}+1,j_{\tau_2}+1,l_{\tau_3}+1}, \tau=1,\cdots,m-1, \label{eq-lottery-3} \\
%     && (\ref{eq:PRO_MILP_3m-d})-(\ref{eq:PRO_MILP_3m-i}), (\ref{eq:PRO_MILP_3m-m}).
% \end{eqnarray}
% Since $u(x_{i_m},y_{j_m},z_{l_m})\in[0,1]$, then $I_1^m,I_2^m\in[0,1]$.
% Hence we set $p^l=\frac{I_1^m+I_2^m}{2}$,
% and use the true utility function $u^*(x_{i_m},y_{j_m},z_{l_m})$ to check whether
% $$
% \bbe[u^*(\bdcz_2)]>\bbe[u^*(\bdcz_1)],
% $$
% that is, $p^l<u^*(x_{i_m},y_{j_m},z_{l_m})$.
% If it holds, then $\bdcz_2$ is preferred over $\bdcz_1$.
% The total number of questionnaires is $M=M_1M_2M_3-2$.
% The following algorithm describe the procedures for constructing $\calu_{N}\cap\scru$.

%  Set $l=l+1$.
% If $(1-p^l)u^*(\underline{x},\underline{y},\underline{z})+p^l u^*(\overline{x},\overline{y},\bar{z})\leq u^*(x,y,z)$,
% set $h_m=1$ and we regard the DM prefers $Z_2^m$ to $Z_1^m$,
% in which case only those $u_N$ whose values at $(x,y,z)$ fall in the interval $[p^l,I_2^l]$, that is, $p^l<u^*(x^m,y^m,z^m)$ will be considered and subsequently set
% $$
% {\cal U}_N^{m} \cap \mathscr{U}:={\cal U}_N^{m-1} \bigcap \{u_N\in \mathscr{U}: \bbe[u^*(\bdcz_2^{m})]\geq \bbe[u^*(\bdcz_1^{m})]\}.
% $$
% Otherwise let $h_m=-1$
% and set 
% $$
% {\cal U}_N^{m} \cap \mathscr{U}:={\cal U}_N^{m-1} \bigcap \{u_N\in \mathscr{U}: \bbe[u^*(\bdcz_2^{m})]\leq \bbe[u^*(\bdcz_1^{m})]\}.
% $$
% XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
 % }

\subsection{Multi-attribute case}
\label{sec:m-dim-u}

% {\color{red}Sainan: Today, I will check this method by reading several papers.}

Since a large number of simplices are needed to partition hypercubes of dimension greater than three
(see \cite{hughes1996simplexity}),
we give a general framework 
%characterization 
for the $m$ attributes case.
%  Note that there are different methods to characterize in which simplex %sub-cube or generally
% %polyhedron
% the target point $({x}_1,...,x_m)\in \R^m$ located %by
% using
% a continuous variable and a binary variable,
% see e.g. \cite{ViN11,KDN04}.
For ${\bm x}\in \R^m$,
we  can divide the domain of utility %$\mathscr{X}\subset
$\bigtimes_{i=1}^{m}[\underline{x}_i,\overline{x}_i]$ into $(N_1-1)\times (N_2-1)\times \cdots \times (N_m-1)$ subsets $\{\bigtimes_{i=1}^{m}[x_{i_j},x_{i_{j+1}}]:j=1,\cdots,N_{i}-1\}$.
We denote the values of $u_N$ at $(x_{1_{j_1}},\cdots,x_{m_{j_m}})$ by $u_{1_{j_1},\cdots,m_{j_m}}$ for
$j_i=1,\cdots,N_i$,
$i=1,\cdots,m$.
We {\color{black}
reshape $(u_{1_{j_1},\cdots,m_{j_m}})_{N_1\times \cdots \times N_m}\in \R^{N_1\times \cdots \times N_m}$
as a vector ${\bm u}=(u_1,\cdots,u_V)^T\in \R^{V}$}
with $V:=N_1\times\cdots \times N_m$,
and label the 
corresponding vertices 
%are 
% labelled 
by $1,\cdots,V$.
{\color{black}
We divide the domain $\bigtimes_{i=1}^{m}[\underline{x}_i,\overline{x}_i]$ into mutually exclusive simplices and  label them
%the simplices
by $1,\cdots,S$.}
The $v$-th vertice is ${\bm x}_v:=(x_{1_v},\cdots,x_{i_v},\cdots,x_{m_v})^T \in \R^m$ for $v=1,\ldots,V$.
Let ${\cal V}_s$ denote the set of vertices of the $s$-th simplex.
As in the bi-attribute and tri-attribute cases,
for given
%$\bdx_t^k=(x_{t1}^k,\cdots,x_{tm}^k,\cdots,x_{tM}^k)$ and
${\bm f}(\bdz,\bdxi^k)=(f_1(\bdz,\bdxi^k),f_2(\bdz,\bdxi^k),\cdots,f_m(\bdz,\bdxi^k))^T$,
we can
identify 
 the simplex containing 
 %the representation of ${\bm u}$ 
%at 
${\bm f}(\bdz,\bdxi^k)$
and obtain
the coefficients 
%of the convex combination 
of the representation of ${\bm u}$ 
at ${\bm f}(\bdz,\bdxi^k)$
in terms of the utility values at the 
vertices of the simplex
% of the simplex containing the representation of ${\bm u}$ 
% at ${\bm f}(\bdz,\bdxi^k)$
by solving a system of linear equalities and
inequalities:
% Then we can reformulate the utility maximization problem  $\max_{\bdz\in Z}\bbe_{P_K}[u_N({\bm f(\bdz,\bdxi^k)})]$ as 
% a mixed integer problem,
% see e.g. \cite{VAN10},
% \begin{subequations}
% \label{eq:mixed-integer-Rm}
% \begin{eqnarray}
%  & \max\limits_{\bdz\in Z} &  \sum_{k=1}^K p_k \sum_{s=1}^{S}\alpha_{s}^k u_s???\\
\begin{subequations}
\label{eq:mixed-integer-Rm}
\begin{align}
& \sum_{v=1}^{V}
\alpha_{v}^k=1,\;\;k=1,\cdots,K,
\label{eq:mixed-integer-Rm-b}\\
& \sum_{v=1}^V \alpha_{v}^k x_{i_v}=f_i(\bdz,\bdxi^k),\;\;i=1,\cdots,m,\;
k=1,\cdots, K,
\label{eq:mixed-integer-Rm-c}\\
% && \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1}
% \sum_{l=1}^{N_3-1}
% h_{ijlk}^{u}+h_{ijlk}^{m}+h_{ijlk}^{l}=1, \;\; h_{ijlk}^{u}, \; h_{ijlk}^{m}\; h_{ijlk}^{l}\in \{0,1\},\;\; k=1,\cdots,K,\\
% && \alpha_{ijl}^k \leq h_{ijlk}^{u}+h_{ijlk}^{m}+h_{ijlk}^{l} +h_{i-1jlk}^{u}+h_{i-1j-1lk}^{l}
% +h_{i-1j-1lk}^{u}+h_{i-1jlk}^{l},\\
% && \qquad \qquad \qquad \qquad 
% i=1,\cdots,N_1,\; j=1,\cdots,N_2, \;
% l=1,\cdots,N_3,\; k=1,\cdots,K,
& \sum_{s=1}^S h_s^k=1,\;\;h_s^k\in \{0,1\}, \;s=1,\cdots,S, \;k=1,\cdots,K,
\label{eq:mixed-integer-Rm-d}\\
&  0\leq \alpha_v^k\leq \sum_{s:{\bm x}_v\in {\cal V}_s} h_s^k, \; v=1,\cdots,V,\;k=1,\cdots,K,
\label{eq:mixed-integer-Rm-e}
\end{align}
\end{subequations}
where $s:{\bm x}_v\in {\cal V}_s$ means all $s\in \{1,\cdots,S\}$ satisfying that 
the vertice ${\bm x}_v$ belongs to the set ${\cal V}_s$. 
% where 
% %$\bdx_t^k=(x_{t1}^k,\cdots,x_{tm}^k,\cdots,x_{tM}^k)$ and
% ${\bm f}(\bdz,\bdxi^k)=(f_1(\bdz,\bdxi^k),f_2(\bdz,\bdxi^k),\cdots,f_m(\bdz,\bdxi^k))$.
%The last c
Constraint
(\ref{eq:mixed-integer-Rm-e})
%imposes that 
implies that
%the
only $\alpha_{v}$ values different from $0$ 
%can be those 
are those associated with the
vertices of %such 
the simplex.
Then we can reformulate the multi-attribute utility maximization problem 
$\max_{\bdz\in Z}\sum_{k=1}^K p_k[u_N({\bm f(\bdz,\bdxi^k)})]$ as 
an MIP
%mixed integer problem 
(see e.g. \cite{VAN10}),
\begin{subequations}
\label{eq:mixed-integer-Rm-2}
\begin{align}
\max\limits_{\bdz \in Z, {\bm \alpha}, {\bm h}} \; &  \sum_{k=1}^K p_k \sum_{v=1}^{V}\alpha_{v}^k u_v\\
{\rm s.t.}\;\;\; & 
\inmat{constraints }(\ref{eq:mixed-integer-Rm-b})-(\ref{eq:mixed-integer-Rm-e}),
\end{align}
\end{subequations}
where 
${\bm \alpha}:=({\bm \alpha}^1,\cdots,{\bm \alpha}^K)\in \R^{V\times K}$,
${\bm h}:=({\bm h}^1,\cdots,{\bm h}^K)\in \R^{S \times K}$.
If $f({\bdz,\bdxi})$ is linear in $\bdz$,
then problem (\ref{eq:mixed-integer-Rm-2}) is an MILP.
% For the cubic $\times_{m=1}^{M}[x_{mi},x_{mi+1}]$,
% let ${\bm v}_k=\left(v_k^{(1)},
% \cdots,v_k^{(M)}\right)$, $k=1,\cdots,2^M$
% be the points such that each $v_k^{(m)}$ ($m=1,\cdots,M$) is either $x_{m,i}$, $x_{m,i+1}$,  $m=1,\cdots,M$.
% Let
% $n({\bm v}_k)\in \{1,\cdots,M\}$ be the number of lower symbols $x_{mi}$, $m=1,\cdots,M$.
% Then the Lebesgue-Stieltjes measure induced by $u$ is
% \bgeq
% \mu_u([x_i, x_{i+1}]\times[y_j,y_{j+1}]\times [z_l,z_{l+1}])
% &=&\sum_{k=1}^{2^M}(-1)^{n({\bm v}_k)} u({\bm v}_k).
% \edeq
%The
We continue to assume that the ambiguity set ${\cal U}_N$ %can be
is constructed by pairwise comparisons of lotteriess ${\bm A}_l$ and ${\bm B}_l$ with $l=1,\cdots,M$.
Let 
$
\tilde{\cal U}_N={\cal U}_N \bigcap \{u_N: u_N \inmat{ is Lipschtz with its modulus }L\}.
$
% Then we can reformulate 
% (UPRO-N) 
% $\max_{\bdz\in Z} \min_{u_N\in \tilde{\cal U}_N}\bbe_{P_K}[u_N({\bm f(\bdz,\bdxi^k)})]$ 
% as:
% \bgeq
% {\cal U}_N
% &=&\{u_N\in \mathscr{U}_N: \bbe_{\mathbb{P}}[u({\bm B}_l)]\leq \bbe_{\mathbb{P}}[u({\bm A}_l)],\;l=1,\cdots,M\}\\
% &=&\left\{u_N\in \mathscr{U}_N:\sum_{i_1=1}^{N_1} \cdots\sum_{i_m=1}^{N_m}  \mathbb{P}({\bm B}_l=(x_{1i_1},\cdots,x_{mi_m}))u_{{1i_1},\cdots,{mi_m}}\right.\\
% &&\qquad \qquad \qquad  \qquad  \left. \leq \sum_{i_1=1}^{N_1} \cdots \sum_{i_m=1}^{N_m} \mathbb{P}({\bm A}_l=(x_{1i_1},\cdots,x_{mi_m}))u_{{1i_1},\cdots,{mi_m}}, 
% \;l=1,\cdots,M\right\}.
% \edeq
% Let $u_{l}=u({\bm x}_l)$,
% $l=1,\cdots,T$,
Consequently,
we can solve 
the multi-attribute utility preference robust  problem
%preference robust utility maximization problem 
by solving %the approximate 
MUPRO-N problem
$\max_{\bdz\in Z} \min_{u_N\in \tilde{\cal U}_N}\sum_{k=1}^K p_k[u_N({\bm f(\bdz,\bdxi^k)})]$ 
as 
an MIP:
%mixed-integer problem:
\begin{subequations}
\begin{align}
\label{eq:PRO_m-dim}
\disp{ \max_{\bdz\in Z} \min_{{\bm u}, {\bm \alpha},{\bm h} } }\; &
\sum_{k=1}^K p_k \sum_{v=1}^{V}\alpha_{v}^k u_v \\
{\rm s.t.} \;\; &
\inmat{constraints }(\ref{eq:mixed-integer-Rm-b})-(\ref{eq:mixed-integer-Rm-e}), \;\\
& u_{1_{j_1},\cdots,i_{j_{i}+1},\cdots,m_{j_m}}
\geq u_{1_{j_1},\cdots,i_{j_{i}},\cdots,m_{j_m}}, \nonumber \\
& \hspace{12em}
j_i=1,\cdots,N_i,
i=1,\cdots,m,
\label{eq:PRO_m-dim-c}\\
% &&  \frac{u_{{i_1},\cdots,x_{i_{\tau+1}},x_{i_m}}-u_{{i_1},\cdots,x_{i_{\tau}},x_{i_m}}}{x_{i_{\tau+1}}-x_{i_{\tau}}} \nonumber \\
% && 
% \geq \frac{u_{{i_1},\cdots,x_{i_{\tau}},\cdots,x_{i_m}}-u_{{i_1},\cdots,x_{i_{\tau-1}},\cdots,x_{i_m}}}{x_{i_{\tau}}-x_{i_{\tau-1}}},\qquad \nonumber\\
% && \qquad \qquad 
% \qquad \qquad 
% \tau=1,\cdots,m,i_j=1,\cdots,N_j,j=1,\cdots,m,\label{eq:PRO_m-dim-d} \\
& u_{1_{j_1},\cdots,i_{j_{i}+1},\cdots,m_{j_m}}
-
u_{1_{j_1},\cdots,i_{j_{i}},\cdots,m_{j_m}} \leq L(x_{i_{j_i+1}}-x_{i_{j_i}}), \nonumber \\
& \hspace{12em}
j_i=1,\cdots,N_i,i=1,\cdots,m, \qquad~~\;
\label{eq:PRO_m-dim-e}\\
& u_1=0,\; u_V=1,\label{eq:PRO_m-dim-f}\\
&\sum_{v=1}^{V}  \mathbb{P}({\bm B}_l=\bdx_v)u_v\leq \sum_{v=1}^{V} \mathbb{P}({\bm A}_l=
\bdx_v)u_v, 
\;l=1,\cdots,M,
\end{align}
\end{subequations}
where ${\bm u}\in \R^{V}$,
${\bm \alpha}=({\bm \alpha}^1,\cdots,{\bm \alpha}^K)\in \R^{V\times K}$,
${\bm h}=({\bm h}^1,\cdots,{\bm h}^K)\in \R^{S \times K}$.
Constraint (\ref{eq:PRO_m-dim-c}) represents the %increasing 
non-decreasing
property of the utility function.
% (\ref{eq:PRO_m-dim-d}) represents the concave property of the utility,
% where ``$\geq$'' can also be changed to ``$\leq$'' if the marginal utility is concave.
Constraint (\ref{eq:PRO_m-dim-e}) represents the Lipschitz continuity of $u_N$
and (\ref{eq:PRO_m-dim-f}) characterizes the normalization of $u_N$.
% }
% Note that the above maximin problem can be solved %by
% using
% derivative-free method.
% For the error arises from the above PLA of utility function,
% since we consider the ambiguity set ${\cal U}_N$,
% where no error occurs for it,
% we only need to show that $u_N(x,y)\rightarrow u(x,y)$ when $N\rightarrow \infty$ for all $(x,y)\in [\underline{x},\bar{x}]\times [\underline{y},\bar{y}]$.
% }





% {\color{blue}
% The function value $f({x},{y})$ is approximated by increments at sub-intervals of lines $AB: [(x_0,y_0),(x_{N_1},y_0)]$, $AC: [(x_0,y_0),(x_{0},y_{N_2})]$,
% $AD: [(x_0,y_0),(x_{N_1},y_{N_2})]$ and other diagonals parallel to AD.
% Here we consider the case that $N_1< N_2$.

% In Figure~\ref{fig:u(x,y)-1}~(a),
% we consider the case that $(x,y)$ located at $[x_i,x_{i+1}]\times [y_j,y_{j+1}]$
% under the line AD,
% that is, 
% $i\leq j$.
% \begin{figure}[!htbp]
%   \centering
%   \subfigure[case 1]{
% %     \label{subfig-main-contuor-ptb10} %% label for second subfigure
%     \includegraphics[width=2.4in]{figures/case-1.pdf}
%     }
%   \subfigure[case 2]{
% %     \label{subfig-main-contuor-ptb10} %% label for second subfigure
%     \includegraphics[width=2.4in]{figures/case-3.pdf}
%   }
%   \caption{$u(x,y)$}
%   \label{fig:u(x,y)-1} %% label for entire figure
% \end{figure}
% Note that
% % \underline{Case 1}.
% % In Figure~\ref{fig:u(x,y)}~(a),
% % if $i<j$,
% % then 
% \bgeq
% u(x_i,y_j)&=&\sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'}\\
% u(x_{i+1},y_{j+1})&=&\sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i,j'=i'-i+j}  v_{i'j'}\\
% u(x_{i+1},y_{j})&=&
% \sum_{ i'\leq i-j+1} v_{i'0}
% +\sum_{i-j+2\leq i'\leq i,j'=i'-i+j-1}  v_{i'j'}\\
% u(x_i,y_{j+1})&=&\sum_{i'\leq i-j-1} v_{i'0}+\sum_{i-j\leq i'\leq i-1, j'=i'-i+j+1}v_{i'j'}
% \edeq

% Then we have for $(x,y)\in [x_{i},x_{i+1}]\times [y_j,y_{j+1}]$ and  satisfies that
% ${y}\leq y_j+({x}-x_i)\frac{y_{j+1}-y_j}{x_{i+1}-x_i}$,
% we have 
% \bgeq
% u(x,y)&=&\frac{x_{i+1}-x}{x_{i+1}-x_i} \left(\sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'} \right)\\
% &&
% +\left( \frac{x-x_i}{x_{i+1}-x_i}-\frac{y-y_j}{y_{j+1}-y_j} \right) \left(\sum_{ i'\leq i-j+1} v_{i'0}
% +\sum_{i-j+2\leq i'\leq i,j'=i'-i+j}  v_{i'j'} \right) \\
% &&
% +\frac{y-y_j}{y_{j+1}-y_j}
% \left(\sum_{ i'\leq i-j} v_{i'0}
% +\sum_{i-j+1\leq i'\leq i,j'=i'-i+j}  v_{i'j'} \right)\\
% &=& \left( \sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'} \right)\\
% &&
% \left(\frac{y-y_j}{y_{j+1}-y_j}-\frac{x-x_i}{x_{i+1}-x_i}\right)
% \left(  \sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'} \right) +\frac{y-y_j}{y_{j+1}-y_j} v_{i,j}\\
% &&+\left( \frac{x-x_i}{x_{i+1}-x_i}-\frac{y-y_j}{y_{j+1}-y_j} \right) \left(\sum_{ i'\leq i-j+1} v_{i'0}
% +\sum_{i-j+2\leq i'\leq i,j'=i'-i+j}  v_{i'j'} \right)\\
% &=&  \left( \sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'} \right)
% +\frac{y-y_j}{y_{j+1}-y_j} v_{i,j}\\
% &&+ \left(\frac{y-y_j}{y_{j+1}-y_j}-\frac{x-x_i}{x_{i+1}-x_i}\right) \left( -v_{i-j+1,0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'} -
% \sum_{i-j+2\leq i'\leq i,j'=i'-i+j}  v_{i'j'} \right).
% % &&
% % +\frac{y-y_j}{y_{j+1}-y_j} \left(v_{i-j+1,0}+\sum_{i-j+2\leq i'\leq i,j'=i'-i+j}  v_{i'j'}-\sum_{i-j+1\leq i'\leq i,j'=i'-i+j-1}  v_{i'j'}
% % \right).
% \edeq
% Let 
% \bgeq
% \tilde{v}_{m,k}=
% \left\{ \begin{array}{ll}
% v_{k,0} & \inmat{if } m\leq N_1, k\leq m,\\
% v_{k,k-m} & \inmat{if } m\leq N_1, m+1\leq k \leq N_1,\\
% v_{0,k} & \inmat{if } N_1+1\leq m\leq N_1+N_2, k\leq m-N_1,\\
% v_{k,m+k} & \inmat{if } N_1+1\leq m\leq N_1+N_2, m-N_1\leq k \leq N_1+N_2.\\
% \end{array}
% \right.
% \edeq
% Then 
% \bgeq
% u(x,y)&=&\left[\sum_{k\leq i-1}  \tilde{v}_{i-j,k} +\frac{y-y_j}{y_{j+1}-y_j} \tilde{v}_{i-j,i}\right.\\
% && \left.
% +\left(\frac{y-y_j}{y_{j+1}-y_j}-\frac{x-x_i}{x_{i+1}-x_i}\right) \left(\sum_{i-j+1\leq k\leq i}\tilde{v}_{i-j+1,k} -\sum_{i-j+2\leq k\leq i}\tilde{v}_{i-j+2,k} -\tilde{v}_{i-j+1,i-j+1}
% \right)\right] \\
% && \times \mathbbm{1}_{x_i\leq x\leq x_{i+1}, y_j\leq y\leq y_{j+1}, {y}\leq y_j+({x}-x_i)\frac{y_{j+1}-y_j}{x_{i+1}-x_i}}\\
% &=& \sum_{m=1}^{N_1} \sum_{k=1}^{N_1} g_{m,k}(x,y)^T \tilde{v}_{m,k}.
% \edeq

% For $(x,y)\in [x_{i},x_{i+1}]\times [y_j,y_{j+1}]$ and  satisfies that
% ${y}\geq y_j+({x}-x_i)\frac{y_{j+1}-y_j}{x_{i+1}-x_i}$,
% we have 
% \bgeq
%     u(x,y) &=& \frac{y_{j+1}-y}{y_{j+1}-y_j} \left(\sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'}\right)\\
%     &&
%     +\left(\frac{y-y_j}{y_{j+1}-y_j}-\frac{x-x_i}{x_{i+1}-x_i}\right) 
%     \left(\sum_{i'\leq i-j-1} v_{i'0}+\sum_{i-j\leq i'\leq i-1, j'=i'-i+j+1}v_{i'j'}\right)\\
%     && 
%     +\frac{x-x_i}{x_{i+1}-x_i} \left(\sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i,j'=i'-i+j}  v_{i'j'}\right),\\
%     &=& \left( \sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'} \right)\\
% &&
% \left(-\frac{y-y_j}{y_{j+1}-y_j}+\frac{x-x_i}{x_{i+1}-x_i}\right)
% \left(  \sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'} \right) +\frac{x-x_i}{x_{i+1}-x_i} v_{i,j}\\
% &&+\left( -\frac{x-x_i}{x_{i+1}-x_i}+\frac{y-y_j}{y_{j+1}-y_j} \right) \left(\sum_{ i'\leq i-j-1} v_{i'0}
% +\sum_{i-j\leq i'\leq i-1,j'=i'-i+j+1}  v_{i'j'} \right)\\
%     &=&  \left( \sum_{i'\leq i-j} v_{i'0}+\sum_{i-j+1\leq i'\leq i-1, j'=i'-i+j}v_{i'j'} \right)
% +\frac{x-x_i}{x_{i+1}-x_i} v_{i,j}\\
% &&+ \left(\frac{y-y_j}{y_{j+1}-y_j}-\frac{x-x_i}{x_{i+1}-x_i}\right) \left( -v_{i-j,0}+\sum_{i-j\leq i'\leq i-1, j'=i'-i+j+1}v_{i'j'} -
% \sum_{i-j+1\leq i'\leq i,j'=i'-i+j}  v_{i'j'} \right).
%     % (x_{i+1},y_{j+1})
% \edeq
% Then 
% \bgeq
% u(x,y)&=&\left[\sum_{k\leq i-1}  \tilde{v}_{i-j,k} +\frac{x-x_i}{x_{i+1}-x_i} \tilde{v}_{i-j,i}\right.\\
% && \left.
% +\left(\frac{y-y_j}{y_{j+1}-y_j}-\frac{x-x_i}{x_{i+1}-x_i}\right) \left(\sum_{i-j\leq k\leq i-1}\tilde{v}_{i-j,k} -\sum_{i-j+1\leq k\leq i}\tilde{v}_{i-j+1,k} -\tilde{v}_{i-j,i-j}
% \right)\right]\\
% && \times \mathbbm{1}_{x_i\leq x\leq x_{i+1}, y_j\leq y\leq y_{j+1}, {y}\geq y_j+({x}-x_i)\frac{y_{j+1}-y_j}{x_{i+1}-x_i}}\\
% &=& \sum_{m=1}^{N_1} \sum_{k=1}^{N_1} g_{m,k}(x,y)^T \tilde{v}_{m,k}.
% \edeq
% \begin{figure}[!htbp]
%   \centering
%   \subfigure[case 3]{
% %     \label{subfig-main-contuor-ptb10} %% label for second subfigure
%     \includegraphics[width=2.4in]{figures/case-2.pdf}
%   }
%   \subfigure[case 4]{
% %     \label{subfig-main-contuor-ptb10} %% label for second subfigure
%     \includegraphics[width=2.4in]{figures/case-4.pdf}
%   }
%   \caption{$u(x,y)$}
%   \label{fig:u(x,y)} %% label for entire figure
% \end{figure}
% }



\section{Error bounds for  the PLA}
%the approximation}
\label{sec-errorbound}

In the previous section, we outline
computational schemes 
%for solving 
to solve BUPRO-N problem. In this section, we investigate 
the error bounds of the optimal value and the optimal solutions obtained from solving BUPRO-N problem when we use them to approximate the optimal value and optimal solutions of BUPRO problem.
Notice that 
%Since 
the only difference between 
the two maximin optimization problems 
%are identical except for the ambiguity sets, it suffices to look into the difference between 
is the feasible set of the inner minimization problem,
thus we proceed with our investigation 
by quantifying the difference between
$\calu_N$ and $\calu$ and then apply classical 
stability results in parametric programming to
derive the error bounds of the optimal value and optimal solutions.
Proofs of all technical results are deferred to the appendix. 

%{\color{RoyalBlue}
To ease the exposition, we 
%By defining 
write $\langle u, \psi_l \rangle$ for 
$\int_T u(x,y) d \psi_l(x,y)$,
%we can the pairwise comparison conditions 
and subsequently 
(\ref{eq:ambiguity_set}) as
\begin{equation}
\label{eq-pseume}
    \calu=\{u\in\scru: \la u,\bdps \ra \leq \bdc\},
\end{equation}
where $\bdps := (\psi_1(x,y), \ldots, \psi_M(x,y))^T \in \R^M$, $\bdc:=(c_1,\ldots,c_M)^T\in \R^M$.
Note that $\langle u, \psi_l \rangle$ should 
not be read as a kind of inner product as 
we cannot swap the positions between $u$ and $\psi_l$.
We adopt the 
%representation 
notation since
(\ref{eq-pseume})
clearly      indicates 
${\cal U}$ as the set of the solutions of the 
inequality system $\la u,\bdps \ra \leq \bdc$
relative to $\scru$.
% , and 
% the inner product is defined componentwise.
% Here we use a similar strategy to analyse the error bound with the analysis in the univariate UPRO problem \cite{GuX21} but there is a significant difference.
% In \cite{GuX21}, they use $\la \psi,u \ra:=\int_a^b \psi(x) d u(x)$ to define the ambiguity set of normalized utility functions.
% Since $u(a)=0$ and $u(b)=1$, then 
% \begin{equation}
% \label{eq-1D-exchange}
%     \la u,\bdps \ra + \la \bdps,u \ra =\psi(b),
% \end{equation}
% which is a constant where the inner product is similarly defined in the one-dimensional case.
% Hence $\la \psi,u \ra\leq c$ is equivalent to $-\la u,\psi \ra\leq c-\psi(b)$ and the analysis can be carried out with this equivalent formulation.
% In the multiattribute case, similar equality as \cref{eq-1D-exchange} does not exist.
% Hence exchanging the place of $u$ and $\bdps$ would leads to considerably different analysis.
% % }
% integrations $\int_T u(x,y) d \psi_l(x,y), l=1,\cdots,M$ in 
% linear coupling forms
% {\color{red}HX: what does the integral mean?
% We used this a lot in the proof of Thm 5.1.
% Please clarify.
% }
% , i.e, $\langle u, \psi_l \rangle:= \int_T u(x,y) d \psi_l(x,y)$. 
% Note that the ``linear coupling'' is a bit peculiar 
% {\color{red} HX: edit! in that $\langle u, \psi_l \rangle \neq 
% \langle \psi_l,u \rangle$. 
% When $u$ is a univariate utility function, 
% $\la u,\bdps \ra + \la \bdps,u \ra =c $ for some constant $c$.
% }
% However, when $u$ is a multivariate utility function,
% %Different from the one-dimensional case, 
% $\la u,\bdps \ra + \la \bdps,u \ra$  is not a constant. Fortunately, our discussions in this section do not require the two components in the coupling to swap positions.  
% %cannot be a constant,
% With the notation, we can effectively 
% write 
% \cref{eq:ambiguity_set} as
% \begin{equation}
% \label{eq-pseume}
%     \calu=\{u\in\scru: \la u,\bdps \ra \leq \bdc\},
% \end{equation}
% where $\bdps = (\psi_1(x,y), \ldots, \psi_l(x,y))^T \in \R^M$
% and $\bdc=(c_1,\ldots,c_M)^T$.
% and its impact on the optimal values and the optimal solutions. 
% To this end, we start by deriving a
% kind of Hoffman’s lemma for the linear system in (3) which quantifies the deviation of any $u\in\scru$
% from set $\calu$ by the residual of the linear system defining $\calu$.
% \subsection{Pseudo metric}
% For each fixed $u\in \mathscr{U}$,
% we write the integrals in Definition \ref{defi:ambguity-set} in bilinear forms 
% $\langle u, \psi_l \rangle:= \int_{\underline{x},\underline{y}}^{\bar{x},\bar{y}} u(x,y) d \psi_l(x,y)$. 
% % $\langle \psi_{1,m}, u_i\rangle:= \int_{\underline{x}}^{\bar{x}}\psi_{1,m} (x) d u_i(x)$ and $\langle \psi_{2,m}, u_j \rangle
% % :=\int_{\underline{y}}^{\bar{y}} \psi_{2,m} (y) d u_j(y)$
% % where $u_i=u(\cdot,\underline{y})$ and $u_i=u(\underline{x},\cdot)$.
% Let
% $ \bdps = (\psi_1(x,y), \ldots, \psi_l(x,y))^T \in \R^M$
% and $\bdc=(c_1,\ldots,c_M)^T$.
% Then we can rewrite (\ref{eq:reformulate-pair-definition}) as
% \begin{equation}
% \label{eq-pseume}
%     \calu=\{u\in\scru: \la u,\bdps \ra \leq \bdc\},
% \end{equation}
% where 
% all inner products are calculated componentwise, that is, 
% $\la u,\bdps \ra:=(\la u,\psi_1 \ra,\ldots,\la u,\psi_l \ra)$.
% Note that the ambiguity set $\calu$ is indeed the solution set of a system of linear inequalities defined in the functional space $\scru$.
% {\color{red} 
% Notice that in the one-dimensional case, for a normalized utility function defined over $[a,b]$, $\int_a^b u(t) d \psi(t)= \psi(b)-\int_a^b  \psi(t) d u(t)$ if $u(t)$ is a normalized continuous utility function,  
% {\color{green} HX: I cannot see why}
% {\color{blue} QW: I have modified the equation.}
% which implies that $\la u,\bdps \ra + \la \bdps,u \ra =c $ for some constant $c$.
% Different from the one-dimensional case, $\la u,\bdps \ra + \la \bdps,u \ra$ cannot be a constant, which makes the linear system vulnerable to the order of two components.}
% In what follows, we introduce a kind of pseudo metric which measures
% the discrepancy of any two sets in $\scru$.
% Let $\scrg$ be a set of measurable functions defined over $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$.
To quantify the difference between two utility functions,
we  define, 
for any $u,v\in\scru$,
the pseudo-metric between $u$ and $v$
%corresponding to the function set 
under the function set
$\scrg$ by
\begin{equation*}
    \dd_{\scrg}(u,v):=\sup_{g\in\scrg} |\la g,u \ra - \la g,v \ra|.
\end{equation*}
% Notice that the utility functions is of bounded variation over $T$ even though it only induces a negative measure since the conservative conditions, monotonicity, and normalized conditions hold.
It is easy to observe that $\dd_{\scrg}(u,v)=0$ if and only if $\la g,u \ra=\la g,v \ra$ for all $g\in\scrg$. 
In practice,
we may regard $\scrg$ as a set of “test functions” associated with some prospects and interpret $u$ as a measure induced by utility.
The pseudo-metric means that if $u$ and $v$ give the same average value
for each $g\in\scrg$, then they are regarded as “equal” under $\dd_{\scrg}$ 
%albeit 
although
they may not be identical.
Thus $\dd_{\scrg}$ is a kind of pseudo-metric defined over the space of utility-induced measures $\scru$.
This definition is in parallel to a similar definition in probability theory, where $u$ and $v$ are 
in a position of probability measures and the corresponding pseudo-metric is known as $\zeta$-metric, see \cite{Rom03}.
Here we continue to adopt the terminology although
the background is different.
%Note also that $\scrg$ must be restricted
%to ensure $\dd_{\scrg} (u,v)$ to be well defined.
% We state
% this in the following assumption.


% \begin{assumption}
% \label{A:finite-value}
% The set $\scrg$ is chosen such that $\dd_{\scrg}(u,v)$ is finite-valued for any $u,v\in\scru$.
% \end{assumption}

% \cref{A:finite-value} is satisfied when $\scrg$ takes some specific structures
% as in the next example.

\begin{example}
\label{exm-g}
% % \cite{GiS02}
Recall that $T= [\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$.

(a) Let 
$$
\scrg=\scrg_M:=\lt\{ g: T \rightarrow \R \,\lt|\; 
\inmat{g is measurable,} \sup_{\bdt\in T}|g(\bdt)| \leq 1 \rt.\rt\}.
$$ 
Then $\dd_{\scrg_M}(u,v)$ corresponds to the total variation metric and $\dd_{\scrg}(u,v)\leq 1$.

(b) Let 
\begin{equation}
\label{eq-kantorovich}
    \scrg=\scrg_K:=\{g:T\to\R \,|\; \inmat{g is Lipschitz continuous with the modulus bounded by 1}\}.
\end{equation}
Then $\dd_{\scrg_K}(u,v)$ corresponds to the Kantorovich metric in which case we have
%\begin{equation*}
$\dd_{\scrg_K}(u,v)=\int_T \|\bdt-\bdt'\|  d\pi(\bdt,\bdt') \leq \sqrt{(\bar{x}-\underline{x})^2+(\bar{y}-\underline{y})^2)}
$,
%\end{equation*}
where $\int_T \pi(\bdt,\bdt') d \bdt'=u(\bdt)$, $\int_T \pi(\bdt,\bdt') d \bdt=v(\bdt')$
and $\|\bdt\|$ denotes the Euclidean norm.

(c) Let $\scrg:=\scrg_L\cap\scrg_M$. Then $\dd_{\scrg}(u,v)$ corresponds to the bounded Lipschitz metric and $\dd_{\scrg}(u,v)\leq\min\left\{1,\sqrt{(\bar{x}-\underline{x})^2+(\bar{y}-\underline{y})^2)}\right\}$.

(d) Let 
\begin{equation*}
    \scrg=\scrg_I:=\{g:T\to\R \,|\; g=\1_{[\underline{x},x]\times[\underline{y},y]}(\cdot), (x,y)\in T \}.
\end{equation*}
Then $\dd_{\scrg_I}(u,v)$ corresponds to the Kolmogorov metric in which case we have $\dd_{\scrg_I}(u,v)\leq~1$.
\end{example}


For any two sets $U, V\subset \mathscr{U}$, 
let
$
\mathbb{D}_{\mathscr{G}}(U,V)  := \sup_{u\in U}\inf_{v\in V} \dd_{\mathscr{G}}(u,v),
$
which quantifies the deviation of $U$ from $V$
and
$
\mathbb{H}_{\mathscr{G}}(U,V) := \max\left\{\mathbb{D}_{\mathscr{G}}(U,V), \mathbb{D}_{\mathscr{G}}(V,U)\right\},
$
which denotes the Hausdorff distance between the two sets under the pseudo-metric.
By convention, when $U=\{u\}$ is a singleton, we write
the distance $\dd_{\mathscr{G}}(u,V)$ from $u$ to set $V$ 
%reduces to
rather than
$\mathbb{D}_{\mathscr{G}}(U,V)$. 


%\subsection{Hoffman's lemma}
Using the pseudo-metric, we 
%are able to 
can
derive an error bound of any utility function $u\in\scru$ deviating from $\calu$ in terms of the residual of the linear system defining ${\cal U}$. 
This 
%kind 
type
of result is known as Hoffman’s lemma.
We state this in the next lemma.



\begin{lemma}[Hoffman's lemma]
\label{lem-hof}
Consider (\ref{eq-pseume}).
%Suppose 
%\cref{A:finite-value} holds 
Assume: (a)
 $\mathscr{G}$ is chosen so that 
the resulting pseudo-distance 
between any two utility functions is finite-valued,
and 
(b) there exist a constant $\alpha$ and a function $u^0\in\calu$ such that 
\begin{equation}
\label{eq-sla}
    \la u^0, \bdps \ra -\bdc+\alpha \mathbb{B}^M \subset \R_-^M.
\end{equation}
Then 
\begin{equation}
\label{eq-hof}
    \dd_{\scrg} (u,\calu) \leq \frac{\dd_{\scrg}(u,u^0)}{\alpha} 
    \|(\la u, \bdps \ra -\bdc)_+\| \quad \forall u\in\scru,
\end{equation}
where $({\bm a})_+:=\max\{0,{\bm a}\}$ which is taken componentwise.
% {\color{red}$\|\cdot\|$ is any norm in $\R^M$?}
\end{lemma}
%{\color{red} HX: 
%comment on Slater?}

Condition (\ref{eq-sla}) is known as %the 
Slater’s condition. It implies that there is at least one utility function $u^0$ such that 
 $\langle u,\bdps\rangle $ lies in the interior of $\R^M_-$. 
 This kind of condition %consition
 is widely used in the literature of %the 
 Hoffman’s lemma for linear and convex systems, see \cite{Rob75} and references therein. 
% Inequality (\ref{eq-hof}) gives an upper bound
% for the deviation of the utility function $u\in\scru$ from the ambiguity set $\calu$ 
% and the bound can be easily computed when $\scrg$ takes special structures as in Example \ref{exm-g}. 
% Note that the conditions that all utility functions are defined over a rectangle and their ranges are scaled to $[0,1]$ is not used in this lemma, thus (\ref{eq-hof}) holds when
% the utility functions are defined over $\R^2$ and non-normalized once $\dd_\scrg(u,u^0)$ is bounded.
% Note also that the residual term would be same if $u$ and $v$ give same inner product value.
Since the proof of Hoffman’s lemma in the case that utility function in $\R^2$ is similar to the case with utility function in $\R$ (\cite{GXZ21}), we omit the details.


% {\color{red} QW: Almost same, can be removed. 

% \textbf{Proof.}
% Let $\rho:=\|(\la u, \bdps \ra -\bdc)_+\|$ and $\hatu:=(1-\frac{\rho}{\rho+\alpha}) u+\frac{\rho}{\rho+\alpha} u^0$.
% Since $u, u^0\in\scru$ and $\scru$ is a convex set, then $\hatu\in\scru$. 
% Let $\bm{e}_M\in\R^M$ be a vector with all component being $1$.
% Then $\la u,\bdps \ra -\bdc\leq \rho\bm{e}_M$
% and hence 
% \begin{eqnarray*}
%     \la \hatu,\bdps \ra -\bdc
%     & = & \lt( 1-\frac{\rho}{\rho+\alpha} \rt) (\la u,\bdps \ra -\bdc)
%     +\frac{\rho}{\rho+\alpha} (\la u^0,\bdps \ra-\bdc) \\
%     && \leq \lt( 1-\frac{\rho}{\rho+\alpha} \rt) \rho\bm{e}_M-\frac{\rho}{\rho+\alpha} \alpha \bm{e}_M =0,
% \end{eqnarray*}
% which means $\hatu\in\calu$.
% Hence
% \begin{eqnarray*}
%     \dd_{\scrg}(u,\calu) 
%     &\leq& \dd_{\scrg}(u,\hatu) =\sup_{g\in\scrg} 
%     \lt\{ \lt| \la g,u \ra - 
%     \lt\langle g, \lt( 1-\frac{\rho}{\rho+\alpha} \rt) u +\frac{\rho}{\rho+\alpha} u^0 \rt\rangle \rt| \rt\} \\
%     &=& \frac{\rho}{\rho+\alpha} \sup_{g\in\scrg} |\la g,u \ra-\la g,u^0 \ra| \\
%     &\leq& \frac{\|(\la u,\bdps \ra -\bdc)_+\|}{\alpha} \dd_{\scrg}(u,u^0),
% \end{eqnarray*}
% which leads to (\ref{eq-hof}).
% }


\subsection{Error bound on the ambiguity set}

We move on to quantify the difference between $\calu$ and $\calu_N$.
First, we give the following technical result.

\begin{proposition}
\label{prop-d}
Let $u\in\scru$ and $u_N$ be the PLA of $u$ defined as in Proposition~\ref{prop-uti-N}, then the following assertions hold:

(i) If $\scrg=\scrg_K$, 
% and $u$
% satisfies the 
% conservative property (\ref{eq:conservative}), 
then
\begin{equation}
    \label{}
    \dd_{\scrg_K} (u,u_N)\leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2},
\end{equation}
where $\scrg_K$ is defined as in Example~\ref{exm-g}~(b)
 and
\begin{equation}
\label{eq-be}
    \beta_{N_1}:=\max_{i=2,\ldots,N_1}(x_i-x_{i-1}), \; \beta_{N_2}:=\max_{j=2,\ldots,N_2}(y_j-y_{j-1}).
\end{equation}

(ii) If $\scrg=\scrg_I$ and 
% Assume that 
$u$ is Lipschitz continuous over
% $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$ 
$T$
with the modulus $L$,
% the conditional utility functions
% $u_i$ and $u_j$ are Lipschitz continuous over $[\underline{x},\bar{x}]$ and $[\underline{y},\bar{y}]$ with modulus $L_1$ and $L_2$. 
% and $u_{N}$ is the 
%piecewise linear approximation
% PLA of $u$ defined as in \cref{prop-uti-N}.
then 
\begin{equation}
\label{eq-d}
    \dd_{\scrg_I} (u,u_{N}) \leq 
    %{\color{blue}  |u-u_{N}|_\infty \leq}
    2L\lt( \beta_{N_1}+\beta_{N_2} \rt),
\end{equation}
%{\color{red} The blue one is incorrect.}
where $\scrg_I$ is defined as in Example~\ref{exm-g}~(d).
% Moreover,  $L\geq 1/(\bar{x}-\underline{x}+\bar{y}-\underline{y})$.


% {\color{red}Wei: since $\bar{x}>\underline{x}$, we may omit $|\cdot|$. Moreover, we may specify the definition of Lipschitz continuous w.r.t norm $\|\cdot\|_1$.}
% $L_1\geq 1/(\bar{x}-\underline{x})$ and $L_2\geq 1/(\bar{y}-\underline{y})$.
\end{proposition}

% {\color{red} To be commented, the background of Lipschitz of utility.}

With Lemma~\ref{lem-hof} and Proposition~\ref{prop-d}, we are ready to quantify the difference between $\calu_N$ and $\calu$.

\begin{theorem}[Error bound on $\mathbb{H}_{\scrg}(\calu_N,\calu)$]
\label{thm-erramb}
Assume: 
(a) %the 
Slater's condition
%s (a) and 
in Lemma~\ref{lem-hof} is satisfied;
%\cref{A:finite-value} holds; 
%(b) the Slat-er's condition \cref{eq-sla} is fulfilled with $u^0\in\calu$;
(b) 
%{\color{RoyalBlue} 
$\int_T d\psi_l(\bdt)$ is well-defined;
{\color{black}
(c)
% Assume that 
$u$ is Lipschitz continuous over
% $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$ 
$T$
with the modulus $L$.}
% and bounded. 
% $\psi_l$
% is continuously differentiable except at finite number of points.
%{\color{red}wei: here $\psi_l$ should be of bounded variation. }
Then there exist a positive constant  $\hat{\alpha}<\alpha$, $N^0_1$ and $N^0_2$ such that the following assertions hold for specific $\scrg$ defined as in Example~\ref{exm-g}.

(i) If $\scrg=\scrg_K$,
% {\color{red}and $u$
% satisfies the 
% conservative property (\ref{eq:conservative})}, 
then
% \begin{equation}
% \label{eq-erram-L}
% \begin{split}
%     \mathbb{H}_{\scrg_K}(\calu,\calu_{N})\leq & 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2} (1-u(\underline{x},\bar{y})-u(\underline{y},\bar{x})) + L(\beta_{N_1}+ \beta_{N_2}) \\
%     &  \times \frac{\lt((\bar{x}-\underline{x})^2+(\bar{y}-\underline{y})^2\rt)^{1/2}}{\hat{\alpha}} \lt(\sum_{m=1}^M \lt| \int_T d \psi_l (t)\rt|^2\rt)^{1/2},
% \end{split}
% \end{equation}
\begin{equation}
\label{eq-erram-L}
\begin{split}
    \mathbb{H}_{\scrg_K} (\calu,\calu_{N}) \leq  & 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2} \\
    &  + L(\beta_{N_1}+ \beta_{N_2}) 
    \frac{\lt((\bar{x}-\underline{x})^2+(\bar{y}-\underline{y})^2\rt)^{1/2}}{\hat{\alpha}} \lt(\sum_{l=1}^M \lt| \int_T d \psi_l (\bdt)\rt|^2\rt)^{1/2}
\end{split}
\end{equation}
for all $N_1\geq N_1^0$ and $N_2\geq N_2^0$.


(ii) If $\scrg=\scrg_I$, then
\begin{equation}
\label{eq-erram-I}
    \mathbb{H}_{\scrg_I}(\calu,\calu_{N})\leq
    L\lt( \beta_{N_1}+\beta_{N_2} \rt) \lt( 2+\frac{1}{\hat{\alpha}}  \lt(\sum_{l=1}^M \lt| \int_T d \psi_l (t)\rt|^2\rt)^{1/2} \rt)
\end{equation}
for all $N_1\geq N_1^0$ and $N_2\geq N_2^0$,
where $\beta_{N_1}$, $\beta_{N_2}$ are defined as in (\ref{eq-be}) and $\beta_{N_i}\to 0$ as $N_i\to\infty$ for $i=1,2$. 
\end{theorem}


The constant $\hat{\alpha}$ is related to %the
Slater’s condition for the linear system when the utility function is restricted to space $\scru_N$, 
{\color{black} see \cite[pages 16]{GXZ21}.}
It is well-known that Kolmogorov metric $\dd_{\scrg_I}$ is tighter than Kantorovich metric $\dd_{\scrg_K}$ defined as in Example~\ref{exm-g}~(b) and (d) because the former is about the largest difference between two  utility functions whereas the latter is about the area between the graphs of the two utility functions, see \cite{GiS02}.
Consequently,
$\mathbb{H}_{\scrg_I}(\calu_N,\calu)$ is tighter than $\mathbb{H}_{\scrg_K}(\calu_N,\calu)$.
% Denote by $B_1$ and $B_2$ the respective bounds in (\ref{eq-erram-L}) and 
% (\ref{eq-erram-I}). 
% {\color{red} Then $B_1-B_2=???$. To be continued. }
% Note also that in \cref{thm-erramb}, the error bounds
% in \cref{eq-erram-L} and \cref{eq-erram-I} comprise two terms: 
% {\color{red} HX ???
% the first term bounds the difference between $u$ and $u_N$ under the pseudo-metric $\dd_{\scrg}$ and the second term bounds the error arising from the moment system when $u$ is approximated by $u_N$.
% {\color{red}To see this,????}
% Note 
% also that
The following corollary shows that the second term disappears in both cases 
% {\color{red}(5.7) or (5.8) or both and why?} 
when $\psi_l$, $l=1,\ldots,M$
are simple functions. 
% taking constant values 
% %of each of the cells of
% %over 
% over each cell of $T$.
% $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$ 
% {\color{red} 
% with jumps at some $(x_i,y_j)$ 
% ???} 
% for 
% % $i=1,\ldots,N_1, j=1,\ldots,N_2$, and 
% $l=1,\ldots,M$

% are simple functions {\color{purple}with jumps} at some gridpoints of $u_N$.
% (x,y)$. 
% }
% The following corollary states this.


\begin{corollary}
\label{cor-err-optval-discrete}
Let $u\in\scru$. Assume that $u$ is Lipschitz continuous over
% $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$ 
$T$
with the modulus $L$.
% {\color{red}and $u$
% satisfies the 
% conservative property (\ref{eq:conservative})}.
% $u_i$ and $u_j$ are Lipschitz continuous over $[\underline{x},\bar{x}]$ and $[\underline{y},\bar{y}]$ with modulus $L_1$ and $L_2$. 
 If $\psi_l$ is a simple function 
taking constant values 
%of each of the cells of
%over 
over each cell of  $T$
% $[\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$ 
% {\color{red} 
% with jumps at some $(x_i,y_j)$ 
% ???} 
for 
% $i=1,\ldots,N_1, j=1,\ldots,N_2$, and 
$l=1,\ldots,M$, then $\mathbb{H}_{\scrg_K}(\calu_{N},\calu)\leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2}$ and $\mathbb{H}_{\scrg_I}(\calu_{N},\calu)\leq 2L\lt( \beta_{N_1}+\beta_{N_2} \rt)$.
\end{corollary}

% {\color{purple}
The corollary provides us with some useful insights: 
if $\psi_l$ is a simple function for $l=1,\cdots,M$
(which 
%is the case that $\psi_l$ corresponds to discrete random vector
corresponds to the case when the DM's preference is elicited via pairwise comparison lotteries), 
then we can construct the grid of $T$ in such a way that $\psi_l$ 
is constant over $T_{i,j}$ (the vertices of the cells comprise
all outcomes of the lotteries).
% we may choose 
% the outcomes of the 
% $(x_i,y_j)$ from those points {\color{red}where $\psi_l$ jumps.} 
In this way, we may effectively reduce 
the modelling error arising from  PLA of the utility function.
Note also that in this case, %the 
Slater’s condition is not 
%needed 
required,
which means 
that the error bound holds
for all $N_1$ and $N_2$ rather than for them to be sufficiently large.
% }




\subsection{Error bound on the optimal value and the optimal solution}

We are now ready to quantify the difference between the BUPRO-N and BUPRO models. 
Let $\vt_N$ and $\vt$ denote the respective optimal values, and $Z_N^*$
and $Z^*$ denote the corresponding sets of optimal solutions.

\begin{theorem}[Error bound on the optimal value and the optimal solution]
\label{thm-optval}
Assume the settings and conditions of  
%If the assumptions in 
Theorem~\ref{thm-erramb}.
%are satisfied.
%}
% Assume: 
% (a) \cref{A:finite-value} holds; 
% (b) the Slater's condition \cref{eq-sla} is fulfilled with $u^0\in\calu$.
% {\color{red} HX: why not condition (c)??}
Then the following assertions hold.

(i)
\begin{equation}
\label{eq-err-vt}
    |\vt_{N}-\vt| \leq L\lt( \beta_{N_1}+\beta_{N_2} \rt) \lt( 3+\frac{1}{\hat{\alpha}}  \lt(\sum_{l=1}^M \lt| \int_T d \psi_l (t)\rt|^2\rt)^{1/2} \rt)
\end{equation}
for all $N_1\geq N_1^0$ and $N_2\geq N^0_1$, where $L$, $\hat{\alpha}$, $\beta_{N_1}$, $\beta_{N_2}$, $N_1^0$ and $N^0_2$ are defined as in Theorem~\ref{thm-erramb}.

(ii) 
Let $v(\bdz):=\min_{u\in\calu} \bbe_P[u(\bdf(\bdz,\bdxi))]$. 
Define the growth function 
$\Lambda(\tau):=\min\{v(\bdz)-\vt^*:d(\bdz,Z^*)\geq \tau,\forall\, \bdz\in Z\}$
and $\Lambda^{-1}(\eta):=\sup\{\tau:\Lambda(\tau)\leq\eta\}$ where $d(\bdz,Z^*)=\inf_{\bdz'\in Z^*} \|\bdz-\bdz'\|$.
Then 
\begin{equation}
\label{eq-err-so}
    \mathbb{D}(Z_{N}^*,Z^*)\leq \Lambda^{-1} \lt( 2L\lt( \beta_{N_1}+\beta_{N_2} \rt) \lt( 3+\frac{1}{\hat{\alpha}}  \lt(\sum_{l=1}^M \lt| \int_T d \psi_l (t)\rt|^2\rt)^{1/2} \rt) \rt),
\end{equation}
where $\mathbb{D}(Z_{N}^*,Z^*):=\sup_{\bdz\in Z_{N}^*} \inf_{\bdz' \in Z^*} \|\bdz-\bdz'\|$.
\end{theorem}





\begin{remark}
\label{rem:distance}
(i) Note that $\vt$ is not computable whereas $\vt_N$ is.
The error bound established in (\ref{eq-err-vt}) gives
the DM an interval centred at $\vt_N$ which contains $\vt$.
We can say that for a specified precision $\epsilon$, we can use the inequality to estimate $\beta_N$ such that $|\vt_N-\vt|\leq\epsilon$.
In the case when $x_1,\ldots,x_{N_1}$ and $y_1,\ldots,y_{N_2}$ 
are evenly spread over $[\underline{x},\bar{x}]$ and $[\underline{y},\bar{y}]$, we know the specified precision is reached when 
$L\lt( \frac{\bar{x}-\underline{x}}{N_1}+\frac{\bar{y}-\underline{y}}{N_2} \rt) \lt( 3+\frac{1}{\hat{\alpha}}  \lt(\sum_{l=1}^M \lt| \int_T d \psi_l (\bdt)\rt|^2\rt)^{1/2} \rt)\leq\epsilon$.

(ii) The error bound (\ref{eq-err-vt}) is established without restricting
%requiring 
the utility functions to being concave and it is derived under the PLA scheme. We envisage that similar results may be obtained using spline approximation and leave interested readers to investigate. 
Note that these are mesh-dependent approximation schemes which means that the quality of approximation depends on the number of
gridpoints $N=N_1N_2$.
% In numerical optimization, there are many mesh-free schemes 
%for approximating 
% to approximate a continuous function.

(iii) 
Let $u^{\rm worst}_N\in \arg\min_{u_N\in {\cal U}_N} \sum_{k=1}^K p_k u_N({\bm f}(\bdz^N,\bdxi^k))$,
where $\bdz^N$ denotes the optimal solution of (\ref{eq:MAUT-robust-N-dis}).
Then
$\dd_{\mathscr{G}_I}(u^*,u_N^{\rm worst})=\sup_{\bdt\in T} |u^*(\bdt)-u_N^{\rm worst}(\bdt)|$.
Let $u_N^*$ denote the PLA of $u^*$ with identical values at the gridpoints.
Then 
$$
\dd_{\mathscr{G}_I}(u^*,u^*_N)=\sup_{\bdt\in T}|u^*(\bdt)-u^*_N(\bdt)|=\sup_{\substack{i=1,\cdots,N_1-1, \\ j=1,\cdots,N_2-1}} \sup_{\bdt \in T_{i,j}}|u^*(\bdt)-u_N^*(\bdt)|\leq L (\beta_{N_1}+\beta_{N_2}),
$$
and 
$\dd_{\mathscr{G}_I}(u^*_N,u_N^{\rm worst})=\sup_{\bdt\in T} |u_N^*(\bdt)-u_N^{\rm worst}(\bdt)|=\max_{i=1,\cdots,N_1,j=1,\cdots,N_2}|u_N^*(\bdt_{i,j})-u_N^{\rm worst}(\bdt_{i,j})|$,
where $\bdt_{i,j}:=(x_i,y_j)$.
In Section~\ref{sec:numerical results},
we will examine how $u_{N}^{\rm worst}$ converges to $u^*$ as the number of queries increases.

(iv) The error bounds established 
under $\dd_{\mathscr{G}_I}$ and $\dd_{\mathscr{G}_K}$ require
conservative property of the utility function.
Specifically, the bound of Hausdorff distance between $\calu$ and $\calu_N$ is related to two terms $\dd_{\scrg}(u,u_N)$ and $\dd_{\scrg}(u_N,u_N^0)$ (see (\ref{eq-uU})),
where $u\in\calu$, $u_N$ is the PLA of $u$, and $u_N^0$ is defined in (\ref{eq-sla-0}).
It can be observed that in the case that $\scrg=\scrg_K$, the bound of $\dd_{\scrg}(u,u_N)$ relies on the conservative property as shown in (\ref{eq-u-u-N}), whereas in the case $\scrg=\scrg_I$, the bound of $\dd_{\scrg}(u_N,u_N^0)$ relies on the conservative property in Example~\ref{exm-g} (d). This makes it difficult to extend 
the theoretical results to multivariate utility case. We leave this for future research. 



% whereas 
% the error bound established 
% under $\dd_{\mathscr{G}_I}$ does not.
% {\color{red} 
% The latter can be easily extended to multivariate utility case 
% whereas the former requires more work.}

% Since the conservative condition in multivariate 
% case is presented in a different manner from bivariate case, it means that we will not be able to
% present similar bounds for the multivariate utility case under the $\dd_{\mathscr{G}_K}$




% in this section may be extended to multivariate utility function. 
% In that case, the conservative property is presented in more complex manner and consequently, we 
% cannot link a utility function satisfying the conservative condition to a negative measure.
% However, since we assume the utility function is componentwise non-decreasing and bounded, the difference between errors of bi-attribute case and multi-attribute caused by conservative condition only appears when $\scrg=\scrg_K$.



\end{remark}

\begin{example}
Consider the ambiguity set 
%is
defined as in (\ref{eq:ambi-U-ex}). 
Since ${\bm A}$ is preferred,
there exists some $u^0\in\scru$ and a small positive number $\epsilon$ such that $\int_T u^0(x,y) d(F_{\bm A}(x,y)-F_{\bm B}(x,y))<-\epsilon$.
Let $\alpha=-\epsilon-\int_T u^0(x,y) d(F_{\bm A}(x,y)-F_{\bm B}(x,y))>0$.
Then %the 
Slater’s condition (\ref{eq-sla}) is satisfied.
Let $\hat{\alpha}\in(0,\alpha)$ be such that $\int_T u^0_N(x,y) d(F_{\bm A}(x,y)-F_{\bm B}(x,y))+\hat{\alpha}
%\subset
\in 
\R_-$.
%Observe that in this example, 
Observe that
$\psi(x,y) :=F_{\bm A}(x,y)-F_{\bm B}(x,y)$ 
%satisfying
satisfies 
 $|\psi(x,y)|\leq2$ for all $(x,y)\in T$.
By Theorem~\ref{thm-optval},
$
|\vt-\vt_N| \leq L\lt( \beta_{N_1}+\beta_{N_2} \rt) \lt( 3+\frac{2}{\hat{\alpha}} \rt).
$
Moreover, if ${\bm A}$ and ${\bm B}$ follow discrete distributions, then $\psi$ is a step function. 
In that case, we may select the gridpoints in ${\cal X}\times {\cal Y}$ (in the PLA) from the gridpoints
%breakpoints
of $\psi$ and subsequently it follows by Corollary~\ref{cor-err-optval-discrete} that $|\vt-\vt_N| \leq \mathbb{H}_{\scrg_I} (\calu,\calu_{N})+ L(\beta_{N_1}+\beta_{N_2})\leq 3L(\beta_{N_1}+\beta_{N_2})$.
\end{example}
% {\color{red}Wei: this example cannot illustrate the derived error bound is tight. 
% First, $\phi_1(x)$ and $\phi_2(x)$ are not independent. 
% Second, one dimensional example cannot be used for the two-dimensional case. 
% Third, other issues. 
% }
% {\color{blue}
% Sainan: I know the  following example is not for specific ${\cal U}$, ${\cal U}_N$, $P$, $\bdxi$, $f$.
% I just explain why we say the error bound derived in this way is tight. Please try to find a specific example of these function and  sets.
% It's easy to extend to $2$-dimension case.

% }
% {\color{blue}
% \begin{example}
% [Tight error bound]
% The error bound on the optimal value 
% established in Theorem \ref{thm-optval} is tight. This
% can be observed from  the estimation
% \bgeqn
% |\vt_N-\vt|
% &\leq&
% \max_{{\bm x} \in X}\left|\displaystyle{ \min_{u\in \mathcal{U}}  }  \;\;  \bbe_P[u(f({\bm x},{\bm \xi}))]
% -
% \displaystyle{ \min_{u_N\in \mathcal{U}_N}}    \;\;  \bbe_P[u_N(f({\bm x},{\bm \xi}))]\right| \nonumber\\
% &\leq&
% \mathbb{H}_{{\mathscr G}_I}( \mathcal{U}_N, \mathcal{U}).
% % =\mathbb{H}_{\mathscr{G}_I}( \mathcal{U}_N, \mathcal{U})
% \label{eq:thm-appro-4}
% \edeqn
% Note that equality may hold in the first inequality.
% %states that 
% % the difference of the optimal values 
% % is bounded by the maximal difference of the objective functions over the feasible set $X$.
% To see this more clearly,
% let 
% $$
% \phi_1(\bdx) :=\displaystyle{ \min_{u\in \mathcal{U}}}  \bbe_P[u(f({\bm x},{\bm \xi}))]
% \inmat{  and  }
% \phi_2(\bdx) :=
% \displaystyle{ \min_{u_N\in \mathcal{U}_N}} \bbe_P[u_N(f({\bm x},{\bm \xi}))]
% $$
% Then
% $$
% \vt-\vt_N = \max_{\bdx\in X} \phi_1(\bdx)
% -\max_{\bdx\in X} \phi_2(\bdx).
% $$
% Consequently the first inequality in (\ref{eq:thm-appro-4}) can be equivalently written as
% \bgeqn
% \label{eq:ppi1-[phi2]}
% |\vt-\vt_N|
% \leq  \max_{\bdx\in X} |\phi_1(\bdx)-
% \phi_2(\bdx)|.
% \edeqn
% Equality may hold in  (\ref{eq:ppi1-[phi2]}).
% %the above equation. 
% To illustrate this \footnote{This is an illustration only, constructing 
% an example with details of $u$, $u_N$ and $f$ would be far more complicated 
% and would be usually pathological.},
% consider a simple one dimensional case:
% $$
% \phi_1(x_1,x_2)=\left(\frac{x_1+x_2}{2}\right)^{1/2},\;
% \phi_2(x_1,x_2)=2\left(\frac{x_1+x_2}{2}\right)^{1/3}
% $$
% and $X=[0,1]\times [0,1]$.
% It is easy to see that
% $$
% \max_{\bdx\in [0,1]\times [0,1]}\phi_1(\bdx) = 1,
% \quad \inmat{and}\quad
% \max_{\bdx\in [0,1]\times [0,1]}\phi_2(\bdx) = 2,
% $$
% The maximum difference of the two functions is
% %are attained at $1$.
% $$
% \max_{\bdx\in [0,1]\times [0,1]} 
% \left|\left(\frac{x_1+x_2}{2}\right)^{1/2}-2\left(\frac{x_1+x_2}{2}\right)^{1/3}\right|=1.
% $$
% Thus
% $$
% \left|
% \max_{\bdx\in [0,1]\times [0,1]}\phi_1(\bdx)
% -
% \max_{\bdx\in [0,1]\times [0,1]}\phi_2(\bdx)
% \right|= |1-2| =\max_{\bdx\in [0,1]\times [0,1]} 
% \left|\left(\frac{x_1+x_2}{2}\right)^{1/2}-2\left(\frac{x_1+x_2}{2}\right)^{1/3}\right|. 
% $$
% See Figure \ref{fig:examples_phi12}.
% The second inequality in (\ref{eq:thm-appro-4}) is also tight.
% To see this, assume without loss of generality 
% that the minimum 
% in $\min_{u\in \mathcal{U}}  \bbe_P[u(f({\bm x},{\bm \xi}))]$ and $
% \displaystyle{ \min_{u_N\in \mathcal{U}_N}}  \bbe_P[u_N(f({\bm x},{\bm \xi}))]$
% can be attained. Let $u^{\bdx}$ and $u_N^{\bdx}$  be respective optimal solutions. Then 
% \bgeq
% \min_{u\in \mathcal{U}}  \;\;  \bbe_P[u(f({\bm x},{\bm \xi}))]
% -
% \displaystyle{ \min_{u_N\in \mathcal{U}_N}}    \;\;  \bbe_P[u_N(f({\bm x},{\bm \xi}))]
% &=& 
%  \bbe_P[u^{\bdx}(f(\bdx,\bdxi))]-  \bbe_P[u^{\bdx}_N(f(\bdx,\bdxi))]\\
% &\leq& \sup_{{\bm t}\in [a,b]\times [a,b]} |u^{\bdx}({\bm t})- u^{\bdx}_N({\bm t})|\\
% &\leq & \mathbb{H}_{{\mathscr G}_I}( \mathcal{U}_N, \mathcal{U}).
% \edeq
% From the inequalities above, we can see that equality
% may hold in each of the inequalities depending on 
% $f(\bdx,\bdxi)$, $u^x$ and $u_N^x$.
% % If $\{\bdx^T\bdxi: \bdx\in X, \bdxi\in \Xi\}=[a,b]$,
% Again construction of an example will be pathological.
% % {\color{red}Sainan: I added an example to illustrate the tight error bound.}
% \begin{figure}[!ht]
% \hspace{0.3cm}
% \begin{minipage}[t]{\linewidth}
% \centering
% \includegraphics[width=3.2in]{figures/phi_2d.pdf}
% % \text{\tiny{Increasing utility: optimal values}}
% %\label{fig: interval_quasiconcave-new}
% \end{minipage}
% \caption{Functions $\phi_1$, $\phi_2$, $|\phi_1-\phi_2|$ over $[0,1]$ attains maximum $1$ at $\bdx=(1,1)$.}
% \label{fig:examples_phi12}
% \end{figure}
% \end{example}
% }


\section{
%Preference robust 
BUPRO models for 
constrained optimization problem}
\label{sec:constrained}


%{\color{black}
In this section, we extend the UPRO model to the
expected utility maximization problem with expected utility constraints. Specifically, we consider the following problem:
\begin{equation}
\label{eq:SPR-x}
\begin{split}
    {\vt}^*:=\max_{\bdz\in Z} \;\; & \bbe_P[ u(\bdf(\bdz,\bdxi))] \\
    \st \;\;\, &
    \bbe_P[u(\bdg(\bdz,\bdxi))] \geq c,
\end{split}
\end{equation}
% \begin{equation}
% \begin{array}{cl}
% \displaystyle{\max_{\bdz\in Z}  }  & \bbe[ u(\bdf(\bdz,\bdxi))]\\
% \inmat{s.t.} &
% \bbe[u(\bdg(\bdz,\bdxi))] \geq c,
% %\bbe[u(Y(\zeta))].
% \end{array}
% \label{eq:SPR-x}
% \end{equation}
where $\bdf$ and $\bdg$ are continuous functions and $c$ is a constant. We may interpret 
$\bdf$ as the total return of a portfolio 
and $\bdg$ is an important part of it or vice versa. 
Suppose that the true utility function is unknown but it is possible to construct an ambiguity set ${\cal U}$
using partially available information as we discussed earlier. 
Then we may consider the following maximin preference robust optimization problem
\begin{equation}
\label{eq:PRO-x}
\begin{split}
   \hat{\vt}:= \max_{\bdz\in Z} \;\;
    \min_{u\in {\cal U}} \;\; & \bbe_P[ u(\bdf(\bdz,\bdxi))] \\
    \st \;\;  &
    \bbe_P[u(\bdg(\bdz,\bdxi))] \geq c.
\end{split}
\end{equation}
% \begin{equation}
%\inmat{(PRO-x)}  \quad \;\;
% \begin{array}{ccl}
% \displaystyle{\max_{\bdz\in Z}  }  &
% \displaystyle{\min_{u\in {\cal U}}} & \bbe[ u(\bdf(\bdz,\bdxi))]\\
% &\inmat{s.t.} &
% \bbe[u(\bdg(\bdz,\bdxi))] \geq c.
% %\bbe[u(Y(\zeta))].
% \end{array}
% \label{eq:PRO-x}
% \end{equation}
In this formulation, we consider the same worst-case utility function in the objective and constraint. There is an alternative
way to develop a robust formulation of (\ref{eq:SPR-x}):
\begin{equation}
\label{eq:PRO-x-1}
\begin{split}
   \tilde{\vt}:= \max_{\bdz\in Z} \;\;
    \min_{u\in {\cal U}} \;\; & \bbe_P[ u(\bdf(\bdz,\bdxi))] \\
    \st \;\;\,
    \min_{u\in {\cal U}} \;\; &
    \bbe_P[u(\bdg(\bdz,\bdxi))] \geq c.
\end{split}
\end{equation}
% \bgeqn
%\inmat{(PRO-x)}  \quad \;\;
% \begin{array}{ccl}
% \displaystyle{\max_{\bdz\in Z}  }  &
% \displaystyle{\min_{u\in {\cal U}}} & \bbe[ u(\bdf(\bdz,\bdxi))]\\
% \inmat{s.t.} &
% \disp{\min_{u\in {\cal U}}} &
% \bbe[u(\bdg(\bdz,\bdxi))] \geq c.
% %\bbe[u(Y(\zeta))].
% \end{array}
% \label{eq:PRO-x-1}
% \edeqn
Formulation (\ref{eq:PRO-x-1}) means that the worst-case 
utility in the objective and in the constraint  might differ.
It is easy to observe that $\tilde{\vt} \leq \hat{\vt}$
%It is easy to observe that
which means (\ref{eq:PRO-x-1}) is more conservative than
(\ref{eq:PRO-x}).
Moreover,
if the true utility $u^*$ lies within ${\cal U}$,
then $\tilde{\vt}\leq \vt^*$.
%because the optimal value of the former
%is smaller than %the optimal value 
%that of the latter.
However, under some conditions,
the two formulations are equivalent. The next proposition states this.


\begin{proposition}
\label{Prop-equivalence}
Let $\hat{\bdz}$ denote the optimal solution of problem (\ref{eq:PRO-x}) and
\begin{equation}
    \tilde{Z}:= 
    \left\{\bdz\in Z \,:\, \inf_{u\in {\cal U}} \; \bbe_P[u(\bdg(\bdz,\bdxi))] -c
    %-\bbe[u(Y(\zeta))] 
    \geq 0  \right\}.
\label{eq:x*-PRO-U}
\end{equation}
If $\hat{\bdz}\in \tilde{Z}$, then  
$\hat{\vt}=\tilde{\vt}$.
%problems (\ref{eq:PRO-x})
%and (\ref{eq:PRO-x-1}) are equivalent in terms of the optimal values. 
\end{proposition}


\noindent
\textbf{Proof.} Let $\hat{v}(\bdz)$ denote the optimal value of
the inner minimization problem of (\ref{eq:PRO-x})
and 
$$
\tilde{v}(\bdz) :=  \inf_{u\in {\cal U}} \; \bbe_P[ u(\bdf(\bdz,\xi))].
$$
Let
$\hat{\vt}$ and $\tilde{\vt}$ be defined as in
%denote  respectively the optimal values of 
%the outer maximization problems.
(\ref{eq:PRO-x}) and (\ref{eq:PRO-x-1}).
Define
$$
{\cal U}(\bdz) := \{u\in {\cal U}: \bbe_P[u(\bdg(\bdz,\bdxi))] \geq 
c
\}.
$$
Since ${\cal U}(\bdz)\subset {\cal U}$, then
$\hat{v}(\bdz)\geq \tilde{v}(\bdz)$ for all $\bdz\in Z$.
Moreover, since
%the feasible set of the outer maximization problem
%of (\ref{eq:PRO-x-1}) is contained in
$\tilde{Z}\subset Z$, then
$$
\hat{\vt} = \max_{\bdz\in Z} \hat{v}(\bdz) \geq \max_{\bdz\in \tilde{Z}}
\tilde{v}(\bdz)  = \tilde{\vt}.
$$
Conversely, for any $\bdz\in \tilde{Z}$,
$
{\cal U}(\bdz)={\cal U}.
$
%and hence if
Thus, the assumption that $\hat{\bdz}\in \tilde{Z}$ implies that
%satisfies (\ref{eq:x*-PRO-U}),
$
{\cal U}(\hat{\bdz})={\cal U}
$
and subsequently $\hat{v}(\hat{\bdz}) = \tilde{v}(\hat{\bdz})$.
%then $x^*$ is also the optimal solution of problem (\ref{eq:PRO-x-1}).
This shows
$
\hat{\vt} =\hat{v}(\hat{\bdz}) = \tilde{v}(\hat{\bdz}) \leq \tilde{\vt}
$
because $\hat{\bdz}\in \tilde{Z}$.
%is a feasible solution of the outer maximization problem of
%(\ref{eq:PRO-x-1}).
\hfill $\Box$

From a practical point of view, Proposition~\ref{Prop-equivalence}
is not useful 
%because 
in that we do not know the optimal solution $\hat{\bdz}$ and hence are unable to verify the condition $\hat{\bdz}\in \tilde{Z}$.
Consequently, 
%we consider
it might be sensible to consider (\ref{eq:PRO-x})
as (\ref{eq:PRO-x-1}) might be too conservative.
% In the literature of 
% distributionally robust optimization where ambiguity
% arises in the probability
% of underlying exogenous uncertainty, 
% the worst probability distributions 
% in the objective and in the 
% constraints are considered 
% separately. 
% This 
% %motivates
% prompts
% us to develop a computational method 
% %for solving
% to solve
% problem (\ref{eq:PRO-x}). 
% {\color{red} comment!}
Using the definition of ${\cal U}(\bdz)$, we 
%may 
can
write (\ref{eq:PRO-x}) succinctly as
\begin{equation}
\label{eq:PRO-x-DD}
    \inmat{(BUPRO-D)\quad} \max_{\bdz\in Z} \; \min_{u\in {\cal U}(\bdz)} \; \bbe_P[ u(\bdf(\bdz,\bdxi))].
\end{equation}
% \bgeqn
% \displaystyle{\max_{\bdz\in Z}  }  \;
% \displaystyle{\min_{u\in {\cal U}(\bdz)}}\;\; \bbe[ u(\bdf(\bdz,\bdxi))].
% \label{eq:PRO-x-DD}
% \edeqn
Problem (\ref{eq:PRO-x-DD}) looks as if the ambiguity set 
${\cal U}(\bdz)$ is decision-dependent. 
% This motivates us to consider PRO model with decision-dependent ambiguity set so that it may cover a larger class of decision making problems.
% \subsection{PRO with general decision-dependent ambiguity set}
% }
% % }
% In the preceding discussions, we assume that the ambiguity set of bi-attribute utility functions are independent of decision variables. 
% In some practical cases and/or some special settings, however, the ambiguity set of a DM's utility function may depend on decision variables essentially. 
% For instance, 
% consider the following preference robust one-stage stochastic program with expected stochastic constraints, 
% %Consider the following inequality constrained expected utility maximization problem 
% \begin{equation}
% \label{eq:SPR-x-example}
% \inmat{(BUPRO-D)} \quad
%     \begin{array}{rl}
%         \disp{\max_{\bdz\in Z} \min_{u\in\mathcal{U}} } & \bbe[ u(\bdf(\bdz,\bdxi))] \\
%         \inmat{s.t.} & \bbe[u(\boldsymbol{g}(\bdz,\bdxi))] \geq c,    \end{array}
% \end{equation}
% where $g:\R^n\times \R^m\rightarrow\R^2$ is a continuous vector-valued function representing a financial position with two attributes or performance of an investment in practice and $c$ is a preprecised constant. 
% From a practical point of view, the constraint (\ref{eq:SPR-x-example}b) imposes an additional moment condition (depending on $\bdz$) on ambiguity set $\mathcal{U}$. 
% Let
% \begin{equation}
% \label{eq-decidepen-ambset}
%     \mathcal{U}(\bdz):=\{u\in\mathcal{U}: \bbe[u(\boldsymbol{g}(\bdz,\bdxi))] \geq c\}.
% \end{equation}
% Then we can reformulate problem (\ref{eq:SPR-x-example}) into problem (\ref{eq:MAUT-robust}) with the 
% decision-dependent ambiguity set of utilities $\mathcal{U}(\bdz)$, 
% % ``virtual ambiguity set'' $\mathcal{U}(\bdz)$ depending on the decision variable $\bdz$, 
% that is, 
% \begin{equation}
% \label{eq:SPR-x}
% \max_{\bdz\in Z}\min_{u\in {\cal U}(\bdz)} \bbe_P[u(\bdf(\bdz,\bdxi))]. 
% \end{equation}
% % In this section, we consider problem (\ref{eq:SPR-x}) with the following general moment-type based decision-dependent ambiguity set of utilities
% % \begin{equation}
% % \label{eq-decidepen-ambset}
% %     \mathcal{U}(\bdz):=\left\{ u\in \calu: \bbe[u(\boldsymbol{g}(\bdz,\bdxi))] \geq c \right\},
% % \end{equation}
% % where $c$ is fixed constant.
% % where $\psi_l:T\times Z\rightarrow \R$, $m=1,\ldots,M$ are real-valued functions for fixed $\bdz$ such that the Lebesgue-Stieltjes 
% % integrals are well-defined 
% % % (in other words, $\psi_l$ is real-valued function with bounded variation, see, e.g., \citet[P. 129]{hildebrandt1963introduction}, \cite{Mcs47}) 
% % and $c_m$, $m=1,\ldots,M$ are some given constants. 
%Specifically, we 
%We propose to use the piecewise linear utility to approximate the utility function of $\mathcal{U}(\bdz)$; the ambiguity set of piecewise linear utility functions is defined as
We propose to use the PLA
%piecewise linear approximation 
approach
%for solving 
to solve problem (\ref{eq:PRO-x-DD}).
In this case, 
\[
\mathcal{U}_N(\bdz):=\{u_N\in\mathcal{U}_N \,|\; \bbe_P[u_N(\boldsymbol{g}(\bdz,\bdxi))] \geq c\},
\]
where $\mathcal{U}_N$ is defined as in 
(\ref{eq:U_N-PLA}).
The approximate BUPRO %model
%is defined by 
can be subsequently written as
\begin{equation}
\label{eq:SPR-x-approx}
\inmat{(BUPRO-DN)} \quad \max_{\bdz\in Z}\;\min_{u\in {\cal U}_N(\bdz)}\; \bbe_P[u(\bdf(\bdz,\bdxi))]. 
\end{equation}
% To solve the problem (\ref{eq:SPR-x}), we also employ the piecewise linear approximation schemes developed in the preceding sections. 
% Consequently, by piecewise linear approximation of $u$, we obtain the tractable formulation of BUPRO-DN problem (\ref{eq:SPR-x-approx})
The inner minimization problem 
based on EPLA 
%is an 
can be reformulated as an LP: %linear program:
\begin{align}
\label{eq:PRO-x-inner}
    \disp{ \min_{{\bm u} }} \; & \sum_{k=1}^K p_k \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \1_{T_{i,j}} (\bdf^k) 
    \lt[ u^{1l}_{i,j}(f_1^k,f_2^k) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} 
    \lt( \frac{f_2^k-y_j}{f_1^k-x_i} \rt) \rt. \nonumber \\
    & \lt. + u^{1u}_{i,j}(f_1^k,f_2^k) \1_{\lt( \frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty \rt)} \lt(\frac{f_2^k-y_j}{f_1^k-x_i}\rt) \rt]  \nonumber \\
    \st \; & 
    % \int_{\underline{x},\underline{y}}^{\bar{x},\bar{y}} \psi_l(x,y) d u(x,y)
    % + \int_{\underline{x}}^{\bar{x}} \psi_{1,m}(x)d u(x,\underline{y}) \nonumber \\
    % && \hspace{7em} +\int_{\underline{y}}^{\bar{y}} \psi_{2,m} (y) d u(\underline{x},y) \leq c_m(\bdz), m=1,\ldots,M, \label{eq-dd-traform-paircom} \\
    %&&
    \sum_{k=1}^K p_k \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \1_{T_{i,j}} (\bdg^k) 
    \lt[ u^{1l}_{i,j}(g_1^k,g_2^k) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} 
    \lt( \frac{g_2^k-y_j}{g_1^k-x_i} \rt) \rt. \nonumber  \\
    & \lt. + u^{1u}_{i,j}(g_1^k,g_2^k) \1_{\lt( \frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty \rt)} \lt(\frac{g_2^k-y_j}{g_1^k-x_i}\rt) \rt] \geq c,  \\
    & \inmat{constraints} \; (\ref{eq-traform-paircom})-(\ref{eq-traform-norm1}), \nonumber
\end{align}
where ${\bm u}={\rm vec}\left((u_{i,j})_{1\leq i\leq N_1}^{1\leq j\leq N_2}\right)\in \R^{N_1N_2}$,
% ${\bm u}:=(u_{1,1},\cdots,u_{N_1,1},\cdots,u_{1,N_2},\cdots,u_{N_1,N_2})^T\in \R^{N_1N_2}$,
$\bdg^k:=\bdg(\bdz,\bdxi^k)=(g_1^k,g_2^k)^T$ with $g_1^k:=g_1(\bdz,\bdxi^k)$, $g_2^k:=g_2(\bdz,\bdxi^k)$.
Since (\ref{eq:PRO-x-inner})
is an LP
%linear program 
for fixed $\bdz$, we can use a Dfree method to solve (\ref{eq:SPR-x-approx}).
Similar formulations can be 
derived based on the IPLA approach.
% {\color{red} and other types}.



\section{Numerical results}
\label{sec:numerical results}

We have carried out numerical %experiments 
tests on the performances of the proposed models
and computational schemes discussed in the %proceeding
previous sections 
%with the application
by applying them to a portfolio optimization problem. 
In this section, we report the test results.


\subsection{Setup}
\label{eq:setup-1}
As an example of a real-life portfolio selection problem with uncertain project outcomes, we consider an application of the UPRO models in healthcare resource allocation problem studied by Airoldi~et~al.~\cite{airoldi2011healthcare}.
In this application, public health officials (PHO) decide on a portfolio of projects that seek to improve the quality of life.
Specifically, the health benefits of $n=8$ projects (access to dental, workforce development, primary prevention, Obesity training, CAMHS School, early detection and diagnostics, palliative \&
EOL, active treatment)
%from two different commissioning areas (children and cancer) 
are evaluated 
%as
% {\color{blue} 
%with 
through two attributes, 
commissioning areas of children and cancer.
% }.
%that is, 
Moreover, the outcomes of the projects are uncertain and 
%can be evaluated with %continuous 
represented by
discretely distributed
random 
%variables
vector $\bdxi^k=(\xi^k_1,\ldots,\xi^k_8)^T$ {\color{black}supported by $\Xi\subset \R^8$} with equal probabilities $p_k:=1/K$ for $k=1,\ldots, K$.
Let $\bdz=(z_1,\ldots,z_8)^T$ be the proportions of a fixed fund.
%normalized to $1$.
% {\color{blue}
For the convenience of calculation,
we generate 
samples of 
$\bdxi^k$ 
%following 
by the uniform distribution over $[0,1]^8$.
% }
We consider a situation where 
the PHO's utility of 
the 
bi-attribute outcomes 
is ambiguous and the optimal allocation
is based on the worst-case utility in 
ambiguity 
set $\calu$
%the expected utility of the outcomes %with respect to 
%w.r.t. the optimal allocation of the fund is maximized, that is,
\begin{equation*}
    \max_{\bdz\in Z} \min_{u\in\calu} \; \sum_{k=1}^K p_ku(\bdf(\bdz,\bdxi^k)),
\end{equation*}
where $f_1(\bdz,\bdxi^k):=\sum_{i=1}^{5} z_i \xi_i^k
\in [0,1]$, $f_2(\bdz,\bdxi^k):=\sum_{i=6}^{8} z_i \xi_i^k
\in [0,1]$ and $Z:=\{\bdz\in\R^8_+ : \sum_{i=1}^8 z_i=1\}$.
To examine the performance of BUPRO-N, we carry out the tests with a specified true utility function and investigate how the optimal value and the worst-case utility function converge as %the
information %of
about the PHO's utility preference increases.
We 
%set 
consider the true utility
$
u(x,y)=e^x-e^{-y}-e^{-x-2y}
$
defined over $[0,1]\times [0,1]$
and normalize it 
by setting $u^*(x,y) :=
(u(x,y)-u(0,0))/(u(1,1)-u(0,0))$. 
%$\hat{u}(0,0)=1$ and $\hat{u}(1,1)=1$. 
This function
%which 
satisfies the 
conservative 
%conditions
property
%that we discussed earlier
(\ref{eq:conservative}), and
is convex 
%with respect to
w.r.t.~$x$ and concave 
%with respect to
w.r.t.~$y$.
Although the PHO is unaware that the preference can be characterized as this function, we assume that the decision of PHO never contradicts with results suggested by such a function
unless specified otherwise (we will remove this assumption in Section~\ref{subsec:preference-incon}), 
see similar assumption in \cite{AmD15}.
% Note that 
% %the outcomes of 
% $\bdxi^k\in [0,1]^8$ such that
% %projects 
% $\bdf(\bdz,\bdxi^k)$ are located in the interval $[0,1]^2$. 
We may refine %restrict 
$\scru$ to %be 
a set of %all
normalized 
%increasing 
non-decreasing
utility functions mapping from $[0,1]^2$ to $[0,1]$, and $\scru_N$ the corresponding set of PLA functions.
%piecewise linear approximation functions. 
All of the tests are carried out in MATLAB R2022a installed on a PC (16GB, CPU 2.3 GHz) with an Intel Core i7 processor.
We use GUROBI and YALMIP \cite{lofberg2004yalmip} to solve the inner minimization problem (LP or MILP) and single MILP, 
and SURROGATEOPT to solve the outer maximization problem (unconstrained problem
(\ref{eq:MAUT-robust-N-dis}) and constrained problem (\ref{eq:SPR-x-approx})).


\subsection{Design of the pairwise comparison lotteries}
\label{sec:PC-design}

As we discussed earlier, the ambiguity set of utility functions $\calu_N$ is characterized by  available %the
information about the DM's preferences.
% We use a similar idea of the utility split approach (see \cite{AmD15}) to generate pairs of random prospects.
% {\color{blue}
We ask PHO
% {\color{red}the investor
% HX: PHO??}
questions by 
%comparing
showing preference between 
% a risky gamble 
a risky lottery
with two
outcomes and a 
%certain 
lottery 
with certain outcome
(we call it ``certain lottery'' 
following the terminology of
\cite{AmD15}),
%with certain outcome 
denoted respectively by
\begin{equation}
\label{eq:lottery}
    \bdcz_1 = \lt\{
\begin{array}{ll}
    (\underline{x},\underline{y}) & \inmat{\;w.p.\;} 1-p, \\
    (\bar{x},\bar{y}) & \inmat{\;w.p.\;} p,
\end{array} 
\rt.
\inmat{ and }
\bdcz_2=(x,y) \inmat{\;w.p.\;} 1,
\end{equation}
% }
% {\color{red} HX: what does this kind of question mean in this context?}
where $\underline{x},\bar{x}, \underline{y}$ and $\bar{y}$
are fixed and 
% $\bdcz_1$ 
% %refers to 
% represents 
% two potential project outcomes with uncertainty and $\bdcz_2$ refers to deterministic project outcomes,
% and
% $x$ and $y$ are generated randomly over
 $(x,y)\in [\underline{x},\bar{x}]\times[\underline{y},\bar{y}]$
is randomly generated.
%In this way, %there remain
%only %three unknown parameters
%$x,y,p$ are to be identified in this question 
Since we assume that 
$u(\underline{x},\underline{y})=0$ and $u(\bar{x},\bar{y})=1$,
the only parameters to be identified are $x,y,p$, 
so that the question is properly posed.
% {\color{red}HX: Please explain why you have to choose $Z_1$ is this particular way.}
% {\color{blue}Sainan: We can also choose two points $(x_1,y_1)$ $(x_3,y_3)$ randomly from $[\underline{x},\bar{x}]\times [\underline{y},\bar{y}]$ 
% and $Z_1$ takes this two points with probability $1-p$ and $p$ respectively.
% $Z_2=\frac{1}{2}((x_1,y_1)+ (x_3,y_3))$ with probability $1$.
% In this case, we need to identify four numbers $x_1,y_1,x_3,y_3,p$.
% While in this study, we only need to identify three numbers $x,y,p$.
% }
Observe that 
$$
\bbe_{\mathbb{P}}[u(\bdcz_1(\omega))]=(1-p) u(\underline{x},\underline{y})+ p u(\bar{x},\bar{y})=p \inmat{\quad and\quad} \bbe_{\mathbb{P}}[u(\bdcz_2(\omega))]=u(x,y). 
$$
%Recall the normalization condition, then the question is to check
Thus the question is down to checking 
%the following inequality 
whether inequality $u(x,y) \geq p$ holds or not.
% \begin{equation}
%     u(\bdcz_2) \geq p.
% \end{equation}
% This approach 
% %differs slightly from the random relative utility split %(RRUS) 
% {\color{blue}
% corresponds to the random utility split} 
% scheme used in single-attribute PRO models \cite{AmD15}.
%where 
% {\color{red}
% the two outcomes of 
% $\bdcz_1$ 
% are 
% randomly generated, 
% whereas 
% the outcome of $\bdcz_2$ 
% is the average of the two outcomes of $\bdcz_1$.
% This is purely because we want to simplify the computation in the two-variate utility case.

% BTW, here the outcome of $Z_1$ is fixed, what we choose is the outcome of $Z_2$ and then based on the range of $u(Z_2)$ to determine the value of $p$. The idea is the random utility split not the relative utility split. 
% }
% is similar to the random relative utility split 
% (RRUS) scheme in single-attribute PRO models \cite{AmD15}, 
% {\color{red}
% where ${\bm Z}_1$ is randomly generated,
% whereas ${\bm Z}_2$ in \cref{eq:lottery} is randomly generated.

% This is because in two variate case,

% I feel that this approach is essentially the same as the random utility split scheme in the
% single attribute case. why? because we randomly generate $x,y$ from $[\underline{x},\overline{x}]$ and $[\underline{y},\overline{y}]$ separately and then choose the probability $p$ such that $\mathbb{E}[u^*(Z_1)]=p$ is equal to the midpoint of the interval $\{u(x,y):u\in\mathcal{U}\}$. 
% YES.
% }
% {\color{blue}
% where the two outcomes of 
% $\bdcz_1$ 
% are 
% randomly generated, 
% whereas 
% %the outcomes 
% the outcome of $\bdcz_2$ 
% is the average of the two outcomes of $\bdcz_1$.
% In this context,
% the outcome of  $\bdcz_2$
% would be
% %are the middle point of $\bdcz_1's$, that is, $\bdcz_1$ takes values $(x_1,y_1)$ with probability $1-p$ and $(x_2,y_2)$ with probability $p$, and $\bdcz_2$ takes values
% $(\frac{x_1+x_2}{2},\frac{y_1+y_2}{2})$.
% %with probability $1$.
% The main difference is that the undetermined parameters are $x$, $y$ and $p$ in the pair of lotteries defined as in \cref{eq:lottery} rather than 
% $x_1,y_1$, $x_2,y_2$ and $p$ in RRUS scheme.
% Extending to the pairwise comparison with $m$ attributes, the number of undetermined parameters is $m+1$ if the lotteries are defined as in \cref{eq:lottery} and $2m+1$ under the RRUS scheme, which results in less computational burden.
% }
% The main differences are that the outcomes of $Z_1$ are fixed and the outcome of $Z_2$ is not the midpoint between $(\underline{x},\underline{y})$ 
% and $(\bar{x},\bar{y})$. 
% {\color{red}in which the undetermined parameters are $x$, $y$ and $P$, instead of $x_1,y_1$, $x_2,y_2$ and $p$ in RRUS scheme.}

% Here we use the Random utility split instead of Random relative utility split, Please double check. We may see the argumentation in the second sentence of this subsection. 
% Yes.

%Now
Next,
we turn to discuss how to generate $M$ lotteries, or more specifically how to
%choose 
set values for $x$, $y$ and $p$.
We generate randomly $M_1$ points of the first attribute including $\underline{x}$, $\bar{x}$, and $M_2$ points of the second attribute including $\underline{y}$ and $\bar{y}$. Thus
%hen
the number of 
% the outcomes of
the 
certain 
% deterministic 
lotteries %with these points
is at most $M=M_1M_2-2$.
Let 
$$
S:=\{(x_{i_l},y_{j_l}), i_l\in\{1,\ldots,M_1\}, j_l\in\{1,\ldots,M_2\}, l=1,\ldots,M \}
$$
be the set of all 
%deterministic 
certain
lotteries except points $(\underline{x},\underline{y})$, $(\bar{x},\bar{y})$ and $\calu_{N}^{l-1}$ be the set of all piecewise linear utility functions which are consistent to the
%former
previously
generated $l-1$ questions.
Assume that 
the $l$th %certain lottery 
lottery with the certain outcome
is $\bdcz_2^l=(x_{i_l},y_{i_l})$. %\in S$.
Define 
\begin{equation}
\label{eq:I1-I2}
    I_1^l:=\min_{u\in\scru_N\cap\calu_{N}^{l-1}} u(\bdcz_2^l) 
    \inmat{\quad and\quad} 
    I_2^l:=\max_{u\in\scru_N\cap\calu_{N}^{l-1}} u(\bdcz_2^l).
\end{equation}
Since $u(x_{i_l},y_{i_l})\in[0,1]$, then $I_1^l,I_2^l\in[0,1]$. 
We set $p^l:=\frac{I_1^l+I_2^l}{2}$,
and 
use the true utility function $u^*$ to 
check whether inequality
\begin{equation}
\label{eq:lottery-l}
    u^*(x_{i_l},y_{i_l}) =\bbe_{\mathbb{P}}[u^*(\bdcz_2^l(\omega))] \geq \bbe_{\mathbb{P}}[u^*(\bdcz_1^l(\omega))] = p^l
\end{equation}
holds or not.
If it holds,
then $\bdcz_2^l$ is preferred 
%over 
to $\bdcz_1^l$.
% Notice that, even we generate no extra point between $[\underline{x},\bar{x}]$ and $[\underline{y},\bar{y}]$, there are two certain lotteries $(\underline{x},\bar{y})$ and $(\bar{x},\underline{y})$, then the total number of questionnaires is $M=M_1M_2-2$.
The following algorithm describes the procedures %for constructing
% to obtain
for constructing
$\calu_{N}=\calu_N^M$.


% \begin{minipage}{0.9\linewidth}
\vspace{0.6em}
\begin{breakablealgorithm}
\caption{}
% \STATE
{
\noindent
\textbf{Initialization.}
Set $m_1:=1, m_2:=1, l:=1$, 
% $h_0:=0$, 
% $p_0=0$, $u_{i_0,j_0}=0$ 
$\calu_N^0:=\scru_N$
and $S:=\emptyset$. 
}
% {\color{blue}
\begin{algorithmic}[1]
\begin{small}
\STATE
% \textbf{Step 1.}
Choose two positive integers $M_1$ and $M_2$ 
as the numbers of the 
gridpoints of the two attributes.
Generate $M_1-2$ points 
%from
within $[\underline{x},\bar{x}]$ and $M_2-2$ points
%from 
within $[\underline{y},\bar{y}]$ randomly
using the uniform distribution; 
sort them out 
in %the
increasing order
of their values
and label them by $x_i, i=1,\ldots,M_1-2$ and $y_j, j=1,\ldots,M_2-2$.
Let ${\cal X}:= \{\underline{x},x_1,\ldots,x_{M_1-2},\bar{x}\}$ and ${\cal Y}:= \{\underline{y},y_1,\ldots,y_{M_1-2},\bar{y}\}$, and 
let ${\cal X}\times{\cal Y}:=\{(x_i,y_j),x_i\in{\cal X}, y_j\in{\cal Y}\}$ be the set of the gridpoints.
% }

% {\color{blue}
\STATE
% \textbf{Step 2. (Identification of $p$)}
Let the $l$th certain lottery be $\bdcz_2^l=(x_{i_l},y_{j_l})$,
% Normalize the utility function by setting
% $u(\underline{x},\underline{y})=0$ and $u(\overline{x},\overline{y})=1$.
% Let $I^l=[I_1^l,I_2^l]$ and $p^l=\frac{I_1^l+I_2^l}{2}$, where $I_1^l$ is obtained by solving the following problem:
% with $m_1=1,\ldots,M_1$ and $m_2=1,\ldots,M_2$,
% }
% For $m_1=1,\ldots,M_1$ and $m_2=1,\ldots,M_2$, 
solve the 
% following 
problem (\ref{eq:I1-I2}) to obtain $I_1^l$ and $I_2^l$.
% {\color{blue}\begin{align}
%     I_1^l = \min_{\substack{u_{i,j}, i=1,\ldots,M_1\\j=1,\ldots,M_2}} & 
%     \sum_{i=1}^{M_1-1} \sum_{j=1}^{M_2-1} \1_{T_{i,j}} (x_{i_l},y_{j_l})
%     \lt[ u^{lo}_{i,j}(x_{i_l},y_{j_l}) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} 
%     \lt( \frac{y_{j_l}-y_j}{x_{i_l}-x_i} \rt) \rt. \nonumber \\
%     & \lt. + u^{up}_{i,j}(x_{i_l},y_{j_l}) \1_{\lt( \frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty \rt)} \lt(\frac{y_{j_l}-y_j}{x_{i_l}-x_i}\rt) \rt]  \notag \\
%     \st \qquad& 
%     h_{l'} (p_{l'}- u_{i_{l'},j_{l'}})\leq 0, l'=0,\ldots,l-1, \label{eq-lottery} \\
%     & \frac{u_{i+1,j}-u_{i,j}}{x_{i+1}-x_i} \geq \frac{u_{i,j}-u_{i-1,j}}{x_i-x_{i-1}}, i=2,\ldots,M_1-1, j=1,\ldots,M_2, \notag \\
%     & \frac{u_{i,j+1}-u_{i,j}}{y_{j+1}-y_j} \leq \frac{u_{i,j}-u_{i,j-1}}{y_{j}-y_{j-1}}, i=1,\ldots,M_1, j=2,\ldots,M_2-1, \notag \\
%     & \inmat{constraints\;} \cref{eq-traform-mon1}- \cref{eq-traform-norm1}. \notag
% \end{align}
% }
% Solve the maximization counterpart and let $I_2^l$ be the corresponding optimal value, and 
Let $I^l=[I_1^l,I_2^l]$, $p^l=\frac{I_1^l+I_2^l}{2}$ and 
$S=S\cup\{\bdcz_1^l,\bdcz_2^l\}$.
% {\color{red}where $I_1^l$ is obtained by solving the following problem:}
% Set $p^l=\frac{I_1^l+I_2^l}{2}$ and check whether $p^l<u^*(x^l,y^l)$. 
% If holds, let $h_m=1$. Otherwise, let $h_m=-1$.
\STATE
% \textbf{Step 3. }
If 
% $(1-p^l)u^*(\underline{x},\underline{y})+p^l u^*(\overline{x},\overline{y})\leq u^*(x_{i_l},y_{j_l})$,
$p^l\leq u^*(x_{i_l},y_{j_l})$,
then 
%we 
% set $h_l=1$ and we 
% regard the DM prefers $\bdcz_2^l$ to $\bdcz_1^l$ and 
%set 
% in which case only those $u_N$ whose values at $(x_{i_l},y_{j_l})$ fall in the interval $[p^l,I_2^l]$, 
% that is, $p^l\leq u_N(x_{i_l},y_{j_l})$ will be considered and subsequently set
$$
{\cal U}_N^{l}:={\cal U}_N^{l-1} \bigcap \lt\{u_N\in \mathscr{U}_N: p^l\leq u_N(x_{i_l},y_{j_l}) \rt\}.
$$
Otherwise, 
% let $h_m=-1$
% and 
%we set 
$$
{\cal U}_N^{l} :={\cal U}_N^{l-1} \bigcap \lt\{u_N\in \mathscr{U}_N: p^l\geq u_N(x_{i_l},y_{j_l})\rt\}.
$$
Set $l:=l+1$, and go
%back
to Step 1.
\end{small}
\end{algorithmic}
\end{breakablealgorithm}
\vspace{0.5em}
% \end{minipage}



Steps 1-2 generate a lottery for pairwise comparison. 
Note that the minimization problem in (\ref{eq:I1-I2}) can be formulated as 
\begin{subequations}
\label{eq-lottery-I_1}
\begin{align}
    I_1^l = \min_{{\bm u}}\;
%    \substack{u_{i,j}, i=1,\ldots,M_1\\j=1,\ldots,M_2}} 
\; & u_{i_l,j_l}
    % \sum_{i=1}^{M_1-1} \sum_{j=1}^{M_2-1} \1_{T_{i,j}} (x_{i_l},y_{j_l})
    % \lt[ u^{lo}_{i,j}(x_{i_l},y_{j_l}) \1_{\lt[0,\frac{y_{j+1}-y_j}{x_{i+1}-x_i}\rt]} 
    % \lt( \frac{y_{j_l}-y_j}{x_{i_l}-x_i} \rt) \rt. 
    \nonumber \\
    % & \lt. + u^{up}_{i,j}(x_{i_l},y_{j_l}) \1_{\lt( \frac{y_{j+1}-y_j}{x_{i+1}-x_i},+\infty \rt)} \lt(\frac{y_{j_l}-y_j}{x_{i_l}-x_i}\rt) \rt]  \notag \\
    \st \;\; & 
    h_{l'} (p_{l'}- u_{i_{l'},j_{l'}})\leq 0, l'=0,\ldots,l-1, \label{eq-lottery} \\
    & \frac{u_{i+1,j}-u_{i,j}}{x_{i+1}-x_i} \geq \frac{u_{i,j}-u_{i-1,j}}{x_i-x_{i-1}}, i=2,\ldots,M_1-1, j=1,\ldots,M_2, \label{eq-single-concave} \\
    & \frac{u_{i,j+1}-u_{i,j}}{y_{j+1}-y_j} \leq \frac{u_{i,j}-u_{i,j-1}}{y_{j}-y_{j-1}}, i=1,\ldots,M_1, j=2,\ldots,M_2-1, \label{eq-single-convex} \\
    & \inmat{constraints\;} (\ref{eq-traform-mon1})- (\ref{eq-traform-norm1}), \notag
\end{align}
\end{subequations}
where  ${\bm u}:=(u_{1,1},\cdots,u_{N_1,1},\cdots,u_{1,N_2},\cdots,u_{N_1N_2})^T$,
%:i=1,\cdots,M_1,j=1,\cdots,M_2\}$,
(\ref{eq-lottery}) requires the answer to the $l$th question to be consistent with the previous $l-1$ questions 
(if $\bdcz_1^{l'}$ is preferred, then (\ref{eq:lottery-l}) holds for $l=l'$ and we set $h_{l'}=1$, otherwise we set $h_{l'}=-1$),
(\ref{eq-single-concave}) and (\ref{eq-single-convex}) comply with the assumption that the single-attribute utility function $u(\cdot,\hat{y})$ is concave and $u(\hat{x},\cdot)$ is convex for any fixed $\hat{x}\in X$ and $\hat{y}\in Y$.
% Note that since $u_N$ is normalized, then $I\subset[0,1]$.
Step 3 asks the DM to choose 
between the risky lottery and the certain lottery. 
% one with the %deterministic 
% certain
% return. 
Here the true utility function $u^*$ 
(defined in Section~\ref{eq:setup-1})
is used to ``act as the DM''. 
After the DM makes a choice, an expected utility inequality is created and added to the ambiguity set $\calu_N$.
Since $p^l$ is chosen as the midpoint of $I^l$, we deduce that the true utility function value at $(x_{i_l},y_{j_l})$ lies within the right or left half of the interval $I^l$ and the pairwise comparison effectively reduces the ambiguity set by ``half'' in the sense that those $u_N$ whose values (at point $(x_{i_l},y_{j_l})$) lie within the other half of the interval $I^l$ are excluded from the ambiguity set.

\begin{example}
We use a simple example to explain the above steps where the true utility function $u^*$ (defined in Section~\ref{eq:setup-1})
%=e^x-e^{-y}-e^{-x-2y}$ 
is defined over $[0,1]^2$ and the piecewise utility functions have $N=M_1M_2=6$ gridpoints including $(0,0)$ and $(1,1)$.
We randomly generate one point in $[0,1]$ for the second attribute as the non-end gridpoints.
Then ${\cal X}=\{0,1\}$ and ${\cal Y}=\{0,0.3706,1\}$.
The number of questions is $M=M_1 M_2-2=4$.

\noindent
\textbf{Lottery 1} $(l=1)$. Set $i_1:=1, j_1:=2$ and $(x_{i_1},y_{j_1})=(0,0.3706)$.
By solving (\ref{eq-lottery-I_1}) and the corresponding maximization problem, we obtain that $[I_1^1,I_2^1]=[0,1]$ and set $p^1=0.5$.
By checking $u^*(0,0.3706)=0.252\leq p^1$, we set $h_1:=-1$.
% {\color{red}Here, $I_1^1=0$ since the corresponding $u_N(0,0.3706)=0$ in the minimization problem.}

\noindent
\textbf{Lottery 2} $(l=2)$. Set $i_2:=1, j_2:=3$ and $(x_{i_2},y_{j_2})=(0,1)$.
Solve (\ref{eq-lottery-I_1}) and the corresponding maximization problem to obtain $[I_1^2,I_2^2]=[0,0.5]$,
%then 
so $p^2=0.25$.
By checking whether $u^*(0,1)=0.454\geq p^2$ or not, we set $h_2:=1$.

\noindent
\textbf{Lottery 3} $(l=3)$. 
Set $i_3:=2, j_3:=1$ and $(x_{i_3},y_{i_3})=(1,0)$.
We obtain $[I_1^3,I_2^3]=[0.5,1]$, %then
and $p^3=0.75$.
By checking $u^*(1,0)=0.712\leq p^3$, we set $h_3:=-1$.

\noindent
\textbf{Lottery 4} $(l=4)$. 
Set $i_4:=2, j_4:=2$ and $(x_{i_4},y_{j_4})=(1,0.3706)$ to obtain $[I_1^4,I_2^4]=[0.75,1]$,
%then 
and $p^4=0.875$.
By checking $u^*(1,0.3706)=0.864\leq p^4$, we set $h_4:=-1$.
\end{example}


\subsection{Convergence results}

In this subsection, we 
%consider 
investigate 
the convergence of 
the 
worst-case approximate utility functions of the unconstrained problem (\ref{eq:MAUT-robust-N}) and the constrained optimization problem (\ref{eq:SPR-x-approx}) under EPLA and IPLA schemes as $N$ increases.
%, and the comparison between EPLA and IPLA.
% we mainly consider the following three experiments: the worst-case piecewise linear conservative utility function; the impact of the ambiguity set on the optimal decision solution as well as the optimal value; the impact of data perturbation on the optimal value. 

\textbf{(i) EPLA and IPLA for unconstrained problem (\ref{eq:MAUT-robust-N}).}

\underline{EPLA approach.}
%The aim of this part of the tests is to 
We begin by
examining
the performance of the EPLA approach 
%when the true utility function is 
%increasing 
%non-decreasing
with different 
%divisions 
types of partitions 
discussed in Section~\ref{sec:numer-methods}.
%The
% where
We assume
the number of the scenarios of  $\bdxi$ 
%is discrete distributed  
%with the sample size
is  $K=1000$.
The convergence results are displayed in 
Figures~\ref{fig-utility-main}-\ref{fig-utility-mixed},
and Tables~\ref{tab-result-N}-\ref{tab:distance}.
Figures~\ref{fig-utility-main}-\ref{fig-utility-mixed} %display %,(\ref{fig-utility-counter}),
depict
the true utility function and the worst-case %approximate
 utility functions for Type-1 PLA (Figure~\ref{fig-utility-main}), 
Type-2 PLA (Figure~\ref{fig-utility-counter}), 
and mixed-type PLA (see Remark~\ref{rem:BUPRO-DF}~(ii) for the definition) in Figure~\ref{fig-utility-mixed}.
We can see that the worst-case utility functions move closer and closer 
to the true utility function as more questions 
are asked in %{\color{Green}all cases}. 
% {\color{blue}
all the three cases,
which is in accordance
with our anticipation in Remark~\ref{rem:distance}~(iii) and Table~\ref{tab:distance}. 
% {\color{red}Comparing Figures~\ref{fig-utility-main}-\ref{fig-utility-counter},
% we find that the Type-1 PLA over every cell is linear,
% whereas Type-2 PLA has distinct forms 
% %over
% between the upper and the 
% lower triangles of a cell.}
% }
%It is worth noting that in the Type-1 case, the two pieces in a cell are on the same plane but not in the Type-2 case.
% In \cref{fig-utility-mixed}, the cell with no diagonal line means 
% that the Type-1 PLA
% %approximate utility 
% is the same as the Type-2 PLA over it. %approximate utility.
Table~\ref{tab-result-N} displays the optimal solutions, the optimal values, 
the errors of the 
optimal values (which is defined as 
the difference between the true and the approximate optimal value), and computation time (CPU time).
%deviation of the optimal values of the
% UPRO 
% the EPLA model BUPRO-N \cref{eq:PRO-N-reformulate}
% from the true optimal value $\vt^*$ of
% utility maximize problem $\max_{\bdz\in Z} \sum_{k=1}^K p_k u^*(\bdf(\bdz,\bdxi^k))$.
We find that the %objective
optimal
values
increase as the number of queries increases.
This 
%is due to the fact that
% arises
is
because
the ambiguity set $\calu_N$ becomes smaller as the number of queries increases.
%, and consequently, the set of feasible solutions of the inner minimization problem in the BUPRO-N model becomes smaller, and hence the objective values increase.
Moreover, 
the errors
%gaps between the optimal values 
decrease as the number of 
%queries
questions  increases. 
The optimal values in the Type-1 PLA and mixed-type PLA are smaller than that of the Type-2 PLA
% case, this is due to 
in that the conservative condition
%s which 
makes the utility value of Type-2 %approximate utility 
PLA
larger than the other cases, 
see Figure~\ref{fig-division}.
% % It should be mentioned that in this set of tests, the grid points $(x_i,y_j)$, $i=1,\ldots, N_1, j=1,\ldots, N_2$ are taken from the outcomes in the queries and endpoints, then the number of queries is $N_1\times N_2-2$.
% Hence, as the number of queries increases, the number of discretization points also increases. 
% This implies that $\psi_l$ is a simple function with jumps at $(x_i,y_j)$ for $i=1,\ldots,N_1, j= 1,\ldots,N_2$.
% Since $u$ is non-concave, the optimal value of BUPRO-N differs from that of BUPRO model, i.e., $\vt_N\neq\vt$.
% By Theorem \ref{thm-optval}, $\vt\in[\vt_N-2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2}-L(\beta_{N_1}+\beta_{N_2}), \vt_N+2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2}+L(\beta_{N_1}+\beta_{N_2})]$ with $L=2$.
% [\scriptsize{$5\times 5$}]
\begin{figure}[!ht]
  \centering
  \vspace{-0.5cm}
  \subfigure{
    \includegraphics[width=0.3\linewidth]{ut-m-5.pdf}
  } 
  \hspace{-0.5em}
  \subfigure{
    \includegraphics[width=0.3\linewidth]{ut-m-10.pdf}
  }
  \hspace{-0.5em}
  \subfigure{
    \includegraphics[width=0.3\linewidth]{ut-m-15.pdf}
  }
  \vspace{-0.2cm}
  \captionsetup{font=footnotesize}
  \caption{{\bf Type-1 EPLA}: the convergence of the worst-case utility function of EPLA model (\ref{eq:PRO-N-reformulate}) to the
  true utility function (in blue) as the number of questions 
  %changes
  increases
  from $5\times 5$ to $15\times 15$.} 
  %and Type-1 worst-case utility functions}
  \label{fig-utility-main} %% label for entire figure
  \vspace{-0.5cm}
\end{figure}

% [\scriptsize{$5\times 5$}]
\begin{figure}[!ht]
  \centering
  \vspace{-0.5cm}
  \subfigure{
    \label{subfig-utility-b-counter}
    \includegraphics[width=0.3\linewidth]{ut-c-5.pdf}
  } \hspace{-0.7em}
  \subfigure{
    \label{subfig-utility-c-counter}
    \includegraphics[width=0.3\linewidth]{ut-c-10.pdf}
  } \hspace{-0.7em}
  \subfigure{
    \label{subfig-utility-d-counter}
    \includegraphics[width=0.3\linewidth]{ut-c-15.pdf}
  }
  \captionsetup{font=footnotesize}
  \caption{{\bf Type-2 EPLA}: the convergence of the worst-case utility function of Type-2 EPLA model
  to the true utility function (in purple) as the number of questions %changes
  increases from $5\times 5$ to $15\times 15$.}
%  and Type-2 worst-case utility functions (in blue)}
% \vspace{-0.5cm}
  \label{fig-utility-counter} %% label for entire figure
\end{figure}


\begin{figure}[!ht]
  \centering
  \vspace{-0.5cm}
  \subfigure{
    \label{subfig-utility-b-mixed}
    \includegraphics[width=0.3\linewidth]{ut-mixed-5.pdf}
  } \hspace{-0.7em}
  \subfigure{
    \label{subfig-utility-c-mixed}
    \includegraphics[width=0.3\linewidth]{ut-mixed-10.pdf}
  } \hspace{-0.7em}
  \subfigure{
    \label{subfig-utility-d-mixed}
    \includegraphics[width=0.3\linewidth]{ut-mixed-15.pdf}
  }
  \vspace{-0.2cm}
  \captionsetup{font=footnotesize}
  \caption{{\bf Mixed-type EPLA}:
 the convergence of the worst-case utility functions of mixed-type EPLA model to the true utility function (in green).
%  In \cref{fig-utility-mixed}, 
  The cell with no diagonal line means 
that Type-1 and Type-2 PLAs 
coincide because
in this case 
${\bm f}(\bdz,\bdxi^k)$ does not fall into the cell
for $k=1,\cdots,K$.
%approximate utility 
%is the same as the  over it.
%   \caption{The true utility function and mixed-type worst-case utility functions 
  }  
  % \vspace{-0.5cm}
  \label{fig-utility-mixed} %% label for entire figure
\end{figure}


\begin{table}[!ht]
% \vspace{-0.5cm}
\tiny
    \centering
    \captionsetup{font=scriptsize}
    \caption{Computational results of BUPRO-N problem ($K=1000$, $\vt^*=0.3392$)}
    \vspace{-0.2em}
    \renewcommand\arraystretch{1.1}
    \begin{threeparttable}
    \resizebox{0.8\linewidth}{!}
    {
    \begin{tabular}{c|ccccc}
        \hline
         \textbf{EPLA} & 
        Lotteries & Optimal solutions & Optimal values & Error & CPU time (s) \\
        \hline
        \multirow{3}{*}{\makecell{Type-1 \\ }} & $5\times5$ & $[0,0,0,1,0,0,0,0]$ & 0.3122 & 0.0270 & 113.6 \\
        & $10\times10$ & $[0,0,0,0.955,0,0,0.016,0.029]$ & 0.3321 & 0.0071 & 151.6 \\
        & $15\times15$ & $[0,0,0,1,0,0,0,0]$ & 0.3349 & 0.0043 & 223.3 \\
        \hline
        \multirow{3}{*}{\makecell{Type-2 \\ }} &
        $5\times5$ & $[0,0,0,1,0,0,0,0]$ &  0.3122 & 0.0270 & 115.2 \\
        & $10\times10$ & $[0,0,0,0.961,0,0,0.008,0.031]$ & 0.3324 & 0.0068 & 164.7 \\
        & $15\times15$ & $[0,0,0,1,0,0,0,0]$ & 0.3349 & 0.0043 & 220.5 \\
        \hline
        \multirow{3}{*}{\makecell{Mixed-type \\ }} &
        $5\times5$ & $[0,0,0,1,0,0,0,0]$ &  0.3122 & 0.0270 & 886.2 \\
        & $10\times10$ & $[0,0,0,0.955,0,0.003,0.009,0.033]$ & 0.3321 & 0.0071 & - \\
        & $15\times15$ & $[0,0,0,1,0,0,0,0]$ & 0.3349 & 0.0043 & - \\
        \hline
    \end{tabular}
    }
    \begin{tablenotes}
    \raggedleft
        \item `-' implies runtime $>$ 3600s. 
    \end{tablenotes}
    \end{threeparttable}
    \vspace{-0.3cm}
    \label{tab-result-N}
\end{table}


\begin{table}[!htbp]
   \tiny
    \centering
    \captionsetup{font=scriptsize}
    \caption{{\bf EPLA:} upper bound for $\dd_{\mathscr{G}_I}(u^*,{u}_N^*)$ and distance $\dd_{\mathscr{G}_I}({u}_N^{*},u_{\rm worst}^N)$}
    \vspace{-0.2cm}
  \renewcommand\arraystretch{1.2}
    \begin{threeparttable}
    \resizebox{0.9\linewidth}{!}
    {
    \begin{tabular}{ccccccc}
        \hline
      Lotteries &
        $L(\beta_{N_1}+\beta_{N_2})$
        & $\dd_{\mathscr{G}_I}({u}_N^*,u_{\rm worst}^N)$ (Type-1) &  $\dd_{\mathscr{G}_I}({u}_N^*,u_{\rm worst}^N)$ (Type-2) & $\dd_{\mathscr{G}_I}({u}_N^*,u_{\rm worst}^N)$ (Mixed-type) \\
        \hline
       $5\times 5$ & $1.8541$ & $0.0763$ & $0.0763$ & $0.0763$\\
         $10\times 10$ & $1.0611$ & $ 0.0233$ &  $0.0306$ & $0.0226$\\
        $15\times 15$ & $0.7682$ & $0.0141$ &  $0.0230$ & $0.0141$ \\
        \hline
%         \hfill
%   $5\times 5$ & $1.8541$ &  \\
%       &  $10\times 10$ & $1.0611$ &  $0.0763$\\
%       & $15\times 15$ & $0.7682$ &  \\
%         \hline
    \end{tabular}
    }
    \end{threeparttable}
    \vspace{-0.5cm}
    \label{tab:distance}
\end{table}


\underline{IPLA in bi-attribute case.}
% In this subsection,
% we calculate the PRO model (\ref{eq:PRO_MILP_mina2}) when
Set $K=20$ (take the first $20$ from $1000$ samples, we do so because the problem size of (\ref{eq:PRO_MILP_mina2}) and (\ref{eq:PRO_MILP_single}) 
depends on the $K$ whereas 
problem size of (\ref{eq:PRO-N-reformulate}) under EPLA 
is independent of $K$),
the true utility $u^*$ is the same as in EPLA case.
In this set of tests,
%part,
the convexity/concavity of single-variate utility functions $u_N(\cdot,\hat{y})$ and $u_N(\hat{x},\cdot)$ for all $\hat{x}\in X$ and $\hat{y}\in Y$ is not considered to facilitate
comparison of the three models (maximin EPLA, maximin IPLA and single MILP using IPLA).
 because  in problem (\ref{eq:PRO_MILP_single}), we have not incorporated 
% the dual formulation of the problem with 
% the convexity/concavity 
the constraints (see our comments there).
%are very complex. 
In Table~\ref{tab-result-N-MILP},
we compare the three models for the tractable reformulation of BUPRO-N:
EPLA 
(\ref{eq:PRO-N-reformulate}),
IPLA (\ref{eq:PRO_MILP_mina2})
and the single MILP (\ref{eq:PRO_MILP_single}) using IPLA,
for both Type-1 PLA and Type-2 PLA in terms of the optimal solution, the optimal value, error 
between BUPRO-N and utility maximization problem $\max_{\bdz \in Z} \sum_{k=1}^K p_k[u^*({\bm f}(\bdz,\bdxi^k))]$, and CPU time.
We %also
find that the optimal values $\vt_N$ converge to the true optimal value $\vt^*$ in all cases.
We also find that for both types,
the EPLA
(\ref{eq:PRO-N-reformulate}) where the inner problem is an LP is most efficient,
 the single MILP (\ref{eq:PRO_MILP_single})
 obtains the best approximate optimal values but takes %the 
 longest CPU time.
 Note that although the three models are 
 equivalent theoretically,
 the actual computational results 
 %are different 
 differ  slightly  because of computational rounding errors.
%{\color{Green} takes the largest values but the longest CPU time,}
% and IPLA (\ref{eq:PRO_MILP_mina2}),
% where the inner problem is an MILP,
% %gets close 
% obtains more accurate
% optimal values
% %with
% than
% (\ref{eq:PRO-N-reformulate}) but 
% takes 
% longer CPU time.
Figures~\ref{fig:question-Utility_MILP}-\ref{fig:question-Utility-MILP2}  display the worst-case utility functions of IPLA maximin model (\ref{eq:PRO_MILP_mina2}) for Type-1 and Type-2 respectively.
% {\color{blue}
We can see that the worst-case utility function 
%does not converge in a tamed manner ( 
displays some ``oscillations''
although it converges to the true.
The phenomenon disappears
when we  confine  
$u_N(\cdot,\hat{y})$ and $u_N(\hat{x},\cdot)$ 
% single-variate utility function of the PLA $u_N$
to convex and concave functions respectively.

% We can find that the worst-case utility functions are 
% %not
% % smooth 
% accidented
% when we do not confine the single-variate utility functions of the PLA $u_N$ to  a concave/convex function.}
% {\color{red}The word ``smooth" is not precise since the piecewise linear function is in general not smooth. Please double check.}
% %$n=8$, 
%and $u(x,y)=e^{x}-e^{-y}-e^{-x-2y}$,
% which is defined over $[0,1]\times [0,1]\times [0,1]$.
% See the worst-case utility function for $5\times 5$, $10\times 10$, $15\times 15$ questions in \cref{fig:question-Utility-MILP2},
% and the optimal solution, optimal value, CPU time in \cref{tab-result-N-MILP}.
% From \cref{tab-result-N-MILP},
% we can see that the CPU time of solving BUPRO-N problem %by
% using
% derivative-free method when the inner problem is modeled %by
% using
% MILP,
% is relatively long.
% Then we will only consider one type of division since it performs well 
% %in 
% during
% preference elicitation process
% and incorporating two types to construct ambiguity set and solving the corresponding the BUPRO-N will cost much more time.
% {\color{blue}
% There are three tractable reformulations of BUPRO-N,
% %$\max_{\bdz\in Z}\min_{u\in {\cal U}_N} \frac{1}{K}\sum_{k=1}^Kp_ku(f(\bdz,\bdxi^k))$,
% see problems \cref{eq:PRO-N-reformulate,eq:PRO_MILP_mina2,eq:PRO_MILP_single},
% % When $f(\bdz,\bdxi)$ is linear in $\bdz$,
% % the inner minimization problem of (\ref{eq:PRO-N-reformulate}) is a LP for fixed $\bdz$,
% % the inner problem of (\ref{eq:PRO_MILP_mina2}) is an MILP,
% % while problem (\ref{eq:PRO_MILP_single}) is a single MILP,
% see the results in \cref{tab-result-N-MILP}.
% % Next,
% % we will present the results of these three models with sample size $K=20$.
% }
% The following example just uses the increasing utility properties, not the concave or Lipschtiz continuity property.
\begin{figure}[!ht]
  \centering
  \vspace{-1.5em}
      \subfigure%[$5\times 5$]
      {
    \includegraphics[width=0.23\linewidth]{Copy_of_Utility_5_5.pdf}
  }
  \hspace{-0.5em}
    \subfigure%[$10\times 10$]
    {
    \includegraphics[width=0.23\linewidth]{Copy_of_Utility_10_10.pdf}
  }
    \hspace{-0.5em}
    \subfigure%[$15\times 15$]
    {
    \includegraphics[width=0.23\linewidth]{Copy_of_Utility_15_15.pdf}
  }
    \hspace{-0.5em}
      \subfigure%[$15\times 15$]
    {
    \includegraphics[width=0.23\linewidth]{Copy_2_of_Utility_15_15.pdf}
  }
  \captionsetup{font=footnotesize}
  \vspace{-0.2cm}
  \caption{{\bf Type-1 IPLA}: the convergence of the worst-case utility function solved by the Dfree method for IPLA  model (\ref{eq:PRO_MILP_mina2}) without convex/concave constraints.
  }
  \vspace{-0.3cm}
  \label{fig:question-Utility_MILP} 
\end{figure}

\begin{figure}[!ht]
  \centering
  \vspace{-0.4cm}
      \subfigure{
    \includegraphics[width=0.23\linewidth]{Copy_of_Utility_5_5_counter.pdf}
  }
    \hspace{-0.5em}
    \subfigure{
    \includegraphics[width=0.23\linewidth]{Copy_of_Utility_10_10_counter.pdf}
   }
     \hspace{-0.5em}
    \subfigure{
    \includegraphics[width=0.23\linewidth]{Copy_of_Utility_15_15_counter.pdf}
  }
    \hspace{-0.5em}
      \subfigure{
    \includegraphics[width=0.23\linewidth]{Copy_2_of_Utility_15_15_counter.pdf}
  }
  \vspace{-0.2cm}
  \captionsetup{font=footnotesize}
%   \vspace{-1.5em}
  \caption{{\bf Type-2 IPLA}: the convergence of the worst-case utility function solved by Dfree method for IPLA  model (\ref{eq:PRO_MILP_mina2}) with (\ref{eq:mixed-integer-R2-f}) being replaced by 
(\ref{eq:constraint-alpha}).
}
  %without convex/concave constraints}
  % \vspace{-0.3cm}
  \label{fig:question-Utility-MILP2} 
\end{figure}

\begin{table}[!ht]
    \tiny
    \centering
    \captionsetup{font=scriptsize}
    \caption{{\bf The bi-attribute case:} comparison of the results of BUPRO-N problem (K=20, $\vt^*=0.3835$)}
    \vspace{-0.2cm}
    \renewcommand\arraystretch{1.1}
    \resizebox{0.9\linewidth}{!}
    {
    \begin{tabular}{c|ccccc}
        \hline
         & 
        Lotteries & Optimal solutions & Optimal values & Error & CPU time (s) \\
        \hline              
        \multirow{3}{*}{\makecell{Type-1 \\ Maximin \\({\bf EPLA})}} & $5\times5$ & $[ 0.112, 0.037,0,0.439,0.024, 0,0.054,0.335]$ &  0.2835  &  0.1000 & 47.4 \\
        & $10\times10$ & $[0,  0.599,0,0.316,0.008,0.037,0.012,0.027]$  & 0.3479 & 0.0356  & 82.8 \\
      & $15\times15$  & $[0,1,0,0,0,0,0,0]$  & 0.3754 & 0.0081 & 146.2 \\
        \hline
        \multirow{3}{*}{\makecell{Type-1 \\ Maximin
        \\ ({\bf IPLA}) } } &
      $5\times5$ & $[0.0996,0.0313,0.0297,0.4525,0, 0,0.0469,0.3400]$ &  0.2824 & 0.1011 & 240.5 \\
        & $10\times10$ & $[0, 0.9467,0,0, 0,0.0476,0,0.0057]$  & 0.3697 &  0.0138 & 916.0 \\
        & $15\times15$ &  $[0,0.9902,0,0.0038,0,0.0060,0,0]$  & 0.3748  & 0.0087 & 2442.2 \\
        \hline
        %  \multirow{3}{*}{\makecell{Type-1 \\ (LP)}} & $5\times5$ & $[0,0,0,1,0,0,0,0]$ & 0.3119&  & 103.0744 \\
        % & $10\times10$ & [0,0.455,0,0.483,0, 0.029,0.016,0.016]  &  0.3341 &&115.9924  \\
        % & $15\times15$ &$[0,1,0,0,0,0,0,0]$   &  0.3535 &  &189.8908 \\
        %   \hline
        % \multirow{3}{*}{\makecell{Type-1 \\ (MILP)}} & $5\times5$ & $[0,0,0,1,0,0,0,0]$ & 0.3119&  &  870.3648\\
        % & $10\times10$ & $[0.002,0.484,0, 0.476,0,0,0,0.038]$  & 0.3329 &&  3328.4393 \\
        % & $15\times15$ & $[0,1,0,0,0,0,0,0]$  & 0.3525 &  & 8962.4213\\
        %   \hline
    %     \multirow{3}{*}{\makecell{Type-1 \\(MILP) } } &
    %   $5\times5$ & $[0, 0,0,0.746,0, 0.076,0,0.178]$ &  0.2805 &  & 252.0013 \\
    %     & $10\times10$ & $[0, 0.550,0,0.371, 0.006,0.040,0.024,0.008]$  & 0.3465 &  & 973.5908 \\
    %     & $15\times15$ &  $[0,1,0,0,0,0,0,0]$  & 0.3754  &  & 2937.4662\\
    %     \hline
        \multirow{3}{*}{\makecell{Type-1\\  Single MILP \\
        ({\bf IPLA})} } &
        $5\times5$ & $[0,1,0,0,0,0,0,0]$ &  0.3232 & 0.0603 & 1103.2 \\
        & $10\times10$ & $[0,0.946,0,0,0,0.043,0,0.011]$ & 0.3698 &  0.0137 & 4552.2 \\
        & $15\times15$ & $[0,1,0,0,0,0,0,0]$ & 0.3754 & 0.0081 &  3421.1 \\
        \hline
        \hline
        %  \multirow{3}{*}{\makecell{Type-2 \\ (LP)}
        % } & $5\times5$ & $[0,0,0,0.828,0, 0.165,0.007,0]$ & 0.3140 &  &  71.6129\\
        % & $10\times10$ & $[0, 0.366,0,0.576,0,0.030,0.005,0.022]$   &0.3333 && 115.9925 \\
        % & $15\times15$ &   $[0,1,0,0,0,0,0,0]$ &  0.3535 &  & 183.4941 \\
        % \hline      \multirow{3}{*}{\makecell{Type-2 \\ (MILP)}
        % } &
        % $5\times5$ & $[0,1,0,0,0,0,0,0]$  & 0.3279 &  &  838.5859 \\
        % & $10\times10$ & $[0,1,0,0,0,0,0,0]$  &  0.3445&  & 3198.2773 \\
        % & $15\times15$ & $[0,0.595,0, 0.405,0,0,0,0]$ & 0.3388 & & 10104.5450   \\
        % \hline
         \multirow{3}{*}{\makecell{Type-2 \\ Maximin 
         \\({\bf EPLA})}
        } & $5\times5$ & $[0,0.1542,0.0117,0.1210,0.3554,0.1679,0,0.1898]$ &0.3113 &  0.0722 &  43.7 \\
        & $10\times10$ &  $[0, 0.5301,0,0.1857,0.2030,0.0410,0.0308,0.0094]$ & 0.3475 & 0.0360 &  56.4 \\
        & $15\times15$ &  $[0,1,0,0,0,0,0,0]$  & 0.3754 &  0.0081 & 98.2 \\
        \hline
        \multirow{3}{*}{\makecell{Type-2 \\ Maximin
        \\({\bf IPLA})}
        } &
        $5\times5$ & $[0, 0.8875,0.1125,0,0,0,0,0]$  & 0.3102  &  0.0733 &  213.6 \\
        & $10\times10$ & $[0,0.975,0.025,0,0,0,0,0]$  & 0.3410 & 0.0425 &  828.2 \\
        & $15\times15$ &  $[0,0.7129,0.0986,0,0.1884,0,0,0]$ & 0.3474 & 0.0392 &  2126.9 \\
        \hline
      \multirow{3}{*}{\makecell{Type-2\\ Single MILP \\
      ({\bf IPLA})} } &
        $5\times5$ & $[0,1,0,0,0,0,0,0]$ &  0.3232 & 0.0603 & 952.0 \\ 
        & $10\times10$ &$[0,0.9470,0,0,0, 0.0467,0,0.0062]$  &  0.3704 & 0.0131 & 1359.1\\
        & $15\times15$ & $[0,1,0,0,0,0,0,0]$  &  0.3754 & 0.0081 &  2680.9\\
        \hline
    \end{tabular}
    }
    \label{tab-result-N-MILP}
    \vspace{-0cm}
\end{table}




% {\color{blue}
% Sainan: to add one more result in the table.}

% \begin{table}[!ht]
%     \tiny
%     \centering
%     \captionsetup{font=scriptsize}
%     \caption{Comparison of the results of problems (\ref{eq:PRO-N-reformulate}), (\ref{eq:PRO_MILP_mina2}) and (\ref{eq:PRO_MILP_single})}
%     \vspace{-1em}
%     \renewcommand\arraystretch{1.08}
%     \resizebox{0.85\linewidth}{!}
%     {
%     \begin{tabular}{c|ccccc}
%         \hline
%          & 
%         Lotteries & Optimal solutions & Optimal values && Time(s) \\
%         \hline
%         \multirow{3}{*}{\makecell{Type-1 \\ (LP)}} & $5\times5$ & $[ 0.112, 0.037,0,0.439,0.024, 0,0.054,0.335]$ &  0.2835  &  & 47.3603 \\
%         & $10\times10$ & $[0,  0.599,0,0.316,0.008,0.037,0.012,0.027]$  & 0.3479 &   & 82.7862 \\
%       & $15\times15$  & $[0,1,0,0,0,0,0,0]$  & 0.3754 &  & 146.2440\\
%         \hline
%         \multirow{3}{*}{\makecell{Type-1 \\(MILP) } } &
%       $5\times5$ & $[0, 0,0,0.746,0, 0.076,0,0.178]$ &  0.2805 &  & 252.0013 \\
%         & $10\times10$ & $[0, 0.550,0,0.371, 0.006,0.040,0.024,0.008]$  & 0.3465 &  & 973.5908 \\
%         & $15\times15$ &  $[0,1,0,0,0,0,0,0]$  & 0.3754  &  & 2937.4662\\
%         \hline
%         \multirow{3}{*}{\makecell{Type-1\\ (Single MILP)} } &
%         $5\times5$ & $[0,1,0,0,0,0,0,0]$ &  0.3232 &  & 1103.2002 \\
%         & $10\times10$ & $[0,0.946,0,0,0,0.043,0,0.011]$ & 0.3698 &  & 4552.2126\\
%         & $15\times15$ & $[0,1,0,0,0,0,0,0]$ & 0.3754 &  &  3421.1479\\
%         \hline
%     \end{tabular}
%     }
%     \label{tab:single-MILP}
% \end{table}

% \newpage


\underline{IPLA in tri-attribute case.}
The sample is the same as in the bi-attribute case with $K=20$.
The true utility function is
%takes the form 
$u(x,y,z)=e^{x}-e^{-y}-e^{-z}-e^{-x-2y-z}:[0,1]^3 \to [0,1]$
and normalize it by setting $u^*(x,y,z)=(u(x,y,z)-u(0,0,0))/(u(1,1,1)-u(0,0,0))$.
% Note that $u^*$ is convex in $x$, and concave in $y$ and $z$ respectively.
We divide the eight projects into three groups in order of importance as the three attributes, that is,
$f_1^k:=\sum_{i=1}^{3} w_i \xi_i^k$, $f_2^k:=\sum_{i=4}^{6} w_i \xi_i^k$,
$f_3^k:=\sum_{i=7}^{8} w_i \xi_i^k$,
and ${\bm w} \in Z:=\{{\bm w}\in\R^8_+:\sum_{i=1}^8 w_i=1\}$.
%we use the same samples as in the two-dimensional case.
Table~\ref{tab-result-N-MILP3} indicates that the IPLA model (\ref{eq:PRO_MILP_3m}) in  tri-attribute case is effective and the optimal values $\vt_N$ of the TUPRO-N problem converge to the true optimal value $\vt^*$ as the number of lotteries increases.
% from $3\times 3\times 3$ to $6\times 6\times 6$.

\begin{table}[!ht]
\vspace{-0.3cm}
    \tiny
    \centering
    \captionsetup{font=scriptsize}
    \caption{{\bf The tri-attribute case}: computational results of TUPRO-N problem in (K=20, $\vt^*=0.3193$)}
    \vspace{-1em}
    \renewcommand\arraystretch{1.08}
    \begin{threeparttable}
    \resizebox{0.8\linewidth}{!}
    {
    \begin{tabular}{c|ccccc}
        \hline
         & 
        Lotteries & Optimal solutions & Optimal values & Error & CPU time (s) \\
        \hline
        \multirow{3}{*}{ {\bf IPLA}
        %Type  -1
        %  \makecell{Type-1\\ }
        } & $3\times3 \times 3$ & $[0,0,1,0,0,0,0,0]$ & 0.1994 & 0.1198 & 320.8 \\
        & $4 \times 4 \times 4$ & $[ 0.4992,0.5008,0,0, 0,0,0,0]$  & 0.2076 &  0.1117 & 944.1 \\
        & $5\times 5 \times 5$ & $[0,1,0, 0,0,0,0,0]$  & 0.2498 & 0.0694 & 2083.7 \\
        & $6\times 6 \times 6$  & $[0,1,0, 0,0,0,0,0]$ & 0.2774 & 0.0418 &  - \\
          \hline
    \end{tabular}
    }
    \begin{tablenotes}
    \raggedleft
        \item `-' implies runtime $>$ 3600s. 
    \end{tablenotes}
    \end{threeparttable}
    \vspace{-0.5cm}
    \label{tab-result-N-MILP3}
\end{table}


\textbf{(ii) EPLA for the constrained optimization problems (\ref{eq:PRO-x}) and 
(\ref{eq:PRO-x-1}).}
The second part of numerical tests is concerned with 
% decision-dependent
problems (\ref{eq:PRO-x}) and (\ref{eq:PRO-x-1}).
We set $g_1(\bdz,\bdxi^k):=\sum_{i=3}^5 z_i\xi_i^k$ and 
$g_2(\bdz,\bdxi^k):=\sum_{i=7}^8 z_i\xi_i^k$,
% $g_2(\bdz,\bdxi)=\sum_{i=7}^8 z_i\xi_i$ 
which represent the 
effects of part of the 
%outcomes
projects on 
%of partial 
mental health and cancer 
%projects, 
commissioning areas.
PHO expects %the decision-dependent expected utility of this part 
this part of effects to
reach at least level $c$.
We consider two cases: (a)
%when 
$c=0.1$ and (b) $c=0.3$.

\underline{Case (a)}. %the optimal solutions and 
%the optimal solutions and 
The optimal values of problem (\ref{eq:PRO-x}) and problem (\ref{eq:PRO-x-1}) coincide (see
%are the same as those
%the counterpart as 
%in 
Table~\ref{tab-result-N}) because 
the optimal solution of the former
falls into set (\ref{eq:x*-PRO-U}),
%and we omit them here, 
%which verifies the analysis in  
which is consistent with our theoretical analysis in Proposition~\ref{Prop-equivalence}.
%In the case that} 
 \underline{Case (b)}. We repeat the tests but with different observations.
% The results of problem (\ref{eq:PRO-x}) are presented in Table~\ref{tab-result-DN-2}
% and the results of problem (\ref{eq:PRO-x-1}) are listed in Table~\ref{tab-result-DN-3}.
% 
Recall that the optimal values of problems (\ref{eq:SPR-x}), (\ref{eq:PRO-x}) and (\ref{eq:PRO-x-1}) are denoted by $\vt^*$, $\hat{\vt}$ and $\tilde{\vt}$, respectively.

Observation 1. 
For problem (\ref{eq:PRO-x-1}),
%as analysed earlier, 
we can see from  Table~\ref{tab-result-DN-3}
that $\tilde{\vt}<\vt^*$ and $\tilde{\vt}$  increases as $M$ increases. This is 
consistent with our theoretical analysis.
The increasing trend is underpinned 
by the fact that 
as $M$ increases,
%since the ambiguity set 
$\calu_N$
becomes smaller
%since the size of ${\cal U}$ reduces
%as the number of lotteries increases
%and then the objective function increases, and constraint function increase which makes the feasible set w.r.t. 
and consequently both the objective function $\min_{u\in {\cal U}} \bbe_{P}[u({\bm f}(\bdz,\bdxi))]$ and  the feasible set  
$\tilde{Z}$ (see  (\ref{eq:x*-PRO-U}))
become larger.

Observation 2. 
For
problem (\ref{eq:PRO-x}),
% The optimal values 
we can see from Table~\ref{tab-result-DN-2} that
$\vt^*<\hat{\vt}$ 
% and $\tilde{\vt}$ are different 
for the cases that $5\times 5$ and $10\times 10$ lotteries are used.
%% since $\hat{\bdz}\notin \tilde{Z}$.
%and $\vt^*>\hat{\vt}$. 
% The optimal values $\vt^*$ and $\tilde{\vt}$ are the same 
%when $15\times 15$ lotteries are used.
Note that by theory,
$\tilde{\vt}\leq \hat{\vt}$ and $\tilde{\vt}\leq \vt^*$. Moreover,
 when $\hat{\bdz}\in \tilde{Z}$,
we are guaranteed that $\tilde{\vt}=\hat{\vt}\leq \vt^*$.
The observed trend reflects the fact that $\hat{\vt}>\vt^*$ may occur when
$\hat{\bdz}\notin \tilde{Z}$.
%whereas the second observed tread
%Observation 3. 
Moreover,
$\vt^*>\hat{\vt}$ 
% The optimal values $\vt^*$ and $\tilde{\vt}$ are the same 
when $15\times 15$ lotteries are used since
$\hat{\bdz}\in \tilde{Z}$.
% the increasing number of questions 
% reduces the size of ${\cal U}$ and 
% $\min_{u\in {\cal U}} \bbe_{P}[u({\bm g}(\bdz,\bdxi))]$  increases.
% % thus
% %will restrict
% % %confine
% % % restrain 
% % the 
% % %value 
% % range of the PLA
% % %piecewise linear 
% % utility function 
% % %may take at a specific point
% % to a smaller interval.
% Subsequently, the size of the 
% feasible set $\tilde{Z}$ defined in
% (\ref{eq:x*-PRO-U}) increases,
% % of problem (\ref{eq:PRO-x-1})
% which makes the condition 
% of Proposition~\ref{Prop-equivalence}
% to be satisfied more easily. . 

Observation 3. The optimal value $\hat{\vt}$ is decreasing from Table~\ref{tab-result-DN-2} as the number of questions increases. 
This phenomena is a bit difficult to explain. On one hand,
when the size of ${\cal U}_N$ decreases,
$\hat{v}(z)$ increases 
and on the other hand
the size of $\hat{Z}:=\{\bdz:\bbe_P[u(\bdg(\bdz,\bdxi))] \geq c\}$ decreases.
Note that $\hat{\vt} = \max_{z\in \hat{Z}}\hat{v}(z)$,
 it seems the reduction 
of the size of $\hat{Z}$ has more effect than
that of the increase of  $\hat{v}(z)$ in this test. 

We have not tested IPLA as our focus here is on the difference between 
model (\ref{eq:PRO-x}) and model (\ref{eq:PRO-x-1}) rather than 
different performances of EPLA and IPLA.

% we obtain
% $\hat{\vt} > \vt^*$ 
% % get the 
% %different 
% %results
% % the relationship between the optimal values and optimal solutions
% % of problems (\ref
% with $5\times 5$ and $10\times 10$ lotteries,
% but $\hat{\vt}< \vt^*$
% % the same optimal values and optimal solutions
% with
% $15\times 15$ lotteries,
% see
% %and present them %the computational results are presented 
% %in
% Tables~\ref{tab-result-DN-2}-\ref{tab-result-DN-3}.
% where 
% %Notice that
% the optimal solutions and optimal values are different from those in A\cref{tab-result-N}. 
% for problem (\ref{eq:PRO-x-1}),
% as expected,
% % envisaged,
% $\tilde{\vt}\leq \vt^*$  (in Table~\ref{tab-result-DN-3}).
% %the optimal value of
% %For 
% %problem 
% %(\ref{eq:PRO-x-1})
% %, the optimal value 
% % is smaller than that
% % %the optimal value 
% % of %problem 
% % (\ref{eq:SPR-x}) (calculated 
% % with the true utility function).
% % Moreover, 
% %the optimal values
% We can see  that $\tilde{\vt}$ increases %when
% as the number of lotteries increases since the ambiguity set becomes smaller.
% For problem (\ref{eq:PRO-x}),
% %the optimal value
% $\hat{\vt}$ (in Table~\ref{tab-result-DN-2}) decreases and moves toward $\tilde{\vt}$ (in Table~\ref{tab-result-DN-3}).
% %towards that
% %the optimal value 
% %of problem (\ref{eq:PRO-x-1})
% This is primarily because
% %since 
% the increasing number of questions 
% reduces the size of ${\cal U}$ and 
% $\min_{u\in {\cal U}} \bbe_{P}[u({\bm g}(\bdz,\bdxi))]$  increases.
% % thus
% %will restrict
% % %confine
% % % restrain 
% % the 
% % %value 
% % range of the PLA
% % %piecewise linear 
% % utility function 
% % %may take at a specific point
% % to a smaller interval.
% Subsequently, the size of the 
% feasible set $\tilde{Z}$ defined in
% (\ref{eq:x*-PRO-U}) increases,
% % of problem (\ref{eq:PRO-x-1})
% which makes the condition 
% of Proposition~\ref{Prop-equivalence}
% to be satisfied more easily.
% From Table~\ref{tab-result-DN-2},
% %-\ref{tab-result-DN-3},
% we obtain that
% $\hat{\vt} > \vt^*$ 
% % get the 
% %different 
% %results
% % the relationship between the optimal values and optimal solutions
% % of problems (\ref
% with $5\times 5$ and $10\times 10$ lotteries because
% $\hat{\bdz}\notin\tilde{Z}$ (defined in (\ref{eq:x*-PRO-U})),
% but $\hat{\vt}< \vt^*$ 
% % the same optimal values and optimal solutions
% with
% $15\times 15$ lotteries.
% %and present them %the computational results are presented 
% %in
% % In the case that $5\times 5$ and $10\times 10$ lotteries are used (in Tables~\ref{tab-result-DN-2}-\ref{tab-result-DN-3}),
% % %It is easy to verify that 
% % $\hat{\bdz}\notin\tilde{Z}$ (defined in (\ref{eq:x*-PRO-U})) 
% % %in the case that $5\times 5$ and $10\times 10$ lotteries are used,
% % which results in
% %  $\hat{\vt}>\tilde{\vt}$.
%  Moreover, we find that $\hat{\vt}$ (in Table~\ref{tab-result-DN-2})
% increases as the number of questions increases (the size of ${\cal U}_N$ decreases). This phenomena is a bit difficult to explain. On one hand,
% when the size of ${\cal U}_N$ decreases,
% $\hat{v}(z)$ increases 
% and on the other hand
% the size of $\hat{Z}:=\{\bdz:\bbe_P[u(\bdg(\bdz,\bdxi))] \geq c\}$ decreases.
% Note that $\hat{\vt} = \max_{z\in \hat{Z}}\hat{v}(z)$,
%  it seems the reduction 
% of the size of $\hat{Z}$ has more effect than
% that of the increase of  $\hat{v}(z)$ in this test. We have not tested IPLA as our focus here is on the difference between 
% model (\ref{eq:PRO-x}) and model (\ref{eq:PRO-x-1}) rather than 
% different performances of EPLA and IPLA.
%leads to
% the optimal values of problem (\ref{eq:PRO-x})
% %are bigger than 
% to be larger than those of problem (\ref{eq:PRO-x-1}).


% Figures \ref{fig-utility-dd-main}-\ref{fig-utility-dd-mixed} display the worst-case piecewise linear utility function and
% the true utility function with the main diagonal, counter diagonal and mixed partition.
% Table \ref{tab-result-DN-2} and \ref{tab-result-DN-3} display the optimal solutions, optimal values, error of optimal values from the true optimal values and computation time with the main diagonal, counter diagonal and mixed partition corresponding to problem (\ref{eq:PRO-x}) and (\ref{eq:PRO-x-1}).
% we can see that the optimal values are increasing if the number of lotteries increase. This is because when the number of lotteries increase, the ambiguity set of utility is smaller. 
\begin{table}[!ht]
    \tiny
    \centering
    % \vspace{-0.6cm}
    \captionsetup{font=scriptsize}
    \caption{Computational results of
    % constrained preference robust optimization 
    problem (\ref{eq:PRO-x-1}) ($K=1000$, $c=0.3$, $\vt^*=0.3387$)}
    \vspace{-0.5em}
    \renewcommand\arraystretch{1.1}
    % \resizebox{\linewidth}{}
    \begin{threeparttable}
    \resizebox{0.8\linewidth}{!}
    {
    \begin{tabular}{c|ccccc}
        \hline
         \textbf{EPLA} & 
        Lotteries & Optimal solutions & $\tilde{\vt}$ & Error & CPU time (s) \\
        \hline
        \multirow{3}{*}{Type-1
        % \makecell{Type-1 \\ partition} 
        } & $5\times5$ & $[0,0,0,1,0,0,0,0]$ & 0.3122 & 0.0265 & 237.0 \\
        & $10\times10$ & $[0,0,0,0.955,0,0,0.011,0.034]$ & 0.3321 & 0.0066 & 379.9 \\
        & $15\times15$ & $[0,0,0,1,0,0,0,0]$ & 0.3349 & 0.0038 & 436.7 \\
        \hline
        \multirow{3}{*}{Type-2
        % \makecell{Type-2 \\ partition}
        } &
        $5\times5$ & $[0,0,0,1,0,0,0,0]$ &  0.3122 & 0.0265 & 307.1 \\
        & $10\times10$ & $[0,0,0,0.959,0,0,0,0.041]$ & 0.3323 & 0.0064 & 321.2 \\
        & $15\times15$ & $[0,0,0,1,0,0,0,0]$ & 0.3349 & 0.0038 & 486.0 \\
        \hline
        \multirow{3}{*}{ Mixed-type
        % \makecell{Mixed\\ partition}
        } &
        $5\times5$ & $[0,0,0,1,0,0,0,0]$ &  0.3122 & 0.0265 & 2476.2 \\
        & $10\times10$ & - & - & - & - \\
        & $15\times15$ & - & - & - & - \\
        \hline
    \end{tabular}
    }
    \begin{tablenotes}
    \raggedleft
        \item `-' implies runtime $>$ 3600s.
    \end{tablenotes}
    \end{threeparttable}
    \vspace{-0.6cm}
    \label{tab-result-DN-3}
\end{table}

\begin{table}[!ht]
% \vspace{-0.5cm}
    \tiny
    \centering
    \captionsetup{font=scriptsize}
    \caption{Computational results of
    % maximin preference robust optimization 
    problem (\ref{eq:PRO-x}) ($K=1000$, $c=0.3$, $\vt^*=0.3387$)}
    % \resizebox{\textwidth}{3em}
    % \renewcommand\arraystretch{1.15}
    \vspace{-0.2cm}
    \renewcommand\arraystretch{1.1}
    \begin{threeparttable}[b]
    \resizebox{0.9\linewidth}{!}
    {
    \begin{tabular}{c|ccccc}
        \hline
        \textbf{EPLA} & 
        Lotteries & Optimal solutions & $\hat{\vt}$ & Error & CPU time (s)\\
        \hline
        \multirow{3}{*}{Type-1
        % \makecell{Type-1 \\ partition} 
        } & $5\times5$ & $[0.118,0.115,0.178,0.179,0,0.130,0.112,0.169]$ & 0.3873 & -0.0486 & 216.0 \\
        & $10\times10$ & $[0,0.111,0,0.883,0,0.006,0,0]$ & 0.3413 & -0.0026 & 255.5 \\
        & $15\times15$ & $[0.007,0.098,0,0.875,0.020,0,0,0]$ & 0.3377 & 0.0010 & 363.4 \\
        \hline
        \multirow{3}{*}{Type-2
        % \makecell{Type-1 \\ partition} 
        } &
        $5\times5$ & $[0.176,0.129,0,0.077,0,0.178,0.084,0.357]$ &  0.4186 & -0.0799 & 242.6 \\
        & $10\times10$ & $[0.027,0.082,0,0.891,0,0,0,0]$ & 0.3384 & 0.0003 & 261.7 \\
        & $15\times15$ & $[0,0,0,1,0,0,0,0]$ & 0.3349 & 0.0038 & 335.2 \\
        \hline
        \multirow{3}{*}{Mixed-type
        % \makecell{Type-1 \\ partition} 
        } &
        $5\times5$ & $[0.073,0.131,0.118,0.022,0.151,0.194,0.107,0.205]$ & 0.4095 & -0.0708 & 2287.6 \\
        & $10\times10$ & - & - & - & - \\
        & $15\times15$ & - & - & - & - \\
        \hline
    \end{tabular}
    }
    \hspace{-0.1cm}
    \begin{tablenotes}
    \raggedleft
        \item `-' implies runtime $>$ 3600s.
    \end{tablenotes}
    \end{threeparttable}
       \vspace{-0.5cm}
    \label{tab-result-DN-2}
\end{table}
% \begin{figure}[!htbp]
%   \centering
%   \subfigure{
%     \label{subfig-utility-b-m-dd}
%     \includegraphics[width=1.96in]{ut-m-dd-5.pdf}
%   } 
%   \subfigure{
%     \label{subfig-utility-c-m-dd}
%     \includegraphics[width=1.96in]{ut-m-dd-10.pdf}
%   }
%   \subfigure{
%     \label{subfig-utility-d-m-dd}
%     \includegraphics[width=1.96in]{ut-m-dd-15.pdf}
%   }
%   \vspace{-1em}
%   \captionsetup{font=footnotesize}
%   \caption{The true utility function and worst-case utility functions with the main diagonal in decision-dependent problem}
%   \label{fig-utility-dd-main} %% label for entire figure
% \end{figure}


% \begin{figure}[!htbp]
%   \centering
%   \subfigure{
%     \label{subfig-utility-b-dd}
%     \includegraphics[width=1.96in]{ut-c-dd-5.pdf}
%   } 
%   \subfigure{
%     \label{subfig-utility-c-dd}
%     \includegraphics[width=1.96in]{ut-c-dd-10.pdf}
%   }
%   \subfigure{
%     \label{subfig-utility-d-dd}
%     \includegraphics[width=1.96in]{ut-c-dd-15.pdf}
%   }
%   \vspace{-1em}
%   \captionsetup{font=footnotesize}
%   \caption{The true utility function and worst-case utility functions with the counter diagonal in decision-dependent problem}
%   \label{fig-utility-dd-counter} %% label for entire figure
% \end{figure}

% \begin{figure}[!htbp]
%   \centering
%   \subfigure{
%     \label{subfig-utility-b}
%     \includegraphics[width=1.96in]{ut-mixed-dd-5.pdf}
%   } 
%   \subfigure{
%     \label{subfig-utility-c}
%     \includegraphics[width=1.96in]{ut-mixed-dd-10.pdf}
%   }
%   \subfigure{
%     \label{subfig-utility-d}
%     \includegraphics[width=1.96in]{ut-mixed-dd-15.pdf}
%   }
%   \vspace{-1em}
%   \captionsetup{font=footnotesize}
%   \caption{The true utility function and worst-case utility functions with the mixed partition in decision-dependent problem}
%   \label{fig-utility-dd-mixed} %% label for entire figure
% \end{figure}



\subsection{Perturbation analysis}
This part of numerical tests is concerned with data perturbation including (i) elicitation data perturbation and (ii) sample average approximation (SAA) of the exogenous uncertainties.
SAA is needed when the true 
probability distribution $P$ in
(\ref{eq:MAUT-robust}) is continuously distributed. In this case, Assumption \ref{assu-discrete}
and the subsequent UPRO models
may be viewed as sample average approximations.
We skip the theoretical analysis about errors 
arising from SAA and refer interested readers to \cite{GXZ21}
in single-attribute case.


\textbf{(i) Perturbation %of
in the data in the ambiguity set.}
%}{\color{red}Elicitation contamination.}}
In this set of experiments, we will test the performance of the PLA scheme
% illustrate the theoretical results about the convergence of the optimal value and the error bounds established in Theorem \ref{thm-pertb}  
when the ambiguity sets $\calu$ and $\tilde{\calu}$ are replaced by  $\calu_N$ and $\tilde{\calu}_N$ respectively.
We begin by considering a situation where the underlying functions $\psi_l, l=1,\ldots,M$ in the ambiguity set are perturbed by the observation error of the random data in pairwise comparison questions, i.e., 
\begin{equation*}
    \tilde{\psi}_l(x,y):= \1_{[\hat{x}^l+\delta_1,1]\times [\hat{y}^l+\delta_2,1]}(x,y)-(1-p^l) \1_{[0,1]\times[0,1]\setminus (1,1)}(x,y)- \1_{(1,1)}(x,y), l=1,\ldots,\hat{M},
\end{equation*}
where $\hat{M}$ is the number of perturbed functions $\psi_l$.
Notice that some lotteries are on the boundary of rectangle $T$ which can only be perturbed inwards.
Thus we assume that these lotteries are not perturbed for the convenience of discussion.
Let $\calu_N=\{u_N\in\scru_N: \la u_N,\psi_l \ra\leq c_l, l=1,\ldots,M\} $
% \begin{equation*}
%     \calu_N=\{u_N\in\scru_N: \la u_N,\psi_l \ra\leq c_l, l=1,\ldots,M\}
% \end{equation*}
and 
\begin{equation*}
    \tilde{\calu}_N=\{u_N\in\scru_N: \la u_N,\tilde{\psi}_l \ra\leq c_l, l=1,\ldots,M\}.
\end{equation*}
We can  solve problem (\ref{eq:PRO-N-reformulate}) with $\psi_l$ being replaced by $\tilde{\psi}_l$ to obtain the optimal value and the corresponding worst-case utility function.
Specifically, we assume $\delta_2=0$, which means we only consider the case that the first attribute is slightly perturbed but the second attribute is not.
%implies that 
%{\color{blue} only the first attribute is slightly perturbed.}
Figures~\ref{fig-ptb-ut-main}-\ref{fig-ptb-ut-counter} depict the convergence of the worst-case utility functions as the number of questions increases for fixed $\delta_1=0.1$ with Type-1 PLA and Type-2 PLA.
%main and counter diagonals respectively.
% Figure \ref{fig-contour-main} and \ref{fig-contour-counter} depict the isopreference curves of the true utility function and the worst-case utility function with the main and counter diagonals respectively.
Figures~\ref{subfig-main-ov}-\ref{subfig-counter-ov} depict the changes of the optimal values as $\delta_1$ varies from $0.01$ to $0.1$ with different $M$.
%queries.
% We can see that the worst-case utility function converges to the true utility with the same perturbations as the number of lotteries increases, and the optimal values increase.



% [\scriptsize{$5\times 5$}]
\begin{figure}[!htbp]
  \centering
  \vspace{-0.2cm}
  \subfigure{
    \label{subfig-main-ut-ptb5} %% label for second subfigure
    \includegraphics[width=0.3\linewidth]{ut-m-ptb-6.pdf}
  }
    \hspace{-0.5em}
  \subfigure{
    \label{subfig-main-ut-ptb10} %% label for second subfigure
    \includegraphics[width=0.3\linewidth]{ut-m-ptb-8.pdf}
  }
    \hspace{-0.5em}
  \subfigure{
    \label{subfig-main-ut-ptb15}
    \includegraphics[width=0.3\linewidth]{ut-m-ptb-10.pdf}
  }
  \vspace{-1em}
  \captionsetup{font=footnotesize}
  \caption{{\bf Type-1 EPLA}: the worst-case utility function with $\delta_1=0.1$ }
  \vspace{-0.2cm}
  \label{fig-ptb-ut-main} %% label for entire figure
\end{figure}



% [\scriptsize{$5\times 5$}]
\begin{figure}[!htbp]
  \centering
  \vspace{-0.2cm}
  \subfigure{
    \label{subfig-counter-ut-ptb5} %% label for second subfigure
    \includegraphics[width=0.3\linewidth]{ut-c-ptb-6.pdf}
  }
    \hspace{-0.5em}
  \subfigure{
    \label{subfig-counter-ut-ptb10} %% label for second subfigure
    \includegraphics[width=0.3\linewidth]{ut-c-ptb-8.pdf}
  }
    \hspace{-0.5em}
  \subfigure{
    \label{subfig-counter-ut-ptb15}
    \includegraphics[width=0.3\linewidth]{ut-c-ptb-10.pdf}
  }
  \vspace{-1em}
  \captionsetup{font=footnotesize}
  \caption{{\bf Type-2 EPLA}: the worst-case utility function with $\delta_1=0.1$ }
  \vspace{-0.2cm}
  \label{fig-ptb-ut-counter} %% label for entire figure
\end{figure}



\begin{figure}[!ht]
% \vspace{-0.5cm}
  \centering
  \subfigure[]{
    \label{subfig-main-ov} %% label for second subfigure
    \includegraphics[width=0.23\linewidth]{ptb-main-ov.pdf}
  }
  \hspace{-1.5em}
  \subfigure[]{
    \label{subfig-counter-ov} %% label for second subfigure
    \includegraphics[width=0.23\linewidth]{ptb-counter-ov.pdf}
  }
  \hspace{-1.5em}
  \subfigure[]{
    \label{subfig-SAA-main}
    \includegraphics[width=0.23\linewidth]{SAA-m.pdf}
  }
  \hspace{-1.5em}
  \subfigure[]{
    \label{subfig-SAA-counter}
    \includegraphics[width=0.23\linewidth]{SAA-c.pdf}
  }
  \vspace{-1.2em}
  \captionsetup{font=footnotesize}
  \caption{{\bf EPLA:} the optimal values with $\delta_1=0.01$ and SAA problem as sample size increases}
  \vspace{-0.2cm}
  \label{fig-ptb-ov}
\end{figure}



\textbf{(ii) SAA 
%Sample average approximation 
of exogenous uncertainty. }
In this set of experiments, we 
% consider using
use
sample data to approximate the true probability distribution $P$ (of $\bdxi$), which is also known as  SAA.
%sample average approximation. 
We include this in the category of data perturbation in the sense that empirical distribution constructed with sample data may be regarded as a perturbation of $P$. 
We investigate how the variation of sample size affects the optimal values and the optimal solutions.
We solve problem (\ref{eq:PRO-N-reformulate}) with different sample size $K$ and run $20$ simulations for each fixed sample size $K$. 
We plot a boxplot diagram 
to examine the convergence of the optimal values as %sample sizes 
$K$ increases in Figures~\ref{subfig-SAA-main}-\ref{subfig-SAA-counter}.
We can see that as the sample size reaches 400, the optimal values of the SAA problem are close to the true optimal value in both Type-1 PLA and Type-2 PLA.
%with the main and the counter diagonal.



\subsection{Preference inconsistency}
\label{subsec:preference-incon}

In Section~\ref{sec:PC-design}, we consider pairwise comparisons to elicit the DM’s preference.
In practice, various errors may occur 
%in 
during
the elicitation process such as measurement errors
%data perturbation 
and DM’s wrong responses, 
all of which may lead to preference inconsistency. 
In this part, we examine the effects of the inconsistencies on 
the worst-case utility functions
and the optimal value  
in the following
%Below we consider
two types of inconsistency 
%in 
during the preference elicitation process.

\textbf{(i) Limitation on the total quantity of errors. }
We consider 
%the moment 
the rhs of the inequality constraints in the definition of
${\cal U}_N$
%constraints 
to be perturbed by positive constants $\gamma_l$, that is, $\la u_N,\psi_l \ra\leq c_l+\gamma_l, l=1,\ldots,M$.
% The perturbation enables ${\cal U}_N$
% to accommodate potential inconsistent 
% preferences
% {\color{red} 
% The perturbation 
% allows 
% $u_N$ to be feasible 
% when $\la u_N,\psi_l \ra\geq c_l, l=1,\ldots,M$.
% }
% {\color{blue}
The perturbation is required for the feasibility of problem (\ref{eq:PRO-N-reformulate}) to hold
when noise  corrupts
the expected utility evaluation when
a comparison is made. 
In other words, the perturbed inequalities accommodate potentially inconsistent responses.
We restrict the total inconsistency by setting $\sum_{l=1}^M \gamma_l\leq \Gamma$,
where $\Gamma$ is the total error to be tolerated.
Figures~\ref{fig-incon-ut-main}-\ref{fig-incon-ut-counter} depict the worst-case utility functions 
% and the corresponding gap of isopreference curves 
% between the worst-case utility functions 
and the true utility function.
Figures~\ref{subfig-incon-main}-\ref{subfig-incon-counter} depict the optimal values with $\Gamma$ varies from $0$ to $1$.
As $\Gamma$ increases, the optimal values decrease.
From the figures, we %can see
find that our PLA approach works very well for this type of inconsistency.
%case.

% [\scriptsize{$5\times5$}]
\begin{figure}[!ht]
  \centering
  \vspace{-0.5cm}
  \subfigure{
    \label{subfig-incon-main-ut1}
    \includegraphics[width=0.3\linewidth]{ut-m-tte-5.pdf}
  }
  \hspace{-0.5em}
  \subfigure{
    \label{subfig-incon-main-ut2}
    \includegraphics[width=0.3\linewidth]{ut-m-tte-10.pdf}
  }
  \hspace{-0.5em}
  \subfigure{
    \label{subfig-incon-main-ut3}
    \includegraphics[width=0.3\linewidth]{ut-m-tte-15.pdf}
  }
   \vspace{-1em}
  \captionsetup{font=footnotesize}
  \caption{{\bf Type-1 EPLA}: worst-case utility with $\Gamma=0.5$}
   \vspace{-0.2cm}
  \label{fig-incon-ut-main} %% label for entire figure
\end{figure}


% \begin{figure}[!htbp]
%   \centering
%   \subfigure[\scriptsize{$\epsilon=0.1$}]{
%     \label{subfig-incon-main-c1}
%     \includegraphics[width=1.96in]{ct-m-tte-5.pdf}
%   }
%   \subfigure[\scriptsize{$\epsilon=0.2$}]{
%     \label{subfig-incon-main-c2}
%     \includegraphics[width=1.96in]{ct-m-tte-10.pdf}
%   }
%   \subfigure[\scriptsize{$\epsilon=0.3$}]{
%     \label{subfig-incon-main-c3}
%     \includegraphics[width=1.96in]{ct-m-tte-15.pdf}
%   }
%   \vspace{-1em}
%   \captionsetup{font=footnotesize}
%   \caption{The isoprefernce curves with $\Gamma=0.5$ and main diagonal}
%   \label{fig-incon-contour-main} %% label for entire figure
% \end{figure}

% [\scriptsize{$5\times5$}]
\begin{figure}[!ht]
 % \vspace{-0.2cm}
  \centering
  \vspace{-0.5em}
  \subfigure{
    \label{subfig-incon-counter-ut1}
    \includegraphics[width=0.3\linewidth]{ut-c-tte-5.pdf}
  }
  \hspace{-0.5em}
  \subfigure{
    \label{subfig-incon-counter-ut2}
    \includegraphics[width=0.3\linewidth]{ut-c-tte-10.pdf}
  }
  \hspace{-0.5em}
  \subfigure{
    \label{subfig-incon-counter-ut3}
    \includegraphics[width=0.3\linewidth]{ut-c-tte-15.pdf}
  }
   \vspace{-1em}
  \captionsetup{font=footnotesize}
  \caption{{\bf Type-2 EPLA}: worst-case utility with $\Gamma=0.5$}
   \vspace{-0.2cm}
  \label{fig-incon-ut-counter} %% label for entire figure
\end{figure}




\begin{figure}[!ht]
 % \vspace{-0.5cm}
  \centering
  \vspace{-0.5em}
  \subfigure[]{
    \label{subfig-incon-main}
    \includegraphics[width=0.23\linewidth]{ov-tte-m.pdf}
  }
  \hspace{-1.5em}
  \subfigure[]{
    \label{subfig-incon-counter}
    \includegraphics[width=0.23\linewidth]{ov-tte-c.pdf}
  }
  \hspace{-1.5em}
  \subfigure[]{
    \label{subfig-responserr-main}
    \includegraphics[width=0.23\linewidth]{ov-resp-m.pdf}
  }
  \hspace{-1.5em}
  \subfigure[]{
    \label{subfig-responserr-counter}
    \includegraphics[width=0.23\linewidth]{ov-resp-c.pdf}
  }
  \vspace{-1em}
  \captionsetup{font=footnotesize}
  \caption{{\bf EPLA}: the optimal values with total errors and erroneous responses}
   \vspace{-0.5cm}
  \label{fig-incon-ov} 
\end{figure}



\textbf{(ii) Limitation on the number of erroneous responses. }
We consider the case that the DM makes mistakes occasionally, that is, 
the DM is mistaken at most $\epsilon M$ of %his/her 
lottery comparisons.
%In this case, 
We introduce binary variable
%s
$\delta_l$,
%$l=1,\cdots,M$
%for the problem, 
which takes
%a value 
value $1$ if the DM is mistaken about lottery $l$ and $0$ otherwise, and we add the constraint $\sum_{l=1}^M \delta_l\leq \epsilon M$ to limit the total number of mistakes.
If the original comparison is $\bbe_{\mathbb{P}}[u(\bdcz_1^l(\omega))]\geq \bbe_{\mathbb{P}}[u(\bdcz_2^l(\omega))]$, then this condition is replaced by:
%the following two constraints:
\begin{equation*}
    \delta_l \hat{M}+\bbe_{\mathbb{P}}[u(\bdcz_1^l(\omega))]\geq \bbe_{\mathbb{P}}[u(\bdcz_2^l(\omega))] 
    \quad \inmat{and} \quad 
    (1-\delta_l) \hat{M}+\bbe_{\mathbb{P}}[u(\bdcz_2^l(\omega))]\geq \bbe_{\mathbb{P}}[u(\bdcz_1^l(\omega))],
\end{equation*}
where $\hat{M}$ is a large constant (``Big $\hat{M}$'').
These constraints make the inner minimization problem become an MILP.
%problem.
% Differing from $\sum_{l=1}^M \gamma_l\leq \Gamma$, we use $\sum_{l=1}^M \delta_l$ to count the number of inconsistent responses.
Figures~\ref{fig-responserr-ut-main}-\ref{fig-responserr-ut-counter} depict the worst-case utility functions, and the gap between 
them and 
%the worst-case utility functions and 
the true utility function for Type-1 PLA and Type-2 PLA.
%with the main and counter diagonal lines.
Figures~\ref{subfig-responserr-main}-\ref{subfig-responserr-counter} depict the optimal values with $\epsilon=\{0.1,0.2,0.3\}$.


% [\scriptsize{$\epsilon=0.1$}]
\begin{figure}[!ht]
 \vspace{-0.2cm}
  \centering
  \subfigure{
    \label{subfig-responserr-main-ut1}
    \includegraphics[width=0.3\linewidth]{ut-m-resp-10-01.pdf}
  }
  \hspace{-0.5em}
  \subfigure{
    \label{subfig-responserr-main-ut2}
    \includegraphics[width=0.3\linewidth]{ut-m-resp-10-02.pdf}
  }
  \hspace{-0.5em}
  \subfigure{
    \label{subfig-responserr-main-ut3}
    \includegraphics[width=0.3\linewidth]{ut-m-resp-10-03.pdf}
  }
  \vspace{-1em}
  \captionsetup{font=footnotesize}
  \caption{{\bf Type-1 EPLA}: worst-case utility with $10\times10$ lotteries}
  \vspace{-0.2cm}
  \label{fig-responserr-ut-main}
\end{figure}
% [\scriptsize{$\epsilon=0.1$}]


\begin{figure}[!ht]
 \vspace{-0.2cm}
  \centering
  \subfigure{
    \label{subfig-responserr-counter-ut1}
    \includegraphics[width=0.3\linewidth]{ut-c-resp-10-01.pdf}
  }
  \hspace{-0.5em}
  \subfigure{
    \label{subfig-responserr-counter-ut2}
    \includegraphics[width=0.3\linewidth]{ut-c-resp-10-02.pdf}
  }
  \hspace{-0.5em}
  \subfigure{
    \label{subfig-responserr-counter-ut3}
    \includegraphics[width=0.3\linewidth]{ut-c-resp-10-03.pdf}
  }
  \vspace{-1em}
  \captionsetup{font=footnotesize}
  \caption{{\bf Type-2 EPLA:} worst-case utility with $10\times10$ lotteries}
   \vspace{-0.2cm}
  \label{fig-responserr-ut-counter} 
\end{figure}




\section{Concluding remarks}
\label{sec:Concluding remarks}


% {\color{purple}
In this paper, we propose EPLA and IPLA approaches to approximate the true unknown utility function
%study a 
in the multi-attribute UPRO models and 
demonstrate how the resulting approximate UPRO model %may 
can 
be solved.
The EPLA approach works only for two-attribute case as it stands because it is complex to derive
an explicit piecewise linear utility function when the utility function has three or more variables. The IPLA is not subject to the limitation of the dimension of the utility function but our numerical test results show that the IPLA-based approach takes considerably longer 
% computational time 
CPU time
to solve
as the numbers of preference elicitation questions and scenarios of exogenous random vector increase. 
This indicates that the formulation is potentially computationally unscalable. It remains an open question as to how to improve the computational efficiency of the IPLA approach. 
%Moreover,
For instance, 
in the 
%multi-attribute 
case when $m\geq 4$,
in order to derive IPLA of the utility function, 
%with a system of equalities and inequalities of continuous variables and binary variables,
we need to 
%clarify 
develop proper triangulation of the hypercube $\bigtimes_{i=1}^{m}[a_{i},b_{i}]$ 
into simplices in $m$-dimensional space.
It will be interesting
to explore such triangulation and
to identify the simplex where the reward function locates efficiently, %{\color{red}Please double check, ``target function" vs ``reward function", and ``locates" vs ``locate"}
see Hughes and Anderson \cite{hughes1996simplexity} and \cite{COTTLE198225,BROADIE198439} for further study.
Design of questionnaires
to elicit the DM's preference
is another point for potential improvement since our strategy is 
fundamentally based 
on
random  utility split 
% (RRUS) 
scheme in single-attribute PRO models \cite{AmD15}. It 
%remains to be explored 
is worthwhile to explore 
%as to whether the 
some optimal design strategies
such as in \cite{Vayanos2020}
because in practice, elicitation 
may be time consuming or costly.
Finally,
%In particular, 
it will be interesting to explore whether the proposed approaches work more efficiently when the true utility function has some copula structure \cite{abbas2009multiattribute,abbas2013utility}. 
We leave all these for future research. 

% {\color{red}ADD the case that $m\geq 4$.
% }

% {\color{blue}
% For the multi-attribute case that }


% {\color{blue}
% For the multi-attribute case that $m\geq 4$,
% in order to represent the IPLA of the utility function by a system of equalities and inequalities of continuous variables and binary variables,
% we need to clarify the triangulation of the hypercube $\bigtimes_{i=1}^{m}[a_{i},b_{i}]$ into simplices in $m$-dimensional space.
% It will be interesting
% to explore such triangulation and
% to identify the simplex where the target function locate, {\color{red}Please double check, ``target function" vs ``reward function", and ``locates" vs ``locate"}
% see Hughes and Anderson \cite{hughes1996simplexity} and \cite{COTTLE198225,BROADIE198439} for further study.
% }
%Another important issue to be investigated is 
%Moreover, the computational schemes are built on UPRO model with moment-type 


% {\color{purple}
% WE NEED TO ADD MORE COMMENTS
% }


% {\color{red}
% We can also use the idea from Vayanos~et~al.~\cite{VMYDR20}, which elicits preference by either
% maximizing worst-case utility 
% or minimizing worst-case regret.
% We can incorporate the former optimal strategy to our random relative utility split (RRUS)  scheme to select a group of questions  
% by solving the following problem
% \begin{equation}
% \max_{{\bm \Psi}} \max_{{\bdz}\in Z} \min_{u_N\in {\cal U}_N({\bm \Psi})} \bbe[u_N({\bm f}(\bdz,\bdxi))],
% \label{eq:selct-questions}
% \end{equation}
% where ${\mathcal U}_N({\bm \Psi}):=\{u_N\in {\mathscr U}_N: \langle {\bm \Psi},u_N\rangle\leq 0\}$,
% % Take the pairwise comparison approach for example,
% ${\bm \Psi}=(\psi_1,\cdots,\psi_{m})$,
% %with $m$ questionnaires,
% $\psi_l(x,y)= \1_{[x^l,1]\times [y^l,1]}(x,y)-(1-p^l) \1_{[0,1]\times[0,1]\setminus (1,1)}(x,y)- \1_{(1,1)}(x,y), l=1,\ldots,m$.
% % $
% % \psi_l(t):=(1-p^l)\mathbbm{1}_{[r_1^l,0.5]}(t)+p^l\mathbbm{1}_{[r_3^l,0.5]}(t)
% % -\mathbbm{1}_{[r_2^l,0.5]}(t), \; l=1,\cdots,m.
% % $
% The optimal selection in \cref{eq:selct-questions} means 
% that we can 
% choose  $m$ queries in 
% such a way 
% that the corresponding worst-case expected utility is maximized. We call the approach 
% {\em optimized RRUS under risk-averse}
% since it is a combination 
% of RRUS and the former optimal selection strategy.
% Moreover,
% we can incorporate the latter optimal strategy to our RRUS scheme to select questions in terms  of minimizing the worst-case regret
% \begin{equation}
% \min_{{\bm \Psi}} \max_{{\bdz}\in Z} \min_{u_N\in {\cal U}_N({\bm \Psi})} \left\{ \max_{\bdz'\in Z} \bbe[u_N({\bm f}(\bdz',\bdxi))]-\bbe[u_N({\bm f}(\bdz,\bdxi))] \right\}.
% \label{eq:selct-questionnaires}
% \end{equation} 
% We call the approach {\em optimized RRUS under regret} since it combines the RRUS scheme with the latter selection strategy.
% }






% we consider the bi-attribute utility preference robust optimization model over the moment-type based ambiguity set when the DM is multivariate risk-averse. 
% To efficiently calculate the expected bi-attribute utility of a random profit, we reformulate it as a moment-type integration where the considered bi-attribute utility can induce a Lebesgue-Stieltjes measure. 
% To solve the inner minimization problem of PRO mode, we use a bivariate piecewise linear function to approximate the utility function and consequently,
% the problem can be reformulated as an LP. %linear programming. 

%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}


% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 

% \section*{Conflict of interest}

% The authors declare that they have no conflict of interest.


% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{siamplain}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{2803-arxiv-main}  % name your BibTeX data base

% Non-BibTeX users please use
% \begin{thebibliography}{}
% %
% % and use \bibitem to create references. Consult the Instructions
% % for authors for reference list style.
% %
% \bibitem{RefJ}
% % Format for Journal Reference
% Author, Article title, Journal, Volume, page numbers (year)
% % Format for books
% \bibitem{RefB}
% Author, Book title, page numbers. Publisher, place (year)
% % etc
% \end{thebibliography}

% % \bibliographystyle{plainnat}
%   \bibliographystyle{apalike}
% %  \bibliography{bibfile}
% %  \bibliographystyle{IEEEtran}
% % \bibliographystyle{informs2014}
% % \bibliography{IEEEexample}
% \bibliography{bibfile}
% % \section{Appendix}

\begin{appendices}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}


\setcounter{figure}{0}
\setcounter{table}{0}

\section{Proofs}



% \begin{eqnarray}
% && = \int_T [\psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},y)+\psi_l(x,y)]d u_N(x,y)\\ \notag
% && \quad +\int_X [\psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},\underline{y})+\psi_l(x,\underline{y})]d u_N(x,\underline{y})\\ 
% && \quad +\int_{Y} [\psi_l(\bar{x},\bar{y})-\psi_l(\underline{x},\bar{y})-\psi_l(\bar{x},y)+\psi_l(\underline{x},y)]d u_N(\underline{x},y) 
%  \nonumber \\
%  &&=
%  \int_T \hat{\psi}_m(x,y) d u_N(x,y)+ \int_{X}\psi_{1,m}(x)d u_N(x,\underline{y}) +\int_{Y} \psi_{2,m}(y)d(\underline{x},y).
% \end{eqnarray}
% where




\subsection{Proof of Proposition~\texorpdfstring{\ref{prop-uti-N}}{3.1}}
\label{app:proof-uN}

Since $\psi_l$, $l=1,\cdots,M$, take constant values over $T_{i,j}$ for 
$i=1,\cdots, N_1-1$ and
$j=1,\cdots, N_2-1$,
%is a step function with jump points 
% $(x_i,y_j)$,
there exist constants $c_{i,j}^l$ such that 
$$
\psi_l(x,y):=\sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} c^l_{i,j} \1_{T_{i,j}}(x,y).
$$
% where $T_{1,1}:=[x_1,x_2]\times[y_1,y_2]$ and $T_{i,j}:=(x_{i},x_{i+1}]\times(y_j,y_{j+1}]$ for $i=2,\ldots,N_1, j=2,\ldots,N_2$.
%We next check the inequality 
Next, we verify that $u_N(x,y)$
 satisfies the following inequalities: 
 \begin{equation*}
    \int_{T} u_N(x,y)d\psi_l(x,y)\leq c_l, l=1,\ldots,M.
\end{equation*}
By integration in parts (see, e.g., \cite{young1917multiple} and \cite{Ans22}),
\begin{equation*}
\label{eq:parts_integral}
\begin{split}
&\int_T u_N(x,y)d\psi_l(x,y) \\ 
& = u_N(\underline{x},\underline{y})[\psi_l]_{\underline{x},\underline{y}}^{\bar{x},\bar{y}}
+\int_T [\psi_l]_{x,y}^{\bar{x},\bar{y}} d u_N(x,y)
+ \int_X [\psi_l]_{x,\underline{y}}^{\bar{x},\bar{y}}d u_N(x,\underline{y})
+\int_{Y} [\psi_l]_{\underline{x},y}^{\bar{x},\bar{y}} d u_N(\underline{x},y).
\end{split}
\end{equation*}
Since $u_N(\underline{x},\underline{y})=0$, it suffices to calculate the rest three terms at the right hand side of the equation.
Let $\hat{\psi}_l(x,y) :=[\psi_l]_{x,y}^{\bar{x},\bar{y}}$. 
Then by definition (see \cite{young1917multiple} and \cite{Ans22}) 
\begin{equation*}
\begin{split}
     \hat{\psi}_l(x,y)
  &= \psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},y)+\psi_l(x,y) \\
  &= \sum_{i=1}^{N_1-2}\sum_{j=1}^{N_2-2} c^l_{i,j} \1_{T_{i,j}}(x,y) -
    %X_i\times Y_j}(x,y) -
    c^l_{N_1-1,N_2-1}\1_{T_{N_1-1,N_2-1}}(x,y).
\end{split}
\end{equation*}
Likewise, we have
\begin{equation*}
\begin{split}
     \psi_{1,l}(x)
  &: = [\psi_l]_{x,\underline{y}}^{\bar{x},\bar{y}} =
  \psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},\underline{y})+\psi_l(x,\underline{y}) \\
  &= \sum_{i=1}^{N_1-1} (c^l_{i,1}-c^l_{i,N_2-1}) \1_{X_i}(x) - c^l_{N_1-1,1}
\end{split}
\end{equation*}
and
\begin{equation*}
\begin{split}
     \psi_{2,l}(y)
  &:= [\psi_l]_{\underline{x},y}^{\bar{x},\bar{y}} =\psi_l(\bar{x},\bar{y})-\psi_l(\underline{x},\bar{y})-\psi_l(\bar{x},y)+\psi_l(\underline{x},y) \\
  &= \sum_{j=1}^{N_2-1} (c^l_{1,j}-c^l_{N_1-1,j}) \1_{Y_i}(y) - c^l_{1,N_2-1}.
\end{split}
\end{equation*}
% By the equivalent representation of the condition (\ref{eq-equipres}) and $u_N(\underline{x},\underline{y})=0$, the above inequality can be reformulate as 
% \begin{equation*}
%     \int_T \hat{\psi}_l(x,y) d u_N(x,y)+ \int_{X}\psi_{1,l}(x) d u_N(x,\underline{y}) +\int_{Y} \psi_{2,l}(y) d u_N(\underline{x},y) \leq c_l, l=1,\ldots,M,
% \end{equation*}
% where 
% \begin{eqnarray*}
%     \hat{\psi}_l(x,y) &=& \psi_l(x,y)+\psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},y) \\ 
%     & =&  \sum_{i=1}^{N_1-2}\sum_{j=1}^{N_2-2} c^l_{i,j} \1_{T_{i,j}}(x,y) -
%     %X_i\times Y_j}(x,y) -
%     c^l_{N_1-1,N_2-1}\1_{T_{N_1-1,N_2-1}}(x,y), \\
% %    X_{N_1-1}\times Y_{N_2-1}}(x,y), \\
%     \psi_{1,l}(x,y) &=& 
%     \psi_l(x,\underline{y})+\psi_l(\bar{x},\bar{y})-\psi_l(x,\bar{y})-\psi_l(\bar{x},\underline{y}) \\ 
%     & =& \sum_{i=1}^{N_1-1} (c^l_{i,1}-c^l_{i,N_2-1}) \1_{X_i}(x) - c^l_{N_1-1,1}, \\
%     \psi_{2,l}(x,y) &=& \psi_l(\underline{x},y)+\psi_l(\bar{x},\bar{y})-\psi_l(\underline{x},\bar{y})-\psi_l(\bar{x},y) \\ 
%     & =& \sum_{j=1}^{N_2-1} (c^l_{1,j}-c^l_{N_1-1,j}) \1_{Y_i}(y) - c^l_{1,N_2-1}.
% \end{eqnarray*}
Consequently, we have
\begin{eqnarray}
\label{eq:Int-by-part-1}
     && \int_T [\psi_l]_{x,y}^{\bar{x},\bar{y}}  d u_{N}(x,y) = \int_T \hat{\psi}_l(x,y) d u_{N}(x,y) \nonumber \\
    &&=  \sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \int_{T_{i,j}} \lt( \sum_{i=1}^{N_1-2}\sum_{j=1}^{N_2-2} c^l_{i,j} \1_{T_{i,j}}(x,y) - c^l_{N_1-1,N_2-1}\1_{T_{N_1-1,N_2-1}}(x,y) \rt) d u_{N}(x,y) \nonumber \\
    && = \sum_{i=1}^{N_1-2}\sum_{j=1}^{N_2-2} \int_{T_{i,j}}  c^l_{i,j} d u_{N}(x,y) - \int_{T_{N_1-1,N_2-1}} c^l_{N_1-1,N_2-1} d u_{N}(x,y) \nonumber \\
    % && =  \sum_{i=1}^{N_1-2}\sum_{j=1}^{N_2-2} c^l_{i,j} \mu_N(T_{i,j})+c^l_{N_1-1,N_2-1} \mu_N(T_{N_1-1,N_2-1}) \nonumber \\
    && = \sum_{i=1}^{N_1-2}\sum_{j=1}^{N_2-2} c^l_{i,j} 
    \lt( u_{N}(x_{i+1},y_{j+1})-u_{N}(x_i,y_{j+1})-u_{N}(x_{i+1},y_j)+u_{N}(x_i,y_j) \rt) \nonumber \\
    && \quad+ c^l_{N_1-1,N_2-1}\lt( u_{N}(x_{N_1},y_{N_2})-u_{N}(x_{N_1-1},y_{N_2-1})-u_{N}(x_{N_1},y_{N_2-1})+u_{N}(x_{N_1-1},y_{N_2}) \rt) \nonumber \\
    && = \sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} 
    c^l_{i,j} \lt( u(x_{i+1},y_{j+1})-u(x_i,y_{j+1})-u(x_{i+1},y_j)+u(x_i,y_j) \rt) \nonumber \\
    && \quad+ c^l_{N_1-1,N_2-1}\lt( u(x_{N_1},y_{N_2})-u(x_{N_1-1},y_{N_2-1})-u(x_{N_1},y_{N_2-1})+u(x_{N_1-1},y_{N_2}) \rt) \nonumber \\
    && = \int_{\underline{x},\underline{y}}^{\bar{x},\bar{y}} \psi_l(x,y) d u(x,y),
\end{eqnarray}
where the third equality 
follows from the definition of %LS 
Lebesgue-Stieltjes
integration given 
that $u_N$ is non-decreasing and 
bounded (see \cite{Mcs47,ash2000probability}).
Likewise, we can show that
%we can also obtain that 
\begin{eqnarray}
\label{eq:Int-by-part-2}
    \int_{X}\psi_{1,l}(x) d u_N(x,\underline{y}) = \int_{X}\psi_{1,l}(x) d u(x,\underline{y}) 
\end{eqnarray}
and 
\begin{eqnarray}
\label{eq:Int-by-part-3}
    \int_{Y} \psi_{2,l}(y) d u_N(\underline{x},y) =  \int_{Y} \psi_{2,l}(y) d u(\underline{x},y).
\end{eqnarray}
Combing (\ref{eq:Int-by-part-1})-(\ref{eq:Int-by-part-3}),
we obtain
\begin{equation}
\label{eq-int-u-u-N}
    \int_{T} u_N(x,y)d\psi_l(x,y) = \int_{T} u(x,y)d\psi_l(x,y) \leq c_l, l=1,\ldots,M.
\end{equation}
The proof is complete.
\hfill\Box
% \bgeq
%     {\cal U}_N
%     &=&\left\{u_N\in \mathscr{U}_N:
%     %\bbe_{\mathbb{P}}[u({\bm B}_l)]\leq \bbe_{\mathbb{P}}[u({\bm A}_l)],
%     \int_{T} u(x,y) d (F_{\bdcb_l}(x,y)-F_{\bdca_l}(x,y))\leq 0,\;l=1,\cdots,M\right\}. \\
% &=&\left\{u_N\in \mathscr{U}_N: \int_{\underline{x},\underline{y}}^{\bar{x},\bar{y}} \psi_l(x,y) d u_N(x,y)
%         + \int_{\underline{x}}^{\bar{x}} \psi_{1,m}(x)d u_N(x,\underline{y})  +\int_{\underline{y}}^{\bar{y}} \psi_{2,m} (y) d u_N(\underline{x},y) \leq 0,\;m=1,\cdot,M \right\}\\
% &=& \left\{u_N\in \mathscr{U}_N:\sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \psi_l(x_i,y_j)
% (u_{i+1j+1}-u_{i,j+1}-u_{i+1j}+u_{ij}) \right.\\
% &&  \qquad  \qquad 
% + \left. \sum_{i=1}^{N_1-1}\psi_{1,m}(x_i) (u_{i+1,1}-u_{i1})
% +\sum_{j=1}^{N_2-1} \psi_{2,m} (y_j) (u_{1,j+1}-u_{1,j}) \leq 0,\; m=1,\cdots,M\right\},
% \edeq

% \begin{proof}
%If

% \vspace{2cm}

%  Since $\psi_l$ takes a constant value over $T_{i,j}$,
% %is a simple function over $T$, 
% then its induced measure $\mu_{\psi_l}^*$\footnote{{\color{black}
% Let ${\cal C}:=\{(a,b]:-\infty \leq a\leq b< \infty\}\cup \{(a,\infty): -\infty \leq a <\infty\}$ and 
% ${\cal C}_2:=\{I_1\times I_2: I_1,I_2\in {\cal C}\}$.
% For $I_1=(a_1,a_2]$,
% $I_2=(b_1,b_2]$, 
% $-\infty<a_1,a_2,b_1,b_2<\infty$,
% define
% $\mu_{\psi_l}(I_1\times I_2):=\psi(a_2+,b_2+)-\psi(a_2+,b_1+)-\psi(a_1+,b_2+)+\psi(a_1+,b_1+)$,
% where $\psi_l(a+,b+):=\lim_{a'\downarrow a, b'\downarrow b} \psi_l(a',b')$.
% $\mu_{\psi}(I_1\times I_2)$ on unbounded sets in ${\cal C}_2$ is defined by the limiting procedure: 
% $\mu_{\psi_l}(I_1\times I_2)=\lim_{n\rightarrow \infty}\mu_{\psi_l}((I_1\times I_2)\cap J_n)$, 
% where $J_n=(-n,n]\times (-n,n]$.
% % When $\psi:[\underline{x},\bar{x}]\times [\underline{y},\bar{y}]$ is non-decreasing ans satisfies that $\psi(\bar{x},\bar{y})+\psi(\underline{x},\underline{y})\geq \psi(\bar{x},\underline{y})+\psi(\underline{x},\bar{y})$.
% The Lebesgue-Stieltjes measure $\mu_{\psi_l}^*$ induced by $\mu_{\psi_l}$  is defined by
% $\mu_{\psi_l}^*(A):=\inf\{\sum_{n=1}^{\infty} \mu_{\psi_l}(A_n): \{A_n\}_{n\geq 1}\subset {\cal C}_2, A\subset \cup_{n\geq 1} A_n\}$,
% % $\mu_{\psi}((\underline{x},\bar{x}]\times (\underline{y},\bar{y}]) = \psi(\bar{x},\bar{y})+\psi(\underline{x},\underline{y})- \psi(\bar{x},\underline{y})-\psi(\underline{x},\bar{y})$ 
% see \cite[Section~1.3.3]{AtL06}.}
% }
% is nonzero over the interior of $T_{i,j}$.
% {\color{blue}
% The Lebesgue-Stieltjes integral of $u_N$ w.r.t. $\psi_l$ is defined as 
% %\cite[Section 1.3.3, Remark 2.3.3]{AtL06}
% \begin{eqnarray}
% \label{eq:definition-LS-integral}
%     && \int_{\underline{x},\underline{y}}^{\bar{x},\bar{y}} u_N(x,y) d \psi_l(x,y) \nonumber \\
%     % &=&
%     % \int_{(\underline{x},\bar{x}]\times (\underline{y},\bar{y}]} u_N(x,y) d \mu_F(x,y) \label{defi:definition-LS-R2} \\
%     &=& \sup\left\{\sum_{i=1}^{k} \lambda_i \mu_{\psi_l}^*(B_i):
%     \begin{array}{ll}
%     & \displaystyle  \sum_{i=1}^{k} \lambda_i  \1_{B_i}(x,y) \leq u_N(x,y),\forall (x,y)\in \bigtimes_{i=1}^{2}(a_i,b_i], \lambda_i\geq 0,\\
%     & \displaystyle B_i\cap B_j= \emptyset, \mbox{ for } i\neq j, i,j=1,\cdots,k, \bigcup_{i=1}^{k} B_i =\bigtimes_{i=1}^{2}(a_i,b_i] 
% \end{array}
% \right\}. \nonumber
% \end{eqnarray} }
% % not equal to zero 
% %only in some vertices of $T_{i,j}$. 
% % This is directly from the definition of induced signed measure, see, e.g.,\citet{Ans22}. 
% % \[
% % \mu_{\psi_l}(B)=\lim_{\delta_1,\delta_2;\epsilon_1,\epsilon_2\downarrow 0} \psi_l(x_i-\delta_1,y_j-\epsilon_1)+\psi_l(x_i-\delta_1,y_j-\epsilon_1)\psi_l(x_i-\delta_1,y_j-\epsilon_1)\psi_l(x_i-\delta_1,y_j-\epsilon_1)
% % \]
% % for all $B=[x_i,x_{i+1}]\times [y_j,y_{j+1}]$ 
% % with $\underline{a}\leq x_i\leq x_{i+1}\leq \overline{a}$ 
% % and $y_0\leq y_j\leq y_{j+1}\leq \overline{b}$.  
% Consequently, for $l=1,\ldots,M$,
% \begin{equation}
% \label{eq-int-u-u-N}
% \begin{split}
%     \int_{T} u_N(x,y)d\psi_l(x,y)
%     &= \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} u_N(x_i,y_j) \mu_{\psi_l}^*(x_i,y_j)\\
%     &= \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} u(x_i,y_j) \mu_{\psi_l}^*(x_i,y_j)
%     =\int_{T} u(x,y)d\psi_l(x,y) \leq c_l,
% \end{split}
% \end{equation}
% where the first and third equality is because the definition of  Lebesgue-Stieltjes,
% the second equality is due to the fact that $u_N(x_i,y_j)=u(x_i,y_j)$.
% %The proof is complete.
% % All three equations lead to 
% % \begin{equation}
% % \label{eq-int-u-u-N}
% %     \int_{T} u_N(x,y)d\psi_l(x,y) = \int_{T} u(x,y)d\psi_l(x,y) \leq c_m, m=1,\ldots,M.
% % \end{equation}
% \hfill\Box
% \end{proof}



\subsection{Proof of Proposition~\texorpdfstring{\ref{prop-int-pl}}{3.2}.}
\label{app:proof-LS}
% \textbf{Proof of Proposition~\ref{prop-int-pl}.}
Since $F$ is a continuous piecewise linear function with two pieces, then
there are only two possibilities that $F$ satisfies the conservative condition (\ref{eq:consevative-condition}) or not.
Without loss of generality, we assume the conservative condition fails.
According to the discussions in 
\cite{Mcs47,ash2000probability},
$F$ generates a %Lebesgue-Stieltjes
LS
(outer) measure $\mu_F^*$ defined as
\begin{equation*}
    \mu_F^*((\underline{a}, \bar{a}]\times(\underline{b}, \bar{b}])= F(\bar{a},\bar{b})-F(\underline{a},\bar{b})-F(\bar{a},\underline{b})+F(\underline{a},\underline{b}).
\end{equation*}
By the definition of the
%Lebesgue-Stieltjes
LS
integration, \begin{equation*}
    \int_{\underline{a}, \underline{b}}^{\bar{a}, \bar{b}}\psi(x,y)d F(x,y) = \int_{(\underline{a}, \bar{a}]\times(\underline{b}, \bar{b}]}\psi(x,y)d \mu_F^*.
\end{equation*}
%{\color{green} $\mu_F$ has not been defined.}
Let $I$ and $II$ denote the triangle regions
in $[\underline{a}, \overline{a}]\times [\underline{b}, \overline{b}]$ above (including) and below (including) 
$AB$ respectively. Let $R=(a,a']\times(b,b']$ be a subset of $I$ or $II$.
Then
% Since $F$ is piecewise linear function, we use $I$ and $II$ to denote the two pieces of $F$.
% For any $R=(x_1,x_2]\times(y_1,y_2]$ with $\underline{a}<x_1\leq x_2\leq\bar{a}$ and $\underline{b}<y_1\leq y_2\leq\bar{b}$, 
% we have 
\[
F(a,b)+ F(a',b')=2F((a+a')/2,(b+b')/2)=F(a,b')+F(a',b),
\]
because of the linearity of $F$ over the $R$. This implies $\mu_F^*(R)=0$. 
%{\color{RoyalBlue}
Now we turn to discuss the measure 
%of
over the boundary of $I\cup II$ (
%which is 
denoted by $\partial (I\cup II)= ((\underline{a},\bar{a}]\times\bar{b})\cup(\bar{a}\times(\underline{b},\bar{b}])$).
For any small constant $\epsilon>0$,
$$
\mu_F^*((\underline{a},\bar{a}]\times\bar{b})\leq \mu_F^*((\underline{a},\bar{a}]\times(\bar{b}-\epsilon,\bar{b}])=F(\underline{a},\bar{b}-\epsilon)-F(\underline{a},\bar{b})-F(\bar{a},\bar{b}-\epsilon)+F(\bar{a},\bar{b}).
$$
By driving $\epsilon$ to zero, 
we obtain
$$
\mu_F^*((\underline{a},\bar{a}]\times\bar{b})\leq \lim_{\epsilon\to0} (F(\underline{a},\bar{b}-\epsilon)-F(\underline{a},\bar{b})-F(\bar{a},\bar{b}-\epsilon)+F(\bar{a},\bar{b}))=0,
$$
which implies $\mu_F^*((\underline{a},\bar{a}]\times\bar{b})=0$.
Likewise, we can also obtain $\mu_F^*(\bar{a}\times(\underline{b},\bar{b}])=0$ and hence $\mu_F^*(\partial (I\cup II))=0$.
%}
% By setting 
% $x_1=x_2=\bar{a}$ and $y_1=y_2=\bar{b}$, 
% then we can show that 
% $\mu_F(\partial (I\cup II))=0$ where $\partial$ denotes
% the boundary of a set.
%by letting $x_1=x_2=\bar{a}$ or $y_1=y_2=\bar{b}$.
Let $\{a_i\}$ and $\{b_i\}$ be two sequences of monotonically increasing numbers such that 
$R_i:=(a_i,a_{i+1}]\times(b_i,b_{i+1}]\subset \inmat{int\,} I$
and $\bigcup_i R_i = I$.
% Moreover, there exists a sequence of $R_i:=(a_i,a_{i+1}]\times(b_i,b_{i+1}]\subset \inmat{int\,} I, i=1,\ldots,N$ such that $\disp{\inmat{int\,} I\subset\cup_i^N R_i}$, where the equation holds since rational numbers are dense and countable.
By the property of outer measure, 
$$
\mu_F^*(\inmat{int\,} I)\leq 
\sum_i\mu_F^*(R_i)=0.
$$
This shows $\mu_F^*(\inmat{int\,} I)=0$.
Likewise, $\mu_F^*(\inmat{int\,} II)=0$.
Consequently,
we have $\mu_F^*(I\cup II)=\mu_F^*(I\cap II)$.
Next, let $t\in (\underline{a},\overline{a}]$ and consider the segment $L=(\underline{a},t]\times(\underline{b},y(t)]\cap(I\cap II)$,
we have
\begin{equation*}
    \mu_F^*(L) = \frac{t-\underline{a}}{\bar{a}-\underline{a}} \, \mu_F^*(I\cap II),
\end{equation*}
where $y(t)$ is the linear function representing 
$I\cap II$ (AB).
% By dividing $AB$ into segments $L_i= (x_{i-1},x_i]\times(y(x_{i-1}),y(x_i)]\cap(I\cap II), i=1,\ldots,N$ with $x_0=\underline{a}$ and $x_N=\bar{a}$,
% then
\begin{equation*}
\begin{split}
    \int_{[\underline{a},\overline{a}]\times [\underline{b}, \overline{b}]}\psi(x,y) & d F(x,y) = \int_{I\cap II} \psi(x,y(x)) d\mu_F^* 
    \\
    & = \frac{\mu_F^*(I\cap II)}{\bar{a}-\underline{a}} \lim_{t\to\bar{a}} \int_{\underline{a}}^t \psi(x,y(x)) dx
    = \frac{\mu_F^*(I\cap II)}{\overline{a}-\underline{a}}\int_{\underline{a}}^{\overline{a}} \psi(x,y(x))d x,
\end{split}
\end{equation*}
where the third equality holds since $\psi(x,y(x))$ is Riemann integrable.
\hfill \Box

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition~\texorpdfstring{\ref{prop:single-MILP}}{4.2}}
%{\color{blue}
By introducing dual variables,
we can write down the Lagrange function of the inner minimization problem (\ref{eq:PRO_MILP_eqi})
w.r.t.~${\bm u}$
\bgeq
&& L({\bm u},{\bm \lambda}^1,{\bm \lambda}^2,{\bm \eta}^1, {\bm \eta}^2, {\bm \tau}, \sigma,{\bm \zeta})\\
&& = \sum_{k=1}^K p_k \sum_{i=1}^{N_1} \sum_{j=1}^{N_2} \alpha_{i,j}^k u_{i,j} 
+\sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2} \lambda_{i,j}^1 (u_{i,j}-u_{i+1,j})
+\sum_{i=1}^{N_1}\sum_{j=1}^{N_2-1} \lambda_{i,j}^2 (u_{i,j}-u_{i,j+1})\\
&& \quad +\sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2} \eta_{i,j}^1 (u_{i+1,j}-u_{i,j}-L(x_{i+1}-x_i))
+\sum_{i=1}^{N_1}\sum_{j=1}^{N_2-1} \eta_{i,j}^2
(u_{i,j+1}-u_{i,j}-L(y_{j+1}-y_j))\\
&& \quad
+\sum_{i=1}^{N_1-1}\sum_{j=1}^{N_2-1} \tau_{i,j} 
(u_{i,j}+u_{i+1,j+1}-u_{i,j+1}-u_{i+1,j}) 
 +\sigma(1-u_{N_1,N_2})+\sum_{l=1}^M \zeta_l\sum_{i=1}^{N_1}\sum_{j=1}^{N_2} Q_{i,j}^l,
 %\mathbb{P}({\bm B}_l=(x_i,y_j))-\mathbb{P}({\bm A}_l=(x_i,y_j)),
\edeq
where ${\bm \lambda}^1\in \R^{(N_1-1)\times N_2}_+$,
${\bm \lambda}^2\in \R^{N_1\times (N_2-1)}_+$,
${\bm \eta}^1\in \R^{(N_1-1)\times N_2}_+$,
${\bm \eta}^2\in \R^{N_1\times (N_2-1)}_+$,
$\tau\in \R^{(N_1-1)\times (N_2-1)}_+$,
$\sigma\in \R$ and $\zeta\in \R^M_+$.
We can then derive 
%obtain 
the Lagrange dual formulation and merge it into the outer 
maximization problem to obtain  (\ref{eq:PRO_MILP_single}).
%of the inner LP of (\ref{eq:PRO_MILP_eqi}). %and  complete the proof.
\hfill
$\Box$
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition~\texorpdfstring{\ref{prop-d}}{5.1}.}
% \noindent
% \textbf{Proof of Proposition~\ref{prop-d}.}
Case (i). %If 
$\scrg=\scrg_K$. 
We have
% By the definition of 
% $u_N$, we have $u(x_i,y_j)=u_N(x_i,y_j)$ for $i=1,\ldots,N_1$, $j=1,\ldots,N_2$.
% Then
%then
\begin{align}
    & \dd_{\scrg_K} (u,u_{N}) \nonumber \\
    & = \sup_{g\in\scrg_K} \lt|\int_T g(x,y) d u(x,y) - \int_T g(x,y) d u_{N}(x,y)\rt| \nonumber \\
    & \leq \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \sup_{g\in\scrg_K} 
    \lt|
    \int_{T_{i,j}} g(x,y) d u(x,y) -  \int_{T_{i,j}} g(x,y) d u_{N}(x,y)
    \rt| \nonumber \\
    & \leq \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \sup_{g\in\scrg_K} 
    \lt|
    \int_{T_{i,j}} g(x,y) d u(x,y)
    - \int_{T_{i,j}} g(x_{i},y_{j}) d u(x,y) \rt. \nonumber \\
    &  \hspace{10em} \lt.
    + \int_{T_{i,j}} g(x_{i},y_{j}) d u(x,y) 
    - \int_{T_{i,j}} g(x,y) d u_{N}(x,y)
    \rt| \nonumber \\
    & \leq \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \sup_{g\in\scrg_K} 
    \lt( \lt|
    \int_{T_{i,j}} |g(x,y)-g(x_{i},y_{j})| 
    d u(x,y) \rt| \rt. \nonumber \\
    & \hspace{10em} \lt. + \lt | \int_{T_{i,j}} |g(x_{i},y_{j})-g(x,y)| d u_{N}(x,y) \rt|
    \rt) \nonumber \\
    & \leq (\beta_{N_1}^2+\beta_{N_2}^2)^{1/2} \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} 
    \lt(\lt|
    \int_{T_{i,j}} d u(x,y)\rt|
    +\lt|\int_{T_{i,j}} d u_{N}(x,y) \rt|
    \rt) \nonumber \\
    & = 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2} |1-u_N(\underline{x},\bar{y})-u_N(\underline{y},\bar{x})| \leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2}, \label{eq-u-u-N}
\end{align}
where the last equality holds due to that $u$ and $u_N$ satisfy 
the conservative conditions, which implies that 
$$
\int_{T_{i,j}} d u(x,y) = \int_{T_{i,j}} d u_N(x,y) 
 = u_{i+1,j+1}-u_{i+1,j}-u_{i,j+1}+u_{i,j}\leq 0.
$$
Then 
$$
\sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \lt| \int_{T_{i,j}} d u(x,y) \rt| = \sum_{i=1}^{N_1-1} \sum_{j=1}^{N_2-1} \lt| \int_{T_{i,j}} d u_N(x,y) \rt| 
=|1-u(\underline{x},\bar{y})-u(\underline{y},\bar{x})|.
$$

Case (ii). $\scrg=\scrg_I$.
We only consider Type 1 PLA. 
Similar arguments can be established 
for Type 2 PLA.
Let $(x,y)\in T_{i,j}$. Consider the 
case that $(x,y)$ lies below the main diagonal, i.e., 
$0\leq\frac{y-y_j}{x-x_i}\leq\frac{y_{j+1}-y_j}{x_{i+1}-x_i}$. Thus
% Sainan: the prove analyses two cases,
% one is that $0\leq\frac{y-y_j}{x-x_i}\leq\frac{y_{j+1}-y_j}{x_{i+1}-x_i}$, that is,
% $(x,y)$ locates at the lower triangle.
% The other is 
% that $\frac{y-y_j}{x-x_i}\geq\frac{y_{j+1}-y_j}{x_{i+1}-x_i}$,
% that is, $(x,y)$ locates at the upper triangle.
% The latter case is similar.
% }
\begin{eqnarray*}
    && |u_{N}(x,y)-u(x,y)| \\
    && = \lt| \lt( 1-\frac{x-x_i}{x_{i+1}-x_i} \rt) u_{i,j}
    +\lt( \frac{x-x_i}{x_{i+1}-x_i} -\frac{y-y_j}{y_{j+1}-y_j} \rt) u_{i+1,j}  +\frac{y-y_j}{y_{j+1}-y_j} u_{i+1,j+1}-u(x,y) \rt| \\
    &&  \leq \lt| \lt(1-\frac{x-x_i}{x_{i+1}-x_i}\rt) (u_{i,j}-u(x,y)) \rt|
    +\lt| \lt(\frac{x-x_i}{x_{i+1}-x_i}-\frac{y-y_j}{y_{j+1}-y_j}\rt) (u_{i+1,j}-u(x,y)) \rt| \\
    && %\hspace{24em} 
    \quad+\lt|\frac{y-y_j}{y_{j+1}-y_j} (u_{i+1,j+1}-u(x,y)) \rt|.
\end{eqnarray*}
% Notice that $u_N$ is constructed by the non-overlapping simplices with the main diagonals.
Since $u_{i,j}=u(x_i,y_j)$, by the Lipschitz continuity 
% and componentwise monotonicity 
of $u$, we have
\begin{equation*}
    |u_{i,j}-u(x,y)| = |u(x_i,y_j)-u(x,y)|\leq L(\beta_{N_1}+\beta_{N_2}).
\end{equation*}
Likewise, we can obtain
$|u_{i+1,j}-u(x,y)|\leq L(\beta_{N_1}+\beta_{N_2})$
and 
$|u_{i+1,j+1}-u(x,y)|\leq L(\beta_{N_1}+\beta_{N_2})$,
which give rise to
\begin{equation}
\label{eq-u-uN}
    |u_{N}(x,y)-u(x,y)|\leq L(\beta_{N_1}+\beta_{N_2}).
\end{equation}
We can obtain the same inequality when $(x,y)\in[x_{i-1},x_i]\times[y_{j-1},y_j]$ and
$\frac{y-y_j}{x-x_i}\geq\frac{y_{j+1}-y_j}{x_{i+1}-x_i}$. 
% and the case that $u_N$ is constructed by the non-overlapping simplices with the counter diagonals. 
%{\color{green}A bit confused, please double check.}
Summarizing the discussions above, we have 
\begin{eqnarray*}
    \dd_{\scrg_I} (u,u_{N}) &=& \sup_{g\in\scrg} \lt| \int_{\underline{x},\underline{y}}^{x,y} d u(x,y) - \int_{\underline{x},\underline{y}}^{x,y} d u_{N}(x,y) \rt| \\
    &=& \sup_{(x,y)\in T} 
    |u(x,y)-u(x,\underline{y})-u(\underline{x},y)-u_{N}(x,y)+u_{N}(x,\underline{y})+u_{N}(\underline{x},y)| \\
    &\leq& 2L\lt(\beta_{N_1}+\beta_{N_2} \rt), 
\end{eqnarray*}
% {\color{red}wei: it might be 3 if we employing the result before ``and". Precisely, it should be 2 because $u_N$ is also Lipschitz continuous with module $L$. }
% {\color{blue} QW: It is 2 because the others are conditional, one leads to $L\beta_{N_1}$, the other leads to $L\beta_{N_2}$.}
which implies (\ref{eq-d}).
% By the normalized conditions, we obtain
% $L\geq1/\lt((\bar{x}-\underline{x})+(\bar{y}-\underline{y})\rt)$.
The proof is complete.
\hfill \Box

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Theorem~\texorpdfstring{\ref{thm-erramb}}{5.1}.}
% \textbf{Proof of Theorem~\ref{thm-erramb}.}
Let $\hat{\alpha}<\alpha$ be a positive number.
Under %the
Slater's condition (\ref{eq-sla}), there exists a function $u^0_{N} \in\calu_{N}$ and a positive number $N^0=N_1^0\times N_2^0$ such that 
\begin{equation}
\label{eq-sla-0}
    \la u^0_{N},\bdps \ra -\bdc+\hat{\alpha} \mathbb{B}^M \subset \R_-^M
\end{equation}
for $N\geq N^0$.
The existence follows from Proposition~\ref{prop-d} in that there exists $u^0$ satisfying (\ref{eq-sla}), 
and by (\ref{eq-u-uN})
we can construct a piecewise linear utility function $u^0_{N}$ of $u^0$ such that $u^0_{N}\to u^0$  
under $\|\cdot\|_\infty$ uniformly
as $\beta_{N_i}\to0$, $i=1,2$. 
%{\color{red}wei: the convergence under which norm? 
%{\color{blue} HX: infinity norm. }
%because Not all norms are equivalent in an infinite-dimensional space. 
%Precisely, here we need $\la u^0_{N},\bdps \ra\rightarrow \la u^0,\bdps \ra$ as $\beta_{N_i}\to0$, $i=1,2$. 
%Consequently, $\hat{\alpha}<\alpha$ would be found. 
%}
By applying Lemma~\ref{lem-hof} to $\calu$ under %the 
Slater's condition (\ref{eq-sla-0}), for any $\tldu\in\scru_N$,
\begin{equation}
\label{eq-bbd}
    \mathbb{D}_{\scrg}(\tldu,\calu_N)\leq \frac{\dd_{\scrg}(\tldu,u^0_{N})}{\hat{\alpha}}
    \|(\la \tldu,\bdps \ra-\bdc)_+\|
\end{equation}
for all $N\geq N^0$. 
%{\color{red}wei: notation is not precise, here it might be better to use $N$ as the partition refinement of $N_0$.}
Let $u\in\calu$ and $u_{N}$ be defined as in Proposition~\ref{prop-uti-N}.
Then 
\begin{align}
    & \|
    \la u_{N},\bdps \ra 
    - \la u,\bdps \ra 
    \|^2 \nonumber \\
    & = \sum_{l=1}^M \lt| 
    \int_T u_{N}(x,y) d \psi_l(x,y)-\int_T u(x,y) d \psi_l(x,y)
    \rt|^2 \nonumber \\   
    & \leq \sum_{l=1}^M \lt| 
    \int_T |u_{N}(x,y)-u(x,y)|d\psi_l(x,y)
    \rt|^2 \nonumber\\
    & \leq
    L^2 (\beta_{N_1}+\beta_{N_2})^2
    \sum_{l=1}^M \lt| \int_T d \psi_l(t)\rt|^2.
    \label{eq-inpro} 
\end{align}
By the triangle inequality for the pseudo-metric and (\ref{eq-bbd}), we have 
\begin{align}
    \dd_{\scrg}(u,\calu_{N}) &\leq \dd_{\scrg}(u,u_{N})+\dd_{\scrg}(u_{N},\calu_{N}) \nonumber \\
    &\leq \dd_{\scrg}(u,u_{N})+\frac{\dd_{\scrg}(u_{N},u_{N}^0)}{\hat{\alpha}}
    \|(\la u_{N},\bdps \ra-\bdc)_+\| \nonumber \\
    &= \dd_{\scrg}(u,u_{N})+\frac{\dd_{\scrg}(u_{N},u_{N}^0)}{\hat{\alpha}}
    [\|(\la u_{N},\bdps \ra-\bdc)_+\|-\|(\la u,\bdps \ra-\bdc)_+\|] \nonumber \\
    &\leq \dd_{\scrg}(u,u_{N})+\frac{\dd_{\scrg}(u_{N},u_{N}^0)}{\hat{\alpha}}
    \|\la u_{N},\bdps \ra-\la u,\bdps \ra)\| \nonumber \\
    &\leq \dd_{\scrg}(u,u_{N})+\frac{\dd_{\scrg}(u_{N},u_{N}^0)}{\hat{\alpha}} 
    L(\beta_{N_1}+\beta_{N_2}) \lt(\sum_{l=1}^M \lt| \int_T d \psi_l(t)\rt|^2\rt)^{1/2},
    \label{eq-uU} 
\end{align}
where the equality holds due to $u\in\calu$, i.e. $(\la u,\bdps \ra-\bdc)_+=0$ and the last inequality comes from (\ref{eq-inpro}).
In what follows, we turn to estimate $\dd_{\scrg}(u,u_{N})$ and $\dd_{\scrg}(u_{N},u_{N}^0)$ when $\scrg$ have specific form.

Case (i). If $\scrg=\scrg_K$, then $\dd_{\scrg_K}(u_N,u^0_N)\leq \lt((\bar{x}-\underline{x})^2+(\bar{y}-\underline{y})^2\rt)^{1/2}$ and $\dd_{\scrg_K}(u,u_N)\leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2}$ 
by Proposition~\ref{prop-d}~(i).
% Together with  and 
Taking supremum w.r.t. $u$ over $\calu$ on both sides of (\ref{eq-uU}), we obtain 
\begin{equation*}
    \mathbb{D}_{\scrg_K}(\calu,\calu_N)\leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2} + 
    L(\beta_{N_1}+\beta_{N_2})
    \frac{\lt((\bar{x}-\underline{x})^2+(\bar{y}-\underline{y})^2\rt)^2}{\hat{\alpha}} 
    \lt(\sum_{l=1}^M \lt| \int_T d \psi_l(t)\rt|\rt)^{1/2}
\end{equation*}
and hence (\ref{eq-erram-L}) holds since $\mathbb{D}_{\scrg_K}(\calu_{N},\calu)=0$.

Case (ii). 
If $\scrg=\scrg_I$, then
$\dd_{\scrg_I}(u_N,u^0_N)\leq1$ and $\dd_{\scrg_I}(u,u_N)\leq 2L\lt( \beta_{N_1}+\beta_{N_2} \rt)$ by Proposition~\ref{prop-d}~(ii).
Following a similar analysis to Case (i), we obtain (\ref{eq-erram-I}).
\hfill $\Box$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Corollary~\texorpdfstring{
\ref{cor-err-optval-discrete}}{5.1}.}
% \textbf{Proof of Corollary~\ref{cor-err-optval-discrete}.}
%{\color{RoyalBlue} Q: this is why we assume $\psi_l$ is simple function. 
Since 
$\calu_{N}\subset\calu$ by definition, then
$
\mathbb{D}_{\scrg}(\calu_{N},\calu)
%=\sup_{u_{N}\in\calu_{N}}\inf_{u\in\calu} \dd_{\scrg}(u_{N},u)
=0
$
for any $\mathscr{G}$.
Thus, it suffices to estimate $\mathbb{D}_{\scrg}(\calu,\calu_{N})$.
For any $u\in \calu$, it follows by Proposition~\ref{prop-uti-N} that
we can construct $u_N$ of Type-1 PLA or Type-2 PLA such that $u_N\in \calu_N$. 
Consequently, 
in the case that $\scrg=\scrg_K$, we have
$$
\dd_{\scrg_K}(u,\calu_{N}) \leq \dd_{\scrg_K}(u,u_{N})+\dd_{\scrg_K}(u_{N},\calu_{N})
= \dd_{\scrg_K}(u,u_{N})\leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2},
$$
where the last inequality follows from Proposition~\ref{prop-d}~(i).
% Moreover, it follows from (\ref{eq-u-u-N}) that
% $$
% \dd_{\scrg_K}(u,u_{N})
% \leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2} (1-u(\underline{x},\bar{y})-u(\underline{y},\bar{x}))
% \leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2}.
% $$
% Combining the two inequalities, we obtain
% $
% \dd_{\scrg_K}(u,\calu_{N}) 
% \leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2} %(1-u(\underline{x},\bar{y})-u(\underline{y},\bar{x})).
% $
% and 
Hence
$$
\mathbb{H}_{\scrg_K} (\calu,\calu_{N})= \max\{0,
\mathbb{D}_{\scrg_K} (\calu,\calu_{N}) \}= \sup_{u\in\calu} \dd_{\scrg_K}(u,\calu_{N})\leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2}. 
$$
% Since $\psi_l$ takes a constant value of each cell $T_{i,j}$,
% %is a simple function 
% for $l=1,\ldots,M$, it follows
% by \cref{eq-int-u-u-N} that 
% for any $u_N\in\calu_N$, 
% %we can find a corresponding 
% %piecewise linear function 
% we can find a corresponding 
% %have 
% $u\in\scru$. 
% %such that $u_N\in\calu_N$.
% %by \cref{eq-int-u-u-N}, 
% %then we have 
% % $\scru_{N}\subset\scru$ and for any $u_{N}\in\calu_{N}$, 
% % $\la u_{N},\bar{\bdps}_m \ra \leq \bdc_m$,
% % for $m=1,\ldots,M$, which means $u_{N}\in\calu$ and
% This shows $\calu_{N}\subset\calu$.
% % }
% Thus
% $$
% \mathbb{D}_{\scrg}(\calu_{N},\calu)=\sup_{u_{N}\in\calu_{N}}\inf_{u\in\calu} \dd_{\scrg}(u_{N},u)=0.
% $$
% Moreover, by \cref{prop-uti-N}, $u_{N}\in\calu_{N}$.
% Thus $\dd_{\scrg}(u_{N},\calu_{N})=0$ for both $\scrg_I$ and $\scrg_K$.
% In the case that $\scrg=\scrg_K$,
% $$
% \dd_{\scrg_K}(u,\calu_{N}) \leq \dd_{\scrg_K}(u,u_{N})+\dd_{\scrg_K}(u_{N},\calu_{N})
% = \dd_{\scrg_K}(u,u_{N}) \leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2} (1-u(\underline{x},\bar{y})-u(\underline{y},\bar{x})),
% $$
% where the last inequality follows from (\ref{eq-u-u-N}).
% Since $u$ is chosen from $\calu$ arbitrarily, by taking supremum w.r.t. $u\in\calu$ on both side of the inequality above we obtain 
% $$
% \mathbb{D}_{\scrg_K} (\calu,\calu_{N}) = \sup_{u\in\calu} \dd_{\scrg_K}(u,\calu_{N})\leq 2(\beta_{N_1}^2+\beta_{N_2}^2)^{1/2}.
% $$
In the case that $\scrg=\scrg_I$, we have 
$$
\dd_{\scrg_I}(u,\calu_{N}) \leq \dd_{\scrg_I}(u,u_{N}) \leq 2L\lt( \beta_{N_1}+\beta_{N_2} \rt),
$$
where the last inequality follows from 
Proposition~\ref{prop-d}~(ii)
and hence 
$$
\mathbb{H}_{\scrg_I} (\calu,\calu_{N})=
\max\{0,\mathbb{D}_{\scrg_I} (\calu,\calu_{N})\}\leq 2L\lt( \beta_{N_1}+\beta_{N_2} \rt).$$
The proof is complete.
\hfill\Box



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\texorpdfstring{\ref{thm-optval}}{5.2}.}
% \textbf{Proof of Theorem~\ref{thm-optval}. }
Part (i). 
It is well-known that 
\begin{equation*}
    |\vt_{N}-\vt|\leq \max_{\bdz\in Z} \Big{|} \min_{u\in \calu_{N}} \bbe_P [u(\bdf(\bdz,\bdxi))] 
    - \min_{u\in \calu} \bbe_P [u(\bdf(\bdz,\bdxi))] \Big{|}. 
\end{equation*}
Let $\delta$ be a small positive number. 
For any $\bdz\in Z$, we can find 
a $\delta$-optimal solution
$u^{\bdz} \in \calu$ 
and $u^{\bdz}_N \in \calu_N$ such that
$$
\bbe_P [u^{\bdz} (\bdf(\bdz,\bdxi))] \leq \min_{u\in \calu} \bbe_P [u(\bdf(\bdz,\bdxi))]+\delta, \quad
\bbe_P [u^{\bdz}_{N} (\bdf(\bdz,\bdxi))] \geq \min_{u\in \calu_{N}} \bbe_P [u(\bdf(\bdz,\bdxi))].
$$
Combing the above inequalities, we have 
\begin{eqnarray}
    \min_{u\in \calu_{N}} \bbe_P [u(\bdf(\bdz,\bdxi))]-\min_{u\in \calu} \bbe_P [u(\bdf(\bdz,\bdxi))] &\leq& \bbe_P [u^{\bdz}_{N} (\bdf(\bdz,\bdxi))-u^{\bdz} (\bdf(\bdz,\bdxi))] +\delta \qquad \nonumber\\
    &\leq& \sup_{(x,y)\in T} |u^{\bdz}_{N}(x,y)-u^{\bdz}(x,y)| +\delta.
    \label{eq:thm2-proof}
    % &\leq& \mathbb{H}_{\scrg_I} (\calu_{N},\calu)+ L(\beta_{N_1}+\beta_{N_2}) +\delta.
\end{eqnarray}
On the other hand,  for any 
$u,v\in {\cal U}$
\begin{eqnarray*}
    && \sup_{(x,y)\in T} |u(x,y)-v(x,y)| \\
    && \leq \sup_{(x,y)\in T} ( |u(x,y)-v(x,y)-u(x,\underline{y})-u(\underline{x},y)+v(x,\underline{y})+v(\underline{x},y)| \\ 
    && \hspace{5em} +|v(x,\underline{y})+v(\underline{x},y)-u(x,\underline{y})-u(\underline{x},y)| ) \\
    && = \sup_{(x,y)\in T}  \lt( \lt| \int_{\underline{x},\underline{y}}^{x,y} d u(x,y)-\int_{\underline{x},\underline{y}}^{x,y} d v(x,y) \rt| + |v(x,\underline{y})+v(\underline{x},y)-u(x,\underline{y})-u(\underline{x},y)| \rt) \\
    && \leq \sup_{g\in\scrg_I} \lt| \int_T g(x,y) d u(x,y)-\int_T g(x,y) d v(t) \rt| + \sup_{(x,y)\in T} |v(x,\underline{y})+v(\underline{x},y)-u(x,\underline{y})-u(\underline{x},y)|.
\end{eqnarray*}
Since ${\cal U}_N\subset {\cal U}$, by setting $u=u^{\bdz}_{N}$ and $v=u^{\bdz}$, we have 
\begin{eqnarray}
\sup_{(x,y)\in T} |u^{\bdz}_{N}(x,y)-u^{\bdz}(x,y)| 
    &\leq& \dd_{\scrg_I} (u_{N}^{\bdz},u^{\bdz})
   + \sup_{(x,y)\in T} |u_{N}^{\bdz}(x,\underline{y})-u^{\bdz}(x,\underline{y})+u_{N}^{\bdz}(\underline{x},y)-u^{\bdz}(\underline{x},y)| \notag \\
     &\leq& 
    \mathbb{H}_{\scrg_I} (\calu_{N},\calu)+ L(\beta_{N_1}+\beta_{N_2}).
      \label{eq:thm2-proof-1}
\end{eqnarray}
Combining (\ref{eq:thm2-proof})-(\ref{eq:thm2-proof-1}), we obtain
%thus
% then
% \begin{eqnarray*}
%     \sup_{(x,y)\in T} |u^{\bdz}_{N}(x,y)-u^{\bdz}(x,y)| &\leq& \dd_{\scrg_I} (u_{N},u)
%   + \sup_{(x,y)\in T} |u_{N}(x,\underline{y})-u(x,\underline{y})+u_{N}(\underline{x},y)-u(\underline{x},y)| \\
%   &\leq& \dd_{\scrg_I} (u,u_{N})+L(\beta_{N_1}+\beta_{N_2}).
%   \end{eqnarray*}
\begin{eqnarray*}
    \min_{u\in \calu_{N}} \bbe_P [u(\bdf(\bdz,\bdxi))]-\min_{u\in \calu} \bbe_P [u(\bdf(\bdz,\bdxi))] 
    %&\leq& \bbe_P [u^{\bdz}_{N} (\bdf(\bdz,\bdxi))-u^{\bdz} (\bdf(\bdz,\bdxi))] +\delta \\
    % &\leq& \sup_{(x,y)\in T} |u^{\bdz}_{N}(x,y)-u^{\bdz}(x,y)| +\delta \\
    \leq \mathbb{H}_{\scrg_I} (\calu_{N},\calu)+ L(\beta_{N_1}+\beta_{N_2}) +\delta.
\end{eqnarray*}
By exchanging the position of $\calu$ and $\calu_{N}$, we can use the same argument to derive
$$
\min_{u\in \calu} \bbe_P [u(\bdf(\bdz,\bdxi))]-\min_{u\in \calu_{N}} \bbe_P [u(\bdf(\bdz,\bdxi))] \leq
\mathbb{H}_{\scrg_I} (\calu,\calu_{N})+ L(\beta_{N_1}+\beta_{N_2}) +\delta.
$$
Since $\delta>0$ can be arbitrarily small, we obtain 
\begin{equation}
\label{eq-dis-vt}
    |\vt_{N}-\vt| \leq \max_{\bdz\in Z} \lt| \min_{u\in \calu} \bbe_P [u(\bdf(\bdz,\bdxi))]-\min_{u\in \calu_{N}} \bbe_P [u(\bdf(\bdz,\bdxi))] \rt| \leq \mathbb{H}_{\scrg_I} (\calu,\calu_{N})+ L(\beta_{N_1}+\beta_{N_2})
\end{equation}
and hence (\ref{eq-err-vt}) follows from (\ref{eq-erram-I}).

Part(ii).
Observe that $\Lambda(\cdot)$ is a non-decreasing function,
thus its generalized inverse is well-defined.
For any $\bdz_{N}^*\in Z_{N}^*$ and $\bdz^*\in Z^*$, 
\begin{eqnarray*}
    \Lambda(d(\bdz_{N}^*,\bdz)) &\leq& v({\bdz_N^*})-\vt^* =v(\bdz_{N}^*)-v(\bdz^*)\leq |v(\bdz_{N}^*)-v_N(\bdz_{N}^*)| + |v(\bdz_{N}^*)-v(\bdz^*)| \\
    &\leq & 2\max_{\bdz\in Z} |v(\bdz)-v_N(\bdz)|.
\end{eqnarray*}
Combining the inequality above with (\ref{eq-err-vt}), we obtain
$$
d(\bdz_{N}^*,Z^*) \leq \Lambda^{-1} \lt( 2\max_{\bdz\in Z} |v(\bdz)-v_N(\bdz)| \rt) \leq \Lambda^{-1}(2 \mathbb{H}_{\scrg_I}(\calu_{N},\calu)+2L(\beta_{N_1}+\beta_{N_2}))
$$
and hence (\ref{eq-err-so}) follows.
\hfill $\Box$
\end{appendices}


% \bibliographystyle{plain}
% \bibliography{bibfile} 




\end{document}
