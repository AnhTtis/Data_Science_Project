
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{array}
\newcolumntype{?}{!{\vrule width 1pt}}
\newcolumntype{:}{!{\vrule width 0.02pt}}

\usepackage[textsize=scriptsize]{todonotes}
\setlength{\marginparwidth}{3cm} % for todo notes

%%-----------------------------------------------------------
%% Begin all commands
\newcommand{\gd}[1]{\todo[color=red!40]{{\bf Greg}: #1}}
\newcommand{\greg}[1]{\textcolor{red}{\textbf{GREG:} #1}}

\newcommand{\jiayi}[1]{\todo[color=blue!20]{{\bf Jiayi}: #1}}
\newcommand{\id}[1]{\todo[color=green!40]{{\bf Isil}: #1}}

\newcommand{\plan}[1]{\textcolor{blue}{\begin{enumerate}#1\end{enumerate}}}
\newcommand{\code}[1]{\texttt{\small #1}}
\newcommand{\toolname}{TypeT5}
\newcommand{\fixme}[1]{\textcolor{red}{#1}}
\newcommand{\tocite}[1]{(\textcolor{red}{cite: #1})}

\newcommand{\preamble}{\vs_{\mathrm{pre}}}
\newcommand{\usees}{\vs_{\mathrm{usee}}}
\newcommand{\users}{\vs_{\mathrm{user}}}

\newcommand{\na}{n/a}
\newcommand{\hh}[1]{\textbf{#1}}  % table header
\newcommand{\hhh}[1]{{#1}}  % table sub header
\newcommand{\best}[1]{\textbf{#1}}


%% End all commands
%%-----------------------------------------------------------


\title{TypeT5: Seq2seq Type Inference\\ using Static Analysis}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% Keywords: Type Inference, Transformers, Code Completion, Static Analysis, Few-shot learning


\author{Jiayi Wei, Greg Durrett, Isil Dillig\\
 %\thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
University of Texas at Austin\\
% Austin, TX 78712, USA \\
\texttt{\{jiayi,gdurrett, isil\}@cs.utexas.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
%As gradual typing becomes widely adopted in mainstream  languages like Python and JavaScript, 
There has been growing interest in automatically predicting missing type annotations in programs written in Python and JavaScript. While prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types. In this paper, we present a new type inference method that treats type prediction as a code infilling task by leveraging CodeT5, a state-of-the-art seq2seq pre-trained language model for code.
Our method uses static analysis to construct dynamic contexts for each code element whose type signature is to be predicted by the model.  
We also propose an iterative decoding scheme that incorporates previous type predictions in the model's input context, allowing information exchange between related code elements. Our evaluation shows that the proposed approach, TypeT5, not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type errors---while enabling easy user intervention.



%At the same time, by leveraging large-scale pretraining, seq2seq models have demonstrated strong performance for tasks like code infilling, and their flexibility allows them to predict novel code expressions with relative ease. 
%We present a new type inference method designed to leverage this strength, generating a typed version of untyped Python code with CodeT5. A key challenge is that type prediction requires reasoning beyond the local context that seq2seq models typically consider, drawing on information like classes defined elsewhere, callers and callees of a function, and so on. Our solution to address this involves applying static analysis techniques to gather all API elements relevant to a prediction, regardless of where they are located, and converting them into model inputs using an encoding scheme designed to best utilize the transformer model's limited receptive field. Furthermore, instead of decoding each type annotation independently, we also propose an iterative decoding scheme that allows the model to condition on previously predicted type signatures. Our evaluation shows that the proposed approach not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type constraint violations.
\end{abstract}

\section{Introduction}

% \plan{
%    \item Gradual typing becomes more popular and get added to dynamically typed programming languages, to reduce the human effort required to transition into fully typed codebases, there has been a growing interest in recent years in developing learning-based approaches to predict type annotations automatically.
%    \item Both parametric and user-defined types are widely used features that are essential for using the compiler effectively. Unfortunately, while some prior work can predict user-defined types using either pointer network or type embedding search, they are limited to predicting nonparametric types and performs very poorly on user-defined types. In this work, we explore supporting both features using a different approach by fine-tuning a Seq2Seq code generation model.
%    \item Compared to prior work, our approach has several advantages: 
%    \begin{enumerate}
%       \item Being a code generation model, CodeT5 is capable of predicting arbitrary code fragments, hence providing a natural solution to predict parametric and user-defined types. 
%       \item By fine-tuning CodeT5 on the type prediction task, we were able to transfer the prior knowledge obtained from pretraining on a large amount of open source code.
%       \item Instead of treating type inference as a classification problem and predicting each type independently, using a Seq2Seq model allows us to model this as a \emph{structured prediction} problem, producing type assignments that are both accurate and coherent.
%    \end{enumerate}
%    \item However, while code generation is usually performed at file level and can work well with a relatively small code context, type inference generally needs to reason about information across the file boundaries. To overcome the limited receptive field of transformer models, we propose to apply static analysis techniques to gather the most relevant context for each API element, regardless of where the elements are located.
%    \item Since distant program variables can be coupled together by the underlying type constraints, instead of predicting the types of each program element individually, we also propose a new decoding scheme that linearizes the project's call graph to allow information to be propagated across different elements, allowing the model to make more informed predictions by conditioning on the type signatures predicted from earlier time steps.
% }

%Dynamically\gd{We can leave it for now, but this whole first paragraph can be replaced with a single sentence. I think the problem is reasonably well-established in the literature} typed languages like Python and JavaScript have gained enormous popularity due to their ability to facilitate fast prototyping.  However, without a static type system to catch common errors and guide refactoring, the task of \emph{maintaining} programs written in dynamically typed language can be a challenge. 

In languages like Python and JavaScript, the lack of a static type system makes it harder to maintain and analyze codebases. 
To address this issue, \emph{gradual typing}~\citep{siek2007gradual} was proposed to  allow type annotations to be incrementally added to untyped codebases, thereby marrying the benefits of static typing with the convenience of easy prototyping. As a result, many mainstream programming languages, including Python and JavaScript, have already adopted this idea, 
and researchers have also developed learning-based techniques to predict missing type annotations~\citep{raychev2015jsnice,hellendoorn2018deep,Wei2020LambdaNet, pradel2020typewriter, allamanis2020typilus, pandi2020opttyper, jesse2021learning, mir2022type4py, jesse2022learning, peng22hityper}.


%Among these efforts, earlier approaches have modeled type prediction as a multiple-class classfition problem and showed promising results on predicting the most common types~\tocite{JsNice, TypeWriter, DeepTyper}. However, these approach are limited to predict from a fixed set of label types and hence lack the ability to predict newly defined types that have never appeared during training time. To address this limitation, some recent approaches have proposed to predict user-defined types using pointer networks~\tocite{LambdaNet} or type embedding search~\tocite{Typilus, TypeBert, Type4py}, essentially performing a form of ``zero-shot learning''. However, these approaches still have relatively poor performance and are constrainted to predict nonparametric types.\footnote{Some approaches, like Typilus and Type4py can predict from a fixed set of frequently used parametric types such as \code{List[int]}, but this only covers a small portion of all used parameteric types and doesn't work with newly defined types.} Hence, we are still in search for a proper way to overcome these limitations if we want to see learning-based approaches to be practically useful.


Meanwhile, with the advent of large-scale pretraining and the explosion of transformer architectures, seq2seq models have proven to be very effective for programming tasks like code comments generation~\citep{panthaplackel2020learning}, completion~\citep{wang2021codet5, ahmad2021unified}, and synthesis~\citep{li2022competition}. One particularly attractive feature of such models is that, due to the use of subword tokenization~\citep{gage1994new, schuster2012japanese, sennrich-etal-2016-neural}, they can generate arbitrary code expressions---including novel identifier names and AST structures---at test time. However, unlike code completion tasks that can often work well with just the surrounding code as  context, effective type inference generally requires non-local information, including code fragments that may belong to an entirely different file. For instance, consider a function $f$ that passes a generically named parameter $x$ directly into another function $g$. It can be hard to figure out the type of $x$ by just looking at $f$'s body. When programmers find themselves in such a situation, they often inspect the callers and callees of $f$, sometimes even transitively, in order to figure out the intended type of $x$. Thus, in many cases, looking at the immediate context of a given variable may be insufficient for accurately predicting its type. 

% In this work, we present a new approach called \toolname{} that views type prediction as a special case of code completion, for which effective pre-trained seq2seq models exist \citep{ahmad2021unified,wang2021codet5}. Because such models can generate arbitrary code expressions---including new identifier names and abstract syntax trees---viewing type inference as a code completion task  leads to a simple and unified solution that addresses some of the key limitations of prior approaches. Our proposed approach utilizes a pre-trained transformer model to generate typed code from its untyped counterpart. 
% Given a code element $e$ (variable or function) whose type is to be predicted, our method constructs context $\vs_e$ for $e$ using static analysis. This context captures information like callers and callees of functions, which may give information about arguments or return types of that function. We train our model to condition its prediction on $\vs_e$.\jiayi{fix this.} Because the context is, by construction,  tailored towards type inference, this design allows  \toolname{} to get both better accuracy as well as more coherent type predictions, especially on rare and complex types. Furthermore,  to  take full advantage of the seq2seq paradigm,  \toolname{} uses an incremental decoding scheme that conditions on prior type predictions, allowing information to be propagated across distant code elements. 



Our approach, \toolname{}, solves this challenge by using static analysis to identify which parts of the codebase are useful for each prediction. In particular, we construct a so-called \emph{usage graph}, where nodes correspond to code elements (i.e., functions or variables whose types we want to predict) and edges denote a potential user-usee relation between them. Given such a graph, we then encode the users and usees of a given code element in a form that resembles normal code and feeds them as additional contexts to the transformer model.  
To take full advantage of the seq2seq paradigm, we also propose an iterative decoding scheme that pass in previous type predictions using the contexts, allowing information to be propagated between distant code elements across the entire codebase. 
% To further take advantage of the seq2seq paradigm, we also propose an incremental decoding scheme that allows information to be propagated across distant code elements.
%  Specifically, we decode twice, in two different orders derived from the usage graph, allowing us to condition on types that were already predicted in order to propagate type information from other contexts.


%To replicate such reasoning process in a transformer-based type inference model, due to its limited receptive field, we cannot simply feed the model with the contents of all files from a project. To address this challenge, we apply static analysis techniques to gather only the most relevant API elements from the entire project for each type prediction, which are then encoded into code fragments that resemble normal code to aim knowledge transfer. Moreover, to further take advantage of the seq2seq paradigm, we also propose a new decoding scheme that performs iterative iterative decoding on  linearized project call graphs to allow information to be propagated across distant API elements. Our ablation studies show that this modification not only boost the model's performance but also lead to type assignments that are significantly more coherent, as measured by the amount of type errors.

We have implemented \toolname{} on top of the popular CodeT5 model and use it to synthesize type annotations for untyped Python code. Our evaluation compares \toolname{} with three state-of-the-art type inference tools \citep{allamanis2020typilus, mir2022type4py, peng22hityper} and a CodeT5 baseline that does not leverage static analysis. The results show that \toolname{} outperforms all baselines by a large margin, while drastically improving the accuracy on rare and complex types.
% We also present ablation studies to justify the use of static analysis results as an input to the model as well as quantify the impact of the proposed iterative decoding scheme.\jiayi{briefly mention interactive setting}
Our ablation studies confirm the benefits of the various modifications we made to the CodeT5 baseline, while an additional type checking experiment shows that the proposed iterative decoding scheme also improves the coherence of the produced type assignments, resulting in fewer type constraint violations. Finally, we explore an alternative use case of our model, where the user interactively inspects the model's predictions and makes necessary corrections. The result demonstrates the usefulness of our approach as a developer tool to annotate entirely untyped projects---on average, the user only needs to to correct one in every five model predictions.

%We also compare our approach against the vanilla CodeT5 model and perform ablation studies to evaluate the importance of our new decoding scheme. These studies highlight the importance combining seq2seq models with our proposed static analysis and encoding/decoding scheme, in terms of both accuracy and the number of type errors.  

%nd the vanilla CodeT5 architecture that does not leverage static analysis. Our experiments show that our approach outperforms all baselines by a large margin, while drastically improving the accuracies on rare and complex types. 
%Our ablation studies confirm the benefits of the various modifications we made over the vanilla CodeT5 architecture, while an additional type checking experiment show that the proposed iterative decoding scheme also improves the coherence of the produced type assignments, resulting in fewer type constraint violations. 

To summarize, this papers makes the following contributions:
\begin{itemize}[leftmargin=*]
   \item We apply CodeT5 to infer Python type annotations and show significant improvement over prior approaches. To our knowledge, this is the first ML-based technique capable of predicting both parametric and user-defined types.
   \item We improve the vanilla CodeT5 model by applying static analysis techniques to help the model reason about information beyond local contexts, further boosting its performance.
   \item We propose an iterative decoding scheme that particularly helps with  \emph{coherence}, as measured by the number of type errors reported by the type checker. We additionally propose the novel setting that combines the seq2seq decoding scheme with user intervention.
\end{itemize}

\section{Overview}\label{sec:overview}

\begin{figure}[!b]
   \centering
   \includegraphics[width=\linewidth]{images/example_code.png}
   \caption{\label{fig:example_code} Simplified code snippets taken from our own codebase. The \code{eval\_on\_dataset} function first calls the \code{chunk\_srcs} function to convert the given textual data into equally sized chunks, and it then feed them into the \code{ModelWrapper.predict} method. }
\end{figure}

In this section, we motivate the design of \toolname{} using the example shown in 
\autoref{fig:example_code}. This example features a method \code{predict} and two functions \code{eval\_on\_dataset} and \code{chuck\_srcs}, each of which is implemented in a {different} file. Given an untyped version of this code, our goal is to automatically infer the type annotations (highlighted in green). This example is challenging for existing type inference techniques due to the heavy use of user-defined types (such as \code{ChunkedDataset}, \code{PythonType}, and \code{ModelWrapper}) and complex parametric type like \code{dict[int,list[PythonType]]}.

%Such features are hard to tackle for existing type inference techniques, and a state-of-the-art tool such as ??? fails to infer the correct type annotation for X out of Y variables.\jiayi{Should we skip this last sentence? This is example is used to emphasize the need for static analysis.}

%For these reasons, existing state-of-the-art type inference techniques achieve at most \todo{X\%} accuracy on this example.

\begin{figure}
   \begin{minipage}{0.6\linewidth}
      \centering
      \includegraphics[height=5.6cm]{images/example_BPE.png}
      \caption{\label{fig:example_BPE} How CodeT5 encodes and decodes source code using BPE. Marker tokens (highlighted in blue) indicate gaps (input) and their corresponding fillers (output).}
   \end{minipage}\hfill
   \begin{minipage}{0.38\linewidth}
      \centering
      \includegraphics[height=5.6cm]{images/example_seq_decoding.png}
      
      \caption{\label{fig:example_seq_decoding} The two-pass iterative decoding process. }
   \end{minipage}
\end{figure}

%1. Transformer architecture (much more weight)
%2. Pre-training (much larger dataset)
%3. Bype pair encoding (Arbitrary ASTs <-> subword tokens with fixed vocabulary)

\paragraph{Type inference as code infilling} In this work, we advocate a new approach that views type inference as an instance of code infilling. Because type annotations can be viewed as missing code fragments, we fine-tune a state-of-the-art code infilling model, namely CodeT5, as our starting point. Since CodeT5 produces sequences of subword tokens using Byte Pair Encoding \citep{radford2019language, gage1994new} (see \autoref{fig:example_BPE}), it can, in principle, predict arbitrary code snippets to fill masked gaps---including type annotations with complex parametric types and user-defined classes. 

%CodeT5 can predict arbitrary code snippets by producing sequences of subword tokens using Byte Pair Encoding\citep{radford2019language, gage1994new}; hence, it can (in principle) synthesize all type annotations, including complex parametric types with nested user-defined types. We illustrate this encoding-decoding process in \autoref{fig:example_BPE}. 


\paragraph{Incorporating context through static analysis}
% use example to illustrate why vanilla codet5 is not sufficient and why we need static analysis
By using surrounding code as the prediction context, our fine-tuned version of CodeT5 can relatively easily predict the correct type annotations of some of the variables. For example, based on  the names and default values of 
 \code{n\_seqs}  (model.py, line 7) or \code{window\_size} (eval.py, line 4), CodeT5 can figure out the correct types of these parameters. However, for other parameters such as \code{model} in line 2 of eval.py, the surrounding context does not contain enough information to make a reasonable prediction. To see why, observe that \code{ModelWrapper} is a new class defined in a separate file, so, (1) it has never been seen during training, and (2) its definition is not available as part of the context. Similarly, it is also very difficult to predict the return type of \code{eval\_on\_dataset} since it directly returns the result  of \code{model.predict}, whose definition is also not available to the model.

 
% it is relatively easy for it to guess their correct types just from the names and default values, many other type annotations require information beyond the local information available in each file. 
%For example, just by looking at the definition of the \code{eval\_on\_dataset} function, it will be very unlikely for our model to predict that the parameter \code{model} has the type \code{ModelWrapper} since this is a newly defined class from a separate file and has never been seen by the model during training. Similarly, it would also be very difficult to predict the return type of this function since it directly returns the result of calling a newly defined method \code{model.predict}, whose definition is also not available to the model.

To address this issue, our approach enhances the prediction context of CodeT5 using static analysis. The  details of how we construct the context from static analysis will be described in \autoref{sec:model_input}, but, in a nutshell, our approach  analyzes the user-usee relations among code elements and pulls in the relevant definitions into the context. For example, when the model is making a prediction for  \code{eval\_on\_dataset}, the context includes 
the definitions of  \code{DefaultWindow} and \code{predict}, which are defined in \code{ModelWrapper} and invoked at line 6 and line 12 of eval.py, respectively. 


\paragraph{TypeT5 architecture} %include figure as greg requested
%While including the immediate users and usees of a code element as part of context is often \emph{necessary} for making accurate type predictions, it turns out that it is \emph{not sufficient}\jiayi{This sounds a bit too strong to me---It's hard to tell whether something is sufficient or insufficient for the model, given that it's conditioned on multiple sources of information and making guesses.}.
As there are many dependencies between different code elements,  making \emph{independent} type predictions for each code element  is not ideal. 
For example, to infer the return type of \code{eval\_on\_dataset}, we would need to know the return type of \code{ModelWrapper.predict}, which depends on the  return type of the \code{self.predict\_on\_batch}  (model.py, line 12). However, since there is limited context that can be fed to the model, it is not feasible to include all transitive users and usees. To deal with this problem, TypeT5 utilizes an iterative decoding scheme that allows  conditioning on previous type predictions. %^This iterative decoding strategy eliminates the need for looking deeper into the usage graph, allowing TypeT5 to make reasonable predictions with even limited context. 
In more detail, our decoding scheme first sorts the user-usee graph topologically and then performs two sequential prediction passes, first from usee to users and then going in the reverse direction, as illustrated in \autoref{fig:example_seq_decoding}. To see why both of these directions are useful, observe that the return type of \code{eval\_on\_dataset} depends on \code{predict}, which in turn depends on \code{predict\_on\_batch}. Thus, propagating information from callees to callers is clearly useful. Conversely, consider predicting the type of \code{data}, the first parameter of \code{predict}. Since the return value of \code{chunk\_srcs} is passed as the first argument of \code{predict}, propagating information in the reverse direction can also be helpful. 


% predicting these types require chain of reasoning - how?
% such chain of reasoning is hard 


\section{Methods}


\subsection{Using CodeT5 for type prediction}


We formulate type prediction as a sequence-to-sequence (seq2seq) task. Let $\vu = (u_1,\ldots,u_n)$ represent a sequence of code tokens, where each token is a single untyped code element $e$ (function or variable). We insert into $\vu$ indexed marker tokens (\code{<extra\_id\_i>}) at each point where we wish to predict types and let the model predict $\vt = (t_1,\ldots,t_m)$, the token sequence that encodes types for the marked locations in $\vu$. Note that $\vt$ only contains the types, no other code tokens, in the format \code{<extra\_id\_1> [type 1 tokens] <extra\_id\_2> [type 2 tokens]}, etc. We use the same tokenizer as CodeT5, which allows encoding any Python expression as a (typically short) sequence of subword tokens.


Our baseline CodeT5 model is a Transformer seq2seq model $P(\vt \mid \vu, \bar{\vu})$ placing a distribution over type sequences $\vt$ given the raw code sequence $\vu$ and its surrounding code $\bar{\vu}$. This process is shown in \autoref{fig:example_BPE}, and $\bar{\vu}$ is omitted for clarity. 

Our improved model, \toolname{}, replaces the surrounding code $\bar{\vu}$ with a context constructed from static analysis, which we denote as $\vs$. Thus, we can write the model as  $P(\vt \mid \vu, \vs)$. Note that, in both models, we remove all comments and Python docstrings as a pre-processing step, as was done in prior work. We now show how $\vs$ is constructed from static analysis.

% \paragraph{Training}\gd{idk whether this section goes here or in implementation or something, but I'm putting the content here anyway} We can derive supervised training pairs $(\vu^*, \vt^*)$ from any typed Python code available to us. We fine-tune from the pre-trained CodeT5 weights to minimize log loss ($-\log P(\vt^* \mid \vu^*)$) of our observed sequence using teacher forcing.\gd{again, possibly comment on the units here: do we segment the code up in some particular way? this can also go in implementation details/appendix if needed}


%To deal with an unbounded space of identifier names and AST structures, CodeT5 employs the Byte Pair Encoding (BPE) tokenizer to perform a one-to-one mapping between source code and subword tokens, as shown in \autoref{fig:??}. Unlike a traditional tokenizer that uses a fixed number of common words as the vocabulary, BPE constructs its vocabulary by first viewing the source code as a sequence of bytes (using the Unicode file encoding) and then recursively builds up its vocabulary by identifying the most common combinations of bytes and merge them into new ``words'', called subword tokens. This way, as long as BPE has at least the 256 unique bytes in its vocabulary, it can be used to encode all possible sequences, while having a larger vocabulary enables it to produce shorter encoding sequences\footnote{CodeT5 made some additional modifications to the original BPE encoder to better support the special characters and whitespaces commonly used in programming languages.}. In CodeT5, the size of the BPE vocabulary is set to \fixme{XX}.

\begin{figure}
   \centering
   \includegraphics[width=0.8\linewidth]{images/example_call_graph.png}
   \caption{\label{fig:example_call_graph} The usage graph corresponding to the code snippets in \autoref{fig:example_code}. \code{WindowArgs} and \code{ChunkedDataset} are assumed to be defined elsewhere in the same project. }
\end{figure}

\subsection{\label{sec:usage_graph} Building the Usage Graph}

To overcome the limitations of using \emph{only} the surrounding code as context, our approach relies on static analysis to extract relevant global information about the codebase.  In particular, our analysis constructs a \emph{usage graph}, whose nodes correspond to code elements and edges represent a direct usage. For example, if $x$ and $y$ are both functions, an edge from $x$ to $y$ means that $x$ calls $y$. Similarly, for variables, an edge from $x$ to $y$ means that the value of $x$ depends on $y$. 
%Note that  identifying the \emph{exact} dependence relationship between code elements is undecidable, so our method over-approximates possible dependencies by also generating \emph{potential} usage edges.


The usage graph for our previous example is shown in \autoref{fig:example_call_graph}. We show two types of usages: a certain usage, shown as solid arrows, and a potential usage, shown as dotted arrows. A certain usage is one that can be statically determined without knowing the types of the involved variables. For example, the global function \code{chunk\_srcs} is directly used by \code{eval\_on\_dataset}, and we also know that  \code{predict\_on\_batch}  is called by \code{predict} (\code{self} method call on line 12). For other cases like a non-\code{self} attribute access or method call, the target  depends on the type of the receiver, so we first collect all attributes and methods with a matching name from the current project and then generate a potential usage for each of them. We give more details of usage graph construction in \autoref{sec:usage_graph_details}.

\subsection{\label{sec:model_input} Constructing Model Inputs}

\toolname{} leverages the usage graph $\mathcal{G}$ to construct the inputs to our model. In particular,  we define the model input for a code element $e$ to be a concatenation of four parts: the preamble $\preamble$, usees $\usees$, main code $\vu$, and users $\users$, which are constructed as follows (see \autoref{sec:real_model_input} for a concrete example).
\begin{itemize}[leftmargin=*]
    \item \textbf{Preamble}: The main purpose of  $\preamble$ is to help the model  map each local name to the  definition that it is referring to, so the preamble includes all  import statements. Additionally, to help the model see which types are available in the current scope, the preamble also includes the headers of all class definitions as well as all the type variable declarations from the current file.
    \item \textbf{Main code}: For the code element $e$ of interest, we use its definition   to construct $\vu$. If $e$ is a function, $\vu$ is just its source code, and if $e$ is a variable, $\vu$ includes all top-level assignment statements in which $e$ appears as the left hand side. Additionally, if $e$ is a method or attribute of a class $C$, we also indent $\vu$ and prepend it with the header of $C$, as shown in \autoref{fig:ex_main_code}.
    \item \textbf{Users}: Given the usage graph $\mathcal{G}$, we construct $\users$ by including the source tokens of all elements from $\mathrm{users}(\mathcal{G}, e)$. Since these elements can come from different source files, we also prepend each element with a special comment denoting the Python module it comes from. Note that these elements can optionally contain the types predicted by our \toolname{} model, as  described later in \autoref{sec:seq_decoding}.
    \item \textbf{Usees}: $\usees$ contains not just the direct users of $e$, but also anything that is used in the user context $\users$. i.e.,  $\usees$ contains the elements from $\mathrm{usees}(\mathcal{G}, e) \cup \bigcup_{e' \in \mathrm{users}(\mathcal{G}, e)} \mathrm{usees}(\mathcal{G}, e')$. Since this generally contains many more elements than $\users$, we only use the (typed or untyped) \emph{signatures} of the elements to construct $\usees$. 
\end{itemize}

We limit the total input size to be at most 4096 subword tokens and cut off any exceeding tokens from both left and right, centered around the main code. Context elements involving certain usages are arranged to be closer to the center so that they are always prioritized over potential usages.

\subsection{Iterative Decoding Inference} \label{sec:seq_decoding}

We now describe how to conduct inference in our base model as well as our context-augmented model using an iterative decoding scheme.

\textbf{CodeT5 decoding:} Given our trained model $P(\vt \mid \vu, \bar{\vu})$, we can infer a most likely set of types $\vt$ for $\vu$ (with surrounding context $\bar{\vu}$) using beam search. Our implementation performs joint prediction of the output types for a single code block $\vu$, since later types in the block condition on the predictions of earlier types. However, note that both $\vu$ and $\bar{\vu}$ are always \emph{completely untyped} code: while we condition on previous types as we predict, these are not inserted into the prediction context for the next element.\footnote{In our experiments, including predicted  types actually hurts the performance due to exposure bias.}

\textbf{\toolname{} iterative decoding:} Part of our motivation for including the context $\vs$ is to exploit its type information at inference time. Crucially, this requires $\vs$ to be \emph{typed}. However, the contexts that are drawn from the original codebase are not typed, so \toolname{} iteratively adds type signatures to these contexts using its own predictions. Let $\mathcal{M}$ be the type assignment produced by the model, which maps each code element $e$ to its predicted type $\vt_e$, and denote $\mathcal{M}(\vs)$ as the context obtained by annotating the elements in $\vs$ according to $\mathcal{M}$. Starting with an empty $\mathcal{M}$ (which leaves any context $\vs$ unchanged), \toolname{} then iteratively updates $\mathcal{M}$ using an iterative decoding scheme that traverses the usage graph $\mathcal{G}$ twice, as shown in \autoref{fig:example_seq_decoding}. The first prediction pass follows the usee-to-user order,\footnote{This requires performing a topological sort over $\mathcal{G}$. When $\mathcal{G}$ is not a DAG, we break the cycles arbitrarily.} while the second pass goes in the reverse direction to allow for bidirectional information flow. At any given time step, we can denote the model's prediction for element $e$ as drawn from $P(\vt_e \mid \vu_e, \mathcal{M}(\vs_e))$, and the predicted types $\vt'_e$ are then used to update $\mathcal{M}$ such that $\mathcal{M}(e) = \vt'_e$.
%Besides the proposed decoding scheme, we also explore other options in \autoref{sec:sequential_decoding_experiment}.

\subsection{Training}
To save computation and improve parallelism during training, we use the available human annotations as a gold type assignment $\mathcal{M}^*$ instead of letting the model condition on its own prediction. Note that this type assignment is generally incomplete and may not contain a label for every missing type. We train the model to maximize the log likelihood of predicting the ground truth, i.e., $\log P(\vt_e^* \mid \vu_e, \mathcal{M}^*(\vs_e))$, for every element $e$ where $\vt_e^*$ is available, using teacher-forcing. We train the CodeT5 baseline model similarly on the same dataset.\footnote{Note that without this training (fine-tuning) step, the original CodeT5 model performs very poorly as it tends to predict arbitrary Python expressions that are not types.}



\section{Experiments}

We implement \toolname{} in Python, whose the source code and model weights can be found on GitHub\footnote{Available at \href{https://github.com/utopia-group/TypeT5}{\code{https://github.com/utopia-group/TypeT5}} .}. We list the hyperparameters and hardware details in \autoref{sec:hyperparameters}.
Below, we first compare \toolname{} against three state-of-the-art type inference systems for Python, namely Typilus~\citep{allamanis2020typilus}, Type4Py~\citep{mir2022type4py}, and HiTyper~\citep{peng22hityper}. We then present ablation studies to evaluate different factors contributing to the model's performance. 


\subsection{Evaluation Setup}\label{sec:eval_setup}

\paragraph{Datasets} In our evaluation, we predict the type annotations for \emph{top-level} code elements of each Python project. These include all class attributes, methods, top-level functions, and global variables. We treat any existing user-added type annotations as the ground truth, and we use per type accuracy as the main performance metric. Following a setting similar to that of \citet{allamanis2020typilus} and \citet{Wei2020LambdaNet}, we split our dataset per Python project. This way, types newly defined in the test projects will not have been seen during training. Such a setting is more challenging than splitting the dataset per file, as is done in Type4Py and other work \citep{pradel2020typewriter, mir2022type4py}, but more closely reflects the model's real-world performance~\citep{gruner2022cross}.

Our main dataset, \textbf{BetterTypes4Py}, is constructed by selecting a high-quality subset from the ManyTypes4Py dataset~\citep{mt4py2021}, which was used to train Type4Py. We describe the selection criteria in \autoref{sec:construct_dataset}. Since our model is fine-tuned from the CodeT5 model (which may have already been pre-trained on some of the test repositories in the aforementioned dataset), we additionally construct \textbf{InferTypes4Py}, a test set derived from the source code of Typilus, Type4Py, and our own tool, none of which were used as CodeT5's (pre-)training data. We further discuss the potential code duplication issue (\autoref{sec:code_duplication}) and label quality issue (\autoref{sec:label_quality}) in the appendix.

\begin{table} 
   \small
   \centering
   \caption{\label{tab:dataset_stats} Basic statistics of our two datasets. }
   \begin{tabular}{l|r r r|r}
      \toprule
       & \multicolumn{3}{c|}{\textbf{BetterTypes4Py}} & \multicolumn{1}{c}{\textbf{InferTypes4Py}} \\
       & \textit{train} & \textit{valid} & \textit{test} & \textit{test} \\
      \midrule
      Projects & 573 & 40 & 50 & 3 \\
      Nonempty files & 16.5K & 1098 & 949 & 99 \\
      Lines of code & 2.4M & 174K & 139K & 21K \\
      Top-level type slots & 541K & 38.2K & 28.4K & 4.6K \\
      Top-level user-added types & 275K & 19.3K & 15.8K & 2.7k \\
      Rare type ratio & 25.7\% & 23.3\% & 35.0\% & 33.8\% \\
      Complex type ratio & 20.4\% & 16.6\% & 20.8\% & 33.4\% \\
      Average type size & 1.42 & 1.33 & 1.43 & 1.72 \\
      \bottomrule
   \end{tabular}
\end{table}

We summarize key properties of both datasets in Table~\ref{tab:dataset_stats}. We define the size of a type as the number of type constructors in its body\footnote{e.g., both \code{int} and \code{foo.Bar} has a size of 1, whereas \code{dict[str, foo.Bar]} has a size of 3.} and categorize a type as \textbf{simple} if its size is 1, and \textbf{complex} otherwise. We also categorize a type as \textbf{rare} or \textbf{common} depending on whether it contains a rare type constructor that is not from the top-100 most frequent type constructors in our training set.

\paragraph*{Accuracy metrics.} Since Python allows the same type to be written in different syntactic forms,\footnote{e.g., both \code{Union[int,None]} and \code{Optional[int]} refer to an integer that can also be \code{None}, and both \code{list} and \code{List[Any]} refer to a python list with untyped elements} 
we first perform type normalization to convert both the predicted and ground-truth types into a canonical form. The details of this normalization step can be found in \autoref{sec:type_normalization}, 
%to accurately measure whether the model has predicted the correct type, we first performance a type normalization step to convert both the predicted and the ground-truth types into a canonical form and then check their equivalence.  
and we use the term \textbf{full accuracy} to refer to the accuracy against all human annotations after normalization. 
To better compare with prior work, we also define \textbf{adjusted accuracy} (our main metric), which (1) filters out all \code{None} and \code{Any} labels (as in prior work); (2) converts fully qualified names to simple names (e.g.,  \code{Tensor} instead of  \code{torch.Tensor}) since some prior approach does not output correctly qualified types;
%since the two prior approaches do not produce context-dependent qualified names like our approach
(3) rewrites any outermost \code{Optional[T]} and \code{Final[T]} into \code{T} since they tend not to be used consistently across programmers.\footnote{e.g., the type checker \code{mypy} has an option to enable implicit \code{Optional} types, so it would not be possible for the model to know if it should output \code{Optional[T]} or \code{T} just from the untyped code.} Finally, we also define a \textbf{base accuracy} metric that is the same as adjusted accuracy except that it only checks the outermost type (e.g., \code{Dict[str,List]} will match any \code{Dict} but, for example, not \code{Mapping}.)



\subsection{Comparing \toolname{} with other approaches}

% metrics for this experiment
% full accuracy
% adjusted accuracy (all, simple, complex)
% base accuracy (all, common, rare)
% all types
% simple types
% complex types
% common types
% rare types

\begin{table}
   \small
   \centering
   \caption{\label{tab:compare_acc_common} Accuracy comparison on \textbf{common} types. }
   \begin{tabular}{l | c:c c c:c | c:c c c: c}
      \toprule
         & \multicolumn{5}{c|}{\normalsize BetterTypes4Py} & \multicolumn{5}{c}{\normalsize InferTypes4Py} \\
         & \multicolumn{1}{c:}{\hh{full}} & \multicolumn{3}{c:}{\hh{adjusted}} & \multicolumn{1}{c|}{\hh{base}} & 
         \multicolumn{1}{c:}{\hh{full}} & \multicolumn{3}{c:}{\hh{adjusted}} & \multicolumn{1}{c}{\hh{base}} \\
         & \hhh{all} & \hhh{all} & \hhh{\small simple} & \hhh{\scriptsize complex} & \hhh{all} & 
         \hhh{all} & \hhh{all} & \hhh{\small simple} & \hhh{\scriptsize complex} & \hhh{all} \\
      \midrule
      Typilus     & \na   & 54.05 & \small{55.12} & \small{33.23} & 60.37 &
                     \na   & 52.33 & \small{52.19} & \small{53.91} & 64.67 \\
      Type4Py     & \na   & 50.34 & \small{51.91} & \small{32.14} & 47.51 &
                     \na   & 32.08 & \small{33.47} & \small{16.54} & 29.83 \\
      HiTyper     & 59.20 & 54.28 & 57.70 & 26.44 & 59.01 &
                     45.67 & 43.54 & 46.00 & 19.27 & 47.99 \\
      CodeT5      & 76.74 & 78.04 & \small{82.43} & \small{53.03} & 82.44 &
                     77.83 & 78.06 & \small{85.31} & \small{63.41} & 81.87 \\
      \toolname{} & \best{79.24} & \best{81.43} & \best{85.69} & \best{56.75} & \best{84.82} &
                     \best{81.75} & \best{82.95} & \best{87.62} & \best{72.78} & \best{84.17} \\
      \bottomrule
   \end{tabular}
\end{table}

\begin{table}
   \vspace{-0.5cm}
   \small
   \centering
   \caption{\label{tab:compare_acc_rare} Accuracy comparison on \textbf{rare} types. }
   \begin{tabular}{l | c:c c c:c | c:c c c: c}
      \toprule
         & \multicolumn{5}{c|}{\normalsize BetterTypes4Py} & \multicolumn{5}{c}{\normalsize InferTypes4Py} \\
         & \multicolumn{1}{c:}{\hh{full}} & \multicolumn{3}{c:}{\hh{adjusted}} & \multicolumn{1}{c|}{\hh{base}} & 
         \multicolumn{1}{c:}{\hh{full}} & \multicolumn{3}{c:}{\hh{adjusted}} & \multicolumn{1}{c}{\hh{base}} \\
         & \hhh{all} & \hhh{all} & \hhh{\small simple} & \hhh{\scriptsize complex} & \hhh{all} & 
         \hhh{all} & \hhh{all} & \hhh{\small simple} & \hhh{\scriptsize complex} & \hhh{all} \\
      \midrule
      Type4Py     & \na   & 12.37 & 13.17 & 4.05 & 14.15 &
                     \na   & 0.25  & 0.14  & 0.98 & 0.17 \\
      HiTyper     & 10.30 & 25.51 & 27.59 & 9.79 & 29.33 &
                     9.36  & 9.36  & 10.79 & 1.19 & 12.33 \\
      CodeT5      & 49.47 & 52.95 & 57.28 & 34.26 & 57.65 &
                     51.64 & 53.28 & 59.97 & 30.62 & 66.47 \\
      \toolname{} & \best{58.56} & \best{61.47} & \best{65.21} & \best{40.22} & \best{68.44} &
                     \best{53.44} & \best{56.27} & \best{61.50} & \best{36.92} & \best{69.23} \\
      \bottomrule
   \end{tabular}
\end{table}

We compare \toolname{} with the basic CodeT5 model described in \autoref{sec:seq_decoding} as well as the released versions of three other state-of-the-art approaches from prior work.\footnote{Since our approach benefits from CodeT5's large-scale pre-training across 8 different programming languages, we use a smaller training set than prior work and do not retrain these prior approaches on our dataset.} Typilus~\citep{allamanis2020typilus} models the program as a graph and applies a graph neural network to compute the types of variables. The original Typilus model can predict from a set of common types as well as (nonparametric) user-defined types, but their released model can only predict common types, so we only evaluate its performance on common types. Type4Py~\citep{mir2022type4py} uses variable names and the surrounding code to compute an embedding and performs type prediction via a nearest-neighbor search in the type embedding space. We run the released Type4Py model via its web interface. HiTyper~\citep{peng22hityper} combines the strengths of a rule-based type inference algorithm with ML type inference models by only invoking the ML model on places where the inference algorithm gets stuck and deducing the types elsewhere using typing constraints. Its implementation uses Type4Py as the default ML backend, which we use to run HiTyper.

We show each tool's accuracy in \autoref{tab:compare_acc_common} (on common types) and \autoref{tab:compare_acc_rare} (on rare types), and we put the combined results in appendix (\autoref{tab:compare_acc_all}).
Since HiTyper was only able to make a predition for about 67\% of all labels, we report its performance on this subset.
From \autoref{tab:compare_acc_common}, we can make the following observations. (1) Our CodeT5 baseline model already outperforms all prior approaches by a large margin, demonstrating the advantage of using a seq2seq pre-trained language model. (2) \toolname{} further improves the adjusted accuracy by 3.4\% and 4.9\% on the two datasets. 
(3) Type4Py's performance on InferTypes4Py dataset is significantly lower than on BetterTypes4Py, likely because Type4Py was originally trained on some of the test files in BetterTypes4Py.\footnote{The accuracies reported in the original Type4Py paper are much higher than we measured here. We analyze this discrepency in \autoref{sec:type4py_discussion}.} 
Looking at \autoref{tab:compare_acc_all}, we see that both CodeT5 and \toolname{} dramatically outperform the other approaches on rare types. Moreover, \toolname{} achieves the largest improvements on types that are both rare and complex, improving upon CodeT5 by about 6\% on both datasets, suggesting that that global information is especially important in these cases. For a qualitative analysis, we also show TypeT5's outputs on a real-world example in \autoref{sec:real_model_input}.


\subsection{Ablations on \toolname{}} \label{sec:ablation}

We next present a series of ablations that evaluate  how various factors contribute to \toolname{}'s performance. In addition to accuracy (on all types), we also report the \textbf{type error count} as a way to estimate how \emph{coherent} the model's predictions are.\footnote{Accuracy does not always correlate with coherency. e.g., when we have \code{x = y}, and \code{x} and \code{y} are equally likely to be a \code{str} or an \code{int}, a coherent type assignment needs to ensure that \code{x} and \code{y} always have the same type, even if this requirement does not lead to a higher accuracy in expectation.} 
Note that we only report errors that are directly related to type coherence (since the type checker also reports errors unrelated to type coherence, such as unresolved import statements. We give more details about this metric in \autoref{sec:type_error_count}.). 


\paragraph*{How do different components contribute to the model's performance?}  To evaluate the impact of each context element, we remove one component at a time and \emph{retrain the model} accordingly. In particular, the \textbf{No Preamble}, \textbf{No Users}, and \textbf{No Usees} ablations correspond to removing the $\preamble$, $\users$, and $\usees$ context elements (introduced in \autoref{sec:model_input}) from the model's input, respectively. The \textbf{Nonincremental} model does not perform iterative decoding and is trained to condition on an untyped context. We use the same input size limit (4096 subword tokens) for all models, so a model that does not utilize one kind of information have more space for other kinds. We show both the adjusted accuracy on all types and type error count in \autoref{tab:perf_ablation} (and show other accuracy metrics in \autoref{tab:perf_ablation_detailed}). We can see that (1) all components improve the overall accuracy, with the preamble having the largest impact, and (2) while the iterative decoding scheme only improves overall accuracy slightly, it significantly improves the model's coherence, resulting in 12\% fewer type errors.


\paragraph*{How do different decoding strategies compare?} In addition to the two-pass iterative decoding strategy  introduced in \autoref{sec:seq_decoding}, we also test four other decoding strategies: (1) \textbf{Independent}, which independently predicts the type signature for each element without conditioning on the model's own prediction (same as the Nonincremental model except not retrained); (2) \textbf{Random}, which visits each element once following a random order; (3) \textbf{UserToUsee}, which visits the users before the usees; (4) \textbf{UseeToUser}, which visits the usees before the users. The results are shown in \autoref{tab:perf_decoding_strategy} (and show other accuracy metrics in \autoref{tab:perf_decoding_detailed}). We can see that our proposed TwoPass decoding scheme yields the largest accuracy and type error improvement over Independent; whereas UserToUsee performs worse than Independent, suggesting that bad decoding ordering can have adverse effects on the model's performance.

\begin{table}
   \begin{minipage}{0.47\textwidth}
      \centering
      \caption{\label{tab:perf_ablation} Performance of different model modifications. All models are retrained with the corresponding inputs.}
      \begin{tabular}{l c c}
         \toprule
         \textbf{Modification} & \textbf{Accuracy} & \textbf{\small Type Error}\\
         \midrule
         No Preamble    & 64.20 & 6067 \\
         No Users       & 71.20 & 7053 \\
         No Usees       & 67.25 & 7332 \\
         Nonincremental  & 72.52 & 5720 \\
         \small{Original (\toolname{})}  & \best{73.02} & \best{5087} \\
         \bottomrule
      \end{tabular}

   \end{minipage}
   \hfill%
   \begin{minipage}{0.48\textwidth}
      \centering
      \caption{\label{tab:perf_decoding_strategy} Performance of different decoding strategies. The same \toolname{} model weights are used for different decoding strategies.}
      \begin{tabular}{l c c}
         \toprule
         \textbf{Strategy} & \textbf{Accuracy} & \textbf{\small Type Error} \\
         \midrule
         Independent & 71.68 & 6876 \\
         Random      & 71.66 & 6215 \\
         UserToUsee  & 70.67 & 7415 \\
         UseeToUser  & 72.65 & 6402 \\
         \small{TwoPass (\toolname{})}  & \best{73.02} & \best{5087} \\
         \bottomrule
      \end{tabular}
   \end{minipage}
\end{table}


% \paragraph*{Does iterative decoding help the model predict more consistent results?} \label{sec:sequential_decoding_experiment} Since accuracy does not directly reflect the coherence of the predicted type assignments,\gd{this comes out of nowhere. Need some transition: ``Coherence of types is important [cites]. Our hypothesis was that iterative decoding would help with this...''} in this study, we measure the number of type errors resulted from applying the model's predictions to the original source code. Since we need to manually install the project dependencies for the type checker \code{MyPy} to work properly, we conduct this experiment on the smaller InferTypes4Py dataset. Since a single incorrect type annotation can cause multiple errors at the useage sites, to reduce the effect of overcounting, we count the number of unique type error messages in each file and report the project totals in \autoref{tab:type_errors}.

% \begin{table}
%    \centering
%    \caption{\label{tab:type_errors} Number of type errors reported by MyPy on InferTypes4Py Dataset. \fixme{Only show total on InferTypes4Py.} }
%    \begin{tabular}{l|c|c|c|c}
%       \toprule
%       \textbf{Strategy} & \multicolumn{4}{c}{\textbf{Number of Errors}}\\
%           & \textit{TypeT5} & \textit{Typilus} & \textit{Type4Py} & \textit{Total} \\
%       \midrule
%       Independent & 585 & 401 & 163 & 1149 \\
%       TwoPass & 478 & 238 & 155 & 871  \\
%       \bottomrule
%    \end{tabular}
% \end{table}

\subsection{User-Guided Interactive Decoding} \label{sec:interactive_decoding} Compared to prior work, our approach has a unique strength: because the model can condition on previous types, it allows easy user intervention by conditioning on any corrections made by the user. 
We thus explore an alternative use case of our model where the user interactively inspects each type signature predicted by the model and makes any necessary corrections before the model moves on to the next prediction.
We emulate this interactive process using the ground-truth human annotations $\mathcal{M}^*$, and we modify the usee-to-user decoding process to let the model predict the types of each element $e$ (as before), but then override the predicted types $\vt_e$ with the corresponding human annotations $\mathcal{M}^*(e)$ if $e \in \mathcal{M}^*$. On the BetterTypes4Py dataset, this interactive decoding process achieves a full accuracy of \textbf{78.04\%} and an adjusted accuracy of \textbf{79.37\%}---meaning that on average, it only requires the user to correct one in every five model-predicted types to fully annotate an entirely untyped project from scratch. Note  that  only  59\% of the test set contains a user type annotation, so some incorrect type annotations may not be corrected immediately and can get propagated to other places. Thus, in an interactive real-world setting, we can expect the actual accuracy to be even higher.  

% \paragraph*{What are the most common types of errors?}

% \paragraph*{A few cool examples? (In the appendix?)}

\section{Related Work}

\paragraph{Deep Learning Type Inference} Most prior approaches  predict user-defined types via some form of type embedding matching. For example, LambdaNet~\citep{Wei2020LambdaNet} tackles this challenge by combining graph neural networks with a pointer network. Typilus~\citep{allamanis2020typilus}  performs nearest-neighbor search in the type embedding space.
% and trains the model with contrastive learning. 
However, neither approach can handle the unbounded type space induced by parametric types. TypeBert~\citep{jesse2021learning} is the first type inference method  based on pre-trained transformer models and has demonstrated superior performance in predicting common types. \citet{jesse2022learning} later improved TypeBert using deep similarity learning to better support user-defined types. Different from our work, TypeBert does not use a seq2seq model or construct context from static analaysis. Apart from just using the source code, other forms of information have also been utilized for type prediction. Both TypeWriter~\citep{pradel2020typewriter} and HiTyper~\citep{peng22hityper} combines type checking with inference-time search to avoid generating type errors. OptTyper~\citep{pandi2020opttyper} explicitly models type constraints by turning them into a continuous optimization objective. NL2Type~\citep{malik2019nl2type} focuses on predicting types from natural language information such as code comments and documentation.

\paragraph{Retrieval-based models in NLP} Similar to our context augmentation with static analysis, a number of methods have been developed to augment pre-trained models with context in natural language processing (NLP). For open-domain question answering \citep{chen-etal-2017-reading}, approaches like REALM \cite{realm} and DPR \citep{karpukhin-etal-2020-dense} can retrieve knowledge relevant to a query, and retrieval-augmented generation \citep{lewis2020} and its extensions (e.g.,  Fusion-in-Decoder \citep{izacard2021}) have shown that it is possible to generate longer outputs using this information. RETRO \citep{retro} and WebGPT \citep{webgpt} take this to a web-scale extreme. 
%However, these applications have different properties from ours: knowing what kind of encyclopedic knowledge is needed to answer a question is a very different problem than knowing what contexts in code are needed to infer type information. We are also able to leverage static analysis based on the usage graph which has no analogue for text; however, we are not able to rely on relatively unambiguous identifiers like names of real-world entities. Finally, our iterative decoding scheme is needed to reckon with the fact that our added context itself contains untyped code that we need to run inference on.
However, we are also able to leverage static analysis based on the usage graph,  which has no analogue for text.

\paragraph{Linking with T5}  While  our use of CodeT5 for type prediction is novel to our knowledge, T5 \citep{t5} has been applied to a range of NLP tasks like summarization. Most similar to ours is its use for entity linking \citep{petroni-etal-2021-kilt}: Systems in this vein generate names of entities token by token \citep{decao_autoregressive} and are able to generalize to new entities like TypeT5. However, the presence of ad hoc types for each new context and the types of context clues needed are very different in the code setting than in natural language.

\paragraph{Structured Prediction} Our iterative decoding process can be viewed as applying a learned policy to perform structured prediction~\citep{bakir2007predicting, daume2009search}. In this work, our training scheme can be viewed as behavior cloning since the model directly conditions on ground-truth human annotations, which can lead to distributional mismatch between training and inference time. Applying more advanced training schemes such as Scheduled Sampling~\citep{NIPS2015_e995f98d}, DAgger~\citep{ross2011reduction}, or reinforcement learning~\citep{sutton2018reinforcement} may help further boost the performance of iterative decoding.

\section{Conclusion}

In this work, we showed that \toolname{}, a system integrating CodeT5 with lightweight static analysis and a new decoding scheme, is effective at predicting types for untyped codebases. We show that fine-tuning a pre-trained code model already results in a strong baseline, outperforming systems from past work, and that incorporating the right context with static analysis is particularly helpful to enable accurate prediction on more challenging cases. We believe that even larger models can further improve the accuracy, but they will still need to leverage the methods in this work to achieve high accuracy on rare and complex types.

\section*{Acknowledgment}
This work is partly funded by NSF Award CCF-1918889.
We would like to express our sincere gratitude to Miltiadis Allamanis and Amir Mir for generously helping us in setting up their tools and completing the comparison experiments. We would also like to thank members of the UTOPIA group, especially Jocelyn Chen, for their assistance in managing the GPU server for the experiments.

\bibliography{citations}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix

\section{Appendix}
\subsection{\label{sec:usage_graph_details} Constructing the usage graph}
To resolve the user-usee relations at the project level (used in \autoref{sec:usage_graph}), we built a custom static analysis pipeline on top of the \code{libcst} library. We use \code{libcst} to parse Python files and also rely on its utilities to resolve symbol references within the same file (i.e., it tells us whether a local name refers to a function defined in the current file or comes from an import statement, etc.) We then implement custom import resolution logic (following Python's module rules) and combine it with \code{libcst} to resolve all certain usages within the current project. For unresolved usages with a syntactic form matching a class attribute/method usage, we generate a potential usage to each class members with a matching name. Since the involved static analysis operations are fairly lightweight and we parallelize the construction of different graphs on CPUs, the time spent on constructing the usage graphs only makes up a tiny fraction of the total training or inference time. For example, using 28 Python processes, it only takes about 8 minutes to process the entire BetterTypes4Py training set, including the time spent on other tasks such as parsing, code transformation, and tokenization. 

While our implementation limits the analysis scope to the current project, it is straight-forward to extend the analysis to also include usages involving library definitions, which may help the model see type signatures of uncommon library APIs. We did not do this in our experiments mainly due to the manual effort it would require to install the correct library dependencies for all the projects in our datasets.



\subsection{\label{sec:construct_dataset} Constructing the BetterTypes4Py dataset}
Our dataset is constructed from the ManyTypes4Py dataset as follows: we first filter out the GitHub repositories that are no longer accessible or that fail to download within 10 seconds. This leaves us with 4890 out of the 5996 original projects. Then, we discard  projects that have not been updated for more than one year (to avoid outdated library APIs), reducing the number of repositories to 1218. To limit the influence of any particular project, we also filter out the 37 projects with more than 50K lines of code. Finally, to exclude those projects that  have very few type annotations, we compare the number of type annotations $n_t$ of each project with its lines of code $n_c$ and filter out those with a $n_t$-to-$n_c$ ratio less than 1:20. This gives us our final set of 663 projects. We then randomly select 50 test and 40 validation projects and use the rest for training.

\subsection{\label{sec:code_duplication} Code Duplication}

Code duplication~\citep{allamanis2019adverse} has been shown to have adverse effects on the performance of ML models and can blur the evaluation results. Hence, prior work like Type4Py applies file-level deduplication to remove duplicated source files. However, this is hard to do in our project-based setting since we need all files to be present during inference. To address this, we have manually verified that our InferTypes4Py dataset does not contain files that are copy-pasted from elsewhere. We also run the popular code duplication tool \code{jscpd}\footnote{\href{https://github.com/kucherenko/jscpd}{https://github.com/kucherenko/jscpd}} on our test set to detect duplicated code blocks. The analysis shows that there is relatively little duplication in the dataset (only around 4\% of duplicated lines), and the majority of these duplications came from the same project rather than across projects, so we believe code duplication is not a major issue under our by-project evaluation.

\subsection{\label{sec:label_quality} Label Quality}

In our evaluation, developer-provided type annotations are used as the ground truth, which are not always accurate or coherent~\citep{ore2018assessing}. This partially motivated us to construct the InferTypes4Py dataset, which consists of high-quality type annotations with a relatively low error rate. In particular, our own codebase (which are included in InferTypes4Py) makes heavy use of type annotations throughout the development process and is continuously type-checked by VSCode. As a very rough metric to approximate label quality, we report the coherence error (defined in \autoref{sec:type_error_count}) of the human labels on both datasets: On BetterTypes4Py and InferTypes4Py, the average coherence error per human annotation is 0.019 and 0.045, respectively.


\subsection{\label{sec:type_normalization} Type Normalization}
To compute the accuracy metrics in \autoref{sec:eval_setup}, we recursively apply the following steps to normalize a Python type: 
\begin{enumerate}[itemsep=-0.4ex]
   \item Rewrite any \code{Optional[T]} to \code{Union[T,None]}.
   \item Sort the arguments of \code{Union} types and flatten any nested \code{Union}s.\\ e.g., rewrite \code{Union[B,Union[C,A]]} into \code{Union[A,B,C]}.
   \item If all type arguments are \code{Any}, drop them all. e.g., rewrite \code{List[Any]} to \code{List}.
   \item Capitalize the names of basic types. e.g., rewrite \code{list} to \code{List}. 
\end{enumerate}

\subsection{\label{sec:hyperparameters} Hyperparameters and Running times}

We initialize our model's weights from the CodeT5 model provided by Huggingface Transformers library~\citep{wolf-etal-2020-transformers} and train our model for exactly one epoch using the library's default optimizer configuration (AdamW with a base learning rate of 2e-5 and a weight decay of 0.01.) During inference, we use beam search with a beam width of 16 and diversity penalty of 1.0. We adaptively set the maximal output sequence length to be $16n + 10$, where $n$ is the number of types to be predicted in the input.

We set the size limit of preamble, usee Context, main code, and user Context to 1000, 2048, 512, 1536, respectively, both during training and test time. Note that preamble uses the space within usee context, so the total maximal input size is 4096.

Training the model took about 11 hours on a single Quadro RTX 8000 GPU with 48GB memory. Performing the two-pass inference on the BetterTypes4Py test set takes about 4 hours, whereas performing a single-pass inference (e.g., UseeToUser) takes about half the time. For comparison, training CodeT5 model on the same machine takes about 3.7 hours, and the corresponding evaluation takes about 0.5 hour.



\begin{table}
   \small
   \centering
   \caption{\label{tab:compare_acc_all} Accuracy comparison on all types (\textbf{common} + \textbf{rare}). }
   \begin{tabular}{l | c:c c c:c | c:c c c: c}
      \toprule
         & \multicolumn{5}{c|}{\normalsize BetterTypes4Py} & \multicolumn{5}{c}{\normalsize InferTypes4Py} \\
         & \multicolumn{1}{c:}{\hh{full}} & \multicolumn{3}{c:}{\hh{adjusted}} & \multicolumn{1}{c|}{\hh{base}} & 
         \multicolumn{1}{c:}{\hh{full}} & \multicolumn{3}{c:}{\hh{adjusted}} & \multicolumn{1}{c}{\hh{base}} \\
         & \hhh{all} & \hhh{all} & \hhh{\small simple} & \hhh{\scriptsize complex} & \hhh{all} & 
         \hhh{all} & \hhh{all} & \hhh{\small simple} & \hhh{\scriptsize complex} & \hhh{all} \\
      \midrule
      Type4Py     & \na   & 34.52 & \small{35.87} & \small{19.68} & 35.61 &
                     \na   & 21.11 & \small{22.35} & \small{ 9.61} & 22.64 \\
      HiTyper     & 42.53 & 41.95 & 44.86 & 19.02 & 47.88 &
                     34.83 & 32.50 & 35.11 & 11.40 & 39.23 \\
      CodeT5      & 67.07 & 67.47 & \small{72.12} & \small{44.05} & 73.44 &
                     68.80 & 68.95 & \small{75.14} & \small{54.04} & 77.78 \\
      \toolname{} & \best{71.89} & \best{73.02} & \best{77.07} & \best{49.72} & \best{78.87} &
                     \best{71.98} & \best{73.15} & \best{77.16} & \best{62.66} & \best{80.20} \\
      \bottomrule
   \end{tabular}
\end{table}


\subsection{\label{sec:type4py_discussion} Why Type4Py has much lower performance on our dataset} We have discussed our experiments with the authors of Type4Py and believe that the discrepancy we observe is likely due to the combination of two reasons: First, while our evaluation only counts the type annotations on all top-level APIs, \citet{mir2022type4py} includes all local variables in their evaluation as well. Second, while we only evaluate on human annotations, they also include machine-inferred type annotations (via the type checker Pyre) as ground-truth labels. As a result, the distributions of labels reported by the two papers are significantly different: in our setting, function annotations (parameters + return types) constitute the majority (87\%) of the labels, whereas in \citet{mir2022type4py}, their portion is significantly smaller, merely 18\%. This suggests that their label set is likely inflated by simple labels inferrable from the type checker, which explain the performance drop when evaluated on our datasets. 

\subsection{\label{sec:type_error_count} Measuring Type Coherence using Type Errors}
To estimate the type coherence (\autoref{sec:ablation}), we call the type checker \code{MyPy}\footnote{\href{http://mypy-lang.org/}{http://mypy-lang.org/}.} on codebases annotated with the types predicted by the model. Since not all errors reported by \code{MyPy} are type errors or are related to type coherence, we only count the errors with the following 5 error codes, whose meaning according to \code{MyPy}'s documentation are:
\begin{itemize}[leftmargin=*]
   \item \textbf{\code{attr-defined}} checks that an attribute is defined in the target class or module when using the dot operator.
   \item \textbf{\code{arg-type}} checks that argument types in a call match the declared argument types in the signature of the called function.
   \item \textbf{\code{return-value}} checks that the returned value is compatible with the type signature of the function.
   \item \textbf{\code{assignment}} checks that the assigned expression is compatible with the assignment target.
   \item \textbf{\code{name-defined}} checks that a name is defined in the current scope.
\end{itemize}
Note that this metric does have its limitations. One undesired property we found is that it can favor predicting a non-existing type over an incorrect type. For instance, when the model predicts an incorrect type on a function, the type checker will check all the usages of that function against this declared type and will thus likely report multiple errors. However, if the model predicts a non-existing type, the type checker will only report a single \code{name-defined} error at the declaration site and will skip checking its usages. This effect has caused the No Preamble variant in \autoref{tab:perf_ablation} to have a lower error count than other two other variants since it tends to predict a lot more non-existing types. But we have verified that it was not the cause of the type error count improvement by our two-pass iterative decoding model. 

\begin{table}
   \small
   \centering
   \caption{\label{tab:perf_ablation_detailed} Detailed comparison of different model modifications on BetterTypes4Py. All models are retrained with the corresponding input format. Accuracy is measured on all types.}
   \begin{tabular}{l | c:c c c:c | c}
      \toprule
      & \multicolumn{5}{c|}{\normalsize Accuracy} &  \\
         \multicolumn{1}{l|}{\normalsize Modification} & \multicolumn{1}{c:}{\hh{full}} & \multicolumn{3}{c:}{\hh{adjusted}} & \multicolumn{1}{c|}{\hh{base}} & \multicolumn{1}{c}{\normalsize Type Error} \\
         & \hhh{all} & \hhh{all} & \hhh{\small simple} & \hhh{\scriptsize complex} & \hhh{all} \\
      \midrule
      No Preamble    & 63.03 & 64.20 & 71.22 & 33.51 & 69.98   & 6067 \\
      No Usees       & 67.70 & 67.15 & 70.27 & 48.09 & 73.01   & 7053 \\
      No Users       & 70.22 & 71.20 & \best{77.87} & 41.34 & 77.26   & 7332 \\
      Nonincremental & 71.86 & 72.52 & 77.23 & 47.26 & 78.47   & 5720 \\
      Original (TypeT5) & \best{71.89} & \best{73.02} & 77.07 & \best{49.72} & \best{78.87} & \best{5087} \\
      \bottomrule
   \end{tabular}
\end{table}

\begin{table}
   \small
   \centering
   \caption{\label{tab:perf_decoding_detailed} Detailed comparison of different decoding strategies on BetterTypes4Py. The same \toolname{} model weights are used for different decoding strategies. Accuracy is measured on all types, and the UserGuided decoding is introduced in \autoref{sec:interactive_decoding}.}
   \begin{tabular}{l | c:c c c:c | c}
      \toprule
      & \multicolumn{5}{c|}{\normalsize Accuracy} &  \\
         \multicolumn{1}{l|}{\normalsize Modification} & \multicolumn{1}{c:}{\hh{full}} & \multicolumn{3}{c:}{\hh{adjusted}} & \multicolumn{1}{c|}{\hh{base}} & \multicolumn{1}{c}{\normalsize Type Error} \\
         & \hhh{all} & \hhh{all} & \hhh{\small simple} & \hhh{\scriptsize complex} & \hhh{all} \\
      \midrule
      Independent & 70.87 & 71.68 & 75.99 & 46.92 & 77.58 & 6876 \\
      Random      & 70.68 & 71.66 & 75.68 & 47.89 & 77.65 & 6215 \\
      UserToUsee  & 69.98 & 70.67 & 74.48 & 47.60 & 76.46 & 7415 \\
      UseeToUser  & 71.58 & 72.65 & 76.80 & 48.88 & 78.48 & 6402 \\
      TwoPass (TypeT5) & \best{71.89} & \best{73.02} & \best{77.07} & \best{49.72} & \best{78.87} & \best{5087} \\
      \midrule
      UserGuided  & 78.09 & 79.36 & 83.27 & 58.61 & 84.02 & \na \\
      \bottomrule
   \end{tabular}
\end{table}

\subsection{\label{sec:real_model_input} Real Examples Produced by \toolname{}}
We run \toolname{} on the actual code corresponding to the example shown in \autoref{fig:example_code} and show the obtained preamble (\autoref{fig:ex_preamble}), usee context (\autoref{fig:ex_usees}), main code (\autoref{fig:ex_main_code}), and user context (\autoref{fig:ex_users}) for the \code{ModelWrapper.predict} element. For each type predicted by the model, we indicate whether it is correct with a green or red marker, and if not, also show the ground truth type in red. Note that these outputs were generated by the model in the second pass of the iterative decoding process (described in \autoref{sec:seq_decoding}), so all the elements have already been annotated at least once (but some may have not been annotate the second time).

\begin{figure}
   \centering
   \includegraphics[width=0.82\linewidth]{images/ex_preamble.png}
   \caption{The preamble gathers all the important statements and class headers from the current file. This helps the model see which types are available and where each symbol comes from.}
   \label{fig:ex_preamble}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[width=0.82\linewidth]{images/ex_usees.png}
   \caption{The usee context shows the signature of the elements that are used by the main code or by elements from the user context. By seeing their predicted type signatures, the model can understand the type-level behavior of these definitions without having to dive into their implementation. }
   \label{fig:ex_usees}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[width=0.9\linewidth]{images/ex_main_code.png}
   \caption{The main code is the element that is being annotated by the model at the current decoding step. The model has made two errors in this example, both of which can be directly attributed to the previous two errors made in the usee context (line 102 and line 97). This shows that the model is making coherent predictions according to the context, and such errors can be avoided if the user has corrected the previous errors (as described in \autoref{sec:interactive_decoding}). }
   \label{fig:ex_main_code}
\end{figure}

\begin{figure}
   \centering
   \includegraphics[width=0.9\linewidth]{images/ex_users1.png}
   \includegraphics[width=0.9\linewidth]{images/ex_users2.png}
   \caption{The user context shows two callers of the \code{predict} method from the main code. We see that the model successfully predicts all user-defined types, despite the fact that these are all new classes defined in the current project.}
   \label{fig:ex_users}
\end{figure}



\end{document}
