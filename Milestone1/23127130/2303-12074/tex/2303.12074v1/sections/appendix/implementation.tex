\section{Implementation Details}
\label{sec:impl_details}
Below we share the implementation details of our training and testing pipelines. To promote reproducibility, we will release the code to the public upon acceptance.

\paragraph{Baselines}
\label{sec:baselines}
 We use the R1 regularization \cite{mescheder2018training} across all datasets and models. We replace the sphere-based camera sampling in EG3D~\cite{chan2022efficient} and GIRAFFE~\cite{niemeyer2021giraffe} to allow freely moving cameras, which are significantly challenging to learn from. As stated in the main text, we sampled the cameras by first performing the distance transform and then randomly sampling locations where the distance value is above the threshold. For bedrooms, we orient the camera toward a randomly sampled point within the bed bounding box and for living rooms, we used the largest object in the scene. For GSN~\cite{devries2021unconstrained}, we do not use depth maps as supervision and train on single-view image collections in the same setup as all other models. We train all models using 3,000,000 images for all datasets and models with batch size 32.

From our qualitative evaluations in the main paper, we note that the generated scenes of GIRAFFE \cite{niemeyer2021giraffe} are quite dark.
 We observed that the low brightness of GIRAFFE results can be prevented with higher R1 regularization, however, this led to many samples being completely black and higher FID scores. Hence, we used the regularization factor which obtains the lowest FID score for a fair comparison.
 
 Even though we show several generated scenes using GSN~\cite{devries2021unconstrained}, we would like to note that training GSN on \threedfront living rooms and \kitti, even with hyperparameter tuning was not trivial and resulted in the model collapsing. We hypothesize that GSN requires depth supervision for better performance on these datasets.


 \paragraph{Training Details}
\label{sec:train_details}
We build our model on top of the EG3D~\cite{chan2022efficient} pipeline, hence
we follow their implementation protocol and hyperparameters unless otherwise stated. We
use a discriminator learning rate of 0.002 and a generator learning rate of 0.0025 with Adam optimizer using $\beta_1 = 0$, $\beta_2 = 0.99$, and $\epsilon = 10^{-8}$. Our mapping network transforms a 512 latent code vector into an intermediate latent code with 2 fully connected layers of dimension 512. We do not apply any pose conditioning to the generator or the discriminator networks. As stated above, we abandon the sphere-sampling of camera locations. Our model takes around 2 days to converge using 4 NVIDIA V100 with 32 GB memory. 

 \paragraph{Semantic Layout Details}
\label{sec:layout}
As mentioned in the main document, we process datasets-dependent layouts $\textbf{L}$ as conditional inputs to our model.
% 
To prepare the input $\textbf{L}$ we discretize the provided semantic 2D floor plans onto the 2D grids. 
For indoor scenes, i.e., 3D-FRONT bedroom and living room scenes, we simply project the 3D bounding boxes of the scenes onto the ground plane and encode the semantic class of each pixel using a one-hot vector and a binary room layout mask. The semantic feature channels are concatenated with the local coordinates of each bounding box~(origin at the left-top corner of the bounding boxes at their canonical orientations), providing orientation information to the subsequent U-Net. In total we concatenate features comprising i) a binary mask of the room layout (1), ii) local coordinates of each object (3), iii) one-hot embedding of the semantic layout (16), and iv) the global latent noise (512). This results in a feature grid with 532 channels for \threedfront. We directly obtain a semantic floorplan representation for outdoor scenes by rendering the 3D semantic annotations from a top-down view. Hence, the semantic maps are pixel-based as opposed to bounding-box-based. For \kitti we concatenate features comprising i) the top-down rendered semantic layouts encoded as a one-hot feature grid (59) and ii) latent noise (512). This results in a feature grid with 571 channels for \kitti. 


% For \threedfront we concatenate features comprising i) a binary mask of the room layout (1), ii) local coordinates of each object (3), iii) one-hot embedding of the semantic layout (16), and iv) a latent noise (512). This results in a feature grid with 532 channels for \threedfront.
% For \kitti we concatenate features comprising i) the top-down rendered semantic layouts encoded as a one-hot feature grid (59) and ii) latent noise (512). This results in a feature grid with 571 channels for \kitti.
% We attach latent noise to the layout as an input to the encoder, which we found to prevent mode collapse.




\subsection{Architecture Details}

Our U-Net is composed of an encoder and a decoder network, each of which is composed of the building blocks of StyleGAN2.
For the encoder, we use the StyleGAN2 synthesis layers except that we replace  the upsampling with the max-pool downsampling operation and use style-modulation with {\em constant} style code 
(i.e., the encoder network is independent of the sampled style code $\bm{s}$). The downsampling layers are repeated to make the feature resolution $4^2$. We turned off the style modulation with the global latents in order to keep the encoder deterministic and only modulate the decoder part of the U-Net. Our U-Net decoder closely follows the StyleGAN2 architecture and starts with a learnable feature with $4^2$ resolution. We use skip connections to concatenate the encoder features to the intermediate features of the corresponding decoding layer. In contrast to the encoder, we modulate the decoder via FiLM with the per-layer style code that is obtained by processing the global style noise vector $\textbf{s}$ with the StyleGAN2 mapping network.

The discriminator architecture follows that of StyleGAN2, except that we attach a decoder network that is symmetric with the encoder network. The two network components are connected via skip connections as in a typical U-Net.

