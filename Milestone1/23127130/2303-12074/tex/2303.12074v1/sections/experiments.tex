
\section{Experiments}

In this section, we provide extensive evaluations of our model on multi-object datasets and compare the results with the most relevant baselines.
%Moreover, we provide ablation studies that justify our design choices.
Additional results as well as implementation details are provided in the supplementary.

\paragraph{Dataset: 3D-FRONT}
%\paragraph{\threedfront}
We conduct experiments on \threedfront \cite{fu20213d} bedrooms and living rooms, following the same pre-processing steps as \cite{paschalidou2021atiss}.
%We filter the original 8134 bedrooms and 3767 living rooms following the pre-processing protocol of ATISS \cite{paschalidou2021atiss}. Based on the remaining 5996 bedrooms and 2962 living rooms
To define a camera pose distribution, we consider sampling cameras not inside or too close to the objects to encourage that the majority of the scene's content can be captured. 
% To estimate the camera poses used to render the images to
% train our model, we first constrain the possible camera positions in the scene by masking out occupied regions, namely scene areas where no object bounding
% boxes are located.
To this end, we perform distance transform on the object bounding boxes of the layouts and sample camera locations with sufficiently high distance values with constant heights.
 The orientation is set toward a dominant object in the scene, i.e., beds for bedrooms and the largest object in the scene for living rooms.
We filter out the scenes where we cannot sample at least 40 unique camera poses, resulting in a total of~5515 bedrooms and~2613 living rooms. For each scene we render images using BlenderProc \cite{denninger2020blender} at $256^2$ resolution.
To generate the conditioning inputs, we render a top-down view of each scene with bounding boxes, where each box is colored based on its semantic class and its local coordinates.

\paragraph{Dataset: KITTI}
To demonstrate the generation capabilities of our model in more challenging real-world scenarios, we also evaluate our model on \kitti \cite{liao2022kitti}.
To render our training images, we use the ground-truth camera poses and intrinsic matrices.
Since KITTI scenes are unbounded (\ie there are no specific boundaries), for a single scene we extract several "sub-scenes'' of size $\text{50m} {\times} \text{10m} {\times} \text{50m}$
and use them instead for training. Furthermore, we discard scenes where the car is turning either left of right. This results in 37691 scenes in total.
To render the semantic masks, used to condition the generation, we render top-down views of the scene with bounding boxes, where boxes are colored based on their semantic class.


\input{sections/figures/fig_depth.tex}

\input{sections/figures/fig_3dfront.tex}
\input{sections/tables/tab_main.tex}
\input{sections/tables/tab_ablations.tex}

%\subsection{Metrics}
\paragraph{Metrics}
We report the Fr√©chet Inception Distance (FID)~\cite{heusel2017gans} and Kernel Inception Distance (KID)~\cite{binkowski2018demystifying} to measure the realism of the rendered images with respect to the ground truth image distributions. We use 50000 images for \threedfront and the maximum 37691 images for \kitti.

\paragraph{Baselines}
We compare our model with several state-of-the-art methods for 3D-aware image synthesis: GIRAFFE~\cite{niemeyer2021giraffe},  GSN \cite{devries2021unconstrained} and
EG3D~\cite{chan2022efficient}. From our evaluation, we omit GIRAFFE-HD~\cite{xue2022giraffe} as it can only generate single-object scenes and GAUDI~\cite{bautista2022gaudi}, as
the authors have not released any code to train their model.



\paragraph{Quantitative Results}
% 
In Tab.~\ref{table:main}, we provide quantitative evaluations in comparison to the baselines. CC3D demonstrates significant improvements across all metrics and achieves state-of-the-art performance on the scene synthesis task both in indoor and outdoor scenes. 
In comparison to GIRAFFE and GSN, we see that CC3D demonstrates significant improvements with several times smaller FIDs and KIDs, validating that our method better scales to scenes with multiple objects.
Although EG3D shows the most competitive results in comparison to CC3D, our synthesized images are more plausible for both benchmarks.

\paragraph{Qualitative Results}
% 
In Fig. \ref{fig:3dfront} and Fig. \ref{fig:kitti}, we provide qualitative results for \threedfront and \kitti respectively. As also validated quantitatively in Tab.~\ref{table:main}, our model produces high quality and view-consistent images from different camera poses. In comparison to previous approaches, CC3D synthesizes scene compositions which are more realistic due to our scene layout conditioning. While EG3D shows promising texture quality, the lack of compositionality leads to low-quality underlying scene structures, evidenced by the depth map visualization results (see Fig. \ref{fig:depth}). Notably, all previous methods produce unrealistic scenes for the case of living rooms, unlike CC3D which produces coherent scenes. For the case of \cite{niemeyer2021giraffe}, we observe that it fails to produce plausible scenes \ie most generated scenes are almost completely dark, as also noted in \cite{xu2022discoscene,or2022stylesdf}. Additional results are provided in the supplementary materials.

\input{sections/figures/fig_kitti.tex}


\subsection{Empirical Analysis}

\paragraph{Layout Conditioning Improves Scene Quality}
% 
Conditioning the generation process with a semantic layout provides compositional guidance to the model. We observe that training an unconditional version of our model leads to a noticeable loss in visual quality as shown in the worse metric scores in Tab.~\ref{table:ablations}. It also aligns with the fact that our conditional method significantly outperforms the existing unconditional GANs, which highlights the importance of providing input conditioning for compositional scene generation.


\paragraph{3D Field Representations}
% 
In Sec.~\ref{sec:representation}, we described how the existing representations for modeling neural fields have trouble modeling large, multi-object scenes. Instead, our 2D-to-3D extrusion method is efficient for using only 2D convolutions and has a strong geometrical inductive bias. 
Hence, to validate our design choice, we substitute 3D field representation with GSN's ``floorplan" and EG3D's tri-plane representations and observe worse performances than ours (Tab.~\ref{table:ablations}), as expected.


\paragraph{Layout Consistency Loss}
% 
As part of our preliminary 2D experiments, we observed that objects are sometimes missing from the output rendering, in particular when there are too many or small objects. Adding our layout consistency loss (Sec.~\ref{sec: discriminator}) during training,  addresses this issue, as shown in Fig. \ref{fig:semantic}. However, we note that the missing object phenomenon still occurs, especially in living room scenes that contain a lot of objects. We will discuss this phenomenon in the supplementary.
Furthermore, we show that using the layout consistency loss improves visual quality (Tab.~\ref{table:ablations}).



\paragraph{Controllable Generations} We showcase that our model enables controlling the 3D scene generation process and supports various editing operations. In Fig.~\ref{fig:3dfront_3d_edit}, we provide examples
of changing the style of the objects, removing objects from the scene and changing the position of an object in the scene.

\input{sections/figures/fig_3dfront_3d_edit.tex}