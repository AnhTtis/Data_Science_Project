\section{Method}\label{sec:method}

% subsection name (3.0) implied, dropped
% \subsection{Method Overview}
The training of CC3D takes a set of single-view 2D RGB images and a set of top-down semantic layouts.
We do not assume the two image sets to be in 1:1 correspondence.
% : i.e., the RGB and semantic layout images are independent of each other.
During training, we randomly choose 2D semantic layout images and sample style latent codes.
The layout images and style codes are passed to the U-Net-style generator network, which outputs 2D features. The 2D feature map is then reshaped into a 3D feature volume, which can be rendered via volume rendering with a small MLP network and a 2D upsampling network.
The realism of the rendered images is scored by the discriminator network against the set of RGB images, and the system is trained with the standard adversarial loss along with the semantic consistency loss from the top-down views; Fig.~\ref{fig:method} illustrates the overall pipeline.

During inference, we provide a semantic layout image and a style code to the generator to obtain a 3D neural radiance field that can be rendered from an arbitrary camera. Our method allows more control over the generation process compared to existing unconditional GANs, as users can specify the layout with various styles and edit the scene.


% 

% \subsection{Semantic Scene Layout Generation}
% \label{sec:layout}
% Semantic layouts for indoor scenes can be computed in three ways: using designer provided layouts, user-created layouts, or AI-generated layouts. 
% AI-generated layouts can be obtained by scene synthesis methods such as ATISS \cite{paschalidou2021atiss} and would enable an end-to-end generation process. For outdoor scenes, layouts can be extracted from semantic annotations that are projected on the ground.
% In our work, we do not further investigate the layout generation but rather assume that layouts are given as an input to the model based on ground truth labels. Our goal is to convert these semantic layouts into 3D scenes supervised only by single-view image collections. We achieve this by formulating a conditional 3D GAN approach that encodes the given semantic layout to generate 3D scenes. 
% Ultimately, we want the generated 3D scene to be consistent with the input layout, while being able to generate it from random noise defining the appearance of the scene.



% \subsection{2D-Conditioned 3D Neural Fields Generation}
\subsection{Neural field generator}
\label{sec:gen_main}
% 
Our generator network $\textbf{G}(\textbf{L},\textbf{s})$ takes as input a 2D layout image $\textbf{L}$ and a style code $\textbf{s}\in \mathcal{R}^{512}$, sampled from a
unit Gaussian distribution, and generates a 3D neural feature field $\textbf{F}\in \mathbb{R}^{N\times N \times N\times C}$, where $N$ and $C$ correspond to the spatial resolution and channel size. In our experiments, we set $N=128$ and $C=32$.

\paragraph{Layout Conditioning}
\label{sec:conditioning}
The input to our generator is a 2D layout conditioning image $\textbf{L} \in \mathbb{R}^{N\times N\times L}$ that contains information about the scene structure, with $L$ being a dataset-dependent feature dimension.
In contrast to the concurrent work \cite{xu2022discoscene} that uses 3D bounding boxes as conditioning, we choose, to guide our generation with 2D semantic layouts images, that
can be generated with basic computer tools.
% The fact that $\textbf{L}$ is a 2D image makes it accessible for everyday users to sketch their desired output scene layout with basic computer tools.
% This is in contrast with a concurrent work \cite{xu2022discoscene} that uses 3D bounding boxes as conditioning.
While there are many ways to obtain the semantic layouts for training and testing, e.g., designer-provided, user-created, or AI-generated layouts, we mainly use the test set layouts provided by the datasets for our experiments.
The feature channels of an input layout are composed of the one-hot encoding of semantic classes and additional information depending on the dataset, which we will describe in detail in the supplementary. \despi{I think the layout conditioning is not properly explained. We need a sentence to clearly stata what is this representation.}
% In our experiments, we showcase convenient editable scene generation thanks to the 2D nature of our layout inputs. 
% 
As illustrated in Fig.~\ref{fig:3dfront_2d}, conditiong the generation on a 2D semantic layout allows us to conveniently control the structure and the style of a scene; see supplementary for more details.

\input{sections/figures/fig_3dfront_2d.tex}


\paragraph{Neural Field Architecture}
\label{sec:representation}
While the neural implicit representations have greatly advanced 3D generative modeling, the original coordinate-based  implicit representations (Fig.~\ref{fig:representations}.a) required using large multi-layer perceptrons (MLPs), resulting in high memory usage during rendering. 

As such, several implicit-explicit hybrid representations have been proposed to pre-load the computational overhead to the generation of explicit features on regular grids. Neural field values of query points are obtained by linear feature interpolations, followed by processing with smaller MLPs. One such method involves directly generating 3D features using 3D CNNs, as demonstrated in VolumeGAN, but it comes with an intractable $N^3$ compute complexity making it unsuitable for our use case.

On the other hand, some recent techniques suggest adopting planar grids to achieve efficient generation. For example, GSN \cite{devries2021unconstrained} introduces 2D "floorplan" features, while EG3D \cite{chan2022efficient} uses tri-plane features. Although these representations show promising results, it is difficult to apply them directly to our 2D-3D translation problem (elaborated later in the section), which motivated us to explore an alternative approach. \despi{I kind of feel that this and the previous paragraph should go to the related work section. It feels a bit out of place here.}

We develop a new hybrid representation that allows efficient  generation of large-scale scenes while maintaining strong physical inductive bias. We first generate a 2D feature map, using a 2D U-Net architecture. Then, instead of separating the 2D features into tri-plane features, as in \cite{chan2022efficient}, we extrude them into 3D volumetric features via reshaping operation (Fig.~\ref{fig:representations}.d). The new scheme leverages highly efficient 2D operations to output 2D feature images, whose individual pixels encode vertical scene information in the height dimension. Therefore, applying local convolution operations on the feature image \todo{allows physically associating} neighboring features, which is not the case for tri-plane representations. 


% For the case of the floorplan representation, the neural field is defined by a MLP that takes the concatenation of the floorplan projected features and height coordinates. Since the height-wise information needs to be ``generated" by the MLP based on the projected 2D features, the heavy lifting is still done by the MLP network, leading to a huge computation burden of a large MLP. 

% On the other hand, the use of tri-plane features leads to efficient 3D modeling since the 3D information of all axes is encoded in the three planar features, allowing a dramatic reduction of the MLP network size. However, these tri-plane features are jointly generated from a regular 2D CNN and reshaped into three separate planes, leading to the processing of the three planes with very different physical positions with the local convolution operations. Moreover, as the scale of the scenes increases, the 2D plane features become less descriptive since completely different objects in the scene share the same plane-projected features.


% Therefore, we devise a new representation scheme that  allows using a small MLP while maintaining strong physical inductive bias. We first generate a 2D feature map, which can be done efficiently via standard 2D CNNs. Then, instead of separating the 2D features into tri-plane features, we \todo{extrude} \AT{this will be confusing to many, as floorplan features are what you would call extrusion?!} them into 3D volumetric features via reshaping operation.
% \AT{move this with rest of red text after the method is fully described?}
% \todo{This representation is most closely related to the ``floorplan" representation, where we can consider the height coordinate features to be pre-computed on discrete height locations and stored in a 3D grid structure, allowing a small MLP network.}
% Moreover, since each pixel of our 2D feature image encodes a column of features in the height dimension, applying local convolution operations associate physically neighboring features, which is not the case for tri-plane representations. 

% \AT{drop? adds nothing}
% \todo{We illustrate the aforementioned neural implicit feature representations in Fig. X.
% We empirically validate the superiority of our proposed representation on modeling large, compositional scenes through experimental comparisons in Sec. X.}

%Next, we define a feature representation that is able to convert the input layout into a 3D neural field. One straight-forward approach would be to define the conditional input in 3D space by discretizing 3D semantic annotations as a voxel grid \cite{xu20223d, schwarz2022voxgraf}. However, this significantly limits the feature grid resolution due to computationally expensive 3D convolutions. While low feature grid resolutions are sufficient to model single object scenes \cite{xu20223d, schwarz2022voxgraf}, we observed low quality renderings in early experiments using voxel grids. We hypothesize that the model struggles to disentangle different objects in a scene because of the low resolution layout conditioning. Consequently, we decided not to further follow this approach but rather opt for a more efficient representation based on 2D layouts.

%Recently, tri-planes \cite{peng2020convolutional, chan2022efficient} have proven to be an efficient and expressive representation for 3D GANs. Yet, using tri-planes conditioned on a 2D layout is difficult to interpret as all three planes are jointly generated. This prevents clear correspondence between input conditioning and output planes. Another computationally efficient approach is the floorplan representation \cite{devries2021unconstrained}. Here, coordinates are projected onto a 2D floorplan to sample features which can be used to condition a radiance field. However, this representation allocates the main computation to the decoder MLP instead of the efficient style-based generator. Our goal is to build an efficient generator by using a small decoder MLP, similarly to EG3D \cite{chan2022efficient}. As shown in our ablations in Sec. \ref{sec:ablations}, using a floorplan representation leads to low quality results. This can be partially compensated by increasing the MLP capacity as in GSN \cite{devries2021unconstrained}, though leading to significantly higher computational demands and training time. Based on these observations, we propose an alternative approach that synthesizes a 3D feature volume from the 2D input layout using an efficient 2D style-based generator. 

\paragraph{Layout-Conditioned 3D Generation}
\label{sec:generation}
Our conditional generator $\textbf{G}(\textbf{L},\textbf{s})$ is composed of a U-Net backbone $\textbf{U}$ that generates a 2D feature image, followed by the \textit{extrusion} operator $E$ that reshapes it into a 3D feature grid:
% 
\begin{equation}
\textbf{G}(\textbf{L},\textbf{s})=E\circ \textbf{U}(\textbf{L},\textbf{m}(\textbf{s})),
\end{equation}
% 
where $\textbf{m}$ is a mapping network as used in StyleGAN2 for FiLM conditioning \cite{perez2018film}. We use a ``StyleGAN-like" U-Net architecture composed of encoder and decoder networks with StyleGAN synthesis layers with style modulations. We use skip connections to concatenate the encoder features to the intermediate features of the corresponding decoding layer. Refer to supplementary for details.

% Our U-Net is composed of an encoder and a decoder network, each of which is composed of the building blocks of StyleGAN2.
% For the encoder layers, we use the StyleGAN2 synthesis layers except that we replace \AT{why is this a good idea?} the upsampling with the max-pool downsampling operation and use style-modulation with {\em constant} style code \AT{why constant style code? what is the alternative? why is this better?}
% (i.e., the encoder network is independent of the sampled style code $\bm{s}$). The downsampling layers are repeated to make the feature resolution $4^2$. Our U-Net decoder closely follows the StyleGAN2 architecture and starts with a learnable feature with $4^2$ resolution. We use skip connections to concatenate the encoder features to the intermediate features of the corresponding decoding layer. In contrast to the encoder, we modulate the decoder with the per-layer style code that is obtained by processing the global style noise vector $\textbf{s}$ with a StyleGAN2 mapping network.


% The goal of our style-based generation is to synthesize a 3D neural field that can be rendered from different camera poses. Our style-based generator takes as an input the semantic layout and processes it with a 2D U-Net \cite{ronneberger2015u} architecture. The purpose of the encoder is to compute a feature representation of the semantic layout that can be decoded by a style-based network.
% For each encoding stage we use building blocks consisting of a StyleGAN2 \cite{karras2020analyzing} synthesis layer and a subsequent max-pooling operation. Encoding stages are repeated until the feature resolution is downsampled to $4^2$. We do not use noise injection in the encoder and modulate the convolutions with constants.
% Moreover, we concatenate latent codes to each semantic label of the input layout, which we found to prevent mode collapse during training.
% Our U-Net decoder follows the StyleGAN2 architecture by starting at a learnable constant with $4^2$ resolution. We use skip connections to concatenate the encoder features to the input features of the respective decoding stage. In contrast to the encoder, we modulate the decoder with an intermediate latent noise vector obtained by a StyleGAN2 mapping network. 

At the last layer of the U-Net, we have a single convolutional layer that increases the number of channels to a multiple of the height dimension of our target 3D feature volume. Then we convert the 2D output of $\textbf{U}$ to be the 3D feature via extrusion $E$, which is done by simply reshaping the channel dimension of $\textbf{U}$ output ($N\times C$) into $N\times C,$ giving height dimension to the 2D feature map. In contrast to voxel-based approaches, we compute a 3D feature grid only at the last layer while keeping our intermediate features in 2D, using computationally efficient 2D convolutions only.
% \AT{this was already stated earlier}
% We set the feature spatial resolution to $N=128$ which we found expressive enough for our setups.

\paragraph{Comparing to Existing Representations}
Recent planar-based neural field representations can be easily combined with 2D architectures for 3D generation. However, as stated earlier, the existing approaches fall short in representing large-scale scenes.\despi{Should we say large-scale? Maybe complex is better.}

For the case of the floorplan representation \cite{devries2021unconstrained} (Fig.~\ref{fig:representations}.b), the neural field is defined by an MLP that takes the concatenation of the floorplan projected features and height coordinates. Since the height-wise information needs to be ``generated" by the MLP based on the projected 2D features, the heavy lifting is still done by the MLP network, leading to prohibitively large MLP size \cite{devries2021unconstrained}. 

Tri-plane features \cite{chan2022efficient} (Fig.~\ref{fig:representations}.c) encode 3D information of all axes, allowing a dramatic reduction of the MLP network size. However, these features are jointly generated from a standard 2D CNN and reshaped into three separate planes, leading to the processing of the three planes with very different physical positions with the local convolution operations. Moreover, as the scale of the scenes increases, the 2D plane features become less descriptive since completely different objects in the scene share the same plane-projected features.

In contrast, our 2D-to-3D extrusion strategy enables us to leverage 2D CNNs and a compact MLP network to generate voxel features that avoid the physically unrealistic properties of tri-planes when modeling multi-object scenes, as validated empirically in our experimental evaluation.


\subsection{Rendering and upsampling}
\label{sec:render}
Given the generated feature volume $\textbf{G}(\textbf{L},\textbf{s}),$ we can query continuous neural field value at any query 3D position by passing the tri-linear interpolated feature to a small MLP~(i.e., consisting of a single hidden layer of 64 hidden dimension and softplus activation functions).
The outputs of the MLP $\textbf{p}$ are a scalar density and a 32-dimensional feature, where the first three channels are interpreted as RGB colors.
We render the image using volumetric rendering as follows
\begin{equation}
\mathcal{I}^{v}_\gamma = \mathcal{R}(\textbf{G}(\textbf{L},\textbf{s}), \textbf{p}, \gamma)
\end{equation}
from a camera viewpoint $\gamma$. We use 48 points along the ray sampled with stratified sampling and another 48 points obtained with importance sampling \cite{mildenhall2020nerf}.

\paragraph{Upsampling}
Volume rendering at our target image resolution of $256^2$ is computationally expensive, so we use the popular 2D super-resolution module and adopt the dual discrimination training scheme of EG3D \cite{chan2022efficient}, which is known to encourage multi-view consistent renderings. We set the volume rendering resolution to $64^2$ which provides a reasonable trade-off between computational costs and multi-view consistency.
The upsampled image
% 
\begin{equation}
\mathcal{I}_\gamma=\textit{upsample}(\mathcal{I}^{v}_\gamma,\textbf{s})
\end{equation}
is a function of the volume rendered image $\mathcal{I}^v_\gamma$ and the style code $\textbf{s}$, as we use the StyleGAN2 network for upsampling. 


% \AT{this goes in the experiments / analysis section?}
% \todo{While EpiGRAF \cite{skorokhodov2022epigraf} and SinGRAF \cite{son2022singraf} adopted patch-based training schemes to discard the 2D upsampling, we were unable to get a similar scheme to converge in our setup, even with careful hyperparameter tuning. We hypothesize that patch-based training is challenging on unaligned, complex scenes with freely moving cameras and leave further investigation as future work.}

% \AT{isn't there a chunk of fig.2 that is not described? the sampled features that feed into the conditioning of the stylegan discriminator?}

\subsection{Discriminator Architecture}\label{sec: discriminator}
Our generator $\mathbf{G}$ is trained with an adversarial loss that involves co-training a discriminator $\mathbf{D}(\mathcal{I}_\gamma,\mathcal{I}_\gamma^v)$, which takes real and fake images to predict their labels. Our discriminator architecture follows that of StyleGAN2 \cite{karras2020analyzing} and takes input as the concatenation of two $256^2$ images following the dual-discrimination scheme \cite{chan2022efficient}.

\paragraph{Enforcing Layout Consistency}
While layout conditioning provides compositional guidance to the generator, we oftentimes observe that some objects from the input layout are missing from the final rendering, as shown in Fig.~\ref{fig:semantic}.To address this, we introduce a semantic layout consistency loss during training that encourages that the generated scene features rendered from the top-down view are consistent with the input layouts. 

Specifically, let us define the $xz$-plane as the floorplan and the $y$-axis the up vector. We want create a 2D image $\textbf{S}$ on the $xz$-plane that summarizes the generated feature $\mathbf{F}$ from the top-down view. For each pixel in $\textbf{S}$ we sample $k$ number of equidistant points along the height ($y$) axis. Then, we perturb the sampled points with a small Gaussian noise and extract features from those points with tri-linear interpolation. The resulting image $\textbf{S}$ has dimension $N\times N \times (k\times C)$, which is passed to a segmentation U-Net that predicts a semantic label for each pixel. Here, we reuse the discriminator $\textbf{D}$ and attach a decoder network to convert it to a U-Net structure. Besides the adversarial process, $\textbf{D}$ additionally takes $\textbf{S}$ and outputs semantic segmentation. $\textbf{D}(\textbf{S})$ is then compared against the input label map $\textbf{L}$ with the standard cross entropy loss, denoted $\mathcal{L}_{layout}$, which is back-propagated for training.

\subsection{Training}
We build on top of recent 3D GAN techniques to train our generator by encouraging the neural field renderings from sampled camera viewpoints via adversarial losses.
Specifically, we sample style code $\textbf{s}$ and camera pose $\gamma$ from a prior distribution $p(\gamma)$ and render through the generated neural fields to obtain a fake image $\mathcal{I}_{\gamma}(\textbf{L},\textbf{s}).$
The discriminator network $\mathbf{D}$ takes as input the fake/real images, and outputs the predicted labels.
The two networks are trained via the standard min-max optimization~\cite{goodfellow2014generative}.



\paragraph{Training Objectives}
Our overall training objective comprises the adversarial training loss with R1 regularization loss~\cite{mescheder2018training} and our proposed layout consistency loss of Sec.~\ref{sec: discriminator}, which are weighted equally:
% 
\begin{equation}
\mathcal{L}=\lambda\mathcal{L}_{adv}+\lambda\mathcal{L}_{R1}+\lambda\mathcal{L}_{layout},
\end{equation}
% 
which we minimize by updating the weights of generator~$\mathbf{G}$, U-Net backbone~$\mathbf{U}$, MLP network~$\mathbf{p}$, and the extended U-Net discriminator~$\mathbf{D}.$



% \subsection{Implementation Details (tentative)}
% To prepare the input $\textbf{L}$ we discretize the provided semantic 2D floor plans onto the 2D grids. For indoor scenes, i.e., 3D-Front bedroom and living room scenes, we simply project the 3D bounding boxes of the scenes onto the ground plane and encode the semantic class of each pixel using a one-hot vector and a binary room layout mask. The semantic feature channels are concatenated with the local coordinates of each bounding box (origin at the left-top corner of the bounding boxes at their canonical orientations), providing orientation information to the subsequent U-Net. For outdoor scenes, we directly obtain a semantic floorplan representation by rendering the 3D semantic annotations from a top-down view.

% Our U-Net is composed of an encoder and a decoder network, each of which is composed of the building blocks of StyleGAN2.
% For the encoder layers, we use the StyleGAN2 synthesis layers except that we replace \AT{why is this a good idea?} the upsampling with the max-pool downsampling operation and use style-modulation with {\em constant} style code 
% (i.e., the encoder network is independent of the sampled style code $\bm{s}$). The downsampling layers are repeated to make the feature resolution $4^2$. Our U-Net decoder closely follows the StyleGAN2 architecture and starts with a learnable feature with $4^2$ resolution. We use skip connections to concatenate the encoder features to the intermediate features of the corresponding decoding layer. In contrast to the encoder, we modulate the decoder with the per-layer style code that is obtained by processing the global style noise vector $\textbf{s}$ with a StyleGAN2 mapping network.

% While the continuous neural implicit representations have greatly advanced the quality of 3D generative modeling, the original coordinate-based  implicit representations (Fig.~\ref{fig:representations}.a) required using large multi-layer perceptrons (MLPs) to output volumetric field values, resulting in huge memory usage during rendering. 
% % 
% To tackle this problem, several implicit-explicit hybrid representations have been proposed. These approaches aim to pre-load the computational overhead to the generation of explicit features, often by generating features on the plane or voxel grids. Neural field values of query points are then obtained by linear feature interpolations, followed by processing with smaller MLPs. One such method involves directly generating 3D feature volumes using 3D CNNs, as demonstrated in VolumeGAN. However, this technique has a $N^3$ compute complexity that is intractable on large-scale scenes, which makes it unsuitable for our use case.

% Alternatively, recent approaches propose representing the scene features on planar grids for efficient generation, such as the use of 2D ``floorplan" features of GSN \cite{devries2021unconstrained} and tri-plane features of EG3D \cite{chan2022efficient}. 

% For the case of the floorplan representation, the neural field is defined by a MLP that takes the concatenation of the floorplan projected features and height coordinates. Since the height-wise information needs to be ``generated" by the MLP based on the projected 2D features, the heavy lifting is still done by the MLP network, leading to a huge computation burden of a large MLP. 

% On the other hand, the use of tri-plane features leads to efficient 3D modeling since the 3D information of all axes is encoded in the three planar features, allowing a dramatic reduction of the MLP network size. However, these tri-plane features are jointly generated from a regular 2D CNN and reshaped into three separate planes, leading to the processing of the three planes with very different physical positions with the local convolution operations. Moreover, as the scale of the scenes increases, the 2D plane features become less descriptive since completely different objects in the scene share the same plane-projected features.