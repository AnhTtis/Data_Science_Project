\section{Introduction}
\label{sec:intro}


Recently, we have witnessed impressive progress in 3D generative technologies, including generative adversarial networks (GANs)~\cite{goodfellow2014generative} that have emerged as a powerful tool for automatically creating realistic 3D content.
 Despite their impressive capabilities, existing 3D GAN-based approaches have two major limitations. First, they typically generate the entire scene
 from a single latent code, ignoring the compositional nature of multi-object scenes, thus struggling to synthesize scenes with multiple objects, as shown in Fig.~\ref{fig:low_quality}.
 Second, their generation process remains largely uncontrollable, making it non-trivial to enable user control.
 While some works \cite{cai2022pix2nerf, lin20223d} allow conditioning the generation of input images via GAN inversion, this optimization process can be time-consuming and prone to local minima.

In this work, we introduce a {\bf{C}}ompositional and {\bf{C}}onditional {\bf{3D}} generative model (\textbf{CC3D}), that generates plausible 3D-consistent scenes with multiple objects,
while also enabling more control over the scene generation process by conditioning on semantic instance layout images, indicating the scene structure (see Fig.~\ref{fig:teaser}). Our approach rhymes with the 2D image-to-image translation works \cite{isola2017image,epstein2022blobgan} that conditionally generate images from user inputs: CC3D generates 3D scenes from 2D user inputs~(\ie scene layouts).

\input{sections/figures/fig_baseline_failures}

To train CC3D we use a set of single-view images and top-down semantic layout images, such as 2D labelled bounding boxes of objects in a scene (\eg Fig.~\ref{fig:teaser}). Our generator network takes a 2D semantic image as input that defines the scene layout and outputs a 3D scene, whose top-down view matches the input layout in terms of object locations.

The key component of our approach is a 2D-to-3D translation scheme that efficiently converts the 2D layout image into a 3D neural field. Our generator network is based on a modified StyleGAN2~\cite{karras2020analyzing} architecture that processes the input 2D layout image into a 2D feature map.
The output 2D feature map is then reshaped into a 3D feature volume that defines a  neural field which can be rendered from arbitrary camera views.
Similar to existing 3D-aware generative models \cite{schwarz2020graf, niemeyer2021giraffe, chan2022efficient}, we train the generator to produce realistic renderings of the neural fields from all sampled viewpoints. In addition, we enforce a semantic consistency loss that ensures the top-down view of the 3D scene matches the semantic 2D layout input. 


\input{sections/figures/fig_method.tex}

We evaluate CC3D on the 3D-FRONT~\cite{fu20213d} bedroom and living room scenes and the KITTI-360 dataset~\cite{liao2022kitti} that contains more challenging outdoor real-world scenes.
Our evaluations demonstrate that existing 3D generative models, such as EG3D~\cite{chan2022efficient} and GSN~\cite{devries2021unconstrained}, produce low-quality 3D scenes, as illustrated in Fig.~\ref{fig:low_quality}.
In comparison, the compositional generation process and the new intermediate 3D feature representation of CC3D significantly improve the fidelity of the synthesized 3D scenes on both datasets, opening the door for realistic multi-object scene generations.
