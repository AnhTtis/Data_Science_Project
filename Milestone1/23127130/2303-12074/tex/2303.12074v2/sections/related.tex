
\section{Related Work}
\paragraph{2D Image Synthesis}
GANs~\cite{goodfellow2014generative} have been extensively utilized to generate photorealistic images \cite{karras2019style, karras2020analyzing, karras2021alias, brock2018large, sauer2022stylegan}, perform image-to-image translation \cite{isola2017image, zhu2017unpaired, choi2018stargan}, and image editing \cite{wang2018high, shen2020interpreting, ling2021editgan}. Recently, compositional approaches \cite{hudson2021generative, arad2021compositional} have also been explored in the context of image generation. Similar to our work, GANformer2~\cite{arad2021compositional} also divides the generation process into two steps: planning and execution. In our work, we guide the {\em 3D} generation process using semantic layouts and demonstrate
that CC3D can render multi-view consistent images of multi-object scenes.

\paragraph{3D Object Generation}
To scale 2D GANs to 3D domain,
many recent works explored combining image generators with 3D representations.
These models are supervised only with unstructured image collections along with a pre-defined camera distribution. While earlier works \cite{nguyen2019hologan, nguyen2020blockgan, schwarz2020graf, chan2021pi, niemeyer2021campari} provided limited visual fidelity and geometric accuracy, recently, several works tried to
address these limitations. The majority of these approaches \cite{zhou2021cips, deng2022gram, gu2022stylenerf, or2022stylesdf, chan2022efficient, xu20223d, skorokhodov2022epigraf,gao2022get3d} use a style-based generator \cite{karras2020analyzing} to synthesize a neural field which can be used for volume rendering \cite{max1995optical}. Although these approaches can produce high quality images for single-object scenes, they fail to scale to complex scenes with multiple objects. In this work, we also employ a style-based generator in combination with volume rendering but as our model explicitly models the compositional nature of 3D scenes, it can successfully generate plausible indoor and outdoor 3D scenes.

\paragraph{Multi-Object Generation}
Our work is closely related to recent approaches that model scenes using 3D-aware image generators
\cite{niemeyer2021giraffe, xue2022giraffe}. Among the first, GIRAFFE \cite{niemeyer2021giraffe} proposed to represent scenes using multiple locally defined NeRFs. However, while \cite{niemeyer2021giraffe} can be efficiently applied on scenes containing only a few objects with limited texture variation, such as the CLEVR \cite{johnson2017clevr} dataset, it fails to generalize to more complex scenes. To improve the
visual quality of \cite{niemeyer2021giraffe},
GIRAFFE-HD \cite{xue2022giraffe} employed a style-based generator. Even though this allows their model to composit multiple objects of the same class, e.g., cars, into a single scene at inference time, learning compositional scene generation from multi-object scenes of different classes remains an open problem.

\paragraph{Large-Scale Scene Generation}
Plan2Scene \cite{vidanapathirana2021plan2scene} focuses on the task of converting a floorplan accompanied by a sparse set of images into a textured mesh for the entire scene.
Although their representation is compositional by construction, \cite{vidanapathirana2021plan2scene} is not generative and requires  multi-view supervision.
Closely related to our work, another line of research \cite{devries2021unconstrained, bautista2022gaudi} aims at generating large-scale scenes using locally conditioned neural fields. Unlike previous approaches that sample camera poses from a sphere targeted towards the origin, constraining them to $S O(3)$, GSN \cite{devries2021unconstrained} considers scene generation conditioned on a freely moving camera defined in $S E(3)$.
Although this setup permits generating scenes from arbitrary viewpoints, it makes training significantly harder, as datasets are not aligned and the range of possible camera poses drastically increases. 
GAUDI \cite{bautista2022gaudi} further improves the quality by disentangling camera poses from geometry and appearance.
Unlike GAUDI \cite{bautista2022gaudi} that assumes multi-view input images with known camera poses, our model can be trained using unstructured set of images.

\paragraph{Indoor Scene Generation}
Recently, several works \cite{wang2018deep, ritchie2019fast, wang2021sceneformer, paschalidou2021atiss,wei2023lego} proposed to pose the scene generation task
as an object layout prediction problem. For example, ATISS \cite{paschalidou2021atiss} uses an autoregressive transformer to generate synthetic indoor environments as an unordered set of objects. LEGO-Net \cite{wei2023lego} learns to iteratively refine random object placements to generate realistic furniture arrangements.  These works represent a scene layout as a set of 3D labeled bounding boxes, which can be replaced with textured meshes from a dataset of assets. In contrast, we rely on a GAN to learn a mapping between a 2D compositional scene layout to a 3D scene, without having to rely on object retrieval to produce 3D objects. We see our work as an orthogonal work to \cite{wang2018deep, ritchie2019fast, wang2021sceneformer, paschalidou2021atiss} as they can be used to generate scene layouts, which in turn can be used as our conditioning.


\paragraph{Concurrent Works}
Several concurrent works explored extending 3D GANs to more complex scenarios. 
3DGP \cite{skorokhodov20233d} tackles non-aligned datasets by incorporating depth estimation and a novel camera parameterization, but their model focuses only on single objects.
SceneDreamer \cite{chen2023scenedreamer} generates unbounded landscapes from 2D image collections and semantic labels. However, their model is supervised with a ground truth height field, whereas we learn the density field only from 2D image collections.
InfiniCity \cite{lin2023infinicity} synthesizes large-scale 3D city environments but requires expensive annotations such as CAD models.
Similar to ours, pix2pix3D~\cite{deng20233d} generates 3D objects given a 2D semantic map, but it only focuses on single-object scenes.
In concurrent work, DisCoScene \cite{xu2022discoscene} investigates compositional scene generation with layout priors using single-view image collections. Their approach follows \cite{niemeyer2021giraffe} and generates each object and the background independently. 
Unlike our work, DisCoScene conditions the scene generation on 3D layout priors, as opposed to 2D layouts, and assumes that the per-object attributes (\ie size, pose) are sampled from a pre-defined prior distribution. Instead, we do not assume this type of supervision. Moreover, unlike \cite{xu2022discoscene}, we explore rendering from freely moving cameras as opposed to cameras on a sphere. 