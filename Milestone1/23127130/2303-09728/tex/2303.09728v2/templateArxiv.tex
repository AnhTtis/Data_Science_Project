\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{float}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{bm} 
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 



  
%% Title
\title{The Cascaded Forward Algorithm for Neural Network Training}

\author{
	Gongpei Zhao, Tao Wang, Yidong Li, Yi Jin, Congyan Lang \\
	Beijing Jiaotong University \\
  \texttt{\{csgpzhao, twang, ydli, yjin, cylang\}@bjtu.edu.cn} \\
  %% examples of more authors
   \And
  Haibin Ling \\
  Stony Brook University \\
  \texttt{hling@cs.stonybrook.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
Backpropagation algorithm has been widely used as a mainstream learning procedure for neural networks in the past decade, and has played a significant role in the development of deep learning. However, there exist some limitations associated with this algorithm, such as getting stuck in local minima and experiencing vanishing/exploding gradients, which have led to questions about its biological plausibility. To address these limitations, alternative algorithms to backpropagation have been preliminarily explored, with the Forward-Forward (FF) algorithm being one of the most well-known. In this paper we propose a new learning framework for neural networks, namely \textbf{Ca}scaded \textbf{Fo}rward  (\textbf{CaFo}) algorithm, which does not rely on BP optimization as that in FF. Unlike FF, our framework directly outputs label distributions at each cascaded block, which does not require generation of additional negative samples and thus leads to a more efficient process at both training and testing. Moreover, in our framework each block can be trained independently, so it can be easily deployed into parallel acceleration systems. The proposed method is evaluated on four public image classification benchmarks, and the experimental results illustrate significant improvement in prediction accuracy in comparison with the baseline. Our
code is available at: \url{https://github.com/Graph-ZKY/CaFo}.
\end{abstract}



\section{Introduction}

Backpropagation (BP) algorithm~\cite{rumelhart1986learning} is a powerful technique that has proven to be effective for training deep neural networks on a wide range of tasks, including image classification~\cite{chen2021crossvit}, semantic segmentation~\cite{strudel2021segmenter} and object detection~\cite{ren2015faster}. The algorithm's ability of adjusting the weights based on the error between the prediction and the ground truth allows the network to learn and improve over time, making it a cornerstone of deep learning and artificial intelligence. Nevertheless, despite its effectiveness, BP also suffers from several limitations in practical applications. These limitations include the problems of local minima~\cite{baldi1989neural}, vanishing/exploding gradients~\cite{hanin2018neural}, overfitting~\cite{lawrence2000overfitting}, slower convergence and non-convex optimization~\cite{dauphin2014identifying}, which may negatively impact the training process. Additionally, BP relies on a complete understanding of the computations performed during the forward pass in order to correctly  calculate the derivatives, making it difficult to be generalized to the black-box systems where the internal workings are not transparent. Therefore it is important to be aware of these limitations and to consider alternative algorithms for training deep neural networks. 







Due to the aforementioned limitations and the apparent difference in mechanisms between deep neural network and real cortical neurons, some researchers~\cite{hinton2022forward} have raised concern about the biological plausibility of backpropagation, questioning whether the brain implements BP and whether it has some other way of getting the gradients needed to adjust the weights on connections. This has prompted researchers to search for alternate algorithms to train deep neural networks. One of such alternatives is the Forward-Forward (FF) algorithm , which replaces the forward and backward passes of BP with two forward passes that work on positive and negative data respectively, with opposite optimization objectives. FF has shown to be a potential alternative to BP, for its simplicity and flexibility in network architecture design. It makes some preliminary investigations on this area, and there is still a lot of room for further research and in-depth exploration.

In this paper we present a flexible and effective learning framework for neural networks, namely \textbf{Ca}scaded \textbf{Fo}rward (\textbf{CaFo}) algorithm, which offers a promising alternative to the traditional BP algorithm. Our CaFo framework consists of multiple cascaded neural blocks, with a layer-wise predictor attached to each block. Each neural block is composed of one or more convolutional layers, pooling layers, normalization layers, activation layers, etc, of which the aim is to project the input feature map into a new feature space. Note that the parameters of all neural blocks are randomly initialized and are fixed across both training and testing, while only the attached layer-wise predictors are trainable. Each layer-wise predictor takes the feature map computed by the corresponding neural block as input and outputs the prediction of the task, and it is trained independently without backpropagation according to the errors between the output and the ground truth. During the test period, the final prediction is a combination of the predictions from all predictors.


Roughly speaking, instead of performing backpropagation using the chain rule, the CaFo network just performs forward pass to directly estimate the prediction errors and updates the parameters in-situ. Overall, our algorithm is a step forward on the basis of FF algorithm and takes several advantages. Firstly, it eliminates the necessary for negative sampling, thereby reducing the impact of sampling quality on model performance and increasing stability. Secondly, our method can directly output the prediction of multi-class distribution rather than a simple goodness estimation, and it is thus more suitable for multi-class prediction tasks. Finally, although the neural blocks are cascaded, the predictor attached to each block is trained independently without the need for the training results from prior blocks, so the CaFo algorithm can be easily deployed into parallel acceleration systems. 



For evaluation we test our algorithm on four public image classification benchmarks, in comparison with FF, the state-of-the-art non-backpropagation algorithm. The experimental results show that our CaFo algorithm exhibits effectiveness and efficiency on image classification task, and outperforms FF by a significant margin.




\section{Related Work}

\subsection{Backpropagation Algorithm}

Backpropagation algorithm is a widely used training algorithm for deep neural networks, the goal of which is to optimize the parameters of the networks by minimizing the prediction error between the network's output and the ground truth. BP uses the gradient descent algorithm for optimization. The gradient of the loss function is calculated using the chain rule~\cite{clark1997constructing} of differentiation, where the error is propagated backwards through the network and the parameters are updated in the direction of the negative gradient. 

BP is favored for its ease of implementation, computational efficiency, and effectiveness on a variety of problems. However, it also suffers from certain limitations that have been deeply studied~\cite{bektacs2010comparison,srivastava2014dropout,zhong2020random,prechelt2012early,glorot2010understanding,zhang2019gradient,xu2020reluplex,he2016deep,ioffe2015batch}. Despite these efforts, problems with BP still occur stochastically, highlighting the need for a deep understanding of deep learning and experience in model tuning. Due to the limitations mentioned above, some recent researches~\cite{hinton2022forward,ororbia2019biologically,ororbia2022neural} have even raised doubts about the biological plausibility of BP.



\begin{figure*}[t]
	\begin{center}
		%\framebox[4.0in]{$\;$}
		\includegraphics[width=1\textwidth]{3.pdf}
		
	\end{center}
	\caption{Overall architecture of the CaFo network. Two main components are cascaded in the network: (1) a neural block that extracts representation from source data (e.g., images) at a specific scale, and (2) a predictor that takes the intermediate representation extracted by the block as input and outputs the prediction results. The parameters of predictors can be optimized either in parallel or in sequence by minimizing the loss between the predicted output and the ground truth.} %Dash lines indicate smaller edge weights.}
\label{fig:framework}
\end{figure*}

\subsection{Forward-Forward Algorithm}

The recent doubt raised by Hinton~\cite{hinton2022forward} about the biological plausibility of the BP has led to the introduction of the Forward-Forward algorithm as an alternative. FF replaces the forward and backward passes of BP with two forward passes that work on positive and negative data with opposite optimization objectives. FF has been validated to be an effective alternative to BP in some tasks, due to its simplicity and flexibility for network architecture and its ability to handle non-differentiable components. Nevertheless, some potential limitations of FF make it hard to be integrated into existing learning systems, including the high computational demanding of two forward passes, the high dependence on positive and negative sampling strategies, and the rough approximation of optimization objective. More recently, an ingenious learning process, the predictive forward-forward~\cite{ororbia2023predictive} (PFF) process proposed as a generalization of the FF, generalizes and combines the forward-forward algorithm and predictive coding into a robust neural system. It simultaneously learns a representation and generative model in a biologically-plausible manner, providing a promising brain-inspired form of forward-only learning.

Our CaFo network, different from the FF algorithm, requires only one forward pass for each training step and eliminates the need for negative sampling, and updates the parameters directly according to the errors between the predicted output and the ground truth. It simplifies both the training and the testing process, and gains significant improvement in prediction accuracy.




\section{The Proposed Method}\label{sec:method}

In this section, we describe the details of  the proposed CaFo framework. Following the work of FF, we apply this method to the task of image classification, a well-established challenge in computer vision. To ensure a clear and consistent presentation throughout the paper, we firstly provide an overview of the notations and mathematical symbols used in our analysis. This is followed by an elaboration of the overall architecture of the CaFo framework. Finally we describe the training and inference processes of the proposed approach.


\subsection{Notations}
%\cite{glorot2010understanding} make a systemantically study on gradient propagation, and propose Xavier initialization to avoid vanishing gradients. \cite{srivastava2014dropout} propose a reqularization technique dropout to prevent overfitting. 

To ensure clarity and consistency in our presentation, we provide a brief overview of the notations we used in this paper. The
scalars used in our analysis are represented by italic lowercase letters (e.g., \textit{m}, \textit{n}), while the vectors used in our method are represented by bold lowercase letters (e.g., \textbf{x}, \textbf{y}). All vectors in this paper are assumed to be column vectors. Bold uppercase letters (e.g., \textbf{W}, \textbf{Z}) are used to represent matrices. Furthermore, the subscripts and superscripts in our notations represent the indices and exponents, respectively. In the rest of the paper, we will provide more detailed definitions and explanations of the notations as needed.


\subsection{Pipeline Overview}

The pipeline of the proposed CaFo framework is depicted in Figure~\ref{fig:framework}, which consists of multiple repeated stacks of two major key components:


\textbf{Neural Block:} The neural block, abbreviated as \textit{block} for convenience in the rest of the paper, is comprised of multiple fundamental units, such as convolutional layer, pooling layer, normalization layer, activation layer, etc. Its architecture is dependent on the specific task, and for image classification in our experiments it consists of a convolutional layer followed by a ReLU activation function, a max-pooling layer and a batch normalization layer.  Multiple blocks with the same or different architecture are cascaded to extract intermediate representations of images at different scales. It is worth noting that the block can be either parametric or non-parametric, which provides flexibility in its construction.

\textbf{Predictor:} As illustrated in Figure~\ref{fig:framework}, each block is equipped with a layer-wise predictor that consists of a fully connected layer followed by an activation function (e.g., softmax) if necessary. The predictor takes the intermediate representation extracted by the block as input and outputs a prediction result for the task. Since there is only one fully connected layer, the predictor can be trained \textit{in situ} without backpropagation.




\subsection{Training Process} \label{sec:method_training}
In the CaFo network, we randomly initialize the parameters of each block with Kaiming uniform~\cite{he2015delving} and keep these parameters fixed across both the training and testing process. The blocks act as filters with fixed template coefficients. The experimental results in Tables~\ref{tab:smalldata} and \ref{tab:largedata} demonstrate that, despite their simplicity, these untrainable blocks can still produce discriminative representations for prediction and achieve better performance in comparison with FF. 

Only the parameters of each predictor are updated during the training phase. It is worth noting that our method enables both parallel and sequential updates of predictors, as the intermediate features extracted by the blocks with fixed parameters, can be obtained by only one forward pass. During the training phase, each predictor independently makes its own greedy decision about the class to which the input belongs, eliminating the need for backpropagation to learn a discriminative model.

For each predictor, we optimize it by minimizing the errors between the prediction and the ground truth.  Different measure functions can be adopted in our framework, which lead to different optimization strategies and algorithms. In experiments we compare three different loss function: mean square error loss (MSE), cross-entropy loss (CE) and sparsemax loss (SL)~\cite{martins2016softmax}. Here we describe the corresponding formulations and optimization algorithms respectively.

\paragraph{MSE loss:} The optimization objective of the predictor can be expressed as follows:
\begin{align}\label{eq:mse}
\arg\min\limits_{\text{\textbf{W}}}\frac{1}{2m}\|\text{\textbf{HW}}-\text{\textbf{Y}}\|_{F}^{2},
\end{align}
where $m$ denotes the number of training samples, $\textbf{H}\in\mathbb{R}^{m\times d}$ the $d-$dimensional intermediate representation output by the block, $\textbf{Y} \in \mathbb{R}^{m \times c}$ the one-hot labels of training samples, $\textbf{W} \in \mathbb{R}^{d \times c}$ the parameters of predictor to be optimized, $\|\cdot\|_F$ the Frobenius norm, and $c$ indicates the number of classes. The close-form solution for Eq.~\ref{eq:mse} can be obtained by solving the following equation for $\textbf{W}$:
\begin{align}\label{eq:mse1}
\frac{\partial}{\partial \bf{W}}(\frac{1}{2m}\|\textbf{HW}-\textbf{Y}\|_{F}^{2})=\frac{1}{m}\textbf{H}^{T}(\textbf{HW}-\textbf{Y})=0,
\end{align}

\noindent and the solution of Eq.~\ref{eq:mse} is:

\begin{align}\label{eq:mse2}
\textbf{W}=(\textbf{H}^{T}\textbf{H})^{-1}\textbf{H}^{T}\textbf{Y}.
\end{align}



\paragraph{Cross-entropy loss:} The optimization objective of the predictor is written as:
\begin{align}\label{eq:ce}
\arg\min \limits_{\textbf{W}}-\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{c}\textbf{Y}_{i,j} \cdot \ln \textbf{P}_{i,j}, 
\end{align}
\begin{align}\label{eq:ce1}
\textbf{P}_{i,j}=\frac{\exp(\textbf{H}_{i,:}\textbf{W}_{:,j})}{\sum_{k=1}^{c}\exp(\textbf{H}_{i,:}\textbf{W}_{:,k})},
\end{align}

\noindent where $\textbf{Y}_{i,j}$ is the value at $i$-th row and $j$-th column of $\textbf{Y}$, and $\textbf{P}_{i,j}$ the output of softmax layer that denotes the confidence level the $i$-th training sample belongs to $j$-th class. $\textbf{H}_{i,:}$ and $\textbf{W}_{:,j}$ respectively denote the $i$-th row of $\textbf{H}$ and the $j$-th row of $\textbf{W}$. 

As the close-form solution for Eq.~\ref{eq:ce} is unavailable, we use gradient descent algorithm to optimize it. To do this, we need to calculate the gradient for $\textbf{W}$ according to the Jacobian matrix, and set a fixed step size to optimize Eq.~\ref{eq:ce} iteratively. The Jacobian matrix is calculated by:
\begin{align}\label{eq:ce2}
\textbf{J}=\textbf{P}-\textbf{Y},
\end{align}

\noindent and the gradient for $\textbf{W}$ is computed as:
\begin{align}\label{eq:ce3}
\frac{1}{m}\sum_{i=1}^{m}[\textbf{H}_{i,:}]^{T}\otimes \textbf{J}_{i,:},
\end{align}
where $\textbf{J}$ denotes the Jacobian matrix, and $\otimes$ the Kronecker product.



%\HL{(\textit{Add description of MSE, cross entropy and sparse loss, and their optimization strategies.})}

\paragraph{Sparsemax loss:} Different to softmax, the sparsemax function~\cite{martins2016softmax} produces sparse output probabilities by enforcing a constraint that the output confidence vector has at most a certain number of non-zero elements. It encourages the model to only assign high probabilities to the most relevant classes, while setting all other probabilities to zero. The sparsemax function achieves this by projecting the input vector onto a simplex, which is a convex polytope whose vertices lie on the coordinate axes. The formulaic expression for sparsemax is as follows:



\begin{align}\label{eq:ce4}
{\rm sparsemax}(\textbf{z}):=\arg\min\limits_{\textbf{p}\in \bigtriangleup^{c-1}}\|\textbf{p}-\textbf{z}\|^{2},
\end{align}

\noindent where $\bigtriangleup^{c-1}:=\{\textbf{p} \in \mathbb{R}^{c}|\textbf{1}^{T}\textbf{p}=1,\textbf{p} \ge \textbf{0}\}$ is the $(c-1)$-dimensional simplex. The resulting projection is unique and can be computed efficiently using a sorting algorithm~\cite{martins2016softmax}. 

We also use the gradient descent algorithm to approximate the sparsemax loss optimization objective:

\begin{align}\label{eq:sl}
\arg\min\limits_{\textbf{W}}\frac{1}{m}\sum_{\textbf{z} \in \textbf{Z}}^{}\sum_{k=1}^{c}L_{\rm sparsemax}(\textbf{z};k),
\end{align}


\begin{align}\label{eq:sl1}
L_{\rm sparsemax}(\textbf{z};k)=-\textbf{z}_{k}+\frac{1}{2}\sum_{j \in S(\textbf{z})}(\textbf{z}_{j}^{2}-\tau^{2}(\textbf{z}))+\frac{1}{2},
\end{align}


\noindent where $\textbf{z} \in \mathbb{R}^{c}$ is the output logits, $\textbf{Z}=[\textbf{z}_{1}; \textbf{z}_{2}; ...; \textbf{z}_{m}]^{T} \in \mathbb{R}^{m \times c}$ the set of output logits of $m$ training samples, $S(\textbf{z})$ the support of ${\rm sparsemax}(\textbf{z})$, and $\tau(\textbf{z})=\frac{(\sum_{j \in S(\textbf{z})}\textbf{z}_{i})-1}{|S(\textbf{z})|}$. The Jacobian matrix is calculated by:
\begin{align}\label{eq:sl2}
\text{\textbf{J}}={\rm sparsemax}(\text{\textbf{Z}})-\text{\textbf{Y}},
\end{align}
\noindent and the gradient for \textbf{W} is computed using the same formula as Eq.~\ref{eq:ce3}.

\subsection{Inference Process}

In the inference phase each predictor outputs a prediction and the final prediction is determined by combining all the individual predictors. In image classification tasks this is typically done by summing the predictions of all the predictors and choosing the index of the maximum value as the predicted class.

The inference process for FF involves generation of samples for every label and computation of their goodness, whereas our method is able to directly provide  multi-class label distributions. Our method gains higher inference efficiency and is more suitable for multi-class prediction tasks.

The detailed optimization algorithm is shown in Algorithm~\ref{alg:algorithm1}. During the training stage, the model generates the prediction block by block at step 3 and step 4, where the $\textbf{h}_{j}^{i}\in \mathbb{R}^{d} $ and $\tilde{\textbf{y}}_{j}^{i} \in \mathbb{R}^{c}$ are the intermediate representation and prediction of $i$-th block for $j$-th training sample. 
At step 5, the predictors are updated based on a specific loss function (e.g., Eqs.~\ref{eq:mse}, ~\ref{eq:ce}, and ~\ref{eq:sl}), using the corresponding optimization strategy as described in  Section~\ref{sec:method_training}. 
The predictors consist of a fully connected layer for MSE, and an additional activation layer is followed  for CE and SL. During the inference stage, each predictor outputs a prediction for the test set at step 3 and step 4, where $\hat{\textbf{y}}_{j}^{i} \in \mathbb{R}^{c}$ is the prediction of $i$-th block for the $j$-th test sample. Then the predictions of each block (e.g., $\hat{\text{\textbf{Y}}}^{i} \in \mathbb{R}^{n \times c}$) are summed to derive the final prediction $\hat{\textbf{\textbf{Y}}} \in \mathbb{R}^{n \times c}$. The predicted class for $j$-th test samples is then calculated at step 7 as: $\hat{y}_{j}=\arg\max\limits_{k} \hat{\textbf{\textbf{Y}}}_{j,k}$.


\begin{algorithm}[!t]
\caption{The CaFo Algorithm}
\label{alg:algorithm1}
\textbf{Input}: \\
$\text{\textbf{X}}=[\text{\textbf{x}}_{1}; \text{\textbf{x}}_{2}; ...; \text{\textbf{x}}_{m}]^{T}$: m samples for training, \\
$\hat{\text{\textbf{X}}}=[\hat{\text{\textbf{x}}}_{1}; \hat{\text{\textbf{x}}}_{2}; ...; \hat{\text{\textbf{x}}}_{n}]^{T}$: n samples for inference, \\
$\text{\textbf{Y}}=[\text{\textbf{y}}_{1}; \text{\textbf{y}}_{2}; ...; \text{\textbf{y}}_{m}]^{T}$: one-hot labels of training samples, \\
%	$L,U$: labeled and unlabeled node set of graph, \\
$\mathcal{B}_{i}$: the $i$-th block, \\
$\mathcal{P}_{i}$: the predictor of $i$-th block. \vspace{1mm}\\
%	$\textit{NCM}$: node classification module.
\textbf{Manual factors}: \\
$r$: the number of blocks of CaFo \vspace{1mm}\\
\textbf{Output}: $\bf{\hat{y}}$$=[\hat{y}_{1}, \hat{y}_{2}, ..., \hat{y}_{n}]^{T}$ \vspace{1mm}\\
\textbf{Training:}
\begin{algorithmic}[1] %[1] enables line numbers
	\STATE ${\text{\textbf{H}}^{0}=[\text{\textbf{h}}^{0}_{1}; \text{\textbf{h}}^{0}_{2}; ...; \text{\textbf{h}}^{0}_{m}]^{T}=\text{\textbf{X}}}$
	\FOR{$i=1 \to r$}
	\STATE Get the intermediate representations of the $i$-th block: \\
	$\text{\textbf{H}}^{i}=[\text{\textbf{h}}^{i}_{1}; \text{\textbf{h}}^{i}_{2}; ...; \text{\textbf{h}}^{i}_{m}]^{T}=\mathcal{B}_{i}(\text{\textbf{H}}^{i-1})$  \\
	\STATE Get the predictions of the $i$-th block: \\
	$\tilde{\text{\textbf{Y}}}^{i}=[\tilde{\text{\textbf{y}}}^{i}_{1}; \tilde{\text{\textbf{y}}}^{i}_{2}; ...; \tilde{\text{\textbf{y}}}^{i}_{m}]^{T}=\mathcal{P}_{i}(\text{\textbf{H}}^{i})$  \\
	\STATE Optimizing $\mathcal{P}_{i}$ according to the errors between $\tilde{\text{\textbf{Y}}}^{i}$ and $\text{\textbf{Y}}$
	\ENDFOR
\end{algorithmic}
\vspace{1mm}
\textbf{Inference:}
\begin{algorithmic}[1] %[1] enables line numbers
	\STATE $\hat{\text{\textbf{H}}}^{0}=[\hat{\text{\textbf{h}}}^{0}_{1}; \hat{\text{\textbf{h}}}^{0}_{2}; ...; \hat{\text{\textbf{h}}}^{0}_{n}]^{T}=\hat{\text{\textbf{X}}}$
	\FOR{$i=1 \to r$}
	\STATE Get the intermediate representations of the $i$-th block: \\
	$\hat{\text{\textbf{H}}}^{i}=[\hat{\text{\textbf{h}}}^{i}_{1}; \hat{\text{\textbf{h}}}^{i}_{2}; ...; \hat{\text{\textbf{h}}}^{i}_{n}]^{T}=\mathcal{B}_{i}(\hat{\text{\textbf{H}}}^{i-1})$  \\
	\STATE Get the predictions of the $i$-th block: \\
	$\hat{\text{\textbf{Y}}}^{i}=[\hat{\text{\textbf{y}}}^{i}_{1}; \hat{\text{\textbf{y}}}^{i}_{2}; ...; \hat{\text{\textbf{y}}}^{i}_{n}]^{T}=\mathcal{P}_{i}(\hat{\text{\textbf{H}}}^{i})$  \\
	\ENDFOR
	\STATE $\hat{\text{\textbf{Y}}}=\sum_{i=1}^{r}\hat{\text{\textbf{Y}}}^{i}$
	\STATE $\hat{\text{\textbf{y}}}=\arg\max\hat{\text{\textbf{Y}}}$
	\RETURN $\hat{\text{\textbf{y}}}$
\end{algorithmic}
\end{algorithm}



\section{Discussion}

\begin{figure*}[t]
\begin{center}
	%\framebox[4.0in]{$\;$}
	\includegraphics[width=.9\linewidth]{4.pdf}
	
\end{center}
\caption{Comparison of BackPropagation (BP) algorithm, Forward-Forward (FF) algorithm  and the proposed Cascaded Forward (CaFo) algorithm.}
\label{fig:comparision}
\end{figure*}

\subsection{Relationship to the Forward-Forward Algorithm} 

Both FF and our method are alternative approaches to backpropagation, as highlighted in \cite{hinton2022forward}. They both have advantages over the traditional backpropagation method, such as the ability to handle non-differentiable components and increased flexibility in network architecture. However, the differences between the two methods are still significant. 

The graphical depiction and comparison of BP, FF, and proposed CaFo are shown in Figure~\ref{fig:comparision}. The diagram style used in the figure is adapted from \cite{ororbia2023predictive}, and we follow the same approach to depict CaFo and highlight the differences between our model and the other two methods. For BP, the model conducts a forward pass to derive the global loss, and then the gradients of the loss function with respect to the weights of the network are computed using the chain rule of calculus. The weights are updated using the obtained gradients and a learning rate hyper-parameter, which controls the size of the step taken in the opposite direction of the gradient. For FF, positive and negative samples are selected and samples are used to calculate a local loss for each layer. Once the local loss of the $i$-th layer is calculated (i.e., $\textbf{L}_{i}$), the weights of the $i$-th layer (i.e., $\textbf{W}_{i}$) will be updated according to $\textbf{L}_{i}$, and after that the $\textbf{W}_{i}$ will be fixed during the next training stage for the $(i+1)$-th layer. The loss for each layer is calculated in sequence, and the layers of FF are sequentially updated by greedy decision. 

Compared with FF, our CaFo network only has one input stream, avoiding the negative sampling process. Although the sequential update of each block in our framework is similar to FF, the local loss in CaFo directly measures the error between the predicted multi-class distribution and ground truth, rather than a simple goodness estimation. In addition, as the weights of each block (i.e. $\textbf{G}_{N}$) are fixed, the predictor (i.e., $\textbf{W}_{N}$) attached to each block is trained independently without the need for the training results from prior blocks, which makes the framework allow for the parallel training of each predictor. 




\subsection{Relationship to Bagging} 
Bagging (Bootstrap Aggregating)~\cite{breiman1996bagging} is an ensemble learning technique that combines multiple models to enhance the overall performance of a machine learning algorithm. The basic idea of bagging is to train some individual models on different subsets of the training data, and then combine their outputs by taking the average of majority vote. Bagging eliminates the variance of the individual models and prevents overfitting by reducing the impact of noisy data points or outliers. This is achieved by generating multiple random samples of the training data, each of which is adopted to train a different model. The results are then combined to make a final prediction.  

Our method can be viewed as a special case of Bagging, where the predictors are trained on intermediate features extracted at various scales, rather than on the original images. In other words, our approach does not train individual models on different subsets of the training data, but assigns each individual model with a specific intermediate representations of the complete training data, which has the advantage of utilizing the information contained in intermediate features extracted at different scales, thus helping to capture more complex patterns and increase the accuracy of the model.

\subsection{Relationship to Co-training} 

Co-training~\cite{blum1998combining} is a popular semi-supervised learning method that trains multiple classifiers on the same dataset with different data views to improve classification accuracy by leveraging unlabeled data. The process involves initializing several classifiers with different feature sets and training them on labeled data subsets. The classifiers then exchange predictions on unlabeled data and retrain on new labeled data subsets using these predictions. This iterative process continues until convergence, with classifiers refining their feature sets and predictions. 

As a semi-supervised learning technique, co-training is usually applied in scenarios where the labeled data is scarce but there is a large amount of unlabeled data available. In contrast, our method is a fully-supervised approach that trains multiple predictors for each block in parallel, allowing for more efficient and stable optimization of the model in supervised learning scenarios. As our method and co-training share the same idea of training multiple modules to improve the overall performance, our approach can be seen as the first stage of a co-training process, and can be easily transformed into a co-training method if an augmentation strategy for training set is introduced. 





\section{Experiments}

The aim of this paper is to introduce the CaFo algorithm and to show that it works in relatively small neural networks containing a few million connections. We evaluate the proposed algorithm on four benchmarks in comparison with backpropagation and FF. Future research will focus on investigating the scalability of the proposed approach to handle large neural networks with a much greater number of connections.






\subsection{Datasets and Experimental Settings}\label{sec:datasets}

The experiments are conducted on  the MNIST~\cite{lecun1998gradient}, CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100~\cite{krizhevsky2009learning} and Mini-ImageNet~\cite{vinyals2016matching} datasets, all of which are widely-used in evaluating image classification algorithms. The standard splits of MNIST, CIFAR-10 and CIFAR-100 are directly used. In the case of Mini-ImageNet, which is often used for evaluating few-shot learning algorithms, all the classes are mixed, and for each class, 500 images are randomly sampled for training and 100 images for testing.


We compare the proposed CaFo algorithm with two baseline algorithms: backpropagation and the FF algorithm. Specifically, for MNIST and CIFAR-10, we report the results of both the original version of FF~\cite{hinton2022forward} and our reproduced version. For CIFAR-100 and Mini-ImageNet, we only report the results of our reproduced version, as the source code of the original version is not publicly available. 

For the proposed CaFo algorithm, we set the number of blocks $r$=3. Each block consists of a convolutional layer followed by a ReLU activation function, a max-pooling layer and a batch normalization layer. The convolutional layer of the three blocks have the same kernel size, padding, and stride, which are set to be $3 \times 3$, 1, 1, and the output channels are set to be 32, 128, and 512 respectively.


\begin{table}[t]
\setlength\tabcolsep{5.pt}%?÷???à
\renewcommand{\arraystretch}{1.35} 
\begin{center}
	\begin{tabular}{lrrrr}
		\hline
		& \multicolumn{2}{c}{\textbf{MNIST}}                                                     & \multicolumn{2}{c}{\textbf{CIFAR-10}}                                                   \\ \hline
		\textbf{Error rate (\%)}& \multicolumn{1}{l}{Training} & \multicolumn{1}{l}{Test} & \multicolumn{1}{l}{Training} & \multicolumn{1}{l}{\ Test}\\ \hline
		\textbf{BP (from \cite{hinton2022forward})}               & - \quad\quad                                      & 1.40                               & 2.00    \ \ \                                 & 39.00     \ \                               \\ 
		\textbf{FF (from \cite{hinton2022forward})}               & -  \quad\quad                                     & 1.37                              & 24.00   \ \ \                                 & 41.00    \ \    \\
		\textbf{FF (reprodu.)}               & 0.57   \ \                                     & 2.02                              & 10.36  \ \ \                                  & 46.03      \ \                                \\
		\hline
		\textbf{CaFo+MSE}             &    0.71      \ \                          &    1.55                           & 12.56    \ \ \                            & 34.79        \ \            \\ 
		\textbf{CaFo+CE} & 0.04       \ \                           & 1.30                              &     9.22  \ \ \                         & 32.57           \ \                 \\ 
		\textbf{CaFo+SL} & 0.06    \ \                     &           1.20                    &    9.58  \ \ \                            &      33.74  \ \                 \\ \hline
	\end{tabular}
\end{center}
\caption{Error rate on MNIST and CIFAR-10}\label{tab:smalldata}
\end{table}
%#0.82    1.89      16.17  6.69

%train error: 0.005119979381561279
%test error: 0.01380002498626709


\begin{table}[t]
\setlength\tabcolsep{5.pt}%?÷???à
\renewcommand{\arraystretch}{1.35} 
\begin{center}
	\begin{tabular}{lrrrr}
		\hline
		& \multicolumn{2}{c}{\textbf{CIFAR-100}}                                                     & \multicolumn{2}{c}{\textbf{Mini-ImageNet}}   \ \ \                                               \\ \hline
		\textbf{Error rate (\%)}& \multicolumn{1}{l}{Training} & \multicolumn{1}{l}{\ Test} & \multicolumn{1}{l}{Training} & \multicolumn{1}{l}{\ Test}\\ \hline
		\textbf{FF (reprodu.)}               & 29.32  \ \                                     & 78.67                              &  27.84 \ \ \                                  &    91.71   \ \                                \\
		\hline
		\textbf{CaFo+MSE}             & 6.06         \ \                         & 63.94                              &  6.66    \ \ \                            &  85.50       \ \            \\ 
		\textbf{CaFo+CE} & 0.06      \ \                           & 59.24                              &   0.01   \ \ \                        &       78.20      \ \                 \\ 
		\textbf{CaFo+SL} &     0.02 \ \                             &         61.96                      &    0.02  \ \ \                            &      81.47  \ \                 \\ \hline
	\end{tabular}
\end{center}
\caption{Error rate on CIFAR-100 and Mini-ImageNet}\label{tab:largedata}
\end{table}



\begin{table*}[t]
\renewcommand{\arraystretch}{1.35} 
\begin{center}
	\begin{tabular}{lrrrrrrrr}
		\hline
		& \multicolumn{2}{c}{\textbf{MNIST}}                                                     & \multicolumn{2}{c}{\textbf{CIFAR-10}}     &
		\multicolumn{2}{c}{\textbf{CIFAR-100}}&
		\multicolumn{2}{c}{\textbf{Mini-ImageNet}}                                              \\ \hline
		\textbf{Time (s)}& \multicolumn{1}{l}{Training} & \multicolumn{1}{l}{Test} & \multicolumn{1}{l}{Training} & \multicolumn{1}{l}{Test}  & \multicolumn{1}{l}{Training} & \multicolumn{1}{l}{Test}   & \multicolumn{1}{l}{Training} & \multicolumn{1}{l}{Test}\\ \hline
		\textbf{FF (reprodu.)}               & 1655.22  \ \ \                                     & 0.02                              & 2374.30  \ \ \                                  & 0.04      \           & 2376.77  \ \ \ & 0.42 & 3668.35 \ \ \ &  2.03    \                 \\
		\hline
		\textbf{CaFo+MSE}             & 0.62         \ \ \                          & 0.06                              & 1.21    \ \ \                            & 0.07        \      & 1.40 \ \ \ & 0.08 & 22.55 \ \ \ & 0.32  \    \\ \hline
		\textbf{CaFo+CE} &  612.22  \ \ \                               & 0.05                              & 873.56      \  \ \                        & 0.07           \     & 1178.42 \ \ \ & 0.08 & 8101.42 \ \ \ & 0.52  \           \\ 
		\textbf{CaFo+SL} & 837.56   \ \ \                      &          0.06                    &    1082.45  \ \ \                            &     0.09   \        & 2192.56 \ \ \ & 0.10 & 14128.06  \ \ \ & 0.49    \     \\    \hline
	\end{tabular}
\end{center}
\caption{Time for training and testing}\label{tab:time}
\end{table*}



As described in Section~\ref{sec:method_training}, different error measure functions can be adopted in our framework to guide the training. For analysis of the affects of different measure functions, in this section we report the results of three versions of our CaFo algorithms that adopt MSE (CaFo+MSE), cross-entropy (CaFo+CE) and sparsemax loss (CaFo+SL) as the measure function respectively. The predictor is a fully connected layer without bias, followed by a softmax layer for CE and SL. For CaFo+CE and CaFo+SL, each predictor is trained for 5000 epochs on MNIST and CIFAR-10, and for 1000 epochs on CIFAR-100 and Mini-ImageNet. All experiments are run on AMD EPYC 7542 32-Core Processor with a Nvidia GeForce RTX 3090 GPU.








\subsection{Performance Comparison}

The comparison of these algorithms on four datasets is summarized in Tables~\ref{tab:smalldata} and \ref{tab:largedata}. It is obvious our method achieves overall improvements in test error rate compared with FF. Even the simplest version (CaFo+MSE) outperforms the reproduced FF on all datasets. For FF, we observe a significant discrepancy in its results between the two tables. FF has low training error when the dataset is easy to discriminate (e.g., CIFAR-10, MNIST), but has high training error when tackling much more complex datasets (e.g., CIFAR-100, Mini-ImageNet). The reason for the underfitting of FF probably lies in that the contrastive loss function based on positive and negative samples may not provide effective guidance. If the training samples for each class are scarce (e.g., CIFAR-10 vs. CIFAR-100) or the samples contain more irrelevant information (e.g., CIFAR-10 vs. Mini-ImageNet), the goodness estimation of positive and negative samples may not effectively help the FF model learn the potential distribution for each class because of the uncertain quality of negative samples and the lack of supervision information directly related to the category information. This may lead to the poor fitting for the training set. 




In contrast, our method exhibits a smaller discrepancy between the two kinds of datasets. The training error rate of CaFo is less than 10\% in most trials, demonstrating better fitting ability than FF. For test error rate, the performance gap between the two is still significant, especially on more complex datasets. For fair comparison, the results of CaFo are reported without deployment of any regularization trick into the model, and thus the performance of CaFo still has the potential to be improved by means of regularization strategies.



Comparing the three variants of our method, we find that they obtain overall similar performance. CaFo+MSE has the highest test error and train error on all the datasets, which indicates MSE may not be good enough for error estimation in comparison with other two variants that adopt classification-oriented loss. In contrast, both CaFo+CE and CaFo+SL exhibit low and very similar training and test error on the four datasets, indicating the effectiveness of gradient descent optimization strategy and the superiority of CE and SL compared with MSE. 


\subsection{Time Comparison}
Furthermore, we report the time required for training and testing our method in comparison with FF as shown in Table~\ref{tab:time}. For each trial, we report the time required for the entire training process and the test time that includes the time for the forward pass of the test samples and the time for calculating the predicted category. The experimental settings are the same as those in Section~\ref{sec:datasets}.



Comparing our three variants with FF, we observe that our methods exhibit significant improvements in training efficiency, especially for CaFo+MSE, which has a much shorter training time than the other three approaches due to its direct calculation of the close-form solution without iterative training. CaFo+CE and CaFo+SL also show significant improvements in training efficiency on MNIST, CIFAR-10 and CIFAR-100, while taking more time on Mini-ImageNet. This is because we divide the training set of CIFAR-100 and Mini-ImageNet into 20 and 200 training batches, respectively, for the two variants due to memory limitation, which leads to the extra time consumption of data movement. However in the trials of MNIST and CIFAR-10 where the batch number equals one, we find the two CaFo variants are trained significantly faster than FF. 







In terms of test time, our methods demonstrate comparable efficiency on MNIST and CIFAR-10, but significantly shorter test time on CIFAR-100 and Mini-ImageNet. The reason for this lies in that our methods directly output the prediction of multi-class distribution, rather than compute the goodness estimation for each candidate label as that in FF. It tremendously improves the test efficiency when the number of categories is relatively large, such as on CIFAR-100 and Mini-ImageNet. 






\subsection{Investigation of the Number of Blocks}

In Tables~\ref{tab:smalldata} and \ref{tab:largedata} we fix the number of blocks $r$=3 for fair comparison with \cite{hinton2022forward}, in which the results of BP and FF with three layers are reported. However, the number of blocks may have diverse effects on different datasets. Here we investigate how the number of blocks influences the performance of our method. 

In Figure~\ref{fig:layers}  the error rates of  CaFo+MSE and CaFo+CE  with the number of blocks ranging from 1 to 10 are reported. As CIFAR-10 allows for a maximum of five $2 \times 2$ max-pooling layers with $stride$=2, and four are available for MNIST, we remove the max-pooling layer from some blocks to keep the number of max-pooling layers within the upper bound if necessary. We observe that the training error rates (dashed lines) continuously decrease with the increasing number of blocks, indicating an enhancement of fitting ability as more learnable predictors are introduced. However, the test error curves (solid lines) show that the model is more or less affected by overfitting when the number of blocks is large.







\subsection{Performance of each Predictor}

To investigate the performance of each predictor, we report the test error rate of each predictor for CaFo+MSE and CaFo+CE, on three datasets. As shown in Figure~\ref{fig:layers1}, the test error of the final prediction is lower than that of each individual predictor. In addition, we surprisingly find that a deep-layer predictor tends to obtain better performance than a shallow-layer one, demonstrating that the deep features play a more important role in enabling the predictor to make correct classifications. Overall, the results in Figure~\ref{fig:layers1} well validate that the combination of all the predictors effectively contributes to the final prediction. Although both FF and our method demonstrate that the simple summing of each block's prediction (goodness) is good enough for comparable performance, however, designing more ingenious heuristic combination strategies for these non-backpropagation approaches remains to be studied. We will concentrate on this problem in future work.



\begin{figure}[t!]
\begin{center}
	
	\subfloat[MNIST]{
		\begin{minipage}[b]{0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{block_mnist.pdf}
		\end{minipage}
	}
	\subfloat[CIFAR-10]{
		\begin{minipage}[b]{0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{block_cifar10.pdf}
		\end{minipage}
	}
	
\end{center}
\caption{Error rates with different numbers of blocks.}
\label{fig:layers}
\end{figure}


\begin{figure}[t!]
\begin{center}
	
	\subfloat[CaFo+MSE]{
		\begin{minipage}[b]{0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{layer_mse.pdf}
		\end{minipage}
	}
	\subfloat[CaFo+CE]{
		\begin{minipage}[b]{0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{layer_ce.pdf}
		\end{minipage}
	}
\end{center}
\caption{Test error rate of each predictor.}
\label{fig:layers1}
\end{figure}

\section{Conclusion}

In this paper, we propose a new learning framework for deep neural networks that provides a viable alternative to the backpropagation algorithm. The proposed CaFo demonstrates the feasibility of using only a single forward pass without the need for additional backward pass in the neural network training process, which offers significant improvements in prediction accuracy, training efficiency and simplicity compared with FF. Extensive experiments well validate the superior performance over the competitive methods on several public benchmarks. 


%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
