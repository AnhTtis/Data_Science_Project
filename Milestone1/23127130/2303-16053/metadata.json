{
    "arxiv_id": "2303.16053",
    "paper_title": "Real-time Multi-person Eyeblink Detection in the Wild for Untrimmed Video",
    "authors": [
        "Wenzheng Zeng",
        "Yang Xiao",
        "Sicheng Wei",
        "Jinfang Gan",
        "Xintao Zhang",
        "Zhiguo Cao",
        "Zhiwen Fang",
        "Joey Tianyi Zhou"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Real-time eyeblink detection in the wild can widely serve for fatigue detection, face anti-spoofing, emotion analysis, etc. The existing research efforts generally focus on single-person cases towards trimmed video. However, multi-person scenario within untrimmed videos is also important for practical applications, which has not been well concerned yet. To address this, we shed light on this research field for the first time with essential contributions on dataset, theory, and practices. In particular, a large-scale dataset termed MPEblink that involves 686 untrimmed videos with 8748 eyeblink events is proposed under multi-person conditions. The samples are captured from unconstrained films to reveal \"in the wild\" characteristics. Meanwhile, a real-time multi-person eyeblink detection method is also proposed. Being different from the existing counterparts, our proposition runs in a one-stage spatio-temporal way with end-to-end learning capacity. Specifically, it simultaneously addresses the sub-tasks of face detection, face tracking, and human instance-level eyeblink detection. This paradigm holds 2 main advantages: (1) eyeblink features can be facilitated via the face's global context (e.g., head pose and illumination condition) with joint optimization and interaction, and (2) addressing these sub-tasks in parallel instead of sequential manner can save time remarkably to meet the real-time running requirement. Experiments on MPEblink verify the essential challenges of real-time multi-person eyeblink detection in the wild for untrimmed video. Our method also outperforms existing approaches by large margins and with a high inference speed.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16053v1"
    ],
    "publication_venue": "Accepted by CVPR 2023"
}