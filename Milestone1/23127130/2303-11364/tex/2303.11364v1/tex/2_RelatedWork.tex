\section{Related Work}\label{sec:relatedwork}
\moniker{} integrates knowledge from several areas of research.
We briefly review these next.

\subsection{3D Scene Reconstruction with NeRF}
Neural radiance fields (NeRFs)~\cite{mildenhall2020nerf} were introduced for novel view synthesis applications.
By combining differentiable volumetric rendering from classic computer graphics and learnable radiance field, NeRF can jointly optimize for the geometry and appearance of a static 3D scene from posed RGB images.
As the performance of the original NeRF noticeably deteriorates under imperfect capturing conditions, much research has investigated the robustness of NeRF under a variety of challenging conditions, such as blur~\cite{ma2022deblur}, noise~\cite{pearl2022nan}, reflection~\cite{guo2022nerfren}, low resolution~\cite{wang2022nerf}, low dynamic range~\cite{huang2022hdr,mildenhall2022nerf}, and occlusion~\cite{martin2021nerf,chen2022hallucinated}.
In this paper, we address haze, an unexplored yet common scenario in real-life captures.
Our method leverages a scattering-aware rendering equation to model the haze phenomenon and disambiguate the surface and haze by using physically inspired inductive biases.
Among those, we adopt a surface-like density field parameterization~\cite{wang2021neus} to encourage solid geometry, although other surface priors may be equally applied in our method~\cite{niemeyer2022regnerf,yariv2021volume,wang2021neus, oechsle2021unisurf}.

\subsection{Dehazing}
We provide a brief summary of existing dehazing methods and refer to~\cite{singh2019comprehensive} for a comprehensive review.
Early works aim to design strong priors to recover the transmission map, global atmospheric light, and scene radiance~\cite{zhu2015fast,bui2017single,fattal2014dehazing, he2010single, berman2016non}, thus they struggle on scenes where these priors do not hold~\cite{liu2019learning}.
More recently, data-driven methods~\cite{liu2019learning,chen2019pms,dong2020multi,mehri2021mprnet,zamir2021multi,guo2022image} achieve better quality using deep learning and large datasets~\cite{li2018benchmarking}, but have difficulty generalizing to images in the real world~\cite{shao2020domain}.
More related to our work are multi-image dehazing methods, which exploit the consistency between neighboring frames.
Similar to single-image approaches, multi-image approaches can be divided into prior-based or data-driven categories.
The former adds photo-consistency regularization between neighboring frames using jointly estimated or known depth~\cite{zhang2011video,li2015simultaneous,fujimura2020dehazing}, while the latter achieves more temporal stability by fusing features from a small number of neighboring frames~\cite{ren2018deep,wang2019edvr,zhang2021learning}.
Yet both approaches are subject to the aforementioned problems exhibited in the single-image scenario.
Furthermore, none of these dehazing methods estimates a holistic 3D structure, and therefore they are not directly applicable to novel view synthesis.

\paragraph{Estimating Scattering Coefficients.}
In addition to estimating the 3D geometry and haze-free appearance in novel views, we also jointly optimize for the scattering coefficient.
Prior work mainly computes the scattering coefficient based on handcrafted priors such as average saturation~\cite{gu2017single}, dark channel mean~\cite{chung2022image}, polarization~\cite{schechner2001instant}, and gamma correction~\cite{ju2019idgcp}.
In data-driven approaches, the scattering coefficient can be estimated explicitly as an intermediary output~\cite{yang2022self,wang2021fully}.
Our method directly estimates the scattering coefficient in 3D, while at the same time utilizing 2D priors such as dark channel mean to address the ambiguity with other physical properties such as airlight.

\paragraph{Seeing through Scattering Media.}
Various methods have been developed to solve the challenging problem of imaging through and within scattering media. These methods can be classified as interference of light, relying on ballistic photons, and being based on diffuse optical tomography. For methods based on interference of light, they leverage information in the speckle pattern to reconstruct the image~\cite{popoff2010image,katz2014non,bertolotti2012non} or adopt wavefront shaping to focus light through or within scattering media~\cite{horstmeyer2015guidestar,vellekoop2007focusing}. On the other hand, ballistic photons can avoid scattered photons since they can travel through a medium without scattering and can be isolated by adopting time-gating~\cite{redo2016terahertz,wang1991ballistic}, coherence-gating~\cite{indebetouw2000imaging,dunsby2003techniques}, or coherent probing and detection of a target at different illumination angles~\cite{kang2015imaging}. The last class is to adopt non-line-of-sight imaging techniques~\cite{velten2012recovering,liu2019non,o2018confocal,lindell2019wave,faccio2020non,liu2020phasor,young2020non} or diffuse optical tomography~\cite{boas2001imaging,gibson2009diffuse,Lindell:2020:CDT} to recover objects by modeling and inverting scattering of light explicitly.
However, these methods all require exotic hardware setups, which are usually expensive, and are limited to some scenarios such as long propagation distance of light or scale of resolution.
Our approach removes the scattering effect using inverse rendering, requiring only a set of RGB images of a hazy scene.
Similar principles have been explored in concurrent and independent work~\cite{levy2023seathrunerf} for underwater scene reconstruction.