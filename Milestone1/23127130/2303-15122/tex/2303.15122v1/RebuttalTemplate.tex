\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
% \usepackage{caption,booktabs}

% \captionsetup{
%   justification = centering
% }

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\Rej}[1]{{\color{red}{kZTK#1}}}
\newcommand{\Bor}[1]{{\color{blue}{FzAK#1}}}
\newcommand{\Acc}[1]{{\color{green}{AhA9#1}}}
% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{7933} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Parameter Efficient Local Implicit Image Function Network for Face Segmentation}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix
We thank the reviewers for correctly characterizing our work as a lightweight, fast, face-parsing model that achieves state-of-the-art results with 26 times fewer parameters than competing models. We are grateful for their reviews and feedback on improving our paper. We provide further clarifications and report numbers on various ablations for our work. \\
\textbf{Choice of network architecture.}[\Rej{},\Bor{},\Acc{}]:
We chose our network architecture based on the approaches of low-dimensional parametric face models, and the incredible performance of implicit neural representation in 3D reconstruction and 2D image-to-image translation (LIIF). The network has a small encoder and LIIF decoder to perform face parsing. The reviewers are correct in inferring that the parameter reduction is due to a small encoder and decoder. And that this approach works well for face parsing because the problem is relatively well structured.
$\bullet$ \textbf{Diff. between LIIF and Conv/ViT+PE: }[\Rej{}] One key difference is that the MLP in a LIIF-like network learns a scale-invariant per-pixel mapping of a function (segmentation). This makes it an effective approach with even small encoders. LIIF performs an ensemble using the unfolded volume by doing a grid sampling over the $w\times h\times9D$ volume. This grid sampling result would differ for a $w \times h \times 9D$ volume and a $w\times h\times D$ volume due to differences in the neighbors of $D$ channels and unfolded $9D$ channels. We thank the reviewer for asking for this clarification and we will add it to the final paper.
$\bullet$ \textbf{Need for different strides: }[\Bor{}] Our two strided-convs in Fig 2 were added to reduce the spatial size of our feature volume from $256\times256$ to $64\times64$ to reduce the memory footprint of the network. 
$\bullet$ \textbf{256x256 input sizing: }[\Bor{}] We argue that with most cameras now yielding mega-pixel resolutions, 256x256 is a reasonable size for real-world face image use cases. We do agree that an even lower resolution can be trained, and we will add the results for that to the supplementary.\\

\noindent \textbf{Ablations}[\Rej{}] As suggested by the reviewer, we will add these ablation results to the final paper:
$\bullet$ \textbf{Network without LIIF Decoder: }
% This option is effective, however, only when an implicit representation is used; a conventional Conv decoder cannot achieve comparable performance. 
% To verify this and as suggested by the reviewer
We replaced the LIIF decoder with a Conv U-Net type decoder. The total parameter count of this model is 9.31M params (3x FP-LIIF). The Mean F1 for this model on LaPa dataset is 84.9 compared to 92.4 of our model. 
$\bullet$ \textbf{EDSR ResBlock vs BN/IN ResBlocks:} The Old-EDSR ResBlock network produces and F1 of 92.3 on the LaPa dataset. New ResBlock + BN produces 92.32 and ResBlock + IN produces 92.4. The slight improvement prompted us to use IN.
$\bullet$ \textbf{Edge-aware Cross-entropy and $\lambda$:}
The table below indicates the effect of $\lambda$ on the modulation of the edge-aware cross-entropy loss. 
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
$\lambda$      & 0     & 10   & 20    & 30    & 40   \\ \hline
F1 on LaPa & 91.73 & 92.2 & 92.29 & 92.34 & 92.4 \\ \hline
\end{tabular}
% \captionsetup{aboveskip=4pt,belowskip=-10pt}
% \caption{}
\label{table:lambda}
\end{table} 

\noindent \textbf{Further Comparisons: }[\Rej{}] As suggested by the reviewer, we will add the following comparisons to the final version of the paper.
$\bullet$ \textbf{SOTA Light-weight segmentation model for faces}:
The Table below shows the results for face segmentation using SFNet\cite{sfnet} which is a recent light-weight segmentation network for cityscapes.
\begin{figure}[!h]
    \centering
    \begin{tabular}{|c|c|c||c|c|c|}
    Class & SFNet & Ours & Class & SFNet & Ours \\
    Skin & 94.75 & 97.6 & R-Eye & 76.12 & 92.2 \\
    Hair & 87.27 & 96.0 & L-Brow & 76.98 & 90.90 \\
    Nose & 98.71 & 972. & R-Brow & 73.8 & 90.60 \\
    I-Mouth & 78.86 & 90.30 & U-Lip & 97.28 & 87.8 \\
    L-Eye & 79.55 & 92.00  & L-Lip & 96.23 & 89.5 \\
    \multicolumn{4}{|r|}{Mean} & 85.96 & 92.41
    \end{tabular}
\end{figure}

$\bullet$ \textbf{Better readability and reporting GFlops}:
We will fix our typos and reorganize the model parameters and inference speed numbers for better readability. DML\_CSR = 253 GFlops, EARG=235 GFlops, and Ours=85GFlops.

% \begin{table}[!ht]
% \centering
% \begin{tabular}{|l|l|l|l|}
% \hline
%       & DML\_CSR & EARG & Ours \\ \hline
% FLOPS & 253G      & 235G  & 85G   \\ \hline  
% \end{tabular}
% \captionsetup{aboveskip=4pt,belowskip=-10pt}
% \caption{flops for various models}
% \label{table:flops}
% \end{table}
$\bullet$ \textbf{Performance on non-aligned faces}:
We test our model over Helen dataset having non-aligned faces containing only 2k training images. Our numbers are still comparable to state-of-the-art model even when we are 26 times fewer in parameters.

\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|}
\hline
         & Mean F1 \\ \hline
DML\_CSR & 93.8   \\ \hline
SFNet    & 72.4   \\ \hline
ours     & 91.2   \\ \hline 
\end{tabular}
\label{table:helen}
\end{table}
$\bullet$ \textbf{Variance in performance over mulplte runs}: Mean and sd of F1 score for Lapa, CelebAMask-HQ and Helen:
\begin{table}[!ht]
\centering
\begin{tabular}{|l|l|l|}
\hline
        & Mean        & SD          \\ \hline
F1 Lapa & 92.35 & 0.06 \\ \hline
F1 Celeb & 85.90 & 0.20 \\ \hline
F1 Helen & 91.12 & 0.10 \\ \hline
\end{tabular}
\captionsetup{aboveskip=4pt,belowskip=-10pt}
\end{table}
 It should be noted that other state-of-the-art works do not report these mean and variance over multiple runs and therefore direct comparison of these numbers is not possible.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
% Table \ref{table:resblock}'s ablation over old edsr resblock, batch norm resblock, and our instnorm resblock influenced our decision to use instance norm in resblocks.
% Another ablation over $\lambda$ which modulates the edge-aware cross-entropy illustrates the affect of $\lambda$ on F1 in table \ref{table:lambda}. 


% \begin{table}[!ht]
% \begin{tabular}{|l|l|l|l|}
% \hline
%            & \begin{tabular}[c]{@{}l@{}}Old EDSR \\ ResBlock\end{tabular} & \begin{tabular}[c]{@{}l@{}}BatchNorm \\ Resblock\end{tabular} & \begin{tabular}[c]{@{}l@{}}Our \\ ResBlock\end{tabular} \\ \hline
% F1 on Lapa & 92.3                                                         & 92.32                                                         & 92.4                                                    \\ \hline
% \end{tabular}
% \captionsetup{aboveskip=4pt,belowskip=-10pt}
% \caption{}
% \label{table:resblock}
% \end{table}



% \textbf{Regarding $3\times3$ unfold vs $3 \times 3$ conv}:
% % \caption{Result of training lightweight cityspaces segmentation network SFNET\cite{sfnet} on Lapa dataset}
% It is true that if the subsequent operation is a standard weight multiplication, a $3\times3$ unfold can be replaced by a $3\times3$ conv. However, Liif performs an ensemble using the unfolded volume by doing a grid sampling over the $w\times h\times9D$ volume. This grid sampling result would differ for a $w \times h \times 9D$ volume and a $w\times h\times D$ volume due to differences in the neighbors of $D$ channels and unfolded $9D$ channels. 