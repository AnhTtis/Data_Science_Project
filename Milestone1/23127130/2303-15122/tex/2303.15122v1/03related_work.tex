\section{Related Work}
\subsection{Face Parsing}
Since face parsing intrinsically involves capturing the parametric relationship between the facial regions, the existing methods in face parsing aim at modeling the spatial dependencies existing in the pixels of the image. Multiple deep learning-based models with multi-objective frameworks have been proposed to handle spatial or inter-part correlations and boundary inconsistencies and capture the image's global context. Liu et al.\cite{lapa} proposed a two-head CNN that uses an encoder-decoder framework with spatial pyramid pooling to capture global context. The other head uses the shared features from the encoder to predict a binary map of confidence that a given pixel is a boundary which is later combined with the features from the first head to perform face parsing. EAGRnet \cite{eagrnet} uses graph convolution layers to encode spatial correlations between face regions into the vertices of a graph. Zheng et al.\cite{dml_csr} combine these approaches to build DML-CSR, a dual graph convolution network that combines graph representations obtained from shallow and deeper encoder layers to capture global context. Additionally, they employ multi-task learning by adding edge and boundary detection tasks and weighted loss functions to handle these tasks. They also use an ensemble-based distillation training methodology claiming that it helps in learning in the presence of noisy labels. They achieve state-of-the-art performance on multiple face-parsing datasets. Recently a transformer-based approach has also achieved state-of-the-art performance but with the help of training with additional data. FaRL\cite{farl} starts by pre-training a model with a face-image captioning dataset to learn an encoding for face images and their corresponding captions. Their image encoder, a ViT \cite{dosovitskiy2020image}, and a transformer-based text encoder from CLIP \cite{radford2021learning} learn a common feature encoding using contrastive learning. They then use the pre-trained image encoder and finetune on various face-related task datasets to report state-of-the-art numbers. We compare our performance numbers with a non-pre-trained version of FaRL\cite{farl} because we wanted to test our model on only the task-related dataset, and using additional image-caption data was out-of-the scope of this work. \\\\
While these approaches handle the image as a whole to predict a single mask segmenting all the components simultaneously, some approaches model the individual classes separately. These approaches, called the local methods, claim that focusing on the facial components (e.g. eyes, nose, etc.) results in more accurate predictions but at the expense of efficient network structure in terms of parameter sharing. Luo et al.\cite{6247963} propose a model which segments each detected facial part hierarchically into components and pixel-wise labels. Zhou et al.\cite{Zhou_2015} built interlinking CNNs to perform localization followed by labeling. Lin et al.\cite{lin2019face} propose an RoI-Tanh operator-based CNN that efficiently uses backbone sharing and joint optimization to perform component-wise label prediction. 
Our proposed method is a whole image-based method that uses a single encoder-decoder pair to parse faces using implicit neural representations on the global image scale.
 

 \subsection{Parametric Human Face Models and Implicit representations}
 Parametric models for the human face have been explored for a long time since the pioneering work of \cite{parke1974parametric}. Principle component analysis (PCA) was used to model face geometry and appearance in \cite{blanz1999morphable} to create a 3D Morphable model (3DMM) of the face. Many of the approaches where the 3D face model is estimated from the 2D image, estimate coefficients of the pre-computed statistical face models\cite{thies2016face2face,ploumpis2020towards,tu2019joint}. Other methods use regression over voxels or 3D meshes\cite{feng2018joint,wei20193d} of face to arrive at a detailed face model. Many approaches have also used 3DMM type models with deep learning techniques \cite{feng2021learning,deng2019accurate,dib2021towards,lattas2020avatarme,shang2020self}.% \\\\
 
With the emergence of Implicit Neural Representation \cite{sitzmann2019siren,park2019deepsdf,mescheder2019occupancy,mildenhall2020nerf}, a new approach to parameterizing signals has been gaining popularity. Instead of encoding signals as discrete pixels, voxels, meshes, or point clouds, implicit neural representation can parameterize these as continuous functions. A lot of work in the field has been around 3D reconstruction and shape modeling \cite{chen2019learning, kellnhofer2021neural, mescheder2019occupancy,mildenhall2020nerf,park2019deepsdf,yariv2020multiview,lombardi2019neural}. Deepsdf \cite{park2019deepsdf} encodes shapes as signed distance fields and models 3D shapes with an order of magnitude lesser parameters. Neural Volume\cite{lombardi2019neural} and NeRF\cite{mildenhall2020nerf} introduced 3D volume rendering by learning a continuous function over 3D coordinates. These works led to the use of implicit representation in the domain of human body or face rendering like \cite{ramon2021h3d,yenamandra2021i3dmm}, that use implicit representation to model human heads and torso in a detailed manner. Others like Pi-Gan\cite{chan2021pi} and NerFACE\cite{gafni2021dynamic} used it in the domain of faces. NerFACE\cite{gafni2021dynamic} can extract a dynamic neural radiance field face from a monocular face video and be used with the parameters of a 3DMM. Besides 3D modeling, implicit neural representation has also been used in 2D image-to-image translation. Local Implicit Image function (LIIF) \cite{liif} proposed an implicit neural representation-based super-resolution model that treats images in the continuous domain. Based on these approaches of low-dimensional parametric face models, stunning performance of implicit neural representation in 3D reconstruction, and 2D image-to-image translation, our choice of method for exploring face segmentation gravitated towards the implicit representation approach of LIIF. The 2D texture-less appearance of the face segmentation mask prompted us to explore a low-parameter version of the LIIF model for face parsing.
 
