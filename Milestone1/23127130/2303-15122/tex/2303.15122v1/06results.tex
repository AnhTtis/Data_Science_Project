\section{Results} 

According to Table \ref{table:lapa_results}'s LaPa results, \OURNAME performs better overall in mean-F1 and in classes such as eyes (left-eye and right-eye), brows (left-brow and right-brow), and skin. Table \ref{table:celeba_results} demonstrates that our approach performs better than the baselines on CelebA in terms of mean-F1 and also at the class-level F1 of skin, nose, eyes (left-eye, right-eye), lips(upper-lips, lower-lips), hair, necklace, neck, and cloth. We have also included a row of results demonstrating our performance when we change our output size to $512 \times 512$. The results show that even without training for a higher resolution output, our network seamlessly generates decent segmentation results at a higher resolution with nominal degradation in LaPa while still matching the current SOTA of $92.38$ by DML\_CSR. Table \ref{table:celeba_results} demonstrate superior performance at 512 resolution with a mean-F1 of $86.14$, which is $0.07$ higher than DML\_CSR. We achieve these results without training on multiple resolutions, i.e., we train on just $256\times 256$ and the network seamlessly scales to multiple resolutions. 
Our results on the Helen dataset, which has a small number of training samples (2000), are in Table \ref{table:helen_results}. Our performance is close to SOTA despite training on non-aligned face images.
Last but not least, in Table \ref{modelsize}, we comprehensively compare the model sizes, GFlops and FPS of all our baselines. With only 2.29 million parameters, \OURNAME is the most compact face-parsing network available; it is 65 times smaller than FARL and 26 times more compact than DML\_CSR. Our fps, evaluated on an Nvidia A100 GPU, stood at 110 frames per second, whereas DML CSR's performance was at 76 frames per second, and EAGR and AGR-Net demonstrated fps of 71 and 70, respectively. Additional comparative analysis of our results with DML\_CSR  is included in the supplementary.
 % Table \ref{tab:comparison_results} shows the results of different approaches 
% \noindent \textbf{Qualitative Visualizations.} 
% In this section, we discuss the key differences between the qualitative  results is illustrated in ~Fig.  \ref{fig:vis_forms_plus_pub}. 


\subsection{Ablations}
To evaluate the effect of several components we conduct the following ablations. 

\textbf{Network without LIIF Decoder: }We replaced the LIIF decoder with a Conv U-Net type decoder. The total parameter count of this model is 9.31M params (3x FP-LIIF). The Mean F1 for this model on LaPa dataset is 84.9 compared to 92.4 for our model. 

\textbf{EDSR ResBlock vs BN/IN ResBlocks:} The Old-EDSR ResBlock network produces an F1 of 92.3 on the LaPa dataset. New ResBlock + BN produces 92.32 and ResBlock + IN produces 92.4. The slight improvement prompted us to use IN.

\textbf{Edge-aware Cross-entropy and $\lambda$:} The table \ref{table:lambda} indicates the effect of $\lambda$ on the modulation of the edge-aware cross-entropy loss. 
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
$\lambda$      & 0     & 10   & 20    & 30    & 40   \\ \hline
F1 on LaPa & 91.73 & 92.2 & 92.29 & 92.34 & 92.4 \\ \hline
\end{tabular}
% \captionsetup{aboveskip=4pt,belowskip=-10pt}
\caption{Effect of edge-aware loss modulated by $\lambda$ on networks performance}
\label{table:lambda}
\end{table} 

\textbf{Comparison with lightweight segmentation model:}
Table \ref{table:sfnet} shows the results for face segmentation on LaPa using SFNet\cite{sfnet} which is a recent lightweight segmentation network for cityscapes.
\begin{figure}[!h]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
    Class & SFNet & Ours && Class & SFNet & Ours \\
    \cline{1-3} \cline{5-7}
    Skin & 94.75 & 97.6 && R-Eye & 76.12 & 92.2 \\
    Hair & 87.27 & 96.0 && L-Brow & 76.98 & 90.90 \\
    Nose & 98.71 & 972. && R-Brow & 73.8 & 90.60 \\
    I-Mouth & 78.86 & 90.30 && U-Lip & 97.28 & 87.8 \\
    L-Eye & 79.55 & 92.00  && L-Lip & 96.23 & 89.5 \\
    \hline
    \multicolumn{5}{|r|}{Mean} & 85.96 & 92.41 \\
    \hline
    \end{tabular}
    }
    \caption{Comparison with SFNet \cite{sfnet}, another lightweight segmentation network.}
    \label{table:sfnet}
\end{figure}
\vspace{-6pt}
\subsection{Low Resource Inference}
\label{sec:lowresource}
One of the practical advantages of \OURNAME is that it enables face parsing on low-resource devices. The primary prerequisite to enable
low resource inference is that a modelâ€™s inference should be low in compute and therefore have a high frame per second (FPS)
count. Our ability to predict segmentation masks at multiple resolutions enables us to meet the demand for low inference costs. To achieve this, we can instruct the network to perform a low-resolution prediction, and the result can be upscaled to a higher resolution. Table
\ref{table:fps-table} shows the FPS for lower-resolution inference on a single Nvidia A100 GPU. However, the shorter inference time should not result in poor quality output when upsampled to a higher resolution. Therefore we compare our upscaled outputs with the ground truth and
present the findings in Table \ref{table:lapa_results}, \ref{table:celeba_results}. Here, we can see that our 192$\times$192 or 128$\times$128 segmentation output, when upscaled to 256$\times$256, leads
to a minimal loss in quality, as can be seen in both classwise and overall F1 scores. 
% Further, it can be seen that the resolution of 192$\times$192 provides the best trade-off between FPS and F1 score. 
In Table \ref{table:lapa_results}, \ref{table:celeba_results} Ours$^{192\rightarrow256}$ denote results of upsampling from 192$\times$192 to 256$\times$256 and similarly Ours$^{128\rightarrow256}$,Ours$^{96\rightarrow256}$ and Ours$^{64\rightarrow256}$ denote results of upsampling from 128, 96 and 64 respectively to 256$\times$256. In addition to faster inference, a low parameter count or smaller size model  helps in model transmission under low bandwidth circumstances.
\begin{table}[!ht]
\centering
\scalebox{0.9}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Res & 64x64 & 96x96 & 128x128 & 192x192 & 256x256 \\ \hline
FPS        & 445   & 332   & 294     & 187     & 110     \\ \hline
FLOPS      & 27.44 & 32.25 & 39      & 58.24   & 85.2    \\ \hline
\end{tabular}
}
\caption{Frame Per Second for different resolution output while keeping the input image resolution constant at 256$\times$ 256}. 
\label{table:fps-table}
\end{table}
\vspace{-0.5cm}
\subsection{Limitation}
Encouraged by the performance of this low parameter \OURNAME network in face segmentation, we tested its effectiveness at semantic segmentation in a more generic domain like Cityscapes \cite{Cordts2016Cityscapes}. We chose this dataset because it lacked the structural regularity that we exploited in this work and segmentation using the current architecture should not be feasible. As expected, the mIoU score on the validation was reported at 62.2, which is 20+ points lower than SOTA models reporting scores in the range of $\sim$85 mIoU. 
% Another aspect of \OURNAME is the inference speed
% \todo{conv decoder} 
% \todo {no edge loss}
% \todo {multi resolution training}
% \label{sec:ablation}
