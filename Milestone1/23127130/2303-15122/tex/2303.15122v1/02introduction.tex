\section{Introduction}
\label{sec:intro}

Face parsing is the task of assigning pixel-wise labels to a face image to distinguish various parts of a face, like eyes, nose, lips, ears, etc. This segregation of a face image enables many use cases, such as face image editing\cite{face-edit,NeuralFace2017,zhang2019synthesis}, face e-beautification \cite{emakeup}, face swapping \cite{faceswap, transferpotrait, nirkin2018face}, face completion \cite{li2017generative}.

Since the advent of semantic segmentation through the use of deep convolutional networks\cite{long2015fully}, a multitude of research has investigated face parsing as a segmentation problem through the use of fully convolutional networks \cite{guo2018residual,jackson2016cnn,lapa,lin2019face,rtnet,liu2015multi}. 
In order to achieve better results, some methods \cite{jackson2016cnn,liu2015multi} make use of conditional random fields (CRFs), in addition to CNNs. Other methods \cite{fprnn,lin2019face}, focus on a two-step approach that predicts bounding boxes of facial regions (nose, eyes, hair. etc.) followed by segmentation within the extracted regions. Later works like AGRNET\cite{agrnet} and EAGR\cite{eagrnet} claim that earlier approaches do not model the relationship between facial components and that a graph-based system can model these statistics, leading to more accurate segmentation. 

\begin{figure}
\begin{center}
  \includegraphics[width=\linewidth]{images/liifdiag.png}
\end{center}
\caption{The simple architecture of Local Implicit Image representation base \OURNAME: A light convolutional encoder of modified resblocks followed by a pixel only MLP decoder} 
\label{fig:banner}
\end{figure}
In more recent research, works such as FaRL\cite{farl} investigate pretraining on a human face captioning dataset. They pre-train a Vision Transformer (ViT) \cite{dosovitskiy2020image} and finetune on face parsing datasets and show improvement in comparison to pre-training with classification based pre-training like ImageNet\cite{ILSVRC15}, etc., or no pre-training at all. The current state-of-the-art model, DML\_CSR\cite{dml_csr}, tackles the face parsing task using multiple concurrent strategies including multi-task learning, graph convolutional network (GCN), and cyclic learning. The Multi-task approach handles edge discovery in addition to face segmentation. The proposed GCN is used to provide global context instead of an average pooling layer. Additionally, cyclic learning is carried out to arrive at an ensemble model and subsequently perform self-distillation using the ensemble model in order to learn in the presence of noisy labels.


\begin{figure*}[hbt!]
\begin{center}
  \includegraphics[width=\linewidth]{images/Encoder.png}
\end{center}
\caption{Encoder Architecture: It has three res-block groups. The first two (2,6) res-block groups, followed by a strided convolution per group, are mainly used to reduce the spatial dimensions of the activation maps. The final group of res-blocks creates the grid of features vectors $Z$. Notice each res-block group has a group-level residual connection.} 
\label{fig:encoder}
\end{figure*}

In this work, we perform face segmentation by taking advantage of the consistency seen in human facial structures. We take our inspiration from various face modeling works \cite{blanz1999morphable,gerig2018morphable,zollhofer2018state} that can reconstruct a 3D model of a face from 2D face images. These works show it is possible to create a low-dimensional parametric model of the human face in 3D. This led us to conclude that 2D modeling of the human face should also be possible with low dimension parametric model. Recent approaches, like NeRF\cite{mildenhall2020nerf} and Siren \cite{sitzmann2019siren} demonstrated that it is possible to reconstruct complex 3D and 2D scenes with implicit neural representation. Many other works \cite{ramon2021h3d,yenamandra2021i3dmm,chan2021pi,gafni2021dynamic} demonstrate that implicit neural representation can also model faces both in 3D and 2D. However, to map 3D and 2D coordinates to the RGB space, the Nerf\cite{mildenhall2020nerf} and Siren\cite{sitzmann2019siren} variants of the models require training a separate network for every scene. This is different from our needs, one of which is that we must map an RGB image into label space and require a single network for the whole domain. That brings us to another method known as LIIF\cite{liif}, which is an acronym for a Local Implicit Image Function and is used to perform image super-resolution. They learn an approximation of a continuous function that can take in any RGB image with low resolution and output RGB values at the sub-pixel level. This allows them to produce an enlarged version of the input image. Thus, given the current success of learning implicit representations and the fact that the human face could be modeled using a low-dimension parametric model, we came to the conclusion that a low parameter count LIIF-inspired model should learn a mapping from a face image to its label space or segmentation domain. In order to test this hypothesis, we modify a low-parameter version of EDSR\cite{edsr} encoder such that it can preserve details during encoding. We also modify the MLP decoder to reduce the computing cost of our decoder. Finally, we generate a probability distribution in the label space instead of RGB values. We use the traditional cross-entropy-based losses without any complicated training mechanisms or loss adaptations. An overview of the architecture is depicted in Figure \ref{fig:banner}, and more details are in Section \ref{sec:method}. Even with a parameter count that is $1/26^{th}$ compared to DML\_CSR\cite{dml_csr}, our model attains state-of-the-art F1 and IoU results for CelebAMask-HQ\cite{celebAmaskHQ} and LaPa\cite{lapa} datasets. Some visualizations of our outputs are shared in Figure \ref{fig:celeb-we-better} and Figure \ref{fig:lapa-we-better}.

To summarise, our key contributions are as follows:

\begin{itemize}[nosep]
\item We propose an implicit representation-based simple and lightweight neural architecture for human face semantic segmentation. 

\item We establish new state-of-the-art mean F1 and mean IoU scores on CelebAMask-HQ\cite{celebAmaskHQ} and LaPa\cite{lapa}.

\item Our proposed model has a parameter count of $1/26^{th}$ or lesser compared to the previous state-of-the-art model. Our model's SOTA configuration achieves an FPS of 110 compared to DML\_CSR's FPS of 76.
\end{itemize}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=0.9\linewidth]{images/Celeb-we-better.png}
\end{center}
\caption{Visualization of a few results in CelebAMask-HQ dataset. The difference between DML\_CSR and our results is highlighted. The cloth region in a), b), eyes in b),c) d) and nose in d),e) are better predicted by \OURNAME } 
\label{fig:celeb-we-better}
\end{figure}

\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=0.9\linewidth]{images/lapa-we-better.png}
\end{center}
\caption{Visualization of a few results in LaPa dataset. The difference between DML\_CSR and our results is highlighted. Eyes in b),c),d), Brows in b),d) are better predicted by \OURNAME } 
\label{fig:lapa-we-better}
\end{figure}