% !TEX root =  ../main.tex

\section{Guarantees in interval settings}\label{sec:guarantees}

Given a learning task oder an MDP where the MDP is treated as a black box, various methods can 
be employed to learn a suitable strategy $\sigma$. Further, from the samples collected in the 
learning process, an IMDP $\cI$ can be built ($\rightarrow$ PAC learning, transition 
probabilities according to Hoeffding bound etc.) which can be reduced to an IDTMC $\cI^\sigma$. 
\pwnote{Point out difference to PAC: they consider all strategies (to give guarantees over the 
systems) while we only consider one concrete strategy for which we give guarantees}.

Given $\cI^\sigma$ and a value function $V$ (see reachability objective, sec 2.3) we ask for the 
instantiation with the worst (lowest) value in the initial state, i.e., $\argmin_{\cM\in [\cI^
\sigma]} V_\cM(s_i)$.

Use \cite{Givan00} interval value iteration on IDTMCs to find $\min_{\cM\in [\cI^
\sigma]} V_\cM(s_i)$ (and from there $\argmin$ if we want).

Even further, given an IMDP $I$ we can use \cite{Givan00} interval value iteration 
on IMDPs to find $\argmin_{\sigma}\min_{\cM\in [\cI^\sigma]} V_\cM(s_i)$ and
the corresponding value function. This is useful in case $\sigma$ is not clear
as is th case in PAC-learning, Q-learning and $\epsilon$-RL. Also we can compute
this to confirm the $\sigma$ matches the learned habits in our approach.
\hrule
New plan: don't use \cite{Givan00} but a combination of \cite{HadMon18}, 
\cite{Ye10} and \cite{Lit95}.

\paragraph{Summary of \cite{HadMon18} (HAL paper):} Basic idea is to use two
value iteration sequences, $(x_s)_{s\in S}$ and $(y_s)_{s\in S}$ with different
initial values, $x$ uses standard VI, initializing goal states with 1 and all
others with 0. $y$ initializes $s_{-}$ states (essentially ECs without goal states)
with 0 and everything else with 1. The true VI fixed point is guaranteed to be 
between $x$ and $y$ and thus a suitable stopping criterion is $\lvert y \rvert - 
\lvert x \rvert < \varepsilon$. Requirements for this method to work:
\begin{itemize}
\item (I)MDP needs to be min-/max-reduced (for $s_{-}$)
\item absolute bounds must be known (to bound the value function so we can 
initialize $x$ and $y$)
\end{itemize}

The former also introduces the problem that the conversion from IMDP to MDP they
perform introduces an exponential amount of actions (exponential in the number of 
successor states). The idea behind this transformation is similar to \cite{Givan00} 
- each action represents an ordering of the states and the resulting probability 
distribution.

The latter does not (immediately) allow general rewards. In fact, \cite{HadMon18} 
only considers (non-discounted) reachability.

If we piggyback on that paper/the idea we need another lower/upper bound. Maybe
some assumption on $p_{min}$ might help. However, the approximation for this tend
to be extremely large - I don't know if we can still guarantee polynomial 
convergence then.

\paragraph{Summary of \cite{Ye10}:} Discounted MDP were known beforehand to be
solvable in polynomial time via PI and VI (\cite{Lit95}). Their contribution is
to show it's strongly polynomial, i.e., polynomial in the number of states and
actions, but not in the number of bits required to specify the LP representation.
Or equivalently, they allow the assumption that arithmetic operations take one unit
of time, irrelevant of the size.

Later, Ye and Post \cite{PostYe13} took the same idea and dropped discounts. 
However, then polynomial complexity only applies to deterministic MDPs, so it's not
relevant for us.

Their argumentation is heavily based on LP (which I honestly know too little about
to understand all of their techniques) and does imo not give much insight as to
how we can extend this to IMDPs without discounts and without ECs since they never
use the graph structure of the MDP which also means they never consider ECs. A
better reference might be \cite{Lit95}: They survey PI, VI, LP and other methods
and do a good job of outlining what definitely does not work in polynomial time.

\paragraph{Summary of \cite{Lit95}:} PI is exponential if a policy change is 
allowed in only one state per policy iteration step. The case for other policy
update rules was unresolved back then but has since been shown to be exponential 
for all cases, including the greedy method widely used \cite{Fea10}.

Discounted VI on the other hand is polynomial in number of states, number of 
actions, number of bits required to specify MDP, and - if you consider non-fixed 
discount rates - $1/(1-\gamma)$. After the VI with stopping criterion of an
absolute change of $\frac{1}{2}\frac{1}{1-\gamma}\varepsilon$ the policy is 
guaranteed to be $\varepsilon$-optimal. However, as plugging $\gamma=1$ suggests, 
this does not apply to the non-discounted case (see \cite{HadMon18} for a 
counterexample).

By, e.g., \cite{KalLN20} the discount aspect of MDPs is widely interchangeable with
only handling contracted MDPs. This implies that the same statements hold for
contracted, undiscounted MDPs that we are interested in.

It remains for us to show transfer polynomiality to IMDPs in one of two ways:
\begin{itemize}
\item by \cite{Lit95} and \cite{KalLN20}, undiscounted contracted MDPs are polynomial. Show that intervals do not not make a difference, or
\item by \cite{HadMon18} discounted IMDPs are polynomial (which I'll have to double check since I'm not 100\% sure). Show that the "discount and contraction"-statement from \cite{KalLN20} does not only hold for MDPs but also IMDPs
\end{itemize}

As a side note for the latter - there is one notable difference in the two use 
cases of IMDPs, the one where you combine similar states to reduce the state space
(see Givan \cite{Givan00}) and the one where you legitimately do not know the
probabilities (SMC, our case). In the former a transition may be the interval 
$[0,p]$ depending on how exactly states are grouped. However, in the latter the
interval can be assumed to be the open interval $(0,p]$ since a transition was
observed and thus the transition probability is strictly greater than 0. Excluding
$[0,p]$ intervals from the IMDP may be helpful in determining ECs later on.

Overview (red - we improve on them / green - we had to drop benefits / yellow - something in between / ours is what we plan)

\begin{center}
\begin{tabular}{| c | c c c c c |} 
 \hline
  & VI (eg PRISM) & Givan \cite{Givan00} & Haddad (HAL) \cite{HadMon18} & Ye \cite{Ye10}, Littman \cite{Lit95} & us \\
 \hline
 discounted & no & \textcolor{red}{yes} & no & \textcolor{red}{yes} & no \\ 
 (non-zero) ECs allowed & \textcolor{green}{yes} & \textcolor{yellow}{no mention} & \textcolor{green}{yes} & \textcolor{green}{yes} & no \\
 rewards & all & all & \textcolor{red}{reachability} & all & all \\
 IMDP & \textcolor{red}{no} & yes & yes & \textcolor{red}{no} & yes \\
 complexity & \textcolor{red}{no conv. guar.} & poly & \textcolor{yellow}{not clear to me} & (str.) poly & ? \\
 solid proof & - & \textcolor{red}{no} & yes & yes & hopefully :) \\
 \hline 
\end{tabular}
\end{center}

\paragraph{Contracting IMDP to discounted IMDP}

We call an IMDP $\cI$ contracting if and only if all instantiations
$\cM \in \cI$ are contracting.  Wlog.  we assume $\cI$
has exactly one terminal state $\bot$.

Given a contracting IMDP $\cI$ we construct a discounted IMDP
$\cI'$,  a discount factor $\gamma \in (0,1)$ and a rational number $\mu$ 
s.t.

\begin{itemize}
\item maximal total reward in $\cI = \mu \cdot$maximal discounted reward in $\cI'$
\item same for minimal reward 
\end{itemize}

An exponential blow-up solution is easily achievable since any IMDP can be
transformed into an MDP with the same max/min reward properties (see Haddad
\cite{HadMon18}). Afterwards we can apply Christel's transformation (see PDF).

If we extend the transformation to IMDPs however,  we may achieve a polynomial
transformation. 

\paragraph{Choice of $\mu_s$}

In Christel's transformation the key aspect is to compute $\mu_s$ for all $s\in S$
as the maximum number of steps required to reach $\bot$ from $s$.  This is not
immediately applicable to IMDPs since it depends on the transition probabilities.
However, the only relevant property of the $\mu_s$ is
\begin{equation}\label{eqn:mu_condition}
\mu_s \geq 1+\sum_{t\in S} P_\cM(s,\alpha,t) \mu_t
\end{equation}
which in the IMDP case would have to hod for all $\cM\in[\cI]$
From there, $\gamma$ and $P_\cI$ can be derived in the same way as
for MDP.

For the IMDP case we choose \footnote{this is doable according to \cite{Wu08}}
\[
\mu_s = \max_{\cM\in[\cI]} m(\cM,s)
\]

where $m(\cM,s)$ is the maximum expected number of steps in $\cM$ to reach
$\bot$ from $s$. Note that by definition $\cM$ may be different for different $s$.
However, we can prove there is an instantiation maximizing all states at the same time:

\begin{lemma}
Let $\cI$ and $(\mu_s)_{s\in S}$ be defined as above. Then there is an MDP $\cM^{+}
\in [\cI]$ s.t. $\mu_s = m(\cM^{+},s)$ for all $s\in S$.
\end{lemma}

\paragraph{Proof.} Assume we have two state $s,t\in S$ with and edge from $s$
to $t$, an action $\beta\in\Act(s)$ and two instantiations, $\cM_1,\cM_2\in[\cI]$ that
agree on the instantiation of all transition probabilities except of $P(s,\alpha,t)$.
Recall that for all $cM\in[\cI]$ and $s\in S$
\[
m(\cM,s) = 
1 + \max_{\alpha\in \Act(s)} \sum_{t\in S} P_\cM(s,\alpha,t)m(\cM,t) 
\]
In the case $\beta$ is not the argmax in the equation above,  it is clear that
$m(\cM_1,\cdot)=m(\cM_2,\cdot)$.
 
Otherwise, wlog., assume $m(\cM_1,s)\geq m(\cM_2,s)$. Since $\cM_1$ and
$\cM_2$ agree on the instantiation of all other transition probabilities,
and $P_\cM(s,\alpha,t)>0$ and $m(\cM,t)>0$, from the system of equations
of $m$ it follows that $m(\cM_1,x)\geq m(\cM_2,x)$ for all states $x\in S$.
In other words, adjusting the instantiation of a transition much that $m$ is
increased in the source state of the transition can nor decrease $m$ in any
state.

Next, construct an IMDP $\cI'$ by adding a fresh state $i$ with
$\Act(s) = {a}$ and for all $s\in S$ let $P_\cI'(i,a,s) = 
[1/\lvert S \rvert,1/\lvert S \rvert]$. Note that since $i$ has no ingoing transitions,
$(\mu_s)_{s\in S}$ is equivalent in $\cI$ and $\cI'$.

Assume $\cM\in [\cI']$ is an instantiation s.t. $m(\cM,i)$ is maximal.  Because
\[
m(\cM,i) = 
1 + \sum_{t\in S} 1/\lvert S \rvert m(\cM,t) 
\]
and we have shown that increasing $m$ locally cannot decrease $m$
in another state globally, this means $m(\cM,t)$is locally maximal for all
$t \in S$, i.e., changing a single transition instantiation can not increase
$m(\cM,t)$ in any state $t \in S$.

\pwnote{this still doesnt prove global max. e.g. it could be that $m$ has two
locally optimal solutions over states $(s_1,s_2)$, e.g., $(1,2)$ and $(3,1)$,
but I doubt this is possible. maybe there's some LP result that tells us there
is only one global maximum?}
$\hfill\qed$

\begin{lemma}
Let $\cI, \cM^{+},(\mu_s)_{s\in S}$ be defined as above. Then 
\Cref{eqn:mu_condition} holds for all $\cM\in [\cI]$.
\end{lemma}

\paragraph{Proof.} This directly follows from expanding the definition of $\mu_s$ as
for all states $s\in S$ and instantiations $\cN\in [\cI]$
\begin{align*}
\mu_s &= \max_{\cM\in[\cI]} m(\cM,s) \\
&= m(\cM^{+},s) \\
&= \max_{\alpha\in\Act(s)} 1+\sum_{t\in S} P_{\cM^{+}}(s,\alpha,t)m(\cM^{+},s) \\
&= \max_{\alpha\in\Act(s)} 1+\sum_{t\in S} P_{\cM^{+}}(s,\alpha,t)\mu_t \\
&\geq 1+\max_{\alpha\in\Act(s)}\sum_{t\in S} P_\cN(s,\alpha,t)\mu_t 
\end{align*}
$\qed$

This means the max/min-reward preserving transformation from contracting
MDP $\cM$ to discounted MDP $\cN$ is applicable to every $\cM\in [\cI]$, i.e., 
\begin{itemize}
\item $(\mu_s)_{s\in S}$ as above
\item $\gamma = \max_{s\in S\setminus \{\bot\}} \frac{\mu_s-1}{\mu_s}$
\item $w_\cN(s)=w_\cM(s)/\mu_s$
\item $P_\cN(s,\alpha,t) = \frac{P_\cM(s,\alpha,t)\mu_t}{\gamma\mu_s}$ for $t\neq\bot$
\item $P_\cN(s,\alpha,\bot) = 1-\sum_{t\in S} P_\cN(s,\alpha,t)$
\end{itemize}

where again \cref{eqn:mu_condition} ensures $P_\cN(s,\alpha,t)$ is a distribution.
Since the transformation of the transition probabilities is linear it is clear that
$P_\cN(s,\alpha,t)$ is minimal iff $P_\cM(s,\alpha,t)$ is minimal (same for maximal)
and continuous. Thus, for a contracting IMDP $\cI$, we can attempt to give the
following transformation to a discounted IMDP $\cI'$:

\begin{itemize}
\item $(\mu_s)_{s\in S}$ as above
\item $\gamma = \max_{s\in S\setminus \{\bot\}} \frac{\mu_s-1}{\mu_s}$
\item $w_\cN(s)=w_\cM(s)/\mu_s$
\item $\underline{P_\cN}(s,\alpha,t) = 
\frac{\underline{P_\cM}(s,\alpha,t)\mu_t}{\gamma\mu_s}$ for $t\neq\bot$
\item $\overline{P_\cN}(s,\alpha,t) = 
\frac{\overline{P_\cM}(s,\alpha,t)\mu_t}{\gamma\mu_s}$ for $t\neq\bot$
\item transitions to $\bot$?
\end{itemize}

However, here it is not clear what to do with $P_\cN(s,\alpha,\bot)$. The issue is
that we need to ensure that any instantiation of $P_\cN(s,\alpha,t)$ would have to
translate back to a valid instantiation of $P_\cM(s,\alpha,t)$. This is however not
the case because the transition to $\bot$ does not exist in $\cM$, and thus, because
all $\mu_t$ might be different, the backwards direction would not always work but
sometimes make it so $\sum_{t\in S} P_\cN(s,\alpha,t)$ sums to less or more than 1.
This is also not fixable with IMDP as a model (at least not with this definition of $\mu$).

Example:

\input{img/contracting_imdp}

with $w(s_1)=1$ and $w(s_2)=2$ and all other weights 0.

This IMDP (even Interval Markov Chain) clearly has a max accumulated weight of $7/4$.
It's also easy to see that $(\mu_{s_i})_{i\in\{0,1,2,3\}} = (7/2,1,3,2)$ and hence $\gamma=5/7$.
Using the transformation we obtain $w(s_1)=1$ and $w(s_2)=2/3$ and the following IMDP:

\input{img/discounted_imdp}

Note that since we are only interested in the expected rewards, the transitions after visiting
$s_1$ and $s_2$ are irrelevant, since the reward is always 0.

If we were to transform the instantiation with $P(s_0,\alpha,s_1)=1/4$, we'd obtain
$P'(s_0,\alpha,\bot)=0$ in the transformed IMDP. So it's reasonable to have $a=0$.

However, if $a=0$ then $P'(s_0,\alpha,s_1)=3/10$ and $P'(s_0,\alpha,s_2)=7/10$ is
a valid instantiation of the discounted IMDP which would have the (maximal) discounted
reward 23/42. Scaling by $\mu_{init}$ (as in the MDP transformation) yields an incorrect
maximal reward of the initial IMDP of $23/12 > 7/4$.

The issue can be explained by running the transformation backwards:
If $P'(s_0,\alpha,s_1)=3/10$ and $P'(s_0,\alpha,s_2)=7/10$ this corresponds to
transitions in the contracting MDP of $P(s_0,\alpha,s_1)=3/4$ and $P(s_0,\alpha,s_2)=7/12$
which, if plugged into the Bellman equations, would indeed produce an expected reward
of $23/12$, but of course is not a valid instantiation of the IMDP.

\vspace{5ex}

An attempt to solve this would be to pass the vector $\mu$ to the discounted IMDP and:

\begin{itemize}
\item $P_\cN(s,\alpha,\bot)=[0,1]$
\item side constraint $\sum_{t\in S}P_\cN(s,\alpha,t)/\mu_t = 1/(\gamma\mu_s)$
\end{itemize}

The side constraint essentially ensures that the transformation is guaranteed to work
backwards and when performed always produces a valid instantiation of the initial
contracting MDP.

The side constraint can possibly be integrated into Givan's algorithm where we add
the additional constraint when building the instantiation from the state-ordering.
We essentially have two constraints:

\begin{itemize}
\item $\sum_{t\in S}P_\cN(s,\alpha,t) = 1$
\item $\sum_{t\in S}P_\cN(s,\alpha,t)/\mu_t = 1/(\gamma\mu_s)$
\end{itemize}

Essentially this is two cost/budget-maximization problems. Solving them in parallel
may be hard (idk, maybe LP has a solution?) but the second
constraint implies $\sum_{t\in S}P_\cN(s,\alpha,t) \leq 1$. From there we can add the
remaining probability in the transition to $\bot$. This does not increase the expected
weight and also it does not break the second condition since $\mu_\bot=0$.

\paragraph{Summary}

Overall, we can:

\begin{itemize}
\item calculate $\mu$ as in \cite{Wu08}
\item transform bounds as Kallenberg \cite{KalLN20}
\item add side constraint
\item use Givan algorithm with adapted MDP construction
\end{itemize}

However,  \cite{Wu08} requires that all states have positive, finite value.
Positivity is given for reachability ($w=1$ for all states), but finiteness
requires intervals to exclude $0$. Hence, we can use Christel's second
proposed definition of $\mu$ anyway. (if it works - there's a small error
I could not fix yet).
Idk whether the above definiton of $\mu$ has any use for us then.

\rule{10cm}{0.4pt}

\paragraph{Stochastic games}
PAC SMC considers SG, utilizing minimizing and maximizing nodes. We can interpret
such systems with the following semantic: Min-nodes are states where we (or our 
agent) have full control over the possible actions. Max-nodes are states where
either an opposing player makes a move (classic game setting) or the environment
non-deterministically performs an action. In either case we want to learn \emph{a}
strategy in min-nodes s.t. \emph{for all} strategies of the opponent/environment
we can guarantee a certain value. So in short: we do habit learning in min-nodes
and PAC SMC in max-nodes.

\pwnote{SG/2PG also ties into CeTI much better and allows for follow-up work.
the issue is that we can't really improve PAC in the max nodes because we assume
optimal play from the opponent and thus have to consider all strategies. if the 
opponent is however human, we can detect their habits and reduce the search space
to only those strategies that respect their habits.}

\cite{Givan00} onlys consider MDPs. On the other hand, \cite{HanMil13} considers 
2PG and shows that strategy iteration works on them, but does not consider the 
interval version. We would have to come up with a combined approach that lets us 
perform strategy iteration on interval SGs and more importantly, prove its 
correctness.

\pwnote{we possibly have to be careful that this does not overshadow the habit
contribution}
