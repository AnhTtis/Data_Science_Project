% !TEX root =  ../main.tex

\section{Learning under limited sampling access}\label{sec:habits}
Previous work has shown that RL algorithms utilizing the OFU principle converge towards
an optimal solution and are also able to perform well in practice~\cite{SutBar18}.
However, they are known to converge rather slowly, requiring lots 
of sampling data and training time.
In this section, we use our IMDP-RL algorithm presented in \Cref{alg:imdp-rl}
in a setting where sampling access is limited, i.e., %In particular we model this by limiting sampling access
the parameters $K$ and $N$ are fixed.
Then, the OFU principle might be not suitable anymore,
as the strategy is learnt under an optimistic view for
increasing confidence in the actions' impacts, which 
requires lots of samples for every action. 
We propose to focus on finding ``good'' strategies 
within the bounded number samples rather than on guaranteed convergence 
to an optimal strategy. Specifically,
we present two complementary methods to reduce the action spaces during sampling:
\emph{lower confidence bound sampling} and \emph{action scoping}.
Both methods are parametrizable and thus can be adapted to the model size as well 
as the bound imposed on the number of samples. 

\subsection{Lower confidence bound sampling}
As new sampling strategy incorporated in \Cref{l:policy} of \Cref{alg:imdp-rl}, 
we propose to choose an action $a$ in a state $s$ 
if it has the highest \emph{lower bound} $\underline{Q}(s,a)$ instead of the highest
\emph{upper bound} as within UCB sampling.
While then the agent still naturally chooses actions that were already sampled often with high
rewards, this avoids further exploring actions with high transition uncertainty.
However, such a \emph{lower confidence bound (LCB)} sampling
might result in performing exploitations only. Hence, we include
an $\epsilon$-greedy strategy~\cite{SutBar18}
into LCB sampling: In each step, with probability $1{-}\epsilon$ the action with the 
highest LCB is sampled and with probability $\epsilon$ a random action is chosen.
In the following, we identify LCB sampling with a degrading $\epsilon$-greedy LCB strategy.
Note that also any other exploration strategies, such as sampling with decaying $\epsilon$ or
\emph{softmax action selection}~\cite{SutBar18}, can easily be integrated into LCB sampling.

While our focus of LCB sampling is on exploiting ``good'' actions, we can still guarantee 
convergence towards an optimal strategy in the long run:

\begin{theorem}\label{thm:pac}
\Cref{alg:imdp-rl} with LCB sampling converges towards an optimal solution,
i.e., for $K\rightarrow\infty$ both $\underline{V}$ and $\overline{V}$ 
converge pointwise towards $V^{*}$, and their corresponding strategies
$\underline{\sigma}$ and $\overline{\sigma}$ converge towards optimal strategies.
\end{theorem}

Similar to how UCB sampling can provide PAC guarantees~\cite{AshKreWei19},
we can provide PAC guarantees for the value function bounds returned by \Cref{alg:imdp-rl} as
\Cref{thm:guarantees} guarantees that the solution %can be guaranteed
is in the computed interval with high probability $1-\delta$
and \Cref{thm:pac} guarantees that the interval can become arbitrarily small
converging towards the optimal solution from both sides.

\subsection{Action scoping}


As another approach to compete with resource constraints, 
we propose to permanently remove unpromising actions from the 
learned IMDP model, forcing the agent to focus on a subset of enabled actions from the environment MDP.
We formalize this idea by setting the \emph{scope} of a state to the set of actions 
that the agent is allowed to perform in that state.

\subsubsection{Scope formation}
As depicted in \Cref{fig:workflow}, scopes are introduced after each episode
based on the samples of that episode.
Initially, all enabled actions are within a scope.
Removing an action $a$ from the scope in $s$ is enforced by modifying the interval transition function
$\hat{T}$ of $\cU$ to the zero interval function at $(s,a)$, i.e., $\hat{T}(s,a,s') = [0,0]$ 
for all $s'\in\Post(s,a)$. 
Scope formation has several notable advantages. First, removing action $a$ from a scope
in $s$ reduces the action space $\Act(s)$, leading to more sampling data for remaining actions
as $\sigma(s)(a)=0$ for all future episodes.
Further, the removal of actions may also reduce the state space in case states are only 
reachable through specific actions.
These positive effects of scoping come at its cost
of the algorithm not necessarily converging towards an optimal strategy anymore (cf. \Cref{thm:pac}).
The reason is in possibly removing an optimal action due to unfortunate sampling.

\subsubsection{Eager and conservative scopes}
We introduce two different scoping schemes: \emph{eager} 
and \emph{conservative}. Both schemes are tunable by a parameter $h\in\ ]0,1[$
that specifies the transition error tolerance
similar as $\eta = \delta/\Nt$ does in our IMDP construction (see \Cref{subsec:mb}).
Intuitively, while the formal analysis by means of \Cref{l:bounds} in \Cref{alg:imdp-rl}
guarantees $1{-}\delta$ correctness, we allow for different 
confidence intervals depending on $h$ when forming scopes. 
Here, greater $h$ corresponds to higher tolerance and hence smaller action scopes.

To define scopes, we introduce $\cU_h = (S,A,\imath,G,R,\hat{T}_h)$. That is, $\cU_h$
is an IMDP with the same topology as the internal model $\cU$, but allows an
error tolerance of $h$ in each transition.
We denote the corresponding solution to the interval Bellman equations of $\cU_h$ by
$\underline{V}_h$ and $\overline{V}_h$, respectively, and the quality functions as
$\underline{Q}_h$ and $\overline{Q}_h$.
Additionally, the \emph{mean quality function} $\dot{Q}$ is computed from the solution of the
Bellman equations on the maximum likelihood MDP $\dot{\cM} = (S,A,\imath,G,R,\dot{T})$ where 
$\dot{T}(s,a,s') = \#(s,a,s')/\#(s,a)$ are the maximum likelihood estimates of the transition probabilities.
Both functions can be computed, e.g., using standard value iteration approaches.

In state $s$ an action $a$ is \emph{eagerly} removed from its scope 
if $\dot{Q}(s,a) < \underline{V}_h(s)$, i.e., 
if the mean quality of $a$ is lower than the lower bound of the (presumably) best action.
The idea is that $a$ is most likely not worth exploring if its expected value 
is lower than what another action provides with high probability.
Likewise, an action $a$ is \emph{conservatively} removed from the scope of a state $s$ 
if $\overline{Q}_h(s,a) < \underline{V}_h(s)$, i.e.,
the upper bound quality of $a$ is lower than the lower bound of the (presumably) best action.
Here the idea is similar as for eager scoping but with a more cautious estimate on
the expected value from action $a$ (observe $\overline{Q}_h(s,a) > \dot{Q}(s,a)$).
Note that the parameter $h$ is only used as an error tolerance in $\cU_h$ in order
to reduce the action scopes. The bound $\underline{V}$ and $\overline{V}$ returned
in \Cref{alg:imdp-rl} still use an error tolerance of $\delta/\Nt$ per transition.
