% !TEX root =  ../main.tex

\section{Implementation and evaluation}\label{sec:evaluation}
To investigate properties of the algorithms presented, we developed a prototypical implementation in \python\ 
and conducted several experimental studies, driven by the following research questions:

\begin{enumerate}[label=\textbf{(RQ\arabic*)},leftmargin=*]
	\item\label{rq:sampling}
		How do UCB and LCB influence the quality of synthesized strategies?
	\item\label{rq:scoping}
		Does action scoping contribute to synthesize nearly optimal strategies 
		when limiting the number of samples?
\end{enumerate}

\subsection{Experiment setup}
We ran our experiments on various community benchmarks from the formal-methods and RL communities.
All our experiments were carried out using \python\ 3.9 on a MacBook Air M1
machine running macOS 11.5.2. For each system variant and scoping parameter, 
we learn $M$ strategies (i.e., run the algorithm $M$ times)
in $K{=}50$ episodes each with batch size $N$ as the number of state-action pairs 
that have a probabilistic successor distribution.
Plots show results averaged over the $M$ learned strategies.
We chose an error tolerance $\delta=0.1$, a total
of $k {\cdot} \lvert S\rvert$ value iteration steps in the $k$-th episode, and
an exploration of $\epsilon{=}0.1$.

\subsubsection{Models}
For an evaluation, we focus on two models: \racetrack\ and multi-armed bandits.
Results for all other experiments can be found in the appendix.

In \racetrack\ 
~\cite{BarBraSin95,PinZil14,Gro19,BaiDubHerKlaKluKoe20},
an agent controls a vehicle in a two-dimensional grid where 
the task is to reach a goal position from some start position, not colliding with wall tiles.
\Cref{fig:racetrack_ucb_lcb} depicts two example tracks from Barto et al.~\cite{BarBraSin95},
which we identify as ``small track'' (left) and ``big track'' (right).
At each step, the movement in the last step is repeated, possibly modified by 
$1$ tile in either direction, leading to $9$ possible actions in each state.
Environmental noise is modelled by changing the vehicle position by $1$ in each direction with small probability. 
We formulate \racetrack\ as RL problem by assigning goal states with one reward and all
other states with zero reward.
In the case that the vehicle has to cross wall tiles towards the new position, %$p_{i+1}$,
the run ends, not obtaining any reward.
In \racetrack\ experiments, we learn $M=10$ strategies constrained by $N=940$ sample runs.

The second main model is a variant of multi-armed bandits with one initial state having
$100$ actions, each with a biased coin toss uniformly ranging from $0.25$
to $0.75$ probability, gaining one reward and returning to the initial state. 
Here, we learn $M=100$ strategies constrained by $N=101$ sample runs.





\subsection{Sampling methods \ref{rq:sampling}}
We investigate the differences of UCB and LCB sampling 
within \racetrack.

\begin{figure}[t]
 \begin{subfigure}[t]{1\textwidth}
  \centering
  \includegraphics[scale=1.50]{img/RacetrackTiny_UCB.pdf}\qquad
  \includegraphics[scale=1]{img/RacetrackSmall_UCB.pdf}
   \caption{UCB sampling}
 \end{subfigure} \\[.5em]
 \begin{subfigure}[t]{1\textwidth}
  \centering
  \includegraphics[scale=1.50]{img/RacetrackTiny_EpsilonLCB.pdf}\qquad
  \includegraphics[scale=1]{img/RacetrackSmall_EpsilonLCB.pdf}
   \caption{LCB sampling}
 \end{subfigure}
\caption{\racetrack\ exploration visualization of sampling methods\label{fig:racetrack_ucb_lcb}
 (tiles colored by start (yellow), goal (green), wall (dark gray), and visit frequency (red-white))}
\end{figure}

\subsubsection{State-space coverage}
UCB and LCB sampling differ notably in covering the state space while learning.
With UCB sampling, actions with high uncertainty are more likely to be executed,
lowering their upper bound and thus increasing the chance of
other actions with higher uncertainty in the next sample run.
Hence, UCB sampling leads to exploration of many actions and thus to a high 
coverage of the state space.
In contrast, LCB sampling increases confidence in one particular action
shown viable in past samples, leading to sample the same action sequences more often.
Hence, LCB sampling is likely to cover only those states visited by
one successful sampling strategy, showing low coverage of the state space.
This can be also observed in our experiments.
\Cref{fig:racetrack_ucb_lcb} shows the frequency of visiting positions in the
small and big example tracks, ranging from high (red) to low (white) frequencies.
Both tracks already illustrate that UCB sampling
provides higher state-space coverage than LCB sampling.
The small track is symmetric and for each
strategy striving towards a lower path, there is a corresponding equally performing strategy
towards an upper path. UCB sampling treats both directions equally, while
the LCB sampling method in essential learns one successful path and increases its confidence, 
which is further reinforced in the following samples. 
reached by one of the symmetric strategies.
\begin{figure}[t]
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{0.7}{\input{img/LCBvsUCB}}
   \caption{Under all strategies}
 \end{subfigure}
 \,
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{0.7}{\input{img/LCBvsUCB2}}
   \caption{Under pessimistically optimal strategy}
 \end{subfigure}
\caption{Comparison of obtained bounds for strategies\label{fig:tiny_ucb_lcb_graphs}}
\end{figure}

\subsubsection{Robustness}
A further difference of the sampling methods is in dealing with less-explored situations,
where UCB sampling is likely to explore new situations but LCB sampling prefers
actions that increase the likelihood of returning to known states of an already 
learned viable strategy. This is due to those states having
smaller confidence intervals and thus a greater lower bound
on the value and quality functions.
\Cref{fig:racetrack_ucb_lcb} shows this effect in the frequency plot of the big track:
LCB sampling leads to only few isolated positions with high visit frequencies, while
UCB shows a trajectory of visited positions.



\subsubsection{Guaranteed bounds}
The different characteristics of UCB and LCB sampling can also be observed
during the learning process in the small track. 
In \Cref{fig:tiny_ucb_lcb_graphs} on the left we show 
$\underline{V}$ and $\overline{V}$ after each episode. 
Note that these bounds apply to different 
strategies, i.e., the optimistically optimal strategy $\underline{\sigma}$ maximizes $\underline{V}$,
while the pessimistically optimal strategy $\overline{\sigma}$ maximizes $\overline{V}$. 
Here, LCB provides values $\underline{V}$ closer to the optimum and, 
due to its exploitation strategy, gains more confidence in its learned strategy.
However, unlike UCB sampling, it cannot improve on $\overline{V}$ significantly, 
since parts of the environment remain mostly unexplored.
We plot bounds under the single fixed strategy $\underline{\sigma}$ on the right. 
After 50 episodes, UCB then can provide value function bounds $[0.29,0.75]$,
while LCB provides $[0.33,0.53]$, being more close to the optimal
value of $0.49$.

LCB is also favourable under limited sampling access, e.g., in
(mostly) symmetric environments as the small track: UCB explores the symmetry and 
requires at least double samples for achieving a similar confidence on the learned strategy.

\hframe{
Concerning \ref{rq:sampling}, we showed that LCB sampling can provide better strategies with
high confidence than UCB sampling, while UCB sampling shows better bounds 
when ranging over all strategies.
}

\subsection{Impact of scoping \ref{rq:scoping}}
We now investigate the impact of action scoping and its parameter $h$
on the multi-armed bandit experiment. 
    
    \begin{figure}[t]
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{.8}{\input{img/UCB_num_habits}}
 \end{subfigure} \,
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{.8}{\input{img/LCB_num_habits}}
 \end{subfigure}
\caption{Action-space reduction by action scoping (UCB left, LCB right)\label{fig:num_habits_ucb_lcb}}
\end{figure}



\subsubsection{Action-space reduction}
\Cref{fig:num_habits_ucb_lcb} shows the number of state-action pairs in the IMDP
after each episode w.r.t. UCB and LCB sampling. Here, eager and conservative action 
scoping is considered with various scoping parameters $h$.
As expected, more actions are removed for greater $h$.
Since $\dot{Q}(s,a) \leqslant \overline{Q}_h(s,a)$, eager scoping leads to
more actions being removed than conservative scoping (cf. eager plots in the lower part 
of the figures). Observe that the choice of eager or 
conservative scoping has more impact than the choice of $h$.
In terms of the sampling method we observe that for conservative scoping with UCB sampling more actions 
are removed from scopes than with LCB sampling. A possible explanation is that 
in LCB sampling, suboptimal actions do not acquire enough samples to 
sufficiently reduce the upper bound of their expected reward.

\begin{figure}[t]
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{.8}{\input{img/appendix/Bandit25-75UCB_bounds}}
 \end{subfigure} \,
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{.8}{\input{img/appendix/Bandit25-75LCB_bounds}}
 \end{subfigure}
\caption{Bounds of the subsystem obtained by scoping (UCB left, LCB right)\label{fig:habit_bounds_Bandit25-75}}
\end{figure}

\begin{figure}[t]
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{.8}{\input{img/appendix/Bandit25-75UCB_corr_bounds}}
 \end{subfigure} \,
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{.8}{\input{img/appendix/Bandit25-75LCB_corr_bounds}}
 \end{subfigure}
\caption{Bounds for pessimistically optimal strategy (UCB left, LCB right)\label{fig:habit_corr_bounds_Bandit25-75}}
\end{figure}

\subsubsection{Strategy bounds}
Next, we investigate the bounds obtained by the strategies returned by \Cref{alg:imdp-rl}. 
For brevity, we focus here on the cases $h{=}\delta/\Nt$ and $h{=}0.05$. 
Our results are plotted in \Cref{fig:habit_bounds_Bandit25-75} and \Cref{fig:habit_corr_bounds_Bandit25-75}
for $\underline{V}$ and $\overline{V}$ on the subsystem obtained by applying action scopes with 
both $\underline{\sigma}$ and $\overline{\sigma}$ and solely $\underline{\sigma}$, respectively.
For UCB sampling, we observe that bounds tighten faster the 
more actions are removed from scopes and reduce the system size,
i.e., particularly for eager scoping and for $h{=}0.05$.
For LCB, scopes do not have such a drastic influence, since actions are only leaving the scope 
if there is an alternative action with high $\underline{V}$, in which case the latter 
action is sampled mostly anyway. 


\begin{figure}[t]
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{.8}{\input{img/appendix/Bandit25-75UCB_real}}
 \end{subfigure} \,
 \begin{subfigure}[t]{.5\textwidth}
  \centering\scalebox{.8}{\input{img/appendix/Bandit25-75LCB_real}}
 \end{subfigure}
\caption{Expected total reward w.r.t. sampling strategy $\sigma$ (UCB left, LCB right)\label{fig:real_strategy_Bandit25-75}}
\end{figure}
\subsubsection{Sampling strategy quality}
In \Cref{fig:real_strategy_Bandit25-75} we plot the expected total 
reward of the employed sampling strategy $\sigma$ in \Cref{alg:imdp-rl} after each episode. 
Eager scoping tremendously improves the quality of the sampling strategy for both
UCB and LCB sampling.
For the UCB strategy we observe an initial monotonic increase of the online performance
that that eventually drops off. This is because a lot of actions cannot
improve on the trivial upper bound of 1 until a lot of samples are acquired.
In the first roughly 20 episodes the number of actions with trivial upper bound decreases
as more samples are collected. Additionally, the actions with upper bound less than 1
are those which were less successful in the past, i.e., likely suboptimal actions, explaining the monotonic increase.
Afterwards, the fluctuations within UCB sampling stem from the fact that after reducing all upper bounds below 1,
UCB sampling may result in a fully deterministic sampling strategy that only updates after each episode.
Hence, exploring a particular action takes the full $N$ runs of an episode, even if the action is suboptimal
and only has a high upper bound due to a lack of samples.
Especially here, scoping helps to eliminate such actions and avoids sampling them for
a full episode just to confirm the action was indeed suboptimal.
For LCB sampling the better performance with scoping is due to suboptimal
actions being removed and thus not eligible in the exploration step with probability $\epsilon$.



\subsubsection{Subsystem bounds}
With the introduction of scopes, our RL algorithm is not guaranteed to converge to 
optimal values. To determine whether optimal actions are removed from scopes in practice, 
we compute the optimal strategy within the subsystem 
generated by only considering actions within the computed scope. 
Note that for the transition function we are using the exact probabilities 
as in the environment MDP. The results are given in \Cref{tbl:subsystem_bounds_bandit}. 
Without scope reduction the subsystem is just the entire environment. 
When introducing scopes, we did not remove the optimal action via conservative scoping a single time
with either sampling method, even for $h=0.05$.
Only with eager scoping we saw the optimal action being removed from the scope, but the optimal strategy
in the scoped subsystem still performs reasonably well compared to the overall optimal strategy. 
The fact we observe this only with eager scoping is
not surprising, as removing
more actions from scopes (recall \Cref{fig:num_habits_ucb_lcb})
of course increases the chance of removing the optimal action in a state.

\begin{table}[t]
\caption{Values of the optimal strategy in the multi-armed bandit model\label{tbl:subsystem_bounds_bandit}}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|c|clcl||clcl}
\toprule
                                                            & no             & \multicolumn{2}{c|}{UCB-cons.}                                 & \multicolumn{2}{c||}{UCB-eager}                                    & \multicolumn{2}{c|}{LCB-cons.}                                 & \multicolumn{2}{c}{LCB-eager}                                    \\
                                                            & \multicolumn{1}{l|}{scoping} & \multicolumn{1}{l|}{$h{=}\delta/\Nt$} & \multicolumn{1}{l|}{$h{=}0.05$} & \multicolumn{1}{l|}{$h{=}\delta/\Nt$} & $h{=}0.05$                  & \multicolumn{1}{l|}{$h{=}\delta/\Nt$} & \multicolumn{1}{l|}{$h{=}0.05$} & \multicolumn{1}{l|}{$h{=}\delta/\Nt$} & $h{=}0.05$                 \\ \midrule
value & 0.75                  & \multicolumn{1}{c|}{0.75}         & \multicolumn{1}{c|}{0.75}     & \multicolumn{1}{c|}{0.75}         & \multicolumn{1}{c||}{0.742} & \multicolumn{1}{c|}{0.75}         & \multicolumn{1}{c|}{0.75}     & \multicolumn{1}{c|}{0.746}         & \multicolumn{1}{c}{0.739}
\\ \bottomrule
\end{tabular}
}
\end{table}
\hframe{
For \ref{rq:scoping}, we conclude that for both UCB and LCB sampling, scoping and
especially eager scoping significantly improves the quality of learned strategies
after few samples, while only slightly deviating from the optimal strategy.
In the UCB setting, scoping leads to further exploitation and thus better bounds.
}


\subsection{Further examples}
We ran our algorithms on several other environment MDPs from the RL and formal-methods communities. 
The bounds of the expected reward obtained for the reduced subsystem, as well as for the strategy
maximizing the lower bound, are given in the appendix (cf. \Cref{tbl:experiments}).
In general, the strategy learned from LCB yields equal or higher lower bounds and tighter bounds for single 
strategies, while UCB sampling gives tighter bounds for the entire system. Employing action scoping generally
tightens the bounds further with the eager scoping emphasizing this effect.
The margins of the differences vary between the examples. In general, both 
LCB sampling and scoping have biggest impact on large action spaces
and on models with high probability deviations 
such as with small error probabilities.
On the flipside, we observed that LCB performs poorly when mostly or fully deterministic actions or even runs
are available, as those incur little uncertainty and thus tend to have relatively large lower bounds even with few samples.
