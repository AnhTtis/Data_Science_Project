% !TEX root =  ../main.tex

\section{Appendix}
In this appendix, we provide content that had to be omitted from the main paper
due to the lack of space.

\subsection{Environment approximation}
While the approximation approach by Ashok et al. \cite{AshKreWei19} does 
not explicitly operate on IMDPs, it can be expressed in our setting by
\[
	\hat{T}(s,a,s')\quad =\quad \left[\max\big(p_{min},\frac{\#(s,a,s')}{\#(s,a)}-c'\big),1\right]
\]
where $c'=\sqrt{\frac{\log\delta_T}{-2\#(s,a)}}$.
The latter is the confidence interval obtained
by applying the Hoeffding inequality \cite{Hoeffding} that
guarantees $(1{-}\delta_T)$-correctness for each transition triple
$(s,a,s') \in S {\times} A {\times} S$ when viewing one sample of action 
$a$ in state $s$ as a Bernoulli trial.
Their method takes only the lower bound of the transition probability
into account, which we extend to also consider the upper bound.

We now prove the correctness of our definition of 

\begin{lemma}\label{lemma:approximation}
Let $\cM=(S,A,\imath,G,R,T)$ be an MDP,  and $\eta \in ]0,1[$.
Assume that for a fixed state $s\in S$ an action $a\in \Act(s)$ has been sampled $\#(s,a) \in \Nat_{>0}$ times and
successor $s'$ was observed $\#(s,a,s')$ times. Then, with probability $1-\eta$ it holds that
$$ T(s,a,s') \in \left[\frac{\#(s,a,s')}{\#(s,a)}-c,\frac{\#(s,a,s')}{\#(s,a)}+c\right]$$
where $c=\sqrt{\frac{\log\eta/2}{-2\#(s,a)}}$
\end{lemma}

\paragraph{Proof.}

We can directly bound the probability of $T(s,a,s')$ being outside the interval for a given $s,a,s'$
by applying the two-sided Hoeffding bound, proving the Lemma.

\begin{align*}
P \left( \left| T(s,a,s') - \frac{\#(s,a,s')}{\#(s,a)} \right|  \geqslant c \right) & \leqslant 2 \exp \left( -2\#(s,a) c^2 \right) \\
& = 2 \exp \left( -2\#(s,a) \sqrt{\frac{\log\eta/2}{-2\#(s,a)}}^2 \right) \\
& = \eta
\end{align*}
\qed

Using this result we can also easily show that $\cM\in\cU=(S,A,\imath,G,R,T_\eta)$
where $\eta = \delta / \Nt$. As $\cM$ and $\cU$ by definition have the same graph structure
and rewards, we only need to check that $T(s,a,s')\in T_\eta(s,a,s')$ for all $s\in S, a\in\Act(s)$ and
$s'\in\Post(s,a)$.

First, note that for a non-probabilistic transition (i.e., $s,a$ with $\Post(s,a)=\{s'\}$)
the statement also holds trivially as $T(s,a,s')=\frac{\#(s,a,s')}{\#(s,a)}\in T_\eta(s,a,s')$.

Using \Cref{lemma:approximation} the probability that $T(s,a,s')$ is outside the interval 
for any of the $\Nt$ probabilistic transitions can thus be bounded by $\eta \Nt = \delta$. 

\subsection{Building minimizing and maximizing instantiations}

\begin{algorithm}[t]
\SetAlgoLined
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{IMDP $\cU=(S,A,\imath,G,R,\hat{T})$, value function estimate $\hat{V}$}
\Output{value minimizing transition function $T^{-}$}
\BlankLine
\ForAll{$s\in S$}{
	\ForAll{$a\in\Act(s)$}{
		\lForAll{$s'\in\Post(s,a)$}{$T^{-}(s,a,s') := \underline{T}(s,a,s')$}
		\While{$\sum_{s'\in\Post(s,a)} T^{-}(s,a,s') < 1$}{
			\If{$1-\sum_{u\in\Post(s,a)} T^{-}(s,a,u) > \overline{T}(s,a,s')-\underline{T}(s,a,s')$}{
				$T^{-}(s,a,s') := \underline{T}(s,a,s')$
			}
			\Else{
				$T^{-}(s,a,s') := 1-\sum_{u\in\Post(s,a),u\neq s'} T^{-}(s,a,u) $
			}
		}
	}
}
\Return{$T^{-}$}
\caption{MINIMIZING\_TRANSITIONS($\cU, \hat{V}$) \label{alg:minimizing_transitions}}
\end{algorithm}

A core idea in the adapted value iteration algorithm for IMDPs is to find an instantiation
that minimizes or maximizes the value in each state with respect to a previously computed
value function \cite{Givan00,Wu08}.

Formally, we can state the problem as follows: Given an IMDP $\cU=(S,A,\imath,G,R,\hat{T})$ with 
value function estimate $\hat{V} : S \rightarrow \Intv(S)$, choose for each state $s$
and action $a\in \Act(s)$ a transition instantiation $t_s^a\in T_s^a$ such that
$\hat{Q}(s,a)=\sum_{s'\in\Post(s,a)}\hat{V}(s') t_s^a$ is minimized (resp. maximized).

As the instantiation of $t_s^a$ only impacts any $\hat{Q}(s,a)$, and $\hat{Q}$ for no other
parameters,  this can be optimized locally in a straight forward way.  We exemplify this for
the minimization case in \Cref{alg:minimizing_transitions}. 
The maximization case is analogous. First, in an effort to ensure that 
$t_s^a\in T_s^a$, we shift the minimum amount of probability mass into each successor state,
i.e., we initialize the value of $t_s^a(s')$ to $\underline{T}(s,a,s')$ for each $s'\in\Post(s,a)$.
We then distribute the remaining probability mass into the state $s^{*}\in\Post(s,a)$ for which
$\hat{V}(s^{*})$ is minimal (resp. maximal) until either the total probability mass of $t_s^a$
reaches $1$, or until $t_s^a(s^{*})$ reaches $\overline{T}(s,a,s')$.
If the former is the case, we are done. Otherwise, we repeat the procedure with the state
out of the other states for which $\hat{V}$ is minimal (resp. maximal).

As we initialized all transition probabilities with $\underline{T}(s,a,s')$ and only increase them,
but to no more than $\overline{T}(s,a,s')$, and as we stop when $t_s^a$ reaches a probability
mass of $1$ we guarantee that $t_s^a\in T_s^a$.  The result also clearly minimizes $\hat{Q}(s,a)$.

\subsection{Value iteration for IMDPs}

\begin{algorithm}[t]
\SetAlgoLined
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{IMDP $\cU=(S,A,\imath,G,R,\hat{T})$, parameter $k$}
\Output{Lower and upper bound value functions $\underline{V}$ and $\overline{V}$}
\BlankLine
$\underline{V}, \overline{V} = \text{INITIALIZE\_BOUNDS}(\cU)$ \;\label{line:init}
\RepTimes{$k\cdot\lvert S \rvert$}{
	$T^{-}:=$ MINIMIZING\_TRANSITIONS$(\cU,\underline{V})$\;
	$T^{+}:=$ MAXIMIZING\_TRANSITIONS$(\cU,\overline{V})$\;
	\ForAll{$s\in S$}{
		$\underline{V}'(s) := R(s) + \max_{a\in \Act(s)} \sum_{s' \in S} \underline{V}(s')\cdot T^{-}(s,a,s')$\;\label{line:vu}
		$\overline{V}'(s) := R(s) + \max_{a\in \Act(s)} \sum_{s' \in S} \overline{V}(s')\cdot T^{+}(s,a,s')$\;
	}
	$\underline{V} := \underline{V}'$\;
	$\overline{V} := \underline{V}'$\;
}
\Return{$\underline{V}',\overline{V}'$}
\caption{COMPUTE\_BOUNDS($\cU, k$) \label{alg:uncertain_vi}}
\end{algorithm}

We briefly recall the IMDP value-iteration algorithm outlined in \Cref{alg:uncertain_vi}
first developed by Givan et al.  \cite{Givan00} for discounted rewards, and later applied by
Wu et al. \cite{Wu08} for positive rewards.\\
An important observation for that is that for any IMDP $\cU$ there is an instantiation that is 
minimizing (or maximizing) the value function for all states simultaneously.
The main idea behind the algorithm is then to guess a total order $\leq_V$ over all states $s \in S$
and assume that $s \leq_V s' \iff V(s) \leqslant V(s')$. Given this assumption one can easily perform a
value iteration step by first computing the minimizing transition instantiation for a state $s$ and an action $a$:
Transitions are initially instantiated with the lower bound of $\hat{T}(s,a,s')$ for each
successor state $s'\in \Post(s,a)$ and then probability mass is added to the transitions to the successors 
which are minimal w.r.t. $\leq_V$ until the sum of the instantiated probabilities reaches $1$. 
The computation for the maximizing MDP is analogous.
Following that, the order $\leq_V$ is redefined according the value function computed in the VI step. 

In total, we perform $k\cdot\lvert S \rvert$ such value iteration steps where $k$ gives the number of episodes that
have passed. \footnote{Technically any function $f\colon \Nat \rightarrow \Nat$ with the restriction that
$\lim_{k\rightarrow\infty} f(k)=\infty$ could be used here, but we choose $k\cdot\lvert S \rvert$ as linear growth
in $k$ avoids unnecessarily many iterations and scaling by $\lvert S \rvert$ adapts the algorithm to the 
environment size}

Recall that there is no stopping criterion with formal guarantees.
Towards proving that the algorithm can be seen as an any-time algorithm, we first show the following Lemma:

\begin{lemma}\label{lemma:vi}
Let $\underline{V}^{*}$ and $\overline{V}^{*}$ be the solution of the 
interval Bellman equations of an IMDP $\cU$, respectively, and
$([\underline{V}_i(s),\overline{V}_i(s)])_{t\in\Nat}$ be a sequence of interval value functions s.t.
$[\underline{V}_0(s),\overline{V}_0(s)] \supseteq [\underline{V}^{*}(s),\overline{V}^{*}(s)]$ for all $s\in S$ 
and $\dot{V}_{i+1}(s)$ where $\dot{V}\in\{ \underline{V},\overline{V} \}$ is computed
according to \Cref{alg:uncertain_vi}, i.e.,
\begin{align*}
\underline{V}_{i+1}(s) & = R(s) + \max_{a\in \Act(s)} \sum_{s' \in S} \underline{V}_i(s')\cdot T^{-}(s,a,s') \\
\overline{V}_{i+1}(s) & = R(s) + \max_{a\in \Act(s)} \sum_{s' \in S} \overline{V}_i(s')\cdot T^{+}(s,a,s')
\end{align*} 
Then $[\underline{V}_i(s),\overline{V}_i(s)] \supseteq [\underline{V}^{*}(s),\overline{V}^{*}(s)] $ for all $i\in\Nat, s\in S$.
\end{lemma}

\paragraph{Proof.}
We first show the statement for the lower bound, i.e., that $\underline{V}_i(s) \leqslant \underline{V}^{*}(s)$.

By induction.  The base case $\underline{V}_0$ holds by assumption.

For the induction step, we assume $\underline{V}_i$ satisfies the condition, i.e.,
$\underline{V}_i(s) \leqslant \underline{V}^{*}(s)$ for all $s\in S$.

First, notice that by definition of MINIMIZING\_TRANSITIONS, we have
$T^{-}(s,a) = \argmin_{t_s^a \in T_s^a} \sum_{s' \in S} \underline{V}(s')\cdot t_s^a(s')$.
This is the case since the function shifts as much probability mass as possible
into states with a low value function in the previous step. 

Then,by induction hypothesis and because $t^a_s(s')\geqslant 0$ the following chain holds:

\begin{align*}
\underline{V}^{*}(s) & = \max_{a\in \Act(s)} \min_{t_s^a \in T_s^a} \sum_{s' \in S} \underline{V}^{*}(s')\cdot t_s^a(s') \\
& \geqslant  \max_{a\in \Act(s)} \min_{t_s^a \in T_s^a} \sum_{s' \in S} \underline{V}_i(s')\cdot t_s^a(s') \\
& = \underline{V}_{i+1}(s)
\end{align*}

The analogous case for upper bounds can be proven in the same way, i.e.,
for any $i\in \mathbb{N}$ and $s\in S$
we can guarantee that $\overline{V}_i(s)\geq\overline{V}^{*}(s)$.

From the inequalities for both bounds we can deduce that indeed 
$[\underline{V}_i(s),\overline{V}_i(s)] \supseteq [\underline{V}^{*}(s),\overline{V}^{*}(s)]$
for all $t\in \mathbb{N}$.\qed

\paragraph{}\noindent
Note that this requires a safe upper and lower bound,  $\underline{V}_0(s)$ and $\overline{V}_0(s)$, 
of the value function in each state $s \in S$.
For reachability tasks the trivial bounds $0$ and $1$ suffices while 
for general reward structures the topology of the environment along with $p_{min}$ can 
be used to compute such bounds \cite{BeiKleLeu17}.

The sequences $(\underline{V}_i)_{i\in \Nat}$ and $(\overline{V}_i)_{i\in \Nat}$ are not necessarily monotonic.
However, one could adapt the definition to 
$$ \underline{V}_{i+1}(s) = \max \{ \underline{V}_i(s), \underline{W}_i \}$$
with
$$\underline{W}_i(s) = R(s) + \max_{a\in \Act(s)} \sum_{s' \in S} \underline{V}_i(s')\cdot T^{-}(s,a,s') $$
and analogously (using the minimum between $\overline{W}$ and $\overline{V}$) for the upper bound, to enforce monotonicity. 
Under this definition \Cref{lemma:vi} still holds, as the proof already handles the case
$\underline{W}_i(s) \geqslant \underline{V}_i(s)$, and the case $\underline{W}_i(s) < \underline{V}_i(s)$
can be proven directly from the induction hypothesis as it implies $\underline{V}_{i+1}(s) = \underline{V}_i(s)$


\begingroup
\def\thetheorem{\ref{thm:guarantees}}
\begin{theorem}
Let $V^{*}$ be the solution to the Bellman equations of a given MDP $\cM$. 
Then for all $\delta\in\ ]0,1[$ and $K,N\in\Nat$
the value function bounds $\underline{V}$ and $\overline{V}$ returned by 
\textup{IMDP\_RL($\cM,\delta,K,N$)} as of \Cref{alg:imdp-rl} contain $V^{*}$ 
with probability at least $1-\delta$, 
i.e., $\Prob\left(\underline{V}(s) \leqslant V^{*}(s) \leqslant \overline{V}(s)\right)\geqslant 1-\delta$
for all $s\in S$. 
\end{theorem}
\addtocounter{theorem}{-1}
\endgroup

\paragraph{Proof.}
By \Cref{lemma:approximation} we have $\cM\in [\cU]$ with probability at least $1-\delta$.
Assume $\cM\in[\cU]$ and let $\underline{V}^{*}$ and $\overline{V}^{*}$ be the solutions to
the interval Bellman equations. Then, clearly $\underline{V}^{*}(s) \leqslant V^{*}(s) \leqslant \overline{V}^{*}(s)$
for all $s\in S$ and further by \Cref{lemma:vi} $\underline{V}_i(s) \leqslant V^{*}(s) \leqslant \overline{V}_i(s)$
for all $t\in\Nat$ where $\underline{V}_i(s)$ and $\overline{V}_i(s)$ denote the computed 
lower and upper bounds of value function in the $t$-th iteration step.  Note that \Cref{lemma:vi}
is applicable since we pick safe initial bounds in \Cref{line:init} in \Cref{alg:uncertain_vi}.

Hence, $\Prob\left(\underline{V}(s) \leqslant V^{*}(s) \leqslant \overline{V}(s)\right)\geqslant 1-\delta$
for all $s\in S$. \qed

\subsection{LCB sampling}

\begin{algorithm}[t]
	\SetAlgoLined
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{An IMDP $\cU=(S,A,\imath,G,R,\hat{T})$ with lower bounds on its value function $\underline{V}$, exploration parameter $\epsilon$}
	\Output{a strategy $\sigma$}
	\BlankLine
	
	$\leq_V := \{ (s,s') \mid V(s) \leqslant V(s') \}$\;
	\ForAll{$s\in S$}{
	\ForAll{$a\in \Act(s)$}{
		$T^{-} :=$ MINIMIZING\_TRANSITIONS$(\cU,\leq_V)$\;
		$\underline{Q}(s,a) := R(s) + \sum_{s'\in S} \underline{V}(s)\cdot T^{-}(s,a,s')$\;
		$\sigma(s,a) := \epsilon / \lvert \Act(s) \rvert$
	}
	$a^{*} := \argmax_{a\in \Act(s)} Q(s,a)$\;
	$\sigma(s,a^{*}) :=  \sigma(s,a^{*}) + (1-\epsilon)$
	}

	\Return $\sigma$
	\caption{UPDATE\_LCB\_STRATEGY($\cU, \underline{V},\epsilon)$\label{alg:policy}}
\end{algorithm}

After computing the bounds of the value function for a given dataset over the unknown MDP (cf. \Cref{alg:uncertain_vi}),
we can utilize the obtained bounds to define the strategy to be used to sample the MDP in further episodes.
We outline this procedure in \Cref{alg:policy}.  Similar to the idea behind value iteration on IMDPs,
we first define a state ordering according to the given lower bounds of the value function.
We then also compute the instantiation that minimizes the value function in all states (cf. \cref{alg:minimizing_transitions}).
This yields the lower bound of the expected reward for each state-action pair.
While each action $a\in\Act(s)$ is allocated a probability of $\epsilon/\Act(s)$, 
the action $a^{*}$ with the highest lower bound is allocated the the remaining $(1-\epsilon)$ probability mass
of the strategy is state $s$.

As described in the main part of the paper, the $\epsilon$-sampling serves as an exploration mechanism
while at the same time guaranteeing the algorithm converges in the long run.

\begingroup
\def\thetheorem{\ref{thm:pac}}
\begin{theorem}
\Cref{alg:imdp-rl} with LCB sampling converges towards an optimal solution,
i.e., for $K\rightarrow\infty$ both $\underline{V}$ and $\overline{V}$ 
converge pointwise towards $V^{*}$, and their corresponding strategies
$\underline{\sigma}$ and $\overline{\sigma}$ converge towards optimal strategies.
\end{theorem}
\addtocounter{theorem}{-1}
\endgroup

\paragraph{Proof.}
Let $\cM$ be an MDP and $\cU$ the IMDP computed according to \Cref{alg:update}, calling \Cref{alg:uncertain_vi}.  
We first show the claim that as $K\rightarrow\infty$, the computed bound of the value function
$\underline{V}$ and $\overline{V}$ converge towards the correct solution to the interval Bellman equations, i.e.,
for all $s\in S$:

\begin{align*}
\underline{V}(s) & = \min_{\cM\in [\cU]} V_\cM(s) \\
\overline{V}(s) & = \max_{\cM\in [\cU]} V_\cM(s)
\end{align*}

As that the main loop, i.e., the value iteration step, is executed $k\cdot \lvert S \rvert$ times in the $k$-th episode,
and the value iteration step is a point-wise contraction mapping
(cf.  \cite[Lemma 5]{Wu08}), we can apply Banach fixed-point theorem to 
show that the value iteration converges as $k\rightarrow\infty$. 

Further, it is known that there are globally reward-minimizing (resp. reward maximizing) MDP $\cM^{-}$ and $\cM^{+}$, i.e.,

\begin{align*}
\underline{V}(s) & = V_{\cM^{-}}(s) \\
\overline{V}(s) & = V_{\cM^{+}}(s)
\end{align*}

for all $s\in S$ \cite{Givan00,Wu08}. As both $\cM^{-}$ and $\cM^{+}$ have the same graph structure as $\cM$,
they are contracting as well and thus there is a unique fixed point of the Bellman equations \cite{BerTsi91}.
Hence, the value iteration converges the the (unique) solution to the Bellman equations.

Next, we show that $\underline{V}$ and $\overline{V}$ converge towards $V^{*}$ under LCB sampling.

It is well known that, as long as each run has the chance to visit any state,
an $\epsilon$-greedy sampling strategy is guaranteed to visit each state infinitely 
often. More precisely, the probability of reaching any state $s$ in $L \geqslant \lvert S \rvert$ steps
can be bounded from below by a constant $(p_{min} \frac{\epsilon}{\lvert A \rvert})^{\lvert S \rvert}>0$. 
Thus, the expected number of times $s$ is visited as well as the number of times any action
$a\in\Act(s)$ is sampled in $S$ go towards infinity.
Hence, the confidence interval size approaches 0, i.e., $\lim_{t\rightarrow\infty}c(s,a) = 0$. 
Further, by the law of large numbers, 
$\lim_{t\rightarrow\infty}\frac{\#(s,a,s')}{\#(s,a)} = T(s,a,s')$ and thus
$\lim_{t\rightarrow\infty}\hat{T}(s,a,s') = [T(s,a,s'), T(s,a,s')]$.

In particular, this implies that
\[
	\lim_{t\rightarrow\infty}\sum_{s\in S}\underline{T}(s,a,s')\quad = \quad
\lim_{t\rightarrow\infty}\sum_{s\in S}\overline{T}(s,a,s') \quad =\quad 1
\] and thus the
distributions $T^{-}$ and $T^{+}$ in COMPUTE\_BOUNDS($\cU)$,
converge pointwise towards the true distribution $T$ which means the solution the 
interval Bellman equations the algorithm is solving coincide with the true Bellman equations 
of the environment, and thus $\underline{V}$ and $\overline{V}$ converge
pointwise towards the solution of the Bellman equations $V^{*}$ of the MDP $\cM$.

Lastly, as in the long run $\underline{V} = \overline{V} = V^{*}$, and
$\underline{\sigma}$ and $\overline{\sigma}$ maximize $\underline{V}$ and $\overline{V}$,
respectively, clearly both also maximize $V^{*}$ and thus converge towards optimal strategies.
\qed

\paragraph{}\noindent
Such random exploration schemes are also called \emph{dithering}. One well known disadvantage of such approaches
is that this unguided exploration does not differentiate between suboptimal actions \cite{SutBar18}.

Another viable sampling strategy would be to use the softmax action selection where, for each 
state-action pair $(s,a)$ we define a \emph{temperature} parameter $\tau_{s,a}$ 
and compute the strategy as

$$\sigma(s)(a) = \frac{e^{\underline{Q}(s,a)/\tau_{s,a}}}{\sum_{a'\in\Act(s)}e^{\underline{Q}(s,a')/\tau_{s,a}}}$$

Specifically, this allows of to perform exploration probabilistically, but unlike 
$\epsilon$-learning introduces a weight to distinguish between non-optimal actions.
One problem that arises however is that finding good parameters $\tau_{s,a}$ 
requires some degree of knowledge about possible action quality values.
In a resource constrained setting there is most likely not enough time to find 
suitable parameters, thus we focus on $\epsilon$-learning in the main part of the paper.
If however good estimates of action quality are present a priori, one might want
to consider other exploration schemes, such as the softmax sampling strategy.

\subsection{Runtime}

\begin{table}[]\caption{Model sizes and runtimes\label{tbl:runtimes}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|ccc|cc}
               & \multicolumn{1}{l|}{}                 & \multicolumn{3}{c|}{model size}                 & \multicolumn{2}{c}{runtime {[}s{]}} \\
model          & \multicolumn{1}{l|}{optimal solution} & \multicolumn{1}{c|}{states} & \multicolumn{1}{l|}{state-action pairs} & transitions & \multicolumn{1}{c|}{no scoping} & \multicolumn{1}{c}{scoping} \\ \hline
Bandit      & 0.99                                  & \multicolumn{1}{c|}{3}    & \multicolumn{1}{c|}{99}                & 198         & \multicolumn{1}{c|}{0.17}     & \multicolumn{1}{c}{0.25}           \\
Bandit25-75      & 0.75                                  & \multicolumn{1}{c|}{3}    & \multicolumn{1}{c|}{101}                & 202         & \multicolumn{1}{c|}{0.17}     & \multicolumn{1}{c}{0.27}           \\
BanditGauss      & 0.7                                  & \multicolumn{1}{c|}{3}    & \multicolumn{1}{c|}{100}                & 200         & \multicolumn{1}{c|}{0.15}     & \multicolumn{1}{c}{0.24}           \\
BettingFav      & 17.5                                  & \multicolumn{1}{c|}{235}    & \multicolumn{1}{c|}{771}                & 1542         & \multicolumn{1}{c|}{2.2}     & \multicolumn{1}{c}{4.2}           \\
BettingUnfav      & 10                                  & \multicolumn{1}{c|}{235}    & \multicolumn{1}{c|}{771}                & 1542         & \multicolumn{1}{c|}{2.2}     & \multicolumn{1}{c}{3.2}           \\
consensus      & 0.11                                  & \multicolumn{1}{c|}{272}    & \multicolumn{1}{c|}{400}                & 492         & \multicolumn{1}{c|}{23}     & \multicolumn{1}{c}{34}           \\
csma           & 0.50                                  & \multicolumn{1}{c|}{1038}   & \multicolumn{1}{c|}{1054}               & 1282        & \multicolumn{1}{c|}{27}     & \multicolumn{1}{c}{30}             \\
Gridworld      & -5.5                                  & \multicolumn{1}{c|}{20}    & \multicolumn{1}{c|}{48}                & 132         & \multicolumn{1}{c|}{3.1}     & \multicolumn{1}{c}{5.2}           \\
pacman         & 0.45                                  & \multicolumn{1}{c|}{6854}   & \multicolumn{1}{c|}{8484}               & 8889        & \multicolumn{1}{c|}{7.6}     & \multicolumn{1}{c}{10}            \\
RacetrackSmall  & 0.49                                  & \multicolumn{1}{c|}{158}    & \multicolumn{1}{c|}{1377}               & 4608        & \multicolumn{1}{c|}{13}     & \multicolumn{1}{c}{19}          \\
RacetrackBig & 0.89                                  & \multicolumn{1}{c|}{12913}  & \multicolumn{1}{c|}{101701}             & 568334     & \multicolumn{1}{c|}{1874}     & \multicolumn{1}{c}{3326}           \\
SnL100         & -17.41                                & \multicolumn{1}{c|}{80}     & \multicolumn{1}{c|}{480}                & 1894        & \multicolumn{1}{c|}{99}  & \multicolumn{1}{c}{99}              \\
wlan\_cost      & -220                                  & \multicolumn{1}{c|}{2954}   & \multicolumn{1}{c|}{3972}               & 5202        & \multicolumn{1}{c|}{134}     & \multicolumn{1}{c}{22}            
\end{tabular}
}
\end{table}

In the main evaluation section we abstracted time constraints by
limiting each learning approach to 50 episodes with a fixed number of runs each.
In \Cref{tbl:runtimes} we give the real-time runtime for each algorithm
as well as the expected reward of the optimal strategy and model sizes.

The runtimes are given for the LCB samploing method, for both without scoping,
and with a scoping parameter $h=0.05$ and eager scoping.
Introducing scoping increases the runtime of up to a factor of 2, as it is
necessary to perform an additional value iteration to compute $\underline{V}_h$
and $\overline{V}_h$ as opposed to only $\underline{V}$ and $\overline{V}$.
We found that neither the sampling method nor the scoping method
(i.e., eager or conservative, and choice of $h$) have a measurable impact on the runtime.
The overhead from checking the scoping criterion is negligible.
In most examples,the reduction of the model does not yield a significantly faster value iteration
since its runtime is mostly dependant on the state-space whereas scopes
mostly only reduce the action space. However, in cases where a lot of actions
are removed from scopes, and certain states become unreachable, we do indeed
observe a significant speedup. This is most emphasized in the
\textrm{wlan\_cost} example.

In the table we can clearly see a correlation between model size and runtime.
The most contributing factors here are the state-space and the topological
complexity of the model, i.e., the number of transitions, especially in the
presence of loops which causes the value iteration to require more iterations.
Even though models with the reachability objective (those with optimal solution $\in [0,1])$)
can initialize the value interval to $[0,1]$ in each state
while models with cost-minimizing objectives (those with negative optimal solutions)
have to start with more conservative bounds, there is no clear correlation between
the reward structure and runtime.

\section{Further examples}
\begin{table}[t]

\caption{Optimistic and pessimistic strategy bounds for further examples\label{tbl:experiments}}
\resizebox{\columnwidth}{!}{%
\rotatebox{90}{
\begin{tabular}{lll|ccc|ccc}
\toprule
                           &              &                 & \multicolumn{3}{c|}{UCB}                             & \multicolumn{3}{c}{LCB}                              \\
\multicolumn{1}{l|}{model} & \multicolumn{1}{l|}{solution}  &   \multicolumn{1}{l|}{strategy}   & no scoping & conservative & eager & no scoping & conservative & eager \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{Bandit}} & \multicolumn{1}{l|}{\multirow{2}{*}{0.99}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [0.71, 0.98] & [0.75, 0.98] & [0.88, 0.98] & [0.01, 0.99] & [0.04, 0.99] & [0.88, 0.98]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [0.84, 0.98] & [0.86, 0.98] & [0.92, 0.98] & [0.87, 0.92] & [0.84, 0.89] & [0.92, 0.97] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{Bandit25-75}} & \multicolumn{1}{l|}{\multirow{2}{*}{0.75}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [0.28, 0.96] & [0.31, 0.95] & [0.71, 0.78] & [0.01, 0.99] & [0.01, 0.99] & [0.61, 0.90]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [0.63, 0.92] & [0.64, 0.91] & [0.71, 0.78] & [0.61, 0.67] & [0.65, 0.72] & [0.69, 0.75] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{BanditGauss}} & \multicolumn{1}{l|}{\multirow{2}{*}{0.7}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [0.21, 0.94] & [0.18, 0.93] & [0.66, 0.72] & [0.01, 0.99] & [0.01, 0.99] & [0.48, 0.92]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [0.57, 0.89] & [0.57, 0.87] & [0.66, 0.72] & [0.54, 0.60] & [0.57, 0.64] & [0.61, 0.68] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{BettingFav}} & \multicolumn{1}{l|}{\multirow{2}{*}{17.55}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [11.84, 23.67] & [12.13, 23.59] & [12.60, 22.40] & [0.12, 57.53] & [0.16, 57.09] & [0.54, 55.50]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [13.17, 20.97] & [12.86, 22.01] & [13.18, 21.26] & [11.46, 12.54] & [11.73, 13.09] & [11.49, 13.03] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{BettingUnfav}} & \multicolumn{1}{l|}{\multirow{2}{*}{10}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [4.47, 11.63] & [4.79, 11.30] & [10.00, 10.00] & [0.00, 53.37] & [0.00, 53.70] & [0.39, 50.07]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [10.00, 10.00] & [10.00, 10.00] & [10.00, 10.00] & [10.00, 10.00] & [10.00, 10.00] & [10.00, 10.00] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{consensus}} & \multicolumn{1}{l|}{\multirow{2}{*}{0.1}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [0.03, 0.25] & [0.04, 0.22] & [0.05, 0.19] & [0.00, 0.98] & [0.00, 0.90] & [0.01, 0.67]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [0.03, 0.25] & [0.04, 0.22] & [0.05, 0.19] & [0.02, 0.14] & [0.03, 0.13] & [0.03, 0.12] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{csma}} & \multicolumn{1}{l|}{\multirow{2}{*}{0.5}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [0.44, 0.56] & [0.44, 0.56] & [0.44, 0.56] & [0.30, 0.69] & [0.32, 0.70] & [0.40, 0.61]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [0.44, 0.56] & [0.44, 0.56] & [0.45, 0.56] & [0.46, 0.55] & [0.45, 0.55] & [0.46, 0.55] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{Gridworld}} & \multicolumn{1}{l|}{\multirow{2}{*}{-5.48}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [-6.26, -5.11] & [-6.23, -5.13] & [-6.21, -5.12] & [-67.53, -4.35] & [-65.07, -4.40] & [-45.05, -4.48]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [-6.14, -5.13] & [-6.15, -5.13] & [-6.13, -5.13] & [-6.35, -5.17] & [-6.25, -5.15] & [-6.21, -5.11] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{pacman}} & \multicolumn{1}{l|}{\multirow{2}{*}{0.45}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [0.36, 0.53] & [0.37, 0.53] & [0.36, 0.53] & [0.37, 0.53] & [0.37, 0.53] & [0.36, 0.53]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [0.36, 0.53] & [0.37, 0.53] & [0.36, 0.53] & [0.37, 0.53] & [0.37, 0.53] & [0.36, 0.53] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{RacetrackBig}} & \multicolumn{1}{l|}{\multirow{2}{*}{0.89}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [0.00, 0.99] & [0.00, 0.99] & [0.00, 1.00] & [0.00, 1.00] & [0.00, 1.00] & [0.00, 1.00]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [0.28, 0.98] & [0.61, 0.95] & [0.51, 0.97] & [0.41, 0.87] & [0.43, 0.86] & [0.44, 0.86] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{RacetrackSmall}} & \multicolumn{1}{l|}{\multirow{2}{*}{0.49}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [0.11, 0.75] & [0.06, 0.75] & [0.18, 0.70] & [0.00, 0.95] & [0.00, 0.95] & [0.00, 0.95]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [0.29, 0.74] & [0.30, 0.73] & [0.31, 0.69] & [0.33, 0.53] & [0.35, 0.52] & [0.34, 0.52] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{SnL100}} & \multicolumn{1}{l|}{\multirow{2}{*}{-17.41}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [-20.24, -15.96] & [-19.66, -15.83] & [-19.62, -15.86] & [-60.05, -13.60] & [-62.04, -13.72] & [-49.32, -13.83]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [-19.41, -15.93] & [-19.42, -15.89] & [-19.38, -15.93] & [-20.03, -16.81] & [-19.84, -16.63] & [-19.86, -16.68] \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{wlan\_cost}} & \multicolumn{1}{l|}{\multirow{2}{*}{-220}} & \multicolumn{1}{c|}{$\overline{\sigma}$}   & [-226.27, -213.56] & [-225.00, -214.79] & [-224.99, -214.77] & [-226.49, -213.34] & [-224.98, -214.72] & [-225.03, -214.78]  \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{c|}{$\underline{\sigma}$}  & [-226.26, -213.58] & [-224.99, -214.80] & [-224.99, -214.77] & [-226.49, -213.34] & [-224.96, -214.75] & [-225.02, -214.78] \\
\bottomrule
\end{tabular}
}
}
\end{table}



Here we provide results for further examples of MDPs from the formal methods and RL community. 
Important model parameters as well as the value function  the optimistically optimal
strategy $\sigma$ and pessimistically optimal strategy $\sigma$ after the full 50 episodes for a
scoping parameter of $h=0.05$ are provided in \Cref{tbl:experiments}.
Additionally, we provide plots of the value function bounds for each example after each episode. 
For the experimental setup and a full description of the plotted values, we refer to \Cref{sec:evaluation}. 
\clearpage
\input{sections/appendix_figures.tex}