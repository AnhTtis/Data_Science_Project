% !TEX root =  ../main.tex

\section{Interval MDP reinforcement learning}\label{sec:algorithms}
In this section, we establish an RL algorithm for contracting gray-box MDP
that generates a white-box IMDP and successively shrinks the transition probability intervals 
of the IMDP while updating the sampling strategy.
Let $\cM=(S,A,\imath,G,R,T)$ be a contracting MDP as above, serving as environmental model.
With RL in a \emph{gray-box} setting, the agent's objective is to determine 
reward-maximizing strategies knowing
all components of $\cM$ except transition probabilities $T$.
We further make the common assumption~\cite{AshKreWei19,BeiKleLeu17,DacHenKrePet2017}
that there is a known constant $p_{min}$ 
that is a lower bound on the minimal transition probability, i.e.,
$p_{min} \leq \min \{ T(s,a,s') \mid T(s,a,s')>0\}$.

To learn strategies in $\cM$, samples are generated according to a \emph{sampling strategy}
determining the next action an agent performs in each state.
\Cref{fig:workflow} shows the overall schema of the algorithm, which
runs in \emph{episodes}, i.e., batches of samples. 
The sampling strategy is updated after each episode by refining an internal IMDP model 
based on the sample runs and an IMDP value iteration. 


\subsection{Generating IMDPs from sampled gray-box MDPs}\label{subsec:mb}
Let $\#(s,a,s')$ denote the number of times the transition $(s,a,s')$
occurred in samples
thus far and let $\#(s,a)=\sum_{s'\in\Post(s,a)} \#(s,a,s')$.
The goal of each episode is to approximate $\cM$ by an IMDP 
$\cU=(S,A,\imath,G,R,\hat{T})$ that is \emph{($\mathit{1-}\delta$)-correct},
i.e., the probability of $\cM$ being an instantiation of $\cU$ is at
least $1{-}\delta$ for a given error tolerance $\delta\in\Real$. Formally
$\prod_{(s,a)\in S\times A} \Prob\big(T(s,a)\in\hat{T}(s,a)\big) \geqslant 1{-}\delta$~\cite{AshKreWei19},
where $\mathbb{P}$ refers to the probabilistic behaviour of the algorithm
due to sampling the gray-box MDP.
The idea towards $(1{-}\delta)$-correct IMDPs is to distribute the error 
tolerance $\delta$ over transitions by defining a 
\emph{transition error tolerance} $\eta\in\Real$.
Given a state $s\in S$, an action $s\in\Act(s)$ and a successor $s'\in\Post(s,a)$,
we define the interval transition probability function 
$\hat{T}_\eta\colon S{\times}A\ra\Intv(S)$ as
\begin{center}
	$\hat{T}_\eta(s,a,s') = 
	\left[\frac{\#(s,a,s')}{\#(s,a)}-c(s,a,\eta),
	\frac{\#(s,a,s')}{\#(s,a)}+c(s,a,\eta)\right] 
	\cap \left[p_{min},1\right].$
\end{center}
where $c(s,a,\eta)=\sqrt{\frac{\log\eta/2}{-2\#(s,a)}}$.
Hoeffding's inequality \cite{Hoeffding} then yields $T_\eta(s,a)\in\hat{T}(s,a)$ with probability
at least $1{-}\eta$. 
To instantiate an environment approximation, we distribute the error tolerance $\delta$
\emph{uniformly}, i.e., to define $\hat{T}_\eta$ and thus obtain $\cU$ 
we set $\eta = \delta / \Nt$ where $\Nt$ 
is the overall number of probabilistic transitions in $\cM$, i.e.,
$\Nt = \lvert\{ (s,a,s') \mid s'\in\Post(s,a) \text{ and } \lvert\Post(s,a)\rvert>1 \}\rvert$.
Note that $\Nt$ only depends on the topology of the MDP and is thus known
in a gray-box setting.


\begin{algorithm}[t]
	\SetAlgoLined
	\DontPrintSemicolon
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{gray-box MDP $\cM=(S,A,\imath,G,R,\cdot)$, error tolerance $\delta$,
	$K,N\in\Nat$
	}
	\Output{pessimistically and optimistically optimal strategies $\underline{\sigma}$ and $\overline{\sigma}$, 
		value function bounds $\underline{V}$ and $\overline{V}$}
	\BlankLine
	\ForAll{$(s,a)\in S\times A$}{
		$\sigma(s)(a) := 1 / \lvert \Act(s) \rvert$\hfill\tcp{initialize}
		\lForAll{$s'\in \Post(s,a)$}{
			$\hat{T}(s,a,s') := [p_{min},1]$
		}
	}
	$\cU := (S,A,\imath,G,R,\hat{T})$\;
	\BlankLine
	\ForAll{$k\in \{1,\dots,K\}$\label{l:episode}}{%\tcp*[f]{episodes}
		\lForAll{$n\in \{1,\dots,N\}$}{	SAMPLE($\cM,\sigma$)\hfill\tcp*[f]{sample runs}\label{l:sample}}
		$\cU :=$ UPDATE\_PROB\_INTERVALS($\cU,\delta$)\label{alg:update}\hfill\tcp{build IMDP model}
		$(\underline{V}, \overline{V}) :=$ COMPUTE\_BOUNDS($\cU, k$)\label{l:bounds}\hfill\tcp{IMDP value iteration}
		$\sigma :=$ UPDATE\_STRATEGY($\cU, \underline{V},\overline{V})$\label{l:policy}\hfill\tcp{compute sampling strategy}
	}
	\ForAll{$s\in(S\setminus G)$}{
		$\big(\underline{\sigma}(s), \overline{\sigma}(s)\big):=\big(\argmax_{a\in\Act(s)}\underline{Q}(s,a),
		\argmax_{a\in\Act(s)}\overline{Q}(s,a)\big)$\;
	}
	\Return $\underline{\sigma},\underline{V}, \overline{\sigma}, \overline{V}$\;
	\caption{IMDP\_RL($\cM,\delta,K,N$)\label{alg:imdp-rl}}
\end{algorithm}

\subsubsection{Value iteration on environment approximations}
We rely on value iteration for IMDPs~\cite{Givan00,Wu08}
to solve the interval Bellman equations for all possible instantiations
of our environment approximation IMDP $\cU$. 
Standard value iteration for IMDPs does not exhibit a stopping criterion
to guarantee soundness of the results. For soundness,
we extend interval value iteration~\cite{BeiKleLeu17,HadMon18}
with a conservative initialization bound for the value function.
For technical details of the value iteration on IMDPs we refer to the appendix.



\subsection{IMDP-based PAC Reinforcement learning}
Piecing together the parts discussed so far, we obtain an IMDP-based RL algorithm
sketched in \Cref{alg:imdp-rl} (cf. also \Cref{fig:workflow}),
comprising $K$ episodes with $N$ sample runs each,
updating the model and performing a value iteration (see \Cref{l:episode} and 
\Cref{l:sample}, respectively).  Both $K$ and $N$ can be seen as parameters
limiting the sampling access of the agent.
The function \textrm{SAMPLE} in \Cref{l:sample} interacts with the environment $\cM$ and chooses either 
a yet not sampled action, or samples an action according to $\sigma$. 
A run ends when entering a goal state $s\in G$, or upon reaching a length of $\lvert S \rvert$.
The latter is to prevent runs from acquiring a large number of samples by simply staying inside a
cycle for as long as possible.
In \Cref{alg:update}, the subroutine \textrm{UPDATE\_PROB\_INTERVALS}
incorporates the fresh gathered samples from the environment into the internal
IMDP representation as outlined in \Cref{subsec:mb}, updating transition probability intervals.
The IMDP value iteration \textrm{COMPUTE\_BOUNDS} in \Cref{l:bounds} yields
new upper and lower value functions bounds.
The number of value iteration steps is $k\cdot\lvert S \rvert$, i.e., increases with each episode to guarantee that
the value function is computed with arbitrary precision for a large number of episodes,
also known as \emph{bounded value iteration}~\cite{AshKreWei19,BraChaChm14}.
The computed bounds are then used in \textrm{UPDATE\_STRATEGY} in \Cref{l:policy}
to update the sampling strategy for the next episode.
The environment approximation $\cU$ can be achieved following several
strategies according to which samples are generated \cite{Amin21}.
A strategy that is widely used in SMC~\cite{AshKreWei19} or
tabular RL~\cite{Auer04,JakOrtAue10,Strehl04,Strehl08} is
\emph{upper confidence bound (UCB)} sampling.
The UCB strategy samples those actions $a$ in state $s$
that have highest upper bound on the quality $\overline{Q}(s,a)$,
resolving the well-known exploration-exploitation dilemma in RL.
This principle is also known as ``optimism in the face of uncertainty'' (OFU),
referring to UCB allocating uncertain probability mass to the best possible outcome~\cite{Amin21}.
In our framework, standard UCB sampling will serve as the baseline approach.
Lastly, we compute and return pessimistic and optimistic strategies
along with their value function bounds, before returning them.
\begin{theorem}\label{thm:guarantees}
Let $V^{*}$ be the solution to the Bellman equations of a given MDP $\cM$. 
Then for all $\delta\in\ ]0,1[$ and $K,N\in\Nat$
the value function bounds $\underline{V}$ and $\overline{V}$ returned by 
\textup{IMDP\_RL($\cM,\delta,K,N$)} as of \Cref{alg:imdp-rl} contain $V^{*}$ 
with probability at least $1-\delta$, 
i.e., $\Prob\left(\underline{V}(s) \leqslant V^{*}(s) \leqslant \overline{V}(s)\right)\geqslant 1-\delta$
for all $s\in S$. 
\end{theorem}
The proof of this theorem is provided in the appendix, essentially showing that (1)
$\cM\in [\cU]$ with probability at least $1-\delta$ and (2) for each state $s$
the solution to the interval Bellman equation of $\cU$ is a subinterval 
of the computed $[\underline{V}(s), \overline{V}(s)]$.
