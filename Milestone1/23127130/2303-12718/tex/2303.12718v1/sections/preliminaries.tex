% !TEX root =  ../main.tex

\section{Preliminaries}\label{sec:preliminaries}


A \emph{distribution} over a finite set $X$ is a function $\mu\colon X\rightarrow [0,1]$
where $\sum_{x\in X} \mu(x) = 1$. The set of distributions over $X$ is denoted by
$\Dist(X)$.

\subsubsection{Markov decision processes (MDPs)} 
An MDP is a tuple $\cM = (S,A,\imath,G,R,T)$ where 
$S$, $A$, and $G \subseteq S$ are finite sets of states, actions, and goal states, respectively,
$\imath \in S$ is an initial state, $R \colon S \rightarrow \Real$ is a reward function,
and $T\colon S{\times}A \pa \Dist(S)$ is a partial transition probability function.
For state $s\in S$ and action $a\in A$ we say that $a$ is \emph{enabled} in $s$ if
$T(s,a)$ is defined. We assume the set $\Act(s)$ of all enabled actions to be empty
in goal states $s\in G$ and non-empty in all other states.
For $(s,a,s')\in S{\times}A{\times}S$ we define $T(s,a,s')=T(s,a)(s')$ 
if $T(s,a)$ is defined and $T(s,a,s')=0$ otherwise.
The \emph{successors} of $s$ via $a$ are denoted by $\Post(s,a) = \{ s' \mid T(s,a,s')>0 \}$.
A \emph{run} of $\cM$ is a sequence $\pi = s_0a_0s_1a_1\dots s_n$
where $s_0=\imath$, $s_n \in S$, $(s_i,a_i)\in (S{\setminus}G)\times A$, 
and $s_{i+1}\in\Post(s_i,a_i)$ for $i= 0,\ldots,n{-}1$.
The set of all runs in $\cM$ is denoted by $\Runs(\cM)$.
The \emph{accumulated reward} of $\pi$ is defined by $R(\pi) = \sum_{i=0}^{n-1} R(s_i)$.



An \emph{interval MDP (IMDP)} is a tuple
$\cU=(S,A,\imath,G,R,\hat{T})$ where $S,A,\imath,G$, and $R$ are as 
for MDPs, and $\hat{T} \colon S {\times} A \pa \Intv(S)$ is an
\emph{interval transition function}. Here, $\Intv(S)$ denotes the set of interval functions
$\nu\colon S \ra \{ [a,b] \mid 0 < a \leq b \leq 1 \} \cup \{ [0,0] \}$ over $S$.
Note that $\Dist(S)\subseteq\Intv(S)$, i.e., every distribution over $S$ is also
an interval function. A distribution $\mu\in\Dist(S)$ is an \emph{instantiation} of 
$\nu\in\Intv(S)$ if $\mu(s)\in\nu(s)$ for all $s\in S$. 
We again say $a$ is enabled in $s$ if $\hat{T}(s,a)$ is defined and denote
the set of enabled actions in $s$ as $\Act(s)$, assumed to be non-empty for all $s\in (S\setminus G)$.
For each $s\in S$ and $a\in \Act(s)$ we denote by $T_s^a$ the set of 
all instantiations $t_s^a$ of $\hat{T}(s,a)$ and define $\Post(s,a)=\{s' \mid \underline{T}(s,a,s')>0\}$.
The MDP $\cM$ is an \emph{instantiation} of $\cU$ if 
$T(s,a)\in T_s^a$ for all $s\in S$, $a\in A$.
We denote by $[\cU]$ the set of all instantiations of $\cU$.
Note that as all instantiations of an IMDP $\cU$ share the same topology, 
the set of runs $\Runs(\cM)$ is the same for all instantiations $\cM\in[\cU]$.


The semantics of the MDP $\cM$ is given 
through \emph{strategies}, i.e., mappings $\sigma\colon S
\rightarrow \Dist(A)$ where $\sigma(s)(a)=0$ for all $a\not\in\Act(s)$. 
We call a run $\pi=s_0a_0s_1a_1\dots s_n$ 
in $\cM$ a \emph{$\sigma$-run} if $\sigma(s_i)(a_i)>0$ for all $i=0,\ldots,n{-}1$. 
The probability of $\pi$ is defined as $\Pr^\sigma(\pi) = \prod_{i=0}^{n-1} \sigma(s_i)(a_i)\cdot T(s_i,a_i,s_{i+1})$ if $\pi$ is a $\sigma$-run and $\Pr^\sigma(\pi)=0$ otherwise.
The probability of some $B\subseteq\Runs(\cM)$ w.r.t. strategy $\sigma$ is defined
by $\Pr^\sigma(B) = \sum_{\pi\in B} \Pr^\sigma(\pi)$.
If $\Pr^\sigma(B)=1$, then the \emph{expected (accumulated) reward} is defined
as $\Exp^\sigma(B) = \sum_{\pi\in B} \Pr^\sigma(\pi)\cdot R(\pi)$.
We call $\cM$ \emph{contracting} \cite{KalLN20} if $\Pr^\sigma(\lozenge G) = 1$ for all strategies $\sigma$, i.e., 
a goal state is almost surely reached for any strategy. 
The semantics of an IMDP $\cU$ is the set of its instantiations $[\cU]$. 
An IMDP $\cU$ is \emph{contracting} iff all MDPs in $[\cU]$ are contracting. 
Note that for IMDPs there is also a notion of an operational semantics that lifts strategies
to instantiations of transitions \cite{SenVisAgh06}. While different in its nature, our contributions 
of this paper can be easily extended also to the latter semantics.


\subsubsection{Value and quality functions}
A \emph{value function} $V_{\cM} \colon S \rightarrow \Real$ of MDP $\cM$ is the solution of the 
\emph{Bellman equations}~\cite{BerTsi91} given by $V_\cM(s)=R(s)$ for $s\in G$ and
\begin{center}
$V(s)\ =\
	R(s) + \max_{a\in \Act(s)} \sum_{s' \in S} V_\cM(s')\cdot T(s,a,s')$ \quad for \enskip $s\not\in G$.%& \text{else}
\end{center}
The \emph{quality} $Q_\cM\colon S\times A \pa \Real$ of $\cM$ is defined for all $s\in S$ and $a\in \Act(s)$ by
\begin{center}$
	Q_\cM(s,a)\ =\ R(s)\ + \sum\nolimits_{s' \in \Post(s,a)} V_\cM(s')\cdot T(s,a,s')
$\end{center}
Intuitively, the quality represents the value of choosing 
an action $a$ in state $s$ continuing with a reward-maximizing strategy.
For an IMDP $\cU$, the value function differs between instantiations, leading to
Bellman equations

\begin{center}
	$\underline{V}_\cU(s)\ =\  \min_{\cM\in [\cU]}{V_\cM(s)} \quad\quad
\overline{V}_\cU(s) =\max_{\cM\in [\cU]}{V_\cM(s)}$
\end{center}
for the lower and upper bounds on possible instantiations, respectively. 
These value functions are naturally lifted to quality functions for IMDPs.
We omit subscript $\cM$ or $\cU$ if clear from the context.
Further, we define the \emph{pessimistically optimal strategy}
$\underline{\sigma}$ for all $s\in (S\setminus G)$ as
$\underline{\sigma}(s)=\argmax_{a\in\Act(s)}\underline{Q}(s,a)$ and similarly the
\emph{optimistically optimal strategy} as
$\overline{\sigma}(s)=\argmax_{a\in\Act(s)}\overline{Q}(s,a)$.

\begin{figure}[t]
	\resizebox{\textwidth}{!}{\input{img/workflow}}
	\caption{\label{fig:workflow}Schema of reinforcement learning in gray-box MDPs}
\end{figure}
