% !TEX root =  ../main.tex

\section{Introduction}\label{sec:introduction}
Many machine learning methods take inspiration from the inner-workings of the human 
brain or human behavior~\cite{Mitchell1997}. For instance, learning based
on neural networks mimics the human brain at a structural level by explicitly modeling
its neurons and their activation. Taking a more high-level
view, \emph{reinforcement learning (RL)}~\cite{SutBar18} formalizes human
learning behavior by reinforcing actions that are repeatedly associated 
with successful task solving~\cite{And00}.
The usual application of RL is to learn reward-optimizing strategies
in environments modeled as \emph{Markov decision processes (MDPs)}~\cite{Puterman} 
where the agent has only partial knowledge and 
learns based on guided exploration through sample runs.
Existing RL approaches prioritize stochastic guarantees and convergence 
to a globally optimal strategy, leading to slow learning performance and infeasibility
for small sample sizes~\cite{SutBar18}.
In contrast, human decision making can compete with limited sampling access, not focusing
on strict optimality but on efficiency. 
The more urgent a task and the less time available for its solving, 
the more humans tend to exploit previously learned strategies -- 
possibly sacrificing optimality but increasing the chance of finishing the task in time~\cite{Schwoebel21}. 
In the extremal case, humans rely on \emph{habits}~\cite{WooRun16}, i.e., 
sequences of actions that, once triggered, are executed mostly independent from 
reasoning about the actual task~\cite{BaiRivDub21}. 
Habits avoid further costly exploration during learning by restricting 
the action space.



In this paper, we take inspiration from humans' ability to reason efficiently 
with few explorations, shaping novel RL algorithms that rapidly synthesize ``good'' strategies.
Specifically, our learning task amounts to an agent being able to determine a strategy with high expected 
accumulated reward until reaching a goal, given a limited number of samples.
We consider the setting where the environment is modeled as \emph{contracting MDP}, i.e., 
goal states are almost surely reached under all strategies,
on which the agent has a \emph{gray-box} view, i.e., 
knows the reward structures and the topology but not the exact probabilities~\cite{AshKreWei19}.
We tackle this task of sample-bounded learning towards nearly-optimal strategies
by introducing two new concepts: \emph{lower confidence bound (LCB) sampling} and \emph{action scoping}.
Classical reward-based sampling in RL is based on upper confidence bounds (UCB)~\cite{Amin21}, 
balancing the \emph{exploration-exploitation dilemma}~\cite{SutBar18}. 
In contrast, our LCB sampling method favors situations already shown viable during the learning process. 
Hence, exploration is limited when there are no good reasons for leaving 
well-known paths, similar to what humans do with habitual sequences of actions~\cite{WooRun16}.
The second learning component is \emph{action scoping}, 
restraining exploration actions when shown to be suboptimal in past samples.
Scoping is parametrized to tune the degree of exploration and balance between fast strategy synthesis
or increasing the chance of learning optimal strategies.

To implement our novel concepts, we provide technical contributions by presenting an RL algorithm
on contracting gray-box MDPs with arbitrary rewards and various sampling methods.
The learning algorithm is a sample-based approach that generates an \emph{interval MDP (IMDP)}
to approximate the environment and whose intervals are iteratively refined.
While methods for analyzing IMDPs have already been considered in the literature~\cite{Givan00,Wu08}, we 
provide a new connection of their use in RL algorithms.
We devise our human-inspired RL algorithms, including LCB and action scoping, 
by modeling knowledge of the agent as IMDP
using concepts from \emph{model-based interval estimation (MBIE)}~\cite{Strehl08} 
and \emph{probably almost correct (PAC) statistical model checking (SMC)}~\cite{AshKreWei19}.
We show that our algorithms on IMDPs are PAC for UCB and LCB sampling,
i.e., the probability %that the learning algorithm returns a 
of a suboptimal strategy can be quantified by an arbitrarily small error tolerance.
This, however, cannot be guaranteed in the case of action scoping.
Towards an evaluation of LCB and action scoping, we implemented our algorithms 
in a prototypical tool~\cite{artifact}.
By means of several experimental studies from the RL and formal-methods community, e.g., 
on multi-armed bandits~\cite{Web92} and \racetrack\ \cite{BarBraSin95}, we show that LCB and action scoping 
foster fast strategy synthesis, providing better strategies after fewer sample runs than RL-style
PAC-SMC methods.
We discuss the impact of scoping parameters and related
heuristics, as well as combinations of sampling strategies.
In summary, our contributions are:
\begin{itemize}
	\item A (model-based) RL algorithm for contracting gray-box MDPs with integer rewards
		relying on IMDP and sampling strategy refinements
	(see \Cref{sec:algorithms})
	\item Instances of this RL algorithm subject to
				lower and upper confidence bound sampling and
			tunable action scoping (see \Cref{sec:habits}).
	\item A prototypical implementation of our RL algorithms and an
	evaluation %of the algorithms and synthesized strategies 
		in examples from both the RL and formal-methods communities.
\end{itemize}

\subsubsection{Related work}
SMC~\cite{Legay2019} for unbounded temporal properties in stochastic systems is most related to 
our setting,
establishing algorithms also in gray-box settings~\cite{YouClaZul10,HeJenBas10}.
Given a lower bound on transition probabilities, SMC algorithms have been presented 
for Markov chains~\cite{DacHenKre16}, MDPs, and even stochastic games~\cite{AshKreWei19}.
Recent SMC algorithms for MDPs also include learning~\cite{BraChaChm14,AshKreWei19}
but only for reachability problems.
IMDPs have been investigated outside of the RL context in formal 
verification~\cite{SenVisAgh06,ChaSenHen08} for $\omega$-regular properties,
for positive rewards in contracting models~\cite{Wu08} by an extension of the well-known 
value-iteration algorithm~\cite{SutBar18}, and
in the performance-evaluation community in the discounted setting \cite{Givan00}.
In particular, the RL algorithms we present in this paper use an adaptation of the latter algorithm
without discounting as a subroutine to successively tighten bounds on the maximal expected accumulated rewards.
More recently, algorithms with convergence guarantees for reachability
objectives in (interval) MDPs have been presented \cite{BeiKleLeu17,HadMon18}.
\emph{Interval estimation} for RL has been introduced by Kaelbling~\cite{Kae93}
towards Q-learning~\cite{WatDay92} and extended to model-based approaches~\cite{Wie98}
such as MBIE~\cite{Strehl04} 
and
the UCRL2 algorithm \cite{JakOrtAue10} using an error tolerance based on the $L_1$-norm
opposed to the $L_\infty$-norm employed in interval MDPs.
Besides UCB sampling, the exploration-exploitation dilemma in reward-based learning 
has also be addressed with exploration bonuses~\cite{Kae93,KaeLitMoo96,Sut91,ThrMol92},
performing well when applied to MBIE~\cite{Ish02,Strehl08}
or in other RL methods such as E$^3$~\cite{KeaSin02} and R$_\mathrm{max}$~\cite{Rmax}.

\subsubsection{Supplements}
The appendix contains proofs and full experimental evaluations.
Our implementation and data sets to reproduce the experiments of this paper
is available~\cite{artifact}.