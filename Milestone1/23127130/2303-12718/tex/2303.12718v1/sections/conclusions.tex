% !TEX root =  ../main.tex

\section{Concluding remarks}\label{sec:conclusions}

We devised novel model-based RL algorithms that are inspired by efficient human reasoning
under time constraints.
Similar to humans tending to stick and return to known situations during strategy learning,
LCB exploration favors to return to states with high confidence and proceeding with
viable learned strategies.
On the action level, scoping implements a reduction of the exploration space
as humans do when favoring known actions without further exploration.
As for humans acting under resource constraints, both ingredients have been shown to yield
better strategies after few sample runs than classical RL methods, especially
when choosing high scoping parameters that foster action scoping.
While our methods synthesize good strategies faster, an optimal strategy
might be not achievable in the limit.
We would like to emphasize that also using IMDPs as internal model for RL is a result by
itself, incorporating the knowledge about action scopes and confidences in the learned
probabilities of the environmental MDP model.

We mainly discussed applications of our techniques in the setting of reinforcement learning.
Nevertheless, they can well be utilized also in the formal methods domain, 
providing an existential variant for statistical model checking of MDPs,
asking for the existence of a strategy to reach a goal with accumulating a certain reward.


In future work, we will further investigate adaptations of our algorithms to reflect
main properties of human reasoning, e.g., by establishing real-world neuroscientific 
experiments that involve humans. Our vision is to use formal methods
to support explanations of human behaviors, to which we see the 
present paper providing a first step in this direction~\cite{BaiDubFun21}.
Future adaptations of our proposed algorithm may focus on more sophisticated
exploration schemes than dithering, such as softmax selection or exploration bonuses,
that allow for deeper exploration of the environment \cite{SutBar18}.
While not the main focus of this paper, it is well possible to extend our approach also 
to a black-box setting, i.e., without knowledge about the topology of the MDP, using 
similar techniques as in \cite{AshKreWei19}. 
One advantage of using the gray-box setting is in also ensuring applicability to the
instance of infinite state MDPs with finitely many actions if this MDP can be effectively
explored. For this, it suffices to consider only a finite fragment of the MDP given
that our algorithms restrict the sample lengths to a fixed bound.
