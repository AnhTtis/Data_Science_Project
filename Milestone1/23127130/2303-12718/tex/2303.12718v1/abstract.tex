% !TEX root =  main.tex
\begin{abstract}
	A central task in control theory, artificial intelligence, and formal 
	methods is to synthesize reward-maximizing strategies for agents that operate in 
	partially unknown environments. 
	In environments modeled by \emph{gray-box} Markov decision processes (MDPs), 
	the impact of the agents' actions are known in terms of successor states but 
	not the stochastics involved.
	In this paper, we devise a strategy synthesis algorithm for gray-box MDPs via
	reinforcement learning that utilizes \emph{interval MDPs} as internal model.
	To compete with limited sampling access in reinforcement learning, 
	we incorporate two novel concepts into our algorithm,
	focusing on rapid and successful learning rather than on stochastic guarantees and optimality:
	\emph{lower confidence bound} exploration 
	reinforces variants of already learned practical strategies 
	and \emph{action scoping} 
	reduces the learning action space to promising actions.
	We illustrate benefits of our algorithms by means of a prototypical implementation 
	applied on examples from the AI and formal methods communities.
\end{abstract}
