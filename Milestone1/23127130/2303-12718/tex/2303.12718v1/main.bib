
@string{lncs = {LNCS}}

@string{springer = {Springer-Verlag}}


@inproceedings{BaiDubFun21,
	address = {Dagstuhl, Germany},
	author = {Baier, Christel and Dubslaff, Clemens and Funke, Florian and Jantsch, Simon and Majumdar, Rupak and Piribauer, Jakob and Ziemek, Robin},
	booktitle = {48th International Colloquium on Automata, Languages, and Programming (ICALP 2021)},
	date-added = {2023-03-22 00:01:59 +0100},
	date-modified = {2023-03-22 00:14:23 +0100},
	isbn = {978-3-95977-195-5},
	issn = {1868-8969},
	pages = {1:1--1:20},
	publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
	series = {Leibniz International Proceedings in Informatics (LIPIcs)},
	title = {{From Verification to Causality-Based Explications}},
	volume = {198},
	year = {2021},
	annote = {Keywords: Model Checking, Causality, Responsibility, Counterfactuals, Shapley value}}

@article{BerTsi91,
	author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
	doi = {10.1287/moor.16.3.580},
	journal = {Mathematics of Operations Research},
	number = {3},
	pages = {580-595},
	title = {An Analysis of Stochastic Shortest Path Problems},
	volume = {16},
	year = {1991}}

@article{SinJaaLitSze00,
	author = {Singh, Satinder and Jaakkola, Tommi and Littman, Michael and Szepesv{\'a}ri, Csaba},
	doi = {10.1023/A:1007678930559},
	journal = {Machine Learning},
	month = {03},
	pages = {287-308},
	title = {Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms},
	volume = {38},
	year = {2000}}

@article{JakOrtAue10,
	author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	date-added = {2022-12-19 11:10:53 +0100},
	date-modified = {2022-12-19 11:10:55 +0100},
	journal = {Journal of Machine Learning Research},
	number = {51},
	pages = {1563--1600},
	title = {Near-optimal Regret Bounds for Reinforcement Learning},
	volume = {11},
	year = {2010}}

@article{Web92,
	author = {Weber, Richard},
	date-added = {2022-12-14 14:19:05 +0100},
	date-modified = {2022-12-14 14:19:05 +0100},
	doi = {10.1214/aoap/1177005588},
	journal = {The Annals of Applied Probability},
	keywords = {Markov decision processes, Multiarmed bandit problem, Optimal stopping, sequential methods, stochastic scheduling},
	number = {4},
	pages = {1024 -- 1033},
	publisher = {Institute of Mathematical Statistics},
	title = {{On the Gittins Index for Multiarmed Bandits}},
	volume = {2},
	year = {1992}}

@book{Mitchell1997,
	author = {Mitchell, Tom},
	date-added = {2022-11-17 11:32:22 +0100},
	date-modified = {2022-11-17 11:32:22 +0100},
	keywords = {ML,book.ml,ebook},
	publisher = {McGraw Hill},
	title = {Machine Learning},
	year = {1997}}

@url{artifact,
	date-added = {2022-01-22 10:21:10 +0100},
	date-modified = {2022-01-22 10:22:19 +0100},
	title = {\url{https://osf.io/r24mu/?view_only=b44cec578cce44e5920f150940f68230}}}

@inproceedings{BaiDubHerKlaKluKoe20,
	address = {Cham},
	author = {Baier, Christel and Dubslaff, Clemens and Hermanns, Holger and Klauck, Michaela and Kl{\"u}ppelholz, Sascha and K{\"o}hl, Maximilian A.},
	booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles},
	date-added = {2022-01-20 16:36:10 +0100},
	date-modified = {2023-03-22 00:15:02 +0100},
	pages = {240--261},
	publisher = {Springer International Publishing},
	series = {Lecture Notes in Computer Science},
	title = {Components in Probabilistic Systems: Suitable by Construction},
	year = {2020},
	abstract = {This paper focusses on the question when and to what extent a particular system component can be considered suitable to use in the context of the dynamics of a larger technical system. We introduce different notions of suitability that arise naturally in the context of probabilistic nondeterministic systems that interact through the exchange of messages in the style of input-output automata. Besides discussing algorithmic aspects for an analysis following our notions of suitability, we demonstrate practical usability of our concepts by means of experiments on a concrete use case.}}

@article{BarBraSin95,
	author = {Barto, Andrew G. and Bradtke, Steven J. and Singh, Satinder P.},
	date-added = {2022-01-20 16:34:53 +0100},
	date-modified = {2022-01-20 16:34:53 +0100},
	journal = {Artif. Intell.},
	number = {1-2},
	pages = {81--138},
	title = {Learning to Act Using Real-Time Dynamic Programming},
	volume = {72},
	year = {1995}}

@inproceedings{PinZil14,
	author = {Pineda, Luis Enrique and Zilberstein, Shlomo},
	booktitle = {ICAPS},
	date-added = {2022-01-20 16:34:53 +0100},
	date-modified = {2022-01-20 16:34:53 +0100},
	title = {Planning Under Uncertainty Using Reduced Models: Revisiting Determinization},
	year = {2014}}

@book{Puterman,
	author = {Puterman, M.},
	date-added = {2022-01-19 01:08:23 +0100},
	date-modified = {2022-01-19 01:08:23 +0100},
	publisher = {John Wiley \& Sons, Inc.},
	title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
	year = {1994}}

@book{And00,
	address = {{Hoboken, NJ, US}},
	author = {Anderson, John Robert},
	date-added = {2022-01-18 23:54:46 +0100},
	date-modified = {2022-01-18 23:54:49 +0100},
	file = {/home/dario/zotero/storage/8X6LSM4D/2000-07354-000.html},
	isbn = {978-0-471-24925-2},
	keywords = {Cognitive Processes,Forgetting,Human Information Storage,Learning,Memory},
	pages = {487},
	publisher = {{John Wiley \& Sons Inc}},
	series = {Learning and Memory: {{An}} Integrated Approach, 2nd Ed},
	shorttitle = {Learning and Memory},
	title = {Learning and Memory: {{An}} Integrated Approach, 2nd Ed},
	year = {2000},
	abstract = {Examines the current state of the traditional learning and cognitive fields. Chapter 1 reviews the history of learning and memory research. It explains ideas and paradigms that have dominated the field and why they became prominent. Chapters 2\textendash{}4 describe the modern understanding of learning and memory through animal learning, classical conditioning, instrumental conditioning, and reinforcement. Chapters 5\textendash{}8 focus on human learning and memory. Chapter 5 examines how information is processed and stored in temporary memories when it is initially received while Chapter 6 investigates how a permanent record of this information is built up in long-term memory. Chapter 7 explores how information is maintained over potentially long periods of time and what underlies forgetting. Chapter 8 looks at the different ways in which information can be retrieved when it is needed. Chapter 9 reviews the learning phenomena that arise when potentially complex skills are acquired. Chapter 10 focuses on issues of inductive learning, how people discover things about the structure of their environment. The final chapter is concerned with the major application of research on learning and memory to education. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}}

@article{LalJaaPot10,
	author = {Lally, Phillippa and van Jaarsveld, Cornelia H. M. and Potts, Henry W. W. and Wardle, Jane},
	copyright = {Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.},
	date-added = {2022-01-18 23:53:50 +0100},
	date-modified = {2022-01-18 23:53:53 +0100},
	doi = {10.1002/ejsp.674},
	file = {/home/dario/zotero/storage/7BTSP52S/ejsp.html},
	issn = {1099-0992},
	journal = {European Journal of Social Psychology},
	keywords = {habits,modeling,unread},
	language = {en},
	number = {6},
	pages = {998--1009},
	shorttitle = {How Are Habits Formed},
	title = {How Are Habits Formed: {{Modelling}} Habit Formation in the Real World},
	volume = {40},
	year = {2010},
	abstract = {To investigate the process of habit formation in everyday life, 96 volunteers chose an eating, drinking or activity behaviour to carry out daily in the same context (for example `after breakfast') for 12 weeks. They completed the self-report habit index (SRHI) each day and recorded whether they carried out the behaviour. The majority (82) of participants provided sufficient data for analysis, and increases in automaticity (calculated with a sub-set of SRHI items) were examined over the study period. Nonlinear regressions fitted an asymptotic curve to each individual's automaticity scores over the 84 days. The model fitted for 62 individuals, of whom 39 showed a good fit. Performing the behaviour more consistently was associated with better model fit. The time it took participants to reach 95\% of their asymptote of automaticity ranged from 18 to 254 days; indicating considerable variation in how long it takes people to reach their limit of automaticity and highlighting that it can take a very long time. Missing one opportunity to perform the behaviour did not materially affect the habit formation process. With repetition of a behaviour in a consistent context, automaticity increases following an asymptotic curve which can be modelled at the individual level. Copyright \textcopyright{} 2009 John Wiley \& Sons, Ltd.}}

@article{WooRun16,
	author = {Wood, Wendy and R{\"u}nger, Dennis},
	date-added = {2022-01-18 23:51:57 +0100},
	date-modified = {2022-01-18 23:52:18 +0100},
	doi = {10.1146/annurev-psych-122414-033417},
	file = {/home/dario/zotero/storage/WQ48CR5A/Wood and R{\"u}nger - 2016 - Psychology of Habit.pdf},
	journal = {Annual Review of Psychology},
	keywords = {habits,unread},
	number = {1},
	pages = {289--314},
	pmid = {26361052},
	title = {Psychology of {{Habit}}},
	volume = {67},
	year = {2016},
	abstract = {As the proverbial creatures of habit, people tend to repeat the same behaviors in recurring contexts. This review characterizes habits in terms of their cognitive, motivational, and neurobiological properties. In so doing, we identify three ways that habits interface with deliberate goal pursuit: First, habits form as people pursue goals by repeating the same responses in a given context. Second, as outlined in computational models, habits and deliberate goal pursuit guide actions synergistically, although habits are the efficient, default mode of response. Third, people tend to infer from the frequency of habit performance that the behavior must have been intended. We conclude by applying insights from habit research to understand stress and addiction as well as the design of effective interventions to change health and consumer behaviors.}}

@article{BhaVarKum21,
	abstractnote = {&lt;p&gt;In urban environments, resources have to be constantly matched to the ``right'' locations where customer demand is present. For instance, ambulances have to be matched to base stations regularly so as to reduce response time for emergency incidents in ERS (Emergency Response Systems); vehicles (cars, bikes among others) have to be matched to docking stations to reduce lost demand in shared mobility systems. Such problems are challenging owing to the demand uncertainty, combinatorial action spaces and constraints on allocation of resources (e.g., total resources, minimum and maximum number of resources at locations and regions).&lt;/p&gt;&lt;p&gt;Existing systems typically employ myopic and greedy optimization approaches to optimize resource allocation. Such approaches typically are unable to handle surges or variances in demand patterns well. Recent work has demonstrated the ability of Deep RL methods in adapting well to highly uncertain environments. However, existing Deep RL methods are unable to handle combinatorial action spaces and constraints on allocation of resources. To that end, we have developed three approaches on top of the well known actor-critic approach, DDPG (Deep Deterministic Policy Gradient) that are able to handle constraints on resource allocation. We also demonstrate that they are able to outperform leading approaches on simulators validated on semi-real and real data sets.&lt;/p&gt;},
	author = {Bhatia, Abhinav and Varakantham, Pradeep and Kumar, Akshat},
	date-added = {2022-01-14 14:50:12 +0100},
	date-modified = {2022-01-14 14:50:12 +0100},
	journal = {Proceedings of the International Conference on Automated Planning and Scheduling},
	month = {May},
	number = {1},
	pages = {610-620},
	title = {Resource Constrained Deep Reinforcement Learning},
	volume = {29},
	year = {2021}}

@article{KriElmFra19,
	address = {New York, NY, USA},
	author = {Krishnan, Sanjay and Elmore, Aaron J. and Franklin, Michael and Paparrizos, John and Shang, Zechao and Dziedzic, Adam and Liu, Rui},
	date-added = {2021-12-17 16:46:06 +0100},
	date-modified = {2021-12-17 16:46:12 +0100},
	doi = {10.1145/3352020.3352022},
	issn = {0163-5980},
	issue_date = {July 2019},
	journal = {SIGOPS Oper. Syst. Rev.},
	month = {jul},
	number = {1},
	numpages = {6},
	pages = {1--6},
	publisher = {Association for Computing Machinery},
	title = {Artificial Intelligence in Resource-Constrained and Shared Environments},
	volume = {53},
	year = {2019},
	abstract = {The computational demands of modern AI techniques are immense, and as the number of practical applications grows, there will be an increasing burden on shared computing infrastructure. We envision a forthcoming era of "AI Systems" research where reducing resource consumption, reasoning about transient resource availability, trading off resource consumption for accuracy, and managing contention on specialized hardware will become the community's main research focus. This paper overviews the history of AI systems research, a vision for the future, and the open challenges ahead.}}

@article{SunChoNie20,
	author = {Sung, Inkyung and Choi, Bongjun and Nielsen, Peter},
	date-added = {2021-12-17 16:32:06 +0100},
	date-modified = {2021-12-17 16:32:11 +0100},
	doi = {https://doi.org/10.1016/j.ifacol.2020.12.2794},
	issn = {2405-8963},
	journal = {IFAC-PapersOnLine},
	keywords = {Job, activity scheduling, Intelligent decision support systems in manufacturing, Resource Allocation, Reinforcement learning control, Deep Q-learning, Activity Iteration, Crashing},
	note = {21st IFAC World Congress},
	number = {2},
	pages = {10493-10497},
	title = {Reinforcement Learning for Resource Constrained Project Scheduling Problem with Activity Iterations and Crashing},
	volume = {53},
	year = {2020},
	abstract = {Resource allocation is a key decision-making process in project management that assigns resources to activities of a project and determines the timing of the allocation in a cost and time effective manner. In this research, we address the resource allocation for a project, where iterations between activities of the project exist and the crashing, a method to shorten the duration of an activity by incorporating additional resources, is available. Considering the stochastic nature of project execution, we formulate the resource allocation as a Markov decision process and seek the best resource allocation policy using a deep reinforcement learning algorithm. The feasibility and performance of applying the algorithm to the resource allocation is then investigated by comparison with heuristic rules.}}

@inproceedings{DehJunKat17,
	address = {Cham},
	author = {Dehnert, Christian and Junges, Sebastian and Katoen, Joost-Pieter and Volk, Matthias},
	booktitle = {Computer Aided Verification},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	editor = {Majumdar, Rupak and Kun{\v{c}}ak, Viktor},
	isbn = {978-3-319-63390-9},
	pages = {592--600},
	publisher = {Springer International Publishing},
	title = {A Storm is Coming: A Modern Probabilistic Model Checker},
	year = {2017},
	abstract = {We launch the new probabilistic model checker Storm. It features the analysis of discrete- and continuous-time variants of both Markov chains and MDPs. It supports the Prism and JANI modeling languages, probabilistic programs, dynamic fault trees and generalized stochastic Petri nets. It has a modular set-up in which solvers and symbolic engines can easily be exchanged. It offers a Python API for rapid prototyping by encapsulating Storm's fast and scalable algorithms. Experiments on a variety of benchmarks show its competitive performance.}}

@book{FilVri96,
	address = {Berlin, Heidelberg},
	author = {Filar, Jerzy and Vrieze, Koos},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	isbn = {0387948058},
	publisher = {Springer-Verlag},
	title = {Competitive Markov Decision Processes},
	year = {1996}}

@book{Bel57,
	address = {Princeton, NJ, USA},
	author = {Bellman, Richard},
	bib2html_rescat = {General RL},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	edition = {1},
	publisher = {Princeton University Press},
	title = {Dynamic Programming},
	year = {1957}}

@article{AltKinSch93,
	author = {Althoen, S. C. and King, L. and Schilling, K.},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	issn = {00255572},
	journal = {The Mathematical Gazette},
	number = {478},
	pages = {71--76},
	publisher = {Mathematical Association},
	title = {How Long Is a Game of Snakes and Ladders?},
	volume = {77},
	year = {1993}}

@inbook{BaiRivDub21,
	author = {Baier, Christel and {Cuevas Rivera}, Dar{\'\i}o and Dubslaff, Clemens and Kiebel, Stefan J},
	booktitle = {Tactile Internet with Human-in-the-Loop},
	chapter = {8},
	date = {2021-01-01},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2022-01-21 22:51:58 +0100},
	editor = {Fitzek, Frank H P and Li, Shu-Chen and Speidel, Stefanie and Strufe, Thorsten and Simsek, Meryem and Reisslein, Martin},
	pages = {173-200},
	publisher = {Academic Press},
	pubstate = {published},
	title = {Human-inspired models for tactile computing},
	tppubtype = {inbook},
	year = {2021}}

@article{FehHar20,
	author = {Feher da Silva, Carolina and Hare, Todd A.},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	day = {01},
	doi = {10.1038/s41562-020-0905-y},
	issn = {2397-3374},
	journal = {Nature Human Behaviour},
	month = {Oct},
	number = {10},
	pages = {1053-1066},
	title = {Humans primarily use model-based inference in the two-stage task},
	volume = {4},
	year = {2020}}

@inproceedings{JamSin04,
	address = {New York, NY, USA},
	author = {James, Michael R. and Singh, Satinder},
	booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	doi = {10.1145/1015330.1015359},
	isbn = {1581138385},
	location = {Banff, Alberta, Canada},
	numpages = {8},
	pages = {53},
	publisher = {Association for Computing Machinery},
	series = {ICML '04},
	title = {Learning and Discovery of Predictive State Representations in Dynamical Systems with Reset},
	year = {2004},
	abstract = {Predictive state representations (PSRs) are a recently proposed way of modeling controlled dynamical systems. PSR-based models use predictions of observable outcomes of tests that could be done on the system as their state representation, and have model parameters that define how the predictive state representation changes over time as actions are taken and observations noted. Learning PSR-based models requires solving two subproblems: 1) discovery of the tests whose predictions constitute state, and 2) learning the model parameters that define the dynamics. So far, there have been no results available on the discovery subproblem while for the learning subproblem an approximate-gradient algorithm has been proposed (Singh et al., 2003) with mixed results (it works on some domains and not on others). In this paper, we provide the first discovery algorithm and a new learning algorithm for linear PSRs for the special class of controlled dynamical systems that have a reset operation. We provide experimental verification of our algorithms. Finally, we also distinguish our work from prior work by Jaeger (2000) on observable operator models (OOMs).}}

@article{HaiKra13,
	author = {Haith, Adrian and Krakauer, John},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	doi = {10.1007/978-1-4614-5465-6_1},
	isbn = {978-1-4614-5464-9},
	journal = {Advances in experimental medicine and biology},
	month = {01},
	pages = {1-21},
	title = {Model-Based and Model-Free Mechanisms of Human Motor Learning},
	volume = {782},
	year = {2013}}

@article{EcoKurLub15,
	author = {Economides, Marcos and Kurth-Nelson, Zeb and L{\"u}bbert, Annika and Guitart-Masip, Marc and Dolan, Raymond},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	doi = {10.1371/journal.pcbi.1004463},
	journal = {PLoS computational biology},
	month = {09},
	pages = {e1004463},
	title = {Model-Based Reasoning in Humans Becomes Automatic with Training},
	volume = {11},
	year = {2015}}

@inproceedings{AshKreWei19,
	address = {Cham},
	author = {Ashok, Pranav and K{\v{r}}et{\'\i}nsk{\'y}, Jan and Weininger, Maximilian},
	booktitle = {Computer Aided Verification},
	editor = {Dillig, Isil and Tasiran, Serdar},
	isbn = {978-3-030-25540-4},
	pages = {497--519},
	publisher = {Springer International Publishing},
	title = {PAC Statistical Model Checking for Markov Decision Processes and Stochastic Games},
	year = {2019},
	abstract = {Statistical model checking (SMC) is a technique for analysis of probabilistic systems that may be (partially) unknown. We present an SMC algorithm for (unbounded) reachability yielding probably approximately correct (PAC) guarantees on the results. We consider both the setting (i) with no knowledge of the transition function (with the only quantity required a bound on the minimum transition probability) and (ii) with knowledge of the topology of the underlying graph. On the one hand, it is the first algorithm for stochastic games. On the other hand, it is the first practical algorithm even for Markov decision processes. Compared to previous approaches where PAC guarantees require running times longer than the age of universe even for systems with a handful of states, our algorithm often yields reasonably precise results within minutes, not requiring the knowledge of mixing time.}}

@inproceedings{BlaGurNac05,
	author = {Blass, Andreas and Gurevich, Yuri and Nachmanson, Lev and Veanes, Margus},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	doi = {10.1007/11759744_3},
	isbn = {978-3-540-34454-4},
	month = {07},
	pages = {32-46},
	title = {Play to Test},
	year = {2005}}

@book{BaiKat08,
	author = {Baier, Christel and Katoen, Joost-Pieter},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:40:12 +0200},
	publisher = {The MIT Press},
	title = {Principles of Model Checking},
	year = 2008}

@inproceedings{KwiNorPar11,
	author = {Kwiatkowska, M. and Norman, G. and Parker, D.},
	booktitle = {Proc. 23rd International Conference on Computer Aided Verification (CAV'11)},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	editor = {Gopalakrishnan, G. and Qadeer, S.},
	pages = {585--591},
	publisher = {Springer},
	series = {LNCS},
	title = {{PRISM} 4.0: Verification of Probabilistic Real-time Systems},
	volume = {6806},
	year = {2011}}

@inproceedings{KwiNorSpr02,
	address = {Berlin, Heidelberg},
	author = {Kwiatkowska, Marta and Norman, Gethin and Sproston, Jeremy},
	booktitle = {Process Algebra and Probabilistic Methods: Performance Modeling and Verification},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	editor = {Hermanns, Holger and Segala, Roberto},
	isbn = {978-3-540-45605-6},
	pages = {169--187},
	publisher = {Springer Berlin Heidelberg},
	title = {Probabilistic Model Checking of the IEEE 802.11 Wireless Local Area Network Protocol},
	year = {2002},
	abstract = {The international standard IEEE 802.11 was developed recently in recognition of the increased demand for wireless local area networks. Its medium access control mechanism is described according to a variant of the Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) scheme. Although collisions cannot always be prevented, randomised exponential backoff rules are used in the retransmission scheme to minimise the likelihood of repeated collisions. More precisely, the backoff procedure involves a uniform probabilistic choice of an integer-valued delay from an interval, where the size of the interval grows exponentially with regard to the number of retransmissions of the current data packet. We model the two-way handshake mechanism of the IEEE 802.11 standard with a fixed network topology using probabilistic timed automata, a formal description mechanism in which both nondeterministic choice and probabilistic choice can be represented. From our probabilistic timed automaton model, we obtain a finite-state Markov decision process via a property-preserving discrete-time semantics. The Markov decision process is then verified using Prism, a probabilistic model checking tool, against probabilistic, timed properties such as ``at most 5,000 microseconds pass before a station sends its packet correctly.''}}

@inproceedings{WatDay92,
	author = {Watkins, Christopher J. C. H. and Dayan, Peter},
	booktitle = {Machine Learning},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	pages = {279--292},
	title = {Q-learning},
	year = {1992}}

@article{KaeLitMoo96,
	address = {El Segundo, CA, USA},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
	issn = {1076-9757},
	issue_date = {Jnauary 1996},
	journal = {J. Artif. Int. Res.},
	month = {may},
	number = {1},
	numpages = {49},
	pages = {237--285},
	publisher = {AI Access Foundation},
	title = {Reinforcement Learning: A Survey},
	volume = {4},
	year = {1996},
	abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.}}

@book{SutBar18,
	added-at = {2019-07-13T10:11:53.000+0200},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	edition = {Second},
	interhash = {ac6b144aaec1819919a2fba9f705c852},
	intrahash = {f46601cf8b13d39d1378af0d79438b12},
	publisher = {The MIT Press},
	timestamp = {2019-07-13T10:11:53.000+0200},
	title = {Reinforcement Learning: An Introduction},
	year = {2018}}

@article{WhiLin95,
	address = {GBR},
	author = {Whitehead, Steven D. and Lin, Long-Ji},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	doi = {10.1016/0004-3702(94)00012-P},
	issn = {0004-3702},
	issue_date = {Feb. 1995},
	journal = {Artif. Intell.},
	month = feb,
	number = {1--2},
	numpages = {36},
	pages = {271--306},
	publisher = {Elsevier Science Publishers Ltd.},
	title = {Reinforcement Learning of Non-Markov Decision Processes},
	volume = {73},
	year = {1995}}

@inproceedings{StoVaa99,
	address = {Berlin, Heidelberg},
	author = {Stoelinga, Mari{\"e}lle and Vaandrager, Frits},
	booktitle = {Formal Methods for Real-Time and Probabilistic Systems},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	editor = {Katoen, Joost-Pieter},
	isbn = {978-3-540-48778-4},
	pages = {53--74},
	publisher = {Springer Berlin Heidelberg},
	title = {Root Contention in IEEE 1394},
	year = {1999},
	abstract = {The model of probabilistic I/O automata of Segala and Lynch is used for the formal specification and analysis of the root contention protocol from the physical layer of the IEEE 1394 (``FireWire'') standard. In our model of the protocol both randomization and real-time play an essential role. In order to make our verification easier to understand we introduce several intermediate automata in between the implementation and the specification automaton. This allows us to use very simple notions of refinement rather than the more general but also very complex simulation relations which have been proposed by Segala and Lynch.}}

@misc{JanKonJun19,
	archiveprefix = {arXiv},
	author = {Jansen, Nils and K{\"o}nighofer, Bettina and Junges, Sebastian and Serban, Alexandru C. and Bloem, Roderick},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	eprint = {1807.06096},
	primaryclass = {cs.AI},
	title = {Safe Reinforcement Learning via Probabilistic Shields},
	year = {2019}}

@inproceedings{LegDelBen10,
	address = {Berlin, Heidelberg},
	author = {Legay, Axel and Delahaye, Beno{\^\i}t and Bensalem, Saddek},
	booktitle = {Runtime Verification},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	editor = {Barringer, Howard and Falcone, Ylies and Finkbeiner, Bernd and Havelund, Klaus and Lee, Insup and Pace, Gordon and Ro{\c{s}}u, Grigore and Sokolsky, Oleg and Tillmann, Nikolai},
	isbn = {978-3-642-16612-9},
	pages = {122--135},
	publisher = {Springer Berlin Heidelberg},
	title = {Statistical Model Checking: An Overview},
	year = {2010},
	abstract = {Quantitative properties of stochastic systems are usually specified in logics that allow one to compare the measure of executions satisfying certain temporal properties with thresholds. The model checking problem for stochastic systems with respect to such logics is typically solved by a numerical approach [31,8,35,22,21,5] that iteratively computes (or approximates) the exact measure of paths satisfying relevant subformulas; the algorithms themselves depend on the class of systems being analyzed as well as the logic used for specifying the properties. Another approach to solve the model checking problem is to simulate the system for finitely many executions, and use hypothesis testing to infer whether the samples provide a statistical evidence for the satisfaction or violation of the specification. In this tutorial, we survey the statistical approach, and outline its main advantages in terms of efficiency, uniformity, and simplicity.}}

@inproceedings{HarKlaPar19,
	address = {Cham},
	author = {Hartmanns, Arnd and Klauck, Michaela and Parker, David and Quatmann, Tim and Ruijters, Enno},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	editor = {Vojnar, Tom{\'a}{\v{s}} and Zhang, Lijun},
	isbn = {978-3-030-17462-0},
	pages = {344--350},
	publisher = {Springer International Publishing},
	title = {The Quantitative Verification Benchmark Set},
	year = {2019},
	abstract = {We present an extensive collection of quantitative models to facilitate the development, comparison, and benchmarking of new verification algorithms and tools. All models have a formal semantics in terms of extensions of Markov chains, are provided in the Jani format, and are documented by a comprehensive set of metadata. The collection is highly diverse: it includes established probabilistic verification and planning benchmarks, industrial case studies, models of biological systems, dynamic fault trees, and Petri net examples, all originally specified in a variety of modelling languages. It archives detailed tool performance data for each model, enabling immediate comparisons between tools and among tool versions over time. The collection is easy to access via a client-side web application at qcomp.orgwith powerful search and visualisation features. It can be extended via a Git-based submission process, and is openly accessible according to the terms of the CC-BY license.}}

@inproceedings{Des17,
	author = {Desai, Nishant},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	title = {Uncertain Reward-Transition MDPs for Negotiable Reinforcement Learning},
	year = {2017}}

@inproceedings{ChaHen08,
	author = {Chatterjee, Krishnendu and Henzinger, Thomas A.},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	booktitle = {25 Years of Model Checking - History, Achievements, Perspectives},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	doi = {10.1007/978-3-540-69850-0\_7},
	editor = {Grumberg, Orna and Veith, Helmut},
	pages = {107--138},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	timestamp = {Tue, 14 May 2019 10:00:36 +0200},
	title = {Value Iteration},
	volume = {5000},
	year = {2008}}

@inproceedings{BraChaChm14,
	address = {Cham},
	author = {Br{\'a}zdil, Tom{\'a}{\v{s}} and Chatterjee, Krishnendu and Chmel{\'\i}k, Martin and Forejt, Vojt{\v{e}}ch and K{\v{r}}et{\'\i}nsk{\'y}, Jan and Kwiatkowska, Marta and Parker, David and Ujma, Mateusz},
	booktitle = {Automated Technology for Verification and Analysis},
	date-added = {2021-05-19 15:34:13 +0200},
	date-modified = {2021-05-19 15:35:42 +0200},
	editor = {Cassez, Franck and Raskin, Jean-Fran{\c{c}}ois},
	isbn = {978-3-319-11936-6},
	pages = {98--114},
	publisher = {Springer International Publishing},
	title = {Verification of Markov Decision Processes Using Learning Algorithms},
	year = {2014}}

@inproceedings{SenVisAgh06,
	address = {Berlin, Heidelberg},
	author = {Sen, Koushik and Viswanathan, Mahesh and Agha, Gul},
	booktitle = {Tools and Algorithms for the Construction and Analysis of Systems},
	editor = {Hermanns, Holger and Palsberg, Jens},
	isbn = {978-3-540-33057-8},
	pages = {394--410},
	publisher = {Springer Berlin Heidelberg},
	title = {Model-Checking Markov Chains in the Presence of Uncertainties},
	year = {2006},
	abstract = {We investigate the problem of model checking Interval-valued Discrete-time Markov Chains (IDTMC). IDTMCs are discrete-time finite Markov Chains for which the exact transition probabilities are not known. Instead in IDTMCs, each transition is associated with an interval in which the actual transition probability must lie. We consider two semantic interpretations for the uncertainty in the transition probabilities of an IDTMC. In the first interpretation, we think of an IDTMC as representing a (possibly uncountable) family of (classical) discrete-time Markov Chains, where each member of the family is a Markov Chain whose transition probabilities lie within the interval range given in the IDTMC. This semantic interpretation we call Uncertain Markov Chains (UMC). In the second semantics for an IDTMC, which we call Interval Markov Decision Process (IMDP), we view the uncertainty as being resolved through non-determinism. In other words, each time a state is visited, we adversarially pick a transition distribution that respects the interval constraints, and take a probabilistic step according to the chosen distribution. We show that the PCTL model checking problem for both Uncertain Markov Chain semantics and Interval Markov Decision Process semantics is decidable in PSPACE. We also prove lower bounds for these model checking problems.}}

@inproceedings{HenJoaZul12,
	author = {Henriques, David and Martins, Jo{\~a}o and Zuliani, Paolo and Platzer, Andr{\'e} and Clarke, Edmund},
	doi = {10.1109/QEST.2012.19},
	journal = {Proceedings - 2012 9th International Conference on Quantitative Evaluation of Systems, QEST 2012},
	month = {09},
	pages = {84-93},
	title = {Statistical Model Checking for Markov Decision Processes},
	year = {2012}}

@inproceedings{JhaClaLan09,
	address = {Berlin, Heidelberg},
	author = {Jha, Sumit K. and Clarke, Edmund M. and Langmead, Christopher J. and Legay, Axel and Platzer, Andr\'e and Zuliani, Paolo},
	booktitle = {Computational Methods in Systems Biology},
	editor = {Degano, Pierpaolo and Gorrieri, Roberto},
	isbn = {978-3-642-03845-7},
	pages = {218--234},
	publisher = {Springer Berlin Heidelberg},
	title = {A Bayesian Approach to Model Checking Biological Systems},
	year = {2009},
	abstract = {Recently, there has been considerable interest in the use of Model Checking for Systems Biology. Unfortunately, the state space of stochastic biological models is often too large for classical Model Checking techniques. For these models, a statistical approach to Model Checking has been shown to be an effective alternative. Extending our earlier work, we present the first algorithm for performing statistical Model Checking using Bayesian Sequential Hypothesis Testing. We show that our Bayesian approach outperforms current statistical Model Checking techniques, which rely on tests from Classical (aka Frequentist) statistics, by requiring fewer system simulations. Another advantage of our approach is the ability to incorporate prior Biological knowledge about the model being verified. We demonstrate our algorithm on a variety of models from the Systems Biology literature and show that it enables faster verification than state-of-the-art techniques, even when no prior knowledge is available.}}

@inproceedings{Gro19,
	address = {Cham},
	author = {Gros, Timo P. and Hermanns, Holger and Hoffmann, J{\"o}rg and Klauck, Michaela and Steinmetz, Marcel},
	booktitle = {Formal Techniques for Distributed Objects, Components, and Systems},
	editor = {Gotsman, Alexey and Sokolova, Ana},
	isbn = {978-3-030-50086-3},
	pages = {96--114},
	publisher = {Springer International Publishing},
	title = {Deep Statistical Model Checking},
	year = {2020},
	abstract = {Neural networks (NN) are taking over ever more decisions thus far taken by humans, even though verifiable system-level guarantees are far out of reach. Neither is the verification technology available, nor is it even understood what a formal, meaningful, extensible, and scalable testbed might look like for such a technology. The present paper is a modest attempt to improve on both the above aspects. We present a family of formal models that contain basic features of automated decision making contexts and which can be extended with further orthogonal features, ultimately encompassing the scope of autonomous driving. Due to the possibility to model random noise in the decision actuation, each model instance induces a Markov decision process (MDP) as verification object. The NN in this context has the duty to actuate (near-optimal) decisions. From the verification perspective, the externally learnt NN serves as a determinizer of the MDP, the result being a Markov chain which as such is amenable to statistical model checking. The combination of a MDP and a NN encoding the action policy is central to what we call ``deep statistical model checking'' (DSMC). While being a straightforward extension of statistical model checking, it enables to gain deep insight into questions like ``how high is the NN-induced safety risk?'', ``how good is the NN compared to the optimal policy?'' (obtained by model checking the MDP), or ``does further training improve the NN?''. We report on an implementation of DSMC inside The Modest Toolset in combination with externally learnt NNs, demonstrating the potential of DSMC on various instances of the model family.}}

@article{Givan00,
	author = {Givan, Robert and Leach, Sonia and Dean, Thomas},
	doi = {https://doi.org/10.1016/S0004-3702(00)00047-3},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	keywords = {Decision-theoretic planning, Planning under uncertainty, Approximate planning, Markov decision processes},
	number = {1},
	pages = {71-109},
	title = {Bounded-parameter Markov decision processes},
	volume = {122},
	year = {2000},
	abstract = {In this paper, we introduce the notion of a bounded-parameter Markov decision process (BMDP) as a generalization of the familiar exact MDP. A bounded-parameter MDP is a set of exact MDPs specified by giving upper and lower bounds on transition probabilities and rewards (all the MDPs in the set share the same state and action space). BMDPs form an efficiently solvable special case of the already known class of MDPs with imprecise parameters (MDPIPs). Bounded-parameter MDPs can be used to represent variation or uncertainty concerning the parameters of sequential decision problems in cases where no prior probabilities on the parameter values are available. Bounded-parameter MDPs can also be used in aggregation schemes to represent the variation in the transition probabilities for different base states aggregated together in the same aggregate state. We introduce interval value functions as a natural extension of traditional value functions. An interval value function assigns a closed real interval to each state, representing the assertion that the value of that state falls within that interval. An interval value function can be used to bound the performance of a policy over the set of exact MDPs associated with a given bounded-parameter MDP. We describe an iterative dynamic programming algorithm called interval policy evaluation that computes an interval value function for a given BMDP and specified policy. Interval policy evaluation on a policy π computes the most restrictive interval value function that is sound, i.e., that bounds the value function for π in every exact MDP in the set defined by the bounded-parameter MDP. We define optimistic and pessimistic criteria for optimality, and provide a variant of value iteration (Bellman, 1957) that we call interval value iteration that computes policies for a BMDP that are optimal with respect to these criteria. We show that each algorithm we present converges to the desired values in a polynomial number of iterations given a fixed discount factor.}}

@article{HanMil13,
	address = {New York, NY, USA},
	articleno = {1},
	author = {Hansen, Thomas Dueholm and Miltersen, Peter Bro and Zwick, Uri},
	doi = {10.1145/2432622.2432623},
	issn = {0004-5411},
	issue_date = {February 2013},
	journal = {J. ACM},
	keywords = {policy iteration, turn-based stochastic games, strategy iteration, strongly polynomial algorithms, Markov decision processes},
	month = feb,
	number = {1},
	numpages = {16},
	publisher = {Association for Computing Machinery},
	title = {Strategy Iteration Is Strongly Polynomial for 2-Player Turn-Based Stochastic Games with a Constant Discount Factor},
	volume = {60},
	year = {2013},
	abstract = {Ye [2011] showed recently that the simplex method with Dantzig's pivoting rule, as
well as Howard's policy iteration algorithm, solve discounted Markov decision processes
(MDPs), with a constant discount factor, in strongly polynomial time. More precisely,
Ye showed that both algorithms terminate after at most O(mn1−γ log n1−γ) iterations,
where n is the number of states, m is the total number of actions in the MDP, and
0 &lt; γ &lt; 1 is the discount factor. We improve Ye's analysis in two respects. First,
we improve the bound given by Ye and show that Howard's policy iteration algorithm
actually terminates after at most O(m1−γ log n1−γ) iterations. Second, and more importantly,
we show that the same bound applies to the number of iterations performed by the strategy
iteration (or strategy improvement) algorithm, a generalization of Howard's policy
iteration algorithm used for solving 2-player turn-based stochastic games with discounted
zero-sum rewards. This provides the first strongly polynomial algorithm for solving
these games, solving a long standing open problem. Combined with other recent results,
this provides a complete characterization of the complexity the standard strategy
iteration algorithm for 2-player turn-based stochastic games; it is strongly polynomial
for a fixed discount factor, and exponential otherwise.}}

@article{HadMon18,
	author = {Haddad, Serge and Monmege, Benjamin},
	doi = {10.1016/j.tcs.2016.12.003},
	hal_id = {hal-01809094},
	hal_version = {v1},
	journal = {{Theoretical Computer Science}},
	keywords = {stochastic verification ; value iteration ; Markov decision processes},
	month = Jul,
	pages = {111 - 131},
	pdf = {https://hal.archives-ouvertes.fr/hal-01809094/file/tcs-version.pdf},
	publisher = {{Elsevier}},
	title = {{Interval Iteration Algorithm for MDPs and IMDPs}},
	volume = {735},
	year = {2018}}

@article{Ye10,
	author = {Ye, Yinyu},
	month = {05},
	title = {The simplex method is strongly polynomial for the Markov decision problem with a fixed discount rate},
	year = {2010}}

@inproceedings{PostYe13,
	author = {Post, Ian and Ye, Y.},
	booktitle = {SODA},
	title = {The simplex method is strongly polynomial for deterministic Markov decision processes},
	year = {2013}}

@inproceedings{Lit95,
	address = {San Francisco, CA, USA},
	author = {Littman, Michael L. and Dean, Thomas L. and Kaelbling, Leslie Pack},
	booktitle = {Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
	isbn = {1558603859},
	location = {Montr\'{e}al, Qu\'{e}, Canada},
	numpages = {9},
	pages = {394--402},
	publisher = {Morgan Kaufmann Publishers Inc.},
	series = {UAI'95},
	title = {On the Complexity of Solving Markov Decision Problems},
	year = {1995},
	abstract = {Markov decision problems (MDPs) provide the foundations for a number of problems of
interest to AI researchers studying automated planning and reinforcement learning.
In this paper, we summarize results regarding the complexity of solving MDPs and the
running time of MDP solution algorithms. We argue that, although MDPs can be solved
efficiently in theory, more study is needed to reveal practical algorithms for solving
large problems quickly. To encourage future research, we sketch some alternative methods
of analysis that rely on the structure of MDPs.}}

@inproceedings{Fea10,
	address = {Berlin, Heidelberg},
	author = {Fearnley, John},
	booktitle = {Automata, Languages and Programming},
	editor = {Abramsky, Samson and Gavoille, Cyril and Kirchner, Claude and Meyer auf der Heide, Friedhelm and Spirakis, Paul G.},
	isbn = {978-3-642-14162-1},
	pages = {551--562},
	publisher = {Springer Berlin Heidelberg},
	title = {Exponential Lower Bounds for Policy Iteration},
	year = {2010},
	abstract = {We study policy iteration for infinite-horizon Markov decision processes. It has recently been shown policy iteration style algorithms have exponential lower bounds in a two player game setting. We extend these lower bounds to Markov decision processes with the total reward and average-reward optimality criteria.}}

@book{KalLN20,
	author = {Kallenberg, Lodewijk},
	month = {02},
	title = {Lecture Notes Markov Decision Problems - version 2020},
	year = {2020}}

@article{Wu08,
	author = {Wu, Di and Koutsoukos, Xenofon},
	doi = {https://doi.org/10.1016/j.artint.2007.12.002},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	keywords = {Reachability analysis, Uncertain systems, Markov decision processes},
	number = {8},
	pages = {945-954},
	title = {Reachability analysis of uncertain systems using bounded-parameter Markov decision processes},
	volume = {172},
	year = {2008},
	abstract = {Verification of reachability properties for probabilistic systems is usually based on variants of Markov processes. Current methods assume an exact model of the dynamic behavior and are not suitable for realistic systems that operate in the presence of uncertainty and variability. This research note extends existing methods for Bounded-parameter Markov Decision Processes (BMDPs) to solve the reachability problem. BMDPs are a generalization of MDPs that allows modeling uncertainty. Our results show that interval value iteration converges in the case of an undiscounted reward criterion that is required to formulate the problems of maximizing the probability of reaching a set of desirable states or minimizing the probability of reaching an unsafe set. Analysis of the computational complexity is also presented.}}

@book{Kae93,
	author = {Kaelbling, Leslie Pack},
	doi = {10.7551/mitpress/4168.001.0001},
	isbn = {9780262288507},
	month = {05},
	publisher = {The MIT Press},
	title = {{Learning in Embedded Systems}},
	year = {1993},
	abstract = {{It is the first detailed exploration of the problem of learning action strategies in the context of designing embedded systems that adapt their behavior to a complex, changing environment; such systems include mobile robots, factory process controllers, and long-term software databases.Learning to perform complex action strategies is an important problem in the fields of artificial intelligence, robotics, and machine learning. Filled with interesting new experimental results, Learning in Embedded Systems explores algorithms that learn efficiently from trial-and error experience with an external world. It is the first detailed exploration of the problem of learning action strategies in the context of designing embedded systems that adapt their behavior to a complex, changing environment; such systems include mobile robots, factory process controllers, and long-term software databases.Kaelbling investigates a rapidly expanding branch of machine learning known as reinforcement learning, including the important problems of controlled exploration of the environment, learning in highly complex environments, and learning from delayed reward. She reviews past work in this area and presents a number of significant new results. These include the intervalestimation algorithm for exploration, the use of biases to make learning more efficient in complex environments, a generate-and-test algorithm that combines symbolic and statistical processing into a flexible learning method, and some of the first reinforcement-learning experiments with a real robot.}}}

@inproceedings{Wie98,
	author = {Wiering, Marco and Schmidhuber, J{\"u}rgen},
	booktitle = {Proceedings of the sixth intercational conference on Simulation of Adaptive Behaviour: From Animals to Animats 6},
	pages = {223--228},
	publisher = {MIT Press/Bradford Books},
	title = {Efficient Model-Based Exploration},
	year = {1998}}

@article{Wie99,
	author = {Wiering, Marco},
	month = {01},
	title = {Explorations in Efficient Reinforcement Learning},
	year = {1999}}

@article{Ish02,
	author = {Ishii, Shin and Yoshida, Wako and Yoshimoto, Junichiro},
	doi = {https://doi.org/10.1016/S0893-6080(02)00056-4},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Reinforcement learning, Exploitation--exploration problem, Neuromodulator, Attention, Partially observable Markov decision process},
	number = {4},
	pages = {665-687},
	title = {Control of exploitation--exploration meta-parameter in reinforcement learning},
	volume = {15},
	year = {2002},
	abstract = {In reinforcement learning (RL), the duality between exploitation and exploration has long been an important issue. This paper presents a new method that controls the balance between exploitation and exploration. Our learning scheme is based on model-based RL, in which the Bayes inference with forgetting effect estimates the state-transition probability of the environment. The balance parameter, which corresponds to the randomness in action selection, is controlled based on variation of action results and perception of environmental change. When applied to maze tasks, our method successfully obtains good controls by adapting to environmental changes. Recently, Usher et al. [Science 283 (1999) 549] has suggested that noradrenergic neurons in the locus coeruleus may control the exploitation--exploration balance in a real brain and that the balance may correspond to the level of animal's selective attention. According to this scenario, we also discuss a possible implementation in the brain.}}

@article{Sut91,
	address = {New York, NY, USA},
	author = {Sutton, Richard S.},
	doi = {10.1145/122344.122377},
	issn = {0163-5719},
	issue_date = {Aug. 1991},
	journal = {SIGART Bull.},
	month = jul,
	number = {4},
	numpages = {4},
	pages = {160--163},
	publisher = {Association for Computing Machinery},
	title = {Dyna, an Integrated Architecture for Learning, Planning, and Reacting},
	volume = {2},
	year = {1991},
	abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution.
Learning methods are used in Dyna both for compiling planning results and for updating
a model of the effects of the agent's actions on the world. Planning is incremental
and can use the probabilistic and ofttimes incorrect world models generated by learning
processes. Execution is fully reactive in the sense that no planning intervenes between
perception and action. Dyna relies on machine learning methods for learning from examples---these
are among the basic building blocks making up the architecture---yet is not tied to
any particular method. This paper briefly introduces Dyna and discusses its strengths
and weaknesses with respect to other architectures.}}

@inproceedings{ThrMol92,
	author = {Thrun, Sebastian B. and M\"{o}ller, Knut},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Moody, J. and Hanson, S. and Lippmann, R. P.},
	publisher = {Morgan-Kaufmann},
	title = {Active Exploration in Dynamic Environments},
	volume = {4},
	year = {1992}}

@misc{Amin21,
	archiveprefix = {arXiv},
	author = {Amin, Susan and Gomrokchi, Maziar and Satija, Harsh and van Hoof, Herke and Precup, Doina},
	eprint = {2109.00157},
	primaryclass = {cs.LG},
	title = {A Survey of Exploration Methods in Reinforcement Learning},
	year = {2021}}

@article{Strehl08,
	author = {Strehl, Alexander and Littman, Michael},
	doi = {10.1016/j.jcss.2007.08.009},
	journal = {Journal of Computer and System Sciences},
	month = {12},
	pages = {1309-1331},
	title = {An Analysis of Model-Based Interval Estimation for Markov Decision Processes},
	volume = {74},
	year = {2008}}

@inproceedings{Strehl04,
	author = {Strehl, Alexander and Littman, Michael},
	doi = {10.1109/ICTAI.2004.28},
	isbn = {0-7695-2236-X},
	month = {12},
	pages = {128- 135},
	title = {An empirical evaluation of interval estimation for Markov decision processes},
	year = {2004}}

@article{Rmax,
	author = {Brafman, Ronen I. and Tennenholtz, Moshe},
	doi = {10.1162/153244303765208377},
	issn = {1532-4435},
	issue_date = {3/1/2003},
	journal = {J. Mach. Learn. Res.},
	keywords = {Markov decision processes, provably efficient learning, learning in games, stochastic games, reinforcement learning},
	month = mar,
	number = {null},
	numpages = {19},
	pages = {213--231},
	publisher = {JMLR.org},
	title = {R-Max - a General Polynomial Time Algorithm for near-Optimal Reinforcement Learning},
	volume = {3},
	year = {2003},
	abstract = {R-MAX is a very simple model-based reinforcement learning algorithm which can attain
near-optimal average reward in polynomial time. In R-MAX, the agent always maintains
a complete, but possibly inaccurate model of its environment and acts based on the
optimal policy derived from this model. The model is initialized in an optimistic
fashion: all actions in all states return the maximal possible reward (hence the name).
During execution, it is updated based on the agent's observations. R-MAX improves
upon several previous algorithms: (1) It is simpler and more general than Kearns and
Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism
for resolving the exploration vs. exploitation dilemma. (3) It formally justifies
the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler,
more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for
learning in single controller stochastic games. (5) It generalizes the algorithm by
Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm
for learning in repeated games, to date, which is provably efficient, considerably
improving and simplifying previous algorithms by Banos and by Megiddo.}}

@article{Laz20,
	author = {Lazaridis, Aristotelis and Fachantidis, A. and Vlahavas, I.},
	journal = {J. Artif. Intell. Res.},
	pages = {1421-1471},
	title = {Deep Reinforcement Learning: A State-of-the-Art Walkthrough},
	volume = {69},
	year = {2020}}

@article{KeaSin02,
	author = {Kearns, Michael and Singh, Satinder},
	doi = {10.1023/A:1017984413808},
	journal = {Machine Learning},
	month = {01},
	title = {Near-Optimal Reinforcement Learning in Polynomial Time},
	volume = {49},
	year = {2002}}

@inproceedings{BayesianRL,
	address = {New York, NY, USA},
	author = {Poupart, Pascal and Vlassis, Nikos and Hoey, Jesse and Regan, Kevin},
	booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	doi = {10.1145/1143844.1143932},
	isbn = {1595933832},
	location = {Pittsburgh, Pennsylvania, USA},
	numpages = {8},
	pages = {697--704},
	publisher = {Association for Computing Machinery},
	series = {ICML '06},
	title = {An Analytic Solution to Discrete Bayesian Reinforcement Learning},
	year = {2006},
	abstract = {Reinforcement learning (RL) was originally proposed as a framework to allow agents
to learn in an online fashion as they interact with their environment. Existing RL
algorithms come short of achieving this goal because the amount of exploration required
is often too costly and/or too time consuming for online learning. As a result, RL
is mostly used for offline learning in simulated environments. We propose a new algorithm,
called BEETLE, for effective online learning that is computationally efficient while
minimizing the amount of exploration. We take a Bayesian model-based approach, framing
RL as a partially observable Markov decision process. Our two main contributions are
the analytical derivation that the optimal value function is the upper envelope of
a set of multivariate polynomials, and an efficient point-based value iteration algorithm
that exploits this simple parameterization.}}

@inproceedings{BayesBonus,
	address = {New York, NY, USA},
	author = {Kolter, J. Zico and Ng, Andrew Y.},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	doi = {10.1145/1553374.1553441},
	isbn = {9781605585161},
	location = {Montreal, Quebec, Canada},
	numpages = {8},
	pages = {513--520},
	publisher = {Association for Computing Machinery},
	series = {ICML '09},
	title = {Near-Bayesian Exploration in Polynomial Time},
	year = {2009},
	abstract = {We consider the exploration/exploitation problem in reinforcement learning (RL). The
Bayesian approach to model-based RL offers an elegant solution to this problem, by
considering a distribution over possible models and acting to maximize expected reward;
unfortunately, the Bayesian solution is intractable for all but very restricted cases.
In this paper we present a simple algorithm, and prove that with high probability
it is able to perform ε-close to the true (intractable) optimal Bayesian policy after
some small (polynomial in quantities describing the system) number of time steps.
The algorithm and analysis are motivated by the so-called PAC-MDP approach, and extend
such results into the setting of Bayesian RL. In this setting, we show that we can
achieve lower sample complexity bounds than existing algorithms, while using an exploration
strategy that is much greedier than the (extremely cautious) exploration of PAC-MDP
algorithms.}}

@inproceedings{Lop12,
	author = {Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Pierre-yves},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	publisher = {Curran Associates, Inc.},
	title = {Exploration in Model-based Reinforcement Learning by Empirically Estimating Learning Progress},
	volume = {25},
	year = {2012}}

@article{Hoeffding,
	author = {Hoeffding, Wassily},
	issn = {01621459},
	journal = {Journal of the American Statistical Association},
	number = {301},
	pages = {13--30},
	publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	title = {Probability Inequalities for Sums of Bounded Random Variables},
	volume = {58},
	year = {1963},
	abstract = {Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for $\Pr \{ S - ES \geq nt \}$ depend only on the endpoints of the ranges of the summands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population.}}

@inproceedings{BeiKleLeu17,
	author = {Baier, C. and Klein, J. and Leuschner, L. and Parker, D. and Wunderlich, S.},
	booktitle = {Proc. 28th International Conference on Computer Aided Verification (CAV'17)},
	pages = {160--180},
	publisher = {Springer},
	series = {LNCS},
	title = {Ensuring the Reliability of Your Model Checker: Interval Iteration for Markov Decision Processes},
	volume = {10426},
	year = {2017}}

@article{Auer04,
	author = {Auer, Peter and Cesa-Bianchi, Nicol{\`o} and Fischer, Paul},
	journal = {Machine Learning},
	pages = {235-256},
	title = {Finite-time Analysis of the Multiarmed Bandit Problem},
	volume = {47},
	year = {2004}}

@article{Schwoebel21,
	article-number = {{102472}},
	author = {Schwoebel, Sarah and Markovic, Dimitrije and Smolka, Michael N. and Kiebel, Stefan J.},
	doi = {{10.1016/j.jmp.2020.102472}},
	eissn = {{1096-0880}},
	issn = {{0022-2496}},
	journal = {{Journal of Mathematical Psychology}},
	month = {{FEB}},
	orcid-numbers = {{Kiebel, Stefan J/0000-0002-5052-1117 Smolka, Michael/0000-0001-5398-5569 Schwobel, Sarah/0000-0001-5232-5729}},
	researcherid-numbers = {{Kiebel, Stefan J/B-1551-2009 Smolka, Michael/B-4865-2011 }},
	title = {{Balancing control: A Bayesian interpretation of habitual and goal-directed behavior}},
	unique-id = {{WOS:000608765300009}},
	volume = {{100}},
	year = {{2021}}}

@inbook{Legay2019,
	address = {Cham},
	author = {Legay, Axel and Lukina, Anna and Traonouez, Louis Marie and Yang, Junxing and Smolka, Scott A. and Grosu, Radu},
	booktitle = {Computing and Software Science: State of the Art and Perspectives},
	editor = {Steffen, Bernhard and Woeginger, Gerhard},
	pages = {478--504},
	publisher = {Springer International Publishing},
	title = {Statistical Model Checking},
	year = {2019}}

@inproceedings{ChaSenHen08,
	address = {Berlin, Heidelberg},
	author = {Chatterjee, Krishnendu and Sen, Koushik and Henzinger, Thomas A.},
	isbn = {3540784977},
	location = {Budapest, Hungary},
	numpages = {16},
	pages = {302--317},
	publisher = {Springer-Verlag},
	series = {FOSSACS'08/ETAPS'08},
	title = {Model-Checking omega-Regular Properties of Interval Markov Chains},
	year = {2008}}

@inproceedings{YouClaZul10,
	author = {Younes, H{\aa}kan L. S. and Clarke, Edmund M. and Zuliani, Paolo},
	booktitle = {SBMF},
	title = {Statistical Verification of Probabilistic Properties with Unbounded Until},
	year = {2010}}

@inproceedings{HeJenBas10,
	author = {He, Ru and Jennings, Paul and Basu, Samik and Ghosh, Arka and Wu, Huaiqing},
	month = {01},
	pages = {225-234},
	title = {A bounded statistical approach for model checking of unbounded until properties.},
	year = {2010}}

@misc{DacHenKre16,
	archiveprefix = {arXiv},
	author = {Daca, Przemys{\l}aw and Henzinger, Thomas A. and K{\v r}et{\'\i}nsk{\'y}, Jan and Petrov, Tatjana},
	eprint = {1504.05739},
	primaryclass = {cs.LO},
	title = {Faster Statistical Model Checking for Unbounded Temporal Properties},
	year = {2016}}

@inproceedings{SenVisAgh05,
	address = {Berlin, Heidelberg},
	author = {Sen, Koushik and Viswanathan, Mahesh and Agha, Gul},
	booktitle = {Computer Aided Verification},
	editor = {Etessami, Kousha and Rajamani, Sriram K.},
	isbn = {978-3-540-31686-2},
	pages = {266--280},
	publisher = {Springer Berlin Heidelberg},
	title = {On Statistical Model Checking of Stochastic Systems},
	year = {2005},
	abstract = {Statistical methods to model check stochastic systems have been, thus far, developed only for a sublogic of continuous stochastic logic (CSL) that does not have steady state operator and unbounded until formulas. In this paper, we present a statistical model checking algorithm that also verifies CSL formulas with unbounded untils. The algorithm is based on Monte Carlo simulation of the model and hypothesis testing of the samples, as opposed to sequential hypothesis testing. We have implemented the algorithm in a tool called VESTA, and found it to be effective in verifying several examples.}}

@article{LasPey08,
	author = {Lassaigne, Richard and Peyronnet, Sylvain},
	doi = {10.1016/j.entcs.2005.05.031},
	journal = {Electronic Notes in Theoretical Computer Science},
	month = {03},
	pages = {101-114},
	title = {Probabilistic Verification and Approximation},
	volume = {143},
	year = {2008}}

@inproceedings{Younes05,
	address = {Berlin, Heidelberg},
	author = {Younes, H{\aa}kan L. S.},
	booktitle = {Computer Aided Verification},
	editor = {Etessami, Kousha and Rajamani, Sriram K.},
	isbn = {978-3-540-31686-2},
	pages = {253--265},
	publisher = {Springer Berlin Heidelberg},
	title = {Probabilistic Verification for ``Black-Box'' Systems},
	year = {2005},
	abstract = {We explore the concept of a ``black-box'' stochastic system, and propose an algorithm for verifying probabilistic properties of such systems based on very weak assumptions regarding system dynamics. Properties are expressed as formulae in a probabilistic temporal logic. Our presentation is a generalization of and an improvement over recent work by Sen et al. on probabilistic verification for ``black-box'' systems.}}

@inproceedings{TerBohLit07,
	address = {Rochester, New York},
	author = {Tetreault, Joel and Bohus, Dan and Litman, Diane},
	booktitle = {Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference},
	month = apr,
	pages = {276--283},
	publisher = {Association for Computational Linguistics},
	title = {Estimating the Reliability of {MDP} Policies: a Confidence Interval Approach},
	year = {2007}}

@article{FuTop14,
	author = {Fu, Jie and Topcu, Ufuk},
	doi = {10.15607/RSS.2014.X.039},
	month = {04},
	title = {Probably Approximately Correct MDP Learning and Control With Temporal Logic Constraints},
	year = {2014}}

@article{DrummNiv20,
	author = {Drummond, Nicole and Niv, Yael},
	eissn = {{1879-0445}},
	issn = {{0960-9822}},
	journal = {{CURRENT BIOLOGY}},
	month = {{AUG 3}},
	number = {{15}},
	pages = {{R860-R865}},
	title = {{Model-based decision making and model-free learning}},
	unique-id = {{WOS:000555595700006}},
	volume = {{30}},
	year = {{2020}}}

@article{MelBarWoo21,
	article-number = {{760841}},
	author = {Melnikoff, David E. and Bargh, John A. and Wood, Wendy},
	doi = {{10.3389/fpsyg.2021.760841}},
	issn = {{1664-1078}},
	journal = {{FRONTIERS IN PSYCHOLOGY}},
	month = {{OCT 21}},
	title = {{Editorial: On the Nature and Scope of Habits and Model-Free Control}},
	unique-id = {{WOS:000721501300001}},
	volume = {{12}},
	year = {{2021}}}

@article{BO2011,
	author = {B{\"a}uerle, Nicole and Ott, Jonathan},
	doi = {10.1007/s00186-011-0367-0},
	file = {/Users/paddy/Zotero/storage/3E68WUDH/B{\"a}uerle and Ott - 2011 - Markov Decision Processes with Average-Value-at-Ri.pdf},
	issn = {1432-5217},
	journal = {Mathematical Methods of Operations Research},
	keywords = {90C40,91B06,Average-Value-at-Risk,Markov Decision Problem,Risk aversion,Time-consistency},
	langid = {english},
	month = dec,
	number = {3},
	pages = {361--379},
	title = {Markov {{Decision Processes}} with {{Average-Value-at-Risk}} Criteria},
	volume = {74},
	year = {2011},
	abstract = {We investigate the problem of minimizing the Average-Value-at-Risk (AVaR{$\tau$}) of the discounted cost over a finite and an infinite horizon which is generated by a Markov Decision Process (MDP). We show that this problem can be reduced to an ordinary MDP with extended state space and give conditions under which an optimal policy exists. We also give a time-consistent interpretation of the AVaR{$\tau$}. At the end we consider a numerical example which is a simple repeated casino game. It is used to discuss the influence of the risk aversion parameter {$\tau$} of the AVaR{$\tau$}-criterion.}}

@article{DacHenKrePet2017,
	address = {New York, NY, USA},
	articleno = {12},
	author = {Daca, Przemys\l{}aw and Henzinger, Thomas A. and K\v{r}et\'{\i}nsk\'{y}, Jan and Petrov, Tatjana},
	doi = {10.1145/3060139},
	issn = {1529-3785},
	issue_date = {April 2017},
	journal = {ACM Trans. Comput. Logic},
	keywords = {statistical model checking, Markov chains, mean payoff, temporal logic, simulation},
	month = {may},
	number = {2},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {Faster Statistical Model Checking for Unbounded Temporal Properties},
	volume = {18},
	year = {2017},
	abstract = {We present a new algorithm for the statistical model checking of Markov chains with respect to unbounded temporal properties, including full linear temporal logic. The main idea is that we monitor each simulation run on the fly, in order to detect quickly if a bottom strongly connected component is entered with high probability, in which case the simulation run can be terminated early. As a result, our simulation runs are often much shorter than required by termination bounds that are computed a priori for a desired level of confidence on a large state space. In comparison to previous algorithms for statistical model checking our method is not only faster in many cases but also requires less information about the system, namely, only the minimum transition probability that occurs in the Markov chain. In addition, our method can be generalised to unbounded quantitative properties such as mean-payoff bounds.}}
