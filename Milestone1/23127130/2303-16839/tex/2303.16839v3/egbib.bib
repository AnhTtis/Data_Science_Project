@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@article{timesformer,
  title={Is Space-Time Attention All You Need for Video Understanding?
},
  author={Gedas Bertasius, Heng Wang, Lorenzo Torresani},
  journal=icml,
  year={2021}
}


@InProceedings{pmlr-v139-cho21a,
  title = 	 {Unifying Vision-and-Language Tasks via Text Generation},
  author =       {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1931--1942},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/cho21a/cho21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/cho21a.html},
}


@inproceedings{wei2021maskedfeature,
author = {Chen Wei and Haoqi Fan and Saining Xie and Chao-Yuan Wu and Alan Yuille and Christoph Feichtenhofer
},
title = {Masked Feature Prediction for Self-Supervised Visual Pre-Training},
booktitle = {arXiv:https://arxiv.org/abs/2112.09133},
year = 2021
}

@article{omniVL,
  title={OmniVL:One Foundation Model for Image-Language and Video-Language Tasks},
  author={Junke Wang and Dongdong Chen and Zuxuan Wu and Chong Luo and Luowei Zhou and Yucheng Zhao and Yujia Xie and Ce Liu and Yu-Gang Jiang and Lu Yuan},
  journal=neurips,
  year={2022}
}

@article{piergiovanni2022tubevit,
  title={Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning},
  author={Piergiovanni, AJ and Kuo, Weicheng and Angelova, Anelia},
  journal={CVPR},
  year={2023}
}

@article{piergiovanni2022cotok,
  title={Video Question Answering with Iterative Video-Text Co-Tokenization},
  author={Piergiovanni, AJ and Morton, Kairo and Kuo, Weicheng and Ryoo, Michael and Angelova, Anelia},
  journal=eccv,
  year={2022}
}





@article{orgtrl,
  title={Object relational
graph with teacher-recommended learning for video captioning},
  author={Ziqi Zhang and Yaya Shi and Chunfeng Yuan and Bing Li and Peijin
Wang and Weiming Hu and Zheng-Jun Zha},
  journal=cvpr,
  year={2020}
}

@article{swinbert,
  title={SwinBERT: End-to-end transformers with sparse attention for
video captioning},
  author={Kevin Lin and Linjie Li and Chung-Ching Lin and Faisal Ahmed and Zhe
Gan and Zicheng Liu and Yumao Lu and Lijuan Wang},
  journal=cvpr,
  year={2022}
}


@article{mvgpt,
  title={End-to-end generative pretraining for
multimodal video captioning},
  author={Paul Hongsuck Seo and Arsha Nagrani and Anurag Arnab and
Cordelia Schmid},
  journal=cvpr,
  year={2022}
}

@article{vid2seq,
  title={Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning},
  author={Antoine Yang and Arsha Nagrani and Paul Hongsuck Seo and Antoine Miech and Jordi Pont-Tuset and Ivan Laptev and Josef Sivic and Cordelia Schmid},
  journal=cvpr,
  year={2023}
}

@inproceedings{openbook,
  title={Open-book
video captioning with retrieve-copy-generate network},
  author={Ziqi Zhang and Zhongang Qi and Chunfeng Yuan and Ying Shan and Bing Li and Ying Deng and Weiming Hu},
  booktitle=cvpr,
  year={2021},
}
@inproceedings{chen2020uniter,
  title={UNITER: UNiversal Image-TExt Representation Learning},
  author={Yen-Chun Chen and Linjie Li and Licheng Yu and Ahmed El Kholy and Faisal Ahmed and Zhe Gan and Yu Cheng and Jingjing Liu},
  booktitle=eccv,
  year={2020},
}

@article{UniVL,
  title={UniVL: A Unified Video and Language Pre-Training Model for
Multimodal Understanding and Generation},
  author={Huaishao Luo and Lei Ji and Botian Shi and Haoyang Huang and
Nan Duan and Tianrui Li and Jason Li and Taroon Bharti and Ming Zhou},
  journal={arXiv preprint arXiv:2002.06353},
  year={2020}
}

@article{VinVL,
  title={VinVL: Revisiting Visual Representations in Vision-Language Models},
  author={Pengchuan Zhang and Xiujun Li and Xiaowei Hu and Jianwei Yang and Lei Zhang and Lijuan Wang and Yejin Choi and Jianfeng Gao},
  journal={arXiv preprint arXiv:2101.00529},
  year={2021}
}





@article{liu2018generating,
  title={Generating Wikipedia by Summarizing Long Sequences},
  author={Peter J. Liu and Mohammad Saleh and Etienne Pot and Ben Goodrich and Ryan Sepassi and Lukasz Kaiser and Noam Shazeer},
  journal=iclr,
  year={2018}
}


@article{swin,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
  journal=cvpr,
  year={2021}
}


@article{glam,
  title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  author={Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong, and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and Liam Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and Kathleen Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V Le and Yonghui Wu and Zhifeng Chen and Claire Cui},
  journal=icml,
  year={2021}
}

@article{gpt3,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah, Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter, Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal=neurips,
  year={2020}
}


@article{wang2022allinone,
  title={All in One: Exploring Unified Video-Language Pre-training},
  author={Wang, Alex Jinpeng and Ge, Yixiao and Yan, Rui and Ge Yuying and Lin, Xudong and Cai, Guanyu  and Wu, Jianping and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2203.07303},
  year={2022}
}

@inproceedings{yang2021justask,
title={Just ask: Learning to answer questions from millions of narrated videos},
author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
pages={1686--1697},
year={2021}}

@inproceedings{dynpretr,
  title={Dynamic Pretraining of Vision-Language Models},
  author={AJ Piergiovanni and Weicheng Kuo and Wei Li and Anelia Angelova},
  booktitle={First workshop on Multimodal Representation Learning, International Conference on Learning Representations (ICLR)},
   year={2023}
}

@inproceedings{videococa,
  title={VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners},
  author={Shen Yan and Tao Zhu and ZiRui Wang and Yuan Cao and Mi Zhang and Soham Ghosh and Yonghui Wu and Jiahui Yu},
  booktitle={ArXiV:2212.04979},
   year={2022}
}


@inproceedings{merlot,
  title={MERLOT: Multimodal Neural Script Knowledge Models},
  author={Rowan Zellers and Ximing Lu and Jack Hessel and Youngjae Yu and Jae Sung Park and Jize Cao and Ali Farhadi and Yejin Choi},
  booktitle=neurips,
   year={2021}
}

@inproceedings{soho,
  title={Seeing out of the box: End-to-end pre-training for vision-language representation learning},
  author={Huang, Zhicheng and Zeng, Zhaoyang and Huang, Yupan and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12976--12985},
  year={2021}
}

@article{wang2022git,
  title={GIT: A Generative Image-to-text Transformer for Vision and Language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={arXiv preprint arXiv:2205.14100},
  year={2022}
}


@inproceedings{ryoo2022tokenlearner,
    title={TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?},
    author={Michael S. Ryoo and AJ Piergiovanni and Anurag Arnab and Mostafa Dehghani and Anelia Angelova},
    booktitle={Submitted to TPAMI},
    year={2022}
}
@inproceedings{ryoo2021tokenlearner_neurips,
    title={TokenLearner: Adaptive Space-Time Tokenization for Videos},
    author={Michael S. Ryoo and AJ Piergiovanni and Anurag Arnab and Mostafa Dehghani and Anelia Angelova},
    booktitle=neurips,
    year={2021}
}



@InProceedings{He_2022_CVPR,
    author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title     = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {CVPR},
    month     = {June},
    year      = {2022},
    pages     = {16000-16009}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}

@InProceedings{Caron_2021_ICCV,
    author    = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
    title     = {Emerging Properties in Self-Supervised Vision Transformers},
    booktitle = {ICCV},
    month     = {October},
    year      = {2021},
    pages     = {9650-9660}
}

@article{li2022language,
  title={Language-driven semantic segmentation},
  author={Li, Boyi and Weinberger, Kilian Q and Belongie, Serge and Koltun, Vladlen and Ranftl, Ren{\'e}},
  journal={arXiv preprint arXiv:2201.03546},
  year={2022}
}

@inproceedings{ghiasi2022scaling,
  title={Scaling Open-Vocabulary Image Segmentation with Image-Level Labels},
  author={Ghiasi, Golnaz and Gu, Xiuye and Cui, Yin and Lin, Tsung-Yi},
  booktitle={European Conference on Computer Vision},
  pages={540--557},
  year={2022},
  organization={Springer}
}

@inproceedings{zhou2022cocoop,
    title={Conditional Prompt Learning for Vision-Language Models},
    author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
    booktitle = {CVPR},
    year={2022}
}


@inproceedings{zang2022open,
  title={Open-vocabulary detr with conditional matching},
  author={Zang, Yuhang and Li, Wei and Zhou, Kaiyang and Huang, Chen and Loy, Chen Change},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part IX},
  pages={106--122},
  year={2022},
  organization={Springer}
}



@article{kuo2022f,
  title={F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models},
  author={Kuo, Weicheng and Cui, Yin and Gu, Xiuye and Piergiovanni, AJ and Angelova, Anelia},
  journal={arXiv preprint arXiv:2209.15639},
  year={2022}
}

@article{kim2022learning,
  title={Learning Open-World Object Proposals without Learning to Classify},
  author={Kim, Dahun and Lin, Tsung-Yi and Angelova, Anelia and Kweon, In So and Kuo, Weicheng},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={2},
  pages={5453--5460},
  year={2022},
  publisher={IEEE}
}


@inproceedings{albef,
  title={Align before Fuse: Vision and Language
Representation Learning with Momentum Distillation},
  author={Junnan Li and Ramprasaath R. Selvaraju and Akhilesh D. Gotmare
and Shafiq Joty and Caiming Xiong and Steven C.H. Hoi},
  booktitle={NeurIPS},
  year={2021},
}


@inproceedings{rovit, 
  author = {Dahun Kim and Anelia Angelova and Weicheng Kuo}, 
 title = {Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers}, 
 booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2023} 
}


@inproceedings{pali, 
  author = {Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski
and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and
and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and
Gaurav Mishra and Linting Xue and Ashish Thapliyal and James Bradbury and Weicheng Kuo and 
Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos  Riquelme and
Andreas  Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
 title = {PaLI: A Jointly-Scaled Multilingual
Language-Image Model}, 
 booktitle = {arXiv preprint arXiv:2209.06794}, 
 year = {2022} 
}



@inproceedings{InternVideo, 
  author = {Yi Wang and  Kunchang Li and Yizhuo Li and Yinan He1 and  Bingkun Huang and  Zhiyu Zhao and Hongjie Zhang and Jilan Xu and Yi Liu and Zun Wang and Sen Xing and Guo Chen and Junting Pan and Jiashuo Yu and
Yali Wang and Limin Wang and Yu Qiao},
 title = {InternVideo: General Video Foundation Models
via Generative and Discriminative Learning}, 
 booktitle = {arXiv preprint arXiv:2212.03191}, 
 year = {2022} 
}



@inproceedings{mPLUG-2, 
  author = {Haiyang Xu and Qinghao Ye and Ming Yan and Yaya Shi and Jiabo Ye and Yuanhong Xu and Chenliang Li and Bin Bi and Qi Qian and Wei Wang and Guohai Xu and Ji Zhang and Songfang Huang and Fei Huang and Jingren Zhou},
 title = {mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video}, 
 booktitle = {arXiv preprint arXiv:2302.00402}, 
 year = {2023} 
}

@inproceedings{fu2023empiricalmvm, 
  author = {Tsu-Jui Fu* and Linjie Li* and Zhe Gan and Kevin Lin and William Yang Wang and Lijuan Wang and Zicheng Liu}, 
 title = {An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling}, 
 booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},  year = {2023} 
}

@inproceedings{fu2021violet, 
  author = {Tsu-Jui Fu and Linjie Li and Zhe Gan and Kevin Lin and William Yang Wang and Lijuan Wang and Zicheng Liu}, 
 title = {VIOLET: End-to-End Video-Language Transformers with Masked Visual-token Modeling}, 
 booktitle = {arXiv:2111.1268}, 
 year = {2021} 
}

@InProceedings{Xiao_2021_ICCV,
    author    = {Xiao, Tete and Reed, Colorado J and Wang, Xiaolong and Keutzer, Kurt and Darrell, Trevor},
    title     = {Region Similarity Representation Learning},
    booktitle = {ICCV},
    month     = {October},
    year      = {2021},
    pages     = {10539-10548}
}

@InProceedings{Bai_2022_CVPR,
    author    = {Bai, Yutong and Chen, Xinlei and Kirillov, Alexander and Yuille, Alan and Berg, Alexander C.},
    title     = {Point-Level Region Contrast for Object Detection Pre-Training},
    booktitle = {CVPR},
    month     = {June},
    year      = {2022},
    pages     = {16061-16070}
}


@inproceedings{
    gu2022openvocabulary,
    title={Open-vocabulary Object Detection via Vision and Language Knowledge Distillation},
    author={Xiuye Gu and Tsung-Yi Lin and Weicheng Kuo and Yin Cui},
    booktitle={ICLR},
    year={2022},
}

@InProceedings{zhong2021regionclip,
    author    = {Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and Gao, Jianfeng},
    title     = {RegionCLIP: Region-based Language-Image Pretraining},
    booktitle = {CVPR},
    year      = {2022},
}

@InProceedings{Zareian_2021_CVPR,
    author    = {Zareian, Alireza and Rosa, Kevin Dela and Hu, Derek Hao and Chang, Shih-Fu},
    title     = {Open-Vocabulary Object Detection Using Captions},
    booktitle = {CVPR},
    year      = {2021},
}

@inproceedings{radford2021clip,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      booktitle={ICML},
      year={2021},
}

@inproceedings{align,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
  booktitle={ICML},
  year={2021}
}

@inproceedings{ccdataset,
  title={Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning},
  author={Piyush Sharma and Nan Ding and Sebastian Goodman and Radu Soricut},
  booktitle={ACL},
  year={2018},
}

@inproceedings{WSDDN,
  title={Weakly supervised deep detection networks},
  author={Bilen, Hakan and Vedaldi, Andrea},
  booktitle = {CVPR},
  year={2016}
}

@inproceedings{Cap2Det,
  title={Cap2det: Learning to amplify weak caption supervision for object detection},
  author={Ye, Keren and Zhang, Mingda and Kovashka, Adriana and Li, Wei and Qin, Danfeng and Berent, Jesse},
  booktitle = {ICCV},
  year={2019}
}

@inproceedings{rahman2020improved,
  title={Improved visual-semantic alignment for zero-shot object detection},
  author={Rahman, Shafin and Khan, Salman and Barnes, Nick},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{coco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
   booktitle = {ECCV},
  year={2014}
}

@inproceedings{zhu2020don,
  title={Don't Even Look Once: Synthesizing Features for Zero-Shot Detection},
  author={Zhu, Pengkai and Wang, Hanxiao and Saligrama, Venkatesh},
  booktitle = {CVPR},
  year={2020}
}

@inproceedings{bansal2018zero,
  title={Zero-shot object detection},
  author={Bansal, Ankan and Sikka, Karan and Sharma, Gaurav and Chellappa, Rama and Divakaran, Ajay},
   booktitle = {ECCV},
  year={2018}
}

@InProceedings{objects365,
author = {Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
title = {Objects365: A Large-Scale, High-Quality Dataset for Object Detection},
booktitle = {ICCV},
year = {2019}
}

@inproceedings{ren2015faster,
  title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross B and Sun, Jian},
  booktitle={NeurIPS},
  year={2015}
}

@inproceedings{fpn,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle = {CVPR},
  year={2017}
}

@inproceedings{he2017mask,
  title={Mask R-CNN},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle = {ICCV},
  year={2017}
}


@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle = {CVPR},
  year={2009},
}

@inproceedings{zhou2022detecting,
  title={Detecting Twenty-thousand Classes using Image-level Supervision},
  author={Zhou, Xingyi and Girdhar, Rohit and Joulin, Armand and Kr{\"a}henb{\"u}hl, Philipp and Misra, Ishan},
  booktitle = {ECCV},
  year={2022}
}

@inproceedings{lvis,
author = {Gupta, Agrim and Dollar, Piotr and Girshick, Ross},
title = {LVIS: A Dataset for Large Vocabulary Instance Segmentation},
booktitle = {CVPR},
year = {2019}
}

@inproceedings{li2021grounded,
      title={Grounded Language-Image Pre-training},
      author={Liunian Harold Li and Pengchuan Zhang and Haotian Zhang and Jianwei Yang and Chunyuan Li and Yiwu Zhong and Lijuan Wang and Lu Yuan and Lei Zhang and Jenq-Neng Hwang and Kai-Wei Chang and Jianfeng Gao},
      booktitle = {CVPR},
      year={2022},
}

@inproceedings{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={ICML},
  year={2019},
}

@inproceedings{ghiasi2021simple,
  title={Simple copy-paste is a strong data augmentation method for instance segmentation},
  author={Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D and Le, Quoc V and Zoph, Barret},
  booktitle = {CVPR},
  year={2021}
}

@inproceedings{du2022learning,
  title={Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model},
  author={Du, Yu and Wei, Fangyun and Zhang, Zihe and Shi, Miaojing and Gao, Yue and Li, Guoqi},
  booktitle = {CVPR},
  year={2022}
}


@inproceedings{SoCo,
 author = {Wei, Fangyun and Gao, Yue and Wu, Zhirong and Hu, Han and Lin, Stephen},
 title = {Aligning Pretraining for Detection via Object-Level Contrastive Learning},
 booktitle={NeurIPS},
 year = {2021}
}

@article{basic,
  author    = {Hieu Pham and
               Zihang Dai and
               Golnaz Ghiasi and
               Hanxiao Liu and
               Adams Wei Yu and
               Minh{-}Thang Luong and
               Mingxing Tan and
               Quoc V. Le},
  title     = {Combined Scaling for Zero-shot Transfer Learning},
  journal   = {CoRR},
  volume    = {abs/2111.10050},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.10050},
  eprinttype = {arXiv},
  eprint    = {2111.10050},
  timestamp = {Mon, 22 Nov 2021 16:44:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-10050.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{zhai2021lit,
    author    = {Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
    title     = {LiT: Zero-Shot Transfer With Locked-Image Text Tuning},
    booktitle = {CVPR},
    year      = {2022},
}

@inproceedings{frome2013devise,
  title={DeViSE: A Deep Visual-Semantic Embedding Model},
  author={Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Ranzato, Marc'Aurelio and Mikolov, Tomas},
  booktitle={NeurIPS},
  year={2013}
}

@article{norouzi2013zero,
  title={Zero-shot learning by convex combination of semantic embeddings},
  author={Norouzi, Mohammad and Mikolov, Tomas and Bengio, Samy and Singer, Yoram and Shlens, Jonathon and Frome, Andrea and Corrado, Greg S and Dean, Jeffrey},
  booktitle={ICLR},
  year={2014}
}

@inproceedings{chen2015webly,
  title={Webly supervised learning of convolutional networks},
  author={Chen, Xinlei and Gupta, Abhinav},
  booktitle = {ICCV},
  year={2015}
}

@inproceedings{divvala2014learning,
  title={Learning everything about anything: Webly-supervised visual concept learning},
  author={Divvala, Santosh K and Farhadi, Ali and Guestrin, Carlos},
  booktitle = {CVPR},
  year={2014}
}

@inproceedings{joulin2016learning,
  title={Learning visual features from large weakly supervised data},
  author={Joulin, Armand and Maaten, Laurens van der and Jabri, Allan and Vasilache, Nicolas},
   booktitle = {ECCV},
  year={2016},
}

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={ICCV},
  pages={2980--2988},
  year={2017}
}

% Text learning.
@inproceedings{desai2021virtex,
  title={Virtex: Learning visual representations from textual annotations},
  author={Desai, Karan and Johnson, Justin},
  booktitle = {CVPR},
  year={2021}
}

@inproceedings{sariyildiz2020learning,
  title={Learning visual representations with caption annotations},
  author={Sariyildiz, Mert Bulent and Perez, Julien and Larlus, Diane},
  booktitle = {ECCV},
  year={2020},
}

@inproceedings{wang2009learning,
  title={Learning Models for Object Recognition from Natural Language Descriptions.},
  author={Wang, Josiah and Markert, Katja and Everingham, Mark and others},
  booktitle={BMVC},
  year={2009}
}

@inproceedings{zhong2021learning,
  title={Learning to generate scene graph from natural language supervision},
  author={Zhong, Yiwu and Shi, Jing and Yang, Jianwei and Xu, Chenliang and Li, Yin},
  booktitle = {ICCV},
  year={2021}
}

@inproceedings{he2017fine,
  title={Fine-grained image classification via combining vision and language},
  author={He, Xiangteng and Peng, Yuxin},
  booktitle = {CVPR},
  year={2017}
}

% Zero-shot detection.
@inproceedings{demirel2018zero,
  title={Zero-shot object detection by hybrid region embedding},
  author={Demirel, Berkan and Cinbis, Ramazan Gokberk and Ikizler-Cinbis, Nazli},
  booktitle={BMVC},
  year={2018}
}

@inproceedings{hayat2020synthesizing,
  title={Synthesizing the Unseen for Zero-shot Object Detection},
  author={Hayat, Nasir and Hayat, Munawar and Rahman, Shafin and Khan, Salman and Zamir, Syed Waqas and Khan, Fahad Shahbaz},
  booktitle={ACCV},
  year={2020}
}

@inproceedings{zheng2020background,
  title={Background Learnable Cascade for Zero-Shot Object Detection},
  author={Zheng, Ye and Huang, Ruoran and Han, Chuanqi and Huang, Xi and Cui, Li},
  booktitle={ACCV},
  year={2020}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle = {ECCV},
  year={2020},
}


@inproceedings{chen2015cococaptions,
  title={Microsoft COCO captions: Data collection and evaluation server},
  author={Xinlei Chen and Hao Fang and Tsung-Yi Lin and Ramakrishna Vedantam and Saurabh Gupta and Piotr Dollar and C. Lawrence Zitnick},
  booktitle={https://arxiv.org/abs/1504.00325},
  year={2015},
}

@inproceedings{li2022exploring,
  title={Exploring plain vision transformer backbones for object detection},
  author={Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
  booktitle = {ECCV},
  year={2022}
}

@InProceedings{minderer2022simple,
 author = {Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and  Dosovitskiy, Alexey and Mahendran,  Aravindh and  Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and Wang, Xiao and Zhai,  Xiaohua and Kipf, Thomas and Houlsby, Neil},
 title = {Simple Open-Vocabulary Object Detection with Vision Transformers},
 booktitle = {ECCV},
 year = {2022}
}

@InProceedings{zhou2022maskclip,
 author = {Zhou, Chong and Loy, Chen Change and Dai, Bo},
 title = {Extract Free Dense Labels from CLIP},
 booktitle = {ECCV},
 year = {2022}
}

@InProceedings{Henaff_2021_ICCV,
    author    = {H\'enaff, Olivier J. and Koppula, Skanda and Alayrac, Jean-Baptiste and van den Oord, Aaron and Vinyals, Oriol and Carreira, Jo\~ao},
    title     = {Efficient Visual Pretraining With Contrastive Detection},
    booktitle = {ICCV},
    month     = {October},
    year      = {2021},
    pages     = {10086-10096}
}

@InProceedings{He_2022_CVPR,
    author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title     = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {CVPR},
    month     = {June},
    year      = {2022},
    pages     = {16000-16009}
}


@InProceedings{Caron_2021_ICCV,
    author    = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
    title     = {Emerging Properties in Self-Supervised Vision Transformers},
    booktitle = {ICCV},
    month     = {October},
    year      = {2021},
    pages     = {9650-9660}
}

@article{li2022language,
  title={Language-driven semantic segmentation},
  author={Li, Boyi and Weinberger, Kilian Q and Belongie, Serge and Koltun, Vladlen and Ranftl, Ren{\'e}},
  journal={arXiv preprint arXiv:2201.03546},
  year={2022}
}

@inproceedings{ghiasi2022scaling,
  title={Scaling Open-Vocabulary Image Segmentation with Image-Level Labels},
  author={Ghiasi, Golnaz and Gu, Xiuye and Cui, Yin and Lin, Tsung-Yi},
  booktitle={European Conference on Computer Vision},
  pages={540--557},
  year={2022},
  organization={Springer}
}

@inproceedings{zhou2022cocoop,
    title={Conditional Prompt Learning for Vision-Language Models},
    author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
    booktitle = {CVPR},
    year={2022}
}

@article{rasheed2022bridging,
  title={Bridging the gap between object and image-level representations for open-vocabulary detection},
  author={Rasheed, Hanoona and Maaz, Muhammad and Khattak, Muhammad Uzair and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2207.03482},
  year={2022}
}

@inproceedings{zhao2022exploiting,
  title={Exploiting Unlabeled Data with Vision and Language Models for Object Detection},
  author={Zhao, Shiyu and Zhang, Zhixing and Schulter, Samuel and Zhao, Long and Stathopoulos, Anastasis and Chandraker, Manmohan and Metaxas, Dimitris and others},
  booktitle = {ECCV},
  year={2022}
}

@article{kuo2022fvlm,
  title={F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models},
  author={Kuo, Weicheng and Cui, Yin and Gu, Xiuye and Piergiovanni, AJ and Angelova, Anelia},
  journal={arXiv preprint arXiv:2209.15639},
  year={2022}
}

@article{kim2022learning,
  title={Learning Open-World Object Proposals without Learning to Classify},
  author={Kim, Dahun and Lin, Tsung-Yi and Angelova, Anelia and Kweon, In So and Kuo, Weicheng},
  journal={IEEE Robotics and Automation Letters},
  volume={7},
  number={2},
  pages={5453--5460},
  year={2022},
  publisher={IEEE}
}

@inproceedings{arnab2021vivit,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6836--6846},
  year={2021}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@misc{flamingo,
  doi = {10.48550/ARXIV.2204.14198},
  
  url = {https://arxiv.org/abs/2204.14198},
  
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Flamingo: a Visual Language Model for Few-Shot Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{wang2021simvlm,
  title={Simvlm: Simple visual language model pretraining with weak supervision},
  author={Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  journal={arXiv preprint arXiv:2108.10904},
  year={2021}
}

@article{zhang2022segvit,
  title={SegViT: Semantic Segmentation with Plain Vision Transformers},
  author={Zhang, Bowen and Tian, Zhi and Tang, Quan and Chu, Xiangxiang and Wei, Xiaolin and Shen, Chunhua and Liu, Yifan},
  journal={arXiv preprint arXiv:2210.05844},
  year={2022}
}


@inproceedings{agrawal2015vqa,
  title={VQA: Visual Question Answering},
  author={Aishwarya Agrawal and Jiasen Lu and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Dhruv Batra and Devi Parikh},
  booktitle=iccv,
  year={2015},
}


@article{
yu2022coca,
title={CoCa: Contrastive Captioners are Image-Text Foundation Models},
author={Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
journal={TMLR},
year={2022},
url={https://openreview.net/forum?id=Ee277P3AYC},
note={}
}

@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={ICCV},
  pages={2641--2649},
  year={2015}
}

@article{chen2015microsoft,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

@inproceedings{singh2022flava,
  title={Flava: A foundational language and vision alignment model},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={CVPR},
  pages={15638--15650},
  year={2022}
}

@inproceedings{yao2021filip,
  title={FILIP: Fine-grained Interactive Language-Image Pre-Training},
  author={Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing},
  booktitle={ICLR},
  year={2021}
}

@article{yuan2021florence,
author = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Liu, Mengchen and Liu, Zicheng and Lu, Yumao and Shi, Yu and Wang, Lijuan and Wang, Jianfeng and Xiao, Bin and Xiao, Zhen and Yang, Jianwei and Zeng, Michael and Zhou, Luowei and Zhang, Pengchuan},
title = {Florence: A New Foundation Model for Computer Vision},
year = {2021},
month = {November},
abstract = {Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.},
url = {https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/},
journal = {arXiv preprint arXiv:2111.11432},
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{chen2020simple,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.05709},
  year={2020}
} 

@article{lemon,
  title={Scaling Up Vision-Language Pre-training for Image Captioning},
  author={Xiaowei Hu and Zhe Gan and Jianfeng Wang and Zhengyuan Yang and
Zicheng Liu and Yumao Lu and Lijuan Wang},
  journal={arXiv preprint arXiv:2111.12233},
  year={2021}
}


@inproceedings{xu2017msvd-qa,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Dejing Xu and Zhou Zhao and Jun Xiao and Fei Wu and Hanwang Zhang and Xiangnan
He and Yueting Zhuang},
  booktitle={ACM Multimedia},
  year={2017},
}

@inproceedings{msvd,
  title={Collecting highly parallel data for paraphrase evaluation},
  author={David Chen and William Dolan},
  booktitle={ACL},
  year={2011},
}



@inproceedings{xu2016msrvtt,
  title={Msr-vtt: A large
video description dataset for bridging video and language},
  author={Jun Xu, Tao Mei, Ting Yao, and Yong Rui},
  booktitle=cvpr,
  year={2016},
}



@article{ScalingViTs,
  title={Scaling vision transformer},
  author={Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer},
  journal=cvpr,
  year={2022}
}


@article{VINDLU,
  title={VINDLU : A Recipe for Effective Video-and-Language Pretraining},
  author={Feng Cheng and Xizi Wang and
Jie Lei1 David Crandall and Mohit Bansal and Gedas Bertasius},
  journal={arXiv preprint arXiv:2212.05051},
  year={2022}
}

@article{UnifiedIO,
  title={Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks},
  author={Jiasen Lu and Christopher Clark and Rowan Zellers and Roozbeh Mottaghi and Aniruddha Kembhavi},
  journal={arXiv preprint arXiv:2206.08916},
  year={2022}
}


@article{refcoco,
  title={Modeling context
in referring expressions},
  author={Licheng Yu and Patrick Poirson and Shan Yang and Alexander C Berg and Tamara L Berg},
  journal=ECCV,
  year={2016}
}

@article{UnifiedVLP,
  title={Unified Vision-Language Pre-Training for Image Captioning and VQA},
  author={Luowei Zhou and Hamid Palangi and Lei Zhang and Houdong Hu and Jason J. Corso and Jianfeng Gao},
  journal=AAAI,
  year={2020}
}


@inproceedings{meter,
  title={An Empirical Study of Training End-to-End Vision-and-Language Transformers},
  author={Zi-Yi Dou and Yichong Xu and Zhe Gan and Jianfeng Wang and Shuohang Wang and Lijuan Wang and Chenguang Zhu and Pengchuan Zhang and Lu Yuan and Nanyun Peng and Zicheng Liu and Michael Zeng},
  booktitle=cvpr,
  year={2022},
}

@inproceedings{tan2019lxmert,
  title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  author={Hao Tan and Mohit Bansal},
  booktitle={EMNLP},
  year={2019},
}

@inproceedings{su2020vlbert,
  title={VL-BERT: Pre-training of Generic Visual-Linguistic Representations},
  author={Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai},
  booktitle=iclr,
  year={2020},
}

@inproceedings{li2020oscar,
  title={Oscar: Object-Semantics Aligned Pre-training
for Vision-Language Tasks},
  author={Xiujun Li and Xi Yin and Chunyuan Li and Pengchuan Zhang and Xiaowei Hu and Lei Zhang and Lijuan Wang and Houdong Hu and Li Dong and Furu Wei and Yejin Choi and Jianfeng Gao},
  booktitle=eccv,
  year={2020},
}

@inproceedings{vilbert2020,
  title={ViLBERT: Pretraining Task-Agnostic Visiolinguistic
Representations for Vision-and-Language Tasks},
  author={Jiasen Lu and
 Dhruv Batra and
 Devi Parikh and
 Stefan Lee},
  booktitle=cvpr,
  year={2019},
}

@inproceedings{wang2021seesaw,
  title={Seesaw loss for long-tailed instance segmentation},
  author={Wang, Jiaqi and Zhang, Wenwei and Zang, Yuhang and Cao, Yuhang and Pang, Jiangmiao and Gong, Tao and Chen, Kai and Liu, Ziwei and Loy, Chen Change and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9695--9704},
  year={2021}
}


@article{wang2022image,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv preprint arXiv:2208.10442},
  year={2022}
}

@article{piergiovanni2022answer,
  title={Answer-me: Multi-task open-vocabulary visual question answering},
  author={Piergiovanni, AJ and Li, Wei and Kuo, Weicheng and Saffar, Mohammad and Bertsch, Fred and Angelova, Anelia},
  journal={arXiv preprint arXiv:2205.00949},
  year={2022}
}

@article{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={NeurIPS},
  year={2021}
}

@article{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  journal={arXiv preprint arXiv:2201.12086},
  year={2022}
}

@article{wang2022unifying,
  title={Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  journal={arXiv preprint arXiv:2202.03052},
  year={2022}
}

@article{lei2022loopitr,
  title={LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval},
  author={Lei, Jie and Chen, Xinlei and Zhang, Ning and Wang, Mengjiao and Bansal, Mohit and Berg, Tamara L and Yu, Licheng},
  journal={arXiv preprint arXiv:2203.05465},
  year={2022}
}

@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={CVPR},
  pages={18995--19012},
  year={2022}
}
@article{zang2022open,
  title={Open-Vocabulary DETR with Conditional Matching},
  author={Zang, Yuhang and Li, Wei and Zhou, Kaiyang and Huang, Chen and Loy, Chen Change},
  journal={arXiv preprint arXiv:2203.11876},
  year={2022}
}
@inproceedings{feng2022promptdet,
  title={Promptdet: Towards open-vocabulary detection using uncurated images},
  author={Feng, Chengjian and Zhong, Yujie and Jie, Zequn and Chu, Xiangxiang and Ren, Haibing and Wei, Xiaolin and Xie, Weidi and Ma, Lin},
  booktitle={European Conference on Computer Vision},
  pages={701--717},
  year={2022},
  organization={Springer}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{Li2023BLIP2BL,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.12597}
}