\section{Methodology}

In this section, we first present an overview of a general masked voxel modeling framework for LiDAR point clouds and our Reversed-Furthest-Voxel-Sampling strategy to tackle the uneven distribution of LiDAR points (\cref{subsec:MVM}).
Masked Voxel Jigsaw (MVJ) (\cref{subsec:MVJ}) and Masked Voxel Reconstruction (MVR) (\cref{subsec:MVR}) are both instantiations of the general framework that learns the voxel distributions and local point distributions, respectively.
Masked Voxel Jigsaw and Reconstruction (MV-JAR) consist of these two tasks to synergically learn the data distributions at two levels (\cref{subsec:joint_pre_training}).

\subsection{Masked Voxel Modeling and Sampling}
\label{subsec:MVM}
As illustrated in \cref{fig:MV-JAR}, a general masked voxel modeling framework converts LiDAR point clouds into voxels and samples a proportion of non-empty voxels using a specific sampling strategy and masking ratio. Various features within the voxel can be masked for a particular learning target in this general framework to instantiate different self-supervised learning tasks. Specifically, task-specific features of the points in the sampled voxels are replaced with a learnable vector.
Subsequently, all non-empty voxels are encoded by a voxel encoder, processed by a backbone network to extract features, and followed by a lightweight decoder that reconstructs task-specific targets from each voxel feature. Each stage is described in the following sections.
In this paper, we employ SST~\cite{SST}, which demonstrates superior performance in 3D detection.
The pre-trained weights of the voxel encoder and the SST backbone are utilized as initialization for the downstream 3D detection task.

\noindent\textbf{Voxelization.}\quad Let $p$ denote a point with coordinates $x, y, z$. Given a point cloud $ P = \{ p_{i} = [x_i, y_i, z_i]^{T} \in \mathbb{R}^{3} \}_{i=1...n} $ containing $n$ points and range $W, H, D$, we partition the space into a 3D grid using voxel sizes $v_W, v_H, v_D$ along the three axes, grouping points in the same voxel together. As a common technique~\cite{SST, pointpillars, voxelnet}, we decorate each point with $x^c, y^c, z^c, x^v, y^v, z^v$, where the superscript $c$ denotes distance to the clustering center of all points in each voxel and the superscript $v$ denotes distance to the voxel center.
Considering the sparse and irregular nature of LiDAR point clouds, we focus only on non-empty voxels. Assuming non-empty voxels contain at most $T$ points and a total of $N$ non-empty voxels, we obtain the voxel-based point cloud representation as $M = \{V_i \}_{i=1...N}$, where $V_i = \{p_j = [x_j, y_j, z_j, x^c_j, y^c_j, z^c_j, x^v_j, y^v_j, z^v_j]^{T} \in \mathbb{R}^{9}\}_{j=1...t_i}$ and $t_i \leq T$. We assume each non-empty voxel contains $T$ points for simplicity. We also maintain each voxel's coordinate in the 3D grid with $ C_{i} \in \mathbb{R}^{3}$.

\noindent\textbf{Masked voxel sampling.}\quad
A key component of masked autoencoders is the mask sampling strategy. Unlike 2D pixels that uniformly distribute on the image plane, voxels are unevenly distributed due to the sparse and irregular nature of LiDAR point clouds. Random sampling may result in information loss in regions with sparse points. Inspired by furthest point sampling~\cite{pointnet_plus} (FPS), which samples an evenly distributed point set while maintaining the key structure of the data, we propose a Reversed-Furthest-Voxel-Sampling (R-FVS) scheme. Given a masking ratio $r$, we apply FPS to select $\lfloor N(1-r) \rfloor$ voxels from all non-empty voxels according to each voxel's coordinate $C_i$. These sampled voxels are \emph{kept} to preserve points in sparse regions and maintain key geometric structures in the data, while the remaining unsampled voxels are masked during training.

\noindent\textbf{Masking voxels.}\quad We directly mask the raw inputs of voxels, providing a unified formulation for the masked voxel modeling framework. For each of the remaining unsampled $ \lceil Nr \rceil $ voxels to be masked, we replace some of the original features of each point in the voxel with a shared learnable vector, denoted as a mask token $ m $. The dimensions of $m$ and the replaced point features vary for different tasks.

\noindent\textbf{Voxel encoding and decoding.}\quad
All the non-empty voxels are then encoded by a voxel encoder (VE) and the SST backbone. As the features of masked voxels are also extracted by the backbone, we only need a lightweight task-specific MLP head to decode and predict their original information~\cite{SimMiM}. The loss is computed only on masked voxels, following previous practices~\cite{MAE, SimMiM}.

\subsection{Masked Voxel Jigsaw (MVJ)}\label{subsec:MVJ}
Object detection in LiDAR point clouds necessitates a representation capable of capturing contextual information, such as the distribution and relationships among voxels. We propose a jigsaw puzzle that obscures position information, compelling the model to learn these relationships.

\noindent\textbf{Masking position information.}\quad
Given a position-masked voxel, we replace the absolute coordinates $x_j, y_j, z_j$ of each point with a mask token $m_v \in \mathbb{R}^{3}$, while preserving the local shape by retaining the decorated coordinates of each point within the voxel (Fig.~\ref{fig:MV-JAR}). The masked voxel is then represented as $V_{\text{masked}} = \{p_j=[m_v, x^c_j, y^c_j, z^c_j, x^v_j, y^v_j, z^v_j]^T \in \mathbb{R}^{9} \}_{j=1...T}$. All voxels are subsequently input to the voxel encoder and the SST backbone. Notably, positional embeddings are not added to the voxels before being fed to the backbone, as the model is tasked with predicting position information.

\noindent\textbf{Prediction target and loss function.}\quad
Instead of directly predicting the absolute position of the masked voxels, we formulate a classification task to facilitate optimization. Owing to the extensive range of LiDAR point clouds, the SST backbone partitions the 3D voxelized grid into windows, akin to the Swin-transformer~\cite{swin-transformer}, to enable efficient attention calculation within windows. We employ the same window partitioning method, requiring the model to predict only the relative index of the masked voxel within its corresponding window. Assuming a 3D window contains $N_x, N_y, N_z$ voxels along the $x, y, z$ axes and the voxel coordinates of a masked voxel are $X, Y, Z$, the relative coordinates $I_x, I_y, I_z$ of the masked voxel with respect to its window are calculated as $I_x = X \mod N_x, I_y = Y \mod N_y, I_z = Z \mod N_z$. The relative index is then given by $I = I_x + I_yN_x + I_zN_xN_y$. The prediction head outputs a classification vector $\hat{v} \in \mathbb{R}^{N_xN_yN_z}$ representing the probabilities of the masked voxel occupying each position. The prediction loss for all masked voxel is calculated using the Cross-Entropy loss as follows:
\begin{equation}
L_{MVJ} = \frac{1}{R_p}\sum_{i=1}^{R_p}\text{CrossEntropy}(\hat{v}_i, I_i),
\end{equation}
where $R_p$ denotes the number of position-masked voxels.

\subsection{Masked Voxel Reconstruction (MVR)}\label{subsec:MVR}
To incorporate local shape information within each voxel, we introduce masked voxel reconstruction (MVR). MVR masks the absolute and relative coordinates of all points in shape-masked voxels to learn point distributions, preserving only one point to provide the voxel position.

\noindent\textbf{Masking voxel shapes.}\quad
In a given voxel, both the original and decorated coordinates of points represent the local shape of the voxel (Fig.~\ref{fig:MV-JAR}). We employ a shared learnable token $m_{p} \in \mathbb{R}^{9}$ to replace all point features in every shape-masked voxel, except for one point. This particular point conveys the voxel's positional information without revealing the voxel shape, which would render the reconstruction task trivial. Consequently, the masked voxel becomes $V_{\text{masked}} = \{[x_1, y_1, z_1, x^c_1, y^c_1, z^c_1, x^v_1, y^v_1, z^v_1]\} \cup \{p_j = m_p \}_{j=2...T}$.

\noindent\textbf{Reconstruction target and loss function.}\quad
We reconstruct each masked voxel and use the $L2$ Chamfer Distance loss to measure the discrepancy between the reconstructed and target point distributions. This loss function is insensitive to point density~\cite{balancedCD}, which is crucial since each voxel may contain varying numbers of points. Assuming each reconstructed voxel contains $n$ points, the reconstruction head outputs a vector containing the coordinates of each point in the masked voxel, $\hat{v} \in \mathbb{R}^{3n}$. To pre-train the model more stably, we normalize the target point distribution using each point's distance to the voxel center as ground truth and scale the distance between 0 and 1. Supposing each reconstructed masked voxel is $\hat{V} = \{ \hat{p}_j = [\hat{x}_j, \hat{y}_j, \hat{z}_j] \}_{j=1...n}$ and the ground truth is $V = \{p_j = [x^v_j, y^v_j, z^v_j] \}_{j=1...t_i}$, the total loss across all masked voxels is given by:
\begin{equation}
L_{MVR} = \frac{1}{R_s}\sum_{i=1}^{R_s}{L_{CD}}(\hat{V}_i, V_i ),
\end{equation}
where $R_{s}$ represents the number of shape-masked voxels and $L_{CD}$ is the $L2$ Chamfer Distance loss calculated as:
\begin{equation}
\begin{split}
        L_{CD} & = \frac{1}{\left|\hat{V}_i\right|} \sum_{\hat{p} \in \hat{V}_i} \min _{p \in V_i}\|\hat{p} - p\|_{2}^{2} \\
        & + \frac{1}{\left|V_i\right|} \sum_{p \in V_i} \min _{\hat{p} \in \hat{V}_i}\|p-\hat{p}\|_{2}^{2}.
\end{split}
\end{equation}

\subsection{Joint Pre-Training}
\label{subsec:joint_pre_training}
We jointly pre-train the model using both MVJ and MVR tasks to enable the model to learn both point and voxel distributions. We use R-FVS to sample $R_s$ voxels for MVR and $R_p$ voxels for MVJ. As illustrated in \cref{fig:MV-JAR}, the voxel encoder and the backbone extract features from both types of masked voxels and unmasked voxels. Masked features are decoded to recover their corresponding targets, and Cross-Entropy loss and Chamfer Distance loss are utilized for supervision. The total loss for joint pre-training is expressed as follows:
\begin{equation}
L = \alpha L_{MVJ} + \beta L_{MVR}
\end{equation}
where $\alpha$ and $\beta$ are balancing coefficients.

\input{tables/data_efficient_benchmark_Waymo.tex}