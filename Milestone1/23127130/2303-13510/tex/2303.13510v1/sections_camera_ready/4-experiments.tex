\section{Data-Efficient Benchmark on Waymo}
\label{sec:benchmark}

To evaluate the pre-trained representation, we transfer the weights of the voxel encoder and the backbone for object detection. Previous benchmarks fail to effectively reveal the effectiveness of pre-training strategies due to data diversity issues. We propose a new data-efficient benchmark to address these limitations.

\noindent\textbf{Observation: Incomplete model convergence.}\quad
Prior works~\cite{gcc-3d, proposalcontrast} fine-tune the model on different splits with the same \emph{epochs}, causing reduced iteration numbers when dataset sizes decrease. As illustrated in \cref{fig:existing_benchmark}, by training for more epochs (but the same \emph{iterations}), the performance of SST significantly improves, suggesting that the models may not fully converge on smaller splits when training epochs remain constant. This entangled factor prevents accurate evaluation of pre-training strategies.

\noindent\textbf{Issue: Fine-Tuning splits with similar data diversities.}\quad
With the same training iterations, models trained on different splits display comparable performance, despite significant differences in data amounts (\cref{fig:existing_benchmark}). We find that scene diversity is a crucial factor. Previous works \cite{gcc-3d, proposalcontrast} uniformly sample the entire Waymo training set to create different fine-tuning splits, for instance, sampling one frame from every two consecutive frames for a 50\% split. However, neighboring frames in self-driving datasets are similar due to the short time difference between consecutive frames (e.g., only 0.1$s$ in Waymo~\cite{Waymo}). This results in similar scene diversity across splits, leading to comparable detection performance using the same training iterations. 

We argue that such an experimental setting does not accurately represent a typical application scenario of self-supervised pre-training, where annotated fine-tuning data is less abundant and diverse compared to pre-training data. Furthermore, it fails to evaluate how pre-training benefits downstream tasks when varying amounts and diversities of labeled data are available. We provide an additional example in the supplementary material to further illustrate these issues with previous benchmarks. Our proposed data-efficient benchmark addresses these issues, offering a more precise evaluation of pre-training strategies.

\noindent\textbf{Solution: Sequence-Based data sampling.}\quad
We sample fine-tuning data using different scene sequences rather than uniformly sampling varying numbers of frames from identical sequences. The Waymo training split comprises 798 distinct scene sequences, each lasting 20$s$ with approximately 200 frames\cite{Waymo}. We randomly sample {5\%, 10\%, 20\%, 50\%} of the scene sequences, respectively, using all frames within the sampled sequences for fine-tuning. Consequently, data diversities and amounts vary across each split. Each larger split also encompasses the smaller ones, enabling measurement of performance changes with additional fine-tuning data. Furthermore, we randomly sample three 5\% and 10\% splits and report average results to reduce variance on these smaller sets (see our supplementary material).

For different fine-tuning splits, we train the model until performance saturates (i.e., overfitting) and determine the number of epochs used for experiments accordingly. For example, for 5\% data, we try \{48, 60, 66, 72, 78, 84\} epochs and ultimately select 72. The baseline performances across various training iterations are shown in \cref{fig:our_benchmark}. With our new benchmark, baseline performance and convergence speed vary as anticipated for different splits.

\begin{figure}
\centering
        \begin{subfigure}{0.49\linewidth}
            \includegraphics[width=\linewidth]{figures/existing_benchmark.pdf}
        \caption{Existing}
        \label{fig:existing_benchmark}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.49\linewidth}
            \includegraphics[width=\linewidth]{figures/our_benchmark.pdf}
        \caption{Ours}
        \label{fig:our_benchmark}
        \end{subfigure}

\caption{Comparison of data-efficient benchmarks. Previous uniformly sampled splits exhibit similar diversity, as evidenced by similar performance with the same training iterations. Our benchmark features varying data amounts and diversities, ensuring model convergence. Dashed lines show baseline performances.}
\label{partial-ft}
\end{figure}

\section{Experiments}

\subsection{Experimental Settings}

\noindent\textbf{Datasets.}\quad The Waymo dataset~\cite{Waymo} consists of 1150 self-driving scenes with 798 scenes as the training split, 202 scenes as the validation split, and 150 scenes as the testing split. We pre-train on the training split, which contains 158,240 annotated frames. For the downstream data-efficient 3D object detection task, we fine-tune the model using our sampled splits (as described in \cref{sec:benchmark}) and evaluate performance on the standard Waymo validation split with 3 classes (cars, pedestrians, and cyclists) and two difficulty levels (L1 and L2). We adopt L2 mean average precision (L2 mAP) and L2 mean average precision with heading (L2 mAPH) as main evaluation metrics. We also evaluate the model on the KITTI dataset\cite{KITTI_Det} for the detection task. It contains 7481 training samples and 7518 test samples.

\noindent\textbf{Implementation details.}\quad We employ the official implementation and training settings of SST\cite{SST} for pre-training and fine-tuning unless specified otherwise. The Waymo dataset's point cloud range is $W \times H \times D=149.76m \times 149.76m \times 6m$, and the voxel size is $v_W \times v_H \times v_D = 0.32m \times 0.32m \times 6m$. For MVR, we predict 15 points per voxel with a masking ratio of 0.05. For MVJ, the window size is $N_x \times N_y \times N_z = 12 \times 12 \times 1$ and the masking ratio is 0.1. We set the loss weight $\alpha=\beta=1$, pre-train for 6 epochs with an initial learning rate of $5e-6$. When fine-tuning, we initialize the voxel encoder and SST backbone with pre-trained weights, and other training hyper-parameters remain unchanged. We set KITTI's point cloud range to $69.12m \times 79.36m \times 4m$, voxel size to $0.32m \times 0.32m \times 4m$, and train for 160 epochs. We use the AdamW optimizer with a batch size of 8 and a cyclic learning rate scheduler with cosine annealing for all training.

\subsection{Main Results on Waymo}
\noindent\textbf{Baselines.}\quad We fine-tune the model using data splits described in \cref{sec:benchmark}. For \{5\%, 10\%, 20\%, 50\%, 100\%\} data, the model is trained for \{72, 60, 42, 36, 24\} epochs. We use the randomly initialized model's performances as our baselines. We also include experiments with models using convolutional backbones in the supplementary material.

\noindent\textbf{Results with different initialization.}\quad \cref{tab:data-efficient_benchmark_waymo} presents the SST model's performances with various initializations on our proposed data-efficient benchmark. Our pre-training method consistently improves SST baselines across all fine-tuning data amounts, especially when data is scarce. Our method achieves up to 15.7\% relative improvement, from 40.34\% L2 mAPH to 46.68\% L2 mAPH, when using only 5\% fine-tuning data. The performance gain exists across object categories, indicating effective general representation learning. As more fine-tuning data is introduced, the performance gain persists, and when fine-tuning with the entire Waymo training set, our pre-training enhances detection performance from 65.54\% mAPH to 66.20\% mAPH.

We note that with 50\% and 100\% fine-tuning data, the improvements from our pre-training are not as significant as with less data. We observe that the baseline result on 50\% data (63.36\% mAPH) is comparable to that on 100\% data (65.54\% mAPH), suggesting that training data is no longer the bottleneck. The original SST, with only 2.1M parameters, is a lightweight network. We scale up SST to 8.3M parameters by using larger hidden layer channels and observe larger benefits from pre-training on the 50\% fine-tuning split, from 63.13\% L2 mAPH to 64.24\% L2 mAPH. Therefore, we argue that the SST model's capacity, rather than the fine-tuning data amount, becomes the bottleneck for detection performance.

\noindent\textbf{Comparisons with contrastive learning methods.}\quad We utilize the official implementation of the recently proposed ProposalContrast~\cite{proposalcontrast} and reimplement PointContrast~\cite{PointContrast} for comparison. As demonstrated in \cref{tab:data-efficient_benchmark_waymo}, both contrastive learning methods offer benefits only with 5\% fine-tuning data, and their improvements diminish with higher baseline performances. This may be due to the difficulty in smoothly learning representations. A key challenge in LiDAR-based contrastive learning is constructing matched pairs. The point pairs used by PointContrast face the issue of spatially adjacent points with similar features being assigned as negative samples. We observe difficulty in convergence when sampling excessive pairs. ProposalContrast alleviates the issue by contrasting regions but does not entirely resolve it. Alternatively, our method offers a new avenue for developing self-supervised pre-training through masked voxel modeling on LiDAR point clouds.

\subsection{Pre-training Accelerates Convergence}
To investigate how MV-JAR pre-training accelerates convergence, we fine-tune the SST model on the entire Waymo training split. We train the SST model for different epochs and report the performance of both fine-tuning and training from scratch. As illustrated in \cref{fig:data-efficient_speed_up_convergence}, MV-JAR pre-training significantly enhances the convergence speed of SST. By fine-tuning for only 3 epochs, the SST model achieves 62.48\% L2 mAPH. Remarkably, fine-tuning for just 12 epochs results in a performance comparable to training from scratch for 24 epochs, effectively halving the convergence time. As the number of fine-tuning epochs increases, the performance gain provided by MV-JAR does not wane. Pre-training can still improve the model's converged performance by 0.74\% L2 mAPH, highlighting the superiority of the pre-trained representation.

\subsection{Transferring Results on KITTI} 
\input{tables/transferring_KITTI.tex}

To evaluate the transferability of the learned representation, we fine-tune various pre-trained SST models on the KITTI dataset. The original SST paper\cite{SST} did not conduct experiments on the KITTI dataset; thus, we follow the training schedule and setup of SST and PointPillars\cite{pointpillars} within the MMDetection3D framework\cite{mmdet3d2020}.
As demonstrated in \cref{tab:transfer_kitti}, the performance gain from our MV-JAR pre-training persists when the representation is transferred to a different domain, indicating that the model learns a generic representation through pre-training. While the KITTI training samples account for approximately 5\% of the entire Waymo training split, the relative improvement is much smaller than transferring to the 5\% Waymo split. We hypothesize that this may be attributed to the domain gap between the Waymo and KITTI datasets.

\subsection{Ablation Studies}   
\begin{figure*}[t]
  \centering
    \includegraphics[width=\linewidth]{figures/visualization.pdf}
  \caption{Visualization of the reconstruction results of MV-JAR on the Waymo validation set, compared with ground truths.}
  \label{fig:visualization}
\end{figure*}

\input{tables/ablation.tex}
In this section, we conduct ablation studies to investigate the effectiveness of our proposed Reversed-Furthest-Voxel-Sampling (R-FVS) strategy. Additionally, we explore the optimal masking ratio and the impact of MVR and MVJ pre-training. All pre-training experiments utilize the entire Waymo training split, and we fine-tune the model on one of our 5\% data splits.

\noindent\textbf{Mask sampling strategy.}\quad 
Our R-FVS mask sampling strategy masks voxels that are not sampled by FPS, aiming to avoid masking voxels located in sparse regions. We also examine random sampling and an opposing sampling strategy called FVS, which masks voxels sampled by FPS. With FVS sampling, regions with sparse points are more likely to be masked, leading to greater information loss compared to random sampling and R-FVS sampling. \cref{tab:sampling_strategy} presents the fine-tuning results of pre-training with the three sampling strategies. R-FVS sampling performs the best, while FVS performs the worst. These findings confirm that minimizing information loss benefits pre-training and validate the effectiveness of our R-FVS strategy.

\noindent\textbf{Masking ratio.}\quad
\cref{tab:mask_ratio} illustrates the impact of different masking ratio combinations. Specifically, MVR with a 0.1 masking ratio and MVJ with a 0.05 masking ratio yield the best results. With this combination, the overall masking ratio is 0.15, which is significantly lower than the masking ratios in the image\cite{MAE, SimMiM} or video domain\cite{Video-MAE, Video-MAEW}. A high masking ratio in masked autoencoders typically indicates information redundancy\cite{MAE, Video-MAE, Video-MAEW, SimMiM}. We postulate that two factors contribute to our low masking ratio: 1) LiDAR point clouds represent a vast space where each meaningful object occupies only a small fraction. The information on each object is not highly redundant, especially when the objects are distant from the sensor. This contrasts with images in datasets like ImageNet, where objects often encompass a significant portion of the image. 2) The SST model calculates attention only within a partitioned window, which restricts the information used. We leave this question for future exploration.

\noindent\textbf{Pre-training effects of MVR and MVJ.}\quad
We investigate the individual pre-training effects of MVR and MVJ by optimizing the masking ratio for each task and reporting the fine-tuning performances in \cref{tab:Pre-training_Effects}. Both MVR and MVJ pre-training enhance performance compared to random initialization, with MVJ outperforming MVR. This finding suggests that capturing voxel distributions plays a more significant role in representation learning, which is consistent with the fact that LiDAR detectors process points by downsampling them into voxels. Our integrated MV-JAR method demonstrates further improvement over MVR and MVJ, validating the effectiveness of jointly capturing point and voxel distributions. Additionally, we offer an analysis of improvements across various distances in our supplementary material.

\subsection{Visualization}
\Cref{fig:visualization} displays the MV-JAR reconstruction results on the Waymo validation set, including MVJ and MVR outcomes. In our pre-training, MVJ achieves approximately 89\% classification accuracy of masked voxels on the validation set, accurately reconstructing masked voxels in the correct positions most of the time. MVR can capture the point distributions within voxels, evidenced by reconstructed ground points aligning with ground circles. However, MVR struggles to capture detailed distributions, likely due to the discrete sampling of LiDAR points from continuous surfaces, resulting in considerable variability in the data and increased difficulty in capturing finer details.