\vspace{-3pt}
\section{Introduction}
\label{sec:intro}
\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{figures/teaser.pdf}
\caption{3D object detection results on the Waymo dataset. Our MV-JAR pre-training accelerates model convergence and greatly improves the performance with limited fine-tuning data.}
\label{fig:data-efficient_speed_up_convergence}
\vspace{-10pt}
\end{figure}

Self-supervised pre-training has gained considerable attention, owing to its exceptional performance in visual representation learning. Recent advancements in contrastive learning~\cite{moco,mocov2,simclr,swav,zhang2022densesiam} and masked autoencoders~\cite{beit, MAE, SimMiM, ibot,cae} for images have sparked interest among researchers and facilitated progress in modalities such as point clouds.

However, LiDAR point clouds differ from images and dense point clouds obtained by reconstruction as they are naturally sparse, unorganized, and irregularly distributed. Developing effective self-supervised proxy tasks for these unique properties remains an open challenge. Constructing matching pairs for contrastive learning in geometry-dominant scenes is more difficult~\cite{gcc-3d,proposalcontrast}, as points or regions with similar geometry may be assigned as negative samples, leading to ambiguity during training. To address this, our study explores masked voxel modeling paradigms for effective LiDAR-based self-supervised pre-training. 

Downstream LiDAR-based 3D object detectors~\cite{SECOND, centerpoint, pointpillars, SST, reconfig_voxels, ssn} typically quantize the 3D space into voxels and encode point features within them. Unlike pixels, which are represented by RGB values, the 3D space presents a scene-voxel-point hierarchy, introducing new challenges for masked modeling. Inspired by this, we design masking and reconstruction strategies that consider voxel distributions in the scene and local point distributions in the voxel. Our proposed method, Masked Voxel Jigsaw And Reconstruction (MV-JAR), harnesses the strengths of both voxel and point distributions to improve performance.

To account for the uneven distribution of LiDAR points, we first employ a Reversed-Furthest-Voxel-Sampling (R-FVS) strategy that samples voxels to mask based on their sparseness. This approach prevents masking the furthest distributed voxels, thereby avoiding information loss in regions with sparse points. To model voxel distributions, we propose Masked Voxel Jigsaw (MVJ), which masks the voxel coordinates while preserving the local shape of each voxel, enabling scene reconstruction akin to solving a jigsaw puzzle. For modeling local point distributions, we introduce Masked Voxel Reconstruction (MVR), which masks all coordinates of points within the voxel but retains one point as a hint for reconstruction. Combining these two methods enhances masked voxel modeling.

Our experiments indicate that existing data-efficient experiments~\cite{gcc-3d, proposalcontrast} inadequately evaluate the effectiveness of various pre-training methods. The current benchmarks, which uniformly sample frames from each data sequence to create diverse fine-tuning splits, exhibit similar data diversity due to the proximity of neighboring frames in a sequence~\cite{Waymo, KITTI_Det, nuscenes}. Moreover, these experiments train models for the same number of epochs across different fine-tuning splits, potentially leading to incomplete convergence. As a result, the benefits of pre-trained representations become indistinguishable across splits once the object detector is sufficiently trained on the fine-tuning data. To address these shortcomings, we propose sampling scene sequences to form diverse fine-tuning splits and establish a new data-efficient 3D object detection benchmark on the Waymo~\cite{Waymo} dataset, ensuring sufficient model convergence for a more accurate evaluation.

We employ the Transformer-based SST~\cite{SST} as our detector and pre-train its backbone for downstream detection tasks. Comprehensive experiments on the Waymo and KITTI~\cite{KITTI_Det} datasets demonstrate that our pre-training method significantly enhances the model's performance and convergence speed in downstream tasks. Notably, it improves detection performance by 6.3\% mAPH when using only 5\% of scenes for fine-tuning and reduces training time by half when utilizing the entire dataset (Fig.~\ref{fig:data-efficient_speed_up_convergence}). With the representation pre-trained by MV-JAR, the 3D object detectors pre-trained on Waymo also exhibit generalizability when transferred to KITTI.