\section{Related Work}

\noindent\textbf{Self-supervised learning for point clouds.}\quad
Annotating point clouds demands significant effort, necessitating self-supervised pre-training methods. Prior approaches primarily focus on object CAD models~\cite{foldingnet, reconstructsapce, point-mae, point-bert, maskpoint, point-m2ae} and indoor scenes~\cite{PointContrast, scenecontrast, depthcontrast}. Point-BERT~\cite{point-bert} applies BERT-like paradigms for point cloud recognition, while Point-MAE~\cite{point-mae} reconstructs point patches without the tokenizer. To acquire scene-level representation, PointContrast~\cite{PointContrast} introduces a contrastive learning approach that compares points in two static partial views of a reconstructed indoor scene. Its successor, SceneContrast~\cite{scenecontrast}, incorporates spatial information into the contrastive learning framework. However, LiDAR point clouds are irregular and dynamic, with mainstream LiDAR perception models often exhibiting distinct architectures, which obstructs the direct adaptation of object-level and indoor scene pre-training methods. STRL~\cite{strl} employs contrastive learning with two temporally-correlated frames for spatiotemporal representation. GCC-3D~\cite{gcc-3d} presents a framework that combines geometry-aware contrast and pseudo-instance clustering harmonization. ProposalContrast~\cite{proposalcontrast} targets region-level contrastive learning to improve 3D detection. CO3~\cite{co3} leverages infrastructure-vehicle-cooperation point clouds to construct effective contrastive views. In contrast, our work explores masked voxel modeling, diverging from the prevailing contrastive learning paradigm.

\noindent\textbf{Masked autoencoders for self-supervised pre-training.}\quad
Masked language modeling plays a pivotal role in self-supervised pre-training for Transformer-based networks in natural language processing~\cite{bert,gpt-2,gpt-3}. This approach typically masks portions of the input and pre-trains networks to predict the original information. With the successful integration of Transformers into computer vision~\cite{vit}, researchers have increasingly focused on masked image modeling~\cite{beit, MAE, SimMiM, ibot, cae, position_prediction} to mitigate the data-intensive issue of ViT~\cite{vit}. BEiT~\cite{beit} employs a tokenizer to generate discrete tokens for image patches and utilizes a BERT-like framework to pre-train ViT. MAE~\cite{MAE} introduces an asymmetric autoencoder for reconstructing RGB pixels of original images, eliminating the need for an extra tokenizer. SimMiM~\cite{SimMiM} encodes masked raw images with Transformers and employs a lightweight prediction head for recovery. Demonstrating considerable potential in image recognition, masked autoencoders have been applied in video understanding~\cite{Video-MAE,Video-MAEW}, medical image analysis~\cite{mae_for_medical}, and audio processing~\cite{mae_for_audio}. In this work, we investigate the application of this powerful pre-training technique to LiDAR point clouds using voxel representation.

\begin{figure*}[ht]
\centering
\includegraphics[width=\linewidth]{figures/method.pdf}
\caption{Overview of Masked Voxel Jigsaw and Reconstruction (MV-JAR). Our R-FVS sampling method selects a specific ratio of non-empty voxels to mask. For position-masked voxels, we mask the absolute coordinates of all points within the voxel while maintaining the relative coordinates. For shape-masked voxels, we mask both absolute and relative coordinates of all points, preserving only one point. Features are extracted from all non-empty voxels, and masked features are decoded by a dedicated head to recover corresponding targets.}
\label{fig:MV-JAR}
\end{figure*}

\noindent\textbf{Transformer-based 3D object detection.}\quad
The recent success of Vision Transformers~\cite{vit} has inspired extensive research into the application of Transformer-based architectures for 3D object detection~\cite{3DTR, pointformer, voxelformer, SST, SWFormer}. 3DETR~\cite{3DTR} presents an end-to-end object detection framework with modified Transformer blocks for 3D point clouds. Pointformer~\cite{pointformer} employs hierarchical Transformer blocks to extract point features for 3D detection. Voxel Transformer~\cite{voxelformer} develops a Transformer-based 3D backbone to establish long-range relationships between voxels. Recently, SST~\cite{SST} introduced a single-stride sparse Transformer as a replacement for the PointPillars~\cite{pointpillars} backbone, abandoning the multi-resolution strategy to enhance the detection of small objects. Emulating the Swin-Transformer~\cite{swin-transformer}, SST partitions the space into windows and calculates attention only within each window for efficiency. This approach demonstrates impressive detection results, and we adopt it for our experiments.