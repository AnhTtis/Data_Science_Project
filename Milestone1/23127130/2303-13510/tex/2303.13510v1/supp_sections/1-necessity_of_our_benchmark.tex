\section{Necessity of Our New Benchmark}

To demonstrate the importance of full convergence and our proposed benchmark, we follow previous works~\cite{gcc-3d,proposalcontrast} and fine-tune SST~\cite{SST} on uniformly sampled 5\% data. We conduct fine-tuning for 6 epochs and 84 epochs, respectively. The results in \cref{tab:necessity_of_our_benchmark} reveal that ProposalContrast significantly improves the baseline when the model is trained with few iterations. However, it slightly diminishes performance when the model is fully converged.

The disappearance of pre-training benefits underscores the necessity of adequate fine-tuning. Therefore, fine-tuning the model on different uniformly sampled splits using the same number of \emph{epochs}, which may result in incomplete convergence on smaller splits, cannot precisely assess pre-training effects. In our main paper, we present evidence that uniformly sampled splits are actually similar when the model reaches full convergence. Our proposed benchmark, which samples data by scene sequences to create diverse splits, can effectively and comprehensively reveal the pure improvements of pre-training.

 %% necessity of benchmark
\begin{table}[h]
\begin{minipage}{\linewidth}
    \centering
    \caption{Fine-tuning on uniformly sampled 5\% data.}
    \label{tab:necessity_of_our_benchmark}
    \scalebox{0.8}{\tablestyle{8pt}{1.0}
        \begin{tabular}{@{}ccccc@{}}
        \toprule        
        \multirow{2}{*}{Initialization} & \multicolumn{2}{c}{6 Epochs} & \multicolumn{2}{c}{84 Epochs} \\ \cmidrule(l){2-5} 
         & L2 mAP & L2 mAPH & L2 mAP & L2 mAPH \\ \midrule
        Random & 40.90 & 34.12 & 63.00 & 58.86 \\
        ProposalContrast\cite{proposalcontrast} & 47.56 & 41.30 & 62.57 & 58.75 \\
        \textbf{MV-JAR} & \textbf{50.67} & \textbf{45.02} & \textbf{65.14} & \textbf{61.74} \\ \bottomrule
        \end{tabular}
        }
\end{minipage}
\end{table}