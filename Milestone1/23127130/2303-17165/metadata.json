{
    "arxiv_id": "2303.17165",
    "paper_title": "Second-order Properties of Noisy Distributed Gradient Descent",
    "authors": [
        "Lei Qin",
        "Michael Cantoni",
        "Ye Pu"
    ],
    "submission_date": "2023-03-30",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "math.OC"
    ],
    "abstract": "We study a fixed step-size noisy distributed gradient descent algorithm for solving optimization problems in which the objective is a finite sum of smooth but possibly non-convex functions. Random perturbations are introduced to the gradient descent directions at each step to actively evade saddle points. Under certain regularity conditions, and with a suitable step-size, it is established that each agent converges to a neighborhood of a local minimizer and the size of the neighborhood depends on the step-size and the confidence parameter. A numerical example is presented to illustrate the effectiveness of the random perturbations in terms of escaping saddle points in fewer iterations than without the perturbations.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17165v1"
    ],
    "publication_venue": null
}