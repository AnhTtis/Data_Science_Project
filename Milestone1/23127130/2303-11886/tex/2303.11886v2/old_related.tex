
\section{Related Work}
Solving for elastodynamics usually involves having to construct a large sparse system of equations, and then solving it each timestep. Accelerating elastodynamics has been a primary point of interest in graphics since the the pioneering work of \cite{Terzopoulos1987}. 

The most obvious way of speeding up this system solve is by using a fast linear solver. Because the system usually changes each timestep, direct solvers making use of matrix pre-factorizations are off the table. Iterative solvers, such as Conjugate Gradient, LBFGS or multigrid solvers \cite{LiuMultigrid2021} find a solution in time complexity \emph{linear} with the mesh resolution. 

Aside from selecting an optimal linear solver, we can also reformulate the discretization of the underlying physics.
Position based dynamics 
\cite{MullerPBD2007, MullerMeshlessDefo2005, MacklinPBDStableNeohookean2021} model  elasticity using only point masses and a set of elasticity-enforcing constraints. The point masses are integrated in time with a forward euler integration step, after which their positions are projected to an elasticity-inspired constraint manifold. This process is repeated until satisfaction. The resulting dynamics are tied to the number of iterations/constraint projection steps are performed, where each iteration scales linearly in complexity with the number of points and constraint projection steps.

\paragraph{Projective Dynamics}
\cite{Bouaziz:2014:Projective} formalizes the process above by identifying exactly a class of elastic energies that can be described by a non-linear constraint projection step, and a standard linear solver. Specifically, these energies satisfy that the constraint projection step handles all of the complex changing non-linear behavior, leaving the resulting system static. The static system allows us to prefactorize the system. 
\cite{QuasiNewtonLiu2017}  and \cite{Narain2016} generalize projective dynamics to a broader class of elastic materials by linking it to broader optimization schemes, like ADMM and Quasi-Newton Methods.
Iteratively solving for the constraints and the degrees of freedom can cause slow convergence, \cite{MixedFEMTrusty2022} greatly accelerate the convergence through a Mixed Finite Element scheme, essentially linking the constraints to the degrees of freedom with lagrange multipliers to make sure the two are always in agreement. 



 Boundary element methods \cite{Sugimoto:2022:BEM, artDefoJames1999} for elasticity require dense solves and a complicated implementation overhead to obtain linear-time advertised speedups.


\cite{McAdams2011} provide a grid-discretized cubtature accelerated elasticity model.

\subsection{Reduced Models for Elasticity}


\paragraph{Linear  Subspaces from Modal Analysis} Subspaces obtained via modal analysis have been used in graphics since \citet{PentlandWilliams1989} as a way to accelerate physics-simulations. Instead of obtaining the linear basis from a statistical basis, they obtain it through a modal analysis of the elastic energy. Here, the subspace is spanned by the lowest-energy deformations as computed from the rest state of the system, which end up being the eigenvectors of the rest state elastic energy Hessian. The resulting physical simulation is then performed in the subspace, where the simulator aims to find how much to activate each of these low-energy vectors. The  resulting motions suffer from linearization artefacts, and are not rotation equivariant. \cite{BarbicJames:RealTimeSTVK}.  

\cite{HarmonSubspaceLocalDeformation2013} and \cite{Teng:SubspaceCondensation} add local enrichment functions on the fly.
\cite{melzi2018localized} compute local modes with a weighing parameter in a least squares sense. 
\cite{compressedModesADMM}  and \cite{Brandt2017CompressedVibrationModesofElasticBodies} compute modes with a sparsity penalizing $l_1$ regularization term and minimizing the resulting energy with ICCM or ADMM respectively. This sparsity term is balanced by the smoothness measured by the elastic energy hessian creates modes that are \emph{local}. This results in a slow process that requires a lot of tweaking of parameters, and the delicate balance between the smoothness and the sparsity term can easily be corrupted with additional constraints in the optimization.


\paragraph{Augmenting Subspaces with Modal Derivatives}
A big drawback of linear subspaces is just that, their \emph{linearity}. As a subspace simulation is excited and undergoes large deformation (deviating from the rest state), the set of basis vectors spanning our subspace quickly becomes sub-optimal. Modal derivatives \cite{BarbicJames:RealTimeSTVK} augment these linear bases with a new set of \emph{derivative} modes, whose aim is to provide the support needed for the simulation to update the primary set of modes in order to better acommodate the large deformation. These modes effectively make the subspace a \emph{quadratic} one instead of a linear one.  While the quality of subspace simulations making use of modal derivatives is greatly increased,  this is a band aid. Modal derivatives struggle to reproduce even simple global rotations as shown in Figure \ref{fig:subspace-comparisons}.  
\paragraph{Modal Warping}
\alec{What do you mean by ``floating frame''? How is that different from a ``frame''? There a bunch of ``frame-based'' methods (see Benjamin Gilles and Francois Faure) methods that we should distinguish ourselves from.} \Otman{ah good point... i consider Gilles and Faures methods to be a skinning subspaces for deformation. we definitely should distinguish our method from theirs in that section. what I mean here by floating frame is just updating the displacement modes each timestep with a best fit rotation matrix.}
\citet{ModalWarping} Computes a best-fit rotation matrix at each vertex, and updates each vertex's subspace with that corresponding rotation. While this allows the subspace simulation to accommodate local rotations and maintain rotation equivariance, the process of updating the subspace is a full space operation. This method quickly become prohibitively expensive for large meshes. 

\paragraph{Sub-structuring}
We can instead identify sub-structures in our mesh that should exhibit similar rotational properties, either specified by a user \cite{Barbic:2011:RealTimeLargeDefoSubstructuring}, or automatically detected via a clustering-like approach \cite{KimJamesMultiDomainStitching}.The subspace is then updated on a per-region basis (instead of per-vertex like Modal Warping). Too few sub-structures means our subspace loses out on representing local rotational motion. More sub-structures allow for the capture of rotational deformations, but requires updating the subspace for each sub-structure; an operation that becomes the bottleneck in our subspace simulation, limiting the richness of the dynamics we can obtain at real-time speeds. Additionally, to perform this subspace update quickly, one would use a Fast Sandwich Transform \citet{FST}, which requires a heavy memory budget at the pre-computation stage (Each substructure requires precomputing any matrix involving the subspace 9 times, 1 for each rotation parameter). 

\paragraph{Rotation Strain Coordinates}
Rotation strain coordinates take linearized deformations and project them to a best-fit rotational motion via an exponential map. Ironically, that deformation \emph{needs} to be linearized... if the deformation were a simple non-linear global rotation, it is prone to catastrophic collapse. To avoid this you need to use a floating frame, or many floating frames, for which we discuss disadvantages above. Additionally, enforcing the complementary constraint in this rotation-strain coordinate system is not immediately obvious, where should we enforce it? On our simulation step, or on the projection step? On both?

\paragraph{Skinning Subspaces}
Using linear blend skinning as a deformation subspace is not new. All prior methods differ in how they construct their linear blend skinning subspace.
\citet{RSPHahn} let's a user specify their own physics-rig and develop a physical simulation that acts on that rig-space specifically.  This burdens the user with specifying their own rig and rig-weights an often unintuitive task. A user may easily be able to specify low frequency physical motions they may want in their subspace, but it's often harder for them to think of the fine detail.  Our method assumes a linear blend skinning subspace for secondary motion, and we build an optimal secondary linear blend skinning subspace automatically from the primary control rig, unburdening this task user. 
\citet{Brandt2018HyperReducedPD} build their linear blend skinning subspace weights very quickly via normalized radial basis functions. While this method is very fast and effective, it unfortunately is not material aware. Resulting in inefficient simulations. 
\citet{Gilles2011} compute a material aware \emph{Dual Quaternion Skinning} subspace (as opposed to a skinning subspace) for deformation. They require a user to specify as input \emph{frames}, and then from those frames they construct weight functions using harmonic coordinates \cite{Joshi2007}.  While these weights are smooth and localized around each frame, they are not material aware.
\citet{Faure2011} improve upon the above, and specifically automatically determine where to place the initial input frames, and compute weights proportional to a material-aware quantity: \emph{the compliance distance}. 


All three of the above cannot accommodate the complementary constraint.

\paragraph{Cubature}
To accommodate rotational deformations, elastic energies require the use of per-tet non-linear energetic quantities. As a result, even with the use of a subspace approximation on our degrees of freedom, our simulation  complexity is still tied to the resolution the meshes in question. To remedy this \citet{OptimizingCubature} make use of a \emph{cubature} integration scheme, approximating these per-tet non-linearities with a sparse set of carefully chosen sample tetrahedra. These non-linearities are then computed on this smaller set of tetrahedra, which are blended with cubature weights to recuperate the per-tetrahedral non-linearities. 

This approach's success is heavily dependant on the selection of the sample tetrahedra. These samples are often found by a least squares fitting to some training data. A poor sample set can result in obvious deformations not in the training data to go completely ignored. We instead make use of clusters, which are not constructed from training data, but rather an elasticity prior.

\paragraph{Clustering}
Addressing the same issue as above \cite{Jacobson-12-FAST} make use of non-linearity \emph{clusters}. Instead of approximating these non-lineariteis with a sparse set of samples, clustering groups tetrahedra together and performs a weighed average on these non-linear quantities. 
As a result, any deformation occurring at any tetrahedra can still be represented and felt. Their use case applies clustering only to an ARAP elastic energy, whereas we show we can apply this clustering methodology to a much larger set of elastic energies. 



\paragraph{Neural Elastodynamic Simulation}
While recent approaches like \cite{zheng2021deep} show promise in being capable of learning efficient encoding for physical simulation, they still suffer from a number of difficulties. They require tons of training data. Their capacity to generalize across meshes has yet to be battle tested. While simulating a normal elastic material is a current open research problem, a neural simulator capable of outputting motions that satisfy the control constraint of complementary dynamics is still a while away. 

