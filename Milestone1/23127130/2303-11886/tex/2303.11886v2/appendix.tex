\appendix


\section{Proofs}
\subsection{A subspace simulation is rotation equivariant if and only if the subspace is closed under rotations} 
\label{sec:appendix-proof-sim-rotation-equivariance}
Consider the minimization of a rotation invariant elastodynamic energy $E(\boldsymbol{u} + \boldsymbol{x})$, where $\boldsymbol{u}, \boldsymbol{x} \in \mathbb{R}^{3n}$. By rotation equivariance we mean that 
\begin{align}
 \forall \boldsymbol{R}\, , \ \  
  \forall \boldsymbol{x}\, , \ \
    \argmin_{\boldsymbol{u}} E(\boldsymbol{u}+  \repR\boldsymbol{x} ) = \repR \argmin_{\boldsymbol{v}} E(\boldsymbol{v} +  \boldsymbol{x} ) \ . \nonumber
\end{align}
Here and henceforth the domains for $\boldsymbol{R}$ and $\boldsymbol{x}$ shall be the rotations $\mathcal{SO}(3)$ and the positions $\mathbb{R}^{3n}$, respectively.

% In a full space simulation, we can easily show that the above holds.
% Using rotation invariance, $E( \boldsymbol{u} + \boldsymbol{R}\boldsymbol{x}) = E( [\boldsymbol{R}^T]\boldsymbol{u}+ \boldsymbol{x})$:

% \begin{align}
%     \argmin_{\boldsymbol{u}} E(\boldsymbol{u}+  \repR\boldsymbol{x} ) = \repR \argmin_{\boldsymbol{u}} E(\boldsymbol{u} +  \boldsymbol{x} ) \quad \forall \boldsymbol{R}  \in \mathcal{SO}(3) \nonumber
% \end{align}

In a full space simulation, the above holds. For a subspace simulation where $\boldsymbol{u} = \boldsymbol{B} \boldsymbol{z}$, and  $\mathrm{Col}(\boldsymbol{B}) \subset \mathbb{R}^{3n}$, the equivalent statement,
\begin{align}
 \forall \boldsymbol{R}\, , \ \  
  \forall \boldsymbol{x}\, , \ \
    \boldsymbol{B} \argmin_{\boldsymbol{z}} E(\boldsymbol{Bz}+  \repR\boldsymbol{x} ) = \repR \boldsymbol{B} \argmin_{\boldsymbol{w}} E(\boldsymbol{Bw} +  \boldsymbol{x} ) \, ,
    \label{appendix-eq:rotation-equivariant-subspace-sim}
\end{align}
no longer holds for all subspaces. We shall prove that the above holds \emph{if and only if} our subspace is closed under rotations, 
\begin{align}
    \forall \, \boldsymbol{R}\in SO(3), \  \forall \, \boldsymbol{z}\in\mathbb{R}^m, \  \exists \, \boldsymbol{w}\in\mathbb{R}^m, \,  \boldsymbol{Bz} = \repR \boldsymbol{B} \boldsymbol{w}  \ .\quad 
\label{appenix-eq:closed-under-rotations}
\end{align}

\subsubsection{Forward Proof}
Given that our subspace satisfied closure under rotations (Eq. \eqref{appenix-eq:closed-under-rotations}), we prove that Eq. \eqref{appendix-eq:rotation-equivariant-subspace-sim} holds. 

\paragraph{Theorem: 
$\eqref{appenix-eq:closed-under-rotations} \rightarrow \eqref{appendix-eq:rotation-equivariant-subspace-sim} $} \textbf{Proof:}
Consider (without loss of generality) some rotation $\boldsymbol{R}$ and position $\boldsymbol{x}$.
Let $\boldsymbol{z}^*$ be the minimizer of the left hand side of Eq. \eqref{appendix-eq:rotation-equivariant-subspace-sim}:
\begin{align}
\boldsymbol{z}^* = \argmin_{\boldsymbol{z}} E(\boldsymbol{Bz}+  \repR\boldsymbol{x} ) \label{appendix-eq:definition-z-star}
\end{align}
By closure of our subspace under rotations, 
$ \exists \, \boldsymbol{w}^* \, , \  \boldsymbol{Bz^*} = \repR\boldsymbol{Bw^*}. $

The minimum energy is thus
\begin{align} 
E^* = E(\boldsymbol{Bz}^* + \repR \boldsymbol{x}) = E(\repR\boldsymbol{B} \boldsymbol{w}^*  + \repR \boldsymbol{x}) 
=E(\boldsymbol{B} \boldsymbol{w}^* +  \boldsymbol{x})
\nonumber 
\end{align}
where (from left to right) the equalities use that $\boldsymbol{z}^*$ is a minimizer, $\boldsymbol{B}$ is closed under rotations, and $E(\cdot)$ is rotation invariant, respectively. Consequently, $\boldsymbol{w}^* = \argmin_{\boldsymbol{w}} E(\boldsymbol{Bw}+  \boldsymbol{x} )$, which we substitute along with \eqref{appendix-eq:definition-z-star} into \eqref{appenix-eq:closed-under-rotations} to complete the proof.$\qed$

% \subsubsection{old route by contradiction:}
% Now we consider evaluating the energy at $\boldsymbol{z}^{*}$:
% \begin{align} 
% E(\boldsymbol{Bz}^* + \repR \boldsymbol{x}) = E(\repR\boldsymbol{B} \boldsymbol{w}^*  + \repR \boldsymbol{x}) = E^* \nonumber 
% \end{align}

% We know that $\boldsymbol{z}^*$ is a minimizer of this energy by construction, we prove that $\boldsymbol{w}^*$ is as well by contradiction.

% Suppose:
% \begin{align}
% \exists \, \boldsymbol{\tilde{w}} \quad  \text{s.t.} \, E(\repR\boldsymbol{B} \boldsymbol{\tilde{w}}  + \repR \boldsymbol{x}) < E^* \nonumber
% \end{align}
% Recalling closure of our subspace under rotations:
% \begin{align}
% \exists \boldsymbol{\tilde{z}} 
%  \quad \text{s.t.} \boldsymbol{B} \boldsymbol{\tilde{z}} =\repR \boldsymbol{B} \boldsymbol{\tilde{w}}  \nonumber
% \end{align}
% And by consequence:
% \begin{align}
% E(\boldsymbol{B} \boldsymbol{\tilde{z}} + \boldsymbol{R} \boldsymbol{x}) < E^*  \nonumber 
% \end{align}
% There exists an energy that is smaller than that obtained at $\boldsymbol{z}^*$, contradicting our construction of $\boldsymbol{z}^*$ from Equation \ref{appendix-eq:definition-z-star}. Therefore $\boldsymbol{w}^*$ is the minimizer of $E(\repR\boldsymbol{B} \boldsymbol{w}  + [\boldsymbol{R]} \boldsymbol{x}) $.
% With the above, we can use the rotation invariance of our energy to rewrite:
% \begin{align}
% \boldsymbol{w}^*
% &= \argmin_{\boldsymbol{w}} E(\repR\boldsymbol{B} \boldsymbol{w}  + \repR 
% \boldsymbol{x} )  \nonumber  \\
% &= \argmin_{\boldsymbol{w}} E(\boldsymbol{B} \boldsymbol{w}  + 
% \boldsymbol{x} ) \nonumber 
% \end{align}
% Recalling:
% \begin{align}
% \boldsymbol{B} \boldsymbol{z}^* &= \repR \boldsymbol{B} \boldsymbol{w}^*  \nonumber \\
% \boldsymbol{B} \argmin_{\boldsymbol{z}} E(\boldsymbol{B} \boldsymbol{z}  + 
% \repR \boldsymbol{x} ) &= \repR \boldsymbol{B} \argmin_{\boldsymbol{w}} E(\boldsymbol{B} \boldsymbol{w}  + 
% \boldsymbol{x} )  \qed  \nonumber 
% \end{align}

\subsubsection{Backward Proof}
Given our subspace simulation is rotation equivariant (Eq. \eqref{appendix-eq:rotation-equivariant-subspace-sim}), we prove that Eq. \eqref{appenix-eq:closed-under-rotations} holds.

\paragraph{Lemma} 
\begin{align}
\forall \boldsymbol{R}^* \in \mathcal{SO}(3),  \forall \boldsymbol{z}^* \in \mathbb{R}^{m},\\
\exists \boldsymbol{x}^*\, , \ \ \argmin_{\boldsymbol{z}} E(\boldsymbol{Bz} + \rep{\R^*}\boldsymbol{x}^*) = \boldsymbol{z}^* \, . \end{align}

\textbf{Proof:} 
Let $\boldsymbol{\tilde{z}}$ be some minimizer of $E(\boldsymbol{Bz})$ by construction. 
To satisfy the Lemma we choose
$\boldsymbol{x}^* = \rep{\R^*}^{-1} \boldsymbol{B} (\boldsymbol{\tilde{z}} - \boldsymbol{z}^*)$, because
\begin{align}
\argmin_{\boldsymbol{z}} E(\boldsymbol{Bz} + \rep{\R^*} \boldsymbol{x}^*) \nonumber  = \\
\argmin_{\boldsymbol{z}} E(\boldsymbol{Bz} + \rep{\R^*}\rep{\R^*}^{-1}\boldsymbol{B} (\boldsymbol{\tilde{z}} - \boldsymbol{z}^*))  = \\
\argmin_{\boldsymbol{z}} E(\boldsymbol{B} (\boldsymbol{z} + \boldsymbol{\tilde{z}} - \boldsymbol{z}^*)) \, .
\nonumber 
\end{align}
Thus $w = \boldsymbol{z} + \boldsymbol{\tilde{z}} - \boldsymbol{z}^*$ minimizes $E(\boldsymbol{Bw})$, and so does $w = \boldsymbol{\tilde{z}}$ (by construction). Since $w=w$,  then $\boldsymbol{z} = \boldsymbol{z}^*$ minimizes $E(\boldsymbol{B}\boldsymbol{z} + \rep{\R^*} \boldsymbol{x}^* )  \qed$.

%Thus $w = \boldsymbol{z} + \boldsymbol{\tilde{z}} - \boldsymbol{z}^*$ minimizes $E(\boldsymbol{Bw})$, and so does $w = \boldsymbol{\tilde{z}}$ (by construction). Since $w=w$,  then $\boldsymbol{z} = \boldsymbol{z}^*$ minimizes $E(\boldsymbol{B}\boldsymbol{z} + \rep{\R^*} \boldsymbol{x}^* )  \qed$.


% \textbf{Proof:} 
% % \Eitan{this proof can be further simplified by choosing $\boldsymbol{\tilde{x}}=0$, which makes
% % that symbol disappear from the proof. to do ....}
% Begin with some arbitrarily chosen $\boldsymbol{\tilde{x}} \in \mathbb{R}^{3n}$, and let
% \begin{align}
% \boldsymbol{\tilde{z}}  = \argmin_{\boldsymbol{z}} E(\boldsymbol{Bz} + \rep{\R^*} \boldsymbol{\tilde{x}})  \,  .\label{eq:definition-z-tilde}
% \end{align}

% To satisfy the Lemma we choose
% $\boldsymbol{x}^* = \boldsymbol{\tilde{x}} + \rep{\R^*}^{-1} \boldsymbol{B} (\boldsymbol{\tilde{z}} - \boldsymbol{z}^*)$, because
% \begin{align}
% \argmin_{\boldsymbol{z}} E(\boldsymbol{Bz} + \rep{\R^*} \boldsymbol{x}^*) \nonumber  &=
% \argmin_{\boldsymbol{z}} E(\boldsymbol{Bz} + \rep{\R^*} \boldsymbol{\tilde{x}} + \boldsymbol{B} (\boldsymbol{\tilde{z}} - \boldsymbol{z}^*))  \nonumber  \\
% &= \argmin_{\boldsymbol{z}} E(\boldsymbol{B} (\boldsymbol{z} + \boldsymbol{\tilde{z}} - \boldsymbol{z}^*)+ \rep{\R^*} \boldsymbol{\tilde{x}} ) \, .
% \nonumber 
% \end{align}
% Thus $w = \boldsymbol{z} + \boldsymbol{\tilde{z}} - \boldsymbol{z}^*$ minimizes $E(\boldsymbol{Bw} + \rep{\R^*} \boldsymbol{\tilde{x}})$, and, by \eqref{eq:definition-z-tilde}, so does $w = \boldsymbol{\tilde{z}}$. Equating these expressions for $w$, we see that $\boldsymbol{z} = \boldsymbol{z}^*$ minimizes $E(\boldsymbol{B}\boldsymbol{z} + \rep{\R^*} \boldsymbol{x}^* )  \qed$.


\paragraph{Theorem: 
$\eqref{appendix-eq:rotation-equivariant-subspace-sim} \rightarrow \eqref{appenix-eq:closed-under-rotations}$} \textbf{Proof:}
For any given $\boldsymbol{R}^*$, $\boldsymbol{z}^*$, we produce $\boldsymbol{w}^*$ satisfying
\begin{align}
\boldsymbol{B} \boldsymbol{z}^* = \rep{\R^*} \boldsymbol{B} \boldsymbol{w}^*  \, . \nonumber
\end{align}

Pick a specific $\boldsymbol{R}^*, \boldsymbol{z}^*$. Using the Lemma, 
we pick $\boldsymbol{x}^*$ such that $\boldsymbol{z}^*$ 
minimizes $E(\boldsymbol{Bz} + \rep{\R^*}\boldsymbol{x}^*)$.
By the rotation equivariance of our minimization,
\begin{align}
\boldsymbol{B} \argmin_{\boldsymbol{z}} E(\boldsymbol{B} \boldsymbol{z}  + 
\rep{\R^*} \boldsymbol{x}^*) &=  \rep{\R^*} \boldsymbol{B} \argmin_{\boldsymbol{w}} E(\boldsymbol{B} \boldsymbol{w}  + \boldsymbol{x}^* ) \nonumber \\
\boldsymbol{B} \boldsymbol{z}^*  &=  \rep{\R^*} \boldsymbol{B} \argmin_{\boldsymbol{w}} E(\boldsymbol{B} \boldsymbol{w}  + 
\boldsymbol{x}^*)  \nonumber \\
&=  \rep{\R^*} \boldsymbol{B}\boldsymbol{w}^* \, ,
\nonumber 
\end{align}
where we have chosen
$\boldsymbol{w}^*  = \argmin_{\boldsymbol{w}} E (\boldsymbol{B} \boldsymbol{w} + \boldsymbol{x}^*) \, .\qed
$


% We want to prove that a rotation equivariant linear subspace will ensure a rotation equivariant simulation.

% A rotation equivariant subspace satisfies that 
% \begin{align}
%    \exists  \quad \mathcal{R} \in \mathbb{R}^{m \times m} \quad \text{s.t.} \quad \repR \boldsymbol{B} \boldsymbol{z}  = \boldsymbol{B} \boldsymbol{\mathcal{R}}  \boldsymbol{z}  \quad \forall \boldsymbol{R} \in \mathcal{SO}(3) , \boldsymbol{z} \in \mathbb{R}^m \nonumber 
% \end{align}
% Where $\boldsymbol{B}$ is our subspace basis and $\repR \in \mathbb{R}^{3n \times 3n}$ is a block diagonal matrix with the rotation matrix $\boldsymbol{R}$ repeated along the diagonal .


% A rotation equivariant simulation, with elastic energy $E(\boldsymbol{u}, \boldsymbol{x}_0)$, which depends on the rest positions $\boldsymbol{x}_0 \in \mathbb{R}^{3n}$ and a displacement field $\boldsymbol{u} \in \mathbb{R}^{3n}$,  satisfies that :
% \begin{align}
%     \boldsymbol{B} \argmin_{\boldsymbol{z}} E(\boldsymbol{Bz}+  \repR\boldsymbol{x}_0 ) = \repR \boldsymbol{B} \argmin_{\boldsymbol{z}} E(\boldsymbol{Bz} +  \boldsymbol{x}_0 ) \quad \forall \boldsymbol{R}  \in \mathcal{SO}(3) \nonumber
% \end{align}
% We wish to show that the above is true if $\boldsymbol{B}$ is rotation equivariant.

% Starting with the left hand side, and making use of the common assumption that our elastic energy is rotation invariant ($E(\boldsymbol{u} +\repR \boldsymbol{x}_0) = E( \repR^{-1}\boldsymbol{u}+ \boldsymbol{x}_0)$):
% \begin{align}
% \boldsymbol{B} \argmin_{\boldsymbol{z}} E(\boldsymbol{Bz} +  \repR\boldsymbol{x}_0 ) &=  \boldsymbol{B} \argmin_{\boldsymbol{z}} E([\boldsymbol{R}^{-1}]\boldsymbol{Bz}+  \boldsymbol{x}_0 ) \nonumber  \end{align}
% Assuming our subspace can represent rotations of itself: 
% \begin{align}
% &=  \boldsymbol{B} \argmin_{\boldsymbol{z}} E(\boldsymbol{B} \mathcal{R}^{-1} \boldsymbol{z} +  \boldsymbol{x}_0 ) \nonumber \end{align}
% Introducing an auxiliary variable $\boldsymbol{z}' = \mathcal{R}^{-1} \boldsymbol{z}$
% \begin{align}
% &=  \boldsymbol{B}  {\mathcal{R}} \argmin_{\boldsymbol{z}'} E(\boldsymbol{B} \boldsymbol{z}' +  \boldsymbol{x}_0 ) \nonumber
% \end{align}
%  Again, leveraging rotation equivariance to move $\mathcal{R}$ to the left of $\boldsymbol{B}$:
%  \begin{align}
%   &=  \repR \boldsymbol{B}  \argmin_{\boldsymbol{z}'} E(\boldsymbol{B} \boldsymbol{z}' +   \boldsymbol{x}_0 ) \nonumber   \end{align}
% Which proves that a rotation equivariant subspace leads to a rotation equivariant simulation.


% \subsection{ Necessary Conditions for Rotation Equivariance}
% \label{sec:appendix-proof-necessary-conditions-for-rotation-equivariance}
% What are the necessary conditions for subspace basis $\boldsymbol{B} \in \mathbb{R}^{3n \times m}$ to be rotation equivariant? Rotation equivariance requires that
% \begin{align}
% \forall\, \boldsymbol{R} \in SO(3)\quad \exists \, \boldsymbol{\bar{R}} 
%  \in \mathbb{R}^{m \times m} \quad  \text{s.t.} 
%  \quad \repR \boldsymbol{B} = \boldsymbol{B} \boldsymbol{\bar{R}}  \ ,
%  \label{eq:rotation-equivariance-property}
% \end{align}
% where  $\repR \in \mathbb{R}^{3n \times 3n}$ is block diagonal with repeated blocks $\boldsymbol{R}$.

% We can treat satisfying the property in Equation \ref{eq:rotation-equivariance-property} as trying to solve for each of the $m$ columns of $\boldsymbol{\bar{R}}$ that uphold rotation equivariance:
% \begin{align}
%     \boldsymbol{B} \boldsymbol{\bar{R}}^j = \repR \boldsymbol{B}^j \quad \forall j \in \{1, m\}
% \end{align}
% Each of these $m$ systems of equations have the same tall rectangular $3n \times m$ system matrix $\boldsymbol{B}$, making them all over-determined. These systems only admit \emph{exact} solutions if the rotated deformation $\repR\boldsymbol{B}^j$ lies within the column space $\mathrm{Col}(\boldsymbol{B})$ of the system matrix $\boldsymbol{B}$ \cite{strang2006linear}. In other words, any motion that lies in our subspace, must \emph{still} lie in our subspace when rotated. $\forall\, u \in \mathrm{Col}(\boldsymbol{B}),\ \repRu \in \mathrm{Col}(\boldsymbol{B})$
% and in particular:
% \begin{align}
%  \repR \boldsymbol{B}^j \in \mathrm{Col}(\boldsymbol{B}) \quad  \forall j \in \{1, m\}
% \end{align}
% Leveraging that $\repR\boldsymbol{B}^j$ is linear in each of the 9 rotation parameters of $\boldsymbol{R}$, we can rewrite the condition above in terms of the flattened $\boldsymbol{r} = \text{vec}(\boldsymbol R)$:

% \begin{align}
% \repR\boldsymbol{B}^j 	= 
% (\boldsymbol{I}_3 \otimes 
% \begin{bmatrix}
% \boldsymbol{B}^j_x & \boldsymbol{B}^j_y & \boldsymbol{B}^j_z
% \end{bmatrix} )
% \boldsymbol{r} \in  \mathrm{Col}(\boldsymbol{B})
% \nonumber
% \end{align}
% Where we have split our subspace column $\boldsymbol{B}^j$ into its three dimensional components:
% \begin{align}
% \boldsymbol{B} = \begin{bmatrix} \boldsymbol{B}_x \\ \boldsymbol{B}_y \\ \boldsymbol{B}_z\end{bmatrix} \quad \boldsymbol{B}_d \in \mathbb{R}^{n \times m} \forall d \in \{x, y, z\} \nonumber 
% \end{align}


% We can expand the kronecker product to get a better idea for what this means:
% \begin{align}
% \begin{bmatrix}
% \boldsymbol{B}^j_x & \boldsymbol{B}^j_y & \boldsymbol{B}^j_z & & & & &  & \\
% & & & \boldsymbol{B}^j_x & \boldsymbol{B}^j_y & \boldsymbol{B}^j_z & & & \\
% & & &  & & & \boldsymbol{B}^j_x & \boldsymbol{B}^j_y & \boldsymbol{B}^j_z
% \end{bmatrix}
% \boldsymbol{r} \in \mathrm{Col}( \boldsymbol{B}) \nonumber
% \end{align}
% Assuming  the ceofficients in $\boldsymbol{r}$  are non-zero and follow an arbitrary distribution, \Otman{We should be assuming arbitrary rotations, not random linear parameters} we can omit $\boldsymbol{r}$ and satisfy the constraint column by column to obtain a set of 9 constraints:
% \begin{align}
% \begin{matrix}
% \begin{bmatrix}
%  \boldsymbol{B}_x^j \\
%  \boldsymbol{0} \\
%  \boldsymbol{0}
%  \end{bmatrix} \in  \mathrm{Col}( \boldsymbol{B})  & 
%   \begin{bmatrix}
%  \boldsymbol{B}_y^j \\
%  \boldsymbol{0} \\
%  \boldsymbol{0}
%  \end{bmatrix} \in  \mathrm{Col}( \boldsymbol{B}) &
%   \begin{bmatrix}
%  \boldsymbol{B}_z^j \\
%  \boldsymbol{0} \\
%  \boldsymbol{0}
%  \end{bmatrix} \in \mathrm{Col}( \boldsymbol{B}) \\
%  \begin{bmatrix}
%  \boldsymbol{0} \\
%   \boldsymbol{B}_x^j \\
%  \boldsymbol{0}
%  \end{bmatrix} \in \mathrm{Col}( \boldsymbol{B}) & 
%   \begin{bmatrix}
%  \boldsymbol{0} \\
%  \boldsymbol{B}_y^j \\
%  \boldsymbol{0}
%  \end{bmatrix} \in  \mathrm{Col}( \boldsymbol{B}) &
%   \begin{bmatrix}
%  \boldsymbol{0} \\
%   \boldsymbol{B}_z^j \\
%  \boldsymbol{0}
%  \end{bmatrix} \in  \mathrm{Col}( \boldsymbol{B}) \\
%  \begin{bmatrix}
%  \boldsymbol{0} \\
%  \boldsymbol{0} \\
%    \boldsymbol{B}_x^j 
%  \end{bmatrix} \in  \mathrm{Col}( \boldsymbol{B}) & 
%  \begin{bmatrix}
%  \boldsymbol{0} \\
%  \boldsymbol{0} \\
%   \boldsymbol{B}_y^j 
%  \end{bmatrix} \in  \mathrm{Col}( \boldsymbol{B}) &
%  \begin{bmatrix}
%  \boldsymbol{0} \\
%  \boldsymbol{0} \\
%    \boldsymbol{B}_z^j 
%  \end{bmatrix} \in  \mathrm{Col}( \boldsymbol{B}) 
% \end{matrix} \quad \forall j \in \{1, ... m\}
% \end{align}

% \subsection{A Rotation Spanning Subspace is Rotation Equivariant}
% We prove that any rotation spanning subspace is also rotation equivariant.

% A rotation spanning deformation subspace $\boldsymbol{B}$ satisfies that:
% \begin{align}\forall \boldsymbol{R} \in \mathcal{SO}(3) \quad 
% \exists \, \mathcal{R} \in \mathbb{R}^{m \times m} \quad \text{s.t.} \quad  \repR \boldsymbol{x}_0 = \boldsymbol{x}_0 + \boldsymbol{B}\mathcal{R} \boldsymbol{z}  \nonumber
% \end{align}

% We want to show that if the above is true, then $\boldsymbol{B}$ is also rotation equivariant:

% \begin{align}
% \forall\, \boldsymbol{R} \in \mathcal{SO}(3)\quad \exists \, \mathcal{R} 
%  \in \mathbb{R}^{m \times m} \quad  \text{s.t.} 
%  \quad \repR \boldsymbol{B} = \boldsymbol{B} \mathcal{R}  \ ,
%  \label{eq:rotation-equivariance-property}
% \end{align}

% We start with our definition of a rotation spanning subspace:
% \begin{align}
% \underbrace{\repR \boldsymbol{x}_{0} - \boldsymbol{x}_0}_{\boldsymbol{u}} =  \boldsymbol{B}\mathcal{R} \boldsymbol{z}  \nonumber \\
%  \begin{bmatrix}
%  \boldsymbol{B} &
%  \boldsymbol{C}
%  \end{bmatrix} 
%  \begin{bmatrix}
%   \boldsymbol{w} \\
%  \boldsymbol{y}
%  \end{bmatrix}
%  = \boldsymbol{B}\mathcal{R} \boldsymbol{z}  \nonumber 
% \end{align}



\section{Matrix Derivations}
\subsection{Weight Space Skinning Jacobian Products}
\label{appendix-sec:weight-space-skinning-jacobians}
The product $\boldsymbol{J}^T\boldsymbol{B}_{lbs}$ frequently arises in our derivations. Here $\boldsymbol{J} \in \mathbb{R}^{n(d) \times c}$ can be a generic matrix and $\boldsymbol{B}_{lbs} \in \mathbb{R}^{n(d) \times d(d+1)(m)}$ is our Linear Blend Skinning Jacobian matrix, which depends linearly on some set of weights $\boldsymbol{W} \in \mathbb{R}^{n(d) \times m}$. We leverage the relationship between $\boldsymbol{B}_{lbs}$ and $\boldsymbol{W}$ to rewrite the entries of the matrix product $\boldsymbol{J}^T \boldsymbol{B}_{lbs}$ entirely in terms of the weights:
\begin{align}
      \boldsymbol{B}_{lbs} = (\boldsymbol{I_d} \otimes \underbrace{((\boldsymbol{1}_m^T \otimes [\boldsymbol{V} \boldsymbol{1}_n])   \odot  (\boldsymbol{W} \otimes \boldsymbol{1}_{d+1}^T))}_{\boldsymbol{\mathcal{B}}}) \nonumber
\end{align}
 We assume the 3D case $d=3$, and restate the final result 2D. The full product whose entries we wish to relate linearly to our weights $\boldsymbol{W}$ is
\begin{align}
    \boldsymbol{J}^T (\boldsymbol{I_3} \otimes \mathcal{B}) 
\end{align}
Separating the mutiplying matrix into its 3 sets of columns:
\begin{align}
\boldsymbol{J}^T  = [\boldsymbol{J}_x^T  \; | \; \boldsymbol{J}_y^T  \; | \; \boldsymbol{J}_z^T ]
\end{align}
Expanding the rows of this product:
\begin{align}
    \begin{bmatrix}\boldsymbol{J}^T_x  \mathcal{B} \; |
    \; \boldsymbol{J}^T_y \mathcal{B} \; | 
    \; \boldsymbol{J}^T_z \mathcal{B} 
    \end{bmatrix} 
\end{align}
We focus on one set of columns $i \in \{x, y, z\}$ at a time:
\begin{align}
\boldsymbol{J}^T_i \mathcal{B} =  \boldsymbol{J}^T_i ((\boldsymbol{1}_m^T \otimes [\boldsymbol{X} \boldsymbol{1}_n])   \odot  (\boldsymbol{W} \otimes \boldsymbol{1}_{4}^T)))  \in \mathbb{R}^{j \times (4)m} \nonumber
\\
  \boldsymbol{J}^T_i ((\boldsymbol{1}_m^T \otimes [\boldsymbol{\bar{x}} \,\boldsymbol{\bar{y}} \, \boldsymbol{\bar{z}} \, \boldsymbol{1}_n])   \odot  ([\boldsymbol{w}_1 \boldsymbol{w}_2 ... \boldsymbol{w}_m] \otimes \boldsymbol{1}_{4}^T))) \nonumber
 \end{align}

 We recall the definition of the Linear Blend Skinning Jacobian
  where we split 
  
  $\boldsymbol{X} = [\boldsymbol{\bar{x}} \,\boldsymbol{\bar{y}} \, \boldsymbol{\bar{z}} ]$ into its individual columns.
 Expanding the two Kronecker products:
 \begin{align}
 \boldsymbol{J}^T_i ( [\underbrace{\boldsymbol{\bar{x}} \,\boldsymbol{\bar{y}} \, \boldsymbol{\bar{z}} \, \boldsymbol{1}_n}_{1} | ... | \underbrace{\boldsymbol{\bar{x}} \,\boldsymbol{\bar{y}} \, \boldsymbol{\bar{z}} \, \boldsymbol{1}_n}_b ]   \odot  [\underbrace{\boldsymbol{w}_1 \boldsymbol{w}_1 \boldsymbol{w}_1 \boldsymbol{w}_1}_1  | ... | \underbrace{\boldsymbol{w}_m \boldsymbol{w}_m \boldsymbol{w}_m \boldsymbol{w}_m}_{m}] )  \nonumber
 \end{align}
Distributing the products further:
 \begin{align}
  \boldsymbol{J}^T_i [\underbrace{([\boldsymbol{\bar{x}} \,\boldsymbol{\bar{y}} \, \boldsymbol{\bar{z}} \, \boldsymbol{1}_n] \odot  [\boldsymbol{w}_1 \boldsymbol{w}_1 \boldsymbol{w}_1 \boldsymbol{w}_1]}_1| ... | \underbrace{[\boldsymbol{\bar{x}} \,\boldsymbol{\bar{y}} \, \boldsymbol{\bar{z}} \, \boldsymbol{1}_n] \odot  [\boldsymbol{w}_m \boldsymbol{w}_m \boldsymbol{w}_m \boldsymbol{w}_m]}_m]  )  \nonumber
 \end{align}
Moving the matrix product inside, and looking at one block $q \in [1, ... m]$ at a time:
 \begin{align}
 & \boldsymbol{J}^T_i ( [ \boldsymbol{\bar{x}} \,\boldsymbol{\bar{y}} \, \boldsymbol{\bar{z}} \, \boldsymbol{1}_n] \odot  [\boldsymbol{w}_q \boldsymbol{w}_q \boldsymbol{w}_q \boldsymbol{w}_q])  \nonumber \\
 &= \boldsymbol{J}^T_d ([\boldsymbol{\bar{x}} \odot\boldsymbol{w}_q |  \boldsymbol{\bar{y}} \odot \boldsymbol{w}_q |\boldsymbol{\bar{z}} \odot \boldsymbol{w}_q | \boldsymbol{1}_n \odot \boldsymbol{w}_q]) \nonumber  
 \end{align}
 Rewriting the individual Hadamard products as diagonal matrix multiplication where we introduce the capitalized overbar notation $\boldsymbol{\bar{X}}, \boldsymbol{\bar{Y}}, \boldsymbol{\bar{Z}}$ to indicate the diagonal matrices whose diagonal entries are $\boldsymbol{\bar{x}},\boldsymbol{\bar{y}}, \boldsymbol{\bar{z}} $ respectively.
 \begin{align}
 &= \boldsymbol{J}^T_d ([\boldsymbol{\bar{X}}\boldsymbol{w}_q |  \boldsymbol{\bar{Y}}\boldsymbol{w}_q | \boldsymbol{\bar{Z}} \boldsymbol{w}_q |  \boldsymbol{w}_q])  \nonumber \\
  &= [ \boldsymbol{J}^T_d \boldsymbol{\bar{X}}\boldsymbol{w}_q |   \boldsymbol{J}^T_d \boldsymbol{\bar{Y}}\boldsymbol{w}_q |  \boldsymbol{J}^T_d \boldsymbol{\bar{Z}} \boldsymbol{w}_q | \boldsymbol{J}^T_d  \boldsymbol{w}_q]  \nonumber
 \end{align}
Applying this for all weights $b$:
\begin{align}
[\boldsymbol{J}^T_i \boldsymbol{\bar{X}} \boldsymbol{W} |   \boldsymbol{J}^T_i \boldsymbol{\bar{Y}} \boldsymbol{W} |  \boldsymbol{J}^T_i \boldsymbol{\bar{Z}} \boldsymbol{W} | \boldsymbol{J}^T_i  \boldsymbol{W}]  \nonumber
 \end{align}
 Finally, applying this to all 3 dimensions of $i$:
 \begin{align}
 \boldsymbol{J}^T \boldsymbol{B}_{lbs} = 
[& \boldsymbol{J}^T_x \boldsymbol{\bar{X}} \boldsymbol{W} \; | \;   \boldsymbol{J}^T_x \boldsymbol{\bar{Y}} \boldsymbol{W} \; | \;  \boldsymbol{J}^T_x \boldsymbol{\bar{Z}} \boldsymbol{W} \; | \; \boldsymbol{J}^T_x  \boldsymbol{W}] \; |\;  ... \nonumber  \\
& \boldsymbol{J}^T_y \boldsymbol{\bar{X}} \boldsymbol{W} \; |  \;  \boldsymbol{J}^T_y \boldsymbol{\bar{Y}} \boldsymbol{W} \; | \;  \boldsymbol{J}^T_y \boldsymbol{\bar{Z}}  \boldsymbol{W} \; | \; \boldsymbol{J}^T_y  \boldsymbol{W}] \; |\;  ... \nonumber
\\
& \boldsymbol{J}^T_z \boldsymbol{\bar{X}}  \boldsymbol{W} \; |\;    \boldsymbol{J}^T_z \boldsymbol{\bar{Y}} \boldsymbol{W} \; |\;   \boldsymbol{J}^T_z \boldsymbol{\bar{Z}}  \boldsymbol{W} \; | \; \boldsymbol{J}^T_z  \boldsymbol{W}] 
 \end{align}

We can derive our weight Skinning Jacobians by rewriting the entries  $\boldsymbol{B}_{lbs}$ alone in terms of the weights $\boldsymbol{W}$ as a special case of the above result.
If we choose $\boldsymbol{J}$ to be the identity, and slice out the 3 sets of columns belonging to the identity, we obtain our dimensional selection matrices $\boldsymbol{J} = \boldsymbol{I}_{3n} = [\boldsymbol{P}_x \boldsymbol{P}_y  \boldsymbol{P}_z]$. We can then rewrite :

 \begin{align}
 \boldsymbol{B}_{lbs} = 
[& \boldsymbol{P}_x \boldsymbol{\bar{X}} \boldsymbol{W} \; | \;   \boldsymbol{P}_x \boldsymbol{\bar{Y}} \boldsymbol{W} \; | \;  \boldsymbol{P}_x \boldsymbol{\bar{Z}} \boldsymbol{W} \; | \; \boldsymbol{P}_x  \boldsymbol{W}] \; |\;  ... \nonumber  \\
& \boldsymbol{P}_y \boldsymbol{\bar{X}} \boldsymbol{W} \; |  \;  \boldsymbol{P}_y \boldsymbol{\bar{Y}} \boldsymbol{W} \; | \;  \boldsymbol{P}_y \boldsymbol{\bar{Z}}  \boldsymbol{W} \; | \; \boldsymbol{P}_y  \boldsymbol{W}] \; |\;  ... \nonumber
\\
& \boldsymbol{P}_z \boldsymbol{\bar{X}}  \boldsymbol{W} \; |\;    \boldsymbol{P}_z \boldsymbol{\bar{Y}} \boldsymbol{W} \; |\;   \boldsymbol{P}_z \boldsymbol{\bar{Z}}  \boldsymbol{W} \; | \; \boldsymbol{P}_z  \boldsymbol{W}] 
 \end{align}
Where we identify  12 weight-Space Skinning Jacobian matrices:
\begin{align}
\begin{matrix}
\boldsymbol{A}_{1, 1} = \boldsymbol{P_x} \boldsymbol{\bar{X}}  &\boldsymbol{A}_{1, 2} = \boldsymbol{P_x} \boldsymbol{\bar{Y}} &\boldsymbol{A}_{1, 3} = \boldsymbol{P_x} \boldsymbol{\bar{Z}} &
\boldsymbol{A}_{1, 4} = \boldsymbol{P_x} \\
\boldsymbol{A}_{2, 1} = \boldsymbol{P_y} \boldsymbol{\bar{X}} &\boldsymbol{A}_{2, 2} = \boldsymbol{P_y} \boldsymbol{\bar{Y}} &\boldsymbol{A}_{2, 3} = \boldsymbol{P_y} \boldsymbol{\bar{Z}} &
\boldsymbol{A}_{2, 4} = \boldsymbol{P_y} \\
\boldsymbol{A}_{3, 1} = \boldsymbol{P_z} \boldsymbol{\bar{X}}  &\boldsymbol{A}_{3, 2} = \boldsymbol{P_z} \boldsymbol{\bar{Y}} &\boldsymbol{A}_{3, 3} = \boldsymbol{P_z} \boldsymbol{\bar{Z}} &
\boldsymbol{A}_{3, 4} = \boldsymbol{P_z}
\end{matrix}
\label{eq:weight-space-skinning-jacobian-3D}
\end{align}
We can derive the 2D case of these matrices by following the equivalent steps as above, but with one less set of columns for $\boldsymbol{I}$ and one less column in our rest positions $\boldsymbol{X}$:
\begin{align}
\begin{matrix}
\boldsymbol{A}_{1, 1} = \boldsymbol{P_x} \boldsymbol{\bar{X}}  &\boldsymbol{A}_{1, 2} = \boldsymbol{P_x} \boldsymbol{\bar{Y}} &\boldsymbol{A}_{1, 3} = \boldsymbol{P_x}  \\
\boldsymbol{A}_{2, 1} = \boldsymbol{P_y} \boldsymbol{\bar{X}} &\boldsymbol{A}_{2, 2} = \boldsymbol{P_y} \boldsymbol{\bar{Y}} &\boldsymbol{A}_{2, 3} = \boldsymbol{P_y}  
\end{matrix}
\label{eq:weight-space-skinning-jacobian-2D}
\end{align}

\section{Hyper-Reduced Local Global Solvers for Different Elastic Energies}
\subsection{Hyper-Reduced ARAP}
\label{sec:appendix-hyper-reduced-arap-elasticity}
The full space ARAP elastic energy can efficiently be written as:
\begin{align}
    E_{ARAP}(\boldsymbol{x}) = \sum_t^{k} m_t  \mu_t || \boldsymbol{F}_t -  \boldsymbol{R}_t||^2_F,
\end{align}
where $\boldsymbol{F}_t$ is our per-tet deformation gradient (a linear function of our degrees of freedom) and $\boldsymbol{R}_t$ is a best-fit rotation on the deformation gradient, a non-linear function of our degrees of freedom.
Expanding the square Frobemius norm, we can identify a quadratic component and non-linear component to our energy:
\begin{align}
    & E_{ARAP}(\boldsymbol{x}) = \underbrace{\sum_t^{k} m_t \mu_t tr(\boldsymbol{F}_t^T\boldsymbol{F}_t)}_{\Psi}  + \underbrace{ \sum_t^{k}- 2 m_t \mu_t tr(\boldsymbol{F}^T_t \boldsymbol{R}_t)}_{\Phi} \nonumber \\
     &=  \Psi(\boldsymbol{u})  + \Phi(\boldsymbol{u}). \nonumber 
\end{align}
We can rewrite this energy in its hyper-reduced form by recalling $\boldsymbol{x} = \boldsymbol{Bz}$ and approximating $\Phi \approx \tilde{\Phi}$. We expand our clustered non-linear component to the elastic energy:
\begin{align}
    E_{ARAP}(\boldsymbol{z}) &=   \Psi(\boldsymbol{z})  + \underbrace{\sum_c^{r}   - 2 m_c \mu_c tr(\boldsymbol{F}^T_c \boldsymbol{R}_c)  }_{\tilde{\Phi}} \nonumber \\ 
     &=  \Psi(\boldsymbol{z})  + \tilde{\Phi}(\boldsymbol{z} ). 
\end{align}
We then expose the source of the non-linearities, the best fit per-cluster rotation matrices, as auxillary degrees of freedom.
\begin{align}
    E_{ARAP}(\boldsymbol{z}, \boldsymbol{\tilde{R}}) &=    \Psi(\boldsymbol{z})  + \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ).
\end{align}
The hyper reduced local step becomes:
\begin{align}
    \boldsymbol{\tilde{R}} &=  \argmin_{\boldsymbol{\tilde{R}}}   \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ) \nonumber \\ 
    &= \argmin_{\boldsymbol{\tilde{R}}} \sum_c^{k} m_c  \mu_c tr(  \boldsymbol{F}^T_c \boldsymbol{R}_c)  \nonumber \\
    &= \text{PolarSVD}(\boldsymbol{\tilde{F}}) \nonumber
\end{align}
which is found through polar decomposition of the per-cluster deformation gradient $\boldsymbol{\tilde{F}}$, obtainable via a reshape operation of $\boldsymbol{\tilde{f}} = \boldsymbol{x}^T \boldsymbol{K}^T \boldsymbol{G} $.
The hyper reduced global step minimizes
\begin{align}
      \boldsymbol{z} &= \argmin_{\boldsymbol{z}} \Psi(\boldsymbol{z})   + \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ) \nonumber \\
       &= \argmin_{\boldsymbol{z}} \boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{K}^T \mathcal{U} \boldsymbol{V} \boldsymbol{K} \boldsymbol{B} \boldsymbol{z} +  \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ) \nonumber \\
         &= \argmin_{\boldsymbol{z}} \boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} \boldsymbol{z} + \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ), \nonumber 
\end{align}
where we introduce the heterogeneous Laplacian matrix $\boldsymbol{L} = \boldsymbol{K}^T \mathcal{U} \boldsymbol{V} \boldsymbol{K}$ for brevity. Here, $\boldsymbol{K} \in \mathbb{R}^{9k \times 3n} $ is the vector gradient operator over our mesh, mapping deformed coordinates to per-tet deformation gradients, $\boldsymbol{V} \in \mathbb{R}^{9k \times 9k}$ is a diagonal matrix of tetrahedron volumes, $\mathcal{U}  = (\boldsymbol{I}_9  \otimes \text{diag}(\boldsymbol{\mu}))) \in \mathbb{R}^{9k \times 9k}$ is a diagonal containing first lamé parameter $\mu_t$ for each tet $t$ along its diagonal. 

For ARAP, the global-step optimization is \emph{exactly} quadratic in the degrees of freedom $\boldsymbol{z}$ (because $\tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}})$ is linear in $\boldsymbol{z})$, and can directly be found every local-global iteration via a single prefactorizable system solve:
\begin{align}
        \boldsymbol{0} &=  2 \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} \boldsymbol{z} + 
        \frac{\partial \boldsymbol{x}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}
        \frac{\partial \boldsymbol{\tilde{f}}}{\partial \boldsymbol{f}} 
        \frac{\partial \tilde{\Phi}}{\partial \boldsymbol{\tilde{f}}} \nonumber  \\
        &=  \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} \boldsymbol{z}  +
        \frac{1}{2} \boldsymbol{B}^T \boldsymbol{K}^T \boldsymbol{G}
        \frac{\partial \tilde{\Phi}}{\partial \boldsymbol{\tilde{f}}}  \nonumber  \\ 
           \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} \boldsymbol{z}  &= - \frac{1}{2} 
        \boldsymbol{B}^T \boldsymbol{K}^T \boldsymbol{G}
        \frac{\partial \tilde{\Phi}}{\partial \boldsymbol{\tilde{f}}}. \nonumber 
\end{align}
Above, we made use of the linear relationships $\boldsymbol f = \boldsymbol{Kx} $, $\boldsymbol x = \boldsymbol{Bz} $, and $\boldsymbol{\tilde{f}} = \boldsymbol{G}_9 \boldsymbol{f} $ to derive the chained partial derivatives   $\frac{\partial \boldsymbol{x}}{\partial \boldsymbol{z}}, \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}},   \frac{\partial \boldsymbol{\tilde{f}}}{\partial \boldsymbol{f}} $. The last unknown $  \frac{\partial \tilde{\Phi}}{\partial \boldsymbol{\tilde{f}}}$ changes each local-global iteration and can efficiently be computed during the local step when looping through each cluster:
\begin{align}
    \frac{\partial{\tilde{\Phi}}}{\partial \boldsymbol{\tilde{f}}} =\text{vec}(
    \begin{bmatrix} 
     \vdots \\
     - \frac{\partial}{\partial \boldsymbol{\tilde{F}}_c} m_{c} \mu_{c} tr(  \boldsymbol{\tilde{F}}^T_{c}\boldsymbol{\tilde{R}}_{c}) \\
     \vdots
    \end{bmatrix}) =
    \text{vec}(
    \begin{bmatrix} 
  \vdots \\  - m_{c} \mu_{c} \boldsymbol{R}_{c} \\ \vdots
    \end{bmatrix})  \nonumber  
    \\
     \text{vec}\left( 
    \ \begin{bmatrix} 
  \vdots \\   -m_{c} \mu_{c} \boldsymbol{R}_{c} \\ \vdots
    \end{bmatrix}\ \right)
\end{align}

\subsection{Hyper-Reduced Co-Rotational Elasticity}
\label{sec:appendix-hyper-reduced-corotational-elasticity}
A discrete linear co-rotational elastic energy is defined as
\begin{align}
    E_{CoRot}(\boldsymbol{u}) = \sum_t^{k} m_t  \mu_t || \boldsymbol{F}_t -  \boldsymbol{R}_t||^2_F +  m_t \frac{\lambda_t}{2}tr^2(\boldsymbol{R}_t^T \boldsymbol{F}_t - \boldsymbol{I}) \nonumber 
\end{align}
Where $\boldsymbol{F}_t$ is our per-tet deformation gradient (a linear function of our degrees of freedom) and $\boldsymbol{R}_t$ is a best-fit rotation on the deformation gradient, a non-linear function of our degrees of freedom.
Expanding the square Frobemius norm, we can identify a quadratic component and non-linear component to our energy:
\begin{align}
    & E_{CoRot}(\boldsymbol{u}) = \nonumber  \\
    & \underbrace{\sum_t^{k} m_t \mu_t tr(\boldsymbol{F}_t^T\boldsymbol{F}_t)}_{\Psi}  + \underbrace{ \sum_t^{k}- 2 m_t \mu_t tr(\boldsymbol{F}^T_t \boldsymbol{R}_t)  \nonumber +   m_t \frac{\lambda_t}{2}tr^2(\boldsymbol{R}_t^T \boldsymbol{F}_t - \boldsymbol{I})}_{\Phi} \\
     &=  \Psi(\boldsymbol{u})  + \Phi(\boldsymbol{u}).  \nonumber 
\end{align}
We can rewrite this energy in its hyper-reduced form by recalling $\boldsymbol{x} = \boldsymbol{Bz}$ and approximating $\Phi \approx \tilde{\Phi}$. We expand our clustered non-linear component to the elastic energy:
\begin{align}
    E_{CoRot}(\boldsymbol{z}) &=   \Psi(\boldsymbol{z})  + \underbrace{\sum_c^{\mathcal{C}}   - 2 m_c \mu_c tr(\boldsymbol{F}^T_c \boldsymbol{R}_c)  \nonumber +   m_c \frac{\lambda_c}{2}tr^2(\boldsymbol{R}_c^T \boldsymbol{F}_c - \boldsymbol{I})}_{\tilde{\Phi}} \nonumber \\ 
     &=  \Psi(\boldsymbol{z})  + \tilde{\Phi}(\boldsymbol{z}).  \nonumber
\end{align}

We then expose the source of the non-linearities, the best fit per-cluster rotation matrices, as auxillary degrees of freedom.
\begin{align}
    E_{CoRot}(\boldsymbol{z}, \boldsymbol{\tilde{R}}) &=    \Psi(\boldsymbol{z})  + \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ).  \nonumber 
\end{align}
The hyper reduced local step becomes:
\begin{align}
    \boldsymbol{\tilde{R}} &=  \argmin_{\boldsymbol{\tilde{R}}}   \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ) \nonumber \\ 
    &= \argmin_{\boldsymbol{\tilde{R}}} \sum_c^{\mathcal{C}} m_c  \mu_c tr(  \boldsymbol{F}^T_c \boldsymbol{R}_c) +  m_c \frac{\lambda_c}{2}tr^2(\boldsymbol{R}_c^T \boldsymbol{F}_c - \boldsymbol{I}) \nonumber \\
    &= \text{PolarSVD}(\boldsymbol{\tilde{F}})
    \nonumber 
\end{align}
which is found through polar decomposition of the per-cluster deformation gradient $\boldsymbol{\tilde{F}}$, obtainable via a reshape operation of $\boldsymbol{\tilde{f}} = \boldsymbol{x}^T \boldsymbol{K}^T \boldsymbol{G} $ .

The hyper reduced global step minimizes:
\begin{align}
      \boldsymbol{z} &= \argmin_{\boldsymbol{z}} \Psi(\boldsymbol{z})   + \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ) \nonumber \\
       &= \argmin_{\boldsymbol{z}} \boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{K}^T \mathcal{U} \boldsymbol{V} \boldsymbol{K} \boldsymbol{B} \boldsymbol{z} +  \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ) \nonumber \\
         &= \argmin_{\boldsymbol{z}} \boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} \boldsymbol{z} + \tilde{\Phi}(\boldsymbol{z}, \boldsymbol{\tilde{R}} ) \nonumber 
\end{align}
where we introduce the heterogeneous Laplacian matrix $\boldsymbol{L} = \boldsymbol{K}^T \mathcal{U} \boldsymbol{V} \boldsymbol{K}$ for brevity. Here, $\boldsymbol{K} \in \mathbb{R}^{9k \times 3n} $ is the vector gradient operator over our mesh, mapping deformed coordinates to per-tet deformation gradients, $\boldsymbol{V} \in \mathbb{R}^{9k \times 9k}$ is a diagonal matrix of tetrahedron volumes, $\mathcal{U}  = (\boldsymbol{I}_9  \otimes \text{diag}(\boldsymbol{\mu})) \in \mathbb{R}^{9k \times 9k}$ is a diagonal containing first lamé parameter $\mu_t$ for each tet $t$ along its diagonal. 

The above optimization can be performed without recomputing full space second order terms with an iterative Quasi-Newton method, where each timestep we solve for the search direction $\boldsymbol{z}_{j+1}  = \boldsymbol{z}_{j} + \alpha d\boldsymbol{z} $:
\begin{align}
        \boldsymbol{0} &=  2 \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} d\boldsymbol{z} + 2 \boldsymbol{B}^T\boldsymbol{L} \boldsymbol{B} \boldsymbol{z}_j + 
        \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{z}}
        \frac{\partial \boldsymbol{\tilde{f}}}{\partial \boldsymbol{f}} 
        \frac{\partial \tilde{\Phi}}{\partial \boldsymbol{\tilde{f}}} \nonumber  \\
        &=    \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} d\boldsymbol{z} + 
         \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} \boldsymbol{z}_j +
        \frac{1}{2} \boldsymbol{B}^T \boldsymbol{K}^T \boldsymbol{G}
        \frac{\partial \tilde{\Phi}}{\partial \boldsymbol{\tilde{f}}}  \nonumber  \\ 
           \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} d\boldsymbol{z}  &= -  \boldsymbol{B}^T \boldsymbol{L} \boldsymbol{B} \boldsymbol{z}_j - \frac{1}{2} 
        \boldsymbol{B}^T \boldsymbol{K}^T \boldsymbol{G}
        \frac{\partial \tilde{\Phi}}{\partial \boldsymbol{\tilde{f}}}\nonumber 
\end{align}
where we have ignored any of the second order terms buried $\frac{\partial \tilde{\Phi}}{\partial \boldsymbol{\tilde{f}}} $, which would normally incur a full space update in the system matrix every iteration. Because the search direction is no longer \emph{perfect}, we make use of a back-tracking line search to figure out the step size we take along that search direction $\boldsymbol{z}_{j+1}  = \boldsymbol{z}_{j} + \alpha d\boldsymbol{z} $.


Above, we made use of the linear relationships $\boldsymbol f = \boldsymbol{Kx} $, $\boldsymbol x = \boldsymbol{Bz} $, and $\boldsymbol{\tilde{f}} = \boldsymbol{G}_9 \boldsymbol{f} $ to derive the chained partial derivatives   $\frac{\partial \boldsymbol{x}}{\partial \boldsymbol{z}}, \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}},   \frac{\partial \boldsymbol{\tilde{f}}}{\partial \boldsymbol{f}} $. The last unknown $  \frac{\partial \tilde{\Phi}}{\partial \boldsymbol{\tilde{f}}}$ changes each local-global iteration and can efficiently be computed during the local step when looping through each cluster:

% \alec{ when putting big things instead of \texttt{vec()}= use \texttt{vec \textbackslash left( \textbackslash right)} to make the parenthesis scale with the argument.}
\begin{align}
    \frac{\partial{\tilde{\Phi}}}{\partial \boldsymbol{\tilde{f}}} &=\text{vec}\left( 
    \begin{bmatrix} 
     \vdots \\
     \frac{\partial}{\partial \boldsymbol{\tilde{F}}_c} -m_{c} \mu_{c} tr(  \boldsymbol{\tilde{F}}^T_{c}\boldsymbol{\tilde{R}}_{c}) +  m_c \frac{\lambda_c}{2}tr^2(\boldsymbol{R}_c^T \boldsymbol{F}_c - \boldsymbol{I}) \nonumber  \\
     \vdots
    \end{bmatrix}\right) \nonumber \\ &=
    \text{vec}\left( 
    \ \begin{bmatrix} 
  \vdots \\   -m_{c} \mu_{c} \boldsymbol{R}_{c} + m_{c} \frac{\lambda_c}{2} \boldsymbol{R}_c tr(\boldsymbol{R}_c^T \boldsymbol{F}_c - \boldsymbol{I}) \\ \vdots
    \end{bmatrix}\ \right) 
\end{align}

\subsection{Adding Hyper-Reduced Inertia}
To add dynamics to our simulation, we add a Kinetic energy term to the elastic energies mentioned in the previous section. We employ a Backward Euler \cite{BaraffWitkin, Liu:2013:FastMassSprings} time integrator:
\begin{align}
E_{Kinetic} = \frac{1}{2h^2}(\boldsymbol{x} -  \boldsymbol{y})^T \boldsymbol{M}(\boldsymbol{x} -  \boldsymbol{y}) \nonumber 
\end{align}
Where $\boldsymbol{y}$ represents position history $\boldsymbol{x}_{hist} = 2\boldsymbol{x}_{curr} - \boldsymbol{x}_{prev}$ and $h$ is the length of the timestep chosen.

We show we can represent this energy fully in our reduced space, without having to go to the full space. 
First we directly rewrite the final displacement in terms of our reduced complementary displacement, and our rigged possition.
$\boldsymbol{x} = \boldsymbol{Bz} + \boldsymbol{Jp}$

\begin{align}
E_{Kinetic}(\boldsymbol{z}, \boldsymbol{p}) &= \frac{1}{2h^2}( \boldsymbol{Bz} + \boldsymbol{Jp}  -  \boldsymbol{x}_{hist})^T \boldsymbol{M}( \boldsymbol{Bz} + \boldsymbol{Jp}  -  \boldsymbol{x}_{hist})  \nonumber 
\end{align}
Expanding this out and only keeping the terms that depend on our degrees of freedom $\boldsymbol{z}$ leads to:
\begin{align}
 &= \frac{1}{2h^2}\boldsymbol{z}^T \boldsymbol{B}^T\boldsymbol{MB}\boldsymbol{z}  +  \frac{1}{h^2}(\boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{M} \boldsymbol{J} \boldsymbol{p}  - \boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{M}  \boldsymbol{x}_{hist}) 
\end{align}
Writing $\boldsymbol{x}_{hist}$ in terms of our subspace and control rig: 
\begin{align}
\boldsymbol{y} = \boldsymbol{B} \boldsymbol{z}_{hist} + \boldsymbol{J} \boldsymbol{p}_{hist} \nonumber
\end{align}

Where $\boldsymbol{z}_{hist} = 2\boldsymbol{z}_{curr} - \boldsymbol{z}_{prev}$ and $\boldsymbol{p}_{hist} = 2\boldsymbol{p}_{curr} - \boldsymbol{p}_{prev}$ are our reduced space complementary and rig displacement histories respectively.

Plugging the above into our reduced energy so far:
\begin{align}
 &= \frac{1}{2h^2}\boldsymbol{z}^T \boldsymbol{B}^T\boldsymbol{MB}\boldsymbol{z}  +  \frac{1}{h^2} \boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{M} \boldsymbol{J}  (\boldsymbol{p} - \boldsymbol{p}_{hist}) -  \frac{1}{h^2} \boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{M} \boldsymbol{B} \boldsymbol{z}_{hist}
\end{align}

Adding this kinetic energy to the prior elastic energies, then minimizing would simply mean adding $  \frac{1}{h^2}\boldsymbol{B}^T \boldsymbol{M} \boldsymbol{B}$ to the quadratic terms, and then 
$ \boldsymbol{f}_{inertia} =  \frac{1}{h^2}\boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{M} \boldsymbol{J}  (\boldsymbol{p} - \boldsymbol{p}_{hist}) - \frac{1}{h^2}\boldsymbol{z}^T \boldsymbol{B}^T \boldsymbol{M} \boldsymbol{B} \boldsymbol{z}_{hist}$
to the linear terms, as shown in  Algorithm \ref{alg:simulationStep}.
