\section{Experiments}

In this section, we evaluate \modelname on COCO~\cite{lin2014coco} and compare it with several state-of-the-art methods. We also conduct detailed ablation studies to verify the effectiveness of each proposed component.


\begin{table*}[t]
  \centering
  
  \tablestyle{4pt}{1.2}\scriptsize\begin{tabular}{y{58} | x{42} | x{24} x{24} | x{30} | x{24}x{24}x{24} | x{24}x{24}x{24}}
  method & backbone & epochs & size & FPS & \apm & \apl \\
  \shline
  MEInst~\cite{zhang2020MEInst} & R50 & 36 & 512 & 26.4 & 32.2 & 53.9 & 33.0 & 13.9 & 34.4 &48.7 \\
  CenterMask~\cite{lee2020centermask} & R50 & 48	& 600 & 32.6 & 32.9	& - & -	& 12.9	& 34.7 & 48.7 \\
  SOLOv2~\cite{wang2020solov2} & R50 & 36 & 448 & 41.3 & 34.0 & 54.0 & 36.1 & 10.3 & 36.3 & 54.4 \\
  % PolarMask & R50 & 12 & 600 & 21.7$^*$ & 27.6 & 47.5 & 28.3 & \phantom{0}9.8 & 30.1 & 43.1 & \\
  OrienMask~\cite{du2021realtime} & D53 & 100 & 544 & 51.0 & 34.8 & 56.7 & 36.4 & 16.0 & 38.2 & 47.8  \\
  SparseInst~\cite{cheng2022sparseInst} & R50 & 144 & 608 & 51.2 & 34.7 & 55.3 & 36.6 & 14.3 & 36.2 & 50.7 \\	
  YOLACT~\cite{yolact-iccv2019} & R50 & 54 & 550 & 53.5 & 28.2 & 46.6 & 29.2 & \phantom{0}9.2 & 29.3 & 44.8 \\
  \gr
  \textbf{\modelname-D1} (ours) & R50 & 50 & 576 & \textbf{53.8} & 35.6 & 56.7 & 37.2 & 8.8 & 53.0 & 72.8 \\
  CondInst~\cite{tian2020conditional} & R50 & 36 & 800 & 20.0	& 37.8	& 59.1	& 40.5 & \textbf{21.0}	& 40.3	& 48.7 \\
  Mask2Former$^{\text{\textdagger}}$ & R50 & 50 & 640 & 25.3 & 38.0 & \textbf{60.3} & 39.8 & 10.8 & 54.9 & 74.3 \\
  \gr
  \textbf{\modelname-D3} (ours) & R50 & 50 & 640 & 35.5 & \textbf{38.6} & 60.2 & \textbf{40.6} & 10.8 & \textbf{56.2} & \textbf{75.2} \\
  \hline
  YOLACT~\cite{yolact-iccv2019} & R101 & 54 & 700 & 33.5 & 31.2 & 50.6 & 32.8 & 12.1 & 33.3 & 47.1 \\
  \gr
  \textbf{\modelname-D1} (ours) & R101 & 50 & 640 & \textbf{35.3} & 38.3 & 60.2 & 40.5 & 10.7 & 55.8 & 74.8  \\
  SOLOv2~\cite{wang2020solov2} & R101 & 36 & 800 & 15.7 & 39.7 & 60.7 &  \textbf{42.9} & 17.3 &  42.9 & 57.4 \\
  CondInst~\cite{tian2020conditional} & R101 & 36 & 800 & 16.4 & 39.1 & 60.9 & 42.0 &  \textbf{21.5} & 41.7 & 50.9 \\
  Mask2Former$^{\text{\textdagger}}$ & R101 & 50 & 640 & 21.1 & 39.5 &  \textbf{61.7} & 41.6 & 11.2 & 56.3 & 75.8 \\
  \gr
  \textbf{\modelname-D3} (ours) & R101 & 50 & 640 & 28.0 &  \textbf{39.9} &  61.5 & 42.3 & 11.4 &  \textbf{57.1} &  \textbf{76.6} \\
  \hline
  SOLOv2~\cite{wang2020solov2} & R50-DCN & 36 & 512 & 32.0 & 37.1 & 57.7 & 39.7 & 12.9 & 40.0 & 57.4 \\
  YOLACT++~\cite{yolact-plus-tpami2020} & R50-DCN & 54 & 550 & 39.4 & 34.1 & 53.3 & 36.2 & 11.7 & 36.1 & 53.6 \\
  SparseInst~\cite{cheng2022sparseInst} & R50-d-DCN & 144 & 608 & 46.5 & 37.9 & 59.2 & 40.2 & \textbf{15.7} & 39.4 & 56.9 \\
  \gr
  \textbf{\modelname-D1} (ours) & R50-d-DCN & 50 & 576 & \textbf{47.8} & 38.0 & 59.7 & 39.9 & 10.0 & 54.9 & 74.5  \\
  \gr
  \textbf{\modelname-D3} (ours) & R50-d-DCN & 50 & 640 & 32.5 & \textbf{40.5} & \textbf{62.6} & \textbf{42.9} & 11.9 & \textbf{57.9} & \textbf{76.7} \\
  
  \end{tabular}
  % \vspace{-1mm}

   \caption{\textbf{Instance segmentation on COCO \texttt{test-dev}.} \modelname outperforms most previous real-time instance segmentation algorithms in both accuracy and speed.
   	 Mask2Former$^{\text{\textdagger}}$ denotes a light version of Mask2Former~\cite{cheng2021mask2former} that exploits PPM-FPN as the pixel decoder, as in \modelname and SparseInst~\cite{cheng2022sparseInst}.  \modelname-D$\alpha$ represents \modelname with $\alpha$ Transformer decoder layers. ``R50-d-DCN" means ResNet-50-d~\cite{he2019bag} backbone with deformable convolutions~\cite{zhu2019deformable}.
   	 For a fair comparison, all entries are \textit{single-scale} results.}

%\vspace{-3mm}

\label{tab:insseg:coco}
\end{table*}



\subsection{Implementation details}
\label{sec:exp:impl}

Our model is implemented using Detectron2~\cite{wu2019detectron2}. We use the AdamW~\cite{loshchilov2018decoupled} optimizer with a step learning rate schedule. The initial learning rate is 0.0001, and the weight decay is 0.05. We apply a learning rate multiplier of 0.1 to the backbone, which is ImageNet-pretrained, and decay the learning rate by 10 at fractions 0.9 and 0.95 of the total number of training iterations. 
Following \cite{cheng2021mask2former}, we train our model for 50 epochs with a batch size of 16. For data augmentation, we use the same scale jittering and random cropping as in \cite{cheng2022sparseInst}. For example, the shorter edge varies from 416 to 640 pixels, and the longer edge is no more than 864 pixels. We set the loss weights $\lambda_\text{cls}$, $\lambda_\text{ce}$, and $\lambda_\text{dice}$ to 2.0, 5.0, and 5.0, respectively, as in \cite{cheng2021mask2former}. $\lambda_\text{cls-q}$ and $\lambda_\text{loc}$ are set to 20.0 and 1000.0, respectively. We use 100 IA-guided queries and 8 auxiliary learnable queries by default. We report the AP performance as well as the FLOPs and FPS. FLOPs are averaged using 100 validation images. FPS is measured on a V100 GPU with a batch size of 1 using the entire validation set. Unless specified, we use a shorter edge of 640 pixels with a longer edge not exceeding 864 pixels to test and benchmark models.

\subsection{Main results}
We compare \modelname with state-of-the-art methods on the COCO dataset in \tabref{tab:insseg:coco}.
Since the goal of \modelname is for an efficient real-time instance segmenter, we mainly compare it with state-of-the-art real-time instance segmentation algorithms in terms of accuracy and inference speed. The evaluation is conducted on COCO \texttt{test-dev}. We provide \modelname with different backbones and different numbers of Transformer decoder layers to achieve a trade-off between speed and accuracy. The results show that \modelname outperforms most previous \sota real-time instance segmentation methods with better performance and faster speed. For example, with a ResNet-50~\cite{he2016deep} backbone, the designed \modelname-D1 model outperforms a strong convolutional baseline SparseInst~\cite{cheng2022sparseInst} by 0.9 AP while using fewer training epochs and less inference time. We also compare \modelname with the query-based model Mask2Former~\cite{cheng2021mask2former}. To keep the speed and accuracy in a similar order, we replace the MSDeformAttn~\cite{zhu2021deformable} pixel decoder in Mask2Former with the PPM-FPN-based one, which is the same as \modelname as well as SparseInst~\cite{cheng2022sparseInst}. Meanwhile, for a fair comparison, the training setting of Mask2Former, including data augmentation, is replaced with the same as \modelname. As expected, Mask2Former relies on a strong pixel decoder and performs worse than \modelname in both accuracy and speed with a lighter pixel decoder (even if it has 9 decoder layers), showing less efficiency in the real-time benchmark. Besides, with ResNet-50-d-DCN~\cite{he2019bag,zhu2019deformable} backbone, our algorithm achieves 32.5 FPS and 40.5 AP, the only algorithm in \tabref{tab:insseg:coco} with AP $\!>\!40$ while maintaining a real-time speed ($\geq\!$ 30 FPS). \figref{fig:tradeoff} illustrates the speed-accuracy trade-off curve, which also demonstrates the superiority of our method.


\begin{table}[t]
	\centering
	\tablestyle{2pt}{1.2}
	\scriptsize
	\begin{tabular}{p{40px} | x{18} | x{25} x{21} x{21}x{21}|x{26}x{21}}
		 & $D$ & \phantom{00}\apm$^\texttt{val}$ & \aps & \phantom{0}FLOPs & FPS \\
		\shline
		 zero~\cite{detr} & 1 & \phantom{0}31.5 & 10.8 & 33.6 & 52.6  & \phantom{0}58.4G & 50.0 \\
		\hline
		learnable~\cite{cheng2021mask2former} & 1 &  \phantom{0}34.6  & 13.5 & 37.5 & 55.4 & \phantom{0}58.4G & 50.0 \\		
		resize~\cite{pei2022osformer} & 1 & \phantom{0}34.9  & 13.7 & 37.9 & 56.2 & \phantom{0}58.4G & 49.7 \\
		\hline
		\textbf{IA-guided} & 1 & \phantom{0}\textbf{35.6}  & \textbf{14.3} & \textbf{38.8} & \textbf{56.6} & \phantom{0}59.6G & 48.8 \\ 
		\hline\hline
		 zero~\cite{detr} & 3 & \phantom{0}37.2 & 15.4 & 40.3 & 58.6 & \phantom{0}74.3G & 36.1 \\
		 \hline
		learnable~\cite{cheng2021mask2former} & 3 & \phantom{0}37.5 & 15.0 & 40.6 & 59.0 & \phantom{0}74.3G & 36.1 \\
		resize~\cite{pei2022osformer} & 3 & \phantom{0}37.6  & 15.6 & 40.4 & 59.7 & \phantom{0}74.3G & 36.0 \\
		\hline
		\textbf{IA-guided} &  3 & \phantom{0}\textbf{37.9}  & \textbf{16.0} & \textbf{40.7} & \textbf{60.1} & \phantom{0}75.5G & 35.5 \\
		\hline
	\end{tabular}
\caption{\textbf{IA-guided queries.} Our IA-guided queries perform better than other methods, especially when the Transformer decoder layer number (\ie, $D$) is small.}
\vspace{-3mm}
\label{tab:ablation:iaquery}
\end{table}


\begin{table}[t]
	\centering
	\tablestyle{2pt}{1.2}
	\scriptsize
	\begin{tabular}{p{75px} | x{10} | x{21} x{17} x{17}x{17}|x{24}x{20}}
		& $D$ & \phantom{00}\apm$^\texttt{val}$ & \aps & \phantom{0}FLOPs & FPS \\
		\shline
		single pixel feat. update & 6 & \phantom{0}32.5  & 13.9 & 36.3 & 50.5 & \phantom{0}85.4G & 35.5  \\
		single query update~\cite{cheng2021mask2former} & 6 & \phantom{0}36.9  & 15.0 & 39.6 & 59.8 & \phantom{0}63.3G & 35.0 \\
		\hline
		dual query-then-pixel & 3 & \phantom{0}37.8 & \textbf{16.0} & 40.6 & 60.0 & \phantom{0}75.5G & 35.5  \\
		\hline
		\textbf{dual pixel-then-query} & 3 & \phantom{0}\textbf{37.9}  & \textbf{16.0} & \textbf{40.7} & \textbf{60.1} & \phantom{0}75.5G & 35.5  \\
		\hline
	\end{tabular}
	\caption{\textbf{Dual-path update strategy.} Our dual pixel-then-query update strategy consistently outperforms single-path update strategies. We double the Transformer decoder layers (\ie, $D$) for the single-path update strategies for a fair comparison.}
	\vspace{-3mm}
	\label{tab:ablation:dualpath}
\end{table}


\begin{table}[t]
	\centering
	\tablestyle{2pt}{1.2}
	\scriptsize
	\begin{tabular}{p{65px} | x{36} | x{20} x{15} x{15}x{15}|x{20}x{15}}
		& backbone & \phantom{0}\apm$^\texttt{val}$ & \aps & FLOPs & FPS \\
		\shline
		w/o GT mask guidance & R50 & 37.4 & 15.2 & 40.6 & 59.6 & 75.5G & 35.5 \\
		\textbf{w/  GT mask guidance} & R50 & \textbf{37.9} & \textbf{16.0} & \textbf{40.7} & \textbf{60.1} & 75.5G & 35.5 \\
		\hline\hline
		w/o GT mask guidance & R50-d-DCN & 39.7	& 17.5 & 43.1 & 61.9 & - & 32.5 \\
		\textbf{w/  GT mask guidance} & R50-d-DCN & \textbf{40.1} & \textbf{17.7} & \textbf{43.2} & \textbf{62.4} & - & 32.5 \\
		\hline
	\end{tabular}
	\caption{\textbf{GT mask-guided learning.} Our GT mask-guided learning improves the performance across different backbones.}
	\vspace{-3mm}
	\label{tab:ablation:gtmask}
\end{table}


\begin{table}[t]
	\centering
	\tablestyle{2pt}{1.2}
	\scriptsize
	\begin{tabular}{y{72} | x{25} x{20} x{20}x{20}|x{24}x{20}}
		& \phantom{0}\apm$^\texttt{val}$ & \aps & FLOPs & FPS \\
		\shline
		FPN~\cite{lin2016feature} & 37.4 & 15.5 & 40.3 & 59.4 & 75.4G & 35.7 \\
		\hline
		Transformer-Encoder~\cite{detr} & 38.9 & 17.3 & 41.6 & 61.4 & 78.5G & 29.5 \\
		MSDeformAttn~\cite{zhu2021deformable} & \textbf{40.0} & \textbf{17.5} & \textbf{43.5} & \textbf{62.2} & 114.7G & 21.2 \\				
		\hline
		\textbf{PPM-FPN}~\cite{cheng2022sparseInst} & 37.9  & 16.0 & 40.7 & 60.1 & 75.5G & 35.5 \\
	\end{tabular}
	\caption{\textbf{Pixel decoder.} Stronger pixel decoders lead to higher performance but consume more computation. PPM-FPN obtains a better trade-off between accuracy and speed.}
	\vspace{-3mm}
	\label{tab:ablation:pixeldecoder}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{table*}[t]
 	\begin{subtable}{0.32\linewidth}
  	\centering
  	\tablestyle{3pt}{1.2}
  	\scriptsize
  	\begin{tabular}{c | x{15}x{15}x{15}x{15} | x{20}x{15}}
  		$D$ & \apm$^\texttt{val}$ & \aps & FLOPs & FPS \\
  		\shline
		0 & 30.5 & 12.1 & 34.5 & 48.6 & 51.6G & 60.2 \\
		1 & 35.6 & 14.3 & 38.8 & 56.6 & 59.6G & 48.8 \\
		2 & 37.1 & 15.8 & 39.7 & 58.8 & 67.5G & 41.0 \\
		3 & 37.9 & 16.0 & 40.7 & 60.1 & 75.5G & 35.5 \\
		6 & 38.7 & \textbf{16.6} & 41.7 & 61.1 & 99.3G & 25.1 \\
		9 & \textbf{39.1} & 16.1 & \textbf{42.3} & \textbf{62.1} & 123.2G & 19.2 \\
  	\end{tabular}
  	\caption{\textbf{Transformer decoder layer number.} \modelname benefits from more Transformer decoder layers. }
  	\label{tab:ablation:numdecoderlayer}
  	\end{subtable}\hspace{2mm}
  	\begin{subtable}{0.32\linewidth}
  	\centering
 		\tablestyle{3pt}{1.2}
 		\scriptsize
  	\begin{tabular}{l | x{15}x{15}x{15}x{15} | x{20}x{15}}
  		$N_a$& \apm$^\texttt{val}$  & \aps & FLOPs & FPS \\
  		\shline
  		10 & 31.2 & 10.2 & 32.9 & 54.2 & 71.9G & 39.8  \\
		50 & 36.9 & 14.5 & 39.6 & 58.5 & 73.5G & 37.7  \\
		100 & 37.9 & 16.0 & 40.7 & 60.1 & 75.5G & 35.5 \\ 
		200 & 38.5 & 16.2 & 41.3 & \textbf{60.8} & 79.5G & 31.6 \\
		300 & \textbf{38.8} & \textbf{17.5} & \textbf{42.0} & 60.2 & 83.5G & 28.6 \\
  	\end{tabular}
	\caption{\textbf{Effect of IA-guided query number on AP.} Increasing IA-guided query number contributes to AP performance.}
	\label{tab:ablation:numiaqueryap}
	\end{subtable}\hspace{2mm}
  	\begin{subtable}{0.32\linewidth}
	\centering
	\tablestyle{3pt}{1.2}
	\scriptsize
	\begin{tabular}{l | x{20}x{18}x{18}x{18}}
		$N_a$& \phantom{0}AR$^\texttt{val}$  & AR$_\text{S}$ & AR$_\text{M}$ & AR$_\text{L}$ \\
		\shline
		10 & \phantom{0}38.2 & 14.4 & 40.5 & 62.8   \\
		50 & \phantom{0}49.8 & 26.2 & 53.3 & 72.6   \\
		100 & \phantom{0}52.0 & 28.9 & 55.3 & 74.2  \\ 
		200 & \phantom{0}53.4 & 30.5 & 57.4 & 74.9  \\
		300 & \phantom{0}\textbf{54.0} & \textbf{31.3} & \textbf{57.6} & \textbf{75.3}  \\
	\end{tabular}
	\caption{\textbf{Effect of IA-guided query number on AR@100.}  Larger IA-guided query number improves object recalls, especially for small objects.}
	\label{tab:ablation:numiaqueryar}
	\end{subtable}
	\vspace{2mm}

 	\begin{subtable}{0.32\linewidth}
	\centering
	\tablestyle{3pt}{1.2}
	\scriptsize
	\begin{tabular}{y{8}| x{10} | x{14}x{14}x{14}x{14} | x{18}x{14}}
		$N_b$ & $N_a$ & \apm & \aps & FLOPs & FPS \\
		\shline
		0 &100 & 37.7 & 15.7 & 40.4 & \textbf{60.3} & 75.2G & 35.6 \\
		0 &108 & 37.6 & 15.6 & 40.7 & 59.9 & 75.6G & 35.3 \\
		8 &100 & \textbf{37.9} & 16.0 & 40.7 & 60.1 & 75.5G & 35.5 \\
		16 & 100 & 37.8 & \textbf{16.1} & \textbf{40.9} & 59.9 & 75.7G & 35.1 \\
	\end{tabular}
	\caption{\textbf{Auxiliary learnable query number.} Adding a few (\ie, 8) auxiliary learnable queries performs better than setting all queries as IA-guided ones.}
	\label{tab:ablation:numauxquery}
	\end{subtable}\hspace{2mm}
	\begin{subtable}{0.3\linewidth}
	\centering
	\tablestyle{3pt}{1.2}
	\scriptsize
	\begin{tabular}{l | x{15}x{15}x{15}x{15} | x{20}x{15}}
		& \apm & \aps & FLOPs & FPS \\
		\shline
		E$_5$ & 37.8 & 15.5 & 40.7 & 60.2 & 74.6G & 35.6 \\
		E$_4$ & 37.9 & 16.0 & 40.7 & 60.1 & 75.5G & 35.5 \\
		E$_3$ & \textbf{38.0} & \textbf{16.3} & \textbf{41.2} & \textbf{60.6} & 79.1G & 34.7 \\
	\end{tabular}
	\caption{\textbf{Query selection source.} Selecting IA-guided queries from larger feature maps leads to better results, but the gain is limited. E$_4$ is a trade-off choice between accuracy and speed.
	}
	\label{tab:ablation:querylevel}
	\end{subtable}\hspace{2mm}
	\begin{subtable}{0.34\linewidth}
	\centering
	\tablestyle{3pt}{1.2}
	\scriptsize
	\begin{tabular}{l | x{13}x{13}x{13}x{13} | x{18}x{14}}
		 & \apm & \aps & FLOPs & FPS \\
		\shline
%		semantic & 35.7	& 14.2 & 38.1 & 57.5 & 75.5G & 35.5 \\
%		Hung. w/o $\mathcal{L}_\text{loc}$ & 37.1 & 14.9 & 40.0 & 59.3 & 75.5G & 35.5 \\
%		Hung. w/ $\mathcal{L}_\text{loc}$ & 37.9 & 16.0 & 40.7 & 60.1 & 75.5G & 35.5 \\
		Baseline & \textbf{37.9} & \textbf{16.0} & \textbf{40.7} & \textbf{60.1} & 75.5G & 35.5 \\
		$-$ bi. matching & 35.7	& 14.2 & 38.1 & 57.5 & 75.5G & 35.5 \\
		$-$ loc. cost & 37.1 & 14.9 & 40.0 & 59.3 & 75.5G & 35.5  \\
	\end{tabular}
	\caption{\textbf{Instance activation loss.} We remove one component at a time. When removing the bipartite matching strategy, we use a fixed target assignment for each pixel according to their semantic class labels.}
	\label{tab:ablation:ialoss}
	\end{subtable}
	
	\caption{\textbf{Several ablations for \modelname}. Results are evaluated on COCO \texttt{val2017}.}
	\label{tab:ablation:maskformer}
	% \vspace{-3mm}
\end{table*}


\begin{table}[t]
	\centering
	\tablestyle{2pt}{1.2}
	\scriptsize
	\begin{tabular}{y{84} |x{20} x{18} x{18}x{18}|x{22}x{18}}
		 & \phantom{0}\apm$^\texttt{val}$ & \aps &  \phantom{0}FLOPs & FPS \\
		\shline
		\modelname-D3 & 37.9 & 16.0 & 40.7 & 60.1 &  \phantom{0}75.5G & 35.5 \\
		$-$ learnable pos. embeddings & 37.9 & 16.4 & 40.6 & \textbf{60.3} & \phantom{0}75.5G & 32.9 \\
		\hline
	\end{tabular}
	\caption{\textbf{Positional embeddings.} When removing  learnable positional embeddings, we use the non-parametric sinusoidal positional embeddings, as in \cite{detr, cheng2021mask2former} }
	% \vspace{-3mm}
	\label{tab:ablation:largefeature}
\end{table}


\subsection{Ablation studies}

We now perform a series of ablation studies to analyze \modelname. We first verify the effectiveness of three proposed key components, \ie, IA-guided queries, dual-path update strategy, and GT mask-guided learning, and then study the effect of some other designs about \modelname. Unless specified otherwise, we conduct experiments on \modelname-D3 with ResNet-50~\cite{he2016deep} backbone. All ablation results are evaluated on the COCO \texttt{val2017} set.


\noindent\textbf{IA-guided queries.} As shown in \tabref{tab:ablation:iaquery}, our IA-guided queries achieve better results than zero~\cite{detr} or learning-based ones~\cite{cheng2021mask2former}. Recent work~\cite{pei2022osformer} proposes to use resized multi-scale features as instance queries. However, such a fix-position query selection strategy is hard to extract representative embeddings for all potential objects and thus obtains lower performance. Note that IA-guide queries achieve a more significant result when the model is equipped with only one Transformer decoder layer, which shows their great efficiency in lightweight model design.


\noindent\textbf{Dual-path update strategy.} \tabref{tab:ablation:dualpath} shows the effectiveness of our dual-path update strategy. Thanks to the co-optimization of query and pixel features, our dual-path update strategy performs better than the conventional single query update strategy~\cite{cheng2021mask2former} in our lightweight pixel decoder setting. 
% Also, the result indicates we can obtain good performance with only a few Transformer decoder layers,  which is advantageous in real-time.
The update order of query and pixel features does not matter much in our experiments. 


\noindent\textbf{GT mask-guided learning}. As shown in \tabref{tab:ablation:gtmask}, GT mask-guided learning improves the model performance by up to 0.5 AP, indicating that this technique indeed helps the Transformer decoder learn how to update queries for better object embeddings under masked attention mechanism. \tabref{tab:ablation:gtmask} also demonstrates the generality of GT mask-guided learning for different backbones.

The above ablations demonstrate the effectiveness of our proposed three key techniques. We refer interested readers to the Appendix for more ablations about them, \eg, the changes and the corresponding improvements based on the original Mask2Former.
We then explore the effect of some other designs about \modelname.

\noindent\textbf{Pixel decoder.}
\modelname is compatible with any existing pixel decoders.
\tabref{tab:ablation:pixeldecoder} shows the performance of \modelname with different pixel decoders. Stronger pixel decoders produce better contextual features and lead to higher results but consume more computation. For fast real-time instance segmentation, PPM-FPN~\cite{cheng2022sparseInst} is a good trade-off choice. 


\noindent\textbf{Transformer decoder layer number.} As shown in \tabref{tab:ablation:numdecoderlayer}, increasing the number of Transformer decoder layers contributes to the segmentation performance in \modelname. In particular, the mask performance achieves 30.5 AP without using the Transformer decoder. This is mainly attributed to the effectiveness of IA-guided queries, which carry rich embedding information about potential objects at the initial. In addition, our segmentation performance is saturated at around the sixth layer. Continuing to increase decoder layers only marginally improve it.
Also note that FastInst can obtain good performance with only a few Transformer decoder layers,  which is advantageous in real-time.


\begin{figure}[t]
    \begin{subtable}{1.0\linewidth}
    \centering
    \bgroup
    \def\arraystretch{0.2}
    \setlength\tabcolsep{0.2pt}
    \begin{tabular}{cccc}
    \includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000001268/000000001268_semantic.jpg} &
    \includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000001268/000000001268_noloc.jpg} &
    \includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000001268/000000001268_baseline.jpg} &
    \includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000001268/000000001268_gt.jpg} \\
    \includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000002473/000000002473_semantic.jpg} &
	\includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000002473/000000002473_noloc.jpg} &
	\includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000002473/000000002473_baseline.jpg} &
	\includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000002473/000000002473_gt.jpg} \\
    \includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000377814/000000377814-4.jpg} &
	\includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000377814/000000377814-0.jpg} &
	\includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000377814/000000377814-1.jpg} &
	\includegraphics[width=0.24\linewidth, height=0.18\linewidth]{figures/vis_query_points/img_000000377814/000000377814-5.jpg} \\
    \end{tabular} \egroup
    \end{subtable}
  %\vspace{0mm}
  % \vspace{-2mm}
  \caption{\textbf{Visualization of IA-guided queries.} The \textbf{first and second rows} show the distributions of IA-guided queries with different supervision losses for the auxiliary classification head. First column: dense pixel-wise semantic classification loss. Second column: bipartite matching-based Hungarian loss without the location cost. Third column: bipartite matching-based Hungarian loss with the location cost (ours). Fourth column: ground truth. The query points under our designed loss (third column) are more concentrated on the region of each foreground object. The \textbf{last row} shows four predicted masks (\textbf{blue}) with corresponding IA-guided query locations (\textbf{red}).}
  \label{fig:analysis:query}
\end{figure}



\noindent\textbf{IA-guided query number.} In Mask2Former, increasing the query number to more than 100 will slightly degrade the instance segmentation performance. In \tabref{tab:ablation:numiaqueryap}, the results indicate that increasing the number of IA-guided queries will contribute to the segmentation performance in \modelname. We attribute this to the improved object recall (see \tabref{tab:ablation:numiaqueryar}) and increased object embedding information for decoding. On the other hand, growing IA-guided queries will affect the model inference speed. Note that even with 10 IA-guided queries, our model can obtain 31.2 AP on COCO dataset, which has 7.7 instances per image on average~\cite{lin2014coco}. This indicates the effectiveness of IA-guided queries again.

\noindent\textbf{Auxiliary learnable query number.} Auxiliary queries aim to gather background and image-independent information for pixel feature updates and query updates. They do not participate in object predictions. \tabref{tab:ablation:numauxquery} shows that adding a few auxiliary learnable queries helps improve the performance, better than setting all queries as IA-guided queries.

\noindent\textbf{Query selection source.} As shown in \tabref{tab:ablation:querylevel}, selecting IA-guided queries from larger feature maps leads to better results. E$_4$ is a good trade-off choice between accuracy and speed. Nevertheless, the contribution of the selection source to the model performance is limited.

\noindent\textbf{Instance activation loss.} We study the effect of two components in instance activation loss. 
As shown in \tabref{tab:ablation:ialoss}, the bipartite matching-based target assignment strategy leads to a significant gain, which provides a sparse pixel embedding activation for IA-guided query selection. 
Here when removing the bipartite matching strategy, we use the semantic class label as the target of each pixel, as in common semantic segmentation tasks~\cite{deeplabV2,deeplabV3plus}. 
The location cost also plays a vital role in the matching loss, which reduces matching space and accelerates model convergence. 
\figref{fig:analysis:query} visualizes the distributions of  IA-guided queries, which also shows the superiority of our designed loss.

%with three losses induced by above three different supervision signals. The IA-guided queries under our designed loss are more concentrated on the region of each foreground object.
\noindent\textbf{Positional embeddings.}  \tabref{tab:ablation:largefeature} demonstrates that using learnable positional embeddings instead of non-parametric sinusoidal positional embeddings can improve the model inference speed without compromising the performance.

% \noindent\textbf{1/4-high-resolution feature.} In \tabref{tab:ablation:largefeature}, we add 1/4 feature map from the backbone and pixel decoder to the upsampled refined pixel features of the last decoder layer to predict final segmentation masks, as done in Mask2Former~\cite{cheng2021mask2former}. The results indicate that the high-resolution feature contributes largely to the segmentation performance, especially for small objects. We can sacrifice a little speed with a large feature map for a performance boost of \modelname.



