
\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/tease.pdf}
	% \vspace{-10pt}
	\caption{\textbf{Model overview.} FastInst consists of three modules: backbone, pixel decoder, and Transformer decoder. The backbone and pixel decoder extract and refine multi-scale features (\secref{sec:method:arch:pixel_decoder}). The Transformer decoder selects $N_a$ instance activation-guided queries (IA-guided queries) from the feature map E$_4$ (\secref{sec:method:arch:query}) and concatenates them with $N_b$ auxiliary learnable queries as initial queries. Then taking the initial queries and the flattened feature map E$_3$ as input, the Transformer decoder performs the object classification and segmentation at each layer with a dual-path update strategy (\secref{sec:method:arch:decoder}). During training, we introduce ground truth (GT) mask-guided learning to improve the performance of masked attention (\secref{sec:method:arch:gtmask}). For readability, we omit positional embeddings in this figure.
	}
	\label{fig:main_arch}
	\vspace{-10pt}
\end{figure*}


\section{Methods}

\subsection{Overall architecture}

As illustrated in \figref{fig:main_arch} , \modelname consists of three modules: backbone, pixel decoder, and Transformer decoder.
%where (1) the backbone is applied to extract the multi-scale features from the input image, (2) the pixel decoder is employed to refine the multi-scale features given by the backbone, and (3) the Transformer decoder is designed for the final object classification and segmentation.

Our model feeds an input image $\mathbf{I}\!\in\!\mathbb{R}^{H \times W\times 3}$ to the backbone and obtains three feature maps C$_3$, C$_4$, and C$_5$, of which the resolutions are $1/8$, $1/16$, and $1/32$ of the input image, respectively. We project these three feature maps to the ones with 256 channels by a $1\!\times\!1$ convolutional layer and feed them into the pixel decoder. The pixel decoder aggregates the contextual information and outputs the enhanced multi-scale feature maps E$_3$, E$_4$, and E$_5$. After that, we pick $N_a$ instance activation-guided queries from the feature map E$_4$, concatenated with $N_b$ auxiliary learnable queries to obtain the total queries $\mathbf{Q}\in\!\mathbb{R}^{N\times256}$, where $N=N_a+N_b$. The  Transformer decoder takes as input the total queries $\mathbf{Q}$ as well as the flattened high-resolution pixel feature E$_3$, denoted as $\mathbf{X}\in\!\mathbb{R}^{L\times256}$, where $L=H/8\times W/8$. Then in the Transformer decoder, we update the pixel features $\mathbf{X}$ and the queries $\mathbf{Q}$ in a dual-path way and predict the object class and segmentation mask at each decoder layer. 

We now discuss each component in detail.

\subsection{Lightweight pixel decoder}
\label{sec:method:arch:pixel_decoder}

Multi-scale contextual feature maps are essential for image segmentation~\cite{kirillov2019panopticfpn,wang2020solov2,deeplabV2}. However, using a complicated multi-scale feature pyramid network increases the computational burden. Unlike previous methods~\cite{cheng2021maskformer,cheng2021mask2former}, which directly employ the underlying feature maps from the pixel decoder, we use the refined pixel features in the Transformer decoder to produce segmentation masks. 
This setup reduces the requirement of the pixel decoder for heavy context aggregation.
We thus can use a lightweight pixel decoder module. 
For a better trade-off between accuracy and speed, we use a variant called PPM-FPN~\cite{cheng2022sparseInst} instead of vanilla FPN~\cite{lin2016feature}, which adopts a  pyramid pooling module~\cite{zhao2017pspnet} after C$_5$ to enlarge receptive fields for improved performance. 
%We can also employ stronger pixel decoders such as Transformer encoders~\cite{vaswani2017attention} or MSDeformAttn encoders~\cite{zhu2021deformable} to enhance the multi-scale representations. Stronger pixel decoders lead to better results but will introduce much more computations.

\subsection{Instance activation-guided queries}
\label{sec:method:arch:query}

Object queries play a crucial role in Transformer architecture~\cite{detr}.
One of the reasons for the slow convergence of DETR is that its object queries are zero-initialized. Although learnable queries~\cite{cheng2021mask2former} mitigate this issue, they are still image-independent and require many Transformer decoder layers to refine.
Inspired by Deformable DETR~\cite{zhu2021deformable}, which selects the query bounding boxes from pyramidal features for object detection, we propose instance activation-guided queries that straightforwardly pick the queries with high semantics from underlying multi-scale feature maps.
Specifically, given the output feature maps of the pixel decoder, we add an auxiliary classification head, followed by a softmax activation, on top of the feature map E$_4$ to yield the class probability prediction  $\mathbf{p}_i \!\in\!\Delta^{K \!+\!1}$ for each pixel, where $\Delta^{K+1}$ is the $(K+1)$-dimensional probability simplex, $K$ is the number of classes, added by one for ``no object" ($\varnothing$), $i$ is the pixel index, and the auxiliary classification head is composed of two convolutional layers with $3\!\times\!3$ and $1\!\times\!1$ kernel sizes, respectively.
Through $\mathbf{p}_i$ we obtain the foreground probability $p_{i,k_i}$, $k_i \!=\!\operatorname{argmax}_k\{p_{i,k}|p_{i,k}\!\in\!\mathbf{p}_i, k \!\in\!\{1,\cdots, K\}\}$ for each pixel. Then we select $N_a$ pixel embeddings from the feature map E$_4$ with high foreground probabilities as the object queries. Here we first select the ones with $p_{i,k_i}$ that is the \textit{local maximum} in the corresponding class plane (\ie, $p_{i,k_i}\!\geq\! p_{n,k_i}$, $n \!\in\!\delta(i)$, where $\delta(i)$ is the spatially 8-neighboring index set of $i$) and then pick the ones with the top foreground probabilities in $\{p_{i,k_i}\}_i$.
Note that a pixel with a non-local-maximum probability in the corresponding class plane means there exists a pixel in its 8-neighborhood which has a higher probability score of that class. With locations so close, we naturally prefer to pick its neighboring pixel rather than it as the object query. 

During training, we apply matching-based Hungarian loss~\cite{stewart2016,detr} to supervise the auxiliary classification head. 
Unlike~\cite{zhu2021deformable}, 
which employs prior anchor boxes and binary classification scores for the matching problem, 
we simply use the class predictions with a \textit{location cost} $\mathcal{L}_\text{loc}$ to compute the assignment costs. The location cost $\mathcal{L}_\text{loc}$  is defined as an indicator function that is 0 when the pixel is located in the region of that object; otherwise, it is 1. The intuition behind this cost is that only pixels that fall inside an object can have a reason to infer the class and mask embedding of that object. Also, the location cost reduces the bipartite matching space and speeds up training convergence. 

We term the queries generated from the above strategy as instance activation-guided (IA-guided) queries. Compared to the zero~\cite{detr} or learnable queries~\cite{cheng2021mask2former}, IA-guided queries hold rich information about potential objects at the initial and improve the efficiency of query iterations in the Transformer decoder. Note that we can also select the queries from feature maps E$_3$ or E$_5$. Larger feature maps contain richer instance clues but suffer heavier computational burdens. We use the middle-size feature map E$_4$ for a trade-off.

\subsection{Dual-path Transformer decoder}
\label{sec:method:arch:decoder}

After selecting $N_a$ IA-guided queries from the underlying feature map, we concatenate them with $N_b$ auxiliary learnable queries to obtain the total queries $\mathbf{Q}$, where auxiliary learnable queries are used to facilitate grouping background pixel features and provide general image-independent information in the subsequent dual update process. Then the total queries $\mathbf{Q}$ combined with the flattened $1/8$ high-resolution pixel features $\mathbf{X}$ are fed into the Transformer decoder.
In the Transformer decoder, we add positional embeddings for queries $\mathbf{Q}$ and pixel features $\mathbf{X}$,  followed by successive Transformer decoder layers to update them. One Transformer decoder layer \textit{contains} one pixel feature update and one query update.
The whole process is like an EM (Expectationâ€“Maximization) clustering algorithm. E step: update pixel features according to the centers (queries) they belong to; M step: update cluster centers (queries).
Compared with the single-path update strategy~\cite{cheng2021mask2former}, the dual-path update strategy co-optimizes both pixel features and queries, reducing the dependence on heavy pixel decoders and acquiring more fine-grained feature embeddings.
Finally, we use the refined pixel features and queries to predict the object classes and segmentation masks at each layer.

\noindent\textbf{Positional embeddings.}
Location information is critical in distinguishing different instances with similar semantics, especially for objects with the same class~\cite{tian2020conditional,wang2020solo,wang2020solov2}. Instead of non-parametric sinusoidal positional embeddings~\cite{cheng2021mask2former}, we use the learnable positional embeddings, which we find can improve the model inference speed without compromising the performance. Specifically, we employ a fixed-size learnable spatial positional embedding $\mathbf{P}\in\!\mathbb{R}^{S\times S\times 256}$, where $S$ is the spatial size and we empirically set it to the rounded square root of the IA-guided query number $N_a$. During forwarding, we interpolate $\mathbf{P}$ to two different sizes. One is with the same size as E$_3$, which is then flattened as positional embeddings for pixel features $\mathbf{X}$; the other is with the same size as E$_4$, from which we select the positional embeddings for IA-guided queries according to their locations $\{(x_i,y_i)\}_{i=1}^{N_a}$ in the feature map E$_4$. The auxiliary learnable queries employ additional $N_b$ learnable positional embeddings.

\noindent\textbf{Pixel feature update.}
We first update the pixel features.
%because the extracted queries already contain better features for objects. 
Given the flattened pixel features $\mathbf{X}$ and the queries $\mathbf{Q}$,  the pipeline of pixel feature update consists of a  cross-attention layer and a feedforward layer, as illustrated in the right side of \figref{fig:main_arch}.
The positional embeddings are added to queries and keys at every cross-attention layer~\cite{detr}.
For the update of pixel features, we do not use self-attention, which will introduce a massive computation and memory cost due to the long sequence length of pixel features. The global features can be aggregated through cross-attention on queries.

\noindent\textbf{Query update.}
Asymmetrically, we use masked attention followed by self-attention and feedforward network for the query update, as in Mask2Former~\cite{cheng2021mask2former}. 
Masked attention restricts the attention of each query to only attend within the foreground region of the predicted mask from the previous layer, and the context information is hypothesized to be gathered through following self-attention. Such a design has significantly improved query-based model performance in image segmentation tasks~\cite{cheng2021mask2former}.
Here the positional embeddings are also added to queries and keys at every masked- and self-attention layer. 

\noindent\textbf{Prediction.} We apply two separate 3-layer MLPs on top of refined IA-guided queries at each decoder layer to predict object classes and mask embeddings, respectively. Each IA-guided query needs to predict the probability of all object classes, including the "no object" ($\varnothing$) class. A linear projection is added to the refined pixel features to obtain mask features. Then mask embeddings are multiplied with mask features to obtain the segmentation masks for each query.
Here the parameters of MLPs and linear projection at each Transformer decoder layer are not shared, since queries and pixel features are updated alternately and their features can be in different representation spaces at different decoder layers. 
In addition, instance segmentation requires a confidence score for each prediction for evaluation. We follow previous work~\cite{cheng2021mask2former} and multiply the class probability score with the mask score  (\ie, the average of mask probabilities in the foreground region) as the confidence score.

%Note that during inference, the classification heads of the previous layers are not required and can be discarded. The auxiliary learnable queries do not participate in predictions in our model. 

\subsection{Ground truth mask-guided learning}
\label{sec:method:arch:gtmask}

Although masked attention introduces prior sparse attention knowledge that accelerates model convergence and improves the performance, it restricts the receptive field of each query and may cause the Transformer decoder to fall into a suboptimal query update process. To mitigate this issue, we introduce ground truth (GT) mask-guided learning. Firstly, we use the last layer's bipartite matched ground truth mask to replace the predicted mask used in $l$-th layer's masked attention. For the queries that do not match any instance in the last layer (including auxiliary learnable queries), we use the standard cross attention, \ie,
%\vspace{-1mm}
\begin{align}
	\mathbf{M}^l_i = \left\{\begin{array}{ll}
		\mathbf{M}^{gt}_j  & \text{if~} (i,j)\in\sigma \\
		\varnothing & \text{otherwise}
	\end{array}\right..
\end{align}
where $\mathbf{M}^l_i$ is the attention mask for the $i$-th query in the $l$-th layer,  $\sigma\!=\!\{(i,j)|i\in\{1, \cdots, N_a\}, j\in\{1, \cdots, N_{obj}\}\}$ is the matching of the last decoder layer, and $\mathbf{M}^{gt}_j$ is the matched ground truth mask for the $i$-th query in the last layer. Here $N_{obj}$ denotes the number of ground truth targets.
Then we use the replaced attention mask $\mathbf{M}^l$ combined with original output queries and pixel features of $l$-th layer, which are refined and for better guidance, as input to forward the  $l$-th Transformer decoder layer again. The new output is supervised according to the fixed matching $\sigma$, consistent with the last layer's bipartite matching results. This fixed matching ensures the consistency of the predictions at each Transformer decoder layer and saves the matching computation cost during training.
By such guided learning, we allow each query to see the whole region of its target predicted object during training, which helps the masked attention attend within a more appropriate foreground region.

\subsection{Loss function}
\label{sec:method:arch:loss}

The overall loss function for \modelname can be written as:
\begin{equation}
\mathcal{L} = \mathcal{L}_\text{IA-q}+\mathcal{L}_\text{pred}+\mathcal{L}'_\text{pred}
\end{equation}
where $\mathcal{L}_\text{IA-q}$ is the instance activation loss of the auxiliary classification head for IA-guided queries, $\mathcal{L}_\text{pred}$ and $\mathcal{L}'_\text{pred}$ are prediction loss and GT mask-guided loss, respectively.

\noindent\textbf{Instance activation loss.} The $\mathcal{L}_\text{IA-q}$ is defined as:
\begin{equation}
	\mathcal{L}_\text{IA-q} =\lambda_\text{cls-q}\mathcal{L}_\text{cls-q} 
\end{equation}
where $\lambda_\text{cls-q}$ is a hyperparameter and $\mathcal{L}_\text{cls-q}$ is the  cross-entropy loss with a weight $1/T$ for ``no object" class. Here $T \!=\!(H/16)\!\times\!(W/16)$ is the spatial size of E$_4$ from which IA-guided queries are selected. We use the Hungarian algorithm~\cite{kuhn1955hungarian} to search for the optimal bipartite matching between the prediction and ground truth sets. For the matching cost, we add an additional location cost $\mathcal{L}_\text{loc}$ of a weight $\lambda_\text{loc}$ to the above classification cost, as illustrated in \secref{sec:method:arch:query}.

\noindent\textbf{Prediction loss.} Following the prior work~\cite{cheng2021mask2former}, the prediction loss $\mathcal{L}_\text{pred}$ for the Transformer decoder is defined as:
\begin{equation}
	\mathcal{L}_\text{pred} = \sum_{i=0}^{D}  (\lambda_\text{ce}\mathcal{L}^i_\text{ce} + \lambda_\text{dice} \mathcal{L}^i_\text{dice}) + \lambda_\text{cls} \mathcal{L}^i_\text{cls}
	\label{sec:loss:prediction}
\end{equation}
where $D$ denotes the number of Transformer decoder layers and $i \!=\!0$ represents the prediction loss for IA-guided queries before being fed into the Transformer decoder, $\mathcal{L}^i_\text{ce}$ and  $\mathcal{L}^i_\text{dice}$ denote the binary cross-entropy loss and dice loss~\cite{milletari2016v} for segmentation masks, respectively, and $\mathcal{L}_\text{cls}$ is the cross-entropy loss for object classification with a ``no object" weight of $0.1$. $\lambda_\text{ce}$, $\lambda_\text{dice}$, and $\lambda_\text{cls}$ are the hyperparameters to balance three losses. Similarly, we exploit the Hungarian algorithm to search for the best bipartite matching for target assignments. And for the matching cost, we add an additional location cost $\lambda_\text{loc} \mathcal{L}_\text{loc}$ for each query. 
%We follow \cite{cheng2021mask2former} that uses point loss in mask loss for efficiency.

\noindent\textbf{GT mask-guided loss.} The  GT mask-guided loss $\mathcal{L}'_\text{pred}$ is similar to Equation~\eqref{sec:loss:prediction}. The only differences are that it does not count the $0$-th layer's loss and uses a fixed target assignment strategy, which is consistent with the bipartite matching result of the last Transformer decoder layer.
