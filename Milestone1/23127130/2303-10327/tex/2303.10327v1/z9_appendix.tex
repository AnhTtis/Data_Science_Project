\appendix
\clearpage
\renewcommand\thesection{\Alph{section}}
\setcounter{page}{1}
\setcounter{section}{0}
\setcounter{figure}{0}

{\LARGE\textbf{Supplementary Materials}}



\section{Proof for \theoref{theo-2}}
\label{appendix-prop2}
From $V_i(x_i, p_i)\leq c_i(p_i)$ we know that the entering state $x_i$ is within the maximum $\epsilon$-stable level set of equilibrium point $x^*$, hence the entering state $x_i$ is within the $\epsilon$-RoA of mode $i$. Next, we show that the next entering state $x_j = h_i(\bar{x}_i,u;p_i,p_j)$ is also within the $\epsilon$-RoA of mode $j$.

Since $x_i$ is within $\epsilon$-RoA of mode $i$,  we know  $||\bar{x}_i-x^*||\leq \epsilon$. Then from $\alpha_j||x-x^*||\leq V_j(x, p_j)\leq \beta_j||x-x^*||$ we have
\begin{equation}
\begin{aligned}
    V_j(x_j, p_j) & = V_j(h_i(\bar{x}_i,u;p_i,p_j), p_j)  \\
    & \text{(Definitions of jump maps and entering/exiting states)}\\
    & \leq \beta_j ||h_i(\bar{x}_i,u;p_i,p_j)|| \\ 
    & \text{(Lyapunov bounding condition)}\\
    & \leq \beta_j ||h_i(x^*,u;p_i,p_j)|| +\beta_j K_i ||\bar{x}_i-x^*|| \\ 
    & \text{(Local Lipschitz condition for $h_i$ at $x^*$)}\\
    % & \leq \beta_j ||h_i(x^*,u;p_i,p_j)|| + \beta_j K_i ||\bar{x}_i-x^*|| \quad \text{(Triangle inequality)} \\
    % & \leq \beta_j ||h_i(\bar{x}_i,u,p_i)-h_i(x_i^*)||+ \beta_j ||h_i(x_i^*)-x_j^*|| \quad \text{(Triangle inequality)} \\
    % & \leq \beta_j K_i ||x_w - x_i ^*|| + \beta_j ||h(x_i^*)-x_j^*|| \quad \text{(Lipschitz condition for $h_i$)} \\
    & \leq \beta_j||h_i(x^*,u;p_i,p_j)|| + \beta_j K_i \epsilon \\ 
    & \text{(Definition of $\epsilon$-RoA for mode $i$)}\\
    & \leq \frac{\beta_j}{\alpha_j} V_j(h(x_i^*, u; p_i, p_j), p_j) + \beta_j K_i \epsilon \quad \\
    & \text{(Lyapunov function bounding condition)}
    \\
    & \leq \frac{\beta_j}{\alpha_j} \Big(\frac{\alpha_j}{\beta_j} c_j(p_j)-\alpha_jK_i\epsilon\Big) + \beta_j K_i \epsilon \\ 
    & \text{(The condition in \eqref{eq:v-condition})} \\ 
    & \leq c_j(p_j)
\end{aligned}
\end{equation}
therefore we derive that $V_j(x_j,p_j)\leq c_j(p_j)$, which means $x_j$ is within the $\epsilon$-RoA for mode $j$. So the whole hybrid system is $\epsilon$-stable according to Def.~\ref{def:e-stable}.



\section{Proof for \theoref{theo-3}}
\label{appendix-prop3}
We consider the lower bound of $||p_i-p_k||$ for every jump. We know that the Lyapunov value at the entering state of mode $k$ (denote the switching $i\to k$) is:
\begin{equation}
\begin{aligned}
& V_k(h_i(x_i,u;p_i,p_k), p_k) \\
& = V_k(h^+(x_i, p_i)+p_i-p_k, u; p_k) \\ 
& \text{(definition of the special system)} \\
& \leq \beta_k ||h^+(x_i, p_i)+p_i-p_k|| \\ 
& \text{(Lyapunov bounding condition)}\\
& \leq \beta_k ||h^+(x_i, p_i)||+\beta_k||p_i-p_k|| \\ 
& \text{(Triangle inequality)}\\
& \leq \beta_k ||h^+(x^*, p_i)||+K_m\beta_k||x_i-x^*||+\beta_k||p_i-p_k|| \\ 
& \text{(Local Lipschitz condition)} \\
& = K_m\beta_k||x_i-x^*||+\beta_k||p_i-p_k|| \\ 
& \text{(Since $h^+(x^*, p_i)=x^*$)}\\
& \leq \beta_k K_m \epsilon + \beta_k ||p_i-p_k|| \\ 
& \text{(Definition of $\epsilon$-RoA)} \\
 \end{aligned}
\label{eq:appd-dist}
\end{equation}

If the optimization is feasible and the optimal $p$ exists, then from the assumption we know that $V_k(h^+(x_i,p_i)+p_i-p_k, p_k)$ for the optimal $p$ must be no larger than $c_k(p_k)$ (zero the first loss in \eqref{eq:loss-e-heur}). We are going to show that $V_k(h^+(x_i,p_i)+p_i-p_k, p_k)$ must strictly equal to $c_k(p_k)$. If not, based on the continuity of the $V_k$, there must exist a $\tilde{p}_k$ around $p_k$ that can also zero the first loss term in \eqref{eq:loss-e-heur}, and make $||\tilde{p}_k-p_j||\leq ||p_k-p_j||$ which brings contradiction. Thus we have $V_k(h^+(x_i, p_i)+p_i-p_k, p_k)=c_k(p_k)$, hence based on \eqref{eq:appd-dist}, we have:
\begin{equation}
    \begin{aligned}
    ||p_i-p_k||\geq \frac{c_k(p_k)}{\beta_k} - K_m \epsilon
    \end{aligned}
\label{eq:appd-dist2}
\end{equation}
For each jump, the step length is lower bounded as shown in \eqref{eq:appd-dist2}. Thus we have the number of jumps is:
\begin{equation}
    \begin{aligned}
    N\leq \left\lceil\frac{||p_j-p_i||}{\min\limits_{m}\frac{c_m}{\beta_m}-K_{m} \epsilon}\right\rceil 
    \end{aligned}
\end{equation}

\section{Details for the simulation environments}
\label{appendix-sim-env}
\subsection{Car tracking control}

The goal here is to make sure the car can drive on the road under different road conditions. Given a reference state $(x, y, v, \psi)^T$ for a Dubins car model, the state of the car model is $(x_e, y_e, \delta, v_e, \psi_e, \dot{\psi}_e, \beta)^T$, where $x_e, y_e$ represent the Cartesian error, $\delta$ denotes the steering angle, $v_e$ denotes the velocity error, $\psi_e$ and $\dot{\psi}_e$ are the heading angle error and angular velocity error, and $\beta$ is the slip angle. The dynamics are given by $\dot{x}=f(x)+g(x)u$, with 
\begin{equation}
    f(x)=\begin{pmatrix}
    v\cos(\psi_e) - v_{ref} + \omega_{ref} y_e \\
    v\sin(\psi_e)-\omega_{ref} x_e\\
    0\\
    0\\
    \dot{\psi}_e\\
    % -\frac{\mu m}{v I_x (l_r+l_f)} (l_f^2 C_{Sf} g l_r + l_r ^2 C_{Sr} g l_f)(\dot{\psi}_e+\omega_{ref}) + \frac{\mu m}{I_x(l_r+l_f)} (l_r C_{Sr} g l_f - l_f C_{Sf} g l_r) \beta + \frac{\mu m}{I_x(l_r+l_f)} (l_fC_{Sf}gl_r)\delta \\ 
    % (\frac{\mu}{v^2(l_r+l_f)}(C_{Sr}gl_fl_r - C_{Sf} g l_r l_f) - 1) (\dot{\psi}_e-\omega_{ref})-\frac{\mu}{v(l_r+l_f)}(C_{Sr}gl_f + C_{Sf}gl_r)\beta + \frac{\mu}{v(l_r+l_f)}(C_{Sf} g l_r)\delta
    C_1(\dot{\psi}_e+\omega_{ref}) + C_2 \beta + C_3 \delta \\ 
    C_4 (\dot{\psi}_e-\omega_{ref}) + C_5\beta + C_6 \delta \\
    % -\frac{\mu m}{v I_x (l_r+l_f)}(l_f^2 C_{Sf} g l_r + l_r ^2 C_{Sr} g l_f)(\dot{\psi}_e+\omega_{ref}) \\
    % \frac{\mu m}{I_x(l_r+l_f)} (l_r C_{Sr} g l_f - l_f C_{Sf} g l_r) \beta \\
    % \frac{\mu m}{I_x(l_r+l_f)} (l_fC_{Sf}gl_r)\delta \\
    % \\
    % (\frac{\mu}{v^2(l_r+l_f)}(C_{Sr}gl_fl_r - C_{Sf} g l_r l_f) - 1) (\dot{\psi}_e-\omega_{ref}) \\
    % \frac{\mu}{v(l_r+l_f)}(C_{Sr}gl_fC_{Sf}gl_r)\beta + \frac{\mu}{v(l_r+l_f)}(C_{Sf} g l_r)\delta 
    \end{pmatrix}
\end{equation}
with
\begin{equation}
    \begin{cases}
    C_1=-\frac{\mu m}{v I_x (l_r+l_f)} (l_f^2 C_{Sf} g l_r + l_r ^2 C_{Sr} g l_f) \\
    C_2=\frac{\mu m}{I_x(l_r+l_f)} (l_r C_{Sr} g l_f - l_f C_{Sf} g l_r) \\
    C_3=\frac{\mu m}{I_x(l_r+l_f)} (l_fC_{Sf}gl_r)\\
    C_4=\frac{\mu}{v^2(l_r+l_f)}(C_{Sr}gl_fl_r - C_{Sf} g l_r l_f) - 1 \\
    C_5=-\frac{\mu}{v(l_r+l_f)}(C_{Sr}gl_f + C_{Sf}gl_r)\\
    C_6=\frac{\mu}{v(l_r+l_f)}(C_{Sf} g l_r)\\
    \end{cases}
\end{equation}
and 
\begin{equation}
    g(x)=\begin{pmatrix}
    0 & 0 \\
    0 & 0 \\
    1 & 0 \\
    0 & 1 \\
    0 & 0
    \end{pmatrix}
\end{equation}
where $u$ is the acceleration and the steering angle output, $I_x, l_r, l_f, C_{Sf}, C_{Sr}, g$ are constant factors, and $\mu$ is the road friction factor. More details can be found in~\citep{althoff2017commonroad}.

The road consists of multiple segments with different road conditions (different $\mu$). Each segment belongs to a system mode with the configuration of reference waypoint $(X^E, Y^E)$, reference velocity $v^E$ and the friction factor $\mu$. Different combinations of friction factor and the velocity will give different traction force for the vehicle. At junctions of the two segments, the mode switching causes the system state jump because of the change of the reference waypoint.

\subsection{Pogobot navigation} The state of the pogobot is $s=(x, \dot{x}, y, \dot{y})^T$, where $x,y$ are the 2d coordinate of the pogobot head, and the $\dot{x}, \dot{y}$ are the corresponding velocities. The movement of a pogobot involves two phases. In the flight phase, the pogobot follows a ballistic dynamics $\dot{s}=f(s)$ where:
\begin{equation}
    f(s)= \begin{pmatrix}\dot{x} \\ 0 \\ \dot{y} \\ -g  \end{pmatrix}
\end{equation}
here $g$ is the gravity and the stance foot is determined by the pogobot pose $\theta$ (which can be controlled instantly, since we assume a mass-less leg). In the stance phase, together with the stance foot position $(x_f, y_f)^T$, the dynamics becomes
\begin{equation}
    \dot{s}=\begin{bmatrix}  
    \dot{x}\\
    \frac{x-x_f}{L} \left(k\left(L-l_0\right) + F\right)\\
    \dot{y}\\
    \frac{y-y_f}{L} \left(k\left(L-l_0\right)+F\right)-g
    \end{bmatrix}
\end{equation}
where $L=\sqrt{(x-x_f)^2+(y-y_f)^2}$ denotes the length of the current pogobot, $l_0$ denotes the original length of the pogobot, $k$ denotes the spring constant factor, and $F$ and $\theta$ are the control inputs (the stance force and the swing leg angle). Here we consider the apex-to-apex control strategy. We first collect the simulation data to learn an apex-to-apex dynamic estimator. Then we use this dynamic estimator to train our Lyapunov function and controllers (as well as the RL-based methods).

\subsection{Bipedal walker locomotion}
The state of the Bipedal walker is $s=(q, \dot{q})^T$ where $q=(q_1, q_2)^T$, and $q_1$ is the angle between the normal vector of the ground and the stance leg, and the $q_2$ is the angle between the stance leg and the swing leg. $\dot{q}_1$ and $\dot{q}_2$ are the corresponding angular velocities. Within each mode, the continuous dynamics of the system follows the manipulator equation:
\begin{equation}
    \dot{s}=\begin{bmatrix} \dot{q} \\ D^{-1}_s(q)[-C_s(q,\dot{q})-G_s(q)+B_s(q)u] \end{bmatrix}
\end{equation}
where $D_s, C_s, G_s, B_s$ are functions of $q$ (and $\dot{q}$), $u$ is the control input (torque in this case), and a state jump will occur when $q_2 + 2 q_1 = 0$. Here $B=(1, 0)^T$. We denote the leg mass $m$, the original leg length $l$ with center of mass (CoM) location $l_c$, the acceleration due to gravity $g_0$, and the leg inertial about leg CoM as $I$. Then the $D_s(q)$ can be written as:
\begin{equation}
    \begin{cases}
        (D_s(q))_{1,1}=(l-l_c)^2 m + I\\
        (D_s(q))_{1,2}= ml(l-l_c)\cos(q_2) - (l-l_c)^2 m - I\\
        (D_s(q))_{2,2}=-2ml (l-l_c) \cos(q_2) +\\
        \quad \quad \quad \quad\quad \quad \quad \quad\quad \quad (2(l_c^2 + l^2)-2l_c l) m + 2 I
    \end{cases}
\end{equation}
and the nonzero entries in $C_s$ are:
\begin{equation}
    \begin{cases}
      (C_s(q,\dot{q}))_{1,2}=-ml\sin(q_2) (l-l_c) \dot{q}_1 \\
      (C_s(q_2,\dot{q}_1))_{2,1}=-ml\sin(q_2) (l-l_c) (\dot{q}_2 - \dot{q}_1) \\
      (C_s(q_2,\dot{q}_1))_{2,2}=-ml\sin(q_2) (l-l_c) \dot{q}_2 \\
    \end{cases}
\end{equation}
and the nonzero entries in $G_s$ are:
\begin{equation}
    \begin{cases}
        (G_s(q_1,q_2))_1 = m g_0 \sin(q_2-q_1) (l-l_c) \\
        (G_s(q_1, q_2))_2 = m g_0 ((l_c-l) \sin (q2-q1)-\sin(q_1)(l_c+l))
    \end{cases}
\end{equation}

We recommend readers to~\citep{choi2022computation} for more details.


\section{Implementation of our approach}
\label{appendix-our-impl}


As mentioned in Algorithm. 1, we first learn the control Lyapunov function (CLF) and the NN controller, then we estimate the RoA, and finally we conduct online optimization for the planner in the deployment phase. 

For the CLF and controller learning phase (take the car tracking experiment as an example), we uniformly sample 1000 states from an initial set, then at every epoch we sample trajectories for 100 time steps from the corresponding environment simulator given the NN controller, We use the trajectories to train our CLF and NN controller. The CLF and the NN controller are 2-hidden-layer NNs with 256 hidden units in each layer and ReLU activation in the intermediate layers. The last layer of the controller uses TanH activation function to clip the output signal in a reasonable range. We train the CLF and NN controller via the loss in \eqref{eq:loss}. We use RMSprop gradient descent method for the optimization with the learning rate of $10^{-4}$. We train CLF and controller for (at maximum) 1000 epochs, where inside each epoch the CLF and the controller will be updated for 500 steps. We stop the training when the validation loss is not dropping significantly.

For the RoA Estimation phase, we use the CLF and the NN controller trained in the previous step to generate $10^3\sim 10^4$ trajectories in 100 time steps. Using CLF, we are able to find the largest Lyapunov value $c_i^*$  for all the sampled initial states that having the exiting states within the $\epsilon$-ball of the equilibrium. We set the $\epsilon=10^{-2}$. Then we train the RoA estimator using the loss in \eqref{eq:loss-roa} for 50000 iterations, with RMSprop optimizer and learning rate of $10^{-4}$.

For the online planning (deployment) phase, we use the differentiable planner to plan for valid configurations, and use the controller to "follow" that configuration to maintain stability. For the differentiable planner, at every mode switching instant, we first randomly generate 1000 configuration hypothesis. Then we use RMSprop optimizer and conduct gradient descent with learning rate of 0.05 for 5 steps. Then we pick the updated configuration hypothesis with the lowest loss.

\subsection{Car tracking control}

Before entering the $i$-th segment, we optimize for the configuration $p_i$, which is the waypoint $W_i=(x^{ref}_i, y^{ref}_i)$ at the junction between the $i$-th segment and the $i+1$-th segment, and the reference velocity $v^{ref}_i$ to track on the $i$-th segment. And at the $i$-th segment, we use the environment reference waypoint $(x^E_{i+1}, x^E_{i+1})$ and the reference velocity $v^E_{i+1}$ for the next configuration. We make sure:
(1) the current entering state $x_i$ is within the RoA of the current system under the configuration of $p_i=(W_i,v^{ref}_i)$. (2) the next entering state $x_{i+1}$ is within the RoA of the system at segment $i+1$ with configuration $p_{i+1}$.

\subsection{Pogobot navigation}
Before entering the $i$-th segment, we optimize for the configuration $p_i$, which is the reference apex state height and reference apex state horizontal velocity at the next cycle (during the $i$-the segment). We can find the last apex state $\tilde{x}_i$ before exiting the $i$-the segment using the dynamics estimator, and make sure $\tilde{x}_i$ is within the RoA for the $i+1$-th segment under the reference apex state $X^E_{i+1}$ given from the environment.


\subsection{Bipedal walker locomotion}
In this case, due to the difficulty to synthesize a control Lyapunov function, for the low-level controller, we directly use the QP controller derived from~\citep{choi2022computation} and the corresponding Lyapuonv function is replaced by an RoA classifier, which outputs value $<$0.9 if the entering state is within the RoA, and outputs value $>$1.1 for the rest case. During the planning, we use the differentiable planner but with the loss \eqref{eq:loss-e-heur} in to find the optimal configuration (in this case, the reference gait). 


\section{Implementation of baseline approaches}
\label{appendix-bsl-impl}
\partitle{Model-based reinforcement learning approaches} We compare with the Model-based Policy Optimization (MBPO) method~\citep{janner2019trust} implemented by an open-sourced library~\citep{pineda2021mbrl}. Based on the hyper-parameters provided in the library configuration files, we then searched for 3$\sim$ 10 different hyper-parameters (learning rates, policy update frequency, etc) and picked the one with the highest task-performance. We finalize the hyper-parameters and train the MBPO policy under 3 different random seeds for 24 hours for each experiment. 

\partitle{Model-free reinforcement learning approaches} We modify the RL implementation code from \url{https://github.com/RITCHIEHuang/DeepRL_Algorithms}, created the RL environments for each experiment, train each method with 3 random seeds for 24 hours each and take the average performance in the testing. For the car experiment, we use the reward as a combination of Root Mean Square Error (RMSE) penalty with the reference state and the valid rate of the trajectory segments. For the pogobot experiment, we use the RMSE, the collision rate with the ceiling/floor, and the distance to the goal as the rewards/penalties. For the Bipedal walker, we use the L2-distance from the current state to the reference state on the target gait as the penalty to guide the controller to converge to the target gait.

\partitle{Model predictive control approaches} We use the CasADi~\citep{andersson2019casadi} to implement non-linear optimization for each tasks. In each case, the system is simulated under some parameters (controls) and the cost function is computed to optimize the control input. For the car experiment, the cost function is the tracking error within the prediction horizon (T=20). For the pogobot experiment, the cost function is a penalty term with collisions and a tracking error term to the horizontal reference velocity. For the bipedal walker experiment, the cost function is the L2-norm between the leg angle $q_1$ after the switching and the target gait leg angle $q_1^{ref}$. \postac{Due to the difficulty/scalability to encode the RoA conditions (as they are the outputs from the neural networks) in the traditional solver, we did not use RoA condition constraints in the MPC optimization.}

\partitle{Linear quadratic control approaches} At each segment, we require the car to track the segment endpoint and the designed reference velocity on the current segment given the friction factors. We compute the error dynamics, and synthesize the controller by solving the Algebraic Ricatti Equation similar as in ~\citep{dawson2022safe}. 

\partitle{Control Lyapunov function approaches} Followed by \citep{chang2019neural}, we jointly train a single NN Lyapunov function for all the system modes with a NN controller (that can also take modes as inputs), with the same amount of training time used as for our approach. 

\partitle{CLF-QP approach} We directly use the QP controller derived from~\citep{choi2022computation} for the Bipedal walker comparison.

\partitle{Hamilton Jacobian based approach} We directly use the computed result (the value function) from~\citep{choi2022computation} for the comparison for the target gait with the leg angle $q_1=0.13$ rad.



% \newpage
% \section{Comparison to Model-base RL method}
% %We also conduct additional experiments to compare with model-base RL method. 
% We compare with the Model-based Policy Optimization (MBPO) method~\citep{janner2019trust} implemented by an open-sourced library~\citep{pineda2021mbrl}. Based on the hyper-parameters provided in the library configuration files, we then searched for 3$\sim$ 10 different hyper-parameters (learning rates, policy update frequency, etc) and picked the one with the highest task-performance. We finalize the hyper-parameters and train the MBPO policy under 3 different random seeds for 24 hours for each experiment. 

% \begin{figure}[!htbp]
% \floatconts{fig:mbrl-car-comparisons}
% {\caption{Quantitative result (including model-base RL method) for car experiment}}
% {
% \subfigure[Lane deviation]{  % 50 0.30
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_car_masked_dev.png}
% }
% \subfigure[Mean square error]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_car_masked_rmse.png} 
% }
% \subfigure[Distance to goal]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_car_dist_goal.png}
% }
% \subfigure[Computation time]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_car_runtime_step.png}
% }
% }
% \end{figure}


% \begin{figure}[!htbp]
% \floatconts{fig:mbrl-pogo-comparisons}
% {\caption{Quantitative result (including model-base RL method) for pogobot experiment}}
% {
% \subfigure[Velocity error]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_pogo_v_err.png}
% }
% \subfigure[Distance to goal]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_pogo_goal_len.png}
% }
% \subfigure[Collision rate]{  % 50 0.30
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_pogo_hit.png} 
% }
% \subfigure[Runtime]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_pogo_runtime_step.png}
% }
% }
% \end{figure}


% \begin{figure}[!htbp]
% \floatconts{fig:mbrl-bp-comparisons}
% {\caption{Bipedal walker comparison (including model-base RL method) under same ((a)$\sim$(d)) and different target gaits ((e)$\sim$(h)).}}
% {
% \subfigure[RMSE]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_walker_rmse.png}
% }
% \subfigure[Failure rate]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_walker_fail.png}
% }
% \subfigure[Invalid rate]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_walker_invalid.png}
% }
% \subfigure[Computation time]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_walker_runtime_step.png}
% }
% \subfigure[RMSE]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_walkerMul_rmse.png}
% }
% \subfigure[Failure rate]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_walkerMul_fail.png}
% }
% \subfigure[Invalid rate]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_walkerMul_invalid.png}
% }
% \subfigure[Computation time]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new/bar_walkerMul_runtime_step.png}
% }
% }
% \end{figure}


% As shown in \figref{fig:mbrl-car-comparisons}, \figref{fig:mbrl-pogo-comparisons} and \figref{fig:mbrl-bp-comparisons}, the MBPO result is on par of other RL baselines. MBPO achieves the lowest collision rate in the pogobot experiment, which is because it jumps out of the valid region before resulting in collision (which can be inferred by the high distance-to-goal measure for MBPO).

%%%%%%%%%%%%%%%%% MBPO using our learned dynamics %%%%%%%%%%%%%%%%%
% To figure out how well, under the pogobot experiment, we further train the MBPO with a pretrained dynamic model derived from our method, and continue the training for 12 hours. As shown in \figref{fig:mbrl-pret-pogo-comparisons}, though the distance-to-goal performance is slightly better, still far from the result our method can achieve.



% \begin{figure}[!htbp]
% \floatconts{fig:mbrl-pret-pogo-comparisons}
% {\caption{Quantitative result (including model-base RL method with pretrained dynamic model) for pogobot experiment}}
% {
% \subfigure[Velocity error]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new2/bar_pogo_v_err.png}
% }
% \subfigure[Distance to goal]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new2/bar_pogo_goal_len.png}
% }
% \subfigure[Collision rate]{  % 50 0.30
% \includegraphics[width=0.23\textwidth]{figures/supple_new2/bar_pogo_hit.png} 
% }
% \subfigure[Runtime]{  % 27 0.18
% \includegraphics[width=0.23\textwidth]{figures/supple_new2/bar_pogo_runtime_step.png}
% }
% }
% \end{figure}



% \newpage
\section{Ablation studies for our method in the car experiment}
\label{appendix-abl-hyper}
We first study the selection of hyperparameters in differentiable planning process. We tuned for different possible values for the $\eta$ and $\kappa$ in the differential planning process. As shown in Table~\ref{table:abl-eta} and Table~\ref{table:abl-kappa}, the performance is not sensitive to the hyperparameter selections. For all our experiments, we choose $\eta=0.9$ and $\kappa=10^{-2}$.
%%%%%%%%%%%%%%%%% ablation: opt-hyper-params %%%%%%%%%%%%%%%%%
\begin{table}[!htbp]
\begin{center}
% \subfloat[How different $\eta$ will affect the performance in car experiment.]{
\begin{tabular}{|c c c c c|} 
 \hline
 $\eta$ & $\kappa$ & Lane deviation & RMSE & Distance to goal \\ 
 \hline
1.2 & $10^{-2}$ & 2.082 & 0.53018 & 0.117 \\
\hline
1.0 & $10^{-2}$ & 2.100 & 0.523 & 0.166 \\
\hline
0.9 & $10^{-2}$ & 2.087 & 0.514 & 0.117 \\
\hline
0.8 & $10^{-2}$ & 2.084 & 0.514 & 0.117 \\
\hline
0.5 & $10^{-2}$ & 2.121 & 0.538 & 0.167 \\
\hline
\end{tabular}
\caption{How different $\eta$ will affect the performance in car experiment.}
% }
\label{table:abl-eta}
\end{center}
\end{table}
\begin{table}[!htbp]
\begin{center}
% \quad
% \subfloat[How different $\kappa$ will affect the performance in car experiment.]{
\begin{tabular}{|c c c c c|} 
 \hline
 $\eta$ & $\kappa$ & Lane deviation & RMSE & Distance to goal \\ 
 \hline
0.9 & 0.0 & 2.107 & 0.519 & 0.166 \\
\hline
0.9 & $10^{-3}$ & 2.116 & 0.516 & 0.166 \\
\hline
0.9 & $10^{-2}$ & 2.087 & 0.514 & 0.117 \\
\hline
0.9 & $10^{-1}$ & 2.147 & 0.545 & 0.217 \\
\hline
0.9 & $10^0$ & 2.106 & 0.521 & 0.117 \\
\hline
\end{tabular}
\caption{How different $\kappa$ will affect the performance in car experiment.}
\label{table:abl-kappa}
\end{center}
\end{table}


Our method uses Euler method to approximate the continuous dynamics, which might lead to estimation error. Now we study how different approximations for continuous ODE will affect the control performance. We choose different simulation time duration $\Delta t$ (from 10ms to 0.16ms) and conduct the testing for our pretrained controller on the car benchmark. As shown in Table~\ref{table:abl-dt}, the performance is consistent across varied $\Delta t$ and all the metrics converges as $\Delta t\to 0$. As the time duration $\Delta t\to 0$, the estimation error will also decrease and the control performance will gradually converge to the performance on the real continuous dynamics. \postac{A larger $\Delta t $($>$10ms) might result in great estimation error in forward Euler approximation, thus we do not perform those experiments. We aim to tackle the controller synthesis problem under imperfectly estimated system dynamics in future works.}

%%%%%%%%%%%%%%%%% ablation: dt-differenece  %%%%%%%%%%%%%%%%%
\begin{table}[!htbp]
\begin{center}
\begin{tabular}{|c c c c|} 
 \hline
 Simulation $\Delta t$ (ms) & Lane deviation & RMSE & Distance to goal \\ 
 \hline
10.0 & 2.114 & 0.533 & 0.166 \\
\hline
5.0 & 2.125 & 0.522 & 0.216 \\
\hline
2.5 & 2.118 & 0.528 & 0.216 \\
\hline
1.25 & 2.106 & 0.531 & 0.166 \\
\hline
0.625 & 2.104 & 0.531 & 0.166 \\
\hline
0.3125 & 2.105 & 0.531 & 0.166 \\
\hline
0.15625 & 2.104 & 0.531 & 0.166 \\
\hline
\end{tabular}
\caption{How different simulation $\Delta t$ will affect the performance in car experiment.}
\label{table:abl-dt}
\end{center}
\end{table}

\postac{\section{Comments about data distribution}
In all our experiments, we use uniform sampling techniques to sample data from the state space (and configuration space) to learn the Control Lyapunov function, the controller, and the RoA estimator. This might not be the most efficient way to sample the data, as in high dimensional space, the feasible (stable) trajectories normally reside in a small volume of reachable sets. Commonly-used method to tackle this issue is to use a counter-example guided approach~\citep{chang2019neural} or to use rare-case event estimation to reweight the sampling distribution~\citep{sinha2020neural}. Improving the sampling quality and efficiency is beyond the scope of this project and we hope to address those in future work.}


\section{Success rate for Bipedal walker locomotion under different initial conditions}


\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{figures/supple/walker_succ_curve.png}
\caption{Walker success rate comparison under different initial states}
\label{fig:supple-walker-init}
\end{figure}


For the bipedal walker experiment, we compare our approach with multiple RL-based approaches (A2C, DDPG, PPO, SAC, TD3, TRPO, VPG) and classic methods (QP, HJB) under different initial gait angles. As shown in~\figref{fig:supple-walker-init}, our approach can achieve similar-to-HJB performance, outperforming all the RL-baselines and the QP baseline. The largest improvement (comparing to RL methods) is from the "small initial angles". And our gain compared to QP-based methods is mostly from the "large initial angles", which might because the large deviation from the target gait angle makes the linearization more inaccurate, hence the QP-based method cannot achieve good performance.


\section{Visualization of learned RoA}
\label{appendix-roa-viz}
From \figref{fig:supple-car-roa-02} to \figref{fig:supple-walker-roa-04}, we show the visualization of the learned RoA in all three experiments under different configurations.


\input{z11_roa_car}
\input{z11_roa_pogo}
\input{z11_roa_bipedal}
\clearpage

\section{Visualization for the simulations}
\label{appendix-sim-viz}

From \figref{fig:supple-car-sim-04} to \figref{fig:supple-cgw-simx-430}, we visualize simulation results for all three experiments under different configurations. 

\input{z11_sim_car}
\input{z11_sim_pogo}
\input{z11_sim_bipedal}

