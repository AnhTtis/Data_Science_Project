\input{fig_latex/fig1_env.tex}

\section{Experiments}
\begin{wrapfigure}{r}{0.55\textwidth}
\floatconts{fig:car-roa-comparisons}
{\caption{RoA comparison with LQR.}}
{
\subfigure[RoA]{  % 28 27 0.18
\includegraphics[width=0.15\textwidth]{figures/car_roa_curve.png} \hfill
}
\hfill
\subfigure[x-y plane]{  % 33 27 0.18
\includegraphics[width=0.17\textwidth]{figures/new_roa_dim_1_0.png} \hfill
}
\hfill
\subfigure[v-$\delta$ plane]{  % 33 27 0.18
\includegraphics[width=0.17\textwidth]{figures/new_roa_dim_1_1.png} \hfill
}
}
\end{wrapfigure}
We conduct three challenging experiments shown in \figref{fig:env}. Our method achieves the best performance (success rate, RMSE, etc) than most baselines and is 10$\sim$50X faster than MPC. Our approach also results in shorter training time (0.5X of RL methods and 0.1X of HJB approaches).
\partitle{Baselines} For all cases we compare with: 
 model-base RL (MBPO~\citep{janner2019trust}), model-free RL(SAC~\citep{haarnoja2018soft}, PPO~\citep{schulman2017proximal} and DDPG~\citep{lillicrap2015continuous}) and model predictive control (MPC). Besides, for the car case, we compare with Linear Quadratic Regulator (LQR) and single CLF~\citep{chang2019neural}. For the bipedal we compare with quadratic program (QP) and Hamilton-Jacobian (HJB)~\citep{choi2022computation}.
We did not compare with HJB for (9-dim) car or (8-dim) pogobot as the state dimension is too high for HJB to handle. 
\partitle{Implementation details}
For CLF, the controller and the RoA estimator, we use 2-layer NNs with 256 units in each layer and ReLU~\citep{nair2010rectified} in intermediate layers. The controller uses TanH~\citep{lecun2012efficient} in the last layer to clip the output in a reasonable range. We implement our method in PyTorch~\citep{paszke2019pytorch}. The training takes 3$\sim$6 hours on an RTX2080 Ti GPU. More details are in \appref{appendix-bsl-impl}. 



\partitle{Remarks on sample efficiency}
As shown in \figref{fig:env}, compared to RL under the same sample size, we achieve the highest rewards. This is because RL directly interacts with the hybrid systems, whereas ours learns to control the \postac{system } under each mode, which is easier. %At deployment, our planner tackles the transitions of the hybrid system and the total run time is on par of RL. 
%Case-by-case analysis is shown in following sections.

\input{fig_latex/fig4_car_exp.tex}
\input{fig_latex/fig5_car_viz.tex}

\subsection{Car control under different road conditions}
We consider the tracking control problem for the single-track model in~\citep{althoff2017commonroad} under varied frictions. The system is hybrid as varied frictions and tracking velocities lead to different dynamics. The challenge is that when the friction is low (e.g., on icy roads), the road cannot provide enough traction to keep the car on the lane. To stabilize the system, the controller needs to track proper configurations (speeds and waypoints). Our method ensures the entering state at the connection of segments is always in the RoA of the next segment. \partitle{Setups} We generate 25 maps with 10 randomly sampled segments. Each segment has friction $\in\{0.1,1\}$ and length $\in[7.5\text{m},37.5\text{m}]$. The angle between consecutive segments $\in[\frac{3\pi}{4},\pi]$. \partitle{Metrics} We measure average lane deviation, mean square error (MSE) to the reference trajectory, distance to the goal (before driving out of lane), and the run time per step.



% We first train our NN Lyapunov function and the controller (pre-trained from LQR) under different configurations, and then we compare the RoA with the LQR method under a specific configuration. As shown in \figref{fig:car-roa-comparisons}(a), the RoA of our approach surpasses the LQR method in just a few epochs and converges in 100 epochs, becoming nearly 2X larger than the LQR RoA. We also visualize the RoA projected onto different planes shown in \figref{fig:car-roa-comparisons}(b, c). Our gain is mainly from handling broader velocity error, tire angle error, and longitudinal and lateral errors. This shows our learning framework can enlarge the RoA to stabilize more states. Due to page limit, we visualize the RoA for other configurations and the rest experiments in \appref{appendix-roa-viz}.
As shown in \figref{fig:car-roa-comparisons}(a), the RoA of our approach surpasses the LQR method in just a few epochs and converges in 100 epochs, becoming nearly 2X larger than the LQR RoA. From the RoA visualization in \figref{fig:car-roa-comparisons}(b, c), we show that the gain is mainly from velocity error, tire angle error, longitudinal and lateral errors. This shows our method can enlarge the RoA to stabilize more states. The rest visualizations can be found in \appref{appendix-roa-viz}.



Next, we compare with other baselines. As shown in~\figref{fig:car-comparisons}, we achieve the lowest lane deviation and MSE, 67\%$\sim$75\% reduction in the distance to goal, and low run time on par of RL, which is less than 0.1X of the time used by MPC. Qualitatively, as shown in \figref{fig:car-comparisons} (here we omit MBPO, SAC and PPO because they cannot provide reasonable trajectories), our approach is the only method that can keep the car on the road. The reason is that our method learns to decelerate and turn left to prepare for the next icy road segment (\figref{fig:car-comparisons}(a)), so that the car can gain more traction on the icy road for a normal left turn (\figref{fig:car-comparisons}(b)) where other methods fail to keep the car on the road (\figref{fig:car-comparisons}(c)).


\input{fig_latex/fig6_pogo_exp.tex}
\input{fig_latex/fig7_pogo_viz.tex}

\subsection{Pogobot navigation}
We control a pogobot (with the model in~\citep{zamani2019feedback}) to jump through 2d mazes shown in \figref{fig:pogo-comparisons}. The pogobot alternates between flight and stance phases. 
%In the flight phase the pogobot follows a ballistics in the air.
%After landing, it enters the stance phase, where its leg gets compressed and then recovers to push the pogobot to the air again. 
The apex state is when the pogobot is at the top of a trajectory in the flight phase. Given the reference apex states, the goal is to jump through the maze safely. Our method \postac{first learns the unknown apex state dynamic then } plans configurations (reference apex state) to ensure apex states are within the RoA of the next reference apex state. \partitle{Setups} We generate 25 maps by randomly sampling 3$\sim$5 segments with a segment length $\in[3m,6m]$, a floor height $\in[-0.5m,2m]$, a ceiling height $\in[1.5m,3.5m]$, and a reference speed $\in[0.5m/s,1.5m/s]$. \partitle{Metrics} We measure the velocity error, the remaining distance to goal, the collision rate, and the computation runtime.

Since we use a learned dynamics in this case, we also compare with MBPO with pretrained dynamic model learnt from our approach (denoted as $\text{MBPO}^*$). As shown in \figref{fig:pogo-comparisons}, our approach achieves the lowest velocity tracking error, remaining distance to goal, low collision rate and computation time (MBPO achieves the lowest collision rate as it often jumps out of valid region before crashing - which explains its high distance-to-goal metric). Our approach uses just 1/70X of the computation time as needed for MPC. The simulation in \figref{fig:pogo-comparisons} shows that ours is the only approach to safely jump through the 2d maze, whereas DDPG and PPO methods start to jump to the left afterwards (here we omit the SAC result because it cannot plan for one cycle), and MPC method results in collisions. More details can be found in \apprefref{appendix-sim-env}{appendix-our-impl}. %\appref{appendix-sim-env} $\sim$ \appref{appendix-our-impl}.

\input{fig_latex/fig8_bipedal_exp.tex}

\subsection{Bipedal walker locomotion}

We control the bipedal robot~\citep{choi2022computation} to converge to a target gait. The configuration here is the leg angle $q_1$ of the next gait at the switch surface shown in \figref{fig:env}(c). It is hard to converge directly to the goal gait when the initial gait is far away. Thus, we use the loss in \eqref{eq:loss-e-heur} in planning to ensure the gait at each switching is closer to the target gait and our controller guarantees the state converges to the intermediate gait. \partitle{Setups} We calculate the gaits using Frost Library~\citep{hereid2017frost}, uniformly sample the initial states around each gait with angle $\in[0.04,0.18]$ and target gait~$\in[0.04,0.18]$. When compared to HJB, we set the target gait$=$0.13$\text{rad}$ since HJB only handles that gait. We compare other baselines with different target gaits. \partitle{Metrics} We measure RMSE towards reference gaits, failure rate for convergence, invalid trajectories rate and run time.

As shown in \figref{fig:bipedal-comparisons}(a)-(d), our approach achieves the closest RMSE, failure rate, and invalid trajectory rate to HJB approach. The third best method for failure rate is MPC, which takes a much longer computation time. MBPO has the worst performance probably due to the complicated dynamics which is hard to learn. Compared to HJB, our advantage is in the quick adaptation to other targeted gaits. Trained in less than 6 hours, our method can also learn to converge to other target gaits, whereas the learning time for the HJB method to converge to one target gait is 36 hours. We compare our method with other non-HJB approaches for converging to different gaits. As shown in \figref{fig:bipedal-comparisons}(e)-(h), we achieve the lowest RMSE, failure rate, invalid rate, and low running time. 




\subsection{Limitations}
\label{sec:limitations}
%Although our method achieves success in challenging benchmarks, we admit there are some limitations. 
Firstly, the RoA estimation is after the controller training, thus a refinement for the RoA estimator is needed if the controller is updated. One thought is to design a ``robust'' RoA estimator that can tolerate mild changes in controller parameters.
Besides, since the certificates are NNs with limited amount of neurons, we cannot guarantee the certificates being satisfied in the whole state space. Also we might bring in errors when numerically approximate the continuous dynamic, though we show in~\appref{appendix-abl-hyper} that the performance is consistent across varied $\Delta t$. In addition, we assume we can obtain full information for the system state and assume no disturbances or noises, which might not hold in real-world experiments. We aim to solve these in future works. 

