\section{Experiments}
\label{sec:experiment}
In this section, we conduct extensive experiments on three public datasets to answer the following research questions:
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ1:} How does CDR perform under user preference shifts as compared to the baselines?
    % \item \textbf{RQ2:} How can we adjust the environment numbers in the training and inference to improve the generalization performance of CDR?
    \item {\textbf{RQ2:} How can the different designs in CDR (\eg the environment numbers, the sparse structure, inference strategies, multi-objective loss, and hyper-parameters) affect the performance?}
    \item {\textbf{RQ3:} How can we intuitively understand the effectiveness of CDR by case studies?}
\end{itemize}


\subsection{Experimental Settings}
\vspace{1pt}
\noindent$\bullet\quad$\textbf{Datasets.}
We evaluate the baselines and the proposed CDR on three real-world datasets: 1) Yelp\footnote{\url{https://www.yelp.com/dataset.}} is a public restaurant recommendation dataset, which contains rich interaction features such as ratings and timestamps; 2) Book is one of the Amazon product review datasets\footnote{\url{https://jmcauley.ucsd.edu/data/amazon/.}}, which covers extensive users' ratings over books;
and 3) Electronics is also from the Amazon datasets, in which users interact with various electrical products.  

% \begin{itemize}[leftmargin=*]
%     \item [1)] Yelp\footnote{\url{https://www.yelp.com/dataset.}} is a public restaurant recommendation dataset, which contains rich interaction features such as ratings and timestamps.
%     \item [2)] Book is one of the Amazon product review datasets\footnote{\url{https://jmcauley.ucsd.edu/data/amazon/.}}, which covers extensive users' ratings over books.
%     \item [3)] Electronics is also from the Amazon product datasets, in which users interact with various electrical products.  
% \end{itemize}
The statistics of datasets are summarized in Table~\ref{tab:datasets_statics}.
To ensure the data quality~\cite{he2020lightgcn}, we only keep the users and items with at least 20 interactions on Yelp and Book. Besides, we only discard the users and items with less than 10 interactions on Electronics because the numbers of users and items are relatively small as shown in Table \ref{tab:datasets_statics}. Moreover, only the interactions with ratings $\geq 4$ are considered as positive samples on all three datasets. We sort user interactions chronologically, and then split the interactions of each user by the ratio of $80\%$, $10\%$, $10\%$ into training, validation, and test sets, respectively.

% statics of datasets
\begin{table}[t]
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{0cm}
\caption{Statistics of the three datasets.}
\label{tab:datasets_statics}
\begin{center}
% \setlength{\tabcolsep}{1mm}{
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{lllll}
\toprule
{Dataset} & {\#User} & {\#Item} & {\#Interaction} & {Density} \\ \hline 
{Yelp} & 11,622 & 9,095 & 487,000 & 0.004607 \\ 
{Book} & 21,923 & 23,773 & 1,125,676 & 0.002159 \\
{Electronics} & 9,279 & 6,065 & 158,979 & 0.002825\\ \bottomrule
\end{tabular}
%}
}
\end{center}
\vspace{-0.5cm}
\end{table}







\vspace{5pt}
\noindent$\bullet\quad$\textbf{Baselines.}
We compare CDR with the state-of-the-art collaborative filtering, disentangled and sequential models.

\textbf{- MF}~\cite{rendle2009bpr} is one of the most influential collaborative filtering methods, which factorizes the sparse interaction matrix into the user and item embedding matrices. % , which paves the way for powerful recommendation methods using neural networks.  

\textbf{- LightGCN}~\cite{he2020lightgcn} is a powerful GCN-based recommender model, which discards the useless feature transformation and nonlinear activation in GCN, and highlights the most essential neighborhood aggregation for collaborative filtering.
% as the key part of GCN for  recommendation, where only linearly propagating of item embeddings are implemented.  

\textbf{- MultiVAE}~\cite{li2017collaborative} is the most representative VAE-based recommender model, which captures the interaction generation process but ignores the temporal preference shifts.

\textbf{- MacridVAE}~\cite{ma2019learning} proposes the disentangled user representations at the intention and preference levels. The disentanglement enhances the model robustness against preference shifts. 
% proposes a hierarchical disentanglement of user intents, bringing out more robust representations of the user as user preference shifts.  

{\textbf{- DIB}~\cite{liu2021mitigating} utilizes information theory to disentangle biased and unbiased embeddings, and only considers unbiased embeddings for robust interaction prediction.}

{\textbf{- COR}~\cite{wang2022causal} proposes a causal OOD framework to handle the observed user feature shifts.}

{\textbf{- DIEN}~\cite{zhou2019deep} focuses on a new structure to model the interest evolving process, leading to more expressive user representations.}

{\textbf{- MGS}~\cite{lai2022attribute} applies a session graph generated from the user interaction sequence to capture transition patterns of user preference. We do not use user and item features in COR, DIEN, and MGS for fair comparison with other methods.} 


\textbf{- DSSRec}~\cite{ma2020disentangled} introduces the techniques of self-supervised learning and disentangled representations to sequential recommendation. However, it ignores the advantages of learning disentangled representations from multiple environments.
% Besides, to capture the user intention in complex interaction sequences, it performs intention clustering and alignment in the disentangled latent space.
% a competitive sequential recommendation method, proposes
% compares the sub-sequences similarity

\textbf{- ACVAE}~\cite{xie2021adversarial} is one of the state-of-the-art sequential recommender models, which incorporates contrastive learning and adversarial training into the VAE-based method. 

\textbf{- CauseRec}~\cite{zhang2021cause} constructs counterfactual sequences by keeping the indispensable interactions and replacing the dispensable ones. These counterfactual sequences are then used as augmented samples for contrastive training. 
% takes a causal perspective to user behaviors and construct counterfactual data by keeping the indispensable data while replacing those unnecessary ones. Contrastive user representation learning is also utilized in this work.  

We omit more sequential models such as GRU4Rec~\cite{GRU4Rec} and BERT4Rec~\cite{Bert4Rec} since ACVAE and CauseRec have shown better performance than them. 

\vspace{5pt}
\noindent$\bullet\quad$\textbf{Evaluation.}
% introduce all ranking protocol. Metrics 
We follow the all-ranking protocol~\cite{he2020lightgcn} to evaluate the performance of all methods, where all non-interacted items are used for ranking and top-ranked items are returned as recommendations. Thereafter, we adopt Recall@$K$ (R@$K$) and NDCG@$K$ (N@$K$) as the evaluation metrics, where $K = 10$ or $20$ on three datasets. 
%  We report the performance by R@$K$ and N@$K$ with $K=\{10,20\}$ on three datasets. 


\vspace{5pt}
\noindent$\bullet\quad$\textbf{Hyper-parameter settings.}
% introduce the search scope of hyper-parameters for our model and baselines. 
Based on the default settings of baselines, we enlarge their hyper-parameter search scope and tune hyper-parameters as follows:

{- MF}{ \& LightGCN}: The learning rate is searched in $\{0.001, 0.01, 0.1\}$. We search the best embedding size from $\{32, 64, 128\}$. For LightGCN, we tune the weight decay in $\{1e^{-5}, 1e^{-4}, 1e^{-3}\}$ and the number of GCN layers in $\{3, 4, 5\}$.
% because they affect the model capability greatly.

{- MultiVAE}{ \& MacridVAE}: We follow the default settings and additionally tune the learning rate, the hidden size, the regularization coefficient $\beta$ in $\{1e^{-4}, 1e^{-3}, 1e^{-2}\}$, $\{[800], [600, 200], [800, 500]\}$, and \{0.3, 0.5, 0.7, 0.9\}, respectively. As to special hyper-parameters in MacridVAE, we choose the number of macro factors from \{2, 4, 10, 20\}, the number of micro factors from \{200, 300, 400, 500\}, and the coefficient $\tau$ from \{0.05, 0.1, 0.2\}.

{{- DIB}{ \&COR}: For DIB, the embedding size is set in \{32, 64, 128\}. We adjust $\alpha$ in \{0.001, 0.1, 0\}, $\beta$ in \{0.0001, 0.001, 0.01, 0.1\} and $\gamma$ in \{0, 0.1, 0.2\} to make a balance between the biased and unbiased vector. For COR, we search the hidden size of encoder $q(\cdot)$ and the KL coefficient $\beta$ in $\{[300], [800], [800,600]\}$, and $\{0.3,0.5,0.7,0.9\}$, respectively. The sizes of $Z_1$ and $Z_2$ are chosen in $\{100,200,300\}$.}

{{- DIEN}{ \& MGS}: For DIEN, the embedding size is set in $\{32, 64, 128, 256, 512\}$. We search the dropout ratio in $\{0.1, 0.3, 0.5, 0.6\}$ and the weight of auxiliary loss in $\{0, 0.4, 0.6, 0.8, 1.0\}$. For MGS, we choose the embedding size from $\{50, 100, 200\}$, the number of GNN layers from $\{1, 3, 5, 6\}$, and the sequence length from $\{10, 30, 50\}$.}

{- DSSRec}{ \& ACVAE}{ \& CauseRec}: We search the training sequence length and the embedding size in $\{50, 100, 200\}$ and $\{32, 64, 128\}$, respectively. For DSSRec, the number of latent categories $K$ is chosen from $\{1, 2, 4, 8\}$ and the weight decay is set by $\{0, 0.01, 0.05\}$. For ACVAE, the weight of contrastive loss term $\beta$ is searched in $\{0.1, 0.3, 0.5, 0.7\}$. For CauseRec, the number of concepts and weight decay are tuned in $\{20, 30, 40\}$ and $\{1e^{-5}, 1e^{-4}, 1e^{-3}\}$, respectively.

As to CDR, we implement it by Pytorch and utilize Adam for optimization. For fair comparison, we choose the hyper-parameters by following the settings of the baselines. 
We set the batch size as $500$ and the learning rate as $1e^{-4}$. The dropout ratio is chosen from $\{0.4, 0.5, 0.6\}$. The hidden size of $g_{\phi}(\cdot)$ is searched in $\{[800], [600, 200], [800, 500]\}$. The sizes of $\bm{e}_t$ and $\bm{z}_t$ are tuned in $\{200, 300, 400, 500\}$. Both $f_{\theta_1}(\cdot)$ and $f_{\gamma}(\cdot)$ are set as a fully-connected layer to save parameters. $T$, $\lambda_1$, $\lambda_2$, $\lambda_3$ and $C$ are tuned in $\{1, 2, ..., 6\}$, $\{0.1, 0.2, ..., 0.9\}$, $\{0.1, 0.2, ..., 1\}$, $\{1e^{-5}, 1e^{-4}, 1e^{-3}, 1e^{-1}\}$, and $\{1, 2, 3, 4, 10, 20\}$, respectively. Moreover, early stopping is performed for model selection, \ie stop training if recall@10 on the validation set does not increase for 10 successive epochs. More details can be found in the released code. 




\begin{table*}[t]
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\caption{The performance comparison between the baselines and CDR on the three datasets. The best results are highlighted in bold and the second-best ones are underlined. \%improve. indicates the relative improvements of CDR than the second-best results. $*$ implies the improvements over the best baseline are statistically significant ($p\text{-value}< 0.05$) under one-sample t-tests.}
\label{tab:overall_per}
\begin{center}
\renewcommand\arraystretch{1.1}
\setlength{\tabcolsep}{0.6mm}{
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccc|cccc|cccc}
\hline
 & \multicolumn{4}{c|}{\textbf{Yelp}} & \multicolumn{4}{c|}{\textbf{Book}} & \multicolumn{4}{c}{\textbf{Electronics}} \\ 
 % MF DIEN Lightgcn | DIB COR | MultiVAE MacridVAE | MGS DSSRec ACVAE CausalRec
Methods & R@10 & R@20 & N@10 & N@20 & R@10 & R@20 & N@10 & N@20 & R@10 & R@20 & N@10 & N@20 \\ \hline
MF & 0.0385 & 0.0659 & 0.0269 & 0.0365 & 0.0206 & 0.0355 & 0.0142 & 0.0193 & 0.0333 & 0.0551 & 0.0186 & 0.0243 \\
LightGCN & 0.0402 & 0.0695 & 0.0288 & 0.0390 & 0.0252 & 0.0434 & 0.0170 & 0.0233 & 0.0365 & 0.0560 & 0.0194 & 0.0246 \\ \hline
MultiVAE & 0.0427 & 0.0728 & 0.0303 & 0.0409 & 0.0280 & 0.0475 & 0.0197 & 0.0264 & 0.0419 & 0.0658 & 0.0224 & 0.0286 \\
MacridVAE & {\ul 0.0442} & {\ul 0.0770} & 0.0319 & {\ul 0.0434} & 0.0409 & 0.0667 & 0.0288 & 0.0378 & 0.0424 & 0.0654 & 0.0246 & 0.0306 \\ \hline
{DIB} & {0.0375} & {0.0654} & {0.0264} & {0.0362} & {0.0211} & {0.0362} & {0.0144} & {0.0196} & {0.0319} & {0.0530} & {0.0176} & {0.0231} \\
{COR} & {0.0411} & {0.0690} & {0.0293} & {0.0392} & {0.0400} & {0.0681} & {0.0286} & {0.0385} & {0.0428} & {0.0625} & {0.0237} & {0.0289} \\ \hline
% 0.0275	0.0449	0.0202	0.0263	
% 0.0279	0.0382	0.0250	0.0284
% 0.0347	0.0512	0.0196	0.0240
{DIEN} & {0.0275} & {0.0449} & {0.0202} & {0.0263} & {0.0279} & {0.0382} & {0.0250} & {0.0284} & {0.0347} & {0.0512} & {0.0196} & {0.0240} \\
{MGS} & {0.0423} & {0.0696} & {0.0313} & {0.0409} & {0.0515} & {0.0738} & {0.0458} & {0.0532} & {0.0415} & {0.0625} & {0.0230} & {0.0285}\\
DSSRec & 0.0413 & 0.0697 & 0.0299 & 0.0400 & 0.0539 & 0.0790 & 0.0448 & 0.0534 & 0.0503 & {\ul 0.0780} & 0.0269 & 0.0343 \\
ACVAE & 0.0439 & 0.0750 & {\ul 0.0322} & 0.0432 & {\ul 0.0563} & {\ul 0.0860} & {\ul 0.0477} & {\ul 0.0576} & {\ul 0.0510} & 0.0766 & {\ul 0.0290} & {\ul 0.0359} \\
CauseRec & 0.0433 & 0.0762 & 0.0300 & 0.0417 & 0.0484 & 0.0753 & 0.0391 & 0.0482 & 0.0445 & 0.0744 & 0.0230 & 0.0309 \\ \hline 
CDR & \textbf{0.0528*} & \textbf{0.0880*} & \textbf{0.0392*} & \textbf{0.0518*} & \textbf{0.0721*} & \textbf{0.1042*} & \textbf{0.0598*} & \textbf{0.0708*} & \textbf{0.0647*} & \textbf{0.0933*} & \textbf{0.0373*} & \textbf{0.0449*} \\
\% Improve. & 19.46\% & 14.29\% & 21.74\% & 19.35\% & 28.06\% & 21.16\% & 25.37\% & 22.92\% & 26.86\% & 19.62\% & 28.62\% & 25.07\% \\\hline
\end{tabular}
}
}
\end{center}
\vspace{-0.1cm}
\end{table*}



\subsection{Overall Performance (RQ1)}\label{sec:overall_per}
We present the results of the baselines and CDR on the three datasets in Table \ref{tab:overall_per}. From the table, we have the following observations:



\begin{itemize}[leftmargin=*]
    % macrivae consisently 优于multivae和lightGCN MF
    \item MacridVAE consistently outperforms MF, LightGCN, and MultiVAE on the three datasets. We attribute the superior performance to the disentangled user representations of MacridVAE. The preference shifts only affect partial user representations while most disentangled user representations of MacridVAE are robust to the shifts. Besides, the sequential models (\ie DSSRec, ACVAE, and CauseRec) {and the session-based model (\ie MGS)} usually perform better than MF, LightGCN, and MultiVAE, which verifies the effectiveness of considering temporal information in capturing preference shifts.
    
    % macridVAE 优于seq 在yelp上 
    % sequential models 显著优于macrivae on Book and Electronics
    \item {The performance of MacridVAE is better than that of sequential models (DSSRec, ACVAE, and CauseRec) on Yelp while the sequential models surpass MacridVAE on Book and Electronics. Meanwhile, DIEN outperforms MF on Book and Electronics while yields inferior performance on Yelp.}
    % This is due to the competition between disentangled representations and sequential modeling.
    This is because the effect of preference shifts is quite different on the three datasets. 
    Temporal preference shifts are stronger on Book and Electronics, and thus sequential modeling is more effective to capture the shifts. In contrast, user preference over food is relatively stable on Yelp, where MacridVAE is superior to model the invariant preference. 
    
    % ACVAE 优于 DSSRec & causeRec
    \item In sequential models, ACVAE usually achieves higher performance than DSSRec and CauseRec. This is probably because ACVAE introduces adversarial training and contrastive learning into sequential VAE, which also encourages the independence of latent factors in user representations. Such independence might have a similar effect as disentangled representations of CDR, \ie the sparse structure. The main difference is that the disentangled representations of CDR are learned from multiple environments, which are more robust under preference shifts. Besides, the inferior performance of DSSRec and CauseRec might be attributed to the improper intention clustering~\cite{ma2020disentangled} and inaccurate identification of dispensable concepts~\cite{zhang2021cause}, respectively. 
    
   \item {COR is usually comparable with MacridVAE on Book and Electronics while performs worse than sequential models such as ACVAE. This is reasonable since COR eliminates the out-of-date information and reuses the stable preference, leading to robust user representations against user preference shifts. However, COR ignores the temporal feature shifts, resulting in worse performance than sequential models. Besides, DIB shows relatively worse results, which is possible because that biased embeddings might be still useful and totally discarding them loses critical user preference.}  
    
    % CDR best performance
    \item CDR significantly yields the best performance on the three datasets. Specifically, the performance improvements of CDR over the best baseline \wrt Recall@10 are 19.46\%, 28.06\%, and 26.86\% on Yelp, Book, and Electronics, respectively. This justifies the superiority of handling user preference shifts via the CDR framework. CDR does not only capture the temporal preference trend between environments for better preference estimation, but also learns a robust structure from user preference to interactions, leading to better interaction prediction. 

\end{itemize}

\begin{figure}[t]
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
  \centering 
  \hspace{-0.1in}
  \subfigure{
    \includegraphics[width=1.8in]{figures/group_test_yelp.pdf}} 
   \hspace{-0.1in}
  \subfigure{
    \includegraphics[width=1.8in]{figures/group_test_book.pdf}} 
   \hspace{-0.1in}
  \subfigure{
    \includegraphics[width=1.8in]{figures/group_test_ele.pdf}} 
  \hspace{-0.1in}
  \caption{{Performance comparison on three user groups with different strengths of shifts, where the shifts increase from G1, G2, to G3.}}
  \label{fig:user_cmp}
\end{figure}


{To evaluate the performance of CDR under different strengths of shifts, we split users into groups according to the KL divergence between training and testing environments \wrt their interacted item categories. As shown in Figure~\ref{fig:user_cmp}, the preference shifts increase from G1, G2, to G3. From the figure, we can find that 1) CDR consistently achieves better performance across three groups; and 2) the performance of the best baselines, MacridVAE and ACVAE, usually decreases in the G3 group with large shifts while CDR still shows large improvements over the baselines, validating its stronger OOD generalization ability.} 






% hp of T
\begin{figure*}[t]
%  \vspace{-0.3cm}
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{0cm}
  \centering 
%   \hspace{-0.1in}
% \subfigbottomskip=-2pt
  \subfigure{
    \includegraphics[width=1.35in]{figures/hp_Tt_Yelp.pdf}} 
%   \hspace{-0.1in}
%   \vspace{-0.1in}
  \subfigure{
    \includegraphics[width=1.35in]{figures/hp_Tt_Book.pdf}} 
%   \hspace{-0.1in}
  \subfigure{
    \includegraphics[width=1.35in]{figures/hp_Tt_ele.pdf}} 
  \subfigure{
    \includegraphics[width=1.35in]{figures/hp_Ti_Yelp.pdf}} 
% \hspace{-0.1in} 
  \subfigure{
    \includegraphics[width=1.35in]{figures/hp_Ti_Book.pdf}} 
%   \hspace{-0.1in}
  \subfigure{
    \includegraphics[width=1.35in]{figures/hp_Ti_ele.pdf}} 
%   \hspace{-0.1in}
%   \vspace{0.15in}
  \caption{{Effect of environment numbers in the training ($T_t$) and inference ($T_i$) stages. We obtain the results of different $T_i$ by using the best $T_t=3$ for training and vary $T_i$ for inference.}
%   and similarly we show the performance of various $T_t$ by fixing the best $T_i$ for inference
  }
  \label{fig:envi_num}
  \vspace{-0.5cm}
\end{figure*}





% \vspace{-0.3cm}
\subsection{{In-depth Analysis (RQ2)}}
In addition to overall performance comparison, we conduct the in-depth analysis to study the effectiveness of different components in CDR, including multiple environments, the sparse structure for the disentanglement, conditional relations, inference strategies, the multi-objective loss, and hyper-parameter settings. Lastly, we provide some cases to show the effectiveness of CDR at a fine-grained level. 

% ablation of W
\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
  \centering 
  \hspace{-0.1in}
  \subfigure{
    \includegraphics[width=1.8in]{figures/hp_W_yelp.pdf}} 
   \hspace{-0.1in}
  \subfigure{
    \includegraphics[width=1.8in]{figures/hp_W_book.pdf}} 
   \hspace{-0.1in}
  \subfigure{
    \includegraphics[width=1.8in]{figures/hp_W_ele.pdf}} 
  \hspace{-0.1in} 
%   \vspace{-0.1in}
  \caption{Ablation study of the sparse structure (\ie $\bm{W}_z$ and $\bm{W}_x$). We only show the best baseline on each dataset to save space.}
  \label{fig:ablation_W}
\end{figure}

\subsubsection{\textbf{Effect of Multiple Environments}}
As illustrated in Section \ref{sec:environment}, the choice of the environment number $T$ is essential. 
As such, we study the effect of multiple environments by varying the environment numbers in the training and inference periods. We report the results with different environment numbers during training ($T_t$) and inference ($T_i$) in Figure \ref{fig:envi_num}. From the figure, we have the following findings.
\begin{itemize}[leftmargin=*]
    \item During training, the performance rises at first, and then drops with the increase of $T_t$. The rise validates the effectiveness of learning user representations from multiple environments instead of one environment. Besides, the performance drop also verifies the arguments in Section \ref{sec:environment}: more environments will make the interactions in each environment sparse, hindering the learning of invariant user preference within an environment and the disentangled representations (\ie the sparse structure $\bm{W}_z$ and $\bm{W}_x$). 
    
    \item In the inference stage, a larger $T_i$ than $T_t$ is able to further improve the performance of CDR, especially on Book and Electronics. This is due to the better utilization of temporal information. During inference, the sparsity issue in each environment will not affect the optimization of CDR. As such, we can fully utilize the temporal information of interactions and consider more fine-grained temporal preference shifts by using larger $T_i$ for the inference. 
    
    % todo: electronics上时序信息强，但是增加环境不太好
    \item The effectiveness of increasing $T_i$ is more significant on Book and Electronics than that of Yelp. The underlying reason is that Book and Electronics have stronger temporal shifts as discussed in Section \ref{sec:overall_per} and more environments help to capture more fine-grained shifts between environments. In contrast, user preference is relatively stable on Yelp, and thus CDR leverages fewer environments to better capture the invariant preference within each environment. This shows that CDR can flexibly balance the learning of invariant and shifted preference on different datasets by adjusting the environment number. 
\end{itemize}


% 1. training 先升后降。2）yelp 和 book 有差距是因为book sequential 信息更强。
% 3. inference 增加environment number performance更好。




\subsubsection{\textbf{Effect of Sparse Structure}}
To validate the effectiveness of the sparse structure from user preference to interactions, we perform the ablation studies over the two matrices $\bm{W}_z$ and $\bm{W}_x$. 
The results with (\ie w/ $\bm{W}$) and without (\ie w/o $\bm{W}$) the two matrices are provided in Figure \ref{fig:ablation_W}. The ablation of $\bm{W}_z$ and $\bm{W}_x$ denotes that CDR only uses an MLP model $f_\gamma(\cdot)$ to obtain $f_{\theta_2}(\bm{z}_t)$ in Eq. (\ref{eqn:f_theta_2}). From Figure \ref{fig:ablation_W}, we can observe that: 
\begin{itemize}[leftmargin=*]
    \item The performance declines if the two matrices are removed, showing the effectiveness of the sparse structure in modeling the effect of user preference shifts.
    \item CDR without $\bm{W}_z$ and $\bm{W}_x$ still outperforms the best baselines, \ie MacridVAE and ACVAE. Such improvements are attributed to the division of environments: without the disentanglement via the sparse structure, CDR still captures both the preference shifts between environments and the invariant preference within each environment by following the robust causal relations~\cite{locatello2020weakly, yoshua2020a}. 
\end{itemize}
% 1) ; and 2) 





% \begin{figure}[t]
% % \vspace{-0.2cm}
% \setlength{\abovecaptionskip}{0cm}
% \setlength{\belowcaptionskip}{0cm}
%   \centering 
% %   \hspace{-0.7in}
%   \subfigure{
%     \includegraphics[width=2.2in]{figures/hp_lam2_Yelp.pdf}}
%   \hspace{0in}
%   \subfigure{
%     \includegraphics[width=2.2in]{figures/hp_lam3_Yelp.pdf}}
%   \hspace{0in}
%   \subfigure{
%     \includegraphics[width=2.2in]{figures/hp_lam2_book.pdf}}
%   \hspace{0in}
%   \subfigure{
%     \includegraphics[width=2.2in]{figures/hp_lam3_book.pdf}}
% %   \hspace{-0.7in} 
% %   \vspace{-0.1in}
%   \caption{Effect of the sparsity and variance regularizers. $\lambda_2$ and $\lambda_3$ are their coefficients in the multi-objective loss.}
%   \label{fig:lambda_2_3}
% \end{figure}

% xinyu-todo: add results
\begin{table}[t]
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{0cm}
\caption{{Performance comparison of CDR with and without conditional relations.}}
\label{tab:conditional_cmp}
\begin{center}
\setlength{\tabcolsep}{2.8mm}{
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{2}{c}{Yelp} & \multicolumn{2}{c}{Book} & \multicolumn{2}{c}{Electronics} \\
 & R@10 & N@10 & R@10 & N@10 & R@10 & N@10 \\ \hline
ACVAE & 0.0439 & 0.0322 & 0.0563 & 0.0477 & 0.0510 & 0.0290 \\
CDR with $E_{t-1}\rightarrow E_t$ & 0.0450 & 0.0335 & 0.0584 & 0.0466 & 0.0549 & 0.0306 \\
CDR with $X_{t-1}\rightarrow Z_{t}$ & 0.0458 & 0.0340 & 0.0572 & 0.0491 & 0.0553 & 0.0314\\
Vanilla CDR & 0.0528 & 0.0392 & 0.0721 & 0.0598 & 0.0647 & 0.0373 \\\bottomrule
\end{tabular}
}
}
\end{center}
\vspace{-0.2cm}
\end{table}

\subsubsection{{\textbf{Effect of Conditional Relations}}}\label{sec:condition_rel}
{
We conduct experiments to compare the CDR performance with and without considering the conditional relations of $E_{t-1}\rightarrow E_{t}$ and $X_{t-1}\rightarrow Z_{t}$. Considering them will change the encoder $q(\bm{e}_t|\bm{x}_t)$ and the decoder module $p(\bm{z}_t|\bm{e}_t, \bm{z}_{t-1})$ into $q(\bm{e}_t|\bm{x}_t, \bm{e}_{t-1})$ and $p(\bm{z}_t|\bm{e}_t, \bm{z}_{t-1}, \bm{x}_{t-1})$, respectively. 
Such changes introduce more parameters due to the larger input dimension. 
The experimental results are presented in Table~\ref{tab:conditional_cmp}, from which we observe that 1) CDR with $E_{t-1}\rightarrow E_{t}$ or $X_{t-1}\rightarrow Z_{t}$ has inferior performance than the vanilla CDR. The possible reasons are that these conditional relations are not strong over a large proportion of users, and meanwhile the CDR with more parameters might overfit the training data, hurting the generalization ability in a new environment. Besides, 2) CDR with $E_{t-1}\rightarrow E_{t}$ or $X_{t-1}\rightarrow Z_{t}$ still surpasses the best baseline ACVAE, validating the effectiveness of modeling preference shifts and sparse influence by this CDR framework. 
}

\begin{table}[t]
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{0cm}
\caption{{Performance comparison of three inference strategies.}}
\label{tab:infer_cmp}
\begin{center}
\setlength{\tabcolsep}{2.8mm}{
\resizebox{0.66\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{2}{c}{Yelp} & \multicolumn{2}{c}{Book} & \multicolumn{2}{c}{Electronics} \\
 & R@10 & N@10 & R@10 & N@10 & R@10 & N@10 \\ \hline
1)  $\bm{z}_T$ & 0.0528 & 0.0392 & 0.0721 & 0.0597 & 0.0647 & 0.0373 \\
2) \text{avg} $\bm{x}_{1:T}$ & 0.0429 & 0.0312 & 0.0288 & 0.0206 & 0.0476 & 0.0258 \\
3) $\bm{z}_{T+1}$ & 0.0470 & 0.0340 & 0.0550 & 0.0449 & 0.0613 & 0.0336 \\\bottomrule
\end{tabular}
}
}
\end{center}
\vspace{-0.2cm}
\end{table}

\subsubsection{{\textbf{Effect of Inference Strategies}}}\label{sec:infer_unk}

{In Table~\ref{tab:infer_cmp}, we report the results of the three inference strategies detailed in Section~\ref{sec:task}. From Table~\ref{tab:infer_cmp}, we can find that 1) the first strategy outperforms the second and third strategies, and 2) the second one shows the worst results. These findings are reasonable because 1) the second strategy ignores the temporal distribution shifts and uniformly averages the predictions, and 2) the third strategy is better since it partly considers the shifts by $\bm{z}_T$ while $\bm{e}_{T+1}$ is still obtained by average, inevitably losing some temporal patterns. In future work, it is promising to explore more strategies to better capture temporal patterns for the inference. 
}


% hp of lam1
\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0.10cm}
\setlength{\belowcaptionskip}{0cm}
  \centering 
  \hspace{-0.30in}
  \subfigure{
    \includegraphics[width=1.65in]{figures/hp_lam1_Yelp.pdf}}
  \hspace{-0.1in} 
  \subfigure{
    \includegraphics[width=2in]{figures/hp_lam2_Yelp.pdf}}
  \hspace{-0.1in}
  \subfigure{
    \includegraphics[width=2in]{figures/hp_lam3_Yelp.pdf}
    \hspace{-0.30in}
    }
%   \hspace{-0.30in}
    % \vspace{-0.1in}
  \subfigure{
    \hspace{-0.30in}
    \includegraphics[width=1.65in]{figures/hp_lam1_Book.pdf}}
  \hspace{-0.10in}
  \subfigure{
    \includegraphics[width=2in]{figures/hp_lam2_book.pdf}}
  \hspace{-0.10in}
  \subfigure{
    \includegraphics[width=2in]{figures/hp_lam3_book.pdf}
    \hspace{-0.30in}
    }
%   \vspace{-0.1in}
  \caption{
%   Effect of the KL divergence. $\lambda_1$ is the coefficient of KL divergence in the multi-objective loss, which controls the regularization effect over $\bm{e}_t$.
    Effect of the coefficients in the multi-objective loss (\ie $\lambda_1$, $\lambda_2$, and $\lambda_3$). Specifically, $\lambda_1$ is the coefficient of KL divergence; $\lambda_2$ controls the sparsity regularization; and $\lambda_3$ adjusts the variance penalty term.
  }
  \label{fig:lambda}
\end{figure}

\subsubsection{\textbf{Effect of Multi-objective Loss}}
To analyze the influence of KL divergence, the sparsity, and variance regularizers in the multi-objective loss, we conduct experiments to compare the performance by changing their coefficients $\lambda_1$, $\lambda_2$, and $\lambda_3$. 
The results \wrt the three coefficients on Yelp and Book are presented in Figure \ref{fig:lambda}. The results on Electronics and Book are similar so that we omit the one on Electronics. From Figure \ref{fig:lambda}, we have the following observations:
\begin{itemize}[leftmargin=*]
    \item Increasing $\lambda_1$ of KL divergence is useful to improve the performance. Actually, as indicated by~\cite{higgins2017beta}, a large $\lambda_1$ for KL divergence regulates the independence of latent factors to facilitate disentangled representations; while a small $\lambda_1$ helps the model to fit the user interactions better~\cite{liang2018variational}. From Figure \ref{fig:lambda}, we find that $\lambda_1\in[0.5, 0.7]$ usually leads to a good balance and achieves superior performance, which is consistent with the findings in~\cite{liang2018variational}.
    \item The sparsity regularizer is essential to improve the performance because Recall and NDCG drop significantly when $\lambda_2$ is changed from $0.5$ to $0$. Besides, $\lambda_2$ cannot be too large, which will limit the representation capability of $\bm{W}_z$ and $\bm{W}_x$.
    \item The decreased performance from $\lambda_3=1e^{-4}$ to $\lambda_3=0$ justifies the usefulness of the variance regularizer, which balances the gradient optimization across multiple environments. Moreover, it should be noted that $\lambda_3$ is expected to be tuned in a small magnitude $[0, 1e^{-3}]$ because the stronger gradient regularization will inevitably disturb the normal optimization of parameters.
\end{itemize}

% The results \wrt $\lambda_1$ are presented in Figure \ref{fig:lambda}, from which we can find that KL divergence is useful to improve the performance. Actually, as indicated by~\cite{higgins2017beta}, a large $\lambda_1$ for KL divergence regulates the independence of latent factors in the representation to facilitate disentanglement while a small $\lambda_1$ helps the model to fit the user interactions better~\cite{liang2018variational}. From Figure \ref{fig:lambda}, we find that $\lambda_1\in[0.5, 0.7]$ usually leads to a good balance and achieves superior performance, which is consistent with the findings in~\cite{liang2018variational}.
% As to the results \wrt $\lambda_2$ and $\lambda_3$ in Figure \ref{fig:lambda}, our observations are as follows. 
% \begin{itemize}[leftmargin=*]
%     \item The sparsity regularizer is essential to improve the performance because Recall and NDCG drop significantly when $\lambda_2$ is changed from $0.5$ to $0$. Besides, $\lambda_2$ cannot be too large, which will limit the representation capability of $\bm{W}_z$ and $\bm{W}_x$.
%     \item The decreased performance from $\lambda_3=1e^{-4}$ to $\lambda_3=0$ justifies the usefulness of the variance regularizer, which balances the gradient optimization across multiple environments. Moreover, it should be noted that $\lambda_3$ is expected to be tuned in a small magnitude $[0, 1e^{-3}]$ because the stronger gradient regularization will inevitably disturb the normal optimization of parameters.
% \end{itemize}


% \begin{figure}[t]
% % \vspace{-0.2cm}
% \setlength{\abovecaptionskip}{0cm}
% \setlength{\belowcaptionskip}{0cm}
%   \centering 
% %   \hspace{-0.7in}
%   \subfigure{
%     \includegraphics[width=2.2in]{figures/hp_lam2_Yelp.pdf}}
%   \hspace{0in}
%   \subfigure{
%     \includegraphics[width=2.2in]{figures/hp_lam3_Yelp.pdf}}
%   \hspace{0in}
%   \subfigure{
%     \includegraphics[width=2.2in]{figures/hp_lam2_book.pdf}}
%   \hspace{0in}
%   \subfigure{
%     \includegraphics[width=2.2in]{figures/hp_lam3_book.pdf}}
% %   \hspace{-0.7in} 
% %   \vspace{-0.1in}
%   \caption{Effect of the sparsity and variance regularizers. $\lambda_2$ and $\lambda_3$ are their coefficients in the multi-objective loss.}
%   \label{fig:lambda_2_3}
% \end{figure}

\begin{figure}[t]
\setlength{\abovecaptionskip}{0.1cm}
\centering
\includegraphics[scale=0.25]{figures/hp_C_Yelp.pdf}
\caption{Effect \wrt different category numbers $C$ in the structure learning.}
\label{fig:hp_C_Yelp}
\end{figure}

\subsubsection{\textbf{Effect of Category Number}}
% 验证preference over 多个category的作用。为factorized W设置1、2、3、4的item category做了对比实验。 （1）大于1好 （2）2比较好
% To verify the effect of the multi-category classification in invariant structure learning, we compare the performances of experiments with different $C$, \ie the number of classified item categories in factorized matrix $\bm{W}_x$ and $\bm{W}_z$. 
To verify the effect of category numbers in sparse structure learning, we compare the performances with $C$ changing from 1 to 4. 
% We change $C$ from 1 to 4 and the performances are given in Figure \ref{fig:hp_C_Yelp}. 
% Note that setting $C$ at 1 is equivalent to removing the category-level preference, indicating all items belong to the same category. 
Note that $C=1$ is equivalent to removing the sparse structure, \ie using pure MLP to predict the interaction probability based on $\bm{z}_t$. 
We report the performance of CDR in Figure \ref{fig:hp_C_Yelp}.
By comparing the results \wrt Recall@10 and NDCG@10, we can find that:
\begin{itemize}[leftmargin=*]
    % \item The inferior performance of $C=1$ justifies that factorizing user representation into categorical-level preference benefits the learning of invariant structure. This is rational since items are often classified into various categories in real-world scenarios, \eg Romance and Mystery in genres of books.
    \item The inferior performance of $C=1$ justifies that disentangling user representations into categorical-level preference alleviates the negative effect of preference shifts. This is rational since it aligns with the real-world scenarios: items are classified into various categories and users have different preference over such categories. Once users have preference shifts, only partial factors in the user representations change and subsequently affect partial interactions, leading to better OOD generalization. 
    % \item The fluctuated performances from $C=2$ to $C=4$ indicate that neither Recall@10 nor NDCG@10 is simply proportional to the number of factorized categories over items. Instead, we can observe that $C=2$ leads to a surpassing performance from the results on Yelp.
    \item The fluctuated performance from $C=2$ to $C=4$ indicates that the performance increase is not proportional to the category number and $C=2$ shows better results. However, we usually have more categories in the real-world scenarios, for example, a variety of books. This implies that 1) it is non-trivial to recover the category-level preference from pure interaction data; and 2) incorporating item category into recommender models might help the disentanglement, which is left to future exploration.
    % and 3) another contrary possibility is that there exists the inconsistency between the labeled item categories and the category-level preference in users' interactions, which possibly causes that using item categories for some recommender models (\eg Factorization Machines~\cite{rendle2010factorization}) instead decreases the performance~\cite{wang2021deconfounded}. The positive or negative impacts of item categories can be studied in future work.
    
    % neither Recall@10 nor NDCG@10 is simply proportional to the number of factorized categories over items. Instead, we can observe that $C=2$ leads to a surpassing performance from the results on Yelp.
\end{itemize}


% hp of hidden size and dimension
\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0cm}
\setlength{\belowcaptionskip}{-0.1cm}
  \centering 
  \hspace{-0.7in}
  \subfigure{
    \includegraphics[height=1.6in]{figures/hp_hdsz_Yelp.pdf}}
%   \hspace{-0.1in}
  \subfigure{
    \includegraphics[height=1.6in]{figures/hp_dim_Yelp.pdf}}
  \hspace{-0.7in} 
%   \vspace{-0.1in}
  \caption{Effect of the hidden sizes of $g_\phi(\cdot)$, $\bm{e}_t$, and $\bm{z}_t$.}
  \label{fig:hidden_size}
\end{figure}


\subsubsection{\textbf{Effect of Hidden Size}}
% To investigate the effect of the hidden size and latent dimension, we carry out experiments \wrt the hidden size of $g_\phi(\cdot)$, the dimension of latent representation $e_t$, \ie $K$, and the dimension of user preference $z_t$, \ie $H$. 
We conduct experiments to investigate the effect of the hidden sizes of user representations and the latent dimension of VAE networks. In particular, we present the results \wrt varying hidden sizes of $\bm{e}_t$ ($K$), $\bm{z}_t$ ($H$), and the decoder network $g_\phi(\cdot)$.
% We change the hidden size of $g_\phi(\cdot)$, $K$ and $H$ in \{[200],[400],[800],[800,500]\}, \{200,300,400,500\}, respectively. From the results summarized in Figure \ref{fig:hidden_size}, we have the following findings:
From the results reported in Figure \ref{fig:hidden_size}, we have the following findings:
\begin{itemize}[leftmargin=*]
    % \item When the hidden size of $g_\phi(\cdot)$ is changed from [200] to [800], it is observed that a wider $g_\phi(\cdot)$ yields superior performance attributed to larger amount of parameters. Besides, from the comparison of different $e_t$ and $z_t$ dimension in range \{200, 300, 400\}, larger $H$ and $K$ improves Recall@10 by enriching the expressiveness of latent representation $e_t$ and user preference $z_t$. 
    \item When the hidden size of $g_\phi(\cdot)$ changes from $[200]$ and $[400]$ to [800], we can observe that a wider $g_\phi(\cdot)$ yields superior performance. Besides, by comparing the hidden sizes of $\bm{e}_t$ and $\bm{z}_t$ in the range \{200, 300, 400\}, we find a larger size significantly improves Recall@10 and NDCG@10. Such improvements are attributed to enriching the representation abilities of the encoder network and user representations. 

    \item Nevertheless, the model will suffer from over-fitting issue if we blindly increase the number of parameters. For example, the performance drops as we add the layers of $g_\phi(\cdot)$ from $[800]$ to $[800,500]$. 
    % Therefore, we should tune the hidden size of the encoder, $K$ and $H$ in an appropriate scope by taking the trade-off between generalization and avoidance of over-fitting into consideration.
    Therefore, controlling the parameter number for better OOD generalization is a wise choice. We should carefully tune the hidden sizes to balance the trade-off between the representation ability and over-fitting issue. 
\end{itemize}
We do not show the results on the decoder network because it is implemented by two full-connected layers (\ie $f_{\theta_1}(\cdot)$ and $f_{\lambda}(\cdot)$) whose dimension is decided by $K$, $H$, and the item number $I$ in the dataset. Similar to the encoder network $g_\phi(\cdot)$, we have validated that increasing the layer number of $f_{\theta_1}(\cdot)$ and $f_{\lambda}(\cdot)$ fails to improve the OOD generalization performance.
% todo: 在parameter settings 那里解释一下vae based method参数多。

{\subsection{Case Study (RQ3)}}
\subsubsection{{\textbf{Alignment between User Representation and Shifted Preference}}}
% To illustrate the effectiveness of our proposed CDR framework in capturing user preference shift over time, we analyze the user preference representation $z_t$ from views of user-level and population-level. 
% Specifically, we analyze the variation of $z_t$ across two environments against the actual preference shift. The variation of $z_t$ is measured by cosine similarity and euclidean distance and the actual preference shift between two environments is presented by KL divergence of category distribution of interacted items 
To intuitively understand how CDR captures the preference shifts, we conduct the case study from the user level and population level, respectively. Specifically, for each user, we extract user preference representation $\bm{z}_t$ and the interaction distribution over item categories at each environment. Thereafter, we study whether the shifts of user presentations align with the category-level interaction distribution, which can reflect if the user representations capture preference shifts well. 



\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\centering 
\includegraphics[width=3in]{figures/case_study_user_1.pdf}
\caption{Visualization of the KL divergence and Euclidean distance for the sampled user in Book. Note that KL divergence reflects the preference shifts in the interactions while Euclidean distance describes the similarity between the learned user representations.}
\label{fig:case_study_user}
\end{figure}

\begin{figure}[t]
\setlength{\abovecaptionskip}{0.2cm}
\centering
\includegraphics[scale=0.48]{figures/case_study_2space.pdf}
\caption{Visualization of the interaction distributions and the learned representations for two users in Book. The left figure depicts the user's dramatic shift over category-level preference between two environments. By contrast, the user in the right figure shows relatively stable preference for book genres. The changes of user representations have a good alignment with the user preference shifts.}
\label{fig:case_study_user_2}
\end{figure}


% As to the user-level analysis, we randomly select a user with significant preference shifts over 5 continuous environments in Book and collect the user representation in each environment. 
% We then calculate the cosine similarity of $z_t$ and $z_{t+1}$ for each $t \in \{1, 2, 3, 4\}$ and visualize the actual preference shift and similarity of user representation in Figure \ref{fig:case_study_user}.
% In addition to the qualitative analysis, we further present the specific category distribution of another randomly selected user and illustrate the relation between actual preference shift and user representation inferred by CDR in Figure \ref{fig:case_study_user_2}. 
\noindent$\bullet$ \textbf{User-level analysis.} We randomly select a user from the Book dataset who has significant shifts over the category-level interaction distribution.
% Specifically, these users' KL divergence variance in five continuous environments is larger than a threshold. 
For the sampled user, we calculate the KL divergence between the category-level interaction distributions in every two continuous environments. Besides, we estimate the user representation similarity between continuous environments by Euclidean distance. The changes of KL divergence and Euclidean distance in five environments are presented in Figure \ref{fig:case_study_user}. 
Furthermore, we also visualize the specific interaction distribution of another two randomly selected users at a more fine-grained level, whose user representations and interaction distributions are provided in Figure \ref{fig:case_study_user_2}.
From the two figures, we have the following conclusions.
\begin{itemize}[leftmargin=*]
    % \item The similarity of user representation performs an opposite pattern of KL divergence between two environments. In Figure \ref{fig:case_study_user}, when the KL divergence rises drastically, \eg from T2-T3 to T3-T4, the similarity of user representation is lessened accordingly. On the contrary, when the discrepancy of actual user preference between two environments declines, \eg from T1-T2 to T2-T3, the user representation tends to become more similar to each other. Therefore, the user representation $z_t$ inferred from CDR is capable of reflecting the preference shift over time.
    \item The Euclidean distance between user representations has a consistent pattern with the KL divergence between the category-level interaction distributions. For example, when the KL divergence is large in Figure \ref{fig:case_study_user} (\ie T1-T2 and T3-T4), the user representations show the large distance correspondingly. This indicates that user representations are less similar if the preference in the interactions is significantly shifted. In other words, such user representations are capable of capturing the preference shifts.  
    
    % \item As shown in Figure \ref{fig:case_study_user_2}, when the user's favor for Children's Books radically drops from 71.4\% to 16.7\% over all categories and start purchasing other categories, \eg Business, Romance, the KL divergence is large while similarity of user representation is small between two environments. However, if the user's taste for books is relatively stable, \eg Children's Books remains to be the favorite category, followed by Literature \& Fiction, the KL divergence is small while the similarity of user representation is large. 
    % user-level的specific example，当kld小，similarity大，当kld大，similarity小
    \item As to the specific examples in Figure \ref{fig:case_study_user_2}, we can find that the KL divergence well describes the preference shifts over categories: the first user's preference for Children's Books radically drops from 71.4\% to 16.7\% and we have a high KL divergence; by contrast, the interests of the second user are stable, and thus this user has a smaller KL divergence. More importantly, we observe that the user representations have the same distance shifts as the interaction distributions, which is consistent with the findings in Figure \ref{fig:case_study_user}.
\end{itemize}


\begin{figure}[t]
\setlength{\abovecaptionskip}{0.2cm}
\centering
\includegraphics[scale=0.4]{figures/case_study_population.pdf}
\caption{Visualization of KL divergence and Euclidean distance at the population level, which shows a strong positive correlation.}
\label{fig:case_study_pop}
\end{figure}

\begin{figure}[t]
\setlength{\abovecaptionskip}{0.2cm}
\centering
\includegraphics[scale=0.48]{figures/intervention.pdf}
\caption{{Visualization of the recommendation changes when exchanging $E_t$ of two users in Book. The recommendations are more similar to each other after the intervention.}}
\label{fig:case_intervention}
\end{figure}


% As to the population-level analysis, we rank the users by average actual preference shift over 6 continuous environments and divide all users into 50 groups from weak preference shift to strong preference shift. We then calculate the average KL divergence within each group as the representative of the group, where a smaller KL divergence indicates users with  more stable preferences over time. We utilize euclidean distance of user representation in analysis of the correlation between user representation distance and actual preference KL divergence. Visualization of the correlation is present in Figure \ref{fig:case_study_pop}.
\noindent$\bullet$ \textbf{Population-level analysis.} Although the user representations learned by CDR well capture the preference shifts for these users, how does CDR perform over the whole dataset? To answer this question, we do the population-level analysis. 
Specifically, we rank the users in Book according to the average KL divergence in multiple environments, and then divide all users into 50 groups based on the ranking. 
Next, we calculate the average KL divergence and the average Euclidean distance between user representations within each group. Visualization of the correlation between the KL divergence and Euclidean distance is shown in Figure \ref{fig:case_study_pop}, which validates that the euclidean distance between user representations is proportional to the KL divergence between user interactions. This demonstrates that our proposed CDR learns the user representations well to capture the preference shifts in the whole dataset. 
% The more unstable the user preference is, the farther away the user presentation is from each other.



\subsubsection{{{\textbf{Case Study on $do(E_t=\hat{\bm{e}}_t)$}}}}\label{sec:case_do_E}
{We do another case study to inspect whether the do-operation over $E_t$ generates reasonable changes in the recommendations. Specifically, we select two users in the Book dataset, exchange their user features $E_t$, and compare the changes of recommendations. Figure~\ref{fig:case_intervention} shows that their recommendations become similar to each other after the intervention. 
More ``Literature \& Fiction'' books are recommended to User 376 due to the high preference of User 11337 while some textbooks are exposed to User 11337 because User 376 likes them. These observations demonstrate that intervening $E_T$ can affect the predictions of $X_T$ and cause reasonable changes to recommendations in the CDR framework.}

