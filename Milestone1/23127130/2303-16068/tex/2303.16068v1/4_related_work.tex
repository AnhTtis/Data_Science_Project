\section{Related Work}
\label{sec:related_work}
In this work, we propose a causal disentangled framework to handle the preference shifts in recommendation, which is closely related to causal recommendation, disentangled recommendation, sequential recommendation, and preference shifts in recommendation. 

% \vspace{5pt}
% \noindent$\bullet\quad$\textbf{Causal recommendation.}
\subsection{Causal Recommendation}
% refer to www2022
In the past decade, data-driven recommender systems have been widely employed to alleviate the issue of information explosion on the Web~[\citealp{wu2019context, causpref, wang2022causal},~{\citealp{saito2022counterfactual}}]. 
Even though great success has been achieved, such data-driven approaches suffer from the issues of bias~[\citealp{zhang2021causal},~{\citealp{wu2022opportunity}}], unfairness~\cite{DiCiccio2020Evaluating}, and filter bubbles~\cite{Ge2020Understanding}.
Recently, the emerging causal approaches shed light on them~[\citealp{,Jadidinejad2021the, zhang2021deep, bonner2018causal,zou2020counterfactual,model-Agnostic},~{\citealp{xu2021causal}}]. 
Specifically, two strands of frameworks receive the most attention: the potential outcome framework~\cite{rubin2005causal} and structural causal models~\cite{pearl2009causality}. 
The potential outcome framework leverages inverse propensity scoring~\cite{saito2020unbiased} or doubly robust~\cite{wang2019doubly} to address the problem of biases in  explicit and implicit feedback~\cite{Zhang2020Large}, such as position bias~\cite{Thorsten2017unbiased} and exposure bias~\cite{saito2020unbiased}. Regarding the structural causal models,  intervention~\cite{zhang2021causal} and counterfactual inference~\cite{wang2021counterfactual,zhang2021cause,zou2020counterfactual} are used to estimate the causal effects~\cite{Pearl2018the} based on the causal relationships. And then the causal effects are more reliable to address recommendation issues, for example, debiasing, unfairness, OOD recommendation, and explanation~\cite{wang2021click, li2021towards, Khanh2021Counterfactual, causpref, wang2022causal}.



Although causal reasoning has been widely applied to recommendation~\cite{xu2023causal,zhu2023causal,gao2022causal,luo2023survey} {and some studies explore the OOD recommendation~\cite{causpref, wang2022causal}, current methods usually utilize user and item features for generalization and ignore the temporal preference shifts across environments.
% still follow the IID assumption between training and testing periods. The assumption is infeasible in practice since user preference is shifting over time. Existing causal models seldom consider the generation process of preference shifts across environments. 
To fill the gap, this work targets at the under-explored temporal preference shifts without using extra user-item features. }In detail, we discover the generation process of preference shifts under multiple environments and develop a causal framework to alleviate the detrimental effect of preference shifts. 
% Without considering such preference shifts may inevitably bring a series of problems, such as inducing 
% and declining the generalization ability in OOD environments~\cite{scholkopf2021toward}. Therefore, in this work, we target at the under-explored problem of user's preference shifting over time.


% \vspace{5pt}
% \noindent$\bullet\quad$\textbf{Disentangled recommendation.}
% refer to www2022
% \vspace{-0.1cm}
\subsection{Disentangled Recommendation}
Disentangled recommendation learns the independent representations for the hidden factors (\eg user intention and preference) behind the complex user behaviors, which can bring various merits such as offering explanation or improving the model robustness in the drifted distributions~\cite{ma2020disentangled,Pearl2018the}. 
In order to capture users' diverse preference on items, previous work often disentangles user preference by encouraging the independence of user representations~\cite{ma2020disentangled,wang2020disentangled,wang2020disenhan}. 
For instance, MacridVAE~\cite{ma2019learning} identifies the high-level intention representations for macro disentanglement, and forces each individual dimension in the intention representation to be independent for micro disentanglement. 
% reflect an independent micro factor by using a regularizer. 
Besides, Wang \etal proposed to disentangle the user representations in GCN-based recommender models to model the finer granularity of user intention. 
Lastly, MTIN \cite{jiang2020what} designs a time-aware mask network to distill the interaction sequence and adopts an interest mask network to aggregate fine-grained user preference representations. 

However, previous studies typically learn disentangled representations from the IID data, and thus ignore the significance of capturing robustness across multiple environments~\cite{locatello2020weakly}, decreasing the generalization ability under user preference shifts. 
% Our work advances and extends the disentanglement model to the scenarios with distribution (preference) shift. 
This work extends the disentangled approaches by considering both the multiple OOD environments and temporal preference shifts across environments. 
Additionally, we reformulate the disentangled representations in recommendation by two matrices and learn the matrices via the sparsity and variance regularization. 


% \vspace{5pt}
% \noindent$\bullet\quad$\textbf{Sequential recommendation.}
% \vspace{-0.12cm}
\subsection{Sequential Recommendation}
% refer to the related work in sequential baselines.
Collaborative filtering-based methods are widely employed in recommender systems~[\citealp{liang2018variational,sarwar2001item},~{\citealp{zhou2019deep,latifi2021session}}], where the user-item matching score is obtained based on user/item representation learned from user historical interactions. However, users' interactions are not independently generated because sequential patterns usually exist within users' consecutive behaviors. Therefore, sequential recommendation, which aims to recommend the next item to a target user, emerges and becomes popular in recent years~\cite{zhu2021learning,manotumruksa2020sequential,quadrana2017personalizing,sachdeva2019sequential}. Early work utilizes Markov Chain to capture the lower-order dependencies~\cite{rendle2010factorizing,he2016fusing}. Later on, deep sequential models (\eg RNN~[\citealp{GRU4Rec},~{\citealp{zhou2018deep}}], CNN~\cite{Caser}, Transformer~\cite{SASRec,Bert4Rec}, GNN~\cite{SRGNN,SURGE} and others~\cite{sabour2017dynamic}) are employed to capture the higher-order dependencies. In addition to general sequential methods, CauseRec~\cite{zhang2021cause} conditionally constructs the counterfactual interaction sequences, and then performs contrastive representation learning by using both counterfactual and observational data. 
DSSRec~\cite{ma2020disentangled} disentangles the intentions behind the user interaction sequence, and constructs seq2seq training samples by using only pairs of sub-sequences with the same intention, leading to better sequential modeling. 
Recently, ACVAE \cite{xie2021adversarial} has introduced adversarial learning to the variational Bayes framework for sequential recommendation and utilized contrastive learning to learn better user representations.
% , which ensure the personalized and salient characteristics of different users.
% Different from the previous studies, we aim to leverage the invariant structure among preference shifts over time thus obtain a more robust disentanglement across environments.

Different from sequential recommendation, CDR focuses on the generalizable recommendation to handle preference shifts, which aims to predict the user preference in the new OOD environment instead of next-item recommendation. Technically, CDR learns the invariant user preference in a short period while capturing temporal preference shifts between multiple environments. Furthermore, CDR formulates a sparse structure from the preference representation to interaction prediction for more robust disentanglement.  


% \vspace{-0.15cm}
\subsection{Preference Shifts in Recommendation}
% User preference may shift over time for some substantial reasons, such as personal shift, or transient and circumstantial ones. 
User preference may shift over time for many reasons, including the changes of user features (\eg income and pregnancy) and environment factors (\eg seasonal variation). 
% such as personal shift, or transient and circumstantial ones. 
% For example, in clothing domain, a user can change his/her preference due to seasonal changes or items may become popular in the specific holidays.
For example, in the scenario of food recommendation, a user might become liking expensive but healthy food if the user's income increases. 
% Since these temporal drifts occur over time, the models should continuously update to reflect the users' present taste. 
Since such preference shifts are frequent in the real-world scenarios, the recommender models should update the user representations adaptively over time. 
% Disregarding the drifting nature of preferences in modeling users' preferences can lead to incorrect recommendations. 
Ignoring the shifting nature of preference to learn user representation will lead to inappropriate recommendations. In addition to disentangled recommendation and sequential recommendation, there exists some work with the potential of addressing this issue. 
% To address the above issues, Aspect-MF \cite{zafari2019modelling} analyses the dynamicity of temporal preference aspects using a component-based approach, and shows which aspects are more subject to drift over time. 
In particular, Aspect-MF \cite{zafari2019modelling} analyses the dynamicity of temporal preference aspects using a component-based approach, and identifies the aspects that are easy to drift. 
ST-LDA \cite{yin2016adapting} learns region-dependent personal interests and crowd preference to adapt to preference shifts. 
Lastly, MTUPD \cite{wangwatcharakul2021novel} utilizes a forgetting curve function to calculate the correlations of user preference in different time periods. 


\vspace{5pt}
\noindent$\bullet\quad$\textbf{Long- and short-term interest modeling.}
% Furthermore, apart from the short-term changes, user preferences are also subject to long-term drifts. For instance, a user may love reading fairy tales in his/her childhood, while his/her preference may shift towards literature or science books.
% Therefore, different users show different dependencies on short- and long-term impact. 
Another possibly related direction considers the modeling of both long- and short-term interests, where short-term interests are inferred from recent interactions to capture the preference shifts, and long-term interests represent the stable preference over time.
% Many recent studies have been proposed to distinguish between different types of preference drifting and model them individually to achieve the highest accuracy.
Many studies have shed light on the modeling of long- and short-term user interests, for instance, PLSPL \cite{sritrakool2021personalized} utilizes attention mechanism to characterize the long-term preference while integrating the location and category information to capture the short-term preference. 
LSTPM \cite{sun2020go} develops a context-aware nonlocal network structure to explore the temporal and spatial correlations in users' trajectories for the long-term preference, and adopts a geo-dilated RNN to fully exploit the geographical relations for the short-term preference. 
Lastly, KERL \cite{wang2020kerl} fuses knowledge graph into a reinforcement learning framework to capture long-term preference and predict short-term interests. 
% These approaches take different approaches to capture long- and short-term representations of user preferences and then capture their shifts. 
% Different from previous work, we consider the invariant user preference during a short period and across environments. CDR aims to capture the invariance and temporal preference shifts across environments.
Although the short-term preference might infer the preference shifts, such work has not discovered the causal reasons for invariant and shifted preference. By contrast, we model the invariant and shifted preference simultaneously by leveraging their underlying causal relations and conduct the disentangled preference learning from multiple OOD environments. 



% 纯研究用户兴趣偏移的工作
% 长短期兴趣结合的工作，也算是preference shifts
% 描述跟他们的不同。

% 另起一段：% domain adaptation的工作，参考www22的一个subsection
\vspace{5pt}
\noindent$\bullet\quad$\textbf{Domain adaptation.}
Domain adaptation has been widely applied to solve the problem of distribution shifts, which improves the adaptation ability by using less data.
In recommendation, its main application scenarios include cross-domain recommendation \cite{zhu2021personalized, zhao2019cross, zhao2020catn} and cold-start recommendation \cite{yuan2020parameter}. 
Technically, model adaptation can be implemented by parameter patch \cite{sheng2021one, yuan2020parameter}, feature transformation \cite{lin2021task}, and meta learning \cite{yu2021personalized}.
% However, different from cross-domain recommendations, OOD recommendation in this work considers items in a single domain with user preference shits across environments. t'f's
% Besides, user preferences change partially, which differs from the cold-start problem with new users/items. 
% Therefore, how to improve the OOD generalization and fast adaptation abilities of recommender models considering user preference shifts over time is still an open problem.
These techniques have been well studied to estimate the preference of the users in a new domain or the cold-start users within the same domain. However, the preference shift in OOD recommendation is totally different because it is related to the same users with dynamic interests over time. This requires us to consider the connection between the interactions in different environments, \ie the cross-environment preference learning. 

% 把前三个subsection扩充一下，每一类工作里多描述几个最相关的例子。不同之处也展开多讲讲。
% related work大概拓展到2页多纸
