\subsection{CDR Framework}
\label{sec:method}


% \subsection{Invariant Structure Learning}
In this subsection, we present the CDR framework to model the interaction generation procedure under multiple environments. In particular, we utilize a novel temporal VAE to capture the preference shifts ($Z_{t-1}\rightarrow Z_{t}$) and conduct sparse structure learning to disentangle the sparse influence from user preference to interactions ($Z_{t}\rightarrow X_t$). 

We construct the recommender model by following the causal relations in Figure \ref{fig:causal_graph}. Specifically, for each user $u$ in the environment $t$, we first sample a $K$-dimensional latent representation $\bm{e}_t$ from the standard Gaussian prior $\mathcal{N}\left(0, \mathbf{I}_{K}\right)$~\cite{liang2018variational, yang2021causalvae}, where the covariance $\mathbf{I}_{K}$ is an identity matrix. We then obtain user preference $\bm{z}_t \in \mathbb{R}^{H}$ based on $\bm{e}_t$ and the previous $\bm{z}_{t-1}$. Thereafter, $\bm{z}_t$ is used to predict the interaction probability over $I$ items and the historical interactions $\bm{x}_t\in \mathbb{R}^{I}$ are assumed to be drawn from the interaction probability distribution. In this work, we assume that $\bm{z}_t$ and $\bm{x}_t$ follow the factorized Gaussian and multinomial priors due to their superiority shown in previous work~\cite{liang2018variational, ma2019learning}.
Formally,
\begin{equation}
\label{eqn:all_prior}
\left\{
\begin{aligned}
& \bm{e}_t \sim \mathcal{N}\left(0, \mathbf{I}_{K}\right), \\
& \bm{z}_t \sim \mathcal{N}\left(\bm{\mu}_{\theta_1}(\bm{e}_t, \bm{z}_{t-1}), \text{diag}\{\bm{\sigma}^2_{\theta_1}(\bm{e}_t, \bm{z}_{t-1})\}\right), \\
& \bm{x}_t \sim \text{Mult}\left(N_t, \pi\left(f_{\theta_2}(\bm{z}_t)\right)\right). \\
\end{aligned}
\right.
\end{equation}
Specifically, we explain the generative process in Eq. (\ref{eqn:all_prior}), which is consistent with the causal relations in Figure \ref{fig:causal_graph}: 
\begin{itemize}[leftmargin=*]
    \item $(\bm{e}_t, \bm{z}_{t-1}) \rightarrow \bm{z}_t$: $\bm{\mu}_{\theta_1}(\bm{e}_t, \bm{z}_{t-1})$ and $\bm{\sigma}^2_{\theta_1}(\bm{e}_t, \bm{z}_{t-1})$ denote the \textit{mean} and \textit{diagonal covariance} of the Gaussian distribution of $\bm{z}_t$, which are estimated from $\bm{e}_t$ and $\bm{z}_{t-1}$ via a network $f_{\theta_1}(\cdot)$.
    \item $\bm{z}_t \rightarrow \bm{x}_t$: $\bm{x}_t$ is sampled from a multinomial distribution affected by $\bm{z}_t$, where $N_t=\sum_{i=1}^{I}x_{t,i}$ represents the interaction number of user $u$ in the environment $t$, $\pi(\cdot)$ is the \textit{softmax} function, and the network $f_{\theta_2}(\bm{z}_t)$ transforms $\bm{z}_t$ to produce the interaction probability over $I$ items. 
\end{itemize}

To train the recommender model, we aim to optimize the parameters $\{\theta_1, \theta_2\}$ by maximizing the generative probability of observed user interactions $\bm{x}_{1:T}$ in $T$ environments. {Formally, following~\cite{chung2015recurrent}, we can factorize the joint distribution $p(\bm{x}_{1:T})$ and maximize the log-likelihood as follows:} 
\begin{equation}
\label{eqn:log_likelihood}
% \small
\begin{aligned}
{\log p(\bm{x}_{1:T})} & = {\log \int p(\bm{x}_{1:T}|\bm{e}_{1:T})p(\bm{e}_{1:T})d\bm{e}_{1:T}} \\
                    &= {\log \int \prod_{t=1}^T p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})p(\bm{e}_{1:T})d\bm{e_{1:T}}},
\end{aligned}
\end{equation}
{where $p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})$ aligns with the generation procedure in Eq. (\ref{eqn:all_prior}) and we will further factorize it with $\bm{z}_{1:t}$ in the decoding process \ie Eq. (\ref{eqn:decoder}).
Nevertheless, maximizing Eq. (\ref{eqn:log_likelihood}) is intractable because it involves the integral over unobserved $\bm{e}_{1:T}$. 
To solve the problem, we embrace \textit{variational inference}~\cite{liang2018variational} to approximate $\log p(\bm{x}_{1:T})$ by using a variational distribution $q(\bm{e}_{1:T}|\cdot)$. Formally,}
\begin{subequations}
\label{eqn:ELBO}
% \small
\begin{align}
{\log p(\bm{x}_{1:T})} &= {\log \int \prod_{t=1}^T         p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})p(\bm{e}_{1:T})\frac{q(\bm{e}_{1:T}|\cdot)}{q(\bm{e}_{1:T}|\cdot)}d\bm{e_{1:T}}}\\
                    &\geq {\mathbb{E}_{q(\bm{e}_{1:T}|\cdot)} \left[\log \frac{\prod_{t=1}^Tp(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})p(\bm{e}_{1:T})}{q(\bm{e}_{1:T}|\cdot)}\right] \quad (\text{ELBO})}\\ 
                    &= {\mathbb{E}_{q(\bm{e}_{1:T}|\cdot)}\left[\sum_{t=1}^T\left(\log p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})-\text{KL}\left[q(\bm{e}_t|\cdot)\|p(\bm{e}_{t})\right]\right)\right]},
\end{align}
\end{subequations}
where variational inference introduces the Evidence Lower BOund (ELBO) of Eq. (\ref{eqn:ELBO}a) by using $q(\bm{e}_{1:T}|\cdot)=\prod_{t=1}^Tq(\bm{e}_t|\cdot)$. Meanwhile, the first term in Eq. (\ref{eqn:ELBO}c) represents the probability of collecting observed $\bm{x}_t$ conditioned on $\bm{x}_{1:t-1}$ and $\bm{e}_{1:t}$ while the second term denotes the Kullback-Leibler (KL) divergence between the variational distribution $q(\bm{e}_t|\cdot)$ and the prior of $\bm{e}_t$. By maximizing the ELBO in Eq. (\ref{eqn:ELBO}c), we are able to increase the log-likelihood $\log p(\bm{x}_{1:T})$. 
Note that we avoid factorizing {$p(\bm{x}_{t}|\bm{x}_{1:t-1},\bm{e}_{1:t})$} with $\bm{z}_{1:t}$ in Eq. (\ref{eqn:ELBO}), and then we do not estimate the distribution of unobserved $\bm{z}_t$ and $\bm{e}_t$ simultaneously by variational inference. This is because we choose the alternative Monte Carlo (MC) sampling to efficiently approximate the posterior distribution of $\bm{z}_t$~\cite{chen2012monte}. {MC sampling constructs a random sampling of $\bm{z}_t$ (\ie draw samples from $p(\bm{z}_t|\bm{e}_t,\bm{z}_{t-1})$) to estimate its distribution, which avoids unnecessary prior hypothesis over the mean and covariance of $\bm{z}_t$ (see Eq. (\ref{eqn:decoder})).}

So far, the key of calculating the ELBO in Eq. (\ref{eqn:ELBO}c) lies in estimating $q(\bm{e}_t|\cdot)$ and {$p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})$}, which can be obtained by the encoder and decoder networks, respectively. We present the intuitive illustration of CDR with the encoder and decoder networks in Figure \ref{fig:CDR}.




\begin{figure}[t]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{0cm}
\centering
\includegraphics[scale=0.6]{figures/CDR.pdf}
\caption{Illustration of the CDR framework, where the encoder network predicts the hidden user features $\bm{e}_t$ in the environment $t$, and then the decoder reconstructs the interaction generation procedure from $\bm{e}_t$ and $\bm{z}_{t-1}$ to the interaction probability $f_{\theta_2}(\bm{z}_t)$. The entire encoder-decoder process repeats T times while $\bm{W}_z$ and $\bm{W}_x$ are shared across $T$ environments.} % , consisting of the encoder and decoder networks
\label{fig:CDR}
\end{figure}

\subsubsection{\textbf{Encoder Network}}

To estimate $q(\bm{e}_t|\cdot)$, we incorporate an encoder network $g_{\phi}(\cdot)$, which predicts $\bm{e}_t$ by the user interaction $\bm{x}_t$. The underlying motivation is that unobserved factors (\eg income) can be inferred from users' behaviors (\eg purchasing expensive products). In particular, 
\begin{equation}
\label{eqn:encoder}
\begin{aligned}
q(\bm{e}_t|\cdot) = q(\bm{e}_t|\bm{x}_t) = \mathcal{N}\left(\bm{e}_t; \bm{\mu}_{\phi}(\bm{x}_t), \text{diag}\{\bm{\sigma}^2_{\phi}(\bm{x}_t)\}\right),
\end{aligned}
\end{equation}
where $\bm{\mu}_{\phi}(\bm{x}_t)$ and $\bm{\sigma}^2_{\phi}(\bm{x}_t)$ denote the mean and diagonal covariance of $\bm{e}_t$, respectively. They are estimated by the encoder network $g_\phi(\cdot)$ parameterized by $\phi$. Formally, we have $g_{\phi}(\bm{x}_t) = [\bm{\mu}_{\phi}(\bm{x}_t), \bm{\sigma}_{\phi}(\bm{x}_t)]\in \mathbb{R}^{2K}$. In this work, we instantiate $g_\phi(\cdot)$ by a Multi-Layer Perceptron (MLP), which outputs the Gaussian parameters of $\bm{e}_t$. Note that the encoder ignores the temporal interaction sequence in $\bm{x}_t$ and fairly encodes every interaction. 



\subsubsection{\textbf{Decoder Network}}
We factorize {$p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})$} by following the causal relations in the interaction generation process:
\begin{equation}
\label{eqn:decoder}
% \small
\begin{aligned}
% p(\bm{x}_t|\bm{e}_t) &= \int p(\bm{x}_t|\bm{z}_t)p(\bm{z}_t|\bm{e}_t, \bm{z}_{t-1})p(\bm{z}_{t-1}|\cdot)d\bm{z}_t d\bm{z}_{t-1},
{p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})} &= {\int p(\bm{x}_t|\bm{z}_t)\prod_{a=1}^tp(\bm{z}_a|\bm{z}_{a-1},\bm{e}_a)d\bm{z}_{1:t}},
\end{aligned}
\end{equation}
{where $p(\bm{z}_a|\bm{z}_{a-1},\bm{e}_a)$ denotes the probability distribution of the user preference $\bm{z}_a$ in the environment $a$; and when $a=1$, $\bm{z}_{0}$ is set as the constant vector $\bm{0}$. Besides, to approximate the distribution of $\bm{z}_{t}$, we use MC sampling~\cite{chen2012monte} to draw samples from $p(\bm{z}_t|\bm{z}_{t-1}, \bm{e}_t)$. Then we can calculate $p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})$ based on $p(\bm{x}_t|\bm{z}_t)$ while marginalizing over $\bm{z}_{1:t}$ via the samples from MC sampling. 
To iteratively calculate $p(\bm{z}_t|\bm{z}_{t-1}, \bm{e}_t)$, we adopt an MLP model $f_{\theta_1}(\cdot)$ to output the $\bm{\mu}_{\theta_1}(\cdot)$ and $\bm{\sigma}_{\theta_1}(\cdot)$ of $\bm{z}_t$. Formally, we have $f_{\theta_1}(\bm{z}_{t-1}, \bm{e}_t) = \left[\bm{\mu}_{\theta_1}(\bm{z}_{t-1}, \bm{e}_t), \bm{\sigma}_{\theta_1}(\bm{z}_{t-1}, \bm{e}_t)\right] \in \mathbb{R}^{2H}$. Thereafter, the remaining challenge is estimating $p(\bm{x}_t|\bm{z}_t)$ in Eq. (\ref{eqn:decoder}).} 






\vspace{10pt}
% \noindent$\bullet\quad$\textbf{Estimation of $p(\bm{x}_t|\bm{z}_t)$.}
\noindent$\bullet\quad$\textbf{Sparse structure learning.} 
To estimate $p(\bm{x}_t|\bm{z}_t)$, we incorporate $f_{\theta_2}(\cdot)$ to transform $\bm{z}_t$ into the interaction probability over $I$ items. However, to align with the causal relations in Figure \ref{fig:causal_graph} and learn the sparse influence from user preference to interactions, we do not simply use an MLP model for the implementation of $f_{\theta_2}(\cdot)$. We instead resort to \textit{sparse structure learning} in multiple environments, which aims to discover a sparse structure from user preference representations to interactions and requires the structure is robust across all the environments with distribution shifts. Consequently, 1) the sparse structure learned from multiple environments instead of one environment will encode the robust relations between user representations and interactions, which are likely to be reliable in future environments with preference shifts; and 2) if partial user preference has shifted, only a subset of interactions are affected due to the sparse structure. Such characteristics will improve the generalization ability of CDR under preference shifts. 

% Following~\cite{ma2019learning}, \edit{we assume that users have the category-level preference over items, and thus we disentangle the representations of $\bm{z}_t\in \mathbb{R}^{H}$ into category-level preference.}
Following~\cite{ma2019learning}, {the user preference representation $\bm{z}_t\in \mathbb{R}^{H}$ can cover the preference over multiple item categories, and we disentangle  $\bm{z}_t$ into several category-specific preference representations.}
Specifically, to implement the sparse structure, we introduce a matrix $\bm{W}_z \in \mathbb{R}^{H\times C}$ to factorize the user representation into the preference over $C$ item categories. In particular, ${W}_z[h,c]\in \bm{W}_z$ denotes the probability of the $h$-th factor in $\bm{z}_t$ belonging to the preference over the $c$-th category. 
Correspondingly, we leverage a matrix $\bm{W}_x \in \mathbb{R}^{I\times C}$ to classify items into $C$ categories. Inspired by~\cite{yamada2020feature, liu2021heterogeneous}, we draw $\bm{W}_z$ and $\bm{W}_x$ from the clipped Gaussian distributions parameterized by $\bm{\alpha}\in \mathbb{R}^{H\times C}$ and $\bm{\beta}\in \mathbb{R}^{I\times C}$, respectively. Formally, for each ${W}_z[h,c]\in \bm{W}_z$ and ${W}_x[i,c]\in \bm{W}_x$, we have 
\begin{equation}
\label{eqn:W_z_W_x}
% \small
\left\{
\begin{aligned}
{W}_z[h,c] &= \min\left(\max\left(\alpha_{h,c} + \epsilon, 0\right), 1\right), \\
{W}_x[i,c] &= \min\left(\max\left(\beta_{i,c} + \epsilon, 0\right), 1\right), \\
\end{aligned}
\right .
\end{equation}
where the noise $\epsilon$ is drawn from $\mathcal{N}\left(0, \sigma_{\epsilon}^2\right)$. We clip the values of $\bm{W}_z$ and $\bm{W}_x$ into $[0,1]$ to ensure a valid range for the probabilities. Besides, to encourage each factor or item belonging to one category, we add a softmax function at the dimension of $C$ categories in $\bm{W}_z$ and $\bm{W}_x$. 
As illustrated in Figure \ref{fig:theta_2}, we then implement $f_{\theta_2}(\cdot)$ to estimate the parameters of $\bm{x}_t$ in Eq. (\ref{eqn:all_prior}) by
\begin{equation}
\label{eqn:f_theta_2}
% \small
\begin{aligned}
f_{\theta_2}(\bm{z}_t) = \sum_{c=1}^C {W}_x[:,c] \odot f_{\gamma}({W}_z[:,c] \odot \bm{z}_t),
\end{aligned}
\end{equation}
where $\theta_2=\{\bm{\alpha}, \bm{\beta}, \gamma\}$, $\odot$ denotes the element-wise multiplication, and $f_{\gamma}(\cdot)$ can be any function transforming $\bm{z}_t$ to the interaction probability distribution over $I$ items. Following~\cite{liang2018variational}, we implement $f_{\gamma}(\cdot)$ by an MLP model.

\begin{figure}[t]
\setlength{\abovecaptionskip}{0.2cm}
% \setlength{\belowcaptionskip}{0cm}
\centering
\includegraphics[scale=0.8]{figures/theta_2.pdf}
\caption{Illustration of the calculation of $f_{\theta_2}(\bm{z}_t)$ in Eq. (\ref{eqn:f_theta_2}). Similar to masking mechanisms, $\bm{W}_z$ and $\bm{W}_x$ disentangle the user preference representations, leading to sparse connection from user preference to interactions. Note that we simplify $\bm{W}_z$ and $\bm{W}_x$ as discrete matrices with $\{0,1\}$ for better understanding.}
\label{fig:theta_2}
% \vspace{-0.2cm}
\end{figure}


\vspace{10pt}
\noindent$\bullet\quad$\textbf{Likelihood estimation.} 
As shown in Figure~\ref{fig:CDR}, given the user interactions $\bm{x}_{1:T}$ of user $u$, we feed them into the encoder network to sample $\bm{e}_{1:T}$, and then iteratively pass $\bm{e}_{1:T}$ to the decoder network to obtain the parameters of the multinomial distribution for $\bm{x}_{1:T}$ (\ie $f_{\theta_2}(\bm{z}_{1:T})$). Thereafter, {the log-likelihood $\log p(\bm{x}_t|\bm{z}_{t})$ can be calculated by} 
\begin{equation}
\label{eqn:likelihood}
\begin{aligned}
{\log p(\bm{x}_t|\bm{z}_{t})} &\overset{c}{=} \sum_{i=1}^{I}x_{t,i}\log \pi_i\left(f_{\theta_2}(\bm{z}_{t})\right),
\end{aligned}
\end{equation}
% where $x_{t,i} \in \{0,1\}$ indicates whether user $u$ has interacted with item $i$ in the environment $t$ ($x_{t,i}=1$) or not ($x_{t,i}=0$). 
where $x_{t,i} \in \{0,1\}$ indicates whether user $u$ has interacted with item $i$ in the environment $t$ or not. 
Besides, the softmax function $\pi(\cdot)$ is used to normalize $f_{\theta_2}(\cdot)$ and $\pi_i(f_{\theta_2}(\cdot))$ denotes the normalized prediction score for item $i$. {Intuitively, the log-likelihood $p(\bm{x}_t|\bm{z}_{t})$ estimates the probability of drawing observed $\bm{x}_t$ from the multinomial distribution by sampling $N_t$ times, where $N_t$ is the interaction number of user $u$ in the environment $t$.}
% Furthermore, here $p(\bm{x}_t|\bm{z}_{t})$ is an approximate estimation of $p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})$ by using MC sampling to marginalize over $\bm{z}_{1:t}$.
% , \ie $N_t = \sum_{i=1}^I x_{t,i}$


% \begin{algorithm}
% \DontPrintSemicolon \KwIn{Click matrix $\mbX \in \mathbb{R}^{U \times I}$}
% Randomly initialize $\theta$, $\phi$\;
% \While{not converged}{
%   Sample a batch of users $\mathcal{U}$\;
%   \ForAll{$u\in\mathcal{U}$}{
%     Sample $\mb\epsilon\sim \cN(0, \mbI_K)$ and compute $\mbz_u$ via reparametrization trick\;
%     Compute noisy gradient $\nabla_\theta \cL$ and $\nabla_\phi \cL$ with $\mbz_u$\;
%   }
%   Average noisy gradients from batch\;
%   Update $\theta$ and $\phi$ by taking stochastic gradient steps\;
% }
% \Return{$\theta$, $\phi$}\;
% \caption{{\sc VAE-SGD} Training collaborative filtering \gls{VAE} with stochastic gradient descent.}
% \label{alg:vae}
% \end{algorithm}

\begin{algorithm}[t]
	\caption{Training of CDR under Multiple Environments}  
	\label{algo:training}
	\begin{algorithmic}[1]
		\Require $X_{1:T}$ of all $U$ users; $g_\phi(\cdot)$, $f_{\theta_1}(\cdot)$, and $f_{\theta_2}(\cdot)$ with initialized $\phi$, $\theta_1$, and $\theta_2$, respectively.
		\While{\textit{not converged}}
		    \State Sample a batch of users $\mathcal{U}$
		    \ForAll{$u \in \mathcal{U}$}
		        \ForAll{$t \in \{1,2,...,T\}$}
		            \State Sample $\bm{e}_t$ by feeding $\bm{x}_t$ into $g_\phi(\bm{x}_t)$;
    		        \State Sample $\bm{z}_t$ by feeding $\bm{z}_{t-1}$ and $\bm{e}_t$ into $f_{\theta_1}(\bm{z}_{t-1}, \bm{e}_t)$;
    		        \State Calculate $f_{\theta_2}(\bm{z}_t)$ via Eq. (\ref{eqn:f_theta_2});
    		        \State Obtain the probability of drawing $\bm{x}_t$ by Eq. (\ref{eqn:likelihood});
    		        \State Calculate the gradients \wrt the loss in Eq. (\ref{eqn:overall_loss});
                \EndFor
		    \EndFor
		    \State Average the gradients over $|\mathcal{U}|$ users and $T$ environments;
		    \State Update $\phi$, $\theta_1$, and $\theta_2$ via Adam;
		\EndWhile
		\Ensure $g_\phi(\cdot)$, $f_{\theta_1}(\cdot)$, and $f_{\theta_2}(\cdot)$.
	\end{algorithmic}
\end{algorithm}
\vspace{-0.1cm}
% \setlength{\textfloatsep}{0.2cm}


\subsubsection{\textbf{CDR Optimization}}
We maximize the ELBO to increase the log-likelihood in Eq. (\ref{eqn:log_likelihood}) by optimizing the parameters (\ie $\phi$ and $\theta=\{\theta_1, \theta_2\} $) in CDR. The parameters are updated by stochastic gradient descent. However, we conduct the sampling of $\bm{e}_t$ in Eq. (\ref{eqn:encoder}) and $\bm{z}_t$ in Eq. (\ref{eqn:decoder}), which prevents the back-propagation of gradients. To solve this problem, we utilize the \textit{reparameterization trick}~\cite{Kingma2014Auto, liang2018variational}. Besides, we leverage the \textit{KL annealing trick}~\cite{liang2018variational} to control the effect of the KL divergence, which introduces an additional hyper-parameter $\lambda_1$ into Eq. (\ref{eqn:ELBO}c). To summarize, the optimization objective for user $u$ is to minimize the following loss:
\begin{equation}
\label{eqn:rec_loss}
\notag
\small
\begin{aligned}
{\mathcal{L}^u} = {-\mathbb{E}_{q_{\phi}(\bm{e}_{1:T}|\cdot)} \left[\sum_{t=1}^T\left(\log p(\bm{x}_t|\bm{x}_{1:t-1},\bm{e}_{1:t})-\lambda_1\text{KL}\left[q(\bm{e}_t|\bm{x}_t)\|p(\bm{e}_t)\right]\right)\right]},
\end{aligned}
\end{equation}
{which becomes negative timestamp-wise ELBO~\cite{chung2015recurrent} over the $T$ environments.} 

\vspace{10pt}
\noindent$\bullet\quad$\textbf{Sparsity and variance regularization.}
In addition to the ELBO objective, we additionally consider two regularization terms: 1) the sparsity of $\bm{W}_z$ and $\bm{W}_x$, and 2) the variance of the gradients across $T$ environments. 
In Eq. (\ref{eqn:f_theta_2}), we expect that the structure implemented by $\bm{W}_z$ and $\bm{W}_x$ is sparse because the sparse connection between user representations and interactions is more robust under preference shifts. Therefore, we introduce an $L_0$ regularization term $\|\bm{W}_z\|_0 + \|\bm{W}_x\|_0$ to restrict the number of non-zero values. 

As to the variance regularization, it can facilitate the sparse structure learning across multiple environments. Training over multiple environments easily leads to imbalanced optimization: the performance in some environments is good while in other environments has inferior results. Consequently, disentangled preference representations via $\bm{W}_z$ and $\bm{W}_x$ might not be reliable across multiple environments. 
% probably causing unreasonable disentanglement. 
As such, we incorporate the variance penalty regularizer used in invariant learning~\cite{koyama2021invariance, liu2021heterogeneous}, which regulates the variance of the gradients under $T$ environments. Specifically, we calculate the variance regularization for user $u$ by $\sum_{t=1}^T\left\| \nabla_\theta \mathcal{L}^u_t - \nabla_\theta \mathcal{L}^u\right\|^2$,
% \begin{equation}
% \label{eqn:variance}
% \begin{aligned}
% % \text{trace}\left(\text{Var}_{1:T}(\nabla_\theta \mathcal{L}^u)\right) 
% \sum_{t=1}^T\left\| \nabla_\theta \mathcal{L}^u_t - \nabla_\theta \mathcal{L}^u\right\|^2,
% \end{aligned}
% \end{equation}
where $\mathcal{L}^u_t$ is the optimization loss for the environment $t$ in Eq. (\ref{eqn:rec_loss}), $\nabla_\theta$ denotes the gradients \wrt the learnable parameters $\theta$, and $\nabla_\theta \mathcal{L}^u$ represents the average gradients over $T$ environments.

Intuitively, the variance regularizer will restrict the gradient difference among $T$ environments, and thus update the parameters $\theta$ synchronously for multiple environments. This will alleviate the problem that the parameters are unfairly optimized to improve the performance of few environments~\cite{liu2021heterogeneous}. To sum up, we have the final optimization loss for user $u$ as follows:
\begin{equation}
\label{eqn:overall_loss}
% \notag
\begin{aligned}
\mathcal{L}^u + \lambda_2 \cdot (\|\bm{W}_z\|_0 + \|\bm{W}_x\|_0) + \lambda_3 \cdot \sum_{t=1}^T\left\| \nabla_\theta \mathcal{L}^u_t - \nabla_\theta \mathcal{L}^u\right\|^2,
\end{aligned}
\end{equation}
where two hyper-parameters $\lambda_2$ and $\lambda_3$ control the strength of sparsity and variance regularization terms, respectively. 

% \vspace{10pt}
% \noindent$\bullet\quad$\textbf{Summary.}




% \vspace{5pt}
% \noindent$\bullet\quad$\textbf{ Penalty}


% \vspace{5pt}
% \noindent$\bullet\quad$\textbf{Environment division.}
\subsubsection{\textbf{Environment division}}\label{sec:environment}
For the temporal interaction sequence of user $u$, we split it into $T$ pieces according to the equal interaction number in every environment. 
The choice of $T$ is essential because it balances the learning of shifted and invariant preference. CDR only considers the cross-environment preference shifts and assumes the intra-environment preference is invariant by ignoring the temporal information of interactions. 
Therefore, a larger $T$ will expose more sequential information to CDR. 
% because CDR only considers the temporal changes between environments and ignores the chronological order of interactions to capture the invariant preference 
% within an environment (\cf Eq. (\ref{eqn:encoder}) and (\ref{eqn:decoder})). 
Nevertheless, the large $T$ value will increase the sparsity of interactions in each environment, hurting the learning of the encoder and decoder networks. To alleviate the dilemma, we choose a relatively small $T$ during training to ensure the interaction density of each environment. Once the parameters are well learned, we adopt a larger $T$ to fully utilize the sequential information in the inference stage. 


\subsubsection{\textbf{Summary}}\label{sec:summary}
The detailed training procedure can be found in Algorithm~\ref{algo:training}.
To train the encoder and decoder networks in CDR, we divide the training interactions into multiple environments and utilize them to minimize the loss function in Eq. (\ref{eqn:overall_loss}) over all users. 
During the inference stage, we use the latest user preference $\bm{z}_T$ to calculate $f_{\theta_2}(\bm{z}_T)$ for the ranking of item candidates, and then recommend top-ranked items to each user.

To summarize, the encoder network infers unobserved $E_t$ from users' interactions. Thereafter, the decoder network leverages the inferred $E_t$ to iteratively update $Z_t$ for better preference estimation. Besides, the decoder network conducts sparse structure learning to model the sparse influence from $Z_t$ to $X_t$ for better effect estimation of user preference. As compared to traditional VAE-based methods~\cite{liang2018variational, wang2021personalized, xia2021Collaborative}, CDR is more robust in OOD environments because it constructs the encoder and decoder networks by following the causal relations in Figure \ref{fig:causal_graph}. 
Besides, thanks to modeling causal relations, CDR supports the intervention over the causal graph. {As illustrated in Section~\ref{sec:case_do_E}, we can estimate the counterfactual user preference $Z_t$ and the corresponding recommendations by intervening on $E_t$, \ie changing $E_t={\bm{e}}_t$ to $do(E_t=\hat{\bm{e}}_t)$~\cite{pearl2009causality}.} 

% \vspace{6pt}
% \begin{center}
% \fcolorbox{black}{gray!8}{\parbox{0.98\linewidth}{\noindent$\bullet$ \textbf{Summary.}
% We minimize the loss function in Eq. (\ref{eqn:overall_loss}) over all users for CDR training. The detailed training procedure can be found in Algorithm~\ref{algo:training}.
% During the inference stage, we use the latest user preference $\bm{z}_T$ to calculate $f_{\theta_2}(\bm{z}_T)$ for the ranking of item candidates, and then recommend top-ranked items to each user.
% }}
% \end{center}
% \vspace{6pt}