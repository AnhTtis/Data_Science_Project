%%Lambrecht2013
@article{Lambrecht2013,
author = {Jens Lambrecht and Martin Kleinsorge and Martin Rosenstrauch and Jörg Krüger},
title ={Spatial Programming for Industrial Robots through Task Demonstration},
journal = {International Journal of Advanced Robotic Systems},
volume = {10},
number = {5},
pages = {254},
year = {2013},
doi = {10.5772/55640},

URL = { 
        https://doi.org/10.5772/55640
    
},
eprint = { 
        https://doi.org/10.5772/55640
    
}
,
    abstract = { Abstract We present an intuitive system for the programming of industrial robots using markerless gesture recognition and mobile augmented reality in terms of programming by demonstration. The approach covers gesture-based task definition and adaption by human demonstration, as well as task evaluation through augmented reality. A 3D motion tracking system and a handheld device establish the basis for the presented spatial programming system. In this publication, we present a prototype toward the programming of an assembly sequence consisting of several pick-and-place tasks. A scene reconstruction provides pose estimation of known objects with the help of the 2D camera of the handheld. Therefore, the programmer is able to define the program through natural bare-hand manipulation of these objects with the help of direct visual feedback in the augmented reality application. The program can be adapted by gestures and transmitted subsequently to an arbitrary industrial robot controller using a unified interface. Finally, we discuss an application of the presented spatial programming approach toward robot-based welding tasks. }
}

%%Leutert2013
@INPROCEEDINGS{Leutert2013,  
author={Leutert, Florian and Herrmann, Christian and Schilling, Klaus}, 
booktitle={2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},   
title={A Spatial Augmented Reality system for intuitive display of robotic data},  
year={2013},  
volume={}, 
number={}, 
pages={179-180},  
doi={10.1109/HRI.2013.6483560}
}

%%Gaschler2014
@INPROCEEDINGS{Gaschler2014,  
author={Gaschler, Andre and Springer, Maximilian and Rickert, Markus and Knoll, Alois},  
booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)},  
title={Intuitive robot tasks with augmented reality and virtual obstacles},   
year={2014},  
volume={},  
number={},  
pages={6026-6031},  
doi={10.1109/ICRA.2014.6907747}
}

%%Chakraborti2018
@INPROCEEDINGS{Chakraborti2018,  
author={Chakraborti, Tathagata and Sreedharan, Sarath and Kulkarni, Anagha and Kambhampati, Subbarao}, 
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},   
title={Projection-Aware Task Planning and Execution for Human-in-the-Loop Operation of Robots in a Mixed-Reality Workspace},   year={2018},  
volume={}, 
number={}, 
pages={4476-4482},  
doi={10.1109/IROS.2018.8593830}
}

%%Liu2018
@INPROCEEDINGS{Liu2018,  
author={Liu, Hangxin and Zhang, Yaofang and Si, Wenwen and Xie, Xu and Zhu, Yixin and Zhu, Song-Chun}, 
booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},  
title={Interactive Robot Knowledge Patching Using Augmented Reality},   
year={2018}, 
volume={}, 
number={},  
pages={1947-1954},  
doi={10.1109/ICRA.2018.8462837}
}

%%Krupke2018
@INPROCEEDINGS{Krupke2018, 
author={Krupke, Dennis and Steinicke, Frank and Lubos, Paul and Jonetzko, Yannick and Görner, Michael and Zhang, Jianwei}, 
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
title={Comparison of Multimodal Heading and Pointing Gestures for Co-Located Mixed Reality Human-Robot Interaction}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-9}, 
doi={10.1109/IROS.2018.8594043}
}

%%Quintero2018
@INPROCEEDINGS{Quintero2018,
  author={Quintero, Camilo Perez and Li, Sarah and Pan, Matthew KXJ and Chan, Wesley P. and Machiel Van der Loos, H.F. and Croft, Elizabeth},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Robot Programming Through Augmented Trajectories in Augmented Reality}, 
  year={2018},
  volume={},
  number={},
  pages={1838-1844},
  doi={10.1109/IROS.2018.8593700}
}
  
%%Gadre2019
@INPROCEEDINGS{Gadre2019,
  author={Gadre, Samir Yitzhak and Rosen, Eric and Chien, Gary and Phillips, Elizabeth and Tellex, Stefanie and Konidaris, George},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 
  title={End-User Robot Programming Using Mixed Reality}, 
  year={2019},
  volume={},
  number={},
  pages={2707-2713},
  doi={10.1109/ICRA.2019.8793988}
}
  
%%Luebbers2019
@inproceedings{Luebbers2019,
  title={Augmented reality interface for constrained learning from demonstration},
  author={Luebbers, Matthew B and Brooks, Connor and Kim, Minjae John and Szafir, Daniel and Hayes, Bradley},
  booktitle={Proceedings of the 2nd International Workshop on Virtual, Augmented and Mixed Reality for HRI (VAM-HRI)},
  year={2019}
}

%%Ostantin2019
@article{Ostantin2019,
title = {Interactive Robots Control Using Mixed Reality ⁎⁎The work presented in this paper was supported by the grant of Russian Science Foundation 17-19-01740.},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {695-700},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.307},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319312613},
author = {M. Ostanin and R. Yagfarov and A. Klimchik},
keywords = {Human-robot interaction, Mixed reality, Augmented reality, Industrial robots, Mobile robots, HoloLens},
abstract = {The paper presents a mixed reality-based approach for interactive control of robotic manipulators and mobile platforms. In particular, we designed an interactive and understandable interface for human-robot interaction. The interface provides tools for robot path programming and visualizes it. Path visualization helps workers understand robot behavior, it is important for safety human-robot interaction. The paper presents an architecture of that system and the implementation for an industrial robot KUKA iiwa and mobile robot platform Plato. The main issue of a multi-platform system is related to the synchronization of coordinate frames for all elements. To deal with this problem we realized 3 setting options: manually, by a camera with markers, point clouds processing. We implemented our interface on Microsoft HoloLens and evaluated it on users.}
}

%%Puljiz2019
@INPROCEEDINGS{Puljiz2019,
  author={Puljiz, David and Stöhr, Erik and Riesterer, Katharina S. and Hein, Björn and Kröger, Torsten},
  booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={General Hand Guidance Framework using Microsoft HoloLens}, 
  year={2019},
  volume={},
  number={},
  pages={5185-5190},
  doi={10.1109/IROS40897.2019.8967649}
}
  
%%Chan2020
@INPROCEEDINGS{Chan2020,
  author={Chan, Wesley P. and Hanks, Geoffrey and Sakr, Maram and Zuo, Tiger and Machiel Van der Loos, H.F. and Croft, Elizabeth},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={An Augmented Reality Human-Robot Physical Collaboration Interface Design for Shared, Large-Scale, Labour-Intensive Manufacturing Tasks}, 
  year={2020},
  volume={},
  number={},
  pages={11308-11313},
  doi={10.1109/IROS45743.2020.9341119}
}
  
%%Quesada2020
@INPROCEEDINGS{Quesada2020,
  author={Chacon-Quesada, Rodrigo and Demiris, Yiannis},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Augmented Reality User Interfaces for Heterogeneous Multirobot Control}, 
  year={2020},
  volume={},
  number={},
  pages={11439-11444},
  doi={10.1109/IROS45743.2020.9341422}
}

%%Puljiz2020
@INPROCEEDINGS{Puljiz2020,
  author={Puljiz, David and Krebs, Franziska and Bosing, Fabian and Hein, Bjorn},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={What the HoloLens Maps Is Your Workspace: Fast Mapping and Set-up of Robot Cells via Head Mounted Displays and Augmented Reality}, 
  year={2020},
  volume={},
  number={},
  pages={11445-11451},
  doi={10.1109/IROS45743.2020.9340879}
}

%%Fennel2021
@ARTICLE{Fennel2021,
  author={Fennel, Michael and Zea, Antonio and Mangler, Johannes and Roennau, Arne and Hanebeck, Uwe D.},
  journal={IEEE Control Systems Letters}, 
  title={Haptic Rendering of Arbitrary Serial Manipulators for Robot Programming}, 
  year={2022},
  volume={6},
  number={},
  pages={716-721},
  doi={10.1109/LCSYS.2021.3086059}
}

%%Hoang2021
@article{Hoang2021,
  title={Virtual Barriers in Augmented Reality for Safe and Effective Human-Robot Cooperation in Manufacturing},
  author={Cong Hoang, Khoa and Chan, Wesley P and Lay, Steven and Cosgun, Akansel and Croft, Elizabeth},
  journal={arXiv e-prints},
  pages={arXiv--2104},
  year={2021}
}

%5Hoang2021b
@article{5Hoang2021b,
  title={ARviz--An Augmented Reality-enabled Visualization Platform for ROS Applications},
  author={Hoang, Khoa C and Chan, Wesley P and Lay, Steven and Cosgun, Akansel and Croft, Elizabeth A},
  journal={arXiv preprint arXiv:2110.15521},
  year={2021}
}

%%Liu2021
@article{Liu2021,
author = {Liu, Hangxin and Zhu, Yixin and Zhu, Song-Chun},
title = {Patching interpretable And-Or-Graph knowledge representation using augmented reality},
journal = {Applied AI Letters},
volume = {2},
number = {4},
pages = {e43},
keywords = {augmented reality (AR), explainable artificial intelligence (XAI), robot learning},
doi = {https://doi.org/10.1002/ail2.43},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.43},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ail2.43},
abstract = {Abstract We present a novel augmented reality (AR) interface to provide effective means to diagnose a robot's erroneous behaviors, endow it with new skills, and patch its knowledge structure represented by an And-Or-Graph (AOG). Specifically, an AOG representation of opening medicine bottles is learned from human demonstration and yields a hierarchical structure that captures the spatiotemporal compositional nature of the given task, which is highly interpretable for the users. Through a series of psychological experiments, we demonstrate that the explanations of a robotic system, inherited from and produced by the AOG, can better foster human trust compared to other forms of explanations. Moreover, by visualizing the knowledge structure and robot states, the AR interface allows human users to intuitively understand what the robot knows, supervise the robot's task planner, and interactively teach the robot with new actions. Together, users can quickly identify the reasons for failures and conveniently patch the current knowledge structure to prevent future errors. This capability demonstrates the interpretability of our knowledge representation and the new forms of interactions afforded by the proposed AR interface.},
year = {2021}
}

%%Luebbers2021
@INPROCEEDINGS{Luebbers2021,
  author={Luebbers, Matthew B. and Brooks, Connor and Mueller, Carl L. and Szafir, Daniel and Hayes, Bradley},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={ARC-LfD: Using Augmented Reality for Interactive Long-Term Robot Skill Maintenance via Constrained Learning from Demonstration}, 
  year={2021},
  volume={},
  number={},
  pages={3794-3800},
  doi={10.1109/ICRA48506.2021.9561844}
}

%%Zea2021
@article{Zea2021,
title = {iviz: A ROS visualization app for mobile devices},
journal = {Software Impacts},
volume = {8},
pages = {100057},
year = {2021},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2021.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2665963821000051},
author = {Antonio Zea and Uwe D. Hanebeck},
keywords = {Robotics, Data visualization, Augmented reality},
abstract = {In this work, we introduce iviz, a mobile application for visualizing data in the Robot Operating System (ROS). In the last few years, the popularity of ROS has grown enormously, making it the standard platform for robotic programming. However, the availability of this environment is generally restricted to PCs with the Linux operating system. Thus, users wanting to see what is happening in the system with a smartphone or a tablet are stuck with solutions such as screen mirroring or web browser versions of rviz, making newer visualization modalities such as Augmented Reality impossible. Our application iviz, based on the Unity engine, addresses these issues by providing a visualization platform designed from scratch to be usable in mobile platforms such as iOS, Android, and UWP, and including native support for Augmented Reality for all three platforms. If desired, it can also be used in a PC with Linux, Windows, or macOS without any changes.}
}

  
