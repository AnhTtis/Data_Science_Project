\pdfoutput=1
%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf,9pt]{acmart}
%\addtolength\abovecaptionskip{-0.1cm}
 \addtolength\textfloatsep{-0.215cm}
 \addtolength{\dbltextfloatsep}{-0.12cm}
 \addtolength\parskip{-0.025cm}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{ulem}
\theoremstyle{remark}
\newtheorem{definition}{Definition}[section]

\newcommand{\Xv}{\boldsymbol{X}}
\newcommand{\Yv}{\boldsymbol{Y}}
\newcommand{\Zv}{\boldsymbol{Z}}

\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\zv}{\boldsymbol{z}}
%%\usepackage{makecell}
\usepackage{subcaption}
\theoremstyle{remark}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.

%\setcopyright{acmlicensed}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.

\acmConference[SIGMOD '24]{the 2024
International Conference on Management of Data}{June 9--15,
  2024}{Santiago, AA, Chile}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}

\acmISBN{978-1-4503-XXXX-X/18/06}

%\copyrightyear{2024} 
%\acmYear{2024} 
%\setcopyright{acmlicensed}\acmConference[SIGMOD-Companion
%'24]{Companion of the 2024 International Conference on Management of
%Data}{June 9--15, 2024}{Santiago, AA, Chile}
%\acmBooktitle{Companion of the 2024 International Conference on
%Management of Data (SIGMOD-Companion '24), June 9--15, 2024, Santiago,
%AA, Chile}
%\acmDOI{10.1145/3626246.3654733}
%\acmISBN{979-8-4007-0422-2/24/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{UniTS: A Universal Time Series Analysis Framework Powered by Self-Supervised Representation Learning}

%%
%% The "author" command and its associated commands are UniTS: A Universal Time Series Analysis Framework with Self-supervised Representation Learningused to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Zhiyu Liang}
\orcid{0000-0003-0083-2547}
\affiliation{%
	\institution{Harbin Institute of Technology}
	\city{Harbin}
	\country{China}
}
\email{zyliang@hit.edu.cn}

%\author{Zhiyu Liang$^1$,\ Chen Liang$^1$,\  Zheng Liang$^1$,\ Hongzhi Wang$^1$,\ Bo Zheng$^2$}
%\affiliation{%
%	\institution{$^1$ Harbin Institute of Technology}
	%\city{Harbin}
	%\country{China}
%}
%\email{zyliang@hit.edu.cn, 1190201818@stu.hit.edu.cn, {lz20, wangzh}@hit.edu.cn}
%\affiliation{%
%	\institution{$^2$ CnosDB Inc}
	%\city{Harbin}
	%\country{China}
%}
%\email{harbour.zheng@cnosdb.com}
\author{Chen Liang}
\orcid{0000-0002-1093-0362}
\affiliation{%
	\institution{Harbin Institute of Technology}
	\city{Harbin}
	\country{China}
}
\email{1190201818@stu.hit.edu.cn}

\author{Zheng Liang}
\orcid{0000-0003-1844-4366}
\affiliation{%
	\institution{Harbin Institute of Technology}
	\city{Harbin}
	\country{China}
}
\email{lz20@hit.edu.cn}

\author{Hongzhi Wang}
\orcid{0000-0002-7521-2871}
\affiliation{%
	\institution{Harbin Institute of Technology}
	\city{Harbin}
	\country{China}
}
\email{wangzh@hit.edu.cn}\authornote{Corresponding author.}

\author{Bo Zheng}
\orcid{0009-0005-5309-3364}
\affiliation{%
	\institution{CnosDB Inc.}
	\city{Beijing}
	\country{China}
}
\email{harbour.zheng@cnosdb.com}

%\author{Huifen Chan}
%\affiliation{%
%  \institution{Tsinghua University}
%  \streetaddress{30 Shuangqing Rd}
%  \city{Haidian Qu}
%  \state{Beijing Shi}
%  \country{China}}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Zhiyu Liang et al.}
%% No italics


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
 Machine learning has emerged as a powerful tool for time series analysis. Existing methods are usually customized for different analysis tasks and face challenges in tackling practical problems such as partial labeling and domain shift. To improve the performance and address the practical problems universally, we develop UniTS, a novel framework that incorporates self-supervised representation learning (or pre-training). The components of UniTS are designed using sklearn-like APIs to allow flexible extensions. We demonstrate how users can easily perform an analysis task using the user-friendly GUIs, and show the superior performance of UniTS over the traditional task-specific methods without self-supervised pre-training on five mainstream tasks and two practical settings. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257</concept_id>
       <concept_desc>Computing methodologies~Machine learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002950.10003648.10003688.10003693</concept_id>
       <concept_desc>Mathematics of computing~Time series analysis</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Mathematics of computing~Time series analysis}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Time series analysis, Self-supervised learning, Pre-training.}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
%A time series is a sequence of data points indexed in time order. The time series data analysis tasks such as classification, forecasting and anomaly detection play vital roles in the real-world applications.

Machine learning methods have achieved high performance in many time series analysis tasks, such as classification, forecasting, and anomaly detection\cite{sktime,tslearn,vldb22-ad-evaluation}. However, existing learning-based time series analysis algorithms still face challenges in real-world scenarios. First, the real-world time series data are usually partially labeled due to the high cost or the lack of knowledge for labeling, while the supervised machine learning techniques require adequate labels to perform well. Second, a common problem in practical applications is the domain shift, i.e., the distributions between the training data samples and the data encountered when deploying the models are different, which makes the models difficult to generalize. Last but not least, while there are many task- and domain-specific approaches, it is an open question to determine appropriate methods for a given application (a.k.a. analysis task and dataset). 


%\noindent
\textbf{\underline{Contributions}.}  To deal with the above issues, in this paper, we propose \texttt{UniTS}, a novel universal framework for time series analysis. Our idea is to first perform self-supervised pre-training using the unlabeled data to get unified time series representations, which are more independent of the tasks (e.g. classification or anomaly detection) and domains (i.e., data distributions). Next, the models are learned for arbitrary analysis tasks by appending an output module upon the representations and then fine-tuning the model parameters. \textbf{\textit{Compared to traditional machine learning pipelines that learn end-to-end models from scratch for specific tasks and domains, the \texttt{UniTS} framework has several advantages.}}  

\textbf{\textit{First}}, the pre-training module can leverage the inherent structure of the unlabeled data to learn class-distinguishing information, so that only a few labels are needed for fine-tuning. The additional information learned via self supervision can also improve the performance of different tasks. \textbf{\textit{Second}}, benefiting from the self-supervised representation learning, the pre-training module can learn features transferable across domains by disentangling the domain and class information~\cite{shen2022connect}. \textbf{\textit{Third}}, as \texttt{UniTS} produces unified representations for different pre-training models and downstream analysis tasks, we design a feature fusion module that automatically combines the features of diverse models to jointly facilitate the tasks, so that to avoid the method selection for applications. \textbf{\textit{In addition}}, the \texttt{UniTS} pipeline can be more efficient when performing several tasks on one dataset, because the pre-training is needed only once, while the fine-tuning usually requires a much smaller number of iterations compared to the training from scratch.


\begin{table*}[t]
    \centering
    \caption{Examples of five mainstream time series analysis tasks.}
    \vspace{-3ex}
    \resizebox{.9\linewidth}{!}{
    \begin{tabular}{lcc}
    \toprule
    \textbf{Task}     &  \textbf{Target} & \textbf{Description} \\
    \midrule
    Classification & $y_i \in \{c_j|j=1,\ldots,C\}$ & The class label. \\
    \hline
    Clustering & $y_i \in \{j|j=1,\ldots,C\}$ & The cluster assignment.\\
    \hline
    Forecasting & $\yv_i = \xv_{i,\ T+1:T+H}$ & The values at the $H$ subsequent time steps.\\
    \hline
    Anomaly detection & $\yv_i \in \{0,1\}^{T}$ & \makecell[c]{The bool values indicating whether the observations at each time step is anomalous.}\\
    \hline
    \makecell[l]{Missing value imputation} & $\yv_i = \xv_{i,j,k}, (j,k) \in P_i$ & \makecell[c]{The missing values. $P_i$ is the set of index indicating the positions of the missing values.} \\
    \bottomrule
    \end{tabular}}
    \label{tab:task_examples}
    \vspace{-1ex}
\end{table*}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.67\linewidth]{UniTS/figures/framework-embed.pdf}
    \vspace{-1.8ex}
    \caption{Framework overview of \texttt{UniTS}.}
    \label{fig:framework}
    \vspace{-1.2ex}
\end{figure*}
While there exist several machine learning tools for time series, such as tslearn~\cite{tslearn} and sktime~\cite{sktime}, to the best of our knowledge, our \texttt{UniTS} is \textbf{\textit{the first to incorporate self-supervised representation learning for universal time series analysis to achieve the aforementioned benefits}}. We integrate user-friendly Web interfaces and flexible modes of hyper-parameter configuration for usability. \texttt{UniTS} is designed as a framework that is \textbf{\textit{agnostic to model architecture, pre-training algorithm, feature fusion method and analysis task.}} The model architecture is taken as hyper-parameters, and the latter three components are designed as templates using the popular sklearn-like APIs, which allows easy extension to support different models, algorithms and tasks.

In this paper, we demonstrate the usage of \texttt{UniTS} for five mainstream time series analysis tasks, including \textbf{\textit{classification}}, \textbf{\textit{clustering}}, \textbf{\textit{forecasting}}, \textbf{\textit{anomaly detection}}, and \textbf{\textit{missing value imputation}}.  We also examine the performance of \texttt{UniTS} in these tasks and the practical problems of \textbf{\textit{partial labeling}} and \textbf{\textit{domain shift}}. We have made the source code and the supplementary materials publicly available at \textbf{\textit{\url{https://github.com/LceOmlet/UniTS}}} to enable the community to use and extend the system.



%\vspace{-1ex}
\section{System Overview}

\subsection{Problem Formulation}
We first introduce the unified formulation of \textbf{\textit{time series analysis}}, which serves as the basis of our framework.


\begin{definition}[Time Series Analysis]
%\vspace{-0.4ex}
Given a time series sample $\xv_i = [\xv_{i,1}, \ldots, \xv_{i,T}] \in \mathbb{R}^{D \times T}$ where $\xv_{i,t} \in \mathbb{R}^D$ is the observation at time $t$, $D$ is the number of dimensions, and $T$ is the length of time ranges, a time series analysis task aims to build a function $f$ that can map $\xv_i$ to a task-dependent prediction $\hat{\yv}_i$ (or $\hat{y}_i$), such that the prediction is close to the (usually unknown) target   $\yv_i$ ($y_i$) . Table~\ref{tab:task_examples} shows examples of five important time series tasks.
%\vspace{-0.4ex}
\end{definition}

%For example, in classification task, the prediction is a discrete value of a label set, i.e., $\hat{y}_i \in \{c_j\}_{j=1}^C$, while $\hat{\yv}_i \in \mathbb{R}^{H}$ for the forecasting task of $H$ time steps.
 
In machine learning methods, the mapping function $f(\xv_i)$ is learned from a training dataset $\Xv = [\xv_1,\ldots,\xv_N] \in \mathbb{R}^{N \times D \times T}$ and an optional label set $\Yv \in \mathbb{R}^{N \times *}$. \textit{Unlike the traditional approaches that learn $f_{_\mathcal{T}}(\xv_i)$ from scratch for each task $\mathcal{T}$, our \texttt{UniTS} first \textbf{pre-trains} one or more \textbf{task-independent encoders} $h_m$ using only $\Xv$ to map the time series to unified representations, as $\zv_{i,m} = h_m(\xv_i) \in \mathbb{R}^{K}$ ($m=1,\ldots,M$). Then, given a task $\mathcal{T}$, it fuses the features $\zv_{i,1},\ldots,\zv_{i,M}$ to one vector $\zv_i^\prime \in \mathbb{R}^{K^\prime}$ to build a \textbf{task-specific model based on the encoders}, with $f_{_\mathcal{T}}(\xv_i) = g_{_\mathcal{T}}(\zv_i^\prime)$ where $g_{_\mathcal{T}}$ is an output model.} The design goal is to take advantage of various self-supervised pre-training methods to easily achieve universal performance improvement for different analysis tasks and to address the challenges of partial labeling and domain shift.
For the description, we denote $\Zv_m = [\zv_{1,m},\ldots,\zv_{N,m}] \in \mathbb{R}^{N \times K}$.

%\vspace{-2ex}
\subsection{UniTS Framework}
To achieve universal time series analysis via self-supervised pre-training, \texttt{UniTS} is designed with three main modules, including the \textbf{\textit{Pre-training Module}}, the \textbf{\textit{Feature Fusion Module}}, and the \textbf{\textit{Analysis Task Module}}, as illustrated in Figure~\ref{fig:framework}.

First of all, \texttt{UniTS} creates one or more instances of the pre-training templates with their hyper-parameters, where \textbf{\textit{each template is a self-supervised learning method}}. \texttt{UniTS} has implemented various types of template, as discussed in Section~\ref{sec:pre-training-module}. During pre-training, each instance \textbf{\textit{separately}} learns its encoder $h_m$ ($m = 1, \ldots, M$), while all encoders $h_1, \ldots, h_M$ are \textbf{\textit{jointly}} used for the analysis tasks. The variety of pre-trained representations can be complementary with each other to achieve better performance.

After pre-training, the feature fusion module combines all learned representations of each sample $\xv_i$ into one embedding, denoted as $\zv_i^\prime \in \mathbb{R}^{K^\prime}$. The goal of this module is to \textbf{\textit{automatically}} fuse the information from different pre-training instances to better facilitate the tasks. The details of this module are shown in Section~\ref{sec:feature-fusion}.

Any analysis task $\mathcal{T}$ can be performed on top of $\zv_i^\prime$ by using a \textbf{\textit{task-specific}} function $g_{_{\mathcal{T}}}$ to map $\zv_i^\prime$ to the prediction $\hat{\yv}_i$ (or $\hat{y}_i$) of the target, and then \textbf{\textit{fine-tuning}} the models (including the encoders, the learnable feature fusion model and the task-specific layers) by minimizing a loss function of the task. Presently, \texttt{UniTS} supports the five mainstream tasks illustrated in Table~\ref{tab:task_examples}, while other tasks can be seamlessly integrated using the sklearn-style APIs. The analysis task module is discussed in detail in Section~\ref{sec:tasks}.

%\noindent
\textbf{\underline{Discussion}.} From the above description, \textit{any time series analysis task can be carried out with \texttt{ UniTS} in a \textbf{\textit{unified}} way}. At the training stage, the model $f_{_{\mathcal{T}}}$ is built following the above pipeline. During the inference stage, the learned $f_{_{\mathcal{T}}}$ is used to map the input series to the predictions. To deal with the problems of partial labeling, only a small size of the labeled data is required for fine-tuning, while \textbf{\textit{the pre-training stage does not rely on any labels (Figure~\ref{fig:sub:pipe}).}} Similarly, when facing the problem of domain shift, \textbf{\textit{the user can pre-train the encoders using the data from an available source domain to get the transferable representations}}, and then fine-tune $f_{_{\mathcal{T}}}$ using a small data set from the target domain (Figure~\ref{fig:sub:pipe}). In any case, \textbf{\textit{the pre-trained encoders can be directly used without re-training}}, and the data size and the number of iterations for fine-tuning can be much smaller than the training from scratch~\cite{CSL} while guaranteeing competitive performance (Figure~\ref{fig:performance}).      

%To facilitate the usage, \texttt{UniTS} provides \textbf{\textit{three modes}} for flexible hyper-parameter configuration: i) \textit{Default} mode runs all algorithms with our pre-defined hyper-parameters that have been widely shown effective; ii) \textit{Manual} mode allows the user to set the hyper-parameters manually; iii) \textit{Smart} mode adopts the popular Bayesian optimization for automated hyper-parameter tuning. Besides, we design user-friendly GUIs to allow code-free interaction.



\vspace{-0.5ex}
\section{System Internals}
This section introduces the technical details of the three main modules in \texttt{UniTS}. \textbf{\textit{All the techniques mentioned below have been implemented in the current version.}}

\vspace{-1.2ex}
\subsection{Pre-training Module}\label{sec:pre-training-module}
\texttt{UniTS} provides various self-supervised representation learning methods that can be used as pre-training templates.  Below, we explain the currently implemented pre-training templates, and discuss how to easily add new templates via the sklearn-like APIs. 



In general, \texttt{UniTS} has integrated the following pre-training methods, which are \textbf{\textit{divided into three types based on different pre-training objective functions}} (see the references for more details).

%\noindent
\textbf{\underline{Contrastive Learning}.}  \texttt{UniTS} currently supports time series contrastive learning at three levels, i.e. the whole-series level~\cite{CSL}, the sub-sequence level~\cite{T-Loss}, and the timestamp level~\cite{ts2vec}. These methods \textbf{\textit{encourage the representations generated from the similar inputs to be closer than those of dissimilar inputs}}, which has shown effective in extracting informative features.   



%\noindent
\textbf{\underline{Autoregression.}} The autoregression-based pre-training algorithm~\cite{TST} learns the representations by masking some observations of the input series (e.g., set to 0) and then \textbf{\textit{predicting the masked values}} using the unmasked data.  It is inspired by the masked language model in natural language processing, since both time series and sentences share the same nature of sequential dependencies. 


\textbf{\underline{Hybrid}.}  This branch of approach~\cite{TS-TCC} optimizes a \textbf{\textit{hybrid objective of the two types above}}, which may be better than the individual objectives in some cases, but not always.

The pre-training template is designed using a unified sklearn-like interface to allow flexible extension. A new algorithm can be seamlessly integrated as long as it is wrapped in a Python class with two methods: \textit{fit} which takes the input $\Xv$ for pre-training, and \textit{transform} which maps $\Xv$ to the representations $\Zv$.   

\vspace{-1.5ex}
\subsection{Feature Fusion Module}\label{sec:feature-fusion}
This module fuses the representations of a variety of pre-training instances. The module is also designed using sklearn-style interfaces. A feature fusion class contains a \textit{transform} function that maps a set of representations $(\Zv_1,\ldots,\Zv_M)$ into one unified $\Zv^{\prime} \in \mathbb{R}^{N \times K^\prime}$, and $\Zv^{\prime}$ is used for the analysis tasks. Note that the parameters of a feature fusion instance can be optimized at the fine-tuning stage if they are learnable, which allows \textbf{\textit{automatically extracting information from all pre-trained representations}}. \texttt{UniTS} now supports two basic feature fusion methods as described below.      

\textbf{\underline{Concatenation.}} The simplest but most popular way of feature fusion is directly concatenating the features of each sample, i.e., $\zv_i^\prime = \zv_{i,1} \oplus \ldots \oplus \zv_{i,M}$, where $\oplus$ is the concatenation operation.


\textbf{\underline{Projection.}} One can also use a learnable model $p$ to project the concatenated features into another latent space. This is especially effective in some cases such as clustering where dimension reduction is usually required. Formally, we have $\zv_i^\prime = p(\zv_{i,1} \oplus \ldots \oplus \zv_{i,M})$.

\vspace{-2.8ex}
\subsection{Analysis Task Module}\label{sec:tasks}
The analysis task module is designed as a template wrapped with the sklearn-like APIs, where each task is an instance. The template contains two major components: a \textit{fit} method which performs the task-specific fine-tuning, and a \textit{predict} method which outputs the final prediction. \texttt{UniTS} currently supports the five important tasks described in Table~\ref{tab:task_examples}. Below, we briefly explain the technical details. 


\textbf{\underline{Classification.}} This task is performed in a \textbf{\textit{standard}} manner. The output model projects the representations into $C$ classes, and the \textbf{\textit{softmax}} function is used to generate the class distribution. The standard \textbf{\textit{cross-entropy loss}} is used for fine-tuning, and the class that gives the maximum probability is determined as the prediction. 


\textbf{\underline{Clustering.}} The clustering task can be directly performed by running a classical clustering algorithm (e.g., $k$-Means) on top of the representations. Moreover, one can also fine-tune the models for better performance. The fine-tuning is designed based on the \textbf{\textit{$k$-Means loss}}. At each epoch, the $k$-Means algorithm is run on the representations to obtain the $C$ centroids. Then, the sum of the L2 norms between each representation vector and its centroid is added to the pre-training loss as a regularization term, which encourages the clustering structure of the representations. Note that we do not directly minimize the k-means loss to avoid the trivial solution, i.e, all representations become equal to their centroids.

\textbf{\underline{Forecasting.}} Forecasting is performed in the \textbf{\textit{standard}} way. A decoder is used to transform the representations into predictions. Then, the forecasting loss such as the \textbf{\textit{mean square error}} (MSE) or the \textbf{\textit{mean absolute error}} (MAE) is minimized for fine-tuning. In the inference stage, the decoder outputs are the forecasted values. 

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=.8\linewidth]{figures/FT.pdf}
%    \caption{Demonstrations of \texttt{UniTS}}
%    \label{fig:demo}
%\end{figure}

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=\linewidth]{figures/performance.pdf}
%    \caption{Demonstrations of \texttt{UniTS}}
%    \label{fig:demo}
%\end{figure}


\textbf{\underline{Anomaly detection.}} We employ a popular \textbf{\textit{reconstruction-based}} framework~\cite{vldb22-ad-evaluation} for anomaly detection. A decoder is added to project the representations to the predicted inputs $\hat{\xv}_i$. The objective is to minimize the \textbf{\textit{reconstruction loss}} as $||\xv_i - \hat{\xv}_i||$. For detection, an anomaly score $s_t$ is computed at each time step $t$ as $|\hat{\xv}_{i,t} - \xv_{i,t}|$. The observation with the score larger than a threshold $\tau$ is determined as an anomaly, i.e., $\hat{\yv}_{i,t} = 1$ if $s_t > \tau$ and 0 otherwise.   



\textbf{\underline{Missing value imputation.}} This task is performed using a modern structure named \textbf{\textit{denoising autoencoder}} (DAE)~\cite{dae}. During fine-tuning, we generate a random binary mask $\boldsymbol{m}_i \in \{0,1\}^{D \times T}$ for each $\xv_i$. The masked sample $\xv_i \otimes \boldsymbol{m}_i$ is input to the pre-trained encoders, where $\otimes$ denotes the element-wise multiplication. Then, a decoder is used on top of the representations to reconstruct the entire input $\xv_i$ as $\hat{\xv}_i$. DAE is learned by minimizing the \textbf{\textit{reconstruction loss}} $||\xv_i - \hat{\xv}_i||$. At the inference stage, the missing values of an input $\xv_i$ are first replaced by 0, i.e.,  $\xv_{i,j,k} = 0, \forall (j,k) \in P_i$. Next, $\xv_i$ is input into the fine-tuned model to predict $\hat{\xv}_i$. To this end, the missing value at position $(j,k) \in P_i$ is imputed using $\hat{\xv}_{i,j,k}$.


\begin{figure}[t]
	\centering
	\subcaptionbox{ Pipelines for partially-labeled and cross-domain tasks.\label{fig:sub:pipe}}
	{\includegraphics[width=.51\linewidth]{UniTS/figures/FT-embed.pdf}}
	\subcaptionbox{\centering \texttt{UniTS} interfaces.\label{fig:sub:gui}}
	{\includegraphics[width=.48\linewidth]{UniTS/figures/GUI-classification-narrow-notitle.png}}
    %\subcaptionbox{Overall performance\label{fig:sub:performance}}
	%{\includegraphics[width=.35\linewidth]{figures/performance.pdf}}
 \vspace{-1.5ex}
	\caption{Demonstration overview of \texttt{UniTS}}
  \label{fig:demo}
 % \vspace{-0.6ex}
\end{figure}

%\vspace{-0.5ex}
\section{Demonstration}
In our demonstration, we intend to show how \texttt{UniTS} can help the user to perform different time series analysis tasks, and how it can tackle the practical problems of partial labeling and domain shift. 

%\noindent
%\hspace{-2ex}
\textbf{\underline{Unified pipeline for time series analysis.}} We prepare the UEA datasets~\cite{UEA} for the audiences to interact with \texttt{UniTS}. They can also analyze their own data using our system. Overall, the user takes the following two steps to perform an anslysis task using \texttt{UniTS}. 

\textbf{\text{$\ \ \bullet$ Pre-training.}} In this step, the user adopts the \texttt{UniTS} interfaces to load the data set and configure the pre-training methods with the templates to generate the instances, and then clicks the ``Pre-training'' button to start the self-supervised learning. \texttt{UniTS} visualizes the loss curves to help the user for monitoring. The user can select and save the encoders for the analysis tasks. \textit{Note that this step is \textbf{only needed once}. All the following processes can be repeatedly performed using the pre-trained encoders without re-training.}    

\textbf{\text{$\ \ \bullet$ Fine-tuning.}} In this stage, the user loads the pre-trained encoders and the training data of the task. The user can configure the feature fusion and analysis task modules through the GUIs. Once the ``Fine-tuning'' button is activated, \texttt{UniTS} starts to learn the model for the selected task. The loss curves are plotted as the pre-training process. \texttt{UniTS} also provides the visualization and evaluation of the results for different tasks to validate the models.  After fine-tuning, the user can save the model as a standard JSON file which can be employed by any machine learning tool for inference.
   


%\noindent
%\hspace{-2ex}
\textbf{\underline{Addressing the practical problems.}} 
Benefiting from the self-supervised pre-training which learns informative and transferable features, the user can  tackle the problems of partial labeling and domain shift using \texttt{UniTS}. The process is illustrated in Figure~\ref{fig:sub:pipe}.

\text{\textbf{$\ \ \bullet$ Partial labeling.}} In this scenario, the user only needs to load the available labeled data for fine-tuning. \texttt{UniTS} can achieve \textbf{\textit{significantly better performance}} compared to traditional training from scratch using only the labeled data (Figure~\ref{fig:performance}). 


\text{\textbf{$\ \ \bullet$ Domain shift.}} To achieve cross-domain analysis using \texttt{UniTS}, the user can fine-tune the models with only a small size of data from the target domain based on the pre-trained encoders. The models learned with \texttt{UniTS} can be \textbf{\textit{more generalizable}} than the models \underline{\textbf{t}}rained \underline{\textbf{f}}rom \underline{\textbf{s}}cratch using the same data from the target domain (i.e. \texttt{TFS-Target}) or both domains (\texttt{TFS-Both}) (Figure~\ref{fig:performance}).

\begin{figure}[t]
    \centering
    \includegraphics[width=.88\linewidth]{UniTS/figures/performance-v2.pdf}
    \vspace{-1.6ex}
    \caption{Overall performance of \texttt{UniTS}}
    \label{fig:performance}
    \vspace{-0.6ex}
\end{figure}


%\noindent
%\hspace{-2ex}
\textbf{\underline{Superior performance.}} We also evaluate \texttt{UniTS} using real-world tasks and settings. Our application scenarios include \textbf{\textit{human action recognition (HAR)}},  \textbf{\textit{fault detection (FD)}} for two machines under different working conditions (a.k.a. domains) and \textbf{\textit{server monitoring (SM)}}. For benchmarking, we create one instance for every pre-training template described in Sec.~\ref{sec:pre-training-module} using their \textit{default encoders}, and use a \textit{linear layer} to fuse all representations into 64 dimensions. For task-specific output models, we use a standard \textit{linear layer for classification tasks} and a simple \textit{multilayer perceptron (MLP) with one hidden layer as the decoder} for forecasting, anomaly detection, and imputation. We compare \texttt{UniTS} with the traditional solutions of \textit{directly training the entire model $f_{_{\mathcal{T}}}$ (encoders + fusion layer + output model) from scratch using the task-specific loss function without self-supervised pre-training.} The model architectures are the same as \texttt{UniTS}.  The results in Figure~\ref{fig:performance} indicate the \textbf{\textit{superiority of \texttt{UniTS} powered by self-supervised representation learning.}}

%\vspace{-1ex}
\begin{acks}
 This paper was supported by the NSFC grant (62232005, 62202126), the National Key Research and Development Program of China (2021YFB3300502), and the Postdoctoral Fellowship Program of CPSF (GZC20233457).
\end{acks}
%\vspace{-1ex}
%\clearpage
\normalem
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample}
\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
