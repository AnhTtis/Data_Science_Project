% VLDB template version of 2020-08-03 enhances the ACM template, version 1.7.0:
% https://www.acm.org/publications/proceedings-template
% The ACM Latex guide provides further information about the ACM template


\documentclass[sigconf, nonacm]{acmart}

%\addtolength\topskip{-1cm}
\addtolength\textheight{3ex}
\addtolength\abovecaptionskip{-0.1cm}
% \addtolength\textfloatsep{-0.215cm}
 \addtolength{\dbltextfloatsep}{-0.1cm}
 \addtolength\parskip{-0.025cm}
\usepackage{makecell}
\usepackage{subcaption}
\theoremstyle{remark}
\newtheorem{definition}{Definition}[section]

\newcommand{\Xv}{\boldsymbol{X}}
\newcommand{\Yv}{\boldsymbol{Y}}
\newcommand{\Zv}{\boldsymbol{Z}}

\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\zv}{\boldsymbol{z}}


%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{URL_TO_YOUR_ARTIFACTS}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 

\begin{document}
\title{UniTS: A Universal Time Series Analysis Framework with Self-supervised Representation Learning}

%%
%% The "author" command and its associated commands are used to define the authors and their affiliations.
\author{Zhiyu Liang}
\affiliation{%
	\institution{Harbin Institute of Technology}
	%\city{Harbin}
	%\country{China}
}
\email{zyliang@hit.edu.cn}

\author{Chen Liang}
\affiliation{%
	\institution{Harbin Institute of Technology}
	%\city{Harbin}
	%\country{China}
}
\email{1190201818@stu.hit.edu.cn}

\author{Zheng Liang}
\affiliation{%
	\institution{Harbin Institute of Technology}
	%\city{Harbin}
	%\country{China}
}
\email{lz20@hit.edu.cn}

\author{Hongzhi Wang}

\affiliation{%
	\institution{Harbin Institute of Technology}
	%\city{Harbin}
	%\country{China}
}
\email{wangzh@hit.edu.cn}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Machine learning has emerged as a powerful tool for time series analysis. Existing methods are usually customized for different analysis tasks and face challenges in tackling practical problems such as partial labeling and domain shift. To achieve universal analysis and address the aforementioned problems, we develop  UniTS, a novel framework that incorporates self-supervised representation learning (or pre-training). The components of UniTS are designed using sklearn-like APIs to allow flexible extensions. We demonstrate how users can easily perform an analysis task using the user-friendly GUIs, and show the superior performance of UniTS over the traditional task-specific methods without self-supervised pre-training on five mainstream tasks and two practical settings. 
\end{abstract}

\maketitle

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
\vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. \\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
}\addtocounter{footnote}{-1}\endgroup
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
%\ifdefempty{\vldbavailabilityurl}{}{
%\vspace{.3cm}
%\begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
%The source code, data, and/or other artifacts have been made available at %\url{\vldbavailabilityurl}.
%\endgroup
%}
%%% VLDB block end %%%

\section{Introduction}
%A time series is a sequence of data points indexed in time order. The time series data analysis tasks such as classification, forecasting and anomaly detection play vital roles in the real-world applications.

Machine learning methods have achieved high performance in many time series analysis tasks, such as classification, forecasting, and anomaly detection\cite{sktime,tslearn,vldb22-ad-evaluation}. However, existing learning-based time series analysis algorithms still face challenges in real-world scenarios. First, the real-world time series data are usually partially labeled due to the high cost or the lack of knowledge for labeling, while the supervised machine learning techniques require adequate labels to perform well. Second, a common problem in practical applications is the domain shift, i.e., the data distributions between the training datasets and the data encountered when deploying the models are different, which makes the models difficult to generalize. The last but not least, while there are many task- and domain-specific approaches, it is an open question to determine appropriate methods for a given application (dataset). 
\vspace{0.05cm}

\noindent
\textbf{Contributions.}  To deal with the above issues, in this paper, we propose \texttt{UniTS}, a novel framework for universal time series analysis. Our idea is to perform self-supervised pre-training using the unlabeled data to get unified time series representations, which are more independent of the tasks (e.g. classification or anomaly detection) and domains (i.e., data distributions). Next, the models can be learned for arbitrary analysis tasks by appending an output module upon the representations and then fine-tuning the model parameters. Compared to traditional machine learning pipelines that learn end-to-end models from scratch for specific tasks and domains, the \texttt{UniTS} framework has several advantages.  

\textit{First}, the pre-training module can leverage the inherent structure of the unlabeled data to learn class-distinguishing information, so that only a few labels are needed for fine-tuning. \textit{Second}, benefiting from the self-supervised representation learning, the pre-training module can learn features transferable across domains by disentangling the domain and class information~\cite{shen2022connect}. \textit{Third}, as \texttt{UniTS} produces unified representations for different pre-training models and downstream analysis tasks, we design a feature fusion module that automatically combines the features of diverse models to jointly facilitate the tasks, so that to avoid the method selection for applications. \textit{Furthermore}, the additional information learned via pre-training can improve the performance of the analysis tasks. \textit{And} the \texttt{UniTS} pipeline can be more efficient when conducting several tasks on one dataset, because the pre-training is needed only once while the fine-tuning usually requires a much less number of iterations compared to the training from scratch.

While there exist several machine learning tools for time series, such as tslearn~\cite{tslearn} and sktime~\cite{sktime}, to the best of our knowledge, our \texttt{UniTS} is the first to \textit{incorporate the self-supervised representation learning for universal time series analysis} to achieve the aforementioned benefits. We also integrate user-friendly Web interfaces and flexible modes of hyper-parameter configuration for usability. \texttt{UniTS} is designed as a framework that is \textit{agnostic to model architecture, pre-training algorithm, feature fusion method and analysis task.} The model architecture is taken as hyper-parameters, and the latter three components are designed as templates using the popular sklearn-like APIs, which allows \texttt{UniTS} to be easily extended to support different models, algorithms and tasks.

In this paper, we demonstrate the usage of \texttt{UniTS} for five mainstream time series analysis tasks, including classification, clustering, forecasting, anomaly detection, and missing value imputation.  We also examine the performance of \texttt{UniTS} on these tasks and on the practical problems of partial labeling and domain shift. We have made the source code and the supplementary materials publicly available at \url{https://github.com/LceOmlet/UniTS} to enable the community to use and extend the system.

\begin{table*}[htbp]
    \centering
    \caption{Examples of five mainstream time series analysis tasks.}
    \resizebox{.98\linewidth}{!}{
    \begin{tabular}{lcc}
    \toprule
    \textbf{Task}     &  \textbf{Target} & \textbf{Description} \\
    \midrule
    Classification & $y_i \in \{c_j|j=1,\ldots,C\}$ & The class label. \\
    \hline
    Clustering & $y_i \in \{j|j=1,\ldots,C\}$ & The cluster assignment.\\
    \hline
    Forecasting & $\yv_i = \xv_{i,\ T+1:T+H}$ & The values at the $H$ subsequent time steps.\\
    \hline
    Anomaly detection & $\yv_i \in \{0,1\}^{T}$ & \makecell[c]{The bool values indicating whether the observations at each time step is anomalous.}\\
    \hline
    \makecell[l]{Missing value \\imputation} & $\yv_i = \xv_{i,j,k}, (j,k) \in P_i$ & \makecell[c]{The missing values. $P_i$ is the set of index indicating the positions of the missing values.} \\
    \bottomrule
    \end{tabular}}
    \label{tab:task_examples}
\end{table*}



\section{System Overview}

\subsection{Problem Formulation}
We first introduce the unified formulation of \textit{time series analysis}, which serves as the basis of our framework.

\begin{definition}[Time Series Analysis]
Given a time series sample $\xv_i = [\xv_{i,1}, \ldots, \xv_{i,T}] \in \mathbb{R}^{D \times T}$ where $\xv_{i,t} \in \mathbb{R}^D$ is the observation at time $t$, $D$ is the number of dimensions, and $T$ is the length of time ranges, a time series analysis task aims to build a function $f$ that can map $\xv_i$ to a task-dependent prediction $\hat{\yv}_i$ (or $\hat{y}_i$), such that the prediction is close to the (usually unknown) target   $\yv_i$ ($y_i$) . Table~\ref{tab:task_examples} shows examples of five important time series tasks.
\end{definition}

%For example, in classification task, the prediction is a discrete value of a label set, i.e., $\hat{y}_i \in \{c_j\}_{j=1}^C$, while $\hat{\yv}_i \in \mathbb{R}^{H}$ for the forecasting task of $H$ time steps.
 
In machine learning methods, the mapping function $f(\xv_i)$ is learned from a training dataset $\Xv = [\xv_1,\ldots,\xv_N] \in \mathbb{R}^{N \times D \times T}$ and an optional label set $\Yv \in \mathbb{R}^{N \times *}$. Unlike the traditional approaches that learn $f_{_\mathcal{T}}(\xv_i)$ from scratch for each task $\mathcal{T}$, our \texttt{UniTS} first pre-trains one or more task-independent encoders $h_m$ using only $\Xv$ to map the time series to unified representations, as $\zv_{i,m} = h_m(\xv_i) \in \mathbb{R}^{K}$ ($m=1,\ldots,M$). Then, given a task $\mathcal{T}$, it fuses the features $\zv_{i,1},\ldots,\zv_{i,M}$ to one vector $\zv_i^\prime \in \mathbb{R}^{K^\prime}$ to build a task-specific model based on the encoders, with $f_{_\mathcal{T}}(\xv_i) = g_{_\mathcal{T}}(\zv_i^\prime)$ where $g_{_\mathcal{T}}$ is an output model. The design goal is to take advantage of various self-supervised pre-training methods to easily achieve universal performance improvement for different analysis tasks and to address the challenges of partial labeling and domain shift.
For the description, we denote $\Zv_m = [\zv_{1,m},\ldots,\zv_{N,m}] \in \mathbb{R}^{N \times K}$.

\subsection{UniTS Framework}

To achieve universal time series analysis via self-supervised pre-training, \texttt{UniTS} is designed with three main modules, including the \textit{Pre-training Module}, the \textit{Feature Fusion Module}, and the \textit{Analysis Task Module}, as illustrated in Figure~\ref{fig:framework}.

First of all, \texttt{UniTS} creates one or more instances of the pre-training templates with their hyper-parameters, where each template is a self-supervised learning method. \texttt{UniTS} currently supports diverse types of templates, as discussed in Section~\ref{sec:pre-training-module}. During pre-training, each instance separately learns its encoder $h_m$ ($m = 1, \ldots, M$), while all encoders $h_1, \ldots, h_M$ are jointly used for the analysis tasks. The variety of pre-trained representations can be complementary with each other to achieve better performance.

After pre-training, the feature fusion module combines all learned representations of each sample $\xv_i$ into one embedding, denoted as $\zv_i^\prime \in \mathbb{R}^{K^\prime}$. The goal of this module is to automatically fuse the information from different pre-training instances to better facilitate the tasks. The details of this module are shown in Section~\ref{sec:feature-fusion}.

Any analysis task $\mathcal{T}$ can be conducted on top of $\zv_i^\prime$ by using a task-specific function $g_{_{\mathcal{T}}}$ to map $\zv_i^\prime$ to the predicted target $\yv_i$ (or $y_i$), and then fine-tuning the models (including the encoders, the learnable feature fusion model and the task-specific layers) by minimizing a loss function of the task. Presently, \texttt{UniTS} supports the five mainstream tasks illustrated in Table~\ref{tab:task_examples}, while other tasks can be seamlessly integrated using the sklearn-style APIs. The analysis task module is discussed in detail in Section~\ref{sec:tasks}.

\noindent
\textbf{Discussion.} From the above description, any time series analysis task can be conducted with \texttt{UniTS} in a unified way. At the training stage, the model $f_{_{\mathcal{T}}}$ is built following the above pipeline. During the inference stage, the learned $f_{_{\mathcal{T}}}$ is used to map the input series to the predictions. To deal with the problems of partial labeling, only a small size of the labeled data is required for fine-tuning, while the pre-training stage does not rely on any labels (Figure~\ref{fig:sub:pipe}). Similarly, when facing the problem of domain shift, the user can pre-train the encoders using the data from an available source domain to get the transferable representations, and then fine-tune $f_{_{\mathcal{T}}}$ using a small data set from the target domain (Figure~\ref{fig:sub:pipe}). In any case, the pre-trained encoders can be directly used without re-training, and the data size and the number of iterations for fine-tuning can be much smaller than the training from scratch while guaranteeing competitive performance (Figure~\ref{fig:performance}).      

To facilitate the usage, \texttt{UniTS} provides three modes for flexible hyper-parameter configuration: i) \textit{Default} mode runs all algorithms with our pre-defined hyper-parameters that have been widely shown effective; ii) \textit{Manual} mode allows the user to set the hyper-parameters manually; iii) \textit{Smart} mode adopts the popular Bayesian optimization for automated hyper-parameter tuning. Besides, we design user-friendly GUIs to allow code-free interaction.


\section{System Internals}
This section introduces the technical details of the three main modules in \texttt{UniTS}.

\subsection{Pre-training Module}\label{sec:pre-training-module}
\texttt{UniTS} provides various self-supervised representation learning methods that can be used as pre-training templates.  Below, we explain the currently supported pre-training templates, and discuss how to easily add new templates via the sklearn-like APIs. 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.86\linewidth]{figures/framework.pdf}
    \caption{Framework overview of \texttt{UniTS}.}
    \label{fig:framework}
\end{figure*}

Overall, \texttt{UniTS} now supports three types of pre-training templates based on different pre-training objectives.

\textbf{Contrastive Learning.} These methods encourage the representations of views (e.g., augmentations) of the same input to be more similar than that of different inputs, which has shown effectiveness in extracting informative features. \texttt{UniTS} currently supports time series contrastive learning at three levels, i.e. the whole-series level\cite{tfc}, the sub-sequence level~\cite{T-Loss}, and the timestamp level~\cite{ts2vec}.   




\textbf{Autoregression.} The autoregression-based pre-training learns the representations by masking some observations of the input time series (e.g., set to 0) and then predicting the masked values using the unmasked data~\cite{TST}.  It is inspired by the success of the masked language model in natural language processing, since both time series and sentences share the same nature of sequential dependencies. 


\textbf{Hybrid.}  This branch of approach optimizes a hybrid objective of the two types above~\cite{TS-TCC}, which may be better than the individual objectives in some cases, but not always.

The pre-training template is designed using a unified sklearn-like interface to allow flexible extension. A new algorithm can be seamlessly integrated as long as it is wrapped in a Python class with two methods: \textit{fit} which takes the input $\Xv$ for pre-training, and \textit{transform} which maps $\Xv$ to the representations $\Zv$.   

\subsection{Feature Fusion Module}\label{sec:feature-fusion}
This module fuses the representations of a variety of pre-training instances. The module is also designed using sklearn-style interfaces. A new feature fusion class should contain a \textit{transform} function that maps a set of representations $(\Zv_1,\ldots,\Zv_M)$ into one unified $\Zv^{\prime} \in \mathbb{R}^{N \times K^\prime}$, which is used for the analysis tasks. Note that the parameters of a feature fusion instance can be optimized at the fine-tuning stage if they are learnable, which allows \textit{automatically extracting information from all pre-trained representations}. \texttt{UniTS} now supports two basic feature fusion methods as below, and we expect more advanced techniques to be integrated in the future.      

\textbf{Concatenation.} The simplest but most popular way of feature fusion is directly concatenating the features of each sample, i.e., $\zv_i^\prime = \zv_{i,1} \oplus \ldots \oplus \zv_{i,M}$, where $\oplus$ is the concatenation operation.


\textbf{Projection.} One can also use a learnable model $p$ to project the concatenated features into another latent space. This is especially effective in some cases such as clustering where dimension reduction is usually required. Formally, we have $\zv_i^\prime = p(\zv_{i,1} \oplus \ldots \oplus \zv_{i,M})$.

\subsection{Analysis Task Module}\label{sec:tasks}
The analysis task module is designed as a template wrapped with the sklearn-like APIs, where each task is an instance. The template contains two major components: a \textit{fit} method which performs the task-specific fine-tuning, and a \textit{predict} method which outputs the final prediction. \texttt{UniTS} currently supports the five important tasks described in Table~\ref{tab:task_examples}. Below, we briefly explain the technical details. 


\textbf{Classification.} This task is performed in a standard manner. The output model projects the representations into $C$ classes, and then the softmax function is used to generate the class distribution. The classification loss (e.g., cross-entropy loss) is used for fine-tuning, and the class that gives the maximum probability is determined as the prediction. 


\textbf{Clustering.} The clustering task can be directly performed by running a classical clustering algorithm (e.g., $k$-means) on top of the representations. Moreover, one can also fine-tune the models for better performance. The fine-tuning is designed based on the $k$-means loss. At each epoch, the $k$-means algorithm is run on the representations to obtain the $C$ centroids. Then, the sum of the L2 norms between each representation vector and its centroid is added to the pre-training loss as a regularization term, which encourages the clustering structure of the representations. Note that we do not directly minimize the k-means loss to avoid the trivial solution, i.e, all representations become equal to their centroids.

\textbf{Forecasting.} Forecasting is performed using the standard method. A decoder is used to transform the representations into predictions. Then, the forecasting loss such as the mean square error (MSE) or the mean absolute error (MAE) is minimized for fine-tuning. In the inference stage, the decoder outputs are the forecasted values. 

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=.8\linewidth]{figures/FT.pdf}
%    \caption{Demonstrations of \texttt{UniTS}}
%    \label{fig:demo}
%\end{figure}

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=\linewidth]{figures/performance.pdf}
%    \caption{Demonstrations of \texttt{UniTS}}
%    \label{fig:demo}
%\end{figure}


\textbf{Anomaly detection.} We employ a popular reconstruction-based framework~\cite{vldb22-ad-evaluation} for anomaly detection. A decoder is added to project the representations to the predicted inputs $\hat{\xv}_i$. The objective is to minimize the reconstruction error as $||\xv_i - \hat{\xv}_i||$. For detection, an anomaly score $s_t$ is computed at each time step $t$ as $|\hat{\xv}_{i,t} - \xv_{i,t}|$. The observation with the score larger than a threshold $\tau$ is determined as an anomaly, i.e., $\hat{\yv}_{i,t} = 1$ if $s_t > \tau$ and 0 otherwise.   



\textbf{Missing value imputation.} This task is performed using a modern structure named denoising autoencoder (DAE)~\cite{dae}. During fine-tuning, we generate random binary mask $\boldsymbol{m}_i \in \{0,1\}^{D \times T}$ for each $\xv_i$. The masked sample $\xv_i \otimes \boldsymbol{m}_i$ is input to the pre-trained encoders, where $\otimes$ denotes the element-wise multiplication. Then, a decoder is used on top of the representations to reconstruct the entire input $\xv_i$ as $\hat{\xv}_i$. The DAE is learned through minimizing the reconstruction loss $||\xv_i - \hat{\xv}_i||$. At the inference stage, the missing values of an input $\xv_i$ are first replaced by 0, i.e.,  $\xv_{i,j,k} = 0, \forall (j,k) \in P_i$. Next, $\xv_i$ is input to the fine-tuned model to predict $\hat{\xv}_i$. To this end, the missing value at position $(j,k) \in P_i$ is imputed using $\hat{\xv}_{i,j,k}$.


\begin{figure}[t]
	\centering
	\subcaptionbox{ Pipelines for partially-labeled and cross-domain tasks.\label{fig:sub:pipe}}
	{\includegraphics[width=.51\linewidth]{figures/FT.pdf}}
	\subcaptionbox{\centering \texttt{UniTS} interfaces.\label{fig:sub:gui}}
	{\includegraphics[width=.48\linewidth]{figures/GUI-classification-narrow.jpg}}
    %\subcaptionbox{Overall performance\label{fig:sub:performance}}
	%{\includegraphics[width=.35\linewidth]{figures/performance.pdf}}
	\caption{Demonstration overview of \texttt{UniTS}}
  \label{fig:demo}	
\end{figure}

\section{Demonstration}
In our demonstration, we intend to show how \texttt{UniTS} can help the user to perform different time series analysis tasks, and how it can tackle the practical problems of partial labeling and domain shift. 

\noindent
\textbf{Unified pipeline for time series analysis.} Overall, \texttt{UniTS} takes two steps for an anslysis task, i.e., \textit{pre-training} and \textit{fine-tuning}.

\textit{Pre-training.} In this step, the user adopts the \texttt{UniTS} interfaces to load the data set and configure the pre-training methods with the templates to generate the instances, and then clicks the ``Pre-training'' button to start the self-supervised learning. \texttt{UniTS} visualizes the loss curves to help the user for monitoring. The user can select and save the encoders for the analysis tasks. \textit{Note that this step is only needed once. All the following processes can be repeatedly performed using the pre-trained encoders without re-training.}    

\textit{Fine-tuning.} In this stage, the user loads the pre-trained encoders and the training data of the task. The user can configure the feature fusion and analysis task modules through the GUIs. Once the ``Fine-tuning'' button is activated, \texttt{UniTS} starts to learn the model for the selected task. The loss curves are also plotted as the pre-training process. \texttt{UniTS} also provides the result visualization and evaluation for different tasks to help the users to validate their model.  After fine-tuning, the user can save the model as a standard JSON file which can be employed by any machine learning tool for inference.
   


\noindent
\textbf{Addressing the practical problems.} Benefiting from the self-supervised pre-training which learns informative and transferable features, the user can easily tackle the problems of partial labeling and domain shift using \texttt{UniTS}. The process is illustrated in Figure~\ref{fig:sub:pipe}.

\textit{Partial labeling.} In this scenario, the user only needs to load the available labeled data for fine-tuning. \texttt{UniTS} can achieve competitive performance using \textit{several times less} labels compared to the traditional training from scratch with the labeled data (Figure~\ref{fig:performance}). 


\textit{Domain shift.} In this scenario, only a small size of data in the target domain is required by \texttt{UniTS}. The user can fine-tune the models based on the pre-trained encoders to achieve cross-domain analysis. The models learned with \texttt{UniTS} can be more generalizable than the models trained from scratch using both the source domain data and the target domain data of the same size (Figure~\ref{fig:performance}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/performance.pdf}
    \caption{Overall performance of \texttt{UniTS}}
    \label{fig:performance}
\end{figure}


\noindent
\textbf{Superior performance.} We evaluate the performance of \texttt{UniTS} on several important tasks and settings. The data come from various real-world applications, e.g., human action recognition, fault detection and server monitoring. The result in Figure~\ref{fig:performance} indicates the superiority of \texttt{UniTS} over the traditional solutions of directly training task-specific model $f_{_{\mathcal{T}}}$ without self-supervised pre-training.


\begin{acks}
 This paper was  supported by NSFC grant (62232005, 62202126, U1866602).
\end{acks}

%\clearpage

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample}

\end{document}
\endinput
