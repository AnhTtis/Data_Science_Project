\documentclass[preprint,12pt]{elsarticle}


\usepackage{amssymb}


\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[colorlinks=blue,
            linkcolor=blue,
            anchorcolor=blue,
            citecolor=blue]{hyperref}
\usepackage{multirow}
\newcommand{\setParDis}{\setlength {\parskip} {0.3cm} }

\usepackage[ruled,linesnumbered]{algorithm2e}

\usepackage{graphicx}
\usepackage{float}
\usepackage{subfigure}

\usepackage{booktabs}    
\usepackage{stfloats}

\usepackage{array}
\usepackage{booktabs}
\usepackage{setspace}
\newcommand{\WW}[1]{{\color{red}{[(WW) #1]}}}

\journal{Pattern Recognition}

\begin{document}
\doublespacing
\begin{frontmatter}


\title{Multi-task Meta Label Correction for Time Series Prediction}


\author[inst1]{Luxuan Yang}
\ead{luxuan\_yang@hust.edu.cn}
\affiliation[inst1]{organization={School of Mathematics and Statistics \& Center for Mathematical Sciences},%Department and Organization
            addressline={\\Huazhong University of Science and Technology}, 
            city={Wuhan},
            postcode={430074}, 
            country={China}}

\author[inst1]{Ting Gao\corref{cor1}}
\ead{tgao0716@hust.edu.cn}
\cortext[cor1]{Corresponding author}

\author[inst2]{Wei Wei}
\ead{weiw_sjtu@sjtu.edu.cn}
\affiliation[inst2]{organization={Institute of Natural Sciences},%Department and Organization
            addressline={Shanghai Jiao Tong University}, 
            city={Shanghai},
            postcode={200240},
            country={China}}
\author[inst3]{Min Dai}
\ead{mindai@whut.edu.cn}
\affiliation[inst3]{organization={School of Science},%Department and Organization
            addressline={Wuhan University of Technology}, 
            city={Wuhan},
            postcode={430070},
            country={China}}

\author[inst1]{Cheng Fang}
\ead{fangcheng1@hust.edu.cn}

\author[inst5]{Jinqiao Duan}
\ead{duan@gbu.edu.cn}
\affiliation[inst5]{organization={Department of Mathematics and Department of Physics},%Department and Organization
            addressline={Great Bay University }, 
            city={Dongguan},
            postcode={Guangdong 523000},
            country={China}}
\begin{abstract}

Time series classification faces two unavoidable problems. One is  partial feature information and the other is poor label quality, which may affect model performance. To address the above issues, we create a label correction method to time series data with meta-learning under a multi-task framework. There are three main contributions. First, we train the label correction model with a two-branch neural network for the outer loop. While in the model-agnostic inner loop, we use pre-existing classification models in a multi-task way and jointly update the meta-knowledge, which makes us achieve adaptive labeling on complex time series. Second, we devise new data visualization methods for both image patterns of the historical data and data in the prediction horizon. Finally, we test our method with various financial datasets, including XOM, S\&P500, and SZ50. Results show that our method is more effective and accurate than some existing label correction techniques.

\end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
% \includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
% \item Adaptive labeling method for time series forecasting.

% \item Label information representation with image patterns in the prediction horizon.

% \item Enhanced visualization methods to avoid tendency confusion issue.

% \item Two-branch neural network as a label corrector for task generalization.

% \item The multi-task framework under bi-level optimization with application on financial data.

% \end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Data visualization \sep bi-level optimization \sep meta-learning \sep multi-task learning 
% %% PACS codes here, in the form: \PACS code \sep code
% \PACS 0000 \sep 1111
% %% MSC codes here, in the form: \MSC code \sep code
% %% or \MSC[2008] code \sep code (2000 is the default)
% \MSC 0000 \sep 1111
\end{keyword}

\end{frontmatter}

%% \linenumbers
%% For citations use: 
%%       \citet{<label>} ==> Jones et al. [21]
%%       \citep{<label>} ==> [21]
%%
%% main text
\section{Introduction}

Even though deep learning outperforms many conventional methods through parameter optimization in the field of time series, it still suffers from two inescapable challenges, which include data information shortage and label quality uncertainty.

As time series like stock prices merely contain data values, the matter of data information inadequacy is more likely to emerge in this scenario. In our paper, we improve data information extraction in two aspects: historical time series data patterns (treated as features) and image patterns on the prediction horizon (treated as classification labels). Some efforts have been made for the former part. For example, Wang and Oates \citep{WangGAF} establish Gramian Angular Field and Markov Transition Field methods to encode time series as images to obtain multi-level features. Moreover, a method for visualizing the behavior of nonlinear dynamical systems called Recurrence Plot \citep{eckmann1995recurrence} is also applied to time series in real-world applications. To name a few examples, Shankar et al. \citep{shankar2021analysis} utilize 2D recurrence plot generation to EEG data and train a convolutional neural network to analyze epileptic seizures. Barra et al. \citep{barra2020deep} apply Gramian Angular Field for financial data to train an ensemble of CNNs. Chen et al. \citep{chen2016financial} compare the effectiveness of several visualization methods for converting financial time series to images, such as Candlestick Chart, Gramian Angular Field, Moving Average Mapping, and others. They find that Gramian Angular Field achieves the highest accuracy results. Sezer and Ozbayoglu \citep{sezer2018algorithmic} also convert stock time series data into 15 × 15 sized 2-D images and create a trading model called CNN-TA to offer Buy–Sell–Hold signals. Nevertheless, researchers find that some visualization methods such as Recurrence Plots exist a tendency confusion problem\citep{ZHANG2022108385}. To avoid this issue, we provide some enhanced transformation methods for historical data without neglecting tendency characteristics. As for the latter part to handle image patterns on the prediction horizon (to be used for creating classification labels), we convert data in the prediction horizon into images with a different transformation method called Relative Ratio Plot. The motive is due to that conventional labeling techniques, including the triple barrier approach, ensemble method and so on \citep{bounid2022advanced}, need to manually set thresholds, which makes prediction results occasionally fall short of expectations (see Fig.\ref{fig:imageexample}). For example, as the trend label is determined by the first stopping time that hits the boundaries of the rectangular, we may sometimes get the wrong labels based on different patterns(Fig.\ref{fig:imageexample}(b)), which needs us to adjust the rectangular vertically. While in some scenarios (Fig.\ref{fig:imageexample}(c)), the pattern is still uncertain which may need to adjust the horizontal length in an appropriate way. To solve these issues, we design a new meta-learning algorithm under a multi-task framework to obtain more accurate labels in an adaptive way. 
%In brief, we develop visualization techniques for historical data as well as data from the forecasting horizon, which enables us to gather better information for the model to implement adaptive labeling and label correction.

\begin{figure}[h]
    \centering
    \subfigure[]{\includegraphics[width=0.5\textwidth,height=0.25\textwidth]{TRIPLEBARRIERMETHOD.jpeg}\label{fig: TRIPLE BARRIER METHOD}}
    \subfigure[]{\includegraphics[width=0.23\textwidth,height=0.23\textwidth]{wronglabel2.jpg}\label{fig: wrong label 2}}
    % \hspace{.5in}
    \subfigure[]{\includegraphics[width=0.23\textwidth,height=0.23\textwidth]{wronglabel1.jpg}\label{fig: wrong label 1}}
    \caption{The triple barrier method with wrong label examples. (a). Manual labeling using triple barrier method, where the label of the price trends is determined by the amount of change from beginning to the first stopping time that hits the boundaries of the rectangular. (b). The stock goes down as a whole but the manual label was up. (c). The stock trend is uncertain in the whole picture but the manual label is down.}
    \label{fig:imageexample}
\end{figure}

% Among these inputs, labels are particularly important, and correcting noisy labels has become a popular strategy in the face of challenges associated with obtaining high-quality labels. 

To address label quality issue, some related research on label correction models have emerged as common approaches to handle the difficulties of high-quality label requirements. Most of these methods are established on meta-learning to accomplish the objectives of label correction and label prediction. For instance, Zheng et al.\citep{zheng2021meta} propose a method called Meta Label Correction (MLC) under the framework of meta-learning. They construct two neural networks, one for classification prediction and the other for label correction, which are jointly trained using bi-level optimization. Their approach outperforms conventional re-weighting techniques and has been shown to work efficiently in tasks concerning both language and images. Wu et al.\citep{wu2021learning} also utilize meta-learning, known as Meta Soft Label Corrector (MSLC), to modify labels automatically. Meanwhile, Mallem et al.\citep{mallem2023efficient} propose a new method dubbed CO-META, which applied multiple networks in the inner loop to reduce the risk of over-fitting and improve the quality of extracted features. Alongside the above label correction techniques that utilize weak supervision, label generation is an alternative approach that predominantly relies on semi-supervised and self-supervised methodologies. Pham et al.\citep{pham2021meta} train a network called the teacher network to generate pseudo-labels on unlabeled data, which is also used to instruct another network called the student network under the framework of semi-supervised learning. Ma et al.\citep{ma2022denoised} take label generation as the pretext task with a self-supervised method and focus on obtaining better financial labels.



Each of the aforementioned methods employs the framework of bi-level optimization, which exhibits several challenges in gradient updating and computational memory management, which has attracted a large number of researchers working in gradient-based bi-level optimization including explicit and implicit updating rules. For the former case, Gao et al.\citep{gao2022value} utilize the value function approach to solve tractable convex sub-problems and develop a theoretical framework for sequential convergence towards stationary solutions, without requiring the lower level of the bi-level to be strongly convex and smooth. As the assumption of lower-level convexity is too restrictive, Liu et al.\citep{liu2021towards} propose an algorithm called Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient Method (IAPTT-GM) to handle large-scale and non-convex bi-level optimization problems with a first-order explicit gradient method and dynamic initialization. Beyond first-order gradient methods, second-order gradient techniques that entail hyper-gradient computation at the upper level are also prevalent in optimization literature. To avoid the expensive computational cost for the Hessian matrix, Liu et al.\citep{liu2018darts} use the finite difference approximation to reduce its complexity. As for the latter case, implicit gradient update methods are also increasingly garnering attention. Rajeswaran et al.\citep{rajeswaran2019meta} establish an implicit MAML (iMAML) algorithm based on the implicit function theorem to find an approximate solution to the inner-level optimization problem. This approach avoids storing or differentiating the inner optimization path, making it more memory efficient. Zhang et al.\citep{zhang2021idarts} also apply the implicit function theorem to address the hyper-gradient computation of Differentiable Architecture Search (DARTS).


Our goal includes automatic label correction with the help of a small portion of clean labels and better classification performance independent of backbone models. Motivated by label correction methods in the image field and data visualization techniques for time series, we create a correction architecture for financial data applications with four essential steps in the implementation. First,  we convert time series data in both historical and prediction horizons into image patterns during data processing. Second, we create the framework based on bi-level optimization and extend it to include multi-task learning, which will benefit from meta-learning's generalization for multi-task implementation. Specifically, each task employs separate classifiers, but the same label corrector allows our model to deliver the results of multiple tasks simultaneously, which makes it more applicable. Third, for gradient optimization, we implement the second-order explicit gradient method, where the Hessian matrix in the upper level is approximated by the finite difference method. Finally, We evaluate the effectiveness of our approach on financial data-sets. Note that \ref{acronyms} includes a list of all abbreviation definitions in this paper for quick reference.


Our method achieves the goal of adaptive labeling for financial time series prediction and makes the following contributions, in brief:


$\bullet$ We propose better data visualization methods to overcome  tendency confusion problems  for historical data and replace values of labels with patterns in the prediction horizon to obtain more Buy–Sell–Hold signals.


$\bullet$ We create a two-branch neural network to combine the features of historical data and samples in the prediction horizon without the need for real-world noisy labels, which is more akin to semi-supervised learning.

$\bullet$ We fuse meta-updating rules within the multi-task learning framework and utilize bi-level optimization to achieve joint training, which enhances the adaptability with respect to the prediction time period.


\section{\textbf{Meta Label Correction}}
High-quality labels, unlike noisy ones, enable the significant improvement of neural network performance. Currently, people mainly focus on the label correction for images and test their algorithms on  public benchmark datasets such as  CIFAR-10,  CIFAR-100 \citep{krizhevsky2009learning}, MNIST \citep{lecun1998gradient}, Clothing1M \citep{xiao2015learning}, etc. However, accurately labeled financial data also plays an essential role in predicting stock trends, and this motivates us to investigate label correction methods for financial data. In this section, we will first implement data visualization methods to transfer the stock price data and labels into two-dimensional images, which differs from the standard practices of financial time series prediction. Then, we continue to train bi-level neural networks for these images. 


\subsection{\textbf{Data Processing}}\label{AA}
Following Zheng et al.\citep{zheng2021meta} and Wu et al.\citep{wu2021learning}, we divide the dataset into two parts: a small amount of data with clean labels called the \emph{meta dataset}, and a large amount of data with noisy labels called the \emph{noisy dataset}. Due to the high cost of clean labels, the meta dataset is considerably smaller than the noisy dataset. Moreover, in order to show Buy–Sell–Hold signals more intuitively, the visualization method of prediction horizon is different from the approach of historical data.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.7\linewidth]{HH.jpeg}}
\caption{ Historical data is used for generating samples of training features denoted as ``X" in the following algorithm. Prediction horizon is used for generating labels, whose image patterns are denoted by ``Y" in the following.}
\label{XY}
\end{figure}

\subsubsection{\textbf{Historical data visualization method}} 

The traditional methods we used for stock prediction, such as the LDE-Net model \citep{yang2023neural}, often focus on univariate time series, which makes it difficult to capture potential correlations among data. As the effectiveness of the Gramian Angular Field (GAF) \citep{liu2016encoding} in financial prediction has been demonstrated by Chen et al. \citep{chen2016financial}, we adopt it to transform 1-D time series data into 2-D images using the Gramian Matrix in Eq.~\eqref{GM}
\begin{equation}\label{GM}
\begin{aligned}
& \qquad G=\left[\begin{array}{ccc}
\langle \tilde{x}_1, \tilde{x}_1 \rangle  & \cdots & \langle \tilde{x}_1, \tilde{x}_n\rangle \\
\vdots & \ddots & \vdots \\
\langle \tilde{x}_n, \tilde{x}_1\rangle & \cdots & \langle \tilde{x}_n, \tilde{x}_n\rangle
\end{array}\right], \\
& \text { where }\langle\mathrm{x}, \mathrm{y}\rangle=\mathrm{x} \cdot \mathrm{y}-\sqrt{1-\mathrm{x}^2} \cdot \sqrt{1-\mathrm{y}^2}.
\end{aligned}
\end{equation}

Here, $\tilde{x}_i\ (i=1,\ldots,n)$ in $[0,1]$ represents the scaling value of  $x_i\ (i=1,\ldots ,n)$ in time series data $\mathrm{X}=\left\{x_1, x_2, \ldots, x_n\right\}$. The design of this transformation method not only retains the original information through the upper and left parts of the matrix but also captures additional hidden information through the inner product $\langle \cdot \rangle$, thereby incorporating temporal dependency and correlations. However, as demonstrated in Eq.\eqref{GM} and Fig.\ref{gaf}, the GAF method suffers from a tendency to confuse different patterns, which poses a challenge to accurately predict stock trends. To address this issue, we propose two novel methods, which have been demonstrated to be effective in our experiments.
\begin{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.65\linewidth]{modifygaf.jpg}}
\caption{Tendency confusion problem. The upper left is $y=2x+N(0,0.5)$. $N(0,0.5)$ is a Gaussian noise. The upper middle is the corresponding GAF of $y=2x+N(0,0.5)$. The lower left is $y=2x+N(0,0.5)$ in reverse order. The lower middle is the corresponding GAF plot of reverse data. The right column with our proposed method SGAF, shows different image patterns of the ascending and descending trends respectively, while the middle column (original GAF) has tendency confusion which presents the same patterns for different trends.}
\label{gaf}
\end{figure}

\item
We propose the method named SGAF (Sign function multiplied with Gramian Matrix) is to add a plus-minus sign in front of the Gramian Matrix. Mathematically, the sign function is given by 
\begin{equation}\label{sign}
\begin{aligned}
sign(G_i)=\begin{cases}1, 
  & \text{ if } \frac{x_{n+i}-x_{1+i}}{n-1}\ge 0,\\
  -1, & \text{ if } \frac{x_{n+i}-x_{1+i}}{n-1}<  0.
\end{cases}
\end{aligned}
\end{equation}
Then, the modified GAF matrix used for generating the image becomes
\begin{equation}\label{mgaf}
\begin{aligned}
\widetilde{G} = sign(G_i)\cdot G_i,
\end{aligned}
\end{equation}
where $G_i = \left[\begin{array}{ccc}
\langle \tilde{x}_{1+i}, \tilde{x}_{1+i}\rangle & \cdots & \langle \tilde{x}_{1+i}, \tilde{x}_{n+i}\rangle \\
\vdots & \ddots & \vdots \\
\langle \tilde{x}_{n+i}, \tilde{x}_{1+i}\rangle & \cdots & \langle \tilde{x}_{n+i}, \tilde{x}_{n+i}\rangle
\end{array}\right]$.\\

Note that $x_i$ represents one of the values in the original financial time series, and $\tilde{x}_{i}$ is the scaled value of $x_i$. Moreover, $n$ represents the window size of our training data $x$. While retaining the original characteristics of the GAF matrix, this construction can aid in identifying the overall trend of the sequence (see Fig.~\ref{gaf}).


\item 
Leveraging widely-used image fusion technology from the field of computer vision is another strategy. Recurrence Plot (RP) has been developed as a visualization tool to study complex dynamic systems. By modifying an appropriate threshold and percentage, it can resolve the tendency confusion issue (see Fig.~\ref{gafrp}). This is why we attempted to fuse the GAF and modified RP images, called the GAFRP (Fusion of GAF and modified RP) method, which is the weighted average of pixels from GAF and RP. 
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.65\linewidth]{gafrp.jpg}}
\caption{An example shows tendency confusion problem. (The upper left is $y=2x+N(0,0.5)$. $N(0,0.5)$ is a Gaussian noise. The upper middle is the corresponding RP of $y=2x+N(0,0.5)$. The lower left is $y=2x+N(0,0.5)$ in reverse order. The lower middle is the corresponding RP plot of reverse data. The right column is the corresponding fusion method of the left column. Here the weights of GAF and modified RP are both 0.5. The fusion of both GAF in Fig.~\ref{gaf} and RP actually makes our image patterns have more richful information.}
\label{gafrp}
\end{figure}

\subsubsection{\textbf{Prediction horizon visualization method}}
Different from traditional classification methods, we substitute values of labels with patterns in the forecasting horizon, which assists in obtaining more information on labels to capture Buy–Sell–Hold signals. To categorize the tendency into three categories: rise, stationary, and fall, we propose a novel transformation technique called Relative Ratio Plot (RRP, see  Fig.~\ref{rrp}). The relative ratio is calculated using Eq.~\eqref{RR},
\begin{equation}\label{RR}
\begin{aligned}
    ratio_i = \frac{x_{n+i+m}}{x_{n+i}}-1, \quad \text{for}\; i=1,\dots,k,
\end{aligned}
\end{equation}
where $m$ is the length of the prediction horizon and $n$ is the window size of training data, and $k$ is the number of splitted sequences.
\begin{figure}[htbp]
\centerline{\includegraphics[width=0.7\linewidth]{rrp.jpg}}
\caption{An example explains Relative Ratio Plot. (The left represents fall. The middle is stationary. The right shows a rise.}
\label{rrp}
\end{figure}

\subsubsection{\textbf{Labeling method}} 
For the prediction horizon, we employ Triple Barrier Method \citep{bounid2022advanced}, a conventional technique, to label it. However, since this approach requires the manual definition of thresholds, it is not always reliable in financial systems with complex uncertainties, resulting in noisy labels (see Fig.\ref{fig:imageexample}). To obtain clean labels, we provide a benchmark (see Eq.~\eqref{rb}) according to \citep{li2022selective} and \citep{lai2009evolving}. The approach labels the prediction horizon as a rise (fall) when the mean of the ratios in the prediction horizon is beyond (below) the baseline (negative baseline), and the noisy label is also a rise (fall). For the remaining labels, we manually label them due to the small number of data points.
\begin{equation}\label{rb}
\begin{aligned}
baseline = \omega * (\frac{(1+rate)^{m+1}-(1+rate)}{rate\times m} - 1).
\end{aligned}
\end{equation}
Here, $\omega$ is a constant adjusted by stock price vale, $rate = 0.005$ and $m$ is the forecasting time period (\citep{li2022selective} and \citep{liu2016encoding}). From Fig.~\ref{tsne}, the difference between noisy and clean labels in the tsne embedding can be shown. It can be found that the embedding of $Y$ images corresponding to clean labels is more clearly divided into three categories than the embedding of $Y$ images corresponding to noisy labels.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.7\linewidth]{tsne.jpg}}
\caption{The left is image embedding that corresponds to noisy labels with tsne. The right is the same image data corresponds to the clean labels in the tsne embedding. Note that``0", ``1"and ``2" in the figure represent fall, stationary, and rise respectively. }
\label{tsne}
\end{figure}

\subsection{\textbf{Meta learning for label correction}}\label{BB}
Given the high cost of collecting large numbers of clean labels, we propose a multi-task meta-learning method label correction called \textbf{Multi-task Meta Label Correction} (MMLC) for financial data. Following Zheng et al.\citep{zheng2021meta}, we also establish a framework with \emph{meta model}  regarded as a label correction network (LCN) and \emph{main models}  regarded as classification prediction networks. Nevertheless,  unlike the traditional label correction method, there is no need to collect the noisy labels in our \emph{meta model} which only  utilizes images of the prediction horizon and historical data to correct labels. Additionally, based on the relationship between meta-learning and multi-task learning, our architecture is constructed with multi-task learning, allowing us to achieve both adaptivity and multi-period prediction using the same \emph{meta model} and historical data.

According to the above descriptions, our architecture not only corrects labels to upgrade the quality of obtained classifiers but also achieves the goal of adaptive labeling and multi-step prediction. In this section, we  provide more details to describe the entire framework.

\subsubsection{\textbf{Label correction formulation based on meta learning and multi-task learning}}\label{labelcorrection}
In our framework, we provide meta-training tasks $\left\{\mathcal{T}_i\right\}_{i=1}^N$ obtained from $P(\mathcal{T})$. Moreover, each task $\mathcal{T}_i$ associated with clean data samples $\mathcal{D}_i=\{\mathbf{X},\mathbf{y}\}^{m}$ and noisy data samples $\mathcal{D'}_i =\{\mathbf{X'},\mathbf{Y'}\}^{M}$, representing predictions for different days ahead, where $m \ll M$. Here, $\mathbf{X} \in \mathcal{X}$ and $\mathbf{y} \in \mathcal{Y}$ denote the image data of historical data in the clean domain and their corresponding clean labels, while $\mathbf{X'} \in \mathcal{X'}$ and $\mathbf{Y'} \in \mathcal{Y'}$ denote the image data of historical data and prediction horizon in the noisy domain. (See Fig.\ref{XY})

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\textwidth,height=0.6\textwidth]{metamodel.pdf}}
\caption{The label correction network (LCN) architecture $g_{\boldsymbol{\alpha}}\left(\mathbf{X}^{\prime}, \mathbf{Y}^{\prime}\right)$, the inputs of which are noisy examples $\{\mathbf{X'},\mathbf{Y'}\}$. The output of this \emph{meta model} is the corrected label $\mathbf{y}^{c}$.}
\label{metamodel}
\end{figure}

First, when fixing a task $\mathcal{T}i$, we explain here on our label correction method based on bi-level optimization. In order to exploit more information of prediction horizon and historical data without interference of noisy label, we attempt to utilize \emph{meta model} to generate corrected labels with $\{\mathbf{X'},\mathbf{Y'}\}$. The structure of \emph{meta model} (see Fig.~\ref{metamodel}) is composed of two branch neural networks and a generation module, which is similar to Pseudo‐Siamese network \citep{jiang2021pseudo} with smaller size. Simultaneously, the parameters of \emph{meta model} are thought of as \emph{meta-knowledge} so that the \emph{meta model} LCN is parameterized as a function with \emph{meta-knowledge} $\boldsymbol{\alpha}$, denoted as $g_{\boldsymbol{\alpha}}\left(\mathbf{X}^{\prime}, \mathbf{Y}^{\prime}\right)$. Moreover, the \emph{main models} are used for prediction after training and are independent for each task $\mathcal{T}i$, with task-specific parameters $\mathbf{w}_{i}$ using $D_{i}^{\prime}$, which are constructed as $f_{\mathbf{w}_{i}}$. Therefore, the goal of task $\mathcal{T}i$ is to find a good \emph{meta-knowledge} $\boldsymbol{\alpha}$ to generalize across all tasks, including new tasks, and to learn task-specific parameters $\mathbf{w}_{i}$ to minimize the population loss.

Next, when considering the multi-task case, the entire training process can be described as shown in Fig.~\ref{MLC}. \textcolor{red}{\textcircled{1}} We select noisy and clean examples from tasks $\mathcal{T}_i \sim P(\mathcal{T}), i = 1\cdots N $ and feed the noisy example with the pair $(\mathbf{X'},\mathbf{Y'})$ to the LCN to obtain its corrected label. \textcolor{red}{\textcircled{2}} We use the noisy data $\mathbf{X'}$ as input to the classifier under the current task to gain the corresponding prediction. \textcolor{red}{\textcircled{3}} We update the parameters of the classifier based on the loss with the corrected label and predicted label. \textcolor{red}{\textcircled{4}} We feed the clean example pair $(\mathbf{X},\mathbf{y})$ into the new classifier and compute the classification loss. \textcolor{red}{\textcircled{5}} We repeat the aforementioned procedure for each task and obtain the respective classification loss. Then, we calculate the total classification loss and apply the gradient of the total loss to update the parameters of the LCN. Moreover, bi-level optimization is utilized to link the two networks, which is formulated as follows:
\begin{equation}
\begin{aligned}
& \underset{\boldsymbol{\alpha}}{\min } \sum_{i=1}^N \mathbb{E}_{(\mathbf{X}, \mathbf{y}) \in D_{i}} \ell\left(\mathbf{y}, f_{\mathbf{w}_{i}^*(\boldsymbol{\alpha})}(\mathbf{X})\right) \\
 \text { s.t. } \quad & \mathbf{w}_{i}^*(\boldsymbol{\alpha})=\underset{\mathbf{w}_{i}}{\arg \min } \mathbb{E}_{\left(\mathbf{X}^{\prime}, \mathbf{Y}^{\prime}\right) \in D_{i}^{\prime}}\ell\left(g_{\boldsymbol{\alpha}}\left(\mathbf{X}^{\prime}, \mathbf{Y}^{\prime}\right), f_{\mathbf{w}_{i}}(\mathbf{X}^{\prime})\right),
\end{aligned}
\label{bilevel}
\end{equation}
where $N$ is the number of tasks and $\ell(\cdot)$ is cross entropy loss for classification. In this bi-level optimization problem, whenever \emph{meta-knowledge} $\boldsymbol{\alpha}$ update, the optimal $\mathbf{w}_{i}^*$ with each task $\mathcal{T}_i$ is required.
\begin{figure}[htbp]
\centerline{\includegraphics[width=1\linewidth]{MLC.pdf}}
\caption{Block diagram of the proposed MMLC method. Here, we only show three tasks. Each task has an independent \emph{main model} $f_{w_i}$ with parameters $w_i, (i=1,2,3)$ and a common \emph{meta model} (see Fig.~\ref{metamodel}) with parameters $\alpha$. In each task $\mathcal{T}_i \sim P(\mathcal{T})$, $(\mathbf{X}^{'},Y^{'})$ and $(\mathbf{X},\mathbf{y})$ are from noisy and clean samples, respectively. }
\label{MLC}
\end{figure}

\subsubsection{\textbf{Gradient-based bi-level optimization}}\label{gtadient}

In bi-level optimization, it's necessary to update the parameters in \emph{main model} and \emph{meta model} with two loops. There have been a variety of methods to alleviate the issue of this two-loop gradient optimization. 

For the lower-level (inner-loop)\footnote{In bi-level framework, we called the upper-level and lower-level, which correspond to the outer-loop and inner-loop in the meta learning.} gradient updating rule, various methods can be utilized to obtain the optimal $\mathbf{w}_{i}^*$ in each task $\mathcal{T}_i$ for a given $\boldsymbol{\alpha}$, such as one step \citep{zheng2021meta} or multiple steps \citep{finn2017model} of SGD approximation, projected gradient descent iterations \citep{liu2021towards}, and so on. In our \emph{main model}, we select K-step SGD approximation to obtain the optimal main model for each task $\mathcal{T}_i$ given a $\boldsymbol{\alpha}$. Furthermore, we only use the last step of the SGD update in each task of the \emph{main model} (see Eq.~\eqref{mainopt}) for updating the \emph{meta-knowledge} $\boldsymbol{\alpha}$. 
\begin{equation}
\mathbf{w}_{i}^*(\boldsymbol{\alpha}) \approx \mathbf{w}^{\prime}_{i}(\boldsymbol{\alpha})=\mathbf{w}_{i}-\eta \nabla_{\mathbf{w}_{i}} \mathcal{L}_{D^{\prime}_{i}}(\boldsymbol{\alpha}, \mathbf{w}_{i}).
\label{mainopt}
\end{equation}
Here, $\mathbf{w}^{\prime}_{i}(\boldsymbol{\alpha})\triangleq\mathbf{w}_{i,K}(\boldsymbol{\alpha})$ is the parameters after the last step update in task $\mathcal{T}_i$.  Also, $\nabla_{\mathbf{w}_{i}} \mathcal{L}_{D^{\prime}_{i}}(\boldsymbol{\alpha},\mathbf{w}_{i})\triangleq\nabla_{\mathbf{w}_{i,K-1}}\mathbb{E}_{\left(\mathbf{X}^{\prime}, \mathbf{Y}^{\prime}\right) \in D_{i}^{\prime}}\ell\left(g_{\boldsymbol{\alpha}}\left(\mathbf{X}^{\prime}, \mathbf{Y}^{\prime}\right), f_{\mathbf{w}_{i,K-1}}(\mathbf{X}^{\prime})\right)$ is the lower-level loss function and $\mathbf{w}_{i}\triangleq\mathbf{w}_{i,K-1}$ is the parameters after the penultimate update. The learning rate $\eta$ in each \emph{main model} is assumed to be the same. 

\begin{algorithm}
\caption{\emph{Main Model} Gradient Computation}\label{inneral}
\KwIn{Task $\mathcal{T}_i$, \emph{meta-knowledge} $\boldsymbol{\alpha}$, learning rate $\eta$, noisy dataset $\mathcal{D'}_i =\{\mathbf{X'},\mathbf{Y'}\}^{M}$,\emph{meta model} $g_{\boldsymbol{\alpha}}$ }
\KwOut{\emph{Main model} parameters $\mathbf{w}^{\prime}_{i}(\boldsymbol{\alpha}) = \mathbf{w}_{i,K}$ }
Initialize \emph{main model} parameters $\mathbf{w}_{i,0}$.\\
\For{$k=0$ to $K-1$}{
$\{\mathbf{X'},\mathbf{Y'}\} \leftarrow \text{SampleMiniBatch}(\mathcal{D'}_i,n)$.\\
\text{Update main model parameters} $\mathbf{w}_{i,k+1} = \mathbf{w}_{i,k} - \eta \nabla_{\mathbf{w}_{i,k}} \mathcal{L}_{D^{\prime}_{i}}(\boldsymbol{\alpha}, \mathbf{w}_{i,k}) $\\
}
\end{algorithm}

For the upper-level (outer-loop), it is inevitable to calculate $\frac{\partial \mathbf{w}_{i}^*(\boldsymbol{\alpha})}{\partial \boldsymbol{\alpha}}$ that involves second-order gradient computation so that there exist many types of approaches to calculate such as explicit gradient update, implicit function update, etc. \citep{chen2022gradient}. We select method called meta-parameter with K-step SGD from main-parameters in \citep{zheng2021meta} to update our \emph{meta model}. But, different from it, we calculate the hessian matrix $\frac{\partial^2}{\partial \mathbf{w_i} \partial \mathbf{w_i}} \mathcal{L}_{D_{i}^{\prime}}(\boldsymbol{\alpha}, \mathbf{w_i})$ with the Taylor expansion method mentioned in \citep{liu2018darts} to avoid large resource demand. Denote the meta loss as $\sum_{i=1}^N\mathcal{L}_{D_{i}}\left(\mathbf{w}_{i}^{*}(\boldsymbol{\alpha})\right)\triangleq\sum_{i=1}^N \mathbb{E}_{(\mathbf{X}, y) \in D_{i}} \ell\left(y, f_{\mathbf{w}_{i}^{*}(\boldsymbol{\alpha})}(\mathbf{X})\right)$. Then, we have
\begin{equation}
\begin{aligned}
\min _{\boldsymbol{\alpha}} \sum_{i=1}^N\mathcal{L}_{D_{i}}\left(\mathbf{w}_{i}^{*}(\boldsymbol{\alpha})\right) &\approx \sum_{i=1}^N\mathcal{L}_{D_{i}}\left(\mathbf{w}^{\prime}_{i}(\boldsymbol{\alpha})\right)\\
&=\sum_{i=1}^N\mathcal{L}_{D_{i}}\left(\mathbf{w}_{i}-\eta \nabla_{\mathbf{w}_{i}} \mathcal{L}_{D^{\prime}_{i}}(\boldsymbol{\alpha}, \mathbf{w}_{i})\right).
\end{aligned}
\label{metaopt}
\end{equation}
Subsequently, the update rule of \emph{meta-knowledge} $\boldsymbol{\alpha}$ can be written as follows:
\begin{equation}
\boldsymbol{\alpha}=\boldsymbol{\alpha}-\mu \nabla_{\boldsymbol{\alpha}}\sum_{i=1}^N \mathcal{L}_{D_{i}}\left(\mathbf{w}^{\prime}_{i}(\boldsymbol{\alpha})\right).
\label{metaknowledge}
\end{equation}
Then, the meta-parameter gradient from previous $T$ step in upper-level is shown as\footnote{Because of linearity, $\sum_{i=1}^N$ and $\nabla_{\boldsymbol{\alpha}}$ can be switched.}:
\begin{equation}
\begin{split}
\frac{\partial \mathcal{L}_{D_{i}}\left(\mathbf{w}^{\prime}_{i}(\boldsymbol{\alpha})\right)}{\partial \alpha} 
=& g_{\mathbf{w}_{i}^{\prime}}(I-\eta \nabla_{\mathbf{w}_{i},\mathbf{w}_{i}}\mathcal{L}_{D^{\prime}_{i}}(\boldsymbol{\alpha}, \mathbf{w}_{i})) \frac{g_{\mathbf{w}_{i}^{\top}}}{\left\|g_{\mathbf{w}_{i}}\right\|^2}\frac{\partial \mathcal{L}_{D_{i}}(\mathbf{w}_{i})}{\partial\boldsymbol{\alpha}}\\
&-\eta \nabla_{\boldsymbol{\alpha}}\left(\nabla_{\mathbf{w}_{i}}^{\top}\mathcal{L}_{D^{\prime}_{i}}(\boldsymbol{\alpha}, \mathbf{w}_{i})\nabla_{\mathbf{w}_{i}^{\prime}} \mathcal{L}_{D_{i}}\left(\mathbf{w}_{i}^{\prime}\right)\right).
%g_{\mathbf{w}_{i}^{\prime}}\nabla_{\boldsymbol{\alpha},\mathbf{w}_{i}}\mathcal{L}_{D_{i}^{\prime}}\left(\mathbf{w}_{i}^{\prime}(\boldsymbol{\alpha})\right)
\end{split}
\label{metagrad}
\end{equation}
The $g_{\mathbf{w}_{i}^{\prime}}\nabla_{\mathbf{w}_{i},\mathbf{w}_{i}}\mathcal{L}_{D^{\prime}_{i}}(\boldsymbol{\alpha}, \mathbf{w}_{i})$ can be calculated as
\begin{equation}
\frac{\nabla_{\mathbf{w}_{i}}\mathcal{L}_{D_{i}^{\prime}}(\boldsymbol{\alpha}, \mathbf{w}_{i}+\epsilon g_{\mathbf{w}_{i}^{\prime}}) - \nabla_{\mathbf{w}_{i}}\mathcal{L}_{D_{i}^{\prime}}(\boldsymbol{\alpha}, \mathbf{w}_{i}-\epsilon g_{\mathbf{w}_{i}^{\prime}})}{2 \epsilon}, 
\label{hessian}
\end{equation}
where $g_{\mathbf{w}_{i}^{\prime}} =\frac{\partial L_{D_i}\left(y, f_{\mathbf{w}_{i, K}\left(\boldsymbol{\alpha}\right)}(\mathbf{X})\right)}{\partial \mathbf{w}_{i,K}\left(\boldsymbol{\alpha}\right)}$ represents the gradient of training loss.

\SetKwInOut{KwIn}{Requrie}
\begin{algorithm}
\caption{\textbf{MMLC} Gradient Computation}\label{outeral}
\KwIn{$P(\mathcal{T})$: distribution over tasks}
\KwIn{$\mu, \eta$: learning rate}
Initialize \emph{main model} parameters $\mathbf{w}_{i,0}$ and \emph{meta model} parameters $\boldsymbol{\alpha}$.\\
\While{not done}{
Sample batch of tasks $\mathcal{T}_i \sim P(\mathcal{T})$;\\
\For{all $\mathcal{T}_{i}$}{
\text{Update \emph{main model} parameters $\mathbf{w}_{i}$ by \textbf{Algorithm }\ref{inneral}}\\
}
\text{Update \emph{meta model} parameters $\boldsymbol{\alpha}$ by Eq.~\eqref{metaknowledge}}
}
\end{algorithm}

\section{\textbf{Experiment}}
\subsection{\textbf{Datasets and Setup}}\label{datasets}
\textbf{Datasets.} Based on recent research \citep{li2022selective} and \citep{bhandari2022predicting}, we also evaluate our method on three different stocks: XOM stock from \textbf{KDD17} \citep{zhang2017stock} public benchmark, American stock index S\&P500 from Yahoo! Finance and Chinese stock index Shangzheng50 (SZ50) from JoinQuant. XOM contains stock from 2007 to 2017, S\&P500 spans from 2010 to 2020, and SZ50 is from 2010 to 2022. Although the time spans are different, the training set and the test set are divided in the same proportions.

\textbf{Baseline.} We choose the following state-of-the-art methods for comparison. Here, the settings of \textbf{Resnet32}\footnote{https://github.com/microsoft/MLC} and \textbf{CNN}\footnote{https://github.com/IliaOzhmegov/TradingNeuralNetwork} are the same as the  classifier networks in our proposed model for label correction.

\subsection{\textbf{Experiment results}}\label{results}
\subsubsection{\textbf{Effects of image transformation method}}
We compare our proposed transformation method with traditional methods using different models as classifiers in the inner loop. The models we use are popular image classification algorithms. Since we only have a small amount of data with clean labels, we use the same training data and testing data with clean labels for all experiments to obtain accuracy, precision, and F1 score. The experimental settings are consistent across all experiments.

\begin{table}[ht]
\caption{Performance comparison of \textbf{Resnet32}and \textbf{CNN} on stock dataset}
\begin{center}
\scalebox{0.8}{
\begin{tabular}{*{6}{c}}
\hline
\multirow{2}{*}{Stock ID}&\multicolumn{2}{c}{\multirow{2}{*}{Methods}}& \multicolumn{3}{c}{Error Measures}\\
\cline{4-6}

&\multicolumn{2}{c}{} &
\textbf{\textit{Accuracy}}& \textbf{\textit{Precision}}& \textbf{\textit{F1-score}}  \\
\hline

\multirow{8}{*}{XOM}&\multirow{4}{*}{Resnet32} & GAF & 41.21 & 40.07 &40.57\\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} &RP & 43.82	& 42.72	& 43.16\\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} &GAFRP &44.90	&44.98	&44.76\\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} &SGAF &\textbf{47.29}	&\textbf{46.16}	& \textbf{46.66}\\
\cline{2-6}

\multirow{8}{*}{}&\multirow{4}{*}{CNN} &GAF  &43.60	&42.34	&42.85\\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} &RP &43.60	&38.91	&40.84\\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} &GAFRP  &47.07	&\textbf{44.23}	& \textbf{45.41}\\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} &SGAF &\textbf{48.81}	&43.49	&45.18\\
\hline

\multirow{8}{*}{S\&P500}&\multirow{4}{*}{Resnet32} &GAF & 39.47 & 33.49  &34.76 \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} & RP & 42.11 	& 39.71	&38.09 \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} & GAFRP & 43.86	& \textbf{41.93} 	& 40.23 \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} & SGAF & \textbf{45.83}	& 41.15 & \textbf{41.37}  \\
\cline{2-6}

\multirow{8}{*}{}&\multirow{4}{*}{CNN} & GAF & 39.69 	& 37.79	& 36.14 \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{ } & RP & 37.94 	& 33.81 	& 34.50 \\

\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{ } & GAFRP & \textbf{45.83} 	& \textbf{40.79} 	& \textbf{40.58}   \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{ } & SGAF & 44.08 	& 38.40 	& 39.87 \\
\hline

\multirow{8}{*}{SZ50}&\multirow{4}{*}{Resnet32} &GAF & 40.58 &41.68  & 40.90\\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} & RP &37.35  & 38.22 & 37.69\\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} & GAFRP & 42.11 &42.91  & 42.28 \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{} & SGAF & \textbf{42.78} & \textbf{42.59} & \textbf{42.56}\\
\cline{2-6}

\multirow{8}{*}{}&\multirow{4}{*}{CNN} & GAF &38.03  &37.21 &37.58 \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{ } & RP &41.09  &38.98  &39.81 \\

\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{ } & GAFRP &\textbf{43.12}  &39.37  &40.19 \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{4}{*}{ } & SGAF &42.62  &\textbf{41.90}  &\textbf{41.93} \\
\hline
\end{tabular}}
\label{tab1}
\end{center}
\end{table}

From Table \ref{tab1}, we can find that the transformation methods we proposed have better performances for each stock, among which the overall performance of the SGAF method outperforms the others. Nevertheless, even though different optimal visualization methods exist for distinct stock data, they all demonstrate that the trend information in the image is more useful for three-class classification prediction.

\subsubsection{\textbf{ MMLC on Stock movement prediction with single task}}
To better understand the performance of the \textbf{MMLC} model, we conducted a comparative experiment against innovative methods such as \textbf{MLC} \citep{zheng2021meta} and \textbf{IAPTT-GM} \citep{liu2021towards}, as well as a baseline model, \textbf{Resnet32}, which serves as the classifier in the inner loop of both \textbf{MMLC} and \textbf{MLC}. It should be noted that the inputs of \textbf{MLC}, \textbf{IAPTT-GM}, and \textbf{Resnet32} include noisy labels, making them supervised learning models. In contrast, \textbf{MMLC} does not require noisy labels, but rather images of the prediction horizon, making it more similar to a semi-supervised learning model. As state-of-the-art methods are typically designed for a single task, we present the results of a single task with a 10-day prediction in this comparative experiment. Additionally, to demonstrate the advantage of correcting noisy labels, the training data for \textbf{Resnet32} focuses only on noisy labels, which differs from the data used in Table \ref{tab1}.

\begin{table}[ht]
\caption{Test accuracy, precision and F1-score of single task of 10-day prediction on stock dataset}
\begin{center}
\scalebox{0.8}{
\begin{tabular}{*{6}{c}}
\hline
\multirow{2}{*}{Stock ID}&\multicolumn{2}{c}{\multirow{2}{*}{Methods}}& \multicolumn{3}{c}{Error Measures}\\
\cline{4-6}

&\multicolumn{2}{c}{} &
\textbf{\textit{Accuracy}}& \textbf{\textit{Precision}}& \textbf{\textit{F1-score}}  \\
\hline

\multirow{8}{*}{XOM}&\multirow{3}{*}{GAFRP} & Resnet32 & 34.71 &42.99  &36.67  \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & IAPTT-GM &36.23  &38.56  &34.77   \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MLC & 56.40  &40.68 & 43.46  \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MMLC &\textbf{57.70}  &\textbf{50.48} &\textbf{44.33}  \\
\cline{2-6}

\multirow{8}{*}{}&\multirow{3}{*}{SGAF} & Resnet32 &36.62 &  40.04 &37.46  \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & IAPTT-GM &38.40   & 39.80 &37.10    \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MLC &54.23  &39.92 & 43.61   \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MMLC &\textbf{57.27} & \textbf{41.05} &\textbf{45.28}   \\
\hline
\multirow{8}{*}{S\&P500}&\multirow{3}{*}{GAFRP} & Resnet32 & 37.50   &41.56 & 38.23  \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & IAPTT-GM &44.30   &19.62  &27.20    \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MLC &44.52  &32.52  &32.16    \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MMLC &\textbf{46.27} &\textbf{45.09}  &\textbf{40.55}   \\
\cline{2-6}

\multirow{8}{*}{}&\multirow{3}{*}{SGAF} & Resnet32 &39.69 & 43.72  &40.67  \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & IAPTT-GM &42.98 & 44.49  &42.57   \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MLC & 45.18 & 33.24 &35.91  \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MMLC & \textbf{53.07}  & \textbf{49.11} & \textbf{46.96}  \\
\hline
\multirow{8}{*}{SZ50}&\multirow{3}{*}{GAFRP} & Resnet32 & 34.80 & 39.70   & 35.87 \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & IAPTT-GM & \textbf{53.31}  & 28.42   & 37.08   \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MLC & 45.67  & 37.19  & 39.70    \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MMLC & 52.80 &\textbf{45.11 }  &\textbf{41.35 }   \\
\cline{2-6}

\multirow{8}{*}{}&\multirow{3}{*}{SGAF} & Resnet32 & 37.69  & 41.90 &39.10  \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & IAPTT-GM & 25.30  & 37.40  & 22.81  \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MLC & 49.41  & 37.27  &39.07 \\
\cline{3-6}

\multirow{8}{*}{}&\multirow{3}{*}{} & MMLC & \textbf{51.78}  & \textbf{42.71} & \textbf{41.18 }  \\
\hline
\end{tabular}}
\label{tab2}
\end{center}
\end{table}

From the results in Table \ref{tab2}, we can observe that as a whole, the MMLC approach we propose has higher accuracy, precision, and F1-score compared to the other methods. In particular, the accuracy of the MMLC approach with the SGAF visualization method exceeds 50\% for all three stocks. Even with the GAF visualization method, the lowest accuracy rate for all stocks using our MMLC method was 46.27\%. Furthermore, under both proposed visualization methods, all precision and F1-score metrics exceed 40\%.

\begin{figure}[htbp]
\begin{minipage}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{SZ50CMMSLC.jpg} 
\end{minipage}%
\begin{minipage}[t]{0.5\textwidth}
    \includegraphics[width=\linewidth]{SZ50CM.jpg}
\end{minipage} 
\caption{The confusion matrix of SZ50 index stock price prediction with GAF visualization methods. Left: IAPTT-GM method. Right: MMLC method.}
\label{SZ50CMcompare}
\end{figure}

However, for the SZ50 stock, while the IAPTT-GM method achieves higher accuracy, its F1-score and precision are quite low. According to the confusion matrix of the IAPTT-GM method (shown in Fig.~\ref{SZ50CMcompare}), all labels have been estimated to have the value 1. On the other hand, our MMLC method can differentiate between the stationary class (label 1) and the two other classes (fall and rise, corresponding to labels 0 and 2, respectively). To address this issue, we analyze the distribution of classes in clean data samples from both the meta-training and test datasets. As shown in Table \ref{tab3}, we acknowledge that the stationary class (label 1) has the highest proportion for all stocks, both in the clean and test datasets. As a result, it is easier for the model to capture the features of the stationary class. When the stationary class is excluded, i.e., only circumstances where the models predict that the stock price will go up or down, differentiating between the rising class (label 2) and the fall class (label 0) becomes much more accurate. Stock datasets in the three-class problem will inevitably result in data imbalance, with the stationary class data accounting for the largest proportion, a topic that has been discussed in several recent publications such as \citep{shabani2023augmented}. This discovery will help improve the direction of imbalanced data optimization in future research.

\begin{table*}[ht]
\caption{The distribution of classes of selected stock in the clean dataset of meta training and test dataset on single task of 10-day prediction.$^{\mathrm{a}}$}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{*{7}{c}}
  \toprule
  \multirow{2}*{Stock ID } & \multicolumn{3}{c}{Clean Dataset} & \multicolumn{3}{c}{Test Dataset }\\
  \cmidrule(lr){2-4}\cmidrule(lr){5-7}
  &Class 0  &Class 1 &Class 2 &Class 0 &Class 1 &Class 2 \\
  \midrule
  \multirow{1}{*}{XOM} &97 &260 &105 &100 &270 &91  \\
  \multirow{1}{*}{(proportion)} &(20.99\%)   &(56.28\%) &(22.72\%)  &(21.69\%)   &(58.57\%)   &(18.74\%)   \\
  \midrule
  \multirow{1}{*}{S\&P500} &40 & 276 & 141  & 102 & 202 & 152  \\
  \multirow{1}{*}{(proportion)} &(8.75\%)  &(60.39\%) &(30.86\%) &(22.37\%)  &(44.30\%) &(33.33\%)  \\
  \midrule
  \multirow{1}{*}{SZ50} &137 &294 &159  &148 &314 & 127    \\
  \multirow{1}{*}{(proportion)} &(23.22\%)   &(49.83\%)  &(26.95\%)  &(25.13\%) &(53.31\%) & (21.56\%)    \\
  \bottomrule
  \multicolumn{7}{l}{$^{\mathrm{a}}$Class 0: stock price falls. Class 1: stock price remains stable. Class 2: stock price rises.}\\
\end{tabular}}
\label{tab3}
\end{center}
\end{table*}

\subsubsection{\textbf{ MMLC on Stock movement prediction with multi tasks}}
Multi-task learning in our algorithm can assist us in predicting the stock price for multiple days simultaneously. Hence, we assign tasks with 10-day, 13-day, and 15-day predictions to achieve long-term predictions. Additionally, since our method is model-agnostic, we can employ a variety of current classification models in the inner loop. In our experiments, we employ \textbf{Resnet32} and \textbf{CNN} as the classifier networks. The experimental results are shown in Tables \ref{tab4} and \ref{tab5}.

\begin{table}[htbp]
\caption{Test accuracy, precision and F1-score of multiple task on stock dataset with \textbf{Resnet32}}
\begin{center}
\scalebox{0.7}{
\begin{tabular}{*{6}{c}}
\hline
\multirow{2}{*}{Stock ID}&\multicolumn{2}{c}{\multirow{2}{*}{Methods}}& \multicolumn{3}{c}{Error Measures}\\
\cline{4-6}

&\multicolumn{2}{c}{} &
\textbf{\textit{Accuracy}}& \textbf{\textit{Precision}}& \textbf{\textit{F1-score}}  \\
\hline

\multirow{6}{*}{XOM}&\multirow{3}{*}{GAFRP+MMLC} & \textbf{10-day} &54.73     &43.44    &45.23     \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{13-day} &55.82     &42.72    &45.91      \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day} &\textbf{58.46 }   &\textbf{44.71 }   &\textbf{47.82}      \\
\cline{2-6}

\multirow{6}{*}{}&\multirow{3}{*}{SGAF+MMLC} &\textbf{10-day}  &58.46    &\textbf{49.28 }   &46.36    \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} &\textbf{13-day} &59.34    &46.88    &46.34    \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day}  &\textbf{60.66 }   &40.65   &\textbf{47.29 }  \\
\hline
\multirow{6}{*}{S\&P500}&\multirow{3}{*}{GAFRP+MMLC} & \textbf{10-day}  &44.00    &40.53    &39.34  \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{13-day}  &\textbf{73.33 }   &\textbf{56.37}    &\textbf{63.74 }    \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day}  &73.11    &54.87    &62.69      \\
\cline{2-6}

\multirow{6}{*}{}&\multirow{3}{*}{SGAF+MMLC} &\textbf{10-day}  &47.11    &43.98    &41.11    \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} &\textbf{13-day} &71.78    &\textbf{60.06}    &\textbf{64.33 }   \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day}  &\textbf{72.00 }   &59.33    &63.49   \\
\hline
\multirow{6}{*}{SZ50}&\multirow{3}{*}{GAFRP+MMLC} & \textbf{10-day}  &50.77     &40.47    &41.29    \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{13-day} &53.69     &\textbf{43.66}    &\textbf{44.06}     \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day}  &\textbf{54.55}     &39.31   &43.36        \\
\cline{2-6}

\multirow{6}{*}{}&\multirow{3}{*}{SGAF+MMLC} & \textbf{10-day} &51.46     &44.64    &43.09     \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} &\textbf{13-day}  &50.09     &42.04    &43.72        \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day}  &\textbf{52.66}     &\textbf{45.12}    &\textbf{46.31}      \\
\hline
\end{tabular}}
\label{tab4}
\end{center}
\end{table}

Compared to Table \ref{tab2}, which presents the results of single-task 10-day predictions using the Resnet32 classifier, we discover that our algorithm can also perform effectively for 10-day forecasting with the multi-task loss. For instance, using the SGAF transformation method, the accuracy for XOM stock price, S\&P500 stock price, and SZ50 stock price under multi-task learning can reach 58.46\%, 47.11\%, and 51.46\%, respectively. With single-task learning, the accuracy can reach 57.27\%, 53.07\%, and 51.78\%. Additionally, all of the precision and F1 scores of the selected stocks are above 41\%, and some of them may even be higher than the value obtained using single-task learning.

\begin{table}[htbp]
\caption{Test accuracy, precision and F1-score of multiple task on stock dataset with \textbf{CNN}}
\begin{center}
\scalebox{0.7}{
\begin{tabular}{*{6}{c}}
\hline
\multirow{2}{*}{Stock ID}&\multicolumn{2}{c}{\multirow{2}{*}{Methods}}& \multicolumn{3}{c}{Error Measures}\\
\cline{4-6}

&\multicolumn{2}{c}{} &
\textbf{\textit{Accuracy}}& \textbf{\textit{Precision}}& \textbf{\textit{F1-score}}  \\
\hline

\multirow{6}{*}{XOM}&\multirow{3}{*}{GAFRP+MMLC} & \textbf{10-day} &57.36    &56.57    &44.20    \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{13-day} &59.78    &40.80   &46.28    \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day} &\textbf{61.10}    &\textbf{59.91 }   &\textbf{48.13}    \\
\cline{2-6}

\multirow{6}{*}{}&\multirow{3}{*}{SGAF+MMLC} &\textbf{10-day}  &\textbf{57.80}     &\textbf{54.99}   &46.57   \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} &\textbf{13-day} &57.14    &54.18   &48.48    \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day}  &56.70     &54.61    &\textbf{51.13}  \\
\hline
\multirow{6}{*}{S\&P500}&\multirow{3}{*}{GAFRP+MMLC} & \textbf{10-day}  &45.33    &35.61    &30.44  \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{13-day} &\textbf{75.33}    &\textbf{56.75}    &\textbf{64.74}     \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day}  &74.22    &55.09    &63.24      \\
\cline{2-6}

\multirow{6}{*}{}&\multirow{3}{*}{SGAF+MMLC} &\textbf{10-day}  &52.22     &40.06    &45.21     \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} &\textbf{13-day} &\textbf{75.78}     &\textbf{75.45 }   &\textbf{65.77}    \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day} &73.56     &60.71    &63.68    \\
\hline
\multirow{6}{*}{SZ50}&\multirow{3}{*}{GAFRP+MMLC} & \textbf{10-day}  &54.72     &\textbf{53.69}    &42.44   \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{13-day}  &54.89     &40.81    &41.65      \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day}  &\textbf{56.43}     &46.34    &\textbf{43.90}       \\
\cline{2-6}

\multirow{6}{*}{}&\multirow{3}{*}{SGAF+MMLC} & \textbf{10-day} & 51.97    &45.37    &42.13     \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} &\textbf{13-day} &\textbf{53.34}     &\textbf{48.44}    &\textbf{45.70}       \\
\cline{3-6}

\multirow{6}{*}{}&\multirow{3}{*}{} & \textbf{15-day}  &52.66     &45.25    &44.39     \\
\hline
\end{tabular}}
\label{tab5}
\end{center}
\end{table}

Table \ref{tab5} presents the prediction results obtained with the CNN classifier, demonstrating the model-agnostic nature of our algorithm. For example, for XOM stock, when using the GAFRP transformation method and the Resnet32 classifier, we achieve accuracies of 54.73\%, 55.82\%, and 58.46\% for 10-day, 13-day, and 15-day predictions, respectively. However, when replacing Resnet32 with CNN, we are able to achieve even higher accuracies of 57.36\%, 59.78\%, and 61.10\% for XOM stock, respectively. For 10-day, 13-day, and 15-day F1-scores of XOM stock with GAFRP transformation method, Resnet can separately reach 45.23\%, 45.91\% and 47.82\%, and CNN can separately get 44.20\%, 46.28\% and 48.13\% as well. Therefore, any classification model can be substituted for the classifier in the inner loop without making an impact. 


\section{Conclusion}

In this study, we investigate a novel research issue pertaining to financial time series, which aims to effectively correct labels with adaptive labeling in order to improve the performance of prediction models for the three-class time-series classification of stocks. To achieve this, we first propose two distinct methods for converting time-series data into images with features of the trend, leveraging more information from image patterns. We then verify the validity of these approaches in a general image classification setting. After that, we attempt to substitute labels with patterns in the prediction horizon in order to gather additional information and achieve adaptive labeling. Under bi-level optimization, we develop an architecture for correcting noisy labels with a limited number of clean labels, where the label corrector model takes the visualization image of historical data and prediction horizon as inputs, without explicit noisy labels, setting it apart from existing label corrector models. Additionally, we treat the corrector model's parameters as meta-knowledge that is generalized across multiple tasks, thanks to the advantages of meta-learning and multi-task learning. With this enhancement, we can achieve multi-step predictions simultaneously. Through various comparative experiments, we demonstrate the superior performance of our \textbf{MMLC} model and the model-agnostic nature of our bi-level framework. We also identify the issue of imbalanced datasets as a significant challenge, and our future work will focus on further analysis of this issue, as well as theoretical problems related to bi-level optimization.


\section*{Acknowledgment}
We would like to thank Yubin Lu and Yufu Lan for the helpful discussions. This work was supported by National Key Research and Development Program of China 2021ZD0201300, National Natural Science Foundation of China (NSFC) 12141107 and Fundamental Research Funds for the Central Universities 5003011053.  

\appendix

\section{Table of acronyms}
\label{acronyms}
For convenient guidance, Tabel \ref{tab6} provides definitions for all abbreviations mentioned throughout the paper.

\setcounter{table}{0}
\begin{table}[ht]
\caption{Table of abbreviations}
\begin{center}
\scalebox{0.8}{
\begin{tabular}{*{6}{c}}
\hline
\multirow{1}{*}{Abbreviation}&\multirow{1}{*}{Explanation}\\
\cline{1-2}
\specialrule{0em}{1pt}{1pt}
\multicolumn{2}{c}{\textbf{Data preprocessing methods}}
 \\
\cline{1-2}
GAF & Gramian Angular Field \citep{WangGAF} \\
\cline{1-2}
RP & Recurrence Plot \citep{eckmann1995recurrence} \\
\cline{1-2}
SGAF & 
Sign function multiplied with Gramian Matrix \\
\cline{1-2}
GAFRP & Fusion of GAF and modified RP\\
\cline{1-2}
\specialrule{0em}{1pt}{1pt}
\multicolumn{2}{c}{\textbf{Experiments}}
 \\
\cline{1-2}
MLC & Meta Label Correction \citep{zheng2021meta}\\
\cline{1-2}
IAPTT-GM &  Initialization Auxiliary and Pessimistic \\ & Trajectory Truncated Gradient Method \citep{liu2021towards} \\
\cline{1-2}
MMLC &  Multi-task Meta  Label Correction \\
\cline{1-2}
%LCN & Label correction network \\
\cline{1-2}
%CPNs & Classification prediction network \\
\hline
\end{tabular}}
\label{tab6}
\end{center}
\end{table}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
\bibliographystyle{elsarticle-num-names} 
\bibliography{references}


\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.
