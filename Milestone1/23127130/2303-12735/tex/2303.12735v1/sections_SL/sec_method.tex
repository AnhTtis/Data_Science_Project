\section{{\us}: SMoothed UnrollinG}
\label{sec: approach}

In this section, we tackle the above problems \textbf{(Q1)}--\textbf{(Q2)} by taking the unrolling characteristics of {\modl}  into the design of a RS-based robust MRI reconstruction. The proposed  novel integration of RS with {\modl} is termed 
% SMoothed UnrollingG 
{\textsc{\underline{Sm}oothed \underline{U}nrollin\underline{g}}}
(\textbf{\us}).

% In this section, we describe our proposed {\us} method, in which we integrate {\modl} \cite{aggarwal2018modl} with randomized smoothing \cite{cohen2019certified}, to solve the instability problems mentioned in Sec. \ref{sec: preliminaries}. We first explicitly describe the architecture of our {\us} method in Sec. \ref{sec: unrolling}. Later, we elaborate on our training scheme in Sec. \ref{sec: how to smoothing}.

\subsection{Solution to (\textbf{Q1}): RS at intermediate unrolled denoisers}
\label{sec: unrolling}

Recall from \textbf{Fig.\,\ref{fig: RS-E2E}} that the RS operation is applied to {\modl} in an end-to-end fashion. Yet, the vanilla {\modl} framework  consists of multiple unrolling steps, each of which is naturally dissected into a \ding{172} \textit{denoising block} (denoted by $\mathcal D$)  and a \ding{173} \textit{DC block} (denoted by $\mathcal D \mathcal C$).  Taking the above architecture into account,  {RS} can also be integrated with each intermediate unrolling step of {\modl} instead of  following {\ref{eq: denoised smoothing mri}}. This leads to two \textit{new}   smoothing architectures of {\modl}:

\noindent \textbf{(a)  {\usold}}: In this scheme, the RS operation is incorporated into  {\modl} at each unrolled step (\textit{i.e.}, $\mathrm{RS}(\mathcal D + \mathcal{DC}) $). Formally, at the $n$th step, we have
$\mathrm{RS}(\mathcal D + \mathcal{DC}) =\mathbb{E}_{\boldsymbol \nu \sim\mathcal{N}(\mathbf 0, \sigma^2\mathbf I)} [ {\mathbf{x}}_{n} (\mathbf x_{n-1} + \boldsymbol \nu) ]  $, where 
${\mathbf{x}}_{n} (\mathbf x_{n-1} + \boldsymbol \nu)$ denotes the output of the $n$th unrolling step given the input $\mathbf x_{n-1}$ with Gaussian random noise $\boldsymbol \nu$. \textbf{Fig.\,\ref{fig: model-arch}-(a)} provides a schematic overview of {\usold}.

\noindent \textbf{(b)  {\us}}: Different from {\usold},  {\us} only applies RS  to the denoising network, leading to $\mathrm{RS}(\mathcal D) $ at each unrolling step. However, this seemingly simple modification aligns with  a robustness certification technique, called `denoised smoothing' \cite{salman2020denoised}, where a smoothed denoiser prepended to a victim model is sufficient to achieve   provable     robustness for this model. Formally, at the $n$th unrolling step, we have
\begin{align}
    \mathrm{RS}(\mathcal D)  =  \mathbb{E}_{\boldsymbol \nu \sim\mathcal{N}(\mathbf 0, \sigma^2\mathbf I)} [ \mathcal D_{\btheta} ( \mathbf x_{n-1} + \boldsymbol \nu) ] \Def \mathbf{z}_n,
    \label{eq: SMUG_RS}
\end{align}
together with the standard DC step 
$
\mathbf x_{n+1} = \argmin_{\mathbf x} 
  \|\mathbf A \bx - \by \|^{2}_2 + \lambda \| \bx - \mathbf z_n \|_2^2
$. \textbf{Fig.\,\ref{fig: model-arch}-(b)} shows the architecture of {\us}. 


% Following the denoised smoothing \cite{salman2020denoised}, we use a custom-trained denoiser to make the base model, \textit{i.e.} {\modl}, robust to Gaussian noise before applying randomized smoothing to generate robustness against adversarial perturbations. But instead of introducing a new denoiser, we exploit the denoiser in {\modl} for model simplicity and overall time efficiency. Compared to appending a new denoiser, our approach of reusing the existing denoiser in {\modl} is capable of removing Gaussian noises in every unrolling step, which enhances the Gaussian noise robustness of the base model and makes it more potential for randomized smoothing.

% We then apply randomized smoothing \cite{cohen2019certified} to {\modl}. 
% We present two smoothing architectures here: 1) {\us} on the whole ({\us} ($\cD + \mathcal{DC}$)), and 2) {\us}. {\us} is proposed to integrate randomized smoothing with {\modl} naturally. The differences between these architectures and \ref{eq: denoised smoothing mri} mainly focus on where to add the Gaussian noises and where to take the majority vote, \textit{i.e.} estimate the expectation in MRI reconstruction. Their comparison can also be seen in Fig. \ref{fig: model-arch}.
% In \ref{eq: denoised smoothing mri}, the Gaussian noises $\epsilon$ are added on the input of the entire {\modl}, $\bx_0$, and the expectation \textit{w.r.t.} $\epsilon$ is estimated on the output of the entire {\modl}, $\bx_K$. In {\us}, however, the Gaussian noises $\epsilon$ are added on the input of \textit{every} unrolling step in {\modl}, $\bx_k$, and the expectations are estimated in \textit{every} unrolling step on the output of the whole unrolling step, \textit{i.e.} the output of the $\mathcal{DC}$, for {\us} on the whole, and the output of the denoiser for {\us}. 

\begin{figure}[htb]
% \vspace*{-3mm}
    \centering
    % \begin{subfigure}{.48\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{Figures/E2E_Smoothing.pdf}
    %     \caption{\footnotesize{\ref{eq: denoised smoothing mri}
    %     }}
    % \end{subfigure}
\begin{tabular}{c}
    \includegraphics[width=.45\textwidth]{Figures/SMUGv0.png}  \vspace*{-2mm}\\
   \footnotesize{(a) {\usold}  
        }\vspace*{2mm}\\
    \includegraphics[width=.45\textwidth]{Figures/SMUG.png} \vspace*{-2mm}\\
    \footnotesize{(b) \us}
\end{tabular}
    % \begin{subfigure}{.48\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{Figures/SMUGv0.pdf}
    %     \vspace*{-3mm}
    %     \caption{\footnotesize{{\usold}  
    %     }}
    % \end{subfigure}

    % \begin{subfigure}{.48\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{Figures/SMUG.pdf}
    %     \caption{\footnotesize{\us}}
    % \end{subfigure}
    \vspace*{-1em}
    \caption{\footnotesize{Architectures of smoothed unrolling for {\modl}. 
    %\hui{[changed DC Layer to DC block.]}
    % \SL{[Please update above figures following the style used in  Fig.\,\ref{fig: RS-E2E}.]}
    }}
    \label{fig: model-arch}
    % \vspace*{-4mm}
\end{figure}

% To be congruent with the presence of the denoiser in every unrolling step, we adopt an unrolling version of randomized smoothing, which we call {\us}. {\us} exploits the full potential of the unrolling presence of the denoiser in Gaussian noise robustness, thus obtaining high robustness for {\modl}. 

% More concretely, our method of {\us} can be formularized as the following iterative algorithm, 
% \begin{subequations}
% \label{eq: unrolling smoothing}
% \begin{align}
%     \bz_k &= \mathbb{E}_{\epsilon\sim\mathcal{N}(0,\sigma^2I)} \cD(\bm{\theta}; \bx_k + \epsilon)
%     \\
%     \bx_{k+1} &=\underset{\bx}{\arg\min} \sum_{c=1}^{N_c}||A_c \bx - \by_c||_2^2 + \lambda ||\bx-\bz_k||^2
%     \label{eq: unrolling smoothing optimization}
% \end{align}
% \end{subequations}
% which is initialized with $\bx_0=A^H\by$, where $A^H$ transforms k-space data to the image domain. $\bz_k$ is estimated through the Monte Carlo sampling. In every unrolling step (\textit{i.e.} iteration), we first generate a number of Gaussian noises $\epsilon \sim \mathcal N(0, \sigma I^2)$, and add it to $\bx_k$. Then, we estimate the expectation of the denoised version of these images.
% The denoiser $\cD (\cdot)$ is different from that in {\modl}, because the denoiser here is custom-trained to remove the Gaussian noises as well as alias artifacts and noises obtained in the undersampling and the upstream image processing. 
% Finally, the conjugate gradient method is used to solve the optimization problem \eqref{eq: unrolling smoothing optimization}.


As will be evident later, our empirical results in Sec.\,\ref{sec: experiment} (\textit{e.g.}, \textbf{Fig.\,\ref{fig:archi_PSNR}}) show that {\us} and {\usold} can significantly outperform \ref{eq: denoised smoothing mri} in   adversarial robustness. In particular,  {\us} achieves the best robust performance without sacrificing its  standard  accuracy when evaluated on benign testing data. 

% We evaluate the effectiveness of the three architectures towards adversarial robustness, and the results are shown in Table \ref{tab: exp_smoothing} and Fig. \ref{fig:archi_PSNR}. Experiment shows that {\us} has the highest robustness and comparable accuracy with other models. This implies the effectiveness of our proposed {\us}. We suspect that {\us} exploits the full potential of the presence of denoiser in each unrolling step compared to \ref{eq: denoised smoothing mri}, and avoids the noises introduced by the conjugate gradient method compared to {\us} on the whole.

\subsection{Solution to \textbf{(Q2)}: {\us}'s pre-training and fine-tuning}
\label{sec: how to smoothing}
In what follows, we develop the training scheme of  {\us}. Spurred by the currently celebrated `pre-training + fine-tuning'  technique \cite{zoph2020rethinking,salman2020denoised}, we propose to train the {\us} model 
following this learning paradigm. 
Our rationale is that pre-training  is able to provide a  robustness-aware initialization of the DL-based denoising network for ease of fine-tuning.
%Similar to the ordinary RS-based training loss \cite{salman2020denoised},  
{To pre-train the denoising network $\mathcal D_{\btheta}$, we consider a mean squared error (MSE) loss that 
measures the Euclidean distance between  images denoised by $\mathcal D_{\btheta}$ and the labels (\textit{i.e.}, target images, denoted by $\mathbf t$). This leads to the \textbf{pre-training} step:
%$\mathcal D_{\btheta}$ following
\begin{equation}
   \btheta_\mathrm{pre} = \displaystyle \argmin_{\btheta} \mathbb{E}_{\mathbf t \in \mathcal D} [ \mathbb{E}_{\boldsymbol \nu }  || \mathcal D_{\boldsymbol \theta} (  \mathbf t + \nu)  -  \mathbf t ||_2^2]
%\nonumber
\label{eq: pre-train_loss}
\end{equation}
where
$\mathcal{D}$ denotes the set of labels, 
$\boldsymbol \nu \sim\mathcal{N}(0,\sigma^2I)$. Note that the MSE loss \eqref{eq: pre-train_loss} does not engage the entire unrolled network. Thus, the pre-training is computational inexpensive and time-efficient.}

% To pre-train the denoising network $\mathcal D_{\btheta}$, we consider a stability loss  that 
% measures the Euclidean distance between smoothed reconstruction at the presence of Guassian noise and that obtained from a noise-free standard {\modl} network. We then  \textbf{pre-train}  $\mathcal D_{\btheta}$ following
% \begin{equation}
%    \btheta_\mathrm{pre} = \displaystyle \argmin_{\btheta} \mathbb{E}_{\mathbf y \in \mathcal D} [ \mathbb{E}_{\boldsymbol \nu }  || g_{\boldsymbol \theta} (  \mathbf A^H \mathbf y)  -  \hat{\mathbf x}_\text{\modl} (\mathbf A^H \mathbf y) ||_2^2 ], %,\quad\epsilon\sim\mathcal{N}(0, \sigma I^2)
% %\nonumber
% \label{eq: pre-train_loss}
% \end{equation}
% where
% $\mathcal{D}$ is the set of measurements, 
% $\boldsymbol \nu \sim\mathcal{N}(0,\sigma^2I)$, 
% $g_{\boldsymbol \theta} $ is defined in \eqref{eq: denoised smoothing mri} with trainable denoiser's parameters $\btheta$, and $\hat{\mathbf x}_\text{\modl} (\mathbf A^H \mathbf y)$ is the output of a given standard {\modl} model at the input $\mathbf A^H \mathbf y$. Note that the   stability loss  \eqref{eq: pre-train_loss} is independent of ground-truth  reference images (known as target images to be recovered). Thus, it falls into the unsupervised learning paradigm.  

%Regarding $\btheta_\mathrm{pre}$ as the initial point, 
We next develop the fine-tuning scheme to improve  $\btheta_\mathrm{pre}$ based on labeled MRI datasets, \textit{i.e.}, with access to target images (denoted by $\mathbf t$). 
Since RS in  {\us}  (\textbf{Fig.\,\ref{fig: model-arch}-(b)})  is applied to every unrolling step, we propose an \textit{unrolled stability (\textbf{UStab}) loss} for fine-tuning the denoiser $\mathcal{D}_{\btheta}$:
\begin{align}
    \ell_{\mathrm{UStab}}(\btheta;   \mathbf y, \mathbf t)=
    \sum_{n=0}^{N-1} \mathbb{E}_{\boldsymbol \nu }||\cD_{\btheta}( \bx_n+\boldsymbol \nu)-\cD_{\btheta}(\mathbf t)||^2_2,
    %+ 
   % \lambda'||\|  \bx_N-\bx_\mathrm{label}||_2^2
    \label{eq: unrolling loss}
    %\tag{UStab}
\end{align}
where $N$ is the total number of unrolling steps, 
$\mathbf x_0 = \mathbf A^H \mathbf y $, and $\boldsymbol \nu \sim\mathcal{N}(0,\sigma^2I)$. {The UStab loss \eqref{eq: unrolling loss} relies on  target images, bringing in a key benefit: the denoising stability 
is guided by the  reconstruction accuracy of  the ground-truth image, yielding a graceful tradeoff between  robustness and accuracy. }

Integrating the UStab loss \eqref{eq: unrolling loss} with the vanilla reconstruction loss of {\modl} \cite{Aggarwal2019MoDL:Problems}, we obtain the \textbf{fine-tuned} $\btheta$ 
% by solving the problem
% \hui{
by using
% }
\begin{equation}
   %\btheta_\mathrm{ft} =
   \displaystyle 
   % \min_{\btheta}    \mathbb{E}_{(\mathbf y, \mathbf t) \in \mathcal D} [
   % \hui{
   \ell(\btheta; \mathbf y, \mathbf t)=\lambda_\ell
   % }
   \| 
   %\hat{\mathbf x}_\text{\modl} 
   \mathbf x_N
   (\boldsymbol \theta; \mathbf A^H \mathbf y) - \mathbf t \|_2^2  +  \ell_{\mathrm{UStab}}(\btheta;   \mathbf y, \mathbf t) 
   % ]
   , %,\quad\epsilon\sim\mathcal{N}(0, \sigma I^2)
%\nonumber
\label{eq: finetune_loss}
\end{equation}
where $\mathcal D$ denotes the labeled dataset, 
%$\hat{\mathbf x}_\text{\modl} (\boldsymbol \theta; \mathbf A^H \mathbf y)$ 
$ \mathbf x_N$
is the reconstructed image using {RS-applied} {\modl} ({\textit{i.e.}, {\usold} and {\us}}) with the denoising network of parameters $\btheta$ and input  $\mathbf A^H \mathbf y$,
%\hui{[Or just use $x_N$ in \eqref{eq: finetune_loss}?]}
and $\lambda_\ell > 0$ 
% \hui{($\lambda$ has been used in~\eqref{eq:inv_pro}!)}
is a regularization parameter to strike the balance between  reconstruction error (for accuracy) and denoising stability (for robustness). 
% We optimize Problem~\eqref{eq: finetune_loss} 
% \hui{
We fine-tune $\btheta$
% }
using $\btheta_\mathrm{pre}$ as   initialization.
% that the initialization 


% We propose \textbf{unrolling loss} for better performance, as
% {
% \begin{align}
%     \cL_{\mathrm{Unrolling}}(\theta)&=\lambda'||\|  \bx_N-\bx_\mathrm{label}||_2^2
%     \notag
%     \\
%     +\sum_{n=0}^{N-1}&~\mathbb{E}_{\boldsymbol \nu \sim\mathcal{N}(\mathbf 0, \sigma^2\mathbf I)}||\cD(\bm{\theta}; \bx_n+\boldsymbol \nu)-\cD(\bm{\theta}; \bx_\mathrm{label})||^2_2
%     \label{eq: unrolling loss}
% \end{align}
% }%
% where $\bx_N$ is the $  g(\mathbf A^H \mathbf y)$ where it mention at eq~\ref{eq: denoised smoothing mri}, \textit{i.e.} the final reconstructed image, $\bx_\mathrm{label}$ is the ground truth image. Inspired by TRADES \cite{zhang2019theoretically}, our \textbf{unrolling loss} contains a robustness regularizer after data consistence for accuracy, and a hyper-parameter $\lambda'$ is introduced to balance the trade-off between accuracy and robustness. When $\lambda'=0$, the \textbf{unrolling loss} exploits the full potential of our {\us} architecture towards robustness; when $\lambda'$ approaches $+\infty$, the \textbf{unrolling loss} penalties the model's inaccuracy directly. The expectation here is, again, estimated through the Monte Carlo sampling using the same noises as the forwarding propagation.

% Moreover, we do not only rely on \textbf{unrolling loss} to train {\us}. In general, we adopt MSE + Unrolling Stab scheme following MSE + Stab scheme in denoised smoothing \cite{salman2020denoised}. First, the denoiser is pre-trained on the ground truth image to remove added Gaussian noises. We use an MSE loss as denoised smoothing, but $\bx$ here is the ground truth image.
% This pre-training only takes a small amount of time compared to the following fine-tuning. However, the pre-training can find a good starting point for fine-tuning, which leads to remarkably higher robustness of the final model.
% Then, the denoiser is fine-tuned through a stability loss. Conventionally, an end-to-end (E2E) stability loss is used, as
% \begin{equation}
%     \cL_{\mathrm{E2E}} = ~\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\sigma^2I)}||f(\bm{\theta}; \bx, \epsilon) - f_\mathrm{base}(\bx)||_2^2%,\quad\epsilon\sim\mathcal{N}(0, \sigma I^2)
% \end{equation}
% where $f$ is the smoothed model, including \ref{eq: denoised smoothing mri}, and $f_\mathrm{base}$ is the base model, \textit{i.e.} the trained vanilla {\modl}. However, we adopt \textbf{unrolling loss} for fine-tuning. We study the effectiveness of our training scheme in Sec. \ref{sec: experiment}.

% We, instead, adopt an unrolling form of stability loss to exploit the unrolling architecture of {\us}, which we call \textit{unrolling loss}, as
% \begin{align}
%     \cL_{\mathrm{Unrolling}}&=\lambda'||\bx_K-\bx_\mathrm{label}||_2^2
%     \notag
%     \\
%     +\sum_{k=0}^{K-1}&~\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\sigma^2I)}||\cD(\bm{\theta}; \bx_k+\epsilon)-\cD(\bm{\theta}; \bx_\mathrm{label})||^2_2
%     \label{eq: unrolling loss}
% \end{align}
% where $\bx_K$ is the output of the $K$-th and the final unrolling step, \textit{i.e.} the final reconstructed image, $\bx_\mathrm{label}$ is the ground truth image. Inspired by TRADES \cite{zhang2019theoretically}, our unrolling loss contains a robustness regularizer besides one for accuracy in our unrolling loss, and $\lambda'$ is a hyper-parameter that balances the trade-off between accuracy and robustness. When $\lambda'=0$, the unrolling loss exploits the full potential of our {\us} architecture towards robustness; when $\lambda'$ approaches $+\infty$, the unrolling loss penalties the model's inaccuracy directly. The expectation here is, again, estimated through the Monte Carlo sampling using the same noises as the forwarding propagation.

% We emphasize that $\cD(\bm{\theta}; \bx_\mathrm{label})$ in the unrolling loss as the reference image is carefully selected. Because of the convergence and added Gaussian noises, the denoiser $\cD(\cdot)$ and its input image $\bx_n + \epsilon$ are both unstable in the training process. Consequently, the choice for the reference image in the unrolling loss is significant.
% We select $\cD(\bm{\theta};\bx_\mathrm{label})$ as the reference image, in which $\cD(\cdot)$ countacts the instability of the denoiser, and $\bx_\mathrm{label}$ makes the training more aggressive, promoting training efficiency.

% \vspace{-3mm}
