\section{Preliminaries and Problem Statement}
\label{sec: preliminaries}

In this section, we provide a brief background on MRI reconstruction and motivate the problem of our interest. 


\vspace*{1mm}
\noindent \textbf{Setup of MRI reconstruction.} MRI reconstruction is an 
 ill-posed inverse problem~\cite{compress}, which aims to reconstruct the original signal $\mathbf x \in\mathbb{C}^{q}$ from its  measurement $\mathbf y \in  \mathbb{C}^p$ with $p < q$. The imaging system in MRI can be modeled as a linear system $\mathbf y \approx  \mathbf A \mathbf x $, {where $\mathbf A$ may take on different forms for single-coil or parallel (multi-coil) MRI, etc.}
 For example,  $\mathbf A = \mathbf S \mathbf F$ in the single-channel Cartesian MRI acquisition setting, where $\mathbf F$ is the 2-D discrete Fourier
transform and $\mathbf S$ is a (fat) Fourier subsampling matrix, and its sparsity is controlled by the measurement sampling rate or acceleration factor.
 With the linear observation model, %the problem of 
 MRI reconstruction is often formulated as
 %Let $\bx\in\mathbb{C}^{q}$ and $\mathbf y_c \in  \mathbb{C}^p$ (with $p < q$) denote the image to be recovered and its measurement associated with the $c$th  magnetic coil. The problem of MRI reconstruction  is given by 
%  can be described roughly as a model-fitting method to reconstruct   image with a data-consistency term. This is typically accomplished by adding a regularization term that enforces the sparsity-inducing prior on $\by$.
% The regularizer imposes additional constraints on the desired type of solution, resulting in an eventual solution that is more stable. 
% For multi-coil MRI reconstruction of an image 
% $\bx\in\mathbb{C}^{q}$, the optimization problem can be mathematically framed as
%\vspace*{-3mm}
\begin{align}
    \hat{\bx}=\underset{\bx}{\arg\min} ~ \|\mathbf A \bx - \by \|^{2}_2 + \lambda \mathcal{R}(\bx),
    \label{eq:inv_pro}
\end{align}
where   $\mathcal{R}(\cdot)$ is a regularization function (\textit{e.g.}, $\ell_1$ norm to impose a sparsity prior), and $\lambda > 0$ is a regularization parameter.  

% In \eqref{eq:inv_pro},  $A_c = M \mathcal{F} S_c$ denotes   the forward system   operation,
% where $M \in \{0,1\}^{p\times q}$ is a given sampling mask, 
% $\mathcal{F}\in \mathbb{C}^{q\times q}$ is 
% the $2D$ Fourier transformation, and $S_c \in 
% \mathbb{C}^{q\times q}$ is a complex-valued position-dependent sensitivity map. 

% And the   regularizer $\mathcal{R}(\cdot)$  in \eqref{eq:inv_pro} is used to restrict the 
% solutions to the space of desirable
% images, and $\lambda$ is the regularization parameter controlling the trade-off between the residual norm and regularity.

% Choices for the regularizer in MRI reconstruction can vary from the $\ell_1$ penalty on wavelet coefficients~\cite{wave} or the most widely used CS method is total variation~\cite{totalv} denoising which enforces piecewise constant images by uniformly penalizing image gradients.

%To effectively solve Problem~\eqref{eq:inv_pro}, 

\underline{Mo}del-based reconstruction using \underline{D}eep \underline{L}earned priors
({\modl}) \cite{Aggarwal2019MoDL:Problems} was proposed recently as a deep learning-based alternative approach to solving Problem~\eqref{eq:inv_pro}, and
has attracted much interest as it 
merges the power of model-based reconstruction schemes with DL.
In {\modl}, the hand-crafted regularizer $\mathcal R$ is replaced by a learned network-based prior (involving a deep convolutional neural network (CNN)).
%that helps remove aliasing artifacts and noise at $\mathbf x$. 
The corresponding formulation is
{%\vspace*{-3mm}
\begin{align}
    \hat{\bx}_{\boldsymbol \theta}=\underset{\bx}{\arg\min} ~  \|\mathbf A \bx - \by \|^{2}_2 + \lambda \| \bx - \cD_{\boldsymbol \theta} (\bx) \|_2^2,
\label{eq:altmin}    
\end{align}
}%
where $\cD_{\boldsymbol \theta} (\bx)$  denotes a %denoising 
deep network with  parameters $\boldsymbol \theta$, with input $\mathbf x$.
%where $\nu \geq 0$ weights the data-fidelity term and $\mu \geq 0$ weights the proximity of $x$ to $z$ above. 
To obtain $  \hat{\bx}_{\boldsymbol \theta}$,   an alternating process based on variable splitting is typically used, which involves the following two steps \ding{172}--\ding{173}, executed iteratively. 

\noindent 
\ding{172} \textbf{Denoising step:} Given an updated solution $\mathbf x_n$ at the $n$th iteration (also known as `unrolling step'), {\modl} 
uses the ``denoising" network to obtain $\mathbf z_n :=\cD_{\boldsymbol \theta} (\bx_n)$. 

\noindent 
\ding{173}   \textbf{Data-consistency (DC) step:}  {\modl} then solves a least-squares problem with a denoising prior as
$
\mathbf x_{n+1} = \argmin_{\mathbf x} 
  \|\mathbf A \bx - \by \|^{2}_2 + \lambda \| \bx - \mathbf z_n \|_2^2
$, which is convex (fixed $\mathbf z_n$) with closed-form solution. 
 
In {\modl}, this alternating process is unrolled for a few iterations and the denoising network's weights are trained end-to-end in a supervised manner. 
% \hui{
For the rest of this paper, the function $\mathbf x_{\modl}(\cdot)$ denotes the image reconstruction process of {\modl}. 
% to distinguish it from $\hat{\mathbf{x}}_{\modl}$ in optimization problem~\eqref{eq:altmin}.
% }
%The   steps \ding{182}--\ding{183} are   performed iteratively. 

% We refer readers to \cite{Aggarwal2019MoDL:Problems} for more details.

% The problem can be solved by alternating between updating $\bx$ and $\cD (\bm{\theta};\bx)$.
% The {\modl} scheme performs the update of $\bx$ (\textit{i.e.}, $\mathcal{DC}$) by Conjugate Gradient descent, and the update of $\cD (\bm{\theta};\bx)$ involving with a CNN denoiser ($\cD (\cdot)$) that is applied to the updated $\bx$.

\vspace*{1mm}
\noindent \textbf{Motivation: Lack of robustness in {\modl}.}
It was shown in \cite{antun2020instabilities} that DL may lack stability in image reconstruction, especially when facing tiny, almost undetectable input perturbations. 
Such perturbations are known as `adversarial attacks', and have been well studied in  DL for image classification~\cite{Goodfellow2015explaining}. 
Let $\boldsymbol \delta$ denote a small   perturbation of a point that falls in an $\ell_\infty$ ball of radius $\epsilon$, \textit{i.e.}, $\| \boldsymbol \delta \|_\infty \leq \epsilon$. Adversarial attack then corresponds to the worst-case input perturbation $\boldsymbol \delta$ that maximizes the reconstruction error, \textit{i.e.}, 
% \SL{[is the following correct? w.r.t. true target image?]}
\begin{equation}
\begin{array}{ll}
\displaystyle \minimize_{\| \bdelta \|_\infty \leq \epsilon }     &  - \|  
% \hat
{\bx}_\text{{\modl}} (\mathbf A^H \mathbf y + \boldsymbol \delta) -  \mathbf t \|_2^2,
\end{array}
    % \delta=\underset{\delta\sim\mathcal{S}}{\arg\max}~\cL(f(\bx+\delta), f(\bx))
  %  \underset{||\delta||_\infty\le r}{\arg\max}~||f(\bx_0+\delta) - f(\bx_0)||_2^2
  \label{eq: atk_perturb}
\end{equation}
where $\mathbf t$ is a target  image (\textit{i.e.}, label), the operator $\mathbf A^H $  transforms the measurements $\mathbf y$ to the image domain, and $\mathbf A^H \mathbf y$ is the input (aliased) signal for the {\modl}-based   reconstruction network.  Given a   {\modl} model,  problem \eqref{eq: atk_perturb} can be effectively solved using the iterative projected gradient descent (PGD) method~\cite{madry2017towards}. The resulting solution is   called `PGD attack'.
% where $\mathcal{S}$ refers to a certain sphere around the data point $\bx$ with some distance measure.

In \textbf{Fig.\,\ref{fig: weakness}-(a)} and \textbf{(b)}, we demonstrate an example of the reconstructed image 
$\bx_\text{{\modl}}$ 
from a benign input (\textit{i.e.}, clean and unperturbed input) and a PGD attacked input, respectively. As we can see, the quality of reconstructed image significantly degrades in the presence of adversarial (very small) input perturbations. 
Although robustness against adversarial attacks is a primary focus of this work, \textbf{Fig.\,\ref{fig: weakness}-(c)} and \textbf{(d)} show two other types of instabilities that {\modl} may suffer at testing time: the change of the measurement sampling rate (which leads to `perturbations' to the sparsity of sampling mask in $\mathbf A$) \cite{antun2020instabilities}, and the number of unrolling steps \cite{gilton2021deep} used in {\modl} for test-time image reconstruction.
We observe that a over sampling mask  (\textbf{Fig.\,\ref{fig: weakness}-(c)}) and a larger number of unrolling steps (\textbf{Fig.\,\ref{fig: weakness}-(d)}), which deviate from the training-time setting of  {\modl}, can lead to much poorer image reconstruction performance than the original setup (\textbf{Fig.\,\ref{fig: weakness}-(a)}) even in the absence of an adversarial input.
In Sec.\,\ref{sec: experiment}, we will show that our proposed approach (originally designed for improving {\modl}'s robustness against adversarial attacks) 
yields resilient reconstruction performance against all perturbation types shown in \textbf{Fig.\,\ref{fig: weakness}}.
% where $f$ is the reconstruction model, and $r$ is the distance measure of the perturbation. Also, Modl will struggle with generalization to  novel experimental settings at test time with different undersampling rate and unrolling step.To be specific, different undersampling rates refer to 
% the new undersampling mask $M$ with different density is used to obtain the k-space measurements $\mathbf{y}_c$ from the ground truth in \eqref{eq:altmin} in the evaluation stage. And different unrolling steps mean a different number of iterations is used to solve \eqref{eq:altmin} in the evaluation stage.
% In Fig. \ref{fig: weakness}, the reconstructed images blurs when adversarial perturbations are added to the undersampled input, a denser undersampling mask is used, and a larger number of unrolling steps is adopted in {\modl}.
 
\begin{figure}[htb]
%\vspace*{-3mm}
%    \centering
%    \begin{subfigure}{.10\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{Figures/output_vanilla_vis.pdf}
%        \caption{\footnotesize{clean data
%        }}
%    \end{subfigure}
%    \begin{subfigure}{.10\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{Figures/visualization/vanilla_MoDL_eps0.5_255.pdf}
%        \caption{\footnotesize{adversarial perturbations
%        }}
%    \end{subfigure}
%    \begin{subfigure}{.10\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{Figures/undersampling_vis.pdf}
%        \caption{\footnotesize{undersampling rate}}
%    \end{subfigure}
%    \begin{subfigure}{.10\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{Figures/unrolling_steps_vis.pdf}
%        \caption{\footnotesize{number of unrolling steps
%        }}
%    \end{subfigure}
    
%    \vspace*{-1em}
%    \caption{\footnotesize{{\modl}'s %instabiliites \textit{w.r.t.} adversarial perturbations, undersampling rate and number of unrolling steps}}
%    \label{fig: weakness}
%    \vspace*{-2mm}
\begin{tabular}[b]{cccc}
        \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/vanilla_MoDL_recon.pdf}
        &
        \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/vanilla_MoDL_eps0.5_255.pdf}
        &
        \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/undersampling_2.0_vis.pdf}
        &
        \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/unrolling_steps_16_clean_vis.pdf}
        \\[-0pt]
        \scriptsize{(a)} 
        & 
        \scriptsize{(b)} 
        &  
        \scriptsize{(c)} 
        &
        \scriptsize{(d)}

\end{tabular}
\vspace*{-5mm}
\caption{\footnotesize{
%\hui{
{\modl}'s instabilities against perturbations to input data, the measurement sampling rate, and the number of unrolling steps used at testing time shown on an image from the \texttt{fastMRI} \cite{zbontar2018fastmri}
dataset. We refer readers to  Sec.\,\ref{sec: experiment} for more experiment details.
(a) {\modl} reconstruction from benign (\textit{i.e.}, clean) measurement with $ 4\times$ acceleration (\textit{i.e.}, 25\% sampling rate) and 8 unrolling steps. (b) {\modl} reconstruction from adversarial input of perturbation strength $\epsilon = 0.002$ (other settings are same as
 (a)). 
(c) {\modl} reconstruction  from clean measurement   with $ 2\times$ acceleration (\textit{i.e.}, 50\% sampling rate) and  using 8 unrolling steps.
(d) {\modl} reconstruction from clean measurement with $ 4\times$ acceleration and using  16 unrolling steps.
%}
% \SL{Missing specific experiment setup.}
}}
\label{fig: weakness}
\vspace*{-4mm}
\end{figure}

\vspace*{1mm}
\noindent \textbf{Randomized smoothing (RS).} %and denoised smoothing}
RS creates multiple random noisy copies of input data and takes an averaged output over these noisy inputs so as to gain  robustness against input noises \cite{cohen2019certified}. Formally, given a base function $f(\mathbf x)$, RS turns this base function to a smoothing version  $g(\mathbf x) \Def \mathbb{E}_{\boldsymbol \nu \sim\mathcal{N}(\mathbf 0, \sigma^2\mathbf I)} [ f( \bx + \boldsymbol \nu) ]$, where $\boldsymbol \nu \sim\mathcal{N}(\mathbf 0, \sigma^2\mathbf I)$ denotes the Gaussian distribution with zero mean and $\sigma^2$-valued variance. 
In the literature, RS  has been used as an effective adversarial defense in image classification \cite{cohen2019certified,salman2020denoised,zhang2022how}.
However, it remains elusive whether or not RS is an effective solution to improving robustness of {\modl} and other image reconstructors. A preliminary study towards this direction was provided by \cite{wolfmaking}, which integrates RS with image reconstruction in an end-to-end (\textbf{E2E}) manner.   For {\modl}, this yields
\begin{align}
    g(\mathbf A^H \mathbf y)=\mathbb{E}_{\boldsymbol \nu \sim\mathcal{N}(\mathbf 0, \sigma^2\mathbf I)} [ 
    % \hat
    {\mathbf{x}}_{\text{\modl}} (\mathbf A^H \mathbf y + \boldsymbol \nu) ]
    % ,
    .
    \label{eq: denoised smoothing mri}
    \tag{RS-E2E}
\end{align}
% where $\hat{\mathbf{x}}_{\text{\modl}} (\mathbf A^H \mathbf y + \boldsymbol \nu) $ is the image reconstruction function defined in \eqref{eq:altmin}   with $\mathbf A^H \mathbf y + \boldsymbol \nu$ as the input signal.  
% \textbf{Fig.\,\ref{fig: RS-E2E}} provides an illustration of \ref{eq: denoised smoothing mri}-baked {\modl}.

\begin{figure}[htb]
\vspace*{-3mm}
    \centering
    %\begin{subfigure}{.48\textwidth}
        %\centering
        \includegraphics[width=0.46\textwidth]{Figures/RS-E2E.png}
        \caption{\footnotesize{A schematic overview of \ref{eq: denoised smoothing mri}.}}
        \label{fig: RS-E2E}
\end{figure}

\textbf{Fig.\,\ref{fig: RS-E2E}} provides an illustration of \ref{eq: denoised smoothing mri}-baked {\modl}.
Although \ref{eq: denoised smoothing mri} renders a simple application of RS to {\modl}, it remains unclear if \ref{eq: denoised smoothing mri} is the most effective way to bake RS into {\modl}, considering the latter's    learning specialities,
\textit{e.g.}, the involved denoising step %(\ding{182}) 
and the DC step.
%(\ding{183}). 
In the rest of the paper, we will focus on studying two main questions \textbf{(Q1)}--\textbf{(Q2)}.
 \begin{tcolorbox}[left=1.2pt,right=1.2pt,top=1.2pt,bottom=1.2pt]
 %\begin{center}
\textbf{(Q1)}: \textit{Where should the RS operator   be integrated into {\modl}?}
\textbf{(Q2)}: \textit{How to design the denosier $\mathcal D(\boldsymbol \theta; \cdot)$   in the presence of RS?}
%\end{center}
\end{tcolorbox}

%\SL{[I stop here.]}





