\section{Experiments}
\label{sec: experiment}

% \SL{[A summary paragraph to summarize your key metrics and results. E.g., In this section, we evaluate the effectiveness of our proposal from the following aspects xxxx. With a detailed comparison with the heuristics-based poisoning strategy, we find that xxx. ]}
%In this section, we evaluate different smoothing architectures and different training schemes discussed in Sec. \ref{sec: unrolling} and Sec. \ref{sec: how to smoothing} respectively. And we demonstrate the effectiveness of our proposed {\us} in three aspects of robustness mentioned in Sec. \ref{sec: preliminaries}.
% Result show that our proposed {\us} outperforms all other baselines in these three aspects.

% \vspace{-3mm}
\subsection{Experiment setup} 
%\vspace*{1mm}
\noindent \textbf{Models \& datasets.} 
The studied RS-baked {\modl} architectures are shown %in Fig.\,\ref{fig: model-arch} architecture is shown 
in \textbf{Figs.\,\ref{fig: RS-E2E}} and \textbf{\ref{fig: model-arch}}.
In experiments, we set  the total number of unrolling steps to $N = 8$, and set the denoising regularization parameter   $\lambda = 1$ in vanilla {\modl}. %\SL{[is my understanding  on $\lambda$ correct?]}\hui{[yes]}. 
%The number of unrolling steps in {\modl} $K$ and hyper-parameter $\lambda$ are set to 8 and 1 respectively to reach high reconstruction quality and time efficiency as well. 
For the denoising network $\mathcal{D}_{\btheta}$, we  use the Deep Iterative Down-Up Network (DIDN) \cite{yu2019deep} with three down-up blocks and 64 channels. We adopt 
the conjugate gradient method \cite{Aggarwal2019MoDL:Problems}  with tolerance $1e^{-6}$ to implement the DC block. 
We conduct our experiments on 
the \texttt{fastMRI} dataset \cite{zbontar2018fastmri}. 
The observed data $\by$ are obtained with $15$ coils and are  cropped to the resolution of $320\times320$ for MRI reconstruction. To implement the observation model, we adopt a Cartesian mask  at $4\times$ acceleration (\textit{i.e.}, $25\%$ sampling rate). 
The coil sensitivity maps for all cases were obtained using the BART toolbox \cite{tamir2016generalized}.
% % \JH{add references}
% %, 
% and all the coil sensitivity maps were estimated from undersampled (center of k-space) data to stimulate the realistic representation of real MRI experiments.

% \vspace*{-4mm}
\vspace*{1mm}
\noindent \textbf{Training \& evaluation.}
% Do we need to use the past tense?
We use 304 images for training, 32 images for validation, and 64 images for testing (that are unseen during training). 
%\SL{And we use xxx testing images selected from xxx?}
At \textbf{training time}, the batch size is set to $2$   trained on   two GPUs. 
%For every batch of images, the same noises $\epsilon$ were used in all unrolling steps.
We use the the Adam optimizer to train studied MRI reconstruction models with the momentum parameters $(0.5, 0.999)$.
%The models were trained using the Adam optimizer with parameter $\beta\text{s}=[0.5,0.999]$.
The number of epochs is set to $60$ with a linearly decaying learning rate from $10^{-4}$ to $0$ after epoch 20. The stability parameter $\lambda_\ell$ 
% \hui{$\lambda$ here}
in \eqref{eq: finetune_loss} is tuned so that  the standard accuracy  of the learned model is comparable to the vanilla {\modl}.
In RS, we set the standard deviation of Gaussian noise as 
$\sigma = 0.01$, and use $10$ Monte Carlo samplings to implement the smoothing operation. 
% \vspace*{-4mm}
%\paragraph*{Evaluation setup.}
% \SL{[mention baselines here and metrics to evaluate the performance]}
At \textbf{testing time}, 
we evaluate our methods on clean data, random noise-injected data and adversarial examples generated by 10-step PGD attack \cite{antun2020instabilities} of $\ell_\infty$-norm radius $\epsilon = 0.004$. The quality of reconstructed images is measured using  
peak signal-to-noise ratio (PSNR) and structure similarity (SSIM).
In addition to adversarial robustness, we also
evaluate the performance of our methods at the presence of another two perturbation sources (\textit{i.e.}, altered sampling rate and unrolling step number at testing time), as shown in \textbf{Fig.\,\ref{fig: robustness}}. 
% \hui{
% }
 
% against three   perturbation sources as s in Sec. \ref{sec: preliminaries} by changing the number of unrolling steps and the undersampling rate of masks.
 \begin{table}[htb]
\centering
% \vspace*{-1em}
\vspace*{-3mm}
\caption{\footnotesize{
% Clean accuracy and 
Accuracy performance  of different smoothing architectures (\ref{eq: denoised smoothing mri}, {\usold}, {\us}), together with the vanilla  {\modl}. Here `Clean Accuracy', `Noise Accuracy', and `Robust Accuracy' refer to PSNR/SSIM evaluated on benign data, random noise-injected data, and PGD attack-enabled adversarial data, respectively.
%with different training schemes, where \textsc{US} represents {\us}. 
%The noise accuracy and robust accuracy are evaluated with Gaussian noise $||\epsilon||_2 \le 0.004$ and adversarial perturbation $||\delta||_2 \le 0.004$ respectively.
$\uparrow$ signifies that a higher   number indicates a better reconstruction accuracy. The result $a${\tiny{$\pm b$}} represents mean $a$ and standard deviation $b$ over {64} testing images.
The relative performance is reported with respect to that of vanilla {\modl}.
% \hui{, in the format mean\tiny{$\pm$std}.} 
% The best performance is highlighted in \textbf{bold}.
}}
\label{tab: exp_smoothing}
\vspace*{-2mm}
\resizebox{0.48\textwidth}{!}{%
\begin{tabular}{c|cc|cc|cc}
\toprule[1pt]
\midrule
Models 
& \multicolumn{2}{c}{Clean Accuracy} 
& \multicolumn{2}{c}{Noise Accuracy} 
& \multicolumn{2}{c}{Robust Accuracy} 
\\
Metrics 
& PSNR \textcolor{red}{$\uparrow$} & SSIM \textcolor{red}{$\uparrow$}
& PSNR \textcolor{red}{$\uparrow$} & SSIM \textcolor{red}{$\uparrow$}
& PSNR \textcolor{red}{$\uparrow$} & SSIM \textcolor{red}{$\uparrow$}
\\
\midrule
Vanilla {\modl}
& 29.73\footnotesize{$\pm$3.27}
& 0.900\footnotesize{$\pm$0.07}
& 28.70\footnotesize{$\pm$2.77}
& 0.874\footnotesize{$\pm$0.07}
& 22.91\footnotesize{$\pm$2.42}
& 0.729\footnotesize{$\pm$0.07}
\\
\midrule
%\ref{eq: denoised smoothing mri}
RS-E2E
& \textbf{+0.09}\footnotesize{$\pm$3.24}
% & \textbf{+0.0016}
& \textbf{+0.002}\footnotesize{$\pm$0.07}
& +0.38\footnotesize{$\pm$2.90}
% & +0.0101
& +0.010\footnotesize{$\pm$0.07}
& +0.78\footnotesize{$\pm$2.70}
& \textbf{+0.034}\footnotesize{$\pm$0.08}
\\
{\usold}  %($\cD + \mathcal{DC}$)}
& -1.01\footnotesize{$\pm$3.07}
& -0.014\footnotesize{$\pm$0.08}
& -0.09\footnotesize{$\pm$2.99}
% & +0.0079
& +0.008\footnotesize{$\pm$0.08}
& +3.08\footnotesize{$\pm$2.42}
& -0.014\footnotesize{$\pm$0.11}
\\
\rowcolor[gray]{.8}
{\us} (ours)
& -0.34\footnotesize{$\pm$3.06}
% & -0.0057
& -0.006\footnotesize{$\pm$0.08}
& \textbf{+0.53}\footnotesize{$\pm$2.98}
& \textbf{+0.016}\footnotesize{$\pm$0.08}
& \textbf{+3.87}\footnotesize{$\pm$2.28}
% & +0.0082
& +0.008\footnotesize{$\pm$0.11}
\\
\midrule
\bottomrule[1pt]
\end{tabular}%
}
\vspace*{-4mm}
\end{table}

\subsection{Experiment results}
   
%\SL{[You can also give paragraph title, like above, to organize your experiment findings.]}
   
%\SL{In Figure/Table, we present xxxxx. As we can see, xxx.  This implies that xxx}

%\SL{In Figure/Table, we show xxxx. We observe that xxx.}


%\begin{figure}[htb]
\begin{wrapfigure}{r}{43mm}
\vspace*{-5mm}
   % \begin{minipage}{0.23\textwidth}
    \centering
  \hspace*{-3mm}  \includegraphics[width=0.25\textwidth]{Figures/smoothing_archi_PSNR.pdf}
    \vspace*{-3mm}
    \caption{\footnotesize{PSNR  of   baseline methods and proposed {\us} versus perturbation strength $\epsilon$ used in   PGD attack-generated adversarial examples at testing time. The case of $\epsilon =0$ corresponds to clean accuracy. 
    % \SL{[update figure legend, {\usold}]}
    %(\textit{i.e.} PGD $\epsilon = 0$) and adversarial examples generated by 10-step PGD. \ref{eq: denoised smoothing mri} is trained using MSE + Stab scheme, and {\us} is trained using MSE + Unrolling Stab scheme.
    }}
    \label{fig:archi_PSNR}
  %  \end{figure}
  \vspace*{-5mm}
  \end{wrapfigure}
\textbf{Table\,\ref{tab: exp_smoothing}} shows PSNR and SSIM values for different smoothing architectures with different training schemes, along with vanilla {\modl} as a baseline, evaluated on clean and adversarial test datasets. We present the PSNR results for these models under different scales of adversarial perturbations (\textit{i.e.}, attack strength $\epsilon$) in \textbf{Fig.\,\ref{fig:archi_PSNR}}.
We observe that our method {\us} outperforms all other models in robustness, consistent with the visualization of reconstructed images in \textbf{Fig.\,\ref{fig: vis}}. Also, {\us} yields a promising clean accuracy performance, which is better  than {\usold} and   comparable to the vanilla {\modl} model. This shows the effectiveness of our proposed method for improving robustness while preserving clean accuracy \emph{(i.e., without the perturbations)}.   
%overall architecture, {\us} with unrolling loss training scheme, in providing adversarial robustness for {\modl}. 
%This discovery indicates integrating randomized smoothing with complicated models, compared to the single neural network architecture in the image classification task. All we need is to convert the denoiser into a smoothed one and leave the rest of the model unchanged, \textit{i.e.} adding Gaussian noises before the denoiser and estimating the expectation of outputs of the denoiser \textit{w.r.t.} that noise. Moreover, a specific loss function should be designed to match the architecture of the model to reach high robustness.



% \vspace*{-3mm}
% \begin{wrapfigure}{l}{.22\textwidth}
%     \centering
%     \includegraphics[width=.22\textwidth]{Figures/smoothing_archi_PSNR.pdf}
%     \caption{\footnotesize{PSNR results of different smoothing architectures evaluated on clean data ($\epsilon$=0) and adversarial data generated by 10-step PGD.}}
%     \label{fig:archi_PSNR}
% \end{wrapfigure}
% \begin{wrapfigure}{r}{.22\textwidth}
%     \centering
%     \includegraphics[width=.22\textwidth]{Figures/reference_image_PSNR.pdf}
%     \caption{\footnotesize{PSNR results of {\us}, trained using different reference image in unrolling loss, evaluated on clean data ($\epsilon$=0) and adversarial data generated by 10-step PGD.}}
%     \label{fig:reference_PSNR}
% \end{wrapfigure}

% \begin{figure}[htb]
%     \begin{minipage}{0.23\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figures/smoothing_archi_PSNR.pdf}
%     \vspace*{-6mm}
%     \caption{\footnotesize{PSNR results of different smoothing architectures evaluated on clean data (\textit{i.e.} PGD $\epsilon = 0$) and adversarial examples generated by 10-step PGD. \ref{eq: denoised smoothing mri} is trained using MSE + Stab scheme, and {\us} is trained using MSE + Unrolling Stab scheme.}}
%     \label{fig:archi_PSNR}
% \end{minipage}\hfill
% \begin{minipage}{0.23\textwidth}

%     \centering
%     \includegraphics[width=\textwidth]{Figures/reference_image_PSNR.pdf}
%     \vspace*{-6mm}
%     \caption{\footnotesize{PSNR results of {\us}, trained using different reference images in the unrolling loss, evaluated on clean data (\textit{i.e.} PGD $\epsilon = 0$) and adversarial examples generated by 10-step PGD. $\cD_{\mathrm{base}}$ represents the denoiser in the base model, \textit{i.e.} trained vanilla MoDL.}}
%     \label{fig:reference_PSNR}
%     \end{minipage}\hfill
% \vspace*{-4mm}
% \end{figure}

\begin{figure}[htb]

\begin{tabular}[b]{cccc}
        \includegraphics[width=.21\linewidth, trim=70 10 70 10]{Figures/visualization/ground_truth.pdf}
        &\hspace{-0.2cm}
        \includegraphics[width=.21\linewidth, trim=70 10 70 10]{Figures/visualization/vanilla_MoDL_eps0.5_255.pdf}
        &\hspace{-0.2cm}
        \includegraphics[width=.21\linewidth, trim=70 10 70 10]{Figures/visualization/E2E_smoothing_E2E_loss_eps0.5_255.pdf}

        &\hspace{-0.2cm}
        \includegraphics[width=.21\linewidth, trim=70 10 70 10]{Figures/visualization/unrolling_smoothing_unrolling_loss_eps0.5_255.pdf}
        \\[-0pt]
        \scriptsize{(a) Ground Truth} 
        &\hspace{-0.2cm} 
        \scriptsize{(b) Vanilla {\modl}} 
        &\hspace{-0.2cm} 
        \scriptsize{(c) RS-E2E}
        &\hspace{-0.2cm} 
        \scriptsize{(d) \textsc{SMUG}} 
        \\ 
\end{tabular}
\vspace*{-4mm}
\caption{\footnotesize{Visualization of   ground-truth   and reconstructed images using different methods, evaluated on PGD attack-generated adversarial inputs of perturbation strength $\epsilon = 0.002$.
%with perturbation strength $\epsilon = 0.002$.
%, and {\us}. 
%\ref{eq: denoised smoothing mri} is trained using the E2E loss and {\us} is trained using the unrolling loss.
}}
\label{fig: vis}
\vspace*{-2mm}
\end{figure}


% \begin{table}[htb]
% \centering
% % \vspace*{-1em}
% \caption{Clean accuracy and robust accuracy of {\us} with different reference image in loss function, where $\cD_{\theta}$ is the denoier in {\us} and $\cD_{\theta_0}$ is the denoier in vanilla {\modl}. The robust accuracy is tested with adversarial perturbation $||\delta||_2 \le 0.5/255$.
% % The best performance is highlighted in \textbf{bold}.
% }
% \label{tab: exp_unrolling_loss}
% \resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{c|ccc|ccc}
% \toprule[1pt]
% \midrule
% Reference image & \multicolumn{3}{c}{Clean Accuracy} & \multicolumn{3}{c}{Robust Accuracy} \\
% Metrics & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ \\
% \midrule
% $\bx_n$
% & 0.0256
% & 30.2319
% & 0.9058
% & 0.0347
% & 27.3069
% & 0.8017
% \\
% $\bx_\text{label}$
% & 0.026
% & 30.0599
% & 0.904
% & 0.0428 
% & 25.4393
% & 0.7793
% \\
% $\cD_{\theta_0}(\bx_n)$
% & 0.0259 
% & 30.0624 
% & 0.9038 
% & 0.0339 
% & 27.6306
% & 0.8681
% \\
% $\cD_{\theta_0}(\bx_\text{label})$
% & 0.0262 
% & 29.9721 
% & 0.9034 
% & 0.0385 
% & 26.4517 
% & 0.831
% \\
% $\cD_{\theta}(\bx_n)$
% & 0.0269 
% & 29.7071 
% & 0.897 
% & 0.0319 
% & 28.1294 
% & \textbf{0.8751}
% \\
% \rowcolor[gray]{.8}
% $\cD_{\theta}(\bx_\text{label})$
% & 0.0278 
% & 29.3913 
% & 0.8939 
% & \textbf{0.0308}
% & \textbf{28.4137} 
% & 0.8467
% \\
% \midrule
% \bottomrule[1pt]
% \end{tabular}%
% }
% \vspace*{-3mm}
% \end{table}

Next, we evaluate the  effectiveness  of MRI reconstruction methods when facing sampling rate and unrolling step perturbations at testing time.  In other words, there exists a test-time shift for the training  setup of MRI reconstruction.  
In \textbf{Fig.\,\ref{fig: robustness}}, we present the evaluation results of {\us}, with two baselines, vanilla {\modl} and \ref{eq: denoised smoothing mri}, on different unrolling steps and sampling rates. Note that   these models are trained with the number of unrolling steps $K=8$ and sampling masks with the $4\times$ {acceleration (\textit{i.e.}, 25\% sampling rate).}
% undersampling factor. 
As we can see, {\us} achieves a remarkable improvement in robustness against different sampling rates and unrolling steps, which {\modl} and \ref{eq: denoised smoothing mri} fail to achieve.
Although we do not intentionally design our method to mitigate {\modl}'s instabilities against perturbed sampling rate and unrolling step number, {\us} still provides improved PSNRs over  other baselines. 
We credit the improvement to the close relationships between these two instabilities with adversarial robustness. 
%For instability against the undersampling rates, we notice that masks of different undersampling rates are similar to adversarial perturbed masks. And for instability against the number of unrolling steps, we argue that {\us} alleviates the instability with the adversarial robustness of \textit{each} unrolling step, thereby preventing degradation through steps.
% Furthermore, \textbf{Fig.\,\ref{fig: smug weakness}} shows examples of  reconstructed images associated with \textbf{Fig.\,\ref{fig: robustness}}.
%And the visualization of these is shown in Fig. \ref{fig: weakness} for {\modl} and for {\us}.

\begin{figure}[htb]
\vspace*{-3mm}
    \centering
\begin{tabular}{cc}
  \hspace*{-2mm}  \includegraphics[width=0.24\textwidth]{Figures/robustness/undersampling_rate_PSNR_new.pdf}   &    \hspace*{-5mm} \includegraphics[width=0.24\textwidth]{Figures/robustness/unrolling_steps_PSNR.pdf}
\end{tabular}
    % \begin{subfigure}{.23\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{Figures/robustness/undersampling_rate_PSNR.pdf}
    %     \caption{\footnotesize{Undersampling Rate
    %     }}
    % \end{subfigure}
    % \begin{subfigure}{.23\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{Figures/robustness/unrolling_steps_PSNR.pdf}
    %     \caption{\footnotesize{Unrolling Step
    %     }}
    % \end{subfigure}
    \vspace*{-5mm}
    \caption{\footnotesize{
    %\hui{
    PSNR results of different MRI reconstruction methods versus
    different measurement sampling rates ($4\times$ acceleration \textit{i.e.}, $25\%$ sampling rate at training; Left plot) and
    unrolling steps (8 at training; Right plot). 
  %  }
    % \SL{caption is wrong.}
    %The robust accuracy is evaluated with adversarial perturbation $||\delta||_2 \le 0.002$. %{It's worth noting that the higher the undersampling rate is, the sparser the mask is.}
    %\hui{x-axis needs to be changed.}
    }}
    \label{fig: robustness}
    \vspace*{-2mm}
\end{figure}

% \begin{figure}[htb]
% % \vspace{-3mm}
% \centering
% \begin{tabular}[b]{ccc}
%         \includegraphics[width=.28\linewidth, trim=70 10 70 10]{Figures/visualization/SMUG_recon.pdf}
%         &
%         \includegraphics[width=.28\linewidth, trim=70 10 70 10]{Figures/visualization/SMUG_undersampling_2.0_vis.pdf}
%         &
%         \includegraphics[width=.28\linewidth, trim=70 10 70 10]{Figures/visualization/SMUG_unrolling_steps_16_clean_vis.pdf}
%         \\[-0pt]
%         \scriptsize{(a)} 
%         & 
%         \scriptsize{(b)} 
%         &  
%         \scriptsize{(c)}
% \end{tabular}
% \vspace*{-2mm}
% \caption{\footnotesize{Visualization of reconstruction results of {\us} for different undersampling rates and   unrolling steps. (a)  Reconstructed image from the benign measurement; (b) Reconstructed image when   using $2\times$ acceleration (corresponding to Fig.\,\ref{fig: robustness}-Left); (c)  Reconstructed image   when   using 16 unrolling steps (corresponding to Fig.\,\ref{fig: robustness}-Right).
% \SL{[caption is wrong, and consider to remove it.]}
% }}
% \label{fig: smug weakness}
% \end{figure}

% \begin{table}[htb]
% \centering
% % \vspace*{-1em}
% \caption{Clean accuracy and robust accuracy of {\us} with different $\sigma$. The robust accuracy is tested with adversarial perturbation $||\delta||_2 \le 0.5/255$.
% % The best performance is highlighted in \textbf{bold}.
% }
% \label{tab: ablation_sigma}
% \resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{c|ccc|ccc}
% \toprule[1pt]
% \midrule
% $\sigma$ & \multicolumn{3}{c}{Clean Accuracy} & \multicolumn{3}{c}{Robust Accuracy} \\
% Metrics & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ \\
% \midrule
% 0.001 
% & 0.0274 
% & 29.5286 
% & 0.8943 
% & 0.0302 
% & 28.464 
% & 0.7916
% \\
% 0.005 & 0.0282 & 29.2658 & 0.8929 & 0.0311 & 28.3198 & 0.8594
% \\
% 0.01 & 0.0278 & 29.3913 & 0.8939 & 0.0308 & 28.4137 & 0.8467
% \\
% 0.05 & 0.0325 & 28.0298 & 0.874 & 0.0333 & 27.7624 & 0.8499
% \\
% 0.1 & 0.0322 & 28.0921 & 0.8764 & 0.0325 & 28.0224 & 0.8746
% \\
% \midrule
% \bottomrule[1pt]
% \end{tabular}%
% }
% \vspace*{-3mm}
% \end{table}

% \begin{table}[htb]
% \centering
% % \vspace*{-1em}
% \caption{Clean accuracy and robust accuracy of {\us} with different Monte Carlo sampling number $n$. The robust accuracy is tested with adversarial perturbation $||\delta||_2 \le 0.5/255$.
% % The best performance is highlighted in \textbf{bold}.
% }
% \label{tab: ablation_n}
% \resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{c|ccc|ccc}
% \toprule[1pt]
% \midrule
% $n$ & \multicolumn{3}{c}{Clean Accuracy} & \multicolumn{3}{c}{Robust Accuracy} \\
% Metrics & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ \\
% \midrule
% 3 & 0.0306 & 28.5343 & 0.8815 & 0.0324 & 27.9376 & 0.8235
% \\
% 5 & 0.0292 & 28.9498 & 0.8886 & 0.0315 & 28.1755 & 0.8301
% \\
% 10 & 0.0278 & 29.3913 & 0.8939 & 0.0308 & 28.4137 & 0.8467
% \\
% \midrule
% \bottomrule[1pt]
% \end{tabular}%
% }
% \vspace*{-3mm}
% \end{table}

%\subsection{Ablation study}



\begin{wrapfigure}{r}{43mm}
\vspace*{-6mm}
\centering
  \hspace*{-3mm}  \includegraphics[width=0.25\textwidth]{Figures/reference_image_PSNR.pdf}
    \vspace*{-4mm}
    \caption{\footnotesize{PSNR vs. adversarial attack strength ($\epsilon)$ of {\us} for different configurations of   UStab loss \eqref{eq: unrolling loss}. %$\mathcal{D}_{{\btheta}_{\textsc{MoDL}}}$ represents the denoiser in vanilla {\modl} \eqref{eq:altmin}. The case of $\epsilon =0$ corresponds to clean accuracy. 
    }}
    \label{fig: reference_PSNR}
  %  \end{figure}
  \vspace*{-5mm}
\end{wrapfigure}
We conduct additional experiments showing the importance of integrating    target image denoising into {\us}'s training pipeline in \eqref{eq: unrolling loss}. \textbf{Fig.\,\ref{fig: reference_PSNR}} shows PSNR versus perturbation strength ($\epsilon$) when using different alternatives to $\mathcal D_{\btheta} (\mathbf t)$ in~\eqref{eq: unrolling loss}, including 
$\mathbf t$ (the original target image), $\mathcal D_{\btheta}(\mathbf x_n)$ (denoised   output of each unrolling step), and their variants when using the fixed, vanilla {\modl}'s denoiser $\mathcal D_{\btheta_\text{\modl}}$ instead.
% we 
% on different reference images in UStab loss \eqref{eq: unrolling loss} instead of $\mathcal D_{\btheta}(\mathbf t)$, whose results are shown in Fig. \ref{fig: reference_PSNR}. 
As we can see, the performance of {\us} varies when the UStab loss \eqref{eq: unrolling loss} is configured differently. The proposed $\mathcal D_{\btheta}(\mathbf t)$ outperforms the other baselines. A possible reason is that it infuses supervision of target images in an adaptive, denoising-friendly manner, \textit{i.e.}, taking influence of $\mathcal D_{\btheta}$ into consideration.
%We suspect $\mathcal D_{\btheta}(\mathbf t)$ countacts the influence of the instability of $\cD_{\btheta}$ in the training process and makes training aggressive by having $\mathbf{t}$ in it.


% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak
\vspace*{-4mm}
