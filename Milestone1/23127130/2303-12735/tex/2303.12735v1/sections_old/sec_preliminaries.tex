\section{Preliminaries and Problem Statement}
\label{sec: preliminaries}

In this section, we provide a brief background on MRI reconstruction and randomized smoothing. We will then motivate the lack of robustness   of MRI reconstruction and the problem of our interest. 


\vspace*{1mm}
\noindent \textbf{Setup of MRI reconstruction.} MRI reconstruction is an 
 ill-posed inverse problem \cite{compress}, which aims to reconstruct the original signal from its sparse observations. 
 In the context of multi-coil MRI reconstruction, let $\bx\in\mathbb{C}^{q}$ and $\mathbf y_c \in  \mathbb{C}^p$ (with $p < q$) denote the image to be recovered and its measurements associated with the $c$th  magnetic coil. We can cast the problem of MRI reconstruction  as 
%  can be described roughly as a model-fitting method to reconstruct   image with a data-consistency term. This is typically accomplished by adding a regularization term that enforces the sparsity-inducing prior on $\by$.
% The regularizer imposes additional constraints on the desired type of solution, resulting in an eventual solution that is more stable. 
% For multi-coil MRI reconstruction of an image 
% $\bx\in\mathbb{C}^{q}$, the optimization problem can be mathematically framed as
{
%\vspace*{-3mm}
\begin{align}
    \hat{\bx}=\underset{\bx}{\arg\min} ~\sum_{c=1}^{N_c}\|A_c \bx - \by_c \|^{2}_2 + \lambda \mathcal{R}(\bx),
    \label{eq:inv_pro}
\end{align}
}%
where $ N_c$ is the number of magnetic coils, $\mathcal{R}(\cdot)$ is a regularization function (\textit{e.g.}, $\ell_1$ norm to impose a sparse prior), and $\lambda > 0$ is a regularization parameter.  
In \eqref{eq:inv_pro},  $A_c = M \mathcal{F} S_c$ denotes   the forward system   operation,
where $M \in \{0,1\}^{p\times q}$ is a given sampling mask, 
$\mathcal{F}\in \mathbb{C}^{q\times q}$ is 
the $2D$ Fourier transformation, and $S_c \in 
\mathbb{C}^{q\times q}$ is a complex-valued position-dependent sensitivity map. 

% And the   regularizer $\mathcal{R}(\cdot)$  in \eqref{eq:inv_pro} is used to restrict the 
% solutions to the space of desirable
% images, and $\lambda$ is the regularization parameter controlling the trade-off between the residual norm and regularity.

% Choices for the regularizer in MRI reconstruction can vary from the $\ell_1$ penalty on wavelet coefficients~\cite{wave} or the most widely used CS method is total variation~\cite{totalv} denoising which enforces piecewise constant images by uniformly penalizing image gradients.

\subsection{MoDL and  the lack of robustness}
\label{sec: modl}


{\modl} relies on a formulation similar to \eqref{eq:inv_pro}, where the hand-crafted image regularization penalties in \eqref{eq:inv_pro} are replaced with learned priors. {\modl} has 
shown promise for MRI reconstruction
by combining a denoising network with a data-consistency ($\mathcal{DC}$) module in each stage
\cite{aggarwal2018modl}.
In the {\modl} model, Problem~\eqref{eq:inv_pro} is following:
    
{
\vspace*{-3mm}
\begin{align}
    \hat{\bx}=\underset{\bx}{\arg\min} ~  \sum_{c=1}^{N_c} \|A_c \bx - \by_c\|_2^2 + \lambda \| \bx - \cD (\bm{\theta};\bx) \|_2^2,
\label{eq:altmin}    
\end{align}
}%
%where $\nu \geq 0$ weights the data-fidelity term and $\mu \geq 0$ weights the proximity of $x$ to $z$ above. 
The problem can be solved by alternating between updating $\bx$ and $\cD (\bm{\theta};\bx)$.
The {\modl} scheme performs the update of $\bx$ (\textit{i.e.}, $\mathcal{DC}$) by Conjugate Gradient descent, and the update of $\cD (\bm{\theta};\bx)$ involving with a CNN denoiser ($\cD (\cdot)$) that is applied to the updated $\bx$.


\SL{[This is an important part to motivate our work. You need to define adversarial perturbations, undersampling rates, and unrolling steps, and associate them with math notations.]}
{\modl} is unstable against adversarial perturbations, different undersampling rates
% (When the sampling factor equals four,it will only collect $8\%$ of central k-space lines)
, and different unrolling steps
% ( $k$ for the number of alternative step between CNN denoiser update and Data consistence update) 
with training setup. 
The adversarial perturbation refers to a small, almost invisible to naked eyes, perturbation $\delta$ added to the input image $\bx_0$, which is constructed to deteriorate the performance of the model, as
\begin{equation}
    % \delta=\underset{\delta\sim\mathcal{S}}{\arg\max}~\cL(f(\bx+\delta), f(\bx))
    \underset{||\delta||_\infty\le r}{\arg\max}~\cL(f(\bx_0+\delta), f(\bx_0))
\end{equation}
% where $\mathcal{S}$ refers to a certain sphere around the data point $\bx$ with some distance measure.
where $f$ is the reconstruction model, and $r$ is the distance measure of the perturbation. Different undersampling rates refer to 
the new undersampling mask $M$ with different density is used to obtain the k-space measurements $\mathbf{y}_c$ from the ground truth in \eqref{eq:altmin} in the evaluation stage. And different unrolling steps mean a different number of iterations is used to solve \eqref{eq:altmin} in the evaluation stage.
In Fig. \ref{fig: weakness}, the reconstructed images blurs when adversarial perturbations are added to the undersampled input, a denser undersampling mask is used, and a larger number of unrolling steps is adopted in {\modl}.
 
\begin{figure}[htb]
%\vspace*{-3mm}
%    \centering
%    \begin{subfigure}{.10\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{Figures/output_vanilla_vis.pdf}
%        \caption{\footnotesize{clean data
%        }}
%    \end{subfigure}
%    \begin{subfigure}{.10\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{Figures/visualization/vanilla_MoDL_eps0.5_255.pdf}
%        \caption{\footnotesize{adversarial perturbations
%        }}
%    \end{subfigure}
%    \begin{subfigure}{.10\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{Figures/undersampling_vis.pdf}
%        \caption{\footnotesize{undersampling rate}}
%    \end{subfigure}
%    \begin{subfigure}{.10\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{Figures/unrolling_steps_vis.pdf}
%        \caption{\footnotesize{number of unrolling steps
%        }}
%    \end{subfigure}
    
%    \vspace*{-1em}
%    \caption{\footnotesize{{\modl}'s %instabiliites \textit{w.r.t.} adversarial perturbations, undersampling rate and number of unrolling steps}}
%    \label{fig: weakness}
%    \vspace*{-2mm}
\begin{tabular}[b]{cccc}
        \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/vanilla_MoDL_recon.pdf}
        &
        \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/vanilla_MoDL_eps0.5_255.pdf}
        &
        \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/undersampling_2.0_vis.pdf}
        &
        \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/unrolling_steps_16_clean_vis.pdf}
        \\[-0pt]
        \scriptsize{(a)} 
        & 
        \scriptsize{(b)} 
        &  
        \scriptsize{(c)} 
        &
        \scriptsize{(d)}

\end{tabular}
\vspace*{-5mm}
\caption{\footnotesize{{\modl}'s instabilities against adversarial perturbations, different undersampling rates, and different unrolling steps. (a) the reconstruction image from the clean image, (b) the reconstruction image from the adversarial perturbed image with the perturbation $||\delta||_{\infty}\le0.002$, (c) the reconstruction image when evaluated using a mask with the undersampling rate being 2 \hui{50\%?} (4 \hui{25\%?} in training), (d) the reconstruction image when evaluated using 16 unrolling steps (8 in training).}}
\label{fig: weakness}
\vspace*{-4.5mm}
\end{figure}

\subsection{Randomized smoothing} %and denoised smoothing}


Randomized smoothing \cite{cohen2019certified} uses a model's 
\SL{[why just classifier? makes it more general.]}
robustness to random noise to create a new model robust to adversarial perturbations. In classfication case, given a base classifier mapping from image domain $\mathbb R^d$ to classes $\mathcal Y$, the smoothed classifier $g$ 
%Given the CNN network by augmenting the data with noise ${x_{aliased}^{i},y^{i}}$
%\begin{equation}
%    f_{rs} = \frac{1}{K} \sum_{k=1}^{K} 
%    f_{\theta_{\text{noise}}}(x+\epsilon^{k}),\quad \epsilon\sim\mathcal N(0, \sigma^2I)
%\end{equation}
 returns the label which the base classifier $f$ is most likely to return when image $\bx$ is perturbed by Gaussian noise, as
is
{
\begin{align}
    g(\bx)=\underset{c\in\mathcal Y}{\arg\max}~ \mathbb P(f(\bx+\epsilon)=c),\quad \epsilon\sim\mathcal N(0, \sigma^2I)
    \label{eq: randomized smoothing}
\end{align}
}%
where the base classifier $f$ is trained with Gaussian noise augmented data, and Monte Carlo sampling is used to estimate the probability.

% Proved by Neyman-Pearson lemma, the smoothed classifier $g$ is certified to be robust to adversarial perturbations within a small $l_2$ radius compared to $\sigma$. This $l_2$ radius is related with the robustness of base classifier $f$ towards Gaussian noise. Consequently, in randomized smoothing, the base classifier is trained with Gaussian noise augmented data.

To avoid retraining the off-the-shelf model with Gaussian noise augmented data, paper \cite{salman2020denoised} proposed Denoised Smoothing where a denoiser $\cD (\cdot)$ is appended to the classifier $f$ to form a new base classifier $f \circ \cD$ robust to Gaussian noise. Then the smoothed classifier takes the majority vote over $f \circ \cD$, as
{
\begin{align}
    g(\bx)=\underset{c\in\mathcal Y}{\arg\max}~ \mathbb P(f(\cD (\bm{\theta};\bx + \epsilon))=c),\quad \epsilon\sim\mathcal N(0, \sigma^2I)
    \label{eq: denoised smoothing}
\end{align}
}

Noting that \eqref{eq: randomized smoothing} and \eqref{eq: denoised smoothing} cannot be directly used in MRI reconstruction tasks due to their regression-based learning objective, we convert \eqref{eq: denoised smoothing} into 
{
\begin{align}
    g(\bx)=\mathbb{E}_{\epsilon\sim\mathcal{N}(0, \sigma^2I)} f(\cD (\bm{\theta};\bx + \epsilon))
    \label{eq: denoised smoothing mri}
\end{align}
}
to get the smooth prediction.
%In denoised smoothing, the classifier $f$ is trained on clean data, and no data augmentation is needed. However, the denoiser $\cD (\cdot)$ is custom-trained to remove the Gaussian noise. They proposed two training objectives, mean square error (MSE) and stability (Stab). The latter takes the downstream classification task into account compared to the traditional MSE one. These two losses are defined as
%{\begin{align}
%    \cL_\mathrm{MSE}&=\mathbb{E}_{\epsilon\sim\mathcal N(0, \sigma^2I)}||\cD(\bm{\theta};\bx+\epsilon)-\bx||_2^2
%    \label{eq: MSE loss}
%    \\
%    \cL_\mathrm{Stab}&=\mathbb{E}_{\epsilon\sim\mathcal N(0, \sigma^2I)}||f(\cD(\bm{\theta};\bx+\epsilon))-f(\bx)||_2^2
    \label{eq: Stab loss}
%\end{align}}%

%MSE + Stab training scheme is used in denoised smoothing for efficiency and accuracy.

\SL{[why do we need to mention denoised smoothing? I did not get the point.]}