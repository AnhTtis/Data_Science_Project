\section{{\us}: SMoothed UnrollinG}
\label{sec: approach}

In this section, we describe our proposed {\us} method, in which we integrate {\modl} \cite{aggarwal2018modl} with randomized smoothing \cite{cohen2019certified}, to solve the instability problems mentioned in Sec. \ref{sec: preliminaries}. We first explicitly describe the architecture of our {\us} method in Sec. \ref{sec: unrolling}. Later, we elaborate on our training scheme in Sec. \ref{sec: how to smoothing}.

\subsection{Unrolling: key in smoothing on MoDL}
\label{sec: unrolling}

Following the denoised smoothing \cite{salman2020denoised}, we use a custom-trained denoiser to make the base model, \textit{i.e.} {\modl}, robust to Gaussian noise before applying randomized smoothing to generate robustness against adversarial perturbations. But instead of introducing a new denoiser, we exploit the denoiser in {\modl} for model simplicity and overall time efficiency. Compared to appending a new denoiser, our approach of reusing the existing denoiser in {\modl} is capable of removing Gaussian noises in every unrolling step, which enhances the Gaussian noise robustness of the base model and makes it more potential for randomized smoothing.

We then apply randomized smoothing \cite{cohen2019certified} to {\modl}. 
We present two smoothing architectures here: 1) {\us} on the whole ({\us} ($\cD + \mathcal{DC}$)), and 2) {\us}. {\us} is proposed to integrate randomized smoothing with {\modl} naturally. The differences between these architectures and \ref{eq: denoised smoothing mri} mainly focus on where to add the Gaussian noises and where to take the majority vote, \textit{i.e.} estimate the expectation in MRI reconstruction. Their comparison can also be seen in Fig. \ref{fig: model-arch}.
In \ref{eq: denoised smoothing mri}, the Gaussian noises $\epsilon$ are added on the input of the entire {\modl}, $\bx_0$, and the expectation \textit{w.r.t.} $\epsilon$ is estimated on the output of the entire {\modl}, $\bx_K$. In {\us}, however, the Gaussian noises $\epsilon$ are added on the input of \textit{every} unrolling step in {\modl}, $\bx_k$, and the expectations are estimated in \textit{every} unrolling step on the output of the whole unrolling step, \textit{i.e.} the output of the $\mathcal{DC}$, for {\us} on the whole, and the output of the denoiser for {\us}. 

\begin{figure}[htb]
% \vspace*{-3mm}
    \centering
    \begin{subfigure}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/E2E_Smoothing.pdf}
        \caption{\footnotesize{\ref{eq: denoised smoothing mri}
        }}
    \end{subfigure}
    
    \begin{subfigure}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/SMUG_whole.pdf}
        \caption{\footnotesize{{\us} on the whole
        }}
    \end{subfigure}

    \begin{subfigure}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/SMUG.pdf}
        \caption{\footnotesize{\us}}
    \end{subfigure}
    
    \vspace*{-1em}
    \caption{\footnotesize{The architecture of (a) \ref{eq: denoised smoothing mri}, (b) {\us} on the whole, and (c) {\us}.}}
    \label{fig: model-arch}
    % \vspace*{-4mm}
\end{figure}

% To be congruent with the presence of the denoiser in every unrolling step, we adopt an unrolling version of randomized smoothing, which we call {\us}. {\us} exploits the full potential of the unrolling presence of the denoiser in Gaussian noise robustness, thus obtaining high robustness for {\modl}. 

More concretely, our method of {\us} can be formularized as the following iterative algorithm, 
\begin{subequations}
\label{eq: unrolling smoothing}
\begin{align}
    \bz_k &= \mathbb{E}_{\epsilon\sim\mathcal{N}(0,\sigma^2I)} \cD(\bm{\theta}; \bx_k + \epsilon)
    \\
    \bx_{k+1} &=\underset{\bx}{\arg\min} \sum_{c=1}^{N_c}||A_c \bx - \by_c||_2^2 + \lambda ||\bx-\bz_k||^2
    \label{eq: unrolling smoothing optimization}
\end{align}
\end{subequations}
which is initialized with $\bx_0=A^H\by$, where $A^H$ transforms k-space data to the image domain. $\bz_k$ is estimated through the Monte Carlo sampling. In every unrolling step (\textit{i.e.} iteration), we first generate a number of Gaussian noises $\epsilon \sim \mathcal N(0, \sigma I^2)$, and add it to $\bx_k$. Then, we estimate the expectation of the denoised version of these images.
The denoiser $\cD (\cdot)$ is different from that in {\modl}, because the denoiser here is custom-trained to remove the Gaussian noises as well as alias artifacts and noises obtained in the undersampling and the upstream image processing. 
Finally, the conjugate gradient method is used to solve the optimization problem \eqref{eq: unrolling smoothing optimization}.


We evaluate the effectiveness of the three architectures towards adversarial robustness, and the results are shown in Table \ref{tab: exp_smoothing} and Fig. \ref{fig:archi_PSNR}. Experiment shows that {\us} has the highest robustness and comparable accuracy with other models. This implies the effectiveness of our proposed {\us}. We suspect that {\us} exploits the full potential of the presence of denoiser in each unrolling step compared to \ref{eq: denoised smoothing mri}, and avoids the noises introduced by the conjugate gradient method compared to {\us} on the whole.

\subsection{Unrolling loss}
\label{sec: how to smoothing}

The effectiveness of {\us} highly depends on the training scheme. We propose \textbf{unrolling loss} for better performance, as
{
\begin{align}
    \cL_{\mathrm{Unrolling}}&=\lambda'||\bx_K-\bx_\mathrm{label}||_2^2
    \notag
    \\
    +\sum_{k=0}^{K-1}&~\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\sigma^2I)}||\cD(\bm{\theta}; \bx_k+\epsilon)-\cD(\bm{\theta}; \bx_\mathrm{label})||^2_2
    \label{eq: unrolling loss}
\end{align}
}%
where $\bx_K$ is the output of the $K$-th and the final unrolling step, \textit{i.e.} the final reconstructed image, $\bx_\mathrm{label}$ is the ground truth image. Inspired by TRADES \cite{zhang2019theoretically}, our \textbf{unrolling loss} contains a robustness regularizer besides a residual norm for accuracy, and a hyper-parameter $\lambda'$ is introduced to balance the trade-off between accuracy and robustness. When $\lambda'=0$, the \textbf{unrolling loss} exploits the full potential of our {\us} architecture towards robustness; when $\lambda'$ approaches $+\infty$, the \textbf{unrolling loss} penalties the model's inaccuracy directly. The expectation here is, again, estimated through the Monte Carlo sampling using the same noises as the forwarding propagation.

Moreover, we do not only rely on \textbf{unrolling loss} to train {\us}. In general, we adopt MSE + Unrolling Stab scheme following MSE + Stab scheme in denoised smoothing \cite{salman2020denoised}. First, the denoiser is pre-trained on the ground truth image to remove added Gaussian noises. We use an MSE loss as denoised smoothing, but $\bx$ here is the ground truth image.
This pre-training only takes a small amount of time compared to the following fine-tuning. However, the pre-training can find a good starting point for fine-tuning, which leads to remarkably higher robustness of the final model.
Then, the denoiser is fine-tuned through a stability loss. Conventionally, an end-to-end (E2E) stability loss is used, as
\begin{equation}
    \cL_{\mathrm{E2E}} = ~\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\sigma^2I)}||f(\bm{\theta}; \bx, \epsilon) - f_\mathrm{base}(\bx)||_2^2%,\quad\epsilon\sim\mathcal{N}(0, \sigma I^2)
\end{equation}
where $f$ is the smoothed model, including \ref{eq: denoised smoothing mri}, and $f_\mathrm{base}$ is the base model, \textit{i.e.} the trained vanilla {\modl}. However, we adopt \textbf{unrolling loss} for fine-tuning. We study the effectiveness of our training scheme in Sec. \ref{sec: experiment}.

% We, instead, adopt an unrolling form of stability loss to exploit the unrolling architecture of {\us}, which we call \textit{unrolling loss}, as
% \begin{align}
%     \cL_{\mathrm{Unrolling}}&=\lambda'||\bx_K-\bx_\mathrm{label}||_2^2
%     \notag
%     \\
%     +\sum_{k=0}^{K-1}&~\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\sigma^2I)}||\cD(\bm{\theta}; \bx_k+\epsilon)-\cD(\bm{\theta}; \bx_\mathrm{label})||^2_2
%     \label{eq: unrolling loss}
% \end{align}
% where $\bx_K$ is the output of the $K$-th and the final unrolling step, \textit{i.e.} the final reconstructed image, $\bx_\mathrm{label}$ is the ground truth image. Inspired by TRADES \cite{zhang2019theoretically}, our unrolling loss contains a robustness regularizer besides one for accuracy in our unrolling loss, and $\lambda'$ is a hyper-parameter that balances the trade-off between accuracy and robustness. When $\lambda'=0$, the unrolling loss exploits the full potential of our {\us} architecture towards robustness; when $\lambda'$ approaches $+\infty$, the unrolling loss penalties the model's inaccuracy directly. The expectation here is, again, estimated through the Monte Carlo sampling using the same noises as the forwarding propagation.

We emphasize that $\cD(\bm{\theta}; \bx_\mathrm{label})$ in the unrolling loss as the reference image is carefully selected. Because of the convergence and added Gaussian noises, the denoiser $\cD(\cdot)$ and its input image $\bx_n + \epsilon$ are both unstable in the training process. Consequently, the choice for the reference image in the unrolling loss is significant.
We select $\cD(\bm{\theta};\bx_\mathrm{label})$ as the reference image, in which $\cD(\cdot)$ countacts the instability of the denoiser, and $\bx_\mathrm{label}$ makes the training more aggressive, promoting training efficiency.

% \vspace{-3mm}
