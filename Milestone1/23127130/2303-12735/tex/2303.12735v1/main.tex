\documentclass{article}

% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------

\usepackage{spconf,amsmath,amsfonts,amssymb,graphicx}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\card}{card}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator*{\minimize}{\text{minimize}}
\DeclareMathOperator*{\maximize}{\text{maximize}}
\usepackage{adjustbox} 
\input{math_commands.tex}
% \usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{tablefootnote}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{colortbl}
\usepackage{hyperref} 
\usepackage{bm}

\DeclareMathOperator*{\st}{\text{subject to}}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
% Example definitions.
% --------------------
% \def\x{{\mathbf x}}
% \def\L{{\cal L}}

\usepackage{color}
\newcommand{\SL}[1]{\textcolor{red}{SL: #1}}
\newcommand{\hui}[1]{\textcolor{blue}{Hui: #1}}
\newcommand{\JH}[1]{\textcolor{cyan}{Jinghan: #1}}
\newcommand{\yuguang}[1]{\textcolor{green}{yuguang: #1}}
\newcommand{\modl}{\textsc{MoDL}}
\newcommand{\us}{\textsc{SMUG}}
\newcommand{\usold}{\textsc{SMUGv0}}
\usepackage{cite}
\usepackage{tcolorbox}


% Title.
% ------

\title{SMUG: Towards Robust MRI Reconstruction by Smoothed Unrolling}
%
% Single address.
% ---------------
\name{Hui Li$^1$ \, Jinghan Jia$^2$ \, Shijun Liang$^2$ \, Yuguang Yao$^2$ 
\, Saiprasad Ravishankar$^{2}$ \,  Sijia Liu$^2$}
%\thanks{Thanks to XYZ agency for funding.}
\address{$^1$Huazhong University of Science and Technology, China
\\
%$^2$
%Dept. of Computer Science Engineering, 
$^2$Michigan State University, East Lansing, MI, USA 
% \\ $^3$Dept. of Biomedical Engineering, Michigan State University, East Lansing, MI\\ $^4$Dept. of Computational Mathematics, Science and Engineering, Michigan State University, East Lansing, MI.
% \\
% $^3$Dept. CMSE,  Michigan State University, USA
\thanks{$^1$The work is done during remote internship at MSU.}
}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
\ninept
%
\maketitle
%


% \newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
% \newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
% \def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}


%\newcolumntype{M{>{\centering\arraybackslash}m{\dimexpr.25\linewidth-2\tabcolsep}}


\newcommand{\Def}[0]{\mathrel{\mathop:}=}


\newcommand{\Blue}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}
\newcommand{\tiger}[1]{\Blue{#1}}
 
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\din}{\mathcal D^{\texttt{tr}}}
\newcommand{\dint}{\tilde{\mathcal D}^{\texttt{tr}}}
\newcommand{\dout}{\mathcal D^{\texttt{val}}}
\newcommand{\doutt}{\tilde{\mathcal D}^{\texttt{val}}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bphi}{\boldsymbol{\phi}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\grad}{\nabla}
\newcommand{\egrad}{\widehat{\nabla}}
\newcommand{\task}{\mathcal{T}}
\newcommand{\rnn}{\text{{\tt RNN}}\xspace}
\newcommand{\EI}{\text{EI}}
\newcommand{\lopt}{\text{LO}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\pphi}[1]{\frac{\partial #1}{\partial \bphi}}


%\renewcommand{\baselinestretch}{0.92}

\newcommand{\pre}{\textsc{p}}
\newcommand{\ft}{\textsc{f}}
\newcommand{\adv}{\textsc{a}}
\newcommand{\sta}{\textsc{n}}
\newcommand{\super}{\textsc{s}}
\newcommand{\self}{\textsc{ss}}
\newcommand{\Sp}{\textit{supervised}}
\newcommand{\Ss}{\textit{self-supervised}}

\newcommand{\cL}{\mathcal{L}}
\newcommand{\cD}{\mathcal{D}}

\begin{abstract}
% \JH{Should we mention robustness against unrolling steps, sampling rate?}
Although deep learning ({DL}) has gained much popularity for accelerated magnetic resonance imaging ({MRI}), recent studies have shown that DL-based MRI reconstruction models could be over-sensitive to  tiny input perturbations (that are called `adversarial perturbations'), which cause unstable, low-quality reconstructed images.
This raises the question of how to design robust DL methods for  MRI reconstruction. 
To address this problem, we propose  a novel image reconstruction framework, termed \textsc{\underline{Sm}oothed \underline{U}nrollin\underline{g}} ({\us}), which advances a deep unrolling-based MRI reconstruction model using a randomized smoothing (RS)-based robust learning operation. RS, which improves the tolerance of a model against input noises, has been widely used in the design of adversarial defense for image classification. Yet, we find that  the conventional design that applies RS to  the entire DL process is ineffective for  MRI reconstruction. We show that {\us} addresses the above issue by customizing the RS operation based on the unrolling  architecture  of the DL-based MRI reconstruction model. Compared to   the vanilla RS approach and several variants of {\us},  we show that 
{\us} improves the robustness of MRI reconstruction with respect to a diverse set of  perturbation sources, including perturbations to input measurements, different measurement sampling rates, and different unrolling steps. Code for {\us} will be available at \texttt{\url{https://github.com/LGM70/SMUG}}.
%Although RS has been widely used in adversarial defense for image classification, 
%the problem of MRI reconstruction imposes two challenges: 
% We find that   the tradition method of mounting RS to the end-to-end DL process (like RS for robust image classification) become ineffective to the deep unrolling-based MRI reconstruction. Thus, a careful design is needed to integrate RS into MRI reconstruction. We show that 

% tiny  perturbations to the input image and t transformations, or changes in the number of samples will lead to the instabilities of DL-based MRI reconstruction models. 

% In this paper, we revisit randomized smoothing (RS) on the DL-based MRI reconstruction models; inspired by the special architecture of deep unrolling-based MRI reconstruction models, we propose a novel framework, called \textsc{\underline{Sm}ooth \underline{U}nrollin\underline{g}} ({\us}), that can integrate RS and deep unrolling-based MRI reconstruction models naturally to enhance the robustness of DL-based MRI reconstruction models further compared to the origin randomized smoothing method. We also conduct extensive experiments to demonstrate the effectiveness of our proposed methods.

%{inspired by and extended from the celebrated randomized smoothing (RS) technique in adversarial learning %{\hui{adversarial defense?}}.} 
%{Nonetheless, unlike conventional RS-based robustification techniques for classification networks,we work with deep unrolling-based reconstruction networks (\textit{e.g.}, {\modl}) and we demonstrate that {\us} with unrolling loss training scheme improve the robustness of MODL}

\end{abstract}
%
\begin{keywords}
Magnetic resonance imaging (MRI), machine learning, deep unrolling, adversarial robustness, randomized smoothing.
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
%\SL{[1) Review the importance of DL-based image reconstruction; 2) Despite its high-accuracy, suffers the vulnerability to adversarial attacks. [Need attack definition]. However, nearly all of work towards robust image reconstruction focused on prediction-evasion attacks. 3) Then define and introduce `train-time poisoning attacks'. Why is challenging in image reconstruction. How you address and contributions.]}

%\SL{[Need more references/related work.]}

% \noindent \textbf{Relevant works.}


%\SL{[Needs to rewrite Introduction. (1) One paragraph for DL in MRI reconstruction, and introduction of MoDL (this is our focused model), (2) one paragraph for lack of robustness of DL-based MRI reconstruction, (3) one paragraph for randomized smoothing (e.g., in image classification), and the most relevant work to ours. (4) state your contributions.]}

Magnetic resonance imaging (MRI) is a widely used imaging modality in clinical practice that is used to image both anatomical structures and physiological functions. However, the data collection in MRI is sequential and slow. Thus, many methods\cite{lustig2008compressed,yang2010fast,Aggarwal2019MoDL:Problems} have been developed to provide accurate image reconstructions from limited (rapidly collected) data.

Recently, deep learning (DL) has become a powerful tool   to solve  image reconstruction and inverse problems in general ~\cite{Schlemper2019Sigma-net:Reconstruction,Ravishankar2018DeepReconstruction,Aggarwal2019MoDL:Problems,Schlemper2018AReconstruction}.  In this paper, we focus on the application of DL to MRI reconstruction.
%This paper focuses on the application of DL to magnetic resonance imaging (MRI) reconstruction.
% such as X-ray computed tomography and magnetic resonance imaging (MRI). 
% With the advent of deep learning-based ({DL-based}) approaches in computer vision tasks, convolutional neural networks have gained popularity in image denoising, classification for medical image diagnosis, and accelerated magnetic resonance imaging. 
%\SL{[The two types are unclear.]}
%The DL-based MRI reconstruction approaches can be roughly divided into two types. 
Among DL-based methods, image or sensor domain denoising networks are well-known. The most prevalent deep neural networks include the U-Net~\cite{Unet} and variants~\cite{han2018framing, lee2018deep} that are adapted to correct the artifacts in MRI reconstructions from undersampled data. 
Hybrid-domain methods that combine neural networks together with imaging physics such as forward models have become quite popular. 
One such state-of-the-art algorithm is the unrolled network scheme, MoDL~\cite{Aggarwal2019MoDL:Problems} that mimics an iterative algorithm to solve the regularized inverse problem in MRI reconstruction. Its variants have achieved top performance in recent open data-driven competitions. 
% This method consists of multiple iterations or blocks, as ``unrolled'' implies. 
 
However, many studies \cite{antun2020instabilities,zhang2021instabilities, gilton2021deep} have demonstrated that DL-based MRI reconstruction models suffer from a lack of robustness. It has been shown that DL-based models are vulnerable to tiny input perturbations \cite{antun2020instabilities, zhang2021instabilities}, changes in measurement sampling rate \cite{antun2020instabilities}, and changes in the number of iterations of the model~\cite{gilton2021deep}. In these scenarios, the reconstructed images generated by DL-based models are of poor quality, which may lead to false diagnoses and adverse clinical consequences.

Although many defense methods~\cite{madry2017towards,zhang2019theoretically,cohen2019certified,salman2020denoised} were proposed to address the lack of robustness of DL models on the image classification task, the approaches of robustifying DL-based MRI reconstruction models are under-developed due to their regression-based learning objectives.
%\SL{[This should be the most relevant work to ours!]}
Randomized smoothing (RS) 
% is the technique that used to improve and enhance the convergence rates of algorithms for non smooth convex optimization problems. 
and its variants~\cite{cohen2019certified, salman2020denoised,zhang2022robustify} are quite popular adversarial defense methods in image classification. Different from conventional defense methods\cite{madry2017towards,zhang2019theoretically} which generate empirical robustness and are prone to fail against stronger attacks, RS  guarantees the model's robustness within a small sphere around the input image \cite{cohen2019certified}, which is vital for medical applications like MRI. A recent preliminary work attempted to apply RS to DL-based MRI reconstruction in an end-to-end (E2E) manner~\cite{wolfmaking}.  



%{However, Deep learning techniques for MRI reconstruction are imperfect due to a significant robustness issue that MRI is based on sampling the Fourier transform.} 
% This finding indicates that it is difficult to generalize DL~\cite{antun2020instabilities,MRI_adv_train}.
%{Recently, there have been plenty of methods,including Adversarial Training, TRADES and Randomized Smoothing ~\cite{madry17, zhang2019theoretically,cohen2019certified,salman2020denoised,ryou2020fast,niu2020limitations,salman2019provably,zhao2020maximum}} 
%{was used to address the adversarial robustness issue, but they mainly focus on the image classification tasks.} 
%{For the reconstruction area, Nearly all of existing work focuses on generating and defending such {\it test-time reconstruction evasion attacks}   \cite{antun2020instabilities,raj2020improving,choi2019evaluating,bungert2020robust}. %However, few work studies \textit{train-time poisoning attack} in the context of image reconstruction. \hui{we are not of this type either.}}
%{Besides the adversarial robustness training, the random smoothing technique also has been used for improving and analyzing convergence rates of algorithms for non smooth convex optimization problems. All of the recent works \cite{salman2020denoised,cohen2019certified} is also focus on the classification problem.} 

%2. Robustness of DL-based models in MRI reconstruction

%instable \textit{w.r.t.} adversarial perturbations generated by PGD, \textit{w.r.t.} undersampling rate

%3. Adversarial defense method

%on image classification

%Adversarial training,
%TRADES,
%Randomized Smoothing and its variants

%on MRI reconstruction



% Image reconstruction is an inverse problem that constitutes the recovery of an image from finite indirect measurements and is used in many applications, including critical ones like medical imaging (eg: MRI, CT). Deep learning (DL) provides novel methods which try to solve the problem more accurately and with much lesser samples compared to classical methods \cite{schlemper2017deep,zhu2018image,strack2018ai}. This could potentially make MRI scans, for instance, much efficient and effective \cite{zbontar2018fastmri}.

% Adversarial perturbations are tiny changes in the data input that cause a large degradation in the performance of pre-trained networks \cite{Goodfellow2015explaining,carlini2017towards}. While these DL-based methods have achieved state-of-the-art (SOTA) performance on image reconstruction \cite{Muckley_2021}, these networks have  been also found to be very unstable and prone to adversarial perturbations \cite{antun2020instabilities,raj2020improving},  leading to large degradation in reconstruction accuracy at testing time. 
% Nearly all of   existing work focuses on generating and defending such {\it test-time reconstruction evasion attacks}   \cite{antun2020instabilities,raj2020improving,choi2019evaluating,bungert2020robust}. However, few work studies \textit{train-time poisoning attack} in the context of image reconstruction.  

% \noindent \textbf{Approaches.} 
Given the advantages of RS and deep unrolling-based (hybrid domain) image reconstructors, we propose a novel approach dubbed \textsc{\underline{Sm}oothed \underline{U}nrollin\underline{g}} ({\us}) to mitigate the lack of robustness of DL-based MRI reconstruction models by systematically integrating RS into {\modl}~\cite{Aggarwal2019MoDL:Problems} architectures.
%To mitigate the problems mentioned above, we propose {\us}, which integrates RS \cite{cohen2019certified} with {\modl} \cite{aggarwal2018modl}. 
Instead of inefficient conventional \ref{eq: denoised smoothing mri} \cite{wolfmaking}, we apply  RS  in every unrolling step and on intermediate unrolled denoisers in {\modl}. We follow the `pre-training + fine-tuning' technique \cite{zoph2020rethinking,salman2020denoised}, adopting a mean square error (MSE) loss for pre-training and proposing an unrolling stability (UStab) loss along with the vanilla {\modl} reconstruction loss for fine-tuning.
Different from the existing art, our \textbf{contributions} are summarized as follows.\\
% \noindent $\bullet$ We show that {\modl} suffers from three types of instabilities upon adversarial perturbations, different undersampling rates, and different unrolling steps. 
\noindent $\bullet$ \ We propose {\us} that systematically integrates RS with {\modl} using an deep unrolled architecture.\\
\noindent $\bullet$ \  We study in detail where to apply RS in the unrolled architecture for better performance and propose a novel unrolling loss to improve training efficiency.\\
\noindent $\bullet$ We compared our methods with two related baselines: vanilla {\modl}~\cite{Aggarwal2019MoDL:Problems} and \ref{eq: denoised smoothing mri} \cite{wolfmaking}. Extensive experiments demonstrate the significant effectiveness of our proposed method on the major types of instabilities of {\modl}. 



% \section{Preliminaries and Problem Statement}
% \label{sec: preliminaries}

% In this section, we provide a brief background on MRI reconstruction and randomized smoothing. We will then motivate the lack of robustness   of MRI reconstruction and the problem of our interest. 


% \vspace*{1mm}
% \noindent \textbf{Setup of MRI reconstruction.} MRI reconstruction is an 
%  ill-posed inverse problem \cite{compress}, which aims to reconstruct the original signal from its sparse observations. 
%  In the context of multi-coil MRI reconstruction, let $\bx\in\mathbb{C}^{q}$ and $\mathbf y_c \in  \mathbb{C}^p$ (with $p < q$) denote the image to be recovered and its measurements associated with the $c$th  magnetic coil. We can cast the problem of MRI reconstruction  as 
% %  can be described roughly as a model-fitting method to reconstruct   image with a data-consistency term. This is typically accomplished by adding a regularization term that enforces the sparsity-inducing prior on $\by$.
% % The regularizer imposes additional constraints on the desired type of solution, resulting in an eventual solution that is more stable. 
% % For multi-coil MRI reconstruction of an image 
% % $\bx\in\mathbb{C}^{q}$, the optimization problem can be mathematically framed as
% {
% %\vspace*{-3mm}
% \begin{align}
%     \hat{\bx}=\underset{\bx}{\arg\min} ~\sum_{c=1}^{N_c}\|A_c \bx - \by_c \|^{2}_2 + \lambda \mathcal{R}(\bx),
%     \label{eq:inv_pro}
% \end{align}
% }%
% where $ N_c$ is the number of magnetic coils, $\mathcal{R}(\cdot)$ is a regularization function (\textit{e.g.}, $\ell_1$ norm to impose a sparse prior), and $\lambda > 0$ is a regularization parameter.  
% In \eqref{eq:inv_pro},  $A_c = M \mathcal{F} S_c$ denotes   the forward system   operation,
% where $M \in \{0,1\}^{p\times q}$ is a given sampling mask, 
% $\mathcal{F}\in \mathbb{C}^{q\times q}$ is 
% the $2D$ Fourier transformation, and $S_c \in 
% \mathbb{C}^{q\times q}$ is a complex-valued position-dependent sensitivity map. 

% % And the   regularizer $\mathcal{R}(\cdot)$  in \eqref{eq:inv_pro} is used to restrict the 
% % solutions to the space of desirable
% % images, and $\lambda$ is the regularization parameter controlling the trade-off between the residual norm and regularity.

% % Choices for the regularizer in MRI reconstruction can vary from the $\ell_1$ penalty on wavelet coefficients~\cite{wave} or the most widely used CS method is total variation~\cite{totalv} denoising which enforces piecewise constant images by uniformly penalizing image gradients.

% \subsection{MoDL and  the lack of robustness}
% \label{sec: modl}


% {\modl} relies on a formulation similar to \eqref{eq:inv_pro}, where the hand-crafted image regularization penalties in \eqref{eq:inv_pro} are replaced with learned priors. {\modl} has 
% shown promise for MRI reconstruction
% by combining a denoising network with a data-consistency ($\mathcal{DC}$) module in each stage
% \cite{aggarwal2018modl}.
% In the {\modl} model, Problem~\eqref{eq:inv_pro} is following:
    
% {
% \vspace*{-3mm}
% \begin{align}
%     \hat{\bx}=\underset{\bx}{\arg\min} ~  \sum_{c=1}^{N_c} \|A_c \bx - \by_c\|_2^2 + \lambda \| \bx - \cD (\bm{\theta};\bx) \|_2^2,
% \label{eq:altmin}    
% \end{align}
% }%
% %where $\nu \geq 0$ weights the data-fidelity term and $\mu \geq 0$ weights the proximity of $x$ to $z$ above. 
% The problem can be solved by alternating between updating $\bx$ and $\cD (\bm{\theta};\bx)$.
% The {\modl} scheme performs the update of $\bx$ (\textit{i.e.}, $\mathcal{DC}$) by Conjugate Gradient descent, and the update of $\cD (\bm{\theta};\bx)$ involving with a CNN denoiser ($\cD (\cdot)$) that is applied to the updated $\bx$.


% \SL{[This is an important part to motivate our work. You need to define adversarial perturbations, undersampling rates, and unrolling steps, and associate them with math notations.]}
% {\modl} is unstable against adversarial perturbations, different undersampling rates
% % (When the sampling factor equals four,it will only collect $8\%$ of central k-space lines)
% , and different unrolling steps
% % ( $k$ for the number of alternative step between CNN denoiser update and Data consistence update) 
% with training setup. 
% The adversarial perturbation refers to a small, almost invisible to naked eyes, perturbation $\delta$ added to the input image $\bx_0$, which is constructed to deteriorate the performance of the model, as
% \begin{equation}
%     % \delta=\underset{\delta\sim\mathcal{S}}{\arg\max}~\cL(f(\bx+\delta), f(\bx))
%     \underset{||\delta||_\infty\le r}{\arg\max}~\cL(f(\bx_0+\delta), f(\bx_0))
% \end{equation}
% % where $\mathcal{S}$ refers to a certain sphere around the data point $\bx$ with some distance measure.
% where $f$ is the reconstruction model, and $r$ is the distance measure of the perturbation. Different undersampling rates refer to 
% the new undersampling mask $M$ with different density is used to obtain the k-space measurements $\mathbf{y}_c$ from the ground truth in \eqref{eq:altmin} in the evaluation stage. And different unrolling steps mean a different number of iterations is used to solve \eqref{eq:altmin} in the evaluation stage.
% In Fig. \ref{fig: weakness}, the reconstructed images blurs when adversarial perturbations are added to the undersampled input, a denser undersampling mask is used, and a larger number of unrolling steps is adopted in {\modl}.
 
% \begin{figure}[htb]
% %\vspace*{-3mm}
% %    \centering
% %    \begin{subfigure}{.10\textwidth}
% %        \centering
% %        \includegraphics[width=\textwidth]{Figures/output_vanilla_vis.pdf}
% %        \caption{\footnotesize{clean data
% %        }}
% %    \end{subfigure}
% %    \begin{subfigure}{.10\textwidth}
% %        \centering
% %        \includegraphics[width=\textwidth]{Figures/visualization/vanilla_MoDL_eps0.5_255.pdf}
% %        \caption{\footnotesize{adversarial perturbations
% %        }}
% %    \end{subfigure}
% %    \begin{subfigure}{.10\textwidth}
% %        \centering
% %        \includegraphics[width=\textwidth]{Figures/undersampling_vis.pdf}
% %        \caption{\footnotesize{undersampling rate}}
% %    \end{subfigure}
% %    \begin{subfigure}{.10\textwidth}
% %        \centering
% %        \includegraphics[width=\textwidth]{Figures/unrolling_steps_vis.pdf}
% %        \caption{\footnotesize{number of unrolling steps
% %        }}
% %    \end{subfigure}
    
% %    \vspace*{-1em}
% %    \caption{\footnotesize{{\modl}'s %instabiliites \textit{w.r.t.} adversarial perturbations, undersampling rate and number of unrolling steps}}
% %    \label{fig: weakness}
% %    \vspace*{-2mm}
% \begin{tabular}[b]{cccc}
%         \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/vanilla_MoDL_recon.pdf}
%         &
%         \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/vanilla_MoDL_eps0.5_255.pdf}
%         &
%         \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/undersampling_2.0_vis.pdf}
%         &
%         \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/unrolling_steps_16_clean_vis.pdf}
%         \\[-0pt]
%         \scriptsize{(a)} 
%         & 
%         \scriptsize{(b)} 
%         &  
%         \scriptsize{(c)} 
%         &
%         \scriptsize{(d)}

% \end{tabular}
% \vspace*{-5mm}
% \caption{\footnotesize{{\modl}'s instabilities against adversarial perturbations, different undersampling rates, and different unrolling steps. (a) the reconstruction image from the clean image, (b) the reconstruction image from the adversarial perturbed image with the perturbation $||\delta||_{\infty}\le0.002$, (c) the reconstruction image when evaluated using a mask with the undersampling rate being 2 \hui{50\%?} (4 \hui{25\%?} in training), (d) the reconstruction image when evaluated using 16 unrolling steps (8 in training).}}
% \label{fig: weakness}
% \vspace*{-4.5mm}
% \end{figure}

% \subsection{Randomized smoothing} %and denoised smoothing}


% Randomized smoothing \cite{cohen2019certified} uses a model's 
% \SL{[why just classifier? makes it more general.]}
% robustness to random noise to create a new model robust to adversarial perturbations. In classfication case, given a base classifier mapping from image domain $\mathbb R^d$ to classes $\mathcal Y$, the smoothed classifier $g$ 
% %Given the CNN network by augmenting the data with noise ${x_{aliased}^{i},y^{i}}$
% %\begin{equation}
% %    f_{rs} = \frac{1}{K} \sum_{k=1}^{K} 
% %    f_{\theta_{\text{noise}}}(x+\epsilon^{k}),\quad \epsilon\sim\mathcal N(0, \sigma^2I)
% %\end{equation}
%  returns the label which the base classifier $f$ is most likely to return when image $\bx$ is perturbed by Gaussian noise, as
% is
% {
% \begin{align}
%     g(\bx)=\underset{c\in\mathcal Y}{\arg\max}~ \mathbb P(f(\bx+\epsilon)=c),\quad \epsilon\sim\mathcal N(0, \sigma^2I)
%     \label{eq: randomized smoothing}
% \end{align}
% }%
% where the base classifier $f$ is trained with Gaussian noise augmented data, and Monte Carlo sampling is used to estimate the probability.

% % Proved by Neyman-Pearson lemma, the smoothed classifier $g$ is certified to be robust to adversarial perturbations within a small $l_2$ radius compared to $\sigma$. This $l_2$ radius is related with the robustness of base classifier $f$ towards Gaussian noise. Consequently, in randomized smoothing, the base classifier is trained with Gaussian noise augmented data.

% To avoid retraining the off-the-shelf model with Gaussian noise augmented data, paper \cite{salman2020denoised} proposed Denoised Smoothing where a denoiser $\cD (\cdot)$ is appended to the classifier $f$ to form a new base classifier $f \circ \cD$ robust to Gaussian noise. Then the smoothed classifier takes the majority vote over $f \circ \cD$, as
% {
% \begin{align}
%     g(\bx)=\underset{c\in\mathcal Y}{\arg\max}~ \mathbb P(f(\cD (\bm{\theta};\bx + \epsilon))=c),\quad \epsilon\sim\mathcal N(0, \sigma^2I)
%     \label{eq: denoised smoothing}
% \end{align}
% }

% Noting that \eqref{eq: randomized smoothing} and \eqref{eq: denoised smoothing} cannot be directly used in MRI reconstruction tasks due to their regression-based learning objective, we convert \eqref{eq: denoised smoothing} into 
% {
% \begin{align}
%     g(\bx)=\mathbb{E}_{\epsilon\sim\mathcal{N}(0, \sigma^2I)} f(\cD (\bm{\theta};\bx + \epsilon))
%     \label{eq: denoised smoothing mri}
% \end{align}
% }
% to get the smooth prediction.
% %In denoised smoothing, the classifier $f$ is trained on clean data, and no data augmentation is needed. However, the denoiser $\cD (\cdot)$ is custom-trained to remove the Gaussian noise. They proposed two training objectives, mean square error (MSE) and stability (Stab). The latter takes the downstream classification task into account compared to the traditional MSE one. These two losses are defined as
% %{\begin{align}
% %    \cL_\mathrm{MSE}&=\mathbb{E}_{\epsilon\sim\mathcal N(0, \sigma^2I)}||\cD(\bm{\theta};\bx+\epsilon)-\bx||_2^2
% %    \label{eq: MSE loss}
% %    \\
% %    \cL_\mathrm{Stab}&=\mathbb{E}_{\epsilon\sim\mathcal N(0, \sigma^2I)}||f(\cD(\bm{\theta};\bx+\epsilon))-f(\bx)||_2^2
%     \label{eq: Stab loss}
% %\end{align}}%

% %MSE + Stab training scheme is used in denoised smoothing for efficiency and accuracy.

% \SL{[why do we need to mention denoised smoothing? I did not get the point.]}

%%
\input{sections_SL/sec_preliminaries}
%\input{sections_old/sec_preliminaries}
%%
\input{sections_SL/sec_method}
%%%

%%%
\input{sections_SL/sec_experiments}

% \section{Experiments}
% \label{sec: experiment}

% % \SL{[A summary paragraph to summarize your key metrics and results. E.g., In this section, we evaluate the effectiveness of our proposal from the following aspects xxxx. With a detailed comparison with the heuristics-based poisoning strategy, we find that xxx. ]}
% %In this section, we evaluate different smoothing architectures and different training schemes discussed in Sec. \ref{sec: unrolling} and Sec. \ref{sec: how to smoothing} respectively. And we demonstrate the effectiveness of our proposed {\us} in three aspects of robustness mentioned in Sec. \ref{sec: preliminaries}.
% % Result show that our proposed {\us} outperforms all other baselines in these three aspects.

% % \vspace{-3mm}
% \subsection{Experiment setup} 
% %\vspace*{1mm}
% \noindent \textbf{Datasets \& model architectures.} 
% Following the 
% Overall model architecture has been shown in Fig. \ref{fig: model-arch}. The number of unrolling steps in {\modl} $K$ and hyper-parameter $\lambda$ are set to 8 and 1 respectively to reach high reconstruction quality and time efficiency as well. We use Deep Iterative Down-Up Network (DIDN) \cite{yu2019deep} with three down-up blocks and 64 channels as denoiser $\cD(\cdot)$, and the conjugate gradient method with tolerance $1e^{-6}$ to solve the optimization problem in $\mathcal{DC}$ block. We use part of \texttt{fastMRI} \cite{zbontar2018fastmri} multi-coil knee validation dataset for evaluation. The k-space data $\by$ has 15 coils and is cropped to $320\times320$ for reconstruction. We adopt a Cartesian mask  at 4x acceleration (25.0\% sampling). Also, the coil sensitivity maps for all cases are obtained using the BART toolbox \cite{tamir2016generalized}
% % \JH{add references}
% , and all the coil sensitivity maps are estimated from undersampled data to stimulate the realistic representation of real MRI experiments.

% % \vspace*{-4mm}
% \vspace*{1mm}
% \noindent \textbf{Training \& evaluation.}
% % Do we need to use the past tense?
% We used 304 images for training and 32 images for validation. We used a batch size of 2 which were simultaneously trained on 2 GPUs. For every batch of images, the same noises $\epsilon$ were used in all unrolling steps. The models were trained using the Adam optimizer with parameter $\beta\text{s}=[0.5,0.999]$. The number of epochs is set to 60 with a linearly decaying learning rate from $1e^{-4}$ to 0 after epoch 20. The hyper-parameter $\lambda'$ is selected, if possible, to generate similar accuracy in all scenarios.
% $\sigma$ used in Gaussian noise sampling is set to 0.01 to balance accuracy with robustness, and Monte Carlo sampling number $n$ is set to 10 due to the memory limitation of our GPUs.

% % \vspace*{-4mm}
% %\paragraph*{Evaluation setup.}
% % \SL{[mention baselines here and metrics to evaluate the performance]}

% We evaluate our methods on clean images and adversarial examples generated by 10-step projected gradient descent (PGD) attack \cite{antun2020instabilities} with 64 test images. The quality of reconstructed images is measured using metrics %root mean square error (RMSE) ,
% peak signal-to-noise ratio (PSNR), and structure similarity (SSIM)
% % \JH{full name for them}
% . We also evaluate our methods on the other two types of robustness mentioned in Sec. \ref{sec: preliminaries} by changing the number of unrolling steps and the undersampling rate of masks.
 
% \subsection{Experiment results}
   
% %\SL{[You can also give paragraph title, like above, to organize your experiment findings.]}
   
% %\SL{In Figure/Table, we present xxxxx. As we can see, xxx.  This implies that xxx}

% %\SL{In Figure/Table, we show xxxx. We observe that xxx.}

% Table \ref{tab: exp_smoothing} shows PSNR, and SSIM of different smoothing architectures with different training schemes, along with vanilla {\modl} as a baseline, evaluated on clean and adversarial test dataset. And we present the PSNR results of these models under different scales of adversarial perturbations in Fig. \ref{fig:archi_PSNR}.
% Our method {\us} with unrolling loss outperforms all other models in robustness, consistent with visualization in Fig. \ref{fig: vis}. Also, {\us} shows  better, clean accuracy with other models. This shows the effectiveness of our overall architecture, {\us} with unrolling loss training scheme, in generating adversarial robustness on {\modl}. 
% %This discovery indicates integrating randomized smoothing with complicated models, compared to the single neural network architecture in the image classification task. All we need is to convert the denoiser into a smoothed one and leave the rest of the model unchanged, \textit{i.e.} adding Gaussian noises before the denoiser and estimating the expectation of outputs of the denoiser \textit{w.r.t.} that noise. Moreover, a specific loss function should be designed to match the architecture of the model to reach high robustness.


% \begin{table}[htb]
% \centering
% % \vspace*{-1em}
% % \vspace*{-3mm}
% \caption{\footnotesize{
% % Clean accuracy and 
% Robust accuracy of smoothing architectures with different training schemes, where \textsc{US} represents {\us}. The noise accuracy and robust accuracy are evaluated with Gaussian noise $||\epsilon||_2 \le 0.004$ and adversarial perturbation $||\delta||_2 \le 0.004$ respectively.
% The relative performance is shown with respect to vanilla {\modl}.
% % The best performance is highlighted in \textbf{bold}.
% }}
% \label{tab: exp_smoothing}
% \vspace*{-2mm}
% \resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{c|cc|cc|cc}
% \toprule[1pt]
% \midrule
% Models 
% & \multicolumn{2}{c}{Clean Accuracy} 
% & \multicolumn{2}{c}{Noise Accuracy} 
% & \multicolumn{2}{c}{Robust Accuracy} 
% \\
% Metrics 
% & PSNR \textcolor{red}{$\uparrow$} & SSIM \textcolor{red}{$\uparrow$}
% & PSNR \textcolor{red}{$\uparrow$} & SSIM \textcolor{red}{$\uparrow$}
% & PSNR \textcolor{red}{$\uparrow$} & SSIM \textcolor{red}{$\uparrow$}
% \\
% \midrule
% Vanilla {\modl}
% & 29.7366 
% & 0.8996 
% & 28.7057
% & 0.8735
% & 22.9133
% & 0.7286
% \\
% \midrule
% \ref{eq: denoised smoothing mri}
% & \textbf{+0.0915 }
% & \textbf{+0.0016 }
% & +0.3865
% & +0.0101
% & +0.7869
% & \textbf{+0.0337}
% \\
% {{\us} ($\cD + \mathcal{DC}$)}
% & -1.013
% & -0.0141
% & -0.0945
% & +0.0079
% & +3.0829
% & -0.0141
% \\
% \rowcolor[gray]{.8}
% {\us} (ours)
% & -0.3453 
% & -0.0057
% & \textbf{+0.5376}
% & \textbf{+0.0162}
% & \textbf{+3.8723}
% & +0.0082
% \\
% \midrule
% \bottomrule[1pt]
% \end{tabular}%
% }
% \vspace*{-4mm}
% \end{table}

% % \vspace*{-3mm}
% % \begin{wrapfigure}{l}{.22\textwidth}
% %     \centering
% %     \includegraphics[width=.22\textwidth]{Figures/smoothing_archi_PSNR.pdf}
% %     \caption{\footnotesize{PSNR results of different smoothing architectures evaluated on clean data ($\epsilon$=0) and adversarial data generated by 10-step PGD.}}
% %     \label{fig:archi_PSNR}
% % \end{wrapfigure}
% % \begin{wrapfigure}{r}{.22\textwidth}
% %     \centering
% %     \includegraphics[width=.22\textwidth]{Figures/reference_image_PSNR.pdf}
% %     \caption{\footnotesize{PSNR results of {\us}, trained using different reference image in unrolling loss, evaluated on clean data ($\epsilon$=0) and adversarial data generated by 10-step PGD.}}
% %     \label{fig:reference_PSNR}
% % \end{wrapfigure}

% \begin{figure}
%     \begin{minipage}{0.23\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{Figures/smoothing_archi_PSNR.pdf}
%     \vspace*{-6mm}
%     \caption{\footnotesize{PSNR results of different smoothing architectures evaluated on clean data (\textit{i.e.} PGD $\epsilon = 0$) and adversarial examples generated by 10-step PGD. \ref{eq: denoised smoothing mri} is trained using MSE + Stab scheme, and {\us} is trained using MSE + Unrolling Stab scheme.}}
%     \label{fig:archi_PSNR}
% \end{minipage}\hfill
% \begin{minipage}{0.23\textwidth}

%     \centering
%     \includegraphics[width=\textwidth]{Figures/reference_image_PSNR.pdf}
%     \vspace*{-6mm}
%     \caption{\footnotesize{PSNR results of {\us}, trained using different reference images in the unrolling loss, evaluated on clean data (\textit{i.e.} PGD $\epsilon = 0$) and adversarial examples generated by 10-step PGD. $\cD_{\mathrm{base}}$ represents the denoiser in the base model, \textit{i.e.} trained vanilla MoDL.}}
%     \label{fig:reference_PSNR}
%     \end{minipage}\hfill
% \vspace*{-4mm}
% \end{figure}

% \begin{figure}[htb]

% \begin{tabular}[b]{cccc}
%         \includegraphics[width=.21\linewidth, trim=70 10 70 10]{Figures/visualization/ground_truth.pdf}
%         &\hspace{-0.2cm}
%         \includegraphics[width=.21\linewidth, trim=70 10 70 10]{Figures/visualization/vanilla_MoDL_eps0.5_255.pdf}
%         &\hspace{-0.2cm}
%         \includegraphics[width=.21\linewidth, trim=70 10 70 10]{Figures/visualization/E2E_smoothing_E2E_loss_eps0.5_255.pdf}

%         &\hspace{-0.2cm}
%         \includegraphics[width=.21\linewidth, trim=70 10 70 10]{Figures/visualization/unrolling_smoothing_unrolling_loss_eps0.5_255.pdf}
%         \\[-0pt]
%         \scriptsize{(a) Ground Truth} 
%         &\hspace{-0.2cm} 
%         \scriptsize{(b) Vanilla {\modl}} 
%         &\hspace{-0.2cm} 
%         \scriptsize{(c) RS-E2E}
%         &\hspace{-0.2cm} 
%         \scriptsize{(d) \textsc{SMUG}} 
%         \\ 
% \end{tabular}
% \vspace*{-4mm}
% \caption{\footnotesize{Visualization of the ground truth image and reconstruction results of different model architectures when evaluated on adversarial examples with perturbation $||\delta||_2 \le 0.002$
% , and {\us}. \ref{eq: denoised smoothing mri} is trained using the E2E loss and {\us} is trained using the unrolling loss.}}
% \label{fig: vis}
% \vspace*{-2mm}
% \end{figure}

% We also conduct experiments on different reference images in unrolling loss, whose results are shown in Fig. \ref{fig:reference_PSNR}. We find that the performance of {\us} varies using different reference images in the loss function, and those containing $\cD(\cdot)$ get better results. This conforms with our speculation on the instability of $\cD(\cdot)$ in the training process and substantiates the benefit of using $\cD(\bm{\theta};\bx_\mathrm{label})$ in loss function as a reference image.

% % \begin{table}[htb]
% % \centering
% % % \vspace*{-1em}
% % \caption{Clean accuracy and robust accuracy of {\us} with different reference image in loss function, where $\cD_{\theta}$ is the denoier in {\us} and $\cD_{\theta_0}$ is the denoier in vanilla {\modl}. The robust accuracy is tested with adversarial perturbation $||\delta||_2 \le 0.5/255$.
% % % The best performance is highlighted in \textbf{bold}.
% % }
% % \label{tab: exp_unrolling_loss}
% % \resizebox{0.48\textwidth}{!}{%
% % \begin{tabular}{c|ccc|ccc}
% % \toprule[1pt]
% % \midrule
% % Reference image & \multicolumn{3}{c}{Clean Accuracy} & \multicolumn{3}{c}{Robust Accuracy} \\
% % Metrics & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ \\
% % \midrule
% % $\bx_n$
% % & 0.0256
% % & 30.2319
% % & 0.9058
% % & 0.0347
% % & 27.3069
% % & 0.8017
% % \\
% % $\bx_\text{label}$
% % & 0.026
% % & 30.0599
% % & 0.904
% % & 0.0428 
% % & 25.4393
% % & 0.7793
% % \\
% % $\cD_{\theta_0}(\bx_n)$
% % & 0.0259 
% % & 30.0624 
% % & 0.9038 
% % & 0.0339 
% % & 27.6306
% % & 0.8681
% % \\
% % $\cD_{\theta_0}(\bx_\text{label})$
% % & 0.0262 
% % & 29.9721 
% % & 0.9034 
% % & 0.0385 
% % & 26.4517 
% % & 0.831
% % \\
% % $\cD_{\theta}(\bx_n)$
% % & 0.0269 
% % & 29.7071 
% % & 0.897 
% % & 0.0319 
% % & 28.1294 
% % & \textbf{0.8751}
% % \\
% % \rowcolor[gray]{.8}
% % $\cD_{\theta}(\bx_\text{label})$
% % & 0.0278 
% % & 29.3913 
% % & 0.8939 
% % & \textbf{0.0308}
% % & \textbf{28.4137} 
% % & 0.8467
% % \\
% % \midrule
% % \bottomrule[1pt]
% % \end{tabular}%
% % }
% % \vspace*{-3mm}
% % \end{table}

% In Fig \ref{fig: robustness}, we present the evaluation results of {\us}, trained by unrolling loss, with two baselines, vanilla {\modl} and \ref{eq: denoised smoothing mri}, on different unrolling steps and undersampling rates, noting these models are trained with the number of unrolling steps $K=8$ and masks with the 4x undersampling factor . And the visualization of these is shown in Fig. \ref{fig: weakness} for {\modl} and Fig. \ref{fig: smug weakness} for {\us}.
% As we can see, {\us} achieves a remarkable improvement in robustness against different undersampling rates and unrolling steps, which \ref{eq: denoised smoothing mri} fails to achieve.
% Although we do not intentionally design our method to mitigate {\modl}'s instabilities against undersampling rate and unrolling steps, {\us} still gets better performance compared to other baselines. We accredit the improvement to the close relationships between these two instabilities with the adversarial robustness. %For instability against the undersampling rates, we notice that masks of different undersampling rates are similar to adversarial perturbed masks. And for instability against the number of unrolling steps, we argue that {\us} alleviates the instability with the adversarial robustness of \textit{each} unrolling step, thereby preventing degradation through steps.

% \begin{figure}[htb]
% % \vspace*{-3mm}
%     \centering

%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/robustness/undersampling_rate_PSNR.pdf}
%         \caption{\footnotesize{Undersampling Rate
%         }}
%     \end{subfigure}
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{Figures/robustness/unrolling_steps_PSNR.pdf}
%         \caption{\footnotesize{Unrolling Step
%         }}
%     \end{subfigure}

%     \vspace*{-2mm}
%     \caption{\footnotesize{PSNR results of different smoothing architectures with 
%     different unrolling steps (trained at 8)
%     and measurement sampling rates (trained at 4 \hui{25\%?}). The robust accuracy is evaluated with adversarial perturbation $||\delta||_2 \le 0.002$. %{It's worth noting that the higher the undersampling rate is, the sparser the mask is.}
%     \hui{x-axis needs to be changed.}
%     }}
%     \label{fig: robustness}
%     \vspace*{-2mm}
% \end{figure}

% \begin{figure}[htb]
% % \vspace{-3mm}
% \centering
% \begin{tabular}[b]{ccc}
%         \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/SMUG_recon.pdf}
%         &
%         \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/SMUG_undersampling_2.0_vis.pdf}
%         &
%         \includegraphics[width=.2\linewidth, trim=70 10 70 10]{Figures/visualization/SMUG_unrolling_steps_16_clean_vis.pdf}
%         \\[-0pt]
%         \scriptsize{(a)} 
%         & 
%         \scriptsize{(b)} 
%         &  
%         \scriptsize{(c)}

% \end{tabular}
% \vspace*{-2mm}
% \caption{\footnotesize{Visualization of reconstruction results of {\us} on different undersampling rates and different unrolling steps. (a) the reconstruction image from the clean image, (b) the reconstruction image when evaluated using a mask with the undersampling rate being 2 (4 in training), (c) the reconstruction image when evaluated using 16 unrolling steps (8 in training).}}
% \label{fig: smug weakness}
% \end{figure}

% % \subsection{Ablation Study}

% % \begin{table}[htb]
% % \centering
% % % \vspace*{-1em}
% % \caption{Clean accuracy and robust accuracy of {\us} with different $\sigma$. The robust accuracy is tested with adversarial perturbation $||\delta||_2 \le 0.5/255$.
% % % The best performance is highlighted in \textbf{bold}.
% % }
% % \label{tab: ablation_sigma}
% % \resizebox{0.48\textwidth}{!}{%
% % \begin{tabular}{c|ccc|ccc}
% % \toprule[1pt]
% % \midrule
% % $\sigma$ & \multicolumn{3}{c}{Clean Accuracy} & \multicolumn{3}{c}{Robust Accuracy} \\
% % Metrics & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ \\
% % \midrule
% % 0.001 
% % & 0.0274 
% % & 29.5286 
% % & 0.8943 
% % & 0.0302 
% % & 28.464 
% % & 0.7916
% % \\
% % 0.005 & 0.0282 & 29.2658 & 0.8929 & 0.0311 & 28.3198 & 0.8594
% % \\
% % 0.01 & 0.0278 & 29.3913 & 0.8939 & 0.0308 & 28.4137 & 0.8467
% % \\
% % 0.05 & 0.0325 & 28.0298 & 0.874 & 0.0333 & 27.7624 & 0.8499
% % \\
% % 0.1 & 0.0322 & 28.0921 & 0.8764 & 0.0325 & 28.0224 & 0.8746
% % \\
% % \midrule
% % \bottomrule[1pt]
% % \end{tabular}%
% % }
% % \vspace*{-3mm}
% % \end{table}

% % \begin{table}[htb]
% % \centering
% % % \vspace*{-1em}
% % \caption{Clean accuracy and robust accuracy of {\us} with different Monte Carlo sampling number $n$. The robust accuracy is tested with adversarial perturbation $||\delta||_2 \le 0.5/255$.
% % % The best performance is highlighted in \textbf{bold}.
% % }
% % \label{tab: ablation_n}
% % \resizebox{0.48\textwidth}{!}{%
% % \begin{tabular}{c|ccc|ccc}
% % \toprule[1pt]
% % \midrule
% % $n$ & \multicolumn{3}{c}{Clean Accuracy} & \multicolumn{3}{c}{Robust Accuracy} \\
% % Metrics & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & RMSE $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ \\
% % \midrule
% % 3 & 0.0306 & 28.5343 & 0.8815 & 0.0324 & 27.9376 & 0.8235
% % \\
% % 5 & 0.0292 & 28.9498 & 0.8886 & 0.0315 & 28.1755 & 0.8301
% % \\
% % 10 & 0.0278 & 29.3913 & 0.8939 & 0.0308 & 28.4137 & 0.8467
% % \\
% % \midrule
% % \bottomrule[1pt]
% % \end{tabular}%
% % }
% % \vspace*{-3mm}
% % \end{table}

% % Below is an example of how to insert images. Delete the ``\vspace'' line,
% % uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% % with a suitable PostScript file name.
% % -------------------------------------------------------------------------


% % To start a new column (but not a new page) and help balance the last-page
% % column length use \vfill\pagebreak.
% % -------------------------------------------------------------------------
% %\vfill
% %\pagebreak
% \vspace*{-4mm}
\section{Conclusion}
%\vspace*{-2mm}
In this work, we proposed a scheme for improving robustness of DL-based MRI reconstruction. We showed deep unrolled reconstruction's ({\modl}'s) weaknesses in robustness against adversarial perturbations, sampling rates, and unrolling steps. To improve the robustness of {\modl}, we proposed {\us} with a novel unrolled smoothing loss.
 Compared to the vanilla
{\modl} approach and several variants of {\us}, we empirically showed that
%Our experiments demonstrated the effectiveness of
our approach is effective and can   significantly improve the robustness of {\modl} against a diverse set of external perturbations. In the future, we will study the problem of certified robustness  and derive the certification bound of adversarial perturbations using  randomized smoothing.

%all three types of robustness.

% List and number all bibliographical references at the end of the
% paper. The references can be numbered in alphabetic order or in
% order of appearance in the document. When referring to them in
% the text, type the corresponding reference number in square
% brackets as shown at the end of this sentence \cite{C2}. An
% additional final page (the fifth page, in most cases) is
% allowed, but must contain only references to the prior
% literature.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\clearpage
\newpage 
\bibliographystyle{IEEEbib}
\bibliography{reference,refs_adv}

\end{document}
