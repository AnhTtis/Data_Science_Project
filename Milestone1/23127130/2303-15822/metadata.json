{
    "arxiv_id": "2303.15822",
    "paper_title": "One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization",
    "authors": [
        "Deze Wang",
        "Boxing Chen",
        "Shanshan Li",
        "Wei Luo",
        "Shaoliang Peng",
        "Wei Dong",
        "Xiangke Liao"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.SE",
        "cs.AI"
    ],
    "abstract": "As pre-trained models automate many code intelligence tasks, a widely used paradigm is to fine-tune a model on the task dataset for each programming language. A recent study reported that multilingual fine-tuning benefits a range of tasks and models. However, we find that multilingual fine-tuning leads to performance degradation on recent models UniXcoder and CodeT5.\n  To alleviate the potentially catastrophic forgetting issue in multilingual models, we fix all pre-trained model parameters, insert the parameter-efficient structure adapter, and fine-tune it. Updating only 0.6\\% of the overall parameters compared to full-model fine-tuning for each programming language, adapter tuning yields consistent improvements on code search and summarization tasks, achieving state-of-the-art results. In addition, we experimentally show its effectiveness in cross-lingual and low-resource scenarios. Multilingual fine-tuning with 200 samples per programming language approaches the results fine-tuned with the entire dataset on code summarization. Our experiments on three probing tasks show that adapter tuning significantly outperforms full-model fine-tuning and effectively overcomes catastrophic forgetting.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15822v1"
    ],
    "publication_venue": "Accepted to the 45th International Conference on Software Engineering (ICSE 2023)"
}