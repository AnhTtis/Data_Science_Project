% !TEX root = ./../paper.tex
\section{Two Stage Framework (or Algorithm Name)}
%Variational Distribution Learning via GAN}
\label{sec:two_stage_framework}
\dy{This section describes the details of our two-stage framework for language-free training of T2I models.
In the first stage, the model of variational inference learns to approximate the text feature vector $\bz_\text{txt}$ of an image $\bx_\text{img}$ via adversarial training. 
In the second stage, we explain how a generative model learns text-to-image generation without text labels, while exploiting the approximated text features.}

\subsection{Learning Hidden Representations}
\label{sub:first_stage}
\subsubsection{Robust Objective}
\label{subsub:robust_objective}
Optimizing Eq.~\eqref{eq:vi_detail} with respect to $ \theta^\bzi $, $\theta^\bzt$, and $\phi^{\bzt}$ is equivalent to solve the following optimization problem since $\log p_{\theta^\bx}(\bx | \bzi, \bzt)$ is irrelevant about the parameters, which is given by
\begin{align}
\min_{\Theta, \phi^\bzt} \, & D_\text{KL} (q_{\phi^{\bzt}}( \bzt | \bzi) || p_{\theta^\bzt} (\bzt) )\nonumber \\ 
 -&\mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)} [\log p_{\theta^\bzi} (\bzi | \bzt) ],
\label{eq:stage1_objective}
\end{align}
where $\Theta$ denotes $\{ \theta^\bzi, \theta^\bzt \}$ and from now on we will omit the parameters for simplicity.
However, the KL divergence is generally intractable unless the two distributions belong to specific families of probability distributions (\eg Gaussian distribution).  
%Moreover, since $\bzt$ is located at the unit hypersphere as mentioned in~\cite{radford2021learning},     
Alternatively, we minimize the distance between the two distributions using Jensen-Shannon divergence, where the two divergences encourage the two distributions to become equal.
%hare the same property that the outputs of the divergences are zero when the two di 
%are zero when the two distributions are exactly same.
Then, as mentioned in~\cite{goodfellow2014generative, nowozin2016f}, we can equivalently solve the following minmax game as
%
\begin{align}
&\min_{\Theta, \phi^\bzt}  \max_{\phi^D} \, \mathbb{E}_{p (\bzt)}[\log D(\bzpt)] \nonumber \\
%+ \mathbb{E}_{q( \bzt | \bzi)}[\log (1-D(\bzt))] \nonumber \\
%(q_{\phi^{\bzt}}  ( \bzt | \bzi) || p_{\theta^\bzt} (\bzt) )\nonumber \\ 
&+ \mathbb{E}_{q( \bzt | \bzi)}[\log (1-D(\bzt)) - \log p(\bzi | \bzt) ],
\label{eq:stage1_objective_adv}
\end{align}
%
where $\phi^D$ is a parameter of the discriminator, $D(\cdot; \phi^D)$.
% KL vs Adv in experimental section  
We empirically observe that computing the KL divergence combined with variational auto-encoder (VAE) frameworks~\cite{kingmaauto} incurs much worse performance as presented in Table ???, where  Gaussian mixture models are used for the prior distribution modeling for the VAE frameworks.
Therefore, we solve the optimization problem in Eq.~\eqref{eq:stage1_objective_adv} instead of Eq.~\eqref{eq:stage1_objective}.


% Option 2: Alternatively, we employ the Donsker-Varadhan representation~\cite{donsker1983asymptotic} as a dual representation of the KL divergence.
% Then, we aim to solve the following minmax game as
% %
% \begin{align}
% &\min_{\Theta, \phi^\bzt}  \max_{\phi^T} \, -\log \mathbb{E}_{p (\bzt)}[\exp (T(\bzt))] \nonumber \\
% %+ \mathbb{E}_{q( \bzt | \bzi)}[\log (1-D(\bzt))] \nonumber \\
% %(q_{\phi^{\bzt}}  ( \bzt | \bzi) || p_{\theta^\bzt} (\bzt) )\nonumber \\ 
% &+ \mathbb{E}_{q( \bzt | \bzi)}[T(\bzt) - \log p(\bzi | \bzt) ],
% \label{eq:stage1_objective_kl_conjugate}
% \end{align}
% %
% \iffalse
% %
% \begin{align}
% &\min_{\Theta, \phi^\bzt}  \max_{\phi^T} \, -\log \mathbb{E}_{p (\bzt)}[\exp (T(\bzt))] \nonumber \\
% %+ \mathbb{E}_{q( \bzt | \bzi)}[\log (1-D(\bzt))] \nonumber \\
% %(q_{\phi^{\bzt}}  ( \bzt | \bzi) || p_{\theta^\bzt} (\bzt) )\nonumber \\ 
% &+ \mathbb{E}_{q( \bzt | \bzi)}[T(\bzt) - \log p(\bzi | \bzt) ] + \textcolor{red}{0.5 * T^2(z_\text{txt})},
% \label{eq:stage1_objective_kl_conjugate}
% \end{align}
% %
% \fi
where $T(\cdot; \phi^T)$ is a scalar-valued function parametrized by a deep neural network which takes the variational or prior samples as an input. 
We empirically observe that computing the KL divergence combined with variational auto-encoder (VAE) frameworks~\cite{kingmaauto} incurs much worse performance compared with the Donsker-Varadhan representation as presented in Table ???, where  Gaussian mixture models are used for the prior distribution modeling for the VAE frameworks.
Therefore, we solve the optimization problem in Eq.~\eqref{eq:stage1_objective_kl_conjugate} instead of Eq.~\eqref{eq:stage1_objective}.
%and the network output is a real value. 
%as the statistics network following the convention used in .



\subsubsection{Generating Samples for Text Embedding}
\label{subsub:variational_distribution_samples}
We employ implicit generative models to obtain samples $\bzth$ drawn by $q(\bzt | \bzi)$ using a generator $G(\cdot;\phi_\bzt)$ as follows:
%
\begin{align}
\bzth= \text{Normalize} (\bzi + r \cdot \frac{ G(\bzi)}{\| G(\bzi) \|}), 
%\; \bzth = \frac{\tbzt}{\| \bzt \|} 
\label{eq:fake_txt}
\end{align}
%
where $r \geq 0$ is a hyperparameter. 
The following proposition validates that the obtained sample $\bzth$ always has a high cosine similarity with the corresponding image feature $\bzi$ similar to the true embedding. 
\begin{proposition}
%\vspace{0.2cm}
Let $\bzth$ be the sample obtained by Eq.~\eqref{eq:fake_txt} based on $\bzi$. 
Then, the following inequality always holds for $ G(\cdot ; \phi^\bzt)$ with any $\phi^\bzt$ as 
\begin{align}
\bzth ^T \bzi \geq \sqrt{1-r^2},
\end{align}
where we assume that $r$ is less than 1.
\label{propostion:1}
\end{proposition}
%
\begin{proof}
%See Appendix ???
First of all, the inner product can be expressed as 
%
\begin{equation}
\bzth ^ T \bzi = \bzi^T \frac{(\bzi + r \cdot \bgi)}{\| \bzi + r \cdot \bgi \|},  
\label{eq:prop_LHS}
\end{equation}
%
where we define $\bgi$ as $\text{Normalize}(G(\bzi))$.
In addition, the denominator in Eq.~\eqref{eq:prop_LHS} is given by
%
\begin{align}
\| \bzi + r \cdot \bgi \| &= \sqrt{(\bzi + r \cdot \bgi)^T(\bzi + r \cdot \bgi)} \nonumber \\
&= \sqrt{1 + 2 r \cdot \bzi^T \bgi + r^2}.
\label{eq:magnitude_variational_samples}
\end{align}
%
Based on the two equations, we have 
%
\begin{align}
\bzth ^ T \bzi &=  \frac{\bzi^T(\bzi + r \cdot \bgi)}{ \sqrt{1 + 2 r \cdot \bzi^T \bgi + r^2}} \nonumber \\
&=  \frac{1 + r \cdot \bzi^T \bgi}{ \sqrt{1 + 2 r \cdot \bzi^T \bgi + r^2}} \nonumber \\
&= \frac{2 + 2 r \cdot \bzi^T \bgi}{2 \sqrt{1 + 2 r \cdot \bzi^T \bgi + r^2}} \nonumber \\
&=\frac{(1 + 2 r \cdot \bzi^T \bgi + r^2) + (1 - r^2)}{ 2\sqrt{1 + 2 r \cdot \bzi^T \bgi + r^2}} \nonumber \\
&\geq \sqrt{1-r^2}  \; (\because \text{A.M-G.M inequality}), 
\end{align}
%
which concludes the proof due to the inequality of arithmetic and geometric means, where $1 + 2 r \cdot \bzi^T \bgi + r^2$ and $1 - r^2$ are non-negative real numbers.
%For the inequality, we can easily show that $1 + 2 r \cdot \bzi^T \bgi + r^2$ and $1 - r^2$ are non-negative real numbers, which concludes the proof.
%Therefore, we want to show that the following inequality holds as  
 \end{proof}
%
%\noindent Proposition~\ref{propostion:1} implies that the sample procedure significantly reduces the search space of the approximated sample $\bzth$ by ignoring the irrelevant region of the , which can lead to the efficient optimization procedure.
\noindent Proposition~\ref{propostion:1} implies that the variational distribution ignores the region where the true posterior distribution has low density, which facilitates the optimization procedure by constraining the search space of the variational distribution.
%search space of the variational distribution. 

%optimization procedures the samples $\bzth$ drawn from the variational distribution 
%the samples $\bzth$ obtained by Eq.~\eqref{eq:fake_txt} are located close to $\bzi$ similar to the true embedding while ignoring the region of 
%$\bzt$ given by the text supervision which is not available during training time.} 

\subsubsection{Relational Representation Transfer}
\label{subsub:weak_supervision}
In addition, we encourage the predicted text samples $\bzth$ to mimic the correlation of the training samples of $\bzi$ to mitigate the difficulty about the lack of the supervision. 
The intuition is that the structural relation of text embeddings will resemble that of image representations since the contrastive objective leads to maximize the mutual information between the two representations.
For example, the two text embeddings should be located closer if the image representations are closer, and vice versa.
In order to achieve the goal, we apply a relational knowledge distillation framework~\cite{park2019relational}, which encourages a student to mimic the relations among data embeddings given by a teacher.
In our algorithm, we view the image embeddings as the teacher while the text embeddings are regarded as the student.
%In addition to the variational lower bound, we additionally minimize the relational distillation loss, which is given by
Therefore, we additionally minimize the relational distillation loss, which is given by
%
\begin{align}
\mathcal{L}_\text{rkd} := \mathbb{E}[ \ell_\delta(&\psi_A(\bzi^i, \bzi^j, \bzi^k)  \nonumber \\ 
-&\psi_A(\bzth^i, \bzth^j, \bzth^k)) ], 
%q(\bzt^i, \bzt^j, \bzt^k|\bzi^i, \bzi^j, \bzi^k)}[ \ell_\delta(\psi_A(), \psi_A()) ]
\label{eq:rkd_angle_loss}
\end{align}
%
%where $\bzi^i$, $\bzi^j$, and $\bzi^k$ are arbitrary triplet image embeddings while $\bzt^i$, $\bzt^j$, and $\bzt^k$ are corresponding samples drawn by the variational distribution.
where the expectation is taken over triplet image embeddings ($\bzi^i, \bzi^j, \bzi^k$) and corresponding samples drawn by the variational distribution ($\bzth^i, \bzth^j, \bzth^k$).
In the above equation, $\ell_\delta(\cdot)$ is the huber loss and $\psi_A(\cdot, \cdot, \cdot)$ is defined as  
\begin{align}
\psi_A(\bz^i, \bz^j, \bz^k) := \text{sim}(\bz^i- \bz^j, \bz^i-\bz^k),
\end{align}
where sim$(\cdot, \cdot)$ denotes the cosine similarity between the two vectors.
% contrastive or RKD

\subsubsection{Prior Samples}
\label{subsub:prior_samples}
%On the other hand, the prior samples $\bzpt$ which are recognized as the fake samples by the discriminator should be also available to solve the minmax game. 
On the other hand, we obtain prior samples $\bzpt$ from a text corpus which is easily available online.
Note that we do not employ an additional dataset which consists of images and their text captions.
%To deal with the issue, we can obtain $\bzpt$ drawn by $p(\bzpt)$ using a text corpus (\eg Wikipedia) which is easily available online.
Specifically, we randomly choose a text sentence $\bxtp$ from the text corpus, and we can get the prior sample by embedding the sentence as follows: 
%using the CLIP text encoder $f_\text{txt}(\cdot; \theta^\bzt)$. %as follows:
%\iffalse
%
\begin{align}
\bzpt = f_\text{txt}(\bxtp), %\text{Normalize}(f_\text{txt}(\bxtp)), 
%\; \bzpt = \frac{\tbzpt}{\| \tbzpt \|},  
\end{align}
%\fi
%
where $f_\text{txt}(\cdot; \theta^\bzt)$ is the CLIP text encoder.
We do not learn the encoder parameters $\theta^\bzt$, but fix them to reduce the computational burden. 

\subsubsection{Image Embedding Reconstruction Samples}
\label{subsub:image_embedding_reconstruction_samples}
Similar to the sampling process of $q(\bzt | \bzi)$, we define the sampling process of $p(\bzi | \bzt)$ using a network $F(\cdot; \theta^\bzi)$ as   
\begin{align}
\tbzpi = \bzth + r \cdot \frac{ F(\bzth)}{\| F(\bzth) \|}, \; \bzpi = \text{Normalize} (\tbzpi) + \sigma \epsilon, 
%\; \bzpi = \frac{\tbzpi}{\| \tbzpi \|} + \sigma \epsilon,
\label{eq:fake_img}
\end{align} 
where $\sigma$ is a hyperparameter to control diagonal elements of the covariance matrix for $\epsilon$. 
Likewise, the mean of $\bzpi$ also satisfies the property that it has a high cosine similarity with $\bzth$ as supported by Proposition~\ref{propostion:1}.   
For the tractable computation of the log-likelihood $\log p(\bzi|\bzt)$ in Eq.~\eqref{eq:stage1_objective_adv}, we additionally introduce $\epsilon$ in Eq.~\eqref{eq:fake_img}.
%In the above equation, $F(\cdot)$ is a network to estimate the mean of $\bzi$.
%with zero mean a while $\sigma$ is a hyperparameter to control the standard deviation of the noise
Using the equation, the expected negative log-likelihood $\mathcal{L}_{\text{recon}}$ can be expressed as follows:
\begin{align}
\mathcal{L}_{\text{recon}} &:= \mathbb{E}_{q( \bzt | \bzi)}[ - \log p(\bzi | \bzt) ] \nonumber \\ 
&=\frac{1}{2\sigma^2} \mathbb{E}_{q( \bzt | \bzi)} \Big[ \| \bzi - \frac{\tbzpi }{\| \tbzpi  \|}  \|^2 \Big] + C 
%&=\frac{1}{2\sigma^2} \mathbb{E}_{q( \bzt | \bzi)} [ -\text{sim} (\bzi,  \tbzpi) ] + C,
\label{eq:recon}
\end{align}
where $C$ is a constant with respect to $\theta^\bzi$. %and it can be ignored. 
%$C$. 

\subsubsection{Total Objective}
\label{subsub:final_objective}
% Given a hyperparameter \textcolor{red}{$\lambda_\text{adv}$} is a predefined value
The total optimization of the first stage is summarized as:
\begin{align}
&\min_{\theta^\bzi, \phi^\bzt}  \max_{\phi^D} \textcolor{red}{\mathcal{L}_{\text{adv}}} + \mathcal{L}_{\text{recon}} + \lambda_\text{rkd} \mathcal{L}_{\text{rkd}},
\end{align}
%where \textcolor{red}{$\lambda_\text{adv}$} is a predefined value, and $\mathcal{L}_\text{adv}$ is defined as 
where the adversarial loss $\mathcal{L}_\text{adv}$ is defined as
\begin{align}
\mathcal{L}_\text{adv} :=  &\mathbb{E}_{p (\bzt)}[\log D(\bzpt)] \nonumber \\
+& \mathbb{E}_{q( \bzt | \bzi)}[\log (1-D(\bzt))].
\label{eq:adv}
\end{align}
%

%  $\mathcal{L}_\text{kl}$ is defined as
% (option 2:)
% \begin{align}
% \mathcal{L}_\text{kl} :=   - \log \mathbb{E}_{p (\bzt)}[\exp  (T(\bzpt))] + \mathbb{E}_{q( \bzt | \bzi)}[T(\bzt)].
% \label{eq:kl}
% \end{align}
% %Detail: Additionally, we add  R1 regularization for Discrimnator (Supple)

\subsection{Learning Text-to-Image Generation using Approximated Text Posterior}
\label{sub:second_stage}
%\ms{This section}
%\subsubsection{Training}
%\label{subsub:training}
%$q_{\phi^{\bzt}}( \bzt | \bzi)$
After the optimization step for the first stage, we maximize Eq.~\eqref{eq:vi_detail} with respect to $\theta^\bx$, which is equivalent to solve the following problem:
%perform maximum likelihood estimation as follows: 
\begin{align}
\max_{\theta^\bx} \mathbb{E}_{q( \bzt | \bzi)} [ \log p_{\theta^\bx}(\bx | \bzi, \bzt)], 
\end{align}
where $p_{\theta^\bx}(\cdot)$ is a text-to-image generative model. 
Different from CLIP-GEN~\cite{wang2022clip} and LAFITE~\cite{zhou2022towards}, the image data is generated conditioned on $\bzi$ as well as $\bzt$. 
The generative model can belong to an autoregressive~\cite{dalle,VQGAN,rqvae,cogview,nuwa}, bidirectional transformers~\cite{draftNrevise} or diffusion models~\cite{ddpm,adm,song2020score,austin2021structured}, and even GAN frameworks~\cite{brock2018large, karras2019style, karras2020analyzing, karras2021alias} by replacing the maximum likelihood objective with the Jensen-Shannon divergence.  
Following LAFITE~\cite{zhou2022towards}, we adopt a StyleGAN2~\cite{karras2020analyzing} for a fair comparison. 
\ms{In addition, we also employ a diffusion model~\cite{} to further validate the effectiveness of the proposed method.}
%TODO LAFITE, CLIPGEN Objective
%Different from our algorithm, the image generating process of LAFITE and CLIP-GEN only depends on the text feature, which leads to learn using the following objective:   



\subsection{Inference}
\label{sub:inference}
We obtain a sample $\bzt$ by mapping a given sentence using the CLIP text encoder.
Then, we get $\bzih$ from $p(\bzi | \bzt)$ and sample an image $\bx$ using $p(\bx | \bzih, \bzt)$.

