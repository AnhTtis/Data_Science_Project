% !TEX root = ./../paper.tex

\section{Introduction}
\label{sec:introduction}
Recent advances in text-to-image (T2I) generation techniques~\cite{dalle, glide, cogview, VQGAN, rqvae, LDM, dalle2, imagen, parti, draftNrevise} have shown promising results by employing generative adversarial networks~\cite{goodfellow2014generative}, autoregressive models~\cite{van2016pixel}, or diffusion models~\cite{ddpm, songscore} to synthesize images based on their text captions.
However, these approaches require a paired dataset that consists of images and their corresponding text captions, and, consequently, incur significant annotation costs, especially for labeling image captions. 
To alleviate this limitation, unsupervised learning methods for T2I generation have recently drawn attention to the computer vision community, where the models learn to generate images without paired text captions. 




Existing T2I models~\cite{wang2022clip, zhou2022towards, kNNDiff, rdm} based on unsupervised learning exploit Contrastive Language-Image Pretraining (CLIP)~\cite{radford2021learning} to sidestep the absence of text captions during training.
Specifically, after a text embedding is estimated using a given image embedding, the T2I model is trained to synthesize an image conditioned on the estimated text embedding.
However, although image and text embeddings extracted by CLIP are not accurately aligned, existing approaches assume that the distinction is ignorable~\cite{wang2022clip} or simple to recover by just adding Gaussian noises~\cite{zhou2022towards} without considering the underlying structure of text embeddings.
Thus, those algorithms may suffer from large discrepancies between true and estimated text embeddings at both training and testing.
%





%
To tackle the challenge, we propose a variational distribution learning technique for unsupervised T2I generation, where the lower-bound of the data log-likelihood is maximized in a principled way.
Specifically, we first regard a text embedding as a hidden random variable while an image and its CLIP embedding are observable random variables.
%
Then, we decompose the variational lower-bound into three parts: 1) the similarity between the text embedding prior and posterior, 2) the log-likelihood of the image embedding given the text embedding, 3) the log-likelihood of the image given the image and text embeddings in the trained T2I model.
%
Since the lower-bound formulation enforces the matching between the prior and posterior distributions of text embedding, our method achieves a more accurate estimation of the embedding and reduces the discrepancy between the true and estimated embeddings.

For the optimization of the variational objective, we employ a two-stage training strategy for T2I models.
In the first stage, we learn an encode-decoder architecture that takes the image embedding as an input and the estimated text embedding as a latent bottleneck.
%
Then, our network estimates two conditional distributions of CLIP embeddings, one for the variational distribution of the text embedding given the image embedding and the other for the model distribution of the image embedding given the text embedding.
The parameters of the two distributions are obtained from the first two terms in the variational lower-bound objective.
%
Note that we relax the Kullback-Leibler (KL) divergence term in the training objective of the first stage to an adversarial training loss, specifically, the Jensen-Shannon divergence. 
Since the KL divergence is only tractable for a confined family of distributions, this relaxation allows more flexible choices for the conditional and the prior distributions.
In the second stage, a T2I model learns the conditional distribution of the images given the estimated text embeddings and the image features. 
Altogether, the proposed method achieves outstanding performance on widely adopted datasets~\cite{lin2014microsoft, cc3m}.
The main contributions of our work are summarized below: 
%
\begin{itemize}
 \item We propose a principled approach for unsupervised and semi-supervised text-to-image generation tasks based on a variational inference technique.
 \item We theoretically show that our method considers the underlying structure of text embeddings, which can eventually lead to better generalization performance. 
 \item We empirically confirm that the proposed algorithm outperforms the existing methods by large margins.
\end{itemize}

The rest of our paper is organized as follows. 
Section~\ref{sec:related} overviews the related work about unsupervised training methods in text-to-image generation. 
Section~\ref{sec:proposed algorithm} describes the main idea of our approach while Sections~\ref{sec:two_stage_framework} and~\ref{sub:training_t2i} discuss the procedures of the first and second training stages, respectively.
The experimental results are presented in Section~\ref{sec:experiments}, and we finally conclude this paper in Section~\ref{sec:conclusion}.


