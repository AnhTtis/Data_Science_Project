% !TEX root = ./../paper.tex
%


%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}
This section compares the proposed method referred to as VDL with existing approaches on the standard datasets under unsupervised and semi-supervised text-to-image generation settings, and analyzes the proposed components.
%
% 
\begin{table*}[t!]
\caption{Results of semi-supervised text-to-image generation on the MS-COCO dataset using StyleGAN2.
    The `Ratio' column shows the fractions of the labeled text captions in each dataset. 
   %
    }
\scalebox{0.90}{
    \begin{tabular}{cccccccc}
        \toprule
        Model & Dataset & Method & Ratio & IS ($\uparrow$) & FID ($\downarrow$) & $\text{Sim}_\text{txt}$ ($\uparrow$) & $\text{Sim}_\text{img}$ ($\uparrow$) \\
        %
        \midrule
        \multirow{7}{*}{StyleGAN2~\cite{karras2020analyzing}} & \multirow{7}{*}{MS-COCO~\cite{lin2014microsoft}} & LAFITE~\cite{zhou2022towards}  & $0.0$ & $27.20$ & $18.04$ & 0.0965 & -- \\
        & & LAFITE*~\cite{zhou2022towards}  & $0.1$ & $18.82$ & $20.65$ & 0.8340 & -- \\
        & & LAFITE*~\cite{zhou2022towards}  & $0.2$ & $21.19$ & $17.74$ & 0.8373 & -- \\
      & & LAFITE*~\cite{zhou2022towards}  & $0.3$ & $21.39$ & $15.76$ & \textbf{0.8385} & -- \\
      & & VDL (Ours) & $0.0$ & $30.30$ & $13.22$ & 0.6237 & 0.7105 \\
      &  & VDL (Ours) & $0.1$ & $32.81$ & $\textbf{11.24}$ & 0.7130 & 0.7536\\
      &  & VDL (Ours) & $0.2$ & $\textbf{33.90}$ & $\textbf{11.24}$ & 0.7261 & \textbf{0.7596}\\
        \bottomrule
    \end{tabular}
    }
    \centering
    \label{tab:Semi}
\end{table*}
%






\subsection{Datasets}
\label{subsec:datasets}
We employ MS-COCO~\cite{lin2014microsoft} and Conceptual Captions 3M~\cite{cc3m} (CC3M) datasets, which are widely used for the evaluation of text-to-image generation tasks. 
As training and validation datasets, the MS-COCO dataset contains 82k and 40k images while the CC3M dataset consists of 3.3M and 16k examples, respectively.   
As a preprocessing, we resize all images to 256$\times$256 pixels for training a T2I network while the images are resized to 224$\times$224 pixels before feeding them into the CLIP image encoder.   
%
As text corpora, we use text captions in CC3M for MS-COCO and 3 million randomly sampled texts from Conceptual 12M~\cite{cc12m} for CC3M.
Note that we do not utilize the image-text pairs for the proposed algorithm like other unsupervised methods such as CLIP-GEN~\cite{wang2022clip} and LAFITE~\cite{zhou2022towards}.








\subsection{Evaluation Metrics}
\label{subsec:evaluation_metrics}
We select the Fr\'{e}chet Inception Distance (FID)~\cite{heusel2017gans} and Inception Score (IS)~\cite{salimans2016improved} to evaluate and compare the visual quality of generated images. 
%
We measure the two metrics following the experimental protocol of previous works~\cite{zhu2019dm, dalle, zhou2022towards} for fair comparisons.
Additionally, we report $\text{Sim}_\text{txt}$ and $\text{Sim}_\text{img}$ using the validation dataset, which are calculated by the expected cosine similarity between the true and predicted text features and the similarity between the image and its inferred embeddings, respectively.
Note that $\text{Sim}_\text{img}$ cannot be measured for the baseline algorithms since they do not consider reconstructing the image feature. 
The discrepancy between the training and inference becomes lower when each of the similarities is higher.








\subsection{Implementation Details}
\label{subsec:implementation_details}
The proposed method is implemented with the official code of LAFITE\footnote{https://github.com/drboog/Lafite} based on PyTorch~\cite{paszke2019pytorch}. 
For the encoder $G(\cdot)$, decoder $F(\cdot)$, and discriminator $D(\cdot)$, we adopt networks consisting of 10 fully-connected layers with the leaky ReLU activations, where each hidden layer has 2048 units.
%
For the discriminator, we add an $R_1$ regularization~\cite{mescheder2018training} for training stability, which suppresses the magnitudes of gradients. 
We use the Adam optimizer~\cite{diederik2015adam} for the three networks with an initial learning rate of $0.001$ with a batch size of 512.



During the second training stage, we train a T2I model using StyleGAN2~\cite{karras2020analyzing} to follow the experimental protocol used in LAFITE~\cite{zhou2022towards} for fair comparisons.
Specifically, we optimize StyleGAN2 using the Adam optimizer, and set a batch size to 64 and an initial learning rate to $2.5 \times 10^{-3}$. %2.5e-3. 
%
Also, $R_1$ regularization is also performed for the discriminator every 16 iterations to save training time. 




\subsection{Unsupervised Setting Results}
\label{subsec:unsupervised}
Table~\ref{tab:stylegan2_on_coco}  shows the text-to-image generation results on the MS-COCO~\cite{lin2014microsoft} and CC3M~\cite{cc3m} datasets under the unsupervised setting, where the image captions are not available during training.
As demonstrated in the table, VDL achieves the best performance in terms of $\text{Sim}_\text{txt}$, FID, and IS by large margins both on the two datasets.
Although the noise injection for text prediction in LAFITE~\cite{zhou2022towards} degrades $\text{Sim}_\text{txt}$, LAFITE achieves better T2I performance in terms of FID and IS than CLIP-GEN. 
This is partly because the regions corresponding to the noisy text features predicted by LAFITE are larger than the deterministic point features given by CLIP-GEN and LAFITE consequently has more chance to identify accurate text features within the region.
%






\subsection{Semi-Supervised Setting Results}
\label{subsec:semi-supervised}
In Table~\ref{tab:Semi}, we present the performance of LAFITE~\cite{zhou2022towards} and VDL in semi-supervised learning scenarios on the MS-COCO dataset~\cite{lin2014microsoft}, where only a fraction of image and text pairs are accessible; the ratio of $0.0$ indicates the unsupervised setting.
According to the table, the proposed method outperforms LAFITE by large margins in terms of FID and IS.
Also, VDL trained even in the unsupervised setting outperforms LAFITE with the ratio of $0.3$, which implies that the proposed method is more annotation-efficient.
%
As more text captions become available, VDL obtains higher IS, $\text{Sim}_\text{txt}$, and $\text{Sim}_\text{img}$ while FID is unfortunately saturated at an early stage.
In terms of $\text{Sim}_\text{txt}$ under semi-supervised settings, LAFITE outperforms VDL. 
That this is partly because LAFITE trains a learnable network on the paired data by making the predicted text embeddings close to the true ones.
However, the strategy turns out to be ineffective for improving FID and even degrades the IS score.
%
% 
%
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{./CVPR2023_MainFigure.pdf}
%\vspace{-6mm}
\caption{Qualitative results on the MS-COCO and Conceptual Captions 3M datasets using StyleGAN2. VDL generates visually higher-quality images than LAFITE and CLIP-GEN.}
\label{fig:coco_stylegan2_qual}
\end{figure*}
%
% 
%
%
\begin{table}[t!]
    \caption{Comparison between the KL divergence with the JS divergence under the unsupervised setting on the MS-COCO dataset using StyleGAN2. 
    KLD optimizes~\eqref{eq:stage1_objective} with the assumption that the variational distribution follows the Gaussian distribution with Gaussian mixture models for the prior.
    DualKLD optimizes the dual representation of the KL objective in~\cite{donsker1983asymptotic}.
    }
\scalebox{0.90}{
    \begin{tabular}{lccccc}
        \toprule
         Method & IS ($\uparrow$) & FID ($\downarrow$) & $\text{Sim}_\text{txt}$ ($\uparrow$) & $\text{Sim}_\text{img}$ ($\uparrow$) \\
        %
        \midrule
        KLD & $20.89$ & $34.42$ & 0.1396 & 0.2833 \\
        DualKLD & $27.83$ & $15.60$ & \textbf{0.6236} & 0.7440\\
        JSD (VDL) & $\textbf{30.30}$ & $\textbf{13.22}$ & 0.6104 & \textbf{0.7655} \\ 
        \bottomrule
    \end{tabular}
    }
    \centering
    \label{tab:KLvsJSD}
\end{table}
%
%
\subsection{Analysis}
\label{subsec:ablation}







%
\subsubsection{Jensen-Shannon Divergence}
\label{subsubsec:JSD}
We study the effect of using the JS divergence instead of KL divergence under the unsupervised setting with the StyleGAN2 architecture on MS-COCO. 
As reported in Table~\ref{tab:KLvsJSD}, our strategy using the JS divergence is more effective than employing the KL divergence and its variation. 
Specifically, we compute the objective in~\eqref{eq:stage1_objective} using two different ways. 
First, motivated by variational autoencoders~\cite{kingmaauto}, we model the variational and prior distributions with a Gaussian distribution and its mixture, respectively, and this approach is referred to as KLD.
%
The other method denoted by DualKLD replaces the KL divergence with its dual form, the Donsker-Varadhan representation~\cite{donsker1983asymptotic}, which also performs the minimax optimization.
In the case of KLD, the performance significantly degrades because the Gaussian assumptions for the variational and prior distributions are not effective; the Gaussian distribution has a non-zero density outside the unit-hypersphere, where CLIP text features are not located.
On the other hand, DualKLD, which is free from the distribution restriction, outperforms KLD although it is still worse than our approach.
However, we observe that DualKLD is sensitive to the hyperparameter partly due to its unbounded property contrary to the JS divergence.  






%
\begin{table}[t!]
\caption{Ablation study results on MS-COCO with StyleGAN2 under an unsupervised setting. 
     VDL w/o $S_\text{VDL}$ directly predicts a text feature and then normalizes it to locate at the unit-hypersphere without using $S_\text{VDL}$ while VDL w/o $\mathcal{L}_\text{rkd}$ does not employ $\mathcal{L}_\text{rkd}$. 
    }
\scalebox{0.90}{
    \begin{tabular}{lccccc}
        \toprule
         Method & IS ($\uparrow$) & FID ($\downarrow$) & $\text{Sim}_\text{txt}$ ($\uparrow$) & $\text{Sim}_\text{img}$ ($\uparrow$) \\
        \midrule
        VDL w/o $S_\text{VDL}$ & $19.86$ & $36.37$ & 0.5498 & 0.5256 \\
        VDL w/o $\mathcal{L}_\text{rkd}$ & $28.45$ & $15.75$ & \textbf{0.6128} & \textbf{0.7637} \\
        VDL (Ours) & $\textbf{30.30}$ & $\textbf{13.22}$ & \textbf{0.6104} & \textbf{0.7655} \\ 
        \bottomrule
    \end{tabular}
    }
    \centering
    \label{tab:ablation}
\end{table}
%




\subsubsection{Component analysis}
We analyze the contributions of the individual components in our approach.
As presented in Table~\ref{tab:ablation}, $S_\text{VDL}$ is helpful for improving $\text{Sim}_\text{txt}$, $\text{Sim}_\text{img}$, and T2I performance in unsupervised settings.
Although $\mathcal{L}_\text{rkd}$ is conceptually irrelevant to reduce the discrepancy between the true and predicted text features, it improves generation performance by learning relational embeddings between two modalities.






\subsubsection{Qualitative Results}
Figure~\ref{fig:coco_stylegan2_qual} visualizes generation results on the MS-COCO and CC3M datasets using CLIP-GEN~\cite{wang2022clip}, LAFITE~\cite{zhou2022towards}, and VDL. 
As illustrated in the figure, the proposed method successfully generates images based on given sentences with enhanced visual quality while the others sometimes fail to understand the overall meaning of text captions. 


% 



 