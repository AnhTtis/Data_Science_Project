% !TEX root = ./../paper.tex
\section{Related Work}
\label{sec:related}
%
Text-to-image generative models have shown astonishing performance via learning with large-scale datasets composed of image-text pairs.
Existing algorithms often represent each image with a sequence of discrete tokens and learn autoregressive~\cite{dalle,VQGAN,rqvae,cogview,nuwa} or bidirectional~\cite{draftNrevise} transformers to generate high-resolution images given text inputs.
Recently, the introduction of diffusion models~\cite{ddpm,cascadedDiff,adm,clsfree} has paved the way to learn large-scale T2I models and generate high-quality images conditioned on text.
%


For training T2I models without captions describing images, previous approaches~\cite{wang2022clip,zhou2022towards,kNNDiff} typically exploit the pretrained CLIP~\cite{radford2021learning} to approximate missing text captions.
Specifically, CLIP-GEN~\cite{wang2022clip} assumes that a CLIP image embedding is perfectly aligned with the corresponding text embedding, and utilizes the image embedding as a proxy of its text embedding for T2I generation.
On the other hand, LAFITE~\cite{zhou2022towards} adds a Gaussian random noise to the image embedding for estimating the unknown true text embedding.
However, these algorithms fail to consider the underlying structure of the text embeddings, which eventually results in the imprecise approximation of text embedding.
Retrieval-based approaches~\cite{kNNDiff,rdm} employ image features similar to CLIP-GEN~\cite{wang2022clip} for text conditions while $k$-nearest image embeddings are additionally utilized for the construction of the conditions; note that these approaches are orthogonal to our method and can be combined with the proposed method to further enhance generation performance.

On the other hand, we propose a principled framework relying on a variational inference to effectively reduce the discrepancy between the true embedding employed during inference and the approximated one drawn by the variational distribution used for training.

