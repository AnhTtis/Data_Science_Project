% !TEX root = ./../paper.tex

%\section{VDL-GAN: Variational Distribution Learning via Adversarial Training}
\section{Variational Distribution Learning}
\label{sec:two_stage_framework}
This section describes the first stage of our algorithm based on variational distribution learning (VDL). 
VDL approximates the posterior of text features given image embeddings, $q_{\phi^{\bzt}}(\bzt | \bzi)$, via an adversarial training and reconstructs the image embeddings given the text features using $p_{\theta^\bzi} (\bzi | \bzt)$.







\subsection{Sampling}
\label{sub:sampling}
We first discuss the sampling procedure in VDL that involves the sampling of $\bzth$ for training to estimate the text embeddings and the sampling of $\bzih$ for inference to obtain the image embeddings.
Also, we describe the sampling process of the text prior $\bzpt$, which is only required for training.%
%





\subsubsection{Generating Samples for Text Embedding}
\label{subsub:variational_distribution_samples}
%
Let $G(\cdot)$ with parameters of $\phi^\bzt$ be an encoder implemented with a multilayer perceptron, 
Using the encoder, we draw a sample $\bzth$ from an implicitly defined variational distribution $q_{\phi^{\bzt}}(\bzt | \bzi)$ as follows:
%
\begin{align}
\bzth &\sim S_\text{VDL}(z_\text{img}, G, r)  \nonumber \\
 &:= \text{Normalize} \left( \bzi + r \cdot \frac{ G(\bzi)}{\| G(\bzi) \|} \right), 
%
\label{eq:fake_txt}
\end{align}
%
where $S_\text{VDL}(\cdot, \cdot, \cdot)$ denotes our sampling strategy and $r \geq 0$ is a hyperparameter. 
The following proposition validates the high cosine similarity between the estimated text sample $\bzth$ and the image feature $\bzi$ that has a similar representation with the unknown text embedding $\bzt$.
%
%
%
%
\begin{proposition}
\vspace{0.1cm}
Let $\bzth$ be a sample obtained by the proposed sampling strategy $S_\text{VDL}$ defined in \eqref{eq:fake_txt} based on $\bzi$. 
Then, the following inequality always holds for $G(\cdot;\phi^\bzt)$ with arbitrary values of its parameter $\phi^\bzt$:
% 
%
%
\begin{align}
\tbzth  \bzi \geq \sqrt{1-r^2}, \nonumber
\end{align}
where $r<1$.
\label{propostion:1}
\end{proposition}
%
%
%
\noindent 
We provide the proof of this proposition in the supplementary document.
Proposition~\ref{propostion:1} implies that a sample $\bzth$ drawn by~\eqref{eq:fake_txt} is guaranteed to be close to $\bzi$ by setting $r$ to a sufficiently small number in $[0,1]$. 
The proposed sampling strategy also results in the acceleration of the optimization procedure by constraining the search space of the variational distribution.
Thanks to the optimization procedure, $\bzth$ is expected to achieve high cosine similarity with $\bzt$, which is also empirically confirmed in Section~\ref{sec:experiments}.





\subsubsection{Generating Samples for Image Embedding}
\label{subsub:image_embedding_reconstruction_samples}
During training, we reconstruct the image embedding samples to compute the second expectation in~\eqref{eq:vi_detail} while we obtain the embeddings to synthesize the images during inference.
Similar to the sampling process of $q(\bzt | \bzi)$, we reconstruct the image embedding based on the text feature $\bzt$  via the sampling process of $p(\bzi | \bzt)$ using a decoder $F(\cdot)$ with $\theta^\bzi$, which is given by   
%
\begin{align}
\bzih &\sim S_\text{VDL} (\bzt, F, r) \nonumber \\ 
&=\text{Normalize} \left( \bzt + r \cdot \frac{ F(\bzt)}{\| F(\bzt) \|} \right), 
\label{eq:fake_img}
\end{align} 
%
where $\bzih$ is a reconstructed image embedding. 
According to Proposition~\ref{propostion:1}, $\bzih$ is also expected to have a high cosine similarity with $\bzt$.   






\subsubsection{Generating Prior Samples for Text Embedding}
\label{subsub:prior_samples}
For the first stage of training, we require samples from the prior to minimize the difference between the variational and text prior distributions with respect to the parameters in the encoder $G(\cdot)$.
Refer to Section~\ref{sub:training} for the detailed optimization procedure.
We draw a sample $\bzpt$ from the prior distribution using a text corpus, which is available online.
Note that we do not employ an additional dataset consisting of image-text pairs, but randomly choose a sentence $\bxt'$ from the text corpus.
Using the sampled sentence, the representation of the prior sample is obtained by
%
\begin{align}
\bzpt = f_\text{txt}(\bxt'). 
\end{align}
%
%
%
In our framework, the pretrained CLIP encoder is fixed to reduce the computational burden. 
%


%
\begin{figure*}[t]
\centering
\begin{subfigure}[t]{0.43\linewidth}
\centering
\includegraphics[width=0.96\linewidth]{./CVPR2023_Frameworks_a.pdf}
\caption{First Stage Training}
\label{fig:framework_a}
\end{subfigure}
~ 
\begin{subfigure}[t]{0.26\linewidth}
\centering
\includegraphics[width=0.96\linewidth]{./CVPR2023_Frameworks_b.pdf}
\caption{Second Stage Training}
\label{fig:framework_b}
\end{subfigure}
~ 
\begin{subfigure}[t]{0.26\linewidth}
\centering
\includegraphics[width=0.96\linewidth]{./CVPR2023_Frameworks_c.pdf}
\caption{Inference}
\label{fig:framework_c}
\end{subfigure}
\caption{Overview of the proposed method. We illustrate the first and second stage training procedures and then depict the inference step.} 
\label{fig:framework}
\end{figure*}
%
%




\subsection{Training}
\label{sub:training}
\subsubsection{Robust Objective}
\label{subsub:robust_objective}
The optimization of \eqref{eq:vi_detail} with respect to $ \theta^\bzi $ and $\phi^{\bzt}$ is equivalent to solve the following minimization problem because $\log p_{\theta^{\bxi}}(\bxi | \bzi, \bzt)$ is irrelevant.
Hence, the objective function for the first stage is given by
%
\begin{align}
\min_{\theta^\bzi, \phi^\bzt} \, & D_\text{KL} (q_{\phi^{\bzt}}( \bzt | \bzi) || p (\bzt) )\nonumber \\ 
 -&\mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)} [\log p_{\theta^\bzi} (\bzi | \bzt) ].
\label{eq:stage1_objective}
\end{align}
%
However, the KL divergence is generally intractable unless the two distributions belong to specific families of probability distributions, \eg, Gaussian distribution.  
Such restricted distributions are different from the true distributions of the CLIP features, which only have non-zero densities at the surface of the unit hypersphere in the feature space.
Therefore, the use of the restricted distributions for modeling the variational and prior distributions would cause high approximation errors~\cite{cremer2018inference}.
To reduce the distribution gap, the von Mises-Fisher distribution can be employed but this probability density involves the Bessel function, which also leads to the intractable KL divergence.



To address the issue, we alternatively minimize the difference between the two distributions using the Jensen-Shannon (JS) divergence that also enforces the two distributions to become identical via its minimization.
Contrary to the KL divergence, the JS divergence is always bounded and free from density assumption, which allows us to use implicit generative networks for flexible modeling of the two distributions.
We observe that replacing the KL divergence with the JS divergence is more effective, which will be empirically validated in Section~\ref{subsubsec:JSD}.
By using the property discussed in~\cite{goodfellow2014generative, nowozin2016f}, the objective is reformulated as the following minimax game, which is given by
%
\begin{align}
\label{eq:stage1_objective_adv}
\min_{\theta^\bzi, \phi^\bzt} \hspace{-.5mm} \max_{\rho^D}  ~~ & \mathbb{E}_{p (\bzt)}[\log D(\bzt)] \nonumber \\
+ & \mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)} \left[ \log (1 - D(\bzt)) \right] \nonumber \\
- & \mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)} \left[ \log p_{\theta^{\bzi}}(\bzi | \bzt) \right], 
\end{align}
%
where $D(\cdot)$ is a discriminator parametrized by $\rho^D$.
%
The last term of~\eqref{eq:stage1_objective_adv} denoted by $\mathcal{L}_{\text{recon}}$ is expressed as
%
\begin{align}
\mathcal{L}_{\text{recon}} :&= \mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)} \left[ - \log p_{\theta^{\bzi}}(\bzi | \bzt) \right] \nonumber \\ 
&=\frac{1}{2\sigma^2} \mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)} \left[ \| \bzi - \bzih)  \|^2 \right], %+ C, 
%
\label{eq:recon}
\end{align}
%
%
where we employ the $\ell_2$ loss for reducing the negative log-likelihood with a balancing factor $\sigma$.




%
\begin{table*}[t!]
\caption{Results of unsupervised text-to-image generation on the MS-COCO~\cite{lin2014microsoft} and Conceptual Captions 3M~\cite{cc3m} datasets using StyleGAN2~\cite{karras2020analyzing}.
    Captioning indicates a text-to-image generation baseline method relying on a state-of-the-art image captioning algorithm~\cite{zhang2021vinvl}, where the results of the baselines are retrieved from~\cite{zhou2022towards}. 
    Methods with asterisks * report the results of our reproduction.
    A bold-faced number denotes the best performance in each column while `--' indicates that the number is unavailable.
    }    
\scalebox{0.90}{
    \begin{tabular}{ccccccc}
        \toprule
      T2I Model & Dataset & Method & IS ($\uparrow$) & FID ($\downarrow$) & $\text{Sim}_\text{txt}$ ($\uparrow$) & $\text{Sim}_\text{img}$ ($\uparrow$)  \\ %
        \midrule
        \multirow{8}{*}{StyleGAN2~\cite{karras2020analyzing}}  & \multirow{4}{*}{MS-COCO~\cite{lin2014microsoft}}  & Captioning~\cite{zhang2021vinvl} & $15.83$ & $56.36$ & - & - \\ %
        & & CLIP-GEN*~\cite{wang2022clip} & $16.94$ & $58.63$ & 0.3042 & -\\ %
        % 
        & & $\text{LAFITE}$~\cite{zhou2022towards} & $27.20$ & $18.04$ & 0.0965 &- \\ 
        & & VDL (Ours) & $\textbf{30.30}$ & $\textbf{13.22}$ & \textbf{0.6104} & \textbf{0.7655} \\
       %
        \cmidrule{2-7}
        & \multirow{3}{*}{Conceptual Captions 3M~\cite{cc3m}} &  CLIP-GEN*~\cite{wang2022clip} & $7.88$ & $84.16$ & 0.2896 &-\\ 
        & & $\text{LAFITE}*$~\cite{zhou2022towards} & $16.06$ & $22.95$ & 0.0912 & -\\ 
        & & VDL (Ours) & $\textbf{23.66}$ & $\textbf{17.37}$ & \textbf{0.6237} & \textbf{0.7105} \\ 
         \bottomrule
    \end{tabular}
    }
    \centering
    \label{tab:stylegan2_on_coco}
\end{table*}
%



\subsubsection{Relational Representation Transfer}
\label{subsub:weak_supervision}
In addition, we encourage the approximate text samples $\bzth$ to mimic the correlation of the observed image embedding samples $\bzi$ in order to mitigate the challenge posed by the lack of supervision. 
The intuition behind this strategy is that the structural relation of text embeddings will resemble that of image representations. 
For example, the two text embeddings should be located close if the image representations are similar, and vice versa.
To impose the constraint on the text embedding samples, we employ a relational knowledge distillation framework~\cite{park2019relational}, which makes a student mimic the relations among data embeddings given by a teacher.
In our framework, we view image embeddings as teacher samples while text embeddings are regarded as student ones.
Therefore, we additionally minimize the relational distillation loss, which is given by
%
\begin{equation}
\hspace{-0.2cm} \mathcal{L}_\text{rkd} := \mathbb{E}[ \ell_\delta(\psi_A(\bzii, \bzij, \bzik) \hspace{-0.05cm} - \hspace{-0.05cm} \psi_A(\bzthi, \bzthj, \bzthk)) ], 
\label{eq:rkd_angle_loss}
\end{equation}
%
where the expectation is taken over any triplet image embeddings ($\bzii, \bzij, \bzik$) and the corresponding samples drawn by the variational distribution over ($\bzthi, \bzthj, \bzthk$).
In the above equation, $\ell_\delta(\cdot)$ and $\psi_A(\cdot, \cdot, \cdot)$ are defined as  
\begin{align}
\ell_\delta(a) & :=
\begin{cases}
    \frac{1}{2}a^2,& \text{for } \| a \| \leq \delta, \\
    \delta \cdot (\| a \| - \frac{1}{2} \delta) ,              & \text{otherwise},
\end{cases}
\\
\psi_A(\bz^i, \bz^j, \bz^k) &:= \text{sim}(\bz^i- \bz^j, \bz^i-\bz^k),
\end{align}
where sim$(\cdot, \cdot)$ denotes the cosine similarity between two vectors.
We set the hyperparameter $\delta$ of the Huber loss $\ell_\delta (\cdot)$ to 1 instead of searching for it.








\subsubsection{Total Objective}
\label{subsub:final_objective}
%
In summary, the final objective function of the first stage is given by
%
\begin{align}
&\min_{\theta^\bzi, \phi^\bzt}  \max_{\phi^D} \mathcal{L}_{\text{adv}} + \mathcal{L}_{\text{recon}} + \lambda_\text{rkd} \mathcal{L}_{\text{rkd}},
\end{align}
%
where $\lambda_\text{rkd}$ is a hyperparameter and $\mathcal{L}_\text{adv}$ denotes the first two terms of~\eqref{eq:stage1_objective_adv} constituting the adversarial loss.
%
Figure~\ref{fig:framework_a} illustrates the optimization procedure of the first stage.
In the case of the semi-supervised setting, we additionally employ the reconstruction loss for labeled examples, $\mathcal{L}_\text{semi}$, which enforces the variational text samples to mimic the true ones.
The reconstruction loss is formally given by  
%
\begin{equation}
\mathcal{L}_\text{semi} :=   \mathbb{E}_{q_{\phi^{\bzt}}( \bzth | \bzi)} \left[ \| \bzth -\bzt \|_1  \right],
%
\label{eq:semi}
\end{equation}
%
where $\| \cdot \|_1$ indicates the $\ell_1$-norm.