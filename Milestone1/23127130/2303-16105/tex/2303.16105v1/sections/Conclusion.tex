% !TEX root = ./../paper.tex


%-------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}
We presented an annotation-efficient method for text-to-image generation when image and text caption pairs are rarely available or text information is completely inaccessible.  
To address the challenge, we employ the off-the-shelf CLIP model to estimate hidden text features given observable images, where we rely on the CLIP's multi-modal joint embedding quality.
To further improve the quality of text embedding, we approximate its intractable true posterior probability by exploiting the variational inference technique.
Given the inferred features and their image embeddings, we learn a conditional generative model to reconstruct images. 
Experimental results verify that the proposed method achieves outstanding performance on the unsupervised and semi-supervised learning environments.

