% !TEX root = ./../paper.tex
\section{Appendix}
\label{sec:appendix} 
This document first provides the proof of Proposition 1. 
Then, we present additional results on the LN-COCO~\cite{pont2020connecting} dataset under the unsupervised T2I generation task based on StyleGAN2~\cite{karras2020analyzing}. 
We also evaluate the performance of the T2I results using a diffusion-based text-to-image generative model~\cite{LDM} to validate the generality of the proposed approach.
Finally, we demonstrate additional qualitative results supplementing Figure 3 of the main paper, which visualizes the synthesized results on the MS-COCO~\cite{lin2014microsoft} and Conceptual Captions 3M~\cite{cc3m} datasets given by CLIP-GEN~\cite{wang2022clip}, LAFITE~\cite{zhou2022towards}, and VDL based on StyleGAN2 under the unsupervised setting.
%





\subsection{Proof of Proposition 1}
\label{sec:Proof}
%
\begin{proposition}
%
Let $\bzth$ be a sample obtained by the proposed sampling strategy $S_\text{VDL}$ defined in \eqref{eq:fake_txt} based on $\bzi$. 
Then, the following inequality always holds for $G(\cdot;\phi^\bzt)$ with arbitrary values of its parameter $\phi^\bzt$:
% 
\begin{equation}
\tbzth  \bzi \geq \sqrt{1-r^2}, \nonumber
\end{equation}
where $r<1$.
\label{propostion:1}
\end{proposition}
%
\begin{proof}
%
%
The inner product can be expressed as 
%
\begin{equation}
\tbzth  \bzi = \tbzi  \frac{\bzi + r \cdot \bgi}{\| \bzi + r \cdot \bgi \|},  
\label{eq:prop_LHS}
\end{equation}
%
where $\bgi$ is equivalent to $\text{Normalize}(G(\bzi))$.
In addition, the denominator in \eqref{eq:prop_LHS} is given by
%
\begin{align}
\| \bzi + r \cdot \bgi \| &= \sqrt{(\bzi + r \cdot \bgi)^T (\bzi + r \cdot \bgi)} \nonumber \\
&= \sqrt{1 + 2 r \cdot \tbzi \bgi + r^2}.
\label{eq:magnitude_variational_samples}
\end{align}
%
Based on the two equations, we have 
%
\begin{align}
\tbzth  \bzi &=  \frac{\tbzi (\bzi + r \cdot \bgi)}{ \sqrt{1 + 2 r \cdot \tbzi \bgi + r^2}} \nonumber \\
&=  \frac{1 + r \cdot \tbzi \bgi}{ \sqrt{1 + 2 r \cdot \tbzi \bgi + r^2}} \nonumber \\
%
&=\frac{(1 + 2 r \cdot \tbzi \bgi + r^2) + (1 - r^2)}{ 2\sqrt{1 + 2 r \cdot \tbzi \bgi + r^2}} \nonumber \\
&\geq \sqrt{1-r^2}, \nonumber
\end{align}
%
where the last inequality is derived by using the inequality of arithmetic and geometric means.
%  
\end{proof}
%





\subsection{Experiments on LN-COCO}
\label{sec:Unsupervised}
%
We compare the proposed method with CLIP-GEN~\cite{wang2022clip} and LAFITE~\cite{zhou2022towards} on the LN-COCO~\cite{pont2020connecting} dataset using StyleGAN2 under the unsupervised setting. 
As presented in Table~\ref{tab:LN-COCO}, our method outperforms existing approaches by large margins.
Figure~\ref{fig:mscoco} depicts synthesized images given by VDL and CLIP-GEN~\cite{wang2022clip}, where the results of LAFITE are not provided since its pre-trained model is not publicly available. 
%





\subsection{Ablation on T2I models}
\label{sec:LDM}
We remark that our approach is model-agnostic; any type of conditional image generation network is applicable to our framework.
To validate the effectiveness of the proposed method without carefully designing the T2I network, we replace StyleGAN2~\cite{karras2020analyzing} with a diffusion model, Latent Diffusion Model (LDM)~\cite{LDM}, and perform experiments on the MS-COCO~\cite{lin2014microsoft} and Conceptual Captions 3M~\cite{cc3m} (CC3M) datasets under the unsupervised setting.
  





  
  
\subsubsection{Implementation Details}
For the second-stage training, we optimize LDM~\cite{LDM} for 150k and 300k iterations on the MS-COCO~\cite{lin2014microsoft} and Conceptual Captions 3M~\cite{cc3m} datasets using the Adam optimizer with a batch size of 64 and an initial learning rate of $6.4 \times 10^{-5}$.
We set the resolution of the latent space to 64, where a pretrained Vector Quantized GAN~\cite{VQGAN} is selected as a latent perceptual compression network without an extra fine-tuning.
Following the latent conditioning strategy used in~\cite{preechakul2022diffusion}, we inject CLIP features to the noisy predictions of the backbone network based on U-Net~\cite{ronneberger2015u} inside the LDM framework~\cite{LDM}.
Specifically, we replace the last group normalization layer in each residual block of the U-Net with an adaptive group normalization layer whose scale and shift parameters are computed by applying a single fully connected layer to the temporal positional embeddings.
For conditioning given sentences, the outputs of the normalization layer are further multiplied with the projected CLIP features using a single fully connected layer.








\subsubsection{Results unser Unsupervised Setting}
We present quantitative results in Table~\ref{tab:ldm} while the generated images on the MS-COCO~\cite{lin2014microsoft} and Conceptual Captions 3M~\cite{cc3m} datasets are provided in Figure~\ref{fig:mscoco_ldm} and \ref{fig:cc3m_ldm}, respectively.
These results show that VDL archives superior performance when combined with the diffusion based LDM~\cite{LDM} for the T2I model, and the proposed method is agnostic to the types of the T2I network.
%


%-----------------------------------------------------------------
\subsection{Additional Qualitative Results}
\label{sec:additional_qualitative}
Figure~\ref{fig:mscoco} and \ref{fig:cc3m} visualize additional qualitative results from the proposed approach compared to existing methods.
The results clearly show that VDL generates visually more faithful and realistic images considering the given text descriptions and the natural image distribution while the other two methods often fail to meet text conditions and/or generate natural images.
%






%
\begin{table*}[t!]
    \caption{Results of unsupervised text-to-image generation on the LN-COCO~\cite{pont2020connecting} dataset using StyleGAN2~\cite{karras2020analyzing}.
    Methods with asterisks (*) report the results of our reproduction. 
    A bold-faced number denotes the best performance in each column. 
   %
    }
\scalebox{0.90}{
    \begin{tabular}{lccccccc}
        \toprule
         T2I Model & Dataset & Method & IS ($\uparrow$) & FID ($\downarrow$) & $\text{Sim}_\text{txt}$ ($\uparrow$) & $\text{Sim}_\text{img}$ ($\uparrow$) \\
        %
        \midrule
        %
       \multirow{3}{*}{StyleGAN2~\cite{karras2020analyzing} } & \multirow{3}{*}{LN-COCO~\cite{pont2020connecting}} &CLIP-GEN*~\cite{wang2022clip} & $12.12$ & $83.87$ & $0.2750$ & $-$ \\
       & &  LAFITE~\cite{zhou2022towards} & $18.49$ & $38.95$ & $0.0872$ & $-$ \\
       & &  VDL (Ours) & $\textbf{21.55}$ & $\textbf{31.33}$ & $\textbf{0.6118}$ & $\textbf{0.7025}$ \\ %& $-$ & $-$ & $-$ & $-$\\
        \bottomrule
    \end{tabular}
    }
    \centering
    \label{tab:LN-COCO}
\end{table*}
%
%
%
%
%
\begin{figure*}[!t]
\centering
%
\includegraphics[width=\linewidth]{./CVPR2023_LNCOCO_StyleGAN2_Supple.pdf}
%
\caption{Qualitative results on the LN-COCO dataset using StyleGAN2. VDL generates visually higher-quality images than CLIP-GEN.}
%
\label{fig:mscoco}
%
\end{figure*}
%
%




%
\begin{table*}[t!]
\caption{Results of unsupervised text-to-image generation on the MS-COCO~\cite{lin2014microsoft} and Conceptual Captions 3M~\cite{cc3m} datasets using LDM~\cite{LDM}. 
    }    
\scalebox{0.90}{
    \begin{tabular}{ccccccc}
        \toprule
      T2I Model & Dataset & Method & IS ($\uparrow$) & FID ($\downarrow$) & $\text{Sim}_\text{txt}$ ($\uparrow$) & $\text{Sim}_\text{img}$ ($\uparrow$)  \\ 
      %
        %
        \midrule
        %
        \multirow{6}{*}{LDM~\cite{LDM}}  & \multirow{3}{*}{MS-COCO~\cite{lin2014microsoft}}  & CLIP-GEN*~\cite{wang2022clip} & $12.96$ & $48.14$ & 0.3042 & -\\ 
        %
        & & LAFITE*~\cite{zhou2022towards} & $16.53$ & $23.92$ & 0.0965 &- \\ 
        %
       %
        & & VDL (Ours) & $\textbf{23.25}$ & $\textbf{13.68}$ & \textbf{0.6104} & \textbf{0.7655} \\ 
        %
        %
        \cmidrule{2-7}
        %
        & \multirow{3}{*}{Conceptual Captions 3M~\cite{cc3m}} &  CLIP-GEN*~\cite{wang2022clip} & $10.08$ & $47.53$ & 0.2896 &-\\ 
        %
        & & LAFITE*~\cite{zhou2022towards} & $10.98$ & $33.98$ & 0.0912 & -\\ 
        %
        & & VDL (Ours) & $\textbf{15.09}$ & $\textbf{23.03}$ & \textbf{0.6237} & \textbf{0.7105} \\ 
         \bottomrule
    \end{tabular}
    }
    \centering
    \label{tab:ldm}
\end{table*}
%
%
\begin{figure*}[!t]
\centering
%
\includegraphics[width=\linewidth]{./CVPR2023_MSCOCO_LDM_Supple.pdf}
%
\caption{Qualitative results on the MS-COCO dataset using LDM. VDL generates visually higher-quality images than LAFITE and CLIP-GEN.}
%
\label{fig:mscoco_ldm}
\end{figure*}
%
%
\begin{figure*}[!t]
\centering
%
\includegraphics[width=\linewidth]{./3M_LDM_Supple.pdf}
%
\caption{Qualitative results on the Conceptual Captions 3M dataset using LDM. VDL generates visually higher-quality images than LAFITE and CLIP-GEN.}
%
\label{fig:cc3m_ldm}
\end{figure*}
%
%

%
\begin{figure*}[!t]
\centering
%
%
\includegraphics[width=\linewidth]{./CVPR2023_MSCOCO_StyleGAN2_Supple.pdf}
%
\caption{Additional qualitative results on the MS-COCO dataset using StyleGAN2. VDL generates visually higher-quality images than LAFITE and CLIP-GEN.}
%
\label{fig:mscoco}
\end{figure*}
%
%
\begin{figure*}[!t]
\centering
%
\includegraphics[width=\linewidth]{./3M_StyleGAN2_Supple.pdf}
%
\caption{Additional qualitative results on the Conceptual Captions 3M dataset using StyleGAN2. VDL generates visually higher-quality images than LAFITE and CLIP-GEN.}
%
\label{fig:cc3m}
\end{figure*}
%




