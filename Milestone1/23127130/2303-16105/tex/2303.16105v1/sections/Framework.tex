% !TEX root = ./../paper.tex
%
\section{Main Framework}
%
\label{sec:proposed algorithm}
%
This section presents the existing unsupervised training approaches for T2I models.
Then, we formulate our variational inference framework for unsupervised training that effectively reduces the discrepancy between true and estimated text embeddings.
Figure~\ref{fig:graphical_model} illustrates the graphical model of the data generating process in our approach.
%




%
\begin{figure}[t]
\centering
\includegraphics[width=0.96\linewidth]{./CVPR2023_OurGraphicalModels_1Column.pdf}
%
\caption{Probabilistic graphical models of the proposed method, where shaded nodes represent observable random variables while unshaded nodes represent hidden random variables.}
\label{fig:graphical_model}
\end{figure}
%





\subsection{Unsupervised Training of T2I Models}
Unsupervised training of T2I models learns to generate images without textual annotations while the model generates high-quality images based on given captions at inference time.
Conventional supervised training of T2I models exploits both an image, $\bxi$, and its textual caption, $\bxt$, to estimate a conditional distribution $p(\bxi|\bxt)$.
However, unsupervised training assumes that the text condition $\bxt$ is unavailable during training.
Existing approaches estimate a latent text representation $\bzt$ and formulate the T2I model as a task to derive the conditional distribution $p(\bxi | \bzt)$.
In other words, an encoder $E(\cdot)$ approximates the text condition, \ie, $\bzth=E(\bxi)$, during training and $\bzth$ is replaced with a representation of a given text sentence $\bxt$ at inference time.






The previous studies commonly incorporate the pretrained CLIP model~\cite{radford2021learning}, which consists of two separated encoders for images $f_\text{img}(\cdot)$ and texts $f_\text{txt}(\cdot)$. 
%
The vision-language model is learned to make a pair of image and text embeddings, denoted respectively by $f_\text{img}(\bxi)$ and $f_\text{txt}(\bxt)$, have a high cosine similarity.
% where the two embeddings are located at a unit hypersphere. 
CLIP-GEN~\cite{wang2022clip} and retrieval-based models~\cite{kNNDiff,rdm} approximate the text condition using the CLIP image embedding, $\bzth \approx f_\text{img}(\bxi)$, during training.
Since the two embeddings are not exactly aligned, LAFITE~\cite{zhou2022towards} additionally adds a perturbation to the CLIP image embedding and approximates the text condition $\bzt$ as
\begin{align}
\bzth \approx \text{Normalize} ( f_\text{img}(\bxi) + \xi \| f_\text{img}(\bxi) \| \bep / \| \bep \| ),
\label{eq:LAFITE}
\end{align} 
%
where $\| \cdot \|$ and $\text{Normalize}(\cdot)$ are the Euclidean norm and an operator to divide the input by its magnitude while $\xi$ is a hyperparameter and $\bep$ is a random noise drawn from the Gaussian distribution, $\mathcal{N}(\bzero, \bI)$.
Although the previous methods are proposed to approximate the absent text condition, we observe that they are insufficient to reduce the gap between the training and the inference environments of T2I models. 
%\







\subsection{Variational Inference for Training T2I Models}
We aim to formulate a variational inference framework for unsupervised training of T2I models to reduce the discrepancy between training and inference while improving the performance of T2I models.
Given CLIP image and text embeddings, $\bzi=f_\text{img}(\bxi)$ and $\bzt=f_\text{txt}(\bxt)$, our T2I generation model is defined as 
%
\begin{equation} \label{eq:t2i}
    p_{\theta^{\bxi}}(\bxi | \bzi, \bzt),
\end{equation}
%
where $\theta^{\bxi}$ is a set of parameters in the T2I model.
Note that the text representation $\bzt$ is unavailable during training while the image embedding $\bzi$ is absent for inference as illustrated in Figure~\ref{fig:graphical_model}.
Thus, the challenges lie in the precise approximation of the text embedding $\bzth$ based on $\bzi$ during training while approximating $\bzih$ given $\bzt$ at inference.




Our approach naturally maximizes the marginal log-likelihood $\log p_\theta(\bxi)$ with respect to $\theta$, a set of parameters of our data generating process, without the observation of the text embedding $\bzt$ during training.
The log-likelihood $\log p_\theta(\bxi)$ is computed by marginalizing out $\bzt$ as
%
\begin{align}
 \log p_\theta(\bxi) &= \log p_\theta(\bxi, \bzi)  \nonumber \\
 &= \log \int p_\theta(\bxi, \bzi, \bzt) d \bzt. 
%
\label{eq:mar}
\end{align} 
%
%
Unfortunately, it is intractable to compute the marginal log-likelihood either directly or through the estimation of the posterior, $p_\theta(\bzt | \bxi, \bzi)$.
Hence, we employ a variational inference technique that approximates the true posterior $p_\theta(\bzt |\bxi, \bzi)$ using a variational distribution $q_{\phi^{\bzt}}(\bzt | \bzi)$ parametrized by $\phi^{\bzt}$ under the assumption of $q_{\phi^\bzt}(\bzt | \bxi, \bzi)=q_{\phi^\bzt}(\bzt | \bzi)$.






To learn the true posterior distribution via the variational posterior, we equivalently maximize the lower bound on the log-likelihood, which is given by
%
\begin{align} 
\label{eq:vi} 
 &\log p_\theta(\bxi, \bzi) \nonumber\\
 &\geq \mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)} \left[ \log \frac{p_\theta(\bxi, \bzi, \bzt)}{q_{\phi^{\bzt}}(\bzt | \bzi)} \right],
\end{align}
%
where the lower bound in~\eqref{eq:vi} is defined as $\mathcal{L}_\text{ELBO}$.
We achieve the tight lower bound when the variational distribution is exactly same as the true posterior.
%




\paragraph{ELBO}
By factorizing $p_\theta(\bxi, \bzi, \bzt)$ into the product of $p_{\theta^{\bxi}}(\bxi | \bzi, \bzt)$,  $p_{\theta^{\bzi}}(\bzi | \bzt)$, and $p(\bzt)$ according to the probabilistic graphical model in Figure~\ref{fig:graphical_model}, $\mathcal{L}_\text{ELBO}$ is decomposed as follows:
%
\iffalse
%
\begin{align} \nonumber
\mathcal{L}_\text{ELBO} &= \mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)} [ \log p_{\theta^{\bxi}}(\bxi | \bzi, \bzt)] \\ \nonumber
&+ \mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)}[\log p_{\theta^\bzi} (\bzi | \bzt) ] \\ 
&- D_\text{KL} (q_{\phi^{\bzt}}( \bzt | \bzi) || p(\bzt) ). \label{eq:vi_detail}
\end{align}
%
\fi
%
\begin{align} \nonumber
\mathcal{L}_\text{ELBO} &= -D_\text{KL} (q_{\phi^{\bzt}}( \bzt | \bzi) || p(\bzt) ) \\ \nonumber
&+ \mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)}[\log p_{\theta^\bzi} (\bzi | \bzt) ] \\ 
&+ \mathbb{E}_{q_{\phi^{\bzt}}( \bzt | \bzi)} [ \log p_{\theta^{\bxi}}(\bxi | \bzi, \bzt)]. \label{eq:vi_detail}
\end{align}
%
%
We reformulate our task as the maximization of $\mathcal{L}_\text{ELBO}$, which involves the optimization of the T2I model in \eqref{eq:t2i} under the unsupervised setting.
Specifically, the maximization of the first term in \eqref{eq:vi_detail} encourages the estimated sample $\bzth$ drawn from $q_{\phi^{\bzt}}( \bzt | \bzi)$ to lie on the structure of $\bzt$ by minimizing the KL divergence.
Also, the second term learns $p_{\theta^\bzi}(\bzi | \bzt)$ to reconstruct $\bzi$ based on the given $\bzt$ from $q_{\phi^{\bzt}}( \bzt | \bzi)$, where $p_{\theta^\bzi}(\bzi | \bzt)$ is employed to estimate $\bzi$ in \eqref{eq:t2i} at inference.
Finally, the third term implies training T2I models without text captions, where $q_{\phi^\bzt}(\bzt | \bzi)$ estimates the absent text embedding $\bzt$ based on $\bzi$.
The details about how to optimize the model parameters are discussed in the next section.





To maximize $\mathcal{L}_\text{ELBO}$ of \eqref{eq:vi_detail} in practice, we employ the following two-stage optimization procedure:
%
\begin{enumerate}
\item \noindent{\bf Fix $\{ \theta^{\bxi} \} $ and optimize $ \theta^\bzi$ and $\phi^{\bzt}$.}

With the parameter $\{ \theta^{\bxi} \} $ fixed, maximize the objective in \eqref{eq:vi_detail} with respect to $ \theta^\bzi $ and $\phi^{\bzt}$.

\item \noindent{\bf Fix $\{ \theta^\bzi, \phi^{\bzt} \}$ and optimize $\theta^{\bxi}$.}

With the parameters $\{ \theta^\bzi, \phi^{\bzt} \}$ fixed, maximize the objective in \eqref{eq:vi_detail} with respect to $\theta^{\bxi}$.

\end{enumerate} 
In other words, we first train the generative and variational parameters to approximate $\bzi$ and $\bzt$, and then train a T2I model under the unsupervised setting.
%
We present the detailed description of the first and second stages in Section~\ref{sec:two_stage_framework} and~\ref{sub:training_t2i}, respectively.






