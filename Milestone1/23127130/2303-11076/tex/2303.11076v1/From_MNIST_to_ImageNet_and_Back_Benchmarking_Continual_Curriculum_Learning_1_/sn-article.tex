%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[sn-mathphys]{sn-jnl}% Math and Physical %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%

% Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%



%%%%%%%%%%%%%
% MACH FORMAT
%%%%%%%%%%%%
% \jyear{2021}%

% %% as per the requirement new theorem styles can be included as shown below
% \theoremstyle{thmstyleone}%
% \newtheorem{theorem}{Theorem}%  meant for continuous numbers
% %%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
% %% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
% \newtheorem{proposition}[theorem]{Proposition}% 
% %%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

% \theoremstyle{thmstyletwo}%
% \newtheorem{example}{Example}%
% \newtheorem{remark}{Remark}%

% \theoremstyle{thmstylethree}%
% \newtheorem{definition}{Definition}%

% \raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads
%%% END OF MACH FORMAT


% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

% CUSTOM PACKAGES
\usepackage{graphicx}
\usepackage{xcolor}
% \usepackage{algorithm2e}
\usepackage{float}
\usepackage{amssymb}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{hyperref}
% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
% Use the following line for the initial blind version submitted for review:
\usepackage{amsmath}
\usepackage{mathtools}
% \usepackage{amsthm}


%%%%%%%%%%%%%%%%
%%% CUSTOM PACKAGES FOR SVJOUR %%%%
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}






%%%%%%%%%%%%%%%%%%%%%%

% END OF CUSTOM PACKAGES

\newcommand{\av}[1]{\textbf{\textcolor{orange}{{av: #1}}}}
\newcommand{\roc}[1]{\textbf{\textcolor{teal}{{rc: #1}}}}
\newcommand{\kf}[1]{\textbf{\textcolor{red}{{kf: #1}}}}
\newcommand{\dz}[1]{\textbf{\textcolor{blue}{{dz: #1}}}}
\newcommand{\map}[1]{\textbf{\textcolor{green}{{mp: #1}}}}

\newcommand{\greencheck}{{\color{green}\checkmark}}
\newcommand{\orangecheck}{{\color{orange}\checkmark}}
\newcommand{\redx}{{\color{red}x}}

%
% AV: we need some cool shortname for the benchmarks
% these are temporary
% \newcommand{\mnistimageshort}{Mnist2Imagenet-Short}
% \newcommand{\mnistimageshort}{MNI2IMG-Short}
% % \newcommand{\imagemnistshort}{Imagenet2Mnist-Short}
% \newcommand{\imagemnistshort}{IMG2MNI-Short}
% \newcommand{\mnistimagelong}{MNI2IMG-Long}
% \newcommand{\imagemnistlong}{IMG2MNI-Long}

\newcommand{\mnistimageshort}{M2I}
% \newcommand{\imagemnistshort}{Imagenet2Mnist-Short}
\newcommand{\imagemnistshort}{I2M}
\newcommand{\mnistimagelong}{MNI2IMG-Long}
\newcommand{\imagemnistlong}{IMG2MNI-Long}

\newcommand{\mnistsiximgshortbalanced[1]}{MNI6IMG\-ShortBalanced#1}
\newcommand{\imgsixmnistshortbalanced[1]}{IMG6MNI\-ShortBalanced#1}
% \newcommand{\mnistsiximgshortbalanced500}{MNI6IMG\-ShortBalanced500}
% \newcommand{\mnistsiximgshortbalanced5000}{MNI6IMG\-ShortBalanced5000}
% \newcommand{\imgsixmnistshortbalanced500}{IMG6MNI\-ShortBalanced500}
% \newcommand{\imgsixmnistshortbalanced5000}{IMG6MNI\-ShortBalanced5000}


\begin{document}

%\title[From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning]{From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning}

\title{From MNIST to ImageNet and Back: Benchmarking Continual Curriculum Learning}

\titlerunning{M2I and I2M: Benchmarking Continual Curriculum Learning}



% \shorttitle[From MNIST to ImageNet and Back]

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

%%%%%%%%%%%%%
% MACH FORMAT
%%%%%%%%%%%%
% \author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

% \author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
% \equalcont{These authors contributed equally to this work.}

% \author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
% \equalcont{These authors contributed equally to this work.}

% \affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

% \affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

% \affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%%%%%%%%%%%%%%%%%%%%%%%%
% SMACL CONDENSED FORMAT
%%%%%%%%%%%%%%%%%%%%%%%%

\author{Kamil Faber \and
        Dominik Zurek \and
        Marcin Pietron \and
        Nathalie Japkowicz \and
        Antonio Vergari$^{*}$ \and
        Roberto Corizzo$^{*}$
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Kamil Faber, Dominik Zurek, Marcin Pietron \at
              AGH University of Science and Technology, Cracow, 30059, Poland \\
              \{kfaber,dzurek,pietron\}@agh.edu.pl
              %*\email{xxxxxxxx@alu.ufc.br}           %  \\
           \and
           Antonio Vergari \at
              University of Edinburgh, Edinburgh, EH8 9AB, UK \\
              avergari@ed.ac.uk
          \and
              %*\email{kfaber,dzurek,pietron@agh.edu.pl} \and          %  \\
           Nathalie Japkowicz, Roberto Corizzo \at
           American University, Washington, DC, 20016, USA\\
           \{japkowic,rcorizzo\}@american.edu\\
           % **\email{rcorizzo@american.edu} \\
           \\
           * Equal Supervision \\
           Corresponding Author: Roberto Corizzo \email{rcorizzo@american.edu} 
}

\date{Received: date / Accepted: date}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%
\maketitle
\abstract{Continual learning (CL) is one of the most promising trends in recent machine learning research. Its goal is to go beyond classical assumptions in machine learning and develop models and learning strategies that present high robustness in dynamic environments. This goal is realized by designing strategies that simultaneously foster the incorporation of new knowledge while avoiding forgetting past knowledge. The landscape of CL research is fragmented into several learning evaluation protocols, comprising different learning tasks, datasets, and evaluation metrics. Additionally, the benchmarks adopted so far are still distant from the complexity of real-world scenarios, and are usually tailored to highlight capabilities specific to certain strategies. In such a landscape, it is hard to clearly and objectively assess models and strategies. In this work, we fill this gap for CL on image data by introducing two novel CL benchmarks that involve multiple heterogeneous tasks from six image datasets, with varying levels of complexity and quality. Our aim is to fairly evaluate current state-of-the-art CL strategies on a common ground that is closer to complex real-world scenarios. We additionally structure our benchmarks so that  tasks are presented in increasing and decreasing order of complexity -- according to a curriculum -- in order to evaluate if current CL models are able to exploit structure across tasks. We devote particular emphasis to providing the CL community with a rigorous and reproducible  evaluation protocol for measuring the ability of a  model to generalize and not to forget while learning.
Furthermore, we provide an extensive experimental evaluation showing that popular CL strategies, when challenged with our proposed benchmarks, yield sub-par performance, high levels of forgetting, and present a limited ability to effectively leverage curriculum task ordering. 
We believe that these results highlight the need for rigorous comparisons in future CL works as well as pave the way to design new CL strategies that are able to deal with more complex scenarios.
}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{continual learning, lifelong learning, curriculum learning, neural networks, computer vision, image classification}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\section{Introduction}
\label{sec:intro}

% what is lifelong leaning; why its important
% Lifelong learning is an emerging machine learning paradigm that aims at designing new methods to provide accurate analyses in complex and dynamic real-world environments in which the models are challenged with multiple tasks over their lifetime. 
Continual Learning (CL), also known as Lifelong Learning, is a promising learning paradigm to design models that have to learn how to perform \textit{multiple tasks} across different environments over their lifetime \cite{parisi2019continual} \footnote{To uniform the language and enhance the readability of the paper we adopt the unique term continual learning (CL).}.
%
% Methods are focused on simultaneous model adaptation and knowledge retention, which allows to  maintain a high performance across all tasks.
Ideal CL models in the real world should be able to quickly adapt to new environments and tasks, while perfectly retaining what they learned in the past, thus only increasing, and not decreasing, their performance as they experience more tasks.
%
In practice, this is quite challenging due to the hardness of generalizing from one environment to another when there is a huge \textit{distribution shift} between them \cite{lopezpaz2017,krawczyk2021tensor,li2017learning,cano2022rose}, and to the fact that models tend to (sometimes catastrophically) \textit{forget} what they learned for previous tasks.


% current benchmarks and what they lack
% This exciting paradigm is attracting increasing attention from research communities, which led to a number of works being proposed, particularly in the context of computer vision. 
The great attention around this paradigm has brought many communities to focus on how to address these challenges, including reinforcement learning \cite{baker2023domain} \cite{abel2018policy} and anomaly detection \cite{faber2022active} \cite{corizzo2022cpdga}.
%
However, the majority of attention has been devoted to computer vision, generating a Cambrian explosion of CL models \cite{lopezpaz2017,li2017learning,aljundi2018memory,kang2022,chaudhry2019,zenke2017,rolnick2019experience,hihn2022hierarchically}, where the most common task is to learn models that can classify different kinds of images while preventing catastrophic forgetting or quickly adapting to new image classes or image datasets.
%
Every new model has been evaluated in a slightly different setting -- \textit{using a different dataset, evaluation metrics and learning protocols} -- thus generating a number of CL learning and evaluation schemes. 
% also called \textit{scenario type}, e.g., task-incremental \av{REF}, class-incremental \av{REF} and domain-incremental \av{REF} settings. 
%
The result is that \textit{the benchmark panorama of CL in computer vision is quite fragmented}, and therefore it has become tougher to measure catastrophic forgetting and domain adaptation in a fair and homogeneous way for the many CL models we have in the literature these days. 
%
Furthermore, all previous evaluation protocols are designed to highlight some specific model characteristics and, as such, are generally over-simplified w.r.t. real-world data \cite{cossu2022class}.



% 1 dataset benchmarks
% Many of the recent works in the literature create a characterization of tasks by separating the overall number of classes into different subsets \cite{van2019three, DeLange2022ACL}. The most prominent example is splitMNIST in which the 10 digits from MNIST dataset are usually divided into 5 tasks consisting of 2 digits each. Another widely adopted examples following similar approach are CIFAR10 \cite{Krizhevsky2009LearningML} and TinyImagenet \cite{le2015tiny}. There are also some datasets such as Continuous Object Recognition (CORe50) \cite{pmlr-v78-lomonaco17a} that are specifically designed for lifelong learning. 
For example, one of the most popular evaluation protocol for CL models in computer vision is to design different tasks to classify different (subsets of the) classes of a single dataset \cite{DeLange2022ACL,van2019three}. 
%
The most prominent example is splitMNIST in which the 10 digits from MNIST \cite{lecun1998mnist} are (usually) divided into 5 tasks consisting of 2 digits each. 
Similar approaches are proposed for CIFAR10 \cite{Krizhevsky2009LearningML}, and TinyImagenet \cite{le2015tiny}.
%
Other datasets, such as Continuous Object Recognition (CORe50) \cite{pmlr-v78-lomonaco17a}, specifically designed for LL, still make the same assumptions to generate tasks.
%
Clearly, these protocols are not suited to detect distribution shifts, due to the high inter-task similarity. 
Consequently, catastrophic forgetting is much easier to prevent in these cases. 
Therefore the reported metrics for models evaluated in this way can be overly optimistic. 


% There are also some datasets such as Continuous Object Recognition (CORe50) \cite{pmlr-v78-lomonaco17a} that are specifically designed for lifelong learning. 
% %...
% The major disadvantage of diving a single dataset into a few tasks is the high inter-task similarity, which limits model exposure to a simplistic view of the possible tasks compared to the variety of the possible real-world scenarios. For this reason, even if the experimental results reported in these studies are useful to assess lifelong methods' performance, the numeric scores may be overly optimistic.

% multiple datasets benchmarks
% Fortunately, researchers have recently started adopting more challenging settings, where tasks are sampled from different datasets. For instance, \cite{lopezpaz2017} proposed a benchmark in which the model is trained and evaluated on Imagenet first, and then challenged with the Places365 dataset. The \cite{li2017learning} follows a similar approach, but considering more scenarios, starting with Imagenet or Places365, and then moving on to the VOC/CUB/Scenes datasets. Despite being a step forward towards more challenging scenarios for the methods, it is still not enough to reproduce real-world conditions with many diverse varying tasks. 
To deal with domain shifts, researchers have recently started to sample tasks from two different datasets. 
For instance, \cite{lopezpaz2017} proposed to train and evaluate a model on Imagenet first and then challenge its performance on the Places365 dataset. 
\cite{li2017learning} considers more scenarios, starting with Imagenet or Places365, and then moving on to the VOC/CUB/Scenes datasets. 
%Despite being a step forward towards more challenging scenarios for the methods, it is still not enough to reproduce real-world conditions with many diverse varying tasks. 
% more than two
%
Few works propose more advanced scenarios built on top of more than two datasets. The two most prominent examples are the so-called 5-datasets \cite{ebrahimi2020adversarial} and RecogSeq \cite{aljundi2018}, which provide models with more challenging scenarios than previous attempts, increasing the number of considered datasets to 5 and 8, respectively. Unfortunately, those datasets provide a similar task complexity due to the limited differences across datasets.
Furthermore, when different datasets are employed, it is important to ``calibrate the meaning'' of the employed metrics, taking into account the number of classes involved in each task.
%As we will show in Section \av{REF} transfer metrics can be misleading if computed on tasks having different number of classes.

%The former brings a few advantages, including the consideration of black and white datasets in addition to RGB. Unfortunately, those datasets provides a similar level of challenge to the methods. RecogSeq is a sequence of 8 datasets organized in 8 tasks. It brings a lot of new challenges. 

% \begin{table}[]
% \small
%     \setlength{\tabcolsep}{5pt}
%     \centering
%     \begin{tabular}{lcccccccc}
%     \toprule
%     Benchmark        &  i) & ii) & iii) & CI & TI & M\\
%     \midrule
%     Imagenet/Places365 to \\ VOC/CUB/Scenes \\\cite{li2017learning}     & \orangecheck & \redx &  \redx  & \redx  & \greencheck & \orangecheck\\
%     Imagenet to Places365 \\ \cite{mallya2017} & \orangecheck & \redx & \redx & \redx  & \greencheck& \orangecheck\\
%     5-Datasets       \\ \cite{ebrahimi2020adversarial}  & \greencheck & \orangecheck  & \redx &   \greencheck & \redx& \orangecheck\\ 
%     RecogSeq \cite{aljundi2018}       \\ \cite{DeLange2022ACL} & \greencheck & \orangecheck & \redx & \greencheck & \greencheck& \orangecheck\\
%     \midrule
%     M2I, I2M \textit{(ours)}              & \greencheck  & \greencheck  & \greencheck  & \greencheck & \greencheck & \greencheck \\
%     \bottomrule
%     % \midrule
%     \end{tabular}
%     \caption{Benchmarks comparison; we included only multi-dataset benchmarks. Columns refer to supporting multiple heterogeneous tasks; ii) varying task complexity and quality and iii) evaluating curriculum strategies and classi- (CI) and task-incremental (TI) scenarios. M refers to the full metrics set used (see \ref{sec:metric_and_evaluation_protocol}).
%     The symbols have the following meaning: \greencheck - criterion is covered; \orangecheck - criterion is covered at some part or with some limitations; \redx - criterion is not covered at all.}
%     \label{tab:benchmarks_comparison_2}
% \end{table}





% MOTIVATION 
% Despite the significant progress done in lifelong learning in a limited time span, the current state-of-the-art is quite far from the desired goals of the field. 
% One of the main expectations of lifelong learning is to bring models closer to real life, significantly improving their generalization capabilities to deal with an evolving environment. To reach this level, it is important to incorporate close-to-real-world complexities in the learning and evaluation scenarios.   
Despite all this progress, we argue that there is still not a robust and standardized evaluation benchmark for the many CL models in the literature.
%
We argue that a modern benchmark for CL should provide the following aspects. 
%
First, \textbf{\textit{multiple heterogeneous tasks}} that do not restrict to a single set of concepts, e.g., digits in MNIST or SVHN  or naturalistic images as in Imagenet or CIFAR10.
%
Second, \textbf{\textit{a varying quality and complexity of the tasks}}, e.g. alternating from black and white (B\&W) to RGB images and vice-versa, considering different image sizes, and a number of concepts.
%
Third, a way to systematically evaluate if \textbf{\textit{learning on a curriculum}} of task complexities help with domain generalization and catastrophic forgetting.
For example, evaluating if a model trained on B\&W digits can better generalize to B\&W letters and then to RGB digits and letters, or if learning them in the inverse order is more beneficial.
%
Fourth, \textbf{\textit{a rigorous way to measure generalization and forgetting}} in terms of modern backward and forward transfer metrics \cite{diaz2018don} in a number of different evaluation scenarios, i.e., when classes or tasks are introduced incrementally \cite{van2019three}.  
%
Lastly, all results should be \textit{\textbf{exactly reproducible out-of-the-box}}. 
We argue that all the previous CL works discussed above do not consider one or more of these criteria, as highlighted in Table \ref{tab:benchmarks_comparison_2}. 
In addition to the five desiderata, we also cover both class and task-incremental learning settings, which is not usually the case for other surveyed works.
%
In this paper, we aim to overcome these limitations.


\begin{table}[t]
% \small
    % \setlength{\tabcolsep}{5pt}
    \centering
      \caption{Benchmarks comparison considering only multi-dataset benchmarks. Columns refer to: i) supporting multiple heterogeneous tasks; ii) varying task complexity and quality; iii) evaluating curriculum strategies; iv) rigorous way to measure generalization and forgetting, and v) exactly reproducible out-of-the-box. In addition, we consider the coverage of class (CI) and task-incremental (TI) learning settings. 
    The symbols have the following meaning: \greencheck - criterion is covered; \orangecheck - criterion is covered at some part or with some limitations; \redx - criterion is not covered at all.}
    \begin{tabular}{lccccccc}
    \toprule
    Benchmark        &  i) & ii) & iii) & iv) & v) & CI) & TI) \\
    \midrule
    Imagenet/Places365 to  VOC/CUB/Scenes \cite{li2017learning}     & \orangecheck & \redx &  \redx  & \orangecheck & \orangecheck & \redx & \greencheck  \\
    Imagenet to Places365  \cite{mallya2017} & \orangecheck & \redx & \redx & \orangecheck & \greencheck  & \redx & \greencheck \\
    5-Datasets        \cite{ebrahimi2020adversarial}  & \greencheck & \orangecheck  & \redx & \greencheck &  \greencheck  & \greencheck & \redx \\ 
    RecogSeq \cite{aljundi2018}      \cite{DeLange2022ACL} & \greencheck & \orangecheck & \redx & \orangecheck & \greencheck & \greencheck & \greencheck \\
    \midrule
    M2I, I2M \textit{(ours)}              & \greencheck  & \greencheck  & \greencheck  & \greencheck & \greencheck & \greencheck & \greencheck \\
    \bottomrule
    % \midrule
    \end{tabular}
  
    \label{tab:benchmarks_comparison_2}
\end{table}
%
% In this paper, we propose \textit{a set of benchmarks} built on a curriculum of 6 datasets -- \textit{from MNIST to TinyImageNet} (M2I) and back (I2M) -- that simultaneously satisfies all the above desiderata. 
%
%
%In this perspective, models should be ready to deal not only with simple lifelong learning task change, but they must be ready to deal with real-world dynamic. 
% For example, a lifelong image classification model exposed to one type of problem (e.g. recognizing digits) should be not only ready to deal with new variants of the same type of problem (e.g. new digits), but should be capable of transitioning from simple tasks, such as handwritten digits recognition, to much more complex ones, as scene classification or high-resolution animal images. Moreover, it would be also unrealistic to assume that technical aspects will not change over the lifespan of the model, which we aim to be as long as possible (e.g. a camera may be changed to another with a better resolution, or noise could arise due to device issues, and the model should be able to consider it without manual intervention).
%
%
% % WHAT WE WANT FROM BENCHMARK
% Following this analysis, we argue that there is a need for a more challenging benchmark that simultaneously considers the following criteria: 
% \textit{i)} multiple heterogeneous tasks that do not restrict to a single homogeneous dataset, e.g. from Imagenet to Places365, to CUB200, etc.; 
% % spanning across heterogeneous datasets
% \textit{ii)} varying task complexity, considering semantically different tasks (e.g. from digits to letters, to birds, etc.) 
% \textit{iii)} varying technical quality of tasks (e.g. from black and white to RGB images, to different image sizes, etc.)

%
Specifically, the contributions of the paper are as follows:
\begin{itemize}
    \item We 
    propose \textit{a set of benchmarks} built on 6 image datasets ordered in a curriculum of complexity -- \textit{from MNIST to TinyImageNet} (M2I) and back \textit{from TinyImageNet to MNIST} (I2M) -- that simultaneously satisfies all the above desiderata. 
    These benchmarks have varying task complexity, starting with simple digits and going to complex naturalistic images and viceversa (see Figure \ref{fig:workflow});
    % \item The benchmark is organized in a curriculum learning fashion, in which task in devised scenarios are ordered in increasing order of complexity (from simple to complex), and vice-versa, which is an important characteristic in many real-world scenarios.
    %, which deserves proper attention in lifelong learning research. 
    %the consideration of a curriculum learning  \ref{iconic-paper} approach in the evaluation of lifelong learning methods would be ideal. It can be a great simulation of some realistic real-world conditions in which the model is challenged with increasing difficulty of tasks. For this reason, we devise  
    \item  We provide an exhaustive experimental evaluation including 9 state-of-the-art continual learning methods, covering the key categories of approach (architectural, regularization, and rehearsal) in both class and task-incremental settings, and evaluating results using the most recent metrics adopted in the continual and lifelong learning community. 
\end{itemize}


\begin{figure*}[h]
\centering
\includegraphics[width=1.0\linewidth]{figures/benchmark-variant-new.pdf}
\caption{The Proposed M2I and I2M continual learning benchmarks. There are 6 different tasks, each sampled from a different dataset: MNIST \cite{lecun1998mnist}, OMNIGLOT \cite{lake2015human}, Fashion MNIST \cite{xiao2017fashion}, SVHN \cite{Netzer2011ReadingDI}, CIFAR10 \cite{Krizhevsky2009LearningML} and TinyImageNet \cite{le2015tiny}. Tasks are organized in two curriculum ordering, from simple to harder (left to right) and backward (right to left). Every task sports 10 classes, as to make the performance metric meaning intuitive and faithful.}
\label{fig:workflow}
\end{figure*}


\section{Background}
\label{sec:background}


\subsection{CL scenario types}
\label{sec:scenario_types}
% There are multiple attempts to define scenario types that could be leveraged for effective and meaningful lifelong learning research while reflecting the real-life challenges. As a result, a wide range of scenarios was designed and discussed in recent studies. However, in image classification domain two main scenarios seem to be the most discussed and widely adopted: i) task-incremental and ii) class-incremental. The scenarios and the distinction between them can be best explained and categorized by focusing on three characteristics: availability of task labels, task boundaries, and a type of novelty in the scenario. 
%
A wide range of scenarios was designed and discussed in recent studies to design effective CL models while trying to reflect real-world challenges. 
In image classification, two main scenarios are the most widely adopted: i) \textbf{\textit{task-incremental}} \cite{de2021continual} and ii) \textbf{\textit{class-incremental}} \cite{belouadah2021comprehensive} LL. 
%
% learning new tasks - classes
% tasks are presented one by one, model knows that there is a new task
% the difference - task labels
In both scenarios, the model has to learn new tasks, which are presented sequentially.
%The model is aware when a new task is presented (task boundaries), 
Each incoming task provides the model with new, previously unseen classes, that need to be incorporated.

% TASK BOUNDARIES
A common characteristic for both mentioned scenarios is the availability of task boundaries, which make the CL method aware that a new task is presented. 
% TASK LABELS
The most relevant difference between the two scenarios is the availability of task labels, which provide the model with additional information on which task is being processed at the moment, during both training and inference. 
Specifically, a task-incremental scenario assumes the availability of task labels, whereas in class incremental learning, this information is not available.
% WHY IS IT IMPORTANT TO USE BOTH IN THE EXPERIMENTS

\textbf{\textit{It is worth stressing that the same data presented in different types of scenarios can yield significantly different results}}, since certain CL methods may be tailored for task-incremental scenarios, and as such the exploitation of task labels improve their performance, while they may significantly suffer in class-incremental scenarios, where this information is not available.

% EXAMPLE
The most widely adopted benchmark for class and task-incremental scenarios is split-MNIST \cite{kirk2017} consisting of 5 tasks. It leverages the original MNIST separating it into five tasks, each containing two digits. In this class-incremental scenario, the model is not aware of whether what the current task is. It is only aware of the fact that it encountered a new task and needs to adjust itself. During the testing phase, the model is also not informed about which set of digits is currently provided, so the model has to classify one of the ten classes (digits 0-9). On the other hand, in the task-incremental variant of split-MNIST, the model is aware of which task is currently being presented, and only decides whether the image belongs to the first or the second class of the current task. This prediction, combined with information about the current task id, leads to the specific digit prediction.

% TASK AGNOSTIC OR TASK FREE 
Less commonly, certain scenarios relax the assumptions of class and task-incremental scenarios \cite{lomonaco2019nicv2}. 
%\av{as a general comment, you should put citations for every statement you make that is not supported by the paper itself}. 
Notable examples include domain-incremental scenarios \cite{baker2023domain} where new distributions of the same classes are presented over time, as well as task-agnostic scenarios, where neither task labels nor task boundaries are not available, and reliance on external methods is necessary to detect task changes \cite{faber2022lifewatch}. 
%Another example is that of task-agnostic scenarios, which require 
%include domain incremental, task  this assumption This availability represents a simplification of the learning setting, since the mo  
%The task boundaries allow the strategy to focus on learning a new task and lift the responsibility for detecting a transition between tasks. 

%Furthermore, we adopt the commonly used assumption that each task is seen only once during training, for efficiency purposes \av{REFs}.

%three characteristics: a) availability of task labels, b) what is considered to be novel in the scenario, and c) task boundaries. 

% Task labels describe which task is being processed at the moment, during both the training and inference phases. Learning strategies can leverage this information to control and optimize the learning process. 
% Considering the novelty type, we can distinguish two main categories: new classes and new instances. New classes case means that each incoming task provides the model with new, previously unseen classes that need to be recognized and classified. On the other hand, new instances case challenges the strategy to keep improving and adjusting the model by providing new data from the existing set of classes. There is also a possibility of a scenario in which new tasks bring new classes and instances. However, this scenario is not among the most often considered ones. 
% Another characteristic common for all three mentioned scenarios is the availability of task boundaries. They allow the learning strategy to be aware that the task changed and needs to learn new data. The task boundaries allow the strategy to focus on learning a new task and lift the responsibility for detecting a transition between tasks. It is noteworthy that there are research trends to work with scenarios that do not have clear task boundaries and are often called task-agnostic or task-free.

% Learning strategies can leverage this information to control and optimize the learning process. 
% Considering the novelty type, we can distinguish two main categories: new classes and new instances. 
% %
% %
% On the other hand, the new instances case challenges the strategy to keep improving and adjusting the model by providing new data from the existing set of classes. 
% There is also a possibility of a scenario in which new tasks bring new classes and instances. However, this scenario is not among the most often considered ones. 

% \av{UP TO HERE it is not clear what is the distinction between class and task incremental, and why it is important to measure both}



% As an addition for those two most explored scenarios, there is also a domain-incremental scenario that includes task boundaries, but instead of providing new classes the novelty focuses on new instances. An example of such benchmarks is RotatedMNIST, which provides the same classes (all digits), but rotated in order to simulate change in a data distribution. 
% It is noteworthy that all previously mentioned scenarios follow the evaluation protocol described in \ref{sec:metric_and_evaluation_protocol} that assumes each task is seen only once for training purposes. However, recent research works such as \cite{cossu2022class} pointed out usefulness of exploring scenarios that includes the recurrence of the same task in the training stream.


% \ref{sec:metric_and_evaluation_protocol} that assumes each task is seen only once for training purposes. However, recent research works such as \cite{cossu2022class} pointed out usefulness of exploring scenarios that includes the recurrence of the same task in the training stream.

% \kf{It probably does not make sense to keep the table with just two rows - maybe some picture instead}
% \begin{table}[h]
%     \centering
%     \begin{tabular}{l|ccc}
%     Scenario     &  Task labels    &  Type of novelty       & Task boundaries\\
%     \midrule
%     Task-incremental     & Yes     &   New classes        & Yes\\ 
%     Class-incremental    & No      &   New classes        & Yes\\ 
%     % Domain-incremental   & No      &   New instances      & Yes \\ 
%     \end{tabular}
%     \caption{Most common lifelong learning scenarios}
%     \label{tab:lifelong_scenarios}
% \end{table}



\subsection{CL Strategies}
%\section{State-of-the-art of continual learning}
\label{sec:sota}
% \av{Brief intro to the sota methods ***we are using*** and why they are sota}

From a broad perspective, CL strategies belong to three main groups: using \textit{\textbf{regularization}}, \textit{\textbf{dynamic architectures}}, and \textit{\textbf{rehearsal}} (also known as experience replay). 
In this paper, we consider popular and largely adopted CL strategies. We now describe each strategy and the rationale for its adoption in the benchmarks.

Regularization strategies influence the model weights adjustment process that takes place during model training in the attempt to preserve knowledge of previously learned tasks. %adjusting weights in an existing model architecture as new tasks are presented.
The regularization strategies considered include Elastic Weight Consolidation (EWC) \cite{kirk2017}, Learning without Forgetting (LwF), Synaptic Intelligence (SI) \cite{zenke2017}, and Memory Aware Synapses (MAS) \cite{aljundi2018memory}.
LwF \cite{li2017learning} aims at achieving  output stability through knowledge distillation. When a new task is observed, the new model is incentivized to predict values that are close to the outputs of the model learned prior to this task.
EWC \cite{kirk2017} and SI \cite{zenke2017} adopt a weighted quadratic regularization loss which penalizes moving weights that are important for previous tasks. The EWC loss is based on the Fisher Information Matrix which presents a higher computational complexity than the surrogate loss used in the SI method. 
%(based on simple difference between current and previous values of weights). %Similarly, SI \dots \cite{zenke2017} is based on a weighted regularization loss.
Similarly, Memory Aware Synapses (MAS) \cite{aljundi2018memory} estimates the cumulative importance of model weights as new tasks are encountered, penalizing changes to weights that are crucial for previously learned tasks.  
% We cannot divide in paragraphs or dynamic architectures would be a single sentence. 
Shifting the focus on dynamic architectures, CWRStar \cite{lomonaco2019nicv2} adapts weights exclusively for the last layer before the prediction layer, freezing all previous layers. 
Finally, rehearsal strategies considered include GDumb \cite{prabhu2020}, Replay \cite{rolnick2019experience}, Gradient Episodic Memory (GEM) \cite{lopezpaz2017}, and Average Gradient Episodic Memory (AGEM) \cite{chaudhry2019}.
GDumb \cite{prabhu2020} is a greedy strategy that stores samples for all classes in a buffer, and uses them to iteratively retrain a model from scratch.
Replay \cite{rolnick2019experience} follows a similar approach, but stores a balanced number of samples per task, which are used to fine-tune previously trained models.
A more sophisticated approach is pseudo-rehearsal with generative models. GEM \cite{lopezpaz2017} is a fixed-size memory that stores a subset of old patterns and influences the loss function through inequality constraints. 
AGEM \cite{chaudhry2019} is a revised version of GEM that performs averaging to increase efficiency.
% Cumulative \cite{avalanche} is a variant of replay with unlimited memory, which is unrealistic in practice but provides interesting insights with respect to upper-bound mode capabilities. 

The rationale for the adoption of the  aforementioned strategies in our benchmark is that they are heterogeneous in terms of approaches, and are particularly prevalent in the CL community. They represent the foundations in the CL field, and are often used to assess the competitiveness of emerging CL methods with respect to consolidated and diversified approaches. Moreover, they are easy to use and favor reproducibility, thanks to publicly available tools such as the Avalanche library \cite{lomonaco2021avalanche}.
 
%% Cumulative, MAS, Naive, Replay, 

% \kf{TODO @Marcin: write about general SOTA (like 3 types of methods: regularization, dynamic architectures, experience replay, current works, etc). Shortly describe the methods we use, including: ['AGEM' yes, 'CoPE', 'Cumulative', 'CWRStar' yes, 'EWC' yes, 'GEM' yes, 'LFL', 'LwF' yes, 'Replay' yes, 'SI' yes] from Avalanche. Dominik can have a more complete list}


% \map{in progress text}

% In continual learning there are three main groups of methods: architectural strategies, regularization strategies and rehearsal strategies. 
% \textcolor{blue}{to add ICARL, cumulative, LFL, MAS, GDumb}

% \av{this section needs a complete rewrite}

% \subsubsection{Architectural strategies} 

% The architectural methods concentrate on topology of neural model. The architecture is dynamic during the sequence of episodes. Many of these methods seem to be forgetting resistant. 
% %specific architectures, layers, activation functions, and/or weight freezing strategies are used to mitigate forgetting.
% Progressive neural networks \cite{rusu2016} are one of the first approaches in this group. The methodology is based on a clever combination of parameter freezing and network expansion. While PNN was shown to be effective on short series of simple tasks, the number of the model parameters keeps increasing at least linearly with the number of tasks, making it difficult to use for long sequences. Its main drawback is that is not effective for many tasks. One of the most popular architecture base approach are: CWR \cite{pmlr-v78-lomonaco17a} and CWRStar \cite{lomonaco2019nicv2} which do a copy weight with re-init in a last layers, a simpler and lighter counterpart to PNN (at the cost of a lower flexibility), with a fixed number of shared parameters and already proven to be useful on longer sequences of tasks. More fancier and efficient methods are those based on pruning. PackNet \cite{mallya2017}, SupSup \cite{wortsman2020} and WSN \cite{kang2022} are most well known representatives of this class of approaches. They are completely resistant for forgetting. One of the latest solution is Active Dendrite Neuron approach \cite{} - (model/neuron modification) new model of neuron (brain inspired neuron) based on brain neocortex. The dendrite connections strengths the context of previous tasks. It includes dual-memory models attempting to imitate hippocampus-cortex duality. Promising results were already achieved on small datasets (like permuted mnist) and reinforcement learning tasks. Method can be used (merged or mixed) also with other CL techniques.

% \subsubsection{Regularization strategies}
% This group of methods is using regularization enhancements to avoid catastrophic forgetting. The standard loss function is extended with additional loss terms promoting selective consolidation of the weights which are important to retain past memories. Include basic regularization techniques such as weight sparsification, dropout or early stopping. LwF \cite{li2017learning} method called learning without forgetting is a regularization strategy attempting to preserve the model accuracy on old tasks by imposing output stability through knowledge distillation. XdG \cite{masse2018} is context dependent gating adding a context-dependent gating signal, such that only sparse, mostly nonoverlapping patterns of units are active for any one task. EWC \cite{kirk2017} is elastic weight consolidation, weighted quadratic regularization loss which penalizes moving weights which are important for old tasks (it computes fisher ...), it has extended version called EWC++. Synaptic intelligence (SI) \cite{zenke2017} is based on weighted regularization loss which penalizes moving weights which are important for old tasks. These group of methods achieve in general worse results than architectural and rehearsal strategies because they do not use additional model or memory capacity. 

% \subsubsection{Rehearsal strategies}
% This group of methods is mainly based on remembering some data from past episodes. Past information is periodically replayed to the model to strengthen connections for memories it has already learned. A simple approach is storing part of the previous training data and interleaving them with new patterns for future training (Replay, \cite{}). A more challenging approach is pseudo-rehearsal with generative models. Gradient Episodic Memory (GEM) \cite{lopezpaz2017} is a fixed memory to store a subset of old patterns and add regularization constraints to the loss optimization, aimed not only at controlling forgetting but also at improving accuracy on previous tasks while learning the subsequent ones (positive backward transfer). A-GEM \cite{chaudhry2019} is average episodic memory with small modifications to previous method. GDumb \cite{prabhu2020} greedily stores samples in memory as they come and and at test time, trains a model from scratch using samples only in the memory. Incremental Classifier and Representation Learning (ICARL) \cite{sylvestre2016} learns strong classifiers and a data representation simultaneously. This distinguishes it from many other works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures (method both rehearsal and regularization). Gradient-Based Sample Selection (GSS) is a replay method that diversifies the gradients of the samples in the replay memory. Maximally Interfered Retrieval (MIR) is a replay method that retrieves memory sample with loss increases given the estimated parameter update based on the current batch. Generative replay (GR) (e.g. BI-R brain inspired generative replay) \cite{deven2020} is another alternative to storing data, here authors focus on generating the data to be replayed with a learned generative neural network model of past observations. The method is strongly inspired by human brain. Most challenging in this group is to reduce the memory for replay (which decrease memory size and training time).

% There are some methods which are hybrid. It means that they combine techniques from two or all three groups. The main drawback of the presented approaches is lack of consistent benchmarks that can be used across all of these methods. Very often it is difficult to compare the because they are run on different scenarios. Second thing is there is lack of complex benchmark which can make continual learning task 'closer to' real world environment. Most often combination of MNIST and CIFAR datasets are taken as baseline scenarios. 


\subsection{CL evaluation protocol and metrics}
\label{sec:metric_and_evaluation_protocol}
The standard evaluation procedure applied in continual image classification assumes the availability of a set of tasks, each defined with a set of classes.  The learning scenario consists of $N$ tasks $T = t_1, t_2, \dots, t_n$ where the model has to learn new tasks without forgetting previous tasks. 


% A pseudo-code of the evaluation protocol is presented in Algorithm \ref{alg:eval-protocol}. 
% \av{This Algorithm feels useless. Is it something people do not know? it seems to be the usual basic stuff}

% \begin{algorithm}
% \begin{algorithmic}[1]
% \Require{$\mathbf{T}$ -- a sequence of $N$ tasks, each one containing a training set $T_{L_i}$ and evaluation (testing) set $T_{E_i}$ } 
% \Require{$L$ -- a learning strategy} 

% \State Initialize results matrix $ACC_{N \times N}$
% \For{$T_i \in \mathbf{T}$}
%     \State Train $L$ on $T_{L_i}$
    
%     \For{$T_j \in \mathbf{T}$}
%         \State Compute accuracy for the strategy $L$ given data $T_{E_i}$ \State Save results to $ACC_{i, j}$ 
%     \EndFor
% \EndFor

% \State \Return {$ACC$}
% \end{algorithmic}
% \caption{CL Evaluation Protocol \av{REMOVE}}
% \label{alg:eval-protocol}
% \end{algorithm}


Metrics in continual learning usually focus on assessing the performance of a model (e.g., its accuracy) with respect to (at least one of) three crucial properties:
\textit{i)} performance on newly encountered tasks; \textit{ii)} performance retention capabilities on previously learned tasks (i.e., the \textit{ability to avoid or mitigate forgetting}); and \textit{iii)} knowledge transfer from learned tasks to new ones (i.e., \textit{the ability to generalize over newly occurring challenges}).
%
The first works in CL  proposed three metrics: average accuracy, backward transfer, and forward transfer to measure the above desiderata~\cite{lopezpaz2017}. However, in their original definition, only the performance of the model after learning all tasks was considered.

%
Instead, we consider model performances for all tasks and at all stages of the learning process, as understanding how performance changes before and after every task can provide several insights into the strength and weaknesses of every model \cite{diaz2018don}.
%
% We now discuss our set of metrics in detail.
%
For simplicity, we will be storing the 
partial model performance, measured as classification accuracy, in a matrix $\mathsf{R}$ whose entries $\mathsf{R}_{i, j}$ represent the accuracy on a given task $j$ after learning task $i$. 

%
%
% \av{This paragraph is detached from the above, why do we care? introduce it differently}
% The authors in \cite{lopezpaz2017} proposed three metrics: Average Accuracy, Backward Transfer, and Forward Transfer. However, their definitions were focused on the performance of the final model, after learning all tasks. The authors of \cite{diaz2018don} pointed out that it may be beneficial to assess models, also based on their performance during the whole scenario, and further extended their metrics to consider this aspects. 
% %
% They leverage the matrix $ACC$ defined in the evaluation protocol (see Algorithm \ref{alg:eval-protocol}, whose entries $ACC_{i, j}$ represent the Accuracy on a given task $j$ after learning task $i$. Metrics are defined as follows:

\textbf{Average Accuracy ($\mathsf{ACC}$)} -- It measures the average accuracy of the model after learning each task, evaluating only  the current and all previously learned tasks: 
    \begin{equation}
        % ACC = \frac{\sum_{i \ge j}^N ACC_{{i,j}}}{\frac{N(N-1)}{2}}
        \mathsf{ACC} = \sum\nolimits_{i \ge j}^N \mathsf{R}_{{i,j}}/ (N(N-1)/2),
    \end{equation}
defined as the average performance over all tasks the model has seen so far.

\vspace{3pt}

\textbf{Backward Transfer ($\mathsf{BWT}$)} -- It measures the impact of learning new tasks on the performance of all previously learned tasks. Negative backward transfer indicates that learning a new task is harmful to the performance of previously learned tasks (this issue is known as forgetting):
%There is also a commonly used term catastrophic forgetting that describes a strongly negative backward transfer which usually means that the model after learning a new task almost completely forgets how to handle all previous tasks.
    \begin{equation}
        \mathsf{BWT} = \sum\nolimits_{i=2}^N\sum\nolimits_{j=1}^{i-1} (\mathsf{R}_{i, j} - \mathsf{R}_{j,j})/ (N(N-1)/2),
        \label{eq:bwt_all}
    \end{equation}
defined as the average amount of forgetting presented by the model on the overall scenario.

\vspace{3pt}

\textbf{Forward Transfer ($\mathsf{FWT}$)} -- It measures the impact of learned tasks on the performance of tasks learned in the future: 
    \begin{equation}
        \mathsf{FWT} = \sum\nolimits_{i<j}^{N} \mathsf{R}_{i, j}/(N(N-1)/2),
        \label{eq:fwt_all}
    \end{equation}
defined as the average model performance on yet unseen tasks.

\vspace{2pt}


% \av{There are missing metrics, we list later  LACC and FACC}    

% \begin{table}[h]
%     \caption{The panorama of lifelong benchmarks. \av{T-C} is for Task or Class incremental}
%     \label{tab:benchmarks}
%     \centering
%     \begin{tabular}{lllrrrr}
%     \toprule
%     Works & Type & Dataset Used     &  \# Classes & \# Tasks    &  results & \\
%     \midrule
%     \cite{kirkpatrick2017overcoming,zenke2017} & \av{T-C} &
%     \av{split?}MNIST     & 10     &  & \\ 
%     & & CIFAR10    & 10     &   &\\ 
%     \cite{zenke2017}  &&CIFAR100  & 100     &  &\\ 
%     & & CORe50         & 50     &   &\\ 
%     & & TinyImagenet   & 10     &   &\\ 
%     & & Stream-51     & 10     &   &\\ 
%     & & Cub-200     & 10     &   &\\ 
%     & & Omniglot     & 10     &   &\\ 
%     \cite{li2017learning} & & ImageNet/Places365 to VOC/CUB/Scenes & ?   &\\
%     \cite{lopezpaz2017} & & Birds/Scenes to Scenes/Birds & ? &  &\\
%     \cite{kang2022} & & CIFAR10/MNIST/SVHN/FashionMNIST/notMNIST & 10 & 5 & 93.4\\
%     \cite{kang2022} & & Permuted Mnist & 10 & 10 & 96.4\\
%     \cite{kang2022} & & Omniglot Rotation & 100 & 12 & 87.28\\
%     \cite{kang2022} & & CIFAR100 & 10 & 10 & 76.38\\
%     \cite{kang2022} & & CIFAR100 Superclass & 20 & 20 & 61.79\\
%     \cite{kang2022} & & TinyImagenet & 40 & 5 & 71.96\\
%     \cite{mallya2017} & & Imagenet to Place365 &  & 2 & $\sim$ 64\\
%     \cite{mallya2017} & & Imagenet/CUBS/Stanford Cars/Flowers &  & 4 & $\sim$ 80\\
%     A-GEM & & MNIST/CIFAR/CUBS/TinyImagenet &  &  &\\
%     LwF & & Imagenet to CUB & & &  \\
%     LwF & & Imagenet to Scenes & & &  \\
%     LwF & & Places365 to VOC & & &  \\
%     EWC/HAT & & CIFAR10/MNIST/SVHN/FashionMNIST/notMNIST & 10 & 5 & 88.64/91.32\\
%     EWC/HAT & & Permuted Mnist & 10 & 10 & 92.01/\\
%     EWC/HAT & & Omniglot Rotation & 100 & 12 & 68.66/\\
%     EWC/GEM/ICARL/ER & & CIFAR100 & 10 & 10 & 72.77/70.15/53.50/70.07\\
%     EWC/GEM/ICARL/ER & & CIFAR100 Superclass & 20 & 20 & 50.26/50.35/49.05/51.64\\
%     EWC/GEM/ICARL/ER & & TinyImagenet & 40 & 5 & /50.57/54.77/48.32\\
%     \midrule
%     ours\\
%     \bottomrule
%     \end{tabular}
% \end{table}


% \section{State-of-the-art of continual learning}
% \label{sec:sota}
% \av{Brief intro to the sota methods ***we are using*** and why they are sota}

% \kf{TODO @Marcin: write about general SOTA (like 3 types of methods: regularization, dynamic architectures, experience replay, current works, etc). Shortly describe the methods we use, including: ['AGEM' yes, 'CoPE', 'Cumulative', 'CWRStar' yes, 'EWC' yes, 'GEM' yes, 'LFL', 'LwF' yes, 'Replay' yes, 'SI' yes] from Avalanche. Dominik can have a more complete list}


% \map{in progress text}

% In continual learning there are three main groups of methods: architectural strategies, regularization strategies and rehearsal strategies. 
% \textcolor{blue}{to add ICARL, cumulative, LFL, MAS, GDumb}

% \subsubsection{Architectural strategies} 

% The architectural methods concentrate on topology of neural model. The architecture is dynamic during the sequence of episodes. Many of these methods seem to be forgetting resistant. 
% %specific architectures, layers, activation functions, and/or weight freezing strategies are used to mitigate forgetting.
% Progressive neural networks \cite{rusu2016} are one of the first approaches in this group. The methodology is based on a clever combination of parameter freezing and network expansion. While PNN was shown to be effective on short series of simple tasks, the number of the model parameters keeps increasing at least linearly with the number of tasks, making it difficult to use for long sequences. Its main drawback is that is not effective for many tasks. One of the most popular architecture base approach are: CWR \cite{pmlr-v78-lomonaco17a} and CWRStar \cite{lomonaco2019nicv2} which do a copy weight with re-init in a last layers, a simpler and lighter counterpart to PNN (at the cost of a lower flexibility), with a fixed number of shared parameters and already proven to be useful on longer sequences of tasks. More fancier and efficient methods are those based on pruning. PackNet \cite{mallya2017}, SupSup \cite{wortsman2020} and WSN \cite{kang2022} are most well known representatives of this class of approaches. They are completely resistant for forgetting. One of the latest solution is Active Dendrite Neuron approach \cite{} - (model/neuron modification) new model of neuron (brain inspired neuron) based on brain neocortex. The dendrite connections strengths the context of previous tasks. It includes dual-memory models attempting to imitate hippocampus-cortex duality. Promising results were already achieved on small datasets (like permuted mnist) and reinforcement learning tasks. Method can be used (merged or mixed) also with other CL techniques.

% \subsubsection{Regularization strategies}
% This group of methods is using regularization enhancements to avoid catastrophic forgetting. The standard loss function is extended with additional loss terms promoting selective consolidation of the weights which are important to retain past memories. Include basic regularization techniques such as weight sparsification, dropout or early stopping. LwF \cite{li2017learning} method called learning without forgetting is a regularization strategy attempting to preserve the model accuracy on old tasks by imposing output stability through knowledge distillation. XdG \cite{masse2018} is context dependent gating adding a context-dependent gating signal, such that only sparse, mostly nonoverlapping patterns of units are active for any one task. EWC \cite{kirk2017} is elastic weight consolidation, weighted quadratic regularization loss which penalizes moving weights which are important for old tasks (it computes fisher ...), it has extended version called EWC++. Synaptic intelligence (SI) \cite{zenke2017} is based on weighted regularization loss which penalizes moving weights which are important for old tasks. These group of methods achieve in general worse results than architectural and rehearsal strategies because they do not use additional model or memory capacity. 

% \subsubsection{Rehearsal strategies}
% This group of methods is mainly based on remembering some data from past episodes. Past information is periodically replayed to the model to strengthen connections for memories it has already learned. A simple approach is storing part of the previous training data and interleaving them with new patterns for future training (Replay, \cite{}). A more challenging approach is pseudo-rehearsal with generative models. Gradient Episodic Memory (GEM) \cite{lopezpaz2017} is a fixed memory to store a subset of old patterns and add regularization constraints to the loss optimization, aimed not only at controlling forgetting but also at improving accuracy on previous tasks while learning the subsequent ones (positive backward transfer). A-GEM \cite{chaudhry2019} is average episodic memory with small modifications to previous method. GDumb \cite{prabhu2020} greedily stores samples in memory as they come and and at test time, trains a model from scratch using samples only in the memory. Incremental Classifier and Representation Learning (ICARL) \cite{sylvestre2016} learns strong classifiers and a data representation simultaneously. This distinguishes it from many other works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures (method both rehearsal and regularization). Gradient-Based Sample Selection (GSS) is a replay method that diversifies the gradients of the samples in the replay memory. Maximally Interfered Retrieval (MIR) is a replay method that retrieves memory sample with loss increases given the estimated parameter update based on the current batch. Generative replay (GR) (e.g. BI-R brain inspired generative replay) \cite{deven2020} is another alternative to storing data, here authors focus on generating the data to be replayed with a learned generative neural network model of past observations. The method is strongly inspired by human brain. Most challenging in this group is to reduce the memory for replay (which decrease memory size and training time).

% There are some methods which are hybrid. It means that they combine techniques from two or all three groups. The main drawback of the presented approaches is lack of consistent benchmarks that can be used across all of these methods. Very often it is difficult to compare the because they are run on different scenarios. Second thing is there is lack of complex benchmark which can make continual learning task 'closer to' real world environment. Most often combination of MNIST and CIFAR datasets are taken as baseline scenarios. 

% \subsection{Previous CL benchmarks and protocols}
% \label{sec:existing_benchmarks}
% some limitations in current benchmarks; what kind of limitations? like: they focus on datasets, don't check what happens when you change the order, usually a single or two datasets, not realistic splitMNIST;

% We can divide the most popular lifelong learning benchmarks into a few categories. The first one consist of benchmarks built upon a single dataset. The most prominent examples are splitMNIST that separates ten MNIST digits into a given number of tasks (usually five tasks with two digits each). Another widely leveraged benchmark is CIFAR10 \cite{Krizhevsky2009LearningML} that contains images for 10 classes such as cars, birds, cats, etc. A similar type of images is included in TinyImagenet  \cite{le2015tiny} that is often leveraged to create lifelong learning benchmarks. It contains a collection of 50 domestic objects from 10 categories, such as plug adapters or mobile phones. 

% There also some datasets that are very rarely used as a single benchmark, but are often leveraged to create multi-dataset. Very basic example is Street View House Numbers dataset that contains RGB images of 10 digits cut out of street view images. Very often 
% \kf{ we need to write more here} 

% The second type of lifelong learning benchmarks consist of benchmarks built on top of more than one dataset. A common approach widely applied in works such as \cite{lopezpaz2017,li2017learning,mallya2017} is to build a scenario based on two datasets, when model starts with the first one and then has to readapt to the second one. The authors of \cite{li2017learning} simulated lifelong learning settings by training model first on one of two datasets: ImageNet, Places365 and then challenging model with learning one of three other datasets: VOC, CUB, Scenes. Similar approach is followed in \cite{mallya2017}, where the model faces Imagenet at first, and then is challenged with adaptation to Places365 dataset. A prominent, but not yet widely leveraged example of benchmarks build upon more than just one dataset is a 5-Datasets benchmark described in \cite{ebrahimi2020adversarial}. As its name suggest, it leverages 5 datasets: SVHN, CIFAR10, not-MNIST, Fashion-MNIST and MNIST. The uniqure feature that distinguish this benchmark from the others is the fact that some datasets contain black and white images, while the others have 3-channeled RGB. Another multi-dataset benchmark is RecogSeq \cite{aljundi2018} that provides challenges such as an unbalanced data distribution and highly varying tasks. It is built on top of 8 datasets. 

% \kf{we need to write more here}

% \kf{TODO @Marcin: while writing about SOTA, let's gather info about what benchmarks were used for evaluation in each of those papers. See table \ref{tab:benchmarks}}
% Most popular benchmarks consist of single dataset. They are MNIST/Permuted MNIST or CIFAR10/100 datasets. The
% task incremental are built from different MNIST datasets. These scenarios are used in \cite{zenke2017}
% , \cite{}, EWC, CWR (cifar100 and core50)
% There are run without changing the order of episodes.

% \textcolor{blue}{The most diverse dataset and episode scenarios are in WSN (winning subnetworks). There are task and class incremental benchmarks. They starts form permutated mnist and different mix of mnist datasets combintions. The most complex scenarios are run on Tiny Imagenet The Tiny ImageNet is divided to 5 tasks with 40 classes. The other approach starts with pretrained ImageNet (both for WSN and PackNet) and then next episodes consist of less complex data like mnist datasets. One benchmark consists of mixing data and classes from different datasets but it does not include ImageNet}.
% \textcolor{blue}{The results show that new architectural methods like WSN, SupSup (wothout forgetting) outperform others like GEM, EWC, SI, ICARL. The difference is higher when datasets and scenarios are more complex (Omniglot, TinyImaganet)}

% \kf{table columns: metrics? task \& class incremental; models comparison;  }
% \begin{table}[]
%     \setlength{\tabcolsep}{3pt}
%     \centering
%     \begin{tabular}{lcccccccc}
%     Benchmark     &  Works    &  TI & CI      & BW \& RGB  & Balanced  & Many models & Metrics &  Curriculum  \\
%     \midrule
%     % splitMNIST       &   &  \redx & \redx & \greencheck & \redx \\
%     % Fashion MNIST    &   &  \redx & \redx & \greencheck & \redx \\
%     % CIFAR10          &   &  \redx & \redx & \greencheck & \redx \\
%     % Omniglot         &   &  \redx & \redx & \greencheck & \redx \\
%     % TinyImagenet     &   &  \redx & \redx & \greencheck & \redx \\
%     % CORe50           &   &  \redx & \redx & \greencheck & \redx \\
%     % \midrule
%     Imagenet/Places365 to \\ VOC/CUB/Scenes    & \cite{li2017learning} & \greencheck & \redx & \redx & \redx & \greencheck & ACC & \redx \\
%     Imagenet to Places365 & \cite{mallya2017} & \greencheck & \redx & \redx & \redx & \greencheck & ACC & \redx \\
%     5-Datasets       & \cite{ebrahimi2020adversarial}  & \redx & \greencheck & \greencheck & \greencheck & \redx & FACC, BWT & \redx \\ 
%     RecogSeq        & \cite{aljundi2018, DeLange2022ACL} & \greencheck & \greencheck & \redx & \redx & \greencheck & ACC, LACC, BWT & \redx \\
%     \midrule
%     Ours             &   & \greencheck  & \greencheck  & \greencheck  & \greencheck & \greencheck & LACC, BTW, FWT, FACC & \greencheck \\
    
%     % \midrule
%     \end{tabular}
%     \caption{Benchmarks comparison; metrics = metrics other than ACC; TI = task incremental; CI = class incremental}
%     \label{tab:benchmarks_comparison}
% \end{table}

% \begin{table}[]
%     % \setlength{\tabcolsep}{3pt}
%     \centering
%     \begin{tabular}{lcccccccc}
%     Benchmark     &  Works    &  i) & ii) & iii) & c)  \\
%     \midrule
%     Imagenet/Places365 to \\ VOC/CUB/Scenes    & \cite{li2017learning} & \orangecheck & \redx & \redx & \redx \\
%     Imagenet to Places365 & \cite{mallya2017} & \orangecheck & \redx & \redx & \redx \\
%     5-Datasets       & \cite{ebrahimi2020adversarial}  & \greencheck & \orangecheck & \orangecheck & \redx \\ 
%     RecogSeq        & \cite{aljundi2018, DeLange2022ACL} & \greencheck & \orangecheck & \orangecheck & \redx \\
%     \midrule
%     Ours             &   & \greencheck  & \greencheck  & \greencheck  & \greencheck \\
    
%     % \midrule
%     \end{tabular}
%     \caption{Benchmarks comparison; we included only multi-dataset benchmarks. Columns i), ii), iii) references the criteria described in introduction. i) multiple heterogeneous tasks; ii) varying task complexity, considering semantically different tasks; iii) varying technical quality of tasks. c) describes curriculum learning. The symbols have the following meaning: \greencheck - criterion is covered; \orangecheck - criterion is covered at some part or with some limitations; \redx - criterion is not covered at all.}
%     \label{tab:benchmarks_comparison_2}
% \end{table}


\section{Our benchmarks: M2I and I2M}
% In addition to fitting the devised criteria, our study includes an exhaustive consideration of the most recent metrics adopted in the lifelong learning community. 

% Specifically, our benchmark 

% However, there are not enough.
% What we would expect from the proper benchmark is:
% - Consider both TI and CI
% - Consider both BW and RGB
% - Balanced classes
% - Varying degree of task complexity
% - Many models - TOO BOLD FOR US TO SAY - WE DO ONLY 2 MODELS - 1 IN APPENDIX - ONLY 1 STUDY DOESN'T HAVE GREEN TICK 
% - Metrics
% - Curriculum 

% However, the degree of complexity is still fairly limited, since task transitions are limited to either a single task per dataset, or they do not consider a varying degree of co  multi-dataset settings in which

% novelty compared to other studies

% what we propose



\label{sec:benchmark}
%\av{we need a catchy name for the benchmark}
% FROMIN - FROM MNIST to IMAGENET
% M2I-CL - MNIST to IMAGENET LIFELONG
% M4IM or M6IM - like kubernetes = k8s; 4 - four datasets except mnist and imagenet; 6 - six datasets in total
% FROM2IM - From MNist 2 Imagenet
% CURMIN - Curriculum MNIt to Imagenet


%%% WHY : i) ii) iii) from introduction makes sense;

% i) multiple heterogeneous tasks
% ii) varying quality and complexity of the tasks
% iii) learning on a curriculum
% iv) rigorous way to measure generalization and forgetting
% v) exactly reproducible out-of-the-box

% Why is this desiderata important
In Section~\ref{sec:intro} we pointed out essential desiderata for continual learning benchmarks that are designed to reflect real-life environments and challenges. 
% CHALLENGES AND HOW WE DEAL WITH THEM
In the following, we further elaborate on each criterion, providing a rationale for its importance, and we describe how our benchmark tackles these challenges. 

First, it is important to \textbf{\textit{consider multiple heterogeneous tasks}}. The rationale is that, since continual learning models should adapt to new and unprecedented situations, as human beings usually act in real environments, they should be evaluated on sequences of heterogeneous tasks. While common benchmarks focus on homogeneous  tasks, such as different classes of handwritten digits (e.g. as in splitMNIST), heterogeneous tasks have the advantage of reflecting more realistic cases where the model is challenged by unprecedented tasks with great diversity.
%%% OUR BENCHMARK covers all desideratas
% 1) multiple heterogeneous tasks
To deal with multiple heterogeneous tasks,
our benchmark leverages 6 largely-varying image classification datasets: MNIST (handwritten digits) \cite{lecun1998mnist}, Omniglot (alphabets) \cite{lake2015human}, Fashion MNIST (clothing items) \cite{xiao2017fashion}, SVHN (street view house numbers) \cite{Netzer2011ReadingDI}, CIFAR10 (small real-world images) \cite{Krizhevsky2009LearningML}, and TinyImagenet (multi-domain large-scale real-world images) \cite{le2015tiny}. Each dataset is regarded as a task, resulting in a learning scenario with six tasks  with heterogeneous characteristics. 
We provide more details about the datasets included and the preprocessing they underwent into the benchmark in Table \ref{tab:datasets}.


Second, it is important to devise scenarios with \textbf{\textit{varying quality and task complexity}}, since an ideal model should present generalization capabilities dealing with easy, moderate, and difficult tasks at the same time, as found in the real-world. 
%
Ideal scenarios should avoid simplistic sequences of tasks with high task similarity, and prefer introducing new tasks that are different enough from the previous one, thus challenging the model in a significant way. 
%
% Moreover, a different technical quality of tasks is worth consideration, since inputs to the model could present changing conditions, such as new data shapes, changing visual and chromatic quality, and noise, due to changes in the environment, as well as changes in the imaging device used.
This aspect should comprise having tasks on images varying in terms of visual and chromatic quality and difficulty of classification.
% 2) varying task complexity
% To take this aspect into consideration, our benchmark considers datasets with a very varying level of task complexity, including very complex multi-domain real-world image classification in TinyImagenet (hard), as well as handwritten digit recognition in MNIST (easy), and letter recognition in different alphabets in Omniglot (moderate).
Our benchmarks take this into consideration as they include very complex multi-domain real-world image classification such as TinyImagenet (harder classification), as well as handwritten digit recognition in MNIST (easier classification), and letter recognition in different alphabets in Omniglot (moderate difficulty).
This choice of datasets creates ambitious but realistic challenges for CL strategies, allowing us to test their limitations.
%By doing so, our benchmark covers the second desiderata, i.e. varying task complexity.
% 3) different technical quality of tasks
%Moreover, the images in our benchmark vary significantly from task to task, e.g., different sizes and color schemes across tasks.
%\av{this is a bit bs, as ultimately you resize all images to one same resolution. I would remove this paragraph}. 
% For instance, MNIST contains black and white images of size 28x28, while Omniglot contains black and white images of size 105x105, and TinyImagenet contains RGB images of size 64x64. In this way, we are able to challenge models with different technical quality of tasks. %, covering our third desiderata.

%%% WHY CURRICULUM LEARNING
Third, the hardness of each task is relative to the ordering in which the task is presented to the model.
%
E.g., task ordering is important for us humans as we do not learn challenging new tasks from scratch but, instead, incrementally build up the necessary skills to perform these new tasks, leveraging a combination of skills learned in the past.
%
We would require the same efficiency from a continual learner.
% We expect that the same behavior should hold for models.
%
Therefore it is crucial to evaluate models \textit{\textbf{learning on a direct or inverse curriculum}}.
% challenge is \textit{\textbf{learning on a curriculum}}. 
%In addition to these three crucial points, 
 The adoption of direct curriculum learning -- learning on tasks of increasing complexity -- in conventional machine learning research showcased that significant improvements in generalization can be achieved, increasing the speed of convergence of the training process \cite{bengio2009curriculum} \cite{gao2022learning} \cite{song2020ada}. 

% Nevertheless, there is currently a lack of consideration for curriculum learning in lifelong learning works. 
When it comes to LL, however, direct and inverse curriculum learning are overlooked.
%
Indeed, in the best cases, multiple random task orderings are provided in addition to a single task order. 
% curriculum learning
To properly consider curriculum learning, our benchmark considers curriculum learning by devising a task order according to their difficulty. 
%
The scenario starts with MNIST (black \& white handwritten digits), which is regarded as an easy task. The following tasks are Omniglot (alphabets) and Fashion MNIST (clothing items), which present a spike of complexity compared to MNIST. Subsequently, SVHN (street view house numbers) brings real-world complexity by introducing images gathered from cameras with colors. CIFAR10 presents the same challenges of real-world colored images and extends them with more challenging patterns encountered in complex objects. Finally, the highest level of complexity is provided by multi-domain large-scale images from TinyImagenet.
%and TinyImagenet (multi-domain large-scale real-world images). 
In addition to the direct curriculum direction where tasks are ordered as described (from MNIST to TinyImageNet, aka M2I), we also cover the opposite case of decreasing order of difficulty (from TinyImageNet to MNIST, aka I2M). 
% Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones

% we can argue that it can be an indicator of when does the model crashes - at what complexity - on what task

Fourth, \textbf{\textit{rigorous way to measure generalization and forgetting}}.
% It is worth noting that metrics should be adopted taking into account the scenario type at hand. For instance, $\mathsf{FWT}$ is ill-defined in a class-incremental scenario, since the model will never predict classes that were never presented before. In some other cases, scenario specifications may yield metric values that are not expected or difficult to interpret.
% In all cases, it is important to devise tasks with the same number of classes. Otherwise, the average accuracy computed across tasks will be cumbersome to interpret, since tasks with a reduced number of classes will exhibit a random performance that is higher (e.g. 0.5 for 2 classes) than in tasks with a higher number of classes (e.g. 0.1 for 10 classes).
The most important aspect of continual machine learning is to design strategies and models that are able to incorporate new tasks during their lifespan, without forgetting previous tasks. 
% In this context, CL metrics such as Backward Transfer and Forward Transfer provide a way to measure how a model is able to deal with such characteristics. 
Metrics such as $\mathsf{BWT}$ and $\mathsf{FWT}$ are introduced for this reason, see Section~\ref{sec:metric_and_evaluation_protocol}
%
However, they can be cumbersome to interpret or lose their meaning, depending on the learning setting at hand.
%
% It is worth noting that metrics should be adopted taking into account the scenario type at hand. 
%
For instance, $\mathsf{FWT}$ is ill-defined in a class-incremental scenario since the model will never predict classes that were never presented before.
Another example is that of multi-dataset benchmarks where tasks contain a varying number of classes.
Specifically, tasks with a reduced number of classes will exhibit a random performance that is higher (e.g., 0.5 for 2 classes) than tasks with a higher number of classes (e.g., 0.1 for 10 classes).
%
As results are generally aggregated (i.e., averaged) across tasks \cite{li2017learning} \cite{mallya2017}, \cite{ebrahimi2020adversarial}, \cite{aljundi2018}, \cite{DeLange2022ACL}, CL metrics will be hard to interpret due to a different reference point for random performance. Ideal benchmarks should take these aspects into consideration to make sure that the calculation and the interpretation of results are correct.

% Our benchmark
To consider this aspect, we designed each task in our benchmark to contain 10 classes. In the case of MNIST and Fashion MNIST, SVHN, and CIFAR10, we use all classes. In the case of TinyImageNet and Omniglot, we select 10 classes. For TinyImagenet, we use Egyptian cat; reel; volleyball; rocking chair; lemon; bullfrog; basketball; cliff; espresso; plunger.
As for Omniglot, we select classes corresponding to  characters from the Alphabet of the Magi.
%
This setting allows us to preserve a high interpretability of all the resulting metric values
%, such as Accuracy ($\mathsf{ACC}$),
overcoming the limitation of tasks with imbalanced number of classes, where interpretability can be lost. 
%
Furthermore, to deal with class imbalance, we align the size of majority classes to that of minority classes. By doing so, we isolate the learning setting and avoid typical issues that arise in imbalanced learning, which might undermine the analysis of the final results.
%  balance number of images 

Fifth, \textbf{\textit{exactly reproducible out-of-the-box}}. Many benchmarks are not reproducible due to the lack of precise details on model configurations and experimental settings. This issue is exacerbated when the code is unavailable and it is required to implement the scenario and the evaluation scheme from scratch. In other cases, when the code is available, it is not general enough to be leveraged in different settings, e.g. when comparing with the latest models and strategies. 
%Ideally, a benchmark should be ready to use out-of-the-box, and it should be possible to easily reproduce experiments and conduct future research by extending the existing code-base. 
To this end, our benchmark is implemented on top of Avalanche \cite{lomonaco2021avalanche} -- the state-of-the-art open-source library for LL. This choice ensures the reproducibility of the experiments and paves the way for the adoption and extension of the benchmark for future research. The code for our benchmarks is publicly available at the following repository URL: \url{https://github.com/lifelonglab/M2I_I2M_benchmark}.


% {\color{blue}
% In \ref{tab:benchmarks_comparison_2} we provide a comparison of multi-dataset benchmarks assessing how many described criteria they cover and whether they are organized in a curriculum learning way. We consider only multi-dataset benchmarks, as the ones built on top of a single dataset do not provide usually provide homogeneous tasks with similar task complexity and technical quality of tasks. 
% The approach applied in \cite{li2017learning, mallya2017} where the model first encounters a task based on one dataset and then is challenged with another task based on the second dataset provides a limited feature of having multiple heterogeneous tasks. The included datasets have similar complexity and technical quality of tasks.
% 5-datasets \cite{ebrahimi2020adversarial} is an example of a benchmark that provides multiple heterogeneous tasks. However, despite having five tasks, the variety of level of task complexity is not high enough, as the most different tasks are MNIST (handwritten digits) and notMNIST (letters). The advantage of 5-datasets benchmark is also the fact that it provides a limited but significant challenge related to technical difficulty of tasks, as it included black and white tasks (e.g. MNIST) and RGB tasks (SVHN).  
% The other prominent example of benchmark with multiple heterogenous tasks is RecogSeq \cite{aljundi2018}. Unfortunately, despite leveraging a few datasets, it yields a limited variation of complexity between tasks and limited challenge related to technical quality of tasks, as the datasets are all focused on RGB image classification with similar complexity of classes.



% }
% -------------------------
% MISSING: 

% {\color{blue}
%  In the following section we provide experimental results for 12 strategies, reporting more complete set of metrics than most of the previous studies.
% }

% - we will provide results for 12 strategies

%  - public, reproducible, implemented with avalanche

% practitioners can add easily a new method or dataset

% -------------------------
%We can build scenario from the easier one to the most challenging one and in reverse order (curriculum learning).

%Our benchmark is devised to cover all three described aspects and curriculum learning. 

% The important things that we would like to point out:

% discrimination capabilities on all complexities, such as tasks characterized by low noise as well as high noise, and   with digits, it is possible to )    

% , we devise scenarios with multiple tasks that are not restricted to a single dataset. 

% Third ...

% WHAT: we devise a benchmark that follows those three rules; comparison to other benchmarks
% As we pointed out in \ref{sec:existing_benchmarks}, most of the benchmarks present in lifelong learning is built upon a single dataset separated into many tasks or upon two datasets.
% Moreover, multi-dataset benchmarks usually consists datasets that are similar to each other in terms of difficulty, complexity and parameters such as number of channels or image size.
% These benchmarks, despite their limitations, are already challenging for most of the lifelong learning methods. Nevertheless, we want to provide more challenging and realistic benchmark. 
% - why is this benchmark different from the rest?
%   - many datsets
%   - organized in curicculum learning
%   - many tasks
%   - balanced tasks
%   - 


%% Describe benchmark, datasets, etc

% The benchmark provides the following challenges in addition to the standard lifelong learning ones:
% \begin{itemize}
    % \item Dealing with increasing or decreasing difficulty of tasks (curriculum learning).
    % \item Consisting of tasks that are black and white (1 channel) and RGB tasks (3 channels).
    % \item Consisting of tasks with very various image sizes (28x28 vs 105x105).
% \end{itemize}
% 1-2 SENTENCES PER DATASET

% 

% are competitive and representative of an extension

% \av{desiderata for our benchmark: many tasks, same number of classes, challenging datasets, increasing/decreasing difficulty}

% \kf{TODO: @Everyone: Write why this benchmark is different from the rest}

% {\color{red}
% text fully in progress, just writing down thoughts
% The idea for our benchmark was to create a series of tasks that would follow the following criterias: i) built on top of at least a few datasets; ii) having increasing or decreasing difficulty of tasks (curicculum learning); iii) providing significant differences between not only classes in tasks, but also between data between tasks, iv) containing balanced number of classes for each task.
%  \kf{verify after Marcin finishes gathering benchmarks}.

\begin{table}[]
    % \footnotesize
    \caption{Overview of original datasets involved in our benchmarks. The datasets present heterogeneous characteristics, i.e., domains and technical quality.
    % \av{what does this mean? we are resizing everything afterall. Better to specify here which 10 classes we pick for the Omniglot and TinyImagenet. Write their names in the table}
    For TinyImagenet, we select the following classes: Egyptian cat; reel; volleyball; rocking chair; lemon; bullfrog; basketball; cliff; espresso; plunger.
    As for Omniglot, we select characters from the alphabet of the Magi.
    % \av{We need to talk about which are the 10 classes for Omniglot and TinyIMGnet}
    }
    \label{tab:datasets}
    \centering
    \begin{tabular}{lllrr}
    \toprule
    Dataset     & Colors    & Size      & Classes    & Available images \\
    \midrule
    MNIST           & BW        & 28x28     & 10        & 70 000 \\
    Omniglot        & BW        & 105x105   & 1632      & 32 460  \\
    Fashion MNIST   & BW        & 28x28     & 10        & 70 000  \\
    SVHN            & RGB       & 32x32    & 10        & 630 420  \\
    CIFAR10         & RGB       & 32x32     & 10        & 60 000  \\
    TinyImagenet    & RGB       & 64x64     & 200       & 100 000  \\
    \bottomrule
    \end{tabular}
\end{table}


\section{Experiments and discussion}
\label{sec:exp}
We carry out experiments involving both the \textit{task-incremental} and \textit{class-incremental} CL scenario types described in Section \ref{sec:scenario_types}, the CL strategies devised in Section \ref{sec:sota}, and the CL evaluation protocol and metrics defined in Section \ref{sec:metric_and_evaluation_protocol}. 
% We run all this on both benchmarks described in Section \ref{sec:benchmark}: M2I and I2M. 
% set-up, data pre-processing, experimental setup, technical specifications / details
%
We run an exhaustive series of experiments on our proposed M2I and I2M benchmarks for LL, resulting in 88 complete experiments (considering M2I and I2M with 11 CL strategies, 2 learning settings, and 2 model backbones) and 528 runs (model training and evaluation).
%
We aim to answer the following research questions: 
\begin{itemize}
    %\item \textbf{RQ1)} Are state-of-the-art CL strategies accurate and robust when exposed to highly complex lifelong scenarios? 
    % Is forgetting a `burden for them? - forgetting is the answer : manifestation that they are not robust
    % benchmarks following our desiderata introduced in Section \ref{sec:intro} and detailed in Section \ref{sec:benchmark}
    \item \textbf{RQ1)} Do our benchmarks provide challenging scenarios for state-of-the-art CL strategies as discussed in Section~\ref{sec:sota}? That is, are these strategies still as accurate and robust w.r.t the metrics defined in Section~\ref{sec:metric_and_evaluation_protocol} as originally introduced in their papers when exposed to M2I and I2M?
    %that follow our desired properties for considered tasks (varying complexity, increasing order of complexity, varying technical quality)?
    \item \textbf{RQ2)}
    %Can state-of-the-art CL strategies leverage curriculum task ordering to maximize their performance? Do different task orderings with varying task complexity have an impact on the final performance?
    Can state-of-the-art CL strategies leverage direct and indirect curriculum task ordering to maximize their backward and forward transfer? Or alternatively, do different task orderings with varying task complexity have an impact on the final performance?
    % \item \av{\textbf{RQ3)}
    % What is the effect of increasing the parameter size of the model used in any strategy? Or, how dependent on a specific neural architecture are the results of state-of-the-art CL strategies?}
\end{itemize}

% To answer these questions, in the following, we first devise the experimental setup of our experiments. Subsequently, we provide a detailed discussion of the extracted results.
%
We first detail the experimental setup of our experiments and then provide an in-depth discussion of the results gathered. 
%
For the curious reader, the short answer to both questions is that overall the state-of-the-art models underperform when executed on our challenging benchmarks, despite many of these models were supposed to be robust to catastrophic forgetting and multiple tasks. 

\subsection{Experimental setup}


As mentioned in Section \ref{sec:benchmark}, our benchmark provides multiple heterogeneous tasks with varying quality and task complexity. For instance, 3 of the 6 tasks contain black and white images, whereas the remainder contain colored images. Moreover, the image size varies across all tasks. There may be different ways to deal with different image channel types and sizes, which can have an impact on the final performance. However, we recognize that finding the optimal solution is an open challenge for researchers working with our benchmark, and it is out of the scope of this paper. For simplicity, for image sizes, we adopt the most frequently adopted approach, which consists in resizing all images to the same size ($64 \times 64$). To deal with different image channels, we consider the largest number of channels (RGB) for all tasks ($3$) and replicate the single-channel encountered in BW images to all $3$ channels.
We recall that, in order to provide a rigorous way to measure generalization and forgetting, we balance class sizes by taking $500$ images from each of them, for both the training and evaluation phases. By doing so, we isolate possible issues deriving from class imbalance from our evaluation. 

\paragraph{Network architecture.}
We leverage two commonly used model backbones in CL with different parameter sizes, as to measure the effect of overparametrization w.r.t. our performance metrics in LL.
Each network architecture is being used across all strategies.
We employ a Wide VGG9 \cite{simonyan2014very} as a smaller neural network for image data and an EfficientNet-b1 \cite{tan2019efficientnet} as a larger alternative. 
The hyperparameter configuration used in the experiments is: \textit{\{ epochs=50, learning\_rate=0.001, momentum=0.9 \}}. Optimization takes place through Stochastic Gradient Descent (SGD) using the Cross-Entropy loss. 
% and the ADAM optimizer.
We experimented with different negative powers of $10$ for the configuration of the learning rate as suggested in \cite{bengio2012practical}, 
%and with different powers of $2$ for the $batch\_size$. 
For the number of epochs, we experimented with similar values to those reported in the original publications of CL strategies \cite{rolnick2019experience}\cite{aljundi2018}. 
Preliminary experiments showed that different configurations did not provide a significant difference in terms of performance metric values.

\paragraph{CL strategies.}
In addition to the state-of-the-art CL strategies covered by our experiments and described in Section \ref{sec:sota}, we adopt two additional baseline approaches which loosely correspond to lower and upper bound model performance:
\begin{itemize}
\item \textbf{Naive (fine-tuning)}: The model is incrementally fine-tuned without considering any mechanism to preserve past knowledge, which, in principle, should yield a high degree of forgetting. 
This strategy allows us to compare the performance (in terms of accuracy) and forgetting (in terms of backward transfer) of smarter CL strategies.
\item \textbf{Cumulative}: New data is accumulated as it comes, and the model is retrained using all available data. 
The rationale for this baseline is to simulate upper-bound performance assuming full knowledge of the data, and unlimited computational resources to deal with stored data (storage) and model retraining (time). Cumulative can also be regarded as a variant of Replay with unlimited memory. This baseline is interesting since it allows us to estimate the accuracy that could be achieved at a much higher computational cost.
\end{itemize}
For technical details on the hyperparameter setting of the CL strategies are provided in our above-mentioned GitHub repository, which includes the code to reproduce our experiments.

\subsection{Discussion: RQ1}

% Do our benchmarks provide challenging scenarios for state-of-the-art CL strategies? Are the strategies accurate and robust when exposed to benchmarks following our desiderata introduced in Section \ref{sec:intro} and detailed in Section \ref{sec:benchmark}?
% Specifically, we analyze results in Figures \ref{fig:heatmaps_vgg9_mnist_img_class} -- \ref{fig:heatmaps_efficientnet_img_mnist_task} and Tables \ref{tab:results_vgg9_mnist_img} -- \ref{tab:efficient_net_not_pretrained_I2M}.
%
% In this subsection, we focus on the assessment of the robustness of CL strategies when exposed to our benchmarks.
% Our results are presented recurring to two modalities. 
% First, tabular results in Tables \ref{tab:results_vgg9_mnist_img} -- \ref{tab:efficient_net_not_pretrained_I2M} (aggregated) and, visual heatmaps (disaggregated) in Figures \ref{fig:heatmaps_vgg9_mnist_img_class} -- \ref{fig:heatmaps_efficientnet_img_mnist_task}, show how the model performance changes in the learning scenario progression, each time a new task is learned.
%
We present our results both as aggregated metrics computed after all the tasks have been learned in Tables \ref{tab:results_vgg9_mnist_img} -- \ref{tab:efficient_net_not_pretrained_I2M} as well as disaggregated accuracy results evaluating every task past task after learning a new one (as entries in the matrix $\mathsf{R}$, see Section~\ref{sec:metric_and_evaluation_protocol}) as heatmaps shown in Figures \ref{fig:heatmaps_vgg9_mnist_img_class} -- \ref{fig:heatmaps_efficientnet_img_mnist_task}.
%
These heatmaps allow us to understand at a finer grain what are the failure modes of a strategy and whether certain tasks are harder than another. 
% For heatmaps, each entry depicts the accuracy of a given task at a specific moment in time. Entries on the main diagonal and below allow assessing model performance on the current and previously learned tasks, whereas entries above the main diagonal represent performance on future tasks. To avoid confusion, we provide the former in most of our visualizations, restricting the analysis of the latter to Figure \ref{fig:cifar10_forward}. Rows represent training tasks, and columns represent evaluation tasks. For example, according to this scheme, the entry in position row: 3, column: 2 corresponds to the accuracy on task 2 after learning task 3.
%% END OF IJCNN
%for the two model backbones considered in our study (VGG9, EfficientNet). 
% RQ2: Does our benchmark provide challenging scenarios for methods that follow our desired properties for considered tasks (varying complexity, increasing order of complexity, varying technical quality)?
% Considering all combinations of two scenario types (class-incremental, task-incremental) and two task orderings (M2I and I2M), experimental results can be analyzed by discussing how the performance of different CL strategies varies in these four settings.
We now discuss four different settings, comprising either a class-incremental or task-incremental scenario and two task orderings (M2I or I2M). We start from employing the smaller model backbone: VGG9.

\subsubsection{VGG9}

For both M2I and I2M (see Tables \ref{tab:results_vgg9_mnist_img} -- \ref{tab:results_vgg9_img_mnist}), aggregated $\mathsf{ACC}$, $\mathsf{BWT}$ and $\mathsf{FWT}$ (only for task-incremental scenarios, see Section~\ref{sec:benchmark}) are disappointing for all strategies discussed in Section~\ref{sec:sota}.
They do not achieve a positive backward transfer and all highlight a systematic catastrophic forgetting, while the forward transfer floats around chance level (10\%\footnote{We remark that this trend is easy to spot and understand in our benchmarks as all tasks sports only 10 classes.}).
Staple strategies such as LwF, MAS, GDUMB and SI
 are generally comparable with the Naive strategy (i.e., just applying fine-tuning). 
%
% The ranking observed in the experiments is  concerning since complex \textcolor{red}{mp: I don't know if complex is good word here because there are more complx and efficient methods outside avalanche} and well-known CL strategies appear unreliable in our devised scenario, showcasing an unsatisfactory performance that is comparable to the Naive strategy. 
%
% Results on M2I and I2M (see Tables \ref{tab:results_vgg9_mnist_img} -- \ref{tab:results_vgg9_img_mnist}) show that
% %The general trend that can be observed across all learning settings is that 
% the Naive strategy achieves close results to some of the methods (e.g. LwF, MAS, GDUMB, SI).
% %as evident in the metric values and rankings reported in Table \ref{tab:results_vgg9_mnist_img} - \ref{tab:results_vgg9_img_mnist}. 
% % M2I AND I2M FOR BOTH CLASS/TASK INCREMENTAL

We inspect rankings over the $\mathsf{ACC}$ score to see if any notable trend manifests between different scenario types.
We found that  
AGEM is very weak in the class-incremental setting (ranked 11 and 9, respectively), but quite robust in the task-incremental setting (ranked 4 and 3). Surprisingly, GEM seems to be robust in both settings (ranked 3 in three out of four settings, and 8 in the fourth setting). CWRStar achieves a moderately high position in the class-incremental setting (ranked 4 in the ranking for M2I). 
%
While its ranking is surprisingly high, we remark that the raw performance is clearly unsatisfactory, when considered in absolute terms in this context. 
%In fact, its performance is still unsatisfactory when considered in absolute terms. 
A much lower ranking is observed in the task-incremental setting, despite the slight improvement in its performance. Overall, CWRStar appears ineffective in preventing catastrophic forgetting across all tasks in our settings, and it appears that it just focuses on memorizing the first task.
% The ranking observed in the experiments is  concerning since complex \textcolor{red}{mp: I don't know if complex is good word here because there are more complx and efficient methods outside avalanche} and well-known CL strategies appear unreliable in our devised scenario, showcasing an unsatisfactory performance that is comparable to the Naive strategy. 

% High level of forgetting, 
The method that seems to be the least prone to forgetting is Replay. This result is surprising since the total memory size chosen for the replay buffer in the experiment is just 200 samples. 
% \av{Where are all the hyperparams of all the strategies used written? We should create one appendix for it}. 
It is also interesting to observe that Replay presents a performance that is quite close to Cumulative (an $8\%-18\%$ decrease in accuracy across the four mentioned settings) using a fraction of available data (less than 1\%). 
% TOP
As expected, Cumulative presents the best performance across all four learning settings. However, \textit{it should be seen as an unrealistic upper bound}, since it assumes that infinite memory and training time are allowed for the model. On a different note, the positive results confirm that the scenarios designed in our benchmark are reasonable and can be, in principle, learned by the model but current CL strategies.
While these have shown to be reliable in conventional CL scenarios, they are not bulletproof and present limited robustness when exposed to more complex scenarios, such as our M2I (\textbf{RQ1}).

% HEATMAPS
Observing the heatmaps in Figure \ref{fig:heatmaps_vgg9_mnist_img_class} -- \ref{fig:heatmaps_vgg9_img_mnist_task}  allows us to zoom in and pinpoint the performance drops of different strategies on specific tasks. In this context, observing a decreasing performance on previously learned tasks is a clear manifestation of forgetting.
%
Results are quite negative for M2I in the class-incremental setting (see Figure \ref{fig:heatmaps_vgg9_mnist_img_class}). GEM preserves a good performance until the third task is presented, and then dramatically drops in the following tasks, due to their increasing complexity. GDumb presents a high performance on the second task throughout the entire scenario, but an unsatisfactory performance on all other tasks. This behavior likely depends on the fact that it is possible to learn the second task (Omniglot) with a limited number of samples, whereas this is too difficult for all other tasks. All other strategies, except Replay and Cumulative, struggle to preserve the knowledge of previous tasks and are successful at learning the last task exclusively, as evidenced by the very low-performance scores. 
The results for M2I in the task-incremental setting (see Figure \ref{fig:heatmaps_vgg9_mnist_img_task}) showcase a higher overall performance, with lower forgetting than the class-incremental setting. For instance, it can be observed that MAS and SI is able to preserve much more knowledge for some tasks, while its forgetting was rather drastic in the class-incremental setting. 

% VGG9: I2M CLASS AND TASK INCREMENTAL 
For I2M in the class-incremental setting (see Figure \ref{fig:heatmaps_vgg9_img_mnist_class}), a similar behavior to the class-incremental counterpart of M2I can be observed, with drastic forgetting, which is even worse than the M2I scenario.  
As for I2M in the task-incremental setting (see Figure \ref{fig:heatmaps_vgg9_img_mnist_task}), it is also interesting to observe worse results than in the M2I task-incremental setting. 
%
This is also counterintuitive, as successfully learning a harder task should provide the model with enough knowledge not to perform so poorly on much simpler tasks that, as MNIST, might require learning only simple edge detectors.
%
We conjecture that this behavior might depend on the fact that once a model is presented with very different but complex tasks earlier in the scenario (e.g., ImageNet and CIFAR10) it might have a harder time learning to abstract useful features for simpler tasks later.
%


% \textcolor{red}{mp: maybe we should add more here, if the forgetting is constant or larger between some datasets, it also mentioned somewhere if model start as pretrained or not pretrained} \av{did we pretrain the model on the full Imagenet?}

    \begin{table}[]
        % \scriptsize
        \centering
        \input{tables-dual-types/results_short_mnist_omniglot_fmnist_svhn_cifar10_imagenet_balanced_500_wide_VGG9}
        \caption{Experimental results (Wide-VGG99 -- M2I) in terms of average performance (and rank) for all CL strategies in two learning settings.}
        \label{tab:results_vgg9_mnist_img}
    \end{table}
    
    \begin{table}[]
        % \scriptsize
        \centering
        \input{tables-dual-types/results_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_500_wide_VGG9}
        \caption{Experimental results (Wide-VGG99 -- I2M) in terms of average performance (and rank) for all CL strategies in two learning settings.}
        \label{tab:results_vgg9_img_mnist}
    \end{table}
    
    \begin{figure*}[h]
        \centering
        \includegraphics[width=\textwidth]{plots/strategies_wide_VGG9_short_mnist_omniglot_fmnist_svhn_cifar10_imagenet_balanced_500_class_incremental.pdf}
        \caption{Experimental results (Wide-VGG9 -- M2I -- Class-incremental) in terms of disaggregated performance ($\mathsf{ACC}$) on single tasks after learning previous tasks.
        % \av{Is it possible to replot the heatmaps having a much larger text (it is not readable now) with just the name of the strategy in the top right corner of the matrix?} % We improved it a little
        }
        \label{fig:heatmaps_vgg9_mnist_img_class}
    \end{figure*}  


    \begin{figure*}[h]
        \centering
        \includegraphics[width=\textwidth]{plots/strategies_wide_VGG9_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_500_class_incremental.pdf}
        \caption{Experimental results (Wide-VGG9 -- I2M -- Class-incremental) in terms of disaggregated performance ($\mathsf{ACC}$) on single tasks after learning previous tasks. 
        % \av{Is it possible to replot the heatmaps having a much larger text (it is not readable now) with just the name of the strategy in the top right corner of the matrix?}
        }
        \label{fig:heatmaps_vgg9_img_mnist_class}
    \end{figure*}  
    
    \begin{figure*}[h]
        \centering
        \includegraphics[width=\textwidth]{plots/strategies_wide_VGG9_short_mnist_omniglot_fmnist_svhn_cifar10_imagenet_balanced_500_task_incremental.pdf}
        \caption{Experimental results (Wide-VGG9 -- M2I -- Task-incremental) in terms of disaggregated performance ($\mathsf{ACC}$) on single tasks after learning previous tasks.}
        \label{fig:heatmaps_vgg9_mnist_img_task}
    \end{figure*} 
    
    
    \begin{figure*}[h]
        \centering
        \includegraphics[width=\textwidth]{plots/strategies_wide_VGG9_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_500_task_incremental.pdf}
        \caption{Experimental results (Wide-VGG9 -- I2M -- Task-incremental) in terms of disaggregated performance ($\mathsf{ACC}$) on single tasks after learning previous tasks.}
        \label{fig:heatmaps_vgg9_img_mnist_task}
    \end{figure*}  


\subsubsection{EfficientNet}
% TABLE 5 - TABLE 6
% Naive in comparison with others
Results on M2I and I2M (see Tables \ref{tab:efficient_net_not_pretrained_M2I} -- \ref{tab:efficient_net_not_pretrained_I2M}) show that the Naive strategy (ranked 5) achieves a performance that is close to some of the CL strategies (e.g. MAS, SI, EWC) but is significantly inferior to top performing strategies (Replay, Cumulative). 
% Methods that are not robust in class-incremental but are robust in task-incremental
When comparing class-incremental and task-incremental settings for M2I, some methods appear significantly more robust in the latter, with a simultaneous increase in their performance and position in the ranking\footnote{It is important to track both aspects, since the task-incremental setting is fundamentally easier than class-incremental, and observing only the absolute performance of the methods is not indicative of an improvement.}. This is the case for AGEM (ranked 9 and 5, respectively),  and GEM (ranked 4 and 3, respectively). For I2M, comparing class and task-incremental settings, the same phenomenon can be observed for a larger number of methods AGEM (ranked 9 and 6, respectively), MAS (ranked 8 and 7, respectively), SI (ranked 7 and 4, respectively), and EWC (ranked 6 and 5, respectively). 
% Methods that are robust in both settings
Some strategies present a rather stable behavior in the two learning settings, since they appear to preserve their ranking. For M2I, this is the case for GDumb, LwF, and Replay. For I2M, this phenomenon applies to CWRStar, GDumb, and Replay.
% TOP performance: Cumulative
As observed in VGG9 results, Cumulative showcases the best performance across all four learning settings, resulting in the top-ranked strategy.
% General consideration about the ranking and performances
Therefore, in absolute terms, the performance of informed strategies can be regarded as unsatisfactory, as it appears significantly lower than Cumulative. 
%
This result suggests that even increasing the parametrization of the backbone model does not provide these staple CL strategies to significantly improve over our simpler baselines in complex benchmarks such as our M2I  and I2M (\textbf{RQ1}).


    \begin{table}[h]
        % \scriptsize
        \centering
        \input{tables-dual-types/results_short_mnist_omniglot_fmnist_svhn_cifar10_imagenet_balanced_500_EfficientNet_NotPretrained.tex}
        \caption{Experimental results (EfficientNet -- M2I) in terms of average performance (and rank) for all CL strategies in two learning settings.}
        \label{tab:efficient_net_not_pretrained_M2I}
    \end{table}

    \begin{table}[h]
        % \scriptsize
        \centering
        \input{tables-dual-types/results_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_500_EfficientNet_NotPretrained.tex}
        \caption{Experimental results (EfficientNet -- I2M) in terms of average performance (and rank) for all CL strategies in two learning settings.}
        \label{tab:efficient_net_not_pretrained_I2M}
    \end{table}


% HEATMAPS
% FIGURE 6 - FIGURE 7
 Shifting our focus to the heatmaps in Figure \ref{fig:heatmaps_efficientnet_mnist_img_class} -- \ref{fig:heatmaps_efficientnet_img_mnist_task} we are able to analyze in detail the forgetting of the different strategies throughout the experimental scenario. 

Figure \ref{fig:heatmaps_efficientnet_mnist_img_class} shows a vast amount of forgetting across all strategies. Some exceptions can be sparsely observed. For instance, MAS preserves its performance on task 0 after learning task 1, before dropping to values that are close to zero for previously encountered tasks. CWRStar preserves a remarkably high performance on the first task, but a very limited ability to incorporate new tasks. This result is in contrast with what was observed with VGG9, where the performance on the first task was preserved but decaying as new tasks are presented. This phenomenon may depend on the number of layers involved in the model backbone, since EfficientNet is a much larger model, and the weight adaptation strategy  used in CWRStar exclusively involves the last layer. As a result, a larger model such as EfficientNet will be more prone to knowledge retention than adaptation.
Moving to I2M in the class-incremental setting (see Figure \ref{fig:heatmaps_efficientnet_img_mnist_class}), a noteworthy result is GEM preserving knowledge of task 3 throughout the entire scenario, while not being able to preserve its performance on the other tasks. In this setting, CWRStar is fundamentally unable to learn any of the tasks presented in the scenario. 


        
    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{plots/strategies_EfficientNet_NotPretrained_short_mnist_omniglot_fmnist_svhn_cifar10_imagenet_balanced_500_class_incremental.pdf}
        \caption{Experimental results (EfficientNet -- M2I -- Class-incremental) in terms of disaggregated performance ($\mathsf{ACC}$) on single tasks after learning previous tasks.}
        \label{fig:heatmaps_efficientnet_mnist_img_class}
    \end{figure}  
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{plots/strategies_EfficientNet_NotPretrained_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_500_class_incremental.pdf}
        \caption{Experimental results (EfficientNet -- I2M -- Class-incremental) in terms of disaggregated performance ($\mathsf{ACC}$) on single tasks after learning previous tasks.}
        \label{fig:heatmaps_efficientnet_img_mnist_class}
    \end{figure}  

        \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{plots/strategies_EfficientNet_NotPretrained_short_mnist_omniglot_fmnist_svhn_cifar10_imagenet_balanced_500_task_incremental.pdf}
        \caption{Experimental results (EfficientNet -- M2I -- Task-incremental) in terms of disaggregated performance ($\mathsf{ACC}$) on single tasks after learning previous tasks.}
        \label{fig:heatmaps_efficientnet_mnist_img_task}
    \end{figure}  
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{plots/strategies_EfficientNet_NotPretrained_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_500_task_incremental.pdf}
        \caption{Experimental results (EfficientNet -- I2M -- Task-incremental) in terms of disaggregated performance ($\mathsf{ACC}$) on single tasks after learning previous tasks.}
        \label{fig:heatmaps_efficientnet_img_mnist_task}
    \end{figure}  
    


Interestingly, task similarity between two tasks, manifested by positive backward transfer, allows for improvement on previously learned tasks in some instances. In Figure \ref{fig:heatmaps_efficientnet_mnist_img_task}, for instance, learning task 4 is, in some cases, beneficial for the model's performance on task 1, as observed for MAS, GEM, and Naive. 

In the task-incremental setting (see Figure \ref{fig:heatmaps_efficientnet_img_mnist_task}), GEM presents a similar behavior to that observed in class incremental on task 2, but also preserves knowledge of task 0 throughout the entire scenario, whereas the performance on other tasks is fundamentally sub-optimal. In this setting, however, CWRStar behaves as in the M2I class incremental setting, i.e., the performance on task 0 is preserved throughout the entire scenario.  



% TABLE
% HEATMAPS
%    M2I - T C
%    I2M - T C

\subsubsection{Summary: RQ1}
In summary, results observed across the two learning settings (class-incremental, task-incremental) in the two presentation orders (M2I, I2M) show unsatisfactory performance for all learning strategies and that catastrophic forgetting is a real burden for many of the covered methods.

Considering that the results observed are inferior when compared to what is commonly reported in continual learning research, we can argue that our benchmark provides more challenging conditions for the CL strategies.
It is noteworthy that forgetting in CL strategies is also observed in perceptually similar tasks (e.g. MNIST, Omniglot, SVHN), as evident in our heatmaps. This behavior is indicative of the objective lack of robustness presented by CL strategies as they are exposed to tasks from different datasets.
%, which satisfies the core of our research question. 
The five desiderata described in Section \ref{sec:benchmark} and adopted to design our benchmarks set up a higher standard for the evaluation of CL strategies, and will hopefully stimulate the design and implementation of new, more robust strategies. 

% Moreover, results show that catastrophic forgetting is a real burden for many of the covered methods and in both learning settings (class-incremental, task-incremental). This result is evident from the accuracy values dropping as we evaluate the model on all tasks after learning a new task. 

\subsection{Discussion: RQ2}
In this subsection, we focus on the assessment of the ability of CL strategies to leverage curriculum task ordering to maximize their performance when exposed to our benchmarks devised in \ref{sec:benchmark}.

% Methods do not fully leverage curriculum but there is hope

% M2I higher results than I2M: but obscure 

% Task similarity

% Metrics are not particularly useful

% Do our benchmarks provide challenging scenarios for state-of-the-art CL strategies? Are the strategies accurate and robust when exposed to benchmarks following our desiderata introduced in Section \ref{sec:intro} and detailed in Section 

% Can state-of-the-art methods leverage tasks ordered in a curriculum manner to maximize their learning performance?
To answer this question, we start by analyzing results in Tables \ref{tab:results_vgg9_mnist_img} -- \ref{tab:efficient_net_not_pretrained_I2M}, which show metric values for the two scenarios: M2I (direct curriculum learning) and I2M (inverse curriculum learning). Almost all methods present a better performance in the curriculum learning setting (M2I) when compared with the inverse curriculum setting (I2M). 
%This phenomenon can be observed by comparing values in Tables \ref{tab:results_vgg9_mnist_img} -- \ref{tab:efficient_net_not_pretrained_I2M}.
% ordering.
% VGG9?
Comparing values in Tables \ref{tab:results_vgg9_mnist_img} and \ref{tab:efficient_net_not_pretrained_M2I}, significant examples for VGG9 include Replay (from $0.550$ to $0.755$ in class-incremental and from $0.571$ to $0.730$ in task-incremental) and GEM (from $0.297$ to $0.572$ in class-incremental and from $0.269$ to $0.613$ in task-incremental).   Other CL strategies present a smaller margin of improvement. For instance, LwF (from $0.216$ to $0.222$ in class-incremental, and from 0.262 to $0.349$ in task-incremental) and EWC (from $0.190$ to $0.220$ in class-incremental, and from $0.340$ to $0.395$ in task-incremental).
% EFFICIENTNET
Shifting the focus on results with EfficientNet  (comparing Tables \ref{tab:efficient_net_not_pretrained_M2I} and \ref{tab:efficient_net_not_pretrained_I2M}), examples include Replay (from $0.417$ to $0.655$ in class-incremental, and from $0.438$ to $0.605$ in task-incremental), and Cumulative (from $0.606$ to $0.834$ in class-incremental, and from $0.596$ to $0.656$ in task-incremental). Other CL strategies present a more limited but still significant improvement. For instance, CWRStar (from $0.053$ to $0.326$ in class-incremental, and from $0.171$ to $0.367$ in task-incremental), and GEM (from $0.218$ to $0.312$ in class-incremental, and from $0.276$ to $0.420$ in task-incremental). Counterexamples, where the model's performance is higher in the inverse curriculum setting (I2M), include LwF (from $0.203$ to $0.165$ in class-incremental and from $0.241$ to $0.240$ in task-incremental). This result shows that different strategies behave differently when presented with a different task ordering. 

% ________________________
% Point 2: Task Similarity
% ________________________
Another interesting point pertaining to our research question is the opportunity to identify whether learning new tasks favors performance on previously learned tasks, emphasized in our heatmaps. This phenomenon may happen if the model is able to capture similarities between tasks that can be fruitfully leveraged for inference. To show some examples, we focus on task-incremental experiments. 
In the M2I scenario with VGG9, Figure \ref{fig:heatmaps_vgg9_mnist_img_task} shows that different strategies (AGEM, MAS, SI) are able to improve performance on task 1 (Omniglot) after learning task 5 (TinyImageNet). 
We also observe that multiple strategies (AGEM, EWC, SI, MAS) can leverage the skills learned in task 3 (SVHN) to improve performance on task 1 (MNIST). 
This result is intuitive since learning the complexity of street numbers in images acquired with a camera, strongly benefits the predictive capabilities on an easier dataset from the similar domain, i.e., MNIST. 
Similar behavior can be observed in Figure \ref{fig:heatmaps_efficientnet_mnist_img_task}, where AGEM improves the performance on task 2 (Fashion MNIST) after learning task 5 (TinyImagenet). 
In the I2M scenario with VGG9, Figure \ref{fig:heatmaps_vgg9_img_mnist_task} and Figure \ref{fig:heatmaps_efficientnet_img_mnist_task} show that almost all strategies improve the performance on task 3 (Fashion MNIST) and task 4 (Omniglot) after learning task 5 (MNIST).
This result shows that the knowledge learned from MNIST can boost the performance on more complex tasks learned before. 
Overall, results show that task ordering and task similarity can be leveraged to improve performance on a previously learned task, although the currently adopted CL strategies are sparsely able to yield this capability. This consideration paves the way for the design of new strategies that further leverage curriculum task ordering to boost forward and backward transfer.

% ________________________
% Point 3: Discussion on validity of metrics
% ________________________
An additional consideration pertains to the connection between curriculum learning and the appropriateness of CL metrics in this context. 
For instance, we note that MNIST is the simplest task and it is presented as the first task in the curriculum learning setting, it will be considered multiple times in the evaluation protocol, i.e., each time a new task is presented, which may boost the final average result presented by the accuracy metric. In turn, it is more likely for the curriculum learning setting to achieve higher average performance. This behavior poses issues in the interpretability of metric values, which are still unaddressed by currently available metrics. 
%can see that task ordering is particularly useful  
A clearer perspective is provided by the  heatmaps in Figures \ref{fig:heatmaps_vgg9_mnist_img_class}, \ref{fig:heatmaps_vgg9_img_mnist_class}, \ref{fig:heatmaps_vgg9_mnist_img_task}, \ref{fig:heatmaps_vgg9_img_mnist_task}. 


    % - Curriculum: is there a clear difference in going from one direction to another? Compare figure 2 vs. 3 > they do change
    % - Architecture: do they make a difference (vgg9 vs. efficientnet)
    % - Appendix> EfficientNet

% Input Resolution 64x64x3 (upscale accordingly) 
% \begin{itemize}
%     \item make sure classes are not overlapping
%     \item make sure each dataset has good results on its own
%     \item make sure random classes are not random for each execution
%     \item check if SVHN can be added quickly, if not, remove it from the scenarios 
% \end{itemize}

% Proposal for the short scenario 
% \begin{itemize}
%     \item MNIST (1 task)
%     \item OMNIGLOT (1 task)
%     \item Fashion Mnist (1 task)
%     \item SVHN (Street House Number) (1 task)
%     \item CIFAR10 (1 task)
%     \item TINYIMAGENET (1 task)
% \end{itemize}

% Proposal for the long scenario 
% \begin{itemize}
%     \item MNIST (1 task)
%     \item OMNIGLOT (10 task)
%     \item FMNIST (1 task)
%     \item SVHN (1 task)
%     \item CIFAR100 (10 task)
%     \item TINYIMAGENET (10 task)
% \end{itemize}


% \subsection{Evaluation procedure and metrics}
%     \kf{TODO @ : describe evaluation procedure and metrics}
    % \cite{diazrodriguez2018dont}
    
    % The authors pointed out that the metrics from \cite{2016lopez_paz_gradient_episodic_memory} do not consider the results during the whole sequence evaluation. They follow the same approach as in \cite{2016lopez_paz_gradient_episodic_memory}, create the same matrix $R^{T \times T}$, but other definition of metrics. They take into account each step between learning given task and the end of the sequence.
    
    % Accurracy: \\
    % \includegraphics[width=0.2\textwidth]{figures/tmp/accuracy.png} \\
    % Backward Transfer:\\
    % \includegraphics[width=0.4\textwidth]{figures/metrics/tmp/more-than-forgetting/bwt.png} \\
    % Forward Transfer:\\
    % \includegraphics[width=0.3\textwidth]{../figures/metrics/tmp/more-than-forgetting/fwt.png} \\
    % Additional explanation:\\
    % \includegraphics[width=0.9\textwidth]{figures/metrics/tmp/more-than-forgetting/matrix.png}


\subsubsection{Summary: RQ2}

Overall, results observed across two learning settings (class-incremental, task-incremental) in the two presentation orders (M2I, I2M) show that current methods are not able to fully leverage curriculum learning. 
One reason may be the fact that most of the CL strategies are heavily impacted by forgetting since they are challenged by the complexities involved in our proposed benchmarks. 
% \textcolor{red}{mp: maybe it should be added that in case of strategies without forgetting (e.g. PackNet, WSN) performed research is also useful, it can help how to estimate size of the subnetworks for each new task, reviewer can ask what about CL methods without forgetting}
Comparing the performance and behavior of the CL strategies between M2I and I2M scenarios, we can also observe that different task orderings significantly impact the final outcomes.
%the performance of the models.
% In general, methods can deal better with the task-incremental setting than the class-incremental setting, presenting a lower amount of forgetting. 
On the other hand, methods appear to partially benefit from task similarity in some specific cases, as highlighted in our analysis of results. This outcome leads us to the consideration that task similarity could be further exploited by CL strategies to yield models that simultaneously use the knowledge acquired from different tasks to perform better in every single task. 

%For instance, CWRStar retains good performance only on the first task (MNIST), presenting a large amount of forgetting for the remaining tasks. 
% due to the fact that it fine tunes only the last layer
%GEM appears to deal better in scenarios that are organized in a curriculum learning setting rather than inverse curriculum learning. 
% probably because learning is more conducive in the first task, since in later tasks learning late will be reduced, so it retains the knowledge and that helps learning more complex stuff later on

% RQ4: Do different task orderings provide significant differences in performance when challenged with varying task complexity?

\section{Conclusions}
In this work, we focused on the problem of benchmarking CL methods, which is often conducted in  heterogeneous ways, and with significant simplifications for the learning setting.
%, including high inter-task similarity and... .
Specifically, we proposed two novel benchmarks that involve multiple heterogeneous tasks with varying qualities and complexities.
%, ordered in a curriculum fashion (M2I and I2M). 
Our benchmarks involved six image datasets in increasing (M2I) and decreasing (I2M) difficulty order, following the curriculum learning paradigm. 
The heterogeneity across datasets allowed us to inject realistic complexities into the learning scenario, resulting in challenging conditions for CL strategies.
Particular emphasis was put on the rigorous and reproducible evaluation of model generalization capabilities and forgetting.
Our extensive experimental evaluation showed that popular CL strategies, which are known to be robust on commonly adopted scenarios, fail to achieve satisfactory performance with our benchmarks. Moreover, CL strategies are affected by forgetting and are not able to effectively leverage curriculum task ordering to improve their performance and robustness, missing on the opportunity of simultaneously using knowledge from different tasks to perform better in every single task.
Our results represent a starting point to assess the impact of curriculum learning on CL strategies. 
Future work includes the design of new CL strategies that are able to deal with the complexities devised in our benchmarks. Moreover, from an evaluation perspective, new metrics could be investigated to fully capture the spectrum of model behavior with different task orderings. 
Finally, an interesting line of research pertains to the analysis of the behavior of non-conventional CL strategies, which are not yet incorporated in known frameworks due to their emerging nature.

% describe results 
% all: find some patterns and explanations
% 

    


% 
% \onecolumn

% \subsection{Class Incremental}

%     \begin{table}[h]
%         % \scriptsize
%         \centering
%         \input{tables/results_class_incremental_wide_VGG9_balanced_500}
%         \caption{Test table: wideVGG9 NOT pretrained balanced 500}
%     \end{table}
    
%   \begin{figure}[h]
%         \centering
%         \includegraphics[width=\textwidth]{plots/strategies_wide_VGG9_short_mnist_omniglot_fmnist_svhn_cifar10_imagenet_balanced_500_class_incremental.pdf}
%         \caption{\mnistsiximgshortbalanced{} class incremental wide VGG9 NOT pretrained balanced 500}
%         \label{fig:my_label}
%     \end{figure} 
    
%     \begin{figure}[h]
%         \centering
%         \includegraphics[width=\textwidth]{plots/strategies_wide_VGG9_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_500_class_incremental.pdf}
%         \caption{\mnistsiximgshortbalanced{} class incremental wide VGG9 NOT pretrained balanced 500}
%         \label{fig:my_label}
%     \end{figure}  

    

% \subsection{Task Incremental}
%     \begin{table}[h]
%         % \scriptsize
%         \centering
%         \input{tables/results_task_incremental_wide_VGG9_balanced_500}
%         \caption{Test table: Task incremental wideVGG9 NOT pretrained balanced 500}
%     \end{table}
    
    


% \section{Other results}


% % \subsubsection*{Author Contributions}
% % If you'd like to, you may include  a section for author contributions as is done
% % in many journals. This is optional and at the discretion of the authors.

% % \subsubsection*{Acknowledgments}
% % Use unnumbered third level headings for the acknowledgments. All
% % acknowledgments, including those to funding agencies, go at the end of the paper.

% %

%     \begin{table}[h]
%         % \scriptsize
%         \centering
%         \input{tables/results_class_incremental_EfficientNet_balanced_500}
%         \caption{Test table: Class Incremental EfficientNet pretrained balanced 500}
%     \end{table}
    
    
%         \begin{table}[h]
%         % \scriptsize
%         \centering
%         \input{tables/results_class_incremental_EfficientNet_NotPretrained_balanced_5000}
%         \caption{Test table: Class Incremental EfficientNet NOT pretrained balanced 5000}
%     \end{table}
    
%         \begin{figure}[h]
%         \centering
%         \includegraphics[width=\textwidth]{plots/strategies_EfficientNet_short_mnist_omniglot_fmnist_svhn_cifar10_imagenet_balanced_500_class_incremental.pdf}
%         \caption{\mnistsiximgshortbalanced{} class incremental pretrained balanced 500}
%         \label{fig:my_label}
%     \end{figure}  
    
%     \begin{figure}[h]
%         \centering
%         \includegraphics[width=\textwidth]{plots/strategies_EfficientNet_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_500_class_incremental.pdf}
%         \caption{\imgsixmnistshortbalanced{} class incremental pretrained balanced 500}
%         \label{fig:my_label}
%     \end{figure}  
    
    
        
%     \begin{figure}[h]
%         \centering
%         \includegraphics[width=\textwidth]{plots/strategies_EfficientNet_NotPretrained_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_5000_class_incremental.pdf}
%         \caption{\imgsixmnistshortbalanced{} class incremental NOT pretrained balanced 5000}
%         \label{fig:my_label}
%     \end{figure}  
    
        
%     \begin{figure}[h]
%         \centering
%         \includegraphics[width=\textwidth]{plots/strategies_EfficientNet_NotPretrained_short_imagenet_cifar10_svhn_fmnist_omniglot_mnist_balanced_5000_class_incremental.pdf}
%         \caption{\mnistsiximgshortbalanced{} class incremental NOT pretrained balanced 5000}
%         \label{fig:my_label}
%     \end{figure}  
    
    
%         \begin{table}[h]
%         % \scriptsize
%         \centering
%         \input{tables/results_task_incremental_EfficientNet_balanced_500}
%         \caption{Test table: Task Incremental EfficientNet balanced 500}
%     \end{table}
    
    
        
%     \begin{table}[h]
%         % \scriptsize
%         \centering
%         \input{tables/results_task_incremental_EfficientNet_multihead_balanced_500}
%         \caption{Test table: Task Incremental MultiHead EfficientNet multihead balanced 500}
%     \end{table}
    
%         \begin{table}[h]
%         % \scriptsize
%         \centering
%         \input{tables/results_task_incremental_EfficientNet_MultiHead_NotPretrained_balanced_500}
%         \caption{Test table: Task Incremental MultiHead NOT PRETRAINED EfficientNet multihead balanced 500}
%     \end{table}
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% \section*{Appendix A: Selected classes}

% Classes selected for TinyImagenet: Egyptian cat; reel; volleyball; rocking chair; lemon; bullfrog; basketball; cliff; espresso; plunger.
% Classes selected for Omniglot: characters from Alphabet of the Magi.
% \backmatter

% \bmhead{Supplementary information}

% If your article has accompanying supplementary file/s please state so here. 

% Authors reporting data from electrophoretic gels and blots should supply the full unprocessed scans for key as part of their Supplementary information. This may be requested by the editorial team/s if it is missing.

% Please refer to Journal-level guidance for any specific requirements.

% \bmhead{Acknowledgments}


% \section*{Acknowledgments}


% Acknowledgments are not compulsory. Where included they should be brief. Grant or contribution numbers may be acknowledged.

% Please refer to Journal-level guidance for any specific requirements.

\newpage 
\section*{Declarations}

% Some journals require declarations to be submitted in a standardised format. Please check the Instructions for Authors of the journal to which you are submitting to see if you need to complete this section. If yes, your manuscript must contain the following sections under the heading `Declarations':

\begin{itemize}
\item \textbf{Funding} -- The paper was supported by the Polish Ministry of Science and Higher Education allocated to the AGH UST. \\

\item \textbf{Conflict of interest/Competing interests} -- All authors certify that they have no affiliations with or involvement in any organization or entity with any financial interest in the subject matter or materials discussed in this manuscript. \\

\item \textbf{Ethics approval} -- Not applicable. \\

\item \textbf{Consent to participate} -- This study does not involve human subjects or any sensitive data. \\

\item \textbf{Consent for publication} -- This study does not involve human subjects or any sensitive data.  \\

\item \textbf{Availability of data and materials} -- The data and materials to reproduce the experiments are available at the following repository URL: \url{https://github.com/lifelonglab/M2I_I2M_benchmark} \\

\item \textbf{Code availability} -- The code of the proposed benchmarks is available at the following repository URL: \url{https://github.com/lifelonglab/M2I_I2M_benchmark} \\

\item \textbf{Authors' contributions} -- Kamil Faber: \textit{Data Curation, Investigation, Software, Visualization, Writing} -- Dominik Zurek: \textit{Data Curation, Investigation, Resources, Software, Writing (Review \& Editing)} -- Marcin Pietron, Nathalie Japkowicz: \textit{Resources, Validation, Writing (Review \& Editing)} -- Antonio Vergari, Roberto Corizzo: \textit{Conceptualization, Supervision, Methodology, Project Administration, Writing} \\
\end{itemize}

\noindent


% %%===================================================%%
% %% For presentation purpose, we have included        %%
% %% \bigskip command. please ignore this.             %%
% %%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}

% \begin{appendices}
% \section{Section title of first appendix}\label{secA1}

% An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

% \end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

%\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%% Default %%
%%\input sn-sample-bib.tex%

%\clearpage

%\bibliographystyle{sn-mathphys}
%\bibliographystyle{plain}
\bibliographystyle{sn-mathphys}
\bibliography{referomnia}
% \bibliography{sn-bibliography}

\end{document}
