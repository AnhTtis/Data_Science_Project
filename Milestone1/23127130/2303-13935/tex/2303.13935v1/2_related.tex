% !TEX root = root.tex

\section{Related work}
\label{sec:2_related}
The compositional agents in our framework build upon the two transfer learning 
frameworks: VC and SF-GPI. \textbf{VC}-based methods mainly came from the 
maximum entropy framework\cite{ziebart2010modeling}. By encouraging policy 
randomness, the resulting policy is usually more robust against the noise 
\cite{fox2015taming} and exhibits better exploratory 
behavior\cite{haarnoja2017reinforcement}. The value composition methods often 
exploit the property that the optimal value function of conjunction tasks can be 
well approximated by the average of the constituent value functions 
\cite{haarnoja2018composable, hunt2019composing}. Interestingly, boolean 
operators can further expand the valid compositions in the discrete setting and 
achieve a super-exponential amount of task 
transfer\cite{van2019composing,nangue2020boolean}. However, none of these 
approaches consider concurrent composition scenarios. 

On the other hand, \textbf{SF-GPI}\cite{barreto2020fast} applies successor 
features \cite{dayan1993improving} to estimate a set of Q-functions. Maximizing 
these Q-functions results in the policy improvement. The results in 
\cite{barreto2020fast} indicate that training several primitives simultaneously 
can significantly improve the sample efficiency more than training individually. 
Our work can be seen as an extension to SF-GPI by bringing valid operations from 
VC to successor feature-enabled concurrent learning. 
