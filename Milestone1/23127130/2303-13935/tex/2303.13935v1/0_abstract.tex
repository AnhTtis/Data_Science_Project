%% LyX 2.3.6.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\special{papersize=\the\paperwidth,\the\paperheight}

\makeatother
% \begin{abstract}
% The deep reinforcement learning (DRL) frameworks has shown its capability in solving high-dimensional robotics continuous control tasks. However, due to the lack of sample efficiency, applying RL for online learning in the robotics domain is still practically infeasible. One reason is that RL agents do not leverage the solution of previous tasks on the new tasks. Recent work has developed multi-tasking RL agents based on successor features and proven to be quite promising to increase sample efficiency further. In this work, we unify two prior multi-task RL frameworks, SF-GPI and value composition, to the continuous control domain and manually construct features better suited for the robot control problem. To further increase sample efficiency, we exploit compositional properties of the successor feature to compose the policy distribution without training any new policy. Lastly, to understand the multi-tasking mechanism, we have prepared a new benchmark multi-task continuous control environment based on Raisim, which facilitates large-scale parallelization to accelerate the experiments. We show that this novel approach to defining feature space and policy composition can construct a task-agnostic agent and improve the sample efficiency compared to direct RL methods. XXX add link to github
% \end{abstract}

\begin{abstract}
Deep reinforcement learning (DRL) frameworks are increasingly used to
solve high-dimensional continuous-control tasks in robotics. However, due to 
the lack of sample efficiency, applying DRL for online learning is still 
practically infeasible in the robotics domain. One reason is that DRL agents do 
not leverage the solution of previous tasks for new tasks. Recent work 
on multi-tasking DRL agents based on successor features has proven to be quite 
promising in increasing sample efficiency. In this work, we present a new 
approach that unifies two prior multi-task RL frameworks, SF-GPI and value 
composition, for the continuous control domain. 
We exploit compositional properties of successor features to compose a policy distribution from a set of primitives without training any new policy. Lastly, to demonstrate the multi-tasking mechanism, we present a 
new benchmark for multi-task continuous control environment based on Raisim. 
This also facilitates large-scale parallelization to accelerate the 
experiments. Our experimental results in the Pointmass environment show that our 
multi-task agent has single task performance on par with soft actor critic (SAC) 
and the agent can successfully transfer to new unseen tasks where SAC fails. We 
provide our code as open-source at 
\url{https://github.com/robot-perception-group/concurrent_composition} for the 
benefit of the community.
\end{abstract}



%\begin{IEEEkeywords}
%AI-Enabled Robotics, Reinforcement Learning;
%\end{IEEEkeywords}


