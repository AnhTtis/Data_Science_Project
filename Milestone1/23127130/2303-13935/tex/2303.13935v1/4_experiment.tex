% !TEX root = root.tex

\section{Experiments and Results}
\label{sec:4_experiment}
The experiment aims to understand the transfer of the primitives. Then we test if the multi-task RL agents generated by the proposed framework, i.e., SF-GPI, MSF, DAC, DAC-GPI, can be task-agnostic and transfer to any unseen task and whether they can leverage knowledge transfer and speed up learning. We perform all experiments on a single computer (AMD Ryzen Threadripper 3960X, 24x 3.8GHz, NVIDIA GeForce RTX2080 Ti, 11GB). Agents are implemented based on Pytorch\cite{paszke2019pytorch}. The hyper-parameter tuning tools are provided by Wandb\cite{wandb}.


\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.35\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{./img/pi_connect}  
		%\includesvg[width=0.8\textwidth]{./img/pi_connect.svg}  
		\caption{primitive network.}
		\label{fig:pi_connect}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.35\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{./img/sf_connect}
		%\includesvg[width=0.95\textwidth]{./img/sf_connect.svg}  
		\caption{successor feature network.}
		\label{fig:sf_connect}
	\end{subfigure}
	\caption{network architecture. } 
	\label{fig:net_architecture}
\end{figure}

\begin{figure}[b]
	\centering
	\begin{subfigure}[t]{0.23\textwidth}
		\includegraphics[width=1.0\textwidth]{./img/primitives}
		%	\includesvg[width=0.5\textwidth]{./img/primitives.svg}
		\caption{The first row corresponds to the two action components from the primitive $\pi_x=(a_x,a_y)$ and the second row for $\pi_y$ in the 2D plane. The color bar represents the action magnitude between $[-1,1]$. }
		\label{fig:primitives}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{./img/kappa}
		%	\includesvg[width=0.5\textwidth]{./img/kappa.svg}
		\caption{The first row corresponds to the two components from kappa $\kappa^{\pi_x}=(\kappa^{\pi_x}_x,\kappa^{\pi_x}_y)$ and the second row for $\kappa^{\pi_y}$ in the 2D plane. The magnitude $[0,1]$ is represented by the color bar.}
		\label{fig:kappa}
	\end{subfigure}
	\caption{Composition visualization of Pointmass2D-Simple}
	\label{fig:pm2_visual}
\end{figure}

\begin{figure*}
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{./img/sfgpi_feature_set}
		%	\includesvg[width=0.5\textwidth]{./img/sfgpi_feature_set.svg}
		\caption{The SF-GPI agent with different feature sets. The higher slope of the learning curve implies a faster learning speed.} 
		\label{fig:sfgpi_feature_set}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{./img/sac_fail}
		%	\includesvg[width=0.5\textwidth]{./img/sac_fail.svg}
		\caption{ When the evaluation task is different from the training one, the baseline SAC\cite{haarnoja2018soft} fails to generalize to the new task while multi-task agents are less affected.} 
		\label{fig:sac_fail}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{./img/hyperparam_tuning}
		%	\includesvg[width=0.5\textwidth]{./img/hyperparam_tuning.svg}
		\caption{The hyper-parameter tuning is conducted with the composite task $w^{navigation}=( \mathbf{1}, 0.5, 1) $ and $w^{hover}=( \mathbf{1}, 1,1)$ and during evaluation, the success weight is increased to 100 for observing the transfer ability. Here we show the best five sets of hyper-parameters.} 
		\label{fig:hyperparam_tuning}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{./img/sample_efficiency}
		%	\includesvg[width=0.5\textwidth]{./img/sample_efficiency.svg}
		\caption{We compare composition agents with the baseline in the single task $w=(\mathbf{1}, 0.5, 1)$. Baselines SAC\cite{haarnoja2018soft} is the SOTA maximum entropy RL agent while SQL\cite{haarnoja2017reinforcement} applies an energy-based training method to achieve expressive multi-modal policy.} 
		\label{fig:sample_efficiency}
	\end{subfigure}
	\caption{Every curve is represented by the mean with two standard deviations of 5 experiments run by the best agent from the hyper-parameter tuning. The hyper-parameter search is conducted by bayesian optimization. We perform in total a hundred searches for every agent.  The ablation study indicates that dropout layer can hurt the performance while the effect of other techniques remain unclear, including layernorm, entropy tuning, importance sampling, activation function, etc. Since DAC-GPI behaves fairly similarly to DAC and MSF performs relatively poorly, we neglect them for some experiments.  }
\end{figure*}


\subsection{Network Architecture}
We represent primitives and successor features by two neural networks as displayed in Fig.~\ref{fig:net_architecture}. Since the layers or representations are shared, the knowledge transfer can happen within the network. To reduce over-estimation in the SFs and enable higher update-to-data ratio (UTD ratio) \cite{hiraoka2021dropout}, we include \textit{dropout} and \textit{layernorm} to the SF network. Since \textit{ReLU} activation can result in negative transfer \cite{wang2022investigating}, we incorporate \textit{SeLU}\cite{klambauer2017self} instead. To further mitigate the positive bias \cite{fujimoto2018addressing}, we apply the minimum double critic technique for the SF estimation, i.e., $\hat{\psi}^{\pi} = min(\psi_{\theta_1}^{\pi}, \psi_{\theta_2}^{\pi})$.


\subsection{Environment and Tasks}
We build our training environment in the Raisim\cite{raisim} simulator, which allows massive vectorization for the environment to collect samples in parallel. Even though Raisim has several built-in robots, to address our problem, we introduce an abstract omnidirectional robot, Pointmass. 
%We next introduce this environment, features $\phi \in \mathbb{R}^d$ and task weight $w\in \mathbb{R}^d$.


\subsubsection{Pointmass} 
We first introduce the state and action space of the environment, 
\begin{itemize}
	\item state space  $\mathcal{S}$: $\Delta{\mathbb{X}}, \Delta{\mathbb{V}}, \Delta{\Theta}, \Delta{\Omega} \in \mathbb{R}^3$, are relative position, velocity, orientation, and angular velocity between the robot and goal respectively.
	\item action space $\mathcal{A}$:  $(a_x, a_y, a_z)$, are the applied force on x-, y-, and z-axis of the Pointmass respectively.
\end{itemize}

Features, or sub-tasks, can be defined arbitrarily as long as they can be derived from the state space. We pre-define three sets of features for our experiments, as shown in Table.~\ref{tab:pm_features}. The default will be \textit{Regular} if not explicitly mentioned.  

\begin{table}[h]
	\centering
	\resizebox{0.4\textwidth}{!}{%		
		\begin{tabular}{ | p{1.5cm} || p{7cm} | }
			\hline
			Regular       & $(-|\Delta{\mathbb{X}}|, -||\Delta{\mathbb{V}}||_2,  Success)$  \\
			\hline
			Simple    & $(-|\Delta{\mathbb{X}}|)$  \\
			\hline
			Augment & $(-|\Delta{\mathbb{X}}|, -||\Delta{\mathbb{X}}||_2, -|\Delta{\mathbb{V}}|, -||\Delta{\mathbb{V}}||_2,  Success)$  \\
			\hline
	\end{tabular}}
	
	\caption{Feature sets for Pointmass environment. $|\cdot|$ is the absolute value whereas $||\cdot||_2$ is the L2 norm. $Success$ is a binary feature triggered when relative distance is less than 1 meter, i.e., $Success=1$ if $||\mathbb{X}||_2 <1$ else $0$. \label{tab:pm_features} }
\end{table}


The \textit{Regular} feature set represents a standard way to define reward functions in robotics,  \textit{Simple} is simplified for the demonstration purpose, and \textit{Augment} includes the redundant features, and each generates an extra redundant primitive,  e.g., both $-|\Delta{\mathbb{X}}|$ and $-||\Delta{\mathbb{X}}||_2$ can achieve position control. The goal of each primitive is to maximize its corresponding feature or sub-task. The effect of features is displayed in Fig.~\ref{fig:sfgpi_feature_set}. By adding more features, we obtain more primitives with diverse behavior and increase the learning speed (the slope of the curve).

Though the overall environment is defined in 3D space, it can be reduced to a lower dimension by reducing the action space, e.g., Pointmass2D has the action space $\mathcal{A}=(a_x, a_y)$, and in this case, the primitive associated with altitude feature is redundant.


\subsubsection{Composite Task}
We introduce a composite task, which includes two tasks during training, \textit{navigation} and \textit{hover} task. The navigation task aims to bring the point mass closer to the goal position, while the hover task requires the robot's velocity to approach zero when near the goal. The task switches from navigation to hover when the distance to the goal is less than five meters. We can formulate this accordingly,  

\begin{equation}
	\begin{array}{l}
		w=\begin{cases}
			w^{navigation} & \text{if \ensuremath{d(s_{pointmass},s_{target})\geq 5m} }\\
			w^{hover} & \text{otherwise}
		\end{cases}
		
	\end{array}\label{eqn:pm_weight}
\end{equation}

where  $w^{navigation}=( \mathbf{1}, 0.5, 1) $ and $w^{hover}=( \mathbf{1}, 20,1)$ and bold symbols denote the vector of the same length as the corresponding feature.
%i.e., the first $\mathbf{1}=(1,1,1)$ will perform the dot product with features $-|\Delta{\mathbb{X}}|=(-|\Delta{x}|, -|\Delta{y}|, -|\Delta{z}|)$. 
Because the velocity penalty is significant when the agent is within the hovering range, the agent will stop immediately for both multi-tasking and standard agents. Now, to test the agents' transfer capability, in evaluation, we change the composite task to emphasize the success reward, i.e., $w^{navigation}_{eval}=( \mathbf{1}, 0.5, 10000) $ and $w^{hover}_{eval}=( \mathbf{1}, 20,10000)$. We can demonstrate in Fig.~\ref{fig:sac_fail} that the multi-task agent is less affected when the evaluation task changes. 
%Note that the reason for the different scales of the features in task weight is related to the frequency and magnitude of acquiring the reward. Since the success reward is relatively sparse and the magnitude is less significant than other features, its successor feature will have much less magnitude, and considerable weight is used for compensation.


\subsection{Training Stability and Composition Noise}
The composition noise stems from the action components redundant to the transfer task and it could undermine the training stability. In Pointmass2D-Simple, the primitives corresponding to the features,$-|\Delta{X}|, -|\Delta{Y}|$ produce the distributions $\pi_x(a|s) = ( \mathcal{N}^{\pi_x}_x, \epsilon)$ and $\pi_y= ( \epsilon, \mathcal{N}^{\pi_y}_y)$, where $\epsilon$ is the composition noise of unknown distribution (Fig.~\ref{fig:primitives}). With MSF, the composite contains the noise in both action components, $\pi^{MSF}=  ( \mathcal{N}^{\pi_x}_x\epsilon, \mathcal{N}^{\pi_y}_y\epsilon)$, whereas SF-GPI contains noise in one of the action components $\pi^{GPI}=( \mathcal{N}^{\pi_x}_x\epsilon, \mathcal{N}^{\pi_y}_y)$ or $( \mathcal{N}^{\pi_x}_x, \mathcal{N}^{\pi_y}_y\epsilon)$. Unlike MSF, GPI can utilize critics to select the less noisy one. 

Ultimately, only DAC and DAC-GPI can attenuate the noise in both action components, $\pi^{DAC}= ( (\mathcal{N}^{\pi_x}_x) ^{\kappa^{\pi_x}_x} (\epsilon)^{\kappa^{\pi_y}_x} , (\mathcal{N}^{\pi_y}_y) ^{\kappa^{\pi_x}_y} (\epsilon)^{\kappa^{\pi_y}_y} ) $, when the critics reduce $\kappa$ of the noisy term (Fig.~\ref{fig:kappa} displays $\kappa$).
Fig.~\ref{fig:hyperparam_tuning} reflects that the ability to filter out the composition noise is critical to learning speed. DAC-GPI has the best convergence speed, followed by DAC and then SF-GPI. We found the MSF composition unstable due to the composition noise. 

Lastly, compared to the maximum entropy RL baselines, i.e., SAC, SQL, in the single task setting (Fig.~\ref{fig:sample_efficiency}), the multi-task agents show a similar convergence property as SAC. 
%This is rather disappointing as we expect the knowledge transfer between primitives to increase the sample efficiency further. 




