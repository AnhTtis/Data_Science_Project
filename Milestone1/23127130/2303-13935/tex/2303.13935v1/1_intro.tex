% !TEX root = root.tex

\section{Introduction}
\label{sec:Introduction}

One approach to achieve optimal continuous control in robotics is through 
reinforcement learning-based methods (RL) \cite{kober2013reinforcement}. 
However, RL agents are usually optimized for one specific task and require more 
training if the reward definition changes. On the other hand, multi-task RL 
frameworks aim to design an RL agent that allows recycling old policies for 
other tasks to achieve higher sample efficiency\cite{taylor2009transfer}. 

% define problem better
One way to achieve multi-task RL is through transfer learning by manipulating 
the policies trained on the old tasks. In this work, we focus on the transfer 
learning methods that define tasks using linearly decomposable reward 
functions, commonly used to define many robot control tasks \cite{haarnoja2018learning}.


\begin{figure}[t!]
	\centering
	\includegraphics[width=0.4\textwidth]{./img/illustration}	
	%\includesvg[width=0.5\textwidth]{./img/illustration.svg}
	\caption{In concurrent composition, policy extraction is too expensive to perform in every time step. Instead, we propose composing the primitives directly in the run time.}
	\label{fig:illustration}
\end{figure}

Currently, there are two major transfer learning frameworks following the 
aforementioned assumption. The first framework, using \textit{successor 
features} and \textit{generalized policy improvement} (SF-GPI, 
\cite{barreto2020fast, barreto2019option}), derives the new policy by choosing 
the sub-policies, or primitives, with the largest associated action-value. The 
other framework directly composes the action-value of different tasks, which we 
refer to as \textit{value composition} (VC \cite{haarnoja2018composable, 
hunt2019composing, van2019composing, nangue2020boolean}). This framework first 
defines the composite value functions of new tasks by linearly combining the 
constituent action-value functions and then extracts the policy. 

Different from the \textit{Option} framework\cite{sutton1999between}, which is 
an another approach to multi-task RL that combines the primitives in temporal 
order, in our novel approach we focus on concurrent policy composition by 
composing the primitives at every time step. The samples collected by the 
composite policy expose each primitive to a higher quality data and thus enable 
the knowledge transfer and increase the sample efficiency 
\cite{barreto2020fast}. Policy extraction is prohibitive since it requires 
updating policies multiple times. We consider real-time performance as critical. 
Therefore, we must adapt VC-based methods for online composition. We do so by 
multiplicative policy composition (MCP \cite{peng2019mcp}). We show that this 
can be achieved via the successor features framework, where SF-GPI becomes one 
special case.  


In summary our contributions include the following.
\begin{itemize}
	\item Derivation of the relation between value composition and policy 
composition.
	\item A novel method unifying the GPI and VC under the 
successor feature-based online concurrent composition framework.
	\item Introduction of a new benchmark environment based on Raisim for 
multi-task reinforcement learning.
\end{itemize}



