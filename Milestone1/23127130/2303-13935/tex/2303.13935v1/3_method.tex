% !TEX root = root.tex
%


\section{Background}
\label{sec:Background}

\subsection{Multi-Task Reinforcement Learning}
RL improves a control policy through interaction with the environment, and it formulates this procedure by \textit{Markov decision processes} (MDPs), $\mathcal{M}\equiv<\mathcal{S},\mathcal{A},p,r,\gamma>$. $\mathcal{S}$ and $\mathcal{A}$ define the state space and action space respectively. The dynamics $p(s_{t+1}|s_t,a_t)$ defines the transition probability from one state to another. Reward function $r(s_t, a_t, s_{t+1})$ governs the desired agent behavior. The discount $\gamma \in [0,1)$ determines the valid time horizon of the task. The goal is to find the optimal control policy $\pi(a|s): S\rightarrow A$, such that the expected discounted return, $J(\pi)=\mathbb{E}_{\pi, \mathcal{M}}[\sum_{\tau=t}^{\infty} \gamma^{\tau-t}r_\tau]$, can be maximized. 

We consider transfer within a set of MDPs $\mathcal{M}_w\in\mathcal{T}$ where  $\mathcal{M}_w \equiv<\mathcal{S},\mathcal{A},p,r_w,\gamma>$ , and each has its unique reward function, $r_w(s_t, a_t, s_{t+1})=\phi(s_t, a_t, s_{t+1})^\top \cdot w$, which is a linear combination of a set of commonly shared $d$ features, $\phi(s_t, a_t,s_{t+1})\in \mathbb{R}^d$ and the task weight $w\in \mathbb{R}^d$. The goal is to find a set of primitives that can solve the subset of tasks $\mathcal{T}' \in \mathcal{T}$ such that we can recover the optimal policies from the unseen tasks by the found primitives. 


\subsection{Maximum Entropy RL}
The maximum RL framework includes a bonus policy entropy term in the expected reward function. The goal is to find an optimal control policy that not only maximizes the reward but also encourages the policy randomness, 
\begin{equation}
	J(\pi)=\mathbb{E}_{\pi, \mathcal{M}_w}\left[ \sum_{\tau=t}^{\infty} \gamma^{\tau-t}(r_{w,\tau}+\alpha \mathcal{H}(\pi(\cdot|s_{\tau})))\right], 
\end{equation}
where $\alpha$ is a temperature scaler controlling the entropy of the policy and $\mathcal{H}(\cdot)$ is an entropy estimator. Prior works have found that the bonus term can encourage exploration \cite{haarnoja2018soft}, increase the training policy's robustness against noise \cite{fox2015taming}, and preserve the policy multimodality \cite{haarnoja2017reinforcement}. 

One tractable way to achieve maximum entropy RL is through temporal difference learning by boostrapping the value estimation. Here, we define the action-value function, or Q-function, of policy $\pi$ on task $w$ as:
\begin{equation}
	Q^{\pi}_{w}(s_t,a_t)\equiv r_{w,t}+\mathbb{E}_{\pi}\left[\sum_{\tau=t+1}^{\infty} \gamma^{\tau-t}(r_{w,\tau}+\alpha \mathcal{H}(\pi(\cdot|s_{\tau})))\right].
\end{equation}
Note when $\alpha=0$, we recover the standard RL Q-function. The Q-function can be perceived as a performance metric of its policy and the estimation quality can be improved recursively through the \textit{soft policy iteration}.

\subsection{Soft Policy Iteration}
The soft policy iteration can be defined in two steps: (1) \textit{soft policy evaluation} (2) \textit{soft policy improvement} \cite{haarnoja2017reinforcement}.  The soft policy evaluation step allows to improve the quality of Q-value estimation and it can be formulated as follows, 
\begin{align}
	Q^{\pi}_{w}(s_t,a_t) & = r_w(s_t,a_t) + \gamma \mathbb{E}_{s_{t+1}\sim p}\left[ V(s_{t+1})\right] \label{Eqn:backup} \\
	V^{\pi}_{w}(s_t) &= \mathbb{E}_{a_t\sim\pi}\left[Q^{\pi}_{w}(s_t,a_t) - \log \pi(a_t|s_t)\right].
\end{align}

The policy distribution can be updated according to the soft policy improvement step:
\begin{equation}
	 \pi_{new}=\argmin_{\pi'}D_{KL}\left(\pi'(\cdot|s_t)||\frac{\exp(Q^{\pi_{old}}_{w}(s_t,\cdot))}{Z^{\pi_{old}}(s_t)}\right), \label{eqn:soft policy improvement}
\end{equation}
where $D_{KL}$ denotes the Kullback-Leiber divergence and the normalizing factor, $Z^{\pi_{old}}(s_t)$, can be neglected because it does not contribute to the gradient computation. Intuitively, the goal is to match the policy distribution to the Q-value estimation of different actions; therefore, the higher the Q-value, the higher the probability of selecting the action, i.e. 
\begin{equation}
	\pi(\cdot|s) \propto e^{Q^{\pi}(s,\cdot)}. 
\end{equation}

Alternating between the policy evaluation step and policy improvement step will eventually converge to the optimal Q-function $Q^*_w$ on task $w$ with its corresponding optimal policy $\pi^*$, where $Q^*_w \equiv Q^{\pi^*}_w$ \cite{haarnoja2017reinforcement}.


\subsection{Successor Feature (SF)}
Given that  $r_w(s_t, a_t, s_{t+1})=\boldsymbol{\phi}(s_t, a_t, s_{t+1})^\top \cdot w$, we can rewrite the Q-function in following form, 
\begin{align}
	Q^{\pi}_w(&s_t,a_t)=\mathbb{E}_{\pi}\left[\sum_{\tau=t}^{\infty} \gamma^{\tau-t}r_{w,\tau}\right] \\
	&=\mathbb{E}_{\pi}\left[\sum_{\tau=t}^{\infty} \gamma^{\tau-t}\boldsymbol{\phi}^\top_t \right]\cdot w 
	=\boldsymbol{\psi}^{\pi}(s_t,a_t)^\top \cdot w \label{eqn:sf.w}
\end{align}
where $\boldsymbol{\psi}^{\pi}\in \mathbb{R}^d$ is the \textit{successor 
feature} of policy $\pi$\cite{dayan1993improving}. To approximate it, we can utilize function 
approximators and apply soft value iteration by replacing the reward with 
features in analogous to (\ref{Eqn:backup}). We introduce the successor 
features to the maximum entropy framework by including the entropy term. As a 
result, we obtain the policy iteration for the successor features,
\begin{align}
	\boldsymbol{\psi}^{\pi}(s_t,a_t) &= \boldsymbol{\phi}_t + \mathbb{E}_{\pi} \left[ \sum_{\tau=t}^{\infty} \gamma^{\tau-t} (\boldsymbol{\phi}_{\tau}  +\alpha \mathcal{H}(\pi(\cdot|s_{\tau}) ) \right] \\
	&= \boldsymbol{\phi}_t + \gamma \mathbb{E}_{s_{t+1}\sim p(\cdot|s_t,a_t)} \left[ \boldsymbol{\Upsilon}(s_{t+1}) \right], \label{eqn:sf_evaluation}\\
	\boldsymbol{\Upsilon}^{\pi}(s_t) &= \mathbb{E}_{a_t\sim\pi}\left[\boldsymbol{\psi}^{\pi}(s_t,a_t) - \log \pi(a_t|s_t)\right]. \label{eqn:sfv_evaluation}
\end{align}

The policy improvement step is similar to ($\ref{eqn:soft policy improvement}$) by replacing the Q-function with the approximation from the successor features and the desired primitive task weight, which is defined in Sec.~\ref{sec:Construct Policy Primitives}.

\subsection{Generalized Policy Improvement Composition (GPI)}
Given policies $\Pi=\{\pi_1, \pi_2, ..., \pi_n\}$ and their corresponding Q-functions in a new task $w$, $Q^{\Pi}_w=\{Q^{\pi_1}_w, Q^{\pi_2}_w, ..., Q^{\pi_n}_w\}$ .  If a new policy $\pi^{GPI}$ is constructed in the following way, 
\begin{equation}
	\pi^{GPI}(s) \in \argmax_{a \in \mathcal{A}} \max_{\pi \in \Pi} Q^{\pi}_{w}(s,a)   ,       
\end{equation}
then the GPI theorem \cite{barreto2020fast} states that the new policy $\pi^{GPI}$ is no worse than all other policies since its Q-function is always greater than its members, i.e. $	Q^{\pi^{GPI}}_w(s,a) \geq \sup_{\pi \in \Pi} Q^{\pi}_w(s,a).$


This composition is not the optimal composition and the composite value is an under-estimate of the true value $Q^{GPI}_w=  \max_{\pi \in \Pi} Q^{\pi}_w(s,a)\leq Q^*_w(s,a)$. And subsequently, the composite policy performance might not be optimal $Q^{\pi^{GPI}}_w \leq Q^{\pi^*}_w $ \cite{hunt2019composing}.

Successor features provide one quick and convenient way to evaluate performance of a policy $\pi$ in any task $w$. Therefore, SF-GPI algorithm can evaluate all policies' performance on the new task $w$ instantaneously by the SFs, called \textit{generalized policy evaluation}, or GPE. Then GPI can be applied to select the action that leads to the highest value. 
%GPI composition does not assume the constituent to be optimal and allows online composition during training. 

Note the $\argmax_{a\in \mathcal{A}}$ expression is not tractable in continuous action space and we instead sample actions from the primitives, i.e. $\argmax_{a \sim \Pi}$. We describe our implementation of the continuous SF-GPI in Sec.~\ref{sec:GPI Composition}.


\subsection{Value Composition}
\label{sec:ValueComposition}
Combining the Q-functions allows to obtain Q-function of the new task without solving them at all. Prior works have defined a set of valid operations in multi-goal discrete control \cite{van2019composing, nangue2020boolean}. In continuous control, however, we focus on the linear task combinations which do not assume binary task weights. That is, if a new task is a linear combination of old tasks, $r_{new} = a\cdot r_a+b\cdot r_b$, then the Q-value of the new task can be approximated by $\hat{Q}^{\sum}_{new}\approx a\cdot Q^*_a + b\cdot Q^*_b$.  The new policy can be extracted from the composite Q-function. However, since policy extraction would require several policy improvement steps, it is intractable during interaction with the environment. 

Note that the composition is an overestimate of the true optimal value function, i.e. $\hat{Q}^{\sum}_{new} \geq Q^{*}_{new}$. Since it assume all constituent tasks can be achieved at the same time across the whole state space \cite{hunt2019composing, haarnoja2018composable}.  When this is not the case, the composite policy's is not optimal, i.e. $Q^{\pi^{\sum}}_{new} \leq Q^{\pi^*}_{new} $. 

\subsection{Multiplicative Compositional Policy (MCP)}
MCP  \cite{peng2019mcp}  has shown to compose the policy primitive in the 
immitation learning scenario. Here, we introduce it to concurrent reinforcement 
learning.  Given a set of $d$ Gaussian primitives $\Pi \in 
\mathcal{N}^{dim(\mathcal{A})\times d}$ and a gating vector $g \in [0,1]^d$ 
where $||g||_1=1$, the MCP function $MCP(\Pi, g): 
\mathcal{N}^{dim(\mathcal{A})\times d} \times \mathbb{R}^d\rightarrow 
\mathcal{N}^{dim(\mathcal{A})\times 1}$ is defined as follows,
\begin{equation}
  \pi(a|s) =MCP(\Pi, g) \equiv \frac{1}{Z(s)}\prod_{i=0}^{d} \pi_i(a|s)^{g_i}.
\end{equation}

where $Z(s)$ is a partition function to normalize the distribution.  If all the primitives are Gaussian, then the composite is also a Gaussian. 
The author did not explain why this composition is valid, but we show that this is a natural outcome from value composition in the policy space  (Sec.~\ref{sec:Approximate True Policy by Composing Primitives}). For convenience, we refer to the MCP-based compositional method as \textit{compositional policy improvement}, or CPI, in contrast to GPI.

\section{Methodology}
\label{sec:Methodology}
Our novel approach is described in the following sub-sections.
\subsection{Successor Feature-based Composition}
\label{sec:Successor Feature-based Composition}
In contract to the GPI composition (Sec.~\ref{sec:GPI Composition}), we introduce two more SF-based compositions.

\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}

\subsubsection{Successor Feature based Value Composition (SFV)}
First we introduce the linear value composition (Sec.~\ref{sec:ValueComposition}) to the successor feature framework.  Given the task $r=\boldsymbol{\phi}^\top \cdot w$, we can treat features $\boldsymbol{\phi}$ as sub-tasks. From this perspective, $\boldsymbol{\phi}$ becomes the constituent sub-tasks and $r$ is the composite task. Thus, the successor features then represent the policy's performance on each sub-task. Therefore, (\ref{eqn:sf.w}) can be seen as a special case for value composition of the same policy.

Since we do not know the task policy $\pi$, we can only approximate it by the existing primitives $\boldsymbol{\Pi}=[\pi_0, ...,\pi_n]$. Value composition theorems state that the average of the Q-function is a sufficiently good approximation of the true Q-function \cite{haarnoja2018composable,van2019composing, nangue2020boolean}.

\begin{align}
	Q^{\pi}_w(s,a) \approx Q^{{\sum}}_w(s,a) =  \frac{1}{n} \sum_{i=0}^n Q_w^{\pi_i}(s,a)= \frac{\boldsymbol{w}^{\top}}{n} \sum_{i=0}^n \boldsymbol{\psi}^{\pi_i}(s,a)
	\label{eqn:SFV value}
\end{align}

From this expression, one can see that SFV composition can only achieve an average performance in all sub-tasks. Consider optimal SF, since SFV assumes all tasks can be achieved at the same time, this is also an overestimate of the true value function, i.e. $\hat{Q}^{SFV}_w \equiv \hat{Q}^{\sum}_w\geq Q^*_w$.  We can define the greedy policy extracted by the following composition.
\begin{equation}
		\pi^{SFV}(s) \in \argmax_{a \in \mathcal{A}} \frac{\boldsymbol{w}^{\top}}{n} \sum_{i=0}^n \boldsymbol{\psi}^{\pi_i}(s,a).     
\end{equation}
Note when the sub-tasks are not simultaneously achievable, this policy can only achieve a sub-optimal performance $Q^{\pi^{SFV}}_w(s,a)\leq Q^{\pi^*}_w(s,a)$.


\subsubsection{Maximum Successor Feature Composition (MSF)}
If we can treat successor features as the sub-tasks, then it naturally raises the question: can we combine primitives in a certain way such that we achieve the current best achievable performance in all sub-tasks and subsequently any composite task? That is, given SFs of their corresponding $n$ primitives, $\Psi=\{\boldsymbol{\psi}^{\pi_0}, \boldsymbol{\psi}^{\pi_1},..., \boldsymbol{\psi}^{\pi_n}\}$, where $\boldsymbol{\psi}^{\pi_i}=\{\psi_0^{\pi_i}, \psi_1^{\pi_i}, ...\psi_d^{\pi_i} \}$. Then the desired composition operator $\mathcal{U}$ can be defined as $\mathcal{U}\psi^{\pi}\equiv \{\max_{\pi \in \Pi}{\psi_0^{\pi}},...,\max_{\pi \in \Pi}{\psi_d^{\pi}} \}$. With this, we define a composition rule, 
\begin{equation}
	\pi^{MSF}(s) \in \argmax_{a \in \mathcal{A}} w^\top\mathcal{U} \psi^{\pi}(s,a).   \label{eqn:MSF value}    
\end{equation}

Considering optimal SF, then this composition will have the largest over-estimation than any other compositions, i.e.
\begin{equation}
	\hat{Q}^{MSF}_w \geq \hat{Q}^{SFV}_{w} \geq Q^{*}_{w}  \geq Q^{GPI}_w .
\end{equation}

\subsection{Construct Policy Primitives}
\label{sec:Construct Policy Primitives}
We construct policy primitives based on the sub-tasks and each primitive is only responsible for its own sub-task. Therefore, we have the same number of sub-tasks and primitives (i.e. $d=n$). All primitives are trained by soft policy iterations and follow its convergence guarantee, i.e.
\begin{align}
	 \pi_i(\cdot|s) \propto e^{Q^{\pi_i}_{\mathbf{e}_i}(s,\cdot)}=e^{\mathbf{e}_i^{\top}\cdot\psi^{\pi_i}(s,\cdot)}.  \label{eqn:base_tasks}
\end{align}

The base tasks $\mathbf{e}_i \in \mathbb{R}^d$ can be constructed arbitrarily, as long as the base task and the transfer task share the same sub-tasks. For example, the base task defined as the zero vector will fail since this task has no overlapping with the transfer task.  For simplicity, we define it as a zero vector with the i-th component equal to 1.  Our goal is to compose these primitives such that the true task policy can be recovered.  

\subsection{Approximate True Policy by Composing Primitives}
\label{sec:Approximate True Policy by Composing  Primitives}
This is the core part of our method. We provide analytical view on how the task policy can be reconstructed by a set of task primitives given their relation in value space introduced in Sec.~\ref{sec:Successor Feature-based Composition}.



\subsubsection{MSF Composition}
\label{sec:MSF Composition}
Following MSF in value space (\ref{eqn:MSF value}) we derive the correspondent policy composition,
\begin{align}
	\pi(\cdot|s) &\propto e^{Q^{\pi}_w(s,\cdot)}=e^{w^{\top}\cdot \boldsymbol{\psi}^{\pi}(s,\cdot)} 
%	= e^{w^{\top}\cdot [ \psi^{\pi}_0,...,\psi^{\pi}_d ] (s,\cdot)} \\
	\approx   e^{w^{\top}\cdot \mathcal{U} \psi^{\pi}(s,a) } \label{eqn:MSF approx}\\
	&= e^{w^{\top}\cdot [ \psi^{\pi_0}_0, \psi^{\pi_1}_1,...\psi^{\pi_d}_d ] (s,\cdot)} 
	\propto \prod_{i=0}^{d} \pi_i(\cdot|s)^{w_i}. \label{eqn:MSF Composition}
\end{align}
The approximation in (\ref{eqn:MSF approx}) assumes the maximum SF of each sub-task is most likely to be the SF of the primitive trained from the respective sub-task. Then the MSF composition can be achieved by MCP, i.e. $\hat{\pi}(\cdot|s) = MCP(\Pi, g^{MSF})$, where $g^{MSF} = \frac{w}{|w|}$  is the normalized gating vector.

\begin{algorithm}
	\label{alg:msf}
	\caption{MSF Composition}
	\begin{algorithmic}[1]
		\Function{MSF}{s, w}:
		\State $\hat{\pi}(\cdot|s) = MCP(\Pi(\cdot|s), \frac{w}{|w|})$ \Comment{CPI}
		\State $a\sim \hat{\pi}(\cdot|s)$ \Comment{sample from composite}
		\State \Return $a$
		\EndFunction		
	\end{algorithmic}
\end{algorithm}


\subsubsection{SFV Composition}
Following SFV in value space (\ref{eqn:SFV value}) we derive the correspondent policy composition,
\begin{align}
	&\pi(\cdot|s) \propto e^{Q^{\pi}_w(s,\cdot)}\approx e^{Q^{{\sum}}_w(s,\cdot)}  
%	= e^{\frac{1}{d} \sum_{i=0}^n Q^{\pi_i}_w(s,\cdot)}\\
%	&= \prod_{i=0}^{d} e^{\frac{1}{d} Q^{\pi_i}_w(s,\cdot)}  
	=  \prod_{i=0}^{d} e^{\frac{\boldsymbol{w}^{\top}}{d} \boldsymbol{\psi}^{\pi_i}(s,\cdot)} \\
%	=  \prod_{i=0}^{d} \prod_{j=0}^{d} e^{\frac{1}{d}  w_j \psi_j^{\pi_i}(s,\cdot)} \\
	&=  \prod_{i=0}^{d} e^{\frac{1}{d}  w_i \psi_i^{\pi_i}(s,\cdot)}  \prod_{i=0}^{d}  \prod_{j\neq i}^{d} e^{\frac{1}{d}  w_j \psi_j^{\pi_i}(s,\cdot)}   
	\propto \prod_{i=0}^{d} \pi_i(\cdot|s)^{\frac{w_i}{d}}\cdot \epsilon  
\end{align}
where $\epsilon$ is the artefactual noise. Because of the noise, the SFV composition is not a valid composition in policy space with the set of primitives introduced in Sec.~\ref{sec:Construct Policy Primitives}. 

\subsubsection{GPI Composition}
\label{sec:GPI Composition}
Though this is already a SF-based composition, we reformulate it for completeness.
\begin{align}
	\pi(\cdot|s) \propto e^{Q^{\pi}_w(s,\cdot)} &  \approx e^{\max_{\pi_i \in \Pi}Q^{\pi_i}_w(s,\cdot)}  \\
	& \propto \argmax_{\pi_i \in \Pi}Q^{\pi_i}_w(s,\cdot)
\end{align}
The approximation can be achieved by successor features, i.e. $\hat{\pi}(\cdot|s)=\argmax_{\pi_i \in \Pi}Q^{\pi_i}_w(s,\cdot)=\argmax_{\pi_i \in \Pi}w^\top\psi^{\pi_i}(s,\cdot)$. 
%Since GPI does not assume any relation between the primitives and tasks, it is more flexible and can use any number of primitives. 

\begin{algorithm}
	\label{alg:gpi}
	\caption{GPI Composition}
	\begin{algorithmic}[1]
		\Function{SF-GPI}{s, w}:
		\State $\boldsymbol{a}\sim \Pi(\cdot|s)$ \Comment{sample from primitives}
		\State $\hat{\boldsymbol{Q}}^{\pi}_w = \boldsymbol{\psi}^{\pi}(s,\boldsymbol{a})\cdot w$ \Comment{GPE}
		\State $a = \argmax_{a \sim \Pi} \hat{\boldsymbol{Q}}^{\pi}_w$ \Comment{GPI}
		\State \Return $a$
		\EndFunction		
	\end{algorithmic}
\end{algorithm}

At this point, we have constructed an analytic method to find the policy composition corresponding to the composition in value space. Note that if primitives are constructed with different base tasks $\mathbf{e}_i$, the composition in policy space will change accordingly.  In addition, the VC-based method will always end up with MCP composition in policy space, and therefore, they fall into the CPI category. Next. we heuristically extend the idea to \textit{direct action composition}, which allows composition happens at action component level.

\subsection{Multiplicative Compositional Action (MCA)}
Rewriting the Gaussian primitives $\pi_i \in \Pi$ as the following,
\begin{align}
	\pi_i(\cdot|s)=[\mathcal{N}^{\pi_i}_0, ...,\mathcal{N}^{\pi_i}_{\mathcal{A}}|s]
\end{align}
where $\mathcal{N}^{\pi_i}_j=\mathcal{N}^{\pi_i}(\mu_j,\sigma_j)$ is the shorthand for the j-th action component of $\pi_i$ . Then the MSF composition ($\ref{eqn:MSF Composition}$) has the following form,

\begin{align}
\hat{\pi}(\cdot|s) = MCP(\Pi, g)=[\prod_{i=0}^{d} {(\mathcal{N}^{\pi_i}_0)^{g_i}}, ...,  \prod_{i=0}^{d} {(\mathcal{N}^{\pi_i}_{\mathcal{A}})^{g_i}} ].
\end{align}

However, since all action components are coupled by the scale $g_i$, we cannot manipulate them differerntly. Alternatively, given $\kappa\in [0,1]^{dim(\mathcal{A})\times n}$ where $||\kappa||_1=1$, we define the direct action composition function $MCA(\Pi, \kappa): \mathcal{N}^{dim(\mathcal{A})\times n} \times \mathbb{R}^{dim(\mathcal{A})\times n}\rightarrow \mathcal{N}^{dim(\mathcal{A})\times 1}$:

\begin{align}
	\hat{\pi}(\cdot|s) = MCA(\Pi, \kappa) \equiv [\prod_{i=0}^{d} {(\mathcal{N}^{\pi_i}_0)^{\kappa^{\pi_i}_0}}, ...,  \prod_{i=0}^{d} {(\mathcal{N}^{\pi_i}_{\mathcal{A}})^{\kappa^{\pi_i}_{\mathcal{A}}}} ]. \label{eqn:mca}
\end{align}

We define $\kappa^{\pi_i}_j$  in such a way that action components with lower successor feature value will be filtered out and higher value to be emphasized, i.e.  $	\kappa^{\pi_i} \propto \mathcal{P}_{\mathcal{A}\times d} \cdot \boldsymbol{\psi}^{\pi_i}$, where $\mathcal{P}$ is an \textit{impact matrix} defining the mapping from successor feature to the action components. 

\subsection{Impact Matrix}
\label{sec:impact matrix}
We would like to know how each action component is related to each feature throughout the whole trajectory, that is, the successor feature. Then given some successor features $\boldsymbol{\psi}=\{\psi_0, \psi_1, ...\psi_d \}$, we can define the impact matrix as absolute value of a Jacobian of $\psi$ w.r.t action, i.e.

\begin{gather}
	\mathcal{P}\equiv |\nabla_a{\psi(s,a)}|=
	\begin{bmatrix}
		|\frac{\partial \psi_0}{\partial a_0}|  & ... & |\frac{\partial \psi_d}{\partial a_0} | \\
		\vdots & \ddots & \\
		|\frac{\partial \psi_0}{\partial a_{\mathcal{A}}}|  & ... & |\frac{\partial \psi_d}{\partial a_{\mathcal{A}}}|  
	\end{bmatrix}
	_{\mathcal{A} \times d}
\end{gather}

The major concern of this expression is the amount of computation it requires and accuracy of the derivative. Luckily, in practice, this estimation can be computed efficiently by vectorized computation via GPUs. Yet the accuracy depends mostly on the function smoothness.  Fortunately, the value estimation usually becomes smoother during training and increasing the robustness of the function approximator can create similar effect as well \cite{pauli2021training}. But still, the estimation can be very noisy in the early phase of training. We can attenuate the noise by averaging over all policies $\boldsymbol{\psi} \approx \frac{1}{n}\sum_n{\boldsymbol{\psi}^{\pi_n}}$. Therefore, the more primitives the less noisy it will become. To further alleviate the noise, we include the temporal dependency to the estimation by averaging with the impact matrix from the previous time step, i.e. $\hat{\mathcal{P}}\approx (\mathcal{P}_{t-1}+\mathcal{P}_{t})/2$.

\subsection{Direct Action Composition (DAC)}
With the impact matrix, we define $\kappa \equiv \mathcal{P}_{\mathcal{A}\times d} \cdot \Psi_{d \times n}\odot w_{d\times 1}$, where  
\begin{gather}
	\Psi=\{\boldsymbol{\psi}^{\pi_0}, \boldsymbol{\psi}^{\pi_1},..., \boldsymbol{\psi}^{\pi_n}\}=
	\begin{bmatrix}
		\psi_0^{\pi_0}  & ... & \psi_0^{\pi_n}  \\
		\vdots & \ddots & \\
		\psi_d^{\pi_0}  & ... & \psi_d^{\pi_n} 
	\end{bmatrix}
	_{d\times n},
\end{gather}
$\odot$ denotes the Hadamard product, and $n=d$. Intuitively, task weights $w$ first filter the task-relevant successor features and then map their impact to each action components by the impact matrix. In practice, we replace $\Psi$ with advantage $\Gamma$ for numerical stability, i.e. $\Gamma=\Psi - \frac{1}{n}\sum_n{\boldsymbol{\psi}^{\pi_n}}$. To make sure $||\kappa||_1=1$, $softmax$ is applied. Empirically, we found the performance much better when we filter out the advantage below zero, i.e. $\Gamma_{clip}=max(0,\Gamma)$. Finally we have 
\begin{align}
	\hat{\kappa}=softmax (\mathcal{P}\cdot \Gamma_{clip}\odot w ),
\end{align}
then we can achieve DAC by $MCA(\Pi, \hat{\kappa})$ as defined in (\ref{eqn:mca}).

\begin{algorithm}
	\label{alg:dac}
	\caption{DAC Compostion}
	\begin{algorithmic}[1]
		\Function{DAC}{s, w}:
		\State $\boldsymbol{a} \sim \Pi(\cdot|s)$ \Comment{sample from primitives}
		\State $\hat{\kappa}=softmax(\hat{\mathcal{P}}(s,\boldsymbol{a})\cdot \Gamma_{clip}(s,\boldsymbol{a}) \odot w  )$ 
		\State $\hat{\pi}(\cdot|s) = MCA(\Pi(\cdot|s), \hat{\kappa})$ \Comment{CPI}
		\State $a\sim \hat{\pi}(\cdot|s)$ \Comment{sample from composite}
		\State \Return $a$
		\EndFunction		
	\end{algorithmic}
\end{algorithm}


Lastly, we introduce another variant, DAC-GPI, by choosing the action component with the largest $\hat{\kappa}$, i.e. 
\begin{align}
	\hat{\pi}(\cdot|s)  \equiv [ \mathcal{N}^{ \argmax_{\pi \in \Pi} {\hat{\kappa}^{\pi}_0}}_0, ..., \mathcal{N}^{ \argmax_{\pi \in \Pi} {\hat{\kappa}^{\pi}_{\mathcal{A}}}}_{\mathcal{A}}]. 
\end{align}

\begin{algorithm}
	\label{alg:dacgpi}	
	\caption{DAC-GPI Compostion}
	\begin{algorithmic}[1]
		\Function{DAC-GPI}{s, w}:
		\State $\boldsymbol{a} \sim \Pi(\cdot|s)$ \Comment{sample from primitives}
		\State $\hat{\kappa}=softmax(\hat{\mathcal{P}}(s,\boldsymbol{a})\cdot \Gamma_{clip}(s,\boldsymbol{a}) \odot w  )$ 
		\State $a = [\argmax_{a_0} \hat{\kappa}_{0}^{\pi}, ..., \argmax_{a_{\mathcal{A}}} \hat{\kappa}_{\mathcal{A}}^{\pi} ]$ \Comment{GPI}
		\State \Return $a$
		\EndFunction		
	\end{algorithmic}
\end{algorithm}
Notice that when all the action components come from the same policy, DAC-GPI reduces to SF-GPI. Moreover, when each primitive shares the same exponent across all action components, DAC reduces to MSF. The major difference between DAC and DAC-GPI is that the former compose action components from all primitives according to the value while the latter only selects the best one. Therefore, we expect DAC to scale better with increasing amount of features. 

In summary, we have constructed a method to bridge the composition in value space to policy space. This provides an effective analytic method to generate different kinds of concurrent composition algorithms. We then show that the composition can not only happen in policy space but also in action space with the help of the impact matrix. Different from standard RL agents, the compositional agents usually involve critics or SFs to the action selection process and filter out the lower quality actions before delivery. 
%This is probably the main reason why it is capable of outperforming the single task counterpart.

\subsection{Training the Network}
% collect training data
The training data is gathered by direct interaction with the environment by the concurrent composition. Therefore, we can treat the concurrent compositional agent just as any regular RL agent and train in an end-to-end fashion without any pre-training. 

% training
In total, we have $n$ pairs of SFs and primitives with the $i$-th successor feature network $\psi^{\pi_i}$, target network $\bar{\psi}^{\pi_i}$, and the primitive network $\pi_i$ paramterized by $\theta_{\psi^{\pi_i}}$, $\theta_{\bar{\psi}^{\pi_i}}$, and $\theta_{\pi_i}$ respectively. The training scheme follows the soft policy iteration, which includes the evaluation step (\ref{eqn:sf_evaluation},\ref{eqn:sfv_evaluation}) and improvement step (\ref{eqn:soft policy improvement}). The evaluation step now evaluates the successor feature instead of Q-function while the policy improvement step stays the same by following the base task Q-function $Q^{\pi_i}_{\mathbf{e}_i}$.  Therefore, the loss for the i-th successor feature can be computed as follows,
\begin{align}
	J_{\boldsymbol{\psi}^{\pi_i}}(\theta_{\boldsymbol{\psi}^{\pi_i}}) &= \mathbb{E}_{(s_t,a_t)\sim\mathcal{D}}    \left[     ( \boldsymbol{\psi}^{\pi_i}(s_t,a_t) - \hat{\boldsymbol{\psi}}^{\pi_i}(s_t,a_t)  )^2	\right],
\end{align}
where $\mathcal{D}$ is the replay buffer and we have the TD-target $\hat{\boldsymbol{\psi}}^{\pi_i}$ approximated by the target network $\bar{\psi}^{\pi_i}$,
\begin{align}
		\hat{\boldsymbol{\psi}}^{\pi_i}(s_t,a_t)&=  \boldsymbol{\phi}_t + \gamma \mathbb{E}_{s_{t+1}\sim p(\cdot|s_t,a_t)} \left[ \boldsymbol{\Upsilon}^{\pi_i}(s_{t+1}) \right], \\
		\boldsymbol{\Upsilon}^{\pi_i}(s_t) &=   \mathbb{E}_{a_t \sim \pi_i}\left[  \boldsymbol{\bar{\psi}}^{\pi_i}(s_t,a_t)-\log \pi_i(a_t|s_t) \right].
\end{align}

Then the primitive can be improved by reducing the KL-divergence in (\ref{eqn:soft policy improvement}), which can be approximated by minimizing the loss,
\begin{align}
	J_{\pi_i}(\theta_{\pi_i})=\mathbb{E}_{s_t\sim \mathcal{D}, a_t \sim \pi_i} \left[ \log \pi_i( a_t|s_t ) -  Q^{\pi_i}_{\mathbf{e_i}}(s_t, a_t) \right],
\end{align}
with the primitive action value defined by (\ref{eqn:base_tasks}).
%\begin{align}
%	Q^{\pi_i}_{\mathbf{e_i}}(s_t, a_t) = \mathbf{e}_i\cdot \psi^{\pi_i}(s_t,a_t)
%\end{align}

% importance sampling
To reduce the effect of off-policyness, we apply retrace \cite{munos2016safe}, a variant of importance sampling with less variance, in SFs estimation. By assuming that each primitive has the best performance in its own sub-task, the correction can be performed conveniently by comparing the action distribution with other primitives instead of the behavior policy, which has complicated compositional distribution.

We summarize our novel approach by Algorithm.~\ref{alg:summary}. The method alternates between the interaction phase to collect samples by the compositional policy and learning phase that updates the function approximators' parameters by stochasitc gradient descend with samples collected previously. In the algorithm, looping over all primitives seems intimidating, but in practice we can vectorize the computation and update all primitives simultaneously in one pass. Therefore, the total amount of computation only scales linearly with the amount of primitives and the computation time will be the same with a GPU.


\begin{algorithm}
	\caption{Composition Agent}\label{alg:summary}
	\begin{algorithmic}[1]
		\Statex Choose $method\in$ MSF, SF-GPI, DAC, DAC-GPI
		\Statex Initialize network parameters $\boldsymbol{\theta_{\psi^{\pi}}}, \boldsymbol{\theta_{\bar{\psi}^{\pi}}}, \boldsymbol{\theta_{\pi}}$

		
		\For{N steps} 
			\If{exploration} $a\leftarrow$ Uniform($\mathcal{A}$)  
			\Else $~a\leftarrow method(s)$ 				
			\EndIf
			\State $\mathcal{D}\leftarrow \mathcal{D} \cup (s_t,a_t,r(s_t,a_t),\phi(s_t,a_t), s_{t+1})$  

			\For{each gradient step}
				\For{$i \leftarrow 1,2,...,n$}
					\State $\theta_{\psi^{\pi_i}}\leftarrow \theta_{\psi^{\pi_i}}- \lambda_{\psi} \nabla_{\theta_{\psi^{\pi_i}}}J_{\psi^{\pi_i}}(\theta_{\psi^{\pi_i}}) $
					\State $\theta_{\pi_i}\leftarrow \theta_{\pi_i}- \lambda_{\pi} \nabla_{\theta_{\pi_i}}J_{\pi_i}(\theta_{\pi_i}) $
					\State $\theta_{\bar{\psi}^{\pi_i}} \leftarrow \tau \theta_{\bar{\psi}^{\pi_i}} + (1-\tau) \theta_{\psi^{\pi_i}}$ 
				\EndFor
			\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}


% transfer
lastly, for clarity, we note that in the proposed approach there exist 3 folds of transfer mechanism and each complement with one another to accelerate the learning. First, the SFs allows transfer to any arbitrary task by specifying different task weights $w$. The task transfer enable the policy evaluation in a new task without any training. This kind of transfer is associated with composition in the value space. 
Second, since the composite policy is guaranteed to have better performance than the primitives, the trajectory collected by this composite is expected to explore higher value region in the composite task. In this case, the knowledge is transferred from the expert policy to the primitives, similar to \textit{teacher-student} \cite{schmitt2018kickstarting} or \textit{learn from demonstration} \cite{hester2018deep}. 
Third, if the primitives and SFs share representations, such as in Fig.~\ref{fig:net_architecture}, the knowledge is transferred among themselves, and each SF treat others as auxiliary tasks to improve the representations \cite{jaderberg2016reinforcement, wang2022investigating}. Or, given a new SF, it is likely that the learning will be faster than training from scratch due to the pre-trained feature extraction layers.  

