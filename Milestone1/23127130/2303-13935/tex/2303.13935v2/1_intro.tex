% !TEX root = root.tex

\section{Introduction}
\label{sec:Introduction}


One approach to achieving optimal continuous control in robotics is through 
reinforcement learning-based methods (RL) \cite{kober2013reinforcement}. 
However, if the reward definition changes, RL agents are usually optimized for one specific task and require more 
training. On the other hand, multi-task RL 
frameworks aim to design an RL agent that allows recycling old policies for 
other tasks to achieve higher sample efficiency \cite{lazaric2012transfer}. 

% define the problem better
One way to achieve multi-task RL is through transfer learning by manipulating 
the policies trained on the old tasks. This work focuses on the transfer 
learning methods that define tasks using linearly decomposable reward 
functions, commonly used to define many robot control tasks \cite{ng1999policy}.


\begin{figure}[t!]
	\centering
	\includegraphics[width=0.4\textwidth]{./img/illustration}	
	%\includesvg[width=0.5\textwidth]{./img/illustration.svg}
	\caption{In concurrent composition, policy extraction is intractable online. Instead, we propose composing the primitives directly in the run time.}
	\label{fig:illustration}
\end{figure}


Currently, there are two major transfer learning frameworks following the 
aforementioned assumption. The first framework, using \textit{successor 	features} and \textit{generalized policy improvement} (SF-GPI, \cite{barreto2020fast, barreto2019option}), derives the new policy by choosing 
the sub-policies, or primitives, with the largest associated action-value. The 
other framework directly composes the action-value of different tasks, which we 
refer to as \textit{value composition} (VC \cite{haarnoja2018composable, 	hunt2019composing, van2019composing, nangue2020boolean}). This framework first 
defines the composite value functions of new tasks by linearly combining the 
constituent action-value functions and then extracts the policy. 

Different from the \textit{Option} framework \cite{sutton1999between}, which is another approach to multi-task RL that combines the primitives in temporal order, in our novel approach, we focus on concurrent policy composition by composing the primitives at every time step. 
The main objective is to train primitives and enable the online composition to transfer knowledge for solving arbitrary tasks, thereby enhancing sample efficiency \cite{barreto2020fast}. Policy extraction is prohibitive since it requires updating policies multiple times. We consider real-time performance critical. 
Therefore, we must adapt VC-based methods for online composition. We do so by multiplicative policy composition (MCP \cite{peng2019mcp}). We show this can be achieved via the successor features framework, where SF-GPI becomes one special case.  

In summary, our contributions include the following:
\begin{itemize}
	\item A novel successor feature-based online concurrent composition framework unifying the GPI and VC methods.
	\item Deriving the relation between composition in value space and policy space.
	\item Extending this framework to composition in action space via the impact matrix.
	\item Introducing two benchmark multi-task environments, Pointmass and Pointer, based on IsaacGym \cite{liang2018gpu}.
\end{itemize}
