% !TEX root = root.tex

\section{Experiments and Results}
\label{sec:4_experiment}
The experiment aims to test if the multi-task RL agents generated by the proposed framework, i.e., SF-GPI, MSF, DAC, DAC-GPI, can be task-agnostic and transfer to unseen tasks. 

\begin{figure}[b]
	\centering
	\begin{subfigure}[b]{0.35\textwidth}
		\centering
		\includegraphics[width=0.8\textwidth]{./img/pi_connect}  
		%\includesvg[width=0.8\textwidth]{./img/pi_connect.svg}  
		\caption{primitive network.}
		\label{fig:pi_connect}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.35\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{./img/sf_connect}
		%\includesvg[width=0.95\textwidth]{./img/sf_connect.svg}  
		\caption{successor feature network.}
		\label{fig:sf_connect}
	\end{subfigure}
	\caption{network architecture. } 
	\label{fig:net_architecture}
\end{figure}

\begin{figure*}
	\centering
	\begin{subfigure}[t]{0.31\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{./img/transfer_performance_pm2d}
		\caption{\textbf{Transfer performance}: Train with a velocity control task and evaluate with a mixture task $w^{mix}$ (~\ref{eqn:pm_weight}).}
		\label{fig:transfer_performance_pm2d}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.31\textwidth}
	\centering
	\includegraphics[width=0.95\textwidth]{./img/feature_set_pm2d}
	\caption{\textbf{Effect of feature sets in transfer}: Compared to regular features (solid), using augmented features (dotted) is beneficial.} 
	\label{fig:feature_set_pm2d}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.31\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{./img/single_task_pm2d}
		\caption{\textbf{Single task performance}: Due to the composition loss, composition agents have lower performance compared to the SAC in a single task $w=(\mathbf{1}, 0, 1)$.} 
		\label{fig:single_task_pm2d}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.31\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{./img/transfer_performance_pt2d}
		\caption{\textbf{Transfer performance}: Train with a velocity control task and evaluate with a mixture task $w^{mix}$ (~\ref{eqn:pm_weight}). }
		\label{fig:transfer_performance_pt2d}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.31\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{./img/transfer_task2_pt2d}
		\caption{\textbf{Transfer performance}: Train with a velocity control task but evaluate with a position control task $w=(1, 0, 0, 0, 10)$.}
	\label{fig:transfer_task2_pt2d}
	\end{subfigure}
	\hfill	
	\begin{subfigure}[t]{0.31\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{./img/single_task_pt2d}
		\caption{\textbf{Single task performance}: Due to the composition loss, composition agents have lower performance compared to the SAC in a single task $w=(1,0,0,0,1)$.}
		\label{fig:single_task_pt2d}
	\end{subfigure}
	\hfill
	
	\caption{Empirical results on Pointmass (top row) and Pointer (bottom row) show that the proposed composition agents can gradually improve transfer performance to unseen tasks and have comparable single-task performance to baseline SAC \cite{haarnoja2018soft}. SAC is presented in transfer tasks to show that tasks are not generalizable. Each curve represents the mean with two standard deviations of 5 experiments run by the best model from the hyper-parameter tuning. The tuning process is conducted by Bayesian optimization with hundred searches for each agent. Our ablation study indicates that dropout can hurt the performance while other techniques' effects remain unclear, including layer norm, prioritized experience replay, entropy tuning, and activation function.}
\end{figure*}


\subsection{Network Architecture}
We represent primitives and successor features by two neural networks as displayed in Fig.~\ref{fig:net_architecture}. Knowledge transfer can happen within the network since the layers or representations are shared. To reduce over-estimation in the SFs and enable higher update-to-data ratio (UTD ratio) \cite{hiraoka2021dropout}, we include \textit{dropout} and \textit{layernorm} to the SF network. To further mitigate the positive bias \cite{fujimoto2018addressing}, we apply the minimum double critic technique for the SF estimation, i.e., $\hat{\psi}^{\pi} = min(\psi_{\theta_1}^{\pi}, \psi_{\theta_2}^{\pi})$.


\subsection{Environment and Tasks}
Our training environment is implemented using IsaacGym, leveraging its extensive parallelization capabilities. While IsaacGym offers a range of pre-existing robots, we introduce abstract omni- and uni-directional robots, namely Pointmass and Pointer, designed to tackle our problem and gain valuable insights.

\subsubsection{State, Action, and Feature Space} 
The state and action space of the environments are presented in Table.~\ref{tab:env_space]}, 

\begin{table}[h]
	\centering
	\resizebox{0.4\textwidth}{!}{%		
		\begin{tabular}{ c||c|c }
			\hline
			 &  \textit{Pointmass} & \textit{Pointer} \\
			\hline
			$\mathcal{A}$    & $(a_x, a_y, a_z)$ & $(a_{thrust}, a_{roll}, a_{pitch}, a_{yaw})$  \\
			\hline
			$\mathcal{S}$ & \multicolumn{2}{c}{$\Delta{\mathbb{X}}, \Delta{\mathbb{V}}, \Delta{\Theta}, \Delta{\Omega} \in \mathbb{R}^3$}   \\
			\hline
		\end{tabular}}
		\caption{ The action space for Pointmass are applied force on the x-, y-, and z-axis while for Pointer are applied thrust and attitude controls. Both environments share the same state space, namely relative position, velocity, orientation, and angular velocity between the robot and goal.
		\label{tab:env_space]} 
	}
\end{table}
Features, or sub-tasks, can be defined arbitrarily as long as they can be derived from the state space. We define three sets of features for the Pointmass and one for the Pointer, as shown in Table.~\ref{tab:feature_set}. The feature sets used in our study include the following variations: \textit{Regular}, which represents a standard approach for defining reward functions in robotics; \textit{Simple}, a simplified version used for demonstration purposes; and \textit{Augment}, which includes redundant features that generate additional primitives. For example, both $-|\Delta{\mathbb{X}}|$ and $-||\Delta{\mathbb{X}}||_2$ can achieve position control. Each primitive aims to maximize its corresponding feature or sub-task. In general, the task can be solved only when a sufficient amount of features are provided.
\begin{table}[h]
	\centering
	\resizebox{0.48\textwidth}{!}{%		
		\begin{tabular}{ c||c|c }
			\hline
			\multirow{3}{*}{\textit{Pointmass}} & Regular & $-|\Delta{\mathbb{X}}|, -||\Delta{\mathbb{V}}||_2,  
			Success$  \\
			\cline{2-3}
			&Simple  & $-|\Delta{\mathbb{X}}|$  \\
			\cline{2-3}
			&Augment & $-|\Delta{\mathbb{X}}|, -||\Delta{\mathbb{X}}||_2, -|\Delta{\mathbb{V}}|, -||\Delta{\mathbb{V}}||_2,  
			Success$  \\
			\hline
			\textit{Pointer} & Regular & $-||\Delta{\mathbb{X}}||_2, -||\Delta{\mathbb{V}}||_2, 
			-||\Delta{\Theta}||_2,
			-||\Delta{\Omega}||_2,
			 Success$  \\
			\hline
	\end{tabular}}
	
	\caption{Environment feature sets. The absolute value and L2 norm are denoted as $|\cdot|$, $||\cdot||_2$, respectively. $Success$ is a binary feature triggered when the relative distance is less than 1 meter, i.e., $Success=1$ if $||\mathbb{X}||_2 <1$ else $0$. The default will be \textit{Regular} if not explicitly mentioned.  
	\label{tab:feature_set} }
\end{table}
\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.235\textwidth}
		\includegraphics[width=1.0\textwidth]{./img/primitives_pm2d}
		\caption{Sample the end position of 20 trajectories for x- (blue), y- (yellow) primitives and their composite (green). }
		\label{fig:primitives_pm2d}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.235\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{./img/primitives_pt2d}
		\caption{Sample one trajectory for the position (blue), angle (yellow) primitives, and their composite (green).}
		\label{fig:primitives_pt2d}
	\end{subfigure}
	\caption{Visualize primitive and composite distributions by sample trajectories starting from the initial position $X_0$ (red) to the goal at the origin. Composition agents allow solving new tasks by composing existing primitives. }
	\label{fig:primitive_composite_distribution}
\end{figure}


\subsubsection{Mixture Task}
\label{sec: Mixture Task}
We introduce a mixture task, which includes two tasks: \textit{navigation} and \textit{hover} tasks. The navigation task aims to bring the robot closer to the goal position, while the hover task requires the robot's velocity to approach the target velocity when near the goal. The task switches from navigation to hover when the distance to the goal is less than three meters. We can formulate this accordingly,  
\begin{equation}
	\begin{array}{l}
		w^{mix}=\begin{cases}
			w^{nav} & \text{if \ensuremath{d(s_{robot},s_{target})\geq 3m} }\\
			w^{hov} & \text{otherwise}.
		\end{cases}
	\end{array}\label{eqn:pm_weight}
\end{equation}
where by default, in Pointmass, $w^{mix}$ is set as $[(\mathbf{1}, 0, 10), (\mathbf{0}, 1, 10)]$ (bold symbols are vectors of the same length to the corresponding features). In Pointer, the default $w^{mix}$ is $[(1, .5, 0, 0, 10), (0, 1, 0, 0, 10)]$. The navigation task is essentially a position control task. In contrast, in hover tasks, the robots must maintain a circular motion to stay within the hover range and track the desired velocity simultaneously.

We assess transferability by employing different tasks for the training and evaluation phases. Specifically, we train primitives in a velocity control task, with $w_{train}=(\mathbf{0}, 1, 0)$ for Pointmass and $w_{train}=(0, 1, .5, 0, 0)$ for Pointer. Then, we evaluate the agents on the mixture task (Fig.~\ref{fig:transfer_performance_pm2d}~\ref{fig:transfer_performance_pt2d}) or a position control task (Fig.\ref{fig:transfer_task2_pt2d}). We observe that multi-task agents demonstrate greater resilience when the evaluation task differs from the training task. The impact of feature sets is illustrated in Fig.~\ref{fig:feature_set_pm2d}, where we find that using the Regular feature set is sufficient to solve the task, and additional features can further enhance transfer performance for MSF and DAC-GPI. Finally, Fig.~\ref{fig:primitive_composite_distribution} demonstrates the effectiveness of primitive composition in solving composite tasks that individual primitives alone cannot handle. We conclude that compositional agents transfer to different tasks if primitives are adequately trained. However, there is no performance guarantee for single task or transfer task due to the composition loss.


\begin{table}[h]
	\centering
	\resizebox{0.35\textwidth}{!}{%		
		\begin{tabular}{ c||c }
			\hline
			\textit{composite} & \textit{action components $(a_x,a_y)$} \\
			\hline
			$\hat{\pi}^{MSF}$ & $  ( \mathcal{N}^{\pi_x}_x\epsilon, \mathcal{N}^{\pi_y}_y\epsilon)$   \\
			\hline
			$\hat{\pi}^{GPI}$ & $( \mathcal{N}^{\pi_x}_x\epsilon, \mathcal{N}^{\pi_y}_y)$ or $( \mathcal{N}^{\pi_x}_x, \mathcal{N}^{\pi_y}_y\epsilon)$   \\
			\hline
			$\hat{\pi}^{DAC}$ & $ ((\mathcal{N}^{\pi_x}_x)^{\kappa^{\pi_x}_x}(\epsilon)^{\kappa^{\pi_y}_x}, (\mathcal{N}^{\pi_y}_y)^{\kappa^{\pi_x}_y}(\epsilon)^{\kappa^{\pi_y}_y})$   \\
			\hline
	\end{tabular}}
	\caption{Composite distributions in Pointmass2D-Simple.  $\epsilon$ is the composition noise of unknown distribution. 
		\label{tab:composition noise} 
	}
\end{table}
\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.23\textwidth}
		\includegraphics[width=1.0\textwidth]{./img/primitives}
		%	\includesvg[width=0.5\textwidth]{./img/primitives.svg}
		\caption{Top row: action components in $\pi_x=(a_x,a_y)$. Bottom row: $\pi_y$. Color bar range in $[-1,1]$. }
		\label{fig:primitives}
	\end{subfigure}
	~
	\begin{subfigure}[t]{0.23\textwidth}
		\centering
		\includegraphics[width=1.0\textwidth]{./img/kappa}
		%	\includesvg[width=0.5\textwidth]{./img/kappa.svg}
		\caption{Top row: kappa $\kappa^{\pi_x}=(\kappa^{\pi_x}_x,\kappa^{\pi_x}_y)$. Bottom row: $\kappa^{\pi_y}$. Color bar range in $[0,1]$.}
		\label{fig:kappa}
	\end{subfigure}
	\caption{In Pointmass2D-Simple, DAC effectively removes compositional noise (Fig.(a) top right and bottom left) by reducing the corresponding $\kappa$. Each sub-figure represents a 2D plane within $[-20, 20]$ meters.}
	\label{fig:pm2_visual}
\end{figure}


\subsection{Composition Noise and Training Stability in Pointmass2D}


The presence of composition noise resulting from redundant action components in the primitive task reduces the training stability. In the Pointmass2D-Simple environment, the primitive actions corresponding to the features $-|\Delta{X}|$ and $-|\Delta{Y}|$ generate the distributions $\pi_x = ( \mathcal{N}^{\pi_x}_x, \epsilon)$ and $\pi_y= ( \epsilon, \mathcal{N}^{\pi_y}_y)$, introducing composition noise $\epsilon$ (Fig.~\ref{fig:primitives}) and composites (Table.~\ref{tab:composition noise}). In the case of MSF, the composite action contains noise in both components, while SF-GPI only contains noise in one component. Notably, DAC(-GPI) effectively mitigate noise in both action components, as the successor features reduce the impact of the noisy term (Fig.~\ref{fig:kappa}). The importance of filtering out composition noise for effective transfer is evident in Fig.~\ref{fig:transfer_performance_pm2d}. DAC(-GPI) exhibits the best asymptotic performance, followed by SF-GPI and MSF. We observed that composition noise hinders the MSF training stability (Fig.~\ref{fig:single_task_pm2d},~\ref{fig:single_task_pt2d}).

We conclude that applying MSF is impractical. Unlike other methods, which can handle any number of primitives, MSF requires that the feature dimension matches the number of primitives (i.e., n=d). Moreover, its optimistic assumption of linearity in both task and policy spaces is rarely valid.

\subsection{Single Task Performance}
Compared to baseline SAC \cite{haarnoja2018soft}, the multi-task agents exhibit lower asymptotic performance in the single task due to the composition loss (Fig.~\ref{fig:single_task_pm2d},~\ref{fig:single_task_pt2d}). We found that SF-GPI tends to explore less, potentially leading to faster yet premature convergence. We observed that the impact matrix estimation is often noisy, limiting DAC(-GPI)'s effectiveness. Hence, it may be beneficial to initially train primitives using alternative composition methods, transitioning to DAC(-GPI) once SFs achieve satisfactory precision and smoothness.







