
@article{todorov2006linearly,
  title={Linearly-solvable Markov decision problems},
  author={Todorov, Emanuel},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2006}
}


@inproceedings{liang2018gpu,
	title={Gpu-accelerated robotic simulation for distributed reinforcement learning},
	author={Liang, Jacky and Makoviychuk, Viktor and Handa, Ankur and Chentanez, Nuttapong and Macklin, Miles and Fox, Dieter},
	booktitle={Conference on Robot Learning},
	pages={270--282},
	year={2018},
	organization={PMLR}
}


@article{todorov2009compositionality,
  title={Compositionality of optimal control laws},
  author={Todorov, Emanuel},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}

@incollection{lazaric2012transfer,
	title={Transfer in reinforcement learning: a framework and a survey},
	author={Lazaric, Alessandro},
	booktitle={Reinforcement Learning: State-of-the-Art},
	pages={143--173},
	year={2012},
	publisher={Springer}
}

@inproceedings{ng1999policy,
	title={Policy invariance under reward transformations: Theory and application to reward shaping},
	author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
	booktitle={Icml},
	volume={99},
	pages={278--287},
	year={1999},
	organization={Citeseer}
}


@article{schmitt2018kickstarting,
  title={Kickstarting deep reinforcement learning},
  author={Schmitt, Simon and Hudson, Jonathan J and Zidek, Augustin and Osindero, Simon and Doersch, Carl and Czarnecki, Wojciech M and Leibo, Joel Z and Kuttler, Heinrich and Zisserman, Andrew and Simonyan, Karen and others},
  journal={arXiv preprint arXiv:1803.03835},
  year={2018}
}


@article{barto2013intrinsic,
	title={Intrinsic motivation and reinforcement learning},
	author={Barto, Andrew G},
	journal={Intrinsically motivated learning in natural and artificial systems},
	pages={17--47},
	year={2013},
	publisher={Springer}
}

@InProceedings{pmlr-v70-pathak17a,
	title = 	 {Curiosity-driven Exploration by Self-supervised Prediction},
	author =       {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
	booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
	pages = 	 {2778--2787},
	year = 	 {2017},
	editor = 	 {Precup, Doina and Teh, Yee Whye},
	volume = 	 {70},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {06--11 Aug},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf},
	url = 	 {},
	abstract = 	 {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agentâ€™s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.}
}

@article{haarnoja2018learning,
	title={Learning to walk via deep reinforcement learning},
	author={Haarnoja, Tuomas and Ha, Sehoon and Zhou, Aurick and Tan, Jie and Tucker, George and Levine, Sergey},
	journal={Robotics: Science and Systems},
	year={2019}
}



@inproceedings{ORBi,
	author = {Defourny, Boris and Ernst, Damien and Wehenkel, Louis},
	TITLE = {Risk-aware decision making and dynamic programming},
	LANGUAGE = {English},
	YEAR = {2008},
	SIZE = {8},
	LOCATION = {Whistler, Canada},
	ABSTRACT = {This paper considers sequential decision making problems under uncertainty, the tradeoff between the expected return and the risk of high loss, and methods that use dynamic programming to find optimal policies. It is argued that using Bellman's Principle determines how risk considerations on the return can be incorporated. The discussion centers around returns generated by Markov Decision Processes and conclusions concern a large class of methods in Reinforcement Learning.},
	ORGANIZATION = {F.R.S.-FNRS - Fonds de la Recherche Scientifique}
}

@article{paszke2019pytorch,
	title={Pytorch: An imperative style, high-performance deep learning library},
	author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
	journal={Advances in neural information processing systems},
	volume={32},
	year={2019}
}

@misc{wandb,
	title = {Experiment Tracking with Weights and Biases},
	year = {2020},
	note = {Software available from wandb.com},
	url={},
	author = {Biewald, Lukas},
}
@article{klambauer2017self,
	title={Self-normalizing neural networks},
	author={Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@article{colas2022autotelic,
	title={Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: a short survey},
	author={Colas, C{\'e}dric and Karch, Tristan and Sigaud, Olivier and Oudeyer, Pierre-Yves},
	journal={Journal of Artificial Intelligence Research},
	volume={74},
	pages={1159--1199},
	year={2022}
}
@article{narvekar2020curriculum,
	title={Curriculum learning for reinforcement learning domains: A framework and survey},
	author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
	journal={The Journal of Machine Learning Research},
	volume={21},
	number={1},
	pages={7382--7431},
	year={2020},
	publisher={JMLRORG}
}

@article{pauli2021training,
	title={Training robust neural networks using Lipschitz bounds},
	author={Pauli, Patricia and Koch, Anne and Berberich, Julian and Kohler, Paul and Allg{\"o}wer, Frank},
	journal={IEEE Control Systems Letters},
	volume={6},
	pages={121--126},
	year={2021},
	publisher={IEEE}
}

@inproceedings{hester2018deep,
	title={Deep q-learning from demonstrations},
	author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and others},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={32},
	number={1},
	year={2018}
}


@article{kober2013reinforcement,
	title={Reinforcement learning in robotics: A survey},
	author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
	journal={The International Journal of Robotics Research},
	volume={32},
	number={11},
	pages={1238--1274},
	year={2013},
	publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{schaul2015universal,
	title={Universal value function approximators},
	author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
	booktitle={International conference on machine learning},
	pages={1312--1320},
	year={2015},
	organization={PMLR}
}

@inproceedings{espeholt2018impala,
	title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
	author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
	booktitle={International conference on machine learning},
	pages={1407--1416},
	year={2018},
	organization={PMLR}
}

@article{raisim,
	title={Per-contact iteration method for solving contact dynamics},
	author={Hwangbo, Jemin and Lee, Joonho and Hutter, Marco},
	journal={IEEE Robotics and Automation Letters},
	url="www.raisim.com",
	volume={3},
	number={2},
	pages={895--902},
	year={2018},
	publisher={IEEE}
}

@article{jaderberg2016reinforcement,
	title={Reinforcement learning with unsupervised auxiliary tasks},
	author={Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
	journal={},
	year={2016}
}

@article{schulman2017equivalence,
	title={Equivalence between policy gradients and soft q-learning},
	author={Schulman, John and Chen, Xi and Abbeel, Pieter},
	journal={arXiv preprint arXiv:1704.06440},
	year={2017}
}


@inproceedings{fujimoto2018addressing,
	title={Addressing function approximation error in actor-critic methods},
	author={Fujimoto, Scott and Hoof, Herke and Meger, David},
	booktitle={International conference on machine learning},
	pages={1587--1596},
	year={2018},
	organization={PMLR}
}


@inproceedings{haarnoja2017reinforcement,
	title={Reinforcement learning with deep energy-based policies},
	author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
	booktitle={International conference on machine learning},
	pages={1352--1361},
	year={2017},
	organization={PMLR}
}

@inproceedings{haarnoja2018composable,
	title={Composable deep reinforcement learning for robotic manipulation},
	author={Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey},
	booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
	pages={6244--6251},
	year={2018},
	organization={IEEE}
}

@inproceedings{haarnoja2018soft,
	title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
	author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	booktitle={International conference on machine learning},
	pages={1861--1870},
	year={2018},
	organization={PMLR}
}

@book{ziebart2010modeling,
	title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
	author={Ziebart, Brian D},
	year={2010},
	publisher={Carnegie Mellon University}
}

@article{fox2015taming,
	title={Taming the noise in reinforcement learning via soft updates},
	author={Fox, Roy and Pakman, Ari and Tishby, Naftali},
	journal={Conference on Uncertainty in Artificial Intelligence. AUAI Press},
	year={2015}
}

@article{nachum2017bridging,
	title={Bridging the gap between value and policy based reinforcement learning},
	author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}


@inproceedings{hunt2019composing,
	title={Composing entropic policies using divergence correction},
	author={Hunt, Jonathan and Barreto, Andre and Lillicrap, Timothy and Heess, Nicolas},
	booktitle={International Conference on Machine Learning},
	pages={2911--2920},
	year={2019},
	organization={PMLR}
}


@article{barreto2020fast,
	title={Fast reinforcement learning with generalized policy updates},
	author={Barreto, Andr{\'e} and Hou, Shaobo and Borsa, Diana and Silver, David and Precup, Doina},
	journal={Proceedings of the National Academy of Sciences},
	volume={117},
	number={48},
	pages={30079--30087},
	year={2020},
	publisher={National Acad Sciences}
}



@article{tasse2022world,
	title={World Value Functions: Knowledge Representation for Multitask Reinforcement Learning},
	author={Tasse, Geraud Nangue and James, Steven and Rosman, Benjamin},
	journal={},
	year={2022}
}

@article{nangue2020boolean,
		title={A Boolean task algebra for reinforcement learning},
	author={Nangue Tasse, Geraud and James, Steven and Rosman, Benjamin},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={9497--9507},
	year={2020}
}



@article{peng2019mcp,
	title={Mcp: Learning composable hierarchical control with multiplicative compositional policies},
	author={Peng, Xue Bin and Chang, Michael and Zhang, Grace and Abbeel, Pieter and Levine, Sergey},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	year={2019}
}



@inproceedings{saxe2017hierarchy,
	title={Hierarchy through composition with multitask LMDPs},
	author={Saxe, Andrew M and Earle, Adam C and Rosman, Benjamin},
	booktitle={International Conference on Machine Learning},
	pages={3017--3026},
	year={2017},
	organization={PMLR}
}

@article{taylor2009transfer,
	title={Transfer learning for reinforcement learning domains: A survey.},
	author={Taylor, Matthew E and Stone, Peter},
	journal={Journal of Machine Learning Research},
	volume={10},
	number={7},
	year={2009}
}


@inproceedings{van2019composing,
	title={Composing value functions in reinforcement learning},
	author={Van Niekerk, Benjamin and James, Steven and Earle, Adam and Rosman, Benjamin},
	booktitle={International conference on machine learning},
	pages={6401--6409},
	year={2019},
	organization={PMLR}
}



@article{barreto2019option,
	title={The option keyboard: Combining skills in reinforcement learning},
	author={Barreto, Andr{\'e} and Borsa, Diana and Hou, Shaobo and Comanici, Gheorghe and Ayg{\"u}n, Eser and Hamel, Philippe and Toyama, Daniel and Mourad, Shibl and Silver, David and Precup, Doina and others},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	year={2019}
}


@inproceedings{bacon2017option,
	title={The option-critic architecture},
	author={Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={31},
	number={1},
	year={2017}
}

@article{sutton1999between,
	title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
	author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
	journal={Artificial intelligence},
	volume={112},
	number={1-2},
	pages={181--211},
	year={1999},
	publisher={Elsevier}
}


@article{machado2017eigenoption,
	title={Eigenoption discovery through the deep successor representation},
	author={Machado, Marlos C and Rosenbaum, Clemens and Guo, Xiaoxiao and Liu, Miao and Tesauro, Gerald and Campbell, Murray},
	journal={},
	year={2017}
}



@article{hiraoka2021dropout,
	title={Dropout q-functions for doubly efficient reinforcement learning},
	author={Hiraoka, Takuya and Imagawa, Takahisa and Hashimoto, Taisei and Onishi, Takashi and Tsuruoka, Yoshimasa},
	journal={International Conference on Learning Representations (ICLR)},
	year={2022}
}

@article{dayan1993improving,
	title={Improving generalization for temporal difference learning: The successor representation},
	author={Dayan, Peter},
	journal={Neural computation},
	volume={5},
	number={4},
	pages={613--624},
	year={1993},
	publisher={MIT Press}
}


@article{wang2022investigating,
	title={Investigating the properties of neural network representations in reinforcement learning},
	author={Wang, Han and Miahi, Erfan and White, Martha and Machado, Marlos C and Abbas, Zaheer and Kumaraswamy, Raksha and Liu, Vincent and White, Adam},
	journal={arXiv preprint arXiv:2203.15955},
	year={2022}
}


@article{klambauer2017self,
	title={Self-normalizing neural networks},
	author={Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@inproceedings{46503,
	title	= {Searching for Activation Functions},
	author	= {Prajit Ramachandran and Barret Zoph and Quoc Le},
	year	= {2018},
	URL	= {}
}


@article{munos2016safe,
	title={Safe and efficient off-policy reinforcement learning},
	author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
	journal={Advances in neural information processing systems},
	volume={29},
	year={2016}
}
