\subsection{Input data format}
The performance of DL models depends crucially on the type and quality of their
input data. In the GW field, different options have been explored depending on
how heavily the data have been transformed~\cite{Cuoco:2020ogp}.
Directly using GW strain time series
would allow a flexible training in terms of both amplitude and
frequency evolution of potential signals.
The whitened detector strain has been used e.g. in
Ref.~ \cite{Gebhard:2019ldz,PhysRevLett.120.141103,Schafer:2021fea} for CBC signals.
For CW-like signals, different approaches have been investigated. Some
examples include Fourier transforms of blocks of data in the search for long-duration
transients from newly-born neutron stars \cite{2019PhRvD.100f2005M}. Also, 
for the search of CWs from spinning NSs,
time-frequency spectrograms and Viterbi maps \cite{2020PhRvD.102h3024B}
and power-based statistics \cite{2019PhRvD.100d4009D, Yamamoto:2020pus, Yamamoto:2022adl}
have been used.

As stated in Sec.~\ref{sec:search}, the most expensive step of the algorithm
from Ref.~\cite{Prix:2011qv}
for post-glitch transients is typically the calculation of the partial
sums of the \Fstat atoms over the transient parameters $\TP$. In comparison, the
cost of computing the \Fstat atoms (the main matched-filter stage) is marginal.
Therefore we decide to
use as input data to our network the \Fstat atoms, which contain all the
information needed to determine the presence of a signal in the data. The exact
expressions of the atoms are derived in Appendix~\ref{sec:fstat_derivation}.
When passing to the network, we do not perform any type of preprocessing to the
\Fstat atoms, since they are already normalized to order $\mathcal{O}(1)$
both for noise-only data and for signals within the range of SNRs considered here.

As we will demonstrate, this choice allows for a comparatively simple DL model
architecture and training setup to reach detection sensitivity close to that of
traditional detection statistics. However, it means that we keep to the specific
frequency evolution of the standard CW model, and the atoms still have to be
computed for each set of parameters $\Dop$.

By using \Fstat atoms as input to the network,
we can also easily implement a multi-detector configuration by
merging the single-detector atoms into a set of equi-spaced timestamps
spanning the total observing time:
the result will correspond to
the single-detector values at timestamps when only one detector is available,
the sum when multiple detectors are on,
and filled up with zeros as required.
The merged atoms span 4190 timestamps, spaced by 1800\,s as the original SFTs.
The summing of the multi-detector atoms allows us to implement a simpler
network in contrast to e.g. multi-channel or multi-input models and comes with
the gain in sensitivity from coherently combining the detectors.

\subsection{CNN architecture}
\label{sec:architecture}
\begin{figure*}
    \includegraphics[width=\textwidth]{figures/cnn_architecture.pdf}
    \caption{Architecture of the convolutional neural network (CNN) model,
    which as its input takes the \Fstat atoms. The CNN is made up of a stack of three
    convolutional layers (labelled Conv1D in figure) followed by four fully connected layers (FC) and the output
    layer. The convolutional and fully connected layers all use the ReLU activation function.
    The convolutional part and the fully connected part of the network
    are separated by a flattening layer, needed to transform the output feature
    maps to a digestible input for the dense layers, and a dropout layer.
    }
    \label{fig:cnn_architecture}
\end{figure*} 

CNNs are a type of artificial neural networks that use convolutional layers as
building blocks \cite{GoodBengCour16,cnn_indolia}. While in fully-connected
layers neurons are connected to all those of the previous layer, this would be
computationally unfeasible for a high-dimensional input such as an image, for
which a fully-connected layer would use a weight for each pixel. Convolutional
layers avoid this problem by exploiting the spatial structure and repeating
patterns in the data. To achieve this they convolve various independent filters
(also called convolution kernels) with the input. Each filter is a matrix of
weights to be trained, which slides along the input data, producing a feature map.
These maps are then transformed through a non-linear activation function.
This way, each filter learns to identify a pattern or characteristic of the data,
regardless of location,
which is preserved in the feature maps.
These maps are then stacked to yield the input for the next layer.
This enables the CNN to learn where features, or mixtures of features, are located in the data.

For this work we use a simple CNN,
made up of three one-dimensional convolutional layers.
This means that in each layer the filters slide along the time dimension (vertical axis)
and convolve all columns at once. 
We always set the filter size to 3 along the time dimension in each layer,
and to increasingly narrow down important features we use a decreasing number of 128, 64 and 8 independent filters per layer
and set the stride parameter to 3, 2 and 1 respectively.
The stride is the number of pixels the filter moves while sliding through the
input matrix, i.e. the bigger the stride, the smaller the output feature maps.
We do not implement pooling layers \cite{726791}, since we find that for our particular
architecture they are detrimental to the performance of the network.

The convolutional layers are then
followed by a flattening and dropout
layer \cite{dropout} and four fully-connected layers. 
Again we decrease the number of neurons from layer to layer (256, 128 and 64)
so that the number of trainable parameters
decreases when approaching the final output.
The last of the fully-connected layers
gives the output of the network. We choose this to be a single continuous,
positive value $\rho_{\textrm{CNN}}$
which we train to match the injected SNR from Eq.~\eqref{eq:snr}
for each sample in the signal training set, and to return 0 for pure-noise samples.
We use the Rectified Linear Unit (ReLU) activation function
\cite{2018arXiv180308375A} for all layers, including for the output,
which produces the desired range of values for an SNR-like quantity:
$\rho_{\textrm{CNN}}\in [0,+\infty)$.
The dropout layer, which helps to prevent the CNN from overfitting
\cite{dropout}, has a dropout rate of $0.33$.

The full network architecture is illustrated in
Fig.~\ref{fig:cnn_architecture} and has been implemented using the Keras
library \cite{chollet2015keras} based on the Tensorflow infrastructure \cite{tensorflow2015-whitepaper}.
For the various hyperparameters, we chose starting values from an exploratory
run with the optimization framework Optuna \cite{optuna_2019}, minimizing the
loss on the validation set using a sampler based on the Tree-structured Parzen
Estimator algorithm \cite{NIPS2011_86e8f7ab}, and further tuned them
manually.

\subsection{Training strategy}
\label{sec:training_strategy}
The CNN is trained by minimizing a loss function $L$ over a training set
containing both pure-noise samples and simulated signals.
We use the mean squared error loss:
\begin{equation}
    L = \frac{1}{n}\sum_{i=1}^n(\rho_{\textrm{inj}}-\rho_{\textrm{CNN}})^2 \,,
\end{equation}
where $\rho_{\textrm{inj}}$ is the injected SNR of the training signals,
corresponding to Eq.~\eqref{eq:snr}.
The training of our CNN's continuous output function can
therefore be considered a regression learning approach.

In general, we separate the training into two stages which differ with respect to the injection
set (both in the number of training samples and the SNR range of signal samples), optimizer
and number of epochs. A summary of the parameters used in each stage are shown
in Tab.~\ref{tab:curriculum} and will be explained in more detail below.

When training a network, the weights are updated by an optimizer through
gradient descent. There is a variety of well-established optimizers built to
e.g. prioritize speeding up convergence or improving generalization.
We use the Adam optimizer \cite{2014arXiv1412.6980K} in a
first training stage, i.e. for the first 200 epochs, and the Stochastic Gradient
Descent algorithm (SGD) \cite{2016arXiv160904747R} for the remaining 1000
epochs. We found that the Adam algorithm converges rapidly while the SGD
generalizes better, reducing the unwanted effect of overfitting
\cite{2017arXiv171207628S}.

We also use the curriculum learning (CL) training
strategy \cite{Bengio2009}. CL has been applied on GW data in previous works
\cite{PhysRevD.103.063011,PhysRevD.103.102003} and consists of training on datasets
of increasing difficulty. The difficulty criterion depends on the type
of data and problem, and for our case we choose the range of injected SNRs.
We use a simple two-stage CL
strategy, first training on an easier dataset with strong signals
(high SNR) and then adding a more difficult dataset, i.e. weaker
signals. We align the stages of CL to the change in optimizer.
More details on the
training sets are given in Sec.~\ref{sec:training_sets}.
Alternative CL approaches avoid re-using the easier
dataset in further stages, but we decide to keep it also in the final stage so
that the model still remains robust to the stronger signals.

We train two CNN models with the same architecture, the same CL setup, etc.:
one on only-rectangular signals and one on only-exponential signals.
All training sets are equally split into noise and signal atoms.
The validation set,
used to compute the loss function after each epoch,
is $33\%$ of the training data.


\begin{table}[]
    \begin{tabular}{cccc}
    \hline
    \hline
                      &  & CL stage 1 & CL stage 2 \\ 
    \multirow{2}{*}{injection} & $\rho_{\textrm{inj}}$ & $U[6,40]$ & $\textrm{set}\,1+ U[4,10]$ \\ 
                    set  & $N_{\textrm{train}}$ & $4\times 10^4$ &
                      $\textrm{set}\,1+ 6\times 10^4$\\ 
                    optimizer  &  & Adam & SGD  \\ 
                    epochs  &  & 200 & 1000 \\ 
                    \hline
                    \hline
    \end{tabular}
    \caption{Parameters used for the two stages of curriculum learning (CL). We
    first train on strong signals with $\rho_{\textrm{inj}}$ randomly drawn from a uniform
    distribution $U[6,40]$, and then we add to the training weaker signals with
    $\rho_{\textrm{inj}}$ randomly drawn from $U[4,10]$. The number of samples we train on is
    $4\times 10^4$ in the first CL stage, and during the second CL
    stage we add $6\times 10^4$ more samples, this time with $\rho_{\textrm{inj}}$ in the weaker
    range. The final stage uses the full $10^5$ samples training dataset,
    including both high-SNR and low-SNR signals.
    The $\{t^0, \tau\}$ parameter ranges of the signals match the ones used for the
    search, which are listed in Tab.~\ref{tab:ephem}.
    }
    \label{tab:curriculum}
\end{table}

We start by training and testing on simulated Gaussian noise only,
but with realistic gaps matching those of the O2 run
(results will be shown in Sec.~\ref{sec:trainsynth_testsynth}). Real data, however, can present
instrumental artifacts which differ from a Gaussian background.
For tCW searches in particular, the disturbances that
most affect them are narrow spectral features similar to quasi-monochromatic CWs or tCWs
(so called ``lines'')
\cite{PhysRevD.97.082002, Keitel:2013wga, Keitel:2015ova}.
We also test these CNNs on real data
(Sec.~\ref{sec:trainsynth_testreal}). But as we will see, to be robust to non-Gaussianities of
the data one must use real data also in the training set. There are different
approaches that can be employed, e.g. only using real data in the training set,
or using a mixture of both simulated and real data. We explain our choice and
show our results in Sec.~\ref{sec:synth2real}.

\subsection{Training sets}
\label{sec:training_sets}
\subsubsection{Gaussian-noise synthetic data}
\label{sec:sets_synth}
For Gaussian noise training data, we use the approach of generating ``synthetic''
detection statistics (and atoms, see appendix A.4 of Ref.~\cite{Prix:2011qv}),
which avoids generating and analyzing SFT files
and therefore is considerably faster than full searches
over simulated Gaussian noise data.
Specifically, we use the
\texttt{synthesize\_TransientStats} program from \texttt{LALSuite} which draws
samples of the quantities needed to compute atoms,
either under the pure Gaussian noise assumption
or as expected for signals (with randomized parameters) in Gaussian noise.
It then produces the atoms
we need as input for the CNNs
and also the \Fstatmax and $\Btrans$ statistics
(for given transient parameter search ranges)
we will use for comparing the detection performance of the CNNs.

In each stage of the CL,
the training set is balanced, with half of the samples being pure noise and the
other half containing tCW signals. All signals correspond to Vela's sky location.
Frequency and spindowns do not matter for the synthesizing method.
The parameters $\cos\iota$, $\psi$, and $\phi_0$ are randomized over their natural ranges,
i.e. $\cos\iota\in[-1,1],\psi\in[0,\pi/2],\phi\in[0,2\pi]$.
The transient parameters of the signals
are drawn from the search ranges shown in Tab.~\ref{tab:curriculum},
which also summarizes the SNR ranges and set sizes for the two CL levels.

\subsubsection{Real data}
\label{sec:sets_real}
The real-data training set atoms are generated from analyzing a $0.1$\,Hz band
of LIGO O2 data \cite{LIGOScientific:2019lzm,O2SFTs}.
To avoid training bias,
we have to choose this band as disjoint from the search band around the nominal GW frequency of Vela,
but it should be close enough to have similar noise characteristics.
We center this band around $22.2$\,Hz, which yields a frequency region
without visible lines in the PSDs of the two detectors.

The noise samples are produced by running a grid-based search
with \texttt{PyFstat} \cite{Keitel:2021xeq, pyfstat}
and storing the output atoms,
using the same setup as in Tab.~\ref{tab:ephem}
except for the shift in frequency
and using a coarser frequency resolution
to reduce correlations between atoms at different $\Dop$:
namely \mbox{$df=5 \times 10^{-6}$\,Hz} for the first CL stage
and \mbox{$df=3 \times 10^{-6}$\,Hz} for the second.

The signal samples of this training set are generated by injecting
signals at random frequencies within the same offset band,
with spindowns fixed to the nominal GW values
(twice those from pulsar timing).
Atoms are then produced by a single-template \texttt{PyFstat} analysis
for each injection.

\subsection{Timings}
\label{sec:timings}
The training of the network takes about $1.8$\,hours on a Tesla V100-PCIE-32GB
for only synthetic data.
When including real data, the training takes twice as long in total. After
training, evaluation on the same GPU takes
\mbox{$c_{\textrm{CNN}} \approx 4\times10^{-4}$\,s} per sample, when averaged over a batch
of $10^4$ samples using Tensorflow's method \texttt{Model.predict\_on\_batch}.
This compares favorably with costs for the \Fstatmax or $\Btrans$ statistics, which
for the same set of transient parameters take $\approx
10^{-2}$\,s per sample for rectangular windows (similar with both the \texttt{LALSuite} CPU
code on a Intel Xeon Gold 6130 2.10GHz and the \texttt{PyFstat} GPU version from
Ref.~\cite{Keitel:2018pxz} on the same V100) and for exponential windows over 15\,s
on the CPU and \mbox{$c_{\Btrans^{\textrm{e}}}^{\textrm{GPU}} \approx 3\times
10^{-2}$\,s} on the V100.

\section{Evaluation method}
\label{sec:evaluation_method}
    The output $\rho_{\textrm{CNN}}$ can be used as a detection statistic for hypothesis testing.
We compare the performance of
the CNNs with other detection statistics using receiver operating characteristic
(ROC) curves on the separate test sets described below.
These show the probability of detection $p_{\textrm{det}}$, corresponding
to the fraction of signals above threshold, or true positives, as a function of the probability of false alarm
$p_{\textrm{FA}}$, which is equal to the fraction of false positives from pure-noise samples. The number of noise
samples determines how deep in $p_{\textrm{FA}}$ the curves can go, while the
number of signal samples determines the accuracy of the $p_{\textrm{det}}$
estimate. In the following we always use $\approx 10^7$ noise samples and $10^4$
signal samples.
The latter corresponds to $p_{\textrm{det}}$ uncertainties of $\lesssim 2\%$,
and we will consider differences in ROC curves below this level as marginally significant.
This disparity in noise and signal set sizes is due to the fact that, as mentioned
in Sec.~\ref{sec:search}, typical template bank sizes for standard searches can
reach the order of millions, and so the operating $p_\textrm{FA}$ at which we
assess the performance of our method must reach at least $10^{-6}$ to $10^{-7}$. 

We will compare two versions of the CNN,
one trained on only rectangular windows (output
$\rho_{\textrm{CNN}}^{\textrm{r}}$)
and the other trained on only exponential windows 
(output $\rho_{\textrm{CNN}}^{\textrm{e}}$),
with the detection statistics \Fstatmax and $\Btrans$.
We also use the r/e superscripts on these statistics
depending on the window assumed in their computation.
In addition, in this section we introduce
a two-stage filtering process combining a CNN with $\Btrans$.
\subsection{Testing sets}
Our test sets are constructed to match the O2 Vela search
discussed in Sec.~\ref{traditional_analysis}.
For both the synthetic and real data case, we generate two separate test sets:
both with the same $10^7$ noise samples,
corresponding to the number of $\lambda$ parameters in the template bank for our
reference search (compare Tab.~\ref{tab:ephem}).
But the $10^4$ signal samples are rectangular-shaped for
the first set, and exponentially-decaying-shaped for the second.
The generation methods for each set are the same as described in Sec.~\ref{sec:training_sets}.
The transient parameter ranges of the signal testing set are the same as those of the training sets,
while the $\rho_{\textrm{inj}}$ of the testing sets
are drawn from $U[4,40]$. The real data signal frequencies are randomly drawn from a $0.1$\,Hz
band around the nominal GW frequency of Vela.

\begin{figure}
    \scalebox{0.8}{
    \begin{tikzpicture}[node distance=2cm]
        \node (sfts) [process] {SFTs};
        \node (atoms) [process, below of=sfts,] {atoms};
        \node (Fmn) [process, below of=atoms, xshift=-2.5cm, yshift=-2cm] {$2\mathcal{F}(t^0,\tau)$};
        \node (cnn) [process, below of=atoms, xshift=2.5cm, yshift=-0.60cm] {$\rho_{\textrm{CNN}}$};
\node (logB) [process, below of=Fmn, yshift=-1cm]{$B_{\textrm{tS/G}}$};
        \node (detection) [decision, below of=cnn, yshift=-2.5cm,]{detection?};
        \node[inner sep=0,minimum size=0,left of=cnn, xshift=-3cm] (k) {}; % invisible node

        \draw [->, line width=0.4mm] (sfts) -- node[anchor=west]{matched filter}(atoms);
        \draw [->, line width=0.4mm] (atoms.west) -| node[anchor=east,
        yshift=-1.1cm]{partial sums}(Fmn.north);
        \draw [->, line width=0.4mm] (atoms.east) -| node[anchor=west, yshift=-1.1cm]{CNN}(cnn.north);
        \draw [->, line width=0.4mm] (Fmn) -- node[anchor=east]{marginalize}(logB);
        \draw [->, line width=0.4mm] (logB) -- (detection);
        \draw [->, line width=0.4mm] (cnn) -- (detection);
        \draw [-, line width=0.4mm, dashed] (cnn) -- node[anchor=south]{two-stage filtering}(k);
    \end{tikzpicture}
    }
    \caption{Flowchart of three different methods for the tCW search:
    from the SFT data to \Fstat atoms
    and then either via partial sums and marginalization to the $\Btrans$ statistic,
    or to the CNN.
    In the two-stage filtering method, after the CNN
    a subset of candidates get passed back to the branch leading to $\Btrans$.
    (For ``synthetic'' data, one starts directly from atoms.)}
    \label{fig:flowchart}
    \end{figure}

\subsection{Two-stage filtering}
\label{sec:pipeline}
We have seen in Sec.~\ref{sec:timings} that evaluating the CNNs is very fast.
In particular, for exponential windows it is faster than even the GPU implementation
of $2\mathcal{F}_{\max}^{\,\textrm{e}}$ and $\Btrans^{\textrm{e}}$
from Ref.~\cite{Keitel:2018pxz}.
Motivated by this, in all of the following tests we also evaluate a combined detection approach,
using the CNN as a preliminary filter stage
with $\Btrans$ as a second stage. The CNN can first be run
on all the available data, and then a follow-up with the traditional detection
statistic is done only if $\rho_{\textrm{CNN}}$ is above a given threshold. A
simplified flowchart of this method,
in comparison with the purely traditional and the pure CNN approaches,
is shown in Fig.~\ref{fig:flowchart}.

The amount of follow-up candidates can be set by choosing an operating
$p_{\textrm{FA}}^{\textrm{CNN}}$ for the CNN stage, so that we
gain as much sensitivity as possible while the whole pipeline is still
computationally less expensive than directly computing $\Btrans$ over all templates.

Towards low overall $p_{\textrm{FA}}$, we will see that
performance improves compared to the single-stage CNN
and approaches that of the pure $\Btrans$ more closely.
It is also possible for this two-stage method to achieve
higher $p_\textrm{det}$ at low $p_{\textrm{FA}}$ than pure $\Btrans$
if the CNN learns to better discard some noise features that would cause loud $\Btrans$ outliers.
On the other hand, the ROC
curves of this method will not arrive at the point $(p_{\textrm{det}} = 1,
p_{\textrm{FA}}=1)$ as is normally the case for single-stage detection methods.
Any signals lost, i.e. the amount of
$p_{\textrm{det}}$ lost, during the first stage cannot be restored, since only
the candidates passing that stage's threshold are passed on to $\Btrans$. Therefore, by
choosing a particular $p_{\textrm{FA}}^{\textrm{CNN}}$, the highest
probability of detection the two-stage method can reach will be at most
$p_{\textrm{det}}^{\textrm{CNN}}(p_{\textrm{FA}}^{\textrm{CNN}})$.

We choose
$p_{\textrm{FA}}^{\textrm{CNN}} = 10^{-3}$ as an operating point, since it offers a
good compromise between computational cost for the follow-up and the loss of
overall $p_{\textrm{det}}$ at that operating point, as we will see in
Sec.~\ref{sec:trainsynth_testsynth}.

In the case of an exponential window test set
with \mbox{$N \approx 1.15 \times 10^{7}$} templates
and with the hardware and timings as in Sec.~\ref{sec:timings},
the first stage takes \mbox{$c_{\textrm{CNN}} \times N \approx 1.3$\,hours}.
We pass a fraction of $10^{-3}$ of candidates to the second stage
where $\Btrans^{\textrm{e}}$ is evaluated.
This is then expected to take
\mbox{$c_{\Btrans^{\textrm{e}}}^{\textrm{GPU}}\times p_{\textrm{FA}}^{\textrm{CNN}} \times N \approx350$\,s.}
The two-stage filtering method thus takes in total less than
$1.5$\,hours against the \mbox{$c_{\Btrans^{\textrm{e}}}^{\textrm{GPU}}\times N \approx 95$\,hours} needed
without the first CNN stage. A single-stage $\Btrans^{\textrm{e}}$ search on a CPU (Intel Xeon
Gold 6130 2.10GH) would even take $\gtrsim 4\times10^4\,$hours.
