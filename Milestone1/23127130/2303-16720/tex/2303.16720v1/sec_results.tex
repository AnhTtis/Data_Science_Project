\subsection{Results on synthetic data}
\label{sec:trainsynth_testsynth}
\begin{figure}[t]
    \includegraphics[width=\columnwidth]{figures/ROC_curves_trainsynth_testsynth.pdf}
    \caption{ROC curves with synthetic data
    as testing set and CNNs trained on synthetic
    data. Note the vertical axis is zoomed in. Both panels use the same noise
    testing set, but the signals in the testing sets are rectangular-windowed in the
    upper panel, and exponentially decaying in the bottom panel. For both
    panels, the black lines are the ROC curves for $2\mathcal{F}_{\max}^{\,\textrm{r}}$ (dash-dotted) and
    for $\Btrans^{\textrm{r}}$ (dashed), always searching for a rectangular signal (even when
    the test set contains exponentially decaying signals). The solid windowed
    lines are the ROC curves for the CNNs, trained on only rectangular-shaped
    signals (blue) and on onlys exponentially decaying signals (orange). The
    dotted colored lines are the ROC curves for the two-stage filtering method,
    with the corresponding windows used in training.}
    \label{fig:roc_synth}
\end{figure}

We start by testing the CNNs that were trained on synthetic data only,
also using purely synthetic data for the test. As mentioned in
Sec.~\ref{sec:sets_synth}, synthetic data are random draws of the \Fstat
atoms, assuming an underlying Gaussian noise distribution, from which the
tCW detection statistics $2\mathcal{F}_{\max}^{\,\textrm{r}}$ and $\Btrans^{\textrm{r}}$
can then be calculated.\footnote{
We omit the much more expensive $2\mathcal{F}_{\max}^{\,\textrm{e}}$ and $\Btrans^{\textrm{e}}$ statistics in this section.
We will only compute them on a full test set for comparison once, on real data, in Sec.~\ref{sec:synth2real}.}
The detection problem is easier for this case than for real data,
so we show these results as a starting point.
In Fig.~\ref{fig:roc_synth} we show ROC curves comparing the CNNs
against the other two detection statistics.
The two subplots refer to the two different test sets, which differ only in the
window function of the injected signals, but for both cases the standard
detection statistics use rectangular windows.

As expected \cite{Prix:2011qv}, $\Btrans^{\textrm{r}}$ performs marginally better than
$2\mathcal{F}_{\max}^{\,\textrm{r}}$. Both their probabilities of detection surpass
\mbox{$p_{\textrm{det}}=90\%$} even at $p_{\textrm{FA}}$ as low as $10^{-7}$.
The two CNNs (trained on only rectangular and only exponentially decaying windows)
perform similarly overall,
with at most $\lesssim 1\%$ difference in $p_{\textrm{det}}$
between each other
and a loss with respect to $\Btrans^{\textrm{r}}$ of
at most $7\%$ for rectangular
and $5\%$ for exponential windows.

The two-stage filtering reaches a plateau at higher $p_{\textrm{FA}}$ values,
corresponding (as mentioned in Sec.~\ref{sec:pipeline}) to the fixed
\mbox{($p_{\textrm{FA}}=10^{-3},p_{\textrm{det}}<1)$} operating point of the
first stage of the method.
However, at low $p_{\textrm{FA}}$ it considerably improves $p_{\textrm{det}}$ for both CNNs.
It actually performs marginally better than $\Btrans^{\textrm{r}}$
below \mbox{$p_{\textrm{FA}}<10^{-5}$} for both types of signals:
by $\approx1$\% for rectangular and $\approx2$\% for exponentially decaying injections.
\begin{figure*}[t]
    \includegraphics[width=\textwidth]{figures/SNR_panel.pdf}
    \caption{Estimated SNRs from three different methods
    (one for each row) as a function of injected SNRs in real LIGO O2 data.
    From top to bottom, the
    estimated SNRs are $\rho_{\mathcal{F}_{\max}}^{\,\textrm{r}}$ estimated from
    $2\mathcal{F}_{\max}^{\,\textrm{r}}$, in the second row
    $\rhocnn^{\textrm{r/e}}$ is estimated from the CNNs trained on
    only synthetic data with windows matching that of the test set and lastly,
    in the third row $\rhocnn^{\textrm{r/e}}$ is estimated from the
    CNNs trained on both synthetic and real data. The injected signals are
    rectangular-shaped on the left column and exponentially decaying on the
    right column. The color scale (shared between all panels)
    indicates the durations of the injected signals.
    For guidance, we always plot the diagonal of the
    subplots, where ideally the points should align.}
    \label{fig:SNR_panel}
\end{figure*}

The improvement is not necessarily significant compared with counting uncertainty,
and in the exponential case where it seems larger we have also not compared
to the traditional statistics using exponential windows.
Still, in general such an improvement is indeed possible
because $\Btrans$ is not an optimal statistic in either case.
As discussed in Ref.~\cite{Prix:2009tq} (for CWs), a better detection statistic for a given signal population
can in general be obtained by marginalization over an appropriate amplitude prior,
as opposed to the unphysical prior that the maximization used for the \Fstat
(and derived statistics such as \Fstatmax and $\Btrans$)
corresponds to.
In addition, Ref.~\cite{PhysRevD.105.124007} has demonstrated how in particular for short data stretches
a more sensitive statistic than the standard \Fstat can be constructed,
mainly exploiting a reduction in noise degrees of freedom.
Therefore, it seems consistent that a CNN can also learn to behave better than the standard
statistics, and more similarly to the improved alternatives, in at least certain
parts of the tCW parameter space.

\subsection{Results of synthetic-trained CNNs on real data}
\label{sec:trainsynth_testreal}
In the previous section we have shown how CNNs can be used as a competitive
method against the standard detection statistics, at least in the context of
synthetic data.
We now evaluate the same models trained with synthetic data,
but now we test on real data. 
The corresponding CNN detection probabilities suffer additional losses,
compared to Fig.~\ref{fig:roc_synth},
of up to $7\%$ and $16\%$ for rectangular and exponential windows respectively.
Instead of showing these ROC curves,
we proceed directly to identifying and addressing the main reason for these losses.

For real-data injections,
we show in Fig.~\ref{fig:SNR_panel} the estimated $\rhocnn$
compared with the traditional estimator~\cite{Prix:2015cfs}
\begin{equation}
    \rho_{\mathcal{F}_{\max}}^{\textrm{r}} = \sqrt{2\mathcal{F}_{\textrm{max}}^{\,\textrm{r}} -4} \,,
    \label{eq:rho_2Fmax}
\end{equation}
both as functions of $\rho_{\textrm{inj}}$.
Different injection windows are covered in the two columns. Ideally, each estimator would
align all the points on the diagonals of these plots.

We see that $\rho_{\mathcal{F}_{\max}}^{\textrm{r}}$ (first row) aligns well with the
injected $\rho_{\textrm{inj}}$ for rectangular windows, but for exponentially decaying
signals there is a loss in recovered SNR (i.e. the points do not align with the
diagonal) because the injected window (exponential) and the search window
(rectangular) do not match. The loss in SNR is also enhanced by a 12-day gap in
the data, which we discuss in more detail in Appendix~\ref{sec:gaps}. 

In the second row, we see that the CNN trained on synthetic data
and rectangular windows recovers $\rho_{\textrm{CNN}}^{\textrm{r}}$
that mostly align with $\rho_{\textrm{inj}}$, but the loss
increases as the duration of the injected signal
decreases. The loss in SNR is
even more evident for $\rho_{\textrm{CNN}}^{\textrm{e}}$ on exponentially decaying signals.
Since both of these CNNs were
trained on synthetic data only, it is natural to expect that they are not robust
to detector artifacts that could contaminate the data, especially at shorter durations
\cite{LIGO:2021ppb,Keitel:2015ova}. We will now demonstrate that it helps to include real data
in the training set to mitigate this effect.

\subsection{Performance of training on real data}
\label{sec:synth2real}
\begin{figure}
    \includegraphics[width=\columnwidth]{figures/ROC_curves_trainmix_testreal.pdf}
    \caption{ROC curves with real data as the
    testing set and CNNs trained on both synthetic and
    real data. Note the vertical axis is zoomed in. Both panels use the same
    noise testing set, but the test signals are rectangular-shaped in
    the upper panel, and exponentially decaying in the bottom panel. The legend
    structure is the same as in Fig.~\ref{fig:roc_synth}, with the
    addition of two extra ROC curves in the exponential test case corresponding
    to the $2\mathcal{F}_{\max}^{\,\textrm{e}}$ and $\Btrans^{\textrm{e}}$
    statistics computed with the more expensive exponential window.}
    \label{fig:roc_synth2real}
\end{figure}

To improve sensitivity, we also trained CNNs using
real data, keeping the same architecture.
As mentioned before in Sec.~\ref{sec:training_strategy}, there are different ways one
can incorporate real data in the training set. We have tried different
implementations using the same total number of training epochs: training a model from scratch
on only real data, training a model on a mixture of synthetic and real data, and
taking the previous synthetic-only-trained model and continuing
its training on only real data (similar to a CL approach).

We find that training
exclusively on real data yields a good signal recovery, i.e. removes the SNR losses seen
in Fig.~\ref{fig:SNR_panel}, but overestimates the SNR of the noise samples,
leading to inferior ROC results overall. This effect is mitigated when training
with both synthetic and real data. Between
the two options of training on a mixture of both all at once, and
first training on synthetic and then on real data, the latter
performs best. More precisely,
when first training on synthetic and then on real data, we take the previously
trained CNNs on synthetic data for 200+1000 epochs (first CL stage + second CL
stage) and continue their training on only real data for another 200+1000 epochs.

As before, we train two models,
each on a single window type. In the third row
of Fig.~\ref{fig:SNR_panel} we show the recovered $\rhocnn^{\textrm{r/e}}$
from these networks (trained first on synthetic and then real data)
on real-data injections.
For rectangular injections, the loss in SNR is now
reduced compared to the CNNs with only synthetic training data.
The issue of more SNR loss for shorter signals is
largely resolved except for a few underestimated outliers with short durations:
the CNN is now able to recover both long and short signals well. In the exponential
case, $\rhocnn^{\textrm{e}}$ also aligns well with the injected $\rho_{\textrm{inj}}$,
with just a wider scatter.

The models trained on both synthetic and real data also perform well on real noise
samples, not producing overly loud outliers. ROC curves for this case are shown
in Fig.~\ref{fig:roc_synth2real}. For rectangular signals, sensitivities of
all the different statistics are quite similar to those on synthetic data
(top panel of Fig.~\ref{fig:roc_synth}). However, while for synthetic data the
two-stage filtering performance matched or exceeded that of the standard
detection statistics at low $p_{\textrm{FA}}$,
it now shows a marginal loss in the same regime with respect to the standard
statistics, but of only $\approx 1\%$.

Also, the CNN trained on exponential windows performs marginally better even with
rectangular window injections at the lowest values of $p_{\textrm{FA}}$. It was
noted by Ref.~\cite{Prix:2011qv} that also
$2\mathcal{F}_{\max}^{\textrm{\,e}}$ can outperform $2\mathcal{F}_{\max}^{\textrm{\,r}}$ on
rectangular signals. However, without larger studies on different configurations
and data sets it cannot be determined if the observed phenomenon in the CNNs is
related or just due to specifics of the training and test data sets.

On the other hand, for the exponential test set, $p_{\textrm{det}}$ for all
statistics has systematically worsened by $10\%$ to $15\%$ compared to the
synthetic results from Fig.~\ref{fig:roc_synth}. This could be because the weak
late-time portions of the signals are more difficult to pick up in real noise.
The CNNs lose relatively less detection power, meaning that the gap in
$p_{\textrm{det}}$ against the traditional statistics has narrowed down: at the lowest
$p_{\textrm{FA}}$, the difference in $p_{\textrm{det}}$ between
$\Btrans^{\textrm{r}}$ and $\rhocnn^{\textrm{e}}$ is down to $3\%$. 

Since this is our most complete test set, we also show the results of the more computationally expensive statistics computed with windows matching the injections, i.e. $2\mathcal{F}_{\max}^{\,\textrm{e}}$ and
$\Btrans^{\textrm{e}}$. To obtain these we used the GPU implementation
from Ref.~\cite{Keitel:2018pxz}. Their ROC curves improve by $2-3\%$ over the statistics with
rectangular windows. 

For \mbox{$p_{\textrm{FA}}<10^{-5}$}
the two-stage filtering ROC curve very closely matches that of $\Btrans^{\textrm{e}}$.
It cannot yield higher $p_{\textrm{det}}$ (as it did in the synthetic case)
on this specific data set,
because the template corresponding to the loudest $\Btrans^{\textrm{e}}$ outlier
also passes the first-stage $\rho_{\textrm{CNN}}^{\textrm{r/e}}$ threshold.
But it does converge to the sensitivity of the traditional statistic
when far enough to the left of the ROC to go below the plateau level
set by the first stage.
As discussed before, it also takes 80 times less time even with the GPU implementation of Ref.~\cite{Keitel:2018pxz}.

It is important to remember that ROC performance depends on the population of
signal candidates considered. In this case we use signals with
$\rho_{\textrm{inj}} \in [4,40]$. In Sec.~\ref{sec:upper_limits} we will revisit
the same real-data noise set but with a different injection set matching the one
used in Ref.~\cite{Keitel:2019zhb} for establishing upper limits.

