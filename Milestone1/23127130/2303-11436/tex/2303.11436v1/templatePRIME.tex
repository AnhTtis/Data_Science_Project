\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Mind meets machine: Unravelling GPT-4's cognitive psychology}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Mind meets machine: Unravelling GPT-4's cognitive psychology
%%%% Cite as
%%%% Update your official citation here when published 
% \thanks{\textit{\underline{Citation}}: 
% \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Sifatkaur \\
  Department of Psychology, Nowrosjee Wadia College \\
  Pune, India\\
  \texttt{sifatkaurd13@gmail.com} \\
  %% examples of more authors
   \And
  Manmeet Singh \\
  Indian Institute of Tropical Meteorology \\
  Pune, India\\
  \texttt{manmeet.cat@tropmet.res.in} \\
  \And
  Vaisakh SB \\
  Indian Institute of Tropical Meteorology \\
  Pune, India\\
  \texttt{vaisakh.sb@tropmet.res.in} \\
  \And
  Neetiraj Malviya \\
  Defence Institute Of Advanced Technology \\
  Pune, India\\
  \texttt{neetirajmalviya@gmail.com} \\
  % \And
  % Souvik Paul \\
  % Indian Institute of Tropical Meteorology \\
  % Pune, India\\
  % \texttt{manmeet.cat@tropmet.res.in} \\
  %% examples of more authors
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
Commonsense reasoning is a basic ingredient of intelligence in humans, empowering the ability to deduce conclusions based on the observations of surroundings. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans such as medical exam, bar exam and others has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Though, the GPT-4 paper has shown performance on some common sense reasoning tasks, a comprehensive assessment of GPT-4 on common sense reasoning tasks, particularly on the existing well-established datasets is missing. In this study, we focus on the evaluation of GPT-4's performance on a set of common sense reasoning questions from the widely used CommonsenseQA dataset along with tools from cognitive psychology. In doing so, we understand how GPT-4 processes and integrates common sense knowledge with contextual information, providing insight into the underlying cognitive processes that enable its ability to generate common sense responses. We show that GPT-4 exhibits a high level of accuracy in answering common sense questions, outperforming its predecessor, GPT-3 and GPT-3.5. We show that the accuracy of GPT-4 on CommonSenseQA is 83 \% and it has been shown in the original study that human accuracy over the same data was 89 \%. Although, GPT-4 falls short of the human performance, it is a substantial improvement from the original 56.5 \% in the original language model used by the CommonSenseQA study. Our results strengthen the already available assessments and confidence on GPT-4's common sense reasoning abilities which have significant potential to revolutionize the field of AI, by enabling machines to bridge the gap between human and machine reasoning.
%Here, we introduce DeepClouds.ai which consists of a 3D-UNET emulating the outputs from a rising cloud DNS experiment. 

\end{abstract}


% keywords can be removed
% \keywords{Urban downscaling, Deep learning, Smart city}


\section{Introduction}
\label{sec:intro}

Language models have come a long way since the first statistical models for modelling language were introduced. With the advent of deep learning and the availability of large amounts of data, recent years have seen a rapid evolution of language models that have achieved human-like performance on many language tasks. Large Language Models (LLMs) are a type of artificial intelligence framework that have garnered significant attention in recent years due to their remarkable language processing capabilities \cite{harrer2023attention}. These models are trained on vast amounts of text data and are able to generate coherent, human-like responses to natural language queries. One of the key features of LLMs is their ability to generate novel and creative responses to text-based prompts, which has led to their increasing use in fields such as chatbots, question answering systems, and language translation. The Transformer architecture has emerged as one of the most prominent architectures for LLMs. The architecture was first introduced by \cite{vaswani2017attention} wherein the authors introduced a new approach to sequence processing based on the self-attention mechanism, which replaced the traditional Recurrent Neural Network (RNN) architecture. The Transformer architecture uses self-attention to enable the model to selectively focus on the most relevant parts of the input sequence, allowing for efficient and accurate generation of the output sequence. The Transformer architecture has demonstrated remarkable success in a wide range of natural language processing tasks, including language modeling, machine translation, and question answering systems, among others. The use of self-attention has been a key factor in this success, as it allows for more efficient and accurate modeling of long-range dependencies within the input sequence, resulting in better performance compared to traditional RNN-based models. The Transformer architecture has been a significant development in the field of LLMs, and its success has spurred further research and innovation in the field.

LLMs have demonstrated impressive performance on a wide range of language tasks, including language modeling, machine translation, sentiment analysis, and text classification. For example, the OpenAI GPT-3 model having 175 billion parameters and has shown remarkable capabilities, including generating coherent and relevant text, completing sentences, and even composing poems. These capabilities have led to the increased use of LLMs in various fields, including language-based customer service, virtual assistants, and creative writing. One of the key areas measuring intelligence in humans, other species and machines is the common sense reasoning. At present, there is no universally accepted definition of "common sense". There are several tasks that are considered to be the benchmarks for common sense reasoning. Some of them are text interpretation, computer vision, planning and reasoning. For common sense psychology to work, we rely on a complex and potent social practise: the attribution and assessment of thoughts and actions \cite{madaan2022text}. In addition to reflecting aspects of cognition and behaviour, the commonsense notions also appear to reflect environmental facts and social norms and customs. As part of maturing into social creatures, we acquire the ability to make sense of, or interpretation of, one another. The scientific psychology of cognition and behaviour, a relatively recent innovation, focuses primarily on the information-processing mechanisms and activities that characterise human cognitive and behavioural capabilities.

When responding to questions, people draw upon their experience and intuition in addition to their understanding of logical relationships, causal chains, scientific principles, and cultural norms. If we ask, "Where was Simon when he heard the lawn mower?" we can deduce that it is located near Simon, possibly outside and at street level. This information may seem elementary to a person, yet it is a good test of common sense reasoning. Researchers have attempted to create systems that could use natural language to reason about their surroundings \cite{mccarthy1959basis} or that could use a world model to get a more profound comprehension of spoken language \cite{winograd1972understanding}. Normal thinking significant advancements have been achieved in four areas of automated commonsense reasoning: reasoning about taxonomic categories, time, actions, and change, and the sign calculus. Each of these fields has a well-established theory that may rationalise a variety of deductions made. Knowledge representation and automated reasoning about timings, durations, and time intervals is a largely addressed problem. The idea of action, events, and change is another well-understood part of commonsense thinking. Qualitative reasoning is a sort of commonsense thinking that has been studied extensively and at its most fundamental level, concerns the trend of change in related quantities. Commonsense thinking is the capacity to reason deductively, inductively, and decisively in light of ordinary information and experience. Making sense of our surroundings and figuring out how to get around in it requires integrating data from various sources. Since it allows us to reason about the world in a malleable and adaptable fashion, commonsense reasoning is an essential part of human cognition. We may deduce that touching a hot stove is harmful from our own experiences with heat and fire, and we can use social standards to determine that wearing pyjamas to a business dinner is unacceptable.

Thinking with common sense requires a wide range of abilities, from observation and memory to focus and deliberation. In addition, it has strong links to other cognitive skills including language comprehension and problem-solving. Interest in creating AI systems capable of reasoning like humans has been on the rise in recent years. Natural language processing, autonomous cars, and robotics are just few of the many fields that might benefit from the implementation of such systems. Yet, it's still a major obstacle for AI researchers to create systems with common sense at par with humans. Many different types of thinking are involved in the production of common sense. Perception is the mental process through which we take in and make sense of data from our senses. Perception is the foundation of common sense since it is how we take in and make sense of the world around us. Being able to remember things and recall them later is what we mean when we talk about memory. The ability to use one's common sense comes from having the ability to form conclusions about the world around oneself based on one's prior experiences and knowledge. Focused mental energy that may be directed on a specific task at hand, without letting irrelevant details slip through the cracks is one such ability. Paying close attention allows you to focus on the data that matters most and act wisely as a result. To make a decision means to select one course of action over another in light of personal values and interests. Choices grounded in common sense are grounded in decision-making because they are both effective and efficient. Processing linguistic input in order to generate meaningful output is known as language processing. In order to understand and interact with the world around us, common sense depends on the ability to comprehend and use language. Skills in recognising and resolving issues are what we mean when we talk about problem-solving. Everyday issues and difficulties can only be overcome via the use of common sense and the ability to problem solve. 

Commonsense, in its whole, is a multifaceted cognitive process that draws upon a wide range of other cognitive abilities to help us make sense of the world. GPT-4 \cite{gpt4openai} has tested the HellaSwag \cite{zellers2019hellaswag} and WinoGrande \cite{sakaguchi2021winogrande}. Hellaswag entails the task of finishing a sentence and WinoGrande involves identifying the correct noun for the pronouns in a sentence. Other tasks and standardized datasets \cite{li2022systematic} which test the psychology are needed in order to perform a comprehensive assessment of commonsense reasoning for GPT-4. Moreover GPT-4 needs to go through complex reasoning tasks than just predicting the last word of the sentence such as in Hellaswag, to emerge as a model capable of high-level intelligence. \cite{shiffrin2023probing} note that SuperGLUE \cite{wang2019superglue}, CommonsenseQA \cite{talmor2018commonsenseqa}, MATH \cite{hendrycks2021measuring} and HANS \cite{mccoy2019right} are four such datasets that need to be tested for a comprehensive commonsense reasoning evaluation of AI models. In this study, we evaluate the performance of GPT-3.5 and GPT-4 on the CommonsenseQA dataset. This is a work in progress and we are performing continuous tests with the other datasets as suggested by \cite{shiffrin2023probing}. Our study can be used to build up on higher-order psychological tests using GPT-4. 


\section{Datasets and Methodology}
In this study, we use four datasets to test the commonsense reasoning of GPT-4. This version shows the results from CommonsenseQA, which is a question answering dataset consisting of 12, 247 examples. The goal of the dataset is to evaluate the commonsense knowledge using CONCEPTNET to generate difficult questions. The language model tested in the CommonsenseQA paper has an accuracy of 55.9 \% whereas the authors report that human accuracy on the dataset is around 89 \%.

\begin{figure*}
  \centering
  \includegraphics[width=0.95\linewidth]{psych_gpt.png}

  \caption{Examples of sample prompts and the respective responses of GPT-3.5 and GPT-4 on CommonsenseQA dataset}
  \label{fig-1}
\end{figure*}
%\section{Methodology}
We test the CommonsenseQA data to GPT-4 and GPT-3.5 using the ChatGPT-Plus offered by OpenAI. The prompts and responses from the two models are shown in the supplementary. We evaluate these models using different accuracy metrics which are shown in the results section.


\section{Results}
Figure 1 shows the output of prompts for GPT-3.5 and GPT-4 models. It can be seen that GPT-3.5 does not provide the answer options whereas GPT-4 provides the answer options as the predictions. Moreover, in certain prompts, GPT-3.5 and GPT-4 provide multiple options as outputs. Interestingly, sometimes both the models are unable to provide any answer from the available options and explicitly mention the same. Comparing the performance (Figure 1) of GPT-3.5 and GPT-4 on the CommonSenseQA data, we find that GPT-4 has an accuracy of 83.2 \% whereas GPT-3.5 has an accuracy of 66 \%. The human performance on this dataset has been found out to be ~ 89 \%, and GPT-4 is hitherto the best model to reach near the human skill on common sense reasoning.

\begin{figure*}
  \centering
  \includegraphics[width=0.6\linewidth]{accuracy_gpt4_gpt3.5.png}

  \caption{Accuracy of GPT-3.5 and GPT4 on CommonSenseQA}
  \label{fig-2}
\end{figure*}
\section{Conclusions}
GPT-4, which is a state-of-the-art large language model, is a revolution in the field of psychology since it gives psychologists unprecedented resources to use in their studies and work. This sophisticated AI model offers psychologists and psychiatrists to learn more about the human mind and come up with novel treatment theories and approaches. It provides an avenue for improved efficacy of psychological therapies and allowing professionals to spend more time with clients, leading to deeper and more fruitful therapeutic bonds. The potential applications of GPT-4 can only be realized if the model is thoroughly tested on basic tests of reasoning and cognition. Commonsense reasoning is the fundamental cognition task which enables the humans to perform various activities \cite{aher2022using} in their personal and professional lives. We show that the performance (83 \%) of GPT-4 greatly surpasses the language model used in the original study on CommonsenseQA (around 56.5 \%) and is close to the human accuracy (89 \%) which can make it a tool of day-to-day utility for psychologists. This development can lead to cascading benefits in addressing the mental health challenges faced by today's society.

% We have presented a deep learning based approach to model atmospheric formaldehyde without using complex chemical reaction mechanisms and minimum input variables. We develop a trustworthy model of estimating formaldehyde from the inputs directly involved in the formation or decomposition of atmospheric HCHO. Atmospheric temperature at 2 m height above surface and incoming solar radiation were used as meteorological input variables. Leaf area index was used as proxy for biogenic emission. Methane, isoprene and higher VOCs were considered as chemical precursor of HCHO and used as input variable. Our results demonstrate superior skills relative to the dynamical atmospheric chemistry modelling.

% We also estimate the causal linkages and find that using only meteorology as input resulted in correlation coefficient of 0.3 between modelled and satellite HCHO. Although methane is very important for HCHO formation, using methane as input resulted in poor results. Probably higher atmospheric lifetime (hence low reaction) compared to other chemical input makes it an unsuitable choice as an input. Simulation with meteorology and isoprene gives most promising results with overall correlation coefficient of 0.47. 

% In future we plan to use advanced deep learning algorithms for HCHO modelling and further improve the performance. We also plan to study the effect of different input variable on model output and finally finding the optimal model with causal explanation. Our aim is to develop a low resource intensive, optimised alternate to dynamical model which can be used by relevant research groups without extensive domain knowledge of atmospheric chemistry. 






% \begin{table*}
%   \centering
%   \begin{tabular}{@{}lc@{}}
%     \toprule
%     Variable combination & Pearson correlation coefficient over South Asia \\
%     \midrule
%     only meteorology & 0.30 \\
%     meteorology + lai & 0.19 \\
%     meteorology + ch4 & -0.03\\
% meteorology + c5h8 & 0.47 \\
%     meteorology + ch4 + c5h8 + voc & -0.39 \\
%     meteorology + voc & 0.31\\
% meteorology + ch4 +c5h8 & -0.47 \\
%     meteorology + ch4 + c5h8 + lai & -0.33 \\
%     meteorology + ch4 + c5h8 +lai + voc & 0.14\\
    
%     \bottomrule
%   \end{tabular}
%   \caption{Table for total correlation coefficient between different modelled and satellite formaldehyde. The simulation with meteorology and isoprene as input variable gives the best correlation.}
%   \label{tab:example}
% \end{table*}


% \begin{table*}
%   \centering
%   \includegraphics[width=1.0\linewidth]{hcho_daily_corr.pdf}

%   \caption{Results.   Ours is better.}
%   \label{tab:daily}
% \end{table*}





    



% \section{Headings: first level}
% \label{sec:headings}

% \lipsum[4] See Section \ref{sec:headings}.

% \subsection{Headings: second level}
% \lipsum[5]
% \begin{equation}
% \xi _{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\theta)= {\frac {\alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\sum _{i=1}^{N} \sum _{j=1}^{N} \alpha _{i}(t)a^{w_t}_{ij}\beta _{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}
% \end{equation}

% \subsubsection{Headings: third level}
% \lipsum[6]

% \paragraph{Paragraph}
% \lipsum[7]

% \section{Examples of citations, figures, tables, references}
% \label{sec:others}
% \lipsum[8] \cite{kour2014real,kour2014fast} and see \cite{hadash2018estimate}.

% The documentation for \verb+natbib+ may be found at
% \begin{center}
%   \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
% \end{center}
% Of note is the command \verb+\citet+, which produces citations
% appropriate for use in inline text.  For example,
% \begin{verbatim}
%   \citet{hasselmo} investigated\dots
% \end{verbatim}
% produces
% \begin{quote}
%   Hasselmo, et al.\ (1995) investigated\dots
% \end{quote}

% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}


% \subsection{Figures}
% \lipsum[10] 
% See Figure \ref{fig:fig1}. Here is how you add footnotes. \footnote{Sample of the first footnote.}
% \lipsum[11] 

% \begin{figure}
%   \centering
%   \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
%   \label{fig:fig1}
% \end{figure}

% \subsection{Tables}
% \lipsum[12]
% See awesome Table~\ref{tab:table}.

% \begin{table}
%  \caption{Sample table title}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
%   \label{tab:table}
% \end{table}

% \subsection{Lists}
% \begin{itemize}
% \item Lorem ipsum dolor sit amet
% \item consectetur adipiscing elit. 
% \item Aliquam dignissim blandit est, in dictum tortor gravida eget. In ac rutrum magna.
% \end{itemize}


% \section{Conclusion}
% Your conclusion here

% \section*{Acknowledgments}
% The authors thank Harsh Kamath and Ting-Yu Dai for their constructive comments.

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{templatePRIME}  


\end{document}
