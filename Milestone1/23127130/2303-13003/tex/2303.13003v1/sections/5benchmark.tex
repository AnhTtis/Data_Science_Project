\section{Benchmark of PTQ Reliability}

We have presented the process for measuring the reliability of PTQ and systematically evaluated the impact of each step in the PTQ workflow on reliability. 
We have found that PTQ performance varies greatly across different networks and tasks, and it is not possible for us to present and analyze each of them in this paper. 
In addition, there are still many aspects of PTQ reliability that require further analysis and exploration. 
Therefore, we have developed a framework that can test the PTQ reliability of different networks according to the proposed approach in this paper.
As shown in Figure~\ref{im_overview}, the framework provide a standardized approach for evaluating the reliability of PTQ by assessing both the average performance and performance on various cetegories.
We have used this framework to test the PTQ reliability performance on various tasks, PTQ methods, and network architectures\footnote{The results are provided in supplementary materials.}, which can serve as a benchmark.
Furthermore, the framework can be extended to evaluate the reliability of PTQ on other datasets or tasks, as well as to investigate the effect of different calibration metrics or other parameters on the quantization accuracy. 
We hope that the proposed framework and the benchmark can contribute to the development and improvement of PTQ and other quantization methods, and ultimately enable more efficient deployment of deep learning models on more applications.