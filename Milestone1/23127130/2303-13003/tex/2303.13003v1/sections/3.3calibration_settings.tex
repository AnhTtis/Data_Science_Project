\subsection{Quantization Settings}
\label{sec_quantization_settings}

Once the calibration dataset is collected, the next step is to select appropriate quantization settings. 
% The utilization of non-uniform quantization necessitates specific hardware for its implementation and is not commonly employed. 
In this paper, we only examine the effects of bit-width configurations on uniform quantization.
Mixed-precision quantization and non-uniform quantization are expected to be the future directions of research.

\begin{table}[t]
\centering
\caption{The influence of quantization settings. We report mean±std over 50 runs with different random seeds. W6A6 means 6-bit weight and 6-bit activation.}
\label{table_quantization_settings}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}ccccc@{}}
\toprule
Task     & \multicolumn{2}{c}{CIFAR-10 ResNet20} & \multicolumn{2}{c}{ImageNet MobilenetV2} \\
bit-width & W6A6              & W4A4              & W8A8                & W6A6               \\ \midrule
Average  & 90.66±0.08        & 88.01±0.21        & 72.0±0.03           & 70.2±0.07          \\
Class 0  & 90.14±0.27        & 92.18±0.43        & 84.0±0.00           & 82.8±1.27          \\
Class 1  & 95.77±0.19        & 93.22±0.45        & 74.1±1.44           & 70.6±1.13          \\
Class 2  & 88.54±0.30        & 84.55±0.61        & 76.4±1.00           & 74.3±2.53          \\
Class 3  & 82.64±0.51        & 76.42±0.69        & 77.4±0.90           & 74.7±1.82          \\
Class 4  & 90.76±0.27        & 86.56±0.61        & 62.4±1.44           & 60.6±1.89          \\
Class 5  & 85.51±0.33        & 81.40±0.67        & 95.9±0.39           & 95.2±1.38          \\
Class 6  & 92.14±0.28        & 94.19±0.45        & 94.0±0.00           & 85.6±1.56          \\
Class 7  & 93.19±0.29        & 89.52±0.46        & 62.0±0.28           & 65.7±2.09          \\
Class 8  & 93.15±0.21        & 88.84±0.70        & 82.0±0.00           & 79.9±1.60          \\
Class 9  & 94.82±0.21        & 93.23±0.43        & 93.6±0.77           & 91.4±1.56          \\ \bottomrule
\end{tabular}
}
\end{table}

% We use multiple random seeds to sample the calibration dataset and perform quantization under different settings. 
We conducted experiments by setting the same bit-width for all layers in the network and tested the prediction accuracy across 50 trials.
The results are demonstrated in Table~\ref{table_quantization_settings}.
We observe that the bit-width have a significant impact on the performance of quantized networks. 
Specifically, a higher bit-width results in not only more accurate but also more stable quantized networks. 
For example, the mean and std of average prediction accuracy is 36.6±0.06 on W8A8, while that of W6A6 is 31.4±0.41 for YOLOv5s quantization. 
% In higher bit-width, power-of-two (PoT) quantization has similar prediction accuracy as uniform quantization and it results in smaller standard deviation on both average accuracy and individual class accuracy. 
% For example, the standard deviation of PoT is 0.05, while that of uniform quantization is 0.1 on ResNet20 W6A6 quantization. 
% While in lower bit-width, PoT is worse than uniform quantization in both prediction accuracy and the stability.

We also observe that the performance of individual classes varies from each other.
Some classes experience a significant drop in prediction accuracy, while others do not. 
For example, Class 6 of ImageNet experience a decrease of more than 6\% in prediction accuracy from W8A8 to W6A6, while Class 5 of ImageNet experience a decrease of less than 1\%. 
% Additionally, some classes experience significant drops from W8A8 to W6A6, while others only drop from W6A6 to W4A4. 
We think this is because different classes require different dynamic ranges to achieve a certain prediction accuracy. 
The reduced number of bits used for representing the network's parameters leads to a loss of information and reduced dynamic range.
When the bit-width drops, the dynamic range drops significantly. 
Consequently, the accuracy of some classes may decrease significantly.

It is worth noting that even a slight decrease in average prediction accuracy can result in substantial drops in both their accuracy and stability on certain categories. 
In order to maintain prediction accuracy and stability of quantization, it is important to exercise caution when decreasing the bit-width.


