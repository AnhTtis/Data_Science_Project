\subsection{Construction of Calibration Dataset}
\label{sec_calibration_dataset}

%当前想到的三个因素：1.inter-class bias 2. intra-class bias 3. numbers of calibration samples

% The accuracy of PTQ heavily depends on the quality of the calibration dataset used. 
The calibration dataset is used to estimate the distribution of the activations in the network we want to quantize. 
If the calibration dataset fails to capture the statistical characteristics of the real-world data, the accuracy of the quantized network may decrease, since the quantization parameters are derived based on the distribution of the activations.
In this subsection, we analyze the impact of constructing a calibration dataset on the reliability of PTQ. 
We consider three factors that can affect the distribution of the calibration dataset: noise data, inter-class bias and dataset size.

% It should be representative of the data that the network is expected to encounter during inference. 
% The distribution of the calibration dataset should be similar to the real distribution. 
% The quantization parameters heavily depends on the quality of the calibration dataset used, influencing the prediction accuracy of the quantized network.

% \begin{table}[]
% \caption{The influence of noise data. We report the mean±std over 50 runs.}
% \label{table_noise_data}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{@{}ccccc@{}}
% \toprule
% Noise   & 1\%        & 5\%        & 10\%       & 50\%       \\ \midrule
% \multicolumn{5}{c}{CIFAR10 ResNet20 W4A4}               \\ \midrule
% Average & 87.9±0.2  & 87.5±0.21 & 87.3±0.23 & 85.8±0.2  \\
% Class 0 & 92.3±0.41 & 92.7±0.45 & 92.8±0.52 & 93.3±0.47 \\
% Class 1 & 93.3±0.42 & 93.5±0.52 & 93.6±0.46 & 93.3±0.42 \\
% Class 2 & 84.3±0.57 & 83.8±0.7  & 83.3±0.85 & 81.9±0.63 \\
% Class 3 & 76.0±0.66 & 75.1±0.73 & 74.3±0.91 & 72.9±0.76 \\
% Class 4 & 86.6±0.72 & 85.8±0.66 & 85.2±0.69 & 81.6±0.71 \\
% Class 5 & 81.4±0.58 & 80.8±0.63 & 80.4±0.52 & 80.3±0.69 \\
% Class 6 & 94.2±0.43 & 94.1±0.49 & 94.1±0.35 & 93.4±0.42 \\
% Class 7 & 89.6±0.5  & 89.2±0.59 & 89.2±0.53 & 87.6±0.59 \\
% Class 8 & 88.3±0.57 & 87.2±0.62 & 86.4±0.63 & 80.9±0.68 \\
% Class 9 & 93.3±0.41 & 93.3±0.4  & 93.3±0.36 & 92.8±0.47 \\ \midrule
% \multicolumn{5}{c}{ImageNet ResNet18 W6A6}              \\ \midrule
% Average & 70.3±0.05 & 70.3±0.05 & 70.2±0.06 & 70.2±0.05 \\
% Class 0 & 80.9±1.07 & 81.6±1.33 & 82.0±1.41 & 81.3±0.95 \\
% Class 1 & 69.1±2.16 & 68.8±2.04 & 68.4±1.96 & 68.9±1.66 \\
% Class 2 & 73.6±1.66 & 73.6±1.82 & 73.1±2.05 & 72.4±1.82 \\
% Class 3 & 85.4±1.02 & 85.4±1.06 & 85.3±1.1  & 85.6±0.83 \\
% Class 4 & 52.0±1.79 & 51.6±1.7  & 51.6±1.44 & 52.1±1.7  \\
% Class 5 & 96.0±0.0  & 95.9±0.47 & 95.8±0.65 & 95.8±0.54 \\
% Class 6 & 86.2±0.88 & 85.9±1.09 & 85.6±0.92 & 86.8±1.33 \\
% Class 7 & 69.8±1.82 & 69.8±1.89 & 69.8±1.48 & 69.5±1.9  \\
% Class 8 & 82.6±0.92 & 82.3±0.8  & 82.3±0.93 & 82.4±0.8  \\
% Class 9 & 93.2±1.12 & 93.2±1.06 & 93.2±1.19 & 92.4±1.26 \\ \bottomrule
% \end{tabular}
% }
% \end{table}

\begin{figure}[tbp] % h:here 当前位置 % b bottom % t top % p 浮动
    \centering
    \includegraphics[width=0.5\textwidth]{ims/noise.pdf} %ims/xx.png
    \caption{Influence of noisy calibration data. This figure plots the relative performance change to the clean case with varying data noise amounts. The change values are demonstrated in different colors (red means accuracy increment while blue means decrement). We execute 50 trails for each percentage of noise.}
    \label{im_noise_data}
\end{figure}


\textbf{Noise data} refers to data samples that are not representative of the underlying distribution of the data. 
Including noise data in a calibration dataset can have a negative impact on PTQ, as the noise data can bias the distribution of activation and lead to inaccurate quantization parameters. 
To assess the impact of noise data, we construct the calibration dataset with some images sampled from training set and some randomly generated noise images.

Figure~\ref{im_noise_data} demonstrates the prediction accuracy mean value change and standard deviation change comparing with experiments without noise on the prediction accuracy.
We observe that increasing the percentage of noise data leads to a reduction on the average accuracy.
The more noise data, the more average accuracy drop.
However, the impact on individual classes is much larger than average accuracy. 
We observe that most of the classes are vulnerable to the noise data.
For instance, the prediction accuracy of Class 8 of CIFAR-10 drops significantly when noise data is introduced. 
While some classes, such as Class 1 and Class 9 of CIFAR-10, exhibit consistent predictive accuracy. 
Additionally, it is noteworthy that the noise data changes the standard deviation of individual classes, while the standard deviation on Average not changes too much.

% Table~\ref{table_noise_data} shows the results of adding a certain proportion of noise data.
% 从表格中可以看出，The average accuracy is decreasing as increasing the number of noise samples.
% However, the effect on individual class is not the varies from class to class.
% Some classes kept the prediction accuracy, such as the Class 0 and 1 of CIFAR-10, while some classes are vulnerable to the noise data.
% For example, the prediction accuracy of Class 3, 4 on CIFAR-10 dramatically drops.
% We also not observe a significant change in standard variance after adding noise data.

\textbf{Inter-class bias} refers to the presence of differences in the distribution between classes. 
To evaluate the impact of inter-class bias on the quantization process, we construct unbalanced calibration datasets, where the number of samples from each class is different.
We build the unbalanced dataset by increasing the sampling probability of a specific class. 
Specifically, we constructed calibration datasets in which the sampling probability of a certain class was set to 50\%, while the remaining classes were included with equal probability.

\begin{figure}[tbp] % h:here 当前位置 % b bottom % t top % p 浮动
    \centering
    \includegraphics[width=0.5\textwidth]{ims/data_cls_bias.pdf} %ims/xx.png
    \caption{Influence of unbalanceed calibration datasets. The reported top-1 accuracy is averaged over 50 runs with different random seeds. The prediction accuracy change is demonstrated in different colors (red means increment and blue means decrement). The standard deviation change is annotated as text (positive means increment and negative means decrement).}
    \label{im_data_cls_bias}
\end{figure}


% Figure~\ref{im_data_cls_bias} shows the results using unbalanced calibration datasets.
% We observe that increasing the number of different classes results in decrement on average prediction accuracy in most cases.
% The prediction accuracy of most individual classes decrease, while some cases slightly improve.
% Some classes are more vulnerable to the unbalanced calibaration dataset.
% For example, the prediction accuracy of class 8 in CIFAR10 will significantly decrease.
% We also observe that increasing the number of a certain class cannot improve the prediction accuracy on this class.
% Furthermore, we don't observe a significant change in standard deviation.
% Specifically, while adding some classes can improve the average accuracy, the unbalanced calibration datasets can lead to a reduction in the average accuracy in most cases.

Figure~\ref{im_data_cls_bias} depicts the results obtained using unbalanced calibration datasets. 
The figure demonstrates that increasing the number of different classes leads to slight change on the average prediction accuracy, while significant change on the prediction accuracy of individual classes.
% Specifically, the prediction accuracy of most individual classes reduces, while some cases slightly improve. 
The results also indicate that some classes are more susceptible to the unbalanced calibration dataset. 
For instance, the prediction accuracy of Class 6 on ImageNet significantly decreases. 
It is also worth noting that increasing the number of a certain class does not necessarily improve the prediction accuracy on this class. 
In addition, we note that the standard deviation of the average prediction accuracy almost remains unchanged, whereas the standard deviation of individual classes displays a marked variation.
The distribution of various classes may vary, and an unbalanced dataset can alter the distribution of the calibration dataset. 
Inter-class bias can impact the selection of quantization parameters, leading to variations in prediction accuracy. 
However, the average prediction accuracy remains relatively stable, indicating its robustness to changes in quantization parameters. 
Conversely, significant variations in the prediction accuracy of individual classes suggest their vulnerability to perturbations caused by changes in the calibration dataset.
% Overall, these results demonstrate the importance of using a balanced calibration dataset to achieve high PTQ reliability.
% In the unbalanced dataset that we constructed, samples were concentrated in a specific class, which reduced the diversity of the data. 
% This in turn led to a decrease in the generalization performance of the chosen quantization parameters, reducing both the average accuracy and the accuracy of other classes.
% Specifically, in most cases, the distribution of the calibration dataset is different from the distribution of the test dataset, there is a reduction in both the average accuracy and the accuracy of other classes. 
% However, in rare cases where the distribution of the selected class is similar to that of another class, an improvement in prediction accuracy for that class can be observed.

% There are variations in the distribution of different classes, which may impact the chosen of quantization parameters and cause the change of prediction accuracy. 
% In most cases, there is difference between in the distribution of the selected class and the test distribution, resulting in a reduction in both average accuracy and accuracy of other classes.
% In a few cases, the distribution of the selected class is similar to another class, it can lead to an improvement in prediction accuracy of that class.
% In most cases, the overall performance tends to be lowered due to the presence of inter-class bias.
% By ensuring each class has an equal number of samples (balance), we can prevent one class from dominating the calibration process, thereby reducing the probability of inter-class bias occurring.
% Although it only achieve a sub-optimal result, using a balanced dataset is a recommended approach for achieving reliable PTQ results because it is hard to select the classes having the proper distribution.

% However, we have a chance to select samples from each class in the calibration dataset.

\begin{table}[tb]
\centering
\caption{The influence of different numbers of calibration samples. We report the mean±std over 50 runs.}
\label{table_dataset_size}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}ccccc@{}}
\toprule
Dataset Size & 1         & 32        & 256       & 1024      \\ \midrule
\multicolumn{5}{c}{CIFAR-10 ResNet20 W4A4}                   \\ \midrule
Average      & 88.0±0.53 & 88.0±0.30 & 88.1±0.18 & 88.0±0.17 \\
Class 0      & 92.6±0.81 & 92.3±0.41 & 92.1±0.52 & 92.0±0.45 \\
Class 1      & 92.3±0.80 & 93.0±0.52 & 93.1±0.33 & 93.1±0.36 \\
Class 2      & 83.2±1.09 & 84.4±0.75 & 84.8±0.57 & 84.8±0.48 \\
Class 3      & 77.8±1.70 & 76.6±0.87 & 76.3±0.69 & 76.2±0.77 \\
Class 4      & 86.5±1.22 & 86.7±0.58 & 86.8±0.47 & 86.9±0.50  \\
Class 5      & 81.7±1.65 & 81.5±0.71 & 81.6±0.50 & 81.5±0.61 \\
Class 6      & 94.1±0.50 & 94.2±0.42 & 94.1±0.45 & 94.0±0.38 \\
Class 7      & 89.4±0.69 & 89.4±0.57 & 89.7±0.53 & 89.6±0.47 \\
Class 8      & 88.9±1.23 & 88.9±0.99 & 88.7±0.58 & 88.6±0.46 \\
Class 9      & 93.5±0.50 & 93.5±0.47 & 93.3±0.41 & 93.3±0.44 \\ \midrule
\multicolumn{5}{c}{Imagenet MobilenetV2 W6A6}                \\ \midrule
Average      & 70.0±0.38 & 70.2±0.06 & 70.2±0.05 & 70.2±0.07 \\
Class 0      & 82.8±1.44 & 83.1±1.34 & 83.3±1.48 & 82.8±1.13 \\
Class 1      & 68.8±2.22 & 70.4±1.34 & 70.2±1.40 & 70.4±1.15 \\
Class 2      & 72.8±2.71 & 74.4±2.58 & 74.6±2.31 & 73.8±2.77 \\
Class 3      & 74.6±2.33 & 73.6±2.04 & 74.8±1.83 & 74.3±2.28 \\
Class 4      & 61.2±2.11 & 60.8±1.92 & 61.2±1.65 & 60.8±1.79 \\
Class 5      & 95.1±1.28 & 95.2±1.12 & 95.0±1.15 & 95.2±1.26 \\
Class 6      & 84.4±2.53 & 85.3±1.90 & 85.4±1.79 & 85.2±1.21 \\
Class 7      & 63.5±2.49 & 64.6±2.06 & 65.9±2.12 & 65.4±2.37 \\
Class 8      & 80.8±1.54 & 80.0±1.52 & 80.1±1.92 & 80.0±1.62 \\
Class 9      & 91.8±1.90 & 91.7±1.74 & 91.3±1.68 & 91.6±1.80  \\ \bottomrule
\end{tabular}
}
\end{table}


\textbf{The size of calibration dataset} is an important factor for PTQ. 
It is generally understood that small dataset can lead to inaccurate quantization parameters, while a large dataset can increase the accuracy of the quantization, resulting in a stable and reliable result.
To evaluate the number of calibration samples, experiments with different numbers of samples are be conducted.
As shown in Table~\ref{table_dataset_size}, we observe that larger dataset size results in higher and more stable average prediction accuracy, aligning with the generally accepted understanding. 
As the calibration dataset size increases from 1 to 32, there is a significant reduction in the variance of prediction accuracy.
For instance, when size increases from 1 to 32, the standard deviation on Class 4 of CIFAR-10 decreases from 1.22 to 0.58.

However, we observe that increasing the dataset size beyond 32 has little effect on reducing the variance of accuracy on individual classes.
Despite increasing the calibration dataset size to 1024, there may still be significant variance in the accuracy of some classes.
For instance, the Class 2 of ImageNet has a standard deviation of 2.77 on prediction accuracy.
% This finding may be attributed to the objective of the quantization optimization process, which is to improve the overall accuracy on the entire calibration dataset, as opposed to improving the accuracy on individual classes.
% It is plausible that the optimization process may prioritize improving the performance of some classes at the expense of others in order to improve the overall accuracy. 
Some classes may inherently have more variability than others, and increasing the dataset size may not necessarily reduce this variability. 
% This could lead to an increase in variance, even when the dataset size is increased.
Therefore, simply increasing the dataset size may not be sufficient to reduce the variance. 

% In situations where certain classes are more important than others, it may be necessary to increase the weight of these classes to ensure their accuracy, even if doing so may result in a decrease in overall accuracy.
% It is important to carefully consider the trade-off between the accuracy of individual classes and the overall accuracy of the model when adjusting class weights.

% If the calibration dataset is not diverse enough, does not cover a wide range of inputs, or contains outliers or biases, the quantization parameters estimated from this dataset may not be optimal for the network's activations during inference, leading to biased performance of the final quantized network.
% If the calibration dataset is biased towards certain classes, the quantization parameters estimated from this dataset may not be optimal for the activations of the network on other classes, leading to reduced accuracy on those classes.
% To evaluate the bias introduced by a biased calibration dataset, our framework perform class-wise evaluation of the quantized model's performance.
% In addition, outliers can have a significant impact on the distribution of the data and may lead to inaccurate quantization parameters. 
% Our framework perform multiple random samples to observe the variation in the estimated quantization parameters and identify the effect of outliers.

% observe 网络对biased dataset会有很大影响
% class-wise evaluate 判定对于类别的bias；outliers 根据samples多次来判定

% The number of calibration samples also affect the reliability of PTQ.
% Table~\ref{} shows that 

% 提出一些方法来检测出现了问题，并且提出一些可能下一步方案

% It is important to recognize the bias and take corrective measures. One possible approach is to perform exploratory data analysis to identify the sources of bias and the extent to which different classes or inputs are affected. For example, if the biased dataset has an over-representation of certain classes or data points, this can be identified through visualization and statistical analysis.
% Once the sources of bias are identified, it may be necessary to collect additional data to ensure that the calibration dataset is diverse and representative of the data that the network is likely to encounter during inference. Alternatively, data augmentation techniques can be applied to artificially increase the diversity of the existing calibration dataset.
% When the calibration dataset contains outliers or unusual data points, it may be necessary to remove or downweight the outliers or use robust estimation techniques to obtain more accurate quantization parameters.