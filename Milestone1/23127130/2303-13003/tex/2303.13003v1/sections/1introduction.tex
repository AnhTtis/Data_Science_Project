\section{Introduction}


% Introduce post-training quantization and its real applications

% Explain the limiation of current quantization techniques in the open environment with distribution shift.

% Introduce the our evaluation setting and sumarize basic observations.
\begin{figure}[tb] % h:here 当前位置 % b bottom % t top % p 浮动
    \centering
    \includegraphics[width=0.48\textwidth]{ims/overview.pdf} %ims/xx.png
    \caption{Overview of the framework for assessing reliability of PTQ.}
    \label{im_overview}
\end{figure}

Modern deep neural networks (DNNs) are widely used in various domains ranging from daily-life applications such as image classification and machine translation to risk-sensitive areas such as autonomous driving \cite{muhammad2020deeplearning_autodrive} and finance \cite{zhang2021application_finance}. The remarkable performance of DNNs always depends on huge amounts of network parameters, which further brings expensive computational and memory costs. Prior research aims to compress DNN's parameters from different directions. Among these directions, post-training quantization (PTQ)
is typically identified as one of the most practical way, which offers the advantage of compressing DNNs without modifying the original training procedure, model structures, and parameters, making them highly desirable for practical applications.

At a high-level, PTQ is a technique for compressing neural networks by reducing the number of bits used to represent the weights and activations of the network. 
The calibration process of PTQ determines the optimal quantization parameters for a given neural network. 
The calibration dataset is small number of input samples, which is used to estimate the statistical properties of the activation values that the neural network produces during inference.
These statistical properties are then used to determine the optimal quantization parameters that minimize the quantization error.
Previous efforts have been made to improve PTQ from different dimensions such as proposing better calibration metrics~\cite{yuan2022ptq4vit,liu2022pdquant} and devising new optimizing algorithms~\cite{nagel2020adaround,li2021brecq}, especially in the low-bit regime. 
To date, state-of-the-art method can achieve nearly lossless accuracy on the image classification task in the 4-bit setting~\cite{wang2022leveraging}. 

While the PTQ technique has been found to be effective and convenient, they are often based on the assumption that the calibration, and test sets share the same underlying distribution, a condition known as the "close-environment" assumption. 
In real-world open environment, however, this assumption is typically impractical due to distribution shifts between the calibration and test distributions. 
A natural question arises: \textit{Is current PTQ method reliable enough when facing extreme test samples such as worst-case-category or out-of-distribution samples?}. 
Answering this question is crucial for deploying the quantized DNNs to real-world applications. 
Despite its importance, this question remains largely unexplored currently in the research community.

In this paper, the reliability of various commonly-used PTQ methods are deeply investigated and comprehensively evaluated for the first time. 
In contrast to most of previous work that only focus on the average performance, we aim to evaluate PTQ's performance on the worst-case subgroups in the test distribution. To this end, we first perform extensive experiments and observe that some specific test categories suffer significant performance drop after post-training quantization.

%For example, we assess the reliability of PTQ in the setting of subpopulation shift, wherein training and test set consists of the same subpopulations but differ in their frequencies \footnote{label imbalance can be seen as an special case of subpopulation shift.}\cite{}. 
%Studying this issue is important for many risk-sensitive fields, such as in autonomous driving, where developers are typically concerned about the performance of DNNs in extreme scenarios such as rainstorm. 

%found that these methods cannot generalize well to test samples from worst-case subgroups, even if they achieved high average calibration performance.
In practical applications, we further found that PTQ's performance is influenced by numerous factors, including the distribution of the calibration set, PTQ setting, and calibration metrics. 
We delve deeper into the influence of these factors and their collective impact on the reliability of previous methods in the face of the worst test cases such as subpopulation shift. 
Specifically, we aim to answer the following research questions:
\begin{itemize}[itemsep=0pt]
    \item How does the variation in the distribution of the calibration data affect the performance of quantized network?
    \item How do different quantization settings affect the performance among different categories?
    \item How do different PTQ methods affect the performance among different categories?
\end{itemize}
To answer these questions, this paper introduces a framework designed to systematically evaluate the impact of PTQ from a reliability perspective. 
Using this framework, we conducted experiments spanning a broad range of tasks and commonly-used PTQ paradigms.
For each of the research questions, we present quantitative results, along with a thorough analysis of the underlying factors.
% Quantitative results are provided for each of the research questions, and the underlying reasons for these phenomena are analyzed. 
The primary observations derived from our experiments are summarized as follows:
\begin{itemize}[itemsep=0pt]
    \item Significant accuracy drop of individual categories  is observed  on the quantized model, which indicates the reliability issue of current PTQ methods. (Sec~\ref{sec_evaluation_method})
    % \item The construction of calibration dataset has a significant impact on the reliability of individual categories, but a comparatively minor effect on the average prediction accuracy.
    \item The average prediction accuracy remains resilient to variations in the composition of the calibration dataset, in the presence of noise and intra-class bias. However, the prediction accuracy for individual categories displays sensitivity to these factors.(Sec~\ref{sec_calibration_dataset})
    \item Reducing the bit-width has the potential to lead to substantial degradation in specific categories. (Sec~\ref{sec_quantization_settings})
    \item Certain optimization algorithms, such as gradient-based PTQ, exhibit comparatively diminished reliability, despite attaining superior average predictive accuracy. (Sec~\ref{sec_optimization})
\end{itemize}
From the above observations, it is apparent that most of the existing mainstream methods are not reliable enough.
In certain cases, the quantized model suffers significant accuracy drop on some categories or groups (e.g., small object in the object detection), which is unacceptable for risk-sensitive scenarios.
We indicate it is necessary to devise reliable and more robust PTQ methods that can effectively handle distribution shift scenarios.
As an additional contribution, we provide a benchmark for PTQ reliability that encompasses various tasks, PTQ methods, and network architectures. 
We will further release this benchmark to facilitate future analyses of PTQ reliability and contribute to the advancement of PTQ methods.

% Therefore, we propose a benchmark of PTQ reliability including various tasks, PTQ methods, and network architectures. 
% We hope that this benchmark can benefit to the future analyze on the PTQ reliability and contribute to the improvement of PTQ.
% This paper xxx.
% The contribution of this paper is summarized as follows:

%From above observations, we 
%Additionally, the paper discusses how the interaction of different construction factors, such as the choice of calibration set, affects performance and provides guidelines to help researchers choose appropriate calibration methods based on their unique situations.
