\section{Related Work}

\subsection{Quantization}

Quantization is one of the most effective methods to compress neural networks and to accelerate the inference of networks~\cite{deng2020model_compress_survey,nagel2021white,yuan2022ptq4vit}.
Quantizing a network means reducing the precision of the weights and activations of the network in order to make it more computationally efficient while maintaining a similar level of prediction accuracy. 
The quantized weights and activations are typically stored as integers (such as INT8 and INT4), which reduces the amount of memory required to store them.
Lower precision calculations can be performed faster than higher precision calculations, resulting in faster inference times and reduced energy in real-time applications.
There are two main types of quantization used for neural networks: Post-Training Quantization (PTQ) and Quantization Aware Training (QAT).

Post-Training Quantization (PTQ) is a quantization technique that is applied to a pre-trained neural network~\cite{migacz2017tensorrt,banner2019ptq4bit_rapid_deploy,choukroun2019lowbit_for_efficient_inference}. 
It involves quantizing the weights and activations of the network after training is complete. 
Quantization Aware Training (QAT) is a technique in which the quantization process is integrated into the training process itself~\cite{krishnamoorthi2018quantizing_whitepaper,choi2018pact,gong2019dsq,esser2020lsq}. 
During QAT, the network is trained using a combination of full-precision and lower-precision weights and activations, which helps it to learn to be more robust to the effects of quantization.
PTQ is generally simpler to implement than QAT, as it involves quantizing a pre-trained network without any additional training. 
This can be useful in situations where re-training the network with QAT would be time-consuming or impractical~\cite{nagel2021white} (e.g., when the training dataset is not available).

Some previous work has explored the stability of quantization and the influence of calibration dataset.
\cite{hubara2021ptq_with_small_calibraiton_set} explore the problems of using small calibration dataset in quantization. 
PD-Quant~\cite{liu2022pdquant} identifies a disparity between the distribution of calibration activations and their corresponding real activations, and proposes a technique for adjusting the calibration activations accordingly.
SelectQ~\cite{zhang2022selectq} shows that randomly selecting data for calibration in PTQ can result in performance instability and degradation due to activation distribution mismatch.
Although prior research has explored the impact of the calibration dataset on the performance of quantized neural networks, these investigations have only focused on a limited range of factors and their effects on the stability of quantization. 
In this paper, we present a framework that enables us to evaluate the reliability of PTQ and conduct a comprehensive analysis of the impact of various factors on the reliability of quantization.

% While previous research may have investigated the effect of the calibration dataset on the performance of quantized neural networks, the impact of on reliability of PTQ have not been fully explored. 
% In this paper, we will develop a framework to assess the reliability and comprehensively analyze the impact of these factors from a reliability standpoint.

\subsection{Reliability of Neural Network}

The reliability of deep models has become an increasingly important topic in recent years, particularly as these models are increasingly being used in critical applications such as healthcare, autonomous vehicles, and financial systems. The reliability of deep models is often measured through different dimensions, some commonly used dimensions include: (1) model performance in various situations (including performance on the existing worst categories, noise samples, and out-of-distribution samples, etc.), (2) model robustness against test-time attacks such as adversarial attacks, and (3) the quality of the model's confidence. This paper mainly focus on the first dimension and leaves other two as the future work. Therefore, here we only give a brief introduction of related works in the first dimension.

\noindent \textbf{Model reliability in the worst case:} For the first dimension, various studies have been conducted to evaluate and improve the reliability of models against worst-case scenarios involving distribution shift and data noise, among other factors\cite{OOD_survey, label_noise_survey}. In this context, the evaluation metric typically used is the worst-case accuracy among existing test categories or out-of-distribution (OOD) samples\cite{OOD_survey, dro_survey}. It has been observed that models trained conventionally with the assumption of independent and identically distributed (IID) data fail to generalize in real-world testing environments with challenges such as hardness, noise, or OOD samples\cite{co-teaching, duchi_dro}. To enhance model reliability in such settings, two common approaches have been proposed: (1) explicit employment of robust training paradigms, such as distributionally robust optimization (DRO) and learning with noisy labels (LNL), to enhance model robustness; and (2) implicit data augmentation techniques, such as Mixup\cite{zhang2018mixup}. Further literature on this subject can be found in prior surveys\cite{OOD_survey, label_noise_survey, dro_survey}. Despite the efforts made by the community, the model reliability of PTQ methods in such cases remains largely unexplored.

%\noindent \textbf{Test-time attack:} In contrast to the first dimension, model reliability against test-time attack typically emphasize the model performance over handcraft "bad" samples such as adversarial samples \cite{}. 

%\noindent \textbf{Confidence estimation:}

%There are several factors that can impact the reliability of a neural network. One key factor is the quality and quantity of the training data used to develop the model. If the training data is biased, incomplete, or otherwise flawed, this can lead to inaccurate predictions and unreliable outcomes. Additionally, neural networks can be vulnerable to adversarial attacks, where an attacker intentionally introduces small modifications to the input data to cause the model to make incorrect predictions. In some cases, these attacks may be designed to exploit weaknesses in the neural network's architecture or decision-making processes.

%Another challenge that can impact the reliability of neural networks is data drift, which occurs when the statistical properties of the input data change over time. This can happen, for example, in healthcare applications where patient populations may evolve over time, or in financial systems where market conditions may shift rapidly. If a neural network is trained on data that is no longer representative of the current environment, this can lead to errors and inaccuracies in its predictions.

%To address these challenges, researchers have developed a range of techniques for improving the reliability of neural networks. For example, uncertainty estimation methods can be used to quantify the confidence of a neural network's predictions and identify cases where the model is uncertain or has low confidence. Adversarial training techniques can be used to improve the robustness of a neural network to adversarial attacks, while domain adaptation methods can be used to adjust a neural network's parameters to new data distributions.

%In conclusion, the reliability of neural networks is a critical issue that has important implications for their deployment in real-world applications. Understanding the factors that impact the reliability of these models, and developing effective techniques for improving their robustness and accuracy, is essential for ensuring their safe and trustworthy use in a range of critical domains.