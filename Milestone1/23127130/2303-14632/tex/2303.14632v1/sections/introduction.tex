\section{Introduction}
\label{sec:intro}

% Q: What problem are we solving?
The problem of anomaly detection in graphs has a multitude of real-world applications in many fields.
An anomaly may be a rare item, event, or entity that does not fit with the more `typical' trends in a set of data - i.e. an outlier.
Examples include bank fraud in a set of financial transactions, typos in a text, intrusions in networks, `bot' users in social media, or a sudden drop in sales caused by a global pandemic.
In this paper, we propose \textbf{T}emporal \textbf{E}gonet \textbf{S}ubgraph \textbf{T}ransitions (TEST): a comprehensive way of summarizing the behavioral patterns of nodes in a network.

% brief overview
%As the causes of this anomaly may be out of the knowledge realm of the data gathered, it is often more useful to simply detect these anomalies for further manual examination rather than attempt to classify them.
We represent interactions between users over time as a sequence of graphs, representing the evolution of interactions between a set of nodes over time.
In this framework, we are interested in modeling the behavior individual nodes exhibit in order to distinguish different patterns of interaction.
To that end, TEST creates a vector representation of each node using the changes in its neighborhood---or \emph{egonet}---over time.
These vector representations can then be used for traditional tasks, such as clustering and classification, depending on the application and data quality.

% key advantages
TEST is flexible enough to lend itself to a variety of standard tasks and application domains because it does not depend on labeled data or any kind of iterative learning procedure.
Nodes' vector representations are computed purely based on how their egonets change over time.
%for the most part of the anomaly detection pipeline, which makes it a good alternative for problems that would require a large investment in human work for labeling data, if that is a feasible alternative at all.
Once in the embedding space, the data can be analyzed from various perspectives.
For example, given labeled data, the latent representations can be used for classification tasks such as bot-detection on social networks.
However, in the absence of ground truth regarding nodes, the latent representations can still be analyzed to find \emph{communities}: clusters of points in the embedding space signalling nodes with similar temporal behavior.

% model assumptions
Despite its flexibility, TEST makes some important assumptions about the data.
Not only should the data be graphical, it should be a discrete sequence $G_1, G_2, \dots G_T$, where the events described by $G_{t}$ precede those in $G_{t+1}$.
This means that datasets presented as streams of timestamped edges must be \emph{discretized}.
If the discretization process is too coarse, signals in the data will be lost;
however, if the discretization is too fine, the model will not significantly compress the data.
While some datasets and application domains are concerned with clustering or classifying edges---as is the case in, for example, detecting fraudulent transactions on financial networks---our focus in this paper is on characterizing nodes' behavior.
%The second assumption is that a node is an agent that performs an action (edge transition) and that is clustered.
%Some problems are represented in a way that it's desirable to cluster edges, not nodes.
%In which case it might be necessary to apply a transformation to map the edges adjacencies to vertex adjacencies.
% LineGraph function: https://reference.wolfram.com/language/ref/LineGraph.html and https://networkx.org/documentation/networkx-1.10/reference/generated/networkx.generators.line.line_graph.html

% Three broad categories of anomaly detection techniques exist.[4] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the learnt model.

% Q: How are we solving it?

% Classification with EVM if using clusters?
%   Fits probabilistic models for the clusters and 

% Use clusters to build classification model

% Daniel's overview:

% Two nodes, at different times t and t+1, each node has a neighborhood i.e. a list of nodes directly connected to it. At the following timestep, the same node may have a different neighborhood. Let's say we lost C and A as a neighbor, and their connections changed (d-e), there might be new nodes too. We can summarize these changes by counting the different kinds of subgraphs (e.g. triangles) present in each timestep (T = number of timesteps). We'd go through all 3 node subgraphs (exponential? More than factorial). We put all subgraph transictions in a vector (hence the 6^6 vector for 3(?) nodes). Then we can represent these vectors in a timeline. We can consider an aggregation of transition vectors as the node embedding.

% So we don't care about which nodes are part of the transition, as we're just counting them to include in the feature vector.

% > But why use subgraphs to see the differences?
% Counting the subgraph transitions gives us an idea how the topology of the egonet changes over time. Triangle counting is very used in social media to count things like wedges and triangles. It lets you create rudimentary link prediction models that recommned closing that triangle with some good probability. (paper: https://arxiv.org/abs/1802.06916). (simplicial closure). One way is edit distance, but it does not take into account the topological structure, which might be rich.

% > How should we aggregate the transition vectors?
% Good point of discussion for the paper. Average might be the simplest one. Max, min, sum have different implicatiions and are hyperparameters to find.

% > Selection of a dataset.
% Just general anomaly detection without tying it to a real-world problem or dataset. Although we might have use-case studies in our evaluation.

% > Binning timesteps
% The datasets we have contain very granular data. The bins have to be big enough, but there also need to be enough buckets.

% + Node2Vec might be a good paper, as the method it's a semi-supervised approach.
% + Simplicial closure
% + SST paper - joint subgraph-to-subgraph transitions (maybe not useful as it's for link prediction)

% "We ran into a problem" section

% > Synthetic dataset generation
% > 
