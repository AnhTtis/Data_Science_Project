\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\newcommand\sanbao[1]{\textcolor{blue}{Sanbao #1}}
\newcommand\hsy[1]{\textcolor{red}{Songyang: #1}}
\newcommand\hsh[1]{\textcolor{red}{Sihong: #1}}
% \usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}{Proposition}
\newtheorem{defi}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
% \theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Collaborative Multi-Object Tracking \\with Conformal Uncertainty Propagation
}

\begin{document}

\maketitle

\section{AC}

\textbf{Resubmit not later than 11:59 pm Pacific Time December 7, 2023. If this paper can be accepted before Nov.30th, it can be presented at ICRA. Resubmit a revised version, along with a
Statement of Changes as a single pdf file indicating how comments by
the Editor and by reviewers have been addressed and a list of changes
made in the paper.}

Thank you for your submission. Please see the reviewers' comments on
how to improve the paper. One common point is to highlight the novelty
in comparison with the SotA, as reviewers have questioned how it
compares with the literature and specific papers.

\textbf{A}: We sincerely thank our reviewers for the constructive feedback. 

% \sanbao{Compare with our ICRA paper?}

\section{Review 1}

The authors present the following claims in their paper: They introduce
a novel framework, MOT-CUT, which addresses object detection - a
critical component of autonomous driving systems - and multiple object
tracking. Accurate detection of objects and the quantification of the
associated uncertainties are of utmost importance. While the accuracy
of detection improves with collaborative multi-object tracking, this
work uniquely quantifies the uncertainty and links it to enhanced MOT
performance. In this framework, uncertainties are propagated through
direct modeling and conformal prediction, achieving improvements in
accuracy. Simulations using V2X-sim demonstrate enhanced performance in
terms of accuracy and uncertainty handling compared to baseline
algorithms, with particularly notable improvements in scenarios with
high occlusion.

The proposed method is clearly and comprehensively described in the
manuscript. The references are also appropriately cited.

The experimental results are well-organized into tables, making it easy
to understand. Differences due to the methodology, as well as
comparisons based on different evaluation metrics, are also clearly
presented.


Q1: "In Section IV, 'D. Uncertainty Evaluation,' the comparison
experiments, primarily Table III, only differentiate between scenarios
with or without CP. I believe that, for CP, it would be necessary to
compare multiple methods, not just CRPS.

We add dropout (DO) and deep ensemble (DE) as baselines to compare with the conformal prediction on the uncertainty evaluation part.


\begin{table*}
  \centering
  \caption{NLL \& CRPS comparison on detection and MOT with/without conformal prediction (CP). The best results are shown in \textbf{bold}.}
  \tabcolsep=3pt
  \vspace{-10pt}
  \label{tab:nll}
  %\tabcolsep=4pt
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
   \multirow{2}{*}{Base} & \multirow{2}{*}{Method} & \multirow{2}{*}{DO} & \multirow{2}{*}{DE} & \multirow{2}{*}{CP}& \multicolumn{3}{c|}{NLL @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{NLL @IoU=0.7 $\downarrow$} & \multicolumn{3}{c|}{CRPS @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{CRPS @IoU=0.7 $\downarrow$}\\
  \cline{6-17}
  & & &&&UB& DN & LB& UB& DN& LB & UB& DN & LB& UB& DN& LB \\
  \hline
  & \multirow{4}{*}{detection}& &&&193 & 222 & 335 & 96 & 147 & 166 & 0.453 & 0.498 & 0.693 & 0.392 & 0.459 & 0.514\\
  \cline{3-17}
  &&\checkmark&&&81.01 & 52.97 & 263 & 36.05 & 26.98 & 124 & 0.512 & 0.554 & 0.745 & 0.453 & 0.517 & 0.572\\
  \cline{3-17}
  &&&\checkmark&&44.01 & 46.98 & 128 & 23.64 & 29.30 & 63.80 & 0.482 & 0.518 & 0.703 & 0.423 & 0.480 & 0.531 \\
  \cline{3-17}
  SORT& & & & \checkmark & 25.70 & 26.21 & 25.17 & 14.54 & 19.50 & 13.04 & 0.424 & 0.466 & 0.652 & 0.364 & 0.427 & 0.475\\
  \cline{2-17}
  \cite{bewley2016simple}&\multirow{4}{*}{MOT-CUP}  &&&  & 9.61 & 12.09 & 14.45 & 7.87 & 11.21 & 11.06 & 0.312 & 0.355 & 0.463 & 0.297 & 0.347 & 0.409\\
  \cline{3-17}
  &&\checkmark&&&6.54 & 3.48 & 18.34 & 4.38 & 3.44 & 11.40 & 0.345 & 0.379 & 0.489 & 0.331 & 0.374 & 0.436 \\
  \cline{3-17}
  &&&\checkmark&&2.56 & 2.31 & 6.15 & 2.17 & 2.23 & 3.82 & 0.338 & 0.360 & 0.483 & 0.324 & 0.354 & 0.431 \\
  \cline{3-17}
  &  &&& \checkmark& \textbf{0.94} & \textbf{0.95} & \textbf{1.30} & \textbf{0.74} & \textbf{0.90} & \textbf{1.06} & \textbf{0.301} & \textbf{0.336} & \textbf{0.444} & \textbf{0.286} & \textbf{0.329} & \textbf{0.392} \\
  \hline
  & \multirow{4}{*}{detection} &&&& 1801 & 540 & 461 & 870 & 198 & 121 & 0.392 & 0.391 & 0.597 & 0.338 & 0.357 & 0.358 \\
  \cline{3-17}
  &&\checkmark&&&101& 35.08 & 269 & 41.17 & 24.86 & 101 & 0.524 & 0.561 & 0.792 & 0.468 & 0.531 & 0.550\\
  \cline{3-17}
  &&&\checkmark&&72.56 & 38.41 & 156.59 & 35.99 & 30.51 & 57.10 & 0.492 & 0.523 & 0.753 & 0.436 & 0.493 & 0.518\\
  \cline{3-17}
  Byte-& & &&\checkmark& 43.65 & 32.50 & 68.94 & 23.47 & 19.25 & 11.78 & 0.381 & 0.376 & 0.596 & 0.328 & 0.343 & 0.358 \\
  \cline{2-17}
  Track& \multirow{4}{*}{MOT-CUP}&&& & 29.49 & 25.30 & 57.23 & 17.67 & 17.57 & 11.27 & 0.302 & 0.318 & 0.412 & 0.276 & 0.308 & 0.310 \\
  \cline{3-17}
  &&\checkmark&&&24.35 & 7.66 & 26.43 & 6.97 & 7.37 & 21.70 & 0.385 & 0.436 & 0.532 & 0.359 & 0.432 & 0.437 \\
  \cline{3-17}
  &&&\checkmark&&23.52 & 11.61 & 24.62 & 6.20 & 9.89 & 20.77& 0.362 & 0.406 & 0.512 & 0.336 & 0.402 & 0.421 \\
  \cline{3-17}
  \cite{zhang2021bytetrack}& &&&\checkmark& \textbf{20.01} & \textbf{6.94} & \textbf{4.05} & \textbf{2.41} & \textbf{1.99} & \textbf{0.99} & \textbf{0.280} & \textbf{0.286} & \textbf{0.376} & \textbf{0.254} & \textbf{0.275} & \textbf{0.276}\\
  \hline
  \end{tabular}
  \vspace{-5pt}
\end{table*}


Q2: While the paper claims to focus on collaborative multi-object tracking
in its title and objectives, the content does not seem to reflect a
collaborative scenario. Indeed, the dataset used in the simulations,
V2X-sim, appears to have been generated in a collaborative scenario.
However, the proposed method itself does not seem to incorporate
elements unique to a collaborative scenario. In other words, isn't the
method indistinguishable from one used in a single-agent scenario? If
so, the authors' emphasis on the term "collaborative" does not seem to
provide a discernible advantage.

% \sanbao{
% In general, our method can be applied to single-agent object detection, as shown for LB. But for space limitation and the start point we focus on collaborative object detection, we did not do experiments on the single-agent dataset. We don't want to overclaim.
% LB is the single-agent object detection.
% Collaborative object detection (COD) extends beyond the scope of traditional single-agent object detection (SOD) by harnessing the collective information obtained from multiple agents or sensors to enhance the overall detection performance. Collaboration introduces new challenges and opportunities in handling complex scenarios, including occlusions, varying viewpoints, and scalability. These factors distinguish COD from SOD. Our focus is on utilizing the uncertainty quantification of COD to improve the MOT task, rather than solely emphasizing the significance of COD itself. 
% Hence, we believe that this comment does not directly impact the contributions of our paper.}

\textbf{A2}: We deeply appreciate your valuable feedback on our paper. While it is true that our method could potentially be applied to single-agent object detection, as Lowerbound is fundamentally a single-agent object detection method, we chose to concentrate on collaborative object detection due to space constraints and the initial focus of our work. Regrettably, we did not conduct experiments on a single-agent dataset in this instance. To avoid making unsupported claims in the paper, we refrained from asserting that our method is universally applicable to the single-agent scenario. Instead, we underscore that the primary focus of our approach lies in addressing uncertainty propagation from collaborative object detection to multi-object tracking. Your insights are invaluable, and we will carefully consider them for future work.

Q3: Reference [12] also employs V2X-sim for uncertainty-aware tracking, and
its problem formulation seems strikingly similar to this paper. Has
there been any comparison with that particular work?

%\sanbao{Reference [12] is our ICRA paper. Highlight the different tasks, focus different. Object detection uses direct modeling + bootstrap. Compare on tracking accuracy and uncertainty.}

\textbf{A3}: We sincerely appreciate your insightful feedback on our paper. It's important to note that our work primarily centers around addressing uncertainty propagation from collaborative object detection to multi-object tracking (MOT) and enhancing MOT performance. In contrast, Reference [12] introduces a distinct Double-M Quantification method, concentrating on uncertainty quantification for collaborative object detection. Given the disparate nature of the tasks between Reference [12] and our paper, direct comparisons may be unnecessary. We appreciate your consideration and will certainly take your feedback into account for future discussions and improvements.

\section{Review 2}

This paper presents a method which utilises the uncertainty of
multi-sensor object detections to improve tracking performance. The
paper is mostly well-written and the idea seems quite reasonable. 


Q1: A point that should be clarified is the relationship between the
collaborative aspect and the uncertainty-based method. In the
development of the method, it seems like it could be equally applied to
data from a single sensor, or a collaborative arrangementâ€”the
contribution seems to be independent of that. Is it novel even in the
single sensor case?

\textbf{A1}: We deeply appreciate your valuable feedback on our paper. While it is true that our method could potentially be applied to single-agent object detection, as Lowerbound is fundamentally a single-agent object detection method, we chose to concentrate on collaborative object detection due to space constraints and the initial focus of our work. Regrettably, we did not conduct experiments on a single-agent dataset in this instance. To avoid making unsupported claims in the paper, we refrained from asserting that our method is universally applicable to the single-agent scenario. Instead, we underscore that the primary focus of our approach lies in addressing uncertainty propagation from collaborative object detection to multi-object tracking. Your insights are invaluable, and we will carefully consider them for future work.

Q2: In classical tracking literature (eg , the standard association score
is the log likelihood, which in the case of the KF is $residual^T (H P
H^T + R )^{-1} residual$, where H is the forward model and P is the KF
covariance. Eq (6) looks like it excludes the KF covariance, ie it will
end up as $residual^T R^{-1} residual$. Has the other alternative been
considered? Linking to this extensive literature is important given
that the proposed concept is standard there.

\sanbao{Find literature about change the covariance part of Kalman Filter. Kalman Filter for deep neural network.}

%\sanbao{
%Is there any other method applied on KF to improve performance? Search literature. Alternative of KF, such as extended KF. The time complexity they said seems to be the time complexity of Kalman Filter. However in our method, we did not influence the time complexity of the Kalman filter, we just changed the measurement uncertainty from the identity matrix to our predicted uncertainty. Eq (6) is for the association part, it won't influence the time complexity of the Kalman filter.}

\end{document}