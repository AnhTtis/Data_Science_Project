\section{Methodology}
\label{sec:approach}

\subsection{Approach Overview} 

We design a novel framework for uncertainty propagation of collaborative object detection (COD) to MOT, named Multiple Object Tracking with Conformal Uncertainty Propagation (MOT-CUP). Fig.~\ref{fig:overview} presents the methodology overview. The major novelties are: \textit{(1)} MOT-CUP framework first rigorously quantifies the uncertainty in the COD stage based on direct modeling and conformal prediction. \textit{(2)} The uncertainty information is leveraged in the motion prediction stage of MOT, where a Standard Deviation-based Kalman Filter (SDKF) takes the uncertainty quantification (UQ) of COD as its input to improve the predicted precision of location. \textit{(3)} We utilize the Negative Log Likelihood as the similarity metric for the association step, called NLLAI, to improve the accuracy and reduce the uncertainty of MOT.

In this section, we first introduce the conformal prediction in Subsection~\ref{subsec:pre} as preliminary literature of UQ and a useful method to construct predicted uncertainty. We describe our proposed MOT-CUP (Multiply Object Tracking with Conformal Uncertainty Propagation) method as shown in Algorithm~\ref{alg:whole} and Subsection~\ref{subsec:our_app}, followed by the detailed process of UQ of COD based on direct modeling and conformal prediction in Subsection~\ref{subsec:uq_detection}, and uncertainty propagation to MOT in Subsection~\ref{subsec:uq_track}. %The major novelty of MOP-UP includes: we introduce the process of estimating uncertainty on the object detection stage with direct modeling and conformal prediction. Then, in subsection~\ref{subsec:uq_track}, we propose SDKF and NLLAI methods to leverage the uncertainty to improve the performance of MOT.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/overview_v2.pdf}
    % \vspace{-2mm}
    \caption{\textbf{Overview of our MOT-CUP framework.} The red color highlights the novelties and important techniques in our MOT-CUP framework. In the collaborative object detection (COD) stage, we rigorously calculate uncertainty quantification (UQ) of each object detection via direct modeling (DM) and conformal prediction (CP).  In the motion prediction stage of MOT, we adopt a Standard Deviation-based Kalman Filter (SDKF) to enhance the Kalman Filter process, that leverages the UQ results and predicts the locations of the objects in the next time step with higher precision. In the association step, we first apply the baseline association method and then associate the unmatched detections and tracklets with the Negative Log Likelihood similarity metric, called NLLAI.}
    \label{fig:overview}
    \vspace{-5mm}
\end{figure}
\subsection{Preliminary}\label{subsec:pre} 
Conformal prediction (CP)~\cite{angelopoulos2021gentle} is a statistical method to generate prediction sets for any model. It is a method to convert any heuristic notion of uncertainty (e.g. an estimate of the standard deviation) to rigorous UQ. 
%The beauty of CP lies in its explicit, non-asymptotic guarantees that are not contingent upon distribution or model assumptions. 
For example, we assume that an uncertain scalar follows Gaussian distribution and train a model to output the mean and standard deviation. To be precise, we choose to model $Y_{test} \sim \mathcal{N}(\mu(x), \sigma(x)) \mid_{X_{test} = x}$, where $X_{test}$ is a testing data, and $Y_{test}$ is the corresponding label. We train $\hat{\mu}(x)$ and $\hat{\sigma}(x)$ to maximize the likelihood of the data. Conformal prediction can turn this heuristic uncertainty notion into rigorous prediction intervals of the form $(\hat{\mu}(x) \pm \hat{q}\hat{\sigma}(x))$, where $\hat{q}$ is a quantile found by CP. % \hsh{$Y_{test} = f(X_{test} = x) \sim \mathcal{N}(\mu(x), \sigma(x))$}

Consider the validation data $(X_1, Y_1), ..., (X_N, Y_N)$ with $N$ data points that are never seen during training, the CP for input $x$ and output $y$ includes the following steps:
\textit{(1)} Define the score function $s(x, y) \in \mathbb{R}$. (Smaller scores encode better agreement between $x$ and $y$). \textit{(2)} Compute $\hat{q}$ as the $\frac{\lceil (N+ 1) (1-\alpha) \rceil}{N}$ quantile of the validation scores $s_1 = s(X_1, Y_1), ..., s_N = s(X_N, Y_N)$, where $\alpha \in [0, 1]$ is a user-chosen error rate. \textit{(3)} Use this quantile to form the prediction sets $\mathcal{C}(X_{test})$ for new examples:
\vspace{-5pt}
\begin{equation}
    \label{equ:prediction_set}
        \mathcal{C}(X_{test}) = \{ y:s(X_{test}, y) \leq \hat{q}\},
\end{equation}
\iffalse    
\vspace{-5pt}
\begin{itemize}
    \item Define the score function $s(x, y) \in \mathbb{R}$. (Smaller scores encode better agreement between $x$ and $y$)
    \vspace{-5pt}
    \item Compute $\hat{q}$ as the $\frac{\lceil (N+ 1) (1-\alpha) \rceil}{N}$ quantile of the validation scores $s_1 = s(X_1, Y_1), ..., s_N = s(X_N, Y_N)$, where $\alpha \in [0, 1]$ is a user-chosen error rate.
    \vspace{-5pt}
    \item Use this quantile to form the prediction sets for new examples:
    \vspace{-5pt}
    \begin{equation}
    \label{equ:prediction_set}
        \mathcal{C}(X_{test}) = \{ y:s(X_{test}, y) \leq \hat{q}\},
    \end{equation}
    % \vspace{-5pt}
\end{itemize}
\fi
Note that $(X_{test}, Y_{test})$ is a fresh test point from the same distributions of the validation data. The CP provides a coverage guarantee, as stated in the following lemma.

\begin{lemma}[\textbf{Conformal Coverage Guarantee}~\cite{angelopoulos2021gentle}]
\label{lemma:coverage}
Suppose $(X_k, Y_k)_{k=1, ..., N}$ and $(X_{test}, Y_{test})$ are $i.i.d.$, then the following holds:
\vspace{-5pt}
\begin{equation}
    1- \alpha \leq \Pr(Y_{test} \in \mathcal{C}(X_{test})) \leq 1- \alpha + \frac{1}{N + 1}.
\end{equation}
\end{lemma}

In other words, the probability that the prediction set contains the correct label is almost exactly $1-\alpha$. %Note that this is only a special case of CP, called split conformal prediction. This is the most widely used version of CP. %Specifically, when we select the score function as $s(x, y) = \frac{\left| y - \hat{\mu}(x) \right|}{\hat{\sigma}(x)}$, the prediction set is $\mathcal{C}(x) = \left[\hat{\mu}(x) - \hat{\sigma}(x)\hat{q}, \hat{\mu}(x) + \hat{\sigma}(x)\hat{q} \right]$.

\subsection{MOT-CUP Algorithm} 
% change to Solution Overview?
\label{subsec:our_app}

%\textcolor{blue}{Need a figure for solution overview}
%Use Direct modeling to measure the standard deviation of each bounding box


The detail of MOT-CUP is presented in Algorithm~\ref{alg:whole}. 
%\hsh{does every frame contain the same number of objects? if so, should we point out this assumption here?} 
% For each frame, the trained collaborative object detector with direct modeling would generate a set of detected objects $D = \{ \hat{p}_j,  \{\hat{y}_{ij}, \hat{\sigma}_{ij}\}_{i=1}^I \}_{j=1}^J$ (see Line~\ref{alg:line_detector}). The set includes the predicted classification probability $\hat{p}_j$ and the location of each object which is represented by $I$ variables where  $\hat{y}_{ij}$ is the mean and $\hat{\sigma}_{ij}$ is the standard deviation. \hsh{try this: "For each frame, the well-trained collaborative object detector with direct modeling would generate a detected result set for each object $j = 1, ..., J$: $D^j = \{ \hat{p}_j,  \{\hat{y}_{ij}, \hat{\sigma}_{ij}\}_{i=1}^I \}$ where $\hat{p}_j$ is the predicted classification probability, $\hat{y}_{ij}$ is the $i$th mean and $\hat{\sigma}_{ij}$ is the $i$th standard deviation of the $I$-dimentional position variable of object $j$. We use $D = \{D_j\}_{j=1}^J$ to denote the set of all detected result sets. For notational convenience, when there is no confusion, we omit the subscript $j$ and use $\{ \hat{p},  \{\hat{y}_{i}, \hat{\sigma}_{i}\}_{i=1}^I \}$ to denote a realization of detected results set."} This object detector not only predicts the location of each object but also provides a measure of uncertainty. 
For each frame in the point cloud sequence $S$, there are $J$ objects. For each frame, the trained collaborative object detector with direct modeling would generate a set of detected objects $\mathcal{D} = \{ \hat{p}_j,  loc_j \}_{j=1}^J$ (Line~\ref{alg:line_detector}). The set includes the predicted classification probability $\hat{p}_j$ and the location of each object $loc_j$. The location of each object is represented by $I$ random variables parameterized by $\{\hat{y}_{i}, \hat{\sigma}_{i}\}_{i=1}^I$ where  $\hat{y}_{i}$ is the mean and $\hat{\sigma}_{i}$ is the standard deviation for $i$-th variable. This object detector not only predicts the location of each object but also provides a measure of uncertainty.

To provide more accurate measures of uncertainty, we leverage the quantiles computed by CP to adjust the standard deviation $\hat{\sigma}_{i}$ (see Lines~\ref{alg:line_cp_begin}-\ref{alg:line_cp_end}). Then, to track the detected objects across multiple frames, we employ a Kalman Filter to predict the current state of the tracklets, which is commonly used in MOT~\cite{bewley2016simple} (see Lines~\ref{alg:line_pred_tracklet_begin}-\ref{alg:line_pred_tracklet_end}). In the association step, we first apply the origin association method (see Line~\ref{alg:line_ba}) and store all matched pairs $(d, t)$ in $\mathcal{A}_{matched}$ for $d \in \mathcal{D}$ and $t\in \mathcal{T}$. Then we associate the unmatched detections and tracklets with the Negative Log Likelihood similarity metric for lower-quality detected objects (see Line~\ref{alg:line_nllai}). The detail of Negative Log Likelihood-based Association Improvement (NLLAI) will be introduced in Algorithm~\ref{alg:nllai}. To update the tracklets with the matched detections, we go beyond the traditional MOT algorithms by incorporating the detected standard deviation $\hat{\sigma}_i$ in addition to the detected mean $\hat{y}_i$ (see Lines~\ref{alg:line_update_begin}-\ref{alg:line_update_end}). This allows us to more accurately model the uncertainty associated with each detection and incorporate it into the tracklet. The detected standard deviations are also applied to generating new tracklets with unmatched detections (see Lines~\ref{alg:line_add_tracklet_begin}-\ref{alg:line_add_tracklet_end}). %In the following subsection~\ref{subsec:uq_detection}, we introduce the process of estimating uncertainty on the object detection stage with direct modeling and conformal prediction. Then, in subsection~\ref{subsec:uq_track}, we propose SDKF and NLLAI methods to leverage the uncertainty to improve the performance of MOT. \hsh{SDKF is not introduced before then appears here. is SDKF one kind of KF? can this algorithm 1 be used with any kind of KF?}
\vspace{-10pt}
\begin{algorithm}[ht]
\small
\caption{Multiple Object Tracking with Conformal Uncertainty Propagation (MOT-CUP)}
\label{alg:whole}
\KwData{input point cloud sequence $S$, the trained collaborative object detector $F$, NLL threshold $\tau$, baseline's Kalman Filter $KF$, baseline's association method $BA$, quantile of CP $\hat{q}$}
\KwResult{Tracklet list $\mathcal{T}$}
Initialization: $\mathcal{T} \leftarrow \emptyset$ \\
\For{point cloud frame $C$ in $S$ in time sequence}{
    %$D = \{\hat{p}_j,\{\hat{y}_{ij},\hat{\sigma}_{ij}\}_{i=1}^I\}_{j=1}^J= F(C)$\label{alg:line_detector}\\
    % $D = F(C) =\ J\ objects,\ each\ containing\ \{\hat{p},\{\hat{y}_{i},\hat{\sigma}_{i}\}_{i=1}^I\}$\label{alg:line_detector}\\
    $\mathcal{D} = F(C) = \{ \hat{p}_j,  loc_j \}_{j=1}^J$, where each location contains $\{\hat{y}_{i}, \hat{\sigma}_{i}\}_{i=1}^I$ \label{alg:line_detector}\\
    \For{each object}{ \label{alg:line_cp_begin}
    \For{i\ from\ 1\ to\ I}{
    $\hat{\sigma}_{i} = \hat{\sigma}_{i} \times \hat{q}_i$
    }
    }\label{alg:line_cp_end}
    % \tcc{\textcolor{blue}{Adjust standard deviation by \textbf{CP}}}
    \tcc{Adjust standard deviation by \textbf{CP}}
    \For{$t$\ in\ $\mathcal{T}$}{\label{alg:line_pred_tracklet_begin}
    Apply Kalman Filter (KF)~\cite{kalman1960contributions}\\
    }\label{alg:line_pred_tracklet_end}
    $\mathcal{A}_{matched}, \mathcal{D}_{unmatched}, \mathcal{T}_{unmatched} = BA(\mathcal{D}, \mathcal{T})$\label{alg:line_ba}\\
    $\mathcal{A}'_{matched}, \mathcal{D}'_{unmatched}, \mathcal{T}'_{unmatched} = \text{NLLAI}(\mathcal{A}_{matched}, \mathcal{D}_{unmatched}, \mathcal{T}_{unmatched}, \tau)$\label{alg:line_nllai}\\
    % \tcc{\textcolor{blue}{\textbf{NLLAI} is Algorithm~\ref{alg:nllai}}}
    \tcc{\textbf{NLLAI} is Algorithm~\ref{alg:nllai}}
    \For{$(d, t)$ in $\mathcal{A}'_{matched}$}{\label{alg:line_update_begin}
        Apply KF with updated standard deviation $\hat{\sigma}$\\
    }\label{alg:line_update_end}
    $\mathcal{T} = \mathcal{T} \backslash \mathcal{T}'_{unmatched}$\\
    \For{$d$ in $\mathcal{D}'_{unmatched}$}{\label{alg:line_add_tracklet_begin}
    $\mathcal{T} = \mathcal{T} \cup \{d\}$ where $d=\{\hat{y}_{i},\hat{\sigma}_{i}\}_{i=1}^I$\\
    }\label{alg:line_add_tracklet_end}
}
\end{algorithm}\vspace{-15pt}

\subsection{UQ on Collaborative Object Detection}\label{subsec:uq_detection}
We use direct modeling~\cite{Su2022uncertainty,he2019bounding} to estimate the standard deviation of each variable of the COD stage. We assume that all variables are independent and the distribution of each variable is a single-variate Gaussian distribution. For the distribution of each variable of the ground truth, we assume it as a Dirac delta function~\cite{he2019bounding}. Then we define the regression loss function for the $i$-th variable as the Kullback-Leibler (KL) divergence between the single-variate Gaussian distribution and the Dirac delta function~\cite{murphy2012machine}:

\vspace{-10pt}
\begin{equation}
    \mathcal{L}^i_{KL}(y_i, \hat{y}_i, \hat{\sigma}_i) =  \frac{(y_i-\hat{y}_i)^2}{2\hat{\sigma}_i^2}  + \log |\hat{\sigma}_i|,
\label{eq:kl-loss}
\end{equation}
where $y_i$ is the ground-truth value for $i$-th variable. An additional regression header is incorporated to forecast all standard deviations $\hat{\sigma}_i$, with a comparable structure as the regression header for $\hat{y}_i$. This is accomplished based on the original collaborative object detector where no alterations have been made to the remaining components. 

After we have the trained object detection model, we compute the quantile for the standard deviation of each variable by CP~\cite{angelopoulos2021gentle} based on the validation dataset, as introduced in Subsection~\ref{subsec:pre}.

We define the score function for the $i$-th variable as:
\begin{equation}
    s(x_i, y_i) = \frac{|y_i - \hat{y}_i(x_i)|}{\hat{\sigma}_i(x_i)},
\label{eq:score_func}
\end{equation}
where $x_i$ is the point cloud input and it can be comprehended as a multiplicative correction factor applied to the standard deviation where $s(x_i, y_i) \hat{\sigma}_i(x_i) = |y_i - \hat{y}_i(x_i)|$. After testing the detection model on the validation dataset and calculating the score function, we obtained a set of scores $\{s_{i1},s_{i2},...,s_{iM}\}$ for the $i$-th variable where $M$ is the number of all detected objects in all the frames in the validation set. Given an error rate $\alpha$, we select the quantile $\hat{q}_i$ as the $\frac{\lceil (1-\alpha)(1+M) \rceil}{M}$ quantile of the score set. The prediction set for $x_i$ is constructed following the proposition.

\begin{prop}
When we assume the uncertain scalar for $i$-th variable follows the Gaussian distribution with mean $\hat{y}_i(x_i)$ and standard deviation $\hat{\sigma}_i(x_i)$ and select the score function as $s(x_i, y_i) = \frac{\left| y_i - \hat{y}_i(x_i) \right|}{\hat{\sigma_i}(x_i)}$ in CP, the prediction set for $i$-th variable is $\mathcal{C}_i(x_i) = \left[\hat{y}_i(x_i) - \hat{\sigma}_i(x_i)\hat{q}_i, \hat{y}_i(x_i) + \hat{\sigma}_i(x_i)\hat{q}_i \right]$.
\end{prop}
\begin{proof}
From Lemma~\ref{lemma:coverage}, for a test point $(X_{test}= x_i, Y_{test} = y_i)$, it holds that 
\begin{align}
    & \Pr(Y_{test} \in \mathcal{C}(X_{test}))  \geq 1 - \alpha \nonumber \\
\Rightarrow & \Pr( s(X_{test}, Y_{test}) \leq \hat{q}_i) \geq 1- \alpha  ~~~~(\text{Eq.~\eqref{equ:prediction_set}}) \nonumber \\
\Rightarrow & \Pr( \frac{|y_i - \hat{y}_i(x_i)|}{\hat{\sigma}_i(x_i)} \leq \hat{q}_i) \geq 1- \alpha \nonumber ~~~~(\text{Eq.~\eqref{eq:score_func}})\\
\Rightarrow & \Pr( |y_i - \hat{y}_i(x_i)| \leq \hat{\sigma}_i(x_i)\hat{q}_i  ) \geq 1- \alpha \nonumber\\
\Rightarrow & \mathcal{C}_i(x_i) = \left[\hat{y}_i(x_i) - \hat{\sigma}_i(x_i)\hat{q}_i, \hat{y}_i(x_i) + \hat{\sigma}_i(x_i)\hat{q}_i \right].
\end{align}
\end{proof}

% which ensures that:
% \begin{equation}
%     P[s(y_i, \hat{y}_i, \hat{\sigma}_i) \leq q_i] \geq 1-\alpha \Rightarrow P[|y_i - \hat{y}_i| \leq \hat{\sigma}_i q_i] \geq 1-\alpha
% \label{eq:score_ensure}
% \end{equation}

Then we adjust the standard deviation by $\hat{\sigma}_i = \hat{\sigma}_i \hat{q}_i$ to achieve rigorously estimated uncertainty.


%For the single-variate Gaussian distribution with $\hat{y}_i$ as mean and  $\hat{\sigma}'_i$ as the expected standard deviation, the values less than or equal to three standard deviations away from the mean account for $99.73\%$, which means $P[|y_i - \hat{y}_i| \leq 3\hat{\sigma}'_i] \geq 99.73\%$. If we set $\alpha = 1-99.73\%=0.27\%$, $3\hat{\sigma}'_i = \hat{\sigma}_i q_i \Rightarrow \hat{\sigma}'_i = \hat{\sigma}_i q_i / 3$. Thus we can correct the estimated standard deviation to be close to the expected standard deviation by the quantile obtained by conformal prediction on the Validation dataset.

\subsection{Uncertainty Propagation to MOT}
\label{subsec:uq_track}
After obtaining the corrected standard deviation for each variable of detected objects, how to utilize and propagate it into the MOT stage remains a significant challenge. Here, we propose SDKF and NLLAI methods to leverage the uncertainty in both the motion prediction and association which are the primary steps of MOT.

\textbf{Standard Deviation-based Kalman Filter (SDKF):} As shown in Section~\ref{sec:relatedwork}, Kalman Filter (KF)~\cite{kalman1960new} is one important step for motion prediction. The inputs of KF encompass the observed state and measurement uncertainty. Compared to the existing MOT algorithm, we leverage our rectified standard deviation as the measurement uncertainty in place of the pre-established values. By taking into account both the mean and standard deviation of the detections, we are able to better account for the uncertainty of objects and provide more robust tracklets over time. SDKF does not significantly impact the time complexity of algorithms, as it only modifies the measurement uncertainty input from fixed values to rigorously estimated ones.

\vspace{-10pt}
\begin{algorithm}
\small
\caption{NLL-based Association Improvement method (NLLAI)}
\label{alg:nllai}
\KwData{Matched detection and tracklet list $\mathcal{A}_{matched}$, unmatched detection list $\mathcal{D}_{unmatched}$, unmatched tracklet list $\mathcal{T}_{unmatched}$, NLL threshold $\tau$}
\KwResult{New association results $\mathcal{A}'_{matched}$, $\mathcal{D}'_{unmatched}$, $\mathcal{T}'_{unmatched}$}

Similarity matrix $SNLL = NLL(\mathcal{D}_{unmatched}, \mathcal{T}_{unmatched})$ \label{alg:line_snll} \\
$\mathcal{A}'_{matched} \leftarrow$ assoicate $\mathcal{D}_{unmatched}$ and $\mathcal{T}_{unmatched}$ by Hungarian Algorithm with $SNLL$\label{alg:line_hungarian}\\
$\mathcal{D}'_{unmatched} \leftarrow \emptyset, \mathcal{T}'_{unmatched} \leftarrow \emptyset$\\
\For{ $(d, t)$ in $\mathcal{A}'_{matched}$}{\label{alg:line_unmatched_begin}
\If{$SNLL(d,t) > \tau$}{
    $\mathcal{A}'_{matched} = \mathcal{A}'_{matched} \backslash \{(d,t)\}$\\
    $\mathcal{D}'_{unmatched} = \mathcal{D}'_{unmatched} \cup \{d\}$\\
    $\mathcal{T}'_{unmatched} = \mathcal{T}'_{unmatched} \cup \{t\}$\\
}
}\label{alg:line_unmatched_end}

$\mathcal{A}'_{matched} = \mathcal{A}_{matched} \cup \mathcal{A}'_{matched}$\\

\end{algorithm}
\vspace{-10pt}

\textbf{Negative Log Likelihood-based Association Improvement (NLLAI):} Using Intersection over Union (IoU)-based similarity score cannot match low-quality detection results as shown in Section~\ref{sec:intro}, which poses a significant challenge during the association stage. To address this issue, we propose the NLLAI technique as shown in Algorithm~\ref{alg:nllai}. We first define Negative Log Likelihood (NLL) between the predicted locations of tracklets and the detected locations as a novel similarity score:
\vspace{-10pt}
\begin{equation}
    snll = - \frac{1}{I} \sum_{i=1}^I \log P(\dot{y}_i|\hat{y}_i, \hat{\sigma}_i),
\label{eq:nll_sim}
\end{equation}
%\vspace{-8pt}
where $\dot{y}_i$ is the predicted value for the $i$-th variable of the tracklet from the motion prediction model such as KF. As Subsection~\ref{subsec:uq_detection}, the distribution of each $i$-th variable for detected objects is a single-variate Gaussian distribution where $\hat{y}_i$ is the mean and $\hat{\sigma}_i$ is the standard deviation. Given the set of unmatched detections and unmatched tracklets after the original association method, we compute the NLL similarity matrix $SNLL$ with Equation~\ref{eq:nll_sim} (see Line~\ref{alg:line_snll}). 

Then we utilize the Hungarian algorithm to establish associations between unmatched detections and unmatched tracklets based on $SNLL$.
To eliminate matched pairs with high NLL scores, we introduce a hyperparameter denoted by $\tau$ as the NLL score threshold. Specifically, any matched pairs with $s_{NLL} > \tau$ shall be deemed ineligible for further consideration (see Lines~\ref{alg:line_unmatched_begin}-\ref{alg:line_unmatched_end}). 

The time complexity of NLLAI depends on the number of input unmatched detections $N_D$ and the number of input unmatched tracklets $N_T$. The time complexity of computing NLL can be optimized to be $O(1)$~\cite{numerical3}, so computing the similarity matrix needs $O(N_DN_T)$ time. Assuming $N_D>N_T$, the time complexity of associating with the Hungarian Algorithm can be $O(N_D^3)$~\cite{kuhn1955hungarian}. Thus the time complexity of our NLLAI is $O(N_D^3)$ which is polynomial.
