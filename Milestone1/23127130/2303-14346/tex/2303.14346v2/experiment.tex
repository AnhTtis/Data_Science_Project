\section{Experiment}
\label{sec:experiment}

\begin{figure}[t]
    \centering    
    \includegraphics[width=0.48\textwidth]{figs/exp.pdf}
    %\vspace{-8pt}
    \caption{\textbf{Visualization of results of the detection, original SORT, and our MOT-CUP framework over consecutive three frames.} The collaborative object detector here is Upper-bound. Green boxes are ground truth bounding boxes, orange boxes are detected bounding boxes, and red boxes are tracklets' bounding boxes as the output of MOT. The numbers next to the red boxes indicate object IDs. We observe that our MOT-CUP outperforms the original SORT algorithm in tracking object 332, as indicated by the red arrow. Furthermore, MOT-CUP improves the accuracy of location, compared with the object detector, such as object 332 in frame 60. Overall, our results demonstrate the importance of considering uncertainty in MOT.}
    \label{fig:exp}
    \vspace{-20pt}
\end{figure}

% \begin{table*}
%   \centering
%   \caption{CRPS comparison on object detection and MOT with/without conformal prediction. The best results are shown in \textbf{bold}.}
%   %\vspace{-5pt}
%   \label{tab:crps}
%   %\tabcolsep=4pt
%   \begin{tabular}{|c|c|c|c|c|c|c|c|}
%   \hline
%    \multirow{2}{*}{Base} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{CRPS @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{CRPS @IoU=0.7 $\downarrow$}\\
%   \cline{3-8}
%   & & UB& DN & LB& UB& DN& LB \\
%   \hline
%   & detection w/o CP & 0.453 & 0.498 & 0.693 & 0.392 & 0.459 & 0.514 \\
%   \cline{2-8}
%   SORT& detection w CP & 0.424 & 0.466 & 0.652 & 0.364 & 0.427 & 0.475 \\
%   \cline{2-8}
%   \cite{bewley2016simple}& MOT-CUP w/o CP & 0.312 & 0.355 & 0.463 & 0.297 & 0.347 & 0.409 \\
%   \cline{2-8}
%   & MOT-CUP w CP & \textbf{0.301} & \textbf{0.336} & \textbf{0.444} & \textbf{0.286} & \textbf{0.329} & \textbf{0.392} \\
%   \hline
%   & detection w/o CP & 0.392 & 0.391 & 0.597 & 0.338 & 0.357 & 0.358 \\
%   \cline{2-8}
%   Byte-& detection w CP & 0.381 & 0.376 & 0.596 & 0.328 & 0.343 & 0.358 \\
%   \cline{2-8}
%   Track& MOT-CUP w/o CP & 0.302 & 0.318 & 0.412 & 0.276 & 0.308 & 0.310 \\
%   \cline{2-8}
%   \cite{zhang2021bytetrack}& MOT-CUP w CP & \textbf{0.280} & \textbf{0.286} & \textbf{0.376} & \textbf{0.254} & \textbf{0.275} & \textbf{0.276} \\
%   \hline
%   \end{tabular}
% \end{table*}

% \begin{table*}
%   \centering
%   \caption{NLL comparison on object detection and MOT with/without conformal prediction. The best results are shown in \textbf{bold}.}
%   %\vspace{-5pt}
%   \label{tab:nll}
%   %\tabcolsep=4pt
%   \begin{tabular}{|c|c|c|c|c|c|c|c|}
%   \hline
%    \multirow{2}{*}{Base} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{NLL @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{NLL @IoU=0.7 $\downarrow$}\\
%   \cline{3-8}
%   & & UB& DN & LB& UB& DN& LB \\
%   \hline
%   & detection w/o CP & 193 & 222 & 335 & 96 & 147 & 166 \\
%   \cline{2-8}
%   SORT& detection w CP & 25.70 & 26.21 & 25.17 & 14.54 & 19.50 & 13.04 \\
%   \cline{2-8}
%   \cite{bewley2016simple}&  MOT-CUP w/o CP & 9.61 & 12.09 & 14.45 & 7.87 & 11.21 & 11.06 \\
%   \cline{2-8}
%   &  MOT-CUP w CP & \textbf{0.94} & \textbf{0.95} & \textbf{1.30} & \textbf{0.74} & \textbf{0.90} & \textbf{1.06} \\
%   \hline
%   & detection w/o CP & 1801 & 540 & 461 & 870 & 198 & 121 \\
%   \cline{2-8}
%   Byte-& detection w CP & 43.65 & 32.50 & 68.94 & 23.47 & 19.25 & 11.78 \\
%   \cline{2-8}
%   Track& MOT-CUP w/o CP & 29.49 & 25.30 & 57.23 & 17.67 & 17.57 & 11.27 \\
%   \cline{2-8}
%   \cite{zhang2021bytetrack}& MOT-CUP w CP & \textbf{20.01} & \textbf{6.94} & \textbf{4.05} & \textbf{2.41} & \textbf{1.99} & \textbf{0.99} \\
%   \hline
%   \end{tabular}
% \end{table*}


\subsection{Experimental Setups}
We evaluate the uncertainty propagation framework MOT-CUP using the V2X-Sim dataset~\cite{li2022v2x}, which comprises 80 scenes for training, 10 scenes for validation, and 10 scenes for testing. V2X-Sim was generated using the CARLA simulation~\cite{Dosovitskiy17}. Each scene includes 
%20-second traffic flows at specific intersections, captured at a 5Hz record frequency, resulting in 
100 time-series frames and features 2-5 connected vehicles, from which 3D point clouds are collected using LiDAR sensors. Except for V2X-Sim, there are currently no other open-source datasets tailored explicitly to support COD and MOT. Therefore, our experiments focus solely on utilizing the V2X-Sim dataset. The host machine is a server with Intel Core i9-10900X processors and four NVIDIA Quadro RTX 6000 GPUs.

MOT methods use the tracking-by-detection framework. Object detection quality has a significant impact on tracking performance~\cite{bewley2016simple}. We consider three collaborative object detectors for all tracking approaches as follows:

% \begin{enumerate}
    % \item 
   \textit{ Lower-bound (LB)}~\cite{li2022v2x}: The single-agent object detector, which operates independently by utilizing point cloud data from one single LiDAR sensor without the need for collaboration with other detectors.
    
    % \item 
    \textit{DiscoNet (DN)}~\cite{li2021learning}: The intermediate collaborative object detector employs a directed graph with matrix-valued edge weight to dynamically aggregate features from various agents.
    %while suppressing noisy spatial regions and enhancing informative ones. 
    It demonstrates a favorable trade-off between performance and bandwidth. % consumption by sharing a compact and context-aware scene representation.
    
    % \item 
    \textit{Upper-bound (UB)}~\cite{li2022v2x}: The early collaborative object detector employs raw point cloud data from all connected vehicles to facilitate collaboration. This detector achieves high performance, while retaining lossless information. However, the approach often requires high communication bandwidth.
% \end{enumerate}

We apply our uncertainty propagation framework to two tracking baselines, SORT~\cite{bewley2016simple} and ByteTrack~\cite{zhang2021bytetrack}, and compare their performance in accuracy and uncertainty.
% with that of the respective baselines.
SORT~\cite{bewley2016simple} is a pragmatic approach with simple, effective algorithms by using the Kalman Filter for estimation and the Hungarian algorithm for data association. Instead of only associating detection boxes with high scores, ByteTrack~\cite{zhang2021bytetrack} also utilizes similarities between the low score detection boxes and tracklets 
%to recover true objects and filter out the background detections 
to improve the performance on data association. In our MOT-CUP, we select the NLL threshold  $\tau=1000$ for SORT and $\tau=80$ for ByteTrack. Other hyperparameters such as the IoU threshold, are directly inherited from the original designs of \cite{li2022v2x, zhang2021bytetrack, bewley2016simple, angelopoulos2021gentle}.


% Many metrics have been proposed 
% Here we consider the most widely recognized and accepted ones as follows:
\subsection{Evaluation Metrics}

\noindent \textbf{Accuracy Metrics}:  

    \textit{Higher Order Tracking Accuracy (HOTA)}~\cite{luiten2021hota}: captures the effect of accurate object detection, association, and localization in a well-balanced way. Such a unified measure captures the synergistic impact of these critical aspects and most comprehensively assesses the algorithm's effectiveness.
    
    \textit{Multiple Object Tracking Accuracy (MOTA)}~\cite{bernardin2008evaluating}: quantifies missed detections, false positives and false negatives for detection, and identity switches for the association.
    
    \textit{Multiple Object Tracking Precision (MOTP)}~\cite{bernardin2008evaluating}: measures the ability to estimate precise object locations.
    
    \textit{Frames Per Second (FPS)}: refers to the number of frames processed per second, and measures the time complexity.

It is important to note that higher values of the aforementioned performance metrics indicate better performance in the context of MOT evaluation. %Thus, maximizing the values of these metrics is desirable in order to improve the overall accuracy and effectiveness of MOT algorithms. 
When assessing the performance of MOT algorithms on the same object detector, even slight improvements in HOTA, MOTA, and MOTP mean good progress, as reported by~\cite{zhang2021bytetrack, lakshminarayanan2017simple}.

\noindent \textbf{Uncertainty Metrics}:

    \textit{Negative Log Likelihood (NLL)}~\cite{harakeh2021estimating}: a prevalent metric employed to assess the level of uncertainty in the predicted probability distribution of a given test dataset~\cite{boris2022propagating, lakshminarayanan2017simple, meyer2020learning}.
    
    \textit{Continuous Ranked Probability Score (CRPS)}~\cite{v2019online}: measures the discrepancy between predicted and ground-truth probability distributions~\cite{chung2021uncertainty,korotin20a}. %It evaluates the accuracy of predicted probability distribution by computing the integrated distance between the cumulative distribution function of predicted probabilities and a step function representing the ground-truth value. 
    %CRPS typically takes on values between 0 and 1.

 %Regarding these two uncertainty metrics under consideration, lower values connote a higher degree of precision in uncertainty estimation.
 
Lower values indicate more precise uncertainty estimation.
%for these two uncertainty metrics under consideration.



\subsection{Accuracy Evaluation}\label{subsec:acc}

\begin{table}
\caption{Performance Evaluation of our Uncertainty Propagation Framework on different MOT baselines and object detectors}
\vspace{-15pt}
\label{tab:accuracy}
\tabcolsep=2pt
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Base & Detector & Method & HOTA $\uparrow$& MOTA $\uparrow$& MOTP $\uparrow$& FPS$\uparrow$\\
\hline
 & \multirow{2}{*}{UB} & Base & 41.34 & 52.60 & 85.42 & 1026\\
\cline{3-7}
& & Our & 42.19 & 53.72 & 86.07& 877 \\
\cline{2-7}
SORT& \multirow{2}{*}{DN} & Base & 41.80 & 50.79 & 85.42 & 1052 \\
\cline{3-7}
\cite{bewley2016simple}& & Our & 42.49 & 51.73 & 85.85 & 885 \\
\cline{2-7}
& \multirow{2}{*}{LB} & Base & 31.28 & 27.09 & 85.62 & 1568 \\
\cline{3-7}
& & Our & 31.69 & 27.70 & 85.69 & 1317 \\
\hline
 & \multirow{2}{*}{UB} & Base & 42.05 & 52.90 & 84.47 & 1251 \\
\cline{3-7}
& & Our & 42.56 & 53.77 & 85.49 & 1067 \\
\cline{2-7}
Byte-& \multirow{2}{*}{DN} & Base & 42.64 & 51.48 & 84.01 & 1153 \\
\cline{3-7}
Track& & Our & 43.14 & 52.28 & 84.94 & 1074 \\
\cline{2-7}
\cite{zhang2021bytetrack}& \multirow{2}{*}{LB} & Base & 32.27 & 29.15 & 84.57 & 1637 \\
\cline{3-7}
& & Our & 32.55 & 29.65 & 84.97 & 1457 \\
\hline
\end{tabular}
\vspace{-15pt}
\end{center}
\end{table}

\begin{table}[ht]
% \vspace{-10pt}
\caption{Performance evaluation of our MOT-CUP Framework on various occlusion-level scenarios.}
\vspace{-15pt}
\label{tab:occlusion}
\tabcolsep=2pt
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
Scenario & Method & HOTA $\uparrow$& MOTA $\uparrow$& MOTP $\uparrow$\\
\hline
High & Base & 29.00 & 34.57 & 68.13  \\
\cline{2-5}
OCL & Our & 30.16 (4.01\%) & 36.14 (4.52\%) & 68.85 (1.06\%) \\
\hline
Low & Base & 43.51 & 57.24 & 85.51  \\
\cline{2-5}
OCL & Our & 44.25 (1.71\%) & 58.03 (1.39\%) & 86.07 (0.67\%) \\
\hline
\end{tabular}
\vspace{-25pt}
\end{center}
\end{table}

% \begin{table*}
%   \centering
%   \caption{NLL \& CRPS comparison on detection and MOT with/without conformal prediction (CP). The best results are shown in \textbf{bold}.}
%   \tabcolsep=3pt
%   \vspace{-10pt}
%   \label{tab:nll}
%   %\tabcolsep=4pt
%   \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
%   \hline
%    \multirow{2}{*}{Base} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{NLL @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{NLL @IoU=0.7 $\downarrow$} & \multicolumn{3}{c|}{CRPS @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{CRPS @IoU=0.7 $\downarrow$}\\
%   \cline{3-14}
%   & & UB& DN & LB& UB& DN& LB & UB& DN & LB& UB& DN& LB \\
%   \hline
%   & detection w/o CP & 193 & 222 & 335 & 96 & 147 & 166 & 0.453 & 0.498 & 0.693 & 0.392 & 0.459 & 0.514\\
%   \cline{2-14}
%   SORT& detection w CP & 25.70 & 26.21 & 25.17 & 14.54 & 19.50 & 13.04 & 0.424 & 0.466 & 0.652 & 0.364 & 0.427 & 0.475\\
%   \cline{2-14}
%   \cite{bewley2016simple}&  MOT-CUP w/o CP & 9.61 & 12.09 & 14.45 & 7.87 & 11.21 & 11.06 & 0.312 & 0.355 & 0.463 & 0.297 & 0.347 & 0.409\\
%   \cline{2-14}
%   &  MOT-CUP w CP & \textbf{0.94} & \textbf{0.95} & \textbf{1.30} & \textbf{0.74} & \textbf{0.90} & \textbf{1.06} & \textbf{0.301} & \textbf{0.336} & \textbf{0.444} & \textbf{0.286} & \textbf{0.329} & \textbf{0.392} \\
%   \hline
%   & detection w/o CP & 1801 & 540 & 461 & 870 & 198 & 121 & 0.392 & 0.391 & 0.597 & 0.338 & 0.357 & 0.358 \\
%   \cline{2-14}
%   Byte-& detection w CP & 43.65 & 32.50 & 68.94 & 23.47 & 19.25 & 11.78 & 0.381 & 0.376 & 0.596 & 0.328 & 0.343 & 0.358 \\
%   \cline{2-14}
%   Track& MOT-CUP w/o CP & 29.49 & 25.30 & 57.23 & 17.67 & 17.57 & 11.27 & 0.302 & 0.318 & 0.412 & 0.276 & 0.308 & 0.310 \\
%   \cline{2-14}
%   \cite{zhang2021bytetrack}& MOT-CUP w CP & \textbf{20.01} & \textbf{6.94} & \textbf{4.05} & \textbf{2.41} & \textbf{1.99} & \textbf{0.99} & \textbf{0.280} & \textbf{0.286} & \textbf{0.376} & \textbf{0.254} & \textbf{0.275} & \textbf{0.276}\\
%   \hline
%   \end{tabular}
%   \vspace{-5pt}
% \end{table*}

\begin{table*}
  \centering
  \caption{NLL \& CRPS comparisons on detection and MOT-CUP with different uncertainty quantification methods: Dropout (DO), Deep Ensemble (DE) and Conformal Prediction (CP). The best results are shown in \textbf{bold}.}
  \tabcolsep=3pt
  \vspace{-10pt}
  \label{tab:nll}
  %\tabcolsep=4pt
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
   \multirow{2}{*}{Base} & \multirow{2}{*}{Method} & \multirow{2}{*}{DO} & \multirow{2}{*}{DE} & \multirow{2}{*}{CP}& \multicolumn{3}{c|}{NLL @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{NLL @IoU=0.7 $\downarrow$} & \multicolumn{3}{c|}{CRPS @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{CRPS @IoU=0.7 $\downarrow$}\\
  \cline{6-17}
  & & &&&UB& DN & LB& UB& DN& LB & UB& DN & LB& UB& DN& LB \\
  \hline
  & \multirow{4}{*}{Detection}& &&&193 & 222 & 335 & 96 & 147 & 166 & 0.453 & 0.498 & 0.693 & 0.392 & 0.459 & 0.514\\
  \cline{3-17}
  &&\checkmark&&&81.01 & 52.97 & 263 & 36.05 & 26.98 & 124 & 0.512 & 0.554 & 0.745 & 0.453 & 0.517 & 0.572\\
  \cline{3-17}
  &&&\checkmark&&44.01 & 46.98 & 128 & 23.64 & 29.30 & 63.80 & 0.482 & 0.518 & 0.703 & 0.423 & 0.480 & 0.531 \\
  \cline{3-17}
  SORT& & & & \checkmark & 25.70 & 26.21 & 25.17 & 14.54 & 19.50 & 13.04 & 0.424 & 0.466 & 0.652 & 0.364 & 0.427 & 0.475\\
  \cline{2-17}
  \cite{bewley2016simple}&\multirow{4}{*}{MOT-CUP}  &&&  & 9.61 & 12.09 & 14.45 & 7.87 & 11.21 & 11.06 & 0.312 & 0.355 & 0.463 & 0.297 & 0.347 & 0.409\\
  \cline{3-17}
  &&\checkmark&&&6.54 & 3.48 & 18.34 & 4.38 & 3.44 & 11.40 & 0.345 & 0.379 & 0.489 & 0.331 & 0.374 & 0.436 \\
  \cline{3-17}
  &&&\checkmark&&2.56 & 2.31 & 6.15 & 2.17 & 2.23 & 3.82 & 0.338 & 0.360 & 0.483 & 0.324 & 0.354 & 0.431 \\
  \cline{3-17}
  &  &&& \checkmark& \textbf{0.94} & \textbf{0.95} & \textbf{1.30} & \textbf{0.74} & \textbf{0.90} & \textbf{1.06} & \textbf{0.301} & \textbf{0.336} & \textbf{0.444} & \textbf{0.286} & \textbf{0.329} & \textbf{0.392} \\
  \hline
  & \multirow{4}{*}{Detection} &&&& 1801 & 540 & 461 & 870 & 198 & 121 & 0.392 & 0.391 & 0.597 & 0.338 & 0.357 & 0.358 \\
  \cline{3-17}
  &&\checkmark&&&101& 35.08 & 269 & 41.17 & 24.86 & 101 & 0.524 & 0.561 & 0.792 & 0.468 & 0.531 & 0.550\\
  \cline{3-17}
  &&&\checkmark&&72.56 & 38.41 & 156.59 & 35.99 & 30.51 & 57.10 & 0.492 & 0.523 & 0.753 & 0.436 & 0.493 & 0.518\\
  \cline{3-17}
  Byte-& & &&\checkmark& 43.65 & 32.50 & 68.94 & 23.47 & 19.25 & 11.78 & 0.381 & 0.376 & 0.596 & 0.328 & 0.343 & 0.358 \\
  \cline{2-17}
  Track& \multirow{4}{*}{MOT-CUP}&&& & 29.49 & 25.30 & 57.23 & 17.67 & 17.57 & 11.27 & 0.302 & 0.318 & 0.412 & 0.276 & 0.308 & 0.310 \\
  \cline{3-17}
  \cite{zhang2021bytetrack}&&\checkmark&&&24.35 & 7.66 & 26.43 & 6.97 & 7.37 & 21.70 & 0.385 & 0.436 & 0.532 & 0.359 & 0.432 & 0.437 \\
  \cline{3-17}
  &&&\checkmark&&23.52 & 11.61 & 24.62 & 6.20 & 9.89 & 20.77& 0.362 & 0.406 & 0.512 & 0.336 & 0.402 & 0.421 \\
  \cline{3-17}
  & &&&\checkmark& \textbf{20.01} & \textbf{6.94} & \textbf{4.05} & \textbf{2.41} & \textbf{1.99} & \textbf{0.99} & \textbf{0.280} & \textbf{0.286} & \textbf{0.376} & \textbf{0.254} & \textbf{0.275} & \textbf{0.276}\\
  \hline
  \end{tabular}
  \vspace{-5pt}
\end{table*}

\begin{table*}
\caption{Ablation Study on MOT with the Upper-bound and DiscoNet detectors. The best results are shown in \textbf{bold}.}
\label{tab:ablation_ub}
\vspace{-15pt}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Base} & \multirow{2}{*}{CP} & \multirow{2}{*}{SDKF} & \multirow{2}{*}{NLLAI} & \multicolumn{4}{c|}{Upper-bound} & \multicolumn{4}{c|}{DiscoNet} \\
\cline{5-12}
 &  &  &  & HOTA $\uparrow$& MOTA $\uparrow$& MOTP $\uparrow$& FPS $\uparrow$& HOTA $\uparrow$& MOTA $\uparrow$& MOTP $\uparrow$& FPS $\uparrow$\\
\hline
& & & & 41.34 & 52.60 & 85.42 & \textbf{1026} & 41.80 & 50.79 & 85.42 & \textbf{1052}\\
\cline{2-12}
& & \checkmark & & 41.67 & 52.35 & 86.25 & 962 & 42.23 & 50.79 & 86.15 & 985\\
\cline{2-12}
& & & \checkmark & 41.34 & 52.60 & 85.42 & 865 & 41.80 & 50.79 & 85.42 & 873 \\
\cline{2-12}
SORT& \checkmark& \checkmark& & 41.80 & 52.65 & 86.20 & 991 & 42.23 & 50.93 & 86.09 & 993 \\
\cline{2-12}
\cite{bewley2016simple}& \checkmark& &\checkmark & 41.73 & 53.50 & 84.81 & 911 & 41.98 & 51.32 & 84.70 & 899 \\
\cline{2-12}
& & \checkmark& \checkmark& 41.67 & 52.35 & \textbf{86.25} & 841 & 42.23 & 50.80 & \textbf{86.15} & 852 \\
\cline{2-12}
& \checkmark& \checkmark& \checkmark& \textbf{42.19} & \textbf{53.72} & 86.07 & 877 & \textbf{42.49}& \textbf{51.73} & 85.85 & 885 \\
\hline
& & & & 42.05 & 52.90 & 84.47 & \textbf{1251} & 42.64 & 51.48 & 84.01 & \textbf{1153} \\
\cline{2-12}
& & \checkmark & & 42.49 & 53.63 & 85.82 & 1195 & 42.85 & 52.13 & 85.29 & 1078 \\
\cline{2-12}
Byte-& & & \checkmark & 42.06 & 52.97 & 84.45 & 1072 & 42.67 & 51.61 & 83.98 & 1010 \\
\cline{2-12}
Track& \checkmark& \checkmark& & 42.62 & 53.85 & 85.54 & 1219 & 43.09 & 52.21 & 85.04 & 1141 \\
\cline{2-12}
\cite{zhang2021bytetrack}& \checkmark& &\checkmark & 42.12 & 53.21 & 84.30 & 1057 & 42.68 & 51.65 & 83.89 & 1029 \\
\cline{2-12}
& & \checkmark& \checkmark& 42.50 & 53.68 & \textbf{85.80} & 1091 & 42.89 & 52.26 & \textbf{85.26} & 1042 \\
\cline{2-12}
& \checkmark& \checkmark& \checkmark& \textbf{42.56} & \textbf{53.77} & 85.49 & 1067 & \textbf{43.14} & \textbf{52.28} & 84.94 & 1074\\
\hline
\end{tabular}
\vspace{-25pt}
\end{center}
\end{table*}

The outcomes of our framework on the V2X-SIM dataset with three distinct object detectors and two diverse MOT baselines are presented in Table~\ref{tab:accuracy}. The results indicate that our MOT-CUP framework is capable of leveraging quantified uncertainty from COD to enhance the performance of all original MOT algorithms, with up to $0.85$ improvement on HOTA, up to $1.13$ improvement on MOTA and up to $1.03$ improvement on MOTP.  The performance of object detectors can significantly impact the performance of  MOT. Specifically, when the object detector is capable of detecting more objects, such as Upper-bound, our framework can significantly enhance the performance of MOT algorithms. 

\textit{MOT-CUP on high occlusion-level scenarios:} We divide the entire test dataset into two subsets: one with high occlusion scenarios and the other with low occlusion scenarios. We conduct experiments using our MOT-CUP with SORT and Upper-bound, as in Table~\ref{tab:occlusion}. The results demonstrate that our MOT-CUP exhibits superior improvements in high occlusion-level scenarios, with a notable 4.01\% enhancement in HOTA compared to a 1.71\% improvement in HOTA for low occlusion-level scenarios. In high occlusion-level scenarios, the presence of poorly detected objects caused by occlusion 
%typically 
leads to high uncertainty, which our SDKF and NLLAI %technologies effectively 
utilize to enhance the tracking performance.

Fig.~\ref{fig:exp} presents visualizations of Upper-bound, original SORT, and our MOT-CUP framework's results over three consecutive frames. Our MOT-CUP outperforms the original SORT in tracking object 332, as indicated by the red arrow. Moreover, our MOT-CUP improves location accuracy, as shown for object 332 in frame 60, compared to the object detector. These results showcase the effectiveness of our approach in accurately tracking objects, even in challenging scenarios with poor detection or occlusion. Additionally, incorporating uncertainty into the Kalman Filter and association step enables better tracking performance over time.
%\hsy{Fig.~\ref{fig:exp} presents visualizations of Upper-bound, original SORT, and our MOT-CUP framework's results over three consecutive frames. Our MOT-CUP outperforms original SORT in tracking object 332, as indicated by the red arrow. Moreover, our MOT-CUP improves location accuracy, as shown for object 332 in frame 60, compared to the object detector. These results showcase the effectiveness of our approach in accurately tracking objects, even in challenging scenarios with poor detection or occlusion. Additionally, incorporating uncertainty into the Kalman Filter and association step enables better tracking performance over time.}

\subsection{Uncertainty Evaluation}\label{subsec:uncertainty}

We use Negative Log Likelihood (NLL)~\cite{harakeh2021estimating} and Continuous Ranked Probability Score (CRPS)~\cite{v2019online} at IoU thresholds of 0.5 and 0.7 as the uncertainty measurement. 
%We compare four methods: detection without Conformal Prediction (CP), detection with CP, our MOT without CP, and our MOT with CP. 
We compare the uncertainty results of different UQ methods on object detection and MOT-CUP, including dropout (DO), deep ensemble (DE), and our conformal prediction (CP) in Table~\ref{tab:nll}. The vanilla baseline only utilizes direct modeling (DM). The implementations of DO and DE are as same as~\cite{feng2021review}.
%Specifically, the CP methodology is applied to both object detection and MOT, employing a consistent approach across both domains as shown in Subsection~\ref{subsec:uq_detection}. 
The representation formats of bounding boxes utilized by SORT and ByteTrack diverge, necessitating the training of distinct detection models. Consequently, the results on uncertainty are dissimilar between SORT and ByteTrack.

%Table~\ref{tab:nll} shows the NLL and CRPS results on object detection and MOT. The table distinguishes between the performances achieved with and without CP. In terms of NLL, CP can always reduce NLL, with up to $41\times$ improvement. Furthermore, in comparison to detection with CP, our MOT-CUP framework applied to MOT produces outputs with a considerably greater ability for precise uncertainty estimation, with up to $2.67 \times$ improvement. And overfitting of the training dataset in direct modeling results in a substantial increase in the NLL of  detection without CP.

%Table~\ref{tab:nll} shows the NLL and CRPS results on object detection and MOT. 
For NLL, CP outperforms all baselines, with up to $41\times$ improvement. In particular, CP achieves up to 95\% reduction compared to DO and DE. Furthermore, in comparison to object detection, our MOT-CUP framework with CP produces precise uncertainty estimation, with up to $2.67 \times$ improvement. The vanilla object detection shows a significantly large NLL for the DM overfits the training dataset and overestimates the uncertainty of the test dataset.

 % CRPS results demonstrate that CP can effectively reduce the uncertainty in object detection and MOT, improving up to $7.28\%$ in most cases. As same as NLL, our MOT-CUP framework on MOT can  significantly reduce the CRPS, improving of up to $30.8\%$.

Compared with all baselines, CP can effectively reduce the CRPS, with up to 37\% reduction. Specially, it achieves up to 31\%, 37\% and 35\% reduction compared with the vanilla baseline, DO and DE. The reason that DO and DE increase the CRPS might be they cannot fully capture the entire distribution of possible values while CRPS requires the entire predicted distribution to be considered~\cite{chung2021uncertainty}.

\subsection{Ablation Study on Accuracy}\label{subsec:ablation}



We conduct an ablation study to evaluate the contributions of each proposed technique in our MOT-CUP framework as shown in Table~\ref{tab:ablation_ub} with two different detectors and two diverse MOT baselines. CP is shown to contribute significantly to both SDKF and NLLAI. NLLAI, which focuses on refining the association step with a new SNLL metric, yields marked improvements in metrics capturing associations such as HOTA and MOTA. However, an increase in matching potential may lead to a decline in the precision of object localization, as reflected by the decrease in MOTP metric. In contrast, SDKF, where the Kalman Filter takes the COD uncertainty information as its input,  primarily enhances metrics measuring localization, such as HOTA and MOTP, thereby improving the accuracy of object localization estimates. Notably, our proposed framework combined with diverse collaborative object detectors and MOT baselines always achieves the optimal performance outcomes.

\paragraph{Limitation}
In terms of FPS, our framework results in an average decrease of 13.2\%, yet it does not affect the real-time capacity of the MOT algorithms.  It is noteworthy that the increase in time incurred by our framework is polynomial, as discussed in Section~\ref{sec:approach}. Furthermore, we have not implemented any specific strategies aimed at optimizing the quality of code with respect to running time. Therefore, the computational overhead of our framework is acceptable.




