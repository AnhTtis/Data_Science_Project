\documentclass[11pt]{article}
\evensidemargin=-0.1in \oddsidemargin=-0.in
\textwidth=6.5in
\usepackage{amsmath,amssymb}
\setlength{\parindent}{0in}
\setlength{\parskip}{.20in}
\usepackage{color}
\usepackage{hyperref}
\usepackage{soul} % for highlighting
\usepackage[pdftex]{graphicx} % graphics package
\usepackage{subfigure} % subplot package
\graphicspath{{images/}} % declare the path where graphic files are
\DeclareGraphicsExtensions{.pdf,.jpg,.png} % grafic files extensions
\usepackage{epstopdf} % package to include eps. files

\soulregister\ref7
\soulregister\cite7
% \soulregister\url7
% \newcommand{\highlight}[1]{\colorbox{yellow}{$\displaystyle#1$}}

\usepackage{romannum}
\newcommand{\yiming}[1]{\textbf{\textcolor{blue}{TBD: #1}}}
\newcommand{\attention}[1]{\textbf{\textcolor{red}{Attention! #1}}}

\usepackage{booktabs}
\usepackage{multirow}

\begin{document}
	\begin{center}
		Original title: \textbf{Collaborative Multi-Object Tracking \\with Conformal Uncertainty Propagation} \\\vspace{2mm}
		Manuscript No.: \textbf{23-2095}\\
		\vspace{.3in}
		
		\vspace{.4in}
		Responses to Reviewers' Comments
	\end{center}
	Dear Dr. Cesar Cadena, Editor (Visual Perception and Learning), IEEE RA-L
	
	
	The authors greatly appreciate the Associate Editor and the Reviewers for providing the constructive comments, as well as their valuable time and efforts. We have put the utmost efforts to revise the paper accordingly. In this response letter, we include the Reviewers' comments and explain our revisions in response to these specific comments.
	
	Modifications to the paper are highlighted in the new version in order to be tracked easily.
	
	Our responses to the specific comments of the Reviewers are given in the following pages. We have included all comments in \textit{italics} for completeness.
	
	
	Sincerely, \\
	All authors of the submitted manuscript
% 	Chen Feng (Corresponding Author)
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\newpage
	%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\paragraph*{\underline{Comments of Editor-in-Chief:}\\} %\\[0.5cm] 
	%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%{\em The linguistic quality needs improvement. It is essential to make sure that the manuscript reads smoothly- this helps the reader fully appreciate your research findings. Consult a professional. Show all changes made to the revised manuscript.}
	%
	%\hl{We appreciate Editor in Chief for pointing out this problem in our paper. The linguistic quality of this work has been improved after several proofreading and cross-checks.}
	%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%{\em References: order the entries alphabetically.}
	%
	%\hl{We have ordered the  entries alphabetically.}
	%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\paragraph*{\underline{{Comments of the Associate Editor:}}\\} %\\[0.5cm] 
	
	{\em  	Please see the reviewers' comments on
how to improve the paper. One common point is to highlight the novelty
in comparison with the SotA, as reviewers have questioned how it
compares with the literature and specific papers.
        }
	
	\hl{Thank you for highlighting the Reviewers' major comments. 

    To summarize, we have tried our best to address the reviewers' concerns, and our main modifications are listed as follows:
	}
        \begin{itemize}
        \item \hl{Add experimental results and analysis on dropout and deep ensemble methods as additional baselines in Subsection IV.D Uncertainty Evaluation. Experimental results demonstrate that our proposed conformal prediction outperforms these baselines in both two uncertainty evaluation metrics.}
        \item \hl{Add explanation of the reasons why we concentrate on collaborative object detection in Section II Related Work.}
        \item \hl{Add introduction on Reference [12] and explanation of why we do not consider the method in Reference [12] in Section II Related Work.}
        \item \hl{Add the introduction of more similarity scores in the association step, including the Mahalanohis distance and the association log-likelihood distance in Section II Related Work.}
        \end{itemize}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\clearpage
	\paragraph*{\underline{Comments of Reviewer 8:}\\} %\\[0.5cm] 
	
	{\em  
		The authors present the following claims in their paper: They introduce
a novel framework, MOT-CUT, which addresses object detection - a
critical component of autonomous driving systems - and multiple object
tracking. Accurate detection of objects and the quantification of the
associated uncertainties are of utmost importance. While the accuracy
of detection improves with collaborative multi-object tracking, this
work uniquely quantifies the uncertainty and links it to enhanced MOT
performance. In this framework, uncertainties are propagated through
direct modeling and conformal prediction, achieving improvements in
accuracy. Simulations using V2X-sim demonstrate enhanced performance in
terms of accuracy and uncertainty handling compared to baseline
algorithms, with particularly notable improvements in scenarios with
high occlusion.

The proposed method is clearly and comprehensively described in the
manuscript. The references are also appropriately cited.

The experimental results are well-organized into tables, making it easy
to understand. Differences due to the methodology, as well as
comparisons based on different evaluation metrics, are also clearly
presented.
	}    
	
	\hl{Thank you sincerely for this comment. We have carefully revised our paper according to your insightful suggestions.}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	{\em  
		(1)"In Section IV, 'D. Uncertainty Evaluation,' the comparison
experiments, primarily Table III, only differentiate between scenarios
with or without CP. I believe that, for CP, it would be necessary to
compare multiple methods, not just CRPS.
	}    
	
	\hl{ Your suggestion of adding baselines is well received. We have added dropout (DO) and deep ensemble (DE) as baselines to compare with the conformal prediction on the uncertainty evaluation part, using metrics of NLL and CRPS, as shown in Table~\ref{tab:nll}. Here we have multiple baselines, including the vanilla method with direct modeling, dropout and deep ensemble, and multiple uncertainty metrics, including NLL and CRPS, to demonstrate the performance of CP.
    In the context of both object detection and MOT-CUP, all experimental settings remain consistent while employing various uncertainty quantification methods. The vanilla baseline only utilizes direct modeling (DM). The implementations of dropout and deep ensemble are as same as [1]. The findings, as detailed in Table~\ref{tab:nll}, reveal a notable superiority of conformal prediction over alternative methods, specifically deep ensemble and dropout, in terms of NLL and CRPS.
    Specially, compared with the dropout method, conformal prediction achieves up to 95\% reduction in NLL and 37\% reduction in CRPS. Similarly, compared with the deep ensemble method, conformal prediction exhibits a remarkable up to 95\% reduction in NLL and a substantial 35\% in CRPS. The reason that dropout and deep ensemble increase the CRPS might be that they cannot fully capture the entire distribution of possible values while CRPS requires the entire predicted distribution to be considered [49].}

 \begin{table}
  \centering
  \caption{\hl{NLL \& CRPS comparisons on detection and MOT-CUP with different uncertainty quantification methods: Dropout (DO), Deep Ensemble (DE) and Conformal Prediction (CP). The best results are shown in \textbf{bold}.}}
  \tabcolsep=3pt
  \small
  % \vspace{-10pt}
  \label{tab:nll}
  \tabcolsep=1pt
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
   \multirow{2}{*}{Base} & \multirow{2}{*}{Method} & \multirow{2}{*}{DO} & \multirow{2}{*}{DE} & \multirow{2}{*}{CP}& \multicolumn{3}{c|}{NLL @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{NLL @IoU=0.7 $\downarrow$} & \multicolumn{3}{c|}{CRPS @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{CRPS @IoU=0.7 $\downarrow$}\\
  \cline{6-17}
  & & &&&UB& DN & LB& UB& DN& LB & UB& DN & LB& UB& DN& LB \\
  \hline
  & \multirow{4}{*}{Detection}& &&&193 & 222 & 335 & 96 & 147 & 166 & 0.453 & 0.498 & 0.693 & 0.392 & 0.459 & 0.514\\
  \cline{3-17}
  &&\checkmark&&&81.01 & 52.97 & 263 & 36.05 & 26.98 & 124 & 0.512 & 0.554 & 0.745 & 0.453 & 0.517 & 0.572\\
  \cline{3-17}
  &&&\checkmark&&44.01 & 46.98 & 128 & 23.64 & 29.30 & 63.80 & 0.482 & 0.518 & 0.703 & 0.423 & 0.480 & 0.531 \\
  \cline{3-17}
  SORT& & & & \checkmark & 25.70 & 26.21 & 25.17 & 14.54 & 19.50 & 13.04 & 0.424 & 0.466 & 0.652 & 0.364 & 0.427 & 0.475\\
  \cline{2-17}
  [14]&\multirow{4}{*}{MOT-CUP}  &&&  & 9.61 & 12.09 & 14.45 & 7.87 & 11.21 & 11.06 & 0.312 & 0.355 & 0.463 & 0.297 & 0.347 & 0.409\\
  \cline{3-17}
  &&\checkmark&&&6.54 & 3.48 & 18.34 & 4.38 & 3.44 & 11.40 & 0.345 & 0.379 & 0.489 & 0.331 & 0.374 & 0.436 \\
  \cline{3-17}
  &&&\checkmark&&2.56 & 2.31 & 6.15 & 2.17 & 2.23 & 3.82 & 0.338 & 0.360 & 0.483 & 0.324 & 0.354 & 0.431 \\
  \cline{3-17}
  &  &&& \checkmark& \textbf{0.94} & \textbf{0.95} & \textbf{1.30} & \textbf{0.74} & \textbf{0.90} & \textbf{1.06} & \textbf{0.301} & \textbf{0.336} & \textbf{0.444} & \textbf{0.286} & \textbf{0.329} & \textbf{0.392} \\
  \hline
  & \multirow{4}{*}{Detection} &&&& 1801 & 540 & 461 & 870 & 198 & 121 & 0.392 & 0.391 & 0.597 & 0.338 & 0.357 & 0.358 \\
  \cline{3-17}
  &&\checkmark&&&101& 35.08 & 269 & 41.17 & 24.86 & 101 & 0.524 & 0.561 & 0.792 & 0.468 & 0.531 & 0.550\\
  \cline{3-17}
  &&&\checkmark&&72.56 & 38.41 & 156.59 & 35.99 & 30.51 & 57.10 & 0.492 & 0.523 & 0.753 & 0.436 & 0.493 & 0.518\\
  \cline{3-17}
  Byte-& & &&\checkmark& 43.65 & 32.50 & 68.94 & 23.47 & 19.25 & 11.78 & 0.381 & 0.376 & 0.596 & 0.328 & 0.343 & 0.358 \\
  \cline{2-17}
  Track& \multirow{4}{*}{MOT-CUP}&&& & 29.49 & 25.30 & 57.23 & 17.67 & 17.57 & 11.27 & 0.302 & 0.318 & 0.412 & 0.276 & 0.308 & 0.310 \\
  \cline{3-17}
  [13]&&\checkmark&&&24.35 & 7.66 & 26.43 & 6.97 & 7.37 & 21.70 & 0.385 & 0.436 & 0.532 & 0.359 & 0.432 & 0.437 \\
  \cline{3-17}
  &&&\checkmark&&23.52 & 11.61 & 24.62 & 6.20 & 9.89 & 20.77& 0.362 & 0.406 & 0.512 & 0.336 & 0.402 & 0.421 \\
  \cline{3-17}
  & &&&\checkmark& \textbf{20.01} & \textbf{6.94} & \textbf{4.05} & \textbf{2.41} & \textbf{1.99} & \textbf{0.99} & \textbf{0.280} & \textbf{0.286} & \textbf{0.376} & \textbf{0.254} & \textbf{0.275} & \textbf{0.276}\\
  \hline
  \end{tabular}
  \vspace{-5pt}
\end{table}


	{\em 
		(2) While the paper claims to focus on collaborative multi-object tracking
in its title and objectives, the content does not seem to reflect a
collaborative scenario. Indeed, the dataset used in the simulations,
V2X-sim, appears to have been generated in a collaborative scenario.
However, the proposed method itself does not seem to incorporate
elements unique to a collaborative scenario. In other words, isn't the
method indistinguishable from one used in a single-agent scenario? If
so, the authors' emphasis on the term "collaborative" does not seem to
provide a discernible advantage.
	}

	\hl{We deeply appreciate your valuable feedback on our paper. While it is true that our method could potentially be applied to the single-agent object detection (SOD), we chose to concentrate on collaborative object detection (COD) due to the initial focus of our work and space constraints. In our study, our initial emphasis on investigating uncertainty propagation from COD is rooted in its superior performance compared to SOD. We want to show that even though object detection performance has already been improved, uncertainty propagation from an advanced model such as COD is still important to enhance the overall performance of subsequent modules. Lower-bound is fundamentally a single-agent object detection method and we showed the experimental results on Lower-bound in Table I and Table III. The experiment results with Lower-bound as the object detector can show our proposed method can be applied for SOD. Due to space limitations, we cannot add additional experiments on single-agent datasets, such as KITTI or nuScenes,  for this paper. To avoid making unsupported claims in the paper, we refrained from asserting that our method is universally applicable to the single-agent scenario. Instead, we underscore that the primary focus of our approach lies in addressing uncertainty propagation from COD to multi-object tracking. Your insights are invaluable, and we will carefully consider working more on single-agent datasets as future work.}
    
    
	
	{\em 
		(3) Reference [12] also employs V2X-sim for uncertainty-aware tracking, and
its problem formulation seems strikingly similar to this paper. Has
there been any comparison with that particular work? }
	
	\hl{We sincerely appreciate your insightful feedback on our paper. It's important to note that our work primarily centers around addressing uncertainty propagation from collaborative object detection (COD) to multi-object tracking (MOT) and enhancing MOT performance. In contrast, Reference [12] introduces a distinct Double-M Quantification method, concentrating on uncertainty quantification for COD only. Given the disparate nature of the tasks between Reference [12] and our paper, direct comparisons may be unnecessary. 
    Furthermore, it is essential to note that the bounding box definition presented in Reference [12] relies on corner coordinates, a representation that may not be congruent with tracking algorithms such as SORT and ByteTrack. These tracking methodologies characterize bounding boxes through the utilization of center coordinates, height, and width, thereby rendering the corner-coordinate-based definition unsuitable for seamless integration with SORT and ByteTrack. We have also added more explanation about reference [12] in the "Related Work" Section in our paper revision, to avoid confusions from the readers.
    %We appreciate your consideration and will certainly take your feedback into account for future discussions and improvements.
}

\vspace{1cm}

% \clearpage
\hl{We would like to thank Reviewer 8 again. We have revised the paper based on your meticulous comments. Please kindly check the highlighted parts in the revised version of the submitted work. Thank you sincerely for your precious time and kind efforts, which greatly help us in improving the quality of our paper.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\paragraph*{\underline{Comments of Reviewer 27:}\\} %\\[0.5cm] 

{\em  
This paper presents a method which utilises the uncertainty of
multi-sensor object detections to improve tracking performance. The
paper is mostly well-written and the idea seems quite reasonable.
}    

\hl{We appreciate your constructive comments.  We have carefully revised our paper based on your feedback. Each of your comments is addressed one by one in the following responses.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\em  
	(1)  A point that should be clarified is the relationship between the
collaborative aspect and the uncertainty-based method. In the
development of the method, it seems like it could be equally applied to
data from a single sensor, or a collaborative arrangementâ€”the
contribution seems to be independent of that. Is it novel even in the
single sensor case?
	}    

\hl{We deeply appreciate your valuable feedback on our paper. We agree with you that our method could potentially be applied to the single sensor case. Lower-bound is fundamentally a single-agent object detection method and we showed the experimental results on Lower-bound in Table I and Table III. And it is novel even in the single sensor case. To the best of our knowledge, there is no work on uncertainty propagation from object detection (including on the single sensor and the collaborative arrangement) to multi-object tracking yet. In our study, we chose to concentrate on collaborative object detection (COD) due to the initial focus of our work and space constraints. Our initial emphasis on investigating uncertainty propagation from COD is rooted in its superior performance compared to SOD. We want to show that even though object detection performance has already been improved, uncertainty propagation from an advanced model such as COD is still important to enhance the overall performance of subsequent modules. Due to space limitations, we cannot add additional experiments on a single-agent dataset or a single-sensor dataset for this time. To avoid making unsupported claims in the paper, we refrained from asserting that our method is universally applicable to the single-sensor scenario. Instead, we underscore that the primary focus of our approach lies in addressing uncertainty propagation from COD to multi-object tracking. Your insights are invaluable, and we will carefully consider them for future work.
}



{\em  
	(2) In classical tracking literature (eg , the standard association score
is the log likelihood, which in the case of the KF is $residual^T (H P
H^T + R )^{-1} residual$, where H is the forward model and P is the KF
covariance. Eq (6) looks like it excludes the KF covariance, ie it will
end up as $residual^T R^{-1} residual$. Has the other alternative been
considered? Linking to this extensive literature is important given
that the proposed concept is standard there.
	} 

%\hl{Thank you sincerely for this comment. There are other similarity scores for the standard association step in the literature. We have added the introduction for the Mahalanobis distance as you mentioned and the ReId. The Mahalanobis distance is also a widely used similarity score for considering the measurement uncertainty from detections and state uncertainty of motion prediction models [38], [39]. Nonetheless, substantial state uncertainty could result in minimal distances, leading to erroneous matching [17], [40]. Compared with the Mahalanobis distance, our Eq (6) only considers the measurement uncertainty, which will avoid these erroneous matchings. Appearance similarity is measured by the cosine similarity of the Re-ID features which is extracted by a stand-alone Re-ID model [17]. However, the presence of multiple objects sharing similar appearances can lead to identity switches or mismatches and it is not suitable to point cloud data [41]. }

\hl{Thank you sincerely for this comment. (1). There are other similarity scores for the standard association step in the literature, we appreciated the reviewer to point this out and we have modified our paper accordingly. We have added the introduction about the Mahalanobis distance and the association log-likelihood distance which you mentioned in the "Related Work" Section part (c) of our paper revision. (2). We would like to clarify some misunderstandings about the question related to $residual^T (H P
H^T + R )^{-1} residual$. The Mahalanobis distance is a widely used similarity score by quantifying the dissimilarity between the mean of detected objects and the distributions of tracklets from the KF model with $\dot{y}$ as the mean and $(HP^{-}H^T+R)$ as the variance [38]. Nonetheless, substantial uncertainty could result in minimal distances, leading to erroneous matching [39], [17]. The work in [39] proposes the association log-likelihood distance to overcome this problem by computing the logarithmized association probability between the detections and distributions of tracklets from the KF model with $\dot{y}$ as the mean and $(HP^{-}H^T+R)$ as the variance. In the formula, $(HP^{-}H^T+R)$, the variance $R$  comes from the variance of the object in the previous frames, not the current detected object. Our proposed Negative Log Likelihood (NLL) is $- \frac{1}{I} \sum_{i=1}^I \log P(\dot{y}_i|\hat{y}_i, \hat{\sigma}_i)$, where $\dot{y}_i$ is the predicted value for the $i$-th variable of the tracklet from the KF model. The distribution of each $i$-th variable for the current detected objects is a single-variate Gaussian distribution where $\hat{y}_i$ is the mean and $\hat{\sigma}_i$ is the standard deviation. We compute the NLL between the mean of tracklets and the distribution of the detected objects. The $\hat{\sigma}_i$ in our Eq (6) and the $R$ in $(HP^{-}H^T+R)$ are not corresponding to the same object in the same frame. So our NLL in Eq (6) is not the modified log-likelihood distance version that excludes the KF covariance. We hope this explanation clarified the meaning of the NLL metric and Eq (6) in this paper.} 

\hl{Here, we also utilize the log operation to overcome the problem of large uncertainty. Compared with [39], our NLL focuses on the distribution of current detections and can facilitate more accurate associations for low-quality detections. Due to space limitations, we did not add experiments to compare these similarity scores. Your insights are invaluable, and we will carefully consider them for future work.}
	

\vspace{1cm}
	\hl{Finally, we also would like to thank Reviewer 27 again. We have carefully revised the paper based on your meticulous comments. Please kindly check the highlighted parts in the new version of the paper. Thank you sincerely for your great endeavors to improve the quality of our work.}
	
	
% \begin{figure}[t] % t
% 	\centering
% 	\includegraphics[width=1\textwidth]{image/seg_compress.png} 
% 	\caption{Segmentation compression (w/ RSU)}\label{fig:CLE}
% \end{figure} 

% \begin{figure}[t] % t
% 	\centering
% 	\includegraphics[width=1\textwidth]{image/seg_compress_no_rsu.png} 
% 	\caption{Segmentation compression (w/o RSU)}\label{fig:CLE}
% \end{figure} 

% \begin{figure}[t] % t
% 	\centering
% 	\includegraphics[width=1\textwidth]{image/seg_noise.png} 
% 	\caption{Segmentation noise (w/ RSU)}\label{fig:CLE}
% \end{figure} 

% \begin{figure}[t] % t
% 	\centering
% 	\includegraphics[width=1\textwidth]{image/seg_noise_no_rsu.png} 
% 	\caption{Segmentation noise (w/o RSU)}\label{fig:CLE}
% \end{figure} 

% \begin{figure}[t] % t
% 	\centering
% 	\includegraphics[width=1\textwidth]{image/seg_noise_test_only.png} 
% 	\caption{Segmentation noise (w/ RSU), noise only applied during testing}\label{fig:CLE}
% \end{figure} 

% \begin{figure}[t] % t
% 	\centering
% 	\includegraphics[width=1\textwidth]{image/seg_noise_test_only_no_rsu.png} 
% 	\caption{Segmentation noise (w/o RSU), noise only applied during testing}\label{fig:CLE}
% \end{figure} 

\end{document} 