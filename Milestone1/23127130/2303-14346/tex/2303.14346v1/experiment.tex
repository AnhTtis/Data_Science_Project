\section{Experiment}
\label{sec:experiment}

\begin{figure}[t]
    \centering    
    \includegraphics[width=0.48\textwidth]{figs/exp.pdf}
    %\vspace{-8pt}
    \caption{\textbf{Visualization of results of the detection, original SORT, and our MOT-CUP framework over consecutive three frames.} The collaborative object detector we used here is Upper-bound. In this visualization, green boxes are ground truth bounding boxes, orange boxes are detected bounding boxes, and red boxes are tracklets' bounding boxes as the output of MOT. The numbers beside the red boxes indicate object IDs. We observe that our MOT-CUP outperforms the original SORT algorithm in tracking object 332, as indicated by the red arrow. Furthermore, our MOT-CUP improves the accuracy of location, compared with the object detector, such as object 332 in frame 60. Overall, our results demonstrate the importance of considering uncertainty in MOT.}
    \label{fig:exp}
    \vspace{-20pt}
\end{figure}

\begin{table}
\caption{Performance Evaluation of our Uncertainty Propagation Framework on different MOT baselines and object detectors}
\vspace{-15pt}
\label{tab:accuracy}
\tabcolsep=2pt
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Base & Detector & Method & HOTA $\uparrow$& MOTA $\uparrow$& MOTP $\uparrow$& FPS$\uparrow$\\
\hline
 & \multirow{2}{*}{UB} & Base & 41.34 & 52.60 & 85.42 & 1026\\
\cline{3-7}
& & Our & 42.19 & 53.72 & 86.07& 877 \\
\cline{2-7}
SORT& \multirow{2}{*}{DN} & Base & 41.80 & 50.79 & 85.42 & 1052 \\
\cline{3-7}
\cite{bewley2016simple}& & Our & 42.49 & 51.73 & 85.85 & 885 \\
\cline{2-7}
& \multirow{2}{*}{LB} & Base & 31.28 & 27.09 & 85.62 & 1568 \\
\cline{3-7}
& & Our & 31.69 & 27.70 & 85.69 & 1317 \\
\hline
 & \multirow{2}{*}{UB} & Base & 42.05 & 52.90 & 84.47 & 1251 \\
\cline{3-7}
& & Our & 42.56 & 53.77 & 85.49 & 1067 \\
\cline{2-7}
Byte-& \multirow{2}{*}{DN} & Base & 42.64 & 51.48 & 84.01 & 1153 \\
\cline{3-7}
Track& & Our & 43.14 & 52.28 & 84.94 & 1074 \\
\cline{2-7}
\cite{zhang2021bytetrack}& \multirow{2}{*}{LB} & Base & 32.27 & 29.15 & 84.57 & 1637 \\
\cline{3-7}
& & Our & 32.55 & 29.65 & 84.97 & 1457 \\
\hline
\end{tabular}
\vspace{-20pt}
\end{center}
\end{table}

\begin{table*}
  \centering
  \caption{NLL \& CRPS comparison on object detection and MOT with/without conformal prediction. The best results are shown in \textbf{bold}.}
  \tabcolsep=3pt
  %\vspace{-2pt}
  \label{tab:nll}
  %\tabcolsep=4pt
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
   \multirow{2}{*}{Base} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{NLL @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{NLL @IoU=0.7 $\downarrow$} & \multicolumn{3}{c|}{CRPS @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{CRPS @IoU=0.7 $\downarrow$}\\
  \cline{3-14}
  & & UB& DN & LB& UB& DN& LB & UB& DN & LB& UB& DN& LB \\
  \hline
  & detection w/o CP & 193 & 222 & 335 & 96 & 147 & 166 & 0.453 & 0.498 & 0.693 & 0.392 & 0.459 & 0.514\\
  \cline{2-14}
  SORT& detection w CP & 25.70 & 26.21 & 25.17 & 14.54 & 19.50 & 13.04 & 0.424 & 0.466 & 0.652 & 0.364 & 0.427 & 0.475\\
  \cline{2-14}
  \cite{bewley2016simple}&  MOT-CUP w/o CP & 9.61 & 12.09 & 14.45 & 7.87 & 11.21 & 11.06 & 0.312 & 0.355 & 0.463 & 0.297 & 0.347 & 0.409\\
  \cline{2-14}
  &  MOT-CUP w CP & \textbf{0.94} & \textbf{0.95} & \textbf{1.30} & \textbf{0.74} & \textbf{0.90} & \textbf{1.06} & \textbf{0.301} & \textbf{0.336} & \textbf{0.444} & \textbf{0.286} & \textbf{0.329} & \textbf{0.392} \\
  \hline
  & detection w/o CP & 1801 & 540 & 461 & 870 & 198 & 121 & 0.392 & 0.391 & 0.597 & 0.338 & 0.357 & 0.358 \\
  \cline{2-14}
  Byte-& detection w CP & 43.65 & 32.50 & 68.94 & 23.47 & 19.25 & 11.78 & 0.381 & 0.376 & 0.596 & 0.328 & 0.343 & 0.358 \\
  \cline{2-14}
  Track& MOT-CUP w/o CP & 29.49 & 25.30 & 57.23 & 17.67 & 17.57 & 11.27 & 0.302 & 0.318 & 0.412 & 0.276 & 0.308 & 0.310 \\
  \cline{2-14}
  \cite{zhang2021bytetrack}& MOT-CUP w CP & \textbf{20.01} & \textbf{6.94} & \textbf{4.05} & \textbf{2.41} & \textbf{1.99} & \textbf{0.99} & \textbf{0.280} & \textbf{0.286} & \textbf{0.376} & \textbf{0.254} & \textbf{0.275} & \textbf{0.276}\\
  \hline
  \end{tabular}
\end{table*}

\begin{table*}
\caption{Ablation Study on Tracking with the Upper-bound and DiscoNet detectors. The best results are shown in \textbf{bold}.}
\label{tab:ablation_ub}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Base} & \multirow{2}{*}{CP} & \multirow{2}{*}{SDKF} & \multirow{2}{*}{NLLAI} & \multicolumn{4}{c|}{Upper-bound} & \multicolumn{4}{c|}{DiscoNet} \\
\cline{5-12}
 &  &  &  & HOTA $\uparrow$& MOTA $\uparrow$& MOTP $\uparrow$& FPS $\uparrow$& HOTA $\uparrow$& MOTA $\uparrow$& MOTP $\uparrow$& FPS $\uparrow$\\
\hline
& & & & 41.34 & 52.60 & 85.42 & \textbf{1026} & 41.80 & 50.79 & 85.42 & \textbf{1052}\\
\cline{2-12}
& & \checkmark & & 41.67 & 52.35 & 86.25 & 962 & 42.23 & 50.79 & 86.15 & 985\\
\cline{2-12}
& & & \checkmark & 41.34 & 52.60 & 85.42 & 865 & 41.80 & 50.79 & 85.42 & 873 \\
\cline{2-12}
SORT& \checkmark& \checkmark& & 41.80 & 52.65 & 86.20 & 991 & 42.23 & 50.93 & 86.09 & 993 \\
\cline{2-12}
\cite{bewley2016simple}& \checkmark& &\checkmark & 41.73 & 53.50 & 84.81 & 911 & 41.98 & 51.32 & 84.70 & 899 \\
\cline{2-12}
& & \checkmark& \checkmark& 41.67 & 52.35 & \textbf{86.25} & 841 & 42.23 & 50.80 & \textbf{86.15} & 852 \\
\cline{2-12}
& \checkmark& \checkmark& \checkmark& \textbf{42.19} & \textbf{53.72} & 86.07 & 877 & \textbf{42.49}& \textbf{51.73} & 85.85 & 885 \\
\hline
& & & & 42.05 & 52.90 & 84.47 & \textbf{1251} & 42.64 & 51.48 & 84.01 & \textbf{1153} \\
\cline{2-12}
& & \checkmark & & 42.49 & 53.63 & 85.82 & 1195 & 42.85 & 52.13 & 85.29 & 1078 \\
\cline{2-12}
Byte-& & & \checkmark & 42.06 & 52.97 & 84.45 & 1072 & 42.67 & 51.61 & 83.98 & 1010 \\
\cline{2-12}
Track& \checkmark& \checkmark& & 42.62 & 53.85 & 85.54 & 1219 & 43.09 & 52.21 & 85.04 & 1141 \\
\cline{2-12}
\cite{zhang2021bytetrack}& \checkmark& &\checkmark & 42.12 & 53.21 & 84.30 & 1057 & 42.68 & 51.65 & 83.89 & 1029 \\
\cline{2-12}
& & \checkmark& \checkmark& 42.50 & 53.68 & \textbf{85.80} & 1091 & 42.89 & 52.26 & \textbf{85.26} & 1042 \\
\cline{2-12}
& \checkmark& \checkmark& \checkmark& \textbf{42.56} & \textbf{53.77} & 85.49 & 1067 & \textbf{43.14} & \textbf{52.28} & 84.94 & 1074\\
\hline
\end{tabular}
\vspace{-20pt}
\end{center}
\end{table*}





% \begin{table*}
%   \centering
%   \caption{CRPS comparison on object detection and MOT with/without conformal prediction. The best results are shown in \textbf{bold}.}
%   %\vspace{-5pt}
%   \label{tab:crps}
%   %\tabcolsep=4pt
%   \begin{tabular}{|c|c|c|c|c|c|c|c|}
%   \hline
%    \multirow{2}{*}{Base} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{CRPS @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{CRPS @IoU=0.7 $\downarrow$}\\
%   \cline{3-8}
%   & & UB& DN & LB& UB& DN& LB \\
%   \hline
%   & detection w/o CP & 0.453 & 0.498 & 0.693 & 0.392 & 0.459 & 0.514 \\
%   \cline{2-8}
%   SORT& detection w CP & 0.424 & 0.466 & 0.652 & 0.364 & 0.427 & 0.475 \\
%   \cline{2-8}
%   \cite{bewley2016simple}& MOT-CUP w/o CP & 0.312 & 0.355 & 0.463 & 0.297 & 0.347 & 0.409 \\
%   \cline{2-8}
%   & MOT-CUP w CP & \textbf{0.301} & \textbf{0.336} & \textbf{0.444} & \textbf{0.286} & \textbf{0.329} & \textbf{0.392} \\
%   \hline
%   & detection w/o CP & 0.392 & 0.391 & 0.597 & 0.338 & 0.357 & 0.358 \\
%   \cline{2-8}
%   Byte-& detection w CP & 0.381 & 0.376 & 0.596 & 0.328 & 0.343 & 0.358 \\
%   \cline{2-8}
%   Track& MOT-CUP w/o CP & 0.302 & 0.318 & 0.412 & 0.276 & 0.308 & 0.310 \\
%   \cline{2-8}
%   \cite{zhang2021bytetrack}& MOT-CUP w CP & \textbf{0.280} & \textbf{0.286} & \textbf{0.376} & \textbf{0.254} & \textbf{0.275} & \textbf{0.276} \\
%   \hline
%   \end{tabular}
% \end{table*}

% \begin{table*}
%   \centering
%   \caption{NLL comparison on object detection and MOT with/without conformal prediction. The best results are shown in \textbf{bold}.}
%   %\vspace{-5pt}
%   \label{tab:nll}
%   %\tabcolsep=4pt
%   \begin{tabular}{|c|c|c|c|c|c|c|c|}
%   \hline
%    \multirow{2}{*}{Base} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{NLL @IoU=0.5 $\downarrow$}&\multicolumn{3}{c|}{NLL @IoU=0.7 $\downarrow$}\\
%   \cline{3-8}
%   & & UB& DN & LB& UB& DN& LB \\
%   \hline
%   & detection w/o CP & 193 & 222 & 335 & 96 & 147 & 166 \\
%   \cline{2-8}
%   SORT& detection w CP & 25.70 & 26.21 & 25.17 & 14.54 & 19.50 & 13.04 \\
%   \cline{2-8}
%   \cite{bewley2016simple}&  MOT-CUP w/o CP & 9.61 & 12.09 & 14.45 & 7.87 & 11.21 & 11.06 \\
%   \cline{2-8}
%   &  MOT-CUP w CP & \textbf{0.94} & \textbf{0.95} & \textbf{1.30} & \textbf{0.74} & \textbf{0.90} & \textbf{1.06} \\
%   \hline
%   & detection w/o CP & 1801 & 540 & 461 & 870 & 198 & 121 \\
%   \cline{2-8}
%   Byte-& detection w CP & 43.65 & 32.50 & 68.94 & 23.47 & 19.25 & 11.78 \\
%   \cline{2-8}
%   Track& MOT-CUP w/o CP & 29.49 & 25.30 & 57.23 & 17.67 & 17.57 & 11.27 \\
%   \cline{2-8}
%   \cite{zhang2021bytetrack}& MOT-CUP w CP & \textbf{20.01} & \textbf{6.94} & \textbf{4.05} & \textbf{2.41} & \textbf{1.99} & \textbf{0.99} \\
%   \hline
%   \end{tabular}
% \end{table*}


\subsection{Experimental Setups}
We evaluate the uncertainty propagation framework for MOT using the V2X-Sim dataset~\cite{li2022v2x}, which comprises 80 scenes for training, 10 scenes for validation, and 10 scenes for testing. V2X-Sim was generated using the CARLA simulation~\cite{Dosovitskiy17}. Each scene includes 20-second traffic flows at specific intersections, captured at a 5Hz record frequency, resulting in 100 time-series frames. Each scene features 2-5 connected vehicles, from which 3D point clouds are collected using LiDAR sensors. The host machine is a server with Intel Core i9-10900X processors and four NVIDIA Quadro RTX 6000 GPUs.

MOT methods use the tracking-by-detection framework. Object detection quality has a significant impact on tracking performance~\cite{bewley2016simple}. We consider three collaborative object detection for all tracking approaches as follows:

% \begin{enumerate}
    % \item 
   \textit{ Lower-bound (LB)}~\cite{li2022v2x}: The individual object detector, which operates independently by utilizing point cloud data from one individual LiDAR sensor without the need for collaboration with other detectors.
    
    % \item 
    \textit{DiscoNet (DN)}~\cite{li2021learning}: The intermediate collaborative object detector employs a directed graph with matrix-valued edge weight to dynamically aggregate features from various agents while suppressing noisy spatial regions and enhancing informative ones. It demonstrates a favorable trade-off between performance and bandwidth consumption by sharing a compact and context-aware scene representation.
    
    % \item 
    \textit{Upper-bound (UB)}~\cite{li2022v2x}: The early collaborative object detector employs raw point cloud data from all connected vehicles to facilitate collaboration. This detector typically achieves high-performance levels, while retaining lossless information. However, the approach often requires high communication bandwidth.
% \end{enumerate}

We apply our uncertainty propagation framework to  two tracking baselines, SORT~\cite{bewley2016simple} and ByteTrack~\cite{zhang2021bytetrack}, and compare their performance in terms of accuracy and uncertainty.
% with that of the respective baselines.
SORT~\cite{bewley2016simple} is a practical and pragmatic approach with a focus on simple, effective algorithms by using the Kalman Filter for estimation and the Hungarian algorithm for data association. Instead of only associating detection boxes with the high scores, ByteTrack~\cite{zhang2021bytetrack} also utilizes similarities between the low score detection boxes and tracks to recover true objects and filter out the background detections to improve the performance on data association.


% Many metrics have been proposed 
% Here we consider the most widely recognized and accepted ones as follows:
\subsection{Evaluation Metrics}

\noindent \textbf{Evaluation metrics for accuracy}:  

    \textit{Higher Order Tracking Accuracy (HOTA)}~\cite{luiten2021hota}: captures the effect of accurate object detection, association, and localization in a well-balanced way. Such a unified measure captures the synergistic impact of these critical aspects and most comprehensively assesses the algorithm effectiveness.
    
    \textit{Multiple Object Tracking Accuracy (MOTA)}~\cite{bernardin2008evaluating}: measures the percentage of missed detections, false positives and false negatives for detection, and identity switches for the association.
    
    \textit{Multiple Object Tracking Precision (MOTP)}~\cite{bernardin2008evaluating}: measures the ability to estimate precise object locations.
    
    \textit{Frames Per Second (FPS)}: refers to the number of frames processed per second, and measures the time complexity of the algorithm.

It is important to note that higher values of the aforementioned performance metrics are indicative of better performance in the context of MOT evaluation. Thus, maximizing the values of these metrics is desirable in order to improve the overall accuracy and effectiveness of MOT algorithms. When assessing the performance of MOT algorithms on the same object detector, even slight improvements in HOTA, MOTA, and MOTP can be indicative of good progress, as reported by~\cite{lakshminarayanan2017simple, zhang2021bytetrack}.

\noindent \textbf{Evaluation metrics for uncertainty}:

    \textit{Negative Log Likelihood (NLL)}~\cite{harakeh2021estimating}: a prevalent metric employed to assess the level of uncertainty in the predicted probability distribution of a given test dataset~\cite{boris2022propagating, lakshminarayanan2017simple, meyer2020learning}.
    
    \textit{Continuous Ranked Probability Score (CRPS)}~\cite{v2019online}: a metric that measures the discrepancy between predicted and ground-truth probability distributions. It evaluates the accuracy of predicted probability distribution by computing the integrated distance between the cumulative distribution function of predicted probabilities and a step function representing the ground-truth value~\cite{chung2021uncertainty,korotin20a}. CRPS typically takes on values between 0 and 1.

 Regarding these two uncertainty metrics under consideration, lower values connote a higher degree of precision in uncertainty estimation.

\subsection{Accuracy Evaluation}\label{subsec:acc}



The outcomes of our framework on the V2X-SIM dataset with three distinct object detectors and two diverse MOT baselines are presented in Table~\ref{tab:accuracy}. The results indicate that our MOT-CUP framework is capable of leveraging quantified uncertainty from collaborative object detection to enhance the performance of all original MOT algorithms, with up to $0.85$ improvement on HOTA, up to $1.13$ improvement on MOTA and up to $1.03$ improvement on MOTP.  The performance of object detectors can significantly impact the performance of  MOT. Specifically, when the object detector is capable of detecting more number of objects, such as Upper-bound, our framework can significantly enhance the performance of MOT algorithms. 

Fig.~\ref{fig:exp} presents visualizations of Upper-bound, original SORT, and our MOT-CUP framework's results over three consecutive frames. Our MOT-CUP outperforms original SORT in tracking object 332, as indicated by the red arrow. Moreover, our MOT-CUP improves location accuracy, as shown for object 332 in frame 60, compared to the object detector. These results showcase the effectiveness of our approach in accurately tracking objects, even in challenging scenarios with poor detection or occlusion. Additionally, incorporating uncertainty into the Kalman Filter and association step enables better tracking performance over time.
%\hsy{Fig.~\ref{fig:exp} presents visualizations of Upper-bound, original SORT, and our MOT-CUP framework's results over three consecutive frames. Our MOT-CUP outperforms original SORT in tracking object 332, as indicated by the red arrow. Moreover, our MOT-CUP improves location accuracy, as shown for object 332 in frame 60, compared to the object detector. These results showcase the effectiveness of our approach in accurately tracking objects, even in challenging scenarios with poor detection or occlusion. Additionally, incorporating uncertainty into the Kalman Filter and association step enables better tracking performance over time.}

\subsection{Uncertainty Evaluation}\label{subsec:uncertainty}


We use Negative Log Likelihood (NLL)~\cite{harakeh2021estimating} and Continuous Ranked Probability Score (CRPS)~\cite{v2019online} at IoU thresholds of 0.5 and 0.7 as the uncertainty measurement. 
We measure these metrics on four methods: detection without Conformal Prediction, detection with Conformal Prediction, our MOT without Conformal Prediction, and our MOT with Conformal Prediction. Specifically, the Conformal Prediction methodology is applied to both object detection and MOT, employing a consistent approach across both domains as shown in Subsection~\ref{subsec:uq_detection}. The representation formats of bounding boxes utilized by SORT and ByteTrack diverge, necessitating the training of distinct detection models for each. Consequently, the results on uncertainty are dissimilar between SORT and ByteTrack.

Table~\ref{tab:nll} shows the NLL and CRPS results on object detection and MOT. The table distinguishes between the performances achieved with and without Conformal Prediction. In terms of NLL, Conformal Prediction can always reduce NLL, with up to $41\times$ improvement. Furthermore, in comparison to object detection with Conformal Prediction, our uncertainty propagation framework applied to MOT produces outputs with a considerably greater ability for precise uncertainty estimation, with up to $2.67 \times$ improvement. And overfitting of the training dataset in direct modeling results in a substantial increase in the NLL of object detection without Conformal Prediction.

 The results on CRPS also demonstrate that Conformal Prediction can effectively reduce the CRPS for object detection and MOT, with improvements of up to $7.28\%$ observed in most cases. As same as NLL, our uncertainty propagation framework on MOT can  significantly reduce the CRPS, with improvements of up to $30.8\%$ observed.

\subsection{Ablation Study on Accuracy}\label{subsec:ablation}

We conduct an ablation study to evaluate the contributions of each proposed technique in our uncertainty propagation framework as shown in Table~\ref{tab:ablation_ub} with two different detectors and two diverse MOT baselines. Conformal prediction is shown to contribute significantly to both SDKF and NLLAI. NLLAI, which focuses on refining the association step with a new SNLL metric, yields marked improvements in metrics capturing associations such as HOTA and MOTA. However, an increase in matching potential may lead to a decline in the precision of object localization, as reflected by the decrease in MOTP metric. In contrast, SDKF, where the Kalman Filter takes the COD uncertainty information as its input,  primarily enhances metrics measuring localization, such as HOTA and MOTP, thereby improving the accuracy of object localization estimates. Notably, our proposed framework combining with diverse collaborative object detectors and MOT baselines always achieves the optimal performance outcomes.

\paragraph{Limitation}
In term of FPS, our framework results in an average decrease of 13.2\%, yet it does not affect the real-time capacity of the MOT algorithms.  It is noteworthy that the increase in processing time incurred by our framework is polynomial, as discussed in Section~\ref{sec:approach}. Furthermore, we have not implemented any specific strategies aimed at optimizing the quality of code with respect to running time. Therefore, the computational overhead of our framework is acceptable.




