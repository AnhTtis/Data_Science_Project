@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=CVPR,
  year={2016}
}

% self-supervised vit features related work
@inproceedings{
    dino-spectral,
    title={Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised Semantic Segmentation and Localization},
    author={Luke Melas-Kyriazi and Christian Rupprecht and Iro Laina and Andrea Vedaldi},
    year={2022},
    booktitle=CVPR
}

@article{dino-discover,
  title={Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation},
  author={Wouter Van Gansbeke and Simon Vandenhende and Luc Van Gool},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.06363}
}

@article{dino-UnsupervisedSS,
  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},
  author={Mark Hamilton and Zhoutong Zhang and Bharath Hariharan and Noah Snavely and William T. Freeman},
  journal=ICLR,
  year={2021}
}

@inproceedings{dino-LOST,
   title = {Localizing Objects with Self-Supervised Transformers and no Labels},
   author = {Oriane Sim\'eoni and Gilles Puy and Huy V. Vo and Simon Roburin and Spyros Gidaris and Andrei Bursuc and Patrick P\'erez and Renaud Marlet and Jean Ponce},
   booktitle = BMVC,
   year = {2021}
}

@inproceedings{dino-tokencut,
          title={Self-supervised Transformers for Unsupervised Object Discovery using Normalized Cut},
          author={Wang, Yangtao and Shen, Xi and Hu, Shell Xu and Yuan, Yuan and Crowley, James L. and Vaufreydaz, Dominique},
          booktitle=CVPR,
          year={2022}
        }

% --------------------------------------

@article{ibot,
  title={iBOT: Image BERT Pre-Training with Online Tokenizer},
  author={Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
  journal=ICLR,
  year={2022}
}

@article{beit,
      title={{BEiT}: {BERT} Pre-Training of Image Transformers}, 
      author={Hangbo Bao and Li Dong and Furu Wei},
      year={2022},
      journal={ICLR},
}

@inproceedings{mae,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle=CVPR,
  year={2022}
}

@article{deepvit,
	  author    = {Shir Amir and Yossi Gandelsman and Shai Bagon and Tali Dekel},
  	  title     = {Deep ViT Features as Dense Visual Descriptors}, 
	  journal   = {ECCVW What is Motion For},
	  year      = {2022},
  }

@article{affcorrs,
    title = {{One-Shot Transfer of Affordance Regions? AffCorrs!}},
    year = {2022},
    journal = {CoRL},
    author = {Hadjivelichkov, Denis and Zwane, Sicelukwanda and Deisenroth, Marc and Agapito, Lourdes and Kanoulas, Dimitrios}
}

@article{aff_vis_survey,
  title={Visual affordance and function understanding: A survey},
  author={Hassanin, Mohammed and Khan, Salman and Tahtali, Murat},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={3},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{neural_observation_learning,
  title={Neural mechanisms of observational learning},
  author={Burke, Christopher J and Tobler, Philippe N and Baddeley, Michelle and Schultz, Wolfram},
  journal={Proceedings of the National Academy of Sciences},
  volume={107},
  number={32},
  pages={14431--14436},
  year={2010},
  publisher={National Acad Sciences}
}

% Self-supervised ViT related references:
@inproceedings{bas,
  title={Background activation suppression for weakly supervised object localization},
  author={Wu, Pingyu and Zhai, Wei and Cao, Yang},
  booktitle=CVPR,
  year={2022},
}

@article{ag_from_exocentric_imgs+,
  title={Grounded Affordance from Exocentric View},
  author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
  journal={arXiv preprint arXiv:2208.13196},
  year={2022}
}

@article{van2022discovering,
  title={Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation},
  author={Van Gansbeke, Wouter and Vandenhende, Simon and Van Gool, Luc},
  journal={arXiv preprint arXiv:2206.06363},
  year={2022}
}

@article{hamilton2022unsupervised,
  title={Unsupervised Semantic Segmentation by Distilling Feature Correspondences},
  author={Hamilton, Mark and Zhang, Zhoutong and Hariharan, Bharath and Snavely, Noah and Freeman, William T},
  journal={arXiv preprint arXiv:2203.08414},
  year={2022}
}


% ---------------------------------------

@inproceedings{scops,
	title = {SCOPS: Self-Supervised Co-Part Segmentation},
	author = {Hung, Wei-Chih and Jampani, Varun and Liu, Sifei and Molchanov, Pavlo and Yang, Ming-Hsuan and Kautz, Jan},
	booktitle = CVPR,
	year = {2019}
}

@inproceedings{dino-vit,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle=ICCV,
  year={2021}
}

@inproceedings{aff_transfer_hoi,
  title={Affordance Transfer Learning for Human-Object Interaction Detection},
  author={Hou, Zhi and Yu, Baosheng and Qiao, Yu and Peng, Xiaojiang and Tao, Dacheng},
  booktitle=CVPR,
  year={2021}
}

@INPROCEEDINGS{hico-det,
  author = {Yu-Wei Chao and Yunfan Liu and Xieyang Liu and Huayi Zeng and Jia Deng},
  title = {Learning to Detect Human-Object Interactions},
  booktitle = {WACV},
  year = {2018},
}

@INPROCEEDINGS{hico,
  author = {Yu-Wei Chao and Zhan Wang and Yugeng He and Jiaxuan Wang and Jia Deng},
  title = {{HICO}: A Benchmark for Recognizing Human-Object Interactions in Images},
  booktitle = ICCV,
  year = {2015},
}

@article{charades-ego,
  title={Charades-ego: A large-scale dataset of paired third and first person videos},
  author={Sigurdsson, Gunnar A and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
  journal={arXiv preprint arXiv:1804.09626},
  year={2018}
}

@inproceedings{ego-exo,
  title={Ego-exo: Transferring visual representations from third-person to first-person videos},
  author={Li, Yanghao and Nagarajan, Tushar and Xiong, Bo and Grauman, Kristen},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{joint_hand_hotspot,
  title={Joint Hand Motion and Interaction Hotspots Prediction from Egocentric Videos},
  author={Liu, Shaowei and Tripathi, Subarna and Majumdar, Somdeb and Wang, Xiaolong},
  booktitle=CVPR,
  year={2022}
}

@article{kd,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@inproceedings{grad_cam++,
  title={Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks},
  author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  booktitle = {WACV},
  year={2018}
}

@inproceedings{grad_cam,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle=ICCV,
  year={2017}
}

@inproceedings{self_wsss,
  title={Self-supervised Image-specific Prototype Exploration for Weakly Supervised Semantic Segmentation},
  author={Chen, Qi and Yang, Lingxiao and Lai, Jian-Huang and Xie, Xiaohua},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{Re-CAM,
  title={Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation},
  author={Chen, Zhaozheng and Wang, Tan and Wu, Xiongwei and Hua, Xian-Sheng and Zhang, Hanwang and Sun, Qianru},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{cllims,
  title={CLIMS: Cross Language Image Matching for Weakly Supervised Semantic Segmentation},
  author={Xie, Jinheng and Hou, Xianxu and Ye, Kai and Shen, Linlin},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{bridge_wsol,
  title={Bridging the Gap between Classification and Localization for Weakly Supervised Object Localization},
  author={Kim, Eunji and Kim, Siwon and Lee, Jungbeom and Kim, Hyunwoo and Yoon, Sungroh},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{cream,
  title={CREAM: Weakly Supervised Object Localization via Class RE-Activation Mapping},
  author={Xu, Jilan and Hou, Junlin and Zhang, Yuejie and Feng, Rui and Zhao, Rui-Wei and Zhang, Tao and Lu, Xuequan and Gao, Shang},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{TS-CAM,
  title={Ts-cam: Token semantic coupled attention map for weakly supervised object localization},
  author={Gao, Wei and Wan, Fang and Pan, Xingjia and Peng, Zhiliang and Tian, Qi and Han, Zhenjun and Zhou, Bolei and Ye, Qixiang},
  booktitle=ICCV,
  year={2021}
}

@inproceedings{SPA,
  title={Unveiling the potential of structure preserving for weakly supervised object localization},
  author={Pan, Xingjia and Gao, Yingguo and Lin, Zhiwen and Tang, Fan and Dong, Weiming and Yuan, Haolei and Huang, Feiyue and Xu, Changsheng},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{egogaze,
  title={Predicting gaze in egocentric video by learning task-dependent attention transition},
  author={Huang, Yifei and Cai, Minjie and Li, Zhenqiang and Sato, Yoichi},
  booktitle=ECCV,
  year={2018}
}

@inproceedings{EIL,
  title={Erasing integrated learning: A simple yet effective approach for weakly supervised object localization},
  author={Mai, Jinjie and Yang, Meng and Luo, Wenfeng},
  booktitle=CVPR,
  year={2020}
}

@article{deepgazeII,
  title={DeepGaze II: Reading fixations from deep features trained on object recognition},
  author={K{\"u}mmerer, Matthias and Wallis, Thomas SA and Bethge, Matthias},
  journal={arXiv preprint arXiv:1610.01563},
  year={2016}
}

@inproceedings{Mlnet,
  title={A deep multi-level network for saliency prediction},
  author={Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
  booktitle=ICPR,
  year={2016},
}

@inproceedings{weakaff2,
  title={Adaptive binarization for weakly supervised affordance segmentation},
  author={Sawatzky, Johann and Gall, Jurgen},
  booktitle={ICCVW},
  year={2017}
}

@article{cad120,
  title={Learning human activities and object affordances from rgb-d videos},
  author={Koppula, Hema Swetha and Gupta, Rudhir and Saxena, Ashutosh},
  journal={The International journal of robotics research},
  volume={32},
  number={8},
  pages={951--970},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{railroad,
  title={Railroad is not a train: Saliency as pseudo-pixel supervision for weakly supervised semantic segmentation},
  author={Lee, Seungho and Lee, Minhyun and Lee, Jongwuk and Shim, Hyunjung},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{RCA,
  title={Regional semantic contrast and aggregation for weakly supervised semantic segmentation},
  author={Zhou, Tianfei and Zhang, Meijie and Zhao, Fang and Li, Jianwu},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{Acol,
  title={Adversarial complementary learning for weakly supervised object localization},
  author={Zhang, Xiaolin and Wei, Yunchao and Feng, Jiashi and Yang, Yi and Huang, Thomas S},
  booktitle=CVPR,
  year={2018}
}

@article{igibson,
abstract = {We present iGibson, a novel simulation environment to develop robotic solutions for interactive tasks in large-scale realistic scenes. Our environment contains fifteen fully interactive home-sized scenes populated with rigid and articulated objects. The scenes are replicas of 3D scanned real-world homes, aligning the distribution of objects and layout to that of the real world. iGibson integrates several key features to facilitate the study of interactive tasks: i) generation of high-quality visual virtual sensor signals (RGB, depth, segmentation, LiDAR, flow, among others), ii) domain randomization to change the materials of the objects (both visual texture and dynamics) and/or their shapes, iii) integrated sampling-based motion planners to generate collision-free trajectories for robot bases and arms, and iv) intuitive human-iGibson interface that enables efficient collection of human demonstrations. Through experiments, we show that the full interactivity of the scenes enables agents to learn useful visual representations that accelerate the training of downstream manipulation tasks. We also show that iGibson features enable the generalization of navigation agents, and that the human-iGibson interface and integrated motion planners facilitate efficient imitation learning of simple human demonstrated behaviors. iGibson is open-sourced with comprehensive examples and documentation. For more information, visit our project website: http://svl.stanford.edu/igibson/},
archivePrefix = {arXiv},
arxivId = {2012.02924},
author = {Shen, Bokui and Xia, Fei and Li, Chengshu and Mart{\'{i}}n-Mart{\'{i}}n, Roberto and Fan, Linxi and Wang, Guanzhi and Buch, Shyamal and D'Arpino, Claudia and Srivastava, Sanjana and Tchapmi, Lyne P. and Tchapmi, Micael E. and Vainio, Kent and Fei-Fei, Li and Savarese, Silvio},
eprint = {2012.02924},
file = {:D$\backslash$:/Mendeley Papers/iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes - 2020 - Shen et al.pdf:pdf},
mendeley-groups = {Embodied AI/Simulator},
title = {{iGibson, a Simulation Environment for Interactive Tasks in Large Realistic Scenes}},
year = {2020}
}

@article{partafford,
abstract = {Understanding what objects could furnish for humans-namely, learning object affordance-is the crux to bridge perception and action. In the vision community, prior work primarily focuses on learning object affordance with dense (e.g., at a per-pixel level) supervision. In stark contrast, we humans learn the object affordance without dense labels. As such, the fundamental question to devise a computational model is: What is the natural way to learn the object affordance from visual appearance and geometry with humanlike sparse supervision? In this work, we present a new task of part-level affordance discovery (PartAfford): Given only the affordance labels per object, the machine is tasked to (i) decompose 3D shapes into parts and (ii) discover how each part of the object corresponds to a certain affordance category. We propose a novel learning framework for PartAfford, which discovers part-level representations by leveraging only the affordance set supervision and geometric primitive regularization, without dense supervision. The proposed approach consists of two main components: (i) an abstraction encoder with slot attention for unsupervised clustering and abstraction, and (ii) an affordance decoder with branches for part reconstruction, affordance prediction, and cuboidal primitive regularization. To learn and evaluate PartAfford, we construct a part-level, cross-category 3D object affordance dataset, annotated with 24 affordance categories shared among {\textgreater}25, 000 objects. We demonstrate that our method enables both the abstraction of 3D objects and part-level affordance discovery, with generalizability to difficult and cross-category examples. Further ablations reveal the contribution of each component.},
archivePrefix = {arXiv},
arxivId = {2202.13519},
author = {Xu, Chao and Chen, Yixin and Wang, He and Zhu, Song-Chun and Zhu, Yixin and Huang, Siyuan},
eprint = {2202.13519},
file = {:D$\backslash$:/Mendeley Papers//PartAfford Part-level Affordance Discovery from 3D Objects - 2022 - Xu et al.pdf:pdf},
mendeley-groups = {Affordance(HOI)/Vision},
month = {feb},
title = {{PartAfford: Part-level Affordance Discovery from 3D Objects}},
year = {2022}
}


@inproceedings{cam,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle=CVPR,
  year={2016}
}

@article{vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@article{dex3,
abstract = {Dexterous multi-fingered robotic hands have a formidable action space, yet their morphological similarity to the human hand holds immense potential to accelerate robot learning. We propose DexVIP, an approach to learn dexterous robotic grasping from human-object interactions present in in-the-wild YouTube videos. We do this by curating grasp images from human-object interaction videos and imposing a prior over the agent's hand pose when learning to grasp with deep reinforcement learning. A key advantage of our method is that the learned policy is able to leverage free-form in-the-wild visual data. As a result, it can easily scale to new objects, and it sidesteps the standard practice of collecting human demonstrations in a lab -- a much more expensive and indirect way to capture human expertise. Through experiments on 27 objects with a 30-DoF simulated robot hand, we demonstrate that DexVIP compares favorably to existing approaches that lack a hand pose prior or rely on specialized tele-operation equipment to obtain human demonstrations, while also being faster to train. Project page: https://vision.cs.utexas.edu/projects/dexvip-dexterous-grasp-pose-prior},
archivePrefix = {arXiv},
arxivId = {arXiv:2202.00164v1},
author = {Mandikal, Priyanka and Grauman, Kristen and Austin, U T},
eprint = {arXiv:2202.00164v1},
file = {:D$\backslash$:/Mendeley Papers/DexVIP Learning Dexterous Grasping with Human Hand Pose Priors from Video - 2021 - Mandikal, Grauman, Austin.pdf:pdf},
keywords = {computer vision,demonstrations,dexterous manipulation,learning from,learning from observations},
mendeley-groups = {Affordance(HOI)/Vision},
number = {CoRL},
pages = {1--11},
title = {{DexVIP : Learning Dexterous Grasping with Human Hand Pose Priors from Video}},
year = {2021}
}


@article{dex1,
abstract = {Dexterous robotic hands are appealing for their agility and human-like morphology, yet their high degree of freedom makes learning to manipulate challenging. We introduce an approach for learning dexterous grasping. Our key idea is to embed an object-centric visual affordance model within a deep reinforcement learning loop to learn grasping policies that favor the same object regions favored by people. Unlike traditional approaches that learn from human demonstration trajectories (e.g., hand joint sequences captured with a glove), the proposed prior is object-centric and image-based, allowing the agent to anticipate useful affordance regions for objects unseen during policy learning. We demonstrate our idea with a 30-DoF five-fingered robotic hand simulator on 40 objects from two datasets, where it successfully and efficiently learns policies for stable functional grasps. Our affordance-guided policies are significantly more effective, generalize better to novel objects, train 3× faster than the baselines, and are more robust to noisy sensor readings and actuation. Our work offers a step towards manipulation agents that learn by watching how people use objects, without requiring state and action information about the human body. Project website with videos: http://vision.cs.utexas.edu/projects/graff-dexterous-affordance-grasp.},
archivePrefix = {arXiv},
arxivId = {2009.01439},
author = {Mandikal, Priyanka and Grauman, Kristen},
eprint = {2009.01439},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mandikal, Grauman - 2021 - Learning Dexterous Grasping with Object-Centric Visual Affordances(2).pdf:pdf},
isbn = {9781728190778},
issn = {10504729},
mendeley-groups = {Affordance(HOI)/Robotics,Affordance(HOI)/Vision},
pages = {6169--6176},
title = {{Learning Dexterous Grasping with Object-Centric Visual Affordances}},
year = {2021}
}

@article{dex2,
abstract = {Dexterous manipulation with a multi-finger hand is one of the most challenging problems in robotics. While recent progress in imitation learning has largely improved the sample efficiency compared to Reinforcement Learning, the learned policy can hardly generalize to manipulate novel objects, given limited expert demonstrations. In this paper, we propose to learn dexterous manipulation using large-scale demonstrations with diverse 3D objects in a category, which are generated from a human grasp affordance model. This generalizes the policy to novel object instances within the same category. To train the policy, we propose a novel imitation learning objective jointly with a geometric representation learning objective using our demonstrations. By experimenting with relocating diverse objects in simulation, we show that our approach outperforms baselines with a large margin when manipulating novel objects. We also ablate the importance on 3D object representation learning for manipulation. We include videos, code, and additional information on the project website - https://kristery.github.io/ILAD/ .},
archivePrefix = {arXiv},
arxivId = {2204.02320},
author = {Wu, Yueh-Hua and Wang, Jiashun and Wang, Xiaolong},
eprint = {2204.02320},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Wang, Wang - 2022 - Learning Generalizable Dexterous Manipulation from Human Grasp Affordance.pdf:pdf},
mendeley-groups = {Affordance(HOI)/Robotics,Robotics},
title = {{Learning Generalizable Dexterous Manipulation from Human Grasp Affordance}},
year = {2022}
}


@article{manipu_oriented,
abstract = {In order to enable robust operation in unstructured environments, robots should be able to generalize manipulation actions to novel object instances. For example, to pour and serve a drink, a robot should be able to recognize novel containers which afford the task. Most importantly, robots should be able to manipulate these novel containers to fulfill the task. To achieve this, we aim to provide robust and generalized perception of object affordances and their associated manipulation poses for reliable manipulation. In this work, we combine the notions of affordance and category-level pose, and introduce the Affordance Coordinate Frame (ACF). With ACF, we represent each object class in terms of individual affordance parts and the compatibility between them, where each part is associated with a part category-level pose for robot manipulation. In our experiments, we demonstrate that ACF outperforms state-of-the-art methods for object detection, as well as category-level pose estimation for object parts. We further demonstrate the applicability of ACF to robot manipulation tasks through experiments in a simulated environment.},
archivePrefix = {arXiv},
arxivId = {2010.08202},
author = {Chen, Xiaotong and Zheng, Kaizhi and Zeng, Zhen and Basu, Shreshtha and Cooney, James and Pavlasek, Jana and Jenkins, Odest Chadwicke},
eprint = {2010.08202},
file = {:D$\backslash$:/Mendeley Papers/Manipulation-Oriented Object Perception in Clutter through Affordance Coordinate Frames - 2020 - Chen et al.pdf:pdf},
mendeley-groups = {Affordance(HOI)/Robotics},
title = {{Manipulation-Oriented Object Perception in Clutter through Affordance Coordinate Frames}},
year = {2020}
}

@article{aff_semantic_relations,
  title={Learning grasp affordance reasoning through semantic relations},
  author={Ard{\'o}n, Paola and Pairet, Eric and Petrick, Ronald PA and Ramamoorthy, Subramanian and Lohan, Katrin S},
  journal={IEEE Robotics and Automation Letters},
  volume={4},
  number={4},
  pages={4571--4578},
  year={2019},
  publisher={IEEE}
}

@inproceedings{aff_spcae,
  title={Learning affordance space in physical world for vision-based robotic object manipulation},
  author={Wu, Huadong and Zhang, Zhanpeng and Cheng, Hui and Yang, Kai and Liu, Jiaming and Guo, Ziying},
  booktitle={ICRA},
  year={2020},
}

@inproceedings{aff_detect_task_specific_grasp,
  title={Affordance detection for task-specific grasping using deep learning},
  author={Kokic, Mia and Stork, Johannes A and Haustein, Joshua A and Kragic, Danica},
  booktitle={2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids)},
  pages={91--98},
  year={2017},
  organization={IEEE}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=CVPR,
  year={2009}
}

@inproceedings{aff_with_CNN_umd,
  title={Object-based affordances detection with convolutional neural networks and dense conditional random fields},
  author={Nguyen, Anh and Kanoulas, Dimitrios and Caldwell, Darwin G and Tsagarakis, Nikos G},
  booktitle={IROS},
  year={2017}
}

@article{aff_obj_parts,
abstract = {As robots begin to collaborate with humans in everyday workspaces, they will need to understand the functions of tools and their parts. To cut an apple or hammer a nail, robots need to not just know the tool's name, but they must localize the tool's parts and identify their functions. In this extended abstract, we give an overview of our work on localizing and identifying object part affordance. We present a framework which provides 3D predictions of functional parts that can be used by a robot, and we introduce a new RGB-D Part Affordance Dataset with 105 kitchen, workshop, and garden tools. We analyze the usefulness of different features, and show that geometry is key for this task. Finally, we demonstrate that the approach can generalize to novel object categories, so robots like PR2, Asimo, and Baxter could use tools never seen before.},
author = {Myers, Austin and Kanazawa, Angjoo and Fermuller, Cornelia and Aloimonos, Yiannis},
file = {:D$\backslash$:/Mendeley Papers/Affordance of Object Parts from Geometric Features - 2015 - Myers et al.pdf:pdf},
journal = {Int. Conf. Robot. Autom.},
mendeley-groups = {Affordance(HOI)/Vision},
pages = {5--6},
title = {{Affordance of Object Parts from Geometric Features}},
year = {2015}
}

@inproceedings{robothor,
  title={Robothor: An open simulation-to-real embodied ai platform},
  author={Deitke, Matt and Han, Winson and Herrasti, Alvaro and Kembhavi, Aniruddha and Kolve, Eric and Mottaghi, Roozbeh and Salvador, Jordi and Schwenk, Dustin and VanderBilt, Eli and Wallingford, Matthew and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3164--3174},
  year={2020}
}

@inproceedings{partnet,
  title={Partnet: A large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding},
  author={Mo, Kaichun and Zhu, Shilin and Chang, Angel X and Yi, Li and Tripathi, Subarna and Guibas, Leonidas J and Su, Hao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={909--918},
  year={2019}
}

@inproceedings{mask-rcnn,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2961--2969},
  year={2017}
}

@inproceedings{one-shot_aff_detect,
  title={One-Shot Affordance Detection},
  author={Hongchen Luo and Wei Zhai and Jing Zhang and Yang Cao and Dacheng Tao},
  booktitle={IJCAI},
  year={2021}
}

@article{ag_from_exocentric_imgs,
  title={Learning Affordance Grounding from Exocentric Images},
  author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
  journal={arXiv preprint arXiv:2203.09905},
  year={2022}
}

@article{one-shot,
  title={One-Shot Object Affordance Detection in the Wild},
  author={Zhai, Wei and Luo, Hongchen and Zhang, Jing and Cao, Yang and Tao, Dacheng},
  journal={arXiv preprint arXiv:2108.03658},
  year={2021}
}

@article{review,
  title={Building Affordance Relations for Robotic Agents-A Review},
  author={Ard{\'o}n, Paola and Pairet, {\`E}ric and Lohan, Katrin S and Ramamoorthy, Subramanian and Petrick, Ronald},
  journal={arXiv preprint arXiv:2105.06706},
  year={2021}
}

@article{o2o,
  title={O2O-Afford: Annotation-free large-scale object-object affordance learning},
  author={Mo, Kaichun and Qin, Yuzhe and Xiang, Fanbo and Su, Hao and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2106.15087},
  year={2021}
}

@inproceedings{sapien,
  title={Sapien: A simulated part-based interactive environment},
  author={Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu and Wang, He and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11097--11107},
  year={2020}
}

@article{ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  journal={arXiv preprint arXiv:2110.07058},
  year={2021}
}

@article{ai2thor,
  title={Ai2-thor: An interactive 3d environment for visual ai},
  author={Kolve, Eric and Mottaghi, Roozbeh and Han, Winson and VanderBilt, Eli and Weihs, Luca and Herrasti, Alvaro and Gordon, Daniel and Zhu, Yuke and Gupta, Abhinav and Farhadi, Ali},
  journal={arXiv preprint arXiv:1712.05474},
  year={2017}
}

@article{epic,
  title={The epic-kitchens dataset: Collection, challenges and baselines},
  author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others},
  journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  number={01},
  pages={1--1},
  year={2020},
  publisher={IEEE Computer Society}
}

@inproceedings{demo6,
  title={DexPilot: Vision-based teleoperation of dexterous robotic hand-arm system},
  author={Handa, Ankur and Van Wyk, Karl and Yang, Wei and Liang, Jacky and Chao, Yu-Wei and Wan, Qian and Birchfield, Stan and Ratliff, Nathan and Fox, Dieter},
  booktitle={ICRA},
  year={2020}
}

@article{demo7,
  
  
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  journal={arXiv preprint arXiv:1709.10087},
  year={2017}
}

@inproceedings{demo5,
  title={Deep imitation learning for complex manipulation tasks from virtual reality teleoperation},
  author={Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
  booktitle={ICRA},
  year={2018},
}

@inproceedings{demo1,
  title={Visual semantic planning using deep successor representations},
  author={Zhu, Yuke and Gordon, Daniel and Kolve, Eric and Fox, Dieter and Fei-Fei, Li and Gupta, Abhinav and Mottaghi, Roozbeh and Farhadi, Ali},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={483--492},
  year={2017}
}

@inproceedings{demo3,
  title={Two body problem: Collaborative visual task completion},
  author={Jain, Unnat and Weihs, Luca and Kolve, Eric and Rastegari, Mohammad and Lazebnik, Svetlana and Farhadi, Ali and Schwing, Alexander G and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6689--6699},
  year={2019}
}

@inproceedings{demo4,
  title={Embodied question answering},
  author={Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1--10},
  year={2018}
}

@inproceedings{demo2,
  title={Alfred: A benchmark for interpreting grounded instructions for everyday tasks},
  author={Shridhar, Mohit and Thomason, Jesse and Gordon, Daniel and Bisk, Yonatan and Han, Winson and Mottaghi, Roozbeh and Zettlemoyer, Luke and Fox, Dieter},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10740--10749},
  year={2020}
}

@INPROCEEDINGS{chu2016,
  author={Chu, Vivian and Fitzgerald, Tesca and Thomaz, Andrea L.},
  booktitle={2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Learning object affordances by leveraging the combination of human-guidance and self-exploration}, 
  year={2016},
  volume={},
  number={},
  pages={221-228}}

@ARTICLE{fig1,  author={Cruz, Francisco and Magg, Sven and Weber, Cornelius and Wermter, Stefan},  journal={IEEE Transactions on Cognitive and Developmental Systems},   title={Training Agents With Interactive Reinforcement Learning and Contextual Affordances},   year={2016},  volume={8},  number={4},  pages={271-284}}

@book{gibson,
	year = {1979},
	author = {James J. Gibson},
	title = {The Ecological Approach to Visual Perception: Classic Edition},
	publisher = {Houghton Mifflin}
}

@article{Group,
author = {Group, Computational Neuroscience},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Group - Unknown - Learning to Segment Affordances.pdf:pdf},
pages = {769--776},
title = {{Learning to Segment Affordances}}
}

@inproceedings{3d-affordancenet,
  title={3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding},
  author={Deng, Shengheng and Xu, Xun and Wu, Chaozheng and Chen, Ke and Jia, Kui},
  booktitle={CVPR},
  year={2021}
}

@article{shaping,
abstract = {Complex physical tasks entail a sequence of object interactions, each with its own preconditions -- which can be difficult for robotic agents to learn efficiently solely through their own experience. We introduce an approach to discover activity-context priors from in-the-wild egocentric video captured with human worn cameras. For a given object, an activity-context prior represents the set of other compatible objects that are required for activities to succeed (e.g., a knife and cutting board brought together with a tomato are conducive to cutting). We encode our video-based prior as an auxiliary reward function that encourages an agent to bring compatible objects together before attempting an interaction. In this way, our model translates everyday human experience into embodied agent skills. We demonstrate our idea using egocentric EPIC-Kitchens video of people performing unscripted kitchen activities to benefit virtual household robotic agents performing various complex tasks in AI2-iTHOR, significantly accelerating agent learning. Project page: http://vision.cs.utexas.edu/projects/ego-rewards/},
archivePrefix = {arXiv},
arxivId = {2110.07692},
author = {Nagarajan, Tushar and Grauman, Kristen},
eprint = {2110.07692},
file = {:C$\backslash$:/Users/LI GEN/OneDrive - University of Edinburgh/Desktop/2110.07692.pdf:pdf},
number = {IL},
title = {{Shaping embodied agent behavior with activity-context priors from egocentric video}},
year = {2021}
}
@article{Hou2021,
abstract = {Reasoning the human-object interactions (HOI) is essential for deeper scene understanding, while object affordances (or functionalities) are of great importance for human to discover unseen HOIs with novel objects. Inspired by this, we introduce an affordance transfer learning approach to jointly detect HOIs with novel objects and recognize affordances. Specifically, HOI representations can be decoupled into a combination of affordance and object representations, making it possible to compose novel interactions by combining affordance representations and novel object representations from additional images, i.e. transferring the affordance to novel objects. With the proposed affordance transfer learning, the model is also capable of inferring the affordances of novel objects from known affordance representations. The proposed method can thus be used to 1) improve the performance of HOI detection, especially for the HOIs with unseen objects; and 2) infer the affordances of novel objects. Experimental results on two datasets, HICO-DET and HOI-COCO (from V-COCO), demonstrate significant improvements over recent state-of-the-art methods for HOI detection and object affordance detection. Code is available at https://github.com/zhihou7/HOI-CL},
archivePrefix = {arXiv},
arxivId = {2104.02867},
author = {Hou, Zhi and Yu, Baosheng and Qiao, Yu and Peng, Xiaojiang and Tao, Dacheng},
eprint = {2104.02867},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hou et al. - 2021 - Affordance Transfer Learning for Human-Object Interaction Detection.pdf:pdf},
pages = {495--504},
title = {{Affordance Transfer Learning for Human-Object Interaction Detection}},
year = {2021}
}
@article{Mo,
archivePrefix = {arXiv},
arxivId = {arXiv:2106.15087v1},
author = {Mo, Kaichun and Qin, Yuzhe and Xiang, Fanbo and Su, Hao and Guibas, Leonidas},
eprint = {arXiv:2106.15087v1},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mo et al. - Unknown - Object-Object Affordance Learning.pdf:pdf},
keywords = {large-scale learning,object-object affordance,vision for robotics},
pages = {1--17},
title = {{Object-Object Affordance Learning}}
}
@article{aff_robot_survey,
  title={Affordances in robotic tasks--a survey},
  author={Ard{\'o}n, Paola and Pairet, {\`E}ric and Lohan, Katrin S and Ramamoorthy, Subramanian and Petrick, Ronald},
  journal={arXiv preprint arXiv:2004.07400},
  year={2020}
}
@article{Author2021,
abstract = {Learning generalizable manipulation skills is central for robots to achieve task automation in environments with endless scene and object variations. However, existing robot learning environments are limited in both scale and diversity of 3D assets (especially of articulated objects), making it difficult to train and evaluate the generalization ability of agents over novel objects. In this work, we focus on object-level generalization and propose SAPIEN Manipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale learning-from-demonstrations benchmark for articulated object manipulation with visual input (point cloud and image). ManiSkill supports object-level variations by utilizing a rich and diverse set of articulated objects, and each task is carefully designed for learning manipulations on a single category of objects. We equip ManiSkill with high-quality demonstrations to facilitate learning-from-demonstrations approaches and perform evaluations on common baseline algorithms. We believe ManiSkill can encourage the robot learning community to explore more on learning generalizable object manipulation skills.},
archivePrefix = {arXiv},
arxivId = {arXiv:2107.14483v1},
author = {Author, Anonymous and Address, Affiliation},
eprint = {arXiv:2107.14483v1},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Author, Address - 2021 - ManiSkill Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills.pdf:pdf},
keywords = {generalization,learning-from-demonstrations,reinforcement learning},
number = {CoRL},
title = {{ManiSkill : Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills}},
year = {2021}
}
@article{weakly_supervised_affordance_detection,
   abstract = {Localizing functional regions of objects or affordances is an important aspect of scene understanding and relevant for many robotics applications. In this work, we introduce a pixel-wise annotated affordance dataset of 3090 images containing 9916 object instances. Since parts of an object can have multiple affordances, we address this by a convolutional neural network for multilabel affordance segmentation. We also propose an approach to train the network from very few keypoint annotations. Our approach achieves a higher affordance detection accuracy than other weakly supervised methods that also rely on keypoint annotations or image annotations as weak supervision.},
   author = {Johann Sawatzky and Abhilash Srikantha and Juergen Gall},
   isbn = {9781538604571},
   journal = CVPR,
   title = {Weakly supervised affordance detection},
   year = {2017},
}

@inproceedings{tool_parts_iff,
  title={Affordance detection of tool parts from geometric features},
  author={Myers, Austin and Teo, Ching L and Ferm{\"u}ller, Cornelia and Aloimonos, Yiannis},
  booktitle={ICRA},
  year={2015},
}

@article{affordancenet,
   abstract = {We propose AffordanceNet, a new deep learning approach to simultaneously detect multiple objects and their affordances from RGB images. Our AffordanceNet has two branches: an object detection branch to localize and classify the object, and an affordance detection branch to assign each pixel in the object to its most probable affordance label. The proposed framework employs three key components for effectively handling the multiclass problem in the affordance mask: a sequence of deconvolutional layers, a robust resizing strategy, and a multi-task loss function. The experimental results on the public datasets show that our AffordanceNet outperforms recent state-of-the-art methods by a fair margin, while its end-to-end architecture allows the inference at the speed of 150ms per image. This makes our AffordanceNet well suitable for real-time robotic applications. Furthermore, we demonstrate the effectiveness of AffordanceNet in different testing environments and in real robotic applications. The source code is available at https://github.com/nqanh/affordance-net.},
   author = {Thanh Toan Do and Anh Nguyen and Ian Reid},
   journal = {ICRA},
   title = {AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection},
   year = {2018},
}

@article{Gadre2021,
abstract = {People often use physical intuition when manipulating articulated objects, irrespective of object semantics. Motivated by this observation, we identify an important embodied task where an agent must play with objects to recover their parts. To this end, we introduce Act the Part (AtP) to learn how to interact with articulated objects to discover and segment their pieces. By coupling action selection and motion segmentation, AtP is able to isolate structures to make perceptual part recovery possible without semantic labels. Our experiments show AtP learns efficient strategies for part discovery, can generalize to unseen categories, and is capable of conditional reasoning for the task. Although trained in simulation, we show convincing transfer to real world data with no fine-tuning.},
archivePrefix = {arXiv},
arxivId = {2105.01047},
author = {Gadre, Samir Yitzhak and Ehsani, Kiana and Song, Shuran},
eprint = {2105.01047},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gadre, Ehsani, Song - 2021 - Act the Part Learning Interaction Strategies for Articulated Object Part Discovery.pdf:pdf},
title = {{Act the Part: Learning Interaction Strategies for Articulated Object Part Discovery}},
year = {2021}
}
@article{aff-landscape,
abstract = {Embodied agents operating in human spaces must be able to master how their environment works: what objects can the agent use, and how can it use them? We introduce a reinforcement learning approach for exploration for interaction, whereby an embodied agent autonomously discovers the affordance landscape of a new unmapped 3D environment (such as an unfamiliar kitchen). Given an egocentric RGB-D camera and a high-level action space, the agent is rewarded for maximizing successful interactions while simultaneously training an image-based affordance segmentation model. The former yields a policy for acting efficiently in new environments to prepare for downstream interaction tasks, while the latter yields a convolutional neural network that maps image regions to the likelihood they permit each action, densifying the rewards for exploration. We demonstrate our idea with AI2-iTHOR. The results show agents can learn how to use new home environments intelligently and that it prepares them to rapidly address various downstream tasks like “find a knife and put it in the drawer.” Project page: http://vision.cs.utexas.edu/projects/interaction-exploration/},
archivePrefix = {arXiv},
arxivId = {2008.09241},
author = {Nagarajan, Tushar and Grauman, Kristen},
eprint = {2008.09241},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagarajan, Grauman - 2020 - Learning affordance landscapes for interaction exploration in 3D environments.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
number = {NeurIPS},
title = {{Learning affordance landscapes for interaction exploration in 3D environments}},
volume = {2020-Decem},
year = {2020}
}
@article{where2act,
  title={Where2act: From pixels to actions for articulated 3d objects},
  author={Mo, Kaichun and Guibas, Leonidas and Mukadam, Mustafa and Gupta, Abhinav and Tulsiani, Shubham},
  journal={arXiv preprint arXiv:2101.02692},
  year={2021}
}
@article{Khazatsky2021,
abstract = {A generalist robot equipped with learned skills must be able to perform many tasks in many different environments. However, zero-shot generalization to new settings is not always possible. When the robot encounters a new environment or object, it may need to finetune some of its previously learned skills to accommodate this change. But crucially, previously learned behaviors and models should still be suitable to accelerate this relearning. In this paper, we aim to study how generative models of possible outcomes can allow a robot to learn visual representations of affordances, so that the robot can sample potentially possible outcomes in new situations, and then further train its policy to achieve those outcomes. In effect, prior data is used to learn what kinds of outcomes may be possible, such that when the robot encounters an unfamiliar setting, it can sample potential outcomes from its model, attempt to reach them, and thereby update both its skills and its outcome model. This approach, visuomotor affordance learning (VAL), can be used to train goal-conditioned policies that operate on raw image inputs, and can rapidly learn to manipulate new objects via our proposed affordance-directed exploration scheme. We show that VAL can utilize prior data to solve real-world tasks such drawer opening, grasping, and placing objects in new scenes with only five minutes of online experience in the new scene.},
archivePrefix = {arXiv},
arxivId = {2106.00671},
author = {Khazatsky, Alexander and Nair, Ashvin and Jing, Daniel and Levine, Sergey},
eprint = {2106.00671},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khazatsky et al. - 2021 - What Can I Do Here Learning New Skills by Imagining Visual Affordances(2).pdf:pdf},
title = {{What Can I Do Here? Learning New Skills by Imagining Visual Affordances}},
year = {2021}
}
@article{demo2vec,
abstract = {Watching expert demonstrations is an important way for humans and robots to reason about affordances of unseen objects. In this paper, we consider the problem of reasoning object affordances through the feature embedding of demonstration videos. We design the Demo2Vec model which learns to extract embedded vectors of demonstration videos and predicts the interaction region and the action label on a target image of the same object. We introduce the Online Product Review dataset for Affordance (OPRA) by collecting and labeling diverse YouTube product review videos. Our Demo2Vec model outperforms various recurrent neural network baselines on the collected dataset.},
author = {Fang, Kuan and Wu, Te Lin and Yang, Daniel and Savarese, Silvio and Lim, Joseph J.},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fang et al. - 2018 - Demo2Vec Reasoning Object Affordances from Online Videos.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = CVPR,
title = {{Demo2Vec: Reasoning Object Affordances from Online Videos}},
year = {2018}
}
@article{grounded,
abstract = {Learning how to interact with objects is an important step towards embodied visual intelligence, but existing techniques suffer from heavy supervision or sensing requirements. We propose an approach to learn human-object interaction 'hotspots' directly from video. Rather than treat affordances as a manually supervised semantic segmentation task, our approach learns about interactions by watching videos of real human behavior and anticipating afforded actions. Given a novel image or video, our model infers a spatial hotspot map indicating where an object would be manipulated in a potential interaction even if the object is currently at rest. Through results with both first and third person video, we show the value of grounding affordances in real human-object interactions. Not only are our weakly supervised hotspots competitive with strongly supervised affordance methods, but they can also anticipate object interaction for novel object categories. Project page: Http://vision.cs.utexas.edu/projects/interaction-hotspots/.},
archivePrefix = {arXiv},
arxivId = {1812.04558},
author = {Nagarajan, Tushar and Feichtenhofer, Christoph and Grauman, Kristen},
journal = ICCV,
title = {{Grounded human-object interaction hotspots from video}},
year = {2019}
}

@article{ego-topo,
abstract = {First-person video naturally brings the use of a physical environment to the forefront, since it shows the camera wearer interacting fluidly in a space based on his intentions. However, current methods largely separate the observed actions from the persistent space itself. We introduce a model for environment affordances that is learned directly from egocentric video. The main idea is to gain a human-centric model of a physical space (such as a kitchen) that captures (1) the primary spatial zones of interaction and (2) the likely activities they support. Our approach decomposes a space into a topological map derived from first-person activity, organizing an ego-video into a series of visits to the different zones. Further, we show how to link zones across multiple related environments (e.g., from videos of multiple kitchens) to obtain a consolidated representation of environment functionality. On EPIC-Kitchens and EGTEA+, we demonstrate our approach for learning scene affordances and anticipating future actions in long-form video.},
archivePrefix = {arXiv},
arxivId = {2001.04583},
author = {Nagarajan, Tushar and Li, Yanghao and Feichtenhofer, Christoph and Grauman, Kristen},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagarajan et al. - 2020 - Ego-topo Environment affordances from egocentric video.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.},
pages = {160--169},
title = {{Ego-topo: Environment affordances from egocentric video}},
year = {2020}
}
@article{sufficient?,
archivePrefix = {arXiv},
arxivId = {arXiv:2107.02095v1},
author = {Caselles-dupr{\'{e}}, Hugo and Garcia-ortiz, Michael and Filliat, David and Caselles-dupr{\'{e}}, Hugo},
eprint = {arXiv:2107.02095v1},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Caselles-dupr{\'{e}} et al. - 2021 - Are standard Object Segmentation models sufficient for Learning Affordance Segmentation.pdf:pdf},
number = {CoRL},
pages = {1--10},
title = {{Are standard Object Segmentation models sufficient for Learning Affordance Segmentation ?}},
year = {2021}
}

@article{ag_from_demo_video,
  title={Learning Visual Affordance Grounding from Demonstration Videos},
  author={Luo, Hongchen and Zhai, Wei and Zhang, Jing and Cao, Yang and Tao, Dacheng},
  journal={arXiv preprint arXiv:2108.05675},
  year={2021}
}
@article{phrase_aff_detect,
  title={Phrase-Based Affordance Detection via Cyclic Bilateral Interaction},
  author={Lu, Liangsheng and Zhai, Wei and Luo, Hongchen and Kang, Yu and Cao, Yang},
  journal={arXiv preprint arXiv:2202.12076},
  year={2022}
}
@article{Hassanin2021,
abstract = {Nowadays, robots are dominating the manufacturing, entertainment, and healthcare industries. Robot vision aims to equip robots with the capabilities to discover information, understand it, and interact with the environment, which require an agent to effectively understand object affordances and functions in complex visual domains. In this literature survey, first, "visual affordances"are focused on and current state-of-The-Art approaches for solving relevant problems as well as open problems and research gaps are summarized. Then, sub-problems, such as affordance detection, categorization, segmentation, and high-level affordance reasoning, are specifically discussed. Furthermore, functional scene understanding and its prevalent descriptors used in the literature are covered. This survey also provides the necessary background to the problem, sheds light on its significance, and highlights the existing challenges for affordance and functionality learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.06775v1},
author = {Hassanin, Mohammed and Khan, Salman and Tahtali, Murat},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassanin, Khan, Tahtali - 2021 - Visual Affordance and Function Understanding.pdf:pdf},
issn = {15577341},
journal = {ACM Comput. Surv.},
keywords = {Affordance prediction,deep learning,functional scene understanding,visual reasoning},
number = {3},
pages = {1--26},
title = {{Visual Affordance and Function Understanding}},
volume = {54},
year = {2021}
}
@article{Mo2021,
abstract = {One of the fundamental goals of visual perception is to allow agents to meaningfully interact with their environment. In this paper, we take a step towards that long-term goal -- we extract highly localized actionable information related to elementary actions such as pushing or pulling for articulated objects with movable parts. For example, given a drawer, our network predicts that applying a pulling force on the handle opens the drawer. We propose, discuss, and evaluate novel network architectures that given image and depth data, predict the set of actions possible at each pixel, and the regions over articulated parts that are likely to move under the force. We propose a learning-from-interaction framework with an online data sampling strategy that allows us to train the network in simulation (SAPIEN) and generalizes across categories. But more importantly, our learned models even transfer to real-world data. Check the project website for the code and data release.},
archivePrefix = {arXiv},
arxivId = {2101.02692},
author = {Mo, Kaichun and Guibas, Leonidas and Mukadam, Mustafa and Gupta, Abhinav and Tulsiani, Shubham},
eprint = {2101.02692},
file = {:C$\backslash$:/Users/LI GEN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mo et al. - 2021 - Where2Act From Pixels to Actions for Articulated 3D Objects.pdf:pdf},
title = {{Where2Act: From Pixels to Actions for Articulated 3D Objects}},
year = {2021}
}

@inproceedings{learn2act_properly,
  title={Learning to act properly: Predicting and explaining affordances from images},
  author={Chuang, Ching-Yao and Li, Jiaman and Torralba, Antonio and Fidler, Sanja},
  booktitle=CVPR,
  year={2018}
}