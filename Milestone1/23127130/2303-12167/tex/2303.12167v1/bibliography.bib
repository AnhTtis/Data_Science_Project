# ------- A ------- #

# ------- B ------- #
@article{bartolozzi07,
  author  = {Bartolozzi, Chiara and Indiveri, Giacomo},
  journal = {Neural Computation},
  title   = {Synaptic Dynamics in Analog {VLSI}},
  year    = {2007},
  volume  = {19},
  number  = {10},
  pages   = {2581-2603},
  doi     = {10.1162/neco.2007.19.10.2581}
}

@article{buechel21,
  title   = {Supervised training of spiking neural networks for robust deployment on mixed-signal neuromorphic processors},
  author  = {Julian B{\"u}chel and Dmitrii Zendrikov and Sergio Solinas and G. Indiveri and Dylan Richard Muir},
  journal = {Scientific Reports},
  year    = {2021},
  volume  = {11},
  doi     = {10.1038/s41598-021-02779-x},
}

# ------- C ------- #

@mastersthesis{cakal22,
  title  = {{DynapSIM}: A Fast, Optimizable, and Mismatch Aware Mixed-Signal Neuromorphic Chip Simulator},
  school = {Middle East Technical University},
  author = {Çakal, Uğurcan},
  year   = {2022},
  url    = {https://hdl.handle.net/11511/98616}
}

@article{chicca14,
  author  = {Chicca, Elisabetta and Stefanini, Fabio and Bartolozzi, Chiara and Indiveri, Giacomo},
  journal = {Proceedings of the {IEEE}},
  title   = {Neuromorphic Electronic Circuits for Building Autonomous Cognitive Systems},
  year    = {2014},
  volume  = {102},
  number  = {9},
  pages   = {1367-1388},
  doi     = {10.1109/JPROC.2014.2313954}
}

# ------- D ------- #

# ------- E ------- #

# ------- F ------- #

# ------- G ------- #

# ------- H ------- #

# ------- I ------- #

# ------- J ------- #
@misc{jax2018github,
  author  = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title   = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url     = {http://github.com/google/jax},
  version = {0.2.5},
  year    = {2018}
}

# ------- K ------- #
@ARTICLE{Kaiser2020,
  
AUTHOR={Kaiser, Jacques and Mostafa, Hesham and Neftci, Emre},   
	 
TITLE={Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={14},           
	
YEAR={2020},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnins.2020.00424},       
	
DOI={10.3389/fnins.2020.00424},      
	
ISSN={1662-453X},   
   
ABSTRACT={A growing body of work underlines striking similarities between biological neural networks and recurrent, binary neural networks. A relatively smaller body of work, however, addresses the similarities between learning dynamics employed in deep artificial neural networks and synaptic plasticity in spiking neural networks. The challenge preventing this is largely caused by the discrepancy between the dynamical properties of synaptic plasticity and the requirements for gradient backpropagation. Learning algorithms that approximate gradient backpropagation using local error functions can overcome this challenge. Here, we introduce Deep Continuous Local Learning (DECOLLE), a spiking neural network equipped with local error functions for online learning with no memory overhead for computing gradients. DECOLLE is capable of learning deep spatio temporal representations from spikes relying solely on local information, making it compatible with neurobiology and neuromorphic hardware. Synaptic plasticity rules are derived systematically from user-defined cost functions and neural dynamics by leveraging existing autodifferentiation methods of machine learning frameworks. We benchmark our approach on the event-based neuromorphic dataset N-MNIST and DvsGesture, on which DECOLLE performs comparably to the state-of-the-art. DECOLLE networks provide continuously learning machines that are relevant to biology and supportive of event-based, low-power computer vision architectures matching the accuracies of conventional computers on tasks where temporal precision and speed are essential.}
}

@inproceedings{kingma15,
  author    = {Diederik P. Kingma and Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980}
}

# ------- L ------- #

@ARTICLE{Lee2016,
  
AUTHOR={Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},   
	 
TITLE={Training Deep Spiking Neural Networks Using Backpropagation},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={10},           
	
YEAR={2016},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fnins.2016.00508},       
	
DOI={10.3389/fnins.2016.00508},      
	
ISSN={1662-453X},   
   
ABSTRACT={Deep spiking neural networks (SNNs) hold the potential for improving the latency and energy efficiency of deep neural networks through data-driven event-based computation. However, training such networks is difficult due to the non-differentiable nature of spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are considered as noise. This enables an error backpropagation mechanism for deep SNNs that follows the same principles as in conventional deep networks, but works directly on spike signals and membrane potentials. Compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statistics of spikes more precisely. We evaluate the proposed framework on artificially generated events from the original MNIST handwritten digit benchmark, and also on the N-MNIST benchmark recorded with an event-based dynamic vision sensor, in which the proposed method reduces the error rate by a factor of more than three compared to the best previous SNN, and also achieves a higher accuracy than a conventional convolutional neural network (CNN) trained and tested on the same data. We demonstrate in the context of the MNIST task that thanks to their event-driven operation, deep SNNs (both fully connected and convolutional) trained with our method achieve accuracy equivalent with conventional neural networks. In the N-MNIST example, equivalent accuracy is achieved with about five times fewer computational operations.}
}

@inproceedings{livi09,
  author    = {Livi, Paolo and Indiveri, Giacomo},
  booktitle = {2009 {IEEE} International Symposium on Circuits and Systems},
  title     = {A current-mode conductance-based silicon neuron for address-event neuromorphic systems},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {2898-2901},
  doi       = {10.1109/ISCAS.2009.5118408}
}

# ------- M ------- #
@article{moradi18,
  author  = {Moradi, Saber and Qiao, Ning and Stefanini, Fabio and Indiveri, Giacomo},
  journal = {{IEEE} Transactions on Biomedical Circuits and Systems},
  title   = {A Scalable Multicore Architecture With Heterogeneous Memory Structures for Dynamic Neuromorphic Asynchronous Processors ({DYNAPs})},
  year    = {2018},
  volume  = {12},
  number  = {1},
  pages   = {106-122},
  doi     = {10.1109/TBCAS.2017.2759700}
}


@misc{muir19,
  author    = {Muir, Dylan R. and
               Bauer, Felix and
               Weidel, Philipp},
  title     = {Rockpool Documentaton},
  month     = sep,
  year      = 2019,
  publisher = {Zenodo},
  url       = {https://doi.org/10.5281/zenodo.3773845}
}

# ------- N ------- #
@ARTICLE{Neftci2019,
  author={Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  journal={IEEE Signal Processing Magazine}, 
  title={Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks}, 
  year={2019},
  volume={36},
  number={6},
  pages={51-63},
  doi={10.1109/MSP.2019.2931595}}


# ------- O ------- #
# ------- P ------- #
# ------- Q ------- #
# ------- R ------- #
# ------- S ------- #
# ------- T ------- #
# ------- U ------- #
# ------- V ------- #
# ------- W ------- #
# ------- X ------- #
# ------- Y ------- #
# ------- Z ------- #

@article {Zendrikov2022,
	author = {Zendrikov, Dmitrii and Solinas, Sergio and Indiveri, Giacomo},
	title = {Brain-inspired methods for achieving robust computation in heterogeneous mixed-signal neuromorphic processing systems},
	elocation-id = {2022.10.26.513846},
	year = {2022},
	doi = {10.1101/2022.10.26.513846},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Neuromorphic processing systems implementing spiking neural networks with mixed signal analog/digital electronic circuits and/or memristive devices represent a promising technology for edge computing applications that require low power, low latency, and that cannot connect to the cloud for off-line processing, either due to lack of connectivity or for privacy concerns. However these circuits are typically noisy and imprecise, because they are affected by device to device variability, and operate with extremely small currents. So achieving reliable computation and high accuracy following this approach is still an open challenge that has hampered progress on one hand and limited widespread adoption of this technology on the other. By construction, these hardware processing systems have many constraints that are biologically plausible, such as heterogeneity and non-negativity of parameters. More and more evidence is showing that applying such constraints to artificial neural networks, including those used in artificial intelligence, promotes robustness in learning and improves their reliability. Here we delve even more in neuroscience and present network-level brain-inspired strategies that further improve reliability and robustness in these neuromorphic systems: we quantify, with chip measurements, to what extent population averaging is effective in reducing variability in neural responses, we demonstrate experimentally how the neural coding strategies of cortical models allow silicon neurons to produce reliable signal representations, and show how to robustly implement essential computational primitives, such as selective amplification, signal restoration, working memory, and relational networks, exploiting such strategies. We argue that these strategies can be instrumental for guiding the design of robust and reliable ultra-low power electronic neural processing systems implemented using noisy and imprecise computing substrates such as subthreshold neuromorphic circuits and emerging memory technologies.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/10/27/2022.10.26.513846},
	eprint = {https://www.biorxiv.org/content/early/2022/10/27/2022.10.26.513846.full.pdf},
	journal = {bioRxiv}
}

@article{Zenke2018,
    author = {Zenke, Friedemann and Ganguli, Surya},
    title = "{SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks}",
    journal = {Neural Computation},
    volume = {30},
    number = {6},
    pages = {1514-1541},
    year = {2018},
    month = {06},
    abstract = "{A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in silico. Here we revisit the problem of supervised learning in temporally coding multilayer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three-factor learning rule capable of training multilayer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric, and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike time patterns.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01086},
    url = {https://doi.org/10.1162/neco\_a\_01086},
    eprint = {https://direct.mit.edu/neco/article-pdf/30/6/1514/1039264/neco\_a\_01086.pdf},
}



