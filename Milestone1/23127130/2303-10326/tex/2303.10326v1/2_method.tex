\section{Method}

\begin{figure}[!t]
\includegraphics[width=0.9\textwidth]{Figures/diffusion_framework_15.pdf}
%% 训练阶段为一步去噪的过程，测试阶段通过迭代的方式预测分割标签，并通过我们设计的S-U Fusion模块提升分割结果的鲁棒性。
\caption{The overview of proposed Diff-UNet. (A) is the training phase of Diff-UNet to learn a denoising function by the Denoising Module. (B) is the testing phase to generate segmentation results by an iterative way. (C) is the computation process of our SUF module. 
% We take BraTS2020 dataset as an example. The image modality number of volume data is 4, and we concatenate them in the channel dimension as $I$. The number of label is 3, and we encode them by one-hot encoding as $x_0$.
} \label{fig1}
\end{figure}

%% Fig1 展示了我们提出的Diff-UNet的训练，测试过程和两个子模块：DM和SU。
Fig.~\ref{fig1} shows the training stage, the testing stage, and two sub-modules of our proposed Diff-UNet: Denoising module and Step-Uncertainty based fusion module (SUF).
%% 与常规的医学图像分割方法直接输入体积的医学图像来预测对应分割标签不同，扩散模型学习去噪过程-将添加了连续t时间步噪声的分割标签与体积图像共同作为输入，学习消除分割标签中的噪声从而生成清晰的分割结果。
Unlike conventional medical image segmentation methods that directly input volume data to predict the corresponding segmentation labelmap, the diffusion model learns the denoising process. The diffusion model takes the volumetric image and the segmentation labelmap with noise as input and learns to remove the noise to generate clear segmentation results.

\subsection{Label Embedding}
%% One-hot 编码通常用于转换多分类标签为多个二分类标签。
One-hot encoding is usually used to convert multi-categorical tags into multiple bicategorical tags.
A one-hot vector $v$ is a binary vector of length $c$ where only a single entry can be one, all others must be zero. For example, assuming that there are 3 segmentation targets, we convert the segmentation labels $(0,1,2)$ by one-hot encoding to $((0,0,1),(0,1,0),(1,0,0))$.
% 与常规的医学图像分割算法不同，扩散模型是一类生成模型，所以它无法直接的预测labelmap。 因此，我们首先将单通道标签通过onehot编码转换为多通道1-bit标签。
The traditional Diffusion model only generates continuous data, which cannot predict the multi-target labels. Therefore, we first convert single-channel labelmap with size $D \times W \times H$ to multi-channel labels: ${x}_0 \in \mathbb{R}^{N \times D \times W \times H}$ by the one-hot encoding, where $N$ is the number of label, $(D, W, H)$ is the spatial resolutions of volumetric medical image.
%% 之后，我们对转换后的多通道分割labelmap添加连续的t步噪声
Then, we add successive $t$ step noise $\epsilon$ for the converted multi-channel labels, called the diffusion forward process.
\begin{equation}
    \label{Eq:x_t}
    \mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon} \ .
\end{equation}
After getting the labelmap $x_t$ with $t$ step noise, our target is to predict the clear labelmap $x_0$ based on $x_t$ and the raw volume data by the Denoising Module.
\subsection{Denoising Module}
%% DU(Denoising-UNet) 分为编码器与解码器两个部分。首先，原始图像I与有噪声的one-hot标签xt are concatenated channel-wise 输入到DU网络的编码器中，得到多尺度特征I_f1。同时，为了更好的引入原始图像特征，我们通过一个与DU网络的编码器相同尺寸的特征编码器提取原始图像的多尺度特征I_f2。由于I_f2与I_f1中包含的特征数量与大小均相同,因此我们将对应尺度的特征进行相加，得到融合特征。
As shown in Fig.~\ref{fig1} (A), the Denoising Module consisting of a Feature Encoder (FE) and a Denoising-UNet (DU) is a main part of Diff-UNet. 
The Denoising-UNet also contains two parts, an encoder, and a decoder. First, given the volume data $I \in \mathbb{R}^{M \times D \times W \times H}$, where $M$ is the number of modal images, $I$ and the noisy one-hot label $x_t$ are concatenated channel-wise into DU's encoder to obtain the multi-scale feature $\hat{I}_f:[\mathbb{R}^{if \times \frac{D}{i} \times \frac{W}{i} \times \frac{H}{i}}]_{i=1}^{16}$, where $f$ is the feature size and $i$ is the scale. Meanwhile, to better introduce the raw volumetric image features, we extract the multi-scale features $\tilde{I}_f$ of the volume data through a feature encoder which has the same size with the DU's encoder. Since $\tilde{I}_f$ and $\hat{I}_f$ contain the same number and size of features, we sum the features of the corresponding scales to obtain the fused features.
%% 之后，融合的多尺度特征输入至DU网络的解码器中，得到预测结果\hat{x_0}。
After that, we input the fused multi-scale features to the decoder of DU network to obtain the prediction result $\hat{x_0} \in \mathbb{R}^{N \times D \times W \times H}$:
\begin{equation}
    \hat{x_0} = \mathrm{DU}(\mathrm{cat}(I, x_t), t, \tilde{I}_f).
\end{equation}
%% 传统的diffusion被训练使用l2去噪损失。在该任务中，我们将医学图像分割任务建模为离散数据生成问题，而且直接预测x0而不是噪声epsilon，因此我们使用Dice Loss，BCE Loss，MSE Loss共同监督diffusion的训练过程。
Classical diffusion is trained using $\mathcal{L}_2$ denoising loss. In this task, we model the medical image segmentation task as a discrete data generation problem and directly predict $x_0$ instead of noisy $\epsilon$. Diff-UNet is trained by combing Dice Loss, BCE Loss, and MSE Loss, and thus the total loss $\mathcal{L}_{total}$ of our Diff-UNet is: %is computed by:
\begin{equation}
    \mathcal{L}_{total} = \mathcal{L}_{dice}(\hat{x_0}, x_0) + \mathcal{L}_{bce}(\hat{x_0}, x_0) + \mathcal{L}_{mse}(\hat{x_0}, x_0).
\end{equation}

\subsection{Step-Uncertainty based Fusion}

%% Diffusion模型在预测阶段会迭代N次，在常规的生成任务中，将最后一次的预测作为最终生成结果。而在医学图像分割任务中，每次迭代均会产生一次分割map，且随着预测时间步的增大，预测结果越准确，预测不确定性越低。因此，为了更好的提升diffusion模型的分割鲁棒性，我们基于采样步数与不确定性进行输出融合。
The diffusion model iterates $t$ times in the testing phase by the Denoising Diffusion Implicit Models (DDIM) method. In a conventional generation task, the last prediction is taken as the final generation result, while each iteration of Diff-UNet generates a segmentation map. As the prediction time step increases, the more accurate the prediction result is and the lower the prediction uncertainty is. Therefore, to improve the segmentation robustness of Diff-UNet model, we fuse the output based on the number of prediction steps and uncertainty.

%% Monte Carlo Dropout 首先激活网络中dropout层，然后通过使用S次随机的前向过程估计不确定性地图。而Diffusion在预测时会初始化一个随机噪声xt，因此不需要激活dropout层便能够为网络引入随机性。类似于Monte Carlo Dropout，Diffusion的预测过程包含t步，并且每一步都会预测S次，用来计算不确定性。计算公式如下.
The way we compute the uncertainty is similar to the Monte Carlo Dropout (MC Dropout)~\cite{gal2016dropout}, which activates the network's dropout layer, then performs $S$ forward passes to estimate the uncertainty map. On the other hand, Diff-UNet initializes a random noise $x_t$ in the testing phase (Fig.~\ref{fig1} (B)), so it can introduce randomness into the network without activating the dropout layer. Like Monte Carlo dropout, Diffusion's testing process consists of $t$ steps, and each step predicts $S$ outputs, which are used to calculate the uncertainty. The formula is as:
\begin{equation}
\label{Eq:u_i}
    u_i = -\bar{p}_i log(\bar{p}_i) ,~ \text{where} ~\bar{p}_{i} = \frac{1}{S}\sum_{s=1}^{S}{p_i^s}.
\end{equation}
% 结合采样步数与不确定性的权重计算方式如下。
The fusion weights combining the number of prediction steps and uncertainty are calculated as $w_{i} = e^{\sigma(\frac{i}{scale}) \times (1 - u_{i})}$, where $\sigma$ is sigmoid funciton, $i$ denotes the current prediction step and $u$ is the uncertainty matrix.
%% 我们融合权重对每一步的预测结果进行加权，得到最终的融合结果，将其作为模型的输出。
We use $w$ to weight the prediction results of each step to obtain the final fusion result $Y$, which is used as the output of our network. Finally, $Y$ is given by: $Y = \sum_{i=1}^{t}{w_i \times \bar{p}_i}$.

% \begin{figure}[h]
% \centering
% \includegraphics[width=\textwidth]{Figures/code.pdf}
% % \caption{Multi-modal MRIs for (a) Gliomas; and (b) Meningiomas.} 
% \label{fig:code}
% \end{figure}

% \begin{figure*}[ht!]
%     \centering
%       \begin{subfigure}{0.48\textwidth}
%         \centering
%         \begin{minted}[frame=lines,framesep=2mm]{python}
% def train_loss(images, labels):
%     # images: (b, c, d, w, h)
%     # labels: (b, d, w, h)

%     # x:(b, n, d, w, h)
%     x = convert_one_bit(labels)

%     # Encode image features.
%     h = image_encoder(images)

%     # Select t from 1 to T random.
%     t = uniform(1, T)

%     eps = normal(mean=0, std=1)

%     x_t = sqrt(gamma(t)) * x + 
%             sqrt(1 - gamma(t)) * eps
%     # Predict and compute loss.
%     pred = denoised_unet(x_t, t, h,
%                     condition=images)
%     loss = mse(pred, x) + 
%             dice(pred, x) + 
%                 bce(pred, x)
%     return loss.mean()
%         \end{minted}
%           \caption{caption\_for\_sub1}
%           \label{fig:sub1}
%       \end{subfigure}      
%     %   \hfill
%       \begin{subfigure}{0.48\textwidth}
%         \centering   
%         % \lstinputlisting[language=Python,linewidth={1\linewidth}]{Figures/patch.list}
%         \begin{minted}[frame=lines,framesep=2mm]{python}
% def infer(images, steps=10):
%     # images: (b, c, d, w, h)

%     # Encode image features.
%     h = image_encoder(images)

%     # Initial state
%     m_t = normal(mean=0, std=1)

%     pred_list = []
    
%     # Iterate to get all the results.
%     for step in range(steps):
%         # Get model prediction.
%         pred = denoised_unet(m_t, t, h,
%                          condition=images)
%         # Get the input of next step. 
%         m_t = ddim_step(m_t, pred, step)
%         pred_list.append(pred)

%     # Fuse all prediction by uncertainty.
%     fused_pred = fuse(pred_list)

%     return fused_pred
%         \end{minted}
%           \caption{caption\_for\_sub2}
%           \label{fig:sub2}
%       \end{subfigure}
%   \caption{
%   \label{fig:total}
%   write\_caption\_here
%   }
% \end{figure*}





