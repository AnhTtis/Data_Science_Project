
\section{Introduction}

%% 3D医学图像分割是医学图像分析中常见的任务，该任务从高维度医学图像数据集（e.g., CT, MRI）中以像素为单位标记出病灶区域。更准确的分割结果可以为医生提供更加丰富的信息，更好的辅助医生诊断疾病。
Medical volumetric segmentation is a critical task for medical image analysis~\cite{litjens2017survey,khan2014survey,shamshad2201transformers}, involving the identification of lesion areas in high-dimensional medical image datasets on a pixel-by-pixel basis. More accurate segmentation results can provide valuable information to doctors, assisting them in diagnosing diseases.
Conventional 3D medical segmentation algorithms typically employ an encoder-decoder structure~\cite{ronneberger2015u,7785132,3d_unet} and incorporate skip-connections to enable the decoder to reuse features extracted by the encoder. Many current 3D medical image segmentation algorithms designed for model structures achieve promising segmentation results. For instance, SegResNet~\cite{myronenko20183d} uses variational Auto-Encoder~\cite{kingma2013auto} to add reconstruction branches, improving the feature extraction capability of the model. However, since its structure is based on a convolutional neural network, it may not be able to extract global features effectively.

Recenlty, the Transformer structure has gained popularity in modeling global features due to its global self-attention mechanism~\cite{vaswani2017attention,Shamshad2022}. TransBTS~\cite{wang2021transbts} leverages 3D-CNN to extract local spatial features and then applies the transformer to model global dependencies in high-level features. UNETR~\cite{hatamizadeh2022unetr} utilizes ViT~\cite{dosovitskiy2020image} as an encoder to model global features directly and outputs segmentation results using a CNN-based decoder with skip connections. However, the above methods are limited in their ability to extract multi-scale features due to the computational complexity of the Transformer structure. 
% The Swin-Transformer~\cite{liu2021swin} structure converts global attention to local attention by utilizing window attention through the shifted-window mechanism and achieves inter-window information interaction through window panning. 
SwinUNETR~\cite{hatamizadeh2022swin} leverages Swin-Transformer~\cite{liu2021swin} as an encoder to extract multi-scale features and employs a CNN-based decoder to generate the output, achieving state-of-the-art medical image segmentation results.

%% Denoising diffusion models 最近在多种生成任务中已经展示了相当大的成功。同时Diffusion也可以被应用于医学图像分割任务，并实现精确的分割性能。例如MedSegDiff通过分割UNet与去噪UNet，并通过傅立叶变换进行结构间信息交互，实现2D医学图像分割任务。J Wolleb 对多步采样结果添加了模型集成方式，使得分割结果更加鲁棒。然而，目前基于Diffusion的医学图像分割方法均局限于2D分割，无法直接生成多目标分割标签。同时DDPM使得生成分割结果时需要采样1000步，因此预测过程十分耗时。

Denoising diffusion models~\cite{ho2020denoising,song2020denoising,nichol2021improved} have shown significant success in various generative tasks, including medical image segmentation. For instance, MedSegDiff~\cite{wu2022medsegdiff} achieves 2D medical image segmentation by segmenting Denoising-UNet, and interacting with inter-structural information through Fourier transform. Wolleb et al.~\cite{wolleb2022diffusion} employ the diffusion model to solve the 2D medical image segmentation problem and improve the robustness of the segmentation results by fusing the output results of each diffusion step using a summation manner during testing. However, these methods are limited to 2D segmentation, and the diffusion model cannot generate multi-label segmentation directly.


% Wolleb 利用diffusion解决2D医学图像分割问题，并在测试阶段将diffusion每一步的输出结果以叠加的方式进行融合，提升了分割结果的鲁棒性。但是上述方法均局限于2D分割，且使用的diffusion模型无法直接生成多目标分割标签。

%% 由于传统的模型只能解决二类分割问题，我们设计了一个标签转化层，使得A可以分割多个目标。
Compared to traditional segmentation methods, the Diffusion model introduces noise at the input and iteratively predicts the segmentation label map, both of which can improve the robustness of the Diffusion model's prediction. To exploit the Diffusion model's potential, we propose a generic Diffusion-based end-to-end 3D medical image segmentation algorithm, called \textbf{Diff-UNet}, to solve the high-dimensional medical image segmentation problem.
However, the conventional diffusion model can only solve the binary segmentation problem. To segment multiple class, we design a Label Embedding operation that converts the segmentation label map into one-hot labels. This enables Diff-UNet to segment multiple targets simultaneously.
To extract semantic information from the input volume, we design a Denoising module that contains a Denoising-UNet and an independent Feature Encoder to learn the denoising process. This module outputs a clear segmentation label map from a noisy label map.
Finally, we design a Step-Uncertainty based Fusion (SUF) module that fuses multiple predictions from the Denoising module to obtain more robust segmentation results during the testing phase.
Extensive experiments on the BraTS2020 multimodal brain tumor segmentation dataset~\cite{menze2014multimodal,bakas2018identifying}, BTCV multi-organ segmentation dataset~\cite{BTCV}, and MSD Liver and Liver tumor segmentation dataset~\cite{antonelli2022medical} demonstrate that our method significantly outperforms state-of-the-art approaches.
\footnote{We will release our code after acceptance.}
%% 我们提出了一个基于Diffusion的通用的端到端3D医学图像分割算法，解决高维度医学图像分割问题。我们首先将多目标标签转换为多个1-bit标签，从而将连续生成任务转化为离散生成任务，使得Diffusion模型能够分割多个目标。
% Compared with the traditional segmentation methods, Diffusion model introduces the noise at the input, and predicts the segmentation labelmap by an iterative way, both of which can improve the robustness of diffusion model's prediction. To exploit the diffusion model's advancement, we propose a generic Diffusion-based end-to-end 3D medical image segmentation algorithm, called \textbf{Diff-UNet}, to solve the high-dimensional medical image segmentation problem. 
% The conventional diffusion model only can solve the two-class (\textit{e.g.}, background and another class) segmentation problem. To make the Diff-UNet segment multiple targets, we design a Label Embedding operation to convert the segmentation labelmap into one-hot labels.
% Then, to better extract the semantic information from the input volume, we design a Denoising module that contains a Denoising-UNet and an independent Feature Encoder to learn the denoising process which generates the clear segmentation labelmap from the noisy labelmap. 
% Conventional diffusion model only can solve the two-class (\textit{e.g.}, background and another class) segmentation problem. To make the Diff-UNet segment multiple targets, we design a Label Embedding operation to convert the segmentation labelmap into one-hot labels.
% % First, we convert the multi-target labels into one-hot labels by one-hot encoding~\cite{harris2015digital}, enabling the Diffusion model to segment multiple targets.
% %% 我们使用一个去噪UNet网络作为核心，学习将添加了t步噪声的one-hot标签转换为清晰的分割标签。同时，为了更好的增强去噪UNet网络的特征表达能力，我们还加入了一个独立的特征编码器提取原始图像的多尺度特征，并与去噪UNet网络进行融合。
% We design a Denoising module which contains a Denoising-UNet and a independent Feature Encoder as the core to learn the denoising process to generate the clear segmentation labelmap. 
%
%% 在测试阶段，扩散模型通过迭代产生多个预测结果。因此，我们基于迭代的步数与预测结果的不确定性，设计了一个S-U Fusion模块融合多个预测结果，得到更加鲁棒的分割结果。
% In the testing phase, the diffusion model produces multiple prediction results by iteration. Therefore, we design an S-U Fusion module to fuse multiple predictions to obtain more robust segmentation results based on the number of steps of iterations and the uncertainty of prediction results.
% In the testing phase, we design an Step and Uncertainty based Fusion (SUF) module to fuse multiple predictions from the Denoising module to obtain more robust segmentation results.

%% 广泛的实验在BraTS2020多模态脑肿瘤分割和BTCV多器官分割数据集展示了我们的显著超越了state-of-the-art 方法。



% \begin{figure}[h]
% \centering
% \includegraphics[width=\textwidth]{images/intro_pic.pdf}
% % \vspace{5mm}
% \caption{Multi-modal MRIs for (a) Gliomas; and (b) Meningiomas.} 
% \label{fig:MMdata}
% \end{figure}


% \begin{figure}[t]
% \centering
% \includegraphics[width=\textwidth]{images/framework_16.pdf}
% \caption{An overview of the proposed NestedFormer. We design a Nested Modality-aware Feature Aggregation (NMaFA) module to model both the intra- and inter-modality features for multi-modal fusion.} 
% \label{fig:framework}
% \vspace{-1mm}
% \end{figure}

