% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produc\textit{}e the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbding}

\usepackage[accsupp]{axessibility}

\input{macros}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{makecell}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5008} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\LaTeX\ Author Guidelines for \confName~Proceedings}
\title{Grid-guided Neural Radiance Fields for Large Urban Scenes}

\author{Linning Xu$^{\star 1}$,
	Yuanbo Xiangli$^{\star 1}$,
	Sida Peng$^4$,
	Xingang Pan$^3$, \\
	Nanxuan Zhao$^5$, 
	Christian Theobalt$^3$, 
	Bo Dai$^2$\Envelope, 
	Dahua Lin$^{1,2}$\\
	$^1$The Chinese University of Hong Kong,
	$^2$Shanghai AI Laboratory, \\
	$^3$Max Planck Institute for Informatics, 
	$^4$Zhejiang University,
	$^5$Adobe Research \\
	\tt\small
	\{xl020,xy019,dhlin\}@ie.cuhk.edu.hk,
	\{xpan,theobalt\}@mpi-inf.mpg.de,\\
	\tt\small
	pengsida@zju.edu.cn, nanxuanzhao@gmail.com, daibo@pjlab.org.cn \\
	\\
}

\twocolumn[{%
	\renewcommand
	\twocolumn[1][]{#1}%
	\maketitle
	\begin{center}
		\centering
		\vspace{-30pt}
		\includegraphics[width=\textwidth]{figs/teaser.pdf}
		\vspace{-15pt}
		\captionof{figure}{ \small We perform large urban scene rendering with a novel grid-guided neural radiance fields. 
		An example of our target large urban scene is shown on the left,
		which spans over 2.7$km^2$ ground areas captured by over $5k$ drone images. 
		We show that the rendering results from NeRF-based methods~\cite{mildenhall2020nerf,turki2021mega} 
		are blurry and overly smoothed with limited model capacity, while feature grid-based methods~\cite{chen2022tensorf,mueller2022instant} 
		tend to display noisy artifacts when adapting to large-scale scenes with high-resolution feature grids. 
		Our proposed two-branch model combines the merits from both approaches and achieves photorealistic novel view renderings with remarkable improvements over existing methods. Both two branches gain significant enhancements over their individual baselines. (Project page: \href{https://city-super.github.io/gridnerf/}{ https://city-super.github.io/gridnerf})}
		\label{fig:teaser}
	\end{center}
}]


\input{sections/abstract.tex}
\input{sections/introduction.tex}
\input{sections/related.tex}
\input{sections/method.tex}
\input{sections/experiment.tex}
\input{sections/conclusion.tex}


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
