% !TEX root = ../main.tex
\section{Background}
\label{sec:background}
We first discuss the background on pure MLP-based representation (NeRF) and grid-based approaches for neural based scene representations, which forms two mainstreams of state-of-the-art solutions. 
\paragraph{Neural Radiance Field with MLP} 
%NeRF \cite{mildenhall2020nerf} parameterizes the volumetric density and color as a function of input coordinates, using the weights of a multilayer perceptron (MLP).
%%
%A ray $\mathbf{r}(t)$ is emitted from the camera's center of projection and passes through each pixel of the image. 
%Stratified sampling is proposed to determine a vector of sorted distances $\{t_k\}$ on the ray between the camera's predefined near and far planes. 
%For each sampled point $\mathbf{r}(t_{k})$ on the ray, the MLP takes in its Fourier transformed features,~\ie position encoding (PE), and outputs the color and density with:
%\begin{equation}
%	%\forall t_{k} \in \mathbf{t}, \quad
%	\left(\tau_{k}, \mathbf{c}_{k}\right)
%	=\operatorname{MLP}\left(\gamma\left(\mathbf{r}\left(t_{k}\right)\right)\right).
%\end{equation}
%PE here is implemented by the concatenation of a set of sine and cosine mappings of the 3D position $\mathbf{x}$ (and optionally viewing direction) up to a pre-defined frequency degree $M$:
%\begin{equation}
%	\resizebox{.95\linewidth}{!}{$
%		\gamma(\mathbf{x})=\left[\sin (2^0\mathbf{x}), \cos (2^0\mathbf{x}), \ldots, \sin \left(2^{M-1} \mathbf{x}\right), \cos \left(2^{M-1} \mathbf{x}\right)\right]^{\mathrm{T}},
%		$}
%	\label{eq:position}
%\end{equation}
%where $\mathbf{x}$ is normalized to lie in $[-\pi,\pi]$. The network is then optimized abiding by the classical volume rendering, where
%the estimated densities and colors for all the sampled points $\mathbf{r}(t_{k})$ are used to approximate the volume rendering integral using numerical quadrature \cite{max1995optical}:
%\begin{equation}
%	\begin{gathered}
%		\mathbf{C}(\mathbf{r} ; \mathbf{t})=\sum_{k} T_{k}\left(1-\exp \left(-\tau_{k}\left(t_{k+1}-t_{k}\right)\right)\right) \mathbf{c}_{k}, \\
%		\text { with } \quad T_{k}=\exp \left(-\sum_{k^{\prime}<k} \tau_{k^{\prime}}\left(t_{k^{\prime}+1}-t_{k^{\prime}}\right)\right),
%	\end{gathered}
%	\label{eq:integral}
%\end{equation}
%where $\mathbf{C}(\mathbf{r}; \mathbf{t})$
%%\footnote{We eliminate the notation for hierarchical sampling~\cite{mildenhall2020nerf} for breivity.} 
%is the final predicted color of the pixel. 
%The final loss is the total squared error between the rendered and ground truth colors:
%% for both the coarse and fine renderings:
%%\amber{unify coarse and fine stage}
%\begin{equation}
%	\min _{\Theta} \sum_{\mathbf{r} \in \mathcal{R}}\left(\left\|\mathbf{C}^{*}(\mathbf{r})-\mathbf{C}\left(\mathbf{r} ; \mathbf{t}\right)\right\|_{2}^{2}\right),
%	%\min _{\Theta} \sum_{\mathbf{r} \in \mathcal{R}}\left(\left\|\mathbf{C}^{*}(\mathbf{r})-\mathbf{C}\left(\mathbf{r} ; \mathbf{t}^{c}\right)\right\|_{2}^{2}+\left\|\mathbf{C}^{*}(\mathbf{r})-\mathbf{C}\left(\mathbf{r} ; \mathbf{t}^{f}\right)\right\|_{2}^{2}\right),
%\end{equation}
%where $\mathbf{C}^{*}(\mathbf{r})$ is the ground truth color, and $\mathcal{R}$ is the collection of sampled rays within a batch. 
While NeRF is capable of capturing complex geometry and view-dependent appearance of small scenes by reasoning about coordinates and a band of Fourier frequencies. 
It has a difficult time to achieve comparable results on large scenes, always with severe artifacts and low visual fidelity.
The training of NeRF requires it to reason about coordinates spanning the entire space in a coarse-to-fine manner. To fit large urban scene which is flat (not object-centric) and inherently contains large variation in color and geometry, large amounts of samples will be wasted in void space, and fine details are easily skipped.
Despite larger NeRF models have the potential to represent larger scenes, it can be slow to train and infer. 
Targeting this issue, attempts have been made to apply NeRF on real-world large visual captures covering multiple buildings or city blocks~\cite{tancik2022block,turki2021mega} .
Instead of naively expanding the model, they propose to geographically divide the scene into sub-regions, and deploy one sub-NeRF on each sub-space and fuse rendering results among models at inference time. A direct observation is that, when the target scene expands, the systems becomes heavier with more spatial partitions and sub-NeRF models, leading to excessive consumption on computational resources. 
%there are still lots of wasted samples when training each NeRF. Furthermore, once the targeting region expands, more submodules need to be introduced, which is neither economic nor efficient. 

%there are still lots of wasted samples when training each NeRF. Furthermore, once the targeting region expands, more submodules need to be introduced, which is neither economic nor efficient.

\paragraph{Grid Based Methods}
Despite NeRF models require small memory with several layers of MLP, training and inference usually take a long time. 
To accelerate, multiple works~\cite{Mller2022InstantNG,Liu2020NeuralSV,Martel2021ACORNAC,Takikawa2021NeuralGL} construct an explicit voxel grid of features with a small MLP rendered to interpret features into volume density and color. 
The catch is, these methods require high resolution grid that matches with the finest detail in the images to faithfully reconstruct the scene. However, such requirement is impractical for large scene at city-scale.
On the other hand, using low to moderate grid resolution constantly resulting in blurry and fuzzy results due to the trilinear interpolation mechanism used to obtain continuous sample features.
%achieve good reconstruction quality, which works well for small to moderate scene but is impractical to fit large scenes with cubically growth volume~\misscite. 
%Previous works commonly use bilinear interpolation to obtain continuous sample features, 
The ability to express variations within a single voxel is restricted, which makes it unfriendly to capture thin geometry and texture in large scenes. 
To obtain a compact representations, method like tri-plane~\cite{Chan2021EfficientG3}, hash tables~\cite{muller2022instant}, tensor factorization~\cite{chen2022tensorf}, model distillation~\cite{Yu2021PlenOctreesFR} have been proposed in various applications.
While~\cite{muller2022instant,chen2022tensorf} have shown effectiveness in reducing the voxel number and speed up the fitting, those kinds of information reduction are still prohibitive for city scale scenes, and did not consider the unique characters of urban scenes which can help design better representations suitable for city-scale scene modeling.


%TensoRF~\cite{chen2022tensorf} represents radiance fields as an explicit voxel grid of features, and view a feature grid as a 3D tensor plus an extra feature dimension, which can be factorized into low-rank components to lower memory footprint. Specifically, they propose vector-matrix (VM) decomposition that factorizes the tensor as a sum of vector-matrix outer products:
%\begin{equation}
%	\mathcal{T} = \sum_{r=1}^{R} \mathbf{v}_r^{X} \circ \mathbf{M}_r^{Y,Z} + \mathbf{V}_r^{Y} \circ \mathbf{M}_r^{X,Z} + \mathbf{V}_r^{Z} \circ \mathbf{M}_r^{X,Y} 
%	\label{eqn:vmrrr}
%\end{equation} 
%where $\mathbf{V}_r^{X} \in \mathbb{R}^{I}$, $\mathbf{V}_r^{Y} \in \mathbb{R}^{J}$, and $\mathbf{V}_r^{Z} \in \mathbb{R}^{K}$ are factorized vectors of the three tensor modes correspond to XYZ axes for the $r$th component;
%where $\mathbf{M}_r^{Y,Z} \in  \mathbb{R}^{J\times K}$, $\mathbf{M}_r^{X,Z} \in  \mathbb{R}^{I\times K}$, $\mathbf{M}_r^{X,Y} \in  \mathbb{R}^{I\times J}$ are matrix factors for two (denoted by its superscripts) of the three modes corresponds to planes;
%$\circ$ represents outer product.