% !TEX root = ../main.tex
\section{Grid-guided Neural Radiance Fields}
\label{sec:methodology}

\begin{figure*}
	\centering
	\includegraphics[width=\linewidth]{figs/pipeline.pdf}
	\caption{\small \textbf{Overview of our framework}. The core of our model is a novel two-branch structure, namely the grid branch and NeRF branch. 
	1) We start by capturing the scene with a pyramid of feature planes at the pre-train stage, and performing a coarse sampling of ray points and predicting their radiance values through a shallow MLP renderer (grid branch), supervised by the MSE loss on the volumetrically integrated pixel colors. 
	This step yields a set of informative multi-resolution density/appearance feature planes.
	2) Next, we proceed to the joint learning stage and perform a finer sampling. We use the learned feature grid to guide NeRF branch sampling to concentrate on the scene surface. 
	The sampled points' grid feature is inferred by bilinear interpolation on the feature planes. 
	The features are then concatenated with the positional encoding and fed to NeRF branch to predict volume density and color. Note that, the grid branch outputs maintain being supervised with the ground truth images along with the fine-rendering results from the NeRF branch during the joint training.}
	\vspace{-5mm}
	\label{fig:network}
\end{figure*}


Recall that NeRF-based representations obtain the point density and color by passing the positional encoding (PE) of point coordinates into an 8-layer MLP~\cite{mildenhall2020nerf}. Such a model is highly compact as the entire scene contents are encoded in the MLP weights taking PE embedding as inputs, yet they face difficulty in scaling up limited by model capacities. In contrast, grid-based representations encode a scene into a feature grid, which can be intuitively thought of as a 3D voxel grid with grid resolution matched with the actual 3D space. Each voxel stores a feature vector at the vertices and can then be interpolated to extract a feature value at the query point coordinate and converted to the point density and color via a small network. As feature grids are often implemented as high-dimensional tensors, various factorization methods can also be applied to obtain more compact feature grid representations~\cite{chen2022tensorf}.

To effectively represent large urban scenes, we propose the grid-guided neural radiance fields, which combines the expertise of NeRF-based and grid-based methods.  The grid features are enforced to reliably capture as much local information as possible with a \emph{multi-resolution ground feature plane}.
We then let the positional encoded coordinates information pick up the missing high-frequency details and produce high-quality renderings. With a rough construction of feature grid that captrue the scene at multiple resolution,
the density field is also used to guide NeRF's sampling procedure. 
When training the NeRF branch, the grid features are jointly optimized and supervised with the reconstruction loss from both two branches. 

Fig.~\ref{fig:network} illustrates the overall pipeline of our system. 
In Sec.~\ref{subsec:multires_grid_pretrain}, we describe the pre-train of our multi-resolution ground feature plane representation; Sec.~\ref{subsec:grid_guided_nerf} introduces the grid-guided learning of neural radiance fields, corresponding to the NeRF branch in Fig.~\ref{fig:network}; Finally, we elaborate how the NeRF branch helps refine the pre-trained grid feature of the grid branch in Sec.~\ref{subsec:rectified_grid_feature}. 

\subsection{Multi-resolution Feature Grid Pre-train}
\label{subsec:multires_grid_pretrain}
Fig.~\ref{fig:teaser} illustrates a representative scenario of large urban scenes. Inspired by the fact that large urban scenes mainly ground on the $xy$-plane, we propose to represent the target large urban scene with a major \emph{ground feature plane} by constructing a multi-resolution plane-vector feature space. 
Enforcing the ground plane compression gives more informative feature planes compared to the ones obtained from full 3D grids.
This compact representation is especially suitable in urban scene scenarios and behaves robustly to sparse view training data.
Various operations (\eg, concatenation, outer product) can then be considered here to recover the 3D information from the 2D ground feature planes. 
The outer product operation was adopted in~\cite{chen2022tensorf} from the perspective of tensor factorization with low-rank approximation, which achieves a more compressed memory footprint while maintaining high quality.
The volume density $\sigma \in \mathbb{R}^+$ and view-dependent color $c \in \mathbb{R}^3$ grid-planes are separately learned to capture more environmental effects that only influences appearence. 
Formally, our grid-based radiance field is written as:
$\sigma, c = F_{\sigma}(\mathcal{G}_\sigma(X)), F_c(\mathcal{G}_c(X),\operatorname{PE}(d))$
where $\mathcal{G}_\sigma(X) \in \mathbb{R}^{R_\sigma}$, $\mathcal{G}_c(X) \in \mathbb{R}^{R_c}$ are the extracted interpolated feature values from the two grid-planes at location $X\in \mathbb{R}^3$. $F_{\sigma}, F_c$ are two fusing functions, implemented with two small MLP, that translate the concatenated density/appearance features to $\sigma, c$, and
$d \in \mathbb{S}^2$ is the viewing direction.
$\operatorname{PE}$ here represents the positional encoding $(\sin (\cdot), \cos (\cdot), \ldots, \sin \left(2^{L-1} (\cdot)\right), \cos \left(2^{L-1} (\cdot)\right))$ as in \cite{mildenhall2020nerf}.
The grid branch is then trained with $N$ query samples along the ray and predicts the pixel color following the volume rendering process as in~\cite{mildenhall2020nerf}, where the loss is the total squared error between	the rendered and true pixel colors for this coarse sampling stage, as shown in Fig.~\ref{fig:network} (a).
	

We approximate the full density and appearance grid features with the channel-wise outer product of the ground feature planes $R_{\sigma}$ and $R_c$, as well as the globally encoded $z$-axis feature vectors, following the practice of~\cite{chen2022tensorf}.
For each channel $r\in R_{\sigma}$ and $R_c$, the corresponding tensor grid of features are: where $\mathbf v^{z}$ represents the vector along $z$-axis, $\mathbf{M}^{xy}$ denotes the  matrix spanning $xy$-plane, and $\circ$ represents the outer product.
With the constraints of learning a shared $z$-axis feature vector, the optimized ground feature plane is encouraged to encode sufficient local scene contents, that can be translated by a globally shared MLP renderer.
For a specific grid resolution $n$, the density and the appearance tensors $\mathcal{G}_\sigma^n, \mathcal{G}_c^n$ are then obtained as the concatenation of $R_{\sigma}$, $R_c$ feature components:
	\begin{equation}
		\mathcal{G}_{\sigma}^n = \oplus[ (\mathbf{v}_{\sigma, r}^{z} \circ \mathbf{M}_{\sigma,r}^{xy})]_{R_\sigma}, \quad
		\mathcal{G}_{c}^n = \oplus[(\mathbf{v}_{c, r}^{z} \circ \mathbf{M}_{c,r}^{xy})]_{R_c},
	\end{equation}
where $\oplus$ denotes the concatenation operation over the $R_\sigma$ and $R_c$ dimension.
To capture different degrees of scene local complexity, 
we learn a multi-resolution feature grid with $\mathcal{G}_\sigma = \{\mathcal{G}_\sigma^n\}$ and $\mathcal{G}_c = \{\mathcal{G}_c^n\}$.
The yielding multi-resolution feature grid contains features at different granularity to describe the scene, which is particularly suitable for urban environments with objects appearing in different scales. 

\subsection{Grid-guided Neural Radiance Field}
\label{subsec:grid_guided_nerf}
A NeRF trained from scratch is required to reason about the whole scene from purely positional inputs, which only provides a band of Fourier frequencies in PE. For large urban scenes that naturally bear a wide range of granularity for geometry and texture details, NeRF constantly biases towards learning low-frequency functions, as pointed out in~\cite{tancik2020fourfeat,xiangli2022bungeenerf}. This problem gets amplified in large scenes where a large amount of information needs to be encoded. To remedy this, we propose to compress the sampling space of NeRF with the pre-trained feature grid density and enrich NeRF's pure coordinates inputs with the coarse grid features initialized in the pre-train stage.
	
Despite being of limited accuracy and granularity, the pre-trained grid feature can already offer an approximation of the scene which can be used to 1) guide NeRF's point sampling and 2) provide intermediate features as a complement to the coordinate inputs. As demonstrated in Fig.~\ref{fig:network}, instead of mapping coordinates spanning the entire sample space, NeRF can now concentrate on the approximated scene surface for more efficient and denser point sampling, and evoke high-frequency Fourier features in positional encoding to recover finer details.
Meanwhile, points along the sampled ray are projected onto the multi-resolution feature planes to retrieve density and appearance features via bilinear interpolation. The inferred grid features are then concatenated to the positional encoding as input to NeRF branch. The per-point density and color $\sigma^{\prime}, c^{\prime}$ are predicted via the NeRF branch network $F^\prime$ as:
\begin{equation}
	(\sigma^{\prime}, c^{\prime}) = F^{{\prime}}(\mathcal{G}_\sigma(X), \mathcal{G}_c(X),\operatorname{PE}(X), \operatorname{PE}(d)).
	\label{eqn:rf}
 \end{equation}
The multi-resolution feature plane plays a critical role as it provides information about the scene at multiple granularities, relieving the fitting burden of NeRF's PE so that it can concentrate on refining the fine details of the scene.
Particularly, while a high grid resolution can guarantee that each voxel in space to captures its local contents, the quality grows at the cost of storage regardless of the possible heterogeneity of detail level across the scene. It is therefore more efficient to provide such high-frequency details with Fourier features that only cost several dimensions in PE and can be adapted to scene throughout the learning process.
	
Note that the two-branch supervision and the two-stage training are necessary as: (1) a randomly initialized feature grid can hardly provide informative scene contents and may entangle the role of two types of network inputs. (2) The pre-train stage is much faster than the one with NeRF branch included, making it more efficient to reliably construct a coarse geometry with grid branch only. (3) Unlike~\cite{sun2021direct} which freezes the voxel grids when supplying PE inputs, we will later show that the feature grids can gain further refinement with its jointing learning with the NeRF branch. Moreover, as the grid branch is also supervised with reconstruction loss, it enforces the grid branch to continue enriching its captured scene information where the PE input can focus on the missing high-frequency details.
	
	
\subsection{Refined Grid Feature Planes from NeRF}
\label{subsec:rectified_grid_feature}
Recall that the feature grid relies on bilinear interpolation on the ground feature plane to obtain a feature vector of points within a voxel. The mechanism can yield detailed reconstruction results given sufficiently high grid resolution, such that the finest variation in the scene can be recovered. However, learning a grid with matching resolution can be highly memory-consuming for large urban scenes, as indicated in~\cite{mueller2022instant}. Moreover, the grid feature lacks the incentive to capture accurate variations within a voxel with merely reconstruction loss on ground truth RGB.
We therefore jointly optimize the feature plane and vector with NeRF to enhance the supervision signal for grid features with point-wise guidance from the supplied NeRF inputs. Another benefit NeRF brings is the global regularization on the independently optimized grid features. Fig.~\ref{fig:teaser} and Fig.~\ref{fig:comparison} show that grid-based methods suffer from noisy artifacts because of the lack of constraints on space continuity and semantic similarity. NeRF, on the contrary, uses a shared MLP for the entire scene space. We will later show that the rendered novel views interpreted from the grid branch can get largely improved after its joint training with the NeRF branch.
	
	
