\section{Introduction}
\label{sec:intro}

Large urban scene modeling has been drawing lots of research attention with the recent emergence of neural radiance fields (NeRF) due to its photorealistic rendering and model compactness~\cite{tewari2021advances,mildenhall2020nerf,barron2021mipnerf360,tancik2022block,xiangli2022bungeenerf,turki2021mega}. Such modeling can enable a variety of practical applications,  including autonomous vehicle simulation~\cite{Li2019AADSAA,Yang2020SurfelGANSR,Ost2021NeuralSG}, aerial surveying~\cite{Du2018TheUA,Bozcan2020AUAIRAM}, and embodied AI~\cite{Morad2021EmbodiedVN,Truong2021BiDirectionalDA}.
NeRF-based methods have shown impressive results on object-level scenes with their \emph{continuity prior} benefited from the MLP architecture and \emph{high-frequency details} with the globally shared positional encodings. However, they often fail to model large and complex scenes. These methods suffer
%NeRF based 
 from underfitting due to limited model capacity and only produce blurred renderings without fine details~\cite{xiangli2022bungeenerf,tancik2022block,turki2021mega}.
BlockNeRF~\cite{tancik2020fourfeat} and MegaNeRF~\cite{turki2021mega} propose to geographically divide urban scenes and assign each region a different sub-NeRF to learn in parallel.
Subsequently,
when the scale and complexity of the target scene increases,
they inevitably suffer from a trade-off between the number of sub-NeRFs and the capacity required by each sub-NeRF to fully capture all the fine details in each region.
Another stream of grid-based representations represents the target scene using a grid of features~\cite{liu2020nsvf,yu2021plenoctrees,yu2021plenoxels,mueller2022instant,liu2020nsvf,Martel2021ACORNAC,Takikawa2021NeuralGL}. 
These methods are generally much faster during rendering and more efficient when the scene scales up.
However, as each cell of the feature grid is individually optimized in a locally encoded manner, the resulting feature grids tend to be less continuous across the scene compared to NeRF-based methods.
Although more flexibility is intuitively beneficial for capturing fine details, the lack of inherent continuity makes this representation vulnerable to suboptimal solutions with noisy artifacts, as demonstrated in Fig.~\ref{fig:teaser}.

To effectively reconstruct large urban scenes with implicit neural representations, in this work, we propose a two-branch model architecture that takes a unified scene representation that integrates both \textbf{grid-based} and \textbf{NeRF-based} approaches under a joint learning scheme.
Our key insight is that these two types of representations 
can be used complementary to each other:
while feature grids can easily fit local scene content with explicit and independently learned features,
NeRF introduces an inherent \emph{global continuity} on the learned scene content with its shareable MLP weights across all 3D coordinate inputs.
NeRF can also encourage capturing high-frequency scene details by matching the \emph{positional encodings} as Fourier features with the bandwidth of details.
However, unlike feature grid representation, NeRF is less effective in compacting large scene contents into its globally shared latent coordinate space.

Concretely,
we firstly model the target scene with a feature grid in a pre-train stage, which coarsely captures scene geometry and appearance. The coarse feature grid is then used to
1) guide NeRF's point sampling to let it concentrate around the scene surface;
and 2) supply NeRF's positional encodings with extra features about the scene geometry and appearance at sampled positions.
Under such guidance, NeRF can effectively and efficiently pick up finer details in a drastically compressed sampling space. Moreover, as coarse-level geometry and appearance information are explicitly provided to NeRF, a light-weight MLP is sufficient to learn a mapping from global coordinates to volume densities and color values.
The coarse feature grids get further optimized with gradients from the NeRF branch in the second joint-learning stage,
which regularizes them to produce more accurate and natural rendering results when applied in isolation.
To further reduce memory footprint and learn a reliable feature grid for large urban scenes,
we adopt a compact factorization of the 3D feature grid to approximate it without losing representation capacity.
Based on the observation that essential semantics such as the urban layouts are mainly distributed on the ground (\ie, $xy$-plane), we propose to factorize the 3D feature grid into 2D ground feature planes spanning the scene and a vertically shared feature vector along the $z$-axis.
The benefits are manifold:
1) The memory is reduced from $\mathcal{O}(N^3)$ to $\mathcal{O}(N^2)$.
2) The learned feature grid is enforced to be disentangled into
highly compact ground feature plans, offering explicit and informative scene layouts.
Extensive experiments show the effectiveness of our unified model and scene representation. 
When rendering novel views in practice, users are allowed to use either the grid branch at a faster rendering speed, or the NeRF branch, with more high-frequency details and spatial smoothness, yet at the cost of a relatively slower rendering.