% !TEX root = ../main.tex
\section{Related Works and Background}
\label{sec:related}

\noindent \textbf{Large-scale Scene Reconstruction and Rendering.}  This is a long-standing problem in computer vision and graphics, and many early works \cite{fruh2004automated, snavely2006phototourism, pollefeys2008detailed, li2008modeling, agarwal2011building,zhu2018very, schonberger2016structure} have attempted to solve it.
Most of these methods adopt a three-stage pipeline. They firstly detect salient points \cite{lowe2004sift, hassner2012sifts} in 2D images and construct the point descriptors \cite{rublee2011orb, bay2006surf}. The point descriptors are then matched across images to obtain 2D correspondences that are used for camera pose estimation and 3D point triangulation. Finally, the camera poses and 3D points are jointly optimized to minimize the difference between the projected and image points using bundle adjustment \cite{hartleyziss2004, triggs1999bundle}.
These methods have shown impressive reconstruction performance on large-scale scenes.
\cite{shan2013turing, losasso2004geometry} utilize the reconstruction results to accomplish the free-viewpoint navigation of scenes.
However, there are often artifacts or holes in the reconstructed scenes, which limit the rendering quality.
Recent methods \cite{li2020crowdsampling, meshry2019neural, martin2021nerf} exploit deep learning techniques to improve the results of image synthesis.
\cite{meshry2019neural} rasterizes the recovered point clouds into deep buffers, which are then interpreted into 2D images using the 2D convolutional neural networks.
More recently, \cite{martin2021nerf} leverages the technique of generative latent optimization to reconstruct radiance fields from photo collections, enabling it to achieve photorealistic rendering results. \cite{devries2021unconstrained,sharma2022seeing} adopted a similar ground plane representation as us for scene generation and static-dynamic disentanglement tasks respectively, yet the scale of their applicable scenes and the rendering qualities remain limited.

\smallskip
\noindent \textbf{Volumetric Scene Representations.}
Coordinate-based multi-layer perceptrons have become a popular representation for 3D shape modeling \cite{park2019deepsdf, chabra2020deep, mescheder2019occupancy, chen2019learning} and novel view synthesis \cite{niemeyer2020differentiable, mildenhall2020nerf, barron2021mip, barron2021mipnerf360}.
To represent high-resolution 3D shapes, some methods adopt MLP networks that take continuous 3D coordinates as input and predict the target values~\cite{park2019deepsdf,mescheder2019occupancy,chibane2020neural}.
\cite{chibane2020implicit, peng2020convolutional, chen2021multiresolution} introduces convolutional neural networks to learn powerful scene priors, enabling the coordinate-based representations to handle larger scenes.
In the field of novel view synthesis, NeRF \cite{mildenhall2020nerf} represents 3D scenes as density and color fields and optimizes this representation from images through volume rendering techniques.
\cite{yu2021plenoctrees, hedman2021baking, liu2020nsvf, reiser2021kilonerf} extend NeRF with efficient data structures to accelerate the rendering process.
\cite{chan2021efficient, schwarz2020graf, niemeyer2021giraffe, xu20213d} combine NeRF with generative models and achieve 3D-aware image generation.
\cite{li2021neural, park2021nerfies, pumarola2021d} augment NeRF with motion fields, enabling them to handle dynamic scenes.

\smallskip
\noindent \textbf{NeRF Scale-up.}
While the aforementioned NeRF approaches mainly consider scenes of a limited scale, scaling up NeRF to handle large-scale scenes such as cities would enable broader applications.
\cite{liu2020nsvf, xu2022point, turki2021mega} have tried to improve the rendering quality of NeRF on large-scale scenes.
Feature grid methods \cite{liu2020nsvf, li2022neural} map input coordinates to a high dimensional space with a lookup from the predefined table of learnable feature vectors, which augments the approximation ability of neural networks.
PointNeRF \cite{xu2022point} regresses the radiance field from the point cloud and accomplishes the high-quality rendering of indoor scenes.
BungeeNeRF \cite{xiangli2022bungeenerf} designs a multi-scale representation that efficiently models the scene content and improves the rendering quality.
\cite{turki2021mega, tancik2022block} decompose the scene into several spatial regions that are separately represented by NeRF networks.
When processing large scenes, another critical problem is how to reduce the training time.
Several techniques such as image encoder \cite{wang2021ibrnet, zhang2022nerfusion}, auto-decoder \cite{ramon2021h3d, dupont2022data} and meta-learning \cite{tancik2021learned, bergman2021fast} are used to pretrain networks on a dataset to learn the scene prior for improving the optimization process.
\cite{sun2021direct, mueller2022instant, yu2021plenoxels,sun2022improved} further explore the grid representations to accelerate the training speed.
TensoRF \cite{chen2022tensorf} investigate the factorization of 3D scenes, enabling it to compactly represent scenes and achieve the fast training. Instant-NGP~\cite{mueller2022instant} adopts a multi-resolution hash table of feature vectors that enables extremely fast renderings. However, we notice both these methods suffer from noisy feature gridss when applied to large scenes.