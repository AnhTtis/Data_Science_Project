In this section we prove Theorem \ref{thm:markov-intro} (restated
formally below as Theorem \ref{thm:markov-formal}), establishing that
in two-player Markov games, there is no computationally efficient
algorithm that computes a sequence $\sigma\^1, \ldots, \sigma\^T$ of product Markov policies so that each player has small regret
under this sequence. This section serves as a warm-up for our results in \cref{sec:nonmarkov}, which remove the assumption that $\sigma\ind{1},\ldots,\sigma\ind{T}$ are Markovian.


\arxiv{\subsection{\MarkCCE problem and computational model}}
\icml{\subsection{\MarkCCE and computational model}}
\label{sec:model}
As discussed in the introduction, our lower bounds for no-regret learning are a consequence of lower bounds for the \emph{\GenCCE} problem. In what follows, we formalize this problem (specifically, the Markovian variant, which we refer to as \emph{\MarkCCE}), as well as our computational model.

\paragraph{Description length for Markov games (constant $m$).}
Given a Markov game $\MG$, we let $\beta(\MG)$ denote the maximum number
of bits needed to describe any of the rewards $R_{i,h}(s, \ba)$ or transition probabilities $\BP_h(s' | s,\ba)$ in binary.\footnote{We emphasize that $\beta(\cG)$ is defined as the maximum number of bits required by any particular $(s,\ba)$ pair, not the total number of bits required for \emph{all} $(s,\ba)$ pairs.} We define $|\MG| := \max\{ S, \max_{i \in [m]} A_i, H, \beta(\MG) \}$. The interpretation of $|\MG|$ depends on the number of players $m$: If $m$ is a constant (as will be the case in the current section and Section \ref{sec:nonmarkov}), then $|\MG|$ should be interpreted as the description length of the game $\MG$, up to polynomial factors. In particular, for constant $m$, %
the game $\MG$ can be described using $|\MG|^{O(1)}$
bits. %
In Section \ref{sec:multiplayer}, we discuss the interpretation of $|\MG|$ when $m$ is large.


\paragraph{The \MarkCCE problem.}  
From Fact \ref{fac:no-regret-cce}, we know that the problem of computing a sequence $\sigma\^1, \ldots, \sigma\^T$ of
joint product Markov policies for which each player has at most $\eps\cdot{}T$ regret is equivalent to computing a sequence $\sigma\^1, \ldots, \sigma\^T$ for which the uniform mixture forms an $\eps$-approximate CCE. We define $(T, \ep)$-\MarkCCE as the computational problem of computing such a CCE directly.

\begin{defn}[\MarkCCE problem]
  \label{def:markcce}
  For an $m$-player Markov game $\MG$ and parameters $T \in \BN$ and $\ep > 0$ (which may depend on the size of the game $\MG$), $(T, \ep)$-\MarkCCE is the problem of finding a sequence $\sigma\^1, \ldots, \sigma\^T$, with each $\sigma\^t \in \PiMarkov$, such that the distributional policy $\ol \sigma = \frac{1}{T} \sum_{t=1}^T \indic{\sigma\^t} \in \Delta(\Pirndrnd)$ is an $\ep$-CCE of $\MG$ (or equivalently, such that for all $i \in [m]$, $\Reg_{i,T}(\sigma\^1, \ldots, \sigma\^T) \leq \ep \cdot T$).
\end{defn}
Decentralized learning algorithms naturally lead to solutions to the \MarkCCE problem. In particular, consider any decentralized protocol which runs for $T$ episodes, where at each timestep $t\in\brk{T}$, each player $i \in [m]$ chooses a Markov policy $\sigma_i\^t \in \PiMarkov_i$ to play, without knowledge of the other players' policies $\sigma_{-i}\ind{t}$ (but possibly using the history); any strategy in which players independently run online learning algorithms falls under this protocol. If each player experiences overall regret at most $\ep \cdot T$, then the sequence $\sigma\ind{1},\ldots,\sigma\ind{T}$ is a solution to the $(T,\ep)$-\MarkCCE problem. However, one might expect the $(T,\ep)$-\MarkCCE problem to be much easier than decentralized learning, since it allows for algorithms that produce $(\sigma\^1, \ldots, \sigma\^T)$ satisfying the constraints of Definition \ref{def:markcce} in a centralized manner. The main result of this section, Theorem \ref{thm:markov-formal}, rules out the existence of \emph{any} efficient algorithms, including centralized ones, that solve the \MarkCCE problem.


{Before moving on, let us give a sense for what sort of scaling one should expect for the parameters $T$ and $\eps$ in the $(T,\eps)$-\MarkCCE problem.}\xspace First, we note that there always exists a solution to the $(1,0)$-\MarkCCE problem in a Markov game, which is given by a (Markov) Nash equilibrium of the game; of course, Nash equilibria are intractable to compute in general.\icmlfncut{Such a Nash equilibrium can be seen to exist by using backwards induction to specify the player's joint distribution of play at each state at steps $H, H-1, \ldots, 1$.}
For the special case of normal-form games (where there is only a single state, and $H=1$), no-regret learning (e.g., Hedge) yields a computationally efficient solution to the $(T, \til O(1/\sqrt{T}))$-\MarkCCE problem, where the $\til O(\cdot)$ hides a $\max_i \log |A_i|$ factor. Refined convergence guarantees of \citet{daskalakis2021nearoptimal,anagnostides2022uncoupled} improve upon this result, and yield an efficient solution to the $(T, \til O(1/T))$-\MarkCCE problem.





\subsection{Main result}
\label{sec:markov-formal}
\begin{theorem}
  \label{thm:markov-formal}
There is a constant $C_0 > 1$ so that the following holds. Let $n \in \BN$ be given, and let $\Tn \in \BN$ and $\epn > 0$ satisfy $\Tn < \exp(\epn^2 \cdot n^{1/2}/2^{5})$. Suppose there is an algorithm that, given the description of any 2-player Markov game $\MG$ with $|\MG| \leq n$, solves the $(\Tn, \epn)$-\MarkCCE problem in time $U$, for some $U \in \BN$. %
  Then, for each $n \in \BN$, the 2-player $(\lfloor n^{1/2}\rfloor, 4 \cdot \epn)$-\Nash problem (Definition \ref{def:nash}) can be solved in time $(n\Tn U)^{C_0}$. \loose
\end{theorem}
\dfcomment{based on reviewer suggestion: add remark on why restriction to $T\leq\exp(.)$ is natural.}\noah{added it}


We emphasize that the range $T < \exp(n^{O(1)})$ ruled out by Theorem \ref{thm:markov-formal} is the most natural parameter regime, since the runtime of any decentralized algorithm which runs for $T$ episodes and produces a solution to the \MarkCCE problem is at least linear in $T$. 
Using that 2-player $(n, \ep)$-\Nash is \PPAD-complete for $\ep = n^{-c}$ (for any $c > 0$) \citep{daskalakis2009complexity,chen2006computing,rubinstein2018inapproximability}, we obtain the following corollary.
\begin{corollary}[\MarkCCE is \PPAD-complete]
  \label{cor:markov-formal-ppad}
For any constant $C > 4$, if there is an algorithm which, given the description of a 2-player Markov game $\MG$, solves the $(|\MG|^C, |\MG|^{-\frac{1}{C}})$-\MarkCCE problem  in time $\poly(|\MG|)$, then $\PPAD = \PP$.
\end{corollary}
The condition $C > 4$ in Corollary \ref{cor:markov-formal-ppad} is set to ensure that $|\MG|^C < \exp(|\MG|^{-2/C} \cdot \sqrt{|\MG|} / 2^6)$ for sufficiently large $|\MG|$, so as to satisfy the condition of Theorem \ref{thm:markov-formal}. 
Corollary \ref{cor:markov-formal-ppad} rules out the existence of a polynomial-time algorithm that solves the \MarkCCE problem with accuracy $\eps$ polynomially small and $T$ polynomially large in $\abs{\cG}$.
\icmlcut{Using a stronger complexity-theoretic assumption, the Exponential Time Hypothesis for \PPAD \cite{rubinstein2016settling}, we can obtain a stronger hardness result which rules out efficient algorithms even when 1)  the accuracy $\eps$ is constant and 2) $T$ is quasipolynomially large.\icmlfncut{This is a consequence of the fact that for some absolute constant $\ep_0 > 0$, there are no polynomial-time algorithms for computing $\ep_0$-Nash equilibria in 2-player normal-form games under the Exponential Time Hypothesis for \PPAD \cite{rubinstein2016settling}.}
\begin{corollary}[ETH-hardness of \MarkCCE]
There is a constant $\ep_0 > 0$ such that if there exists an algorithm that solves the $(|\MG|^{o(\log |\MG|)}, \ep_0)$-\MarkCCE problem in $|\MG|^{o(\log |\MG|)}$ time, then the Exponential Time Hypothesis for \PPAD fails to hold.
\end{corollary}
}



\paragraph{Proof overview.} The proof of Theorem \ref{thm:markov-formal} is based on a reduction, which shows that any algorithm that efficiently solves the $(T, \ep)$-\MarkCCE problem, for $T$ not too large, can be used to efficiently compute an approximate Nash equilibrium of any given normal-form game. In particular, fix $n_0\in\bbN$, and let a 2-player normal form game $G$ with $n_0$ actions be given. We construct a Markov game $\MG = \MG(G)$ with horizon $H = n_0$ and action sets identical to those of the game $G$, i.e., $\MA_1 = \MA_2 = [n_0]$. The state space of $\MG$ consists $n_0^2$ states, which are indexed by joint action profiles; the transitions are defined so that the value of the state at step $h$ encodes the action profile taken by the agents at step $h-1$.\footnote{For technical reasons, this only is the case for even values of $h$; we discuss further details in the full proof in Section \ref{sec:markov-proof}.} At each state of $\MG$, the reward functions are given by the payoff matrices of $G$, scaled down by a factor of $1/H$ (which ensures that the rewards received at each step belong to $[0,1/H]$). In particular, the rewards and transitions out of a given state do not depend on the identity of the state, and so $\MG$ can be thought of as a repeated game where $G$ is played $H$ times. The formal definition of $\MG$ is given in Definition \ref{def:mg-g}.


Fix any algorithm for the \MarkCCE problem, and recall that for each step $h$ and state $s$ for $\MG$, $\sigma\^t_h(s) \in \Delta(\MA_1) \times \Delta(\MA_2)$ denotes the joint action distribution taken in $s$ at step $h$ for the sequence of $\sigma\ind{1},\ldots,\sigma\ind{T}$ produced by the algorithm. The bulk of the proof of \cref{thm:markov-formal} consists of proving a key technical result, Lemma \ref{lem:2nash-main}, which states that if $\sigma\ind{1},\ldots,\sigma\ind{T}$ indeed solves $(T,\eps)$-\MarkCCE, then there exists some tuple $(h,s,t)$ such that $\sigma_h\^t(s)$ is an approximate Nash equilibrium for $G$. With this established, it follows that we can find a Nash equilibrium efficiently by simply trying all $HST$ choices for $(h,s,t)$.


 
To prove Lemma \ref{lem:2nash-main}, we reason as follows. Assume that $\ol \sigma := \frac 1T \sum_{t=1}^T \indic{\sigma\^t}\in\Delta(\Pirndrnd)$ is an $\eps$-CCE. If, by contradiction, none of the distributions $\crl*{\sigma_h\^t(s)}_{h\in\brk{H},s\in\cS,t\in\brk{T}}$ are approximate Nash equilibria for $G$, then it must be the case that for each $t$, one of the players has a profitable deviation in $G$ with respect to the product strategy $\sigma_h\^t(s)$, at least for a constant fraction of the tuples $(s,h)$. We will argue that if this were to be the case, it would imply that there exists a non-Markov deviation policy for at least one player $i$ in Definition \ref{def:cce}, meaning that $\ol \sigma$ is not in fact an $\ep$-CCE.

To sketch the idea, recall that to draw a trajectory from $\ol \sigma$, we first draw a random index $t^\st \sim [T]$ uniformly at random, and then execute $\sigma\^{t^\st}$ for an episode. We will show (roughly) that for each player $i$, it is possible to compute a non-Markov deviation policy $\pi_i^\dagger$ which, under the draw of a trajectory from $\ol \sigma$, can ``infer'' the value of the index $t^\st$ within the first few steps of the episode. Then policy $\pi_i^\dagger$ then, at each state $s$ and step $h$ after the first few steps, play a best response to their opponent's portion of the strategy $\sigma_h\^{t^\st}(s)$. If, for each possible value of $t^\st$, none of the distributions $\sigma_h\^{t^\st}(s)$ are approximate Nash equilibria of $G$, this means that at least one of the players $i$ can significantly increase their value in $\MG$ over that of $\ol \sigma$ by playing $\pi_i^\dagger$, which contradicts the assumption that $\ol \sigma$ is an $\ep$-CCE.

It remains to explain how we can construct a non-Markov policy $\pi_i^\dagger$ which ``infers'' the value of $t^\st$. Unfortunately, exactly inferring the value of $t^\st$ in the fashion described above is impossible: for instance, if there are $t_1 \neq t_2$ so that $\sigma\^{t_1} = \sigma\^{t_2}$, then clearly it is impossible to distinguish between the cases $t^\st= t_1$ and $t^\st = t_2$. Nevertheless, by using the fact that each player observes the full joint action profile played at each step $h$, we can construct a non-Markov policy which employs \emph{Vovk's aggregating algorithm} for online density estimation \cite{vovk1990aggregating,cesa2006prediction} in order to compute a distribution which is \emph{close} to $\sigma_h\^{t^\st}(s)$ for most $h \in [H]$.\icmlfncut{Vovk's aggregating algorithm is essentially the exponential weights algorithm with the logarithmic loss. A detailed background for the algorithm is provided in Section \ref{sec:online-density}.} This guarantee is stated formally in an abstract setting in Proposition \ref{prop:online-tvd}, and is instantiated in the proof of Theorem \ref{thm:markov-formal} in (\cref{eq:sum-tv-bound}). As we show in Section \ref{sec:markov-proof}, approximating $\sigma_h\^{t^\st}(s)$ as we have described is sufficient to carry out the reasoning from the previous paragraph. %



