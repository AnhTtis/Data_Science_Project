\documentclass[11pt, oneside]{article}
\usepackage{etoolbox}
\newcommand{\arxiv}[1]{\iftoggle{icml}{}{#1}}
\newcommand{\icml}[1]{\iftoggle{icml}{#1}{}}
\newtoggle{icml}
\global\togglefalse{icml}

\icml{
  \usepackage[subtle]{savetrees} %
  }

\icml{
  \usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\newcommand{\theHalgorithm}{\arabic{algorithm}}
}

\icml{
  \newcommand{\State}{\STATE}
  \newcommand{\For}{\FOR}
  \newcommand{\EndFor}{\ENDFOR}
  \newcommand{\If}{\IF}
  \newcommand{\EndIf}{\ENDIF}
  \newcommand{\Return}{\RETURN}
  \newcommand{\Statex}{\State}
}



  \usepackage{comment}

\arxiv{
  \usepackage{geometry}
  \geometry{letterpaper}
  }

\usepackage{graphicx}	
\usepackage{amssymb}
\usepackage{amsfonts,latexsym,amsthm,amssymb,amsmath,amscd,euscript}
\usepackage{bbm}
\usepackage{framed}
\arxiv{\usepackage{fullpage}}
\usepackage{mathrsfs}


\newcommand{\loose}{\looseness=-1}

\PassOptionsToPackage{dvipsnames}{xcolor}

\usepackage{etoolbox}



\arxiv{\usepackage{algorithm}}
\arxiv{\usepackage{verbatim}}
\arxiv{\usepackage[noend]{algpseudocode}}
\newcommand{\multiline}[1]{\parbox[t]{\dimexpr\linewidth-\algorithmicindent}{#1}}


\usepackage{mathabx}
\usepackage{accents}
\usepackage{setspace}
\usepackage{tikz-cd}
\usepackage{xspace}
\usepackage[final]{showlabels}
\renewcommand{\showlabelfont}{\small\color{blue}}


\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}
\makeatother

\newcommand{\xref}{\nameref}
\makeatletter
\newcommand\xlabel[2][]{\phantomsection\def\@currentlabelname{#1}\label{#2}}
\makeatother

\icml{\usepackage{icml2023}}

\usepackage{hyperref}
\hypersetup{colorlinks=true,citecolor=blue,urlcolor=black,linkbordercolor={1 0 0}}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=blue,
  citecolor = black,      
  urlcolor=cyan,
}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newreptheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{postulate}[theorem]{Postulate}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{question}[theorem]{Question}
\newtheorem{problem}[theorem]{Problem}

\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{theorem}{section}

\icml{\icmltitlerunning{Hardness of Independent Learning in Markov Games}}


\input{dylan_minus_noah}


\usepackage[suppress]{color-edits}
\addauthor{df}{ForestGreen}
\addauthor{ng}{purple}

\arxiv{
  \usepackage{parskip}
}

\newcommand{\citet}[1]{\cite{#1}}
\newcommand{\citep}[1]{\cite{#1}}

 
\icml{
\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}
  }

  \newcommand{\utility}{reward\xspace}

  \newcommand{\vlearning}{\texttt{V-learning}\xspace}



  \input{macros}
  \icml{
\renewcommand{\paragraph}{\textbf}
  }

\arxiv{
\title{Hardness of Independent Learning and \\ Sparse Equilibrium Computation in Markov Games}

\author{  Dylan J. Foster\\{\small \texttt{dylanfoster@microsoft.com}} \and Noah Golowich\\{\small \texttt{nzg@mit.edu}}\\ \and Sham M.  Kakade\\{\small \texttt{sham@seas.harvard.edu}}}
\date{\today}
}

\begin{document}

\icml{
\twocolumn[
\icmltitle{Hardness of Independent Learning \\ and Sparse Equilibrium
  Computation in Markov Games}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Firstname1 Lastname1}{equal,yyy}
\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Firstname3 Lastname3}{comp}
\icmlauthor{Firstname4 Lastname4}{sch}
\icmlauthor{Firstname5 Lastname5}{yyy}
\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
\icmlauthor{Firstname7 Lastname7}{comp}
\icmlauthor{Firstname8 Lastname8}{sch}
\icmlauthor{Firstname8 Lastname8}{yyy,comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

\icmlkeywords{Independent Learning, Nash equilibrium, Folk theorem}

\vskip 0.3in
]



\printAffiliationsAndNotice{}  %
}

\arxiv{\maketitle}

\begin{abstract}
\input{abstract}
\end{abstract}


\section{Introduction}
\label{sec:intro}
\input{section_intro}


  \section{Preliminaries}
  \label{sec:prelim}
\input{section_prelims}


\arxiv{\section{Lower bound for Markovian algorithms}}
\icml{\section{Lower bound for Markovian algorithms}}
\label{sec:markov}
\input{section_markov}


\arxiv{\section{Lower bound for non-Markov algorithms}}
\icml{\section{Lower bound for non-Markov algorithms}}
\label{sec:nonmarkov}
\input{section_nonmarkov}


\arxiv{
\section{Multi-player games: Statistical lower bounds}
\label{sec:multiplayer}
\input{section_multiplayer}
}


\arxiv{
\section{Discussion and interpretation}
\label{sec:discussion}
\input{section_discussion}
}

\section*{Acknowledgements}
This work was performed in part while NG was an intern at Microsoft Research. NG is supported at MIT by a Fannie \& John Hertz Foundation Fellowship and an NSF Graduate Fellowship. 
This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. SK acknowledges funding from the Office of Naval Research under award N00014-22-1-2377 and the National Science Foundation Grant under award \#CCF-2212841.


\newpage

 \arxiv{
  \bibliographystyle{alpha}
  \bibliography{refs.bib}
   }


\icml{
\bibliography{refs.bib}
\bibliographystyle{icml2023}
}

\newpage

\appendix
\icml{\onecolumn}

\renewcommand{\contentsname}{Contents of Appendix}
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
{
  \hypersetup{hidelinks}
  \tableofcontents
}




\icml{
  \part{Additional results and discussion}

  \icml{
    \section{Tighter computational lower bounds under ETH for \PPAD}
    \label{sec:eth}
    \input{section_eth}
    }

\section{Multi-player games: Unconditional lower bounds}
\label{sec:multiplayer}
\input{section_multiplayer}

\section{Discussion and interpretation}
\label{sec:discussion}
\input{section_discussion}
}

\icml{
  \part{Proofs}
  }
  \section{Additional preliminaries}

  \icml{
  \subsection{Additional preliminaries for Markov games}
  
     \paragraph{Deterministic policies.}
  It will be helpful to introduce notation for
  \emph{deterministic} general (non-Markov) policies, which correspond
  to the special case of randomized policies where each policy $\sigma_{i,h}$ exclusively maps to singleton distributions. {In particular, a deterministic general policy of agent $i$ is 
  a collection of mappings $\pi_i =
  (\pi_{i,1}, \ldots, \pi_{i,H})$, where $\pi_{i,h} : \CH_{i,h-1} \times
  \MS \ra \MA_i$.} %
  We denote by $\Pidet_i$ the space of deterministic
  general policies of agent $i$, and further write $\Pidet:= \Pidet_1
  \times \cdots \times \Pidet_m$ to denote the space of \emph{joint
    deterministic policies}. {We use the convention throughout that
  deterministic policies are denoted by the letter $\pi$, whereas
  randomized policies are denoted by $\sigma$.}


\paragraph{Additional facts on regret and CCE.}
The following facts regarding deterministic policies and the definition of coarse correlated equilibria and regret are well-known:
\begin{itemize}
\item In the context of Definition \ref{def:cce} (defining an $\ep$-CCE), the maximizing policy $\sigma_i'$ can always be chosen to be determinimistic, so $\distp \in \Delta(\Pirndrnd)$ is an $\ep$-CCE if and only if $\max_{\pi_i \in \Pidet_i} V_i^{\pi_i \times \distp_{-i}} - V_i^\distp \leq \ep$.
  \item  In the context of (\ref{eq:reg-defn}) in the definition of regret, the maximum over $\sigma_i \in \Pirndrnd_i$ is always achieved by a deterministic general policy, so we have $\Reg_{i,T} = \max_{\pi_i \in \Pidet_i} \sum_{t=1}^T \prn[\big]{ V_i^{\pi_i \times \sigma_{-i}\^t} - V_i^{\sigma\^t} }$.
\end{itemize}
Next, the following standard result
shows that the uniform average of any no-regret sequence forms an
approximate coarse correlated equilibrium.
\begin{fact}[No-regret is equivalent to CCE]
  \label{fac:no-regret-cce}
 Suppose that a sequence of policies $\sigma\^1, \ldots, \sigma\^T\in
 \Pirndrnd$ satisfies $\Reg_{i,T}(\sigma\^1, \ldots, \sigma\^T ) \leq
 \ep \cdot T$ for each $i \in [m]$. Then the uniform average of these
 $T$ policies, namely the distributional policy $\ol \sigma :=
 \frac{1}{T} \sum_{t=1}^T \indic{\sigma\^t} \in \Delta(\Pirndrnd)$, is
 an $\ep$-CCE.

Likewise if a sequence of policies $\sigma\^1, \ldots, \sigma\^T\in
\Pirndrnd$ has the property that the distributional policy $\ol \sigma
:= \frac{1}{T} \sum_{t=1}^T \indic{\sigma\^t} \in \Delta(\Pirndrnd)$,
is an $\ep$-CCE, then we have $\Reg_{i,T}(\sigma\^1, \ldots, \sigma\^T ) \leq \ep \cdot T$ for all $i \in [m]$.
\end{fact}
Fact \ref{fac:no-regret-cce} is an immediate consequence of Definitions \ref{def:cce} and \ref{def:regret}. 
 }

\subsection{Nash equilibria and computational hardness.}
\label{sec:nash-prelims}
\noah{I moved this permanently here (not just icml), in part due to laziness but also b/c I think it's pretty standard}
The most foundational and well known solution concept for normal-form games is the \emph{Nash equilibrium} \cite{nash1951noncooperative}. 
\begin{defn}[$(n,\ep)$-\Nash problem]
    \label{def:nash}
    For a normal-form game $G = (M_1, \ldots, M_m)$ and $\ep > 0$, a product distribution $p \in \prod_{j=1}^m \Delta([n])$ is said to be an $\ep$-Nash equilibrium for $G$ if for all $i \in [n]$,
  \begin{align}
\max_{a_i' \in [n]} \E_{\ba \sim p} [(M_i)_{a_i', \ba_{-i}} ] - \E_{\ba \sim p}[(M_i)_\ba] \leq \ep\nonumber.
  \end{align}

  We define the \emph{$m$-player $(n,\ep)$-\Nash problem} to be the
  problem of computing an $\ep$-Nash equilibrium of a given $m$-player
  $n$-action normal-form game.\footnote{One must also take care to
    specify the bit complexity of representing a normal-form game. We
    assume that the payoffs of any normal-form game given as an
    instance to the $(n, \ep)$-\Nash problem can each be expressed
    with $\max\{n,m\}$ bits; this assumption is without loss of
    generality as long as $\ep \geq 2^{-\max\{n,m\}}$ (which it will
    be for us). \label{fn:nash-bits}}
\end{defn}
Informally, $p$ is an $\ep$-Nash equilibrium if no player $i$ can gain
more than $\ep$ in \utility by deviating to a single fixed action
$a_i'$, while all other players randomly choose their actions
according to $p$. Despite the
  intuitive appeal of Nash equilibria, they are intractable to compute:
  for any $c > 0$, it is \PPAD-hard to solve the $(n, n^{-c})$-\Nash problem, namely, to compute $n^{-c}$-approximate Nash equilibria in 2-player $n$-action normal-form games \cite{daskalakis2009complexity,chen2006computing,rubinstein2018inapproximability}. We recall that the complexity class \PPAD consists of all total search
  problems which have a polynomial-time reduction to the
  \texttt{End-of-The-Line (EOTL)} problem. \PPAD is the most well-studied
  complexity class in algorithmic game theory, and it is widely
  believed that $\PPAD\neq\PP$. We refer the reader to \cite{daskalakis2009complexity,chen2006computing,rubinstein2018inapproximability,papadimitriou1994complexity} for further background on the class \PPAD and the \texttt{EOTL} problem.
  
  
\subsection{Query complexity of Nash equilibria}
\label{sec:query-complexity-proof}
Our statistical lower bound for the \GenCCE problem in \cref{thm:statistical-lb} relies on existing query complexity lower  bounds for computing approximate Nash equilibria in $m$-player normal-form games. We first review the query complexity model for normal-form games.

\paragraph{Oracle model for normal-form games.} For $m,n \in \BN$, consider an $m$-player $n$-action normal form game $G$, specified by payoff tensors $M_1, \ldots, M_m$. Since the tensors $M_1, \ldots, M_m$ contain a total of $mn^m$ real-valued payoffs, in the setting when $m$ is large, it is unrealistic to assume that an algorithm is given the full payoff tensors as input. Therefore, prior work on computing equilibria in such games has studied the setting in which the algorithm makes adaptive \emph{oracle queries} to the payoff tensors.

In particular, the algorithm, which is allowed to be randomized, has access to a \emph{payoff oracle} $\MO_G$ for the game $G$, which works as follows. At each time step, the algorithm can choose to specify an action profile $\ba \in [n]^m$ and then query $\MO_G$ at the action profile $\ba$. The oracle $\MO_G$ then returns the payoffs $(M_1)_{\ba}, \ldots, (M_m)_{\ba}$ for each player if the action profile $\ba$ is played.

\paragraph{Query complexity lower bound for approximate Nash equilibrium.} The following theorem gives a lower bound on the number of queries any randomized algorithm needs to make to compute an approximate Nash equilibrium in an $m$-player game.
\begin{theorem}[Corollary 4.5 of \cite{rubinstein2016settling}]
  \label{thm:query-lbs}
  There is a constant $\ep_0 > 0$ so that any randomized algorithm which solves the $(2, \ep_0)$-\Nash problem for $m$-player normal-form games %
  with probability at least $2/3$ must use at least $2^{\Omega(m)}$ payoff queries. 
\end{theorem}
We remark that \cite{babichenko2016query,chen2017wellsupported} provide similar, though quantitatively weaker, lower bounds to that in Theorem \ref{thm:query-lbs}. We also emphasize that the lower bound of Theorem \ref{thm:query-lbs} applies to \emph{any} algorithm, i.e., including those which require extremely large computation time. 










  \section{Proofs of lower bounds for \MarkCCE (\cref{sec:markov})}

\subsection{Preliminaries: Online density estimation}

\label{sec:online-density}
Our proof makes use of tools for online learning with the logarithmic
loss, also known as conditional density estimation. In particular, we use \dfedit{a variant of the exponential weights
  algorithm known as \emph{Vovk's aggregating algorithm} in the context
  of density estimation \cite{vovk1990aggregating,cesa2006prediction}}. We consider the following setting with two players, a \emph{Learner} and \emph{Nature}. Furthermore, there is a set $\MY$, called the \emph{outcome space}, and a set $\MX$, called the context space; for our applications it suffices to assume $\MY$ and $\MX$ are finite. For some $T \in \BN$, there are $T$ time steps $t = 1, 2, \ldots, T$. At  each time step $t \in [T]$:
\begin{itemize}
\item Nature reveals a context $x\^t \in \MX$; %
\item Having seen the context $x\^t$, the learner predicts a distribution $\wh q\^t \in \Delta(\MY)$;
\item Nature chooses an outcome $y\^t \in \MY$, and the learner suffers loss
$
\lgls\^t(\wh q\^t) := \log \left( \frac{1}{\wh q\^t(y\^t)} \right).
$
\end{itemize}
For each $t \in [T]$, we let $\MH\^t = \{ (x\^1, y\^1, \wh q\^1),
\ldots, (x\^t, y\^t, \wh q\^t) \}$ denote the history of interaction
up to step $t$; we emphasize that each context $x\^t$ may be chosen adaptively as a function of $\MH\^{t-1}$.   Let $\CF\^t$ denote the sigma-algebra generated by $(\MH\^t, x\^{t+1})$. 
We measure performance in terms of regret against a set $\MI$ of
\emph{experts}, also known as the \emph{expert setting}. Each expert $i \in \MI$ consists of a function $p_i : \MX \ra \Delta(\MY)$. The \emph{regret} of an algorithm against the expert class $\MI$ when it receives contexts $x\^1, \ldots ,x\^T$ and observes outcomes $y\^1, \ldots, y\^T$ is defined as
\begin{align}
\Reg_{\MI, T} = \sum_{t=1}^T \lgls\^t(\wh q\^t) - \min_{i \in \MI} \sum_{t=1}^T \lgls\^t(p_i(x\^t))\nonumber.
\end{align}
Note that the learner can observe the expert predictions
$\crl{p_i(x\ind{t})}_{i\in\cI}$ and use them to make its own prediction at each round $t$.
\begin{proposition}[Vovk's aggregating algorithm]
  \label{prop:vovk}
  Consider Vovk's aggregating algorithm, which predicts via
  \begin{align}
\wh q\^t(y) := \E_{i \sim \til q\^t}[p_i(x\^t)], \quad\text{where}\quad \til q\^t(i) \ldef \frac{\exp \left( -\sum_{s=1}^{t-1} \lgls\^s(p_i(x\^s))\right)}{\sum_{j \in \MI} \exp \left(-\sum_{s=1}^{t-1} \lgls\^s(p_j(x\^s))\right)}\label{eq:aggregation}.
  \end{align}
  This algorithm guarantees a regret bound of $\Reg_{\MI, T} \leq \log|\MI|$. 
\end{proposition}
Recall that for probability distributions $p,q$ on a finite set $\MB$, their total variation distance is defined as
\begin{align}
\tvd{p}{q} = \max_{\ME \subset \MB} |p(\ME) - q(\ME)|.
\end{align}
As a (standard) consequence of Proposition \ref{prop:vovk},
in the \emph{realizable} setting in which the distribution of $y\^t | x\^t$ follows
$p_{i^\st}(x\^t)$ for some fixed (unknown) expert $i^\st \in \MI$, we can obtain
a bound on the total variation distance between the algorithm's
predictions and those of $p_{i^\st}(x\^t)$.
\begin{proposition}
  \label{prop:online-tvd}
  If the distribution of outcomes is \emph{realizable}, i.e., there exists an expert $i^\st \in \MI$ so that $y\^t \sim p_{i^\st}(x\^t)  \ | \ x\^t, \MH\^{t-1}$ for all $t \in [T]$, then the predictions $\wh q\^t$ of the aggregation algorithm (\ref{eq:aggregation}) satisfy
  \begin{align}
\sum_{t=1}^T \E \left[ \tvd{\wh q\^t}{p_{i^\st}(x\^t)} \right] \leq \sqrt{T \log |\MI|}\nonumber.
  \end{align}
\end{proposition}

For completeness, we provide the proof of Proposition \ref{prop:online-tvd} here.
\begin{proof}[Proof of Proposition \ref{prop:online-tvd}]
To simplify notation, for an expert $i \in \MI$, a context $x \in \MX$, and an outcome $y \in \MY$, we write $p_i(y | x)$ to denote $p_i(x)(y)$. 
  
  Proposition \ref{prop:vovk} gives that the following inequality holds (almost surely):
  \begin{align}
\Reg_{\MI, T} = \sum_{t=1}^T \log \left( \frac{1}{\wh q\^t(y\^t)} \right) - \sum_{t=1}^T \log \left( \frac{1}{p_{i^\st}(y\^t  | x\^t)} \right) \leq \log |\MI|\nonumber.
  \end{align}
  For each $t \in [T]$, note that $\wh q\^t$ and $x\^t$ are  $\CF\^{t-1}$-measurable (by definition). Then %
  \begin{align}
    \sum_{t=1}^T   \tvd{\wh q\^t}{p_{i^\st}(x\^t)}^2 \leq & \sum_{t=1}^T   \kld{p_{i^\st}(x\^t)}{\wh q\^t}  \nonumber\\
    = & \sum_{t=1}^T  \sum_{y \in \cY} p_{i^\st}(y | x\^t) \cdot \log \left( \frac{p_{i^\st}(y | x\^t)}{\wh q\^t(y)} \right) \nonumber\\
    = &  \sum_{t=1}^T \E \left[ \log \left( \frac{1}{\wh q\^t(y\^t)} \right) - \log \left( \frac{1}{p_{i^\st}(y\^t | x\^t)}\right) \ | \ \CF\^{t-1} \right]\nonumber,
  \end{align}
  where the first inequality uses Pinsker's inequality and the final
  equality uses the fact that $y\^t \sim p_{i^\st}(x\^t) | x\^t,
  \MH\^{t-1}$. It follows that
  \[
    \En\brk*{\sum_{t=1}^T   \tvd{\wh q\^t}{p_{i^\st}(x\^t)}^2} \leq
    \E[\Reg_{\MI, T}] \leq\log\abs{\cI}.
    \]
    Jensen's inequality now gives that
  \begin{align}
    \E \left[ \sum_{t=1}^T \tvd{\wh q\^t}{p_{i^\st}(x\^t)} \right] \leq & \sqrt{T} \cdot \sqrt{ \E\left[\sum_{t=1}^T \tvd{\wh q\^t}{p_{i^\st}(x\^t)}^2 \right]}
    \leq  \sqrt{T \log |\MI|}\nonumber.
  \end{align}
\end{proof}

\subsection{Proof of Theorem \ref{thm:markov-formal}}
\label{sec:markov-proof}
\begin{proof}[Proof of Theorem \ref{thm:markov-formal}]
  Fix $n \in \BN$, which we recall represents an upper bound on the description length of the Markov game. 
  Assume that we are given an algorithm $\CB$ that solves the $(\Tn, \epn)$-\MarkCCE problem for Markov games $\MG$ satisfying $|\MG| \leq n$ in time $U$. %
  We proceed to describe an algorithm which solves the 2-player $(
  \lfloor n^{1/2}/2\rfloor, 4 \cdot \epn)$-\Nash problem in time
  $(n\Tn U)^{C_0}$, as long as $\Tn < \exp(\epn^2 \cdot
  n^{1/2}/2^{5})$. First, define $n_0 := \lfloor n^{1/2}/2\rfloor$,
  and consider an arbitrary 2-player $n_0$-action normal form $G$,
  which is specified by payoff matrices $M_1, M_2 \in [0,1]^{n_0
    \times n_0}$, so that all entries of the game can be written in
  binary using at most $n_0$ bits (recall, per footnote \ref{fn:nash-bits}, that we may assume that the entries of an instance of $(n_0, 4 \cdot \ep)$-\Nash can be specified with $n_0$ bits). Based on $G$, we construct a 2-player
  Markov game $\MG := \MG(G)$ as follows:
  \begin{defn}
    \label{def:mg-g}
    We define the game $\MG(G)$ to consist of the tuple $\MG(G) = (\MS, H, (\MA_i)_{i \in [2]}, \BP, (R_i)_{i \in [2]}, \mu)$, where:
  \begin{itemize}
\item The horizon of $\MG$ is $H = 2 \lfloor n_0/2 \rfloor$ (i.e., the largest even number at most $n_0$). %
\item Let $A = n_0$; the action spaces of the 2 agents are given by $\MA_1 = \MA_2 = [A]$.
\item There are a total of $A^2 + 1$ states: in particular, there is a state $\mf s_{(a_1, a_2)}$ for each $(a_1, a_2) \in [A]^2$, as well as a distinguished state $\mf s$, so we have:
  \begin{align}
\MS = \{ \mf s \} \cup \{ \mf s_{(a_1, a_2)} \ : \ (a_1, a_2) \in [A]^2 \}\nonumber.
  \end{align}
\item For all odd $h \in [H]$, the reward to agents $j \in [2]$ given that the action profile $(a_1, a_2)$ is played at step $h$ is given by $R_{j,h}(s, (a_1, a_2)) := \frac{1}{H} \cdot (M_j)_{a_1, a_2}$, for all $s \in \MS$. %
  All agents receive 0 reward at even steps $h \in [H]$.
\item At odd steps $h \in [H]$, if actions $a_1, a_2 \in [A]$ are taken, the game transitions to the state $\mf s_{ (a_1, a_2)}$. At even steps $h \in [H]$, the game always transitions to the state $\mf s$.
\item The initial state (i.e., at step $h=1$) is $\mf s$ (i.e., $\mu$
  is a singleton distribution supported on $\mf s$).
\end{itemize}
It is evident that this construction takes polynomial
time, and satisfies $|\MG| \leq A^2+1 \leq
n_0^2+1 \leq  n$. We will now show by applying the algorithm $\scrB$
to $\cG$, we can efficiently compute $4 \cdot \epn$-approximate Nash
equilibrium for the original game $G$. To do so, we appeal to \cref{alg:2nash}.

\end{defn}
        \begin{algorithm}[ht]
    \setstretch{1.3}
     \begin{algorithmic}[1]
       \State \textbf{Input:} 2-player, $n_0$-action normal form game $G$. 
       \State Construct the 2-player Markov game $\MG = \MG(G)$ per Definition \ref{def:mg-g}, which satisfies $|\MG| \leq n$.
       \State Call the algorithm $\CB$ on the game $\MG$, which produces a sequence $\sigma\^1, \ldots, \sigma\^T$, where each $\sigma\^t \in \PiMarkov$.
       \For{$t \in [T]$ and odd $h \in [H]$:}
       \If{$\sigma\^t_h(\mf s) \in \Delta(\MA_1) \times \Delta(\MA_2)$ is a $(4 \cdot \ep,n)$-Nash equilibrium of $G$:}
       \Return $\sigma\^t_h(\mf s)$.
       \EndIf
       \EndFor
       \State \textbf{if} the for loop terminates without returning:
       return \textbf{fail}. 
     \end{algorithmic}
     \caption{Algorithm to compute Nash equilibrium used in proof of Theorem \ref{thm:markov-formal}.}
     \label{alg:2nash}
     \end{algorithm}

Algorithm \ref{alg:2nash} proceeds as follows. First, it constructs
the 2-player Markov game $\MG(G)$ as defined above, and calls the
algorithm $\CB$, which returns a sequence $\sigma\^1, \ldots,
\sigma\^T \in \PiMarkov$ of product Markov policies with the property
that the average $\ol \sigma := \frac{1}{T} \sum_{t=1}^T
\indic{\sigma\^t}$ is an $\epn$-CCE of $\MG$. It then enumerates over
the distributions $\sigma\^t_h(\mf s) \in \Delta(\MA_1) \times
\Delta(\MA_2)$ for each $t \in [T]$ and $h \in [H]$ odd,
and checks whether each one is a $4 \cdot \epn$-approximate Nash
equilibrium of $G$. If so, the algorithm outputs such a Nash
equilibrium, and otherwise, it fails. The proof of Theorem
\ref{thm:markov-formal} is thus completed by the following lemma, which states that as long as $\ol \sigma$ is an $\epn$-CCE of $\MG$, Algorithm \ref{alg:2nash} never fails. %
\begin{lemma}[Correctness of Algorithm \ref{alg:2nash}]
  \label{lem:2nash-main}
Consider the normal form game $G$ and the Markov game $\MG = \MG(G)$ as constructed above, which has horizon $H$. For any $\ep_0 > 0$, $T \in \BN$, if $T < \exp(H \cdot \ep_0^2/ 2^8)$ and $\sigma\^1, \ldots, \sigma\^T \in \PiMarkov$ are product Markov policies so that $\frac{1}{T} \sum_{t=1}^T \indic{\sigma\^t}$ is an $(\ep_0/4)$-CCE of $\MG$, then there is some odd $h \in [H]$ and $t \in [T]$ so that $\sigma_h\^t(\mf s)$ is an $\ep_0$-Nash equilibrium of $G$. 
\end{lemma}
The proof of Lemma \ref{lem:2nash-main} is given below. Applying Lemma \ref{lem:2nash-main} with $\ep_0 = 4 \epn$ (which is a valid application since $T < \exp(n_0 \cdot (4\epn)^2 / 2^{8})$ by our assumption on $\Tn, \epn$), yields that Algorithm \ref{alg:2nash} always finds a $4\epn$-Nash equilibrium of the $n_0$-action normal form game $G$, thus solving the given instance of the $(n_0, 4\cdot \epn)$-\Nash problem. Furthermore, it is straightforward to see that Algorithm \ref{alg:2nash} runs in time $U + (nT)^{C_0} \leq  (UnT)^{C_0}$, for some constant $C_0 \geq 1$.

    
   \end{proof}

   \begin{proof}[Proof of Lemma \ref{lem:2nash-main}]
  Consider a sequence of product Markov policies $\sigma\^1, \ldots,
  \sigma\^T$ with the property that the average $\ol \sigma =
  \frac{1}{T} \sum_{t=1}^T\indic{ \sigma\^T}$ is an $(\ep_0/4)$-CCE of
  $\MG$. For all odd $h \in [H]$ and $j \in [2]$, let $p\^t_{j,h} :=
  \sigma\^t_{j,h}(\mf s) \in \Delta(\MA_j)$, which is the distribution
  played under $\sigma\^t$ by player $j$ at step $h$ (at the unique
  state $\mf s$ with positive probability of being reached at step
  $h$). For odd $h$, we have $\sigma_h\^t(\mf s) = p_{1,h}\^t \times
  p_{2,h}\^t$, and our goal is to show that for some odd $h \in [H]$
  and $t \in [T]$, $p_{1,h}\^t \times p_{2,h}\^t$ is an $\ep_0$-Nash
  equilibrium of $G$. To proceed, suppose for the sake of contradiction that this is not the case.
  
Let  us write $\MO_H := \{ h \in [H] : h\ \rm{ odd} \}$ to denote the
set of odd-numbered steps, and $\ME_H = [H] \backslash \MO_H$ to
denote the set of even-numbered steps. Let $H_0 = |\MO_H| = |\ME_H| =
H/2$. We first note that for $j \in [2]$, agent $j$'s value under the mixture policy $\ol \sigma$ is given as follows:
\begin{align}
V_{j}^{\ol \sigma} = \frac{1}{TH} \sum_{t=1}^T \sum_{h \in \MO_H} \E_{a_1 \sim p_{1,h}\^t, a_2 \sim p_{2,h}\^t} \left[ (M_j)_{a_1, a_2} \right]\nonumber.
\end{align}
 For each $j \in [2]$, we will derive a contradiction by constructing
 a (non-Markov) deviation policy for player $j$ in $\MG$, denoted
 $\pi_j^\dagger \in \Pidet_j$, which will give player $j$ a
 significant gain in value against the policy $\ol \sigma$. To do so,
 we need to specify $\pi_{j,h}^\dagger(\tau_{j,h-1}, s_h) \in \MA_j$,
 for all $\tau_{j,h-1} \in \CH_{j,h-1}$ and $s_h \in \MS$; note that
 we may restrict our attention only to histories $\tau_{j,h_0-1}$ that occur with positive probability under the transitions of $\MG$.

Fix any $h_0 \in [H]$, $\tau_{j,h_0-1} \in \CH_{j,h_0-1}$, and
$s_{h_0} \in \MS$. If $\tau_{j,h_0-1}$ occurs with positive
probability under the transitions of $\MG$, then for each $h \in
\MO_H$, $h < h_0-1$ and both $j' \in [2]$, the action played by agent $j'$
at step $h$ is determined by $\tau_{j,h}$. Namely, if the state at
step $h+1$ of $\tau_{j,h_0-1}$ is $\mf s_{ (a_1', a_2')}$, then player
$j'$ played action $a_{j}'$ at step $h$. So, for each $h \in
\MO_H$ with $h < h_0-1$, we may define $(a_{1,h}, a_{2,h})$ as the
action profile played at step $h$, which is a measurable function of $\tau_{j,h_0-1}$. 
  With this in mind, we define $\pi_{j,h_0}^\dagger(\tau_{j,h_0-1},
  s_{h_0})$ by applying Vovk's aggregating algorithm (Proposition
  \ref{prop:online-tvd}) as follows.
\begin{enumerate}
\item If $h_0$ is even, play an arbitrary action (note that the actions at even-numbered steps have no influence on the transitions or rewards).
\item If $h_0$ is odd, define $\wh q_{j,h_0} \in \Delta(\MA_j)$, by $\wh q_{j,h_0} := \E_{t \sim \til q_{j,h_0}} [ p_{-j, h}\^t]$, where $\til q_{j,h_0} \in \Delta([T])$ is defined as follows: for $t \in [T]$,
  \begin{align}
\til q_{j,h_0}(t) := \frac{\exp \left( - \sum_{h < h_0: \ h \in \MO_H} \log \left( \frac{1}{p\^t_{-j, h}(a_{-j,h})} \right)\right)}{\sum_{t'=1}^T\exp \left( - \sum_{h < h_0:\ h \in \MO_H} \log \left( \frac{1}{p\^{t'}_{-j, h}(a_{-j,h})} \right)\right)}\nonumber.
  \end{align}
  Note that $\wh q_{j,h_0}$ is a function of $\tau_{j,h_0-1}$ via the
  action profiles $\crl*{(a_{1,h}, a_{2,h})}_{h<h_0:h\in\cO_H}$; to
  simplify notation, we suppress this
  dependence. %
  \item Then for any state $s_{h_0} \in \MS$, define $\pi_{j,h_0}^\dagger(\tau_{j,h_0-1}, s_{h_0})$ to be  a best response to $\wh q_{j,h_0}$, namely 
    \begin{align}
      \pi_{j,h_0}^\dagger(\tau_{j,h_0-1}, s_{h_0}) := \argmax_{a_j \in \MA_j} \E_{a_{-j} \sim \wh q_{j,h_0}} \left[ R_{j,h}(\mf s_{h_0}, (a_1, a_2)) \right]=\argmax_{a_j \in \MA_j} \E_{a_{-j} \sim \wh q_{j,h_0}} \left[ (M_j)_{a_1, a_2} \right].\label{eq:define-br-ol}
    \end{align}
  
\end{enumerate}
Note that, for odd $h_0$, the distribution $\wh q_{j,h_0} \in
\Delta(\MA_j)$ defined above can be viewed as an application of Vovk's online aggregation algorithm at step $(h_0+1)/2$ in the following setting:
the number of steps ($T$, in the notation of Proposition \ref{prop:online-tvd};
note that $T$ plays a different role in the present proof) is
$H_0=H/2$, the context space is $\MO_H$, and the outcome space is
$\MA_{-j}$.\footnote{Here $-j$ denotes the index of the player who is
  not $j$.} There are $T$ experts $\til p\^1, \ldots, \til p\^T$
(i.e., we have $\cI=\crl*{\til p\ind{t}}_{t\in\brk{T}}$), whose predictions on a context $h \in \MO_H$ are defined as follows: the expert $\til p\^t$ predicts $\til p\^t(h) := p_{-j,h}\^t$.  %
Then, the distribution $\wh q_{j,h_0}$ is obtained by updating the aggregation algorithm with the context-observation pairs $(h, a_{-j, h})$, for \emph{odd} values of $h < h_0$. %


We next analyze the value of $V_{j}^{\pi_j^\dagger, \ol \sigma_{-j}}$
for $j \in [2]$ to show that the deviation strategy we have defined indeed obtains
significant gain. To do so, recall that this value represents the
payoff for player $j$ under the process in which we draw an index
$\tstar\in\brk*{T}$ uniformly at random, then for each step
$h\in\brk{H}$, player $j$ plays according to $\pi_j^\dagger$ and player $-j$ plays according to $\sigma_{-j}\^{t^\st}$. (In particular, at odd-numbered steps, player $-j$ plays according to $p_{-j,h}\^{t^\st}$.) %
We recall that $\E_{\pi_j^\dagger \times \ol
  \sigma_{-j}} \brk*{\cdot}$ denotes the expectation under this process. We let $\tau_{j,h-1} \in \CH_{j,h-1}$ denote the random variable which
is the history observed by player $j$ in this setup, i.e., when the
policy played is $\pi_j^\dagger \times \ol \sigma_{-j}$, and let
$\crl*{(a_{1,h}, a_{2,h})}_{h\in\cO_H}$ denote the action
profiles for odd rounds, which are a measurable function of each
player's trajectory. 

We apply Proposition \ref{prop:online-tvd} with the time horizon as
$H_0$, and with the set of experts set to $\cI\ldef{}\{ \til p\^1,
\ldots, \til p\^T \}$ as defined above. The context sequence the
sequence of increasing values of $h \in \MO_H$, and for each $h \in
\MO_H$, the outcome at step $(h+1)/2$ (for which the context is $h$)
is distributed as $a_{-j,h} \sim \til p\^{t^\st}(h) = p\^{t^\st}_{-j,
  h}$ conditioned on $\tstar$, which in particular satisfies the realizability assumption stated in Proposition \ref{prop:online-tvd}.  %
Then, since (as remarked above), the distributions $\wh q_{j,h}$, for
$h \in \MO_H$, are exactly the predictions made by Vovk's aggregating
algorithm, Proposition \ref{prop:online-tvd} gives that\footnote{In
  fact, Proposition \ref{prop:online-tvd} implies that a similar bound holds uniformly
  for each possible realization of $\tstar$, but
  \cref{eq:sum-tv-bound} suffices for our purposes.}
\begin{align}
\E_{\pi_j^\dagger \times \ol \sigma_{-j}} \left[ \sum_{h \in \MO_H}\tvd{\wh q_{j,h}}{p_{-j,h}\^{t^\st}} \right] =\E_{\pi_j^\dagger \times \ol \sigma_{-j}} \left[ \sum_{h \in \MO_H}\tvd{\wh q_{j,h}}{\til p\^{t^\st}(h)} \right] \leq \sqrt{H_0 \log T}\label{eq:sum-tv-bound}.
\end{align}
Recall that we have assumed for the sake of contradiction that
$p_{1,h}\^{t}\times p_{2,h}\^{t}$ is not an $\ep_0$-Nash equilibrium
of $G$ for each $h \in [H]$ and $t\in\brk{T}$. Consider a fixed draw
of the random variable $\tstar\in\brk{T}$ defined above. Then it holds
that for $j \in [2]$ and $h \in [H]$, defining
\begin{align}
\ep_{0,j,h} := \max_{a_j \in [A]} \E_{a_{-j} \sim p_{-j,h}\^{t^\st}} \left[ (M_j)_{a_1, a_2} \right] - \E_{a_1 \sim p_{1,h}\^{t^\st}, a_2 \sim p_{2,h}\^{t^\st}} \left[ (M_j)_{a_1, a_2} \right],\label{eq:disp0}
\end{align}
we have $\ep_{0,1,h} + \ep_{0,2,h} \geq \ep_0$. 
Consider any $j \in [2]$, $h \in \MO_H$, and a history $\tau_{j,h-1}
\in \CH_{j,h-1}$ of agent $j$ up to step $h-1$ (conditioned on
$\tstar$). Let us write $\delta_{-j,h}\^{t^\st} :=
\tvd{p_{-j,h}\^{t^\st}}{\wh q_{j,h}}$; note that
$\delta_{-j,h}\^{t^\st}$ is a function of $\tau_{j,h-1}$, through its
dependence on $\wh q_{j,h}$. We have, by the definition of
$\pi_{j,h}^\dagger(\tau_{j,h-1}, s_h)$ in (\ref{eq:define-br-ol}) and
the definition of $\delta_{-j,h}\^{t^\st}$,
\begin{align}
  \E_{a_{-j} \sim p_{-j,h}\^{t^\st}} \left[ (M_j)_{\pi_{h,j}^\dagger(\tau_{j,h-1}, \mf s), a_{-j}}\ | \ t^\st,\ \tau_{j,h-1} \right] \geq & \E_{a_{-j} \sim \wh q_{j,h}} \left[ (M_j)_{\pi_{h,j}^\dagger(\tau_{j,h-1}, \mf s), a_{-j}}\ | \ t^\st,\ \tau_{j,h-1} \right] - \delta_{-j,h}\^{t^\st}\nonumber\\
  = & \max_{a_j \in [A]} \E_{a_{-j} \sim \wh q_{j,h}} \left[ (M_j)_{a_j, a_{-j}} \ | \ t^\st,\ \tau_{j,h-1} \right] - \delta_{h,-j}\^{t^\st}\nonumber\\
  \geq & \max_{a_j \in [A]} \E_{a_{-j} \sim p_{h,-j}\^{t^\st}} \left[ (M_j)_{a_j, a_{-j}} \right] - 2 \delta_{-j,h}\^{t^\st}\label{eq:disp1}.
\end{align}
Combining \cref{eq:disp0} and \cref{eq:disp1}, we get that for any fixed $h \in \MO_H$, $j \in [2]$, and $\tau_{j,h-1} \in \CH_{j,h-1}$,
\begin{align}
\E_{a_{-j} \sim p_{-j,h}\^{t^\st}} \left[ (M_j)_{\pi_{j,h}^\dagger(\tau_{j,h-1}, \mf s), a_{-j}}\ | \ t^\st,\ \tau_{j,h-1} \right] - \E_{a_1 \sim p_{1,h}\^{t^\st}, a_2 \sim p_{2,h}\^{t^\st}} \left[ (M_j)_{a_1, a_2} \right]  > \ep_{0,j,h} - 2 \delta_{-j,h}\^{t^\st}\label{eq:delta-deviation}.
\end{align}
Averaging over the draw of $t^\st \in [T]$, which we recall is chosen uniformly, we see that %
\begin{align}
  & \sum_{j \in [2]} V_{j}^{\pi_j^\dagger \times \ol \sigma_{-j}} - V_{j}^{\ol \sigma} \nonumber\\
  = &  \frac{1}{T} \sum_{t=1}^T \sum_{j \in [2]} V_{j}^{\pi_j^\dagger\times \sigma\^t_{-j}} - V_{j}^{\sigma\^t}\label{eq:split-into-k}\\
  = & \frac{1}{T} \sum_{t=1}^T \sum_{j \in [2]} \E_{\pi_j^\dagger \times \sigma_{-j}\^t} \left[ \sum_{h \in \MO_H} \E_{a_{-j} \sim p_{-j,h}\^t}[R_{j,h}(\mf s, (\pi_{j,h}^\dagger(\tau_{j,h-1}, \mf s), a_{-j})) \ | \ t,\ \tau_{j,h-1}] - \E_{a_1 \sim p_{1,h}\^t, a_2 \sim p_{2,h}\^t} [ R_{j,h}(\mf s, (a_1, a_2)) ]\right]\nonumber\\
  = & \frac{1}{TH} \sum_{t=1}^T \sum_{j \in [2]} \E_{\pi_j^\dagger \times \sigma_{-j}\^t} \left[ \sum_{h \in \MO_H} \E_{a_{-j} \sim p_{-j,h}\^t} [ (M_j)_{\pi_{j,h}^\dagger(\tau_{j,h-1}, \mf s), a_{-j}} \ | \ t, \ \tau_{j,h-1}] - \E_{a_1 \sim p_{1,h}\^t, a_2 \sim p_{2,h}\^t} [ (M_j)_{a_1, a_2} ]\right]\nonumber\\
  \geq & \frac{1}{TH} \sum_{t=1}^T \sum_{j \in [2]} \E_{\pi_j^\dagger \times \sigma_{-j}\^t} \left[ \sum_{h \in \MO_H} \left( \ep_{0,j,h} - 2 \delta_{-j,h}\^{t}  \right)\right]\label{eq:use-deviation}\\
  \geq & \frac{\ep_0}{2} - \frac{2}{TH} \sum_{t=1}^T 2 \sqrt{H_0 \log T} \geq \frac{\ep_0}{2} - 4 \sqrt{ \log (T)/H}\label{eq:use-phk-tvd},
\end{align}
where (\ref{eq:split-into-k}) follows from the definition $\ol \sigma
= \frac{1}{T} \sum_{t=1}^T \indic{\sigma\^t}$,
(\ref{eq:use-deviation}) follows from (\ref{eq:delta-deviation}), and
(\ref{eq:use-phk-tvd}) uses (\ref{eq:sum-tv-bound}). As long as $T <
\exp(H \cdot(\ep_0/16)^2)$, the this expression is bounded below by
$\ep_0/4$, meaning that $\ol\sigma$ is not an $\ep_0/4$-approximate
CCE. This completes the contradiction.
   \end{proof}

   \section{Proofs of lower bounds for \GenCCE (\cref{sec:nonmarkov,sec:multiplayer})}
   \label{sec:nonmarkov-proof}

   In this section we prove our computational lower bounds for solving the \GenCCE problem with $m = 3$ players (Theorem \ref{thm:nonmarkov-formal} and Corollary \ref{cor:nonmarkov-formal-ppad}), as well as our statistical lower bound for solving the \GenCCE problem with a general number $m$ of players (Theorem \ref{thm:statistical-lb}).

Both theorems are proven as consequences of a more general result
given in Theorem \ref{thm:nonmarkov-multiplayer} below, which reduces the \Nash problem in $m$-player normal-form games to the \GenCCE problem in $(m+1)$-player Markov games. %
In more detail, the theorem shows that (a) if an algorithm for \GenCCE makes few calls to a generative model oracle, then we get an algorithm for the \Nash problem with few calls to a payoff oracle (see Section \ref{sec:query-complexity-proof} for background on the payoff oracle for the \Nash problem), and (b) if the algorithm for \GenCCE is \emph{computationally} efficient, then so is the algorithm for the \Nash problem.
\begin{theorem}
  \label{thm:nonmarkov-multiplayer}
  There is a constant $C_0 > 0$ so that the following holds. 
  Consider $n,m \in \BN$, and 
suppose $\Tnm, \Nnm, \Qnm \in \BN$ and $\epnm > 0$ satisfy $1 < \Tnm < \exp \left( \frac{\epnm^2 \cdot \lfloor n/m \rfloor}{m^2} \right)$.
  Suppose there is an algorithm $\mathscr{B}$ which, given a generative model oracle for a $(m+1)$-player Markov game $\MG$ with $|\MG| \leq n$, solves the $(\Tnm, \epnm, \Nnm)$-\GenCCE problem for $\MG$ using $\Qnm$ generative model oracle queries. Then the following conclusions hold:
  \begin{itemize}
  \item   For any $\delta > 0$, the $m$-player $(\lfloor n/m \rfloor, 16(m+1) \cdot \epnm )$-\Nash problem for any normal-form game $G$ can be solved, with failure probability $\delta$, using at most $C_0 \cdot (\Qnm \cdot \log(1/\delta)) + (\log(1/\delta) \cdot nm/\epnm)^{C_0}$ queries to a payoff oracle $\MO_G$ for $G$. 
  \item If the algorithm $\mathscr{B}$ additionally runs in time $U$ for some $U \in \BN$, then the algorithm solving \Nash from the previous bullet point runs in time $(nm\Tnm \Nnm U \log(1/\delta) / \epnm)^{C_0}$. %
  \end{itemize}
\end{theorem}
Theorem \ref{thm:nonmarkov-formal} follows directly from Theorem \ref{thm:nonmarkov-multiplayer} by taking $m=2$.
\begin{proof}[Proof of Theorem \ref{thm:nonmarkov-formal}]
  Suppose there is an algorithm which, given the description of any 3-player Markov game $\MG$ with $|\MG| \leq n$, solves the $(\Tnm, \epnm, \Nnm)$-\GenCCE problem in time $U$. Such an algorithm immediately yields an algorithm which can solve the $(\Tnm, \epnm, \Nnm)$-\GenCCE problem in time $U + |\MG|^{O(1)}$ using only a generative model oracle, since the exact description of the Markov game can be obtained with $HS |\MA| \leq HS (\max_i A_i)^3 \leq |\MG|^5$ queries to the generative model (across all $(h,s,\ba)$ tuples). 
  We can now solve the problem of computing a $50\cdot \epnm$-Nash equilibrium of a given 2-player $\lfloor n/2 \rfloor$-action normal form game $G$ as follows. %
  We simply apply the algorithm of Theorem \ref{thm:nonmarkov-multiplayer} with $m=2$, noting that the oracle $\MO_G$ in the theorem statement can be implemented by reading the corresponding bits of input of the input game $G$. The second bullet point yields that this algorithm takes time $(nTNU\log(1/\delta)/\ep)^{C_0}$, for some constant $C_0$. Furthermore, the assumption $T < \exp(\ep^2 \cdot \lfloor n/m \rfloor / m^2)$ of Theorem \ref{thm:nonmarkov-multiplayer} is implied by the assumption that $T < \exp(\ep^2 n / 16)$ of Theorem \ref{thm:nonmarkov-formal}. 
\end{proof}
In a similar manner, Theorem \ref{thm:statistical-lb} follows from Theorem \ref{thm:nonmarkov-multiplayer} by applying Theorem \ref{thm:query-lbs}, which states that there is no randomized algorithm that finds approximate Nash equilibria of $m$-player, 2-action normal form games in time $2^{o(m)}$.
\begin{proof}[Proof of Theorem \ref{thm:statistical-lb}]
    Let $\ep_0$ be the constant from Theorem \ref{thm:query-lbs}, and consider any $m \geq 3$. 
    Suppose there is an algorithm which, for any $m$-player Markov
    game $\MG$ with $|\MG| \leq 2m^6$, makes $Q$ oracle queries to a
    generative model oracle for $\MG$, and solves the $(T,\ep_0/(10m),
    N)$-\GenCCE problem for $\MG$ for some $T, N \in \BN$ so that $T <
    \exp(cm)$, for a sufficiently small absolute constant $c$. %
    Then, by Theorem \ref{thm:nonmarkov-multiplayer} with $\ep = \ep_0/(10m)$ and $n = m^6$ (which ensures that $\Tnm < \exp((\ep_0/(10m))^2 \cdot \lfloor n/m \rfloor / m^2)$ as long as $c$ is sufficiently small), there is an algorithm which solves the $(m^5, \ep_0)$-\Nash problem---and thus the $(2, \ep_0)$-\Nash problem---for $(m-1)$-player games with failure probability $1/3$, using $O(\Qnm) + m^{O(1)}$ queries to a payoff oracle. But by Theorem \ref{thm:query-lbs}, any such algorithm requires $2^{\Omega(m)}$ queries to a payoff oracle. It follows that $Q \geq 2^{\Omega(m)}$, as desired. 
\end{proof}

\subsection{Proof of Theorem \ref{thm:nonmarkov-multiplayer}}

\begin{proof}[Proof of Theorem \ref{thm:nonmarkov-multiplayer}]
  Fix any $m \geq 2$, $n \in \BN$.  Suppose we are given an algorithm
  $\CB$ that solves the $(m+1)$-player $(\Tnm, \epnm, \Nnm)$-\GenCCE
  problem for Markov games $\MG$ satisfying $|\MG| \leq n$, running in
  time $U$ and using at most $Q$ generative model queries.
  We proceed to describe an algorithm which solves the $m$-player
  $(\lfloor n/m \rfloor, 16(m+1)\cdot \epnm)$-\Nash problem using $C_0
  \cdot (\Qnm \cdot \log(1/\delta)) + (\log(1/\delta) \cdot
  nm/\epnm)^{C_0}$ queries to a payoff oracle, and
  running in time $(nm\Tnm \Nnm U \log(1/\delta) / \epnm)^{C_0}$,
  where $\delta$ represents the failure probability. Define $n_0 :=
  \lfloor n/m \rfloor$, and assume we are given an arbitrary $m$-player $n_0$-action normal form $G$, which is specified by payoff matrices $M_1, \ldots, M_m \in [0,1]^{n_0 \times \cdots \times n_0}$. %
  We assume that all entries of each of the matrices $M_j$ have only
  the most significant $\max\{ n_0, \lceil \log 1/\epnm \rceil\}$ bits
  nonzero; this assumption is without loss of generality, since by
  truncating the utilities to satisfy this assumption, we change all
  payoffs by at most $\epnm$, which degrades the quality of any
  approximate equilibrium by at most $2\epnm$ (in addition, we have $\lceil \log 1/\ep \rceil \leq n_0$ since we have assumed $1 < T < \exp(\ep^2 n_0/m^2)$).
  We assume $\ep \leq 1/2$ without loss of generality. %
  Based on $G$, we construct an $(m+1)$-player Markov game $\MG := \MG(G)$ as follows.
\begin{defn}
  \label{def:mg-g-multiplayers}
      We define the Markov game $\MG(G)$ as the tuple $\MG(G) = (\MS, H, (\MA_i)_{i \in [2]}, \BP, (R_i)_{i \in [2]}, \mu)$, where:
\begin{itemize}
\item The horizon of $\MG$ is chosen to be the power of $2$ satisfying
   $n_0 \leq H < 2n_0$.%
\item Let $A \ldef n_0$. The action spaces of agents $1, 2, \ldots, m$ are given by $\MA_1 = \cdots = \MA_m = [A]$. The action space of agent $m+1$ is %
  \begin{align}
\MA_{m+1} = \{ (j, a_j) \ : \ j \in [m], a_j\in \MA_j \}\nonumber,
  \end{align}
  so that $|\MA_{m+1}| = Am \leq n$. 

 We write $\MA = \prod_{j=1}^m \MA_j$ to denote the joint action space of the first $m$ agents, and $\ol \MA  := \prod_{j=1}^{m+1} \MA_j$ to denote the joint action space of all agents. %

\item There is a single state, denoted by $\mf s$, i.e., $\MS = \{ \mf
  s \}$ (in particular, $\mu$ is a singleton distribution supported on
  $\mf s$).

\item For all $h \in [H]$, the reward for agent $j \in [m+1]$, given
  an action profile $\ba = (a_1, \ldots, a_{m+1})$ at the unique state
  $\mf s$, is as follows: writing $a_{m+1} = (j', a_{j'}')$, we have%
  \begin{align}
R_{j,h}(\mf s, \ba) =  \ol R_{j,h}(\mf s, \ba) + \frac{1}{H} \cdot 2^{-3 \lceil \log 1/\ep \rceil} \cdot \enc(\ba)\label{eq:rewards-multiplayers-true},
  \end{align}
  where $\ol R_{j,h}(\mf s, \ba)$ is defined per the kibitzer construction of \cite{borgs2008myth}:
\begin{align}
   \ol R_{j,h}(\mf s, \ba) := \begin{cases}
    0 &: j \not \in \{j', m+1\} \\
  \frac{1}{H} \cdot \left((M_j)_{a_1, \ldots, a_m} - (M_j)_{a_1, \ldots, a_{j'}', \ldots, a_m}\right) &: j=j' \\
  \frac{1}{H} \cdot \left( (M_j)_{a_1, \ldots, a_{j'}', \ldots, a_m} - (M_j)_{a_1, \ldots, a_m} \right) &: j=m+1.
  \end{cases}\label{eq:rewards-multiplayers}
\end{align}
In (\ref{eq:rewards-multiplayers-true}) above,  $\enc(\ba) \in [0,1]$ is the binary representation of a binary
encoding of the action profile $\ba$. In particular, if the binary encoding of $\ba$ is $(b_1, \ldots, b_N)$, with $b_i \in \{0,1\}$, then $\enc(\ba) = \sum_{i=1}^N 2^{-i} \cdot b_i$. Note that $\enc(\ba)$ takes $N = O(m
\log n_0) \leq O(m \log n)$ bits to specify.
\end{itemize}
\end{defn}

      \begin{algorithm}[H]
    \setstretch{1.3}
     \begin{algorithmic}[1]
       \State \textbf{Input:}
       \Statex \quad Parameters $n,n_0, m, T \in \BN$, $\delta = \ep / (6H)$, $K = \lceil 4  \log(mn_0/\delta) / \ep^2 \rceil$. 
       \Statex \quad An $m$-player, $n_0$-action normal form game $G$, with utilies accessible by oracle $\MO_G$. 
       \Statex \quad An algorithm $\CB$ for computing approximate CCE of Markov games.
       \State \label{line:call-b}Call the algorithm $\CB$ on the
       $(m+1)$-player Markov game $\MG = \MG(G)$ constructed as in Definition \ref{def:mg-g-multiplayers}, which produces a sequence $\sigma\^1, \ldots, \sigma\^T$, where each $ \sigma\^t = ( \sigma\^t_1, \ldots,  \sigma\^t_{m+1})$ with $ \sigma\^t_j \in \Pirndrnd_j$. Here, we use the oracle $\MO_G$ to simulate generative model oracle queries made by $\CB$.  
       \State Draw $t^\st \in [T]$ uniformly at random.
       \State For each $j \in [m]$, initialize $\tau_{j,0}$ to be an empty trajectory. 
       \For{$h \in [H]$:} \arxiv{\Comment{\emph{Simulate a trajectory from $\MG$}}}
       \State Set $s_h = \mf s$ (per the transitions of $\MG$). %
       \State \label{line:compute-hatq-multiplayer}\multiline{ For each $j \in [m]$, define $\wh q_{j,h} := \E_{t \sim \til q_{j,h}} \left[ \sigma_{j,h}\^t(\tau_{j,h-1}, s_h) \right] \in \Delta(\MA_j)$, where $\til q_{j,h} \in \Delta([T])$ is defined as follows: for $t \in [T]$,}
       \begin{align}
\til q_{j,h}(t) :=  \frac{\exp \left( - \sum_{g < h} \log \left( \frac{1}{\sigma\^t_{j, g}(a_{j,g} | \tau_{j,g-1},  s_g)} \right)\right)}{\sum_{t'=1}^T\exp \left( - \sum_{g < h} \log \left( \frac{1}{\sigma\^{t'}_{j, g}(a_{j,g} | \tau_{j,g-1},  s_g)} \right)\right)}\nonumber.
       \end{align}
       \vspace{-5pt}
       \State Draw $K$ i.i.d.~samples $\ba_h^1, \ldots, \ba_h^K \sim \bigtimes_{j \in [m]} \wh q_{j,h}$. \label{line:draw-samples}
       \State \multiline{For each $a' \in \MA_{m+1}$, define $\wh R_{m+1,h}(a'):= \frac{1}{K} \sum_{k=1}^K R_{m+1,h}(s_h, (\ba_h^k, a'))$. Here, we use the oracle $\MO_G$ to compute $R_{m+1,h}(s_h, (\ba_h^k, a'))$ for each tuple $(\ba_h^k, a')$. \label{line:mp1-br}}



       \State For each $j \in [m]$, draw $a_{j,h} \sim  \sigma_{j,h}\^{t^\st}(\cdot  | \tau_{j,h-1}, s_h)$.\label{line:draw-ajh}
       \State \label{line:mp1-br} \multiline{Choose the action $a_{m+1,h}$ of player $m+1$ as follows: \emph{(Action $a_{m+1,h}$ is corresponds to the action selected by the policy $\pi_{m+1}^\dagger$ of player $m+1$ defined within the proof of Lemma \ref{lem:algorithmic-lemma}; this policy is well-defined because the action profiles of all players $i\in\brk{m}$ can be extracted from the lower-order bits of player $m+1$'s reward)}}
       \begin{align}
         a_{m+1,h} := \argmax_{a' \in \MA_{m+1}}\left\{\wh
         R_{m+1,h}(a') \right\} \label{eq:choose-mp1-action}.
       \end{align}         \vspace{-10pt}
       \State \multiline{ For each $j \in [m+1]$, let $r_{j,h} = R_{j,h}(s_h, (a_{1,h}, \ldots, a_{m+1,h}))$. \label{line:reward-simulate}} 
       \State \multiline{ Each player $j$ constructs $\tau_{j,h}$ by updating $\tau_{j,h-1}$ with $(s_h, a_{j,h}, r_{j,h})$.}
\If{$\wh R_{m+1,h}(a_{m+1,h}) \leq 14(m+1) \cdot \ep/H$}\label{line:check-nash}
\Return $\wh q_h := \bigtimes_{j \in [m]} \wh q_{j,h}$ as a candidate approximate Nash equilibrium for $G$.
       \EndIf       
       \EndFor
       \State \textbf{if} the for loop terminates without returning: return \textbf{fail}. 
     \end{algorithmic}
     \caption{Algorithm to compute Nash equilibrium used in proof of Theorem \ref{thm:nonmarkov-multiplayer}.}
     \label{alg:3nash}
   \end{algorithm}

   It is evident that this construction takes
   polynomial time and satisfies $|\MG| \leq mn_0
   \leq n$. Furthermore, it is clear that a single generative model
   oracle call for the Markov game $\MG$ (per Definition
   \ref{def:generative-model}) can be implemented using at most 2
   calls to the oracle $\MO_G$ for the normal-form game $G$. %
   We will now show by applying the algorithm $\scrB$
to $\cG$, we can efficiently (in terms of runtime and oracle calls) compute a $16(m+1)\cdot \epn$-approximate Nash
equilibrium for the original game $G$. To do so, we appeal to \cref{alg:3nash}.

   
   Algorithm \ref{alg:3nash} proceeds as follows. First, it calls the
   algorithm $\CB$ on the $(m+1)$-player Markov game $\MG(G)$, using the oracle $\MO_G$ to simulate $\CB$'s calls to the generative model oracle for $\MG$. 
By assumption, the algorithm $\CB$ returns a sequence $\sigma\^1, \ldots, \sigma\^T$ of product policies of the form $ \sigma\^t = ( \sigma\^t_1, \ldots,  \sigma\^t_{m+1})$, so that each $ \sigma\^t_j \in \Pirndrnd_j$ is $N$-computable, and so that the average $\ol \sigma := \frac{1}{T} \sum_{t=1}^T \indic{\sigma\^t}$ is an $\ep$-CCE of $\MG$.
   Next, Algorithm \ref{alg:3nash} samples a trajectory from $\MG$ in
   which:
   \begin{itemize}
   \item Players $1, \ldots, m$ each play according to a policy
     $ \sigma\^{t^\st}$ for an index $t^\st \in [T]$ chosen uniformly
     at the start of the episode.
   \item Player $m+1$ plays
     according to a strategy that, at each step $h \in [H]$, computes
     distributions $\wh q_{j,h}$ representing its ``belief'' of what action
     each player $j \in [m]$ will play at step $h$ (Line
     \ref{line:compute-hatq-multiplayer}), and plays an approximate
     best response to the product of the strategies $\wh q_{j,h}$,
     $j \in [m]$ (Line \ref{line:mp1-br}).
   \end{itemize}
   In order avoid exponential dependence on the number of players $m$
   when computing an approximate best response to $\bigtimes_{j \in
     [m]} \wh q_{j,h}$, we draw $K := \lceil 4 \log
   (mn_0/\delta)/\ep^2 \rceil$ (for $\delta = \ep/(6H)$) samples from
   $\bigtimes_{j \in [m]} \wh q_{j,h}$ and use these samples to
   compute the best response. In particular, letting
   $\ba_h^K\in\cA$ denote the $k$th sampled action profile, we  construct a function
   $\wh R_{m+1,h} : \MA_{m+1} \ra \BR$ in Lines
   \ref{line:draw-samples} and \ref{line:mp1-br} which, for each $a'
   \in \MA_{m+1}$, is defined as the average over samples
   $\crl{\ba_h^k}_{k\in\brk{K}}$ of the realized payoffs $R_{m+1,h}(s_h, (\ba_h^k, a'))$;
   note that to compute the payoffs for each sample, Algorithm \ref{alg:3nash} needs only two oracle calls to $\MO_G$. 

   The following lemma, proven in the sequel, gives a correctness guarantee for Algorithm \ref{alg:3nash}.
\begin{lemma}[Correctness of Algorithm \ref{alg:3nash}]
  \label{lem:algorithmic-lemma}
  Given any $m$-player $n_0$-action normal form game $G$, if the
  algorithm $\CB$ solves the $(T, \ep, N)$-\GenCCE problem for the
  game $\MG(G)$ with $T, \ep, N$ satisfying $T \leq
  \exp(n_0\ep^2/m^2)$, then Algorithm \ref{alg:3nash} outputs a $16(m+1) \cdot \ep$-approximate Nash equilibrium of $G$  with probability at least $1/3$, and otherwise fails.
\end{lemma}





The assumption that $\Tnm < \exp \left( \frac{\epnm^2 \cdot \lfloor
    n/m \rfloor}{m^2} \right)$ from the statement of Theorem
\ref{thm:nonmarkov-multiplayer} yields that $T  \leq \exp(n_0\ep^2 /
m^2)$, so Lemma \ref{lem:algorithmic-lemma} yields that Algorithm
\ref{alg:3nash} outputs a $16(m+1) \cdot \ep$-Nash equilibrium of $G$
with probability at least $1/3$ (and otherwise fails). By iterating
Algorithm \ref{alg:3nash} for $\log(1/\delta)$ times, we may thus compute a $16(m+1) \cdot \ep$-Nash equilibrium of $G$ with failure probability $1-\delta$. 

We now analyze the oracle cost and computational cost of Algorithm \ref{alg:3nash}. It takes $2 Q$ oracle calls to $\MO_G$ to simulate the $Q$ generative model oracle calls of $\CB$, and therefore, if $\CB$ runs in time $U$, then the call to $\CB$ on Line \ref{line:call-b}, using oracle calls to $\MO_G$ to simulate simulate the generative model oracle calls, runs in time $O(U)$. 
Next, the computations of $\til q_{j,h}$ (and thus $\wh q_{j,h}$) in Line \ref{line:compute-hatq-multiplayer} can be performed in $(nmTN)^{O(1)}$ time, the computation of $\wh R_{m+1,h} : \MA_{m+1} \ra \BR$ in Line \ref{line:mp1-br} requires time (and oracle calls to $\MO_G$) bounded above by $O(|\MA_{m+1}| \cdot K) \leq (nm\log(1/\delta)/\ep)^{O(1)}$,  constructing the actions $a_{j,h}$ (for $j \in [m+1]$) in Lines \ref{line:draw-ajh} and \ref{line:mp1-br} takes time $(Nmn)^{O(1)}$ (using the fact that the policies $\sigma_{j,h}\^{t^\st}$ are $N$-computable), and constructing the rewards $r_{j,h}$ on Line \ref{line:reward-simulate} requires another $2(m+1)$ oracle calls to $\MO_G$. 
Altogether, Algorithm \ref{alg:3nash} requires $2Q + (nm \log(1/\delta)/\ep)^{C_0}$ oracle calls to $\MO_G$ and, if $\CB$ runs in time $U$, then Algorithm \ref{alg:3nash} takes time $(nmTNU \log(1/\delta)/\ep)^{C_0}$, for some absolute constant $C_0$. 
  
\end{proof}

\begin{remark}[Bit complexity of exponential weights updates]
In the above proof we have noted that $\til q_{j,h}$ (as defined in
Line \ref{line:compute-hatq-multiplayer} of \cref{alg:3nash}) can be
computed in time $(nmTN)^{O(1)}$. A detail we do not handle formally
is that, since the values of
$\til q_{j,h}(t)$ are in general irrational, only the $(nmTN)^{O(1)}$
most significant bits of each real number $\til q_{j,h}(t)$ can be
computed in time $(nmTN)^{O(1)}$. To give a truly polynomial-time implementation of
\cref{alg:3nash}, one can compute only the $(nmTN)^{O(1)}$ most
significant bits of each distribution $\til q_{j,h}$, which is
sufficient to approximate the true value of $\wh q_{j,h}$ to within
$\exp(-(nmTN)^{O(1)})$ in total variation distance. Since $\wh
q_{j,h}$ only influences the subsequent execution of \cref{alg:3nash}
via the samples $\ba_h^1, \ldots, \ba_h^K \sim \bigtimes_{j \in [m]}
\wh q_{j,h}$ drawn in Line \ref{line:draw-samples}, by a union bound,
the approximation of $\wh q_{j,h}$ we have described perturbs the
execution of the algorithm by at most $O(KH) \cdot
\exp(-(nmTN)^{O(1)})$ in total variation distance. In particular, the
correctness guarantee of Lemma \ref{lem:algorithmic-lemma} still
holds, with sucess probability at least  $1/3 - \exp(-(nmTN)^{O(1)}) > 1/4$. 
\end{remark}

It remains to prove Lemma \ref{lem:algorithmic-lemma}, which is the bulk of the proof of Theorem \ref{thm:nonmarkov-multiplayer}.
\begin{proof}[Proof of Lemma \ref{lem:algorithmic-lemma}]
   We will establish the following two facts:
   \begin{enumerate}
   \item First, the choices of $a_{m+1, h}$ in Line \ref{line:mp1-br}
     (i.e., Eq. \ref{eq:choose-mp1-action}) of Algorithm
     \ref{alg:3nash} correspond to a valid policy
     $\pi_{m+1}^\dagger \in \Pirndrnd$ for player $m+1$ (representing
     a strategy for deviating from the equilibrium $\ol \sigma$), in that they
     can be expressed as a function of player $(m+1)$'s history,
     $(\tau_{m+1,h-1}, s_h)$ at each step $h$.
   \item Second, we will show
     that, since $\ol \sigma$ is an $\ep$-CCE of $\MG$, the strategy
     $\pi_{m+1}^\dagger$ cannot not lead to a large increase of value
     for player $m+1$, which will imply that Algorithm \ref{alg:3nash}
     must return a Nash equilibrium with high enough
     probability. %
   \end{enumerate}

   \paragraph{Defining $\pi_{i}^\dagger$ for $i \in [m+1]$.} We begin
   by constructing the policy $\pi_{m+1}^\dagger$ described; for later
   use in the proof, it will be convenient to construct
   a collection of closely related policies $\pi_i^\dagger \in
   \Pirndrnd$ for $i\in\brk{m}$, also representing
   strategies for deviating from the equilibrium $\ol \sigma$.

Let $i\in\brk{m+1}$ be fixed. For $h \in [H]$, the mapping $\pi_{i,
  h}^\dagger : \CH_{i, h-1} \times \MS \ra \MA_{i}$ is defined as
follows. Given a history $\tau_{i, h-1} = (s_1, a_{i,1}, r_{i,1},
\ldots, s_{h-1}, a_{i,h-1}, r_{i,h-1}) \in \CH_{i,h-1}$ (we assume
without loss of generality that $\tau_{i,h-1}$ occurs with positive
probability under some sequence of general policies) and a current
state $s_h$, we define $\pi_{i,h}^\dagger(\tau_{i,h-1}, s_h) \in
\MA_{i}$ through the following process.
   \begin{enumerate}
   \item First, we claim that for all players
     $j\in\brk{m+1}\setminus\crl{i}$, it is possible to extract the
     trajectory $\tau_{j,h-1}$ from the trajectory $\tau_{i,h-1}$ of
     player $i$.
     \begin{enumerate}
     \item Recall that for each $g < h$, %
       from the definition in \cref{eq:rewards-multiplayers-true} and
       the function $\enc(\ba)$, the bits following position
       $3 \lceil \log 1/\ep \rceil$ of the reward $r_{i,g}$ given to
       player $i$ at step $g$ of the trajectory $\tau_{i,g-1}$ encode
       an action profile $\ba_g \in \ol \MA$. Since $\tau_{i,h-1}$
       occurs with positive probability, this is precisely the action
       profile which was played by agents at step $g$. Note we also
       use here that by definition of the rewards $R_{j,h}(s, \ba)$ in
       (\ref{eq:rewards-multiplayers-true}), the component
       $\ol R_{j,h}(s, \ba)$ of the reward only affects the first
       $2 \lceil \log 1/\ep \rceil$ bits.

     \item For $g < h$ and $j \in [m+1]\backslash \{ i\}$, define
       $r_{j,g} := R_{j,g}(s_{g}, \ba_{g})$.

     \item For $j \in [m+1]\backslash \{ i\}$, write
       $\tau_{j,h-1} := (s_1, a_{j,1}, r_{j,1}, \ldots, s_{h-1},
       a_{j,h-1}, r_{j,h-1})$; in particular, $\tau_{j,h-1}$ is a
       deterministic function of $(\tau_{i,h-1}, s_h)$.  (Note that,
       since $\tau_{i,h-1}$ occurs with positive probability, the
       history $\tau_{j,h-1}$ observed by player $j$ up to step $h-1$
       can be computed from it via Steps (a) and
       (b)). %
       Going forward, for $g < h-1$, we let $\tau_{j,g}$ denote the prefix of
       $\tau_{j,h-1}$ up to step $g$.
     \end{enumerate}
   \item Now, using that player $i$ can compute all players'
     trajectories, for each $j \in [m+1]$ we define
     \begin{align}
       \wh q_{j,h} := \E_{t \sim \til q_{j,h}} \left[ \sigma_{j,h}\^t(\tau_{j,h-1}, s_h) \right] \in \Delta(\MA_j)\label{eq:define-whq-real},
     \end{align}
      where $\til q_{j,h} \in \Delta([T])$ is defined as follows: for $t \in [T]$, %
       \begin{align}
          \til q_{j,h}(t) :=
         \frac{\exp \left( - \sum_{g < h} \log \left( \frac{1}{\sigma\^t_{j, g}(a_{j,g} | \tau_{j,g-1},  s_g)} \right)\right)}{\sum_{t'=1}^T\exp \left( - \sum_{g < h} \log \left( \frac{1}{\sigma\^{t'}_{j, g}(a_{j,g} | \tau_{j,g-1},  s_g)} \right)\right)}\label{eq:define-tilq-real}.
       \end{align}
       Note that $\wh q_{j,h}$ is a random variable which depends on the trajectory $(\tau_{j,h-1}, s_h)$ (which can be computed from $(\tau_{i,h-1}, s_h)$). 
       In addition, the definition of $\wh q_{j,h}$
    (for each $j \in [m]$) %
	   is exactly as is defined in Line \ref{line:compute-hatq-multiplayer}
    of Algorithm \ref{alg:3nash}. 
  \item For $i\in\brk{m}$, define $\pi_{i,h}^\dagger(\tau_{i, h-1}, s_h)$ as follows:
    \begin{align}
  \pi_{i,h}^\dagger(\tau_{i, h-1}, s_h) :=     \argmax_{a' \in \MA_{i}} \E_{\ba_{-i} \sim \bigtimes_{j \neq i} \wh q_{j,h}} \left[R_{m+1,h}(s_h, (a', \ba_{-i}))\right].\label{eq:define-pii-dagger}
    \end{align}
    For the case $i =m+1$, define $\pi_{m+1,h}^\dagger(\tau_{m+1,h-1},
    s_h) \in \Delta(\MA_{m+1})$ \dfedit{(implicitly)} to be the following distribution over $a_{m+1,h}^\dagger \in \MA_{m+1}$: draw $\ba_h^1, \ldots, \ba_h^K \sim \bigtimes_{j \in [m]} \wh q_{j,h}$, define $\wh R_{m+1,h}(a') := \frac 1K \sum_{k=1}^K R_{m+1,h}(s_h, (\ba_h^k, a'))$ for $a' \in \MA_{m+1}$, and finally set
    \begin{align}
      a_{m+1,h}^\dagger:= \argmax_{a' \in \MA_{m+1}} \left\{ \wh R_{m+1,h}(a') \right\}\label{eq:argmax-rhat-policy}.
    \end{align}
  Note that, for each choice of $(\tau_{m+1,h-1}, s_h)$, the
  distribution  $\pi_{m+1,h}^\dagger(\tau_{m+1,h-1}, s_h)$ as defined
  above coincides with the distribution of the action
  $a_{m+1,h}^\dagger$ defined in Eq. \ref{eq:choose-mp1-action} in Algorithm \ref{alg:3nash}, when player $m+1$'s history is $\tau_{m+1,h-1}$ and the state at step $h$ is $s_h$. %
  The following lemma, for use later in the proof, bounds the
  approximation error incurred in sampling $\ba_h^1, \ldots, \ba_h^K \sim \bigtimes_{j \in [m]} \wh q_{j,h}$.
  \begin{lemma}
    \label{lem:sampling-qhat}
    Fix any $(\tau_{m+1,h-1}, s_h) \in \CH_{j,h-1}$. With probability
    at least $1-\delta$ over the draw of $\ba_h^1, \ldots, \ba_h^K \sim \bigtimes_{j \in [m]} \wh q_{j,h}$, it holds that for all $a' \in \MA_{m+1}$,
     \begin{align}
 \left|\wh R_{m+1,h}(a') - \E_{a_j \sim \wh q_{j,h} \ \forall j \in [m]} [R_{m+1,h}(s_h, (a_1, \ldots, a_m, a'))]\right| \leq \frac{\ep}{H}\nonumber,
     \end{align}
     which implies in particular that with probability at least $1-\delta$ over the draw of $a_{m+1,h}^\dagger \sim \pi_{m+1,h}^\dagger(\tau_{m+1,h-1}, s_h)$,
     \begin{align}
\max_{a' \in \MA_{m+1}} \left\{ \E_{a_j \sim \wh q_{j,h} \ \forall j \in [m]} [R_{m+1,h}(s_h, (a_1, \ldots, a_m, a'))] \right\}-\frac{2\ep}{H} \leq  & \E_{a_j \sim \wh q_{j,h} \ \forall j \in [m]} [R_{m+1,h}(s_h, (a_1, \ldots, a_m, a_{m+1,h}^\dagger))]
         \label{eq:max-hat-diff}.
     \end{align}
  \end{lemma}
   \end{enumerate}
   It is immediate from our construction above that the following fact holds.
   \begin{lemma}
     \label{lem:alg-simulation}
     The joint distribution of $\tau_{j,h}$, for $j \in [m+1]$ and $h \in [H]$, as computed by Algorithm \ref{alg:3nash}, coincides with the distribution of $\tau_{j,h}$ in an episode of $\MG$ when players follow the policy $\pi_{m+1}^\dagger \times \ol \sigma_{-(m+1)}$. 
   \end{lemma}

   \paragraph{Analyzing the distributions $\wh q_{j,h}$.} %
   Fix any $i \in [m+1]$. We next  prove some facts about the
   distributions $\wh q_{j,h}$ defined above (as a function of
   $(\tau_{i,h-1}, s_h)$) in the process of computing
   $\pi_{i,h}^\dagger(\tau_{i,h-1}, s_h)$.
   

   For each $h \in [H]$, consider any choice of $(\tau_{i,h-1}, s_h)
   \in \CH_{i,h-1} \times \MS$; note that for each $j \in [m+1]$, the
   distributions $\wh q_{j,h} \in \Delta(\MA_j)$ for $h \in [H]$ may
   be viewed as an application Vovk's aggregating algorithm (Proposition
  \ref{prop:online-tvd}) in the following setting: the number of steps
  ($T$, in the context of Proposition
  \ref{prop:online-tvd}; note that $T$ has a different meaning in the
  present proof)
  horizon is $H$, the context space is $\bigcup_{h=1}^H \CH_{j,h-1}
  \times \MS$, and the output space is $\MA_j$. The expert set is
  $\cI=\crl{\rho_j\^1, \ldots, \rho_j\^T}$ (which has $\abs{\cI}=T$),
  and the experts' predictions on a context $(\tau_{j,h-1}, s) \in \CH_{j,h-1} \times \MS$ are defined via $\rho_j\^t(\cdot | \tau_{j,h-1}, s) :=  \sigma_{j,h}\^t( \cdot | \tau_{j,h-1}, s) \in \Delta(\MA_j)$. Then for each $h \in [H]$, the distribution $\wh q_{j,h}$ is obtained by updating the aggregating algorithm with the context-observation pairs $(\tau_{j,h'-1}, a_{j,h'})$ for $h' = 1, 2, \ldots, h-1$.

  In more detail, fix any $t^\st \in [T]$ and $j \in [m+1]$ with $i
  \neq j$. We may apply Proposition \ref{prop:online-tvd} with the
  number of steps set to $H$, the set of experts as $\cI=\{ \rho_j\^1,
  \ldots, \rho_j\^T \}$, and contexts and outcomes generated according
  to the distribution induced by running the policy $\pi_{i}^\dagger
  \times \sigma_{-i}\^{t^\st}$ in the Markov game $\MG$ as follows:
  \begin{itemize}
  \item For each $h \in [H]$, we are given, at steps $h' < h$, the actions $a_{k,h'}$ rewards $r_{k,h'}$ for all agents $k \in [m+1]$, as %
    well as the states $s_1, \ldots, s_h$.
    \begin{itemize}
    \item For each $k \in [m+1]$, set $\tau_{k,h-1} = (s_1, a_{k,1}, r_{k,1}, \ldots, s_{h-1}, a_{k,h-1}, r_{k,h-1})$ to be agent $k$'s history.
    \item The \emph{context} fed to the aggregation algorithm at step $h$ is $(\tau_{j,h-1}, s_h)$. 
    \item The \emph{outcome} at step $h$ is given by $a_{j,h} \sim \sigma_{j,h}\^{t^\st}(\cdot | \tau_{j,h-1}, s_h)$; note that this choice satisfies the realizability assumption in Proposition \ref{prop:online-tvd}.
    \item To aid in generating the next context at step $h+1$, choose $a_{k,h} \sim \sigma_{k,h}^{t^\st}(\tau_{k,h-1}, s_h)$ for all $k \in [m+1] \backslash \{ i,j \}$ and $a_{i,h} = \pi_{i,h}^\dagger(\tau_{i,h-1}, s_h)$. 
      Then set $s_{h+1}$ to be the next state given the transitions of $\MG$ and the action profile $\ba_h = (a_{1,h}, \ldots, a_{m+1,h})$. %
    \end{itemize}
  \end{itemize}

  By Proposition \ref{prop:online-tvd}, it  follows that   
  for any fixed $t^\st \in [T]$ and $j \in [m+1]$ with $j \neq i$,
  under the process described above we have
  \begin{align}
    \E_{\pi_{i}^\dagger\times \sigma_{-i}\^{t^\st}} \left[ \sum_{h=1}^H \tvd{\sigma_{j,h}\^{t^\st}(\tau_{j,h-1}, s_h)}{\wh q_{j,h}} \right] \leq \sqrt{H\cdot\log T}.\label{eq:tvd-qhat-nonmarkov}
  \end{align}

   \paragraph{Analyzing the value of $\pi_{m+1}^\dagger$.}
   Next, using the development above, we show that if \cref{alg:3nash}
   successfully computes a Nash equilibrium with constant probability (via $\pi_{m+1}^\dagger$)
   whenever $\wb{\sigma}$ is an $\eps$-CCE. We first state the following claim,
   which is proven in the sequel by analyzing the values $V_i^{\pi_i^\dagger \times \ol \sigma_{-i}}$ for $i \in [m]$.
     \begin{lemma}
    \label{lem:main-players-lb}
    If $\ol \sigma$ is an $\ep$-CCE of $\MG$, then it holds that for all $i \in [m]$,
    \begin{align}
V_{i}^{\ol \sigma} \geq -\ep - m\sqrt{\log(T)/H}\nonumber.
    \end{align}
  \end{lemma}
  Note that in the game $\MG$, since for all $h \in [H]$, $s \in \MS$
  and $\ba \in \ol \MA$, it holds that $\left|\sum_{j=1}^{m+1} R_{j,h}
  (s,\ba)\right| \leq \frac{(m+1)\ep^2}{H}$ (which holds since in (\ref{eq:rewards-multiplayers-true}), $\enc(\ba)$ is multiplied by $\frac{1}{H} \cdot 2^{-3\lceil \log 1/\ep \rceil}$),
    it follows that $\left|\sum_{j=1}^{m+1} V_{j}^{\ol \sigma}\right| \leq (m+1)\ep^2$. Thus, by Lemma \ref{lem:main-players-lb}, we have
  $V_{m+1}^{\ol \sigma} \leq (m+1)\ep^2 + m \cdot (\ep +
  m\sqrt{\log(T)/H})$, and since $\ol \sigma$ is an $\ep$-CCE of $\MG$ it
  follows that
  \begin{align}
    \label{eq:vmdagger-ub}
    V_{m+1}^{\pi_{m+1}^\dagger\times \ol \sigma_{-(m+1)}} \leq 2(m+1) \cdot \ep  + m^2\cdot \sqrt{\log(T)/H}.
  \end{align}

  To simplify notation, we will write $\wh q_h := \wh q_{1,h} \times
  \cdots \times \wh q_{m,h}$ in the below calculations, where we
  recall that each $\wh q_{j,h}$ is determined given the history up to step $h$, $(\tau_{j,h-1}, s_h)$, as defined in (\ref{eq:define-whq-real}) and (\ref{eq:define-tilq-real}). %
  An action profile drawn from $\wh q_h$ is denoted as $\ba \sim \wh q_h$, with $\ba \in \MA$.  
We may now write $V_{m+1}^{\pi_{m+1}^\dagger \times \ol
  \sigma_{-(m+1)}}$ as follows:
  \begin{align}
    & V_{m+1}^{\pi_{m+1}^\dagger\times \ol \sigma_{-(m+1)}}\nonumber\\
    =& \E_{t^\st \sim [T]} \sum_{h =1}^H \E_{\pi_{m+1}^\dagger\times \sigma_{-(m+1)}\^{t^\st}} \E_{\substack{a_{j,h} \sim \sigma_{j,h}\^{t^\st}(\tau_{j,h-1}, s_h) \ \forall j \in [m] \\ a_{m+1,h} \sim \pi_{m+1,h}^\dagger(\tau_{m+1,h-1}, s_h) \\ \ba := (a_{1,h}, \ldots, a_{m+1,h})}} \left[ R_{m+1,h}(s_h, \ba)\right]\nonumber\\
    \geq & \E_{t^\st \sim [T]} \sum_{h=1}^H \E_{\pi_{m+1}^\dagger\times \sigma_{-(m+1)}\^{t^\st}} \Bigg(\E_{\substack{a_{j,h} \sim \wh q_{j,h} \ \forall j \in [m] \\ a_{m+1,h} \sim \pi_{m+1,h}^\dagger(\tau_{m+1,h-1}, s_h) \\ \ba := (a_{1,h}, \ldots, a_{m+1,h})}} \left[ R_{m+1,h}(s_h, \ba)\right]\icml{ \nonumber\\
    & \qquad \qquad } - \frac 1H \sum_{j \in [m]} \tvd{\sigma_{j,h}\^{t^\st}(\tau_{j,h-1}, s_h)}{\wh q_{j,h}} \Bigg) \nonumber\\
    \geq &  \E_{t^\st \sim [T]} \sum_{h=1}^H\E_{\pi_{m+1}^\dagger\times \sigma_{-(m+1)}\^{t^\st}}  \Bigg(\max_{a_{m+1,h}' \in \MA_{m+1}} \E_{\ba \sim \wh q_h} \left[  R_{m+1,h}(s_h, (\ba, a_{m+1,h}'))\right] - \frac{2\ep}{H} - \frac{\delta}{H} \icml{\nonumber\\
    & \qquad \qquad} - \frac 1H \sum_{j \in [m]} \tvd{\sigma_{j,h}\^{t^\st}(\tau_{j,h-1},s_h)}{\wh q_{j,h}} \Bigg) \nonumber\\
    \geq & \frac{1}{H}\cdot  \E_{t^\st \sim [T]} \sum_{h =1}^H \E_{\pi_{m+1}^\dagger\times \sigma_{-(m+1)}\^{t^\st}} \left( \max_{j \in [m], a_{j,h}' \in \MA_j} \E_{\ba \sim \wh q_h }[(M_j)_{a_j', \ba_{-j}} - (M_j)_{\ba}]\right) - \frac{m}{H} \cdot \sqrt{H \log T} - 2\ep - \delta - \ep^2\nonumber,
  \end{align}
  where:
  \begin{itemize}
  \item The first inequality follows from the fact that $R_{m+1,h}(\cdot)$ takes values in $[-1/H,1/H]$ and the fact that the total variation between product distributions is bounded above by the sum of total variation distances between each of the pairs of component distributions.
  \item The second inequality follows from the inequality (\ref{eq:max-hat-diff}) of Lemma \ref{lem:sampling-qhat}. %
  \item The final equality follows from the definition of the rewards
    in (\ref{eq:rewards-multiplayers-true}) and
    (\ref{eq:rewards-multiplayers}), and by summing
    (\ref{eq:tvd-qhat-nonmarkov}) over $j \in [m]$. We remark that the $-\ep^2$ term in the final line comes from the term $\frac{1}{H} \cdot 2^{-3\lceil \log 1/\ep \rceil} \cdot \enc(\ba)$ in (\ref{eq:rewards-multiplayers-true}).
  \end{itemize}
  Rearranging and using (\ref{eq:vmdagger-ub}) as well as the fact that $\delta + \ep^2= \ep/(6H) + \ep^2 \leq \ep$ (as $\ep \leq 1/2$), we get that
  \begin{align}
     & \E_{t^\st \sim [T]}  \E_{\pi_{m+1}^\dagger\times \sigma_{-(m+1)}\^{t^\st}} \sum_{h=1}^H \left( \max_{j \in [m], a_{j,h}' \in \MA_j} \E_{\ba \sim \wh q_h } [(M_j)_{a_j', \ba_{-j}} - (M_j)_{\ba}] \right) \nonumber\\
    \leq & 2H \cdot \ep \cdot (m+1) +(m+1)m \cdot  \sqrt{H \log T} + 3H\ep\nonumber.
  \end{align}
  Since $\wh q_h$ is a product distribution a.s., we have that
  \[
    \max_{j \in [m], a_{j,h}' \in \MA_j} \E_{\ba \sim \wh q_h }
    [(M_j)_{a_j', \ba_{-j}} - (M_j)_{\ba}] \geq 0.
  \] 
  Therefore, by Markov's inequality, with probability at least $1/2$
  over the choice of $t^\st \sim [T]$ and the trajectories $(\tau_{j,h-1}, s_h) \sim \pi_{m+1}^\dagger\times \sigma_{-(m+1)}\^{t^\st}$ for $j \in [m]$ (which collectively determine $\wh q_h$), there is some $h \in [H]$ so that
  \begin{align}
    \max_{j \in [m], a_{j,h}' \in \MA_j} \E_{\ba \sim \wh q_h } [(M_j)_{a_j', \ba_{-j}} - (M_j)_{\ba}] \leq 10(m+1) \cdot  \ep + 2(m+1)m \cdot \sqrt{ \log(T)/H} \leq 12(m+1) \cdot \ep\label{eq:hatq-ne},
  \end{align}
  where the final inequality follows as long as $H \cdot \ep^2 \geq
  m^2 \log T$, i.e., $T \leq \exp \left( \frac{H \cdot \ep^2}{m^2}
  \right)$, which holds since $H \geq n_0$ and we have assumed that $T \leq \exp(\ep^2 \cdot n_0 / m^2)$. 

  Note that (\ref{eq:hatq-ne}) implies that with probability at least $1/2$ under an episode drawn from $\pi_{m+1}^\dagger \times \ol \sigma_{-(m+1)}$, there is some $h \in [H]$ so that $\wh q_h$ is a $12(m+1) \cdot \ep$-Nash equilibrium of the stage game $G$. Thus, by Lemma \ref{lem:alg-simulation}, with probability at least $1/2$ under an episode drawn from the distribution of Algorithm \ref{alg:3nash}, there is some $h \in [H]$ so that $\wh q_h$ is a $12(m+1) \cdot \ep$-Nash equilibrium of $G$.

  Finally, the following two observations conclude the proof of Lemma \ref{lem:algorithmic-lemma}.
  \begin{itemize}
  \item If $\wh q_h$ is a $12(m+1) \cdot \ep$-Nash equilibrium of $G$,
    then by definition of the reward function $R_{m+1, h}(\cdot)$ in (\ref{eq:rewards-multiplayers-true}), upper bounding $\frac{1}{H} \cdot 2^{-3\lceil \log 1/\ep \rceil} \cdot \enc(\ba)$ by $\ep^2/H$, 
    \begin{align}
\max_{a' \in \MA_{m+1}} \E_{\ba \sim \wh q_h} \left[ R_{m+1, h}(\mf s, (\ba, a'))\right] \leq \frac{1}{H} \cdot 12(m+1) \cdot \ep + \frac{\ep^2}{H}\nonumber,
    \end{align}
    which implies, by Lemma \ref{lem:sampling-qhat}, that with
    probability at least $1-\delta$ over the draw of $\ba_h^1, \ldots, \ba_h^K$,
    \begin{align}
\max_{a' \in \MA_{m+1}} \left\{ \wh R_{m+1, h}(a') \right\} \leq \frac{1}{H} \cdot 12(m+1) \cdot \ep + \frac{\ep^2}{H} + \frac{\ep}{H} \leq \frac{1}{H} \cdot 14(m+1) \cdot \ep\nonumber,
    \end{align}
    i.e., the check in Line \ref{line:check-nash} of Algorithm \ref{alg:3nash} will pass and the algorithm will return $\wh q_h$ (if step $h$ is reached). 
  \item Conversely, if $\max_{a' \in \MA_{m+1}} \left\{ \wh R_{m+1,h}(a') \right\} \leq 14(m+1) \cdot \ep$, i.e., the check in Line \ref{line:check-nash} passes, then by Lemma \ref{lem:sampling-qhat}, with probability at least $1-\delta$ over $\ba_h^1, \ldots, \ba_h^K$,
    \begin{align}
\max_{a' \in \MA_{m+1}} \E_{\ba \sim \wh q_h} \left[ R_{m+1, h}(\mf s, (\ba, a')) \right] \leq \frac{1}{H} \cdot 14(m+1) \cdot \ep + \frac{\ep}{H}\leq \frac{1}{H} \cdot 15(m+1) \cdot \ep \nonumber,
    \end{align}
    which implies, by the definition of $R_{m+1,h}(\cdot)$ in (\ref{eq:rewards-multiplayers-true}) and (\ref{eq:rewards-multiplayers}), that $\wh q_h$ is a $16(m+1) \cdot \ep$-Nash equilibrium of $G$. 
  \end{itemize}
  Taking a union bound over all $H$ of the probability-$\delta$
  failure events from Lemma \ref{lem:sampling-qhat} for the sampling
  $\ba_h^1, \ldots, \ba_h^K \sim \wh q_h$ (for $h \in [H]$), as well as over the probability-$1/2$ event that there is no $\wh q_h$ which is a $12(m+1) \cdot \ep$-Nash equilibrium of $G$, we obtain that with probability at least $1 - 1/2 - H \cdot \ep/(6H) \geq 1/3$, Algorithm \ref{alg:3nash} outputs a $16(m+1) \cdot \ep$-Nash equilibrium of $G$.
\end{proof}

Finally, we prove the remaining claims stated without proof above. %

\begin{proof}[Proof of Lemma \ref{lem:sampling-qhat}]
  Since $R_{m+1,h}(\mf s, \ba) \in [-1/H,1/H]$ for each $\ba \in
  \ol\MA$, by Hoeffding's inequality, for any fixed $a' \in
  \MA_{m+1}$, with probability at least $1-\delta/ |\MA_{m+1}| =
  1-\delta/(mn_0)$ over the draw of $\ba_h^1, \ldots, \ba_h^K \sim \bigtimes_{j \in [m]} \wh q_{j,h}$, it holds that
  \begin{align}
 \left|\wh R_{m+1,h}(a') - \E_{a_j \sim \wh q_{j,h} \ \forall j \in [m]} [R_{m+1,h}(s_h, (a_1, \ldots, a_m, a'))]\right| \leq \frac{2}{H} \cdot \sqrt{\frac{\log mn_0/\delta}{K}} \leq \frac{\ep}{H} \nonumber,
  \end{align}
  where the final inequality follows from the choice of $K = \lceil 4 \log(mn_0/\delta) / \ep^2 \rceil$. The statement of the lemma follows by a union bound over all $|\MA_{m+1}|$ actions $a'\in\cA_{m+1}$. 
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:main-players-lb}]
  Fix any agent $i \in [m]$. We will argue that the policy
  $\pi_i^\dagger \in \Pidet_i$ defined within the proof of
  Lemma \ref{lem:algorithmic-lemma} satisfies $V_{i}^{\pi_i^\dagger, \ol \sigma_{-i}} \geq -m\sqrt{\log(T)/H}$. Since $\ol \sigma$ is an $\ep$-CCE of $\MG$, it follows that
  \begin{align}
\ep \geq V_{i}^{\pi_i^\dagger, \ol \sigma_{-i}} - V_{i}^{\ol \sigma} \geq -m\sqrt{\log(T) / H} - V_{i}^{\ol \sigma}\nonumber,
  \end{align}
  from which the result of Lemma \ref{lem:main-players-lb} follows after rearranging terms.
   
    To simplify notation, let us write $\wh q_{-i,h} := \bigtimes_{j
      \neq i} \wh q_{j,h}$, where we recall that each $\wh q_{j,h}$ is determined given the history up to step $h$, $(\tau_{j,h-1}, s_h)$, as defined in (\ref{eq:define-whq-real}) and (\ref{eq:define-tilq-real}). An action profile drawn from $\wh q_{-i,h}$ is denoted by $\ba_{-i} \sim \wh q_{-i,h}$, with $\ba_{-i} \in \ol \MA_{-i}$.
    We compute
      \begin{align}
        & V_{i}^{\pi_i^\dagger\times \ol \sigma_{-i}}\nonumber\\
        =& \E_{t^\st \sim [T]} \sum_{h=1}^H \E_{\pi_i^\dagger\times \sigma_{-i}\^{t^\st}} \E_{\ba_{-i} \sim \bigtimes_{j \neq i}  \sigma_{j,h}\^{t^\st}(\tau_{j,h-1}, s_h)} \left[ R_{i,h}(s_h, (\pi_{i,h}^\dagger(\tau_{i,h-1}, s_h), \ba_{-i}))\right]\nonumber\\
        \geq & \E_{t^\st \sim [T]} \sum_{h=1}^H \E_{\pi_i^\dagger\times \sigma_{-i}\^{t^\st}} \left(\E_{\ba_{-i} \sim \wh q_{-i,h}} \left[R_{i,h}(s_h, (\pi_{i,h}^\dagger(\tau_{i,h-1}, s_h), \ba_{-i}))\right] - \frac{1}{H} \sum_{j \neq i} \tvd{\sigma_{j,h}\^{t^\st}(\tau_{j,h-1}, s_h)}{\wh q_{j,h}}\right)\nonumber\\
        \geq & \E_{t^\st \sim [T]} \sum_{h =1}^H \E_{\pi_i^\dagger\times \sigma_{-i}\^{t^\st}}  \left(\max_{a_i' \in \MA_i} \E_{\ba_{-i} \sim \wh q_{-i,h}} \left[ R_{i,h}(s_h, (a_i', \ba_{-i}))\right] \right) - \frac{m}{H} \cdot \sqrt{H \log T}\nonumber\\
        \geq & - m\sqrt{\log(T) / H}\nonumber,
      \end{align}
      where:
      \begin{itemize}
      \item The first inequality follows from the fact that the rewards $R_{i,h}(\cdot)$ take values in $[-1/H,1/H]$ and that the total variation between product distributions is bounded above by the sum of total variation distances between each of the pairs of component distributions.
      \item The second inequality follows from the definition of $\pi_{i,h}^\dagger(\tau_{i,h-1}, s_h)$ in terms of $\wh q_{-i,h}$ in (\ref{eq:define-pii-dagger}) as well as (\ref{eq:tvd-qhat-nonmarkov}) applied to each $j \neq i$ and each $t^\st \in [T]$.
        \item The  final inequality follows by Lemma
          \ref{lem:playerk-nonneg} below, applied to agent $i$ and to
          the distribution $\wh q_{-i,h}$, which we recall is a
          product distribution almost surely. %
      \end{itemize}
     \end{proof}
    \begin{lemma}
      \label{lem:playerk-nonneg}
      For any $i \in [m]$, $s \in \MS, h \in [H]$, and any product distribution $q \in \Delta(\ol \MA_{-i})$, it holds that
      \begin{align}
\max_{a_i'\in \MA_i} \E_{\ba \sim q} \left[ R_{i,h}(s, (a_i', \ba))\right] \geq 0\nonumber.
      \end{align}
    \end{lemma}
    \begin{proof}
      Choose $a_i^\st := \argmax_{a_i' \in \MA} \E_{\ba\sim q} \left[
        (M_i)_{a_i', \ba}\right]$. Now we compute 
\begin{align}
  H \cdot   \E_{\ba \sim q} \left[ R_{i,h}(s, (a_i^\st, \ba))\right] \geq &H \cdot \min_{a_{m+1}' \in \MA_{m+1}} \E_{\ba\sim q} \left[ R_{i,h}(s, (a_i^\st, a_{m+1}', \ba_{-(m+1)}))\right]\nonumber\\
  \geq& \min_{(j, a_j') \in \MA_{m+1}} \One{j = i} \cdot  \E_{\ba \sim q} \left[   (M_i)_{a_i^\st, \ba} - (M_i)_{a_i', \ba}\right]\nonumber\\
  \geq & 0\nonumber,
\end{align}
where the first inequality follows since $q$ is a product distribution, the second inequality uses that $\enc(\cdot)$ is non-negative, and the final inequality follows since by choice of $a_i^\st$ we have $\E_{\ba \sim q} \left[ (M_i)_{a_i^\st, \ba} \right] \geq \E_{\ba \sim q} \left[ (M_i)_{a_i', \ba} \right]$ for all $a_i' \in \MA_i$. 
\end{proof}



\subsection{Remarks on bit complexity of the rewards}
The Markov game $\MG(G)$ constructed to prove Theorem
\ref{thm:nonmarkov-multiplayer} uses lower-order bits of the rewards
to record the action profile taken each step. These lower order bits
may be used by each agent to infer what actions were taken by other
agents at the previous step, and we use this idea to construct the
best-response policies $\pi_i^\dagger$ defined in the proof.  As a
result of this aspect of the construction, the rewards of the game
$\MG(G)$ each take $O(m \cdot \log(n) + \log(1/\ep))$ bits to
specify. As discussed in the proof of Theorem
\ref{thm:nonmarkov-multiplayer}, it is without loss of generality to
assume that the payoffs of the given normal-form game $G$ take $O(\log
1/\ep)$ bits each to specify, so when either $m \gg 1$ or $n \gg
1/\ep$, the construction of $\MG(G)$ uses more bits to express its
rewards than what is used for the normal-form game $G$.

It is possible to avoid this phenomenon by instead using the state transitions
of the Markov game to encode the action profile taken at each step, as
was done in the proof of Theorem \ref{thm:markov-formal}. The idea,
which we sketch here, is to replace the game $\MG(G)$ of Definition \ref{def:mg-g-multiplayers} with the following game $\MG'(G)$:
\begin{defn}[Alternative construction to Definition \ref{def:mg-g-multiplayers}]
  \label{def:mg-g-gen}
  Given an $m$-player, $n_0$-action normal-form game $G$, 
      we define the game $\MG'(G) = (\MS, H, (\MA_i)_{i \in [2]}, \BP,
      (R_i)_{i \in [2]}, \mu)$ as follows.
\begin{itemize}
\item The horizon of $\MG$ is $H = n_0$. %
\item Let $A = n_0$. The action spaces of agents $1, 2, \ldots, m$ are given by $\MA_1 = \cdots = \MA_m = [A]$. The action space of agent $m+1$ is %
  \begin{align}
\MA_{m+1} = \{ (j, a_j) \ : \ j \in [m], a_j\in \MA_j \}\nonumber,
  \end{align}
  so that $|\MA_{m+1}| = Am \leq n$. 

 We write $\MA = \prod_{j=1}^m \MA_j$ to denote the joint action space of the first $m$ agents, and $\ol \MA  := \prod_{j=1}^{m+1} \MA_j$ to denote the joint action space of all agents. Then $|\ol \MA| = A^m \cdot (mA) = mA^{m+1} \leq n$. %

\item The state space $\MS$ is defined as follows. There are $|\ol \MA|$ states, one for each action tuple $\ba \in \ol \MA$. For each $\ba \in \ol \MA$, we denote the corresponding state by $\mf s_\ba$. %

\item For all $h \in [H]$, the reward to agent $j \in [m+1]$ given action profile $\ba = (a_1, \ldots, a_{m+1})$ at any state $s \in \MS$ is as follows: writing $a_{m+1} = (j', a_{j'}')$,%
\begin{align}
  R_{j,h}(s, \ba) := \begin{cases}
    0 &: j \not \in \{j', m+1\} \\
  \frac{1}{H} \cdot \left((M_j)_{a_1, \ldots, a_m} - (M_j)_{a_1, \ldots, a_{j'}', \ldots, a_m}\right) &: j=j' \\
  \frac{1}{H} \cdot \left( (M_j)_{a_1, \ldots, a_{j'}', \ldots, a_m} - (M_j)_{a_1, \ldots, a_m} \right) &: j=m+1.
  \end{cases}\label{eq:define-nonmarkov-rewards}
\end{align}
\item At each step $h\in [H]$, if action profile $\ba \in \ol\MA$ is taken, the game transitions to the state $\mf s_{\ba}$. %
\end{itemize}
\end{defn}
Note that the number of states of $\MG'(G)$ is equal to $|\ol \MA| =
mn_0^{m+1}$, and so $|\MG'(G)| = mn_0^{m+1}$. As a result, if we were
to use the game $\MG'(G)$ in place of $\MG(G)$ in the proof of Theorem
\ref{thm:nonmarkov-multiplayer}, we would need to define $n_0 :=
\lfloor n^{1/(m+1)}/m \rfloor$ to ensure that $|\MG'(G)| \leq n$, and
so the condition $T < \exp(\ep^2 \cdot \lfloor n/m \rfloor / m^2)$
would be replaced by $T < \exp(\ep^2 \cdot \lfloor n^{1/(m+1)}/m
\rfloor / m^2)$. This would only lead to a small quantitative
degradement in the statement of Theorem \ref{thm:nonmarkov-formal},
with the condition in the statement replaced by $T < \exp(c \cdot \ep^2 \cdot n^{1/3})$ for some constant $c > 0$. However, it would render the statement of Theorem \ref{thm:statistical-lb} essentially vacuous. For this reason, we opt to go with the approach of Definition \ref{def:mg-g-multiplayers} as opposed to Definition \ref{def:mg-g-gen}.

We expect that the construction of Definition
\ref{def:mg-g-multiplayers} can nevertheless still be modified to use
$O(\log 1/\ep)$ bits to express each reward in the Markov game
$\MG$. In particular, one could introduce stochastic transitions to
encode in the state of the Markov game a small number of random bits
of the full action profile played at each step. We leave such an
approach for future work.



\section{Equivalence between $\Pirndrnd_j$ and $\Delta(\Pidet_j)$}
\label{sec:equivalence}
In this section we consider an alternate definition of the space $\Pirndrnd_i$ of randomized general policies of player $i$, and show that it is equivalent to the one we gave in Section \ref{sec:prelim}.

In particular, suppose we were to define a randomized general policy of agent $i$ as a distribution over deterministic general
  policies of agent $i$: we write $\Pirnd_i := \Delta(\Pidet_i)$ to
  denote the space of such distributions. %
  Moreover, write $\Pirnd := \Pirnd_1 \times \cdots \times \Pirnd_m = \Delta(\Pidet_1) \times \cdots \times \Delta(\Pidet_m)$ to denote the space of product distributions over agents' deterministic policies. %
  Our goal in this section is to show that policies in $\Pirnd$ are equivalent to those in $\Pirndrnd$ in the following sense: there is an embedding map $\PE : \Pirndrnd  \ra \Pirnd$, not depending on the Markov game, so that the distribution of a trajectory drawn from any $\sigma \in \Pirndrnd$, for any Markov game, is the same as the distribution of a trajectory drawn from $\PE(\sigma)$ (Fact \ref{fac:gen-decompose}). Furthermore, $\PE$ is surjective in the following sense: any policy $\til \sigma \in \Pirnd$ produces trajectories that are distributed identically to those of $\PE(\sigma)$ (and thus of $\sigma$), for some $\sigma \in \Pirndrnd$ (Fact \ref{fac:emb-inverse}). 
In Definition \ref{def:define-embed} below, we define $\PE$.   
  
\begin{defn}
  \label{def:define-embed}
  For $j \in [m]$ and $ \sigma_j \in \Pirndrnd_j$, define $\PE_j( \sigma_j) \in \Pirnd_j = \Delta(\Pidet_j)$ to put the following amount of mass on each $\pi_j \in \Pidet_j$:
  \begin{align}
(\PE_j( \sigma_j))(\pi_j) := \prod_{h=1}^H \prod_{(\tau_{j,h-1}, s_h) \in \CH_{j,h-1} \times \MS}  \sigma_j(\pi_{j,h}(\tau_{j,h-1}, s_h) \ | \ \tau_{j,h-1}, s_h)\label{eq:emb-def}
  \end{align}
  Furthermore, for $\sigma = (\sigma_1, \ldots, \sigma_m) \in \Pirndrnd$, define $\PE(\sigma) = (\PE(\sigma_1), \ldots, \PE(\sigma_m))$. 
\end{defn}
Note that, in the special case that $ \sigma_j \in \Pidet_j$, $\PE_j( \sigma_j)$ is the point mass on $\sigma_j$. 
\begin{fact}[Embedding equivalence]
  \label{fac:gen-decompose}
  Fix a $m$-player Markov game $\MG$ and, arbitrary policies $ \sigma_j \in \Pirndrnd_j$. %
  Then a trajectory drawn from the product policy $ \sigma = ( \sigma_1, \ldots,  \sigma_m) \in \Pirndrnd_1 \times \cdots \times \Pirndrnd_m$ is distributed identically to a trajectory drawn from $\PE( \sigma) \in \Pirnd$.%
\end{fact}
\noah{maybe Mention that this corresponds to strategic form strategy equivalence in EFG literature}  The proof of Fact \ref{fac:gen-decompose} is provided in Section \ref{sec:equivalence-proofs}. 
Next, we show that the mapping $\PE$ is surjective in the following sense:
\begin{fact}[Right inverse of $\PE_j$]
  \label{fac:emb-inverse}
  There is a mapping $\PF : \Pirnd \ra \Pirndrnd$ so that for any Markov game $\MG$ and %
   any $\til\sigma \in \Pirnd$, the distribution of a trajectory drawn from $\til\sigma$ is identical to the distribution of a trajectory drawn from $\PE \circ \PF(\til\sigma)$. 
\end{fact}
We will write $\PF((\til\sigma_1, \ldots, \til\sigma_m)) := (\PF_1(\til\sigma_1), \ldots, \PF_m(\til\sigma_m))$. Fact \ref{fac:emb-inverse} states that the policy $\PF(\til \sigma)$ maps, under $\PE$, to a policy in $\Pirnd$ which is equivalent to $\til\sigma$ (in the sense that their trajectories are identically distributed for any Markov game). 

An important consequence of Fact \ref{fac:gen-decompose} is that the expected reward (i.e., value) under any $ \sigma \in \Pirndrnd$ is the same as that of $\PE(\sigma)$. Thus given a Markov game, the induced normal-form game in which the players' pure action sets are $\Pirndrnd_1, \ldots, \Pirndrnd_m$ is equivalent to the normal-form game in which the players' pure action sets are $\Pidet_1, \ldots, \Pidet_m$, in the following sense: for any mixed strategy in the former, namely a product distributional policy $\distp \in \Delta(\Pirndrnd_1) \times \cdots \times \Delta(\Pirndrnd_m)$, the policy $\E_{\sigma \sim \distp}[\PE(\sigma)] \in \Delta(\Pidet_1) \times \cdots \times \Delta(\Pidet_m) = \Pirnd$ is a mixed strategy in the latter which gives each player the same value as under $\distp$. (Note that $\E_{\sigma \sim \distp}[\PE(\sigma)]$ is indeed a product distribution since $\distp$ is a product distribution and $\PE$ factors into individual coordiantes.) Furthermore, by Fact \ref{fac:emb-inverse}, any distributional policy in $\Pirnd$ arises in this manner, for some $\distp \in \Delta(\Pirndrnd_1) \times \cdots \times \Delta(\Pirndrnd_m)$; in fact, $\distp$ may be chosen to place all its mass on a single $\sigma \in \Pirndrnd_1 \times \Pirndrnd_m$. Since $\PE$ factors into individual coordinates, it follows that $\PE$ yields a one-to-one mapping between the coarse correlated equilibria (or any other notion of equilibria, e.g., Nash equilibria or correlated equilibria) of these two normal-form games. 
\noah{this is a bit handwavy but it is annoying to write down the defns of the normal-form games in detail and it's not really necessary for the paper}\noah{probably want to say that if $\distp$ is product then so is $\E_{\sigma \sim \distp}[\PE(\sigma)]$.}

\subsection{Proofs of the equivalence}
\label{sec:equivalence-proofs}
\begin{proof}[Proof of Fact \ref{fac:gen-decompose}]
  Consider any trajectory $\tau = (s_1, \ba_1, \br_1, \ldots, s_H, \ba_H, \br_H)$ consisting of a sequence of $H$ states and actions and rewards for each of the $m$ agents. Assume that $r_{i,h} = R_{i,h}(s, \ba_h)$ for all $i, h$ (as otherwise $\tau$ has probability 0 under any policy). Write:
  \begin{align}
p_\tau := \prod_{h=1}^{H-1} \BP_h(s_{h+1} | s_h,\ba_h)\nonumber.
  \end{align}
  Then the probability of observing $\tau$ under $ \sigma$ is
  \begin{align}
    p_\tau \cdot \prod_{h=1}^{H-1} \prod_{j=1}^m  \sigma_{j,h}(a_{j,h} | \tau_{j,h-1}, s_h)\label{eq:prob-tilsigma}
  \end{align}
  where, per usual, $\tau_{j,h-1} = (s_1, a_{j,1}, r_{j,1}, \ldots, s_{h-1}, a_{j,h-1}, r_{j,h-1})$. Write $\sigma =(\sigma_1, \ldots, \sigma_m)= \PE(\sigma)$. The probability of observing $\tau$ under $\sigma$ is
  \begin{align}
p_\tau \cdot \prod_{j \in [m]} \sum_{\pi_j \in \Pidet_j :\ \forall h,\ \pi(\tau_{j,h-1}, s_h) = a_{j,h}} \sigma_j(\pi_j)\label{eq:prob-sigma}
  \end{align}
  It is now straightforward to see from the definition of $\sigma_j(\pi_j)$ in (\ref{eq:emb-def}) that the quantities in (\ref{eq:prob-tilsigma}) and (\ref{eq:prob-sigma}) are equal.
\end{proof}

\begin{proof}[Proof of Fact \ref{fac:emb-inverse}]
  Fix a policy $\til\sigma_j \in \Pirnd_j = \Delta(\Pidet_j)$. We define $\PF_j(\til\sigma_j)$ to be the policy $ \sigma_j \in \Pirndrnd_j$, which is defined as follows: for $\tau_{j,h-1} = (s_{j,1}, a_{j,1}, r_{j,1}, \ldots, s_{j,h-1}, a_{j,h-1}, r_{j,h-1}) \in \CH_{j,h-1}$, $s_h \in \MS$, we have, for $a_{j,h} \in \MA_j$, 
  \begin{align}
 \sigma_j(\tau_{j,h-1}, s_h)(a_{j,h}) = \frac{\til\sigma_j \left( \{ \pi_j \in \Pidet_j \ : \ \pi_j(\tau_{j,g}, s_g) = a_{j,g} \ \forall g \leq h \} \right)}{\til\sigma_j \left( \{ \pi_j \in \Pidet_j \ : \ \pi_j(\tau_{j,g}, s_g) = a_{j,g} \ \forall g \leq h-1 \} \right)}\nonumber.
  \end{align}
  If the denominator of the above expression is 0, then $ \sigma_j(\tau_{j,h-1}, s_h)$ is defined to be an arbitrary distribution on $\Delta(\MA_j)$. (For concreteness, let us say that it puts all its mass on a fixed action in $\MA_j$.) Furthermore, for $\til \sigma \in \Pirnd$, define $\PF(\til \sigma) := (\PF_1(\til \sigma_1), \ldots, \PF_m(\til \sigma_m)) \in \Pirndrnd$. 

  Next, fix any $\til\sigma = (\til\sigma_1, \ldots, \til\sigma_m) \in \Pirnd_1 \times \cdots \times \Pirnd_m$.
  Let $\sigma = \PF(\til\sigma)$. By Fact \ref{fac:gen-decompose}, it suffices to show that the distribution of trajectories under $\sigma$ is the same as the distribution of trajectories drawn from $ \sigma$.

   So consider any trajectory $\tau = (s_1, \ba_1, \br_1, \ldots, s_H, \ba_H, \br_H)$ consisting of a sequence of $H$ states and actions and rewards for each of the $m$ agents. Assume that $r_{i,h} = R_{i,h}(s, \ba_h)$ for all $i, h$ (as otherwise $\tau$ has probability 0 under any policy). Write:
  \begin{align}
p_\tau := \prod_{h=1}^{H-1} \BP_h(s_{h+1} | s_h,\ba_h)\nonumber.
  \end{align}
  Then the probability of observing $\tau$ under $ \sigma$ is
  \begin{align}
    &    p_\tau \cdot \prod_{h=1}^{H} \prod_{j=1}^m  \sigma_{j,h}(a_{j,h} | \tau_{j,h-1}, s_h)\nonumber\\
    =& p_\tau \cdot \prod_{j=1}^m  \prod_{h=1}^{H} \frac{\til\sigma_j \left( \{ \pi_j \in \Pidet_j \ : \ \pi_j(\tau_{j,g}, s_g) = a_{j,g} \ \forall g \leq h \} \right)}{\til\sigma_j \left( \{ \pi_j \in \Pidet_j \ : \ \pi_j(\tau_{j,g}, s_g) = a_{j,g} \ \forall g \leq h-1 \} \right)}\nonumber\\
    =& p_\tau \cdot \prod_{j=1}^m \til\sigma_j \left( \{ \pi_j \in \Pidet_j \ : \ \pi_j(\tau_{j,g}, s_g) = a_{j,g} \ \forall g \leq H \} \right)\nonumber,
  \end{align}
  which is equal to the probability of observing $\tau$ under $\til\sigma$. 
\end{proof}



\end{document}
