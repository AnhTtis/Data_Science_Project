
Theorems \ref{thm:markov-formal}, \ref{thm:nonmarkov-formal}, and \ref{thm:statistical-lb} present barriers---both computational and statistical---toward developing efficient decentralized no-regret guarantees for multi-agent reinforcement learning. We emphasize that no-regret algorithms are the only known approach for obtaining fully decentralized learning algorithms (i.e., those which do not rely even on shared randomness) in normal-form games, and it seems unlikely that a substantially different approach would work in Markov games. Thus, these lower bounds for finding subexponential-length sequences of policies with the no-regret property represent a significant obstacle for fully decentralized multi-agent reinforcement learning. 
Moreover, these results rule out even the prospect of developing efficient \emph{centralized} algorithms that produce no-regret sequences of policies, i.e., those which ``resemble'' independent learning.
In this section, we compare our lower bounds with recent upper bounds for decentralized learning in Markov games, and explain how to reconcile these results.

\dfcomment{Based on icml reviewer discussion: recap significance of no-regret in the context of decentralized algorithms.}\noah{added it above}

\dfcomment{Based on icml reviewer discussion: We may want to add a discussion around why our lower bounds are different in nature from \citet{daskalakis2022complexity,jin2022complexity}, even though it is obvious if you're familiar.}\noah{added it at end of section}

\subsection{Comparison to \vlearning}

The \texttt{V-learning} algorithm \cite{jin2021vlearning,song2022when,mao2021provably} is a polynomial-time decentralized learning algorithm that proceeds in two phases. In the first phase, the $m$ agents interact over the course of $K$ episodes in a decentralized fashion, playing product Markov policies $\sigma\^1, \ldots, \sigma\^K \in \PiMarkov$. In the second phase, the agents use data gathered during the first phase to produce a distributional policy $\wh \sigma \in \Delta(\Pirndrnd)$, which we refer to as the \emph{output policy} of \texttt{V-learning}. As discussed in Section \ref{sec:intro}, one implication of Theorem \ref{thm:markov-formal} is that the first phase of \texttt{V-learning} cannot guarantee each agent sublinear regret. Indeed if $K$ is of polynomial size (and $\PPAD \neq \PP$), this follows because a bound of the form $\Reg_{i,K}(\sigma\^1, \ldots, \sigma\^K) \leq \ep K$ for all $i$ implies that $(\sigma\^1, \ldots, \sigma\^K)$ solves the $(K, \ep)$-\MarkCCE problem.

The output policy $\wh{\sigma}\in\Delta(\Pirndrnd)$ produced by \vlearning is an approximate CCE (per Definition \ref{def:cce}), and it is natural to ask how many product policies it takes to represent $\sigmahat$ as a uniform mixture (that is, whether $\sigmahat$ solves the $(T,\ep)$-\MarkCCE problem for a reasonable value of $T$).
First, recall that \texttt{V-learning} requires $K = \poly(H, S, \max_i A_i) / \ep^2$ episodes to ensure that $\wh \sigma$ is an $\ep$-CCE. It is straightforward to show that $\wh \sigma$ can be expressed as a \emph{non-uniform} mixture of at most $K^{KHS+1}$ policies in $\Pirndrnd$ (we prove this fact in detail below). By discretizing the non-uniform mixture, one can equivalently represent it as \emph{uniform} mixture of $O(1/\ep) \cdot K^{KHS+1}$ product policies, up to $\eps$ error. Recalling the value of $K$, we conclude that we can express $\sigmahat$ as a uniform mixture of $T = \exp( \til O(1/\ep^2)\cdot \poly(H, S, \max_i A_i))$ product policies in $\Pirndrnd$. %
Note that the lower bound of Theorem \ref{thm:nonmarkov-formal} rules out the efficient computation of an $\eps$-CCE represented as a uniform mixture of $T \ll \exp(\ep^2 \cdot \max\{H, S, \max_i A_i \})$ efficiently computable policies in $\Pirndrnd$. Thus, in the regime where $1/\ep$ is polynomial in $H, S, \max_i A_i$, %
this upper bound on the sparsity of the policy $\wh \sigma$ produced \texttt{V-learning} matches that from Theorem \ref{thm:nonmarkov-formal}, up to a polynomial in the exponent. 

\paragraph{The sparsity of the output policy from \texttt{V-learning}.}
\label{sec:vlearning-output}
We now sketch a proof of the fact that the output policy $\wh \sigma$ produced by \texttt{V-learning} can be expressed as a (non-uniform) average of $K^{KHS+1}$ policies in $\Pirndrnd$, where $K$ is the number of episodes in the algorithm's initial phase. We adopt the notation and terminology from \citet{jin2021vlearning}.

Consider Algorithm 3 of \citet{jin2021vlearning}, which describes the second phase of \texttt{V-learning}, which produces the output policy $\wh \sigma$. We describe how to write $\wh \sigma$ as a weighted average of a collection of product policies, each of which is indexed by a function $\phi : [H] \times \MS \times [K] \ra [K]$ and a parameter $k_0 \in [K]$: in particular, we will write $\wh \sigma = \sum_{k_0, \phi} w_{k_0, \phi} \cdot \sigma_{k_0, \phi} \in \Delta(\Pirndrnd)$, where $w_{k_0, \phi} \in [0,1]$ are mixing weights summing to 1 and $\sigma_{k_0, \phi} \in \Pirndrnd$. The number of tuples $(k_0, \phi)$ is $K^{1+KHS}$.

We define the mixing weight allocated $w_{k_0, \phi}$ to any tuple $(k_0,\phi)$ to be:
\begin{align}
\arxiv{w_{k_0, \phi} :=} \frac{1}{K} \cdot \prod_{(h,s,k) \in [H] \times \MS \times [K]} \One{\phi(h,s,k) \in [N_h^k(s)]} \cdot \alpha_{N_h^k(s)}^{\phi(h,s,k)}\nonumber,
\end{align}
where $N_h^k(s) \in [K]$ and $\alpha_{N_h^k(s)}^i \in [0,1]$ (for $i \in [N_h^k(s)]$) are defined as in \cite{jin2021vlearning}.

Next, for each $k_0, \phi$, we define $\sigma_{k_0, \phi} \in \Pirndrnd$ to be the following policy: it maintains a parameter $k \in [K]$ over the first $h\leq{}H$ steps of the episode (as in Algorithm 3 of \cite{jin2021vlearning}), but upon reaching state $s$ at step $h$, given the present value of $k \in [K]$, sets $i := \phi(h, s, k)$, and updates $k \gets k_h^i(s)$, and then samples an action $\ba \sim \pi_h^k(\cdot | s)$ (where $k_h^i(s), \pi_h^k(\cdot | s)$ are defined in \cite{jin2021vlearning}). Since the mixing weights $w_{k_0, \phi}$ defined above exactly simulate the random draws of the parameter $k$ in Line 1 and the parameters $i$ in Line 4 of \cite[Algorithm 3]{jin2021vlearning}, it follows that the distributional policy $\wh \sigma$ defined by \cite[Algorithm 3]{jin2021vlearning} is equal to $\sum_{k_0, \phi} w_{k_0, \phi} \cdot \sigma_{k_0, \phi} \in \Delta(\Pirndrnd)$. \dfcomment{not clear where $\pi_h^k(\cdot\mid{}s)$ is defined above}\noah{added rmk saying it's defined in v-learning paper}

\subsection{No-regret learning against Markov deviations}

As discussed in Section \ref{sec:intro}, \citet{erez2022regret} showed the existence of a learning algorithm with the property that if each agent plays it independently for $T$ episodes, then no player can achieve regret more than $O(\poly(m,H,S, \max_i A_i) \cdot T^{3/4})$ by deviating to any fixed \emph{Markov policy}. This notion of regret corresponds to, in the context of Definition \ref{def:regret}, replacing $\max_{\sigma_i \in \Pirndrnd_i}$ with the smaller quantity $\max_{\sigma_i \in \PiMarkov_i}$. Thus, the result of \citet{erez2022regret} applies to a weaker notion of regret than that of the \GenCCE problem, and so does not contradict any of our lower bounds. One may wonder which of these two notions of regret (namely, best possible gain via deviation to a Markov versus non-Markov policy) is the ``right'' one. We do not believe that there is a definitive answer to this question, but we remark that in many empirical applications of multi-agent reinforcement learning it is standard to consider non-Markov policies \citep{leibo2021scalable,agapiou2022melting}. Furthermore, as shown in the proposition below, there are extremely simple games, e.g., of constant size, in which Markov deviations lead to ``vacuous'' behavior: in particular, all Markov policies have the same (suboptimal) value but the best non-Markov policy has much greater value:
\begin{proposition}
  \label{prop:nonmarkov-deviation}
There is a 2-player, 2-action, 1-state Markov game with horizon $2$ and a non-Markov policy $\sigma_2 \in \Pirndrnd_2$ for player 2 so that for all $\sigma_1 \in \PiMarkov_1$,  $ V_1^{\sigma_1 \times \sigma_2} = 1/2$ yet $\max_{\sigma_1 \in \Pirndrnd_1} \left\{ V_1^{\sigma_1 \times \sigma_2} \right\} = 3/4$.%
\end{proposition}
The proof of Proposition \ref{prop:nonmarkov-deviation} is provided in Section \ref{sec:discussion-proofs} below. 

Other recent work has also proved no-regret guarantees with respect to deviations to restricted policy classes. In particular,  \citet{zhan2022decentralized} studies a setting in which each agent $i$ is allowed to play policies in an arbitrary restricted policy class $\Pi_i' \subseteq \Pirndrnd_i$ in each episode, and regret is measured with respect to deviations to any policy in $\Pi_i'$. \citet{zhan2022decentralized} introduces an algorithm, \texttt{DORIS}, with the property that when all agents play it independently, each agent $i$ experiences regret $O\left(\poly(m, A, S, H) \cdot \sqrt{T \sum_{i=1}^m \log |\Pi_i'|}\right)$ to their respective class $\Pi'_i$.\footnote{Note that in the tabular setting, the sample complexity of \texttt{DORIS} \dfedit{(Corollary 1)} scales with the size $A$ of the \emph{joint} action set, since each player's value function class consists of the class of all functions $f : \MS \times \MA \ra [0,1]$, which has Eluder dimension scaling with $S \cdot A$, i.e., exponential in $m$. \noah{that paper has a remark on the top of p.16 saying they avoid curse of multi-agents, but I think that's wrong for the reason I wrote here...} \dfcomment{agreed--i think the covering number in their bound should scale with $A$ as well}}

\texttt{DORIS} is not computationally efficient, since it involves performing exponential weights over the class $\Pi_i'$, which requires space complexity $\abs{\Pi'_I}$. Nonetheless, one can compare the statistical guarantees the algorithm provides to our own results. Let $\PiMarkovdet_i \subset \PiMarkov_i$ denote the set of deterministic Markov policies of agent $i$, namely sequences $\pi_i = (\pi_{i,1}, \ldots, \pi_{i,H})$ so that $\pi_{i,h} : \MS \ra \MA_i$. 
In the case that $\Pi_i' = \PiMarkovdet_i$, $\Pi_i'$, we have $\log |\Pi_i'| = O(SH \log A_i)$, which means that \texttt{DORIS} obtains no-regret against Markov deviations when $m$ is constant, comparable to \citet{erez2022regret}.\footnote{\citet{erez2022regret} has the added bonus of computational efficiency, even for polynomially large $m$, though has the significant drawback of assuming that the Markov game is known.} However, we are interested in the setting in which each player's regret is measured with respect to all deviations in $\Pirndrnd_i$ (equivalently, $\Pidet_i$). Accordingly, if we take $\Pi_i' = \Pidet_i \subset \Pirndrnd_i$,\footnote{\texttt{DORIS} plays distributions over policies in $\Pi_i' = \Pidet_i$ at each episode, whereas in our lower bounds we consider the setting where a policy in $\Pirndrnd_i$ is played each episode; Facts \ref{fac:gen-decompose} and \ref{fac:emb-inverse} shows that these two settings are essentially equivalent, in that any policy in $\Pirndrnd_1 \times \cdots \times \Pirndrnd_m$ can be simulated by one in $\Delta(\Pidet_1) \times \cdots \times \Delta(\Pidet_m)$, and vise versa.} then $\log |\Pi_i'| > (SA_i)^{H-1}$, meaning that \texttt{DORIS} does not imply any sort of sample-efficient guarantee, even for $m=2$.

Finally, we remark that the algorithm \texttt{DORIS} \citep{zhan2022decentralized}, as well as the similar algorithm \texttt{OPMD} from earlier work of \citet{liu2022learning}, obtains the same regret bound stated above even when the opponents are controlled by (possibly adaptive) adversaries. However, this guarantee crucially relies on the fact that any agent implementing \texttt{DORIS} must observe the policies played by opponents following each episode; this feature is the reason that the regret bound of \texttt{DORIS} does not contradict the exponential lower bound of \citet{liu2022learning} for no-regret learning against an adversarial opponent. As a result of being restricted to this ``revealed-policy'' setting, \texttt{DORIS} is not a fully decentralized algorithm in the sense we consider in this paper.


\subsection{On the role of shared randomness}
A key assumption in our lower bounds for no-regret learning is that each of the joint policies $\sigma\^1, \ldots, \sigma\^T$ produced by the algorithm is a \emph{product policy}; such an assumption is natural, since it subsumes independent learning protocols in which each agent $i$ selects $\sigma_i\ind{t}$ without knowledge of $\sigma_{-i}\ind{t}$. Compared to general (stochastic) joint policies, product policies have the desirable property that, to sample a trajectory from $\sigma\^t = (\sigma_1\^t, \ldots, \sigma_m\^t) \in \Pirndrnd_1 \times \cdots \times \Pirndrnd_m = \Pirndrnd$, the agents do no require access to shared randomness. In particular, each agent $i$ can independently sample its action from $\sigma_i\^t$ at each of the $h$ steps of the episode. It is natural to ask how the situation changes if we allow the agents to use shared random bits when sampling from their policies, which corresponds to allowing $\sigma\ind{1},\ldots,\sigma\^T$ to be non-product policies. In this case, \texttt{V-learning} yields a positive result via a standard ``batch-to-online'' conversion: by applying the first phase of \texttt{V-learning} during the first $T^{2/3}$ episodes and playing trajectories sampled i.i.d.~from the output policy produced by \texttt{V-learning} during the remaining $T-T^{2/3}$ episodes (which requires shared randomness), it is straightforward to see that a regret bound of order $\poly(H, S, \max_i A_i) \cdot T^{2/3}$ can be obtained. Similar remarks apply to \texttt{SPoCMAR} \citep{daskalakis2022complexity}, which can obtain a slightly worse regret bound of order $\poly(H, S, \max_i A_i) \cdot T^{3/4}$ in the same fashion. In fact, the batch-to-online conversion approach gives a generic solution for the setting in which shared randomness is available. That is, \emph{the assumption of shared randomness eliminates any distinction between no-regret algorithms and (non-sparse) equilibrium computation algorithms}, modulo slight loss in rates. For this reason, the shared randomness assumption is too strong to develop any sort of distinct theory of no-regret learning.

\dfcomment{We may want to add a remark in this subsection that shared randomness is essentially equivalent to removing the sparsity assumption, or allowing for $(\infty,\eps)$-\GenCCE. I think we had a remark about this earlier in the paper, but we may have commented it out. we can move it here}\noah{in the context of decentralized learning, it's not clear to me this is the case -- given a decentralized no-regret procedure without shared randomness, it's not clear how to convert it to a decentralized no-regret procedure with much smaller $T$ and shared randomness.}

\subsection{Comparison to lower bounds for finding stationary CCE}
A separate line of work \citet{daskalakis2022complexity,jin2022complexity} has recently shown \PPAD-hardness for the problem of finding stationary Markov CCE in infinite-horizon discounted stochastic games. These results are incomparable with our own: stationary Markov CCE are not sparse (in the sense of Definition \ref{def:markcce}), whereas we do not require stationarity of policies (as is standard in the finite-horizon setting).



\subsection{Proof of Proposition \ref{prop:nonmarkov-deviation}}
\label{sec:discussion-proofs}
Below we prove Proposition \ref{prop:nonmarkov-deviation}.
\begin{proof}[Proof of Proposition \ref{prop:nonmarkov-deviation}]
  We construct the claimed Markov game $\MG$ as follows. The single state is denoted by $\mf s$; as there is only a single state, the transitions are trivial. We denote each player's action space as $\MA_1 = \MA_2 = \{1,2\}$. The rewards to player 1 are given as follows: for all $(a_1, a_2) \in \MA$, 
  \begin{align}
R_{1,1}(\mf s, (a_1, a_2)) = \frac 12 \cdot \indic{a_2 = 1}, \qquad R_{1,2}(\mf s, (a_1, a_2)) = \frac 12 \cdot \indic{a_1 = a_2} \nonumber.
  \end{align}
  We allow the rewards of player 2 to be arbitrary; they do not affect the proof in any way.

  We let $\sigma_2  = (\sigma_{2,1}, \sigma_{2,2})\in \Pirndrnd_2$ be the policy which plays a uniformly random action at step 1 and then plays the same action at step 2: formally, $\sigma_{2,1}(s_1) = \Unif(\MA_2)$, and $\sigma_{2,2}((s_1, a_{2,1}, r_{2,1}), s_2) = \indic{a_{2,1}}$. Then for any Markov policy $\sigma_1 \in \PiMarkov_1$ of player 1, we must have $\BP_{\sigma_1 \times \sigma_2}(a_{1,2} = a_{2,2}) = 1/2$, which means that $V_1^{\sigma_1 \times \sigma_2} = \frac 12 \cdot \E_{\sigma_1 \times \sigma_2}[\indic{a_{2,1} = 1} + \indic{a_{1,2} = a_{2,2}}] = 1/2 \cdot (1/2 + 1/2) = 1/2$.

  On the other hand, any general (non-Markov) policy $\sigma_1 \in \Pirndrnd_1$ which satisfies
  \begin{align}
    \sigma_{1,2}((s_1, a_{1,1}, r_{1,1}), s_2) = \begin{cases}
    \indic{1}: & r_{1,1} = 1/2  \\
    \indic{2}: & r_{1,1} = 0
  \end{cases}\nonumber
  \end{align}
  has $V_1^{\sigma_1 \times \sigma_2} = 1/2 \cdot (1/2 + 1) = 3/4$. 
\end{proof}











