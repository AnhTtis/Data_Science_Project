\noah{I feel like the paper was longer before, make sure nothing is getting cut out due to switch to arxiv}

The framework of \emph{multi-agent reinforcement learning
  (MARL)}, which describes settings in which multiple agents interact
in a dynamic environment, has played a key role in recent breakthroughs in artificial intelligence,
including the development of agents that approach or surpass human
performance in games such as Go \cite{silver2016mastering}, Poker
\cite{brown2018superhuman}, Stratego \cite{perolat2022mastering}, and
Diplomacy \cite{kramar2022negotiation,bakhtin2022human}. MARL also
shows promise for real-world multi-agent systems, including autonomous driving
\cite{shalevshwartz2016safe}, and cybersecurity
\cite{malialis2015distributed}, and economic policy
\cite{zheng2022ai}. These applications, where reliability is critical, necessitate the development of algorithms that are practical and efficient, yet provide strong formal guarantees and robustness.


Multi-agent reinforcement learning is typically studied using the framework of \emph{Markov games} (also known as \emph{stochastic games}) \cite{shapley1953stochastic}. In a Markov game, agents interact over a finite number of steps: at each step, each agent observes the \emph{state} of the environment, takes an \emph{action}, and observes a \emph{reward} which depends on the current state as well as the other agents' actions. Then the environment transitions to a new state as a function of the current state and the actions taken. An \emph{episode} consists of a finite number of such steps, and agents interact over the course of multiple episodes, progressively learning new information about their environment.
Markov games generalize the well-known model of \emph{Markov Decision Processes (MDPs)} \cite{puterman_markov_1994}, which describe the special case in which there is a single agent acting in a dynamic environment, and we wish to find a policy that maximizes its reward. %
By contrast, for Markov games, we typically aim to find a distribution over agents' policies which constitutes some type of \emph{equilibrium}.


  \subsection{Decentralized learning}

In this paper, we focus on the problem of \emph{decentralized} (or, independent) learning in Markov games. In decentralized MARL, each agent in the Markov game behaves independently, 
optimizing their policy myopically while treating the effects of the other agents as exogenous. Agents observe local
information (in particular, their own actions and rewards), but do not observe the actions of the other agents directly. %
Decentralized learning enjoys a number of desirable properties, including scalability\icmlcut{\xspace(computation is inherently linear in the number of agents)}, versatility\icmlcut{\xspace(by virtue of independence, algorithms can be applied in uncertain environments in which the nature of the interaction and number of other agents are not known)}, and practicality\icmlcut{\xspace(architectures for single-agent reinforcement learning can often be adapted directly)}. The central question we consider is whether there exist decentralized learning algorithms which, when employed by all agents in a Markov game, lead them to play near-equilibrium strategies over time.





\dfcomment{it might be good to mention below that we like CCE because Nash is intractable (since we will refer to nash in the next section of the intro).}\noah{did so in below paragraph}

Decentralized equilibrium computation in MARL is not well understood theoretically, and algorithms with provable guarantees are scarce. To motivate the challenges and most salient issues, it will be helpful to contrast with the simpler problem of decentralized learning in \emph{normal-form games}, which may be interpreted as Markov games with a single state. \icmlcut{Normal-form games enjoy a rich and celebrated theory of decentralized learning, dating back to Brown's work on fictitious play \cite{brown1949some} and Blackwell's theory of approachability \cite{blackwell1956analog}. }
Much of the modern work on decentralized learning in normal-form games centers on \emph{no-regret} learning, where agents select actions independently using \emph{online learning} algorithms \cite{cesa2006prediction} designed to minimize their \emph{regret} (that is, the gap between realized payoffs and the payoff of the best fixed action in hindsight).
In particular, a foundational result is that if each agent employs a no-regret learning strategy,
then the average of the agents' joint action distributions approaches a \emph{coarse correlated equilibrium (CCE)} for the normal-form game \cite{cesa2006prediction,hannan1957approximation,blackwell1956analog}. %
CCE is a natural relaxation of the foundational concept of \emph{Nash equilibrium}, which has the downside of being intractable to compute. 
On the other hand, there are many efficient algorithms that can achieve vanishing regret in a normal-form game, even when opponents select their actions in an arbitrary, potentially adaptive fashion, and thus converge to a CCE  \cite{vovk1990aggregating,littlestone1994weighted,cesa1997how,hart2000simple, syrgkanis2015fast}.

\dfcomment{I am on the fence about whether to shorten these two sentences on motivation for no-regret in normal-form games (or perhaps merge with the general motivation for decentralized learning in the first paragraph).}\noah{I think it's ok}

This simple connection between no-regret learning and decentralized convergence to equilibria has been influential in game theory, leading to numerous lines of research including
fast rates of convergence to equilibria
\cite{syrgkanis2015fast,chen2020hedging,daskalakis2021nearoptimal,anagnostides2022uncoupled}, price of anarchy bounds for smooth games \cite{roughgarden2015intrinsic}, and lower bounds on query and communication complexity for equilibrium computation \cite{fearnley2013learning,rubinstein2016settling,babichenko2017communication}.  Empirically, no-regret algorithms such as regret matching \cite{hart2000simple} and Hedge \cite{vovk1990aggregating,littlestone1994weighted,cesa1997how} have been used to compute equilibria that can achieve state-of-the-art performance in application domains such as Poker \cite{brown2018superhuman} and Diplomacy \cite{bakhtin2022human}. 
Motivated by these successes, we ask whether an analogous theory can be developed for Markov games. In particular:
\begin{center}
\emph{Are there efficient
  algorithms \icml{\\}for no-regret learning in Markov games?}
\end{center}
\icmlcut{
  Any Markov game can be viewed as a large normal-form game where each agent's action space consists of their exponentially-sized space of policies, and their utility function is given by their expected reward. Thus, any learning algorithm for normal-form games can also be applied to Markov games, but the resulting sample and computational complexities will be intractably large. Our goal is to explore whether more efficient decentralized learning guarantees can be established.
  }



\paragraph{Challenges for no-regret learning.}
In spite of active research effort and many promising pieces of progress \cite{jin2021vlearning,song2022when,mao2021provably,daskalakis2022complexity,erez2022regret}, no-regret learning guarantees for Markov games have been elusive. A barrier faced by naive algorithms is that
it is intractable to ensure no-regret against an \emph{arbitrary} adversary, both computationally \cite{bai2020nearoptimal,abbasi2013online} and statistically \cite{liu2022learning,kwon2021rl,foster2022complexity}.

Fortunately, many of the implications of no-regret learning (in particular, convergence to equilibria) do not require the algorithm to have sublinear regret against an arbitrary adversary, but rather only against other agents who are running the same algorithm independently. This observation has been influential in normal-form games, where the \arxiv{well-known }line of work on fast rates of convergence to equilibrium \cite{syrgkanis2015fast,chen2020hedging,daskalakis2021nearoptimal,anagnostides2022uncoupled} holds only in this more restrictive setting. This motivates the following relaxation to our central question.\loose
  \icml{\vspace{-5pt}}
\begin{problem}
  \label{pr:main-question}
Is there an efficient algorithm that, when adopted by all agents in a Markov game and run independently, leads to sublinear regret for each individual agent?
  \end{problem}
  \icml{\vspace{-5pt}}

  \paragraph{Attempts to address Problem
  \ref{pr:main-question}.}
Two recent lines of research have made progress toward addressing Problem
\ref{pr:main-question} and related questions. In one direction,
several recent papers have provided algorithms, including
\texttt{V-learning}
\cite{jin2021vlearning,song2022when,mao2021provably} and
\texttt{SPoCMAR} \cite{daskalakis2022complexity}, that do not achieve no-regret, but can nevertheless compute and then sample from a coarse
correlated equilibrium in a Markov game in a (mostly) \emph{decentralized}
fashion, with the caveat that they require a shared source of
  random bits as a mechanism to coordinate. Notably, \texttt{V-learning} depends only mildly on
the shared randomness: agents first play policies in a fully independent fashion (i.e.,
without shared randomness) according to a simple learning algorithm
for $T$ episodes, and use shared random bits only once learning finishes as part of a post-processing procedure to  extract a CCE policy. A question left open by these works, is whether the sequence of policies played by 
the \texttt{V-learning} algorithm in the initial independent phase can itself guarantee each
agent sublinear regret\icml{.}\arxiv{; this would eliminate the need for a separate
post-processing procedure and shared randomness.}



Most closely related to our work, \citet{erez2022regret} recently
showed that Problem \ref{pr:main-question} can be solved positively
for a restricted setting in which regret for each agent is defined as the maximum gain in
value they can achieve by deviating to a fixed \emph{Markov}
  policy. Markov policies are those whose choice of action depends
only on the current state as opposed to the entire history of
interaction. This notion of deviation is restrictive because in general, even when the opponent plays a sequence of Markov policies, the best response will be \emph{non-Markov}. In challenging settings that abound in practice, it is standard to consider non-Markov policies \cite{leibo2021scalable,agapiou2022melting}, since they often achieve higher value than Markov policies; we provide a simple example in Proposition \ref{prop:nonmarkov-deviation}.
Thus, while a regret guarantee with respect to the class of Markov policies (as in \cite{erez2022regret}) is certainly interesting, it may be too weak in general, and it is of great interest to understand whether Problem \ref{pr:main-question} can be answered positively in the general setting.\icmlfncut{We remark that the \texttt{V-learning} and \texttt{SPoCMAR} algorithms mentioned above do learn equilibria that are robust to deviations to non-Markov policies, though they do not address Problem \ref{pr:main-question} since they do not have sublinear regret.}




\subsection{Our contributions}
We resolve Problem
\ref{pr:main-question} in the negative, from both a computational and statistical perspective.

\dfcomment{It occurs to me that we never mention in the intro that we work in an episodic setting. I think it might be more clear if we do this, and then use ``episodes'' in place of iterations}\noah{good point, made this change}

\paragraph{Computational hardness.}
We provide two computational lower bounds (\cref{thm:markov-intro,thm:nonmarkov-intro}) which show that under standard complexity-theoretic assumptions, there is no efficient algorithm that runs for a polynomial number of episodes and guarantees each agent non-trivial (``sublinear'') regret when used in tandem by all agents. Both results hold even if the Markov game is explicitly known to the algorithm designer; \cref{thm:nonmarkov-intro} is stronger and more general, but applies only to $3$-player games, while \cref{thm:markov-intro} applies to $2$-player games, but only for agents restricted to playing Markovian policies.



To state our first result, \cref{thm:markov-intro}, we define a \emph{product Markov policy} to be a joint policy in which players choose their actions independently according to Markov policies (see Sections \ref{sec:prelim} and \ref{sec:markov} for formal
definitions). Note that if all players use independent no-regret algorithms to choose Markov policies at each episode, then their joint play at each round is described by a product Markov policy, since any randomness in each player's policy must be generated independently. %

\begin{theorem}[Informal version of Corollary \ref{cor:markov-formal-ppad}]
  \label{thm:markov-intro}
  If $\PPAD \neq \PP$, then there is no polynomial-time algorithm that, given the description of a 2-player Markov game, outputs a sequence of joint product Markov policies which guarantees each agent sublinear regret. %
\end{theorem}
\cref{thm:markov-intro} provides a decisive negative resolution to Problem
\ref{pr:main-question} under the assumption that $\PPAD\neq
\PP$,\footnote{Technically, the class we are denoting by $\PP$, namely of total search problems that have a deterministic polynomial-time algorithm, is sometimes denoted by $\mathsf{FP}$, as it is a search problem. We ignore this distinction.} which is standard in the theory of computational
complexity
\cite{papadimitriou1994complexity}.\footnote{\PPAD is the most well-studied complexity class in algorithmic game theory, and is widely believed to not admit polynomial time algorithms.
  Notably, the problem of computing a Nash equilibrium for normal-form games with two or more players is \PPAD-complete \cite{daskalakis2009complexity,chen2006computing,rubinstein2018inapproximability}.}
Beyond simply ruling out the existence of fully
decentralized no-regret algorithms, it rules out existence of
\emph{centralized} algorithms that compute a sequence of product 
policies for which each agent has sublinear regret, even if such a
sequence does not arise naturally as the result of agents
independently following some learning algorithm. Salient implications include:
\begin{itemize}
\item \cref{thm:markov-intro} provides a separation between Markov games and normal-form games, since standard no-regret algorithms for normal-form games i) run in polynomial time and ii) produce sequences of joint product policies that guarantee each agent sublinear regret. Notably, no-regret learning for  normal-form games is efficient whenever the number of agents is polynomial, whereas \cref{thm:markov-intro} rules out polynomial-time algorithms for as few as two agents.
  \item A question left open by the work of \citet{jin2021vlearning,song2022when,mao2021provably} was whether the sequence of policies played by 
    the \texttt{V-learning} algorithm during its independent learning phase can guarantee each
    agent sublinear regret.
  Since \texttt{V-learning}
  plays product Markov policies during the independent phase and is computationally efficient, \cref{thm:markov-intro} implies that these policies \emph{do not} enjoy sublinear regret (assuming $\PPAD \neq \PP$). 
\end{itemize}



Our second result, Theorem \ref{thm:nonmarkov-intro}, extends the
guarantee of Theorem \ref{thm:markov-intro} to the more general setting in
which agents can select arbitrary, potentially \emph{non-Markovian} policies at each episode. This comes at the cost of only providing hardness for
3-player games as opposed to 2-player games, as well as relying on the slightly stronger complexity-theoretic assumption that $\PPAD\nsubseteq\RP$.\footnote{We use $\RP$ to denote the class of total search problems for which there exists a polynomial-time randomized algorithm which outputs a solution with probability at least $2/3$, and otherwise outputs ``fail''.}
\begin{theorem}[Informal version of Corollary \ref{cor:nonmarkov-formal-ppad}]
  \label{thm:nonmarkov-intro}
    If $\PPAD \nsubseteq \RP$, then there is no polynomial-time algorithm that, given the description of a 3-player Markov game, outputs a sequence of joint product general policies (i.e., potentially non-Markov) which guarantees each agent sublinear regret. 
  \end{theorem}

\paragraph{Statistical hardness.}
\cref{thm:markov-intro,thm:nonmarkov-intro} rely on the widely-believed complexity
  theoretic assumption that $\PPAD$-complete problems cannot be solved
  in (randomized) polynomial time. Such a restriction is inherent if
  we assume that the game is known to the algorithm designer.
  To avoid complexity-theoretic assumptions, we consider a setting in which the Markov game is \emph{unknown} to the algorithm designer, and algorithms must learn about the game by executing policies (``querying'') and observing the resulting sequences of states, actions, and rewards.
  Our final result, \cref{thm:query-intro}, shows \emph{unconditionally} that, for $m$-player Markov games whose parameters
  are unknown, any algorithm computing a
  no-regret sequence as in Theorem \ref{thm:nonmarkov-intro} requires a number of queries that is exponential in $m$.
\begin{theorem}[Informal version of \cref{thm:statistical-lb}]
  \label{thm:query-intro}
  Given query access to a $m$-player Markov game, no algorithm that makes fewer than $2^{\Omega(m)}$ queries can output a sequence of joint product policies which guarantees each agent sublinear regret.
\end{theorem}
Similar to our computational lower bounds, \cref{thm:query-intro} goes far beyond decentralized algorithms, and rules out even centralized algorithms that compute a no-regret sequence by jointly controlling all players. The result provides another separation between Markov games and normal-form games, since standard no-regret algorithms for normal-form games can achieve sublinear regret using $\poly(m)$ queries for any $m$. The $2^{\Omega(m)}$ scaling in the lower bound, which does not rule out query-efficient algorithms when $m$ is constant, is to be expected for an unconditional result: If the game has only polynomially many parameters (which is the case for constant $m$), one can estimate all of the parameters using standard techniques \cite{jin2020reward}, then directly find a no-regret sequence.



\paragraph{Proof techniques: the \GenCCE problem.}
\icml{
Our proofs proceed via establishing lower bounds for a computational problem we refer to as \GenCCE. In the \GenCCE problem, the aim is to compute a CCE that can be represented as the mixture of a small number of product policies. See \cref{sec:markov,sec:nonmarkov} for detailed proof overview. 
  }

\icmlcut{
Rather than directly proving lower bounds for the problem of no-regret learning, we establish lower bounds for a simpler problem we refer to as \GenCCE. In the \GenCCE problem, the aim is to compute by any means---centralized, decentralized, or otherwise---a coarse correlated equilibrium that is ``sparse'' in the sense that it can be represented as the mixture of a small number of product policies. Any algorithm that computes a sequence of product policies with sublinear regret (in the sense of \cref{thm:nonmarkov-intro}) immediately yields an algorithm for the \GenCCE problem, as---using the standard connection between CCE and no-regret---the uniform mixture of the policies in the no-regret sequence forms a sparse CCE. Thus, any lower bound for the sparse \GenCCE problem yields a lower bound for computation of no-regret sequences.}

\icmlcut{
To provide lower bounds for the \GenCCE problem, we reduce from the problem of Nash equilibrium computation in normal-form games. We show that given any two-player normal-form game, it is possible to construct a Markov game (with two players in the case of \cref{thm:markov-intro} and three players in the case of \cref{thm:nonmarkov-intro}) with the property that i) the description length is polynomial in the description length of the normal-form game, and ii) any (approximate) \GenCCE for the Markov game can be efficiently transformed into a approximate Nash equilibrium for the normal-form game. With this reduction established, our computational lower bounds follow from celebrated \PPAD-hardness results for approximate Nash equilibrium computation in two-player normal-form games, and our statistical lower bounds follow from query complexity lower bounds for Nash. Proving the reduction from Nash to \GenCCE constitutes the bulk of our work, and makes novel use of aggregation techniques from online learning \cite{vovk1990aggregating,cesa2006prediction}, as well as techniques from the literature on anti-folk theorems in game theory \cite{borgs2008myth}.
}







  


\icml{
\paragraph{Organization.} \cref{sec:prelim} presents preliminaries, and \cref{sec:markov,sec:nonmarkov} provide our computational lower bounds. \textbf{\emph{Due to space limitations, our statistical lower bounds for multi-player Markov games are in \cref{sec:multiplayer}.}}
  }
\icmlcut{
  \subsection{Organization}
  \cref{sec:prelim} presents preliminaries on no-regret learning and equilibrium computation in Markov games and normal-form games. \cref{sec:markov,sec:nonmarkov,sec:multiplayer} present our main results:
  \begin{itemize}
  \item \cref{sec:markov,sec:nonmarkov} provide our computational lower bounds for no-regret in Markov games. \cref{sec:markov} gives a lower bound for the setting in which algorithms are constrained to play Markovian policies, and \cref{sec:nonmarkov} builds on the approach in this section to give a lower bound for general, potentially non-Markovian policies.
  \item \cref{sec:multiplayer} provides statistical (query complexity) lower bounds for multi-player Markov games.
  \end{itemize}
  Proofs are deferred to the appendix unless otherwise stated.
}
\dfcomment{Add a sentence here in bold telling people to look at Part I of the appendix.}\noah{did so}


  



\noahold{Main motivating work about no-regret learning in MDPs:
  \begin{itemize}
  \item In the worst case, it is computationally hard (due to hardness of Latent MDPs) and statistically hard (Chi's paper on adversarial MDPs, which uses statistical hardness of latent MDPs; also adversarial DEC paper).
  \item Question we consider (considered by many works in literature in other simpler online learning settings): is there some ``independent learning algorithm'' (broadly construed; see prelims for formal defn) so that if each player in a MG plays it, they have no regret? This avoids above upper bounds since the opponent is no longer acting adversarially.
\item Upper bounds against general policy classes: V-learning, spocmar give decentralized algorithms, though not no-regret (playing the output policy requires sharing random bits -- this is a very important point)!
\item Upper bounds against Markovian policies: recent work by Erez et al shows that you can get no-regret independent learning algorithm (i.e., don't share any random bits during the course of the algorithm) if (a) you know (or can explore) the MG, and (b) your comparison class is Markov policies. In particular, the policies' average is a CCE (nice!). 
\item But remember that the best response to a mixture of markov policies is in general non-Markov -- so can you remove the requirement of Erez et al of competing against class of Markov policies?

  We should drive the point home that in many settings (esp in game theory/economics settings, where people consider repeated games; also POMGs, cite Deepmind's melting pot), history dependent policies are the norm.
\item Our answer is no:

  (a) first show this if the individual policies played by the algorithm are Markov (product) policies. (Simpler proof)

  (b) More involved proof applies to when the inidividual policies played by the algorithm are general product policies.
  \end{itemize}
}

\noahold{Also mention statistical implications for $n$-player games.}

\noahold{Open questions:
  \begin{itemize}
  \item $O(1)$-player, $O(1)$-action games: can we show lower bounds? No, there's actually upper bounds: you can find Nash equilibria in polynomial time. 
  \item 2-player  games with non-Markov policies: here I expect there is an upper bound (for sparse CCE, as long as accuracy is $> 1/H$). 
  \end{itemize}
}
  \paragraph{Notation.} 
  For $n \in \BN$, we write $[n] := \{ 1, 2, \ldots, n\}$. For a finite set $\MT$, $\Delta(\MT)$ denotes the space of distributions on $\MT$. For an element $t\in\cT$, $\indic{t}\in\Delta(\cT)$ denotes the delta distribution that places probability mass $1$ on $t$. We adopt standard
        big-oh notation, and write $f=\bigoht(g)$ to denote that $f =
        \bigoh(g\cdot{}\max\crl*{1,\mathrm{polylog}(g)})$, with $\bigom(\cdot)$ and $\bigomt(\cdot)$ defined analogously.


