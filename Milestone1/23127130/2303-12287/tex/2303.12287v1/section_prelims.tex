
This section contains preliminaries necessary to present our main
results. We first introduce the Markov game framework
(\cref{sec:markov_games}), then provide a brief review of normal-form
games (\cref{sec:normal_form}), and finally introduce the concepts of
coarse correlated equilibria and regret minimization (\cref{sec:regret}).
  
  

\subsection{Markov games}
\label{sec:markov_games}

  We consider general-sum Markov games in a finite-horizon, episodic
  framework. For $m \in \BN$, an $m$-player Markov game $\MG$ consists of a tuple $\MG = (\MS,H,  (\MA_i)_{i \in [m]} , \BP, (R_i)_{i \in [m]}, \mu)$, where:\loose
  \begin{itemize}
  \item $\MS$ denotes a finite state space and $H \in \BN$ denotes a finite time horizon. We write $S := |\MS|$. 
  \item For $i \in [m]$, $\MA_i$ denotes a finite action space for agent $i$. We let $\MA := \prod_{i=1}^m \MA_i$ denote the \emph{joint action space} and $\MA_{-i} := \prod_{i' \neq i} \MA_{i'}$.
    We denote joint actions in \arxiv{boldface}\icml{bold}, \arxiv{for example}\icml{e.g.}, $\ba = (a_1, \ldots, a_m) \in \MA$. We write $A_i \ldef |\MA_i|$ and $A \ldef |\MA|$. 
  \item $\BP = (\BP_1, \ldots, \BP_H)$ is the transition kernel, with
    each $\BP_h : \MS \times \MA \ra \Delta(\MS)$ denoting the kernel
    for step $h\in\brk{H}$. In particular, $\BP_h(s' | s, \ba)$ is the probability of transitioning to $s'$ from the state $s$ at step $h$ when agents play $\ba$.
  \item For $i \in [m]$ and $h \in [H]$, $R_{i,h} : \MS \times \MA \ra
    [-1/H,1/H]$ is the \arxiv{instantaneous reward function of}\icml{reward function for} agent
    $i$:\footnote{We assume that rewards lie in $\brk*{-1/H,1/H}$ for
      notational convenience, as this ensures that the cumulative reward for each
      episode lies in $\brk*{-1,1}$. This assumption is not important
      to our results.}
the reward agent $i$
    receives in state $s$ at step $h$ if agents play
    $\ba$ is \arxiv{given by }$R_{i,h}(s, \ba)$.\footnote{\arxiv{We consider Markov games in which the rewards at
      each step are a deterministic function of the state and action
      profile. While some works consider the more general case of
      stochastic rewards, since our main goal is to prove lower
      bounds, it is without loss for us to assume that rewards are deterministic.}\icml{We restrict our attention to Markov games in which the rewards at each step are a deterministic function of the state and action profile. Since our goal is to prove lower bounds, this is without loss.}}\loose
  \item $\mu \in \Delta(\MS)$ denotes the initial state distribution. 
  \end{itemize}
  An \emph{episode} in the Markov game proceeds as follows:
  \icml{
   the initial state $s_1$ is drawn from the initial state
    distribution $\mu$.
    Then, for each $h
    \leq H$, given state $s_h$, each agent $i$ plays action $a_{i,h} \in
    \MA_i$, and given the joint action profile $\ba_h = (a_{1,h}, \ldots,
    a_{m,h})$, each agent $i$ receives reward of $r_{i,h}= R_{i,h}(s_h,
    \ba_h)$ and the state of the system transitions to $s_{h+1} \sim
    \BP_h(\cdot | s_h, \ba_h)$.
    }
  \arxiv{
  \begin{itemize}
  \item the initial state $s_1$ is drawn from the initial state
    distribution $\mu$.
    \item For each $h
    \leq H$, given state $s_h$, each agent $i$ plays action $a_{i,h} \in
    \MA_i$, and given the joint action profile $\ba_h = (a_{1,h}, \ldots,
    a_{m,h})$, each agent $i$ receives reward of $r_{i,h}= R_{i,h}(s_h,
    \ba_h)$ and the state of the system transitions to $s_{h+1} \sim
    \BP_h(\cdot | s_h, \ba_h)$.
  \end{itemize}
  }
  We denote the tuple of agents' rewards at each step $h$ by $\br_h =
  (r_{1,h}, \ldots, r_{m,h})$, and refer to the resulting sequence
$\tau_H\ldef{}(s_1,\ba_1,\br_1),\ldots,(s_H,\ba_H,\br_H)$ as a
\emph{trajectory}. For $h\in\brk{H}$, we define the prefix of the
trajectory via
$\tau_h\ldef{}(s_1,\ba_1,\br_1),\ldots,(s_h,a_h,r_h)$.

\arxiv{\paragraph{Indexing.} }
  We use the following notation: for some quantity $x$ (e.g., action, reward, etc.) indexed by agents, i.e., $x = (x_1, \ldots, x_m)$, and an agent $i \in [m]$, we write $x_{-i} = (x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_m)$ to denote the tuple consisting of all $x_{i'}$ for $i' \neq i$.
  
 



  \subsection{Policies and value functions}
  \label{sec:polval-prelim}
We now introduce the notion of policies and value functions for Markov
games. Policies are mappings from states (or sequences of states) to
actions for the agents. We consider several different types of policies, which play a
  crucial role in distinguishing the types of equilibria that are
  tractable and those that are intractable to compute
  efficiently. 

  \paragraph{Markov policies.} A randomized \emph{Markov policy} for
 agent $i$ is a sequence $\sigma_i = (\sigma_{i,1}, \ldots,
\sigma_{i,H})$, where $\sigma_{i,h} : \MS \ra \Delta(\MA_i)$. We
denote the space of randomized Markov policies for agent $i$ by
$\PiMarkov_i$. We write $\PiMarkov := \PiMarkov_1 \times \cdots \times
\PiMarkov_m$ to denote the space of \emph{product Markov policies}, which are
joint policies in which each agent $i$ independently follows a policy
in $\PiMarkov_i$. In
particular, a policy $\sigma\in \PiMarkov$ is specified by a
collection $\sigma = (\sigma_1, \ldots, \sigma_H)$, where $\sigma_h :
\MS \ra \Delta(\MA_1) \times \cdots \times \Delta(\MA_m)$.  We
additionally define $\PiMarkov_{-i} := \prod_{i' \neq i}
\PiMarkov_{i'}$, and for a policy $\sigma \in \PiMarkov$, write
$\sigma_{-i}$ to denote the collection of mappings $\sigma_{-i} =
(\sigma_{-i,1}, \ldots, \sigma_{-i,H})$, where $\sigma_{-i,h} : \MS
\ra \prod_{i' \neq i} \Delta(\MA_{i'})$ denotes the tuple of all but
player $i$'s policies.


When the Markov game $\cG$ is clear from context, for a policy
$\sigma\in\PiMarkov$ we let $\bbP_{\sigma}\brk*{\cdot}$
denote the law of the trajectory $\tau$ when players select actions
via $\ba_{h}\sim{}\sigma(s_h)$, and let $\En_{\sigma}\brk*{\cdot}$
denote the corresponding expectation.

    \paragraph{General (non-Markov) policies.} In addition to Markov
    policies, we will consider general \emph{history-dependent} (or,
    \emph{non-Markov}) policies, which select actions based on the
    \emph{entire sequence of states and actions} observed up the
    current step. To streamline notation, %
  for $i \in [m]$, let $\tau_{i,h} = (s_1, a_{i,1}, r_{i,1}, \ldots,
  s_h, a_{i,h}, r_{i,h})$ denote the history of agent $i$'s states,
  actions, and reward up to step $h$. Let $\CH_{i,h} = (\MS \times
  \MA_i \times [0,1])^h$ denote the space of all possible histories of
  agent $i$ up to step $h$. %
  For $i \in [m]$, a \emph{randomized general (i.e., non-Markov)
    policy of agent $i$} is a collection of mappings $\sigma_i =
  (\sigma_{i,1}, \ldots, \sigma_{i,H})$ where $\sigma_{i,h} :
  \CH_{i,h-1} \times \MS \ra \Delta(\MA_i)$ is a mapping that takes
  the history observed by agent $i$ up to step $h-1$ and the current
  state and outputs a distribution over actions for agent
  $i$.\loose

  We denote by $\Pirndrnd_i$ the space of randomized general policies
  of agent $i$, and further write $\Pirndrnd := \Pirndrnd_1 \times
  \cdots \times \Pirndrnd_m$ to denote the space of product general
  policies; note that $\PiMarkov_i \subset \Pirndrnd_i$ and $\PiMarkov
  \subset \Pirndrnd$. In particular, a policy $\sigma \in \Pirndrnd$ is
  specicfied by a collection $(\sigma_{i,h})_{i \in [m], h \in [H]}$,
  where $\sigma_{i,h} : \CH_{i,h-1} \times \MS \ra
  \Delta(\MA_i)$. When agents play according to a general policy
  $\sigma\in \Pirndrnd$, at each step $h$, each agent, given the
  current state $s_h$ and their history $\tau_{i,h-1} \in
  \CH_{i,h-1}$, chooses to play an action $a_{i,h} \sim
  \sigma_{i,h}(\tau_{i,h-1}, s_h)$, independently from all other
  agents. For a policy $\sigma\in\Pirndrnd$, we let
  $\bbP_{\sigma}\brk*{\cdot}$ and $\En_{\sigma}\brk*{\cdot}$
denote the law and expectation operator for the trajectory $\tau$ when players select actions
via $\ba_{h}\sim{}\sigma(\tau_{h-1}, s_h)$, and write
  $\sigma_{-i}$ to denote the collection of policies of all agents but
  $i$, i.e., $\sigma_{-i} = (\sigma_{j,h})_{h \in [H], j \in
    [m]\backslash \{ i \}}$. 

  
  We will also consider distributions over product randomized general
  policies, namely elements of $\Delta(\Pirndrnd)$.\footnote{When
    $\cT$ is not a finite set, we take $\Delta(\cT)$ to be the set
    of Radon probability measures over $\cT$ equipped with the Borel $\sigma$-algebra.} We will refer to elements of $\Delta(\Pirndrnd)$ as \emph{distributional policies}. To play \arxiv{according to some}\icml{a} distributional policy $\distp\in\Delta(\Pirndrnd)$, agents draw a randomized policy $\sigma \sim \distp$ (so that $\sigma \in \Pirndrnd$) and then play \arxiv{according to }$\sigma$.

  \icmlcut{
  \begin{remark}[Alternative definition for randomized general policies]
    Instead of defining distributional policies as above, one might alternatively define $\Pirndrnd_i$ as the set of distributions over agent $i$'s deterministic general policies, namely as the set  $\Delta(\Pidet_i)$. We show in Section \ref{sec:equivalence} that this alternative definition is equivalent to our own: in particular, there is a mapping from $\Pirndrnd$ to $\Delta(\Pidet_1) \times \cdots \times \Delta(\Pidet_m)$ so that, for any Markov game, any policy $\sigma \in \Pirndrnd$ produces identically distributed trajectories to its corresponding policy in $\Delta(\Pidet_1) \times \cdots \times \Delta(\Pidet_m)$. Further, this mapping is one-to-one if we identify policies that produce the same distributions over trajectories for all Markov games. %
  \end{remark}
  }
  



  \icmlcut{
    \paragraph{Deterministic policies.}
  It will be helpful to introduce notation for
  \emph{deterministic} general (non-Markov) policies, which correspond
  to the special case of randomized policies where each policy $\sigma_{i,h}$ exclusively maps to singleton distributions. {In particular, a deterministic general policy of agent $i$ is 
  a collection of mappings $\pi_i =
  (\pi_{i,1}, \ldots, \pi_{i,H})$, where $\pi_{i,h} : \CH_{i,h-1} \times
  \MS \ra \MA_i$.} %
  We denote by $\Pidet_i$ the space of deterministic
  general policies of agent $i$, and further write $\Pidet:= \Pidet_1
  \times \cdots \times \Pidet_m$ to denote the space of \emph{joint
    deterministic policies}. {We use the convention throughout that
  deterministic policies are denoted by the letter $\pi$, whereas
  randomized policies are denoted by $\sigma$.}
}






  \paragraph{Value functions.} For a general policy
  $\sigma \in \Pirndrnd$, we define the value function for agent $i \in
  [m]$ as
    \icml{
$V_i^\sigma := \E_\sigma \left[ \sum_{h=1}^H R_{i,h}(s_h, \ba_h) \ \mid s_1 \sim \mu \right]$;%
  }
  \arxiv{
  \begin{align}
V_i^\sigma := \E_\sigma \left[ \sum_{h=1}^H R_{i,h}(s_h, \ba_h) \ \mid s_1 \sim \mu \right];\label{eq:valfn}
  \end{align}
  }
  this represents the expected reward that agent $i$ receives when
  each agent chooses their actions via $a_{i,h} \sim \sigma_h(\tau_{i,h-1}, s_h)$. For a distributional policy $\distp \in \Delta(\Pirndrnd)$, we extend this notation by defining $V_i^{\distp} := \E_{\sigma\sim \distp} [V_i^\sigma]$.

  \subsection{Normal-form games}
  \label{sec:normal_form}
  \noah{I feel like it might make sense to swap this subsection and the following one -- currently we're going back and forth between MGs and normal form.}
  \dfedit{To motivate the solution concepts we consider for Markov games, let us first revisit the notion of normal-form games, which may be interpreted as Markov games with a single state.}
  For $m,n \in \BN$, an \emph{$m$-player $n$-action normal-form game
    $G$} is specified by a tuple of $m$ \emph{\utility tensors} $M_1,
  \ldots, M_m \in [0,1]^{n \times \cdots \times n}$, where each tensor
  is of order $m$ (i.e., has $n^m$ entries). We will write $G = (M_1,
  \ldots, M_m)$.  We assume for simplicity that each player has the
  same number $n$ of actions, and identify each player's action space
  with $[n]$. Then an an action profile is specified by $\ba \in
  [n]^m$; if each player acts according to $\ba$, then the \utility for
  player $i \in [m]$ is given by $(M_i)_{\ba} \in [0,1]$. 
    Our hardness results will use the standard notion of \emph{Nash equilibrium} in normal-form games.       We define the \emph{$m$-player $(n,\ep)$-\Nash problem} to be the
  problem of computing an $\ep$-approximate Nash equilibrium of a given $m$-player
  $n$-action normal-form game. (See Definition \ref{def:nash} for a formal definition of $\ep$-Nash equilibrium.)  A celebrated result is that Nash equilibria are \PPAD-hard to approximate, i.e., the 2-player $(n, n^{-c})$-\Nash problem is \PPAD-hard for any constant $c > 0$  \cite{daskalakis2009complexity,chen2006computing}. We refer the reader to Section \ref{sec:nash-prelims} for further background on these concepts.


  
  

\arxiv{\subsection{Markov games: Equilibria, no-regret, and
    independent learning}}
\icml{\subsection{Markov games: Equilibria and no-regret}}
  \label{sec:regret}

  
We now turn our focus back to Markov games, and introduce the main
solution concepts we consider, as well as the notion of no-regret. Since computing Nash equilibria is intractable even for normal-form
games, much of the work on efficient equilibrium computation has
focused on alternative notions of equilibrium, notably \emph{coarse
  correlated equilibria}\icmlcut{{} and \emph{correlated equilibria}. We focus on coarse correlated equilibria: being a superset of
correlated equilibria, any lower bound for computing a coarse
correlated equilibrium implies a lower bound for computing a
correlated equilibrium}.


For a distributional policy $\distp \in \Delta(\Pirndrnd)$ and a
randomized policy $\sigma_i' \in \Pirndrnd_i$ of player $i$, we let
$\sigma_i' \times \distp_{-i} \in \Delta(\Pirndrnd)$ denote the
distributional policy which is given by the distribution of
$(\sigma_i', \sigma_{-i}) \in \Pirndrnd$ for  $\sigma \sim \distp$
(and $\sigma_{-i}$ denotes the marginal of $\sigma$ on all players but
$i$). For $\sigma \in \Pirndrnd$, we write $\sigma_i' \times
\sigma_{-i}$ to denote the policy given by $(\sigma_i', \sigma_{-i}) \in \Pirndrnd$.
 Let us fix a Markov game $\MG$, which in particular determines the players' value functions $V_i^\sigma$\arxiv{\xspace as in (\ref{eq:valfn})}.
\begin{defn}[Coarse correlated equilibrium]
  \label{def:cce}
  For $\ep > 0$, a distributional policy $\distp \in \Delta(\Pirndrnd)$ is defined to be an \emph{$\ep$-coarse correlated equilibrium (CCE)} if for each $i \in [m]$, it holds that \icml{$\max_{\sigma_i' \in \Pirndrnd_i} V_i^{\sigma_i' \times \distp_{-i}} - V_i^\distp \leq \ep$}.
  \arxiv{
  \begin{align}
\max_{\sigma_i' \in \Pirndrnd_i} V_i^{\sigma_i' \times \distp_{-i}} - V_i^\distp \leq \ep\nonumber.
  \end{align}
  }
\icmlcut{The maximizing policy $\sigma_i'$ can always be chosen to be determinimistic, so $\distp$ is an $\ep$-CCE if and only if $\max_{\pi_i \in \Pidet_i} V_i^{\pi_i \times \distp_{-i}} - V_i^\distp \leq \ep$. }
\end{defn}


Coarse correlated equilibria can be computed efficiently for both
normal-form games and Markov games, and are fundamentally connected to
the notion of no-regret and independent learning, which we now introduce.

\paragraph{Regret.}
For a policy $\sigma \in \Pirndrnd$, we denote the distributional
policy which puts all its mass on $\sigma$ by $\indic{\sigma} \in
\Delta(\Pirndrnd)$. Thus $\frac 1T \sum_{t=1}^T \indic{\sigma\^t} \in
\Delta(\Pirndrnd)$ denotes the distributional policy which randomizes
uniformly over the $\sigma\^t$.  We define \emph{regret} as follows.
  \begin{defn}[Regret]
    \label{def:regret}
    Consider a sequence of policies $\sigma\^1, \ldots, \sigma\^T \in \Pirndrnd$. For $i \in [m]$, the \emph{regret of agent $i$} with respect to this sequence is defined as:
    \begin{align}
\arxiv{\Reg_{i,T} = }\Reg_{i,T}(\sigma\^1, \ldots, \sigma\^T) = \max_{\sigma_i \in \Pirndrnd_i} \sum_{t=1}^T  V_i^{\sigma_i \times \sigma_{-i}\^t} -  V_i^{\sigma\^t} \label{eq:reg-defn}.
    \end{align}
\icmlcut{    In (\ref{eq:reg-defn}) the maximum over $\sigma_i \in \Pirndrnd_i$ is always achieved by a deterministic general policy, so we have $\Reg_{i,T} = \max_{\pi_i \in \Pidet_i} \sum_{t=1}^T \prn[\big]{ V_i^{\pi_i \times \sigma_{-i}\^t} - V_i^{\sigma\^t} }$. }
\end{defn}

\icml{It is immediate from the above definitions that a sequence of policies $\sigma\^1, \ldots, \sigma\^T \in \Pirndrnd$ satisfies $\Reg_{i,t}(\sigma\^1, \ldots, \sigma\^T) \leq \ep \cdot T$ if and only if the distributional policy $\ol \sigma := \frac 1T \sum_{t=1}^T \indic{\sigma\^t}$ is an $\ep$-CCE (stated formally in Fact \ref{fac:no-regret-cce} in the appendix).}

\icmlcut{The following standard result
shows that the uniform average of any no-regret sequence forms an
approximate coarse correlated equilibrium.



\begin{fact}[No-regret is equivalent to CCE]
  \label{fac:no-regret-cce}
 Suppose that a sequence of policies $\sigma\^1, \ldots, \sigma\^T\in
 \Pirndrnd$ satisfies $\Reg_{i,T}(\sigma\^1, \ldots, \sigma\^T ) \leq
 \ep \cdot T$ for each $i \in [m]$. Then the uniform average of these
 $T$ policies, namely the distributional policy $\ol \sigma :=
 \frac{1}{T} \sum_{t=1}^T \indic{\sigma\^t} \in \Delta(\Pirndrnd)$, is
 an $\ep$-CCE.

Likewise if a sequence of policies $\sigma\^1, \ldots, \sigma\^T\in
\Pirndrnd$ has the property that the distributional policy $\ol \sigma
:= \frac{1}{T} \sum_{t=1}^T \indic{\sigma\^t} \in \Delta(\Pirndrnd)$,
is an $\ep$-CCE, then we have $\Reg_{i,T}(\sigma\^1, \ldots, \sigma\^T ) \leq \ep \cdot T$ for all $i \in [m]$.
\end{fact}
}

\paragraph{No-regret learning.}
\icmlcut{Fact \ref{fac:no-regret-cce} is an immediate consequence of Definitions \ref{def:regret} and \ref{def:cce}. }
A standard approach to
decentralized equilibrium computation, which exploits Fact \ref{fac:no-regret-cce}, is to select
$\sigma\ind{1},\ldots,\sigma\ind{T} \in\Pirndrnd$ using independent \emph{no-regret
  learning} algorithms. A no-regret learning algorithm for player $i$ selects $\sigma_i\ind{t} \in \Pirndrnd_i$ based on the realized trajectories $\tau\ind{1}_{i,H},\ldots,\tau\ind{t-1}_{i,H} \in \CH_{i,H}$ that player $i$ observes over the course of play,\icmlfncut{An alternative model allows for player $i$ to have knowledge of the previous joint policies $\sigma\^1, \ldots, \sigma\^{t-1}$, when selecting $\sigma_i\^t$.}\xspace 
 but
with no knowledge of $\sigma\ind{t}_{-i}$, so as to ensure that
no-regret is achieved: $\Reg_{i,T}(\sigma\^1, \ldots, \sigma\^T ) \leq
\ep \cdot T$. If each player $i$ uses their own, independent no-regret
learning algorithm, this approach yields \emph{product policies}
$\sigma\ind{t}=\sigma_1\ind{t}\times\cdots\times\sigma\ind{t}_m$, and the uniform average of the $\sigma\^t$ yields a CCE as long as all of the players
can keep their regret small.\footnote{In \cref{sec:discussion}, we
  discuss the implications of relaxing the stipulation that
  $\sigma\^t$ be product policies \dfedit{(for example, by allowing
    the use of shared randomness, as in \texttt{V-learning})}. In short, allowing $\sigma\^t$ to be non-product
  essentially trivializes the problem.
  \label{fn:shared-randomness}}


\icml{
For the special case of normal-form games, there are several efficient
algorithms, %
which---when run independently---ensure that each player's regret after $T$ episodes is bounded
above by $O(\sqrt{T})$ (that is $\eps=\bigoh(1/\sqrt{T})$), even when
the other players' actions are chosen
\emph{adversarially}.
}

\arxiv{
For the special case of normal-form games, the no-regret learning approach
has been fruitful.
There are several efficient algorithms, including regret matching \cite{hart2000simple}, Hedge (also known as exponential weights) \cite{vovk1990aggregating,littlestone1994weighted,cesa1997how}, and generalizations of Hedge based on the
\emph{follow-the-regularized-leader (FTRL)} framework \cite{shalevshwartz2012online},
which ensure that each player's regret after $T$ episodes is  bounded
above by $O(\sqrt{T})$ (that is $\eps=\bigoh(1/\sqrt{T})$), even when
the other players' actions are chosen
\emph{adversarially}.
All of these guarantees, which bound regret by a sublinear function in
$T$, lead to efficient, decentralized computation of approximate
coarse correlated equilibrium in normal-form games. The success of
this motivates our central question, which is whether similar
guarantees may be established for Markov games. In particular, a
formal version of Problem \ref{pr:main-question} asks: \emph{Is there an efficient
algorithm that, when adopted by all agents in a Markov game and run
independently, ensures that for all $i$, $\Reg_{i,T}\leq\eps\cdot{}T$
for some $\eps=o(1)$?} \noah{This last sentence feels somewhat
repetitive -- I'm not sure if the change from Problem
\ref{pr:main-question}, namely the $\ep = o(1)$, adds much?}
\dfcomment{I wonder if we should formally define the model for
  \emph{independent} no-regret algorithms
  somewhere (in particular, what info they have access to when selecting
  $\sigma\ind{t}_i$). I kind of sketched this above, but it's a bit informal/vague.}\noah{there's not necessarily a fixed model, so not sure it makes sense to do so more formally -- I think what you wrote above is good.}
}












 