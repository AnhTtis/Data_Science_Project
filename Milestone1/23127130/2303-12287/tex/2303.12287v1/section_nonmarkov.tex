In this section, we prove Theorem \ref{thm:nonmarkov-intro} (restated formally below as Theorem \ref{thm:nonmarkov-formal}), which strengthens Theorem \ref{thm:markov-formal} by allowing the sequence $\sigma\^1, \ldots, \sigma\^T$ of product policies to be non-Markovian. This additional strength comes at the cost of our lower bound only applying to \emph{3-player} Markov games (as opposed to Theorem \ref{thm:markov-formal}, which applied to 2-player games). 

\subsection{\GenCCE problem and computational model}
   

To formalize the computational model for the \GenCCE problem, we must first describe how the non-Markov product policies $\sigma\^t =
(\sigma\^t_1, \ldots, \sigma\^t_m)$ are represented.
Recall that a non-Markov policy $\sigma_i\^t \in \Pirndrnd_i$ is, by definition, a mapping from agent $i$'s history and current state to a distribution over their next action. Since there are exponentially many possible histories, it is information-theoretically impossible to express an arbitrary policy in $\Pirndrnd_i$ with polynomially many bits. As our focus is on computing a sequence of such policies $\sigma\^t$ in polynomial time, certainly a prerequisite is that $\sigma\^t$ can be expressed in polynomial space. %
Thus, we adopt the representational assumption, stated formally in Definition \ref{def:computable-policy}, that each of the policies $\sigma_i\^t \in \Pirndrnd_i$ is described by a bounded-size circuit that can compute the conditional distribution of each next action given the history. This assumption is satisfied by essentially
all empirical and theoretical work concerning non-Markov policies (e.g.,
\cite{leibo2021scalable,agapiou2022melting,jin2021vlearning,song2022when}). %
\noah{may want to state that each player's output policies from V-learning do satisfy it as well, though nontrivially since you have to do dynamic programming to compute the distributions -- in particular, you compute the probability of going from each $k_h$ to each $k_{h'}'$ for all $h < h'$, for increasing $h'-h$, given the sequence of states/actions of agent $i$;  I will leave this for a later version in the interest of time.} 

\begin{defn}[Computable policy]
  \label{def:computable-policy}
  Given a $m$-player Markov game $\MG$ and $N \in \BN$, we say that a policy $\sigma_i \in \Pirndrnd_i$ is \emph{$N$-computable} if for each $h \in [H]$, there is a circuit of size $N$ that,\footnote{For concreteness, we suppose that ``circuit'' means ``boolean circuit'' as in \cite[Definition 6.1]{arora2006computational}, where probabilities are represented in binary. The precise model of computation we use does not matter, though, and we could equally assume that the policies $\sigma_i$ may be computed by Turing machines that terminate after $N$ steps.} on input $(\tau_{i,h-1}, s) \in \CH_{i,h-1} \times \MS$, outputs the distribution $\sigma_i(\tau_{i,h-1}, s) \in \Delta(\MA_i)$. A policy $\sigma = (\sigma_1, \ldots, \sigma_m) \in \Pirndrnd$ is $N$-computable if each constituent policy $\sigma_i$ is. \loose
\end{defn}
{\noah{Perhaps discuss that it is more or less equivalent for it to be a randomized circuit that outputs a sample from this distribution, since given that circuit we can learn the distribution to inverse poly accuracy, which should(?) be enough for the aggregation algorithm? -- leaving this for later.}}
Our lower bound applies to algorithms that produce sequences $\sigma\^1, \ldots, \sigma\^T$ for which each $\sigma\^t$ is $N$-computable, where the value $N$ is taken to be polynomial in the description length of the game $\MG$. For example, Markov policies whose probabilities can be expressed with $\beta$ bits are $O(HSA_i \beta)$-computable for each player $i$, since one can simply store each of the probabilities $\sigma_{i,h}(s_h, a_{i,h})$\icmlcut{\xspace (for $h \in [H]$, $i \in [m]$, $a_{i,h} \in \MA_i$, $s_h \in \MS$)}, each of which takes $\beta$ bits to represent. 



\paragraph{The \GenCCE problem.}
\GenCCE is the problem of computing a sequence of non-Markov product policies $\sigma\^1, \ldots, \sigma\^T$ such that the uniform mixture forms an $\eps$-approximate CCE. The problem generalizes \MarkCCE (Definition \ref{def:markcce}) by relaxing the condition that the policies $\sigma\^t$ be Markov.
   \begin{defn}[\GenCCE Problem]
     \label{def:nonmarkcce}
  For an $m$-player Markov game $\MG$ and parameters $T,N \in \BN$ and $\ep > 0$ (which may depend on the size of the game $\MG$), $(T, \ep,N)$-\GenCCE is the problem of finding a sequence $\sigma\^1, \ldots, \sigma\^T\in\Pirndrnd$, with each $\sigma\^t$ being $N$-computable, such that the distributional policy $\ol \sigma = \frac{1}{T} \sum_{t=1}^T \indic{\sigma\^t} \in \Delta(\Pirndrnd)$ is an $\ep$-CCE for $\MG$ (equivalently, such that for all $i \in [m]$, $\Reg_{i,T}(\sigma\^1, \ldots, \sigma\^T) \leq \ep \cdot T$).
   \end{defn}


   
   



   

\subsection{Main result}

Our main theorem for this section, Theorem \ref{thm:nonmarkov-formal}, shows that for appropriate values of $T$, $\ep$, and $N$, solving the $(T, \ep, N)$-\GenCCE problem is at least as hard as computing Nash equilibria in normal-form games. 

\begin{theorem}
  \label{thm:nonmarkov-formal}
  Fix $n \in \BN$, and let $\Tn, \Nn \in \BN$, and $\epn > 0$ satisfy $1 < \Tn < \exp \left( \frac{\epn^2 \cdot n}{16} \right)$. Suppose there exists an algorithm that, given the description of any $3$-player Markov game $\MG$ with $|\MG| \leq n$, solves the $(\Tn, \epn, \Nn)$-\GenCCE problem in time $U$, for some $U \in \BN$. Then, for any $\delta > 0$, the $2$-player $(\lfloor n/2 \rfloor, 50\epn)$-\Nash problem can be solved in randomized time $(n\Tn\Nn U \log(1/\delta)/\epn)^{C_0}$ with failure probability $\delta$, where $C_0>0$ is an absolute constant.
\end{theorem}
By analogy to Corollary \ref{cor:markov-formal-ppad}, we obtain the following immediate consequence.
\begin{corollary}[\GenCCE is hard under $\PPAD\nsubseteq\RP$]
  \label{cor:nonmarkov-formal-ppad}
For any \arxiv{constant} $C > 4$, if there is an algorithm which, given the description of a 3-player Markov game $\MG$, solves the $(|\MG|^C, |\MG|^{-\frac{1}{C}}, |\MG|^C)$-\GenCCE problem in time $\poly(|\MG|)$, then $\PPAD \subseteq \RP$.\loose
\end{corollary}










\paragraph{Proof overview for Theorem \ref{thm:nonmarkov-formal}.}
The proof of \cref{thm:nonmarkov-formal} has a similar high-level structure to that of \cref{thm:markov-formal}: given an $m$-player normal-form $G$, we define an $(m+1)$-player Markov game $\MG = \MG(G)$ which has $n_0 := \lfloor n/m \rfloor$ actions per player and horizon $H \approx n_0$. The key difference in the proof of \cref{thm:nonmarkov-formal} is the structure of the players' reward functions. To motivate this difference and the addition of an $(m+1)$-th player,
\icml{we explain why the proof of \cref{thm:markov-formal} fails to extend: a sequence $\sigma\^1, \ldots, \sigma\^T$ can hypothetically solve the \GenCCE problem by attempting to punish any one player's deviation policy, and thus avoid having to compute a Nash equilibrium of $G$. In particular, if player $i$ plays according to the policy $\pi_i^\dagger$ that we described in Section \ref{sec:markov-formal}, then other players $j \neq i$ can use the non-Markov property of $\sigma_j\^t$ to adjust their choice of actions in later rounds to decrease player $i$'s value.
  }
\icmlcut{let us consider what goes wrong in the proof of \cref{thm:markov-formal} when the policies $\sigma\^t$ are allowed to be non-Markov. We will explain how a sequence $\sigma\^1, \ldots, \sigma\^T$ can hypothetically solve the \GenCCE problem by attempting to punish any one player's deviation policy, and thus avoid having to compute a Nash equilibrium of $G$. In particular, for each player $j$, suppose $\sigma_j\^t$ tries to detect, based on the state transitions and player $j$'s rewards, whether every other player $i \neq j$ is playing according to $\sigma_i\^t$. If some player $i$ is not playing according to $\sigma_i\^t$ at some step $h$, then at steps $h' > h$, the policy $\sigma_j\^t$ can select actions that attempt to minimize player $i$'s rewards. In particular, if player $i$ plays according to the policy $\pi_i^\dagger$ that we described in Section \ref{sec:markov-formal}, then other players $j \neq i$ can adjust their choice of actions in later rounds to decrease player $i$'s value. \dfcomment{Are we supposed to be assuming that $\sigma_1\ind{t},\ldots,\sigma_T\ind{t}$ satisfy some property of interest above (eg, $\eps$-CCE)? Right now it is not immediately clear why it is useful to know that player $i$ is not playing according to $\sigma_i\ind{t}$. I guess the point we want to make clear is: we are punishing a hypothetical deviation policy in Def \ref{def:cce}. If we were not able to do this, it could mean that $\wb{\sigma}$ is not actually a CCE. But the fact that $\sigma$ are non-Markov makes, makes it easier for them to punish hypothetical deviations}\noah{added sentence accordingly}
}
  
\icml{
  \nocite{fudenberg1994folk}
  This behavior is reminiscent of ``tit-for-tat'' strategies which are used to establish the \emph{folk theorem} in the theory of repeated games \cite{maskin1986folk}. The folk theorem describes how Nash equilibria are more numerous in repeated games than in single-shot normal form games. As it turns out, the folk theorem does not yield to worst-case speedups in repeated games, when the number of players is at least 3. Indeed, \citet{borgs2008myth} gave an ``anti-folk theorem'', showing that computing Nash equilibria in $(m+1)$-player repeated games is \PPAD-hard for $m \geq 2$, via a reduction to $m$-player normal-form games. We adapt their reduction to our setting: roughly speaking, this approach adds an $(m+1)$-th player whose actions represent potential deviations for each of the $m$ players. The structure of the rewards ensures that if $\ol \sigma = \frac 1T \sum_{t=1}^T \indic{\sigma\^t}$ is an $\ep$-CCE, then for some policy $\pi_{m+1}^\dagger$ of the $(m+1)$-th player, the first $m$ players will play an approximate Nash of $G$ with constant probability, under a trajectory drawn from the joint policy $\ol \sigma_{-(m+1)} \times \pi_{m+1}^\dagger$. Thus, in order to efficiently find a Nash (see \cref{alg:3nash}), we need to simulate the policy $\ol \sigma_{-(m+1)} \times \pi_{m+1}^\dagger$, which involves running Vovk's algorithm. This approach is in contrast to the proof of \cref{thm:markov-formal}, which used Vovk's algorithm as an ingredient in the proof but not in the Nash computation algorithm. 
  }

\icmlcut{
This behavior is reminiscent of ``tit-for-tat'' strategies which are used to establish the \emph{folk theorem} in the theory of repeated games \cite{maskin1986folk,fudenberg1994folk}. The folk theorem describes how Nash equilibria are more numerous (and potentially easier to find) in repeated games than in single-shot normal form games. As it turns out, the folk theorem does not provably yield to worst-case computational speedups in repeated games, at least when the number of players is at least 3. Indeed, \citet{borgs2008myth} gave an ``anti-folk theorem'', showing that computing Nash equilibria in $(m+1)$-player repeated games is \PPAD-hard for $m \geq 2$, via a reduction to $m$-player normal-form games. We utilize their reduction, for which the key idea is as follows: given an $m$-player normal form game $G$, we construct an $(m+1)$-player Markov game $\MG(G)$ in which the $(m+1)$-th player acts as a \emph{kibitzer},\icmlfncut{Kibitzer is a Yiddish term for an observer who offers advice. \noah{need to cite dictionary?} \dfcomment{i think it's fine haha}} with actions indexed by tuples $(j, a_j)$, for $j \in [m]$ and $a_j \in \MA_j$. The kibitzer's action $(j,a_j)$ represents 1) a player $j$ to give advice to, and 2) their advice to the player, which is to take action $a_j$.
In particular, if the kibitzer plays $(j, a_j)$, it receives reward equal to the amount that player $j$ would obtain by deviating to $a_j$, and player $j$ receives the negation of the kibitzer's reward. Furthermore, all other players receive 0 reward.

\dfcomment{I think it would be good to remind below (or elsewhere) that 1) we draw $t^\st$ uniformly and want to learn it and 2) our goal is to show that the first $m$ players *do* often play nash equilibria.}\noah{did so below}

To see why the addition of the kibitzer is useful, suppose that $\sigma\^1, \ldots, \sigma\^T$ solves the \GenCCE problem, so that $\ol\sigma := \frac 1T \sum_{t=1}^T \indic{\sigma\^t}$ is an $\ep$-CCE. We will show that, with at least constant probability over a trajectory drawn from $\ol \sigma$ (which involves drawing $t^\st \sim [T]$ uniformly), the joint strategy profile played by the first $m$ players constitutes an approximate Nash equilibrium of $G$.
Suppose for the purpose of contradiction that this were not the case.
We show that there exists a non-Markov deviation policy $\pi_{m+1}^\dagger$ for the kibitzer which, similar to the proof of \cref{thm:markov-formal}, learns the value of $t^\st$ and plays a tuple $(j, a_j)$ such that action $a_j$ increases player $j$'s payoff in $G$, thereby increasing its own payoff. Even if the other players attempt to punish the kibitzer for this deviation, they will not be able to since, roughly speaking, the kibitzer game as constructed above has the property that for any strategy for the first $m$ players, the kibitzer can always achieve reward at least $0$.

The above argument shows that under the joint policy $\ol \sigma_{-(m+1)} \times \pi_{m+1}^\dagger$ (namely, the first $m$ players play according to $\ol \sigma$ and the kibitzer plays according to $\pi_{m+1}^\dagger$) then with constant probability over a trajectory  drawn from this policy, the distribution of the first $m$ players' actions is an approximate Nash equilibrium of $G$. Thus, in order to efficiently find such a Nash (see Algorithm \ref{alg:3nash}), we need to simulate the policy $\ol \sigma_{-(m+1)} \times \pi_{m+1}^\dagger$, which involves running Vovk's aggregating algorithm. This approach is in contrast to the proof of \cref{thm:markov-formal}, for which Vovk's aggregating algorithm was an ingredient in the proof but was not actually used in the Nash computation algorithm (Algorithm \ref{alg:2nash}). 
The details of the proof of correctness of \cref{alg:3nash} are somewhat delicate, and may be found in \cref{sec:nonmarkov-proof}.

\dfcomment{should we add one final sentence remarking that vovk is used algorithmically in the reduction, in this case?}\noah{did so}
}

\paragraph{Two-player games.} One intruiging question we leave open is whether the \GenCCE problem remains hard for two-player Markov games. Interestingly, as shown by \citet{littman2005polynomial}, there is a polynomial time algorithm to find an exact Nash equilibrium for the special case of repeated two-player normal-form games. Though their result only applies in the infinite-horizon setting, it is possible to extend their results to the finite-horizon setting, which 
rules out naive approaches to extend the proof of \cref{thm:nonmarkov-formal} and Corollary \ref{cor:nonmarkov-formal-ppad} to two players.  



