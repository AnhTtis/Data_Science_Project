
In this section we present Theorem \ref{thm:query-intro} (restated formally below as \cref{thm:statistical-lb}), which gives a statistical lower bound for the \GenCCE problem. The lower bound applies to any algorithm, regardless of computational cost, that accesses the underlying Markov game through a \emph{generative model}.
\begin{defn}[Generative model]
  \label{def:generative-model}
  For an $m$-player Markov game $\MG = (\MS, H, (\MA_i)_{i \in [m]}, \BP, (R_i)_{i \in [m]}, \mu)$, a \emph{generative model oracle} is defined as follows: given a query described by a tuple $(h, s, \ba) \in [H] \times \MS \times \MA$, the oracle returns the distribution $\BP_h(\cdot | s, \ba) \in \Delta(\MS)$ and the tuple of rewards  $(R_{i,h}(s, \ba))_{i \in [m]}$. 
\end{defn}
From the perspective of lower bounds, the assumption that the algorithm has access to
a generative model is quite reasonable, as it encompasses most standard access models in RL, including the online access model, in which the algorithm repeatedly queries a policy and observes a
trajectory drawn from it, as well as the \emph{local access generative model}
used in from \cite{yin2022efficient,weisz2021query}. We remark that it is slightly more standard to assume that queries to the generative model only return a \emph{sample} from the distribution $\BP_h(\cdot
| s, \ba)$ as opposed to the distribution itself \cite{kakade2003sample,kearns1999sparse}, but since our goal is to prove lower bounds, the notion in Definition \ref{def:generative-model} only makes our results stronger.



To state our main result, we recall the definition $|\MG| = \max\{ S, \max_{i \in [m]} A_i, H, \beta(\MG) \}$. In the present section, we consider the setting where the number of players $m$ is large. Here, $|\MG|$ does not necessarily correspond to the description length for $\cG$, and should be interpreted, roughly speaking, as a measure of the description complexity of $\cG$ $|\MG|$ with respect to \emph{decentralized} learning algorithms. In particular, from the perspective of an individual agent implementing a decentralized learning algorithm, their sample complexity should depend only on the size of their \emph{individual action set} (as well as the global parameters $S, H, \beta(\MG)$), as opposed to the size of the \emph{joint action set}, which grows exponentially in $m$; the former is captured by $\abs{\cG}$, while the latter is not. Indeed, a key advantage shared by much prior work on decentralized RL \cite{jin2021vlearning,song2022when,mao2021provably,daskalakis2022complexity} is their avoidance of the \emph{curse of multi-agents}, which describes the situation where an algorithm has sample and computational costs that scale exponentially in $m$.

Our main result for this section, Theorem \ref{thm:statistical-lb}, states that for $m$-player Markov games, exponentially many generative model queries (in $m$) are necessary to produce a solution to the $(T,\ep, N)$-\GenCCE problem, unless $T$ itself is exponential in $m$.
\begin{theorem}
  \label{thm:statistical-lb}
  Let $m\geq{}2$ be given.  There are constants $c, \ep > 0$ so that the following holds. 
 Suppose there is an algorithm $\CB$ which, given access to a generative model for a $(m+1)$-player Markov game $\MG$ with $|\MG| \leq 2m^6$, solves the $(T, \ep/(10m), N)$-\GenCCE problem for $\MG$ for some $T$ satisfying $1 < T < \exp(cm)$, and \emph{any} $N \in \BN$. Then $\CB$ must make at least $2^{\Omega(m)}$ queries to the generative model. 
\end{theorem}
Theorem \ref{thm:statistical-lb} establishes that there are $m$-player Markov games, where the number of states, actions per player, and horizon are bounded by $\poly(m)$, but any algorithm with regret $o(T/m)$ must make $2^{\Omega(m)}$ queries (via Fact \ref{fac:no-regret-cce}). In particular, if there are $\poly(m)$ queries per episode, as is standard in the online simulator model where a trajectory is drawn from the policy $\sigma\^t$ at each episode $t \in [T]$, then $T > 2^{\Omega(m)}$ episodes are required to have regret $o(T/m)$.  \dfcomment{this is assuming $O(1)$ queries/episode? maybe we should be a little more clear on the episodic model.}\noah{added a sentence} This is in stark contrast to the setting of normal-form games, where even for the case of bandit feedback (which is a special case of the generative model setting), standard no-regret algorithms have the property that each player's regret scales as $\til O(\sqrt{Tn})$ (i.e., independently of $m$), where $n$ denotes the number of actions per player \cite{lattimore2020bandit}. As with our computational lower bounds, Theorem \ref{thm:statistical-lb} is not limited to decentralized algorithms, and also rules out \emph{centralized} algorithms which, with access to a generative model, compute a sequence of policies which constitutes a solution to the \GenCCE problem. Furthermore, it holds for arbitrary values of $N$, thus allowing the policies $\sigma\^1, \ldots, \sigma\^T \in \Pirndrnd$ solving the \GenCCE problem to be arbitrary general policies. 

\dfcomment{should we remark on the fact that $N$ can be arbitrary?}\noah{did so}

