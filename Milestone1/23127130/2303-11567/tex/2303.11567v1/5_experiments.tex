\section{Experiments}
\subsection{Datasets and Implementation Details}
\textbf{Datasets.} To verify the effectiveness of our o2f method, we conduct experiments on the COCO~\cite{coco} and CrowdHuman~\cite{crowdhuman} datasets. COCO is a challenging benchmark which consists of 118k training images and 5k validation images from 80 classes. We use the standard COCO metric, \ie, AP, by averaging over 10 IoU thresholds ranging from 0.5 to 0.95. CrowdHuman is a widely used dataset for human detection in crowded scenes. It has 15k images for training and 4k images for validation. As suggested by the official paper~\cite{crowdhuman}, mMR, which is the average log miss rate over false positives per-image, is taken as the main metric. In addition, we report the AP and recall results of competing algorithms for reference. 

\textbf{Implementation.} We implement our o2f method by using the MMDetection toolbox~\cite{mmdetection}. All the backbones are initialized by the weights pre-trained on ImageNet~\cite{imagenet} with frozen batch normalizations. The ablation studies are based on ResNet-50~\cite{resnet} backbone with FPN~\cite{fpn}. The input images are resized so that the shorter side is 800 pixels in ‘1$\times$’ schedule. Multi-scale training is applied in ‘3$\times$’ schedule. We train all the models on 8 GPUs with a mini-batch size of 16. When using CNNs as the backbone, we utilize SGDM~\cite{sgd} optimizer with an initial learning rate of 0.01, a momentum of 0.9, and a weight decay of $10^{-4}$. When using Transformers~\cite{swin} as the backbone, we use AdamW~\cite{adamw} with an initial learning rate of 0.0001 and a weight decay of 0.05. In the ‘1$\times$’ schedule, we train the model for 12 epochs and decay the learning rate by a factor of 10 in the $8^{th}$ and $11^{th}$ epochs, respectively. In the ‘3$\times$’ schedule, we train the model for 36 epochs and decay the learning rate in the $24^{th}$ and $33^{th}$ epochs, respectively.

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\textwidth]{compare_poto_ours}
    \caption{Comparisons of detection head between DeFCN (POTO)~\cite{poto} and our o2f method. In o2f, the classification score and the centerness score are multiplied as the final cls-IoU joint score. DeFCN applies o2o on the joint score and applies o2m on the classification score, while our method applies the dynamic o2f assignment on the joint score. }
    \label{compare_poto_ours}
\end{figure*}
\input{table3}
\input{table4}
\input{table5}
\subsection{Ablation Studies}
\textbf{The number of ambiguous anchors.} In our o2f method, we select $K$ ambiguous anchors that can have both positive and negative losses. We ablate this hyper-parameter $K$ in Tab.~\ref{table2} on COCO \texttt{val} set. One can see that the best AP performance 39.0 (without NMS) is obtained when $K$ equals to 7. We set $K$ to 7 in the rest experiments.

\textbf{$\boldsymbol{T^{max}}$ and $\boldsymbol{T^{min}}$.} These two parameters control the positive loss weights of ambiguous anchors in the first and last epoch, respectively,  during the training process. Tab.~\ref{table3} shows the results on COCO \texttt{val} set by using different combinations of $T^{max}$ and $T^{min}$. We can see that when $T^{max}$ is set to a higher value like 0.6, increasing $T^{min}$ will decrease the end-to-end detection performance without NMS. This implies that a small $T^{min}$ is helpful to narrow down the performance gap between ‘without NMS’ and ‘with NMS’. Meanwhile, when $T^{min}$ is set to a smaller value like 0.1, a small $T^{max}$ can decrease the performance with and without NMS due to the limited number of positive samples. The best AP performance 39.0 is achieved when we set $T^{max}$ and $T^{min}$ to 0.6 and 0.2, respectively, which is the default setting in our experiments.

\textbf{Selection metric.} We further explore the effect of matching functions in selecting the certain positive anchor and the ambiguous anchors. As shown in Tab.~\ref{table4}, the method `Multiply' means the matching function in Eq.~\ref{eq1}, while the method ‘Add’ means that we change Eq.~\ref{eq1} to  $(1-\alpha)\times p_{i,c_j}+\alpha  \times IoU(b_i,b_j)$, which is a frequently used metric in DETRs~\cite{deformdetr,detr}. We see that considering only the classification score (\ie, $\alpha=0$) can decrease the performance without NMS. Considering only the regression IoU (\ie, $\alpha=1$)  can result in duplicated predictions and significantly enlarge the gap with NMS. We can also see that ‘Multiply’ is more suitable for dense detection which achieves 1.1 points gains over the best ‘Add’ result. We choose ‘Multiply’ and set $\alpha=0.8$ in the rest experiments. 

\subsection{Results on COCO}
We mainly compare our o2f method with DeFCN~\cite{poto}, which is the representative and leading o2o method for fully convolutional dense detection. DeFCN employs a Prediction-aware One-to-One (POTO) label assignment scheme for classification and holds the state-of-the-art end-to-end dense detection performance. The framework comparison between DeFCN and our o2f is shown in Fig.~\ref{compare_poto_ours}. DeFCN adds an auxiliary o2m loss for the classification score in addition to the standard o2o loss. It also introduces a 3D Max Filtering (3DMF) module in the centerness branch to increase the local discrimination ability of convolutions. In contrast, we apply a single dynamic o2f label assignment loss in the final loss function and introduce several lightweight convolutional layers in the centerness branch with negligible computation overhead.

\textbf{Main results.} Tab.~\ref{table5} presents the detailed comparisons between POTO and our method in both detection accuracy and speed. The original FCOS uses o2m label assignment during training, achieving 38.6 AP/17.7 AP with/without NMS in inference by using ‘1$\times$’ schedule. Such a huge gap between `with' and without `NMS' indicates that o2m can easily produce duplicated predictions. By replacing o2m with o2o during training, POTO significantly improves the end-to-end performance from 17.7 to 36.5, demonstrating that o2o plays a crucial role in removing duplicated predictions. POTO further increases the performance to 37.6 by adding 3DMF and the auxiliary loss. However, the o2m label assignment in the auxiliary loss has to be carefully tuned. Using the FCOS-style o2m strategy in the auxiliary loss can decrease the performance. What’s more, DeFCN increases the forward time from 27.7 ms to 30.3 ms and runs slower than the original FCOS (33.0 FPS vs 35.2 FPS).

Our o2f method achieves 39.0 AP in `1$\times$' schedule when using three extra convolution layers in the centerness branch, which achieves 1.4 points gains (39.0 AP vs. 37.6 AP) and a faster inference speed over DeFCN (35.5 FPS vs. 33.0 FPS). It is worth mentioning that our method also achieves better accuracy and faster inference speed than FCOS with NMS. 
%We have implemented another option by changing the last layer of the classification head to a deformable convolutional network. This setting runs at a similar speed to DeFCN but gains 1.8 points higher performance. When training the detector with the `3$\times$' schedule, o2f achieves 42.2 AP, 1.0 points higher than DeFCN.

%Besides the average prevision (AP), our method achieves more significant improvements on average recall (AR). NMS-based FCOS achieves 57.2 AR and 59.1 AR in the `1$\times$' and `3$\times$' schedules, respectively. DeFCN achieves 58.7 AR and 61.2 AR in the `1$\times$' and `3$\times$' schedules, respectively. Our method improves DeFCN by 2.5 and 2.3 points in the  `1$\times$' and `3$\times$' schedules, respectively. This demonstrates the superiority of our o2f method over previous methods in the recall rate.

A recent similar approach to ours is the Hybrid-Epoch scheme in H-DETR~\cite{hybriddetr}, which can also be seen as a combination of o2o and o2m. The key difference lies in that H-DETR uses static loss weights, where the weights of o2o and o2m are \{0,1\} in the early training epochs and \{1,0\} in the later training epochs. In contrast, we use a single soft LA branch and dynamically adjust the loss weights. We implement the Hybrid-Epoch scheme on FCOS and achieve 37.3 and 40.6 AP under the 1$\times$ and 3$\times$ schedules, respectively, which are 1.7 and 1.6 points lower than our o2f. This shows that static loss weights are less effective than dynamic loss weights for end-to-end dense detection. 

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.4\textwidth]{ap_curve.pdf}
    \caption{The object detection performance $w.r.t.$ training epochs for o2o and o2f methods. All models are based on ResNet-50 backbone and `1$\times$' schedule. The threshold in NMS is 0.6.}
    \label{ap_curve}
    \vspace{-4mm}
\end{figure}

\textbf{Model performance during training process.}
We show in Fig.~\ref{ap_curve} the performance of our model during the training process. One can see that at the very beginning, the gap between `w/o NMS' and `w/ NMS' is large because we assign relatively large positive weights to the ambiguous anchors. As the training progresses, we gradually decrease the positive loss weights of the ambiguous anchors and thus the gap becomes smaller and smaller. This phenomenon conforms to our motivation that we focus on `feature representation learning' in the early training stage and `duplicated prediction removal' in the later training stage. In contrast, the model trained with the o2o label assignment keeps a small gap during the whole training process, but finally leads to inferior performance due to the limited number of positive training samples.

\textbf{Larger backbones.}
To further demonstrate the robustness and effectiveness of our method, we provide experiments with larger backbones. The detailed results are shown in Tab.~\ref{res101} and Tab.~\ref{swin}. When using ResNet-101 as the backbone, our method performs better than POTO (o2o) by 2.3 points and 1.8 points under the `1$\times$’ and `3$\times$’ schedules, respectively. When using Swin-T  as the backbone, our method achieves 2.1 and 2.1 AP gains over POTO under the the `1$\times$' and `3$\times$' schedules, respectively.

\textbf{Visualization.} In Fig.~\ref{map_vis}, we visualize the classification scores and assigned labels in the early and later training stages of our o2f method. From left to right, the three maps in each stage correspond to the instances of `tie', `pot' and `person', respectively. In the early training stage, we assign relatively large positive weights to the ambiguous anchors so that they contribute more to feature representation learning. This causes duplicated predictions as shown in the score map, where several anchors are highly activated for one instance. In the later training stage, we gradually decrease the positive weights of the ambiguous anchors, and the classification score maps become much sparser, which demonstrates that the network has learned to remove the duplicated predictions. The above results indicate that to achieve end-to-end detection, there is no need to enforce sparse predictions at the early training stage, where `feature learning' should be paid more attention.

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.85\textwidth]{map_vis}
    \caption{Visualization of the predicted classification scores and assigned soft labels in the early and later training stages. There are three instances of different scales in the input image, \ie, person, tie and pot. The heatmaps from the left to right in each stage correspond to the FPN layers of `P5', `P6' and `P7', respectively.}
    \label{map_vis}
\end{figure*}

\input{table6}
\input{table7}
\input{table8}
\subsection{Results on CrowdHuman}
To further validate the generalization ability of our method, we conduct experiments on CrowdHuman, which is a widely used dataset for human detection in crowded scenes. NMS-based methods use an IoU threshold to filter out duplicated predictions, which suffers from a dilemma: a high threshold can bring more false positives while a low threshold can suppress true positives. This problem becomes more severe in crowed scenes. Our proposed end-to-end dense detector is free of NMS and hence it can avoid this problem. The results are shown in Tab.~\ref{crowdhuman}. One can see that our o2f method significantly outperforms the NMS-based FCOS by 10 points in mAR, which is the main metric in crowed detection. It also outperforms POTO by 6.8 points in mAR, clearly demonstrating the advantages of our o2f strategy over o2o and o2m strategies. %Visual results can be found in the \textbf{supplementary file.} 


\subsection{Results on Instance Segmentation}
Considering that the framework of instance segmentation shares similarities with object detection and it also employs NMS to suppress duplicated prediction masks, we apply the proposed o2f label assignment strategy to the popular single-stage dense instance segmentation method CondInst~\cite{condinst} to validate its effectiveness. The results based on ResNet-50 backbone with `1$\times$' and `3$\times$' learning schedules are shown in Tab.~\ref{condinst}.
The original CondInst trained with o2m strategy has very poor performance without using NMS in inference. Using o2o during training can effectively improve the end-to-end segmentation performance to 32.8 AP and 36.1 AP under the `1$\times$' and `3$\times$' schedules, respectively. Our o2f label assignment strategy can further boost the performance by 3.1 and 1.9 points, respectively. 
Visual results can be found in the \textbf{supplementary file.}
\input{table9}
