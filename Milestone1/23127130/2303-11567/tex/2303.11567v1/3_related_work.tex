\section{Related Work}
The past decade has witnessed tremendous progress in object detection with the rapid development of deep learning techniques~\cite{swin,transformer1,transformer2,transformer3,vgg,resnet}. Modern object detectors can be roughly categorized into two types: convolutional neural networks (CNNs) based detectors~\cite{fasterrcnn,focalloss,yolo,yolo3,yolo4,yolo6,yolo7,ssd,focalloss,cascadercnn,fcos} and transformer based detectors~\cite{detr,conditionaldetr,deformdetr,dabdetr,anchordetr,efficientdetr,dynamicdetr}. 

\subsection{CNN-based Object Detectors}
CNN-based detectors can be further divided into two-stage detectors and one-stage detectors. Two-stage detectors~\cite{fasterrcnn,cascadercnn} generate region proposals in the first stage and refine the locations and predict the categories of these proposals in the second stage, while one-stage detectors~\cite{ssd,focalloss} directly predict the categories and location offsets of dense anchors on convolutional feature maps. The early detectors mostly utilize pre-defined anchors as training samples.  The hyper-parameters of anchor shapes and sizes have to be carefully tuned since the suitable settings vary across different datasets. To overcome this issue, anchor-free detectors~\cite{fcos, foveabox} have been proposed to simplify the detection pipeline. FCOS~\cite{fcos} and CenterNet~\cite{centernet} replace the anchor boxes with anchor points and directly use the points to regress the target objects. CornerNet~\cite{cornernet} first predicts object keypoints and then groups them to bounding boxes using associate embeddings. 

Most CNN-based detectors adopt the one-to-many (o2m) label assignment scheme in the training process. Early detectors such as Faster RCNN~\cite{fasterrcnn}, SSD~\cite{ssd} and RetinaNet~\cite{focalloss} use IoU as a metric to define positive and negative anchors.  FCOS restricts the positive anchor points to be within certain scales and ranges of the object. Recent  methods~\cite{paa,freeanchor,varifocalnet,autoassign,tood,dual} often consider the quality and distribution of the network predictions for more reliable label assignment of anchors. However, the o2m label assignment requires a post-processing step, namely non-maximum suppression (NMS), to remove duplicated predictions. NMS introduces a parameter to compromise precision and recall for all instances, which is however sub-optimal, especially for crowded scenes. In this paper, we aim to remove this hand-crafted NMS step in CNN-based detectors and achieve end-to-end dense detection.

\subsection{Transformer-based Object Detectors}
As a pioneer transformer-based detector, DETR~\cite{detr} utilizes a spare set of learnable object queries as the training candidates to interact with the image feature. It achieves competitive end-to-end detection performance using o2o bipartite matching and the global attention mechanism. However, DETR suffers from slow convergence and inferior performance on small objects. Many following works~\cite{dynamicdetr, acceleratdetr,fastdetr,boxdetr} aim to improve the attention modeling mechanism between the feature map and object queries so that more relevant and precise features can be extracted to boost the performance on small objects. Recent studies~\cite{groupdetr,hybriddetr,dndetr} indicate that it is the limited number of positive samples that slows down the convergence of DETR. Therefore, they introduce several extra decoders to increase the number of positive samples. Nevertheless, these methods are all based on sparse candidates and their computational cost can be unaffordable when performing dense predictions. Different from these methods, we propose a soft label assignment scheme to introduce more positive samples so that end-to-end dense detectors can be more easily trained.