\section{One-to-Few Soft Labeling}
\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.4\textwidth]{compare_o2o_o2m.pdf}
    \caption{The comparison between o2o label assignment and o2m label assignment. The anchors that have opposite classification labels between o2o and o2m are defined as ambiguous anchors.}
    \label{compare_o2o_o2m}
\end{figure}
\input{table1}
\subsection{Ambiguous Anchors}
Label assignment in dense detection aims to assign each anchor a classification label to supervise the network training. Fig.~\ref{compare_o2o_o2m} illustrates the o2o and o2m label assignments for a ‘person’ instance. One can see that the o2o labeling scheme selects only one anchor as the positive sample, while o2m assigns multiple positive anchors. In both o2o and o2m, the remaining anchors other than positive ones are all defined as negative samples. 

We argue that some anchors actually lie between positive and negative ones, and they should not be simply assigned a single positive or negative label. As shown in Fig.~\ref{compare_o2o_o2m}, we name the anchor (highlighted with red borders) that is positive in both o2o and o2m as the “certain anchor” since there is generally no ambiguity on it. In contrast, we name the anchors (highlighted with green borders) which are positive in o2m but negative in o2o as “ambiguous anchors” as they have conflict labels in o2o and o2m schemes.  

Now we have divided the anchors into three groups: one certain positive anchor, a few ambiguous anchors, and the remaining multiple negative anchors. The ambiguous anchors are labeled as negative in o2o in order to avoid duplicated predictions, whereas they can help learning robust feature representations in o2m. One interesting question is can we find a way to integrate the merits of o2o and o2m schemes so as to improve the performance of end-to-end dense detection? We advocate that the key to solve this problem is how to properly introduce more positive supervision signals into o2o. To find out the solution to this question, let’s test two options first.

The first option is to change o2o to one-to-two by adding one more positive sample for each instance.  The second option is to assign a soft label $t$ to each ambiguous anchor, where $0\leq t \leq 1$ is its positive degree and hence $1-t$ is its negative degree. We define the classification losses of positive and negative anchors as $-log(p)$ and $-log(1-p)$, respectively, where $p$ is the predicted classification score. Then the classification loss of the second option will be $-t\times log(p)-(1-t) \times log(1-p)$. The detection results on the COCO dataset are shown in Table~\ref{table1}, from which we can see that the one-to-two label assignment scheme significantly decreases the performance, even if only one more positive sample is added. In contrast, assigning suitable soft labels to ambiguous anchors can effectively improve the end-to-end performance. (The details of soft label assignment will be discussed in the following sections.)

The above results imply that enabling an ambiguous anchor to be positive and negative simultaneously could be a feasible solution for effective end-to-end dense detection. We therefore propose a one-to-few (o2f) label assignment strategy which selects one certain anchor to be fully positive, a few ambiguous anchors to be both positive and negative, and the remaining anchors to be negative samples. The positive and negative degrees of the ambiguous anchors are dynamically adjusted during the training process so that the network can keep strong feature representation ability and achieve end-to-end detection capability at the same time. 

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.9\textwidth]{framework.pdf}
    \caption{The overall structure of our method. Each FPN layer has a detection head that predicts three outputs: the classification score map of $H \times W \times C$, the regresssion offset map of $H \times W \times 4$, and the centerness/objectness map of $H \times W \times 1$. The structure of the detection head is the same as the one in FCOS except that some extra lightweight convolutional layers with 64 output channels are added before the centerness score map. }
    \label{framework}
\end{figure*}

\subsection{Selection of Certain Positive Anchor}
In our method, a certain positive anchor will be selected for each instance. Previous o2o-based detectors all utilize a prediction-aware selection metric, which considers the cost of both classification and regression to select a unique positive sample. We follow this principle, and incorporate both classification score and IoU into the selection metric for the certain anchor, which is defined as:
\begin{equation}
\begin{aligned}
S_{i,j} = \mathbbm{1}\left[ i \in \Omega _{j} \right] \times p_{i,c_j}^{1-\alpha} \times IoU(b_i,b_j)^\alpha,
\end{aligned}
\label{eq1}
\end{equation}
where  $S_{i,j}$ represents the matching score between anchor $i$ and instance $j$,  $c_j$ is the category label of instance $j$, $p_{i,c_j}$ is the predicted classification score of anchor $i$ belonging to category $c_j$, $b_i$ is the predicted bounding box coordinates of anchor $i$, $b_j$ denotes the coordinates of instance $j$, and $\alpha$ controls the importance degree of classification and regression. $\mathbbm{1}\left[ i \in \Omega _{j} \right]$ is a spatial indicator that outputs 1 when the center point of anchor $i$ is within the central region $\Omega _{j}$  of instance $j$; otherwise it outputs 0. This spatial prior has been commonly utilized in both o2o and o2m methods based on the observation that anchors in the central region of an instance are more likely to be positive ones.

The anchors can be sorted in a descending order according to the metric  $S_{i,j}$. Previous works~\cite{detr,deformdetr} often formulate the positive anchor selection as a bipartite matching problem and solve it by using the Hungarian algorithm~\cite{hungarian}. For simplicity, in this work we directly select the top scored one as the certain positive anchor for each instance. 

\subsection{Label Assignment for Ambiguous Anchors}
Apart from the certain positive anchor, we select the top-$K$ anchors based on the score $S_{i,j}$ as ambiguous anchors since they have similar semantic contexts to the certain positive anchor. To reduce the possibility of duplicated predictions, we assign dynamic soft labels to these ambiguous anchors. Suppose that we train the network for $N$ epochs, the classification loss of each ambiguous anchor $i$ during the $j^{th}$ epoch is defined as:
\begin{equation}
\begin{aligned}
l_i^j = -t_i^j \times log(p_i) - (1-t_i^j) \times log(1-p_i),
\end{aligned}
\label{eq2}
\end{equation}
where $p_i$ is the predicted classification score of anchor $i$, $t_i^j$  and ($1-t_i^j$ ) are the positive and negative degrees (\ie, loss weights) of this anchor at the $j^{th}$ epoch, respectively. $t_i^j$ is dynamically defined as:
\begin{equation}
\begin{aligned}
t_i^j&=\frac{p_i}{\max _k p_k} \times T^j, \\
T^j&=\frac{T^{\min }-T^{\max }}{N-1} \times j+T^{\max },
\end{aligned}
\label{eq3}
\end{equation}
where $T^j$ is a time-dependent variable that is assigned the same value for all samples in the $j^{th}$ epoch, and $T^{max}$ and $T^{min}$ control the positive degree of ambiguous anchors in the first epoch and last epoch, respectively.
We set the loss weights to be positively correlated with the classification scores considering that the anchors with higher prediction scores should contribute more to the positive signals. 
Directly using $p_i$ as the weight will make the training unstable on hard samples because the predicted scores of them are much smaller than that of easy samples. So we use the ratio between $p_i$ and max\{$p$\} to normalize the weights of different samples into the same scale.
Dynamically adjusting $T^j$ is important as it controls the trade-off between `feature learning' and `duplication removal' in different training stages. In the early training stage, we set $T^j$ relatively large to introduce more positive supervision signals for representation learning so that the network can rapidly converge to a robust feature representation space. As the training progresses, we gradually decrease the positive degrees of the ambiguous anchors so that the network can learn to remove duplicated predictions. 

\input{table2}
\subsection{Network Structure}
We instantiate the proposed o2f label assignment strategy to FCOS, which is a typical fully convolutional dense detector. The network structure is shown in Fig.~\ref{framework}. A detection head that consists of two parallel convolutional branches is attached to the output of each Feature Pyramid Network (FPN) layer. One branch predicts a score map of size $H \times W \times C$, where $C$ is the number of categories in the dataset, $H$ and $W$ are the height and width of the feature map, respectively. Another branch predicts a location offsets map of size $H \times W \times 4$ and a centerness map of size $H \times W \times 1$. Following previous works~\cite{poto,autoassign, li2019dynamic}, we multiply the centerness map with the classification score map as the final classification-IoU joint score.

For each instance, we select one certain positive anchor and $K$ ambiguous anchors. The remaining anchors are set as negative samples. The training objective of the classification branch for each instance is formulated as:
\begin{equation}
\begin{aligned}
L_{c l s}=B C E\left(p_c, 1\right)+\sum_{i \in \mathcal{A}} B C E\left(p_i, t_i\right)+\sum_{i \in \mathcal{B}} F L\left(p_i, 0\right),
\end{aligned}
\label{eq4}
\end{equation}
where $p_c$ is the classification score of the single certain anchor, $\mathcal{A}$ and $\mathcal{B}$ represent the set of ambiguous anchors and negative anchors, respectively. BCE indicates the Binary Cross Entropy loss and FL indicates the Focal Loss~\cite{focalloss}. The regression loss is defined as:
\begin{equation}
\begin{aligned}
\begin{matrix}
L_{reg}=\sum_{i \notin \mathcal{B}} GIoU\left(b_i, b_{gt}\right),
\end{matrix}
\end{aligned}
\label{eq5}
\end{equation}
where  GIoU loss is a location loss based on General Intersection over Union~\cite{giou}, $b_i$ is the predicted location of anchor $i$ and $b_{gt}$ is the location of GT object corresponding to anchor $i$. Note that we apply the regression loss on both the positive anchor and the ambiguous anchors.
