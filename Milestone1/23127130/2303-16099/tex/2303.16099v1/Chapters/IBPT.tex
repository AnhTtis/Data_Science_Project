% Chapter Template

\chapter{Context Aware Network for 3D Glioma Segmentation} % Main chapter title
\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}
\section{Introduction}
Glioma is one of the most common primary brain tumors with fateful health damage impacts and high mortality. To provide sufficient evidence for early diagnosis, surgery planning and post-surgery observation, Magnetic Resonance Imaging (MRI) is a widely used technique to provide reproducible and non-invasive measurement, including structural, anatomical and functional characteristics. Different 3D MRI modalities, such as T1, T1 with contrast-enhanced (T1ce), T2 and Fluid Attenuation Inversion Recover (FLAIR), can be used to examine different biological tissues.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\textwidth]{TaskFig1.png}
    \caption{Examples of multi-modality data slices from BraTS17 with ground-truth and our segmentation result. In this figure, green represents GD-Enhancing Tumor, yellow represents Pertumoral Edema and red represents NCR$\backslash$ECT.}
\label{fig:task}
\end{figure}

Medical image segmentation provides fundamental guidance and quantitative assessment for medical professionals to achieve disease diagnosis, treatment planning and follow-up services. However, manual segmentation requires certain professional expertise and usually tends to be time and labor-consuming. Fig. \ref{fig:task} shows a general view of the brain tumor segmentation task. Early research on automated brain tumor segmentation was based on traditional machine learning algorithms \cite{bauer2012segmentation, subbanna2012probabilistic, shin2012hybrid, festa2013automatic}, which rely on hand-crafted features, such as textures \cite{reza2013multi} and local histograms \cite{goetz2014extremely}. However, finding the best hand-crafted features or optimal feature combinations in a high dimensional feature space is impracticable. In recent years, deep learning techniques, especially deep convolutional neural networks (DCNNs), can be used to effectively learn high dimensional discriminative features from data and have been widely used on various computer vision tasks \cite{long2015fully}.

Inter-class ambiguity is a common issue in brain tumor segmentation. This issue makes it hard to achieve accurate dense voxel-wise segmentation if only considering isolated voxels, as different classes' voxels may share similar intensity values or close feature representations. To address this issue, we propose a context-aware network, namely CANet, to achieve accurate dense voxel-wise brain tumor segmentation in MRI images. The proposed CANet contains a novel Hybrid Context Aware Feature Extractor (HCA-FE) and a novel Context Guided Attentive Conditional Random Field (CG-ACRF). Our contributions in this work are summarised below:

\begin{itemize}
    \item We propose a novel HCA-FE built with a 3D feature interaction graph neural network and a 3D encoder-decoder convolutional neural network. Different from previous works that usually extract features in the convolutional space, HCA-FE learns hybrid context guided features both in a convolutional space and a feature interaction graph space (the relationship between neighboring feature nodes is utilised and continuously updated). To our knowledge, this is the first practice on brain tumor segmentation, which incorporates adaptive contextual information with graph convolution updates.
    \item We further propose a novel CG-ACRF based fusion module that attentively aggregates features from the feature interaction graph and convolutional spaces. Moreover, we formulate the mean-field approximation of the inference in the proposed CG-ACRF as a convolution operation, enabling the CG-ACRF to be embedded within any deep neural network seamlessly to achieve end-to-end training.
    \item We conduct extensive evaluations and demonstrate that our proposed CANet outperforms several state-of-the-art technologies using different measure metrics on the Multimodal Brain Tumor Image Segmentation Challenge (BraTS) datasets, i.e. BraTS2017 and BraTS2018.
\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{totalsystem.png}
    \caption{The architecture of the proposed dual stream network. Best viewed in color.}
    \label{fig:totalsystem}
\end{figure*}

\section{Review of Brain Glioma Segmentation methods}
Early research on brain tumor segmentation was based on traditional machine learning algorithms such as clustering \cite{shin2012hybrid}, random decision forests \cite{festa2013automatic}, Bayesian models \cite{corso2008efficient} and graph-cuts \cite{wels2008discriminative}. Shin \cite{shin2012hybrid} used sparse coding for generating edema features and K-means for clustering the tumor voxels. However, how to optimise the size of the sparse coding dictionary is still an intractable problem. Pereira et al. \cite{pereira2016brain} proposed to classify each voxelâ€™s label by using random decision forests, which relied on hand-crafted features and complicated post-processing. Corso et al. \cite{corso2008efficient} used a Bayesian formulation for incorporating soft model assignments into the affinities calculation. This method brought the weighted aggregation of multi-scale features but ignored the relationship between different scales. Wels et al. \cite{wels2008discriminative} proposed a graph-cut based method to learn optimal graph representation for tumor segmentation, leading to superior performance. However, this method required a long inference time for dense segmentation tasks, as the number of vertices in its graph is proportional to the number of the voxels.

Promising achievements have been made on multi-modal MRI brain tumor segmentation using deep convolutional neural networks. Zikic et al. \cite{zikic2014segmentation} is one of the pioneers applying DCNNs onto brain tumor segmentation. Havaei et al \cite{havaei2017brain} further improved DCNNs with different sizes of convolutional kernels in order to capture local and global information. Zhao et al. \cite{zhao2018deep} proposed a modified FCN connected with conditional random fields for refining brain tumor segmentation using three MRI modalities. Dong et al. \cite{dong2017automatic} proposed a modified U-Net for brain tumor segmentation. These previous works used 2D convolutional kernels on 2D MRI slices made from original 3D volumetric MRI data. Methods using 2D slices do decrease the number of the used parameters and require less memory due to dimensionality reduction. However, this pre-processing procedure also leads to the spatial context missing. To minimise the information loss and capture evidence from adjacent slices, Lyksborg et al. \cite{lyksborg2015ensemble} ensembled three 2D CNNs on three orthogonal 2D patches. 

To fully make use of 3D contextual information, recent works applied 3D convolutional kernels on original volume data. Kamnitsas et al. \cite{kamnitsas2017efficient} proposed two pathway 3D CNN followed with dense CRF called DeepMedic for brain tumor segmentation. Authors of \cite{kamnitsas2017efficient} further extended the work by using model ensembling \cite{kamnitsas2017ensembles}. The proposed system EMMA ensembled models from FCN, U-Net and DeepMedic for processing 3D patches. To avoid over-fitting problems in 3D voxel-level segmentation on limited training datasets, Myronenko \cite{myronenko20183d} proposed a 3D CNN with an additional variational autoencoder to regularise the decoder by reconstructing the input image. The architecture built in \cite{myronenko20183d} is further developed in various recent works. Su et al. \cite{su2020multimodal} extends the architecture built in \cite{myronenko20183d} into two sub-networks to fuse the information learned from different modalities. Jiang et al. \cite{jiang2019two} proposed two-stage networks where each stage adopts a similar network in \cite{myronenko20183d}. The first stage network generates a coarse result and the second stage network refines the segmentation result. The final result in \cite{jiang2019two} reaches state-of-the-art by ensemble 12 model instances, which requires huge computational resources. Other works also try to fuse information brought by images in a different modality. Wang et al. \cite{wang2020modality} paired data from a different modality and designed the consistency loss to learn the relationship between features in different modalities. Dorent et al. \cite{dorent2019hetero} utilize the network in \cite{myronenko20183d} for multi-task learning, e.g. joint modality completion and  segmentation together. However, the aforementioned approaches only consider the relationship that lies within the modality and ignores the spatial relationship among features, which is more important to achieve accurate segmentation. 

Recent research works began to focus on using graph neural networks for object semantic segmentation.  Qi et al. \cite{qi20173d} and Landrieu et al. \cite{landrieu2018large} construct graph networks for point cloud semantic segmentation based on energy minimization. However, the data used in these approaches are point clouds. Point cloud contains the point nodes, which can be directly used for building graphs. Lu et al. \cite{lu2019graph} construct a graph-FCN for object semantic segmentation. However, this approach builds the graph by extracting nodes using convolutional kernel and ignores the information regulation between normal convolution and graph convolution. The same issue lies in \cite{liu2020scg}  and \cite{zhang2019dual} as the proposed model cannot adaptively make a preference between features from normal convolution and features from graph convolution.


Medical image datasets (e.g. BraTS) usually have an imbalance and inter-class interference problems. To address these issues whilst maintaining segmentation performance, Chen et al. \cite{chen2018focus} and Wang et al. \cite{wang2017automatic} both applied cascaded network structures for segmenting brain tumors, where the input of the inner region segmentation network is the output of the outer region segmentation network. However, these cascaded structures force the networks to crop data in the cascading stage and hence cause information loss. The summary of MRI based brain tumor segmentation is shown in Table \ref{table:lr}.

\begin{sidewaystable}
\centering
\caption{Summary of existing brain tumor segmentation methods.}
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{lllll}
\hline
Authors         & Base Model                                                                            & Data Format           & Highlights                                                                                                                                                                                                          & Limitations                                                                                                                                                                        \\ \hline
Wels et al. \cite{wels2008discriminative}     & Graph Cut                                                                             & 3D Volumes            & \begin{tabular}[c]{@{}l@{}}(1) A statistical formulation of \\ brain tumor segmentation.\end{tabular}                                                                                                               & \begin{tabular}[c]{@{}l@{}}(1) The size of graph vertices can be large.\\ (2) Using hand-crafted features.\end{tabular}                                                           \\ \hline
Corso et al. \cite{corso2008efficient}   & \begin{tabular}[c]{@{}l@{}}Bayesian Classifier + \\ the weighted Aggragation\end{tabular} & 2D Single-view slice  & \begin{tabular}[c]{@{}l@{}}(1) Explicitly learned the hierarchical \\ information of tumor tissue structures.\end{tabular}                                                                                            & \begin{tabular}[c]{@{}l@{}}(1) The final segmentation performace \\ heavily relys on the result of weighted \\ aggragation.\end{tabular}                                           \\ \hline
Shin \cite{shin2012hybrid}            & \begin{tabular}[c]{@{}l@{}}Spase coding + \\ K-means Clustering\end{tabular}          & 2D Single-view Slice  & (1) Fast and easy implementation.                                                                                                                                                                                   & \begin{tabular}[c]{@{}l@{}}(1) Clustering performance relys on \\ the quality of sparse coding features.\end{tabular}                                                                  \\ \hline
Festa et al. \cite{festa2013automatic}  & Random Decision Forest                                                                & 3D Volumes            & \begin{tabular}[c]{@{}l@{}}(1) Good interpretation based on\\  the classifier decisions.\end{tabular}                                                                                                               & (1) Hand-crafted features.                                                                                                                                                         \\ \hline
Zikic et al. \cite{zikic2014segmentation}   & 2D CNN                                                                                & 2D Patches            & (1) Computational efficient.                                                                                                                                                                                        & \begin{tabular}[c]{@{}l@{}}(1) Cannot directly learn information \\ from 3D space.\end{tabular}                                                                                    \\ \hline
Pereira et al. \cite{pereira2016brain}  & 2D CNN                                                                                & 2D Single-view slice  & \begin{tabular}[c]{@{}l@{}}(1) Stack small size kernels to \\ capture larger receptive field.\end{tabular}                                                                                                          & \begin{tabular}[c]{@{}l@{}}(1) Patch-classification based segmentation.\\ (2) Requires complicated post-processing.\end{tabular}                                                   \\ \hline
Havaei et al. \cite{havaei2017brain}   & 2D CNN                                                                                & 2D Patches            & \begin{tabular}[c]{@{}l@{}}(1) Replace final fully connected layer \\ with convolution layer, which leads\\  to a significant speed up.\\ (2) Studied the effectiveness \\ of different connections.\end{tabular}   & (1) Patch-classification based segmentation.                                                                                                                                       \\ \hline
Dong et al. \cite{dong2017automatic}     & 2D FCNN                                                                               & 2D Single-view Slice  & \begin{tabular}[c]{@{}l@{}}(1) Introduced a novel soft \\ dice loss function.\end{tabular}                                                                                                                          & \begin{tabular}[c]{@{}l@{}}(1) UNet based FCNN, which \\ used simple concatenation \\ for feature fusion.\end{tabular}                                                             \\ \hline
Kamnitsas et al. \cite{kamnitsas2017efficient} & 3D FCNN + CRF                                                                         & 3D Patches            & \begin{tabular}[c]{@{}l@{}}(1) 3D kernel to learn information \\ from volumetric space.\end{tabular}                                                                                                                & \begin{tabular}[c]{@{}l@{}}(1) Patch-classification based segmentation\\ (2) Cascaded connection between\\  FCNN and CRF.\end{tabular}                                             \\ \hline
Kamnitsas et al. \cite{kamnitsas2017ensembles} & 3D CNN Ensembling                                                                     & 3D Volumes            & \begin{tabular}[c]{@{}l@{}}(1) High accuracy benefited from \\ multiple segmentation models.\end{tabular}                                                                                                           & (1) Computation resource exhausted.                                                                                                                                                \\ \hline
Wang et al. \cite{wang2017automatic}     & 2D Cascaded CNN                                                                       & 2D Single-view Slices & \begin{tabular}[c]{@{}l@{}}(1) Explicitly model the hierachical \\ information of tumor tissue structures.\end{tabular}                                                                                              & \begin{tabular}[c]{@{}l@{}}(1) Information and receptive field\\  loss during crop operation.\\ (2) Over-parameterization by \\ introducing complicated sub-networks.\end{tabular} \\ \hline
Zhao et al. \cite{zhao20173d}     & 2D FCNN + CRF                                                                         & 2D Patches            & \begin{tabular}[c]{@{}l@{}}(1) Fully convolutional network to \\ generate segmentation map directly.\end{tabular}                                                                                                   & \begin{tabular}[c]{@{}l@{}}(1) Cascaded connection between \\ FCNN and CRF.\end{tabular}                                                                                           \\ \hline
Chen et al. \cite{chen2018focus}    & 2D Cascaded CNN                                                                       & 2D Single-view Slices & \begin{tabular}[c]{@{}l@{}}(1) Explicitly model the hierachical \\ information of tumor tissue structures.\end{tabular}                                                                                              & \begin{tabular}[c]{@{}l@{}}(1) Information and receptive field\\  loss during crop operation.\\ (2) Over-parameterization by\\  introducing complicated sub-networks.\end{tabular} \\ \hline
Myronenko \cite{myronenko20183d}      & 3D FCNN                                                                               & 3D Volumes            & \begin{tabular}[c]{@{}l@{}}(1) Additional autoencoder branch \\ for encoder backbone regularization.\end{tabular}                                                                                                   & \begin{tabular}[c]{@{}l@{}}(1) Cannot explicitly learn the hierachical \\ information of tumor tissue structures.\end{tabular}                                                      \\ \hline
Ours            & 3D FCNN                                                                               & 3D Volumes            & \begin{tabular}[c]{@{}l@{}}(1) Effectively modeling the hierachical \\ information of tumor tissue structures \\ by learning feature \\ interaction information.\\ (2) Built-in CRF for feature fusion.\end{tabular} & \begin{tabular}[c]{@{}l@{}}(1) No specific strategy to handle \\ from the data imbalance issue.\end{tabular}                                                                       \\ \hline
\end{tabular}
\end{adjustbox}
\label{table:lr}
\end{sidewaystable}

\section{Proposed Method}
\label{Proposed Method}
In this section, we describe our proposed CANet for dense voxel-wise segmentation of 3D MRI brain tumor images. We first describe the proposed HCA-FE with the feature interaction graph and convolutional space contexts in detail. Then we introduce the proposed novel fusion module, CG-ACRF, which deals with the features generated from two branches in HCA-FE and learns to output an optimal feature map. Finally, the formulation of mean-field approximation inference in CG-ACRF as convolutional operations is described, enabling the network to achieve end-to-end training. An illustration of the proposed segmentation framework is shown in Fig. \ref{fig:totalsystem}. Fig. \ref{fig:trainingflow} summarises the training steps of our CANet.

\begin{figure*}[!htb]
    \centering
    \includegraphics[width=\textwidth]{trainingflow.png}
    \caption{Training flow of the proposed CANet. Best viewed in colors.}
    \label{fig:trainingflow}
\end{figure*}

Different from previous works, our proposed HCA-FE can capture long-range contextual information in the feature space by learning the feature interaction, which has not been fully studied in the past. Both streams take the feature map $\mathcal{X} \in \mathbb{R}^{N \times C}$ derived from the shared encoder backbone as input, where $N = H\times W \times D$ is the total number of the voxels in a 3D MRI image. $H$, $W$, and $D$ represent the height, width and depth of the 3D MRI image respectively. $C$ is the number of the feature dimension. The graph stream generates representations in the feature interaction graph space $\mathcal{X}_{\mathcal{G}} \in \mathbb{R}^{N \times C}$ and the convolution stream generates a coordinate space representation $\mathcal{X}_{\mathcal{C}}\in \mathbb{R}^{N \times C}$.

The main concept behind the design of CG-ACRF is to estimate a segmentation map $\mathbf{T} \in \mathcal{T}$ associated with an MRI image $\mathbf{I} \in \mathcal{I}$ by exploiting the relationship between the final representation $\mathcal{X}_{\mathcal{F}} \in \mathbb{R}^{N \times C}$ and the intermediate feature representation $\mathcal{X}$ with auxiliary long-range contextual information $\mathcal{X}_{\mathcal{G}}$, generated from the interaction space with its convolution features $\mathcal{X}_{\mathcal{C}}$. Different from the simple concatenation $\mathcal{X}_{\mathcal{F}} = \textit{concat}(\mathcal{X}, \mathcal{X}_{\mathcal{G}}, \mathcal{X}_{\mathcal{C}})$ or element-wise summation $\mathcal{X}_{\mathcal{F}} = \mathcal{X} + \mathcal{X}_{\mathcal{G}} + \mathcal{X}_{\mathcal{C}}$, we aim to learn a set of latent feature representations $\mathcal{X}_{\mathcal{F}}^\mathcal{H} \in \mathbb{R}^{N \times C}$ through a new CRF. Due to the context information from $\mathcal{X}_{\mathcal{C}}$ and $\mathcal{X}_{\mathcal{G}}$ may contribute differently during the learning $\mathcal{X}_{\mathcal{F}}^\mathcal{H}$, we adopt the idea of an attention mechanism and generalise it into an gate node in CRF. The gate node can regulate the information flow and automatically discover the relevance between different contexts and latent features.
\subsection{Hybrid Context Aware Feature Extractor}
\label{HCG-FA}
\subsubsection{Graph Context Branch}
\textbf{Projection with Adaptive Sampling} We first use the collected feature map to create a feature interaction space by constructing an interaction graph $\mathcal{G} = \{\mathcal{V}, \mathcal{E}, A\}$, where $\mathcal{V}$ represents the set of nodes in the interaction graph, $\mathcal{E}$ represents the edges between the interaction nodes and $A$ represents the adjacency matrix. Given a learned high dimensional feature $X = \{x_{i}\}_{i=1}^{N} \in \mathbb{R}^{N \times C}$  with each $x_{i} \in \mathbb{R}^{1 \times C}$ from the back-bone network, we first project the original feature onto the feature interaction space, generating a projected feature $X_{proj} = \{x_{i}^{proj}\}_{i=1}^{N} \in \mathbb{R}^{K \times C'}$. $K$ is the number of the interaction nodes in the interaction graph and $C'$ is the interaction space dimension. A naive method for getting each element $x_{i}^{proj} \in X_{proj}, i=\{1,...,K\}$ is using the linear combination of its neighbor elements:
\begin{equation}
x_{i}^{proj} = \sum_{\forall j \in \mathcal{N}_{i}} w_{ij}x_{j} A[i,j]
\end{equation}
where $\mathcal{N}_{i}$ denotes the neighbors of pixel $i$. The naive approach normally employs a fully-connected graph with redundant connections and parameters between the interaction nodes, which is very difficult to optimise. More importantly, the linear combination method lacks an ability to perform adaptive sampling because different images contain different contextual information of brain tumors (e.g. location, size and shape). We deal with this issue by performing an adaptive sampling strategy:
\begin{equation}
    \begin{split}
        \triangle \textit{j} &= W_{i,j}x_{i} + b_{i,j}\\
        x_{i}^{proj} &= \sum_{\forall j \in \mathcal{N}_{i}} w_{ij}\rho (x_{j} | \mathcal{V}, j, \triangle j) A[i,j]
    \end{split}
\end{equation}

where $W_{i,j} \in \mathcal{R}^{3 \times (K \times C)}$ and $b_{i,j} \in \mathcal{R}^{3 \times 1}$ are the shift distances which are learned individually for each source feature $x_{i}$ through stochastic gradient decent. $\rho(\dot)$ is the trilinear interpolation sampler which can sample a shifted interaction node $x_{j}^{d}$ around feature node $x_{j}$, given the learned deformation $\triangle j$ and the total set of interaction graph nodes $\mathcal{V}$.

\textbf{Interaction Graph Reasoning} After having projected the input features into the interaction graph $\mathcal{G}$ with $K$ interaction nodes $\mathcal{V} = \{v_{1},...,v_{k}\}$ and edges $\mathcal{E}$, we follow the definition of the graph convolution network \cite{DBLP:conf/iclr/KipfW17}. In particular, we define $A_{\mathcal{G}}$ as the adjacency matrix on $K \times K$ nodes and $W_{\mathcal{G}} \in \mathbb{R}^{D \times D}$ as the weight matrix, and the formulation of the graph convolution operation is formulated as follows:
\begin{equation}
\begin{split}
    X_{\mathcal{GC}} &= \sigma(A X_{proj} W_{\mathcal{G}})\\
    &= \sigma((I - \hat{A}) X_{proj} W_{\mathcal{G}})
\end{split}
\end{equation}
where $\sigma()$ is sigmoid activation function. We first apply Laplacian smoothing and update the adjacency matrix to $(I - \hat{A})$ so as to propagate the node feature over the entire graph.
In practice, we implement $\hat{A}$ and $W_{\mathcal{G}}$ using a $1 \times 1$ convolution layer. We also achieve the implementation of $I$ as a residual connection which can maximise the gradient flow with a faster convergence speed.

\textbf{Re-Projection} Once the feature propagation has been finished, we re-project the features back to the original coordinate space with output $\mathcal{X}_{\mathcal{G}} \in \mathbb{R}^{N \times D}$. Similar to the projection step, we use trilinear interpolation here to calculate each elements $x_{\mathcal{G}}^{i} \in \mathcal{X}_{\mathcal{G}}, i \in \{1,...,N\}$ after having transformed the feature from the interaction space to the coordinate space. As a result, we have the feature $\mathcal{X}_{\mathcal{G}}$ with feature dimension $D$ at all $N$ grid coordinates.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{differentCRF4.png}
    \caption{{{A graph model illustration of previous fusion schemes: (a) basic encoder-decoder neural network, (b) multi-scale neural network, (c) multi-scale CRF, and  our proposed (d) context guided attentive fusion CRF. $I$ denotes the input 3D MRI image. $f_s$ denotes the feature map at scale $s$. $a_s$ indicates the attention map generated from the corresponding feature $f$ at scale $s$. $h_c$ and $h_g$ represent the hidden feature generated from convolutional features and graph convolutional features respectively. $L$ means the final segmentation labeling output. Best viewed in color.}}}
    \label{fig:crfcomp}
\end{figure*}

\subsubsection{Convolution Context Branch}
The convolution context branch is composed of a contracting path (encoder) and an expansive path (decoder) with skip connections between these two paths. The contracting path reduces the spatial dimensionality of the pooling layer in a pyramidal scale whilst the expansive path recovers the spatial dimensionality and the details of the object with the corresponding pyramid scale. One of the advantages of using this architecture is that it fully utilises the features with different scales of contextual information, where large scale features can be used to localise objects and small scale but high dimensionality features can provide more detailed and accurate information for classification.

However, 3D volumetric images require more parameters to learn during feature extraction. It is often observed that training such 3D model often fails for various reasons such as over-fitting and gradient vanishing or exploding. Besides, simple or complicated augmentation technologies used to extend the training dataset may result in a slow convergence speed. To address the issues mentioned above, we develop a deep supervised mechanism that inherits the advantages of the convolution context branch. The proposed deep supervision mechanism thus reinforces the gradient flow and improves the discriminative capability during the training procedure.

Specifically, we use additional upsampling layers to reshape the features created at the deep supervised layer to be of the resolution of the final input. For each transformed layer, we apply the softmax function to obtain additional dense segmentation maps. For these additional segmentation results, we calculate the segmentation errors with regards to the ground-truth segmentation maps. The auxiliary losses are integrated with the loss from the output layer of the whole network and we further back-propagate the gradient for parameter updating during each iteration in the training stage.

We denote the set of the parameters in the deep supervised layers as $W_{S} = \{w^{i}\}_{i=1}^{S}$ and $w^{s}$ as the parameters of the upsampling layer correspond to layer $s$. The auxiliary loss for a deep supervision layer $s$ is formulated using cross-entropy:
\begin{equation}
    \mathcal{L}_{s}(\mathcal{X};W_{S}) = \sum_{i=1}^{S}\sum_{j=1}^{N}-\log  \mathbbm{1}(p(y_{j}|x_{j}^{s};w^{s}))
\end{equation}
where $\mathbbm{1}$ is the indicator function which is 1 if the segmentation result is correct, otherwise 0.  $\mathcal{Y} = \{y_{i}\}_{i=1}^{N}$ is the ground-truth of voxel $i$ and $\mathcal{X}_{S} = \{x^{s}_{i}\}_{i=1,s=1}^{N,S}$ is the predicted segmentation label of voxel $i$ generated from the upsampling layer $s$. Finally, the deep supervision loss $\mathcal{L}_{s}$ can be integrated with the loss $\mathcal{L}_{T}$ from the final output layer. The parameters of the deep supervised layers $W_{S}$ can be updated with the rest parameters $W$ from the whole framework simultaneously using back-propagation:
\begin{equation}
\begin{split}
      \mathcal{L} = \mathcal{L}_{T}(\mathcal{Y}| & \mathcal{X};W, W_{S}) + \sum_{s=1}^{S}\delta_{s}\mathcal{L}_{s}(\mathcal{X};w^{s})\\
      &+ \lambda(||W||^{2} + \sum_{s=1}^{S}(||w^{s}||^{2}))
\end{split}
\label{cnnstreamtogetherloss}
\end{equation}
where $\delta_{s}$ represents the weight factor for the supervision loss of each upsampling layer. As the training procedure continues to approach to the optimal parameter sets, $\delta_{s}$ reduces gradually. The final operation of Eq. (\ref{cnnstreamtogetherloss}) is the $L$2-regularisation of the total trainable weights with the weight factor $\lambda$.

%\subsection{Discrete Adaboost}
%myclassification algorithm is built upon the discrete Adaboost algorithm proposed by Freud et al. \cite{freund1996experiments}. 

\subsection{Context Guided Attentive Conditional Random Field}
We further propose a novel context guided attentive CRF module to perform feature fusion, motivated from two perspectives. A graph model of our proposed CG-ACRF is illustrated in Fig. \ref{fig:crfcomp}. There are two reasons to use CG-ACRF for feature fusion. Firstly, assigning segmentation labels by maximising probabilities may result in blurry boundaries due to the neighboring voxels sharing similar spatial contexts. Secondly, previous works fuse information from different sources (e.g. multi-scale or multi-stage) by using simple channel-wise concatenation or element-wise summation mechanism. However, these mechanisms do not take into account the heterogeneity between different feature maps (e.g. shallow layers tend to focus on low-level visual features while deep layers tend to attend abstract features). Simplifying the relationship between different source feature maps (e.g. feature maps of large kernels tend to represent the object outline while feature maps of small kernels tend to encode the details of the object structure) results in information loss. Different from previous related works and using the inference ability of the probabilistic graphical model, we employ the conditional random field model to learn optimised latent fusion features for final segmentation. As information from different contexts may contribute to the final results at different degrees, we integrate the attention gates of the CRF to regulate how much information should flow between features generated from different contexts. We further show the convolution formulation of CG-ACRF mean-field approximation inference, which allows our attentive CRF fusion module to be integrated into neural networks as a layer and trained in an end-to-end fashion. Compared with previous architectures such as an encoder-decoder neural network (Fig. \ref{fig:crfcomp} (a)) and multi-scale neural network (Fig. \ref{fig:crfcomp} (b)), our proposed CG-ACRF (Fig. \ref{fig:crfcomp} (d)) has a strong inference ability and can jointly learn the hidden representation of features encoded by the neural network backbone, improving the generalisation ability of the segmentation model. Compared with previous architectures such as multi-scale CRF (Fig. \ref{fig:crfcomp} (c)), our proposed CG-ACRF model first uses an attention gate by directly modeling the cost energy in the network (Eq. (\ref{EnergyDefinition})). The attention gate thus regulates the information flow from the features encoded by the backbone neural network to the latent representations by minimising the total energy cost. Moreover, our proposed CG-ACRF learns to project the features into two spaces, i.e. a convolutional space and a feature interaction graph. Hidden representations from different spaces can further boost feature fusion performance. We evaluate the effectiveness of each component in the experiment section.

\subsubsection{Definition}
Given the feature map $\mathcal{X}_{\mathcal{C}} = \{x_{\mathcal{C}}^{i}\}_{i=1}^{N}$ from the convolution context branch and the feature map $\mathcal{X}_{\mathcal{G}} = \{x_{\mathcal{G}}^{i}\}_{i=1}^{N}$ from the interaction graph branch, our goal is to estimate the fusion representation $H_{\mathcal{G}} = \{h_{\mathcal{G}}^{i}\}_{i=1}^{N}, H_{\mathcal{C}} = \{h_{\mathcal{C}}^{i}\}_{i=1}^{N}$ and the attention variable $A = \{a_{\mathcal{G}\mathcal{C}}^{i}\}_{i=1}^{N}$. We formalise the problem by designing a Context Guided Attentive Conditional Random Field with a Gibbs distribution as follows:
\begin{equation}
    P(H,A|I,\Theta) = \frac{1}{Z(I, \Theta)}exp\{-E(H,A,I,\Theta)\}
\end{equation}
where $E(H,A,I,\Theta)$ is the associated energy, and
\begin{equation}
     E(H,A,I,\Theta) = \Phi_{\mathcal{G}}(H_{\mathcal{G}}, X_{\mathcal{G}}) + \Phi_{\mathcal{C}}(H_{\mathcal{C}}, X_{\mathcal{C}}) + \Psi_{\mathcal{G}\mathcal{C}}(H_{\mathcal{G}}, H_{\mathcal{C}}, A)
\label{EnergyDefinition}
\end{equation}
where $I$ is the input 3D MRI image and $\Theta$ is the set of parameters. In Eq.(\ref{EnergyDefinition}), $\Phi_{\mathcal{G}}$ is the unary potential between the latent graph representation $h_{\mathcal{G}}^{i}$ and the graph features $x_{\mathcal{G}}^i$. $\Phi_{\mathcal{C}}$ is the unary potential related to latent convolution representation $h_{\mathcal{C}}^i$ and convolution feature $x_{\mathcal{C}}^i$. In order to enable the estimated latent representation $h^i$ to be close to the observation $x^i$, we use the Gaussian function created in previous works \cite{krahenbuhl2011efficient}:

\begin{equation}
    \Phi(H, X) = \sum_{i}\phi(h^i, x^i) = -\sum_{i=1}^{N}\frac{1}{2}||h^{i} - x^{i}||^{2}.
\end{equation}

The final term shown in Eq. (\ref{EnergyDefinition}) is the attention guided pairwise potential between the latent convolution representation $h_{\mathcal{C}}^i$ and the latent graph representation $h_{\mathcal{G}}^i$. The attention term $a_{\mathcal{GC}}^{i}$ controls the information flow between the two latent representations where the graph representation may or may not contribute to the estimated convolution representation. We define:
\begin{equation}
\begin{split}
    \Psi_{\mathcal{G}\mathcal{C}}(H_{\mathcal{G}}, H_{\mathcal{C}}, & A) = \sum_{i=1}^{N}\sum_{j \in \mathcal{N}_{i}}\psi(a_{\mathcal{GC}}^{j}, h_{\mathcal{C}}^{i}, h_{\mathcal{G}}^{j})\\
    & = \sum_{i=1}^{N}\sum_{j \in \mathcal{N}_{i}}a_{\mathcal{GC}}^{j} h_{\mathcal{C}}^{i} \Upsilon_{\mathcal{GC}}^{i,j} h_{\mathcal{G}}^{j}
\end{split}
\end{equation}
where $\Upsilon_{\mathcal{GC}}^{i,j} \in \mathbb{R}^{D_{\mathcal{G}} \times D_{\mathcal{C}}}$ and $D_{\mathcal{G}}, D_{\mathcal{C}}$ represent the dimensionality of the features $X_{\mathcal{G}}$ and $X_{\mathcal{C}}$ respectively.

\subsubsection{Inference}
\label{MFCRF}
By learning latent feature representations to minimise the total segmentation energy, the system can produce an appropriate segmentation map, \textit{e.g.} the maximum a posterior $P(H,A|I,\Theta)$. However, the optimisation of $P(H, A|I,\Theta)$ is intractable due to the computational complexity in normalising constant $Z(I, \Theta)$, which is exponentially proportional to the cardinality of $h \in H$ and $a \in A_{\mathcal{GC}}$. Therefore, in order to derive the maximum a posterior in an efficient way, we adopt mean-field approximation to approximate a complex posterior probability distribution. We have:
\begin{equation}
    P(H,A | I,\Theta) \approx Q(H,A) = \prod_{i=1}^{N} q_{i}(h_{\mathcal{G}}^i)q_{i}(h_{\mathcal{C}}^i)q_{i}(a_{\mathcal{G}\mathcal{C}}^i)
\label{mfdef}
\end{equation}

Here we use the product of independent marginal distributions $q(h_{\mathcal{G}}^i)$, $q(h_{\mathcal{C}}^i)$ and $q(a_{\mathcal{G}\mathcal{C}}^i)$ to approximate the complex distribution $P(H,A,I,\Theta)$. To achieve a satisfactory approximation result, we minimise the Kullback-Leibler (KL) divergence $D_{KL}(Q||P)$ between the two distributions $Q$ and $P$. By replacing the definition of the energy $E(H,A,I,\Theta)$, we formulate the KL divergence in Eq. (\ref{mfdef}) as follows:
\begin{equation}
\begin{split}
    D_{KL}(Q||P) &= \sum_{h}Q(h)\ln (\frac{Q(h)}{P(h)})\\
    &=\sum_{h}Q(h)E(h) + \sum_{h}Q(h)\ln Q(h) + \ln Z
\end{split}
\label{kl1}
\end{equation}
From Eq.(\ref{kl1}), we can minimise the KL divergence by directly minimising the free energy $FE(Q) = \sum_{h}Q(h)E(h) + \sum_{h}Q(h)\ln (Q(h))$. In $FE(Q)$, the first item represents the cross-entropy between two distributions $Q$ and $E$ and the second item represents the entropy of distribution $Q$. We can further expand the expression of $FE(Q)$ by replacing $Q$ and $E$ with Eqs. (\ref{mfdef}) and (\ref{EnergyDefinition}) respectively:

\begin{equation}
\begin{split}
    FE(Q) &= \sum_{i=1}^{N}q_{i}(h_{\mathcal{G}}^i)q_{i}(h_{\mathcal{C}}^i)q_{i}(a_{\mathcal{G}\mathcal{C}}^i)(\Phi_{\mathcal{G}}+\Phi_{\mathcal{C}}+\Psi_{\mathcal{GC}})\\
    &+ \sum_{i=1}^{N}q_{i}(h_{\mathcal{G}}^i)q_{i}(h_{\mathcal{C}}^i)q_{i}(a_{\mathcal{G}\mathcal{C}}^i)(\ln (q_{i}(h_{\mathcal{G}}^i)q_{i}(h_{\mathcal{C}}^i)q_{i}(a_{\mathcal{G}\mathcal{C}}^i)))
\end{split}
\label{FEDef}
\end{equation}

Eq. (\ref{FEDef}) shows that the problem of minimising $FE(Q)$ can be transferred to a constrained optimisation problem with multiple variables, which can be formally formulated below:

\begin{equation}
\begin{split}
    & \min_{q_{i}(h_{\mathcal{G}}^i), q_{i}(h_{\mathcal{C}}^i), q_{i}(a_{\mathcal{G}\mathcal{C}}^i)} FE(Q), \forall i  \in N\\
    & \textrm{s.t.} \sum_{l=1}^{L}q_{i}(h_{\mathcal{G}}^i) = 1, \sum_{l=1}^{L}q_{i}(h_{\mathcal{C}}^i) = 1, \int_{0}^{1} q_{i}(a_{\mathcal{GC}}^i)da^{i}_{\mathcal{GC}} = 1
\end{split}
\label{optDef}
\end{equation}
where $l$ represents the index of the segmentation label. We can calculate the first order partial derivative by differentiating $FE(Q)$ w.r.t each variable. For example, we have:

\begin{equation}
\begin{split}
    \frac{\partial FE}{\partial q_{i}(h^{i}_{\mathcal{C}})} = \phi_{\mathcal{C}}(h_{\mathcal{C}}^{i}, x_{\mathcal{C}}^{i}) +& \sum_{j\in \mathcal{N}_{i}}\mathbb{E}_{q_{j}(a_{\mathcal{GC}}^{j})} \{ a_{\mathcal{GC}}^{j} \} \mathbb{E}_{q_{j}(h_{\mathcal{G}}^j)} \psi (h_{\mathcal{C}}^i, h_{\mathcal{G}}^j)\\
    &- \ln q_{i}(h^{i}_{\mathcal{C}}) + \text{const}
\end{split}
\label{partialDer}
\end{equation}

By assigning 0 to the left of Eq. (\ref{partialDer}), we reach:
\begin{equation}
\begin{split}
     q_{i}(h_{\mathcal{C}}^{i}) \propto & \exp \Big\{ \phi_{\mathcal{C}}(h_{\mathcal{C}}^{i}, x_{\mathcal{C}}^{i}) + \\
     & \sum_{j \in N_{i}} \mathbb{E}_{q_{j}(a_{\mathcal{GC}}^{j})} \{ a_{\mathcal{GC}}^{j} \} \mathbb{E}_{q_{j}(h_{\mathcal{G}}^j)} \psi (h_{\mathcal{C}}^i, h_{\mathcal{G}}^j) \Big \}
\end{split}
\label{qhcdef}
\end{equation}

Eq. (\ref{qhcdef}) shows that, once the other two independent variables $q(h_{\mathcal{G}})$ and  $q(a_{\mathcal{GC}})$ are fixed, how $ q(h_{\mathcal{C}})$ is updated during the mean-field approximation inference. Further more, we follow the above procedure and obtain the updating of the remaining two variable as follows:
\begin{equation}
\begin{split}
     q_{i}(h_{\mathcal{G}}^{i}) \propto & \exp \Big\{ \phi_{\mathcal{G}}(h_{\mathcal{G}}^{i}, x_{\mathcal{C}}^{i}) + \\
     & \mathbb{E}_{q_{j}(a_{\mathcal{GC}}^{j})} \{ a_{\mathcal{GC}}^{j} \} \sum_{j \in N_{i}} \mathbb{E}_{q_{j}(h_{\mathcal{C}}^j)} \psi (h_{\mathcal{C}}^i, h_{\mathcal{G}}^j) \Big \}
\end{split}
\label{qhgdef}
\end{equation}
\begin{equation}
    q_{i}(a_{\mathcal{GC}}^{i}) \propto \exp \Big\{a_{\mathcal{GC}}^{i} \mathbb{E}_{q_{i}(h_{\mathcal{C}}^{i})} \{ \sum_{j \in N_{i}} \mathbb{E}_{q_{j}(h_{\mathcal{G}}^{j})} \{ \psi(h_{\mathcal{C}}^{i}, h_{\mathcal{G}}^{j})\} \} \Big \}
\label{qatdef}
\end{equation}
where $\mathbb{E}_{q()}$ represents the expectation with respect to the distribution $q()$. Eqs. (\ref{qhcdef}-\ref{qatdef}) shown above denote the computational procedure of seeking an optimal posterior distributions of $h_{\mathcal{C}}$, $h_{\mathcal{G}}$ and $a_{\mathcal{GC}}$ during the mean-field approximation. Intuitively, Eq. (\ref{qhcdef}) shows that, the latent convolution feature $h_{\mathcal{C}}^{i}$ for voxel $i$ can be  used to describe the observation, referred to feature $x_{\mathcal{C}}^{i}$. Afterwards, we use the re-weighted messages from the latent features of the neighboring voxels to learn the co-occurrent relationship of the pixels. The attention weight between the latent convolution and the graph features for voxel $i$ allows us to re-weight the pairwise potential message from the neighbours of voxel $i$, and then use the attention variable to re-weight the total value of voxel $i$. By denoting $\Bar{a}_{\mathcal{G}\mathcal{C}}^{i} = \mathbb{E}_{q(a_{\mathcal{G}\mathcal{C}}^{i})}\{a_{\mathcal{G}\mathcal{C}}^{i}\}$ and $\Bar{h}^{i} = \mathbb{E}_{q(h^{i})}\{h^{i}\}$, we have the feature update as follows:
\begin{equation}
    \Bar{h}_{\mathcal{G}}^{i} = x_{\mathcal{G}}^{i} + \Bar{a}_{\mathcal{G}\mathcal{C}}^{i} \sum_{j \in N_{i}} \Upsilon_{\mathcal{G}\mathcal{C}}^{i,j} \Bar{h}_{\mathcal{C}}^{j}
\end{equation}

\begin{equation}
    \Bar{h}_{\mathcal{C}}^{i} = x_{\mathcal{C}}^{i} + \sum_{j \in N_{i}} \Bar{a}_{\mathcal{G}\mathcal{C}}^{j}  \Upsilon_{\mathcal{G}\mathcal{C}}^{i,j} \Bar{h}_{\mathcal{G}}^{j}
\end{equation}

$\Bar{a}_{\mathcal{G}\mathcal{C}}^{i}$ is also derived from the probabilistic distribution, \textit{i.e.} its value lies in $[0,1]$. Here, we choose the Sigmoid function to formulate the updates for $\Bar{a}_{\mathcal{G}\mathcal{C}}^{i}$:
\begin{equation}
    \Bar{a}_{\mathcal{G}\mathcal{C}}^{i} = \sigma(- \sum_{j \in N_{i}}a_{\mathcal{GC}}^{j} h_{\mathcal{C}}^{i} \Upsilon_{\mathcal{GC}}^{i,j} h_{\mathcal{G}}^{j})
\end{equation}
where $\sigma(.)$ denotes the Sigmoid activation function.

\subsubsection{Mean Field Inference as Convolution Operation}
To achieve joint training and end-to-end optimisation of the proposed CRF with the backbone network, we implement the mean-field approximation of the proposed CRF in neural networks. We aim to perform the updating of the latent feature and attention maps according to the derivation described in Section \ref{MFCRF}. The algorithm for implementing mean-field approximation using convolutional operations is described in Algorithm \ref{Alg:convmf}. A graph illustration of Algorithm \ref{Alg:convmf} is shown in Fig. \ref{fig:crfmf}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{CGACRFMFIteration.png}
    \caption{Details of the mean-field updates within CG-ACRF. The circled symbols indicate message-passing operations within the CG-ACRF block. Best viewed in colors.}
    \label{fig:crfmf}
\end{figure}


\begin{algorithm}%[H]
\caption{Algorithm for Mean-Field Approximation.}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Feature interaction graph output $x_{\mathcal{G}}$ and convolution output $x_{\mathcal{C}}$. Initialize hidden graph feature map $h_{\mathcal{G}}$ with $x_{\mathcal{G}}$. Initialize hidden convolutional feature map $h_{\mathcal{C}}$ with $x_{\mathcal{C}}$
\ENSURE Estimated optimised hidden convolution feature map $h$.
\WHILE {in iteration number}
 \STATE $\hat{a}_{\mathcal{G}\mathcal{C}} \leftarrow h_{\mathcal{C}} \odot (\Upsilon_{\mathcal{G}\mathcal{C}} \ast h_{\mathcal{G}})$;
 \STATE $\Bar{a}_{\mathcal{G}\mathcal{C}} \leftarrow \sigma(-(\hat{a}_{\mathcal{G}\mathcal{C}}))$;
 \STATE $h_{\mathcal{G}} \leftarrow \Upsilon_{\mathcal{G}\mathcal{C}} \ast h_{\mathcal{G}}$;
 \STATE $\Bar{h}_{\mathcal{C}} \leftarrow \Bar{a}_{\mathcal{G}\mathcal{C}} \odot h_{\mathcal{G}}$;
 \STATE $h \leftarrow x \oplus \Bar{h}_{\mathcal{C}}$;
\ENDWHILE
\RETURN Optimised hidden feature map $h$.
\end{algorithmic}
\label{Alg:convmf}
\end{algorithm}

where $\ast, \odot, \oplus$ represent the convolution, element-wise dot product, and element-wise summation respectively. First, the latent feature map is initialised with corresponding observation inputs $x_{\mathcal{G}}$ and $x_{\mathcal{C}}$, while the attention map is initialised from the message passing on the two latent feature maps. Then, we activate and normalise the attention map. The latent convolutional feature map is updated from the message passing on the latent graph feature map. Finally, the updated attention map is used to refine the latent convolutional feature map $h$. We output $h$ with the unary term $x$ by establishing residual connections.


\section{Experiment Setup}
\label{Experimental Setup}
To demonstrate the effectiveness of the proposed CANet for brain tumor segmentation, we conduct experiments on two publicly available datasets: the Multimodal Brain Tumor Segmentation Challenge 2017 (BraTS2017) and the Multimodal Brain Tumor Segmentation Challenge 2018 (BraTS2018). 

\subsection{Datasets and Evaluation Metrics}
\textbf{Datasets.} The \textbf{BraTS2017}\footnote{https://www.med.upenn.edu/sbia/brats2017.html} consists of 285 cases of patients in the training set and 44 cases in the validation set. \textbf{BraTS2018}\footnote{https://www.med.upenn.edu/sbia/brats2018.html} shares the same training set with BraTS2017 and includes 66 cases in the validation set. Each case is composed of four MR sequences, namely native T1-weighted (T1), post-contrast T1-weighted (T1ce), T2-weighted (T2) and Fluid Attenuated Inversion Recovery (FLAIR). Each sequence has a 3D MRI volume of 240$\times$240$\times$155. Ground-truth annotation is only provided in the training set, which contains the background and healthy tissues (label 0), necrotic and non-enhancing tumor (label 1), peritumoral edema (label 2) and GD-enhancing tumor (label 4). We first consider the 5-fold cross-validation on the training set where each fold contains (random division) 228 cases for training and 57 cases for validation. We then evaluate the performance of the proposed method on the validation set. The validation result is generated from the official server of the contest to determine the segmentation accuracy of the proposed methods.	

\textbf{Evaluation Metrics.} Following previous works \cite{wang2017automatic}, \cite{kamnitsas2017efficient}, \cite{bakas2017advancing}, the segmentation accuracy is measured by Dice score, Sensitivity, Specificity and Hausdorff95 distance respectively. In particular,
\begin{itemize}
    \item Dice score: $Dice(P,T) = \frac{|P_1 \cap T_1|}{(|P_1|+|T_1|)/2}$
    \item Sensitivity: $Sens(P,T) = \frac{|P_1 \cap T_1|}{|T_1|}$
    \item Specificity: $Spec(P,T) = \frac{|P_0 \cap T_0|}{|T_0|}$
    \item Hausdorff Distance: $Haus(P,T) = \\
    max\{sup_{p\in P_1}inf_{t\in T_1} d(p,t), sup_{t\in T_1}inf_{p\in P_1} d(t,p)\}$
\end{itemize}
where $P$ represents the model prediction and $T$ represents the ground-truth annotation. $T_1$ and $T_0$ are the subset voxels predicted as positives and negatives for the tumor region. Similar set-ups are made for $P_1$ and $P_0$. Furthermore, the Hausdorff95 distance measures how far the model prediction deviates from the ground-truth annotation. $sup$ represents the supremum and $inf$ represents the infimum. For each metric, three regions namely enhancing tumor (ET, label 1), whole tumor (WT, labels 1, 2 and 4) and the tumor core (TC, labels 1 and 4) are evaluated individually.
\subsection{Data Augmentation and Implementation Details}
\textbf{Data Augmentation} For each sequence in each case, we set all the voxels outside the brain to zero and normalise the intensity of the non-background voxels to be of zero mean and unit variance. During the training, we use randomly cropped images of size 128$\times$128$\times$128.
We further set up a common augmentation strategy for each sequence in each case: (i) randomly rotate an image with the angle between [-20$^{\circ}$, +20$^{\circ}$]; (ii) randomly scale an image with a factor of 1.1; (iii) randomly mirror flip an image across the axial coronal and sagittal planes with the probability of 0.5; (iv) random intensity shift between [-0.1, +0.1]; (v) random elastic deformation with $\sigma = 10$.

\textbf{Implementation Details} We implement the proposed CANet and other benchmark experiments using the PyTorch framework and deploy all the experiments on 2 parallel Nvidia Tesla P100 GPUs for 200 epochs with a batch size of 4. We use the Adam optimizer with an initial learning rate $\alpha_0 = 1\mathrm{e}{-4}$. The learning rate is decreased by a factor of 5 after 100, 125 and 150 epochs. We use a $L$2 regulariser with a weight decay of $1\mathrm{e}{-5}$. We store the weights for each epoch and use the weights that lead to the best dice score for inference.

\section{Experimental Result} 
\label{resultdiscussion}
In this section, we present both quantitative and qualitative experimental results of different evaluations. We first conduct an ablation study of our method to show the effective impact of HCA-FE and CG-ACRF on the segmentation performance. We also perform additional analysis of the encoder backbone and different iteration numbers of approximation for CG-ACRF. Afterward, we compare our approach with several state-of-the-art methods on different datasets. Finally, we present the analysis of failure cases.

\subsection{Ablation Studies}
We first evaluate the effect of HCA-FE and CG-ACRF. To this end, we apply a 5-fold cross-evaluation on the BraTS2017 training set and report the mean result. Table \ref{table:ablationstudy} shows the quantitative results, while the qualitative results can be found in Fig. \ref{fig:ablation} as an example of the segmentation outputs. We start from two baselines. The first baseline is in the fully convolution format with deep supervision on the backbone convolution encoder (CC). The second baseline only uses graph convolution in the convolution encoder without deep supervision (GC). We then evaluate the proposed whole HCA-FE (CC+GC) without any feature fusion method, \textit{i.e.} concatenating feature maps from CC and GC together. Finally, we evaluate the proposed feature fusion module CG-ACRF, which takes a feature map with different contexts from HCA-FE and outputs the optimal latent feature map for the final segmentation. For the experiments are shown in Table \ref{table:ablationstudy} and Fig. \ref{fig:ablation}, we use the encoder of UNet as the backbone network with 5 iterations in CG-ACRF. The experiments described later include the analysis of different backbones and iteration numbers.

\begin{table*}[!ht]
\centering
\caption{Quantitative results of the CANet components by five fold cross-validation for the BraTS2017 training set (dice, sensitivity and specificity). All the methods are based on CANet with UNet as the backbone. The best result is shown in bold text and the runner-up result is underlined.}
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
              & \multicolumn{3}{c|}{DICE}                              & \multicolumn{3}{c|}{Sensitivity}                       & \multicolumn{3}{c|}{Specificity}                       & \multicolumn{3}{c|}{Hausdorff95}                       \\ \hline
Backbone+         & ET               & WT               & TC               & ET               & WT               & TC               & ET               & WT               & TC               & ET               & WT               & TC               \\ \hline
CC            & \textbf{0.68628} & 0.87467          & 0.82068          & 0.85684          & {\underline{0.92495}}    & 0.86324          & {\underline{0.99708}}    & {\underline{0.99094}}    & 0.99562          & \textbf{6.79149} & 6.88633          & 7.93923          \\ \hline
GC            & 0.6373           & {\underline{0.89365}}    & {\underline{0.82246}}    & \textbf{0.97704} & \textbf{0.96964} & \textbf{0.94428} & 0.98723          & 0.98742          & \textbf{0.99665} & 9.89949          & {\underline{6.40312}}    & {\underline{5.81216}}    \\ \hline
CC+GC+Concatenation         & 0.68194          & 0.86073          & 0.80306          & {\underline{0.85725}}    & 0.92243          & 0.86085          & 0.99672          & 0.98913          & 0.99351          & {\underline{7.75539}}    & 9.37745          & 11.43241         \\ \hline
CC+GC+CG-ACRF & {\underline{0.68489}}    & \textbf{0.90338} & \textbf{0.87291} & 0.80651          & 0.92363          & {\underline{0.86989}}    & \textbf{0.99746} & \textbf{0.99307} & {\underline{0.99592}}    & 7.80448          & \textbf{3.56898} & \textbf{4.03629} \\ \hline
\end{tabular}
\end{adjustbox}
\label{table:ablationstudy}
\end{table*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{ablation.png}
    \caption{Qualitative comparison of different baseline models and the proposed CANet by cross-validation on BraTS2017 training set. From left to right, each column represents the input FLAIR data, ground truth annotation, segmentation result of CANet with only the convolution branch, segmentation result of CANet with only the graph convolution branch, segmentation output of CANet with HCA-FE and concatenation fusion scheme, segmentation output of CANet with HCA-FE and CG-ACRF fusion module. Best viewed in colors.}
    \label{fig:ablation}
\end{figure*}

From Table \ref{table:ablationstudy}, we observe that the GC obtains better performance than CC. For the dice score, GC achieves 0.89365 for the entire tumor and 0.82246 for the tumor core. CC only achieves a dice score of 0.87467 on the entire tumor and 0.82068 on the tumor core, which is 2\% and 0.2\% lower than those by GC respectively. For hausdorff95, GC achieves 6.40312 on the entire tumor and 5.81216 on the tumor core. CC achieves 6.88633 and 7.93923, which are 0.49321 and 2.12707 higher than those of GC on the entire tumor and the tumor core, respectively. From Fig. \ref{fig:ablation}, we observe that GC can accurately predict individual regions. For example, the GD-enhanced tumor region normally does not appear at the outside of the tumor region. This superior performance may benefit from the information learned from the feature interactive graph as the feature nodes of different tumor regions have a strong structural association between them. Learning the relationship may help the system to predict correct labels of the tumor regions. However, the sensitivity of GC is much higher than that of CC. In Table \ref{table:ablationstudy}, for example, the sensitivity score of GC is higher than that of CC: 12.02\% higher on the enhancing tumor, 4.469\% higher on the entire tumor, 8.104\% higher on tumor core, respectively. We observe poor segmentation results at the NCR/ECT region by GC, worse than CC and the ground truth shown in Fig. \ref{fig:ablation}.

We then evaluate the complete HCA-FE with the extracted feature maps by CC and GC simultaneously. Here, we fuse the feature maps of CC and GC using a naive concatenation method. The HCA-FE has less over-segmentation results, depicted in Table \ref{table:ablationstudy}, where the sensitivity of CC+GC is much lower than that of GC. The sensitivity of CC+GC is  0.85725 on the enhancing tumor (ET), 0.92243 on the whole tumor (WT) and 0.86085 on the tumor core (TC), respectively. From Fig. \ref{fig:ablation}, we witness that by introducing the complete HCA-FE, the segmentation model can correct some misclassified regions produced by CC. However, the concatenation fusion method does not demonstrate any benefit to the overall segmentation. CC+GC has a dice score of 0.86073 on the whole tumor and 0.80306 on the tumor core, which is 3.292\% and 1.94\% lower than those of GC respectively. We also observe the loss of the boundary information in Fig. \ref{fig:ablation}, especially the boundaries of NCR/ECT and GD-enhancing tumors excessively shrinks compared with those of GC and CC.

We finally evaluate the effectiveness of our proposed CG-ACRF. By introducing the CG-ACRF fusion module, our segmentation model outperforms the other methods. Benefiting from the inference ability of CG-ACRF, it presents a satisfactory segmentation output. For the whole tumor and the tumor core, its Dice scores are 0.90338 and 0.87291 respectively, which are the top scores in the leader-board. Its Hausdorff95 also is the lowest. For the whole tumor and the tumor core, its hausdorff95 values are 3.56898 and 4.03629 respectively. Referring to much lower sensitivity scores reported in Table \ref{table:ablationstudy}, we conclude that the superior performance has been achieved by the complete CANet. The same conclusion can be drawn from Fig. \ref{fig:ablation} where CG-ACRF can detect optimal feature maps that benefit the downstream deconvolution networks and outline small tumor cores and edges, which may be lost when we use a down-sampling operation in the encoder backbone.


\subsection{Iteration Test}
As described in Algorithm 1, we manually set the iteration number in the mean-field approximation of CG-ACRF. Since the mean-field approximation cannot guarantee a convergence point, we examine the effectiveness of different iteration numbers. Table \ref{table:convmf} reports the quantitative result of using different iteration numbers, i.e. 1, 3, 5, 7, and 10. With the increase of iterations, our proposed model performs better. However, no additional benefit is gained when the iteration number becomes over 5. Fig. \ref{fig:ProbMap} presents the probability map during segmentation, where the light color represents the region with a lower probability while the dark color represents the area with a higher probability. We observe that using only one iteration, CANet can outline the region of interest using the fused feature maps. By increasing the iteration number to 3 or 5, CG-ACRF can gradually extract an optimal feature map, leading to accurate segmentation. We further increase the iteration number to 7 and 10 but no further improvement has been made. Therefore, we set the iteration number to 5 as a good trade-off between the segmentation performance and the number of the engaged parameters.

\begin{table*}[!ht]
\centering
\caption{Quantitative results of different iteration numbers by CG-ACRF mean-field approximation on the five fold cross-validation of the BraTS2017 training set with respect to Dice, Sensitivity, Specificity and Hausdorff95. The best result is in bold and the runner-up result is underlined.}
% Please add the following required packages to your document preamble:
% \usepackage[normalem]{ulem}
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
             & \multicolumn{3}{c|}{Dice}                              & \multicolumn{3}{c|}{Sensitivity}                       & \multicolumn{3}{c|}{Specificity}                       & \multicolumn{3}{c|}{Hausdorff95}                       \\ \hline
Iteration \# & ET               & WT               & TC               & ET               & WT               & TC               & ET               & WT               & TC               & ET               & WT               & TC               \\ \hline
1            & 0.65697          & 0.86066          & 0.79005          & \textbf{0.90075} & 0.92017          & 0.85205          & 0.9953           & 0.98993          & {\underline{0.99426}}    & 7.99666          & 7.74909          & 10.48848         \\ \hline
3            & 0.68131          & {\underline{0.87267}}    & {\underline{0.80679}}    & {\underline{0.87265}}    & 0.92288          & {\underline{0.86948}}    & 0.99638          & {\underline{0.99007}}    & 0.99397          & \textbf{7.61352} & {\underline{6.8011}}     & {\underline{8.94057}}    \\ \hline
7            & 0.6643           & 0.85534          & 0.76902          & 0.85384          & 0.92108          & 0.86033          & 0.99644          & 0.98955          & 0.99336          & 9.84976          & 9.72             & 12.04193         \\ \hline
10           & {\underline{0.68484}}    & 0.85043          & 0.7839           & 0.83708          & \textbf{0.93128} & 0.85847          & {\underline{0.99675}}    & 0.98757          & 0.99383          & 8.06683          & 11.14894         & 11.64947         \\ \hline
Ours(5)    & \textbf{0.68489} & \textbf{0.90338} & \textbf{0.87291} & 0.80651          & {\underline{0.92363}}    & \textbf{0.86989} & \textbf{0.99746} & \textbf{0.99307} & \textbf{0.99592} & {\underline{7.80448}}    & \textbf{3.56898} & \textbf{4.03629} \\ \hline
\end{tabular}
\end{adjustbox}
\label{table:convmf}
\end{table*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{ProbMap_Iteration.png}
    \caption{Examples to illustrate the effectiveness of different iteration numbers by mean-field approximation in CG-ACRF. Columns from top to bottom represent different patient cases. Rows from left to right indicate FLAIR data, ground truth annotation, attentive map generated by CANet with different iteration numbers (from 1 to 10) in CG-ACRF respectively. Best viewed in colors.}
    \label{fig:ProbMap}
\end{figure*}

\subsection{Comparison with State-of-The-Art methods}
We choose several state-of-the-art deep learning model based brain tumor segmentation methods, including 3D UNet \cite{cciccek20163d}, Attention UNet \cite{oktay2018attention}, PRUNet \cite{brugger2019partially}, NoNewNet \cite{isensee2018no} and 3D-ESPNet \cite{mehta20193despnet}. We first consider the 5-fold cross-validation on the BraTS2017 training set. Each fold contains randomly chosen 228 cases for training and 57 cases for validation. In these cross-validation experiments on the training set, we consider CANet with complete HCA-FE and CG-ACRF fusion module with 5-iteration, which leads to the best performance in the ablation tests. As shown in Table \ref{table:SOTACompare}, our CANet outperforms the rest State-of-The-Art methods on several metrics while the results of the other metrics are competitive. The Dice score of CANet is 0.90338 and 0.87291 for the whole tumor and the tumor core respectively. The former is 8\% higher and the latter is 3\% higher than individual runner-up results. The Hausdorff95 values of CANet are 3.56898 and 4.03629 for the whole tumor and the tumor core, which are much lower than the runner up scores, i.e. 4.15649 and 5.77847, respectively.

To further evaluate the segmentation output, we compare the segmentation output of the proposed approach against the ground-truth. Fig. \ref{fig:SOTA} shows that the proposed CANet can effectively predict the correct regions including small tumor cores and complicated edges while the other state of the art methods fail to do so. Fig. \ref{fig:3D} presents the example segmentation result and the ground-truth annotation in 3D visualisation. From Fig. \ref{fig:3D}, we can observe that our proposed CANet effectively captures 3D forms and shape information in all different circumstances.

Fig. \ref{fig:training record} reports the training curve of CANet and the other state-of-the-art methods. Our proposed method converges to a lower training loss using fewer epochs. Taking the advantage of the powerful HCA-FE and the proposed fusion module CG-ACRF, CANet achieves satisfactory outlining for the brain tumors. With the training epoch increasing, CANet can fine-tune the segmentation map and successfully detect small tumor cores and boundaries.\\

\begin{table*}[!ht]
\centering
\caption{Quantitative results of the state-of-the-art models by cross-validation for the BraTS2017 training set with respect to dice, sensitivity, specificity and hausdorff. The best result is shown in bold and the runner-up result is underlined.}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
               & \multicolumn{3}{c|}{Dice}                              & \multicolumn{3}{c|}{Sensitivity}                       & \multicolumn{3}{c|}{Specificity}                       & \multicolumn{3}{c|}{Hausdorff95}                       \\ \hline
Model          & ET               & WT               & TC               & ET               & WT               & TC               & ET               & WT               & TC               & ET               & WT               & TC               \\ \hline
3D-UNet\cite{cciccek20163d}        & 0.70646          & 0.86492          & 0.81032          & 0.80275          & 0.9064           & 0.82906          & 0.99791          & 0.99005          & 0.99493          & 6.62407          & 8.19351          & 8.95848          \\ \hline
No-New Net\cite{isensee2018no}     & {\underline{0.74108}}    & 0.87083          & 0.8125           & 0.76688          & 0.89296          & 0.83115          & \textbf{0.99853} & {\underline{0.99243}}    & 0.99528          & \textbf{3.93033} & 7.05536          & 7.64098          \\ \hline
Attention UNet\cite{oktay2018attention} & 0.67174          & 0.8634           & 0.77837          & \textbf{0.84741} & 0.9001           & 0.86171          & 0.99591          & 0.98961          & 0.99186          & 9.34711          & 9.67562          & 10.66793         \\ \hline
PRUNet\cite{brugger2019partially}         & 0.71015          & 0.89072          & 0.81447          & 0.78826          & 0.90028          & 0.84056          & {\underline{0.99804}}    & 0.99002          & 0.99586          & 7.20534          & 7.41411          & 9.1874           \\ \hline
3D-ESPNet\cite{mehta20193despnet}      & 0.68949          & {\underline{0.89548}}    & {\underline{0.84397}}    & 0.80535          & \textbf{0.94666} & \textbf{0.88085} & 0.99671          & 0.99026          & {\underline{0.99677}}    & 6.89359          & {\underline{4.15649}}    & {\underline{5.77847}}    \\ \hline
CANet (Ours)   & 0.68489          & \textbf{0.90338} & \textbf{0.87291} & 0.80651          & {\underline{0.92363}}    & {\underline{0.86989}}    & 0.99746          & \textbf{0.99307} & 0.99592          & 7.80448          & \textbf{3.56898} & \textbf{4.03629} \\ \hline
\end{tabular}
\end{adjustbox}
\label{table:SOTACompare}
\end{table*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{SOTAComparision.pdf}
    \caption{Examples of segmentation results by cross validation for the BraTS2017 training set. Qualitative comparisons with other brain tumor segmentation methods are presented. The eight columns from left to right show the frames of the input FLAIR data, the ground truth annotation, the results generated from our CANet (UNet encoder backbone with HCA-FE and 5-iteration CG-ACRF), 3DUNet \cite{cciccek20163d}, NoNewNet \cite{isensee2018no}, Attention UNet \cite{oktay2018attention}, PRUNet \cite{mehta20193despnet}, respectively. Black arrows indicate the failure in these comparison methods. Best viewed in colors.}
    \label{fig:SOTA}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{3D.png}
    \caption{3D segmentation results of two volume cases by cross-validation on the BraTS2017 training set. The first and the third rows indicate the ground truth annotation. The second and the fourth rows indicate the segmentation result of our proposed CANet with HCA-FE and 5-iteration CG-ACRF. Rows from left to right indicate the qualitative comparison for the whole tumor, NCR/ECT, GD-enhancing tumor and Pertumoral Edema respectively. Best viewed in colors.}
    \label{fig:3D}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{NewTraingingLog.png}
    \caption{The learning curve of the state of the art methods and our proposed CANet with HCA-FE and 5-iteration CG-ACRF. Best viewed in color.}
    \label{fig:training record}
\end{figure*}

\begin{table*}[!ht]
\centering
\caption{Quantitative results comparison between CANet and other state of the art results on the BraTS2017 validation set for Dice and Hausdorff95. The best results of these methods are underlined. The bold shows the best score of each tumor region by single prediction approaches. '-' depicts that the result of the associated method has not been reported yet.}
% Please add the following required packages to your document preamble:
% \usepackage[normalem]{ulem}
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{|cc|ccc|ccc|}
\hline
                  &                  & \multicolumn{3}{c|}{\textbf{Dice}}               & \multicolumn{3}{c|}{\textbf{Hausdorff95}}        \\ \hline
\textbf{Approach} & \textbf{Method}  & \textbf{ET}    & \textbf{WT}    & \textbf{TC}    & \textbf{ET}    & \textbf{WT}    & \textbf{TC}    \\ \hline
                  & Kamnitsas et al. \cite{kamnitsas2017ensembles} & 0.738          & 0.901          & 0.797          & 4.500          & 4.230          & 6.560          \\
                  & Wang et al. \cite{wang2017automatic}      & {\underline {0.786}}    & {\underline {0.905}}    & {\underline {0.838}}    & {\underline {3.282}}    & {\underline {3.890}}    & {\underline {6.479}}    \\
Ensemble          & Zhao et al. \cite{zhao20173d}     & 0.754          & 0.887          & 0.794          & -              & -              & -              \\
                  & Isensee et al. \cite{isensee2017brain}  & 0.732          & 0.896          & 0.797          & 4.550          & 6.970          & 9.480          \\
                  & Jungo et al. \cite{jungo2017towards}    & 0.749          & 0.901          & 0.790          & 5.379          & 5.409          & 7.487          \\ \hline
                  & Islam et al. \cite{islam2017multi}     & 0.689          & 0.876          & 0.761          & 12.938         & 9.820          & 12.361         \\
                  & Jesson et al. \cite{jesson2017brain}    & 0.713          & \textbf{0.899} & 0.751          & 6.980          & \textbf{4.160} & \textbf{8.650} \\
Single Prediction & Roy et al. \cite{roy2018recalibrating}      & 0.716          & 0.892          & 0.793          & 6.612          & 6.735          & 9.806          \\
                  & Pereira er al. \cite{pereira2019adaptive}   & 0.719          & 0.889          & 0.758          & 5.738          & 6.581          & 11.100         \\
                  & CANet (Ours)    & \textbf{0.728} & 0.892          & \textbf{0.821} & \textbf{5.496} & 7.392          & 10.122         \\ \hline
\end{tabular}
\end{adjustbox}
\label{table:17val}
\end{table*}

\begin{table*}[!ht]
\centering
\caption{Quantitative results of the BraTS2018 validation set with respect to Dice and Hausdorff95. The best results of these methods are underlined. The bold results show the best score of each tumor region using single prediction approaches. '-' represents the result of the associated method has not been reported yet.}
\begin{adjustbox}{width=1\textwidth}
\begin{tabular}{|cc|ccc|ccc|}
\hline
                                  &                                  & \multicolumn{3}{c|}{\textbf{Dice}}                                                                           & \multicolumn{3}{c|}{\textbf{Hausdorff95}}                                                                    \\ \hline
\textbf{Approach} & \textbf{Method} & \textbf{ET}    & \textbf{WT}    & \textbf{TC}    & \textbf{ET}    & \textbf{WT}    & \textbf{TC}    \\ \hline
                                  & Isensee et al. \cite{isensee2018no}                   & \uline{0.796}  & 0.908                           & 0.843                           & 3.120                           & 4.790                           & 8.020                           \\
                                  & McKinley et al. \cite{mckinley2018ensembles}                  & 0.793                           & 0.901                           & \uline{0.847}  & 3.603                           & 4.062                           & \uline{4.988}  \\
Ensemble                           & Zhou et al. \cite{zhou2018learning}                      & 0.792                           & \uline{0.907}  & 0.836                           & \uline{2.800}  & 4.480                           & 7.070                           \\
                                  & Cabezas et al. \cite{cabezas2018survival}                   & 0.740                           & 0.889                           & 0.726                           & 5.304                           & 6.956                           & 11.924                          \\
                                  & Feng et al. \cite{feng2018brain}                      & 0.787                           & 0.906                           & 0.834                           & 3.964                           & \uline{4.018}  & 5.340                           \\ \hline
                                  & Sun et al. \cite{sun2018tumor}                       & 0.751                           & 0.865                           & 0.720                           & -                               & -                               & -                               \\
                                  & Myronenko \cite{myronenko20183d}                        & \textbf{0.816} & \textbf{0.904} & \textbf{0.860} & \textbf{3.805} & \textbf{4.483} & 8.278                           \\
Single Prediction                  & Weninger et al. \cite{weninger2018segmentation}                  & 0.712                           & 0.889                           & 0.758                           & 8.628                           & 6.970                           & 10.910                          \\
                                  & Gates et al. \cite{gates2018glioma}                    & 0.678                           & 0.806                           & 0.685                           & 14.523                          & 14.415                          & 20.017                          \\
                                  & CANet(Ours)                     & 0.767                           & 0.898                           & 0.834                           & 3.859                           & 6.685                           & \textbf{7.674} \\ \hline
\end{tabular}
\end{adjustbox}
\label{table:18val}
\end{table*}
We further investigate the segmentation results on the BraTS2017 and BraTS2018 validation sets, where the quantitative result of each patient case is generated from the online evaluation server. The mean result is reported in Table \ref{table:17val} and Table \ref{table:18val}. Box plot in Fig. \ref{fig:boxplot} shows the distribution of the segmentation result among all the patient cases in the validation set. For the BraTS2017 validation set, our proposed CANet with complete HCA-FE and 5-iteration CG-ACRF achieves the state-of-the-art results of Dice on ET, Dice on TC and Hasdorff95 on ET among the single model segmentation benchmarks. Our CANet has Dice on ET of 0.728, which is higher than the approach reported in \cite{pereira2019adaptive}. The Dice on TC by CANet is 0.821, which is higher than the runner-up result reported in \cite{roy2018recalibrating}. The Hausdorff95 on ET of CANet is 5.496, which is much lower than the runner-up generated in \cite{pereira2019adaptive}. For the BraTS2018 validation set, our proposed CANet achieves the state-of-the-art result for Hausdorff95, i.e. 7.674, on the tumor core, while the other results are all runner-ups. Note that the method proposed by Myronenko \cite{myronenko20183d} has the best performance using most of the evaluation metrics. In Myronenko's method, they set up an additional branch using autoencoder to regularise the encoder backbone by reconstructing the input 3D MRI image. This autoencoder branch greatly enhances the feature extraction capability of the backbone encoder. In our framework, we regularise the network weights using a L2-regularisation without any additional branch, and the result of our proposed CANet is better than the other single prediction methods. Be reminded that the standard single prediction models generate the segmentation only using one network, and do not need much computational resources and a complicated voting scheme. Compared with the ensemble methods, the result of our proposed CANet is still very competitive.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.6\textwidth]{MetricScore.png}
    \caption{Boxplot of the segmentation results by CANet with HCA-FE and 5-iteration CG-ACRF. Dots within yellow boxes are individual segmentation results generated for the BraTS2017 validation set. Dots within blue boxes are individual segmentation results generated for the BraTS2018 validation set. Best viewed in color.}
    \label{fig:boxplot}
\end{figure*}

Note that even we incorporate our designed CG-ACRF as iterative convolutional blocks within the system, our model still maintains a relatively small space. Our final model, i.e. HCA-FE with five times convolution approximation for CG-ACRF maintains a parameter size at 1.84E7. Compared with baseline methods such as Isensee et al. \cite{isensee2018no} with a parameter size at 1.45E7 and Myronenko et al. \cite{myronenko20183d} with a parameter size at 2.01E7 \cite{zhang2020exploring}, our system maintains the parameter size at an intermediate level, which prevents the occurrence of over-fitting.


\subsection{Failure Case Studies}
Fig. \ref{fig:imbalance} shows the statistical information of the BraTS2017 training set. As an example, we here report two failure segmentation cases by our proposed approach, which are shown in Fig. \ref{fig:FailureCase}. During the whole training process, CANet focuses on extracting feature maps with different contextual information, e.g. convolutional and graph contexts. However, we have not designed specific strategies for handling the imbalanced issue of the training set. The imbalanced issue is presented in two aspects. Firstly, there exists an unbalanced number of voxels in different tumor regions. As the exemplar case named "Brats17\_TCIA\_605\_1" is shown in Fig. \ref{fig:FailureCase}, the NCR/ECT region is much smaller than the other two regions, suggesting the poor performance of segmenting NCR/ECT. Secondly, there exists an unbalanced number of patient cases from different institutions. This imbalance introduces an annotation bias where some annotations tend to connect all the small regions into a large region while the other annotation tends to label the voxels individually. As the exemplar case named "Brats17\_2013\_23\_1" is shown in Fig. \ref{fig:FailureCase}, the ground truth annotation tends to be sparse while the segmentation output tends to be connected. In future work, we will consider an effective training scheme based on active/transfer learning which can effectively handle the imbalance issue in the dataset. Despite the imbalance issue, our segmentation method on the overall cases qualitatively outperforms the other state-of-the-art methods.
% Fig. \ref{fig:imbalance} shows the statistical information of the BraTS2017 training set.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{imbalance.png}
    \caption{Statistics of the BraTS2017 training set. The left-hand side figure of (a) shows the FLAIR and T2 intensity projection, and the right-hand side figure shows the T1ce and T1 intensity projection. (b) is the pie chart of the training data with labels, where the top figure shows the HGG data labels while the bottom figure shows the LGG labels. There are a large region and label imbalance cases here. Best viewed in colors.}
    \label{fig:imbalance}
\end{figure*}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{FailureCase.png}
    \caption{Qualitative comparisons in the failure cases. Rows from left to right indicate the input data of the FLAIR modality, ground truth annotation, segmentation result from our CANet, segmentation result from the other SOTA methods respectively. Our results look better than the SOTA methods results. Best viewed in colors.}
    \label{fig:FailureCase}
\end{figure*}

\section{Discussion}
In summary, we have proposed a novel 3D MRI brain tumor segmentation approach called CANet. Considering different contextual information in standard and graph convolutions, we proposed a novel hybrid context-aware feature extractor combined with a deep supervised convolution and a graph convolution stream. Different from previous works that used naive feature fusion schemes such as element-wise summation or channel-wise concatenation, we here designed a novel feature fusion model based on the conditional random field called context guided attentive conditional random field (CG-ACRF), which effectively learns the optimal latent features for downstream segmentations. Furthermore, we formulated the mean-field approximation within CG-ACRF as convolutional operations, which incorporate the CG-ACRF in a segmentation network to perform end-to-end training. We conducted extensive experiments to evaluate the effectiveness of the proposed HCA-FE, CG-ACRF and the complete CANet frameworks. The results have shown that our proposed CANet achieved state-of-the-art results in several evaluation metrics. In the future, we consider combining the proposed network with novel training methods that can better handle the imbalance issue in the datasets.
%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------


