% Chapter Template

\chapter{Hierarchical Homography Estimation Network for Medical Image Mosaicing} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Introduction}
\label{C4:S1}

In Chapter \ref{Chapter3}, we introduced an example of implicit deep relational learning paradigm on medical image analysis, i.e. use graph convolution kernel to learn the correlation information between different features on the built feature graph. One of the shortcomings of this implicit relational learning is that it only focuses on the relationship of features on one image while ignoring the relation relevance of multiple images along the time dimension. To this end, we propose a novel Hierarchical Homography Estimation Network (HHEN) for medical image mosaicing, which can explicitly model the spatial relationship between frames in both short and long ranges.

Image mosaicing refers to sequentially transferring and combining a set of narrow field-of-view images to form a new image or video with a wider field-of-view. Traditional image mosaicing has been widely used in various areas such as satellite photographing \cite{bu2016map2dfusion}, augmented reality \cite{azzari2008markerless} and panoramic photo edition \cite{li2018deep}. In the last few years, the interest in medical image mosaicing has grown in the medical image analysis community, especially in fetoscopic laser photography mosaicing \cite{bano2020deep, bano2020deep2}. Clinical physicians use fetoscopic video to detect vascular anastomoses, make treatment plans and control surgical robots. Thus clinicians can be able to treat various diseases such as abnormal vascular anastomoses in twin-to-twin transfusion syndrome (TTTS). However, it is a challenging problem for clinicians to fully use the fetoscope due to its narrow FoV and low visibility. Image mosaicing can compose a set of fetoscopic images with correlation transformation to generate a new image with wider FoV, thus it can offer computer-assisted diagnosis evidence to effectively detect the vascular anastomoses location.

Traditional mosaicing methods commonly use low-level features, i.e. point-based mosaicing and HoG based mosaicing \cite{kanazawa2004image, zagrouba2009efficient, kourogi1999real}. However, traditional mosaicing methods do not perform well due to poor medical image quality, e.g. turbid amniotic fluid in TTTS treatment. Recently, several deep-learning-based mosaicing methods have been proposed \cite{lv2019improved, nguyen2018unsupervised}. These methods learned deep features or homography estimation automatically and achieved superior performance on various datasets. Despite the remarkable progress that has been made, one open challenge is that these methods only focus on learning local information between adjacent frames while ignoring the non-local information in long series, which is important for time serious tasks. To overcome this, we propose a novel hierarchical medical image mosaicing network using neural homography estimation. Our contributions are summarized as follows:

\begin{enumerate}
\item We propose a new deep hierarchical homography estimation network to automatically and hierarchically estimate the 8 degree-of-freedom homography, upon which multi-scale (local level between adjacent frames and non-local level between long-range frames) homography are jointly learned and optimized in a data-driven manner.

\item Inspired by recent works on color homography \cite{finlayson2017color}, we propose a new data generation method called Partially Image Generation (PIG). PIG only perturb the color, rotation, and translation movement between adjacent frames. The generated frames by PIG can be used for evaluating model performance during network training.

\item We conduct extensive experiments on five different video clips from UCL Fetoscopy Placenta dataset. The results show that our method achieves state-of-the-art performance and generalizes well on different clips under different numbers of frames and different acquisition methods.
\end{enumerate}

The rest of this Chapter is organized as follows. In Section \ref{C4:S2}, we briefly review related works on image mosaicing. In Section \ref{C4:S3}, we introduced our proposed method, including HHEN in Section \ref{C4:S3:SS1} and PIG in Section \ref{C4:S3:SS2} respectively. In Section \ref{C4:S4}, we demonstrate the dataset and implementation details. In Section \ref{C4:S5}, our proposed HHEN is evaluated and compared with the state-of-the-art methods. In addition, the results are analysed in detail. This Chapter is finally concluded in Section \ref{C4:S6}.


\section{Review of Recent Image Mosaicing Methods}
\label{C4:S2}
In this chapter, we briefly review related works on image mosaicking, which refers to the alignment of multiple overlapping images into a large combination with larger FoV.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{MosaicingFlow.png}
    \caption{A schematic pipeline for image mosaicing.}
\label{fig:mosaicingflow}
\end{figure}

Finding the point-to-point or other feature correlation between two images is the fundamental part of conventional image mosaicking, which can be found in the mosaicing schematic pipeline shown in Fig. \ref{fig:mosaicingflow}. Traditional image mosaicing methods commonly use low-level features such as edge and point to find the correlation between two images. Kanazawa et al. \cite{kanazawa2004image} extract 100 feature points generated from the Harris operator to find the correlation between two images. Zagrouba et al. \cite{zagrouba2009efficient} extract both Harris points and the semantic segmentation as multi-level features for correlation learning. Kourogi et al. \cite{kourogi1999real} first generated the pseudo-motion vector on each pixel, and then estimate the affine motion parameters by using pixel-wise motion vector matching. Heikkila et al. \cite{heikkila2005image} extract the correlation feature using SIFT first and then reject the outlier points by applying RANSAC algorithms. Botterill et al. \cite{botterill2010real} first represent the image with the Bag-of-Words (BoW) method to find the adjoining frames and then use a perspective transformation to register adjoining frames to achieve video mosaicing.

Different from natural images, the quality of medical images is interfered with by many factors, such as fetoscopic photography captured within a turbid amniotic fluid environment. And traditional feature extraction algorithms for points or edges cannot extract effective and meaningful features. Although the aforementioned algorithms are simple and fast with low demands on computing resources. However, the aforementioned algorithms are difficult to be applied to tasks that require higher precision, such as medical image mosaicing.

Recently, various deep-learning-based image mosaicing methods have been presented. Lv et al. \cite{lv2019improved} proposed a CNN based Speed-up Robust Feature (SURF) extraction method for image mosaicing. Nguyen et al. \cite{nguyen2018unsupervised} proposed an unsupervised learning network for homography estimation in image mosaicing. However, these two methodsâ€™ input is still hand-crafted features, which cannot be jointly optimized with the mosaicing network. Zhang et al. \cite{zhang2019convolutional} use a CNN for registration based microscopic image mosaicing. Bano et al. \cite{bano2020deep} use a VGG-stylized CNN for homography estimation between two adjacent frames. However, these methods only focus on adjacent frame homography estimation (local information), while ignoring the correlation between long-range frames, which has been proven as an important factor for sequential frame-based tasks \cite{wang2018non, jiang2018modeling, anderson2018bottom}. To address this open challenge, we propose our novel hierarchical neural homography estimation network (HHEN) for fetoscopic photography mosaicing, which can automatically and hierarchically estimate the 6 degree-of-freedom homography. Besides, we propose a new generation method called Partially Image Generation (PIG). PIG only perturbs the color, rotation, and translation movement between adjacent frames. The generated frames by PIG can be used for evaluating model performance during training.

\section{Proposed Method}
\label{C4:S3}
Recently, deep image homography (DIH) estimation \cite{detone2016deep} have been proposed that use a regression neural network to estimate the homography between two local patches in one image. Based on DIH, several image mosaicing methods \cite{bano2020deep, bano2020deep2} have been proposed to stitching a set of images by estimating homography between every adjacent frame. However, DIH and proposed mosaicing work only focused on adjacent frame feature learning (local information) while ignored the correlation between long-range frames (non-local information), which has been proved as an important factor for achieving video and serious tasks. In this chapter, we first propose our novel image homography estimation framework called Hierarchical Homography Estimation Network (HHEN) in Chapter \ref{C4:S3:SS1}. HHEN treat the target frame as a query frame while transformed both local and non-local template frame as key and value frame. Thus the homography can be learned and optimize hierarchically with local and non-local information in a data-driven manner. We further propose a novel generation method called Partially Image Generation (PIG). PIG only controls the rotation, translation movement and color restoration between adjacent frames as the incision point of fetoscopic photography are fixed. Without any external sensors like electromagnetic tracker (EMT) used in \cite{tella2018probabilistic}, our proposed HHEN with PIG minimizes the drift error effectively and achieve the state-of-the-art result on fetoscopic photography mosaicing.

\subsection{Hierarchical Homography Estimation Network}
\label{C4:S3:SS1}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{DIH.png}
    \caption{A schematic pipeline for Deep Image Homography (DIH) estimation.}
\label{fig:DIH}
\end{figure}

A prilimary of our proposed HHEN is deep image homography (DIH) shown in Fig. \ref{fig:DIH}, where DIH estimates the homography between two relative image local patches $\left( P_{A} and P_{B} \right)$ from a single image. Similar to DIH, we also use a VGG style backbone network to estimate homography $\textbf{H}$. Instead of using $\textbf{H}$ with 9 degree-of-freedom, we here define the target homography is 3 point related due to the rotation and shear operation in \cite{detone2016deep} has little effect in our scenario (Due to the fetoscopic photography data is obtained through a fixed incision point). By defining arbitrary three corner points coordinate $\left( x_i, y_i \right)$ of $P_{A}$ and $\left( x_{i}^{\prime}, y_{i}^{\prime} \right)$ of $P_{B}$ with $i=1,2,3$, the 3 point related homography $\textbf{H}$ is defined as:

\[
\textbf{H} =
\begin{bmatrix}
\Delta x_1 & \Delta x_2 & \Delta x_3\\
\Delta y_1 & \Delta y_2 & \Delta y_3
\end{bmatrix}^{\top}
\]

where $\Delta x_i = x_{i}^{\prime} - x_{i}$, $\Delta y_i = y_{i}^{\prime} - y_{i}$. The fourth corner can be calculated due to the patch is extracted as a rectangle. Note that DIH generates $P_B$ by randomly displacing $P_A$ as illustrated in Fig. \ref{fig:DIH}, thus the drift error is not acceptable for image-based mosaicing tasks as the drift error will be accumulated. Mosaicing requires relative homography between adjacent frames with respect to the template frame. For minimizing the drift error in relative homography, several related works \cite{bano2020deep2, bano2020deep} have been proposed to let the backbone network learn the homography between patches extracted from adjacent frames. However, the homography estimation networks in \cite{bano2020deep2, bano2020deep} only considered the local information between adjacent frames while ignoring the non-local information between long-range consecutive frames, which has been proven as an important factor for video applications \cite{wang2018non, jiang2018modeling, anderson2018bottom}. Therefore, we propose a hierarchical homography estimation network (HHEN) that learns long-range information between a set of consecutive frames. As shown in Fig. \ref{fig:HHEN}, we extend the definition of a pair of adjacent frames to a hierarchical composition. We set the frame that needs to be moved (target frame) as \textit{Query Frame} while the template frame is assembled with long-range \textit{Key Frame} and short-range \textit{Value Frame}. Intuitively, for the target \textit{Query Frame}, the network estimates the homography with respect how \textit{Value Frame} moves based on previous non-local \textit{Key Frame}.

Inspired by non-local image de-noising operation \cite{buades2005non} and non-local neural network for video classification \cite{wang2018non}, we define the generic non-local frame fusion head in HHEN as:

\begin{equation}
y_{i} = G(x_{i}^{Q})\frac{1}{N(x)}\sum_{\forall j < i} F(x_{i}^{V}, x_{j}^{K})G(x_{j}^{Q})
\label{eqn:nonlocal}
\end{equation}

wherein Eqn. \ref{eqn:nonlocal}, $i$ is the position of the output in the spatial domain and $j$ is the neighborhood position of $i$ (we use the 8-neighbor pixels in this paper). $x^{Q}, x^{K}, x^{V}$ denotes the input image patch from the query frame, key frame and value frame respectively. $y$ is the output fusion feature map with the same size as $x$. $F(\cdot)$ is the pairwise function that takes the pairwise input $(x_{i}^{V} , x_{j}^{K})$. The output of $F(\cdot)$ is a scalar weight between two inputs, which represents the affinity transformation between $x_{i}^{V}$ and $x_{j}^{K}$. $G(\cdot)$ is the unary function that outputs the representation of the input signal. $N(\cdot)$ is the normalization factor. Overall, Eqn. \ref{eqn:nonlocal} denotes that, for a given input $x_{i}^{Q}$ at position $j$ from query frame $x^{Q}$, $F(\cdot)$ learns how the related input from value frame is updated based on a key frame, then the non-local operator uses the learned affinity transformation between value frame and key frame to update the feature computed by $G(\cdot)$ in query frame.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{HHEN.png}
    \caption{A schematic pipeline for Hierarchical Homography Estimation Netowork (HHEN) estimation.}
\label{fig:HHEN}
\end{figure}


The natural instantiation choice  of $F(\cdot)$ is the Gaussian function as implemented in the non-local mean operator \cite{buades2005non}, which is formalized as:

\begin{equation}
F(x_{i}^{V}, x_{j}^{K}) = e^{{x_{i}^{V}}^\top x_{j}^{K}}
\label{eqn:gaussian}
\end{equation}

In Eqn. \ref{eqn:gaussian}, the ${x_{i}^{V}}^\top x_{j}^{K}$ is the dot product to measure the similarity between $x_{i}^{V}$ and $x_{j}^{K}$. Correspondingly, the normalization factor is $N(x) = \sum_{\forall j < i} F(x_{i}^{V}, x_{j}^{K})$. Since the entire system is trained in an end-to-end fashion, in order to enable the final regression loss to directly adjust the parameter update of the non-local operator, we expand the Gaussian function (Eqn. \ref{eqn:gaussian}) to an embedding manner:

\begin{equation}
F(F(x_{i}^{V}, x_{j}^{K})) = e^{\theta(x_{i}^{V}) \top \phi(x_{j}^{K})}
\label{eqn:gaussianembedding}
\end{equation}

In Eqn. \ref{eqn:gaussianembedding}, $\theta(\cdot)$ and $\phi(\cdot)$ are two linear embedding functions, i.e. $\theta(x_{i}^{V}) = W_{\theta}x_{i}^{V}$ and $\phi(x_{j}^{K}) = W_{\phi}x_{j}^{K}$. By using the embedding Gaussian function, given the normalization factor is $N(x) = \sum_{\forall j < i} F(x_{i}^{V}, x_{j}^{K})$, the implementation of the non-local fusion head can be regarded as a softmax activation with inputs from two convolution features generated from $1 \times 1$ convolution kernel:

\begin{equation}
\textbf{y} = G(\textbf{x}) \textit{softmax}(\textbf{x}^{\top} W_{\theta}^{\top} W_{\phi} \textbf{x})G(\textbf{x})
\label{eqn:nonlocalimpl}
\end{equation}

By choosing the same implementation using $1 \times 1$ convolution for $G(\cdot)$, the non-local fusion head can be jointly trained with the downstream network. The output of the HHEN is the homography $H^{Q}$ with respect to the target frame, i.e. \textit{Query Frame}. By iterate every frame in an image set as \textit{Query Frame} and compute every frame's $H^{Q}$, we can complete mosaicing all the image frames in the given image set. 

\subsection{Partially Image Generation}
\label{C4:S3:SS2}
Note that in \cite{detone2016deep}, the author used the MS-COCO dataset \cite{lin2014microsoft} for training and testing. Compared with the MS-COCO dataset which contains natural real images, fetoscopic photography contains particular artifacts such as color distortion caused by amniotic fluid particles. Also, the shear and scale contribute little effect when HHEN is trained with fetoscopic images as the fetoscopic image is obtained through a fixed incision point with constrained distance from the placenta. Thus to minimize the drift error during mosaicing, we propose our Partially Image Generation (PIG) generate the training dataset. PIG only assumes that only rotation, translation and color homography are related between the pair of key-value and value-query frames. For a given image $I_{A}$ and its extracts patch $P_{A}$ with three corners coordinates ($x_{i}, y_{i}$), ($i=1,2,3$). We applied:

\begin{itemize}
\item Rotation with angle $\alpha$ and translation with movement distance $\delta x$ on x coordinate and $\delta y$ on y coordinate.\\
\[
\begin{bmatrix}
x_{i}^{\prime} \\
y_{i}^{\prime}
\end{bmatrix} = \begin{bmatrix}
cos(\alpha) & sin(\alpha) \\
-sin(\alpha) & cos(\alpha)
\end{bmatrix} \begin{bmatrix}
x_{i} \\
y_{i}
\end{bmatrix} + \begin{bmatrix}
\Delta x \\
\Delta y
\end{bmatrix}
\]

\item Color homography transformation: we transfer the original color image with every pixel $\textit{R, G, B}$ value to the greyscale image with every pixel's intensity $\textit{Y}$:\\
\[ 
\textit{Y} = \frac{max(\textit{R, G, B}) + min(\textit{R, G, B})}{2}
\label{Eqn:PIAGrey}
\]
\end{itemize}

to obtain the corresponding pair image $I_{B}$ with perturbed patch $P_{B}$. We empirically set the rotation angle $\alpha$ and translation movement distance $\Delta x$, $\Delta y$ (Sec. \ref{C4:S4:SS2}). By implementing the PIG, the HHEN can learn the homography ($\alpha$, $\Delta x$ and $\Delta y$) without interference from color distortion (the color with artifacts are transformed into greyscale images).

\section{Experiment Setup}
\label{C4:S4}
We conduct extensive experiments to evaluate our proposed fetoscopic photography mosaicing method using HHEN with PIG. We first introduce the datasets and evaluation metrics first (Sec. \ref{C4:S4:SS1}). Then we express the details of method implementations (Sec. \ref{C4:S4:SS2}).

\subsection{Datasets and Evaluation Metrics}
\label{C4:S4:SS1}
We use the 5 fetoscopic video clips from UCL Fetoscopy Placenta Data \cite{bano2020vessel} \footnote{https://www.ucl.ac.uk/interventional-surgical-sciences/fetoscopy-placenta-data}, which includes two synthetic video clips (SYN1 and SYN2), one TTTS Phantom in the water video clip (TTTS1) and two in-vivo TTTS procedure video clips (INVT1 and INVT2). We show the details of all 5 datasets in Table \ref{table:mosaicingdata}. We can observe from Table \ref{table:mosaicingdata} that the datasets are varied among different factors such as object texture, ambient reflected light and capture motion, which post the challenges for mosaicing methods.

\begin{sidewaystable}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Frame Example      &   \raisebox{-\totalheight}{\includegraphics[width=0.15\textwidth, height=30mm]{Figures/SYN1.jpg}}         &\raisebox{-\totalheight}{\includegraphics[width=0.15\textwidth, height=30mm]{Figures/SYN2.jpg}}           &\raisebox{-\totalheight}{\includegraphics[width=0.15\textwidth, height=30mm]{Figures/TTTS1.jpg}}                       & \raisebox{-\totalheight}{\includegraphics[width=0.15\textwidth, height=30mm]{Figures/INVT1.png}}                       & \raisebox{-\totalheight}{\includegraphics[width=0.15\textwidth, height=30mm]{Figures/INVT2.png}}                       \\ \hline
Data Type          & \begin{tabular}[c]{@{}c@{}}Synthetic\\ (SYN1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Synthetic\\ (SYN2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}TTTS Phantom \\ in water\\ (TTTS1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}In-vivo \\ TTTS Procedure\\ (INVT1)\end{tabular} & \begin{tabular}[c]{@{}c@{}}In-vivo \\ TTTS Procedure\\ (INVT2)\end{tabular} \\ \hline
Total Frame Num & 500       & 200       & 200                   & 400                    & 100                    \\ \hline
Image Resolution   & 500 x 500 & 500 x 500 & 600 x 600             & 448 x 448              & 448 x 448              \\ \hline
Camera View        & Planar    & Planar    & Planar                & Non Planar             & Planar                 \\ \hline
Motion Type        &Spiral & Circular & Circular                                                           & \begin{tabular}[c]{@{}c@{}}Exploratory \\ Freehand\end{tabular}   & \begin{tabular}[c]{@{}c@{}}Exploratory \\ Freehand\end{tabular}   \\ \hline
\end{tabular}
\caption{Details of the selected datasets for experimental analysis}
\label{table:mosaicingdata}
\end{sidewaystable}

We compare our method with state-of-the-art deep-learning-based methods: deep image homography (DIH) \cite{detone2016deep} and deep sequential mosaicking (DSM) \cite{bano2020deep} and a hand-crafted feature-based method (FEAT with SURF feature matching for homography estimation) \cite{brown2007automatic}. For the synthetic datasets SYN1 and SYN2, we evaluate our HHEN with PIG against all baseline methods and report the mean residual error (MSE) against ground truth data. For datasets obtained from TTTS clinical surgery (TTTS1, INVT1, INVT2), we evaluate the performance of our HHEN with PIG against all baseline methods and report the average root mean square error (RMSE) between image and generated image. Following \cite{brasch2018semantic, godard2019digging}, we also evaluate the average photometric error (APE) by using the $L1$ distance between two images generated by the estimated homography.

\subsection{Implementation Details}
\label{C4:S4:SS2}
We first randomly select 435 images from all datasets except the INVT1 to compose a training set. The image frames in the training set are not considered in the testing set as they are not a full video clip. INVT1 thus is unseen for the model during the training. By using PIG, all images are transferred into greyscale images where the intensity in Eqn. \ref{Eqn:PIAGrey} represent the brightness converted from RGB color to avoid color distortion. Our proposed network and other baseline methods are complied in Keras \footnote{https://keras.io/} platform with a Tensorflow \footnote{https://www.tensorflow.org/} backend. We train our proposed network on a single Nvidia Tesla T4 GPU (16GB) for 2000 epochs. We use the Stochastic Gradient Descent (SGD) as the optimizer with a learning rate $lr = 0.0001$ and momentum with $0.9$. Due to the limitation of GPU memory, we set the batch size as 16. We use the Euclidean loss function as the main goal of the homography is to estimate the distance of rotation and transition between adjacent frames. During training with PIG, we manually set the rotation angle $\alpha$ between ($-8^{\circ}, +8^{\circ}$) and the translation distance $\Delta x$, $\Delta y$ between ($-15, +15$) pixels. The manually perturbed rotation angle and transition distance are stored as ground truth. The network takes the original image patch and the perturbed image patch as inputs and outputs the numerical results. The loss can be estimated between the numerical outputs and the manually settled angel and transition distance.



\section{Experiment Results}
\label{C4:S5}

We conduct sufficient experiments on 5 video clips from UCL Fetoscopy Placenta Data against several baseline methods including FEAT, DIH and DSM. We discuss the experimental results from various perspectives in the following sections.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/mrecurve.png}
    \caption{Quantitative result comparison on mean residual error between HHEN and baseline methods.}
\label{fig:MRE}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/RMSECompare.png}
    \caption{Quantitative result comparison on average RMSE between HHEN and baseline methods.}
\label{fig:RMSE}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/APECompare.png}
    \caption{Quantitative result comparison on average photometric error between HHEN and baseline methods.}
\label{fig:APE}
\end{figure}


\subsection{Comparison with Hand-Crafted Methods}
\label{C4:S5:SS1}
We first compare the result between our proposed HHEN and FEAT. Note that FEAT  achieves the mosaicing based on the Speed Up Robust Features (SURF). SURF constructs a Hessian matrix to generate the point-of-interests on the edges. However, such kind of traditional-feature-based mosaicing methods fail on fetoscopic photography mosaicing due to the vessels are blurred and surrounded with turbid amniotic fluid. Compared with synthetic data (SYN1 and SYN2) which ignored these kinds of environmental interference (Table \ref{table:mosaicingdata}), the FEAT failed to achieve robust mosaicing on real data (TTTS1, INVT1, INVT2). As shown in Fig. \ref{fig:RMSE}, FEAT got relatively low RMSE on TTTS1, INVT1 and INVT2 with scores 8.13, 7.86, 7.11 respectively. However, the HHEN benefits from deep feature learning while PIG avoids the color restoration during training, which help HHEN outperforms the 
FEAT with a large margin (2.72 on TTTS1, 2.45 on INVT1, 1.61 on INVT2). The same result is also shown on the evaluation of APE (Fig. \ref{fig:APE}), while HHEN outperforms the FEAT on all clips. Besides, the MRE of FEAT generated mosaicing rise up after 150 frames while the HHEN continuously maintains the MRE at a relatively low level due to the non-local information learned from long-range frames. We can also observe the same phenomenon from the qualitative results shown in Fig. \ref{fig:SYNGT} as the FEAT mosaicing starts drifting away at 150 frames. 


\subsection{Comparison with State-of-The-Art Deep Learning Based Methods}
\label{C4:S5:SS2}

We also compared our proposed HHEN with two state-of-the-art image mosaicing methods, i.e. DIH and DSM. For MSE, DIH explodes very quickly as there are random rotation and translation during training data generation. DSM performs well on short frames due to it learns adjacent homography between adjacent frames. However, the error of DSM starts to accumulate after 300 frames and the mosaicing starts drifting (Fig. \ref{fig:SYNGT}) due to the local information is insufficient to achieve a stable mosaicing during long-range videos. In contrast, our proposed HHEN observed a continuous low MRE even for long-range frames that benefited from non-local information. HHEN also outperforms these two methods on both RMSE and APE. On RMSE, HHEN achieves 0.92, 1.33, 2.72, 2.45 and 1.61 on SYN1, SYN2, TTTS1, INVT1 and INVT2 respectively, while the other two methods obtained higher error (DIH: 3.08, 3.27, 3.86, 4.29, 4.41, DSN: 2.53, 2.17, 2.95, 2.08 3.53). On APE, HHEN also outperforms these two baseline methods on SYN1, SYN2, TTTS1, INVT1 and INVT2 (HHEN: 1.64, 1.76, 2.03, 2.28, 1.04, DIH: 3.27, 3.88, 2.91, 4.65, 3.74, DSM: 2.13, 1.86, 1.90, 2.85, 1.53). We also visualize the mosaicing result of testing sequence TTTS1, INVT1 and INVT2 in Fig. \ref{fig:testresult}. We can observe from Fig. \ref{fig:testresult} that our proposed HHEN with PIG generates the most meaningful mosaicing result even for unseen clip INVT1, which shows the high robustness of our method dealing with different fetoscopic photography scenarios.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/mosaicresult.png}
    \caption{Qualitative result visualization comparison on SYN1 and SYN2 dataset between HHEN and baseline methods. We highlight the mosaicing drift error with red ellipse. Best viewed in colors.}
\label{fig:SYNGT}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/nogtresult.png}
    \caption{Qualitative result visualization on TTTS1, INVT1 and ONVT2 dataset using generated homography from HHEN. We can observe that HHEN generates meaningful and stable mosaicing even for unseen data INVT1. Best viewed in colors.}
\label{fig:testresult}
\end{figure}


\section{Discussion}
\label{C4:S6}
In this project, we utilize explicit deep relation learning on medical image mosaicing. Specifically, we propose a novel hierarchical homography estimation network to effectively learn the homography between adjacent frames that benefited from both local and non-local information. To further generate sufficient training data, we propose a novel image generation method called partially image generation which only perturbs the image with rotation, translation and color transformation. PIG can accelerate HHEN to learn relative homography while ignoring external interference such as color restoration. We conduct extensive experiments on 5 video data clips and the result shows our proposed method performs superior compared with state-of-the-art mosaicing methods. However, there are some blank spaces left for us to improve the mosaicing method. First, the whole network parameter is shared and updated across the data frames, which can be replaced by a recurrent neural network or a long-short term memory network to model the relationship between frames more effectively. Another future direction is to utilize the vessel segmentation mask as auxiliary information, which represents the saliency information, to achieve more accurate image mosaicing.


