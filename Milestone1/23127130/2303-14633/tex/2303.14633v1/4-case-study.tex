\section{Evaluating MOMs}
\label{sec:case-study}


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/case_study/method.pdf}
\vspace{-2em}
\caption{MOMs evaluated in this paper}
% \zhuohan{TODO: Modify this table to a latex table.}}
\label{fig:case_study_method}
\end{figure}

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
  
% \begin{table*}[t]
%     \centering
%     \caption{TODO}
%     \label{tab:notation}
%     \resizebox{\linewidth}{!}{
%         \begin{tabular}{ p{18em} | p{8em} | p{18em}| p{15em} | p{15em} }
%         \toprule
%         Model & Quantization & Gradient\newline Checkpointing & Swapping & Combination \\
%         \midrule
%         ResNet-50~\cite{he2016deep} & \multirow{4}{15em}{ActNN~\cite{chen2021actnn}}  & \multirow{2}{13em}{DTR~\cite{kirisame2020dynamic}} 
%                                     & \multirow{4}{15em}{Synchronous Swapping}        & \multirow{2}{15em}{Quantization + Swapping} \\
%         Wide-ResNet-50~\cite{zagoruyko2016wide} & & & & \\
%         Bert-Large~\cite{devlin2018bert} &  & Checkpoint First~\cite{wolf2019huggingface} & &   \specialcell{Gradient Checkpointing + Swapping \\
%                                                                                                 Quantization + Swapping \\
%                                                                                                 Gradient Checkpointing + Quantization} \\
%         Swin-Large~\cite{liu2021swin}   &  & Checkpoint First~\cite{liu2021swin} &  & Quantization + Swapping \\
%         GPT-Small~\cite{radford2021learning} & & Checkpoint First~\cite{radford2021learning}  & & \specialcell{Quantization + Swapping \\
%                                                                                                   Gradient Checkpointing + Swapping~\cite{}} \\
%         \bottomrule
%     \end{tabular}}
% \end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/case_study/case_study.pdf}
\vspace{-0.3in}
\caption{Training with different MOMs. The cross means a bigger batch size will cause the out of memory error. }
\label{fig:case_study_all}
\end{figure*}

\begin{figure}[t]
\centering 
\includegraphics[width=0.9\linewidth]{figures/case_study/case_study_36_48.pdf}
\caption{Bert training throughput with 36 layers and 48 layers.}
\label{fig:bert_36_48}
\end{figure}

\begin{figure}[t]
\centering 
\includegraphics[width=0.85\linewidth]{figures/case_study/case_study_model.pdf}
\caption{Bert training throughput with various widths and depths.}
\label{fig:bert_model_size}
\end{figure}

In this section, we compare the throughput of several MOMs on a variety of machine learning models. We begin by outlining the experimental setup, then introduce the tested models and assessed MOMs.

The majority of MOMs do allow for greater training batch sizes, as illustrated in Figure~\ref{fig:case_study_all}, but do so at the expense of training speed. In addition, we find that as the model size increases, the maximum throughput gap between MOMs and the original %without MOM 
decreases. Based on the observation, we hypothesize that MOMs are better for larger models.

To verify the hypothesis, we apply MOMs to Bert with various widths and depths. We show that when the model is wide or deep enough, training with MOMs can indeed increase throughput, showing that only in specific circumstances are MOMs useful and that applying them haphazardly may actually harm training by increasing overall training time.

% \textbf{Experiment Setting.} 
% \textbf{Hardware.}
We conduct our study on the AWS p3.2xlarge instance~\cite{awsp3} that has a single NVIDIA Tesla V100 GPU with 16 GB memory and 8 Intel Xeon Scalable (Skylake) vCPUs with 60 GB RAM.

\subsection{Results on Various Models}
\textbf{Evaluated Models.} We evaluate different MOMs on both convolutional neural networks (CNN) and transformer based models. For CNNs, we evaluate on ResNet-50~\cite{he2016deep} and Wide-ResNet-50~\cite{zagoruyko2016wide}. For transformer based models, we test both language tasks (Bert-Large~\cite{devlin2018bert}, GPT-Small~\cite{radford2021learning}) and vision tasks (Swin-Large~\cite{liu2021swin}).

\textbf{Evaluated MOMs.}  We list all evaluated MOMs and their combinations in Figure~\ref{fig:case_study_method}. For the state-of-art activation compressed training, we utilize the ActNN~\cite{chen2021actnn} quantizer, which quantizes each element of the activation layer into four bits. For gradient checkpointing on CNNs, we choose DTR~\cite{kirisame2020dynamic}, a greedy online algorithm that evicts activations based on different heuristics when the peak memory exceeds the hardware limit. We test mainstream checkpointing policy that checkpoints the input of each transformer block for transformer based models. We select the checkpoint first policy for transformer based models because it has the best support across different models and frameworks, thus adopted most widely. For swapping, although we are aware of different swapping techniques as described in Section~\ref{sec:related-work}, it is difficult to find an open-source implementation that is not based on simulation and provides good support for tested networks and frameworks. Therefore, we implement synchronous swapping in PyTorch ourselves. Additionally, we test the combinations of different MOMs as shown in the Figure~\ref{fig:case_study_method}.


\textbf{Findings.}
As shown in Figure~\ref{fig:case_study_all}, the throughput of most training instances increases with batch size, although to varying degrees. The only exception is DTR on Wide-ResNet-50, where the maximum throughput drops from 80.6 records/s to 65.1 records/s as batch size increases from 160 to 200. This is because DTR determines the rematerialization policy based on the memory limit dynamically, and evicts more activations as batch size increases and memory budget decreases.

All training instances display diminishing return as batch size increases. For the original training on Bert-Large and Swin-large, the training process is aborted due to out of memory error before reaching a throughput plateau, whereas all other training instances reach a plateau where increasing the batch size barely increases training throughout.

Lastly, when compared to the original training, all MOMs increase the maximum batch size. On average, different MOMs can increase the maximum batch size by 2.6$\times$, 2.8$\times$, 11.2$\times$, 4.5$\times$, 1.8$\times$ for the five evaluated models, which aligns with previous work.

However, when considering the purpose of efficient model training as mentioned above, the maximum throughput should be the primary criterion. And only gradient checkpointing and quantization on Swin-Large increase the maximum throughput by 8.8\% (76.86 vs 70.65) and 6.7\% (75.35 vs 70.65) respectively. On average, the maximum training throughput are only 43.6\%, 46.1\%, 50.6\%, 76.6\%, 64.4\% of the original on five evaluated models when MOMs are applied.
In this case, the two metrics (maximum batch size vs. maximum throughput) lead to completely opposite conclusions: MOMs are promising when evaluated using maximum batch size as the metric, while training is actually slowed down when evaluated using maximum throughput, which argues that the majority of MOMs are actually detrimental to training.
% As mentioned in Section~\ref{sec:eval-metric}, since maximum throughput is a better metric, the experiments actually show that the majority of MOMs are detrimental to training.

% In comparison to the original training, the training with quantization and swapping in Figure~\ref{fig:bert_opt} appears very impressive by improving the maximum batch size from 32 to 712. However, regardless of the batch size, the throughput never exceeds the original training, implying that the increased throughput from a bigger batch size does not compensate for the memory saving method's trade-off.
% As a result, rather than focusing on batch size, we should focus on the max throughput that a memory saving strategy can provide.


\subsection{Case Study on Bert}
Next, we ask if MOMs can be used to improve {\em both} batch size and throughput. From our results shown in Figure~\ref{fig:case_study_all}, training with quantization and gradient checkpointing are the most promising as they achieve the highest maximum throughput among all MOMs. 
Therefore, we apply those two MOMs on various Bert model sizes to see if they improve the maximum throughput. 
We first conduct experiments on Bert models with 36 layers and 48 layers.
As shown in Figure~\ref{fig:bert_36_48}, for Bert with 36 layers, the maximum training throughput with gradient checkpointing marginally exceeds that of the original. For the Bert 48 model, both training with quantization and checkpointing improve throughput, improving maximum throughput by 80.1\% (38.7 vs 69.7) and 32.3\% (38.7 vs 51.2) respectively.

Based on our findings, we hypothesize that MOMs are more advantageous for larger models. To verify our hypothesis, we experiment with Bert models of various widths (hidden size ranges from 384 to 1920) and depths (layer number ranges from 8 to 62) and investigate the max throughput while training using MOMs. As shown in Figure~\ref{fig:bert_model_size}, for both the original training and training with different MOMs, the maximum throughput decreases as model size increases (either depth or width). Additionally, the original training outperforms the memory optimized ones when model size is relatively small, and there is a critical point in model size where a certain MOM starts to increase throughput. Concretely, only when the Bert model is deeper than 34 layers or is wider than 1344 with respect to the hidden size do MOMs start to display benefits for training speed.


% when the model size changes, the exact original training still outperforms quantization. However, we notice that as the model grows larger, the gap between peak throughput is reduced. When there are 36 Bert layers, the quantization achieves 30.42 records/s peak throughput, which is comparable with the peak throughput of the original training (31.18 records/s). 
% % \philip{better swap the quantization and original}
% We believe this is because with a larger model, the original training is more constrained by the device memory limit. After applying quantization, training could gain more throughput by converting saved memory to larger batch sizes.

% To explain this phenomenon, we hypothesize that if we want to make memory saving training beat the original training, we should focus on two directions. First, we should train very large models, which makes original training bound by memory at an early stage and only achieve very small max batch size. Second, we should use GPU with very strong computation ability, which helps mitigate the overhead introduced by applying memory saving methods.

% Next, we verify our conjecture by applying quantization and checkpointing to the training of the Bert Model with various model size settings. We first conduct experiment on a model depth of 36 layers and 48 layers in search of the scenario where memory saving training prevails in throughput. Then we experiment with 25 various model widths (hidden sizes) ranging from 384 to 1920 and investigate the max throughput of memory saving strategies. Moreover, we train in the FP16 mode to maximize the computation ability. From the result shown in Figure~\ref{fig:bert_36}, we can find that the training with checkpointing achieves a very close throughput to the original training. For the Bert 48 model as shown in Figure~\ref{fig:bert_48}, the training with both quantization and checkpointing finally beats the original training with regard to throughput. On top of that, Figure~\ref{fig:model_width} demonstrates that in all three cases, the max throughput decreases with the increasing of model width. Moreover, the original training outperforms the memory optimized ones when model size is relatively small, and there exists a critical point in model size where a certain memory saving method starts to prevail. This illustrate again the previous situation that the Figure~\ref{fig:bert_36} is the case before the critical point and the Figure~\ref{fig:bert_48} is after.

% All of the above discoveries are intriguing, and some of them are counter-intuitive.
From these results, we find that MOMs are only beneficial under limited scenarios and can even degrade training throughput. Therefore, we want an efficient, accurate and intuitive cost model to evaluate the benefits of MOMs in improving training throughput. The cost model should be simple and general enough such that it can be easily used before training starts to determine if a MOM would be beneficial. Such model can bring tremendous benefits to reduce unnecessary waste of valuable computational resources in training deep learning models.