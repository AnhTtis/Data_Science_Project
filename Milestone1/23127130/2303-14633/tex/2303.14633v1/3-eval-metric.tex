\section{An Evaluation Metric for Memory Optimization Methods}
\label{sec:eval-metric}

We categorize existing MOM evaluation metrics in Section~\ref{sec:existing-metric}, and then describe their drawbacks. 
For example, max batch size, which is commonly used in prior work, simply measures reduction in memory consumption and ignores the computational overhead introduced by the given MOM. 
Therefore, it is difficult for users to determine if such optimizations should be used as both training speed and the peak memory consumption are crucial considerations when selecting MOMs. Perhaps surprisingly, as we will describe in Section~\ref{sec:case-study}, applying such optimizations can actually reduce maximum training throughput and therefore increase overall training time. Based on our observations, we propose a new metric -- maximum throughput, which reflects the result of computation-memory trade off and its impact on overall training time, to evaluate the efficiency of MOMs in a straightforward manner.

\subsection{Existing Evaluation Metrics}
\label{sec:existing-metric}

\begin{figure}[t]
\centering 
\includegraphics[scale=0.36]{figures/ill.pdf}
\caption{Existing evaluation metrics. The direction that a MOM is better under that metric is indicated by a green arrow.}
\label{fig:ill}
\end{figure}

Previous evaluation of various MOMs can be categorized as follows:
(1) \citet{chen2021actnn, peng2020capuchin, pmlr-v162-liu22v} compare the largest batch size the MOM enables given the same memory budget as shown in Figure~\ref{fig:ill}(a), which typically is the memory capacity of the hardware.
(2) Similarly, \citet{chen2021actnn, pmlr-v162-liu22v} compare the largest model the MOM allows given the same batch size and memory budget, where the largest depth and width of the model are used to assess the efficacy of a particular MOM.
(3) \citet{kirisame2020dynamic, chen2021actnn, huang2020swapadvisor, jain2020checkmate, pmlr-v162-liu22v} compare the performance overhead of a MOM with the original training under different memory thresholds, given the same batch size on a specified model, as shown in Figure~\ref{fig:ill} (b). 
Most prior work uses metrics (1) and (3).

\subsection{Drawbacks of Existing Evaluation Metrics}
\label{sec:metric-drawback}
As shown in Section~\ref{sec:existing-metric}, the evaluation metrics lack a direct and unified reflection of their effects on model training. 
First, all MOMs trade extra computation, more memory access or communication
%Sounds a bit vague, should we change memory access to communication?}
for reduced memory consumption.
We argue that
only taking memory reduction (such as the maximum batch size or maximum model size) into account
%we are discussing trade-offs 
is unfair as the overhead (i.g., extra training time) is ignored. % \alvin{what is comp expense? time?}
As shown in Section~\ref{sec:case-study}, MOMs increase max batch size when evaluated using maximum batch size (e.g., some can increase batch size by up to 11.2$\times$), but training is actually slowed down (50.6$\%$ max throughput of the original training, which means the overall training time is 1.97$\times$ of the original if the total number of training epochs keeps unchanged).

Moreover, the overhead given a batch size is also insufficient
% \philip{is also an incomplete metric|
as it ignores the fact that MOM permits a larger batch size, which may speed up training due to increased hardware usage and amortized framework overhead, thus speeding up end-to-end training even though it introduces some computational or communication overhead. 
%\alvin{adding overhead somehow increases speed??}\lily{yes, add overhead -- reduce memory -- enable larger batch size than the original training -- improve hardware utilization and amortize framework overhead -- improve throughput}


% Last but not least, the disparate criteria used in earlier research on various MOMs lack unanimity and generality, making it difficult to compare the effects horizontally.

% As shown in Section~\ref{sec:existing-metric}, the evaluation metrics vary among different papers and lack a direct and unified reflection of their effects on model training. First of all, most metrics mentioned above are indirect. In fact, with the same hardware configuration and same model, users are unconcerned about the largest batch size or the extra latency required to train one batch for a given batch size. Instead, the ultimate purpose of memory saving is to boost the speed of reducing training loss. Most of previously used metrics fail to directly serve such purpose. 

% Moreover, varied metrics that previous work adopts on different memory saving strategies lacks unity and generality, making it hard to horizontally compare the effects.


\subsection{A New Evaluation Metric}
\label{sec:better-metric}

\begin{table}[]
    \centering
    \caption{Notation used in this paper}
    \label{tab:notation}
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{c|c|c|c}
        \toprule
        Symbol & Description & Symbol & Description \\
        \midrule
        $m$ &  Peak memory of training & $\alpha$ 
            & Linear coefficient of $m$ \\
        $t$ &  Training latency of one batch
            & $\beta$ & Constant coefficient of $m$\\
        $v$ & Throughput
            & $\gamma$ & Linear coefficient of $t$ \\
        $x$ & Batch size
            & $\delta$ & Constant coefficient of $t$\\
        $M$ & Device memory limit
            & $T$ & Total training latency\\
        \bottomrule
    \end{tabular}}
\end{table}


Considering the drawbacks of previously proposed metrics, we propose to use the maximum training throughput (i.e., number of records the training technique can process per second) given a fixed machine configuration as the quantitative indicator of a MOM's effectiveness. Maximum throughput is general enough to be adopted on any MOM and models. 
%We argue that it is also a more direct reflection of the effectiveness. 
Given fixed epoch and data set size, if throughput $v$ increases, then the total training latency $T$ will decrease. An ideal MOM should boost model training by increasing higher throughput to reduce overall training time. For the following we will use the notations described in Table~\ref{tab:notation}.

All existing MOMs trade extra computation (e.g., recompute evicted activations) or communication (e.g., swap between CPU and GPU) for memory reduction. Therefore, given the same batch size, all such approaches have a lower throughput than the original. However, with extra memory, training can be done using larger batch sizes. As illustrated in our case study, a larger batch size usually results in a higher throughput. As a result, such MOMs are useful only if the maximum batch sizes they can achieve contribute to a higher maximum throughput than training without MOM.
%\alvin{isn't this always true? why would larger batch size not lead to higher tput?}\lily{Changed. I want to say compared with training without MOM..}
Therefore, we propose to use maximum throughput as our evaluation metric in this work.

In this paper, we focus on the training memory and throughput trade off.
%\alvin{sometimes you use latency and sometimes you use speed. pls be consistent} 
We leave the accuracy trade off to future research and assume that training accuracy remains constant regardless of batch size. As assumed in prior work~\citep{goyal2017accurate, kaplan2020scaling, li2021terapipe}, training should converge if the learning rate grows proportionally to the batch size until the batch size becomes sufficiently large (8k in ImageNet), which is often unattainable given the amount of memory available in typical systems.