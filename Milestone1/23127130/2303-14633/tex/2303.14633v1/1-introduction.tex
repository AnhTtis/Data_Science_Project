\section{Introduction}
\label{sec:intro}

Deep neural networks (DNNs) are increasing in size and complexity as larger models improve the accuracy and generalization in machine learning applications. However, increasing model sizes is limited by the amount of device (typically a GPU) memory since the cost of storing feature maps and their gradients grows linearly with the \emph{depth} (i.e., number of layers) and quadratically with the \emph{width} (i.e., hidden state dimension) of the network.

% The past several years have witnessed the trend towards ever-increasing 
% neural network (NN) models to improve accuracy and generalization in various machine learning (ML) applications, and their scale and application scenarios continue to grow aggressively. However, the ability to explore bigger models and perform large model training is restricted by the amount of device (typically a GPU) memory since the cost of storing feature maps and their gradients rises linearly with the \emph{depth} (i.e., number of layers) and quadratically with the \emph{width} (i.e., hidden state dimension) of the network. 

In response, various memory optimization methods (MOMs) have been developed to reduce training memory footprint. Activation compressed training (ACT) compresses stored tensors to minimize saved activation during forward propagation, and most work uses quantization~\citep{chakrabarti2019backprop, fu2020don,chen2021actnn, pmlr-v162-liu22v} as the compression tool. Gradient checkpointing~\citep{chen2016training} trades computation for memory by discarding intermediate activations during forward propagation and recalculating them during the backward pass. Swapping~\citep{peng2020capuchin, huang2020swapadvisor, wang2018superneurons, kirisame2020dynamic}
offloads GPU memory to the CPU when memory usage exceeds the device memory limit.
Such methods introduce training overhead
while reducing memory consumption for a given training batch size.

In this work, we survey the recent MOMs to understand when these techniques are helpful. We raise a number of questions: do those techniques really benefit training? 
% \alvin{what does benefit mean?} \lily{it means accelerate training, but it's wired to directly say a 'memory saving' method speeds up training process}
Which technique achieves the best computation-memory trade-off? Are there strategies that work best on specific architectures or hardware?

% In this work, we surveyed recent papers on memory optimization in the hopes of extracting practical lessons for the broader community. For example: do those techniques really benefit training? Which technique achieves the best computation-memory trade off? Are there strategies that work best on specific architectures or hardware?

We find that MOMs indeed help in two ways. First, they reduce the peak memory footprint compared to standard training for a fixed batch size. Second, they enable larger maximum batch sizes compared to standard training for a given hardware configuration.

Unfortunately, as we demonstrate, MOMs deliver mixed results when placed in the context of end-to-end training. Our experiments show that one major drawback of MOMs is that they can \emph{slow down training by up to 9.3$\times$}. Furthermore, one of the commonly used evaluation metrics, maximum batch size, does not directly demonstrate the effectiveness of MOMs, and can simply be 
increased via \emph{gradient accumulation}~\citep{grad-acc} of smaller mini-batches, where we run forward and backward passes of different mini-batches sequentially and accumulate the gradients of each mini-batch. 
% \alvin{so?} \lily{Therefore, maximum batch size does not directly indicate the effectiveness of MOMs because it can be achieved easily with gradient accumulation. Modify the sentence a bit.}

% \ddkang{
% Rewrite for the following paragraph. Some parts still need editing. Using MOMs for memory optimziation methods 

% Memory optimization methods (MOMs) indeed help in two ways. First, they reduce the peak memory footprint compared to standard training for a fixed batch size. Second, they enable larger maximum batch sizes compared to standard training for a fixed hardware configuration.

% Unfortunately, as we demonstrate, MOMs deliver mixed results when placed in the context of end-to-end training. Our experiments show that one major drawback of MOMs is that they can \emph{slow down training by up to TODO$\times$}. Furthermore, one of the commonly used evaluation metrics, maximum batch size, can simply be 
% increased via the \emph{gradient accumulation}~\citep{grad-acc} of smaller mini-batches, where we run forward and backward passes of different mini-batches sequentially and accumulate the gradients of each mini-batch.
% }

% There are indeed several consistent results: Memory optimization methods reduce the peak memory compared with the original training given the same batch size. And given the hardware, memory optimization enables a bigger maximum batch size compared with the original training. However, our central finding is that the state-of-the-art literature uses insufficient evaluation metrics such that our motivating questions are impossible to answer. 
% For instance, one of the commonly used evaluation metrics, maximum batch size, can simply be 
% increased via the \emph{gradient accumulation}~\citep{grad-acc} of smaller mini-batches, where we run forward and backward passes of different mini-batches sequentially and accumulate the gradients of each mini-batch. 
% Furthermore, we show that the gain in batch size claimed by previous works sometimes actually slows down the overall training process.

Given our results, we argue that the existing evaluation metrics (e.g., maximum batch size given the hardware, reduced peak memory with a given batch size, throughput overhead given the memory threshold and batch size, etc.) are insufficient in indicating the costs and benefits of memory optimization for training. 
Since the goal of training is to lower the loss as quickly as possible, we propose to use \emph{maximum training throughput} to evaluate the effectiveness of various MOMs, as it can directly indicate the  effectiveness of a training process. 


% \ddkang{This should be rewritten once the experiments are done, especially if it's on more than just Bert/V100}
Based on the maximum throughput, we conduct a measurement study on training various machine learning models, including convolutional neural networks (CNN) and transformer based models. We study different MOMs and also various model sizes to empirically understand the relationship among maximum throughput, batch size and model size. Aligned with literature, the maximum batch size is increased by all MOMs across the five examined models. However, the majority of MOMs add too much overhead to effectively assist training. We further conduct a case study on Bert and show that MOMs only begin to give a higher maximum throughput and accelerate the training process when the model is larger than a certain threshold (more than 34 layers or the network width is greater than 1344).

% Surprisingly, we observe that for all tested memory saving methods, only in limited scenarios are they beneficial for training, which is indicated by a higher max throughput given the same model and hardware configuration compared with the original training.

Unfortunately, it takes significant computation resources to 
% naively \alvin{what does naively mean?} \lily{remove naive because the following sentences explain why it's expensive}
measure the maximum training throughput of training with MOMs. A number of iterations must be performed in order to accurately measure the throughput for a particular batch size. This procedure must be repeated for various batch sizes in order to determine the maximum throughput. Finally, the same measurement process is needed to determine the maximum throughput for each MOM. More importantly, we anticipate a performance model that goes beyond estimating performance and is able to quantify the trade-off between memory and compute for the evaluation of MOMs. 
% \alvin{I don't get the last sentence} \lily{we need a cost model to tell us the tradeoff between memory and overhead, not just tell us the maximum throughput. what about now?}

In this work, we propose a performance model -- \tool, that can quantify the efficiency of MOMs based on small amount of profiling results. 
% Moreover, \tool also computes the tradeoff that relates to \alvin{what does it relate training efficiency to?} the training efficiency of a MOM. \lily{removed} 
We demonstrate the validity and effectiveness of \tool on various model architectures: ResNet-50~\citep{he2016deep}, Wide-ResNet-50~\citep{zagoruyko2016wide}, Bert~\citep{devlin2018bert}, Swin~\citep{liu2021swin}, and GPT3~\citep{brown2020language},
while running them on different hardware configurations, including different number of GPUs (1, 2, 4, 8 GPUs).
Experiments show that \tool achieves an $R$ score higher than 0.97 in predicting the maximum throughput and peak memory. \tool also accurately predicts whether a certain strategy can accelerate the training process. Finally, starting from the \tool, we mathematically derive and verify two implications where applying MOMs will be beneficial: when the trained model is significantly large and when training is distributed. We believe these findings will provide insights for future research on developing efficient MOMs.


In summary, we make the following contributions:
\begin{asparaitem}
    \item In Sections~\ref{sec:eval-metric} and~\ref{sec:case-study}, we show that previous evaluation metrics for evaluating MOMs, such as maximum batch size given memory budget, size of reduced peak memory given batch size, etc., are insufficient, as the benefits of larger batch sizes a memory method enables can be outweighed by the computational overhead it introduces.
    Instead, we evaluate MOMs by their maximum throughput improvement (record/s).
    % \item In Section~\ref{TODO}, we conduct comprehensive analysis on memory saving strategies for various models to understand 
    %
    \item In Section~\ref{sec:cost-estimation}, we use maximum throughput as the metric and develop an intuitive performance model \tool that allow users to quickly assess the effectiveness of each MOM. \tool is simple and general enough to apply on a wide range of models, and quantifies the trade-off between memory and compute.
    %
    \item In Section~\ref{sec:impl}, derived from \tool, we describe situations where applying MOMs are beneficial with theoretical proof and experimental verification.
\end{asparaitem}
