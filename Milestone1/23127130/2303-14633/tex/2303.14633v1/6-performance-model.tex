\subsection{Performance Model Simplification and Verification}
\label{sec:perf-model-verify}
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/formula.pdf}
	\vspace{-1em}
	\caption{Derivation of the relationship between batch sizes and memory consumption} 
\label{fig:formula}
\vspace{-1em}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/perf_model/papaya.pdf}
	\caption{Normalized \tool Score and normalized maximum throughput on five evaluated models. Normalized maximum throughput = maximum throughput of MOM / maximum throughput of the original training. Normalized P score = \tool Score of MOM / \tool Point.} 
    \label{fig:p_score_max_tpt}
\end{figure*}

Once we accurately estimate the memory and throughput, we can leverage them to solve Inequality~\ref{eq:perf_cond} to determine if a given OOM should be applied.

First, as indicated in Equation~\ref{eq:simplified-tpt}, throughput is a monotonic function of batch size. We need to increase the batch size as much as possible to obtain the best throughput. Device memory, on the other hand, limits the largest batch size possible. As shown in Figure~\ref{fig:formula}, given device memory limit $M$, with Equation~\ref{eq:org-cond}, we can calculate the value of $x_{0}$ such that the original training achieves the maximum throughput $\max(v_{0})$, as shown in Equation~\ref{eq:org-tpt}:
\begin{gather}
    \alpha_{0} x_{0} + \beta_{0} = M
    \label{eq:org-cond} \\
    \max(v_{0}) = \frac{x_{0}}{\gamma_{0}x_{0} + \delta_{0}}
    \label{eq:org-tpt}
\end{gather}
Next, as illustrated in Figure~\ref{fig:formula}, we can find a $x_{mom}$ such that the MOM achieves the same throughput as the $\max(v_{0})$ as represented in Equation~\ref{eq:ms-tpt}.
\begin{equation}
    v_{mom} = \frac{x_{mom}}{\gamma_{mom}x_{mom} + \delta_{mom}} = \max(v_{0})
    \label{eq:ms-tpt}
\end{equation}
Compared with the original training, to increase batch size and get an equal or greater throughput for MOM, the memory cost $m_{mom}$ at batch size $x_{mom}$ should not exceed the device memory capacity, i.e., $m_{mom}(x_{mom}) \leq M$.
% which means $m_{ms} \leq M$ as shown in Equation~\ref{eq:ms-cond}.
% \begin{equation}
%     \alpha_{ms} x_{ms} + \beta_{ms} \leq M
%     \label{eq:ms-cond}
% \end{equation}

After substituting Equation~\ref{eq:org-tpt} and Equation~\ref{eq:ms-tpt} into Inequality~\ref{eq:mem-constraint}, we get:
% \begin{equation}
%     M - \beta_{0} \leq \frac{\delta_{ms}\alpha_{ms} - \delta_{0}\alpha_{0}}{\gamma_{0} - \gamma_{ms}}
%     \label{eq:f1}
% \end{equation}
\begin{equation}
   \frac{\delta_{mom}\alpha_{mom}}{(\gamma_{0}-\gamma_{mom})(M - \beta_{0}) + \delta_{0}\alpha_{0}} \leq \frac{M - \beta_{mom}}{M - \beta_{0}}
    \label{eq:f1}
\end{equation}
Furthermore, as the fixed training latency overhead should be the same for the original and different MOMs, we can assume that
\begin{equation}
    \beta_{0} = \beta_{mom} = \beta
    \label{eq:approx1}
\end{equation}
Similarly, we further assume that the fixed execution cost is the same in both cases, in other words:
\begin{equation}
    \delta_{0} = \delta_{mom} = \delta
    \label{eq:approx2}
\end{equation}
Finally, we can simplify Inequality~\ref{eq:f1} to:
\begin{equation}
    \frac{\alpha_{0} - \alpha_{mom}}{\gamma_{mom} - \gamma_{0}} \geq \frac{M-\beta}{\delta}
    \label{eq:f2}
\end{equation}
Inequality~\ref{eq:f2} is interesting in that it is independent of batch size. It is determined only by $\gamma_{0},$ $ \gamma_{mom},$ $ \alpha_{0},$ $\alpha_{mom},$ $ \delta,$ $ \beta,$ $ M$, which are all fixed once the MOM, hardware, and model architecture are chosen.

Moreover, Inequality~\ref{eq:f2} is very intuitive. Since $\alpha$ is the incremental memory cost of increasing the batch size, $\alpha_{0} - \alpha_{mom}$ is just the incremental memory saving provided by the MOM. Similarly, $\gamma$ represents the incremental execution cost of increasing the batch size, hence $\gamma_{mom} - \gamma_{0}$ is the overhead incurred by the MOM. Inequality~\ref{eq:f2} implies that 
{\em training with a given MOM is beneficial only when the ratio between memory savings and overhead imposed by the MOM is larger than a threshold established by the fixed memory and execution cost.}

We define the left side of Inequality~\ref{eq:f2} as a score to evaluate the efficiency of a memory optimization method, and we call that the \tool Score:
\begin{equation}
    P = \frac{\alpha_{0} - \alpha_{mom}}{\gamma_{mom} - \gamma_{0}}
    \label{eq:pscore}
\end{equation}
In the score, the numerator indicates how much memory a given MOM can save, while the computational overhead is the denominator. The higher the \tool Score, the more memory can be saved for the same amount of overhead, 
and the memory saved also translates to significant throughput improvement.
%more efficient the conversion from saved memory to extra throughput. 
A lower \tool Score, on the other hand, indicates that more work is needed to save the same amount of memory. Even if the potential batch size is larger, the overhead wears off the advantages when the \tool Score falls below a threshold and the MOM fails to accelerate training. We define this threshold as the \tool Point $\hat P$, shown in Equation~\ref{eq:ppoint}:
\begin{equation}
    \hat P = \frac{M-\beta}{\delta}
    \label{eq:ppoint}
\end{equation}
 $\hat{P}$ is only related to $M$, $\beta$ and $\delta$, which are determined by device and model but not affected by the batch size.

\textbf{Performance model validation}
Next, we evaluate if the \tool Score is a good indicator of training speed. As can be seen from Figure~\ref{fig:p_score_max_tpt}, for all cases, when the normalized $P$ score is greater than one (i.e., the \tool Score is higher than the \tool Point), the MOM also has a higher maximum throughput than the original training (normalized maximum throughput is greater than one). Additionally, the \tool Score is a monotonic function of the maximum throughput. Therefore, users can use the \tool Score to compare different MOMs as a higher \tool Score also indicates a higher max throughput.
% Further, we can also use Papaya Score to compare different MOMs. In most cases, a higher Papaya Score also indicates a higher max throughput. There is one exception here for Swin on V100. Quantization has a higher Papaya score than gradient checkpointing, but its max throughput is smaller than checkpointing. This is because quantization introduces more memory fragmentation, and the gap between max peak memory and device limit is bigger as shown in Figure~\ref{fig:swin_mem}. Therefore, Swin cannot really hit the theoretical memory bound (device memory), making the actual max batch size and throughput lower than estimated. Moreover, Table~\ref{tab:perf-model} supports our approximation in Equation~\ref{eq:approx1} and Equation~\ref{eq:approx2}, where the fixed memory cost ($\beta$) and fixed execution cost ($\delta$) are similar across different MOMs once the model and hardware are decided.



% \lily{Only keep the following section if we have space}
% \subsection{Performance model for training with gradient accumulation}
% Sometimes, the gradient accumulation~\cite{grad-acc} is used to overcome the problem of batch size being limited by GPU memory. Gradient accumulation is a mechanism to split the batch of samples — used for training a neural network — into several mini-batches of samples that will be run sequentially. Therefore, we also modify our formula to model the behavior of gradient accumulation. The only thing that needs to be modified is Equation~\ref{eq:approx1}, where $f$ is the number of mini-batches within the same batch, and $x$ is the accumulated size of the mini-batches:
% \begin{equation}
%     m = \alpha\frac{x}{f} + \beta
%     \label{eq:approx_grad_acc}
% \end{equation}
% We then perform the same mechanism to derive the performance model as above, only replacing Equation~\ref{eq:approx1} with Equation~\ref{eq:approx_grad_acc}. We get a revised Papaya Score as shown below:
% \begin{equation}
% \frac{1}{f} \times \frac{\alpha_{0} -  \alpha_{ms}}{\gamma_{ms} - \gamma_{0}} 
%     \geq \frac{M-\beta}{\delta}
%     \label{eq:f2_grad}
% \end{equation}
% As seen above, Equation~\ref{eq:f2_grad} equals Equation~\ref{eq:f2} when $f=1$ (no accumulation is performed). Moreover, the Papaya Score gets smaller when $f$ gets bigger, which indicates that it is rarely beneficial
% % \philip{May be too affirmative here to say "never beneficial".}
% to combine memory saving strategies with gradient accumulation. 
% % \philip{A counter-example in extreme cases, Assume Papaya point=150, Papaya Score=1000 for a particular MOM: enabling a GA with update frequency four lowers the Papaya Score to be 250, but still beating the original, in this case throughput of GA_MS>GA_Org>Non_GA_Org, still beneficial. We may instead state that MOM is even more unlikely to provide any benefits with GA enabled, and the more you use GA, the less you will consider MS}
% % Notice here it is even possible for Papaya Score to be negative when $f>\frac{\alpha_0}{\alpha_{ms}}$.

\subsection{Limits of the performance model}
First, as mentioned in Section~\ref{sec:mem-est}, the derivation of the \tool Score and \tool Point does not take memory fragmentation into account. However, we can modify Inequality~\ref{eq:f2} to make them adapt to different fragmentation levels as we describe in Appendix~\ref{sec:appendix}. The modified formula takes the memory utilization ratio as the input. We also describe how binary search can be used to find the maximum efficient memory. However, as shown in Section~\ref{sec:perf-model-verify}, the current \tool Score/Point is accurate enough to make a good prediction of different MOMs' maximum throughput without taking memory fragmentation into account.

Second, our model uses a linear function to predict latency, which is insufficient when the batch size is small (i.e., when the GPU is underutilized). However, as shown in Section~\ref{sec:tpt-est}, current neural models typically fully utilize the hardware even with a small batch size, and hence our linear function can already model the latency in most cases. Moreover, the current model assumes that the batch size is the only factor that affects the peak memory and batch latency, therefore it does not handle dynamic optimization strategies such as DTR as mentioned in Section~\ref{sec:mem-est}.
