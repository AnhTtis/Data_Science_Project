@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{jain2020checkmate,
  title={Checkmate: Breaking the memory wall with optimal tensor rematerialization},
  author={Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Gonzalez, Joseph and Keutzer, Kurt and Stoica, Ion},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={497--511},
  year={2020}
}

@misc{nvidiackpt,
  doi = {10.48550/ARXIV.2205.05198},
  url = {https://arxiv.org/abs/2205.05198},
  author = {Korthikanti, Vijay and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Reducing Activation Recomputation in Large Transformer Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{chakrabarti2019backprop,
  title={Backprop with approximate activations for memory-efficient network training},
  author={Chakrabarti, Ayan and Moseley, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{fu2020don,
  title={Don’t waste your bits! squeeze activations and gradients for deep neural networks via TINYSCRIPT},
  author={Fu, Fangcheng and Hu, Yuzheng and He, Yihan and Jiang, Jiawei and Shao, Yingxia and Zhang, Ce and Cui, Bin},
  booktitle={International Conference on Machine Learning},
  pages={3304--3314},
  year={2020},
  organization={PMLR}
}

@inproceedings{chen2021actnn,
  title={Actnn: Reducing training memory footprint via 2-bit activation compressed training},
  author={Chen, Jianfei and Zheng, Lianmin and Yao, Zhewei and Wang, Dequan and Stoica, Ion and Mahoney, Michael and Gonzalez, Joseph},
  booktitle={International Conference on Machine Learning},
  pages={1803--1813},
  year={2021},
  organization={PMLR}
}

@inproceedings{evans2020jpeg,
  title={Jpeg-act: accelerating deep learning via transform-based lossy compression},
  author={Evans, R David and Liu, Lufei and Aamodt, Tor M},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={860--873},
  year={2020},
  organization={IEEE}
}

@article{pan2021mesa,
  title={Mesa: A Memory-saving Training Framework for Transformers},
  author={Pan, Zizheng and Chen, Peng and He, Haoyu and Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2111.11124},
  year={2021}
}

@inproceedings{
    anonymous2022exact,
    title={{EXACT}: Scalable Graph Neural Networks Training via Extreme Activation Compression},
    author={Anonymous},
    booktitle={Submitted to The Tenth International Conference on Learning Representations },
    year={2022},
    url={https://openreview.net/forum?id=vkaMaq95_rX},
    note={under review}
}

@inproceedings{jin2021novel,
  title={A novel memory-efficient deep learning training framework via error-bounded lossy compression},
  author={Jin, Sian and Li, Guanpeng and Song, Shuaiwen Leon and Tao, Dingwen},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={485--487},
  year={2021}
}

@article{evans2021ac,
  title={AC-GC: Lossy Activation Compression with Guaranteed Convergence},
  author={Evans, R David and Aamodt, Tor},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{wang2018superneurons,
	title={Superneurons: Dynamic GPU memory management for training deep neural networks},
	author={Wang, Linnan and Ye, Jinmian and Zhao, Yiyang and Wu, Wei and Li, Ang and Song, Shuaiwen Leon and Xu, Zenglin and Kraska, Tim},
	booktitle={Proceedings of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming},
	pages={41--53},
	year={2018}
}

@inproceedings{peng2020capuchin,
  title={Capuchin: Tensor-based GPU memory management for deep learning},
  author={Peng, Xuan and Shi, Xuanhua and Dai, Hulin and Jin, Hai and Ma, Weiliang and Xiong, Qian and Yang, Fan and Qian, Xuehai},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={891--905},
  year={2020}
}

@inproceedings{huang2020swapadvisor,
  title={SwapAdvisor: Pushing deep learning beyond the GPU memory limit via smart swapping},
  author={Huang, Chien-Chin and Jin, Gu and Li, Jinyang},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={1341--1355},
  year={2020}
}

@article{kirisame2020dynamic,
  title={Dynamic tensor rematerialization},
  author={Kirisame, Marisa and Lyubomirsky, Steven and Haan, Altan and Brennan, Jennifer and He, Mike and Roesch, Jared and Chen, Tianqi and Tatlock, Zachary},
  journal={arXiv preprint arXiv:2006.09616},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{li2021terapipe,
  title={Terapipe: Token-level pipeline parallelism for training large-scale language models},
  author={Li, Zhuohan and Zhuang, Siyuan and Guo, Shiyuan and Zhuo, Danyang and Zhang, Hao and Song, Dawn and Stoica, Ion},
  booktitle={International Conference on Machine Learning},
  pages={6543--6552},
  year={2021},
  organization={PMLR}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@misc{awsp3,
  author = {AWS},
  title = {{Amazon EC2 P3 Instances}},
  howpublished = "\url{https://aws.amazon.com/ec2/instance-types/p3/}",
  year = {2022},
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@misc{grad-acc,
  doi = {10.48550/ARXIV.1806.00187},
  url = {https://arxiv.org/abs/1806.00187},
  author = {Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Scaling Neural Machine Translation},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{pmlr-v162-liu22v,
  title = 	 {{GACT}: Activation Compressed Training for Generic Network Architectures},
  author =       {Liu, Xiaoxuan and Zheng, Lianmin and Wang, Dequan and Cen, Yukuo and Chen, Weize and Han, Xu and Chen, Jianfei and Liu, Zhiyuan and Tang, Jie and Gonzalez, Joey and Mahoney, Michael and Cheung, Alvin},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {14139--14152},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/liu22v/liu22v.pdf},
  url = 	 {https://proceedings.mlr.press/v162/liu22v.html},
  abstract = 	 {Training large neural network (NN) models requires extensive memory resources, and Activation Compression Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT’s approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training with a 4.2x to 24.7x larger batch size, with negligible accuracy loss.}
}

@article{beaumont2021efficient,
  title={Efficient Combination of Rematerialization and Offloading for Training DNNs},
  author={Beaumont, Olivier and Eyraud-Dubois, Lionel and Shilova, Alena},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23844--23857},
  year={2021}
}

@misc{cudnndoc,
  title = {Bacth Normalization Kernel Description},
  howpublished = {\url{https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx}},
  note = {Accessed: 2022-10-08}
}

@article{kumar2019efficient,
  title={Efficient rematerialization for deep networks},
  author={Kumar, Ravi and Purohit, Manish and Svitkina, Zoya and Vee, Erik and Wang, Joshua},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{Paszke_PyTorch_An_Imperative_2019,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
publisher = {Curran Associates, Inc.},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2019}
}