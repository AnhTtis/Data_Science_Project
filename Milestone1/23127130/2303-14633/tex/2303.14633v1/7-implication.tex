\section{Implications}
\label{sec:impl}
In this section, we show that the \tool Score and \tool Point can be used to predict scenarios where MOMs can improve max throughput. We state our implications followed by a theoretical proof derived from Formula \ref{eq:f2}, then we show experiment results to further validate our implication. For all experiments in this Section, we pick the best performing MOM in Figure~\ref{fig:case_study_all} for a given model.


\subsection{Preference for large models} 

\begin{figure*}[t]
\begin{center}
    \includegraphics[width=\linewidth]{figures/implications/implication.pdf}
\end{center}
\vspace{-0.2in}
\caption{(a), (b) show the maximum throughput ratio of training with 1, 2, 4, 8 V100 GPUs on Bert-Large and Swin-Large. (c) - (f) shows the maximum throughput of training ResNet and GPT with different depths and widths.}
\label{fig:impl}
\end{figure*}
MOMs are more beneficial for larger models. Here, we focus on models within the same family (e.g., Bert models of different layers, GPTs with different numbers of encoder/decoder blocks, etc) as models from different families are not directly comparable.

\textbf{Proof:} 
(1) Model depth: If the depth of a model increases (e.g., add the more transformer blocks), both the incremental memory cost and incremental batch latency increase linearly with the model depth (i.e., $\alpha = C1 \cdot D, \gamma = C2 \cdot D$, where $C1, C2$ are constants and $D$ is the depth of the network). If we increase the model depth by $k\times$, the incremental cost scales $k\times$ as well for both the original training and different MOMs. Putting the new $\alpha$s and $\gamma$s into Inequality~\ref{eq:f2}, the left part keeps unchanged since the $k$ cancels out for the numerator and denominator. On the other hand, the fixed memory cost increases ($\beta$) as the model gets larger. Therefore, the \tool point is lower and the \tool score keeps unchanged. As a result, while increasing the model depth, it is easier for \tool score to surpass \tool point, which means MOMs are more likely to improve the maximum throughput. 

(2) Model width: If the width (e.g., the hidden size of each layer) of a model increases, the incremental memory cost will increase linearly with the width, while the incremental batch time will increase quadratically with the model width (i.e. $\alpha = C1 \cdot W$, $\gamma = C2 \cdot W^2$, where $C1$ and $C2$ are constants and $W$ is the width of the network). After putting the incremental memory/batch time cost into Equation~\ref{eq:pscore}, we get the new \tool Score:
\begin{equation}
   \frac{C1_0 - C1_{mom}}{(C2_{mom} - C2_0)\cdot W}
\label{eq:impl_pscore}
\end{equation}
Meanwhile, the fixed memory cost ($\beta$ in Formula~\ref{eq:ppoint}) increases quadratically with the model (i.e., $\beta=C'1\cdot W^2$, where $C1'$ is a constant and $W$ is the width of the network) because the model-related objects (e.g., model parameters, optimizer states) occupy most of the fixed memory consumption and they increase quadratically with the model width. Similarly, as the fixed time cost is mainly caused by the parameters updates, the fixed batch time also increases quadratically with the model width (i.e., $\delta=C'2\cdot W^2$, where $C'2$ is a constant). After putting them into the \tool Point formula (Formula~\ref{eq:ppoint}), we get the new \tool Point:
\begin{equation}
   \frac{M  - C1'\cdot W^2}{C2'\cdot W^2}
\label{eq:impl_ppoint}
\end{equation}
Comparing the new \tool Score and \tool Point, we get the inequality
\begin{equation}
   \frac{C1_0 - C1_{mom}}{(C2_{mom} - C2_0)} > \frac{M  - C1'\cdot W^2}{C2'\cdot W}
\label{eq:impl_ppoint}
\end{equation}
From the inequality, we find that the left part is unrelated with the model width and right part becomes smaller as the model width increases, indicating that it is easier for the \tool Score to exceed the \tool Point.

\textbf{Verification:} We verify this implication by varying the model depth and width for ResNet and GPT3 models on V100. As shown in Figure~\ref{fig:impl}, when the model is shallow (less than 280 layers in ResNet, less than 18 layers in GPT3) or thin (hidden size is smaller than 248 for ResNet, 1500 for GPT3), applying MOMs actually slows down training. However, as the model gets deeper or wider, MOMs start to speed up training. In the extreme case, the original training fails to run even with batch size of one and applying MOM is needed to enable single GPU training.

% When we train a larger model, the fixed memory cost will be higher. Hence, according to Papaya Point in Equation~\ref{eq:ppoint}, $\beta$ grows in large model training and resulting in a lower Papaya Point, allowing Papaya Score to surpass Papaya Point more easily. This is supported by Figure~\ref{fig:bert_ips},~\ref{fig:swin_ips},~\ref{fig:gpt3_ips}.
% On Bert, even the best memory saving approach in this setup (checkpointing) has only 86\% throughput ratio.
% When the model grows larger (Swin), the maximum throughput of checkpointing is comparable to or slightly higher than the original training (76.86 records/s versus 70.65 records/s), achieving a 109\% throughput ratio. %On GPT3, the largest of the three models, both gradient checkpointing and quantization achieve faster training speed than the original training.
% Note that the fixed memory/latency cost is implementation sensitive, and in our experiment settings, the implementation framework of GPT differs from the other two. Therefore, comparing Papaya point of GPT models with the other two types of models has little implications. Instead, we compare the GPT-Small with GPT-Medium. As GPT-Medium achieves a lower Papaya score as expected (50.3 versus 246.2 on T4), it indeed achieves a higher final throughput ratio (96\% versus 80\%).

% \subsection{Preference for high performance GPU.} 
% \textbf{Proof:} When we use a GPU with a higher computation power, it takes less time to save the same amount of memory, which reduces the relative overhead caused by memory saving methods. Hence, according to Papaya Score in Equation~\ref{eq:pscore}, $\gamma_{ms} - \gamma_{0}$ decreases with a high-performance training, resulting a higher Papaya Score. As a result, while adopting memory saving strategies, it is easier for devices with higher computation power to provide throughput improvements. 

% \textbf{Verification:} Figure~\ref{fig:bert_ips}~\ref{fig:t4_bert_ips} shows the Bert-Large results on V100 and T4. With more computation power, V100 achieves a higher max throughput ratio than T4 (86\% versus 69\%).

\subsection{Preference for multiple GPUs} 
MOMs are more beneficial for multi-GPU data parallel training. The intuition behind the implication is that communication makes MOMs less expensive. 

\textbf{Proof:} Communication and synchronization across multiple devices result in a substantially higher fixed execution cost for multi-GPU training. According to \tool Point in Equation~\ref{eq:ppoint}, $\delta$ (the portion of batch time independent of batch sizes) increases because communication costs in data parallel setting is included in $\delta$, and the synchronous overhead grows when more GPUs are used. As a result, the \tool Point is lower and and it is easier for the \tool Score to surpass the \tool Point.

\textbf{Verification:} We compare the maximum throughput ratio between gradient checkpointing and the original training on Bert-Large and Swin-Large on distributed data parallel training with 1, 2, 4, 8 V100 GPUs. As shown in Figure~\ref{fig:impl}, the ratio increases by adding more GPUs. Specifically, on Bert-Large, gradient checkpointing slows down training on a single GPU setting. However, as we increase the number of GPUs, gradient checkpointing achieves a higher maximum throughput than the original training, and can speedup the training process by $1.7\times$ when there are 8 GPUs.
