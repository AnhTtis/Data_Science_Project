\section{Overview of Memory Optimization Methods}

In this work, we focus on \emph{memory optimization methods}, which aim to reduce the total amount of memory used for DNN training on accelerators. The bulk of memory usage during training time falls under three categories: First, \emph{activations} (i.e., feature maps) are the intermediate tensors stored during the forward pass that are needed to compute gradients during the backward pass. Second, \emph{model related objects} that store the model's state, such as model parameters (e.g., weight matrices), optimizer states, and gradient accumulators, which collect gradients from earlier steps. For example, the momentum technique needs to calculate the parameter updates using both the gradients from the previous and the current step. Third, \emph{workspace memory}, which includes temporary storage for computing calculations. For example, the CuDNN batch normalization backward function needs sufficient workspace memory to activate the fast NHWC semi-persistent kernel and speed up calculation~\cite{cudnndoc}. Workspace memory will be released once the operator finishes execution. 

\begin{figure}
\begin{center}
    \includegraphics[scale=0.54]{figures/memory.pdf}
\end{center}
\vspace{-2em}
\caption{Memory consumption in different models. BS = Batch Size.}
\label{fig:memory_category}
\vspace{-2em}
\end{figure}



% The memory consumption during training consists of (1) Activation, or features, which are the intermediate tensors stored during the forward pass and needed to calculate gradients during the backward pass. Typically, dropout/activation masks and layer output are parts of the activation. (2) Model related objects, including weight matrices, optimizer states, and gradient accumulators. Weight matrices are used to store the model parameters. Optimizer states records the updates from previous iterations, which are used for momentum updates. Additionally, the collected gradients from earlier steps are kept in the gradient accumulators. (3) Workspace memory. 
% Some kernels need temporary storage in order to complete the calculation. For instance, the CuDNN batch normalization backward function needs sufficient workspace memory to activate the fast NHWC semi-persistent kernel and speed up calculation~\cite{cudnndoc}. Workspace memory will be released once the operator finishes execution. 

We show the memory consumption of various components for a range of models in Figure~\ref{fig:memory_category}. As demonstrated in the graph and mentioned in previous work~\cite{jain2020checkmate}, activation takes up a significant portion of total memory for convolutional neural networks (CNNs). With the introduction of transformer-based model after 2017, the size of model-related objects becomes non-negligible.The majority of memory-saving solutions concentrate on lowering activation memory and they 
% there is also a need for methods that can reduce model-related object size. In this work, we focus on MOMs that aim to reduce the activation memory. \alvin{then why mention there is also need to reduce model related obj size?} \lily{removed}
can be divided into three categories:

\label{sec:related-work}
\textbf{Gradient Checkpointing}~\cite{chen2016training} trades computation for memory by dropping some of the activations during the forward pass from memory and recomputing them during the backward pass.
\citet{jain2020checkmate, kumar2019efficient} formalize the problem of trading computation for memory as a linear programming problem to generate a near-optimal schedule. To handle not only static but also dynamic computation graphs, \citet{kirisame2020dynamic} proposes a greedy online algorithm for heuristically checkpointing arbitrary models. \citet{nvidiackpt} combines gradient checkpointing with model parallelism for training transformer models.

\textbf{Activation Compressed Training (ACT)} compresses the training activation during the forward pass and decompresses the saved activation when calculating the gradient during back propagation. It has been applied to convolution neural networks using different compressors, such as quantizers~\citep{chakrabarti2019backprop, fu2020don,chen2021actnn, pmlr-v162-liu22v}, JPEG~\citep{evans2020jpeg}, or scientific data compression algorithms~\citep{jin2021novel,evans2021ac}. ACT has also been applied to transformers~\citep{pan2021mesa} and graph neural networks~\citep{anonymous2022exact}.
 
\textbf{Swapping} offloads activation or model parameters to external memory (e.g., CPU memory). 
\cite{wang2018superneurons, huang2020swapadvisor, peng2020capuchin} explore the search space of jointly optimizing operator scheduling, memory allocation, and swap decisions for static graphs. %\alvin{what are swap decisions}
% To handle dynamic computation graphs, \cite{kirisame2020dynamic} employs a greedy online algorithm that is extensible and general for different tensor eviction policies and model architectures. 
Recent work~\citep{beaumont2021efficient} also explores combining gradient checkpointing and swapping.