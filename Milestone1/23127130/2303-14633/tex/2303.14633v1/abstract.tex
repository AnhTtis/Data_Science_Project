\begin{abstract}

Large neural networks can improve the accuracy and generalization on tasks across many domains. However, this trend cannot continue indefinitely due to limited hardware memory. As a result, researchers have devised a number of memory optimization methods (MOMs) to alleviate the memory bottleneck, such as gradient checkpointing, quantization, and swapping.
In this work, we study memory optimization methods and show that, although these strategies indeed lower peak memory usage, they can actually \emph{decrease training throughput} by up to 9.3$\times$.
To provide practical guidelines for practitioners, we propose a simple but effective performance model \tool to quantitatively explain the memory and training time trade-off. \tool can be used to determine when to apply the various memory optimization methods in training different models.
We outline the circumstances in which memory optimization techniques are more advantageous based on derived implications from \tool. We assess the accuracy of \tool and the derived implications on a variety of machine models, showing that it achieves over 0.97 $R$ score on predicting the peak memory/throughput, and accurately predicts the effectiveness of MOMs across five evaluated models on vision and NLP tasks.


% In this work, we first conduct a case study using the Bert model to see how effective such memory saving solutions are. 
% Surprisingly, we find that although these strategies indeed lower peak memory usage, the associated overhead 
% (e.g., recomputation, communication between CPU and GPU) with these strategies is too high to actually benefit training.
% To reason why this is the case, we devise an intuitive performance model to quantitatively explain the memory and training time trade-off. 
% We then show how our performance model can be used to determine when to apply the various memory optimization strategies in training different models.
% Evaluation of our performance models on Bert, Swin Transformer, and GPT-3 demonstrates our model's ability to accurately evaluate and estimate the effectiveness of multiple memory saving strategies.
% More importantly, we outline the circumstances in which memory saving techniques are more advantageous based on derived implications from the cost model. We assess the accuracy of the cost model and the derived implications on a variety of machine models.

\end{abstract}