\section{Conclusion}
\label{sec:conclusion}
In this paper, we argue that existing evaluation metrics for MOMs are insufficient and propose to use the maximum throughput to evaluate different MOMs. 
Based on the new metric, we investigate the effectiveness of different MOMs on various models and perform detailed analysis on Bert. We find that existing MOMs bring limited benefits for training deep learning models, where larger batch sizes does not necessarily translate to faster training time.
We then propose linear models to predict the peak memory use and batch execution latency. The linear model partially explains the inefficiency of those MOMs: the throughput increases as we have a bigger batch size, but the benefits of memory reduction (as demonstrated by bigger batch size) wear off too quickly to result in any throughput increase. 
We capture this insight in a new metric called the \tool Score, and use it to evaluate different MOMs while training a number of deep learning models. The \tool Score is a good indicator of their efficiency. Starting from \tool, we also provide insights for future research on developing MOMs by identifying cases where applying MOMs can improve maximum throughput.