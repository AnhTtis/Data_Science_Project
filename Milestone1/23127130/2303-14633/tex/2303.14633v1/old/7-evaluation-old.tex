\section{Evaluation}
\label{sec:evaluation}
\textbf{Experiment Setting.}
\begin{table}[]
    \centering
    \caption{Evaluation Platform}
    \label{tab:hardware}
    \resizebox{0.9\linewidth}{!}{
        \begin{tabular}{c|c|c|c}
        \toprule
        Instance Type & GPU Version & Number of GPUs & GPU Memory (GiB) \\
        \midrule
        p3.2xlarge    & V100 & 1 & 16 \\
        p3.8xlarge    & V100 & 4 & 16 \\
        g4dn.4xlarge  & T4   & 1 & 15 \\
        \bottomrule
    \end{tabular}}
\end{table}
% Hardware: V100, T4, multi-GPU.
% Model: Bert Large, Swin Large, GPT3 Small
% We conduct experiments on three types of AWS instances, covering GPUs of different generations and single/multi GPU settings. For single GPU, we evaluate on p3.2xlarge instance with a single NVIDIA V100 GPU that has 16 GiB memory and g4dn.4xlarge instance with a single NVIDIA T4 GPU that has 15 GiB memory. For multi GPU, we use p3.8xlarge instance with four NVIDIA Tesla V100 GPUs, each of which has 16 GiB memory. 
We conduct all experiments on AWS and list hardware information in Table~\ref{tab:hardware}.
We test on three models, Bert-Large~\cite{devlin2018bert}, Swing-Large~\cite{liu2021swin}, and GPT3-Small~\cite{brown2020language}, covering both vision and NLP tasks. All models are trained with FP16 and we use data parallel for multi-GPU training.


% \begin{figure}
% \centering 
% \begin{subfigure}[b]{0.22\linewidth}
%     \includegraphics[width=\linewidth]{figures/v100/bert_batch_time.pdf}
%     \caption{Bert batch time.}
%     \label{fig:bert_time}
% \end{subfigure}
% \begin{subfigure}[b]{0.21\linewidth}
%     \includegraphics[width=\linewidth]{figures/v100/swin_batch_time.pdf}
%     \caption{Swin batch time.}
%     \label{fig:swin_time}
% \end{subfigure}
% \begin{subfigure}[b]{0.22\linewidth}
%     \includegraphics[width=\linewidth]{figures/v100/gpt3_batch_time.pdf}
%     \caption{GPT3 batch time.}
%     \label{fig:gpt3_time}
% \end{subfigure}

% \begin{subfigure}[b]{0.22\linewidth}
%     \includegraphics[width=\linewidth]{figures/v100/bert_mem.pdf}
%     \caption{Bert peak memory.}
%     \label{fig:bert_mem}
% \end{subfigure}
% \begin{subfigure}[b]{0.22\linewidth}
%     \includegraphics[width=\linewidth]{figures/v100/swin_mem.pdf}
%     \caption{Swin peak memory.}
%     \label{fig:swin_mem}
% \end{subfigure}
% \begin{subfigure}[b]{0.22\linewidth}
%     \includegraphics[width=\linewidth]{figures/v100/gpt3_mem.pdf}
%     \caption{GPT3 ory.}
%     \label{fig:gpt3_mem}
% \end{subfigure}
% \caption{Profiling and prediction of batch time and ory on V100. We plot the profiling data points and the prediction line for Bert-Large, Swin-Large, and GPT3-Small. 20\% of profiling data points are used to fit prediction lines.}
% \label{fig:batch-time-memory-cost-est}
% \end{figure}

\subsection{Cost Estimation}
\label{sec:cost-est}
We first test our cost estimation on the three models. We profile the execution latency and memory consumption across different batch sizes. Then we sample 20\% of profiling data points to fit prediction lines.  We show detailed profiling and prediction results in Appendix.
For all three evaluated models on different hardware, the linear models for memory and latency estimation achieve over 0.98 $R^2$ score.


% \begin{figure}[h]
% \begin{center}
% \begin{subfigure}[b]{0.22\linewidth}
%     \includegraphics[width=\linewidth]{figures/v100/swin_batch_time.pdf}
%     \caption{Swin batch latency.}
%     \label{fig:swin_mem}
% \end{subfigure}
% \begin{subfigure}[b]{0.22\linewidth}
%     \includegraphics[width=\linewidth]{figures/v100/bert_batch_time.pdf}
%     \caption{Bert batch latency.}
%     \label{fig:bert_mem}
% \end{subfigure}
% \begin{subfigure}[b]{0.22\linewidth}
%     \includegraphics[width=\linewidth]{figures/v100/gpt3_batch_time.pdf}
%     \caption{GPT batch latency.}
%     \label{fig:gpt_mem}
% \end{subfigure}
% \end{center}
% \begin{center}

% % \begin{subfigure}[b]{0.22\linewidth}
% %     \includegraphics[width=\linewidth]{figures/v100/bert_ips.pdf}
% %     \caption{Bert throughput on V100.}
% %     \label{fig:bert_ips}
% % \end{subfigure}
% % \begin{subfigure}[b]{0.22\linewidth}
% %     \includegraphics[width=\linewidth]{figures/v100/swin_ips.pdf}
% %     \caption{Swin throughput on V100.}
% %     \label{fig:swin_ips}
% % \end{subfigure}


% % \begin{center}
% %     \begin{subfigure}[b]{0.22\linewidth}
% %     \includegraphics[width=\linewidth]{figures/v100/gpt3_ips.pdf}
% %     \caption{GPT3 throughput on single V100.}
% %     \label{fig:gpt3_ips}
% %     \end{subfigure}
% %     \begin{subfigure}[b]{0.22\linewidth}
% %         \includegraphics[width=\linewidth]{figures/t4/bert_ips.pdf}
% %         \caption{Bert throughput on single T4.}
% %         \label{fig:t4_bert_ips}
% %     \end{subfigure}
% %     \begin{subfigure}[b]{0.22\linewidth}
% %         \includegraphics[width=\linewidth]{figures/4-V100/bert_ips.pdf}
% %         \caption{Bert throughput on 4 V100s.}
% %         \label{fig:4v100_bert_ips}
% %     \end{subfigure}
% % \end{center}
% \caption{Profiling and prediction of batch latency and peak memory on V100. We plot the profiling data points and the prediction line for Bert-Large, Swin-Large, and GPT3-Small. 20\% of profiling data points are used to fit prediction lines.}
% \label{fig:batch-time-memory-cost-est}
% \end{figure}


\textbf{Batch Latency.} In general, we accurately fit the profiled data in all three models (with $R > 0.99$). When the batch size is small, nolinearity is observed. 
This is expected since GPUs are underutilized when batch sizes are small, and a bigger batch size increases training time sublinearly as GPU utilization improves. 
%We also observe that the fit is less accurate on GPT3. This is because we collect the data points every time we increase batch size by 1 and hardware alignment may make different batch sizes have similar performance.
% figure[] is the placeholder to be replaced

To ensure the robustness of our cost estimation, investigation into the initial non-linearity is conducted to develop a deeper understanding of its cause and pattern. We examine the GPU utilization based on two metrics: GPU utilization and kernel occupancy.
Specifically, GPU utilization is measured in time dimension, as the percentage of GPU busy time against total training time. Kernel occupancy, on the other hand, is the ratio of active warps on an SM to the maximum number of active warps supported by the SM.
% which is a fine-grained metrics reflecting low-level GPU utilization. 
We conduct experiments on a single V100 GPU. 
The GPU utilization is sampled every 0.2s.
We record the occupancy of every launched kernel and take the aggregated average across all the kernels grouped by every 0.1s.

It is found that kernel occupancy remains stable as the batch size increases. For instance, the average occupancy level tested on Bert-Base model when batch sizes are 4 (Figure~\ref{fig:bert_occupancy_4}) and 64 (Figure~\ref{fig:bert_occupancy_64}) exhibit no significant difference (0.91 and 0.90).
On the other hand, the time-based GPU utilization increases dramatically along the growth of batch size when batch size is relatively small and later reaches a plateau. 
%Tested with GPT-Small model, the average GPU utilization (Figure~\ref{fig:gpt_utilization}) surges when batch size is under 5 and becomes steady when batch size is larger than 6, which explains the small non-linear segment in Figure~\ref{fig:gpt_util_latency} when batch size is under 6. Such finding is further supported with Bert-Base model, in which the turning point is around 28 (Figure~\ref{fig:bert_utilization}), consistent with the batch latency turning point (Figure~\ref{fig:bert_util_latency}.
Tested with Bert-Base model, the average GPU utilization (Figure~\ref{fig:bert_utilization}) surges when batch size is under 20 and becomes steady when batch size is larger than 30, which explains the small non-linear segment in Figure~\ref{fig:bert_util_latency} when batch size is under 20. Such finding is further supported with GPT-small model, the largest model in our experiments, and the utilization reaches a plateau much earlier at batch size of 6 (Figure~\ref{fig:gpt_utilization}), consistent with the batch latency turning point (Figure~\ref{fig:gpt_util_latency}.

We conclude from aforementioned observations that the idle time of GPU (i.e. low utilization) when batch size is small causes the non-linearity, and confirm the linearity for batch sizes beyond the turning point. As occupancy remains steady, the low utilization can be caused by the framework overhead, which is better amortized when batch size increases. As for the pattern, the larger the model, the smaller batch size the GPU reaches the utilization plateau and the batch latency establishes linearity at. Thus, we only need to avoid a small under-utilized range of batch sizes when sampling to make the calculation of our performance model accurate and robust. In the big model training setting, GPU reaches the utilization plateau when batch size is small. Therefore, the non-linear part has little effect on the performance prediction and is even hard to observe.
% \begin{figure}
% \begin{center}
% \subfigure[Bert-Base, batch size = 4]{
%     \includegraphics[scale=0.3]{figures/Utilization/Occupancy/bert_base_4.pdf}
%     \label{fig:bert_occupancy_4}
% }
% \subfigure[Bert-Base, batch size = 64]{
%     \includegraphics[scale=0.3]{figures/Utilization/Occupancy/bert_base_64.pdf}
%     \label{fig:bert_occupancy_64}
% }
% \end{center}
% \caption{Kernel occupancy on V100 with different batch sizes.}
% % \label{fig:batch-time-memory-cost-est}
% \end{figure}

% \begin{figure}
% \begin{center}
% \subfigure[Bert-Base]{
%     \includegraphics[scale=0.3]{figures/Utilization/SMI/bert_base_avg.pdf}
%     \label{fig:bert_utilization}
% }
% \subfigure[GPT-Small]{
%     \includegraphics[scale=0.3]{figures/Utilization/SMI/gpt_avg.pdf}
%     \label{fig:gpt_utilization}
% }
% \subfigure[Bert-Base]{
%     \includegraphics[scale=0.3]{figures/Utilization/SMI/bert_batch_time.pdf}
%     \label{fig:bert_util_latency}
% }
% \subfigure[scale=0.3]{
%     \includegraphics[scale=0.3]{figures/Utilization/SMI/gpt_batch_time.pdf}
%     \label{fig:gpt_util_latency}
% }
% \end{center}

% \caption{GPU utilization on V100 with different batch sizes.}
% % \label{fig:batch-time-memory-cost-est}
% \end{figure}



\textbf{Maximum Throughput.} Maximum throughput given a fixed machine configuration is the core building block of our Papaya model, which can be derived from batch latency and peak memory according to Equation~\ref{eq:org-cond} and Equation~\ref{eq:org-tpt}. Based on predicted batch latency and peak memory, our experiments successfully predict the maximum throughput achieved by memory saving strategies (original, checkpointing, quantization and swapping) on Bert-Large, Swin-Large and GPT-Small models on both NVIDIA T4 and V100 GPU with a high accuracy above 0.999.


% \begin{figure*}[t]
% \begin{center}
% \subfigure{
%     \includegraphics[scale=0.3]{figures/legend.pdf}
% }

% \subfigure[Swin-Large on V100]{
%     \includegraphics[scale=0.3]{figures/v100/swin_mem.pdf}
%     \label{fig:swin_mem}
% }
% \subfigure[Bert-Large on V100]{
%     \includegraphics[scale=0.3]{figures/v100/bert_mem.pdf}
%     \label{fig:bert_mem}
% }
% \subfigure[GPT-Small on V100]{
%     \includegraphics[scale=0.3]{figures/v100/gpt3_mem.pdf}
%     \label{fig:gpt_mem}
% }
% % \begin{subfigure}[b]{0.24\linewidth}
% %     \includegraphics[width=\linewidth,height=\linewidth]{figures/4-V100/bert_ips.png}
% %     \caption{Bert-Large on 4V100}
% %     \label{fig:gpt_mem_4v100}
% % \end{subfigure}
% \end{center}
% \centering 
% \subfigure[Swin-Large on V100]{
%     \includegraphics[scale=0.3]{figures/v100/swin_ips.pdf}
%     \label{fig:swin_ips}
% }
% \subfigure[Bert-Large on V100]{
%     \includegraphics[scale=0.3]{figures/v100/bert_ips.pdf}
%     \label{fig:bert_ips}
% }
% \subfigure[GPT-Small on V100]{
%     \includegraphics[scale=0.3]{figures/v100/gpt3_ips.pdf}
%     \label{fig:gpt3_ips}
% }
% \subfigure[Bert-Large on T4]{
%     \includegraphics[scale=0.3]{figures/t4/bert_ips.pdf}
%     \label{fig:t4_bert_ips}
% }
% \caption{(a)-(c) profiled peak memory on a single V100, (d)-(g) profiled throughput on %4V100
% single V100 and single T4. 20\% of profiling data points are used to fit prediction curves.}
% % \label{fig:batch-time-memory-cost-est}
% \end{figure*}



% \begin{table*}[t]
%     \centering
%     \caption{Evaluation of the performance model on three models and three hardware settings. We present the coefficients of memory and execution cost estimation ($\alpha$, $\beta$ $\gamma$, $\delta$, as well as the Papaya Score derived with Equation~\ref{eq:pscore}. The Papaya Point is also listed for each hardware and model combination. Max throughput is the maximum throughput the method can achieve across different batch sizes. Ratio is the max throughput of given method divided by the max throughput of the original.}
%     \label{tab:perf-model}
%     \resizebox{0.96\linewidth}{!}{
%         \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
%         \toprule
%         Device & Model & Saving Strategy & $\alpha$ & $\beta$  & $\gamma$ &  $\delta$  & P Score & P Point & Max throughput (ratio) \\
%         \midrule
%          & Bert-Large & Checkpointing  & 0.021 & 5.39 & 0.0068 & 0.042 & 75.91 & 169.30 & 145.88 (86\%)\\
%          & Bert-Large & Quantization   & 0.043 & 5.94 & 0.0068 & 0.13  & 67.90 & 169.30 & 131.43 (77\%)\\
%          & Bert-Large & Swapping       & 0.0049 & 5.66& 0.039  & 0.17  & 6.27  & 169.30 & 25.16 (15\%)\\
%          & Swin-Large & Checkpointing  & 0.062 & 5.19 & 0.012  & 0.12  & 75.24 & 68.37  & 76.86 (109\%)\\
%         V100 & Swin-Large & Quantization   & 0.058 & 4.74 & 0.011  & 0.18  & 107.20& 68.37  & 75.35 (107\%)\\
%          & Swin-Large & Swapping       & 0.032 & 5.11 & 0.058  & 0.19  & 5.31  & 68.37  & 17.08 (24.18\%)\\
%          & GPT-Small & Checkpointing  & 0.32  & 2.30 & 0.013  & 0.045  & 105.63& 327.87  & 70.22 (85\%)\\
%          & GPT-Small & Quantization   & 0.41  & 2.30 & 0.017  & 0.045  & 32.99 & 327.87  & 50.12 (60\%)\\
%          & GPT-Small & Swapping       & 0.31  & 2.29 & 0.47   & 1.23  & 2.94  & 327.87  & 7.6 (9\%)\\
%         \hline
%           & Bert-Large & Checkpointing  & 0.022 & 5.31 & 0.023 & -0.09 & 21.48 & 203.32 & 43.1 (69\%)\\
%           & Bert-Large & Quantization   & 0.042 & 5.93 & 0.022 & 0.14  & 21.56 & 203.32 & 43.02 (69\%)\\
%           & Bert-Large & Swapping       & 0.0049& 5.67 & 0.075 & 0.28  & 3.15  & 203.32 & 13.09 (21\%)\\
%           & Swin-Large & Checkpointing  & 0.062 & 5.19 & 0.038 & 0.13  & 18.45 & 66.34 & 24.7 (83\%)\\
%           & Swin-Large & Quantization   & 0.059 & 4.70 & 0.038 & 0.14  & 18.66 & 66.34 & 24.84 (83\%)\\
%          T4  & Swin-Large & Swapping       & 0.033 & 5.06 & 0.11  & 0.27  & 2.82  & 66.34 & 20.65 (69\%)\\
%           & GPT-Small & Checkpointing  & 0.317  & 2.31 & 0.047 & 0.031 & 24.55 & 27.18 & 21.09 (78\%)\\
%           & GPT-Small & Quantization   & 0.410  & 2.30 & 0.063 & 0.042  & 8.38  & 27.18 & 15.51 (77\%)\\
%           & GPT-Small & Swapping       & 0.307  & 2.31 & 0.247  & 0.21  & 1.65  & 27.18 & 3.99 (15\%)\\
%           & GPT-Medium & Checkpointing  & 0.282  & 7.29 & 0.115 & 0.167  & 29.62 & 50.32 & 8.3 (96\%)\\
%           & GPT-Medium & Quantization  & 0.916  & 7.094 & 0.092 & 0.179  & 39.66  & 50.32 & 8.16 (94\%)\\
%         \hline
%          & Bert-Large & Checkpointing & 0.021  & 6.67 & 0.0068 & 0.15 & 77.30 & 49.42 & 548.88 (133\%)\\
%          & Bert-Large & Quantization  & 0.042  & 7.18 & 0.0069 & 0.21 & 64.42 & 49.42 & 469.44 (114\%)\\
%          & Bert-Large & Swapping      & 0.0046 & 6.95 & 0.076  & 0.29 & 2.7   & 49.42 & 51.72 (13\%)\\
%          & Swin-Large & Checkpointing & 0.062  & 5.18 & 0.012  & 0.13 & 98.35 & 41.53 & 293.76 (143\%)\\
%         4-V100 & Swin-Large & Quantization  & 0.058  & 4.76 & 0.013  & 0.11 & 81.18 & 41.53 & 287.8 (140\%)\\
%          & Swin-Large & Swapping      & 0.032  & 5.10 & 0.11   & 0.11 & 2.60  & 41.53 & 79.48 (39\%)\\
%          & GPT-Small & Checkpointing & 0.079  & 1.88 & 0.089  & 0.053& 102.86 & 9.97 & 55.51 (157\%)\\
%          & GPT-Small & Quantization  & 0.11   & 1.83 & 0.093  & 0.72 & 14.05  & 9.97 & 40.76 (115\%)\\
%          & GPT-Small & Swapping      & 1.73e-6 & 1.75  & 0.97 & 1.90 & 0.19 & 9.97 & 4.16 (12\%)\\
%         \bottomrule
%     \end{tabular}}
% \end{table*}

\subsection{Performance Model and Implication}
Next, we evaluate if the Papaya Score is a good indicator of training speed. As can be seen from Table~\ref{tab:perf-model}, for all cases, when Papaya Score is higher than the Papaya Point, the memory saving method also has a higher throughput than the original training. We can also use Papaya Score to compare different memory saving methods. In most cases, a higher Papaya Score also indicates a higher max throughput. There is one exception here for Swin on V100. Quantization has a higher Papaya score than gradient checkpointing, but its max throughput is smaller than checkpointing. This is because quantization introduces more memory fragmentation, and the gap between max peak memory and device limit is bigger as shown in Figure~\ref{fig:swin_mem}. Therefore, Swin cannot really hit the theoretical memory bound (device memory), making the actual max batch size and throughput lower than estimated. Moreover, Table~\ref{tab:perf-model} supports our approximation in Equation~\ref{eq:approx1} and Equation~\ref{eq:approx2}, where the fixed memory cost ($\beta$) and fixed execution cost ($\delta$) are similar across different memory saving methods once the model and hardware are decided.

Next, we derive some interesting findings from Papaya Score and verify the findings with experiment results. Several factors make memory saving methods more beneficial for model training:


\textbf{Preference for large models.} When we train a larger model, the fixed memory cost will be higher. Hence, according to Papaya Point in Equation~\ref{eq:ppoint}, $\beta$ grows in large model training and resulting in a lower Papaya Point, allowing Papaya Score to surpass Papaya Point more easily. This is supported by Figure~\ref{fig:bert_ips},~\ref{fig:swin_ips},~\ref{fig:gpt3_ips}.
On Bert, even the best memory saving approach in this setup (checkpointing) has only 86\% throughput ratio.
When the model grows larger (Swin), the maximum throughput of checkpointing is comparable to or slightly higher than the original training (76.86 records/s versus 70.65 records/s), achieving a 109\% throughput ratio. %On GPT3, the largest of the three models, both gradient checkpointing and quantization achieve faster training speed than the original training.
Note that the fixed memory/latency cost is implementation sensitive, and in our experiment settings, the implementation framework of GPT differs from the other two. Therefore, comparing Papaya point of GPT models with the other two types of models has little implications. Instead, we compare the GPT-Small with GPT-Medium. As GPT-Medium achieves a lower Papaya score as expected (50.3 versus 246.2 on T4), it indeed achieves a higher final throughput ratio (96\% versus 80\%).

\textbf{Preference for high performance GPU.} When we use a GPU with a higher computation power, it takes less time to save the same amount of memory, which reduces the relative overhead caused by memory saving methods. Hence, according to Papaya Score in Equation~\ref{eq:pscore}, $\gamma_{ms} - \gamma_{0}$ decreases with a high-performance training, resulting a higher Papaya Score. As a result, while adopting memory saving strategies, it is easier for devices with higher computation power to provide throughput improvements. Figure~\ref{fig:bert_ips}~\ref{fig:t4_bert_ips} shows the Bert-Large results on V100 and T4. With more computation power, V100 achieves a higher max throughput ratio than T4 (86\% versus 69\%).

\textbf{Preference for multiple GPUs.}
\begin{figure}[t]
\begin{center}
% \subfigure{
%     \includegraphics[scale=0.3]{figures/legend.pdf}
% }
\subfigure[Bert-Large]{
    \includegraphics[scale=0.3]{figures/implications/bert_v100_gpu.pdf}
    \label{fig:bert_dist}
}
\subfigure[Swin-Large]{
    \includegraphics[scale=0.3]{figures/implications/swin_v100_gpu.pdf}
    \label{fig:swin_dist}
}
% \subfigure[GPT-Small on V100]{
%     \includegraphics[scale=0.3]{figures/v100/gpt3_mem.pdf}
%     \label{fig:gpt_mem}
% }
\end{center}
\caption{Max throughput with different number of GPUs.}
% \label{fig:batch-time-memory-cost-est}
\end{figure}
Communication and synchronization across multiple devices will result in a substantially higher fixed execution cost for multi GPU training. As a result, according to Papaya Point in Equation~\ref{eq:ppoint}, $\delta$ increases in the multiple GPU setting, lowering Papaya Point and allowing Papaya Score to easily surpass Papaya Point. We observe the phenomenon when training Bert with single V100 and four V100 GPUs. None of the memory saving approaches help while training with a single GPU. With four GPUs, however, checkpointing and quantization achieve a higher maximum throughput than the original training.