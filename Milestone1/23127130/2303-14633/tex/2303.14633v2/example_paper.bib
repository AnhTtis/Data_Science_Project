@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{jain2020checkmate,
  title={Checkmate: Breaking the memory wall with optimal tensor rematerialization},
  author={Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Gonzalez, Joseph and Keutzer, Kurt and Stoica, Ion},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={497--511},
  year={2020}
}

@misc{nvidiackpt,
  doi = {10.48550/ARXIV.2205.05198},
  url = {https://arxiv.org/abs/2205.05198},
  author = {Korthikanti, Vijay and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Reducing Activation Recomputation in Large Transformer Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{chakrabarti2019backprop,
  title={Backprop with approximate activations for memory-efficient network training},
  author={Chakrabarti, Ayan and Moseley, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{fu2020don,
  title={Don’t waste your bits! squeeze activations and gradients for deep neural networks via TINYSCRIPT},
  author={Fu, Fangcheng and Hu, Yuzheng and He, Yihan and Jiang, Jiawei and Shao, Yingxia and Zhang, Ce and Cui, Bin},
  booktitle={International Conference on Machine Learning},
  pages={3304--3314},
  year={2020},
  organization={PMLR}
}

@inproceedings{chen2021actnn,
  title={Actnn: Reducing training memory footprint via 2-bit activation compressed training},
  author={Chen, Jianfei and Zheng, Lianmin and Yao, Zhewei and Wang, Dequan and Stoica, Ion and Mahoney, Michael and Gonzalez, Joseph},
  booktitle={International Conference on Machine Learning},
  pages={1803--1813},
  year={2021},
  organization={PMLR}
}

@inproceedings{evans2020jpeg,
  title={Jpeg-act: accelerating deep learning via transform-based lossy compression},
  author={Evans, R David and Liu, Lufei and Aamodt, Tor M},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={860--873},
  year={2020},
  organization={IEEE}
}

@article{pan2021mesa,
  title={Mesa: A Memory-saving Training Framework for Transformers},
  author={Pan, Zizheng and Chen, Peng and He, Haoyu and Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2111.11124},
  year={2021}
}

@inproceedings{
    anonymous2022exact,
    title={{EXACT}: Scalable Graph Neural Networks Training via Extreme Activation Compression},
    author={Anonymous},
    booktitle={Submitted to The Tenth International Conference on Learning Representations },
    year={2022},
    url={https://openreview.net/forum?id=vkaMaq95_rX},
    note={under review}
}

@inproceedings{jin2021novel,
  title={A novel memory-efficient deep learning training framework via error-bounded lossy compression},
  author={Jin, Sian and Li, Guanpeng and Song, Shuaiwen Leon and Tao, Dingwen},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={485--487},
  year={2021}
}

@article{evans2021ac,
  title={AC-GC: Lossy Activation Compression with Guaranteed Convergence},
  author={Evans, R David and Aamodt, Tor},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{wang2018superneurons,
	title={Superneurons: Dynamic GPU memory management for training deep neural networks},
	author={Wang, Linnan and Ye, Jinmian and Zhao, Yiyang and Wu, Wei and Li, Ang and Song, Shuaiwen Leon and Xu, Zenglin and Kraska, Tim},
	booktitle={Proceedings of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming},
	pages={41--53},
	year={2018}
}

@inproceedings{peng2020capuchin,
  title={Capuchin: Tensor-based GPU memory management for deep learning},
  author={Peng, Xuan and Shi, Xuanhua and Dai, Hulin and Jin, Hai and Ma, Weiliang and Xiong, Qian and Yang, Fan and Qian, Xuehai},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={891--905},
  year={2020}
}

@inproceedings{huang2020swapadvisor,
  title={SwapAdvisor: Pushing deep learning beyond the GPU memory limit via smart swapping},
  author={Huang, Chien-Chin and Jin, Gu and Li, Jinyang},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={1341--1355},
  year={2020}
}

@article{kirisame2020dynamic,
  title={Dynamic tensor rematerialization},
  author={Kirisame, Marisa and Lyubomirsky, Steven and Haan, Altan and Brennan, Jennifer and He, Mike and Roesch, Jared and Chen, Tianqi and Tatlock, Zachary},
  journal={arXiv preprint arXiv:2006.09616},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{li2021terapipe,
  title={Terapipe: Token-level pipeline parallelism for training large-scale language models},
  author={Li, Zhuohan and Zhuang, Siyuan and Guo, Shiyuan and Zhuo, Danyang and Zhang, Hao and Song, Dawn and Stoica, Ion},
  booktitle={International Conference on Machine Learning},
  pages={6543--6552},
  year={2021},
  organization={PMLR}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@misc{awsp3,
  author = {AWS},
  title = {{Amazon EC2 P3 Instances}},
  howpublished = "\url{https://aws.amazon.com/ec2/instance-types/p3/}",
  year = {2022},
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@misc{grad-acc,
  doi = {10.48550/ARXIV.1806.00187},
  url = {https://arxiv.org/abs/1806.00187},
  author = {Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Scaling Neural Machine Translation},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{pmlr-v162-liu22v,
  title = 	 {{GACT}: Activation Compressed Training for Generic Network Architectures},
  author =       {Liu, Xiaoxuan and Zheng, Lianmin and Wang, Dequan and Cen, Yukuo and Chen, Weize and Han, Xu and Chen, Jianfei and Liu, Zhiyuan and Tang, Jie and Gonzalez, Joey and Mahoney, Michael and Cheung, Alvin},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {14139--14152},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/liu22v/liu22v.pdf},
  url = 	 {https://proceedings.mlr.press/v162/liu22v.html},
  abstract = 	 {Training large neural network (NN) models requires extensive memory resources, and Activation Compression Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT’s approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training with a 4.2x to 24.7x larger batch size, with negligible accuracy loss.}
}

@article{beaumont2021efficient,
  title={Efficient Combination of Rematerialization and Offloading for Training DNNs},
  author={Beaumont, Olivier and Eyraud-Dubois, Lionel and Shilova, Alena},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23844--23857},
  year={2021}
}

@misc{cudnndoc,
  title = {Bacth Normalization Kernel Description},
  howpublished = {\url{https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationBackwardEx}},
  note = {Accessed: 2022-10-08}
}

@article{kumar2019efficient,
  title={Efficient rematerialization for deep networks},
  author={Kumar, Ravi and Purohit, Manish and Svitkina, Zoya and Vee, Erik and Wang, Joshua},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{Paszke_PyTorch_An_Imperative_2019,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
publisher = {Curran Associates, Inc.},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2019}
}

@inproceedings{viewMaterialization1,
  author    = {Eric N. Hanson},
  title     = {A Performance Analysis of View Materialization Strategies},
  booktitle = {Proceedings of the Association for Computing Machinery Special Interest
               Group on Management of Data 1987 Annual Conference, San Francisco,
               CA, USA, May 27-29, 1987},
  pages     = {440--453},
  publisher = {{ACM} Press},
  year      = {1987}
}

@article{viewMaterialization2,
  author    = {Alon Y. Halevy},
  title     = {Answering queries using views: {A} survey},
  journal   = {{VLDB} J.},
  volume    = {10},
  number    = {4},
  pages     = {270--294},
  year      = {2001}
}

@inproceedings{compression1,
  author    = {Huanchen Zhang and
               Xiaoxuan Liu and
               David G. Andersen and
               Michael Kaminsky and
               Kimberly Keeton and
               Andrew Pavlo},
  title     = {Order-Preserving Key Compression for In-Memory Search Trees},
  booktitle = {Proceedings of the 2020 International Conference on Management of
               Data, {SIGMOD} Conference 2020, online conference [Portland, OR, USA],
               June 14-19, 2020},
  pages     = {1601--1615},
  publisher = {{ACM}},
  year      = {2020}
}

@article{compression2,
  author    = {Vijayshankar Raman and
               Gopi K. Attaluri and
               Ronald Barber and
               Naresh Chainani and
               David Kalmuk and
               Vincent KulandaiSamy and
               Jens Leenstra and
               Sam Lightstone and
               Shaorong Liu and
               Guy M. Lohman and
               Tim Malkemus and
               Ren{\'{e}} M{\"{u}}ller and
               Ippokratis Pandis and
               Berni Schiefer and
               David Sharpe and
               Richard Sidle and
               Adam J. Storm and
               Liping Zhang},
  title     = {{DB2} with {BLU} Acceleration: So Much More than Just a Column Store},
  journal   = {Proc. {VLDB} Endow.},
  volume    = {6},
  number    = {11},
  pages     = {1080--1091},
  year      = {2013}
}

@inproceedings{compression3,
  author    = {Yinan Li and
               Craig Chasseur and
               Jignesh M. Patel},
  editor    = {Timos K. Sellis and
               Susan B. Davidson and
               Zachary G. Ives},
  title     = {A Padded Encoding Scheme to Accelerate Scans by Leveraging Skew},
  booktitle = {Proceedings of the 2015 {ACM} {SIGMOD} International Conference on
               Management of Data, Melbourne, Victoria, Australia, May 31 - June
               4, 2015},
  pages     = {1509--1524},
  publisher = {{ACM}},
  year      = {2015}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@article{peng2021random,
  title={Random feature attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2103.02143},
  year={2021}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{daras2020smyrf,
  title={Smyrf-efficient attention using asymmetric clustering},
  author={Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6476--6489},
  year={2020}
}

@misc{n1config,
  title = {N1 Instance Configuration},
  howpublished = {\url{https://cloud.google.com/compute/docs/general-purpose-machines#n1_machines}},
  note = {Accessed: 2023-05-08}
}

@misc{a2,
  title = {A2 Instance Configuration},
  howpublished = {\url{https://cloud.google.com/blog/products/compute/a2-vms-with-nvidia-a100-gpus-are-ga/}},
  note = {Accessed: 2023-05-08}
}

@misc{n1bandwidth,
  title = {N1 Instance Cross-node Bandwidth},
  howpublished = {\url{https://cloud.google.com/compute/docs/network-bandwidth}},
  note = {Accessed: 2023-05-08}
}

@article{megatron,
  author       = {Mohammad Shoeybi and
                  Mostofa Patwary and
                  Raul Puri and
                  Patrick LeGresley and
                  Jared Casper and
                  Bryan Catanzaro},
  title        = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
                  Model Parallelism},
  journal      = {CoRR},
  volume       = {abs/1909.08053},
  year         = {2019},
  url          = {http://arxiv.org/abs/1909.08053},
  eprinttype    = {arXiv},
  eprint       = {1909.08053},
  timestamp    = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1909-08053.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{korthikanti2022reducing,
  title={Reducing activation recomputation in large transformer models},
  author={Korthikanti, Vijay and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2205.05198},
  year={2022}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{ren2021zero,
  title={ZeRO-Offload: Democratizing Billion-Scale Model Training.},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={USENIX Annual Technical Conference},
  pages={551--564},
  year={2021}
}

@inproceedings{zheng2022alpa,
  title={Alpa: Automating Inter-and $\{$Intra-Operator$\}$ Parallelism for Distributed Deep Learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={559--578},
  year={2022}
}

@article{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{narayanan2019pipedream,
  title={PipeDream: Generalized pipeline parallelism for DNN training},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={1--15},
  year={2019}
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}


@inproceedings{
liu2022exact,
title={{EXACT}: Scalable Graph Neural Networks Training via Extreme Activation Compression},
author={Zirui Liu and Kaixiong Zhou and Fan Yang and Li Li and Rui Chen and Xia Hu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=vkaMaq95_rX}
}