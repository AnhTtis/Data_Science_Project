\section{When is MOM more beneficial?}
In this section, we present two key findings regarding the optimal utilization of MOMs for model training. Subsequently, we conduct experiments to validate and substantiate these findings.

\textbf{MOMs are more beneficial for larger models}. In our study, we concentrate on comparing models within the same family (e.g., different layers of Bert models, GPT models with varying numbers of encoder/decoder blocks, etc.), as models from different families are not directly comparable. As models grow larger in size, the likelihood of the original training being underutilized increases. Therefore, the application of MOM becomes more advantageous as it enables an increase in batch size and improves hardware utilization. 
% A detailed explanation is provided in the appendix, where we outline the complete reasoning.

To validate this implication, we conducted experiments by varying the depth and width of ResNet and GPT-3 models on V100 GPUs. The results, as depicted in Figure~\ref{fig:impl}, demonstrate that when the model is shallow enough (with fewer than 280 layers for ResNet or fewer than 18 layers for GPT-3) or narrow (with a hidden size smaller than 248 for ResNet or 1500 for GPT-3), the application of MOMs actually slows down the training process. However, as the model becomes deeper or wider, MOMs start to accelerate the training. In the most extreme case, the original training fails to execute even with a batch size of one, emphasizing the necessity of applying MOMs to enable single-GPU training. These findings provide evidence supporting the claim that the benefits of MOMs are more pronounced with increasing model depth and width.

\textbf{MOMs are more beneficial for multi-GPU data parallel training.} The intuition behind this is that communication makes MOMs less expensive. Communication and synchronization across multiple devices result in a substantially higher fixed execution cost. 
% This cost increases with the synchronous overhead when more GPUs are utilized. Consequently, as discussed above, when both the original training and MOM fully utilize the GPU, satisfying the inequality $\frac{\beta_{0} - \beta_{MOM}}{C_B^{MOM} - C_B^{0}} > \frac{M-\delta}{b}$ becomes easier, as the right side of the inequality decreases with an increase in $b$. Furthermore, the left side remains unchanged since neither the model nor the memory optimization method is altered.

We compare the maximum throughput ratio between gradient checkpointing and the original training on Bert-Large and Swin-Large on distributed data-parallel training with 1, 2, 4, 8 V100 GPUs on a single machine. As shown in Figure~\ref{fig:impl}, the ratio increases by adding more GPUs. Specifically, on Bert-Large, gradient checkpointing slows down training on a single GPU setting. However, as we increase the number of GPUs, gradient checkpointing achieves a higher maximum throughput than the original training, and can speed up the training process by 1.7Ã— when there are 8 GPUs.

