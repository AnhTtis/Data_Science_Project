\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/metric.pdf}
\caption{Evaluation metrics for MOMs.}
\label{fig:ill_all}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{figures/model.pdf}
\caption{Training throughput of different MOMs when varying model depth and width. Batch size is fixed across different training settings. For ResNet, batch size=64. For Bert, batch size=24. The cross indicates an out-of-memory error.}
\label{fig:model_size}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{figures/case_study.pdf}
\caption{Training throughput of different MOMs when varying batch size on a single V100. The cross indicates an out-of-memory error.}
\label{fig:case_study_all}
\end{figure*}

\section{MOM Application Scenario and Evaluation Metrics}

% \sid{I cut "Existing Evaluation Metrics and Problems" because I think it can be summarized in a sentence or two}
Previous evaluations of various MOMs can be categorized as follows:
(1) \citet{chen2021actnn, peng2020capuchin, pmlr-v162-liu22v} compare the largest batch size the MOM enables given the same memory budget as shown in Figure~\ref{fig:ill_all}(a), which is limited by the memory capacity of the hardware.
(2) Similarly, \citet{chen2016training, chen2021actnn, pmlr-v162-liu22v} compare the largest model the MOM allows given the same batch size and memory budget, using the largest depth and width of the model as a measure of efficacy.
(3) \citet{kirisame2020dynamic, chen2021actnn, huang2020swapadvisor, jain2020checkmate, pmlr-v162-liu22v} compare the performance overhead of a MOM with the original training under different memory thresholds, given the same batch size on a specified model, as shown in Figure~\ref{fig:ill_all} (b).

However, there is an existing gap between these evaluation metrics and practical use. For those wanting faster training, a larger batch size may not result in higher throughput due to the MOM's overhead. For those wanting to train larger models, looking at just the largest trainable model is not sufficient as it must be trained using a realistic throughput, which the MOM hinders. Thus we argue that MOMs should be evaluated in the context of their application. We list three main scenarios in which practitioners may find it useful to employ MOMs.

% Without taking the scenarios into consideration, different evaluation metrics might give conflicting results. For example, as shown in Section~\ref{sec:experiment}, maximum batch size and maximum throughput lead to completely opposite conclusions: MOMs are promising when evaluated using maximum batch size as the metric, while training is actually slowed down when evaluated using maximum throughput, which argues that the majority of MOMs are actually detrimental to training.

% To fix the issue, we argue that MOMs can potentially benefit the training process in the following three main scenarios. We list the benefits, fixed training variables, and corresponding evaluation metrics below:

\textbf{Training larger models: }\textit{Fixed hardware, evaluated with the maximum model size that satisfies a minimum throughput threshold}: The first scenario involves a fixed hardware setting with the goal of training a larger model within the memory limit. 
Under this scenario, evaluating based on the model size is essential. However, it is equally important to consider the training speed. To address this, we propose a new evaluation metric: the maximum model size that satisfies a minimum throughput threshold. This metric takes into account both the model size expansion achieved through MOM and the practical feasibility of training in terms of throughput.


\textbf{Faster training: }\textit{Fixed hardware, evaluated with the maximum throughput}: The second scenario involves a fixed hardware setting and aims to enable a larger batch size, reducing the number of iterations required to train a model while increasing device utilization and speeding up the training process.
In this case, the evaluation metric to be used is the maximum throughput. As demonstrated in \autoref{sec:experiment}, throughput increases with larger batch sizes. Therefore, the maximum throughput is attained when the model is trained using the maximum batch size. This metric provides a measure of the highest achievable training speed, indicating the effectiveness of MOM in improving the overall throughput of the training process.

\textbf{Cheaper training: }\textit{Fixed training setting, evaluated with the cost-latency trade-off curve}: Finally, the third scenario involves a fixed training setting, but the hardware configuration is varied to find the best cost/time trade-off with and without MOM. In this setting, the monetary cost of training a model is directly proportional to the number of GPUs employed and the time taken for training. By leveraging MOM, it is possible to reduce the required number of GPUs, resulting in a potentially lower total training cost, despite an extended training time. Therefore, when the model, batch size, and training setting are held constant, a trade-off frontier is established, as illustrated in \autoref{fig:ill_all} (c). 
% \sid{I want to remove the following sentence since its not really explained well right now. This is basically a comparison between MOMs. Not a comparison between MOM and no MOM.}
% If the MOM is successful in moving the trade-off curve closer to the origin, it is deemed advantageous. 
The practitioner's preferences (time vs cost) determine what point on the frontier is chosen for training.