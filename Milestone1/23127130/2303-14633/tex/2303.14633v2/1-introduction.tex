\section{Introduction}
To improve accuracy and generalization, deep neural networks (DNNs) have rapidly grown in size~\cite{brown2020language, chowdhery2022palm, touvron2023llama}. The further growth of these models is constrained by hardware limitations. In particular, the limited memory on GPUs restricts how large a model we can train. 
%This limitation arises from the fact that the storage requirements for feature maps and their gradients increase linearly with the depth (number of layers) and quadratically with the width (hidden state dimension) of the network.
In response, researchers have developed various memory optimization methods (MOMs) to reduce the memory footprint of training DNNs. Popular MOMs include gradient checkpointing, swapping, and activation reduced training (ACT). We describe these in detail in \autoref{sec:different-moms}.
% Gradient checkpointing, swapping, and activated reduced training (ACT) are the three main MOMs to reduce activation memory. Gradient checkpointing~\citep{chen2016training} discards intermediate activations during forward propagation and recalculates them during the backward pass. Swapping~\citep{peng2020capuchin, huang2020swapadvisor, wang2018superneurons, kirisame2020dynamic} offloads data from GPU memory to the CPU to avoid exceeding the device memory limit. Activation compressed training (ACT) condenses stored tensors during forward propagation to minimize saved activation overhead.
% \jianfei{might need to differentiate MOMs for model/optimizer/gradients (e.g. ZerO-3) and MOMs for activations. I think you are targeting the latter?}


In this study, we conduct a survey of MOMs used in DNN training to understand when MOMs prove advantageous. Specifically, we aim to address several key questions: Does the application of these techniques truly improve the training process? Which technique achieves the best time-memory trade-off? Are there MOMs that demonstrate superior performance on particular architectural configurations? 
% \jianfei{State the problem you want to solve. What are the challenges? You need to explain the challenges more clearly. They are not apparent. For example, apparently all MOMs paper claim themselves to be practically useful, and this is the idea what the readers have. Why do you raise the question what they might not truly improve the training process then? This needs elaboration.}
% By answering these questions, we aim to provide an extensive analysis of MOMs, highlighting their respective strengths and weaknesses. Ultimately, our aim is to offer valuable insights into how these methods can be effectively employed to enhance the training of DNNs.

In short, consistent with previous literature, we find that MOMs help in two ways. First, they reduce the peak memory compared to standard training for a fixed batch size. Second, they enable larger maximum batch sizes compared to standard training for a given hardware configuration. 
% Unfortunately, as we demonstrate, MOMs deliver mixed results when placed in the context of end-to-end training. \jianfei{what is end-to-end training?}
However, our central finding is that it is difficult to answer our motivating questions with existing ways of evaluating MOMs.
Most of these issues stem from the absence of standard networks, metrics, and experimental practices. 
Crucially, previous evaluation metrics only consider a single aspect of training and ignore the application scenarios of MOMs. Some of these metrics even conflict with each other.
% \alvin{either tone this down or give evidence}
For example, as shown in \autoref{sec:experiment}, synchronous swapping performs better than gradient checkpointing on the Swin-Large model when evaluated with maximum batch size, but gradient checkpointing is more efficient when considering the execution overhead at a given batch size compared with the original training. 

% Evidently, the effectiveness of MOM in the end-to-end setting is not well understood. 
% Furthermore, most prior studies compare MOMs within the same category (e.g., different gradient checkpointing methods), but only a few compare them across different categories (e.g., gradient checkpointing vs swapping). Experiments are frequently conducted on specific hardware configurations, but the applicability of different methods may vary depending on the task and hardware. Therefore, a comprehensive evaluation is imperative to determine the optimal MOM or combination of MOMs for a given task and hardware setup.

In this paper, we provide a comprehensive overview of various MOMs and outline their respective application scenarios. Recognizing the diversity of these scenarios, we propose using distinct evaluation metrics tailored to each scenario. By employing these evaluation metrics, we conduct a thorough assessment of numerous mainstream network architectures. Surprisingly, our findings reveal that MOMs only demonstrate benefits in particular settings. 
Additionally, we provide a detailed discussion of the conditions under which applying MOMs is advantageous, thereby aiding future researchers in making informed decisions regarding the selection and adoption of MOMs.



In summary, the paper makes the following contributions: (1) We provide a summary of MOM application scenarios and propose tailored evaluation metrics for each scenario. (2) We conduct a comprehensive evaluation of popular MOMs in widely-used frameworks, analyzing their performance in both single GPU and distributed settings.
% \begin{itemize}
%     \vspace{-1em}
%     \item We provide a summary of MOM application scenarios and propose tailored evaluation metrics for each scenario.
%     \vspace{-1em}
%     \item We conduct a comprehensive evaluation of popular MOMs in widely-used frameworks, analyzing their performance in both single GPU and distributed settings.
%     % \item We propose a simple performance model that enables quantitative analysis of MOM effectiveness to identify scenarios in which MOMs are most beneficial and supports decision-making regarding MOM adoption.
% \end{itemize}

% Together, these contributions aim to provide a more comprehensive and systematic understanding of MOMs and their effectiveness, as well as offer insights into their practical implementation for improving deep learning performance.
