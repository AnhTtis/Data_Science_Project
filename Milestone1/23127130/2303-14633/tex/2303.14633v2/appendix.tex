% \section{Appendix}
% \label{sec:appendix}
% \subsection{Related Work}
% We list more related work about different techniques to reduce training memory here.

% \textbf{Gradient Checkpointing}
% \citet{jain2020checkmate, kumar2019efficient} formalize the problem of trading computation for memory as a linear programming problem to generate a near-optimal schedule. To handle not only static but also dynamic computation graphs, \citet{kirisame2020dynamic} proposes a greedy online algorithm for heuristically checkpointing arbitrary models.

% \textbf{Activation Compressed Training (ACT)} ACT has been successfully applied to a variety of models such as convolutional neural networks, transformers, and graph neural networks \cite{pmlr-v162-liu22v, pan2021mesa, liu2022exact}. In exchange for lower memory consumption, ACT increases computation and may negatively impact the model's accuracy due to lossy compression.
 
% \textbf{Swapping}
% \cite{wang2018superneurons, huang2020swapadvisor, peng2020capuchin} explore the search space of jointly optimizing operator scheduling, memory allocation, and swap decisions for static graphs. 
% Swapping can be applied to memory consumption other than activations. ZeRO-Offload~\cite{ren2021zero} offloads gradients and optimizer states to the CPU, but stores parameters and activations on the GPU.
% Recent work~\citep{beaumont2021efficient} also explores combining gradient checkpointing and offloading. 


% \textbf{Distributed Training}
% Distributed training is another way of reducing the memory consumption for each device. Recent research has proposed various methods for partitioning the model or different batches of data across available hardware, which typically refers to 3D parallelism. Data parallelism~\cite{krizhevsky2014one} involves replicating the model across machines, with each device responsible for training a portion of the data. In contrast, tensor parallelism~\cite{megatron} involves partitioning a single operator across different machines, requiring each device to communicate with others to synchronize and obtain final results. Pipeline parallelism~\cite{huang2019gpipe, narayanan2019pipedream} divides the computation graph into several blocks, each containing multiple succinct layers, with each device responsible for computing a single block. Communication occurs at the block boundary.
