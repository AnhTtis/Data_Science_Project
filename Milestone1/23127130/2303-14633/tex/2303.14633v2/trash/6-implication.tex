\section{Implication}
In this section, we first list interesting conclusions derived from the performance model. Then we verify the conclusion with experiments.

\subsection{When is MOM beneficial for increasing maximum throughput?}
\begin{em}
    \begin{enumerate}
        \item MOM is beneficial when the original training is underutilized.
        \item MOM is beneficial when $\frac{\beta_{0} - \beta_{MOM}}{C_B^{MOM} - C_B^{0}} > \frac{M-\delta}{b}$.
        \item MOMs are more beneficial for larger models.
        \item MOMs are more beneficial for multi-GPU data parallel training.
    \end{enumerate}
\end{em}

\textbf{MOMs are more beneficial for larger models}. In our study, we concentrate on comparing models within the same family (e.g., different layers of Bert models, GPT models with varying numbers of encoder/decoder blocks, etc.), as models from different families are not directly comparable. As models grow larger in size, the likelihood of the original training being underutilized, i.e., running out of memory before reaching the saturation point, increases. Therefore, the application of MOM becomes more advantageous as it enables an increase in the batch size and improves hardware utilization. A detailed explanation is provided in the appendix, where we outline the complete reasoning.

To validate this implication, we conducted experiments by varying the depth and width of ResNet and GPT3 models on V100 GPUs. The results, as depicted in Figure~\ref{fig:impl}, demonstrate that when the model is shallow enough (with fewer than 280 layers for ResNet or fewer than 18 layers for GPT3) or narrow (with a hidden size smaller than 248 for ResNet or 1500 for GPT3), the application of MOMs actually slows down the training process. However, as the model becomes deeper or wider, MOMs start to accelerate the training. In the most extreme case, the original training fails to execute even with a batch size of one, emphasizing the necessity of applying MOMs to enable single-GPU training. These findings provide evidence supporting the claim that the benefits of MOMs are more pronounced with increasing model depth and width.

\textbf{MOMs are more beneficial for multi-GPU data parallel training.} The intuition behind the implication is that communication makes MOMs less expensive. Communication and synchronization across multiple devices result in a substantially higher fixed execution cost $b$. 
This cost increases with the synchronous overhead when more GPUs are utilized. Consequently, as discussed above, when both the original training and MOM fully utilize the GPU, satisfying the inequality $\frac{\beta_{0} - \beta_{MOM}}{C_B^{MOM} - C_B^{0}} > \frac{M-\delta}{b}$ becomes easier, as the right side of the inequality decreases with an increase in $b$. Furthermore, the left side remains unchanged since neither the model nor the memory optimization method is altered.

We compare the maximum throughput ratio between gradient checkpointing and the original training on Bert-Large and Swin-Large on distributed data-parallel training with 1, 2, 4, 8 V100 GPUs. As shown in Figure~\ref{fig:impl}, the ratio increases by adding more GPUs. Specifically, on Bert-Large, gradient checkpointing slows down training on a single GPU setting. However, as we increase the number of GPUs, gradient checkpointing achieves a higher maximum throughput than the original training, and can speedup the training process by 1.7Ã— when there are 8 GPUs.


