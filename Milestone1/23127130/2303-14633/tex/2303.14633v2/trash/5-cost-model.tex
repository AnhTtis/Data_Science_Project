\section{Performance Modeling}
In this section, we introduce a simple but effective performance model to quantitatively model and explain the experiment results in the last section.
The performance model provides predictions of training performance by estimating training throughput and evaluating the trade-off between cost and latency. 
In the next section, we will utilize the model to empower practitioners in making well-informed decisions about the optimal MOMs for their specific requirements. 

\subsection{Batch Latency Modeling}
\begin{equation}
    L(B, K) = max(a, B * \frac{C_B}{p(K)}) + b
\end{equation}
$max$ here tries to model the hardware utilization: $a$ is a fixed time consumption when the hardware is underutilized. As the batch size $B$ increases, the GPU becomes fully utilized, resulting in a linear increase in batch latency proportional to the batch size $B$. The incremental latency cost for each batch size increment is $\frac{C_B}{p(K)}$, where $C_B$ represents the floating-point operations required by the model per batch size, and $p(K)$ denotes the effective FLOPS (floating point operations per second) provided by the hardware. Notice here $p(K)$ is dependent on the number of devices $K$, as increasing the number of devices will also increase the effective FLOPS available.
Additionally, $b$ denotes a fixed execution cost that remains constant regardless of the batch size. This execution cost typically encompasses tasks such as weight updates and framework overhead. 

\subsection{Throughput Modeling}
Throughput $T$ is defined as the batch size divided by the batch latency:
\begin{equation}
    T(B, K) = \frac{B}{L(B, K)} = \frac{B}{max(a, \frac{B * C_B}{p(K)}) + b}
\end{equation}
\begin{equation}
    T(B, K) = 
    \begin{cases}
        \frac{B}{a+b}, B \leq B_s \\
        \frac{p(K)}{C_B + \frac{b}{B}}, B > B_s
    \end{cases}
\label{eq:tpt}
\end{equation}
We introduce the concept of the saturation point, denoted as $B_s=\frac{a*p}{C_B}$. 
As observed from the formula, when the batch size is smaller than the saturation point, the throughput exhibits a linear relationship with the batch size. However, when the batch size surpasses the saturation point, further increasing the batch size leads to a continued increase in throughput, albeit with diminishing returns. In essence, the saturation point represents the batch size beyond which additional increments yield diminishing improvements in throughput. 

\textbf{Maximum Throughput Modeling} 
As can be seen from Equation~\ref{eq:tpt}, throughput is a monotonic function of batch size. Therefore, the maximum throughput is achieved with the maximum batch size:
\begin{equation}
    max(T) = T(B_{max})
\label{eq:maxtpt}
\end{equation}
In the first application scenario (\textit{Fixed hardware, larger maximum throughput}), the model ($C_B$) and hardware ($p(K)$) remain the same for a given MOM so that we can get those parameters through simple profiling. Moreover, memory consumption is a linear function of batch size, we can easily get the maximum batch size and plugin in Equation~\ref{eq:maxtpt} to get the maximum throughput. 

\subsection{Maximum Model Size Modeling}
We first introduce a model amplification factor $\gamma$, which denotes the multiplication factor of the floating point operations per batch size when the model size is increased. The throughput $T$ can be rewritten as below:
\begin{equation}
    T(B, \gamma) = \frac{B}{max(a, B * \frac{\gamma C_B}{p(K)}) + b(\gamma)}
\end{equation}
\begin{equation}
    T(B, \gamma) = 
    \begin{cases}
        \frac{B}{a+b(\gamma)}, B \leq B_s \\
        \frac{p(K)}{\gamma C_B + \frac{b(\gamma)}{B}}, B > B_s
    \end{cases}
\end{equation}
Where $B_s = \frac{a * p}{\gamma C_B}$, the saturation point shifts to a smaller number as the model gets larger. 
In the second application scenario (\textit{Fixed hardware, larger model}), the batch size ($B$) and hardware $p(K)$ remain fixed. Assuming the fixed execution cost is a linear function of the amplification factor $b(\gamma)=\gamma * b$. Through simple profiling, we can fit the relationship between batch latency and model size, therefore modeling the curve between model size and throughput.

\subsection{Cost Latency Trade-off Curve Modeling}
Likewise, the cost associated with training a single batch can be calculated as the product of the GPU price ($price$) and the number of GPUs utilized ($K$), multiplied by the batch latency ($L(B, K)$). This relationship can be expressed by the following formulation.
\begin{equation}
    Cost(B, K) = L(B, K) * K * price 
\end{equation}
where,
\begin{equation}
    L(B, K) =  \max(a, B * \frac{C_B}{K * p}) + b
\end{equation}
In the above equation, we expand $p(K) = K * p$, where $p$ is the computation capability of each GPU. When GPU is fully utilized, the cost formula simplifies to 
\begin{equation}
    Cost(B, K) = \frac{L(B, K)}{L(B, K) - b} * \frac{B * C_B}{p} * price
\label{eq:cost}
\end{equation}
In the third application scenario (\textit{Fixed training setting, better cost/time trade-off}), the batch size ($B$), hardware computation capability ($p$), and model complexity ($C_B$) remain fixed, the term $\frac{B*C_B}{p}$ becomes a constant. Consequently, the aforementioned formula mathematically represents the trade-off curve between training cost and training latency.
