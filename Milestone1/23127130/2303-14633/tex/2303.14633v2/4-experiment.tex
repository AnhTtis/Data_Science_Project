

\section{How good are existing MOMs, really?}
\label{sec:experiment}

% \subsection{From System Perspective, Low-Precision Training is Clearly a Win}
% \begin{enumerate}
%     \item fp32, fp16 on Bert, GPT (fp8?)
% \end{enumerate}
\begin{figure}
\centering
\includegraphics[width=0.96\linewidth]{figures/method.pdf}
\caption{MOMs evaluated in this paper.}
\label{fig:case_study_method}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=0.8\linewidth]{figures/case_study2.pdf}
\caption{Case Study on Bert.}
\label{fig:case_study2}
\end{figure*}

\subsection{Experiment Setup}
\textbf{Evaluated Models.} We evaluate different MOMs on both convolutional neural networks (CNN) and transformer-based models. For CNNs, we evaluate ResNet-50~\cite{he2016deep} and Wide-ResNet-50~\cite{zagoruyko2016wide}. For transformer-based models, we test both language tasks (Bert~\cite{devlin2018bert}, GPT~\cite{radford2021learning}) and vision tasks (Swin-Large~\cite{liu2021swin}).

\textbf{Evaluated MOMs.}  We list all evaluated MOMs and their combinations in \autoref{fig:case_study_method}. For the state-of-art activation compressed training, we utilize the ActNN~\cite{chen2021actnn} quantizer, which quantizes each element of the activation layer into four bits. For gradient checkpointing on CNNs, we choose DTR~\cite{kirisame2020dynamic}, a greedy online algorithm that evicts activations based on different heuristics when the peak memory exceeds the hardware limit. For transformer-based models we use the widely-adopted checkpointing policy that checkpoints the input of each transformer block. We implement synchronous swapping in PyTorch ourselves. Additionally, we test the combinations of different MOMs as shown in \autoref{fig:case_study_method}.


\subsection{Maximum Model Size that Satisfies a Minimum Throughput Threshold}
In this section, we try to answer the question of whether MOMs can enable training larger models without excessive overhead. We investigate the maximum model size that different MOMs can train on a single V100 GPU. By fixing the batch size, we record the training throughput of ResNet/Bert models with varying depths/widths. As depicted in \autoref{fig:model_size}, we observe a decrease in throughput as the model size increases across all training settings, as larger models demand more computation capability. 

% On average, MOMs can increase the maximum model size by \lily{TODO}X, \lily{TODO}X, \lily{TODO}X, \lily{TODO}X. 
% While different MOMs demonstrate varying levels of support for increasing the model depth/width on ResNet, they display similar abilities on Bert. This can be attributed to \lily{TODO}.


\begin{figure*}[t]
\begin{center}
    \includegraphics[width=\linewidth]{figures/implication.pdf}
\end{center}
\caption{(a), (b) show the maximum throughput ratio of training with 1, 2, 4, 8 V100 GPUs on Bert-Large and Swin-Large. (c) - (f) shows the maximum throughput of training ResNet and GPT with different depths and widths.}
\label{fig:impl}
\end{figure*}

\subsection{Maximum Throughput on Various Models}
In this section, we try to determine if MOMs can enable faster training by increasing the maximum throughput.
We test the throughput of different training settings by varying the batch size on a single V100.
As shown in \autoref{fig:case_study_all}, the throughput of most training instances increases with batch size, although to varying degrees. The only exception is DTR on Wide-ResNet-50, where the maximum throughput drops from 80.6 records/s to 65.1 records/s as batch size increases from 160 to 200. This is because DTR determines the rematerialization policy based on the memory limit dynamically, and evicts more activations as batch size increases and memory budget decreases.

All training instances display diminishing returns as batch size increases. For the original training on Bert-Large and Swin-large, the training process is aborted due to out-of-memory error before reaching a throughput plateau, whereas all other training instances reach a plateau where increasing the batch size barely increases training throughout.

Lastly, when compared to the original training, all MOMs increase the maximum batch size. On average, different MOMs can increase the maximum batch size by 2.6$\times$, 2.8$\times$, 11.2$\times$, 4.5$\times$, 1.8$\times$ for the five evaluated models, which aligns with previous work.

However, when considering the purpose of increasing the maximum throughput as mentioned above, only gradient checkpointing and quantization on Swin-Large increase the maximum throughput by 8.8\% (76.86 vs 70.65) and 6.7\% (75.35 vs 70.65) respectively. On average, the maximum training throughput are only 43.6\%, 46.1\%, 50.6\%, 76.6\%, 64.4\% of the original on five evaluated models when MOMs are applied.
In this case, the two metrics (maximum batch size vs. maximum throughput) lead to completely opposite conclusions: MOMs are promising when evaluated using maximum batch size as the metric, while training is actually slowed down when evaluated using maximum throughput, which argues that the majority of MOMs are actually detrimental for faster training.

\subsubsection{Case Study on Bert}
Next, we ask if MOMs can be used to improve {\em both} batch size and throughput. From our results shown in \autoref{fig:case_study_all}, training with quantization and gradient checkpointing are the most promising as they achieve the highest maximum throughput among all MOMs. 
Therefore, we apply these two MOMs on various Bert model sizes to see if they improve the maximum throughput. 
We first conduct experiments on Bert models with 36 layers and 48 layers.
As shown in \autoref{fig:case_study2}, for Bert with 36 layers, the maximum training throughput with gradient checkpointing marginally exceeds that of the original. For the Bert 48 model, both training with quantization and checkpointing improve throughput, improving maximum throughput by 80.1\% (38.7 vs 69.7) and 32.3\% (38.7 vs 51.2) respectively.


\subsection{Cost Time Trade-off}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{figures/cost-t-tradeoff.png}
\caption{Training cost (money cost to run iteration) and batch latency trade-off curve of different MOMs on GPT 6.7B. All training settings use model parallelism. GC: Naive gradient checkpointing. SGC: Selective gradient checkpointing. SP: sequence parallel. Exact: Original training without MOM. We also test methods combination (SGC+SP, GC+SP).}
\label{fig:tradeoff}
\end{figure}
In this experiment, we seek to know if applying MOM can reduce the training cost.
We test the GPT 6.7B model on a single a2-megagpu-16g GCP node~\cite{a2}, with 16 A100 GPUs connected with NVLink, 96 vCPUs, and 1360 GB memory. We tested gradient checkpointing (GC) that only recomputes the input of each transformer block, selective gradient checkpointing (SGC)~\cite{korthikanti2022reducing} that only recomputes the activations within the attention layers, sequence parallel~\cite{korthikanti2022reducing} and their combination provided with Megatron~\cite{megatron}. For all evaluated settings, we apply the same tensor parallelism setting.

As shown in \autoref{fig:tradeoff}, only GC and GC+SP can fit the training on 4 GPUs due to naive gradient checkpointing that only stores the input of each transformer block and greatly reduces stored memory. However, using GC is ineffective in terms of the total cost of training a single iteration, as it has a higher per-iteration cost than not applying any MOM on 8 GPUs.
Although using GC can reduce the total number of nodes, the overall monetary cost remains higher than the original training due to its significant overhead. 

Moreover, we found that using only sequence parallel provides the best latency-cost tradeoff compared to other training settings, as it does not introduce computation or communication overhead when applied with tensor parallelism. It divides the computation across the sequence dimension, reducing the duplicated computation and speeding up the overhead training with the same hardware. 


% \subsection{Gradient Accumulation}
% \lily{TODO? Compare gradient accumulation with MOM?}

% \subsection{Hardware Variation}
% \lily{What to show here??}