\section{Overview of Memory Optimization Methods}
\label{sec:related-work}
\subsection{Memory Consumption in Model Training}
% \sid{Removed the figure of memory consumption}
The bulk of memory usage during training falls under three categories: 
(1) \emph{activations}, which are intermediate tensors stored during the forward pass that are needed to compute gradients during the backward pass. 
(2) \emph{model related objects} that store the model's state, such as model parameters (e.g., weight matrices), optimizer states, and gradient accumulators.
(3) \emph{workspace memory}, which includes temporary storage for calculations. Workspace memory will be released once the operator finishes execution and typically consumes only a small fraction of memory.
\subsection{Different MOMs Evaluated in this Paper}
\label{sec:different-moms}
This study mainly focuses on memory optimization methods (MOMs) from a system perspective. Apart from MOMs, other approaches such as low-rank kernels~\cite{katharopoulos2020transformers, peng2021random} and leveraging sparsity~\cite{kitaev2020reformer, daras2020smyrf} can lower memory consumption. These alternative methods fall under the algorithmic perspective, often involving modifications to the kernels or network architecture.
% , which may affect the model's accuracy \sid{Doesn't ACT do this?}. 
Our current research is primarily focused on exploring system-level memory optimization methods.
% 1. introduce different MOMs.
% 2. Categorize different MOMs based on trade-offs.

% \label{sec:related-work}
% \begin{table}[t]
%     \centering
%     \begin{tabular}{c|c|c|c|c|c|c|c}
%          Method                 & Gradient & Activation    & Parameter & Optimizer States & Computation & Communication & Accuracy\\
%          \toprule
%          Gradient Checkpointing & $\upchi$ & $\checkmark$  & $\upchi$  & $\upchi$   & $\uparrow$ & $\uparrow$             & -    \\
%          \hline
%          Activation Compressed 
%          Training               & $ ? $ & $\checkmark$  &  $\upchi$  & $\upchi$ & $?$  &  $?$           & $\downarrow$ \\
%          \hline
%          Mixed Precision Training & $\checkmark$ & $\checkmark$& $\upchi$ &  $\upchi$  &$\downarrow$ & $?$            & $\downarrow$\\
%          \hline
%           Offloading     & $\checkmark$ &$\checkmark$   & $\checkmark$   & $\checkmark$ & -           & $\uparrow$   & -           \\
%          \hline
%          3D Parallelism         & $\checkmark$&$\checkmark$   &  $\checkmark$   &  $\checkmark$               & $\downarrow$           & $\uparrow$   & -            \\
%          \hline
%          Efficient Optimizer    & $\checkmark$ &$\checkmark$       & $\checkmark$& $\checkmark$    & $\downarrow$           &  $\uparrow$           & -             \\
%          \bottomrule
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:mom_cat}
% \end{table}

\textbf{Gradient Checkpointing}~\cite{chen2016training} trades computation for memory by dropping some of the activations during the forward pass from memory and recomputing them during the backward pass.
\citet{jain2020checkmate, kumar2019efficient} formalize the problem of trading computation for memory as a linear programming problem to generate a near-optimal schedule. To handle not only static but also dynamic computation graphs, \citet{kirisame2020dynamic} proposes a greedy online algorithm for heuristically checkpointing arbitrary models.

\textbf{Activation Compressed Training (ACT)} \cite{pmlr-v162-liu22v, pan2021mesa, liu2022exact} is a technique that compresses the training activation during the forward pass, and then decompresses the saved activation during back propagation to calculate the gradient. 
It has been successfully applied to a variety of models such as convolutional neural networks, transformers, and graph neural networks \cite{pmlr-v162-liu22v, pan2021mesa, liu2022exact}. In exchange for lower memory consumption, ACT increases computation and may negatively impact the model's accuracy due to lossy compression.
 
\textbf{Swapping} Swapping offloads activations to external memory (e.g., CPU memory), which comes at the cost of transferring data to another storage and can increase execution time.
\cite{wang2018superneurons, huang2020swapadvisor, peng2020capuchin} explore the search space of jointly optimizing operator scheduling, memory allocation, and swap decisions for static graphs. 
Swapping can be applied to memory consumption other than activations. ZeRO-Offload~\cite{ren2021zero} offloads gradients and optimizer states to the CPU, but stores parameters and activations on the GPU.
Recent work~\citep{beaumont2021efficient} also explores combining gradient checkpointing and offloading. 


% \begin{table*}[t]
%     \centering
%     \begin{tabular}{c|c|c|c|c|c|c|c}
%          Method                 & Gradients & Activations    & Parameters & Optimizer States \\
%          \toprule
%          Gradient Checkpointing & {\sffamily X} & $\checkmark$  & {\sffamily X}  & {\sffamily X} \\
%          \hline
%          Activation Compressed 
%          Training               & $ ? $ & $\checkmark$  &  {\sffamily X}  & {\sffamily X} \\
%          \hline
%          Mixed Precision Training & $\checkmark$ & $\checkmark$& {\sffamily X} &  {\sffamily X} \\
%          \hline
%           Offloading     & $\checkmark$ &$\checkmark$   & $\checkmark$   & $\checkmark$ \\
%          \hline
%          3D Parallelism         & $\checkmark$&$\checkmark$   &  $\checkmark$   &  $\checkmark$ \\
%          \hline
%          Efficient Optimizer    & $\checkmark$ &$\checkmark$       & $\checkmark$& $\checkmark$           \\
%          \bottomrule
%     \end{tabular}
%     \caption{Caption}
% \end{table*}

% \begin{table*}[t]
%     \centering
%     \begin{tabular}{c|c|c|c|c|c|c|c}
%          Method   & Computation Cost & Communication Cost    & Accuracy \\
%          \toprule
%          Gradient Checkpointing &$\uparrow$ & $\uparrow$             & -     \\
%          \hline
%          Activation Compressed 
%          Training              & $?$  &  $?$           & $\downarrow$   \\
%          \hline
%          Mixed Precision Training &$\downarrow$ & $?$            & $\downarrow$\\
%          \hline
%           Offloading    & -           & $\uparrow$   & -           \\
%          \hline
%          3D Parallelism        & $\downarrow$           & $\uparrow$   & -   \\
%          \hline
%          Efficient Optimizer    & $\downarrow$           &  $\uparrow$           & -             \\
%          \bottomrule
%     \end{tabular}
%     \caption{Caption}
% \end{table*}


% \textbf{Efficient Optimizer} ZeRO-Optimizer~\cite{} is an extension upon data parallelism that partitions optimizer states, gradients, and parameters across data parallel ranks. ZeRO-Optimizer allows for a reduction in peak memory while maintaining the minimal communication volume of classical data parallelism. 