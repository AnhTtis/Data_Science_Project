\section{Training and Evaluation Details}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=16.7cm]{figures/image_condition.pdf}
\end{center}
\vspace*{-0.3cm}
\hspace{2cm} (a) Grilled salmon \hspace{1.8cm} (b) Crepe \hspace{2.2cm} (c) Omelette \hspace{2.1cm} (d) Burrito
\vspace*{-0.4em}
\caption{Image-conditioned open-vocabulary detection. Image queries are from novel classes and the sub-captions represent text corresponding to the image queries.}
\label{fig:image_conditioned}
\vspace*{-0.4cm}
\end{figure*}


We assess our method by conducting experiments on two widely used open-vocabulary object detection datasets, namely OV-COCO and OV-LVIS. Following the literature\,\cite{guopen2022vild, zang2022open}, we split the classes in MS-COCO into 48 base categories and 17 novel categories, where the remaining categories are not included since they do not belong to a synset in the WordNet hierarhiy\,\cite{zareian2021open}. Regarding OV-LVIS, we follow the setup of \cite{zareian2021open}, selecting 337 rare classes as novel categories and the remaining classes as common and frequent categories. Because of the difference in the number of object categories, OV-LVIS has a much larger number of objects to detect. Additionally, the number of object queries should be larger than the number of actual objects in an image. Therefore, the number of object queries is set to 300 and 1,500 for OV-COCO and OV-LVIS. 

\smallskip\smallskip
\noindent\textbf{Training Loss Function.}
\algname{} utilize the loss function from (Deformable) DETR\,\cite{carion2020end, zhu2020deformable}, which includes a detection head that generates a set of bounding boxes. We modify this for OVD such that the detection head only generates the boxes and class labels given by the class prompts. During the training phase, the number of class prompts can vary according to the number of visible base classes in the current mini-batch. 

Hungarian matching is employed to identify a bipartite matching between the predicted boxes ${\rm \hat{B}}$ and the ground-truth boxes ${\rm {B}}$. The training process basically involves three types of losses: a classification loss $\ell_{\rm cls}$, which is the focal loss\footnote{Multi-label classification is performed only for the classes given by the class prompts.}, a box distance loss $\ell_{\rm l1}$ and a generalized IoU loss $\ell_{\rm iou}$ for box regression as \,\cite{songvidt, zhu2020deformable}:
 \begin{equation}
\begin{gathered}
\ell_{\rm cls}(i) = -\text{log}~\hat{\rm {P}}_{\sigma(i),c_i},~~~\ell_{\rm  l1}(i) =  ||{\rm B}_{i}-\hat{\rm B}_{\sigma(i)}||_{1},\\
\ell_{\rm iou}(i) = 1 \!-\! \big( \frac{|{\rm B}_{i} \cap  \hat{{\rm B}}_{\sigma(i)}|}{|{\rm B}_{i} \cup  \hat{{\rm B}}_{\sigma(i)}|} - \frac{|{\sf {\rm B}}({\rm B}_{i}, \hat{{\rm B}}_{\sigma(i)}) \symbol{92} {\rm B}_{i} \cup  \hat{{\rm B}}_{\sigma(i)}| }{|{\sf {\rm B}}({\rm B}_{i}, \hat{{\rm B}}_{\sigma(i)})|} \big),
\end{gathered}
\end{equation}
where $c_i$ is the target base class and $\sigma(i)$ is the bipartite assignment of the $i$-th ground-truth box. Also, the embedding loss $\ell_{\rm embed}$ proposed by \cite{zang2022open} is used to distil the CLIP's knowledge for open vocabulary object detection. Therefore, the final objective of \algname{} is formulated as:
\begin{equation}
\ell = \lambda_{\rm cls}\ell_{\rm cls} + \lambda_{\rm l1}\ell_{\rm l1} + \lambda_{\rm iou}\ell_{\rm iou} + \lambda_{\rm embed}\ell_{\rm embed}, 
\label{eq:naive_roi}
\end{equation}
where $\lambda$s are the balancing parameters, where $\lambda_{\rm cls}=3, \lambda_{\rm l1}=5, \lambda_{\rm iou}=2$, and $\lambda_{\rm embed}=2$. 

\smallskip\smallskip
\noindent\textbf{Evaluation.}
In the testing phase, the class prompts are inclusive of all base and novel classes following the recent literature\,\cite{guopen2022vild, zang2022open, zhong2022regionclip, zhou2022detic, rasheedbridging}. For producing the final results, we apply RoI-based pruning to identify the RoIs with high object scores. Next, the classification results of the selected bounding boxes are ensembled with those from the ViT-based CLIP by Eq.\,\eqref{eq:ensemble}. 
\section{Potential Enhancement}

We discuss three possible ways to further enhance the potential of our framework. 

\smallskip\smallskip
\noindent\textbf{Prediction Ensemble.} In our ensemble method, we adopt an arithmetic mean approach as shown in Eq. \eqref{eq:ensemble}. However, this requires hyperparameter tuning to achieve the best performance, and the results are sensitive to the selection of $\alpha$ values, as indicated in Table \ref{tab:alpha}. The variation in performance highlights the importance of carefully choosing the hyperparameters for \algname{}, as suboptimal selections can have a significant impact on the overall performance. Although the simple ensemble approach provides a satisfactory detection accuracy for OV-COCO and OV-LVIS, we are of the belief that a more effective prediction ensemble strategy can be studied as future work to leverage both CLIP and a detection model in synergy.



\smallskip\smallskip
\noindent\textbf{Unrestricted Setup.} Our primary focus is on developing an end-to-end Transformer-based framework that is both efficient and effective. Therefore, we adopt a restricted setup in which external data is not permitted, as our aim is to investigate the potential of the framework itself in achieving optimal performance. Despite our restricted setup, we acknowledge that in real-world scenarios, large-scale external data are often available and their utilization can significantly enhance the zero-shot detection performance. We are confident that our framework can be extended in this direction, owing to its simple end-to-end encoding and decoding pipeline. However, we leave this as a potential avenue for future work, as our current focus is on investigating the framework's performance under the restricted setup.


\smallskip\smallskip
\noindent\textbf{Instance Segmentation.} A drawback of \algname{} is the considerable discrepancy in the performance between object detection and instance segmentation. While this is partly due to inheriting the limitations of using SOLQ\,\cite{dong2021solq} for end-to-end joint learning, the development of a more effective segmentation approach based on DETR will enhance the performance of our framework significantly. We leave this as an avenue for future research.


\section{Image Conditioned Detection}

One advantage of our framework is that it can detect objects using an {image query} instead of relying solely on the class name. This feature has the potential to identify the target object by using a cropped image of the object itself, even when we may not know the object's name. 

Figure \ref{fig:image_conditioned} shows the results of \algname{} when applying this image-conditioned object detection using four different image queries. Although their class names are not provided, \algname{} is capable of accurately localizing target objects that were not encountered during the training phase.  Additionally, \algname{} can recognize new classes even when the image query has a dissimilar shape of target objects, as depicted in Figure \ref{fig:image_conditioned}(a). These findings suggest that our algorithm is resilient in detecting objects using image queries for open-set scenarios.
%


%We conducted further analysis on the qualitative results depicted in Figure~\ref{fig:image_conditioned} when applying image queries. The findings demonstrate that \algname{} is capable of accurately localizing target objects that were not encountered during the training phase. Moreover, we observe that \algname{} can identify new classes even when the image query has a dissimilar shape of target objects, as illustrated in Figure~\ref{fig:image_conditioned}(a). These results suggest that our algorithm is robust in detecting objects using image queries.

% [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD] [TBD].

\begin{table}[t!]
\centering
\caption{Performance when using different attention layers for RoI-based masked attention on OV-LVIS }
\vspace*{-0.25cm}
\label{tab:blk_num}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{@{}lrrrr@{}}
\toprule 
{Attn. Layer Num.}& $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ \\
\midrule
% & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
$\mathrm{15^{th}}$ & 13.6 & 28.3 & 10.5 & 20.4 \\ 
$\mathrm{20^{th}}$ & 20.4 & 30.1 & 16.2 & 21.9 \\ 
\textbf{$\mathrm{24^{th}}$ (last layer)} & \textbf{29.4} & \textbf{33.0} & \textbf{23.1} & \textbf{24.2} \\
\bottomrule
\end{tabular}%
}
\label{tab:diff_layer}
\vspace*{-0.4cm}
\end{table}

\section{Supplementary Analysis}

We provide supplementary analysis on applying RoI-based masked attention to different attention layers in Appendix \ref{sec:diff_layer} and using different pre-trained ViT backbones with \algname{} in Appendix \ref{sec:diff_pretrain}.




\subsection{Pre-trained Weights.} 
\label{sec:diff_pretrain}

We analyze the impact of using different initial weights for the backbone, ImageNet and MAE, on the overall performance. As outlined in Table~\ref{tab:pretrained_model}, training the backbone using the MAE pre-trained weights yields better performance in dense prediction tasks such as object detection and instance segmentation, even under an open-vocabulary setup, compared to starting from the ImageNet pre-trained weights. This observation is consistent with previous research, such as \cite{li2022exploring}.
It is also worth noting that even when using the ImageNet pre-trained weights, \algname{} still outperforms the baseline\,(OV-DETR) with a much faster inference speed.
In conclusion, the choice of initial weight for the backbone plays a crucial role in the overall performance of the model, and using the MAE pre-trained weights as the starting point results in better performance.





\subsection{Different Layers for RoI-based Attention} 
\label{sec:diff_layer}

We investigate the impact of applying our RoI-based masked attention to different attention layers in the ViT-L/14 image encoder of CLIP. The results, as summarized in Table \ref{tab:diff_layer}, show that detection accuracy improves as the layer number increases, from the 15th to the 24th. Therefore, applying the technique to the last layer is the best design choice for open vocabulary detection with \algname{}.

\begin{table}[t!]
\centering
\caption{Performance when using different pre-trained ViT backbones on OV-LVIS.}
\vspace*{-0.25cm}
\label{tab:pretrained_model}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{@{}lrrrr@{}}
\toprule 
{Pretrained Model}& $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ \\
\midrule
% & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
ImageNet~\cite{deng2009imagenet} & 26.4 & 28.7 & 20.7 & 20.9\\
\textbf{MAE}~\cite{he2022masked} & \textbf{29.4} & \textbf{33.0} & \textbf{23.1} & \textbf{24.2}\\
\bottomrule
\end{tabular}%
}
\end{table}