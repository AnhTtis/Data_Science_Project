%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

\noindent\textbf{Object Detection with Transformers.} 
Transformers\,\cite{vaswani2017attention} have emerged as one of the leading architectures for computer vision. The main advantage of using Transformers is its weaker inductive bias than convolutional neural networks\,(CNN), leading to the state-of-the-art results in image classification benchmarks\,\cite{dosovitskiy2020image, liu2021swin, han2022survey}. The use of Transformers has recently been extended to object detection and then rosolved the limitation of modern two-stage object detector like Mask-RCNN\,\cite{he2017mask} and YOLOS\,\cite{redmon2016you}. DETR\,\cite{carion2020end} eliminates the handcrafted components (e.g., anchor generation
and non-maximum suppression) by combining a CNN backbone and Transformer
encoder-decoders. Deformable DETR\,\cite{zhu2020deformable} introduces the notion of deformable attention to leverage multi-scale features as well as accelerating the slow training convergence of DETR. Next, YOLOS\,\cite{fang2021you} and ViDT\,\cite{songvidt} propose the fully Transformer-based object detection architecture without relying CNNs, proving the potential of ViTs as a generic object detector. In this paper, we present an end-to-end Transformer-based framework for OVD, distinct from the two-stage detectors.

\smallskip\smallskip
\noindent\textbf{Open Vocabulary Object Detection.} 
% write the two stage ovd first with its limitation
Despite significant progress in object detection~\cite{songvidt, fang2021you, he2017mask, redmon2016you}, training and scaling these models remains costly due to their closed-set assumption.
%
To address more object categories, including previously unseen ones, recent advances have leveraged visual language models, e.g., CLIP~\cite{radford2021clip}, to provide additional supervision for detection models. Their goal is open-vocabulary object detection\,(OVD), enabling the detection of previously unseen objects at the time of training.
%However, there have been recent advancements in models (e.g, CLIP~\cite{radford2021clip}) which are able to classify images that were not part of their training dataset. In addition, open vocabulary detection (OVD) has also been developed to detect novel or unseen objects that were not included in the training stage.

One widely used approach in OVD is to employ a pre-trained CLIP model. ViLD~\cite{guopen2022vild} first proposes distilling knowledge from CLIP. Furthermore, DetPro~\cite{du2022detpro} suggests continuously learning prompts based on a pre-trained vision language model, and F-VLM~\cite{kuoopen2023fvlm} suggests that freezing CLIP weights as a backbone is more effective for novel object classes. This family of methods follows the pipeline of Mask-RCNN\,\cite{he2017mask}, inheriting the limitations of the two-stage detection model.

In contrast, OV-DETR~\cite{zang2022open} is the first to adopt DETR~\cite{carion2020end} for an end-to-end OVD. This involves formulating the learning objective as a binary matching problem between object queries and corresponding objects. However, OV-DETR requires significant computational power compared to Mask-RCNN or other two-stage detection methods. In addition, several approaches~\cite{zhong2022regionclip, rasheedbridging, zhou2022detic, minderer2022owlvit} have attempted to relax the problem to an unrestricted setup, which permits the use of external large-scale data that covers most common concepts in the real world, such as CC3M~\cite{sharma2018cc3m} and ImageNet-21k~\cite{deng2009imagenet}. However, comparing these methods with those under the restricted setup that does not allow the use of external data is unfair. It should be noted that our detection framework follows the restricted setup, requiring a stronger zero-shot generalization capability.

%In contrast, OV-DETR~\cite{zang2022open} first adopts the DETR architecture~\cite{carion2020end} for an end-to-end OVD. This involves formulating the learning objective as a binary matching problem between object queries and corresponding objects. However, OV-DETR requires significant computational power compared to Mask-RCNN or other two-stage detection methods. %To overcome these issues, here, we propose utilizing the DETR architecture, which is efficient, making it well-suited for OVD.
%In addition to them, several approaches~\cite{zhong2022regionclip, rasheedbridging, zhou2022detic, minderer2022owlvit} have attempted to relax the problem to unrestricted setup, which permits the use of external large-scale data that probably covers most of the common concepts in real-world, such as CC3M~\cite{sharma2018cc3m} and ImageNet-21k~\cite{deng2009imagenet}.
% or different types of pre-trained large-scale Transformer architectures like MViT~\cite{maaz2021mv
%However, it is unfair to compare them with the others under restricted setup that does not permit to use external data. It should be noted that our detection framework follows the restricted setup, which requires a stronger zero-shot generalization capability.

%However, it should be noted that the OVD framework used in this paper does not permit the use of additional data or pre-trained models, except for CLIP, in order to ensure a fair comparison.
% next write the OV-DETR method with its limitation


\section{Preliminary}
We briefly overview the pipelines of Transformer-based object detector and ViT-based CLIP.

% ViT-DET, ViDT, 
\smallskip\smallskip
\noindent\textbf{Transformer-based Detector.} Detection Transformers (DETR) have replaced the RPN-based two-stage pipeline with a simple Transformer encoder-decoder pipeline\,\cite{carion2020end, zhu2020deformable, li2022exploring}, and its extensions have realized a fully Transformer-based object detector that removes the need for CNN backbones by adopting the pre-trained ViT as its Transformer encoder\,\cite{fang2021you, litransformer, songvidt}. For the fully Transformer architecture, the $L$-layer Transformer encoder receives an image as the input of {patch tokens}, $\mathrm{{I}_{0}=[patch_1, \dots, patch_{n}] \in {\mathbb{R}^{n \times d}}}$, which are progressively calibrated across the layers via self-attention, 
\begin{equation}
\begin{gathered}
{\rm I}_{L}={\rm Encoder}({\rm I}_{0}) \in \mathbb{R}^{n \times d}.
\end{gathered}
\end{equation} 
Subsequently, the Transformer decoder receives \emph{object queries} ${\rm Q}_{0}=[{\rm qeury}_1, \dots, {\rm qeury}_{m}]\in {\mathbb{R}^{m \times d}}$ that aggregate the key contents from the encoder output $\mathrm{I}_{L}$ to produce $m$ different object embeddings via alternating self-attention and cross-attention, 
\begin{equation}
\mathrm{{O}}_{L}={\rm Decoder}({\rm Q}_{0}, {\rm I}_{L})\in \mathbb{R}^{m \times d}.
\label{eq:dec_output}
\end{equation}
Lastly, the object embeddings are directly fed to a 3-layer feedforward neural networks\,(FFNs) for bounding box regression and linear projection for classification, 
\begin{equation}
\begin{gathered}
\mathrm{\hat{B}_{det} =FFN_{3-layer}}({\rm O}_{L}) \in \mathbb{R}^{m \times 4}\\ \mathrm{\hat{P}_{det}=Linear}({\rm O}_{L})\in\mathbb{R}^{m \times k},
\end{gathered}
\label{eq:prediction}
\end{equation}
where $k$ is the number of object classes. The bipartite Hungarian matching is used to train the model in an end-to-end fashion \,\cite{carion2020end}.

\smallskip\smallskip
\noindent\textbf{ViT-based CLIP.} CLIP\,\cite{radford2021clip} is a dual-encoder architecture that consists of visual and text encoders, returning the image-text aligned embeddings of the visual and text inputs. ViT-based CLIP uses a ViT architecture as the visual encoder and a masked self-attention Transformer as the text encoder. It receives the patch tokens of an image as input to the visual encoder, while image-aligned word tokens to the text encoder. Given an image ${\bf x}$ and a text prompt  ${\bf y^{cls}}$ for its class name, CLIP generates the two output embeddings, an image embedding ${\bf e}^{\rm img}_{\rm clip}\in \mathbb{R}^{d^{\prime}}$ and a text embedding ${\bf e}^{\rm txt}_{\rm clip}\in \mathbb{R}^{d^{\prime}}$ as:
\begin{equation}
{\bf e}^{\rm img}_{\rm clip} = {\rm CLIP}_{\rm img}({\bf x}) ~~{\rm and}~~ {\bf e}^{\rm txt}_{\rm clip} = {\rm CLIP}_{\rm txt}({\bf y^{cls}}).
\end{equation}
Since the two output embeddings are aligned each other via contrastive learning, knowledge distillation using CLIP embeddings has been shown to improve the detection performance on previsouly unseen classes\,\cite{guopen2022vild, zang2022open}.

