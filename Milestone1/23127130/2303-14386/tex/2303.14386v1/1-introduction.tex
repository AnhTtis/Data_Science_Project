\begin{comment}
2/23 목요일

- alpha, beta값 조절하면서 eval. 
- inference time 필요 --> rcnn계열은 부탁해보기. 
- goal1: regression대비 classification은 잘 못해서 CLIP model을 가지고 옴.
- goal2: OV-DETR의 slow inference를 해결하였다. 
- motive: 기존 rpn계열은 새로운 cls가 나왔을 때 Box regression하는데 정보를 넘겨주지 못함. 하지만 Detr은 decoder단에서 새로운 cls에 대한 Feature를 넣어서 진행하기 때문에 box regression을 잘 함.
- problem1. decoder에 대한 cls 정보를 Query로 넣어줘야하는데 query의 개수가 SCalable하지 못하다. inference time이 너무 느리다. (OV-DETR) 
- problem2. box의 recall이 높은 대비 precision이 낮으므로 Classification 성능을 높이기 위해 ensemble을 사용하면 더 잘한다. (pruning -> clip ensemble) 
- F-VLM대비 강점 - pruning 기법을 통해 수십개의 box를 가지고 진행. (roi-align과 박스 개수 차이) 

#### Method
- token을 prompt로 넣은 것 
- attention을 앙상블 하는 것 

#### Result 
- main result (coco, lvis) + inference time 
- clip ensemble 을 clip 모델 바꿈에 따라 어떻게 변하는지. 
- box에 대한 Recall값이 높을 것이다. (coco, lvis) 
- 정성평가 (pruning이 왜 잘되는 지를 그림으로 보여주기) 
- pretrained model다르게 했을 때, 어떻게 되는지. 

\end{comment}

%%%%%%%%% BODY TEXT
\section{Introduction}

Object detection is one of crucial tasks for real-world computer vision applications, which localizes and detects visible objects from the scenes\,\cite{he2017mask, padilla2020survey, papageorgiou1998general, redmon2016you}. Recently, the development of visual language models has enabled significant progress towards open-vocabulary object detection\,(OVD), which can detect novel classes that were \emph{not} visible during training\,\cite{guopen2022vild, zang2022open, zhong2022regionclip}. The large visual language models like CLIP\,\cite{radford2021clip} help recognize the concept of common objects using their image-text aligned embeddings. Therefore, it offers a more practical and effective detection pipeline by eliminating the need of re-training object detectors on newly collected data. 

\begin{figure*}[t!]
\begin{center}
\includegraphics[width=15cm]{figures/intro/intro.pdf}
\end{center}
\vspace*{-0.2cm}
\hspace*{4.0cm}{\small (a) OV-DETR.} \hspace*{5.2cm}{\small (b) Prompt-OVD (Ours).}
\vspace*{-0.1cm}
\caption{Architectural comparison: (a) OV-DETR needs linearly increasing number of object queries with respect to the number of classes and (b) \algname{} maintains a constant number of object queries by using class prompts.}
\label{fig:overview}
\vspace*{-0.4cm}
\end{figure*}

% Why OV-DETR and its Limitation
Most OVD approaches heavily rely on the region proposal network\,(RPN), which can easily overfit to base classes\,\cite{guopen2022vild, zhong2022regionclip}, and several approaches have extended it to leverage external data to mitigate this limitation\,\cite{minderer2022owlvit, rasheedbridging, zhou2022detic}. On the other hand, OV-DETR\,\cite{zang2022open} proposes a Transformer-based framework with conditional matching, which has two advantages over RPN-based methods.
%
First, it has the potential to localize novel objects better than the RPN by inject \emph{class-specific} knowledge, i.e., the addition of CLIP embeddings to the object queries as in Figure \ref{fig:overview}(a), to the Transformer decoder. 
%
Second, it can be trained in an \emph{end-to-end} fashion, eliminating the need for handcraft components like anchor generation and non-maximum suppression\,\cite{carion2020end, zhu2020deformable}. However, the main limitation of OV-DETR is the need for multiple forward passes in the decoder due to its large number of object queries, which results in slow inference speeds that may not be suitable for real-world use cases.

In this paper, we propose an efficient yet effective end-to-end Transformer-based framework named \textbf{Prompt-OVD}, as shown in Figure \ref{fig:overview}(b). Our framework leverages class embeddings from CLIP as \emph{class prompts} to the Transformer decoder. Unlike OV-DETR, which forces all object queries to be class-specific by mathematically adding class embeddings, we keep the object queries class-agnostic by prepending the class embeddings as {class prompts} to the decoder. Hence, we can maintain the \emph{constant} number of object queries regardless of increasing number of classes, significantly expediting the inference speed when handling a large number of object classes like LVIS data. In contrast, as shown in Figure \ref{fig:overview}(a), OV-DETR needs linearly increasing number of object queries with respect to the number of classes.
The proposed class prompts provide class-specific instruction instantaneously to the decoder on which object classes should be detected. Therefore, the output object embeddings by the decoder can be generalized to classes given by the prompts, resulting in very high recall for box regression on base and novel object classes.

Furthermore, there is still a significant gap in classification accuracy between base and novel classes in OVD, as only the base classes are visible during the end-to-end training pipeline. To mitigate such gap, we benefit from the {zero-shot} classification ability of the Vision Transformer\,(ViT)-based CLIP, and propose two additional techniques to reduce its computational overhead: efficient \emph{RoI-based masked attention}, which extracts the CLIP embeddings of the given RoIs with a minimal cost of a single inference path, and \emph{RoI pruning}, which selects only the RoIs that have objects with high probability from the entire box predictions. We then ensemble the classification results from the ViT-based CLIP with those from our detection model.

We conduct comprehensive experiments on two popular OVD datasets, namely OV-COCO\,\cite{lin2014microsoft} and OV-LVIS\,\cite{gupta2019lvis}, and compare \algname{} with an end-to-end method OV-DETR (baseline), but also four RPN-based two-phase OVD methods. The results demonstrate that \algname{} has an inference speed \emph{21.2 times faster} than OV-DETR and similar to that of the existing two-stage methods. Additionally, \algname{} achieves 30.6 $\mathrm{AP^{50}}$ of novel classes for OV-COCO and 29.4 $\mathrm{AP^{box}}$ of novel classes for OV-LVIS. Our contributions are as follows:
\begin{itemize}
\item We propose a prompt-based decoding that can keep a constant number of object queries, reducing the computational overhead of the Transformer decoder.  
\vspace*{-0.1cm}
\item We propose RoI-based masked attention and RoI pruning to benefit from a pre-trained ViT-based CLIP at minimal computational cost.
\vspace*{-0.1cm}
\item We considerably improve the efficiency and accuracy of the end-to-end Transformer-based detection pipeline for open vocabulary object detection.
\end{itemize}




%\emph{class-agnostic} object queries as the input to the decoder like the canonical DETR\,\cite{carion2020end} while adding CLIP class emebeddings as the prepended \emph{class prompts}. 
%Unlike OV-DETR that copies all object queries for every possible classes, our prompt-based approach keeps a \emph{constant} number of object queries regardless of the number of classes; it can expedites the inference speed significantly when handling a large number of objects like LVIS data with 1,203 classes. 

%as fully takes advantage of using the Transformer decoder with high inference speed and detection accuracy. 
%As our first contribution, we propose to keep \emph{class-agnostic} queries as the input to the decoder like the canonical DETR\,\cite{carion2020end}, while adding CLIP class emebeddings as the prepended \emph{class prompts}. 
%Unlike OV-DETR that copies all object queries for every possible classes, our prompt-based approach keeps a \emph{constant} number of object queries regardless of the number of classes; it expedites the inference speed significantly when handling a large number of objects like LVIS data with 1,203 classes. The class prompts provide class-specific instruction on which base or novel object classes should be detected via self-attention in the decoder. Therefore, the output object embeddings by the decoder can be generalized to \emph{any} class given by the class prompts, resulting in box regression\,(localization) with very high recall on both base and novel objects.