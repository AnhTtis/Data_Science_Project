%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[t!]
\centering
\caption{{Main Results on OV-COCO and OV-LVIS:} We evaluate box AP with IoU threshold 0.5 ($\mathrm{mAP^{50}}$) on OV-COCO, and box AP ($\mathrm{mAP^{box}}$) and mask AP ($\mathrm{mAP^{mask}}$) on OV-LVIS. Note that $\mathrm{mAP_{novel}}$ and $\mathrm{mAP}$ indicate the performance of zero-shot and the entire of categories, respectively. Lastly, latency implies inference time per image in seconds for OV-LVIS.}
 \vspace*{-0.25cm}
\label{tab:main}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{@{}lrrrrrrrrrrr@{}}
\toprule 
& & & & \multicolumn{3}{c}{OV-COCO}  & \multicolumn{5}{c}{OV-LVIS}  \\
\cmidrule(lr){5-7} \cmidrule(lr){8-12}  
{Methods}& Backbone & CLIP & Res. & $\mathrm{mAP^{50}_{novel}}$ & $\mathrm{mAP^{50}_{base}}$ & $\mathrm{mAP^{50}}$ & $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ & Latency\,(s)\\ 
\midrule
\multicolumn{1}{r}{\normalsize {\sffamily DETR-based}} \vspace*{0.1cm} \\
% OWL-ViT & ViT-L/14 & ViT-L/14 & 25.6 & 34.7 & - & - \\ 
OV-DETR~\cite{zang2022open} & RN50 & ViT-B/32 & 1333  & 29.4 & 61.0 & 52.7 & 18.0 & 27.4 & 17.4 & 26.6 & 12.28 \\
\textbf{Prompt-OVD} & ViT-B/16 & ViT-L/14 & 840 & \textbf{30.6} & \textbf{63.5} & \textbf{54.9} & \textbf{29.4} & \textbf{33.0} & \textbf{23.1} & 24.2 & 0.58 \\
\cmidrule(lr){1-12}
% & %(memory update / robust learning) &
\multicolumn{1}{r}{\normalsize {\sffamily RCNN-based}}&  \multicolumn{4}{r}{\normalsize \!\!\!\!\!\!\!\!{\sffamily(Latency Range: 0.40 -- 0.70 seconds)}}\vspace*{0.1cm} \\ 
% ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
Detic~\cite{zhou2022detic}     & RN50 & ViT-B/32 & 1333 & 27.8 & 51.0 & 45.0 & 23.6 & 30.4 & 21.4 & \textbf{26.9} & 0.47 \\
ViLD~\cite{guopen2022vild}      & RN50 & ViT-B/32 & 1333 & 27.6 & 59.6 & 51.3& 16.7 & 27.8 & 16.6 & 25.5 & 0.48  \\
F-VLM~\cite{kuoopen2023fvlm}     & RN50 & RN50 & 1024 & 28.0 & 43.7 & 39.6& 20.3 & 27.8 & 18.6 & 24.2 & 0.50 \\
DetPro~\cite{du2022detpro}    & RN50 & ViT-B/32 & 1333  & - & -& - & 20.8 & 28.4 & 19.8 & 25.9 & 0.67\\
% F-VLM~\cite{kuoopen2023fvlm}    & RN50x4 & RN50x4 & 1024 & - & - & 26.3 & 28.5 & 0.72\\ 
% \cmidrule(lr){1-9} 
\bottomrule
\end{tabular}%
}
\vspace*{-0.1cm}
\end{table*}

\begin{table*}[t!]
% \begin{wraptable*}{r}{3cm}

\parbox{0.3\linewidth}{
\centering
\caption{Latency change by modifying OV-DETR to \algname{}.}
\vspace*{-0.25cm}
\label{tab:inference_study}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}llr@{}}
\toprule 
& {Modification}& Latency (s)\\ 
\midrule
 & OV-DETR & 12.28\\
% \cmidrule{1-3}
\,\,\,(1)\!\! & ResNet $\xrightarrow{}$ ViT & 12.36\\ 
\,\,\,(2)\!\! & ViT $\xrightarrow{}$ ViTDet & 8.75\\ 
\,\,\,(3)\!\! & Prompt-based Decoding & 2.89\\
\,\,\,(4)\!\! & Ensemble with CLIP & 3.03\\
\,\,\,(5)\!\! & RoI Pruning ($\epsilon=0.3)$ & 0.58\\
\bottomrule
\label{table:modification}
\vspace*{-0.4cm}
\end{tabular}%
}}
{\color{white} \,}
\hfill
\parbox{0.33\linewidth}{
\centering
\caption{Performance with varying $\alpha$ when fixing $\beta=0.4$.}
\vspace*{-0.3cm}
\label{tab:alpha}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{@{}lrrrr@{}}
\toprule 
{$\alpha$}& $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ \\ 
\midrule
% & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
0.0 & 28.1 & 30.8 & 21.9 & 22.4 \\
0.1 & 28.7 & 32.3 & 22.6 & 23.6\\
% \rowcolor{LightCyan}
\textbf{0.2} & \textbf{29.4} & \textbf{33.0} & \textbf{23.1} & \textbf{24.2}  \\
0.3 & 29.5 & 32.2 & 23.2 & 23.7 \\
0.4 & 29.7 & 30.6 & 23.3 & 22.5 \\
0.5 & 29.9 & 28.8 & 23.5 & 21.2\\
1.0 & 30.5 & 14.5 & 24.0 & 10.8 \\
\bottomrule
\label{table:alpha_search}
\vspace*{-0.4cm}
\end{tabular}%
}}
{\color{white} \,}
\hfill
\parbox{0.33\linewidth}{
\centering
\caption{Performance with varying $\beta$ when fixing $\alpha=0.2$.}
\vspace*{-0.3cm}
\label{tab:beta}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}lrrrr@{}}
\toprule 
{$\beta$}& $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ \\ 
\midrule
% & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
0.0 & 15.9 & 30.0 & 12.1 & 21.7\\
0.1 & 22.8 & 31.4 & 17.9 & 22.9 \\
0.2 & 29.0 & 32.7 & 22.5 & 23.9 \\
0.3 & 29.3 & 33.0 & 23.0 & 24.1 \\
\textbf{0.4} & \textbf{29.3} & \textbf{33.0} & \textbf{23.1} & \textbf{24.2} \\
0.5 & 28.6 & 33.0 & 22.4 & 24.1 \\
1.0 & 19.6 & 31.5 & 15.3 & 22.9 \\
\bottomrule
\label{table:beta_search}
\vspace*{-0.4cm}
\end{tabular}%
}
}


\vspace*{-0.35cm}
\end{table*}

\section{Evaluation} 

\noindent\textbf{Datasets.} We evaluate our approach on two popularly used benchmark datasets, namely OV-COCO and OV-LVIS, each of which is modified from MS-COCO~\cite{lin2014microsoft} and LVIS\,(v1)~\cite{gupta2019lvis}; OV-COCO has 121K images with 64 classes while OV-LVIS has 100K images with 1,203 classes.
Following OV-DETR~\cite{zang2022open}, COCO is split into 17 novel classes and 48 base classes. LVIS is split into three categories: 337 novel classes, and 866 common or base classes based on the number of training images. Note that we refer to the two datasets as OV-COCO and OV-LVIS, respectively, and only base classes are used for training. 


\smallskip\smallskip
\noindent\textbf{Algorithms.} We compare \algname{} with an end-to-end OVD detection model named OV-DETR\,\cite{zang2022open} (baseline) and four two-stage OVD models. However, it should be noted that the two-stage OVD models are based on Mask-RCNN or allow the use of external large-scale data, which makes a fair comparison with the end-to-end Transformer-based detectors difficult. Therefore, to ensure a fair comparison, we follow two criteria: (1) the results should be obtained by only using base categories in training, i.e., the restricted OVD setup and (2) the models' inference speed should be in the range of 0.4~--~0.7 seconds/image, which is similar to that of our proposed framework. 

%To validate the effectiveness of our method, we chose OV-DETR and four Mask-RCNN-based methods as baselines. Since Mask-RCNN-based approaches have a completely different model architecture from ours, we chose these methods as baselines based on two criteria for a fair comparison: (i) using only the dataset for base categories during training (not allowing to use the additional dataset), and (ii) having inference times between 0.4 and 0.7 seconds, which are similar to Prompt-OVD.

\smallskip\smallskip
\noindent\textbf{Implementation.} The proposed \algname{} builds upon Deformable DETR\,\cite{zhu2020deformable}, similar to OV-DETR. However, we merge the independent ResNet backbone and Transformer encoder into a single ViT encoder using ViTDet~\cite{li2022exploring}. As a result, our architecture is a purely Transformer encoder-decoder structure, following recent fully Transformer detection pipeline\,\cite{litransformer, songvidt}. 

For training, we initialize the backbone weights with a plain ViT backbone that has been pre-trained as Masked Autoencoders on ImageNet-1K. The entire model is then trained end-to-end for 50 epochs with a batch size of 32, a weight decay of 1e-4, and an AdamW optimizer. We set the initial learning rate to 2e-4 while using an image size of 840$\times$840. We implement and test all algorithms using PyTorch on eight NVIDIA V100 GPUs. 

For inference, there are three hyperparameters: the weights $\alpha$ and $\beta$ for ensembling in Eq.\,\eqref{eq:ensemble}; the threshold $\epsilon$ for RoI pruning in Eq.\,\eqref{eq:pruning}. The former weights are set to be $(0.2, 0.35)$ and $(0.2, 0.4)$ for OV-COCO and OV-LVIS, while the latter pruning threshold is set to 0.125 and 0.3 for OV-COCO and OV-LVIS. As for the ensemble for classification, we leverage the CLIP model that uses ViT-L/14 as its image encoder with a image size of 336$\times$336, which adds very little computational overhead with our RoI-based masked attention and RoI pruning. The detailed analysis of the hyperparameters and additional overhead due to using CLIP is provided in Section \ref{sec:abl_study}. 

In addition, we need to incorporate the mask head into our model for the evaluation on OV-LVIS, as RPN-based two-stage methods have reported both box and mask APs. Following the recent literature\,\cite{dong2021solq, song2022extendable}, we extend our DETR-based detector using SOLQ\,\cite{dong2021solq}, which can perform a joint training of object detection and instance segmentation by simply adding a unified query representation module. The mask vector size is set to be 1,024 while keeping remaining hyperparameters to be the same as SOLQ.

\smallskip\smallskip
\noindent\textbf{Evaluation Metrics.} We evaluate the detection accuracy of our method following exactly the same metrics used in prior OVD studies\,\cite{guopen2022vild, zang2022open, zhong2022regionclip, minderer2022owlvit}. Specifically, for OV-COCO, we use $\mathrm{mAP^{50}}$ which is a measure of the box average precision\,(AP) with an IoU threshold of 0.5. On the other hand, for OV-LVIS, we use both box mAP ($\mathrm{mAP^{box}}$) and mask mAP ($\mathrm{mAP^{mask}}$) obtained by the joint learning of object detection and instance segmentation, respectively. 

Inference time is also a crucial metric for practical applications. To compare the efficiency of different models, we compute the inference time of all methods using the same hardware environment, consisting of a single NVIDIA V100 GPU and six Intel(R) Xeon(R) Gold 5120 CPUs. To ensure an accurate inference time measurement, we calculate the average time of 100 iterations after five initial iterations, using a batch size of 1.

%\noindent\textbf{Implementation.} To enhance the performance of our model, we opt for the DETR~\cite{carion2020end} architecture based on OV-DETR~\cite{zang2022open}, and we replace the backbone and encoder with ViT-DET~\cite{li2022exploring}. The initial weights of the backbone start from the MAE pre-trained model. Our model is trained for 50 epochs, with a 32 batch size, a weight decay of 1e-4, an optimizer AdamW~\cite{loshchilov2017decoupled}, and an initial learning rate of 2e-4, while using an image size of 840x840. It is worth noting that the training process is carried out using 8 NVIDIA A100 GPUs.

%When performing inference, we use the values of ($\alpha$, $\beta) = (0.2, 0.35)$ and (0.2, 0.4) for OV-COCO and OV-LVIS, respectively, to ensemble with CLIP. In addition, we set the values of $\epsilon$ to 0.125 and 0.3 for OV-COCO and OV-LVIS, respectively, for RoI pruning. We limit the number of detections per image to a maximum of 300 and 1500, and the temperature is set to 0.065 and 0.01 for OV-COCO and OV-LVIS, respectively. Finally, we measure the inference time of all methods using the same hardware environment consisting of a NVIDIA V100 GPU and 6 Intel(R) Xeon(R) Gold 5120 CPUs. To ensure accurate inference time measurements, we calculate the average time of 100 iterations after five initial iterations using a batch size of 1.


%\smallskip\smallskip
%\noindent\textbf{Instance Segmentation.} In order to calculate the mask mAP for OV-LVIS, we need to incorporate the mask head into our model. Since the mask head in DETR is a FPN-style network, and our backbone cannot combine with it, we adopt the SOLQ method~\cite{dong2021solq}, which segments objects by jointly learning the unified query representation for three tasks (classification, localization, and segmentation). We utilize the 1024 dimension of the mask vector, while keeping all other parameters the same as SOLQ.

%\smallskip\smallskip
%\noindent\textbf{Evaluation Metrics.} We follow the same metrics as earlier open vocabulary studies. Specifically, for OV-COCO, they use a metric called $\mathrm{mAP^{50}}$ which measures the box AP with an IoU threshold of 0.5. For OV-LVIS, the metrics use both mask mAP ($\mathrm{mAP^{mask}}$) and box mAP ($\mathrm{mAP^{box}}$). 


\subsection{Main Experiment}

We present a comprehensive comparison of \algname{} with other five OVD methods in terms of detection accuracy and speed. To ensure a fair comparison, we only include the results of RCNN-based methods that can operate at a similar inference speed as ours. Table \ref{tab:main} summarizes the results of \algname{} and other five OVD methods.

In general, \algname{} outperforms the previous end-to-end OVD method, OV-DETR, on both datasets. Notably, \algname{}'s inference speed is {$21.2$ times} faster than OV-DETR, thanks to its prompt-based decoding approach. Refer to Section \ref{sec:inference_speed} for an in-depth comparison of efficiency with OV-DETR. %
%
Moreover, Prompt-OVD exhibits superior performance in terms of box mAP, even compared to RCNN-based OVD methods. These results support the effectiveness of our design that utilizes ViT-based CLIP with RoI-based mask attention and RoI pruning, improving the overall performance. Further investigation of the two techniques can be found in Section~\ref{sec:masked_attention} and ~\ref{sec:pruning}.

Although we observe a larger gap between box mAP and mask mAP (29.4 $\mathrm{mAP^{box}_{novel}} \rightarrow 23.1 \mathrm{mAP^{mask}_{novel}})$, this is a result of inheriting the limitation of SOLQ\,\cite{dong2021solq}. Specifically, the vector encoding of 2D segmentation masks using discrete cosine transformation loses object details compared to the conventional FPN-style mask head\,\cite{song2022extendable}.  
%
Furthermore, despite using a larger ViT-B/16 backbone than ResNet-50, \algname{} exhibits comparable inference time to RCNN-based methods, thanks to its simple encoding-decoding pipeline. Therefore, our results demonstrate that \algname{} shows a potential of the end-to-end Transformer-based framework for OVD. 

In Appendices B and C, we discuss potential enhancements to our method and present the results of our experiments on using image queries other than text queries for open-vocabulary object detection, respectively.


%Prompt-OVD performs better than both Mask-RCNN-based and DETR-based models for all metrics on OV-LVIS. Although Prompt-OVD significantly outperforms the other models in terms of box mAP, F-VLM and Detic achieve the highest scores for mask mAP in novel and entire categories, respectively. We believe that SOLQ has limitations in instance segmentation because it only uses 1024 mask vectors that differ from the FPN-style mask head. Consequently, Prompt-OVD has a larger gap between box mAP and mask mAP than models that utilize the FPN-style mask head.

%Also, Table~\ref{tab:main} shows that Prompt-OVD outperforms all other baselines on OV-COCO. It is noteworthy that Prompt-OVD surpasses the others in terms of box $\mathrm{mAP^{50}}$ for both novel and overall categories. We believe that the RoI pruning and score fusion with CLIP may be crucial in improving the overall performance, and we investigate the effects of these techniques in the Section~\ref{sec:abl_study}.


%\smallskip\smallskip
%\noindent\textbf{Inference Time.} Table~\ref{tab:main} also shows the inference time per image on OV-LVIS. Surprisingly, despite using a larger backbone compared to Mask-RCNN-based baselines, there is little difference in inference time. Furthermore, Prompt-OVD reduces the inference time for OV-DETR, which is the method based on DETR, by up to $\mathsf{24x}$.


% \begin{table*}[t!]
% \centering
% \caption{Main Results on OV-LVIS}
% \vspace*{-0.2cm}
% \label{tab:main_lvis}
% \resizebox{1.0\linewidth}{!}{%
% \begin{tabular}{@{}lrrrrrrrr@{}}
% \toprule 
% % & & & \multicolumn{2}{c}{OV-LVIS} & \multicolumn{2}{c}{OV-COCO} \\
% % \cmidrule(lr){4-5} \cmidrule(lr){6-7}  
% {Methods}& Backbone & CLIP & Res. & $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ & s/img\\ 
% \midrule
% \multicolumn{1}{r}{\footnotesize {\sffamily DETR-based}} \\
% % OWL-ViT & ViT-L/14 & ViT-L/14 & 25.6 & 34.7 & - & - \\ 
% OV-DETR~\cite{zang2022open} & RN50 & ViT-B/32 & 1024 & 18.0 & 27.4 & 17.4 & 26.6 & 12.28\\
% \textbf{Prompt-OVD} & ViT-B/16 & ViT-L/14 & 840 & \textbf{29.4} & \textbf{33.0} & \textbf{23.1} & 24.2 & 0.58\\
% \cmidrule(lr){1-9}

% % & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% % ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
% Detic~\cite{zhou2022detic}     & CenterNet2 & ViT-B/32 & 1333 & 23.6 & 30.4 & 21.4 & \textbf{26.9} & 0.47 \\
% ViLD~\cite{guopen2022vild}      & RN50 & ViT-B/32 & 1333 & 16.7 & 27.8 & 16.6 & 25.5 & 0.48 \\
% F-VLM~\cite{kuoopen2023fvlm}     & RN50 & RN50 & 1024 & 20.3 & 27.8 & 18.6 & 24.2 & 0.50 \\

% DetPro~\cite{du2022detpro}    & RN50 & ViT-B/32 & 1333 & 20.8 & 28.4 & 19.8 & 25.9 & 0.67 \\

% % F-VLM~\cite{kuoopen2023fvlm}    & RN50x4 & RN50x4 & 1024 & - & - & 26.3 & 28.5 & 0.72\\ 


% % \cmidrule(lr){1-9} 

% \bottomrule
% \end{tabular}%
% }
% % \vspace*{-0.5em}
% \end{table*}



% \begin{table}[t!]
% \centering
% \caption{Main Results on OV-COCO}
% \vspace*{-0.2cm}
% \label{tab:main_coco}
% \resizebox{0.9\linewidth}{!}{%
% \begin{tabular}{@{}lrrr@{}}
% \toprule 
% % & & & \multicolumn{2}{c}{OV-LVIS} & \multicolumn{2}{c}{OV-COCO} \\
% % \cmidrule(lr){4-5} \cmidrule(lr){6-7}  
% {Methods}& $\mathrm{mAP^{novel}_{50}}$ & $\mathrm{mAP^{base}_{50}}$ & $\mathrm{mAP_{50}}$ \\ 
% \midrule
% % & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily DETR-based}} \\
% % OWL-ViT & ViT-L/14 & ViT-L/14 & 25.6 & 34.7 & - & - \\ 
% OV-DETR~\cite{zang2022open} & 29.4 & 61.0 & 52.7 \\
% \textbf{Prompt-OVD} & \textbf{30.6}& \textbf{63.5} &\textbf{54.9} \\
% \cmidrule(lr){1-4}

% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% % ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
% Detic~\cite{zhou2022detic}      &  27.8 & 51.0 & 45.0 \\
% ViLD~\cite{guopen2022vild}      &  27.6 & 59.6 & 51.3 \\
% F-VLM~\cite{kuoopen2023fvlm}     &  28.0 & 43.7 & 39.6 \\

% \bottomrule
% \end{tabular}%
% }
% \end{table}
\begin{figure*}[t]
\begin{center}
\includegraphics[width=16.7cm]{figures/ablation_study.pdf}
\end{center}
\vspace*{-0.5cm}
\begin{subfigure}{0.3\textwidth}
\label{fig:gt}
\caption{GT}
\end{subfigure}
\begin{subfigure}{0.15\textwidth}
\label{fig:rpn}
\caption{RPN}
\end{subfigure}
\begin{subfigure}{0.31\textwidth}
\caption{No pruning ($\epsilon = 0$)}
\label{fig:noprune}
\end{subfigure}
\begin{subfigure}{0.16\textwidth}
\caption{Pruning ($\epsilon =0.3$)}
\label{fig:prune}
\end{subfigure}
\vspace*{-0.4em}
\caption{Box predictions: (a) ground-truth boxes, (b) boxes estimated by RPN from DetPro~\cite{du2022detpro}, (c)--(d) boxes estimated by \algname{} without and with RoI pruning. Due to numerous predicted boxes in (b) and (c), we limit the number of boxes to 40 for better visualization. Red and green boxes are the ground-truth of novel and base classes, while blue and white ones represent predicted boxes that are either in close proximity or not in close proximity to the ground-truth, respectively.}
\label{fig:pruning_abl}
\vspace*{-0.4cm}
\end{figure*}


\subsection{Main Ablation Study}
\label{sec:abl_study}

\subsubsection{Inference Speed Up from OV-DETR} 
\label{sec:inference_speed} 
Table \ref{table:modification} summarizes the change in inference speed when replacing each design component of OV-DETR with our proposed ones on OV-LVIS. (1) Despite having more parameters, using ViT-B/16 incurs little additional latency, as it rather reduces the computational burden for the multi-scale deformable attention of the DETR encoder. (2) The use of local attention and the replacement of the DETR encoder with a simple feature pyramid network, as suggested by \cite{li2022exploring, song2022extendable}, result in a meaningful reduction in latency. (3) The primary speedup comes from replacing the decoding of OV-DETR with prompt-based decoding. (4) The ensemble with ViT-based CLIP only adds very little latency thanks to our efficient RoI-based masked attention. (6) RoI pruning also significantly contributes to speeding up by reducing the number of RoI candidates for detection and segmentation, without sacrificing detection accuracy. Overall, \algname{} speeds up inference by $21.2$ times over OV-DETR.

\begin{table}[t!]
\centering

\caption{Performance between RoI Align and RoI-based Masked Attention (RMA) on OV-LVIS.}% We set $\epsilon$ as 0.3 while pruning.} % If RoI pruning is employed, $\epsilon$ is assigned a value of 0.3; otherwise, it should be set to 0.0.
\vspace*{-0.3cm}
\label{tab:roi_pool}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{@{}lrrrrrr@{}}
\toprule 
{RoI Proc.}& $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ & Latency\\ 
\midrule
% & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
Naive & 12.8 & 28.3 & 9.9 & 20.3 & 13.51 \\
\hspace{3mm}+Pruning & 13.7 & 28.3 & 10.2 & 20.4 & 1.85\\
Align~\cite{he2017mask} & 24.2 & 31.7 & 19.1 & 23.0 & 3.06\\
\hspace{3mm}+Pruning & 26.2 & 32.0 & 20.8 & 23.4 & 0.60 \\
RMA (ours) & 26.6 & 32.5 & 21.0 & 23.8 & 3.03 \\
\hspace{3mm}+Pruning & \textbf{29.4} & \textbf{33.0} & \textbf{23.1} & \textbf{24.2} & 0.58 \\


\bottomrule
\end{tabular}%
}
\vspace*{-0.3cm}
\label{tab:rma_analysis}
\end{table}

%\vspace*{-0.18cm}
\subsubsection{Ensemble Coefficient}
%\label{sec:hyperparams}
We investigate the influence of the ensembling weights $\alpha$ and $\beta$ for base and novel classes. We vary the value of each hyperparameter while keeping the other constant, as summarized in Tables \ref{tab:alpha} and \ref{tab:beta}, with a fixed RoI pruning threshold $\epsilon$ of 0.3. In general, the overall performance increases and then reaches at their maximum values when $\alpha=0.2$ and $\beta=0.4$. Using extreme values 0.0 or 1.0 for either coefficient results in significantly worse performance than using a more balanced ensemble. Moreover, the results show that the ensemble is more effective for novel classes than base classes, as evidenced by the significant impact of $\beta$ on the results of $\mathrm{mAP_{novel}}$. This suggests that the knowledge from CLIP has a greater positive impact on novel classes than on base classes. We set the values of $\alpha$ and $\beta$ to 0.2 and 0.4 for all experiments.
%

%Also, for getting optimal probability weights $\alpha$ and $\beta$, we report the performance change as $\alpha$ and $\beta$ increase in Table~\ref{tab:alpha} and ~\ref{tab:beta}, respectively. Table~\ref{tab:alpha} illustrates that while the zero-shot performance ($\mathrm{mAP_{novel}}$) increases monotonically, the overall performance ($\mathrm{mAP}$) increases and peaks at $\alpha=0.2$, then decreases. Interestingly, Table~\ref{tab:beta} demonstrates that both zero-shot and overall performances increase and reach a maximum value when $\beta=0.4$. Therefore, we anticipate that the knowledge from CLIP have a greater positive impact on new classes than the base classes, as the optimal value of $\beta$ is larger than that of $\alpha$.
%Note that we set the values of $\alpha$ and $\beta$ to 0.2 and 0.4, respectively, for all the experiments.



\subsubsection{RoI-based Masked Attention} 
\label{sec:masked_attention}
We conduct a comparison between our RoI-based masked attention method with both the naive approach and the commonly used RoI Align method, as summarized in Table \ref{tab:rma_analysis}. The naive approach implies that CLIP infers all the cropped images of RoIs according to Eq.~\eqref{eq:iter_infer}. To apply the RoI Align method to CLIP's ViT encoder, we reconstruct its patch tokens into a 2D feature map prior to the final Transformer layer. Compared to the naive approach, our RoI-based masked attention method has a significantly smaller computational overhead. Additionally, the efficiency and effectiveness of our method can be further improved by utilizing RoI pruning, which removes background RoIs. In contrast, RoI Align shows substantially lower $\mathrm{mAP^{box}_{novel}}$ and $\mathrm{mAP^{box}}$ than our method, as it is not optimized for the Transformer structure. Surprisingly, the naive crop method did not perform well, likely due to resizing a small object to be too large. Therefore, using masked attention is a more appropriate approach for Transformers than others. 

%To validate our proposed RoI-based masked attention, we compare it with RoI Align, which is utilized by Mask-RCNN in Table~\ref{tab:roi_pool}. Masked attention overcomes RoI Align for all the metrics with a large margin. Moreover, we analyze the optimal usage of the attention layer number in Table~\ref{tab:blk_num}, and using the last layer has the best results compared to the others for all the metrics. Through those emperical studies, therefore, we select RoI-based masked attention with the last attention layer instead of RoI align. 
\label{sec:eval_RMA}
\begin{table}[t!]
\centering
\caption{Performance trade-off with varying $\epsilon$ on OV-LVIS.}
\vspace*{-0.2cm}
\label{tab:roi_thres}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{@{}lrrrrr@{}}
\toprule 
{$\epsilon$}& $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ & Latency (s)\\ 
\midrule
% & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
0.0 & 26.6 & 32.5 & 21.0 & 23.8 & 3.03\\
0.1 & 27.7 & 32.8 & 21.9 & 24.0 & 1.38\\
0.2 & 28.3 & 33.0 & 22.3 & 24.1 & 0.86\\
% \rowcolor{LightCyan}
\textbf{0.3} & \textbf{29.4} & \textbf{33.0} & \textbf{23.1} & \textbf{24.2} & 0.58\\
0.4 & 28.3 & 31.9 & 22.5 & 23.4 & 0.43\\
0.5 & 25.3 & 28.1 & 19.8 & 20.8 & 0.37\\
\bottomrule
\end{tabular}%
}
\vspace*{-0.2cm}
\end{table}



\subsubsection{Box Regression over RPN}
%\label{sec:eval_prune}
% tab2에서 masked attention과 align과의 비교. 
% block number 마지막으로 선택한 이유. 
%\vspace*{-0.1cm}
We validate the effectiveness of \algname{} in terms of box regression compared with the existing RPN-based method. Figure \ref{fig:pruning_abl} compares their estimated bounding boxes based on the ground-truth ones. %Specifically, Figure \ref{fig:pruning_abl}(a) shows the base and novel objects with their ground-truth bounding boxes. 
Figure \ref{fig:pruning_abl}(b) is an example of the scenario where the RPN method fails to accurately localize objects, i.e., a  missing box for carriage (novel class) and two deviated boxes from the ground-truth for the bread (novel class) and plate (base class). In contrast, \algname{} successfully localizes all base and novel objects with high recall using the prompt-guided decoding, as shown in Figure \ref{fig:pruning_abl}(c). Despite the presence of background or inaccurate boxes in the box candidates, \algname{} successfully covers all the ground-truth boxes with its predictions. Furthermore, as seen in Figure \ref{fig:pruning_abl}(d), RoI pruning effectively excludes such irrelevant boxes from the detection process.


%In order to validate the effectiveness of RoI pruning, we compare the proposals before and after pruning in Fig.~\ref{fig:pruning_abl}. Prior to pruning, there are numerous false positive boxes that did not match the ground truth, and those might negatively affect the performance due to score fusion with CLIP. However, pruning significantly reduces the number of non-matching boxes. In addition, since object detection consumes their time for handling lots of proposals, RoI pruning improves both performance and inference speed. We believe that RoI pruning narrow the gap in inference time between our method and the baseline despite using a larger backbone. 
%\vspace*{-0.3cm}
\subsubsection{RoI Pruning Threshold}
\label{sec:pruning}
%\vspace*{-0.1cm}
We investigate the trade-off between detection performance and computational efficiency by varying the pruning threshold $\epsilon$, as summarized in Table \ref{tab:roi_thres}. As the threshold increases, fewer bounding boxes are retained, as the number of boxes with object scores greater than the threshold decreases. For instance, when the threshold is set to be 0.0, which means RoI pruning is not applied, the detection accuracy deteriorates due to the inclusion of background boxes, also resulting in a high latency. On the contrary, the detection accuracy improves as $\epsilon$ increases within the reasonable range of 0.0~--~0.3, but deteriorates with a larger threshold of 0.4~--~0.5. That is, as $\epsilon$ increases, more false positive boxes begins to be excluded, leading to improved performance. However, further increasing the threshold leads to the removal of true positive boxes, causing performance degradation. Therefore, we set the value of $\epsilon$ to 0.3 for all experiments.

%, we analyze the trend of performance and inference time as $\epsilon$ increases and report the results in Table~\ref{tab:roi_thres}. It shows that latency decreases as $\epsilon$ increases, since the number of boxes of which score is bigger than $\epsilon$ decreases. Especially, the inference time is drastically higher when not using RoI prunning ($\epsilon=0.0$). On the contrary, the performance increases as $\epsilon \in [0.0, 0.3]$, and decreases as $\epsilon \in [0.3, 0.5]$. We expect that, as $\epsilon$ increases, it drops more false positive boxes to improve the performance at $\epsilon \in [0.0, 0.3]$, then it causes degradation of performance by dropping true positive boxes at $\epsilon \in [0.3, 0.5]$. Hence, we select $\epsilon$ as 0.3, which has the best performance with valid inference time. 


\begin{table}[t!]
\centering
\caption{Performance when using different CLIP models for ensembling on OV-LVIS.}
\vspace*{-0.25cm}
\label{tab:clip_arch}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{@{}lrrrrr@{}}
\toprule 
{CLIP}& $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ & Latency (s)\\ 
\midrule
% & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\
None     & 15.4 & 28.1 & 11.7 & 20.3 & 0.54 \\
ViT-B/32 & 20.5 & 30.1 & 16.4 & 21.9 & 0.56 \\
ViT-B/16 & 22.3 & 31.1 & 17.5 & 22.6 & 0.57\\
\textbf{ViT-L/14} & \textbf{29.4} & \textbf{33.0} & \textbf{23.1} & \textbf{24.2} & 0.58 \\


\bottomrule
\end{tabular}%
}
\label{tab:clip_size}
\vspace{-1.2em}
\end{table}


\smallskip\smallskip
\subsection{Additional Design Choice}
%\vspace*{-0.1cm}
\noindent We explore two supplementary design choices to utilize the ViT-based CLIP model in a manner that achieves the best balance between OVD performance and inference speed. %Following this, we further examine the impact of utilizing different initial weights for the backbone. %on the overall performance of \algname{}.
We provide more supplementary analysis on applying RoI-based masked attention to different attention layers and using different pre-trained ViT backbones with \algname{} in Appendix D.

%\vspace*{-0.3cm}
\subsubsection{CLIP Model Size} 
%\vspace*{-0.1cm}
% We conduct experiments to examine the impact of classification ensemble using CLIP with respect to its model size. 
Table \ref{tab:clip_size} summarizes the performance obtained after the ensemble with three different sizes of ViT-based CLIP models, including the scenario where CLIP is not used at all. We observe that without using the ensemble technique, the zero-shot performance is very poor compared to when CLIP is employed. However, as the size of the CLIP model increases, both zero-shot and overall performance improve, albeit with slightly higher inference speed. The difference in inference time is negligible due to our proposed efficient techniques; RoI-based masked attention and RoI pruning. Therefore, we conclude that the benefits obtained from using a larger CLIP encoder with ViT-L/14 outweigh the minimal increase in inference time.

%Since we expect that the zero-shot results ($\mathrm{mAP_{novel}}$) could be influenced by the utilization of the CLIP model for ensembling, we report the performance on OV-LVIS in Table~\ref{tab:clip_arch} as the CLIP model is varied. Specifically, the zero-shot performance is noticeably worse when not utilizing CLIP for ensembling compared to when CLIP is used. As the size of the CLIP model increases, both zero-shot and overall performance increase, and the inference time also increases slightly. However, the difference in inference time is negligible in comparison to the performance benefits gained. As such, we can conclude that boxes from DETR architecture contains not only base classes but also novel classes, and CLIP can more contribute the classification for unseen classes as the size increases without increasing latency much.

%\vspace*{-0.3cm}
\subsubsection{CLIP Input Resolution} 
%$\vspace*{-0.1cm}
Another design consideration is the resolution of input to the ViT-based CLIP. To evaluate the performance trade-off between detection accuracy and latency of CLIP, we vary the input image size and report the results in Table \ref{tab:image_size}. Similar to the model size, the latency change by input resolution is negligible thanks to our efficient methodological design. This indicates that we can use a variety of image resolutions without sacrificing the latency of \algname{}. However, the best image resolution for the ViT-L/14 encoder is $336 \times 336$, which is the original input size used to train the CLIP model, while the detection accuracy for base and novel classes drops with a larger $672\times672$. The $336 \times 336$  provides a good balance between detection accuracy and inference time, so it is the recommended input resolution. %for the proposed \algname{} framework. 

%Here, we evaluate the performance trade-off between detection accuracy and latency with of CLIP by varying the input image size, and report the results in Table~\ref{tab:clip_img_res}. Although there is a significant difference in performance, the latency is negligible when using image resolutions of 168 and 336. However, when compared to image resolutions of 336 and 672, the performance difference is minimal, but there is a significant gap in inference time. As a result, we choose an image resolution of 336 when feeding images into the CLIP model. 



%e analyze the performance is affected by using different initial weights for the backbone, specifically ImageNet and MAE. Table~\ref{tab:pretrained_model} displays the performance results when training the backbone using these two pretrained models. As a result, starting from the MAE pretrained model yields better performance than starting from the ImageNet pretrained model. Therefore, we use the trained model that starts from the MAE pretrained model for all experiments.







\begin{table}[t!]
\centering
\caption{Performance when using different input image resolution for ViT-based CLIP on OV-LVIS.}
\vspace*{-0.25cm}
\label{tab:clip_img_res}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{@{}lrrrrr@{}}
\toprule 
{img. res.}& $\mathrm{mAP^{box}_{novel}}$ & $\mathrm{mAP^{box}}$ & $\mathrm{mAP^{mask}_{novel}}$ & $\mathrm{mAP^{mask}}$ & Latency \\ 
\midrule
% & %(memory update / robust learning) &
% \multicolumn{1}{r}{\footnotesize {\sffamily RCNN-based}} \\
% ViLD-text & 10.1 & 24.9 & 5.9 & 49.3 \\

168$\times$168 & 28.5 & 30.1 & 20.9 & 22.0 & 0.57 \\ 
\textbf{336$\times$336} & \textbf{29.4} & \textbf{33.0} & \textbf{23.1} & \textbf{24.2} & 0.58 \\ 
672$\times$672 & 29.2 & 33.0 & 23.0 & 24.2 & 0.83 \\


\bottomrule
\end{tabular}%
}
\vspace*{-0.35cm}
\label{tab:image_size}
\end{table}


% \begin{table}[t!]
% \centering
% \caption{Latency change as modifying OV-DETR to Prompt-OVD on OV-LVIS.}
% \label{tab:inference_study}
% \resizebox{0.7\linewidth}{!}{%
% \begin{tabular}{@{}llr@{}}
% \toprule 
% & {Modification}& Latency (s)\\ 
% \midrule
%  & OV-DETR & 12.28\\
% % \cmidrule{1-3}
% (1) & ResNet $\xrightarrow{}$ ViT & \\ 
% (2) & ViT $\xrightarrow{}$ ViTDET & \\ 
% (3) & Encoder $\xrightarrow{}$ FPN & \\
% (4) & prompt decode & \\
% (5) & + Ensemble with CLIP & \\


% \bottomrule
% \end{tabular}%
% }

% \end{table}

