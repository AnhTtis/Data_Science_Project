{
    "arxiv_id": "2303.13336",
    "paper_title": "Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI",
    "authors": [
        "Chenshuang Zhang",
        "Chaoning Zhang",
        "Sheng Zheng",
        "Mengchun Zhang",
        "Maryam Qamar",
        "Sung-Ho Bae",
        "In So Kweon"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2023-03-24"
    ],
    "latest_version": 1,
    "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
    ],
    "abstract": "Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13336v1"
    ],
    "publication_venue": "18 pages"
}