@inproceedings{yang2020DesignAI,
author = {Yang, Qian and Steinfeld, Aaron and Ros\'{e}, Carolyn and Zimmerman, John},
title = {Re-Examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376301},
doi = {10.1145/3313831.3376301},
abstract = {Artificial Intelligence (AI) plays an increasingly important role in improving HCI and user experience. Yet many challenges persist in designing and innovating valuable human-AI interactions. For example, AI systems can make unpredictable errors, and these errors damage UX and even lead to undesired societal impact. However, HCI routinely grapples with complex technologies and mitigates their unintended consequences. What makes AI different? What makes human-AI interaction appear particularly difficult to design? This paper investigates these questions. We synthesize prior research, our own design and research experience, and our observations when teaching human-AI interaction. We identify two sources of AI's distinctive design challenges: 1) uncertainty surrounding AI's capabilities, 2) AI's output complexity, spanning from simple to adaptive complex. We identify four levels of AI systems. On each level, designers encounter a different subset of the design challenges. We demonstrate how these findings reveal new insights for designers, researchers, and design tool makers in productively addressing the challenges of human-AI interaction going forward.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {sketching, artificial intelligence, prototyping, user experience},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@misc{bommasani2021Foundation,
  doi = {10.48550/ARXIV.2108.07258},
  
  url = {https://arxiv.org/abs/2108.07258},
  
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Opportunities and Risks of Foundation Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{wu2022AIChains,
author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
title = {AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517582},
doi = {10.1145/3491102.3517582},
abstract = {Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by “unit-testing” sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {385},
numpages = {22},
keywords = {Human-AI Interaction, Large Language Models, Natural Language Processing},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{bansal2021HAI,
author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
title = {Does the Whole Exceed Its Parts? The Effect of AI Explanations on Complementary Team Performance},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445717},
doi = {10.1145/3411764.3445717},
abstract = {Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {81},
numpages = {16},
keywords = {Augmented intelligence, Human-AI teams, Explainable AI},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{smithrenner2020Explainability,
author = {Smith-Renner, Alison and Fan, Ron and Birchfield, Melissa and Wu, Tongshuang and Boyd-Graber, Jordan and Weld, Daniel S. and Findlater, Leah},
title = {No Explainability without Accountability: An Empirical Study of Explanations and Feedback in Interactive ML},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376624},
doi = {10.1145/3313831.3376624},
abstract = {Automatically generated explanations of how machine learning (ML) models reason can help users understand and accept them. However, explanations can have unintended consequences: promoting over-reliance or undermining trust. This paper investigates how explanations shape users' perceptions of ML models with or without the ability to provide feedback to them: (1) does revealing model flaws increase users' desire to "fix" them; (2) does providing explanations cause users to believe - wrongly - that models are introspective, and will thus improve over time. Through two controlled experiments - varying model quality - we show how the combination of explanations and user feedback impacted perceptions, such as frustration and expectations of model improvement. Explanations without opportunity for feedback were frustrating with a lower quality model, while interactions between explanation and feedback for the higher quality model suggest that detailed feedback should not be requested without explanation. Users expected model correction, regardless of whether they provided feedback or received explanations.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {interactive machine learning, explainable machine learning},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{davis2015Drawing,
author = {Davis, Nicholas and Hsiao, Chih-PIn and Singh, Kunwar Yashraj and Li, Lisa and Moningi, Sanat and Magerko, Brian},
title = {Drawing Apprentice: An Enactive Co-Creative Agent for Artistic Collaboration},
year = {2015},
isbn = {9781450335980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2757226.2764555},
doi = {10.1145/2757226.2764555},
abstract = {This paper describes a co-creative web-based drawing application called the Drawing Apprentice. This system collaborates with users in real time abstract drawing. We describe the theory, interaction design, and user experience of the Drawing Apprentice system. We evaluate the system with formative user studies and expert evaluations from a juried art competition in which a Drawing Apprentice submission won the code-based art category.},
booktitle = {Proceedings of the 2015 ACM SIGCHI Conference on Creativity and Cognition},
pages = {185–186},
numpages = {2},
keywords = {cognitive science, creativity support tools, collaboration, art, computational creativity},
location = {Glasgow, United Kingdom},
series = {C\&C '15}
}

@inproceedings{davis2016Drawing,
author = {Davis, Nicholas and Hsiao, Chih-PIn and Yashraj Singh, Kunwar and Li, Lisa and Magerko, Brian},
title = {Empirically Studying Participatory Sense-Making in Abstract Drawing with a Co-Creative Cognitive Agent},
year = {2016},
isbn = {9781450341370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2856767.2856795},
doi = {10.1145/2856767.2856795},
abstract = {This paper reports on the design and evaluation of a co-creative drawing partner called the Drawing Apprentice, which was designed to improvise and collaborate on abstract sketches with users in real time. The system qualifies as a new genre of creative technologies termed "casual creators" that are meant to creatively engage users and provide enjoyable creative experiences rather than necessarily helping users make a higher quality creative product. We introduce the conceptual framework of participatory sense-making and describe how it can help model and understand open-ended collaboration. We report the results of a user study comparing human-human collaboration to human-computer collaboration using the Drawing Apprentice system. Based on insights from the user study, we present a set of design recommendations for co-creative agents.},
booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
pages = {196–207},
numpages = {12},
keywords = {creativity support tools, collaboration, computational creativity},
location = {Sonoma, California, USA},
series = {IUI '16}
}

@inproceedings{oh2018Drawing,
author = {Oh, Changhoon and Song, Jungwoo and Choi, Jinhan and Kim, Seonghyeon and Lee, Sungwoo and Suh, Bongwon},
title = {I Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial Intelligence},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174223},
doi = {10.1145/3173574.3174223},
abstract = {Recent advances in artificial intelligence (AI) have increased the opportunities for users to interact with the technology. Now, users can even collaborate with AI in creative activities such as art. To understand the user experience in this new user--AI collaboration, we designed a prototype, DuetDraw, an AI interface that allows users and the AI agent to draw pictures collaboratively. We conducted a user study employing both quantitative and qualitative methods. Thirty participants performed a series of drawing tasks with the think-aloud method, followed by post-hoc surveys and interviews. Our findings are as follows: (1) Users were significantly more content with DuetDraw when the tool gave detailed instructions. (2) While users always wanted to lead the task, they also wanted the AI to explain its intentions but only when the users wanted it to do so. (3) Although users rated the AI relatively low in predictability, controllability, and comprehensibility, they enjoyed their interactions with it during the task. Based on these findings, we discuss implications for user interfaces where users can collaborate with AI in creative works.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {artificial intelligence, human-ai interaction, human computer collaboration},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{green2014Translation,
author = {Green, Spence and Chuang, Jason and Heer, Jeffrey and Manning, Christopher D.},
title = {Predictive Translation Memory: A Mixed-Initiative System for Human Language Translation},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647408},
doi = {10.1145/2642918.2647408},
abstract = {The standard approach to computer-aided language translation is post-editing: a machine generates a single translation that a human translator corrects. Recent studies have shown this simple technique to be surprisingly effective, yet it underutilizes the complementary strengths of precision-oriented humans and recall-oriented machines. We present Predictive Translation Memory, an interactive, mixed-initiative system for human language translation. Translators build translations incrementally by considering machine suggestions that update according to the user's current partial translation. In a large-scale study, we find that professional translators are slightly slower in the interactive mode yet produce slightly higher quality translations despite significant prior experience with the baseline post-editing condition. Our analysis identifies significant predictors of time and quality, and also characterizes interactive aid usage. Subjects entered over 99% of characters via interactive aids, a significantly higher fraction than that shown in previous work.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {177–187},
numpages = {11},
keywords = {mixed-initiative, language translation, interface design, empirical study},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{clark2018Writing,
author = {Clark, Elizabeth and Ross, Anne Spencer and Tan, Chenhao and Ji, Yangfeng and Smith, Noah A.},
title = {Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172983},
doi = {10.1145/3172944.3172983},
abstract = {As the quality of natural language generated by artificial intelligence systems improves, writing interfaces can support interventions beyond grammar-checking and spell-checking, such as suggesting content to spark new ideas. To explore the possibility of machine-in-the-loop creative writing, we performed two case studies using two system prototypes, one for short story writing and one for slogan writing. Participants in our studies were asked to write with a machine in the loop or alone (control condition). They assessed their writing and experience through surveys and an open-ended interview. We collected additional assessments of the writing from Amazon Mechanical Turk crowdworkers. Our findings indicate that participants found the process fun and helpful and could envision use cases for future systems. At the same time, machine suggestions do not necessarily lead to better written artifacts. We therefore suggest novel natural language models and design choices that may better support creative writing.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {329–340},
numpages = {12},
keywords = {machine in the loop, creative writing, natural language processing},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{gero2019Metaphoria,
author = {Gero, Katy Ilonka and Chilton, Lydia B.},
title = {Metaphoria: An Algorithmic Companion for Metaphor Creation},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300526},
doi = {10.1145/3290605.3300526},
abstract = {Creative writing, from poetry to journalism, is at the crux of human ingenuity and social interaction. Existing creative writing support tools produce entire passages or fully formed sentences, but these approaches fail to adapt to the writer's own ideas and intentions. Instead we posit to build tools that generate ideas coherent with the writer's context and encourage writers to produce divergent outcomes. To explore this, we focus on supporting metaphor creation. We present Metaphoria, an interactive system that generates metaphorical connections based on an input word from the writer. Our studies show that Metaphoria provides more coherent suggestions than existing systems, and supports the expression of writers' unique intentions. We discuss the complex issue of ownership in human-machine collaboration and how to build adaptive creativity support tools in other domains.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {generative art, human-computer collaboration, co-creativity, natural language processing, writing support},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{koch2019Ideation,
author = {Koch, Janin and Lucero, Andr\'{e}s and Hegemann, Lena and Oulasvirta, Antti},
title = {May AI? Design Ideation with Cooperative Contextual Bandits},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300863},
doi = {10.1145/3290605.3300863},
abstract = {Design ideation is a prime creative activity in design. However, it is challenging to support computationally due to its quickly evolving and exploratory nature. The paper presents cooperative contextual bandits (CCB) as a machine-learning method for interactive ideation support. A CCB can learn to propose domain-relevant contributions and adapt their exploration/exploitation strategy. We developed a CCB for an interactive design ideation tool that 1) suggests inspirational and situationally relevant materials ("may AI?"); 2) explores and exploits inspirational materials with the designer; and 3) explains its suggestions to aid reflection. The application case of digital mood board design is presented, wherein visual inspirational materials are collected and curated in collages. In a controlled study, 14 of 16 professional designers preferred the CCB-augmented tool. The CCB approach holds promise for ideation activities wherein adaptive and steerable support is welcome but designers must retain full outcome control.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {ideation support, interactive machine-learning, mood board design, creativity support tools},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{karimi2019Drawing,
author = {Karimi, Pegah and Davis, Nicholas and Maher, Mary Lou and Grace, Kazjon and Lee, Lina},
title = {Relating Cognitive Models of Design Creativity to the Similarity of Sketches Generated by an AI Partner},
year = {2019},
isbn = {9781450359177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325480.3325488},
doi = {10.1145/3325480.3325488},
abstract = {This paper presents and evaluates a new method for inspiring creativity in a co-creative design system. The method uses a computational model of aconceptual shift based on clustering of deep features from a database of sketches. The co-creative sketching tool maps a user's sketch to a sketch of a distinct category that has high, medium, or low visual and semantic similarity. We hypothesize that the degree of similarity between the user's and the system's sketches is associated with a range of cognitive models of creativity in a design context. We report on the findings of an empirical study that analyzes different design scenarios in which the user sketches in response to a proposed conceptual shift. The findings show that how similar the computational agent's sketch is to the user's original sketch is related to the presence of three types of design creativity in the user's response: combinatorial, exploratory, and transformational.},
booktitle = {Proceedings of the 2019 on Creativity and Cognition},
pages = {259–270},
numpages = {12},
keywords = {sketching, design creativity, collaboration, co-creativity},
location = {San Diego, CA, USA},
series = {C\&C '19}
}

@inproceedings{mccormack2019Music,
author = {McCormack, Jon and Gifford, Toby and Hutchings, Patrick and Llano Rodriguez, Maria Teresa and Yee-King, Matthew and d'Inverno, Mark},
title = {In a Silent Way: Communication Between AI and Improvising Musicians Beyond Sound},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300268},
doi = {10.1145/3290605.3300268},
abstract = {Collaboration is built on trust, and establishing trust with a creative Artificial Intelligence is difficult when the decision process or internal state driving its behaviour isn't exposed. When human musicians improvise together, a number of extra-musical cues are used to augment musical communication and expose mental or emotional states which affect musical decisions and the effectiveness of the collaboration. We developed a collaborative improvising AI drummer that communicates its confidence through an emoticon-based visualisation. The AI was trained on musical performance data, as well as real-time skin conductance, of musicians improvising with professional drummers, exposing both musical and extra-musical cues to inform its generative process. Uni- and bi-directional extra-musical communication with real and false values were tested by experienced improvising musicians. Each condition was evaluated using the FSS-2 questionnaire, as a proxy for musical engagement. The results show a positive correlation between extra-musical communication of machine internal state and human musical engagement.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {extra-musical communication, ai systems, improvisation},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{louie2020Music,
author = {Louie, Ryan and Coenen, Andy and Huang, Cheng Zhi and Terry, Michael and Cai, Carrie J.},
title = {Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376739},
doi = {10.1145/3313831.3376739},
abstract = {While generative deep neural networks (DNNs) have demonstrated their capacity for creating novel musical compositions, less attention has been paid to the challenges and potential of co-creating with these musical AIs, especially for novices. In a needfinding study with a widely used, interactive musical AI, we found that the AI can overwhelm users with the amount of musical content it generates, and frustrate them with its non-deterministic output. To better match co-creation needs, we developed AI-steering tools, consisting of Voice Lanes that restrict content generation to particular voices; Example-Based Sliders to control the similarity of generated content to an existing example; Semantic Sliders to nudge music generation in high-level directions (happy/sad, conventional/surprising); and Multiple Alternatives of generated content to audition and choose from. In a summative study (N=21), we discovered the tools not only increased users' trust, control, comprehension, and sense of collaboration with the AI, but also contributed to a greater sense of self-efficacy and ownership of the composition relative to the AI.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {co-creation, human-ai interaction, generative deep neural networks},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@InProceedings{Sbai_2018_ECCV_Workshops,
author = {Sbai, Othman and Elhoseiny, Mohamed and Bordes, Antoine and LeCun, Yann and Couprie, Camille},
title = {DesIGN: Design Inspiration from Generative Networks},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
month = {September},
year = {2018}
}

@inproceedings{jeon2021Fashion,
author = {Jeon, Youngseung and Jin, Seungwan and Shih, Patrick C. and Han, Kyungsik},
title = {FashionQ: An AI-Driven Creativity Support Tool for Facilitating Ideation in Fashion Design},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445093},
doi = {10.1145/3411764.3445093},
abstract = {Recent research on creativity support tools (CST) adopts artificial intelligence (AI) that leverages big data and computational capabilities to facilitate creative work. Our work aims to articulate the role of AI in supporting creativity with a case study of an AI-based CST tool in fashion design based on theoretical groundings. We developed AI models by externalizing three cognitive operations (extending, constraining, and blending) that are associated with divergent and convergent thinking. We present FashionQ, an AI-based CST that has three interactive visualization tools (StyleQ, TrendQ, and MergeQ). Through interviews and a user study with 20 fashion design professionals (10 participants for the interviews and 10 for the user study), we demonstrate the effectiveness of FashionQ on facilitating divergent and convergent thinking and identify opportunities and challenges of incorporating AI in the ideation process. Our findings highlight the role and use of AI in each cognitive operation based on professionals’ expertise and suggest future implications of AI-based CST development.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {576},
numpages = {18},
keywords = {fashion design, creativity support tool, cognitive operation, artificial intelligence utilization, ideation process},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{quanz2020machine,
  title={Machine learning based co-creative design framework},
  author={Quanz, Brian and Sun, Wei and Deshpande, Ajay and Shah, Dhruv and Park, Jae-eun},
  journal={arXiv preprint arXiv:2001.08791},
  year={2020}
}

@article{buschek2021nine,
  title={Nine Potential Pitfalls when Designing Human-AI Co-Creative Systems},
  author={Buschek, Daniel and Mecke, Lukas and Lehmann, Florian and Dang, Hai},
  publisher = {Association for Computing Machinery},
  journal = {Workshops at the International Conference on Intelligent User Interfaces (IUI)},
  year={2021}
}

@article{huang2020AISong,
  doi = {10.48550/ARXIV.2010.05388},
  url = {https://arxiv.org/abs/2010.05388},
  author = {Huang, Cheng-Zhi Anna and Koops, Hendrik Vincent and Newton-Rex, Ed and Dinculescu, Monica and Cai, Carrie J.},
  keywords = {Sound (cs.SD), Human-Computer Interaction (cs.HC), Machine Learning (cs.LG), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, J.5; I.2},
  title = {AI Song Contest: Human-AI Co-Creation in Songwriting},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{pmlr-v139-ramesh21a,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}

@misc{ramesh2022DALLE2,
  doi = {10.48550/ARXIV.2204.06125},
  url = {https://arxiv.org/abs/2204.06125},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@InProceedings{Rombach_2022_CVPR,
    author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},
    title     = {High-Resolution Image Synthesis With Latent Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {10684-10695}
}

@misc{saharia2022Imagen,
  doi = {10.48550/ARXIV.2205.11487},
  url = {https://arxiv.org/abs/2205.11487},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J and Norouzi, Mohammad},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{yu2022Parti,
  doi = {10.48550/ARXIV.2206.10789},
  url = {https://arxiv.org/abs/2206.10789},
  author = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
 
@misc{dalle1Web,
  author = {OpenAI},
  title = {DALL·E: Creating Images from Text},
  url = {https://openai.com/blog/dall-e/},
  year = {2022},
  note = {(Accessed on 08/31/2022)}
}

@misc{dalle2Web,
  author = {OpenAI},
  title = {DALL·E 2},
  url = {https://openai.com/dall-e-2/},
  year = {2022},
  note = {(Accessed on 08/31/2022)}
}

@misc{stableDiffusionWeb,
  author = {Stability AI},
  title = {Stable Diffusion launch announcement — Stability.Ai},
  url = {https://stability.ai/blog/stable-diffusion-announcement},
  year = {2022},
  note = {(Accessed on 08/31/2022)}
}

@misc{midjourneyWeb,
  author = {},
  title = {Midjourney},
  url = {https://www.midjourney.com/home/},
  year = {2022},
  note = {(Accessed on 09/01/2022)}
}

@misc{partiWeb,
  author = {Google},
  title = {Parti: Pathways Autoregressive Text-to-Image Model},
  url = {https://parti.research.google/},
  year = {2022},
  note = {(Accessed on 08/31/2022)}
}

@misc{imagenWeb,
  author = {Google},
  title = {Imagen: Text-to-Image Diffusion Models},
  url = {https://imagen.research.google/},
  year = {2022},
  note = {(Accessed on 08/31/2022)}
}

@misc{mansimov2015ImgFromCaption,
  doi = {10.48550/ARXIV.1511.02793},
  url = {https://arxiv.org/abs/1511.02793},
  author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Generating Images from Captions with Attention},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{gan2014Goodfellow,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative Adversarial Networks},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3422622},
doi = {10.1145/3422622},
abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
journal = {Commun. ACM},
month = {oct},
pages = {139–144},
numpages = {6}
}

@InProceedings{Zhu_2017_ICCV,
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
title = {Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@InProceedings{Karras_2019_CVPR,
author = {Karras, Tero and Laine, Samuli and Aila, Timo},
title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{gan2018Brock,
  author    = {Andrew Brock and
               Jeff Donahue and
               Karen Simonyan},
  title     = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
  journal   = {CoRR},
  volume    = {abs/1809.11096},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.11096},
  eprinttype = {arXiv},
  eprint    = {1809.11096},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-11096.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{lieberjurassic,
  title={Jurassic-1: Technical Details and Evaluation, White paper, AI21 Labs, 2021},
  author={Lieber, O and Sharir, O and Lentz, B and Shoham, Y},
  url={URL: https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6\_jurassic\_tech\_paper.pdf},
  month={Aug},
  year={2021}
}

@misc{betz2021Aloud,
  doi = {10.48550/ARXIV.2103.13033},
  url = {https://arxiv.org/abs/2103.13033},
  author = {Betz, Gregor and Richardson, Kyle and Voigt, Christian},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{liu2021InContext,
  doi = {10.48550/ARXIV.2101.06804},
  url = {https://arxiv.org/abs/2101.06804},
  author = {Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {What Makes Good In-Context Examples for GPT-$3$?},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{lu2021PromptOrder,
  doi = {10.48550/ARXIV.2104.08786},
  url = {https://arxiv.org/abs/2104.08786},
  author = {Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{rezwana2022AICoCreation,
author = {Rezwana, Jeba and Maher, Mary Lou},
title = {Understanding User Perceptions, Collaborative Experience and User Engagement in Different Human-AI Interaction Designs for Co-Creative Systems},
year = {2022},
isbn = {9781450393270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527927.3532789},
doi = {10.1145/3527927.3532789},
abstract = {Human-AI co-creativity involves humans and AI collaborating on a shared creative product as partners. In a creative collaboration, communication is an essential component among collaborators. In many existing co-creative systems, users can communicate with the AI, usually using buttons or sliders. Typically, the AI in co-creative systems cannot communicate back to humans, limiting their potential to be perceived as partners rather than just a tool. This paper presents a study with 38 participants to explore the impact of two interaction designs, with and without AI-to-human communication, on user engagement, collaborative experience and user perception of a co-creative AI. The study involves user interaction with two prototypes of a co-creative system that contributes sketches as design inspirations during a design task. The results show improved collaborative experience and user engagement with the system incorporating AI-to-human communication. Users perceive co-creative AI as more reliable, personal, and intelligent when the AI communicates to users. The findings can be used to design effective co-creative systems, and the insights can be transferred to other fields involving human-AI interaction and collaboration.},
booktitle = {Creativity and Cognition},
pages = {38–48},
numpages = {11},
keywords = {AI to human Communication, Human-AI Creative Collaboration, Interaction design, Co-creativity, Human-AI Communication},
location = {Venice, Italy},
series = {C\&C '22}
}

@article{koch2020CollaborativeAI,
author = {Koch, Janin and Taffin, Nicolas and Beaudouin-Lafon, Michel and Laine, Markku and Lucero, Andr\'{e}s and Mackay, Wendy E.},
title = {ImageSense: An Intelligent Collaborative Ideation Tool to Support Diverse Human-Computer Partnerships},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW1},
url = {https://doi.org/10.1145/3392850},
doi = {10.1145/3392850},
abstract = {Professional designers create mood boards to explore, visualize, and communicate hard-to-express ideas. We present ImageCascade, an intelligent, collaborative ideation tool that combines individual and shared work spaces, as well as collaboration with multiple forms of intelligent agents. In the collection phase, ImageCascade offers fluid transitions between serendipitous discovery of curated images via ImageCascade, combined text- and image-based Semantic search, and intelligent AI suggestions for finding new images. For later composition and reflection, ImageCascade provides semantic labels, generated color palettes, and multiple tag clouds to help communicate the intent of the mood board. A study of nine professional designers revealed nuances in designers' preferences for designer-led, system-led, and mixed-initiative approaches that evolve throughout the design process. We discuss the challenges in creating effective human-computer partnerships for creative activities, and suggest directions for future research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {may},
articleno = {45},
numpages = {27},
keywords = {creativity support tool, ideation, agency, mood board design}
}

@inproceedings{frih2019CSTinHCI,
author = {Frich, Jonas and MacDonald Vermeulen, Lindsay and Remy, Christian and Biskjaer, Michael Mose and Dalsgaard, Peter},
title = {Mapping the Landscape of Creativity Support Tools in HCI},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300619},
doi = {10.1145/3290605.3300619},
abstract = {Creativity Support Tools (CSTs) play a fundamental role in the study of creativity in Human-Computer Interaction (HCI). Even so, there is no consensus definition of the term 'CST' in HCI, and in most studies, CSTs have been construed as one-off exploratory prototypes, typically built by the researchers themselves. This makes it difficult to clearly demarcate CST research, but also to compare findings across studies, which impedes advancement in digital creativity as a growing field of research. Based on a literature review of 143 papers from the ACM Digital Library (1999-2018), we contribute a first overview of the key characteristics of CSTs developed by the HCI community. Moreover, we propose a tentative definition of a CST to help strengthen knowledge sharing across CST studies. We end by discussing our study's implications for future HCI research on CSTs and digital creativity.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–18},
numpages = {18},
keywords = {literature review, creativity, creativity support tools (csts), meta-analysis},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{suh2021GenerativeMusic,
author = {Suh, Minhyang (Mia) and Youngblom, Emily and Terry, Michael and Cai, Carrie J},
title = {AI as Social Glue: Uncovering the Roles of Deep Generative AI during Social Music Composition},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445219},
doi = {10.1145/3411764.3445219},
abstract = {Recent advances in deep generative neural networks have made it possible for artificial intelligence to actively collaborate with human beings in co-creating novel content (e.g. music, art). While substantial research focuses on (individual) human-AI collaborations, comparatively less research examines how AI can play a role in human-human collaborations during co-creation. In a qualitative lab study, we observed 30 participants (15 pairs) compose a musical phrase in pairs, both with and without AI. Our findings reveal that AI may play important roles in influencing human social dynamics during creativity, including: 1) implicitly seeding a common ground at the start of collaboration, 2) acting as a psychological safety net in creative risk-taking, 3) providing a force for group progress, 4) mitigating interpersonal stalling and friction, and 5) altering users’ collaborative and creative roles. This work contributes to the future of generative AI in social creativity by providing implications for how AI could enrich, impede, or alter creative social dynamics in the years to come.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {582},
numpages = {11},
keywords = {machine learning, human-AI co-creation, music composition},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{hancock2020AICommunication,
    author = {Hancock, Jeffrey T and Naaman, Mor and Levy, Karen},
    title = "{AI-Mediated Communication: Definition, Research Agenda, and Ethical Considerations}",
    journal = {Journal of Computer-Mediated Communication},
    volume = {25},
    number = {1},
    pages = {89-100},
    year = {2020},
    month = {01},
    abstract = "{We define Artificial Intelligence-Mediated Communication (AI-MC) as interpersonal communication in which an intelligent agent operates on behalf of a communicator by modifying, augmenting, or generating messages to accomplish communication goals. The recent advent of AI-MC raises new questions about how technology may shape human communication and requires re-evaluation – and potentially expansion – of many of Computer-Mediated Communication’s (CMC) key theories, frameworks, and findings. A research agenda around AI-MC should consider the design of these technologies and the psychological, linguistic, relational, policy and ethical implications of introducing AI into human–human communication. This article aims to articulate such an agenda.}",
    issn = {1083-6101},
    doi = {10.1093/jcmc/zmz022},
    url = {https://doi.org/10.1093/jcmc/zmz022},
    eprint = {https://academic.oup.com/jcmc/article-pdf/25/1/89/32961176/zmz022.pdf},
}

@article{seeber2020AITeammates,
title = {Machines as teammates: A research agenda on AI in team collaboration},
journal = {Information \& Management},
volume = {57},
number = {2},
pages = {103174},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103174},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619303337},
author = {Isabella Seeber and Eva Bittner and Robert O. Briggs and Triparna {de Vreede} and Gert-Jan {de Vreede} and Aaron Elkins and Ronald Maier and Alexander B. Merz and Sarah Oeste-Reiß and Nils Randrup and Gerhard Schwabe and Matthias Söllner},
keywords = {Artificial intelligence, Design, Duality, Research agenda, Team collaboration},
abstract = {What if artificial intelligence (AI) machines became teammates rather than tools? This paper reports on an international initiative by 65 collaboration scientists to develop a research agenda for exploring the potential risks and benefits of machines as teammates (MaT). They generated 819 research questions. A subteam of 12 converged them to a research agenda comprising three design areas – Machine artifact, Collaboration, and Institution – and 17 dualities – significant effects with the potential for benefit or harm. The MaT research agenda offers a structure and archetypal research questions to organize early thought and research in this new area of study.}
}

@inproceedings{wang2020HumanAi,
author = {Wang, Dakuo and Churchill, Elizabeth and Maes, Pattie and Fan, Xiangmin and Shneiderman, Ben and Shi, Yuanchun and Wang, Qianying},
title = {From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3381069},
doi = {10.1145/3334480.3381069},
abstract = {Artificial Intelligent (AI) and Machine Learning (ML) algorithms are coming out of research labs into the real-world applications, and recent research has focused a lot on Human-AI Interaction (HAI) and Explainable AI (XAI). However, Interaction is not the same as Collaboration. Collaboration involves mutual goal understanding, preemptive task co-management and shared progress tracking. Most of human activities today are done collaboratively, thus, to integrate AI into the already-complicated human workflow, it is critical to bring the Computer-Supported Cooperative Work (CSCW) perspective into the root of the algorithmic research and plan for a Human-AI Collaboration future of work. In this panel we ask: Can this future for trusted human-AI collaboration be realized? If so, what will it take? This panel will bring together HCI experts who work on human collaboration and AI applications in various application contexts, from industry and academia and from both the U.S. and China. Panelists will engage the audience through discussion of their shared and diverging visions, and through suggestions for opportunities and challenges for the future of human-AI collaboration.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {ai-powered healthcare, trusted ai, explainable ai, computer-supported corporative work, ai partner, group collaboration, human-ai collaboration},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@article{wolf2019AICollab,
author = {Wolf, Christine and Blomberg, Jeanette},
title = {Evaluating the Promise of Human-Algorithm Collaborations in Everyday Work Practices},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359245},
doi = {10.1145/3359245},
abstract = {Human-algorithm interaction is a growing phenomenon of interest as the use of machine learning (ML) capabilities in everyday technologies becomes more commonplace. In the workplace, such developments raise questions about how people not only make sense of algorithmic actions, but also figure out ways to collaborate with tools and systems that integrate algorithmic outputs. We draw on a field study of IT infrastructure design and report on the experiences of highly-skilled IT architects with the natural language processing (NLP) capabilities in an intelligent system under development to support their solution design work. While architects were supportive of the potential of NLP to enhance their solutioning work, they faced challenges in integrating such capabilities into their existing collaborative work practices. We discuss how these findings add nuance and complexity to discourse around the future of work.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {143},
numpages = {23},
keywords = {IT infrastructure design, future of work, workplace transformation, requirements analysis, computer-supported cooperative work, work practices, natural language processing, human-algorithm interaction}
}

@misc{ScratchI75:online,
author = {},
title = {Scratch - Imagine, Program, Share},
howpublished = {\url{https://scratch.mit.edu/}},
month = {Sep},
year = {2022},
note = {(Accessed on 09/07/2022)}
}

@misc{Dynamicl51:online,
author = {},
title = {Dynamicland},
howpublished = {\url{https://dynamicland.org/}},
month = {Sep},
year = {2022},
note = {(Accessed on 09/07/2022)}
}

@article{bruckman1998community,
  title={Community support for constructionist learning},
  author={Bruckman, Amy},
  journal={Computer Supported Cooperative Work (CSCW)},
  volume={7},
  number={1},
  pages={47--86},
  year={1998},
  publisher={Springer}
}

@article{wang2017literature,
  title={A literature review on individual creativity support systems},
  author={Wang, Kai and Nickerson, Jeffrey V},
  journal={Computers in Human Behavior},
  volume={74},
  pages={139--151},
  year={2017},
  publisher={Elsevier}
}

@incollection{roque2016supporting,
  title={Supporting diverse and creative collaboration in the Scratch online community},
  author={Roque, Ricarose and Rusk, Natalie and Resnick, Mitchel},
  booktitle={Mass collaboration and education},
  pages={241--256},
  year={2016},
  publisher={Springer}
}