\section{Discussion}\label{sec:discussion}
Our work contributes new insights into the use of \LPIMs{} in the design process by addressing our initial research questions:
\begin{itemize}
    \item RQ1: \textit{How does using prompt-based image generation change the design process?} Our qualitative results show that \LPIMs{} allow for rapid exploration of the design space, and designers use them as a new form of reflective design material by formulating descriptions of their image ideas, then coming up with scenarios for expressing, testing, and finally refining their prompts either by affirming their initial image ideas or formulating new ones.
    
    \item RQ2: \textit{How do prompt-based image generation models change collaborative dynamics during design?} Based on the pair collaborations we observed, prompt-based image generation also allows for fluid collaboration and rapid prototyping for the collaborative synthesis of ideas. However, prompt visibility and the non-determinism in current models modulated the effectiveness of such collaboration.
\end{itemize}

Informally, we also observed that participants were delighted both by the surprisingly good images and images that were ``hilariously bad''.
% TODO: Need Emily P#
% (\Emily{}). 
In addition, many participants reported that their experience with \imagen{} was more ``fun.'' (P4) Below, we discuss some emergent questions based on our findings. % Add something here about how sticky and fun it is. 
% \todo{this block of text above (everything between 5 and 5.1) could be shortened or removed for space}

\subsection{Tool support for prompts as an indirect design material}
We found that the indirect nature of prompting both supported the design process (by augmenting creative freedom) and made it more challenging (while participants worked on rephrasing prompts to match intent). In some ways, prompts occupy a similar role in visual design as HTML did in early web design. By seeing how a webpage was constructed, designers could rapidly adopt good ideas, remix them, and popularize them widely. In such designerly practices, the role of Web browsers was also key -- by making ``View Source'' a universal feature, browsers likely transformed millions of people from web ``readers'' to web ``writers.'' Our work suggests that a similar ``View Source'' feature would also catalyze visual design. For instance, models could embed prompts as image EXIF data. Such tool support could also allow for more straightforward iteration and remixing and make prompt-based image generation even more accessible. For example, one could imagine mixing a partner's prompt (or prompt embedding) into one's prompt, creating a hybrid prompt. Finally, since prompts are text, many ideas from text editors and version control may also be relevant. For example, collaborating partners could send each other suggested edits, similar to text editors, or discover good prompting strategies (such as including clauses like \prompt{framed art}). Other materials such as tutorials or can also aid in this discovery (already, prompt guides are emerging).  

In addition, we can build on our observation that participants saw prompts as indirect design materials interpreted by opinionated image models. In light of these challenges, future work could empower end-users to control the model properties better. For example, users could swap out back-end models or fine-tune models on specific data (e.g., the same base model could have its weights fine-tuned by training on illustrations to help create art or furniture pictures for use by interior designers).

Low-level properties (e.g., position and layout) were challenging to control and remain an open question. Finally, while participants pushed back on opinionated models through prompt design, it may also be beneficial to develop models that are constrained in their output (e.g., not to crop parts of a salient object).


\subsection{Support for iterative design with prompts}
Prompt-based models are currently being optimized for "one-shot" interactions: each run of the model uses a random seed without the ability to be anchored on prior results. Interestingly, although models have been optimized for single-shot accuracy, users may be treating it more as a thinking tool, working incrementally, rephrasing, steering, and backtracking as a fundamental part of their design process.  %Yet, as noted in Section \todo{TODO}, the indirect nature of text-based prompting can also catalyze the design process, enabling users to take on new artistic identities through text alone. 

In the future, supporting \textit{iterative prompting} as a first-class objective would better allow people to use prompts as flexible design material. For example, users could toggle the random seed on and off depending on whether they were iterating on a design or starting in a new direction. Alternatively, they could bias the model to constrain or broaden exploration. Allowing users to navigate a history of states could also support exploring multiple ideas and backtracking. Since users may spend an inordinate amount of time rephrasing prompts to get the model output they desire, it may be valuable to combat potential design fixation by explicitly supporting the parallel exploration of \textit{multiple} ideas, a process that is known to improve design results \cite{dow2010parallel}. {A recent study showed that designers can directly interact with concepts in the model latent space by using semantic guidance to steer the diffusion process along several directions which enabled them to perform more sophisticated image composition and editing \cite{kwon2022diffusion}.}




\subsection{Multimodal affordances} 
The multi-modality of prompt-based models also creates new affordances. For example, such multimodality might help with greater control. In the current study, participants could not quickly iterate on a promising image result (e.g., ask the model to generate more results like Image 2, but with a bigger birthday cake) and instead resorted to updating the prompt with the desired property (e.g., giant birthday cake). Similarly, model support for multimodality might help with better composited images, for instance by moving an object into the background with pointer selection, and extending images for a wider field of view (with text prompts). Such affordances would improve iteration, and help designers express more complex visual concepts. 
