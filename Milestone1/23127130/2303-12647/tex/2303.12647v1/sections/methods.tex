
\section{\LPIM{}}
% below description is from Imagen https://imagen.research.google/ 
Participants in our study used a \LPIM{} that has not been released to the public. This model, \Imagen{}, is a text-to-image diffusion model that achieves a very high degree of photorealism and a deep level of language understanding (comparable in quality to DALL-E 2 \cite{ramesh2022DALLE2,dalle2Web}, Imagen \cite{imagenWeb}, and Parti \cite{partiWeb}). {The model is a "diffusion model". Diffusion models are trained by first destroying training data through the successive addition of noise, and then learning to recover the data by reversing this noising process. After training, a diffusion model can be used to generate data by simply passing sampled noise through the learned de-noising process. To guide the reconstruction trajectory, more recent implementations of diffusion models use text, semantic maps, or other images to condition what possible image should be generated (reconstructed) from the space of all possible options, with different probabilities (also called the latent space). Our model uses text to guide reconstruction.} In particular, it uses a generic large language model, trained on text-only corpora, for encoding text inputs (prompts), and a specially-trained diffusion model for generating images. 

{While the objective of this paper is not to study a specific TTI model, to help contextualize our results, we note that previous work demonstrated that the \Imagen{} model exhibits state-of-the-art performance on automated metrics (FID), human-rater comparable performance on text-image alignment to the MS-COCO dataset itself, and a high degree of human-perceived photorealism in generated images (redacted reference).}
%
%{While the objective of this paper is not to study a specific TTI model, to help contextualize our results, Table~f{tab:model-perf} compares the performance of the model we use with a few other text-to-image models, showing both automated metrics (FID) and human-preference metrics using images from Microsoft COCO}~\cite{lin2014microsoftCOCO}{ (We show metrics for images without people, mirroring our task design.) Qualitatively, images produced by the model we study are of high-quality, but not indistinguishable from non-generated images (Photorealism score $< 50\%$).} 
While \Imagen{} can often produce legible, correctly spelled text in images, it is not guaranteed to do so. In addition, generated images are constrained to be square and of fixed resolution and size. 

% \begin{table}[]
% \begin{tabular}{llll}
% \hline
% \textbf{Model}                                  & \textbf{\begin{tabular}[c]{@{}l@{}}Zero-shot\\ FID-30K\end{tabular}} & \textbf{Photorealism} & \textbf{Alignment} \\ \hline
% DALL-E~\cite{ramesh2021DallE}                                 & 17.89                                                                &                       &                    \\ \hline
% LAFITE~\cite{zhou2021lafite}                                 & 26.94                                                                &                       &                    \\ \hline
% GLIDE~\cite{nichol2021glide}                                  & 12.24                                                                &                       &                    \\ \hline
% DALL-E 2~\cite{ramesh2022DALLE2}                    & 10.39                                                                &                       &                    \\ \hline
% Validation dataset (original images; no people) &                                                                      & 50.0\%                & 92.2 ± 0.54        \\ \hline
% \Imagen{} (Our Work)                               & 7.27                                                                 & 43.9 ± 1.01\%         & 92.1 ± 0.55        \\ \hline
% \end{tabular}
% \caption{{(NEW TABLE) Comparison of model performance of model used in our study with a few other text-to-image models on the MS-COCO 256 × 256 dataset. FID is an automated image-fidelity metric. Photorealism is response to question ``Which image is more photorealistic (looks more real)?'' given two randomly chosen images from the dataset -- if images are equally photorealistic, this metric would be 50\%. Alignment is answer to ``Does the caption accurately describe the above image?'' (``yes'', ``somewhat'', or ``no'', coded as 100, 50, and zero respectively). For human preference metrics, the dataset was filtered to remove prompts containing
% one of man, woman, person, adults, player, workers, etc. (Table adapted from [Redacted reference.]}}
% \label{tab:model-perf}
% \end{table}

Participants used \Imagen{} through a web-based user-interface. This web-based UI has one text box for the input text (prompt) and generates eight candidate images at a time (presented in two rows as a $4x2$ grid.) Generating these images takes approximately 20 seconds once a prompt is entered. The UI currently does not record a history of past prompts used. Because \Imagen{} uses a random seed as input (which is not visible to the user), users see different image results on consecutive runs of the model, even when providing an identical prompt input. Both the prompt and the generated images are saved, and a unique URL is generated for every image-generation run, allowing easier sharing. (Visiting this URL reloads the previously generated images.) The current UI does not have any other collaborative features. {Finally, the authors of this paper had no involvement in the creation of the model or the interface used in the study.}

Our study focused on the practices of non-professional designers. We are  motivated to study how practices of non-professional designers might change particularly because TTIs may empower those without professional artistic or design training to create images with a textual description alone rapidly.
TTIs may also allow non-professional designers  to rapidly prototype, see and borrow from examples, and share multiple designs, accruing benefits that professional designers obtain from these practices~\cite{dow2010parallel}.


\subsection{Participants}
Participants in our study were invited from a pool of employees at a large, US-based tech corporation who filled out a survey suggesting that they have used \Imagen{} or similar models in the past. 

It must be noted here that our study makes a distinction between \emph{design practices} -- i.e. the individual and collaborative cognitive processes common to the task of design, and the practices of professional designers, which are learned through participation in this community of practice. Our paper is focused on the former as we are interested in the effects of TTI models among non-professional designers. 

As a result, {we filtered this survey to find only those who had been exposed to these models outside their job responsibilities and who were not professional designers (such as UX designers, visual designers, etc.). Specifically, to target non-professionals, we narrowed our pool to participants who reported that they had interacted with these models as ``part of your creative work pipeline'' or ``out of curiosity (not work-related)''. For practical reasons, recruiting those who had some initial exposure to the model also allowed us to observe the task within the scope of the 1-hour study. }

From this pool, we invited 16 participants across the company to participate in our design study. 14 participated (11 identified as male, three as female.) {Participants had a variety of job roles, from sales/marketing, software development, and project management. We sought to balance the participant pool to include both tech and non-tech participants.} Two participants did not fill out the pre-study or post-study survey but fully participated in our study.  We present the results of the survey without these participants. Participants were given a company-internal gift card {valued at US \$30)} for participating in the study. 

 

Two expert visual designers, both identifying as female (working as UX designers in the company), rated participants' final outputs blind to experimental conditions and participant identity.

{Because the model was not released to the public (at the time the study was run), we were limited to employees working at our organization. Despite this limitation, the deployment allowed us to gain fruitful insights from people across a range of different job roles, including non-tech roles. As TTI models become more widely available, future work should examine the generalizability of our results.}

\subsection{Study design}


\subsubsection{\IS{} as a baseline}
{To find a reasonable baseline condition to compare against how participants used TTIs, we conducted pilot experiments to find existing design practices among non-professional designers.  We found that participants overwhelmingly started with searching for images online, and if necessary, editing these images in slide decks or similar software.} 

While reasons varied (from thinking of returned images as inspiration or as building blocks to be used right away), \IS{} {enables users to find images that depicted what they wanted using high-level, natural language descriptions. Like TTI, Image Search also uses text as input.

%This influenced our decision to use Image Search as a baseline condition in our study. In addition, Image Search has the added benefit that it produces imagery very quickly (currently even faster than TTI models), and both methods use text as input.

While a generative image-editing tool might seem like a more natural comparison to generative TTI models than} \IS{}{, which only surfaces pre-existing images, notably none of our pilot participants used generative image-editing tools (e.g. Adobe Illustrator) or machine learning-powered image generation (e.g. GANs). Given the complexity of existing image-generation tools like Illustrator relative to TTI, such a comparison might also make this comparison unfair.
}
\subsubsection{Procedure}
Our study was conducted entirely online, with participants using Google Meet. Participants shared their videos so they could see each other and their active window to see each others' work. In addition to the participant, one or two experimenters joined the video call. Participants were asked to share their browser window to allow both the experimenter and their partner to follow their progress. Before the start of the experiment, participants filled out a short demographic survey after consenting to participation. {Before proceeding with the study, we reminded participants that we had no involvement in the creation of the model or the interface used in the study, and that we welcomed their honest reflections throughout the study.}

Participants were assigned to a design partner at the start of the study. Our study was designed as a within-subjects study with two design sessions. In one design session, in the \IS{} condition, participant pairs were allowed only to use \IS{} (with Google \IS{}) -- participants could perform an unlimited number of queries, and use resulting images in their invitation. In the other design session, in the \Imagen{} condition, they were additionally allowed to use \Imagen{}; participants could similarly generate images for an unlimited number of prompts, and use resulting images. Each design session lasted 20 minutes, in which participants were asked to create a complete design. The two experimental conditions were counterbalanced, so half the pairs of participants completed designs with \IS{} alone first, and the other half completed designs with additional access to \Imagen{}.

Throughout the study, participants worked with the same assigned design partner. In the first design session, the pair of participants collaboratively designed a party invitation to the birthday party of Alice from \textit{Alice in Wonderland}. In the second session, the pair of participants collaboratively designed a party invitation to celebrate the 55th anniversary of the first Moon Landing. Appendix~f{app:task-descroption} includes both design briefs in full. Because \Imagen{} does not allow the creation of images that include photo-realistic people, participants were told not to use photo-realistic images of people in their design. Further, to respect creator rights and to simulate a realistic design task, participants were also not allowed to use images that were under copyright. (Images that were public domain or licensed under a Creative Commons license were allowed -- the \IS{} interface has filters to search for such images.)~{This restriction is similar to prior work in this area (e.g.}~\cite{dow2010parallel}{.)}

In both sessions, participants created their invitations in Google Slides. Using Slides, participants could for instance crop images, compose multiple images into a single composition, and add text. Google Slides does not currently have features to remove backgrounds from images, or perform other image editing, such as adding filters. We chose to use Google Slides rather than a professional tool such as Adobe Illustrator because we wanted to study the practices of non-professional designers. Furthermore, Google Slides is used extensively in the organization we studied, ensuring that all participants would be familiar with how to use it. Participants were told that their designs would be rated for creativity, appropriateness to the design brief, and completeness and that the best designs would receive a prize.


After the study, each participant privately filled out a survey self-assessing their design along the dimensions of creativity, appropriateness, and completeness (based on survey scales adapted from~\cite{dow2010parallel}) {(Modifications: tailor language to invitations instead of advertisements in the original study, replace adherence to the ``client's theme'' with adherence to the theme, omit questions related to graphic design principles such as typographic balance because that was not the focus of our study)}. We also asked participants about their collaborative experience (based on ``relationship conflict'' scales from \cite{jehn2001dynamic}). We also asked two expert designers (who did not participate in the study and were not involved in the research) to rate participants' designs along these dimensions. These raters used the same scales as participants and were blind to the condition. {(Each rater rated all designs produced.)}



\subsection{Data collection and analysis}
Our study resulted in pre-study survey data and video recordings of all the design sessions. {Recordings were automatically transcribed, and corrected by either of the first two authors.} The {first two} authors analyzed the video transcriptions and noted comments on participants' non-verbal interactions for the qualitative analyses. The final corpus included 168 pages of transcripts (48169 words). The first two authors each reviewed the transcript data independently, looking for ways of explaining the experimental conditions. In this process, the authors analyzed each transcript using emic codes that emerged from the study sessions \cite{miles1984drawing,patton1990qualitative}. After a final coding frame was developed, the second author coded all the transcripts. If new codes emerged, the first two authors discussed discrepancies in the analyses until they reached an agreement. The final list of codes, their definitions, and examples is included in the appendix.
This process was used to develop categories, which were then conceptualized into broad themes after further discussion. The two first authors extracted salient themes from the study transcripts and independently generated hypotheses and points of discussions \cite{braun2006using}. Using these data, the authors participated in two interpretation sessions to arrive at the primary themes reported in this paper. {While we are limited by our sample size, we did not observe large differences in participant processes by gender.}

