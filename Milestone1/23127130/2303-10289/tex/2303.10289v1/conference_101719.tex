\documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{fancyhdr}
\doublespacing

%%%%%%%%%% Original setting %%%%%%%%%%%%%
\setlength{\abovedisplayskip}{0pt plus 0pt minus 0pt}
\setlength{\belowdisplayskip}{0pt plus 0pt minus 0pt}
\setlength\abovedisplayshortskip{0pt plus 0pt minus 0pt}
\setlength\belowdisplayshortskip{0pt plus 0pt minus 0pt}

% \setlength{\abovedisplayskip}{3pt plus 1pt minus 1.0pt}
% \setlength{\belowdisplayskip}{3pt plus 1pt minus 1.0pt}
% \setlength\abovedisplayshortskip{0.0pt plus 2pt}
% \setlength\belowdisplayshortskip{3pt plus 1pt minus 1.0pt}


% \usepackage[skip=2pt]{caption}
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% \setlength{\topmargin}{-0.7in}
% \setlength{\belowdisplayskip}{4pt plus 1pt minus 1.0pt} 
% \setlength{\belowdisplayshortskip}{4pt plus 1pt minus 1.0pt}
% \setlength{\abovedisplayskip}{4pt plus 1pt minus 1.0pt} \setlength{\abovedisplayshortskip}{0.0pt plus 2.0pt} 
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}} 





\begin{document}

% \title{\textit{play-to-earn} in the Metaverse: Multi-Agent-Loss-Sharing (MALS) approach to Mobile Edge Computing}
\pagestyle{fancy}
\fancyhead[C]{This paper is submitted to IEEE Transactions on Wireless Communications (TWC), 2023.}
\title{Play to Earn in the Metaverse with Mobile Edge Computing over Wireless Networks: A Deep Reinforcement Learning Approach}

% \author{\IEEEauthorblockN{Terence Jie Chua}
% \IEEEauthorblockA{Graduate College\\Nanyang Technological University\\
% terencej001@e.ntu.edu.sg }
% \and

% \IEEEauthorblockN{Wenhan Yu}
% \IEEEauthorblockA{Graduate College\\Nanyang Technological University\\
% wenhan002@e.ntu.edu.sg }
% \and

% \IEEEauthorblockN{Jun Zhao}
% \IEEEauthorblockA{School of Computer Science \& Engineering\\ Nanyang Technological University\\
% junzhao@ntu.edu.sg }
% }
\author{\IEEEauthorblockN{Terence Jie Chua, Wenhan Yu, and Jun Zhao} \vspace{-40pt}
\thanks{The authors are all with Nanyang Technological University, Singapore. Email: JunZHAO@ntu.edu.sg 
}
 \thanks{A 7-page short version containing partial results is under review for the 2023 IEEE International Conference on Communications (ICC)~\cite{ICC2023MALS}. IEEE Communications Society (ComSoc) writes ``It is possible to submit the journal and conference version at the same time'' at \url{https://www.comsoc.org/publications/journals/ieee-transactions-wireless-communications/conference-vs-journal}}
}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
The Metaverse play-to-earn games have been gaining popularity as they enable players to earn in-game tokens which can be translated to real-world profits. With the advancements in augmented reality (AR) technologies, users can play AR games in the Metaverse. However, these high-resolution games are compute-intensive, and in-game graphical scenes need to be offloaded from mobile devices to an edge server for computation. In this work, we consider an optimization problem where the Metaverse Service Provider (MSP)'s objective is to reduce downlink transmission latency of in-game graphics, the latency of uplink data transmission, and the worst-case (greatest) battery charge expenditure of user equipments (UEs), while maximizing the worst-case (lowest) UE resolution-influenced in-game earning potential through optimizing the downlink  UE-Metaverse Base Station (UE-MBS) assignment and the uplink transmission power selection. The downlink and uplink transmissions are then executed asynchronously. We propose a multi-agent, loss-sharing (MALS) reinforcement learning model to tackle the asynchronous and asymmetric problem. We then compare the MALS model with other baseline models and show its superiority over other methods. Finally, we conduct multi-variable optimization weighting analyses and show the viability of using our proposed MALS algorithm to tackle joint optimization problems.

% demonstrate the feasibility of utilizing our proposed MALS algorithm to tackle joint optimization problems.
\end{abstract}

\begin{IEEEkeywords}
Metaverse, mobile edge computing, deep reinforcement learning, resource allocation.
\end{IEEEkeywords}


\section{Introduction}
\textbf{Background. }The emergence of the Metaverse has brought a myriad of opportunities \cite{goldman}, and play-to-earn is one of the most iconic features. Prior to the introduction of the Metaverse, players would typically play games that sit in their isolated virtual worlds. Each game's virtual world is disconnected from  other games' worlds and in-game tokens can only be used within the isolated game~\cite{browne_2021}. From an ownership standpoint, users in these traditional games barely have any ownership over their in-game earnings, tokens and characters as they ultimately belong to the game developers and creators. Yet now, the Metaverse allows players to gain ownership over their in-game token earnings and possessions through blockchain contracts~\cite{browne_2021}. These contracts certify the legitimacy and ownership of in-game items thus enabling their real-world value. One way players can become profitable is by playing the Metaverse games, such as those within virtual worlds \textit{Axie Infinity}, \textit{Decentraland} and \textit{The Sandbox}. Players can earn in-game tokens, gain in-game possessions, and trade them for real-world fiat currencies \cite{browne_2021}.  In this paper, we will coin the concept of ``earning potential'' to represent the maximum earning that a player can obtain, expressed by a function of the parameters (in particular, the resolution of the in-game graphics) that the player has. The earning potential will be a part of our problem formulation.

\textbf{Differences between this Journal and Our ICC Conference Paper~\cite{ICC2023MALS}. }This journal is an extension of the work done in ~\cite{ICC2023MALS}. In this journal, we considered the worst-case (lowest) earning potential, across users and transmission steps, in our optimization function in the downlink stage. This is in contrast to our ICC conference version, where we considered the sum of earning potential of the users, across transmission steps. Similarly, for the uplink stage, we considered the worst-case (greatest) battery charge expenditure, across users and transmission steps, in our optimization function. This is in contrast to our ICC conference version, where we considered the sum of battery charge expenditure of users across transmission steps. Another notable difference is that we added a multi-variable optimization weighting analysis in this journal to show the viability of using our algorithm to tackle joint optimization problems. Finally, we provide more details pertaining to the rationale of our work and algorithm.

\vspace{-0.4cm}

% The introduction and rapid expansion of the Metaverse has brought a myriad of opportunities \cite{goldman}, and \textit{play-to-earn} is one of the most iconic features. Traditionally, and prior to the introduction of the Metaverse, players' do not have complete ownership over their in-game earnings, assets and tokens as they ultimately belonged to the game developers and creators. Yet now, the existence of the Metaverse allows players to gain ownership over their in-game token earnings and possessions through blockchain contracts~\cite{browne_2021}. These contracts certify the legitimacy and ownership of in-game items thus enabling its real-world value.

\subsection{Problem}

% With rapid development within the gaming industry, the general trend of modern play-to-earn games such as \textit{GRIT: A Wild West Royale}~\cite{playtoearn1}, and \textit{The Walking Dead: Empires}~\cite{playtoearn2} are also moving in the direction of delivering high-quality graphics.

\textbf{Compute-intensive Mobile Augmented Reality (MAR). }Many play-to-earn games are developed for mobile devices~\cite{roos_2022} enabling people to play them whenever and wherever possible to earn in-game currency. Compute-intensive Metaverse play-to-earn games such as \textit{Polkacity}~\cite{polka_city} and \textit{Reality Clash}~\cite{english} are attributed to high-resolution in-game graphics, complex in-game interactions, and increasing adoption of Mobile Augmented Reality (MAR). The core of augmented reality (AR) is to overlay physical world scenes with digital elements, creating a highly immersive and interactive visual experience for AR users~\cite{goldman}. Despite rapid development in AR technologies, current mobile computing technologies
 %are insufficient to power, and 
are struggling to keep up with the increasing demands of AR applications such as play-to-earn games. Yet in the context of AR play-to-earn games, the fluidity of the gaming experience should be highly regarded as it influences the players' gameplay. One viable way of dealing with the lack of mobile computing power is to offload the computation and rendering of play-to-earn games to an edge computing server~\cite{cai2022compute}.

\textbf{Inefficient edge computing concerns. }In the context of the play-to-earn Metaverse games, an in-game graphics offloading example pipeline can be described as such: In a single iteration of transmissions, high-resolution in-game graphics can first be downloaded from the Metaverse Service Provider's Base Stations (MBSs) to the players, in which players will view in-game scenes and take actions, altering the in-game scenes. For clarity, note that MBSs can be viewed as edge-computing base stations assigned by the Metaverse service provider. These altered in-game scenes are then offloaded back to the MBSs for computation. In the \textbf{downlink} leg, delivery of poor in-game graphic resolution and large downlink latency is often a concern. Take the following scenario as an example - Several MBSs are catering in-game graphic computation services to multiple players; i.e., user equipments (UEs). If there is inefficient allocation of users to MBSs (i.e., too many UEs are allocated to the same MBS or UEs are allocated to MBS situated far away), there will be greater signal interference or lower transmission efficiency, which reduces the overall rate of transmission of in-game graphical data. In such a case, this results in an increase in downlink latency and also overall poor in-game graphical smoothness. In the \textbf{uplink} leg, there is often a concern pertaining to both the latency of uplink transmission and UE device energy usage. Specifically, if the selected power for the uplink transmission is optimal, the effective signal between the UEs and MBS will be improved. This will result in a decrease in uplink transmission latency. Conversely, a poor UE uplink power selection may result in greater signal interference between UEs and MBS, reducing the effective signal between UEs and MBS and hence increasing the latency for uplink transmission. This results in a decrement in in-game earning potential, fluidity, and hours of play.

% Many of the play-to-earn games are developed for mobile devices~\cite{roos_2022}, enabling people to play them whenever and wherever possible to earn in-game currency and items. With rapid development within the gaming industry, the general trend of modern play-to-earn games such as \textit{Polkacity}~\cite{polka_city}, and \textit{Reality Clash}~\cite{english} are also moving in the direction of delivering high-quality, Mobile Augmented Reality (MAR) graphics, rendering these Metaverse game graphics to be much more compute-intensive.


\textbf{Formulated Problem. }Finding an optimal strategy to maximize the UEs' play-to-earn profitability, while minimizing device energy consumption and transmission latency is non-trivial. First and foremost, a Metaverse play-to-earn framework will have to account for (i) profitability of players, (ii) fluidity (smoothness) of in-game player experience, and (iv) player device's battery level and energy consumption, in the interest of player retention. These three factors are key considerations when developing a Metaverse play-to-earn framework. We formulate the problem in a way where the Metaverse Service Provider (MSP) aims to minimize the (i) graphical data latency of downlink transmission delay, (ii) latency of uplink data transmission, (iii) and worst-case (greatest) UE device battery charge expenditure, while maximizing (iv) the players' worst-case (lowest) in-game earning potential influenced by graphic resolution, by optimizing the UE-MBS allocation, and UE uplink transmit power. This is to ensure player retention by improving their overall in-game fluidity, experience, profitability and UE battery charge for long periods of play. We choose to maximize the worst-case (lowest) profit and minimize the worst-case (greatest) batter charge expenditure across the UEs as our aim is to ensure that no players' interests are neglected, in hopes of player retention.
\vspace{-0.3cm}

% This problem is non-trivial as there is a clear trade-off between downlink transmission latency and in-game earning potential influenced by graphic resolution, and trade-off between UE battery consumption for uplink transmission and uplink transmission latency. 

\vspace{-10pt}\subsection{Our Approach and Rationale \vspace{-5pt}}\label{approach}To tackle these abovementioned problems, we introduce a novel Multi-Agent Loss-Sharing (MALS) reinforcement learning (RL)-based orchestrator which houses a downlink (DL) agent and an uplink (UL) agent that optimizes the UE-MBS allocation and uplink transmission power to maximize the utility function (the aim of the Metaverse Service Provider).  Specifically, we designed the DL agent to be a discrete-action space RL agent which chooses the UE-MBS allocation, and a continuous-action space UL agent which selects UE uplink power to maximize the utility function. However, instead of providing the agents with an overarching objective function, the agents are assigned to tackle objectives that are within their control. It is intuitive to have different objectives in each DL and UL stages as the data transmitting party in each stage, and hence, the variables in which they control, differ. This is to avoid ``confusing" the agents while providing the agents with more explicit goals. Note that the agents are \textit{asymmetric}, in that they have  separate objectives and action space types. The UL and DL agents are also executed \textit{asynchronously}. The DL agent will first select the UE-MBS allocation, conduct the DL data transfer, followed by the UL transmission and UL power selection by the UL agent. Although the DL and UL processes seem distinct, asymmetric, and are executed asynchronously, it is important to optimize both stages concurrently.

% We do not employ the traditional Multi-Agent Reinforcement Learning (MARL)~\cite{lowe2017multi} as our proposed problem is asynchronous. In our proposed scenario, the downlink and uplink agent will execute actions in an alternate fashion. However, the traditional MARL considers all agents' actions and each agent executes actions simultaneously in each time-step, rendering the traditional MARL inefficient in our proposed scenario.

\textbf{Reinforcement learning approach over convex optimization. }We proposed a complex play-to-earn mobile edge computing problem which is not naturally convex as the UE-MBS allocation is constrained to integer values. By utilizing convex optimization approaches, we can only yield approximate solutions by relaxing the constraints. Secondly, our proposed problem involves the optimization of UE-MBS allocation. The complexity of the allocation problem is large, due to the possible combinations of UE-MBS allocation and the number of transmission iterations (sequential problem). Such high-complexity problems render convex optimization approaches infeasible. Thirdly, we consider the cumulative in-game profit and cumulative UE device energy expenditure in our problem. This renders our problem highly sequential and the objective functions cannot be solved independently at each transmission iteration.

% In addition, the UE battery charge in our work is collective and depleting in each time step. Convex optimization techniques would need to model this sequential variable in its objective function at each transmission iteration, rendering it computationally inefficient.


% \textbf{Firstly}, our defined problem scenario aims to reflect as ``true-to-real world" scenario. As such, our problem formulation (objective function and constraints) are not naturally convex. There are three options we considered: (1) \textit{Redefining our problem as a convex one. }This would certainly make the defined problem much easier to solve, but does not represent an accurate solution of the real-world. A clear example of problem redefinition for proposed problem would be the \textit{constraint relaxation} of the discrete UE-MBS allocation into a continuous one. (2) \textit{Finding an approximate solution to the \mbox{non-convex} problem .} Solution will then only be an approximate solution and it is difficult to gauge how far it is from the optimal solution. (3) \textit{Adopting Deep Reinforcement Learning techniques .} Deep Reinforcement Learning (RL) techniques may not provide the quickest nor most optimal solution. However, with sufficient model training, the RL agents are able to handle complex \mbox{non-convex} problems and propose close to optimal solutions. \textbf{Secondly}, the UEs' battery charge at each transmission iteration is sequential (collective) and changing, which when modeled as a convex optimization problem, will have its own objective function in each transmission iteration. This increases the solution search space indefinitely, rendering convex optimization techniques as an infeasible approach.

\textbf{What is Multi-Agent-Loss-Sharing? }We proposed a novel Multi-Agent-Loss-Sharing (MALS) reinforcement learning-based algorithm to handle the training of two agents in a mix-cooperative scenario. The MALS algorithm builds on the actor-critic (AC) based reinforcement learning algorithms. Similar to traditional interacting, independent dual agents (IDA), each agent will have an actor which executes actions independent of information other than the state it receives. In contrast to IDA, each agent under the MALS implementation will share a critic model with multiple heads which takes in inputs from both actors to calculate personalized advantage values for the update of each actor. The critic losses from each head are summed and used to update the critic. The rationale for summing the losses of both critic heads is to facilitate the cooperative training of both agents. This ensures that the training process reduces the critic multi-head losses concurrently, not neglecting either agent's model improvement.

% There exist several adaptations of the MARL algorithm that have obtained impressive results~\cite{lowe2017multi,rashid2018qmix}.
% Each agent in a MARL set-up possesses an actor and a critic.

\textbf{Why not apply the CTDE framework? }Similar to our proposed MALS implementation, traditional Multi-Agent Reinforcement Learning~\cite{lowe2017multi} utilizes an actor-critic, Centralized Training and Decentralized Execution (CTDE) framework. The actor of each agent executes independently (decentralized execution). In contrast to MALS, the critic for each agent under the CTDE framework will share a similar model, taking into account all other agents' actions (centralized training) in each time step. This CTDE framework is unsuitable to tackle our proposed problem as: (1) \textit{our proposed problem is asymmetric. }The agents in our work have distinct action spaces, where the DL agent selects discrete-valued UE-MBS allocation while the UL agent selects continuous-valued UE uplink transmission power. The underlying model and hence critic are dissimilar, rendering CTDE an unsuitable method. (2) \textit{Our proposed problem is asynchronous. }In our proposed scenario, the downlink and uplink agents will alternately execute actions. However, MARL considers all agents' actions and each agent executes actions simultaneously in each time step. When dealing with asynchronous action executions, each agent's observation space will only contain the state and action of the other agent. This results in a highly sparse state input, which renders CTDE to be computationally inefficient in tackling our proposed scenario.

% A traditional POMDP transition can be written as: $\left [S_1^1\times A_1^1  \rightarrow R_1^1 , S_1^2 \right ]$, where the superscript and the subscript denote the transmission iteration and agent index, respectively. However, as the two agents in our work are executed asynchronously, the POMDP is written as: $\left [S_1^1 \times A_1^1 \rightarrow S_2^1, ~R_1^1 \right ]$, $\left [S_2^1 \times A_2^1 \rightarrow S_1^2, R_2^1 \right ]$.

\textbf{Multi-agent sequential dependency. }Moreover, the formulation of an asymmetric and asynchronous MEC resource allocation problem causes dependency between agents in the Partially Observation Markov Decision Process (POMDP). In a conventional deterministic, single-agent environment, the environment is only impacted by a single agent's actions. Hence the environment and rewards perceived by the agent are a result of its own actions. However, in a two-agent, asynchronous execution environment, the environment and rewards perceived by an agent are not solely a consequence of its own actions, but also the action of the other agent. Hence the transition probabilities learnt by an agent are subject to the decisions made by the other agent.  Consider the following scenario: An optimal MBS-UE channel allocation in the DL process may result in high data transfer rates from the MBSs to UEs. Consequently, the UEs will receive more data. This may not always be good for such a multi-stage system, as more data received in the DL stage means more data needs to be transmitted in the UL stage. Therefore, the UL stage may face a high data traffic flow, resulting in an overall larger UE power required for the UL transmission. This may result in greater interference due to an overall higher power output by each device. There is strong dependency between an agent facilitating the DL process and the agent facilitating the UL process. Therefore, we have to consider the concurrent orchestration of both UL and DL stages. We seek a new reinforcement learning structure to tackle an asynchronous setting.

% Furthermore, the consideration of an asymmetric multi-process transmission and existence of two agents with different states and actions lead to a new problem within the  Partially Observable Markov Decision Process (POMDP). In traditional POMDP, the transition in each step can be formulated as $\left [S_1^1\times A_1^1 \times \rightarrow R_1^1 , S_1^2 \right ]$. However, in a two-agent POMDP, the transition has to be represented as such: $\left [S_1^1 \times A_1^1 \rightarrow S_2^1, ~R_1^1 \right ]$, $\left [S_2^1 \times A_2^1 \rightarrow S_1^2, R_2^1 \right ]$, which means the state of $Agent2$ is dependent on and influenced by $Agent1$, and they are unable to perform actions simultaneously. Here the superscript and subscript denote time step (transmission iteration) and agent index, respectively.



\textbf{Play-to-earn under mobility. }The existence of Metaverse games with play-to-earn features on mobile devices would mean that players are expected to be on the move. As players move, their distance from the MBSs changes. This changing distance results in varying channel gain and hence effective rate of data transfer. In our proposed Metaverse play-to-earn framework, we have to take into account players' (UEs') mobility.
\vspace{-0.3cm}






% In the \textbf{downlink} leg, delivery of poor in-game graphic resolution and large downlink latency is often a concern. Take the following scenario as an example - Several MBS are catering in-game graphic computation services to multiple UEs. If there is inefficient allocation of users to MBSs (i.e., too many UEs are allocated to the same MBS or UEs are allocated to MBS situated far away), there will be greater signal interference and lower transmission efficiency which reduces the overall rate of transmission of in-game graphical data. In such case, this results in both an increase in downlink latency and also overall poor in-game graphical smoothness. In the \textbf{uplink} leg, there is often a concern pertaining to both the uplink transmission latency and UE energy consumption. Specifically, there is a trade-off between achieving lower uplink transmission latency and user devices' battery consumption. If UEs allocated larger power to transmit the graphical scenes, the uplink latency will be significantly reduced, while the energy consumption will be significantly increased. Conversely, if the user devices uses lesser power to facilitate the uplink transmission of the in-game graphics, there will be greater conservation of device battery, at the expense of uplink latency.

% \subsection{Our Problem Scenario} 
% From the discussions above, we arrived at a simulated environment design in which players (UEs) and Metaverse Service Provider Base Stations (MBSs) are distributed randomly across a geographic space, in which players will move about randomly across this space. In each iteration, in-game graphical data will be downloaded from the MBSs to the UEs, in which users will perform actions, causing a change in the in-game graphics. The changes in in-game graphics are then offloaded to the MBSs for computation and rendering. Since UEs are on the move, the location, and hence distance from the MBS are changing in each transmission iteration. This results in possible handovers (change in UE-MBS allocation). We designed the system in such a way that the objective of the MBSs is to minimize the (i) graphical data downlink transmission delay while maximizing (ii) players' in-game earning capabilities, by optimizing the UE-MBS allocation. On the other hand, the players' (UE) objectives are to minimize the total uplink transmission latency and total battery consumption, while ensuring that the UE's battery charge do not fall below zero, through optimizing the UE uplink power selection. This problem is non-trivial as there is a clear trade-off between user device power output for uplink transmission and uplink transmission latency.

% We do not consider utilizing a convex optimization approach as the UEs' battery charge at each transmission iteration is sequential (collective), rendering convex optimization techniques infeasible.



\vspace{-10pt}\subsection{Existing Literature and Our Contributions \vspace{-5pt}} 


% \textbf{AR and XR over wireless networks. }There are several works~\cite{chen2017resource,wang2021meta,liu2018edge,wang2020user} which have studied resource management or optimization problems concerning virtual, augmented or extended reality over wireless networks. Chen~\textit{et~al.}~\cite{chen2017resource} considered a resource allocation problem and devised a distributed machine learning algorithm to tackle it. Wang \textit{et al.}~\cite{wang2021meta} 
% introduced an indoor virtual reality scenario in which they utilized reinforcement learning to improve convergence speed and the sum of successful transmission probabilities by optimizing VLC access points (VAP) and user-base station association. Liu \textit{et al.}~\cite{liu2018edge} proposed an algorithm to tackle the trade-off between network latency and video object detection accuracy for mobile augmented reality systems. Wang \textit{et al.}~\cite{wang2020user}  aimed to minimize the energy consumption of users using mobile augmented reality systems by optimizing the MAR configuration as well as the radio resource allocation.

% , and via independent reinforcement learning agents, without considering the partially collaborative nature of interacting agents in carrying out data transmission tasks.

%%%%%%%%%%% have to comment out due to limitations in space %%%%%%%%%%%%
% \textbf{Task offloading to edge. }Previous works which focused on task offloading in mobile edge computing (MEC) have considered communication and computation latency between the user devices and edge servers. Tan et al.~\cite{tan2017online} proposed an online algorithm which aims to reduce latency by allocating compute tasks among cloud servers. 
% Similarly, the work by \cite{jia2016cloudlet} showed that the latency between edge devices and cloud could be minimized through a load-balancing framework. Tong et al.~\cite{tong2016hierarchical} aimed to reduce the task execution time by using a task dispatch algorithm and hierarchical cloud architecture to allocate tasks to different edge servers dynamically.

In this subsection, we first present related work and then highlight the contributions of this paper. We divide related studies into different categories: 1) Metaverse communication framework, 2) AR over wireless networks, and 3) deep reinforcement learning for task offloading, where the first two categories are also discussed in our recent conference paper~\cite{Chua2022}.


\textbf{Metaverse communication framework. } Since the Metaverse is still relatively new, limited studies consider Metaverse edge computing problems. In our conference paper~\cite{Chua2022}, we designed a vehicular Metaverse Augmented Reality socialization over 6G wireless networks scenario and devised artificial intelligence techniques to tackle their proposed optimization problem. In another of our conference paper~\cite{ICC2023Adversarial}, we devised a machine learning approach to tackle an adversarial patch detection in defence of digital twinning over wireless networks problem. The authors of~\cite{han2022dynamic} introduced an Internet of Things (IoT)-based digital twinning over wireless networks scenario and proposed a framework to tackle a wireless network resource allocation problem. Ng \textit{et al.}~\cite{ng2022unified} designed stochastic optimization-based resource allocation to obtain the minimum cost of a Metaverse   service provider in the education sector context. Xu~\textit{et~al.}~\cite{xu2022wireless} presented an incentive mechanism based on machine learning for VR in the Metaverse, using auction theory to obtain the optimal pricing and allocation, and deep reinforcement learning to accelerate the auction process.

% Li~\textit{et~al.}~\cite{li2022internet} surveyed how the IoT can be combined with the Metaverse.
% Yang~\textit{et~al.}~\cite{yang2022metafi} proposed an IoT-enabled pose estimation scheme for the Metaverse.




\textbf{AR over wireless networks. }Apart from Metaverse-related resource allocation studies, there are several works~\cite{chen2017resource,wang2021meta,liu2018edge,wang2020user} which have studied resource management or optimization problems concerning virtual, augmented or extended reality over wireless networks. Chen~\textit{et~al.}~\cite{chen2017resource} considered a resource allocation problem and devised a distributed machine learning algorithm to tackle it. The authors of~\cite{wang2021meta} studied the optimization of VLC access points (VAP) and user-base station association to empower virtual reality over wireless networks for indoor environments. Liu \textit{et al.}~\cite{liu2018edge} proposed an MAR over wireless network scenario and utilized convex optimization techniques to solve an object detection for MAR and latency of computation offloading problem. Wang \textit{et al.}~\cite{wang2020user}  aimed to minimize the energy consumption of users using mobile augmented reality systems by optimizing the MAR configuration as well as the radio resource allocation. In \cite{chen2018virtual}, the authors studied optimization problems which consider the Quality of Experience (QoE) of users.

% , with Chen \textit{et al.}~\cite{chen2018virtual} using a game theoretic approach, and Guo~\textit{et~al.}~\cite{guo2020adaptive} presenting both game theoretic and reinforcement learning approaches.

\textbf{Deep reinforcement learning for task offloading. }There are several works utilizing deep reinforcement learning to optimize task offloading~\cite{lu2020optimization,alfakih2020task, RLC}. However, these works consider the system as a single entity. In practice, the DL and UL processes may involve different parties in the transmission process and may have different objectives.

In contrast to these abovementioned works, there are several works which consider the concurrent optimization of several variables within their objective function and proposed reinforcement learning approaches to handle them. Guo \textit{et al.}~\cite{JO1} utilized a Centralized Training and Decentralized Execution (CTDE) framework to tackle handover and power selection. Similarly, He \textit{et al.}~\cite{JO2} adopted the CTDE~\cite{lowe2017multi} framework to address the user to channel allocation and transmission power selection problem. Nevertheless, the abovementioned works are not concerned with a multi-stage, asynchronous transmission scenario, as studied in our paper.

\textbf{Contributions.} We make the following contributions to the field:
\begin{itemize}
\item \emph{\textbf{Dual Joint Optimization:}} We formulate a multi-variable optimization problem for    play-to-earn enabled Metaverse with mobile edge computing.

\item \emph{\textbf{Multi-Agent-Loss-Sharing:} }We propose a novel \textit{asymmetric} and \textit{asynchronous} multi-agent loss-sharing reinforcement learning-based orchestrator to tackle a novel asynchronous and asymmetric wireless communication problem.

\item \emph{\textbf{Comparison of our proposed methods against baseline models:}} We compare our (i) proposed method MALS with baseline methods such as (ii) independent dual-agent (IDA) and (iii) CTDE~\cite{lowe2017multi} reinforcement learning algorithm. The comparison shows the superiority of our proposed method in handling asynchronous executions.

\item \emph{\textbf{Analyses of weighted utility objective functions:}} We provide in-depth analyses of how the different weights on each factor in the joint optimization functions influence reward and other variables. These analyses provide insights into solving multi-agent joint-optimization problems.

\end{itemize}
We organize the rest of the paper as follows. Section~\ref{models} covers the details of our Metaverse play-to-earn optimization problem while Section~\ref{RL} discusses the formulation of the problem in the context of reinforcement learning. Section~\ref{methodss} introduces our proposed novel MALS deep reinforcement learning model and discusses the algorithm in detail. In Section~\ref{experiment}, we carry out thorough experiments to compare different methods to showcase the superiority of the MALS algorithm in handling \textit{asymmetric} and \textit{asynchronous} problems. Analyses on the weighting of variables in the joint utility functions are also conducted to demonstrate the feasibility of our proposed approach across various weights of joint-variable optimization problems. Finally, Section~\ref{section:conclusion} concludes the paper.


% Note that intra-cell interference represents the interference between a sender and a receiver, which is caused by other transmitting entities that is assigned to the same receiver??. On the other hand, inter-cell interference represents the interference between a sender and a receiver caused by other transmitting entities on similar bandwidth that is assigned to a different receiver. 

\vspace{-15pt}\section{System Model \vspace{-5pt}}
\label{models}
\textbf{Problem Scenario. }
Consider the real-time downlink (DL) and uplink (UL) transmission of $N$ players from a set $\mathcal{N}=\{1,2,...,N\}$ players (UE) moving about. In each complete transmission iteration, each UE $i \in \mathcal{N}$ begins downloading Metaverse in-game scenes from an MBS. We consider both intra-cell interference and inter-cell interference in the DL transmission. Note that the intra-cell interference is the disruption of signal between a UE and an MBS, caused by wireless signals between other UEs assigned to the same MBS. Inter-cell interference is the disruption of signal between a UE and an MBS, caused by wireless signals between other UEs assigned to other MBSs. After the DL of in-game graphical data, we consider the UL transmission of the same set of $N$ players' (UE) changes to in-game graphical scenes, to its allocated MBS $(\mathcal{M} = \{1,2,...,M\}$. Each UE $i \in \mathcal{N}$ will upload their Metaverse in-game graphical scenes to their allocated MBS.  We consider both intra-cell and inter-cell interferences in the uplink transmission. We use iterations to refer to the time step. Each iteration includes downlink and uplink data transmissions. To simplify an already complex scenario, we consider the scenario in which users only make movement after each iteration of downlink and uplink data transmissions.


\begin{figure}[t]
\centering
\includegraphics[width=0.7\textwidth]
{system_model.pdf}
\caption{System model illustrating the interaction between the UEs and the MBSs, agents' objectives and variables-of-control.}
\label{fig:system_model}
\vspace{-0.5cm}
\end{figure}


\vspace{-13pt}\subsection{Downlink Communication \vspace{-5pt}}\label{sub:Communication-Model}

% Furthermore, we denote $r^{t} = \{r_{1}^{t},r_{2}^{t},...,r_{N}^{t}\}$ as the different in-game graphics downlink data transfer rate at transmission iteration $t$. Specifically, $r_{i}^{t}$, where $i \in \mathcal{N}$ is the downlink data transfer rate from the MBS $v$ to UE $i$ at iteration ${t}$.

Each of the MBS from $\mathcal{M}=\{1,2,...,M\}$ will be allocated to some UEs from $\mathcal{N}=\{1,2,...,N\}$. We define the size of the in-game graphics to be downloaded from the MBSs to the UEs as $D^t = \{D^t_{1}, D^t_{2}, ..., D^t_{N}\}$, where $D^t_i$ represents the in-game data size to be downloaded from an MBS to UE $i \in \mathcal{N}$ at iteration $t$. In addition, the UE-MBS association is defined as: $\boldsymbol{c}^{t}=(c_1^t, ..., c_N^t)$. Each of the $c_i^t$ where $i \in \mathcal{N}$ is allocated to an MBS $v$ where $v \in \mathcal{M}$ at iteration $t$. The effective signal, or \textit{signal-to-interference plus noise-ratio} between a UE and its assigned MBS at step $t$ can be defined as:
% \begin{align}
%     \Gamma_i^t=\frac{g^{t}_{v,i}p^{t}_{v,i} }{ g^{t}_{v,i}  \sum\limits_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t}p^{t}_{v,n}  + \sum\limits_{j \in\mathcal{M} \setminus \{v\}} g^{t}_{j,i} \sum\limits_{k: c_k^t = j} p^{t}_{j,k}+B\sigma^{2}}.\label{eq:R1}
% \end{align} 
% {\color{red}}
\begin{align} \Gamma_{i}^t(\boldsymbol{c}^{t}) &=\frac{g^{t}_{c_i^t,i}p^{\text{d},t}_{c_i^t,i} }{ \sum\limits_{k \in\mathcal{N} \setminus \{i\}} g^{t}_{c_k^t,i} p^{\text{d},t}_{c_k^t,k} +B\sigma^{2}}\nonumber \\ &=\frac{g^{t}_{c_i^t,i}p^{\text{d},t}_{c_i^t,i} }{ g^{t}_{c_i^t,i}  \sum\limits_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t}p^{\text{d},t}_{c_n^t,n}  + \sum\limits_{j \in\mathcal{M} \setminus \{c_i^t\}} g^{t}_{j,i} \sum\limits_{k \in \mathcal{N} : c_k^t = j} p^{\text{d},t}_{j,k}+B\sigma^{2}},\label{eq:R1}
\end{align}
where $p^{\text{d},t}_{c_i^t,i}$ is the downlink transmission power provided by MBS $c_i^t$ to facilitate the downlink transmission of in-game graphical scenes to UE $i$ at transmission step $t$ (the superscript ``d'' means downlink). The channel gain of MBS $c_i^t$ and UE $i$ at transmission step $t$ is denoted by $g^t_{c_i^t,i}$. $p^{\text{d},t}_{c_n^t,n}$ is the downlink transmission power provided by MBS $c_n^t$ to facilitate the downlink transmission of in-game graphical scenes to UE $n \neq i$ at transmission step $t$. However, we assume the MBS to have sufficient downlink transmission power. Hence, we choose to not optimize the downlink transmission power as the amount of power assigned for the downlink transmission to each UE can be taken to be constant, regardless of number of UE.   $p^{\text{d}}_{c_k^t,k}$ is the downlink transmission power provided by MBS $c_k^t$ to facilitate the downlink transmission of in-game graphical scenes to UE $k$ at transmission step $t$. The channel gain of other MBS $c_k^t$ and UE $i$ is denoted by $g^{t}_{c_k^t,i}$. We denote the bandwidth of each MBS and power spectral density to be $B$ and $\sigma^2$, respecitvely. Note that the power spectral density follows the additive white Gaussian noise. Fundamentally, $g^{t}_{c_i^t,i}\cdot p^{\text{d},t}_{i}$ is the  signal received at UE $i$ from MBS $c_i^t$ at iteration $t$, $\left [ g^{t}_{c_i^t,i}  \sum\limits_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t}p^{\text{d},t}_{c_n^t,n} \right]$ is the intra-cell interference as a result of the communication between MBS $c_i^t$ and other UE $n \neq i$ to UE $i$, and $\sum\limits_{j \in\mathcal{M} \setminus \{c_i^t\}} g^{t}_{j,i} \sum\limits_{k \in \mathcal{N} : c_k^t = j} p^{\text{d},t}_{j,k}$ is the inter-cell interference caused by the signals of all other MBSs $j \neq c_i^t$ to UE $i$.

The downlink transmission rate from an MBS to UE $i$ is denoted as $r^{\text{d},t}_i$ at each transmission iteration $t$. This transmission rate is influenced by the \textit{signal-to-noise-ratio} based on the Shannon-Hartley theorem~\cite{pierce2012introduction}:
\begin{align}
&r_{i}^{\text{d},t}=B\cdot \log_2 \left(1+\Gamma_i^t\right),\label{eq:R2}
\end{align}
A higher downlink data transmission rate will reduce the downlink data transmission latency for a given in-game data size to be downloaded. The downlink transmission delay can be defined as:
\begin{align}
\ell^{\text{d},t}_i = \frac{D^{t}_i }{r^{\text{d},t}_i}
\label{eq:R5} 
\end{align}
where $D^{t}_i$ is the size of in-game data  to be downloaded from an MBS to UE $i$ at transmission step $t$ and the resolution-impacted earning potential function $\omega(r^{\text{d},t}_i)$ is influenced by the UE $i$'s SINR. Intuitively, a more efficient UE to MBS allocation results in a higher UE $i$ SINR, UE $i$ downlink transmission rate  $r^{\text{d},t}_{i}$ (data transmitted per unit of time), and lower  downlink transmission delay $\ell^{\text{d},t}_i$, at iteration $t$. A higher data transmitted per unit of time is translated to improved graphic resolution per unit of time, resulting in an enhancement in players' in-game visuals, which translates to better in-game performance and hence, better resolution-influenced earning potential. 
\vspace{-0.3cm}


% Intuitively, as UE $i$ downlink data size (graphic resolution) at iteration step $t$ increases, there will be improvement in players' in-game visuals, which translates to better in-game performance and hence, better resolution-influenced earning potential. In contrast, as UE $i$ downlink data size at iteration $t$ decreases, the downlink latency at iteration step $t$ increases, resulting in the fluidity of the in-game experience to decrease.

% s, causing a decrease in in-game winnings and hence, decrease in overall profitability.




\vspace{-10pt}\subsection{Uplink Communication }\label{sub:Communication-up-Model}
In the uplink leg of the communication model, each player (UE) allocated to an MBS will upload its graphical changes to the previously downloaded Metaverse in-game scene data, to its assigned MBS. In the context of Play-to-Earn games, these in-game graphical changes could be induced by UEs' physical-world actions. These graphical changes to the scenes are denoted $F^t = \{F^t_{1}, F^t_{2}, ..., F^t_{N}\}$. We consider the intra-MBS and inter-MBS interference in the uplink leg, as well. Hence the \textit{signal-to-interference-ratio}, or effective signal between MBS $v$ and UE $i$ at transmission step $t$ can be defined as:
% \begin{align}
%     \beta_{v,i}^t=\frac{g^t_{v,i}\cdot p^t_{i,v} }{   \sum\limits_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t}p^t_{n,v}\cdot g^t_{v,i} + \sum\limits_{k \in\mathcal{N} \setminus \{i,n\}} g^{t}_{v,k} \sum\limits_{k \in \mathcal{N}, j \in\mathcal{M} \setminus \{v\} : c_k^t = j} p^{t}_{k,j} + B\sigma^{2}}
%     \label{eq:rate_function2}
% \end{align}
% {\color{red}}
\begin{align}
    \Lambda_{i}^t(\boldsymbol{p}^{\text{u},t}) &=\frac{g^t_{i,c_i^t}\cdot p^{\text{u},t}_{i} }{   \sum\limits_{k \in \mathcal{N}\setminus \{i\}} g^{t}_{k,c_i^t}p^{\text{u},t}_{k} + B\sigma^{2}} \nonumber \\ &=\frac{g^t_{i,c_i^t}\cdot p^{\text{u},t}_{i} }{   \sum\limits_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t}g^t_{n,c_i^t} p^{\text{u},t}_{n}  +    \sum\limits_{k \in \mathcal{N}, j \in\mathcal{M} \setminus \{c_i^t\} : c_k^t = j} g^{t}_{k,c_i^t}p^{\text{u},t}_{k} + B\sigma^{2}},
    \label{eq:rate_function2}
\end{align}
where $p^{\text{u},t}_{i}$  (the superscript ``u'' means uplink) is UE $i$'s power selected to facilitate the uplink transmission of in-game data graphical changes at iteration $t$. $p^{\text{u},t}_{n}$ is other UE $n$'s power selected to facilitate the uplink transmission of in-game data graphical changes at iteration $t$. Essentially, the interference generated by UEs allocated to MBS $c_i^t$ is $\left[  \sum\limits_{n \in\mathcal{N} \setminus \{i\}:c_n^t=c_i^t}  g^t_{n,c_i^t}p^{\text{u},t}_{n}\right]$, and the interference generated by UEs not allocated to MBS $c_i^t$ is $\left[ \sum\limits_{k \in \mathcal{N}, j \in\mathcal{M} \setminus \{c_i^t\} : c_k^t = j} g^{t}_{k,c_i^t}p^{\text{u},t}_{k} \right]$. The uplink data rate $r^{u,t}_i$ can similarly be represented by $r_{i}^{\text{u},t}(\boldsymbol{p}^{\text{u},t})=B\cdot \log_2\left(1+\Lambda_{i}^t(\boldsymbol{p}^{\text{u},t})\right)$, where $B$ is the bandwidth of the transmitting UE $i$. As the number of UEs allocated to the same MBS increases, the SINR and hence rate of data transfer, decreases.

The uplink transmission latency of UE $i$ at transmission step $t$ can be formulated as:
\begin{align}
\ell^{\text{u},t}_i = \frac{F^{t}_{i} }{r^{\text{u},t}_{i}}
\label{eq:R10} 
\end{align}
where $F^{t}_{i}$ is the uplink in-game graphics data size of UE $i$ at transmission step $t$, $r^{\text{u},t}_{i}$ is the uplink data transmission rate of UE $i$ at transmission step $t$. UE $i$'s energy expenditure for the uplink transmission of in-game graphics data at transmission step $t$ can be formulated as:
\begin{align}
E^t_{i} = p^{\text{u},t}_{i} \cdot \ell^{\text{u},t}_i.
\label{eq:R11} 
\end{align}

% In each round (iteration or step $t$) of in-game data uplink, the battery-life remaining in UE $i$ is: 
% \begin{align}
% Bat^{t}_{i} = Bat^{t-1}_i - E^t_{i}
% \label{eq:R12} 
% \end{align}

% where $Bat^{0}_i$ is the battery capacity of UE $i$ at full charge.

We define $Q^t_{i}$ to be the UE device $i$ energy consumption as a percentage of the UE device $i$'s initial battery amount, at transmission step $t$. $Q^t_{i}$ can be defined as: $Q^t_{i} = \frac{E^t_{i}}{Bat^0_{i}}\cdot 100$. In this context, $Bat^0_{i}$ represents the UE $i$'s initial battery amount while $E^t_{i}$ represents UE $i$'s energy consumption, at transmission step $t$. Utilizing a UE device energy consumption as a percentage of the UE device's initial battery charge is a natural choice. This is because normalized values are more conducive for training reinforcement learning agents. The control of uplink transmission power influences the uplink transmission latency, which influences fluidity. On the other hand, the selection of uplink transmission power also influences UE battery expenditure.
\vspace{-0.4cm}

% \subsection{UE Movement Patterns}
% After each round of data transmission (downlink followed by uplink), UEs will take steps in a 2-dimensional geographical space, a step-size of $x \sim \mathcal{U}(-x_{max}, x_{max})$ plus a step size of $y \sim \mathcal{U}(-y_{max}, y_{max})$. Note that $x$ refers to the step-size in 1-dimension, while $y$ refers to a step size in the other dimension, orthogonal to $x$. $x_{max}$ refers to the maximum allowable step-size in the $x$-direction, while $y_{max}$ refers to the maximum allowable step-size in the $y$-direction. At each given iteration step $t$, a UE's location is defined as $(\mathcal{X}^t_i,\mathcal{Y}^t_i)$ where $\mathcal{X}^t_i \in [0, X_{max}]$ and $\mathcal{Y}^t_i \in [0, Y_{max}]$. $\mathcal{X}^t_i$ refers to the position of UE $i$ at iteration step $t$ in the $x$-dimension, while $\mathcal{Y}^t_i$ refers to the position of UE $i$ at iteration step $t$ in the $y$-dimension. $X_{max}$ refers to the furthest geographical boundary in the $x$-dimension while $Y_{max}$ refers to the furthest geographical boundary in the $y$-dimension. Intuitively, this would mean that the location of UE $i$ at any given iteration step $t$ is confined to a fixed geographical space.



% \begin{table}[t]
% \caption{Notations\vspace{-5pt}}
% \begin{center}
% \begin{tabular}{|l|p{6cm}|}
% \hline
% {}{Symbol}&{}{Description} \\\hline
% {}{$n$ or $i$}&{}{index of UEs}\\
% {}{$m$ or $v$}&{}{index of MBS}\\
% {}{$t$}&{}{transmission iteration step}\\
% {}{$\mathcal{M}$}&{}{set of MBSs}\\
% {}{$\mathcal{N}$}&{}{set of UEs}\\
% {}{$d^{t}$}&{}{data size to transmit to each UE at iteration step $t$}\\
% {}{$E^t$}&{}{energy consumed for uplink transmission at iteration $t$}\\
% {}{\textbf{$c^t$}}&{}{access management at iteration $t$}\\
% {}{$\ell^t$}&{}{delay of either uplink or downlink at iteration $t$.}\\
% {}{$r^t$}&{}{down or uplink transmission rate to or from UE at iteration $t$}\\
% {}{$B$}&{}{bandwidth}\\
% {}{$p_{v,i}$}&{}{power of MBS $v$ used to transmit to UE $i$}\\
% {}{$p^t_{i,v}$}&{}{power of UE $i$ used to transmit to MBS $v$ at iteration $t$}\\
% {}{$g_{i,v}^t$}&{}{channel gain between UE $i$ and MBS $v$ at iteration $t$}\\
% {}{$\sigma$}&{}{noise parameter}\\
% {}{$\lambda$}&{}{wavelength of transmission}\\
% {}{$\beta^t_i$}&{}{uplink SINR of UE $i$ at iteration $t$}\\
% {}{$\Gamma_i^t$}&{}{Downlink SINR of UE $i$ at iteration $t$}\\
% {}{$Bat^t_i$}&{}{Battery charge of UE $i$ at iteration $t$}\\
% {}{$Q^t_i$}&{}{Battery charge as a percentage of UE $i$ at iteration $t$}\\
% {}{$dist^t_{v,i}$}&{}{distance between MBS $v$ and UE $i$ at iteration $t$}\\
% {}{$a$}&{}{data-size factor}\\
% {}{$b$}&{}{downlink variable order balancing coefficient}\\
% {}{$f$}&{}{uplink variable order balancing coefficient}\\
% {}{$q$}&{}{uplink variable weighting analysis control variable}\\
% {}{$h$}&{}{downlink variable weighting analysis control variable}\\
% \hline
% \end{tabular}
% \vspace{-20pt}
% \end{center}
% \end{table}

\vspace{-5pt}\subsection{UE Movement Patterns \vspace{-5pt}}

We consider the network area to be a two-dimensional coordinate system, $x$-dimension and $y$-dimension, where the UEs and MBSs are positioned. 
At each given iteration step $t$, UE $i$ is located at $(\mathcal{X}^t_i,\mathcal{Y}^t_i)$. The locations of UEs are restricted to an area, $\mathcal{Y}^t_i \in [0, Y_{max}]$ and $\mathcal{X}^t_i \in [0, X_{max}]$. $\mathcal{X}^t_i$ refers to UE $i$'s position in the $x$-dimension at transmission step $t$, while $\mathcal{Y}^t_i$ refers to UE $i$'s position in the $y$-dimension at transmission step $t$. $X_{max}$ is the defined boundary in the $x$-dimension while $Y_{max}$ is the defined boundary in the $y$-dimension. After each round of data transmission (downlink followed by uplink), UEs will take steps $x$ and $y$ in both $x$-dimension and $y$-dimension, with the step size uniformly sampled from $(-x_{max}, x_{max})$ and $(-y_{max}, y_{max})$, respectively. $x_{max}$ refers to the maximum allowed distance a UE can move in a single transmission iteration in the $x$-dimension. $y_{max}$ refers to the maximum allowed distance a UE can move in a single transmission iteration in the $y$-dimension. The $x$-dimension position $\mathcal{X}^t_i$ can be defined as:
\begin{align}
    \mathcal{X}^t_i =
    \begin{cases}
    0,& \text{if } \mathcal{X}^{t-1}_i + x\leq 0,\\
    X_{max},              & \text{if } \mathcal{X}^{t-1}_i + x\geq X_{max},\\
    \mathcal{X}^{t-1}_i + x,
    & \text{otherwise,}
    \label{eq:x_movement}
    \end{cases}
\end{align}
Likewise, the $y$-dimension position $\mathcal{Y}^t_i$ can be defined as:
\begin{align}
    \mathcal{Y}^t_i =
    \begin{cases}
    0,& \text{if } \mathcal{Y}^{t-1}_i + y\leq 0,\\
    Y_{max},              & \text{if } \mathcal{Y}^{t-1}_i + y\geq Y_{max},\\
    \mathcal{Y}^{t-1}_i + y,
    & \text{otherwise,}
    \label{eq:y_movement}
    \end{cases}
\end{align}
\vspace{-0.5cm}

% If either steps $x$ or $y$ taken causes a UE's next location ($\mathcal{X}^{t+1}_i$,$\mathcal{Y}^{t+1}_i$) to exceed their bounds $[0, X_{max}]$ and $[0, Y_{max}]$, the UE's location will be restricted to its range $[0, X_{max}]$ and $[0, Y_{max}]$.

 % Note that $x$ refers to the step-size in 1-dimension, while $y$ refers to a step size in the other dimension, orthogonal to $x$. $x_{max}$ refers to the maximum allowable step-size in the $x$-direction, while $y_{max}$ refers to the maximum allowable step-size in the $y$-direction. $X_{max}$ refers to the furthest geographical boundary in the $x$-dimension while $Y_{max}$ refers to the furthest geographical boundary in the $y$-dimension. Intuitively, this would mean that the location of UE $i$ at any given iteration step $t$ is confined to a fixed geographical space.

\vspace{-10pt}\subsection{Problem formulation \vspace{-5pt}} \label{problemform}
To sum up, the MSP's DL objective is to find the optimal UE-MBS allocation $\boldsymbol{c}^{t}$ which minimizes the total downlink latency $\ell^{\text{d},t}_i$ while maximizing the worst-case (lowest) resolution-influenced UE earning potential $\min_{i \in \mathcal{N}}\sum_{t=1}^{T}\omega(r^{\text{d},t}_i)$ across all UEs. The number of transmission steps in an episode is denoted by $T$. We formulate our downlink utility function as:
\begin{align}
\setlength{\belowdisplayskip}{4pt plus 1pt minus 1.0pt}
\setlength{\belowdisplayshortskip}{4pt plus 1pt minus 1.0pt}
\setlength{\abovedisplayskip}{4pt plus 1pt minus 1.0pt} \setlength{\abovedisplayshortskip}{0.0pt plus 2.0pt}
\min_{\boldsymbol{c}^{t}} & \left [ q \cdot\left(\frac{1}{N}\sum_{i\in\mathcal{N}}  \sum_{t=1}^{T}  \ell^{\text{d},t}_i \right )- (1-q) \cdot b\cdot \min_{i \in \mathcal{N}}\sum_{t=1}^{T}\omega(r^{\text{d},t}_i)\right ]   , \label{eq:M1}\\
s.t.~~& c_{i}^{t}=v \in \mathcal{M}, \forall t \in \mathcal{T},~\forall i \in \mathcal{N},
\label{eq:c2}
\end{align}
% & m_{i,j}^{t} \in \{0,1\},  \forall i \in \mathcal{N}, \forall j \in \mathcal{M}, \forall t \in [1,T],
 where $\mathcal{T}$ represents the set of all transmission steps in an episode. Our resolution-impacted in-game profit is motivated by a recent work \cite{feng2022resource} and is modeled as: $\omega(r^{\text{d},t}_i) = P\cdot \ln\left(1+{r^{\text{d},t}_i}\right )$, where $P$ is the player-specific profitability factor (using such logarithmic function as the utility also appears in the classical work on crowdsourcing~\cite{yang2012crowdsourcing}). The logarithmic function has a increasing first-order derivative and decreasing second-order derivative property. When applied to the Play-to-Earn profit model, the increasing first-order derivative implies that UE in-game profits increases as the downlink transmission rate increases, while the decreasing second-order derivative implies that the rate of profit increment decreases as the downlink transmission rate increases. It is natural to model our profit model as such as a larger downlink data (resolution) per unit time improves UEs' visibility within the games, allowing them to perform better and reap higher profits. However, as the resolution improves, its impact on UE's visibility and hence earnings, decreases, resulting in a slowing rate of profit increment with increasing downlink transmission rate. The constraint~(\ref{eq:c2}) ensures that each UE is assigned to one MBS in each transmission step. $q$ is a factor which determines the weighting on $\sum_{i\in\mathcal{N}}  \sum_{t=1}^{T}\ell^{\text{d},t}_i$ and $b\cdot\min_{i \in \mathcal{N}}\sum_{t=1}^{T}\omega(r^{\text{d},t}_i)$. $b$ is a factor that places both terms $\sum_{i\in\mathcal{N}}  \sum_{t=1}^{T}\ell^{\text{d},t}_i$ and $b\cdot \min_{i \in \mathcal{N}}\sum_{t=1}^{T}\omega(r^{\text{d},t}_i)$ on the same measurement unit.

On the other hand, MSP's UL objective is to obtain the optimal UE uplink power $p^{\text{u},t}$ that minimizes the total latency of the data uplink transmissions $\ell^{\text{u},t}_i$ and worst-case (greatest) UE device energy expenditure $Q^t_i$. We formulate our uplink utility function as:
\begin{align}
\setlength{\belowdisplayskip}{4pt plus 1pt minus 1.0pt}
\setlength{\belowdisplayshortskip}{4pt plus 1pt minus 1.0pt}
\setlength{\abovedisplayskip}{4pt plus 1pt minus 1.0pt} \setlength{\abovedisplayshortskip}{0.0pt plus 2.0pt}
\min_{\boldsymbol{p^{t}}} &\left [ h \cdot\left (\frac{1}{N} \sum_{i\in\mathcal{N}}\sum_{t=1}^{T}  \ell^{\text{u},t}_i \right ) + (1-h) \cdot f\cdot \max_{i \in \mathcal{N}}\sum_{t=1}^{T}Q^t_i \right ], \label{eq:M3}\\
s.t.~~& \sum_{t=1}^{T}Q^t_i \leq 100, \forall i \in \mathcal{N}.\label{eq:c3}
\end{align}
where $\ell^{\text{u},t}_i$ is the player (UE) $i$'s uplink data transmission delay at transmission iteration $t$. $h$ is a factor that determines the weighting on $\sum_{i\in\mathcal{N}}\sum_{t=1}^{T} \ell^{\text{u},t}_i$ and $f\cdot \max_{i \in \mathcal{N}}\sum_{t=1}^{T}Q^t_i$. Scaling factor $f$ in the objective function is to place both terms $\sum_{i\in\mathcal{N}}\sum_{t=1}^{T}\ell^{\text{u},t}_i$ and $f\cdot \max_{i \in \mathcal{N}}\sum_{t=1}^{T}Q^t_i$ on the same measurement unit. Finally, constraint~(\ref{eq:c3}) ensures that the UE battery consumption has to be less than 100\% (or battery charge (\%) has to be greater than 0) for the uplink transmission.


\vspace{-15pt}\section{Reinforcement learning settings \vspace{-5pt}}
\label{RL}
We assign two reinforcement learning agents, one (DL agent) controlling the downlink transmission UE-MBS allocation, and the other (UL agent) controlling the uplink transmission power selection. The DL agent and UL agent serve their objectives (\ref{eq:M1}) and (\ref{eq:M3}), respectively.
\vspace{-0.4cm}

\subsection{State \vspace{-5pt}}

For the DL agent's observation state $s^{\text{d}}$, we choose to include 1) channel gain between UEs and MBSs: $g^t_{v,i}$, 2) in-game scene DL data size at each transmission iteration $t$: $D^{t}_{i}$, as $D^{t}_{i}$ influences latency $\ell^{\text{d},t}_{i}$, and $g^t_{v,i}$ influences both data DL transmission latency $\ell^{\text{d},t}_{i}$ and worst-case (lowest) resolution-influenced in-game earning potential $\min_{i \in \mathcal{N}}\sum_{t=1}^{T}\omega(r^{\text{d},t}_i)$.

For the UL agent's observation state $s^{\text{u}}$, we choose to include 1) channel gain between UEs and MBSs: $g^t_{v,i}$, as it influences data UL transmission rate, latency $\ell^{\text{u},t}_{i}$ and uplink transmission battery consumption $Q^{t}_{i}$. As we consider the cumulative battery amount consumed by UE devices and are enforcing that each UE has positive battery-charge to continue the uplink transmission at each transmission iteration step $t$, we have to include 2) UE device battery life (\%): $\left [ \frac{Bat^{t}_{i}}{Bat^{0}_{i}}\cdot 100 \right ]$ , as UE device battery life changes in each iteration.
\vspace{-0.4cm}

% battery charge consumed (\%) of UEs $Q^{t-1}_i$ from the previous iteration for the calculation of the battery charge consumed (\%) $Q^{t}_i$ in the present iteration.

\vspace{-2pt}\subsection{Action \vspace{-5pt}}
In the DL communication model, the DL agent's action involves deciding the UE to MBS assignment. The UE to MBS allocation action at transmission step $t$ is defined as: $a^{\text{d},t}=\textbf{c}^{t}=\{c_1^t,...c_N^t\}.$ In the UL communication model, the UE device power for uplink data transmission at transmission step $t$ is defined as: $a^{\text{u},t}=\textbf{p}^{\text{u},t}=\{p^{\text{u},t}_1, ... ,p^{\text{u},t}_N\}$. $p^{\text{u},t}_i$ represents UE $i$'s selected power for the uplink data transmission at transmission step $t$.
\vspace{-0.3cm}

% Fortunately, as large discrete action space even continuous action space cases have been studied comprehensive in the literature, we can adopt a state-of-art algorithm, Proximal Policy Optimization (PPO) for our work. We discussed the algorithm in \ref{algorithms}.

\vspace{-5pt}\subsection{Reward \vspace{-5pt}}
\label{subsection:reward}
For the \textbf{downlink} communication model, the reward given to the DL agent at transmission step $t$ is given as such: $R^{\text{d},t} = - q\cdot\frac{ \sum_{i \in \mathcal{N}}\ell^{\text{d},t}_i }{N} +  \left(1-q\right) \cdot b\cdot\min_{i \in \mathcal{N}}\sum_{\tau=1}^{t}\omega(r^{\text{d},t}_i)$ if the total battery consumed as of transmission step $t$ $\sum_{\tau=1}^{t}Q^t_i \leq 100$. Otherwise,  $R^{\text{d},t} = \varrho$, where $\varrho$ is a very large penalty. $\tau$ denotes the summation of index. For the uplink communication model, the reward issued to the UL agent at transmission step $t$ is defined as: $R^{\text{u},t} = -h\cdot\frac{ \sum_{i \in \mathcal{N}}\ell^{\text{u},t}_i}{N} + \left(1-h\right) \cdot f \cdot \min_{i \in \mathcal{N}}\sum_{\tau=1}^{t}Q^t_i,$ if the total battery consumed as of transmission step $t$ $\sum_{\tau=1}^{t}Q^t_i \leq 100$. Otherwise, $R^{\text{u},t} = \varrho$, where $\varrho$ is a very large penalty. The intuition for the downlink and uplink reward assignment follows the utility function in Equations (\ref{eq:M1}) and (\ref{eq:M3}), respectively.
\vspace{-0.3cm}

\vspace{-5pt}\section{Methodology \vspace{-5pt}}
\label{methodss}
% In this section, we will discuss the inspiration behind our proposed Multi-Agent Loss-Sharing (MALS) reinforcement learning. We will then introduce the Proximal Policy Optimization (PPO) algorithm which our MALS model is built on. Finally, we will elaborate on the MALS algorithm and structure in detail.

\textbf{Inspiration. }
In our problem definition, the Metaverse Service Provider (MSP) aims to minimize the latency of the downlink and uplink data transmission, and worst-case (greatest) UE uplink transmission battery charge expenditure, while maximizing the worst-case (lowest) UE's resolution-impacted earning potential. Given that the MSP has multiple objectives, rudimentary reinforcement learning agents employed struggle to achieve them and may fail to achieve convergence. One reason is that independent agents act in their own interest, choosing the best actions to maximize their own rewards, even if it is at the expense of other agents. In a mixed-cooperative scenario as proposed in our study where each agent acts in its interest, agents' greedy actions may result in sub-optimal action solutions learned across all agents. To tackle this issue, we proposed a novel multi-input, multi-objective (head) critic which provides actors with actor-centric guidance, facilitating the actors in breaking down complex objectives into simpler ones. A (multi) dual-head critic refers to a critic model with (multiple) two distinct neural network input layers, each to accommodate to the input state of each agent, while sharing a common neural network backbone. The critic head serving the uplink actor beacons the UL actor in (i) minimizing the UL transmission delay and UE battery consumption in the uplink transmissions. The critic head serving the downlink actor beacons the DL actor in (ii) minimizing the DL transmission delay and (iii) maximizing the UE resolution-influenced in-game earning potential. To ensure that each agent involved learns a jointly optimal solution, the loss value of each critic's head is shared (summed) for the joint update of the critic model.
 
In the following section, we will introduce the PPO algorithm, its mechanism as it is the backbone algorithm in which our MALS algorithm is built upon.
\vspace{-0.5cm}

% \subsection{Preliminary: Proximal Policy Optimization (PPO)}
%  \label{algorithms}
%  \subsubsection{Proximal Policy Optimization (PPO)}
%  In summary, Proximal Policy Optimization (PPO)~\cite{PPO} uses separate policies $\pi_\theta$ and $\pi_{\theta^{'}}$ for sampling trajectories (during training) and evaluation to increase efficiency as shown:
% \begin{align}
%     \mathbb{E}_{(s^t,a^t)\sim\pi_\theta}[\pi_\theta(a^t|s^t) A^t] &=  \mathbb{E}_{(s^t,a^t)\sim\pi_{\theta^{'}}}[\frac{\pi_\theta(a^t|s^t)}{\pi_{\theta^{'}}(a^t|s^t)} A^t] \label{eq:sample}
% \end{align}
% In order to prevent major policy changes in each update, a policy constraint is imposed on every observation. The objective function can be rewritten as: $\mathbb{E}_{(s^t,a^t)\sim\pi_{\theta^{'}}}[f^t(\theta)A^t]$,
% where $f^t(\theta)=min\{r^t(\theta), clip(r^t(\theta), 1-\epsilon, 1+\epsilon)\}$.

% An optimal solution can be solved by gradient ascent. Therefore, the gradient can be written as:
% \begin{align}
%     \Delta\theta = \mathbb{E}_{(s^t,a^t)\sim\pi_{\theta^{'}}}[\triangledown f^t(\theta)A^t] \label{eq:actorobj}
% \end{align}

% In terms of the value network (Critic), PPO uses identical Critic as per other Actor-Critic algorithms; and the loss function can be formulated in~\cite{PPO} as:
% \begin{align}
%     L(\phi) = [V_\phi(s^t)-(R^t+\gamma V_{\phi'}(s^{t+1}))]^2 \label{eq:criticloss}
% \end{align}

% where $V(s)$ is the widely used state-value function~\cite{RLintro}, which is estimated by a learned critic network with parameter $\phi$. We update $\phi$ by minimizing the $L(\phi)$, and the parameter $\phi'$ of target state-value function periodically with $\phi$.


\begin{figure}[t] \label{fig:RL_structure}
\centering
\includegraphics[width=1\linewidth]{RL_structure.pdf}
\vspace{-30pt}\caption{Reinforcement Learning Multi-Agent-Loss-Sharing Proximal Policy Optimization (MALS-PPO) structure and model update in a single time step.}
\label{fig:model}
\vspace{-25pt}
\end{figure}


\subsection{Proximal Policy Optimization (PPO)}
As we emphasize on developing a multi-agent RL model to tackle \textit{asymmetric} tasks, \textit{asynchronously}, policy stability is essential. The Proximal Policy Optimization (PPO) algorithm proposed by OpenAI~\cite{schulman2017proximal} is an enhancement of the traditional policy gradient algorithm. PPO has better sample efficiency by using a separate policy for sampling, and is more stable by embedding a policy constraint.

% As $\pi_{\theta^{'}}$ is used to sample data for training, the state-action pair expectation can be rewritten as:
% \begin{align}
%     \mathbb{E}_{(s^t,a^t)\sim\pi_\theta}[\pi_\theta(a^t|s^t) A^t] &=  \mathbb{E}_{(s^t,a^t)\sim\pi_{\theta^{'}}}[\frac{\pi_\theta(a^t|s^t)}{\pi_{\theta^{'}}(a^t|s^t)} A^t], \label{eq:sample}
% \end{align}
% as per~\cite{schulman2017proximal}, where $A^{t}$ is the advantage value.

In summary, PPO has two main characteristics in its policy network (Actor): (1) \textit{Increased sample efficiency.} PPO uses two separate policies for sampling trajectories and optimization to improve sample efficiency. $\pi_\theta$ and $\pi_{\theta^{'}}$ are defined as the evaluation policy and data-sampling policy, respectively. $\theta$ is the parameter for the evaluation policy and $\theta_{'}$ is the parameter of the data sampling policy. 

(2) \textit{Policy constraint.} After switching the data sampling policy from $\pi_{\theta}$ to $\pi_{\theta^{'}}$, an issue still remains. Although the expected value of the evaluation and data sampling policy is similar, their variances are starkly distinct. A policy constraint is implemented in the form of threshold clipping and the state-action pair expectation is defined as:
$\mathbb{E}_{(s^t,a^t)\sim\pi_{\theta^{'}}}[\min\{\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)}, \operatorname{clip}(\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)}, 1-\epsilon, 1+\epsilon)\}\cdot A^t]$, where $A^{t}$ is the advantage value. Gradient ascent is then used to improve the policy:
\begin{align}
    \Delta\theta = \mathbb{E}_{(s^t,a^t)\sim\pi_{\theta^{'}}}[\Delta\min\{\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)}, \operatorname{clip}(\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)}, 1-\epsilon, 1+\epsilon)\}\cdot A^t]. \label{eq:actorobj}
\end{align}
With regards to the critic network, all actors under the PPO algorithm share a similar critic network model. Hence, the authors of ~\cite{schulman2017proximal} formulate the critic loss to be: 
\begin{align}
&L(\phi) = (V_\phi(s^t)-A^t+\gamma V_{\phi_{targ}}(s^{t}))^2. \label{eq:criticloss}
\end{align}
$\phi$ is the parameter of the critic model. The state-values $V_{\phi}(s^{t})$ and $V_{\phi_{targ}}(s^{t})$ are estimated with the critic model and the target critic model, respectively. Essentially, the critic model utilizes the loss function $L(\phi)$ to update the critic network.
\vspace{-0.3cm}

% PPO is a state-of-the-art, effective RL algorithm, which has been actively used in wireless communication research ~\cite{li2020trajectory}. PPO is a suitable RL algorithm to tackle our proposed scenario as PPO can handle both discrete and continuous action spaces through fitting different output heads on the actor network. Next we will expound PPO, focusing on its three main underlying features: (i) Policy gradient, (ii) Importance sampling and (iii) Policy constrain.

% \textbf{Based on policy gradient methods.} The widely used policy gradient method computes an estimator and embeds it into a stochastic gradient ascent algorithm to maximize the expected reward:
% \begin{align}
%     J(\theta)               &= \sum_\tau \pi_\theta(\tau) R(\tau), \label{eq:1} \\ 
%     \triangledown J(\theta) &= \sum_\tau \triangledown \pi_{\theta}(\tau) R(\tau)   = \sum_\tau \pi_{\theta}(\tau) \triangledown log \pi_{\theta}(\tau) R(\tau)   = \mathbb{E}_{\tau\sim\pi_\theta}[\triangledown log\pi_\theta(\tau) R(\tau)], \label{eq:2}
% \end{align}
% where $\pi_\theta$ is a stochastic policy, $R(...)$ denotes the reward function, and $\tau$ denotes the trajectories including $(s^0, a^0, ..., s^t, a^t)$. Recent works use an advantage function instead of a reward function to make training more stable. Thus, we rewrite Equation (\ref{eq:1}) into:
% \begin{align}
%     J(\theta) = \mathbb{E}_{\tau\sim\pi_\theta}[A(\tau)]. \label{eq:3}
% \end{align}
% Note that this expected value $\mathbb{E}=[...]$ represents the average value of the sampled data.


% \textbf{Use of importance sampling.} Importance sampling (IS) ~\cite{owen2000safe} is a method where an expectation with respect to a target distribution is approximated from another distribution. Hence, IS is an important trick adopted in PPO as it allows PPO to use different policies for sampling and evaluating trajectories, increasing the overall sample efficiency~\cite{schulman2017proximal}.

% Here we use $\pi_\theta$ as the policy for evaluation and $\pi_{\theta'}$ as the policy for sampling data for training, and Equation (\ref{eq:3}) can be rewritten as:
% \begin{align}
%     \mathbb{E}_{\tau\sim\pi_\theta}[A(\tau)]= \mathbb{E}_{\tau\sim\pi_{{\theta'}}} \left [\frac{\pi_\theta(\tau)}{\pi_{{\theta'}}(\tau)} A(\tau) \right ]. \label{eq:5}
% \end{align}
% However, in practice, we use state-action pairs instead of trajectories to update the gradient. Thus, the objective function of the actor can be written as:
% \begin{align}
%     &J(\theta) =\mathbb{E}_{(s^t,a^t)\sim\pi_{{\theta'}}} \left [\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)} A(s^t,a^t) \right ].
% \end{align}

% \textbf{Add KL-divergence penalty.}
% After switching to $\pi_{{\theta'}}$ for data sampling, there remains an issue of unequal variances. Although Equations (\ref{eq:3}) and (\ref{eq:5}) have the same expectations of objective function, their variances are very different, as shown below:
% \begin{align}
%     &Var_{\tau\sim\pi_\theta} [A(\tau)]  =\mathbb{E}_{\tau\sim\pi_\theta}[A^2(\tau)] - (\mathbb{E}_{\tau\sim\pi_\theta}[A(\tau)])^2, \label{eq:var1}\\
%     &Var_{\tau\sim\pi_{{\theta'}}} \left [\frac{\pi_\theta(\tau)}{\pi_{{\theta'}}(\tau)} A(\tau) \right ] =\sum_{\tau\sim\pi_{\theta'}} \frac{\pi_\theta^2(\tau)}{\pi_{\theta'}^2(\tau)} A^2(\tau) \pi_{{\theta'}}(\tau)  - 
%     \left (\sum_{\tau\sim\pi_{\theta'}} \frac{\pi_\theta(\tau)}{\pi_{{\theta'}}(\tau)} A(\tau) \pi_{\theta'}(\tau) \right )^2 \nonumber\\
%     &=\sum_{\tau\sim\pi_{\theta'}} \frac{\pi_\theta^2(\tau)}{\pi_{\theta'}(\tau)} A^2(\tau) -
%     \left (\sum_{\tau\sim\pi_{\theta'}} \pi_\theta(\tau) A(\tau) \right )^2  =\mathbb{E}_{\tau\sim\pi_\theta} \left [ \frac{\pi_\theta(\tau)}{\pi_{{\theta'}}(\tau)} A^2(\tau) \right ] - (\mathbb{E}_{\tau\sim\pi_\theta}[A(\tau)])^2. \label{eq:var2}
% \end{align} 
% From the two variances (\ref{eq:var1}) and (\ref{eq:var2}), we can see that the distance between the distributions $\theta$ and $\theta'$ can not be large. Therefore, PPO adds a KL divergence penalty to the Actor objective function to constrain the distance:
% \begin{align}
%     &J(\theta) = \mathbb{E}_{(s^t,a^t)\sim\pi_{{\theta'}}}[\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)} A(s^t,a^t)] \label{Actorobj} \\
%     &s.t.~~D_{KL}(\pi_{\theta}||\pi_{{\theta'}}) \leq \sigma, \label{trustregion}
% \end{align}
% where $\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)}$ is the probability ratio between old and new policies. $D_{KL}(\cdot||\cdot)$ denotes the Kullback-Leibler (KL) divergence for measuring the distance between $\pi_\theta$ and $\pi_{{\theta'}}$.

% These strategies make PPO an effective and reliable RL algorithm to tackle wireless communication optimization problems. Nevertheless, this KL divergence is impractical to calculate in practice as this constraint is imposed on every observation. Thus, in~\cite{schulman2017proximal}, the objective function is finally represented as:
% \begin{align}
%     \mathbb{E}_{(s^t,a^t)\sim\pi_{{\theta'}}}[f^t(\theta, A(s^t,a^t))], \label{eq:obj}
% \end{align}
% where
% \begin{align}
%     f^t(\theta, A^t)=\min\{\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)}(\theta)A(s^t,a^t),
%     clip(\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)}(\theta), 1-\epsilon, 1+\epsilon)A(s^t,a^t)\}.
% \end{align}

% The problem in~(\ref{eq:obj}) is solved by gradient ascent, therefore, the gradient is finally written as:
% \begin{align}
%     \Delta\theta = \mathbb{E}_{(s^t,a^t)\sim\pi_{{\theta'}}}[\triangledown f^t(\theta,A(s^t,a^t))]. \label{eq:actorobj}
% \end{align}

% \textbf{Critic loss.} In terms of the critic, PPO uses a critic with identical network to the actor, just like in other Actor-Critic algorithms. The loss function is formulated in~\cite{schulman2017proximal} as:
% \begin{align}
%     L(\phi) = (V_\phi(s^t)-V_{targ})^2. \label{eq:criticloss}
% \end{align}
% where $V_{targ}=A(s^t,a^t)+\gamma V_{\phi'}(s^{t})$.

% $V(s)$ is the widely used state-value function~\cite{sutton1999policy}, which is estimated by a learned critic network with parameter $\phi$. We update $\phi$ by minimizing the $L(\phi)$, and the parameter $\phi'$ of target state-value function periodically with $\phi$.

\vspace{-5pt}\subsection{Multi-Agent-Loss-Sharing (MALS) \vspace{-5pt}}
 In this paper, MALS is an RL structure that can accommodate asymmetric actors of different action spaces. We adopt PPO as the underlying algorithm of MALS and utilize two actors, one which has a discrete UE-MBS allocation action, and the other which has a continuous power selection action. The MALS algorithm adopts a multi-head critic, with each head serving as a beacon to an actor.

\textbf{Transmission mechanism}:
In a single transmission with one DL-UL iteration, the downlink state $s^{\text{d},t}$ will be observed by the DL agent. The DL agent will then choose an action $a^{\text{d},t}$ based on its policy and observed state. This action causes an impact on the environment, and a DL reward $R^{\text{d},t}$ is issued to the DL agent in accordance to the merit of its actions. The altered environment consequently influences the UL state $s^{\text{u},t}$ observed by UL agent. Based on its policy, the UL agent will choose an action $a^{\text{u},t}$ which corresponds to the current observed state. This action by the UL agent influences the environment, and yields the UL agent a corresponding reward $R^{\text{u},t}$ and the next DL state $s^{\text{d},t+1}$. After several iterations, the rewards ($R^{\text{d},t},R^{\text{u},t}$) with states ($s^{\text{d},t},s^{\text{u},t}$) and their corresponding next states ($s^{\text{d},t+1},s^{\text{u},t+1}$) will be sent to the multi-head critic. The critic will then utilize these rewards, states, and next iteration states to calculate state-value $V$ and advantage value $A$ to update the actor network parameters. This transmission mechanism is illustrated in Fig \ref{fig:model}.

\textbf{Asymmetric Actor}:
In MALS, there are two Actors: one arranges UE-MBS allocation in the downlink transmission, and the other selects the UE power for the uplink transmission. The UE-MBS allocation is a discrete case selection and hence probabilities are computed for each action based on the DL agent policy~\cite{sutton1999policy}. The transmission power selection in the UL stage falls along a continuum and hence a probability distribution over the output power values are learnt by the UL agent.

In addition to having distinct action space types, the agents have different objectives to tackle. The DL and UL agents are given only portions of the overarching objective that they control as this is to avoid ``confusing" agents with rewards that they do not control. For instance, in our work, we do not present the UL agent with rewards pertaining to downlink transmission latency as downlink transmission latency is not within the UL agent's control.



% Each agent aims to maximize all the related returns. And we give them both their role-specific reward and global reward to achieve their objectives while stopping them from reaching the other's role-specific reward. It is because the role-specific objective couldn't be influenced by the other agent, giving the views of the other agent's role-specific objective and reward will make it hard to train and converge. 
In MALS, $\theta^{\text{d}}$ and $\theta^{\text{u}}$ are the network weights of DL agent and UL agent, and the gradients $\Delta\theta^{\text{d}}$, $\Delta\theta^{\text{u}}$ of the DL agent and UL agent are:
\begin{align}
    &\Delta\theta^{\text{u}} = \mathbb{E}_{(s^{\text{u},t},a^{\text{u},t})\sim\pi_{{\theta^{\text{u}}}'}}[\triangledown f^t(\theta^{\text{u}})A^{\text{u}}(s^{\text{u},t})], 
    \label{eq:gradient1}\\
    &\Delta\theta^{\text{d}} = \mathbb{E}_{(s^{\text{d},t},a^{\text{d},t})\sim\pi_{{\theta^{\text{d}}}'}}[\triangledown f^t(\theta^{\text{d}})A^{\text{d}}(s^{\text{d},t})],
    \label{eq:gradient2}
\end{align}
where $f^t(\theta)=\min\{\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)}, \operatorname{clip}(\frac{\pi_\theta(a^t|s^t)}{\pi_{{\theta'}}(a^t|s^t)}, 1-\epsilon, 1+\epsilon)\}$ in the case of PPO, in which 
% $R^t$ is the reward obtained by an agent at iteration $t$,
 $\epsilon$ is the clipping parameter, and $A(\cdot)$ is the advantage function of both UL and DL agents.

\textbf{Advantage function. }We utilize the state-value function ($V$) and the truncated version of the Generalized Advantage Estimation (GAE)~\cite{GAE} method to calculate the advantage values. The policy is then executed for $\bar{T}$ time steps, where $\bar{T}$ is the number of steps in a trajectory. A trajectory of $\bar{T}$ number of steps represents the path of an agent through the state space up till horizon $\bar{T}$. The advantage value in our work can be calculated as:
\begin{align}
    &A^{\text{u},t} = \delta^{\text{u},t} + (\gamma\lambda)\delta^{\text{u},t+1}+...+(\gamma\lambda)^{\bar{T}-1}\delta^{\text{u},t+\bar{T}-1} ,\text{ for}~\delta^{\text{u},t}=R^{\text{u},t}+\gamma V_{\phi_{targ}}(s^{\text{u},t+1})-V_{\phi_{targ}}(s^{\text{u},t}),
\end{align}
\begin{align}
     &A^{\text{d},t} = \delta^{\text{d},t} + (\gamma\lambda)\delta^{\text{d},t+1}+...+(\gamma\lambda)^{\bar{T}-1}\delta^{\text{d},t+\bar{T}-1} ,\text{ for}~\delta^{\text{d},t}=R^{\text{d},t}+\gamma V_{\phi_{targ}}(s^{\text{d},t+1})-V_{\phi_{targ}}(s^{\text{d},t}),
\end{align}
where $\bar{T}$, $\gamma$, and $\lambda$ denote the trajectory length, discount factor, and GAE parameter, respectively. $\phi$ and $\phi_{targ}$ are the parameters of the critic model and the target model, respectively.

\textbf{Multi-head Critic}:
We proposed an asymmetric problem in our work, where each agent optimizes different variables and has different objectives. A multi-head critic caters to the asymmetric actors by providing personalized calculations of state-values $V_\phi^u, V_\phi^d$ using a neural network with two heads. Each critic head produces a loss value corresponding to the state-value that it produces, and the loss values of the critic heads are weighted-summed ($\kappa_1$ and $\kappa_2$) to give a conclusive loss value as follows:
\begin{align}
    &L^{\text{u}}(\phi) = (V_\phi(s^{\text{u},t})-\gamma V_{targ}^{\text{u},t}))^2, \\
    &L^{\text{d}}(\phi) = (V_\phi(s^{\text{d},t})-\gamma V_{targ}^{\text{d},t}))^2 ,\\
    &L(\phi) = \kappa_1 \times L^{\text{u}}(\phi) + \kappa_2 \times L^{\text{d}}(\phi) .\label{eq:HCloss}
\end{align}
The specific values of $\kappa_{1}$ and $\kappa_{2}$ will be specified in the experiments. The multi-head critic's model parameter is updated with the loss in Equation (\ref{eq:HCloss}) to improve the critics model in guiding the asymmetric actors. The MALS algorithm is detailed in Algorithm~\ref{alg:PPO}.
\vspace{-0.4cm}

% The state-values can be calculated as shown:
% \begin{align}
%     &V_\phi^{\text{u}} = F(s^{\text{u}}), \\
%     &V_\phi^{\text{d}} = F(s^{\text{d}}).
% \end{align}


\begin{figure}[!t] 
        \renewcommand{\algorithmicrequire}{\textbf{Initiate:}}
        \renewcommand{\algorithmicensure}{\textbf{Output:}}
        \begin{algorithm}[H]
            \caption{\label{alg:PPO}MALS-PPO (proposed RL structure)}
            \begin{algorithmic}[1]
                \REQUIRE critic model $\phi$ and target model $\phi_{targ}$, downlink actor model $\theta^{\text{d}}$ and sampling model $(\theta^{\text{d}})'$, uplink actor model $\theta^{\text{u}}$ and sampling model $(\theta^{\text{u}})'$, initial state $s^{\text{d},1}$. \STATE $s^{\text{d},t} \leftarrow s^{\text{d},1}$;
                \FOR{iteration  $t=1,2,...$}
                    \STATE DL agent executes action according to $\pi_{(\theta^{\text{d}})'}(a^{\text{d},t}|s^{\text{d},t})$;
                    \STATE Get reward $R^{\text{d},t}$ and uplink state $s^{\text{u},t}$;
                    \STATE UL agent executes action according to $\pi_{(\theta^{\text{u}})'}(a^{\text{u},t}|s^{\text{u},t})$;
                    \STATE Get reward $R^{\text{u},t}$ and next step downlink state $s^{\text{d},t+1}$;
                    \STATE DL agent executes action according to $\pi_{(\theta^{\text{d}})'}(a^{\text{d},t+1}|s^{\text{d},t+1})$;
                    \STATE Get reward $R^{\text{d},t+1}$ and uplink state $s^{\text{u},t+1}$;
                    \IF{iteration $t\geq 2$}
                    \STATE Sample ($s^{\text{d},t},a^{\text{d},t},s^{\text{u},t},a^{\text{u},t},R^{\text{d},t},R^{\text{u},t},s^{\text{d},t+1},s^{\text{u},t+1}$) iteratively till end of episode;
                    \ENDIF
                    \STATE $s^{\text{d},t}  \leftarrow s^{\text{d},t+1}$, $s^{\text{u},t} \leftarrow s^{\text{u},t+1}$,
                    $a^{\text{d},t} \leftarrow a^{\text{d},t+1}$,
                    $R^{\text{d},t} \leftarrow R^{\text{d},t+1}$;
                
                    \STATE Compute advantages $\{A^{\text{d},t},A^{\text{u},t}\}$
                    \STATE Compute target values \{$V^{\text{d},t}_{targ},V^{u,t}_{targ}$\}
    
                    \FOR{$k$ = $1,2,...,K$}
                        \STATE Assign samples into training groups;
                        \FOR{each training group}
                            \STATE Compute UL and DL actor model gradients:
                            $\triangledown \theta^{\text{d}}, \triangledown \theta^{\text{u}}$
                            \STATE Apply gradient ascent on $\theta^{\text{d}}$ using $\triangledown \theta^{\text{d}}$ by Eq.~(\ref{eq:gradient2})
                            \STATE Apply gradient ascent on $\theta^{\text{u}}$ using $\triangledown \theta^{\text{u}}$ by Eq.~(\ref{eq:gradient1})
                            \STATE Update the critic model with loss from Eq.~(\ref{eq:HCloss})
                        \ENDFOR
                    \ENDFOR
                    \STATE Update the critic target model parameters $\phi_{targ}$ with the critic model parameters $\phi$ every $C$ steps where $C$ is the critic model update interval;
                \ENDFOR
            \end{algorithmic}
        \end{algorithm}
\vspace{-0.5cm}
\end{figure}

\vspace{-5pt}\section{Experiments \vspace{-5pt}}
\label{experiment}
We conduct several experiments and showcase the superiority of our MALS RL structure in handling asymmetric and asynchronous problems. We first discuss the numerical settings for the experiments we conduct. We then show how the proposed MALS algorithm is effective in improving resource management when compared against commonly adopted metrics. We then compare the proposed MALS model with baseline algorithms (i) independent dual agent (IDA)~\cite{zhang2021multi} and (ii) Centralized Training and Decentralized Execution multi-agent model (CTDE)~\cite{foerster2018counterfactual}. Finally, we vary the weighting on each variable in the DL and UL agents' objective functions to study the performance of the MALS model on varying variable weights and their implications on the model performance.
\vspace{-1.5mm}

\vspace{-10pt}\subsection{Baseline algorithms \vspace{-5pt}}
We pit the baseline algorithms (IDA and CTDE) against our proposed MALS model in tackling the asymmetric and asynchronous task. The IDA model encompasses two agents which are only dependent on the states each agent observes. The reward for the UL and DL agent are $R^{\text{u},t}$ and $R^{\text{d},t}$, respectively. The CTDE model is based on \cite{lowe2017multi} which utilizes two actors and identical critics. The rewards obtained by the critics are summed to produce an overall reward of $R^{\text{u},t}+R^{\text{d},t}$.
\vspace{-5mm}

% We compare some baselines with our proposed method in solving our problem. For demonstrating the performance of our algorithm, these baselines all contain two separate agents, and the agents work asynchronously just the same as our method. 


\subsection{Configuration} \label{Configuration}
% \vspace{-8mm}

\begin{figure*}[t]

\centering
\subfigtopskip=2pt
\subfigbottomskip=2pt
%\renewcommand*{\thesubfigure}{}

\subfigure[Average downlink delay.]{
\begin{minipage}[t]{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures/delay_down.pdf}
\label{fig:delay_down}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Average UE earning potential.]{
\begin{minipage}[t]{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures/earning_ability.pdf}
\label{fig:earning_ability}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Average uplink delay.]{
\begin{minipage}[t]{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures/log_delay_up.pdf}
\label{fig:log_delay_up}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Average battery consumption.]{
\begin{minipage}[t]{0.24\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures/log_battery.pdf}
\label{fig:log_battery_percent_consumed}
\vspace{-10mm}
\end{minipage}
}%
\caption{Key metrics performance obtained by MALS across training steps.}
\label{fig:complete_weight_1}
\vspace{-0.5cm}
\end{figure*}


% \begin{figure*}[htb]

% \centering
% \subfigtopskip=2pt
% \subfigbottomskip=2pt
% \renewcommand*{\thesubfigure}{}

% \subfigure[Downlink reward for MALS method.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{pictures/proposed_d.pdf}
% \label{fig:proposed_full_r_d_v2}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Uplink reward for MALS method.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{pictures/proposed_u.pdf}
% \label{fig:proposed_full_r_u_v2}
% \vspace{-10mm}
% \end{minipage}
% }%
% \subfigure[Downlink reward for IDA model.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{pictures/independent_d.pdf}
% \label{fig:independent_all_r_d}
% \vspace{-10mm}
% \end{minipage}%
% }%

% \subfigure[Uplink reward for IDA model.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{pictures/independent_u.pdf}
% \label{fig:independent_all_r_u}
% \vspace{-10mm}
% \end{minipage}
% }%
% \subfigure[Downlink reward for CTDE model.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{pictures/singleC_d.pdf}
% \label{fig:singlec_full_r_d_v2}
% \vspace{-10mm}
% \end{minipage}
% }%

\begin{figure*}[htb]

\centering
\subfigtopskip=2pt
\subfigbottomskip=2pt
%\renewcommand*{\thesubfigure}{}

\subfigure[Downlink reward for 3 MBS, 4 UE.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures3/all_d_34.pdf}
\label{fig:4a}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Downlink reward for 3 MBS, 5 UE.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures3/all_d_35.pdf}
\label{fig:4b}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Downlink reward for 3 MBS, 6 UE.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures3/all_d_36.pdf}
\label{fig:4c}
\vspace{-10mm}
\end{minipage}%
}%

\subfigure[Uplink reward for 3 MBS, 4 UE.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures3/all_u_34.pdf}
\label{fig:4d}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Uplink reward for 3 MBS, 5 UE.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures3/all_u_35.pdf}
\label{fig:4e}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Uplink reward for 3 MBS, 6 UE.]{
\begin{minipage}[t]{0.33\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures3/all_u_36.pdf}
\label{fig:4f}
\vspace{-10mm}
\end{minipage}
}%
\caption{Uplink and downlink agent rewards obtained for MALS model, Independent Dual Agent model, CTDE model, across congestion settings and seed 0 to 9. Bands within the graph indicate the range of reward values obtained across the seeds.}
\label{fig:complete_weight_2}
\vspace{-0.5cm}
\end{figure*}

% We design the in-game resolution-influenced earning potential function to be: $\omega(r^{\text{d},t}_i) = P\cdot\left (1-\text{exp}^{-r^{\text{d},t}_i}\right )$, where $P$ controls the maximum profit a UE can make. Intuitively, as UE $i$ downlink rate increases, downlink data size per unit time increases. This translates to an increase in the resolution of the in-game graphics, resulting in an increase in resolution-influenced in-game earning potential.

We design an experiment with three scenarios: 3 MBSs and 4 to 6 UEs to test our proposed Multi-Agent-Loss-Sharing (MALS) reinforcement learning-based method. $T$ which is defined as the number of transmission steps within an episode is set as $100$. Exhaustive search is an infeasible method to optimize the UE-MBS assignment as the sequential nature of our proposed problem renders the complexity to be $(M^{N})^{T}$, where $M$ is the number of MBS, $N$ is the number of UEs, and $T$ is the number of transmission iterations in an episode. The bandwidth $B$ is set as $10$ MHz, while the noise $\sigma^2$ is set as $100$ dBm. The output power by the MBS for the downlink transmission is set to be within $(1.5,2.0)$ Watt. We initialize the locations of the MBSs and UEs to be random, within a 100m by 100m area. The Metaverse in-game graphic size $D_i^t$ of each UE at every step is uniformly distributed between $800$ and $1000$ Mb. We set $P$, $b$ and $f$ to be 100, 50 and 75, respectively, after empirical tuning, and $h$ and $q$ to be 0.5. $P$ is defined in subsection~\ref{Configuration}, $b$, $f$, $h$ and $q$ are defined in subsection~\ref{problemform}. We set the values of both $\kappa_{1}$ and $\kappa_{2}$ from Equation (\ref{eq:HCloss}) to be $0.5$. We set the maximum distance UEs can move in each iteration ($x_{max}$ and $y_{max}$) to be 10m. The ADAM optimizer~\cite{adam} is utilized for the algorithms we study. We trained the models for 1,000,000 iterations (steps) for all congestion settings. We conducted the training and simultaneous evaluation of the models for each configuration at different seed settings: seed 0 to seed 9.
\vspace{-3mm}

% After multiple tests and extensive parameter adjustments, we list the critical hyper-parameter settings for different algorithms under different congestion degrees in Table \ref{table:parameter}.

% \begin{figure}[t]
% \centering
% \subfigtopskip=0pt
% \subfigbottomskip=0pt
% \subfigure[Downlink reward for proposed method.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{proposed_full_r_d_v2.csv.png}
% \label{fig:proposed_full_r_d_v2}
% \vspace{-20pt}
% \end{minipage}
% }%
% \caption{Simulation results.}
% \vspace{-0.5cm}


% \subfigure[Uplink reward for Proposed method.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{proposed_full_r_u_v2.csv.png}
% \label{fig:proposed_full_r_u_v2}
% \vspace{-20pt}
% \end{minipage}
% }%
% %\caption{Simulation results.}
% \vspace{-0.5cm}
% \end{figure}
\vspace{-5pt}\subsection{Result analyses \vspace{-5pt}}
\subsubsection{\textbf{MALS model convergence}}
In training our MALS model, all three configurations (i) 3 MBS, 4 UE, (ii) 3 MBS, 5 UE and (iii) 3 MBS, 6 UE showed a general increment of achieved test reward for both the downlink and the uplink models (shown in Figure \ref{fig:complete_weight_2}) and improvement in the underlying metrics (shown in Figure \ref{fig:complete_weight_1}). Note that at the start of training, the 3 MBS, 4 UE configuration received a higher reward than the 3 MBS, 5 UE and 3 MBS, 6 UE as there is less complexity in its scenario, allowing the model to converge quicker and achieve a higher reward. Conversely, a more complex model like 3 MBS, 6 UE converges slower due to the higher complexity of the action space and having more UEs to manage.  Overall, the improvement in both the downlink and uplink reward signifies that the overall utility function for both agents are improving. More explicitly, training of the downlink agent and uplink agent improves its UE-MBS allocation and output power allocation, respectively.

% It is important to note that the downlink agent reward can be positive as higher in-game worst-case earning capabilities of the UEs can earn the downlink agent positive rewards. On the other hand, the uplink rewards received by the uplink agent are strictly negative as both the factors, uplink latency and worst-case battery charge consumption receive negative rewards in each iteration.




% \begin{figure}[t]
% \centering
% \subfigtopskip=0pt
% \subfigbottomskip=0pt
% \subfigure[Average downlink delay.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{delay_down.png}
% \label{fig:delay_down}
% \vspace{-20pt}
% \end{minipage}
% }%
% \caption{Simulation results.}
% \vspace{-0.5cm}


% \subfigure[Average UE Earnings.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{earning_ability.png}
% \label{fig:earning_ability}
% \vspace{-20pt}
% \end{minipage}
% }%
% %\caption{Simulation results.}
% \vspace{-0.5cm}
% \end{figure}

\textbf{Downlink. }The gradual increment in the downlink reward, as training progresses, can be attributed to the decrease in in-game downlink transmission time (shown in Figure~\ref{fig:delay_down}) and increase in worst-case (lowest) earning potential of the UEs (shown in Figure~\ref{fig:earning_ability}) with each training step. These observations directly reflect that the downlink agent is allocating UE-MBS more efficiently, achieving a higher downlink data transfer rate which decreases the downlink latency and improves the worst-case (lowest) earning capability of the UEs. We should note that the simpler configurations such as 3 MBS, 4 UE and 3 MBS, 5 UE still achieves better rewards, shorter downlink latency and higher worst-case earnings in the final training steps as it is easier to obtain an optimal solution for simpler configurations as opposed to more complex scenarios such as 3 MBS, 6 UE.

% \begin{figure}[t]
% \subfigure[Average uplink delay.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{log_delay_up.png}
% \label{fig:log_delay_up}
% \vspace{-20pt}
% \end{minipage}
% }%
% \caption{Simulation results.}
% \vspace{-0.75cm}


% \subfigure[Average battery consumption percentage.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{log_battery_percent_consumed.png}
% \label{fig:log_battery_percent_consumed}
% \vspace{-20pt}
% \end{minipage}
% }%
% %\caption{Simulation results.}
% \vspace{-0.5cm}
% \end{figure}

\textbf{Uplink. }Similarly, we note that the improvement of the uplink reward received by the uplink agent, as training progresses, can be attributed to the more optimal use of UE transmission output power for the uplink transmission, resulting in a lower average uplink delay (shown in Figure.~\ref{fig:log_delay_up}) and lower worst-case (greatest) battery charge expenditure. This is reflected in Figure \ref{fig:log_battery_percent_consumed}. Note that both the UL transmission delay and worst-case (greatest) battery charge expenditure are shown in log-scale as the initial uplink transmission delay and worst-case (greatest) battery charge expenditure at the start of training were very large.

\subsubsection{\textbf{Comparison with other reinforcement learning model structures}} \label{rlstructcompare} We study how (i) our proposed model (MALS model structure) compared to other reinforcement learning-based structures. We adopt two reinforcement learning-based model structures; the most intuitive (ii) independent dual agent (IDA) (which employs two individual agents in which information transfer is only via the states), and (iii) CTDE (which is the framework used for multi-agent reinforcement learning~\cite{lowe2017multi}). 

% We conduct the training and simultaneous evaluation of the models for each configuration at different seed settings; seed 0 to seed 9.

Our proposed model (MALS model structure) achieves a smooth convergence for each configuration, exhibiting a narrow range of downlink and uplink rewards, across different seeds (shown in Figure \ref{fig:complete_weight_2}). The solid line in the charts represents the mean reward of the different seed settings, for each configuration. The color hue bands around the solid line represent the minimum (lower bound) and maximum (upper bound) of the rewards received. We observe that our proposed model obtains approximately around 20 points for the downlink reward and -4 for the uplink reward for each configuration. The narrow bands indicate that the proposed model is stable across the different seed settings, for each of the configurations.

% \begin{figure}[t]
% \centering
% \subfigtopskip=0pt
% \subfigbottomskip=0pt
% \subfigure[Downlink reward for Independent Dual Agent model.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{independent_all_r_d.csv.png}
% \label{fig:independent_all_r_d}
% \vspace{-20pt}
% \end{minipage}
% }%
% \caption{Simulation results.}
% \vspace{-0.5cm}


% \subfigure[Uplink reward for Independent Dual Agent model.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{independent_all_r_u.csv.png}
% \label{fig:independent_all_r_u}
% \vspace{-20pt}
% \end{minipage}
% }%
% %\caption{Simulation results.}
% \vspace{-0.5cm}
% \end{figure}

 \begin{figure*}[t]

\centering
\subfigtopskip=2pt
\subfigbottomskip=2pt
%\renewcommand*{\thesubfigure}{}

\subfigure[Earning potential wrt $q$.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures2/earning1.pdf}
\label{fig:earning1}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Downlink delay wrt $q$.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures2/delay_down1.pdf}
\label{fig:delay_down1}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Battery consumption   wrt $q$.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures2/log_bat1.pdf}
\label{fig:log_bat1}\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Uplink delay values wrt $q$.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures2/log_del_up1.pdf}
\label{fig:log_del_up1}
\vspace{-10mm}
\end{minipage}%
}%

\subfigure[Earning potential wrt $h$.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures2/earning2.pdf}
\label{fig:earning2}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Downlink delay wrt $h$.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures2/delay_down2.pdf}
\label{fig:delay_down2}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Battery consumption  wrt $h$.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures2/log_bat2.pdf}
\label{fig:log_bat2}\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Uplink delay   wrt $h$.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{pictures2/log_del_up2.pdf}
\label{fig:log_del_up2}
\vspace{-10mm}
\end{minipage}%
}%
\caption{Metric values with respect to (wrt)  different $q$ and $h$}
\label{fig:complete}
\vspace{-0.8cm}
\end{figure*}

On the other hand, the (ii) IDA model took significantly more steps to converge (shown in Figure \ref{fig:complete_weight_2}). We observe that the independent dual agents model obtained significantly higher downlink reward in the 3 MBS, 4 UE and 3 MBS, 5 UE configuration, with a mean of approximately 50 points for the 3 MBS, 4 UE configuration and for the 3 MBS, 5 UE configuration. However, the variance of downlink rewards obtained is much larger for the independent dual agents model, with rewards as high as sub-hundred in the 3 MBS, 4 UE configuration, and as low as in the tens for the 3 MBS, 6 UE configuration. This indicates instability and hyper-sensitivity of the model when handling different seed settings and minute changes in environment. Nevertheless, the excellence of the independent model observed in the downlink reward of some seeds seems to be reflected as a compromise in the performance of the independent dual agent's uplink reward. From Figure \ref{fig:4d}, \ref{fig:4e}, we observe that the average 3 MBS, 4 UE and 3 MBS, 5 UE configurations have a slight improvement in the uplink rewards in the early training steps, followed by a subsequent decline in rewards obtained. The variance of the rewards obtained for each configuration enormous as well, with the highest rewards obtained being close to 0 points, while the lowest rewards obtained being south of -20 points. Some seeds show an evident decline in uplink reward obtained as training proceeds. This shows a convergence failure for some of these seeds in the independent dual-agent model. It is apparent that the downlink agent's rewards are prioritised over the uplink agent's rewards in this case, which renders the IDA model unsuitable for real-world deployment.

The (iii) CTDE model achieves comparable performance to our proposed methods (MALS) for the 3 MBS, 4 UE configuration for both the downlink and uplink rewards (shown in Figure \ref{fig:4a}, \ref{fig:4d}), but achieved lackluster performance for both 3 MBS, 5 UE and 3 MBS, 6 UE configurations for both the downlink and uplink rewards (shown in Figure \ref{fig:4b}, \ref{fig:4c}, \ref{fig:4e}, \ref{fig:4f}). The CTDE produces a large variance of downlink and uplink reward outputs across the different seeds, albeit not as severe as the IDA model, as the model still converges in all seeds. However, the large variance and sub-par rewards obtained renders the CTDE inferior to our proposed methods due to its poorer performance, instability and unreliability.

\subsubsection{\textbf{Weighting Emphasis On Joint Utility function}}
 When dealing with joint optimization problems, it is essential for us to observe how our proposed model handles the varying weights on variables and their influence on variables of interest, as this provides future research insights on multi-agent joint utility function design. 


 
 \textbf{Downlink weight analysis. } Firstly, we vary the weight $q$ in the downlink reward $R^{\text{d},t}$ in subsection~\ref{subsection:reward} with values from the set of \{0, 0.25, 0.50, 0.75, 1\}. We then take the average results of the last 20,000 training steps the different variables of interest (downlink delay, worst-case (lowest) earning potential, uplink transmission delay, worst-case (greatest) battery charge expenditure). These averaged results are then compared with a random policy that assigns the downlink UE-MBS allocation and uplink power output randomly. We compare our proposed model with a random policy to show how our proposed model adds value in optimizing UE-MBS allocation and power selection. Please refer to the Appendix for the complete comparison with the reinforcement learning model structures discussed (IDA and CTDE). We do not focus on them as they have been shown to produce unreliable performance with high variances (in Fig. \ref{fig:complete1} and \ref{fig:complete2}).
 
%  and do not make worthy comparisons. 
 
%  Nevertheless, we included the complete results of the comparisons between all algorithms in Appendix~\ref{full_comparisons}.
 
 Across all $q$ weight values, the earnings of our proposed MALS model outperform random allocation (with the exception of $q$ = 1), which shows that our model improves the earning capability of the UEs (shown in Figure \ref{fig:earning1}). It is not surprising that the random policy agent allows users to earn higher earnings at $q$ = 1 than our model as higher $q$ values place more weight emphasis on downlink delay, and less weight emphasis on the worst-case (lowest) UE in-game earning capability. At $q$ = 1, the downlink agent in our proposed model focuses entirely on minimizing downlink latency, with no regard for maximizing the worst-case (lowest) UE earning capabilities. Another point to note is that, across each scenario configuration of our proposed method, the worst-case (lowest) UE earning potential decreases as $q$ increases. This is expected as the agent places more weight emphasis on minimizing downlink latency and less weight emphasis on maximizing the worst-case (lowest) UE earning potential at higher $q$ values.

We observe that our proposed downlink latency across different configurations, across different $q$ values, are much lower than the random policy with the exception of the 3 MBS, 6 UE configuration at $q$ = 0 (shown in Figure \ref{fig:delay_down1}). Poorer performance in 3 MBS, 6 UE configuration is not unexpected as at $q$ = 0, there is no weight emphasis on minimizing downlink delay, and whole emphasis on maximizing the worst-case (lowest) UE earning potential. Another thing to note is that the overall downlink latency for each configuration decreases as $q$ increases. This is because a higher value of $q$ increases its weight emphasis on minimizing agent downlink delay.




% \begin{figure}[t]
% \centering
% \subfigtopskip=0pt
% \subfigbottomskip=0pt
% \subfigure[Conclusive earnings across different q values.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{earning1.png}
% \label{fig:earning1}
% \vspace{-20pt}
% \end{minipage}
% }%
% \caption{Simulation results.}
% \vspace{-0.5cm}


% \subfigure[Conclusive downlink delay across different q values.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{delay_down1.png}
% \label{fig:delay_down1}
% \vspace{-20pt}
% \end{minipage}
% }%
% %\caption{Simulation results.}
% \vspace{-0.5cm}
% \end{figure}

% \begin{figure}[t]
% \subfigure[Conclusive battery consumption value across different q values.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{log_bat1.png}
% \label{fig:log_bat1}
% \vspace{-20pt}
% \end{minipage}
% }%
% \caption{Simulation results.}
% \vspace{-0.75cm}


% \subfigure[Conclusive uplink delay values across different q values.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{log_del_up1.png}
% \label{fig:log_del_up1}
% \vspace{-20pt}
% \end{minipage}
% }%
% %\caption{Simulation results.}
% \vspace{-0.5cm}
% \end{figure}

It may seem that the uplink model factors such as uplink delay and uplink battery charge consumption are not directly influenced by $q$ since $q$ is the weighting parameter that controls downlink model factors such as worst-case (lowest) UE earning ability and downlink latency. However, we note that for both the uplink transmission delay and worst-case (greatest) battery charge expenditure, there is a gradual decrease in both as the value of $q$ increases (shown in Figure \ref{fig:log_bat1}, \ref{fig:log_del_up1}), placing more weight emphasis on minimizing downlink transmission latency. We hypothesize that it could be the process of emphasizing minimization of the downlink latency which influences the downlink loss function, guiding the uplink agent to find a lower local minimum in the uplink agent's loss function (due to the shared loss function in proposed method). Extensive results are shown in Fig. \ref{fig:complete1}.

% \begin{figure*}[t]

% \centering
% \subfigtopskip=2pt
% \subfigbottomskip=2pt
% \renewcommand*{\thesubfigure}{}

% \subfigure[Conclusive earnings aross different h values.]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{earning2.png}
% \label{fig:earning2}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Conclusive downlink delay across different h values.]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{delay_down2.png}
% \label{fig:delay_down2}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Conclusive battery consumption value across different h values.]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{log_bat2.png}
% \label{fig:log_bat2}\vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Conclusive uplink delay values across different h values.]{
% \begin{minipage}[t]{0.5\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{log_del_up2.png}
% \label{fig:log_del_up2}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \caption{Complete results - changing $h$}
% \label{fig:complete}
% \vspace{-0.5cm}
% \end{figure*}

% \begin{figure}[t]
% \centering
% \subfigtopskip=0pt
% \subfigbottomskip=0pt
% \subfigure[Conclusive earnings aross different h values.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{earning2.png}
% \label{fig:earning2}
% \vspace{-20pt}
% \end{minipage}
% }%
% \caption{Simulation results.}
% \vspace{-0.5cm}


% \subfigure[Conclusive downlink delay across different h values.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{delay_down2.png}
% \label{fig:delay_down2}
% \vspace{-20pt}
% \end{minipage}
% }%
% %\caption{Simulation results.}
% \vspace{-0.5cm}
% \end{figure}


\textbf{Uplink weight analyses. }We vary the weighting variable $h$ which controls the weighting emphasis in the uplink model ($h$ in $R^{\text{u},t}$ within subsection~\ref{subsection:reward})  for the variables latency of uplink transmission and worst-case (greatest) battery charge expenditure across a set of values \{0, 0.25, 0.50, 0.75, 1\}. We find that the worst-case (greatest) battery charge expenditure for our proposed model is significantly lower than that of the random policy across each configuration and different $h$ values (shown in Figure \ref{fig:log_bat2}). This shows that our proposed model made a significant contribution in the power allocation process. On closer inspection of the changes in the worst-case (greatest) battery charge expenditure across the weight $h$, we notice that for each configuration, there is a general increase in the worst-case battery charge consumption as $h$ increases. This is because as $h$ increases, there is a greater weight emphasis for the uplink agent to minimize uplink latency as opposed to worst-case (greatest) battery charge expenditure.

% \begin{figure}[t]
% \subfigure[Conclusive battery consumption across different h values.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{log_bat2.png}
% \label{fig:log_bat2}
% \vspace{-20pt}
% \end{minipage}
% }%
% \caption{Simulation results.}
% \vspace{-0.75cm}


% \subfigure[Conclusive uplink delay across different h values.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{log_del_up2.png}
% \label{fig:log_del_up2}
% \vspace{-20pt}
% \end{minipage}
% }%
% %\caption{Simulation results.}
% \vspace{-0.75cm}
% \end{figure}







We also note that the uplink latency achieved by our proposed model for each configuration and across the different weights are much lower than the random policy (shown in Figure \ref{fig:log_del_up2}). For our proposed model, we note that the uplink delay increases as $h$ increases. This is expected as a higher value of $h$ increases the uplink weight emphasis on minimizing the uplink latency. Although it may seem that the factors in the downlink model are not directly influenced by the uplink model weighting $h$ which controls the agent's emphasis on uplink variables, there is a general decrease in downlink earnings and increase in downlink delay as weight $h$ increases (shown in Figure \ref{fig:earning2}, \ref{fig:delay_down2}). Extensive results are shown in Fig. \ref{fig:complete2}.
\vspace{-0.3cm}


\vspace{-7pt}\section{Conclusion \vspace{-7pt}}
\label{section:conclusion}

In this work, we consider a Metaverse play-to-earn mobile edge computing framework and formulate an asymmetric (discrete-continuous) and asynchronous (alternating DL and UL) dual-joint optimization where the Metaverse Service Provider's objective is to minimize in-game graphics downlink, uplink data transmission latency, and worst-case (greatest) battery charge expenditure, while maximizing the UEs' in-game resolution-influenced worst-case (lowest) earning potential. We then propose a novel multi-agent loss-sharing (MALS) RL model to tackle the abovementioned asynchronous and asymmetric problem, and demonstrate its superiority in performance over other methods. Finally, we conduct joint optimization weighting analyses and show the viability of utilizing our proposed MALS algorithm to tackle joint-optimization problems across different variable weighting emphases.



% \begin{figure}[t]
% \centering
% \subfigtopskip=0pt
% \subfigbottomskip=0pt
% \subfigure[Downlink reward for Single Critic model.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{singlec_full_r_d_v2.csv.png}
% \label{fig:singlec_full_r_d_v2}
% \vspace{-20pt}
% \end{minipage}
% }%
% \caption{Simulation results.}
% \vspace{-0.5cm}


% \subfigure[Uplink reward for Single Critic model.]{
% \begin{minipage}[t]{1\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{singlec_full_r_u_v2.csv.png}
% \label{fig:singlec_full_r_u_v2}
% \vspace{-20pt}
% \end{minipage}
% }%
% %\caption{Simulation results.}
% \vspace{-0.5cm}
% \end{figure}



% \section*{References}
\begin{spacing}{1.18}
\renewcommand{\refname}{~\\[-25pt]References\vspace{0pt} }

%\bibliographystyle{IEEEtran}
%\bibliography{ref}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{ICC2023MALS}
\BIBentryALTinterwordspacing
T.~J. Chua, W.~Yu, and J.~Zhao, ``Play to earn in the Metaverse over wireless
  networks with deep reinforcement learning,'' \emph{submitted to the 2023 IEEE
  International Conference on Communications (ICC)}. [Online]. Available:
  \url{https://personal.ntu.edu.sg/JunZhao/ICC2023MALS.pdf}
\BIBentrySTDinterwordspacing

\bibitem{goldman}
E.~Sheridan, M.~Ng, L.~Czura, A.~Steiger, A.~Vegliante, and K.~Campagna,
  ``Framing the future of web 3.0: Metaverse edition,''
  \url{https://www.goldmansachs.com/insights/pages/framing-the-future-of-web-3.0-metaverse-edition.html},
  2021.

\bibitem{browne_2021}
\BIBentryALTinterwordspacing
R.~Browne, ``Cash grab or innovation? the video game world is divided over
  {NFTs},'' Dec 2021. [Online]. Available:
  \url{https://www.cnbc.com/2021/12/20/cash-grab-or-innovation-the-video-game-world-is-divided-over-nfts.html}
\BIBentrySTDinterwordspacing

\bibitem{roos_2022}
\BIBentryALTinterwordspacing
N.~Roos, ``Best {P}lay-to-{E}arn games with {NFT} or crypto,'' November 2022.
  [Online]. Available: \url{https://www.playtoearn.online/games/}
\BIBentrySTDinterwordspacing

\bibitem{polka_city}
\BIBentryALTinterwordspacing
``Polka city.'' [Online]. Available: \url{https://www.polkacity.io/}
\BIBentrySTDinterwordspacing

\bibitem{english}
\BIBentryALTinterwordspacing
``Reality clash.'' [Online]. Available: \url{https://realityclash.com/}
\BIBentrySTDinterwordspacing

\bibitem{cai2022compute}
Y.~Cai, J.~Llorca, A.~M. Tulino, and A.~F. Molisch, ``Compute-and
  data-intensive networks: The key to the {M}etaverse,'' \emph{arXiv preprint
  arXiv:2204.02001}, 2022.

\bibitem{lowe2017multi}
R.~Lowe, Y.~I. Wu, A.~Tamar, J.~Harb, O.~Pieter~Abbeel, and I.~Mordatch,
  ``Multi-agent actor-critic for mixed cooperative-competitive environments,''
  \emph{Advances in {N}eural {I}nformation {P}rocessing {S}ystems}, vol.~30,
  2017.

\bibitem{Chua2022}
T.~J. Chua, W.~Yu, and J.~Zhao, ``Resource allocation for mobile Metaverse with
  the {Internet of Vehicles} over {6G} wireless communications: {A} deep
  reinforcement learning approach,'' in \emph{8th IEEE World Forum on the
  Internet of Things (WFIoT)}, 2022, also available online at
  \url{https://arxiv.org/pdf/2209.13425.pdf}\hfill

\bibitem{ICC2023Adversarial}
\BIBentryALTinterwordspacing
T.~J. Chua, W.~Yu, and J.~Zhao, ``Mobile edge adversarial detection for digital twinning to the Metaverse with deep reinforcement learning,'' \emph{submitted to the 2023
  IEEE International Conference on Communications (ICC)}. [Online]. Available:
  \url{https://personal.ntu.edu.sg/JunZhao/ICC2023Adversarial.pdf}
\BIBentrySTDinterwordspacing

\bibitem{han2022dynamic}
Y.~Han, D.~Niyato, C.~Leung, C.~Miao, and D.~I. Kim, ``A dynamic resource
  allocation framework for synchronizing {M}etaverse with {IoT} service and
  data,'' in \emph{ICC 2022-IEEE International Conference on
  Communications}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp.
  1196--1201.

\bibitem{ng2022unified}
W.~C. Ng, W.~Y.~B. Lim, J.~S. Ng, Z.~Xiong, D.~Niyato, and C.~Miao, ``Unified
  resource allocation framework for the edge intelligence-enabled
  {M}etaverse,'' in \emph{ICC 2022-IEEE International Conference on
  Communications}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp.
  5214--5219.

\bibitem{xu2022wireless}
M.~Xu, D.~Niyato, J.~Kang, Z.~Xiong, C.~Miao, and D.~I. Kim, ``Wireless
  edge-empowered {M}etaverse: A learning-based incentive mechanism for virtual
  reality,'' in \emph{IEEE International Conference on Communications (ICC)},
  2022, pp. 5220--5225.

\bibitem{chen2017resource}
M.~Chen, W.~Saad, and C.~Yin, ``Resource management for wireless virtual
  reality: Machine learning meets multi-attribute utility,'' in \emph{GLOBECOM
  2017-2017 IEEE Global Communications Conference}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2017, pp. 1--7.

\bibitem{wang2021meta}
Y.~Wang, M.~Chen, Z.~Yang, W.~Saad, T.~Luo, S.~Cui, and H.~V. Poor,
  ``Meta-reinforcement learning for immersive virtual reality over {THz/VLC}
  wireless networks,'' in \emph{ICC 2021-IEEE International Conference on
  Communications}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 1--6.

\bibitem{liu2018edge}
Q.~Liu, S.~Huang, J.~Opadere, and T.~Han, ``An edge network orchestrator for
  mobile augmented reality,'' in \emph{IEEE INFOCOM 2018-IEEE Conference on
  Computer Communications}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018,
  pp. 756--764.

\bibitem{wang2020user}
H.~Wang and J.~Xie, ``User preference based energy-aware mobile ar system with
  edge computing,'' in \emph{IEEE INFOCOM 2020-IEEE Conference on Computer
  Communications}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp.
  1379--1388.

\bibitem{chen2018virtual}
M.~Chen, W.~Saad, and C.~Yin, ``Virtual reality over wireless networks:
  Quality-of-service model and learning-based resource management,'' \emph{IEEE
  Transactions on Communications}, vol.~66, no.~11, pp. 5621--5635, 2018.

\bibitem{lu2020optimization}
H.~Lu, C.~Gu, F.~Luo, W.~Ding, and X.~Liu, ``Optimization of lightweight task
  offloading strategy for mobile edge computing based on deep reinforcement
  learning,'' \emph{Future Generation Computer Systems}, vol. 102, pp.
  847--861, 2020.

\bibitem{alfakih2020task}
T.~Alfakih, M.~M. Hassan, A.~Gumaei, C.~Savaglio, and G.~Fortino, ``Task
  offloading and resource allocation for mobile edge computing by deep
  reinforcement learning based on sarsa,'' \emph{IEEE Access}, vol.~8, pp.
  54\,074--54\,084, 2020.

\bibitem{RLC}
N.~C. Luong, D.~T. Hoang, S.~Gong, D.~Niyato, P.~Wang, Y.-C. Liang, and D.~I.
  Kim, ``Applications of deep reinforcement learning in communications and
  networking: A survey,'' \emph{IEEE Communications Surveys \& Tutorials},
  vol.~21, no.~4, pp. 3133--3174, 2019.

\bibitem{JO1}
D.~Guo, L.~Tang, X.~Zhang, and Y.-C. Liang, ``Joint optimization of handover
  control and power allocation based on multi-agent deep reinforcement
  learning,'' \emph{IEEE Transactions on Vehicular Technology}, vol.~69,
  no.~11, pp. 13\,124--13\,138, 2020.

\bibitem{JO2}
C.~He, Y.~Hu, Y.~Chen, and B.~Zeng, ``Joint power allocation and channel
  assignment for {NOMA} with deep reinforcement learning,'' \emph{IEEE Journal
  on Selected Areas in Communications}, vol.~37, no.~10, pp. 2200--2210, 2019.

\bibitem{pierce2012introduction}
J.~R. Pierce, \emph{An introduction to information theory: symbols, signals and
  noise}.\hskip 1em plus 0.5em minus 0.4em\relax Courier Corporation, 2012.

\bibitem{feng2022resource}
J.~Feng and J.~Zhao, ``Resource allocation for augmented reality empowered
  vehicular edge metaverse,'' \emph{arXiv preprint arXiv:2212.01325}, 2022.

\bibitem{yang2012crowdsourcing}
D.~Yang, G.~Xue, X.~Fang, and J.~Tang, ``Crowdsourcing to smartphones:
  Incentive mechanism design for mobile phone sensing,'' in \emph{Annual
  International Conference on Mobile Computing and Networking (MobiCom)}, 2012,
  pp. 173--184.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov, ``Proximal
  policy optimization algorithms,'' \emph{arXiv preprint arXiv:1707.06347},
  2017.

\bibitem{sutton1999policy}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour, ``Policy gradient
  methods for reinforcement learning with function approximation,''
  \emph{Advances in {N}eural {I}nformation {P}rocessing {S}ystems}, vol.~12,
  1999.

\bibitem{GAE}
J.~Schulman, P.~Moritz, S.~Levine, M.~Jordan, and P.~Abbeel, ``High-dimensional
  continuous control using generalized advantage estimation,'' \emph{arXiv
  preprint arXiv:1506.02438}, 2015.

\bibitem{zhang2021multi}
K.~Zhang, Z.~Yang, and T.~Ba{\c{s}}ar, ``Multi-agent reinforcement learning: A
  selective overview of theories and algorithms,'' \emph{Handbook of
  Reinforcement Learning and Control}, pp. 321--384, 2021.

\bibitem{foerster2018counterfactual}
J.~Foerster, G.~Farquhar, T.~Afouras, N.~Nardelli, and S.~Whiteson,
  ``Counterfactual multi-agent policy gradients,'' in \emph{Proceedings of the
  AAAI conference on artificial intelligence}, vol.~32, no.~1, 2018.

\bibitem{adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\end{thebibliography}



\end{spacing}

% \begin{center}
% Appendix: Comparisons between different reinforcement learning methods\vspace{-10pt}
% \end{center}

% \appendices
% \section{Comparisons between different reinforcement learning methods}
% \label{appendix:training}


\vspace{-7pt}\begin{center}
{Appendix: Weighting comparisons between different reinforcement learning methods \vspace{-12pt}} %\vspace{-10pt}
\end{center}


% \appendices
% \section{Comparisons}
% \label{full_comparisons}

\begin{figure*}[h]

\centering
\subfigtopskip=2pt
\subfigbottomskip=2pt
% \renewcommand*{\thesubfigure}{}

% \subfigure[Downlink Reward 34.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_d_1_34.png}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Downlink Reward 35.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_d_1_35.png}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Downlink Reward 36.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_d_1_36.png}
% \vspace{-10mm}
% \end{minipage}
% }%

% \subfigure[Uplink Reward 34.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_u_1_34.png}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Uplink Reward 35.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_u_1_35.png}
% \vspace{-10mm}
% \end{minipage}
% }%
% \subfigure[Uplink Reward 36.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_u_1_36.png}
% \vspace{-10mm}
% \end{minipage}
% }%

\subfigure[Downlink delay with 4 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/delay_down_1_34.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Earning potential with  4 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/earnings_1_34.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Up-link delay with 4 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_del_up_1_34.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Battery usage with 4 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_bat_1_34.pdf}
\vspace{-10mm}
\end{minipage}%
}%

\subfigure[Downlink delay with 5 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/delay_down_1_35.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Earning potential with  5 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/earnings_1_35.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Up-link delay with 5 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_del_up_1_35.pdf}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Battery usage with 5 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_bat_1_35.pdf}
\vspace{-10mm}
\end{minipage}%
}%

\subfigure[Downlink delay with 6 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/delay_down_1_36.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Earning potential with  6 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/earnings_1_36.pdf}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Up-link delay with 6 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_del_up_1_36.pdf}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Battery usage with 6 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_bat_1_36.pdf}
\vspace{-10mm}
\end{minipage}
}%
\caption{Complete results - Metric values across different $q$ values.}
\label{fig:complete1}
\vspace{-0.5cm}
\end{figure*}

\begin{figure*}[t]

\centering
\subfigtopskip=2pt
\subfigbottomskip=2pt
% \renewcommand*{\thesubfigure}{}

% \subfigure[Downlink Reward 34.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_d_2_34.png}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Downlink Reward 35.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_d_2_35.png}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Downlink Reward 36.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_d_2_36.png}
% \vspace{-10mm}
% \end{minipage}
% }%

% \subfigure[Uplink Reward 34.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_u_2_34.png}
% \vspace{-10mm}
% \end{minipage}%
% }%
% \subfigure[Uplink Reward 35.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_u_2_35.png}
% \vspace{-10mm}
% \end{minipage}
% }%
% \subfigure[Uplink Reward 36.]{
% \begin{minipage}[t]{0.33\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{appendix_pictures/r_u_2_36.png}
% \vspace{-10mm}
% \end{minipage}
% }%

\subfigure[Downlink delay with 4 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/delay_down_2_34.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Earning potential with  4 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/earnings_2_34.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Up-link delay with 4 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_del_up_2_34.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Battery usage with 4 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_bat_2_34.pdf}
\vspace{-10mm}
\end{minipage}%
}%


\subfigure[Downlink delay with 5 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/delay_down_2_35.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Earning potential with 5 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/earnings_2_35.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Up-link delay with 5 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_del_up_2_35.pdf}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Battery usage with 5 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_bat_2_35.pdf}
\vspace{-10mm}
\end{minipage}%
}%


\subfigure[Downlink delay with 6 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/delay_down_2_36.pdf}
\vspace{-10mm}
\end{minipage}
}%
\subfigure[Earning potential with 6 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/earnings_2_36.pdf}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Up-link delay with 6 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_del_up_2_36.pdf}
\vspace{-10mm}
\end{minipage}%
}%
\subfigure[Battery usage with 6 UEs.]{
\begin{minipage}[t]{0.25\linewidth}
\centering
\includegraphics[width=1\linewidth]{appendix_pictures_2/log_bat_2_36.pdf}
\vspace{-10mm}
\end{minipage}
}%
\caption{Complete results - Metric values across different $h$ values.}
\label{fig:complete2}
\vspace{-0.5cm}
\end{figure*}

Fig.~\ref{fig:complete1} and \ref{fig:complete2} on the next page show how the performance metrics (downlink delay, worst-case (lowest) earning potential, uplink delay and worst-case (greatest) UE battery charge expenditure) vary across different $q$ and $h$ values. The changes in $q$ and $h$ corresponds to changing weight emphasis between variables downlink latency and resolution-influenced earning potential, and uplink transmission latency and worst-case (greatest) UE battery charge expenditure, respectively. Value of $h$ is $0.5$ when $q$ is varying, and vice versa.

% By varying the values of $q$ and $h$, we can observe how our proposed HAPPO model compare to baseline models in terms of performance metrics and model reliability, across different weight emphasis. 
 

\end{document}
