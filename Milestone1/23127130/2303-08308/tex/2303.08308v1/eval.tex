
\section{Evaluation}


\noindent\textbf{Setup}. We evaluate our method on ImageNet-1k dataset~\cite{imagenet} and two popular edge devices. The INT8 latency constraints  are \{8, 10, 15, 20, 25\} \textit{ms} for Intel CPU, and \{15, 20, 25, 30, 35\} \textit{ms} for Pixel4.  For each device, we search 5k search spaces in total and return the one with highest Q-T score. The population size $P$ is 500 and sample size $S$ is 125. 

Once {\algname} discovers a quantization-friendly search space for the target device, we train a \textit{quantized-for-all} supernet. 
We start by pretraining a full-precision supernet without quantizers on ImageNet for 360 epochs. We adopt the sandwich rule and inplace distillation  in BigNAS~\cite{bignas}.  Then, we perform quantization-aware training (QAT) on the trained supernet for 50 epochs, which follows the same training protocol (i.e., sandwich rule and inplace distillation). We use LSQ+ as the QAT algorithm for better quantized accuracy.
% The full-precision supernet follows a similar training receipt and hyperparameter setting in BigNAS~\cite{bignas} and AlphaNet~\cite{alphanet}. For the supernet QAT, we use a 10$\times$ smaller initial learning rate. 
To derive INT8 model for deployment, we use the evolutionary search in OFA~\cite{ofa} to search 5k models for various given INT8 latency constraints. We list out detailed training settings in supplementary materials.  In the following, we refer to the two searched spaces as  \textit{{\algname}@CPU} and \textit{{\algname}@Pixel4}, the searched model families are  \textit{{\sysname}@cpu} and \textit{{\sysname}@pixel4}. 
 
 
 
 %The searched models can be directly deployed without retraining.
 
% \subsection{Search Space Search for Different Hardware}
 
 %Report the search results and compare the difference with sota search spaces. 
% Summarize quantization-friendly search space design.
 
% \textbf{Quality comparison with SOTA search spaces}. 
 
 %\textbf{Implications}. 
%\vspace{-0.2ex}
\vspace{-1ex}
\subsection{The Effectiveness of {\algname}}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{searchspace_comparison2.png}	
	\vspace{-4ex}
	\caption{Best searched INT8 models with comparison to state-of-the-art NAS search spaces. Our searched spaces are proven to be the most quantization-friendly for the target device.}
	\label{fig:searchspace_comparison}
\end{figure}


\noindent\textbf{Comparison with SOTA search spaces}. To demonstrate the high-performance of our searched spaces,  we compare  with prior art manually-designed search spaces including: (1) MobileNetV3 search space that is adopted in two-stage quantization NAS  OQAT~\cite{oqa} and BatchQuant~\cite{batchquant}; (2)  ProxylessNAS and AttentiveNAS search spaces that achieve superior performance on mobile devices; and (3) ResNet50 search space proposed by OFA that is a  handcraft quantization-friendly space on our two devices. 
%quantization-friendly ResNet50 search space~\citep{ofa}. While Conv block is quantization-friendly on all measured devices, we add ResNet50 space as baseline. 
For fair comparison, we use one supernet training and QAT receipt for all search spaces.    We conduct evolutionary search to compare the  best-searched models from each search space. %Specifically, for each space, we measure its min and max latency and search the Pareto-frontier models within that range.  
We use the random seed of 0.  For all experiments, search space is the only difference.



 Fig.~\ref{fig:searchspace_comparison} compares the best searched INT8 models from different search spaces. {\algname}@CPU and {\algname}@Pixel4 consistently deliver superior quantized models than state-of-the-art search spaces. Under the same latency, the best quantized models from {\algname}@cpu significantly surpass the existing state-of-the-art search spaces with +0.7\% to +3.8\% (+0.4\% to +3.2\% on Pixel4 ) higher accuracy. Moreover, our search space is the only one that is able to deliver superior quantized models under both extremely low (only$\sim$5ms) and large latency constraints.   %Results also demonstrate that 
%Compared to the manual-designed quantization-friendly ResNet50 space, our spaces deliver superior
%our tiny models are 3.6$\times$ faster and 2.5$\times$ faster while achieving a same-level accuracy. 
%our models run 3.6$\times$ faster and 2.5$\times$ faster than the best models when achieving a same accuracy (shown in Fig.~\ref{fig:searchspace_comparison}), our models run 3.6$\times$ faster and 2.5$\times$ faster than the best models from ResNet50 space on VNNI and Pixel4, respectively. 

%Specifically, AttentiveNAS is a newly state-of-the-art space. Compared to AttentiveNAS, our search spaces deliver better quantized models with +0.4\% -1.8\% higher accuracy under a same latency. Moreover, our spaces can deliver lower latency models than AttentiveNAS. Finally, our spaces also outperform existing quantization-friendly ResNet50 space, where the sub-networks can achieve higher speedups ($\sim$3$\times$) than us. Remarkably, when achieving a same accuracy (shown in Fig.~\ref{fig:searchspace_comparison}), our searched models run 3.6$\times$ faster and 2.5$\times$ faster than the best models from ResNet50 space on VNNI and Pixel4, respectively.
%Now we demonstrate how NAS can benefit from a high-quality search space.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\columnwidth]{smallsearchspace1.png}
	\vspace{-1ex}
	\caption{Search space design under diverse INT8 latency constraints. {\algname} (6-25 ms) delivers superior tiny INT8 models.}
	\label{fig:smallspace}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{searchcost.png}
	\vspace{-4ex}
	\caption{Search cost measured on 8 Nvidia V100 GPUs. }
	\label{fig:searchcost}
\end{figure}

\noindent\textbf{{\algname} under diverse latency constraints.} We extensively study the effectiveness of {\algname} under different latency constraints. Specifically, we perform  space search under two tight constraints of \{10, 15, 20, 25, 30\}ms and \{6, 10, 15, 20, 25\}ms on Pixel4. The results are shown in 
Fig.~\ref{fig:smallspace}. Our proposed method can handle the diverse latency requirements and produce  high-quality spaces.  As expected, the searched spaces under 10-30ms and 6-25ms have much more low-latency quantized models.

 To further verify the effectiveness of these low-latency models, we compare with existing SOTA tiny models. Significantly, even under the extremely low latency constraints of 6-25 ms, our searched space delivers very competitive tiny quantized models. Compared to the smallest model ShuffleNetV2x0.5, we can achieve +10.1\% higher accuracy under the same latency of 4.3 ms. 



%\lz{vnni test on pixel4, pixel4 test on vnni}
% \lz{oqa and batchquant are the most relevant , they use mobilenetv3}


\begin{table}
	%	\begin{adjustwidth}{-.12in}{-.1in}  
		\centering
		\fontsize{8.2}{8.2} \selectfont
		%	\small
		\caption{ImageNet results compared with SOTA  quantized models on two devices. $^*$: latency compared to FP32 inference.  }
		\vspace{-2.5ex}	
		\begin{tabular}	
			%{@{\hskip1pt}c@{\hskip1pt}|@{\hskip2pt}c@{\hskip2pt}|@{\hskip3pt}c@{\hskip1pt}c@{\hskip1pt}|c@{\hskip2pt}c@{\hskip0pt}}	
			{@{\hskip1pt}c|c|cc|c@{\hskip1pt}c@{\hskip1pt}}			
			\toprule
			\multicolumn{6}{c}{\textbf{(a) Results on the Intel VNNI CPU with onnxruntime}}\\
			\multirow{2}{*}{Model}  &Acc\%& \multicolumn{2}{c|}{CPU Latency}&Acc\%& \multirow{2}{*}{FLOPs} \\
			&\textbf{ INT8}& \textbf{INT8}&\textbf{speedup$^*$} &   FP32&\\ 
			
			\midrule
			MobileNetV3Small & 66.3&4.4 ms &1.1$\times$  & 67.4 &56M \\
			\textbf{	\sysname @cpu-A0}   & \textbf{74.7} & \textbf{4.4 ms} & \textbf{2.0$\times$} &74.8& 163M\\
			\hline
			MobileNetV2 &71.4 &7.3 ms &2.2$\times$ &72.0&300M \\
			
			ProxylessNAS-R &74.6  &8.8 ms  &1.8$\times$&74.6 &320M \\
			OQAT-8bit & 74.8 & 9.8 ms &1.8$\times$ &75.2&214M \\
			MobileNetV3Large  &74.5&10.3 ms &1.5$\times$&75.2 &219M  \\
			
			OFA (\#25)  &75.6&11.2 ms & 1.5$\times$ &76.4&230M\\
			\textbf{	\sysname @cpu-A1}    & \textbf{77.4} & \textbf{8.8 ms} & \textbf{2.4$\times$}&77.5 & 358M\\
			\hline
			APQ-8bit& 73.6&15.0 ms & 1.5$\times$& 73.6& 297M\\
			
			AttentiveNAS-A0  &76.1 &15.1 ms &1.4$\times$& 77.3 & 203M\\
			OQAT-8bit & 76.3 & 14.9 ms &1.7$\times$ &76.7& 316M\\
			EfficientNet-B0 &76.7&18.1 ms & 1.6$\times$&77.3 &390M\\
			\textbf{	\sysname @cpu-A2} &\textbf{78.5} & \textbf{14.1 ms} & \textbf{2.4$\times$}  &78.8 & 638M\\
			\hline
			APQ-8bit  & 74.9&20.0 ms & 1.5$\times$&75.0&393M\\
			OQAT-8bit  & 76.9 & 19.5 ms &1.6$\times$&77.3&405M \\
			AttentiveNAS-A1&77.2&22.4 ms &1.4$\times$&78.4&279M \\
			AttentiveNAS-A2&77.5&22.5 ms & 1.3$\times$ &78.8&317M\\	
			\textbf{	\sysname @cpu-A3}  & \textbf{79.5} & \textbf{18.9 ms}& \textbf{2.6$\times$}&79.6& 981M \\
			\hline
			FBNetV2-L1 &75.8&25.0 ms &1.2$\times$ &77.2& 325M\\
			FBNetV3-A &78.2 &27.7 ms&1.3$\times$& 79.1 & 357M \\
			\textbf{	\sysname @cpu-A4}   &\textbf{80.0} & \textbf{24.4 ms} &\textbf{ 2.4$\times$}& 80.1& 1267M \\
			\toprule
			\multicolumn{6}{c}{ \textbf{(b) Results on the Google Pixel 4 with TFLite}}\\
			\multirow{2}{*}{Model}  &Acc\%& \multicolumn{2}{c|}{Pixel4 Latency}&Acc\%& \multirow{2}{*}{FLOPs} \\
			&\textbf{ INT8}& \textbf{INT8}&\textbf{speedup$^*$} &   FP32&\\ 
			
			\midrule
			
			%	MobileNetV1 & & & & & & &\\
			MobileNetV3Small & 66.3&6.4 ms & 1.3$\times$& 67.4 &56M \\
			\textbf{	\sysname @pixel4-A0}& \textbf{73.6} & \textbf{5.9 ms} & \textbf{2.1$\times$}  &73.7 & 107M \\
			\hline
			MobileNetV2  &71.4&16.5 ms  & 1.9$\times$&72.0&300M\\
			ProxylessNAS-R  &74.6 &18.4 ms &1.8$\times$&74.6&320M  \\
			
			MobileNetV3Large   &74.5&15.7 ms &1.5$\times$&75.2&219M \\
			APQ-8bit & 74.6&14.9 ms &2.0$\times$ &74.4&340M\\
			OFA (\#25) &75.6&14.8 ms & 1.7$\times$&76.4 &230M\\
			OQAT-8bit & 75.8 & 15.2 ms & 1.9$\times$&76.2 &287M\\
			AttentiveNAS-A0 &76.1 &15.2 ms  &2.0$\times$& 77.3& 203M \\
			\textbf{	\sysname @pixel4-A1}& \textbf{77.6} & \textbf{14.7 ms} & \textbf{2.2$\times$} & 77.7& 274M  \\
			\hline
			APQ-8bit  &75.1 & 20.0 ms &1.9$\times$&75.1&398M \\
			OQAT-8bit  & 76.5 & 20.4 ms & 1.8$\times$&76.8&347M\\
			AttentiveNAS-A1&77.2&21.1 ms & 2.0$\times$&78.4&279M\\
			AttentiveNAS-A2&77.5&22.7 ms & 2.0$\times$ &78.8&317M\\	
			
			\textbf{	\sysname @pixel4-A2} & \textbf{78.3} & \textbf{19.4 ms} & \textbf{2.3$\times$} & 78.4& 402M\\
			\hline
			%	APQ-8bit && &75.5 & 30 ms & \\
			
			FBNetV2-L1 &75.8&26.7 ms &1.5$\times$&77.2& 325M \\
			OQAT-8bit  & 77.0 & 29.9 ms & 1.7$\times$&77.2&443M\\
			FBNetV3-A  &78.2 &30.5 ms & 1.5$\times$& 79.1& 357M\\
			\textbf{	\sysname @pixel4-A3}  & \textbf{79.5} & \textbf{30.8 ms} & \textbf{2.1$\times$} & 79.5& 591M\\
			\hline
			EfficientNet-B0 &76.7&36.4 ms &1.7$\times$&77.3&390M \\
			\textbf{	\sysname @pixel4-A4}   & \textbf{79.9} & \textbf{35.5 ms} & \textbf{2.2$\times$} &80.0& 738M\\				
			\hline
		\end{tabular}
		\label{tbl:endtoend}
	\end{table}

\noindent\textbf{Search cost}.  As depicted in Fig.~\ref{fig:searchcost}, our algorithm, {\algname}, is designed to be lightweight and suitable for real-world usage, requiring only 25 GPU hours to search a space of 5000 iterations. This remarkable speed is mainly due to our block-wise search space quantization scheme, which significantly reduces the cost of search space quality evaluation. In comparison, Fig.~\ref{fig:searchcost} demonstrates that training each search space from scratch without this scheme would consume an impractical 1200k GPU hours.


% The total search cost is the sum of space search cost (24 hours) and a standard quantization-for-all supernet training and model search cost (312 hours). Compared to existing once-for-all NAS such as BigNAS, 
%applying {\algname} requires an additional space search process. Therefore, we mainly compare the space search cost with STOA methods. The cost is evaluated as the time to
%obtain search spaces for $N$ deployment scenarios (one scenario is denoted by a given range of latency constraints). %Compared to NSE and S3, our method can search both the operators and configurations for a high-quality space under a set of target latency constraints. 
%As  in Table ~\ref{tbl:searchcost1},  we can search a  space  under a given constraint ($N$=1)  in 1 day with much more iterations, which saves cost by 22$\times$ and 38$\times$ compared to NSE~\cite{nse} and S3~\cite{autoformerv2}, respectively. %NSE and S3 are time-consuming, as they need to train each search space from scratch for quality evaluation. In contrast, {\algname} only requires a preprocess of accuracy lookup table construction (24 hours), which amounts one time cost. %Specifically, the blockwise quantization can be trained concurrently by stage in 24 hours. During the space search, we can directly evaluate the quality by accuracy lut and nn-Meter without training. Thus,  an iteration can be done in seconds. 
%{\algname} is lightweight and feasible for real-world usage. 





\subsection{The Effectiveness of  Discovered INT8 Models}
\label{sec:mainresult}
%We now perform evolutionary search over the searched spaces. Specifically, we first use nn-Meter to profile the latency bounds of the spaces, then we set a 
In this section, we demonstrate that our searched spaces deliver state-of-the-art quantized models. We compare with two strong baselines: (1) \textit{prior art manually-designed and NAS-searched models}; and (2) \textit{quantization-aware NAS}. For baseline (1), we collect official pre-trained FP32 checkpoints and conduct LSQ+ QAT to get the quantized accuracy. The hyperparameter settings follow the original LSQ+ paper, except that we set a larger epoch of 10 to achieve better accuracy. The latency numbers are measured on our devices. For (2), we compare with strong baselines including APQ~\cite{apq} and OQAT~\cite{oqa}. Specifically, we limit APQ to search for the fixed 8bit (INT8) models. Since OQAT has no 8bit supernet checkpoint, we follow the official source code and conduct supernet QAT for 50 epochs. The final INT8 models are searched under the same INT8 latency predictors for fair comparison.



%We first compare with design-then-quantize method.  We set strong baselines by picking prior art NAS models on ImageNet, including MobileNetV3~\citep{mobilenetv3}, OFA~\citep{ofa}, FBNetV2~\citep{fbnetv2}, FBNetV3~\citep{fbnetv3}, AttentiveNAS~\citep{attentivenas}, ProxylessNAS~\citep{proxylessnas} and EfficientNet~\citep{efficienetnet}. For fair comparison, we collect the official pre-trained checkpoints and conduct a long-time LSQ+~\citep{lsqplus} QAT to get the quantized accuracy. We follow the original settings in LSQ+, and set a larger training epoch of 10 to achieve better accuracy.
\noindent\textbf{Results}.  Table~\ref{tbl:endtoend}  summarizes comparison results. Remarkably, our searched model family,  {\sysname} significantly outperform SOTA efficient models and quantization-aware NAS searched models, with higher INT8 quantized accuracy, lower INT8 latency and better speedups.
  %Remarkably, our searched model family, {\sysname} significantly outperform SOTA  models and quantization-aware NAS with higher INT8 quantized accuracy, lower INT8 latency and better speedups.
Without finetuning, our tiny models {\sysname}@cpu-A0 and  {\sysname}@pixel4-A0 achieve 74.7\% and 73.6\% top1 accuracy on ImageNet, which is 8.4\% and 7.3\% higher than MobileNetV3-Small (56M FLOPs) while maintaining the same level quantized latency. For larger models, {\sysname}@cpu-A4 (80.0\%) outperforms FBNetV3-A with 1.8\% higher accuracy while runs 3.3ms faster.  In particular, to achieve the same level accuracy (i.e., around 77.2\%), AttentiveNAS-A1 has 22.4ms latency while 
 {\sysname}@cpu-A1 (77.4\%) only needs 8.8 ms (2.6 $\times$ faster). More importantly, our searched models can better utilize the INT8 hardware optimizations:  the latency speedups compared to full-precision inference are all larger than 2$\times$, and this leaves room to search large-size models with higher accuracy.
	
%\begin{table}[t]
%	\begin{center}

%		\fontsize{8.8}{8.8} \selectfont
		
%		\begin{tabular}	{c|c|c|c|cc|cc}
%			\hline
%				\multirow{2}{*}{Model}& \multirow{2}{*}{MFLOPs}&Top1-Acc & Top1-Acc	& \multicolumn{2}{c|}{Latency on Pixel 4} & \multicolumn{2}{c}{Latency on VNNI }\\
			
%				 & &FP32(\%)&\textbf{INT8}(\%) & {FP32  }& {INT8  }& {FP32 }& INT8  \\
			
%			\hline 
		%	MobileNetV1 & & & & & & &\\
%			MobileNetV3Small &56M & 67.4 & 66.3&8.01 &6.35 &4.79 &4.40\\
%			MobileNetV2&300M &72.0 &71.4&31.46&16.54 &16.22&7.24\\
		
%		ProxylessNAS (mobile) &320M&74.6 &74.6 &33.67&18.43 &16.29&8.83 \\
%		MobileNetV3Large &219M&75.2  &74.5&24.30 &15.71&16.15&10.25\\
	
%		OFA (\#25) &230M&76.4 &75.6&25.4&14.77&17.20&11.21\\
%		\hline
%		AttentiveNAS-A0 & 203M & 77.3&76.1 &31.43&15.15 &20.91&15.07\\
		%FBNetV2-F4 & 238M & 76.0 && & &23.58&18.11\\
%		EfficientNet-B0 &390M&77.3&76.7&61.34 &36.39&29.93&18.12\\
%		\hline
%		AttentiveNAS-A1&279M&78.4&77.2&43.07&21.09&31.20&22.38\\
%		AttentiveNAS-A2&317M&78.8&77.5&47.20&22.69&30.27&22.49\\	
%		FBNetV2-L1& 325M&77.2 &75.8&39.21 &26.26 &31.72&25.03\\
%		FBNetV3-A & 357M& 79.1 &78.2 &46.87&30.48 &36.03&27.69\\
		
		
	%		\hline
	%	\end{tabular}
%		\caption{Comparison with design-then-quantize models. We select state-of-the-art FP32 models for comparison. }
%		\label{tbl:endtoend}
%	\end{center}
%\end{table}

%\noindent\textbf{Comparison with fixed-precision quantization-aware NAS}.
%compare the advantages of end-to-end results. 
%\lz{use their original search spaces, supernet training for int8, and search constraint (flops and int8 latency) }
%SPOS
%APQ
%OQA+lsq, FLOPS,



%\noindent\textbf{Comparison with Quantization-aware NAS.} We now compare {\sysname} with quantization-aware NAS.  OQA, APQ

%\lz{search larger latency constraints. then say under the same accuracy, we are xx faster than OQA and APQ.}









%\begin{figure}[t]
%	\centering
%	\includegraphics[width=1\columnwidth]{figs/results/searchspace_comparison.png}	
	
%	\caption{Comparison of best searched INT8 models from different spaces. Our searched spaces deliver a much wider latency range of better  quantized models than existing SOTA search spaces.  }
%	\label{fig:searchspace_comparison}
%\end{figure}




%\begin{figure*}
%	\centering
%	\begin{minipage}{.64\textwidth}
%		\centering
%	\includegraphics[width=1\textwidth]{figs/results/searchspace_comparison1.png}	
	
%	\caption{Comparison of best searched INT8 models from different spaces. Our searched spaces deliver a much wider latency range of better  models than  existing manually-designed search spaces.  }
%	\label{fig:searchspace_comparison}
%	\end{minipage}%
%	\hfill
%	\begin{minipage}{.32\textwidth}
%		\centering
%		\includegraphics[width=1\linewidth]{figs/results/smallsearchspace.png}
%		\captionof{figure}{Space search under diverse INT8 latency constraints. {\algname}(6-25) delivers superior tiny INT8 models.}
%		\label{fig:smallspace}
%	\end{minipage}
%\end{figure*}




%\begin{itemize}
%	\item \textit{Different devices require a specialized quantized search space.}
%	\item \textit{Shallow stages should set large channel numbers that can best utilize INT8 quantization optimizations.} 
%	\item\textit{The shallow stages should set quantization-friendly blocks for the given device, while the deep stages should maintain accuracy-crucial blocks. }
%\end{itemize}
\vspace{-1.5ex}
\subsection{Ablation Study}
\vspace{-1.5ex}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{spaceranking1.png}
	\vspace{-1.5ex}
	\caption{Q-T score  effectiveness (Kendall's $\tau$) on ranking search space quality. We achieve a high space ranking correlation. }
	\label{fig:rankingmodel}
\end{figure}




\noindent\textbf{Q-T  score effectiveness}. Q-T score is crucial as it guides the space evolution process. To evaluate its effectiveness, we randomly sample 30 search spaces, and measure the rank correlation (Kendall's $\tau$) between their Q-T score and their actual Pareto-frontier models' accuracies. Specifically,  we use Intel CPU as the test device and set a same latency constraints of \{8, 10, 15, 20, 25\}ms. For each sampled space, we train it from scratch for 50 epochs, and conduct evolutionary search to get the Pareto-frontier models' accuracies. As shown in Fig.~\ref{fig:rankingmodel}, the Kendall's $\tau$ between the Q-T score and the actual Pareto-frontier models' accuracies is 0.8, which indicates a very high rank correlation. 

%Since the space evolution process is guided by Q-T score, it's crucial 
%its effectiveness on ranking different search spaces is crucial for space search. 
%We measure its effectiveness by quantifying the performance of ranking across different search spaces. Specifically, we use VNNI as the test device and set the latency constraints of [8, 10, 15, 20, 25] ms. Then, we randomly sample 30 search spaces and record their P-T scores. For each search space, we train it from scratch for 50 epochs. As for space ranking test, we conduct evolutionary each and evaluate the Pareto-Frontier models' accuracy in each space. Fig.~\ref{fig:rankingmodel}  shows the performance of how P-T score rank across search spaces. Each sampled space has a point in the figure, representing the correlation between its P-T score and true accuracy of top-tier models. Our results show a high correction (kt=0.67) between P-T score and practical search space quality. Notably, P-T score can rank well across top search spaces, which proves the effectiveness of using P-T score to guide the space evolution.


 

\begin{table}[t]
	\centering
	\fontsize{8.5}{8.5} \selectfont
	%\small
	\caption{Different space search methods and their best resulting quantized models on Pixel4. Baseline is a SOTA mobile-friendly AttentiveNAS space. $^*$: the search dimension use the same settings in AttentiveNAS. }
	\vspace{-2.5ex}
	\begin{tabular}
		%	{@{\hspace{0.3\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.3\tabcolsep}}}	
		%	{ccccc}
		{@{\hspace{0.2\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{0.2\tabcolsep}}}
		\hline
		\multirow{2}{*}{Method} &\multirow{2}{*}{Op} & \multirow{2}{*}{Width} &\multicolumn{5}{c}{Best quantized models}\\
		& & &10ms& 15ms & 20ms  &30ms & 36ms\\
		
		\hline 
		Baseline&-&-&-&76.6&77.2 &79.0&79.5\\
		{\algname}-op & search & fix$^*$&75.0&76.6&77.8 &78.6&78.8\\
		{\algname}-width &fix$^*$ &search& 75.4&77.4&78.0  &79.1&79.5\\
		{\algname} &search&search&\textbf{75.7}&\textbf{77.6} & \textbf{78.3} &\textbf{79.5}&\textbf{79.9}\\
		\hline
	\end{tabular}
	%\vspace{-2.5ex}
	\label{tbl:search_compare}
\end{table}

\noindent\textbf{Ablation study on two search dimensions}. In Sec.~\ref{sec:analysis}, we conclude that operator type and configuration are two key factors impacting INT8 latency, which serves as the two search objectives of {\algname}. To verify the effectiveness, we create two strong baselines based on  the SOTA edge-regime AttentiveNAS search space: (1) {\algname}-op: we fix each elastic stage's width to AttentiveNAS  space, then allow each elastic stage to search for the optimal operator; and (2) {\algname}-width: we fix all elastic stages' block types to AttentiveNAS space, then  search for the optimal width. Table~\ref{tbl:search_compare} reports the space comparison between different search methods on the Pixel4. By searching both operator type and width, {\algname} finds the optimal search space where its best searched quantized models achieve the highest accuracy under all latency constraints. Moreover, even searching for one dimension, {\algname}-op and {\algname}-width outperform  the manually-designed AttentiveNAS space under small latency constraints. 



\noindent\textbf{Search space design implications}. We now summarize our
learned experience and implications for designing quantization-friendly search spaces. We notice that the searched spaces show different preferences when targeting different devices: (i) All stages should use much wider channel widths compared to existing manually-designed spaces on the cpu, while only early stages prefer wider channels on Pixel  4. (ii) Since SE and Swish are INT8 latency-friendly on mobile phones, so our auto-generated search spaces for Pixel4 have many MBv3 stages. On Intel CPU, INT8 quantization slows down SE, Hardwish, and Swish, making FusedMB and MBv2 the priority for search spaces, with only the last two stages using MBv3. The  details are provided in supplementary. 


%use AttentiveNAS space block types and only allow elastic stages
%{\algname} searches both the operator type and configuration for the optimal elastic stages, as they are the key factors impacting INT8 latency. To evaluate the elastic stages for While existing space evolution works either search operator or configurations, our algorithm search both of them. To evaluate the benefit, we conduct comparison with two strong baselines based on AttentiveNAS search space: (1) search only operator: we use AttentiveNAS search space macro-architecture and conduct our space evolution to search for the optimal layer-wise block; and (2) search only width: we apply AttentiveNAS's block architecture and conduct search for the optimal layer-wise width. As shown in Table~\ref{tbl:search_compare}, xxx

 
% \begin{wraptable}{r}{0.4\textwidth}
%	\centering
%	\fontsize{8.5}{8.5} \selectfont
	
%	\caption{Best quantized models by different search methods on Pixel4.
%	 $^*$: we use the setting in AttentiveNAS space.}
%	\begin{tabular}
%		{@{\hspace{0.3\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.3\tabcolsep}}}	
	%	{ccccc}
%		\hline
		
%			Method&Op &Width & 10ms& 20ms\\
		
%		\hline 
%		AttentiveNAS&-&-&-&77.2\\
%		{\algname}-op & search & fix$^*$&75.0&77.8 \\
%	 {\algname}-config &fix$^*$ &search& & \\
%		{\algname} &search&search&75.7 & 78.3\\
%		\hline
%	\end{tabular}
%	\label{tbl:search_compare}
%	\vspace{-15pt}
%\end{wraptable} 


%\noindent\textbf{Space Under Different Latency Range.} 8-25 vs 10-30 vs 15-35

%\noindent\textbf{Analysis of searched models}. why our models achieve higher quantization speedup.


%\begin{table}[t]
%	\begin{center}

%		\fontsize{8.8}{8.8} \selectfont
%		\caption{Comparison with state-of-the-art space automation methods. We measure the time cost on 8 Tesla V100 GPUs. $N$ denotes the number of deployed model constraints.}
%		\begin{tabular}
%			{@{\hspace{0.1\tabcolsep}}c@{\hspace{0.1\tabcolsep}}|@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}}
	%		\hline
	%		Method	 & Operators&Configurations  &\makecell{Hardware-\\aware}&\makecell{Initialization \\ cost}&  \makecell{ Per-iteration\\ cost} & \makecell{Search \\iterations} &\makecell{Total Cost \\ GPU hours}\\
%			\hline
%			NSE~\citep{nse} & searchable& fixed&\xmark  &80 hours& 80 hours&6&560$N$\\
%			S3~\citep{autoformerv2} &fixed &searchable  &\xmark &240 hours& 240 hours &2& 720$N$\\
%			
%			{\algname} (Ours) & searchable &searchable & \cmark& \makecell{1day in parallel\\144 hours}&1 sec.&\textbf{5000}& \textbf{24+1$N$}\\
			
			
%			
%			\hline
%		\end{tabular}
%		\label{tbl:searchcost}
%	\end{center}
%\end{table}



%\begin{table}[t]
%	\begin{center}
		%	\small
%		\fontsize{8}{8} \selectfont
%	\caption{Comparison  with SOTA space search methods. We measure the time cost on 8 Tesla V100 GPUs. $N$ denotes the number of deployment scenarios.}
%	\label{tbl:searchcost1}
%	\vspace{-1ex}
%		\begin{tabular}
		%	{@{\hspace{0.1\tabcolsep}}c@{\hspace{0.1\tabcolsep}}|@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}@{\hspace{0.25\tabcolsep}}c@{\hspace{0.25\tabcolsep}}}
%			{@{\hspace{0.8\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{0.8\tabcolsep}}}
%			\hline
%			Method	 &Model& Operators&Configurations  &\makecell{Hardware-\\aware}& \makecell{Search \\iterations} &\makecell{Total Cost \\ GPU hours}\\
%			\hline
%			NSE~\cite{nse} &FP32 CNN& searchable& fixed&  &6&560$N$\\
%			S3~\cite{autoformerv2} &FP32 Transformer&fixed &searchable  & &3& 960$N$\\
%			{\algname} (Ours) &Quantized CNN &searchable &searchable & &\textbf{5000}& \textbf{24+1${N}$}\\		
			
%			\hline
%		\end{tabular}
		
%	\end{center}
%\vspace{-1ex}
%\end{table}



 

 
%\begin{table}[t]
%	\centering
		%	\small
%		\fontsize{8.2}{8.2} \selectfont
%		\begin{tabular}
%			{@{\hspace{0.1\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{\hspace{0.1\tabcolsep}}}
%			\hline
%			Method	& \makecell{Hardware\\-aware}& Op&Width & %\makecell{Search \\iterations} &\makecell{Total Cost \\ GPU hours}\\
%			\hline
%			NSE~\cite{nse} &&searchable& fixed  &6&560$N$\\		
%				S3~\cite{autoformerv2}&&fixed &searchable  &3& 960$N$\\
%		Ours &&searchable &searchable  &\textbf{5000}& \textbf{24+1${N}$}\\		
%			
%			\hline
%		\end{tabular}
	%	\vspace{-2.5ex}
%		\caption{Comparison  with SOTA space search methods. We measure the time cost on 8 Tesla V100 GPUs. $N$ denotes the number of deployment scenarios.}
%		\label{tbl:searchcost1}
%\end{table}
 




