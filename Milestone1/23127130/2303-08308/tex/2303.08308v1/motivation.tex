
\section{On-device Quantization Efficiency Analysis}
\label{sec:analysis}
\vspace{-1ex}

%\lz{to explain}

%Most existing  works optimize the quantized models through reducing FLOPs. While such metric does not reflect the real on-device latency for quantized models. To understand what factors and design choices slow down the INT8 latency, we conduct a comprehensive study on two widely-used edge devices: an Intel CPU device supported with VNNI instructions and onnxruntime (abbr Intel VNNI) and a Pixel 4 phone CPU with TFLite 2.7 (abbr Pixel 4). 
 %We reveal key observations as follows:
 
 
To understand what factors  lead to quantization-unfriendly issue, we conduct a comprehensive study on two widely-used edge devices equipped with high-performance inference engine: an Intel CPU device supported with VNNI instructions and onnxruntime~\cite{onnxruntime} (abbr Intel CPU) and a Pixel 4 phone CPU with TFLite 2.7~\cite{tflite} (abbr Pixel 4). Note that we follow existing practices~\cite{nnmeter,spacestudy,vitstudy} to measure the latency. 
 We reveal key observations as follows:
 
\begin{figure}[t]
	\centering
	\includegraphics[width=1\columnwidth]{model_latency2.png}
	\vspace{-4ex}
	\caption{INT8 latency and speedups (annotated) for SOTA models. FLOPs and FP32 latency are not good indicators of INT8 latency; Compact models have very limited INT8 speedup ($\sim$1.5$\times$). }
	\label{fig:quant_model_benchmark}
\end{figure}

\vspace{2px}
\noindent\textbf{Observation 1}: \textit{FP32 latency and FLOPs are not good indicators of INT8 latency. }

 To deploy  on edge devices, a common belief is that a compact model with low FLOPs or FP32 latency is preferred than a larger model. However,  Fig.~\ref{fig:quant_model_benchmark} shows that neither of them is a good indicator of INT8 latency. In Fig.~\ref{fig:quant_model_benchmark}(b), a very large model (ResNet18) can be even faster than a compact model (EfficientNetB0) after quantization. Moreover, the recent SOTA compact models searched by  OFA~\cite{ofa} and AttentiveNAS~\cite{attentivenas} all have marginal INT8 speedups, suggesting that optimizing FLOPs and FP32 latency can not lead to lower INT8 latency.  % This is because of their quantization-unfriendly search spaces, which will be discussed next. 


\noindent\textbf{Observation 2}:  \textit{The choices of operators' types and configurations greatly impact the INT8 latency. } % \lz{add some sentences to explain the reason why existing sota models achieve minimal speedups.}

\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{conv1x1_channel2.png}
\vspace{-3ex}
\caption{ (a) The choice of operator type leads to significantly different quantized speedup. (b) Conv1x1 speedups with various channel numbers. Config: $HW$=28, $C_{out}$=4x$C_{in}$(expand=4). }

\label{fig:convchannel}
\end{figure}
The prior art search spaces adopted in recent two-stage NAS works 
are MobileNetV2 or MobileNetV3 based chain-structures, where each search space comprises a sequence of blocks (stages). The block type is \textit{fixed} to the  MBConv %~\footnote{ Abbr MbV2. We refer it as MbV3 if it has SE, and the activation is changed to Swish or HardSwish.}
 and is allowed to search from a \textit{handcraft range of hyper-parameter configurations} including kernel size, expansion ratio, channel width and depth, which  are designed with human wisdom. For instance, many works~\cite{proxylessnas,attentivenas} observe that edge-regime CNNs prefer deeper depths and narrower channels, and manually set  small channel numbers but large depths in the search space.  

%While models  edge regime require a small FLOPs ($\leq$600M FLOPs), the prior art search spaces in recent NAS works are MobileNetV2 or MobileNetV3 based architectures. Specifically, the search spaces are factorized into a sequence of search blocks, where each search block is \textit{fixed} to the inverted bottle residual block (MBConv~\footnote{ Abbr MbV2. We refer it as MbV3 if it has SE, and the activation is changed to Swish or HardSwish.}), and allowed to search from a \textit{handcraft range of configurations} including kernel size, expansion ratio, channel numbers and depths. In particular, these handcraft configurations are designed with human wisdom. For instance, many works~\citep{proxylessnas,attentivenas} observe that mobile CNNs prefer more depths and narrower channels, and manually set a range of small channel numbers but large depths for each search block.  



However, we find that many block type and configuration choices in current search spaces unexpectedly slow down the INT8 latency. We first study the operator type impact in Fig.~\ref{fig:convchannel}(a). SE and Hardswish are lightweight operators in edge-regime search spaces, but their INT8 inference becomes slower on Intel CPU. Compared to Conv, DWConv can greatly reduce the FLOPs, but it benefits less from INT8 quantization.  The root cause is that quantization introduces extra cost, such as (1) data transformation between INT32 and INT8~\cite{spacestudy}, and (2) additional computation caused by scaling fators and zero points~\cite{jacob2018quantization}. If the operator has low data-reuse-rate, such as the activation functions (Hswish), the extra cost may outweigh the latency reduction by the low-bit computation. For high data-reuse operators (Conv), this cost is amortized and thus achieve large speedup~\cite{spacestudy}. 

% Quantization introduces extra cost, to transform the input tensor to INT8 by the classical formula $r=s(q-z)$~\cite{jacob2018quantization}. If the operator has low data-reuse-rate, such as the activation functions (Hswish), the extra cost may outweigh the latency reduction by the low-bit computation~\cite{spacestudy}. For high data-reuse operators (Conv), this cost is amortized. 

Besides the operator type, the configuration choices also determine the quantization efficiency. Fig.~\ref{fig:convchannel}(b) shows the  speedups of Conv1$\times$1 under various channel widths. Results show that small channel widths in OFA search space cannot benefit well from quantization.  This is because the additional quantization cost  has a large impact when the channel width is small, limiting the latency acceleration.  In contrast,  {\algname} can automatically design a search space with larger channel widths for better efficiency.%to better utilize hardware capability. 


%However, exact latency improvements and
%energy savings are highly dependent on the target hardware

%However, we find that many operator and configuration choices in current search space unexpectedly slow down the INT8 latency. We first apply MobileNetV3 configurations and study major operators' speedups by INT8 quantization in Table~\ref{tbl:op_speedup}. SE and Hardswish are effective and lightweight operators in edge regime search spaces, but their INT8 inference become slower on Intel VNNI. Compared to Conv, DWConv can greatly reduce the FLOPs and latency, but it benefits less from INT8 quantization. Besides the operator type, improper choice of configuration can slow down a quantization-friendly operator. Fig.~\ref{fig:convchannel} shows the quantization speedups of Conv1$\times$1 under various channel numbers. Results suggest that small channel number settings in prior art search spaces hardly utilize quantization with small speedups. In contrast,  {\algname} can automatically find a larger channel number with better hardware utilization.







\noindent\textbf{Observation 3}: \textit{Quantization-friendly settings are diverse and contradictory across devices.} %For quantized-for-all supernet NAS, it requires hardware specialized search space.} 

In Fig.~\ref{fig:convchannel}, we also observe that the quantization-friendly operators are different and can be contradictory on diverse devices. For instance,  Swish achieves a 2$\times$ speedup on Pixel4, but it is a quantization-unfriendly operator on CPU with a 0.8$\times$ slowdown.  
The reason is that quantization speedups are highly dependent on the inference engines and hardware~\cite{spacestudy,9638444}.  Intel VNNI supports the VPDPBWSD hardware instruction~\cite{vnni}, which fuses three instructions into one to speedup INT8 computation. Without VNNI, INT8 hardly gains speedup on Intel CPUs. %ARM CPUs have direct hardware units for INT8, while mobile GPUs does not have. 
Moreover, the implementations in inference engines have to fully utilize hardware instructions for latency reduction.
For example, onnxruntime does not implement a quantization kernel for Hardswish. Even on a VNNI CPU, the use of Hardswish in a quantized model slows down the  latency. 


\vspace{2pt}
\noindent\textbf{The need for hardware specialized search space}.
The above observations suggest that there is no single structure (block types in a model) that is optimal for quantization on all hardware. This poses a challenge for the two-stage NAS paradigm, as the supernet training requires all models in the search space to share an isomorphic structure.  To address this issue, 
our work proposes to design a specialized quantization-friendly search space for each hardware. Each search space is tailored to the unique characteristics of the hardware and includes an optimal structure with elastic depths, widths, and kernel sizes. 
%The different quantization efficiency on diverse devices phenomenon has also been observed in other works~\cite{spacestudy,9638444}

%\noindent\textbf{System analysis.} To understand the results above, we expose that the quantization speedup is comphrehensively determined by operator types, hardware devices, and kernel implementations. Quantization introduces extra cost, to transform the input tensor to INT8 by the classical formula $r=s(q-z)$~\cite{jacob2018quantization}. If the operator has low data-reuse-rate, such as the activation functions, the extra cost may outweigh the latency reduction by the low-bit computation~\cite{spacestudy}. The quantization speedup also relies on the hardware support. For example, Intel VNNI supports the VPDPBWSD hardware instruction~\cite{vnni}, which fuses three instructions into one to speedup INT8 computation. Without VNNI, INT8 hardly gains speedup on Intel CPUs. ARM CPUs have direct hardware units for INT8, while mobile GPUs does not have. Given the hardware instructions, the kernel implementations in inference engines have to fully utilize these instructions. Otherwise, there could still be no speedup. For example, onxnxruntime does not implement a quantization kernel for Hardswish operator. Even on a VNNI CPU, the use of Hardswish in a quantized model slows down the latency. In summary, there is no one-for-all quantization-friendly space for all devices. 

%In once-for-all supernet NAS (i.e., OFA and BigNAS), each stage is  fixed to a specific operator (building block) to utilize weight sharing and sandwich rule~\cite{bignas} techniques. The advantage is that the searched subnet can directly inherit supernet weights for deployment with comparable accuracy to those retrained from scratch, which eliminates the huge cost of training or fine-tuning of each subnets individually. 

%However, the quantization-friendly operators are different and can be contradictory on diverse devices as shown  in Fig.~\ref{fig:convchannel}(a). For instance,  Swish achieves a 2$\times$ speedup on Pixel4, but  it's a quantization-unfriendly operator on VNNI with a 0.8$\times$ slowdown. Therefore, we argue that each device requires a specialized search space to construct the quantized-for-all supernet, in which each layer chooses an optimal quantization-friendly operator. 



%\noindent\textbf{Opportunities and challenges}. Our observations suggest that there is a huge room for the design space improvement of quantized models on real-world devices. We aim to design  specialized search space for each device, which is comprised of optimal quantization-friendly operator and configurations. 
%However, we face the following challenges. 

%Search space plays a vital role in NAS, as it determines the accuracy and efficiency bounds. Most NAS~\cite{ofa,bignas,attentivenas,alphanet} still manually design the search space, which requires human-specified settings by levering information about existing CNNs, which are the result of years of expertise and many trial-and-error experiments. However, manually designing specialized search space for each device  requires additional hardware expertise and significantly increases the overhead, which is impractical. 

%Recently, NSE~\cite{nse} and S3~\cite{autoformerv2} can shrink  better search spaces without any human efforts. However, they  focus on optimizing the accuracy and cannot be applied to search a quantization-friendly  space for two reasons. Firstly, none of them search the space for both operator types and configurations, which are the crucial factors impacting INT8 latency; Secondly, they train each candidate  space from scratch to evaluate quality. Due to the expensive training cost, the current search iterations is limited by 10.   However, searching two space dimensions bring a huge search space, while 10 iterations  may be insufficient.
