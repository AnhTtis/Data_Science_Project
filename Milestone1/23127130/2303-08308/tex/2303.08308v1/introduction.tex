\vspace{-3ex}
\section{Introduction}
\vspace{-1ex}
INT8 Quantization\cite{datafreequant,8bitquant,lsq,lsqplus} is a widely used technique for deploying DNNs on edge devices by reducing 4$\times$ in model size and memory cost for full-precision (FP32) models. However, prior art DNN models achieve only marginal speedup from INT8 quantization (in Figure~\ref{fig:quant_model_benchmark}(a)), the still high latency after quantization making them difficult to deploy on latency-critical scenarios. Designing models that achieve high accuracy and low latency after quantization becomes the important but challenging problem.

Neural Architecture Search (NAS) is a powerful tool for automating efficient quantized model design~\cite{haq,dnas,spos,apq,mps}. 
Recently, OQAT~\cite{oqa} and BatchQuant~\cite{batchquant}  achieve remarkable search efficiency and accuracy by adopting a two-stage paradigm. The first stage trains a weight-shared quantized supernet assembling all candidate architectures in the search space. This allows all the sub-networks (subnets) to simultaneously reach comparable quantized accuracy as  when trained from scratch individually. The second stage uses typical search algorithms to find  subnets with best quantized accuracy under different FLOPs constraints.
This approach avoids the need to retrain each subnet for accuracy evaluation, greatly improving the search efficiency.


Though promising in optimizing FLOPs, we find that directly applying two-stage NAS to low quantized latency scenario leads to poor performance due to the  
\textit{quantization-unfriendly search space} issue: prior art search spaces cannot be well applied to quantization on diverse devices, as the current design  can unexpectedly \textit{hurt the INT8 latency}. %For instance, Squeeze-and-Excitation (SE) (Hu et al., 2018) and Hardswish are widely-used operators in CNN search space as it improves accuracy with little latency introduced, but their INT8 inference is slower than full-precision inference on CPU (Fig.~\ref{fig:convchannel}(a)).  As a result, we are forced to search small-size models to meet the latency requirements, as INT8 quantization offers a marginal speedup. This greatly limits NAS to find better quantized models for edge devices.
Since INT8 quantization only offers a marginal speedup, we are forced to search for small-sized models to meet the latency requirements, which can unfortunately limit NAS to find better quantized models for edge devices. Then, a question naturally arise: \textit{Can we design a quantization-friendly search space, allowing NAS to  discover larger and superior models that meet the low INT8 latency requirements?}





 %Conventional deployment process adopts a two-stage design-quantize scheme. It first designs an efficient model with low FLOPs or latency and then quantizes it to 8bit for minimal accuracy loss~\cite{8bitquant,lsq}. Unfortunately, neither FLOPs nor FP32 latency can well reflect INT8 quantized latency on real-world devices (Fig.~\ref{fig:quant_model_benchmark}), hence this traditional paradigm may fail to get a good quantized model.  %We observe that existing compact models with low FLOPs have marginal latency speedup by INT8 quantization, and the quantized latency can be unexpectedly longer than a much larger model as shown in Fig.~\ref{fig:quant_model_benchmark}(b).  %  For example, EfficientNetB0 (390M FLOPs) runs 2$\times$ faster  than ResNet18 (2G FLOPs)  on an Intel VNNI CPU, but it becomes slower after INT8 quantization. 
% To bridge the  latency gap, a natural approach is to leverage  Neural Architecture Search (NAS)~\cite{ofa,bignas,oqa,apq}. However,  we find that directly applying NAS achieves poor performance due to the \textit{quantization-unfriendly search space} issue: prior art search spaces cannot be well applied to quantization on diverse devices, as the current design  can unexpectedly \textit{hurt the INT8 latency}. As a result, existing search spaces can undesirably limit NAS to find better quantized models.
  
%Conventional INT8 quantization methods~\citep{datafreequant, lsq} focus on minimizing the quantized accuracy loss for one given model architecture. Consequently, only compact models with  low-FLOPs  are considered for quantization and deployment on resource-limited edge devices. Unfortunately, the INT8 latency of the resulted quantized model is sub-optimal. As shown in Fig.~\ref{fig:quant_model_benchmark}, the quantized latency of  compact models with lower FLOPs and FP32 latency can be longer than large models. For example, EfficientNetB0 (390M FLOPs) runs 2$\times$ faster  than ResNet18 (2G FLOPs)  on an Intel VNNI CPU, but it becomes slower after INT8 quantization. Applying  Neural Architecture Search (NAS)~\citep{oqa,apq} seems to be an alternative, that searches  quantized models for target INT8 latency  from a SOTA search space. However, we argue that \textit{prior art search spaces cannot be well applied to quantization on the diverse edge devices},  as the current design  can unexpectedly \textit{hurt the INT8 latency}. For instance, Squeeze-and-Excitation (SE) ~\citep{se} and Hardswish are widely-used operators  in  CNN search space as it improves accuracy with little latency introduced, but their INT8 inference is slower than full-precision inference on VNNI CPU (Table~\ref{tbl:op_speedup}). As a result, these SOTA search spaces undesirably limit NAS to find better quantized models.% as they cannot utilize on-device optimizations.
% \begin{figure}[t]
%	\centering
%	\includegraphics[width=0.8\columnwidth]{figs/arch3.png}	
%	\vspace{-2ex}
%	\caption{We formulate search space search into neural architecture search process with 3 major components: hyperspace (search spaces pool), search algorithm and  search space quality estimator. }
%	\label{fig:arch}
%\end{figure}



We start by conducting an in-depth study  to understand 
the factors that determine INT8 quantized latency and how they affect search space design.  Our study shows: \textit{(1) both operator type and configurations (e.g., channel width) greatly impact the INT8 latency}; Improper selections can slow down the INT8 latency. For instance, Squeeze-and-Excitation (SE) \cite{se} and Hardswish~\cite{mobilenetv3} are widely-used operators  in  current  search spaces as it improves accuracy with little latency introduced. However, their INT8 inference speeds are \emph{slower} than FP32 inference on Intel CPU (Fig.~\ref{fig:convchannel}(a)), because the extra costs (\textit{e.g.}, data transformation between INT32 and INT8) introduced by quantization outweigh the latency reduction by INT8 computation.   \textit{(2) The quantization efficiency varies across different devices, and the preferred operator types can be contradictory.} 

The above study motivates us to design specialized quantization-friendly search spaces for each hardware, rather than relying on a single, large search space as seen in SPOS~\cite{spos} for all hardware, which provides different operator options per layer. This is because two-stage NAS requires the search space to adhere to a specific condition for training the supernet, wherein each layer must utilize a fixed operator. Our study indicates significant variations in optimal operators across different hardware. Thus, customizing the search space for each hardware is crucial for optimal results.
 However, designing such specialized quantization-friendly search spaces for various edge devices presents a significant challenge,  requiring expertise in both AI and hardware domains, as well as many trial and error attempts to optimize accuracy and INT8 latency for each hardware.
 
  
 In this paper, we propose {\algname}, a novel method for automatically designing specialized quantization-friendly search space for each hardware. The search space is comprised of hardware-preferred operators and configurations, enabling the search of larger and better models with low INT8 latency.  With the discovered search space, we  leverage two-stage quantization NAS to train a quantized-for-all supernet, and utilize evolution search~\cite{ofa} to find best quantized models under various INT8 latency constraints. Our approach  addresses three key challenges: (1) What is the definition of a quantization-friendly search space in terms of both quantized accuracy and latency? (2) How to automatically design a search space without human expertise? (3)  How to handle with the prohibitive cost caused by quality evaluation of a candidate search space?


%However, we face the challenge of designing such quantization-friendly search space for the large variety of edge devices, as it requires domain knowledge of both CNN design experience and hardware expertise to optimize the accuracy and INT8 latency. \lz{in the past, years of efforts are devoted to design a high-quality search space.}
%Recently, ~\cite{nse,autoformerv2} can automatically shrink a large space to  better search spaces without any human efforts. Inspired by this, the questions naturally arise: \textit{1) Can we design a search space pool and automatically search a good one that is consisting of quantization-friendly operators and configurations for the target device? 2) How to handle with the prohibitive cost caused by quality evaluation of a candidate search space?}


%However, these works focus on optimizing the accuracy and cannot be applied to search a quantization-friendly  space for two reasons.
%Firstly, none of them search the space for both operator type and configurations (e.g., the channel width for each layer), which are the crucial factors impacting INT8 latency; Secondly, 
% each iteration requires training a candidate space from scratch to evaluate its quality,  the current search is constrained within 10 iterations due to the huge training cost.
 
   
 To address the first challenge, we  propose a latency-aware \textit{Q-T} score to quantify the effectiveness of a candidate search space, which measures the INT8 accuracy-latency quality of \textbf{top-tier} subnets in a search space. The behind intuition is that the goal of NAS is to search top subnets with better accuracy-latency tradeoffs.

Then, we introduce an evolutionary-based search algorithm that can effectively  search a quantization-friendly search space with highest \textit{Q-T} score. Searching a search space involves discovering \textit{a collection of model population} that contains billions of models, which is challenging and easily introduce complexity. To address this challenge, we propose to factorize and encode a search space into a sequence of elastic stages, which have flexible operator types and configurations. Through this design, the task of search space design is then simplied to find a search space with the optimal elastic stages, so that existing  search algorithms can be easily applied. Specifically, we design a stage-wise hyperspace to include many candidate search spaces and leverage aging evolution~\cite{real2019regularized} to perform random mutations of elastic stages for search space evolution. 
The  evolution is guided by maximizing the \textit{Q-T}  score. 
%This design is different from existing  space shrinking works~\cite{angle-based,asap,pcnas,padnas,nse,autoformerv2}, in which only one space dimension (either operators or configurations) can be searched.  



Finally, estimating the quality score (Q-T score) of a search space involves a costly training process for evaluating the accuracy of sub-networks, which presents a significant obstacle for our evolutionary algorithm. Naively adopting a two-stage NAS approach, training a supernet for each candidate search space~\cite{nse,autoformerv2}, is prohibitively expensive, taking  of thousands GPU hours. To address this issue, we draw inspiration from block-wise knowledge distillation~\cite{donna,dna} and propose a \textit{block-wise search space quantization scheme}. This scheme trains each elastic stage separately and rapidly estimates a model's quantized accuracy by summing block-level loss with a quantized accuracy lookup table, as shown in Fig.~\ref{fig:bkd}. This significantly reduces the training and evaluation costs, while providing effective accuracy rankings among search spaces. 
%We train a once-for-all supernet over the searched space containing a variety of well-quantized models with adaptive latency.An evolutionary search ~\cite{ofa} with an INT8 latency predictor is performed to get  Pareto-frontier INT8 models. The specialized models can be directly  deployed on target hardware without retraining or quantization cost.  Extensive experiments on ImageNet with two popular edge devices (Google Pixel 4  and Intel VNNI) demonstrate that our searched spaces consistently produce superior INT8 quantized models than the existing manually-designed search spaces with much higher accuracy, lower INT8 latency and better speedups (relative to FP32 latency). Code  will be released.   
We summarize our contributions as follows:
%In summary, we make the following contributions:
\begin{itemize}
	\vspace{-1ex}
	\item We study the  INT8 quantization efficiency on real-world edge devices and find that the choices of operator types and configurations in a quantized model can significantly impact the INT8 latency, leaving a huge room for design optimization of quantized models.
	
	\item We propose {\algname} to automatically design a hardware-dedicated quantization-friendly search space and leverage two-stage quantization NAS to produce superior INT8 models under various latency constraints. 
	
	\item We present three innovative techniques that enable the first-ever efficient and cost-effective evolution search  to explore a search space comprising billions of models.
	%To reduce the extremely expensive search cost, we further propose a novel block-wise quantization scheme to build accuracy lookup table.
	\item Extensive experiments on two edge devices and ImageNet demonstrate that our automatically designed search spaces significantly surpass previous manually-designed search spaces. Our discovered models establish the new state-of-the-art INT8 quantized accuracy-latency tradeoffs. For instance, {\sysname}@cpu-A4, achieves 80.0\% accuracy on ImageNet, which is
	3.3ms faster with 1.8\% higher accuracy than FBNetV3-A. %{\sysname}@pixel4-A1 runs 2.1$\times$ faster than EfficientNetB0 with 0.9\% higher accuracy. 
	Moreover, {\algname} produces superior tiny models, achieving up to 10.1\% accuracy improvement over the tiny ShuffleNetV2x0.5 (41M FLOPs, 4.3ms). 
	
	\end{itemize}

%We first formulate the space search  into neural architecture search (NAS) process as shown in Fig.~\ref{fig:arch}(a). Specifically, we factorize and encode a search space into a sequence of elastic stages, which have flexible operator types and configurations. Then, we design a large hyperspace to include many candidate search spaces and leverage  aging evolution~\cite{real2019regularized} to perform random elastic stage mutations for search space evolution. The evolution process is guided by \textit{Q-T}  score, which is proposed to evaluate the INT8 accuracy-latency quality of top-tier sub-networks in a search space. However, Q-T score requires expensive accuracy evaluation for numerous sub-networks. Therefore,  instead of naively training a candidate search space from scratch, we further propose a \textit{block-wise quantization scheme} to build a quantized accuracy lookup table in Fig.~\ref{fig:arch}(b).This significantly reduces the training and evaluation cost, while providing effective accuracy rankings among search spaces.  Finally, we train a quantized-for-all supernet over the searched space containing a variety of well-quantized models with adaptive sizes and latency. An evolutionary search ~\cite{ofa} with an INT8 latency predictor is performed to get  Pareto-frontier INT8 models. The specialized models can be directly deployed on target hardware without additional training or quantization cost.


% Extensive experiments on ImageNet and two popular edge devices (Google Pixel 4  and Intel VNNI ) demonstrate that our searched spaces consistently produce superior INT8 quantized models than the existing manually-designed search spaces with much higher accuracy, lower latency and better speedups. Code and models will be released.     In summary, we make the following contributions:
%\begin{itemize}
%	\vspace{-1ex}
 %	\item We systematically study the INT8 quantization efficiency on real-world edge devices and find that the choices of operator types and configurations in a quantized model can significantly impact the INT8 inference latency, leaving a huge room for quantized model design optimization.

 %	\item Motivated by our study, we propose {\algname}, the first space search algorithm that evolves a quantization-friendly search space producing superior quantized models under desired INT8 latency constraints.
% To reduce the extremely expensive search cost, we further propose a novel block-wise quantization scheme to build accuracy lookup table.
% 	\item We demonstrate the effectiveness and efficiency of {\algname} on two edge devices and ImageNet dataset. Our discovered model, {\sysname}@vnni-A4 achieves 80.0\% accuracy on ImageNet, which is
 %	3.3ms faster with 1.8\% higher accuracy than FBNetV3-A. {\sysname}@pixel4-A1 runs 2.1$\times$ faster than EfficientNetB0 with 0.9\% higher accuracy. Moreover, {\algname} produces superior tiny models, achieving up to 10.1\% accuracy improvement over the tiny ShuffleNetV2x0.5 (41M FLOPs, 4.3ms). % Compared to ShuffleNetV2x0.5 (41M FLOPs, 4.3ms), we achieve a new SOTA with 10.1\% higher accuracy. 
 	%Remarkably, 
 %	we achieve 69.6\% quantized accuracy with only 4.3 ms on the Pixel 4 mobile phone, outperforming ShuffleNetV2x0.5 (41M FLOPs) by 10.1\% higher accuracy. 
% \end{itemize}
 

 

 %In this paper, we present a new design paradigm for quantized networks that eliminates the gap between  algorithms and hardware optimizations, called {\sysname}. 
 %Instead of following the current design-then-quantize pipeline,  we directly search the best-performing quantized networks for deployment hardware. The key insight is to design a \textit{hardware-aware quantization-friendly space}, that is optimized for the target hardware and contains many low-latency and high accuracy quantized networks. 
% We train a quantization-for-all~\cite{batchquant,oqa} supernet that contains different sizes of quantized sub-networks. To search an optimal quantized network, we build a latency model to predict the quantized latency and conduct latency-aware search~\cite{}. In this way, 
%  the searched quantized network can be efficiently deployed on the given hardware without any training or quantization cost.
  
 %  \begin{figure*}[t]
  % 	  	\centering
 % 	\includegraphics[width=0.9\textwidth]{figs/arch1.png}	
% \vspace{-2ex}
 %		\caption{(a) Our space search concept is analogous to NAS: it has three major components: hyperspace (search spaces pool), search algorithm and space quality estimator; (b) we adopt block-wise knowledge distillation to  reduce search space quality evaluation cost.  }
%  	\label{fig:arch}
%  \end{figure*}


 

 
 
 %However, we face the challenge of designing such \textit{quantization-friendly INT8} search space for the large variety of hardware.
  %Existing hardware-aware NAS methods~\cite{proxylessnas,ofa,bignas,fbnet,attentivenas,alphanet} search over a manual designed search space that is usually consisting of one block (e.g., MbV2 or ResNet) and fixed layer-wise configuration. But these search spaces are not optimized for quantized performance and are sub-optimal. In particular, we observe that \textit{both operator/block type and configuration impact quantized performance.} An optimal search space should consider quantized-friendly blocks and configurations. 
 %  Moreover,  manually designing a novel search space is expensive and unscalable, since it requires joint optimizations from both hardware and algorithm for the target hardware.  Inspired by recent search space evolution works~\cite{autoformerv2,padnas,nse}, we naturally arise the question: \textit{Given the target hardware, can we automatically search a   space that is optimized for quantized performance?} 
  
  

%To the end, we present an efficient search space design automation algorithm, called \textit{{\algname}}, to firstly search the optimal hardware-preferred INT8 search space with a feasible cost.  Fig.~\ref{fig:method} shows the overall algorithm. 
%The search concept is analogous to NAS, we leverage aging evolution~\cite{real2019regularized} to search from a novel large hyper space, where each candidate denotes a possible search space consisting of various operator type and configurations. {\algname} is based on three main techniques:   \textit{(i)} we propose a novel Pareto-font metric, called \textit{P-T Score} to measure the quality for a candidate space. Then the goal of aging evolution is to search a candidate space with a highest P-T Score.   \textit{(ii)}  To search optimal design space, we  operator and configuration range we conduct two mutation actions that randomly mutate the operator and configuration architecture for a stage, respectively. \textit{(iii)} To reduce the training cost caused by each candidate space, we introduce a blockwise quantization scheme that divides a search space into blocks to quickly get INT8 blocks. We pre-quantize all the blocks in the hyper space before the search. Therefore, the design space search process involves only evaluation cost and the search is rapid, that solves the limitations in other methods. \lz{refine, keyword: stage-wise}
 


  
%  To the end, we propose
%  an efficient search space search approach to automatically find hardware-optimized space for the target hardware. Analogous to search a network, our approach leverages  aging evolution (AE)~\cite{real2019regularized} to search for the optimal quantized-friendly space from a large space, offering better quantized networks than existing manual designed search space. Our approach is based on three main ideas:  \textit{(i)} We propose a new quantization-aware metric called \textit{P-T Score} to measure the quality of each candidate  space.  Specifically, given a space $S$, the "P" part  is the possibility of sampling a network under the target quantized latency constraint, while the "T" part considers the quantized loss of top-tier networks over the space.  \textit{(ii)}
 %  With the proposed P-T score, the goal for AE is to find the search space with highest quality score among all possible spaces. We make a change to the random mutations in AE. Specifically, we  factorize a search space into a set of  blocks.  At each iteration, we sample a best space (i.e., the highest P-T score) in the population and mutate a new  block for next iteration. However, it's extremely computation-costly to directly compute the P-T score, as training a space from scratch to get the quantized loss requires an astonishingly high cost.  \textit{(iii)} To reduce the training cost, we employ the blockwise knowledge distillation~\cite{dna,donna} to train each block, and utilize LSQ+~\cite{lsqplus} to get quantized weights for blocks. Since every space is consisting of blocks, we can easily combine the corresponding quantized blocks for a candidate space. Consequently, the training cost is significantly reduced and we can conduct rapid evolution search.  
  
  
   
  
  %we propose \textit{quantization-friendly} search space for a given hardware. 
  
 %Despite the success of  existing search spaces~\cite{} in hardware-aware NAS (e.g., the well-known MBV2 search space), they are manually-designed for FP32 latency on CPU and not optimized for quantized latency on various types of hardware. 

 
%However, designing the \textit{quantization-friendly} search space for various hardware is extremely challenging, since it requires both hardware and network design expertise to jointly optimize for the target hardware.

 
%it's challenging to find such \textit{quantization-friendly} search space for various hardware. Existing search spaces in ~\cite{} are manually designed with lots of expertise leveraged. For instance, the 

 
 
 
 







