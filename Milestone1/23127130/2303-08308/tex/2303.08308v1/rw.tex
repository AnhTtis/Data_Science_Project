\vspace{-1.5ex}
\section{Related works}
\vspace{-1ex}
\noindent\textbf{Quantization} has been widely used for efficiency in deployment. Extensive efforts can be classified into post-training quantization (PTQ)~\cite{datafreequant,4bitptq} and quantization-aware training (QAT)~\cite{8bitquant,relaxedquant,lsq,lsqplus}. QAT generally outperforms PTQ in quantizing compact DNNs to  8bit and very low-bit (2, 3, 4bit) by finetuning the quantized weights. Despite their success, traditional quantization methods focus on minimizing accuracy loss for a given pre-trained model, but ignore the real-world inference efficiency. 
% In our work, we utilize LSQ+~\citep{lsqplus}, an extension of LSQ~\citep{lsq}, that works well on INT8 quantization with trainable scale and offset parameters. 
%Our work considers both the  accuracy and on-device latency. 
%only INT8 quantization as most commercial edge devices support only for integer 8 or floating-point inference. 
 

% PTQ~\citep{datafreequant,4bitptq} directly quantizes a pre-trained FP32 model~\citep{8bitquant,relaxedquant,lsq}, while QAT generally outperforms PTQ with finetuning the quantized weights.  While SOTA QAT work well on typical 8bit and very low-bit (2, 3, 4bit) quantization, little attention is paid on studying the real-world inference latency of quantized networks. In practical, only 8bit has been widely supported on commercial edge devices~\citep{}, and many works~\citep{apq,haq} report simulated latency on specialized hardware like FPGA. 


%\textbf{Neural Architecture Search}. Introduce NAS algorithms, and then introduce existing quantization-aware NAS to search layer bit-width. 


%Recent studies combine quantization and NAS to automatically search for layer bit-width with given architecture or search for operations with given bit-width. HAQ~\citep{haq} searches for mixed-bits for a given network architecture. SPOS~\citep{spos} trains a quantized one-shot supernet to search for bit-width. APQ~\citep{apq} builds upon a full precision one-shot supernet and  build a proxy quantized accuracy predictor. Most existing methods fall into two categories. The first optimizes for a singled weighted
%MPS~\citep{mps}

%DNAS~\citep{dnas}

%Recently, formulated mixed-precision network quantization as an instance of NAS. 


%only 8bit has been widely supported on commercial edge devices~\citep{}.


%have achieved great process in 8bit and other extremely-low bits like 4,2,1 bits. However, 

% There have been extensive works on compressing DNNs by quantization, that can be classified into post-training methods  ~\citep{8bitquant,4-2bitquant} propose to 

% INT8 quantization has been widely used for efficiency in deployment. 

%The quantization community has shown low bits as 

%it was possible to quantize to 8-bit integer or even lower as 2-bit integers with minimal accuracy loss. 

%There exists two main methods of quantizing pre-trained NNs: QAT and post-training quantization (PTQ). 

%https://arxiv.org/pdf/2111.03759.pdf



\noindent\textbf{NAS for Quantization}. %Early NAS works focus on automating  network design for SOTA accuracy. Recent hardware-aware NAS methods~\cite{proxylessnas,ofa,fbnetv3} consider both  accuracy and  efficiency by introducing latency predictors.However, these works consider the latency of FP32 models, leading to a big gap and performance degradation for  quantized models.  
Early works~\cite{haq,dnas,spos,apq,mps} formulates mixed-precision problem into NAS to search layer bit-width with a given architecture. Recently,  ~\cite{oqa,batchquant}  train a quantized-for-all supernet to search both architecture and bit-width. The searched models can be directly evaluated with comparable accuracy to train-from-scratch. However, little attention is paid on optimizing quantized  latency on real-world devices. Through searching quantization-friendly search space, our discovered quantized models can achieve both high accuracy and low latency.

 


%On the other hand, ~\citep{haq,dnas,spos,apq,mps} formulates mixed-precision problem into NAS to search layer bit-width with given architecture or search for operations with given bit-width. Recently, ~\citep{oqa,batchquant} leverage NAS techniques in ~\citep{ofa,bignas,sandwichrule} and train a quantization-for-all supernet for quantized model search. However, current quantization-aware NAS focus on mixed-precision quantization, 
%that requires specialized hardware support (e.g., FPGA) or kernel implementation~\citep{hawqv3}.  In our work, we consider INT8 quantization as it's widely supported on commercial edge devices. Our approach optimizes for a quantization-friendly search space, and is orthogonal to these methods.  
 

\noindent\textbf{Search Space Design}. % A good search space is crucial to the NAS performance\citep{regnet,attentivenas}.
 Starting from ~\cite{proxylessnas}, the manually-designed MBConv-based space becomes the dominant in most NAS works~\cite{proxylessnas,ofa,bignas,attentivenas}.
 RegNet~\cite{regnet} is the first to present  standard guidelines to optimize a search space by each dimension.  Recently, ~\cite{angle-based,asap,pcnas,padnas,nse,autoformerv2} propose to  shrink to a better compact search space by either pruning unimportant operators or configurations.  
  However, these works focus on optimizing the accuracy and little attention is paid on quantization-friendly search space design. Our work is the first lightweight solution towards this direction.
  %cannot be applied to search a quantization-friendly  space for two reasons.
% Firstly, none of them search the space for both operator type and configurations, which are the crucial factors impacting INT8 latency; Secondly, 
%each iteration requires training a candidate space from scratch to evaluate its quality,
% the current search is constrained within 10 iterations due to the huge training cost.  Our work is the first lightweight solution for hardware-friendly search space automation.
 %However, they focus on optimizing model accuracy and the exploration of hardware efficient search space is limited. Moreover, none of them can efficiently search both operator and configuration on large datasets (e.g., ImageNet). Our work is the first  along this direction with a feasible search cost.
 
 %However, these works focus on optimizing the accuracy and cannot be applied to search a quantization-friendly  space for two reasons.
 %Firstly, none of them search the space for both operator type and configurations (e.g., the channel width for each layer), which are the crucial factors impacting INT8 latency; Secondly, 
 % each iteration requires training a candidate space from scratch to evaluate its quality,  the current search is constrained within 10 iterations due to the huge training cost.

% none of them can efficiently search both operator and configuration for a quantization-friendly space on large datasets (e.g., ImageNet).
%Recently, several works~\citep{angle-based,asap,pcnas,padnas,nse}  automatically shrink a better compact search space and achieve better accuracy. They prune unimportant operators by the importance ranking during  model architecture search.  While they search only for the operators, S3~\citep{autoformerv2} is the first work that searches the space configurations for vision transformer by search dimension decomposition and linear parameterization. However, none of them can efficiently search both operator and configuration for a quantization-friendly space on large datasets (e.g., ImageNet). In contrast, {\algname} is the first lightweight search space search work that automatically leverages hardware optimizations like INT8 quantization. Moreover, our method decouples space search and architecture search, so that the searched space can apply to many existing NAS algorithms. 



%S3~\cite{autoformerv2} is the first work that searches the configurations for vision transformer search space by 

%add hurriance, discuss which work search operator, which search configurations. what are their search cost

%sota search spaces 

%auto search space design
%\cite{regnet,autoformerv2,padnas,nse}

%~\cite{hurricane}


%\lz{~\cite{pcnas}: to solve the weight sharing issue: posterior fading: as the number of models in the supergraph increases, the kl-divergence between true parameter posterior and proxy posterior also increases. Solution: divide the training of supergraph into several intervals and maintain a pool of high potential partial models and progressively update this pool. }

%\lz{~\cite{asap}, xnas: pcnas: "introduce pruning during the training of over-parameterized networks.  Similar to these approaches, we start with an overparameterized network and then reduce the search space to
%	derive the optimized architecture. Instead of focusing on
%	the speed-up of training, we further improve the rankings of
%	models and evaluate operators directly on validation set"}

%\lz{~\cite{asap}: a differentiable search space that allows the annealing of architecture weights, while gradually pruning inferior operations. It allows gradual pruning of weak weights, which reduces the number of computed connections through the search. Search space: darts and enas, cifar10. use the operator weights to reduce operators.}

%\lz{xnas: dynamically wipe out inferior architectures and enhance superior ones. cifar10}

%\lz{angle-based: weight-sharing nas, during the search process.  existing nas usually use accuracy-based metric or magnitude-based metric to guide the shrinking process. This paper propose a novel angle-based metric to guide the shrinking process. Save heavy computation overhead, higher stability and ranking correlation. Prune operators. small dataset}

%We: decouple the neural architecture search and search space search. 


