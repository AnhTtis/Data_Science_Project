
\begin{abstract}
The combination of Neural Architecture Search (NAS) and quantization has proven successful in automatically designing low-FLOPs INT8 quantized neural networks (QNN). However, directly applying NAS to design accurate QNN models that achieve low latency on real-world devices leads to inferior performance. In this work, we find that the poor INT8 latency is due to the quantization-unfriendly issue: the operator and configuration (e.g., channel width) choices  in prior art search spaces lead to \textit{diverse} quantization efficiency and can slow down the INT8 inference speed. To address this challenge, we propose {\algname}, 
an automatic method for designing a dedicated, quantization-friendly search space for each target hardware. The key idea of {\algname} is to automatically search hardware-preferred operators and configurations to construct the search space, guided by a metric called \textit{Q-T score} to quantify how quantization-friendly a candidate search space is. We further train a quantized-for-all supernet over our discovered search space,  enabling the searched models to be directly deployed without extra retraining or quantization. Our discovered  models
establish new SOTA  INT8 quantized accuracy under various latency constraints, achieving up to 10.1\% accuracy improvement on ImageNet than prior art CNNs under the same  latency. Extensive experiments on diverse edge devices demonstrate that  {\algname} consistently outperforms existing manually-designed search spaces with up to 2.5$\times$ faster speed while achieving the same accuracy.
	%Our study addressed the challenges of hardware-friendly search space design  in NAS and paved the way  towards efficient deployment.
	
	
	
%	INT8 quantization is an essential compression approach to deploy a deep neural network (DNN) on resource-limited edge devices. While it greatly reduces model size and memory cost, current  edge-regime DNN models	cannot well utilize INT8 quantization to reduce real latency.  In this work, we find that the poor INT8 latency performance is due to the quantization-unfriendly issue: the operator and configuration (e.g., channel width) choices  in a normal model design space lead to \textit{diverse} quantization efficiency and can slow down the INT8 inference speed. To address this issue, we propose {\algname} to automatically design a novel hardware dedicated, quantization-friendly search space for once-for-all NAS, so that the searched models can be directly deployed while achieving both superior quantization efficiency and accuracy. The key idea is to automatically search hardware-preferred operators and configurations to construct the search space, guided by a metric called \textit{Q-T score} to quantify how quantization-friendly a candidate search space is. 	On diverse  devices, {\algname} consistently outperforms existing manually-designed search spaces by producing both tiny and large quantized models with superior ImageNet accuracy and hardware efficiency. The discovered  models, named {\sysname}, achieve up to 10.1\% higher accuracy under the same  latency.  Our study addressed the challenges of hardware-friendly search space design  in NAS and paved the way  towards efficient deployment.
	
	
	
	
%	To this end, we propose {\sysname}, a novel quantization paradigm,
	% that searches a hardware-aware, quantization-friendly search space to fully utilize on-device optimizations, and then train a quantized-for-all (QFA) supernet for fast INT8 model specialization and deployment. The key idea is to automatically incorporate hardware-preferred settings to a search space while maintains the same or higher level of accuracy as normal search spaces. Our search algorithm, {\algname}, is the first to quickly find such search space for a given hardware with three novel techniques. In particular, we define a well-abstracted hyperspace to include many search spaces;  a quality estimator to guide evolution search to find the best  search space; and a blockwise quantization framework to quickly evaluate a search space. %	 We demonstrate the effectiveness of our method on ImageNet and achieve SOTA Top-1 accuracy under xxxx.
	
%	We address the challenging problem of efficient INT8 inference with a full hardware utilization across diverse edge devices. To deliver an INT8 quantized neural network, conventional approaches either find an optimal model for INT8 quantization or use quantization-aware neural architecture search (NAS) to search an INT8 model. However, none of them consider the diverse on-device quantization behaviours  in current search spaces, resulting in sub-optimal INT8 models with minimal latency speedup. In this work, we observe that the poor INT8 latency performance is due to the quantization-unfriendly issue: the operator and configuration choices in a normal search space can slow down the INT8 latency. To this end, we propose {\algname} to deliver a wide-range of Pareto-frontier quantized networks that can better utilize on-device quantization optimizations, by searching a novel  hardware-aware, quantization-friendly search space. The key idea is to automatically incorporate hardware-preferred settings to a search space while maintains the same or higher level of accuracy as normal search spaces. Our search algorithm, {\algname}, is the first to quickly find such search space for a given hardware with three novel techniques. In particular, we define a well-abstracted hyperspace to include many search spaces;  a quality estimator to guide evolution search to find the best  search space; and a blockwise quantization framework to quickly evaluate a search space. 	We demonstrate the effectiveness of our method on ImageNet and achieve SOTA Top-1 accuracy under xxxx.
	
	
	
	
	%To this end, we propose {\sysname}, a novel quantization paradigm,
%	that searches a hardware-aware, quantization-friendly search space to fully utilize on-device optimizations, and then train a quantized-for-all (QFA) supernet for fast INT8 model specialization and deployment. The key idea is to automatically incorporate hardware-preferred settings to a search space while maintains the same or higher level of accuracy as normal search spaces. Our search algorithm, {\algname}, is the first to quickly find such search space for a given hardware with three novel techniques. In particular, we define a well-abstracted hyperspace to include many search spaces;  a quality estimator to guide evolution search to find the best  search space; and a blockwise quantization framework to quickly evaluate a search space. 	We demonstrate the effectiveness of our method on ImageNet and achieve SOTA Top-1 accuracy under xxxx.
	

\end{abstract}