\section{Formal Regret Analysis of Algorithm~\ref{alg:time-varying-cost}}
\label{sec:igw-alg-proof}
In this section, we shall formalize the results in Section~\ref{sec:igw-alg-sol} and present a rigorous proof leveraging ideas from~\cite{foster2020beyond}.
Additionally, we will derive the regret guarantees for several additional examples of function classes $\mathcal{F}$ on the suppliers' optimal production with respect to the context $\theta_t$ and the price $p_t$.
First, we quantify the descriptiveness of the function class $\mathcal{F}$ with the following property.
\begin{definition}
A function class $\mathcal{G} : \mathcal{A} \to \mathcal{B}$ is \textit{well-specified} with respect to the ground truth $g^*$ if $g^* \in \mathcal{G}$, and $\mathcal{G}$ is $\varepsilon$-miss-specified with respect to the ground truth $g^*$ if:
\[\exists \bar{g} \in \mathcal{G} \text{, s.t. } \forall a \in \mathcal{A} \text{, we have } \norm{\bar{g}(a) -  g^*(a)}_\mathcal{B} \le \varepsilon. \]
Note that being well-specified is equivalent to being $0$-miss-specified.
\end{definition}

Recall that in Section~\ref{sec:igw-alg-sketch}, we showed that the three desired regret metrics are all upper bounded by the proxy regret (up to some positive constants):
\[ \textsc{Reg}(T) = \sum_{t=1}^T \EE_{p_t \sim \Delta_t} \left[ |x^*(p_t; \theta_t) - d_t|\mid \mathcal{H}_{t-1} \right]. \]
For ease of notation, we use $\lesssim$ as a shorthand notation that the left-hand side is smaller than some fixed constant times the right-hand side.
And we can bound this proxy regret as follows.

\begin{theorem}\label{thm:igw-regret}
    If the function class $\mathcal{F}$ is $\varepsilon$-miss-specified with respect to the suppliers' optimal production $x^*(p_t; \theta_t)$, then with probability $1-\delta$, and for any sequence of contexts $\theta_t$ and demands $d_t$, Algorithm~\ref{alg:time-varying-cost} achieves the following proxy regret bound:
    \[ \textsc{Reg} \lesssim \sqrt{KT \cdot \est(T)} + \varepsilon \sqrt{K} \cdot T + \frac{T}{K} + \sqrt{KT \log(1/\delta)}.\]
\end{theorem}

Noting from Section~\ref{sec:igw-alg-sketch} that the three desired regret metrics are all upper bounded by the proxy regret (up to some positive constants), Theorem~\ref{thm:igw-regret} allows us to formally state the result given by Theorem~\ref{thm:igw-bound-informal} as follows.
\begin{theorem} \label{thm:main-contextual}
    If the function class $\mathcal{F}$ is $\varepsilon$-miss-specified with respect to the suppliers' optimal production $x^*(p_t; \theta_t)$, then with probability $1-\delta$, and for any sequence of contexts $\theta_t$ and demands $d_t$, Algorithm~\ref{alg:time-varying-cost} achieves the following bound on our regret metrics:
    \[ \EE_{p_t \sim \Delta_t, t \in [T]}[U_T(p_1, \dots, p_T)] \lesssim \sqrt{KT \cdot \est(T)} + \varepsilon \sqrt{K} \cdot T + \frac{T}{K} + \sqrt{KT \log(1/\delta)},\]
    and similarly for $\EE_{p_t \sim \Delta_t, t \in [T]}[P_T(p_1, \dots, p_T)]$ and $\EE_{p_t \sim \Delta_t, t \in [T]}[C_T(p_1, \dots, p_T)]$, where $K$ is the number of prices in the uniformly discretized price set.
\end{theorem}

We refer to Appendix~\ref{sec:full-pf-contextual-thm} for a complete proof of Theorem~\ref{thm:igw-regret}. Furthermore, in Appendix~\ref{sec:implications-thm-contextual}, we present explicit regret bounds corresponding to Theorem~\ref{thm:igw-regret} for various function classes $\mathcal{F}$.

\subsection{Regret Bounds for Different Function Classes $\mathcal{F}$} \label{sec:implications-thm-contextual}

%Before presenting the proof of Theorem~\ref{thm:igw-regret}, 

In this section, we shall combine Theorem~\ref{thm:main-contextual} with results in the statistical learning literature to establish concrete regret bounds for several examples of function classes $\mathcal{F}$.
To this end, first recall from Section~\ref{sec:igw-alg-sol} that we already obtained an explicit bound on the regret for finite function classes $\mathcal{F}$. Thus, we focus the following discussion on infinite function classes.
To do so, we first quantify the ``size'' of an infinite function class with the notion of \textit{sequential covering}.

\begin{definition}
    Given a real-valued function space $\mathcal{G} : \mathcal{A} \to \RR$ and a sample set $S = \{a_1, \dots, a_n\}$, we say that a finite set of functions $\mathcal{G}'$ is an \textit{$\varepsilon$-sequential cover} of $\mathcal{G}$ with respect to $S$ if 
    \[ \forall \, g \in \mathcal{G}, \exists \, g' \in \mathcal{G}' \text{ s.t. } \left(\frac{1}{n} \sum_{i=1}^n (g(a_i) - g'(a_i))^2\right)^{1/2} < \varepsilon. \]
    Then, the \textit{$\varepsilon$-sequential covering number} of $\mathcal{G}$ is the size of the smallest $\varepsilon$-sequential cover with respect to the sample set $S$, 
    \[ \mathcal{N}_2(\mathcal{G}, \varepsilon, S) = \min \{|\mathcal{G}'| : \mathcal{G}' \text{ is a $\varepsilon$-sequential cover of $\mathcal{G}$ with respect to $S$}\}. \]
    Finally, denote $\mathcal{N}_2(\mathcal{G}, \varepsilon) = \sup_{S \text{ finite}} \mathcal{N}_2(\mathcal{G}, \varepsilon, S)$.
\end{definition}

As shown in~\cite{rakhlin2014online}, the prediction accuracy can be expressed in terms of the sequential covering number of the function class $\mathcal{F}$.
\begin{theorem}[see \cite{rakhlin2014online}]
    There exist online regression oracles achieving the following bounds:
    \begin{itemize}
        \item If $\mathcal{F}$ is finite, then $\est(T) \le \log |\mathcal{F}|$.
        \item If $\mathcal{F}$ is parametric in the sense that $\mathcal{N}_2(\mathcal{F}, \varepsilon) \in O(\varepsilon^{-m})$, then $\est(T) \lesssim m \cdot \log(T)$.
        \item If $\mathcal{F}$ is non-parametric in the sense that $\log \mathcal{N}_2(\mathcal{F}, \varepsilon) \in O(\varepsilon^{-m})$, then $\est(T) \lesssim T^{1 - 2/(2+m)}$ if $m \in (0, 2)$ and $\est(T) \lesssim T^{1 - 1/m}$ if $m \ge 2$.
    \end{itemize}
\end{theorem}
In many cases, we can easily construct efficient algorithms that match or nearly match these bounds.
For example, for finite $\mathcal{F}$, we can achieve the bound $\est(T) \le \log |\mathcal{F}|$ with the classical exponential weights update algorithm~\cite{vovk1995game}.
And when $\mathcal{F}$ is a linear class in the sense that
\[\mathcal{F} =\{(p, \theta) \mapsto \langle \phi, \sigma(p, \theta) \rangle : \phi \in B^m_2\},\]
where $\sigma$ is a fixed feature map, then the Vovk-Azoury-Warmuth forecaster achieves $\est(T) \lesssim m \cdot \log(T)$~\cite{vovk1997competitive,azoury2001relative}.
For general function class $\mathcal{F}$, we can nearly achieve the preceding bounds by performing the exponential weights update algorithm on a sequential cover of $\mathcal{F}$ (see e.g.,~\cite{vovk2006metric}).


With these results in mind, we can derive the exact regret bounds for various instances of function class $\mathcal{F}$.
In particular, if the suppliers' optimal production function $x^*$ is contained in one of the function classes listed below, we can compute the regret bounds as follows:
\begin{itemize}
    \item If $\mathcal{F}$ is finite, we have $\est(T) = \log |\mathcal{F}|$ and a choice of $K = (\frac{T}{\log |\mathcal{F}|})^{1/3}$ results in the regret bound $\textsc{Reg}(T) \lesssim T^{2/3} \left(\sqrt[3]{\log |\mathcal{F}|} + \sqrt{\log(1/\delta)}\right)$ with probability $1-\delta$.
    \item If the cost functions are quadratic functions with time-varying coefficients in the sense that
    \[c(x; \phi, \theta_t) = \frac{1}{2\langle \phi, \sigma(\theta_t)\rangle} x^2, \; \phi \in B^m_2,\]
    for some fixed feature map $\sigma$, then the suppliers' optimal production can be expressed as
    \[x^*(p; \phi, \theta_t) = \langle \phi, p \cdot \sigma(\theta_t) \rangle.\]
    So, in this case, $\mathcal{F}$ is a linear function class and we have $\est(T) \lesssim m \cdot \log(T)$.
    If $K = (\frac{T}{d\log T})^{1/3}$, then we get the regret bound $\textsc{Reg}(T) \lesssim T^{2/3} \left(\sqrt[3]{m \cdot \log T} + \sqrt{\log(1/\delta)}\right)$ with probability $1-\delta$.
    \item Consider an Euclidean context $\theta_t \in \RR^m$ encapsulates similarity information on the suppliers' behavior such that $\mathcal{F}$ is the set of bounded Lipschitz functions over $(p, \theta) \in \RR^{m+1}$.
    From this well-specified function class $\mathcal{F}$, we find a subset of $\mathcal{F}$ with sequential covering and apply the miss-specified version of Theorem~\ref{thm:igw-regret} on this subset.
    We can explicitly construct an $\varepsilon$-covering so that $\log \mathcal{N}_2(\mathcal{F}, \varepsilon) \lesssim \varepsilon^{-m-1} $ (see Examples 5.10 and 5.11 in~\cite{wainwright2019high}).
    If we run the exponential weights update algorithm over this covering, we have $\est(T) = \varepsilon^{-m-1}$.
    Since, by construction, the covering is $\varepsilon$-miss-specified with respect to the optimal suppliers' production, we have
    \[\textsc{Reg}(T) \lesssim \sqrt{KT \varepsilon^{-m-1}} + \varepsilon \sqrt{K} \cdot T + \frac{T}{K} + \sqrt{KT \log(1/\delta)}.\]
    If we pick $K = \varepsilon^{-2/3}$ and $\varepsilon = T^{-1/(m+2)}$, then we get that
    \[\textsc{Reg}(T) \lesssim T^{(3m+4)/(3m+6)} + T^{1/2 + 1/(3m+6)} \sqrt{\log(1/\delta)}\]
    with probability $1-\delta$.
    \item If $\mathcal{F}$ is a neural network whose weight matrices' spectral norms are at most 1, then it is known that $\log \mathcal{N}_2(\mathcal{F}, \varepsilon) \lesssim \varepsilon^{-2}$~\cite{bartlett2017spectrally}. 
    Then, we have $\est(T) \in O(T^{1/2})$, and a choice of $K = T^{1/6}$ gives the regret bound $\textsc{Reg} \lesssim T^{5/6} + T^{7/12} \sqrt{\log(1/\delta)}$ with probability $1-\delta$.
\end{itemize}

We can analogously work out the regret bounds if each of the examples is miss-specified.

%We now turn our attention to the proof of Theorem~\ref{thm:igw-regret}.

\subsection{Proof of Theorem~\ref{thm:igw-regret}} \label{sec:full-pf-contextual-thm}

%\begin{proof}[Proof of Theorem~\ref{thm:igw-regret}]
Recall that $x^*$ is the suppliers' optimal production, and since $\mathcal{F}$ is $\varepsilon$-miss-specified with respect to $x^*$, there exists a function $\bar{x} \in \mathcal{F}$ so that
\[\forall (p, \theta), \text{we have } |\bar{x}(p; \theta) -  x^*(p; \theta)| \le \varepsilon.\]
From Lemma~\ref{lem:ProductionLipschitz}, we know that $x^*$ is Lipschitz in price, so we know that for any context $\theta$ and price $p$, there exists $\bar{p}$ from the list of Algorithm~\ref{alg:time-varying-cost}'s choices $\{p_i\}_{i=1}^K$ such that $|x^*(\theta, p) - x^*(\theta, \bar{p})| < O(1/K)$.
Note that for the market-clearing price $p^*_t$, we have $|x^*(p^*_t; \theta_t) - d_t| = 0$.
Therefore, from the triangle inequality we have
\begin{align*}
    \textsc{Reg}(T) 
    &= \sum_{t=1}^T \EE_{p_t \sim \Delta_t} \left[ |x^*(p_t; \theta_t) - d_t|\mid \mathcal{H}_{t-1} \right] - |x^*(p^*_t; \theta_t) - d_t| \\
    &\le \sum_{t=1}^T \EE_{p_t \sim \Delta_t} \left[ |\bar{x}(p_t; \theta_t) - d_t| \mid \mathcal{H}_{t-1} \right] - |\bar{x}( \bar{p}_t; \theta_t) - d_t| + 2\varepsilon T + O(T/K)
\end{align*}
%
Now we attempt to upper bound the quantity
\[ \preg(T) := \sum_{t=1}^T \EE_{p_t \sim \Delta_t} \left[ |\bar{x}(p_t; \theta_t) - d_t| \mid \mathcal{H}_{t-1} \right] - |\bar{x}(\bar{p}_t; \theta_t) - d_t|. \]
%
Recall that $\hat{f}_t$ is the online regression oracle's output at time $t$.
For simplicity, denote
\begin{align*}
    \bar{g}_t(p) &= |\bar{x}(p; \theta_t) - d_t|, \\
    \hat{g}_t(p) &= |\hat{f}_t(p; \theta_t) - d_t|.
\end{align*}
%
Let $\hat{p}_t = \argmin \hat{g}_t(\cdot)$, then we have
\begin{align*}
    & \EE_{p_t \sim \Delta_t} [\bar{g}_t(p_t) \mid \mathcal{H}_{t-1} ] - \bar{g}_t(\bar{p}_t) \\
    ={}& \underbrace{\EE_{p_t \sim \Delta_t} [\hat{g}_t(p_t) - \hat{g}_t(\hat{p}_t) \mid \mathcal{H}_{t-1} ]}_{\textcircled{1}} + \underbrace{\EE_{p_t \sim \Delta_t} [\bar{g}_t(p_t) - \hat{g}_t(p_t) \mid \mathcal{H}_{t-1} ]}_{\textcircled{2}} + \underbrace{(\hat{g}_t(\hat{p}_t) - \bar{g}_t(\bar{p}_t))}_{\textcircled{3}}
\end{align*}
%
Plugging in the chosen distribution $\Delta_t$, we get that the first term is
\[\textcircled{1} = \sum_{i=1}^K \frac{\hat{g}_t(p_i) - \hat{g_t}(\hat{p_t})}{\lambda + 2\gamma (\hat{g}_t(p_i) - \hat{g_t}(\hat{p_t}))} \le \frac{K-1}{2\gamma}. \]
%
By convexity and then AM-GM, the second term can be bounded as 
\[\textcircled{2} \le \sqrt{\EE_{p_t \sim \Delta_t} [(\bar{g}_t(p_t) - \hat{g}_t(p_t))^2 \mid \mathcal{H}_{t-1}]} \le \frac{1}{2\gamma} + \frac{\gamma}{2} \EE_{p_t \sim \Delta_t} [(\bar{g}_t(p_t) - \hat{g}_t(p_t))^2 \mid \mathcal{H}_{t-1}].\]
%
And the third term can be rewritten as
\begin{align*}
    \textcircled{3}
    &= \hat{g}_t(\bar{p}_t) - \bar{g}(\bar{p}_t) - (\hat{g}_t(\bar{p}_t) - \hat{g}_t(\hat{p}_t)) \\
    &\le \frac{\gamma \Delta_t(\bar{p}_t)}{2}(\hat{g}_t(\bar{p}_t) - \bar{g}(\bar{p}_t))^2 + \frac{1}{2\gamma \Delta_t(\bar{p}_t)} - (\hat{g}_t(\bar{p}_t) - \hat{g}_t(\hat{p}_t)) \\
    &\le \frac{\gamma}{2} \EE_{p_t \sim \Delta_t}\left[ (\hat{g}_t(\bar{p}_t) - \bar{g}(\bar{p}_t))^2 \mid \mathcal{H}_{t-1} \right] + \frac{1}{2\gamma \Delta_t(\bar{p}_t)} - (\hat{g}_t(\bar{p}_t) - \hat{g}_t(\hat{p}_t)) \\
    &= \frac{\gamma}{2} \EE_{p_t \sim \Delta_t}\left[ (\hat{g}_t(\bar{p}_t) - \bar{g}(\bar{p}_t))^2 \mid \mathcal{H}_{t-1} \right] + \frac{\lambda + 2\gamma (\hat{g}_t(\bar{p}_t) - \hat{g}_t(\hat{p}_t))}{2\gamma } - (\hat{g}_t(\bar{p}_t) - \hat{g}_t(\hat{p}_t)) \\
    &= \frac{\gamma}{2} \EE_{p_t \sim \Delta_t}\left[ (\hat{g}_t(\bar{p}_t) - \bar{g}(\bar{p}_t))^2 \mid \mathcal{H}_{t-1} \right] + \frac{\lambda}{2\gamma} \\
    &\le \frac{\gamma}{2} \EE_{p_t \sim \Delta_t}\left[ (\hat{g}_t(\bar{p}_t) - \bar{g}(\bar{p}_t))^2 \mid \mathcal{H}_{t-1} \right] + \frac{K}{2\gamma}
\end{align*}
%
Now, summing these three terms yields that
\[ \EE_{p_t \sim \Delta_t} [\bar{g}_t(p_t) \mid \mathcal{H}_{t-1} ] - \bar{g}_t(\bar{p}_t) \le \frac{K}{\gamma} + \gamma \cdot \EE_{p_t \sim \Delta_t} [(\bar{g}_t(p_t) - \hat{g}_t(p_t))^2 \mid \mathcal{H}_{t-1}]. \]
%
After summing over $t =1, \dots, T$, we have
\[ \preg(T) = \sum_{t=1}^T \EE_{p_t \sim \Delta_t} [\bar{g}_t(p_t) \mid \mathcal{H}_{t-1} ] - \bar{g}_t(\bar{p}_t) \le \frac{KT}{\gamma} + \gamma \sum_{t=1}^T \EE_{p_t \sim \Delta_t} [(\bar{g}_t(p_t) - \hat{g}_t(p_t))^2 \mid \mathcal{H}_{t-1}]. \]
%
Next, we upper bound the RHS with some case work.
\begin{itemize}
    \item  $\bar{x}(p_t; \theta_t) \ge d_t \ge \hat{f}_t(p_t; \theta_t)$:
    \begin{align*}
        (|\bar{x}(p_t; \theta_t) - d_t| - |\hat{f}_t(p_t; \theta_t) - d_t|)^2 
        &\le (|\bar{x}(p_t; \theta_t) - d_t| + |\hat{f}_t(p_t; \theta_t) - d_t|)^2 \\
        &= (\bar{x}(p_t; \theta_t) - d_t + d_t - \hat{f}_t(p_t; \theta_t))^2 \\ 
        &= (\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2
    \end{align*}
    \item $\hat{f}_t(p_t; \theta_t) \ge d_t \ge \bar{x}(p_t; \theta_t)$, similar to the previous case, we have
    \[(|\bar{x}(p_t; \theta_t) - d_t| - |\hat{f}_t(p_t; \theta_t) - d_t|)^2 \le (\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2 \]
    \item $\hat{f}_t(p_t; \theta_t), \bar{x}(p_t; \theta_t) \ge d_t$ or $\bar{x}(p_t; \theta_t), \hat{f}_t(p_t; \theta_t) \le d_t$:
    \[(|\bar{x}(p_t; \theta_t) - d_t| - |\hat{f}_t(p_t; \theta_t) - d_t|)^2 = (\bar{x}(p_t; \theta_t) - d_t + d_t - \hat{f}_t(p_t; \theta_t))^2 = (\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2 \]
\end{itemize}
Therefore,
\begin{equation}\label{equ:reg-to-est}
    \preg(T) = \sum_{t=1}^T \EE_{p_t \sim \Delta_t} [\bar{g}_t(p_t) \mid \mathcal{H}_{t-1} ] - \bar{g}_t(\bar{p}_t) \le \frac{KT}{\gamma} + \gamma \sum_{t=1}^T \EE_{p_t \sim \Delta_t} [(\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2 \mid \mathcal{H}_{t-1}].
\end{equation}
%
We finish the proof by claiming that the final term can be bounded by the online regression oracle's prediction error guarantee with high probability.
Since $\bar{x}$ and $\hat{f}_t$ are bounded, we can apply Lemma~\ref{lem:freedman2} to get
\begin{equation}\label{equ:freedman1}
    \sum_{t=1}^T \EE_{p_t \sim \Delta_t} [(\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2 \mid \mathcal{H}_{t-1}] \le 2 \sum_{t=1}^T (\bar{x}(p_t) - \hat{f}_t(p_t))^2 + O(\log(1/\delta))
\end{equation}
%
Let $x_t$ be our observation on suppliers' production at time $t$.
We expand the RHS as follows:
\begin{align*}
    (\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2
    &= \bar{x}(p_t; \theta_t)^2 - 2\bar{x}(p_t; \theta_t)\hat{f}_t(p_t; \theta_t) + \hat{f}_t(p_t; \theta_t)^2 \\
    &= 2 \bar{x}(p_t; \theta_t)^2 - 2\bar{x}(p_t; \theta_t)\hat{f}_t(p_t; \theta_t) + 2 x_t (\hat{f}_t(p_t; \theta_t) - \bar{x}(p_t; \theta_t)) \\
    &\hspace{10em}+ \hat{f}_t(p_t; \theta_t)^2 - 2 x_t (\hat{f}_t(p_t; \theta_t) - \bar{x}(p_t; \theta_t)) - \bar{x}(p_t; \theta_t)^2 \\
    &= 2(x_t - \bar{x}(p_t; \theta_t))(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)) + (\hat{f}_t(p_t; \theta_t) - x_t)^2 - (\bar{x}(p_t; \theta_t) - x_t)^2 
\end{align*}
%
Therefore,
\begin{equation}\label{equ:diff-expand}
\begin{aligned}
    & \sum_{t=1}^T (\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2 \\
    ={}& \sum_{t=1}^T (\hat{f}_t(p_t; \theta_t) - x_t)^2 - \sum_{t=1}^T (\bar{x}(p_t; \theta_t) - x_t)^2 + \sum_{t=1}^T 2(x_t - \bar{x}(p_t; \theta_t))(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)) \\
    \le{}& \sum_{t=1}^T (\hat{f}_t(p_t; \theta_t) - x_t)^2 - \inf_{f \in \mathcal{F}}\sum_{t=1}^T (f(p_t; \theta_t) - x_t)^2 + \sum_{t=1}^T 2(x_t - \bar{x}(p_t; \theta_t))(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)) \\
    ={}& \est(T) + \sum_{t=1}^T 2(x_t - \bar{x}(p_t; \theta_t))(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t))
\end{aligned}
\end{equation}
%
The last term can be expanded as
\begin{align*}
    &\sum_{t=1}^T (x_t - \bar{x}(p_t; \theta_t))(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)) \\
    ={}& \sum_{t=1}^T (x_t - x^*(p_t; \theta_t) + x^*(p_t; \theta_t) - \bar{x}(p_t; \theta_t))(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)) \\
    \le{}& \sum_{t=1}^T (x_t - x^*(p_t; \theta_t))(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)) + \sum_{t=1}^T \varepsilon\cdot |\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)| \\
    \le{}& \sum_{t=1}^T (x_t - x^*(p_t; \theta_t))(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)) + \sum_{t=1}^T 6 \varepsilon^2 + \frac{1}{24}(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t))^2
\end{align*}
Since $\EE[x_t - x^*(p_t; \theta_t) \mid \mathcal{H}_{t-1}] = 0$ and both $x_t - x^*(p_t; \theta_t)$ and $\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)$ are bounded above by $2\bar{d}$, we can apply Lemma~\ref{lem:freedman1} to the first sum with $\eta = 1/(64\bar{d}^2)$.
And for the second sum, we can apply Lemma~\ref{lem:freedman2}.
Then, with probability at least $1-\delta$, we have
\begin{equation}\label{equ:freedman2}
\begin{aligned}
    &\sum_{t=1}^T (x_t - \bar{x}(p_t; \theta_t))(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t)) \\
    \le{}& \sum_{t=1}^T \left(\frac{1}{64 \bar{d}^2} \EE[(x_t - f^*(p_t; \theta_t))^2(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t))^2 \mid \mathcal{H}_{t-1}] + \frac{1}{16}\EE[(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t))^2 \mid \mathcal{H}_{t-1}]\right) \\
    &\hspace{26em}+ O(\varepsilon^2 T + \log(1/\delta)) \\
    \le{}& \frac{1}{8} \sum_{t=1}^T  \EE[(\hat{f}_t(p_t; \theta_t)  - \bar{x}(p_t; \theta_t))^2 \mid \mathcal{H}_{t-1}] + O(\varepsilon^2 T + \log(1/\delta))
\end{aligned}
\end{equation}
%
Putting \eqref{equ:freedman1}, \eqref{equ:diff-expand} and \eqref{equ:freedman2} together gives us
\begin{align*}
    & \sum_{t=1}^T \EE_{p_t \sim \Delta_t} [(\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2 \mid \mathcal{H}_{t-1}] \\
    \le{}& 2 \est(T) + \frac{1}{2} \sum_{t=1}^T \EE_{p_t \sim \Delta_t} [(\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2 \mid \mathcal{H}_{t-1}] + O(\varepsilon^2 T + \log(1/\delta))
\end{align*}
%
Therefore, 
\begin{equation}\label{equ:est-to-oracle}
    \sum_{t=1}^T \EE_{p_t \sim \Delta_t} [(\bar{x}(p_t; \theta_t) - \hat{f}_t(p_t; \theta_t))^2 \mid \mathcal{H}_{t-1}] \le 4 \est(T) + O(\varepsilon^2 T + \log(1/\delta))
\end{equation}
%
Combining \eqref{equ:reg-to-est} and \eqref{equ:est-to-oracle}, and taking
\[\gamma = \sqrt{\frac{KT}{\est(T) + \varepsilon^2 T + \log(1/\delta)}},\]
we conclude the following regret bound with probability $1-\delta$:
\begin{align*}
    \preg(T)
    &\lesssim \frac{KT}{\gamma} + \gamma \cdot \left(\est(T) + \varepsilon^2 T + \log(1/\delta)\right) \\
    &\lesssim \sqrt{KT(\est(T) + \varepsilon^2 T + \log(1/\delta))} \\
    &\lesssim \sqrt{KT \cdot\est(T)} + \varepsilon\sqrt{K} \cdot T + \sqrt{KT \log(1/\delta)}
\end{align*}
%
Finally, we have
\begin{align*}
    \textsc{Reg}(T) 
    &\lesssim \sqrt{KT \cdot\est(T)} + \varepsilon\sqrt{K} \cdot T + \sqrt{KT \log(1/\delta)} + \varepsilon T + \frac{T}{K} \\
    &\lesssim \sqrt{KT \cdot\est(T)} + \frac{T}{K} + \varepsilon\sqrt{K} \cdot T + \sqrt{KT \log(1/\delta)},
\end{align*}
with probability $1-\delta$.
so we are done.



%\end{proof}