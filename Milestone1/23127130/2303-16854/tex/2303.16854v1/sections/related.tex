\section{Related Work}
\subsection{Large-scale Pre-trained Language Models}
GPT (Generative Pre-trained Transformer) is a family of language models developed by OpenAI, designed to generate human-like natural language text. 
The GPT models are based on the Transformer architecture \cite{NIPS2017_3f5ee243}, which are pre-trained  on an enormous corpus of text data with the standard language modeling objective to predict the next token based on the previous context. 
OpenAI has continuously increased the parameters and training data of its models, and has released GPT \cite{Radford2018ImprovingLU}, GPT-2 \cite{Radford2019LanguageMA}, and GPT-3 \cite{NEURIPS2020_1457c0d6} from 2018 to 2020.
GPT-3 is a powerful language model with 175 billion parameters, and make a significant advance in the field of NLP. 
One of the unique features of GPT-3 is its ability to perform in-context learning, where one can apply it to various tasks by simply providing few-shot demonstrations without any gradient updates or fine-tuning. Furthermore, OpenAI fine-tuned GPT-3 on the code data or instruction data, and released Codex \cite{chen2021evaluating} and InstructGPT \cite{ouyangtraining}, respectively. 
Recently, OpenAI released the GPT-3.5 series models, including text-davinci-003 and ChatGPT, by training on text and code data, then tuning with supervised instructions and reinforcement learning with human feedback. Some recent work has shown that GPT-3.5 models have strong few-shot and zero-shot learning ability on various NLP tasks, such machine translation \cite{jiao2023chatgpt}, and information extraction \cite{wei2023zero}. 

In this paper, we first propose that we can readily change GPT-3.5 to a good data annotator for a specific task by providing the detailed annotation instructions similar to  human annotators. 
% the corresponding task description and few-shot chain-of-thought demonstration. 

\subsection{Pseudo Annotated Data} 
Creating human-annotated data is tedious and costly , particularly for complex tasks or specialized  domains where there may not be sufficient data available. 
Therefore, creating pseudo-annotated data has been widely used to generate labeled data for a specific task when there is a limited amount of annotated data available. 
Back-translation involves translating a target language sentence back into the source language, which is first proposed to improve neural machine translation models with synthetic parallel data \cite{sennrich-etal-2016-improving}. Beyond machine translation, the technique of back-translation has been applied to unsupervised text style transfer \cite{prabhumoye-etal-2018-style} and image style transfer \cite{zhu2017unpaired}.
In addition, rule-based methods are also widely used to construct synthetic data. 
For example, \citet{zhang2020pegasus} resort to the lead bias to create paired data to pre-train the text summarization model, PEGASUS. 
 \citet{lee-etal-2019-latent} pre-trained the retriever with the Inverse Cloze Task, which aims to predict the context based on the given sentence. 

 However, these methods are designed for specific tasks and it is difficult to generalize them to other tasks. 
 In this paper, we study how to transform the powerful language model GPT-3.5 into a general-purpose data annotator. By providing the corresponding task description and few-shot chain-of-thought demonstrations, it can be easily used to annotate data for various  tasks. 
 