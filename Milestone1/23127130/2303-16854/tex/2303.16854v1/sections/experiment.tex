\section{Experiment}

\begin{table}
   \footnotesize
%   \scriptsize
  % \tiny
    \centering
      \begin{tabular}
        % {l|rrr|r}
      {
       m{0.18\textwidth}<{\raggedright}|
       m{0.1\textwidth}<{\raggedleft}
       m{0.1\textwidth}<{\raggedleft}
       }
      \toprule

      Task / Partition & Dev & Test \\
      \midrule
      QK & 350 &1000 \\
      BoolQ& 3270 &3245\\   
      WiC & 638 & 1400 \\ 

    \bottomrule
    \end{tabular}
    \caption{ The basic statistics of the QK, BoolQ and WiC datasets. 
    }\label{tab.data}
\end{table}

\subsection{Tasks and Datasets} 
We evaluate AnnoLLM on three different tasks, including the user query and keyword relevance assessment, BoolQ and WiC. The basic statistics of these datasets are shown in Table \ref{tab.data}.

The QK (user query and keyword relevance assessment) task aims to judge whether the user input query is related to the given keywords. 
 
BoolQ, which stands for Boolean Questions and was introduced by \citet{clark-etal-2019-boolq}, is a question-answering task. In this task, each example comprises a brief passage and a yes/no question related to the passage. The users of the Google search engine anonymously and without solicitation submit the questions, which are then matched with a paragraph from a Wikipedia article that provides the answer. 

The WiC (Word-in-Context) task, developed by \citet{pilehvar-camacho-collados-2019-wic}, involves disambiguating word senses through binary classification of sentence pairs. In this task, two text snippets are provided along with a polysemous word that occurs in both sentences. The objective is to determine whether the target word shares the same sense in both sentences. 

Since all three tasks are binary classification tasks, accuracy is used to evaluate their results.

\subsection{Human Performances}
To evaluate the human performance on the QK dataset, we 
use UHRS\footnote{\url{https://prod.uhrs.playmsn.com/uhrs/}}, a crowdsourcing platform, to annotate this data.
% invite the crowdsourced annotators to annotate this data. 
Before annotation, we first present the annotators with the task description, category definitions, and annotated examples. Then, we invite three annotators to label a data instance. If the annotated results of the three workers are consistent, then this result will be considered as the annotated label. Otherwise, we would invite other annotators to continue annotating this data instance until three people have consistent annotation results. We require the crowdsourced annotators to annotate all development and test sets.

BoolQ and WiC are two of the most challenging datasets in superGLUE \cite{wang2019superglue}. 
In the case of BoolQ, three authors labeled 110 randomly chosen examples, with human performance achieving 89\%.
Regarding WiC, \citet{pilehvar-camacho-collados-2019-wic} selected four groups of 100 instances from the test set, assigning each group to an annotator, and achieving a human performance of 80\%.

\subsection{Experimental Results}
\paragraph{User Query and Keyword Relevance Assessment.} 
Table \ref{tab.qk.result} shows our experimental results on the QK development and test sets. Surprisingly, GPT-3.5 performs worse under the few-shot setting than under the zero-shot setting on this task. \citet{fu2022gptroadmap} speculate that the instruction tuning on GPT-3.5 may decrease the model’s in-context learning ability but increase the model’s zero-shot ability. 

On the other hand, GPT-3.5 with a 4-shot CoT prompt outperforms its counterparts under the zero-shot and few-shot settings by around 6 and 8 points, respectively. Impressively, it even surpasses the crowdsourced annotators. 
The experimental results presented herein provide compelling evidence of the effectiveness of our proposed method, AnnoLLM. 
Previous studies \cite{wei2022chain, wang2022self} have shown that the few-shot CoT, constructed using human-written explanations, can enhance the model's reasoning ability on reasoning tasks. However, our approach differs from previous methods in that we utilize explanations generated by the large model itself, thereby allowing the model's reasoning ability to emerge. 
Additionally, we have demonstrated, for the first time, the effectiveness of the CoT method on tasks other than typical reasoning tasks.

\paragraph{Word Sense Disambiguation.} 
Table \ref{tab.wic.result} presents our experimental results on the WiC development and test sets, from which we also see that AnnoLLM, i.e., GPT-3.5 + 8-shot CoT, outperforms its few-shot counterpart significantly. 
Nevertheless, there remains a considerable disparity between AnnoLLM and crowdsourced annotators. This is primarily due to the task's inherent complexity, and currently, even the supervised SOTA models still exhibit a substantial gap compared to manual annotation.

\paragraph{Question Answering.} 
As shown in Table \ref{tab.boolq.result}, AnnoLLM  surpasses crowdsourced annotators on the BoolQ development and test sets, but does not show significant improvement compared to the few-shot method. However, this does not imply that CoT with generated explanation is not useful for this task. In Section \ref{stability}, we found that AnnoLLM with CoT exhibits better stability across various prompts, while its counterpart with the few-shot setting is highly sensitive to templates. 

\begin{table}[t] 
  \centering
 \footnotesize
%   \scriptsize
  % \tiny
   \begin{tabular}{
    m{0.2\textwidth}<{\raggedright}|
    m{0.08\textwidth}<{\centering}
    m{0.08\textwidth}<{\centering}
    }
    \toprule
    \textbf{Models}   &  \textbf{Dev Set} & \textbf{Test Set} \\
    \midrule
    Crowdsourced Annotator    &   65.58  & 71.5 \\
    \midrule
    GPT-3.5 + zero-shot  & 67.71 & 70.00 \\
    GPT-3.5 + 8-shot  & 65.71 & 67.80 \\
    GPT-3.5 + 4-shot CoT  & \textbf{74.17}$^{\ast}$ & \textbf{75.60}$^{\ast}$ \\
    \bottomrule
 \end{tabular}
 \caption{Evaluation results on the QK task. Accuracy is used as the evaluation metric. 
Results marked with an asterisk (*) represent the average result of five few-shot CoT prompts constructed with different generated explanations.
}\label{tab.qk.result} 
\end{table} 


\begin{table}[t] 
  \centering
 \footnotesize
%   \scriptsize
  % \tiny
   \begin{tabular}{
    m{0.26\textwidth}<{\raggedright}
    m{0.07\textwidth}<{\centering}
    m{0.07\textwidth}<{\centering}
    }
    \toprule
    \textbf{Models}   &  \textbf{Dev Set} & \textbf{Test Set} \\
    \midrule
    Crowdsourced Annotator    &   80.0  &80.0 \\
    \midrule
    \multicolumn{3}{l}{\textbf{Zero/Few-shot}} \\
    PaLM 540B + zero-shot &59.1$^{\ddagger}$ & - \\
    PaLM 540B + 5-shot  &64.6$^{\ddagger}$ & - \\
    GPT-3.5 + zero-shot  & 57.52 & 59.79 \\
    GPT-3.5 + 8-shot  & 67.71& 66.36 \\
    GPT-3.5 + 8-shot CoT  & \textbf{71.47}$^{\ast}$ & \textbf{69.17}$^{\ast}$ \\
    \midrule
    \multicolumn{3}{l}{\textbf{Fine-tune}} \\
    T5 11B \cite{raffel2020exploring}& 77.3$^\ddagger$ & 76.9$^{\dagger}$ \\
    PaLM 540B  & 78.8$^{\ddagger}$ & 77.4$^{\dagger}$ \\
    ST-MoE 32B \cite{zoph2022designing} & \textbf{81.0}$^{\ddagger}$   & \textbf{77.7}$^{\dagger}$\\
    \bottomrule
 \end{tabular}
 \caption{Evaluation results on the WiC task. Accuracy is used as the evaluation metric. Results marked with $\dagger$ and $\ddagger$ are from the official SuperGLUE leaderboard\footnotemark[4] and  PaLM \cite{https://doi.org/10.48550/arxiv.2204.02311}, respectively.  
 Results marked with an asterisk (*) represent the average result of five few-shot CoT prompts constructed with different generated explanations. 
 The number behind the model represents the size of the model's parameters.
 }\label{tab.wic.result} 
\end{table} 

\footnotetext[4]{\url{https://super.gluebenchmark.com/leaderboard}}

\begin{table}[t] 
  \centering
 \footnotesize
%   \scriptsize
  % \tiny
   \begin{tabular}{
    m{0.26\textwidth}<{\raggedright}
    m{0.07\textwidth}<{\centering}
    m{0.07\textwidth}<{\centering}
    }
    \toprule
    \textbf{Models}   &  \textbf{Dev Set} & \textbf{Test Set} \\
    \midrule
    Crowdsourced Annotator    &   89.0  & 89.0 \\
    \midrule
    \multicolumn{3}{l}{\textbf{Zero/Few-shot}} \\
    GPT-3 175B + zero-shot & 60.5 & -\\
    Gopher 280B + zero-shot \cite{rae2021scaling}& 79.3 & -\\
    Chinchilla 70B + zero-shot \cite{hoffmann2022training} &  83.7 & -\\
    PaLM 62B  + zero-shot &  84.8 & -\\
    PaLM 540B  + zero-shot &  88.0 & -\\
    LLaMA 65B  + zero-shot \cite{touvron2023llama}&  85.3  & -\\
    % \midrule
    GPT-3.5 + zero-shot & 84.28 & 84.30 \\
    GPT-3.5 + 8-shot  & 89.17& 89.10 \\
    GPT-3.5 + 8-shot CoT & \textbf{89.69} & \textbf{89.20} \\
    % \midrule
    % GPT-3.5 + zero-shot + inco & 85.44 & - \\
    % GPT-3.5 + 16-shot + inco  & 88.65& - \\
    % GPT-3.5 + 8-shot + CoT + inco  & 89.27 & -\\
    \midrule
    \multicolumn{3}{l}{\textbf{Fine-tune}} \\
    T5 11B \cite{raffel2020exploring}& 90.8$^\ddagger$ & 91.2$^{\dagger}$ \\
    PaLM 540B  & 92.2$^{\ddagger}$ & 91.9$^{\dagger}$ \\
    ST-MoE 32B \cite{zoph2022designing} & \textbf{93.1}$^{\ddagger}$   & \textbf{92.4}$^{\dagger}$\\
    
    \bottomrule
 \end{tabular}
 \caption{Evaluation results on the BoolQ task. Accuracy is used as the evaluation metric. Results marked with $\dagger$ and $\ddagger$ are from the official SuperGLUE leaderboard and  PaLM, respectively. 
 The number behind the model represents the size of the model's parameters.
 }\label{tab.boolq.result} 
\end{table} 



\subsection{Ablation Study}
In this section, we perform an ablation study to compare the effect of different methods used to generate explanations on AnnoLLM's performance. 

Firstly, we want to investigate whether the use of ground truth labels is beneficial for generating explanations for demonstrated examples. To answer this question, we induce LLM to generate explanations using prompts that contain ground truth labels, and prompts that do not contain labels (we only replace the last sentence of the original prompt ``\textit{Briefly explain why the relevance is "Bad", with a response length not exceeding 100 words.}'' with "\textit{Briefly explain the relevance between the keyword and query, with a response length not exceeding 100 words.}"). From Table \ref{tab.ablation}, we found that not using true labels when generating explanations will impair the performance of AnnoLLM by about 3 points on the QK test set (row 4 vs. row 1). This is because the model may give explanations for an incorrect answer without the guidance of ground labels. Please refer to Table \ref{tab.qk.explanation} and Table \ref{tab.qk.explanation_wo_label} for the specific prompts and generated explanations.


Based on the method in the fourth row, we filter out the incorrect explanations using true labels and retain three correct explanations for three out of four demonstrated samples. However, for the third sample, all five generated explanations are wrong, and we have to keep three incorrect explanations for this demonstrated example. That is why using the golden label to filter explanations does not bring significant improvement (row 5 vs. row 4). 

As shown in Table \ref{tab.qk.explanation}, we found that LLM reveals the true answer at the beginning of the generated explanation, and then provides an explanation for it. This is different from previous work \cite{wei2022chain, wang2022self}, which induces the model to provide an explanation first and then output the answer. Therefore, we remove the beginning sentence containing the label from the generated explanations (the underlined text in Table \ref{tab.qk.few-shot-cot}). 
However, this does not lead to improvement (row 2 vs. row 1). We speculate that this may be due to the difference in the nature of our task and traditional reasoning tasks.
We remove the last sentence containing the answer to the demonstrated examples (the Italicized text in Table \ref{tab.qk.few-shot-cot}), yet it does not have too much impact on the performance (row 3 vs. row 1). That is because the generated explanations contains the gold answer. 
To be in line with the prompt format of previous work \cite{wei2022chain}, we still append the ground truth label to the generated explanation. 




\begin{table*}[t] 
  \centering
 \footnotesize
%   \scriptsize
  % \tiny
   \begin{tabular}{
   m{0.05\textwidth}<{\centering}|
    m{0.16\textwidth}<{\centering}|
    m{0.15\textwidth}<{\centering}|
    m{0.15\textwidth}<{\centering}|
    m{0.15\textwidth}<{\centering}|
    m{0.07\textwidth}<{\centering}
    m{0.07\textwidth}<{\centering}
    }
    \toprule
    \multicolumn{5}{c|}{\textbf{Variants of GPT-3.5 + 4-shot CoT}}  & \multicolumn{2}{c}{\textbf{Datasets}}\\
    \midrule
    \#&\textbf{Generate E with L} &\textbf{Delete L from E} &\textbf{Filter E with L} &  \textbf{Append L to L}  &  \textbf{Dev Set} & \textbf{Test Set} \\
    \midrule
    1&$\large\checkmark$ &  &  & $\large\checkmark$ & \textbf{74.17} & \textbf{75.60}  \\
    2&$\large\checkmark$ & $\large\checkmark$ &  & $\large\checkmark$ & 72.97 & 74.76 \\
    % \midrule
    3&$\large\checkmark$ &  &  &  & 74.09 & 75.44  \\
    % \midrule
    
    % 4&&  &  &  &72.00 & 72.87\\
    % 5&&  & $\large\checkmark$ & & 73.52$^{\dagger}$& 72.80$^{\dagger}$\\
    % \midrule
    4&&  &  & $\large\checkmark$ & 72.63 & 72.84\\
    5&&  & $\large\checkmark$ & $\large\checkmark$ & 73.05$^{\dagger}$& 73.20$^{\dagger}$\\
    \bottomrule
 \end{tabular}
 \caption{Ablation study on the QK task. `E' and `L' refer to the generated explanations and ground truth labels, respectively. 
Results marked with a dagger ($\dagger$) represent the average outcome of three few-shot CoT prompts, each constructed with different generated explanations, while the remaining results represent the average of five.
}\label{tab.ablation} 
\end{table*} 




\subsection{More Analysis and Discussion}

\paragraph{Consistency Analysis of Generated Explanations.} \label{consistency}
In the ablation study, we found that the performance of AnnoLLM is highly dependent on the generated explanations. 
This leads to a natural inquiry: Are the explanations produced by the LLM consistent enough for the same demonstrated sample? 
To answer this question, we generate five explanations for each demonstrated sample, and obtain five different few-shot CoT prompts. 
The results obtained using different few-shot CoT prompts are presented in Figure \ref{fig.consistency_stability} (a). It can be observed that different few-shot CoT prompts achieve similar performance in the QK, WiC, and BoolQ tasks. This indicates that the quality of the explanations generated by the LLM is sufficiently consistent.



\paragraph{Stability Analysis of Generated Explanations.} \label{stability}
From Figure \ref{fig.consistency_stability} (a), we can see that AnnoLLM with few-shot CoT prompts significantly outperforms its counterpart with the few-shot setting, with a high margin of 5 percentage points on the QK and WiC datasets in most cases. However, the improvement is quite modest on BoolQ, where it is mostly less than 0.5. 
This does not mean that AnnoLLM with few-shot CoT prompts has no effect on the BoolQ task. 

To further analyze this, we make slight modifications to the existing prompts for BoolQ  to obtain three corresponding few-shot CoT and few-shot prompts (Please refer to Appendix \ref{sec.appendix.e} for the few-shot and few-shot CoT prompts). 
The experimental results in Figure \ref{fig.consistency_stability} show that the few-shot method is very sensitive to the templates, since even with slight modifications to the templates, the experimental performances will drop from around 89 to below 80 points. 
In comparison, AnnoLLM with few-shot CoT prompts suffer less performance loss, and we found that in these cases, AnnoLLM with few-shot CoT prompts outperforms its counterpart with few-shot templates by around 4 points. Therefore, we conclude that few-shot settings are more picky about templates, but few-shot CoT exhibits better stability across different templates.

\begin{figure*}
  \centering
    \subfigure[Consistency]{
      \centering
      \includegraphics[width=0.48\textwidth]{./figures/figure1-crop.pdf} 
    }
    \hspace{0mm}
      \subfigure[Stability]{
        \centering
        \includegraphics[width=0.48\textwidth]{./figures/figure2-crop.pdf} 
      }
      \caption{ 
        Subfigure (a) shows the performance on development sets for different few-shot CoT prompts, created with different explanations. 
        Subfigure (b) shows the performance for different few-shot and few-shot CoT prompts on the development set of BoolQ. 
      }
      \label{fig.consistency_stability}




\end{figure*}