\section{Introduction}
Labeled data refers to a dataset that has been manually annotated with predefined target labels or categories. 
It is crucial to develop machine learning models for many NLP tasks, such as sentiment analysis \cite{socher-etal-2013-recursive}, machine translation \cite{seq2seq} and word sense disambiguation \cite{he-yiu-2022-controllable}. 
The process of labeling data is typically done by human annotators who are given specific guidelines and criteria on how to assign labels to each instance in the dataset. For example, in sentiment analysis, each sentence or document may be labeled with a polarity score such as ``positive'', ``negative'', or ``neutral''. 
However, it is very labor-intensive and time-consuming to create a large dataset with human annotation, 
which limits the availability of such data and the applicability of machine learning models in various NLP tasks. 

Previous works have demonstrated that large-scale pre-trained language models (LLMs), such as GPT-3 \cite{NEURIPS2020_1457c0d6} and PaLM \cite{https://doi.org/10.48550/arxiv.2204.02311}, 
achieve impressive results in  various downstream tasks  without collecting large-scale task-specific data or tuning model parameter but only with a few examples as instructions. 
OpenAI has recently launched the GPT-3.5 series, which are the upgraded versions of GPT-3, and trained on a blend of text and code published before the end of 2021. 
In the meanwhile, OpenAI also unveiled ChatGPT, another fine-tuned version of GPT-3.5. 
Within just two months since its release, ChatGPT has garnered a massive following of 100 million users worldwide, garnering significant global attention. 

\citet{wang-etal-2021-want-reduce} showed that augmenting manually labeled data with pseudo-labeled data from GPT-3 could enhance the performance of models, particularly when the labeling budget is restricted. However, the quality of GPT-3's labeled data still lags behind that of manually labeled data. 
Considering the GPT-3.5 model's remarkable few-shot and zero-shot capabilities across various tasks and the expensive nature of manual annotation, we raise an essential and significant inquiry: Can GPT-3.5 potentially replace crowdsourced annotators?

Before answering this question, let us go over the process of crowdsourced data annotation. First, we need to provide the annotators with a specific definition of the task. Then, for classification tasks, we need to tell the annotators the specific meanings of each category. Finally, we need to provide the annotators with a few examples that have already been annotated as references. 
Naturally, we can guide GPT-3.5 to annotate data using the same approach as with human annotators by providing it with task definitions and example samples. 
Furthermore, we found that requesting a LLM to furnish the rationale behind the ground truth label or answer for a particular example can prompt the LLM to produce high-quality explanations. 
Based on this, we construct the few-shot chain-of-thought prompt \cite{wei2022chain} with the self-generated explanations to annotate data. 
We refer to this method as `explain-then-annotateâ€™, which further improves the annotation quality.

We summarize our contributions as follows: \\
(1) For the first time, we propose that AnnoLLM, a \textbf{Anno}tation system powered by \textbf{L}arge \textbf{L}anguage \textbf{M}odels, can replace crowdsourced annotators to annotate data.\\
% We propose for the first time that \textbf{GPT}-3.5 can replace crowdsourced \textbf{anno}tators to label data, termed AnnoGPT. \\
(2) To enhance the data annotation capabilities of LLMs, we suggest a two-step approach called `explain-then-annotate'. In this approach, we leverage ChatGPT to generate a few-shot chain-of-thought prompt, which we then use to annotate unlabeled data. \\
(3) Our results on three datasets verify the feasibility of substituting crowdsourced annotators with GPT-3.5\footnote{In this paper, we focus on using GPT-3.5 series models to annotate data for classification tasks.}, where it either surpasses or matches crowdsourced annotators. 

\begin{figure*}
  \centering
    \includegraphics[width=1\textwidth]{./figures/worker_GPT-crop.pdf} 
      \caption{ 
On the left is the annotation process used by crowdsourced workers, while on the right is AnnoGPT's process. AnnoGPT mimics the manual annotation process, with the exception that it generates explanations for each example before annotation. This ensures that each demonstrated example is accompanied by helpful explanations, making the annotation guidelines more informative and useful."
      }
      \label{fig.annotation}
\end{figure*}
