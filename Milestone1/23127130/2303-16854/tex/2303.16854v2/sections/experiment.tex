\section{Experiment on Data Annotation}

\begin{table}
   % \footnotesize
  \scriptsize
  % \tiny
    \centering
      \begin{tabular}
        % {l|rrr|r}
      {
       m{0.14\textwidth}<{\raggedright}|
       m{0.07\textwidth}<{\raggedleft}
       m{0.07\textwidth}<{\raggedleft}
       m{0.07\textwidth}<{\raggedleft}
       }
      \toprule
      % \hline

      \textbf{Partition / Task}  & \textbf{QK} & \textbf{BoolQ} & \textbf{WiC}  \\
      \midrule
      % \hline
      Dev & 350 & 3270 & 638 \\
      Test & 1000 & 3245 & 1400 \\
    \bottomrule
    % \hline
    \end{tabular}
    \caption{Basic statistics of QK, BoolQ and WiC datasets. 
    }\label{tab.data}
\end{table}

% \begin{table}
%    \footnotesize
% %   \scriptsize
%   % \tiny
%     \centering
%       \begin{tabular}
%         % {l|rrr|r}
%       {
%        m{0.18\textwidth}<{\raggedright}|
%        m{0.1\textwidth}<{\raggedleft}
%        m{0.1\textwidth}<{\raggedleft}
%        }
%       % \toprule
%       \hline

%       Task / Partition & Dev & Test \\
%       % \midrule
%       \hline
%       QK & 350 &1000 \\
%       BoolQ& 3270 &3245\\   
%       WiC & 638 & 1400 \\ 

%     % \bottomrule
%     \hline
%     \end{tabular}
%     \caption{Basic statistics of QK, BoolQ and WiC datasets. 
%     }\label{tab.data}
% \end{table}
\subsection{Experimental Setups}
\paragraph{Datasets.} 
We evaluate AnnoLLM on three different tasks: QK, BoolQ, and WiC. The basic statistics of these datasets are shown in Table \ref{tab.data}. 
The \textbf{QK} task aims to judge whether the user input query is related to the given keywords. 
\textbf{BoolQ} (Boolean Questions) \cite{clark-etal-2019-boolq} is a question-answering task. In this task, each example comprises a brief passage and a yes/no question related to the passage. 
% The users of the Google search engine anonymously and without solicitation submit the questions, which are then matched with a paragraph from a Wikipedia article that provides the answer. 
The \textbf{WiC} (Word-in-Context) task \cite{pilehvar-camacho-collados-2019-wic} involves disambiguating word senses by classifying sentence pairs. 
% In this task, two text snippets are provided along with a polysemous word that occurs in both sentences. 
The goal is to determine if the target word shares the same sense in both sentences. 

\paragraph{Implementation Details.} 
We use ChatGPT (gpt-3.5-turbo) to generate explanations for demonstrated examples and implement AnnoLLM with text-davinci-003 (a powerful GPT-3.5 model). During generation, we set the temperature $t=0$ for text-davinci-003. 
% Since all tasks are binary classification, accuracy is used for evaluation.
As all tasks involve binary classification, accuracy is employed for evaluation.



\paragraph{Human Performances.}
To assess human performance on QK, we use UHRS\footnote{\url{https://prod.uhrs.playmsn.com/uhrs/}}, a crowdsourcing platform, for data annotation. 
% invite the crowdsourced annotators to annotate this data. 
Before annotation, we provide the task description, category definitions, and annotated examples to annotators. 
% Then, three annotators to label a data instance. 
If the annotated results of three workers are consistent, this result will be considered as the annotated label. 
Otherwise, additional annotators will continue annotating this data instance until three annotators have consistent annotation results. We require crowdsourced annotators to annotate all development and test sets. 
BoolQ and WiC are two of the most challenging datasets in superGLUE \cite{wang2019superglue}. 
For BoolQ, three authors labeled 110 randomly chosen examples, with human performance reaching 89\%.
As for WiC, \citet{pilehvar-camacho-collados-2019-wic} selected four groups of 100 test instances, and assigned each group to an annotator, achieving a human performance of 80\%.


\subsection{Experimental Results}
Table \ref{tab.qk.result} shows our experimental results on the QK development and test sets. Surprisingly, GPT-3.5 (text-davinci-003) performs worse in the few-shot setting compared to the zero-shot setting in this task. \citet{fu2022gptroadmap} speculate that the instruction tuning on GPT-3.5 may decrease its in-context learning ability but increase its zero-shot ability. On the other hand, AnnoLLM (text-davinci-003 + 4-shot CoT) outperforms its counterparts under the zero-shot and few-shot settings by around 6 and 8 points, respectively. Impressively, it even surpasses the crowdsourced annotators. 

Table \ref{tab.wic.result} presents our experimental results on WiC, from which we also see that AnnoLLM (text-davinci-003 + 8-shot CoT) outperforms its few-shot counterpart significantly. 
Nevertheless, there remains a considerable disparity between AnnoLLM and crowdsourced annotators. 
This can be attributed to the inherent complexity of the task, since even the best supervised models still exhibit a substantial gap compared to human performance.

As shown in Table \ref{tab.boolq.result}, AnnoLLM (text-davinci-003+8-shot CoT) surpasses human annotators and is comparable to supervised models on BoolQ, but does not show significant improvement compared to the few-shot method. However, this does not imply that CoT with generated explanation is not useful for this task. Section \ref{stability} shows that AnnoLLM with CoT exhibits better stability across different prompts, while its counterpart with the few-shot setting is highly sensitive to templates. 


Overall, AnnoLLM surpasses or matches human performances in three tasks, demonstrating its potential to replace crowdsourced annotators. 
AnnoLLM differs from previous methods \cite{wei2022chain, wang2022self} in two aspects: (1) We use explanations generated by LLMs rather than those written by humans. 
(2) We have shown, for the first time, that the CoT method is effective in tasks beyond typical reasoning tasks.


\begin{table}[t] 
  \centering
 % \footnotesize
  \scriptsize
  % \tiny
   \begin{tabular}{
    m{0.3\textwidth}<{\raggedright}
    m{0.05\textwidth}<{\centering}
    m{0.05\textwidth}<{\centering}
    }
    \toprule
    % \hline
    \textbf{Models}   &  \textbf{Dev} & \textbf{Test} \\
    \midrule
    % \hline
    Crowdsourced Annotator    &   65.58  & 71.5 \\
    \midrule
    % \hline
    text-davinci-003 + zero-shot  & 67.71 & 70.00 \\
    text-davinci-003 + 8-shot  & 65.71 & 67.80 \\
    text-davinci-003 + 4-shot CoT (\textbf{AnnoLLM})   & \textbf{74.17}$^{\ast}$ & \textbf{75.60}$^{\ast}$ \\
    \bottomrule
    % \hline
 \end{tabular}
 \caption{Evaluation results (\%) on QK. 
 Accuracy is used as the evaluation metric. 
Results marked with $\ast$ represent the average result of five CoT prompts constructed with different generated explanations. 
}\label{tab.qk.result} 
\end{table} 


\begin{table}[t] 
  \centering
 % \footnotesize
  \scriptsize
  % \tiny
   \begin{tabular}{
    m{0.3\textwidth}<{\raggedright}
    m{0.05\textwidth}<{\centering}
    m{0.05\textwidth}<{\centering}
    }
    \toprule
    % \hline
    \textbf{Models}   &  \textbf{Dev} & \textbf{Test} \\
    \midrule
    % \hline
    Crowdsourced Annotator    &   \multicolumn{2}{c}{80.0} \\
    \midrule
    % \hline
    \multicolumn{3}{l}{\textbf{Zero/Few-shot}} \\
    PaLM 540B + zero-shot &59.1$^{\ddagger}$ & - \\
    PaLM 540B + 5-shot  &64.6$^{\ddagger}$ & - \\
    text-davinci-003 + zero-shot  & 57.52 & 59.79 \\
    text-davinci-003 + 8-shot  & 67.71& 66.36 \\
    text-davinci-003 + 8-shot CoT (\textbf{AnnoLLM})  & \textbf{71.47}$^{\ast}$ & \textbf{69.17}$^{\ast}$ \\
    \midrule
    % \hline
    \multicolumn{3}{l}{\textbf{Fine-tune}} \\
    T5 11B \cite{raffel2020exploring}& 77.3$^\ddagger$ & 76.9$^{\dagger}$ \\
    PaLM 540B  & 78.8$^{\ddagger}$ & 77.4$^{\dagger}$ \\
    ST-MoE 32B \cite{zoph2022designing} & \textbf{81.0}$^{\ddagger}$   & \textbf{77.7}$^{\dagger}$\\
    \bottomrule
    % \hline
 \end{tabular}
 \caption{Evaluation results (\%) on the WiC task. 
 Accuracy is used as the evaluation metric. Results marked with $\dagger$ and $\ddagger$ are from the official SuperGLUE leaderboard\footnotemark[4] and  PaLM \cite{chowdhery2022palm}, respectively.  
 Results marked with $\ast$ represent the average result of five CoT prompts constructed with different generated explanations. 
 Numbers behind models denote the size of models'  parameters.
 }\label{tab.wic.result} 
\end{table} 

\footnotetext[4]{\url{https://super.gluebenchmark.com/leaderboard}}

\begin{table}[t] 
  \centering
 % \footnotesize
  \scriptsize
  % \tiny
   \begin{tabular}{
    m{0.32\textwidth}<{\raggedright}
    m{0.04\textwidth}<{\centering}
    m{0.04\textwidth}<{\centering}
    }
    \toprule
    % \hline
    \textbf{Models}   &  \textbf{Dev} & \textbf{Test} \\
    \midrule
    % \hline
    Crowdsourced Annotator    &  \multicolumn{2}{c}{89.0} \\
    \midrule
    % \hline
    \multicolumn{3}{l}{\textbf{Zero/Few-shot}} \\
    GPT-3 175B + zero-shot & 60.5 & -\\
    Gopher 280B + zero-shot \cite{rae2021scaling}& 79.3 & -\\
    Chinchilla 70B + zero-shot \cite{hoffmann2022training} &  83.7 & -\\
    PaLM 62B  + zero-shot &  84.8 & -\\
    PaLM 540B  + zero-shot &  88.0 & -\\
    LLaMA 65B  + zero-shot \cite{touvron2023llama}&  85.3  & -\\
    % \midrule
    text-davinci-003 + zero-shot & 84.28 & 84.30 \\
    text-davinci-003 + 8-shot  & 89.17& 89.10 \\
    text-davinci-003 + 8-shot CoT (\textbf{AnnoLLM})& \textbf{89.69} & \textbf{89.20} \\
    % \midrule
    % GPT-3.5 + zero-shot + inco & 85.44 & - \\
    % GPT-3.5 + 16-shot + inco  & 88.65& - \\
    % GPT-3.5 + 8-shot + CoT + inco  & 89.27 & -\\
    \midrule
    % \hline
    \multicolumn{3}{l}{\textbf{Fine-tune}} \\
    T5 11B \cite{raffel2020exploring}& 90.8$^\ddagger$ & 91.2$^{\dagger}$ \\
    PaLM 540B  & 92.2$^{\ddagger}$ & 91.9$^{\dagger}$ \\
    ST-MoE 32B \cite{zoph2022designing} & \textbf{93.1}$^{\ddagger}$   & \textbf{92.4}$^{\dagger}$\\
    
    \bottomrule
    % \hline
 \end{tabular}
 \caption{Evaluation results (\%) on the BoolQ task. Accuracy is used as the evaluation metric. Results marked with $\dagger$ and $\ddagger$ are from the official SuperGLUE leaderboard and PaLM, respectively. 
Numbers behind models denote the size of models' parameters.
 }\label{tab.boolq.result} 
\end{table} 



\subsection{Ablation Study}
In this section, we conduct an experiment to compare the impact of various explanation generation methods on the performance of AnnoLLM.

Firstly, we want to investigate whether using ground truth labels is helpful for generating explanations for demonstrated examples. 
To answer this, we induce LLMs to generate explanations using prompts with and without ground truth labels. 
Specifically, we replace the last sentence of the prompt in Table \ref{tab.qk.explanation} \textit{Briefly explain why the relevance is "Bad"} with \textit{Briefly explain the relevance between the keyword and query} in Table \ref{tab.qk.explanation_wo_label}. 
From Table \ref{tab.ablation}, we found that not using true labels when generating explanations leads to a decrease in AnnoLLM's performance by approximately 3 points on the QK test set (row 4 vs. row 1).
This is because the model may generate explanations for incorrect answers without the guidance of ground truth labels. 
% Please refer to  and  for complete prompts and generated explanations.

% Based on the method in the fourth row, we filter out the incorrect explanations using true labels and retain three correct explanations for three out of four demonstrated samples. However, for the third sample, all five generated explanations are wrong, and we have to keep three incorrect explanations for this demonstrated example. That is why using the golden label to filter explanations does not bring significant improvement (row 5 vs. row 4). 


In Table \ref{tab.qk.explanation}, we found that LLMs initially reveal the true answer, and then provide an explanation for it. This differs from previous work \cite{wei2022chain}, where LLMs are prompted to give an explanation before outputting the answer. 
Therefore, we remove the initial sentence with labels from generated explanations (underlined text in Table \ref{tab.qk.few-shot-cot}). 
However, this modification does not lead to any improvement (row 2 vs. row 1). We speculate that this may be attributed to the disparity between our task and traditional reasoning tasks. 
In addition, we remove the last sentence containing the answer to the demonstrated examples (italicized text in Table \ref{tab.qk.few-shot-cot}), yet it does not have too much impact on the performance (row 3 vs. row 1). 
That is because the generated explanations already contain the correct answers. 
Nonetheless, to align with the format used in previous work \cite{wei2022chain}, we still append ground truth labels to generated explanations. 



\begin{table}[t] 
  \centering
 % \footnotesize
  \scriptsize
  % \tiny
   \begin{tabular}{
   m{0.001\textwidth}<{\centering}|
    m{0.075\textwidth}<{\centering}|
    m{0.07\textwidth}<{\centering}|
    m{0.07\textwidth}<{\centering}|
    m{0.05\textwidth}<{\centering}
    m{0.05\textwidth}<{\centering}
    }
    % \hline
    \toprule
    \multicolumn{4}{c|}{\textbf{text-davinci-003 + 4-shot CoT}}  & \multicolumn{2}{c}{\textbf{Datasets}}\\
    \midrule
    \#&\textbf{Generate E with L} &\textbf{Delete L from E} & \textbf{Append L to E} & \textbf{Dev Set} & \textbf{Test Set} \\
    \midrule
    1&$\large\checkmark$ &   & $\large\checkmark$ & \textbf{74.17} & \textbf{75.60}  \\
    2&$\large\checkmark$ & $\large\checkmark$  & $\large\checkmark$ & 72.97 & 74.76 \\
    3&$\large\checkmark$ &  &  & 74.09 & 75.44  \\
    4&&   & $\large\checkmark$ & 72.63 & 72.84\\
    \bottomrule
 \end{tabular}
 \caption{Ablation study on the QK task. `E' and `L' refer to the generated explanations and ground truth labels, respectively. 
All results are averaged across five few-shot CoT prompts, each consisting of different generated explanations. 
% Results marked with a dagger ($\dagger$) represent the average outcome of three few-shot CoT prompts, each constructed with different generated explanations, while the remaining results represent the average of five.
}\label{tab.ablation} 
\end{table} 





\subsection{More Analysis and Discussion}

\paragraph{Consistency Analysis of Generated Explanations.} \label{consistency}
In the ablation study, we found that the performance of AnnoLLM relies heavily on the generated explanations. 
This leads to a natural inquiry: \textit{Are the explanations produced by ChatGPT consistent enough for the same demonstrated sample?} 
To answer this, we generate five explanations for each sample, and obtain five different few-shot CoT prompts. 
As shown in Figure \ref{fig.consistency_stability} (a), these different few-shot CoT prompts yield similar performance in the QK, WiC, and BoolQ tasks. 
This indicates that the quality of the explanations generated by ChatGPT is sufficiently consistent.

% The results obtained using different few-shot CoT prompts are presented in Figure \ref{fig.consistency_stability} (a). It can be observed that different few-shot CoT prompts achieve similar performance in the QK, WiC, and BoolQ tasks. 









\paragraph{Stability Analysis of Generated Explanations.} \label{stability}
Figure \ref{fig.consistency_stability} (a) shows that AnnoLLM with few-shot CoT prompts significantly outperforms its counterpart with  few-shot settings  on QK and WiC. 
% with a notable margin of 5 percentage points on  QK and WiC in most cases. 
However, the improvement is quite modest on BoolQ, where it is generally less than 0.5. 
This does not mean that AnnoLLM with few-shot CoT prompts has no effect on BoolQ. 
To further analyze this, we make slight modifications to the existing prompts for BoolQ to obtain three few-shot CoT and few-shot prompts (refer to Appendix \ref{sec.appendix.e} for details). 
Figure \ref{fig.consistency_stability} (b) shows that the few-shot method is highly sensitive to templates. Even with slight modifications to templates, the experimental performances drop from around 89 to below 80 points. 
In comparison, AnnoLLM with few-shot CoT prompts suffers less performance loss, which  outperforms its counterpart with few-shot templates by around 4 points. 
To summarize, the few-shot setting is more picky about templates, whereas few-shot CoT exhibits better stability across different templates.




\begin{figure*}
  \centering
    \subfigure[Consistency]{
      \centering
      \includegraphics[width=0.47\textwidth]{./figures/figure1-crop.pdf} 
    }
    % \hspace{0mm}
      \subfigure[Stability]{
        \centering
        \includegraphics[width=0.284\textwidth]{./figures/figure2-crop.pdf} 
      }
      \caption{ 
        Subfigure (a) shows the performance on dev sets for CoT prompts created with different explanations. 
        Subfigure (b) shows the performance for different few-shot and few-shot CoT prompts on the dev set of BoolQ. 
        The X-axis represents the index of CoT prompts, while the Y-axis denotes accuracy.
        % X-axis: Index of CoT prompts. Y-axis: Accuracy.
      }
      \label{fig.consistency_stability}

\end{figure*}


\section{Experiment on Data Creation}\label{ConIR}
% \subsection{Experimental Setups}
\paragraph{Datasets.} 
We construct the conversation-based information retrieval (\textbf{ConIR}) dataset based on the MS-MARCO passage ranking dataset \cite{bajaj2016ms}. 
The sizes of the training and test sets for ConIR are 71,557 and 3,000 respectively. 
\paragraph{Implementation Details.} Since ChatGPT is optimized for chat, we use it to create ConIR, namely using it to enrich paragraphs, generate and filter out irrelevant conversations in Appendix \ref{sec.appendix.f}. 
Following previous work \cite{qu-etal-2021-rocketqa}, we resort to MRR@10 and Recall of top-k (R@k) to evaluate the retrieval performance on different models. 
% which represents the proportion of top k retrieved passages that contain the answers.

% \subsection{Experimental Results} 
\paragraph{Zero-shot Performance.} We train two typical dense retrieval models, DPR \cite{karpukhin-etal-2020-dense} (initialized with DistilBERT \cite{DistilBERT}) and PROD \cite{lin2023prod}, on MS-MARCO, and then evaluate them on the test set of ConIR. Notably, both models exhibit poor performance on ConIR, as demonstrated in Table \ref{table.ConIR}. 
This indicates that dense retrieval models trained on traditional datasets are not directly applicable to conversation-based information retrieval.

\paragraph{In-domain Performance. } As shown in Table \ref{table.ConIR}, DPR fine-tuned on the training set of ConIR performs much better than its zero-shot counterpart, highlighting the necessity of the ConIR dataset. 

\paragraph{Human Evaluation. } We randomly select 100 generated conversations and their paired paragraphs. Three annotators are asked to assess the fluency of conversations on a 5-point Likert scale from 1 (not fluent) to 5 (extremely fluent), and their relevance and factual consistency with the paired passages on a 3-point Likert scale. 
Table \ref{table.ConIR.human} shows that the  conversations of ConIR 
exhibit remarkable fluency, displaying a strong correlation with the paired paragraphs in terms of relevance and factual consistency. 
% are very fluent, and have a strong relevance and consistency with the paired paragraphs. 
The inter-annotator agreement measured using Fleiss' \textit{kappa} \cite{Fleiss1971MeasuringNS} is 0.55, implying moderate agreement \cite{Landis1977TheMO}.
Please refer to Appendix \ref{sec.appendix.g} for more details.
% about human evaluation.


\begin{table}
\scriptsize
  \centering
    \begin{tabular}{
     m{0.12\textwidth}<{\raggedright}
     m{0.05\textwidth}<{\centering}
     m{0.04\textwidth}<{\centering}
     m{0.035\textwidth}<{\centering}
     m{0.035\textwidth}<{\centering}
     m{0.04\textwidth}<{\centering}
     }
    \toprule
    \textbf{Models}& \textbf{MRR@10} &\textbf{R@1}  &\textbf{R@5} &\textbf{R@50} &\textbf{R@100} \\
    \midrule 
    % \multicolumn{3}{l}{\textbf{Zero-shot}} \\
    \textbf{DPR (Zero-shot)} & 7.01 &4.85 & 9.75& 18.70 & 22.08\\
    \textbf{PROD (Zero-shot)} & 10.61 & 7.53 & 14.80 & 28.22 & 32.77 \\
    \midrule
    % \multicolumn{3}{l}{\textbf{Fine-tune}} \\
    \textbf{DPR (Fine-tune)} & \textbf{19.32} & \textbf{12.27} & \textbf{28.60}  & \textbf{56.13} &  \textbf{64.25}\\
    \bottomrule

  \end{tabular}
  \caption{Retrieval results on the test set of ConIR. 
  }\label{table.ConIR}
\end{table}

\begin{table}
\scriptsize
  \centering
    \begin{tabular}{
     m{0.05\textwidth}<{\raggedright}
     m{0.05\textwidth}<{\centering}
     m{0.05\textwidth}<{\centering}
     m{0.2\textwidth}<{\centering}
     }
    \toprule
    \textbf{Fluency}  &\textbf{Relevance} &\textbf{Consistency} & \textbf{Inter-annotator agreement} \\
    \midrule 
    4.99 & 2.53& 2.41& 0.55\\
    \bottomrule

  \end{tabular}
  \caption{Human evaluation results on ConIR.
  }\label{table.ConIR.human}
\end{table}