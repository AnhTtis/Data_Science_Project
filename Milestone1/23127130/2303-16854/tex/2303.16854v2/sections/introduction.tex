\section{Introduction}
Labeled data refers to a dataset that has been manually annotated with predefined target labels or categories. 
It is crucial to develop machine learning models for many NLP tasks, such as sentiment analysis \cite{socher-etal-2013-recursive}, machine translation \cite{seq2seq} and word sense disambiguation \cite{he-yiu-2022-controllable}. 
The process of labeling data is typically done by human annotators 
% who are given 
under 
specific guidelines and criteria on how to assign labels to each instance in the dataset. For example, in sentiment analysis, each sentence or document may be labeled with a polarity score such as ``positive'', ``negative'', or ``neutral''. 
However, it is very labor-intensive and time-consuming to create a large dataset with human annotation, 
which limits the availability of such data in various NLP tasks. 
% and the applicability of machine learning models in various NLP tasks. 

Previous works have shown that LLMs, such as GPT-3 \cite{NEURIPS2020_1457c0d6} and PaLM \cite{chowdhery2022palm}, 
achieve impressive results in many downstream tasks without requiring large-scale task-specific data or parameter tuning, but only with a few examples as instructions. 
OpenAI has recently launched the GPT-3.5 series models, the upgraded versions of GPT-3. 
% which are trained on a blend of text and code published before the end of 2021. 
Shortly after, OpenAI also unveiled ChatGPT, another fine-tuned version of GPT-3.5, which has gained significant global attention since its launch.  
% Within just two months since its release, ChatGPT has garnered a massive following of 100 million users worldwide, garnering significant global attention. 





Augmenting manually labeled data with pseudo-labeled data from GPT-3 is helpful for many NLP tasks, particularly when the labeling budget is restricted \cite{wang-etal-2021-want-reduce}. However, the quality of GPT-3's labeled data still lags behind that of manually labeled data. 
Considering the GPT-3.5 models' remarkable zero/few-shot capabilities, we raise an essential and significant inquiry: \textit{Can GPT-3.5 potentially replace crowdsourced annotators?}

Before answering this question, let us go over the process of crowdsourced data annotation. First, we need to provide  annotators with a specific definition of the task. Then, for classification tasks, we need to tell annotators the specific meanings of each category. Finally, we need to provide annotators with a few examples that have already been annotated as references. 
Naturally, we can guide GPT-3.5 to annotate data using the same approach as with human annotators by providing task definitions and example samples. 
Furthermore, we found that requesting LLMs to furnish the rationale behind the ground truth label for a particular example can prompt LLMs to produce high-quality explanations. 
Based on this, we create the few-shot chain-of-thought (COT) prompt \cite{wei2022chain} with the self-generated explanations to annotate data. 
We refer to this method as \textit{explain-then-annotate}, which further improves the annotation quality.

We summarize our contributions as follows: 
(1) We propose \textbf{AnnoLLM}, an \textbf{Anno}tation system powered by \textbf{L}arge \textbf{L}anguage \textbf{M}odels, which is based on \textit{explain-then-annotate} and has the potential to replace crowdsourced annotators to annotate data. 
% To enhance the data annotation capabilities of LLMs, we suggest a two-step approach called \textit{explain-then-annotate}. 
% In this approach, we leverage ChatGPT to generate a few-shot chain-of-thought prompt, which we then use to annotate unlabeled data. 
(2) Our results on three datasets verify the feasibility of substituting crowdsourced annotators with GPT-3.5, 
% \footnote{In this paper, we focus on using GPT-3.5 series models to annotate data for classification tasks.}
where it either surpasses or matches crowdsourced annotators. 
% (3) We construct the first conversation-based information retrieval dataset with our proposed AnnoLLM. Human evaluation shows that this dataset has high quality in terms of fluency, relevance, and factual consistency. 
(3) Furthermore, AnnoLLM is not limited to annotating classification data, and 
we create the first conversation-based information retrieval (\textbf{ConIR}) dataset using AnnoLLM\footnote{ConIR is available at: \url{https://github.com/NLPCode/AnnoLLM}.}. Through rigorous human evaluation, this dataset exhibits high quality in terms of fluency, relevance, and factual consistency.

\begin{figure*}
  \centering
    \includegraphics[width=1\textwidth]{./figures/worker_GPT-crop.pdf} 
      \caption{ 
On the left is the annotation process used by crowdsourced workers, while on the right is AnnoLLM's process. AnnoLLM mimics the manual annotation process, with the exception that it generates explanations for each example before annotation. This ensures that each demonstrated example is accompanied by helpful explanations, making the annotation guidelines more informative and useful.
      }
      \label{fig.annotation}
\end{figure*}
