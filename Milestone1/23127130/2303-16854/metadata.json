{
    "arxiv_id": "2303.16854",
    "paper_title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators",
    "authors": [
        "Xingwei He",
        "Zhenghao Lin",
        "Yeyun Gong",
        "A-Long Jin",
        "Hang Zhang",
        "Chen Lin",
        "Jian Jiao",
        "Siu Ming Yiu",
        "Nan Duan",
        "Weizhu Chen"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-03-30"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL"
    ],
    "abstract": "Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data. We conduct experiments on three tasks, including user input and keyword relevance assessment, BoolQ and WiC. The annotation results from GPT-3.5 surpasses those from crowdsourced annotation for user input and keyword relevance assessment. Additionally, for the other two tasks, GPT-3.5 achieves results that are comparable to those obtained through crowdsourced annotation.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16854v1"
    ],
    "publication_venue": null
}