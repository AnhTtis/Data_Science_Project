
\documentclass[letterpaper, 10 pt, conference]{ieeeconf-2}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                         % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath}% assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{color}
 \usepackage{dsfont}
 \usepackage{cite}
 \usepackage{url}
 \newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newcommand{\zak}[1]{ ({\color{magenta}Zak: #1})}

\def\Problem{\noindent\hspace{2em}{\itshape Problem: }}



\title{\LARGE \bf
Universal Approximation Property of Hamiltonian Deep Neural Networks
}

\def\Real{\mathbb{R}}
\def\Rn{\mathbb{R}^n}
\def\Rtn{\mathbb{R}^{2n}}
\def\Rp{\mathbb{R}^p}
\def\Rm{\mathbb{R}^m}
\def\Realp{\mathbb{R}_+}
\def\Comp{\mathbb{C}}
\def\Nat{\mathbb{N}}
\def\C{\mathcal{C}}
\def\P{\mathcal{P}}
\def\Wt{\tilde{W}}
\def\bt{\tilde{b}}
\def\dt{\tilde{d}}
\def\rt{\tilde{r}}
\def\etat{\tilde{\eta}}
\def\gammat{\tilde{\gamma}}
\def\At{\tilde{A}}
\def\Jt{\tilde{J}}
\def\L{\mathcal{L}}


\def\ntn{{n\times n}}
\def\eg{\emph{e.g.} }
\def\e{\textnormal{e}}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}


\author{Muhammad Zakwan, Massimiliano d'Angelo and Giancarlo Ferrari-Trecate % <-this % stops a space
\thanks{This research is supported by the Swiss National Science Foundation under the NCCR Automation (grant agreement 51NF40 180545).}% <-this % stops a space
\thanks{Muhammad Zakwan and Giancarlo Ferrari Trecate are with the Institute of Mechanical Engineering, Ecole Polytechnique Fédérale de Lausanne (EPFL), CH-1015 Lausanne, Switzerland {\tt\{muhammad.zakwan, giancarlo.ferraritrecate\}@epfl.ch}.  Massimiliano d'Angelo is with Sapienza University of Rome, Rome, 00185, Italy and with the Istituto di Analisi dei Sistemi e Informatica, Italian National Research Council (IASI-CNR), Rome, 00185, Italy, {\tt mdangelo@diag.uniroma1.it}.}%
%
}
\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
This paper investigates the universal approximation capabilities of Hamiltonian Deep Neural Networks (HDNNs) that arise from the discretization of Hamiltonian Neural Ordinary Differential Equations. 
Recently, it has been shown that HDNNs enjoy, by design, non-vanishing gradients, which provide numerical stability during training. However, although HDNNs have demonstrated state-of-the-art performance in several applications, a comprehensive study to quantify their expressivity is missing. In this regard, we provide a universal approximation theorem for HDNNs and prove that a portion of the flow of HDNNs can approximate arbitrary well any continuous function over a compact domain. This result provides a solid theoretical foundation for the practical use of HDNNs.
%Furthermore, we also analyze the approximation error with respect to complexity.    
% \zak{TO DOs: 
% % \begin{enumerate}
% %     \item See if we can say something about the composition of two function $\alpha$ and $\beta$, such that $|f - \beta \circ \phi \circ \alpha| < \varepsilon$, where $\alpha$ and $\beta$ are injection and projection, respectively. 
% %     \item What would happen if we include input and output to the system ?
% %     \item Discrete Hamiltonians? or continuous?
% % \end{enumerate}}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
%-------------------------------------------------------------------------
%\textcolor{blue}{Why Deep neural networks are important? }
%-------------------------------------------------------------------------
Deep Neural Networks (DNNs) have been crucial for the success of machine learning in several real-world applications like computer vision, natural language processing, and reinforcement learning.
To achieve state-of-the-art performance, a common approach in machine learning is to increase the Neural Network (NN) depth. For instance, Convolutional Neural Networks (CNNs) AlexNet \cite{krizhevsky2017imagenet}, Visual Geometric Group (VGG) network \cite{simonyan2014very}, GoogLeNet/Inception \cite{szegedy2015going}, Residual Network (ResNet) \cite{he2016deep}, or recently developed transformers such as ChatGPT, contain hundreds to thousands of layers. 
It has been empirically demonstrated that deeper networks yield better performance than single-hidden-layer NNs for large-scale and high-dimensional problems \cite{shen2021neural, lin2018resnet,didier2022approximate}. 
However, a rigorous characterization of the approximation capabilities of complex NNs is often missing. Moreover, the understanding of how NN architectures (depth, width, and type of activation function) achieve their empirical success is an open research problem \cite{raghu2017expressive}. 
% The basic layers of DNNs consist of affine linear transformations and nonlinear activation functions.
% The composition of these simple layers can generate a complex network with powerful approximation capabilities. %
%Interestingly, a small approximation bound can be achieved using NN with a relatively small number of parameters compared to the exponential number of parameters required by traditional polynomials, spline, and trigonometric expansions \cite{barron1993universal}.
%-------------------------------------------------------------------------
%\textcolor{blue}{Why we need to study universal approximation ???}
%-------------------------------------------------------------------------

To quantify the representational power of NNs, researchers have focused on studying their Universal Approximation Properties (UAPs), namely their ability to approximate any desired continuous function with an arbitrary accuracy. 
To this aim,  several UAP results for various classes of NNs have been proposed. The UAP of Shallow NNs (SNNs), \emph{i.e.} with single hidden layer has proven in the seminal works of  Cybenko \cite{cybenko1989approximation} and Hornik \cite{hornik1989multilayer}. Exploiting the latter arguments, researchers have provided several results on UAPs for DNNs. 
For instance, in \cite{shen2021neural} it is proved that a DNN with three hidden layers and specific types of activation functions has the UAP. 
% However, this limitation may introduce challenges  to use them for broader applications, such as image classification, and natural language processing. 
The paper \cite{lin2018resnet} demonstrates that a very deep ResNet, with stacked modules having one neuron per hidden layer and rectified linear unit (ReLU) activation functions, can uniformly approximate any integrable function. However, extending these results to other classes of activation functions is not straightforward.  We defer the interested readers to \cite{guhring2020expressivity} for a detailed survey on the subject.
%-------------------------------------------------------------------------
%\textcolor{blue}{General introduction to the problem Deeper does not mean it is good too.}
%-------------------------------------------------------------------------


Recently, an alternate representation of DNNs as dynamical systems has been proposed \cite{haber2017stable}. This idea was later popularized as Neural Ordinary Differential Equations (NODEs) \cite{chen2018neural}. By viewing DNNs through a dynamical perspective, researchers have been able to utilize tools from system theory in order to analyze their properties (\emph{e.g.}, Lyapunov stability, contraction theory, and symplectic properties).
% Moreover, 
% % one can also incorporate  desired properties by construction, and 
% it has been proven empirically that NODEs enjoy non-vanishing gradients \cite{aizawa2020universal}. 
Similar to DNNs, there are some contributions on UAPs for NODEs.
It has been shown in \cite{zhang2020approximation} that capping a NODE with a single linear layer is sufficient to guarantee the UAP, but exclusively for non-invertible continuous functions.
Furthermore, in \cite{tabuada2020universal, tabuada2022universal}, differential geometric tools for controllability analysis were used to provide UAPs for a class of NODEs, while in \cite{li2022deep}, the compositional properties of the flows of NODEs were exploited to obtain UAPs. Both \cite{tabuada2020universal} and \cite{tabuada2022universal} have certain restrictions on the choice of activation functions, whereas \cite{li2022deep} impose constraints on the desired target function.
Finally in \cite{celledoni2022dynamical}, some interesting tools, such as composition of contractive, expansive, and sphere-preserving flow maps, have been used to prove a universal approximation theorem for the flows of dynamical systems. 
%However, none of the aforementioned works focus on classes of NNs with provable non-vanishing gradients, which is the main focus of this paper.

Although DNNs tend to empirically perform well in general, the increasing depth can also present challenges, such as the vanishing/exploding gradient problem during the training via gradient descent algorithms. 
These phenomenon happen when the gradients computed during back-propagation \cite{werbos1990backpropagation}  either approach to zero or diverge. 
In such cases, the learning process may stop prematurely or become unstable, thereby limiting the depth of DNNs that can be utilized and consequently preventing the practical exploitation of UAP in DNNs.
% , while general classes of NNs such as MLPs and recurrent NNs are subject to vanishing or exploding gradients during training via gradient descent algorithms. 
Practitioners have proposed several remedies to address these challenges, including skip connections in ResNet \cite{he2016deep}, batch normalization, subtle weights initialization, regularization techniques such as dropout or weight decay, and gradient clipping \cite{goodfellow2016deep}. 
However, all of these ad hoc methods do not come with provable formal guarantees of non-vanishing gradients. Recently, a class of DNNs called Hamiltonian Deep Neural Networks (HDNNs) have been proposed in \cite{galimberti2023hamiltonian}. These DNNs stem from the discretization of Hamiltonian NODEs, and enjoy non-vanishing gradients \emph{by design} if {symplectic} discretization methods \cite{hairer2006geometric} are used \cite{galimberti2023hamiltonian}. Moreover, the expressivity of HDNNs has been demonstrated empirically on several benchmarks in classification tasks. Nevertheless, the theoretical foundation on the UAP of HDNNs has yet to be explored.  


\subsection{Contributions}
In this paper, we present a rigorous theoretical framework to prove a UAP of HDNNs. First, with a slight modification, we generalize the class of HDNNs considered in \cite{galimberti2023hamiltonian} without compromising the provable non-vanishing gradients property\footnote{For the sake of simplicity, we retain the same name and also refer to the proposed modified version as HDNNs.}. 
Second, we prove that a portion of the flow of HDNNs can approximate any continuous function with arbitrary accuracy. The proof is based on three essential features \emph{i.e.} symplectic discretization through the Semi-Implicit Euler (SIE) method, a careful choice of initial conditions, and an appropriate selection of the flow. 
It is important to note that general DNNs, such as deep Multi-Layered Perceptrons (MLPs) or recurrent NNs, can suffer from vanishing gradients and might fail to approximate arbitrary functions if the training stops early. 
Third, since DNNs arising from the discretization of ODEs are automorphic maps -- they do not alter the dimension of the input data -- based on the composition of functions, we
extend the main result to approximate maps, where the dimensions of domain and co-domain are different. 
Finally, we provide a characterization of the approximation error with respect to the depth.

{\it Organization:} Section \ref{sec:preliminaries} provides preliminaries on Hamiltonian NODEs, the employed discretization scheme, definitions of UAPs, and the problem formulation. In Section \ref{sec:main}, we prove the UAP for HDNNs (Theorem \ref{th:UAP_hamiltonian}), we investigate the case when the desired function is not an automorphic map (Corollary \ref{cor:problem2}), and provide some remarks on the approximation error (Proposition \ref{prop:approx}). Finally, some conclusions are provided in Section \ref{sec:conslusions}.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notation}
We denote the set of non-negative reals with $\Real_+$. For a vector $x \in \mathbb{R}^n$, its 2-norm is represented by $\| x \|$ and its 1-norm $\|x\|_1 := \sum_j |x_j|$. Given an $\L_2$-function $f:\Rn \rightarrow \Rn$ the $\L_2$ norm over the compact set $\Omega \subset \Real^n$ is denoted by $\|f\|_{\L_2(\Omega)}$ and the (essential) supremum norm by $\|f\|_{\L_\infty(\Omega)} = \sup_{x\in\Omega} \|f(x)\|$.  $|A|$ stands for the determinant of a squared matrix $A$. We represent with $0_n$ the zero vector in $\Rn$ and with $0_\ntn$ the matrix with all entries equal to zero in $\Real^\ntn$. We denote the column vector of ones of dimension $n$ with $\mathds{1}_n$. Given $\Omega \subset \Real^n$, $\C(\Omega;\Real^n)$ stands for the space of continuous functions $f: \Omega \to \Rn$. Given $T\in \Real_+$, we refer to $\P([0,T];\Real^p)$ as the space of piecewise constant function $\theta : [0,T] \to \Real^p$. Functions that cannot be represented in the form of a polynomial are referred to as \emph{non-polynomial} functions.
    
\section{Preliminaries and problem formulation}\label{sec:preliminaries}
%-----------------------------------------------------------
\subsection{Hamiltonian Neural Ordinary Differential Equations}\label{subsec:HNODE}
%-----------------------------------------------------------
A Neural ODE \cite{chen2018neural} (NODE) is represented by the dynamical system for $t\in [0, T]$ given by
\begin{align}
  \label{eq:NODE}
  \dot{x}(t)=F(x(t), \theta(t)) \quad \textnormal{with} \ x(0)= x_0 \in \Omega \;,
\end{align}
where $x(t)\in \mathbb{R}^n$ is the state at time $t$ of the NODE with initial condition $x_0$ in some compact set $\Omega \in \Rn$, and $F: \Rn \times \Rp \to \Rn$ is such that $F(x, \theta)$ is Lipschitz continuous with respect to $x$ and measurable with respect to the weights $\theta$. We further assume that $\theta(t) \in \P([0,T];\Real^p)$.
When used in machine learning tasks, the NODE is usually pre- and post- pended with additional layers, \emph{e.g.,} $x_0 = \varphi_{\alpha}(z)$, with $z \in \mathbb{R}^{n_z}$ the input and $\varphi_{\alpha}$ a NNs with parameters $\alpha\in \mathbb{R}^{n_\alpha}$, and the output $y$ is computed as $y=\phi_{\beta}(x(T))$, where $\phi_{\beta}$ is a NNs with parameters $\beta\in \mathbb{R}^{n_\beta}$.

In this paper, we consider a class of NODEs inspired by Hamiltonian systems. In particular, we consider the Hamiltonian function $H: \Rtn \times \Realp \to \Real$ given by 
\begin{equation}\label{eq:Hamiltonian}
    H(x,t) = \tilde{\sigma}( W(t)x + b(t))^\top \mathds{1}_n + \eta(t)^\top\, x\;,
\end{equation}
where $W: \Realp \to \Real^{2n\times 2n}$, $b: \Realp \to \Rtn$, $\eta : \Realp \to \Rtn$ are piece-wise constant, while $\tilde{\sigma} : \Real \to \Real$  is a differentiable map, applied element-wise when the argument is a matrix, and such that $\sigma(x):= \frac{\partial \tilde\sigma}{\partial x}(x)$ is non-polynomial and Lipschitz continuous. As explained below, $\sigma$ will play the role of the so-called \emph{activation function}.
Examples that satisfy the above assumptions are provided in Table \ref{tab:act}. Note that if we set $\eta(t) = 0$ in \eqref{eq:Hamiltonian}, we recover DNNs proposed in \cite{haber2017stable,chang2019antisymmetricrnn,chang2018reversible,galimberti2023hamiltonian}.
We define the Hamiltonian system 
\begin{align}\label{eq:Hamiltonian_dynamics_general}
  \dot{x}(t)=J(t) \frac{\partial H (x(t),t)}{\partial x} \;,
\end{align}
where $J(t)$ is piecewise constant skew-symmetric matrix, namely $J(t)=-J(t)^\top$, in $\Rtn\times \Rtn$ for any $t\geq 0$. By taking into account the expression of the Hamiltonian in \eqref{eq:Hamiltonian}, we obtain the following dynamics
\begin{align}\label{eq:Hamiltonian_dynamics}
  \dot{x}(t)=J(t)\left(W(t)^\top \sigma\big(W(t)x(t) + b(t)\big)+ \eta(t)\right) \;.
\end{align}
Note that the latter equation can be written in the form \eqref{eq:NODE}, when the weights are given by $\theta(t) = \{J(t), W(t),b(t),\eta(t)\}$ for $t\in [0,T]$.\\
For the numerical implementation of NODE \eqref{eq:Hamiltonian_dynamics}, we rely on the SIE discretization \cite{hairer2006geometric} because it can preserve the symplectic flow of time-invariant Hamiltonian systems and is crucial to prove non-vanishing gradient property of the resulting HDNNs (further details will be given in the next section).
In particular, splitting the state of the Hamiltonian systems into $x=(p,q)$, we obtain the HDNN 
\begin{equation}
\label{eq:Hamiltonian_dynamics_general_SIE}
\begin{aligned}
{\left[\begin{array}{c}
{p}_{j+1} \\
{q}_{j+1}
\end{array}\right] } & =\left[\begin{array}{c}
{p}_j \\
{q}_j
\end{array}\right]+ h J_j\left[\begin{array}{c}
\frac{\partial H}{\partial {p}}\left({p}_{j+1}, {q}_j, t_j\right) \\
\frac{\partial H}{\partial {q}}\left({p}_{j+1}, {q}_j, t_j\right)
\end{array}\right],
\end{aligned}
\end{equation}
where $h = T\slash N$, with $N \in \Nat$, is the integration step-size, $j=0,\dots,N-1$ and $p_j$ and $q_j$ are the two state components in $\Real^{n}$. Moreover, by taking into account the expression of the Hamiltonian in \eqref{eq:Hamiltonian}, namely the dynamics \eqref{eq:Hamiltonian_dynamics}, we obtain the following difference equation
\begin{equation}
\label{eq:Hamiltonian_dynamics_SIE}
\begin{aligned}
& {\left[\begin{array}{c}
{p}_{j+1} \\
{q}_{j+1}
\end{array}\right] }  =  \left[\begin{array}{c}
{p}_j \\
{q}_j
\end{array}\right] \\ 
&\qquad + h J_j \left( W_j^{\top} \sigma\left({W}_j\left[\begin{array}{c}
{p}_{j+1} \\
{q}_j
\end{array}\right]+{b}_j\right)  +  \eta_j \right).
\end{aligned}
\end{equation}
Clearly, the set of weights is given by $\theta_j = \{J_j, W_j,b_j,\eta_j\}$ with $j=0,\dots,N-1$. With a little abuse of notation we write $\theta_j \in \Rp$ with $j=0,\dots,N-1$ and appropriate $p\in \Nat$.
Although, in general, one has to compute the update $(p_{j+1},q_{j+1})$ of \eqref{eq:Hamiltonian_dynamics_SIE} through an implicit expression (see for instance \cite{bai2019deep,davydov2022comparative} for NN with implicit layers), it is possible to rewrite it in an explicit form, when the matrices $J_j$ and $W_j$ satisfy some assumptions, \emph{e.g.}, by choosing $J_j$ block anti-diagonal and $W_j$ block diagonal~\cite{galimberti2023hamiltonian}. 

%------------------------------------------------------------------
\subsection{Universal Approximation Property}
%------------------------------------------------------------------
In this section, we present some essential definitions pertaining to universal approximation properties.
\begin{definition} [UAP of a function]\label{def:UAPmaps}
Consider a function $g_\theta: \Real^n \to \Real^n$ with parameters $\theta\in \Rp$ and a compact subset $\Omega \in \Real^n$, then $g_\theta$ has the Universal Approximation Property (UAP) on $\Omega\subset \Real^n$ if for any $f\in \C(\Omega;\Real^n)$ and $\varepsilon>0$, there exists $\theta \in \Rp$ such that
\begin{equation} \label{eq:UAP_scalar}
\sup_{x\in\Omega} \|f(x) - g_\theta(x)\| \leq \varepsilon \; .
\end{equation}
\end{definition}

We provide the following important fact, whose proof is in Appendix \ref{app:Cybenkosum}.
\begin{proposition}\label{prop:UAPmaps}
% Let $\sigma\in \C(\Real;\Real)$ be non-polynomial and $\Omega \subset \Real^n$ be compact. Then, the function  $g:\Real^ \to \Real$ given by
% \begin{equation}\label{eq:Cybenko_sum}
%     g(x):= \sum_{j = 0}^{N-1} \alpha_j \,\sigma(w_j^\top x + \beta_j) \;,
% \end{equation}
% with parameters $N\in\Nat$, $\alpha_j, \beta_j \in \Real$ and $w_j \in \Rn$ for $j = 0,\dots,N-1$, has the UAP on $\Omega$.  \qquad\qquad\qquad\qquad\qquad\QEDclosed
Let $\sigma\in \C(\Real;\Real)$ be non-polynomial, then for any $f\in \C(\Omega;\Rn)$, where $\Omega \in \Real^n$, and $\varepsilon>0$, there exist $N\in\Nat$, $A_j,W_j \in \Real^{n \times n}$ and  $b_j \in \Rn$ such that the function $g:\Real^n \to \Rn$ given by
\begin{equation}\label{eq:Cybenko_sum_matrix}
    g(x):= \sum_{j = 0}^{N-1} A_j \,\sigma(W_j x + b_j) \;,
\end{equation} 
satisfies
\begin{equation} \label{eq:UAP_vector}
\sup_{x\in\Omega} \|f(x) - g(x)\| \leq \varepsilon \;.
\end{equation}
\end{proposition}
Some examples of activation functions $\sigma$, such that $g$ in  \eqref{eq:Cybenko_sum_matrix} satisfies the UAP, are given in Table \ref{tab:act}.
\begin{table}[]
    \centering
    \begin{tabular}{c|c}
    \hline
    Activation Function  & $\sigma(x)$ \\
    \hline 
     ReLU    &  $\max \{x,0 \}$\\
     Sigmoidal  & $(1 + \exp(-x))^{-1}$ \\
     Softplus    & $\log (1 + \exp(x)) $ \\
     Hyperbolic Tangent & $\tanh(x)$ \\
     Radial Basis Function & $\frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2})$ \\
     \hline
    \end{tabular}
    \caption{Examples of activation functions.}
    \label{tab:act}
%\vspace{-0.6cm}
\end{table}

% \begin{definition} [UAP of a flow]\label{def:UAPflows}
% Let $F: \Real^n \times \Real^p \to \Real^n$ be a parameter-dependent vector field, and let $\Phi : \Omega \times [0,T] \times \P([0,T]; \Real^p) \to \Real^n$, with $\Omega \subset \Real^n$ compact and $T\in \Real_+$, be the flow of $F$, namely $\Phi(x_0,t,\theta_{[0,t]})$ is the unique solution at time $t\leq T$ to \eqref{eq:NODE}.
% % \begin{align}
% %   \label{eq:NODE2}
% %   \dot{x}(t)=f(x(t), \theta(t)), \quad x(0)= x_0 \in \Omega,
% % \end{align}
% % with $\theta \in \P([0,T]; \Rp)$. 
% Then the flow $\Phi$ has the Universal Approximation Property (UAP) on $\Omega$ if for any $f\in C(\Omega;\Real^n)$ and $\varepsilon>0$, there exist $T>0$ and parameters $\theta \in \P([0,T]; \Rp)$ such that the function $\varphi: \Omega \to \mathbb{R}^n$ given by $\varphi(x) := \Phi(x,T,\theta_{[0,T]})$  satisfies
% \begin{equation}\label{eq:UAP_flow}
% \sup_{x\in\Omega} \|f(x) - \varphi(x)\| \leq \varepsilon \;.
% \end{equation}
% \end{definition}

% \ 

% In other words, $\Phi$ has the UAP on $\Omega$ if the set
% \begin{align}\label{eq:setS_flow}
%     \mathcal{S} &= \{ \varphi: \Omega \to \mathbb{R}^n \ |\ \varphi(x) = \Phi(x,T,\theta_{[0,T]}), \nonumber \\ & \qquad\qquad\qquad \textnormal{with} \ T\in \Realp, \, \theta \in \P([0,T];\Rp)\}
% \end{align}
% is dense in $\C(\Omega;\Rn)$. 
% It is clear that a straightforward equivalent discrete-time counterpart of Definition \ref{def:UAPflows} can be given when the flow $\Phi$ is the solution to a difference equation (for instance \eqref{eq:Hamiltonian_dynamics_general_SIE}). Also, 
In the sequel, we refer to the UAP with bound $\varepsilon>0$ to quantify the estimation error in equations \eqref{eq:UAP_scalar}, and \eqref{eq:UAP_vector}. This value is typically a function of $N$, $n$, and the desired $f$, and it is  characterized in Proposition \ref{prop:approx}.
%---------------------------------------------
\subsection{Problem formulation}\label{subsec:problem_formulation}
%---------------------------------------------
The goal of our paper can be formulated as follows.
{\it Problem 1: Let $f: \Rn \to \Rn$ be a continuous function, $\Omega \subset \Rn$ be a compact set, and $\varepsilon>0$ be the desired approximation accuracy. Find $N\in\Nat$ and weights $\theta_j = \{J_j, W_j,b_j,\eta_j\}$ with $j=0,\dots,N-1$ of \eqref{eq:Hamiltonian_dynamics_SIE}, such that a portion $\varphi : \Rn \to \Rn$ of the flow $\Phi_N : \Real^{2n} \to \Real^{2n}$ at time $N\in\Nat$ of  \eqref{eq:Hamiltonian_dynamics_SIE} has the UAP on $\Omega$.}\\
We recall that the flow at time $N\in\Nat$ of \eqref{eq:Hamiltonian_dynamics_SIE} is the corresponding unique solution at time $N\in\Nat$. In particular, $\Phi_N : \Real^{2n} \to \Real^{2n}$ is the flow at time $N$ as function of the initial condition. The flow $\varphi$ will be precisely defined in Theorem \ref{th:UAP_hamiltonian}.

Moreover, motivated by real-world applications, we are also interested in approximating arbitrary continuous functions $f: \mathbb{R}^n \rightarrow \mathbb{R}^r$ where $r$ is not necessarily equal to $n$. For instance, in classification tasks, typically $r < n$, as $r$ corresponds to the number of classes to be classified and $n$ represents the number of features.
%\begin{problem}\label{prob:p2}
%Let $f: \Rn \to \mathbb{R}^{r}$ be a continuous function, where $r < n$, $\Omega \subset \Rn$ be a %compact set, and $\varepsilon>0$ be the desired approximation accuracy. Find $N\in\Nat$, parameters %$\theta_j = \{J_j, W_j,b_j,\eta_j\}$ with $j=0,\dots,N-1$ of \eqref{eq:Hamiltonian_dynamics_SIE} and a %Lipschitz continuous function $h:\Rn \to \mathbb{R}^{r}$, such that the composition $h \circ \varphi$, where  $\varphi : \Rn \to \Rn$ is a flow  related to \eqref{eq:Hamiltonian_dynamics_SIE},  has the UAP on $\Omega$.
%\end{problem}
We address this problem in Corollary \ref{cor:problem2}.


%---------------------------------------------------
\section{Main Results}\label{sec:main}
%---------------------------------------------------
In this section, we present our main results whose proofs are given the Appendix.
We address the Problem 1 in Theorem \ref{th:UAP_hamiltonian}, which is a universal approximation theorem for the HDNN \eqref{eq:Hamiltonian_dynamics_general_SIE}. 
% Secondly, we address the case in which the desired function $f$ is not an automorphism in Corollary \ref{cor:problem2}, and finally, we discuss a bound on the desired accuracy of the approximation error. 

\begin{theorem} \label{th:UAP_hamiltonian}
Consider the discrete-time system \eqref{eq:Hamiltonian_dynamics_SIE} with initial condition $(p_0,q_0) = (\xi,0_n)$, for some $\xi \in \Omega$ with $\Omega \subset \Rn$ compact. Then, the \emph{restricted} flow $\varphi : \xi \mapsto q_N$ has the UAP on $\Omega$.
\end{theorem}

In other words, Theorem \eqref{th:UAP_hamiltonian} states that given  the system \eqref{eq:Hamiltonian_dynamics_SIE} with initial condition $(p_0,q_0) = (\xi,0_n)$, for any $f\in C(\Omega;\Real^n)$ and $\varepsilon>0$, there exist $N\in \Nat$ and weights $\theta_j = \{J_j, W_j,b_j,\eta_j\}$ with $j=0,\dots,N-1$ such that the function $\varphi: \xi \mapsto q_N$  satisfies
\begin{equation}\label{eq:app_xi}
    \sup_{\xi\in\Omega} \|f(\xi) - \varphi(\xi)\| < \varepsilon \;.
\end{equation}
% namely the set $\mathcal{S} =  \{\varphi : \Omega \to \Real^n \ |\  \varphi : \xi \mapsto q_{N}   \  \textnormal{with} \ N \in \mathbb{N} \ \textnormal{and} \  \theta_j \in \Real^p \ \textnormal{for all} \  j=1,\dots,N-1\}$ is dense in $\C( \Omega; \Real^n)$.


\begin{remark}[Key ingredients for UAP]
The proof of Theorem \ref{th:UAP_hamiltonian}, besides exploiting arguments from \cite{cybenko1989approximation,hornik1989multilayer} for showing UAPs, it is based on three critical key steps: \emph{i)} the SIE discretization scheme, \emph{ii)} the initial condition $(p_0,q_0) = (\xi,0_n)$, and \emph{iii)} the focus on the \emph{restricted} flow $\xi \mapsto q_N$, which refers to map the initial condition of the $p$ state to the flow of the $q$ state.
\end{remark}

In particular, the choice of the SIE discretization scheme together with the initial condition $(p_0,q_0) = (\xi,0_n)$ allows one to exploit the framework of Cybenko \cite{cybenko1989approximation}  to express the function $\varphi : \xi \mapsto q_N$ as \eqref{eq:Cybenko_sum_matrix} (see equation \eqref{eq:def_varphi} in the Proof of Theorem \ref{th:UAP_hamiltonian} in Appendix \ref{app:th_main}).





% \color{blue}
% It is worth mentioning that an SNN \eqref{eq:Cybenko_sum} is a subset of HDNN with some sparse choices of weight matrices. Suppose our goal is to approximate a desired scalar function $f \in \C(\Omega, \Real)$, then consider the following parametrization for HDNN \eqref{eq:Hamiltonian_dynamics_SIE} 
% \begin{equation}\label{eq:parametrization_structure}
% \begin{aligned}
% J_j &= \begin{bmatrix} 0_\ntn & -D_j \\ D_j & 0_\ntn 
% \end{bmatrix}, \qquad  W_j = \begin{bmatrix} \tilde{Y}_j & 0_\ntn \\ 0_\ntn & 0_\ntn
% \end{bmatrix}, \\  
% b_j &= \begin{bmatrix} \bt_j \\ 0_{n}  \end{bmatrix},\qquad\qquad\quad\quad \ \  \eta_j = 0_n
% \end{aligned}
% \end{equation}
% where $D_j$, $\tilde{Y_j}$ are diagonal matrices. Then \eqref{eq:Hamiltonian_dynamics_SIE} can be written as
% \begin{equation}
% \begin{aligned}
% {\left[\begin{array}{c}
% {p}_{j+1} \\
% {q}_{j+1}
% \end{array}\right] } & = \left[\begin{array}{c}
% {p}_j  \\
% {q}_j 
% \end{array}\right]  + h \left[\begin{array}{c}
%  0_n \\
% D \tilde{Y}_{j} \sigma(\tilde{Y}_{j}p_{j+1} + \bt_j )
% \end{array}\right]
% \end{aligned}
% \end{equation}
% By substituting, the forward equation of $p_{j}$ and using the initial condition $(x_0,0_n), x_0 \in \Omega $, and for $j = N$, we obtain
% \begin{equation} \label{eq:int_stuff}
%     q_{N} = \sum_{j = 1}^N \alpha_j \sigma(\tilde{y}_j^\top x_0 + \tilde{b}_j)\;,
% \end{equation}
% which can be exactly written as \eqref{eq:Cybenko_sum} proving that a single-layered HDNN is equivalent to an SNN.  
% \color{black}
\begin{remark}[Feature augmentation]
By defining the flow $\Phi_N$ of the discrete-time system \eqref{eq:Hamiltonian_dynamics_SIE} (evolving in $\Real^{2n}$), we note that \eqref{eq:app_xi} can be written as
\begin{equation} 
\sup_{x\in\Omega} \|f(x) - \pi \circ \Phi_N \circ \iota (x) \| \leq \varepsilon\;,
\end{equation}
where $\iota: \Real^{n} \to \Real^{2n}$ is the injection given by $\iota(z_1,\dots,z_n) = (z_1,\dots,z_n,0,\dots,0)$ and $\pi : \Real^{2n} \to \Real^n$ is the projection $\pi(x_1,\dots,x_n,x_{n+1},\dots, x_{2n}) = (x_{n+1},\dots,x_{2n})$.
This is equivalent to the common practice in machine learning of augmenting the size of the feature space \cite{dupont2019augmented}. 
It has been demonstrated that this technique can improve DNN performance in several learning tasks. 
Moreover, it is also closely related to the idea of extended space \cite{goodfellow2016deep}, which suggests that by increasing the dimensionality of the feature space, one can capture more complex relationships.
\end{remark}


%------------------------------------------------------
%------------------------------------------------------
%------------------------------------------------------
%------------------------------------------------------
% \begin{theorem} Given the matrix $J = \begin{bmatrix}
%     0_{n} & -X^\top \\ X & 0_{n}
% \end{bmatrix}$, where $X \in \mathbb{R}^{n  \times n}$. Let Assumptions \ref{assum:lip} and \ref{assump:hamiltonian} hold. Then, the set of flows $\mathcal{S}({\Omega}) :=  \{[\xi \mapsto q_{N-1}]  \in \C(\Omega:\mathbb{R}^{n})|  \textnormal{with} \in \mathbb{N}\}$ of the neural ode
% \begin{equation}
% \label{hamiltonian}
%     \begin{bmatrix}
%         \dot{p} \\ \dot{q}
%     \end{bmatrix} = J \begin{bmatrix}
%         \frac{\partial H}{\partial p} \\ 
%         \frac{\partial H}{\partial q}
%     \end{bmatrix}, t \in [0, \ T], 
%     \begin{bmatrix}
%         p(0) \\ q(0)
%     \end{bmatrix} =    \begin{bmatrix}
%         \xi \\ 0    \end{bmatrix} 
% \end{equation} 
%  is {\it dense} in $\C( \Omega; \Real^n)$, where $\xi \in \mathbb{R}^{n}$ denotes the input data.
% % \begin{equation}
% %     \mathcal{S}({D}):= \{[\xi \mapsto q_N]  \in \mathcal{C}(\mathcal{D}:\mathbb{R}^{n/2})| N \in \mathbb{N}\} \;.
% % \end{equation}
% \end{theorem}
% \begin{proof}
% Assume that the number of features $2n \in \mathbb{N}$ is even
% and split the feature vector as $x = (p,q)$ 
% where $p,q \in \mathbb{R}^{n}$. Further, assume that $J$ does not vary across layers. Then, S-IE discretization of \eqref{hamiltonian} with a step-size $h = T/N$ leads to the following layer equation
% \begin{equation}
% \label{disc_sys}
% \begin{aligned}
% {\left[\begin{array}{c}
% {p}_{j+1} \\
% {q}_{j+1}
% \end{array}\right] } & =\left[\begin{array}{c}
% {p}_j \\
% {q}_j
% \end{array}\right]+h {J}\left[\begin{array}{c}
% \frac{\partial H}{\partial {p}}\left({p}_{j+1}, {q}_j, t_j\right) \\
% \frac{\partial H}{\partial {q}}\left({p}_{j+1}, {q}_j, t_j\right)
% \end{array}\right] \\
% & =\left[\begin{array}{c}
% {p}_j \\
% {q}_j
% \end{array}\right]+h {J} \left( W_j^{\top} \sigma\left({W}_j\left[\begin{array}{c}
% {p}_{j+1} \\
% {q}_j
% \end{array}\right]+{b}_j\right)  +  \eta_j \right)  \\ 
% & = \left[\begin{array}{c}
% {p}_j  \\
% {q}_j 
% \end{array}\right]  + h \left[\begin{array}{c}
%   X^\top \eta_j  \\
% X \tilde{W}_{j}^\top \sigma(\tilde{W}_{j}p_{j+1})
% \end{array}\right]  \\
% & = \left[\begin{array}{c}
% {p}_j  \\
% {q}_j 
% \end{array}\right]  + \left[\begin{array}{c}
%  \gamma_j  \\
% A_j \sigma(\tilde{W}_{j}p_{j+1})
% \end{array}\right] 
% \end{aligned}
% \end{equation}
% for $j = 0, 1, \cdots, N-1$, where $\gamma_j = X^\top \eta_j$, and $A_j = X \tilde{W}_{j}^\top$, respectively. 

% Since $\sigma \in \mathcal{C}(\mathbb{R}^{n}; \mathbb{R}^{n})$ and satisfies Definition \ref{def:UAPmaps}; that is, given an arbitrary function $f \in \mathcal{C}(\Omega;\mathbb{R}^{n})$ and a small $\varepsilon > 0$, there exist a positive integer $N$, $\mathbb{R}^{n}$-valued vectors $d_j$ and matrices $A_j, \tilde{W}_j \in \mathbb{R}^{n \times n}$, for all $j = 0,\cdots,N-1$, such that
% \begin{align}
%     g(\xi) = \sum_{j = 0}^{N-1}A_j \sigma(\tilde{W}_j \xi+ d_j) \nonumber  \\ 
%     |g(\xi) - f(\xi)| < \varepsilon
% \end{align}
% for any $\xi \in \Omega$ \cite{cybenko1989approximation,hornik1989multilayer}. Let us define $d_j = \tilde{W}_{j} r_j$ to obtain
% \begin{equation}
%      g(\xi) = \sum_{j = 0}^{N-1}A_j \sigma(\tilde{W}_{j}(\xi + r_j) ) \nonumber  \;.
% \end{equation}
% Furthermore, define 
% \begin{align*}
%     p_{j+1} =  \xi + r_j, \ q_j := \sum_{i = 0}^{j} A_j \sigma(\tilde{W}_{j} p_{j+1})  \;,\\
%  \gamma_j := r_j - r_{j-1}  \;,
% \end{align*}
% for all $j = 0,\cdots,N-1$. Setting  $r_0 = \gamma_0$, $q_0 = 0$, and $p_0 = \xi$ we obtain 
% \begin{align*}
%     p_{j+1} - p_{j} =&  r_j - r_{j-1} =  \gamma_j  \;.
% \end{align*}
% For the second part of the equation \eqref{disc_sys}, we can define 
% \begin{equation}
%     q_{N-1} = \sum_{j = 0}^{N-1} A_j \sigma(\tilde{W}_{j}( \xi + r_j)) = g(\xi) \;.
% \end{equation}
% Therefore, $[\xi \mapsto q_{N-1}] \in \mathcal{S}(\Omega)$. Hence, given $f \in \mathcal{C}(\Omega;\mathbb{R}^n)$ and $\varepsilon > 0$, there exist $N \in \mathbb{N}$, $A_j \in \mathbb{R}^{n \times n}$,  and $\gamma_j \in \mathbb{R}^n$ for $j = 0, \cdots, N-1$ such that 
% \begin{equation}
%     |q_{N-1} - f(\xi)| < \varepsilon
% \end{equation}
% for any $\xi \in \Omega$. 

%------------------------------------------------------
%------------------------------------------------------
%------------------------------------------------------
%------------------------------------------------------
%------------------------------------------------------
%------------------------------------------------------
% Define a function $H:= [\xi \mapsto q_N]:\mathcal{D} \rightarrow \mathbb{R}^{n/2}$ that is the flow of the system.
%\end{proof}
% \begin{assumption}
% \label{assump:hamiltonian}
% The continuously differentiable Hamiltonian function $H: \mathbb{R}^{2n} \mapsto \mathbb{R}$ is parameterized as 
% \begin{equation*}
%     H(x) := \tilde{\sigma}(Wx + b) + x^\top M x + \eta^\top x \;,
% \end{equation*}
% where $W := \begin{bmatrix} W_{11} & 0_{n} \\ 0_{n} & 0_{n} 
% \end{bmatrix}$, $W_{11} \in \mathbb{R}^{n \times n}$, $M := \begin{bmatrix} M_{11} & 0_{n} \\ 0_{n} & 0_{n} 
% \end{bmatrix}$, $M_{11} \in \mathbb{R}^{n \times n}$, $b := \begin{bmatrix}
%     0 \\ b_2
% \end{bmatrix}$, $b_2 \in \Real^n$, and $\eta = \begin{bmatrix}
%     \eta_1 \\ 0
% \end{bmatrix}$, $\eta_1 \in \Real^n$, respectively.
% \end{assumption}
% \begin{theorem} Given the matrix $J = \begin{bmatrix}
%     0_{n} & -X^\top \\ X & 0_{n}
% \end{bmatrix}$, where $X \in \mathbb{R}^{n  \times n}$. Let Assumptions \ref{assum:lip} and \ref{assump:hamiltonian} hold. Then, the set of flows $\mathcal{S}({\Omega}) :=  \{[\xi \mapsto q_{N-1}]  \in \C(\Omega:\mathbb{R}^{n})| N \in \mathbb{N}\}$ of the neural ode
% \begin{equation}
% \label{hamiltonian}
%     \begin{bmatrix}
%         \dot{p} \\ \dot{q}
%     \end{bmatrix} = J \begin{bmatrix}
%         \frac{\partial H}{\partial p} \\ 
%         \frac{\partial H}{\partial q}
%     \end{bmatrix}, t \in [0, \ T], 
%     \begin{bmatrix}
%         p(0) \\ q(0)
%     \end{bmatrix} =    \begin{bmatrix}
%         \xi \\ 0    \end{bmatrix} 
% \end{equation} 
%  is {\it dense} in $\C( \Omega; \Real^n)$, where $\xi \in \mathbb{R}^{n}$ denotes the input data.
% % \begin{equation}
% %     \mathcal{S}({D}):= \{[\xi \mapsto q_N]  \in \mathcal{C}(\mathcal{D}:\mathbb{R}^{n/2})| N \in \mathbb{N}\} \;.
% % \end{equation}
% \end{theorem}
% \begin{proof}
% Assume that the number of features $2n \in \mathbb{N}$ is even
% and split the feature vector as $x = (p,q)$ 
% where $p,q \in \mathbb{R}^{n}$. Further, assume that $J$ does not vary across layers. Then, S-IE discretization of \eqref{hamiltonian} with a step-size $h = T/N$ leads to the following layer equation
% \begin{equation}
% \label{disc_sys}
% \begin{aligned}
% {\left[\begin{array}{c}
% {p}_{j+1} \\
% {q}_{j+1}
% \end{array}\right] } & =\left[\begin{array}{c}
% {p}_j \\
% {q}_j
% \end{array}\right]+h {J}\left[\begin{array}{c}
% \frac{\partial H}{\partial {p}}\left({p}_{j+1}, {q}_j, t_j\right) \\
% \frac{\partial H}{\partial {q}}\left({p}_{j+1}, {q}_j, t_j\right)
% \end{array}\right] \\
% & =\left[\begin{array}{c}
% {p}_j \\
% {q}_j
% \end{array}\right]+h {J} \left( W_j^{\top} \sigma\left({W}_j\left[\begin{array}{c}
% {p}_{j+1} \\
% {q}_j
% \end{array}\right]+{b}_j\right) \right.\\
% &+\left. M_j\left[\begin{array}{c}
% {p}_{j+1} \\
% {q}_j
% \end{array}\right] +  \eta_j \right)  \\ 
% & = \left[\begin{array}{c}
% {p}_j  \\
% {q}_j 
% \end{array}\right]  + h \left[\begin{array}{c}
%  X^\top M_{11,j} p_{j+1} + X^\top \eta_j  \\
% X W_{11,j} \sigma(W_{11}p_{j+1})
% \end{array}\right]  \\
% & = \left[\begin{array}{c}
% {p}_j  \\
% {q}_j 
% \end{array}\right]  + \left[\begin{array}{c}
% B_j p_{j+1} + \gamma_j  \\
% A \sigma(W_{11}p_{j+1})
% \end{array}\right] 
% \end{aligned}
% \end{equation}
% for $j = 0, 1, \cdots, N-1$, where $B_j = X^\top M_{11,j}$, $\gamma_j = X^\top \eta_j$, and $A = XW_{11}$, respectively. 

% Since $\sigma \in \mathcal{C}(\mathbb{R}^{n}; \mathbb{R}^{n})$ and satisfies Definition \ref{def:UAPmaps}; that is, given an arbitrary function $f \in \mathcal{C}(\Omega;\mathbb{R}^{n})$ and a small $\varepsilon > 0$, there exist a positive integer $N$, $\mathbb{R}^{n}$-valued vectors $d_j$ and matrices $A, C_j \in \mathbb{R}^{n \times n}$, for all $j = 0,\cdots,N-1$, such that
% \begin{align}
%     g(\xi) = \sum_{j = 0}^{N-1}A \sigma(C_j \xi+ d_j) \nonumber  \\ 
%     |g(\xi) - f(\xi)| < \varepsilon
% \end{align}
% for any $\xi \in \Omega$. Let the $rank(C_j) = n$ for all $j = 0,\cdots,N-1$ and the $sign( \text{det} W_{11} ) = sign(\text{det} C_j)$ Moreover, there exists $P_j \in \mathbb{R}^{n \times n}$ such that $\text{det} P_j > 0$ and $C_j = W_{11}P_j$, for each $j = 0,\cdots,N-1$. Without loss of generality, let $d_j = W_{11} r_j$ and obtain 
% \begin{equation}
%      g(\xi) = \sum_{j = 0}^{N-1}A \sigma(W_{11}(P_j \xi + r_j) ) \nonumber  
% \end{equation}
% Furthermore, define 
% \begin{align*}
%     p_{j+1} = P_j \xi + r_j, \ q_j := \sum_{i = 0}^{j} A \sigma(W_{11} p_{j+1})  \;,\\
%     B_j := (P_j - P_{j-1})(P_{j})^{-1} \;, \gamma_j := r_j - r_{j-1} - B_j r_{j-1} \;,
% \end{align*}
% for all $j = 0,\cdots,N-1$. Setting $P_0 = I_n$, and $r_0 = 0$, we obtain 
% \begin{align*}
%     p_{j+1} - p_{j} =& (P_j - P_{j-1})\xi + r_j - r_{j-1} \\ 
%      =& (P_j - P_{j-1})\xi + \gamma_j + B_j r_{j}  \\
%      =& B_jP_{j} \xi + \gamma_j + B_j r_{j} \\
%      =& B_j (P_{j} \xi + r_{j}) + \gamma_j \\
%      =& B_j p_{j+1} + \gamma_j
% \end{align*}
% For the second part of the equation \eqref{disc_sys}, we can define 
% \begin{equation}
%     q_{N-1} = \sum_{j = 0}^{N-1} A \sigma(W_{11}(P_j \xi + r_j)) = g(\xi) \;.
% \end{equation}
% Therefore, $[\xi \mapsto q_{N-1}] \in \mathcal{S}(\Omega)$. Hence, given $f \in \mathcal{C}(\Omega;\mathbb{R}^n)$ and $\varepsilon > 0$, there exist $N \in \mathbb{N}$, $A \in \mathbb{R}^{n \times n}$,  $ B_j \in \mathbb{R}^{n \times n}$,  and $\gamma_j \in \mathbb{R}^n$ for $j = 0, \cdots, N-1$ such that 
% \begin{equation}
%     |q_{N-1} - f(\xi)| < \varepsilon
% \end{equation}
% for any $\xi \in \Omega$. 
% % Define a function $H:= [\xi \mapsto q_N]:\mathcal{D} \rightarrow \mathbb{R}^{n/2}$ that is the flow of the system.
% \end{proof}


% \textcolor{blue}{
% \begin{proof}
% Assume that the number of features $2n \in \mathbb{N}$ is even
% and split the feature vector as $x = (p,q)$ 
% where $p,q \in \mathbb{R}^{n}$. Further, assume that $J$ does not vary across layers. Then, S-IE discretization of \eqref{hamiltonian} with a step-size $h = T/N$ leads to the following layer equation
% \begin{equation}
% \label{disc_sys}
% \begin{aligned}
% {\left[\begin{array}{c}
% {p}_{j+1} \\
% {q}_{j+1}
% \end{array}\right] } & =\left[\begin{array}{c}
% {p}_j \\
% {q}_j
% \end{array}\right]+h {J}\left[\begin{array}{c}
% \frac{\partial H}{\partial {p}}\left({p}_{j+1}, {q}_j, t_j\right) \\
% \frac{\partial H}{\partial {q}}\left({p}_{j+1}, {q}_j, t_j\right)
% \end{array}\right] \\
% & =\left[\begin{array}{c}
% {p}_j \\
% {q}_j
% \end{array}\right]+h {J} \left( W_j^{\top} \sigma\left({W}_j\left[\begin{array}{c}
% {p}_{j+1} \\
% {q}_j
% \end{array}\right]+{b}_j\right) \right.\\
% &+\left. M_j\left[\begin{array}{c}
% {p}_{j+1} \\
% {q}_j
% \end{array}\right] +  \eta_j \right)  \\ 
% & = \left[\begin{array}{c}
% {p}_j  \\
% {q}_j 
% \end{array}\right]  + h \left[\begin{array}{c}
%  X^\top M_{22,j} q_{j} + X^\top \eta_j  \\
% X W_{11,j} \sigma(W_{11}p_{j+1} + W_{12}q_j)
% \end{array}\right]  \\
% & = \left[\begin{array}{c}
% {p}_j  \\
% {q}_j 
% \end{array}\right]  + \left[\begin{array}{c}
% B_j q_{j} + \gamma_j  \\
% A \sigma(W_{11}p_{j+1} + W_{12}q_j) 
% \end{array}\right] 
% \end{aligned}
% \end{equation}
% for $j = 0, 1, \cdots, N-1$, where $B_j = X^\top M_{22,j}$, $\gamma_j = X^\top \eta_j$, and $A = XW_{11}$, respectively. 
% Since $\sigma \in \mathcal{C}(\mathbb{R}^{n}; \mathbb{R}^{n})$ and satisfies Definition \ref{def:UAPmaps}; that is, given an arbitrary function $f \in \mathcal{C}(\Omega;\mathbb{R}^{n})$ and a small $\varepsilon > 0$, there exist a positive integer $N$, $\mathbb{R}^{n}$-valued vectors $d_j$ and matrices $A, C_j \in \mathbb{R}^{n \times n}$, for all $j = 0,\cdots,N-1$, such that
% \begin{align}
%     g(\xi) = \sum_{j = 0}^{N-1}A \sigma(C_j \xi+ d_j) \nonumber  \\ 
%     |g(\xi) - f(\xi)| < \varepsilon
% \end{align}
% for any $\xi \in \Omega$. Let the $rank(C_j) = n$ for all $j = 0,\cdots,N-1$ and the $sign( \text{det} W_{11} ) = sign(\text{det} C_j)$ Moreover, there exists $P_j \in \mathbb{R}^{n \times n}$ such that $\text{det} P_j > 0$ and $C_j = W_{11}P_j$, for each $j = 0,\cdots,N-1$. Without loss of generality, let $d_j = W_{11} r_j$ and obtain 
% \begin{equation}
%      g(\xi) = \sum_{j = 0}^{N-1}A \sigma(W_{11}(P_j \xi + r_j) ) \nonumber  
% \end{equation}
% Furthermore, define 
% \begin{align*}
%     p_{j+1} = P_j \xi + r_j, \ q_j := \sum_{i = 0}^{j} A \sigma(W_{11} p_{j+1})  \;,\\
%     B_j := (P_j - P_{j-1})(P_{j})^{-1} \;, \gamma_j := r_j - r_{j-1} - B_j r_{j-1} \;,
% \end{align*}
% for all $j = 0,\cdots,N-1$. Setting $P_0 = I_n$, and $r_0 = 0$, we obtain 
% \begin{align*}
%     p_{j+1} - p_{j} =& (P_j - P_{j-1})\xi + r_j - r_{j-1} \\ 
%      =& (P_j - P_{j-1})\xi + \gamma_j + B_j r_{j}  \\
%      =& B_jP_{j} \xi + \gamma_j + B_j r_{j} \\
%      =& B_j (P_{j} \xi + r_{j}) + \gamma_j \\
%      =& B_j p_{j+1} + \gamma_j
% \end{align*}
% For the second part of the equation \eqref{disc_sys}, we can define 
% \begin{equation}
%     q_{N-1} = \sum_{j = 0}^{N-1} A \sigma(W_{11}(P_j \xi + r_j)) = g(\xi) \;.
% \end{equation}
% Therefore, $[\xi \mapsto q_{N-1}] \in \mathcal{S}(\Omega)$. Hence, given $f \in \mathcal{C}(\Omega;\mathbb{R}^n)$ and $\varepsilon > 0$, there exist $N \in \mathbb{N}$, $A \in \mathbb{R}^{n \times n}$,  $ B_j \in \mathbb{R}^{n \times n}$,  and $\gamma_j \in \mathbb{R}^n$ for $j = 0, \cdots, N-1$ such that 
% \begin{equation}
%     |q_{N-1} - f(\xi)| < \varepsilon
% \end{equation}
% for any $\xi \in \Omega$. 
% % Define a function $H:= [\xi \mapsto q_N]:\mathcal{D} \rightarrow \mathbb{R}^{n/2}$ that is the flow of the system.
% \end{proof}}
% \begin{remark}
% We point out that even though we are approximating a function $f$ in $\Rn$ through the dynamical system \eqref{eq:Hamiltonian_dynamics_SIE} which evolves in $\Real^{2n}$, it is evident from the sparsity structures \eqref{eq:parametrization_structure} of the parameters in the proof of Theorem \ref{th:UAP_hamiltonian}, that the number of critical parameters which are needed to obtain the UAP are the same of a ResNet evolving in $\Rn$. Clearly, the full structure of the latter matrices do not affect the achieved UAP\footnote{We recall that $J_j$ should keep the sparsity structure of \eqref{eq:parametrization_structure} to maintain the non-vanishing gradients property (see \cite[Theorem 2]{galimberti2023hamiltonian}).}.
% \end{remark}


We note that the UAP results in \cite{tabuada2022universal} do not apply in our framework because of the skew-symmetric matrix $J$ multiplying the partial derivative of the Hamiltonian in \eqref{eq:Hamiltonian_dynamics_general}. 
% The latter matrix is essential to obtain the symplectic property of the flow leading to the fundamental non-vanishing gradients property, as previously discussed, which is not guaranteed in \cite{tabuada2022universal}.
Moreover, we provide UAPs directly for implementable discrete-time layer equations \eqref{eq:Hamiltonian_dynamics_SIE} instead of the continuous-time NODEs. Indeed, an arbitrary discretization method may not conserve the desired properties, making it challenging to prove the UAP of discretized NODEs in general. 

Untill this point, we focused on automorphisms on $\mathbb{R}^n$. The next result presents the UAP of a general map from $\Omega \subset \Real^n$ to $\mathbb{R}^r$.

\begin{corollary}\label{cor:problem2}
Consider the discrete-time system \eqref{eq:Hamiltonian_dynamics_SIE} with initial condition $(p_0,q_0) = (\xi,0_n)$, for some $\xi \in \Omega$ with $\Omega \subset \Rn$ compact, and  the \emph{restricted} flow $\varphi : \xi \mapsto q_N$. 
Let $h: \Rn \to \Real^r$ be a Lipschitz continuous function such that $f\left(\Omega \right) \subseteq h\left(\mathbb{R}^n\right)$.
Then, for any $\varepsilon > 0$, the function $h\circ \varphi : \Omega \to \Real^r$, satisfies 
\begin{align}
   \sup_{\xi \in \Omega}\left\|f(\xi)- h \circ \varphi(\xi) \right\|\leq \varepsilon .
 \end{align}
\end{corollary}

A typical example that satisfies the necessity condition $f\left(\Omega \right) \subseteq h\left(\mathbb{R}^n\right)$ is $h(\varphi) = W_o^\top \varphi + b_o$, $W_o \in \Real^{n \times r}$, and $b_o \in \mathbb{R}^r$, which is common in classification problems. 
It is straightforward to see that $h(\cdot)$ is Lipschitz continuous, surjective, and satisfies the condition $f\left(\Omega\right) \subseteq h \left(\mathbb{R}^n\right)$. 

It is worth mentioning that unlike other papers \cite{shen2021neural,lin2018resnet,tabuada2022universal}, our results do not impose restrictive conditions on activation functions, which expands their potential applicability.

\subsection{Auxiliary properties of HDNNs}
In the following, we highlight a few associated properties of HDNNs. First, we provide a bound on the desired accuracy of the approximation error with respect to the depth of HDNNs. Second, we state a remark on their non-vanishing gradients property. 

Let us define the first absolute moment $C_f$ of the Fourier magnitude distribution of a desired function $f$. Thus, given $f : \Rn \to \Rn$, with a Fourier representation of the form $f(x) = \int_{\Rn} \e^{i \omega^\top x} \tilde{f}(\omega) \textnormal{d} x$, we define 
\begin{equation}\label{eq:Cf}
    C_f := \int_{\Rn} \|\omega\|_1 \|\tilde{f}(\omega)\| \textnormal{d}\omega \;.
\end{equation}
The condition \eqref{eq:Cf} is usually interpreted as the integrability of the Fourier transform of the gradient of the function $f$, and a vast list of examples for which bounds on $C_f$ can be obtained  are given in Section IX of \cite{barron1993universal}. 

\begin{proposition}\label{prop:approx}
    Consider the discrete-time system \eqref{eq:Hamiltonian_dynamics_SIE} with  sigmoidal\footnote{The function $\sigma(x)$ is assumed to be a sigmoidal function, if it is a bounded function on the real line satisfying $\sigma(x) \rightarrow 1$ as $x \rightarrow \infty$ and $\sigma(x) \rightarrow -1$ as $x \rightarrow -\infty$ \cite{barron1994approximation}.} $\sigma$ and initial condition $(p_0,q_0) = (\xi,0_n)$, for some $\xi \in \Omega=[-1,1]^n$. Then, the \emph{restricted} flow $\varphi : \xi \mapsto q_N$ has the UAP on $\Omega$ with bound $2^{\frac{n}{2}} \frac{C_f}{\sqrt{N}}$.
\end{proposition}

Proposition \ref{prop:approx} states that for any $f\in \mathcal{C}(\Omega;\Real^n)$ with finite $C_f$ and $N\in\Nat$, there exist  parameters $\theta_j = \{J_j, W_j,b_j,\eta_j\}$ with $j=0,\dots,N-1$, such that the function $\varphi : \xi \mapsto q_N$ satisfies
\begin{equation} 
\sup_{x\in\Omega} \|f(x) - \varphi(x)\| \leq 2^{\frac{n}{2}}\frac{C_f}{\sqrt{N}} \;.
\end{equation}
Further remarks on the evaluation\slash approximation of this bound can be found in \cite{barron1993universal} and \cite{barron1994approximation}.


%These phenomena are related to the convergence of gradient computations to zero or the divergence, respectively, during back-propagation \cite{werbos1990backpropagation}. Both situations are  critical as they imply that the learning process either stops prematurely or becomes unstable.the use of HDNNs has been investigated because of its effectiveness in dependable training.
As mentioned earlier, it has been shown that HDNNs considered in \cite{galimberti2023hamiltonian} are endowed with non-vanishing gradients  or in a special case, non-exploding gradients \cite{zakwan2022robust}, \emph{i.e.}, they ensure numerically well-posed training. We defer the reader to those papers for a formal discussion of the non-vanishing gradients property. 

\begin{remark}[Non-vanishing gradients]
The HDNN given by the discrete-time system \eqref{eq:Hamiltonian_dynamics_SIE} enjoys the non-vanishing gradients property when optimizing a generic loss function. In particular, this property is related to the Backward Sensitivity Matrix  $\frac{\partial x_N}{\partial x_{N-j}} = \prod_{\ell = N-j}^{N-1} \frac{\partial x_{\ell+1}}{\partial x_{\ell}}$, where $x =\left(p, q \right)$, at layer $N-j$ for $j=1,\dots, N-1$.
Although the considered Hamiltonian \eqref{eq:Hamiltonian} is different from the one of \cite{galimberti2023hamiltonian} (because of the linear term), one is able to prove the non-vanishing gradients property (by establishing a lower bound for the Backward Sensitivity Matrix) by following the same arguments of \cite[Theorem 2]{galimberti2023hamiltonian} which relies specifically on the symplectic property of the flow and not on the Hamiltonian structure.
\end{remark}
%---------------------------------------------
\section{Conclusion and Future Work}\label{sec:conslusions}
%--------------------------------------------
We demonstrated the universal approximation property of Hamiltonian Deep Neural Networks (HDNNs) that also enjoy non-vanishing gradients during training.  
This result affirms both the practicality and theoretical foundation of HDNNs. 
In particular, we have demonstrated that a portion of the flow of HDNNs can approximate any continuous function in a compact domain. 
Also, we provide some insights on the approximation error with respect to the depth of neural network. 

Our work opens doors to quantifying the expressivity of other physics-inspired neural networks with special properties, such as \cite{zakwan2022physically, wright2022deep}.
Future research will focus on leveraging differential geometric tools \cite{tabuada2020universal, tabuada2022universal} to establish universal approximation properties for HDNNs, where the Hamiltonian function is parameterized by an arbitrary neural network. 
%
% along with the associated imitation learning problem \cite{kim2018standard, jin1995approximation}. 
% Another interesting direction would be to analyze the universal approximation properties with different discretization schemes.

\appendix
%\subsection{Non-vanishing gradients property by design \cite{galimberti2023hamiltonian}}\label{app:non_vanishing}
%When an HDNN \eqref{eq:Hamiltonian_dynamics_general_SIE} is completed with an output layer $x_{N+1} = \phi(x_N, \theta_N)$ with training parameters $\theta_N$,  when considering, for instance, a multi-class classification task with the training set $\{(\hat{x}_0^k, c^k): k=1,\dots, S\}$\footnote{The training set $\{(\hat{x}_0^k, c^k): k=1,\dots,S\}$ is characterized by a number $S\in\Nat$ of data points, the feature vectors $\hat{x}_0^k$ and  the corresponding labels $c_k \in\{1,\dots,n_c\}$, with $n_c \in \Nat$.}, one typically seeks to minimize a cost function $\L$ of the form
%\begin{equation}
%\label{eq:optimization_problem}
%\begin{aligned}
%&\min_\theta \frac{1}{S} \sum_{k=1}^S\L\Big(\phi(\hat{x}_N^k,\theta_N), \, c_k\Big) + \mathcal{R}\left(\theta_{\{0,\dots,N\}}\right), \\
%&\quad \textnormal{s.t.} \ (6) \ \textnormal{holds,} \quad j=0,1,\dots, N \;, 
%\end{aligned}
%\end{equation}
%where $\theta_{\{0,\dots,N\}}$ is the set $\{\theta_0,\theta_1,\dots,\theta_N\}$ of trainable parameters (the set $\theta_j = \{J_j, W_j,b_j,\eta_j\}$ with $j=0,\dots,N-1$ of \eqref{eq:Hamiltonian_dynamics_SIE}), and $\mathcal{R}$ is a regularization term. We refer to \cite{haber2017stable,chang2018reversible,galimberti2023hamiltonian} for further details.
%In particular, to minimize \eqref{eq:optimization_problem} using gradient descent, we require the gradient of the loss function $\L$ with respect to the parameters must be computed at each iteration. This gradient is calculated using the chain rule for the $i$-th parameter of layer $N-j-1$, namely $\theta_{i, N-J-1}$ as
%\begin{align} \label{eq:gradient}
%    \frac{\partial \L}{\partial \theta_{i, N-j-1}} &= \frac{\partial x_{N-j}}{\partial \theta_{i, N-j-1}} \left(\prod_{\ell = N-j}^{N-1} \frac{\partial x_{\ell+1}}{\partial x_{\ell}} \right) \frac{\partial \L }{\partial x_{N}}\;.
%\end{align}
%The matrix  $\frac{\partial x_N}{\partial x_{N-j}} = \prod_{\ell = N-j}^{N-1} \frac{\partial x_{\ell+1}}{\partial x_{\ell}}$ known as the Backward Sensitivity Matrix (BSM) at layer $N-j$ for $j=1,\dots, N-1$ plays a critical role in computing the gradient \eqref{eq:gradient} and its studying is crucial in investigating the phenomena of vanishing (and exploding) gradients.
%We can now present a formal statement that ensures the non-vanishing gradients property by establishing a lower bound for the BSM.
%\begin{proposition}\label{prop:nonvanishing}
%Consider the discrete-time system \eqref{eq:Hamiltonian_dynamics_general_SIE} where the matrix $J_j$ is non-zero for any $j=0,\dots,N-1$ with the structure 
%\begin{equation}\label{eq:J_prop}
 %   J_j = \begin{bmatrix} 0_\ntn & -X_j \\ X_j & 0_{n\times n} \end{bmatrix} \qquad X_j\in\Real^{n \times n} \;, 
%\end{equation}
%then for all $j=0,\dots,N-1$ we have the lower bound

%\begin{equation}\label{eq:lowerbound_BSM}
%    \left\| \frac{\partial x_N}{\partial x_{N-j}} \right\| \geq 1\;.
%\end{equation}
%\end{proposition}
 
%Despite the linear term in the Hamiltonian function \eqref{eq:Hamiltonian}, which distinguishes it from the one in \cite{galimberti2023hamiltonian}, the proof of Proposition \ref{prop:nonvanishing} remains the same as the one of Theorem 2 of \cite{galimberti2023hamiltonian} since it does not rely on the specific structure of the Hamiltonian. 

%Proposition \ref{prop:nonvanishing} makes it evident that the presence of the lower bound \eqref{eq:lowerbound_BSM} ensures that the gradient \eqref{eq:gradient} cannot vanish during the training of an HDNN.
\subsection{Proof of Proposition \ref{prop:UAPmaps}}\label{app:Cybenkosum}
The proof follows from \cite[Theorem 3.1]{pinkus1999approximation}. Consider the matrix $A_j$ in \eqref{eq:Cybenko_sum_matrix} diagonal, so that each each component $g^{i}$  of the vector function $g$ has the form
\begin{equation}\label{eq:component_g}
    g^{i}(x):= \sum_{j = 0}^{N-1} \alpha^i_j \,\sigma({w^i_j}^{\top} x + \beta^i_j), \quad i = 1, \dots,n, 
\end{equation}
with parameters $N\in\Nat$, $\alpha^i_j\in \Real$ the $i$-th entries of diagonal matrices,  $\beta^i_j \in \Real$ the $i$-th entries of diagonal matrices $A_j$ and vectors $b_j$, and $w_{i,j}^{\top} \in \Rn$ the $i$-th row vector of $W_j$. Then, by \cite[Theorem 3.1]{pinkus1999approximation}, for all $i=1,\dots, n$, the functions $g^{i}$ has the UAP. \hfill \QEDclosed

It is clear that the UAP of \eqref{eq:Cybenko_sum_matrix} does not get affected when matrices $A_j$ are full. 


\subsection{Preliminary lemma}\label{app:preliminary_lemma}
We introduce the following key auxiliary result which is necessary to prove Theorem \ref{th:UAP_hamiltonian}.
\begin{lemma}
\label{lem:W_full_rank}
    Let $g$ be the function in \eqref{eq:Cybenko_sum_matrix} with the UAP on $\Omega$. For any $\tilde{\varepsilon}>0$ we can find $\At_j,\Wt_j \in \Real^{n \times n}$, with $\Wt_j$ full rank, and  $\bt_j \in \Rn$ for $j=0,\dots, N-1$, such that $\tilde g(x):= \sum_{j = 0}^{N} \At_j \,\sigma(\Wt_j  x + \bt_j)$ satisfies $\| \tilde{g} - g \|_{\L_\infty(\Omega)} \leq \tilde{\varepsilon}$.
\end{lemma}


In other words, Lemma \ref{lem:W_full_rank} allows us to assume without loss of generality that the function $g$ in \eqref{eq:Cybenko_sum_matrix} has non-singular matrices $W_j$ for any $j=0,\dots,N-1$.

% \begin{proof}
%   Given the function $g$ in \eqref{eq:Cybenko_sum_matrix} with the UAP on $\Omega$, we consider the case in which there exist the non-empty set $K = \{j\in \{0,\dots,N-1\} :  |W_j|=0\}$. For simplicity, we assume that there exists only one element (index) in $K$, say $\kappa$. If the cardinality of the set $K$ is more than one, then one can apply exactly the same arguments for all $\kappa \in K$. Now consider $W_\kappa = \left(w_\kappa^{(1)},\dots,w_\kappa^{(n)}\right)^\top$. Since $|W_\kappa| = 0$, we split the proof in two cases: \emph{1)} $w_\kappa^{(\ell)}=0_n$ for some $\ell = 1,\dots, n$ and the rest $w_\kappa^{(\ell)}$ independent vectors, and \emph{2)} $w_\kappa^{(\ell)}\neq 0_n$ for all $\ell = 1,\dots, n$ with the linear combination $\sum_{j=1}^n s_j w_\kappa^{(j)}=0$ for $s_j\in\Real$ not all equal to zero. The combination of the two cases easily follows. \\
%   \emph{Case 1)} Assume for simplicity there exists only one $\ell \in\{1,\dots,n\}$ such that $w_\kappa^{(\ell)}=0_n$ (the case with more than one $\ell \in\{1,\dots,n\}$ such that $w_\kappa^{(\ell)}=0_n$ follows with same arguments), say $w_\kappa^{(n)}=0_n$, then the parameters $\At_j,\Wt_j \in \Real^{n \times n}$ and  $\bt_j \in \Rn$ of the function $\tilde g(x)= \sum_{j = 0}^{\tilde N} \At_j \,\sigma(\Wt_j  x + \bt_j)$ can be selected as follows: $\At_j = A_j$, $\bt_j = b_j$ for all $j=0,\dots, N-1$, while $\Wt_j = W_j$ for all $j\neq \kappa$ and $\Wt_j = W_j + \Lambda$, with $\Lambda = (0_{n\times n-1},  \tilde{w}_\kappa^{(n)})^\top$ for $j=\kappa$ where $\tilde{w}_\kappa^{(n)}$ is selected such that $|\Wt_\kappa|\neq 0$ and 
% \begin{equation}\label{eq:ineq_wkappa_proof}
%   \|\tilde{w}_\kappa^{(\ell)}\| \leq \frac{\tilde \varepsilon}{\sqrt{n} \,L_\sigma \, \|x\|_{\L_{\infty}(\Omega)}\,  \max_{1\leq r\leq n} \|a_\kappa^{(r)}\|},
% \end{equation}
% \zak{Maybe we can use $\operatorname{Lip}(\sigma)$ to be consistent with the corollary}
% where $L_\sigma$ is the Lipschitz constant of function $\sigma$ and $a_{\kappa}^{(r)^\top}$ are the rows of the matrix $A_k$\footnote{we implicitly assume the non-trivial case $A_k\neq 0_\ntn$ since if $A_k$ is the zero matrix, then one can select any $\tilde{w}_\kappa^{(\ell)}$ in \eqref{eq:Wtilde_structure_proof} such that $|\Wt_\kappa|\neq 0$ and $\|g-\tilde{g}\|_{\L_{\infty}(\Omega)}=0$.}. In fact, by noticing that for $x\in \Omega$ we have $\left\| (W_\kappa - \tilde{W}_\kappa ) x \right\|  \leq \|\tilde{w}_\kappa^{(\ell)}\|\, \|x\|_{\L_{\infty}(\Omega)}$, by looking at the $r$-th component of the difference $\tilde g - g $, we have for $x\in\Omega$
% \begin{align}
%     |g^{(r)}(x) - \tilde{g}^{(r)}(x)| & \leq L_\sigma \, \|x\|_  {\L_{\infty}(\Omega)}\,   \|w_\kappa^{(\ell)}\| \, \|a_\kappa^{(r)}\| \leq \frac{\tilde{\varepsilon}}{\sqrt{n}}, \nonumber
% \end{align}
% from which we obtain $\|g-\tilde{g}\|_{\L_{\infty}(\Omega)} \leq \tilde\varepsilon$.\\
% \emph{Case 2)} since $w_\kappa^{(\ell)}\neq 0_n$ for all $\ell = 1,\dots, n$, let us define 
% \begin{equation}\label{eq:Wtilde_structure_proof}
%      \Wt_{\kappa,\ell} = \left(\tilde{w}_{\kappa,\ell}^{(1)},\dots,\tilde{w}_{\kappa,\ell}^{(\ell-1)},w_\kappa^{(\ell)}, \tilde{w}_{\kappa,\ell}^{(\ell+1)}, \tilde{w}_{\kappa,\ell}^{(n)}\right)^\top,
%   \end{equation}
% where the $\tilde{w}_{\kappa,\ell}^{(\ell-1)}$ can be selected such that $| \Wt_{\kappa,\ell} |\neq 0$ for all $\ell = 1,\dots, n$. Then, we set $\At$  
% \end{proof}

\begin{proof}
  Given the function $g$ in \eqref{eq:Cybenko_sum_matrix} with the UAP on $\Omega$, we consider the case in which there exists the set $K = \{\kappa\in \{0,\dots,N-1\} :  |W_\kappa|=0\}$ non-empty with cardinality $\Tilde{n}$. For $\kappa \in K$, let $\textnormal{rank}(W_{\kappa}) = n-r_\kappa$, with $r_\kappa>0$ the number of dependent column vectors of $W_\kappa$ so that, up to a row permutation, assume $W_\kappa$ is partitioned as 
  \begin{equation}\label{eq:Wk_partion_proof}
  W_\kappa = \left(w_\kappa^{(1)},\dots,w_\kappa^{(r_\kappa)},w_\kappa^{(r_\kappa+1)},\dots,w_\kappa^{(n)} \right)^\top,
\end{equation}
    with the last $n-r_\kappa$ vectors linearly independent.
  Then, the parameters $\At_j,\Wt_j \in \Real^{n \times n}$ and  $\bt_j \in \Rn$ of the function $\tilde g(x)= \sum_{j = 0}^{N} \At_j \,\sigma(\Wt_j  x + \bt_j)$ can be selected as follows. We set $\At_j = A_j$, $\bt_j = b_j$ for all $j=0,\dots, N-1$. Moreover, $\Wt_j = W_j$ for all $j\notin K$ and, for $\kappa\in K$, $\Wt_\kappa = W_\kappa + \Lambda_\kappa$, where 
  \begin{equation}
   \Lambda_\kappa = \left(\tilde{w}_\kappa^{(1)},\dots,\tilde{w}_\kappa^{(r_\kappa)},0_n,\dots,0_n \right)^\top   
  \end{equation}
   and the vectors $\tilde{w}_\kappa^{(\ell)}$,  $\ell = 1,\dots,r_\kappa$, are selected such that $|\Wt_\kappa|\neq 0$ and 
\begin{align}\label{eq:ineq_wkappa_proof}
  \|\tilde{w}_\kappa^{(\ell)}\| & \leq \frac{\tilde \varepsilon}{r_\kappa \, \tilde{n} \, \sqrt{n} \,L_\sigma \, \|x\|_{\L_{\infty}(\Omega)}\,  \max_{1\leq p\leq n} \|a_\kappa^{(p)}\|}\;,
\end{align}
where $L_\sigma$ is the Lipschitz constant of function $\sigma$ and $a_{\kappa}^{(p)^\top}$, $p=1,\dots,n$, are the rows of the matrix $A_\kappa$\footnote{We implicitly assume the non-trivial case $A_\kappa\neq 0_\ntn$ since if $A_\kappa$ is the zero matrix, then one can select any $\tilde{w}_\kappa^{(\ell)}$ such that $|\Wt_\kappa|\neq 0$ and $\|\tilde{g} - g\|_{\L_{\infty}(\Omega)}=0$.}. By noticing that for $x\in \Omega$ we have 
\begin{equation}
\left\| (\tilde{W}_\kappa - W_\kappa  ) x \right\|  \leq \|x\|_{\L_{\infty}(\Omega)} \sum_{\ell = 1}^{r_\kappa} \| \tilde{w}_\kappa^{(\ell)}\|,
\end{equation}
and by looking at the $p$-th component of the difference $\tilde g - g $, by inequality \eqref{eq:ineq_wkappa_proof}, for $x\in\Omega$, we have
\begin{align}
   \left| \tilde{g}^{(p)}(x) - g^{(p)}(x) \right| & \leq L_\sigma \, \|x\|_  {\L_{\infty}(\Omega)} \sum_{\kappa \in K}\left( \|a_\kappa^{(p)}\| \sum_{\ell = 1}^{r_\kappa}  \|w_\kappa^{(\ell)}\|\right) \nonumber \\
    &\leq \sum_{\kappa \in K} \frac{\tilde{\varepsilon}}{\tilde{n}\sqrt{n}} \leq \frac{\tilde{\varepsilon}}{\sqrt{n}} \;,
\end{align}
from which we obtain $\|\tilde{g} - g\|_{\L_{\infty}(\Omega)} \leq \tilde\varepsilon$.
\end{proof}
%---------------------------------
\subsection{Proof of Theorem \ref{th:UAP_hamiltonian}}\label{app:th_main}
We prove the result by showing that the function $\varphi: \xi \mapsto q_N$ can be written in the form \eqref{eq:Cybenko_sum_matrix}, and thus, satisfying Proposition \ref{prop:UAPmaps}, it has the UAP on $\Omega$.
In fact, by restricting the parameter space as follows 
\begin{equation}\label{eq:parametrization_structure}
\begin{aligned}
J_j &= \begin{bmatrix} 0_\ntn & -X \\ X & 0_\ntn 
\end{bmatrix} \qquad  W_j = \begin{bmatrix} \tilde{W}_j & 0_\ntn \\ 0_\ntn & 0_\ntn
\end{bmatrix}, \\  
b_j &= \begin{bmatrix} \bt_j \\ 0_n  \end{bmatrix} \qquad\qquad\quad\quad \ \  \eta_j= \begin{bmatrix} 0_n \\ -\etat_j \end{bmatrix},
\end{aligned}
\end{equation}
where $X \in \Real^{n\times n}$, $\tilde{W}_j : \Realp \to \Real^{n\times n}$, $\bt_j: \Realp \to \Rn$, $\etat_j: \Realp \to \Rn$, one can write \eqref{eq:Hamiltonian_dynamics_SIE} as
\begin{equation}\label{eq:Hamiltonian_dynamics_SIE_proof_1}
\begin{aligned}
{\left[\begin{array}{c}
{p}_{j+1} \\
{q}_{j+1}
\end{array}\right] } & = \left[\begin{array}{c}
{p}_j  \\
{q}_j 
\end{array}\right]  + h \left[\begin{array}{c}
  X^\top \etat_j  \\
X \Wt_{j}^\top \sigma(\Wt_{j}p_{j+1} + \bt_j )
\end{array}\right]  \\
& = \left[\begin{array}{c}
{p}_j  \\
{q}_j 
\end{array}\right]  + \left[\begin{array}{c}
 \gammat_j  \\
\At_j \sigma(\Wt_{j}p_{j+1} + \bt_j  )
\end{array}\right] 
\end{aligned}
\end{equation}
for $j = 0, 1, \cdots, N-1$, where $\gammat_j = h X^\top \etat_j$, and $\At_j = h X \Wt_{j}^\top$, respectively. 
From the initial condition $(p_0,q_0) = (\xi,0_n)$,  $\xi \in \Omega$, and by substituting the expression of $p_{j+1}$ into the second equation of \eqref{eq:Hamiltonian_dynamics_SIE_proof_1} we have that
\begin{align}\label{eq:def_varphi}
    q_{N} &= \sum_{j = 0}^{N-1} \At_j \sigma(\tilde{W}_{j}( p_j + \gammat_j) + \bt_j ) \nonumber \\
    & = \sum_{j = 0}^{N-1} \At_j \sigma(\tilde{W}_{j} \xi + \dt_j) =: \varphi(\xi) \;,
\end{align}
where $\dt_j = \Wt_j \rt_j + \bt_j $ with $\rt_j=\rt_{j-1} + \gammat_j$ and $\rt_0=\gammat_0$.
Notice that, because of Lemma \ref{lem:W_full_rank}, we can assume without loss of generality that $\Wt_j$ is non-singular. Consequently, the triple $\{\At_j,\Wt_j,\dt_j\}$ can be freely assigned for all $j=0,\dots,N-1$ and thus is in the form of \eqref{eq:Cybenko_sum_matrix} and satisfies  the UAP on $\Omega$ (Proposition \ref{prop:UAPmaps}). \hfill \QEDclosed

Finally, it is clear that by considering full parametrization\footnote{We recall that $J_j$ should keep the sparsity structure \eqref{eq:parametrization_structure} to maintain the non-vanishing gradients property (see \cite[Theorem 2]{galimberti2023hamiltonian}).} in \eqref{eq:parametrization_structure}, we improve the approximation power of HDNNs and, hence, UAP is preserved.  
%-----------------------------------------------------------------------------------------------------------
\subsection{Proof of Corollary \ref{cor:problem2}}\label{app_corollary}
%-----------------------------------------------------------------------------------------------------------
In \cite[Proposition 3.8]{li2022deep} it is shown that there exists a continuous function 
$$\psi(\xi)=\sum_{i=1}^N z_i \psi_i(\xi) \quad \text{for} \, \xi \in \Omega,$$
where $z_i \in h^{-1}\left(F_i\right)$, with $\{F_i\}_{i=1}^N$ a partition of $f(\Omega)$, and continuous functions $\psi_i: \Omega \rightarrow[0,1]$ such that $\psi_i=1$ on  $A_i$ and $\psi_i=0$ on $\cup_{j \neq i} A_j$.
The sets $A_i\subset \Omega_i$, with $\{\Omega_i\}_{i=1}^N$ a partition of $\Omega$, such that $h \circ \psi$ has the UAP on $\Omega$ (provided that the desired function $f$ is such that $f\left(\Omega \right) \subseteq h\left(\mathbb{R}^n\right)$). Now, take $\psi$ such that $$\left\|f-h \circ \psi\right\|_{\L_{\infty}(\Omega)} \leq \varepsilon\slash 2$$ and, by Theorem \ref{th:UAP_hamiltonian}, take $\varphi : \xi \mapsto q_N$ such that $$\left\|\psi - \varphi \right\|_{\L_{\infty}(\Omega)}\leq \varepsilon\slash (2 L_h)\;.$$
Then, for any $\xi \in \Omega$ we have
\begin{equation*}
\begin{aligned}
\|f-h \circ \varphi (\xi)\| & \leq\left\|f(\xi)-h \circ \psi(\xi) \right\|\\ 
&\qquad+\left\|h(\xi) \circ \psi(\xi) - h \circ \varphi(\xi)\right\| \\
& \leq \frac{\varepsilon}{2}+L_h\left\|\psi(\xi)-\varphi(\xi)\right\| \leq \varepsilon\;, 
\end{aligned} 
\end{equation*}
and the proof is completed. \hfill \QEDclosed
%-----------------------------------------------------------------------------------------------------------
\subsection{Proof of Proposition \ref{prop:approx}}\label{app:approx}
%-----------------------------------------------------------------------------------------------------------
 The proof follows from \cite[Theorem 1]{barron1994approximation} by noting that the function $\varphi$ in \eqref{eq:def_varphi} is the NN considered in \cite{barron1994approximation}, by selecting the probability measure $\tilde{\lambda}(\cdot) := \frac{1}{2^n} \lambda(\cdot)$ where $\lambda$ is the Lebesgue measure on $\Omega$, and by recalling the norm inequality $\|\cdot \|_\infty\leq \|\cdot \|_2$.\hfill \QEDclosed



 

% \subsection{Examples of activation functions with UAP}
% \label{sec:appendixA}
% Some examples of activation functions that satisfy UAP are provided in Table \ref{tab:act}.
% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c}
%     \hline
%     Activation Function  & $\sigma(x)$ \\
%     \hline 
%      ReLU    &  $\max \{x,0 \}$\\
%      Sigmoidal  & $(1 + \exp(-x))^{-1}$ \\
%      Softplus    & $\log (1 + \exp(x)) $ \\
%      Hyperbolic Tangent & $\tanh(x)$ \\
%      Radial Basis Function & $\frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2})$ \\
%      \hline
%     \end{tabular}
%     \caption{Typical examples of activation functions with UAP.
% }
%     \label{tab:act}
% \end{table}


\bibliography{bibliography.bib}
\bibliographystyle{unsrt}
\end{document}