\documentclass[tinyml]{acmart}
% \settopmatter{authorsperrow=3}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{rightsretained}
\copyrightyear{2023}
\acmYear{2023}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%   Gaze Detection}{June 03--05, 2018}{Woodstock, NY}

% \usepackage{expl3}
% \usepackage{biblatex}
% \usepackage{cite}
% \usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsmath,amsfonts}
% \usepackage{algorithmic}
% \usepackage{graphicx}
% \usepackage{textcomp}
\usepackage{xcolor}
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{pgfplots}
% \pgfplotsset{width=8cm, compat=1.9}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage[super]{nth}
\usepackage{multirow}
\usepackage{tabularx}
% \usepackage[a4paper, total={184mm,239mm}]{geometry}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{FMAS: Fast Multi-Objective SuperNet Architecture Search for Semantic Segmentation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

%% BM: We can leave off the authors for now to save space.

\author{Zhuoran Xiong}
\email{zhuoran.xiong@mail.mcgill.ca}
\affiliation{%
  \institution{McGill University}
  \streetaddress{address}
  \city{Montreal}
  \country{Canada}
}

\author{Marihan Amein}
\email{marihan.amein@mail.mcgill.ca}
\affiliation{%
  \institution{McGill University}
  \streetaddress{address}
  \city{Montreal}
  \country{Canada}
}

\author{Olivier Therrien}
\email{olivier.therrian@mail.mcgill.ca}
\affiliation{%
  \institution{McGill University}
  \streetaddress{address}
  \city{Montreal}
  \country{Canada}
}

\author{Warren J. Gross}
\email{warren.gross@mcgill.ca}
\affiliation{%
  \institution{McGill University}
  \streetaddress{address}
  \city{Montreal}
  \country{Canada}
}

\author{Brett H. Meyer}
\email{brett.meyer@mcgill.ca}
\affiliation{%
  \institution{McGill University}
  \streetaddress{address}
  \city{Montreal}
  \country{Canada}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{shortauthors}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
We present FMAS, a fast multi-objective neural architecture search framework for semantic segmentation.
FMAS subsamples the structure and pre-trained parameters of DeepLabV3+, without fine-tuning, dramatically reducing training time during search.
To further reduce candidate evaluation time, we use a subset of the validation dataset during the search.
Only the final, Pareto non-dominated, candidates are ultimately fine-tuned using the complete training set. 
We evaluate FMAS by searching for models that effectively trade accuracy and computational cost on the PASCAL VOC 2012 dataset.
FMAS finds competitive designs quickly, e.g., taking just 0.5 GPU days to discover a DeepLabV3+ variant that reduces FLOPs and parameters by 10$\%$ and 20$\%$ respectively, for less than 3\% increased error. We also search on an edge device called GAP8 and use its latency as the metric. FMAS is capable of finding 2.2$\times$ faster network with 7.61\% MIoU loss. 
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.

\keywords{NAS, Segmantic Segmentation, Edge Computing, TinyML}

% \settopmatter{printfolios=true}
%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Semantic image segmentation \cite{semanticImageSeg} is one of the fundamental applications in computer vision: 
it helps us understand scenes by identifying the various objects in an image, and their corresponding locations, by predicting an independent class label for each pixel. 
Image segmentation is essential to many applications that run on resource-constrained embedded hardware, such as: 
autonomous driving, medical imaging, and biometric authentication.

Convolutional neural networks (CNN) that achieve state-of-the-art (SOTA) results in image segmentation have sophisticated structures that are generally optimized for accuracy.
They also often require larger feature maps than image classification tasks to be able to produce pixel-wise labels, resulting in a large memory footprint.
%Since embedded platforms have a limited computing capacity, deploying these sizable models to them yields a low hardware performance in terms of latency and memory consumption. 
In some cases, e.g., autonomous vehicles, image segmentation must be performed in real-time, which makes the development of efficient models for deployment to edge devices critical.

% Prior work has proposed multi-objective neural architecture search (NAS) to identify computationally-efficient networks for hardware deployment.
% Examples include DPC~\cite{DPC}, SqueezeNAS~\cite{SqueezeNAS}, and TASC~\cite{Template-Based}.
% However, such approaches are prohibitively expensive, taking at least 14 GPU days to converge. 
%DPC \cite{DPC} design a proxy task to accelerate their candidate evaluation time. 
%TASC \cite{Template-Based} accelerated the search time with respect to \cite{DPC} by building on the structure of pre-trained classification networks to optimize semantic segmentation networks. 
%SqueezeNAS \cite{SqueezeNAS} reduces search time in comparison to TASC \cite{Template-Based} by using a differentiable supernet design space, and training it using stochastic gradient descent. 
%SuperNAS extends this by not fine-tuning candidates at all, resulting in a dramatic reduction in search time.

%A NAS framework consists of a search space, defining the set of all possible design, an optimization technique, outlining the algorithm that finds the highest performing architectures in terms of accuracy and hardware efficiency, and an evaluation strategy, quantifying the accuracy and hardware performance of the networks. The bottleneck of NAS is the cost of evaluating the accuracy of the searched networks. Ideally, each network should be trained from scratch to evaluate its accuracy, resulting in a long search time. 

%Existing multi-objective Neural Architecture Search (NAS) approaches, such as  proposed methods that search for computationally-efficient networks for hardware deployment. 


Multi-objective network architecture search has been proposed for the purpose of finding efficient models, but the time required to train candidates is prohibitive. %The main bottleneck in multi-objective NAS for efficient is the time required to train candidate models. 
%Existing multi-objective NAS frameworks require a prohibitive search time, mostly spent in training candidates. %, to discover computationally-efficient networks. 
DPC~\cite{DPC}, for instance, requires 2,590 GPU days to converge to good designs.
SqueezeNAS~\cite{SqueezeNAS} and TASC~\cite{Template-Based} require 14 and 16 GPU days, respectively.
Fully training a DeepLabV3+ variant with a Modified Xception backbone requires 0.88 GPU days on average, and an additional 0.014 GPU days for evaluation on the validation data set. 
% Performing a search for DL3+ variants using genetic algorithms with a population of 12 networks for 20 generations would require an total of 220 GPU days (215 for training and 5 for evaluation) if each candidate was trained from scratch.
This makes it challenging to search for efficient variants of off-the-shelf networks in a practical amount of time. 
Once-for-all~\cite{Once-For-All} (OFA) solves this problem by essentially training a supernetwork and all its possible subnetworks simultaneously, at an additional computational cost.
%However, it does not solve the convergence problem of the OFA network.

To accelerate NAS for image segmentation, we propose Fast Multi-objective Architectural Search (FMAS), a fast multi-objective NAS framework for semantic image segmentation at the edge.
FMAS uses DeepLabV3+ as a supernet to search for computationally-efficient models.
%DeepLabV3+~\cite{deeplabv3+} (DL3+) is an encoder-decoder CNN which employs Modified Xception~\cite{deeplabv3+} and MobileNetV2~\cite{mobilenetv2} as backbones for feature extraction.
DeepLabV3+~\cite{deeplabv3+} (DL3+) is a SOTA encoder-decoder CNN which employs backbones like Modified Xception~\cite{deeplabv3+} or MobileNetV2~\cite{mobilenetv2} for feature extraction.
%Its high accuracy is attributed to its ability to capture sharp object boundaries and high-level contextual features on multiple scales.
%DL3+ achieves high accuracy on the widely-used image segmentation dataset, PASCAL VOC 2012 \cite{PASCALVOC}.
%%% BM: This is a great detail, but I'm not sure that there's room for it in the introduction, since this is just one piece of the NAS puzzle.
%Xception is over-parameterized. It consists of computationally-expensive repetitive blocks which we parameterize in our design space.
%We use NSGA-II \cite{nsga2}, an elitist genetic algorithm, to search for hyperparameter combinations that efficiently trade-off accuracy (mean intersection over union, or MIoU) and cost (FLOPs or parameters). CEDL uses a subject of the architectural parameters of DeepLabV3+ to sample candidate networks. Much of the search effort goes into exploring the depth of the Xception Middle Flow, which consists of 16 repeated blocks.
%The bottleneck of the search time is the network evaluation time. 
% In this paper, we propose SuperNAS, a template-based search approach using an well optimized network, DeepLabV3+, as a supernet. 
%In this paper, we propose FMAS, a fast search framework using a well-optimized network, DeepLabV3+ in our case, as a supernet, and efficiently searching among its submodels. 
%Our goal is to reduce the cost of training by sharing pre-trained weights from DeepLabV3+ and evaluating candidates on a subset of the validation set. 
%We use NSGA-II \cite{nsga2}, an elitist genetic algorithm (GA) to quickly search for computationally-efficient image semantic segmentation models.
FMAS addresses the computational complexity of supernet architectural search in two key ways.
First, candidate networks evaluated by FMAS sub-sample the pre-trained weights of DL3+.
FMAS uses the resulting model performance---\emph{without fine-tuning}---and computational cost, to direct the search toward models with advantageous accuracy-cost trade-offs;
%First, candidate network weights are based on the pre-trained weights adopted from the DeepLabV3+ baseline; 
fine-tuning is only performed on final set of Pareto-optimal models. 
Second, FMAS evaluates candidates on a subset of the validation set. % to further cut the evaluation time.
Neither optimization significantly affects NAS decision making, but together they dramatically reduce search time.

We conducted experiments with PASCAL VOC 2012~\cite{PASCALVOC} to evaluate the effectiveness of our FMAS. 
% Using just 1$\%$ of the entire experiment during search, our method is capable of finding model with less than a $3\%$ increase in MIoU error. 
%While reducing training time by 99$\%$ with no fine-tuning and reduced validation set, our method is capable of finding model with less than a $3\%$ increase in MIoU error. 
%While reducing training time by 99$\%$ and evaluation time by 80\%, our method is capable of finding models with less than a $3\%$ increase in MIoU error.
While reducing search time by 99$\%$ by using weight sharing and a subset of the validation set, our method is capable of finding models with less than a $3\%$ increase in MIoU error.
Moreover, we demonstrate the deployment of image segmentation to TinyML-class~\cite{RAY20221595} systems by targeting the ultra-low-power GAP8~\cite{8445101} platform.
%Moreover, we expand the using scenario of TinyML\cite{RAY20221595} by finding an efficient image semantic segmentation network on an ultra-low-power GAP8\cite{8445101} platform. 
% SNAS finds sub-network models that are 2$\times$ faster with only 10$\%$ MIoU loss compared to the supernet.
FMAS finds a sub-network model that is 2.2$\times$ faster on GAP8 with a loss in MIoU of 7.61\% compared to the supernet.
%than the supernet, which is DeeplabV3+ in this case. 
%The accomplishment of finding this network proves that our method can be used in the industrial TinyML project when trying to speed up the inference time and keep the accuracy performance at the same time.
% SNAS relies on NSGA-II \cite{nsga2}, an elitist genetic algorithm, to subsample DL3+ for candidate models, using crossover and mutation to search for hyperparameter combinations that efficiently trade-off accuracy (mean intersection over union, or MIoU) and cost (FLOPs or network parameters).
% Our search is focused primarily on the over-parameterized Xception feature extractor, and the dilation rates of atrous convolutions throughout DL3+;
% this allows SNAS to reduce the computational complexity of DL3+ without retraining during the search.

% We conducted experiments with PASCAL VOC 2012 \cite{PASCALVOC} to evaluate how quickly SNAS can find competitive networks.
% We observe that relying on pre-trained weights without fine-tuning reduces training time by 215 GPU days.
% %%% BM: What about using the smaller validation set? Gotta mention that if we're going to be so specific about pre-trained weights.
% Using just 20\% of the validation set for evaluation during search saves an additional 3 GPU days, resulting in an aggregate reduction of 99\% for the entire experiment during search.
% Fine-tuning the most accurate Pareto-dominate models adds 0.14 GPU days per candidate, and demonstrates that little performance is lost during the dramatically accelerated search.
% We further examine our method under the TinyML\cite{RAY20221595} scene. Specifically, we run SNAS on at ultra-low-power processor called GAP8 \cite{8445101} and use the latency as performing metric. Our results shows that  SNAS can cut the latency by 4.7\% with relatively 5.8\% mIoU loss. A more aggressive version can have 54.1\% latency cut with 11.3\% mIoU loss.
% Running NAS at Edge device for image segmentation task has never been done before. The GAP8 result proves that SNAS is able to extract an efficient sub-network from a pre-trained image segmentation network at edge device, which expand the TinyML scenes.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.55\textwidth]{./images/cedl.png}
  \caption{The supernet DeepLabV3+ of FMAS. The base structure is an encoder-decoder preceded with a Modified Xception backbone~\cite{deeplabv3+}. The search space operations and how they are decoded into the supernet structure are highlighted.}
  \label{Computationally-Efficient DeepLabV3+ search space}
\end{figure*}

Specifically, we make the following contributions:
1) We employ NSGA-II~\cite{nsga2}, an elitist genetic algorithm to quickly search for efficient sub-structures in a pre-trained supernet that has been optimized for the targeted task;
2) To accelerate the search, we use a subset of the validation dataset as a proxy; and b) only fine-tune Pareto-optimal models; % employ weight sharing of the pre-trained network to avoid retraining; 
3) We optimize semantic segmentation for edge inference, deploying models on the GAP8 platform. % \cite{8445101}. 
%We search on the GAP8 in regards of the latency and the mean IoU error and prove the effectiveness of our method. 
To the best of our knowledge, this is the first work to perform NAS for semantic segmentation on ultra-low-power hardware.


\section{Related Work}
%\subsection{Search Space Design}

To cut search time, multi-objective NAS methods for image segmentation represent the search space in either a \emph{hierarchical}~\cite{Auto-DeepLab},
%\cite{DINTS}, \cite{Template-Based}, \cite{DCNAS}, \cite{mixsearch}, 
or a \emph{template-based} way~\cite{DPC, SqueezeNAS, Template-Based}. %, \cite{SqueezeNAS}, \cite{Template-Based}.
%\cite{NAS-Unet}, \cite{AutoSegNet}, \cite{V-NAS}. 
A hierarchical search performs an alternating bi-level optimization, starting by first optimizing the high-level design, then optimizing the internal structure. 
We adopt a template-based search, which tends to converge faster than hierarchical search, as it constrains the high-level structure of the candidates to a pre-defined architecture. % (e.g., DeepLabV3+). %, and only search the structure of within the building units of a network.

%\subsection{Search Strategies}

A number of Reinforcement Learning (RL) NAS approaches for image segmentation have been proposed \cite{Template-Based, fastnas, video}. %, \cite{fastnas}, \cite{video}. 
% They model the search problem as a Markov Decision Process, where a Long Short-Term Memory controller (RL-agent) samples the architectural parameters of the networks and learns a policy from the validation accuracy of the networks to guide the search. 
% A controller acts as an RL-agent, which generate a sequence of tokens that encodes for the network hyperparameters. Candidate networks are then decoded and trained to use their validation accuracy as a reward to update the trainable weights of the LSTM.
RL-based NAS tends to converge slowly, as it prioritizes long-term reward.
Other approaches~\cite{DPC, Auto-DeepLab, SqueezeNAS} relax the architectural parameters in the search space into a continuous, differentiable form, as gradient descent converges faster. % than RL-based methods.
% Each architectural parameter is assigned a weight using a softmax distribution. The architectural weights are trained using gradient-descent to find competitive network structures in a shorter search time than RL-based methods.
It is difficult, however, to represent candidate cost, whether latency or FLOPs, in differentiable form.
Consequently, some approaches~\cite{selfadaptive, EMONAS, nsgaNet} use elitist GAs that consider Pareto-dominance in selection.
%\cite{AdaResU-Net}, \cite{HNAS}
%Elitist GAs maximize short-term reward, which aligns with our objective to reduce search time without necessitating the differentiability of the search objectives. 
We likewise employ NSGA-II.

%\subsection{Accuracy Evaluation}

Several approaches have been proposed to quickly evaluate the accuracy of candidate models. % in a reduced time. 
%Bae \enum{et al.} \cite{RONAS} and Xu et al. \cite{AutoSegNet} shared trained weights between candidates instead of training each one from scratch. 
Chen et al.~\cite{DPC} designed proxy networks, simplified variants of the candidates that are faster to train and representative of the original accuracy. 
Although this saves some time, they still require 2,590 GPU days.
Alternatively, we use weight sharing, adopting a supernet design space structure, as in~\cite{RONAS, enas, SqueezeNAS}; the candidates use the pre-trained weights of DL3+.
%Adopting pre-trained weights from DL3+ is relatively faster, as no candidates need to be trained during the search.
We do not fine-tune candidates during the search at all, dramatically reducing search time without misrepresenting candidate accuracy.

%Chen \emph{et al.} \cite{DPC}, and Shaw \emph{et al.} \cite{SqueezeNAS} use 
Early stopping, adopted by~\cite{DPC, SqueezeNAS}, assumes networks that train faster are more likely to perform better when fully-trained.
We adopt early stopping, but only during evaluation on the validation set. 
We observe that using a subset of the validation set reduces evaluation time without misrepresenting candidate accuracy.


%\subsection{TinyML}

TinyML~\cite{RAY20221595} refers to running machine learning on edge devices with strict resource constraints.
Prior work~\cite{MLSYS2021_a3c65c29,10.1145/3370748.3406588,9458520} has addressed many different tasks, but none so far have attempted semantic segmentation due to the size of intermediate feature maps and the limited memory of TinyML hardware. 
E.g., U-Net~\cite{unet} requires 61MB of RAM. 
This makes deployment on TinyML hardware like the GAP8, which typically has 8MB of RAM, impossible.
\cite{DBLP:journals/corr/abs-2104-14623} proposed a compact network for semantic segmentation on edge devices.
However, it uses MAC and parameter count, and weight memory, as proxies for cost.
%According to our research targeting the GAP8~\cite{8445101}, semantic segmentation networks require substantial memory for intermediate layer feature maps. 
% In our work, we measure task latency on the GAP8 and find efficient models that balance the trade-off between accuracy and latency.
We optimize a model that is too large for TinyML deployment to reduce its memory footprint, and improve inference latency, without substantially degrading accuracy.


\section{FMAS}

We present FMAS, a fast multi-objective NAS framework that uses an elitist genetic algorithm, NSGA-II~\cite{nsga2}, to search for efficient semantic segmentation networks. 
FMAS treats DeepLabV3+, illustrated in Figure~\ref{Computationally-Efficient DeepLabV3+ search space}, 
as a supernet, sub-sampling and reusing its pre-trained parameters.
% The basic structure of DeeplabV3+ with Modified Xception backbone is illustrated in Figure~\ref{Computationally-Efficient DeepLabV3+ search space}.
To further reduce the cost of network evaluation, candidates are evaluated on a subset of the validation dataset.
Only the final, Pareto non-dominated, candidates are fine-tuned using the complete training and validation sets.
We evaluate efficiency by counting parameters and FLOPs, and measuring inference latency on a TinyML platform, the GAP8~\cite{8445101}.

FMAS begins by sampling a population of $M$ model candidates from the design space.
%$M-1$ are derived from the pre-trained parameters of DeepLabV3+.
$M-1$ are derived from the structure of DeepLabV3+, and use its pre-trained weights.
The final model is DeepLabV3+ itself, ensuring that the initial population includes a high-accuracy model to accelerate the search.
\emph{No model training occurs at this time.}
The models are then efficiently evaluated, using a fraction of the original validation set.
The parents of the next generation are identified using NSGA-II's selection criteria (which has a strong preference for non-dominated candidates), then crossover and mutation are performed.
Once again, no training is necessary, only accelerated validation, as all network parameters are derived directly from DeepLabV3+.
This process repeats for $G$ generations; the final set of non-dominated models are fine-tuned for $E$ epochs.

\subsection{Subsaming the DeepLabV3+ Architecture}

We constrain the high-level network architecture of the candidates to the structure of 
% Xception-DeepLabV3+
DeepLabV3+~\cite{deeplabv3+}, using it as a supernet for architecture search.
We experiment with both Modified Xception and MobileNetV2 backbones to perform feature extraction~\cite{deeplabv3+}. % proposed in , as the backbone to perform feature extraction. 
Then, the DeepLabV3+ encoder-decoder structure classifies each pixel. %pixel-wise classification.
% The encoder uses atrous convolutions to capture the larger context of feature maps,  widening the receptive field of the filters without increasing the number of parameters.
The encoder is based on Atrous Spatial Pyramid Pooling (ASPP)~\cite{deeplab}, which harnesses filters with multiple atrous rates and combines their output feature maps to capture multi-scale features in objects and their context. 
The structure of the decoder is based on bi-linear up-sampling, concatenation, and convolution. 
% It produces the final prediction masks and recovers sharp object boundaries.

\subsubsection{DeepLabV3+ with Modified Xception}

Modified Xception 
%i an over-parameterized backbone structure. It consists of computationally-expensive repetitive blocks which we parameterize in our design space. It 
is based on the structure of the Xception~\cite{xception} network and exhibits improved image segmentation performance. 
%It also adds more batch normalization and ReLU activation layers after each 3x3 depth-wise convolution to improve the accuracy of the network.
% DeepLabV3+ \cite{deeplabv3+} has the second highest MIoU score on the PASCAL VOC 2012 benchmark, achieving an MIoU of 84.56\% on the test set, and the twenty-second highest MIoU score on the Cityscapes benchmark, achieving an MIoU of 82.10\% on the test set. 
% It is the only network that achieves SOTA accuracy for the widely-used PASCAL VOC 2012 and Cityscapes datasets, which makes it an appealing choice as a base structure in our search space design. 
%Modified Xception replaces the max-pool layers with depth-wise separable convolution layers.
Notably, it uses sixteen instead of eight middle flow blocks. %, as in Xception.
These computationally-heavy, repeated, structures lend themselves well to parameterized exploration.
%We focus our exploration on the Xception Middle Flow, shown in Figure \ref{Computationally-Efficient DeepLabV3+ search space}, which consists of 16 repeated blocks. 
We also notice that we can optimize other hyper-parameters that can: improve the accuracy, such as the atrous rates of convolutions; and, reduce the computational cost, such as convolution stride sizes; both without requiring additional trainable weights.

\begin{table}[b]
  \caption{Xception Hyperparameter Design Space}
  \label{The Search Space Opetarions}
  \begin{adjustbox}{center}
  \begin{tabular}{lc}
    \toprule
    Hyperparameter& Possible Values\\
    \midrule
    Xception Entry Flow Stride & 1, 2, 3, 4\\
    Xception Middle Flow Atrous Rate & 1, 2, 3, 4\\
    Xception Exit Flow Atrous Rates & (1, 2), (2, 4)\\
    ASPP Atrous Rates    & (6, 12, 18), (12, 24, 36)\\
    Xception Middle Flow Blocks & $( b_1, b_2, \ldots, b_{16} ), b_i \in \{0, 1\}$\\
  \bottomrule
  \end{tabular}
  \end{adjustbox}
  \end{table}

% \begin{table}[b]
% \centering
% \caption{SNAS Architectural Hyperparameters Design Space of Modified Xception backbone}
% \begin{adjustbox}{width=0.9\columnwidth, center}
% \begin{tabular}{|p{4.3cm}|p{2.6cm}|}
%     \hline
%     Hyperparameter & Value\\
%     \hline
%     \hline
%     Xception Entry Flow Stride & 1, 2, 3, 4\\
%     \hline
%     Xception Middle Flow Atrous Rate & 1, 2, 3, 4\\
%     \hline
%     Xception Exit Flow Atrous Rates & (1, 2), (2, 4)\\
%     \hline
%     ASPP Atrous Rates    & (6, 12, 18), (12, 24, 36)\\
%     \hline
%     Xception Middle Flow Blocks & 0 - 16\\
%     \hline
% \end{tabular}
% \end{adjustbox}
% \label{The Search Space Opetarions}
% \end{table}

Table~\ref{The Search Space Opetarions} lists our hyperparameter choices.
We optimize the computational cost of the model by selectively including blocks in the  Xception Middle Flow.
This is the most computationally-expensive section of DeepLabV3+: it requires 65G FLOPs (64\% of DL3+ FLOPs) and 23M parameters (56\% of DL3+ parameters). % <-- This is excellent
Each of the 16 blocks may be included ($b_i = 1$) or excluded ($b_i = 0$).
To create further opportunity to reduce the computational cost, we optimize the Xception Entry Block Stride Size. %, as it heavily affects the number of FLOPs.
We also search the atrous rates in the Xception Middle Flow, Xception Exit Flow, and ASPP, as they have the potential to improve segmentation accuracy by widening the receptive field of the images.

Since the middle flow blocks are repeated, the Xception Middle Block Atrous Rates are the same in all selected blocks.
We only search for Xception Entry Block Stride Size in the last convolution stage in the Xception Entry Flow to avoid having the output prediction masks heavily down-sampled by multiple stages of striding, which could result in bad accuracy for semantic segmentation.
%Each choice for the Xception Exit Flow has two constituent components: Xception Exit Flow Atrous Rate[0] and Xception Exit Flow Atrous Rate[1].
%Each choice for the ASPP Atrous Rates constitutes three components: ASPP Atrous Rate[0], ASPP Atrous Rate[1], ASPP Atrous Rate[2]. 
The choice sets for the atrous rates in the Xception Exit Block and the ASPP module, listed in Table \ref{The Search Space Opetarions}, are adopted from \cite{deeplabv2}.

Each network is represented with a fixed-length genome consisting of 22 bits, encoding five architectural hyperparameters (Table \ref{The Search Space Opetarions}).
Xception Entry Flow Stride and Xception Middle Flow Atrous Rate both have four design choices, each encoded in two bits.
Each of the Xception Exit Flow Atrous Rates and ASPP Atrous Rates has two sets of design choices, each encoded in one bit. 
Each of the remaining 16 bits indicates the presence (1) or absence (0) of a block of the Xception Middle Flow Blocks.
Any mutation at any bit position results in a legal configuration; likewise, cross-over of any two configurations results in a legal configuration.

\subsubsection{DeepLabV3+ with MobileNetV2}

DeepLabV3+ can also be implemented with MobileNetV2 as its backbone. 
The MobileNetV2~\cite{mobilenetv2} backbone uses fully convolution layers and 19 residual bottleneck layers for its feature extractor.
The 19 residual bottleneck layers are separated into five groups with increasing numbers of output channels, 24, 32, 64, 96, and 160 channels. %32 channels to 160 channels. 
Each group consists of several identical residual bottleneck layers.
% The intermediate residual bottleneck layer uses lightweight depthwise convolutions as the filters of feature map to bring in non-linearity.
Similar to the Modified Xception backbone, we reduce the network by parameterized exploration. 
We search for the dilation rates of depthwise convolutions of six selected residual layers. 
Then, we sampled the convolutional stride of the four selected residual layers, ensuring other non-selected layers are functional, like we do with the Modified Xception backbone.
We also focus our search on the number of repeated identical residual layers for each group. % that has different channel count.

Table~\ref{SNAS Architectural Hyperparameters Design Space of MobileNetV2 backbone} lists our hyperparameter choices for the MobileNetV2 backbone. 
We search for the parameters of the residual bottleneck layers in the five groups. 
The value of the number of layers of each group is selected between 1 and the original number of layers of that group. 
We also search for the stride, and the depthwise convolution dilation rate, of the selected residual bottleneck layers.

\begin{table}[b]
  \caption{MobileNetV2 Hyperparameters Design Space}
  \begin{adjustbox}{center}
  \label{SNAS Architectural Hyperparameters Design Space of MobileNetV2 backbone}
  \begin{tabular}{lc}
    \toprule
    Hyperparameter& Possible Values\\
    \midrule
    \nth{2} \& \nth{3} Layer Stride & 2, 3\\
    \nth{14} \& \nth{17} Layer Stride & 1, 2\\
    \nth{12}-\nth{14} Layer Dilation Rate & 1, 2\\
    \nth{15}-\nth{17} Layer Dilation Rate & 1, 2, 3, 4\\
    24-channel Group Layers & $( b_1, b_2), b_i \in \{0, 1\},\sum b_i>0$\\
    32-channel Group Layers & $( b_1, b_2, b_3 ), b_i \in \{0, 1\},\sum b_i>0$\\
    64-channel Group Layers & $( b_1, b_2, b_3, b_4 ), b_i \in \{0, 1\},\sum b_i>0$\\
    96-channel Group Layers & $( b_1, b_2, b_3 ), b_i \in \{0, 1\},\sum b_i>0$\\
    160-channel Group Layers & $( b_1, b_2, b_3 ), b_i \in \{0, 1\},\sum b_i>0$\\
  \bottomrule
  \end{tabular}
  \end{adjustbox}
  \end{table}



% \begin{table}[b]
% \centering
% \caption{SNAS Architectural Hyperparameters Design Space of MobileNetV2 backbone}
% \begin{adjustbox}{width=0.9\columnwidth, center}
% \begin{tabular}{|p{5.5cm}|p{2.6cm}|}
%     \hline
%     Hyperparameter & Value\\
%     \hline
%     \hline
%     % Residual Layer Stride & 1, 2, 3\\
%     % \hline
%     Layer 2 \& 3 Residual Layer Stride & 2, 3\\
%     \hline
%     Layer 14 \& 17 Residual Layer Stride & 1, 2\\
%     \hline
%     % Residual Layer Dilation Rate & 1, 2, 3, 4\\
%     % \hline
%     Layer 12-14 Residual Layer Dilation Rate & 1, 2\\
%     \hline
%     Layer 15-17 Residual Layer Dilation Rate & 1, 2, 3, 4\\
%     \hline
%     % Number of layers of each group & 1 - Maximum\\
%     % \hline
%     Number of layers of 24-channels group & 1 - 2\\
%     \hline
%     Number of layers of 32-channels group  & 1 - 3\\
%     \hline
%     Number of layers of 64-channels group & 1 - 4\\
%     \hline
%     Number of layers of 96-channels group & 1 - 3\\
%     \hline
%     Number of layers of 160-channels group & 1 - 3\\
%     \hline
% \end{tabular}
% \end{adjustbox}
% \label{SNAS Architectural Hyperparameters Design Space of MobileNetV2 backbone}
% \end{table}


\subsection{Evaluating the Accuracy of Candidates}

In order to reduce search time, we employ two strategies targeting candidate training and validation.
First, we adopt the pre-trained weights from DeepLabV3+ and share them with the matching layers of the candidates. 
For the Xception Middle Flow Blocks and Inverted Residual Layers, only the weights for the selected blocks are shared. % with the candidates. 
Changing the atrous rates, dilation rates, and stride size does not change the trainable weights in the model.
%%% BM: You need to describe further what this means in the context of DeepLabV3+, as well as in terms of your subsampling search space.
The intuition of this approach is that if a model is sub-sampled from a supernet, then sharing the network parameters of the supernet and fine-tuning can replace from-scratch training, as demonstrated by~\cite{enas, SqueezeNAS}.

%To fully train DeepLabV3+, Chen \emph{et al.} \cite{deeplabv3+} required 0.88 GPU days on average;
We find that adopting pre-trained weights from DL3+ saves us 0.88 GPU days compared to training a candidate from scratch.
%, and fine-tuning reduces training time of a network by 84\% to become 0.14 GPU days.
To further reduce evaluation time, we evaluate the candidates on a subset of the validation set. 
We observe that the MIoU error of the models starts to stabilize after being evaluated on 20\% of the validation set. 
Therefore, we use the first 20\% of the validation set. %which further reduces evaluation time compared to evaluating a candidate on the entire validation set by 0.014 GPU days. % contributes less significantly to reducing the search time, but helps search faster.
% Using the two above strategies, all design space search decisions (i.e., fitness evaluation and candidate selection in NSGA-II) are performed using pre-trained weights and a subset of the validation set.
%Using the two above strategies results in an aggregate time reduction of approximately 0.89 GPU days.
After the search is completed, the final Pareto non-dominated models are fine-tuned on the entire training set and re-evaluated on the entire validation dataset.


\section{Experimental Setup}

We conduct two experiments on both Xception and MobileNetV2 on PASCAL VOC to demonstrate how quickly FMAS can find computationally efficient alternatives to DeepLabV3+.
We run our experiments on a Tesla P100-PCIE-16GB GPU.
In the first, we measure the GPU time reduction that results when FMAS is applied to a population of 12 designs on PASCAL VOC 2012.  %to find efficient models in terms of MIoU error, and FLOPs, parameters count, or latency. 
We conduct three independent multi-objective searches, for MIoU error and one for each of the computational cost objectives: 
    FLOPs, parameter count, or latency.
%In each case, we search a design space with approximately 4 million alternatives and 8 million alternatives, all derived from DeepLabV3+ and using its pre-trained weights. 
In each case, we search a design space with approximately 4M alternatives for Xception and 8M alternatives for MobileNetV2, all derived from DeepLabV3+ and using its pre-trained weights.
For the MobileNetV2 backbone, we evaluate the network's inference latency on the GreenWaves GAP8 SoC using GVSoC~\cite{Bruschi_2021}.
In the second experiment, we fine-tune the final Pareto non-dominated models and re-evaluate their accuracy on the complete dataset after 20 and 25 generations for Xception and MobileNetV2 respectively.
(The appropriate number of generations to use was determined experimentally by quantifying the improvement in Hyperarea difference between Pareto-fronts from one generation to the next, and trading off solution improvement and search time.)
We compare the MIoU result before and after fine-tuning to determine how the MIoU of Pareto non-dominated models is affected by fine-tuning.
% It validates our hypothesis that SNAS may subsample the pre-trained weights of DL3+, and perform search without fine tuning, without affecting accuracy. 

%For the evaluation of both Modified Xception and MobileNetV2, we run our experiments on a Tesla P100-PCIE-16GB GPU.
% We measure the evaluation and fine-tuning time using a built-in feature in the Keras API~\cite{tf-etal} for functions $\mathit{fit}$ and $\mathit{evaluate}$, and multiply them by the number of evaluations to calculate the time of a full search. 
% This assumes other computations are negligible compared to evaluation and fine-tuning.
% GVSoC profiles applications on GAP8 processor, which is designed by GreenWave Technologies. 
% We then use the resulting inference latency as the model's computational cost. 
% GAP8 is an ultra-low-power processor that has limited memory. 
In order to fit into the limited memory and supported operations of the GAP8 processor, we (1) scale down the size of the input image to 384$\times$384 pixels, (2) prune the original five branches of the encoder to two branches, and (3) change some of the operations of the original model into GAP8-supported operations, including changing $\mathit{Conv2DTranspose}$ into $\mathit{UpSampling2D}$, and $\mathit{GlobalAveragePooling2D}$ into consecutive $\mathit{AveragePooling2D}s$. 
Together, these changes reduce MIoU by 4.44\% compared with DL3+; pruning encoder branches had the most significant effect on memory footprint. %but make the model memory-efficient. % and deployable on GAP8.
% We search with pre-trained weights and subset of the validation set to reduce searching time just like Modified Xception backbone. 
% Inference time and the evaluation time of each model. 
The inference on GAP8 and the evaluation on GPU are done in parallel to reduce the search time of the whole process.

\subsection{Candidate Evaluation Metrics} %Performance: Mean Intersection Over Union}

FMAS optimizes models to minimize mean intersection over union error (MIoU).
MIoU, adopted by~\cite{deeplabv3+} to evaluate DL3+, is widely used to benchmark the accuracy of semantic segmentation.
% We first compute the IoU for every semantic class by calculating the area overlap (true positives) between the predicted and ground truth labels over their union (true positives + true negatives + false positives).
% Then we average the IoU of all classes to calculate MIoU. 
%\subsection{Cost: FLOPS and Trainable Parameters}
In addition to minimizing error, FMAS jointly optimizes models to minimize one of three computational cost metrics.
We quantify the cost of candidate models analytically by measuring either (a) the number of floating-point operations (FLOPs), using keras-flops~\cite{tf-etal}, (b) the number of network parameters, using count\_params, or (c) the latency on the GAP8 processor.
% FLOPs are an approximation of inference latency \cite{DPC};
% the parameter count is an approximation of the required memory footprint and inference energy \cite{Template-Based}.

\subsection{PASCAL VOC 2012}

We conduct our experiments with PASCAL VOC 2012~\cite{PASCALVOC}.
PASCAL VOC is a widely used image segmentation dataset with annotated images for 20 object classes, and one background class.
The object classes fall into four categories: person, vehicle, animal, and indoor.
PASCAL VOC contains 1464 training images, and 1449 for validation.
We use the same label image encoding used by DL3+~\cite{deeplabv3+}. %, which was trained on all 20 object classes and the background class.
% We follow the same label image encoding outlined in \cite{deeplabv3+} for CEDL.
%The label images designed for PASCAL VOC 2012 have their pixels annotated directly (i.e., in gray-scale, with their class labels), making it easy to read their true labels from the pixel values without having to do class mapping.
We use $513 \times 513$ images~\cite{deeplabv3+} for Modified Xception and $384 \times 384$ for MobileNetV2 to fit into the GAP8's memory.

\begin{table*}[t]
    \centering
  %\caption{The performance of FMAS compared to other NAS methods in terms of the GPU time reduction on the PASCAL VOC 2012 validation set. Accuracy is reported both when evaluated on a subset of the validation set, and after fine-tuning and re-evaluation on the entire validation set. Prefixes F, P, FP stand for optimized for FLOPS, Parameters, and both.}
  %\caption{Cost and performance of Xception and derived models}
  \caption{Cost and performance with Modified Xception backbone and derived models}
  \label{NAS Search time}
% \begin{tabularx}{\textwidth}{X X X X X X X X X X X}\toprule
    \begin{tabularx}{\textwidth}{p{2.2cm}P{1cm}P{1cm}P{1.2cm}P{1.4cm}P{2.3cm}P{0.7cm}P{0.7cm}P{0.7cm}P{1.4cm}P{1.5cm}}\toprule
& \multicolumn{5}{c}{Xception Architecture Parameters} & \multicolumn{3}{c}{Cost}&\multicolumn{2}{c}{MIoU Error (\%)}
% & \multicolumn{5}{c}{$Architecture Parameters$} & \multicolumn{5}{c}{$costs & performances$}
\\\cmidrule(lr){2-6}\cmidrule(lr){7-9}\cmidrule(lr){10-11}
           & \small Entry Stride& \small Middle Atrous Rate&\small Exit Atrous Rate&\small ASPP Atrous Rate&\small Middle Blocks&\small GPU Days&FLOPs (G)&\small Params (M)&\small Validation Subset (\%) & \small Fine-tuned + Full Validation \\\midrule
        DeepLabV3+~\cite{deeplabv3+} &2&1&(1,2)&(6,12,18)&1111111111111111&-&101.47&41.26&23.14&22.71\\
        DPC~\cite{DPC}&-&-&-&-&-&2600&99.96&42.70&-&\textbf{19.15}\\
        FMAS-F1&3&1&(1,2)&(6,12,18)&1111111011011111&0.68&\textbf{57.88}&38.00&28.88&27.93\\
        FMAS-F2&2&1&(1,2)&(6,12,18)&1111111011001001&0.52&90.92&33.12&24.95&25.21\\
        FMAS-P1&2&1&(1,2)&(6,12,18)&1111011110010100&\textbf{0.49}&87.41&31.5&27.91&26.64\\
        FMAS-P2&2&1&(1,2)&(6,12,18)&1111111011011111&0.65&101.47&38.00&22.68&22.65\\
        FMAS-FP1&2&1&(1,2)&(6,12,18)&1111111011001101&0.68&94.44&34.75&23.77&24.38\\% 195 eval * 5 min
        FMAS-FP2&2&1&(1,2)&(6,12,18)&1111011010001100&0.80&83.89&\textbf{29.87}&29.72&29.29\\\bottomrule
    \end{tabularx}
  \end{table*}

\begin{table*}[t]
    \centering
    %\caption{The performance of FMAS on MobileNetV2 backbone. The accuracy is demonstrated both when evaluated on a subset of the validation set, and post-finetuning and re-evaluation on the entire validation set. }
    %\caption{Cost and accuracy of MobileNetV2 and derived models}
    \caption{Cost and accuracy with MobileNetV2 backbone and derived models}
    \begin{tabularx}{\textwidth}{p{2.3cm}P{1.8cm}P{1.8cm}P{1.6cm}P{0.7cm}P{0.7cm}P{0.7cm}P{1.2cm}P{1.8cm}P{1.8cm}}\toprule
    & \multicolumn{3}{c}{MobileNetV2 Architecture Paremeters} & \multicolumn{4}{c}{Cost}&\multicolumn{2}{c}{MIoU Error (\%)}
% & \multicolumn{5}{c}{$Architecture Parameters$} & \multicolumn{5}{c}{$costs & performances$}
\\\cmidrule(lr){2-4}\cmidrule(lr){5-8}\cmidrule(lr){9-10}
        & \small Stride& \small Inverted Layers Dilation Rate &\small Inverted Group Layers & \small GPU Days&FLOPs (G)&\small Params (M)&\small Latency (M Cycles)&\small Validation Subset & \small Fine-tuned + Full Validation \\\midrule
        MobileNetV2~\cite{mobilenetv2} &(2,2,1,1)&(2,2,2,4,4,4)&1111111111&-&9.73&2.14&2189&33.03&\textbf{32.61}\\
        FCN-VGG16~\cite{FCN} &-&-&-&-&243.50&134.49&-&-&37.70\\
        FMAS-L1&(2,2,1,2)&(2,2,1,3,4,2)&1111111111&1.46&7.88&2.14&2085&36.94&36.26\\
        FMAS-L2&(2,3,1,1)&(2,2,2,3,2,2)&1111111111&1.46&\textbf{4.62}&2.14&\textbf{1004}&40.61&40.22\\
    \bottomrule
    \end{tabularx}
    \label{MobileNetV2 Search time}
\end{table*}

% \subsection{Architectural Hyperparameters}

% In our experiments, the search space using the modified Xception backbone has over 4 million possible models and the search space using MobileNetV2 has over 8 million.
% The choices for each hyperparameters are summarized in Tables~\ref{The Search Space Opetarions} and \ref{SNAS Architectural Hyperparameters Design Space of MobileNetV2 backbone}.
% We constrain the upper bound of the Stride Size to four to avoid significant accuracy degradation.
% %, resulting from heavily down-sampling the output prediction mask. 
% We have set the upper bound value for the Xception Middle Block Atrous Rate to four, as we have experimentally observed no accuracy improvement beyond it.
% %We encode the Xception Middle Flow Blocks in a 16-bit vector and explore which combinations yield the highest performance.
% We adopt the values of the Xception Middle Block Atrous Rate, Xception Exit Block Atrous Rates, and the ASPP Atrous Rates from~\cite{deeplabv2}. %, based on their manual experimentation with atrous rates.

\subsection{Final Fine-tuning}

%Once NSGA-II has returned the final set of non-dominated models, we perform fine-tuning on each.
%Table \ref{The finetuning details} outlines the training details we used to fine-tune the final Pareto-dominant model. 
We fine-tune the final Pareto non-dominated networks until convergence, about 6-10 epochs, using a learning rate of 1e-6 to ensure that the re-used weights do not diverge, degrading performance.
%we do not use a specific number of epochs for all models. We observe, however, that the number of epochs falls in the range of 6-10. 
%We use a learning rate of 1e-6 to ensure the re-used weights do not diverge from their minima, degrading performance.
We use a batch size of eight, and adopt the remaining hyperparameters in~\cite{deeplabv3+}, with small modifications for stable training: we use a weight decay of 4e-5, and learning rate reduction factor of 0.94.


\section{Results}

\subsection{Reducing Training Time}

\begin{figure}[b]
\centering
    \begin{tikzpicture}[scale=0.65]
        \begin{axis}[
            enlargelimits=false,
            xlabel={FLOPs (G)},
            ylabel={Mean IoU Error (\%)},
            xmin=15, xmax=120,
            ymin=10, ymax=100,
        ]
        \addplot[
            color=cyan,
            dashed]
        table[x=flops, y=miou]
        {data/pascal/flops/gen1.tex};
        \addplot[
            color=cyan,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=flops, y=miou]
        {data/pascal/flops/gen1.tex};
        \addplot[
            color=blue,
            dashed]
        table[x=flops, y=miou]
        {data/pascal/flops/gen5.tex};
        \addplot[
            color=blue,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=flops, y=miou]
        {data/pascal/flops/gen5.tex};
        \addplot[
            color=black, 
            dashed]
        table[x=flops, y=miou]
        {data/pascal/flops/gen10.tex};
        \addplot[
            color=black,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=flops, y=miou]
        {data/pascal/flops/gen10.tex};
        \addplot[
            color=olive, 
            dashed]
        table[x=flops, y=miou]
        {data/pascal/flops/gen15.tex};
        \addplot[
            color=olive,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=flops, y=miou]
        {data/pascal/flops/gen15.tex};
        \addplot[
            color=purple, 
            dashed]
        table[x=flops, y=miou]
        {data/pascal/flops/gen20.tex};
        \addplot[
            color=purple,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=flops, y=miou]
        {data/pascal/flops/gen20.tex};
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (108.5051695, 23.14418554)
        };
        \node at (axis cs:110, 16){\small DL3+};
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (57.876716197, 28.877228498458862)
        };
        \node at (axis cs:57.876716197, 21){\small FMAS-F1};
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (90.924875909, 24.947768449783325)
        };
        \node at (axis cs:90.924875909,17){\small FMAS-F2};
        \legend{{}{Gen 1},{},{}{Gen 5},{},{}{Gen 10},{},{}{Gen 15}, {}, {}{Gen 20}}
        \end{axis}
    \end{tikzpicture}
    \caption{The MIoU-FLOPs Pareto front developed over 20 generations by FMAS on PASCAL VOC 2012.} % to discover networks with low MIoU error and FLOPs.}
    \label{SNAS-FLOPs on PASCAL}
\end{figure}


\begin{figure}[t]
\centering
    \begin{tikzpicture}[scale = 0.65]
        \begin{axis}[
            enlargelimits=false,
            xlabel={Trainable Parameters (M)},
            ylabel={Mean IoU Error (\%)},
            xmin=15, xmax=45,
            ymin=10, ymax=100,
        ]
        \addplot[
            color=cyan,
            dashed]
        table[x=params, y=miou]
        {data/pascal/params/gen1.tex};
        \addplot[
            color=cyan,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=params, y=miou]
        {data/pascal/params/gen1.tex};
        \addplot[
            color=blue,
            dashed]
        table[x=params, y=miou]
        {data/pascal/params/gen5.tex};
        \addplot[
            color=blue,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=params, y=miou]
        {data/pascal/params/gen5.tex};
        \addplot[
            color=black, 
            dashed]
        table[x=params, y=miou]
        {data/pascal/params/gen10.tex};
        \addplot[
            color=black,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=params, y=miou]
        {data/pascal/params/gen10.tex};
        \addplot[
            color=olive, 
            dashed]
        table[x=params, y=miou]
        {data/pascal/params/gen15.tex};
        \addplot[
            color=olive,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=params, y=miou]
        {data/pascal/params/gen15.tex};
        \addplot[
            color=purple, 
            dashed]
        table[x=params, y=miou]
        {data/pascal/params/gen20.tex};
        \addplot[
            color=purple,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=params, y=miou]
        {data/pascal/params/gen20.tex}; 
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (41.258213, 23.14418554)
        };
        \node at (axis cs:42.258213, 18){\small DL3+};
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (31.495733, 27.911359071731567)
        };
        \node at (axis cs:30.495733, 21){\small FMAS-P1};
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (38.004053, 22.683227062225342)
        };
        \node at (axis cs:38.004053,14){\small FMAS-P2};
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (29.868653, 29.715323448181152)
        };
        \node at (axis cs:26.868653,26){\small FMAS-FP1};
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (34.749893, 23.773711919784546)
        };
        \node at (axis cs:34.749893,18){\small FMAS-FP2};
        \legend{{}{Gen 1},{},{}{Gen 5},{},{}{Gen 10},{},{}{Gen 15}, {},{}{Gen 20}}
        \end{axis} 
    \end{tikzpicture}
    \caption{The MIoU-parameters Pareto front developed over the course of 20 generations by FMAS on PASCAL VOC 2012.} % to discover networks with low MIoU error and Params.}
    \label{SNAS-Params on PASCAL}
\end{figure}


\begin{figure}[t]
\centering
    \begin{tikzpicture}[scale = 0.65]
        \begin{axis}[
            enlargelimits=false,
            xlabel={Latency (M Cycles)},
            ylabel={Mean IoU Error (\%)},
            xmin=50, xmax=2500,
            ymin=10, ymax=100,
        ]
        \addplot[
            color=cyan,
            dashed]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen1.tex};
        \addplot[
            color=cyan,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen1.tex};
        \addplot[
            color=blue,
            dashed]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen5.tex};
        \addplot[
            color=blue,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen5.tex};
        \addplot[
            color=black, 
            dashed]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen10.tex};
        \addplot[
            color=black,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen10.tex};
        \addplot[
            color=olive, 
            dashed]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen15.tex};
        \addplot[
            color=olive,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen15.tex};
        \addplot[
            color=gray, 
            dashed]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen20.tex};
        \addplot[
            color=gray,
            only marks,
            mark=halfcircle,
            mark size=2pt]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen20.tex}; 
        \addplot[
            color=purple, 
            dashed]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen25.tex};
        \addplot[
            color=purple,
            mark=halfcircle,
            mark size=2pt,
            dashed]
        table[x=latency, y=miou]
        {data/pascal/Latency/gen25.tex}; 
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (2085.645316, 36.94)
        };
        \node at (axis cs:2085.645316, 28.94){\small FMAS-L1};
        \addplot[only marks, red, mark=*, mark size=2pt] coordinates {
        (1004.415432, 40.61)
        };
        \node at (axis cs:1004.415432, 33.61){\small FMAS-L2};
        
        \legend{{}{Gen 1},{},{}{Gen 5},{},{}{Gen 10},{},{}{Gen 15}, {},{}{Gen 20},{},{}{Gen 25}}
    \end{axis}  
    \end{tikzpicture}
    \caption{The MIoU-latency Pareto front developed over the course of 25 generations by FMAS on PASCAL VOC 2012.} % to discover networks with low MIoU error and Params.}
    \label{SNAS-Latency on PASCAL}
\end{figure}


Figures \ref{SNAS-FLOPs on PASCAL}, \ref{SNAS-Params on PASCAL}, and \ref{SNAS-Latency on PASCAL} plot the MIoU error of the Pareto non-dominated front of the corresponding generation against the FLOPs count, network parameters count, and latency respectively. 
We explore the capacity of FMAS to cut GPU time by evaluating a total of 240 modified Xception variants developed over 20 generations targeting FLOPs, and parameters; and a total of 300 MobileNetV2 variants developed over 25 generations targeting latency.

Table~\ref{NAS Search time} reports the network structures, GPU time consumption, computational cost, accuracy evaluated on a subset of the validation set, and post-fine-tuning accuracy on the entire validation set of selected networks using the Xception backbone.
%Table~\ref{NAS Search time} shows how SNAS cuts the GPU time of finding efficient networks compared to DPC~\cite{DPC}.
It presents a selected subset of the final Pareto models produced from Figures~\ref{SNAS-FLOPs on PASCAL} and \ref{SNAS-Params on PASCAL}, and from their combined Pareto fronts, in comparison with DPC~\cite{DPC}.
%Prefixes F, P, FP in Table \ref{NAS Search time} stand for optimized for FLOPS, parameters, and FLOPS and parameters, respectively.
%As shown in Table \ref{NAS Search time}, 
We observe that FMAS can reproduce the baseline accuracy of DL3+~\cite{deeplabv3+}, achieving MIoU errors of 23\% (e.g., FMAS-FP1), compared to a reported error of 21\% on the validation set.
%Chen~\emph{et al.} improve the MIoU error to 15\% by performing heavy data augmentation, such as left-right flipping the inputs and using multi-scale inputs during evaluation. 
% In this section, we report the post-fine-tuning accuracy of the networks, unless otherwise stated.

Similar to Table~\ref{NAS Search time}, Table~\ref{MobileNetV2 Search time} reports results when using the MobileNetV2 backbone and searching for 25 generations.
%We demonstrate the architecture parameters of the original MobileNetV2 and our selected networks. 
%We compare FLOPs count, parameters count, and inference latency on GAP8 between the original model and the selected models. 
In addition to FLOPs and parameters, we also report inference latency on the GAP8 for the original model, FCN-VGG16, and selected search results.
%The FCN-VGG16 is added as a baseline model to compare the FLOPs count, and parameters count. 
Note that while FCN-VGG16 uses only GAP8-supported operations, making it a suitable baseline, it requires more than $8\times$ more RAM than the GAP8 has, and therefore cannot be deployed. 
%At last, we compare the MIoU performance of all these models.
% As shown in \ref{MobileNetV2 Search time}, SNAS managed to cut the Latency of DL3+ with MobileNetV2 by $50\%$


\subsubsection{Minimizing MIoU Error and FLOPs}

%Figure \ref{SNAS-FLOPs on PASCAL} shows how SNAS discovers efficient networks in a small GPU time.
Figure~\ref{SNAS-FLOPs on PASCAL} illustrates the development in the Pareto front with respect to MIoU error and FLOPs over 20 generations.
DL3+ indicates the performance of the baseline with respect to MIoU and FLOPs.
FMAS-F1 and FMAS-F2 are additionally highlighted because of the trade-offs they represent; Table~\ref{The Search Space Opetarions} reports their hyperparameters.
%We show the effectiveness of SNAS in finding networks, such as SNAS-F1 and SNAS-F2 in little GPU time.
FMAS-F1 cuts the number of FLOPs by 43\% with respect to DL3+, and network parameters by 7.9\%, for a relative increase of 5.2\% in MIoU error; it was discovered in 0.68 GPU days (generation 17).
FMAS-F2 trades off only 2.5\% of the MIoU error of DL3+ for reducing FLOPs by 10\%, and network parameters by 20\%, in 0.52 GPU days (generation 13).

Models in Table~\ref{NAS Search time} required between 0.49 and 0.8 GPU days to be discovered by FMAS, which is negligible compared to the 2,590 GPU days required by DPC~\cite{DPC}.
Although DPC outperforms the MIoU of FMAS-F2 by 6.1\%, FMAS-F2 cuts FLOPs and parameters by 9 and 22\% respectively in only 0.65 GPU days.

%SNAS is able to optimize the structure of DL3+ supernet for computational cost and also achieve a competitive accuracy for some networks in a short search time, compared to other NAS approaches. 
% SqueezeNAS~\cite{SqueezeNAS} and TASC~\cite{Template-Based} have not been applied to PASCAL VOC 2012. 
% However, w
We discover FMAS-F2 in 3.5\% the time required to find SqueezeNAS MAC XLarge~\cite{SqueezeNAS}, and 3.1\% of the time required to discover arch0 and arch1 by Nekrasov~\emph{et al.}~\cite{Template-Based}.

\subsubsection{Minimizing MIoU Error and Network Parameters}

Figure~\ref{SNAS-Params on PASCAL} illustrates the development in the Pareto front with respect to MIoU error and network parameters over 20 generations.
%Similarly, Figure \ref{SNAS-Params on PASCAL} demonstrates the effectiveness of SNAS in cutting the GPU time required to find good designs in MIoU error and network parameters. 
In only 0.49 GPU days (generation 12), we find FMAS-P1, which cuts parameters by 24\%, and FLOPs by 14\% for a relative MIoU error increase of 4\%.
FMAS-P2 slightly outperforms the accuracy of DL3+, achieving an MIoU error of 23\%, and also reduces parameters by 7.9\% in 0.65 GPU days (generation 16).
Table~\ref{The Search Space Opetarions} reports their hyperparameters.

We further observe that optimizing for MIoU error and network parameters also produces competitive designs optimized for error, FLOPs, and parameters.
FMAS-FP1 and FP2 were discovered during this search; FMAS-FP1 slightly increases the MIoU error of DL3+ by 1.7\% for a reduction of 6.9\% in the FLOPs count and 16\% in the network parameters count in 0.68 GPU days (generation 12).
FMAS-FP2 reduces FLOPs by 17\% and the parameters by 28\% for an MIoU degradation of 6.6\% in 0.8 GPU days (generation 20).
% We attribute this to the fact that the only search hyperparameter that affects the parameter count is the number of Xception Middle Flow Blocks, so it appears relatively easier and faster to optimize MIoU error with respect to network parameters.

% \subsubsection{Minimizing MIoU Error, FLOPs, and network parameters}

% We have further combined the final Pareto networks of the 20th generation of Figures~\ref{SNAS-FLOPs on PASCAL} and~\ref{SNAS-Params on PASCAL} to show the  designs that we been able to quickly find optimized for all the three objectives. 
% We observe that analyzing SNAS-FP1 and SNAS-FP2, which were discovered during the search for low MIoU and network parameters, also achieve interesting trade-offs with respect to FLOPs with no additional search to increase GPU time. 
% Network SNAS-FP1 slightly increases the MIoU error of DL3+ by 1.7\% for a reduction of 6.9\% in the FLOPs count and 16\% in the network parameters count in 0.68 GPU days (Generation 12). 
% SNAS-FP2 reduces the FLOPs by 17\% and the parameters by 28\% for an MIoU degradation of 6.6\% in 0.8 GPU days (generation 20). 

% Both designs have been discovered during the search that optimizes network parameters, as we find it relatively easier to find high-accuracy networks that significantly cut the network parameters than to do so using FLOPs. 
% We attribute this to the fact that the only hyperparameter in Table \ref{The Search Space Opetarions} that affects the network parameters is Xception Middle Flow Blocks, so it appears relatively easier and faster to optimize MIoU error with respect to network parameters.

\subsubsection{Minimizing latency and network parameters}

Figure~\ref{SNAS-Latency on PASCAL} illustrates the development of the Pareto front with respect to MIoU error and latency on the GAP8 over 25 generations. 
In generation 24, we find FMAS-L1 and FMAS-L2.
FMAS-L1 cuts the latency of MobileNetV2 backbone DeepLabV3+ by 4.7\%, 
FMAS-L2 by 54.1\%.
% Both are sampled at the 22th generation (after 1.46 GPU days). 
% It needs to be mentioned that this number mainly comes from the deployment on GAP8 since the deployment time dominates the evaluation time. 
Note that most of this search time is spent compiling for and deploying to the GAP8 for inference latency measurement.
Table~\ref{MobileNetV2 Search time} reports their hyperparameters, FLOPs, parameters, and MIoU error. 
We compare our model with the FCN~\cite{FCN} in terms of the FLOPs and Params. 
While FCN's operations are fully supported by the GAP8, it requires too much memory to be deployed. 
% FMAS-L1 and FMAS-L2 are 30$\times$ and 50$\times$ smaller in terms of FLOPs and 60$\times$ smaller in terms of parameters. 


% \subsection{Population Size and Generation Count}

% For the Modified Xception backbone, we evaluated the performance of the Pareto fronts with different population sizes and number of generations using Hyperarea Difference (HD)~\cite{okabe2003critical}. Hyperarea Difference (HD) is a metric that compares the area dominated between two Pareto fronts and a fixed point just outside maximum IoU error and maximum cost evaluation metric.
% We have experimented with population sizes between 10 and 20.
% Comparing the HD values of the same generation but with different population sizes, a population of 20 has better metrics and dominates most of the other population size Pareto fronts. 
% Larger population sizes have a bigger chance of having a good design in a generation.
% However, population 10 has the best improvement from one generation to another with a mean HD value of 0.1157 over 10 generations.
% Higher HD values indicate that the dominated region increases, confirming that good designs are consistently generated.
% To maximize the evaluation of good designs and the exploration of new designs, we chose a population size of 12.
% From one generation to the next, it has a mean HD value of 0.0953 over 10 generations, which is better than the 0.0738 of the population of size 20 and it dominates the Pareto fronts of the same generation of lower population sizes.

% Experimenting with number of generations between 5 and 25, the HD values decrease continuously after the eighth generation, implying improvement falls off at this point.
% After generation 20, the HD value of the future against the current Pareto front stabilizes to a value of 0.0261.
% Adding generations will not improve the Pareto front significantly and will result in wasted search time.


\subsection{Multi-Objective NAS without Fine-Tuning}

In the second experiment, we have selected the final Pareto non-dominated networks that demonstrate the highest accuracy for fined-tuning and re-evaluation on the entire validation set. 
We selected the eight highest-accuracy networks for fine-tuning from Figures~\ref{SNAS-FLOPs on PASCAL},~\ref{SNAS-Params on PASCAL} and the two networks from Figures~\ref{SNAS-Latency on PASCAL}.

After fine-tuning and re-evaluating candidates on the complete validation set, we observed that FMAS either under-estimates or over-estimates the MIoU error by a value that falls in the range between -0.95\% and +1.3\%, as shown in Table~\ref{NAS Search time} and Table~\ref{MobileNetV2 Search time}. 
Such minor changes in accuracy show how FMAS can reliably and quickly evaluate the accuracy of networks sub-sampled from a supernet, without having to fine-tune candidates during the search.

FMAS can bias search toward designs that perform better without training, but it has proven effective in finding efficient designs (e.g. FMAS-F2) in a competitive search time compared to existing multi-objective NAS methods (e.g. DPC and SqueezeNAS). 
Therefore, discarding designs that could potentially perform better is still a worthwhile trade-off given the advantage in search time reduction.


\subsection{Discussion}

Considering the 240 network evaluations and eight fine-tuned networks for each search, FMAS ran each complete search in three GPU days, instead of 220, resulting in an aggregate GPU time reduction of 99\%.
We attribute most of this reduction to re-using the pre-trained weights of DL3+, which eliminates 0.88 GPU days per candidate, saving 215 days.
Evaluating a candidate on the first 20\% of the validation set cuts its evaluation time by 0.014 GPU days. 
For a complete search, this cuts evaluation time by three GPU days.
Fine-tuning each of the final Pareto non-dominated candidates requires approximately 0.14 GPU days.
%, cutting approximately 85\% of the training time required to train a candidate from scratch.
%Compared to saving 215 GPU days from not having to fine-tune, evaluating on a subset contributes less search time reduction.

% However, it is still useful in running our experiments faster and very competitive compared to existing NAS approaches that require 14 and 16 GPU days, as in SqueezeNAS and TASC, respectively.

Several patterns emerge in the hyperparameters selected by the Pareto non-dominated models in our experiments.
Analyzing the Xception Middle Flow Blocks of the final models optimized for FLOPs and MIoU (Figure \ref{SNAS-FLOPs on PASCAL}), blocks 1, 9 and 13 have at least a 90\% chance of being active, whereas block 10 was always absent.
% Similarly, the final models optimized for parameters and MIoU (Figure \ref{SNAS-FLOPs on PASCAL}), have blocks 3, 6 and 9 present at least 90\% of the time, and no particular blocks tend to be absent.
%The rest of the blocks are between 40\% and 70\% present. 
% This supports our intuition that the earlier blocks, which extract coarse features, e.g., the outlines of objects, are more important for accuracy. % <-- is this true?
%capture the high-level contextual features and the latter capture the fine-grained features.
Increasing the Xception Entry Block Stride Size reduces FLOPs, as well as accuracy, compared to other hyper-parameter choices.
The structures of FMAS-F1 and FMAS-P2 are identical except for stride; FMAS-F1 cuts FLOPs by 44\% compared to FMAS-P2, for a relative increase in the MIoU error of 5.2\%. 
The networks in Table~\ref{NAS Search time} adopt the smaller atrous rates for Xception Middle Flow, Xception Exit Flow, and ASPP. 
% It appears that using larger dilation compromises fine details, resulting in loss of accuracy. % <-- does that sum it up? 
We observe that the highlighted networks using the MobileNetV2 backbone, FMAS-L1 and FMAS-L2, use all inverted residual blocks. 
%The first and third stride are unchanged relative the original, whereas the second and forth stride are more likely to be set larger. 
% There is some variation in stride, relative to the original, and the dilation rate, especially of later layers, is generally chosen to be smaller.
The dilation rates of the later layers, e.g. $\nth{15}$ and $\nth{17}$ layer, are more likely to be smaller to reduce inference latency. %, because of the fact that FMAS-L1 and FMAS-L2 have all their $\nth{15}$ and $\nth{17}$ layer's dialation rate smaller than DL3+'s.
%The dilation rate of the first two layers is unchanged as well, but for the last four layers are very likely to be smaller.
% Optimizing the dilation rate of the inverted residual layers also contributes more to the reduction of inference latency while maintaining accuracy, due to the fact that FMAS-L1 and FMAS-L2 have smaller dilation rates for many of their layers, but keep the blocks unchanged.
% Changing the rate of the inverted residual block is more likely to reduce inference time without degrading accuracy, compared to removing some of the blocks. 
%The stride and dilation rate value of certain layer is more stable than other layer when trying to prune the network.


\section{Conclusion}

We present FMAS, a multi-objective NAS framework that significantly reduces the search time for finding efficient semantic segmentation networks. 
FMAS uses NSGA-II to sub-sample candidates from a DeepLabV3+ supernet and improve their performance over the course of generations through crossover and mutation.

We reduce GPU time by re-using the pretrained weights of DL3+ for candidates, and evaluating accuracy on a subset of the validation set. 
We fine-tuned only the final Pareto non-dominated models.
This saved 0.88 GPU days of training time per candidate, and an additional 0.014 GPU days, respectively.
Adding the time required for fine-tuning (0.14 GPU days each), FMAS saved 99\% of the 220 GPU days required to run the entire search.
%In the process we discovered advantageous trade-offs, e.g., one model which reduces DL3+ FLOPs by 10\% and network parameters by 20\% for a relative MIoU error increase of 2.5\%;it was encountered 0.5 GPU days into one search experiment.
We also demonstrate that no fine-tuning is required during search. 
We found that the accuracy of the discovered networks before and after fine-tuning differs from -0.95\% to +1.3\%, an inconsequential difference resulting in substantial time savings.
Besides, we prove that our method can be generalized to different networks and different performance metrics by applying FMAS to DeepLabv3+ with two backbones and three metrics. 
The search results of two backbones show that FMAS is capable of finding latency-efficient models.

\section{Acknowledgements}

This research was made possible by the support of: the Natural Sciences and Engineering Research Council of Canada (NSERC), though grant number CRDPJ 531142-18; and, Synopsys Inc.
\clearpage
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-tinyml}
% \input{\sample-tinyml.bbl}
% \bibliography{sample-base}

\end{document}
\endinput
%%
%% End of file `sample-tinyml.tex'.
