We evaluate our method on the task of imitating an expert in an autonomous driving task. Below, we provide details on our simulator setup, baselines, and metrics for evaluation.

\subsection{Simulator and Experimental Design}
We use the CARLA simulator~\cite{coilICRA2018}, a high-fidelity autonomous driving simulator that allows for the adjustment of a number of environmental factors, such as the positions and behaviors of other agents in the environment, the layout of roads, and the presence of obstacles. For our experimental domain, we consider imitating an expert's behavior in a four-way intersection, where the expert, serving as the ego vehicle, proceeds through the intersection to reach a destination.

In this setting, another vehicle (the \emph{ado}) may be present and rushing through the intersection, ignoring traffic laws.
The ego's trajectory will be affected by the conditions of the intersection environment, which we define as the following parameterized variables: First, the ado vehicle may be spawned at any other side of the intersection (either opposite the ego or to either side of the ego), within a range of varying distances from the intersection. The ado may also perform one of three maneuvers at the intersection: a right turn, a left turn, or proceed straight.
The ego's initial distance from the intersection, as well as the ado vehicle's maximum and minimum speeds, are also variable environment condition parameters. The ado's behavior is controlled by a simple programmatic controller that accelerates the ado to its maximum speed as it traverses the intersection. We vary all of our parameters in order to sample new environments in our data aggregation procedure. Our experimental setup is presented in Fig.~\ref{fig:experimental_domain}.

\begin{figure}
     \centering
\includegraphics[width=.8\linewidth]{imgs/RoadDiagram-Variability.pdf}
     \caption{Our experimental setup in the CARLA simulator. We outline the environment parameters that can vary our scenario.}
     \label{fig:experimental_domain}
\end{figure}
\vspace{-0.3mm}
We define a set of properties in Signal Temporal Logic (STL), each with its own quantitative semantics to measure how close to satisfaction each property is. Our property set, $LP$, for our experiments are as follows:
\begin{enumerate}
    \item \textbf{Avoid collisions.} This property is falsified if we detect the ego vehicle has collided with any object in the environment. We define the quantitative semantics of this property as the minimum distance between the ego vehicle and the ado vehicle during a trajectory. We treat all collisions as the same.
    \item \textbf{Do not halt.} This property is falsified if the ego vehicle comes to a complete stop during the trajectory. The quantitative semantics of this property are measured by the minimum speed the ego reaches during a trajectory.
    \item \textbf{Do not abruptly brake.} This property is falsified if the ego vehicle brakes with an intensity beyond a certain pre-defined threshold (set to 40 percent of the ego's maximum braking capacity). The quantitative semantics of this property are measured by the maximum braking value the ego applies during a trajectory. 
\end{enumerate}
% Our method uses Bayesian Optimization with Gaussian processes as the optimization method when sampling for specific target specifications in our Environment Sampling step.

We compare SGDA with the following environment sampling methods as baselines:

\begin{enumerate}
    \item \textbf{Uniform Prior Sampling}: In order to aggregate more data from the expert, we sample from the environment's prior (for which we set all parameters to be a uniform distribution) and use these samples to collect additional trajectories from the expert for training in each round.
    \item \textbf{Single Specification-Falsification}: Rather than using our STP to sample environments, we instead define a single specification that indicates an outcome of interest and sample for environment conditions that falsify this specification. We set this specification to be the conjunction of all the original (non-negated) properties listed above, which describes `typical' driving behavior for the ego.
    \item \textbf{Individual Property Importance Sampling}: Without the partition created in our SGDA method, an alternative that makes use of all properties in our set would be to sample for the falsification of each property individually (independently of one another). We split up the number of environments to aggregate by the number of properties and importance sample for each, taking an approach for each property similar to that used in~\cite{OKellySNTD18} with a fixed budget of samples.
\end{enumerate}

We measure the efficacy of our approach through a set of quantitative metrics that measure both how \textit{semantically} similar our imitations are to the expert, as well as how \textit{physically} similar expert and imitation trajectories are: 
%We collect a test dataset of expert trajectories and record the environment conditions for each trajectory in order to measure these quantities, which we list as the following:
\begin{enumerate}
    \item \textbf{Outcome Matching:} Similar to the environment selection method described in Section~\ref{sec:spec_guided_data_agg}, we measure how often the expertâ€™s trajectory satisfies the same specification, or \emph{outcome}, as our learned model's trajectory under the same $e$.
    \item \textbf{Trajectory-Length Distance:} 
    %While measuring a test set loss is valuable, it loses out on an important piece of context: a small change in action under the same state may lead to two highly different subsequent states, which may in turn lead to two highly different trajectories.
    To measure trajectory-level differences, we use Dynamic Time Warping (DTW) distance to compare trajectories between our expert and our learned models under the same $e$.
    \item \textbf{Test Set Loss:} %How do our learned model's actions on the same states compare to that of the expert? 
    We use an L1 Loss over a test set of states and actions collected from our expert to measure how our learned models' actions compare to that of the expert under the same states.
\end{enumerate}

We evaluate our method by imitating two different experts: the CARLA built-in autopilot vehicle controller (AP), and a neural-network controller trained on data collected from a human driver who operated in our environment (NN). The neural-network expert controller was trained on two hours of data collected by having a human operate a steering wheel and pedal to drive the ego car in simulation through the intersection repeatedly. The human was instructed to proceed through the intersection as quickly as possible while avoiding collisions, if possible. For the autopilot, we conducted two additional experiments for two additional maneuvers: taking a right turn or a left turn at the intersection. In the case of the autopilot right- and left-turn experiments, we only learn an imitation for the throttle component of the maneuver and let the expert policy control the steering to avoid obscuring the results of imitation with simultaneously acquiring a challenging lateral control skill in a data-constrained setting. 

We use the Behavioral Cloning (BC) algorithm to learn an imitative policy of the expert, where the state space consists of the ego vehicle's information (location, velocity, acceleration), and a noisy estimate of the ado's location and speed if the ado is within the line of sight from the ego vehicle.

 We collect an initial expert dataset of 200 trajectories (which corresponds to roughly 90,000 state-action pairs on average), with environment conditions sampled from the simulator's uniform prior over all environment parameters. Subsequent rounds of data aggregation are 100 episodes each. In the cases of SGDA, the Single-Specification baseline, and the Individual Property Sampling baseline, 200 samples are generated in the sampling step, and 100 samples are selected to use for aggregation. Of these 200 samples, the first 100 are collected uniformly at random to seed our optimization-guided falsifier(s) for each specification. In the case of SGDA, the subsequent 100 samples are collected using Algorithm~\ref{ecs_alg}. In our baselines, those 100 samples are either used to falsify the specification (in our Single-Specification baseline) or equally divided amongst each of our properties and importance sampled (in the case of the Individual-Properties baseline). Our optimization-guided falsifiers use Bayesian Optimization to learn a distribution over the environment parameters, where the target value provided is the quantitative evaluation of the STL specification the optimizer aims to falsify. On top of this target value, we add a term to incentivize exploration and prevent the distribution from centering too much probability mass on a small region of the parameter space (details in the appendix). We report results after two rounds of data aggregation for each method.


\section{Results}
In this section, we evaluate performance against our baselines to see if SGDA produces a more semantically similar imitation to our expert (Fig.~\ref{fig:outcomematch_results} and Table~\ref{tab:outcome_match_percentages}), if trajectories generated by SGDA resemble the experts' more closely (Fig.~\ref{fig:dtw_results}), and if SGDA achieves competitive test-set loss (Table~\ref{tab:loss_values}).

We evaluated the performance of each method on a test dataset collected from each expert in each maneuver. In collecting a test set, we aimed to not overrepresent unlikely outcomes in our dataset, while ensuring that every outcome was at least marginally represented. So as to not bias the test dataset towards SGDA, which explicitly samples for underrepresented specifications, we collected a dataset of 500 trajectories from the expert for each experiment, sampling from a uniform distribution over all environment parameters. To ensure a minimal representation of each specification, we first performed rejection sampling for each specification until trajectories that satisfied a given specification represented at least 
20 trajectories (four percent of the overall dataset).
%A small number of trajectories in each test dataset were discarded due to issues in simulator execution.



% \begin{table}[]
%     \centering
%     \begin{tabular}{|c|c|c|c|}
%         \hline
%         \textbf{NN-Expert} & Avg. Traj. Dist. & Outcome Match Rate & Test Loss \\
%         \hline
%         SGDA & 19 & 2 & 100 \\
%         \hline
%         Uniform & 19 & 2 & 100 \\
%         \hline
%         Single-Spec & 53 & 7 & 300 \\
%         \hline
%         Individual-Props & 22 & 6 & 25 \\
%         \hline
%     \end{tabular}
%     \vspace{2mm}
%     \caption{Results from 2 rounds of data aggregation for models imitating the Human-Data Learned Expert on the straight maneuver.}
%     \label{tab:primary_results}
% \end{table}




\begin{figure}
     \centering
\includegraphics[width=0.8\linewidth]{imgs/ocm_all_sns_rec.png}
     \caption{Results for Outcome Matching rates for imitations of the maneuvers performed by our experts. AP refers to Autopilot controlled maneuvers, and NN refers to maneuvers controlled by our neural network expert. The SGDA method outperforms all baselines in learning a more semantically accurate imitation.}
     \label{fig:outcomematch_results}
\end{figure}

% \hspace{-.05mm}
\begin{table*}[]
    \normalsize
    \centering
    \caption{Number of samples for each specification in the expert test dataset and outcome match rates for each baseline in the NN-expert straight maneuver experiment. SGDA outperforms baselines in most outcome-specific accuracy and achieves comparable accuracy in all outcomes.}
    \label{tab:outcome_match_percentages}
    \begin{tabular}{ccc|c|cccc}
        \toprule
        (1) Avoid & (2) Do not & (3) Do not & & \multicolumn{4}{c}{\textbf{NN-Straight Match Rates}} \\
        Collisions & Halt & Abruptly Brake & \# Outcomes & SGDA & Ind. Props & Single-Spec & Uniform \\
        \midrule
        \cmark & \cmark & \cmark & 197 & 94\% & 96\% & \textbf{97\%} & 95\% \\
        % \hline
        \cmark & \cmark & \xmark & 20 & \textbf{50\%} & 40\% & 0\% & 0\% \\
        % \hline
        \cmark & \xmark & \cmark & 44 & \textbf{52\%} & 41\% & 14\% & 23\% \\
        % \hline
        \cmark & \xmark & \xmark & 65 & 78\% & 62\% & \textbf{86\%} & 57\% \\
        % \hline
        \xmark & \cmark & \cmark & 20 & \textbf{95\%} & 85\% & 70\% & 50\% \\
        % \hline
        \xmark & \cmark & \xmark & 20 & \textbf{45\%} & 0\% & 0\% & 0\% \\
        % \hline
        \xmark & \xmark & \cmark & 94 & \textbf{80\%} & 61\% & 79\% & 66\% \\
        % \hline
        \xmark & \xmark & \xmark & 40 & \textbf{23\%} & 5\% & 0\% & 0\% \\
        \bottomrule
    \end{tabular}
\end{table*}


% \begin{table}[]
%     \centering
%     \begin{tabular}{|c|c|c|c|}
%         \hline
%         \textbf{Autopilot Test Loss} & Straight & Left & Right \\
%         \hline
%         SGDA & \textbf{.044} & \textbf{.066} & \textbf{.052} \\
%         \hline
%         Uniform & .049 & 0.068 & .054 \\
%         \hline
%         Single-Spec & .067 & .071 & .061 \\
%         \hline
%         Individual-Props & .057 & .083 & .056 \\
%         \hline
%     \end{tabular}
%     \vspace{2mm}
%     \caption{Test Losses for the imitations of the three maneuvers performed by the CARLA autopilot. The SGDA method has test set losses comparable to and lower than the baselines.}
%     \label{tab:primary_results}
% \end{table}

\textbf{Evaluative metrics analysis.} In Fig.~\ref{fig:outcomematch_results}, we present results on outcome matching for imitation learned models using SGDA and our baseline methods of environment sampling. In each maneuver and with each model, models trained with SGDA have a higher rate of outcome matching than all of the baseline methods. Intuitively, we can interpret this result as SGDA-learned models being overall more \textit{semantically accurate} with respect to the expert than baselines. Moreover, in Fig.~\ref{fig:dtw_results}, we present the results comparing SGDA to our baselines in measuring the average trajectory-length distance between a learned model and expert pair of trajectories, collected in identical environment conditions. We observe that SGDA-learned models consistently have a lower average trajectory-length distance, supporting the notion that SGDA leads to models that not only semantically imitate the expert more closely, but have closer imitations in a (simulated) physical sense as well. As a final comparison, we compute a test set loss for each model, presented in~\ref{tab:loss_values}. Although this does not capture semantic similarities nor potential errors compounded during a trajectory-length execution, we note that SGDA-learned models have comparable (and marginally lower) test losses than baseline methods. %, suggesting that SGDA-learned models do not suffer in model quality as a result of the change in data distribution when compared to our baseline methods.

\begin{table}[]
    \centering
    \caption{L1 Test Loss values for each baseline on our expert test dataset for each experiment. SGDA achieves consistently lower L1 errors.}
    \label{tab:loss_values}
    \begin{tabular}{c|cccc}
        \toprule
        \textbf{Test Loss} & NN-Straight & AP-Straight & AP-Left & AP-Right \\
        \midrule
        SGDA & \textbf{.059} & \textbf{.044} & \textbf{.066} & \textbf{.052} \\
        % \hline
        Uniform & .072 & .049 & .068 & .054 \\
        % \hline
        Single-Spec & .067 & .067 & .071 & .061 \\
        % \hline
        Ind-Props & .063 & .057 & .083 & .056 \\
        \bottomrule
    \end{tabular}
    \vspace{2mm}
\end{table}

Given the improvements in imitation displayed by SGDA-learned models, we seek to understand what semantic outcomes most strongly contribute to these improvements. To measure this, we compute the rate of outcome matching for each possible specification (outcome) in our test dataset, and compare SGDA against our baselines. 

\begin{figure}
     \centering
\includegraphics[width=0.8\linewidth]{imgs/dtw_all_sns_rec.png}
     \caption{Results for Trajectory-Length Distances for imitations of the maneuvers performed by our experts, measured by Dynamic Time Warping (DTW) distance. The SGDA method outperforms all baselines in producing trajectories that are most similar to the expert.}
     \label{fig:dtw_results}
\end{figure}

\textbf{Outcome-specific analysis.} In Table~\ref{tab:outcome_match_percentages}, we show the outcome matching rates for each specification achieved by the expert in our test dataset for the human-learned neural network imitation experiment. These results show that the improvements the SGDA-learned model enjoys over the baselines are most significantly attributed to less likely outcomes. For example, outcomes where the expert collided and abruptly braked without halting ($\neg 1 \land 2 \land \neg 3$) were not matched at all by baseline methods, but our SGDA method was able to match nearly half of the outcomes. A similar story is the case with outcomes where the expert collided and came to a halt while abruptly braking ($\neg 1 \land \neg 2 \land \neg 3$). Being able to properly match these outcomes, which are highly correlated with other outcomes (i.e., you must brake in order to halt), requires being able to imitate nuanced behavior exhibited by the expert. 
%To this point, we notice nontrivial behavior patterns picked up by our SGDA imitation in this experiment that were not seen in our baselines. 

%For example, in the outcome where the ego abruptly brakes, halts, and collides, we notice that this happens frequently for the expert when the ado is approaching from the right side of the intersection and slowly taking a left turn. Anticipating this turn, the expert abruptly brakes to a halt, but does not do so early enough. The ado starts turning and the ego car is too far forward to avoid an incoming collision with the ado. The SGDA-learned model is able to imitate this behavior due to explicitly sampling for and aggregating expert data on environments that induce this outcome, whereas our baselines do not see this behavior often enough in their training data to accurately replicate it. 
In contrast, SGDA-learned models have a comparable, but often slightly lower, accuracy in more likely outcomes, such as outcomes where the expert did not collide, halt or abruptly brake $( 1 \land 2 \land 3)$, or the case where the expert does not collide but halts and abruptly brakes $( 1 \land \neg 2 \land \neg 3)$. This difference in performance can be directly attributed to imbalances in the training data for each baseline. For our single-spec baseline sampling, for instance, the method aimed to falsify just one specification, and found a large number of environment conditions that did so in very similar ways (specifically, finding conditions that satisfied $( 1 \land \neg 2 \land \neg 3)$). In fact, of the expert training data for the single-spec baseline, 38\% of the data satisfied $( 1 \land 2 \land 3)$, and 53\% satisfied $( 1 \land \neg 2 \land \neg 3)$. In contrast, for SGDA's training data, the percentages for each of those specifications were 35\% and 24\%, respectively. Overall, our results indicate that SGDA will greatly help improve imitated behavior in unlikely but semantically meaningful outcomes, without significantly altering the accuracy in behavior in common outcomes.
%These results are broadly reflected across all of our experiments, where in the worst case we see an eight percent difference in performance between outcomes matched. In practice, we reason that these discrepancies in performance will be reduced as more data is collected from these likely outcomes (and in general). 

\textbf{Property robustness analysis.} Lastly, to provide insight into SGDA's robustness to changing properties in the set $LP$, we repeat our NN-Expert experiment with varying thresholds for the braking value in our ``Do not abruptly brake'' property. We notice that the rarity with which the property occurs indeed has an effect on the ability of SGDA to imitate expert behavior on that property. However, we note that SGDA outperforms our property-agnostic baseline -- uniform sampling -- even when the property becomes rare, and is generally robust to changes when the property is more frequent. We provide full details of this experiment in the appendix.
% The actual experiment process (v1) will look as follows:
% \begin{enumerate}
%     \item our method: we have a set of env params we can tune. we use that in ECS
%     \item we will just collect diversely from the set as our initial collect\_data method
    
%     \item vanilla dagger: use this for now. aggregate from env conditions that were collected based on the prior (which we will set as a uniform prior since that's reasonable)
% \end{enumerate}

