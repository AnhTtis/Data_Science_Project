% we start by defining episodic control in the context of a paramterized environment and policy 

% * define the parameters of the environment and give a ground truth that we do not have access to (E* or something)
% * then define the space of policies and define the expert's - which is our goal
% * the distribution we want to match as perfectly as possible is D(pi * and E *) 

Given an initial dataset collected from an expert policy, a continuous control task, and a controllable environment, we aim to use logical specifications to learn an accurate imitation of the expert. In this section, we formulate our method of semantically sampling possible conditions of our environment for use in a data aggregation-style loop.

We start by outlining episodic control under a given policy in a controllable environment. Let $\mathcal{E}$ represent \textit{all} possible environment conditions (realizations of the environment) that we can represent in our simulator. Given an environment condition $e \in \mathcal{E}$ and a policy in the class of policies available to the learner, $\pi \in \Pi$, we define a control episode as $ \xi_{\pi}(e) = d(\pi, e)$. Here $\xi_{\pi}(e)$ is a trajectory of state and action pairs of the expert, and $d$ represents a function that rolls out the dynamics for a control episode using a fixed policy $\pi$ to control the agent and $e$ to set the conditions of the environment.  
% Whenever understood from the context, we abuse notation and write $\xi$. 
For simplicity, we will assume that our dynamics are deterministic and that $d$ returns a single trajectory. The function $d$ unrolls the dynamics until the trajectory reaches a predefined termination condition, such as arriving at a goal destination, violating a safety condition, or reaching a maximum number of timesteps. 

In this section, we first describe how to semantically divide the space of possible trajectories by constructing a \textit{semantic trajectory partition} (STP), then describe how we use this partition in a data aggregation-style imitation learning loop.

\subsection{Creating a Semantic Trajectory Partition}
\label{sec:env_partition}
% Our goal is to learn a policy that best imitates some expert policy, $\pi^*$, when deployed in environment conditions that are sampled from the ground truth (real-world) distribution of conditions, which we will denote as $\envs^*$. Using the imitation learning objective that is standard in the literature, our objective becomes
% The goal of imitation learning is to learn a policy $\pi$ that approximates a ground truth policy $\pi^*$ as closely as possible by optimizing the following objective:
% \begin{equation}
%     \pi^{\text{IL}} = \text{argmin}_\pi \mathbb{E}_{s \in \mathcal{D}^*}  [\mathcal{L}(\pi(s), \pi^*(s))]
%     \label{eq:vanilla_il}
% \end{equation}

% Here, $\mathcal{D^*}$ is a dataset consisting of trajectories $d(\pi^*, e^*)$, where $e^* \sim \envs^*$. 

% The aforementioned objective brings about the following challenge: In simulation, we do not have access to the ground truth distribution of environment conditions that our agent will see in real-world deployment. This means that a number of safety-critical scenarios that occur in the real world will be elided by most distributions. An imitation trained on a naive distribution of environment conditions will therefore poorly imitate the expert in these critical scenarios.

% In order to address this, we leverage known (user-provided) information about what outcomes of an agent's trajectory are important, and use this information to construct a proxy distribution over environment conditions that induces these important outcomes.   

% \subsection{Defining a Diverse Dataset}
% For our imitation learner, we argue that it is more important to find and expose our learner to as diverse a set of environment conditions as possible than it is to try and perfectly recreate $\envs^*$ for constructing a training dataset $\hat \envs$. 
We begin by having a user provide a set of (temporal) \textit{logical properties, LP}, that are domain-specific and identify meaningful properties of an agent's trajectory. For example, in the context of autonomous driving, these properties could represent avoiding collisions or maintaining speed limits. The formalization of such properties are well-explored and can be adapted from works like~\cite{censi2019rulebooks}.
We allow for flexibility in how these properties are expressed: they can be purely Boolean properties (avoid all collisions) expressed in logics like Metric Temporal Logic (MTL)~\cite{MaierhoferMTL2020} and Higher-Order Logic (HOL)~\cite{RizaldiTrafficRules2015}, or a Boolean property accompanied by a continuous objective (stay a thresholded distance away from all other vehicles), expressed in quantitative semantics like Signal Temporal Logic (STL)~\cite{maler2004monitoring, HekmatnejadRSS2019}.
We denote the size of $LP$ as $\vert LP\vert = l$.

With this property set $LP$, we construct a partition of the space of all agent trajectories. Our approach for creating the STP is the following.  We first treat each property $p \in LP$ as a \textit{propositional variable} that is true if and only if a trajectory $\xi_{\pi}(e)$ satisfies $p$ under environment $e$; that is, the pair $(\xi_{\pi}(e), e) \models p$.  Then, we enumerate all $2^l$ possible Boolean logical formulae conjuncting the properties $p$ in $LP$ or their negations. In doing so, we create a set of \textit{logical specifications}, $\Phi$, that covers all possible combinations of truth values for every property in $LP$. We will refer to elements of our partition $\varphi \in \Phi$ as \textit{specifications}. We will often abuse this notation and refer to specifications both as the logical formula associated with a partition element \textit{and} the set of collected trajectories satisfying that logical formula (i.e., exist within the partition element) when aggregating data.

\begin{figure}
    \centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/example.png}
    \end{subfigure}
    
    \begin{subfigure}{0.245\linewidth}
        \includegraphics[trim={0cm 2.4cm 3.75cm 0cm},clip,width=\linewidth]{imgs/RoadDiagram-NoCollisionNoSpeeding.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}{0.245\linewidth}
        \includegraphics[trim={0cm 2.4cm 3.75cm 0cm},clip,width=\linewidth]{imgs/RoadDiagram-YesCollisionNoSpeeding.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}{0.245\linewidth}
        \includegraphics[trim={0cm 2.4cm 3.75cm 0cm},clip,width=\linewidth]{imgs/RoadDiagram-NoCollisionYesSpeeding.pdf}
    \end{subfigure}%
    \hfill%
    \begin{subfigure}{0.245\linewidth}
        \includegraphics[trim={0cm 2.4cm 3.75cm 0cm},clip,width=\linewidth]{imgs/RoadDiagram-YesCollisionYesSpeeding.pdf}
    \end{subfigure}
    

    \caption{\textbf{Top:} A visualization of how user-provided properties can be treated as propositional variables to form our STP. We enumerate all combinations of truth values for each property to create a set of specifications that act as elements in a partition of the trajectory space. \textbf{Bottom:} An example of trajectories that satisfy each specification for the blue car agent.}
\label{fig:example_figure}
\end{figure}

As a simple example, consider the scenario depicted in Fig.~\ref{fig:example_figure}. In this example, we have a set $LP$ with just two properties: ($A$) avoid collisions, and ($B$) stay within the speed limit. Our partition $\Phi$ would then consist of four elements: $(A \land B)$ (avoid collisions and don't speed), $(\neg A \land B)$ (collide and don't speed), $(A \land \neg B)$ (don't collide and speed), and $(\neg A \land \neg B)$ (collide and speed.) Note that every trajectory that an agent can produce will satisfy only one of the specifications in our partition. Also, notice that in order to explore the full partition, certain environments the agent is in will elicit more samples in specific elements of $\Phi$ --- e.g., an urban environment that favors slower speeds. % \jon{Add a blurb about possible scenario variables needed to perturb, like urban vs. highway, number of lanes, positioning of occlusions, etc.}\guy{wrote one, can resolve}

We note that the size of $\vert \Phi\vert = 2^l$ may limit in practice the number of properties a user can provide. We do not see this as a prohibitive issue, however, primarily due to the fact that users only have to specify a small number of properties in order to sample a diverse set of data, as evidenced by our experiments. Further, our approach can be generalized to a setting where a user can assign importance weights to each property, which can allow for the de-emphasis of specific partition elements. We further expand on this generalization in the appendix.  

During training, we can use our STP to sample equitably from the possible space of semantic results and ensure there is ample coverage of these semantics in the data. We will next discuss how to use the STP and its elements 
% which we will denote as $\varphi \in \Phi$, 
to improve imitation learners via data aggregation.

\subsection{Specification-Guided Data Aggregation}\label{sec:spec_guided_data_agg}

Ultimately, our goal is to use $\Phi$ to improve the accuracy of our imitation learning agent $\pi^{\text{IL}}$ by aggregating a semantically diverse dataset. To do this, we take an approach consisting of four steps: (1) Training, (2) Environment Sampling, (3) Environment Selecting, and (4) Data Aggregation, as outlined in Algorithm~\ref{sgda_alg}. We give an explanation of each step in turn:

 \begin{algorithm}
\SetAlgoLined
\KwIn{Initial dataset $D$, Number of aggregation rounds $n$, STP $\Phi$, expert policy $\pi^*$, \# of samples $k$, \# of selections $m$}
 Learn $\pi^{\text{IL}}$ w.r.t $D$\;
 Initialize $E_\text{prev}, D_\text{prev}$ to $\emptyset$\;
 \ForEach{iteration $i \in 0 \dots n$}{
  $E_\text{new} := \text{EC-Sampling}(\pi^{\text{IL}}, k)$\; %\tcp{can be further refined}
  $E_\text{new} := \text{EC-Select}(E_\text{new}, E_\text{prev}, D_\text{prev}, m)$\;
  $D_\text{new} := \{d(\pi^*, e); \forall (e, \xi_{\pi^{\text{IL}}}(e)) \in E_\text{new}\}$\;
  $D = D \cup D_\text{new}$; \tcp{aggregate} 
  Learn $\pi^{\text{IL}}$ w.r.t $D$\;
  Set $E_\text{prev}, D_\text{prev}$ to $E_\text{new}, D_\text{new}$\;
  }
 \Return{$\pi^{\text{IL}}$}
 \caption{Specification-Guided Data Aggregation (SGDA)}
     \label{sgda_alg}
 \end{algorithm}


\textbf{Training.} To start, we assume an initial dataset 
$D$ collected from an expert. We make no assumptions on the distribution of the environment conditions from which $D$ was collected. We then learn an imitative policy, $\pi^\text{IL}$ from $D$ using any IL algorithm, such as Behavioral Cloning (BC) or GAIL~\cite{ho2016gail}. 

\textbf{Environment Sampling (EC-Sampling).} \label{sec:env_sampling}
In this step, we attempt to collect a semantically broad pool of trajectories by sampling a set of environment conditions $e \in E$ and executing $ \xi_{\pi^\text{IL}} = d(\pi^\text{IL}, e) $. 

When sampling and collecting different environment conditions, we balance two factors: First, we want to collect trajectories for every element of our partition $\Phi$ as equitably as possible. Second, we do not want to attempt to sample for elements in the partition that are very hard to satisfy (i.e., we attempt to sample environment conditions that produce a trajectory in a given partition element $\varphi$, but finding the correct environment conditions is challenging). To find the correct environment conditions for a given $\varphi$, we leverage optimization-based sampling methods, such as Bayesian Optimization or Cross-Entropy, which learn a distribution from already sampled (environment condition, trajectory) pairs that maximizes the chance of a successive sample satisfying our target specification $\varphi$. 

In order to balance the aforementioned two factors, we use the Upper Confidence Bound algorithm~\cite{auer02ucb} to select the next target specification we sample for, adopting notation from~\cite{khandelwal2016mctsucb}. The UCB algorithm balances \textit{exploitation} (sample for partition elements that are under-represented) and \textit{exploration} (don't sample for partition elements that we have made a lot of sample attempts for.) 

We outline our approach in Algorithm~\ref{ecs_alg}. We keep a total count of how many times we sample for a selection with $N$, and a count of how many $(\xi_{\pi}(e), e)$ pairs satisfy each specification with count (line 2). In each iteration, we choose a target specification $\varphi_c$ that we will attempt to sample a satisfying environment condition for. In lines 5-6, we increment the count for $\varphi_c$ and sample for it using our optimization-based sampler, which we denote as $f$. $f$ takes in as arguments the target specification and the collected samples and produces a new sample environment condition that the optimizer believes is likely to satisfy $\varphi_c$. In lines 7-9, we roll out our dynamics and collect the trajectory corresponding to the sampled environment, and add that trajectory to the element partition that it actually falls under by checking satisfaction for every specification in our set. In lines 10-11, we compute the \textit{value} of choosing a specification for the next iteration by prioritizing specifications that have been undersampled relative to other specifications. We use these values along with the number of times we attempted to sample for a given spec ($N$ in our algorithm) in the UCB computation in line 13 to choose the specification to sample for in the next round.
% first, we assume that we have an initial policy learned from some prior distribution. then we 

\begin{algorithm}
\SetAlgoLined
\KwIn{Environment Partition $\Phi$, Optimization-guided sampler $f$, learned policy $\pi^{\text{IL}}$, Number of iterations $k$}%, max depth $\mathit{d_{max}}$}
 Select an initial $\varphi_c$ from $\Phi$\;
 % \tcp{map each $\varphi$ to its $\vdash$ sample set}
 Initialize $N_{\varphi_j}$ to 0 $\forall \varphi_j \in \Phi$\; 
 Initialize $S$ to $\emptyset$\;
 \ForEach{iteration $t \in 0 \dots k$}{
  $e_t := f(\varphi_c, S)$; \tcp{get new sample} 
  Increment $N_{\varphi_c}$\;
  $ \xi_{\pi^{\text{IL}}}(e_t) = d(\pi^{\text{IL}}, e_t) $; \tcp{roll out dynamics}
  Add $\xi_{\pi^{\text{IL}}}(e_t)$ to the $\varphi$ it satisfies\;
  %Increment $\text{count}_{\varphi_i}$\;
  \ForEach{$\varphi_j \in \Phi$}{
    Set $Q_j$ to $max_{\varphi \in \Phi}(|\varphi|) - |\varphi_j|$\;
  }
  Add $(e_t, \xi_{\pi^{\text{IL}}}(e_t))$ to $S$\;
  Set $\varphi_c := \text{UCB}(Q, N, t)$ \tcp{pick next spec}} 
 \Return{S}
 \caption{Environment Condition Sampling (EC-Sampling)}
     \label{ecs_alg}
 \end{algorithm}

\textbf{Environment Selection (EC-Select).} With our collected set of environment conditions and trajectories from the previous step, we want to select a subset of conditions for the next round of data aggregation that will improve our model in a semantically meaningful way. For this, we leverage the following insight: Environment conditions $e$ that lead to different satisfied specifications between our learned policy's trajectory and our expert policy's trajectory should be prioritized higher when selecting. Formally, we want to bias toward environments where, when rolled out with our expert and imitative policies, lead to $(\xi_{\pi^*}(e), e) \models \varphi_i$ and $(\xi_{\pi^{\text{IL}}}(e), e) \models \varphi_j$, respectively, where $\varphi_i \neq \varphi_j$.

In our initial round of data aggregation, we do not have a set of environment conditions with trajectories from both our learned model and the expert model. In this case, we simply sample uniformly at random from our set to select our dataset for aggregation. In future iterations, however, we have the previous iteration's dataset, containing both the collected expert trajectories $(\xi_{\pi^*}(e), e)$ and our learned model's trajectories $(\xi_{\pi^{\text{IL}}}(e), e)$ for many $e$. We can then compare whether the same environment condition led to the same specification being satisfied by both the expert and learned model trajectories. 

\begin{table}[]
    \centering
    \caption{Example computation for environment selection weights.}
    \label{tab:selection_example}
    \begin{tabular}{c|cccc}
        \toprule
        & \multicolumn{4}{c}{IL outcomes} \\
        Expert outcomes & $A \land B$ & $\neg A \land B$ & $A \land \neg B$ & $\neg A \land \neg B$ \\
        \midrule
        $A \land B$: 10 & 8 & 2 & 0 & 0\\
        $\neg A \land \neg B$: 10 & 0 & 0 & 5 & 5 \\
        \midrule
        Selection weights & 0.2 & 1 & 1 & 0.5 \\
        \bottomrule
    \end{tabular}
\end{table}

As an example, consider the two-property partition introduced in Section \ref{sec:env_partition}. Suppose in the previous round of data aggregation, we selected a set of 20 environment conditions. In this hypothetical scenario, our expert avoided collisions and abided by the speed limit $(A \land B)$ in 10 conditions, and collided and violated the speed limit $(\neg A \land \neg B)$ in the other 10 conditions. In the same 10 conditions that led to $(A \land B)$ from the expert, our learned model avoided collisions and abided by the speed limit in eight of them, and in the other two, collided while abiding by the speed limit $(\neg A \land B)$. In the 10 conditions that led to $(\neg A \land \neg B)$ from the expert, our learned model also collided and violated the speed limit in only five of them, and in the other five, didn't collide but violated the speed limit $(A \land \neg B)$. This scenario is outlined in Table~\ref{tab:selection_example}. We would set up our \textit{selection weights} as:
\[
1 -(M_{\varphi} / N_{\varphi}) \quad \text{if} \quad N_{\varphi} > 0, \quad \text{else} \quad 1
\]
 
Here, $N_{\varphi}$ refers to the total number of instances in which a specification was satisfied by either the expert or imitation (without double counting for both), and $M_{\varphi}$ refers to the number of expert outcomes that were matched by the imitation learner. 
%As a result, our selection weights are as follows: $(A \land B)$ has weight $1 - (8/10) = 0.2$, $(\neg A \land \neg B)$ has weight $ 1 - (5/10) = 0.5$, and the remaining two specifications have weight 1. 
These weights are normalized into a categorical distribution, and we sample from the pool of sampled environment conditions (from our previous step) weighted by this distribution to yield our selected batch for the subsequent iteration of data aggregation.
We denote this selection step by $\textrm{EC-Select}(E_\text{new}, E_\text{prev}, D_\text{prev}, m)$ in Algorithm~\ref{sgda_alg}, using the previous expert and imitated data to compute selection weights and select $m$ samples from the new set $E_{\text{new}}$.

\textbf{Data Aggregation.} With a final selected set of environment conditions, we can collect trajectories from the expert policy under these conditions and aggregate the collected data in our overall dataset. This dataset can then be used to train our imitation learned model in the subsequent iteration; that is, we repeat the process with our training step on this aggregated dataset. 

% The overall process described above is captured in Algorithm~\ref{sgda_alg}. 
Algorithm~\ref{sgda_alg} summarizes the process above; it first trains an initial imitation (line 1), then selects new environment conditions using the aforementioned environment sampling and selection methods (lines 4-5), and queries the expert with the conditions (line 6) to produce a dataset that can be aggregated (line 7). The process can be repeated as desired.

\textbf{Overhead of SGDA.} When actuating dynamics for each $e$ and evaluating satisfaction for each $(\xi_{\pi}(e), e) \models \varphi$, SGDA incurs computational overhead relative to approaches that sample environments without any guidance. We find that this overhead scales linearly with the size of $\Phi$, and note that this trade-off in practice is often much smaller in comparison to the high cost of expert-in-the-loop data collection, which SGDA aims to reduce. We provide a more detailed analysis of this overhead in the appendix.
% The method by which environment conditions themselves are sampled using $\Phi$ is described in algorithm~\ref{ecs_alg}. For each specification $\varphi \in \Phi$, we maintain a set of the environment conditions sampled that, when our learned policy is deployed, lead to a trajectory that satisfies $\phi$. We assume access to an optimization-based sampler $A$, that maintains a set of parameters for each specification in $\Phi$, and uses all sampled values to inform a distribution for each specification. $A$ can be any optimization-based sampling technique; for our experiments, we use a Bayesian-Optimization based sampler that builds a Gaussian Process (GP) for each specification. At each iteration of our sampling algorithm, we sample for the specification in $\Phi$ that is \textit{least represented}; that is, the $\varphi$ with the smallest corresponding set of satisfying environment conditions. We execute our function $d(\pi^{\text{IL}}, e)$ and collect a trajectory by deploying our learned policy with the sampled environment conditions. We add this collected trajectory to the sample set that corresponds to its satisfying specification and repeat for $k$ sample iterations. 

% By sampling for a selected specification, we are not guaranteed to retrieve a trajectory that satisfies that specification. As such, we cannot make any guarantees on how well our collected samples will cover the specification space $\Phi$. However, we show in our experiments that in practice our sampling methods enables a more diverse collection of data.

% \subsection{Allowing users to prioritize properties}

% We note that our approach described thus far has two shortcomings: First, there is not currently a way for a user to specify any priority for the property set they provide - all properties in the set are weighted equally. Second, the specification space $\Phi$ grows exponentially with the number of properties provided. To address both of these issues, we allow for a user to provide a \textit{weight} associated with each property in the set. The weight, a value in the range [0, 1], is used in two ways: first, when enumerating the specification space $\Phi$, each specification $\varphi$ will be assigned a weight equal to the product of the individual properties (or negation of those properties.) For example, if a user provides properties A, B, and C with weights 0.5, 0.2, and 0.  

% \subsection{Our approach: Specification-Guided Data Aggregation (SGDA)}
% Following the previous section, we want to find environmental parameters that induce typical and rare events with the intention of collecting data from $\pi*$ in these events to train an accurate learned imitation. However, setting parameters of an environment in the real world is challenging, and we would like to avoid real-world crashes or safety-critical events. As a result, our design relies on access to a simulator in which environment parameterization and data collection is feasible and efficient. 

% Our goal of collecting the appropriate data can then be framed as a goal of finding the right set of environment parameters that enables accurate imitation learning in both typical and rare events. Existing works have tackled this problem in autonomous driving with a variety of methods, ranging from importance sampling (\textbf{cite}) to gradient-based training of other agents on the road (\textbf{cite}). In this work, we present \textbf{Specification-Guided Data Aggregation (SGDA)}, a general approach for finding both typical and rare events that flexibly integrates with existing methods. The key insight behind SGDA introduces \textit{formal specifications} as a guide for environment sampling in the context of imitation learning.

% At a high level, our approach leverages (1) a boolean specification expressed in some logical formulae that captures properties of interest and (2) an existing dataset taken from $\pi^*$ on which an imitation model can be trained. This initial dataset, denoted as $\mathcal{D}_\text{init}$, is collected by sampling from a given (potentially unknown) prior on environment parameters. After training an initial model, we can bring that model into our simulation and begin to analyze it with respect to our specification. More specifically, with a fixed imitation, we can then systematically search for environment parameters by \textit{formal falsification} with respect to our specification. Precisely, we build a set of environment parameters, $\Psi_\text{F}$:

% \begin{equation}
%     \Psi_\text{F} = \forall \psi \in \Psi. \psi \vdash \neg(\varphi(E(\psi)), \pi^{\text{IL}})
%     \label{eq:falsified_set}
% \end{equation}

% Here, we overload $\varphi$ to refer to both our specification of interest, as well as the function that evaluates $\varphi$ on our system (consisting of the parameterized environment and our learned model). In short, equation \ref{eq:falsified_set} seeks to find environment parameters that \textit{falsify} our specification of interest. For example, a specification of interest may encode the property: ``always keep a certain distance from other vehicles, don't exceed the speed limit, and avoid collisions with any objects'' in Linear Temporal Logic. We can then leverage falsification techniques to find environment parameters where our learned imitation violates the property. In doing so, we are enabled to find rare events for our imitation-learned policy in a principled manner. Subsequent rounds of data collection can use environment parameters from 
% $\Psi_\text{F}$ to evaluate whether or not $\pi^*$ falsified $\varphi$ in the same manner our initial imitation model did.

% Note also that properties $\varphi$ can be easily modified in our framework - for instance, we can modify the property mentioned above to find instances where the learned model drives within the speed limit and keeps distances from cars, but does not avoid collisions. In robotics settings, our specifications of interest often involve thresholds on numerical values - in such cases, we can leverage  \textit{optimization-based} falsification \textbf{cite}, where methods such as Bayesian Optimization are used to find regions of numerical values that will falsify the specification with high probability. 

% We view the flexibility of SGDA as one of its primary strengths. Sets of environment parameters found by SGDA can later be ordered by any other metrics of interest, allowing for existing approaches to exist atop the SGDA framework, such as uncertainty-based imitation learning (\textbf{cite}). We demonstrate this flexibility in multiple autonomous vehicle scenarios in our experiments.