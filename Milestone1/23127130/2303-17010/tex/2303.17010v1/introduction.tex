An extensive body of work has demonstrated the effectiveness of \textit{imitation learning} (IL) in a variety of end-to-end control and motion prediction tasks, particularly in the domain of autonomous driving~\cite{bansal2018chauffeurnet, kuefler2017imitating, codevilla2018end, chen2019deep, kebria2019deep}. IL methods, which learn a policy to mimic collected samples of expert behavior, are typically highly general and can be easily implemented in a variety of domains. Despite these successes, IL methods often suffer in performance due to the problem of \textit{covariate shift}: inaccuracies in predicted actions early in a control episode may lead to compounding error when visiting future states outside the original training distribution~\cite{spencer2021ilfeedback}. In this work, we propose a novel method to combat covariate shift that leverages configurable environment domains, such as simulations, to aggregate expert data in \textit{varied environments} from which an accurate imitation can be learned. 

Existing approaches to reduce error caused by covariate shift, namely DAgger-style methods~\cite{ross2011dagger, prakash2020daggerplus, cui2019uail, kelly2019hgdagger, zhang2017safedagger}, propose switching control between an IL agent and the expert policy in order to collect expert data on states that were outside the original training distribution. This family of algorithms generally focuses on \textit{when} to switch control and collect data from the expert, by exploiting additional objectives or the uncertainty of the learning model. We devise an approach that chooses \textit{where} the agent will act in its world, as the world in these methods is usually treated as fixed, not a controllable part of the IL loop.
% Despite their success and broad applicability, IL methods have a number of well-known shortcomings. Most notably, if an IL model's predicted actions are inaccurate during a control episode,  it may lead to future states which are outside the distribution of states seen during training - a phenomenon known as covariate shift~\cite{spencer2021ilfeedback}. This can lead to compounding error with respect to the time horizon of the task~\cite{rossbagnell2010reductions}.
% A number of recent approaches have been introduced to reduce error caused by covariate shift. The DAgger approach~\cite{ross2011dagger} introduced a meta-algorithm that switches control between an IL model and the expert policy in order to collect expert data on states that were outside the original training distribution. Follow-ups and variations on DAgger introduce new criteria for when to switch between the model and expert, how to prioritize collected expert data, and additional objectives to optimize for~\cite{prakash2020daggerplus, cui2019uail, kelly2019hgdagger, zhang2017safedagger}.
%These modifications to the DAgger framework generally focus on the structure of \textit{how} the IL agent learns, such as the agent's set of objectives or the structure of the learning model. 
%A relatively unexplored direction is to devise an approach that chooses \textit{where} the agent will act in its world, as the world is usually treated as fixed, not a controllable part of the training loop.  In this work, we propose a novel method that leverages configurable environment domains, such as simulations, to aggregate a set of expert data from which an accurate imitation can be learned. 

As a motivating example, consider the task of imitating a student driver to better understand how and when the student may make errors during driving. Since the student is a novice, we want to avoid putting the student in a real vehicle and instead opt to have the student operate in simulation. Further, since our goal is to imitate the student as accurately as possible, we are not explicitly seeking to train the safest or most performant learned model. If the student makes a mistake in a particular circumstance, we want our learned imitation to make the same mistake as well.

With a controllable world, the key question becomes: How do we find environments in this world that will help us broaden our understanding of the expert's behavior to yield a better imitation? If the student driver typically drives very hesitantly and avoids getting close to other vehicles, how would the student behave if they were to be suddenly cut off by another car? In domains like driving, unlikely events (such as cutoffs) have high \textit{semantic value} for both the student and teacher (collisions can occur as a result of cutoffs.) When learning an imitation, it is crucial we prioritize these circumstances and find environments that induce such outcomes so that we may collect valuable expert data.

In order to find meaningful environments, we introduce an approach that semantically describes and differentiates amongst the many possible environments in our world, leveraging domain-relevant information in the form of logical properties. In the case of our driving example, one such property could capture whether the student driver encountered a collision, and another could measure whether a speed limit was exceeded. From these properties, we create a set of \textit{formal specifications} by enumerating all of the possible truth value assignments to these properties. Each specification corresponds to \textit{all} possible outcomes that assignment could capture. Following our example, these specifications would separate environments where the student encountered a collision but didn't speed, ones where the student sped but didn't collide, ones where the student both collided and sped, and ones where the student neither collided nor sped.
% We take inspiration from formal methods-guided sampling and design a set of \textit{formal logical specifications} that each corresponds to a different set of environments in our world. Our approach allows for a user to provide a small set of logical \textit{properties of interest} with respect to the imitation learning problem they want to solve. We then form a set of specifications by enumerating all possible combinations of truth values for each property in the provided set. 

This set of specifications effectively \textit{partitions} the space of possible (environment, agent trajectory) pairs in our world: a single pair can only satisfy (i.e., belong to) one specification in our set and no other. This partition offers a semantic characterization of the space of environments that we can then use to guide the data aggregation process. 


\begin{figure*}
     \centering
\includegraphics[width=.98\linewidth]{imgs/overview2.pdf}
     \caption{Our approach for environment sampling for data aggregation. Using a set of logical properties, we sample varied environments and select ones that induce semantically different behavior between our expert and an IL policy.}
    \label{fig:overview}

\end{figure*}

We provide a visualization of our data aggregation algorithm in Fig.~\ref{fig:overview}. We deploy our current IL policy in various environments, using the aforementioned partition to equitably sample from each specification and gather a semantically diverse set of environments. From this set, we select a subset of environments where we most expect the expert and IL policies to \textit{disagree} on their semantic behaviors, and aggregate expert data on this subset.
% With this partition enabling us to semantically differentiate amongst possible environments in our world, we outline our method to select environments for data aggregation. In each round, we attempt to sample as equitably from each specification as possible, without oversampling for specifications that are nearly impossible to satisfy. 
%In previous rounds of data aggregation, we collected a set of environments, and have information regarding which specification was satisfied by our expert and which was satisfied by our IL model under the same environments. If a given environment led to different specifications being satisfied, we prioritize selecting environments from those specifications (those elements of our partition) for the next round of aggregation. 

We concretely instantiate our approach in an algorithm we call \textit{Specification-Guided Data Aggregation} (SGDA). We demonstrate the efficacy of our algorithm in the CARLA autonomous driving simulator by imitating two different expert models as they perform different driving maneuvers at a four-way intersection. Our results show that aggregating data using environments selected by our algorithm leads to better IL accuracy than environments selected by baseline methods, especially in unlikely but semantically meaningful circumstances. 

In summary, our contributions in this paper are threefold: First, we identify a novel formalism -- a semantic partitioning of possible (environment, trajectory) pairs based on the formal specifications the trajectories satisfy -- for systematizing environment sampling. Second, we propose an algorithm that uses the aforementioned partitioning to improve imitation learning models in a data-aggregation loop. Third, we show promising experimental results of our algorithm for human driver modeling, and improved performance over environment sampling baselines in the context of autonomous driving.