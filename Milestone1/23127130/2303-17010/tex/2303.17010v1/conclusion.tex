% to include in conclusion:

% - we want to extend to different uses of the STP
%     - for example in RL where we can use it to train a policy that satisfies the specifications corresponding to each element of the partition
% - use this to actually learn imitation models of individual and groups of humans
%     - particularly in training where simulation is important, to understand from a semantic point of view where these things can be used
In this paper, we present a method that leverages task-relevant information to systematically sample new environments for data aggregation in the context of imitation learning. Within this method, we introduce the notion of a \textit{Semantic Trajectory Partition} that divides up a space of possible environments into elements that each represent semantically different outcomes for an agent. Our experiments show that aggregating data using our method outperforms existing methods of environment sampling, especially in cases that are unlikely but semantically meaningful to a user. 

There are many possible avenues for future work. One could combine SGDA with other DAgger-style approaches to further improve learned imitations by controlling both \textit{when} and \textit{where} expert data should be collected. Another direction is to use SGDA to learn imitations in a human modeling setting, especially for individuals going through training in simulation, to better understand their areas of improvement. %We also see uses of the STP to methods beyond imitation learning, such as sampling environments for learning a goal-conditioned reinforcement learning (RL) policy. 