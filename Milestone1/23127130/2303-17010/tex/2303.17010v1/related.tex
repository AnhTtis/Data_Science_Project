% * things to include in this: 

% (1) methods of learning policies for specifications, generally
% LTL2Action, Reward Machines 

% (2) general multi-agent learning works (MARL, MAGAIL)
% (auto)curriculum work with emergent behaviors across agents
% stable influencing and leading intent

% (3) applications of assume-guarantee reasoning to MDP/RL works
% cite the AG-MDP-POMDP paper
% https://arxiv.org/abs/1708.04236
% https://arxiv.org/abs/1405.0835
% composition of policies (boolean algebra for composition)


\textbf{Robustifying Imitation Learning.}
% \jon{works using various metrics like robustness and confidence, to contrast with our choice of specifications} 
Several works address the problem of robustness in imitation learning, either through modification of the training criteria or extending to environments that cover properties of interest, such as rare events.  \cite{codevilla2019} acknowledge and show that dataset bias can hinder achieving robustness to corner cases, and propose algorithmic improvements and auxiliary training tasks.  When a task is known, \cite{wang2017, Zolna2019TaskRelevantAI, bhattacharyya2020humangail} all use modified generative adversarial imitation learning (GAIL) structures to learn semantic policy embeddings and enforce task-relevancy.  \cite{lu2022imitation} augment pure IL with a reinforcement learning (RL) agent to deal with distribution shifts.  Other works have explored using confidence measures to characterize trajectories and select among them for training.  \cite{zhang2021confidence, Huang2019-ce} use notions of confidence to align imitation with downstream tasks, while \cite{Richter2017-lh, Amini2018-vl} use notions of novelty detections trained on diverse datasets.

We also address the robustness problem, but view it from the perspective of aggregating expert data to ensure a desired level of IL performance can be achieved.  Decoupling from the IL approach eliminates constraints on the learning pipeline and frees users to adopt any IL approach.
% The challenge remains in framing the task with structure that transcends simple constraints and using that to train well-performing imitation learning agents. 

% \begin{itemize}
%     \item Other works leveraging confidence estimates and imitation: \cite{Huang2019-ce,Richter2017-lh, Amini2018-vl}.
% \item Works that explore diversity in imitation learning? \cite{Ladosz2022-bv,Tangkaratt_undated-py,Huang_undated-ig,Wang_undated-xc}
% \end{itemize}

% \jon{Try not to differentiate too strongly - perhaps collapse into broader categories}

\textbf{Falsification and Importance Sampling.}
Several works use formal methods and search-based methods to uncover rare events and property violations in safety-critical systems.  Some handle the \textit{falsification} problem, including \cite{abbas2014conformance}, who propose an approach that gauges conformance of a given model with respect to a reference model for which certain guarantees hold, while \cite{akazaki2018falsification, lee2020} use reinforcement learning to reduce the number of simulations required to find counterexamples. \cite{chou2018using} cast the falsification problem as one of control synthesis, in which invariant sets are used as the basis to concentrate sampling.  Some mature falsification approaches are implemented in software tools such as s-Taliro~\cite{annpureddy2011s} and Verifai~\cite{dreossi2019verifai}.

In contrast to falsification, \textit{importance-sampling} (IS) methods have shown promise in finding counterexamples to desired properties.  Works such as \cite{sankaranarayanan2012falsification} adapt the search for violations of a property using the cross-entropy method, assuming a fixed partitioning of the input space.  
\cite{AriefHKBHDLZ21} extend standard IS by training a deep-learned sampler with efficiency guarantees, while \cite{OKellySNTD18} adopt an adaptive IS procedure with learned distributions over the search space to further accelerate discovery of rare events.  Because many such approaches quickly hone in on rare events, it remains challenging to directly apply them to the task of supplementing data used for learning, as they may miss many semantically-interesting non-failure outcomes.

% include: russ's importance sampling, falsification stuff

\textbf{Scenario Generation for Safe Autonomy.}
% As mentioned in the introduction, there is a substantial amount of literature on generating diverse behaviors and scenarios in the context of Safe Autonomy. Works like Scenic \cite{scenicOriginal} provide a programmatic foundation for defining such scenarios, but do not aim to synthesize them. Other works, such as \cite{DingCLEZ21, PriisaluPPS21}, aim to learn black-box models for generating behaviors and scenarios in the context of testing and safely operating autonomous vehicles. While these works are effective in objective-driven scenario generation, they are not programmatic in nature and therefore lack the benefits and such concept classes provide.
While the above works focus on sampling in general, there is a substantial body of literature focused on diversifying the scenarios used for learning safe autonomy systems.  \cite{DingCLEZ21, DingLJZ21} train flow-based generative models and networks exploiting hierarchical structures to sample environments for both training and validation.  Other works incorporate dynamics and simulations of the world to inform sampling~\cite{WengCOR22, gu2022review, lee2020}.
%for instance, \cite{WengCOR22} sample from a robust invariant set computed from the dynamics, \cite{lee2020} train risk-seeking RL agents, while the goal of safe-RL \cite{gu2022review} is to sample from simulated environments to train safety-abiding RL agents.  
The adaptive sampling works in \cite{OKellySNTD18, WangPTMS0RU21, UKSz19} search over an environment parameter space to rapidly uncover failure modes.
%while \cite{WangPTMS0RU21} aim to produce semantically-meaningful scenarios, and \cite{UKSz19} generate scenarios based on estimates on probability of failure.  
Some works, such as the Scenic language~\cite{scenicOriginal}, have explored more programmatic, user-readable ways of specifying these modes.  While such works perform environment sampling, our approach differs in that we balance our dataset across a semantically-partitioned space, and directly apply sampling methods on training imitation-learned agents.

% \jon{add safe-RL, adversarial training methods}

% things to include here: ding zhao's works \cite{ZhaoLPBLNP17}, russ's importance sampling \cite{OKellySNTD18}, scenic and other simulator-manipulating things

% \cite{OKellySNTD18} use adaptive search over learned distributions to change simulation parameters.  


\textbf{Data Aggregation Techniques.}
Online sampling frameworks for data aggregation such as DAgger~\cite{ross2011dagger} have been employed to address the problem of distribution shift when training imitation learning models.  HG-DAgger~\cite{kelly2019hgdagger} queries a human expert for supplemental corrective actions to improve the robustness of a learner.  Other variants, such as Safe-DAgger~\cite{zhang2017safedagger}, data aggregation for policy learning \cite{prakash2020daggerplus} and uncertainty-aware data aggregation \cite{cui2019uail} all propose query-efficient sampling.  The aims are similar to ours in that their attempts to reduce the data burden from human experts by focusing on safety- and uncertainty-related criteria.  However, in contrast to existing approaches, ours uses task-relevant properties and samples different environments where the learner diverges from the desired property set. We emphasize that our approach can be incorporated into any of the aforementioned approaches.