% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\input{math_commands.tex}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\newcommand*{\affaddr}[1]{#1} % No op here. Customize it for different styles.
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\email}[1]{\small{\texttt{#1}}}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\jtnote}[1] {{$\langle${\textcolor{red}{#1}}$\rangle$}}
%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2218} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Model Extraction Attacks on Split Federated Learning}

\author{%
Jingtao Li\affmark[1], Adnan Siraj Rakin\affmark[1], Xing Chen\affmark[1], Li Yang\affmark[1], Zhezhi He\affmark[2], Deliang Fan\affmark[1], Chaitali Chakrabarti\affmark[1]\\
\affaddr{\affmark[1] School of Electrical Computer and Energy Engineering, Arizona State University, Tempe, AZ}\\
\affaddr{\affmark[2] Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai}
\\
\email{\affmark[1]\{jingtao1, asrakin, xchen382, lyang166, dfan, chaitali\}@asu.edu};\; \email{\affmark[2]\{zhezhi.he\}@sjtu.edu.cn}
}
% \maketitle
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Federated Learning~(FL) is a popular collaborative learning scheme involving multiple clients and a server. FL focuses on protecting clients' data but turns out to be highly vulnerable to Intellectual Property~(IP) threats. Since FL periodically collects and distributes the model parameters, a free-rider can download the latest model and thus steal model IP. Split Federated Learning~(SFL), a recent variant of FL that supports training with resource-constrained clients, splits the model into two, giving one part of the model to clients~(client-side model), and the remaining part to the server~(server-side model). Thus SFL prevents model leakage by design. Moreover, by blocking prediction queries, it can be made resistant to advanced IP threats such as traditional Model Extraction~(ME) attacks. While SFL is better than FL in terms of providing IP protection, it is still vulnerable. In this paper, we expose the vulnerability of SFL and show how malicious clients can launch ME attacks by querying the gradient information from the server side. We propose five variants of ME attack which differs in the gradient usage as well as in the data assumptions. We show that under practical cases, the proposed ME attacks work exceptionally well for SFL. For instance, when the server-side model has five layers, our proposed ME attack can achieve over 90\% accuracy with less than 2\% accuracy degradation with VGG-11 on CIFAR-10.
% We show that under practical cases, the proposed ME attacks work exceptionally well for SFL.  For instance, when the server-side model has five layers, our proposed ME attack can achieve over 90\% accuracy with less than 2\% accuracy degradation with VGG-11 on CIFAR-10.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}


Federated Learning~(FL) has become increasingly popular thanks to its ability to protect users’ data and comply with General Data Protection Regulation policy. In FedAvg~\cite{mcmahan2017communication}, which is the most representative FL scheme, clients locally update their model copies, send them to the server which then aggregates the model parameters and sends the aggregated model back to the clients.
%and the parameter server collects them by averaging the model parameters, then again distributing the averaged model to its clients. 
Such a setting only allows model parameters to be shared with the server, and direct data sharing is avoided.
% One drawback of FL is its clients need to train the entire model locally, which is usually challenging for resource-limited edge devices. 
However, we notice that FL is vulnerable to Intellectual Property~(IP) threat as a malicious client can acquire the entire model for free~(\cref{fig:SFL_extraction}~(a)). Considering the cost of hosting the central server and effort in co-ordinate the training, and the danger in using stolen model to trigger adversarial attacks~\cite{goodfellow2014explaining}, the lack of model IP protection in FL is a significant issue.


\begin{figure}[htbp]
\centering
\begin{subfigure}{0.9\columnwidth}
  \centering
  \includegraphics[clip,width=0.95\columnwidth]{SFL_box.pdf}
  \label{fig:SFL_sub1}
\caption{IP threat in Federated Learning}
\end{subfigure}%
\hfill
\begin{subfigure}{0.9\columnwidth}
  \centering
  \includegraphics[clip,width=0.95\columnwidth]{SFL_query.pdf}
  \label{fig:SFL_sub2}
\caption{IP threat in Split Federated Learning}
\end{subfigure}
\caption{IP threats in Federated Learning. (a) FL suffers from direct model leakage. (b) SFL prevents direct model leakage and is resistant to existing ME attacks by blocking prediction query.}
\label{fig:SFL_extraction}
\end{figure}

Split Federated Learning~(SFL) scheme~\cite{thapa2020splitfed} is a variant of FL for training with resource-constrained clients, where the neural network is split into a client-side model and a server-side model. Each client only computes the forward/backward propagation of the smaller client-side model while the server, which has more compute resources, computes the forward/backward propagation of the larger server-side model.
%where the client-side model is shared among multiple clients and processed locally on their devices. 
%During training, clients only send the intermediate activations to server without sending raw data. Then, the heavy-duty computation is performed at the computation-power server and the computed gradients are sent back to clients. 
SFL follows the same model averaging routine as FL to synchronize the model. 
% SFL avoids collecting clients’ raw data as in FL.
% In addition to the aforementioned computation advantages, SFL can also provide model IP protection which is absent in FL. 
Unlike FL which passes on the entire model over to the clients, SFL preserves the server-side model and prevents the model from direct leakage.
Moreover, according to our investigation, SFL is resistant to existing Model Extraction~(ME) attack~\cite{tramer2016stealing, jagielski2020high} where querying a publicly accessible prediction API is needed. If 
% such an assumption is questionable in SFL and when 
SFL protocol does not allow prediction query access, all the prior ME attacks fail to succeed, as illustrated in~\cref{fig:SFL_extraction}~(b).


% While SFL has better IP protection ability than FL, it still is not immune to ME attacks.  

We intend to answer the following questions: \textbf{``Is SFL resistant to ME attacks? If not, then how can it be made resistant?''} To conduct the study, we test the IP threat resistance of SFL by attacking it. However, existing ME attacks~\cite{correia2018copycat, orekondy2019knockoff, truong2021data} cannot be performed since SFL presents an unique threat model where the access to prediction results is blocked.

In this paper, we propose five novel ME attacks specially designed under SFL's unique threat model. They are listed as Craft-ME, GAN-ME, GM-ME, Train-ME, and SoftTrain-ME. 
These ME attacks cover different gradient usage and data assumptions.
% on data such as noise data~(including randomly generated noise data)~\cite{truong2021data}, only auxiliary data~(out-of-distribution data) and training data~(in-distribution data). 
We also consider both train-from-scratch and fine-tuning SFL applications since ME attacks behave differently in these two cases. We benchmark the performance of five ME attacks on SFL for these two cases and find that when the number of layers in the server-side model~($N$) is small, ME attacks can succeed even without any data. However when $N$ is large, ME attacks fail badly, but SFL schemes with a large $N$ are not practical because fewer layers in client-side model could result in clients' data being compromised. 
%(since the main reason to use SFL is to protect clients' data).
Thus, \textbf{SFL is not inherently resistant to ME attacks}.
% Thus, this can be a key  SFL scheme design
To answer the \textbf{second question}, we find that using L1 regularization on client-side model can improve SFL schemes' resistance to ME attacks.
% we find that increasing layers in server-side model reduces layers in client-side model, improves resistance to ME attacks but compromises clients' privacy. 
% And we finally conclude that using L1 regularization on client-side model can improve SFL schemes' resistance to ME attacks.
In summary, we make the following contributions:

% FedGKT \cite{he2020group} where client do not keep a local copy of the global model, suffers from .

% Some exhibits potential for IP protection, but could also be problematic in real use. For example, FedGKT \cite{he2020group} where a global model is learned at the server while client do not keep a local copy. However, server do return prediction logits

% To answer the above questions, we investigate possible model extraction attacks targeting model-split FL schemes. We focus on SFL (with straightforward generalization to other schemes) as it offers better protection and utility.
% SFL scheme presents a unique threat model that has a major difference from traditional extraction attacks \cite{orekondy2019knockoff, jagielski2020high}. As shown in Fig.~\ref{fig:SFL_extraction} (a), it is a partial model extraction problem, where the attacker already knows part of the model and wants to extract the rest part. Moreover, as shown in Fig.~\ref{fig:SFL_extraction} (b), clients can perform gradient queries but do not necessarily have access to the prediction query. 
% Under the unique threat model, we propose five model extraction attacks, which are Craft-ME, GAN-ME, GM-ME, Train-ME, and SoftTrain-ME with exhaustive combination of different gradient usage and different data assumptions. 
% We compare the effectiveness of five extraction attacks in a real SFL setting to elect the strongest attack, whose effectiveness provides a new evaluation aspect for SFL scheme. 
% We show that with a larger number of layers in the server-side model, SFL scheme can protect model IP well against both accuracy and reconnaissance attacking goals. In summary, we contribute in several aspects: 

% To answer the above questions, we investigate possible model extraction attacks targeting SFL scheme. SFL scheme presents a unique threat model that has a major difference from traditional extraction attacks \cite{orekondy2019knockoff, jagielski2020high}. As shown in Fig.~\ref{fig:SFL_extraction} (a), it is a partial model extraction problem, where the attacker already knows parameters of client-side model and wants to extract the server-side model. Moreover, as shown in Fig.~\ref{fig:SFL_extraction} (b), the attacker can perform gradient queries but do not necessarily have access to the prediction query. 
% Under the unique threat model, previous extraction attacks fail to work. Hence, we propose five model extraction attacks, which are Craft-ME, GAN-ME, GM-ME, Train-ME, and SoftTrain-ME targeting different gradient usage and data assumptions in SFL. We benchmark the IP protection strength based on the performance of five extraction attacks in a practical SFL setting and show that vulnerability to extraction attacks strongly correlates with the number of layers in server-side model. To provide good IP protection, we must increase the number of layers in the server-side model, which will increase server computation overhead and sacrifice clients’ data privacy, revealing an interesting tradeoff. 
% We demonstrate with proper measure~(sufficient layers \& defensive methods), SFL scheme can protect model IP well against both accuracy and adversarial goals. In summary, we contribute to several aspects:

\begin{itemize}
    % \item We perform the first study on investigating model extraction attacks under the unique threat model of SFL. We propose five model extraction attacks exhaustively according to different usage of gradients and data assumptions, which demonstrate the vulnerability of SFL scheme against a malicious extraction attacker.
    
    % \item We find with enough layers being split to the server-side model, SFL can be resistant even to the strongest model extraction attack. We reveal the tradeoff between IP protection, data privacy, and computation overhead of SFL, which provides valuable information on the SFL scheme design. 
    
    % \item We demonstrate the IP protection performance of applying model splitting in FL. Against the strongest model extraction attack, SFL scheme achieves good protection against both accuracy and reconnaissance goals.
    \item We are the first to show that SFL is not immune to ME attacks. We define a practical threat model where prediction query is blocked and attacker has white-box assumption on client-side model and gradient query access. We propose five novel ME attacks under this threat model.  We demonstrate how the white-box assumption on client-side model and gradients information can be used to extract the model. To the best of our knowledge, this is the first work that performs a comprehensive study on ME attacks in SFL.
    
    % \item For a 5-layer-in-server VGG-11 SFL model, even without original training data, attacker can derive a surrogate model with around 85\% accuracy on CIFAR-10. 
    %studies ME attacks for SFL, \textcolor{red}{which is to testify whether SFL can be used for model IP protection}.
    
    \item We study the effect of size of server-side model in SFL  on the success of ME, we find that when server-side model is small, our proposed ME attacks are successful. For a 5-layer-in-server VGG-11 SFL model, even without original training data, attacker can derive a surrogate model with around 85\% accuracy on CIFAR-10. 
    But when client-side model is small, we find that the ME attacks are hard to succeed. However, the small client-side model compromises clients' data and hence is not a good design choice.
    
    \item  To make SFL resistant to ME attacks, we provide a potential ME defense based on L1 regularization and show how it reduces the ME attack performance.
    %However, such a split model configuration compromises clients' privacy. We believe more balanced ME resistance and data privacy can be achieved by considering this tradeoff. 
    

\end{itemize}


\section{Related Work}



\subsection{Model-split Learning Schemes}
The key idea for model-split learning schemes is to split the model so that part of it is processed in the client and the rest is offloaded to the server.
This idea was first proposed in \cite{kang2017neurosurgeon, teerapittayanon2017distributed, liu2018edgeeye} for inference tasks and extended by
%offload majority of the computation overhead to the server.
\cite{gupta2018distributed} for split learning, a collaborative multi-client neural network training. However, the round-robin design in~\cite{gupta2018distributed}  need clients to learn sequentially and thus required long training time. 
%adopts the knowledge distillation idea by transmitting both parties' logits to perform confidence score matching. It 
% supports clients' parallel learning and saved communication overhead due to gradient transmission. However, clients need access to prediction logits, making the scheme vulnerable to ME attacks. 
%SFL is proposed by \cite{thapa2020splitfed}, where clients' learning can simultaneously proceed with a periodic model synchronization like FedAvg~\cite{mcmahan2017communication}. 

\subsection{Split Federated Learning}
In SFL scheme, clients process their local models in parallel and perform periodic synchronization as in FedAvg~\cite{mcmahan2017communication}.
The detailed process of SFL is shown in Algorithm~\ref{alg:sfl}, where we reference to the SFL-V2 scheme~\cite{thapa2020splitfed}. \textbf{We define $N$ as the number of layers in server-side model, as the key design parameter}.

At the beginning of each epoch, server performs the synchronization of client-side model and sends the updated version to all clients. Then, clients perform forward propagation locally till layer $L-N$~(the last layer of client-side model), sending the intermediate activation $\mA_i$ to the server~(line 8). Server accepts the activation and label $\vy_i$ sent from clients, and uses them to calculate the loss and initiates the backward process (line 9). 
The backward process~(line 10) consists of several steps: server performs backward propagation on the loss, updates server-side model and sends back gradient $\nabla_{\mA_i} \Ls$ to clients. Clients then continue the backward propagation on their client-side model copies and perform model updates accordingly. 

% While FedGKT~\cite{he2020group} leaks prediction logits to clients, SFL does not, making it a promising candidate against ME attacks.
% SFL splits the last $N$ layers as the secret server-side model. While for the non-secret \textbf{client-side model with $L - N$ layers}, it is distributed to clients and processed on clients' local devices.

\begin{algorithm}[t]
\caption{Split Federated Learning} 
\label{alg:sfl}
\begin{algorithmic}[1]
\REQUIRE $\quad$ For $M$ clients, instantiate private training data ($\rmX_i, \rmY_i$) for $1, 2, ..., M$. Server-side model $S$ has $N$ layers and client-side model $C_{i}$ has $L-N$ layers.

% Train} ($\rmX_i$, $\rmY_i$)
\STATE initialize $C_{i}, S$
\FOR{epoch $t\gets 1$ to num\_epochs}{
    \STATE $C^{*} = \frac{1}{M}\sum_{i=1}^{M}{C_{i}}$  \hfill\COMMENT{\textcolor{blue}{Model Synchronization}}
    \STATE $C_{i} \gets C^{*} $ for all $i$
    %\COMMENT{\textcolor{gray}{Model Sync.}}
    \FOR{step $s\gets 1$ to num\_batches }{
    \FOR{client $i\gets 1$ to $M$ \textbf{in Parallel}}{
    
     %\COMMENT{\textcolor{gray}{Epoch Counter}}
    \STATE data batch ($\vx_i, \vy_i$) $\gets$ ($\rmX_i, \rmY_i$)
    \STATE $\mA_i = C_{i}(\mW_{C_i};\vx_i)$ \hfill\COMMENT{\textcolor{blue}{Client forward; send $\mA_i$ to Server}}
    }
    \ENDFOR
    \\\hrulefill
    \FOR{client $i\gets 1$ to $M$ \textbf{in Sequential}}{
    \STATE $\Ls = \Ls_{CE}(S(\mW_{S};\mA_i), \vy_i)$ \hfill\COMMENT{\textcolor{blue}{Server forward}}
    
    %\COMMENT{\textcolor{gray}{CE Loss}}
    % \STATE $\min_{\mW_{C}^i,\mW_S}(loss)$ 
    \STATE $\nabla_{\mA_i} \Ls  \gets$ back-propagation
    \hfill\COMMENT{\textcolor{blue}{Server backward, send $\nabla_{\mA_i} \Ls$ to Client}}
    }
    \STATE Update $\mW_{S}$;
    \ENDFOR
    
    \hrulefill
    \FOR{client $i\gets 1$ to $M$ \textbf{in Parallel}}{
    \STATE $\nabla_{\vx_i} \Ls  \gets$ back-propagation
    \hfill\COMMENT{\textcolor{blue}{Client backward}}
    \STATE Update $\mW_{C_i}$;
    }
    \ENDFOR
    
    
    }
\ENDFOR
}
\ENDFOR
\end{algorithmic}

\end{algorithm}


\subsection{Model Extraction Attack}
In SFL, the model is split and so IP threat due to directly downloading the model is non-existent. However, there exists advanced IP threats due to ME attacks.
Such attacks are first demonstrated in \cite{tramer2016stealing}, and the follow-up work \cite{jagielski2020high} shows that high fidelity and accurate model can be obtained with very few model prediction queries.

A successful ME attack not only breaches the model IP, but also makes the model more vulnerable to attacks. ME attack can support transferable adversarial attacks~\cite{goodfellow2014explaining}, mainly targeted ones~\cite{madry2017towards} against the victim model. A high-fidelity surrogate model also be used to perform bit-flip attacks~\cite{rakin2019bit}; for instance, a few bit flips on model parameters can degrade ResNet-18 model accuracy to below 1\%.

\subsection{Data protection in SFL}
Similar to FL, SFL scheme protects clients' data by not sending it directly to the server. However, data protection in SFL can be compromised by attacks such as MI attacks. In model-based MI attack~\cite{fredrikson2015model}, the attacker trains an inverted version of client-side model and can directly reconstruct raw inputs from the intermediate activation. Recent works~\cite{vepakomma2020nopeek, li2022ressfl} provide practical ways to mitigate MI attacks. However, they cannot achieve satisfactory mitigation when the client-side model has very few number of layers (less than 3 in a VGG-11 model).

% when it can be trained to destroy redundant private information while leaving minimum useful features.
% The intermediate activation can be obtained either by eavesdropping on the communication or intermediate activation
% eavesdropping on the intermediate activation transmitted from clients to server.
% which is the major threat for its basic level data protection. 


% \begin{figure*}[ht!]
% \centering
% \begin{subfigure}{.66\columnwidth}
%   \centering
%   \includegraphics[width=1.0\linewidth]{result_N_8.pdf}
%   \label{fig:analysis_sub1}
% \end{subfigure}%
% % \hfill
% \begin{subfigure}{.66\columnwidth}
%   \centering
%   \includegraphics[width=1.0\linewidth]{result_N_5.pdf}
%   \label{fig:analysis_sub2}
% \end{subfigure}
% % \hfill
% \begin{subfigure}{.66\columnwidth}
%   \centering
%   \includegraphics[width=1.0\linewidth]{result_N_2.pdf}
%   \label{fig:analysis_sub3}
% \end{subfigure}
% \caption{tSNE analysis of intermediate activation with different $N$ (8, 5, 2 from left to right) on a VGG11 model on CIFAR-10 dataset. $N$ denotes number of layers in server-side model in SFL.}
% \label{fig:difficulty_analysis}
% \end{figure*}

\begin{figure*}[htbp]
\centering
\begin{subfigure}{.66\columnwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{noblock_attack.pdf}
  \label{fig:noblock_sub1}
\caption{}
\end{subfigure}%
\begin{subfigure}{.66\columnwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{noblock_datafree.pdf}
  \label{fig:noblock_sub2}
\caption{}
\end{subfigure}
\begin{subfigure}{.66\columnwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{inconsist_grads.pdf}
  \label{fig:noblock_sub3}
\caption{}
\end{subfigure}
\caption{Case study VGG-11 CIFAR-10 model with different $N$: If prediction API access is allowed, existing ME attacks are very successful on SFL which suggests prediction API access should be blocked. (a) ME attacks using CIFAR-100 as auxiliary dataset; (b) Data-free ME attack that demonstrates ME attack on part of the model is much easier than ME attack on the entire model; (c) Inconsistent gradient problem in training-from-scratch SFL. The y-axis denotes the change in gradient (lower means more consistent) for the same inputs in different epochs.}
\label{fig:traditional_ME}
\end{figure*}


\section{Threat Model}
\label{sec:attacks}
% In this section, we investigate potential threats on breaching SFL's model IP protection. There is abundant interfaces in the SFL scheme that can potentially leak the server-side model. As described in algorithm~\ref{alg:sfl}, SFL training process leaks (1) client-side model, (2) gradient w.r.t. clients' input \& label pair. To investigate whether using (1) and (2) can extract the entire model, we investigate possible model extraction attacks. Before that, 
% To investigate model extraction attacks, we first give the formal threat model:


% \subsection{Threat Model}
\subsection{Attacker Assumptions}
\textbf{Objective.} 
According to \cite{jagielski2020high}, there are three  model extraction (ME) attack objectives: i) functional equivalence, ii) high accuracy, and iii) high fidelity. Since achieving functional equivalence is difficult in practical applications most of the existing practical ME attacks focus on achieving high accuracy and fidelity. To achieve the accuracy goal, the attacker aims to obtain a model that maximizes the prediction correctness and to achieve the fidelity goal, the attacker aims to derive a model with a similar decision boundary as the victim model before launching adversarial attacks~\cite{biggio2013evasion}.

\textbf{Data Assumption.} We assume that attackers' data assumption can fall into three categories: (1) noise data, (2) natural auxiliary data or (3) limited amount of training data.
Having noise data represents cases where the attacker uses randomly generated noise data. This case can happen when the attacker participates as a ``free-rider'' without contributing any data, or when the attacker does not have a \textit{similar enough} dataset. The second case is motivated by \cite{truong2021data} where it is shown that it is better to use random noise rather than use a drastically different dataset. In natural auxiliary data assumption, the attacker has an auxiliary dataset that is similar but with different labels from the victim's training data. For example, CIFAR-100 is such an auxiliary dataset for CIFAR-10. Furthermore, we assume a practical case where the attacker has only a subset of original training data.
%corresponding to the ideal attacker.

\textbf{Capabilities.} We assume the attacker participate in a multi-client SFL scheme 
%and play against the server~(model owner).
% For the SFL scheme, the entire model of $L$ layers is split to two parts, a server-side model with $N$ layer and a client-side model with $L-N$ layers. 
as outlined in~\cref{fig:SFL_extraction}. We assume the entire model has a total of $L$ layers (or layer-like blocks, i.e. BasicBlock in ResNet) out of which the server processes $N$ layers.
The attacker holds \textit{white-box assumption} on the client-side model~(consists of $L-N$ layers), that is, it knows the exact model architecture and parameters for those layers. The attacker holds a \textit{grey-box assumption} on the $N$-layer server-side model, that is, it knows its architecture and loss function
% \footnote{release as public accessible information, ablation study discusses the significance of architecture} 
while the model parameters are unknown. 
Also, we assume \textit{server blocks the prediction queries} thus neither logits nor prediction labels are accessible by clients during training, but server \textit{allows gradient queries} to let client-side models be updated. 
Based on a client's activation $\mA=C(\vx)$ and its label $\vy$, gradient information $\nabla_\mA \Ls$ is computed and sent back to clients.
% In addition, we assume the attacker can perform gradient queries on any input $\vx$.


\subsection{Analysis}

% Attacker may need to send noisy or out-of-distribution inputs, we assume the server has no outlier detection mechanism and accept all queries.

% \textbf{Data Assumption.} Whether the attacker has data or not and how much data plays a important role in the model extractions. We cover all three cases: zero data, auxiliary data and part of original training data. 
\textbf{Reasons to block prediction APIs.} 
%SFL scheme does not explicitly require prediction APIs for operation. However, si
Allowing predictions APIs makes SFL vulnerable. This is particularly so since according to the white box assumption, SFL gives away the client-side model and so the attacker only needs to extract the server-side model to reveal the entire model. This results in an easier problem setting than most traditional ME attacks' assumption.
% We observe client-side model heavily regularizes the feature space of its output (input of server-side model), making ME attacks easier to succeed, especially when $N$ is small.
% As shown in~\cref{fig:difficulty_analysis}, on a VGG-11~\cite{simonyan2014very} model on CIFAR-10 dataset, as $N$ becomes smaller, tSNE embeddings of intermediate features with different labels are easier to distinguish. For $N=2$, ME attack is as simple as separating different clusters with a linear layer. 
Under this easier problem setting, existing ME attacks can be very successful if  prediction APIs are \textbf{not} blocked.
Specifically, we investigate CopyCat CNN~\cite{correia2018copycat}, Knockoff-random~\cite{orekondy2019knockoff} and data-free ME~\cite{truong2021data}.
As shown in~\cref{fig:traditional_ME}~(a), with auxiliary data (CIFAR-100) and enough query budget, both attacks derive a surrogate model with very high accuracy even for a large $N$ setting.
Moreover, attacker with noise data can also succeed with data-free ME 
as shown in~\cref{fig:traditional_ME}~(a). When the query budget is equal to 2 million, the data-free ME can extract the model with high accuracy even when $N$ is equal to 5. This justifies the reason why SFL should block prediction APIs.


% Thus, we believe a reasonable server party would block the prediction query access since it is completely unnecessary in SFL protocol.

% We study existing ME attacks with different data assumptions on an SFL setup, if the attacker is allowed to access prediction of the server-side model.
% For attacker with natural auxiliary data, we investigate two attacks: CopyCat CNN \cite{correia2018copycat} gets the prediction label of the auxiliary dataset and train the surrogate model, and Knockoff-random \cite{orekondy2019knockoff} where the attacker has access to confidence score and use score matching with respect to the auxiliary data in the surrogate model training. 

%  Study: If SFL allows prediciton.}

% With prediction query access being blocked, existing methods fail to launch, which 
% (data-free attacking methods such as xx, attacking methods using OOD data such as xxx).
% \textbf{Limited.}

% , however, compared to Knockoff-train, the confidence score information is missing and results in a worse extraction performance. 

% Thus, we assume attacker \textit{does not have access to prediction query} $S(\vx)$ for a input $\vx$ since it is not a necessary step. 
%  Because if such access was provided, standard model extraction attack would thrive and IP protection would be much harder. 
% To justify this, we studied the cases with free access to prediction query. 
% After the SFL training, the server open prediction query access to clients on the trained global model. We assume the attacker can only get label information from prediction query. Following the basic assumption to perform knockoff method in xx, we allow the attacker to have an auxiliary dataset.
% On a classification task of CIFAR-10 dataset with VGG-11 model, the attacker uses CIFAR-100 as auxiliary dataset. As shown in xx, using knockoff method, The attacker can approach very similar accuracy as the original model, and the accuracy only degrades a little even with a very large $N$.
% but without prediction query access, the attacker cannot apply knockoff method. Instead, gradient-matching can be used, however, the extracted accuracy is much worse than using knockoff. 
% we use the GAN-ME attack (introduced later, the best performer for auxiliary-only attacker)

% \textbf{Stable/Unstable gradient query for different use case.} 
% For SFL with \textit{pre-trained backbone} model that most part of the model is frozen for transfer learning purposes \cite{park2021federated, tian2022fedbert}, gradient query access can be seen as stable that the client-side as well as server-side model are stable as only a few parameters are updated.
% For SFL with \textit{training-from-scratch} setting \cite{thapa2020splitfed}, the gradient query is unstable as the server-side model and client-side model are both constantly updating. The unstable gradient query make it more difficult for model extractions as gradient queries made at different time are not consistent. 



% The gradient can only be returned from the server during the training when both client-side model and server-side model is constantly changing.
% To conquer the instability, we observe a good time-window that is when the model almost converges till the end of the training. At the time window, no only the target model has a high accuracy and is worthwhile to extract, but also the learning rate being scheduled low hence model parameters being more stable.

\textbf{Ensuring consistency of gradient query.}
%\jtnote{After ruling out existing ME attacks by blocking the prediction API, w
We find Gradient consistency plays an important role for our proposed ME attacks.
For fine-tuning applications~\cite{park2021federated}, attackers get consistent gradient information from gradient query, as server-side model parameters are frozen or updated with a very small learning rate.
However, for a training-from-scratch usage, queries to SFL model obtain inconsistent gradient information as the server-side model drastically changes during training. As shown in~\cref{fig:traditional_ME}~(c), for the same query input, the gradient is drastically different in different epochs.
% We observe for the last several epochs, the change in gradients are relatively small compared to epoch 139 to 162.




\section{Proposed Model Extraction Attack}


\begin{figure}[htbp]
\centering
\begin{subfigure}{.95\columnwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{attack_demo1.pdf}
  \label{fig:attack_sub1}
\caption{}
\end{subfigure}%
\hfill
\begin{subfigure}{.95\columnwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{attack_demo2.pdf}
  \label{fig:attack_sub2}
\caption{}
\end{subfigure}
\caption{Demonstration of proposed ME attacks: (a) ME attacks without training data. (b) ME attacks with training data.}
\label{fig:attacks}
\end{figure}


\begin{table*}[htbp]
\caption{Model Extraction Attack Methods in SFL}
\centering
% \begin{tabular}{lcc} 
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lcccc}
 \toprule
% \hline
 \textbf{Method} & \textbf{Data Assumption} & \textbf{Prediction Query}  & \textbf{Gradient Usage} & \textbf{Client-side model Usage}\\
% \hline
\midrule
Existing MEs & Varies & \textcolor{red}{Required} & None/Assistive & None\\
Naive Baseline & Limited Training & Not Required & None & None\\
\midrule
 Craft-ME & Noise Data & Not Required & Data Crafting& Initialization\\
 GAN-ME & Noise Data & Not Required & Data Generator& Initialization\\
 GM-ME & Natural Auxiliary & Not Required & Gradient Matching& Initialization\\
 \midrule
 Train-ME & Limited Training & Not Required & None & Initialization\\
 SoftTrain-ME & Limited Training & Not Required & Soft Label Crafting& Initialization\\
 
 \bottomrule
\end{tabular}
}
\label{tab:attacks}
\end{table*}

Previously, \cite{milli2019model} demonstrated that using gradients to reveal one-layer linear transformation is trivial. Given $f(\vx) = \mW^T \vx$, one can directly infer $\mW$ from a single gradient query given that $\mW^T = \nabla_\vx f(\vx)$.
However, using gradient only can go no further than one layer. \cite{milli2019model} shows that to recover a two-layer ReLU network of the form $f(\vx) = \sum_{n=1}^{h} g(\vx)_i\mW_i\mA^T_i\vx$, where $g(\vx) = \mathbb{1}$ $\{\mA\vx > 0\}$, $\mA$ is of $\R^{h\times d}$ and $\mW$ is of $\R^h$, using input gradient can recover the absolute value of normal vectors $|\mW_i\mA_i|$ for $i \in [h]$. In order to get the sign information of $\mW_i\mA_i$, prediction query is required which is not supported by  SFL's threat model.

So in this paper, we investigate approximate ME attacks that differ in the data assumptions, gradient usage and loss choices. We propose five novel ME attacks as shown in \cref{fig:attacks}. Each row corresponds to a ME attack from start to finish. For example, Craft-ME first queries the gradients to craft a dataset and uses the crafted dataset to train the surrogate server-side model from scratch using cross-entropy loss.
%We can see five proposed attacks have different data assumptions, gradient usages and loss choices. 
Despite the differences, the five proposed attack methods all train a randomly initialized surrogate server-side model from scratch. For comparison, we also include a naive baseline that ignore white-box assumption and gradients by directly training the entire model from scratch. 
We provide the detailed requirement of traditional ME attacks, naive ME attacks and our proposed five ME attacks in Table~\ref{tab:attacks}. 





%\textcolor{red}{Though with a somewhat familiar algorithmic form, they are conditioned to the unique threat model with a unprecedented problem setting, for which we claim the novelty.}



% Despite the differences, the five proposed attack methods follow the same strategy, that is, they all train a randomly initialized surrogate server-side model from scratch. 
% We provide a detailed illustration of proposed five attacks in~\cref{apx:attack_illustrate}.

% that a surrogate model is initialized which has the same architecture as the server-side model, and freeze the known client-side model in training of the surrogate model.

% We focus on the approximate model extraction methods with different data-availability assumptions. We find that for ``free-rider'' attackers without data or with natural OOD data, model extraction of SFL server-side model is possible with only gradient query access. We will also discuss in-depth for the strongest attacker that with in-distribution (IND) data (i.e. a subset of training data), as it is ordinary in SFL setting that a benign client contribute genuine training data. We show that attacker with very limited IND data can beat ``free-rider'' attackers without using the gradient query. We further show with gradient access, the extraction performance can be further enhanced.
% \tofill{categorize into three ways of manipulating grads. give a exhaustive feeling, add a column to this table.}
% \begin{table*}[htbp]
% \caption{Model Extraction Attack Methods in SFL}
% \centering
% % \begin{tabular}{lcc} 
% \resizebox{0.9\linewidth}{!}{
% \begin{tabular}{lcccc}
%  \toprule
% % \hline
%  \textbf{Method} & \textbf{Data Assumption} & \textbf{Prediction Query}  & \textbf{Gradient Usage} & \textbf{Client-side model Usage}\\
% % \hline
% \midrule
%  Craft-ME & None & No & Data Crafting& Initialization\\
%  GAN-ME & None & No & Data Generator& Initialization\\
%  GM-ME & Natural Auxiliary & No & Gradient Matching& Initialization\\
%  \midrule
%  Train-ME & Limited Training & No & None & Initialization\\
%  SoftTrain-ME & Limited Training & No & Soft Label Crafting& Initialization\\
%  Naive Baseline & Limited Training & No & None & None\\
%  \bottomrule
% \end{tabular}
% }
% \label{tab:attacks}
% \end{table*}





% So far, we present five model extraction methods, including two data-free extraction attacks (Craft-ME, GAN-ME) and three data-available extraction attacks (Grad-match-ME, Train-ME and Soft-train-ME).

% Because of above 
% Thus, we show that even without prediction query access, extraction attack can still be successful under some hard conditions (i.e. without data).  While under the same condition, extracting the entire model is impossible.
% \textbf{Major difference from traditional model extraction.} 
% The problem we study has several major differences from traditional model extraction study
% Not only we assume the attacker has no access to the prediction query to get confidence score, but also the underlying problem is different:
% traditional Model extraction extracts the entire model, where the input to the unknown model has wide dynamic range (as data distribution range is large). In SFL, model extraction is performed only on the unknown part, last few layers of a DNN. 
% For a well-trained model, the dynamic range of the activation input to the unknown part is usually smaller than the data distribution itself (taking confidence score as an extreme case). 
% Thus, extracting same amount of layers in SFL is a easier task compared to the traditional model extraction. For example, without data, recovering a 2-layer ReLU network could be hard, however, we will show recovering last 2 layers of a VGG-11 network could easily be done in a SFL setting.


% We study this problem according to the following sequences: We first study a restricted case, data-free attacker assumption and visit both exact and approximate model extraction methods and an unrestricted case where attacker with part of original training data perform approximate model extraction.

% \subsection{Exact Model Extraction}

% % \cite{carlini2020cryptanalytic} introduces systematic methodology to extract high fidelity model from the inference API, however, SFL does not satisfy the inference request. Interestingly, 
% We first investigate exact ME attack that recover model with its exact parameter (up to a scaling factor).
%  For the extraction of a two-layer ReLU network of the form $f(\vx) = \sum_{n=1}^{h} g(\vx)_i\mW_i\mA^T_i\vx$, where $g(\vx) = \mathbb{1}$ $\{\mA\vx > 0\}$, $\mA$ is of $\R^{h\times d}$ and $\mW$ is of $\R^h$.
% Using input gradient can recover the absolute value of normal vectors $|\mW_i\mA_i|$ for $i \in [h]$. Still, the sign information of $\mW_i\mA_i$ requires prediction query $f(\vx)$ on input $\vx$. This shows even a two-layer ReLU network cannot be exactly extracted solely using input gradients.
% % \cite{carlini2020cryptanalytic} extends  \cite{milli2019model} to neural networks with arbitrary depth. Similarly, it needs 

% \begin{remark}
% \textit{Exact ME attack can be done up to one linear transformation. With best effort, exact ME on a two-layer ReLU network would fail.} 
% \end{remark}



\subsection{ME Attacks without Training Data}

% For four of the five proposed extraction methods, we exploit gradient query to the victim model.  Because that gradient query can only be done in training process, the query budget is very limited in SFL (equals to number of data times number of epoch).
% By the definition of approximate model extraction in \cite{jagielski2020high}, the attacker's objective can be either \textit{accuracy} and \textit{fidelity}. In terms of high accuracy, the attacker's objective is to obtain a model by maximizing the prediction correctness to improve the quality of the service (QoS). On the other hand, the attacker can also persue high fidelity (function equivalency) as reconnaissance prior to improve the effectiveness of other attacks (i.e. adversarial input attacks \cite{biggio2013evasion}, bit-flip attacks \cite{rakin2019bit}). 



% Gradients at the beginning of the training can hardly be valuable if the victim model is training from scratch. For this case, effective gradient query can be performed only during the \textit{near-convergence time window} of the training.




% or only has out-of-distribution dataset, which has different labels with the target dataset (i.e. CIFAR-10 to CIFAR-100). 
% does not present similarity to the target domain, that using a bad prior might lead to a poor approximation of the victim \cite{truong2021data}.
First, we proposes three attacks under weaker data assumptions when the attacker has noise data and natural auxiliary data.

\textbf{Crafting model extraction (Craft-ME).} 
% Inspired by previous analysis in~\cref{fig:difficulty_analysis}, model extraction is equivalent to find out the decision boundary of clearly distinguishable clusters (for a small $N$). We only need to find synthetic data that is representative of each cluster and derive a classification model to extract the target function. 
Inspired by \cite{han2018co}, where data-label pairs (referred to as instances) with small-loss are shown to present useful guidance for knowledge distillation, we 
propose a simple method to craft small-loss instances using gradient queries and use them to train the surrogate model.
We initialize random input $\vx_r$ for every class label $c$, and use the gradient $\nabla_{\vx_r} \Ls$ to update $\vx_r$. For each input, updating is repeated for a number of steps.
% (no threshold can be set as the loss value is not observable for the attacker only has the client-side model).
By varying label $c$, a collection of small-loss instances is derived during SFL training. Then, a surrogate model is trained from scratch on these small-loss instances. 
% The biggest advantage of this method is that it requires fewer queries and many small-loss instances can be crafted using a limited number of queries. 
% However, its extraction performance can suffer due to the unstable gradient query of SFL, for that small-loss instances crafted at an earlier time can become high-loss instance for a later state and thus hurt the extraction performance. 

\textbf{GAN-based model extraction (GAN-ME).} 
% \tofill{address its adaptation ability.}
Recent work~\cite{truong2021data} proposes a GAN-based approach for data-free ME. The key idea is to use a generator $G$ to continually feed fake inputs to the victim model $V$ and surrogate model $S$, and use confidence score matching to let $S$ approach $V$. 
% The original methods instantiate a generator $G$, feeding Gaussian noise $\vz$, and generate fake input $\vx_f$ to the victim model $V$ and surrogate model $S$, which returns model prediction query of $V(\vx_f)$ and $\nabla_{\vx_f} V$ (can be approximated if not available). 
% $V(\vx_f)$ is used to train $S$ to approximate the victim model $V$ (confidence score matching), and the $\nabla_{\vx_f} V$, $\nabla_{\vx_f} S$ are used to train the generator. 
However, the confidence score matching needs prediction query which is not allowed in our case. Thus, we adapt the GAN-based method to gradient-query-only case and propose a two-step method: 
% \begin{itemize}
%     \item (online) We train a conditional-GAN (c-GAN) on the victim model;
%     \item (offline) We use the c-GAN to generate small-loss instances and use them to train a surrogate model from scratch.
% \end{itemize}
First, a conditional-GAN~(c-GAN) model $G(z|c)$ is initialized. The attacker trains the generator during victim model's training by generating fake data $\vx_f$ and label $c$ and performing gradient queries to update $G$. 
After the training is done, generator $G$ is used to supply small-loss instances ($\vx_f, c$) to train the surrogate model (the unknown part). We observe a serious mode collapse problem during the GAN training. So we utilize the distance-aware training introduced in \cite{yang2019diversity}, to encourage the c-GAN to generate more diverse small-loss instances.
In the new method, the training of the generator is not based on a min-max game, 
%which is the major difference from \cite{truong2021data} 
or on traditional GAN training. Instead, it simply trains the generator toward minimizing cross-entropy loss.
While the generator $G$ fails to generate natural-looking inputs even upon convergence, it generates abundant small-loss instances for every label, and divergence loss helps it generate a variety of outputs. During SFL training, the generator  adjusts  to the changing server-side model. 
% A shortcoming of this method is the training of c-GAN needs a longer convergence time.

% \textbf{Describe a ideal setup. and the success of this attack.}
% However, the simple migration fails under a realistic multi-client setting. We present empirical results (with detail setting in appendix) showing that even with poisoning and state-of-the-art anti mode collapse regularization, generator fails to converge.
% First, the poisoning effect is neutralized by benign data sent by other clients. The resulting training of the generator fails to establish a min-max game. By only updating generator to minimize the cross-entropy loss of the server-side model, cannot generate meaningful inputs.



% ed simply because a min-max game can never be established. As the  poisoning step in \cite{pasquini2021unleashing} only works for a single class retrieval

% Thus, server would regard all data being authentic and lack of mechanism to mark fake data generated by the conditional generator as fake. Thus, generator fails to generate meaningful data.

% At the beginning of SFL training, We initialize 
% The \textbf{conditional generator training fails under a well-trained discriminator (victim model)}. Setting VGG-11 as the victim model, we simply get noise-looking images and with serious mode collapse, shown in xxx.


% \textbf{conditional generator training}

% We also found evidence that even a well-trained conditional data generator is not capable to fully recover the accuracy.

% \begin{remark}
% \textit{Without data, GAN-based approximate extraction method can not be done with only input gradient query.} 
% \end{remark}

% utilize confidence score for matching the discriminator
% SO in this case, the only option for the attacker is to perform cryptanalytic methods to crack the model parameter. 


\textbf{Gradient matching model extraction (GM-ME).}
Gradient matching (GM) in ME attack has been investigated in \cite{jagielski2020high, milli2019model} and is used in combination with prediction query to improve the extraction performance. Since, in SFL, prediction query is not allowed, we navigate this strict threat model's restriction by adopting gradient matching (GM) loss.
For a given label $\vy_i$, GM loss has the following form:
% \Ls_{CE}(S(\mW_{S};\mA^i_{ t}), \vy_i)
\begin{equation}
    \Ls_{GM} = |\nabla_{\vx_i}\Ls(S(C(\vx_i)), \vy_i) - \nabla_{\vx_i} \Ls(V(C(\vx_i)), \vy_i)|^2_2
\end{equation}
where, $\vx_i$ denote inputs, $C$ denotes client-side model, $S$ and $V$ denotes the surrogate model and victim model, respectively. For each input, attacker would query gradients with different label $\vy_i$ to get as much information as possible. 
% We differentiates the loss function $\tilde{\Ls}$ used by the attacker from the loss function $\Ls$ in the server.
This attack performs extremely well for small $N$ but degrades significantly for a larger $N$. Its performance also depends on the domain similarity between the auxiliary dataset and the victim dataset. 
% We tested if using pure noise to perform gradient matching, performance degrades drastically.


\subsection{ME Attacks with Training Data} Next we describe the strongest data assumption case, i.e., the attacker has a subset of training data,
% , which is realistic in SFL setting, since attacker's data can contribute to the final model.
% for that attacker can contribute genuine data to the model training process.
% We then discuss the strongest data assumption that attacker has a subset of training data. The other case is the attacker has a subset of the training data, representing the strongest attacker assumption. This latter case is realistic in SFL setting for that attacker can contribute genuine data to the model training process.

\textbf{Training-based model extraction (Train-ME).} For attackers with a subset of the training data, derivation of an accurate surrogate model can be done using supervised learning (through minimizing the cross entropy loss on the available data). We call this Train-ME, similar idea is also adopted in \cite{fu2022label} to extract the entire model of the other party.
Train-ME only relies on the white-box assumption of the client-side model, using it to  initialize the surrogate model and does not need to use the gradient query at all. Surprisingly, it is one of the most effective ME attacks.
%mostly because of it has no requirement on gradients.
% During training, gradient query information can also assist in surrogate model training as a loss term $\Ls_{GM}$ with a constant $\beta$ to control its strength. 
% However, as the classification loss term needs ground-truth label, this method cannot be done for attacker only having auxiliary dataset.

% However, the former case turns out to be quite meanningless in SFL model extraction. In traditional extraction, an auxiliary dataset can be used quite well, confidence score matching can be done by using auxiliary dataset in prediction query. In contrast, auxiliary dataset is hard to utilize in SFL model extraction, directly feeding it to input gradient query can hardly get anything valuable.


\begin{table*}[htbp]
\renewcommand*{\arraystretch}{1.3}
\caption{ME attack performance on SFL on fine-tuning and training-from-scratch applications. The victim is a VGG-11 model on CIFAR-10 with 91.89\% validation accuracy. For Train, SoftTrain and Naive baseline, for the fine-tuning setting, data assumption is 1K training data (randomly sampled), and for the train-from-scratch setting, the number of clients is 10 and each client has 5K training data.}
\label{tab:stable_brief}
\begin{center}
\begin{small}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{|l|c|ccc|ccc|ccc|ccc|} 
 \hline
 \multirow{2}{*}{Metric} & \multirow{2}{*}{$N$} &\multicolumn{6}{c|}{\textbf{Fine-tuning}}  & \multicolumn{6}{c|}{\textbf{Training-from-scratch}}\\
 \cline{3-14}
 & &Craft &GAN &GM &Train &SoftTrain &Naive&Craft &GAN &GM &Train &SoftTrain&Naive\\
 \hline
 \multirow{3}{*}{\makecell{Accuracy\\(\%)}} & 2 &91.64&91.86&92.02&92.05&91.99&49.64&85.99 &85.99 &53.06 &90.58 &90.31&72.63\\
 & 5 &83.46&84.93&80.28&90.82&90.48&49.64&35.58 &40.03 &12.13 &89.86 &87.02&72.63\\
 & 8&35.48&18.82&12.45&70.28&71.32&49.64&15.34 &17.49 &10.88 &78.64 &56.78&72.63\\
 \hline
 \multirow{3}{*}{\makecell{Fidelity\\(\%)}} & 2&98.23&98.42&99.87&99.29&99.10&50.62&92.37 &89.59 &54.63 &99.34 &98.87&72.62\\
 & 5&86.32&87.49&84.33&94.84&94.67&50.62&41.32 &38.72 &11.87 &95.40 &89.83&72.62\\
 & 8&36.11&18.62&12.63&71.79&72.45&50.62&15.63 &17.44 &10.67 &80.01 &57.78&72.62\\
 \hline
\end{tabular}
}
\end{small}
\end{center}
\end{table*}

\textbf{Gradient-based soft label training model extraction (Soft-train-ME).}  If gradient query is allowed and a subset of training data is available, the attacker can achieve better ME attack performance compared to Train-ME. 
To utilize gradients, a naive idea is to combine the GM loss with cross-entropy loss in Train-ME. However, our initial investigation shows they are not compatible; the cross-entropy loss term usually dominates and the GM loss even hurts the performance.
An alternative approach is to use soft label. We build upon the method in \cite{gu2020introspective} which shows that gradient information of incorrect labels is beneficial in knowledge distillation, and use it for surrogate model training. Specifically, for each input $\vx_i$, gradients of the ground truth label as well as incorrect labels are collected $N_C$ times, where $N_C$ is the number of classes.
For an input $\vx_i$ with true label $c$, its soft label $q_i^k$ of $k$-th ($k \neq c$) label is computed as follows:\\
\begin{equation}
    q_i^k = (1-\alpha) * \frac{cos(\ve^k, \ve^c)}{ \sum_{m=1, m \neq c}^{N_C} (cos(\ve^m, \ve^c) + 1)}
\end{equation}

where, $\ve^k$ denotes flattened gradients of label $k$,  $q_i^k$ denotes soft label for the k-th label $k$ and $\alpha$ is a constant ($\alpha > 0.5$). 
% $q_i^c$ denotes soft label for the ground-truth label $c$ is the constant $\alpha$ ($\alpha > 0.5$).
The derived ($\vx_i, \vq_i$) pair is then used in the surrogate model training in addition to the true label $c$ (which is the only difference from the Train-ME).
% \tofill{mention GM + Train-ME does not work}
% So far, we present five model extraction methods, including two data-free extraction attacks (Craft-ME, GAN-ME) and three data-available extraction attacks (Grad-match-ME, Train-ME and Soft-train-ME).
% \section{Experiments}


% \textbf{Metric Definition.} We first needs to establish a metric to evaluate IP protection given the existence of model extractions. Since percentage ($P$) of training data available at the attacker and the number of layers ($N$) in server-side model play major roles, we use accuracy degradation conditioned on $P$ and $N$. For example, GAN-ME having a $Deg_{(P=0, N=3)}$ = 1\% means only 1\% accuracy degradation can be achieved using GAN-ME, if the attacker has 0\% training data (data-free) attacks a SFL scheme where server-side model has 3 layers. In $Deg_{(P=0.2, N=3)}$ = 0.1\% with Train-ME, $P=0.2$ denotes 20\% training data is available for the attacker.

% \textbf{Post-analysis of the IP protection.} To evaluate IP protection strength of a particular SFL setting with $N$ layers in server-side model, with a full sweep of $P$, presenting a diagram of accuracy degradation using the strongest attack is most informative. Each $P$ to the worst-case analysis that an strongest extraction attack may achieve.
% This analysis can be done for a particular setting, however, we will show in most cases, it can be predicted, which directly gives direction on how to choose a proper $N$ to achieve the desired level of protection strength.





% \subsection{Experiment Setup}
\section{Model Extraction Performance} 
% five attacks with a focus on CIFAR-10 using VGG-11 network with batch normalization layers. Moreover, on CIFAR-10 dataset, we evaluate model extraction attacks on other different architectures such as CifarResNet-20, ResNet-18 and Mobilenetv2. We extend VGG-11 network for other datasets include MNIST, FMNIST, SVHN and CIFAR-100. We also use a pre-trained Mobilenetv2 model to evaluate Train-ME on Imagenet dataset. 
% We present complete results for VGG-11 on CIFAR-10 dataset and leave extensive empirical evidence in the appendix. 

% The 50K training data is evenly distributed to all participating clients, with the exception on the attacker (only one) without training data as an extra client. 
% We investigate SFL scheme with different $N$ settings. 


In this section, we demonstrate the performance of the proposed ME attacks and the baseline attack on SFL schemes.  All experiments are conducted on a single RTX-3090 GPU.
We use VGG-11 which has 11 layers as the model architecture. We vary $N$~(the the number of layers in server-side model) from 2 to 8 to generate different SFL schemes and evaluate them on CIFAR-10.

For the SFL model training, we set the total number of epochs to 200, and use SGD optimizer with a learning rate of 0.05 and learning rate decay (multiply by factor of 0.2 at epochs 60, 120 and 160). We assume all clients participate in every epoch with an equal number of training steps. We set the number of clients to 10 which corresponds to the cross-silo case. To perform ME attacks, the attacker uses an SGD optimizer with a learning rate of 0.02 to train the surrogate model and we report the best accuracy and fidelity. We evaluate accuracy of the surrogate model on the validation dataset.
We use the label agreement as fidelity, defined as the percentage of samples that the surrogate and victim models agree with over the entire validation dataset, as in \cite{jagielski2020high}. We include details of the SFL setting detail in~\cref{apx:setting}.
% For stable gradient query case, we set the number of gradient query at 1K and 10K. For unstable case, we don't set the budget for gradient query, as gradient query access automatically stops when training finishes.Specific 




\subsection{ME Attack on SFL with Fine-tuning-based Training} 
\label{sec:fine-tune}

We first perform the proposed ME attacks on fine-tuning SFL version with consistent gradient query. Here we use a pre-trained model and set the number of gradient queries to 100K. On a victim VGG-11 model on CIFAR-10 dataset, whose original accuracy is 91.89\%, performance of all five ME attacks are shown in Table~\ref{tab:stable_brief}. For each of the ME attacks, we vary hyper-parameters and report the one that achieves the best attack performance. When $N=2$, all five ME attacks are successful and can achieve near-optimal accuracy and fidelity performance.
However, when $N$ is large, proposed ME attacks have worse attack performance.
For Craft, GAN and GM ME, the accuracy drops to around 80\% when $N$ is 5, and sharply drops to below 40\% when $N$ is 8. 
And for Train and SoftTrain ME, accuracy slightly degrades when $N$ is 5, and reduces to around 70\% when $N$ is 8. 
These results show that ME attack performance strongly correlates with $N$.  ME attack performance reduces for larger $N$ as the extraction problem becomes harder with more unknown parameters and more complicated input feature space. 


\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{lower_query_budget.pdf}
\caption{ME attacks without training data under limited query budget (1K and 10K).}
\label{fig:query_budget}
\end{figure}

\textbf{Limited Query Budget.} Gradient queries are very important for our proposed attacks. We verify this by limit the qeury budget (original is 100K in Table~\ref{tab:stable_brief}). For Craft, GAN and GM MEs, we lower down the number of gradient queries to 1K and 10K. Results are shown in \cref{fig:query_budget}. Performance of all three ME attacks reduces significantly for a large N. But most of them (except GAN-ME) still achieves success when N is less than 4.


\textbf{Summary.} We summarize our findings based on results in Table~\ref{tab:stable_brief} and extensive evaluation in~\cref{apx:ideal_case}. 
In the fine-tuning case, we see that Craft, GAN and GM ME attacks are successful without training data. We observe the following interesting characteristics:
\textbf{Craft-ME} has a steady attack performance and can succeed even with a tight gradient query budget. 
\textbf{GAN-ME} needs a large query budget to train the c-GAN generator towards convergence but can achieve better accuracy and fidelity than Craft-ME for $N\leq5$. 
\textbf{GM-ME} requires an auxiliary dataset that is similar to the training data and when CIFAR-100 is used to attack CIFAR-10 model, GM-ME achieves almost perfect extraction for small $N$. However, it has slightly worse performance if MNIST or SVHN are used as auxiliary datasets. For large $N$, the surrogate model fails to converge on the GM loss, and its extraction performance suffers from a sharp drop. 
For attacks with training data such as \textbf{Train-ME} and \textbf{ SoftTrain-ME}, both accuracy and fidelity are much higher than attacks without training data. When $N\geq6$, SoftTrain-ME can achieve slightly better accuracy and fidelity than Train-ME.

\subsection{ME Attack on Training-from-scratch SFL}
\label{sec:practical}
Next, we investigate the proposed ME attack performance in training-from-scratch SFL case.
A good attack-time-window for gradient-based ME attacks is at the end of training when gradients do not vary as much and the model converges.
%to almost its final accuracy. 
So for Craft, GAN and GM-ME, we launch the attack at epoch 160 to get more consistent gradients. As the model is updated by multiple clients, the percentage of malicious clients also affects the ME attack performance. We found that with more malicious clients, the server-side model returns more consistent gradients to the attacker.
% In a multi-client SFL where 
Attack performance for three attacks are shown in Table~\ref{tab:stable_brief} for 10-client SFL training-from-scratch case.

\textbf{Summary.} We summarize our findings based on results in Table~\ref{tab:stable_brief} and extensive evaluations in~\cref{apx:practical_case}. 
In the training-from-scratch case, 
for gradient-based attacks without training data (Craft, GAN and GM MEs), we notice significant attack performance drop compared to the consistent query case. However they still can succeed in attacking a small server-side model ($N=5$) with around 86\% accuracy.
The same trend is shown in SoftTrain-ME. However SoftTrain's performance drops and that makes Train-ME the most effective attack for an attacker with training data.
We find that \textit{poison effect} and \textit{inconsistent gradients} contribute to the sharp drop in ME attack performance. Since gradient-based ME attacks require the attacker (as a participant) send noisy inputs (Craft, GAN and GM MEs) or genuine inputs with incorrect labels (SoftTrain-ME), the model accuracy suffers from a 2-3\% degradation, resulting in a less accurate target model. The inconsistent gradients reduces the effect of gradient query since model parameter can change rapidly. For instance, in Craft-ME, crafted inputs that have a small loss at an earlier epoch of the training can have a large loss in the final model because of the update of model parameters. Hence, inconsistent gradient information results in poor accuracy in the surrogate model.
Interestingly, compared to Craft-ME, GAN-ME is more robust to inconsistent gradients as the generator can adjust itself to the change of server-side model, resulting in a better attack performance when $N=5$. However, when $N$ is larger, the generator does not converge well and its performance drops drastically.
Last but not the least, GM-ME completely fails with inconsistent gradients, even for small $N$. This implies that the GM loss is super sensitive to inconsistent gradients and is only effective in consistent query cases.

% \subsection{Summary of observations}


% % \textbf{Hyperparameter Tuning: Fine-tuning case.}
% We summarize our findings in ME attacks on SFL, based on results in Table~\ref{tab:stable_brief} and extensive results on different hyper-parameters for the fine-tuning case and the training-from-scratch case in~\cref{apx:ideal_case} and ~\cref{apx:practical_case}, respectively. 


% We observe different characteristics for each attack based on detailed results in Appendix.~\ref{apx:ideal_case}. Craft-ME keeps a steady attacking performance and can succeed even with a very tight query budget. GAN-ME needs a large query budget to train the c-GAN generator towards convergence but it can achieve better for small and medium $N$ than Craft-ME. GM-ME requires an auxiliary dataset that is similar to the victim domain. If CIFAR-100 is used, it achieves almost perfect extraction for small $N$. However, its performance degrades for MNIST or SVHN as auxiliaries. And for a high $N$, the surrogate model fails to converge on the gradient matching loss, and its extraction performance suffers from a sharp drop.

% \begin{table*}[htbp]
% \caption{ME attack performance with \textbf{consistent gradient query} on VGG-11 model on CIFAR-10. Original Accuracy is 91.9\%. Query Budget is 100K.}
% \label{tab:stable_brief}
% \begin{center}
% \begin{small}
% \resizebox{0.7\linewidth}{!}{
% \begin{tabular}{lcccccc} 
%  \toprule
%  \multirow{2}{*}{Method} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}} & \multicolumn{3}{c}{\textbf{Fidelity (\%)}}\\
%   &N=2 &N=5 &N=8 &N=2 &N=5 &N=8\\
%  \midrule
%   Craft-ME &91.64	&83.46	&35.48	&98.23	&86.32	&36.11\\
%   GAN-ME   &91.86	&84.93	&18.82	&98.42	&87.49	&18.62\\
%   GM-ME    &92.02	&80.28	&12.45	&99.87	&84.33	&12.63\\
%   Train-ME    &92.05	&90.82	&70.28	&99.29	&94.84	&71.79\\
%   SoftTrain-ME    &91.99	&90.48	&71.32	&99.10	&94.67	&72.45\\
%  \bottomrule
% \end{tabular}
% }
% \end{small}
% \end{center}
% \end{table*}



\begin{figure}[htbp]
\centering
\begin{subfigure}{.49\columnwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{four_figures1.pdf}
  \label{fig:four_sub1}
\caption{}
\end{subfigure}%
\begin{subfigure}{.49\columnwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{four_figures2.pdf}
  \label{fig:four_sub2}
\caption{}
\end{subfigure}
\caption{(a) ME attack performance of VGG-11 on other datasets. (b) ME attack performance of other architectures on CIFAR-10 dataset.}
\label{fig:four_figures}
\end{figure}




\subsection{ME Attack on SFL on other datasets and architectures} 
\label{sec:others}

% \jtnote{Through extensive empirical studies on other architectures and datasets. We find carefully selected application domain (datasets) and designed architecture can mitigate IP threats and provide better IP protection.}
% We use Train-ME attack, the best performer for both fine-tuning and training-from-scratch cases. 

First, we perform Train-ME attack with 1K training data on different datasets including MNIST~\cite{lecun1998mnist}, FMNSIT~\cite{xiao2017fashion}, SVHN~\cite{netzer2011reading} and CIFAR-100 datasets~\cite{krizhevsky2009learning}, using a VGG-11 model with $N$ set to 5. As shown in~\cref{fig:four_figures}~(a), for all datasets except CIFAR-100, ME attack achieves accuracy very close to the original. But for CIFAR-100, the extracted accuracy is low that is $>$20\% below the original.
Additionally, we also test Train-ME performance with 2\% and 20\% ImageNet training data on Mobilenet-V2.
%on ImageNet dataset. 
As shown in~\cref{fig:tradeoff}~(a), ME attacks are hard to succeed due to the complexity of ImageNet dataset~\cite{deng2009imagenet}, resulting in a high accuracy gap of 10\% when $N$ is set to 2.
Second, we test Train-ME attack on different architectures including Resnet-20, Resnet-32~\cite{he2016deep} and Mobilenet-V2~\cite{sandler2018mobilenetv2} on CIFAR-10 dataset (with necessary adaptations)
For Resnet and Mobilenet family, we assign last 4 layer-blocks and 1 FC layer to server-side model. As shown in~\cref{fig:four_figures}~(b), with the same proportion of layers (5 out of 11) being assigned to server-side model, ME attack is much less effective on Resnet-20 than on VGG-11. A comparison of the performance of Resnet-32 and Mobilenetv2 with similar proportion of layers being assigned to server-side (5 out of 17 and 20, respectively), ME on Resnet-32 is also much worse than on MobilenetV2.

\textbf{Summary.} We find complex datasets such as CIFAR-100, ImageNet tend to be more resistant to ME attacks. Also, some architectures such as Resnet-20 and Mobilenet-V2 are more resistant than VGG-11.
% \jtnote{\textbf{This suggests ME attacks are harder to succeed on a complex domain.}}
% On other datasets and architectures, we find slightly different story, that we found complex 
% that some architecture and datasets are more resistant to ME attacks.
% \jtnote{\textbf{This suggests that ME attacks are harder to succeed in come architectures.}}

% \begin{table*}[htbp]
% \caption{Model extraction performance of gradient-based attack with stable (100K query budget) and unstable gradient query (10-client SFL), on VGG-11 model (CIFAR-10). Original Accuracy is 91.9\%.}
% \label{tab:gradient_brief}
% \begin{center}
% \begin{small}
% \resizebox{0.75\linewidth}{!}{
% \begin{tabular}{clcccccc} 
%  \toprule
%  \multirow{2}{*}{Case} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}} & \multicolumn{3}{c}{\textbf{Fidelity (\%)}}\\
%   & &N=2 &N=5 &N=8 &N=2 &N=5 &N=8\\
%  \midrule
%   \multirow{3}{*}{\makecell{Stable Query \\ (fine-tuning)}} &Craft-ME &91.64	&83.46	&35.48	&98.23	&86.32	&36.11\\
%   &GAN-ME   &91.86	&84.93	&18.82	&98.42	&87.49	&18.62\\
%   &GM-ME    &92.02	&80.28	&12.45	&99.87	&84.33	&12.63\\
 
%  \midrule
%   \multirow{3}{*}{\makecell{Unstable Query \\ (from-scratch)}} &Craft-ME &85.99	&35.58	&15.34	&92.37	&41.32	&15.63\\
%   &GAN-ME   &85.99	&40.03	&17.49	&89.59	&38.72	&17.44\\
%   &GM-ME    &53.06	&12.13	&10.88	&54.63	&11.87	&10.67\\
%  \bottomrule
% \end{tabular}
% }
% \end{small}
% \end{center}
% \end{table*}

% For a small $N$, GM-method performs typically well, and even surpasses performance of Train and SoftTrain attacker.


% Known architecture [Different data-free Attacks] - able to sweep under idea query evaluation.

% Known architecture [Different data-available Attacks] - able to sweep under idea query evaluation.

% === repeat for different uknown architecture ===

% [On the discussion of surrogate architecture]

% [On the discussion of query budget, data assumption of different Attacks] 

% , and performance gets worse and worse when more inconsistent gradients are used in matching. Thus, we restrict in using only a few gradients at near the end of the training, however, the small number of gradient query is sufficient enough to achieve a good extraction.


% \begin{table*}[htbp]
% \caption{ME attack performance with \textbf{inconsistent gradient query} on a 10-client SFL, VGG-11 model on CIFAR-10.}
% \label{tab:unstable_brief}
% \begin{center}
% \begin{small}
% \resizebox{0.7\linewidth}{!}{
% \begin{tabular}{lcccccc} 
%  \toprule
%  \multirow{2}{*}{Method} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}} & \multicolumn{3}{c}{\textbf{Fidelity (\%)}}\\
%   &N=2 &N=5 &N=8 &N=2 &N=5 &N=8\\
%  \midrule
%   Craft-ME &85.99	&35.58	&15.34	&92.37	&41.32	&15.63\\
%   GAN-ME   &85.99	&40.03	&17.49	&89.59	&38.72	&17.44\\
%   GM-ME    &53.06	&12.13	&10.88	&54.63	&11.87	&10.67\\
%   Train-ME    &90.58	&89.86	&78.64	&99.34	&95.40	&80.01\\
%   SoftTrain-ME    &90.31	&87.02	&56.78	&98.87	&89.83	&57.78\\
%  \bottomrule
% \end{tabular}
% }
% \end{small}
% \end{center}
% \end{table*}


% Overall, the impact on accuracy extraction small to see its impact on extraction attack performance.

% Previous results
% \subsection{}
% \textbf{Stable/Unstable cases for attacks with training data.}
% We find attacks without training data achieve limited ME attack performance, which means using SFL can effectively protect the model from complete free-riders (participants without valid training data). 
% However, the strongest data assumption is valid in SFL that an attacker can have a subset of training data and launch Train-ME or SoftTrain-ME. As shown in Table.~\ref{tab:train_brief}, the attacker can achieve much better extraction performance, even with merely 2\% of training data. For stable query case, using SoftTrain-ME achieves slightly better accuracy and fidelity for larger $N$, while the advantages quickly diminish for the unstable case because of the inconsistent gradients.
% % , leaving the Train-ME the strongest ME attacks. 

% \begin{table*}[htbp]
% \caption{Model extraction performance of attacker with training data (2\%), using Train-ME and SoftTrain-mE with stable (100K query budget) and unstable gradient query (10-client SFL), on VGG-11 model (CIFAR-10). Original Accuracy is 91.89\%.}
% \label{tab:train_brief}
% \begin{center}
% \begin{small}
% \resizebox{0.75\linewidth}{!}{
% \begin{tabular}{lcccccc} 
%  \toprule
%   \multirow{2}{*}{Method} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}} & \multicolumn{3}{c}{\textbf{Fidelity (\%)}}\\
%   &N=2 &N=5 &N=8 &N=2 &N=5 &N=8\\
%  \midrule
%   Train-ME &92.05	&90.82	&70.28	&99.29	&94.84	&71.79\\
%   SoftTrain-ME (stable)   &91.99	&90.48	&71.32	&99.10	&94.67	&72.45\\
%   SoftTrain-ME (unstable) & 90.31	& 87.02	& 56.78	& 98.87	& 89.83	& 57.78\\
%  \bottomrule
% \end{tabular}
% }
% \end{small}
% \end{center}
% \end{table*}


% We this is because the second derivative tends to be less stable
% gan-ME methods behave more robust to the gradient inconsistent


% \textbf{Utilizing gradients comes at a price. Under a 10-client SFL, training time is long, so we don't do sweep, just use the best from section A.}

% 

% 1. Perform each attack during the training, on different datasets, vgg-11-bn model.

% 2. Perform each attack during the training, on different architecture, cifar-10 dataset.

% === focus on finding here, what's interesting? ===

% Show the strongest attack being the attacker with training data.

% \tofill{Extra things on attack:}

% \tofill{1. Attacker with non-iid. (only with data from a few classes) training data, showing with less classes, attack performance worse -- Now in ablation}

% \tofill{2. Attacker without knowledge on the architecture information (with training data), showing architecture does not affect much -- Now in ablation}

% % \tofill{3. Attacks on }

% \tofill{4. Attacks to acheive adversarial attack goal, showing a more successful ME attack boosts adversarial attack performance by a lot -- Now in appendix.}







% \section{Towards Better Protection} 

\section{Discussion}
\label{sec:discussion}




\subsection{Tradeoff between IP and Data protection}
Our evaluation showed that ME attack performance drops with increasing $N$~(the number of layers in server-side model). Thus, a simple idea to improve resistance to ME attack is to use a larger $N$. However this implies that the number of layers in client-side model would be smaller,
thereby undermining clients' data. 
The tradeoff between IP protection and data protection is shown in~\cref{fig:tradeoff}~(b). We use Mean Square Error (MSE) of  reconstructed images by MI attack as a metric to represent the degree of data protection in SFL, as in \cite{li2022ressfl}; its implementation detail is included in ~\cref{apx:MI_attack}. 
From ~\cref{fig:tradeoff}~(b), we can see that with larger $N$, the extracted accuracy decreases  but MSE decreases meaning MI attack is more successful and clients' data protection is compromised. 
Thus, IP threat mitigation cannot be simply done by increasing $N$.

\begin{figure}[htbp]
  \centering
  
  % \hfill
\begin{subfigure}{.46\columnwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{four_figures3.pdf}
  \label{fig:four_sub3}
\caption{}
\end{subfigure}
  \begin{subfigure}{.53\columnwidth}
  \centering
  \includegraphics[width=1.0\linewidth]{four_figures4.pdf}
    \label{fig:four_sub4}
    \caption{}
    \end{subfigure}
\caption{(a) ME attack performance of MobilenetV2 on ImageNet. (b) Tradeoff between ME resistance and degree of data protection (MSE).}
\label{fig:tradeoff}
\end{figure}

%Note that choosing the maximum allowed $N$ (i.e. equal to 11 in VGG-11) makes the scheme equivalent to centralized training where client does not preserve data privacy at all.
% is considered for SFL, and can be used towards a balanced resistances.


\subsection{Potential Defenses}

% \tofill{Only leave the L1-regularization.}
Next we demonstrate how simple regularization can be used as defensive methods against ME attacks on SFL.
The key idea is to 
restrict the useful information in the client-side model that is leaked to the attack.
This is done by applying regularization techniques to restrict the client-side model's feature extraction capabilities. Specifically, on the client-side model, we apply L1 regularization with three different strength ($\lambda$ = 5e-5, 1e-4 and 2e-4) to penalize its weight magnitude.
As shown in~\cref{fig:regularization} for Train-ME with 1K data,
this simple defense effectively improves the resistance to ME attack though there is some accuracy degradation on the original model. For example, accuracy degrades to 90.43\% when $\lambda$ = 5e-5. Thus regularization can be used to defend ME attacks. More details are provided in~\cref{apx:impact_defenses}.



\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]{four_figures5.pdf}
\caption{L1 regularization as effective defense for ME attacks.}
\label{fig:regularization}
\end{figure}

% \textbf{Proxy-model methods. \tofill{This might be unnessary.}}
% Before SFL training starts, the model owner may not have IND training data (or not enough data) to perform the trade-off analysis. Fortunately, the extraction attack performance of a given architecture transfer well between different datasets. As shown in Fig.~\ref{fig:three_figures} (b), we observe different datasets shows a similar pattern in \textit{error rate}, which also holds for other architectures such as ResNet20 and MobileNetv2 (See Appendix~\ref{apx:other_empirical}.5). So that the model owner can use OOD data to evaluate the IP protection (extraction attack performance) of a particular architecture and $N$ setting. 
\subsection{Ablation Study}

\textbf{ME attack with non-IID data.} We consider the non-IID (independent and identically distributed) case where the attacker only has data from $C$ classes of CIFAR-10. Results presented in~\cref{apx:noniid} show that ME attack performance is still good for $C=5$ but degrades sharply when $C=2$.



\textbf{Adversarial attack based on successful ME attack.} The goal of ME attack is to launch more successful adversarial attacks. We perform transfer adversarial attacks using a surrogate model extracted by the strongest Train-ME attack. As shown in~\cref{apx:adversarial}, SFL with proper $N$ achieves better resistance to adversarial attacks.


\textbf{ME attack without architecture information.} In~\cref{apx:diff_arch}, we investigate simple variants (longer, shorter, wider, and thinner) of the original architecture as the surrogate model architecture. We find that the performance of ME attacks is similar for the different architectures -- the exception is GM-ME which fails for different surrogate architectures.  


\section{Conclusion}
In this work, we show that SFL cannot guarantee model IP protection and is vulnerable to ME attacks.
%study the ME attacks on SFL and answer the question whether SFL is sufficient for model IP protection. 
We propose five novel ME attack methods and achieve attack success under an unique threat model where gradient query is allowed but prediction query is not allowed.  
By studying the effect of the model split sizes on ME attack performance, we find that using a large  number of layers in server-side model can better protect IP, but compromises clients' data and is hence impractical. 
%Given the tradeoff, we further answer the question how to make 
Finally, as a first step towards making SFL resistant to ME attack, we show use of regularization as a potential defense mechanism.
%can apply  regularization as defenses.}
% additionally we propose 
% We show that the SFL model is extremely vulnerable to ME attacks if the server-side model is shallow.
% For the case when there are enough number of layers in server-side model, model IP can be protected well and transfer adversarial attack is not successful. However, data privacy could be compromised and so this factor needs to be considered in the development of such schemes.
% We also point out a possible way of defending such attacks through regularization and plan to expand on it in the near future.

% {\bf Broader Impact.} This work points out the model IP vulnerability of SFL
% since that it suffers from ME attacks. Thus one  should be careful in using SFL as a model IP protection method. We believe that the attacks presented here would initiate research in the development of defense schemes to mitigate MEA attacks, and inspire future FL schemes that can protect users' data as well as model IP.

























%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{IEEEtran}
}
\input{appendix}
\end{document}
