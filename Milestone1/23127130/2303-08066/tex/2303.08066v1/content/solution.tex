\section{Implementation}
% Based on above formulation and motivations, we propose our bandwidth control policy, named \Pascal{}. The inspiration and ideal of \Pascal{} is described comprehensively in Section \ref{sec: methodologies}. Besides, the implementation scheme and component modules of \Pascal{} is illustrated in Section \ref{sec: design}.






% By this means, \Pascal{} finally achieves a cooperative control and gives a state-of-the-art performance in bandwidth control test.

% The core idea behind it is straightforward. To some extent, the multi-level hierarchical storage system resembles a hydraulic system that consists of multiple various containers, where the IO requests of workload into the storage system is like the outside force applied to the hydraulic system; the water level of each tier could be considered as the pressure applied to corresponding container; the data migration between various tiers is like the pressure transmission between different containers. The storage system can maintain a relatively stable state if the workload is constant. Once the workload varies, the system could still maintain a stable and high performance via adjusting the data migration (i.e., bandwidth control). 


% \Pascal{} mainly consists of four modules, namely pressure mapper, migration planner, resource predictor and bandwidth allocator. The relation and information flow between above modules are shown in Figure~\ref{fig:pascal}. Inside the framework, we follow the formulation presented in Section~\ref{sec: problem_definition}. For each node $i \in V$, besides the previous-define indicators, we maintain a pressure value. The pressure value denotes how hard data migrates into the node.
% The pressure mapper is responsible for calculating the pressure value of each node given the real-time water level. Then based on the connecting relation between nodes and respective pressure values, the ratio of data to be migrated along edges is derived from the planner module. In order to achieve above data migration, the system is required to allocate bandwidth to various edges. Since the resource efficiency in the system is time-varying, the resource efficiency predictor is used to predicts the resource consumption by per bandwidth in an on-the-fly way. With above information and real-time system resource condition, the bandwidth allocator derives the value of each controlling bandwidth.

% In the following, each module is described at length.


% This state resembles an ideal system comprised by connected closed containers with one entrance and several outlets (see Figure~\ref{fig: problem_description}). Hypothetically, it is assumed that the pressure in the containers only results from the gas squeezed by the liquid and only liquid can flow between containers.  If the pressure of flow into the system is constant, the flows among containers and volume of liquid in each containers are steady, namely, it is in a stable equilibrium. Besides, liquid in containers would never exceed the capacity due to the characteristic of gas (can not be squeezed to zero volume). 

% where steady flows and hydraulic system that consists of multiple various containers, where 
% The IO requests of workload into the storage system is like the flow into to the corresponding physical system. The space of stored data of each tier could be considered as the space  taken by fluid, so that the air in the containers is squeezed. If the pressure of flow into the physical system is constant, this system is in a stable equilibrium.
% In this , 






% pressure applied to corresponding container; the data migration between various tiers is like the pressure transmission between different containers. 
% The storage system can maintain a relatively stable state if the workload is constant. Once the workload varies, the system could still maintain a stable and high performance via adjusting the data migration (i.e., bandwidth control).

\subsection{Overview}

% In \Pascal{}, we follow the formulation presented in Section~\ref{sec: problem_definition}.
\Pascal{} mainly consists of two components, namely \textit{pressure balancer} and \textit{bandwidth allocator}, as shown in Figure~\ref{fig: pascal}. 
% Note that in \Pascal{}, we follow the formulation presented in Section~\ref{sec: formulation}. 
In the pressure balancer, for each node in DAG, we maintain a pressure value according to its real-time watermark level. Then based on the connecting relation between nodes and respective pressure values, the ratio of data to be migrated along edges is derived. As for the bandwidth allocator, it is responsible for calculating the bandwidth controlling results according to data migration ratio derived from the pressure balancer. Since the resource efficiency in HSS is time-varying, the bandwidth allocator is also supposed to consider the real-time resource consumption by one unit of bandwidth. In the following, each module is described at length.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{figure/dac_pascal.pdf}
	\caption{Overview of \Pascal{} bandwidth control policy. It consists of pressure balancer and bandwidth allocator.}
	\label{fig: pascal}
\end{figure}
\vspace{-2ex}

\subsection{Pressure balancer}
\textbf{Pressure mapping.} For each node $i$ in DAG (i.e., storing tier or part of storing tier within HSS), its pressure value $p_i$ denotes how hard data migrates into the node. For example, the larger the pressure is, the harder data moves into the node and vice versa.
Generally, the pressure $p_i$ is mapped from watermark level $w_i$ via a mapping function $\mathcal{F}: w_i \mapsto p_i$. The expression of mapping function is not restricted here. But it should be with the following properties.

\begin{property}[Origin property] Given an ideal pressure value $p_i^*$ corresponding to the ideal watermark level $w_i^*$, the mapping function is supposed to passes through the point of $(w_i^*, p_i^*)$, as shown below:
\begin{equation}
    \mathcal{F}(w_i^*) = p_i^*
    \label{eq: origin property}
\end{equation}
\end{property}

\begin{property}[Non-decreasing property] The pressure value is supposed to (at least) increase when the watermark level rises, as shown below:
\begin{equation}
    \mathcal{F}(w_{i}) \geq \mathcal{F}(w_{i}^{'}) \quad \forall w_i>w_{i}^{'} 
\end{equation}
\end{property}

\begin{property}[Infinity property] The pressure value approaches infinity when the watermark level approaches one, i.e., 
\begin{equation}
     \lim\limits_{w_i \to 1} \mathcal{F}(w_i) \to \infty
\end{equation}
\end{property}
\textsc{Property 1} suggests that the system is able to arrive at the equilibrium, where the ideal watermark level $w_i^*$ could be determined by human experts or tuned by machine learning techniques and the ideal pressure $p_i^*$ is obtained by Eq.(\ref{eq: equilibrium}) and Eq.(\ref{eq: raito_migration}).
\textsc{Property 2} ensures that there is only one equilibrium, so that the system must tend to the predefined equilibrium. And \textsc{Property 3} constrains the occupied capacity of a node to be less than its maximum capacity, which guarantees the system security to some extent. 

% Noted that \textsc{Property 1} relies on a predefined point $(w_i^*, p_i^*)$, the method about how to get $w_i^*$ and $p_i^*$ is described later (See Sec. \ref{sec: learn to cooperate} 
% and Sec.\ref{sec: global policy}).

\noindent\textbf{Data migration planning.}
Though pressure is capable of guiding the bandwidth related to the corresponding node, a precise method to get the specific value for each bandwidth control task is demanded. Considering that a task is normally associated with several nodes, the rule is made that bandwidth of a task is proportional to its pressure difference. Specifically, the pressure difference of a single edge is calculated by subtract the pressure value of both nodes connected by it. And for a task consisting of several edges, its pressure difference is the weighted average of its edges' pressure difference. Generally, the ratio of  data to be migrated by each task can be calculated via:
\begin{equation}
    \mathbf{D}_{norm} = \frac{\mathbf{p}\times(\mathbf{M} \times \mathbf{E})^T}{|\mathbf{p}\times(\mathbf{M} \times \mathbf{E})^T|_1}
    \label{eq: raito_migration}
\end{equation}

\noindent\textbf{Learning to cooperation.} Above process can calculate the data migration ratio among nodes according to real-time watermark level of each node. However it might  not be the best data migration ratio. Because there is no cooperation between different data migration tasks. For example, when the watermarks level of Flash tier is increasing, the most appropriate response is to remain bandwidth of throughput and increase bandwidth of flush, meanwhile decrease the bandwidths of other tasks. This is because throughput is the most significant task in terms of the performance. Therefore, the change of its bandwidth should be treated prudently.

On the other hand, the above process is fully parameterized. Hence it can achieve cooperation between different bandwidth control tasks via machine learning techniques. More specifically, the ideal watermark level $w^*_i$, which is a hyperparameter, has huge impact on the performance of \Pascal{}. It is hard for human expert to determine the best $w_i^*$. And there are many such hyperparameters in HSS (i.e., one node is associated with one ideal watermark level). Considering above, we resort to Bayesian Optimization~\cite{0BOA} to achieve the learning to cooperation. The experimental results illustrate that the performance of \Pascal{} can be enhanced greatly using Bayesian optimization (see Section~\ref{sec: ablation study}).
\vspace{-2ex}
% However, this base policy are still unsatisfactory to improve the performance of a HSS since there is no coordinating mechanism among tasks. Following the policy, in spite of that the bandwidth of a task is associated with all the nodes affected directly by it, it is irrelevant about the other nodes in the system. The lack of cooperation among tasks finally results in inefficiency of the system. For example, when the watermarks level in cache is increasing, the most appropriate response is to 
% remain throughout bandwidth of cache and increase bandwidth of flush, meanwhile decrease bandwidths of other tasks. This is because throughput is the most significant task in terms of the performance, therefore, the change of its bandwidth should be treated prudently. In this situation, all the tasks must be adjusted coordinately, otherwise, performance of the HSS will deteriorate. Hence, machine learning method is applied for the base policy, which is actually a parameter-based strategy. Namely, a set of parameters including $w_i^*$ determines the corresponding action of the policy in different situations. As a consequence, the tasks can cooperate together once a proper set of parameters is assigned for each task. For that purpose, various machine learning methods are applicable. Taking the time assumption of tests in a HSS and the dimension of parameters into consideration, Bayesian optimization
% is finally adopted in our experiment. The results illustrate that the performance of the HSS is enhanced significantly with Bayesian optimization(see Sec.\ref{sec: ablation study}).

\subsection{Bandwidth allocator}
\noindent\textbf{Resource efficiency prediction.}
% As the proportion of each task's bandwidth is calculated by above policy, the specific amount can be calculated once the sum of bandwidths is known.
The run-time environment of the HSS is extremely complex. Various background tasks existing in the system, including bandwidth control, compression, scheduling, deduplication, etc. These tasks might affect each other.
For example, these tasks compete for kinds of data access, system resources throughout the run time. 
It is intractable to fully analyze the mutual influences among them. However it is impractical to assume that the resource efficiency (i.e., $r_h^k$ the resource consumption by one unit of bandwidth) is constant throughout the control process. Instead, we resort to model the resource efficiency and to predict it when allocating the bandwidth. Specifically, for each $r_h^k$ (defined in Section~\ref{sec: formulation}) at time interval $t$, it can be predicted via:
\begin{equation}
    \Tilde{r}_h^{k} = G(\{r_h^{k,t'}|t'=t-1, t-2,...,t-m\})
    \label{eq: time_series_prediction}
\end{equation}
where $\{r_h^{k,t'}|t'=t-1, t-2,...,t-m\}$ is a time series of ground truth value of historical $r_h^k$; $G$ is the predicting function which can either be classical time series forecasting methods such as ARIMA~\cite{2021Visibility} or be supervised learning models such as LSTM~\cite{2014Long, 2016Learning}. In this way, a predicted resource efficiency matrix $\mathbf{r}$ can be constructed:
\begin{equation}
    \mathbf{r}=(\Tilde{r}_h^{k}), \quad h= 1,...,H,  k=1,...,K
\end{equation}

With the data migration ratio and the predicted resource efficiency available, the resource consumption vector $\mathbf{r_S} \in \mathbb{R}^{K}$ of the whole system when migrating one unit of data can be obtained:
\begin{equation}
    \mathbf{r_S} = \mathbf{r} \times \mathbf{D}_{norm}
    \label{eq: r_S}
\end{equation}

\noindent\textbf{Bandwidth allocation.}
Taking as input the data migration ratio and resource efficiency prediction, the bandwidth allocation is finished in this module. Specifically, for each bandwidth $b_h$, it can be obtained from solving the following equations:
\begin{equation}
     \mathbf{b} = (\min \{\frac{R_k}{r^k}|k=1,2,...,K\}) \times \mathbf{D}_{norm}
    \label{eq: bandwidth_allocation}
\end{equation}
where $\mathbf{b}=(b_h)\in \mathbb{R}^H$ is the  bandwidth allocation results derived from \Pascal{}. After obtaining the results, each related task (such as throughput, flush, garbage collection, etc.) will be noticed with the results and execute incident bandwidth control. Besides, the ground truth value of resource efficiency will be stored for next time prediction.

\vspace{-2ex}

% \subsection{Implementation}
% \subsubsection{Overview}
% \Pascal{} mainly consists of four modules, namely pressure mapper, migration planner, resource predictor and bandwidth allocator. The relation and information flow between above modules are shown in Figure~\ref{fig:pascal}.

% \subsubsection{Modules}
% Pressure mapper module is designed according to the local policy described in Sec.\ref{sec: local policy} and tuned by Bayesian optimization. It receives watermark level information from every node in the HSS and 
\begin{algorithm}[t]
	
	\caption{Running process of \Pascal{}}
	\begin{algorithmic}[1] %每行显示行号
		\STATE \textbf{Input:} Architecture of hierarchical storage system (i.e., the DAG), hardware configuration (i.e., upper limit of kinds of resource), pressure mapping function $\mathcal{F}$, resource prediction function $G$
		\STATE \textbf{Initialization:} 1) For each $r_h^k$, initialize queue $Q_h^k=\{\}$ for storing ground truth value of resource efficiency; 2) set timestamp $t=0$
		\WHILE{True}
		\STATE \textbf{Pressure mapping:} For each node $i$ with water level $w_i$, get its pressure value $p_i=\mathcal{F}(w_i)$
		\STATE \textbf{Data migration planning:} Given real-time workload information, connecting matrix and pressure value of each node, get the data migration ratio among edges $\mathbf{D}_{norm}$ according to Eq.(\ref{eq: raito_migration}).
		\STATE \textbf{Resource efficiency prediction:} Given queues of historical resource efficiency, predict current resource efficiency $\mathbf{r}$ according to Eq.(\ref{eq: time_series_prediction}).
		\STATE \textbf{Bandwidth allocation:} Given $\mathbf{D}_{norm}$ and $\mathbf{r}$, calculate the bandwidth allocation using Eq.(\ref{eq: bandwidth_allocation}).
		\STATE \textbf{Data recording:} record the ground truth value of current resource efficiency and store it into queues.
		\STATE $t=t+1$
		\ENDWHILE
	\end{algorithmic}
	\label{alg: pascal}
\end{algorithm}



% \subsubsection{Pressure mapper}
% For each node $i$, it has multiple indicators, such as maximum capacity $C_i\in \mathbb{R}$, water level $w_i\in [0,1]$ (same as definition in Section~\ref{sec: problem_definition}) and ideal water level $w_i^0\in[0,1]$ around which we want the node to keep the water level. The water level $w_i$ will be mapped to a pressure $p_i\in \mathbb{R}$ via a function $\mathcal{F}: w_i \mapsto p_i$. 
% The expression of mapping function $\mathcal{F}$ is not restricted here. But it should at least have the following properties.
% Based on the ideal illustrated in Sec.\ref{local policy}, a module is designed for each node in system to calculate the pressure value for them, which is called pressure mapper. 

% For each tier of interest (also the node of the DAG), we will maintain 

% This module is used in mapping the watermark level of each layer into a pressure value that reflects how much capacity left in the layer for incoming data flow. Actually it is a mapping function, which can be defined according to performance requirements. However, it is supposed to be subject to the following properties.



% For ease of exposition, several examples of pressure mapping function are depicted in Figure~\ref{fig: pv-function}. Using the pressure mapper, we could construct a pressure vector $\mathbf{p}=(p_i)_{i \in V}$ for each node of the DAG.

% \subsubsection{Migration planner}
% \label{sec: migration_planner}



% Besides the mentioned-above indicators, there is another very important feature for node $i\in V$, i.e., the connecting relation $\mathbf{e}_i=(e_{i,j})_{j\in V}$. Basically, the definition of $\mathbf{e}_i$ is equivalent to the edge defined in Section~\ref{sec: problem_definition}. Additionally, we define the value of $e_{i,j}$ as follows:
% \begin{equation}
%     e_{i,j} =\left\{
%     \begin{array}{lr}
%     -1, & \text{if } i\to j \\
%     1, & \text{if } j \to i \\
%     0, & \text{if } i \nleftrightarrow j
%     \end{array}
%     \label{eq: vec_edge}
% \right.
% \end{equation}
% where $\to$ denotes the direction of data migration; $\nleftrightarrow$ represents that there is no connecting relation between the two nodes. According to Eq.(\ref{eq: vec_edge}), a connecting matrix $\mathbf{E}=(\mathbf{e}_{i})_{i \in V}$ could be constructed, that is the connection information shown in Figure~\ref{fig:pascal}. In practice, the time-varying workload might have relatively great impact on the data migration. For example, the changing of write/read ratio, block size and random/sequential access might affect the ratio of hot/cold/garbage data migrating from cache tier to SSD tier (see Figure~\ref{fig:pascal}). Thus it is required that the workload information is taken into consideration in the data migration. Similarly, we can construct a workload information matrix $\mathbf{W}=(wo_{i,j})\in \mathbb{R}^{H\times |V|}$. The value of $wo_{i,j}$ can be observed from historical record of workload statistical information (such as the ratio of hot/cold/garbage data), which is subject to:
% \begin{equation}
%     \sum_{j\in V} wo_{i,j} = 1, \quad \forall i\in V
% \end{equation}
% \begin{equation}
%      0 \leq wo_{i,j} \leq 1
% \end{equation}

% With above information, the ratio of data to be migrated along edges can be calculated via:
% \begin{equation}
%     \mathbf{D} = -\mathbf{W}\times \mathbf{E} \times \mathbf{p}^T
% \end{equation}
% \begin{equation}
%     \mathbf{D}_{norm} = \frac{\mathbf{D}}{|\mathbf{D}|_1}
%     \label{eq: raito_migration}
% \end{equation}
% where $|\cdot|_1$ is the $l_1$ normalization function; $\mathbf{D}_{norm}\in \mathbb{R}^{H}$ is the normalized migration ratio.

% %  In practice, data migration in several edges may be limited by a single amount of bandwidth, which is the sum of data size migrated by them. To guide the bandwidth allocation of a certain task,  a value of pressure difference is demanded to assign to the corresponding task. This pressure difference can be obtained by calculating a weighted average of relevant edges' pressure difference. As for the weight of each edges, it's greatly impacted by the time-varying workload.

% \subsubsection{Resource predictor}

% In practice, the resource (such as CPU, memory and bandwidth) assigned to one task might not be used out in executing particular task (e.g. 100 mb bandwidth are assigned to GC task. And the real amount of collecting space in GC might be only 80 mb). Thus there is a resource efficiency problem in assigning resource to different tasks.

% As described above, the resource efficiency (i.e., the resource consumption by one unit of bandwidth) is time-varying in the hierarchical storage system, due to the complex run-time environment.

% The run-time environment of the hierarchical storage system is extremely complex. Various background tasks existing in the system, including bandwidth control, compression, scheduling, deduplication, etc, might affect each others. For example, these tasks compete for kinds of data access, system resources throughout the run time. It is intractable to fully analyze the mutual influences among these background tasks. However it is impractical to assume that the resource efficiency (i.e., $r_h^k$ the resource consumption by one unit of bandwidth) is constant throughout the control process. Instead, we resort to model the resource efficiency and to predict it when allocating the bandwidth. Specifically, for each $r_h^k$ (defined in Section~\ref{sec: problem_definition}) at time interval $t$, it can be predicted via:
% \begin{equation}
%     \Tilde{r}_h^{k} = G(\{r_h^{k,t'}|t'=t-1, t-2,...,t-m\})
%     \label{eq: time_series_prediction}
% \end{equation}
% where $\{r_h^{k,t'}|t'=t-1, t-2,...,t-m\}$ is a time series of ground truth value of historical $r_h^k$; $G$ is the predicting function which can either be classical time series forecasting methods such as ARIMA~\cite{Zhang2003Time, 2021Visibility} or be supervised learning models such as LSTM~\cite{2014Long, 2016Learning}. In this way, a predicted resource efficiency matrix $\mathbf{r}$ can be constructed:
% \begin{equation}
%     \mathbf{r}=(\Tilde{r}_h^{k}), \quad h= 1,...,H,  k=1,...,K
% \end{equation}

% % There is no explicit recommendation for $G$. The 


% \subsubsection{Bandwidth allocator}
% With the data migration ratio and the predicted resource efficiency available, the resource consumption vector $\mathbf{r_S} \in \mathbb{R}^{K}$ of the whole system when migrating one unit of data can be calculated via:
% \begin{equation}
%     \mathbf{r_S} = \mathbf{r} \times \mathbf{D}_{norm}
%     \label{eq: r_S}
% \end{equation}

% Taking as input the ratio of data to be migrated and resource efficiency prediction, the bandwidth allocation is finished in this module. Specifically, for each bandwidth $b_h$, it can be obtained from solving the following equations:

% \begin{equation}
%      \mathbf{b} = (\min \{\frac{R_k}{r^k}|k=1,2,...,K\}) \times \mathbf{D}_{norm}
%     \label{eq: bandwidth_allocation}
% \end{equation}
% where $\mathbf{b}=(b_h)\in \mathbb{R}^H$ is the  bandwidth allocation results derived from \Pascal{}. After obtaining the results, each related task (such as throughput, flush, garbage collection, etc.) will be noticed with the results and execute incident bandwidth control. Besides, the ground truth value of resource efficiency will be stored for next time prediction.

\subsection{Summary \& Discussion}

\label{sec: discussion}
To summarize the whole procedure of \Pascal{}, a snippet of pseudo code is given in Algorithm~\ref{alg: pascal}. The procedure and function of each module within \Pascal{} are described above. However, we do not claim that our recommended implementation of \Pascal{} is the best one but it outperforms other heuristic method, which is shown in experiments. Readers of interest could adopt other techniques to implement modules of \Pascal{}, to further enhance its performance. For example, the pressure mapping function could be either designed by human expert or learned by neural network~\cite{2022Artificial}, Bayesian optimization method~\cite{0BOA}; The resource efficiency prediction could be improved using  transformer~\cite{DBLP:journals/corr/VaswaniSPUJGKP17}. The further discussion is beyond the scope of the paper.
A valid explanation for the effectiveness of \Pascal{} is that it has a structure similar to a neural network, where the pressure mapping functions as an active layer and data migration planning resembles a connected layer. Due to this structure, \Pascal{} is able to deal with various situation appropriately.
% The pressure mapping function and resource efficiency prediction could adopt other advances learning techniques, such as Bayesian optimization, Transformer, etc.

% \begin{algorithm}[t]
	
% 	\caption{Running process of \Pascal{}}
% 	\begin{algorithmic}[1] %每行显示行号
% 		\STATE \textbf{Input:} Architecture of hierarchical storage system (i.e., the DAG), hardware configuration (i.e., upper limit of kinds of resource), pressure mapping function $\mathcal{F}$, resource prediction function $G$
% 		\STATE \textbf{Initialization:} 1) For each $r_h^k$, initialize queue $Q_h^k=\{\}$ for storing ground truth value of resource efficiency; 2) set timestamp $t=0$
% 		\WHILE{True}
% 		\STATE \textbf{Pressure mapper:} For each node $i$ with water level $w_i$, get its pressure value $p_i=\mathcal{F}(w_i)$
% 		\STATE \textbf{Migration planner:} Given real-time workload information, connecting matrix and pressure value of each node, get the data migration ratio among edges $\mathbf{D}_{norm}$ according to Eq.(\ref{eq: raito_migration}).
% 		\STATE \textbf{Resource predictor:} Given queues of historical resource efficiency, predict current resource efficiency $\mathbf{r}$ according to Eq.(\ref{eq: time_series_prediction}).
% 		\STATE \textbf{Bandwidth allocator:} Given $\mathbf{D}_{norm}$ and $\mathbf{r}$, calculate the bandwidth allocation using Eq.(\ref{eq: bandwidth_allocation}).
% 		\STATE \textbf{Data recorder:} record the ground truth value of current resource efficiency and store it into queues.
% 		\STATE $t=t+1$
% 		\ENDWHILE
% 	\end{algorithmic}
% 	\label{alg: pascal}
% \end{algorithm}

% \begin{algorithm}[h]
% 	\caption{Pseudo code of \Pascal{}}
% 	\begin{algorithmic}[1] %每行显示行号
% 	    \STATE \textbf{Input:} structure of hierarchical storage system $G=(V,E)$, connecting matrix $\mathbf{E}$, pressure mapping function $\mathcal{F}$, ideal water level $w_i^0, i\in V$, ideal pressure value $p_i^0, i\in V$, duration of time interval $T_0$, predictor $G$, 
% 		\STATE \textbf{Initialize:} the policy $\pi$ and action value function $Q$ with random parameters $\theta$ and the memory replay $D=\{\}$
% 		\FOR{$episode=1$ to $maxEpisode$}
% 		\STATE Reset the environment in simulator
% 		\FOR{ $t=1$ to $T$}
% %		\algorithmiccomment{orders in time interval $t$}
% 		\FOR{each order $o_t^i$ in time interval $t$}
% 		\FOR{each vehicle $k=1$ to $K$}
% 		\STATE Get $fe^i_{t,k}$, $rp^i_{t,k}$  $d^i_{t,k}$ via Algorithm~\ref{alg: route planner}
% 		\STATE Compute the individual vehicle state $s_{t,k}^i$ using $d^i_{t,k}$ 
% 		\STATE Update the individual vehicle state $s_{t,k}^i$ with the feasibility flag $fe^i_{t,k}$ according to Eq.(\ref{eq: feasibility check})
% 		\ENDFOR
% 		\STATE Construct the joint state $S_t^i$ via concatenating all vehicles' state $s_{t,k}^i$
% 		\STATE Sample an action with $\epsilon$-greedy policy: $a^i_t \sim \pi (S_t^i; \theta)$
% 		\STATE Compute the instant reward $r^i_t$ according to Eq.(\ref{eq: instant reward})
%         \IF{$o_t^i$ is the last order in time interval $t$}
%         \STATE $\mathcal{F}^i_{t}\leftarrow \mathrm{True}$
%         \ELSE
%         \STATE $\mathcal{F}^i_{t}\leftarrow \mathrm{False}$
%         \ENDIF
% 		\ENDFOR
% 		\ENDFOR
% 		\STATE Compute the long-term reward $\bar{r}^i_t$ according to Eq.(\ref{eq: long term reward})
% 		\STATE Store all state transitions $\mathcal{S}=\{(S^{i}_t, a^i_t, \mathcal{F}^i_t, R^i_t, S^{i+1}_{t})\}$ during the $episode$ into $D$
% 		\STATE Sample a mini-batch $\mathcal{B}$ from the memory replay $D$
%         \FOR{each state transition $(S^{i}_t, a^i_t, \mathcal{F}^i_t, R^i_t, S^{i+1}_{t})$ in $\mathcal{B}$}
% 		\IF{$\mathcal{F}^i_t$ is $\mathrm{True}$}
% 		\STATE $y^i_t\leftarrow R^i_t$
% 		\ELSE
% 		\STATE $y^i_t\leftarrow R^i_t + \gamma{Q'(S^{i+1}_{t}, \arg\max_{a^{i+1}_t\in\mathcal{A}}Q(S^{i+1}_t,a^{i+1}_t;\theta);\theta')} $
% 		\ENDIF
%         \ENDFOR
% 		\STATE Update the evaluate network according to Eq.(\ref{eq1}): $\theta \leftarrow \theta + \nabla_\theta\mathcal{L}(\theta)$
%         \IF{$episode \% \mathcal{T} = 0$}
%         \STATE Update the target network: $\theta' \leftarrow \theta$
% 		\ENDIF
%         \ENDFOR
		
% 	\end{algorithmic}
% 	\label{alg: ddgn}
% \end{algorithm}

% \vspace{-3ex}