\documentclass[runningheads]{llncs}

%\usepackage{fullpage}
\usepackage[
sortcites,
style=lncs,
backend=biber,
doi=false,
url=false]{biblatex}
\addbibresource{references.bib}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
%\usepackage{bibnames}

%\usepackage{natbib}
%\usepackage{cite}
\usepackage{amsmath,amsfonts,amssymb,amsfonts}
\usepackage[algo2e,ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{hhline}
\usepackage{tabu}
\usepackage{tikz}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{calligra}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{enumitem}
%\usepackage{amsthm}
\usepackage{bbding}

\usepackage{mdframed}
\setcounter{tocdepth}{1}

%\usepackage[utf8]{inputenc}
\usepackage{hyperref}
%\usepackage{setspace}
%\usepackage{palatino}
%\usepackage{graphicx}
%\usepackage{float}
%\usepackage{titling} % drop vertical space before the title
\usepackage{multirow}
%\usepackage{lscape}
%\usepackage{subcaption}
%\fontfamily{ppl}\selectfont 


%\usepackage[textwidth=17mm, colorinlistoftodos]{todonotes}

% \newcommand{\tododo}[1]{\todo[inline, shadow, color=CbTeal!80]{#1}} % Ici, rien n'est fait
% \newcommand{\todoref}[1]{\todo[inline, shadow, color=CbGreen!80]{#1}} % Ajouter une r\'ef
% \newcommand{\todomore}[1]{\todo[inline, shadow, color=CbPlum!40]{#1}} % En dire plus
% \newcommand{\todocheck}[1]{\todo[inline, shadow, color=CbOrange!40]{#1}} % V\'erifier ici
% \newcommand{\todono}[1]{\todo[inline, shadow, color=CbFuschia!80]{#1}} % Ãa, c'est pourri
% \newcommand{\todofig}[1]{\missingfigure[figcolor=white]{#1}} % Ici, il faudra une figure

%\usepackage{setspace}
%\renewcommand{\baselinestretch}{1} 

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{corollary}{Corollary}[theorem]
% \newtheorem{lemma}[theorem]{Lemma}

\newtheorem{assumption}{}
\renewcommand{\theassumption}{A-\arabic{assumption}}
\renewcommand{\qed}{\hfill \ensuremath{\Box}}

\newcommand{\Xmat}{\mathbf{X}}
\newcommand{\Xset}{\mathbb{X}}
\newcommand{\Xvec}{\mathbf{x}}
\newcommand{\Kmat}{\mathbf{K}}
\newcommand{\Fmat}{\mathbf{F}}
\newcommand{\Smat}{\mathbf{S}}
\newcommand{\Id}{\mathbf{I}}
\newcommand{\graph}{\mathcal{G}}
\newcommand{\graphset}{\mathbb{G}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\pa}[1]{\left( #1 \right)}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\norminf}[1]{\left\lVert #1 \right\rVert_{\infty}}
\newcommand{\normf}[1]{\left\lVert #1 \right\rVert_{F}}
\newcommand{\normsubgauss}[1]{\left\lVert #1 \right\rVert_{\psi_2}}


\DeclareMathOperator*{\argmax}{arg\,max} % thin space, limits underneath in displays
\DeclareMathOperator*{\argmin}{arg\,min} % thin space, limits underneath in displays
\DeclareMathAlphabet{\mathcalligra}{T1}{calligra}{m}{n}

\newcommand{\fop}{\Av}
\newcommand{\fopi}{\fop_i}
\newcommand{\fopalt}{\mathcal{B}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\synthesisop}{\mathcal{E}^*}
\newcommand{\regu}{\mathcal{S}}
\newcommand{\reguTheta}{\mathcal{S}_{\theta}}
\newcommand{\reconstr}{\mathcal{R}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\reconstrTheta}{\mathcal{R}_\theta}
\newcommand{\genericNN}{\mathbf{\Psi}}
\newcommand{\genericNNTheta}{\mathbf{\Psi_{\theta}}}
\newcommand{\spacesolutions}{X}
\newcommand{\spaceobs}{Y}
\newcommand{\spacespars}{\Xi}
\newcommand{\funspacedef}[2]{#1 \rightarrow #2}
\newcommand{\dotprod}[2]{\langle #1 , #2 \rangle}
\DeclareMathOperator*{\proxop}{Prox}
\newcommand{\prox}[2]{\proxop_{#1}\left( #2 \right)}

\newcommand{\tp}{^\top}
\newcommand{\sqb}[1]{\left[ #1 \right]}
\newcommand{\prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\diag}[1]{\mathrm{diag}\left(#1\right)}
%\newcommand{\ddiag}[1]{\mathrm{ddiag}\left(#1\right)}
\newcommand{\ddiag}[2]{\mathrm{ddiag}_{#1}\left(#2\right)}
\newcommand{\ran}[1]{\mathrm{Im}\left(#1\right)}
\newcommand{\Vari}[1]{\mathrm{Var}(#1)}
\newcommand{\Expect}[2]{\E_{#1}\sqb{#2}}

\newcommand{\av}{\mathbf{a}}
\newcommand{\Av}{\mathbf{A}}
\newcommand{\Mv}{\mathbf{M}}
\newcommand{\zv}{\mathbf{z}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\yvalt}{\mathbf{\widetilde{y}}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\xvz}{\xv_0}
\newcommand{\xvc}{\overline{\xv}}
\newcommand{\uv}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vvl}{\mathbf{v}_\ell}
\newcommand{\vvg}{\mathbf{v}_g}
\newcommand{\thetav}{\pmb{\theta}}
\newcommand{\thetavalt}{\pmb{\widetilde{\theta}}}
\newcommand{\thetavt}{\thetav(t)}
\newcommand{\thetavs}{\thetav(s)}
\newcommand{\thetavtalt}{\thetavalt(t)}
\newcommand{\thetavz}{\thetav_0}
\newcommand{\wl}{\mathbf{w}_\ell}
\newcommand{\wv}{\mathbf{w}}
\newcommand{\wt}[1]{\mathbf{w}_{#1}}
\newcommand{\gdipz}{\mathbf{g}(\uv,\Wv(0))}
\newcommand{\gdip}{\mathbf{g}(\uv,\thetav)}
\newcommand{\gdipt}{\mathbf{g}(\uv, \thetavt)}
\newcommand{\gdiptalt}{\mathbf{g}(\uv, \thetavtalt)}
\newcommand{\gdipwt}[1]{\mathbf{g}(\uv, \Wv_{#1})}
\newcommand{\lmin}{\lambda_{\mathrm{min}}}
\newcommand{\lmax}{\lambda_{\mathrm{max}}}
\newcommand{\jcal}{\mathcal{J}}
\newcommand{\jtheta}{\mathcal{J}(\thetav)}
\newcommand{\jthetat}{\mathcal{J}(\thetav\left(t\right))}
\newcommand{\jthetas}{\mathcal{J}(\thetav\left(s\right))}
\newcommand{\jthetatalt}{\mathcal{J}(\thetavalt\left(t\right))}
\newcommand{\jthetaz}{\mathcal{J}(\thetavz)}
\newcommand{\Hv}{\mathbf{H}}
%\newcommand{\jthetas}{\mathcal{J}(\thetav\left(s\right))}
\newcommand{\jjtheta}{\jtheta\jtheta\tp}
\newcommand{\jwl}{\mathcal{J}(\wl)}
\newcommand{\jjwl}{\jwl\jwl\tp}
\newcommand{\phidxwl}{\phi'(\uv\tp\wl)}
\newcommand{\phidxwone}{\phi'(\uv\tp\wv_1)}
\newcommand{\Cv}{C_{\vv}}
\newcommand{\mumin}{\mu_{\mathrm{min}}}
\newcommand{\Wv}{\mathbf{W}}
\newcommand{\Wvt}{\mathbf{W}_t}
\newcommand{\Wvalt}{\mathbf{\widetilde{W}}}
\newcommand{\Vv}{\mathbf{V}}
\newcommand{\Vvalt}{\widetilde{\mathbf{V}}}
\newcommand{\jW}{\mathcal{J}(\Wv)}
\newcommand{\jWz}{\mathcal{J}(\Wv(0))}
\newcommand{\jWalt}{\mathcal{J}(\Wvalt)}
\newcommand{\phiWu}{\phi(\Wv\uv)}
\newcommand{\phiWui}[1]{\phi(\wv_{#1}\tp\uv)}
\newcommand{\phiWualt}{\phi(\Wvalt\uv)}
\newcommand{\phidW}{\phi'(\Wv(0)\uv)}
\newcommand{\phidWt}{\phi'(\Wv(t)\uv)}
\newcommand{\phidWalt}{\phi'(\Wvalt\uv)}
\newcommand{\yvt}{\yv(t)}
\newcommand{\yvz}{\yv(0)}
\newcommand{\yvc}{\overline{\yv}}
\newcommand{\yvtalt}{\yvalt(t)}
\newcommand{\ysy}{\yv(s) - \yv}%
\newcommand{\yty}[1][]{%
\ifthenelse{\equal{#1}{}}{\yvt - \yv}{\yv(#1) - \yv}%
}
\newcommand{\yzy}[1][]{%
\ifthenelse{\equal{#1}{}}{\yvz - \yv}{\yv(#1) - \yv}%
}
\newcommand{\ytypar}{(\yty)}
\newcommand{\AJJA}{\fop \jtheta \jtheta\tp \fop\tp}
\newcommand{\AJJAz}{\fop \jthetaz \jthetaz\tp \fop\tp}
\newcommand{\AJJAt}{\fop \jthetat \jthetat\tp \fop\tp}
\newcommand{\AVVA}{\fop \Vv\tp \Vv \fop\tp}
\newcommand{\AvvgA}{\fop \vvg \vvg\tp \fop\tp}
\newcommand{\AJt}{\fop \jthetat}
\newcommand{\AJz}{\fop \jthetaz}
\newcommand{\AJ}{\fop \jcal}
\newcommand{\initMisfit}{\norm{\yz - \yv}}
\newcommand{\paramDrift}{\norm{\thetavt - \thetavz}}
\newcommand{\yt}{\yvt}
\newcommand{\yz}{\yvz}
\newcommand{\Cphi}{C_{\phi}}
\newcommand{\Cphid}{C_{\phi'}}
\newcommand{\rfop}{r_{\fop}}
\newcommand{\stddistrib}{\mathcal{N}(0,1)}
\newcommand{\AVi}{(\fop\Vv\tp)_i}
\newcommand{\AVij}{(\fop\Vv\tp)_{i,j}}
\newcommand{\AViq}{(\fop\Vv\tp)_{i,q}}

\newcommand{\sphericaldistrib}[1]{\mathbb{S}^{#1}}


\newcommand{\Gv}{\mathbf{G}}
\newcommand{\gv}{\mathbf{g}}

\newcommand{\Ball}{\mathbb{B}}
\newcommand{\sph}{\mathbb{S}}

\newcommand{\Lip}{\mathrm{Lip}}
\newcommand{\fthetaz}{f(\pmb{\theta}_0)}
\newcommand{\fthetat}{f(\pmb{\theta}_t)}
\newcommand{\y}{\mathbf{y}}
\newcommand{\lammin}{\lambda_{\min}}
\newcommand{\lammax}{\lambda_{\max}}
\newcommand{\sigmin}{\sigma_{\min}}
\newcommand{\sigmax}{\sigma_{\max}}
\newcommand{\sigminA}{\sigma_{\fop}}
\newcommand{\lambz}{\lambda_0}
\newcommand{\riskemp}[1]{\hat{L}(\pmb{\theta}_{#1})}
\newcommand{\deriv}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
\newcommand{\vecmat}[1]{\mathrm{Vec}\left(#1\right)}
\newcommand{\cond}{\mathrm{Cond}}

%\ifthenelse{\boolean{Drafty}}%
%{%
%	\newcommand{\yvain}[1]{{\color{red}\textbf{Yvain: }#1}}
%}%
%{%
%	\newcommand{\yvain}[1]{{}} 
%}%

%\newcommand{\todo}[1]{{\color{red}[#1]}}

\begin{document}



%\title{A Theoretical Bound on the Necessary Overparametrization of Deep Inverse Prior Networks \todo{Change Title}}
\title{Convergence Guarantees of Overparametrized Wide Deep Inverse Prior}
\titlerunning{Overparametrization of DIP Networks}


\author{Nathan Buskulic\Envelope \and Yvain Qu\'eau \and Jalal Fadili}
%\author{}
\authorrunning{N. Buskulic et al.}

\institute{Normandie Univ., UNICAEN, ENSICAEN, CNRS, GREYC, Caen, France\\ 
\Envelope \href{mailto:nathan.buskulic@unicaen.fr}{\texttt{nathan.buskulic@unicaen.fr}}}
%\institute{}


% \author{Nathan Buskulic\\
% \href{mailto:nathan.buskulic@outlook.fr}{\texttt{nathan.buskulic@outlook.fr}}\\
% \textit{Universit\'e de Caen}}

\maketitle

% \todototoc
% \listoftodos

\begin{abstract}
Neural networks have become a prominent approach to solve inverse problems in recent years. Amongst the different existing methods, the Deep Image/Inverse Priors (DIPs) technique is an unsupervised approach that optimizes a highly overparametrized neural network to transform a random input into an object whose image under the forward model matches the observation. However, the level of overparametrization necessary for such methods remains an open problem. In this work, we aim to investigate this question for a two-layers neural network with a smooth activation function. We provide overparametrization bounds under which such network trained via continuous-time gradient descent will converge exponentially fast with high probability which allows to derive recovery prediction bounds. This work is thus a first step towards a theoretical understanding of overparametrized DIP networks, and more broadly it participates to the theoretical understanding of neural networks in inverse problem settings. 

%Neural networks have become a prominent approach to solve inverse problems in recent years. Amongst the different existing methods, the Deep Image/Inverse Priors (DIPs) technique is an unsupervised approach that optimizes a highly overparametrized neural network to transform a random input into an object whose image under the forward model matches the observation. However, the level of overparametrization necessary for such methods remains an open problem. In this work, we aim to investigate this question for a two-layers neural network with a smooth activation function. We provide overparametrization bounds under which such network trained via continuous-time gradient descent will converge exponentially fast with high probability, from which recovery prediction bounds will be derived. This work is thus a first step towards a theoretical understanding of overparametrized DIP networks, and more broadly it participates to the theoretical understanding of neural networks in inverse problem settings. 

%While proving to be successful, the theoretical underpinning of this method remain unknown. Here we show that for a shallow network 

%Neural networks became one of the main tool to solve inverse problems in recent years and while they proved to be highly successful in many cases, the theoretical understanding in this context is still (sparse ? scarce ?)

%Neural networks , while they can provide great success, the theoretical understanding of 

%In this work we are interested in one method called DIP, and we provide we a theoretical analy
\end{abstract}


\keywords{Inverse problems  \and Deep Image/Inverse Prior \and Overparameterization \and Gradient Flow.} \vspace*{-1em}

%\todototoc
%\listoftodos

%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Statement}
A linear inverse problem consists in reliably recovering an object $\xvc \in \R^n$ from noisy indirect observations
\begin{align}\label{eq:forward}
\yv = \fop \xvc + \epsilon ,
\end{align}
%It is ubiquitous in data processing and is formalized as solving
%\todo{Rewrite the math with yhat to denote the observation and x0}
where $\yv \in \R^m$ is the observation, $\fop\in\R^{m\times n}$ is a linear forward operator, and $\epsilon$ stands for some additive noise. Without loss of generality, we will assume throughout that $\yv \in \ran{\fop}$.

% DIP as a way to solve inverse problems without the need of a database of examples
In recent years, the use of sophisticated machine learning algorithms, including deep learning, to solve inverse problems has gained a lot of momentum and provides promising results, see e.g., reviews \cite{arridge_solving_2019,ongie_deep_2020}. Most of these methods are supervised and require extensive datasets for training, which might not be available. An interesting unsupervised alternative~\cite{ulyanov_deep_2020} is known as Deep Image Prior, which is also named Deep Inverse Prior (DIP) as it is not confined to images. 
% DIPs fit a neural network to transform some initial fixed noise into a signal that matches the observation $\yv$. %While this method alleviates the need for a training dataset, it requires to train a network for each observation $\yv$ of interest.
In the DIP framework, a generator network $\mathbf{g}: (\uv,\thetav) \in \R^d\times \R^p \mapsto \xv \in \R^n$, with activation function $\phi$, is optimized to transform some random input $\uv \in \R^d$ into a vector in $\xv \in \R^n$. The parameters $\thetav$ of the network are optimized via (possibly stochastic) gradient descent to minimize the squared Euclidean loss
% mean square error loss:
\begin{align}\label{eq:min}
\loss(\gdip) = \frac{1}{2m}\norm{\fop\gdip - \yv}^2.
\end{align}
%with respect to $\thetav$. 
Theoretical understanding of recovery and establishing convergence guarantees for deep learning-based methods is of paramount importance to make their routine usage in critical applications reliable \cite{mukherjee_learned_2022}. Our goal in this paper is to participate to this endeavour by explaining when gradient descent consistently and provably finds global minima of \eqref{eq:min}, and how this translates into recovery guarantees of \eqref{eq:forward}. For this, we focus on a continuous-time gradient flow applied to \eqref{eq:min}:
\begin{align}
\begin{cases}
\dot{\thetav}(t) = - \nabla_{\thetav} \loss(\gdipt),  \\
\thetav(0) = \thetav_0 .
\end{cases}\label{eq:gradflow}
\end{align}
This is an idealistic setting which makes the presentation simpler and it is expected to reflect the behavior of practical and commonly encountered first-order descent algorithms, as they are known to approximate gradient flows.

%As is often done in this context, we fix $\Vv$ and only optimize over $\Wv$~\cite{oymak_overparameterized_2019,oymak_toward_2020}. In our case, $\thetavt$ is to be understood as the vectorized version of $\Wv$ at time $t$.

%which ensures that under enough overparametrization, the kernel $\jtheta\jtheta\tp$, with $\jthetat = (\nabla_{\thetavt}\gdipt)\tp$ as the Jacobian, is injective.
%This facilitates the analysis of the optimization procedure as it is a continuous process.

%This work focuses on the simplest type of neural networks, that is one-hidden layer networks %(as presented in Figure~\ref{fig:Schema_network}) 
%which are formalized as
%\begin{align}
%    \gdip = \Vv\tp \phi(\Wv\uv),
%\end{align}
%with  $\Vv \in \R^{k \times n}$ and $\Wv \in \R^{k \times d}$ two linear transformations, and $\phi$ an element-wise nonlinear activation function. As is often done in this context, we fix $\Vv$ and only optimize over $\Wv$~\cite{oymak_overparameterized_2019,oymak_toward_2020}. In our case, $\thetavt$ is to be understood as the vectorized version of $\Wv$ at time $t$.

%In our analysis, we fix $\Vv$ and only optimize over $\Wv$ (as often done in this context~\cite{oymak_overparameterized_2019,oymak_toward_2020}) to focus on the nonlinear dynamic of the hidden weights. One should note that in this case, $\thetavt$ is to be understood as the vectorized version of $\Wv$ at time $t$.

% \begin{figure}[!htpb]
%     \centering
%     \includegraphics[scale=0.8]{Illustrations/DIP_OnehiddenLayer.pdf}
%     \caption{schematic representation of a one hidden layer network}
%     \label{fig:Schema_network}
% \end{figure}

%These same works focused on the convergence of shallow neural networks when they are severely overparametrized, that is when the number of hidden neurons is much larger than the dimension of the data or the number of data points. While there has been considerable effort in this direction for general shallow neural networks, it seems that no study of this type has been done for the DIP model. In particular, the amount of overparametrization necessary to reach a zero-loss solution, that is, an optimal solution in the observation space remains unknown, as the convergence rate of DIP models in such an overparametrized regime.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Contributions} 
We will deliver a first theoretical analysis of DIP models in the overparametrized regime. We will first analyze \eqref{eq:gradflow} by providing sufficient conditions for $\yvt := \fop \gdipt$ to converge exponentially fast to a globally optimal solution in the observation space. This result is then converted to a prediction error on $\yvc := \fop\xvc$ through an early stopping strategy. Our conditions and bounds involve the conditioning of the forward operator, the minimum and maximum singular values of the Jacobian of the network, as well as its Lipschitz constant. We will then turn to evaluating these quantities for the case of a two-layer neural network 
\begin{align}\label{eq:dipntk}
    \gdip = \frac{1}{\sqrt{k}}\Vv \phi(\Wv\uv),
\end{align}
with  $\Vv \in \R^{n \times k}$ and $\Wv \times \R^{k \times d}$, and $\phi$ an element-wise nonlinear activation function. The scaling by $\sqrt{k}$ will become clearer later. In this context, the network will be optimized with respect to the first layer (i.e., $\Wv$) while keeping the second (i.e., $\Vv$) fixed. Consequently, $\thetav=\Wv$. We show that for a proper random initialization $\Wv(0)$ and sufficient overparametrization, all our conditions are in force and the smallest eigenvalue of the Jacobian is indeed bounded away from zero independently of time. We provide a characterization of the overparametrization needed in terms of $(k,d,n)$ and the conditioning of $\fop$. Lastly, we show empirically that the behavior of real-world DIP networks is consistent with our theoretical bounds.

% \section{Introduction}
% An inverse problem can be understood as the task of reliably recovering a signal from noisy indirect observations. This framework allows to represent a wide number of tasks in imaging, physics or engineering such as medical image reconstruction (MRI, CT,...), seismic inversion or computational microscopy. 
% An inverse problem can be formalized as solving the following equation:
% \begin{align}
%     \yv = \fop \xvc + \epsilon,
% \end{align}
% %\todo{Rewrite the math with yhat to denote the observation and x0}
% where $\yv \in \R^m$ is the noisy measurement we have, $\xvc \in \R^n$ is the signal we want to reconstruct, $\fop\in\R^{m\times n}$ is a forward operator, that is a transformation (linear in our case) from the signal space to the measurement space, and finally $\epsilon$ is some additive noise.

% % DIP as a way to solve inverse problems without the need of a database of examples
% In recent years, the use of deep learning to solve inverse problems, that is, finding some way to inverse the forward operator $\fop$, has gain a lot of momentum (as for any area where deep learning could be applied \cite{lecun_deep_2015,ciodaro_online_2012,leung_deep_2014,rivenson_deep_2017,zhang_deep_2018}). Some very promising results emerged, but they usually require an extensive dataset of labeled examples to be accurate. An interesting unsupervised alternative~\cite{ulyanov_deep_2020} known as Deep Image Prior (DIP) is to fit a neural network to transform some initial fixed noise into a signal that matches the observation $\yv$. As this method can be applied on other domain than images, we call it in the rest of this work with the more general name Deep Inverse Prior. This method alleviates the need for a training dataset, at the cost of training a network for each $\yv$ we would be interested in. There exist few theoretical work on this and we hope to better understand these methods by questioning the minimum network size for gradient flow to converge to a globally optimal solution.

% % Optimization traditionally through gradient descent but we use gradient flow as it is easier to analyse
% In the DIP framework, a generator network $\mathbf{g}: \funspacedef{\R^d\times \R^p}{\R^n}$ is trained to transform a fixed random input $\uv \in \R^d$ into a signal in $\R^n$. The parameters of the network through time are described by function $\thetav: \funspacedef{\R^{+}}{\R^p}$. The network is trained to minimize the mean square error loss:
% \begin{align}
%     \loss(\gdipt) = \frac{1}{2}\norm{\fop\gdipt - \yv}^2.
% \end{align}
% with respect to $\thetav$. The optimization used to minimize the loss is usually either gradient descent or some derivative of it such as ADAM. In this work we assume that we have access to the gradient flow, that is the limit of gradient descent when the step-size tends to zero. This results in the ordinary differential equation with Cauchy condition $\thetav_0$ randomly initialized:
% \begin{align}
%     &\thetav(0) = \thetav_0,\\
%     &\deriv{\thetavt}{t} = - \nabla_{\thetav} \loss(\gdipt)\label{eq:gradflow}.
% \end{align}
% %This facilitates the analysis of the optimization procedure as it is a continuous process.

% This work focuses on the simplest type of neural networks, that is one layer networks (as presented in Figure~\ref{fig:Schema_network}), to see what can be said about the reconstruction capabilities of DIP in this setting. A one-hidden layer network is simply a nonlinear transformation to a space in $\R^k$ followed by a linear transformation to $\R^n$. The nonlinear transformation is obtained by applying a linear transformation $\Wv \in \R^{k \times d}$ to the input $\uv$ followed by a nonlinear activation function $\phi$ that applies a nonlinear operation element-wise to the new vector in $\R^k$. The final linear transformation is applied with a matrix $\Vv \in \R^{k \times n}$. The network is formalized as %\todo{Mettre le schema du r\'eseau par ici}
% \begin{align}
%     \gdip = \Vv\tp \phi(\Wv\uv) = \sum^k_{l=1} \phi(\uv^\top \wl) \vvl,
% \end{align}
% where $\wl \in \R^d$ contains the weights of the linear transformation of the input $\uv$ to the $\ell$-th hidden neuron. On the other hand $\vvl \in \R^n$ contains the weights of the transformation from the $\ell$-th hidden node to the output space. 
% % These values are gathered in matrices $\Wv$ and $\Vv$ as follows :

% % \begin{align}
% %     \Wv = \begin{bmatrix}
% %             \wv_1\tp\\
% %             \wv_2\tp\\
% %             \vdots\\
% %             \wv_k\tp
% %         \end{bmatrix}
% %     \qquad
% %     \Vv = \begin{bmatrix}
% %             \vv_1\tp\\
% %             \vv_2\tp\\
% %             \vdots\\
% %             \vv_k\tp
% %         \end{bmatrix}.
% % \end{align}
% %In this setting, we assume $\phi$ is twice differentiable and bounded such that $\abs{\phi'(z)} \leq B$ \todo{Mettre Ã§a plus tard, trop calculatoire pour l'intro} for all $z$. 

% \begin{figure}[!htpb]
%     \centering
%     \includegraphics[scale=0.8]{Illustrations/DIP_OnehiddenLayer.pdf}
%     \caption{schematic representation of a one hidden layer network}
%     \label{fig:Schema_network}
% \end{figure}

% In our analysis, we fix $\Vv$ and only optimize over $\Wv$. This procedure is done in many other works \cite{frei_benign_2022,oymak_overparameterized_2019,oymak_toward_2020,brutzkus_sgd_2017} as it allows to understand the nonlinear dynamic of the hidden weights while facilitating the analysis by fixing the linear part of the network. Therefore, $\thetavt$ is to be understood as the vectorized version of $\Wv$ at time $t$.

% %Thus, in this case we have $\thetav = \Wv$.
% %\todo{Mettre Ã§a plus tÃ´t}[Since the training of the network evolves through time, we denote by $\thetavt$ the parameters of the network at time $t$] 
% %\todo{Pas utile pour l'instant ->} and by $\yv_t$ the reconstruction in the measurement space obtained by the network. 

% These same works focused on the convergence of shallow neural networks when they are severely overparametrized, that is when the number of hidden neurons is much larger than the dimension of the data or the number of data points. While there has been considerable effort in this direction for general shallow neural networks, it seems that no study of this type has been done for the DIP model. In particular, the amount of overparametrization necessary to reach a zero-loss solution, that is, an optimal solution in the observation space remains unknown, as the convergence rate of DIP models in such an overparametrized regime.

% In this work we provide a first theoretical analysis of DIP models in the overparametrized regime. We show that for a randomly initialized one-hidden layer model with smooth activation function and sufficient overparametrization, gradient flow converges to a globally optimal solution in the observation space. We provide a characterization of this overparametrization and find that to enter this special regime, $d$ only depends on the dimension of the observation space and the condition number of the forward operator while $k$ only depends on the problem size and the norm of the forward operator. Furthermore, we show that the gradient flow of such networks converges exponentially fast to this solution.

% %The paper is structured as follows. We discuss prior work and how we relate to them in Section~\ref{sec:prior}. We formalize the mathematical setup in Section~\ref{sec:setup}. In Section~\ref{sec:main_res} we present our main result and assumptions and discuss them. Section~\ref{sec:proof} provide the proof of our main result. We discuss some numerical experiment in Section~\ref{sec:expes} and finally we conclude and suggest future research directions in Section \todo{Redo this section to match the new layout of the paper}.

% The paper is structured as follows. We discuss prior work and how we relate to them in Section~\ref{sec:prior}. We present our main result and its assumptions in Section~\ref{sec:main_res}. We discuss some numerical experiment in Section~\ref{sec:expes}. Section~\ref{sec:proof} provides the proof of our main result and finally we conclude and suggest future research directions in Section~\ref{sec:conclu}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Relation to Prior Work}\label{sec:prior}
%
\subsubsection{Data-Driven Methods to Solve Inverse Problems}
Data-driven approaches to solve inverse problems come in various forms~\cite{arridge_solving_2019,ongie_deep_2020}. The first type trains an end-to-end network to directly map the observations to the signals for a specific problem. %\cite{kim_accurate_2016,zhang_beyond_2017}
While they can provide impressive results, these methods can prove very unstable~\cite{mukherjee_learned_2022} as they do not use the physics of the problem which can be severely ill-posed. To cope with these problems, several hybrid models that mix model- and data-driven algorithms were developed in various ways. One can learn the regularizer of a variational problem  \cite{prost_learning_2021} or use Plug-and-Play methods \cite{venkatakrishnan_plug-and-play_2013} for example. Another family of approaches, which takes inspiration from classical iterative optimization algorithms, is based on unrolling %\cite{adler_learned_2018,aggarwal_modl_2019,diamond_unrolled_2018,gilton_neumann_2020} 
(see \cite{monga_algorithm_2021} for a review of these methods). 
%Therein, a ``layer'' of a neural network is thought as one step of a classical optimization algorithm where some parameters of said algorithm are learned to improve their performance. 
Still, all these methods require extensive amount of training data, which may not always be available. Their theoretical recovery guarantees are also not well understood \cite{mukherjee_learned_2022}.
%the masks in diffusion-based inpainting \cite{alt_learning_2022} or use Plug-and-Play methods \cite{venkatakrishnan_plug-and-play_2013} for example. Another family of approaches, which take inspiration from classical iterative optimization algorithms, are the ``unrolled algorithms'' %\cite{adler_learned_2018,aggarwal_modl_2019,diamond_unrolled_2018,gilton_neumann_2020} 
%(see \cite{monga_algorithm_2021} for a review of these methods). Therein, a ``layer'' of a neural network is thought as one step of a classical optimization algorithm where some parameters of said algorithm are learned to improve their performance. Still, all these methods requires extensive amount of training data, which may not always be available. \textbf{Y: ref inpainting un peu parachut\'ee}

\vspace*{-.5em}

\subsubsection{Deep Inverse Prior}
% DIP networks
The DIP model \cite{ulyanov_deep_2020} (and its extensions that mitigate some of its empirical issues ~\cite{liu_image_2019,mataev_deepred_2019,shi_measuring_2022,zukerman_bp-dip_2021}) is an unsupervised alternative to the supervised approches briefly reviewed above. The empirical idea is that the architecture of the network acts as an implicit regularizer and will learn a more meaningful transformation before overfitting to artefacts or noise. With an early stopping strategy, one can get the network to generate a vector close to the sought signal. However, this remains purely empirical and there is no guarantee that a network trained in such manner converges in the observation space (and even less in the signal space). Our work aims at reducing this theoretical gap, by analyzing the behaviour of the network in the observation (prediction) space.
%However, these works do not provide theoretical guarantees over the convergence of the network in the observations space, which is what the loss optimizes, and 
%While being a very interesting behaviour, there is still no theoretical analysis of the opetimization of the network in the observation space 
%It has been empirically found out that during the optimization, the network will generate vectors closer to the signal than the observation before overfiting to the observation as required by the training. This phenomenon is thought to be the result of some intrinsic regularization properties of the network architecture which would explain this behaviour even if DIP networks are overparametrized and should be able to fit the observation vector perfectly.

\vspace*{-.5em}

\subsubsection{Theory of Overparametrized Networks}
In parallel to empirical studies, there has been a lot of effort to develop some theoretical understanding of the optimization of overparametrized networks \cite{bartlett_deep_2021,fang_mathematical_2021}. Amongst the theoretical models that emerged to analyze neural networks, the Neural Tangent Kernel (NTK) captures the behavior of neural networks in the infinite width limit during optimization via gradient descent. In the NTK framework, the neural network behaves as its linearization around the initialization, thus yielding a model equivalent to learning with a specific positive-definite kernel (so-called NTK). In \cite{jacot_neural_2018}, it was shown that in a highly overparametrized regime and random initialization, parameters $\thetavt$ stay near the initialization, and are well approximated by their linearized counterparts at all times (also called the ``lazy'' regime in \cite{chizat_lazy_2019}). With a similar aim, several works characterized the overparametrization necessary to obtain similar behaviour for shallow networks, see e.g., \cite{du_gradient_2019,oymak_overparameterized_2019,allen-zhu_convergence_2019,oymak_toward_2020}. All these works provide lower bounds on the number of neurons from which they can prove convergence rates to a zero-loss solution. Despite some apparent similarities, our setting has important differences. On the one hand, we have indirect measurements through (fixed) $\fop$, the output is not scalar, and there is no supervision. On the other hand, unlike all above works which deal with a supervised training setting, in the DIP model the dimension $d$ of the input is a free parameter, while it is imposed in a supervised setting.
%This naturally involves the Neural Tangent Kernel (NTK) \cite{jacot_neural_2018}, a kernel that captures the behavior of neural networks in the infinite width limit during optimization via gradient descent. 

% \section{Prior Work}\label{sec:prior}

% \subsubsection{Data-Driven Methods to Solve Inverse Problems}
% The landscape of inverse problem solving methods has changed substantially with the arrival of data-driven methods and more specifically neural networks \cite{ongie_deep_2020,lucas_using_2018,arridge_solving_2019}. The first ideas involving neural networks were to train them to directly solve specific inverse problems such as denoising \cite{zhang_beyond_2017} or super-resolution \cite{kim_accurate_2016}. However, for tasks where inverting a forward operator is necessary, this leads to very unstable results as the problem is ill-posed and no regularization is provided to transform the problem into a well-posed one as is classically done in model-driven optimization. Furthermore, these methods did not usually take into account the forward operator at hand and only relied on dataset with examples of problems associated with their solution.

% %\subsubsection{Titre 2}
% % Unrolled methods
% In order to undermine these problems, several hybrid models that mix model and data driven algorithm were developed. From these, the use of learned regularisation in classical variational setting \cite{prost_learning_2021}, the learning of masks in diffusion-based inpainting \cite{alt_learning_2022} or the Plug-and-Play methods \cite{venkatakrishnan_plug-and-play_2013} are prime examples. Another family of approaches, which took inspiration from classical iterative optimization algorithms, are the ``unrolled algorithms'' \cite{gilton_neumann_2020,aggarwal_modl_2019,diamond_unrolled_2018,adler_learned_2018} (see \cite{monga_algorithm_2021} for a review of these methods). The idea behind these methods is to represent a step of the optimization algorithm as a ``layer'' of a neural network and composing these layers is equivalent to doing several steps in the original algorithm. These unrolled networks furthermore build on the data-driven nature of neural networks to learn some of the parameters of the original algorithms and improve their performances on specific tasks, at the cost of mathematical understanding. These unrolled networks usually take into account the physics of the problem by iteratively applying the forward operator or its adjoint at each layer. 

% \subsubsection{Deep Inverse Priors}
% % DIP networks
% These data-driven methods come with a natural drawback which lies in their need of large dataset of solved problems to be efficient. However, many fields don't have access to such kind of datasets and are in need of methods that do not rely so heavily on data. A response to the problem that still uses neural networks appeared through the Deep Image Prior model \cite{ulyanov_deep_2020} (and its extensions that mitigate some of the empirical issues with the methods \cite{shi_measuring_2022,zukerman_bp-dip_2021,mataev_deepred_2019,liu_image_2019}). These models trains a neural network to transform a fixed random input to a target vector in some observation space. 
% %It has been empirically found out that during the optimization, the network will generate vectors closer to the signal than the observation before overfiting to the observation as required by the training. This phenomenon is thought to be the result of some intrinsic regularization properties of the network architecture which would explain this behaviour even if DIP networks are overparametrized and should be able to fit the observation vector perfectly.
% % Remarque sur le fait que pendant l'optimisation le signal devient proche blabla + bien faire comprendre que c'est empirique
% % Dire que c'est bien beau mais on a mÃªme pas de th\'eorie sur ce qu'il se passe au niveau des observations (ce qpour quoi le r\'eseau est entrain\'e)
% % Nous on prouve que les r\'eseaux font bien ce pour quoi ils sont entrain\'es.
% % unsurprinsingly, if no theory on observation space, no theory on signal space.
% In these works their reconstruction result is due to an intriguing behaviour of DIP, which is that during the optimization, the network will generate vectors closer to the signal than the observation before overfiting to the observation as required by the training. This phenomenon is thought to be the result of some intrinsic regularization properties of the network architecture. 
% While this observed behaviour in the signal space is interesting, there is still no theoretical guarantees that the network converges in the simpler to manipulate observation space, which is the space in which the network is optimized, which would be the first step to provide meaningful analysis in the signal space. Our work aims at reducing that theoretical gap by analysing the behaviour of the network in this observation space with the hope to produce a follow-up theory in the signal space in future work.

% %However, these works do not provide theoretical guarantees over the convergence of the network in the observations space, which is what the loss optimizes, and 
% %While being a very interesting behaviour, there is still no theoretical analysis of the opetimization of the network in the observation space 
% %It has been empirically found out that during the optimization, the network will generate vectors closer to the signal than the observation before overfiting to the observation as required by the training. This phenomenon is thought to be the result of some intrinsic regularization properties of the network architecture which would explain this behaviour even if DIP networks are overparametrized and should be able to fit the observation vector perfectly.

% \subsubsection{Theory of Overparemetrized Networks}
% In parallel to the empirical studies, there has been a lot of effort made to develop some theoretical understanding of the optimization of overparametrized networks \cite{bartlett_deep_2021,fang_mathematical_2021}. Several mathematical models emerged to help analyze the behaviour of such networks and usually focused on two-layers networks. The random features model is describing the dynamic of networks where the ``inner'' weights are not trained, that is, where only the linear layer is trained \cite{rahimi_weighted_2008}. This linearization provides interesting insights but is limited to explain the convergence of neural networks in practice.

% A complementary approach to random features model lies in the so-called Neural tangent Kernel (NTK) \cite{jacot_neural_2018}. Under specialized scaling and random initialization of the model, the parameters of said model will stay in a close neighborhood of the initialization when the number of neurons is sufficiently high, making it amendable to linear approximation. From this linearization, we can extract a kernel in the tangent space around the initialization which is called the NTK. This linear regime of the network is also called the ``lazy'' regime in \cite{chizat_lazy_2019}.
% % Should we say that our work is in this setting ?

% While the NTK regime has been defined for infinite width networks, several works characterized the overparametrization necessary to obtain similar behaviour for finite-width network \cite{oymak_overparameterized_2019,allen-zhu_convergence_2019,du_gradient_2019-1}. More specifically, they provide lower bounds on the number of neurons of the network from which they can prove polynomial convergence rates to a zero-loss solution. Our contributions are closely related to this line of work. We also provide a characterization of the overparametrization but in our case, we do so in the DIP setting. This means that we provide in our bounds the dependency of the overparametrization to the properties of the forward operator at hand.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{DIP Guarantees}\label{sec:main_res}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notations}
For a matrix $\Mv \in \R^{a \times b}$ we denote, when dimension requirements are met, by $\lammin(\Mv)$ and $\lammax(\Mv)$ (resp. $\sigmin(\Mv)$ and $\sigmax(\Mv)$) its smallest and largest eigenvalues (resp. non-zero singular values), and by $\kappa(\Mv) = \frac{\sigmax(\Mv)}{\sigmin(\Mv)}$ its condition number. We also denote by $\normf{\cdot}$ the Frobenius norm and $\norm{\cdot}$ the Euclidean norm of a vector (or operator norm of a matrix). We use $\Mv^i$ (resp. $\Mv_i$) as the $i$-th row (resp. column) of $\Mv$. We represent a ball of radius $r$ and center $x$ by $\Ball(x,r)$. 
%$\diag{\uv}$ will denote the diagonal matrix induced by $\uv \in \R^d$. 
%and by $\normsubgauss{X} = \inf\{s > 0 ~\lvert~ \Expect{X}{\exp(X^2/s^2)} \leq 2\}$ the sub-Gaussian norm of a random variable $X$. 
%and $\ddiag{k}{\uv}$ is the $\R^{k \times kd}$ matrix where at the $i$-th row $\uv\tp$ is copied starting at the $d(i-1)$ column and all the other entries at 0.
% \begin{align}
%     \ddiag{k}{\uv} = \begin{bmatrix}
%         \uv\tp & 0 & \hdots & 0 \\
%         0 & \uv\tp & & \vdots \\
%         \vdots & & \ddots & \vdots\\
%         0 & \hdots & \hdots & \uv\tp
%     \end{bmatrix}.
% \end{align}
We also define $\yvt = \fop \gdipt$ and $\yvc = \fop\xvc$. The Jacobian of the network is denoted $\jthetat$. The Lipschitz constant of a mapping is denoted $\Lip(\cdot)$. We set $\Cphi = \sqrt{\Expect{g\sim\stddistrib}{\phi(g)^2}}$ and $\Cphid = \sqrt{\Expect{g\sim\stddistrib}{\phi'(g)^2}}$ with $\Expect{}{X}$ the expected value of $X$. 
% We also present the definition of useful quantities used throughout the paper. First, we define the Jacobian of the network as $\jthetat = (\nabla_{\thetavt}\gdipt)\tp$. Second, we define the output of the network $g$ in observation space as 
% \begin{align}
%     \yvt = \fop \gdipt,
% \end{align}
% we also define $\yvc = \fop \xvc$. Other useful quantities are given by $\Cphi = \sqrt{\Expect{g\sim\stddistrib}{\phi(g)^2}}$ and $\Cphid = \Expect{g\sim\stddistrib}{\phi'(g)^2}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main Result}

\paragraph{\textbf{Standing Assumptions}} 
In the rest of this work, we assume that:
\begin{mdframed}
%\begin{assumption}\label{ass:surjective_op}
%$\fop$ is a surjective linear operator; \vspace*{-.75em}
%\end{assumption}
\begin{assumption}\label{ass:u_sphere}
$\uv$ is drawn uniformly on $\sph^{d-1}$; \vspace*{-.75em}
\end{assumption}
\begin{assumption}\label{ass:w_init}
$\Wv(0)$ has iid entries from $\stddistrib$; \vspace*{-.75em}
\end{assumption}
\begin{assumption}\label{ass:v_init}
$\Vv$ has iid columns with identity covariance and $D$-bounded entries;  \vspace*{-.75em}
\end{assumption}
\begin{assumption}\label{ass:phi_diff}
$\phi$ is a twice differentiable function with $B$-bounded derivatives.  %\vspace*{-.75em}
\end{assumption}
% \begin{assumption}\label{ass:phi_lip}
% Both $\phi$ and $\phi'$ are $B$-Lipschitz, for some $B > 0$.
% \end{assumption}
\end{mdframed}

%In the rest of this work we make several assumptions that we now list. (1) we only consider surjective forward operator \todo{Discuter de pourquoi et des cons\'equences} $\fop \in \R^{m \times n}$ , (2) the non linearity of the network $\phi$ is twice differentiable and both $\phi$ and $\phi'$ are $B$-Lipschitz, (3) \todo{u on the sphere} the input to the network $\uv \in \R^d$ is fixed with all its entries equal to $1/\sqrt{d}$, (4) $\Vv \in \R^{k\times n}$ and $\Wv \in \R^{k\times d}$ \todo{plutÃ´t theta(t=0)} entries are drawn independently from  $\stddistrib$, (5) \todo{Deja dit} we only train the parameters of $\Wv$ and keep $\Vv$ fixed.

%\todo{Discussion globale des hypothÃ¨ses pour savoir si elles sont ok + refs}

Assumptions~\ref{ass:u_sphere}, \ref{ass:w_init} and \ref{ass:v_init} are standard. Assumptions~\ref{ass:phi_diff} is met by many activations such as the softmax, sigmoid or hyperbolic tangent. Including the ReLU would require more technicalities that will be avoided here. 
%As far as Assumption~\ref{ass:surjective_op} is concerned, it is verified in many common inverse problems such as inpainting or compressed sensing. Singular kernels are however not covered by our analysis. Assumption~\ref{ass:surjective_op} is also reminiscent of overparametrized ridgeless least-squares \cite{Hastie22}.

\paragraph{\textbf{Well-posedness}} In order for our analysis to hold, the Cauchy problem~\eqref{eq:gradflow} needs to be well-defined. This is easy to prove upon observing that under \eqref{ass:phi_diff}, the gradient of the loss is both Lipschitz and continuous. Thus, the Cauchy-Lipschitz theorem applies, ensuring that \eqref{eq:gradflow} has a unique global continuously differentiable solution trajectory.

% The assumptions 2,3,4 are rather mild and are not restricting the problem too much.
% Assumptions 5 and 6 on the activation function $\phi$ are respected for a sigmoid or a hyperbolic tangent which are still used in Machine Learning. It restrict from the use of RELU function which is widely used and would be the most realistic use-case.
% Assumption 1 with fop surjective is the most restrictive. It still allows to model a number of inverse problems such as super-resolution, denoising, deconvolution, \todo{Trouver d'autres exemples}

Our main result establishes the prediction error for the DIP model.
\begin{theorem}\label{thm:main}
Consider a network $\gdip$, with $\phi$ obeying \eqref{ass:phi_diff}, optimized via the gradient flow \eqref{eq:gradflow}.
\begin{enumerate}[label=(\roman*)]
\item Let $\sigminA = \inf_{\zv \in \ran{\fop}}\norm{\fop\tp \zv}/\norm{\zv} > 0$. Suppose that
\begin{equation}\label{eq:bndR}
\frac{\norm{\yv - \fop \mathbf{g}(\uv,\thetavz)}}{\sigminA} < \frac{\sigmin(\jthetaz)^2}{4\Lip(\jcal)} .
\end{equation}
Then for any $\epsilon > 0$
\begin{align}\label{eq:convexp}
\norm{\yvt - \yvc} \leq 2 \norm{\epsilon} \qquad \text{for all} \qquad t \geq \frac{4m\log\left(\norm{\yv - \fop \mathbf{g}(\uv,\thetavz)}/\norm{\epsilon}\right)}{\sigminA^2\sigmin(\jthetaz)^2} .
\end{align}

\item Let the one-hidden layer network \eqref{eq:dipntk} with architecture parameters obeying
\begin{align*}
k \geq C_1 \kappa(\fop)^2n \pa{\sqrt{n}\pa{\sqrt{\log(d)} + 1} + \sqrt{m}}^2    
\end{align*}
Then 
%will converge exponentially fast to the optimal solution in measurement space up to the noise level such that
\begin{align*}
\norm{\yvt - \yvc} \leq 2 \norm{\epsilon} \qquad \text{for all} \qquad t \geq \frac{C_2m\log\left(\norm{\yv - \fop \mathbf{g}(\uv,\Wv(0))}\right)}{\sigminA^2\Cphid^2} 
\end{align*}
% \begin{align}
%     \norm{\yvt - \yv}^2 \leq  \norm{\yvz - \yv}^2 \exp\left\{-\frac{1}{4}\sigmin\left(\fop\jthetaz\right) t\right\},
% \end{align}
with probability at least $1 - n^{-1} - d^{-1}$, where $C_i$ are positive constants that depend only on the activation function and the bound $D$.
%where $C_d$, $C_k$ and $c$ are positive constants, with $C_k$ depending on  $\normsubgauss{\sqrt{d}\uv}$, $\norm{y}$, $\tau_1$, $\tau_2$, $\tau_3$, $\tau_4$ and $\Cphi$, $C_d$ depending on $\normsubgauss{\sqrt{d}\uv}$, $\Cphi$, $\Cphid$, $B$ and $\tau_2$ and $c$ depending on $B$ and $\Cphid$.
\end{enumerate}
\end{theorem}

% First discuss the overparametrization of k bound with a sentence about the spectral norm of A.
% Second say that the probability are coherent and explain km > k+n
% Third, express the bound with d, the linear scaling with m and the condition number of A
% 
% Talk about the fact that k plays a bigger role in the overparametrization than d
% 

% Discuss overparametrization of k
Before proceeding with the proof, a few remarks are in order: 
%\begin{remark}
\begin{enumerate}[label=$\bullet$]
\item We start with the scaling of the network architecture parameters required. First, the bound on $k$, the number of neurons of the hidden layer, scales quadratically in $n^2$ and linearly in $m$. We thus have the bound $k \gtrsim n^2m$. The probability of success in our theorem is also dependent on the architecture parameters. More precisely, this probability increases with growing number of observations.\\[.25em]
%the probability with which our result holds grows with the problem size. The only part in this probability depending on the problem size is the ratio $km/(\sqrt{k} + \sqrt{n})^2$ in the third term. Since $k$ is of the order $n^2m$, it means that this ratio will be of the order $m$ which means that the probability will increase with the problem size as desired. \textbf{cinq dernieres lignes pas hyper clair !}

% Discuss proba of d
\item The other scaling of the theorem is on the input size $d$ and informs us that its influence is logarithmic. The bound is more demanding as $\fop$ becomes more ill-conditioned. The latter dependency can be interpreted as follows: the more ill-conditioned the original problem is, the larger the network needs to be. Let us emphasize that, contrary to other learning settings in the literature, the size $d$ of the random input  $\uv$ is free, and so far it has remained unclear how to choose it. Our result provides a first  answer for shallow networks in the overparametrized setting: most of the overparametrization necessary for the optimization to converge is due to $k$.\\[.25em]

\item On our way to prove \eqref{thm:main}, we actually show that $\yv(t)$ converges exponentially to $\yv$, which is converted to a recovery of $\yvc$ through an early stopping strategy. This ensures that the network does not overfit the noise and provides a solution in a ball around $\yvc$ whose radius is linear in the noise level (so-called prediction linear convergence in the inverse problem literature). This provides a first result on convergence of wide DIP networks that ensures they behave well in the observation space. \\[.25em]

\item One has to keep in mind, however, that Theorem~\ref{thm:main} does not say anything about the recovered vector generated by the network and its relation to $\xvc$ (in absence of noise and at convergence, it might be any element of $\xvc + \text{ker}(\fop)$). Of course, when $\fop$ is invertible, then we are done. In the general case, this is a much more challenging question which requires a more involved analysis and a restricted-type injectivity assumption. This will be the subject of a forthcoming paper.
\end{enumerate}
%\end{remark}

%Furthermore, the theorem informs us that once the overparametrization bounds are respected, the network can achieve a perfect match in the observation space between its transformed output and its target observation with an exponential convergence rate. This shows that under the right condition, Deep Inverse Priors models, even with a shallow network, are able to find a signal that will correspond in the measurement space to the target which is the task they are trained for. Of course, it is important to remember that this means that the network will overfit the noise and thus, we are confident that the signal will be matched up to an error depending on the norm of the noise.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof}
The proof consists of two main steps. First, we prove that under \eqref{eq:bndR}, $\yv(t)$ converges exponentially fast with a time-independent rate. We then use a triangle inequality and an early stopping criterion to show our result. The proof of the second claim will consist in verifying that \eqref{eq:bndR} holds with high probability for our random model of the two-layer network under our scaling. Both proofs rely on several technical lemmas, which will be given later.

\begin{enumerate}[label=(\roman*)]
\item The solution trajectory $\thetav(t)$, hence $\yv(t)$, is continuously differentiable, and thus
\begin{align}\label{eq:maindiffineq}
&\deriv{\frac{1}{2}\norm{\yty}^2}{t} 
= \ytypar\dot{\yv}(t)  \nonumber\\
&= \ytypar \fop \jthetat \dot{\thetav}(t)\nonumber\\
&= - \ytypar \fop \jthetat \nabla_{\thetav} \loss(\gdipt)\nonumber\\
&= - \frac{1}{m}\ytypar\tp \fop \jthetat \jthetat\tp \fop\tp \ytypar \nonumber\\
&= - \frac{1}{m}\norm{\jthetat\tp \fop\tp \ytypar}^2 \leq - \frac{\sigmin(\jthetat)^2\sigminA^2}{m} \norm{\yty}^2,
\end{align}
where we used the fact that $\yty \in \ran{\fop}$. In view of Lemma~\ref{lemma:link_params_singvals}\ref{claim:sigval_bounded_everywhere}, we have $\sigmin(\jthetat) \geq \sigmin(\jthetaz)/2$ for all $t \geq 0$ if the initialization error verifies~\eqref{eq:bndR}, and in turn 
\begin{align*}
\deriv{\norm{\yty}^2}{t} 
&\leq - \frac{\sigmin(\jthetaz)^2\sigminA^2}{2m} \norm{\yty}^2 .
\end{align*}
Integrating, we obtain 
\begin{align}\label{eq:expoconvy}
\norm{\yty} \leq \norm{\yvz-\yv} e^{-\tfrac{\sigmin(\jthetaz)^2\sigminA^2}{4m}t} .
\end{align}
Using 
\[
\norm{\yty} \leq \norm{\yvt - \yvc} + \norm{\epsilon} \leq \norm{\yvz-\yv} e^{-\tfrac{\sigmin(\jthetaz)^2\sigminA^2}{4m}t} + \norm{\epsilon}, 
\]
we get the early stopping bound by bounding the exponential term by $\norm{\epsilon}$.


\item To show the statement, it is sufficient to check that \eqref{eq:bndR} holds under our scaling. From Lemma~\ref{lemma:min_eigenvalue_singvalue_init}, we have
\begin{align*}
\sigmin(\jthetaz) \geq \Cphid/2 
\end{align*}
with probability at least $1-n^{-1}$ provided $k \geq C_0 n\log(n)$ for $C_0 > 0$. Combining this with Lemma~\ref{lemma:lip-jacobian} and Lemma~\ref{lemma:bound_initial_misfit} and the union bound, it is sufficient for \eqref{eq:bndR} to be fulfilled with probability at least $1-n^{-1}-d^{-1}$, that
\[
C_1\kappa(\fop)\pa{\sqrt{n}\pa{\sqrt{\log(d)}+1} + \sqrt{m}} < \frac{\Cphid^2\sqrt{k}}{16BD \sqrt{n}} .
\]
%By using Lemma~\ref{lemma:bound_init_error_with_k} we obtain that the error is indeed fulfilling this property when the network is overparametrized as prescribed. 
\end{enumerate}


We now prove the intermediate lemmas invoked in the proof. 
% Let's first observe the following:
% \begin{align}
%      \deriv{\frac{1}{2}\norm{\ytypar}^2}{t} &= - \ytypar\tp \fop \jthetat \jthetat\tp \fop\tp \ytypar \\
%             &= - <\ytypar, \fop \jthetat\jthetat\tp \fop\tp \ytypar>\\
%             & \leq - \sigmin^2(\fop\jthetat) \norm{\ytypar}^2.
% \end{align}

% We see from this that it would be very interesting to bound   $\sigmin^2(\fop\jthetat)$ away from 0 as it would almost directly gives an exponential convergence rate to a zero-loss solution. in order to control $\sigmin^2(\fop\jthetat)$, we first prove that it is bounded at initialization for $t=0$ and we show that we can extend this for all $t$ if the initial error of the network is bounded. This is expressed in the two following lemmas.

\begin{lemma}\label{lemma:link_params_singvals}
    \begin{enumerate}[label=(\roman*)]
        \item \label{claim:singvals_bounded_if_params_bounded} If $\thetav \in \Ball(\thetavz,R)$ with $R=\frac{\sigmin(\jthetaz)}{2\Lip(\jcal)}$, then
        \begin{align*}
            \sigmin(\jtheta) \geq \sigmin(\jthetaz)/2.
        \end{align*}
        
        \item  \label{claim:params_bounded_if_singvals_bounded} If for all $s \in [0,t] $, $\sigmin(\jthetas) \geq \frac{\sigmin(\jthetaz}{2}$, then 
            \begin{align*}
                \thetavt \in \Ball(\thetavz,R') \qquad \text{with} \qquad R' = \frac{2}{\sigminA\sigmin(\jthetaz)}\norm{\yzy}.
            \end{align*}
        
        \item \label{claim:sigval_bounded_everywhere}
        If $R'<R$, then for all $t \geq 0$, $\sigmin(\jthetat) \geq \sigmin(\jthetaz)/2$.
\end{enumerate}
     % If $\thetav \in \Ball(\thetavz,R)$ with $R=\frac{\sigmin(\jthetaz}{2L}$, then
     % \begin{align}
     %     \sigmin(\jtheta) > \sigmin(\jthetaz)/2.\label{eq:singvals_bounded_if_params_bounded}
     % \end{align}

     % Moreover, if for all $s \in [0,t] $, $\sigmin(\jthetas) \geq \frac{\sigmin(\jthetaz}{2}$, then 
     % \begin{align}
     %      \thetav \in \Ball(\thetavz,R') \qquad \text{with} \qquad R' = \frac{2\norm{\yzy}}{\sigmin(\fop)\sigmin(\jthetaz)}.\label{eq:params_bounded_if_singvals_bounded}
     % \end{align}

     % Finally, if $R'<R$, then for all $t \geq 0$, $\sigmin(\jthetat) \geq \frac{1}{2}\sigmin(\jthetaz$.
\end{lemma}
\begin{proof}
\begin{enumerate}[label=(\roman*)]
\item Since $\thetav \in \Ball(\thetavz,R)$, we have
    \begin{align*}
        \norm{\jtheta - \jthetaz} \leq \Lip(\jcal)\norm{\thetav - \thetavz} \leq \Lip(\jcal)R.
    \end{align*}
    By using that $\sigmin(A)$ is 1-Lipschitz, we obtain
    \begin{align*}
        \sigmin(\jtheta) \geq \sigmin(\jthetaz) - \norm{\jtheta - \jthetaz} \geq \frac{\sigmin(\jthetaz}{2} .
    \end{align*}

\item From \eqref{eq:maindiffineq}, we have for all $s \in [0,t]$
\begin{align*}
    \deriv{\norm{\ysy}}{s} &= - \frac{1}{m}\frac{\norm{\jthetas\tp\fop\tp(\ysy)}^2}{\norm{\ysy}}\\
    &\leq - \frac{\sigmin(\jthetaz)\sigminA}{2m}\norm{\jthetas\tp\fop\tp(\ysy)}.
\end{align*}
The Cauchy-Schwarz inequality and \eqref{eq:gradflow} imply that
\begin{align*}
\deriv{\norm{\thetavs - \thetavz}}{s} = \frac{\dot{\thetav}(s)\tp\pa{\thetavs - \thetavz}}{\norm{\thetavs - \thetavz}} \leq \norm{\dot{\thetav}(s)} = \frac{1}{m}\norm{\jthetas\tp\fop\tp(\ysy)}.
\end{align*}
We therefore get
\begin{align*}
\deriv{\norm{\thetavs - \thetavz}}{s} + \frac{2}{\sigmin(\jthetaz)\sigminA}\deriv{\norm{\ysy}}{s} \leq 0 .
\end{align*}
Integrating over $s \in [0,t]$, we get the claim.
%\begin{align}
%\int_0^t \deriv{\norm{\thetav(s) - \thetavz}}{s} \text{d}s &\leq - \frac{1}{\tau} \int_0^t \deriv{\norm{y(s) - \yv}}{s} \text{d}s \\%\nonumber\\
%    %\norm{\thetavt - \thetavz} &\leq \frac{1}{\tau} (\norm{\yty[0]} - \norm{\yty})\\
%    \norm{\thetavt - \thetavz} &\leq \frac{1}{\tau} \norm{\yty[0]}. \label{eq:bound_neighbourhood_parameters}
%\end{align}
%
%We use the fact that $\sigmin(\jthetas) \geq \frac{\sigmin(\jthetaz}{2}$ on the interval $[0,t]$ to get
%    \begin{align*}
%        \deriv{\norm{\yty[s]}}{s} &\leq - \sigmin(\fop)\sigmin(\jthetas)\norm{\deriv{\thetav(s)}{s}}\\
%        &\leq -\frac{1}{4}\sigmin(\fop)^2\sigmin(\jthetaz)^2\norm{\yty[s]}.
%    \end{align*}
%    By integration, we obtain
%    \begin{align*}
%        \norm{\yty[s]} \leq \exp(-\frac{1}{4}\sigmin(\fop)^2\sigmin(\jthetaz)^2s)\norm{\yzy}.
%    \end{align*}
%    We now apply the fundamental theorem of calculus and combine it with the previous result to get
%    \begin{align*}
%        \norm{\thetav(s) - \thetavz} &\leq \int^t_0 \norm{\deriv{\thetav(s)}{s}} ds \leq \norm{\fop}\norm{\jthetas}\int_0^t\norm{\yty[s]}\\
%        &\leq \frac{4 B \norm{\fop}\norm{\Vv}}{\sigmin(\fop)^2\sigmin(\jthetaz)^2}\norm{\yzy},
%    \end{align*}
%    where we used $\norm{\jcal} \leq B\norm{V}$ in the last inequality.
    
    % \begin{align}
    %     \deriv{\norm{\yty[s]}}{s} &\leq - \sigmin(\fop\jthetas) \norm{\deriv{\thetav(s)}{s}}\\ %\leq  -\frac{1}{2}\sigmin(\jthetaz) \norm{\deriv{\thetav(s)}{s}}.
    %     &\leq -\frac{1}{2}\sigmin(\fop)\sigmin(\jthetaz) \norm{\deriv{\thetav(s)}{s}} %\label{eq:diff_normy_geq_norm_diiftheta}\\
    %     %&\leq -\sigmin(\fop)^2\sigmin(\jthetas)^2\norm{\yty[s]}\\
    %     %&\leq -\frac{1}{2}\sigmin(\fop)^2\sigmin(\jthetaz)^2\norm{\yty[s]}.
    % \end{align}
    % We now use Cauchy-Schwarz to show that
    % \begin{align*}
    %     \norm{\deriv{\thetav(s)}{s}} = \norm{\deriv{\thetav(s) - \thetavz}{s}} \geq \deriv{\norm{\thetav(s) - \thetavz}}{s}.
    % \end{align*}
    % By combining the last two results, we get
    % \begin{align*}
    %     \deriv{\norm{\thetav(s) - \thetavz}}{s} \geq -\frac{2}{\sigmin(\fop)\sigmin(\jthetaz)}\deriv{\norm{\yty[s]}}{s}.
    % \end{align*}
    % We now integrate on both side on $[0,t]$ and we get
    % \begin{align*}
    %     \norm{\thetav(s) - \thetavz} \leq \frac{2}{\sigmin(\fop)\sigmin(\jthetaz)}\norm{\yzy},
    % \end{align*}
    % which proves (\textit{\ref{claim:params_bounded_if_singvals_bounded}}).

\item Actually, we prove the stronger statement that $\thetavt \in \Ball(\thetavz,R')$ for all $t \geq 0$, whence our claim will follow thanks to \ref{claim:singvals_bounded_if_params_bounded}. Let us assume for contradiction that $R'<R$ and $\exists~ t < +\infty$ such that $\thetavt \notin \Ball(\thetavz,R')$. By \ref{claim:params_bounded_if_singvals_bounded}, this means that $\exists~ s \leq t$ such that $\sigmin(\jthetas) < \sigmin(\jthetaz)/2$. In turn, \ref{claim:singvals_bounded_if_params_bounded} implies that $\thetav(s) \notin \Ball(\thetavz,R)$. Let us define
    \begin{align*}
        t_0 = \inf\{\tau \geq 0:\thetav(\tau) \notin \Ball(\thetavz,R)\},
    \end{align*}
    which is well-defined as it is at most $s$. Thus, for any small $\epsilon > 0$ and for all $t' \leq t_0 - \epsilon$, $\thetav(t') \in \Ball(\thetavz,R)$ which, in view of \ref{claim:singvals_bounded_if_params_bounded} entails that $\sigmin(\jtheta(t')) \geq \sigmin(\jthetaz)/2$. In turn, we get from \ref{claim:params_bounded_if_singvals_bounded} that $\thetav(t_0-\epsilon) \in \Ball(\thetavz,R')$. Since $\epsilon$ is arbitrary and $\thetav$ is continuous, we pass to the limit as $\epsilon \to 0$ to deduce that $\thetav(t_0) \in \Ball(\thetavz,R') \subsetneq \Ball(\thetavz,R)$ hence contradicting the definition of $t_0$.
    %which means because of (\textit{\ref{claim:singvals_bounded_if_params_bounded}}) and the continuity of $\sigmin(\cdot)$ that for all $t'\in [0,t_0]$, $\sigmin(\jcal(\thetav(t')) \geq \frac{1}{2}\sigmin(\jthetaz)$. Finally, by applying (\textit{\ref{claim:params_bounded_if_singvals_bounded}}), we get $\thetav(t_0) \in \Ball(\thetavz,R')$ which is a contradiction by the definition of $t_0$.
\qed
\end{enumerate}
\end{proof}

\begin{lemma}[Bound on $\sigmin(\jthetaz)$]
\label{lemma:min_eigenvalue_singvalue_init}
For the one-hidden layer network \eqref{eq:dipntk}, under assumptions \eqref{ass:u_sphere}-\eqref{ass:phi_diff}. We have
\begin{align*}
\sigmin(\jthetaz) \geq \Cphid/2 
\end{align*}
%with probability $1 - 2 \exp{(-c\tau^2)}$. Here, $C$, $c > 0$ depend only on the sub-gaussian norm $\normsubgauss{\phidW_1 \Vv_1}$ 
with probability at least $1-n^{-1}$ provided $k \geq C n\log(n)$ for $C > 0$ large enough that depends only on $\phi$ and the bound on the  entries of $\Vv$.
\end{lemma}

\begin{proof}
Define the matrix $\Hv = \jthetaz\jthetaz\tp$. For the two-layer network, and since $\uv$ is on the unit sphere, $\Hv$ reads
\[
\Hv = \frac{1}{k} \sum_{i=1}^k \phi'(\Wv^i(0)\uv)^2\Vv_i\Vv_i\tp .
\]
It follows that
\[
\Expect{}{\Hv} = \Expect{g\sim \stddistrib}{\phi'(g)^2}\frac{1}{k} \sum_{i=1}^k\Expect{}{\Vv_i \Vv_i\tp} = \Cphid^2 \Id_n ,
\]
where we used \ref{ass:u_sphere}-\ref{ass:w_init} and orthogonal invariance of the Gaussian distribution, hence $\Wv^i(0)\uv$ are iid $\stddistrib$, as well as \ref{ass:v_init} and independence between $\Vv$ and $\Wv(0)$. Moreover,
\[
\lammax(\phi'(\Wv^i(0)\uv)^2\Vv_i\Vv_i\tp) \leq B^2 D^2 n .
\]
We can then apply the matrix Chernoff inequality \cite[Theorem~5.1.1]{tropp_introduction_2015} to get
\[
\prob{\sigmin(\jthetaz) \leq \delta\Cphid} \leq ne^{-\frac{(1-\delta)^2k\Cphid^2}{2B^2 D^2 n}} .
\]
Taking $\delta=1/2$ and $k$ as prescribed, we conclude.
%We first show that the Jacobian at initialization has, up to a scaling, sub-gaussian independent and isotropic rows. From this, we deduce the claimed lower bound. We first observe that 
%\begin{align*}
%    \sigmin(\jthetaz) = \sigmin(\diag{\phidW}\Vv) .
%\end{align*}
%%where $\Wv(0)$ has iid $\stddistrib$ entries according to \ref{ass:w_init}.
%We now see that
%\begin{align*}
%    \Expect{}{(\phidW_i \Vv_i)(\phidW_i \Vv_i)\tp} &= \Expect{g\sim \stddistrib}{\phi'(g)^2}\Expect{}{\Vv_i \Vv_i\tp} = \frac{\Cphid^2}{k} \Id ,
%\end{align*}
%where we used \ref{ass:u_sphere}-\ref{ass:w_init} and orthogonal invariance of the Gaussian distribution, hence $\Wv_i(0)\uv$ are iid $\stddistrib$, as well as independence between $\Vv$ and $\Wv(0)$ and that $\Vv_{i,j}$ are iid $\mathcal{N}(0,1/k)$. 
%Furthermore, by \ref{ass:phi_diff}, $\sup_{u \in \R}|\phi'(u)| \leq B$, and one immediately checks that $\frac{1}{\Cphid}\diag{\phidW}\Vv$ has sub-gaussian rows. They are also independent since independence of the $\Wv(0)_i\uv$'s entails that of the $\phidW_i$'s and in turn of the $\phidW_i\Vv_i$'s. We then invoke \cite[Theorem~5.39]{vershynin_introduction_2012} and obtain
%\begin{align*}
%    \sigmin\left(\frac{\sqrt{k}}{\Cphid}\diag{\phidW}\Vv\right) \geq \sqrt{k} - C\sqrt{n} - \tau
%\end{align*}
%with probability at least $1 - 2\exp(-c\tau^2)$ where $C$ and $c$ are positive constants.
%Injectivity of $\fop^*$ (see \ref{ass:surjective_op}) then yields that with the same probability
%\begin{align*}
%    \sigmin(\AJz) \geq \Cphid\sigmin(\fop)(\sqrt{k} - C\sqrt{n} - \tau).
%\end{align*}
\qed
\end{proof}

% lemma Lipschitz constant
\begin{lemma}[Lipschitz constant of the Jacobian]\label{lemma:lip-jacobian}
For the one-hidden layer network \eqref{eq:dipntk}, under assumptions \eqref{ass:u_sphere}, \eqref{ass:w_init} and \eqref{ass:phi_diff}, we have
\[
\Lip(\jcal) \leq B D \sqrt{\frac{n}{k}} .
\]
%\begin{align*}
%\norm{\jW - \jWalt} \leq B D \sqrt{\frac{n}{k}} \normf{\Wv - \Wvalt} , \qquad \forall~ \Wv, \Wvalt \in \R^{k \times d} .
%\end{align*}
\end{lemma}

\begin{proof}
We have for all $\Wv, \Wvalt \in \R^{k \times d}$, 
\begin{align*}
\norm{\jW - \jWalt}^2 
&\leq \frac{1}{k} \sum_{i=1}^k |\phi'(\Wv^i\uv) - \phi'(\Wvalt^i\uv)|^2 \normf{\Vv_i\uv\tp}^2 \\
&=\frac{1}{k} \sum_{i=1}^k |\phi'(\Wv^i\uv) - \phi'(\Wvalt^i\uv)|^2 \norm{\Vv_i}^2 \\
&\leq B^2D^2\frac{n}{k} \sum_{i=1}^k |\Wv^i\uv - \Wvalt^i\uv|^2 \\
&\leq B^2D^2\frac{n}{k} \sum_{i=1}^k \norm{\Wv^i - \Wvalt^i}^2 = B^2D^2\frac{n}{k} \normf{\Wv - \Wvalt}^2 .
\end{align*}
\qed
\end{proof}

\begin{lemma}[Bound on the initial error]\label{lemma:bound_initial_misfit}
Under the main assumptions, the initial error of the network is bounded by
\begin{align*}
\norm{\yv(0) - \yv} \leq \norm{\fop}\pa{C \sqrt{n\log(d)} + \sqrt{n}\norminf{\xvz} + \sqrt{m}\norminf{\epsilon}},
\end{align*}
with probability at least $1 - d^{-1}$.
\end{lemma}

\begin{proof}
We first observe that
\[
\norm{\yvz - \yv} \leq \norm{\fop}\norm{\gdipz} + \norm{\fop}\pa{\sqrt{n} \norminf{\xvz} + \sqrt{m} \norminf{\epsilon}} ,
\]
where $\gdipz = \frac{1}{\sqrt{k}} \sum_{i=1}^k \phi(\Wv^i\uv)\Vv_i$. We now prove that this term concentrates around its expectation. First, we have by independence
\[
\Expect{}{\norm{\gdipz}}^2 \leq \frac{1}{k}\Expect{}{\norm{\sum_{i=1}^k \phi(\Wv^i\uv)\Vv_i}^2} = \Expect{}{\phi(\Wv^1\uv)^2\norm{\Vv_1}^2} = \Cphi^2 n .
\]
In addition,
\begin{align*}
&\abs{\norm{\mathbf{g}(\uv,\Wv)} - \lVert\mathbf{g}(\uv,\Wvalt)\rVert} 
\leq \frac{1}{\sqrt{k}}\norm{\sum_{i=1}^k \pa{\phi(\Wv^i\uv) - \phi(\Wvalt^i\uv)}\Vv_i} \\
&\leq BD\sqrt{n}\pa{\frac{1}{\sqrt{k}}\sum_{i=1}^k \norm{\Wv^i - \Wvalt^i}} \leq BD\sqrt{n}\normf{\Wv - \Wvalt} .
\end{align*}
We then get
\begin{align*}
&\prob{\norm{\mathbf{g}(\uv,\Wv(0))} \geq \Cphi \sqrt{n\log(d)} + \tau} \\
&\leq \prob{\norm{\mathbf{g}(\uv,\Wv(0))} \geq \Expect{}{\norm{\gdipz}} + \tau}\\ 
&\leq e^{-\frac{\tau^2}{2\Lip(\norm{\mathbf{g}(\uv,\Wv(0))})^2}} \leq e^{-\frac{\tau^2}{2nB^2D^2}} .
\end{align*}
Taking $\tau = \sqrt{2}BD \sqrt{n\log(d)}$, we get the desired claim. 
\qed
\end{proof}

%\begin{lemma}\label{lemma:bound_init_error_with_k}
%    Under the main assumptions, if the network architecture is bounded by
%    \begin{align}
%        \frac{\sqrt{d}}{\sqrt{\log(d)+\tau_4}} &\geq \frac{(1+(1+\tau_2)\Cphi\normf{\fop})4\Cphid^2 B \normf{\fop}}{\sqrt{c_{\uv}}\sigmin(\fop)^2}
%        \qquad \text{and} \label{eq:constraint_d}\\
%        \sqrt{k} &\geq C \norm{\fop} \sqrt{mn},
%    \end{align}
%    Then, the initial misfit of the network is bounded such that 
%    %where $C$ and $c$ are positive constants depending on $\normsubgauss{\sqrt{d}\uv}$, $\norm{y}$, $\tau_1$, $\tau_3$ and $\tau_4$. Then the initial misfit of the network is bounded such that
%    \begin{align*}
%        \norm{\yvz - \yv} \leq \frac{\sigmin(\AJz)^2}{4\norm{\fop}L}
%    \end{align*}
%with probability 
%    \begin{align*}
%        1 - 2\exp(-\tau_1^2/2) - \exp{\left(- \tau_2^2\frac{\Cphi^2 k m}{4 (\sqrt{k} + \sqrt{n} + \tau_1)^2 B^2}\right)} - 2 \exp{(-c\tau_3^2)} - 2\exp(-\tau_4),
%    \end{align*}
%    where $C$, $c$, $c_{\uv}$, $\tau_1$, $\tau_2$, $\tau_3$ and $\tau_4$ are positive constant.
%\end{lemma}
%
%
%\begin{proof}
%    We first use the deviations of gaussian matrices (Corollary 5.35 in \cite{vershynin_introduction_2012}) to estimate the norm of $\Vv$ and obtain that with probability $1 - 2\exp(-\tau_1^2/2)$
%    \begin{align*}
%        (\sqrt{k} - \sqrt{n} - \tau_1) \leq \sigmin(\Vv) \leq \norm{\Vv} \leq (\sqrt{k} + \sqrt{n} + \tau_1).
%    \end{align*}
%    We now want to ensure that the error at initialization as given in Lemma~\ref{lemma:bound_initial_misfit} match the bound given by Lemma~\ref{lemma:bounds_param_under_init_misfit} , that is,  $\norm{\yvt - \yv} \leq \sigmin(\AJz)/(4\norm{\fop}L)$. To do so, let's define $\alpha = \sigmin(\AJz)$ and use Lemma~\ref{lemma:min_eigenvalue_singvalue_init} to get its value, and let us use Lemma~\ref{lemma:lip-jacobian} and Lemma~\ref{lemma:inf_norm_unit_vector} to obtain the following inequality to solve
%    \begin{align*}
%        \norm{\yv} + (1 + \tau_2)\Cphi \normf{\fop} \sqrt{k} \leq\frac{\sqrt{d}}{\sqrt{\log(d)+\tau_4}} \frac{\sqrt{c_{\uv}} \sigmin(\fop)^2 (\sqrt{k} - C\sqrt{n} - \tau_3)^2}{4 \Cphid^2 \norm{\fop} B  (\sqrt{k} + \sqrt{n} + \tau_1)},
%    \end{align*}
%    according to the probabilities of the different lemmas.

    %  We can now use Lemma~\ref{lemma:bound_initial_misfit} which bounds the initial error of the network such that
    % \begin{align}
    %     \norm{\fop\Vv\tp\phiWu - \yv} 
    %     &\leq \norm{y} + (1+\tau_2)\Cphi \sqrt{km} \norm{\fop}
    % \end{align}
    % with probability $1 - \exp{\left(- \tau_2^2\frac{\Cphi^2 k m}{4 (\sqrt{k} + \sqrt{n} + \tau_1)^2 B^2}\right)}$.
    
    % We also know from Lemma~\ref{lemma:min_eigenvalue_singvalue_init} and  the combination of Lemma~\ref{lemma:lip-jacobian} and Lemma~\ref{lemma:inf_norm_unit_vector} (presented in the appendix) that:
    % \begin{align}
    %     &\alpha^2 = \sigmin(\AJz)^2 \geq \frac{1}{\Cphid^2}\sigmin(\fop)^2(\sqrt{k} - C\sqrt{n} - \tau_3)^2\\
    %     &L = B \norm{\Vv} \norminf{\uv} \leq B (\sqrt{k} + C\sqrt{n} + \tau_1) \sqrt{\frac{\log(d) + \tau_4}{c_{\uv}}} \frac{1}{\sqrt{d}}
    % \end{align}
    % with probability $1 - 2 \exp{(-c\tau_3^2)}$ and $1 - 2\exp(-\tau_4)$ respectively. This means that with said probability, we have
    % \begin{align}
    %     \frac{\alpha^2}{4\norm{\fop}L} \geq \frac{\sqrt{d}}{\sqrt{\log(d)+\tau_4}} \frac{\sqrt{c_{\uv}} \sigmin(\fop)^2 (\sqrt{k} - C\sqrt{n} - \tau_3)^2}{4 \Cphid^2 \norm{\fop} B  (\sqrt{k} + \sqrt{n} + \tau_1)}.
    % \end{align}
    
    % In order for the lemma to hold, we now need to make our two bounds coincide such that
    % \begin{align}
    %     \norm{\yv} + (1 + \tau_2)\Cphi \normf{\fop} \sqrt{k} \leq\frac{\sqrt{d}}{\sqrt{\log(d)+\tau_4}} \frac{\sqrt{c_{\uv}} \sigmin(\fop)^2 (\sqrt{k} - C\sqrt{n} - \tau_3)^2}{4 \Cphid^2 \norm{\fop} B  (\sqrt{k} + \sqrt{n} + \tau_1)}.
    % \end{align}
    
   % We define $\psi = \sqrt{c_{\uv}}/(4\Cphid^2 B)$, $\Phi = (1+\tau_2)\Cphi$ and $\rfop = \frac{\sigmin(\fop)^2}{\norm{\fop}}$. We can rearrange the equation and use (\ref{eq:constraint_d}) to obtain the equation
%    We define $\Phi = (1+\tau_2)\Cphi$. We can rearrange the equation and use (\ref{eq:constraint_d}) to obtain the equation
%    \begin{multline*}
%        k - \sqrt{k}(\norm{\yv} + \Phi\normf{\fop}(\sqrt{n}(1 + 2C) + \tau_1 + \tau_3) + 2(C\sqrt{n} + \tau_3))\\ - \norm{\yv} (\sqrt{n} + \tau_1) + \Phi\normf{\fop}(C\sqrt{n} + \tau_3)^2 \geq 0. 
%    \end{multline*}
%    % \begin{multline}
%    %     k(\frac{\sqrt{d}}{\sqrt{\log(d)+\tau_4}} \psi\rfop - \Phi\normf{\fop}) \\- \sqrt{k}(\norm{\yv} + \Phi\normf{\fop}(\sqrt{n}+\tau_1) + 2 \frac{\sqrt{d}}{\sqrt{\log(d)+\tau_4}}\psi\rfop (C\sqrt{n} + \tau_3))\\ - \norm{\yv}(\sqrt{n} + \tau_1) + \frac{\sqrt{d}}{\sqrt{\log(d)+\tau_4}}\psi\rfop(C\sqrt{n} + \tau_3)^2 \geq 0.
%    % \end{multline}
%    % By using the assumption on $\sqrt{d}/{\sqrt{\log(d)+\epsilon}} $, we can simplify the problem to
%    % \begin{multline}
%    %     k - \sqrt{k}(\norm{\yv} + \Phi\normf{\fop}(\sqrt{n}(1 + 2C) + \tau_1 + \tau_3) + 2(C\sqrt{n} + \tau_3))\\ - \norm{\yv} (\sqrt{n} + \tau_1) + \Phi\normf{\fop}(C\sqrt{n} + \tau_3)^2 \geq 0. 
%    % \end{multline}
%    We can solve the polynomial and obtain that asymptotically we need $\sqrt{k} \geq C\norm{\fop}\sqrt{mn}$,
%    where we used that $\normf{\fop} \leq \sqrt{m}\norm{\fop}$ and with $C$ a positive constant depending on $\normsubgauss{\sqrt{d}\uv}$, $\norm{y}$, $\tau_1$, $\tau_2$, $\tau_3$, $\tau_4$ and $\Cphi$.
%\qed
%\end{proof}

%In order to bound the initial error of the network, we prove in the first lemma thereafter that this error is bounded by a function depending on $\fop$ and $k$, then we provide in the second lemma thereafter the constraints on the network architecture parameters that allows to control this bound such that it match the one needed to lower bound $\sigmin(\AJt)$.



%With these lemmas in place, we obtain that $\sigmin(\AJt) \geq \frac{\sigmin(\AJz)}{2}$ $\forall t$, which means that by Gr\"onwall lemma:
%\begin{align*}
%    \norm{\yty}^2 \leq \norm{\yvz - \yv}^2 \exp\left(-\frac{\sigmin(\AJz)^2}{4}t\right)
%\end{align*}
%We now take into account the noise and see that
%\begin{align*}
%    \norm{\yvt - \yvc} &\leq \norm{\yvt - \yv} + \norm{\epsilon} \leq 2\norm{\epsilon}
%\end{align*}
%when $t \geq \frac{8\log\left(\norm{\yvz - \yv}/\norm{\epsilon}\right)}{\sigmin(\AJz)^2}$. We now present the technical lemmas.
%
%
%\begin{lemma}\label{lemma:bound_initial_misfit}
%    Under the main assumptions and the constraint that
%    \begin{align*}
%        \norm{\Vv} \leq (\sqrt{k} + \sqrt{n} + \tau_1),\label{eq:sigmin_V_bounded}
%    \end{align*}
%    then, the initial misfit of the network is bounded by
%    \begin{align*}
%        \norm{\fop\Vv\tp\phiWu - \yv} \leq \norm{y} + (1+\tau)\Cphi \sqrt{km} \norm{\fop}
%    \end{align*}
%    with probability $1 - \exp{\left(- \tau^2\frac{\Cphi^2 k m}{4 (\sqrt{k} + \sqrt{n} + \tau_1)^2 B^2}\right)}$.
%    
%\end{lemma}
%
%\begin{proof}
%    % First the Lip prop of AV\tp \phiWu
%    We first note that if $\Wv,\Wvalt \in \R^{k\times d}$ then 
%    \begin{align*}
%        \norm{\fop\Vv\tp\phiWu - \fop\Vv\tp\phiWualt} &\leq \norm{\fop} (\sqrt{k} + \sqrt{n} + \tau_1) B \norm{\Wv - \Wvalt},
%    \end{align*}
%    where we used the mean value theorem and $\sup_{z} |\phi'(z)| \leq B$ as well as (\ref{eq:sigmin_V_bounded}). It is easy to see that this Lipschitz constant will be bigger than the one w.r.t $\Vv$. This indicates that $\norm{\fop\Vv\tp\phiWu}$ is $(\sqrt{2}\norm{\fop} (\sqrt{k} + \sqrt{n} + \tau_1) B)$-Lipschitz with respect to the joint variable ($\Wv$, $\Vv$).
%    We first upper bound the expectation of this norm and then use a gaussian concentration inequality to ensures that the actual values will stay close to it. By Jensen inequality 
%    
%    \begin{align*}
%        \E_{\Wv,\Vv}[\norm{\fop\Vv\tp\phiWu}] &\leq \sqrt{\sum_{i=1}^{n} \E_{\Wv,\Vv}[\dotprod{\AVi}{\phiWu}^2]}\\
%        &= \sqrt{\sum_{i = 1}^{m} \sum_{j = 1}^{k} \sum_{q = 1}^{k} \E_{\Wv,\Vv}[\AVij\AViq\phiWu_j\phiWu_q]}.
%    \end{align*}
%    
%    % We use the fact that $\Wv$ and $\Vv$ are independent to obtain that
%    % \begin{align*}
%    % \Expect{\Wv,\Vv}{\norm{\fop\Vv\tp\phiWu}} \leq \sqrt{\sum_{i = 1}^{m} \sum_{j = 1}^{k} \sum_{q = 1}^{k} \Expect{\Vv}{\AVij\AViq} \Expect{\Wv}{\phiWu_j\phiWu_q}}
%    % \end{align*}
%    
%    We can now use the independence between $\AVij$ and $\AViq$ when $j \neq q$ as well as the independence between $\Vv$ and $\Wv$ alongside the fact that $\E_{\Vv}[\AVij] = 0$ $\forall i,j$ to see 
%    \begin{align*}
%        \E_{\Wv,\Vv}[\norm{\fop\Vv\tp\phiWu}] &\leq %\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{k} \E_{\Vv}[\AVij^2] \E_{g \sim \stddistrib}[\phi(g)^2]}\\
%        %&= \Cphi \sqrt{k} \normf{\fop}\\
%         \Cphi \sqrt{km} \norm{\fop}.
%    \end{align*}
%    We now use Theorem 5.6 in \cite{boucheron_concentration_2013} with a deviation $\epsilon = \tau \Cphi \sqrt{km} \norm{\fop}$ to obtain
%    \begin{align*}
%        \norm{\fop\Vv\tp\phiWu - \yv} \leq \sqrt{m}\norm{\yv}_{\infty} + (1+\tau)\Cphi \sqrt{km} \norm{\fop}
%    \end{align*}
%    with probability $1 - \exp{\left(- \tau^2\frac{\Cphi^2 k m}{4 (\sqrt{k} + \sqrt{n} + \tau_1)^2 B^2}\right)}$.
%\qed
%\end{proof}
%
%
%
%% We bound the initial error
%% The next step is to bound the initial error of the network such that
%% \begin{align}
%%     \sigmin(\AJt) \geq \frac{\sigmin(\AJz)}{2}.
%% \end{align}
%% which is attained, by Lemma~\ref{lemma:bounds_param_under_init_misfit}, if $\norm{\yvt - \yv} \leq \sigmin(\AJz)/(4\norm{\fop}L)$ where $L$ is the Lipschitz constant of the network. We provide such constant in the next lemma.
%
%
%
%\begin{proof}
%% Let's recall the following:
%
%% % A CHANGER
%% \begin{align}
%%     &\loss(\uv, \thetav) = \frac{1}{2} \norm{\fop g(\uv, \thetav) - \yv}^2, \\
%%     &\deriv{\thetavt}{t} = - \nabla_{\thetavt}\loss(\uv, \thetav) = -\jW\tp\fop\tp \ytypar,\\
%%     &\deriv{\yvt}{t} = - \fop\jW\jW\tp\fop\tp \ytypar.
%% \end{align}
%
%Let's observe that:
%\begin{align*}
%    \deriv{\norm{\yty}}{t} %-\frac{\ytypar\tp\fop\jW\jW\tp\fop\tp \ytypar}{\norm{\yty}}\\
%    %&= - \frac{\norm{\jW\tp\fop\tp(\ytypar}^2}{\norm{\yty}}\\
%    &\leq - \sigmin(\AJt)\norm{\jW\tp\fop\tp\ytypar}.
%\end{align*}
%
%Using Cauchy-Schwarz we obtain that:
%\begin{align*}
%    \deriv{\norm{\yty}}{t} + \tau\deriv{\norm{\thetavt - \thetavz}}{t} \leq \deriv{\norm{\yty}}{t} + \tau\norm{\deriv{(\thetavt - \thetavz)}{t}} \leq 0,
%\end{align*}
%with $\tau \in ]0,\sigmin(\AJt)]$. If we now integrate this result:
%\begin{align}
%    \int_0^t \deriv{\norm{\thetav(s) - \thetavz}}{s} \text{d}s &\leq - \frac{1}{\tau} \int_ 0^t \deriv{\norm{y(s) - \yv}}{s} \text{d}s \\%\nonumber\\
%    %\norm{\thetavt - \thetavz} &\leq \frac{1}{\tau} (\norm{\yty[0]} - \norm{\yty})\\
%    \norm{\thetavt - \thetavz} &\leq \frac{1}{\tau} \norm{\yty[0]}. \label{eq:bound_neighbourhood_parameters}
%\end{align}
%\qed
%\end{proof}
%
%
%
%
%\begin{lemma}[Concentration of the infinity norm for random unit vector]\label{lemma:inf_norm_unit_vector}
%Let $\uv$ be a random vector uniformly distributed on the unit Euclidean sphere in $\R^d$ with center at the origin. Then, for any $\tau \geq 0$
%\begin{align*}
%    \prob{\norminf{\uv} \geq \sqrt{\frac{\log(d) + \tau}{c}} \frac{1}{\sqrt{d}}} \leq 2 \exp(-\tau), 
%\end{align*}
%where $c = \normsubgauss{\sqrt{d}\uv}$.
%\end{lemma}
%\begin{proof}
%By theorem 3.4.6 in \cite{vershynin_high-dimensional_2018}, we know that $\sqrt{d}\uv$ is $c$-sub-gaussian, thus, for every $i$ in $\{1,...,d\}$
%\begin{align*}
%    \prob{\abs{\uv_i} \geq \frac{\epsilon}{\sqrt{d}}} \leq 2\exp(-c\epsilon^2).
%\end{align*}
%
%We now take the union bound and observe that
%\begin{align*}
%    \prob{\norminf{\uv} \geq \frac{\epsilon}{\sqrt{d}}} %&\leq \sum_{i=1}^d \prob{\abs{\uv_i} \geq \frac{\epsilon}{\sqrt{d}}}\\
%    &\leq 2d\exp(-c\epsilon^2).
%\end{align*}
%
%If we take $\epsilon = \sqrt{\log(d) + \tau}/\sqrt{c}$, we have
%\begin{align*}
%    \prob{\norminf{\uv} \geq \sqrt{\frac{\log(d) + \tau}{c}} \frac{1}{\sqrt{d}}} \leq 2 \exp(-\tau).
%\end{align*}
%\qed
%\end{proof}


\section{Numerical Experiments}\label{sec:expes}

% 1 to 1.5 pages
% Keep it as short as possible.

% First part is the setup so :
% phi sigmoid
% Gradient descent with stepsize of 1 and with no decay over time
% Experience stopped if threshold attained or 25000 iterations
% 

We conducted numerical experiments to verify our theoretical finding, by evaluating the convergence of networks with different architecture parameters in the noise-free context. Every network was initialized in accordance with the assumptions of our work and we used the sigmoid activation function. Both $\fop$ and $\xvc$ entries were drawn i.i.d from $\stddistrib$. We used gradient descent to optimize the networks with a fixed step size of 1. A network was trained until it reached a loss of $10^{-7}$ or after 25000 optimization steps. For each set of architecture parameters, we did 50 runs and calculated the frequency at which the network arrived at the error threshold of $10^{-7}$. 



\begin{figure}[htb!]
    \centering
    \includegraphics[width=.7\linewidth]{Plots/kvn.png}
    \caption{Probability of arriving at a zero loss solution for networks with fixed number of observations $m$, yet varying number of neurons $k$ and signal size~$n$. This emphasizes that the required level of over-parametrization scales at least quadatically with $n$.}
    \label{fig:plot_1}
\end{figure}


We present in Figure~\ref{fig:plot_1} a first experiment where we fix the number of observations $m=10$ and the input size $d=500$, and we let the number of neurons~$k$ and the signal size $n$ vary. It can be observed in this experiment that for any value of $n$, a zero-loss solution is reached with high probability as long as $k$ is ``large enough'', where the phase transition seems to follow a quadratic law. Given that in this setup $n \gg m$ and $\fop$ is Gaussian, this empirical observation is consistent with the theoretical quadratic relation $k \gtrsim n^2m$ which is predicted by our main theorem. However, one may be surprised by the wide range of values of $n$ which can be handled with a fixed $k$. Consider for instance the case $k=900$: convergence is attained for values of $n$ up to $3000$, which includes cases where $k < n$. This goes against our intuition for such underparametrized cases. 

\begin{figure}[htb!]
    \centering
    \includegraphics[width=.7\linewidth]{Plots/kvm.png}
    \caption{Probability of arriving at a zero loss solution for networks with fixed signal size $n$, and varying number of neurons $k$ and of observations $m$. This emphasizes that the required level of over-parametrization scales linearly with $m$.}
    \label{fig:plot_2}
\end{figure}

Figure~\ref{fig:plot_2} presents a second experiment, where we now fix $n=60$ (still with $d=500$), while letting $k$ vary with $m$. Therein, the expected linear relation between $k$ and $m$ clearly appears, which provides another empirical validation for our theoretical bound. Now, let us consider again a fixed level of over-parametrization, e.g., $k=900$. Contrarily to the previous experiment on the signal size $n$, the range of observations number $m$ which can be tackled is more restricted (here, convergence is observed for values of $m$ up to $25$). For problems where the ratio $m/n$ largely deviates from zero, the level of required over-parametrization is thus much more important than for problems where $n \gg m$. Overall, these experiments validate the order of magnitude of our theoretical bounds, although they also emphasize that these bounds are not really tight.

%\begin{figure}[htb!]
%    \begin{subfigure}{.5\textwidth}
%        \centering
%        \includegraphics[width=.9\linewidth]{Plots/kvn.png}
%        \caption{$k$ vs $n$}
%        \label{fig:heat_s1}
%    \end{subfigure}%
%    \begin{subfigure}{.5\textwidth}
%        \centering
%        \includegraphics[width=.9\linewidth]{Plots/kvm.png}
%        \caption{$k$ vs $m$}
%        \label{fig:heat_s2}
%    \end{subfigure}%
%    \caption{Probability of arriving at a zero loss solution for networks with different architecture parameters. This emphasizes that $k$ must scale linearly with $m$.}
%    \label{fig:plots_heatmaps}
%\end{figure}

% Paragraphe sur la figure 2a, k et n quadratic, spectral norm de l'ordre sqrt(n)
%We observe in Figure~\ref{fig:heat_s1} the relationship between $k$ and $n$ for a fixed $m$. In this setup where $n \gg m$ and $\fop$ is Gaussian, we expect, by our main theorem, a quadratic relationship which seems to be the case in the plot. It is however surprising that with values of $k$ restricted to the range $[20,1000]$, the network converges to a zero-loss solution with high probability for situations where $n > k$ which goes against our intuition for these underparametrized cases.

%Additionally, the observation of Figure~\ref{fig:heat_s2} provides a very different picture when the ratio $m/n$ increases from 0. We first see clearly the expected linear relation between $k$ and $m$. However, contrary to Figure~\ref{fig:heat_s1} where we observed convergence for high values of $n$ (up to 3000) with a fixed $m$, this new setting where $m$ grows gives very different results. For the same range of values for $k$, the networks only converge for small values of $n$ (60 in this case). This indicates that the ratio $m/n$ plays an important role in the level of overparametrization necessary for the network to converge. It is clear from these results that the order of our bounds is consistent with experiments, yet our bounds are not really tight, as we observe convergence for lower values of $k$ than expected.

%However, we used in this experiment $n=60$ and we can see that for the same range of values of $k$, the method has much more difficulty to converge with already small $m$. 

%We observe clearly in these experiments the quadratic relationship between $k$ and $n$ and the linear relationship between $k$ and $m$. An interesting observation is that with a fixed $m=10$ in Figure~\ref{fig:heat_s1}, good result are obtained for high $n$, in the order of thousands, with $k$ relatively small, in the order of a thousand. This result disappear when $m$ grows in Figure~\ref{fig:heat_s2} and a network of similar size can only achieve good result for small $n$ (60 in this case). This indicate that the ratio between $m$ and $n$ seems to play an important role apart from the impact on $\norm{\fop}$. We also note that all the result we present do not respect the bounds of the main theorem while still providing good result, which indicates that our bounds are not tight.

% Furthermore, we look at the evolution of the loss and the probability of achieving global optimum when $n$ is fixed and $k$ varies. We present the results for $n=2100$, $m=10$ and $d=500$ in Figure~\ref{fig:plots_loss_and_proba_fixed_n}. We can observe an interesting phenomenon in Figure~\ref{fig:loss_s1} where the decay of the loss with respect to $k$ is polynomial up until a threshold $k$, here $k = 500$, where the loss suddenly drops to 0. After this point, the network is able to find zero-loss solutions with very high probability. One possible explanation for this drop is that the network reached a sufficiently overparametrized state which allows it to produce zero-loss solutions. However we would like to note that compared to our theorem bounds, this overparametrized state is achieved sooner that predicted as the theorem expect this to happen when $k$ is at least quadratic with respect to $n$ which is clearly not the case here. This reinforces the proofs that our theorem bounds are not tight.

% \begin{figure}[htb!]
%     \begin{subfigure}{.5\textwidth}
%         \centering
%         \includegraphics[width=.9\linewidth]{Plots/lossvk.png}
%         \caption{Evolution of the averaged final loss}
%         \label{fig:loss_s1}
%     \end{subfigure}%
%     \begin{subfigure}{.5\textwidth}
%         \centering
%         \includegraphics[width=.9\linewidth]{Plots/probavk.png}
%         \caption{Evolution of the probability}
%         \label{fig:loss_s2}
%     \end{subfigure}%
%     \caption{Evolution of the loss and the probability of achieving global optimum for different $k$ when $n$ is fixed.}
%     \label{fig:plots_loss_and_proba_fixed_n}
% \end{figure}


\section{Conclusion and Future Work}\label{sec:conclu}

%\todo{Ecrire la conclusion}

This paper studied the convergence of shallow DIP networks and provided bounds on the level of overparametrization, both in the input dimension and the hidden layer dimension, under which the method converges exponentially fast to a zero-loss solution. The proof relies on bounding the minimum singular values of the Jacobian of the network through an overparametrization that ensures a good initialization. These bounds are not tight, as demonstrated by the numerical experiments, but they provide an important step towards the theoretical understanding of DIP methods, and neural networks for inverse problems resolution in general. In the future, this work will be extended in several directions. First, we will study recovery guarantees of the signal $\xvc$. Second, we will investigate the DIP model with unrestricted linear layers and possibly in the multilayer setting.

\medskip

\subsubsection{Acknowledgements} The authors thank the French National Research Agency (ANR) for funding the project ANR-19-CHIA-0017-01-DEEP-VISION.

\newpage

\printbibliography
%\bibliographystyle{splncs04}
%\bibliography{references}

%\clearpage
%
%
%
%
%\begin{lemma}\label{lemma:bound_init_error_with_k}
%    Under the main assumptions, if the network architecture is bounded by
%    \begin{align}
%        \frac{\sqrt{d}}{\sqrt{\log(d)}} &\geq C_1\kappa(A)^2\sqrt{n}
%        \qquad \text{and} \label{eq:constraint_d}\\
%        \sqrt{k} &\geq C_2 \sqrt{n},
%    \end{align}
%    Then, we have R'<R, that is
%    %where $C$ and $c$ are positive constants depending on $\normsubgauss{\sqrt{d}\uv}$, $\norm{y}$, $\tau_1$, $\tau_3$ and $\tau_4$. Then the initial misfit of the network is bounded such that
%    \begin{align*}
%        \frac{8 B \norm{\fop}\norm{\Vv}}{\sigmin(\fop)^2\sigmin(\jthetaz)^2}\norm{\yzy} \leq \frac{\sigmin(\jthetaz}{2L}
%    \end{align*}
%with probability 
%    \begin{align*}
%        1 - \delta - 3\exp(-c\delta^2n) - \frac{2}{d^\delta}
%    \end{align*}
%    where $c$ is a positive constant and $C_1$ and $C_2$ are positive constant that depends on $1/\delta$ with $\delta \in [0,1]$.
%\end{lemma}
%
%\begin{proof}
%    We first use the deviations of gaussian matrices (Corollary 5.35 in \cite{vershynin_introduction_2012}) to estimate the norm of $\Vv$ and obtain that with probability $1 - 2\exp{(-\delta^2 n/2)}$ with $\delta \in [0,1]$.
%    \begin{align*}
%         \norm{\Vv} \leq (1 + (1+\delta)\sqrt{\frac{n}{k}}).
%    \end{align*}
%    We want to meet the condition given by lemma~\ref{lemma:link_params_singvals},\ref{claim:sigval_bounded_everywhere} that is $R > R'$. This gives us the following equation:
%    \begin{align*}
%        \sigmin(\jthetaz) > 8 B L \frac{\norm{\fop}\norm{\Vv}}{\sigmin(\fop)^2\sigmin(\jthetaz)^2}\norm{\yzy}.
%    \end{align*}
%    If we now use lemma~\ref{lemma:bound_initial_misfit} and lemma~\ref{lemma:lip-jacobian} and we obtain with high probability
%    \begin{align*}
%        \sigmin(\jthetaz) > 8 B^2 \kappa(\fop)^2 \frac{\norm{\Vv}^2}{\sigmin(\jthetaz)^2}(\norminf{\xvz} + \frac{\Cphi}{1-\delta})\norminf{\uv}\sqrt{n}.
%        %\sigmin(\jthetaz) > 8 B^2 \kappa(\fop)^2 \frac{\norm{\Vv}^2}{\sigmin(\jthetaz)^2}\sqrt{m}\norminf{\uv}
%    \end{align*}
%    We can see by using lemma~\ref{lemma:min_eigenvalue_singvalue_init} that we now get
%    \begin{align*}
%        1 - C\sqrt{\frac{n}{k}} > 8 B^2 \kappa(\fop)^2 \frac{\norm{\Vv}^2}{\sigmin(\jthetaz)^2}(\norminf{\xvz} + \frac{\Cphi}{1-\delta})\norminf{\uv}\sqrt{n}
%    \end{align*}
%    If $k>C_1n$ with $C_1>C$, we obtain that $\norm{\Vv}/\sigmin(\jthetaz)$ is $O(1)$ as well as the left-hand side of the inequality. We use lemma~\ref{lemma:inf_norm_unit_vector} to ensure that the inequality is true which means that we need $\sqrt{\frac{d}{(1+\delta)\log(d)}} > C_2 \kappa(\fop)^2\sqrt{n}$.
%
%    To obtain the right probability, we look at the probability given by lemma~\ref{lemma:bound_initial_misfit}, \ref{lemma:lip-jacobian}, \ref{lemma:min_eigenvalue_singvalue_init} and \ref{lemma:inf_norm_unit_vector} and by taking a union bound we get that our result hold with probability $1 - \delta - 3\exp(-c\delta^2n) - \frac{2}{d^\delta}$.
%\qed
%\end{proof}




\end{document}