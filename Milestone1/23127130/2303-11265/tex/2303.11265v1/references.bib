
@article{oymak_toward_2020,
	title = {Toward {Moderate} {Overparameterization}: {Global} {Convergence} {Guarantees} for {Training} {Shallow} {Neural} {Networks}},
	volume = {1},
	issn = {2641-8770},
	shorttitle = {Toward {Moderate} {Overparameterization}},
	doi = {10.1109/JSAIT.2020.2991332},
	abstract = {Many modern neural network architectures are trained in an overparameterized regime where the parameters of the model exceed the size of the training dataset. Sufficiently overparameterized neural network architectures in principle have the capacity to fit any set of labels including random noise. However, given the highly nonconvex nature of the training landscape it is not clear what level and kind of overparameterization is required for first order methods to converge to a global optima that perfectly interpolate any labels. A number of recent theoretical works have shown that for very wide neural networks where the number of hidden units is polynomially large in the size of the training data gradient descent starting from a random initialization does indeed converge to a global optima. However, in practice much more moderate levels of overparameterization seems to be sufficient and in many cases overparameterized models seem to perfectly interpolate the training data as soon as the number of parameters exceed the size of the training data by a constant factor. Thus there is a huge gap between the existing theoretical literature and practical experiments. In this paper we take a step towards closing this gap. Focusing on shallow neural nets and smooth activations, we show that (stochastic) gradient descent when initialized at random converges at a geometric rate to a nearby global optima as soon as the square-root of the number of network parameters exceeds the size of the training data. Our results also benefit from a fast convergence rate and continue to hold for non-differentiable activations such as Rectified Linear Units (ReLUs).},
	journal = {IEEE J-SAIT},
	author = {Oymak, Samet and Soltanolkotabi, Mahdi},
	month = may,
	year = {2020},
	keywords = {Biological neural networks, Convergence, Information theory, Neural network training, Stochastic processes, Training, Training data, nonconvex optimization, overparameterization, random matrix theory},
	pages = {84--105},
}

@article{ongie_deep_2020,
	title = {Deep {Learning} {Techniques} for {Inverse} {Problems} in {Imaging}},
	volume = {1},
	issn = {2641-8770},
	url = {https://ieeexplore.ieee.org/document/9084378/},
	doi = {10.1109/JSAIT.2020.2991563},
	number = {1},
	urldate = {2021-09-16},
	journal = {IEEE J-SAIT},
	author = {Ongie, Gregory and Jalal, Ajil and Metzler, Christopher A. and Baraniuk, Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
	month = may,
	year = {2020},
	keywords = {Review},
	pages = {39--56},
}

@inproceedings{mataev_deepred_2019,
	title = {{DeepRED}: {Deep} {Image} {Prior} {Powered} by {RED}},
	booktitle = {{ICCV}},
	author = {Mataev, Gary and Milanfar, Peyman and Elad, Michael},
	month = oct,
	year = {2019},
	pages = {0--0},
}

@inproceedings{jacot_neural_2018,
	title = {Neural {Tangent} {Kernel}: {Convergence} and {Generalization} in {Neural} {Networks}},
	volume = {31},
	shorttitle = {Neural {Tangent} {Kernel}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html},
	abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK.},
	urldate = {2022-11-02},
	booktitle = {{NeurIPS}},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
	year = {2018},
}

@article{bartlett_deep_2021,
	title = {Deep learning: a statistical viewpoint},
	volume = {30},
	issn = {0962-4929, 1474-0508},
	shorttitle = {Deep learning},
	url = {https://www.cambridge.org/core/journals/acta-numerica/article/deep-learning-a-statistical-viewpoint/7BCB89D860CEDDD5726088FAD64F2A5A},
	doi = {10.1017/S0962492921000027},
	abstract = {The remarkable practical success of deep learning has revealed some major surprises from a theoretical perspective. In particular, simple gradient methods easily find near-optimal solutions to non-convex optimization problems, and despite giving a near-perfect fit to training data without any explicit effort to control model complexity, these methods exhibit excellent predictive accuracy. We conjecture that specific principles underlie these phenomena: that overparametrization allows gradient methods to find interpolating solutions, that these methods implicitly impose regularization, and that overparametrization leads to benign overfitting, that is, accurate predictions despite overfitting training data. In this article, we survey recent progress in statistical learning theory that provides examples illustrating these principles in simpler settings. We first review classical uniform convergence results and why they fall short of explaining aspects of the behaviour of deep learning methods. We give examples of implicit regularization in simple settings, where gradient methods lead to minimal norm functions that perfectly fit the training data. Then we review prediction methods that exhibit benign overfitting, focusing on regression problems with quadratic loss. For these methods, we can decompose the prediction rule into a simple component that is useful for prediction and a spiky component that is useful for overfitting but, in a favourable setting, does not harm prediction accuracy. We focus specifically on the linear regime for neural networks, where the network can be approximated by a linear model. In this regime, we demonstrate the success of gradient flow, and we consider benign overfitting with two-layer networks, giving an exact asymptotic analysis that precisely demonstrates the impact of overparametrization. We conclude by highlighting the key challenges that arise in extending these insights to realistic deep learning settings.},
	language = {en},
	urldate = {2022-11-02},
	journal = {Acta Numerica},
	author = {Bartlett, Peter L. and Montanari, Andrea and Rakhlin, Alexander},
	month = may,
	year = {2021},
	note = {Cambridge University Press},
	pages = {87--201},
}

@inproceedings{zukerman_bp-dip_2021,
	title = {{BP}-{DIP}: {A} {Backprojection} based {Deep} {Image} {Prior}},
	shorttitle = {{BP}-{DIP}},
	doi = {10.23919/Eusipco47968.2020.9287540},
	abstract = {Deep neural networks are a very powerful tool for many computer vision tasks, including image restoration, exhibiting state-of-the-art results. However, the performance of deep learning methods tends to drop once the observation model used in training mismatches the one in test time. In addition, most deep learning methods require vast amounts of training data, which are not accessible in many applications. To mitigate these disadvantages, we propose to combine two image restoration approaches: (i) Deep Image Prior (DIP), which trains a convolutional neural network (CNN) from scratch in test time using the given degraded image. It does not require any training data and builds on the implicit prior imposed by the CNN architecture; and (ii) a backprojection (BP) fidelity term, which is an alternative to the standard least squares loss that is usually used in previous DIP works. We demonstrate the performance of the proposed method, termed BP-DIP, on the deblurring task and show its advantages over the plain DIP, with both higher PSNR values and better inference run-time.},
	booktitle = {{EUSIPCO} 2020},
	author = {Zukerman, Jenny and Tirer, Tom and Giryes, Raja},
	month = jan,
	year = {2021},
	keywords = {Deep learning, Electronics packaging, Image restoration, Task analysis, Tools, Training, Training data, image deblurring, loss functions},
	pages = {675--679},
}

@article{ulyanov_deep_2020,
	title = {Deep {Image} {Prior}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/1711.10925},
	doi = {10.1007/s11263-020-01303-4},
	abstract = {Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep\_image\_prior .},
	number = {7},
	urldate = {2021-10-20},
	journal = {IJCV},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	month = jul,
	year = {2020},
	note = {arXiv: 1711.10925},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	pages = {1867--1888},
}

@misc{bolte_characterizations_2008,
	title = {Characterizations of {Lojasiewicz} inequalities and applications},
	url = {http://arxiv.org/abs/0802.0826},
	doi = {10.48550/arXiv.0802.0826},
	abstract = {The classical Lojasiewicz inequality and its extensions for partial differential equation problems (Simon) and to o-minimal structures (Kurdyka) have a considerable impact on the analysis of gradient-like methods and related problems: minimization methods, complexity theory, asymptotic analysis of dissipative partial differential equations, tame geometry. This paper provides alternative characterizations of this type of inequalities for nonsmooth lower semicontinuous functions defined on a metric or a real Hilbert space. In a metric context, we show that a generalized form of the Lojasiewicz inequality (hereby called the Kurdyka-Lojasiewicz inequality) relates to metric regularity and to the Lipschitz continuity of the sublevel mapping, yielding applications to discrete methods (strong convergence of the proximal algorithm). In a Hilbert setting we further establish that asymptotic properties of the semiflow generated by \$-{\textbackslash}partial f\$ are strongly linked to this inequality. This is done by introducing the notion of a piecewise subgradient curve: such curves have uniformly bounded lengths if and only if the Kurdyka-Lojasiewicz inequality is satisfied. Further characterizations in terms of talweg lines -a concept linked to the location of the less steepest points at the level sets of \$f\$- and integrability conditions are given. In the convex case these results are significantly reinforced, allowing in particular to establish the asymptotic equivalence of discrete gradient methods and continuous gradient curves. On the other hand, a counterexample of a convex C{\textasciicircum}2 function in in the plane is constructed to illustrate the fact that, contrary to our intuition, and unless a specific growth condition is satisfied, convex functions may fail to fulfill the Kurdyka-Lojasiewicz inequality.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Bolte, Jerome and Daniilidis, Aris and Ley, Olivier and Mazet, Laurent},
	month = feb,
	year = {2008},
	note = {arXiv:0802.0826 [math]},
	keywords = {03C64, 26D10, 37N40, 49J52, 65K10., Mathematics - Dynamical Systems, Mathematics - Optimization and Control},
}

@article{liu_loss_2022,
	series = {Special {Issue} on {Harmonic} {Analysis} and {Machine} {Learning}},
	title = {Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
	volume = {59},
	issn = {1063-5203},
	url = {https://www.sciencedirect.com/science/article/pii/S106352032100110X},
	doi = {10.1016/j.acha.2021.12.009},
	abstract = {The success of deep learning is due, to a large extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. The purpose of this work is to propose a modern view and a general mathematical framework for loss landscapes and efficient optimization in over-parameterized machine learning models and systems of non-linear equations, a setting that includes over-parameterized deep neural networks. Our starting observation is that optimization landscapes corresponding to such systems are generally not convex, even locally around a global minimum, a condition we call essential non-convexity. We argue that instead they satisfy PL⁎, a variant of the Polyak-Łojasiewicz condition [32], [25] on most (but not all) of the parameter space, which guarantees both the existence of solutions and efficient optimization by (stochastic) gradient descent (SGD/GD). The PL⁎ condition of these systems is closely related to the condition number of the tangent kernel associated to a non-linear system showing how a PL⁎-based non-linear theory parallels classical analyses of over-parameterized linear equations. We show that wide neural networks satisfy the PL⁎ condition, which explains the (S)GD convergence to a global minimum. Finally we propose a relaxation of the PL⁎ condition applicable to “almost” over-parameterized systems.},
	language = {en},
	urldate = {2023-03-09},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
	month = jul,
	year = {2022},
	keywords = {Deep learning, Non-linear optimization, Over-parameterized models, PL condition},
	pages = {85--116},
}

@article{bolte_error_2017,
	title = {From error bounds to the complexity of first-order descent methods for convex functions},
	volume = {165},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/s10107-016-1091-6},
	doi = {10.1007/s10107-016-1091-6},
	abstract = {This paper shows that error bounds can be used as effective tools for deriving complexity results for first-order descent methods in convex minimization. In a first stage, this objective led us to revisit the interplay between error bounds and the Kurdyka-Łojasiewicz (KL) inequality. One can show the equivalence between the two concepts for convex functions having a moderately flat profile near the set of minimizers (as those of functions with Hölderian growth). A counterexample shows that the equivalence is no longer true for extremely flat functions. This fact reveals the relevance of an approach based on KL inequality. In a second stage, we show how KL inequalities can in turn be employed to compute new complexity bounds for a wealth of descent methods for convex problems. Our approach is completely original and makes use of a one-dimensional worst-case proximal sequence in the spirit of the famous majorant method of Kantorovich. Our result applies to a very simple abstract scheme that covers a wide class of descent methods. As a byproduct of our study, we also provide new results for the globalization of KL inequalities in the convex framework. Our main results inaugurate a simple method: derive an error bound, compute the desingularizing function whenever possible, identify essential constants in the descent method and finally compute the complexity using the one-dimensional worst case proximal sequence. Our method is illustrated through projection methods for feasibility problems, and through the famous iterative shrinkage thresholding algorithm (ISTA), for which we show that the complexity bound is of the form \$\$O(q{\textasciicircum}\{k\})\$\$where the constituents of the bound only depend on error bound constants obtained for an arbitrary least squares objective with \$\${\textbackslash}ell {\textasciicircum}1\$\$regularization.},
	language = {en},
	number = {2},
	urldate = {2023-02-13},
	journal = {Mathematical Programming},
	author = {Bolte, Jérôme and Nguyen, Trong Phong and Peypouquet, Juan and Suter, Bruce W.},
	month = oct,
	year = {2017},
	keywords = {65K05, 90C06, 90C25, 90C60, Complexity of first-order methods, Compressed sensing, Convex minimization, Error bounds, Forward-backward method, KL inequality, LASSO},
	pages = {471--507},
}

@article{chill_convergence_2006,
	title = {Convergence and decay rate to equilibrium of bounded solutions of quasilinear parabolic equations},
	volume = {228},
	issn = {0022-0396},
	url = {https://www.sciencedirect.com/science/article/pii/S002203960600074X},
	doi = {10.1016/j.jde.2006.02.009},
	abstract = {We study the convergence and decay rate to equilibrium of bounded solutions of the quasilinear parabolic equationut−diva(x,∇u)+f(x,u)=0 on a bounded domain, subject to Dirichlet boundary and to initial conditions. The data are supposed to satisfy suitable regularity and growth conditions. Our approach to the convergence result and decay estimate is based on the Łojasiewicz–Simon gradient inequality which in the case of the semilinear heat equation is known to give optimal decay estimates. The abstract results and their applications are discussed also in the framework of Orlicz–Sobolev spaces.},
	language = {en},
	number = {2},
	urldate = {2023-02-03},
	journal = {Journal of Differential Equations},
	author = {Chill, Ralph and Fiorenza, Alberto},
	month = sep,
	year = {2006},
	keywords = {Convergence of solutions, Decay rate, Orlicz–Sobolev space, Quasilinear parabolic problems, Łojasiewicz–Simon inequality},
	pages = {611--632},
}

@inproceedings{kurdyka_gradients_1998,
	title = {On gradients of functions definable in o-minimal structures},
	volume = {48},
	booktitle = {Annales de l'institut {Fourier}},
	author = {Kurdyka, Krzysztof},
	year = {1998},
	note = {Issue: 3},
	pages = {769--783},
}

@misc{robin_convergence_2023,
	title = {Convergence beyond the over-parameterized regime using {Rayleigh} quotients},
	url = {http://arxiv.org/abs/2301.08117},
	doi = {10.48550/arXiv.2301.08117},
	abstract = {In this paper, we present a new strategy to prove the convergence of deep learning architectures to a zero training (or even testing) loss by gradient flow. Our analysis is centered on the notion of Rayleigh quotients in order to prove Kurdyka-\{{\textbackslash}L\}ojasiewicz inequalities for a broader set of neural network architectures and loss functions. We show that Rayleigh quotients provide a unified view for several convergence analysis techniques in the literature. Our strategy produces a proof of convergence for various examples of parametric learning. In particular, our analysis does not require the number of parameters to tend to infinity, nor the number of samples to be finite, thus extending to test loss minimization and beyond the over-parameterized regime.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Robin, David A. R. and Scaman, Kevin and Lelarge, Marc},
	month = jan,
	year = {2023},
	note = {arXiv:2301.08117 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{montanari_interpolation_2022,
	title = {The interpolation phase transition in neural networks: {Memorization} and generalization under lazy training},
	volume = {50},
	issn = {0090-5364, 2168-8966},
	shorttitle = {The interpolation phase transition in neural networks},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-5/The-interpolation-phase-transition-in-neural-networks--Memorization-and/10.1214/22-AOS2211.full},
	doi = {10.1214/22-AOS2211},
	abstract = {Modern neural networks are often operated in a strongly overparametrized regime: they comprise so many parameters that they can interpolate the training set, even if actual labels are replaced by purely random ones. Despite this, they achieve good prediction error on unseen data: interpolating the training set does not lead to a large generalization error. Further, overparametrization appears to be beneficial in that it simplifies the optimization landscape. Here, we study these phenomena in the context of two-layers neural networks in the neural tangent (NT) regime. We consider a simple data model, with isotropic covariates vectors in d dimensions, and N hidden neurons. We assume that both the sample size n and the dimension d are large, and they are polynomially related. Our first main result is a characterization of the eigenstructure of the empirical NT kernel in the overparametrized regime Nd≫n. This characterization implies as a corollary that the minimum eigenvalue of the empirical NT kernel is bounded away from zero as soon as Nd≫n and, therefore, the network can exactly interpolate arbitrary labels in the same regime. Our second main result is a characterization of the generalization error of NT ridge regression including, as a special case, min-ℓ2 norm interpolation. We prove that, as soon as Nd≫n, the test error is well approximated by the one of kernel ridge regression with respect to the infinite-width kernel. The latter is in turn well approximated by the error of polynomial ridge regression, whereby the regularization parameter is increased by a “self-induced” term related to the high-degree components of the activation function. The polynomial degree depends on the sample size and the dimension (in particular on logn/logd).},
	number = {5},
	urldate = {2023-01-17},
	journal = {The Annals of Statistics},
	author = {Montanari, Andrea and Zhong, Yiqiao},
	month = oct,
	year = {2022},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62J05, 62J07, kernel ridge regression, memorization, neural tangent kernel, overfitting, overparametrization},
	pages = {2816--2847},
}

@incollection{vershynin_introduction_2012,
	title = {Introduction to the non-asymptotic analysis of random matrices},
	isbn = {978-1-107-00558-7},
	url = {https://www.cambridge.org/core/books/compressed-sensing/introduction-to-the-nonasymptotic-analysis-of-random-matrices/EC438B0A9D9ED5E010D76918C8E9E9A1},
	abstract = {This is a tutorial on some basic non-asymptotic methods and concepts in random matrix theory. The reader will learn several tools for the analysis of the extreme singular values of random matrices with independent rows or columns. Many of these methods sprung off from the development of geometric functional analysis since the 1970s. They have applications in several fields, most notably in theoretical computer science, statistics and signal processing. A few basic applications are covered in this text, particularly for the problem of estimating covariance matrices in statistics and for validating probabilistic constructions of measurement matrices in compressed sensing. This tutorial is written particularly for graduate students and beginning researchers in different areas, including functional analysts, probabilists, theoretical statisticians, electrical engineers, and theoretical computer scientists.IntroductionAsymptotic and non-asymptotic regimesRandom matrix theory studies properties of N × n matrices A chosen from some distribution on the set of all matrices. As dimensions N and n grow to infinity, one observes that the spectrum of A tends to stabilize. This is manifested in several limit laws, which may be regarded as random matrix versions of the central limit theorem. Among them is Wigner's semicircle law for the eigenvalues of symmetric Gaussian matrices, the circular law for Gaussian matrices, the Marchenko–Pastur law for Wishart matrices W = A* A where A is a Gaussian matrix, the Bai–Yin and Tracy–Widom laws for the extreme eigenvalues of Wishart matrices W.},
	urldate = {2022-11-02},
	booktitle = {Compressed {Sensing}: {Theory} and {Applications}},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	year = {2012},
	doi = {10.1017/CBO9780511794308.006},
}

@inproceedings{venkatakrishnan_plug-and-play_2013,
	title = {Plug-and-{Play} priors for model based reconstruction},
	isbn = {978-1-4799-0248-4},
	url = {http://ieeexplore.ieee.org/document/6737048/},
	doi = {10.1109/GlobalSIP.2013.6737048},
	urldate = {2021-10-11},
	booktitle = {{GlobalSIP}},
	author = {Venkatakrishnan, Singanallur V. and Bouman, Charles A. and Wohlberg, Brendt},
	month = dec,
	year = {2013},
	pages = {945--948},
}

@inproceedings{allen-zhu_convergence_2019,
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	url = {https://proceedings.mlr.press/v97/allen-zhu19a.html},
	abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100\% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps   e{\textasciicircum}\{-T\}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).},
	language = {en},
	urldate = {2022-11-02},
	booktitle = {{ICML}},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	month = may,
	year = {2019},
	pages = {242--252},
}

@inproceedings{alt_learning_2022,
	title = {Learning {Sparse} {Masks} for {Diffusion}-{Based} {Image} {Inpainting}},
	isbn = {978-3-031-04881-4},
	doi = {10.1007/978-3-031-04881-4_42},
	abstract = {Diffusion-based inpainting is a powerful tool for the reconstruction of images from sparse data. Its quality strongly depends on the choice of known data. Optimising their spatial location – the inpainting mask – is challenging. A commonly used tool for this task are stochastic optimisation strategies. However, they are slow as they compute multiple inpainting results. We provide a remedy in terms of a learned mask generation model. By emulating the complete inpainting pipeline with two networks for mask generation and neural surrogate inpainting, we obtain a model for highly efficient adaptive mask generation. Experiments indicate that our model can achieve competitive quality with an acceleration by as much as four orders of magnitude. Our findings serve as a basis for making diffusion-based inpainting more attractive for applications such as image compression, where fast encoding is highly desirable.},
	language = {en},
	booktitle = {{PRIA}},
	author = {Alt, Tobias and Peter, Pascal and Weickert, Joachim},
	year = {2022},
	keywords = {Data optimisation, Deep learning, Diffusion, Image inpainting, Partial differential equations},
	pages = {528--539},
}

@inproceedings{du_gradient_2019,
	title = {Gradient {Descent} {Provably} {Optimizes} {Over}-parameterized {Neural} {Networks}},
	url = {https://openreview.net/forum?id=S1eK3i09YQ},
	urldate = {2022-11-02},
	booktitle = {{ICLR}},
	author = {Du, Simon S. and Zhai, Xiyu and Póczos, Barnabás and Singh, Aarti},
	year = {2019},
}

@inproceedings{prost_learning_2021,
	title = {Learning {Local} {Regularization} for {Variational} {Image} {Restoration}},
	isbn = {978-3-030-75549-2},
	doi = {10.1007/978-3-030-75549-2_29},
	abstract = {In this work, we propose a framework to learn a local regularization model for solving general image restoration problems. This regularizer is defined with a fully convolutional neural network that sees the image through a receptive field corresponding to small image patches. The regularizer is then learned as a critic between unpaired distributions of clean and degraded patches using a Wasserstein generative adversarial networks based energy. This yields a regularization function that can be incorporated in any image restoration problem. The efficiency of the framework is finally shown on denoising and deblurring applications.},
	language = {en},
	booktitle = {{SSVM}},
	author = {Prost, Jean and Houdard, Antoine and Almansa, Andrés and Papadakis, Nicolas},
	year = {2021},
	pages = {358--370},
}

@inproceedings{chizat_lazy_2019,
	title = {On {Lazy} {Training} in {Differentiable} {Programming}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/ae614c557843b1df326cb29c57225459-Abstract.html},
	abstract = {In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this lazy training'' phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely thatlazy training'' is behind the many successes of neural networks in difficult high dimensional tasks.},
	urldate = {2022-10-13},
	booktitle = {{NeurIPS}},
	author = {Chizat, Lénaïc and Oyallon, Edouard and Bach, Francis},
	year = {2019},
}

@inproceedings{du_gradient_2019-1,
	title = {Gradient {Descent} {Finds} {Global} {Minima} of {Deep} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v97/du19c.html},
	abstract = {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.},
	language = {en},
	urldate = {2022-10-14},
	booktitle = {{ICML}},
	author = {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
	month = may,
	year = {2019},
	pages = {1675--1685},
}

@article{fang_mathematical_2021,
	title = {Mathematical {Models} of {Overparameterized} {Neural} {Networks}},
	volume = {109},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/9326403/},
	doi = {10.1109/JPROC.2020.3048020},
	number = {5},
	urldate = {2023-01-11},
	journal = {Proceedings of the IEEE},
	author = {Fang, Cong and Dong, Hanze and Zhang, Tong},
	month = may,
	year = {2021},
	pages = {683--703},
}

@inproceedings{liu_image_2019,
	title = {Image {Restoration} {Using} {Total} {Variation} {Regularized} {Deep} {Image} {Prior}},
	doi = {10.1109/ICASSP.2019.8682856},
	abstract = {In the past decade, sparsity-driven regularization has led to significant improvements in image reconstruction. Traditional regularizers, such as total variation (TV), rely on analytical models of sparsity. However, increasingly the field is moving towards trainable models, inspired from deep learning. Deep image prior (DIP) is a recent regularization framework that uses a convolutional neural network (CNN) architecture without data-driven training. This paper extends the DIP framework by combining it with the traditional TV regularization. We show that the inclusion of TV leads to considerable performance gains when tested on several traditional restoration tasks such as image denoising and deblurring.},
	booktitle = {{IEEE} {ICASSP}},
	author = {Liu, Jiaming and Sun, Yu and Xu, Xiaojian and Kamilov, Ulugbek S.},
	month = may,
	year = {2019},
	keywords = {Electronics packaging, Image reconstruction, Image restoration, Noise level, Optimization, Signal to noise ratio, TV, deep image prior, deep learning, image restoration, total variation regularization},
	pages = {7715--7719},
}

@article{monga_algorithm_2021,
	title = {Algorithm {Unrolling}: {Interpretable}, {Efficient} {Deep} {Learning} for {Signal} and {Image} {Processing}},
	volume = {38},
	issn = {1558-0792},
	shorttitle = {Algorithm {Unrolling}},
	doi = {10.1109/MSP.2020.3016905},
	abstract = {Deep neural networks provide unprecedented performance gains in many real-world problems in signal and image processing. Despite these gains, the future development and practical deployment of deep networks are hindered by their black-box nature, i.e., a lack of interpretability and the need for very large training sets. An emerging technique called algorithm unrolling, or unfolding, offers promise in eliminating these issues by providing a concrete and systematic connection between iterative algorithms that are widely used in signal processing and deep neural networks. Unrolling methods were first proposed to develop fast neural network approximations for sparse coding. More recently, this direction has attracted enormous attention, and it is rapidly growing in both theoretic investigations and practical applications. The increasing popularity of unrolled deep networks is due, in part, to their potential in developing efficient, high-performance (yet interpretable) network architectures from reasonably sized training sets.},
	number = {2},
	journal = {IEEE SPM},
	author = {Monga, Vishal and Li, Yuelong and Eldar, Yonina C.},
	month = mar,
	year = {2021},
	keywords = {Deep learning, Machine learning, Network architecture, Neural networks, Performance gain, Signal processing algorithms, Systematics, Training data},
	pages = {18--44},
}

@inproceedings{oymak_overparameterized_2019,
	title = {Overparameterized {Nonlinear} {Learning}: {Gradient} {Descent} {Takes} the {Shortest} {Path}?},
	shorttitle = {Overparameterized {Nonlinear} {Learning}},
	url = {https://proceedings.mlr.press/v97/oymak19a.html},
	abstract = {Many modern learning tasks involve fitting nonlinear models which are trained in an overparameterized regime where the parameters of the model exceed the size of the training dataset. Due to this overparameterization, the training loss may have infinitely many global minima and it is critical to understand the properties of the solutions found by first-order optimization schemes such as (stochastic) gradient descent starting from different initializations. In this paper we demonstrate that when the loss has certain properties over a minimally small neighborhood of the initial point, first order methods such as (stochastic) gradient descent have a few intriguing properties: (1) the iterates converge at a geometric rate to a global optima even when the loss is nonconvex, (2) among all global optima of the loss the iterates converge to one with a near minimal distance to the initial point, (3) the iterates take a near direct route from the initial point to this global optimum. As part of our proof technique, we introduce a new potential function which captures the tradeoff between the loss function and the distance to the initial point as the iterations progress. The utility of our general theory is demonstrated for a variety of problem domains spanning low-rank matrix recovery to shallow neural network training.},
	language = {en},
	urldate = {2022-11-02},
	booktitle = {{ICML}},
	author = {Oymak, Samet and Soltanolkotabi, Mahdi},
	month = may,
	year = {2019},
	pages = {4951--4960},
}

@article{shi_measuring_2022,
	title = {On {Measuring} and {Controlling} the {Spectral} {Bias} of the {Deep} {Image} {Prior}},
	volume = {130},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-021-01572-7},
	doi = {10.1007/s11263-021-01572-7},
	abstract = {The deep image prior showed that a randomly initialized network with a suitable architecture can be trained to solve inverse imaging problems by simply optimizing it’s parameters to reconstruct a single degraded image. However, it suffers from two practical limitations. First, it remains unclear how to control the prior beyond the choice of the network architecture. Second, training requires an oracle stopping criterion as during the optimization the performance degrades after reaching an optimum value. To address these challenges we introduce a frequency-band correspondence measure to characterize the spectral bias of the deep image prior, where low-frequency image signals are learned faster and better than high-frequency counterparts. Based on our observations, we propose techniques to prevent the eventual performance degradation and accelerate convergence. We introduce a Lipschitz-controlled convolution layer and a Gaussian-controlled upsampling layer as plug-in replacements for layers used in the deep architectures. The experiments show that with these changes the performance does not degrade during optimization, relieving us from the need for an oracle stopping criterion. We further outline a stopping criterion to avoid superfluous computation. Finally, we show that our approach obtains favorable results compared to current approaches across various denoising, deblocking, inpainting, super-resolution and detail enhancement tasks. Code is available at https://github.com/shizenglin/Measure-and-Control-Spectral-Bias.},
	language = {en},
	number = {4},
	urldate = {2022-03-28},
	journal = {IJCV},
	author = {Shi, Zenglin and Mettes, Pascal and Maji, Subhransu and Snoek, Cees G. M.},
	month = apr,
	year = {2022},
	pages = {885--908},
}

@book{boucheron_concentration_2013,
	title = {Concentration inequalities: {A} nonasymptotic theory of independence},
	publisher = {Oxford university press},
	author = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
	year = {2013},
}

@misc{kamilov_plug-and-play_2022,
	title = {Plug-and-{Play} {Methods} for {Integrating} {Physical} and {Learned} {Models} in {Computational} {Imaging}},
	url = {http://arxiv.org/abs/2203.17061},
	doi = {10.48550/arXiv.2203.17061},
	abstract = {Plug-and-Play Priors (PnP) is one of the most widely-used frameworks for solving computational imaging problems through the integration of physical models and learned models. PnP leverages high-fidelity physical sensor models and powerful machine learning methods for prior modeling of data to provide state-of-the-art reconstruction algorithms. PnP algorithms alternate between minimizing a data-fidelity term to promote data consistency and imposing a learned regularizer in the form of an image denoiser. Recent highly-successful applications of PnP algorithms include bio-microscopy, computerized tomography, magnetic resonance imaging, and joint ptycho-tomography. This article presents a unified and principled review of PnP by tracing its roots, describing its major variations, summarizing main results, and discussing applications in computational imaging. We also point the way towards further developments by discussing recent results on equilibrium equations that formulate the problem associated with PnP algorithms.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Kamilov, Ulugbek S. and Bouman, Charles A. and Buzzard, Gregery T. and Wohlberg, Brendt},
	month = aug,
	year = {2022},
	note = {arXiv:2203.17061 [eess]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{chen_imaging_2022,
	title = {Imaging with {Equivariant} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2209.01725},
	doi = {10.48550/arXiv.2209.01725},
	abstract = {From early image processing to modern computational imaging, successful models and algorithms have relied on a fundamental property of natural signals: symmetry. Here symmetry refers to the invariance property of signal sets to transformations such as translation, rotation or scaling. Symmetry can also be incorporated into deep neural networks in the form of equivariance, allowing for more data-efficient learning. While there has been important advances in the design of end-to-end equivariant networks for image classification in recent years, computational imaging introduces unique challenges for equivariant network solutions since we typically only observe the image through some noisy ill-conditioned forward operator that itself may not be equivariant. We review the emerging field of equivariant imaging and show how it can provide improved generalization and new imaging opportunities. Along the way we show the interplay between the acquisition physics and group actions and links to iterative reconstruction, blind compressed sensing and self-supervised learning.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Chen, Dongdong and Davies, Mike and Ehrhardt, Matthias J. and Schönlieb, Carola-Bibiane and Sherry, Ferdia and Tachella, Julián},
	month = sep,
	year = {2022},
	note = {arXiv:2209.01725 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{mukherjee_learned_2022,
	title = {Learned reconstruction methods with convergence guarantees},
	url = {http://arxiv.org/abs/2206.05431},
	doi = {10.48550/arXiv.2206.05431},
	abstract = {In recent years, deep learning has achieved remarkable empirical success for image reconstruction. This has catalyzed an ongoing quest for precise characterization of correctness and reliability of data-driven methods in critical use-cases, for instance in medical imaging. Notwithstanding the excellent performance and efficacy of deep learning-based methods, concerns have been raised regarding their stability, or lack thereof, with serious practical implications. Significant advances have been made in recent years to unravel the inner workings of data-driven image recovery methods, challenging their widely perceived black-box nature. In this article, we will specify relevant notions of convergence for data-driven image reconstruction, which will form the basis of a survey of learned methods with mathematically rigorous reconstruction guarantees. An example that is highlighted is the role of ICNN, offering the possibility to combine the power of deep learning with classical convex regularization theory for devising methods that are provably convergent. This survey article is aimed at both methodological researchers seeking to advance the frontiers of our understanding of data-driven image reconstruction methods as well as practitioners, by providing an accessible description of useful convergence concepts and by placing some of the existing empirical practices on a solid mathematical foundation.},
	urldate = {2023-01-06},
	publisher = {arXiv},
	author = {Mukherjee, Subhadip and Hauptmann, Andreas and Öktem, Ozan and Pereyra, Marcelo and Schönlieb, Carola-Bibiane},
	month = sep,
	year = {2022},
	note = {arXiv:2206.05431 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{radhakrishnan_feature_2022,
	title = {Feature learning in neural networks and kernel machines that recursively learn features},
	url = {http://arxiv.org/abs/2212.13881},
	doi = {10.48550/arXiv.2212.13881},
	abstract = {Neural networks have achieved impressive results on many technological and scientific tasks. Yet, their empirical successes have outpaced our fundamental understanding of their structure and function. By identifying mechanisms driving the successes of neural networks, we can provide principled approaches for improving neural network performance and develop simple and effective alternatives. In this work, we isolate the key mechanism driving feature learning in fully connected neural networks by connecting neural feature learning to the average gradient outer product. We subsequently leverage this mechanism to design {\textbackslash}textit\{Recursive Feature Machines\} (RFMs), which are kernel machines that learn features. We show that RFMs (1) accurately capture features learned by deep fully connected neural networks, (2) close the gap between kernel machines and fully connected networks, and (3) surpass a broad spectrum of models including neural networks on tabular data. Furthermore, we demonstrate that RFMs shed light on recently observed deep learning phenomena such as grokking, lottery tickets, simplicity biases, and spurious features. We provide a Python implementation to make our method broadly accessible [{\textbackslash}href\{https://github.com/aradha/recursive\_feature\_machines\}\{GitHub\}].},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13881 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{kim_accurate_2016,
	title = {Accurate {Image} {Super}-{Resolution} {Using} {Very} {Deep} {Convolutional} {Networks}},
	doi = {10.1109/CVPR.2016.182},
	abstract = {We present a highly accurate single-image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Convergence, Convolution, Image reconstruction, Signal resolution, Spatial resolution, Training},
	pages = {1646--1654},
}

@book{bogachev_measure_2007,
	address = {Berlin, Heidelberg},
	title = {Measure {Theory}},
	isbn = {978-3-540-34513-8 978-3-540-34514-5},
	url = {http://link.springer.com/10.1007/978-3-540-34514-5},
	language = {en},
	urldate = {2022-11-08},
	publisher = {Springer},
	author = {Bogachev, Vladimir I.},
	year = {2007},
	doi = {10.1007/978-3-540-34514-5},
	keywords = {Derivative, Lebesgue integral, Measure theory, Transformation, convergence of measures, differential equation, linear optimization, measure, transformation of mesures},
}

@inproceedings{kim_accurate_2016-1,
	title = {Accurate {Image} {Super}-{Resolution} {Using} {Very} {Deep} {Convolutional} {Networks}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.html},
	urldate = {2022-11-02},
	author = {Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu},
	year = {2016},
	pages = {1646--1654},
}

@article{gilton_neumann_2020,
	title = {Neumann {Networks} for {Linear} {Inverse} {Problems} in {Imaging}},
	volume = {6},
	issn = {2333-9403},
	doi = {10.1109/TCI.2019.2948732},
	abstract = {Many challenging image processing tasks can be described by an ill-posed linear inverse problem: deblurring, deconvolution, inpainting, compressed sensing, and superresolution all lie in this framework. Traditional inverse problem solvers minimize a cost function consisting of a data-fit term, which measures how well an image matches the observations, and a regularizer, which reflects prior knowledge and promotes images with desirable properties like smoothness. Recent advances in machine learning and image processing have illustrated that it is often possible to learn a regularizer from training data that can outperform more traditional regularizers. We present an end-to-end, data-driven method of solving inverse problems inspired by the Neumann series, which we call a Neumann network. Rather than unroll an iterative optimization algorithm, we truncate a Neumann series which directly solves the linear inverse problem with a data-driven nonlinear regularizer. The Neumann network architecture outperforms traditional inverse problem solution methods, model-free deep learning approaches, and state-of-the-art unrolled iterative methods on standard datasets. Finally, when the images belong to a union of subspaces and under appropriate assumptions on the forward model, we prove there exists a Neumann network configuration that well-approximates the optimal oracle estimator for the inverse problem and demonstrate empirically that the trained Neumann network has the form predicted by theory.},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Gilton, D. and Ongie, G. and Willett, R.},
	year = {2020},
	keywords = {Image reconstruction, Imaging, Inverse problems, Iterative methods, Neumann network architecture, Neumann network configuration, Neumann series, Neural networks, Training, Training data, convergence, cost function, data-driven method, data-driven nonlinear regularizer, data-fit term, deconvolution, estimation, image matching, image processing tasks, inverse problems, iterative algorithms, learning (artificial intelligence), linear inverse problem, machine learning, model-free deep learning approaches, optimal oracle estimator, traditional inverse problem solution methods, trained Neumann network},
	pages = {328--343},
}

@book{schilling_measures_2017,
	address = {Cambridge},
	edition = {2nd ed},
	title = {Measures, integrals and martingales},
	isbn = {978-1-316-62024-3},
	language = {eng},
	publisher = {Cambridge university press},
	author = {Schilling, René L.},
	year = {2017},
}

@book{schilling_measures_2017-1,
	address = {Cambridge},
	edition = {2nd ed},
	title = {Measures, integrals and martingales},
	isbn = {978-1-316-62024-3},
	language = {eng},
	publisher = {Cambridge university press},
	author = {Schilling, René L.},
	year = {2017},
}

@article{wang_when_2022,
	title = {When and why {PINNs} fail to train: {A} neural tangent kernel perspective},
	volume = {449},
	issn = {0021-9991},
	shorttitle = {When and why {PINNs} fail to train},
	url = {https://www.sciencedirect.com/science/article/pii/S002199912100663X},
	doi = {10.1016/j.jcp.2021.110768},
	abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/PINNsNTK.},
	language = {en},
	urldate = {2022-10-20},
	journal = {Journal of Computational Physics},
	author = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
	month = jan,
	year = {2022},
	keywords = {Gradient descent, Multi-task learning, Physics-informed neural networks, Scientific machine learning, Spectral bias},
	pages = {110768},
}

@inproceedings{rahimi_weighted_2008,
	title = {Weighted {Sums} of {Random} {Kitchen} {Sinks}: {Replacing} minimization with randomization in learning},
	volume = {21},
	shorttitle = {Weighted {Sums} of {Random} {Kitchen} {Sinks}},
	url = {https://papers.nips.cc/paper/2008/hash/0efe32849d230d7f53049ddc4a4b0c60-Abstract.html},
	abstract = {Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. What are you doing?'' asked Minsky.I am training a randomly wired neural net to play tic-tac-toe,'' Sussman replied. Why is the net wired randomly?'' asked Minsky. Sussman replied,I do not want it to have any preconceptions of how to play.'' Minsky then shut his eyes. Why do you close your eyes?'' Sussman asked his teacher.So that the room will be empty,'' replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Specifically, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classification performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities.},
	urldate = {2022-10-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rahimi, Ali and Recht, Benjamin},
	year = {2008},
}

@inproceedings{woodworth_kernel_2020,
	title = {Kernel and {Rich} {Regimes} in {Overparametrized} {Models}},
	url = {https://proceedings.mlr.press/v125/woodworth20a.html},
	abstract = {A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e. when  during training the network behaves as a kernelized linear predictor, and thus, training with gradient descent has the effect of finding the corresponding minimum RKHS norm solution.  This stands in contrast to other studies which demonstrate how gradient descent on overparametrized  networks can induce rich implicit biases that are not RKHS norms.  Building on an observation by {\textbackslash}citet\{chizat2018note\}, we show how the {\textbackslash}textbf\{{\textbackslash}textit\{scale of the initialization\}\} controls the transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a family of simple depth-𝐷DD linear networks that exhibit an interesting and meaningful transition between the kernel and rich regimes, and highlight an interesting role for the {\textbackslash}emph\{width\}  of the models. We further demonstrate this transition empirically for matrix factorization and multilayer non-linear networks.},
	language = {en},
	urldate = {2022-10-18},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3635--3673},
}

@article{sirignano_mean_2020,
	title = {Mean {Field} {Analysis} of {Neural} {Networks}: {A} {Law} of {Large} {Numbers}},
	volume = {80},
	issn = {0036-1399},
	shorttitle = {Mean {Field} {Analysis} of {Neural} {Networks}},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1192184},
	doi = {10.1137/18M1192184},
	abstract = {Machine learning, and in particular neural network models, have revolutionized fields such as image, text, and speech recognition. Today, many important real-world applications in these areas are driven by neural networks. There are also growing applications in engineering, robotics, medicine, and finance. Despite their immense success in practice, there is limited mathematical understanding of neural networks. This paper illustrates how neural networks can be studied via stochastic analysis and develops approaches for addressing some of the technical challenges which arise. We analyze one-layer neural networks in the asymptotic regime of simultaneously (a) large network sizes and (b) large numbers of stochastic gradient descent training iterations. We rigorously prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation. This result can be considered a law of large numbers for neural networks. In addition, a consequence of our analysis is that the trained parameters of the neural network asymptotically become independent, a property which is commonly called “propagation of chaos.”},
	number = {2},
	urldate = {2022-10-18},
	journal = {SIAM Journal on Applied Mathematics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {60-08, 60F99, 60H10, 62M45, machine learning, stochastic analysis, weak convergence},
	pages = {725--752},
}

@misc{emschwiller_neural_2020,
	title = {Neural {Networks} and {Polynomial} {Regression}. {Demystifying} the {Overparametrization} {Phenomena}},
	url = {http://arxiv.org/abs/2003.10523},
	doi = {10.48550/arXiv.2003.10523},
	abstract = {In the context of neural network models, overparametrization refers to the phenomena whereby these models appear to generalize well on the unseen data, even though the number of parameters significantly exceeds the sample sizes, and the model perfectly fits the in-training data. A conventional explanation of this phenomena is based on self-regularization properties of algorithms used to train the data. In this paper we prove a series of results which provide a somewhat diverging explanation. Adopting a teacher/student model where the teacher network is used to generate the predictions and student network is trained on the observed labeled data, and then tested on out-of-sample data, we show that any student network interpolating the data generated by a teacher network generalizes well, provided that the sample size is at least an explicit quantity controlled by data dimension and approximation guarantee alone, regardless of the number of internal nodes of either teacher or student network. Our claim is based on approximating both teacher and student networks by polynomial (tensor) regression models with degree depending on the desired accuracy and network depth only. Such a parametrization notably does not depend on the number of internal nodes. Thus a message implied by our results is that parametrizing wide neural networks by the number of hidden nodes is misleading, and a more fitting measure of parametrization complexity is the number of regression coefficients associated with tensorized data. In particular, this somewhat reconciles the generalization ability of neural networks with more classical statistical notions of data complexity and generalization bounds. Our empirical results on MNIST and Fashion-MNIST datasets indeed confirm that tensorized regression achieves a good out-of-sample performance, even when the degree of the tensor is at most two.},
	urldate = {2022-10-17},
	publisher = {arXiv},
	author = {Emschwiller, Matt and Gamarnik, David and Kızıldağ, Eren C. and Zadik, Ilias},
	month = mar,
	year = {2020},
	note = {arXiv:2003.10523 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@inproceedings{lu_mean_2020,
	title = {A {Mean} {Field} {Analysis} {Of} {Deep} {ResNet} {And} {Beyond}: {Towards} {Provably} {Optimization} {Via} {Overparameterization} {From} {Depth}},
	shorttitle = {A {Mean} {Field} {Analysis} {Of} {Deep} {ResNet} {And} {Beyond}},
	url = {https://proceedings.mlr.press/v119/lu20b.html},
	abstract = {Training deep neural networks with stochastic gradient descent (SGD) can often achieve zero training loss on real-world tasks although the optimization landscape is known to be highly non-convex. To understand the success of SGD for training deep neural networks, this work presents a mean-field analysis of deep residual networks, based on a line of works which interpret the continuum limit of the deep residual network as an ordinary differential equation as the the network capacity tends to infinity. Specifically, we propose a {\textbackslash}textbf\{new continuum limit\} of deep residual networks, which enjoys a good landscape in the sense that {\textbackslash}textbf\{every local minimizer is global\}. This characterization enables us to derive the first global convergence result for multilayer neural networks in the mean-field regime. Furthermore, our proof does not rely on the convexity of the loss landscape, but instead, an assumption on the global minimizer should achieve zero loss which can be achieved when the model shares a universal approximation property. Key to our result is the observation that a deep residual network resembles a shallow network ensemble {\textbackslash}cite\{veit2016residual\}, {\textbackslash}emph\{i.e.\} a two-layer network. We bound the difference between the shallow network and our ResNet model via the adjoint sensitivity method, which enables us to transfer previous mean-field analysis of two-layer networks to deep networks. Furthermore, we propose several novel training schemes based on our new continuous model, among which one new training procedure introduces the operation of switching the order of the residual blocks and results in strong empirical performance on benchmark datasets.},
	language = {en},
	urldate = {2022-10-17},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {6426--6436},
}

@misc{yang_feature_2022,
	title = {Feature {Learning} in {Infinite}-{Width} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2011.14522},
	doi = {10.48550/arXiv.2011.14522},
	abstract = {As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases. More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Yang, Greg and Hu, Edward J.},
	month = jul,
	year = {2022},
	note = {arXiv:2011.14522 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Condensed Matter - Disordered Systems and Neural Networks},
}

@misc{cao_benign_2022,
	title = {Benign {Overfitting} in {Two}-layer {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2202.06526},
	doi = {10.48550/arXiv.2202.06526},
	abstract = {Modern neural networks often have great expressive power and can be trained to overfit the training data, while still achieving a good test performance. This phenomenon is referred to as "benign overfitting". Recently, there emerges a line of works studying "benign overfitting" from the theoretical perspective. However, they are limited to linear models or kernel/random feature models, and there is still a lack of theoretical understanding about when and how benign overfitting occurs in neural networks. In this paper, we study the benign overfitting phenomenon in training a two-layer convolutional neural network (CNN). We show that when the signal-to-noise ratio satisfies a certain condition, a two-layer CNN trained by gradient descent can achieve arbitrarily small training and test loss. On the other hand, when this condition does not hold, overfitting becomes harmful and the obtained CNN can only achieve a constant level test loss. These together demonstrate a sharp phase transition between benign overfitting and harmful overfitting, driven by the signal-to-noise ratio. To the best of our knowledge, this is the first work that precisely characterizes the conditions under which benign overfitting can occur in training convolutional neural networks.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Cao, Yuan and Chen, Zixiang and Belkin, Mikhail and Gu, Quanquan},
	month = jun,
	year = {2022},
	note = {arXiv:2202.06526 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{adler_learned_2018,
	title = {Learned {Primal}-{Dual} {Reconstruction}},
	volume = {37},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8271999/},
	doi = {10.1109/TMI.2018.2799231},
	number = {6},
	urldate = {2022-10-12},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Adler, Jonas and Oktem, Ozan},
	month = jun,
	year = {2018},
	pages = {1322--1332},
}

@article{aggarwal_modl_2019,
	title = {{MoDL}: {Model}-{Based} {Deep} {Learning} {Architecture} for {Inverse} {Problems}},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	shorttitle = {{MoDL}},
	url = {https://ieeexplore.ieee.org/document/8434321/},
	doi = {10.1109/TMI.2018.2865356},
	number = {2},
	urldate = {2022-10-12},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Aggarwal, Hemant K. and Mani, Merry P. and Jacob, Mathews},
	month = feb,
	year = {2019},
	pages = {394--405},
}

@inproceedings{meinhardt_learning_2017,
	address = {Venice},
	title = {Learning {Proximal} {Operators}: {Using} {Denoising} {Networks} for {Regularizing} {Inverse} {Imaging} {Problems}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {Learning {Proximal} {Operators}},
	url = {http://ieeexplore.ieee.org/document/8237460/},
	doi = {10.1109/ICCV.2017.198},
	urldate = {2022-10-12},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Meinhardt, Tim and Moeller, Michael and Hazirbas, Caner and Cremers, Daniel},
	month = oct,
	year = {2017},
	pages = {1799--1808},
}

@misc{diamond_unrolled_2018,
	title = {Unrolled {Optimization} with {Deep} {Priors}},
	url = {http://arxiv.org/abs/1705.08041},
	doi = {10.48550/arXiv.1705.08041},
	abstract = {A broad class of problems at the core of computational imaging, sensing, and low-level computer vision reduces to the inverse problem of extracting latent images that follow a prior distribution, from measurements taken under a known physical image formation model. Traditionally, hand-crafted priors along with iterative optimization methods have been used to solve such problems. In this paper we present unrolled optimization with deep priors, a principled framework for infusing knowledge of the image formation into deep networks that solve inverse problems in imaging, inspired by classical iterative methods. We show that instances of the framework outperform the state-of-the-art by a substantial margin for a wide variety of imaging problems, such as denoising, deblurring, and compressed sensing magnetic resonance imaging (MRI). Moreover, we conduct experiments that explain how the framework is best used and why it outperforms previous methods.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Diamond, Steven and Sitzmann, Vincent and Heide, Felix and Wetzstein, Gordon},
	month = dec,
	year = {2018},
	note = {arXiv:1705.08041 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhang_beyond_2017,
	title = {Beyond a {Gaussian} {Denoiser}: {Residual} {Learning} of {Deep} {CNN} for {Image} {Denoising}},
	volume = {26},
	issn = {1057-7149, 1941-0042},
	shorttitle = {Beyond a {Gaussian} {Denoiser}},
	url = {https://ieeexplore.ieee.org/document/7839189/},
	doi = {10.1109/TIP.2017.2662206},
	number = {7},
	urldate = {2022-10-12},
	journal = {IEEE Transactions on Image Processing},
	author = {Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
	month = jul,
	year = {2017},
	pages = {3142--3155},
}

@inproceedings{arora_fine-grained_2019,
	title = {Fine-{Grained} {Analysis} of {Optimization} and {Generalization} for {Overparameterized} {Two}-{Layer} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v97/arora19a.html},
	abstract = {Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR’17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.},
	language = {en},
	urldate = {2022-10-12},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {322--332},
}

@misc{matthews_gaussian_2018,
	title = {Gaussian {Process} {Behaviour} in {Wide} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1804.11271},
	doi = {10.48550/arXiv.1804.11271},
	abstract = {Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results by Neal (1996) to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Matthews, Alexander G. de G. and Rowland, Mark and Hron, Jiri and Turner, Richard E. and Ghahramani, Zoubin},
	month = aug,
	year = {2018},
	note = {arXiv:1804.11271 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{frei_benign_2022,
	title = {Benign {Overfitting} without {Linearity}: {Neural} {Network} {Classifiers} {Trained} by {Gradient} {Descent} for {Noisy} {Linear} {Data}},
	shorttitle = {Benign {Overfitting} without {Linearity}},
	url = {https://proceedings.mlr.press/v178/frei22a.html},
	abstract = {Benign overfitting, the phenomenon where interpolating models generalize well in the presence of noisy data, was first observed in neural network models trained with gradient descent.  To better understand this empirical observation, we consider the generalization error of two-layer neural networks trained to interpolation by gradient descent on the logistic loss following random initialization.  We assume the data comes from well-separated class-conditional log-concave distributions and allow for a constant fraction of the training labels to be corrupted by an adversary.  We show that in this setting, neural networks exhibit benign overfitting: they can be driven to zero training error, perfectly fitting any noisy training labels, and simultaneously achieve minimax optimal test error.   In contrast to previous work on benign overfitting that require linear or kernel-based predictors, our analysis holds in a setting where both the model and learning dynamics are fundamentally nonlinear.},
	language = {en},
	urldate = {2022-10-11},
	booktitle = {Proceedings of {Thirty} {Fifth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Frei, Spencer and Chatterji, Niladri S. and Bartlett, Peter},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {2668--2703},
}

@misc{brutzkus_sgd_2017,
	title = {{SGD} {Learns} {Over}-parameterized {Networks} that {Provably} {Generalize} on {Linearly} {Separable} {Data}},
	url = {http://arxiv.org/abs/1710.10174},
	doi = {10.48550/arXiv.1710.10174},
	abstract = {Neural networks exhibit good generalization behavior in the over-parameterized regime, where the number of network parameters exceeds the number of observations. Nonetheless, current generalization bounds for neural networks fail to explain this phenomenon. In an attempt to bridge this gap, we study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks. Specifically, we prove convergence rates of SGD to a global minimum and provide generalization guarantees for this global minimum that are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model. This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.},
	urldate = {2022-10-11},
	publisher = {arXiv},
	author = {Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
	month = oct,
	year = {2017},
	note = {arXiv:1710.10174 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{leung_deep_2014,
	title = {Deep learning of the tissue-regulated splicing code},
	volume = {30},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btu277},
	doi = {10.1093/bioinformatics/btu277},
	abstract = {Motivation: Alternative splicing (AS) is a regulated process that directs the generation of different transcripts from single genes. A computational model that can accurately predict splicing patterns based on genomic features and cellular context is highly desirable, both in understanding this widespread phenomenon, and in exploring the effects of genetic variations on AS.Methods: Using a deep neural network, we developed a model inferred from mouse RNA-Seq data that can predict splicing patterns in individual tissues and differences in splicing patterns across tissues. Our architecture uses hidden variables that jointly represent features in genomic sequences and tissue types when making predictions. A graphics processing unit was used to greatly reduce the training time of our models with millions of parameters.Results: We show that the deep architecture surpasses the performance of the previous Bayesian method for predicting AS patterns. With the proper optimization procedure and selection of hyperparameters, we demonstrate that deep architectures can be beneficial, even with a moderately sparse dataset. An analysis of what the model has learned in terms of the genomic features is presented.Contact:frey@psi.toronto.eduSupplementary information:Supplementary data are available at Bioinformatics online.},
	number = {12},
	urldate = {2022-09-30},
	journal = {Bioinformatics},
	author = {Leung, Michael K. K. and Xiong, Hui Yuan and Lee, Leo J. and Frey, Brendan J.},
	month = jun,
	year = {2014},
	pages = {i121--i129},
}

@article{ciodaro_online_2012,
	title = {Online particle detection with {Neural} {Networks} based on topological calorimetry information},
	volume = {368},
	issn = {1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/368/1/012030/meta},
	doi = {10.1088/1742-6596/368/1/012030},
	language = {en},
	number = {1},
	urldate = {2022-09-30},
	journal = {Journal of Physics: Conference Series},
	author = {Ciodaro, T. and Deva, D. and Seixas, J. M. de and Damazio, D.},
	month = jun,
	year = {2012},
	note = {Publisher: IOP Publishing},
	pages = {012030},
}

@article{zhang_deep_2018,
	title = {Deep {Learning} for {Environmentally} {Robust} {Speech} {Recognition}: {An} {Overview} of {Recent} {Developments}},
	volume = {9},
	issn = {2157-6904, 2157-6912},
	shorttitle = {Deep {Learning} for {Environmentally} {Robust} {Speech} {Recognition}},
	url = {https://dl.acm.org/doi/10.1145/3178115},
	doi = {10.1145/3178115},
	abstract = {Eliminating the negative effect of non-stationary environmental noise is a long-standing research topic for automatic speech recognition but still remains an important challenge. Data-driven supervised approaches, especially the ones based on deep neural networks, have recently emerged as potential alternatives to traditional unsupervised approaches and with sufficient training, can alleviate the shortcomings of the unsupervised methods in various real-life acoustic environments. In this light, we review recently developed, representative deep learning approaches for tackling non-stationary additive and convolutional degradation of speech with the aim of providing guidelines for those involved in the development of environmentally robust speech recognition systems. We separately discuss single- and multi-channel techniques developed for the front-end and back-end of speech recognition systems, as well as joint front-end and back-end training frameworks. In the meanwhile, we discuss the pros and cons of these approaches and provide their experimental results on benchmark databases. We expect that this overview can facilitate the development of the robustness of speech recognition systems in acoustic noisy environments.},
	language = {en},
	number = {5},
	urldate = {2022-09-30},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Zhang, Zixing and Geiger, Jürgen and Pohjalainen, Jouni and Mousa, Amr El-Desoky and Jin, Wenyu and Schuller, Björn},
	month = sep,
	year = {2018},
	pages = {1--28},
}

@article{rivenson_deep_2017,
	title = {Deep learning microscopy},
	volume = {4},
	copyright = {\&\#169; 2017 Optical Society of America},
	issn = {2334-2536},
	url = {https://opg.optica.org/optica/abstract.cfm?uri=optica-4-11-1437},
	doi = {10.1364/OPTICA.4.001437},
	abstract = {We demonstrate that a deep neural network can significantly improve optical microscopy, enhancing its spatial resolution over a large field of view and depth of field. After its training, the only input to this network is an image acquired using a regular optical microscope, without any changes to its design. We blindly tested this deep learning approach using various tissue samples that are imaged with low-resolution and wide-field systems, where the network rapidly outputs an image with better resolution, matching the performance of higher numerical aperture lenses and also significantly surpassing their limited field of view and depth of field. These results are significant for various fields that use microscopy tools, including, e.g.,\&\#x00A0;life sciences, where optical microscopy is considered as one of the most widely used and deployed techniques. Beyond such applications, the presented approach might be applicable to other imaging modalities, also spanning different parts of the electromagnetic spectrum, and can be used to design computational imagers that get better as they continue to image specimens and establish new transformations among different modes of imaging.},
	language = {EN},
	number = {11},
	urldate = {2022-09-30},
	journal = {Optica},
	author = {Rivenson, Yair and Göröcs, Zoltán and Günaydin, Harun and Zhang, Yibo and Wang, Hongda and Ozcan, Aydogan},
	month = nov,
	year = {2017},
	note = {Publisher: Optica Publishing Group},
	pages = {1437--1443},
}

@inproceedings{shinde_review_2018,
	address = {Pune, India},
	title = {A {Review} of {Machine} {Learning} and {Deep} {Learning} {Applications}},
	isbn = {978-1-5386-5257-2},
	url = {https://ieeexplore.ieee.org/document/8697857/},
	doi = {10.1109/ICCUBEA.2018.8697857},
	abstract = {Machine learning is one of the fields in the modern computing world.A plenty of research has been undertaken to make machines intelligent. Learning is a natural human behavior which has been made an essential aspect of the machines as well. There are various techniques devised for the same.Traditional machine learning algorithms have been applied in many application areas. Researchers have put many efforts to improve the accuracy of that machinelearning algorithms.Another dimension was given thought which leads to deep learning concept. Deep learning is a subset of machine learning. So far few applications of deep learning have been explored. This is definitely going to cater to solving issues in several new application domains, sub-domains using deep learning. A review of these past and future application domains, sub-domains, and applications of machine learning and deep learning are illustrated in this paper.},
	language = {en},
	urldate = {2022-09-30},
	booktitle = {2018 {Fourth} {International} {Conference} on {Computing} {Communication} {Control} and {Automation} ({ICCUBEA})},
	publisher = {IEEE},
	author = {Shinde, Pramila P. and Shah, Seema},
	month = aug,
	year = {2018},
	pages = {1--6},
}

@inproceedings{shinde_review_2018-1,
	address = {Pune, India},
	title = {A {Review} of {Machine} {Learning} and {Deep} {Learning} {Applications}},
	isbn = {978-1-5386-5257-2},
	url = {https://ieeexplore.ieee.org/document/8697857/},
	doi = {10.1109/ICCUBEA.2018.8697857},
	urldate = {2022-09-30},
	booktitle = {2018 {Fourth} {International} {Conference} on {Computing} {Communication} {Control} and {Automation} ({ICCUBEA})},
	publisher = {IEEE},
	author = {Shinde, Pramila P. and Shah, Seema},
	month = aug,
	year = {2018},
	pages = {1--6},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2022-09-30},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
}

@book{vershynin_high-dimensional_2018,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}},
	isbn = {978-1-108-41519-4},
	shorttitle = {High-{Dimensional} {Probability}},
	url = {https://www.cambridge.org/core/books/highdimensional-probability/797C466DA29743D2C8213493BD2D2102},
	abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
	urldate = {2022-09-15},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	year = {2018},
	doi = {10.1017/9781108231596},
}

@article{rudelson_smallest_2009,
	title = {Smallest singular value of a random rectangular matrix},
	volume = {62},
	issn = {1097-0312},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.20294},
	doi = {10.1002/cpa.20294},
	abstract = {We prove an optimal estimate of the smallest singular value of a random sub-Gaussian matrix, valid for all dimensions. For an N × n matrix A with independent and identically distributed sub-Gaussian entries, the smallest singular value of A is at least of the order √N − √n − 1 with high probability. A sharp estimate on the probability is also obtained. © 2009 Wiley Periodicals, Inc.},
	language = {en},
	number = {12},
	urldate = {2022-09-21},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Rudelson, Mark and Vershynin, Roman},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.20294},
	pages = {1707--1739},
}

@article{rudelson_smallest_nodate,
	title = {{THE} {SMALLEST} {SINGULAR} {VALUE} {OF} {A} {RANDOM} {RECTANGULAR} {MATRIX}},
	abstract = {We prove an optimal estimate of the smallest singular value of a random subgaussian matrix, valid for all dimensions. For an N × n matrix A with independent and identically distributed s√ubgaus√sian entries, the smallest singular value of A is at least of the order N − n − 1 with high probability. A sharp estimate on the probability is also obtained.},
	language = {en},
	author = {Rudelson, Mark and Vershynin, Roman},
	pages = {33},
}

@article{mei_mean_2018,
	title = {A mean field view of the landscape of two-layer neural networks},
	volume = {115},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1806579115},
	doi = {10.1073/pnas.1806579115},
	number = {33},
	urldate = {2022-09-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	month = aug,
	year = {2018},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {E7665--E7671},
}

@misc{heckel_deep_2019,
	title = {Deep {Decoder}: {Concise} {Image} {Representations} from {Untrained} {Non}-convolutional {Networks}},
	shorttitle = {Deep {Decoder}},
	url = {http://arxiv.org/abs/1810.03982},
	doi = {10.48550/arXiv.1810.03982},
	abstract = {Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.},
	urldate = {2022-09-15},
	publisher = {arXiv},
	author = {Heckel, Reinhard and Hand, Paul},
	month = feb,
	year = {2019},
	note = {arXiv:1810.03982 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{cheng_bayesian_2019,
	address = {Long Beach, CA, USA},
	title = {A {Bayesian} {Perspective} on the {Deep} {Image} {Prior}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954206/},
	doi = {10.1109/CVPR.2019.00559},
	urldate = {2022-09-15},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cheng, Zezhou and Gadelha, Matheus and Maji, Subhransu and Sheldon, Daniel},
	month = jun,
	year = {2019},
	pages = {5438--5446},
}

@article{lucas_using_2018,
	title = {Using {Deep} {Neural} {Networks} for {Inverse} {Problems} in {Imaging}: {Beyond} {Analytical} {Methods}},
	volume = {35},
	issn = {1053-5888},
	shorttitle = {Using {Deep} {Neural} {Networks} for {Inverse} {Problems} in {Imaging}},
	url = {http://ieeexplore.ieee.org/document/8253590/},
	doi = {10.1109/MSP.2017.2760358},
	language = {en},
	number = {1},
	urldate = {2022-09-15},
	journal = {IEEE Signal Processing Magazine},
	author = {Lucas, Alice and Iliadis, Michael and Molina, Rafael and Katsaggelos, Aggelos K.},
	month = jan,
	year = {2018},
	pages = {20--36},
}

@article{lucas_using_2018-1,
	title = {Using {Deep} {Neural} {Networks} for {Inverse} {Problems} in {Imaging}: {Beyond} {Analytical} {Methods}},
	volume = {35},
	issn = {1558-0792},
	shorttitle = {Using {Deep} {Neural} {Networks} for {Inverse} {Problems} in {Imaging}},
	doi = {10.1109/MSP.2017.2760358},
	abstract = {Traditionally, analytical methods have been used to solve imaging problems such as image restoration, inpainting, and superresolution (SR). In recent years, the fields of machine and deep learning have gained a lot of momentum in solving such imaging problems, often surpassing the performance provided by analytical approaches. Unlike analytical methods for which the problem is explicitly defined and domain-knowledge carefully engineered into the solution, deep neural networks (DNNs) do not benefit from such prior knowledge and instead make use of large data sets to learn the unknown solution to the inverse problem. In this article, we review deep-learning techniques for solving such inverse problems in imaging. More specifically, we review the popular neural network architectures used for imaging tasks, offering some insight as to how these deep-learning tools can solve the inverse problem. Furthermore, we address some fundamental questions, such as how deeplearning and analytical methods can be combined to provide better solutions to the inverse problem in addition to providing a discussion on the current limitations and future directions of the use of deep learning for solving inverse problem in imaging.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Lucas, Alice and Iliadis, Michael and Molina, Rafael and Katsaggelos, Aggelos K.},
	month = jan,
	year = {2018},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Analytical models, Biological neural networks, Image reconstruction, Inverse problems, Machine learning, Neural networks, Visual systems},
	pages = {20--36},
}

@misc{alberti_continuous_2022,
	title = {Continuous {Generative} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2205.14627},
	doi = {10.48550/arXiv.2205.14627},
	abstract = {In this work, we present and study Continuous Generative Neural Networks (CGNNs), namely, generative models in the continuous setting. The architecture is inspired by DCGAN, with one fully connected layer, several convolutional layers and nonlinear activation functions. In the continuous \$L{\textasciicircum}2\$ setting, the dimensions of the spaces of each layer are replaced by the scales of a multiresolution analysis of a compactly supported wavelet. We present conditions on the convolutional filters and on the nonlinearity that guarantee that a CGNN is injective. This theory finds applications to inverse problems, and allows for deriving Lipschitz stability estimates for (possibly nonlinear) infinite-dimensional inverse problems with unknowns belonging to the manifold generated by a CGNN. Several numerical simulations, including image deblurring, illustrate and validate this approach.},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Alberti, Giovanni S. and Santacesaria, Matteo and Sciutto, Silvia},
	month = may,
	year = {2022},
	note = {arXiv:2205.14627 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{alberti_learning_2021,
	title = {Learning the optimal {Tikhonov} regularizer for inverse problems},
	url = {http://arxiv.org/abs/2106.06513},
	doi = {10.48550/arXiv.2106.06513},
	abstract = {In this work, we consider the linear inverse problem \$y=Ax+{\textbackslash}epsilon\$, where \$A{\textbackslash}colon X{\textbackslash}to Y\$ is a known linear operator between the separable Hilbert spaces \$X\$ and \$Y\$, \$x\$ is a random variable in \$X\$ and \${\textbackslash}epsilon\$ is a zero-mean random process in \$Y\$. This setting covers several inverse problems in imaging including denoising, deblurring, and X-ray tomography. Within the classical framework of regularization, we focus on the case where the regularization functional is not given a priori but learned from data. Our first result is a characterization of the optimal generalized Tikhonov regularizer, with respect to the mean squared error. We find that it is completely independent of the forward operator \$A\$ and depends only on the mean and covariance of \$x\$. Then, we consider the problem of learning the regularizer from a finite training set in two different frameworks: one supervised, based on samples of both \$x\$ and \$y\$, and one unsupervised, based only on samples of \$x\$. In both cases, we prove generalization bounds, under some weak assumptions on the distribution of \$x\$ and \${\textbackslash}epsilon\$, including the case of sub-Gaussian variables. Our bounds hold in infinite-dimensional spaces, thereby showing that finer and finer discretizations do not make this learning problem harder. The results are validated through numerical simulations.},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Alberti, Giovanni S. and De Vito, Ernesto and Lassas, Matti and Ratti, Luca and Santacesaria, Matteo},
	month = nov,
	year = {2021},
	note = {arXiv:2106.06513 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{alberti_inverse_2022,
	title = {Inverse problems on low-dimensional manifolds},
	url = {http://arxiv.org/abs/2009.00574},
	doi = {10.48550/arXiv.2009.00574},
	abstract = {We consider abstract inverse problems between infinite-dimensional Banach spaces. These inverse problems are typically nonlinear and ill-posed, making the inversion with limited and noisy measurements a delicate process. In this work, we assume that the unknown belongs to a finite-dimensional manifold: this assumption arises in many real-world scenarios where natural objects have a low intrinsic dimension and belong to a certain submanifold of a much larger ambient space. We prove uniqueness and H{\textbackslash}"older and Lipschitz stability results in this general setting, also in the case when only a finite discretization of the measurements is available. Then, a Landweber-type reconstruction algorithm from a finite number of measurements is proposed, for which we prove global convergence, thanks to a new criterion for finding a suitable initial guess. These general results are then applied to several examples, including two classical nonlinear ill-posed inverse boundary value problems. The first is Calder{\textbackslash}'on's inverse conductivity problem, for which we prove a Lipschitz stability estimate from a finite number of measurements for piece-wise constant conductivities with discontinuities on an unknown triangle. A similar stability result is then obtained for Gel'fand-Calder{\textbackslash}'on's problem for the Schr{\textbackslash}"odinger equation, in the case of piece-wise constant potentials with discontinuities on a finite number of non-intersecting balls.},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Alberti, Giovanni S. and Arroyo, Ángel and Santacesaria, Matteo},
	month = apr,
	year = {2022},
	note = {arXiv:2009.00574 [math]},
	keywords = {35R30, 58C25, Mathematics - Analysis of PDEs, Mathematics - Differential Geometry, Mathematics - Functional Analysis},
}

@misc{gulcu_stronger_2019,
	title = {Stronger {Convergence} {Results} for {Deep} {Residual} {Networks}: {Network} {Width} {Scales} {Linearly} with {Training} {Data} {Size}},
	shorttitle = {Stronger {Convergence} {Results} for {Deep} {Residual} {Networks}},
	url = {http://arxiv.org/abs/1911.04351},
	doi = {10.48550/arXiv.1911.04351},
	abstract = {Deep neural networks are highly expressive machine learning models with the ability to interpolate arbitrary datasets. Deep nets are typically optimized via first-order methods and the optimization process crucially depends on the characteristics of the network as well as the dataset. This work sheds light on the relation between the network size and the properties of the dataset with an emphasis on deep residual networks (ResNets). Our contribution is that if the network Jacobian is full rank, gradient descent for the quadratic loss and smooth activation converges to the global minima even if the network width \$m\$ of the ResNet scales linearly with the sample size \$n\$, and independently from the network depth. To the best of our knowledge, this is the first work which provides a theoretical guarantee for the convergence of neural networks in the \$m={\textbackslash}Omega(n)\$ regime.},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Gulcu, Talha Cihad},
	month = nov,
	year = {2019},
	note = {arXiv:1911.04351 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{huang_provably_2021,
	title = {A {Provably} {Convergent} {Scheme} for {Compressive} {Sensing} {Under} {Random} {Generative} {Priors}},
	volume = {27},
	issn = {1531-5851},
	url = {https://doi.org/10.1007/s00041-021-09830-5},
	doi = {10.1007/s00041-021-09830-5},
	abstract = {Deep generative modeling has led to new and state of the art approaches for enforcing structural priors in a variety of inverse problems. In contrast to priors given by sparsity, deep models can provide direct low-dimensional parameterizations of the manifold of images or signals belonging to a particular natural class, allowing for recovery algorithms to be posed in a low-dimensional space. This dimensionality may even be lower than the sparsity level of the same signals when viewed in a fixed basis. What is not known about these methods is whether there are computationally efficient algorithms whose sample complexity is optimal in the dimensionality of the representation given by the generative model. In this paper, we present such an algorithm and analysis. Under the assumption that the generative model is a neural network that is sufficiently expansive at each layer and has Gaussian weights, we provide a gradient descent scheme and prove that for noisy compressive measurements of a signal in the range of the model, the algorithm converges to that signal, up to the noise level. The scaling of the sample complexity with respect to the input dimensionality of the generative prior is linear, and thus can not be improved except for constants and factors of other variables. To the best of the authors’ knowledge, this is the first recovery guarantee for compressive sensing under generative priors by a computationally efficient algorithm.},
	language = {en},
	number = {2},
	urldate = {2022-06-30},
	journal = {Journal of Fourier Analysis and Applications},
	author = {Huang, Wen and Hand, Paul and Heckel, Reinhard and Voroninski, Vladislav},
	month = mar,
	year = {2021},
	keywords = {Compressive sensing, Convergence analysis, Generative models, Gradient descent},
	pages = {19},
}

@article{davidson_local_2001,
	title = {Local operator theory, random matrices and {Banach} spaces},
	volume = {1},
	number = {317-366},
	journal = {Handbook of the geometry of Banach spaces},
	author = {Davidson, Kenneth R and Szarek, Stanislaw J},
	year = {2001},
	note = {Publisher: North-Holland, Amsterdam},
	pages = {131},
}

@book{noauthor_notitle_nodate,
}

@book{noauthor_handbook_2001,
	title = {Handbook of the {Geometry} of {Banach} {Spaces}},
	isbn = {978-0-08-053280-6},
	abstract = {The Handbook presents an overview of most aspects of modernBanach space theory and its applications. The up-to-date surveys, authored by leading research workers in the area, are written to be accessible to a wide audience. In addition to presenting the state of the art of Banach space theory, the surveys discuss the relation of the subject with such areas as harmonic analysis, complex analysis, classical convexity, probability theory, operator theory, combinatorics, logic, geometric measure theory, and partial differential equations. The Handbook begins with a chapter on basic concepts in Banachspace theory which contains all the background needed for reading any other chapter in the Handbook. Each of the twenty one articles in this volume after the basic concepts chapter is devoted to one specific direction of Banach space theory or its applications. Each article contains a motivated introduction as well as an exposition of the main results, methods, and open problems in its specific direction. Most have an extensive bibliography. Many articles contain new proofs of known results as well as expositions of proofs which are hard to locate in the literature or are only outlined in the original research papers. As well as being valuable to experienced researchers in Banach space theory, the Handbook should be an outstanding source for inspiration and information to graduate students and beginning researchers. The Handbook will be useful for mathematicians who want to get an idea of the various developments in Banach space theory.},
	language = {en},
	publisher = {Elsevier},
	month = aug,
	year = {2001},
	note = {Google-Books-ID: 1A7ppEZPXEIC},
	keywords = {Mathematics / Geometry / General, Mathematics / Mathematical Analysis, Mathematics / Probability \& Statistics / General},
}

@article{tropp_introduction_2015,
	title = {An {Introduction} to {Matrix} {Concentration} {Inequalities}},
	url = {http://arxiv.org/abs/1501.01571},
	abstract = {In recent years, random matrices have come to play a major role in computational mathematics, but most of the classical areas of random matrix theory remain the province of experts. Over the last decade, with the advent of matrix concentration inequalities, research has advanced to the point where we can conquer many (formerly) challenging problems with a page or two of arithmetic. The aim of this monograph is to describe the most successful methods from this area along with some interesting examples that these techniques can illuminate.},
	urldate = {2022-04-28},
	journal = {arXiv:1501.01571 [cs, math, stat]},
	author = {Tropp, Joel A.},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.01571},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Information Theory, Mathematics - Numerical Analysis, Mathematics - Probability, Primary: 60B20. Secondary: 60F10, 60G50, 60G42, Statistics - Machine Learning},
}

@misc{noauthor_research_nodate,
	title = {Research {School} on {High} {Dimensional} {Approximation} and {Deep} {Learning} - {Sciencesconf}.org},
	url = {https://hidadeel2022.sciencesconf.org/registration?no_account},
	urldate = {2022-03-29},
}

@article{buskulic_maximizing_2021,
	title = {Maximizing {Drift} {Is} {Not} {Optimal} for {Solving} {OneMax}},
	volume = {29},
	issn = {1063-6560},
	doi = {10.1162/evco_a_00290},
	abstract = {It seems very intuitive that for the maximization of the OneMax problem Om(x):=∑i=1nxi the best that an elitist unary unbiased search algorithm can do is to store a best so far solution, and to modify it with the operator that yields the best possible expected progress in function value. This assumption has been implicitly used in several empirical works. In Doerr et al. (2020), it was formally proven that this approach is indeed almost optimal. In this work, we prove that drift maximization is not optimal. More precisely, we show that for most fitness levels between n/2 and 2n/3 the optimal mutation strengths are larger than the drift-maximizing ones. This implies that the optimal RLS is more risk-affine than the variant maximizing the stepwise expected progress. We show similar results for the mutation rates of the classic (1+1) Evolutionary Algorithm (EA) and its resampling variant, the (1+1) EA{\textgreater}0. As a result of independent interest we show that the optimal mutation strengths, unlike the drift-maximizing ones, can be even.},
	number = {4},
	journal = {Evolutionary Computation},
	author = {Buskulic, Nathan and Doerr, Carola},
	month = dec,
	year = {2021},
	note = {Conference Name: Evolutionary Computation},
	keywords = {Parameter control, black-box complexity., evolutionary computation, runtime analysis},
	pages = {521--541},
}

@inproceedings{baas_loud_2019,
	address = {Cham},
	title = {Loud and {Clear}: {The} {VR} {Game} {Without} {Visuals}},
	isbn = {978-3-030-34350-7},
	shorttitle = {Loud and {Clear}},
	doi = {10.1007/978-3-030-34350-7_18},
	abstract = {While visual impairment is relatively common, most sighted people have no idea of what it is like to live without one of the most heavily utilised senses. We developed the game Loud and Clear in order to have them experience the difficulties of being visually impaired, as well as to put in evidence the abilities blind people have developed, which sighted people mostly lack. In this game without visuals, the player has to rely solely on audio to complete objectives within the game. The game consists of a number of puzzle rooms the player has to solve. These puzzles illustrate the challenges of being blind in a playful setting, and challenge the player to use different auditory skills that are key to achieving objectives without vision, such as sound localisation, sound recognition and spatial orientation. The game uses audio spatialisation techniques to give the player a realistic and immersive auditive experience. Preliminary tests of this game show that players acknowledge the initial high difficulty of ‘living’ as a blind person, to which eventually they were able to somehow adapt. In addition, players reported feeling both immersed and educated by the experience.},
	language = {en},
	booktitle = {Games and {Learning} {Alliance}},
	publisher = {Springer International Publishing},
	author = {Baas, Berend and van Peer, Dennis and Gerling, Jan and Tavasszy, Matthias and Buskulic, Nathan and Salamon, Nestor Z. and Balint, J. Timothy and Bidarra, Rafael},
	editor = {Liapis, Antonios and Yannakakis, Georgios N. and Gentile, Manuel and Ninaus, Manuel},
	year = {2019},
	pages = {180--190},
}

@inproceedings{buskulic_labelling_2021,
	title = {Labelling {Sulcal} {Graphs} {Across} {Indiviuals} {Using} {Multigraph} {Matching}},
	doi = {10.1109/ISBI48211.2021.9434035},
	abstract = {The problem of inter-individual comparison is of major importance in neuroimaging to detect patterns indicative of neurological pathology. Few works have been addressing the comparison of individual sulcal graphs in which variations across subjects manifest as changes in the number of nodes, graph topology and in the attributes that can be attached to nodes and edges. Here, we quantitatively evaluated different graph matching approaches in both the pairwise and multigraph matching frameworks, on synthetic graphs simulating the structure and attributes distributions of real data. Our results show that multigraph matching approach outperforms pairwise techniques in all simulations. The application to a set of real sulcal graphs from 134 subjects confirms this observation and demonstrates that multigraph matching approaches can scale and have a great potential in this context.},
	booktitle = {2021 {IEEE} 18th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Buskulic, N. and Dupé, F.X. and Takerkart, S. and Auzias, G.},
	month = apr,
	year = {2021},
	note = {ISSN: 1945-8452},
	keywords = {Brain, Data models, Graph Matching, Image edge detection, Labeling, MRI, Morphometry, Neuroimaging, Pathology, Scalability, Topology},
	pages = {1486--1490},
}

@book{ledoux_concentration_nodate,
	address = {Providence, RI},
	edition = {Nachdr.},
	series = {Mathematical surveys and monographs},
	title = {The concentration of measure phenomenon},
	isbn = {978-0-8218-3792-4 978-0-8218-2864-9},
	language = {eng},
	number = {89},
	publisher = {American Mathematical Society},
	author = {Ledoux, Michel},
}

@book{ledoux_concentration_nodate-1,
	address = {Providence, RI},
	edition = {Nachdr.},
	series = {Mathematical surveys and monographs},
	title = {The concentration of measure phenomenon},
	isbn = {978-0-8218-2864-9 978-0-8218-3792-4},
	language = {eng},
	number = {89},
	publisher = {American Mathematical Society},
	author = {Ledoux, Michel},
}

@book{ledoux_concentration_2001,
	address = {Providence (R.I.)},
	series = {Mathematical {Surveys} and {Monographs}},
	title = {The concentration of measure phenomenon},
	isbn = {978-0-8218-2864-9},
	language = {eng},
	number = {89},
	publisher = {American mathematical society},
	author = {Ledoux, Michel},
	year = {2001},
}

@book{noauthor_positive_2015,
	title = {Positive {Definite} {Matrices}},
	isbn = {978-0-691-16825-8},
	url = {https://press.princeton.edu/books/paperback/9780691168258/positive-definite-matrices},
	language = {en},
	urldate = {2022-02-10},
	month = sep,
	year = {2015},
}

@book{bhatia_matrix_1997,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Matrix {Analysis}},
	volume = {169},
	isbn = {978-1-4612-6857-4 978-1-4612-0653-8},
	url = {http://link.springer.com/10.1007/978-1-4612-0653-8},
	urldate = {2022-02-10},
	publisher = {Springer New York},
	author = {Bhatia, Rajendra},
	year = {1997},
	doi = {10.1007/978-1-4612-0653-8},
}

@book{brezis_analyse_nodate,
	title = {Analyse {Fonctionelle}},
	author = {Brezis, Haïm},
}

@book{scholkopf_learning_2002,
	address = {Cambridge, Mass.},
	edition = {Reprint.},
	series = {Adaptive computation and machine learning series},
	title = {Learning with kernels: support vector machines, regularization, optimization, and beyond},
	isbn = {978-0-262-53657-8 978-0-262-19475-4},
	shorttitle = {Learning with kernels},
	language = {eng},
	publisher = {MIT Press},
	author = {Schölkopf, Bernhard and Smola, Alexander Johannes and Smola, Alexander J.},
	year = {2002},
}

@book{scholkopf_learning_2003,
	address = {Berlin Heidelberg},
	series = {Lecture notes in computer science {Lecture} notes in artificial intelligence},
	title = {Learning theory and {Kernel} machines: 16th {Annual} {Conference} on {Learning} {Theory} and 7th {Kernel} {Workshop}, {COLT}/{Kernel} 2003, {Washington}, {DC}, {USA}, {August} 24-27, 2003 ; proceedings},
	isbn = {978-3-540-40720-1},
	shorttitle = {Learning theory and {Kernel} machines},
	language = {eng},
	number = {2777},
	publisher = {Springer},
	editor = {Schölkopf, Bernhard and Warmuth, Manfred K.},
	year = {2003},
	note = {Meeting Name: Conference on Learning Theory},
}

@article{pinetz_shared_2020,
	title = {Shared {Prior} {Learning} of {Energy}-{Based} {Models} for {Image} {Reconstruction}},
	url = {http://arxiv.org/abs/2011.06539},
	abstract = {We propose a novel learning-based framework for image reconstruction particularly designed for training without ground truth data, which has three major building blocks: energy-based learning, a patch-based Wasserstein loss functional, and shared prior learning. In energy-based learning, the parameters of an energy functional composed of a learned data fidelity term and a data-driven regularizer are computed in a mean-field optimal control problem. In the absence of ground truth data, we change the loss functional to a patch-based Wasserstein functional, in which local statistics of the output images are compared to uncorrupted reference patches. Finally, in shared prior learning, both aforementioned optimal control problems are optimized simultaneously with shared learned parameters of the regularizer to further enhance unsupervised image reconstruction. We derive several time discretization schemes of the gradient flow and verify their consistency in terms of Mosco convergence. In numerous numerical experiments, we demonstrate that the proposed method generates state-of-the-art results for various image reconstruction applications--even if no ground truth images are available for training.},
	urldate = {2022-01-05},
	journal = {arXiv:2011.06539 [cs, eess, math]},
	author = {Pinetz, Thomas and Kobler, Erich and Pock, Thomas and Effland, Alexander},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.06539},
	keywords = {49J15, 65C30, 65K10, 65L09, 68U10, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
}

@unpublished{batard_dip-vbtv_2021,
	title = {{DIP}-{VBTV}: {A} {Color} {Image} {Restoration} {Model} combining a {Deep} {Image} {Prior} and a {Vector} {Bundle} {Total} {Variation}},
	shorttitle = {{DIP}-{VBTV}},
	url = {https://hal.archives-ouvertes.fr/hal-02994439},
	abstract = {In this paper, we introduce a new variational model for color image restoration, called DIP-VBTV, which combines two priors: a deep image prior (DIP), which assumes that the restored image can be generated through a neural network and a Vector Bundle Total Variation (VBTV) which generalizes the Vectorial Total Variation (VTV) on vector bundles. VBTV is determined by a geometric triplet: a Riemannian metric on the base manifold, a covariant derivative and a metric on the vector bundle. Whereas VTV prior encourages the restored images to be piece-wise constant, VBTV prior encourages them to be piece-wise parallel with respect to a covariant derivative. For well-chosen geometric triplets, we show that the minimization of VBTV encourages the solutions of the restoration model to share some visual content with the clean image. Then, we show on experiments that DIP-VBTV benefits from this property by outperforming DIP-VTV and state-of-the-art unsupervised methods, which demonstrates the relevance of combining DIP and VBTV priors.},
	urldate = {2022-01-05},
	author = {Batard, Thomas and Haro, Gloria and Ballester, Coloma},
	month = may,
	year = {2021},
}

@article{mei_mean-field_2019,
	title = {Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
	shorttitle = {Mean-field theory of two-layers neural networks},
	url = {http://arxiv.org/abs/1902.06015},
	abstract = {We consider learning two layer neural networks using stochastic gradient descent. The mean-field description of this learning dynamics approximates the evolution of the network weights by an evolution in the space of probability distributions in \$R{\textasciicircum}D\$ (where \$D\$ is the number of parameters associated to each neuron). This evolution can be defined through a partial differential equation or, equivalently, as the gradient flow in the Wasserstein space of probability distributions. Earlier work shows that (under some regularity assumptions), the mean field description is accurate as soon as the number of hidden units is much larger than the dimension \$D\$. In this paper we establish stronger and more general approximation guarantees. First of all, we show that the number of hidden units only needs to be larger than a quantity dependent on the regularity properties of the data, and independent of the dimensions. Next, we generalize this analysis to the case of unbounded activation functions, which was not covered by earlier bounds. We extend our results to noisy stochastic gradient descent. Finally, we show that kernel ridge regression can be recovered as a special limit of the mean field analysis.},
	urldate = {2022-01-03},
	journal = {arXiv:1902.06015 [cond-mat, stat]},
	author = {Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06015},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@inproceedings{cheng_bayesian_2019-1,
	title = {A {Bayesian} {Perspective} on the {Deep} {Image} {Prior}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Cheng_A_Bayesian_Perspective_on_the_Deep_Image_Prior_CVPR_2019_paper.html},
	urldate = {2021-12-15},
	author = {Cheng, Zezhou and Gadelha, Matheus and Maji, Subhransu and Sheldon, Daniel},
	year = {2019},
	pages = {5443--5451},
}

@article{mukherjee_end--end_2021,
	title = {End-to-end reconstruction meets data-driven regularization for inverse problems},
	url = {http://arxiv.org/abs/2106.03538},
	abstract = {We propose an unsupervised approach for learning end-to-end reconstruction operators for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling, which essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled reconstruction operator. The variational problem is then initialized with the reconstruction of the unrolled operator and solved iteratively till convergence. Notably, it takes significantly fewer iterations to converge, thanks to the excellent initialization obtained via the unrolled operator. The resulting approach combines the computational efficiency of end-to-end unrolled reconstruction with the well-posedness and noise-stability guarantees of the variational setting. Moreover, we demonstrate with the example of X-ray computed tomography (CT) that our approach outperforms state-of-the-art unsupervised methods, and that it outperforms or is on par with state-of-the-art supervised learned reconstruction approaches.},
	urldate = {2021-12-14},
	journal = {arXiv:2106.03538 [cs, math]},
	author = {Mukherjee, Subhadip and Carioni, Marcello and Öktem, Ozan and Schönlieb, Carola-Bibiane},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.03538},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@inproceedings{guo_agem_2019,
	title = {{AGEM}: {Solving} {Linear} {Inverse} {Problems} via {Deep} {Priors} and {Sampling}},
	volume = {32},
	shorttitle = {{AGEM}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/49182f81e6a13cf5eaa496d51fea6406-Abstract.html},
	urldate = {2021-12-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Guo, Bichuan and Han, Yuxing and Wen, Jiangtao},
	year = {2019},
}

@article{kawar_snips_2021,
	title = {{SNIPS}: {Solving} {Noisy} {Inverse} {Problems} {Stochastically}},
	shorttitle = {{SNIPS}},
	url = {http://arxiv.org/abs/2105.14951},
	abstract = {In this work we introduce a novel stochastic algorithm dubbed SNIPS, which draws samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. Our solution incorporates ideas from Langevin dynamics and Newton's method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. Due to its stochasticity, the algorithm can produce multiple high perceptual quality samples for the same noisy observation. We demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing. We show that the samples produced are sharp, detailed and consistent with the given measurements, and their diversity exposes the inherent uncertainty in the inverse problem being solved.},
	urldate = {2021-12-13},
	journal = {arXiv:2105.14951 [cs, eess]},
	author = {Kawar, Bahjat and Vaksman, Gregory and Elad, Michael},
	month = nov,
	year = {2021},
	note = {arXiv: 2105.14951},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{lunz_adversarial_2019,
	title = {Adversarial {Regularizers} in {Inverse} {Problems}},
	url = {http://arxiv.org/abs/1805.11572},
	abstract = {Inverse Problems in medical imaging and computer vision are traditionally solved using purely model-based methods. Among those variational regularization models are one of the most popular approaches. We propose a new framework for applying data-driven approaches to inverse problems, using a neural network as a regularization functional. The network learns to discriminate between the distribution of ground truth images and the distribution of unregularized reconstructions. Once trained, the network is applied to the inverse problem by solving the corresponding variational problem. Unlike other data-based approaches for inverse problems, the algorithm can be applied even if only unsupervised training data is available. Experiments demonstrate the potential of the framework for denoising on the BSDS dataset and for computed tomography reconstruction on the LIDC dataset.},
	urldate = {2021-12-13},
	journal = {arXiv:1805.11572 [cs, math, stat]},
	author = {Lunz, Sebastian and Öktem, Ozan and Schönlieb, Carola-Bibiane},
	month = jan,
	year = {2019},
	note = {arXiv: 1805.11572},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{chen_deep_2022,
	title = {Deep {Photometric} {Stereo} for {Non}-{Lambertian} {Surfaces}},
	volume = {44},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.3005397},
	abstract = {This paper addresses the problem of photometric stereo, in both calibrated and uncalibrated scenarios, for non-Lambertian surfaces based on deep learning. We first introduce a fully convolutional deep network for calibrated photometric stereo, which we call PS-FCN. Unlike traditional approaches that adopt simplified reflectance models to make the problem tractable, our method directly learns the mapping from reflectance observations to surface normal, and is able to handle surfaces with general and unknown isotropic reflectance. At test time, PS-FCN takes an arbitrary number of images and their associated light directions as input and predicts a surface normal map of the scene in a fast feed-forward pass. To deal with the uncalibrated scenario where light directions are unknown, we introduce a new convolutional network, named LCNet, to estimate light directions from input images. The estimated light directions and the input images are then fed to PS-FCN to determine the surface normals. Our method does not require a pre-defined set of light directions and can handle multiple images in an order-agnostic manner. Thorough evaluation of our approach on both synthetic and real datasets shows that it outperforms state-of-the-art methods in both calibrated and uncalibrated scenarios.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, Guanying and Han, Kai and Shi, Boxin and Matsushita, Yasuyuki and Wong, Kwan-Yee K.},
	month = jan,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Analytical models, Deep learning, Estimation, Lighting, Photometric stereo, Shape, Testing, Training, convolutional neural network, non-Lambertian, uncalibrated},
	pages = {129--142},
}

@article{heckel_deep_2019,
	title = {Deep {Decoder}: {Concise} {Image} {Representations} from {Untrained} {Non}-convolutional {Networks}},
	shorttitle = {Deep {Decoder}},
	url = {http://arxiv.org/abs/1810.03982},
	abstract = {Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.},
	urldate = {2021-12-08},
	journal = {arXiv:1810.03982 [cs, stat]},
	author = {Heckel, Reinhard and Hand, Paul},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.03982},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{diamond_unrolled_2018,
	title = {Unrolled {Optimization} with {Deep} {Priors}},
	url = {http://arxiv.org/abs/1705.08041},
	abstract = {A broad class of problems at the core of computational imaging, sensing, and low-level computer vision reduces to the inverse problem of extracting latent images that follow a prior distribution, from measurements taken under a known physical image formation model. Traditionally, hand-crafted priors along with iterative optimization methods have been used to solve such problems. In this paper we present unrolled optimization with deep priors, a principled framework for infusing knowledge of the image formation into deep networks that solve inverse problems in imaging, inspired by classical iterative methods. We show that instances of the framework outperform the state-of-the-art by a substantial margin for a wide variety of imaging problems, such as denoising, deblurring, and compressed sensing magnetic resonance imaging (MRI). Moreover, we conduct experiments that explain how the framework is best used and why it outperforms previous methods.},
	urldate = {2021-12-08},
	journal = {arXiv:1705.08041 [cs]},
	author = {Diamond, Steven and Sitzmann, Vincent and Heide, Felix and Wetzstein, Gordon},
	month = dec,
	year = {2018},
	note = {arXiv: 1705.08041},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{benning_modern_2018,
	title = {Modern {Regularization} {Methods} for {Inverse} {Problems}},
	url = {http://arxiv.org/abs/1801.09922},
	abstract = {Regularization methods are a key tool in the solution of inverse problems. They are used to introduce prior knowledge and make the approximation of ill-posed (pseudo)inverses feasible. In the last two decades interest has shifted from linear towards nonlinear regularization methods even for linear inverse problems. The aim of this paper is to provide a reasonably comprehensive overview of this development towards modern nonlinear regularization methods, including their analysis, applications, and issues for future research.},
	language = {en},
	urldate = {2021-12-06},
	journal = {arXiv:1801.09922 [math]},
	author = {Benning, Martin and Burger, Martin},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.09922},
	keywords = {00A69, 35R30, 47A52, 47J30, 49J40, 49N45, 49R05, 65F22, 65J20, 65J22, 65K10, 65K15, 65M32, 65N21, 65R32, 90C26, 94A08, Mathematics - Numerical Analysis},
}

@article{benning_modern_2018-1,
	title = {Modern regularization methods for inverse problems},
	volume = {27},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/product/identifier/S0962492918000016/type/journal_article},
	doi = {10.1017/S0962492918000016},
	abstract = {Regularization methods are a key tool in the solution of inverse problems. They are used to introduce prior knowledge and allow a robust approximation of ill-posed (pseudo-) inverses. In the last two decades interest has shifted from linear to nonlinear regularization methods, even for linear inverse problems. The aim of this paper is to provide a reasonably comprehensive overview of this shift towards modern nonlinear regularization methods, including their analysis, applications and issues for future research.
            In particular we will discuss variational methods and techniques derived from them, since they have attracted much recent interest and link to other fields, such as image processing and compressed sensing. We further point to developments related to statistical inverse problems, multiscale decompositions and learning theory.},
	language = {en},
	urldate = {2021-12-06},
	journal = {Acta Numerica},
	author = {Benning, Martin and Burger, Martin},
	month = may,
	year = {2018},
	pages = {1--111},
}

@inproceedings{welling_bayesian_2011,
	address = {Madison, WI, USA},
	series = {{ICML}'11},
	title = {Bayesian learning via stochastic gradient langevin dynamics},
	isbn = {978-1-4503-0619-5},
	abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
	urldate = {2021-12-03},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Welling, Max and Teh, Yee Whye},
	month = jun,
	year = {2011},
	pages = {681--688},
}

@article{hagemann_stochastic_2021,
	title = {Stochastic {Normalizing} {Flows} for {Inverse} {Problems}: a {Markov} {Chains} {Viewpoint}},
	shorttitle = {Stochastic {Normalizing} {Flows} for {Inverse} {Problems}},
	url = {http://arxiv.org/abs/2109.11375},
	abstract = {To overcome topological constraints and improve the expressiveness of normalizing flow architectures, Wu, K{\textbackslash}"ohler and No{\textbackslash}'e introduced stochastic normalizing flows which combine deterministic, learnable flow transformations with stochastic sampling methods. In this paper, we consider stochastic normalizing flows from a Markov chain point of view. In particular, we replace transition densities by general Markov kernels and establish proofs via Radon-Nikodym derivatives which allows to incorporate distributions without densities in a sound way. Further, we generalize the results for sampling from posterior distributions as required in inverse problems. The performance of the proposed conditional stochastic normalizing flow is demonstrated by numerical examples.},
	urldate = {2021-12-02},
	journal = {arXiv:2109.11375 [cs, math]},
	author = {Hagemann, Paul and Hertrich, Johannes and Steidl, Gabriele},
	month = nov,
	year = {2021},
	note = {arXiv: 2109.11375},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability},
}

@article{wu_stochastic_2020,
	title = {Stochastic {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/2002.06707},
	abstract = {The sampling of probability distributions specified up to a normalization constant is an important problem in both machine learning and statistical mechanics. While classical stochastic sampling methods such as Markov Chain Monte Carlo (MCMC) or Langevin Dynamics (LD) can suffer from slow mixing times there is a growing interest in using normalizing flows in order to learn the transformation of a simple prior distribution to the given target distribution. Here we propose a generalized and combined approach to sample target densities: Stochastic Normalizing Flows (SNF) -- an arbitrary sequence of deterministic invertible functions and stochastic sampling blocks. We show that stochasticity overcomes expressivity limitations of normalizing flows resulting from the invertibility constraint, whereas trainable transformations between sampling steps improve efficiency of pure MCMC/LD along the flow. By invoking ideas from non-equilibrium statistical mechanics we derive an efficient training procedure by which both the sampler's and the flow's parameters can be optimized end-to-end, and by which we can compute exact importance weights without having to marginalize out the randomness of the stochastic blocks. We illustrate the representational power, sampling efficiency and asymptotic correctness of SNFs on several benchmarks including applications to sampling molecular systems in equilibrium.},
	urldate = {2021-11-30},
	journal = {arXiv:2002.06707 [physics, stat]},
	author = {Wu, Hao and Köhler, Jonas and Noé, Frank},
	month = oct,
	year = {2020},
	note = {arXiv: 2002.06707},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
}

@article{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2021-11-30},
	journal = {arXiv:1806.07366 [cs, stat]},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {arXiv: 1806.07366},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kobyzev_normalizing_2021,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Normalizing {Flows}},
	url = {http://arxiv.org/abs/1908.09257},
	doi = {10.1109/TPAMI.2020.2992934},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	number = {11},
	urldate = {2021-11-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
	month = nov,
	year = {2021},
	note = {arXiv: 1908.09257},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3964--3979},
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2021-11-19},
	journal = {arXiv:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv: 1701.07875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chambolle_introduction_2016,
	series = {Acta {Numerica}},
	title = {An introduction to continuous optimization for imaging},
	volume = {25},
	url = {https://hal.archives-ouvertes.fr/hal-01346507},
	doi = {10.1017/S096249291600009X},
	abstract = {A large number of imaging problems reduce to the optimization of a cost function , with typical structural properties. The aim of this paper is to describe the state of the art in continuous optimization methods for such problems, and present the most successful approaches and their interconnections. We place particular emphasis on optimal first-order schemes that can deal with typical non-smooth and large-scale objective functions used in imaging problems. We illustrate and compare the different algorithms using classical non-smooth problems in imaging, such as denoising and deblurring. Moreover, we present applications of the algorithms to more advanced problems, such as magnetic resonance imaging, multilabel image segmentation, optical flow estimation, stereo matching, and classification.},
	urldate = {2021-11-17},
	journal = {Acta Numerica},
	author = {Chambolle, Antonin and Pock, Thomas},
	year = {2016},
	note = {Publisher: Cambridge University Press (CUP)},
	keywords = {algorithms, convex analysis, imaging, nonsmooth optimization},
	pages = {161--319},
}

@article{bischof_multi-objective_2021,
	title = {Multi-{Objective} {Loss} {Balancing} for {Physics}-{Informed} {Deep} {Learning}},
	url = {https://arxiv.org/abs/2110.09813v1},
	doi = {10.13140/RG.2.2.20057.24169},
	abstract = {Physics Informed Neural Networks (PINN) are algorithms from deep learning leveraging physical laws by including partial differential equations (PDE) together with a respective set of boundary and initial conditions (BC / IC) as penalty terms into their loss function. As the PDE, BC and IC loss function parts can significantly differ in magnitudes, due to their underlying physical units or stochasticity of initialisation, training of PINNs may suffer from severe convergence and efficiency problems, causing PINNs to stay beyond desirable approximation quality. In this work, we observe the significant role of correctly weighting the combination of multiple competitive loss functions for training PINNs effectively. To that end, we implement and evaluate different methods aiming at balancing the contributions of multiple terms of the PINNs loss function and their gradients. After review of three existing loss scaling approaches (Learning Rate Annealing, GradNorm as well as SoftAdapt), we propose a novel self-adaptive loss balancing of PINNs called ReLoBRaLo (Relative Loss Balancing with Random Lookback). Finally, the performance of ReLoBRaLo is compared and verified against these approaches by solving both forward as well as inverse problems on three benchmark PDEs for PINNs: Burgers' equation, Kirchhoff's plate bending equation and Helmholtz's equation. Our simulation studies show that ReLoBRaLo training is much faster and achieves higher accuracy than training PINNs with other balancing methods and hence is very effective and increases sustainability of PINNs algorithms. The adaptability of ReLoBRaLo illustrates robustness across different PDE problem settings. The proposed method can also be employed to the wider class of penalised optimisation problems, including PDE-constrained and Sobolev training apart from the studied PINNs examples.},
	language = {en},
	urldate = {2021-11-10},
	author = {Bischof, Rafael and Kraus, Michael},
	month = oct,
	year = {2021},
}

@article{wang_fast_2021,
	title = {Fast {PDE}-constrained optimization via self-supervised operator learning},
	url = {https://arxiv.org/abs/2110.13297v1},
	abstract = {Design and optimal control problems are among the fundamental, ubiquitous tasks we face in science and engineering. In both cases, we aim to represent and optimize an unknown (black-box) function that associates a performance/outcome to a set of controllable variables through an experiment. In cases where the experimental dynamics can be described by partial differential equations (PDEs), such problems can be mathematically translated into PDE-constrained optimization tasks, which quickly become intractable as the number of control variables and the cost of experiments increases. In this work we leverage physics-informed deep operator networks (DeepONets) -- a self-supervised framework for learning the solution operator of parametric PDEs -- to build fast and differentiable surrogates for rapidly solving PDE-constrained optimization problems, even in the absence of any paired input-output training data. The effectiveness of the proposed framework will be demonstrated across different applications involving continuous functions as control or design variables, including time-dependent optimal control of heat transfer, and drag minimization of obstacles in Stokes flow. In all cases, we observe that DeepONets can minimize high-dimensional cost functionals in a matter of seconds, yielding a significant speed up compared to traditional adjoint PDE solvers that are typically costly and limited to relatively low-dimensional control/design parametrizations.},
	language = {en},
	urldate = {2021-11-10},
	author = {Wang, Sifan and Bhouri, Mohamed Aziz and Perdikaris, Paris},
	month = oct,
	year = {2021},
}

@article{lopez-tapia_deep_2021,
	title = {Deep learning approaches to inverse problems in imaging: {Past}, present and future},
	issn = {1051-2004},
	shorttitle = {Deep learning approaches to inverse problems in imaging},
	url = {https://www.sciencedirect.com/science/article/pii/S1051200421003249},
	doi = {10.1016/j.dsp.2021.103285},
	abstract = {In recent years, deep learning-based models have gained momentum in imaging problems such as image and video super-resolution, image restoration or inpainting. The analytical approaches that have traditionally been used to solve image inverse problems have started to be replaced by deep learning ones, being outperformed in terms of efficacy and efficiency in many applications. However, deep learning-based models lack the adaptability of analytical models, thus making them unsuitable for dealing simultaneously with different forward image formation models. In contrast to analytical methods, deep learning models typically do not use domain knowledge and rely on learning the solution to the inverse problem from large data sets. This is making them susceptible to errors caused by the presence of degradations not seen during training. Hybrid models combining analytical and deep learning approaches have been introduced to solve such generalization issues while retaining the efficacy of deep learning models. In this work, we review deep learning and hybrid methods for solving imaging inverse problems, focusing on image and video super-resolution and image restoration. Furthermore, we discuss open problems in this area that would be of critical importance in the future, the challenges of applying deep learning models to solve them, and how future research could address them.},
	language = {en},
	urldate = {2021-11-10},
	journal = {Digital Signal Processing},
	author = {López-Tapia, Santiago and Molina, Rafael and Katsaggelos, Aggelos K.},
	month = oct,
	year = {2021},
	keywords = {Convolutional neural network, Deep learning, Inverse imaging problems, Video super-resolution},
	pages = {103285},
}

@article{zhang_general_2021,
	title = {A {General} {Framework} for {Inverse} {Problem} {Solving} using {Self}-{Supervised} {Deep} {Learning}: {Validations} in {Ultrasound} and {Photoacoustic} {Image} {Reconstruction}},
	shorttitle = {A {General} {Framework} for {Inverse} {Problem} {Solving} using {Self}-{Supervised} {Deep} {Learning}},
	url = {https://arxiv.org/abs/2110.14970v1},
	abstract = {The image reconstruction process in medical imaging can be treated as solving an inverse problem. The inverse problem is usually solved using time-consuming iterative algorithms with sparsity or other constraints. Recently, deep neural network (DNN)-based methods have been developed to accelerate the inverse-problem-solving process. However, these methods typically adopt supervised learning scheme, which requires ground truths, or labels of the solutions, for training. In many applications, it would be challenging or even impossible to obtain the ground truth, such as the tissue reflectivity function in ultrasound beamforming. In this study, a general framework based on self-supervised learning (SSL) scheme is proposed to train a DNN to solve the inverse problems. In this way, the measurements can be used as both the inputs and the labels during the training of DNN. The proposed SSL method is applied to four typical linear inverse problems for validation, i.e., plane wave ultrasound and photoacoustic image reconstructions, compressed sensing-based synthetic transmit aperture dataset recovery and deconvolution in ultrasound localization microscopy. Results show that, using the proposed framework, the trained DNN can achieve improved reconstruction accuracy with reduced computational time, compared with conventional methods.},
	language = {en},
	urldate = {2021-11-10},
	author = {Zhang, Jingke and He, Qiong and Wang, Congzhi and Liao, Hongen and Luo, Jianwen},
	month = oct,
	year = {2021},
}

@article{bajaj_robust_2021,
	title = {Robust {Learning} of {Physics} {Informed} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2110.13330v1},
	abstract = {Physics-informed Neural Networks (PINNs) have been shown to be effective in solving partial differential equations by capturing the physics induced constraints as a part of the training loss function. This paper shows that a PINN can be sensitive to errors in training data and overfit itself in dynamically propagating these errors over the domain of the solution of the PDE. It also shows how physical regularizations based on continuity criteria and conservation laws fail to address this issue and rather introduce problems of their own causing the deep network to converge to a physics-obeying local minimum instead of the global minimum. We introduce Gaussian Process (GP) based smoothing that recovers the performance of a PINN and promises a robust architecture against noise/errors in measurements. Additionally, we illustrate an inexpensive method of quantifying the evolution of uncertainty based on the variance estimation of GPs on boundary data. Robust PINN performance is also shown to be achievable by choice of sparse sets of inducing points based on sparsely induced GPs. We demonstrate the performance of our proposed methods and compare the results from existing benchmark models in literature for time-dependent Schr{\textbackslash}"odinger and Burgers' equations.},
	language = {en},
	urldate = {2021-11-10},
	author = {Bajaj, Chandrajit and McLennan, Luke and Andeen, Timothy and Roy, Avik},
	month = oct,
	year = {2021},
}

@article{yu_gradient-enhanced_2021,
	title = {Gradient-enhanced physics-informed neural networks for forward and inverse {PDE} problems},
	url = {https://arxiv.org/abs/2111.02801v1},
	abstract = {Deep learning has been shown to be an effective tool in solving partial differential equations (PDEs) through physics-informed neural networks (PINNs). PINNs embed the PDE residual into the loss function of the neural network, and have been successfully employed to solve diverse forward and inverse PDE problems. However, one disadvantage of the first generation of PINNs is that they usually have limited accuracy even with many training points. Here, we propose a new method, gradient-enhanced physics-informed neural networks (gPINNs), for improving the accuracy and training efficiency of PINNs. gPINNs leverage gradient information of the PDE residual and embed the gradient into the loss function. We tested gPINNs extensively and demonstrated the effectiveness of gPINNs in both forward and inverse PDE problems. Our numerical results show that gPINN performs better than PINN with fewer training points. Furthermore, we combined gPINN with the method of residual-based adaptive refinement (RAR), a method for improving the distribution of training points adaptively during training, to further improve the performance of gPINN, especially in PDEs with solutions that have steep gradients.},
	language = {en},
	urldate = {2021-11-10},
	author = {Yu, Jeremy and Lu, Lu and Meng, Xuhui and Karniadakis, George Em},
	month = nov,
	year = {2021},
}

@article{huang_solving_2021,
	title = {Solving {Partial} {Differential} {Equations} with {Point} {Source} {Based} on {Physics}-{Informed} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2111.01394v1},
	abstract = {In recent years, deep learning technology has been used to solve partial differential equations (PDEs), among which the physics-informed neural networks (PINNs) emerges to be a promising method for solving both forward and inverse PDE problems. PDEs with a point source that is expressed as a Dirac delta function in the governing equations are mathematical models of many physical processes. However, they cannot be solved directly by conventional PINNs method due to the singularity brought by the Dirac delta function. We propose a universal solution to tackle this problem with three novel techniques. Firstly the Dirac delta function is modeled as a continuous probability density function to eliminate the singularity; secondly a lower bound constrained uncertainty weighting algorithm is proposed to balance the PINNs losses between point source area and other areas; and thirdly a multi-scale deep neural network with periodic activation function is used to improve the accuracy and convergence speed of the PINNs method. We evaluate the proposed method with three representative PDEs, and the experimental results show that our method outperforms existing deep learning-based methods with respect to the accuracy, the efficiency and the versatility.},
	language = {en},
	urldate = {2021-11-10},
	author = {Huang, Xiang and Liu, Hongsheng and Shi, Beiji and Wang, Zidong and Yang, Kang and Li, Yang and Weng, Bingya and Wang, Min and Chu, Haotian and Zhou, Jing and Yu, Fan and Hua, Bei and Chen, Lei and Dong, Bin},
	month = nov,
	year = {2021},
}

@article{raissi_physics_2017,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {I})},
	url = {http://arxiv.org/abs/1711.10561},
	abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.},
	urldate = {2021-11-10},
	journal = {arXiv:1711.10561 [cs, math, stat]},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10561},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{aggarwal_modl_2019,
	title = {{MoDL}: {Model}-{Based} {Deep} {Learning} {Architecture} for {Inverse} {Problems}},
	volume = {38},
	issn = {1558-254X},
	shorttitle = {{MoDL}},
	doi = {10.1109/TMI.2018.2865356},
	abstract = {We introduce a model-based image reconstruction framework with a convolution neural network (CNN)-based regularization prior. The proposed formulation provides a systematic approach for deriving deep architectures for inverse problems with the arbitrary structure. Since the forward model is explicitly accounted for, a smaller network with fewer parameters is sufficient to capture the image information compared to direct inversion approaches. Thus, reducing the demand for training data and training time. Since we rely on end-to-end training with weight sharing across iterations, the CNN weights are customized to the forward model, thus offering improved performance over approaches that rely on pre-trained denoisers. Our experiments show that the decoupling of the number of iterations from the network complexity offered by this approach provides benefits, including lower demand for training data, reduced risk of overfitting, and implementations with significantly reduced memory footprint. We propose to enforce data-consistency by using numerical optimization blocks, such as conjugate gradients algorithm within the network. This approach offers faster convergence per iteration, compared to methods that rely on proximal gradients steps to enforce data consistency. Our experiments show that the faster convergence translates to improved performance, primarily when the available GPU memory restricts the number of iterations.},
	number = {2},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Aggarwal, Hemant K. and Mani, Merry P. and Jacob, Mathews},
	month = feb,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Deep learning, Image reconstruction, Imaging, Machine learning, Numerical models, Optimization, Training, Training data, convolutional neural network, parallel imaging},
	pages = {394--405},
}

@article{liu_sgd-net_2021,
	title = {{SGD}-{Net}: {Efficient} {Model}-{Based} {Deep} {Learning} with {Theoretical} {Guarantees}},
	volume = {7},
	issn = {2333-9403, 2334-0118, 2573-0436},
	shorttitle = {{SGD}-{Net}},
	url = {http://arxiv.org/abs/2101.09379},
	doi = {10.1109/TCI.2021.3085534},
	abstract = {Deep unfolding networks have recently gained popularity in the context of solving imaging inverse problems. However, the computational and memory complexity of data-consistency layers within traditional deep unfolding networks scales with the number of measurements, limiting their applicability to large-scale imaging inverse problems. We propose SGD-Net as a new methodology for improving the efficiency of deep unfolding through stochastic approximations of the data-consistency layers. Our theoretical analysis shows that SGD-Net can be trained to approximate batch deep unfolding networks to an arbitrary precision. Our numerical results on intensity diffraction tomography and sparse-view computed tomography show that SGD-Net can match the performance of the batch network at a fraction of training and testing complexity.},
	urldate = {2021-09-13},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Liu, Jiaming and Sun, Yu and Gan, Weijie and Xu, Xiaojian and Wohlberg, Brendt and Kamilov, Ulugbek S.},
	year = {2021},
	note = {arXiv: 2101.09379},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Lu  - 1, To\_Print},
	pages = {598--610},
}

@article{liu_recovery_2021,
	title = {Recovery {Analysis} for {Plug}-and-{Play} {Priors} using the {Restricted} {Eigenvalue} {Condition}},
	url = {http://arxiv.org/abs/2106.03668},
	abstract = {The plug-and-play priors (PnP) and regularization by denoising (RED) methods have become widely used for solving inverse problems by leveraging pre-trained deep denoisers as image priors. While the empirical imaging performance and the theoretical convergence properties of these algorithms have been widely investigated, their recovery properties have not previously been theoretically analyzed. We address this gap by showing how to establish theoretical recovery guarantees for PnP/RED by assuming that the solution of these methods lies near the fixed-points of a deep neural network. We also present numerical results comparing the recovery performance of PnP/RED in compressive sensing against that of recent compressive sensing algorithms based on generative models. Our numerical results suggest that PnP with a pre-trained artifact removal network provides significantly better results compared to the existing state-of-the-art methods.},
	urldate = {2021-09-13},
	journal = {arXiv:2106.03668 [cs, eess]},
	author = {Liu, Jiaming and Asif, M. Salman and Wohlberg, Brendt and Kamilov, Ulugbek S.},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.03668},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Electrical Engineering and Systems Science - Signal Processing, Lu - 1},
}

@article{alt_connections_2021,
	title = {Connections between {Numerical} {Algorithms} for {PDEs} and {Neural} {Networks}},
	url = {http://arxiv.org/abs/2107.14742},
	abstract = {We investigate numerous structural connections between numerical algorithms for partial differential equations (PDEs) and neural architectures. Our goal is to transfer the rich set of mathematical foundations from the world of PDEs to neural networks. Besides structural insights we provide concrete examples and experimental evaluations of the resulting architectures. Using the example of generalised nonlinear diffusion in 1D, we consider explicit schemes, acceleration strategies thereof, implicit schemes, and multigrid approaches. We connect these concepts to residual networks, recurrent neural networks, and U-net architectures. Our findings inspire a symmetric residual network design with provable stability guarantees and justify the effectiveness of skip connections in neural networks from a numerical perspective. Moreover, we present U-net architectures that implement multigrid techniques for learning efficient solutions of partial differential equation models, and motivate uncommon design choices such as trainable nonmonotone activation functions. Experimental evaluations show that the proposed architectures save half of the trainable parameters and can thus outperform standard ones with the same model complexity. Our considerations serve as a basis for explaining the success of popular neural architectures and provide a blueprint for developing new mathematically well-founded neural building blocks.},
	urldate = {2021-09-14},
	journal = {arXiv:2107.14742 [cs, math]},
	author = {Alt, Tobias and Schrader, Karl and Augustin, Matthias and Peter, Pascal and Weickert, Joachim},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.14742},
	keywords = {Computer Science - Machine Learning, Lu  - 1, Mathematics - Numerical Analysis},
}

@inproceedings{gregor_learning_2010,
	address = {Madison, WI, USA},
	series = {{ICML}'10},
	title = {Learning fast approximations of sparse coding},
	isbn = {978-1-60558-907-7},
	abstract = {In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors. SC has become a popular method for extracting features from data. For a given input, SC minimizes a quadratic reconstruction error with an L1 penalty term on the code. The process is often too slow for applications such as real-time pattern recognition. We proposed two versions of a very fast algorithm that produces approximate estimates of the sparse code that can be used to compute good visual features, or to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a specific architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and Osher's coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li and Os-her's for the same approximation error. Unlike previous proposals for sparse code predictors, the system allows a kind of approximate "explaining away" to take place during inference. The resulting predictor is differentiable and can be included into globally-trained recognition systems.},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Gregor, Karol and LeCun, Yann},
	month = jun,
	year = {2010},
	pages = {399--406},
}

@inproceedings{gregor_learning_2010-1,
	address = {Madison, WI, USA},
	series = {{ICML}'10},
	title = {Learning fast approximations of sparse coding},
	isbn = {978-1-60558-907-7},
	abstract = {In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors. SC has become a popular method for extracting features from data. For a given input, SC minimizes a quadratic reconstruction error with an L1 penalty term on the code. The process is often too slow for applications such as real-time pattern recognition. We proposed two versions of a very fast algorithm that produces approximate estimates of the sparse code that can be used to compute good visual features, or to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a specific architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and Osher's coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li and Os-her's for the same approximation error. Unlike previous proposals for sparse code predictors, the system allows a kind of approximate "explaining away" to take place during inference. The resulting predictor is differentiable and can be included into globally-trained recognition systems.},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Gregor, Karol and LeCun, Yann},
	month = jun,
	year = {2010},
	pages = {399--406},
}

@article{dittmer_regularization_2020,
	title = {Regularization by architecture: {A} deep prior approach for inverse problems},
	volume = {62},
	issn = {0924-9907, 1573-7683},
	shorttitle = {Regularization by architecture},
	url = {http://arxiv.org/abs/1812.03889},
	doi = {10.1007/s10851-019-00923-x},
	abstract = {The present paper studies so-called deep image prior (DIP) techniques in the context of ill-posed inverse problems. DIP networks have been recently introduced for applications in image processing; also first experimental results for applying DIP to inverse problems have been reported. This paper aims at discussing different interpretations of DIP and to obtain analytic results for specific network designs and linear operators. The main contribution is to introduce the idea of viewing these approaches as the optimization of Tikhonov functionals rather than optimizing networks. Besides theoretical results, we present numerical verifications.},
	number = {3},
	urldate = {2021-10-26},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Dittmer, Sören and Kluth, Tobias and Maass, Peter and Baguer, Daniel Otero},
	month = apr,
	year = {2020},
	note = {arXiv: 1812.03889},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {456--470},
}

@article{hauptmann_model-based_2018,
	title = {Model-{Based} {Learning} for {Accelerated}, {Limited}-{View} 3-{D} {Photoacoustic} {Tomography}},
	volume = {37},
	issn = {1558-254X},
	doi = {10.1109/TMI.2018.2820382},
	abstract = {Recent advances in deep learning for tomographic reconstructions have shown great potential to create accurate and high quality images with a considerable speed up. In this paper, we present a deep neural network that is specifically designed to provide high resolution 3-D images from restricted photoacoustic measurements. The network is designed to represent an iterative scheme and incorporates gradient information of the data fit to compensate for limited view artifacts. Due to the high complexity of the photoacoustic forward operator, we separate training and computation of the gradient information. A suitable prior for the desired image structures is learned as part of the training. The resulting network is trained and tested on a set of segmented vessels from lung computed tomography scans and then applied to in-vivo photoacoustic measurement data.},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Hauptmann, Andreas and Lucka, Felix and Betcke, Marta and Huynh, Nam and Adler, Jonas and Cox, Ben and Beard, Paul and Ourselin, Sebastien and Arridge, Simon},
	month = jun,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Computational modeling, Deep learning, Image reconstruction, Machine learning, Propagation, TV, Three-dimensional displays, Tomography, convolutional neural networks, iterative reconstruction, photoacoustic tomography},
	pages = {1382--1393},
}

@article{jo_deep_2020,
	title = {Deep neural network approach to forward-inverse problems},
	volume = {15},
	issn = {1556-181X},
	url = {http://aimsciences.org//article/doi/10.3934/nhm.2020011},
	doi = {10.3934/nhm.2020011},
	language = {en},
	number = {2},
	urldate = {2021-10-18},
	journal = {Networks \& Heterogeneous Media},
	author = {Jo, Hyeontae and Son, Hwijae and Ju Hwang, Hyung and Heui Kim, Eun and {,Department of Mathematics, Pohang University of Science and Technology, South Korea} and {,Department of Mathematics and Statistics, California State University Long Beach, US}},
	year = {2020},
	pages = {247--259},
}

@article{schwab_deep_2019,
	title = {Deep null space learning for inverse problems: convergence analysis and rates},
	volume = {35},
	issn = {0266-5611},
	shorttitle = {Deep null space learning for inverse problems},
	url = {https://doi.org/10.1088/1361-6420/aaf14a},
	doi = {10.1088/1361-6420/aaf14a},
	abstract = {Recently, deep learning based methods appeared as a new paradigm for solving inverse problems. These methods empirically show excellent performance but lack of theoretical justification; in particular, no results on the regularization properties are available. In particular, this is the case for two-step deep learning approaches, where a classical reconstruction method is applied to the data in a first step and a trained deep neural network is applied to improve results in a second step. In this paper, we close the gap between practice and theory for a particular network structure in a two-step approach. For that purpose, we propose using so-called null space networks and introduce the concept of -regularization. Combined with a standard regularization method as reconstruction layer, the proposed deep null space learning approach is shown to be a -regularization method; convergence rates are also derived. The proposed null space network structure naturally preserves data consistency which is considered as key property of neural networks for solving inverse problems.},
	language = {en},
	number = {2},
	urldate = {2021-10-18},
	author = {Schwab, Johannes and Antholzer, Stephan and Haltmeier, Markus},
	month = jan,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {025008},
}

@article{bai_deep_2020,
	title = {Deep learning methods for solving linear inverse problems: {Research} directions and paradigms},
	volume = {177},
	issn = {0165-1684},
	shorttitle = {Deep learning methods for solving linear inverse problems},
	url = {https://www.sciencedirect.com/science/article/pii/S0165168420302723},
	doi = {10.1016/j.sigpro.2020.107729},
	abstract = {The linear inverse problem is fundamental to the development of various scientific areas. Innumerable attempts have been carried out to solve different variants of the linear inverse problem in different applications. Nowadays, the rapid development of deep learning provides a fresh perspective for solving the linear inverse problem, which has various well-designed network architectures results in state-of-the-art performance in many applications. In this paper, we present a comprehensive survey of the recent progress in the development of deep learning for solving various linear inverse problems. We review how deep learning methods are used in solving different linear inverse problems, and explore the structured neural network architectures that incorporate knowledge used in traditional methods. Furthermore, we identify open challenges and potential future directions along this research line.},
	language = {en},
	urldate = {2021-10-18},
	journal = {Signal Processing},
	author = {Bai, Yanna and Chen, Wei and Chen, Jie and Guo, Weisi},
	month = dec,
	year = {2020},
	keywords = {Deep learning, Linear inverse problems, Neural networks},
	pages = {107729},
}

@article{jin_deep_2017,
	title = {Deep {Convolutional} {Neural} {Network} for {Inverse} {Problems} in {Imaging}},
	volume = {26},
	issn = {1941-0042},
	doi = {10.1109/TIP.2017.2713099},
	abstract = {In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyperparameter selection. The starting point of this paper is the observation that unrolled iterative methods have the form of a CNN (filtering followed by pointwise nonlinearity) when the normal operator (H*H, where H* is the adjoint of the forward imaging operator, H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 × 512 image on the GPU.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {Jin, Kyong Hwan and McCann, Michael T. and Froustey, Emmanuel and Unser, Michael},
	month = sep,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Computed tomography, Convolution, Image reconstruction, Image restoration, Inverse problems, Iterative methods, Neural networks, biomedical imaging, biomedical signal processing, computed tomography, image reconstruction, magnetic resonance imaging, reconstruction algorithms, tomography},
	pages = {4509--4522},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	language = {en},
	urldate = {2021-10-18},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
}

@article{aggarwal_modl_2019-1,
	title = {{MoDL}: {Model}-{Based} {Deep} {Learning} {Architecture} for {Inverse} {Problems}},
	volume = {38},
	issn = {1558-254X},
	shorttitle = {{MoDL}},
	doi = {10.1109/TMI.2018.2865356},
	abstract = {We introduce a model-based image reconstruction framework with a convolution neural network (CNN)-based regularization prior. The proposed formulation provides a systematic approach for deriving deep architectures for inverse problems with the arbitrary structure. Since the forward model is explicitly accounted for, a smaller network with fewer parameters is sufficient to capture the image information compared to direct inversion approaches. Thus, reducing the demand for training data and training time. Since we rely on end-to-end training with weight sharing across iterations, the CNN weights are customized to the forward model, thus offering improved performance over approaches that rely on pre-trained denoisers. Our experiments show that the decoupling of the number of iterations from the network complexity offered by this approach provides benefits, including lower demand for training data, reduced risk of overfitting, and implementations with significantly reduced memory footprint. We propose to enforce data-consistency by using numerical optimization blocks, such as conjugate gradients algorithm within the network. This approach offers faster convergence per iteration, compared to methods that rely on proximal gradients steps to enforce data consistency. Our experiments show that the faster convergence translates to improved performance, primarily when the available GPU memory restricts the number of iterations.},
	number = {2},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Aggarwal, Hemant K. and Mani, Merry P. and Jacob, Mathews},
	month = feb,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Deep learning, Image reconstruction, Imaging, Machine learning, Numerical models, Optimization, Training, Training data, convolutional neural network, parallel imaging},
	pages = {394--405},
}

@article{ye_deep_2018,
	title = {Deep {Convolutional} {Framelets}: {A} {General} {Deep} {Learning} {Framework} for {Inverse} {Problems}},
	volume = {11},
	shorttitle = {Deep {Convolutional} {Framelets}},
	url = {https://epubs.siam.org/doi/abs/10.1137/17M1141771},
	doi = {10.1137/17M1141771},
	abstract = {Recently, deep learning approaches with various network architectures have achieved significant performance improvement over existing iterative reconstruction methods in various imaging problems. However, it is still unclear why these deep learning architectures work for specific inverse problems. Moreover, in contrast to the usual evolution of signal processing theory around the classical theories, the link between deep learning and the classical signal processing approaches, such as wavelets, nonlocal processing, and compressed sensing, are not yet well understood. To address these issues, here we show that the long-sought missing link is the convolution framelets for representing a signal by convolving local and nonlocal bases. The convolution framelets were originally developed to generalize the theory of low-rank Hankel matrix approaches for inverse problems, and this paper further extends this idea so as to obtain a deep neural network using multilayer convolution framelets with perfect reconstruction (PR) under rectified linear unit (ReLU) nonlinearity. Our analysis also shows that the popular deep network components such as residual blocks, redundant filter channels, and concatenated ReLU (CReLU) do indeed help to achieve PR, while the pooling and unpooling layers should be augmented with high-pass branches to meet the PR condition. Moreover, by changing the number of filter channels and bias, we can control the shrinkage behaviors of the neural network. This discovery reveals the limitations of many existing deep learning architectures for inverse problems, and leads us to propose a novel theory for a deep convolutional framelet neural network. Using numerical experiments with various inverse problems, we demonstrate that our deep convolutional framelets network shows consistent improvement over existing deep architectures. This discovery suggests that the success of deep learning stems not from a magical black box, but rather from the power of a novel signal representation using a nonlocal basis combined with a data-driven local basis, which is indeed a natural extension of classical signal processing theory.},
	number = {2},
	urldate = {2021-10-18},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Ye, Jong Chul and Han, Yoseob and Cha, Eunju},
	month = jan,
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {42C40; Secondary, 44A12, 65T60, 92C55, 94A08, 94A12, 97R40, Primary, ReLU, convolutional neural network, deep learning, framelets, inverse problems, perfect reconstruction condition},
	pages = {991--1048},
}

@article{wei_deep_2021,
	title = {Deep {Unfolding} with {Normalizing} {Flow} {Priors} for {Inverse} {Problems}},
	url = {http://arxiv.org/abs/2107.02848},
	abstract = {Many application domains, spanning from computational photography to medical imaging, require recovery of high-fidelity images from noisy, incomplete or partial/compressed measurements. State of the art methods for solving these inverse problems combine deep learning with iterative model-based solvers, a concept known as deep algorithm unfolding. By combining a-priori knowledge of the forward measurement model with learned (proximal) mappings based on deep networks, these methods yield solutions that are both physically feasible (data-consistent) and perceptually plausible. However, current proximal mappings only implicitly learn such image priors. In this paper, we propose to make these image priors fully explicit by embedding deep generative models in the form of normalizing flows within the unfolded proximal gradient algorithm. We demonstrate that the proposed method outperforms competitive baselines on various image recovery tasks, spanning from image denoising to inpainting and deblurring.},
	urldate = {2021-10-18},
	journal = {arXiv:2107.02848 [eess]},
	author = {Wei, Xinyi and van Gorp, Hans and Carabarin, Lizeth Gonzalez and Freedman, Daniel and Eldar, Yonina and van Sloun, Ruud},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.02848},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{rensonnet_solving_2021,
	title = {Solving inverse problems with deep neural networks driven by sparse signal decomposition in a physics-based dictionary},
	url = {http://arxiv.org/abs/2107.10657},
	abstract = {Deep neural networks (DNN) have an impressive ability to invert very complex models, i.e. to learn the generative parameters from a model's output. Once trained, the forward pass of a DNN is often much faster than traditional, optimization-based methods used to solve inverse problems. This is however done at the cost of lower interpretability, a fundamental limitation in most medical applications. We propose an approach for solving general inverse problems which combines the efficiency of DNN and the interpretability of traditional analytical methods. The measurements are first projected onto a dense dictionary of model-based responses. The resulting sparse representation is then fed to a DNN with an architecture driven by the problem's physics for fast parameter learning. Our method can handle generative forward models that are costly to evaluate and exhibits similar performance in accuracy and computation time as a fully-learned DNN, while maintaining high interpretability and being easier to train. Concrete results are shown on an example of model-based brain parameter estimation from magnetic resonance imaging (MRI).},
	urldate = {2021-10-18},
	journal = {arXiv:2107.10657 [cs]},
	author = {Rensonnet, Gaetan and Adam, Louise and Macq, Benoit},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.10657},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{almaeen_variational_2021,
	title = {Variational {Autoencoder} {Inverse} {Mapper}: {An} {End}-to-{End} {Deep} {Learning} {Framework} for {Inverse} {Problems}},
	shorttitle = {Variational {Autoencoder} {Inverse} {Mapper}},
	doi = {10.1109/IJCNN52387.2021.9534012},
	abstract = {Inverse problems - using measured observations to determine unknown parameters - are well motivated but challenging in many science and engineering problems. In this paper, we propose an end-to-end deep learning framework, the Variational Autoencoder Inverse Mapper (VAIM), as an autoencoder-based neural network architecture for inverse problems. The encoder and decoder neural networks approximate the forward and backward mapping, respectively, and a variational latent layer is incorporated into VAIM to learn the posterior parameter distributions with respect to given observables. We demonstrate the effectiveness of VAIM for several toy inverse problems, with both finite and infinite solutions, and for constructing the inverse function mapping quantum correlation functions to observables in a Quantum Chromodynamics analysis of nucleon structure.},
	booktitle = {2021 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Almaeen, Manal and Alanazi, Yasir and Sato, Nobuo and Melnitchouk, W. and Kuchera, Michelle P. and Li, Yaohang},
	month = jul,
	year = {2021},
	note = {ISSN: 2161-4407},
	keywords = {Artificial neural networks, Correlation, Deep learning, Generative adversarial networks, Inverse problems, Software packages, Toy manufacturing industry, end-to-end learning, ill-posed, inverse problems, latent space analysis, variational autoencoder},
	pages = {1--8},
}

@article{duff_regularising_2021,
	title = {Regularising {Inverse} {Problems} with {Generative} {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/2107.11191},
	abstract = {Deep neural network approaches to inverse imaging problems have produced impressive results in the last few years. In this paper, we consider the use of generative models in a variational regularisation approach to inverse problems. The considered regularisers penalise images that are far from the range of a generative model that has learned to produce images similar to a training dataset. We name this family {\textbackslash}textit\{generative regularisers\}. The success of generative regularisers depends on the quality of the generative model and so we propose a set of desired criteria to assess models and guide future research. In our numerical experiments, we evaluate three common generative models, autoencoders, variational autoencoders and generative adversarial networks, against our desired criteria. We also test three different generative regularisers on the inverse problems of deblurring, deconvolution, and tomography. We show that the success of solutions restricted to lie exactly in the range of the generator is highly dependent on the ability of the generative model but that allowing small deviations from the range of the generator produces more consistent results.},
	urldate = {2021-10-18},
	journal = {arXiv:2107.11191 [cs, eess, math]},
	author = {Duff, Margaret and Campbell, Neill D. F. and Ehrhardt, Matthias J.},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.11191},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Mathematics - Optimization and Control},
}

@article{del_alamo_deep_2021,
	title = {Deep learning for inverse problems with unknown operator},
	url = {http://arxiv.org/abs/2108.02744},
	abstract = {We consider ill-posed inverse problems where the forward operator \$T\$ is unknown, and instead we have access to training data consisting of functions \$f\_i\$ and their noisy images \$Tf\_i\$. This is a practically relevant and challenging problem which current methods are able to solve only under strong assumptions on the training set. Here we propose a new method that requires minimal assumptions on the data, and prove reconstruction rates that depend on the number of training points and the noise level. We show that, in the regime of "many" training data, the method is minimax optimal. The proposed method employs a type of convolutional neural networks (U-nets) and empirical risk minimization in order to "fit" the unknown operator. In a nutshell, our approach is based on two ideas: the first is to relate U-nets to multiscale decompositions such as wavelets, thereby linking them to the existing theory, and the second is to use the hierarchical structure of U-nets and the low number of parameters of convolutional neural nets to prove entropy bounds that are practically useful. A significant difference with the existing works on neural networks in nonparametric statistics is that we use them to approximate operators and not functions, which we argue is mathematically more natural and technically more convenient.},
	urldate = {2021-10-18},
	journal = {arXiv:2108.02744 [cs, math, stat]},
	author = {del Alamo, Miguel},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.02744},
	keywords = {65J22, 62G05, 68T07, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{antil_novel_nodate,
	title = {{NOVEL} {DEEP} {NEURAL} {NETWORKS} {FOR} {SOLVING} {BAYESIAN} {STATISTICAL} {INVERSE} {PROBLEMS}},
	abstract = {We consider the simulation of Bayesian statistical inverse problems governed by large-scale linear and nonlinear partial diﬀerential equations (PDEs). Markov chain Monte Carlo (MCMC) algorithms are standard techniques to solve such problems. However, MCMC techniques are computationally challenging as they require several thousands of forward PDE solves. The goal of this paper is to introduce a fractional deep neural network based approach for the forward solves within an MCMC routine. Moreover, we discuss some approximation error estimates and illustrate the eﬃciency of our approach via several numerical examples.},
	language = {en},
	author = {Antil, Harbir and Elman, Howard C and Onwunta, Akwum},
	pages = {22},
}

@article{nayak_instabilities_2021,
	title = {Instabilities in {Plug}-and-{Play} ({PnP}) algorithms from a learned denoiser},
	url = {http://arxiv.org/abs/2109.01655},
	abstract = {It's well-known that inverse problems are ill-posed and to solve them meaningfully, one has to employ regularization methods. Traditionally, popular regularization methods are the penalized Variational approaches. In recent years, the classical regularization approaches have been outclassed by the so-called plug-and-play (PnP) algorithms, which copy the proximal gradient minimization processes, such as ADMM or FISTA, but with any general denoiser. However, unlike the traditional proximal gradient methods, the theoretical underpinnings, convergence, and stability results have been insufficient for these PnP-algorithms. Hence, the results obtained from these algorithms, though empirically outstanding, can't always be completely trusted, as they may contain certain instabilities or (hallucinated) features arising from the denoiser, especially when using a pre-trained learned denoiser. In fact, in this paper, we show that a PnP-algorithm can induce hallucinated features, when using a pre-trained deep-learning-based (DnCNN) denoiser. We show that such instabilities are quite different than the instabilities inherent to an ill-posed problem. We also present methods to subdue these instabilities and significantly improve the recoveries. We compare the advantages and disadvantages of a learned denoiser over a classical denoiser (here, BM3D), as well as, the effectiveness of the FISTA-PnP algorithm vs. the ADMM-PnP algorithm. In addition, we also provide an algorithm to combine these two denoisers, the learned and the classical, in a weighted fashion to produce even better results. We conclude with numerical results which validate the developed theories.},
	urldate = {2021-10-18},
	journal = {arXiv:2109.01655 [cs, eess, math]},
	author = {Nayak, Abinash},
	month = aug,
	year = {2021},
	note = {arXiv: 2109.01655},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Mathematics - Numerical Analysis, Primary 65K05, 65K10, Secondary 65R30, 65R32},
}

@inproceedings{russell_fixing_2019,
	title = {Fixing {Implicit} {Derivatives}: {Trust}-{Region} {Based} {Learning} of {Continuous} {Energy} {Functions}},
	volume = {32},
	shorttitle = {Fixing {Implicit} {Derivatives}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/b0b183c207f46f0cca7dc63b2604f5cc-Abstract.html},
	urldate = {2021-10-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Russell, Chris and Toso, Matteo and Campbell, Neill},
	year = {2019},
}

@article{nayak_regularizing_2021,
	title = {Regularizing ({Stabilizing}) {Deep} {Learning} {Based} {Reconstruction} {Algorithms}},
	url = {http://arxiv.org/abs/2108.13551},
	abstract = {It's well-known that inverse problems are ill-posed and to solve them meaningfully one has to employ regularization methods. Traditionally, popular regularization methods have been the penalized Variational approaches. In recent years, the classical regularized-reconstruction approaches have been outclassed by the (deep-learning-based) learned reconstruction algorithms. However, unlike the traditional regularization methods, the theoretical underpinnings, such as stability and regularization, have been insufficient for such learned reconstruction algorithms. Hence, the results obtained from such algorithms, though empirically outstanding, can't always be completely trusted, as they may contain certain instabilities or (hallucinated) features arising from the learned process. In fact, it has been shown that such learning algorithms are very susceptible to small (adversarial) noises in the data and can lead to severe instabilities in the recovered solution, which can be quite different than the inherent instabilities of the ill-posed (inverse) problem. Whereas, the classical regularization methods can handle such (adversarial) noises very well and can produce stable recovery. Here, we try to present certain regularization methods to stabilize such (unstable) learned reconstruction methods and recover a regularized solution, even in the presence of adversarial noises. For this, we need to extend the classical notion of regularization and incorporate it in the learned reconstruction algorithms. We also present some regularization techniques to regularize two of the most popular learning reconstruction algorithms, the Learned Post-Processing Reconstruction and the Learned Unrolling Reconstruction.},
	urldate = {2021-10-18},
	journal = {arXiv:2108.13551 [cs, eess]},
	author = {Nayak, Abinash},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.13551},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Primary 65K05, 65K10, Secondary 65R30, 65R32},
}

@article{malezieux_comparing_nodate,
	title = {Comparing {Analysis} and {Synthesis} in {Deep} {Prior} {Learning} for {Inverse} {Problems} {Resolution}},
	abstract = {Inverse problems consist in recovering a signal given a noisy transformation when no ground truth is available. Some prior knowledge about the signal is required and must be provided to the reconstruction algorithm to solve the problem. In this work, our goal is to learn the prior in the unsupervised setting by leveraging the sparsity property of natural signals in order to ﬁnd a simpler representation of the data. We design two methods, derived from two diﬀerent models and based on Deep Learning and sparse coding algorithms.},
	language = {en},
	author = {Malézieux, Benoît and Moreau, Thomas and Kowalski, Matthieu},
	pages = {3},
}

@article{jiang_physics-data-driven_2021,
	title = {A {Physics}-{Data}-{Driven} {Bayesian} {Method} for {Heat} {Conduction} {Problems}},
	url = {http://arxiv.org/abs/2109.00996},
	abstract = {In this study, a novel physics-data-driven Bayesian method named Heat Conduction Equation assisted Bayesian Neural Network (HCE-BNN) is proposed. The HCE-BNN is constructed based on the Bayesian neural network, it is a physics-informed machine learning strategy. Compared with the existed pure data driven method, to acquire physical consistency and better performance of the data-driven model, the heat conduction equation is embedded into the loss function of the HCE-BNN as a regularization term. Hence, the proposed method can build a more reliable model by physical constraints with less data. The HCE-BNN can handle the forward and inverse problems consistently, that is, to infer unknown responses from known partial responses, or to identify boundary conditions or material parameters from known responses. Compared with the exact results, the test results demonstrate that the proposed method can be applied to both heat conduction forward and inverse problems successfully. In addition, the proposed method can be implemented with the noisy data and gives the corresponding uncertainty quantification for the solutions.},
	urldate = {2021-10-18},
	journal = {arXiv:2109.00996 [cs, math]},
	author = {Jiang, Xinchao and Wang, Hu and li, Yu},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.00996},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Mathematics - Numerical Analysis},
}

@article{damara_solving_2021,
	title = {Solving {Inverse} {Problems} with {Conditional}-{GAN} {Prior} via {Fast} {Network}-{Projected} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/2109.01105},
	abstract = {The projected gradient descent (PGD) method has shown to be effective in recovering compressed signals described in a data-driven way by a generative model, i.e., a generator which has learned the data distribution. Further reconstruction improvements for such inverse problems can be achieved by conditioning the generator on the measurement. The boundary equilibrium generative adversarial network (BEGAN) implements an equilibrium based loss function and an auto-encoding discriminator to better balance the performance of the generator and the discriminator. In this work we investigate a network-based projected gradient descent (NPGD) algorithm for measurement-conditional generative models to solve the inverse problem much faster than regular PGD. We combine the NPGD with conditional GAN/BEGAN to evaluate their effectiveness in solving compressed sensing type problems. Our experiments on the MNIST and CelebA datasets show that the combination of measurement conditional model with NPGD works well in recovering the compressed signal while achieving similar or in some cases even better performance along with a much faster reconstruction. The achieved reconstruction speed-up in our experiments is up to 140-175.},
	urldate = {2021-10-18},
	journal = {arXiv:2109.01105 [cs, eess, math, stat]},
	author = {Damara, Muhammad Fadli and Kornhardt, Gregor and Jung, Peter},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.01105},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@article{tao_image_2021,
	title = {Image {Restoration} {Based} on {End}-to-{End} {Unrolled} {Network}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2304-6732/8/9/376},
	doi = {10.3390/photonics8090376},
	abstract = {Recent studies on image restoration (IR) methods under unrolled optimization frameworks have shown that deep convolutional neural networks (DCNNs) can be implicitly used as priors to solve inverse problems. Due to the ill-conditioned nature of the inverse problem, the selection of prior knowledge is crucial for the process of IR. However, the existing methods use a fixed DCNN in each iteration, and so they cannot fully adapt to the image characteristics at each iteration stage. In this paper, we combine deep learning with traditional optimization and propose an end-to-end unrolled network based on deep priors. The entire network contains several iterations, and each iteration is composed of analytic solution updates and a small multiscale deep denoiser network. In particular, we use different denoiser networks at different stages to improve adaptability. Compared with a fixed DCNN, it greatly reduces the number of computations when the total parameters are equal and the number of iterations is the same, but the gains from a practical runtime are not as significant as indicated in the FLOP count. The experimental results of our method of three IR tasks, including denoising, deblurring, and lensless imaging, demonstrate that our proposed method achieves state-of-the-art performances in terms of both visual effects and quantitative evaluations.},
	language = {en},
	number = {9},
	urldate = {2021-10-18},
	journal = {Photonics},
	author = {Tao, Xiaoping and Zhou, Hao and Chen, Yueting},
	month = sep,
	year = {2021},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {analytic solution, deep convolutional neural networks, image restoration, unrolled optimization},
	pages = {376},
}

@article{karra_adjointnet_2021,
	title = {{AdjointNet}: {Constraining} machine learning models with physics-based codes},
	shorttitle = {{AdjointNet}},
	url = {http://arxiv.org/abs/2109.03956},
	abstract = {Physics-informed Machine Learning has recently become attractive for learning physical parameters and features from simulation and observation data. However, most existing methods do not ensure that the physics, such as balance laws (e.g., mass, momentum, energy conservation), are constrained. Some recent works (e.g., physics-informed neural networks) softly enforce physics constraints by including partial differential equation (PDE)-based loss functions but need re-discretization of the PDEs using auto-differentiation. Training these neural nets on observational data showed that one could solve forward and inverse problems in one shot. They evaluate the state variables and the parameters in a PDE. This re-discretization of PDEs is not necessarily an attractive option for domain scientists that work with physics-based codes that have been developed for decades with sophisticated discretization techniques to solve complex process models and advanced equations of state. This paper proposes a physics constrained machine learning framework, AdjointNet, allowing domain scientists to embed their physics code in neural network training workflows. This embedding ensures that physics is constrained everywhere in the domain. Additionally, the mathematical properties such as consistency, stability, and convergence vital to the numerical solution of a PDE are still satisfied. We show that the proposed AdjointNet framework can be used for parameter estimation (and uncertainty quantification by extension) and experimental design using active learning. The applicability of our framework is demonstrated for four flow cases. Results show that AdjointNet-based inversion can estimate process model parameters with reasonable accuracy. These examples demonstrate the applicability of using existing software with no changes in source code to perform accurate and reliable inversion of model parameters.},
	urldate = {2021-10-18},
	journal = {arXiv:2109.03956 [physics]},
	author = {Karra, Satish and Ahmmed, Bulbul and Mudunuru, Maruti K.},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.03956},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Computational Physics, Physics - Fluid Dynamics},
}

@article{marz_solving_2021,
	title = {Solving underdetermined inverse problems: {From} advanced sparsity models to deep learning},
	shorttitle = {Solving underdetermined inverse problems},
	url = {https://depositonce.tu-berlin.de/handle/11303/13422},
	doi = {10.14279/DEPOSITONCE-12206},
	abstract = {Die vorliegende kumulative Dissertation entwickelt und untersucht Methoden zur Rekonstruktion unbekannter Signale anhand von stark unterbestimmten linearen Messungen. Derartige inverse Probleme treten in einer Vielzahl von Anwendungen auf, die von medizinischen Bildgebungsverfahren wie der Computertomographie bis hin zur seismischen Inversion in der Geophysik reichen. Obwohl eine einheitliche Rekonstruktionsaufgabe zugrunde liegt, gliedert sich die Arbeit in zwei verschiedene Themenbereiche: Der erste Teil entwickelt die Theorie der modellbasierten Methoden weiter, welche auf verallgemeinerten Sparsity-Annahmen basieren. Die Forschung zu Compressed Sensing hat gezeigt, dass ein Signal bereits aus wenigen indirekten und randomisierten Messungen in robuster Weise wiederhergestellt werden kann, wobei dessen inhärente Struktur ausgenutzt wird. Diese Aufgabe lässt sich häufig durch die Lösung eines konvexen Optimierungsproblems bewältigen, welches auf der Sparsity-kompatiblen l1-Minimierung basiert. Da realistische Signale jedoch in der Regel nicht unmittelbar Sparse sind, ist oftmals eine lineare Transformation erforderlich, um eine geeignete Darstellung mit geringer Komplexität zu erhalten. Eine solche Annahme führt dann zu einem Synthese- oder zu einem Analyse-basierten Sparsity-Modell. Unter der Annahme sub-Gaußscher Zufallsmessungen werden in beiden Fällen neuartige Messraten für l1-Regularisierung hergeleitet. Ein weiterer Beitrag besteht darin, eine Komplexitätsbarriere im wichtigen Spezialfall der totalen Variation in einer räumlichen Dimension zu durchbrechen. Unsere Ergebnisse widersprechen der konventionellen Vorstellung, wonach die Probenkomplexität ausschließlich durch die Sparsity beschrieben wird. Eine Haupterkenntnis dieser Arbeit ist, dass ein Rekonstruktionserfolg jenseits von Orthonormalbasen typischerweise nicht alleine durch Sparsity charakterisiert werden kann. Durch Berücksichtigung anderer, struktureller und signalabhängiger Eigenschaften, können genauere Vorhersagen für die erforderliche Anzahl von Messungen hergeleitet werden. Im zweiten Teil der Arbeit werden Deep-Learning-basierte Rekonstruktionsverfahren in numerischen Simulationsstudien untersucht. Im Gegensatz zum ersten Teil basieren solche Verfahren nicht darauf, ein explizites Datenmodell zu formulieren. Stattdessen werden strukturierte Lösungen anhand der Kenntnis von a priori verfügbaren Trainingsdaten berechnet. Trotz beispielloser empirischer Erfolge sind die mathematischen Funktionsprinzipien künstlicher neuronaler Netze nur unzureichend verstanden. In dieser Arbeit werden zwei Beiträge zu diesem Forschungsgebiet erbracht: Zunächst wird in einer umfangreichen numerischen Studie die Robustheit von gelernten End-to-End-Methoden analysiert. Diese Untersuchung ist dadurch motiviert, dass Klassifikationsnetzwerke bekanntermaßen anfällig für Störungen sind. Entgegen bisheriger Behauptungen in der Literatur wird gezeigt, dass sich dieses Problem nicht zwangsläufig auf Deep-Learning-basierte Lösungsverfahren für inverse Probleme überträgt. Es wird aufgezeigt, dass Standard-Rekonstruktionsnetzwerke bemerkenswert robust gegenüber statistischem und schlimmstmöglichem Rauschen sind. Ein zweiter Beitrag ist die Entwicklung einer hybriden Rekonstruktionsmethode für das schlecht gestellte inverse Problem der Limited-Angle Computertomographie. Die Auswirkungen der besonderen Struktur dieses inversen Problems werden durch klassische Sichtbarkeitsergebnisse beschrieben, welche auf mikrolokaler Analysis basieren. Der vorgeschlagene Algorithmus baut auf dieser mathematischen Charakterisierung auf, indem er zunächst den sichtbaren Teil der Daten durch eine Regularisierung mit einem anisotropen Repräsentationssystem zurückgewinnt. Nur die Informationen, die für diese Methode beweisbar unzugänglich sind, werden daraufhin mittels eines tiefen neuronalen Netzes geschätzt.},
	language = {en},
	urldate = {2021-10-18},
	author = {März, Maximilian Arthus},
	collaborator = {Technische Universität Berlin and Technische Universität Berlin},
	year = {2021},
	note = {Publisher: Technische Universität Berlin},
	keywords = {519 Wahrscheinlichkeiten, angewandte Mathematik, Optimierung, compressed sensing, deep learning, inverse Probleme, inverse problems, optimization, sparsity, tiefes Lernen},
}

@article{obmann_augmented_2021,
	title = {Augmented {NETT} regularization of inverse problems},
	volume = {5},
	issn = {2399-6528},
	url = {https://doi.org/10.1088/2399-6528/ac26aa},
	doi = {10.1088/2399-6528/ac26aa},
	abstract = {We propose aNETT (augmented NETwork Tikhonov) regularization as a novel data-driven reconstruction framework for solving inverse problems. An encoder-decoder type network defines a regularizer consisting of a penalty term that enforces regularity in the encoder domain, augmented by a penalty that penalizes the distance to the signal manifold. We present a rigorous convergence analysis including stability estimates and convergence rates. For that purpose, we prove the coercivity of the regularizer used without requiring explicit coercivity assumptions for the networks involved. We propose a possible realization together with a network architecture and a modular training strategy. Applications to sparse-view and low-dose CT show that aNETT achieves results comparable to state-of-the-art deep-learning-based reconstruction methods. Unlike learned iterative methods, aNETT does not require repeated application of the forward and adjoint models during training, which enables the use of aNETT for inverse problems with numerically expensive forward models. Furthermore, we show that aNETT trained on coarsely sampled data can leverage an increased sampling rate without the need for retraining.},
	language = {en},
	number = {10},
	urldate = {2021-10-18},
	author = {Obmann, Daniel and Nguyen, Linh and Schwab, Johannes and Haltmeier, Markus},
	month = oct,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {105002},
}

@article{liu_physical_2021,
	title = {Physical {Model}-inspired {Deep} {Unrolling} {Network} for {Solving} {Nonlinear} {Inverse} {Scattering} {Problems}},
	issn = {1558-2221},
	doi = {10.1109/TAP.2021.3111281},
	abstract = {In this paper, to bridge the gap between the traditional model-based methods and data-driven deep learning schemes, we propose a physical model-inspired deep unrolling network for solving nonlinear inverse scattering problems, termed PM-Net. The proposed end-to-end network is formed by two consequent steps. First, an augmented Lagrangian method is introduced to transform a constrained objective function to be an unconstrained optimization. In addition, it is further decomposed into four quasi-linear subproblems. Second, we unfold the iterative scheme into a layer-wise deep neural network. Each subproblem is mapped into a module of the deep unrolling network. In PM-Net, these variables including the weight, the regularization of contrast and other parameters are learned and updated alternately by corresponding network layers. PM-Net effectively combines neural network with the knowledge of underlying physics as well as traditional techniques. Unlike existing networks, PM-Net explicitly exploits contrast source and contrast modules. Compared to traditional iterative methods, the performance of PM-Net is comparable or even better than subspace-based optimization method in the high noise level circumstance. Compared to state-of-the-art learning approaches, not only less network parameters need to be learned, but also better performance is achieved by PM-Net.},
	journal = {IEEE Transactions on Antennas and Propagation},
	author = {Liu, Jian and Zhou, Huilin and Ouyang, Tao and Liu, Qiegen and Wang, Yuhao},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Antennas and Propagation},
	keywords = {Deep learning, Inverse problems, Inverse scattering problems, Knowledge engineering, Mathematical model, Neural networks, Permittivity, Training, deep learning, unrolling network},
	pages = {1--1},
}

@article{bui-thanh_model-constrained_nodate,
	title = {{MODEL}-{CONSTRAINED} {DEEP} {LEARNING} {APPROACHES} {FOR} {INVERSE} {PROBLEMS}},
	abstract = {Deep Learning (DL), in particular deep neural networks, by design is purely datadriven and in general does not require physics. This is the strength of DL but also one of its key limitations when applied to science and engineering problems in which underlying physical properties—such as stability, conservation, and positivity—and desired accuracy need to be achieved. DL methods in their original forms is not capable of respecting the underlying mathematical models or achieving desired accuracy even in big-data regimes. On the other hand, many data-driven science and engineering problems, such as inverse problems, typically have limited experimental or observational data, and DL would overﬁt the data in this case. Leveraging information encoded in the underlying mathematical models, we argue, not only compensates missing information in low data regimes but also provides opportunities to equip DL methods with the underlying physics and hence obtaining higher accuracy. This short paper introduces several model-constrained DL approaches—including both feed-forward DNN and autoencoders—that learn not only information hidden in the training data but also in the underlying mathematical models to solve inverse problems. We present and provide intuitions for our formulations for general nonlinear problems. For linear inverse problems and linear networks, the ﬁrst order optimality conditions show that our modelconstrained DL approaches can learn information encoded in the underlying mathematical models, and thus can produce consistent or equivalent inverse solutions, while naive purely data-based counterparts cannot.},
	language = {en},
	author = {Bui-Thanh, Tan},
	pages = {10},
}

@article{lin_admm-adam_2021,
	title = {{ADMM}-{ADAM}: {A} {New} {Inverse} {Imaging} {Framework} {Blending} the {Advantages} of {Convex} {Optimization} and {Deep} {Learning}},
	issn = {1558-0644},
	shorttitle = {{ADMM}-{ADAM}},
	doi = {10.1109/TGRS.2021.3111007},
	abstract = {Alternating direction method of multipliers (ADMM) and adaptive moment estimation (ADAM) are two optimizers of paramount importance in convex optimization (CO) and deep learning (DL), respectively. Numerous state-of-the-art algorithms for solving inverse problems are achieved by carefully designing a convex criterion, typically composed of a data-fitting term and a regularizer. Even when the regularizer is convex, its mathematical form is often sophisticated, hence inducing a math-heavy optimization procedure and making the algorithm design a daunting task for software engineers. Probably for this reason, people turn to solve the inverse problems via DL, but this requires big data collection, quite time-consuming if not impossible. Motivated by these facts, we propose a new framework, termed as ADMM-ADAM, for solving inverse problems. As the key contribution, even just with small/single data, the proposed ADMM-ADAM is able to exploit DL to obtain a convex regularizer of very simple math form, followed by solving the regularized criterion using simple CO algorithm. As a side contribution, a state-of-the-art hyperspectral inpainting algorithm is designed under ADMM-ADAM, demonstrating its superiority even without the aid of big data or sophisticated mathematical regularization.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Lin, Chia-Hsiang and Lin, Yen-Cheng and Tang, Po-Wei},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Adaptive moment estimation (ADAM), Big Data, Convex functions, Deep learning, Hyperspectral imaging, Imaging, Inverse problems, Optimization, alternating direction method of multipliers (ADMM), convex optimization (CO), deep learning (DL), imaging inverse problem.},
	pages = {1--16},
}

@article{holl_physical_2021,
	title = {Physical {Gradients} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2109.15048},
	abstract = {Solving inverse problems, such as parameter estimation and optimal control, is a vital part of science. Many experiments repeatedly collect data and employ machine learning algorithms to quickly infer solutions to the associated inverse problems. We find that state-of-the-art training techniques are not well-suited to many problems that involve physical processes since the magnitude and direction of the gradients can vary strongly. We propose a novel hybrid training approach that combines higher-order optimization methods with machine learning techniques. We replace the gradient of the physical process by a new construct, referred to as the physical gradient. This also allows us to introduce domain knowledge into training by incorporating priors about the solution space into the gradients. We demonstrate the capabilities of our method on a variety of canonical physical systems, showing that physical gradients yield significant improvements on a wide range of optimization and learning problems.},
	urldate = {2021-10-18},
	journal = {arXiv:2109.15048 [physics]},
	author = {Holl, Philipp and Koltun, Vladlen and Thuerey, Nils},
	month = oct,
	year = {2021},
	note = {arXiv: 2109.15048},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
}

@article{hurault_gradient_2021,
	title = {Gradient {Step} {Denoiser} for convergent {Plug}-and-{Play}},
	url = {http://arxiv.org/abs/2110.03220},
	abstract = {Plug-and-Play methods constitute a class of iterative algorithms for imaging problems where regularization is performed by an off-the-shelf denoiser. Although Plug-and-Play methods can lead to tremendous visual performance for various image problems, the few existing convergence guarantees are based on unrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly convex data terms. In this work, we propose a new type of Plug-and-Play methods, based on half-quadratic splitting, for which the denoiser is realized as a gradient descent step on a functional parameterized by a deep neural network. Exploiting convergence results for proximal gradient descent algorithms in the non-convex setting, we show that the proposed Plug-and-Play algorithm is a convergent iterative scheme that targets stationary points of an explicit global functional. Besides, experiments show that it is possible to learn such a deep denoiser while not compromising the performance in comparison to other state-of-the-art deep denoisers used in Plug-and-Play schemes. We apply our proximal gradient algorithm to various ill-posed inverse problems, e.g. deblurring, super-resolution and inpainting. For all these applications, numerical results empirically confirm the convergence results. Experiments also show that this new algorithm reaches state-of-the-art performance, both quantitatively and qualitatively.},
	urldate = {2021-10-18},
	journal = {arXiv:2110.03220 [cs, eess, math]},
	author = {Hurault, Samuel and Leclaire, Arthur and Papadakis, Nicolas},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.03220},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Mathematics - Optimization and Control},
}

@article{hoerig_machine_2021,
	title = {Machine {Learning} in {Model}-free {Mechanical} {Property} {Imaging}: {Novel} {Integration} of {Physics} {With} the {Constrained} {Optimization} {Process}},
	volume = {9},
	issn = {2296-424X},
	shorttitle = {Machine {Learning} in {Model}-free {Mechanical} {Property} {Imaging}},
	url = {http://www.scopus.com/inward/record.url?scp=85112443617&partnerID=8YFLogxK},
	doi = {10.3389/fphy.2021.600718},
	abstract = {The Autoprogressive Method (AutoP) is a fundamentally different approach to solving the inverse problem in quasi-static ultrasonic elastography (QUSE). By exploiting the nonlinear adaptability of artificial neural networks and physical constraints imposed through finite element analysis, AutoP is able to build patient specific soft-computational material models from a relatively sparse set of force-displacement measurement data. Physics-guided, data-driven models offer a new path to the discovery of mechanical properties most effective for diagnostic imaging. AutoP was originally applied to modeling mechanical properties of materials in geotechnical and civil engineering applications. The method was later adapted to reconstructing maps of linear-elastic material properties for cancer imaging applications. Previous articles describing AutoP focused on high-level concepts to explain the mechanisms driving the training process. In this review, we focus on AutoP as applied to QUSE to present a more thorough explanation of the ways in which the method fundamentally differs from classic model-based and other machine learning approaches. We build intuition for the method through analogy to conventional optimization methods and explore how maps of stresses and strains are extracted from force-displacement measurements in a model-free way. In addition, we discuss a physics-based regularization term unique to AutoP that illuminates the comparison to typical optimization procedures. The insights gained from our hybrid inverse method will hopefully inspire others to explore combinations of rigorous mathematical techniques and conservation principles with the power of machine learning to solve difficult inverse problems.},
	number = {600718},
	urldate = {2021-10-18},
	journal = {Frontiers in Physics},
	author = {Hoerig, Cameron and Ghaboussi, Jamshid and Wang, Yiliang and Insana, Michael F.},
	month = jul,
	year = {2021},
	keywords = {Inverse problem, Moyen, elasticity, finite element analysis, neural network constitutive model, ultrasound},
}

@misc{noauthor_learning_nodate,
	title = {Learning {Robust} {Data}-{Driven} {Methods} for {Inverse} {Problems} and {Change} {Detection} - {ProQuest}},
	url = {https://www.proquest.com/openview/f097f0eff9f2daadbe0f416ec20e1878/1?cbl=18750&diss=y&pq-origsite=gscholar&accountid=15558},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
	language = {fr},
	urldate = {2021-10-18},
}

@article{romano_little_2017,
	title = {The {Little} {Engine} {That} {Could}: {Regularization} by {Denoising} ({RED})},
	volume = {10},
	issn = {1936-4954},
	shorttitle = {The {Little} {Engine} {That} {Could}},
	url = {https://epubs.siam.org/doi/10.1137/16M1102884},
	doi = {10.1137/16M1102884},
	language = {en},
	number = {4},
	urldate = {2021-10-11},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Romano, Yaniv and Elad, Michael and Milanfar, Peyman},
	month = jan,
	year = {2017},
	pages = {1804--1844},
}

@article{sreehari_plug-and-play_2016,
	title = {Plug-and-{Play} {Priors} for {Bright} {Field} {Electron} {Tomography} and {Sparse} {Interpolation}},
	volume = {2},
	issn = {2333-9403},
	doi = {10.1109/TCI.2016.2599778},
	abstract = {Many material and biological samples in scientific imaging are characterized by nonlocal repeating structures. These are studied using scanning electron microscopy and electron tomography. Sparse sampling of individual pixels in a two-dimensional image acquisition geometry, or sparse sampling of projection images with large tilt increments in a tomography experiment, can enable high speed data acquisition and minimize sample damage caused by the electron beam. In this paper, we present an algorithm for electron tomographic reconstruction and sparse image interpolation that exploits the nonlocal redundancy in images. We adapt a framework, termed plug-and-play priors, to solve these imaging problems in a regularized inversion setting. The power of the plug-and-play approach is that it allows a wide array of modern denoising algorithms to be used as a “prior model” for tomography and image interpolation. We also present sufficient mathematical conditions that ensure convergence of the plug-and-play approach, and we use these insights to design a new nonlocal means denoising algorithm. Finally, we demonstrate that the algorithm produces higher quality reconstructions on both simulated and real electron microscope data, along with improved convergence properties compared to other methods.},
	number = {4},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Sreehari, Suhas and Venkatakrishnan, S. V. and Wohlberg, Brendt and Buzzard, Gregery T. and Drummy, Lawrence F. and Simmons, Jeffrey P. and Bouman, Charles A.},
	month = dec,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Computational Imaging},
	keywords = {BM3D, Image reconstruction, Interpolation, Microscopy, Noise reduction, Plug-and-play, Redundancy, Tomography, bright field electron tomography, doubly stochastic gradient non-local means, non-local means, prior modeling, sparse interpolation},
	pages = {408--423},
}

@article{lin_admm-adam_2021-1,
	title = {{ADMM}-{ADAM}: {A} {New} {Inverse} {Imaging} {Framework} {Blending} the {Advantages} of {Convex} {Optimization} and {Deep} {Learning}},
	issn = {0196-2892, 1558-0644},
	shorttitle = {{ADMM}-{ADAM}},
	url = {https://ieeexplore.ieee.org/document/9546991/},
	doi = {10.1109/TGRS.2021.3111007},
	urldate = {2021-10-07},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Lin, Chia-Hsiang and Lin, Yen-Cheng and Tang, Po-Wei},
	year = {2021},
	pages = {1--16},
}

@article{miyato_spectral_2018,
	title = {Spectral {Normalization} for {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1802.05957},
	abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
	urldate = {2021-10-04},
	journal = {arXiv:1802.05957 [cs, stat]},
	author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05957},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{adler_deep_2018,
	title = {Deep {Bayesian} {Inversion}},
	url = {http://arxiv.org/abs/1811.05910},
	abstract = {Characterizing statistical properties of solutions of inverse problems is essential for decision making. Bayesian inversion offers a tractable framework for this purpose, but current approaches are computationally unfeasible for most realistic imaging applications in the clinic. We introduce two novel deep learning based methods for solving large-scale inverse problems using Bayesian inversion: a sampling based method using a WGAN with a novel mini-discriminator and a direct approach that trains a neural network using a novel loss function. The performance of both methods is demonstrated on image reconstruction in ultra low dose 3D helical CT. We compute the posterior mean and standard deviation of the 3D images followed by a hypothesis test to assess whether a "dark spot" in the liver of a cancer stricken patient is present. Both methods are computationally efficient and our evaluation shows very promising performance that clearly supports the claim that Bayesian inversion is usable for 3D imaging in time critical applications.},
	urldate = {2021-09-29},
	journal = {arXiv:1811.05910 [cs, math, stat]},
	author = {Adler, Jonas and Öktem, Ozan},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.05910},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@book{kevin_p_murphy_probabilistic_2022,
	title = {Probabilistic {Machine} {Learning}: {An} introduction},
	url = {probml.ai},
	publisher = {MIT Press},
	author = {{Kevin P. Murphy}},
	year = {2022},
}

@book{gelfand_calculus_2000,
	address = {Mineola, N.Y},
	edition = {Unabridged reprint},
	title = {Calculus of variations},
	isbn = {978-0-486-41448-5},
	language = {eng},
	publisher = {Dover Publications},
	author = {Gelʹfand, Izrailʹ M. and Fomin, Sergej V. and Gelʹfand, Izrailʹ M. and Gelʹfand, Izrailʹ M.},
	editor = {Silverman, Richard A.},
	year = {2000},
}

@article{whang_composing_2021,
	title = {Composing {Normalizing} {Flows} for {Inverse} {Problems}},
	url = {http://arxiv.org/abs/2002.11743},
	abstract = {Given an inverse problem with a normalizing flow prior, we wish to estimate the distribution of the underlying signal conditioned on the observations. We approach this problem as a task of conditional inference on the pre-trained unconditional flow model. We first establish that this is computationally hard for a large class of flow models. Motivated by this, we propose a framework for approximate inference that estimates the target conditional as a composition of two flow models. This formulation leads to a stable variational inference training procedure that avoids adversarial training. Our method is evaluated on a variety of inverse problems and is shown to produce high-quality samples with uncertainty quantification. We further demonstrate that our approach can be amortized for zero-shot inference.},
	urldate = {2021-09-16},
	journal = {arXiv:2002.11743 [cs, math, stat]},
	author = {Whang, Jay and Lindgren, Erik M. and Dimakis, Alexandros G.},
	month = jun,
	year = {2021},
	note = {arXiv: 2002.11743},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jagatap_algorithmic_2020,
	title = {Algorithmic {Guarantees} for {Inverse} {Imaging} with {Untrained} {Network} {Priors}},
	url = {http://arxiv.org/abs/1906.08763},
	abstract = {Deep neural networks as image priors have been recently introduced for problems such as denoising, super-resolution and inpainting with promising performance gains over hand-crafted image priors such as sparsity and low-rank. Unlike learned generative priors they do not require any training over large datasets. However, few theoretical guarantees exist in the scope of using untrained neural network priors for inverse imaging problems. We explore new applications and theory for untrained neural network priors. Specifically, we consider the problem of solving linear inverse problems, such as compressive sensing, as well as non-linear problems, such as compressive phase retrieval. We model images to lie in the range of an untrained deep generative network with a fixed seed. We further present a projected gradient descent scheme that can be used for both compressive sensing and phase retrieval and provide rigorous theoretical guarantees for its convergence. We also show both theoretically as well as empirically that with deep network priors, one can achieve better compression rates for the same image quality compared to hand crafted priors.},
	urldate = {2021-09-16},
	journal = {arXiv:1906.08763 [cs, stat]},
	author = {Jagatap, Gauri and Hegde, Chinmay},
	month = mar,
	year = {2020},
	note = {arXiv: 1906.08763},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, To\_Print},
}

@article{pan_physics-based_2020,
	title = {Physics-{Based} {Generative} {Adversarial} {Models} for {Image} {Restoration} and {Beyond}},
	url = {http://arxiv.org/abs/1808.00605},
	abstract = {We present an algorithm to directly solve numerous image restoration problems (e.g., image deblurring, image dehazing, image deraining, etc.). These problems are highly ill-posed, and the common assumptions for existing methods are usually based on heuristic image priors. In this paper, we find that these problems can be solved by generative models with adversarial learning. However, the basic formulation of generative adversarial networks (GANs) does not generate realistic images, and some structures of the estimated images are usually not preserved well. Motivated by an interesting observation that the estimated results should be consistent with the observed inputs under the physics models, we propose a physics model constrained learning algorithm so that it can guide the estimation of the specific task in the conventional GAN framework. The proposed algorithm is trained in an end-to-end fashion and can be applied to a variety of image restoration and related low-level vision problems. Extensive experiments demonstrate that our method performs favorably against the state-of-the-art algorithms.},
	urldate = {2021-09-02},
	journal = {arXiv:1808.00605 [cs]},
	author = {Pan, Jinshan and Dong, Jiangxin and Liu, Yang and Zhang, Jiawei and Ren, Jimmy and Tang, Jinhui and Tai, Yu-Wing and Yang, Ming-Hsuan},
	month = mar,
	year = {2020},
	note = {arXiv: 1808.00605},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, To\_Print},
}

@book{aubert_mathematical_2006,
	address = {New York, NY},
	series = {Applied {Mathematical} {Sciences}},
	title = {Mathematical {Problems} in {Image} {Processing}: {Partial} {Differential} {Equations} and the {Calculus} of {Variations}},
	volume = {147},
	isbn = {978-0-387-32200-1 978-0-387-44588-5},
	shorttitle = {Mathematical {Problems} in {Image} {Processing}},
	url = {http://link.springer.com/10.1007/978-0-387-44588-5},
	language = {en},
	urldate = {2021-09-14},
	publisher = {Springer New York},
	author = {Aubert, Gilles and Kornprobst, Pierre},
	editor = {Antman, S. S. and Marsden, J. E. and Sirovich, L.},
	year = {2006},
	doi = {10.1007/978-0-387-44588-5},
}

@article{dashti_bayesian_2015,
	title = {The {Bayesian} {Approach} {To} {Inverse} {Problems}},
	url = {http://arxiv.org/abs/1302.6989},
	abstract = {These lecture notes highlight the mathematical and computational structure relating to the formulation of, and development of algorithms for, the Bayesian approach to inverse problems in differential equations. This approach is fundamental in the quantification of uncertainty within applications involving the blending of mathematical models with data.},
	urldate = {2021-09-06},
	journal = {arXiv:1302.6989 [math]},
	author = {Dashti, Masoumeh and Stuart, Andrew M.},
	month = jul,
	year = {2015},
	note = {arXiv: 1302.6989},
	keywords = {Mathematics - Probability, Review},
}

@article{antun_instabilities_2020,
	title = {On instabilities of deep learning in image reconstruction and the potential costs of {AI}},
	volume = {117},
	copyright = {© 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/48/30088},
	doi = {10.1073/pnas.1907377117},
	abstract = {Deep learning, due to its unprecedented success in tasks such as image classification, has emerged as a new tool in image reconstruction with potential to change the field. In this paper, we demonstrate a crucial phenomenon: Deep learning typically yields unstable methods for image reconstruction. The instabilities usually occur in several forms: 1) Certain tiny, almost undetectable perturbations, both in the image and sampling domain, may result in severe artefacts in the reconstruction; 2) a small structural change, for example, a tumor, may not be captured in the reconstructed image; and 3) (a counterintuitive type of instability) more samples may yield poorer performance. Our stability test with algorithms and easy-to-use software detects the instability phenomena. The test is aimed at researchers, to test their networks for instabilities, and for government agencies, such as the Food and Drug Administration (FDA), to secure safe use of deep learning methods.},
	language = {en},
	number = {48},
	urldate = {2021-03-01},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Antun, Vegard and Renna, Francesco and Poon, Clarice and Adcock, Ben and Hansen, Anders C.},
	month = dec,
	year = {2020},
	pmid = {32393633},
	note = {Publisher: National Academy of Sciences
Section: Colloquium on the Science of Deep Learning},
	keywords = {AI, Lu  - 1, deep learning, image reconstruction, instability, inverse problems},
	pages = {30088--30095},
}

@article{han_image-based_2021,
	title = {Image-based {3D} {Object} {Reconstruction}: {State}-of-the-{Art} and {Trends} in the {Deep} {Learning} {Era}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Image-based {3D} {Object} {Reconstruction}},
	url = {http://arxiv.org/abs/1906.06543},
	doi = {10.1109/TPAMI.2019.2954885},
	abstract = {3D reconstruction is a longstanding ill-posed problem, which has been explored for decades by the computer vision, computer graphics, and machine learning communities. Since 2015, image-based 3D reconstruction using convolutional neural networks (CNN) has attracted increasing interest and demonstrated an impressive performance. Given this new era of rapid evolution, this article provides a comprehensive survey of the recent developments in this field. We focus on the works which use deep learning techniques to estimate the 3D shape of generic objects either from a single or multiple RGB images. We organize the literature based on the shape representations, the network architectures, and the training mechanisms they use. While this survey is intended for methods which reconstruct generic objects, we also review some of the recent works which focus on specific object classes such as human body shapes and faces. We provide an analysis and comparison of the performance of some key papers, summarize some of the open problems in this field, and discuss promising directions for future research.},
	number = {5},
	urldate = {2021-09-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Han, Xian-Feng and Laga, Hamid and Bennamoun, Mohammed},
	month = may,
	year = {2021},
	note = {arXiv: 1906.06543},
	keywords = {Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Lu - 1, Review},
	pages = {1578--1604},
}

@article{arridge_solving_2019,
	title = {Solving inverse problems using data-driven models},
	volume = {28},
	doi = {10.1017/S0962492919000059},
	abstract = {Recent research in inverse problems seeks to develop a mathematically coherent foundation for combining data-driven models, and in particular those based on deep learning, with domain-specific knowledge contained in physical–analytical models. The focus is on solving ill-posed inverse problems that are at the core of many challenging applications in the natural sciences, medicine and life sciences, as well as in engineering and industrial applications. This survey paper aims to give an account of some of the main contributions in data-driven inverse problems.},
	journal = {Acta Numerica},
	author = {Arridge, Simon and Maass, Peter and Ozan, Öktem and Schönlieb, Carola-Bibiane},
	month = may,
	year = {2019},
	keywords = {Lu  - 1, Review},
	pages = {1--174},
}

@article{liu_convergence_2018,
	title = {On the {Convergence} of {Learning}-based {Iterative} {Methods} for {Nonconvex} {Inverse} {Problems}},
	url = {http://arxiv.org/abs/1808.05331},
	abstract = {Numerous tasks at the core of statistics, learning and vision areas are specific cases of ill-posed inverse problems. Recently, learning-based (e.g., deep) iterative methods have been empirically shown to be useful for these problems. Nevertheless, integrating learnable structures into iterations is still a laborious process, which can only be guided by intuitions or empirical insights. Moreover, there is a lack of rigorous analysis about the convergence behaviors of these reimplemented iterations, and thus the significance of such methods is a little bit vague. This paper moves beyond these limits and proposes Flexible Iterative Modularization Algorithm (FIMA), a generic and provable paradigm for nonconvex inverse problems. Our theoretical analysis reveals that FIMA allows us to generate globally convergent trajectories for learning-based iterative methods. Meanwhile, the devised scheduling policies on flexible modules should also be beneficial for classical numerical methods in the nonconvex scenario. Extensive experiments on real applications verify the superiority of FIMA.},
	urldate = {2021-03-01},
	journal = {arXiv:1808.05331 [cs]},
	author = {Liu, Risheng and Cheng, Shichao and He, Yi and Fan, Xin and Lin, Zhouchen and Luo, Zhongxuan},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.05331},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{gottschling_troublesome_2020,
	title = {The troublesome kernel: why deep learning for inverse problems is typically unstable},
	shorttitle = {The troublesome kernel},
	url = {http://arxiv.org/abs/2001.01258},
	abstract = {There is overwhelming empirical evidence that Deep Learning (DL) leads to unstable methods in applications ranging from image classification and computer vision to voice recognition and automated diagnosis in medicine. Recently, a similar instability phenomenon has been discovered when DL is used to solve certain problems in computational science, namely, inverse problems in imaging. In this paper we present a comprehensive mathematical analysis explaining the many facets of the instability phenomenon in DL for inverse problems. Our main results not only explain why this phenomenon occurs, they also shed light as to why finding a cure for instabilities is so difficult in practice. Additionally, these theorems show that instabilities are typically not rare events - rather, they can occur even when the measurements are subject to completely random noise - and consequently how easy it can be to destablise certain trained neural networks. We also examine the delicate balance between reconstruction performance and stability, and in particular, how DL methods may outperform state-of-the-art sparse regularization methods, but at the cost of instability. Finally, we demonstrate a counterintuitive phenomenon: training a neural network may generically not yield an optimal reconstruction method for an inverse problem.},
	urldate = {2021-03-01},
	journal = {arXiv:2001.01258 [cs]},
	author = {Gottschling, Nina M. and Antun, Vegard and Adcock, Ben and Hansen, Anders C.},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.01258},
	keywords = {65R32, 94A08, 68T05, 65M12, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{hand_global_2018,
	title = {Global {Guarantees} for {Enforcing} {Deep} {Generative} {Priors} by {Empirical} {Risk}},
	url = {http://arxiv.org/abs/1705.07576},
	abstract = {We examine the theoretical properties of enforcing priors provided by generative deep neural networks via empirical risk minimization. In particular we consider two models, one in which the task is to invert a generative neural network given access to its last layer and another in which the task is to invert a generative neural network given only compressive linear observations of its last layer. We establish that in both cases, in suitable regimes of network layer sizes and a randomness assumption on the network weights, that the non-convex objective function given by empirical risk minimization does not have any spurious stationary points. That is, we establish that with high probability, at any point away from small neighborhoods around two scalar multiples of the desired solution, there is a descent direction. Hence, there are no local minima, saddle points, or other stationary points outside these neighborhoods. These results constitute the first theoretical guarantees which establish the favorable global geometry of these non-convex optimization problems, and they bridge the gap between the empirical success of enforcing deep generative priors and a rigorous understanding of non-linear inverse problems.},
	urldate = {2021-03-01},
	journal = {arXiv:1705.07576 [cs, math]},
	author = {Hand, Paul and Voroninski, Vladislav},
	month = dec,
	year = {2018},
	note = {arXiv: 1705.07576},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Probability},
}

@article{schwab_deep_2019-1,
	title = {Deep null space learning for inverse problems: convergence analysis and rates},
	volume = {35},
	issn = {0266-5611},
	shorttitle = {Deep null space learning for inverse problems},
	url = {https://doi.org/10.1088/1361-6420/aaf14a},
	doi = {10.1088/1361-6420/aaf14a},
	abstract = {Recently, deep learning based methods appeared as a new paradigm for solving inverse problems. These methods empirically show excellent performance but lack of theoretical justification; in particular, no results on the regularization properties are available. In particular, this is the case for two-step deep learning approaches, where a classical reconstruction method is applied to the data in a first step and a trained deep neural network is applied to improve results in a second step. In this paper, we close the gap between practice and theory for a particular network structure in a two-step approach. For that purpose, we propose using so-called null space networks and introduce the concept of -regularization. Combined with a standard regularization method as reconstruction layer, the proposed deep null space learning approach is shown to be a -regularization method; convergence rates are also derived. The proposed null space network structure naturally preserves data consistency which is considered as key property of neural networks for solving inverse problems.},
	language = {en},
	number = {2},
	urldate = {2021-03-01},
	journal = {Inverse Problems},
	author = {Schwab, Johannes and Antholzer, Stephan and Haltmeier, Markus},
	month = jan,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {025008},
}

@article{li_nett_2020,
	title = {{NETT}: solving inverse problems with deep neural networks},
	volume = {36},
	issn = {0266-5611},
	shorttitle = {{NETT}},
	url = {https://doi.org/10.1088/1361-6420/ab6d57},
	doi = {10.1088/1361-6420/ab6d57},
	abstract = {Recovering a function or high-dimensional parameter vector from indirect measurements is a central task in various scientific areas. Several methods for solving such inverse problems are well developed and well understood. Recently, novel algorithms using deep learning and neural networks for inverse problems appeared. While still in their infancy, these techniques show astonishing performance for applications like low-dose CT or various sparse data problems. However, there are few theoretical results for deep learning in inverse problems. In this paper, we establish a complete convergence analysis for the proposed NETT (network Tikhonov) approach to inverse problems. NETT considers nearly data-consistent solutions having small value of a regularizer defined by a trained neural network. We derive well-posedness results and quantitative error estimates, and propose a possible strategy for training the regularizer. Our theoretical results and framework are different from any previous work using neural networks for solving inverse problems. A possible data driven regularizer is proposed. Numerical results are presented for a tomographic sparse data problem, which demonstrate good performance of NETT even for unknowns of different type from the training data. To derive the convergence and convergence rates results we introduce a new framework based on the absolute Bregman distance generalizing the standard Bregman distance from the convex to the non-convex case.},
	language = {en},
	number = {6},
	urldate = {2021-03-01},
	journal = {Inverse Problems},
	author = {Li, Housen and Schwab, Johannes and Antholzer, Stephan and Haltmeier, Markus},
	month = jun,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {065005},
}

@inproceedings{jia_retail_2013,
	title = {Retail pricing for stochastic demand with unknown parameters: {An} online machine learning approach},
	shorttitle = {Retail pricing for stochastic demand with unknown parameters},
	doi = {10.1109/Allerton.2013.6736684},
	abstract = {The problem of dynamically pricing of electricity by a retailer for customers in a demand response program is considered. It is assumed that the retailer obtains electricity in a two-settlement wholesale market consisting of a day ahead market and a real-time market. Under a day ahead dynamic pricing mechanism, the retailer aims to learn the aggregated demand function of its customers while maximizing its retail profit. A piecewise linear stochastic approximation algorithm is proposed. It is shown that the accumulative regret of the proposed algorithm grows with the learning horizon T at the order of O(log T). It is also shown that the achieved growth rate cannot be reduced by any piecewise linear policy.},
	booktitle = {2013 51st {Annual} {Allerton} {Conference} on {Communication}, {Control}, and {Computing} ({Allerton})},
	author = {Jia, L. and Zhao, Q. and Tong, L.},
	month = oct,
	year = {2013},
	keywords = {Approximation methods, Demand response, Electricity, Load management, Piecewise linear approximation, Pricing, Real-time systems, Stochastic processes, aggregated demand function, customers, day ahead market, demand response program, dynamic pricing mechanism, electricity, electricity retail pricing, electricity supply industry, learning (artificial intelligence), online learning, online machine learning, optimal stochastic thermal control, piecewise linear policy, piecewise linear stochastic approximation algorithm, pricing, real-time market, retail pricing, retail profit, retailer, retailing, stochastic approximation, stochastic demand, unknown parameters, wholesale market},
	pages = {1353--1358},
}

@article{agarwal_kernel-based_2008,
	series = {Progress in {Modeling}, {Theory}, and {Application} of {Computational} {Intelligenc}},
	title = {Kernel-based online machine learning and support vector reduction},
	volume = {71},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231208000581},
	doi = {10.1016/j.neucom.2007.11.023},
	abstract = {We apply kernel-based machine learning methods to online learning situations, and look at the related requirement of reducing the complexity of the learnt classifier. Online methods are particularly useful in situations which involve streaming data, such as medical or financial applications. We show that the concept of span of support vectors can be used to build a classifier that performs reasonably well while satisfying given space and time constraints, thus making it potentially suitable for such online situations. The span-based heuristic is observed to be effective under stringent memory limits (that is when the number of support vectors a machine can hold is very small).},
	language = {en},
	number = {7},
	urldate = {2021-01-27},
	journal = {Neurocomputing},
	author = {Agarwal, Sumeet and Vijaya Saradhi, V. and Karnick, Harish},
	month = mar,
	year = {2008},
	keywords = {Budget algorithm, Classifier complexity reduction, Online SVMs, Span of support vectors, Support vector machines},
	pages = {1230--1237},
}

@incollection{fontenla-romero_online_2013,
	title = {Online machine learning},
	booktitle = {Efficiency and {Scalability} {Methods} for {Computational} {Intellect}},
	publisher = {IGI Global},
	author = {Fontenla-Romero, Óscar and Guijarro-Berdiñas, Bertha and Martinez-Rego, David and Pérez-Sánchez, Beatriz and Peteiro-Barral, Diego},
	year = {2013},
	pages = {27--54},
}

@misc{noauthor_online_2020,
	title = {Online machine learning},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Online_machine_learning&oldid=997018610},
	abstract = {In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction.
Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.},
	language = {en},
	urldate = {2021-01-12},
	journal = {Wikipedia},
	month = dec,
	year = {2020},
	note = {Page Version ID: 997018610},
}

@article{real_regularized_2019,
	title = {Regularized {Evolution} for {Image} {Classifier} {Architecture} {Search}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4405},
	doi = {10.1609/aaai.v33i01.33014780},
	language = {en},
	number = {01},
	urldate = {2021-01-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {4780--4789},
}

@inproceedings{long_deep_2017,
	title = {Deep {Transfer} {Learning} with {Joint} {Adaptation} {Networks}},
	url = {http://proceedings.mlr.press/v70/long17a.html},
	abstract = {Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation network...},
	language = {en},
	urldate = {2021-01-11},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {2208--2217},
}

@article{liu_learning_2020,
	title = {Learning to {Adapt} to {Evolving} {Domains}},
	volume = {33},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Hong and Long, Mingsheng and Wang, Jianmin and Wang, Yu},
	year = {2020},
}

@article{gao_efficient_2020,
	title = {Efficient {Architecture} {Search} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/2006.04027},
	abstract = {Continual learning with neural networks is an important learning framework in AI that aims to learn a sequence of tasks well. However, it is often confronted with three challenges: (1) overcome the catastrophic forgetting problem, (2) adapt the current network to new tasks, and meanwhile (3) control its model complexity. To reach these goals, we propose a novel approach named as Continual Learning with Efficient Architecture Search, or CLEAS in short. CLEAS works closely with neural architecture search (NAS) which leverages reinforcement learning techniques to search for the best neural architecture that fits a new task. In particular, we design a neuron-level NAS controller that decides which old neurons from previous tasks should be reused (knowledge transfer), and which new neurons should be added (to learn new knowledge). Such a fine-grained controller allows one to find a very concise architecture that can fit each new task well. Meanwhile, since we do not alter the weights of the reused neurons, we perfectly memorize the knowledge learned from previous tasks. We evaluate CLEAS on numerous sequential classification tasks, and the results demonstrate that CLEAS outperforms other state-of-the-art alternative methods, achieving higher classification accuracy while using simpler neural architectures.},
	urldate = {2020-12-08},
	journal = {arXiv:2006.04027 [cs, stat]},
	author = {Gao, Qiang and Luo, Zhipeng and Klabjan, Diego},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04027},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{white_bananas_2020,
	title = {{BANANAS}: {Bayesian} {Optimization} with {Neural} {Architectures} for {Neural} {Architecture} {Search}},
	shorttitle = {{BANANAS}},
	url = {http://arxiv.org/abs/1910.11858},
	abstract = {Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance. In this work, we give a thorough analysis of the "BO + neural predictor" framework by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition optimization strategy. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at https://github.com/naszilla/naszilla.},
	urldate = {2020-12-02},
	journal = {arXiv:1910.11858 [cs, stat]},
	author = {White, Colin and Neiswanger, Willie and Savani, Yash},
	month = nov,
	year = {2020},
	note = {arXiv: 1910.11858},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{white_bananas_2020-1,
	title = {{BANANAS}: {Bayesian} {Optimization} with {Neural} {Architectures} for {Neural} {Architecture} {Search}},
	shorttitle = {{BANANAS}},
	url = {http://arxiv.org/abs/1910.11858},
	abstract = {Over the past half-decade, many methods have been considered for neural architecture search (NAS). Bayesian optimization (BO), which has long had success in hyperparameter optimization, has recently emerged as a very promising strategy for NAS when it is coupled with a neural predictor. Recent work has proposed different instantiations of this framework, for example, using Bayesian neural networks or graph convolutional networks as the predictive model within BO. However, the analyses in these papers often focus on the full-fledged NAS algorithm, so it is difficult to tell which individual components of the framework lead to the best performance. In this work, we give a thorough analysis of the "BO + neural predictor" framework by identifying five main components: the architecture encoding, neural predictor, uncertainty calibration method, acquisition function, and acquisition optimization strategy. We test several different methods for each component and also develop a novel path-based encoding scheme for neural architectures, which we show theoretically and empirically scales better than other encodings. Using all of our analyses, we develop a final algorithm called BANANAS, which achieves state-of-the-art performance on NAS search spaces. We adhere to the NAS research checklist (Lindauer and Hutter 2019) to facilitate best practices, and our code is available at https://github.com/naszilla/naszilla.},
	urldate = {2020-12-02},
	journal = {arXiv:1910.11858 [cs, stat]},
	author = {White, Colin and Neiswanger, Willie and Savani, Yash},
	month = nov,
	year = {2020},
	note = {arXiv: 1910.11858},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{huang_neural_2019,
	title = {Neural {Architecture} {Search} for {Class}-incremental {Learning}},
	url = {http://arxiv.org/abs/1909.06686},
	abstract = {In class-incremental learning, a model learns continuously from a sequential data stream in which new classes occur. Existing methods often rely on static architectures that are manually crafted. These methods can be prone to capacity saturation because a neural network's ability to generalize to new concepts is limited by its fixed capacity. To understand how to expand a continual learner, we focus on the neural architecture design problem in the context of class-incremental learning: at each time step, the learner must optimize its performance on all classes observed so far by selecting the most competitive neural architecture. To tackle this problem, we propose Continual Neural Architecture Search (CNAS): an autoML approach that takes advantage of the sequential nature of class-incremental learning to efficiently and adaptively identify strong architectures in a continual learning setting. We employ a task network to perform the classification task and a reinforcement learning agent as the meta-controller for architecture search. In addition, we apply network transformations to transfer weights from previous learning step and to reduce the size of the architecture search space, thus saving a large amount of computational resources. We evaluate CNAS on the CIFAR-100 dataset under varied incremental learning scenarios with limited computational power (1 GPU). Experimental results demonstrate that CNAS outperforms architectures that are optimized for the entire dataset. In addition, CNAS is at least an order of magnitude more efficient than naively using existing autoML methods.},
	urldate = {2020-12-01},
	journal = {arXiv:1909.06686 [cs, stat]},
	author = {Huang, Shenyang and François-Lavet, Vincent and Rabusseau, Guillaume},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.06686},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yang_cars_2020,
	title = {{CARS}: {Continuous} {Evolution} for {Efficient} {Neural} {Architecture} {Search}},
	shorttitle = {{CARS}},
	url = {http://arxiv.org/abs/1909.04977},
	abstract = {Searching techniques in most of existing neural architecture search (NAS) algorithms are mainly dominated by differentiable methods for the efficiency reason. In contrast, we develop an efficient continuous evolutionary approach for searching neural networks. Architectures in the population that share parameters within one SuperNet in the latest generation will be tuned over the training dataset with a few epochs. The searching in the next evolution generation will directly inherit both the SuperNet and the population, which accelerates the optimal network generation. The non-dominated sorting strategy is further applied to preserve only results on the Pareto front for accurately updating the SuperNet. Several neural networks with different model sizes and performances will be produced after the continuous search with only 0.4 GPU days. As a result, our framework provides a series of networks with the number of parameters ranging from 3.7M to 5.1M under mobile settings. These networks surpass those produced by the state-of-the-art methods on the benchmark ImageNet dataset.},
	urldate = {2020-11-25},
	journal = {arXiv:1909.04977 [cs]},
	author = {Yang, Zhaohui and Wang, Yunhe and Chen, Xinghao and Shi, Boxin and Xu, Chao and Xu, Chunjing and Tian, Qi and Xu, Chang},
	month = mar,
	year = {2020},
	note = {arXiv: 1909.04977},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{white_local_2020,
	title = {Local {Search} is {State} of the {Art} for {Neural} {Architecture} {Search} {Benchmarks}},
	url = {http://arxiv.org/abs/2005.02960},
	abstract = {Local search is one of the simplest families of algorithms in combinatorial optimization, yet it yields strong approximation guarantees for canonical NP-Complete problems such as the traveling salesman problem and vertex cover. While it is a ubiquitous algorithm in theoretical computer science, local search is often neglected in hyperparameter optimization and neural architecture search. We show that the simplest local search instantiations achieve state-of-the-art results on multiple NAS benchmarks (NASBench-101 and NASBench-201), outperforming the most popular recent NAS algorithms. However, local search fails to perform well on the much larger DARTS search space. Motivated by these observations, we present a theoretical study which characterizes the performance of local search on graph optimization problems, backed by simulation results. This may be of independent interest beyond NAS. All code and materials needed to reproduce our results are publicly available at https://github.com/realityengines/local\_search.},
	urldate = {2020-11-24},
	journal = {arXiv:2005.02960 [cs, stat]},
	author = {White, Colin and Nolen, Sam and Savani, Yash},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.02960},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{white_local_2020-1,
	title = {Local {Search} is {State} of the {Art} for {Neural} {Architecture} {Search} {Benchmarks}},
	url = {http://arxiv.org/abs/2005.02960},
	abstract = {Local search is one of the simplest families of algorithms in combinatorial optimization, yet it yields strong approximation guarantees for canonical NP-Complete problems such as the traveling salesman problem and vertex cover. While it is a ubiquitous algorithm in theoretical computer science, local search is often neglected in hyperparameter optimization and neural architecture search. We show that the simplest local search instantiations achieve state-of-the-art results on multiple NAS benchmarks (NASBench-101 and NASBench-201), outperforming the most popular recent NAS algorithms. However, local search fails to perform well on the much larger DARTS search space. Motivated by these observations, we present a theoretical study which characterizes the performance of local search on graph optimization problems, backed by simulation results. This may be of independent interest beyond NAS. All code and materials needed to reproduce our results are publicly available at https://github.com/realityengines/local\_search.},
	urldate = {2020-11-24},
	journal = {arXiv:2005.02960 [cs, stat]},
	author = {White, Colin and Nolen, Sam and Savani, Yash},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.02960},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{casale_probabilistic_2019,
	title = {Probabilistic {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1902.05116},
	abstract = {In neural architecture search (NAS), the space of neural network architectures is automatically explored to maximize predictive accuracy for a given task. Despite the success of recent approaches, most existing methods cannot be directly applied to large scale problems because of their prohibitive computational complexity or high memory usage. In this work, we propose a Probabilistic approach to neural ARchitecture SEarCh (PARSEC) that drastically reduces memory requirements while maintaining state-of-the-art computational complexity, making it possible to directly search over more complex architectures and larger datasets. Our approach only requires as much memory as is needed to train a single architecture from our search space. This is due to a memory-efficient sampling procedure wherein we learn a probability distribution over high-performing neural network architectures. Importantly, this framework enables us to transfer the distribution of architectures learnt on smaller problems to larger ones, further reducing the computational cost. We showcase the advantages of our approach in applications to CIFAR-10 and ImageNet, where our approach outperforms methods with double its computational cost and matches the performance of methods with costs that are three orders of magnitude larger.},
	urldate = {2020-11-24},
	journal = {arXiv:1902.05116 [cs, stat]},
	author = {Casale, Francesco Paolo and Gordon, Jonathan and Fusi, Nicolo},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.05116},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xu_how_2019,
	title = {How {Powerful} are {Graph} {Neural} {Networks}?},
	url = {http://arxiv.org/abs/1810.00826},
	abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
	urldate = {2020-11-20},
	journal = {arXiv:1810.00826 [cs, stat]},
	author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.00826},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yu_how_2020,
	title = {How to {Train} {Your} {Super}-{Net}: {An} {Analysis} of {Training} {Heuristics} in {Weight}-{Sharing} {NAS}},
	shorttitle = {How to {Train} {Your} {Super}-{Net}},
	url = {http://arxiv.org/abs/2003.04276},
	abstract = {Weight sharing promises to make neural architecture search (NAS) tractable even on commodity hardware. Existing methods in this space rely on a diverse set of heuristics to design and train the shared-weight backbone network, a.k.a. the super-net. Since heuristics and hyperparameters substantially vary across different methods, a fair comparison between them can only be achieved by systematically analyzing the influence of these factors. In this paper, we therefore provide a systematic evaluation of the heuristics and hyperparameters that are frequently employed by weight-sharing NAS algorithms. Our analysis uncovers that some commonly-used heuristics for super-net training negatively impact the correlation between super-net and stand-alone performance, and evidences the strong influence of certain hyperparameters and architectural choices. Our code and experiments set a strong and reproducible baseline that future works can build on.},
	urldate = {2020-11-20},
	journal = {arXiv:2003.04276 [cs, stat]},
	author = {Yu, Kaicheng and Ranftl, Rene and Salzmann, Mathieu},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.04276},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{siems_nas-bench-301_2020,
	title = {{NAS}-{Bench}-301 and the {Case} for {Surrogate} {Benchmarks} for {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/2008.09777},
	abstract = {The most significant barrier to the advancement of Neural Architecture Search (NAS) is its demand for large computational resources, which hinders scientifically sound empirical evaluations. As a remedy, several tabular NAS benchmarks were proposed to simulate runs of NAS methods in seconds. However, all existing tabular NAS benchmarks are limited to extremely small architectural spaces since they rely on exhaustive evaluations of the space. This leads to unrealistic results that do not transfer to larger search spaces. To overcome this fundamental limitation, we propose NAS-Bench-301, the first surrogate NAS benchmark, using a search space containing \$10{\textasciicircum}\{18\}\$ architectures, many orders of magnitude larger than any previous tabular NAS benchmark. After motivating the benefits of a surrogate benchmark over a tabular one, we fit various regression models on our dataset, which consists of \${\textbackslash}sim\$60k architecture evaluations, and build surrogates via deep ensembles to also model uncertainty. We benchmark a wide range of NAS algorithms using NAS-Bench-301 and obtain comparable results to the true benchmark at a fraction of the real cost. Finally, we show how NAS-Bench-301 can be used to generate new scientific insights.},
	urldate = {2020-11-20},
	journal = {arXiv:2008.09777 [cs]},
	author = {Siems, Julien and Zimmer, Lucas and Zela, Arber and Lukasik, Jovita and Keuper, Margret and Hutter, Frank},
	month = nov,
	year = {2020},
	note = {arXiv: 2008.09777},
	keywords = {Computer Science - Machine Learning},
}

@article{zela_nas-bench-1shot1_2020,
	title = {{NAS}-{Bench}-{1Shot1}: {Benchmarking} and {Dissecting} {One}-shot {Neural} {Architecture} {Search}},
	shorttitle = {{NAS}-{Bench}-{1Shot1}},
	url = {http://arxiv.org/abs/2001.10422},
	abstract = {One-shot neural architecture search (NAS) has played a crucial role in making NAS methods computationally feasible in practice. Nevertheless, there is still a lack of understanding on how these weight-sharing algorithms exactly work due to the many factors controlling the dynamics of the process. In order to allow a scientific study of these components, we introduce a general framework for one-shot NAS that can be instantiated to many recently-introduced variants and introduce a general benchmarking framework that draws on the recent large-scale tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS methods. To showcase the framework, we compare several state-of-the-art one-shot NAS methods, examine how sensitive they are to their hyperparameters and how they can be improved by tuning their hyperparameters, and compare their performance to that of blackbox optimizers for NAS-Bench-101.},
	urldate = {2020-11-20},
	journal = {arXiv:2001.10422 [cs, stat]},
	author = {Zela, Arber and Siems, Julien and Hutter, Frank},
	month = apr,
	year = {2020},
	note = {arXiv: 2001.10422},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{dong_nas-bench-201_2020,
	title = {{NAS}-{Bench}-201: {Extending} the {Scope} of {Reproducible} {Neural} {Architecture} {Search}},
	shorttitle = {{NAS}-{Bench}-201},
	url = {http://arxiv.org/abs/2001.00326},
	abstract = {Neural architecture search (NAS) has achieved breakthrough success in a great number of applications in the past few years. It could be time to take a step back and analyze the good and bad aspects in the field of NAS. A variety of algorithms search architectures under different search space. These searched architectures are trained using different setups, e.g., hyper-parameters, data augmentation, regularization. This raises a comparability problem when comparing the performance of various NAS algorithms. NAS-Bench-101 has shown success to alleviate this problem. In this work, we propose an extension to NAS-Bench-101: NAS-Bench-201 with a different search space, results on multiple datasets, and more diagnostic information. NAS-Bench-201 has a fixed search space and provides a unified benchmark for almost any up-to-date NAS algorithms. The design of our search space is inspired from the one used in the most popular cell-based searching algorithms, where a cell is represented as a DAG. Each edge here is associated with an operation selected from a predefined operation set. For it to be applicable for all NAS algorithms, the search space defined in NAS-Bench-201 includes all possible architectures generated by 4 nodes and 5 associated operation options, which results in 15,625 candidates in total. The training log and the performance for each architecture candidate are provided for three datasets. This allows researchers to avoid unnecessary repetitive training for selected candidate and focus solely on the search algorithm itself. The training time saved for every candidate also largely improves the efficiency of many methods. We provide additional diagnostic information such as fine-grained loss and accuracy, which can give inspirations to new designs of NAS algorithms. In further support, we have analyzed it from many aspects and benchmarked 10 recent NAS algorithms.},
	urldate = {2020-11-20},
	journal = {arXiv:2001.00326 [cs]},
	author = {Dong, Xuanyi and Yang, Yi},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.00326},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{ying_nas-bench-101_2019,
	title = {{NAS}-{Bench}-101: {Towards} {Reproducible} {Neural} {Architecture} {Search}},
	shorttitle = {{NAS}-{Bench}-101},
	url = {http://proceedings.mlr.press/v97/ying19a.html},
	abstract = {Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers withou...},
	language = {en},
	urldate = {2020-11-20},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ying, Chris and Klein, Aaron and Christiansen, Eric and Real, Esteban and Murphy, Kevin and Hutter, Frank},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7105--7114},
}

@inproceedings{li_random_2020,
	title = {Random {Search} and {Reproducibility} for {Neural} {Architecture} {Search}},
	url = {http://proceedings.mlr.press/v115/li20c.html},
	abstract = {Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In order to help ground the ...},
	language = {en},
	urldate = {2020-11-12},
	booktitle = {Uncertainty in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Li, Liam and Talwalkar, Ameet},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {367--377},
}

@article{stanley_evolving_2002,
	title = {Evolving neural networks through augmenting topologies},
	volume = {10},
	issn = {1063-6560},
	url = {https://doi.org/10.1162/106365602320169811},
	doi = {10.1162/106365602320169811},
	abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
	number = {2},
	urldate = {2020-11-10},
	journal = {Evolutionary Computation},
	author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	month = jun,
	year = {2002},
	keywords = {competing conventions, genetic algorithms, network topologies, neural networks, neuroevolution, speciation},
	pages = {99--127},
}

@article{stanley_designing_2019,
	title = {Designing neural networks through neuroevolution},
	volume = {1},
	copyright = {2019 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-018-0006-z},
	doi = {10.1038/s42256-018-0006-z},
	abstract = {Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field’s contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.},
	language = {en},
	number = {1},
	urldate = {2020-11-10},
	journal = {Nature Machine Intelligence},
	author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
	month = jan,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {24--35},
}

@article{floreano_neuroevolution_2008,
	title = {Neuroevolution: from architectures to learning},
	volume = {1},
	issn = {1864-5917},
	shorttitle = {Neuroevolution},
	url = {https://doi.org/10.1007/s12065-007-0002-4},
	doi = {10.1007/s12065-007-0002-4},
	abstract = {Artificial neural networks (ANNs) are applied to many real-world problems, ranging from pattern classification to robot control. In order to design a neural network for a particular task, the choice of an architecture (including the choice of a neuron model), and the choice of a learning algorithm have to be addressed. Evolutionary search methods can provide an automatic solution to these problems. New insights in both neuroscience and evolutionary biology have led to the development of increasingly powerful neuroevolution techniques over the last decade. This paper gives an overview of the most prominent methods for evolving ANNs with a special focus on recent advances in the synthesis of learning architectures.},
	language = {en},
	number = {1},
	urldate = {2020-11-10},
	journal = {Evolutionary Intelligence},
	author = {Floreano, Dario and Dürr, Peter and Mattiussi, Claudio},
	month = mar,
	year = {2008},
	pages = {47--62},
}

@article{bello_neural_2017,
	title = {Neural {Optimizer} {Search} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1709.07417},
	abstract = {We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a domain specific language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two new optimizers, named PowerSign and AddSign, which we show transfer well and improve training on a variety of different tasks and architectures, including ImageNet classification and Google's neural machine translation system.},
	urldate = {2020-11-10},
	journal = {arXiv:1709.07417 [cs, stat]},
	author = {Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V.},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.07417},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zhou_econas_2020,
	title = {{EcoNAS}: {Finding} {Proxies} for {Economical} {Neural} {Architecture} {Search}},
	shorttitle = {{EcoNAS}},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_EcoNAS_Finding_Proxies_for_Economical_Neural_Architecture_Search_CVPR_2020_paper.html},
	urldate = {2020-11-10},
	author = {Zhou, Dongzhan and Zhou, Xinchi and Zhang, Wenwei and Loy, Chen Change and Yi, Shuai and Zhang, Xuesen and Ouyang, Wanli},
	year = {2020},
	pages = {11396--11404},
}

@article{weng_automatic_2019,
	title = {Automatic {Convolutional} {Neural} {Architecture} {Search} for {Image} {Classification} {Under} {Different} {Scenes}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2906369},
	abstract = {The recent advances in convolutional neural networks (CNNs) have used for image classification to achieve remarkable results. Different fields of image datasets will need different CNN architectures to achieve exceptional performance. However, designing a good CNN architecture is a computationally expensive task and requires expert knowledge. In this paper, we propose an effective framework to solve different image classification tasks using a convolutional neural architecture search (CNAS). The framework is inspired by current research on NAS, which automatically learns the best architecture for a specific training dataset, such as MNIST and CIFAR-10. Many search algorithms have been proposed for implementing NAS; however, insufficient attention has been paid to the selection of primitive operations (POs) in the search space. We propose a more efficient search space for learning the CNN architecture. Our search algorithm is based on Darts (a differential architecture search method), but it considers different numbers of intermediate nodes and replaces some unused POs by channel shuffle operation and squeeze-and-excitation operation. We achieve a better performance than Darts on both the CIFAR10/CIFA100 and Tiny-ImageNet datasets. We retain the none operation in deriving the architecture. The performance of the model has slightly decreased, but the number of architecture parameters has been reduced by approximately 40\%. To balance the performance and the number of architecture parameters, the framework can learn a dense architecture for high-performance machines, such as servers, but a sparse architecture for resource-constrained devices, such as embedded systems or mobile devices.},
	journal = {IEEE Access},
	author = {Weng, Y. and Zhou, T. and Liu, L. and Xia, C.},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {CIFAR-10, CIFAR10/CIFA100, CNN architecture, Computer architecture, Convolution, Feature extraction, Image classification, Microprocessors, NAS, Network architecture, Task analysis, Tiny-ImageNet datasets, Training, automatic convolutional neural architecture search, channel shuffle operation, convolutional neural architecture search, convolutional neural nets, deep learning, dense architecture, differential architecture search method, effective framework, efficient search space, embedded systems, exceptional performance, high-performance machines, image classification, image classification tasks, image datasets, image retrieval, learning (artificial intelligence), neural net architecture, search algorithm, sparse architecture, specific training dataset, squeeze-excitation operation, visual databases},
	pages = {38495--38506},
}

@article{macko_improving_2019,
	title = {Improving {Neural} {Architecture} {Search} {Image} {Classifiers} via {Ensemble} {Learning}},
	url = {http://arxiv.org/abs/1903.06236},
	abstract = {Finding the best neural network architecture requires significant time, resources, and human expertise. These challenges are partially addressed by neural architecture search (NAS) which is able to find the best convolutional layer or cell that is then used as a building block for the network. However, once a good building block is found, manual design is still required to assemble the final architecture as a combination of multiple blocks under a predefined parameter budget constraint. A common solution is to stack these blocks into a single tower and adjust the width and depth to fill the parameter budget. However, these single tower architectures may not be optimal. Instead, in this paper we present the AdaNAS algorithm, that uses ensemble techniques to compose a neural network as an ensemble of smaller networks automatically. Additionally, we introduce a novel technique based on knowledge distillation to iteratively train the smaller networks using the previous ensemble as a teacher. Our experiments demonstrate that ensembles of networks improve accuracy upon a single neural network while keeping the same number of parameters. Our models achieve comparable results with the state-of-the-art on CIFAR-10 and sets a new state-of-the-art on CIFAR-100.},
	urldate = {2020-11-10},
	journal = {arXiv:1903.06236 [cs, stat]},
	author = {Macko, Vladimir and Weill, Charles and Mazzawi, Hanna and Gonzalvo, Javier},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.06236},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{elsken_efficient_2019,
	title = {Efficient {Multi}-objective {Neural} {Architecture} {Search} via {Lamarckian} {Evolution}},
	url = {http://arxiv.org/abs/1804.09081},
	abstract = {Neural Architecture Search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1)the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption, (2) most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the entire Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform both hand-crafted as well as automatically-designed networks.},
	urldate = {2020-11-10},
	journal = {arXiv:1804.09081 [cs, stat]},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	month = feb,
	year = {2019},
	note = {arXiv: 1804.09081},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{chen_renas_2019,
	title = {{RENAS}: {Reinforced} {Evolutionary} {Neural} {Architecture} {Search}},
	shorttitle = {{RENAS}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_RENAS_Reinforced_Evolutionary_Neural_Architecture_Search_CVPR_2019_paper.html},
	urldate = {2020-11-10},
	author = {Chen, Yukang and Meng, Gaofeng and Zhang, Qian and Xiang, Shiming and Huang, Chang and Mu, Lisen and Wang, Xinggang},
	year = {2019},
	pages = {4787--4796},
}

@article{liu_block_2020,
	title = {Block {Proposal} {Neural} {Architecture} {Search}},
	volume = {PP},
	issn = {1941-0042},
	doi = {10.1109/TIP.2020.3028288},
	abstract = {Typical neural architecture search methods usually restrict the search space to the pre-defined block types for a fixed macro-architecture. However, this strategy will affect the search space and architecture flexibility if block proposal search (BPS) is not considered for NAS. As a result, block structure search is the bottleneck in many previous NAS works. In this paper, we propose a new evolutionary algorithm referred as latency EvoNAS (LEvoNAS) for block structure search, and also introduce it to the NAS framework by developing a novel two-stage framework called Block Proposal NAS (BP-NAS). Comprehensive experimental results across multiple computer vision tasks demonstrate the superiority of our approaches over the state-of-the-art lightweight models. For the classification task on the ImageNet dataset, our BPN-A is better than 1.0-MobileNetV2 [1] with similar latency, and our BPN-B saves 23.7\% latency when compared with 1.4-MobileNetV2 with higher top-1 accuracy. Furthermore, for the object detection task on the COCO dataset, the face identification task on the MegaFace dataset, and the re-identification task on Market-1501 dataset, our methods outperform MobileNetV2, which demonstrates the generalization capability of our newly proposed framework.},
	language = {eng},
	journal = {IEEE transactions on image processing: a publication of the IEEE Signal Processing Society},
	author = {Liu, Jiaheng and Zhou, Shunfeng and Wu, Yichao and Chen, Ken and Ouyang, Wanli and Xu, Dong},
	month = oct,
	year = {2020},
	pmid = {33035163},
}

@article{park_variable_2019,
	title = {Variable {Chromosome} {Genetic} {Algorithm} for {Structure} {Learning} in {Neural} {Networks} to {Imitate} {Human} {Brain}},
	volume = {9},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/15/3176},
	doi = {10.3390/app9153176},
	abstract = {This paper proposes the variable chromosome genetic algorithm (VCGA) for structure learning in neural networks. Currently, the structural parameters of neural networks, i.e., number of neurons, coupling relations, number of layers, etc., have mostly been designed on the basis of heuristic knowledge of an artificial intelligence (AI) expert. To overcome this limitation, in this study evolutionary approach (EA) has been utilized to automatically generate the proper artificial neural network (ANN) structures. VCGA has a new genetic operation called a chromosome attachment. By applying the VCGA, the initial ANN structures can be flexibly evolved toward the proper structure. The case study applied to the typical exclusive or (XOR) problem shows the feasibility of our methodology. Our approach is differentiated with others in that it uses a variable chromosome in the genetic algorithm. It makes a neural network structure vary naturally, both constructively and destructively. It has been shown that the XOR problem is successfully optimized using a VCGA with a chromosome attachment to learn the structure of neural networks. Research on the structure learning of more complex problems is the topic of our future research.},
	language = {en},
	number = {15},
	urldate = {2020-10-29},
	journal = {Applied Sciences},
	author = {Park, Kang-moon and Shin, Donghoon and Chi, Sung-do},
	month = aug,
	year = {2019},
	pages = {3176},
}

@article{shao_neighbourhood_2020,
	title = {Neighbourhood {Distillation}: {On} the benefits of non end-to-end distillation},
	shorttitle = {Neighbourhood {Distillation}},
	url = {http://arxiv.org/abs/2010.01189},
	abstract = {End-to-end training with back propagation is the standard method for training deep neural networks. However, as networks become deeper and bigger, end-to-end training becomes more challenging: highly non-convex models gets stuck easily in local optima, gradients signals are prone to vanish or explode during back-propagation, training requires computational resources and time. In this work, we propose to break away from the end-to-end paradigm in the context of Knowledge Distillation. Instead of distilling a model end-to-end, we propose to split it into smaller sub-networks - also called neighbourhoods - that are then trained independently. We empirically show that distilling networks in a non end-to-end fashion can be beneficial in a diverse range of use cases. First, we show that it speeds up Knowledge Distillation by exploiting parallelism and training on smaller networks. Second, we show that independently distilled neighbourhoods may be efficiently re-used for Neural Architecture Search. Finally, because smaller networks model simpler functions, we show that they are easier to train with synthetic data than their deeper counterparts.},
	urldate = {2020-10-29},
	journal = {arXiv:2010.01189 [cs, stat]},
	author = {Shao, Laëtitia and Moroz, Max and Eban, Elad and Movshovitz-Attias, Yair},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.01189},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{laredo_automatic_2020,
	title = {Automatic model selection for fully connected neural networks},
	volume = {8},
	issn = {2195-268X, 2195-2698},
	url = {http://link.springer.com/10.1007/s40435-020-00708-w},
	doi = {10.1007/s40435-020-00708-w},
	language = {en},
	number = {4},
	urldate = {2020-10-29},
	journal = {International Journal of Dynamics and Control},
	author = {Laredo, David and Ma, Shangjie Frank and Leylaz, Ghazaale and Schütze, Oliver and Sun, Jian-Qiao},
	month = dec,
	year = {2020},
	pages = {1063--1079},
}

@inproceedings{liu_shape_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Shape {Adaptor}: {A} {Learnable} {Resizing} {Module}},
	isbn = {978-3-030-58610-2},
	shorttitle = {Shape {Adaptor}},
	doi = {10.1007/978-3-030-58610-2_39},
	abstract = {We present a novel resizing module for neural networks: shape adaptor, a drop-in enhancement built on top of traditional resizing layers, such as pooling, bilinear sampling, and strided convolution. Whilst traditional resizing layers have fixed and deterministic reshaping factors, our module allows for a learnable reshaping factor. Our implementation enables shape adaptors to be trained end-to-end without any additional supervision, through which network architectures can be optimised for each individual task, in a fully automated way. We performed experiments across seven image classification datasets, and results show that by simply using a set of our shape adaptors instead of the original resizing layers, performance increases consistently over human-designed networks, across all datasets. Additionally, we show the effectiveness of shape adaptors on two other applications: network compression and transfer learning.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Liu, Shikun and Lin, Zhe and Wang, Yilin and Zhang, Jianming and Perazzi, Federico and Johns, Edward},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Automated Machine Learning, Neural architecture search, Resizing layer},
	pages = {661--677},
}

@incollection{vedaldi_shape_2020,
	address = {Cham},
	title = {Shape {Adaptor}: {A} {Learnable} {Resizing} {Module}},
	volume = {12357},
	isbn = {978-3-030-58609-6 978-3-030-58610-2},
	shorttitle = {Shape {Adaptor}},
	url = {http://link.springer.com/10.1007/978-3-030-58610-2_39},
	language = {en},
	urldate = {2020-10-29},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Liu, Shikun and Lin, Zhe and Wang, Yilin and Zhang, Jianming and Perazzi, Federico and Johns, Edward},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58610-2_39},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {661--677},
}

@incollection{lu_pd-darts_2020,
	address = {Cham},
	title = {{PD}-{DARTS}: {Progressive} {Discretization} {Differentiable} {Architecture} {Search}},
	volume = {12068},
	isbn = {978-3-030-59829-7 978-3-030-59830-3},
	shorttitle = {{PD}-{DARTS}},
	url = {http://link.springer.com/10.1007/978-3-030-59830-3_26},
	language = {en},
	urldate = {2020-10-29},
	booktitle = {Pattern {Recognition} and {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Li, Yonggang and Zhou, Yafeng and Wang, Yongtao and Tang, Zhi},
	editor = {Lu, Yue and Vincent, Nicole and Yuen, Pong Chi and Zheng, Wei-Shi and Cheriet, Farida and Suen, Ching Y.},
	year = {2020},
	doi = {10.1007/978-3-030-59830-3_26},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {306--311},
}

@incollection{vedaldi_single_2020,
	address = {Cham},
	title = {Single {Path} {One}-{Shot} {Neural} {Architecture} {Search} with {Uniform} {Sampling}},
	volume = {12361},
	isbn = {978-3-030-58516-7 978-3-030-58517-4},
	url = {http://link.springer.com/10.1007/978-3-030-58517-4_32},
	language = {en},
	urldate = {2020-10-29},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58517-4_32},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {544--560},
}

@article{shen_once_2020,
	title = {Once {Quantized} for {All}: {Progressively} {Searching} for {Quantized} {Efficient} {Models}},
	shorttitle = {Once {Quantized} for {All}},
	url = {http://arxiv.org/abs/2010.04354},
	abstract = {Automatic search of Quantized Neural Networks has attracted a lot of attention. However, the existing quantization aware Neural Architecture Search (NAS) approaches inherit a two-stage search-retrain schema, which is not only time-consuming but also adversely affected by the unreliable ranking of architectures during the search. To avoid the undesirable effect of the search-retrain schema, we present Once Quantized for All (OQA), a novel framework that searches for quantized efficient models and deploys their quantized weights at the same time without additional post-process. While supporting a huge architecture search space, our OQA can produce a series of ultra-low bit-width(e.g. 4/3/2 bit) quantized efficient models. A progressive bit inheritance procedure is introduced to support ultra-low bit-width. Our discovered model family, OQANets, achieves a new state-of-the-art (SOTA) on quantized efficient models compared with various quantization methods and bit-widths. In particular, OQA2bit-L achieves 64.0\% ImageNet Top-1 accuracy, outperforming its 2-bit counterpart EfficientNet-B0@QKD by a large margin of 14\% using 30\% less computation budget. Code is available at https://github.com/LaVieEnRoseSMZ/OQA.},
	urldate = {2020-10-29},
	journal = {arXiv:2010.04354 [cs, eess]},
	author = {Shen, Mingzhu and Liang, Feng and Li, Chuming and Lin, Chen and Sun, Ming and Yan, Junjie and Ouyang, Wanli},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.04354},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{lukasik_smooth_2020,
	title = {Smooth {Variational} {Graph} {Embeddings} for {Efficient} {Neural} {Architecture} {Search}},
	url = {http://arxiv.org/abs/2010.04683},
	abstract = {In this paper, we propose an approach to neural architecture search (NAS) based on graph embeddings. NAS has been addressed previously using discrete, sampling based methods, which are computationally expensive as well as differentiable approaches, which come at lower costs but enforce stronger constraints on the search space. The proposed approach leverages advantages from both sides by building a smooth variational neural architecture embedding space in which we evaluate a structural subset of architectures at training time using the predicted performance while it allows to extrapolate from this subspace at inference time. We evaluate the proposed approach in the context of two common search spaces, the graph structure defined by the ENAS approach and the NAS-Bench-101 search space, and improve over the state of the art in both.},
	urldate = {2020-10-29},
	journal = {arXiv:2010.04683 [cs, stat]},
	author = {Lukasik, Jovita and Friede, David and Zela, Arber and Stuckenschmidt, Heiner and Hutter, Frank and Keuper, Margret},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.04683},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yang_ista-nas_2020,
	title = {{ISTA}-{NAS}: {Efficient} and {Consistent} {Neural} {Architecture} {Search} by {Sparse} {Coding}},
	shorttitle = {{ISTA}-{NAS}},
	url = {http://arxiv.org/abs/2010.06176},
	abstract = {Neural architecture search (NAS) aims to produce the optimal sparse solution from a high-dimensional space spanned by all candidate connections. Current gradient-based NAS methods commonly ignore the constraint of sparsity in the search phase, but project the optimized solution onto a sparse one by post-processing. As a result, the dense super-net for search is inefficient to train and has a gap with the projected architecture for evaluation. In this paper, we formulate neural architecture search as a sparse coding problem. We perform the differentiable search on a compressed lower-dimensional space that has the same validation loss as the original sparse solution space, and recover an architecture by solving the sparse coding problem. The differentiable search and architecture recovery are optimized in an alternate manner. By doing so, our network for search at each update satisfies the sparsity constraint and is efficient to train. In order to also eliminate the depth and width gap between the network in search and the target-net in evaluation, we further propose a method to search and evaluate in one stage under the target-net settings. When training finishes, architecture variables are absorbed into network weights. Thus we get the searched architecture and optimized parameters in a single run. In experiments, our two-stage method on CIFAR-10 requires only 0.05 GPU-day for search. Our one-stage method produces state-of-the-art performances on both CIFAR-10 and ImageNet at the cost of only evaluation time.},
	urldate = {2020-10-29},
	journal = {arXiv:2010.06176 [cs]},
	author = {Yang, Yibo and Li, Hongyang and You, Shan and Wang, Fei and Qian, Chen and Lin, Zhouchen},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.06176},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{pham_efficient_2018,
	title = {Efficient {Neural} {Architecture} {Search} via {Parameter} {Sharing}},
	url = {http://arxiv.org/abs/1802.03268},
	abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89\%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65\%.},
	urldate = {2020-10-29},
	journal = {arXiv:1802.03268 [cs, stat]},
	author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.03268},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{cai_proxylessnas_2019,
	title = {{ProxylessNAS}: {Direct} {Neural} {Architecture} {Search} on {Target} {Task} and {Hardware}},
	shorttitle = {{ProxylessNAS}},
	url = {http://arxiv.org/abs/1812.00332},
	abstract = {Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. \$10{\textasciicircum}4\$ GPU hours) makes it difficult to {\textbackslash}emph\{directly\} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize{\textasciitilde}{\textbackslash}emph\{proxy\} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present {\textbackslash}emph\{ProxylessNAS\} that can {\textbackslash}emph\{directly\} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08{\textbackslash}\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\${\textbackslash}times\$ fewer parameters. On ImageNet, our model achieves 3.1{\textbackslash}\% better top-1 accuracy than MobileNetV2, while being 1.2\${\textbackslash}times\$ faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.},
	urldate = {2020-10-29},
	journal = {arXiv:1812.00332 [cs, stat]},
	author = {Cai, Han and Zhu, Ligeng and Han, Song},
	month = feb,
	year = {2019},
	note = {arXiv: 1812.00332},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lu_nsganetv2_2020,
	title = {{NSGANetV2}: {Evolutionary} {Multi}-{Objective} {Surrogate}-{Assisted} {Neural} {Architecture} {Search}},
	shorttitle = {{NSGANetV2}},
	url = {http://arxiv.org/abs/2007.10396},
	abstract = {In this paper, we propose an efficient NAS algorithm for generating task-specific models that are competitive under multiple competing objectives. It comprises of two surrogates, one at the architecture level to improve sample efficiency and one at the weights level, through a supernet, to improve gradient descent training efficiency. On standard benchmark datasets (C10, C100, ImageNet), the resulting models, dubbed NSGANetV2, either match or outperform models from existing approaches with the search being orders of magnitude more sample efficient. Furthermore, we demonstrate the effectiveness and versatility of the proposed method on six diverse non-standard datasets, e.g. STL-10, Flowers102, Oxford Pets, FGVC Aircrafts etc. In all cases, NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that NAS can be a viable alternative to conventional transfer learning approaches in handling diverse scenarios such as small-scale or fine-grained datasets. Code is available at https://github.com/mikelzc1990/nsganetv2},
	urldate = {2020-10-28},
	journal = {arXiv:2007.10396 [cs]},
	author = {Lu, Zhichao and Deb, Kalyanmoy and Goodman, Erik and Banzhaf, Wolfgang and Boddeti, Vishnu Naresh},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.10396},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{zela_understanding_2020,
	title = {Understanding and {Robustifying} {Differentiable} {Architecture} {Search}},
	url = {http://arxiv.org/abs/1909.09656},
	abstract = {Differentiable Architecture Search (DARTS) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures with very poor test performance. We study this failure mode and show that, while DARTS successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify DARTS to find solutions with less curvature and better generalization properties. Based on these observations, we propose several simple variations of DARTS that perform substantially more robustly in practice. Our observations are robust across five search spaces on three image classification tasks and also hold for the very different domains of disparity estimation (a dense regression task) and language modelling.},
	urldate = {2020-10-28},
	journal = {arXiv:1909.09656 [cs, stat]},
	author = {Zela, Arber and Elsken, Thomas and Saikia, Tonmoy and Marrakchi, Yassine and Brox, Thomas and Hutter, Frank},
	month = jan,
	year = {2020},
	note = {arXiv: 1909.09656},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xie_snas_2020,
	title = {{SNAS}: {Stochastic} {Neural} {Architecture} {Search}},
	shorttitle = {{SNAS}},
	url = {http://arxiv.org/abs/1812.09926},
	abstract = {We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets. We have released our implementation at https://github.com/SNAS-Series/SNAS-Series.},
	urldate = {2020-10-28},
	journal = {arXiv:1812.09926 [cs, stat]},
	author = {Xie, Sirui and Zheng, Hehui and Liu, Chunxiao and Lin, Liang},
	month = mar,
	year = {2020},
	note = {arXiv: 1812.09926},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xu_pc-darts_2020,
	title = {{PC}-{DARTS}: {Partial} {Channel} {Connections} for {Memory}-{Efficient} {Architecture} {Search}},
	shorttitle = {{PC}-{DARTS}},
	url = {http://arxiv.org/abs/1907.05737},
	abstract = {Differentiable architecture search (DARTS) provided a fast solution in finding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-network and searching for an optimal architecture. In this paper, we present a novel approach, namely, Partially-Connected DARTS, by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, we perform operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of super-net caused by sampling different channels. We alleviate it using edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search. Thanks to the reduced memory cost, PC-DARTS can be trained with a larger batch size and, consequently, enjoys both faster speed and higher training stability. Experimental results demonstrate the effectiveness of the proposed method. Specifically, we achieve an error rate of 2.57\% on CIFAR10 with merely 0.1 GPU-days for architecture search, and a state-of-the-art top-1 error rate of 24.2\% on ImageNet (under the mobile setting) using 3.8 GPU-days for search. Our code has been made available at: https://github.com/yuhuixu1993/PC-DARTS.},
	urldate = {2020-10-28},
	journal = {arXiv:1907.05737 [cs]},
	author = {Xu, Yuhui and Xie, Lingxi and Zhang, Xiaopeng and Chen, Xin and Qi, Guo-Jun and Tian, Qi and Xiong, Hongkai},
	month = apr,
	year = {2020},
	note = {arXiv: 1907.05737},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{liang_darts_2020,
	title = {{DARTS}+: {Improved} {Differentiable} {Architecture} {Search} with {Early} {Stopping}},
	shorttitle = {{DARTS}+},
	url = {http://arxiv.org/abs/1909.06035},
	abstract = {Recently, there has been a growing interest in automating the process of neural architecture design, and the Differentiable Architecture Search (DARTS) method makes the process available within a few GPU days. However, the performance of DARTS is often observed to collapse when the number of search epochs becomes large. Meanwhile, lots of "\{{\textbackslash}em skip-connect\}s" are found in the selected architectures. In this paper, we claim that the cause of the collapse is that there exists overfitting in the optimization of DARTS. Therefore, we propose a simple and effective algorithm, named "DARTS+", to avoid the collapse and improve the original DARTS, by "early stopping" the search procedure when meeting a certain criterion. We also conduct comprehensive experiments on benchmark datasets and different search spaces and show the effectiveness of our DARTS+ algorithm, and DARTS+ achieves \$2.32{\textbackslash}\%\$ test error on CIFAR10, \$14.87{\textbackslash}\%\$ on CIFAR100, and \$23.7{\textbackslash}\%\$ on ImageNet. We further remark that the idea of "early stopping" is implicitly included in some existing DARTS variants by manually setting a small number of search epochs, while we give an \{{\textbackslash}em explicit\} criterion for "early stopping".},
	urldate = {2020-10-28},
	journal = {arXiv:1909.06035 [cs]},
	author = {Liang, Hanwen and Zhang, Shifeng and Sun, Jiacheng and He, Xingqiu and Huang, Weiran and Zhuang, Kechen and Li, Zhenguo},
	month = oct,
	year = {2020},
	note = {arXiv: 1909.06035},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{chen_progressive_2019,
	title = {Progressive {Differentiable} {Architecture} {Search}: {Bridging} the {Depth} {Gap} {Between} {Search} and {Evaluation}},
	shorttitle = {Progressive {Differentiable} {Architecture} {Search}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Chen_Progressive_Differentiable_Architecture_Search_Bridging_the_Depth_Gap_Between_Search_ICCV_2019_paper.html},
	urldate = {2020-10-28},
	author = {Chen, Xin and Xie, Lingxi and Wu, Jun and Tian, Qi},
	year = {2019},
	pages = {1294--1303},
}

@inproceedings{wu_fbnet_2019,
	title = {{FBNet}: {Hardware}-{Aware} {Efficient} {ConvNet} {Design} via {Differentiable} {Neural} {Architecture} {Search}},
	shorttitle = {{FBNet}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_FBNet_Hardware-Aware_Efficient_ConvNet_Design_via_Differentiable_Neural_Architecture_Search_CVPR_2019_paper.html},
	urldate = {2020-10-28},
	author = {Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing and Keutzer, Kurt},
	year = {2019},
	pages = {10734--10742},
}

@article{liu_darts_2019,
	title = {{DARTS}: {Differentiable} {Architecture} {Search}},
	shorttitle = {{DARTS}},
	url = {http://arxiv.org/abs/1806.09055},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
	urldate = {2020-10-28},
	journal = {arXiv:1806.09055 [cs, stat]},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	month = apr,
	year = {2019},
	note = {arXiv: 1806.09055},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{lohmann_sulcal_1998,
	address = {Santa Barbara, CA, USA},
	title = {Sulcal basins and sulcal strings as new concepts for describing the human cortical topography},
	isbn = {978-0-8186-8460-9},
	url = {http://ieeexplore.ieee.org/document/692384/},
	doi = {10.1109/BIA.1998.692384},
	urldate = {2020-08-28},
	booktitle = {Proceedings. {Workshop} on {Biomedical} {Image} {Analysis} ({Cat}. {No}.{98EX162})},
	publisher = {IEEE Comput. Soc},
	author = {Lohmann, G. and Yves von Cramon, D.},
	year = {1998},
	pages = {24--33},
}

@misc{noauthor_rapport_nodate,
	title = {Rapport stage},
	url = {https://www.overleaf.com/project/5f15645d7115dc0001182c8f},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2020-08-19},
}

@article{irene_kaltenmark_cortical_2020,
	title = {Cortical inter-subject correspondences with optimal group-wise parcellation and sulcal pits labeling},
	abstract = {Sulcal pits are the points of maximal depth within the folds of the cortical surface. These shape descriptors give a unique opportunity to access to a rich, fine-scale rep- resentation of the geometry and the developmental milestones of the cortical surface. However, using sulcal pits analysis at group level requires new numerical tools to es- tablish inter-subject correspondences. Here, we address this issue by taking advantage of the geometrical information carried by sulcal basins that are the local patches of surfaces surrounding each sulcal pit. Our framework consists in two phases. First, we present a new method to generate a population-specific atlas of this sulcal basins organi- zation as a fold-level parcellation of the cortical surface. Then, we address the labeling of individual sulcal pits and corresponding basins with respect to this atlas. To assess their validity, we applied these methodological advances on two different populations of healthy subjects. The first database of 137 adults allowed us to compare our method to the state-of-the-art and the second database of 209 children, aged between 0 and 18 years, illustrates the robustness and relevance of our method in the context of pediatric data showing strong variations in cortical volume and folding.},
	journal = {Medical Image Analysis},
	author = {{Irene Kaltenmark} and Deruelle, Christine and Brun, Lucile and Lefèvre, Julien and Coulon, Olivier and Auzias, Guillaume},
	year = {2020},
}

@article{palacio-nino_evaluation_2019,
	title = {Evaluation {Metrics} for {Unsupervised} {Learning} {Algorithms}},
	url = {http://arxiv.org/abs/1905.05667},
	abstract = {Determining the quality of the results obtained by clustering techniques is a key issue in unsupervised machine learning. Many authors have discussed the desirable features of good clustering algorithms. However, Jon Kleinberg established an impossibility theorem for clustering. As a consequence, a wealth of studies have proposed techniques to evaluate the quality of clustering results depending on the characteristics of the clustering problem and the algorithmic technique employed to cluster data.},
	urldate = {2020-08-05},
	journal = {arXiv:1905.05667 [cs, stat]},
	author = {Palacio-Niño, Julio-Omar and Berzal, Fernando},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05667},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{im_quantitative_2011,
	title = {Quantitative comparison and analysis of sulcal patterns using sulcal graph matching: {A} twin study},
	volume = {57},
	issn = {10538119},
	shorttitle = {Quantitative comparison and analysis of sulcal patterns using sulcal graph matching},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811911004794},
	doi = {10.1016/j.neuroimage.2011.04.062},
	language = {en},
	number = {3},
	urldate = {2020-08-04},
	journal = {NeuroImage},
	author = {Im, Kiho and Pienaar, Rudolph and Lee, Jong-Min and Seong, Joon-Kyung and Choi, Yu Yong and Lee, Kun Ho and Grant, P. Ellen},
	month = aug,
	year = {2011},
	pages = {1077--1086},
}

@article{huang_consistent_2013,
	title = {Consistent {Shape} {Maps} via {Semidefinite} {Programming}},
	volume = {32},
	issn = {01677055},
	url = {http://doi.wiley.com/10.1111/cgf.12184},
	doi = {10.1111/cgf.12184},
	language = {en},
	number = {5},
	urldate = {2020-07-27},
	journal = {Computer Graphics Forum},
	author = {Huang, Qi-Xing and Guibas, Leonidas},
	month = aug,
	year = {2013},
	pages = {177--186},
}

@inproceedings{leordeanu_spectral_2005,
	address = {Beijing, China},
	title = {A spectral technique for correspondence problems using pairwise constraints},
	isbn = {978-0-7695-2334-7},
	url = {http://ieeexplore.ieee.org/document/1544893/},
	doi = {10.1109/ICCV.2005.20},
	urldate = {2020-07-27},
	booktitle = {Tenth {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV}'05) {Volume} 1},
	publisher = {IEEE},
	author = {Leordeanu, M. and Hebert, M.},
	year = {2005},
	pages = {1482--1489 Vol. 2},
}

@article{bin_luo_structural_2001,
	title = {Structural graph matching using the {EM} algorithm and singular value decomposition},
	volume = {23},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/954602/},
	doi = {10.1109/34.954602},
	number = {10},
	urldate = {2020-07-27},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Bin Luo} and Hancock, E.R.},
	month = oct,
	year = {2001},
	pages = {1120--1136},
}

@article{umeyama_eigendecomposition_1988,
	title = {An eigendecomposition approach to weighted graph matching problems},
	volume = {10},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/6778/},
	doi = {10.1109/34.6778},
	number = {5},
	urldate = {2020-07-27},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Umeyama, S.},
	month = sep,
	year = {1988},
	pages = {695--703},
}

@inproceedings{yu_generalizing_2018,
	title = {Generalizing graph matching beyond quadratic assignment model},
	volume = {2018-December},
	url = {https://asu.pure.elsevier.com/en/publications/generalizing-graph-matching-beyond-quadratic-assignment-model},
	language = {English (US)},
	urldate = {2020-02-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Yu, Tianshu and Yan, Junchi and Wang, Yilin and Liu, Wei and Li, Baoxin},
	month = jan,
	year = {2018},
	pages = {853--863},
}

@article{jain_statistical_2016,
	title = {Statistical graph space analysis},
	volume = {60},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S003132031630139X},
	doi = {10.1016/j.patcog.2016.06.023},
	language = {en},
	urldate = {2020-03-04},
	journal = {Pattern Recognition},
	author = {Jain, Brijnesh J.},
	month = dec,
	year = {2016},
	pages = {802--812},
}

@inproceedings{yan_matrix_2015,
	address = {Santiago, Chile},
	title = {A {Matrix} {Decomposition} {Perspective} to {Multiple} {Graph} {Matching}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410388/},
	doi = {10.1109/ICCV.2015.31},
	urldate = {2020-02-20},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Yan, Junchi and Xu, Hongteng and Zha, Hongyuan and Yang, Xiaokang and Liu, Huanxi and Chu, Stephen},
	month = dec,
	year = {2015},
	pages = {199--207},
}

@article{leordeanu_unsupervised_2012,
	title = {Unsupervised {Learning} for {Graph} {Matching}},
	volume = {96},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-011-0442-2},
	doi = {10.1007/s11263-011-0442-2},
	language = {en},
	number = {1},
	urldate = {2020-02-20},
	journal = {International Journal of Computer Vision},
	author = {Leordeanu, Marius and Sukthankar, Rahul and Hebert, Martial},
	month = jan,
	year = {2012},
	pages = {28--45},
}

@article{gold_graduated_1996,
	title = {A graduated assignment algorithm for graph matching},
	volume = {18},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/491619/},
	doi = {10.1109/34.491619},
	number = {4},
	urldate = {2020-02-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Gold, S. and Rangarajan, A.},
	month = apr,
	year = {1996},
	pages = {377--388},
}

@article{lohmann_automatic_2000,
	title = {Automatic labelling of the human cortical surface using sulcal basins},
	volume = {4},
	issn = {13618415},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841500000244},
	doi = {10.1016/S1361-8415(00)00024-4},
	language = {en},
	number = {3},
	urldate = {2020-07-24},
	journal = {Medical Image Analysis},
	author = {Lohmann, Gabriele and von Cramon, D.Yves},
	month = sep,
	year = {2000},
	pages = {179--188},
}

@inproceedings{ester_density-based_1996,
	address = {Portland, Oregon},
	series = {{KDD}'96},
	title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
	abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
	urldate = {2020-05-29},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {AAAI Press},
	author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jörg and Xu, Xiaowei},
	month = aug,
	year = {1996},
	keywords = {arbitrary shape of clusters, clustering algorithms, efficiency on large spatial databases, handling nlj4-275oise},
	pages = {226--231},
}

@article{bengio_machine_2020,
	title = {Machine {Learning} for {Combinatorial} {Optimization}: a {Methodological} {Tour} d'{Horizon}},
	shorttitle = {Machine {Learning} for {Combinatorial} {Optimization}},
	url = {http://arxiv.org/abs/1811.06128},
	abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
	urldate = {2020-05-18},
	journal = {arXiv:1811.06128 [cs, stat]},
	author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
	month = mar,
	year = {2020},
	note = {arXiv: 1811.06128},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jiang_unifying_2020,
	title = {Unifying {Offline} and {Online} {Multi}-graph {Matching} via {Finding} {Shortest} {Paths} on {Supergraph}},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2020.2989928},
	abstract = {This paper addresses the problem of multiple graph matching (MGM) in terms of both offline batch mode and online setting. We explore the concept of cycle-consistency over pairwise matchings and formulate the problem as finding optimal composition path on the supergraph, whose nodes refer to graphs and edge weights denote score function regarding consistency and affinity. By some theoretical study we show that the offline and online MGM on supergraph can be converted to finding all pairwise shortest paths and single-source shortest paths respectively. We adopt the Floyd algorithm [1] and shortest path faster algorithm (SPFA) [2], [3] to effectively find the optimal path. Extensive experimental results show our methods surpass two state-of-the-art MGM methods CAO-C [4] and IMGM [5], for offline and online settings respectively. Source code will be made publicly available.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jiang, Zetian and Wang, Tianzhe and Yan, Junchi},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Graph Matching, Multiple Graph Matching, Online Graph Matching, Shortest Path Search},
	pages = {1--1},
}

@article{leonardos_distributed_2016,
	title = {Distributed {Consistent} {Data} {Association}},
	url = {http://arxiv.org/abs/1609.07015},
	abstract = {Data association is one of the fundamental problems in multi-sensor systems. Most current techniques rely on pairwise data associations which can be spurious even after the employment of outlier rejection schemes. Considering multiple pairwise associations at once significantly increases accuracy and leads to consistency. In this work, we propose two fully decentralized methods for consistent global data association from pairwise data associations. The first method is a consensus algorithm on the set of doubly stochastic matrices. The second method is a decentralization of the spectral method proposed by Pachauri et al.. We demonstrate the effectiveness of both methods using theoretical analysis and experimental evaluation.},
	urldate = {2020-04-20},
	journal = {arXiv:1609.07015 [cs]},
	author = {Leonardos, Spyridon and Zhou, Xiaowei and Daniilidis, Kostas},
	month = oct,
	year = {2016},
	note = {arXiv: 1609.07015},
	keywords = {Computer Science - Multiagent Systems, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@incollection{leordeanu_integer_2009,
	title = {An {Integer} {Projected} {Fixed} {Point} {Method} for {Graph} {Matching} and {MAP} {Inference}},
	url = {http://papers.nips.cc/paper/3756-an-integer-projected-fixed-point-method-for-graph-matching-and-map-inference.pdf},
	urldate = {2020-03-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Leordeanu, Marius and Hebert, Martial and Sukthankar, Rahul},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {1114--1122},
}

@incollection{cour_balanced_2007,
	title = {Balanced {Graph} {Matching}},
	url = {http://papers.nips.cc/paper/2960-balanced-graph-matching.pdf},
	urldate = {2020-03-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Cour, Timothee and Srinivasan, Praveen and Shi, Jianbo},
	editor = {Schölkopf, B. and Platt, J. C. and Hoffman, T.},
	year = {2007},
	pages = {313--320},
}

@inproceedings{feng_zhou_factorized_2012,
	address = {Providence, RI},
	title = {Factorized graph matching},
	isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
	url = {http://ieeexplore.ieee.org/document/6247667/},
	doi = {10.1109/CVPR.2012.6247667},
	urldate = {2020-03-20},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {{Feng Zhou} and De la Torre, F.},
	month = jun,
	year = {2012},
	pages = {127--134},
}

@incollection{hutchison_reweighted_2010,
	address = {Berlin, Heidelberg},
	title = {Reweighted {Random} {Walks} for {Graph} {Matching}},
	volume = {6315},
	isbn = {978-3-642-15554-3 978-3-642-15555-0},
	url = {http://link.springer.com/10.1007/978-3-642-15555-0_36},
	language = {en},
	urldate = {2020-03-10},
	booktitle = {Computer {Vision} – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Cho, Minsu and Lee, Jungmin and Lee, Kyoung Mu},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	year = {2010},
	doi = {10.1007/978-3-642-15555-0_36},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {492--505},
}

@incollection{zhang_kergm_2019,
	title = {{KerGM}: {Kernelized} {Graph} {Matching}},
	shorttitle = {{KerGM}},
	url = {http://papers.nips.cc/paper/8595-kergm-kernelized-graph-matching.pdf},
	urldate = {2020-03-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Zhen and Xiang, Yijian and Wu, Lingfei and Xue, Bing and Nehorai, Arye},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {3335--3346},
}

@inproceedings{maset_practical_2017,
	address = {Venice},
	title = {Practical and {Efficient} {Multi}-view {Matching}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237751/},
	doi = {10.1109/ICCV.2017.489},
	urldate = {2020-03-03},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Maset, Eleonora and Arrigoni, Federica and Fusiello, Andrea},
	month = oct,
	year = {2017},
	pages = {4578--4586},
}

@inproceedings{wang_multi-image_2018,
	address = {Salt Lake City, UT, USA},
	title = {Multi-image {Semantic} {Matching} by {Mining} {Consistent} {Features}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578176/},
	doi = {10.1109/CVPR.2018.00078},
	urldate = {2020-03-02},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Wang, Qianqian and Zhou, Xiaowei and Daniilidis, Kostas},
	month = jun,
	year = {2018},
	pages = {685--694},
}

@article{chen_near-optimal_2014,
	title = {Near-{Optimal} {Joint} {Object} {Matching} via {Convex} {Relaxation}},
	url = {http://arxiv.org/abs/1402.1473},
	abstract = {Joint matching over a collection of objects aims at aggregating information from a large collection of similar instances (e.g. images, graphs, shapes) to improve maps between pairs of them. Given multiple matches computed between a few object pairs in isolation, the goal is to recover an entire collection of maps that are (1) globally consistent, and (2) close to the provided maps --- and under certain conditions provably the ground-truth maps. Despite recent advances on this problem, the best-known recovery guarantees are limited to a small constant barrier --- none of the existing methods find theoretical support when more than \$50{\textbackslash}\%\$ of input correspondences are corrupted. Moreover, prior approaches focus mostly on fully similar objects, while it is practically more demanding to match instances that are only partially similar to each other. In this paper, we develop an algorithm to jointly match multiple objects that exhibit only partial similarities, given a few pairwise matches that are densely corrupted. Specifically, we propose to recover the ground-truth maps via a parameter-free convex program called MatchLift, following a spectral method that pre-estimates the total number of distinct elements to be matched. Encouragingly, MatchLift exhibits near-optimal error-correction ability, i.e. in the asymptotic regime it is guaranteed to work even when a dominant fraction \$1-{\textbackslash}Theta{\textbackslash}left({\textbackslash}frac\{{\textbackslash}log{\textasciicircum}\{2\}n\}\{{\textbackslash}sqrt\{n\}\}{\textbackslash}right)\$ of the input maps behave like random outliers. Furthermore, MatchLift succeeds with minimal input complexity, namely, perfect matching can be achieved as soon as the provided maps form a connected map graph. We evaluate the proposed algorithm on various benchmark data sets including synthetic examples and real-world examples, all of which confirm the practical applicability of MatchLift.},
	urldate = {2020-02-27},
	journal = {arXiv:1402.1473 [cs, math, stat]},
	author = {Chen, Yuxin and Guibas, Leonidas J. and Huang, Qi-Xing},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.1473},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@incollection{hutchison_probabilistic_2005,
	address = {Berlin, Heidelberg},
	title = {Probabilistic {Subgraph} {Matching} {Based} on {Convex} {Relaxation}},
	volume = {3757},
	isbn = {978-3-540-30287-2 978-3-540-32098-2},
	url = {http://link.springer.com/10.1007/11585978_12},
	urldate = {2020-02-27},
	booktitle = {Energy {Minimization} {Methods} in {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Schellewald, Christian and Schnörr, Christoph},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Rangarajan, Anand and Vemuri, Baba and Yuille, Alan L.},
	year = {2005},
	doi = {10.1007/11585978_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {171--186},
}

@article{egozi_probabilistic_2013,
	title = {A {Probabilistic} {Approach} to {Spectral} {Graph} {Matching}},
	volume = {35},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6152128/},
	doi = {10.1109/TPAMI.2012.51},
	number = {1},
	urldate = {2020-02-27},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Egozi, Amir and Keller, Yosi and Guterman, Hugo},
	month = jan,
	year = {2013},
	pages = {18--27},
}

@inproceedings{leordeanu_semi-supervised_2011,
	address = {Barcelona, Spain},
	title = {Semi-supervised learning and optimization for hypergraph matching},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	url = {http://ieeexplore.ieee.org/document/6126507/},
	doi = {10.1109/ICCV.2011.6126507},
	urldate = {2020-02-27},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Leordeanu, Marius and Zanfir, Andrei and Sminchisescu, Cristian},
	month = nov,
	year = {2011},
	pages = {2274--2281},
}

@article{caetano_learning_2009,
	title = {Learning {Graph} {Matching}},
	volume = {31},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/4770108/},
	doi = {10.1109/TPAMI.2009.28},
	number = {6},
	urldate = {2020-02-27},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Caetano, T.S. and McAuley, J.J. and {Li Cheng} and Le, Q.V. and Smola, A.J.},
	month = jun,
	year = {2009},
	pages = {1048--1058},
}

@article{munkres_algorithms_1957,
	title = {Algorithms for the {Assignment} and {Transportation} {Problems}},
	volume = {5},
	issn = {0368-4245, 2168-3484},
	url = {http://epubs.siam.org/doi/10.1137/0105003},
	doi = {10.1137/0105003},
	language = {en},
	number = {1},
	urldate = {2020-02-27},
	journal = {Journal of the Society for Industrial and Applied Mathematics},
	author = {Munkres, James},
	month = mar,
	year = {1957},
	pages = {32--38},
}

@inproceedings{hu_distributable_2018,
	address = {Salt Lake City, UT, USA},
	title = {Distributable {Consistent} {Multi}-object {Matching}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578359/},
	doi = {10.1109/CVPR.2018.00261},
	urldate = {2020-02-27},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Hu, Nan and Huang, Qixing and Thibert, Boris and Guibas, Leonidas},
	month = jun,
	year = {2018},
	pages = {2463--2471},
}

@article{sole-ribalta_graduated_2013,
	title = {Graduated {Assignment} {Algorithm} for {Multiple} {Graph} {Matching} based on a {Common} {Labelling}},
	volume = {27},
	issn = {0218-0014, 1793-6381},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218001413500018},
	doi = {10.1142/S0218001413500018},
	abstract = {In pattern recognition applications, with the aim of increasing efficiency, it is useful to represent the elements by attributed graphs (which consider their structural properties). Under this structural representation of the elements some graph matching problems need a common labeling between the vertices of a set of graphs. Computing this common labeling is a NP-Complete problem. Nevertheless, some methodologies have been presented which obtain a sub-optimal solution in polynomial time. The drawback of these methods is that they rely on pairwise labeling computations, causing the methodologies not to consider the global information during the entire process. To solve this problem, we present a methodology which generates the common labeling by matching all graph nodes to a virtual node set. The method has been tested using three independent datasets, one synthetic and two real. Experimental results show that the presented method obtains better performance than the most popular common labeling algorithm with the same computational cost.},
	language = {en},
	number = {01},
	urldate = {2020-02-20},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Solé-Ribalta, Albert and Serratosa, Francesc},
	month = feb,
	year = {2013},
	pages = {1350001},
}

@article{park_consistent_2019,
	title = {Consistent multiple graph matching with multi-layer random walks synchronization},
	volume = {127},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865518306354},
	doi = {10.1016/j.patrec.2018.09.018},
	language = {en},
	urldate = {2020-02-25},
	journal = {Pattern Recognition Letters},
	author = {Park, Han-Mu and Yoon, Kuk-Jin},
	month = nov,
	year = {2019},
	pages = {76--84},
}

@inproceedings{yan_constrained_2016,
	address = {Cancun},
	title = {A constrained clustering based approach for matching a collection of feature sets},
	isbn = {978-1-5090-4847-2},
	url = {http://ieeexplore.ieee.org/document/7900232/},
	doi = {10.1109/ICPR.2016.7900232},
	urldate = {2020-02-25},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Yan, Junchi and {Zhe Ren} and Zha, Hongyuan and Chu, Stephen},
	month = dec,
	year = {2016},
	pages = {3832--3837},
}

@inproceedings{yan_toward_2019,
	title = {Toward {More} {Robust} {Graph} {Matching} : {Models} and {Algorithms}},
	shorttitle = {Toward {More} {Robust} {Graph} {Matching}},
	abstract = {Graph matching refers to finding vertex correspondence among two or multiple graphs, being fundamental in many applications such as image registration, DNA alignment, and automatic software bug finding. In contrast to classic two-graph setting, recently matching multiple graphs emerge for their practical usefulness and methodological potential. Starting by a brief introduction for traditional twograph matching, we walk through the recent development of multiple graph matching methods, including details for both models and algorithms. We show how learning can be inter-played with graph matching. The hope is to prompt the up-to-date advance to readers in a concrete way. Directions for future work are also discussed.},
	author = {Yan, Junchi},
	year = {2019},
}

@inproceedings{zhou_multi-image_2015,
	address = {Santiago, Chile},
	title = {Multi-image {Matching} via {Fast} {Alternating} {Minimization}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410816/},
	doi = {10.1109/ICCV.2015.459},
	urldate = {2020-02-21},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhou, Xiaowei and Zhu, Menglong and Daniilidis, Kostas},
	month = dec,
	year = {2015},
	keywords = {Not Read},
	pages = {4032--4040},
}

@incollection{fleet_graduated_2014,
	address = {Cham},
	title = {Graduated {Consistency}-{Regularized} {Optimization} for {Multi}-graph {Matching}},
	volume = {8689},
	isbn = {978-3-319-10589-5 978-3-319-10590-1},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1_27},
	language = {en},
	urldate = {2020-02-24},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Yan, Junchi and Li, Yin and Liu, Wei and Zha, Hongyuan and Yang, Xiaokang and Chu, Stephen Mingyu},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	doi = {10.1007/978-3-319-10590-1_27},
	pages = {407--422},
}

@inproceedings{shi_tensor_2016,
	address = {Las Vegas, NV, USA},
	title = {Tensor {Power} {Iteration} for {Multi}-graph {Matching}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780916/},
	doi = {10.1109/CVPR.2016.547},
	urldate = {2020-02-24},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shi, Xinchu and Ling, Haibin and Hu, Weiming and Xing, Junliang and Zhang, Yanning},
	month = jun,
	year = {2016},
	pages = {5062--5070},
}

@inproceedings{birdal_probabilistic_2019,
	address = {Long Beach, CA, USA},
	title = {Probabilistic {Permutation} {Synchronization} {Using} the {Riemannian} {Structure} of the {Birkhoff} {Polytope}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953604/},
	doi = {10.1109/CVPR.2019.01136},
	urldate = {2020-02-21},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Birdal, Tolga and Simsekli, Umut},
	month = jun,
	year = {2019},
	pages = {11097--11108},
}

@inproceedings{tron_fast_2017,
	address = {Venice},
	title = {Fast {Multi}-image {Matching} via {Density}-{Based} {Clustering}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237699/},
	doi = {10.1109/ICCV.2017.437},
	urldate = {2020-02-21},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Tron, Roberto and Zhou, Xiaowei and Esteves, Carlos and Daniilidis, Kostas},
	month = oct,
	year = {2017},
	pages = {4077--4086},
}

@inproceedings{yan_joint_2013,
	address = {Sydney, Australia},
	title = {Joint {Optimization} for {Consistent} {Multiple} {Graph} {Matching}},
	isbn = {978-1-4799-2840-8},
	url = {http://ieeexplore.ieee.org/document/6751315/},
	doi = {10.1109/ICCV.2013.207},
	urldate = {2020-02-21},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Yan, Junchi and Tian, Yu and Zha, Hongyuan and Yang, Xiaokang and Zhang, Ya and Chu, Stephen M.},
	month = dec,
	year = {2013},
	pages = {1649--1656},
}

@incollection{pachauri_solving_2013,
	title = {Solving the multi-way matching problem by permutation synchronization},
	url = {http://papers.nips.cc/paper/4987-solving-the-multi-way-matching-problem-by-permutation-synchronization.pdf},
	urldate = {2020-02-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Pachauri, Deepti and Kondor, Risi and Singh, Vikas},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {1860--1868},
}

@article{tang_initialization_2019,
	title = {Initialization and {Coordinate} {Optimization} for {Multi}-way {Matching}},
	url = {http://arxiv.org/abs/1611.00838},
	abstract = {We consider the problem of consistently matching multiple sets of elements to each other, which is a common task in fields such as computer vision. To solve the underlying NP-hard objective, existing methods often relax or approximate it, but end up with unsatisfying empirical performance due to a misaligned objective. We propose a coordinate update algorithm that directly optimizes the target objective. By using pairwise alignment information to build an undirected graph and initializing the permutation matrices along the edges of its Maximum Spanning Tree, our algorithm successfully avoids bad local optima. Theoretically, with high probability our algorithm guarantees an optimal solution under reasonable noise assumptions. Empirically, our algorithm consistently and significantly outperforms existing methods on several benchmark tasks on real datasets.},
	urldate = {2020-02-20},
	journal = {arXiv:1611.00838 [cs, stat]},
	author = {Tang, Da and Jebara, Tony},
	month = jul,
	year = {2019},
	note = {arXiv: 1611.00838},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yan_consistency-driven_2015,
	title = {Consistency-{Driven} {Alternating} {Optimization} for {Multigraph} {Matching}: {A} {Unified} {Approach}},
	volume = {24},
	issn = {1057-7149, 1941-0042},
	shorttitle = {Consistency-{Driven} {Alternating} {Optimization} for {Multigraph} {Matching}},
	url = {http://ieeexplore.ieee.org/document/7001592/},
	doi = {10.1109/TIP.2014.2387386},
	number = {3},
	urldate = {2020-02-20},
	journal = {IEEE Transactions on Image Processing},
	author = {Yan, Junchi and Wang, Jun and Zha, Hongyuan and Yang, Xiaokang and Chu, Stephen},
	month = mar,
	year = {2015},
	pages = {994--1009},
}

@article{wang_neural_2019,
	title = {Neural {Graph} {Matching} {Network}: {Learning} {Lawler}'s {Quadratic} {Assignment} {Problem} with {Extension} to {Hypergraph} and {Multiple}-graph {Matching}},
	shorttitle = {Neural {Graph} {Matching} {Network}},
	url = {http://arxiv.org/abs/1911.11308},
	abstract = {Graph matching involves combinatorial optimization based on edge-to-edge affinity matrix, which can be generally formulated as Lawler's Quadratic Assignment Problem (QAP). This paper presents a QAP network directly learning with the affinity matrix (equivalently the association graph) whereby the matching problem is translated into a vertex classification task. The association graph is learned by an embedding network for vertex classification, followed by Sinkhorn normalization and a cross-entropy loss for end-to-end learning. We further improve the embedding model on association graph by introducing Sinkhorn based constraint, and dummy nodes to deal with outliers. To our best knowledge, this is the first network to directly learn with the general Lawler's QAP. In contrast, state-of-the-art deep matching methods focus on the learning of node and edge features in two graphs respectively. We also show how to extend our network to hypergraph matching, and matching of multiple graphs. Experimental results on both synthetic graphs and real-world images show our method outperforms. For pure QAP tasks on synthetic data and QAPLIB, our method can surpass spectral matching and RRWM, especially on challenging problems.},
	urldate = {2020-02-20},
	journal = {arXiv:1911.11308 [cs, stat]},
	author = {Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.11308},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yan_adaptive_2018,
	title = {Adaptive {Discrete} {Hypergraph} {Matching}},
	volume = {48},
	issn = {2168-2267, 2168-2275},
	url = {http://ieeexplore.ieee.org/document/7858754/},
	doi = {10.1109/TCYB.2017.2655538},
	number = {2},
	urldate = {2020-02-20},
	journal = {IEEE Transactions on Cybernetics},
	author = {Yan, Junchi and Li, Changsheng and Li, Yin and Cao, Guitao},
	month = feb,
	year = {2018},
	pages = {765--779},
}

@article{wang_learning_2019,
	title = {Learning {Combinatorial} {Embedding} {Networks} for {Deep} {Graph} {Matching}},
	url = {http://arxiv.org/abs/1904.00597},
	abstract = {Graph matching refers to finding node correspondence between graphs, such that the corresponding node and edge's affinity can be maximized. In addition with its NP-completeness nature, another important challenge is effective modeling of the node-wise and structure-wise affinity across graphs and the resulting objective, to guide the matching procedure effectively finding the true matching against noises. To this end, this paper devises an end-to-end differentiable deep network pipeline to learn the affinity for graph matching. It involves a supervised permutation loss regarding with node correspondence to capture the combinatorial nature for graph matching. Meanwhile deep graph embedding models are adopted to parameterize both intra-graph and cross-graph affinity functions, instead of the traditional shallow and simple parametric forms e.g. a Gaussian kernel. The embedding can also effectively capture the higher-order structure beyond second-order edges. The permutation loss model is agnostic to the number of nodes, and the embedding model is shared among nodes such that the network allows for varying numbers of nodes in graphs for training and inference. Moreover, our network is class-agnostic with some generalization capability across different categories. All these features are welcomed for real-world applications. Experiments show its superiority against state-of-the-art graph matching learning methods.},
	urldate = {2020-02-20},
	journal = {arXiv:1904.00597 [cs]},
	author = {Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
	month = sep,
	year = {2019},
	note = {arXiv: 1904.00597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{rota_bulo_graph-based_2011,
	title = {Graph-based quadratic optimization: {A} fast evolutionary approach},
	volume = {115},
	issn = {10773142},
	shorttitle = {Graph-based quadratic optimization},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314211000713},
	doi = {10.1016/j.cviu.2010.12.004},
	language = {en},
	number = {7},
	urldate = {2020-02-20},
	journal = {Computer Vision and Image Understanding},
	author = {Rota Bulò, Samuel and Pelillo, Marcello and Bomze, Immanuel M.},
	month = jul,
	year = {2011},
	pages = {984--995},
}

@inproceedings{albarelli_matching_2009,
	address = {Kyoto},
	title = {Matching as a non-cooperative game},
	isbn = {978-1-4244-4420-5},
	url = {http://ieeexplore.ieee.org/document/5459312/},
	doi = {10.1109/ICCV.2009.5459312},
	urldate = {2020-02-20},
	booktitle = {2009 {IEEE} 12th {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Albarelli, Andrea and Rota Bulo, Samuel and Torsello, Andrea and Pelillo, Marcello},
	month = sep,
	year = {2009},
	pages = {1319--1326},
}

@incollection{ferrari_incremental_2018,
	address = {Cham},
	title = {Incremental {Multi}-graph {Matching} via {Diversity} and {Randomness} {Based} {Graph} {Clustering}},
	volume = {11217},
	isbn = {978-3-030-01260-1 978-3-030-01261-8},
	url = {http://link.springer.com/10.1007/978-3-030-01261-8_9},
	urldate = {2020-02-19},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Yu, Tianshu and Yan, Junchi and Liu, Wei and Li, Baoxin},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01261-8_9},
	pages = {142--158},
}

@inproceedings{yan_short_2016,
	address = {New York, New York, USA},
	title = {A {Short} {Survey} of {Recent} {Advances} in {Graph} {Matching}},
	isbn = {978-1-4503-4359-6},
	url = {http://dl.acm.org/citation.cfm?doid=2911996.2912035},
	doi = {10.1145/2911996.2912035},
	language = {en},
	urldate = {2020-02-19},
	booktitle = {Proceedings of the 2016 {ACM} on {International} {Conference} on {Multimedia} {Retrieval} - {ICMR} '16},
	publisher = {ACM Press},
	author = {Yan, Junchi and Yin, Xu-Cheng and Lin, Weiyao and Deng, Cheng and Zha, Hongyuan and Yang, Xiaokang},
	year = {2016},
	pages = {167--174},
}

@article{williams_multiple_1997,
	title = {Multiple graph matching with {Bayesian} inference},
	volume = {18},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865597001177},
	doi = {10.1016/S0167-8655(97)00117-7},
	language = {en},
	number = {11-13},
	urldate = {2020-02-19},
	journal = {Pattern Recognition Letters},
	author = {Williams, Mark L. and Wilson, Richard C. and Hancock, Edwin R.},
	month = nov,
	year = {1997},
	pages = {1275--1281},
}

@article{yan_multi-graph_2016,
	title = {Multi-{Graph} {Matching} via {Affinity} {Optimization} with {Graduated} {Consistency} {Regularization}},
	volume = {38},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/7254216/},
	doi = {10.1109/TPAMI.2015.2477832},
	number = {6},
	urldate = {2020-02-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Yan, Junchi and Cho, Minsu and Zha, Hongyuan and Yang, Xiaokang and Chu, Stephen M.},
	month = jun,
	year = {2016},
	pages = {1228--1242},
}

@inproceedings{zhou_deformable_2013,
	address = {Portland, OR, USA},
	title = {Deformable {Graph} {Matching}},
	isbn = {978-0-7695-4989-7},
	url = {http://ieeexplore.ieee.org/document/6619220/},
	doi = {10.1109/CVPR.2013.376},
	urldate = {2020-02-19},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhou, Feng and De la Torre, Fernando},
	month = jun,
	year = {2013},
	pages = {2922--2929},
}

@article{escolano_graph_2017,
	title = {Graph {Similarity} through {Entropic} {Manifold} {Alignment}},
	volume = {10},
	issn = {1936-4954},
	url = {https://epubs.siam.org/doi/10.1137/15M1032454},
	doi = {10.1137/15M1032454},
	language = {en},
	number = {2},
	urldate = {2020-02-19},
	journal = {SIAM Journal on Imaging Sciences},
	author = {Escolano, Francisco and Hancock, Edwin R. and Lozano, Miguel A.},
	month = jan,
	year = {2017},
	pages = {942--978},
}

@article{lozano_graph_2013,
	title = {Graph matching and clustering using kernel attributes},
	volume = {113},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231213001793},
	doi = {10.1016/j.neucom.2013.01.015},
	language = {en},
	urldate = {2020-02-19},
	journal = {Neurocomputing},
	author = {Lozano, Miguel Angel and Escolano, Francisco},
	month = aug,
	year = {2013},
	pages = {177--194},
}

@incollection{hutchison_significant_2004,
	address = {Berlin, Heidelberg},
	title = {A {Significant} {Improvement} of {Softassign} with {Diffusion} {Kernels}},
	volume = {3138},
	isbn = {978-3-540-22570-6 978-3-540-27868-9},
	url = {http://link.springer.com/10.1007/978-3-540-27868-9_7},
	urldate = {2020-02-19},
	booktitle = {Structural, {Syntactic}, and {Statistical} {Pattern} {Recognition}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lozano, Miguel Angel and Escolano, Francisco},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Fred, Ana and Caelli, Terry M. and Duin, Robert P. W. and Campilho, Aurélio C. and de Ridder, Dick},
	year = {2004},
	doi = {10.1007/978-3-540-27868-9_7},
	pages = {76--84},
}

@article{zaslavskiy_path_2009,
	title = {A {Path} {Following} {Algorithm} for the {Graph} {Matching} {Problem}},
	volume = {31},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/4641936/},
	doi = {10.1109/TPAMI.2008.245},
	number = {12},
	urldate = {2020-02-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zaslavskiy, M. and Bach, F. and Vert, J.-P.},
	month = dec,
	year = {2009},
	pages = {2227--2242},
}

@article{foggia_graph_2014,
	title = {{GRAPH} {MATCHING} {AND} {LEARNING} {IN} {PATTERN} {RECOGNITION} {IN} {THE} {LAST} 10 {YEARS}},
	volume = {28},
	issn = {0218-0014, 1793-6381},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218001414500013},
	doi = {10.1142/S0218001414500013},
	abstract = {In this paper, we examine the main advances registered in the last ten years in Pattern Recognition methodologies based on graph matching and related techniques, analyzing more than 180 papers; the aim is to provide a systematic framework presenting the recent history and the current developments. This is made by introducing a categorization of graph-based techniques and reporting, for each class, the main contributions and the most outstanding research results.},
	language = {en},
	number = {01},
	urldate = {2020-02-18},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Foggia, Pasquale and Percannella, Gennaro and Vento, Mario},
	month = feb,
	year = {2014},
	pages = {1450001},
}

@article{sole-ribalta_models_2011,
	title = {Models and algorithms for computing the common labelling of a set of attributed graphs},
	volume = {115},
	issn = {10773142},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1077314211000750},
	doi = {10.1016/j.cviu.2010.12.007},
	language = {en},
	number = {7},
	urldate = {2020-02-18},
	journal = {Computer Vision and Image Understanding},
	author = {Solé-Ribalta, Albert and Serratosa, Francesc},
	month = jul,
	year = {2011},
	pages = {929--945},
}

@article{emmert-streib_fifty_2016,
	title = {Fifty years of graph matching, network alignment and network comparison},
	volume = {346-347},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002002551630010X},
	doi = {10.1016/j.ins.2016.01.074},
	language = {en},
	urldate = {2020-02-18},
	journal = {Information Sciences},
	author = {Emmert-Streib, Frank and Dehmer, Matthias and Shi, Yongtang},
	month = jun,
	year = {2016},
	pages = {180--197},
}

@inproceedings{cachia_modeles_2003,
	title = {Modèles statistiques morphométriques et structurels du cortex pour l'étude du développement cérébral},
	abstract = {La recherche des variations anatomiques du cortex, complementaire des investigations fonctionnelles, a ete fortement stimulee ces dernieres annees par le developpement des methodes d'analyse des images cerebrales. Ces nouvelles possibilites ont conduit a la creation de vastes projets de cartographie anatomo-fonctionnelle du cerveau humain, comparables par l'ampleur qu'ils pourraient prendre aux projets de cartographie du genome. Durant les annees 90, la communaute de la neuroimagerie a choisi d'apprehender ce probleme en developpant une technique appelee la normalisation spatiale. Il s'agit de doter chaque cerveau d'un systeme de coordonnees (surfaciques ou volumiques) qui indiquent une localisation dans un cerveau de reference. Ce systeme s'obtient en deformant chaque nouveau cerveau de maniere a l'ajuster autant que possible au cerveau de reference. Cependant, cette morphometrie fond ee sur la technique de normalisation spatiale a des limites. En effet, il est largement admis qu'elle ne permet pas de gerer precisement la tres grande variabilite des plissements corticaux et ne donne acces qu'aux differences anatomiques les plus marquees. Ces considerations ont motive le developpement de nouveaux outils de morphometrie, permettant l'analyse ne des structures corticales. Jusqu'a ces dernieres annees, une telle morphometrie structurelle, prenant en compte les particularites anatomiques individuelles de chaque cortex, etait limitee par la difculte et la lourdeur du travail «manuel» a realiser. Le developpement recent de nouveaux outils d'analyse d'images, permettant d'extraire et de reconnaitre automatiquement les sillons corticaux des images IRM anatomiques, a modie cet etat de fait et a ouvert la voie aux etudes a grandes echelles de morphometrie structurelle. Cependant, d'un point de vue anatomo-fonctionnel, la structure de base du cortex est le gyrus et non pas le sillon. Or, si la litterature propose maintenant de nombreuses methodes dediees aux sillons corticaux, il n'en existe aucune specifique aux gyri, essentiellement a cause de leur tres grande variabilite morphologique. Le premier axe de travail de cette these est le developpement d'une methode entierement automatique pour les segmenter, prenant en compte leur anatomie individuelle. Cette methode propose un formalisme generique pour definir chaque gyrus a partir d'un ensemble de sillons-frontieres le delimitant; un critere de distance, sous-jacent au diagramme de Voronoi utilise pour parcelliser la surface corticale, permet d'extrapoler cette definition dans les zones ou les sillons sont interrompus. L'etude des mecanismes mis en jeu lors du plissement du cortex durant son developpement, ante- et post-natal, est un point cle pour analyser et comprendre les variations de l'anatomie corticale, normale ou non, et caracteriser ses liens avec le fonctionnement du cerveau. Des travaux recents suggerent qu'il existerait une proto-organisation sulcale stable, visible sur le cerveau du foeœtus, et qui laisserait une empreinte dans le relief cortical adulte. Pour le deuxieme axe de travail de cette these, nous avons essaye de recouvrer les traces de ces structures enfouies, les racines sulcales, inscrites dans les plissements corticaux. Nous avons pour cela developpe un modele original du cortex, le primal sketch des courbures, permettant une description multi-echelles et structurelle de la courbure corticale. Cette description est issue d'un lissage surfacique de la carte (2D) de la courbure, obtenu par l'implantation de l'equation de la chaleur, calculee geodesiquement au maillage de la surface corticale. Cette description nous a permis de recouvrer les deux racines sulcales putatives enfouies dans le sillon central, et les quatre racines du sillon temporal superieur. En parallele, nous avons initie une etude directe des premiers plis sulcaux a travers la reconstruction tridimensionnel du cerveau foeœtal in utero.},
	author = {Cachia, Arnaud},
	year = {2003},
}

@book{cachia_these_2003,
	title = {Thèse : {Modèle} statistiques morphométriques et structurels du cortex pour l'étude du développement cérebral},
	author = {Cachia, Arnaud},
	year = {2003},
}

@article{auzias_deep_2015,
	title = {Deep sulcal landmarks: {Algorithmic} and conceptual improvements in the definition and extraction of sulcal pits},
	volume = {111},
	issn = {10538119},
	shorttitle = {Deep sulcal landmarks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811915001007},
	doi = {10.1016/j.neuroimage.2015.02.008},
	language = {en},
	urldate = {2020-02-17},
	journal = {NeuroImage},
	author = {Auzias, G. and Brun, L. and Deruelle, C. and Coulon, O.},
	month = may,
	year = {2015},
	pages = {12--25},
}

@article{takerkart_structural_2017,
	title = {Structural graph-based morphometry: {A} multiscale searchlight framework based on sulcal pits},
	volume = {35},
	issn = {13618415},
	shorttitle = {Structural graph-based morphometry},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841516300251},
	doi = {10.1016/j.media.2016.04.011},
	language = {en},
	urldate = {2020-02-17},
	journal = {Medical Image Analysis},
	author = {Takerkart, Sylvain and Auzias, Guillaume and Brun, Lucile and Coulon, Olivier},
	month = jan,
	year = {2017},
	pages = {32--45},
}
