 % CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{pifont}
\usepackage[misc]{ifsym}


\newcommand{\tf}[1]{{\color{red} [ToFill]}}
\newcommand{\tc}[1]{{\color{red} [ToCite]}}

\newcommand{\lpk}[1]{\textcolor{blue}{\bf \small [#1 --lpk]}}


\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{7436} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Fine-grained Audible Video Description}

\author{$^\star$Xuyang Shen$^{2}$, $^\star$Dong Li$^{1}$, $^\star$Jinxing Zhou$^{3}$, Zhen Qin$^{2}$, Bowen He$^{2}$, Xiaodong Han$^{2}$, Aixuan Li$^{4}$, \\
        Yuchao Dai$^{4}$, Lingpeng Kong$^{5}$, Meng Wang$^{3}$, Yu Qiao$^{1}$, $^\textrm{\Letter}$ Yiran Zhong$^{1}$
\vspace{2mm} \\
$^{1}$Shanghai Artificial Intelligence Laboratory, 
$^{2}$OpenNLPLab,
$^{3}$Hefei University of Technology, \\
$^{4}$Northwestern Polytechnical University,
$^{5}$The University of Hong Kong 
}
\maketitle

\blfootnote{$^{\star}$These authors have equal contributions. $^\textrm{\Letter}$Yiran Zhong is the corresponding author (e-mail: zhongyiran@gmail.com).}



%%%%%%%%% ABSTRACT
% \vspace{3mm}
\begin{abstract}
\vspace{-2mm}
We explore a new task for audio-visual-language modeling called fine-grained audible video description (FAVD). It aims to provide detailed textual descriptions for the given audible videos, including the appearance and spatial locations of each object, the actions of moving objects, and the sounds in videos. Existing visual-language modeling tasks often concentrate on visual cues in videos while undervaluing the language and audio modalities. On the other hand, FAVD requires not only audio-visual-language modeling skills but also paragraph-level language generation abilities. We construct the first fine-grained audible video description benchmark (FAVDBench) to facilitate this research. For each video clip, we first provide a one-sentence summary of the video, \emph{\ie}, the caption, followed by 4-6 sentences describing the visual details and 1-2 audio-related descriptions at the end. The descriptions are provided in both English and Chinese. We create two new metrics for this task: an \emph{EntityScore} to gauge the completeness of entities in the visual descriptions, and an \emph{AudioScore} to assess the audio descriptions. As a preliminary approach to this task, we propose an audio-visual-language transformer that extends existing video captioning model with an additional audio branch. We combine the masked language modeling and auto-regressive language modeling losses to optimize our model so that it can produce paragraph-level descriptions. We illustrate the efficiency of our model in audio-visual-language modeling by evaluating it against the proposed benchmark using both conventional captioning metrics and our proposed metrics. We further put our benchmark to the test in video generation models, demonstrating that employing fine-grained video descriptions can create more intricate videos than using captions. Code and dataset are available at {\color{red}{\textit{\href{https://github.com/OpenNLPLab/FAVDBench}{https://github.com/OpenNLPLab/FAVDBench}}}}. Our online benchmark is available at \color{red}{\textit{\href{http://www.avlbench.opennlplab.cn}{www.avlbench.opennlplab.cn}}}. 

\newpage

\end{abstract}


%%%%%%%%% MAIN CONTENTS
\vspace{-6.3mm}
\input{1_introduction}

    \input{table1}
    

\input{2_related_work}

    \begin{figure*}[t]
      \centering
      \includegraphics[width=0.88\textwidth]{figures/fig2_stat.pdf}
      \vspace{-4mm}
      \caption{\textbf{Video distribution of 5 major categories and 71 sub-classes in FAVDBench.} The value represents the total occurrence in visual of each class. The vehicles class is colored blue, contains 14 sub-classes. There are 24 sub-classes in instruments, colored orange. The people category includes man, woman, boy, girl, and baby, which are colored yellow. The category of human-related activities is colored green and contains 11 sub-classes. There are 17 sub-classes in animals, colored purple. Best view in color.}
      \vspace{-5mm}
      \label{fig:dataset_stat}
    \end{figure*}
    

\input{3_video_description}

  
    % \input{table2}
    
\input{4_methodology}

\input{5_experiments}

\input{6_conclusion}


\input{7_appendix}

\clearpage 


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
