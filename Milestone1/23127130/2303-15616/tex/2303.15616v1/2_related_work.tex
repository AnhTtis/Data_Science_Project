
\section{Related Work}
\label{sec:related_work}


\noindent
\textbf{Video captioning.}
\label{subsec:video_caption}
Video captioning aims to describe the main content in a video with one concise natural language sentence.
The pioneer works mainly follow the sequence to sequence pipeline, where the Convolutional Neural Network is used to encode the video frame features and the Recurrent Neural Network is used to decode the predicted sentence~\cite{venugopalan2015sequence,xu2017learning,wang2018reconstruction,pei2019memory}.
Some following works adopt the Transformer architecture for efficient captioning~\cite{fang2020video2commonsense,luo2020univl,lin2022swinbert,lu2022linear,qin2022devil,lu2022linear,cheng2022implicit,zhong20183d,zhang2021depth}.
Recently, Tang \etal~\cite{tang2021clip4caption} use the pretrained CLIP~\cite{radford2021learning} to extract advanced text and visual features that improve the captioning performance significantly.
Besides, there are several works that try to explore the spatio-temporal clues~\cite{aafaq2019spatio} or object relations~\cite{zhang2020object,pan2020spatio} to give more accurate descriptions.
However, it is hard for these captioning models to detail the rich semantics in a video with only one sentence. 
Our proposed FAVD task carries the FAVDBench dataset which provides fine-grained annotations allowing us to train an intelligent robot that can tell rich video details.


\noindent
\textbf{Dense video captioning.}
\label{subsec:dense_video_caption}
Since there are usually complex content and various events contained in a long and untrimmed video, it is insufficient to describe the video with only one sentence.
For this reason, dense video captioning is proposed ~\cite{krishna2017dense} which aims to automatically localize the multiple temporal events and generate corresponding captions.
Most of the related works follow the ``localize then describe'' scheme~\cite{zhou2018end,li2018jointly,mun2019streamlined,wang2018bidirectional,sun2021getam,sun2023munet} which first predicts many event proposals and then generates its captions.
Unlike this, Wang \etal~\cite{wang2021end} propose an end-to-end captioning model that decodes the event proposals and captions parallelly. 
Some methods utilize the multi-modal information such as audio~\cite{iashin2020multi,iashin2020better,rahman2019watch,zhou2023improving} and motion~\cite{chen2020learning} in videos for better captioning.
Although these methods achieve satisfactory performance on dense video captioning, they all manipulate the frame-level features that prohibit the network from learning fine-grained details, which is indeed the essential expectation of the proposed FAVD task.
Unlike them, the proposed baseline framework encodes the patch-level video representations with the aggregation of multi-modal tokens that enables to give more detailed descriptions.

\noindent
\textbf{Audio captioning.}
\label{subsec:audio_caption}
Unlike video captioning and dense video captioning which mainly focus on the visual domain, audio captioning aims to directly generate text description for audio in the wild~\cite{kim2019audiocaps,mei2021encoder,eren2021audio,liu2021cl4ac,ye2021improving}.
In the initial work, Kim \etal~\cite{kim2019audiocaps} propose an encoder-decoder method, where the Bi-LSTM is used to generate the captions word by word.
Recently, Mei \etal~\cite{mei2021audio} use the Transformer backbone to encode the long-range dependencies in the audio signal and output the whole predicted sentence.
Though these methods enable us to describe the audio to some extent, we argue that it is hard for accurate audio captioning without giving any visual information.
FAVD takes both audio and visual signals as inputs, which eases the description generation.

\noindent
\textbf{Audio-visual-language dataset.}
Existing captioning-related multi-modal datasets mainly focus on two modalities: the \emph{vision-language} datasets for (dense) video captioning, such as MSVD~\cite{chen2011collecting}, MSR-VTT~\cite{xu2016msr}, VATEX~\cite{wang2019vatex}, TVC~\cite{lei2020tvr}, and YouCook\uppercase\expandafter{\romannumeral2}~\cite{ZhXuCoAAAI18};
or the \emph{audio-language} datasets for audio captioning, such as AudioCaps~\cite{kim2019audiocaps} and Clotho~\cite{drossos2020clotho}.
A few video captioning works try to introduce the audio track contained in videos of vision-language datasets, but the audio actually limits to speech domain~\cite{rohrbach2015dataset,ZhXuCoAAAI18,monfort2021spoken,lei2020tvr}.
While videos in the proposed FAVDBench are collected from various life-related scenes that extend the diversity of the audio. Besides, the audio and visual signals are usually semantic corresponding which makes it a truly audio-visual-language dataset.
