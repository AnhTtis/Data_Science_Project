
\vspace{-6mm}
\section{Introduction}
\vspace{-1mm}
\label{sec:intro}

\begin{figure*}[t]
  \centering
   \includegraphics[width=0.88\textwidth]{figures/fig1_dataset_intro.pdf}
   \vspace{-4mm}
   \caption{\textbf{Comparison of the proposed FAVD task with existing captioning tasks.} (a) Video captioning (VC) uses one sentence to describe the main content of the video. (b) Dense video captioning aims to localize the multiple temporal events and generate corresponding descriptions. Both VC and DVC describe the salient events in videos while losing many details, such as the appearance of objects, spatial relations, and sounds. (c) The proposed FAVD tries to generate a paragraph-level description that contains the caption, named as \textbf{Summary}, and the audio-visual descriptions, abbreviated as \textbf{A.} and \textbf{V.}.}
   \vspace{-5mm}
   \label{fig:dataset_intro}
\end{figure*}


Language serves as the primary form of human communication, providing not only complementary information to other modalities such as vision and audio, but also an efficient means of exchanging information~\cite{leech1974semantics,origgi2000evolution,hayakawa2019language}. For example, we can use voice navigation to guide us to our destination. Visually impaired people can watch a movie by listening to its narration. The former shows that the language can provide complementary information to other modalities, while the latter indicates that the language can carry the most information in other modalities.

Recent multi-modal modeling tasks attempt to connect the language to other modalities, including image/video captioning~\cite{Vinyals_2015_CVPR,ordonez2011im2text,li2022comprehending,gurari2020captioning,pan2020x,pei2019memory,wang2018reconstruction,zhang2020object,song2022memorial,song2022contextual,zhou2021semi}, text-to-image/video synthesis~\cite{huang2022dse,wu2022nuwa,gu2022vector,ruan2021dae,ramesh2021zero,ramesh2022hierarchical,hong2022cogvideo,singer2022make,ho2022imagen,villegas2022phenaki}, text-driven image/video manipulation~\cite{nam2018text,li2020manigan,bar2022text2live,zhou2022audio,zhou2023audio}, and \etc.
However, in these tasks, since the language is frequently used to provide complementary information to other modalities, they often fail to give a fine-grained description of the exchange of information between modalities, and only consider the simplified language, \ie, one-sentence captions. 
Due to the conciseness of captions, only salient objects and activities have got descriptions. As a result, the information carried by the captions will be much less than that carried by other modalities, causing significant information loss when exchanging information from other modalities to language. 

In this paper, we consider language as a tool for conveying information from other modalities in multi-modal modeling and propose a new task called Fine-grained Audible Video Description (FAVD).
We compare it with existing captioning tasks in Fig.~\ref{fig:dataset_intro}. Video captioning aims to generate one concise sentence to summarize the whole video. Dense video captioning first detects the multiple temporal events and then provides one caption for each event. The generated sentences of these two tasks merely cover the main objects or movements while losing many details when translating the video information into language.
Unlike this, the FAVD task requires a model to describe videos in a similar way that humans do, \ie, starting with an overview of the video and then focusing on each fine-grained detail to concrete the description. In this case, most video information can be preserved in the language modality. Since a video contains both visual and audio signals, we also include audio descriptions in our task. 

FAVD is a non-trivial task. In addition to long-text modeling abilities, it calls for a finer-grained visual understanding than earlier video captioning tasks, \ie, it needs to recognize all objects and actions in videos, as well as describe the appearance and spatial locations of each object and the sounds in videos. To facilitate this task, we construct the first fine-grained audible video description benchmark (FAVDBench), which allows the model to be trained in a supervised manner.

FAVDBench is made up of over 11,000 video clips culled from millions of YouTube videos and each clip is sourced by querying more than 70 life-related categories to fulfill the diversity of the benchmark. We annotate the video descriptions based on human behaviors. The annotations of each video clip begin with a one-sentence caption that describes the salient objects and activities, followed by 4-6 sentences that describe visual details including the appearance and spatial locations of each object, and the actions or movements of moving objects. To make the task consider audio information, we include 1-2 sentences of audio-related descriptions at the end of the whole descriptions. The descriptions are provided in both English and Chinese with human translation. An annotation example can be found in Fig.~\ref{fig:dataset_intro} and Fig.~\ref{fig:annotation}.
We design two new metrics for the FAVD task. One called \emph{EntityScore}, which assesses the completeness of the information that is transferred from the videos to the descriptions by gauging the completeness of entities in the visual descriptions. The other is \emph{AudioScore}, which measures the audio description in the feature space of a pretrained audio-visual-language model.

We provide a baseline model for this new task. The model is based on an existing end-to-end video captioning model~\cite{lin2022swinbert} with an additional audio branch. We also extend the visual-language transformer to the audio-visual-language transformer in order to mix multi-modal tokens. Existing video captioning models often adopt Masked Language Modelling (MLM) loss to optimize the caption generation. However, because our task requires the model to describe fine-grained details, the model should also be capable of general language modeling. To this end, we include the auto-regressive language modeling (ALM) loss in our baseline model.

We extensively evaluate the baseline model against the proposed FAVDBench using both the conventional captioning metrics and our proposed metrics and demonstrate the effectiveness of our model in audio-visual-language modeling and fine-grained description generation. To qualitatively illustrate the information preservation in modality information exchange, we further put our benchmark to the test in video generation models, showing more intricate videos than using captions.

