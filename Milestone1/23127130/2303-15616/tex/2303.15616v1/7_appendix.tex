\section{Appendix}

In the appendix, we additionally provide the word frequency of FAVDBench and more ablation study results such as the impact of different attention masking strategies in AVLFormer and extended quantitative results on FAVDBench. We also include more video generation samples as well as their quantitative results. A video (\href{https://youtu.be/iWJvTB-bTWk}{https://youtu.be/iWJvTB-bTWk}) containing some samples in FAVDBench is attached to this supplementary material. 

\subsection{The FAVDBench}
\label{sec:appendix_dataset}




\noindent
\textbf{Word Frequency.}
As FAVDBench provides both English and Chinese descriptions, we plot the frequency of English and Chinese vocabularies in Fig.~\ref{fig:word_freq}. The most common words in descriptions, such "man" and "woman," are nouns. Whereas, both adjectives and prepositions also appear in the most frequent vocabularies of descriptions (\eg, ``red'', ``black'', ``left'', ``right''). It is worth noting that sound-related vocabularies occur in the word frequency diagram, such as ``sound''.  It proves that our descriptions of videos thoroughly capture the visual and audio details, including actions, characteristics, and relative spatial relations. 

\begin{figure}[t]
  \centering
  \vspace{1mm}
  \includegraphics[width=0.48\textwidth]{figures/sup_figure1.pdf}
  \caption{\textbf{Word Frequency.} We count the word frequency of Chinese annotation (upper) and English annotation (bottom) on FAVDBench.}
  \label{fig:word_freq}
\end{figure}


\subsection{Experimental Results}
\label{sec:appendix_experiments}



\noindent
\textbf{Impact of different AVLFormer attention masks.}
We ablate the impact of different AVLFormer attention masks in Fig.~\ref{fig:attention_mask}. The empirical results illustrate that the full attention mask (Type \uppercase\expandafter{\romannumeral2}) negatively affects model coverage. It may cause by information leakage when all word tokens are visible to vision and audio tokens when we infer the descriptions in auto-regressive manner. Audio tokens are not visible to vision tokens (Type \uppercase\expandafter{\romannumeral3} - Type \uppercase\expandafter{\romannumeral5}) contributes to performance improvement, compared to the default attention mask (Type \uppercase\expandafter{\romannumeral1}). 



\noindent
\textbf{Extended quantitative results.}
We report 6 additional metrics together with the 8 metrics from the main paper in Table~\ref{tab:more_results}. We extend the Clipscore~\cite{hessel2021clipscore} from the first frame to 32 frames. To obtain the later value, all frames are required to measure the cosine similarity with descriptions, and report the the averaged scores. 
Over the 14 evaluation metrics, AVLFormer leads a substantial performance improvement than PDVC and SwinBERT under various backbone freezing settings. 



\noindent
\textbf{Quantitative results of SwinBERT.}
We evaluate SwinBERT over 7 representative metrics through loading various pretrained weights and fine-tuning on FAVDBench in Table~\ref{tab:swinbert_performance}. Pretrained weights from other captioning dataset contribute to the performance improvement, especially the VATEX. As a comparison, AVLFormer beats all settings of SwinBERT over all evaluation metrics.  




\subsection{Video Generation}
\label{sec:appendix_video_generation}



\noindent
\textbf{Quantitative results.}
To compare the performance between the caption-generated and the description-generated videos, We report the quantitative results over Frechet Video Distance (FVD)~\cite{unterthiner2018towards} metric in Table~\ref{tab:video_generation}. We employ Cogvideo~\cite{hong2022cogvideo} to generate the video based on captions and our fine-grained descriptions. The FVD score reflects that our fine-grained description can generate videos that are more close to the referenced videos. 



\begin{table}[t]\small
\caption{\textbf{Video generation performance on FAVDBench.} FVD~\cite{unterthiner2018towards} is used to measure the generated video from captions and our fine-grained descriptions. As the text-video model is used to directly infer without fine-tuning, the score is higher than normal ranges. Besides, the smaller FVD is the better.} 
\centering
\label{tab:video_generation}
\small
    \begin{tabular}{p{1.8cm}<{\centering}p{2.0cm}<{\centering}p{2.8cm}<{\centering}p{3.0cm}<{\centering}}
    \toprule[0.8pt]\noalign{\smallskip}
    
                                      & Captions  & \multicolumn{2}{c}{Fine-grained Descriptions}   \\  \noalign{\smallskip} \midrule
    FVD (\textbf{\textdownarrow})     & 1,540,010  & \multicolumn{2}{c}{\textbf{1,457,802}}         \\
    \toprule[0.8pt]
    \end{tabular}
\end{table}


\noindent
\textbf{Qualitative results.}
In Fig.~\ref{fig:vid_generation_1} and Fig.~\ref{fig:vid_generation_2}, we provide more video generation examples in our FAVDBench. The videos are generated from captions and our fine-grained descriptions through a state-of-the-art text-video model Cogvideo~\cite{hong2022cogvideo}. FAVDBench can guide video generation more accurately than captions, generating videos that are closer to the reference videos. 



\begin{figure*}[t]
    
    \small
    \centering
    \label{tab:att_mask}
    \small
    \begin{tabular}{p{2cm}<{\centering}p{2cm}<{\centering}p{1.6cm}<{\centering}p{1.6cm}<{\centering}p{1.7cm}<{\centering}p{1.7cm}<{\centering}p{1.6cm}<{\centering}p{1.8cm}<{\centering}p{1.8cm}<{\centering}}
    \toprule[0.8pt]\noalign{\smallskip}
    
    \multicolumn{2}{c}{Attention Types}                               & B@1             & B@4             & METEOR            & ROUGE\_L       & CIDEr          & Clipscore      & EntityScore     \\ 
    \noalign{\smallskip} \cmidrule(r){1-2} \cmidrule(r){3-9}  
    \multicolumn{2}{c}{Type \uppercase\expandafter{\romannumeral1}}   & 44.10           & 10.29           & 18.36             & \textbf{30.93} &\textbf{29.85}  & 69.74          & 42.71           \\
    \multicolumn{2}{c}{Type \uppercase\expandafter{\romannumeral2}}   & 19.72           & 1.97            & 8.94              & 23.03          & 0.89           & 60.25          & 20.06            \\
    \multicolumn{2}{c}{Type \uppercase\expandafter{\romannumeral3}}   & 44.44           & 10.24           & 18.39             & 30.65          & 29.21          & 69.91          & 44.30            \\
    \multicolumn{2}{c}{Type \uppercase\expandafter{\romannumeral4}}   & \textbf{44.45}  & 10.41           & 18.47             & 30.75          & 29.16          & \textbf{70.19} & \textbf{44.58}   \\
    \multicolumn{2}{c}{Type \uppercase\expandafter{\romannumeral5}}   & 44.32           & \textbf{10.50}  & \textbf{18.51}    & 30.86          & 29.07          & 70.06          & 44.41            \\
    \toprule[0.8pt]
    \end{tabular}


  \centering
  \vspace{3mm}
  \includegraphics[width=\textwidth]{figures/sup_figure2.pdf}
  \caption{\textbf{Impact of transformer attention masks.} 5 different types of attention masks in the table are visualized in bottom figure. The masked attention is colored gray. The attention masks of text, vision, and audio are colored brown, blue, and green, respectively. The type \uppercase\expandafter{\romannumeral1} is set as the default of AVLFormer, where all vision and audio tokens are visible to text, and vision tokens and audio tokens are visible to each other. The type \uppercase\expandafter{\romannumeral2} is full attention among three modalities. Apart from the type \uppercase\expandafter{\romannumeral1}, all text tokens are visible to vision and audio tokens. The type \uppercase\expandafter{\romannumeral3} is single-direction attention, where the tokens in preceding order are visible to preceding tokens, and vice versa. In type \uppercase\expandafter{\romannumeral4}, vision tokens are visible to audio, and vice versa. In type \uppercase\expandafter{\romannumeral5}, vision tokens and audio tokens are independent.}
  \label{fig:attention_mask}
\end{figure*}



\begin{table*}[t]\small
\caption{\textbf{More quantitative comparisons with different methods on FAVDBench.} We report different backbone settings for PDVC, SwinBERT and AVLFormer. In the backbone freeze (FRZ.) column, \textbf{-} represents that input is not available; \ding{220} represents that is trainable; \ding{91} represents frozen. As an extension of Table 2 in the main paper, we report Bleu-1, Bleu-2, Bleu-3, Bleu-4, METEOR, ROUGE\_L, CIDEr, Clipscore 1 frame, Reference Clipscore 1 frame, Clipscore 32 frames (in average), Reference Clipscore 32 frames (in average), EntityScore, and AudioScore. Scores of human evaluation are not repeated in this table. For all metrics, higher values are better. }
\centering
\label{tab:more_results}

\begin{tabular}{p{2.4cm}<{\centering}p{.7cm}<{\centering}p{.7cm}<{\centering}p{1.1cm}<{\centering}p{1.1cm}<{\centering}p{1.1cm}<{\centering}p{1.1cm}<{\centering}p{1.5cm}<{\centering}p{1.5cm}<{\centering}p{1.5cm}<{\centering}}
\toprule[0.8pt]
\multirow{2}{*}{Method}  & \multicolumn{2}{c}{Backbone FRZ.} & \multicolumn{7}{c}{Conventional Metric}   \\ \cmidrule(r){2-3} \cmidrule(r){4-10}
                    & Visual          & Audio         & B@1            & B@2            & B@3             & B@4            & METEOR         & ROUGE\_L       & CIDEr          \\ \midrule
PDVC~\cite{wang2021end}                & \ding{220}      & \textbf{-}    & 34.91          & 19.90          & 11.15           & 6.33           & 13.80          & 24.67          & 6.27           \\ 
PDVC w. Audio       & \ding{220}      & \ding{220}    & 35.59          & 20.47          & 11.53           & 6.47           & 14.49          & 26.98          & 13.13          \\ \midrule
SwinBERT~\cite{lin2022swinbert}            & \ding{220}      & \textbf{-}    & 39.68          & 23.54          & 14.26           & 9.05           & 16.78          & 29.60          & 21.69          \\
SwinBERT~\cite{lin2022swinbert}            & \ding{91}       & \textbf{-}    & 41.41          & 24.43          & 14.57           & 9.08           & 17.19          & 29.49          & 23.03          \\ \midrule
AVLFormer           & \ding{220}      & \ding{220}    & \textbf{44.10} & \textbf{26.61} & \textbf{16.21}  & \textbf{10.29} & \textbf{18.36} & \textbf{30.93} & \textbf{29.85}  \\
AVLFormer           & \ding{91}       & \ding{220}    & \textbf{44.10} & 26.39          & 16.02           & 10.23          & 18.25          & 30.54          & 26.31           \\
AVLFormer           & \ding{220}      & \ding{91}     & 42.82          & 25.79          & 15.80           & 10.16          & 17.96          & 30.68          & 25.45            \\
AVLFormer           & \ding{91}       & \ding{91}     & 42.35          & 25.37          & 15.40           & 9.84           & 17.73          & 30.25          & 25.65            \\
\toprule[0.8pt] 
\end{tabular}


\begin{tabular}{p{2.4cm}<{\centering}p{.7cm}<{\centering}p{.7cm}<{\centering}p{1.6cm}<{\centering}p{1.6cm}<{\centering}p{1.6cm}<{\centering}p{1.6cm}<{\centering}p{1.5cm}<{\centering}p{1.5cm}<{\centering}}
\toprule[0.8pt]
\multirow{2}{*}{Method}  & \multicolumn{2}{c}{Backbone FRZ.} & \multicolumn{4}{c}{Clipscore} & \multicolumn{2}{c}{Proposed Metric}   \\ \cmidrule(r){2-3} \cmidrule(r){4-7} \cmidrule(r){8-9}
                    & Visual          & Audio         & Fr@1           & Ref. Fr@1      & Fr@32           & Ref. Fr@32     & EntityScore    & AudioScore              \\ \midrule
PDVC~\cite{wang2021end}                & \ding{220}      & \textbf{-}    & 65.85          & 66.33          & 65.77           & 66.27          & 32.40          & 29.37                   \\ 
PDVC w. Audio       & \ding{220}      & \ding{220}    & 66.35          & 67.16          & 66.15           & 67.04          & 33.09          & 40.33                    \\ \midrule
SwinBERT~\cite{lin2022swinbert}            & \ding{220}      & \textbf{-}    & 68.64          & 70.02          & 68.13           & 69.75          & 38.17          & 34.57                   \\
SwinBERT~\cite{lin2022swinbert}            & \ding{91}       & \textbf{-}    & 68.39          & 70.02          & 67.98           & 69.79          & 39.39          & 35.06                   \\ \midrule
AVLFormer           & \ding{220}      & \ding{220}    & 70.24          & \textbf{72.09} & 69.74           & \textbf{71.82} & \textbf{42.70} & \textbf{52.43}  \\
AVLFormer           & \ding{91}       & \ding{220}    & 69.42.         & 71.30          & 68.98           & 71.04          & 41.69          & 51.18                   \\
AVLFormer           & \ding{220}      & \ding{91}     & \textbf{70.58} & 71.97          & \textbf{70.07}  & 71.69          & 41.36          & 51.72                  \\
AVLFormer           & \ding{91}       & \ding{91}     & 69.82          & 71.33          & 69.33           & 71.05          & 41.39          & 51.01                   \\
\toprule[0.8pt] 
\end{tabular}


\end{table*}





\begin{table*}[t]\small
\vspace{-3mm}
\caption{\textbf{Comparison with different pretrained weights of SwinBERT~\cite{lin2022swinbert} on FAVDBench.} Pretrained weights from 5 captioning datasets are evaluated: MSR-VTT, MSVD, TVC, VATEX, YOUCOOK \uppercase\expandafter{\romannumeral2}. As a comparison, results of SwinBERT and AVLFormer training with default settings are reported. All pretrained weights are provided by the SwinBERT official repository.}
\vspace{-3mm}
\centering
\label{tab:swinbert_performance}
 \begin{tabular}{p{2.4cm}<{\centering}p{0.7cm}<{\centering}p{0.8cm}<{\centering}p{1.0cm}<{\centering}p{1.0cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}p{1.5cm}<{\centering}p{1.5cm}<{\centering}p{1.5cm}<{\centering}}
\toprule[0.8pt]
\multirow{2}{*}{Method} & \multicolumn{2}{c}{\multirow{2}{*}{pretrained Weights}} & \multicolumn{5}{c}{Conventional Metric}  & \multicolumn{2}{c}{Proposed Metric}  \\ \cmidrule(r){4-8} \cmidrule(r){9-10}
                      &             &               & B@1            & B@4            & Meteor          & CIDEr          & Clipscore     & EntityScore     & AudioScore        \\ \midrule
SwinBERT              & \multicolumn{2}{c}{\ding{56}}           & 39.68          & 9.05           & 16.78           & 21.69          & 68.13          & 39.60          & 34.57              \\
SwinBERT & \multicolumn{2}{c}{MSR-VTT~\cite{xu2016msr}}         & 42.59          & 9.66           & 17.84           & 23.16          & \textbf{70.40} & 41.41          & 35.11              \\ 
SwinBERT & \multicolumn{2}{c}{MSVD~\cite{chen2011collecting}}   & 42.16          & \textbf{10.15} & 17.89           & 26.85          & 69.69          & 40.91          & 34.48              \\ 
SwinBERT & \multicolumn{2}{c}{TVC~\cite{lei2020tvr}}            & 42.24          & 9.67           & 17.71           & 24.18          & 70.33          & 41.55          & 35.27              \\ 
SwinBERT & \multicolumn{2}{c}{VATEX~\cite{wang2019vatex}}       & \textbf{43.03} & 10.02          & \textbf{18.01}  & 25.59          & 69.74          & \textbf{41.77} & 35.44              \\ 
SwinBERT & \multicolumn{2}{c}{YOUCOOK \uppercase\expandafter{\romannumeral2}~\cite{ZhXuCoAAAI18}} & 42.67          & 9.05           & 17.96           & \textbf{27.64} & 68.13          & 41.76          & \textbf{35.45}     \\ \midrule
AVLFormer (Ref.)     & \multicolumn{2}{c}{\ding{56}}            & \textbf{44.10} & \textbf{10.29} & \textbf{18.36}  & \textbf{29.85} & \textbf{69.74} & \textbf{44.46} & \textbf{52.43}      \\
\toprule[0.8pt]
\end{tabular}
\end{table*}


\begin{figure*}[t]
  \centering
  \includegraphics[width=0.98\textwidth]{figures/sup_figure3.pdf}
  \vspace{-2mm}
  \caption{\textbf{More qualitative examples of video generation on FAVDBench (1).} The images in each group are sampled from the ground-truth videos and videos produced by Cogvideo through the caption and our fine-grained descriptions, respectively. } 
  \label{fig:vid_generation_1}
\end{figure*}


\begin{figure*}[t]
  \vspace{-50mm}
  \centering
  \includegraphics[width=0.98\textwidth]{figures/sup_figure4.pdf}
  \caption{\textbf{More qualitative examples of video generation on FAVDBench (2).} The images in each group are sampled from the ground-truth videos and videos produced by Cogvideo through the caption and our fine-grained descriptions, respectively. } 
  \label{fig:vid_generation_2}
\end{figure*}