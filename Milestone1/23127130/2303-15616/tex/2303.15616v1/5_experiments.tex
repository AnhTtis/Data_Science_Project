

\begin{figure*}[t]
  \centering
  \vspace{-2mm}
  \includegraphics[width=\textwidth]{figures/figure5.pdf}
  \vspace{-7mm}
  \caption{\textbf{Qualitative result of fine-grained video description on FAVDBench.} We compare the proposed AVLFormer with the PDVC~\cite{wang2021end}, SwinBERT~\cite{lin2022swinbert} and BMT~\cite{iashin2020better}. The generated video summary of each method is underlined. The spatial relation, visual adjectives, and audio-related words are highlighted in blue, orange, and green, respectively.}
  \vspace{-5mm}
  \label{fig:favd_performance}
\end{figure*}


\section{Experimental Results}
\label{sec:experiments}



\vspace{-2mm}
\subsection{Implementation details}\label{subsec:implementation} 
\vspace{-2mm}
We conduct experiments on the proposed FAVDBench. All video clips are sampled uniformly to 32 frames, and all frames are resized to the shape of  $224 \times 224$. All audios are sampled by 32kHz in a mono channel. We set the max sequence length and mask probability to 300 and 0.25, respectively for natural language processing. We use the Adam optimizer with a linear warm-up learning rate. The batch size is set to 8 and the maximum number of training epochs is set to 150.


\noindent
\textbf{Evaluation metrics.}\label{subsec:metrics} 
We include 5 conventional captioning metrics as well as the two newly proposed metrics as our evaluation metrics, namely, BLEU~\cite{papineni2002bleu}, Meteor~\cite{denkowski2014meteor}, Rouge-L~\cite{rouge2004package}, CIDEr~\cite{vedantam2015cider}, Clipscore~\cite{hessel2021clipscore}, EntityScore, and AudioScore. We also perform the human evaluation on the generated descriptions. Among the entire test-set, 10$\%$ of samples are randomly selected to be measured, scoring from 0 to 10, and average values of 20 volunteers are reported. The ground truth descriptions are set to 10.
Evaluators are required to focus on the smoothness of descriptions and the correlation between the descriptions and the inputs. 



\vspace{-2mm}
\subsection{Main results}
\vspace{-2mm}
\label{subsec:performance_on_favdbench} 
We compare our AVLFormer with the state-of-the-art video captioning method SwinBERT~\cite{lin2022swinbert}, video captioning method with original audio input BMT~\cite{iashin2020better}, dense video captioning method PDVC~\cite{wang2021end} on our FAVDBench for the new FAVD task. Both quantitative and qualitative results are provided in the following sections.



\noindent
\textbf{Quantitative comparison.}\label{subsec:quantitative_comparsion} 
We report the quantitative results over the 8 evaluation metrics in Table~\ref{tab:favd_performance}. We compare AVLFormer with PDVC, SwinBERT and BMT under different backbone freezing settings. AVLFormer beats competitors with a clear margin and achieves the best performance when both the visual and audio backbones are unfrozen. Among the 7 automatic evaluation metrics, the EntityScore is trending closer to human evaluation, demonstrating the validity of the evaluation metric.




\noindent
\textbf{Qualitative comparison.}\label{subsec:qualitative_comparsion} 
We illustrate the qualitative results of AVLFormer and other competitors in Fig.~\ref{fig:favd_performance}. AVLFormer provides finer-grained details of entities and their status than other methods, especially for the spatial location descriptions. For example, AVLFormer attempts to capture the relative positions of major objects, such as the spatial relations among the harbor, the reefs and the yacht, and generates descriptions that are close to the reference. For the audio descriptions, AVLFormer describes the sound and the sounding objects whereas PDVC and SwinBERT often neglect the sounding sources.


\vspace{-2mm}
\subsection{Ablations}
\vspace{-2mm}

\begin{table}[t]\small
\caption{\textbf{Impact of video and audio.} Four different inference modes from top to bottom: positive pairs of video and audio, negative pairs of video and audio, random audio, random video.} 
\vspace{-3mm}
\centering
\label{tab:core_component}
\small
 \begin{tabular}{p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{1.3cm}<{\centering}p{1.5cm}<{\centering}p{1.5cm}<{\centering}}
\toprule[0.8pt]\noalign{\smallskip}
\multicolumn{2}{c}{Inference}        & CIDEr  & EntityScore & AudioScore \\ 
\noalign{\smallskip} \cmidrule(r){1-2} \cmidrule(r){3-5}  
\multicolumn{2}{c}{Pos. V.$\&$A.}    & \textbf{29.85}  & \textbf{44.46}      &  \textbf{63.88}     \\
\multicolumn{2}{c}{Neg. V.$\&$A.}    & 18.63  & 8.25       &  31.25     \\
\multicolumn{2}{c}{Video w Random}   & 20.12  & 40.77      &  61.22     \\
\multicolumn{2}{c}{Random w Audio}   & 1.58   & 11.86      &  44.70     \\
\toprule[0.8pt]
\end{tabular}
\vspace{-4mm}
\end{table}

\begin{table}[t]\small
\caption{\textbf{Audio impact on AudioScore.} Four different audio-visual-text pairs are typed from top to bottom: original, audio-shuffled, audio random initialized and audio fixed initialized pairs.} 
\vspace{-3mm}
\centering
\label{tab:novel_metrics}
\small
 \begin{tabular}{p{1.8cm}<{\centering}p{1.8cm}<{\centering}p{2.5cm}<{\centering}p{2.5cm}<{\centering}}
\toprule[0.8pt]\noalign{\smallskip}
\multicolumn{2}{c}{Pair Type}        & Top-1 AudioScore  & Top-2 AudioScore   \\ 
\noalign{\smallskip} \cmidrule(r){1-2} \cmidrule(r){3-4} 

\multicolumn{2}{l}{Type I}    & \textbf{63.88}  & \textbf{64.60}         \\
\multicolumn{2}{l}{Type II}   &   52.59         &  56.31         \\
\multicolumn{2}{l}{Type III}  &   41.93       &  38.06      \\
\multicolumn{2}{l}{Type IV}   &   39.31         &  40.12         \\
\toprule[0.8pt]
\end{tabular}
\vspace{-7mm}
\end{table}

\begin{figure}[t]
  \centering
  \vspace{0mm}
  \includegraphics[width=0.48\textwidth]{figures/figure7_cogvideo.pdf}
  \vspace{-7mm}
  \caption{\textbf{Qualitative examples of video generation on FAVDBench.} We use Cogvideo~\cite{hong2022cogvideo} to generate videos through the caption and our fine-grained descriptions
  We eliminate audio-related input descriptions as current video generation models cannot produce sound. \textit{Caption}: ``A man sings and beats the drums with his hands in the room.''; \textit{Description}: ``In the room, a man in a blue shirt sings and beats drums with both hands. The man is pounding two drums, one large and one small. The two white drums have black dots on them and are wrapped in light brown ropes.''} 
  \vspace{-3mm}
  \label{fig:video_generation}
\end{figure}


We first ablate the impact of audio and visual modalities on AVLFormer by replacing the positive audio-visual pairs with negative pairs or random noises in inference. The results are shown in Table~\ref{tab:core_component}. We then analyze the contribution of the masked language modeling loss and the auto-regressive language modeling loss in Fig.~\ref{fig:lambda_loss}. Finally, we evaluate the validity of our proposed metrics.

\noindent
\textbf{Impact of audio.}\label{subsec:audio_impact}
We empirically find that the trained model can infer partial sound-related descriptions without an audio branch as shown in Fig.~\ref{fig:favd_performance} and Table~\ref{tab:core_component}. It is probably because audio is often related to actions, and the network can predict sound descriptions by understanding the actions. However, in this situation, the sound descriptions are often incorrect or unrelated to the actual audio. For example, the model generates one-third of samples with a pattern like: \emph{somebody/something makes noise}. 



\noindent
\textbf{Impact of visual.}\label{subsec:video_impact}
As shown in Table~\ref{tab:core_component}, replacing visual information with random noises will cause a significant performance drop in all metrics. This indicates that in FAVD, the visual modality is most important. Even with the aid of correct audio information, the model cannot produce related descriptions without visual guidance.


\noindent
\textbf{Analysis of losses.}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.48\textwidth]{figures/figure6.pdf}
  \vspace{-7mm}
  \caption{\textbf{Analysis of losses.} We report model performance from on and EntityScore with respect to different $\lambda$ values of loss.}
  \vspace{-7mm}
  \label{fig:lambda_loss}
\end{figure}

We employ both the masked language modeling loss $\mathcal{L}_{\text{MLM}}$ and the auto-regressive language modeling loss $\mathcal{L}_{\text{ALM}}$ as our objective loss functions. A ratio $\lambda$ is adopted to balance the weights between them (Eq.~\ref{eq:loss}). We train our model with different $\lambda$s under the same setting and plot the test set accuracy in Fig.~\ref{fig:lambda_loss}. As shown, using pure $\mathcal{L}_{\text{MLM}}$ (\ie, $\lambda = 1$) achieves significant better performance than using pure $\mathcal{L}_{\text{ALM}}$ (\ie, $\lambda = 0$), \eg, 27.11 \emph{vs} 22.22 in CIDEr and 44.37 \emph{vs} 42.24 in EntityScore. The model achieves the best performance when $\lambda = 0.9$.



\noindent
\textbf{Analysis of proposed metrics.}
The EntityScore indicates how comprehensively described entities are when compared to reference descriptions. As shown in Table~\ref{tab:favd_performance} and Table~\ref{tab:core_component}, the EntityScore displays a scoring pattern that is fairly similar to human evaluation, which demonstrates the validity of the EntityScore. For the AudioScore, we additionally design an ablation to evaluate its validity. Specifically, we compare four settings of audio-visual-text pairs: Type I: the original audio-visual-text pairs; Type II: shuffle the audios while keeping the visual-text pairs unchanged; Type III: replace the audio with uniformly random initialized noise; Type IV: replace the audio with a fixed 1e-3 value initialization. The results are shown in Table~\ref{tab:novel_metrics}. Type I keeping the semantic-consistent audio and visual-text pairs achieves significantly higher AudioScore than others, proving the validity of the AudioScore.



\vspace{-1mm}
\subsection{Video generation}
\vspace{-2mm}

Existing video generation models can also benefit from our FAVDBench as we provide finer-grained video descriptions than captions. To prove our claim, we compare the generated videos from captions and our fine-grained description using a state-of-the-art text-video model Cogvideo~\cite{hong2022cogvideo}. As shown in Fig.~\ref{fig:video_generation}, the video generated from the descriptions is closer to the reference video than the one generated from the captions. Furthermore, FAVDBench can be used to train video generation models and can substantially reduce the impact of insufficient scene description on video generation tasks as mentioned in~\cite{singer2022make}. 