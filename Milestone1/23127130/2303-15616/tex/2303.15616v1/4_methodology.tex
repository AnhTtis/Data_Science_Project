
\section{The AVLFormer}
\label{sec:method}

\vspace{-1.5mm}
We propose a transformer-based model called Audio-Visual-Language Transformer (AVLFormer) as a baseline method for the FAVD task. AVLFormer is in a form of encoder-decoder structures as shown in Fig.~\ref{fig:model}. A visual encoder and an audio encoder are adapted to process the video clips and audio, respectively. Similar to previous video captioning models~\cite{lin2022swinbert}, AVLFormer also takes textual descriptions as input and uses a word tokenizer and a linear embedding to embed the text. The output of AVLFormer is the fine-grained descriptions of the input videos.

\noindent
\textbf{The encoders.}\label{subsec:econder} 
We employ the video swin transformer~\cite{liu2022video} as our visual encoder as it achieves state-of-the-art performance in video understanding tasks~\cite{liu2022video, lin2022swinbert}. The weights of the visual encoder are pretrained on the ImageNet-22k~\cite{deng2009imagenet} and the Kinetics-600~\cite{carreira2018short} datasets. The visual features $\mathbf F_{v} \in \mathbb{R}^{N_{v} \times d_v}$, where $d_{v}=512$ is the feature dimension and $N_{v}=784$ is the patch sequence length. We adopt the patchout audio transformer (PaSST)~\cite{koutini2021efficient} as our audio encoder. We pretrain it on the ImageNet-1K~\cite{deng2009imagenet} and the AudioSet~\cite{gemmeke2017audio} datasets. The audios are firstly processed by a Fast Fourier Transform and a Mel feature extractor and then sent to PaSST. The audio features $\mathbf F_{a} \in \mathbb{R}^{N_{a} \times d_{a}}$, where $d_{a}=768$ is the dimension and $N_{a}=473$ is the sequence length. For the text input, we apply a word tokenizer and a linear embedding to embed the text as $\mathbf F_{t} \in \mathbb{R}^{N_{t} \times d_{t}}$, where $d_{t}=768$ and $N_{t}=300$.


\noindent
\textbf{Audio-visual-language transformer.}\label{subsec:AVLTransformer} 
Before passing the audio features $\mathbf F_{a}$, visual features $\mathbf F_{v}$, and text embedding $\mathbf F_{t}$ to the audio-visual-language transformer (AVLFormer), a linear projection layer is employed to unify their feature dimensions to $d=768$. AVLFormer takes the concatenated feature $\mathbf F_{avt} \in \mathbb{R}^{(N_t + N_v + N_a) \times d}$ of $\mathbf F_{a}$, $\mathbf F_{v}$, and $\mathbf F_{t}$ as input and processes it with a stack of multi-head self-attention modules. The number of layers of the transformer is set to 12. We illustrate the masking strategy of AVLFormer in Fig.~\ref{fig:model}. Specifically, we use the auto-regressive language model masking strategy to mask the text attention matrix, which ensures that the predicted tokens are only dependent on the previous tokens. For the audio and visual tokens, since there is no ordering restriction for these modalities, we use full attention matrices instead. We also make audio and visual tokens visible to text tokens, ensuring that other modality information can contribute to text generation.



\noindent
\textbf{Loss functions.}\label{subsec:loss} 
Previous video captioning methods~\cite{lei2021less, lin2022swinbert} mainly adopt the masked language modeling loss to make the model concentrate on salient objects and actions, whereas previous vision-language generation methods~\cite{li2022blip} often use auto-regressive language modeling loss for longer text generation. For the FAVD task, since it requires not only fine-grained descriptions for each object but also paragraph-level text generation capability, we employ both the masked language modeling loss $\mathcal{L}_{\mathrm{MLM}}$ and the auto-regressive language modeling loss $\mathcal{L}_{\mathrm{ALM}}$ to train our AVLFormer. In masked language modeling, AVLFormer is required to recover the words that are replaced by [\texttt{MASK}] symbol, as shown in Fig.~\ref{fig:model}. In this way, the model is forced to recuperate the information based on audio-visual information. In auto-regressive language modeling, AVLFormer is asked to understand the semantic context for the entire sentence. Note that AVLFormer works in an auto-regressive manner in inference, the $\mathcal{L}_{\mathrm{ALM}}$ can also mitigate the gap between the training and the inference. The total object function $\mathcal{L}$ can be computed as follows:
\begin{equation}
\small
  \mathcal{L} = \lambda \mathcal{L}_{\mathrm{MLM}} + (1 - \lambda) \mathcal{L}_{\mathrm{ALM}},
\label{eq:loss}
\end{equation}
where $\lambda$ is a scaling factor between $0$ and $1$.

\noindent
\textbf{Inference.} 
AVLFormer takes video frames and audios as the input for visual and audio encoders and the $[\texttt{BOS}]$ token for the word embedding in inference. It then generates descriptions in an auto-regressive manner. Specifically, it predicts the next word iteratively until the $[\texttt{EOS}]$ token or max sequence length is reached.
