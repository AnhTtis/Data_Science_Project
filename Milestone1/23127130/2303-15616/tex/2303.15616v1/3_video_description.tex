
\vspace{-2mm}
\section{The FAVDBench}
\vspace{-1mm}
\label{sec:dataset}




\subsection{Task description}
\vspace{-1mm}
The FAVD task involves generating fine-grained textual descriptions for the given audible videos that include information about appearance and spatial location of each object, the movements of moving objects, and the sounds in the videos. 
Specifically, given an audible video clip, this task aims to generate a paragraph with 6-9 sentences. The first sentence needs to be an overview of the video clip that emphasizes the key visual elements as well as the main event. The next 4-6 sentences should then be used to describe the minute visual details of each object, \ie, the rich semantics that each one possesses and the function that it serves in the video. The final 1-2 sentences characterize the audio information contained in the video clip, describing what is producing the sound or what it sounds like. We compare the annotation differences between the FAVD and previous video captioning tasks in Fig.~\ref{fig:dataset_intro}. 


\vspace{-1mm}
\subsection{Dataset statistics}\label{subsec:statistics}
\vspace{-1mm}
We construct a benchmark called FAVDBench to facilitate the FAVD task. We adopt the technique described in VGGSound~\cite{arandjelovic2018objects} to make sure the audio and visual clips match the intended semantics. All videos are collected from YouTube under \emph{Creative Commons} license. We take into account the number of sounding objects when collecting video clips as we find the difficulty of audio description is positively correlated with the number of sounding objects. As a result, 60.2\% of the video clips have one sounding object and the rest have 2-3 sounding objects.

The FAVDBench consists of 11,424 audible video clips and 143,548 sentences of annotations. Each video clip has a description of 12.6 sentences, 218.9 word tokens on average. Among the clips, 5,289 videos are trimmed to 5 seconds and 6,135 videos to 10 seconds. The total video duration is 24.4 hours and 2.6 million frames. To avoid data leakage and maintain diversity, we enforce a minimum interval of 5 seconds between adjacent videos. We also manually filter clips with similar content. The benchmark is divided into 4 partitions: 7,500 for training, 1,500 for validation, 1,000 for testing, and 1,424 withhold for online benchmarking. The withhold partition will only be visible for contestants in the future FAVD Benchmark competition. 

We compare FAVDBench with other video captioning datasets in Table~\ref{tab:stat_comparsion}. In terms of context differences, we include 5 major categories for video content, \ie, vehicles, instruments, animals, people, and common human activities. We further divide the 5 major categories into 71 sub-classes and the distribution of each category/subclass is plotted in Fig.~\ref{fig:dataset_stat}. By contrast, the contexts of existing captioning datasets TVC~\cite{lei2020tvr} and YouCook\uppercase\expandafter{\romannumeral2}~\cite{ZhXuCoAAAI18} are restricted to movie and cooking scenarios while the MSVD~\cite{chen2011collecting}, MSR-VTT~\cite{xu2016msr}, and VATEX~\cite{wang2019vatex} merely include human activities. Besides, we also provide audio descriptions for each video clip in FAVDBench, which distinguishes us from other datasets. Furthermore, since our benchmark provides fine-grained descriptions for video objects, the percentage of adjectives in our dataset is more than 3 times higher than in other datasets. 


\subsection{Annotation details}\label{subsec:annotation}
\begin{figure}[t]
  \centering
  \vspace{0mm}
  \includegraphics[width=0.48\textwidth]{figures/fig3_samples.pdf}
  \vspace{-7mm}
  \caption{\textbf{An example of FAVDBench annotation.} For each video clip, we provide both Chinese and English annotations, each contains 1 summary \textbf{[S.]}, 4-6 detailed visual descriptions \textbf{[V.]} and 1-2 audio-related descriptions \textbf{[A.]}. Details of spatial locations, appearance, and sounds are highlighted in blue, orange, and green.}
  \vspace{-6mm}
  \label{fig:annotation}
\end{figure}

\begin{figure*}[ht]
  \centering
  \vspace{-3mm}
  \includegraphics[width=0.88\textwidth]{figures/fig4_model.pdf}
  \vspace{-4mm}
    \caption{\textbf{Overview of AVLFormer.} It consists of a word embedding, a visual encoder, an audio encoder, and a transformer decoder. We adopt the video swin transformer and patchout audio transformer as the visual encoder and audio encoder, respectively. They extract visual and audio features from video frames and audio. Masked language modeling and auto-regressive language modeling are configured in training. The attention mask strategy of AVLFormer is illustrated on the right, where the masked attention is colored in gray. The tokens and attention masks of text, vision, and audio are colored brown, blue, and green, respectively.}
  \vspace{-5mm}
  \label{fig:model}
\end{figure*}


FAVDBench provides both English and Chinese annotations. All video clips are primarily annotated in Chinese using crowdsourcing and then translated to English with human translation. To collect large-scale fine-grained descriptions of videos, we design several rules of annotation. Specifically, each video is required to be watched completely, and then described by 1 summary sentence, at least 4 visual-related points, and at least 1 audio-related point. The summary only covers high-level information about the most salient event that the video tries to convey. The order to describe the visual and audio details follows from the most salient objects to the background. The statuses of objects are required to be thoroughly captured, including the actions, characteristics (adjective), and relative spatial relations (preposition). Furthermore, each sentence is required to be larger than 5 Chinese characters. Any subjective thoughts and speculative words (e.g., ``maybe'') are forbidden to appear in labels, especially in the audio description. In addition, information beyond the video and audio is not allowed to include in the annotations, such as the wiki of objects, subtitles, and speech contents. 
A video sample with bilingual annotations is shown in Fig.~\ref{fig:annotation}.

To reduce the annotation bias and ensure the quality of annotation, the annotators are restricted to around 60 people, and all of them are native speakers of Chinese. Each video is annotated by one worker, checked by 2 peer workers, and examined by grammar-checking tools. The English and the Chinese version of descriptions are congruent to prevent any changes to the semantics of sentences.


\input{table2}


\subsection{Evaluation metrics}\label{subsec:eval_metrics}
The generated descriptions should be evaluated in terms of semantics rather than word-for-word precision. For example, we would like to know whether the entities in the videos are addressed and the sounding objects are properly described, rather than whether they are using the same way to describe these things. Existing video caption metrics, such as BLEU~\cite{papineni-etal-2002-bleu}, ROUGE~\cite{10.3115/1073445.1073465}, Meteor~\cite{denkowski-lavie-2014-meteor}, and CIDEr ~\cite{7299087} often concentrate on word-for-word precision by measuring the token similarity between the generated and ground truth texts, which does not meet our needs.
Therefore, to properly evaluate the generated descriptions, we design two new metrics called EntityScore $\mathbb{ES}$ and AudioScore $\mathbb{AS}$.


\emph{EntityScore} measures the extent to which consistently referred words or series of words, known as entities and often manifested as nouns, in the predicted text match those in the annotated text. We use an off-the-shelf Natural Language Toolkit library to extract nouns as entities. The mathematical expression of the EntityScore $\mathbb{ES}$ is as follows:
\begin{equation}
\small
     \begin{array}{lll}
     \vspace{1ex}
      R(\mathbf p,\mathbf r)=\frac{\#\{ \mathbf p \cap \mathbf r \}}{\#\{\mathbf r\}}, 
     C(\mathbf p,\mathbf r)=\frac{\cos(\mathrm{T5}(\mathbf p), \mathrm{T5}(\mathbf r))+1} 2, \\
     \mathbb{ES}(\mathbf p,\mathbf r)= \frac{2 R(\mathbf p,\mathbf r) C(\mathbf p, \mathbf r)}{R(\mathbf p,\mathbf r) + C(\mathbf p, \mathbf r)}, 
     \end{array}
\end{equation} 
where $\mathbf{p}\in \mathbb R^{n}$ is the predicted entities from the model predictions, and $\mathbf{r}\in \mathbb R^{m}$ is the ground truth entities, $n$ and $m$ are the entity numbers. 

$\text{\#}\{ \mathbf r \}$ means the number of entities in the reference, and $\text{\#}\{ \mathbf p \cap \mathbf r \}$ counts the number of correctly described entities in the prediction. ${\cos}$ denotes the cosine similarity between two vectors and T5 represents the pretrained model T5~\cite{2020t5} used to extract entity features. Besides, it is normalized into the range [0,1] to guarantee its positive. Therefore, ${R(\mathbf p,\mathbf r)}$ indicates the recall of the predicted entity that is described using the same word as the reference and ${C(\mathbf p,\mathbf r)}$ measures the comprehensiveness of semantics between the predicted entities and the ground truth entities. In other words, $R$ focuses on syntactic level accuracy on entities while $C$ concentrates on semantic level entity similarities. The $\mathbb{ES}$ considers both syntactic and semantic level accuracy. However, $\mathbb{ES}$ has a limitation in that it places emphasis solely on the presence of entities while disregarding their placement and frequency.


\emph{AudioScore} assesses the accuracy of audio descriptions by computing the product of the extracted audio-visual-text unit features,
where CLIP~\cite{radford2021learning} is used to extract features for video frames and the corresponding descriptions and PaSST~\cite{koutini2021efficient} is used for audio waves. Moreover, we fine-tune these models using contrastive learning on FAVDBench to ensure that the latent features are mapped to a shared space. Specifically, the AudioScore $\mathbb{AS}$ is defined as:
\begin{equation} 
\small
    \begin{gathered}
      \mathbf e_a=\mathrm{PaSST}(\mathbf A),\mathbf e_v=\mathrm{CLIP}(\mathbf V),  \mathbf e_t = \mathrm{CLIP} (\mathbf T), \\
    s = \left(\frac{1}{2} \cos(\mathbf e_a, \mathbf e_t)  + \frac{1}{2} \cos(\mathbf e_a, \mathbf e_v) + 1 \right) \times 0.5,\\
    \mathbb{AS}(\mathbf A,\mathbf V,\mathbf T) = \mathbf f(s),
    \mathbf f(x) = a \exp (-b\exp (-c x)), 
    \end{gathered}
\label{eq:audio_score} 
\end{equation}
wher $\mathbf{A}$ and $\mathbf{V}$ denote the audio and visual frames of one video, respectively, while $\mathbf{T}$ is the predicted audio description. $\mathbf{cos}$ and  $\mathbf{f}$ denote cosine similarity and a specific form of Gompertz function, respectively. We set c=10 empirically, and choose values for a and b ($a=\frac{1}{e^{-0.69e^{-10}}}$, b=0.693) to force specific values of x and f(x) (x=1, f(x)=1 for $a=\frac{1}{e^{-0.69e^{-10}}}$, and x=0, f(x)=0.5 for b=0.693).
We select the last one or two sentences from the whole generated paragraph description as $\mathbf{T}$, as we find that the model prefers describing audio information at the end of the description, mimicking the ground truth. 
Therefore, we report the Top-1 and Top-2 accuracy as the $\mathbb{AS}$ score for the last one or two sentences. 
We use the Top-1 score by default.


