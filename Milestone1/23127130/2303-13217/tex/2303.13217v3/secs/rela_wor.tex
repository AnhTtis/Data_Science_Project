\section{Related Work}

\textbf{In-context Learning}\quad
 Previous research, as cited in \cite{gpt32020brown,gpt22018}, has demonstrated that Large Language Models can complete tasks with zero- or few-shot learning using in-context learning. LLMs perform well with an appropriate prompt. However, recent works~\cite{order2021lu,calibrate2021zhao} have shown that the performance of LLMs  is affected by the prompt used. Therefore, determining the optimal prompt is a crucial and fundamental research area.

\begin{figure}[t]
    \centering
    \subfloat[Selection]{
    \centering
    \includegraphics[width=0.24\linewidth]{figs/originSelection.pdf}
    }  
    \subfloat[Selection (cal)]{
    \centering
    \includegraphics[width=0.24\linewidth]{figs/calSelection.pdf}
    } 
    \subfloat[Permutation]{
    \centering
    \includegraphics[width=0.24\linewidth]{figs/originPermutation.pdf}
    } 
    \subfloat[Permutation (cal)]{
    \centering    \includegraphics[width=0.24\linewidth]{figs/calPermutation.pdf}}
     
    \caption{ICL suffers from high instability due to high variations in demonstrations selection and order, even when  post calibration is performed.}
    \label{fig:obser-var}
\end{figure}


% \textbf{Prompt tuning}\quad
% The most intuitive approach for determining the optimal prompt is to tune the prompts or hidden layers of the LLMs. Many researches supposes such as \cite{hambardzumyan2021warp, qin2021learning, liu2021gpt} that the prompt is not discrete, and optimize it in continuous space using gradient descent. One notable method, proposed by \cite{li2021prefix} named as Prefix-Tuning, which optimizes a small continuous task-specific vector as virtual tokens. This approach has been further improved upon by applying continuous prompts for every layers of the pre-trained model~\cite{liu2021p}, which can be viewed as an adaptation of deep Prompt Tuning. Additionally, \cite{tuning2021lester} propose Parameter-Efficient Prompt Tuning, which adds parametric task prompts and fits them on the development set via backpropagation. While these methods have performed well in hypothetical research scenarios, they may not be directly applicable to current research situations~\cite{sun2022black}, as they are relatively complex or require training examples of sufficient scale to ensure optimized performance. Furthermore, these methods often assume that the input prompt is continuous or that the embedding and gradient of LLMs can be obtained. However, this may not be feasible, as the cost of training large models continues to increase. For business value and model security, gradient may not be provided in the future.

\textbf{Original space searching}\quad
A more intuitive approach for determining the best prompt is to search in the original space by selecting or reordering the prompt sentences entered by users. The searching can be concluded in two perspective. $\bullet$~\textbf{Global view}: A naive strategy is to \view{enumerate} all candidates to find the prompt that can achieve the best performance on validation set, but this strategy is computationally expensive since its complexity is $\sum_{k=1}^nC_n^kk!$. Zhang et al.~\cite{zhang2022automatic} find that errors frequently fall into the same cluster, where each cluster contains similar questions, so they proposed a \view{diversity-guided} searching strategy to select diverse demonstrations. In addition to demonstrations selection, \cite{order2021lu} have identified the impact of the prompt \view{order} on the results. They found the best sequence which yields the most diverse prediction results on the probing set by generating a probing set through LLMs. However, this method is also computationally expensive, and it may be difficult to ensure that the generated probing set is sufficiently balanced. $\bullet$~\textbf{Local view}: Previous studies~\cite{gentile2022fast} show that reducing the model’s \view{uncertainty} helps improve the model’s performance, and \cite{diao2023active} propose Active Prompting to select demonstrations according to the uncertainty of LLMs. KATE~\cite{liu2021makes} selects the prompt based on the \view{distance} amongst embeddings, with the goal of selecting the closest example. However, this method ignores the influence of the order of the examples and requires access to sentence embeddings.
% On the other hand, \cite{rubin2021learning} learns a prompt encoder through comparative learning to select appropriate training examples. This method requires a sufficient search space of training data, as described in the original paper.
\cite{shi2023large} demonstrate that LLMs can be easily distracted by irrelevant context, accordingly they identify several approaches for \view{filtering} out irrelevant information in context.

In the realm of original space searching, most of the current methods tend to focus solely on the influence of a singular factor (highlighted above) on performance, utilizing heuristic metrics to select context demonstrations that perform well according to this criterion. While these investigations certainly bring benefits to the community, they lack a comprehensive consideration of both local and global perspectives. The method proposed in this paper offers a metric to select context demonstrations from the perspective of predictive bias, which naturally facilitates a transition from the local view to global view.

% \textbf{Post-hoc calibration}\quad
% Post-processing is the most simple but effective method for eliminating prediction bias in the model. Context-free calibration proposed by \cite{calibrate2021zhao} adjusts the prediction category probability by removing the bias in the model's prediction results with context-free input. Our research in this paper is mainly inspired by this work. However, unlike the post-hoc method proposed in context-free calibration, our focus is on searching for a suitable prompt. Furthermore, the two studies are orthogonal. In our experiments, we found that a carefully selected prompt can lead to better calibration performance.


% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.97\linewidth]{figs/cover.pdf}
%     \caption{Illustration of Most-fair-prompt. We can find that Accuracy is highly consistency with Fairness (the number in right figure is the mean accuracy on 5 different seeds).}
%     \label{fig:cover}
% \end{figure}


