\appendix
\section{Appendix}
\subsection{Pretrained Large Language Models}
Neural autoregressive language model (LMs) are designed for next token prediction to predict the probability distribution over the next token after a sequence of tokens input, and pre-trained LMs show their superior performance since they are trained on various programming languages and a large-scale curated dataset. Training large natural LMs are very expansive and time-consuming process since they always have billions of parameters, which limits the development of LMs. Fortunately, many pre-trained LMs are open access or limited access, which promotes researchers to pool their time and makes the resources to collectively achieve a higher impact. EleutherAI makes the GPT-J~\cite{gpt-j} and GPT-Neox~\cite{gptnexo2022} public available on Hugging Face. GPT-3~\cite{gpt32020brown} is limited access in OpenAI which can be used by researchers for a fee, and another large open-science open-access multilingual language model named Bloom~\cite{bloom2022} is provided by BigScience.

% \textbf{In-context Learning} Previous researches~\cite{gpt22018,gpt32020brown} show that Large neural LMs can perform tasks in zero- or few-shot manner using in-context learning, and \cite{rethinking2022min} provides a new way of understanding how and why in-context learning works, and shows the model counter-intuitively does not rely on the ground truth input-label mapping provided in the demonstrations. To address the bias issue in in-context learning, \cite{calibrate2021zhao} introduce a calibration strategy by the scaling probability distribution to make the content-free input has uniform scores for each answer. The prompt training examples are used to teach the LMs what task is to be solved, and many researches focus on tuning the prompts for a best performance. \cite{order2021lu} proposes a strategy by generating a probing set to overcome prompt order sensitivity, but generating a probing set is very costly. \cite{tuning2021lester} introduces prompt tuning to find optimal task prompts by maximizing the likelihood of label via backpropagation. However, the method is white-box optimization, so it may impact the security of the LMs. \cite{black2022sun} proposes a block-box tuning method as the gradients of pre-trained LMs are usually unavailable, but it is only designed the continues prompts, while natural language is the most prompt for current famous LMs service, for example, ChatGPT~\cite{chatgpt}. 

\subsection{Open Access Models}
\label{sec:app-models}
\input{tabs/models}


% \begin{algorithm}[tb]
%     \caption{Pseudo code for G-fair-Prompting}
%     \label{alg:app-greedy}
%     \begin{algorithmic}[1] %[1] enables line numbers
%         \STATE \textbf{Given:} development set $S=\{(x_i, y_i)\}^n$, pretrained LLM $M$, transformation template $\Gamma(\cdot)$, and context-free demonstration $\eta$
%         \STATE Initialize $most\_fair \leftarrow 0$; $improve \leftarrow true$; $\rho \leftarrow null$
%         \WHILE{$S$ \emph{is not null} and $improve$ \emph{is true}}
%         \STATE $most\_fair\_step \leftarrow 0$; $\rho_\text{step} \leftarrow null$; $tail \leftarrow null$
%         \FOR{$(x_i, y_i)$ in $S$}
%         \STATE $\rho_\text{tmp} \leftarrow  \Gamma(x_i,y_i)\oplus\rho$ \Comment{Insert the demonstration at the head}
%         \STATE Inference $\hat{P} \leftarrow \{\hat{p}(y|\rho_\text{tmp}\oplus\eta)|y \in \mathcal{Y}\}$ via $M$
%         \STATE Calculate the $fairness(\rho_\text{tmp})$ according to Eq.~\ref{eq:fair}
%         \IF{$fairness(\rho_\text{tmp})>most\_fair\_step$}
%         \STATE $most\_fair\_step \leftarrow fairness(\rho_\text{tmp})$
%         \STATE $\rho_\text{step} \leftarrow \rho_\text{tmp}$; $tail \leftarrow i$
%         \ENDIF
%         \ENDFOR
%         \IF{$most\_fair\_step>most\_fair$}
%         \STATE $most\_fair \leftarrow most\_fair\_step$;~
%          $\rho \leftarrow \rho_\text{step}$ \Comment{Update demonstrations}
%          \STATE Remove $(x_{tail},y_{tail})$ from $S$ \Comment{Every demonstrations occur once at most}
%         \ELSE \STATE $improve \leftarrow false$ \Comment{Stop searching when fairness can't be improved}
%         \ENDIF
%         \ENDWHILE
%         \RETURN $\rho$
 
%     \end{algorithmic}
% \end{algorithm}

\subsection{Additional Figures on Different Settings}\label{sec:app-addfigs}
In additional to the Fig.~\ref{fig:allcandidates}, we shows the performance on different models for enumerating all candidates, note that the shadow indicates the half value of standard deviation for clear presentation since the variance is very high for LLMs.

\begin{figure}[ht]
    \centering
    
    \subfloat[AGNews (GPT2-XL 1.5B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedyagnews.pdf}
    } 
    \subfloat[TREC (GPT2-XL 1.5B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedytrec.pdf}
    } 
    \subfloat[RTE (GPT2-XL 1.5B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedysst.pdf}
    }  \\
    \subfloat[AGNews (LLaMA 33B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedyagnewsllama33B.pdf}
    }
    \subfloat[TREC (LLaMA 33B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedytrecllama33B.pdf}
    }
    \subfloat[SST-2 (LLaMA 33B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedysstllama33B.pdf}
    } 
    \caption{Accuracy is highly consistency with
fairness and greedy search can find a good prompt, where "Random" and "Oracle" indicates the average accuracy of all prompts and the upper-bound performance according to fairness.}
\label{fig:app-allcandidates}
\end{figure}

\subsection{Accuracy Varies with demonstrations}\label{sec:app-ob}

\begin{figure}[t]
    \centering
    \subfloat[Varying amount of examples]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/Quantity-var.pdf}
    \label{fig:sub-quantity}
    }  
    \subfloat[Permutation]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/Permutation-var.pdf}
    \label{fig:sub-order}
    } 
    \subfloat[Select different examples]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/Selection-var.pdf}
    \label{fig:sub-select}
    } 
    \caption{ICL suffers from high instability due to variations in example amount, example order, and example selection.}
    \label{fig:app-obser-var}
\end{figure}
\textbf{Accuracy Varies with Example Amount}\quad
Demonstrations play an important role in imparting task-related information to language models through in-context learning. Then, the question arises - does a larger number of demonstrations necessarily equate to better performance? To answer this question, we evaluated performance in terms of accuracy by gradually increasing the number of demonstrations. We set $\rho=\Gamma(x_1,y_1)\oplus\cdots\oplus\Gamma(x_k,y_k)$, where $k =1,\cdots, n$, and demonstrations are erased with $k$ decreasing from $n$ to $1$. Intuitively, accuracy would vary highly across different numbers of demonstrations, and the phenomenon is observed in Fig.~\ref{fig:sub-quantity}. To our surprise, however, erasing some demonstrations can result in a better performance. Removing some demonstrations can perform better and sometimes GPT-3 achieves best accuracy when there is only a few demonstrations remaining. This highlights the importance of considering the appropriate number of demonstrations.

\textbf{Example Order}\quad
The performance of a model is sensitive to the order of the demonstrations, as has been discussed in \cite{order2021lu}. Even when the demonstrations are the same, different permutations of the demonstrations can result in vastly different outcomes. As there are $n!$ possible permutations, we introducing a strategy of permuting the demonstrations by circularly shifting the index of the demonstrations. The demonstration can be represented as $\rho=\Gamma(x_{k+1},y_{k+1})\oplus\cdots\oplus\Gamma(x_n,y_n)\oplus\Gamma(x_1,y_1)\oplus\cdots\oplus\Gamma(x_k,y_k)$.As shown in Fig.~\ref{fig:sub-order}, the accuracy varies highly with permutation which consistent with the observations in \cite{order2021lu}.





\textbf{Example Selection}\quad
In this paper, we find which demonstrations are selected is influence the model extremely. This scenario can be described as selecting $k$ demonstrations in $n$ training samples. In Fig.~\ref{fig:sub-select}, we only select one example for demonstration to ablate the impact of demonstrations order, and the accuracy also varies highly with different example selected. In this work, we only detail evaluate the proposed probing method on the erasing demonstrations and permutation, although our method improves by $20\%$ in the setting of example selection on SST-2 (GPT2-XL), because selecting $k$ demonstrations on a set with $n$ training samples can't be regarded as $k-$shot learning in the strict sense.

\input{tabs/app-accuracy}



\subsection{Relationship between with- and without-calibration}
\begin{figure}[ht]
\centering
  \includegraphics[width=0.75\linewidth]{figs/pearson_llama65Btrec4.pdf}
 \caption{Illustration of accuracy relationship between with- and without calibration when $Pearson$ is positive.}
 \label{fig:app-with-without}
\end{figure}
$\bullet$ \textbf{\greedy without post-calibration outperforms random demonstrations after post-calibration.}
Based on Table~\ref{tab:four-topk-greedy}, it is apparent that \greedy outperforms random selection prior to post-calibration. This leads to a natural question: do prompts with better performance before calibration also indicate better performance after calibration proposed by Zhao et al.~\cite{calibrate2021zhao}? To investigate the relationship between performance with- and without-calibration, we calculated the Pearson correlation coefficient between the accuracy with- and without-calibration $Pearson(acc_{w/o},acc_{with})$. A positive coefficient value suggests that a prompt with high accuracy before calibration has a higher likelihood of achieving higher accuracy after calibration than other prompts. We take the topic classification task on LLaMA(65B) for illustration to show the relationship between with- and without calibration when $Pearson$ is positive in Fig.\ref{fig:app-with-without}. Table~\ref{tab:app-pearson} presents the Pearson correlation coefficient on accuracy of permutation and \greedy after calibration. The majority of Pearson correlation coefficients were found to be positive, indicating that prompts with better performance before calibration have more potential to perform well after calibration. Furthermore, our results on the LLaMA family reveal that the larger the model, the stronger the correlation between performance with- and without-calibration. For instance, the value of the Pearson correlation coefficient increases from $0$ to $0.7$ as the model size increases.

\begin{theorem}
\label{theorem:cal}
Suppose the performance of the model under certain prompts with- and without-calibration is positively correlated, i.e., $Pearson(acc_{w/o},acc_{with})>0$, if we can assure $\mathbb{E}(acc^{Selected}_{w/o})>\mathbb{E}(acc^{Random}_{w/o})$, then we have $\mathbb{E}(acc^{Selected}_{with})>\mathbb{E}(acc^{Random}_{with})$.
\end{theorem}
\input{tabs/app-pearson}

As analysed in Theorem~\ref{theorem:cal}, if we can find a prompt with high accuracy before calibration, we have a higher likelihood of achieving higher accuracy after calibration than random selection. Our approach consistently identifies an appropriate prompt, as evidenced by the results in Table~\ref{tab:four-topk-greedy}. Moreover, the performance of the model exhibits a positive correlation with and without calibration under certain prompts, as illustrated in Table~\ref{tab:app-pearson}. Therefore, our method is more likely to enhance calibration performance.


\subsection{Complexity of different strategies}

\begin{figure}[ht]
\centering
  \includegraphics[width=0.65\linewidth]{figs/cost.pdf}
 \caption{Computational cost. T-fair and G-fair indicate \topk and \greedy respectively, and "w/c" indicates the worst case.}
 \label{fig:app-cost}
\end{figure}

\subsection{Performance on Zero-shot and SOTA Classifiers}

