\section{Revisiting the Sensitivity across Demonstrations}



% As previous research has indicated, the level of accuracy greatly varies depending on the prompt order~\cite{order2021lu}, recency and the use of comment tokens~\cite{calibrate2021zhao}. However, we have discovered that current strategies may not be effective and require further refinement.

In this section, we will clarify the notations and the templates used in this paper. Then, we will demonstrate some brief empirical results to show how different demonstration construction factors (e.g., example selection and order) affect performance. We further introduce the definition of predictive bias/fairness of a given prompt and show its connection to the predictive performance on different downstream tasks.

\subsection{Notations}
We consider a training set consisting of $N$ samples $S=\{(x_i, y_i)\}_i^N$, where $x_i$ is the sentence and $y_i \in \mathcal{Y}$ is the label of the $i^{th}$ training sample, and $\mathcal{Y}$ is the space of all labels for the task. We use a template $\Gamma(\cdot)$ to transform these sentences and labels into natural language space (i.e., prompt construction). Take an instance from the AGNews dataset~\cite{wang2018glue} for example, we have $x_i=\textit{"Cubans Risking Life for Lure of America."},~y_i=\textit{"World"}$, and $\Gamma(x_i,y_i)$ is $\textit{"Article: Cubans Risking Life for Lure of America. Answer: World"}$. We concatenate these demonstrations to form a prompt $\rho$, which by default is $\rho=\Gamma(x_1,y_1)\oplus\cdots\oplus\Gamma(x_n,y_n)$. At test time, we append the prompt $\rho$ with $\tau=\textit{"Article: <test sentence>. Answer: "}$ and feed it to a large language model $\mathcal{M}$. The predicted class is given by: 
\begin{equation} \hat{y}=\arg\max_{y \in \mathcal{Y}}\hat{p}(y|\rho \oplus \tau),\quad \hat{p}(y|\rho \oplus \tau)=\frac{\mathcal{M}(y|\rho \oplus \tau)}{\sum_{y \in \mathcal{Y}}\mathcal{M}(y|\rho \oplus \tau)},\end{equation} 
where $\mathcal{M}(y|\rho \oplus \tau)$ indicates the probability predicted by LLM, and the probability is normalized to fit the task. We denote the predictive distribution by $\hat{P}(x):=\{\hat{p}(y|\rho \oplus \tau)|y \in \mathcal{Y}\}$. In this paper, we focus on evaluating the instability caused by demonstrations, and we fix the prompt template following prior work~\cite{calibrate2021zhao}.


\begin{figure}[t]
    \centering
    \subfloat[AGNews (BLOOM 176B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedyagnewsbloom.pdf}
    }  
    \subfloat[AGNews (LLaMA 13B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedyagnewsllama13B.pdf}
    } 
    \subfloat[AGNews (LLaMA 65B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedyagnewsllama65B.pdf}
    } \\ 
    \subfloat[TREC (BLOOM 176B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedytrecbloom.pdf}
    } 
    \subfloat[TREC (LLaMA 13B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedytrecllama13B.pdf}
    }
    \subfloat[TREC (LLaMA 65B)]{
    \centering
    \includegraphics[width=0.31\linewidth]{figs/greedytrecllama65B.pdf}
    }
    \caption{Accuracy is highly consistency with fairness and greedy search can find a good prompt, where "Random" and "Oracle" indicate the average accuracy of all prompts and the upper-bound performance according to fairness.}
\label{fig:allcandidates}
\end{figure}

\subsection{Stability of Few-shot Prompting}
As demonstrated by prior research, the few-shot prompting technique is highly susceptible to a variety of factors, including the selection and order of demonstrations \cite{order2021lu,calibrate2021zhao}. In this study, we delve deeper into the stability of few-shot prompting, specifically focusing on the recently released LLaMA family by Meta \cite{touvron2023llama}. Additionally, we evaluate the stability of LLaMA models calibrated using the current state-of-the-art method \cite{zhang2022automatic,liu2021makes}.

To elucidate the impact of demonstration selection, we select four demonstrations for each different seed and randomly sample an order for each combination. Subsequently, we present the performance on AGNews in the form of a boxplot, which displays the data distribution based on a five-number summary (minimum, first quartile [Q1], median, third quartile [Q3], and maximum). As depicted in Fig.\ref{fig:obser-var}(a)(b), the accuracy demonstrates significant variability across various demonstrations.

To investigate the influence of permutations, we examine all possible permutations of four fixed demonstrations, resulting in $4!$ distinct candidates. Fig.\ref{fig:obser-var}(c)(d) also reveals a high degree of variance. While post-calibration contributes to mitigating instability, it is essential to note that the model remains sensitive even after post-calibration. This finding underscores the importance of meticulous demonstration selection. In subsequent experiments, we discover that our approach can be employed to further enhance the performance of the calibrated model.
%As noted in previous works, the model's sensitivity to demonstration construction includes the selection of demonstrations to prompt the model and the order in which they are presented. In this section, we aim to estimate the stability of the LLaMA family and assess its stability after post-calibration~\cite{calibrate2021zhao}. To illustrate the influence of demonstration selection, we select four demonstrations for each different seed and sample a random order for each combination. We then display the performance in the form of a boxplot, which shows the distribution of data based on a five-number summary (minimum, first quartile [Q1], median, third quartile [Q3], and maximum). As demonstrated in Fig.\ref{fig:obser-var}(a)(b), the accuracy exhibits a high degree of variability across different demonstrations. To estimate the influence of permutation, we examine all permutations of four demonstrations for seed 0, resulting in $4!$ different candidates. High variance is also evident in Fig.\ref{fig:obser-var}(c)(d). 

\subsection{Predictive Bias of ICL}\label{sec:pre-bias}
As demonstrated in the preceding discussion, the performance of ICL is significantly impacted by various factors such as demonstration, permutation, and selection (refer to Appendix \ref{sec:app-ob} for additional information). Consequently, devising an efficient method for constructing an appropriate prompt with near-optimal performance is a crucial step in deploying LLMs for diverse downstream tasks. As outlined in the introduction, numerous studies aim to optimize prompts in ICL. This paper further investigates this issue through the lens of predictive bias, which refers to the discrepancy between targeted classes. \footnote{This notion differs slightly from the concept of social bias, which concentrates on specific feature attributes rather than labels. Our approach can be naturally extended to mitigate social bias in various settings.}

To achieve this, we initially introduce an efficient technique to assess the inherent predictive bias of a given prompt, drawing inspiration from previous work \cite{calibrate2021zhao}. We construct a training set-independent metric to measure predictive bias as follows: first, we merge the provided prompt with "semantic-free" test sample information (e.g., "[N/A]", denoted by $\eta$) and obtain the LLM's predictive distribution for this sample. Ideally, the predictive distribution should closely resemble a uniform distribution, as the test sample lacks semantic information. In this paper, we employ entropy as a measure of predictive bias, defined as:
\begin{equation}
    \fair(\rho) = -\sum_{y \in \mathcal{Y}} p(y|\rho \oplus \eta) \log p(y|\rho \oplus \eta)
    \label{eq:fair}
\end{equation}

Previous studies have utilized this metric to calibrate the model's output. In this paper, we conduct a comprehensive examination of the relationship between predictive bias and overall performance. Specifically, in a scenario with four training samples (due to the time-consuming nature of enumerating all prompt cases for a larger number), we enumerate all possible combinations and permutations of demonstrations for various datasets and LLMs. Subsequently, we arrange all candidates in descending order based on fairness, where an "index 0" denotes the prompt with the highest fairness. We perform experiments using five different seeds, resulting in training sets comprising distinct demonstrations while maintaining the test samples with seed 0. Fig.~\ref{fig:allcandidates} displays the results for different models, revealing a strong correlation between the model's performance and fairness score (i.e., fairer prompts yield better performance). The red star, referred to as the "Oracle" represents the optimal average performance, which consistently correlates with higher fairness. This observation prompts us to enhance the ICL performance by identifying the fairest prompt.

Nevertheless, discovering the fairest demonstration combination proves to be a formidable challenge, given the existence of $\sum_{k=1}^NC_N^kk!$ distinct candidates. As the size of the training set increases, this task becomes intractable. In order to tackle this problem, we propose two efficient strategies for approximating the most suitable demonstrations in the subsequent section.


 %Drawing inspiration from~\cite{calibrate2021zhao}, we propose that when the model encounters content without information (denoted by $\eta$), such as ``[N/A]'', its output probability for each category should be identical:
%\begin{equation}
%    p(\hat{y}=c|\rho \oplus \eta) =  p(\hat{y} \neq c|\rho \oplus \eta)
%\end{equation}

% Accordingly, we defined fairness form a perspective of entropy:
% \begin{equation}
%     fairness(\rho) = -\sum_{y \in \mathcal{Y}} p(y|\rho \oplus \eta) \log p(y|\rho \oplus \eta)
%     \label{eq:fair}
% \end{equation}


% In Fig.~\ref{fig:allcandidates}, 







\section{Fairest Prompt Search}

Drawing upon the aforementioned observations, we propose two strategies aimed at identifying the most fair prompt, which have been empirically demonstrated to achieve superior performance. Let us consider a training set $S$ comprising $n$ samples; the goal of these search strategies is to select a subset of samples from the training set and construct the context in a specific order so as to optimize the fairness criterion in Eq.~\ref{eq:fair}.


In an ideal scenario, we would consider the factors of demonstration selection and order permutation by examining $\sum_{k=1}^NC_N^kk!$ distinct candidates, which enumerates all possible situations. Here, $k$ represents the number of demonstrations selected, and $C$ signifies the combinatorial function. However, evaluating every candidate is infeasible, as demonstrated when $N=8$, yielding over $10^6$ candidates. In this paper, we introduce two search strategies to reduce computational cost: \topk and \greedy. The \topk strategy decreases complexity from $\Theta(\sum_{k=1}^NC_N^kk!)$ to $\Theta(N)$, but its performance hinges on the selection of $k$ and may be unstable when an unsuitable value of $k$ is chosen. As a result, we propose an additional greedy search strategy, termed \greedy, which lowers complexity to $O(N^2)$ and offers a superior approximation of the oracle solution.
Fig.~\ref{fig:app-cost} visualizes the computational costs over different training set size.






\begin{figure}[t]
    \centering
    \includegraphics[width=0.90\linewidth]{figs/method.pdf}
    \caption{Overview of Most-fair Prompting.}
    \label{fig:method}
\end{figure}

\subsection{T-fair-Prompting}
The central idea of  \topk is founded on the heuristic understanding that the fairest prompt usually consists of demonstration samples with reduced individual biases. Consequently, \topk  constructs the prompt through a two-stage process. Initially, the prediction bias is assessed when the prompt is formulated using individual demonstrations. Subsequently, the top-$k$ fairest demonstrations are chosen and employed to prompt the LLM. It is important to note that fairer demonstrations are likely to be situated towards the end of the sequence, as the generation is more influenced by proximate demonstrations, in accordance with prior research~\cite{calibrate2021zhao}. A comprehensive description of the process is presented in Algorithm~\ref{alg:topk}, while a visual representation can be found in Fig.~\ref{fig:method}. Specifically, when $k$ is equivalent to the size of the training set, the method degrade to a search for the optimal order of demonstrations. Nevertheless, \topk  is heavily reliant on the chosen value of $k$. More crucially, \topk addresses this issue through a purely local perspective, thereby neglecting considerations from a global standpoint, which typically results in sub-optimal outcomes. As a result, we subsequently introduce the \greedy method, which operates in a local-to-global fashion, as described below.


\subsection{G-fair-Prompting}
The \greedy algorithm adheres to the standard procedure of greedy search, which seeks the optimal solution by making locally optimal choices at each stage. In each step of the algorithm, the chosen demonstration is the one that allows the updated prompts to achieve the highest fairness score. This strategy balances the quality of the search with the worst-case time complexity. By accepting an increased worst-case time complexity of $O(N^2)$, the search quality is significantly enhanced. It is important to note that the \greedy algorithm operates from a local to global perspective as shown by Algorithm. During the initial stages, the bias of individual samples is taken into account, while the later stages focus on reducing global predictive bias. Specifically, at each step, we insert a new demonstration $\Gamma(x_i,y_i)$ from the remaining demonstration set $\mathcal{S}'$ (ensuring demonstrations are not repeated) at the beginning of the current context $\rho$ and select the demonstration that maximizes the  fairness improvement. Formally, at step 9 in Algorithm~\ref{alg:greedy}, the inserted demonstration should satisfy the following criterion:
\begin{equation}
    \arg \max_{x_i\in \mathcal{S}'} \fair(\Gamma(x_i,y_i)\oplus\rho)\quad \text{s.t. }\fair(\Gamma(x_i,y_i)\oplus\rho)>\fair(\rho).
\end{equation}

%The search process terminates when inserting any demonstration fails to improve fairness. The worst-case complexity of this strategy is $\Theta(\frac{1}{2}n(n-1))$.

\begin{minipage}{0.45\textwidth}
\begin{algorithm}[H]
    \caption{\topk}
    \label{alg:topk}
    \begin{algorithmic}[1] %[1] enables line numbers
        \STATE \textbf{Given:} training set $S=\{(x_i, y_i)\}_i^N$, pretrained LLM $\mathcal{M}$, transformation template $\Gamma(\cdot)$, and context-free input $\eta$
        \STATE Initial prompt $\rho$
        \FOR{$(x_i, y_i)$ in $S$}
        \STATE Inference $\hat{P} \leftarrow \{\hat{p}(y|\Gamma(x_i,y_i)\oplus\eta)|y \in \mathcal{Y}\}$ via $\mathcal{M}$
        \STATE Calculate the $\fair(\Gamma(x_i,y_i))$ according to Eq.~\ref{eq:fair}       
        \ENDFOR
        \STATE Sort $\fair_{i=1,\cdots,N}(\Gamma(x_i,y_i))$ in descending order
        \FOR{$d$ in $1, \cdots, k$}
        \STATE \emph{Insert} the most $d$ fair demonstration at the head of $\rho$
        \ENDFOR
        \RETURN $\rho$

    \end{algorithmic}
\end{algorithm}
\end{minipage}
  \hfill
\begin{minipage}{0.53\textwidth}
\begin{algorithm}[H]
    \caption{\greedy}
    \label{alg:greedy}
    \begin{algorithmic}[1] %[1] enables line numbers
        \STATE \textbf{Given:} training set $S=\{(x_i, y_i)\}_i^N$, pretrained LLM $\mathcal{M}$, transformation template $\Gamma(\cdot)$, and context-free input $\eta$
        \STATE Initial prompt $\rho$
        \WHILE{$S$ \emph{is not null}}
        \FOR{$(x_i, y_i)$ in $S$}
        \STATE $\rho_\text{tmp} \leftarrow  \Gamma(x_i,y_i)\oplus\rho$
        \STATE Inference $\hat{P} \leftarrow \{\hat{p}(y|\rho_\text{tmp}\oplus\eta)|y \in \mathcal{Y}\}$ via $\mathcal{M}$
        \STATE Calculate the $\fair(\rho_\text{tmp})$ according to Eq.~\ref{eq:fair}
        \ENDFOR
        \STATE \emph{Insert} the demonstration that can improve fairness best and \emph{remove} it from $S$ 
         \STATE \emph{Stop} searching when fairness can't be improved
        \ENDWHILE
        \RETURN $\rho$
 
    \end{algorithmic}
\end{algorithm}
\end{minipage}



















% \subsection{Tuning prompts on validation set may not a good strategy}

% Previous methods have employed a strategy of selecting an appropriate prompt by identifying the prompt that yields the highest performance on a validation set (or referred as training set~\cite{gao2020making}) within a discrete or continuous space. For instance, certain methods based on prompt tuning assume that the gradient is accessible and attempt to locate an optimal prompt through gradient descent using backpropagation.
% However, a white-box pre-trained language model is vulnerable to attack and service providers are unlikely to provide the weight information of the model, or even will not release the embedding strategy of the model. What's more, the performance of the model adjusted by these methods depends on the distribution of the authentication set. The usage of different users is often different. Although different authentication sets can be used for different users, this strategy is extremely costly and obviously impossible to implement.