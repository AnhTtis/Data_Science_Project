\section{Experiments}
\subsection{Experimental Setup}
\textbf{Models.} There are a large number of available LLMs (Appendix~\ref{sec:app-models}) including open-source models and black-box cloud API. Recently, Meta has released their powerful pretrained LLMs, LLaMA. LLaMA models with 13B parameters can achieve comparable performance in contrast to BLOOM and GPT-3 with much larger model size. In this paper, we evaluate the effectiveness of our method on BLOOM (176B) and LLaMA models of different sizes. We have opted to employ LLaMA (65B) as a substitute for GPT-3 in our experiments, since oepnai strictly restricts the API access to certain areas.


\textbf{Datasets.} We conducted experiments on various text classification datasets~\cite{wang2018glue}, namely SST-2, AGNews, CoLA, TREC, and RTE. Furthermore, the maximum input length of LLaMA is 512, and the sentences in RTE are too long for LLaMA. The task descriptions and statistics are available in Table~\ref{tab:datasets}.
% in the appendix.
\input{tabs/datasets}

\subsection{Results}
\input{tabs/accuracy}
We conducted experiments on different settings and reported the results of five runs. We compared our method with the diversity-guided searching strategy proposed by Zhang et al.\cite{zhang2022automatic} (Global view) and the similarity-guided searching strategy proposed by Liu et al.\cite{liu2021makes} (Local view). Note that methods based on local view are time-consuming since they require searching different demonstrations for every test example. Table~\ref{tab:four-topk-greedy} shows the performance of the different strategies, where "Random" indicates the average accuracy for enumerating all situations, "Diversity" and "Similarity" indicate demonstrations are selected according to diversity and similarity, respectively. For each dataset, we set the size of the training set to 4. "Diversity" and "Similarity" select 4 from 16 demonstrations, as they need more candidates. The baseline is expensive to compute since enumerating all candidates for 4 demonstrations in RTE on BLOOM will take more than 120 NVIDIA A100 GPU hours. We enumerate all candidates for the training set with 4 demonstrations on different models, as shown in Fig.~\ref{fig:allcandidates}. The results on models whose parameters less than 13B are shown in Table~\ref{tab:app-acc} (i.e., GPT2-XL (1.5B), LLaMA (7B), and LLaMA (13B)).


$\bullet$ \textbf{\greedy can reach a close approximation of enumeration.} To evaluate whether the \greedy (Greedy) method can approximate the best performance of enumerating all candidates, we marked the performance of \greedy with a green star (representing the closest value to averaged accuracy of \greedy on the line). We found that \greedy can achieve a very close approximation to enumeration. As shown in Fig.~\ref{fig:allcandidates}, most prompts searched by \greedy achieved a top $20\%$ ranking, and on BLOOM (176B), \greedy almost found the most fair prompt.


$\bullet$ \textbf{\greedy outperforms \topk.} As shown in Table~\ref{tab:four-topk-greedy}, although \topk achieves better performance compared with random selection, \greedy consistently outperforms \topk. Furthermore, Top-2 significantly outperforms Top-4 in most cases (over $5\%$), indicating that the number of demonstrations selected is crucial. Overall, the results demonstrate that \greedy achieves satisfactory performance with only a slight additional cost.

\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-0.6cm}
\centering
  \includegraphics[width=0.90\linewidth]{figs/greedycolabloom.pdf}
 \caption{BLOOM is not sensitive to CoLA.}
 \vspace{-0.3cm}
 \label{fig:cola-bloom}
\end{wrapfigure}
$\bullet$ \textbf{Compared with SOTA methods.} We compared our methods with several State-of-the-Art (SOTA) methods, including diversity-guided and similarity-guided techniques. We observed that our \textbf{greedy} approach outperforms most of these SOTA methods in most situations, and the improvements of over $10\%$ are observed on dataset TREC. The similarity-guided method, on the other hand, achieved the best performance on the topic classification task (AGNews). This is because it searches for a unique prompt for every different test example based on the distance between the embeddings of the training samples and the test example. This strategy selects demonstrations with labels that are the same as the test samples, and Language Models (LLMs) tend to predict biased predictions toward the labels that always appear in the context. However, the similarity-guided method may prove inadequate when applied to other tasks. Specifically, the similarity-guided strategy exhibits lower performance compared to random selection in QC and acceptability tasks. Furthermore, the \greedy approach may occasionally falter when the model's sensitivity to the task is not immediately evident, as observed in the acceptability task on BLOOM (depicted in Fig.~\ref{fig:cola-bloom}). Note that the training set size of compared methods is $4\times$ larger than ours.

$\bullet$ \textbf{Comparison with Calibration Method.} Post-calibration~\cite{calibrate2021zhao}, can enhance the accuracy of a given prompt in most cases. However, when the selected prompt is of poor quality, the performance may remain inadequate even after calibration. We compared the performance of \greedy with random selection with calibration (averaged on all candidates), and found that \greedy can outperform random selection with calibrated most situations. For example, on the topic classification task, \greedy achieves the best performance on most models. Moreover, we find that post calibration can harm the performance of the model and it occurs significantly times, so it is worthwhile to reconsider the influence of manipulating the model's probability directly.

\input{tabs/calibrated}
Post calibration~\cite{calibrate2021zhao} can improve the accuracy of a certain prompt (in most cases), but when the selected prompt is very poor, the performance still very poor even after calibration. We conducted experiments (Table~\ref{tab:calibrated}) to compare the performance of \greedy and random selection with calibration ("Average" and "Worst" indicate averaged accuracy and worst performance on all permutations of training examples), and observed that \greedy outperforms random selection with calibration in most case. For instance, on the CoLA, \greedy exhibited superior performance on most models. Additionally, we find that post-calibration could negatively affect the model's performance in many scenarios while it sometimes can improve the performance significantly even on selected prompts, for example, an improvement by $10\%$ is observed on BLOOM-TREC. Hence, it is crucial to reconsider the impact of directly manipulating the model's probability.
