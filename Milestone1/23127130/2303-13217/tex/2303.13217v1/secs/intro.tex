\section{Introduction}

Large language models (LLMs), such as GPT-3~\cite{gpt32020brown} and BLOOM~\cite{bloom2022}, have demonstrated remarkable ability in performing in-context learning (ICL) on downstream tasks. ICL refers to the process of conditioning an LLM to solve various downstream tasks using prompts constructed from a few demonstration input-output pairs \cite{petroni2019language} (i.e., few-shot prompting). Despite its impressive performance, prior research has shown that ICL suffers from high instability due to variations in the choice of in-context demonstrations, demonstration order, and prompt formats ~\cite{order2021lu,nie2022improving}. Therefore, constructing an appropriate prompt has been identified as a critical factor for improving the performance of ICL \cite{liu2023pre}.

Previous research studies this problem typically from two directions: (1) prompt tuning in the embedding space \cite{li2021prefix,liu2021p,hambardzumyan2021warp,qin2021learning,liu2021gpt} (2) prompt searching in the text space \cite{order2021lu,zhang2022automatic,gentile2022fast,diao2023active,liu2021makes,shi2023large}. The key idea of prompt tuning is to inject task-specific embedding into hidden layers and then tune these embeddings using gradient-based optimization \cite{liu2021p,liu2021makes}. However, these methods require to modify the original inference process of the model, which is impractical for the case of black-box LM services such as GPT3 and ChatGPT \cite{chatgpt}. Furthermore, prompt tuning introduces additional computational and storage costs, which is typically expensive for LLM. 
A more feasible and efficient way is to optimize prompting via searching approximate demonstration samples and ordering in the original text space \cite{order2021lu,liu2021makes}. Bunch of works are presented to constructs prompts from either "global" or "local" views. On the one hand, global-view based methods typically optimize the different elements of the prompt as a whole, with the aim of achieving superior performance. For example, one approach, as described in \cite{diao2023active}, constructs a search procedure that leverages the overall diversity of demonstrations. Another approach \cite{order2021lu} attempts to optimize the ordering of the entire set of demonstrations to achieve better performance. In contrast to the global view, local-view based methods optimize each individual demonstration by designing different heuristic selection criteria such as prior work KATE \cite{liu2021makes}.
These methods have achieved impressive improvements on a wide range of tasks. However, most of them still suffer from  the following limitations: 
% (1) searching prompts along an individual dimension such as training example selection or ordering, how the combination of these dimensions will affect performance still remains. (2) lack of unified interpretation of how these methods work. 
(1) Most of current research mainly focuses on searching prompts along a single dimension, such as example selection or order. However, the overall influence of various dimensions on the performance remains unclear.
(2) These methods are typically based on heuristic criteria, and there is a gap between them and actual performance. A unified view that explains how these methods work is needed.
(3) More importantly, existing methods optimize prompts globally or locally, which may lead to suboptimal performance.

%(3) performance evaluation on additional development/validation set is needed, which introduces nonnegligible computation costs and make the performance highly depends on the quality of the set selection.

%Most prompt tuning methods treat the prompt as a continuous value~\cite{hambardzumyan2021warp, qin2021learning, liu2021gpt} or directly optimize the embedding~\cite{liu2021p}, and fit them on a development set to achieve the best performance. However, since prompts should be in natural language or discrete tokens for many advanced LLMs~\cite{gpt32020brown,chatgpt}, a more intuitive perspective is to search for the best prompt in the original space. This can be achieved by selecting training examples for demonstration and changing the order of training examples. For instance, KATE~\cite{liu2021makes} constructs prompts by selecting the closest training examples to a testing sentence in the embedding space, whereas permutation selection~\cite{order2021lu} obtains the prompt by selecting the order that produces the most diverse answers on a probing set. Nevertheless, most of these methods require obtaining the embedding and even gradient of the LLMs, which can be difficult due to the increasing training costs and security considerations of large models~\cite{black2022sun}. Moreover, these methods often need to generate a probing set or an additional development set, which makes their calculation expensive and their performance dependent on the quality of the probing set and development set.


%To mitigate the above limitations, one straightforward way is to apply a given  prompt to a development set with enough labeled samples. Then the average performance (e.g., accuracy for sentiment classification) over this set can be directly used for assessing the prompt quality. However, such an approach heavily depends on the choose of the development set thus ICL's performance can be significantly affected by the set quality. Moreover, this method introduces nonnegligible computation costs especially when there are massive amounts of prompts need to be evaluated, since multiple inferences are required to traverse samples in the development set. 
In this paper, we revisit this problem from the perspective of \emph{predictive bias}. We find a key insight that the quality of a given prompt depends on its inherent bias. Based on this insight, 
we propose a surrogate metric based on predictive bias for evaluating the quality of prompts. This metric allows us to evaluate a prompt in a single forward process without an additional development set. Specifically, 
we apply a given prompt to a "content-free" input and expect the model output an uniform predictive distribution (a content-free input contains no useful information). Therefore, we employ the uniformity of the predictive distribution to characterize the bias of a give prompt.  This shares a similar idea to the prior work which uses this metric to calibrate the model output \cite{calibrate2021zhao}. In contrast to this work which mainly focus on using this metric for calibration when the prompt is fixed, we further explore its usage  in automatically searching an approximate prompt. Moreover, through extensive experiments, we empirically validate the correlation between the inherent bias of a given prompt and its quality measured by the average task performance on a given test set (see Fig.~\ref{fig:allcandidates}). 

Moreover, this bias-based metric allows us to build prompting optimization techniques in a "local-to-global" manner. We present two novel strategies for efficiently searching high-quality prompts in a  bias-guided way: (1) \topk (2) \greedy . We focus on a general setting where a labeled  set with size $N$ is given. The goal of our strategies is to perform combinatorial optimization over this set to find near-optimal prompts (i.e., select demonstrations and their orders). Specifically, \topk  uses an intuitive way that first computes the bias of each single demonstration (i.e., one-shot prompting) and then select the top-k fair demonstrations to form the final prompts. This strategy can be efficiently done with a complexity of $O(N)$. Note that \topk is based on the assumption that the optimal prompt is usually constructed from demonstrations with the smallest individual bias. However, this may not hold true in real situations and often leads to sub-optimal solutions.
Therefore, we further introduce \greedy to improve the search quality. \greedy follows the normal procedure of the greedy search which  finds the optimal solution by making locally optimal choices at each step. At each step of the algorithm, the selected demonstration is the one which makes the updated prompts achieves the best fairness score.This strategy trades off the quality of the search with the worst-case time complexity. By accepting a higher worst-case time complexity of $O(N^2)$, the search quality is significantly improved. Note that \greedy works from a local to global perspective, wherein bias of individual samples are considered in the early stages while the later stage focus on the reduction of global predictive bias.

To evaluate the effectiveness of our strategies, we conduct extensive experiments with current mainstream models, such as GPT-3~\cite{gpt32020brown}, on various downstream tasks. Our results indicate that our method can significantly enhance the model's in-context learning performance in an effective and interpretable manner. The overall contribution is summarized as follows:
\begin{itemize}
    \item We introduce to use the predictive bias to assess the quality of a given prompt in an efficient and development set independent way and the empirical effectiveness of this metric is comprehensively validated.
    \item Based on the above idea, we propose two efficient and effective strategies, namely, \topk and \greedy to optimize the prompts.
    \item The effectiveness of these two strategies are validated on various LLMs ranging from GPT-series models to LMaMA family~\cite{touvron2023llama} released by Meta recently. Consistent relative improvements of over $10\%$ have been observed over different downstream tasks in contrast to SOTA methods.
\end{itemize}
\noindent \textbf{Relation to Calibration-before-use:}
Our paper shares a similar metric with cal-before-use~\cite{calibrate2021zhao} to asses the predictive bias of a given prompt. However, the prior approach aims to use this metric to calibrate the output, which can be still easily affected by the quality of the used prompt (more results can be found in Table~\ref{tab:calibrated}).  In contrast, our research aims to find a near-optimal prompt on the original space to improve the model's performance, without requiring any post-adjustment to the output of the model. Moreover, we have firstly empirically validated the connection between predictive bias and the final task performance as shown in Fig.~\ref{fig:allcandidates}, which has not been studied in \cite{calibrate2021zhao}.
Through experiments, we have discovered that, even without calibration, the prompt selected by our method can outperform a randomly selected prompt with calibration. 

%Unlike adjusting the model output under a particular prompt, our focus is on using the training set to select appropriate demonstrations and form an appropriate prompt.




%employ the entropy of LLM's output distribution as the fairness metric .
%In order to reduce the computational cost  and  overcome the dependence on the development set, we propose a surrogate metric based on predictive bias for evaluating the quality of prompts. Specifically, this metric is %%%

%this paper introduces a unified metric that can assess the quality of arbitrary prompts varying in training examples, orders and demonstration numbers in an efficient way without additional development set. Specifically, the metric is designed from the view of predictive bias. revisit the problem of prompt construction from a unified perspective of predictive bias. We introduce a novel metric to evaluate the predictive bias of a fixed prompt against labels or given attributes. Predictive bias refers to the deviation of the prompt from the ground truth data, which may cause the model to produce inaccurate or unreliable predictions. We demonstrate that prompts with higher bias always lead to unsatisfactory predictive quality, which highlights the importance of prompt construction in achieving optimal in-context learning performance. To address the problem of prompt construction, we propose a simple-yet-effective search strategy that utilizes the introduced predictive bias metric. Our search strategy aims to find the prompt that leads to better performance with a high probability. To evaluate the effectiveness of our approach, we conduct extensive experiments with current mainstream models, such as GPT-3~\cite{gpt32020brown}, on various downstream tasks. Our results indicate that our method can significantly enhance the model's in-context learning performance in an effective and interpretable manner. Overall, our work contributes to the ongoing effort of improving the performance and stability of large language models in the context of in-context learning tasks. The rest of this paper is organized as follows: we first provide a review of related work in prompt construction and in-context learning. Then, we present our approach, including the predictive bias metric and the search strategy. Finally, we present and discuss our experimental results, followed by our conclusion and future work.



