{
    "arxiv_id": "2303.13217",
    "paper_title": "Fairness-guided Few-shot Prompting for Large Language Models",
    "authors": [
        "Huan Ma",
        "Changqing Zhang",
        "Yatao Bian",
        "Lemao Liu",
        "Zhirui Zhang",
        "Peilin Zhao",
        "Shu Zhang",
        "Huazhu Fu",
        "Qinghua Hu",
        "Bingzhe Wu"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 2,
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "abstract": "Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning. We perform comprehensive experiments with state-of-the-art mainstream models such as GPT-3 on various downstream tasks. Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13217v1",
        "http://arxiv.org/pdf/2303.13217v2"
    ],
    "publication_venue": null
}