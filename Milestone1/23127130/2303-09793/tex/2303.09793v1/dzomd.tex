%\documentclass[9pt,reqno,twocolumn]{IEEEtran}
%\documentclass[journal,twoside,web]{IEEEtran}
%\documentclass[10pt,technote]{IEEEtran}
%\documentclass[9pt,shortletter,twoside,web]{ieeecolor}
%\usepackage{generic}
\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{lcsys}
%\IEEEoverridecommandlockouts                              %  \thanks command
%\overrideIEEEmargins
%\usepackage[most]{tcolorbox}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{textcomp}
\usepackage{epsfig}
%\usepackage{mathptmx}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{float}
\usepackage{siunitx}
\usepackage{scalerel}
\usepackage{cite}
\usepackage{textcomp}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{stackengine}
\usepackage{mathtools}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2017}
{Author \MakeLowercase{\textit{et al.}}: Preparation of letters for textsc{IEEE Control Systems
Letters} (November 2021)}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
 \newtheorem{assumption}{Assumption}
 \newtheorem{Corollary}{Corollary}

\title{\LARGE \bf Distributed Estimation}
%---------------New commands
\newcommand{\ncom}{\newcommand}
\def\deff{\stackrel{\triangle}{=}}
\newcommand{\beqn}{\begin{eqnarray*}}
\newcommand{\eeqn}{\end{eqnarray*}}
\newcommand{\beq}{\begin{eqnarray}}
\newcommand{\eeq}{\end{eqnarray}}
% \ncom{\beqn}{\begin{eqnarray*}}
% \ncom{\eeqn}{\end{eqnarray*}}
% \ncom{\beq}{\begin{eqnarray}} \ncom{\eeq}{\end{eqnarray}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\inprod}[2]{\left\langle #1, #2 \right\rangle}
\ncom\R{\mathbb{R}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\grad}{grad}
\DeclareMathOperator*{\conhull}{co}
\DeclareMathOperator*{\intr}{int}
\DeclareMathOperator*{\bd}{bd}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\definecolor{my-blue}{cmyk}{0.80, 0.13, 0.14, 0.04, 1.00}
%---------------------------------------
\author{Anik Kumar Paul, Arun D Mahindrakar and Rachel K Kalaimani
\thanks{Anik  is a Graduate student in the Department of Electrical Engineering, IIT Madras, Chennai-600036, India
        {email: anikpaul42@gmail.com}}
        \thanks{Arun  and Rachel are  with the Department of Electrical Engineering, Indian Institute of Technology Madras, Chennai-600036, India (email: arun\_dm@iitm.ac.in, rachel@ee.iitm.ac.in) }
    }

\begin{document}
\title{Robust Analysis of Almost Sure Convergence of Zeroth-Order Mirror Descent Algorithm}
\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
This letter presents an almost sure convergence of the zeroth-order mirror descent algorithm. The algorithm admits non-smooth convex functions and a biased oracle which only provides noisy function value at any desired point. We approximate the subgradient of the objective function using Nesterov's Gaussian Approximation (NGA) with certain alternations suggested by some practical applications. We prove an almost sure convergence of the iterates' function value to the neighbourhood of optimal function value, which can not be made arbitrarily small, a manifestation of a biased oracle. This letter ends with a concentration inequality, which is  a finite time analysis that predicts the likelihood that the function value of the iterates is in the neighbourhood of the optimal value at any finite iteration.
\end{abstract}

\begin{IEEEkeywords}
Almost sure convergence, subgradient approximation, mirror descent algorithm
\end{IEEEkeywords}

\section{Introduction}
One of the earliest subfields of optimization is derivative-free optimization \cite{10.1093/comjnl/3.3.175,10.1214/aoms/1177699070,brent2002algorithms}. It refers to an optimization problem with an oracle that only provides noisy function value at a desired point. Following numerous attempts by researchers to accurately approximate a function's subgradient from its value (for example see, \cite{doi:https://doi.org/10.1002/0471722138.ch5,NestSpok17}), it has now gained popularity in the optimization community due to its use in a variety of different domains. For a full introduction to derivative-free optimization and its various applications in diverse domains, see \cite{9186148} (and the references therein).

In this letter, we focus on the zeroth-order mirror descent algorithm \cite{9416872}, where the approximated subgradient established in \cite{NestSpok17} replaces the subgradient of the convex objective function in standard mirror descent algorithm \cite{10.1561/2200000050}.  Originally, the mirror descent algorithm generalizes the standard gradient descent algorithm in a more general non-Euclidean   space \cite{anik}.   In recent years, the mirror descent algorithm has grasped significant attention in the large-scale optimization problems, data-driven control and learning, power system, robotics and game theoretic problems \cite{8409957}. For the stochastic mirror descent algorithm, we refer the reader to \cite{doi:10.1137/120894464}.  However, precise information regarding the convex objective function's subgradient or stochastic subgradient is accessible in these articles. In this letter, we assume that we can only access the noisy evaluation of the convex objective function at a desired point via a ``biased zeroth-order" oracle.  The oracle setting is driven by a large number of practical applications in which only the noisy function values are provided at a point and obtaining a subgradient or stochastic subgradient may not be feasible at that point.  As a result, we must approximate the function's subgradient from the noisy measurement of the function value. This gives rise to  the notion of zeroth-order optimization  \cite{7055287}. Every step  in the zeroth-order algorithm is similar to its first-order counterpart (such as gradient descent or mirror descent), except that the function's subgradient must be approximated at every point. There has recently been a surge of interest generated in different variants of  zeroth-order optimization, for both convex and non-convex functions \cite{doi:10.1137/120880811, https://doi.org/10.48550/arxiv.1809.06474,6870494,8619028,8703066, doi:10.1137/18M119046X}, where the subgradient is approximated by NGA \cite{NestSpok17}.

We extend the analysis of zeroth-order optimization in this letter, focusing on the zeroth-order mirror descent (ZOMD) algorithm. The problem framework and analysis in this work differ significantly from the recent literature. The main objective of this study is to show the almost sure convergence of the function value of  iterates of the ZOMD algorithm to a neighbourhood of optimal value, as compared to the bulk of the literature, which focuses on showing that the expected error in function value converges to the neighbourhood of zero. An almost sure convergence guarantee to a neighbourhood of optimal value is more significant than the convergence in expectation since it  describes what happens to the individual trajectory in each iteration.
%The main challenge arises in the analysis of the almost sure convergence is  the presence of approximated subgradient which is a biased stochastic subgradient in each iteration.
To the best of our knowledge, no prior work on  almost sure convergence  for zeroth-order optimization has been published.   The problem framework in this study differs from most other works in that it includes a biased oracle that delivers only biased measurement of function value (the expectation of noise in the function measurement is non-zero) at any specified point. The motivation to consider ``biased oracle" can be found in application of reinforcement learning and financial risk measurement (see \cite{prasanth9736646} and references therein for more details).
Furthermore, unlike other publications, we consider that the oracle returns distinct noise values for two different points.
Lastly, in addition to showing almost sure convergence, we estimate the likelihood that the function value of the iterates will be in the neighbourhood of optimal value in any finite iteration. This analysis aids in determining the relationship between the convergence of the ZOMD algorithm and the various parameters of the approximated subgradient. The following list summarises the key contribution of this study.
%Firstly, we consider here a biased measurement of function value (the expectation of noise in the function measurement is non-zero) unlike in recent literature where only unbiased measurement is considered. Hence we need to reevaluate the properties (expectation and second moment) of approximated subgradient for Nesterov's Gaussian approximation for biased measurement. The motivation to consider biased function measurement can be found in application of reinforcement learning and financial risk measurement (see \cite{prasanth9736646} and references therein for more details).  Secondly, in most of the literature the focus is on showing that the expected error in function value converges to a neighbourhood of zero. In this letter we will show an almost sure convergence of function value to neighbourhood of optimal value.
 %Finally, in this framework we estimate an probability that the function value of the iterates will be in the  neighbourhood of optimal value in any finite time. This analysis helps to determine the dependence on the various parameter of approximated subgradient to the convergence of the ZOMD algorithm.
 %
 %Providing a bound on the error almost surely is important because it actually describes what happen to the individual trajectory for each iteration. \textit{The question, to which we give a positive answer in this letter is as follows?}
%
%\textbf{Can we provide an almost sure bound on the error between the function value of the iterate sequence and the optimal solution? }
%
%In this note we employ zeroth order mirror descent (ZOMD) algorithm for both centralized and distributed (DZOMD) case with Nesterov's Gaussian Approximation (NGA) technique and also simultaneous perturbation technique (SPSA). We have shown that under suitable condition iterates of the ZOMD (with NGA or, SPSA) algorithm reaches a neighbourhood of an optimal solution eventually. For DZOMD algorithm, we have shown that all agent's iterate achieve consensus almost surely and also reaches a neighbourhood of optimal solution eventually. This letter ends with a very simple numerical example that employs DZOMD algorithm to show the performance of the algorithm.
%
%\textbf{Is it possible to  solve the zeroth order mirror descent (or, gradient descent) algorithm such that we can provide a bound  such that the error between the function value of iterate sequence and optimal solution follows that bound almost surely? }
\begin{enumerate}
    \item  We analyse the ZOMD algorithm under the assumption that a biased oracle returns noisy function value at a predetermined point where the expected error is nonzero. For the biased oracles, we re-evaluate the parameters of the approximated subgradient of the objective function at a specific location, which is calculated using NGA.
    \item We prove that, under certain assumptions, the function values of the iterates of ZOMD algorithm almost surely converges to the neighbourhood of optimal function value. This neighbourhood is determined by several parameters, which are explored in this study.
    \item Finally, we show that for any confidence level and a given neighbourhood around the optimal function value, the function value of the iterate sequence should be in that neighbourhood after some finite iteration with that confidence. We also present an expression for that finite iteration that is influenced by the neighbourhood, confidence level, and other properties stated in the letter.
\end{enumerate}
\section{Notation and Mathematical Preliminaries}
Let $\R$ and $\mathbb{R}^n$ represent the set of real numbers, set of $n$ dimensional real vectors.   Let~$\norm{.}$ denote any \mbox{norm} on~$\R^n$.  Given a norm $\norm{.}$ on $\mathbb{R}^n$, the dual norm of $x\in \R$ is $\norm{x}_\ast := \sup\{ \inprod{x}{y}: {\norm{y}\leq 1}, y\in \R^n \}$, where $\inprod{x}{y}$ denotes the standard inner-product on $\R^n$. $I_n$ is $n \times n$ identity matrix. A random vector  $X\sim \mathcal{N}(0_n, I_n)$ denotes a $n$- dimensional normal random vector with zero-mean and unit standard-deviation. For two random variables $X$ and $Y$,  $\sigma(X,Y)$ is the smallest sigma-algebra generated by  random variables $X$ and $Y$. Because of equivalence of norm $\norm{.}_2 \leq \kappa_1 \norm{.}_\ast$ and $\norm{.}_2 \leq \kappa_2 \norm{.}$ and $\kappa = \kappa_1  \kappa_2$.

Let  $f: \mathbb{R}^n \to \mathbb{R}$ be a convex function.   For $\delta \geq 0$, the vector $g_{\delta}\in \R^n$  is called a $\delta$-subgradient of $f$ at $x$ if and only if $f(y)\geq f(x)+\inprod{g_{\delta}}{y-x} - \delta \;\ \; \forall \; y\in \R^n $ \cite{NestSpok17}. The set of all $\delta$-subgradients at a point $x$ is called the $\delta$-subdifferential of  $f$, denoted by $\partial_\delta f(x)$. If $\delta = 0$, we simply write the notation $\partial f(x)$. If $f$ is differentiable at $x$, then $\partial f(x) = \{ \nabla f(x) \}$, gradient of $f$ at $x$. We say $f \in \mathcal{C}^{0,0}$ if $\exists \; L_0 > 0$ such that $\norm{f(x)-f(y)} \leq L_0 \norm{x-y}$ and $f \in \mathcal{C}^{1,1}$ if $f$ is continuously differentiable and  $\exists \; L_1 >0
$ such that $\norm{\nabla f(x) - \nabla f(y)} \leq L_1 \norm{x-y}$ $\forall \; \; x , y \in \mathbb{R}^n$.

If $f$ has directional derivative in all directions, then we can form the Gaussian approximation as follows: $f_\mu (x) = \frac{1}{(2\pi)^{\frac{n}{2}}}  \int\limits_{\mathbb{R}^n} f(x+\mu u) e^{-\frac{1}{2}\norm{u}^2} du$, where $\mu >0$ is any constant. The function $f_
\mu$ is differentiable at each $x \in \mathbb{R}^n$ and  $\nabla f_{\mu} (x) = \frac{1}{(2 \pi)^{\frac{n}{2}}\mu} \int\limits_{\mathbb{R}^n} u f(x+ \mu u) e^{-\frac{1}{2}\norm{u}^2} du$. It can also be seen that $\nabla f_\mu(x)$ $\in$ $\partial_{\delta} f(x)$, where, $\delta = \mu L_0 \sqrt{n}$ if $f \in \mathcal{C}^{0,0}$ and $\delta = \frac{\mu^2}{2}L_1 \sqrt{n}$ if $f \in \mathcal{C}^{1,1}$.

 Let $(\Omega, \mathcal{F}, \mathbb{P})$ denote a  probability space. An event $A \in \mathcal{F}$ is occurred almost surely (a.s.) if $\mathbb{P}(A) =1$. If $X\sim \mathcal{N}(0_n, I_n)$, it can be shown that $\mathbb{E}[\norm{X}_2^p] \leq n^{\frac{p}{2}}$ if $p \in [0,2]$ and $\mathbb{E}[\norm{X}_2^p] \leq (p+n)^{\frac{p}{2}}$ if $ p > 2$.
We will use the following two Lemmas in our analysis.
\begin{lemma}[  \cite{ROBBINS1971233}]
Let $\{X_t\}_{t \geq 1}$ be a martingale with respect to a filtration $\{\mathcal{F}_t\}_{t \geq 1}$  such that $\mathbb{E}[\norm{X_t}] < \infty$
%and $\mathbb{E}[X_{t+1}|\mathcal{F}_t] =0$ $\forall \; \; t$
and $\{\beta(t)\}$ is a non-decreasing sequence of positive numbers such that $\lim\limits_{t \to \infty} \beta(t) = \infty$ \\and $\sum\limits_{t \geq 1} \frac{\mathbb{E}[\norm{X_t-X_{t-1}}^2|\mathcal{F}_{t-1}]}{\beta(t)^2} < \infty$, then $\lim\limits_{t \to \infty}  \frac{X_t}{\beta(t)}= 0$ a.s.
\label{SLNN1}
\end{lemma}

\begin{lemma}
    If $\{X_t, \mathcal{F}_t\}_{t \geq 1}$ is a non-negative submartingales, then, for any $\epsilon > 0$ we have $ \mathbb{P}(\max\limits_{1 \leq t \leq T} X(t) \geq \epsilon ) \leq \frac{E[X(T)]}{\epsilon}$.
    %\begin{equation*}
     %   \mathbb{P}(\max\limits_{1 \leq t \leq T} X(t) \geq \epsilon ) \leq \frac{E[X(T)]}{\epsilon}.
    %\end{equation*}
    %and if $\{X_t, \mathcal{F}_t\}_{t \geq 1}$ is a martingale then for any $\epsilon > 0$ we have
    %\begin{equation*}
     %   \mathbb{P}(\max\limits_{1 \leq t \leq T} X(t) \geq  \epsilon) \leq \frac{\mathbb{E}[\norm{X(T)}]}{\epsilon}.
    %\end{equation*}
    \label{doob's maximal inequality}
\end{lemma}
%\begin{lemma}[ \cite{ROBBINS1971233}]
%Let $\{B(t)\}_{t \geq 1}$, $\{D(t)\}_{t \geq 1}$, $\{H(t)\}_{t \geq 1}$ be a non-negative random process, adapted with a filtration $\{\mathcal{F}_t\}_{t\geq 1}$. Assume that $\{\zeta(t)\}_{t \geq 1}$ be a non-negative deterministic sequence such that $\sum\limits_{t \geq 1} \zeta(t) < \infty$. Suppose $\mathbb{E}[B(t+1)|\mathcal{F}_t] \leq (1+\zeta(t))B(t) - D(t) + H(t)$ and further $\sum\limits_{t \geq 1} H(t) < \infty$ w.p. 1. Then, the sequence $\{B(t)\}_{t \geq 1}$ converges to a non-negative random variable and $\sum\limits_{t \geq 1} D(t) < \infty$ a.s.
%\label{stochascong}
%\end{lemma}
%In a multi-agent system, the inter-agent communication network can be depicted by a graph.
%$We consider an undirected graph \textit{G} = (\textit{V},  \textit{E}), where $\textit{V} = [N]$ is the set of nodes which represent the agents   and an element of the edge set  $\textit{E} \subseteq \textit{V} \times \textit{V}$ represents the communication link that exists between any two agents. Let $W$ be a symmetric matrix, where  $[W]_{i,j}$ represents the weight associated with the communication link that exists between agent $i$ and $j$. That is
 % \begin{equation*}
  %    \begin{array}{lcl}
           %[W]_{i,j}& > & 0 \ \ \ \ \forall \ \  (j,i) \in \textit{E}  \\
           %& =  & 0 \ \ \ \ \ \ %\text{otherwise}.
      %\end{array}
  %\end{equation*}
%A matrix $W \in \mathbb{R}^{N\times N}$ is said to be doubly stochastic if
 %   $\sum\limits_{i=1}^{N} [W]_{i,j} = \sum\limits_{j=1}^{N} [W]_{i,j} = 1,\;\; \forall \; i,j \in [N]$.
%Let  $f: \mathbb{R}^n \to \Bar{\mathbb{R}}$ be a proper convex function.  A vector $g\in \R^n$  is called a subgradient of $f$ at $x$ if and only if $f(y)\geq f(x)+\inprod{g}{y-x}\;\ \; \forall \; y\in \R^n $. The set of all subgradients at a point $x$ is called the subdifferential  $f$ denoted by $\partial f(x)$. A vector $g\in \R^n$  is called a $\epsilon$-subgradient of $f$ at $x$ if and only if $f(y)\geq f(x)+\inprod{g}{y-x} - \epsilon\;\ \; \forall \; y\in \R^n $. The set of all $\epsilon$- subgradients at a point $x$ is called the $\epsilon$-subdifferential  $f$ denoted by $\partial_\epsilon f(x)$.
%If a function $f$ is $\rho$-strongly convex set in $\mathbb{X}$, then we can write $ \inprod{g(x)-g(y)}{x-y} \geq \rho \norm{x-y}^2 \; \;  \forall \; \; x,y \in \mathbb{X}.$
%
%Consider a function $f: \mathbb{R}^n \to \mathbb{R}$ with directional derivative in all directions. Then we can form the Gaussian approximation as follows: $f_\eta (x) = \frac{1}{(2\pi)^{\frac{n}{2}}}  \int\limits_{\mathbb{R}^n} f(x+\eta u) e^{-\frac{1}{2}\norm{u}^2} du$ where $\eta >0$ is any constant.
%The following lemma regarding the Gaussian approximation is helpful for our analysis.
%\begin{lemma}
%Consider $f:\mathbb{R}^n\to \mathbb{R}$ and $f_\eta$ be its Gaussian approximation. Then the following statements hold.
%\begin{enumerate}
 %   \item If $f$ is convex, $f_\eta$ is convex, differentiable and also if $f$ is $L_f$ Lipschitz continuous then $f(x) \leq f_\eta(x) \leq f(x) + \eta L_f \sqrt{n}$.
  %  \item If $f$ is convex and $L_f$ Lipschitz continuous, then $\nabla f_\eta (x) \in \partial_\epsilon f(x)$, where $\epsilon = \eta L_f \sqrt{n}$, that is,
   % \begin{equation*}
    %    f(y) \geq f(x) - \epsilon + \inprod{\nabla f_\eta(x)}{y-x} \; \; \forall x,y \in \mathbb{R}^n.
    %\end{equation*}
%\end{enumerate}
%\label{NSDAPPROX}
%\end{lemma}
%
%We will use the following two Lemmas in our analysis.
%
%\begin{lemma}[Strong Law of Large Numbers for Martingales]
%Let $\{X_t\}_{t \geq 1}$ be a sequence of random variables and $\{\mathcal{F}_t\}$ be sequence of $\sigma$ algebras such that $\mathbb{E}[\norm{X_t}] < \infty$ and $\mathbb{E}[X_{t+1}|\mathcal{F}_t] =0$ $\forall \; \; t$ and $\{\beta(t)\}$ is a non-decreasing sequence of positive numbers such that $\lim\limits_{t \to \infty} \beta(t) = \infty$ and also $\sum\limits_{t \geq 1} \frac{\mathbb{E}[\norm{X_t}^2|\mathcal{F}_{t-1}]}{\beta(t)^2} < \infty$ , then $\lim\limits_{t \to \infty}     \frac{\sum\limits_{k=1}^{t}X_k}{\beta(t)}= 0$ a.s.
%\label{SLNN}
%\end{lemma}
%
%We next state a lemma that plays a key role in the convergence analysis.
%
%\begin{lemma}[Stochastic convergence Lemma \cite{10.5555/548484}]
%Let $\{B(t)\}_{t \geq 1}$, $\{D(t)\}_{t \geq 1}$, $\{H(t)\}_{t \geq 1}$ be a non-negative random process, adapted with a filtration $\{\mathcal{F}_t\}_{t\geq 1}$. Assume that $\{\zeta(t)\}_{t \geq 1}$ be a non-negative deterministic sequence such that $\sum\limits_{t \geq 1} \zeta(t) < \infty$. Suppose $\mathbb{E}[B(t+1)|\mathcal{F}_t] \leq (1+\zeta(t))B(t) - D(t) + H(t)$ and further $\sum\limits_{t \geq 1} H(t) < \infty$ w.p. 1. Then, the sequence $\{B(t)\}_{t \geq 1}$ converges to a non-negative random variable and $\sum\limits_{t \geq 1} D(t) < \infty$ almost surely.
%\label{stochascong}
%\end{lemma}
\section{Problem Statement}
Consider the following optimization problem
\begin{equation}\tag{CP1}
    \begin{split}
  \min\limits_{x \in \mathbb{X}} f(x)
    \end{split}
    \label{CP1}
\end{equation}
The constraint set $\mathbb{X}$ is a convex and compact subset of $\mathbb{R}^n$ with diameter $D$.  The function $f:  \mathbb{R}^n \to \mathbb{R}$ is a convex.   Define $ f^\ast = \min\limits_{x \in \mathbb{X}} f(x)$ and   $\mathbb{X}^\ast = \{x^\ast \in \mathbb{X} | f(x^\ast) = f^\ast\}$.
%\begin{equation*}
 %   f^\ast = \min\limits_{x \in \mathbb{X}} f(x) \; \; \text{and} \; \;  \mathbb{X}^\ast = \{x^\ast \in \mathbb{X} | f(x^\ast) = f^\ast\}
%\end{equation*}
 Observe that $\mathbb{X}^\ast$ is nonempty due to compactness of the constraint set $\mathbb{X}$ and continuity of $f$. We assume  in this letter  that we have  an oracle which  generates a noisy value of the function at a given point $x \in \mathbb{X}$. That is, at each point $x\in \mathbb{X}$,  we have only the information $\hat{f}(x) = f(x) + e(x, \omega)$, where $e (x, \omega): \mathbb{R}^n \times \Omega \to \mathbb{R}$ is a random variable for each $x \in \mathbb{X}$ satisfying
\begin{equation}
\begin{split}
    & \mathbb{E}[e(x,\omega)]= b(x) \; \; \text{with} \; \; \norm{b(x)}_\ast \leq B \; \; \\ & \text{and} \; \; \mathbb{E}[\norm{e(x,\omega)}^2] \leq \mathrm{V}^2
    \end{split}
    \label{biased oracle}
\end{equation}
where, $B$ is a non-negative constant,  and $V$ can be any constant.
 \begin{remark}
  In the context of  zeroth-order  stochastic optimization problem \cite{doi:10.1137/120880811,https://doi.org/10.48550/arxiv.1809.06474,6870494}, the objective is to solve the optimization problem: $\min\limits_{x \in \mathbb{X}}  f(x) = \mathbb{E}[F(x, \omega)]$   and  the oracle only provides   $F(x, \omega)$ at any desired $x \in \mathbb{R}^n$.  In such a  situation, it is straightforward  to verify that  $\mathbb{E}[e(x,\omega)] = 0$, implying that $B = 0$. The assumption of positive $B$  makes the problem more generic than previous recent studies. In a broader sense, if $B = 0$, we call it an \emph{unbiased oracle}.
\\ However, $B$ is non-zero in many applications (see \cite{prasanth9736646} and references therein for further details), therefore the problem in this study is more general than in other recent works  due to the presence of positive $B$.
 \end{remark}
 %\begin{remark}
 %In many other applications there is no  such probabilistic assumption on the noise. We assume that   $\norm{e(x, \omega)} \leq B$.
 %The consideration of a bounded error  in the problem statement validates  the use of the word ``robustness" in the letter's title.
%\end{remark}

  For sake of brevity, we henceforth use $e(x)$ to denote $e(x,\omega)$.
In the next section, we discuss the zeroth-order mirror descent algorithm.
\section{Zeroth-Order Mirror Descent Algorithm}
Mirror descent algorithm is a generalization of standard subgradient descent algorithm where the Euclidean norm is replaced with a more general Bergman divergence as a proximal function. Let $R$ be the $\sigma_R$-strongly convex function and differentiable over an open set that contains the set $\mathbb{X}$. The  Bergman divergence $\mathbb{D}_R(x,y) : \mathbb{X} \times \mathbb{X} \to \mathbb{R}$ is  $\mathbb{D}_{R}(x,y) := R(x)- R(y) - \inprod{\nabla R(y)}{x-y} \; \; \forall \; \; x,y \in \mathbb{X}.$
It is clear from the definition of strong convexity that
\begin{equation}
    \mathbb{D}_R(x,y) \geq \frac{\sigma_R}{2}\norm{x-y}^2.
    \label{mirrordescent implication1}
\end{equation}
\begin{equation}
\begin{split}
    \mathbb{D}_R(z,y)- \mathbb{D}_R(z,x)-\mathbb{D}_R(x,y) &= \inprod{\nabla R(x) - \nabla R(y)}{z-x} \\
    & \forall \; \;  x,y,z \in  \mathbb{X}.
    \end{split}
    \label{mirrordescent implication2}
\end{equation}

 %Next we state an assumption on Bergman divergence.
  %
  %
  %\begin{assumption}
  %Let $x$ and $\{y(i)\}_{i=1}^{k}, k \in \mathbb{N}$, belong to the convex set $\mathbb{X}$, then the Bergman divergence $\mathbb{D}_R(x,y)$  satisfies the Jensen's inequality with respect to the variable $y \in \mathbb{X}$ and for all  $i \in [k]$, $\beta(i) \in \mathbb{R}_{+}$
  %\begin{equation*}
      %\mathbb{D}_R(x,\sum\limits_{i=1}^{k} \beta(i) y(i)) \leq \sum\limits_{i=1}^{k} \beta(i)  \mathbb{D}_R(x,y(i)),\;  \sum\limits_{i=1}^{n} \beta(i) = 1.
 % \end{equation*}
 % \label{assumption2}
  %\end{assumption}
  %Bergman divergence $\mathbb{D}_R(x,y)$ is convex with respect to the first variable $x$ by virtue of its definition. But Assumption \ref{assumption2} implies $\mathbb{D}_R(x,y)$ is  jointly convex with respect to both the variable $x$ and $y$, which is  standard in the literature on Mirror Descent.
 We outline  the steps of  the mirror descent algorithm.

At iteration $t$, let $x_t$ be the iterates of the ZOMD algorithm.
We  approximate the subgradient  of  function $f(x)$ at $x = x_t$ as follows.
We  generate a normal random vector $u_t \sim \mathcal{N}(0_n, I_n)$. We use the zeroth-order oracles to get the  noisy function values ($\hat{f}$) at two distinct values, that is,

$\hat{f}(x_t + \mu u_t) = f(x_t + \mu u_t) + e (x_t + \mu u_t , \omega_t^1)$ and

$\hat{f}(x_t) = f(x_t) + e (x_t,\omega_t^2)$.
%\begin{equation*}
 %   \hat{f}(x_t + \mu u_t) = f(x_t + \mu u_t) + e (x_t + \mu u_t , \omega_t^1)
%\end{equation*}
%and
%\begin{equation*}
 %   \hat{f}(x_t) = f(x_t) + e (x_t,\omega_t^2).
%\end{equation*}
Note that $\omega_t^1$ and $\omega_t^2$ are two independent realizations from the sample space $\Omega$ according to the probability law $\mathbb{P}$. Hence, we approximate the subgradient of $f$ at $x = x_t$, denoted by $\Tilde{g}(t)$ as $\Tilde{g}(t) = \frac{\hat{f}(x_t+ \mu u_t)-\hat{f}(x_t)}{\mu} u_t$.
%\begin{equation*}
 %   \Tilde{g}(t) = \frac{\hat{f}(x_t+ \mu u_t)-\hat{f}(x_t)}{\mu} u_t .
%\end{equation*}
The next iterate $x_{t+1}$ is calculated as follows:
\begin{equation}
    x_{t+1} = \argmin\limits_{x \in \mathbb{X}} \{ \inprod{\Tilde{g}(t)}{x-x_t} \} + \frac{1}{\alpha(t)} \mathbb{D}_R(x,x_t)) \}
    \label{DSMD}
\end{equation}
where, $\alpha(t)$ is the step-size of the algorithm. To show almost sure convergence, we consider weighted averaging akin to the recent work \cite{doi:10.1137/120894464} in first-order algorithm as $ z_t = \frac{\sum\limits_{j=1}^{t}\alpha(j)x_j}{\sum\limits_{k=1}^{t} \alpha(k)}$. The Bergman divergence should be chosen in such a way that \eqref{DSMD} is computationally easier to execute or a closed form solution to \eqref{DSMD} is available \cite{10.5555/1046920.1194902}.
%The steps of the algorithms have been explained in Algorithm \ref{algorithm1}.

 %\begin{algorithm}
  %\begin{algorithmic}
 % \REQUIRE Total number of iterations $T$, step-size sequence $\{\alpha(t)\}_{t=1}^{T}$, smoothing parameter $\mu >0$.
  %\\ Generate independent normal random vector sequence $\{u_t\}_{t = 1}^{T}$
  %\STATE \textbf{Initialize} \; $x_1 \in \mathbb{X}$
   %   \FOR{$t = 1 \rightarrow T$}
    %    \STATE
     %   \begin{equation*}
      %      \Tilde{g}(t) = \frac{f(x_t + \mu u_t) +
 %e(x_t+ \mu u_t) - f(x_t)-e(x_t)}{\mu} .u_t
  %      \end{equation*}
   %    \STATE   Compute
    %   \begin{equation*}
     %   \begin{array}{lcl}
      % x_{t+1} = \argmin\limits_{x \in \mathbb{X}} \{ \inprod{\Tilde{g}(t)}{x} + \frac{1}{\alpha(t)} \mathbb{D}_R(x,x_t) \}
%\end{array}
 %     \end{equation*}
  %    \begin{equation*}
   %       z_t = \frac{\sum\limits_{j=1}^{t}\alpha(j)x_j}{\sum\limits_{k=1}^{t} \alpha(k)}
    %  \end{equation*}
     %  \STATE Return $x_{t+1}$
      % \ENDFOR


  %\end{algorithmic}
  %\caption{Zeroth-Order Mirror Descent Algorithm}
  %\label{algorithm1}
 % \end{algorithm}

%\textcolor{blue}{Note that by using the iterate sequence $\{x_t\}$ we can show only sub-sequential convergence. But by using the  sequence $\{z_t\}$ we can show exact almost sure convergence to the neighbourhood of optimal value.   }

  \begin{assumption}
The step-size $\alpha(t)$ is a decreasing sequence which satisfies  $\sum\limits_{t=1}^{\infty} \alpha(t) = \infty$ and $ \sum\limits_{t=1}^{\infty} \alpha(t)^2 < \infty.$
\label{assumption 2}
\end{assumption}

From Assumption \ref{assumption 2}, we can conclude that $\lim\limits_{t \to \infty} \alpha (t) =0$.
\begin{assumption} Let the following hold.
    \begin{enumerate}
    \item The generating random vectors $u_t \in \mathbb{R}^n (\forall \; t \in \mathbb{N})$  are mutually independent and normally distributed and for each $t \in \mathbb{N}$ $u_t$ is independent of $x_t$.
        \item The random variables $e(x_t,.):\Omega \to \mathbb{R}$ and $e(x_t + \mu u_t, .) : \Omega \to \mathbb{R}$ ( $\forall \; t \in \mathbb{N}$) are mutually independent and identically distributed in the probability space $(\Omega, \mathcal{F}, \mathbb{P})$.
        \item The random variables $e(x_t+ \mu u_t)$ and $e(x_t)$ are independent of $x_t$ and $u_t$.
    \end{enumerate}
    \label{independence}
\end{assumption}
Using Assumption \ref{independence} and \eqref{biased oracle}, we can write
$ \mathbb{E}[e(x_t+\mu u_t) | \sigma\{x_t,u_t\}] = b(x_t + \mu u_t)$ and $\mathbb{E}[e(x_t) | \sigma\{x_t\}] = b(x_t)$,
%\begin{equation}
%    \mathbb{E}[e(x_t+\mu u_t) | \sigma\{x_t,u_t\}] = b(x_t + \mu u_t)
 %   \label{expect1}
 %       \end{equation}
  %      and
   %     \begin{equation}
    %        \mathbb{E}[e(x_t) | \sigma\{x_t\}] = b(x_t)
     %   \label{expect2}
      %  \end{equation}
        where, $\norm{b(x_t + \mu u_t)}_\ast$ and $\norm{b(x_t)}_\ast$ $ \leq B$ a.s.   Similarly, $\mathbb{E}[\norm{e(x_t+ \mu u_t)}^2| \sigma\{x_t, u_t\}] \leq \mathrm{V}^2$ and $\mathbb{E}[\norm{e(x_t)}^2|\sigma\{x_t\}] \leq \mathrm{V}^2$
a.s.
    %    \begin{equation*}
     %       \mathbb{E}[\norm{e(x_t+ \mu u_t)}^2| \sigma\{x_t, u_t\}] \leq \mathrm{V}^2
      %  \end{equation*}
       % and similarly,
        %\begin{equation*}
         %   \mathbb{E}[\norm{e(x_t)}^2|\sigma\{x_t\}] \leq \mathrm{V}^2.
        %\end{equation*}
        For an unbiased oracle $B =0$.
\begin{remark} Note that, most  recent literature on zeroth-order stochastic optimization computes function value  at two separate points $x_t$ and $x_t + \mu u_t$ under the assumption that the stochastic parameters $e(x_t)$ and $e(x_t + \mu u_t)$ are the same. For many applications, this is rather a stringent assumption. In this letter, we avoid such an assumption, which in turn leads to significant deviation in the properties of approximated subgradient and the pertinent properties will be discussed in the ensuing section.
\end{remark}
\section{Main Result}
In this section we discuss the properties of approximated subgradient, almost sure convergence and the finite time analysis.
 Before proceeding further, first define $\mathcal{F}_t = \sigma \{ x_l | 1\leq l \leq t\}$ $\forall \; t \in \mathbb{N}$.  Hence we get a filtration such as $\mathcal{F}_1 \subseteq \mathcal{F}_2 \subseteq \cdots \subseteq \mathcal{F}_t$. Observe that $\Tilde{g}(t-1)$ is $\mathcal{F}_t$ measurable in view of \eqref{DSMD} and also the  Bergman divergence $\mathbb{D}_R(x,x_t)$ ($\forall \; \; x \in \mathbb{X}$) is  $\mathcal{F}_t$ measurable.  Define another filtration as $\{\mathcal{G}_t\}_{t \geq 1}$ such that $\mathcal{G}_{t-1} = \mathcal{F}_t$, which will be helpful in the subsequent analysis.
\subsection{Properties of Approximated Subgradient}
%In this subsection we use techniques from \cite{NestSpok17} to approximate the subgradient of the function.
The analysis in this subsection borrows some steps from \cite{NestSpok17}.
However, our analysis contains significant deviations, most notably, the result concerning the properties of approximated subgradient, which is derived using the noisy information of the  function value.

%The approximation of subgradient of convex function depends on the following classical definition from
%\cite{NestSpok17}.

%\begin{definition}[Smoothed Mapping]
%Consider $f:\mathbb{R}^n \to \mathbb{R}$ be Lipschitz continuous function and $u$ be a normal random vector in $\mathbb{R}^n$ with zero mean and unit variance then the smoothed version of $f$ is defined as follows
%\begin{equation*}
 %   f_{\mu}(x) = \mathbb{E}[f(x+\mu u)] =  \frac{1}{(2\pi)^\frac{n}{2}}\int\limits_{\mathbb{R}^n} f(x+ \mu u)  e^{\frac{-\norm{u}^2}{2}} du.
%\end{equation*}
%\end{definition}
%Note that even if $f$ is Lipschitz continuous (not necessarily differentiable), $f_\mu$ is differentable because of Stein's identity \cite{10.1214/aos/1176345632} and the gradient of $f_{\mu}$ is given as follows:
%\begin{equation*}
 %   \begin{split}
  %      \nabla f_\mu(x)  = \frac{1}{(2\pi)^{\frac{n}{2}} \mu}\int\limits_{\mathbb{R}^n} u \Big{(}f(x+ \mu u) -f(x) \Big{)} e^{\frac{-\norm{u}^2}{2}} du.
   % \end{split}
%\end{equation*}

%In the following Lemma, we discuss the properties (expectation and second moment) of approximated subgradient $\Tilde{g}_t$.
\begin{lemma}
\begin{equation*}
        \mathbb{E}[\Tilde{g}(t)|\mathcal{F}_t] = \nabla f_{\mu} (x_t) + \mathrm{B}(t) \; \; \text{a.s.}
    \end{equation*}
    where, $\mathrm{B}(t)$ is $\mathcal{F}_t$ measurable and satisfies $\norm{\mathrm{B}(t)}_\ast \leq \frac{2 \kappa_1 \mathrm{B}}{\mu} \sqrt{n}$ a.s and we have (a.s.)
 \begin{equation*}
    \begin{split}
        & \mathbb{E}[\norm{\Tilde{g}(t)}_\ast^2|\mathcal{F}_t]    \leq \\ &
        \begin{cases}
            \kappa_1^2 (2 L_0^2 n + 8 \Big{(}\frac{\mathrm{V}}{\mu}\Big{)}^2 n) \; \; \text{if} \; \; f \in \mathcal{C}^{0,0}
            \\ \kappa_1^2 ( \frac{3}{4} L_1 \mu ^2 \kappa_2^4 (n+6)^3 + 3 G^2 (n+4)^2 + 12 \frac{\mathrm{V}^2}{\mu ^2}n) \; \; \text{if} \; \; f \in \mathcal{C}^{1,1} .
        \end{cases}
        \end{split}
    \end{equation*}
\label{expectation}
\end{lemma}

\begin{proof}
Consider the $\sigma$-algebra $\mathcal{H}_t$ defined as $\mathcal{H}_t = \sigma(\{x_k\}_{k=1}^{t},u_t)$. Consider the term
\begin{equation}
    \begin{split}
        & \mathbb{E}[\Tilde{g}(t)| \mathcal{H}_t]
        \\ = & \nabla f_\mu (x_t) +  \mathbb{E}\Big{[}\frac{e(x_t+ \mu u_t) -e(x_t)}{\mu}u_t| \sigma(x_t,u_t) \Big{]} \; \; \text{a.s.}
    \end{split}
    \label{mimsi}
\end{equation}
Note that because of Assumption \ref{independence}$, \mathbb{E}\Big{[}\frac{f(x_t+ \mu u_t) - f(x_t)}{\mu} u_t| \mathcal{H}_t\Big{]}$ $ =$ $ \mathbb{E}\Big{[}\frac{f(x_t+ \mu u_t) - f(x_t)}{\mu} u_t| \sigma(x_t,u_t)\Big{]}$ a.s. and
\begin{equation*}
\begin{split}
& \norm{\mathbb{E}\Big{[}\frac{e(x_t+\mu u_t) - e(x_t)}{\mu} u_t| \sigma (x_t,u_t) \Big{]}} _\ast
\\ \leq & \norm{ \frac{b(x_t + \mu u_t) - b(x_t)}{\mu} }_\ast\norm{u_t}_\ast \leq \frac{2 B}{\mu} \norm{u_t}_\ast \; \; \text{a.s.}
\end{split}
\end{equation*}
Observe that  $\mathcal{F}_t \subseteq \mathcal{H}_t$ and hence by using the Towering property we get $\mathbb{E} [\Tilde{g}(t)| \mathcal{F}_t]
        = \mathbb{E}[\mathbb{E}[\Tilde{g}(t)|\mathcal{H}_t]|\mathcal{F}_t]
         =  \nabla f_{\mu}(x_t) + \mathrm{B}(t)$, Where, $\mathrm{B}(t) = \mathbb{E}\Big{[} \mathbb{E}\Big{[}\frac{e(x_t+ \mu u_t) -e(x_t)}{\mu}u_t| \sigma(x_t,u_t) \Big{]}| \mathcal{F}_t\Big{]}$ satisfies $\norm{\mathrm{B}(t)}_\ast \leq \frac{2\mathrm{B}\kappa_1}{\mu} \mathbb{E}[\norm{u_t}_2|\mathcal{F}_t] \leq \frac{2\mathrm{B}\kappa_1}{\mu} \sqrt{n}$ a.s.
  \\ Consider  the term $\norm{\frac{f(x_t+\mu u_t) + e(x_t+\mu u_t)-f(x_t) - e(x_t)}{\mu}  u_t }_\ast^2$ $\leq$ $2 \kappa_1^2 \norm{ \frac{f(x_t+\mu u_t)-f(x_t)}{\mu} u_t }_2^2 + 2 \kappa_1^2\norm{\frac{e(x_t+\mu u_t) - e(x_t)}{\mu}.u_t}_2^2$. Applying the definition of $\mathcal{C}^{0,0}$, we have
    \begin{equation}
        \begin{split}
            & \norm{\frac{f(x_t+\mu u_t) + e(x_t+\mu u_t)-f(x_t) - e(x_t)}{\mu} . u_t }_\ast^2
            \\ \leq & 2\kappa^2  L_0^2 \norm{u_t}_2^4  + 2 \kappa_1^2 \norm{\frac{e(x_t+\mu u_t) - e(x_t)}{\mu}.u_t}_2^2.
        \end{split}
        \label{vari}
    \end{equation}
    Consider the term
    \begin{equation*}
        \begin{split}
& \mathbb{E}\Big{[}\norm{\frac{e(x_t+\mu u_t) - e(x_t)}{\mu}u_t}_2^2| \mathcal{H}_t \Big{]}
\\ = & \mathbb{E}\Big{[}\norm{\frac{e(x_t+\mu u_t) - e(x_t)}{\mu}u_t}_2^2| \sigma(x_t,u_t) \Big{]}
\\
\leq & \frac{2}{\mu^2} \Big{(} \mathbb{E}[(e(x_t+\mu u_t))^2 \norm{u_t}_2^2 +e(x_t)^2 \norm{u_t}_2^2|\sigma(x_t,u_t)] \Big{)}
\\ \leq & \frac{4\mathrm{V}^2}{\mu^2} \norm{u_t}^2 \; \; \text{a.s.}
        \end{split}
    \end{equation*}
    Hence, by applying Towering property in \eqref{vari}
     we get,
    \begin{equation*}
        \mathbb{E}[\norm{\Tilde{g}(t)}_\ast^2|\mathcal{F}_t] \leq 2 \kappa^2 L_0^2 n + 8 \kappa_1^2 \frac{\mathrm{V}^2}{\mu^2} n \; \; \text{a.s.}
    \end{equation*}
%\begin{lemma} If the objective function $f \in \mathcal{C}^{1,1}$ then the approximated subgradient satisfies the following
%\begin{equation*}
 %   \mathbb{E}[\Tilde{g}(t)|\mathcal{F}_t] = \nabla f_{\mu} (x_t) + \mathrm{B}(t)
%\end{equation*}
%where, $\mathrm{B}(t)$ is $\mathcal{F}_t$ measurable satisfying $\norm{\mathrm{B}(t)} \leq \frac{2\mathrm{B}}{\mu} \sqrt{n}$ a.s.
%\begin{equation*}
 %   \mathbb{E}[\norm{\Tilde{g}(t)}^2|\mathcal{F}_t] \leq \frac{3}{4} L_1 \mu ^2 (n+6)^3 + 3 G^2 (n+4)^2 + 12 \frac{\mathrm{V}^2}{\mu ^2}n.
%\end{equation*}
%\label{expectation1}
%\end{lemma}
%\begin{proof}
For $f \in \mathcal{C}^{1,1}$, consider
    \begin{equation}
        \begin{split}
& \norm{\frac{f(x_t+ \mu u_t) + e(x_t+\mu u_t)-f(x_t) - e(x_t)}{\mu}  u_t }_\ast^2
\\ \leq & 3 \kappa_1^2 \norm{\frac{f(x_t+\mu u_t) -f(x_t) - \mu \inprod{\nabla f (x_t)}{u_t}}{\mu} u_t}_2^2
%\\ \leq & 3 \kappa_1^2 \Big{(} \frac{L_1^2 \mu ^2 \kappa_2^4}{4}   \norm{u_t}_2^6 +   G^2 \norm{u_t}_2^4 +   \norm{\frac{e(x_t+\mu u_t) - e(x_t)}{\mu}.u_t}_2^2 \Big{)} .
        \end{split}
        \label{sml}
    \end{equation}
    \begin{equation*}
        \begin{split}
            & + 3 \kappa_1^2 \norm{\nabla f(x_t)}_2^2\norm{u_t}_2^4 + 3 \kappa_1^2 \norm{\frac{e(x_t+\mu u_t) - e(x_t)}{\mu}u_t}_2^2
        \end{split}
    \end{equation*}
Note that $\norm{\frac{f(x_t+\mu u_t) -f(x_t) - \mu \inprod{\nabla f (x_t)}{u_t}}{\mu} u_t}_2^2 \leq   \frac{L_1^2 \mu ^2 \kappa_2^4}{4}   \norm{u_t}_2^6$ because of the definition of $\mathcal{C}^{1,1}$.  Taking conditional expectation on \eqref{sml} we get the result.
\end{proof}
Using the  similar procedure we can extend the analysis  for  $f \in \mathcal{C}^{2,2}$ and so on.  It is important to note that because of consideration of more generic framework  $\mathbb{E}[\norm{\Tilde{g}(t)}_\ast^2]=  \mathcal{O}(\frac{1}{\mu^2})$ for small values of $\mu$, as opposed to  \cite{NestSpok17} because of consideration of more general framework. This result plays a significant role in the subsequent discussion of this letter.

%Due to this reason, the bias term in the approximated subgradient cannot be made arbitrarily small. This fact differs the anlysis in this letter with almost sure convergence of zeroth-order optimization with biased subgradient.
%It is important to note that $\mathbb{E}[\norm{\Tilde{g}(t)}_\ast^2| \mathcal{F}_t] = \mathcal{O}(\frac{1}{\mu^2})$ because of the oracles that provide only noisy function value, which makes the analysis different from recent literature which shows almost sure convergence of stochastic mirror descent algorithm.
\begin{Corollary}
    For unbiased oracle, $\mathbb{E}[\Tilde{g}_t|\mathcal{F}_t]$ $ =$ $ \nabla f_{\mu}(x_t)$ a.s.
\end{Corollary}
%\begin{proof}
 %   The proof is straightforward and follows from the proof of Lemma \ref{expectation}, wherein the second term of the RHS of \eqref{mimsi} is zero.
%\end{proof}
%The following reason motivates us to focus on zeroth order mirror descent algorithm. An extensive discussion on random gradient of an objective function can be found in \cite{NestSpok17}. In this letter we assume that we have a black box model and for any given $x\in \mathbb{X}$, we can only get the value of the function $f(x)$ either purely or some noisy measurement in the form of $F(x,\nu)$.%\subsection{Distributed Optimization}%In this subsection the objective function $f$ in \eqref{CP1} is of the following form $f(x) = \sum\limits_{i=1}^{N} f_i(x)$.
%The main challenge  in distributed optimization problem is that  here $N$ agents solving the problem, and each agent $i$ has the access to the value of  local objective function $f_i$.  Each agent is allowed to communicate their local iterates with their neighboring agents through an undirected inter-agent communication network graph.
%Each agent $i$ can only access the value of local objective function $f_i(x)$. In order to apply distributed random gradient free mirror descent algorithm, each agents need to estimate the gradient at every iterates of the algorithm. We discuss the estimation of gradient and  algorithmic steps for both centralized and distributed optimization in details in the next section.
\subsection{Almost Sure Convergence of the ZOMD Algorithm}
%\begin{lemma}
%\begin{equation*}
 %   \mathbb{E}[\norm{g_x}_{\ast}^2|x] = \mathcal{O} (\frac{1}{\eta^2})
%\end{equation*}
%\end{lemma}
%\begin{proof}
%\begin{equation*}
 %   \begin{split}
  %      & \mathbb{E}[\norm{g_x}_\ast^2|x]
   %     \\ = & \mathbb{E}[\norm{g_x- g(x)+g(x)}_{\ast}^2|x]
    %    \\ \leq & 2 \Big{(} \mathbb{E}[\norm{g_x-g(x)}^2|x]+ \norm{g(x)}_{\ast}^2 \Big{)}
     %   \\ = & \mathcal{O}(\frac{1}{\eta^2})
    %\end{split}
%\end{equation*}
%\end{proof}
%Let us suppose $\{\mathcal{F}_t\}$ be the sequence of $\sigma$ algebras generated by the sequence of iterates $\{x_t\}$, then we can say that $\exists \; \; \mathcal{K} > 0$ such that
%\begin{equation*}
 %   \begin{split}
  %      \mathbb{E}[g_t|\mathcal{F}_t] & = \ g(t) + b_t
 %       \\ %\mathbb{E}[\norm{g_t}_\ast^2|\mathcal{F}_t] & \leq \frac{K}{\eta^2}
    %\end{split}
%\end{equation*}
%We can think $b_t$ as the bias of the estimated gradient. From our analysis we can say that $b_t = \mathcal{O}(\eta^2)$. We can also write estimated subgradient as the following:
%\begin{equation*}
 %   g_t = g(t) + \epsilon(t) + b_t
%\end{equation*}
%Clearly, $\epsilon (t)$ is the stochastic error of the subgradient i.e. $\mathbb{E}[\epsilon(t)|\mathcal{F}_t] = 0$. The following result due to Neveu (\textcolor{red}{A Convergence Theorem For Almost Supermartingales And Some Applications}) will play a key role in our analysis.
%\begin{lemma}
%For the  sequence of random vectors $\{\epsilon(t)\}_{t \geq 1}$  a , any  sequence of random variable $\{y_t\}_{t \geq 1}$ such that $y_t \in \mathbb{X}$ is $\mathcal{F}_t$ measurable  and $\forall \; y \in \mathbb{X}$, we get the following that
%\begin{equation*}
 %   \frac{\sum\limits_{t \geq t_0} \alpha(t) \inprod{\epsilon(t)}{y - y_t}}{\sum\limits_{t \geq t_0}\alpha(t)} = 0 \; \; \text{a.s.} \; \; \; \forall \; \;  t_0 \in \mathbb{N}.
%\end{equation*}
%\label{Lemma 1}
%\end{lemma}
%\begin{proof}
%To prove this Lemma we use apply Lemma \ref{SLNN} where, we consider $X(t)$ in Lemma \ref{SLNN} as $X(t) = \alpha(t) \inprod{\epsilon(t)}{y -y_t} $ and $\beta(t) = \sum\limits_{j=t_0}^{t} \alpha(j)$.
 %From the definition of $\epsilon (t)$, since $y_t$ is $\mathcal{F}_t$ measurable we have $\mathbb{E}[ X(t) | \mathcal{F}_{t}]= 0$.
%also assume that the constraint set $\mathbb{X}$ is compact and hence bounded so the diameter of the constraint set is $D$, by applying generalized Cauchy-Schwartz inequality we have $\mathbb{E}[|X(t)|^2|\mathcal{F}_t] \leq K \alpha(t)^2 D^2$. Thus,
%\begin{equation}
%\begin{split}
%& \mathbb{E}[|X(t)|^2|\mathcal{F}_t]
 %   \\ = &  \mathbb{E}[ |\alpha(t) \inprod{\epsilon(t)}{y -y_t}|^2 \mathcal{F}_{t}] %  \\ \leq & \mathbb{E}[|\alpha(t)^2\norm{\epsilon(t)}_{\ast}^2 \norm{y -y_t}^2| \mathcal{F}_{t}]
   % \\ \leq & \alpha(t)^2D^2 \mathbb{E}[\norm{\epsilon(t)}_{\ast}^2|\mathcal{F}_{t}]
    %\\ \leq &  K %\alpha(t)^2 D^2
%\end{split}
%\label{rrr}
%\end{equation}
%The second inequality is due to the fact that the constraint  set $\mathbb{X}$ is compact and hence, bounded and the diameter of the constraint set is $D$. Thus,
%\begin{equation*}
 %   \begin{split}
  %       & \sum\limits_{t \geq t_0} \frac{1}{(\sum\limits_{j = t_0}^{t} \alpha(j))^2}  \mathbb{E}[ |X(t)|^2 | \mathcal{F}_{t}]
   %      < \infty .
   % \end{split}
%\end{equation*}
%By applying  Lemma \ref{SLNN}, we get the result.
%\end{proof}
Based on the discussion in Lemma \ref{expectation},  we redefine properties of    biased subgradient as follows
\begin{equation}
      \Tilde{g}(t) = g_{\delta}(t) + \mathrm{B} (t) +\zeta (t)
        \label{bsubg}
\end{equation}
 where, $g_{\delta} (t) \in \partial_\delta f(x)$ at $x = x_t$ and  $\mathrm{B}(t)$ is $\mathcal{F}_t$  measurable and $\norm{\mathrm{B}(t)}_{\ast} \leq  B_1 $ a.s. Moreover,  $\mathbb{E}[\zeta(t)|\mathcal{F}_t] = 0$ and $ \mathbb{E}[\norm{\Tilde{g}(t)}_\ast ^2 | \mathcal{F}_t] \leq \mathrm{K}$ a.s.
    %\begin{equation*}
     %   \mathbb{E}[\norm{\Tilde{g}(t)}_\ast ^2 | \mathcal{F}_t] \leq \mathrm{K}.
    %\end{equation*}
Note that we can get an expression of $\delta$, $B_1$ and $K$ from Lemma \ref{expectation}  depending on the properties of the noise and the smoothness of $f$.
\begin{theorem}
Under Assumptions \ref{assumption 2} and \ref{independence} and $\forall \; \epsilon >0$, for the iterate sequence generated by ZOMD algorithm $\{x_t\}$, there exists a subsequence $  \{x_{t_k}\}$ such that $f(x_{t_k}) - f^\ast \leq \delta + B_1D + \epsilon$ a.s.
\\ For the iterate sequence $\{z_t\}$,  $\exists \; t_0 \in \mathbb{N}$ such that  $\forall \; t \geq t_0$ we have $f(z_t)-f^\ast \leq \delta + B_1D + \epsilon$ a.s.
\label{mainths}
\end{theorem}
Before proving the Theorem \ref{mainths}, we need the following three Lemmas which we discuss here.
\begin{lemma} $\sum\limits_{t \geq 1} \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2 < \infty. \; \; \text{a.s.}$
%\begin{equation*}
 %   \sum\limits_{t \geq 1} \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2 < \infty. \; \; \text{a.s.}
%\end{equation*}
\label{lemma 6}
\end{lemma}
\begin{proof} $\lim\limits_{t \to \infty} \mathbb{E}\Big{[}\sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_{\ast}^2 \Big{]} \leq \sum\limits_{t \geq 1} \frac{\alpha(t)^2}{2 \sigma_R} \mathrm{K} < \infty$
   % \begin{equation*}
    %    \begin{split}
     %       & \lim\limits_{t \to \infty} \mathbb{E}\Big{[}\sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_{\ast}^2 \Big{]} \leq \sum\limits_{t \geq 1} \frac{\alpha(t)^2}{2 \sigma_R} \mathrm{K} < \infty.
      %  \end{split}
    %\end{equation*}
    By applying  Fatou's Lemma we get
    \begin{equation*}
    \mathbb{E}[\liminf\limits_{t \to \inf} \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2\sigma_R} \norm{\Tilde{g}(k)}_\ast^2] \leq \liminf\limits_{t \to \infty} \mathbb{E}[ \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2\sigma_R} \norm{\Tilde{g}(k)}_\ast^2]
\end{equation*}
$< \infty$.  Hence we can say $\sum\limits_{t \geq 1} \frac{\alpha(t)^2}{2\sigma_R} \norm{\Tilde{g}(t)}_\ast^2 < \infty $ a.s.
\end{proof}

\begin{lemma}
$\exists \; C >0$ such that $\mathbb{E}[\norm{\zeta(t)}_{\ast}^2| \mathcal{F}_t] < C \; \; \text{a.s.}$
%\begin{equation*}
 %   \mathbb{E}[\norm{\zeta(t)}_{\ast}^2| \mathcal{F}_t] < C \; \; \text{a.s.}
%\end{equation*}
\label{zeta}
\end{lemma}
\begin{proof} From the definition of $\zeta(t)$ we get that
    \begin{equation}
        \begin{split}
 \norm{\zeta(t)}_\ast^2 &  \leq 3 \kappa_1^2( \norm{\Tilde{g}(t)}_2^2 +  \norm{\mathrm{B}(t)}_2^2 +  \norm{g_{\delta}(t)}_2^2).
    \end{split}
    \label{4}
    \end{equation}
    Notice that $\exists \; K_1 >0$ such that $\norm{g_{\delta}(t)} \leq K_1$ $\forall \; t$ because of compactness of  $\mathbb{X}$.  Taking expectation on both sides of \eqref{4}, we get  (a.s.) $\mathbb{E}[\norm{\zeta(t)}_{\ast}^2|\mathcal{F}_t] \leq  3 \kappa_1^2 ( \mathrm{K} +  B_1^2 +  K_1) \delequal C. $
\end{proof}

\begin{lemma}
    %$\frac{\sum\limits_{t \geq 1}\alpha(t) \inprod{\zeta(t)}{x - x_t}}{\sum\limits_{t \geq 1} \alpha(t)} = 0 \; \; \text{a.s.}$
    \begin{equation*}
        \frac{\sum\limits_{t \geq 1}\alpha(t) \inprod{\zeta(t)}{x - x_t}}{\sum\limits_{t \geq 1} \alpha(t)} = 0 \; \; \text{a.s.} \; \; \forall \; \; x \in \mathbb{X}.
    \end{equation*}
    \label{SLLN}
\end{lemma}
\begin{proof}
    Define $X(t) = \sum\limits_{k=1}^{t} \alpha(k) \inprod{\zeta(k)}{x - x_{k}}$. In the light   of  definition of $\zeta(t)$ and since $X(t)$ is $\mathcal{F}_t$ measurable we get that $\mathbb{E}[X(t)|\mathcal{F}_t] = X(t-1)$.
    %\begin{equation*}
     %   \mathbb{E}[X(t)|\mathcal{F}_t] = X(t-1).
    %\end{equation*}
    Hence $\{X(t), \mathcal{G}_t\}$ is a martingale. On the other hand, it can be seen that  (a.s.)
    \begin{equation*}
        \begin{split}
            & \sum\limits_{t \geq 1} \frac{\mathbb{E}[\norm{X(t)-X(t-1)}^2|\mathcal{F}_t]}{(\sum\limits_{k=1}^{t} \alpha(k))^2}   \leq  \\&\sum\limits_{t \geq 1} \frac{\mathbb{E}[\alpha(t)^2 \norm{\zeta(t)}_\ast^2\norm{x-x_t}^2|\mathcal{F}_t]}{(\sum\limits_{k=1}^{t} \alpha(k))^2}
             \leq \sum\limits_{t \geq 1} \frac{\alpha(t)^2 D^2C}{(\sum\limits_{k=1}^{t} \alpha(t))^2}<\infty.
        \end{split}
    \end{equation*}
    The last line is because of Lemma \ref{zeta} and the diameter of the compact set $\mathbb{X}$. Hence by applying Lemma \ref{SLNN1},  the result follows.
\end{proof}
Now we are in a position to prove the main result.

\begin{proof}
The application of first-order optimality condition to  \eqref{DSMD} yields
\begin{equation*}
\begin{split}
     & \alpha(t) \inprod{\Tilde{g}(t)}{x - x_{t+1}}
          \geq   -  \inprod{\nabla R (x_{t+1})- \nabla R (x_t)}{x-x_{t+1}}
          \end{split}
\end{equation*}
\begin{equation}
    \begin{split}
         & \geq \mathbb{D}_R(x_{t+1},x_t)+ \mathbb{D}_R(x,x_{t+1})-\mathbb{D}_R(x,x_t).
    \end{split}
    \label{firstor}
\end{equation}
The last inequality in \eqref{firstor} is due to \eqref{mirrordescent implication2}. From the LHS of \eqref{firstor}, we obtain
\begin{equation*}
    \begin{split}
        &\alpha(t) \inprod{\Tilde{g}(t)}{x - x_{t+1}} = \alpha(t) \inprod{\Tilde{g}(t)}{x-x_t+x_t-x_{t+1}}
        \\ \leq & \alpha(t) \inprod{\Tilde{g}(t)}{x-x_t} +  \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2 + \frac{\sigma_R}{2} \norm{x_t-x_{t+1}}^2.
    \end{split}
\end{equation*}
The last inequality  follows by applying the Young-Fenchel inequality to the term $\alpha(t)  \inprod{\Tilde{g}(t)}{x_t - x_{t+1}}$. Hence from \eqref{firstor}, we get that
\begin{equation}
    \begin{split}
        & \mathbb{D}_R(x,x_{t+1})
        \\ \leq & \mathbb{D}_R(x,x_t) + \alpha(t) \inprod{\Tilde{g}(t)}{x-x_t} +  \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2.
    \end{split}
    \label{Ber}
\end{equation}
Notice that $\mathbb{D}_R(x_{t+1},x_t) \geq \frac{\sigma_R}{2} \norm{x_{t+1}-x_t}^2$. Consider the term
\begin{equation*}
    \begin{split}
         & \alpha(t) \inprod{\Tilde{g}(t)}{x-x_t}
         =  \alpha (t) \inprod{g_{\delta} (x_t) +\mathrm{B}(t)+\zeta(t)}{x - x_t}
    \end{split}
\end{equation*}
\begin{equation}
    \begin{split}
         & \leq  \alpha(t) (f(x)-f(x_t)+ \delta + B_1D +\inprod{\zeta(t)}{x-x_t}).
    \end{split}
    \label{sps}
\end{equation}
%Hence from \eqref{firstor}, we get that
%\begin{equation}
 %   \begin{split}
  %      & \mathbb{D}_R(x,x_{t+1})
   %     \\ \leq & \mathbb{D}_R(x,x_t) + \alpha(t) \inprod{\Tilde{g}(t)}{x-x_t} +  \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2.
    %\end{split}
    %\label{Ber}
%\end{equation}
%Observe that $\mathbb{D}_R(x_{t+1},x_t) \geq \frac{\sigma_R}{2} \norm{x_{t+1}-x_t}^2$, letting  $x = x^\ast$ in \eqref{Ber} and considering only the term $\alpha(t) \inprod{\Tilde{g}(t)}{x^\ast-x_t}$, we have
The last inequality in \eqref{sps} is because of $\delta$-subgradient of  function $f$  and the generalized Cauchy-Schwartz inequality. Plugging  \eqref{sps}  into \eqref{Ber}
 and on applying telescopic sum  from $k = 1$ to $t$  we get,
%Hence from \eqref{rumdi} we get that
%\begin{equation*}
 %   \alpha(t) \inprod{g_t}{x^\ast-x_t}
  %        \leq  \alpha(t) (-a + \inprod{\epsilon(t)}{x^\ast -x_t})
%\end{equation*}
%So, from \eqref{Ber}, we get
%\begin{equation*}
 %   \begin{split}
  %      &\mathbb{D}_R(x^\ast,x_{t+1})
   %     \\ \leq & \mathbb{D}_R(x^\ast,x_t) + \frac{\alpha(t)^2}{2 \sigma_R}\norm{\Tilde{g}(t)}_\ast^2 + \alpha (t)  (-a + \inprod{\epsilon(t)}{x^\ast -x_t})
%    \end{split}
%\end{equation*}
%\begin{equation}
 %   \begin{split}
  %      & \mathbb{D}_R (x^\ast,x_{t+1}) \leq \mathbb{D}_R(x^\ast, x_1) +  \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2
      %  \\ \leq & \mathbb{D}_R(x^\ast, x_1) +  \sum\limits_{k=1}^{t}\alpha(k) \Big{(}f^\ast-f(x_k) + \delta + B_1D \\ & + \inprod{\zeta(k)}{x^\ast-x_k}\Big{)}
       %  +  \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2
   %      \\  &  +  \sum\limits_{k=1}^{t}\alpha(k) \Big{(}f^\ast-f(z_t) + \delta + B_1D  + \inprod{\zeta(k)}{x^\ast-x_k}\Big{)} .
        %\\ \leq & \mathbb{D}_R(x^\ast,x_1) + \sum\limits_{k=1}^{t} \alpha(k) \Big{(}f^\ast-f(z_t)+ c_1\eta_1^2D  \\ & + \inprod{\epsilon(k)}{x^\ast-x_k}\Big{)}
        %+ \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2.
        %\\ \leq & \mathbb{D}_R(x^\ast,x_1) + \sum\limits_{k=1}^{t} \alpha(k) (-a + \inprod{\epsilon(k)}{x^\ast-x_k})
         %+ \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2.
    %\end{split}
    %\label{rumsi}
%\end{equation}
\begin{equation}
    \begin{split}
        & \mathbb{D}_R (x^\ast,x_{t+1})
     \leq  \mathbb{D}_R(x^\ast, x_1) +  \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2
         %\\ \leq & \mathbb{D}_R(x^\ast,x_1) + \sum\limits_{k=1}^{t} \alpha(k) \Big{(}f^\ast-f(z_t)+ c_1\eta_1^2D  \\ & + \inprod{\epsilon(k)}{x^\ast-x_k}\Big{)}
        %+ \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2.
        %\\ \leq & \mathbb{D}_R(x^\ast,x_1) + \sum\limits_{k=1}^{t} \alpha(k) (-a + \inprod{\epsilon(k)}{x^\ast-x_k})
         %+ \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2.
    \end{split}
    \label{rumsi}
\end{equation}
\begin{equation*}
    \begin{split}
         &  +  \sum\limits_{k=1}^{t}\alpha(k) \Big{(}f^\ast-f(x_k) + \delta + B_1D  + \inprod{\zeta(k)}{x^\ast-x_k}\Big{)}.
    \end{split}
\end{equation*}
%From the definition of convexity of  $f$ we get that  $\sum\limits_{k=1}^{t}\alpha(k) f (z_t) \leq \sum\limits_{j=1}^{t} \alpha(j) f(x_j)$.
%From the definition of convexity,  we get
 %   \begin{equation}
  %  \begin{split}
   %     & \mathbb{D}_R (x^\ast,x_{t+1})
    %    \\ \leq & \mathbb{D}_R(x^\ast, x_1) +  \sum\limits_{k=1}^{t}\alpha(k) \Big{(}f^\ast-f(z_t) + \mu L_f \sqrt{n} + \mathrm{B}D \\ & + \inprod{\zeta(k)}{x^\ast-x_k}\Big{)}
     %    +  \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2
        %\\ \leq & \mathbb{D}_R(x^\ast,x_1) + \sum\limits_{k=1}^{t} \alpha(k) \Big{(}f^\ast-f(z_t)+ c_1\eta_1^2D  \\ & + \inprod{\epsilon(k)}{x^\ast-x_k}\Big{)}
        %+ \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2.
        %\\ \leq & \mathbb{D}_R(x^\ast,x_1) + \sum\limits_{k=1}^{t} \alpha(k) (-a + \inprod{\epsilon(k)}{x^\ast-x_k})
         %+ \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2.
   % \end{split}
    %\label{rumsi1}
%\end{equation}
Let $\epsilon > 0$ and define the sequence of  stopping times $\{T_p\}_{p \geq 1}$ and $\{T^p\}_{p \geq 1}$ as follows:
\begin{equation*}
    \begin{split}
        T_1 & =  \inf\{f(x_t)-f^\ast \geq  \delta  + B_1D + \epsilon\}
        \\   T^1 &= \inf\{ t \geq T_1 | f(x_t) - f^\ast < \delta + B_1D + \epsilon \}
        \\ & \vdotswithin{=}
        \\  T^p &  = \inf \{ t \geq T_p | f(x_t) -f^\ast <  \delta + B_1D + \epsilon\}
        \\  T_{p+1} & = \inf\{ t \geq T^p | f(x_t)-f^\ast \geq  \delta + B_1D + \epsilon\ \}.
    \end{split}
\end{equation*}
%\begin{equation*}
 %   \begin{split}
  %      & T_1 =  \inf\{f(x_t)-f^\ast \geq  \delta  + B_1D + \epsilon\}
   %     \\ &  T^1 = \inf\{ t \geq T_1 | f(x_t) - f^\ast < \delta + B_1D + \epsilon \}
    %    \\ & \vdots
     %    \\ & T_{p} = \inf\{ t \geq T^{p-1} | f(x_t)-f^\ast \geq  \delta + B_1D + \epsilon\ \}
      %  \\ & T^p  = \inf \{ t \geq T_p | f(x_t) -f^\ast <  \delta + B_1D + \epsilon\}
   % \end{split}
%\end{equation*}
If $\exists \; p \in \mathbb{N}$ such that infimum does not exist, we assume that $T_p = \infty$ or $T^{p} = \infty$ .

Claim-$1$ - If $T_p < \infty$, then $T^p < \infty$ a.s. $\forall \; \; p \in \mathbb{N}$.
\\ Suppose, ad absurdum,  $\exists \; p_0 \in \mathbb{N}$ such that $T_{p_0} < \infty$ but $T^{p_0} = \infty$ with probability (w.p.) $\eta$.  Let $T_{p_0} = t_{0}$, then it  implies that $\forall \; t \geq t_{0}$, $f(x_t)-f^\ast \geq \delta + B_1D + \epsilon$ w.p. $\eta$.  From \eqref{rumsi}, we deduce that $\forall \; t \geq t_0$ (w.p. $\eta$)
\begin{equation}
    \begin{split}
        & \mathbb{D}_R (x^\ast,x_{t+1}) \leq
\mathbb{D}_R(x^\ast, x_{t_0}) +      \sum\limits_{k=t_0}^{t}\alpha(k) \Big{(}-\epsilon   \\ & + \inprod{\zeta(k)}{x^\ast-x_k}\Big{)}  +  \sum\limits_{k=t_{t_0}}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2.
         \end{split}
         \label{eqn 12}
         \end{equation}
Let $t \to \infty$. Notice that in view of Lemma \ref{SLLN}, $\sum\limits_{k \geq t_{0}} \alpha(k) (- \epsilon + \inprod{\zeta(k)}{x^\ast - x_k}) = -\infty $
%\begin{equation*}
%\begin{split}
 %    \sum\limits_{k \geq t_{0}} \alpha(k) (- \epsilon + \inprod{\zeta(k)}{x^\ast - x_k}) \leq -\infty
  %  \end{split}
%\end{equation*}
 and also in view of Lemma \ref{lemma 6} $\sum\limits_{k \geq t_0} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2 < \infty$ a.s.   Hence, from \eqref{eqn 12} we get
$\limsup\limits_{t \to \infty} \mathbb{D}_R(x^\ast,x_t) = - \infty$ w.p. atleast $\eta$, which implies $\eta = 0$.  Thus, $T^{p_0} < \infty$ a.s. This establishes Claim-$1$. Hence $\exists \; \{x_{t_k}\} \subseteq \{x_t\}$ such that $f(x(t_k))- f^\ast \leq \delta + B_1D + \epsilon$ a.s.

From the definition of convexity of  $f$ we get that  $\sum\limits_{k=1}^{t}\alpha(k) f (z_t) \leq \sum\limits_{j=1}^{t} \alpha(j) f(x_j)$. Hence, from \eqref{rumsi} we get that
 \begin{equation}
    \begin{split}
        & \mathbb{D}_R (x^\ast,x_{t+1})
         \leq \mathbb{D}_R(x^\ast, x_1) +    \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2
          \label{rumsi1}
    \end{split}
\end{equation}
\begin{equation*}
\begin{split}
     & +\sum\limits_{k=1}^{t}\alpha(k)  \Big{(}f^\ast-f(z_t) + \delta + B_1D  + \inprod{\zeta(k)}{x^\ast-x_k}\Big{)}.
    \end{split}
\end{equation*}
In a similar fashion, define the sequence of stopping times $\{\Bar{T}_p\}_{p \geq 1}$ and $\{\Bar{T}^p\}_{p \geq 1}$ as follows:
\begin{equation*}
    \begin{split}
        & \Bar{T}_1 =  \inf\{f(z_t)-f^\ast \geq  \delta  + B_1D + \epsilon\}
        \\ &  \Bar{T}^1 = \inf\{ t \geq \Bar{T}_1 | f(z_t) - f^\ast < \delta + B_1D + \epsilon \}
        \\ & \vdotswithin{=}
        \\ & \Bar{T}^p  = \inf \{ t \geq T_p | f(z_t) -f^\ast <  \delta + B_1D + \epsilon\}
        \\ & \Bar{T}_{p+1} = \inf\{ t \geq T^p | f(z_t)-f^\ast \geq  \delta + B_1D + \epsilon\ \}.
    \end{split}
\end{equation*}
%Choose $\epsilon > 0$ and define the sequence of stopping times $\{\Bar{T}_n\}_{n \geq 1}$ and $\{\Bar{T}^n\}_{n \geq 1}$ as follows:
%If $\Bar{T}_p < \infty$ then $\Bar{T}^p < \infty$. The reason is similar to the last one.
If $\Bar{T}_p < \infty$ then $\Bar{T}^p < \infty$ a.s. The reason is similar to the proof of Claim-$1$.

Claim- $2$: $\exists \; p_0 \in \mathbb{N}$ such that $\Bar{T}_{p_0} = \infty$ a.s.  If this claim is true, it proves the second part of the Theorem.

Otherwise, $\forall \;  t_1 \in \mathbb{N}$,  $\exists \; t > t_1$ such that  $(f(z_t) - f^\ast) \geq \delta + B_1D + \epsilon$ with some probability $\eta$.  Hence, from \eqref{rumsi1} we get that  \eqref{eqn 12} holds for that $t$.
%\begin{equation}
 %   \begin{split}
  %      & \mathbb{D}_R (x^\ast,x_{t+1}) \leq
%\mathbb{D}_R(x^\ast, x_{t_0}) \sum\limits_{k=t_0}^{t}\alpha(k) \Big{(}-\epsilon   \\ & + \inprod{\epsilon(k)}{x^\ast-x_k}\Big{)}  +  \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}_\ast^2
 %        \end{split}
  %       \end{equation}
         Letting $t \to \infty$ and using similar arguments  we get
 $\liminf\limits_{t \to \infty} \mathbb{D}_R(x^\ast,x_t) = -\infty$ w.p. atleast $\eta$, that means $\eta = 0$.
Hence, the Claim-$2$ holds.
\end{proof}

%In Theorem \ref{mainths} and \ref{main} , we have shown that the function value of the iterate sequence of Algorithm \ref{algorithm1}  with biased subgradient satisfying \eqref{bsubg} converges to the neighbourhood of the optimal value almost surely. In the following corollary, we discuss the algorithm's performance for several kinds of functions in further detail.
\begin{Corollary}[ZOMD with unbiased oracle]
     For all  $\epsilon > 0$, $\exists \; t_0 \in \mathbb{N}$ such that $\forall \; t \geq t_0$
    \begin{equation*}
        f(z_t) - f^\ast \leq
        \begin{cases}
            \mu L_0 \sqrt{n} + \epsilon \; \; \text{if} \; f \in \mathcal{C}^{0,0}
            \\ \frac{\mu ^2}{2} L_1 n + \epsilon \; \; \;  \text{if} \; f \in \mathcal{C}^{1,1} .      \end{cases}
            \; \; \text{a.s.}
    \end{equation*}
    \label{corro}
\end{Corollary}
Corollary \ref{corro} shows that by selecting a very small $\mu$, the  function value of iterate sequence converges to a small neighbourhood of the optimal value.  Notice that, $\mathbb{E}[\norm{\Tilde{g}(t)}_\ast^2] = \mathcal{O}(\frac{1}{\mu^2})$ for small value of $\mu$, hence we cannot make $\mu$ arbitrarily small. However, an analytic presentation on how small $\mu$ influences the algorithm's performance will be discussed in the ensuing section.
\begin{Corollary}[ZOMD with biased oracle]
    For all  $\epsilon > 0$ $\exists \; t_0 \in \mathbb{N}$ such that $\forall \; t \geq t_0$ the following holds
    \begin{equation*}
        f(z_t) - f^\ast \leq
        \begin{cases}
            \mu L_0 \sqrt{n} +   \frac{2 \kappa_1 B}{\mu} \sqrt{n} D+ \epsilon \; \; \text{if} \; f \in \mathcal{C}^{0,0}
            \\ \frac{\mu ^2}{2} L_1 n +  \frac{2\kappa_1 B}{\mu} \sqrt{n}D + \epsilon \; \; \;  \text{if} \; f \in \mathcal{C}^{1,1}.     \end{cases}
            \; \; \text{a.s.}
    \end{equation*}
    \label{bo}
\end{Corollary}
As Corollary \ref{bo} shows, we can not make $\mu$ very small for biased oracle. Nonetheless, an optimal $\mu^\ast$ can be calculated using Corollary \ref{bo} to show almost sure convergence to an optimal neighbourhood around the optimal value.
%The next Corollary discusses the optimal choice of $\mu$ and the corresponding neighbourhood.
 %   \begin{Corollary}
  %  From Corollary \ref{bo}, the optimal $\mu$ for which we guarantee the best possible neighbourhood around the optimal value is $\mu = \sqrt{\frac{2\kappa_3 B D}{L_0}}$  and the corresponding result is $\exists \; t_0 \in \mathbb{N}$ such that $\forall \; t \geq t_0$  we have
   % \begin{equation*}
    %    f(z_t) - f^\ast \leq 2\sqrt{2}\kappa_3 B D L_0 + \epsilon.
    %\end{equation*}
    %\end{Corollary}
%A similar result, we can extend if $f \in \mathcal{C}^{1,1}$.
%In this section the main focus point is to show that the function value of iterate sequence converges to a neighbourhood of the optimal value. This neighbourhood is dependent on $\mu$ and particularly for unbiased oracle we can make the neighbourhood very close to the optimal value by choosing a very small $\mu$.
\subsection{Concentration Bound - Finite Time Analysis}
In the next Theorem,  we will show that a very small $\mu$ actually deteriorates the convergence rate of the ZOMD algorithm.

%Before proceeding the discussion in this section, first define the following Assumption regarding sub-Gaussian random variable (\textcolor{red}{reference?}) in the context of biased subgradient as defined in \eqref{bsubg}.
%\begin{assumption}
 %  The random vector $\zeta(t)$  as defined in \eqref{bsubg} is a sub-Gaussian random vector with parameter $c > 0$ that is, $\mathbb{E}[\exp{(\lambda \norm{\zeta(t)}_{\ast} )}|\mathcal{F}_t] \leq \exp{(\frac{c^2\lambda^2}{2})}$ $\forall \; \lambda \in \mathbb{R}$.
  % \label{subgaussian}
%\end{assumption}
%It is straightforward from Assumption \ref{subgaussian} that $\mathbb{E}[\exp{(\norm{\Tilde{g}(t)}_\ast)}|\mathcal{F}_t]$ $\leq$ $\exp{(\lambda K_1 +\lambda B_1 + \frac{\lambda^2 c^2}{2}})$.

%Define $K_2 = (\lambda K_1 +\lambda B_1 + \frac{\lambda^2 c^2}{2}) $. Note that Assumption \ref{subgaussian} is stricter than \eqref{bsubg}, this is not uncommon in literature. In particular if we assume that $\epsilon(x_t+ \mu u_t)$ and $\epsilon(x_t)$ are both mutually independent and Sub-Gaussian random variable then Assumption \ref{subgaussian} holds. The details of the proof is in the similar line with Lemma \ref{expectation1}. We skip the details because of page constraints. We refer \textcolor{red}{reference} for more details on Sub-Gaussian random variable.

\begin{theorem}
Consider any  $t_0 \in \mathbb{N}$ such that $\sum\limits_{k=1}^{t_0} \alpha(k) \geq \frac{3}{\epsilon}D$. Then $\forall \; t \geq t_0$ the following holds.
    \begin{equation}
    \begin{split}
         &\mathbb{P}(f(z_t)-f^\ast \geq \delta + B_1D + \epsilon)
         \\ \leq & \frac{3\mathrm{K}}{\epsilon} \frac{\sum\limits_{k=1}^{t} \alpha(k)^2}{\sum\limits_{k=1}^{t} \alpha(k)} + \frac{9CD}{\epsilon^2} \frac{\sum\limits_{k=1}^{t} \alpha(k)^2}{(\sum\limits_{k=1}^{t} \alpha(k))^2}.
    \end{split}
    \label{theorem3}
\end{equation}
\label{concentration}
\end{theorem}

\begin{proof}
    Using the first-order optimality condition as in the proof of Theorem \ref{mainths}, we get,
    \begin{equation}
        \begin{split}
            & f(z_t)-f^\ast \leq \delta + B_1D + \frac{\mathbb{D}_R(x^\ast,x_1)}{\sum\limits_{k=1}^{t}\alpha(k)}
            \\  &
  + \frac{\sum\limits_{k=1}^{t}\alpha(k)\inprod{\zeta(k)}{x^\ast -x_k}}{\sum \limits_{k=1}^{t} \alpha(k)}
   + \frac{\sum\limits_{k=1}^{t} \alpha(k)^2\norm{\Tilde{g}(k)}_\ast^2}{2 \sigma_R \sum\limits_{k=1}^{t}\alpha(k)}.
        \end{split}
        \label{cb}
    \end{equation}
    Define $ X(t) = \sum\limits_{k=1}^{t}\alpha(k)\inprod{\zeta(k)}{x^\ast -x_k}$ and $Y(t) = \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R}\norm{\Tilde{g}(k)}_\ast^2$.
%\begin{equation*}
 %   X(t) = \sum\limits_{k=1}^{t}\alpha(k)\inprod{\zeta(k)}{x^\ast -x_k}
%\end{equation*}
%and
%\begin{equation*}
 %   Y(t) = \sum\limits_{k=1}^{t} \frac{\alpha(k)^2}{2 \sigma_R}\norm{\Tilde{g}(k)}_\ast^2.
%\end{equation*}
It can be seen from the definition of $\Tilde{g}(t)$ that  $ \mathbb{E}[Y(t)|\mathcal{F}_t] = Y(t-1)  + \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2 \geq Y(t-1)$. Hence, $\{Y(t), \mathcal{G}_t\}$ is a non-negative sub-martingale.  It has already been shown in the proof of  Lemma \ref{SLLN} that $\{X(t), \mathcal{G}_t\}$ is  a martingale, which implies $\{\norm{X(t)}^2, \mathcal{G}_t\}$ is a sub-martingale
%\begin{equation*}
 %   \mathbb{E}[Y(t)|\mathcal{F}_t] = Y(t-1)  + \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2 \geq Y(t-1)
%\end{equation*}
Choose a $t_0$ such that $\mathbb{D}_R(x^\ast,x_1) \leq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)$ $\forall \; t \geq t_0$ and in view of  Assumption \ref{assumption 2}, $t_0 < \infty$ .
Consider any $t > t_0$ and from \eqref{cb} if $f(z_t)-f^\ast \geq B_1D +\delta + \epsilon$ then atleast one of the following holds.
\\ $ X(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)  \; \; \text{or,} \; \;  Y(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)$.
%\begin{equation*}
 %   X(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)  \; \; \text{or,} \; \;  Y(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k) .
%\end{equation*}
That implies that $\forall \; t \geq t_0$ \begin{equation}
\begin{split}
    &\mathbb{P}(f(z_t)-f^\ast \geq \delta +
 B_1 D +   \epsilon)
    \\ \leq & \mathbb{P}(X(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)) + \mathbb{P}(Y(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)).
    \end{split}
    \label{ineq}
\end{equation}
Note that
\\ $\mathbb{P}(Y(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)) \leq \mathbb{P}(\max\limits_{1 \leq j \leq t} Y(j) $ $\geq$ $\frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)))$. Hence, by applying Lemma \ref{doob's maximal inequality} we arrive at
\begin{equation}
    \begin{split}
        & \mathbb{P}(Y(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)) \leq \frac{3}{\epsilon} \frac{E[Y(t)]}{\sum\limits_{k=1}^{t} \alpha(k)} \leq \frac{3\mathrm{K}}{\epsilon} \frac{\sum\limits_{k=1}^{t} \alpha(k)^2}{\sum\limits_{k=1}^{t} \alpha(k)}.
    \end{split}
    \label{submartingale}
\end{equation}
and similarly,
\begin{equation}
    \begin{split}
    & \mathbb{P}(X(t) \geq \frac{\epsilon}{3}\sum\limits_{k=1}^{t} \alpha(k)) \leq \mathbb{P} (\norm{X(t)}^2 \geq \frac{\epsilon^2}{9} (\sum\limits_{k=1}^{t} \alpha(k))^2) \\ &  \leq  \frac{9}{\epsilon^2} \frac{\mathbb{E}\norm{X(t)}^2}{(\sum\limits_{k=1}^{t}\alpha(k))^2} \leq \frac{9CD}{\epsilon^2} \frac{\sum\limits_{k=1}^{t} \alpha(k)^2}{(\sum\limits_{k=1}^{t} \alpha(k))^2}.
    \end{split}
    \label{martingale}
\end{equation}
Hence, by plugging \eqref{submartingale} and \eqref{martingale} into \eqref{ineq}, we get \eqref{theorem3}.
%Notice that $\alpha(t)\inprod{\zeta(t)}{x^\ast -x_t}$ is a sub-Gaussian random variable with parameter  $\alpha(t)c D$ according to Assumption \ref{subgaussian}.
%By applying Azuma-Hoeffding's inequality we get that
%\begin{equation*}
 %   \mathbb{P}(X(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)) \leq \exp{\Big{(}-\frac{\epsilon^2 (\sum\limits_{k=1}^{t} \alpha(k))^2}{9D^2c^2\sum\limits_{k=1}^{t}\alpha(k)^2 }\Big{)}}
%\end{equation*}
%On the other sider consider any $\lambda > 0$ then
%\begin{equation*}
 %   \begin{split}
  %      & \mathbb{P}(Y(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)) = \mathbb{P}(\exp{\lambda (Y(t))} \geq \exp{(\lambda \frac{\epsilon}{3}\sum\limits_{k=1}^{t}\alpha(k))} )
  %  \end{split}
%\end{equation*}
%By applying Markov's inequality we get that
%\begin{equation}
 %   \begin{split}
  %      & \mathbb{P}(Y(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)) \leq \frac{\mathbb{E}[\exp{\lambda (Y(t))}]}{\exp{(\lambda \frac{\epsilon}{3}\sum\limits_{k=1}^{t}\alpha(k))}}.
   % \end{split}
    %\label{Markov}
%\end{equation}
%Consider only the term in the numerator and notice that $\{\Tilde{g}(k)\}_{k=1}^{t}$s are independent because of the definition. Hence,
%\begin{equation*}
 %   \begin{split}
  %    &  \mathbb{E}[\exp{(\lambda Y(t))}] = \prod\limits_{k=1}^{t} \mathbb{E}[\exp{(\lambda \frac{\alpha(k)^2}{2 \sigma_R} \norm{\Tilde{g}(k)}^2)}]
   % \end{split}
%\end{equation*}
%It can be seen that $\exists \; K_2>0$ such that $\mathbb{E}[\exp{(\norm{\Tilde{g}(t))}_\ast^2}] \leq \exp{(K_2)}$.
 %Hence from \eqref{Markov}, we get
%\begin{equation*}
 %   \begin{split}
%        & \mathbb{P}(Y(t) \geq \frac{\epsilon}{3} \sum\limits_{k=1}^{t} \alpha(k)) \leq \frac{\exp{(K_2\lambda \sum\limits_{k=1}^{t}\alpha(k)^2)}}{\exp({\frac{\lambda\epsilon}{3}\sum\limits_{k=1}^{t} \alpha(k)})}
 %   \end{split}
%\end{equation*}
%Hence, from \eqref{ineq}, we get that
%\begin{equation*}
 %   \begin{split}
  %       &\mathbb{P}(f(z_t)-f^\ast \geq \delta + B_1D + \epsilon)
   %      \\ \leq & \frac{\exp{(K_2\lambda \sum\limits_{k=1}^{t}\alpha(k)^2)}}{\exp({\frac{\lambda\epsilon}{3}\sum\limits_{k=1}^{t} \alpha(k)})} + \exp{\Big{(}-\frac{\epsilon^2 (\sum\limits_{k=1}^{t} \alpha(k))^2}{9D^2c^2\sum\limits_{k=1}^{t}\alpha(k)^2 }\Big{)}}.
    %\end{split}
%\end{equation*}
\end{proof}
\begin{remark}
   % \item[1.] It can be seen (as it is shown in Lemma \ref{variance}) the constant $K_1 = \frac{C_1}{\mu}$. For a very small $\mu$, $K_1$ is higher which actually slows down the convergence of the ZOMD algorithm from Theorem \ref{concentration}.
     Notice that both $\mathrm{K}$ and $C$ are $\mathcal{O}(\frac{1}{\mu^2})$ from Lemma \ref{expectation}, this implies that an arbitrary small $\mu$ makes the convergence of the function value to the neighbourhood of the optimal solution slower. Hence, there is a trade-off between accuracy of the convergence to the  optimal value and convergence speed of the algorithm in the choice of $\mu$.  In the next Corollary, we capture this in detail.
\end{remark}
\begin{Corollary}
For any $\epsilon > 0$ and a confidence level $0 < p < 1$, let $p_1 = 1-p$.  Define $t_1$  such that $\forall \;  t \geq t_1$
$ \sum\limits_{k=1}^{t} \alpha(k)  \geq \frac{6 \mathrm{K}}{\epsilon p_1} \sum\limits_{k=1}^{t} \alpha(k)^2$ and $(\sum\limits_{k=1}^{t} \alpha(k))^2 \geq \frac{18 C D}{\epsilon^2 p_1} \sum\limits_{k=1}^{t} \alpha(k)^2$.
%\begin{equation*}
 %   \sum\limits_{k=1}^{t} \alpha(k)  \geq \frac{6 \mathrm{K}}{\epsilon p_1} \sum\limits_{k=1}^{t} \alpha(k)^2 \; \; \text{and} \; \;  (\sum\limits_{k=1}^{t} \alpha(k))^2 \geq \frac{18 C D}{\epsilon^2 p_1} \sum\limits_{k=1}^{t} \alpha(k)^2.
%\end{equation*}
%and $\forall \; t \geq t_2$
%\begin{equation*}
 % (\sum\limits_{k=1}^{t} \alpha(k))^2 \geq \frac{18 C D}{\epsilon^2 p_1} \sum\limits_{k=1}^{t} \alpha(k)^2
%\end{equation*}
Then $\forall \; t$ $ \geq$ $ \max \{ t_0, t_1\}$ we obtain
\begin{equation*}
    \mathbb{P}( f(z_t) -f^\ast < \delta +B_1D + \epsilon ) \geq p.
\end{equation*}
Notice that  $t_1$  $< \infty$ due to Assumption \ref{assumption 2}.
\label{concentrationc}
\end{Corollary}
\section{Conclusion}
In this letter, we proved almost sure convergence of function value of ZOMD algorithm to the neighbourhood of the optimal value. Further, we derive the concentration inequality  which provides bounds on how the function value of the iterates of ZOMD algorithm deviates from the neighbourhood in any finite time. This analysis sheds some new insight to the field of zeroth-order optimization. The future path of research will attempt to demonstrate a higher convergence rate using a variance reduction technique.
%But this neighbourhood depends on $D$, the diameter of the constraint set. We can make the neighbourhood independent of $D$ by assuming that the function is strongly convex. Next we show that the iterate sequence converge to neighbourhood of optimal solution independent of $D$.

%\textbf{An Improved Result (When the function is strongly convex)}
%\begin{theorem}
%Suppose if the objective function $f$ is $\rho$-strongly convex function, the for the iterate sequence generated by ZOMD algorithm with SPSA we have the following
%\begin{equation*}
 %    \mathbb{P}\Big{(}\liminf \{ \norm{z_t-x^\ast}^2 \leq \frac{C_1\eta_1^2}{\rho - C_1 \eta^2} \} \Big{)} = 1.
%\end{equation*}
%\label{mainstrong}
%\end{theorem}

%\begin{proof}
%In the proof of this theorem we borrow some results from the proof of Theorem \ref{mainths}.

%From \eqref{Ber} of Theorem \ref{mainths} we get that

%\begin{equation}
 %   \begin{split}
  %      & \mathbb{D}_R(x,x_{t+1})
   %     \\ \leq & \mathbb{D}_R(x,x_t) + \alpha(t) \inprod{\Tilde{g}(t)}{x-x_t} +  \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2
    %\end{split}
    %\label{ber1}
%\end{equation}

%Put $x = x^\ast$ and Consider only the term

%\begin{equation}
 %   \begin{split}
        %& \alpha(t) \inprod{\Tilde{g}(t)}{x^\ast-x_t}
        %\\ = & \alpha(t) \inprod{g(t) + b(t) + \epsilon (t)}{x^\ast - x_t }
        %\\ \leq & \alpha(t) \Big{\{} \inprod{g(t) -g(x^\ast)}{x^\ast-x_t} + \inprod{g(x^\ast)}{x^\ast-x_t} \Big{\}} \\ & + \alpha(t) \Big{(} \norm{x^\ast-x_t}^2 + 1 \Big{)} \norm{b(t)}_\ast + \alpha (t) \inprod{\epsilon(t)}{x^\ast-x_t}.
        %\\ \leq &  \alpha(t) \Big{(} - \rho \norm{x^\ast-x_t}^2 + C_1 \eta^2 \norm{x^\ast - x_t}^2 + C_1 \eta^2 \Big{)} \\ & + \alpha(t) \inprod{\epsilon(t)}{x^\ast-x_t}.
        %\label{str1}
    %\end{split}
%\end{equation}

%The first inequality in \eqref{str1} can be obtained by applying generalized Cauchy-Schwartz inequality.  Observe that $\inprod{g(x^\ast)}{x_t- x^\ast} > 0 $ because of first order optimality condition. The last line of \eqref{str1} can be found by applying strong convexity definition. In this context we are assuming that $\rho > C_1\eta^2$ and note that which is not a restrictive assumption.

%We can follow the same approach as we have done in Theorem \ref{mainths} and show that $\exists\; t _0 \in \mathbb{N}$ such that $\forall \; t \geq t_0$ we have $\norm{z_t-x^\ast}^2 \leq \frac{C_1\eta_1^2}{\rho-C_1\eta_1^2}$ a.s.
%\end{proof}
%\subsection{Zeroth-Order Mirror Descent Algorithm with SPSA}

%In this subsection we consider that gradient is estimated during simultaneous perturbation technique.

%\begin{theorem}
%Under assumptions \textcolor{red}{(?)}, for the iterate sequence generated by ZOMD algorithm with SPSA we have
%\begin{equation*}
 %    \mathbb{P} (\liminf \{f(z_t)-f^\ast \leq C_1 \eta_1^2 D\}) = 1
%\end{equation*}
%\label{mainths}
%\end{theorem}

%\begin{proof}
%We apply first order optimality condition as we approach in Theorem \ref{mainth},
%\begin{equation}
 %   \begin{split}
  %      & \mathbb{D}_R(x^\ast,x_{t+1})
   %     \\ \leq & \mathbb{D}_R(x^\ast,x_t) + \alpha(t) \inprod{\Tilde{g}(t)}{x^\ast-x_t} +  \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2.
    %\end{split}
    %\label{Ber1}
%\end{equation}
%Consider the term
%\begin{equation}
 %   \begin{split}
  %      &\alpha(t) \inprod{\Tilde{g}(t)}{x^\ast-x_t}
   %     \\ \leq & \alpha(t) (f^\ast-f(x_t)+ C_1\eta_1^2D + \inprod{\epsilon(t)}{x^\ast-x_t})
    %\end{split}
    %\label{sps}
%\end{equation}
% The last inequality in \eqref{sps} is because of convexity of the function $f$ and generalized Cauchy- Schwartz inequality.

 %We can take the same approach as Theorem \ref{mainth} and show that $\exists \; t_0 \in \mathbb{N}$ such that $\forall \; t \geq t_0$ we have $f(z_t)-f^\ast \leq C_1\eta_1^2 D$ a.s.
%\end{proof}

 %\textbf{Remark} From Theorem \ref{mainths}, we get that the function value of iterate sequence converges to the neighbourhood of optimal solution almost surely. But this neighbourhood depends on $R$, the diameter of the constraint set. We can make the neighbourhood independent of $R$ by assuming that the function is strongly convex. Next we show that the iterate sequence converge to neighbourhood of optimal solution independent of $D$.

%\textbf{An Improved Result (When the function is strongly convex)}

%\begin{theorem}
%Suppose if the objective function $f$ is $\rho$-strongly convex function, the for the iterate sequence generated by ZOMD algorithm with SPSA we have the following
%\begin{equation*}
 %    \mathbb{P}\Big{(}\liminf \{ \norm{z_t-x^\ast}^2 \leq \frac{C_1\eta_1^2}{\rho - C_1 \eta^2} \} \Big{)} = 1.
%\end{equation*}
%\label{mainstrong}
%\end{theorem}

%\begin{proof}
%In the proof of this theorem we borrow some results from the proof of Theorem \ref{mainth}.

%From \eqref{Ber} of Theorem \ref{mainth} we get that

%\begin{equation}
 %   \begin{split}
  %      & \mathbb{D}_R(x,x_{t+1})
   %     \\ \leq & \mathbb{D}_R(x,x_t) + \alpha(t) \inprod{\Tilde{g}(t)}{x-x_t} +  \frac{\alpha(t)^2}{2 \sigma_R} \norm{\Tilde{g}(t)}_\ast^2
    %\end{split}
    %\label{ber1}
%\end{equation}

%Put $x = x^\ast$ and Consider only the term

%\begin{equation}
 %   \begin{split}
  %      & \alpha(t) \inprod{\Tilde{g}(t)}{x^\ast-x_t}
   %     \\ = & \alpha(t) \inprod{g(t) + b(t) + \epsilon (t)}{x^\ast - x_t }
    %    \\ \leq & \alpha(t) \Big{\{} \inprod{g(t) -g(x^\ast)}{x^\ast-x_t} + \inprod{g(x^\ast)}{x^\ast-x_t} \Big{\}} \\ & + \alpha(t) \Big{(} \norm{x^\ast-x_t}^2 + 1 \Big{)} \norm{b(t)}_\ast + \alpha (t) \inprod{\epsilon(t)}{x^\ast-x_t}.
     %   \\ \leq &  \alpha(t) \Big{(} - \rho \norm{x^\ast-x_t}^2 + C_1 \eta^2 \norm{x^\ast - x_t}^2 + C_1 \eta^2 \Big{)} \\ & + \alpha(t) \inprod{\epsilon(t)}{x^\ast-x_t}.
      %  \label{str1}
    %\end{split}
%\end{equation}

%The first inequality in \eqref{str1} can be obtained by applying generalized Cauchy-Schwartz inequality.  Observe that $\inprod{g(x^\ast)}{x_t- x^\ast} > 0 $ because of first order optimality condition. The last line of \eqref{str1} can be found by applying strong convexity definition. In this context we are assuming that $\rho > C_1\eta^2$ and note that which is not a restrictive assumption.

%We can follow the same approach as we have done in Theorem \ref{mainth} and show that $\exists\; t _0 \in \mathbb{N}$ such that $\forall \; t \geq t_0$ we have $\norm{z_t-x^\ast}^2 \leq \frac{C_1\eta_1^2}{\rho-C_1\eta_1^2}$ a.s.
%\end{proof}
%Now as approached in Theorem \ref{mainth} we want to proof by contradiction. Consider any $a > 0$ and  assume that $\exists \; t_0 \in \mathbb{N}$ such that $\norm{x_t-x^\ast}^2 > \frac{k_1\eta^2+a}{\rho-k_1\eta^2}$  $\forall \; t \geq t_0$.

%Doing the same steps as we have done in Theorem \ref{mainth} we get that
%\begin{equation*}
 %   \limsup\limits_{T \geq t_0} \mathbb{D}_R(x^\ast,x_T) = -\infty
%end{equation*}

%which is clearly a contradiction. Hence we can conclude that
%\begin{equation*}
 %   \mathbb{P}( \cap_{t_0 \geq 1} \cup_{t \geq t_0}  \{\norm{x_t-x^\ast}^2 \leq \frac{C_1\eta^2}{\rho - C_1 \eta^2} \} ) = 1.
%\end{equation*}

%or,






%\textbf{Remark} The results found in Theorem \ref{mainstrong} are more desirable than the Theorem \ref{mainth}. However, to apply this result we need the objective function is strongly convex function and the biased term of the gradient should be smaller than the strong convexity parameter. This is not easy because in this case we don't know the explicit form of the function. Hence estimating the strong convexity parameter  of the objective function is difficult although in recent times there are some methods to estimate the strong convexity parameter  also available in literature.



\bibliographystyle{IEEEtran}
\bibliography{ref}
\end{document} 