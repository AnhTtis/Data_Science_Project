%
\documentclass{midl}

%
%
%
%
\usepackage{csvsimple}
\usepackage{nicematrix}
\usepackage{mathtools}
\usepackage{mwe} %
%
\usepackage{tikz-cd}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\convtwod}[2]{\text{conv2d}(#1,#2)}
\usepackage{textgreek}
\usepackage{xcolor}
%
%
%
%
%
\jmlryear{2023}
\jmlrworkshop{Full Paper -- MIDL 2023}
\jmlrvolume{-- nnn}
\editors{Accepted for publication at MIDL 2023}

\title[SVD-DIP: Overcoming the Overfitting Problem in DIP-based CT Reconstruction]{SVD-DIP: Overcoming the Overfitting Problem in DIP-based CT Reconstruction}

%
%
%
%
%
%

%
%
%
%

%
%
%
%
%


%
%
%
%
%
%
%

%

%
\midlauthor{\Name{Marco Nittscher\midljointauthortext{Contributed equally}} \Email{mnittsch@uni-bremen.de}\\
\addr Center for
Industrial Mathematics, University of Bremen, Germany \AND
\Name{Michael Lameter\midlotherjointauthor} \Email{lameter@uni-bremen.de}\\\addr Center for
Industrial Mathematics, University of Bremen, Germany \AND
\Name{Riccardo Barbano} \Email{riccardo.barbano.19@ucl.ac.uk}\\
\addr Department of Computer Science, University College
London, UK \AND
\Name{Johannes Leuschner} \Email{jleuschn@uni-bremen.de}\\\addr Center for
Industrial Mathematics, University of Bremen, Germany \AND
\Name{Bangti Jin} \Email{b.jin@cuhk.edu.hk}\\
\addr Department of Mathematics, The Chinese University of Hong
Kong, Shatin, N.T., Hong Kong \AND
\Name{Peter Maass} \Email{pmaass@uni-bremen.de}\\\addr Center for
Industrial Mathematics, University of Bremen, Germany 
}

\begin{document}

\maketitle
\thispagestyle{plain}

\begin{abstract}
The deep image prior (DIP) is a well-established unsupervised deep learning method for image reconstruction; yet it is far from being \emph{flawless}. The \emph{DIP overfits to noise} if not early stopped, or optimized via a regularized objective. 
%
We build on the regularized fine-tuning of a pretrained DIP, by adopting a novel strategy that restricts the learning to the adaptation of \emph{singular values}. The proposed SVD-DIP uses \emph{ad hoc} convolutional layers whose pretrained parameters are decomposed via the singular value decomposition. Optimizing the DIP then solely consists in the fine-tuning of the singular values, while keeping the left and right singular vectors fixed.
We thoroughly validate the proposed method on real-measured \textmu CT data of a lotus root as well as two medical datasets (LoDoPaB and Mayo). We report significantly improved stability of the DIP optimization, by \emph{overcoming the overfitting} to noise.
\end{abstract}

\begin{keywords}
Deep Image Prior, Fine-Tuning, Computed Tomography, Singular Value Decomposition
\end{keywords}

\section{Introduction}
In medical imaging, we are often interested in inverse problems of the form $y=Ax+\nu$, with $y\in \mathbb{R}^m$ being a noisy measurement, $x\in \mathbb{R}^n$ the unknown image of interest, $A$ a linear forward operator, and $\nu \sim \mathcal{N}(0,\sigma^2I)$ i.i.d.\ noise. This problem is often ill-posed, and regularization is needed to recover a sensible image \cite{EnglHankeNeubauer:1996,ItoJin:2015}.

In recent years, deep learning has been applied successfully to many imaging modalities, often via a supervised learning paradigm \cite{arridge_maass_oektem_schoenlieb_2019,ongie2020deep}. However, supervised learning tends to require a large amount of paired training data to be effective \cite{Baguer_2020}.
Deep Image Prior (DIP) \cite{Ulyanov_2020} is an unsupervised alternative to applying deep learning to image reconstruction.
Its main advantage over supervised methods is that it requires no training data, and learns only on the observed data sample, relying on the rich structure of convolutional neural networks (CNNs) to have a regularizing effect on the image. 
However, it is not without limitations. The network can overfit to the noise, and has to be freshly trained for every image we intend to reconstruct. The Educated Deep Image Prior (EDIP)  \cite{educatedWarmStart} is a variant of DIP that addresses some of these issues. 
It uses pretraining by initializing the network architecture with a pretrained (warm-start) parameter setting, rather than randomly. 
However, EDIP still suffers from overfitting even if equipped with a regularized objective; iterating beyond a certain point leads to deteriorated quality.

The problem of overfitting is not unique to DIP. 
Recently, \citet{singularValueFinetuning} suggested a novel way of adapting pretrained parameters in a CNN using the singular value decomposition (SVD) to address overfitting and the generalization ability of a CNN specifically in the context of image segmentation. In this paper, our contribution is to integrate the idea of the SVD fine-tuning into the EDIP framework, achieving remarkable stability. 
Specifically, we first train a CNN on synthetic data, and before warm-starting the DIP with the pretrained parameter setting, we replace some or all of the layers of the network using the SVD. In our experiments, the resulting DIP (i.e., SVD-DIP) is much more resistant to overfitting. When iterating for very long, e.g.\ 200k iterations, it retains a high PSNR value, outperforming the classical DIP. On the other hand, the SVD-DIP only leads to a minor drop in the maximal PSNR value.


\section{The Deep Image Prior}

Given a measurement $y$, the DIP \cite{Ulyanov_2020} finds an $x$ that minimizes $||Ax-y||_2^2$. The method trains a CNN, typically deploying a U-Net architecture \cite{Unet}, to fit to the single data sample $(z,y)$, where $z$ is a randomly initialized input (e.g., with i.i.d.\ Gaussian noise). The DIP finds a parameter setting $\theta$ such that the output of the neural network $\varphi_\theta(z)=x$ minimizes the error $||A\varphi_\theta(z)-y||_2^2$. This requires training a CNN for each $y$; the optimization can take many hours, depending on the complexity (esp.\ high-dimensionality) of the reconstruction task \cite{educatedWarmStart}. The method relies on the observation that the CNN structure already captures a sufficient amount of low-level image statistics, such that it can reconstruct an image well even without being trained on any data save for the input image \cite{Dittmer:2020}. When data is scarce or expensive to acquire, this represents a major upside.
However, the Achilles' heel of the DIP is overfitting to noise. When optimizing for too long, the network fits the noise. There are several approaches to alleviate this issue. The use of an explicit regularization term \cite{https://doi.org/10.48550/arxiv.1810.12864} is one of them. For all our DIP variants, we replace the standard objective used in \citet{Ulyanov_2020} $||A\cdot-y||_2^2$ with
\begin{equation}
    ||A\cdot-y||_2^2+\gamma \text{TV}(\cdot)
    \label{eqn:objective}
\end{equation}
where $\gamma>0$ and $\text{TV}(x):=\sum_{i,j}|x_{i+1,j}-x_{i,j}|+\sum_{i,j}|x_{i,j+1}-x_{i,j}|$ is the anisotropic total variation. That is, the DIP baseline uses the regularized objective in \autoref{eqn:objective}. The latter only partially alleviates overfitting to the noise, often not being sufficient to prevent it completely. Other methods rely on the Stein's unbiased risk estimator \cite{Jo_2021_ICCV} (also see \appendixref{SURE}), or on hand-crafted early-stopping criteria  \cite{https://doi.org/10.48550/arxiv.1810.12864, https://doi.org/10.48550/arxiv.2112.06074}. Additionally, one may constrain the network to an under-parameterized regime such that its ability to fit to noise is reduced, e.g., deep decoder \cite{Heckel2019deep_decoder}. 

%
The Educated Deep Image Prior (EDIP) \cite{educatedWarmStart} is a variant of the DIP that makes use of pretraining, motivated by wanting to speed up the DIP, since freshly training DIP on multiple measurements is time-consuming. The idea is to first train the CNN architecture using available or synthetic data. The obtained parameter setting  $\theta^\star$ is used as the starting point for the subsequent DIP reconstructive task on $y$. This ``warm-start'' allows the network to reconstruct the desired image in fewer iterations, exploiting benign inductive biases learned in the pretraining phase.%

\section{Overcoming Overfitting via an SVD-based Pretrained CNN}
While the original EDIP model was motivated by a desire to speed up the DIP, we are instead interested in utilizing pretraining to address overfitting. Intuitively, we can think of the warm-start parameter setting obtained via pretraining as a list of tensors $W$, each coding the weights for a convolutional layer in the CNN.
In EDIP, this parameter setting is used as the initialization. Thus, the only mechanism in place to stop from eventually overfitting to noise is the TV term in the loss. Our approach, the SVD-DIP, integrates the idea from \cite{singularValueFinetuning} of adapting the pretrained parameter setting, into the DIP framework. Specifically,
the pretrained tensor $W\in \mathbb{R}^{C_{out}\times C_{in} \times K \times K}$ (which represents $K \times K$ convolutions) is first folded into a matrix $W'\in \mathbb{R}^{C_{out} \times C_{in}K^2}$. Then, the SVD yields three matrices, $U'\in \mathbb{R}^{C_{out}\times R}$, $S'\in\mathbb{R}^{R \times R}$ (diagonal), $V'\in \mathbb{R}^{R \times C_{in}K^2}\quad (R=\min\{C_{out},C_{in}K^2\})$,
\begin{equation*}
    W' = U' S' V'.
\end{equation*}
These matrices can be ``unfolded'' back into three tensors $U\in \mathbb{R}^{C_{out} \times R \times 1 \times 1}$, $S\in \mathbb{R}^{R \times R \times 1 \times 1}$, $V\in \mathbb{R}^{R \times C_{in} \times K \times K}$ such that composing the tensors' induced convolutions is equivalent to taking the original tensor's induced convolution. For the technical details, see \appendixref{SVD_proof}. 
The key idea of the proposed approach is to represent the network parameters in this form, then freeze the tensors $U$ and $V$, keeping their convolutions fixed, but allowing the tuning of the singular values (SVs), i.e.\ the diagonal of $S'$.

%
\begin{figure}[b]
     %
     %
     \centering
    {
    %
    }%
    {
    %
    }%
    {
    \begin{minipage}{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/Unet_ellipses.pdf}
        \vspace*{-2em}
        \caption{{U-Net architecture used for DIP, as well as for EDIP and SVD-DIP when pretraining on Ellipses.}}
        \label{fig:unet_ellipses}
    \end{minipage}\hfill%
\begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/svd_plot_short.png}
        \caption{SVs taken from the  1st convolutions of Down layer 2 and Up layer 4 of the U-Net on the lotus.}
        \label{fig:singular_values}
    \end{minipage}
    \vspace{-2em}
    
    }
\end{figure}
Freezing $U,V$ and only varying the SVs retains the structure learned by pretraining the network, but the weighting of these structural components can be fine-tuned for a specific task at hand: increasing the weight of more important structures, or reducing less important ones. \autoref{fig:singular_values} shows that the SVs of the pretrained CNN are modified by SVD-DIP, but still retain the overall trend of the original values. 
%

Prior to the DIP fine-tuning, the network encodes the same function whether it is warm-started with the pretrained parameter setting, or its SVD-adapted counterpart, since
with or without the SVD replacement, each layer performs exactly the same convolutional operation. Once fine-tuning begins, the two methods diverge from each other with SVD-DIP being more constrained due to having fewer parameters.
The way the SVD is used to replace convolutional layers can be viewed as parameter compression; if $C_{in}=C_{out}$, the number of SVs is a fraction of the square root of the number of parameters of the original tensor.
This drastic reduction in the number of parameters, while still retaining enough expressive power thanks to the structure learned during pretraining being encoded in $U$ and $V$, greatly alleviates the overfitting issue and stabilizes the convergence, as demonstrated by the experiments below. When constraining the parameter space, we sacrifice some capability to represent finer structure, but gain substantial robustness to overfitting. We are more interested in a proof of concept for a method with high stability, which bypasses the need for reliable early stopping criteria, even when the pretraining and application datasets are different.

\section{Experiments and Results}

\subsection{The Experiment Setup}

We conduct our experiment on both \textmu CT and medical CT image datasets.  During pretraining, the U-Net learns to post-process filtered back-projections of noisy CT projections, that are simulated using the geometry of the target data. See \appendixref{experiment_setup_details} for more details.
We consider the following scenarios:

\paragraph{Pretraining on Ellipses $\to$ fine-tuning on Lotus and LoDoPaB's Chest}
Pretraining is performed on a dataset consisting of synthetically generated ellipses, which is commonly used for inverse problems arising in imaging \cite{Adler_2017} and used to warm-start DIP for CT reconstruction in \cite{educatedWarmStart}. 
Synthetic data is advantageous in applications where data is scarce. %
5\% Gaussian noise is added to the simulated projection data.
We use this approach for two target datasets: (i) real-measured \textmu CT data of a lotus root \cite{bubba2016lotus_paper} (fan-beam geometry with $20$ angles, $429$ detector pixels and image size $128\times 128\,\mathrm{px}^2$);
(ii) simulated medical chest CT data from LoDoPaB \cite{Leuschner2021lodopab} (parallel-beam geometry with $200$ angles, $513$ detector pixels and image size $362\times362\,\mathrm{px}^2$). It is simulated to include Poisson noise corresponding to 4096 photons per pixel before attenuation. As per best practice, the ground truth images with low artifact corruption were preferred when selecting $10$ LoDoPaB test samples via manual inspection before running any experiments.
Note that there is a shift in noise distribution between pretraining that uses $5\%$ Gaussian noise and the target data which is either real-measured or simulated to contain Poisson noise.
The U-Net architecture is shown in \autoref{fig:unet_ellipses}, based on {\cite[Table B5, DIP for LoDoPaB]{Baguer_2020}.}
The SVD replacements reduce the number of parameters from $128^2\cdot 3^2$ to 128 in each convolution layer, i.e., reducing by a factor of $\frac{1}{1152}$.
We refer to the two transfer settings as ``Ellipses-Lotus'' and ``Ellipses-LoDoPaB''.
\paragraph{Pretraining on Chest $\to$ fine-tuning on Mayo's Head, Abdomen and Chest}
Pretraining is performed on LoDoPaB \cite{Leuschner2021lodopab}.
The dataset contains ca.\ 40k chest CT images of size $362\times362\,\mathrm{px}^2$ and simulated parallel-beam projections with 1000 angles, 513 detector pixels and Poisson noise corresponding to 4096 photons per pixel before attenuation.
We experiment with both the original 1000-angle geometry as well as a sub-sampled 200-angle geometry.
We use images of different body parts from \cite{Moen2021mayo_ldct_and_projection} as the images to be reconstructed and simulate observations using the same geometry and noise setting that was used in LoDoPaB.
{ For EDIP and SVD-DIP, we use the U-Net architecture shown in \appendixref{apd:unet_arch_choice} \autoref{fig:unet}(b) trained in \cite{Baguer_2020}, while for DIP we use the architecture shown in \autoref{fig:unet_ellipses}, which is optimal in non-pretrained settings (see \appendixref{apd:unet_arch_choice}).
}
We refer to this setting as ``LoDoPaB-Mayo''.

\medskip
We use the resulting parameters as our pretrained parameters. Both EDIP and SVD-DIP receive the filtered back-projection obtained from test data, $z=\text{FBP}(y)$, matching the post-processing task of the pretraining, while DIP receives an i.i.d. Gaussian noise image $z$. For the target data containing Poisson noise (Ellipses-LoDoPaB and LoDoPaB-Mayo), we replace the squared error $\Vert A\cdot -y\Vert_2^2$ in the objective \eqref{eqn:objective} with the Poisson regression loss matching the noise distribution. A suitable regularization parameter $\gamma$ is chosen for each setting, and the same value is used for all methods. See \appendixref{apd:experiment_details} for more details. We repeat three times for each image from the data sets, for each variant of DIP. We test the classical (randomly initialized) DIP, EDIP and SVD-DIP.
%
%
%
%

\subsection{Comparison of Methods}
\subsubsection{\texorpdfstring{\textmu}{Î¼}CT of lotus root (Ellipses-Lotus)}
    \begin{figure}[!b]
     %
     %
    \floatconts
      {fig:psnrtrace_lotus}
      {\caption{Optimization of DIP, EDIP and SVD-DIP on the lotus, pretraining on ellipses. Each line represents the mean over 3 runs, the colored area the standard deviation. The final SVD-DIP reconstruction is shown on the right, along with the max PSNR reconstructions of DIP and EDIP, which would require ideal early stopping.}}
      {\begin{minipage}{0.52\textwidth}\includegraphics[width=0.75\textwidth]{plots/lotus_psnr_reg}\end{minipage}\hfill%
      \begin{minipage}{0.45\textwidth}\includegraphics[width=0.75\textwidth]{plots/image_plot_lotus.pdf}\end{minipage}
      \vspace{-1.5mm}
      }\vspace{-1.5mm}
    \end{figure}

\autoref{fig:psnrtrace_lotus} shows the convergence behavior of the different DIP frameworks for the lotus.
{EDIP peaks early, reaching a maximum PSNR (max PSNR) value of 31.95 dB. However, after the peak, its PSNR steadily decreases, indicating overfitting.
The vanilla DIP and the (pretrained) SVD-DIP are on par in terms of the max PSNR (31.69 dB and 31.60 dB, resp.), but the vanilla DIP is far less stable, i.e.\ its PSNR falls after the peak value, while the PSNR of SVD-DIP does not decline.
While EDIP shows a similar overfitting behavior as DIP, EDIP reports a higher max PSNR. If we consider both max PSNR and overfitting, pretraining generally improves the DIP, with EDIP achieving the best max PSNR and SVD-DIP avoiding overfitting.
Thus, the pretrained SVD-DIP performs almost as well as EDIP in terms of max PSNR and clearly outperforms EDIP in terms of stability.}
EDIP has the benefit of the speed at which it reaches its best PSNR value.

As an ablation study, we also include the results for SVD-DIP without pretraining, which performs much worse than the rest. This is not surprising, since it can only learn a weighting of the singular vectors that posses no useful structure, due to the random initialization. 

\paragraph{SVD Truncation}\label{truncation}
Since before and after the fine-tuning, {a large portion of} SVs close to zero generally remain so, we observe that the SVD-DIP can be further reduced in the number of parameters by the truncated SVD. \autoref{fig:psnrtrace_lotus_svd} contains a comparison of SVD-DIP settings with different approaches to truncation. For (50\% rd.), we truncated all but the top 50\% of the SVs. For (10\% thresh.), we truncated all SVs which are below $10\%$ of the largest SV.
The graph shows that the different variations of SVD-DIP perform very similarly, the difference between their PSNRs being {within the order of magnitude of the standard deviation observed for repeated runs of the same experiments}. 
Thus, the lotus root image is simple enough to allow for a low-dimensional representation of the parameter space by only the upper 50\% of the SVs (50\% rd.), or only the values greater or equal to 10\% of the largest (10\% thresh.).
{We view these results as a proof-of-concept for SVD truncation, whose advantageous utilization would be subject of future work.}

\begin{figure}[htbp]
    \floatconts
      {fig:psnrtrace_lotus_svd}
      {\vspace{-1em}\caption{Optimization of SVD-DIP for forms of truncation on the lotus, pretraining on ellipses. Each line represents the mean over 3 runs, the colored area the SD.}}
      {\includegraphics[width=0.5\linewidth]{plots/lotus_psnr_svd}
      \vspace{-1em}
      }
      \vspace{-0.5em}
    \end{figure}


\subsubsection{Chest CT (Ellipses-LoDoPaB)}
\input{plots/table_lodo_short.tex}

    \begin{figure}[htb]
     %
     %
    \floatconts
      {fig:psnrtrace_lodo}
      {\caption{Optimization of DIP, EDIP and SVD-DIP on LoDoPaB (Sparse 200) test samples 13, 17 and 43, pretraining on ellipses. %
      Each line represents the mean over 3 runs.
      }}
      {\includegraphics[width=1\linewidth]{plots/plot_lodopab_multiple}
      \vspace{-3em}}
    \vspace{-1em}
    \end{figure}
%

DIP has a better max PSNR on average (see \autoref{tab:lodo_short}). SVD-DIP reaches on average a lower max PSNR than EDIP and DIP, but it is much more stable, with the PSNR trending upwards, rather than declining. \autoref{fig:psnrtrace_lodo} exemplarily shows the behavior of the PSNR observed in the runs.
On average, SVD-DIP outperforms DIP and EDIP, arriving at a slightly lower value in a remarkably stable manner. Thus, the strong regularization induced by fixing the singular vectors comes at the cost of a slightly lower max PSNR, but on the other hand greatly reduces the overfitting. Here, for SVD-DIP we replace the convolutional layers in all down- and up-blocks by SVD representations, except the first three down-blocks.
We can treat the number of down-blocks that remain unchanged as a hyperparameter, which can be adjusted depending on the need to adapt to different input data. We replace fewer blocks as we have a major change in the noise distribution, so as to provide the network with greater flexibility.

\subsubsection{CT of different body parts (LoDoPaB-Mayo)}
    \begin{figure}[h!tbp]
     %
     %
    \floatconts
      {fig:scatter_multiplot}
      {
      \caption{The mean and SD of Final and Maximum PSNR for DIP, EDIP and SVD-DIP on different body parts from LoDoPaB-Mayo, pretraining on LoDoPaB. Both 1000 and 200 angle settings are considered, indicated on the horizontal axis. For each setting, 10 samples with 3 runs each are considered. Final reconstructions of SVD-DIP are shown for the 1000 angle setting, along with the ground truth.}
      }{
    \begin{minipage}{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/scatter_mayo_end_peak_pretrained_dip_with_fbp_unet_arch.png}
    \end{minipage}\hfill
    \begin{minipage}{0.54\textwidth}
    \includegraphics[width=\textwidth]{plots/image_plot_mayo_pretrained_dip_with_fbp_unet_arch.pdf}
    \end{minipage}
    \vspace{-1.5em}
      }\vspace{-0.5em}
    \end{figure}

    \begin{figure}[t]
     %
     %
    \floatconts
      {fig:psnrtrace_lodo_mayo_1000}
      {\caption{The mean and SD over 3 runs of the optimization of DIP, EDIP and SVD-DIP on samples from different body parts from LoDoPaB-Mayo, pretraining on LoDoPaB. The 1000 angle setting is used. %
     }}
      {\includegraphics[width=1\linewidth]{plots/plot_lodopab_mayo_multiple_pretrained_dip_with_fbp_unet_arch.png}
      \vspace{-3em}}\vspace{-1.5em}
    \end{figure}

{
As shown in \autoref{fig:scatter_multiplot}, SVD-DIP on average reports higher final PSNR values when compared to EDIP and DIP, as well as max PSNR values similar to those of EDIP.
On Chest and Abdomen, max PSNR values are comparable across all methods; on Head instead, the DIP achieves a higher max PSNR than EDIP and SVD-DIP. Recall that the architecture shown in \autoref{fig:unet_ellipses} is used for DIP, while for EDIP and SVD-DIP we employ the readily trained networks from \cite{Baguer_2020} with the architecture shown in \appendixref{apd:unet_arch_choice} \autoref{fig:unet}(b).
The latter, designed for a pure post-processing task, differs by being smaller, by including skip connections, and by \emph{missing} a final sigmoid activation.
The Head images have rather simplistic structures and can be fit particularly well by the architecture used for DIP (\autoref{fig:unet_ellipses}), with the final sigmoid activation being an important inductive bias.
In addition, the Head data is outside the training data distribution (LoDoPaB), while Chest and Abdomen are in or close to the training distribution.
See \appendixref{apd:unet_arch_choice} for additional results with the respective other architecture choice.
In all cases, DIP and EDIP show significant overfitting, which is effectively reduced with SVD-DIP.
}
\FloatBarrier


\section{Conclusion}

In this work we built on the regularized fine-tuning of a pretrained DIP and adopted a novel strategy that restricts the learning to the adaptation of singular values of the unfolded network parameter tensor. We proposed a variant of the DIP, named SVD-DIP, that overall overcomes the need for early stopping, but sacrifices some speed relative to EDIP. This approach yields a more stable (less prone to overfit to noise) DIP optimization. {The empirical results suggest that while the SVD-DIP reconstructive properties are usually on par with or slightly worse than those of DIP or EDIP, it loses very little in terms of PSNR, even after iterating for a long time.}

%
\midlacknowledgments{R.B. was supported by the i4health PhD studentship (UK EPSRC EP/S021930/1). J.L. was funded by the German Research Foundation (DFG; GRK 2224/1). The work of B.J. was partially supported by UK EPSRC grants EP/T000864/1 and EP/V026259/1. P.M. acknowledges support by DFG-NSFC project M-0187 of the Sino-German Center mobility programme.}


\bibliography{midl23_14}

\clearpage
\appendix

\section{The Singular Value Decomposition is Sensible}\label{SVD_proof}
While convolutions are linear and can thus be represented using a matrix for a fixed size input, these matrices are distinct from the input-size invariant matrices that we fold our tensors into. The former, for a tensor $W\in \R^{C_{out}\times C_{in} \times K \times K}$ are of the shape $\hat{W}\in \R^{C_{out} J^2 \times C_{in}J^2}$, for input images in $\R^{J \times J}$, while the folded matrix is of the form $W'\in \R^{C_{out}\times C_{in}K^2}$. For the former, it is direct that the SVD's use of matrix multiplication to compose the derived linear transformations yields the original convolution. For the latter, it may not be evident at first glance whether the multiplicative structure of the folded matrices is consistent with the composition of the convolutions they encode. Fortunately this is indeed the case. The key reason for this is that we unfold into $1 \times 1$ convolutions on the left. In the following, we demonstrate this rigorously:\\
For any integer $N$, we denote by the notation $[N]$ the set $\{1,\ldots,N\}$. Then for any integers $N$, $M$, (odd) $K$ and $J$, let 
\begin{align*}
    W&:=(w_{n,m,k_1,k_2})_{(n,m,k_1,k_2)\in [N]\times[M]\times [K]^2}\in \R^{N \times M\times K\times K},\\
X&:=(x_{m,j_1,j_2})_{(m,j_1,j_2)\in [M]\times [J]^2}\in \R^{M \times J \times J},
\end{align*} 
be two tensors. Then we can define the 2d convolution of $X$ with $W$ as a tensor
$$\convtwod{W}{X}:=(y_{n,j_1,j_2})_{(n,j_1,j_2)\in [N]\times [J]^2}\in \R^{N\times J \times J}$$ 
with
\begin{align*}
y_{n,j_1,j_2}:=\sum_{m=1}^M \sum_{k_1=1}^K\sum_{k_2=1}^K w_{n,m,k_1,k_2} \cdot X_{m,j_1+k_1-\frac{K-1}{2}-1,j_2+k_2-\frac{K-1}{2}-1}
\end{align*}
with $X_{m,i_1,i_2}:=0$ when either of the indices $i_1,i_2$ is outside of $[J]$.
\\
\\
Then, for a given weight tensor $W\in \R^{N \times M \times K \times K}$ that can be represented by a matrix $A'B'=W'\in \R^{N\times MK^2}$, and a given feature map $X \in \R^{M \times J \times J}$, taking the 2-dimensional convolution of $X$ with respect to $W$ is identical to convolving $X$ by appropriate tensor representations of $B'\in \R^{R \times M \cdot K \cdot K}$ and (then) $A'\in \R^{N \times R \cdot 1 \cdot 1}$. Note that the second tensor only codes $1\times 1$ convolutions. To prove this result, we need the following lemma:
\begin{lemma}
Let $M,K\in \N_{>0}$. %
Given a bijection between indices $f:[M]\times [K]^2\rightarrow [MK^2]$, we can define a map that folds tensors into matrices:
\begin{align*}
\Phi_f:\biguplus_{N=1}^\infty \R^{N \times M \times K \times K}\rightarrow & \biguplus_{N=1}^\infty \R^{N \times MK^2}\\
(x_{n,m,k_1,k_2})_{(n,m,k_1,k_2)\in [N]\times [M]\times [K]^2} \mapsto & (x_{n,f^{-1}(i)})_{(n,i)\in [N]\times [MK^2]}
\end{align*} 
Let $g:[M]\times [K]^2\rightarrow [MK^2]$ be a bijection, $N,R,J\in \N_{>0}$ be tensor dimensions, $X\in \R^{M \times J \times J}$ an input feature map.
Let $W:=(w_{n,m,k_1,k_2})_{(n,m,k_1,k_2)\in [N]\times[M]\times [K]^2}\in \R^{N \times M\times K\times K}$ be a tensor. Fix a bijection of indices $g:[M]\times [K]^2\rightarrow [MK^2]$. Define $W':=\Phi_g^{-1}(W)$. Let $R\in \N_{>0}$, $A'\in \R^{N\times R}$, $B'\in\R^{R\times MK^2}$ such that $W'=A'B'$. Further, define $A:=\Phi_{(\text{Id}_{[R]},\text{Id}_{[1]},\text{Id}_{[1]})^{-1}}^{-1}(A'), \;
B:=\Phi_g^{-1}(B')$ as the ''unfolded'' tensors of $A',B'$. 
Let $X\in \R^{M \times J \times J}$ be an arbitrary tensor/feature map. Then the following identity holds: 
\begin{equation*}
\convtwod{W}{X}=\convtwod{A}{\convtwod{B}{X}}
\end{equation*}
\end{lemma}
\begin{proof}
We define the following tensors:
\begin{align*}
    (a_{n,r,k_1,k_2})_{(n,r,k_1,k_2)\in [N] \times [R]\times [1]\times [1]}&:=A \\
    (b_{r,m,k_1,k_2})_{(r,m,k_1,k_2)\in [R] \times [M] \times [K]^2}&:=B\\
    (y_{r,j_1,j_2})_{(r,j_1,j_2)\in [R]\times [J]^2}&:=Y:=\convtwod{B}{X}
\end{align*}
Further, let $\pi_3,\pi_4$ be the mappings that extract the third and fourth entries of a given tuple. Then the proof reduces to the following elementary argument, exploiting the linearity of the operators involved:
\begin{align*}
&(\convtwod{A}{\convtwod{B}{X}})_{n,j_1,j_2}=(\convtwod{A}{Y})_{r,j_1,j_2}\\
&=\sum_{r=1}^R \sum_{k_1=1}^1\sum_{k_2=1}^1 a_{n,r,k_1,k_2} \cdot Y_{r,j_1+k_1-\frac{1-1}{2}-1,j_2+k_2-\frac{1-1}{2}-1}\\
&=\sum_{r=1}^R a_{n,r,1,1} \cdot Y_{r,j_1,j_2}\\
&=\sum_{r=1}^R a_{n,r,1,1} \cdot \sum_{m=1}^M \sum_{k_1=1}^K\sum_{k_2=1}^K b_{r,m,k_1,k_2} \cdot X_{m,j_1+k_1-\frac{K-1}{2}-1,j_2+k_2-\frac{K-1}{2}-1}\\
&=\sum_{r=1}^R a_{n,r} \cdot \sum_{i=1}^{MK^2} b_{r,i} \cdot X_{m,j_1+\pi_3(\Phi_g^{-1}(i))-\frac{K-1}{2}-1,j_2+\pi_3(\Phi_g^{-1}(i))-\frac{K-1}{2}-1}\\
&=\sum_{i=1}^{MK^2} \sum_{r=1}^R a_{n,r} \cdot b_{r,i} \cdot X_{m,j_1+\pi_3(\Phi_g^{-1}(i))-\frac{K-1}{2}-1,j_2+\pi_3(\Phi_g^{-1}(i))-\frac{K-1}{2}-1}\\
&=\sum_{i=1}^{MK^2} w_{n,i} \cdot X_{m,j_1+\pi_3(\Phi_g^{-1}(i))-\frac{K-1}{2}-1,j_2+\pi_3(\Phi_g^{-1}(i))-\frac{K-1}{2}-1}\\
&=\sum_{m=1}^{M}\sum_{k=1}^K\sum_{k_2=1}^K w_{n,m,k_1,k_2} \cdot X_{m,j_1+k_1-\frac{K-1}{2}-1,j_2+k_2-\frac{K-1}{2}-1}\\
&=(\convtwod{W}{X})_{n,j_1,j_2}.
\end{align*}
This completes the proof of the claim and hence also the lemma.
\end{proof} 

Note that in the statement, the embedding of $A'$ as a tensor does not permute its indices at all, only adding two more dimensions of size 1. Meanwhile, $B'$ is subjected to the same index transform $g^{-1}$ as the original matrix $W'$.

By applying the lemma twice, first for $U'\cdot (S'V')$, then for $U'\cdot V'$, we deduce that we can replace a convolutional layer with the three induced by the SVD decomposition.

\clearpage
\section{Experimental setup details}\label{experiment_setup_details}
\label{apd:experiment_details}

Python code is publicly available at \url{https://github.com/educating-dip/svd_dip}.

For the Lotus root dataset \cite{bubba2016lotus_paper}, we use the forward operator matrix that has been provided with the dataset to implement fan-beam geometry. We use a TV reconstruction from all 120 angles as our ground truth reference, while our reconstruction task uses a sparse-view setting with 20 angles. The data is real-measured and thus already contains noise.

For the medical datasets, we use the ODL library \cite{adler2018odl} with the CUDA-accelerated ASTRA \cite{aarle2015astra} backend to implement the parallel-beam geometries with 200 and 1000 angles, respectively.
We simulate projections with (pre-log) Poisson noise, i.e., the full post-log model is given by
\begin{equation*}
    A x + \nu = y, \qquad \nu = -A x - \ln(N_1 / N_0), \qquad N_1 \sim \text{Pois}(N_0 \exp(-A x)),
    \label{eq:discrete_model_lodopab}
\end{equation*}
where $N_0=4096$ is the number of photons per detector pixel for an empty scan.
Here, $x$ denotes the linear attenuation coefficients obtained from the Hounsfield unit values $x_\text{HU}$ stored in DICOM files via $x = (20-0.02) x_\text{HU} / 1000 + 20$.
After simulation, both images and projection data are divided by $\mu_\mathrm{max} = {81.35858}$ to normalize images into the range $[0,1]$.
We use the appropriate Poisson regression loss that maximizes the likelihood under this model as our data discrepancy loss,
\begin{align}
 L_\mathrm{Pois}(A\cdot{}\,|\,y) = -\sum_{j=1}^m N_0 \exp(-y_j\mu_\mathrm{max}) (-(A\cdot{})_j\mu_\mathrm{max} + \ln(N_0)) - N_0 \exp(-(A\cdot{})_j\mu_\mathrm{max}),
 \label{eq:pois_reg_loss}
\end{align}
instead of the term $||A\cdot-y||_2^2$ in \autoref{eqn:objective} for all methods.

\begin{table}[h]
    \centering
    \begin{tabular}{llll}
        Dataset to reconstruct & Noise & Data loss & $\gamma$ \\\hline
        Lotus root (Sparse 20) & real-measured & $\frac{1}{n}||A\cdot-y||_2^2$ & 1e-4 \\
        LoDoPaB (Sparse 200): Chest & Poisson & $L_\mathrm{Pois}(A\cdot{}\,|\,y)$ & 4\\
        Mayo (Sparse 200): Chest, Abdomen, Head & Poisson & $L_\mathrm{Pois}(A\cdot{}\,|\,y)$ & 7\\
        Mayo (Sparse 1000): Chest, Abdomen, Head & Poisson & $L_\mathrm{Pois}(A\cdot{}\,|\,y)$ & 7\\
    \end{tabular}
    \caption{Noise type, data losses and regularization parameters used for the different datasets. The respective data loss is used in place of $||A\cdot-y||_2^2$ in the objective \autoref{eqn:objective}. The same choices are used for DIP, EDIP and SVD-DIP.}
    \label{tab:tv_regularization}
\end{table}

\FloatBarrier
\clearpage
\section{U-Net architecture choice and optimization hyperparameters}
\label{apd:unet_arch_choice}


\begin{figure}[htb]
 %
 %
\floatconts
  {fig:unet}
  {\caption{U-Net architectures}}
  {
  \subfigure[{ U-Net used for all experiments on Ellipses-Lotus and Ellipses-LoDoPaB, as well as for non-pretrained DIP on LoDoPaB-Mayo.}]{\includegraphics[width=0.75\linewidth]{plots/Unet_ellipses.pdf}}\\[1.5em]
  \subfigure[{ U-Net used for EDIP and SVD-DIP on LoDoPaB-Mayo. Pretrained network weights were taken from \cite{Baguer_2020}.}]{\includegraphics[width=0.75\linewidth]{plots/Unet_lodopab.pdf}}
  }
\end{figure}

{
We use U-Net architectures for all our experiments, which are shown in \autoref{fig:unet}.
We use Adam to optimize the objective \autoref{eqn:objective}.
Our default learning rate is 1e-4, and we apply a gradient clipping with maximum norm defaulted to 1.

We find that the gradient clipping maximum norm needs to be \emph{well-adjusted} in order to be able to optimize a DIP when using an \emph{output sigmoid activation}.
This is the case for the non-pretrained DIP on LoDoPaB-Mayo, where we fine-tune the gradient clipping maximum norm to 1e-3.
However, in all other results shown in the main text we use the default gradient clipping maximum norm of 1.
For Ellipses-Lotus and Ellipses-LoDoPaB, we use the architecture shown in \autoref{fig:unet}(a), which is the same as the one used in \cite[Table B5]{Baguer_2020} for LoDoPaB (except for the fact that we use group norm instead of batch norm layers). 

For LoDoPaB-Mayo, we utilize the publicly available network parameters of FBP+U-Net on LoDoPaB/LoDoPaB-200 from \cite{Baguer_2020} to initialize EDIP and SVD-DIP using the architecture in \autoref{fig:unet}(b).
We note that this architecture is sub-optimal for non-pretrained (vanilla) DIP reconstruction. 
We observe that different architectures can be favorable for DIP compared to EDIP and SVD-DIP, and vice versa.
We use the FBP+U-Net architecture from \cite{Baguer_2020} (\autoref{fig:unet}(b)) for EDIP and SVD-DIP on LoDoPaB-Mayo, which we find to perform comparatively well on Chest and Abdomen, while for non-pretrained DIP on LoDoPaB-Mayo we use the architecture shown in \autoref{fig:unet}(a), based on the DIP architecture in \cite{Baguer_2020}.

As an ablation study and to ensure a fair comparison between chosen architectures, we include additional investigations. 
We first pretrain on LoDoPaB with the architecture shown in \autoref{fig:unet}(a) for 40 epochs. Here we report our findings. 
\begin{itemize}
    \item \autoref{fig:scatter_different_arch_vanilla_dip} and \autoref{fig:psnrtrace_lodo_mayo_1000_different_arch_vanilla_dip} show results when using the architecture in \autoref{fig:unet}(b) for all methods, including the non-pretrained DIP. The non-pretrained DIP performs considerably worse in terms of max PSNR when compared to our main results in \autoref{fig:scatter_multiplot} (left) and \autoref{fig:psnrtrace_lodo_mayo_1000}, where the architecture in \autoref{fig:unet}(a) is used for the non-pretrained DIP.
    \item \autoref{fig:scatter_different_arch} and \autoref{fig:psnrtrace_lodo_mayo_1000_different_arch} show results when using the architecture in \autoref{fig:unet}(a) for all methods, including EDIP and SVD-DIP. While the final PSNR of EDIP and SVD-DIP here is better in many cases compared to our main results in \autoref{fig:scatter_multiplot} and \autoref{fig:psnrtrace_lodo_mayo_1000}, this advantage comes at the cost of highly fine-tuned optimization hyperparameters. Specifically, non-pretrained DIP worked best with a learning rate of 1e-4 and a gradient clipping maximum norm of 1e-3. For EDIP, we needed to lower the learning rate to 1e-5, again with a gradient clipping maximum norm of 1e-3.
    For SVD-DIP instead, the gradient clipping maximum norm needed to be set to 1, while using a learning of 1e-5 on Chest and Abdomen, and a learning rate of 1e-4 on Head.
    Part of the difficulty in tuning hyperparameters can be attributed to the sigmoid output activation, which we observe to require a well-tuned gradient clipping maximum norm.
    In \autoref{fig:scatter_different_arch} and \autoref{fig:psnrtrace_lodo_mayo_1000_different_arch}, the lowered learning rate (1e-5) of EDIP naturally slows down its overfitting, so the rather high final PSNR values of EDIP are partially just an effect of slow optimization, thus not a clear indicator of stability; indeed, slow overfitting is still observed, in contrast to SVD-DIP, which remains stable.
    We also note that the max PSNR of EDIP for Chest and Abdomen in \autoref{fig:scatter_different_arch} (left) is on par when compared to our main results in \autoref{fig:scatter_multiplot} (left), so unlike the non-pretrained DIP, in these cases EDIP does not benefit from the architecture in \autoref{fig:unet}(a) in terms of max PSNR.
\end{itemize}

To summarize our findings: the standard DIP usually benefits from overparameterization, omitting skip connections at high-resolution scales, and from a final sigmoid activation (to a different degree depending on the image class). Pretrained DIP (EDIP and SVD-DIP) can leverage skip connections, and while a final sigmoid activation can also improve EDIP and SVD-DIP, it needs a careful fine-tuning of the optimization hyperparameters. 
}

\begin{figure}[h!tbp]
\floatconts
  {fig:scatter_different_arch_vanilla_dip}
  {
  \caption{This figure is like \autoref{fig:scatter_multiplot} (left), except that DIP here also uses the architecture shown in \autoref{fig:unet}(b).}
  }{
\includegraphics[width=0.45\textwidth]{plots/scatter_mayo_end_peak_vanilla_dip_with_fbp_unet_arch.png}
\hfill
\vspace{-1.5em}
  }\vspace{-0.5em}
\end{figure}

\begin{figure}[t]
 %
 %
\floatconts
  {fig:psnrtrace_lodo_mayo_1000_different_arch_vanilla_dip}
  {\caption{This figure is like \autoref{fig:psnrtrace_lodo_mayo_1000}, except that DIP here also uses the architecture shown in \autoref{fig:unet}(b).
 }}
  {\includegraphics[width=1\linewidth]{plots/plot_lodopab_mayo_multiple_vanilla_dip_with_fbp_unet_arch.png}
  \vspace{-3em}}\vspace{-1.5em}
\end{figure}

\begin{figure}[h!tbp]
\floatconts
  {fig:scatter_different_arch}
  {
  \caption{This figure is like \autoref{fig:scatter_multiplot}, except that EDIP and SVD-DIP here also use the architecture shown in \autoref{fig:unet}(a).}
  }{
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{plots/scatter_mayo_end_peak.png}
\end{minipage}\hfill
\begin{minipage}{0.54\textwidth}
\includegraphics[width=\textwidth]{plots/image_plot_mayo.pdf}
\end{minipage}
\vspace{-1.5em}
  }\vspace{-0.5em}
\end{figure}

\begin{figure}[t]
 %
 %
\floatconts
  {fig:psnrtrace_lodo_mayo_1000_different_arch}
  {\caption{This Figure is like \autoref{fig:psnrtrace_lodo_mayo_1000}, except that EDIP and SVD-DIP here also use the architecture shown in \autoref{fig:unet}(a).
 }}
  {\includegraphics[width=1\linewidth]{plots/plot_lodopab_mayo_multiple.png}
  \vspace{-3em}}\vspace{-1.5em}
\end{figure}


\clearpage
\FloatBarrier


\clearpage
\section{Additional results}

\input{plots/table_lotus.tex}
\input{plots/table_lodo.tex}
\input{plots/table_mayo_C_1000.tex}
\input{plots/table_mayo_C_200.tex}
\input{plots/table_mayo_L_1000.tex}
\input{plots/table_mayo_L_200.tex}
\input{plots/table_mayo_N_1000.tex}
\input{plots/table_mayo_N_200.tex}
    \begin{figure}[htbp]
     %
     %
    \floatconts
      {fig:psnrtrace_lodo_mayo_200}
      {\caption{The mean and SD over 3 runs of the optimization of DIP, EDIP and SVD-DIP on LoDoPaB-Mayo (Sparse 200) for samples C009, L012 and N005. %
      Each line represents the mean over 3 runs.
      }}
      {\includegraphics[width=1\linewidth]{plots/plot_lodopab_mayo_200_multiple_pretrained_dip_with_fbp_unet_arch.png}}
    \end{figure}
\FloatBarrier
\section{Using TV for Early Stopping}
Simple strategies for early stopping (ES) are hard to come by for DIP or its variants; there is extensive literature on the subject. %
Most ES approaches are tailored to either a specific problem or model, and often do not generalize to different settings. For example, the interesting approach proposed by \cite{spectral_bias} requires a specialized architecture, while \cite{Jo_2021_ICCV} suggest a method for ES in the context of denoising based on Stein unbiased risk estimator (SURE). 
%
First we briefly investigated whether the prior (or regulariser) could act as a proxy for an ES criterion, and discuss SURE later. Specifically we investigated whether it is possible to use the TV for ES. From \autoref{fig:TVLoss_lodo}, the TV fails to yield no useful information about the PSNR curve. The TV trends upwards, with no discernible difference in this trend when the PSNR falls, making it a poor tool for predicting when overfitting occurs.

\begin{figure}[htbp]
    \floatconts        
      {fig:TVLoss_lodo}
      {\caption{PSNR and mean loss output TV of DIP on LoDoPaB.}}
      {\includegraphics[width=0.9\linewidth]{plots/TVLoss_plot_lodo.png}}
      \vspace{-1em}
    \end{figure}


\FloatBarrier

\section{Is SURE a viable Method for CT?}\label{SURE}
One naturally may ask whether a DIP which utilizes Stein's unbiased risk estimator (SURE) could serve as a better baseline than the DIP we have used. Indeed, SURE has reported state-of-the-art results on several image-restoration tasks such as de-noising, de-blurring, and super-resolution.
However, we argue that its applicability to more ``more ill-posed" inverse problems in imaging is currently an open field of research. Recall that the SURE is an unbiased estimate of the mean square error (MSE). Clearly, the MSE has an explicit dependence on the unknown ground truth image. In several settings, there have been very successful ways to estimate the MSE. Initially proposed by Stein for the independent, identically distributed (i.i.d.) Gaussian model, SURE has been extended to exponential distributions (GSURE) by  \cite{Eldar_2009}. 
Lately, GSURE has been successfully applied in many works  \cite{unsuv_SURE,img_rest_GSURE,Jo_2021_ICCV}, which have demonstrated excellent performance of the method on de-noising, de-blurring, super-resolution or, compressed sensing tasks.

In the presence of an operator, the use of SURE is less direct than for de-noising. For example, in a Gaussian linear model (e.g., $y = Ax + \nu $), the projected GSURE provides an unbiased estimate of the projected MSE, which is the expected error of the projections in the column space of the operator $A$. Hence, when the matrix $A$ is rank deficient, SURE is a sub-optimal estimator as it does not estimate the orthogonal complement. Indeed, for very ill-posed inverse problems, e.g., in the highly under-sampled setting (i.e., rank-deficient setting), \cite{unsuv_SURE}  show that the GSURE-based projected MSE is a poor approximation of the actual MSE. To address this issue, \cite{Ensure} propose ENSURE, which generalizes the classical SURE and GSURE formulation where the images are sampled by different operators $A$, chosen randomly from a set. Unfortunately, ENSURE is not applicable to CT problems in conjunction to the unsupervised DIP framework.

\begin{figure}[htbp]
    \floatconts        
      {fig:SURE}
      {\caption{DIP with LS loss compared to DIP with SURE loss for different tasks on the Lotus root. For denoising (first column), we tested the network both without and with skip connections. For deblurring (second column), a well-conditioned ($\kappa(A) \sim 14$) and a badly-conditioned ($\kappa(A) \sim 1335$) problem were tested, both using skip connections in the network. For CT reconstruction (right), we use the sparse 20-angle setting like in the main experiments {(but without TV regularization for both methods)}.}}
      {\includegraphics[width=\linewidth]{plots/plot_sure.png}}
      \vspace{-1em}
    \end{figure}

To confirm these observations, we run the following set of experiments. 
\autoref{fig:SURE} shows that DIP-SURE significantly outperforms DIP when optimized with the least-square objective (DIP-LS) for the de-noising task, where we use the lotus ground truth image $x$ and simulate the noisy image $y$ by adding white noise $\nu \sim \mathcal{N}(0, \sigma^2I)$ to $x$, with $\sigma$ being half of the mean of $|x|$. The difference in the performance between DIP-SURE and DIP-LS is further accentuated if the architecture uses skips connections at every depth. Note that in the above works on SURE, all use a U-Net architecture equipped with skip connections at every depth since skip connections boost the network capacity to overfit to noise.

We then compare DIP-SURE vs. DIP-LS on Gaussian deblurring tasks. We construct two matrices $A$ using the same Gaussian kernel but applying different Tikhonov regularization, thus adding either to the diagonal entries the mean of all diagonal entries, or $0.01$ percent of the diagonal mean. This results in two  blurring matrices with different condition numbers (i.e., $\kappa(A) \sim 14$ and $\kappa(A) \sim 1335$ respectively), i.e. different degree of ill-posedness. DIP-SURE outperforms DIP-LS in the mildly ill-conditioned case, whilst, as we would expect, fails on the more ill-posed task. Similarly, on the Lotus reconstruction tasks, due to ill-posedness of the reconstructive task, DIP-SURE fails to match DIP-LS. 
In CT reconstruction, the operator usually has very small singular values and for sparse-view scans it is also rank-deficient, rendering the problem ill-posed \cite{Buzug2011}.
In sum, our investigation suggests that SURE is not a viable option for our setting.
\end{document}
