\documentclass{article}
%\usepackage[a4paper, portrait, margin=1.1811in]{geometry}
\usepackage[a4paper, portrait, margin=1.0in]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{helvet}
\usepackage{etoolbox}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{xcolor} 
\usepackage[colorlinks, citecolor=cyan]{hyperref}
\usepackage{caption}
\captionsetup[figure]{name=Figure}
\graphicspath{ {./images/} }
\usepackage{scrextend}
\usepackage{fancyhdr}
\usepackage{graphicx}
\newcounter{lemma}
\newtheorem{lemma}{Lemma}
\newcounter{theorem}
\newtheorem{theorem}{Theorem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{multirow}

\fancypagestyle{plain}{
	\fancyhf{}
	\renewcommand{\headrulewidth}{0pt}
	\renewcommand{\familydefault}{\sfdefault}
}

%\pagestyle{fancy}
\makeatletter
\patchcmd{\@maketitle}{\LARGE \@title}{\fontsize{16}{19.2}\selectfont\@title}{}{}
\makeatother

\usepackage{authblk}
\renewcommand\Authfont{\fontsize{10}{10.8}\selectfont}
\renewcommand\Affilfont{\fontsize{10}{10.8}\selectfont}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\setlength{\affilsep}{2em} 
\newsavebox\affbox
\author[1]{\textbf{Michele Pascale (https://orcid.org/0000-0001-6853-2685)}}
\author[2]{\textbf{Vivek Muthurangu (https://orcid.org/0000-0002-4292-6456)}}
\author[2]{\textbf{Javier Montalt Tordera (https://orcid.org/0000-0003-1956-1409)}}
\author[3,4]{\textbf{Heather E Fitzke (https://orcid.org/0000-0003-4713-9985)}}
\author[3,5]{\textbf{Gauraang Bhatnagar (https://orcid.org/0000-0001-6607-1117)}}
\author[3,6]{\textbf{Stuart A Taylor (https://orcid.org/0000-0002-6765-8806)}}
\author[2]{\\\textbf{Jennifer Steeden (https://orcid.org/0000-0002-9792-2022)$^{*}$}}

\affil[1]{Department of Medical Physics and Bioengineering, UCL, London, UK}
\affil[2]{UCL Institute of Cardiovascular Science, UCL, London, UK}
\affil[3]{Centre for Medical Imaging, UCL, London, UK}
\affil[4]{Queen Mary University of London, London, UK}
\affil[5]{Frimley Health NHS Trust, Frimley, UK}
\affil[6]{University College London NHS Trust, London, UK}

\affil[{$^{*}$}]{\small \textbf{Corresponding author.} \textit{
				\textit{E-mail address: \color{cyan}jennifer.steeden@ucl.ac.uk}}}
\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{12pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{12pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}

\titleformat{\section}{\normalfont\fontsize{10}{15}\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\normalfont\fontsize{10}{15}\bfseries}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\normalfont\fontsize{10}{15}\bfseries}{\thesubsubsection.}{1em}{}

\titleformat{\author}{\normalfont\fontsize{10}{15}\bfseries}{\thesection}{1em}{}

\title{\textbf{\huge CLADE: Cycle Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic Medical Images}}
\date{} 

\begin{document}

\pagestyle{headings}
\newpage
\setcounter{page}{1}
\renewcommand{\thepage}{\arabic{page}}

\newcolumntype{b}{X}
\newcolumntype{s}{>{\hsize=.6\hsize}X}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\captionsetup[figure]{labelfont={bf},labelformat={default},labelsep=period,name={Figure }}	\captionsetup[table]{labelfont={bf},labelformat={default},labelsep=period,name={Table }}
\setlength{\parskip}{0.5em}

\maketitle

\noindent\rule{15cm}{0.5pt}

\begin{abstract}
Three-dimensional (3D) imaging is extremely popular in medical imaging as it enables diagnosis and disease monitoring through complete anatomical coverage. Computed Tomography or Magnetic Resonance Imaging (MRI) techniques are commonly used, however, anisotropic volumes with thick slices are often acquired to reduce scan times. Deep learning (DL) can be used to recover high-resolution features in the low-resolution dimension through super-resolution reconstruction (SRR). However, this often relies on paired training data which is unavailable in many medical applications. We describe a novel approach that only requires native anisotropic 3D medical images for training. This method relies on the observation that small 2D patches extracted from a 3D volume contain similar visual features, irrespective of their orientation. Therefore, it is possible to leverage disjoint patches from the high-resolution plane, to learn SRR in the low-resolution plane. Our proposed unpaired approach uses a modified CycleGAN architecture with a cycle-consistent gradient mapping loss: Cycle Loss Augmented Degradation Enhancement (CLADE). We show the feasibility of CLADE in an exemplar application; anisotropic 3D abdominal MRI data. We demonstrate superior quantitative image quality with CLADE over supervised learning and conventional CycleGAN architectures. CLADE also shows improvements over anisotopic volumes in terms of qualitative image ranking and quantitative edge sharpness and signal-to-noise ratio. This paper demonstrates the potential of using CLADE for super-resolution reconstruction of anisotropic 3D medical imaging data without the need for paired training data. \\ \\
\textbf{\textit{Keywords}}: \textit{Anisotropic Super-Resolution; Deep Learning; CycleGAN; Domain Adaptation; Accelerated MRI; 3D Imaging.}
\end{abstract}
\noindent\rule{15cm}{0.5pt}

\section{Introduction}
Three-dimensional (3D) Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) have become a vital part of medical diagnosis, enabling comprehensive assessment of anatomy. However, 3D acquisitions are often time-consuming, resulting in long overall scan times and/or artefacts due to physiological or voluntary motion, e.g. breathing. A common solution to this problem is anisotropic imaging, which can be acquired more rapidly but results in high in-plane resolution with thick slices. This results in low through-plane resolution, which can be improved through conventional interpolation (e.g. nearest neighbour, bilinear, bicubic, B-spline or Lanczos resampling). However, these approaches often result in ``blocky'' edges or blurring, particularly when performing multi-planar reformatting (MPR). An alternative approach is super-resolution reconstruction (SRR), which can recover high-resolution features \cite{SR_med_img, SR_med_img_2, SR_med_img_3} with high signal-to-noise ratio (SNR) \cite{better_images_faster} from low-resolution image data. The main problem with conventional SRR is that it is often hindered by long reconstruction times and sub-optimal recovery of detail. Recently, Deep Learning (DL) has transformed SRR with the ability to produce realistic high-resolution images more rapidly than conventional SRR. DL-SRR has been successfully applied to MRI \cite{brain_super_res,brain_super_res_2,Almansour2021DeepLS}, and CT imaging \cite{ct_sr_chest}. However, most DL-SRR methods rely on supervised learning, where training is performed on large datasets containing paired low-resolution (LR) and high-resolution (HR) images. Unfortunately, in many 3D medical applications, high-resolution isotropic images are unavailable, preventing the use of supervised DL-SRR approaches. 

In this paper, we propose an unsupervised DL-SRR approach that only requires native anisotropic 3D medical images. The framework relies on the observation that small two-dimensional (2D) patches extracted from a 3D volume contain similar visual features, irrespective of their orientation. Therefore, we propose a 2D patch-based approach that learns HR features from the in-plane direction, to guide DL-SRR in the through-plane direction. In this study, we develop a modified Cycle-Consistent Adversarial Network (CycleGAN) to perform unpaired DL-SRR for anisotropic 3D medical images, which we refer to as CLADE (\emph{Cycle Loss Augmented Degradation Enhancement}). CycleGAN's \cite{ZhuPIE17} have been successfully used to perform unpaired image-to-image super-resolution in real-world images \cite{cyclegan_SR, cyclegan_SR2, ct_cyclegan}, with recent approaches showing promise in medical imaging \cite{DASR, UDEAN}. CLADE differs from previous CycleGANs used in medical imaging through the inclusion of: 1) A weight demodulation process in the generator to reduce artefacts caused by image-to-image domain transformation \cite{gladh2021image}, and 2) An optional cycle-consistent gradient mapping loss, consisting of a mixed loss \cite{mixed_grad} between the cycle-consistency and gradient loss. 

The specific aims of this paper are: a) To develop a modified CycleGAN architecture to perform unsupervised DL-SRR of anisotropic 3D medical imaging data, b) To assess the impact of the proposed architectural changes and modified cycle-consistent gradient mapping loss, c) To assess the resultant image quality from the unsupervised CLADE network against a supervised DL-SRR network, and d) To assess the overall image enhancement achieved by CLADE through quantitative assessment (SNR and edge sharpness) and qualitative image scoring. To demonstrate CLADE's utility in medical imaging, we evaluate its performance in a challenging exemplar application; anisotropic 3D MRI images of the abdomen. 

\section{Methodology}

\subsection{Conventional CycleGAN Architecture}
The CycleGAN \cite{ZhuPIE17} is a variant of a Generative Adversarial Network (GAN) \cite{goodfellow_GAN}, which consists of a generator network, $G$, which learns to produce an output, $y' = G(z) \in Y$, (where $Y$ is the target distribution, with elements $y \in Y$, and $y' \approx y$) from some random input noise tensor, $z \in Z$, (where $Z$ is the noise distribution). The generator network is paired with a discriminator network, $D$, which learns to distinguish $y$ from $y'$ (the true vs. generated data, respectively). The generator and discriminator models are trained using a value function, $V$, in a zero-sum, two-player minimax game using an adversarial loss function (Equation \ref{eq:adversarial_loss}):

\begin{equation}
\begin{aligned}
\min _G \max _D V(D, G)=\mathbb{E}_{y \sim p_{\text {data }}(y)}[\log D(y)]+\mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z)))].
\label{eq:adversarial_loss}
\end{aligned}
\end{equation}

The CycleGAN expands this concept to enable image-to-image style transfer between two disjoint image collections $X$ and $Y$ (with images defined as $x \in X$ and $y \in Y$). The CycleGAN attempts to translate features from domain $X$ to domain $Y$ (and vice-versa) by simultaneously learning the forward and backward generator mapping functions, $G_{X}: X \rightarrow Y$ and $G_{Y}: Y \rightarrow X$, respectively. Furthermore, to constrain the space of possible mapping functions, cycle-consistency is imposed where the forward and backward cycle-consistency mapping functions are defined as $G_{X}(G_{Y}(y)) = y'$, where $y' \approx y$ and $G_{Y}(G_{X}(x)) = x'$, where $x' \approx x$, respectively (see Figure \ref{fig:cyclegan_arch}a and \ref{fig:cyclegan_arch}b). This results in an approximately bijective and, therefore, invertible mapping between the two domains. Each generator ($G_X$ and $G_Y$) is paired with a corresponding discriminator, $D_X$ and $D_Y$, respectively, which are trained to classify (discriminate) whether an image from the opposite domain is real or generated (see Figure \ref{fig:cyclegan_arch}c).

The overall loss function for a conventional CycleGAN consists of the weighted sum of an adversarial loss and a cycle-consistency loss. The adversarial loss is calculated between the generator mapping functions and their respective discriminators for both the forward and backward generator mappings, as defined in Equations \ref{eq:cyc_adversarial_loss_1} and \ref{eq:cyc_adversarial_loss_2}, respectively:

\begin{equation}
\begin{aligned}
\mathcal{L}_{\text {adv}}(G_{X}, D_{Y})
&=\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\log D_{Y}(y)\right]\\
&+\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\log (1 - D_{Y}(G_{X}(x))\right]\\
\label{eq:cyc_adversarial_loss_1}
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
\mathcal{L}_{\text {adv}}(G_{Y}, D_{X})
&=\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\log D_{X}(x)\right]\\
&+\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\log (1 - D_{X}(G_{Y}(y))\right]\\
\label{eq:cyc_adversarial_loss_2}
\end{aligned}
\end{equation}

The intrinsic cycle-consistency loss is calculated using the modified cycle-consistent mapping function, defined in Equation \ref{eq:cycle_consistency}:

\begin{equation}
\begin{aligned}
\mathcal{L}_{\mathrm{cyc}}(G_{X}, G_{Y}) & =\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\|G_{Y}(G_{X}(x))-x\|_1\right] \\
& +\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\|G_{X}(G_{Y}(y))-y\|_1\right]
\label{eq:cycle_consistency}
\end{aligned}
\end{equation}

Therefore the overall loss is then calculated as follows:

\begin{equation}
\begin{aligned}
\mathcal{L}_{\text {GAN}}(G_{X}, G_{Y}, D_{X}, D_{Y}) &=\mathcal{L}_{adv}(G_{X}, D_{Y})\\
&+\mathcal{L}_{adv}(G_{Y}, D_{X})\\
&+\lambda_{cyc}\mathcal{L}_{cyc},
\label{eq:cyclegan_final_loss}
\end{aligned}
\end{equation}

where $\lambda_{cyc}$ is the weighting for the cycle-consistency loss term.

\subsection{CLADE Architecture}
Similar to the conventional CycleGAN architecture by Zhu et al. \cite{ZhuPIE17}, the CLADE architecture consists of a pair of generator networks, $G$ (each containing an encoder block, multiple residual blocks and a decoder block) and a pair of discriminator networks, $D$ (each containing a PatchGAN \cite{patchGAN}). The generator architectures in CLADE are similar to the conventional CycleGAN, with six residual blocks \cite{resnets} (Figure \ref{fig:mod_cyclegan_arch_gen}). However, unlike the conventional CycleGAN, we replaced the instance normalisation in the convolution layers with a weight demodulation process. This aims to reduce the presence of droplet or block-like noise artefacts. The proposed approach is similar to that used in StyleGAN2 \cite{StyleGAN2}, which replaces conventional instance normalisation with a weight modulation and demodulation step. In StyleGAN2, the modulation step is used to remove the stylistic influence, $s$ (intrinsic to the StyleGAN architectures), from the output convolutional feature maps $w$, by scaling the weights of the convolutional filters such that $w' = s \cdot w$. As our architecture does not use style vectors, we can assume that $s = 1 = w' = w$, therefore removing the need for the modulation step. However, we retain the demodulation step to scale each output feature map, $j$, to unit standard deviation by a factor of $1 / \sigma_{j}$ (see Equation \ref{eq:weight_demod}).
\begin{equation}
\begin{aligned}
\left. w''_{ijk} = w'_{ijk} \middle/ \sqrt{\sum_{ik} {w'_{ijk}}^{2} + \epsilon}, \right.
\label{eq:weight_demod}
\end{aligned}
\end{equation}
where $i$ denotes the index of the input feature map, $j$ enumerates the output feature maps, and $k$ enumerates the spatial dimensions of the convolutional filter itself. Furthermore, we chose $\epsilon = 1e^{-8}$ as our numerical stability constant. This assumes that all input activation maps are independent and identically distributed (i.i.d) random variables derived from a standard Gaussian distribution, $\mathcal{N}(\mu=0, \sigma=1)$. 

The discriminator network in CLADE, $D$, is identical to the conventional CycleGAN \cite{ZhuPIE17} (Figure \ref{fig:mod_cyclegan_arch_disc}), with instance normalisation layers present, as block-like noise artefacts only occur during image generation and not during the discriminative phase. 

\subsection{Gradient Mapping Cycle-Consistency Loss}
The conventional CycleGAN architecture has been noted to suffer from deformation errors at the boundaries of soft tissues when employed on anatomical data \cite{UDEAN}. In CLADE, the network acts on local patches of the image volume, and therefore the generator has no contextual knowledge of the global image features. Taking advantage of this fact, we opted to investigate the addition of an absolute gradient mapping between the local patches of the cycled high-resolution images to reduce deformation errors, similar to a Mixed Gradient Loss \cite{mixed_grad}. This gradient mapping loss, $\mathcal{L}_{gmap}$, aims to preserve local edge sharpness between gradient maps of input images and “cycled” images: 

\begin{equation}
\begin{aligned}
\mathcal{L}_{\text {gmap}}(G_{X}, G_{Y})
&=\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\|(S_{x}(G_{X}(G_{Y}(y)))-S_{x}(y))+(S_{y}(G_{X}(G_{Y}(y)))-S_{y}(y))\|_1\right]\\
\label{eq:cycle_consistent_grad_map}
\end{aligned}
\end{equation}

where $||\cdot||_{1}$ denotes an $L_{1}$ loss, and $S_x$ and $S_y$, denote the image gradient acquired using a Sobel Operator in the $x$ and $y$ directions, respectively. We combine this gradient mapping loss with the conventional CycleGAN loss, $\mathcal{L}_{GAN}$, (see Equation \ref{eq:cyclegan_final_loss}), to define the CLADE total loss $\mathcal{L}_{total}$ as follows:

\begin{equation}
\begin{aligned}
\mathcal{L}_{\text {total}}(G_{X}, G_{Y}, D_{X}, D_{Y}) &=\mathcal{L}_{\text {GAN}}(G_{X}, G_{Y}, D_{X}, D_{Y}) + \lambda_{gmap}\mathcal{L}_{gmap},
\label{eq:final_loss}
\end{aligned}
\end{equation}

where $\lambda_{gmap}$ denotes the weighting of the gradient mapping loss. 

Throughout the remainder of the paper, ``CLADE (no $\mathcal{L}_{gmap}$)'' is used to refer to the CLADE architecture with the conventional CycleGAN loss (i.e. $\lambda_{gmap} = 0$), and ``CLADE (with $\mathcal{L}_{gmap}$)'' is used to refer to the CLADE architecture with the combined loss, including the conventional CycleGAN loss and the gradient mapping loss (empirically chosen as $\lambda_{gmap} = 50$). 

\subsection{Exemplar Imaging Dataset}
\label{sub:dataset_preproc}
Although the proposed CLADE approach can be applied to any anisotropic 3D data set, we demonstrate utility on 3D MRI images of the abdomen. Volumetric Interpolated Breath-hold Examination (VIBE) is a popular MRI technique that enables full abdominal 3D coverage with high in-plane resolution and lower through-plane resolution within an achievable breath-hold time of $\sim20$ seconds \cite{VIBE}. VIBE produces T1-weighted anisotropic 3D images and permits simultaneous evaluation of soft tissue and vasculature. 

In this study, data were retrospectively collected from 60 patients who undertook abdominal 3D VIBE imaging after gadolinium-based contrast agent administration as part of a clinical protocol. The local research ethics committee approved the study (Ref: 10/H0/720/91). Whole abdominal 3D coverage was achieved in a breath-hold of ~21s, with the following imaging parameters: coronal orientation, matrix size $\sim185 \times 330 \times 23$ (Right-Left, Head-Foot, Anterior-Posterior; RL-HF-AP), field-of-view: $\sim 225 \times 400 \times 150 mm$, resulting in in-plane resolution (RL-HF) $\sim 1.2 \times 1.2mm$ (range: $0.8-1.6mm$) with through-plane resolution (AP) $\sim 6.6mm$ (range: $5.2-7.6mm$). Of these 60 3D VIBE datasets, 45 were used for training the networks, and 15 were reserved for testing. 

\subsection{Pre-processing and Patching}
Before training the networks, the anisotropic 3D data were pre-processed using nearest-neighbour interpolation to create isotropic pixels and ensure consistency between the low-resolution and high-resolution data. The inputs to the network were then created from unpaired, random, 2D patches taken from the low-resolution ($x \in X$) and high-resolution ($y \in Y$) imaging planes of the 3D data. The patch size was chosen to be $(32 \times 32)$, as this is sufficiently small to extract local structural information from the full-size image, but large enough such that the output from the discriminator (which is of size $(8 \times 8)$) provides information from the entire patch with a full receptive field (calculated as ($34 \times 34$) \cite{receptive_field_calc}). After pre-processing, each 3D MRI dataset contained $\sim 128$ slices in the low-resolution, through-plane (AP) direction. For training, this interpolated 3D data was separated into 2D slices in the sagittal plane (high-resolution in HF, interpolated low-resolution in AP direction, $x \in X$) and the coronal plane (high-resolution in both HF and RL directions, $y \in Y$). Four random 2D patches of size ($32\times32$) pixels were then extracted independently from each coronal and sagittal slice, resulting in a total of $23068$ unpaired patches for training.

\subsection{Network Training and Inference}
\label{sub:unsup_data}
All networks were trained in Python3 using Tensorflow 2.0, on an NVIDIA RTX A6000 GPU with 48GB of available memory. Adam \cite{ADAM} was used as the optimiser, and a fixed learning rate of $2e^{-4}$ was used for the first 100 epochs, followed by a linear annealing to $0$ for the remaining 100, as described in the conventional CycleGAN literature \cite{ZhuPIE17}. Furthermore, the default momentum coefficient $\beta = 0.5$ was used throughout. The weightings were empirically chosen as $\lambda_{cyc} = 100$ for the cycle-consistency loss, with the adversarial loss of the discriminator weighted to be five times higher than the loss of the generator to promote training stability. 

At inference, the DL-SRR network can be applied to perform SRR on a full-size anisotropic 3D dataset, which is not of fixed size. Firstly, the anisotropic 3D data must be interpolated in the low-resolution (AP) direction (as above) and separated into 2D sagittal slices. Each slice (of shape $m \times n$) is deconstructed into overlapping $(32\times32)$ pixel patches and independently passed through the DL-SRR network. The stride length determines the overlap between consecutive patches - as there is a trade-off between image quality and reconstruction time, a stride length of six pixels was chosen in this study, resulting in $\sim 772$ patches per slice. The final image is formed using an overlapping patch reconstruction algorithm by stitching together the resultant patches and normalising the times each pixel was sampled (using the Hadamard product). 

\subsection{Comparison of Network Architectures and Losses}
\label{sub:sup_data}
To investigate the impact of the proposed network modifications and gradient loss on the resultant DL-SRR image quality, we trained the following cycle-consistent unsupervised networks: 1) Conventional CycleGAN \cite{ZhuPIE17} (with six residual blocks), 2) CLADE (no $\mathcal{L}_{gmap}$), and 3) CLADE (with $\mathcal{L}_{gmap}$). These networks were all trained using the 45 training datasets described above, with patch-based learning. 

In addition, we wanted to compare the CLADE image quality to a supervised DL-SRR approach. Although the development of 3D supervised networks is not feasible due to the lack of paired high-resolution isotropic 3D data, in this study, we have proposed that 2D patches in one plane appear visually similar to the other planes. Therefore, it is possible to create 2D paired synthetic training data by simulating low-resolution image patches from the high-resolution coronal images (using zero-filling of the outer portion of the data in the Fourier domain in one direction). Once trained, this network can be applied to low-resolution 2D patches taken from the sagittal plane. The supervised DL-SRR network was trained using the same architecture as the generator and loss from CLADE (with $\mathcal{L}_{gmap}$). 

The anisotropic 3D test data set (N=15) was reconstructed using each of the four networks. Image quality was quantitatively assessed by calculating the no-reference image quality metric, PIQUE (Perception-based Image Quality Evaluator) \cite{PIQUE}. As this is a 2D metric, we calculated PIQUE on all 2D slices in the volume from each patient dataset, and use the mean for comparison. To assess image consistency across the volume, the PIQUE scores were calculated from both sagittal and transverse slices in the reconstructed volume. 

\subsection{CLADE Image Quality Assessment}
The improvement in image quality between the original low-resolution data and resulting images from CLADE (with $\mathcal{L}_{gmap}$) network was assessed both quantitatively and qualitatively in the test data (N=15). Quantitative image quality was measured by calculating edge sharpness (ES) and estimated SNR. ES was quantified on 3D MPR images by measuring the maximum gradient of the pixel intensities perpendicularly across the border of four distinct anatomical regions: the abdominal aorta, liver, lower pole of the kidney and the spleen \cite{Steeden2020}. An average ES value was taken for comparison. Estimated SNR was calculated by dividing the mean signal intensity in a region of interest (ROI) drawn in the kidney (an area of contrast uptake) by the standard deviation of the pixel values within a ROI drawn in an area of no signal (air in the stomach). ES and SNR measurements were made using in-house plugins for the OsiriX DICOM viewing platform \cite{osirix}.

Qualitative image scoring was performed by two clinical MR radiologists (S.T and G.B). One sagittal and one transverse slice were extracted from each dataset for scoring, resulting in 30 image pairs. The images were presented in a random order, with the paired images displayed next to each other (randomly as low-resolution vs. CLADE (with $\mathcal{L}_{gmap}$), or CLADE (with $\mathcal{L}_{gmap}$) vs. low-resolution). The observers were asked to choose their preferred images for: 1) Best overall image quality, 2) Sharpest anatomical edges, and 3) Fewest overall artefacts. For each question, the observer could choose the low-resolution image, the CLADE (with $\mathcal{L}_{gmap}$) image, or they could rate them the ``same'' where there was no discernible difference.

\subsection{Statistical Tests}
Statistical analyses were performed using Python (\verb|statsmodels 0.13.5|). Quantitative metrics (PIQUE, ES and SNR) were compared using one-way repeated measures analysis of variance (ANOVA) with posthoc testing using Tukey Honest Significant Difference (HSD) multiple comparisons of means, to determine group-wise relationships and significant results. To ascertain the significance between qualitative ranking, each question had its image frequency grouped as (low-resolution, Same) vs (CLADE (with $\mathcal{L}_{gmap}$)), where (low-resolution, Same) denotes that all of the images in the low-resolution and ``Same'' groups are counted together and then compared against the CLADE (with $\mathcal{L}_{gmap}$) group. A Chi-squared ($\chi^{2}$) contingency test was then used for comparison (using Python, \verb|scipy 1.10.1|).

\section{Results}
\subsection{Training and Inference Times}
The training and inference times were evaluated for the four DL-SRR networks: the supervised DL-SRR model, the conventional CycleGAN, the CLADE (no $\mathcal{L}_{gmap}$) and CLADE (with $\mathcal{L}_{gmap}$). The training time was approximately the same for all unsupervised models, at $\sim 133$ hours; however, the supervised DL-SRR model was significantly shorter, taking $\sim 39$ hours. The inference time was approximately equal for all four networks, taking $\sim 4.8$ minutes for an entire volume. 

\subsection{Comparison of Network Architectures and Losses}
We observed ``blocky'' artefacts when patches were super-resolved using the conventional CycleGAN architecture. After patching, these appeared in the reconstructed volumes as grid artefacts with signal voids. Figure \ref{fig:norm_errors_cyclegan} demonstrates that these normalisation errors were successfully removed using the CLADE architecture. This was true for both CLADE networks (with and without $\mathcal{L}_{gmap}$), demonstrating the utility of the weight demodulation process in removing these artefacts when compared to instance normalisation in the conventional CycleGAN.

The PIQUE scores from the four networks can be found in Table \ref{tab:CLADE_pique_comparisons}. It can be seen that all DL-SRR networks improved the image quality over the low-resolution anisotropic 3D data. Of the four DL-SRR networks tested, the conventional CycleGAN network produced the lowest improvement in image quality. The proposed changes to the network architecture in CLADE (no $\mathcal{L}_{gmap}$) significantly improved image quality compared to the the conventional CycleGAN in both the sagittal and transverse views ($p < 0.05$). The addition of the combined cycle-consistent and gradient mapping loss resulted in a further improvement in PIQUE score, although this did not reach statistical significance ($p = 0.115$). Both CLADE networks significantly outperformed the supervised DL-SRR network ($p < 0.05$) in terms of PIQUE score. Importantly, this pattern was similar for scores calculated from both sagittal and transverse slices in the reconstructed volume. . 

The quantitative PIQUE scores are reflected in Figures \ref{fig:sagg_fig_concat} and \ref{fig:trans_fig_concat}, which show examples of the resultant image quality in one patient for all four networks in both the sagittal and transverse orientations, respectively. These figures demonstrate the incremental improvement in image quality over the conventional CycleGAN, which is achieved using CLADE (no $\mathcal{L}_{gmap}$) and CLADE (with $\mathcal{L}_{gmap}$), particularly in small structures and at the boundaries of tissues.

\subsection{CLADE Image Quality Assessment}
\label{sub:IQA}
Quantitative SNR and ES comparing the low-resolution anisotropic 3D data and CLADE (with $\mathcal{L}_{gmap}$) images are shown in Table \ref{tab:clade_vs_sup}. CLADE (with $\mathcal{L}_{gmap}$) significantly improved the quantitative edge sharpness, compared to the low-resolution data $(p < 0.002)$, with no significant difference in the estimated SNR.

Qualitative image ranking comparing the low-resolution anisotropic 3D data and CLADE (with $\mathcal{L}_{gmap}$) images are shown in Table \ref{tab:qualitative_results}. In terms of both qualitative overall image quality and qualitative edge sharpness, the CLADE (with $\mathcal{L}_{gmap}$) images were ranked better than the low-resolutionanisotropic 3D data (in $\sim67\%$ and $\sim76\%$ of cases, respectively). Additionally, the CLADE (with $\mathcal{L}_{gmap}$) images ranked the same or better than the low-resolution anisotropic 3D images in $\sim83\%$ of cases in terms of image artefacts. Figures \ref{fig:comparison_results_sagittal} and \ref{fig:comparison_results_transverse}, display examples of the visual qualitative ranking results for both sagittal and transverse slices, respectively. 

\section{Discussion and Limitations}
The main findings of this study were: 1) Unsupervised super-resolution reconstruction of anisotropic 3D medical images is feasible using CLADE, 2) The modifications in the CLADE architecture and loss enabled improved spatial resolution and reduced artefacts compared to a conventional CycleGAN, and 3) Qualitative and quantitative scoring improved after CLADE super-resolution reconstruction compared to the original anisotropic images. 

Anisotropic 3D imaging is popular in MRI, as it enables full anatomical coverage in reasonable scan times whilst still having high in-plane resolution. Anisotropic imaging is also commonly used in CT for reducing radiation dose. However, the cost of anisotropic imaging is low through-plane resolution, which can impair diagnostic utility. Super-resolution reconstruction offers a potential method of recovering high-resolution features. However, most DL-SRR techniques require paired high-resolution isotropic 3D data for training. Unfortunately, these approaches are unsuitable in 3D medical applications where only anisotropic data is available. 

In this study, we propose a novel approach to overcome this problem that relies on the idea that 2D patches have similar visual features regardless of orientation. This idea can be leveraged using unsupervised learning approaches, and in this study we use a modified CycleGAN-type architecture; CLADE. The CLADE network is trained to generate super-resolved images from low-resolution 2D patches, which are indistinguishable from true high-resolution patches. We have demonstrated in coronally acquired 3D abdominal VIBE MRI’s that this approach is feasible and is able to achieve good recovery of high-resolution features from low-resolution sagittal slices. Interestingly, although we applied CLADE to 2D patches taken from the sagittal plane, PIQUE scores were similar when calculated across sagittal and transverse slices from the reconstructed volume. This important finding demonstrates that the CLADE SRR improves image quality in all imaging planes without the need to explicitly apply it in both planes.

The main difference between CLADE and the conventional CycleGAN are the inclusion of both a weight demodulation process and gradient-mapping loss. We demonstrated the efficacy of the weight demodulation process to correct normalisation errors that sometimes occur in reconstructions using the conventional-CycleGAN approach. The qualitative and quantitative results demonstrate that our approach significantly reduces blocky artefacts and enhances the visual quality of the generated images. The gradient-mapping loss was leveraged to promote generation of images with well-defined edges and improve visual acuity without degrading anatomical structures. We showed that CLADE (with $\mathcal{L}_{gmap}$) resulted in significant improvements in quantitative and qualitative edge sharpness and overall image quality over the original low-resolution images, as well as in PIQUE scores. Importantly, CLADE (with $\mathcal{L}_{gmap}$) preserved SNR in the images and did not cause any additional artefacts.

We also investigated if a supervised DL-SRR approach was possible from anisotropic data. Specifically, high-resolution coronal patches were synthetically downsampled (to the same resolution as the sagittal slices), to create image pairs for conventional supervised training. In this study, we found that when applied to sagittal slices, the resulting images had significantly lower quality and more artefacts than CLADE, demonstrating that an unsupervised approach is optimal for our use case.

The main limitation of this study was that CLADE does not explicitly perform any motion or artefact suppression, which may occur during abdominal MRI. As a result, low-resolution images that suffer from these artefacts may not be correctly represented due to the lack of ground-truth data. Furthermore, optimisation of loss weightings and network hyperparameters were chosen experimentally via multiple network configurations. Patch sizes were also chosen this way, but further evaluation and experimentation is necessary to ensure optimal image quality. 

\section{Conclusion}
We have developed an unsupervised super-resolution reconstruction technique, CLADE, which enables the recovery of high-resolution features from anisotropic 3D medical images. We have demonstrated the technique in abdominal VIBE MRI data; however, the proposed technique is applicable to any type of anisotropic image. This enables improved visualisation of structures in the low-resolution dimension without the need for any changes to current imaging protocols.

\section{Acknowledgement}
This work was supported by UK Research and Innovation (MR/S032290/1); Innovate UK (105860); the EPSRC-funded UCL Centre for Doctoral Training in Intelligent, Integrated Imaging in Healthcare (i4health) (EP/S021930/1); and the Department of Health’s NIHR- funded Biomedical Research Centre at Great Ormond Street Hospital. 

\newpage
\section{Figures}

\begin{figure}[!ht]
  \renewcommand\figurename{Fig.}
  \captionsetup{labelsep=none}
  \centering
  \subfigure[]{\includegraphics[width=0.35\textwidth]{abstracted_forward_cycle.png}}\hspace{0.01\textwidth} 
  \subfigure[]{\includegraphics[width=0.36\textwidth]{abstracted_back_cycle.png}} \hspace{0.001\textwidth}
  \subfigure[]{\includegraphics[width=0.23\textwidth]{abstracted_cyclegan.png}}
  \caption{\ (a) Forward cycle-consistency loss, (b) Backward cycle-consistency loss and (c) Abstracted CycleGAN model}
  \label{fig:cyclegan_arch}
\end{figure}

\begin{figure}[!ht]
  \renewcommand\figurename{Fig.}
  \captionsetup{labelsep=none}
  \centering
 \includegraphics[width=1\textwidth]{G_arch_nolabel.png}
  \caption{\ CLADE generator architecture, where $G_{in}$ and $G_{out}$ define the input and output layers respectively, and 2D modulated convolution layers are denoted by ModConv}
  \label{fig:mod_cyclegan_arch_gen}
\end{figure}

\begin{figure}[!ht]
  \renewcommand\figurename{Fig.}
  \captionsetup{labelsep=none}
  \centering
 \includegraphics[width=1\textwidth]{D_arch_nolabel.png}
  \caption{\ CLADE PatchGAN discriminator architecture, where $D_{in}$ and $D_{out}$ define the input and output layers, respectively}
  \label{fig:mod_cyclegan_arch_disc}
\end{figure}


\begin{figure}[!ht]
  \renewcommand\figurename{Fig.}
  \captionsetup{labelsep=none}
  \centering
  \includegraphics[width=1\textwidth]{norm_error_figure_0.png}
  \caption{\ Example of a sagittal slice that contains normalisation errors (high-resolution in HF, low-resolution in AP) after patch reconstruction from the conventional CycleGAN (no weight demodulation), which are removed with CLADE (with $\mathcal{L}_{gmap}$ ) - magnified regions within the red box are displayed beneath each image}
  \label{fig:norm_errors_cyclegan}
\end{figure}

\begin{figure}[!ht]
  \renewcommand\figurename{Fig.}
  \captionsetup{labelsep=none}
  \centering
 \includegraphics[width=1\textwidth]{pique_ims_sagg.png}
  \caption{\ Example image quality in the sagittal view (high-resolution in HF, low-resolution in AP) from one subject in the test data set, where the magnified regions within the red box are displayed beneath each image}
  \label{fig:sagg_fig_concat}
\end{figure}

\begin{figure}[!ht]
  \renewcommand\figurename{Fig.}
  \captionsetup{labelsep=none}
  \centering
  \includegraphics[width=1\textwidth]{pique_ims_trans.png}
  \caption{\ Example image quality in the transverse view (high-resolution in RL, low-resolution in AP) from one subject in the test data set, where the magnified regions within the red box are displayed beneath each image}
  \label{fig:trans_fig_concat}
\end{figure}
 
\begin{figure}[!ht]
  \renewcommand\figurename{Fig.}
  \captionsetup{labelsep=none}
  \centering
  \includegraphics[width=1\textwidth]{qual_figs_sagg.png}
  \caption{\  Images from the qualitative results of sagittal images, displaying example images as ranked for each question and model, respectively}
  \label{fig:comparison_results_sagittal}
\end{figure}
 
\begin{figure}[!ht]
 \renewcommand\figurename{Fig.}
 \captionsetup{labelsep=none}
 \centering
 \includegraphics[width=1\textwidth]{qual_figs_trans.png}
  \caption{\ Images from the qualitative results of transverse images, displaying example images as ranked for each question and model, respectively}
  \label{fig:comparison_results_transverse}
\end{figure}

\clearpage

\section{Tables}

\begin{table}[!ht]
  \centering
  \adjustbox{max width=\textwidth}{
  \begin{tabularx}{\linewidth}{M{4cm}*{4}{>{\centering\arraybackslash}X}}
    \hline
    
    \multirow{3}{*}{Model} & & & \multicolumn{2}{c}{Orientation} \\ \\ 
    & & & Sagittal & Transverse \\
    \hline \\
    LR & & & $57.7 \pm 7.4$ & $56.9 \pm 5.9$ \\
    Supervised DL-SRR & & & $53.9 \pm 10.1$ & $53.2 \pm 5.7$ \\
    Conventional CycleGAN & & & $54.3 \pm 10.1$ & $53.9 \pm 6.3$ \\
    CLADE (no $\mathcal{L}_{gmap}$) & & & $47.1 \pm 12.1^{*}$ & $43.1 \pm 7.0^{*}$ \\
    CLADE (with $\mathcal{L}_{gmap}$) & & & $43.0 \pm 10.9^{**}$ & $40.1 \pm 6.3^{**}$ \\ \\
    \hline
  \end{tabularx}
  }
  \caption{Quantitative PIQUE scores ($\mu \pm \sigma$) as calcualted across all slices in the volumes from the low-resolution (LR) data and DL-SRR reconstructions}
  \label{tab:CLADE_pique_comparisons}
\end{table}

$^{*}$ Denotes statistical significance between CLADE (no $\mathcal{L}_{gmap}$) and the conventional CycleGAN architecture, as assessed by repeated measures ANOVA with Tukey HSD multiple comparisons of means for posthoc testing $(p < 0.05)$.

$^{**}$ Denotes statistical significance between CLADE (with $\mathcal{L}_{gmap}$) and the conventional CycleGAN architecture, as assessed by repeated measures ANOVA with Tukey HSD multiple comparisons of means for posthoc testing $(p < 0.05)$. \\ \\

\begin{table}[!ht]
  \centering
  \adjustbox{max width=\textwidth}{
  \begin{tabularx}{\linewidth}{M{4cm}*{2}{>{\centering\arraybackslash}X}}
    \hline \\
    \multirow{1}{*}{Method}
    & LR & CLADE (with $\mathcal{L}_{gmap}$) \\ \\
    \hline \\
    ES $\uparrow$ ($\mu \pm \sigma$) & $2.2 \pm 0.9$ & $2.5 \pm 1.0^{\dag}$ \\ \\
    SNR $\uparrow$ ($\mu \pm \sigma$) & $13.1 \pm 3.4$ & $13.4 \pm 3.6$ \\ \\
    \hline
  \end{tabularx}
  }
  \caption{Quantitative edge sharpness (ES) and estimated SNR as assessed between the low-resolution (LR) images and reconstruction with CLADE (with $\mathcal{L}_{gmap}$)}
  \label{tab:clade_vs_sup}
\end{table}

$^{\dag}$ Denotes statistically significant differences between CLADE (with $\mathcal{L}_{gmap}$) and LR $(p < 0.05)$, as assessed by repeated measures ANOVA with Tukey HSD multiple comparisons of means for posthoc testing $(p < 0.05)$. \\ \\

\begin{table}[!ht]
  \centering
  \adjustbox{max width=\textwidth}{
  \begin{tabularx}{\linewidth}{M{4cm}*{4}{>{\centering\arraybackslash}X}}
    \hline \\
    \multirow{1}{*}{Method}
    & LR & Same & CLADE (with $\mathcal{L}_{gmap}$) \\ \\
    \hline \\
    Q1 - Best Overall Image & $6.89\%$ & $25.86\%$ & $67.24\%^{\dag\dag}$ \\ \\
    Q2 - Sharpest Edges & $13.33\%$ & $10\%$ & $76.67\%^{\dag\dag}$ \\ \\
    Q3 - Least Artefacts & $17.24\%$ & $65.52\%$ & $17.24\%$ \\ \\
    \hline
  \end{tabularx}
  }
  \caption{Qualitative ranking of image quality as assessed between the low-resolution (LR) images and reconstruction with CLADE (with $\mathcal{L}_{gmap}$)}
  \label{tab:qualitative_results}
\end{table}

$^{\dag\dag}$ Indicates statistically significant differences between CLADE (with $\mathcal{L}_{gmap}$) and the grouping of LR \& Same $(p < 0.05)$, as assessed by $\chi^{2}$ contingency test.

\clearpage
\bibliographystyle{unsrt}
\bibliography{main}
\end{document}