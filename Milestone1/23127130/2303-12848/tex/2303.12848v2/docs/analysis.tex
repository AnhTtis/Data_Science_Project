\section{Ablation Studies}\label{sec:analysis}

% \begin{figure}[t]
% \centering
% % \vspace{-5mm}
%   \includegraphics[width=0.85\linewidth]{figs/classification_confusion_matrix.png}
%   \vspace{-3mm}
%   \caption{Confusion matrix for 11 group classes attack classification using MAE-ViT. Overall, MAE-ViT achieves high performance on every attack type.}
% %   \vspace{-5mm}
%   \label{fig:cfm}
% \end{figure}


\subsection{Parameters Analysis for Attack Reversal}
\label{ablation:parameter}
$\bullet$ \textbf{Sensitivity Analysis on Reversed Iter Numbers} We show the analysis of the effect on iteration steps in the PGD update for the reconstruction. We noticed that a larger iteration step leads to a more severe PGD attack on a given input. In our scenario, we aim to find an optimal perturbation for a given input that can reduce the mean square error loss. Here, we compare the results under three different settings, including no reconstruction, reconstruction with noise using random initialization, and reconstruction with a range of iteration steps. Figure~\ref{fig:attack_iters_vs_acc} shows the accuracy and MSE loss on each attack type under the different optimization settings. Compared to the results of no reconstruction and constructing with random gaussian noise, updating only with 1 iter step increases the performance and reduces the loss. We empirically found that while increasing the iter steps to 5 or larger, the curve becomes smooth and the result converges which means we can set up the iter steps as smaller values to reduce the computational cost.




% \begin{figure}[t]
% \centering
%      \subfigure{\includegraphics[width=0.49\linewidth]{figs/attack_iters_vs_acc.png}}
%      \subfigure{\includegraphics[width=0.49\linewidth]{figs/attack_iters_vs_loss.png}}
%      \vspace{-4mm}
%          \caption{Analysis on different number of repaired iters of PGD for reconstruction. X-axis shows the different repaired settings from left to right, including without repairing (w/o), repairing with random noise (rand.), and repairing with different iters (1,5, and 10). The left subfigure shows the accuracy and right one shows the $\mathcal{L}_{\rm mse}$ on different iter steps corresponding to eight kinds of attacks}
%              \label{fig:attack_iters_vs_acc}
%         \vspace{-2mm}
% \end{figure}


% \begin{figure}[t]
%     \centering
%      \subfigure{\includegraphics[width=0.49\linewidth]{figs/epsilon_vs_acc.png}}
%      \subfigure{\includegraphics[width=0.49\linewidth]{figs/epsilon_vs_loss.png}}
%     \caption{Analysis on different number of epsilon $\epsilon$ ranges of PGD for reconstruction. Here, we set the $\epsilon$ as 2/255, 4/255, 8/255, and 16/255.}
%     \label{fig:epsilon_vs_acc}
% \end{figure}





$\bullet$ \textbf{Sensitivity Analysis on Epsilon Range:} 
We further analysis on the effect of different epsilon $\epsilon$ ranges for $\ell_{\infty}$-norm perturbations. We set the epsilon range from small to large with $\lambda = [2/255, 4/255, 8/255, 16/255]$ and fix the update iter as 5. As Figure~\ref{fig:epsilon_vs_acc} shows the accuracy and loss variance on each attack type. When increasing the epsilon range, the accuracy increases. We empirically found that the larger epsilon doesn't get more improvement on the accuracy for overall attacks.


% \begin{figure}[t]
%     \centering
%     \vspace{-2mm}
%     \includegraphics[width=0.90\linewidth]{figs/vis_reconstruct/180_p1_615_p2_3.jpg}
%     \caption{Visualization on reconstructed samples}
%         \vspace{-4mm}
%     \label{fig:reconstructsample}
% \end{figure}


\subsection{Loss Analysis on Attack Detection}
\label{ablation:loss}
To better understand the ability of reconstruction for MAE, we visualize the histogram on the self-supervised loss distribution for adversarial examples.  We compare the loss variance between clean samples, noise samples, and different kinds of attack samples by collecting the loss on every batch during the testing-time inference. As Figure~\ref{fig:mse_loss_dist} shows, the loss variance on clean samples (colored with blue region) and the other eight attack types' samples (red curve) are large before reconstruction. The yellow curve shows the loss distribution of samples with random noise. Compared with using random noise, TRAM uses MSE loss to reconstruct the samples and demonstrates a huge loss shifting after reconstruction (colored with green region) which is more closer to the loss distribution of clean samples. The average loss difference is shown in Table~\ref{tab:reconstruction}.






% \begin{figure}{}
%   \includegraphics[width=0.95\linewidth]{figs/attack_iters_vs_acc.png}
%   \caption{The Acc. v.s. Repaired Iter Numbers. In Figure, We compare without repairing, repairing w/ random noise, and repairing with our methods using different iter numbers (1, 5, and 10) }
%   \label{fig:attn1}
% \end{figure}


% \begin{figure}{}
%   \includegraphics[width=0.95\linewidth]{figs/attack_iters_vs_loss.png}
%   \caption{The Loss. v.s. Repaired Iter Numbers. In Figure, We compare without repairing, repairing w/ random noise, and repairing with our methods using different iter numbers (1, 5, and 10) }
%   \label{fig:attn1}
% \end{figure}

% \begin{figure*}[t]
%     \centering

%      \subfigure{\includegraphics[width=0.45\linewidth]{figs/vis_gradcam/cls_2_idx_3.png}}
%     % \subfigure{\includegraphics[width=0.45\linewidth]{figs/vis_gradcam/cls_18_idx_1.png}}
%       \subfigure{\includegraphics[width=0.45\linewidth]{figs/vis_gradcam/cls_9_idx_5.png}}
%     % \subfigure{\includegraphics[width=0.45\linewidth]{figs/vis_gradcam/cls_16_idx_2.png}}

%     \caption{Visualization on Grad-Cam for different kinds of attacks. In each subfigure,  the first row shows the clean sample and its corresponding GradCam. The second row shows the adversarial sample, and its corresponding noise and GradCAM.}
%     \label{fig:gradcam}
% \end{figure*}

% \begin{figure*}[h]
% \centering
%   \includegraphics[width=0.85\linewidth]{figs/mae_tsne.png}

%   \caption{T-SNE analysis on three different model architectures for 11 group classes attack classfication. From left to right, we show the T-SNE analysis on logit features extracted from ResNet50, base-ViT, and MAE-ViT model.}

%   \label{fig:tsne}
% \end{figure*}
 

\subsection{Attention Visualization for Attack Classification}
\label{ablation:attention}
In Table~\ref{tab:classification}, we show the MAE-ViT achieves the highest performance on attack classification. To better understand the model from the attribution level, we analyze the GradCAM of noise pattern for every attack on the MAE-ViT.
We visualize the attention maps in the intermediate layers of ViT-MAE under FGSM, Patch Attack  by averaging the attention scores across all the attention heads in each layer and visualize the attention score of each token for a given query token (the
center patch). As figure~\ref{fig:gradcam} shows, the first column is the original and attacked samples, the second column is their corresponding noises, and the third and fourth columns are the attention map and the combining of attention and attacked samples. 
We extract the features from the last attention blocks of MAE-ViT and show the last attention layer of MAE-ViT enlarges the perturbation in the adversarial images which can capture the noise pattern. 
% In contrast, the base-ViT can not capture the shape of the noise pattern, demonstrating successful learning of the noise pattern for MAE-ViT. 






% \subsection{Visualization for reconstructed samples}

% We visualize the samples after reconstruction on different kinds of attacks to demonstrate how MAE reconstructs the samples.
% For a given adversarial sample, we first show their clean sample pair. Then, we show the masked-out samples with randomly masked patches. The visualization of reconstructed samples is based on the combining of predicted masked patches and unmasked patches of the given input. As Figure~\ref{fig:reconstructsample} shows, after adding perturbations on inputs, the predicted masked patches visually look closer to the original samples and the noise disappears.


\subsection{Embedding Visualization for Attack Classification}
We visualize the t-distributed stochastic neighbor embedding (tSNE) of logit representations on attack samples for three different models, including MAE-ViT, base-ViT and the ResNet50 baseline. As Figure~\ref{fig:tsne} shows, the tSNE results of MAE-ViT have a clear separation between the embeddings
from 11 group classes of attacks, suggesting that MAE-ViT indeed
learns meaningful and discriminative data representations
to classify the adversarial attack methods. On the other hand, the embedding visualization of ResNet50 shows low class-wise separability.







% \begin{figure}
%   \includegraphics[width=\linewidth]{figs/sample/attn_cls4.pdf}
%   \caption{The attention maps generated by FGSM attack $\ell_\infty=16/255$}
%   \label{fig:attn2}
% \end{figure}

% \begin{figure}
%   \includegraphics[width=\linewidth]{figs/sample/attn_cls16.pdf}
%   \caption{The attention maps generated by DeepFool}
%   \label{fig:attn3}
% \end{figure}



% \begin{figure}[h]
%     \centering
%      \subfigure{\includegraphics[width=0.90\linewidth]{figs/vis_reconstruct/126_p1_296_p2_2.jpg}}
%      \subfigure{\includegraphics[width=0.90\linewidth]{figs/vis_reconstruct/180_p1_615_p2_3.jpg}}
%       \subfigure{\includegraphics[width=0.90\linewidth]{figs/vis_reconstruct/744_p1_113_p2_14.jpg}}
%         \subfigure{\includegraphics[width=0.90\linewidth]{figs/vis_reconstruct/760_p1_381_p2_15.jpg}}
%     \caption{Caption}
%     \label{fig:reconstructsample}
% \end{figure}

  
