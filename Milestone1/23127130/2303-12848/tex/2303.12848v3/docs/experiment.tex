\section{Experiment}\label{sec:experiment}


% \subsection{reconstruction result}
% begin{table*}[]
% \centering
% \begin{tabular}{ccccccccc}
% \multicolumn{1}{r}{}            & FGSM-$\ell_\infty$        & FGSM-$\ell_\infty$         & PGD-$\ell_\infty$          & PGD-$\ell_\infty$       & CW-$\ell_2$            & BIM-$\ell_\infty$       & PGD-$\ell_\infty$           & APGD             \\
% epsilon bound                   & 4/255            & 8/255            & 8/255            & 16/255           & 1                & 8/255            & 1                & 8/255            \\ \hline
% mse loss (before)               & 0.8013           & 0.8285           & 0.8131           & 0.8372           & 0.7559           & 0.8085           & 0.7583           & 0.7872           \\
% mse loss (+noise)               & 0.7918           & 0.8221           & 0.8087           & 0.8355           & 0.7535           & 0.8039           & 0.7531           & 0.7863           \\
% \textbf{mse loss (after re.)}   & \textbf{0.6931}  & \textbf{0.7381}  & \textbf{0.7055}  & \textbf{0.7423}  & \textbf{0.6401}  & \textbf{0.7043}  & \textbf{0.6399}  & \textbf{0.6859}  \\ \hline
% robust acc (before)             & 25.00\%  & 12.50\%          & 21.87\%          & 12.50\%          & 34.37\%          & 18.75\%          & 34.37\%          & 21.87\%          \\
% robust acc (+noise)             & 28.12\%          & 12.50\%          & 22.33\%          & 12.97\%          & 40.62\%          & 25.00\%          & 40.62\%          & 31.25\%          \\
% \textbf{robust acc (after re.)} & \textbf{37.00\%} & \textbf{15.62\%} & \textbf{34.38\%} & \textbf{18.75\%} & \textbf{56.25\%} & \textbf{31.25\%} & \textbf{59.37\%} & \textbf{40.63\%} \\ \hline
% \end{tabular}
%  \caption{MAE reconstruction result on eight adversarial attacks}\
% \end{table*}





\subsection{Experimental Settings}

% This section demonstrates the experiments for performance evaluation on the three modules of TRAM. 
% We also provide the ablation studies in Sec.~\ref{sec:analysis}, including the hyper-parameter analysis for reconstruction, loss distribution analysis for MAE, Grad-CAM analysis on input saliency maps for attack samples, and t-SNE visualizations.

\begin{table*}[t]
\centering
\small
\begin{tabular}{cccccccccc}
\hline
Detector          & FPR & FGSM-$\ell_\infty$  & FGSM-$\ell_\infty$  & PGD-$\ell_\infty$   & PGD-$\ell_\infty$   & PGD-$\ell_2$   & CW-$\ell_2$    & BIM-$\ell_\infty$   & AutoAttack \\
                  &                & 4/255 & 8/255 & 8/255 & 16/255 & 1     & 1     & 8/255 & -         \\ 
                  \hline 
% SSL-Logistic      &                &       &       &       &        &       &       &       &            \\

FS~\cite{xu2017feature} & 0.2            &  0.001        &    0.000   &   0.0015     &  0.000     &  0.0235  & 0.022   &  0.000     &     0.000       \\
ND~\cite{hu2019new}         & 0.2            & 0.6844 & 0.6736 & 0.7024 & 0.6752  & 0.3535 & 0.6296 & 0.697     & 0.712       \\
TD~\cite{hu2019new}          & 0.2              & 0.089     & 0.067     & 0.107      & 0.125      &
0.066    & 0.051     & 0.1165     & 0.1045         \\ 
SSL-\textit{SimCLR}              & 0.2      & 0.2959      & 0.4081   & 0.6479  & 0.8163   & 0.4286   & 0.3980   & 0.6582 & 0.4948          \\
SSL-\textit{Rotation}                 & 0.2      &  0.6582     & 0.8367   & 0.9031
  &  0.9639  &  0.5510  & 0.3520  &   0.8934 & 0.7653    \\
\textbf{DRAM} (ours)           &         0.2       &   \textbf{0.8504}    &   \textbf{0.9680}    &    \
\textbf{0.9129}   &    \textbf{0.9837
}    &     \textbf{0.5900}  &   \textbf{0.5739}    &   \textbf{0.9024}    &    \textbf{0.8201}        \\  \hline
% $\ell_1$-norm + targeted         & -              & -     & -     & -     & -      & -     & -     & -     & -      \\\hline
\end{tabular}
\caption{Detection results on eight kinds of adversarial attacks. We compared with five baselines, including Feature Squeezing (FS), Noise-based Detection (ND), Targeted Detection (TD), SSL detection using contrastive loss (SSL-$\textit{SimCLR}$), and loss of rotation prediction (SSL-$\textit{Rotation}$). We show the true positive rate (TPR) for every attack with the fixed False positive rate as 0.2 for all methods by varying the thresholds. The numbers in bold represent the best performance.}
\label{tab:detection_result}
\end{table*}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  %表格自动换行


\begin{table*}[]
\small
\centering
\begin{tabular}{lccccccccc}
\hline
                           & FGSM-$\ell_\infty$ & FGSM-$\ell_\infty$ & PGD-$\ell_\infty$ & PGD-$\ell_\infty$ & PGD-$\ell_2$  & C\&W-$\ell_2$ & BIM-$\ell_\infty$  & AutoAttack \\
                         & 8/255      & 16/255     & 8/255   & 16/255  & 1  & 1   & 8/255    & -          \\ \hline
Standard   & 6.39   & 8.55   & 0.0 & 0.0 & 0.65 & 0.84 & 0.01 & 0.10          \\

% $\mathcal{L}_{mse}$($\downarrow$)           & 0.3870         & 0.4560      & 0.4429     & 0.4463 & 0.4593   & 0.4050    & 0.3988    & 0.4447   & 0.4335         \\
+ SimCLR~\cite{chen2020simple}       & 9.46      & 8.82     & 1.89     & 0.12   & 15.59  & 17.62       & 1.74    & 3.31          \\
% $\mathcal{L}_{cl}$($\downarrow$)           & -       & -     & -    & - & -   & -   & -  & -  & -        \\
+ Rotation~\cite{gidaris2018unsupervised}      & 11.40      & 9.85     & 1.47     & 0.98  &  0.64   & 1.02    & 3.89    & 1.49              \\

\textbf{+ \sys (ours)}               & \textbf{20.54}      & \textbf{14.14}      & \textbf{21.34}   & 
\textbf{13.12}   & \textbf{39.57}    & \textbf{41.79}    & \textbf{20.51}    & \textbf{22.4}          \\
% $\mathcal{L}_{rp}$($\downarrow$)           & -       & -     & -    & - & -   & -   & -  & -  & -        \\
% Jigzaw Prediction Acc.          & -      & -     & -     & -   & -  & -    & -    & -    & -          \\
% $\mathcal{L}_{rp}$($\downarrow$)           & -       & -     & -    & - & -   & -   & -  & -  & -        \\



\hline

MADRY~\cite{madry2017towards}     & 24.95      & 13.9      & 20.38   & 8.83   & 35.11    & 40.82    & 20.26    & 16.08          \\

% $\mathcal{L}_{mse}$($\downarrow$)           & 0.3870         & 0.4386     & 0.4483      & 0.4350   & 0.4560   & 0.4004    & 0.3934    & 0.4335    & 0.4065          \\
+ SimCLR~\cite{chen2020simple}              & 24.19     & 13.31     & 20.33   & 8.75  & 35.80    & 43.37    & 20.31    & 18.84          \\
% $\mathcal{L}_{cl}$($\downarrow$)           & -       & -     & -    & - & -   & -   & -  & -  & -        \\
+ Rotation~\cite{gidaris2018unsupervised}                 & 22.57     & 12.35     & 19.94   & 8.94  & 34.38    & 41.00    & 20.04    & 22.69          \\
\textbf{+ \sys (ours)}                & \textbf{25.43}      & \textbf{14.05}      & \textbf{23.21}   & \textbf{11.01}   & \textbf{38.71}   & \textbf{43.75}    & \textbf{23.14}    & \textbf{24.84}          \\
% $\mathcal{L}_{rp}$($\downarrow$)           & -       & -     & -    & - & -   & -   & -  & -  & -        \\
% Jigzaw Prediction Acc.          & -      & -     & -     & -   & -  & -    & -    & -    & -          \\
% $\mathcal{L}_{rp}$($\downarrow$)           & -       & -     & -    & - & -   & -   & -  & -  & -        \\

% \sys Acc.           & \yun{73.79}         &7.44      & 9.01     &  0   & 0   & 16.52    & 45.01   & 0.01    & 1.76          \\
% $\mathcal{L}_{mse}$($\downarrow$)           & \yun{-0.05}         & 0.0903      & 0.0732     & 0.0917  & 0.0873   & 0.0701   & 0.0639    & -0.0575    & 0.0835        \\
\hline

\end{tabular}
\caption{The repair results on eight kinds of attack. We compare \sys with the other two SSL baselines, including contrastive learning task (SimCLR~\cite{simclr}) and rotation prediction~\cite{gidaris2018unsupervised}. We show the robust accuracy (\%) on two Resnet50~\cite{he2016deep} models pre-trained with Standard training and $\ell_\infty$ robust training (MADRY~\cite{madry2017towards}). The numbers in bold represent the best performance.}
\label{tab:reconstruction}
\end{table*}

\paragraph{Dataset Detail}
We train and test \sys using the ImageNet dataset~\cite{deng2009imagenet}, a 1K-class large-scale vision dataset. Notice that the training process of \sys uses only the original ImageNet train set with $\sim$1M images.
To evaluate \sys, we generate the adversarial samples on the ImageNet test set~\cite{deng2009imagenet} with 50K images. Here, we consider eight attacks and generate them from two kinds of models, the  standard ~\cite{he2016deep} and robust~\cite{madry2017towards} ResNet50. 
% The details of parameters for each attack are shown in Table~\ref{tab:attack_parameter}. 
% applied to the ResNet50~\cite{he2016deep} model for detection and reconstruction. The details of parameters for each attack are shown in Table~\ref{tab:attack_parameter}. 




% \begin{table*}[t]
% \centering
% \begin{tabular}{cccccccccc}
% \multicolumn{1}{r}{}  &   & FGSM-$\ell_{\infty}$        & FGSM-$\ell_{\infty}$     & PGD-$\ell_{\infty}$        & PGD--$\ell_{\infty}$     & PGD--$\ell_{2}$     & APGD       & CW--$\ell_{2}$          & BIM--$\ell_{\infty}$         \\

%  Method &   Score        & 4/255            & 8/255            & 8/255            & 16/255           & 1                & 8/255            & 1                & 8/255            \\ \hline

%   & \textbf{Accuracy} & 0.6318 & 0.7098 &  0.6560  &  0.7325   &  0.5526  &  0.6269   &  0.5460 &  0.6518 \\  

% Logistic & \textbf{TPR} & 0.6845 & 0.7620  &  0.6994  &  0.7766 &  0.5951    &  0.6735   & 0.5939 & 0.6955  \\
% & \textbf{FPR} &  0.4200  &   0.3416  &  0.3867  & 0.3109 &  0.4892   &  0.4188    &  0.5010   &  0.3911 \\\hline

%  & \textbf{Accuracy}  & 0.7926 &    0.9324   &    0.8562     &    0.9548    &  0.6003   &  0.7849      &    0.5960     &    0.8457      \\
%  KS test & \textbf{TPR}    &   0.7171   &    0.9204   &    0.8108    & 0.9558  &  0.4286      &   0.7029     &   0.4168    &   0.7910    \\
% & \textbf{FPR} &   0.0557    &   0.0435     &    0.0531  & 0.0474   &  0.0563    &   0.0512    &    0.0455    &    0.0448    \\  \hline

%  & \textbf{Accuracy}  &       &         &        &     &        &         &        &      \\
%  $\ell_1$ detection & \textbf{TPR}    &      &       &        &   &        &        &       &       \\
% & \textbf{FPR} &       &        &         &      &       &        &        &     \\  \hline

%  & \textbf{Accuracy}  &       &         &        &     &        &         &        &      \\
%  targeted & \textbf{TPR}    &      &       &        &   &        &        &       &       \\
% & \textbf{FPR} &       &        &         &      &       &        &        &     \\  \hline

% \end{tabular}
% \caption{Detection results using statistic-based learning with reconstruction loss. We compared two kinds methods, including logistic regression and KS testing with P-Value. We report three scores, including accuracy, sensitivity (TP/TP+FN), and precision (TP/TP+FP). Numbers in bold represent the best performance.}
% \label{tab:detection1}

% \end{table*}




% \begin{figure}[h]
% \centering
%      \subfigure{
%      \includegraphics[width=0.65\linewidth]{figs/mae_roc.png}}
%      \subfigure{\includegraphics[width=0.65\linewidth]{figs/mae_cdf.png}}
%     \caption{(Top) the ROC curve for the logistic regression classifier described in section 3.3. (Bottom) the sample CDFs of loss value for clean images and eight kinds of adversarial attacks}
% \label{fig:detection_roc}
% \end{figure}

\paragraph{Model Detail}
The MAE model is based on the visual transformer (ViT) arcitecture~\cite{Visualtransformer}, which consists of the same encoder module as ViT that splits images into patch size 14$\times$14 and encodes their corresponding projected sequences. The encoder module of MAE consists of alternating layers of multiheaded self-attention (MSA) and MLP blocks. The batch normalization layer (BN) is applied before every block, and residual connections after every block. The encoded features are further processed by the decoder module for reconstruction.
% To validate the transferability of adversarial attacks, we concatenate the MLP blocks with fully connected layers behind the encoder module for general 1K image classification.
% For attack classification, we finetune MAE to the downstream classification task and evaluate the 25 adversarial classes. including the transformer-based and DNN-based architecture. 
%Table~\ref{fig:mae_result} shows the group-up results of fully finetuning the MAE-ViT. Table~\ref{fig:compare_result} shows the comparison between different baselines.




\subsection{Test-time Attack Detection}
\label{subsec:detection_result}

We compare the detection part of \sys with the following four kinds of baselines:

$\bullet$ \textbf{Feature Squeezing (FS)}: The baseline~\cite{xu2017feature} utilizes multiple data transformations such as median smoothing, bit quantization, or non-local mean to extract the squeezing features of input samples. The $\ell_1$ distance is then computed between the transformed version and the original version of inputs for detection. If the $\ell_1$ distance is larger than the threshold, the samples are rejected as adversarial.

$\bullet$ \textbf{Noise-based Detection (ND)}: This method~\cite{hu2019new} detects whether an input is adversarial or not via perturbing the input with random noise. It samples the $\epsilon$ in a normal distribution with specific radius $r$ and computes the $\ell_1$ distance as $\parallel h(x) - h(x+ \epsilon) \parallel_1$ for given input $x$. The input $x$ is detected as adversarial if the $\ell_1$ distance is large.

$\bullet$ \textbf{Targeted Detection (TD)}: The detection method~\cite{hu2019new} aims to evaluate the PGD attack algorithm on every input $x$ and record the number of steps $K$ to fool the sample $x$ from the current predicted class to targeted class. If the number of steps $K$ is larger than the threshold, we detect the sample $x$ as adversarial.

$\bullet$ \textbf{Detection with Self-Supervised Loss (SSL)}: The detection method combines SSL loss with the 2-sample KS-test. It measures the distributions of the observations from the clean and attacked samples. The samples are rejected as adversarial if the p-value exceeds a threshold. We compare \sys with two SSL tasks, including contrastive learning (SimCLR~\cite{simclr}) and rotation prediction~\cite{gidaris2018unsupervised}. 



\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{figs/mse_loss_distribution.png}
\vspace{-4mm}
         \caption{Histogram plot of self-supervised losses $\mathcal{L}_{mse}$ for eight adversarial attacks. We show the loss distribution of original images with the color blue, adversarial images as red, and adapted images as green. The figure shows that adversarial images cause a large shift in MAE loss (from blue to red). After using \sys to repair, the loss distribution of adapted images shifts back and gets closer to the original images (red to yellow).}
             \label{fig:mse_loss_dist}
    \vspace{-2mm}
\end{figure*}

% \paragraph{Detection Result}

% TRAM can either use self-supervised or supervised learning for doing the attack detection. For the supervised learning method, we train the binary classifier with two different kinds of training data. As Table~\ref{tab:detection2} shows, method M1 is trained with clean + partial FGSM attack samples and M2 is trained with clean + random noise samples. The detection accuracy of M1 for the unknown adversarial attack is higher than M2 by 19.52\% which means that the detector needs attack information when using the supervised learning method.

% Combined with the statistic learning method KS test, . Here, for KS test, we use a two-sample KS test where we compare batches of 16 images (attacked or clean) to 50,000 non-attacked images from the validation set. 
Table~\ref{tab:detection_result} shows the true positive rate 
(TPR) of our method against eight kinds of adversarial attacks on ImageNet data as the detection rate. Our method combines the $\mathcal{L}_{mse}$ loss with a two-sample KS test and set the p-value threshold based on a fixed false positive rate (FPR) of 0.2. Here, we evaluate the detectors for every baseline using the same FPR.
Compared with feature squeezing~\cite{xu2017feature}, noise-based detection~\cite{hu2019new}, targeted detection~\cite{hu2019new}, and detection with the two SSL tasks, our detection rate hovers around 82\%, which outperforming all other baselines. The superior detection results of \sys suggest that the MAE loss is more sensitive to variance in distributions. Compared to the other two self-supervision tasks, \sys outperforms rotation prediction by up to 20\% and contrastive learning by up to 60\%. Moreover, the two-sample KS-test has the additional benefit of being able to better control the TPR-FPR tradeoff compared to all other baselines by varying the p-value threshold for class prediction. 
% Tuning this threshold parameter is important for adversarial detection since the cost of false positives is larger than false negatives, given the context of the detection problem.

% Comparing the two methodologies, on average, detection accuracy using logistic regression hovers around 90\%, which outperforms KS test accuracy by 14.8\%. In other words, the reconstruction loss $\mathcal{L}_{mse}$  is useful for the logistic regression classifier to correctly identify whether an image has been attacked or not 90\% of the time. 
% The logistic regression also has more balanced sensitivity and precision compared to the KS test, which means the logistic regression tends to produce equal amounts of false positives and false negatives. The KS test leans heavily toward producing more false positives compared to false negatives. 
% \yun{is that false negative correct? I just thought is LR produce equal amounts of the FP and TN}

% Figure~\ref{fig:detection_roc} describes the ROC curve associated with the trained logistic classifier, for 8 different varieties of attacks. Overall, the area under curves (AUCs) for each attack is higher than 93\%, indicating higher separability in data and low bias from the models in the binary classification of clean v.s. attacked samples. 


% \begin{table*}[t]
% \begin{tabular}{cccccccccc}
% \multicolumn{1}{r}{}            & clean   & FGSM-$\ell_{\infty}$        & FGSM-$\ell_{\infty}$     & PGD-$\ell_{\infty}$        & PGD--$\ell_{\infty}$       & CW--$\ell_{2}$          & BIM--$\ell_{\infty}$      & PGD--$\ell_{2}$          & APGD             \\
% $\epsilon$                   & -       & 4/255            & 8/255            & 8/255            & 16/255           & 1                & 8/255            & 1                & 8/255            \\ \hline
% $\mathcal{L}_{mse}$ (before)               & 0.6693  & 0.7538           & 0.7843           & 0.7678           & 0.7968           & 0.7101           & 0.7645           & 0.7112           & 0.7427           \\
% $\mathcal{L}_{mse}$  (+noise)               & -       & 0.7558           & 0.7851           & 0.7691           & 0.7973           & 0.7295           & 0.7659           & 0.7297           & 0.7530           \\
% \textbf{$\mathcal{L}_{mse}$ (after re.)}   & -       & \textbf{0.7139}  & \textbf{0.7494}  & \textbf{0.7290}  & \textbf{0.7635}  & \textbf{0.6838}  & \textbf{0.7254}  & \textbf{0.6839}  & \textbf{0.7112}  \\ \hline
% MAE acc. (before)             & 75.34\% & 19.25\% & 12.84\%          & 16.16\%          & 11.25\%          & 26.62\%          & 16.95\%          & 26.46\%          & 19.40\%          \\
% MAE acc. (+noise)             & -       & 22.69\%          & 14.89\%          & 18.75\%          & 12.73\%          & 33.69\%          & 19.73\%          & 33.69\%          & 23.79\%          \\
% \textbf{MAE acc. (after re.)} & -       & \textbf{27.58\%} & \textbf{18.87\%} & \textbf{23.19\%} & \textbf{12.41\%} & \textbf{38.43\%} & \textbf{24.38\%} & \textbf{59.37\%} & \textbf{28.33\%} \\ \hline
% ResNet50 acc. (before)      & 86.86\% & 17.31\% & 11.14\%          & 11.37\%          & 2.6\%          & 41\%          & 9.49\%          & 34.58\%          & 25.20\%          \\
% \textbf{ResNet50 acc. (after re.)} & -       & \textbf{36.38\%} & \textbf{23.89\%} & \textbf{22.03\%} & \textbf{4.55\%} & \textbf{70.27\%} & \textbf{22.23\%} & \textbf{\%} & \textbf{54.31\%} \\ \hline
% \end{tabular}
% \label{tab:reconstruction}
% \caption{reconstruction result}
% \end{table*}

% \begin{table*}[t]
% \centering
% \begin{tabular}{ccccccc}
%  Model/Trainset                 &             & M1 (clean+FGSM)  &                      &                    & M2 (clean+noise) &                \\ 
%                   & known clean & known adv.      & unknown adv. & known clean & unknown adv.     & avg. test acc. \\ \hline
% \textbf{MAE-ViT}  & \textbf{98.94}\%     & \textbf{99.12}\%         & \multicolumn{1}{c|}{\textbf{93.15}\%}      & \textbf{97.53}\%     & \textbf{73.63}\%          & \textbf{75.50}\%        \\
% \textbf{base-ViT} & 93.66\%  & 98.23\%         & \multicolumn{1}{c|}{93.02\%}      & 100.00\%    & 60.4             & 63.90\%        \\
% \textbf{ResNet50} & 96.13\%     & 98.32\%         & \multicolumn{1}{c|}{92.55\%}      & 98.59\%     & 55.91\%          & 59.30\%        \\
% \textbf{VGG19}    & 97.88\%     & 98.23\%         & \multicolumn{1}{c|}{87.65\%}      & 99.00\%     & 62.63\%          & 65.00\%        \\ \hline
% \end{tabular}
%  \caption{Detection result on using supervised method. We train the binary classifier on four kinds of model architectures with two kinds of training datasets: 1.) Clean + FGSM attack samples, 2.) Clean + random gaussian noise samples. Compared with three model baselines, including base-ViT~\cite{Visualtransformer}, ResNet50~\cite{he2016deep}, and VGG19~\cite{vgg19}, MAE-ViT shows the best performance. Numbers in bold represent the best result.} 
%  \label{tab:detection2}
% \end{table*}

% \begin{table*}[]
% \centering
% \begin{tabular}{cc|cccccc|cc}
%    &   \textbf{Model Type}          &   \textbf{MAE}          &  &           &            &  &           & \textbf{ResNet50} &           \\ \hline
% \textbf{Attack}  &  $\epsilon$ & \begin{tabular}[c]{@{}c@{}} $\mathcal{L}_{mse}$ \\ before re.\end{tabular} &  \begin{tabular}[c]{@{}c@{}} $\mathcal{L}_{mse}$ \\ +noise \end{tabular}       &  \begin{tabular}[c]{@{}c@{}} $\mathcal{L}_{mse}$ \\ after re.\end{tabular}  & \begin{tabular}[c]{@{}c@{}}Acc\\ before re.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Acc\\ +noise \end{tabular} & \begin{tabular}[c]{@{}c@{}}Acc\\ after re.\end{tabular}& \begin{tabular}[c]{@{}c@{}}Acc\\ before re.\end{tabular}& \begin{tabular}[c]{@{}c@{}}Acc\\ after re.\end{tabular} \\ \hline
% clean     & -       & 0.6693     & -             & -         & 75.34\%    & -        & -         & 86.86\%       & -         \\
% FGSM-$\ell_\infty$ & 4/255   & 0.7538     & 0.7558        & \textbf{0.7139}    & 19.25\%    & 22.69\%  & \textbf{27.58}\%   & 17.41\%       & \textbf{36.38}\%   \\
% FGSM-$\ell_\infty$ & 8/255   & 0.7843     & 0.7851        & \textbf{0.7494}    & 12.84\%    & 14.89\%  & \textbf{18.87}\%   & 11.14\%       & \textbf{23.89}\%   \\
% PGD-$\ell_\infty$  & 8/255   & 0.7678     & 0.7691        & \textbf{0.7290}    & 16.16\%    & 18.75\%  & \textbf{23.19}\%   & 11.37\%       & \textbf{22.03}     \\
% PGD-$\ell_\infty$ & 16/255  & 0.7968     & 0.7973        & \textbf{0.7635}    & 11.25\%    & 12.73\%  & \textbf{16.41}\%   & 2.6\%         & \textbf{4.55}\%    \\
% PGD-$\ell_2$    & 1       & 0.7112     & 0.7297        & \textbf{0.6839}    & 26.46\%    & 33.36\%  & \textbf{38.23}\%   & 34.58\%       & \textbf{63.35}\%        \\
% CW-$\ell_2$     & 1       & 0.7101     & 0.7295        & \textbf{0.6838}    & 26.62\%    & 33.69\%  & \textbf{38.43}\%   & 41\%          & \textbf{70.27}\%   \\
% BIM-$\ell_\infty$  & 8/255   & 0.7645     & 0.7659        & \textbf{0.7254}    & 16.95\%    & 19.73\%  & \textbf{24.38}\%   & 9.49\%        & \textbf{22.23}\%   \\
% APGD-$\ell_\infty$      & 8/255   & 0.7427     & 0.7530        & \textbf{0.7112}    & 19.40\%      & 23.79\%  & \textbf{28.33}\%   & 25.20\%       & \textbf{54.31}\%   \\ \hline
% Avg. (Attack)     & -   & 0.7539     & 0.7606       & \textbf{0.72}    & 18.62\%      & 22.45\%  & \textbf{26.93}\%   & 19.10\%       & \textbf{37.13}\%   \\ 
% Diff.    & -  & -    & 0.007        & \textbf{-0.033}    & -     & 3.84\%  & \textbf{8.31}\%   &   -    & \textbf{18.03}\%   \\ \hline
% \end{tabular}
% \caption{The reconstruction result on eight kinds of attack. We compare the mean square error loss and accuracy for MAE on three settings: before reconstruction, adding gaussian noise, and after reconstruction. We shows the results of our method with numbers in bold. We also show the results on ResNet50 before and after reconstruction.}
% \label{tab:reconstruction}
% \end{table*}





% \begin{table*}[]
% \small
% \centering
% \begin{tabular}{cccccccccc}
% \hline
%                            & No Attack & FGSM-$\ell_\infty$ & FGSM-$\ell_\infty$ & PGD-$\ell_\infty$ & PGD-$\ell_\infty$ & PGD-$\ell_2$  & C\&W-$\ell_2$ & BIM-$\ell_\infty$  & AutoAttack \\
%                            &           & 8/255      & 16/255     & 8/255   & 16/255  & 1  & 1   & 8/255    & -          \\ \hline
% % \textbf{Standard ResNet50} & 75.41     & 6.39   & 8.55   & 0.0 & 0.0 & 0.65 & 0.84 & 0.01 & 0.10          \\
% % +MAE Accuracy           & \yun{40.38}         & 20.54      & 14.14      & 21.34   & 13.12   & 39.57    & 41.79    & 20.51    & 22.4          \\
% % +MAE Avg. Loss($\downarrow$)           & 0.3107         & 0.3521      & 0.4142      & 0.3393   & 0.3610   & 0.3307    & 0.3309    & 0.3387    & 0.3373          \\
% % \hline
% \textbf{\tabincell{l}{4 overlap map loss \\ \\ iter=10, alpha=4\\Standard ResNet50}} & 75.41     & 6.39   & 8.55   & 0.0 & 0.0 & 0.65 & 0.84 & 0.01 & 0.10          \\
% +MAE Accuracy           & 55.43         & 25.27      & 15.64      & 28.63   & 15.09   & 52.25    & 54.86    & 28.21    & 29.94          \\
% +MAE Avg. Loss($\downarrow$)           & 0.2572         & 0.2907      & 0.3420      & 0.2810   & 0.2810   & 0.2725    & 0.2725   & 0.2802    & 0.2790          \\
% \hline
% \textbf{\tabincell{l}{4 overlap map loss \\ iter=1, alpha=4\\ Standard ResNet50}} & 75.41     & 6.39   & 8.55   & 0.0 & 0.0 & 0.65 & 0.84 & 0.01 & 0.10          \\
% +MAE Accuracy           & 63.46         & 21.80      & 14.81      & 18.14   & 3.75   & 56.13    & 60.44    & 18.70    & 18.31          \\
% +MAE Avg. Loss($\downarrow$)           & 0.6705         & 0.6991      & 0.7348      & 0.6911  & 0.7066   & 0.6836    & 0.6836   & 0.6903    & 0.6894          \\
% \hline
% \textbf{\tabincell{l}{4 overlap map loss \\ iter=1, alpha=16\\Standard ResNet50}} & 75.41     & 6.39   & 8.55   & 0.0 & 0.0 & 0.65 & 0.84 & 0.01 & 0.10          \\
% +MAE Accuracy           & 57.07         & 26.20      & 16.85      & 27.69   & 11.86   & 53.25    & 55.87    & 28.06    & 27.97          \\
% +MAE Avg. Loss($\downarrow$)           & 0.6843         & 0.7105      & 0.7348      & 0.6794  & 0.6915   & 0.6742    & 0.6742   & 0.6786    & 0.6782          \\
% \hline
% % \textbf{Robust ResNet50}   & 48.15     & 24.95      & 13.9      & 20.38   & 8.83   & 35.11    & 40.82    & 20.26    & 16.08          \\
% % +MAE Accuracy           & 44.05         & 25.43      & 14.05      & 23.21   & 11.01   & 38.71   & 43.75    & 23.14    & 24.84          \\
% % +MAE Avg. Loss($\downarrow$)           & 0.3107         & 0.3408      & 0.3684      & 0.3368   & 0.3548   & 0.3308    & 0.3308    & 0.3366    & 0.3328          \\
% % \hline
% \textbf{\tabincell{l}{4 overlap map loss \\ iter=1, alpha=4\\Robust ResNet50}}  & 48.15     & 24.95      & 13.9      & 20.38   & 8.83   & 35.11    & 40.82    & 20.26    & 16.08          \\
% +MAE Accuracy           & 43.08         & 23.09      & 12.65      & 20.40   & 9.11   & 35.12    & 41.82    & 20.49    & 22.70          \\
% +MAE Avg. Loss($\downarrow$)           & 0.6710         & 0.6889      & 0.6960      & 0.6884  & 0.6967   & 0.6846    & 0.6843   & 0.6883    & 0.6858          \\
% \hline
% \textbf{\tabincell{l}{4 overlap map loss \\ iter=1, alpha=4\\Robust ResNet50}}  & 48.15     & 24.95      & 13.9      & 20.38   & 8.83   & 35.11    & 40.82    & 20.26    & 16.08          \\
% +MAE Accuracy           & 42.29         & 23.14      & 12.68      & 20.58   & 9.36   & 35.41    & 41.35    & 20.72    & 23.29          \\
% +MAE Avg. Loss($\downarrow$)           & 0.6655         & 0.6777      & 0.6792      & 0.6764  & 0.6803   & 0.6751    & 0.6748   & 0.6764    & 0.6755          \\
% \hline
% \end{tabular}
% \caption{The reconstruction result on eight kinds of attack. We compare the mean square error loss and accuracy for resnet50 model. We shows the results of our method with numbers in bold}
% \label{tab:reconstruction}
% \end{table*}

% The differences in results are in part due to the fact that when transforming the KS-test to a binary classification task, we iteratively batch the losses from the adversarial sample (true class 1) and compare that with the whole loss distribution from clean samples (true class 0) to determine whether the two have the same underlying distribution. In our experiment, we set batch size $=16 $ and collect p-values using the KS-test between sample batches belonging to (class 0, class 0) and (class 0, class 1). The detection method determines the p-value cutoff at some threshold that fixes FPR to 0.2. 


\begin{figure}[t]
\centering
\includegraphics[scale=0.51]{figs/daa_robust_acc.png}
\vspace{-3mm}
         \caption{Defense-aware Attack: We show the trade-Off on the robust accuracy and SSL attack strength. When increasing the SSL attack strength $\lambda$, the robust accuracy increases, which means the attack is less effective. When the $\lambda$ is set as zero, \sys has the best performance on robust accuracy. }
\label{fig:daa_robust_acc}

\end{figure}


















% \begin{table}[]
% \centering
% \begin{tabular}{cccl}

% \textbf{Attack Method} & mse loss (1e-2) & detection score & p-value \\ \hline
% clean                  & 62.69  & 100\%  & 0.506   \\
% FGSM-linf16            & 73.09           & 100\%           & 0.003   \\
% Hop-tar.               & 73.81           & 100\%           & 0.003   \\
% Gap                    & 67.5            & 80.64\%         & 0.055   \\
% DeepFool               & 59.42           & 56.29\%         & 0.199   \\
% Patch                  & 67.52           & 28.57\%         & 0.022   \\ \hline
% \end{tabular}
%  \caption{Detection result 1: using MSE loss}
% \end{table}

\subsection{Attack Repair Results}
We compare our repair method with two kinds of SSL baselines (1) Contrastive learning learns the representations that map the feature of the transformations of the same image into a nearby place. We follow the SimCLR~\cite{simclr} to implement the contrastive learning task. We create positive/negative pairs for images by applying some transformations such as random cropping/flipping and random rotation. (2) The rotation prediction~\cite{gidaris2018unsupervised} learns to predict the rotation degree of upcoming input samples. Here, we train the rotation prediction task as one of our SSL baselines with 4 classes ($0^{\circ}$, $90^{\circ}$,$180^{\circ}$, $270^{\circ}$).


% According to the detection result, TRAM can determine if the reconstruction is needed for the upcoming samples. If the detection score is high, we inject small perturbations on the inputs for reconstruction. Otherwise, we do the normal inference. 
As Section~\ref{sec:method} mentioned, we optimize the perturbations for a given input by using the PGD algorithm based on the MAE loss $\mathcal{L}_{\rm mse}$ extracted from MAE. Table~\ref{tab:reconstruction} shows we evaluate the robust accuracy on the target classifiers $\mathcal{C}$ from which our adversarial samples were generated. \sys successfully repairs the adversarial images and improves the robust accuracy without requiring the ground truth labels at the inference time. The target classifiers are pre-trained with two different methods, including standard training and robust training.
As Table~\ref{tab:reconstruction} shows, 
% the accuracy of clean samples is 75.41\% for standard ResNet50 and 48.15\% for robust ResNet50. 
compared with the two baselines using contrastive and rotation losses, \sys improves the robust accuracy after repair by $6\%\sim40\%$ for the attack samples generated from the standard ResNet50 model, whereas contrastive learning (SimCLR) and rotation prediction improves only by 1\%$\sim$7\%. For the Robust ResNet50 model, \sys improves the robust accuracy by 1\%$\sim$8\% for every attack, outperforming the two baselines.
% The results show that TRAM improves robust accuracy on the MAE classifier by 5.16\% $\sim$ 11.81\% and ResNet50 by 1.95\% $\sim$ 29.27\% for overall attack types. As well, we compare with baseline using random noise to do the reconstruction, and the result shows that using the self-supervised objective to reconstruct attack images is much more effective.





%  \binferencee}{}
%   \includegraphics[width=0.95\linewidth]{figs/mae_logisticroc.png}
%   \caption{by ROC cuthe rve for the logistic regression classifier described in section 3.3}
%   \label{fig:roc}
% \end{figure}

%  \begin{figure}{}
%   \includegraphics[width=0.95\linewidth]{figs/mae_samplecdfs.png}
% inferenceon{the sample CDFs of loss value of clean images and various adversarial attacks}
%   \label{fig:cdf}
% \end{figure}

\subsection{Defense-aware Attack (DAA) Results}
As Section~\ref{sec:dda_method} mentioned, we launch the adaptive attack on models and demonstrate the effectiveness of \sys on this stronger attack which assumes the attacker knows our defense method and the MAE model. Figure~\ref{fig:daa_robust_acc} shows the result of the defense-aware attack on two classifiers, including standard ResNet50 and robust ResNet50~\cite{madry2017towards}. While increasing the self-supervised attack strength $\lambda$ for DAA from 0 to 6, the attacks are gradually weakened, and the robust accuracy increases. If the adversary attempts to attack the MAE, the attack will be less effective on the backbone classifiers. On the other hand, if the adversary attempts to attack the classifier by generating samples with high attack success rates, the MAE loss can't be minimized together. While the dotted line shown in Figure~\ref{fig:daa_robust_acc} indicates the result of DAA, the solid line shows the repair result of \sys on top of DAA, which indicates that our method has optimal results when $\lambda$ is set as 0 for both the standard and robust model. Therefore, it is difficult for adversaries to attack both MAE and classifiers at the same time. 

\vspace{-2mm}

\subsection{Loss Analysis on Attack Detection}
\label{ablation:loss}
To better understand the ability of repair for MAE, we visualize the histogram on the self-supervised loss distribution for adversarial examples.  We compare the loss variance between clean samples and different kinds of attack samples by collecting the loss on every batch during the testing-time inference. As Figure~\ref{fig:mse_loss_dist} shows, the loss variance on clean samples (colored with blue region) and the other eight attack types' samples (red curve) are large before repair. The orange curve shows the loss distribution of samples after repair. Here, \sys uses MAE loss to repair the samples and demonstrates a huge loss shifting after repair (colored with yellow region), which is closer to the loss distribution of clean samples. This result inspires us to leverage the MAE loss to detect the adversarial sample with KS-test.
% The average loss reduction is shown in Table~\ref{tab:reconstruction}.

% \subsection{Classification Result}
% \yun{should be removed to other place...}
% To validate the transferability of adversarial attacks, we finetune MAE on 25 attack classifications and compare it with seven baselines, including DNN-based models and transformer-based models.
% For the transformer-based model, we fully and partially finetuning on MAE-ViT. In Table~\ref{tab:classification}, the results show that fully finetuning and partial finetuning on the MLP module of multi-head attention achieve 72\% accuracy on 25 classes of attack classification. We also evaluate the linear probing method without retraining the multi-head attention modules. However, the performance is not good and gives us the insight that finetuning on multi-head attention is necessary, due to the attention layers' need to learn the noise patterns from different attacks. Another baseline is to finetune the base-Vit with pretrained weight trained with ImageNet-1K. The results show that base-ViT has a 10\% performance gap, compared to MAE-ViT. Besides comparing with the transformer-based model, we compare MAE with DNN-based models, including ResNet50~\cite{he2016deep}, VGG19~\cite{vgg19}, and SqueezeNet~\cite{iandola2017squeezenet}. Empirically, we discovered using MAE to finetune outperforms all DNN-based models by 13.23\% $\sim$ 33.97\%. In Figure~\ref{fig:cfm}, we show the confusion matrix for MAE-ViT on 11 group classes classification. The matrix shows more detail in every class. 
% Figure~\ref{fig:acc_curve} shows the classification accuracy rate and loss on every training checkpoint from epoch 0 to 100. Compared to other models, MAE has the best accuracy and convergence on loss. 



% \begin{table}[t]
% \centering
% \begin{tabular}{ccc}
% \textbf{Method}             & \begin{tabular}[c]{@{}c@{}}25 classes \\ top1 acc.\end{tabular} & \begin{tabular}[c]{@{}c@{}}11 classes \\ top1 acc.\end{tabular} \\ \hline
% MAE finetune                & \textbf{72.68\%}                                                & \textbf{89.19\%}                                             \\
% MAE partial finetune & 72.19\%                                                         & 87.98\%                                                               \\
% MAE linear probing       & 30.07\%                                                         & 43.60\% 
% \\
% MAE (small)       & 64.35\%                                                         & 80.65\%   

% \\
% ViT-base fintune                 & 59.66\%                                                         & 75.09\%                                                               \\ \hline
% ResNet50          & 38.71\%                                                         & 56.29\%                                                               \\
% VGG19              & 59.45\%                                                         & 78.50\%                                                               \\
% SqueezeNet       & 52.96\%                                                         & 69.06\%                                                               \\ \hline
% \end{tabular}
%  \caption{Result on the 25 classes and 11 group classes attack classfication. Compared with different kinds of training method, finetuning on MAE-ViT show the best top 1 accuracy on both 25 and 11 classes classification.}\
%  \label{tab:classification}
%  \vspace{-2mm}
% \end{table}


% \begin{figure}[t]
% \centering
% \vspace{-2mm}
% \subfigure{  \includegraphics[width=0.99\linewidth]{figs/mae_training_acc.png}}
% \subfigure{  \includegraphics[width=0.99\linewidth]{figs/mae_training_loss.png}}
% \vspace{-5mm}
% \caption{training accuracy and loss on different training methods}
%  \label{fig:acc_curve}
%  \vspace{-4mm}
% \end{figure}




