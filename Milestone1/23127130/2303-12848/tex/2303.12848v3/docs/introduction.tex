\section{Introduction}\label{sec:introduction}

% Remarkable advances in prediction accuracy have widened the adoption of deep learning (DL) models. They now run in large-scale deployments on millions of edge devices including phones, cameras, VR headsets, and edge servers~\cite{wu2019machine}. Example real-world edge model deployments include face recognition~\cite{fd-example}, object detection~\cite{object-example}, and machine translation~\cite{translate-example}. Such deployments enjoy many benefits, including reduced inference latency, little dependence on network connectivity, and increased privacy because user data no longer needs to be sent to the cloud servers for inference.

% Unfortunately, edge deployments also raise difficult security challenges. 

% \yun{Logically consistent
% Intro. modify; Ml model is fragile to adversarial; Training time defense  is more costly can't adapt to unforeseen; computing resource high; Prior work change model weight storing Complexity; Others change input using ssl task; We show this ssl task is too weak to  detect and reverse
% Third paragraph. Directly jump to mae leverage autoencoder for detect and reconstruct; Bring new stuff in MAE
% MAE Is much better than SSL Jigsaw, rotation, contrastive, MAE
% need to have Topic sentence in the beginning of parag.}

\begin{figure}
    \centering
    \includegraphics[scale=0.70]{figs/DRAM_FLOW2.png}
    \vspace{-5mm}
    \caption{A schematic of \sys. (1) Attack detection: Leverages the MAE loss to detect adversarial samples via a statistic-based test. (2) Attack Repair: Adapts inputs by adding a reversal vector to the input space and optimizing it with MAE loss, which can potentially repair the adversarial sample.}
    \label{fig:flow}
\end{figure}


Despite remarkable advances in prediction accuracy and wide adoption of deep learning (DL) models, they are known to be vulnerable to adversarial attacks in which attackers add carefully crafted, human-imperceptible perturbations to the samples to fool the models~\cite{goodfellow2014explaining,carlini2017towards,chen2017zoo,madry2017towards}.
Most prior defenses happen at training time~\cite{kurakin2016adversarial, madry2018towards, zhang2019theoretically}, which incurs high training cost and cannot defend against unseen attacks. Recent work proposed test-time defenses~\cite{zhou2020deep, wang2021tent, croce2022evaluating, gandelsman2022test} that adapt the model weights to improve prediction accuracy on out-of-distribution data, but they require access to the weights and cannot work on frozen models. Additionally, they may require multiple versions of the same models to operate simultaneously, complicating model management and increasing memory consumption. Another test-time defense~\cite{mao2021adversarial, tsai2023self} reverses attacks by adapting inputs computed using self-supervised learning (SSL) tasks such as contrastive learning~\cite{simclr} or rotation prediction~\cite{gidaris2018unsupervised}. However, as our experiments reveal, these tasks are too weak to capture sufficiently detailed representations for effective detection of adversarial samples.

% and assume one or a couple of known attack~\cite{kurakin2016adversarial, madry2018towards, zhang2019theoretically}. They cannot defend against unseen attacks after a model is deployed. 



% An alternative approach is for the edge devices to send adversarial samples to the cloud to robust-train a new version of the model, but this approach defeats the latency, connectivity, and privacy benefits of edge deployment. Moreover, edge models are frequently compressed, quantized, and pruned to fit the "wimpy" edge devices~\cite{hao2022tale}. Given the many different chipsets (Meta reported 2000+ for just mobile devices alone~\cite{wu2019machine}) and versions of the edge models, any of which may be individually attacked, it is hopeless to count on a one-size-fits-all defense trained in the cloud.

% These challenges call for \emph{test-time} defenses against adversarial attacks. Prior works~\cite{zhou2020deep, wang2021tent, croce2022evaluating, gandelsman2022test} mostly focus on adapting the model weights to improve prediction accuracy on out-of-distribution data. They may require multiple versions of the same models to operate simultaneously, complicating model management and increasing memory consumption. Other works~\cite{mao2021adversarial, tsai2023self} aim to reverse samples by adding additional "reverse vectors" using self-supervised learning (SSL) tasks such as contrastive learning or rotation prediction.

% Existing test-time defenses~\cite{croce2022evaluating, gandelsman2022test, mao2021adversarial}, however, unrealistically assume that an adversarial attack has been detected and focus on repairing the attack

This paper presents \sys, a test-time defense using masked autoencoder (MAE)~\cite{MaskedAutoencoders2021},  one of the strongest SSL tasks, to detect and repair adversarial attacks, including unforeseen ones. MAE is a ViT-based architecture that reconstructs masked patches based on the sparse information of unmasked patches. Prior work~\cite{MaskedAutoencoders2021} showed that MAE is both efficient at reducing redundancy in feature representations and capturing detailed information from holistic image statistics.
We leverage MAE to detect adversarial samples generated from potentially unforeseen attacks via a statistic-based test on the MAE loss. Moreover, without knowing the ground truth of the samples, \sys repairs the samples by minimizing the MAE loss at test time. We highlight our main contribution as follows:


% In contrast to general deep neural network extracting features with convolutional filters, it shares the same encoder architecture as vision transformers (ViT) and contains multi-head attention, which can extract features from sequences of masked image patches. standing beyond low-level image statistics. 



% We show that rotation prediction~\cite{gidaris2018unsupervised} and contrastive learning~\cite{simclr} are insufficient to defend against multiple adversarial attacks.

% The rationale is that those discriminative SSL tasks learn to discard a large portion of their input information. Representations learned from these tasks only keep a small amount of information/features from the input space. Consequently, their objective functions are less sensitive to adversarial attacks. 

% Consequently, small perturbations on the learned input features may completely change the prediction even without changing the image semantics~\cite{postels2021practicality,NEURIPS2019_e2c420d9}.  

% Although current existing SSL pretext tasks (i.e., rotation prediction~\cite{gidaris2018unsupervised}, jigsaw puzzle~\cite{noroozi2016unsupervised}, and contrastive learning~\cite{simclr}, ...etc.) indeed can help to capture the natural inputs manifold, their objective functions are less sensitive to adversarial attacks. They might not be able to do the unforeseen adversarial detection. 


% Our key insight is that adversarial attacks not only degrade the performance of a model but also collaterally damage the intrinsic structure of the natural samples. This motivates us to leverage self-supervised learning (SSL) to capture the intrinsic structure of the natural samples. Subsequently, \sys detects previously unseen adversarial attacks by detecting whether this structure has been damaged. It further repairs the attacked samples by restoring this structure.

% Lastly, to reduce manual forensic work, \sys classifies the attacks based on the auxiliary information contained in this structure into \nattacks known classes with attack hyperparameters, so that developers need to focus only on the attacks \sys fails to confidently classify.

% such as the reconstruction loss of MAE (i.e., mean square error loss).


\iffalse

Deep learning has shown remarkable success in a wide range of machine learning (ML) applications, yet the vulnerability to potential threats impedes them from being deployed on edge devices. The most well-known threat is the adversarial attack, in which the adversarial sample can be generated by adding crafted perturbations on the benign samples to fool the model decisions. Without the server's awareness, such attacks can be launched on targeted edge devices under different model settings. For instance, in the white-box setting, the attacks can be easily launched by back-propagating the model. On the other hand, models without user accessibility are common in the real world, known as black-box models. It restricts an adversary to directly accessing the model's parameters for generating adversarial samples (i.e., \albert{"a" instead of "the" in this sentence, unless referring to a specific api?} a face recognition API on the cloud platform.). By using zeroth-order optimization to estimate the perturbations or by leveraging a surrogate model network for generating transferable attacks, such black-box attacks are untraceable and undetectable.

To defend against the aforementioned attacks, prior works propose adversarial training by training models with both clean and attacked samples. Existing adversarial training methods seem effective and largely enhance the model's robustness, yet the \tapan{communication and computational costs can be very high} computational cost is high and it is implausible to iteratively retrain the edge devices. Consider the case of server-edge systems: although adversarial training methods can happen on the server and deploy the trained model to the edge, most adversarial attacks are launched on the edge, and are difficult to be detected by the server. In this case, the server will need to communicate with the edge to know whether the attacks are detected or not. Hence, the communication cost will be high if we need to iteratively transfer attack samples from the edges to the server for adversarial training.
To mitigate this challenge, the test-time defense method is important to local devices with limited computational resources. \albert{Not sure what "challenge to robust" means here} However, it is challenging to make the edge devices robust without knowing any prior information on adversarial attacks.
Hence, in this paper, we focus on utilizing self-supervised learning to defend against unknown adversarial attacks. 

Prior works~\cite{MaskedAutoencoders2021} shows the MAE is one of the strongest SSL tasks. Learning representation with MAE could largely reduce the redundancy of feature representations and learn holistic image statistics by reconstructing the masked patches based on the unmasked patches' sparse information. 
% MAE learns informative representations through reconstructing the input information, which makes it less focus on individual features and more on global features (closer to human vision); thus, it is more robust to adversarial perturbation.
% Specifically, the encode and decode procedure of MAE is asymmetric, where the encoded latent features contain only the information of unmasked patches, and the decoded module process the latent features with the masked tokens for reconstruction. 
In our work, we extend the learning ability of MAE to do unforeseen detection by combining  the SSL objective function with statistic-based detection. Without knowing the ground truth of upcoming samples, \sys can repair the attack samples during the test time. We highlight our main contribution as follows:
% In contrast to general deep neural network extracting features with convolutional filters, it shares the same encoder architecture as vision transformers (ViT) and contains multi-head attention, which can extract features from sequences of masked image patches. standing beyond low-level image statistics. 

\fi






\iffalse

In this paper, we propose a novel test-time defense framework for adversarial attacks by leveraging self-supervised objectives with masked autoencoder (MAE). Our defense pipeline involves three components: attack detection, classification, and reconstruction. 

%\z{detection, classification and reconstruction? This order may be better since classification results could improve reconstruction results? If we know type of attacks, we can use suitable parameters for reconstruction. For example, for small perturbation attacks, we can use a small reverse vector. Or small iteration numbers for faster convergence. This could be a benefit of attack classification.}
Our intuition is that the adversarial attacks not only degrade the performance of a pre-trained classifier but also collaterally damage the self-supervised objective (SSL), such as the reconstruction loss of MAE (i.e., mean square error loss). This motivates us to utilize auxiliary information from the self-supervised objective, which captures the intrinsic structure of natural images, to detect and reconstruct the adversarial samples.

\fi


\begin{itemize}
    \item We propose a test-time defense method for unknown adversarial attacks using self-supervised objectives and masked autoencoder.
    % This is the first work using MAE structure detect and repair adversarial samples.

    \item We illustrate the method of \sys on both detection and repair: (1) Adversarial detection by statistic-based learning with reconstruction loss. (2) Repair of the adversarial samples by injecting imperceptible perturbations and optimizing with self-supervised loss. (3) We launch an adaptive attack that assumes the attacker has knowledge of our defense method and shows the effectiveness of \sys on the defense-aware attack.
    
    \item \sys achieves 82\% detection rate on eight kinds of adversarial samples average and outperforms other detection baselines. For attack repair, \sys successfully repairs the adversarial samples and improves robust accuracy by 6\%$\sim$41\% on eight attacks for ResNet50 under normal training setting and 3\%$\sim$8\% under robust training setting. Our method outperforms two self-supervision tasks, including contrastive learning and rotation prediction.
    % For attack classification, compared with other transformer-based and DNN-based baselines, TRAM can achieve the highest performance, 72\% accuracy on 25 classes attack classification  
    
    % \item We show detailed analysis and data visualization in the ablation studies for TRAM, including the sensitivity analysis on SSL loss and hyper-parameters for reconstruction, Grad-Cam visualization on the perturbation structures for different kinds of attacks, T-SNE analysis on different attack classification models, 



    % \item \z{ we propose a context-aware attack defense system. Instead of blindly treating every attack in the same way, our system includes an attack classification model to estimate types of attacks as well as their hyperparameters. Information on adversarial attacks can later be used to defending attacks. \yun{To be honest, we didn't do the exp. under this kind of setting. Instead, we automatically reconstruct the adversarial samples only based on the MSE loss magnitude. Hence, I think it is inappropriate to put this in the contribution. We can put this into future work.}}
\end{itemize}

% We believe \sys represents a full pipeline for detecting and repairing adversarial attacks at test time. Each component in \sys is useful in its own right and can defend against adversarial attacks beyond deployed models.
