\section{Method}\label{sec:method}
In this section, we define the problem and present the details of each component in the pipeline of \sys, including test-time detection and repair of attacks.


%In this section, we formulate the problem and illustrate the details of each component in the whole pipeline of TRAM.

\subsection{Problem Definition}
% Considering the case of learning to capture the natural inputs manifold for detecting and repairing the malicious inputs, we formalize the problem as follows.
% Let $\mathcal{M}: \mathcal{X} \mapsto \mathbb{R}^{n \times q \times q}$ be a masked autoencoder that takes any randomly masked-out samples $X \in \mathcal{X}$ as inputs and outputs $n$ predicted masked-out patches with size $q \times q$, where $n$ is the number of masked-out patches. 
% Given a masked autoencoder (MAE) that takes any randomly masked samples of $x \in \mathcal{X}$ as inputs and outputs corresponding predicted masked-out patches, the masked samples are denoted as $\mathbf{b} \odot x$, where  $\mathbf{b} \in \{0,1\}^n$ is binary matrix which is sampled following on uniform distribution, and $\odot$ is an operator for the element-wise product.
% We denote the \textit{encoder} module of MAE\z{, parametrized by $\theta$,} as $f^{\phi} \z{f_{\theta}}: \mathbb{R}^n \to \mathbb{R}^m$ and the \textit{decoder} module \z{, parametrized by $\phi$,} as $f^{\dagger} \z{f^{\dagger}_\phi}: \mathbb{R}^m \to \mathbb{R}^n$. A classifier $h_\psi: \mathbb{R}^m \to \mathbb{R}^d$ is concatenated with the encoder $f^{\phi} \z{f_{\theta}}$ for downstream classification task (i.e, ImageNet-1K classes), where $d$ is the number of classes. The prediction of input $x$ can be described as $\hat{y}=h_\psi(\mathbf{z})$, where $\mathbf{z}=f^\phi\z{f_{\theta}}(\mathbf{x})$. 
% In the class-aware self-supervised learning setting, let $\{x_i, y_i\}_{i=1}^{N_1} \in \mathcal{S}$  \z{is $X$ or $x$?} be the source distribution of clean samples without attacks \z{is it training data?}, and $\{z_i\}_{i=1}^{N_2} \in \mathcal{Z}$ be the unlabeled data points \z{is it testing data?}, where $\mathcal{S} \cap \mathcal{Z} = \emptyset, N_1 \neq N_2$. 
% Noticed that $z \in \mathcal{Z}$ involves both clean and adversarial samples, which makes classifier $h_\psi$ and MAE ($f^{\phi}, f^{\dagger}$) \z{($f_{\theta},f^{\dagger}_\phi$)} more susceptible to inductive bias. We aim to learn a self-supervised objective based on MAE ($f^{\phi}, f^{\dagger}$) such that $h_\psi$ is robust on potential adversarial attacks from $\mathcal{Z}$ during inference time.

We formalize the problem of learning to capture the natural input's manifold for detecting and repairing the malicious inputs. Assuming we have a self-supervised learning (SSL) task $\mathcal{S}$, we denote its objective function as $\mathcal{L}_s$. To capture the manifold's natural inputs for a given classifier $\mathcal{C}$, we learn the SSL task $\mathcal{S}$ using the training set of SSL task $\mathcal{S}$ doesn't involve any attacked samples.




% Let class label of $x$ be $y$, we define a classifier, parameterized by $\psi$, on top of the latent variable $z$ as $h_\psi: \mathbb{R}^m \to \mathbb{R}^d$. The prediction can be described as $y_{\rm pred}=h_\psi(z) = (h_\psi \circ f_\theta)(x)$. 
% In the class-aware self-supervised learning setting, $x \in \mathcal{S}$ be the source distribution of clean samples without attacks, and $u \in \mathcal{Z}$ be the unlabeled target distribution, where $\mathcal{S} \cap \mathcal{Z} = \emptyset$. In the real world, the sample $u \in \mathcal{Z}$ involves both clean and adversarial samples, which makes classifier $h_\psi$ and masked autoencoder $g_\phi \circ f_\theta$ more susceptible to inductive bias.
% We define an adversarial perturbation as $\delta_{\rm adv}$ for a given clean sample $u$ and the corresponding adversarial example is defined $u_{\rm adv} = u + \delta_{\rm adv}$ which causes the output of classifier $\hat{y} = (h_\psi \circ f_\theta)(u_{\rm adv})$ and $\hat{y} \neq$ ground truth $y$. 


During the inference time, given an adversarial input $u_{\rm adv}$ which is created by adding an adversarial perturbation $\delta_{\rm adv}$ to a clean sample $u\in\mathcal{Z}$, resulting in an output $\hat{y} = C(u_{\rm adv})$ that does not match the ground truth label $y$. 
% However, we have discovered that training a masked auto-encoder (MAE) solely on clean images results in a significantly higher mean square error between the input and output of adversarial images. This difference in error can be used to detect whether an image has been subjected to an attack.
To mitigate these adversarial attacks, we propose using a reverse vector $\delta_{\rm rev}$ to minimize the difference between the adapted image and the original input, effectively generating an adversarial perturbation for correction. Our goal is to find an optimal reverse vector $\delta_{\rm rev}$ with a small $\epsilon$ range that can minimize the self-supervised loss $\mathcal{L}_s$ and restore the correct prediction, such that $C(u)=y = C(u + \delta_{\rm adv}+ \delta_{\rm rev})$. 
% In our paper, We aim to (1) detect the occurrence of adversarial attacks for $u \in \mathcal{Z}$, (2) find a reverse vector $\delta_{\rm rev}$ with a small $\epsilon$ range to restore the prediction as $(h_\psi \circ f_\theta)(u)=y = (h_\psi \circ f_\theta)(u + \delta_{\rm adv}+ \delta_{\rm rev}) $.




%A classifier $\mathcal{F}: \mathcal{X} \mapsto \mathbb{R}^d$ is simultaneously concatenated behind the encoder module of $\mathcal{M}$ for downstream classification task that takes the same inputs of $\mathcal{M}$ and generates the $d$-dimensional prediction scores (e.g., softmax outputs), where $d$ is the number of classes. In the class-aware self-supervised learning setting, let $\{x_i, y_i\}_{i=1}^{N_1} \in \mathcal{S}$  \z{is $X$ or $x$?} be the source distribution of clean samples without attacks \z{is it training data?}, and $\{z_i\}_{i=1}^{N_2} \in \mathcal{Z}$ be the unlabeled data points \z{is it testing data?}, where $\mathcal{S} \cap \mathcal{Z} = \emptyset, N_1 \neq N_2$. In the real world, the $z \in \mathcal{Z}$ involves both clean and adversarial samples, which makes classifier $\mathcal{F}$ and masked autoencoder $\mathcal{M}$ more susceptible to inductive bias. We aim to learn a self-supervised objective based on $\mathcal{M}$ such that $\mathcal{F}$ is robust on potential adversarial attacks from $\mathcal{Z}$ during inference time.

\subsection{Learning with Masked Autoencoder (MAE)}

Self-supervised learning (SSL) helps capture the natural inputs manifold, yet standard self-supervision pretext tasks such as rotation prediction, re-colorization, and contrastive learning are not sensitive enough to adversarial attacks. The MAE is one of the strongest self-supervision learning tasks, which can repair the randomly masked-out input patches by minimizing the MAE loss of predicted patches. Motivated by this, we show how to learn the reverse vector for repairing the attacked images with MAE.

Given a masked autoencoder (MAE), we define the encoder, parameterized by $\theta$, as $f_{\theta}: \mathbb{R}^n \to \mathbb{R}^m$. Let $x \in \mathcal{X}$ be an input data in the image space, and $b \in \{0,1\}^n$ be the binary matrix that is sampled following a probability distribution. The encoder takes a masked sample $b \odot x$ as input and outputs a latent variable $z=f_\theta(b \odot x)$. We define a decoder, parameterized by $\phi$, as $g_\phi: \mathbb{R}^m \to \mathbb{R}^n$, which takes a latent variable $z$ and reconstruct the mask regions of the input image as $\hat{x}_{\rm mask} = g_\phi(z)=(g_\phi \circ f_\theta)(b \odot x)$. 



% Specifically, the MAE model involves an encoder function $f_{\theta}: \mathbb{R}^n \to \mathbb{R}^m$ takes a masked sample $b \odot x$ as input, with $x \in \mathcal{X}$ as the input data in the image space, and $b \in \{0,1\}^n$ as the binary matrix sampled from a probability distribution. The encoder produces a latent variable $z=f_\theta(b \odot x)$, which is then used by the decoder function $g_\phi: \mathbb{R}^m \to \mathbb{R}^n$ to reconstruct the mask regions of the input image, resulting in $\hat{x}_{\rm mask} = g_\phi(z)=(g_\phi \circ f_\theta)(b \odot x)$. 
% We propose class-aware self-supervised learning with MAE for training TRAM. Notice that the classifier $h_\psi$ shares the same encoder module $f_{\theta}$ with the MAE, and the latent features of the encoder module are retrieved to train $h_\psi$. The whole training process involves the following steps. First, we leverage the encoder module of the pre-trained masked autoencoder, which is pre-trained on ImageNet-1K~\cite{MaskedAutoencoders2021}. Then, we finetune MAE $\mathcal{M} = (g_{\phi} \circ f_{\theta} ) $ and the classifier $\mathcal{F} = (h_\psi \circ f_\theta)$ with $x$ sampled from $\mathcal{S}$. 
The objective function of MAE is to compute the mean square error between the masked patches of input $x$ and their corresponding predicted patches, which can be defined as Eq.~\ref{eqn:mse_loss_eqn}:
\begin{align}
    \label{eqn:mse_loss_eqn}
\mathcal{L}_{\rm mse}(\cdot)=\|(1-b) \odot x - (g_{\phi} \circ f_{\theta})(b \odot x)\|_2.
\end{align}
We have discovered that the properties of the MAE can effectively capture the underlying structure of natural inputs, allowing us to achieve our objective of detecting and correcting adversarial inputs. This key insight forms an integral part of our proposed \sys method, designed to improve the robustness and reliability of machine learning systems by detecting and repairing adversarial inputs.

% \begin{align}
%     \label{eqn:mse_loss_eqn}
%     \mathcal{L}_{\rm mse}=\|(\mathbf{1}-\mathbf{b}) \odot x -  f^{\dagger}(f^{\phi}(\mathbf{b} \odot \mathbf{x}))\|_2
% \end{align}
% The objective function for jointly training MAE ($f_{\theta}, g_{\phi}$) and classifier $h_\psi$ can be defined as Eq.~\ref{eqn:training_mae}:

%\begin{align}
%    \label{eqn:training_mae}
%    \min_{\theta_{f}, \phi_{g}, \psi_{h}} \mathop{\mathbb{E}}_{(x)\sim\mathcal{S}}
%    \big[
%        \mathcal{L}_{mse}(\cdot) + \mathcal{L}_{ce}(y,  {\color{red}{(g_{\phi} \circ h_\psi)(x))}[Peter]}
%    \big]
%    % \vspace{-2mm}
%\end{align}
%,where the first term is the mean square error loss
%$\mathcal{L}_{mse}$ defined as Eq.~\ref{eqn:mse_loss_eqn}, and the second term is the cross entropy loss for class prediction from $h_\psi$, $y$ is the ground truth label of input $x$. 
%For the reconstruction loss, we only compute the loss between the masked-out patches of the sample and its corresponding predicted patches. The encoder $f^{\phi}$, decoder $f^{\dagger}$, and classifier $h_\psi$  are optimized by minimizing both two loss terms jointly. 
%While training to converge, an optimal $\mathcal{M}$ and $\mathcal{F}$ can be obtained for later detection and reconstruction. 

\subsection{Test-time Attack Detection}

After obtaining a well-trained MAE ($f_{\theta}, g_{\phi}$), the detection of anomalous adversarial attacks is straightforward by using the MAE loss $\mathcal{L}_{ mse}(\cdot)$. The rationale lies in our training for $f_{\theta}$ and $ g_{\phi}$ being on the clean sample sets $\mathcal{S}$, which can be viewed as the manifold learning on in-distribution data. Thus, the magnitude of MAE loss enlarges while encountering adversarial samples $u$ from $\mathcal{Z}$, thus triggering the detection.  
% Suppose there exists an adversarial sample $u$ sampling from $\mathcal{Z}$, we get the classification scores $\hat{y} = (h_\psi \circ f_\theta)(u)$
% and the predicted masked-out patches $\hat{h} = \mathcal{M}(H \odot z)$, where $H$ is a Boolean operator for the purpose of randomly masked-out patches in $z$, and $H \odot z$ induce the unmasked part of the adversarial sample $z$.
% and reconstruction error loss $\mathcal{L}_{mse}(\cdot)$. We train an adversarial detector based on the $\mathcal{L}_{mse}(\cdot)$ calculated by $u$. 
% The trained logistic regression classifier will serve as the adversarial detector for the classifier $h_\psi$.
%We further leverage the discrepancy of MSE loss between benign image and adversarial image on detecting the adversarial attacks.
% The MSE loss for benign and adversarial images can further become the test statistics which can combine with hypothesis testing using p-values to do the detection.
% \yun {add training logistic eq.}

% For training the logistic regression~\cite{logreg} classifier, we label the adversarial samples $u \in \mathcal{Z}$ as class 1 and the clean samples $x \in \mathcal{S}$ as class 0. The feature space $\mathcal{A}$ of the logistic regression consists of the loss value calculated by $\mathcal{L}_{mse}$ associated with the corresponding sample as well as an intercept vector of $1_n$. The objective function for training logistic regression-based detector is as Eq.~\ref{eqn:train_logistic}
% \begin{gather}
%     cost(Q_\theta (A), y) = \begin{cases} -log(Q_\theta(A)) & \mbox{if} \hspace{1mm} y = 1 \\ -log(1-Q_\theta(A))& \mbox{if} \hspace{1mm} y = 0  \end{cases}, \nonumber\\
%     s.t. \hspace{1mm}   A = \begin{bmatrix} \vdots & \vdots \\ \mathcal{L}_{mse} & 1 \\ \vdots & \vdots \end{bmatrix} \nonumber\\
% \vspace{-4mm}
%     \label{eqn:train_logistic}
% \end{gather}

% \vspace{-1mm}
% ,where $Q_\theta$ is the logistic regression model and $A \in \mathcal{A}$  are the logistic features consisting of loss value $\mathcal{L}_{mse}(\cdot)$ and $1_n$. After training, we evaluate the detection score according to the predicted result $\hat{c}$ for adversarial examples as Eq~\ref{eqn:detection_score}:
% \begin{equation}\label{eqn:detection_score}
% \begin{aligned}
%     \text{detection score} &= \cfrac{1}{n}\sum_{i=1}^{n}\mathbbm{1}_{\hat{c}=Y}\\
%     \text{such that} \quad \hat{c} &= \begin{cases} 0 & P \leq 0.5 \\ 1 & P > 0.5 \end{cases},

% \end{aligned}
% \end{equation}
% , where $P(Y|X) = \cfrac{1}{1 + e^{-(\beta_0 A + \beta_1)}}$ is the predicted probability of the loss value belonging to class 1, $\hat{c}$ is the predicted class label from the logistic regression model, $\beta_0$ is the weight vector, and $\beta_1$ is the bias. 

Here, we utilize MAE loss $\mathcal{L}_{ mse}$ with a statistic-based method for detection in \sys, the Kolmogorov-Smirnov test (KS test)~\cite{kstest}.
We hypothesize that loss values associated with non-attacked or clean images will be distinctly lower on average compared to those associated with attached images. Moreover, in Figure~\ref{fig:mse_loss_dist}, the sampled distributions of the loss values formed the histogram which delineates the difference in distribution between non-attacked and attacked images. Combining SSL loss with the KS test for adversarial detection is better than other detection methods that calculate the distance metric for logit outputs. In Section~\ref{subsec:detection_result}, we  illustrate the details of other baselines and demonstrate the detection results.


% {\SetAlgoNoLine%
% \SetNoFillComment



\subsection{Test-time Attack Repair}

The second part of the \sys method involves repairing adversarial attack inputs from malicious to benign. Unlike the existing method~\cite{gong2022reverse}, our method is fully unsupervised and can be done parallel to detection. 
%Noticed that, the expected output of MAE is prediction of masked out part of the input samples. Hence, given a $x_{adv}$, the output $\hat{x_{adv}}$ from MAE is still a malicious sample. Our reverse engineering method is to inject a small perturbation $\delta$ on $x_{adv}$ such that
To repair the potential adversarial samples $u_{adv}$, we inject a reverse vector $\delta_{rev}$ into $u_{adv}$ such that $\hat{u} = u_{adv} + \delta_{rev}$ minimizes the MAE loss $\mathcal{L}_{mse}$. We initialize the $\delta_{rev}$ under the uniform distribution following a specific $\epsilon$ range. After optimization, the adapted sample $\hat{u} = u_{adv} +\delta_{rev}$ is fed to the classifier $C$ for prediction, yielding classification scores $\hat{y} = C(\hat{u})$.
The objective function for optimizing $\delta_{rev}$ is defined as Eq.~\ref{eqn:optim}:
\begin{align}
    \label{eqn:optim}
    \delta_{rev} = \arg\min_{\|\delta_{rev}\|_{\infty} \leq \epsilon} 
    \mathcal{L}_{mse}((1-b) \odot \hat{u}, (g_{\phi} \circ f_{\theta})(b \odot \hat{u})),
    %s.t. \parallel \delta \parallel_{p} \leq \epsilon
    % \mathcal{L}_{r}(\mathcal{M}(x_{adv}+\delta), x_{adv}+\delta) \leq  \mathcal{L}_{r}(\mathcal{M}(x_{adv}), x_{adv}) 
    \vspace{-4mm}
\end{align} 
We aim to find an optimal vector $\delta_{rev}$ for generating a adapted sample $\hat{u} = u_{adv} + \delta_{rev}$ such that the gap between masked patches $(1-b) \odot \hat{u}$ and predicted patches $(g_{\phi} \circ f_{\theta})(b \odot \hat{u})$ of the adapted sample can be reduced. Specifically, we update $\delta_{rev}$ using projected gradient descent, subject to $\ell_{\infty}$ with a specific $\epsilon$ range. The update of $\delta_{rev}$ is described in Eq.~\ref{eqn:deltaupdate}. 
\begin{align}
    \label{eqn:deltaupdate}
    \delta^{t+1}_{rev}  = \text{clip}(\delta^{t}_{rev} + \alpha \cdot \text{sign}(\nabla_{X} \mathcal{L}_{\rm mse}), -\epsilon, \epsilon).
            % Peter---
        %original : \\delta^{t+1}  = \delta^{t} + \text{clip}(\alpha \cdot \text{sign}(\nabla_{X} \mathcal{L}_{\rm mse}), -\epsilon, \epsilon),
%\nonumber
\end{align}
Here, $t$ is the timestamp, $\alpha$ is the step size, and $\epsilon$ is the perturbation range. The update of $\delta_{rev}$ is performed by taking the gradient of the MAE loss $\mathcal{L}_{mse}$ with respect to the input and clipping the update of $\delta_{rev}$ to the $\epsilon$ range. 

\iffalse

,where the $\delta_{rev}$ is updated via using projected gradient descent, subjected to $\ell_{\infty}$ with a specific $\epsilon$ range. To be more specific, we aim to find an optimal vector $\delta_{rev}$ for generating adapted sample $\hat{u} = u_{adv} + \delta_{rev}$, such that the gap between masked patches $(1-b) \odot \hat{u}$  and predicted patches $(g_{\phi} \circ f_{\theta})(b \odot \hat{u})$ of the adapted can be reduced. The update of $\delta$ is as Eq.~\ref{eqn:deltaupdate}:


where $t$ is the timestamp, $\alpha$ is the step size and $\epsilon$ is the perturbation range. We denote the element-wise clipping operation $\text{clip}_{\epsilon}(z)$ as: 
\begin{align*}
\text{clip}_{\epsilon}(z) =
\left\{ \begin{array}{rl}
-\epsilon & \mbox{if } z < -\epsilon  \text{,} \\ 
z & \mbox{if } -\epsilon  \leq z \leq \epsilon  \text{,}  \\
\epsilon  & \mbox{if } \epsilon  < z \text{.}
\end{array}\right.
\end{align*}
We show the detailed algorithm for test-time reconstruction as Algorithm 1.
\jc{The detailed algorithm for test-time reconstruction is presented in Algorithm 1.  We compare different kinds of injection in Sec.~\ref{sec:experiment} and show the sensitivity analysis on $\epsilon$ range in Sec.~\ref{ablation:parameter}.}
\fi


% Given samples, $z \in \mathcal{Z}$, where $z$ is potentially an adversarial samples, the reconstruction loss satisfies following in-equation \yun{revise!}

% \begin{align}
%     \label{eqn:reverse}
%     \mathcal{L}_{mse}(\mathcal{M}(H \odot \hat{z}), \hat{z} - H \odot \hat{z}) \leq  \\
%     \mathcal{L}_{mse}(\mathcal{M}(H \odot z), z - H \odot z) \nonumber
% \end{align}

% \begin{algorithm}[H]
% \label{algo:testtimereverse}
% \setstretch{0.9}

% 	\caption{Test-Time Reconstruction}
% 	\KwIn{Masked autoencoder $\mathcal{M}(\cdot)=(g_\phi \circ f_\theta)(\cdot)$, Classifier $\mathcal{F}(\cdot)=(h_\psi \circ f_\theta)(\cdot)$, Adversarial images $X$, Objective function $\mathcal{L}_{mse}(\cdot)$, Step size $\alpha$, Epsilon range [-$\epsilon$, $\epsilon$], Number of iterations $\mathcal{T}$, Initial perturbation $\delta^{0}$.}
% 	\KwOut{Class prediction $\hat{y}$ for reconstructed sample of $X$}
%     \textbf{Inference}\\
%     % $\ell^\star \gets \infty$\\
%     \tcc{Initialize the parameters}
%     $ \delta^{0} \gets   \delta^{0} + \mathcal{U}\{(-\epsilon, \epsilon)\}$ 
    
% 	\For{$t\in\{1,..., \mathcal{T}\}$} {
%         \tcc{Generate adapted samples} 
% 	      $X^{t} \gets X + \delta^{t}$  \\
% 	    \tcc{Sample binary masks and calculate reconstruction loss}
% 	      $loss = \mathcal{L}_{mse}(\mathcal{M}(b\odot X^{t}), (1 - b) \odot X^{t})$ \\
% 	    \tcc{Update parameters}
% 	    $ {\color{red} \delta^{t+1}\gets \text{clip}(\delta^{t} + \alpha \cdot sign(\nabla_{X} loss), -\epsilon, \epsilon)}$  
%         % Peter---
%         %original : \delta^{t+1}\gets \delta^{t} + \text{clip}(\alpha \cdot sign(\nabla_{X} loss), -\epsilon, \epsilon)

     
%      % \mathop{\arg\min}\limits_{ \delta_i \leq \vert \epsilon_i \vert, K_{i} \in K }
%      %            \mathcal{L_s} (\mathcal{C}(X_r))$
	    
% 	    % $\ell \gets \min \mathcal{L}_{ce}(X_r)$
% 	}
% % 	\tcc{Get optimal parameters} 
% %  	$\delta^\star \gets \delta^t$ \\
%  	\tcc{Get final reconstructed samples} 
%         $X^\star \gets X + \delta^\mathcal{T}$ \\
% 	\KwRet $\hat{y} \gets \mathcal{F}(X^\star)$
% \end{algorithm}%



\subsection{Defending against an Adaptive Attack}
\label{sec:dda_method}
We show that \sys is able to defend against an adaptive attack in which the attackers have knowledge about our defense method, including the MAE architecture, attack detection, and repair. To launch an adaptive attack that can fool both MAE and classifier $\mathcal{C}$, an attacker can generate adversarial examples by jointly optimizing the MAE loss and cross-entropy loss for the classifier. We change the optimization problem of $\delta$ to the following Eq.~\ref{eqn:optim_da} for the adaptive attack. 
\begin{multline}
    \label{eqn:optim_da}
    \text{max } 
    \mathcal{L}_{ce}(u, y), \text{s.t.} \quad 
    \mathcal{L}_{mse}((1-b) \odot u, (g_{\phi} \circ f_{\theta})(b \odot u)) \leq \epsilon
    %\vspace{-8mm}
\end{multline}
%\begin{align}
        
    %\text{max} 
    %\mathcal{L}_{ce}(u, y), \text{s.t.} \quad \mathcal{L}_{mse}((1-b) %\odot u, (g_{\phi} \circ f_{\theta})(b \odot u)) \leq \epsilon
   
    % \mathcal{L}_{r}(\mathcal{M}(x_{adv}+\delta), x_{adv}+\delta) \leq  \mathcal{L}_{r}(\mathcal{M}(x_{adv}), x_{adv}) 
    % \vspace{-4mm}
%\end{align}



The attacker should maximize the cross-entropy loss between the ground truth and output probability score of $u$ from backbone model $\mathcal{C}$, while reducing the mean-square error loss of MAE for $u$. As Eq.~\ref{eqn:constrainedoptim_da} formulates, we incorporate the Lagrange multiplier to solve the constrained optimization problem from Eq.~\ref{eqn:optim_da}.

%the Eq.~\ref{eqn:optim_da} as a constrained optimization problem.

\begin{align}
    \label{eqn:constrainedoptim_da}
    \mathcal{L}_{da}(\cdot) = 
    \mathcal{L}_{ce}(u, y)- 
    \lambda \mathcal{L}_{mse}((1-b) \odot u, (g_{\phi} \circ f_{\theta})(b \odot u)),
    %s.t. \parallel \delta \parallel_{p} \leq \epsilon
    % \mathcal{L}_{r}(\mathcal{M}(x_{adv}+\delta), x_{adv}+\delta) \leq  \mathcal{L}_{r}(\mathcal{M}(x_{adv}), x_{adv}) 
    \vspace{-6mm}
\end{align}
where the $\lambda$ is the parameter to control the adversarial budget. If the $\lambda$ increases, the optimization gradually shifts the budget for attacking the classification loss $\mathcal{L}_{ce}$ to the MAE loss $\mathcal{L}_{mse}$. Optimizing with $\mathcal{L}_{da}$, the optimal $\delta$ can be found to generate the defense-aware adversarial samples. We empirically found the trade-off between $\mathcal{L}_{mse}$ and $\mathcal{L}_{ce}$, where the joint optimization on  these two objective functions leads to a degradation of the attack success rate for the defense-aware adversarial samples. Our discovery is consistent with the theoretical analysis presented by Mao et al.~\cite{mao2021adversarial}. Therefore, our defense method is robust against adaptive attacks.



% \subsection{ViT-based Attack Classification}
% The third part of TRAM is attack classification. We further generate 25 classes of adversarial sample with different kinds of attacks and parameters. For instance, we generate FGSM attack with $\ell_\infty$ range from $2/255$ to $8/255$. We leverage the encoder module of MAE and concatenate it with the classification layers for finetuning. We empirically find the MAE can learn the structural pattern of adversarial perturbations on multiple types of adversarial attacks better. Comparing with other transformer-based baselines such as base-ViT, finetuning the MAE-ViT achieves the highest performance which gives us an insight that MAE-ViT have better generalizability. 



% \begin{table}[t]
% \small
% \centering
% \begin{tabular}{cccc}
%            & Epsilon      & Steps  & Alpha    \\ \hline
% FGSM-$\ell_{\infty}$ & 4/255, 8/255  & -          & -     \\
% PGD-$\ell_{\infty}$  & 8/255, 16/255   & 20     & 2/255     \\
% PGD-$\ell_{2}$   & 1         & 20           & 0.2     \\
% APGD-$\ell_{\infty}$ & 8/255      & 10         & -        \\
% CW-$\ell_{2}$     & 1        & 10         & -          \\
% BIM-$\ell_{\infty}$  & 8/255        & 10        & 2/255    \\ \hline
% \end{tabular}
% \caption{Attack Parameters Setting}
% \label{tab:attack_parameter}
% \end{table}

