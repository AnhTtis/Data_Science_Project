\section{Related Works}\label{sec:related}
\paragraph{Self-Supervised Learning}
Self-supervised learning (SSL) is one of the most efficient ways to learn effective representations from images without annotations~\cite{de1994learning,chen2020improved, NEURIPS2020_70feb62b, hendrycks2019using}. Prior works have shown that representations learned from different pretext tasks (e.g., jigsaw puzzles~\cite{noroozi2016unsupervised}, rotation prediction~\cite{gidaris2018unsupervised}, image colorization~\cite{larsson2016learning} and deep clustering~\cite{ji2019invariant}) can be leveraged on several downstream tasks such as image classification~\cite{chen2020simple}, object detection~\cite{doersch2015unsupervised} and test-time domain adaptation~\cite{pmlr-v119-sun20b}. Another well-known branch of SSL is contrastive learning, which aims at grouping associated features for a set of transformations for samples and distancing from other samples with dissimilar features in the dataset~\cite{chen2020big, he2020momentum, park2020contrastive}. Prior works proposed to use SSL method for outlier detection~\cite{hendrycks2019using, sehwag2021ssd, zeng2021adversarial}, which aims at learning generalizable out-of-detection features and rejecting them during the testing time. However, these discriminative SSL tasks learn to discard a large portion of their input information. Representations learned from these tasks keep only a small amount of information or features from the input space. Consequently, these objective functions are less sensitive to adversarial attacks.


%The rationale is that those discriminative SSL tasks learn to discard a large portion of their input information. Representations learned from these tasks only keep a small amount of information/features from the input space. Consequently, their objective functions are less sensitive to adversarial attacks. 

% However, most of them require information from targeted OOD data distribution.


% A contrastive loss is computed based on the features extracted from an encoder.


% \z{do you need another section for adversarial purification?}
% \yun{No, I think we don't need to. Just mentioned them in the first paragraph of related works "adversarial defense method"}

\paragraph{Masked Autoencoder}
MAE learns to reconstruct the randomly masked patches of a given input. Recent research leverages MAE to learn downstream tasks since it allows the models to learn representations beyond the low-level statistics. It first encodes a small subset of unmasked patches to latent space as embedding. Then, the masked tokens are inserted after encoding. The concatenation of unmasked patches' embedding and masked tokens are processed by the decoder to predict the unmasked patches~\cite{MaskedAutoencoders2021, xie2022simmim}. Existing works show the success of mask-based pre-training on different modalities, such as improving point cloud models for 3D object detection~\cite{Voxel-MAE}, and model adaptation on sequential data~\cite{wang2021tsdae}. In ~\cite{gong2022reverse}, the authors proposed to reverse engineer the perturbation and denoise adversarial samples with an autoencoder via class-aware alignment training.
~\cite{gandelsman2022test} leverages MAE on test-time training by reconstructing each test image at test time for corrupted samples. Inspired by this, we leverage MAE on adversarial attack detection and repair. 

\paragraph{Defense for Adversarial Attacks}
% \yun{training / testing-time defense (detection)}
Robustness to adversarial attacks is a critical issue in the machine learning field~\cite{goodfellow2014explaining}. Large amounts of work have studied adversarial attacks in either whitebox or blackbox  (e.g., FGSM, C\&W, and ZOO attack ~\cite {goodfellow2014explaining,carlini2017towards,chen2017zoo}). To improve model robustness on these attacks, adversarial training is one of the most efficient methods~\cite{kurakin2016adversarial, madry2018towards, zhang2019theoretically, mao2019metric}. Madry et al.~\cite{madry2018towards} propose to train the model by minimizing the worst-case loss in a region around the input. TRADES~\cite{zhang2019theoretically} smoothen the decision boundary by considering the ratio between the loss of clean input and the adversarial input. Mao et al.~\cite{mao2020multitask} shows that learning multiple tasks improves adversarial robustness. However, the previously mentioned methods need retraining on the model.

Test-time defense circumvents retraining and largely reduces the computational cost. Most of the test-time adaptation adjusts the model weights with batch samples during the inference time~\cite{zhou2021domain, dou2019domain, li2018learning, zhou2020deep, wang2021tent, croce2022evaluating, gandelsman2022test }. Test-time transformation ensembling (TTE) \cite{perez2021enhancing} augments the image with a fixed set of transformations and ensemble the outputs through averaging. Test-time Training (TTT)\cite{sun2020test} trains the model with an auxiliary SSL task (e.g., rotation prediction) and adapts the model weights based on the SSL loss. MEMO\cite{memo} adapts every sample individually by minimizing the marginal cross-entropy on the transformation set. Instead of adjusting model weights, 
test-time reverse\cite{mao2021adversarial, tsai2023self} aims at adjusting input by adding some noise vectors and optimizing them with self-supervision tasks. 
% In this paper, we leverage the state-of-the-art SSL task, masked autoencoder, in our defense framework. One advantage of performing the autoencoding task is that it is a dense prediction~\cite{mao2020multitask} and works on a single input.  
% Our framework provides a pipeline for detection, reconstruction, and classification of adversarial examples.
% In this work, our focus is on self-supervised test-time defense, combining three components: 

% In this work, our focus is on self-supervised test-time defense, combining three components: detection, reconstruction, and classification.


% \z{check this paper as well: Test-Time Training with Masked Autoencoders}

