% Template for ICME 2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP/ICME LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% ----------------------------------------------------------------c----------
\documentclass{article}
\usepackage{spconf,amsmath,epsfig}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{tikz}

% \usepackage{etoolbox}
% \AtBeginEnvironment{tabular}{\footnotesize}

\newcolumntype{b}{X}
\newcolumntype{s}{>{\hsize=.5\hsize}X}
\newcommand{\heading}[1]{\multicolumn{1}{c}{#1}}
\def\checkmark{\tikz\fill[scale=0.3](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\usepackage{xspace}
\usepackage{float}

% Add a period to the end of an abbreviation unless there's one
% already, then \xspace.
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother


\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

\pagestyle{empty}


\begin{document}\sloppy

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


% Title.
% ------
\title{Aerial-Ground Person Re-ID}
%
% Single address.
% ---------------
\name{H. Nguyen, K. Nguyen, S. Sridharan, C. Fookes}
%Address and e-mail should NOT be added in the submission paper. They should be present only in the camera ready paper. 
\address{}


\maketitle


%
\begin{abstract}
	Person re-ID matches persons across multiple non-overlapping cameras. Despite the increasing deployment of airborne platforms in surveillance, current existing person re-ID benchmarks' focus is on ground-ground matching and very limited efforts on aerial-aerial matching. We propose a new benchmark dataset - AG-ReID, which performs person re-ID matching in a new setting: across aerial and ground cameras. Our dataset contains 21,983 images of 388 identities and 15 soft attributes for each identity. The data was collected by a UAV flying at altitudes between 15 to 45 meters and a ground-based CCTV camera on a university campus. Our dataset presents a novel elevated-viewpoint challenge for person re-ID due to the significant difference in person appearance across these cameras. We propose an explainable algorithm to guide the person re-ID model's training with soft attributes to address this challenge. Experiments demonstrate the efficacy of our method on the aerial-ground person re-ID task. The dataset will be published and the baseline codes will be open-sourced to facilitate research in this area.
\end{abstract}
%
\begin{keywords}
	person re-id, aerial-ground imagery, uav, video surveillance, attribute-guided, two-stream network
\end{keywords}
%
\section{Introduction}
Aerial surveillance recently attracted researchers due to the advancement of drones and cameras \cite{Li2021UAVHumanAL, AerialSurveillance, Zhu2020VisionMD} enabling new settings for person re-ID in aerial imagery. This paper presents a unique collection of Aerial and Ground visual data called AG-ReID as the first public dataset for developing aerial-ground person re-ID. The AG-ReID dataset is collected with aerial and ground images of pedestrians on a university campus. The combination of an aerial platform (UAV/drone) and a fixed ground-based CCTV camera illustrates realistic real-world surveillance scenarios where resources on the ground and in the air are deployed collaboratively for the surveillance task. A DJI XT2 drone captures aerial images with a 24mm lens flying at multiple altitudes from 15 to 45 meters. The unique high-flying altitudes of aerial cameras make persons appear differently in viewpoints, poses and resolution compared to the images of the same person viewed from ground cameras. Therefore, re-identifying persons across aerial-ground setting present some unique and novel challenges in matching. Exemplar images collected in our AG-ReID dataset can be seen in Fig. \ref{fig:perspective}. Our dataset also characterizes each target person with 15 soft-biometric labels for attribute recognition and improving person re-ID. 

\begin{figure} 
	\centering
	\includegraphics[width=0.49\columnwidth]{figs/aerial_perspective.jpg}
	\includegraphics[width=0.49\columnwidth]{figs/ground_perspective.jpg} \\
	\small  (a) Aerial camera \hspace{0.25\columnwidth} (b) Ground camera 
	\caption{Significant difference between elevated and horizontal viewpoint due to cameras' diversity.}
	\label{fig:perspective}
\end{figure}

Learning across aerial-ground cameras has unique challenges compared to learning with ground-ground and aerial-aerial cameras. The critical challenge is the significant differences in person perspectives where persons appearing in aerial images have a higher elevated viewpoint than ground images, making matching difficult. For this paper, we propose an explainable approach to deal with these challenges of the aerial-ground person re-ID task based on soft-biometric attributes. The approach contains two streams, a transformer-based person re-ID model stream and an explainable person re-ID stream. We explore critical attributes contributing to the similarities between two persons where the person's perspective is different. A key point in the explainable approach is that it enables us to visualize the attributes map contributing to the differences between the images of the same person from different ground-aerial cameras. This enables us to explain the person-matching result since people with similar attributes are more likely to be the same person compared to people with similar visual appearances but different attributes. Our proposed approach enables us to explore and develop explainable methods to learn across aerial and ground person re-ID effectively.

We summarize below the four main contributions, including (i) a new aerial-ground person re-ID dataset named AG-ReID, (ii) a two-stream person re-ID model with an explainable approach, (iii) extensive experimental results, and (iv) public release of the AG-ReID dataset and the baseline codes for the two-stream approach.

\section{Related Work}


%\begin{figure}
%	\centering
%	\includegraphics[width=0.5\columnwidth]{figs/example_attributes.jpg}
%	\caption{15 soft-attribute labels.}
%	\label{fig:example_attributes}
%\end{figure}

In this section, we examine and compare existing person re-ID datasets and discuss the explainable trend in person re-ID. 

     	          
\vspace{3px}
\noindent\textbf{Datasets for Person Re-ID}

\vspace{2px}
\noindent \textit{Ground-Ground Person Re-ID datasets}: This is the most popular setting, where person images across multiple ground cameras are matched. Two popular datasets are Market-1501 \cite{Zheng2015ScalablePR} and DukeMTMC-reID \cite{Gou2017DukeMTMC4ReIDAL}. Introduced in 2015, the Market-1501 dataset has 1,501 person identities, and a total of 32,668 person images \cite{Zheng2015ScalablePR}. Compared to Market-1501, DukeMTMC-reID, introduced by \cite{Gou2017DukeMTMC4ReIDAL}, has more pedestrian images (36,411) with fewer identities (1,404). 

\vspace{2px}                     
\noindent \textit{Aerial-Aerial Person Re-ID datasets}: This setting matches person images captured across multiple drones. Two datasets that exist in the literature are PRAI-1581 \cite{Zhang2021PersonRI} and UAV-Human \cite{Li2021UAVHumanAL}. Presented by \cite{Zhang2021PersonRI} in 2020, the PRAI-1581 contains 1,581 subjects and 39,461 images in total. UAV-Human was introduced by Li \etal \cite{Li2021UAVHumanAL} in 2021. UAV-Human was collected by one UAV, flying over three months at 2 to 8 meters in various areas and times (day and night). The dataset can be used for multiple surveillance tasks; however here we only review its person re-ID task. There are, in total, 1,144 identities and 41,290 person images for the aerial-aerial person re-ID task. 
	      	      	          
\vspace{2px}
\noindent \textit{Aerial-Ground Person Re-ID dataset}: To our knowledge, no research has been performed on a large dataset that is accessible to the public for re-identifying persons across aerial and ground cameras. Our work will be the first to contribute to this gap.

\vspace{3px}
\noindent\textbf{Explainable Person Re-ID} Due to the black box-ness of CNN models \cite{Somers2022BodyPR, Wang2021FeatureEA}, there is a trend in developing explainable approaches to perform person re-ID. Previous works attempt to explain CNN models by visualizing salient maps \cite{Selvaraju2019GradCAMVE}, distilling knowledge \cite{Chen2019ExplainingNN} or learning with a decision tree \cite{Zhang2019InterpretingCV}. In the explainable person re-ID domain, the works of \cite{Yang2019TowardsRF, Zhang2020RelationAwareGA} investigate attention learning as attempts to explain the person re-ID model's prediction. Chen \etal \cite{Chen2021ExplainablePR} proposed an interpreter for the person re-ID model using attributes. Although this work achieves high performance, their work focuses only on the ground-ground setting. In the aerial setting, there does not exist any explainable approach. The requirement of explainability is even more critical in the aerial-ground setting as a person's appearance could be significantly different. For this reason, as the first attempt to deal with the aerial-ground challenges, we develop an explainable person re-ID approach in the aerial-ground setting. 

\begin{figure}
    \centering
    \includegraphics[width=0.9 \columnwidth]{figs/DataCollectionArea_hidden.pdf}
    \caption{Data Collection Area: CCTV camera captures ground data in the blue area and UAV camera captures aerial data from the red area. Blue and red areas are non-overlapping. }
    \label{fig:DataCollectionArea}
\end{figure}

\section{AG-ReID Dataset}

\begin{table*}
	\footnotesize
	\caption{Comparison of our AG-ReID dataset with popular person re-ID datasets. As there is no public dataset in this new aerial-ground setting, we compare our dataset with existing public ground-ground and aerial-aerial datasets.}
	\begin{tabularx}{\textwidth}{XXXXXXX} 
		\hline
		\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Dataset Attributes}}}                                            
		  & \multicolumn{2}{c}{\textbf{Ground-Ground }} & \multicolumn{2}{c}{\textbf{Aerial-Aerial }} & \multicolumn{1}{c}{\textbf{Aerial-Ground     }} \\ 
		\cline{3-7}
										
		\multicolumn{2}{c}{}                                  & \multicolumn{1}{c}{Market-1501} 
		&     \multicolumn{1}{c}{DukeMTMC-reID }         
		  & \multicolumn{1}{c}{PRAI-1581  }             & \multicolumn{1}{c}{UAV-Human }              & \multicolumn{1}{c}{AG-ReID (ours)}               
							
		\\ \hline
							
		\multirow{4}{*}{{Person Re-ID  }       }                             
		&        \multicolumn{1}{l}{\# Annotated IDs  }                    
		& \multicolumn{1}{c}{1,501  }                            
		&   \multicolumn{1}{c}{1,404   }             
		&  \multicolumn{1}{c}{1,581}    
		&  \multicolumn{1}{c}{1,144  }                                                 
		&    \multicolumn{1}{c}{388 }                                                  
		\\ 
		\cline{2-7}
									
							
		&        \multicolumn{1}{l}{\# Image Samples with IDs  }            
		&      \multicolumn{1}{c}{32,668    }                          
		&      \multicolumn{1}{c}{36,411   }          
		&      \multicolumn{1}{c}{39,461 }                   
		&  \multicolumn{1}{c}{41,290}                                         
		&     \multicolumn{1}{c}{21,983 }                                                     \\ 
		\cline{2-7}
									
							
		% \multirow{1}{*}{}   
								 
		& \# Attributes                              
		& \multicolumn{1}{c}{$\times$  }                       
		& \multicolumn{1}{c}{$\times$  }           
		& \multicolumn{1}{c}{$\times$  }            
		& \multicolumn{1}{c}{7  }                                    
		&   \multicolumn{1}{c}{15}
		\\ 
		\cline{2-7}
									
		& \multicolumn{1}{l}{\# Image Samples with Attributes} 
		& \multicolumn{1}{c}{$\times$ }            
		& \multicolumn{1}{c}{$\times$ } 
		& \multicolumn{1}{c}{ $\times$ } 
		& \multicolumn{1}{c}{{22,263}}
		& \multicolumn{1}{c}{21,983}
		\\ 	\hline
									
							
		\multicolumn{2}{c}{Diverse Backgrounds}               & \multicolumn{1}{c}{$\times$}   
		& \multicolumn{1}{c}{$\times$   }           
		& \multicolumn{1}{c}{$\times$   }           
		& \multicolumn{1}{c}{\textbf{$\checkmark$}  }                              
		& \multicolumn{1}{c}{\checkmark  }                                           \\ 
		\hline
									
							
		\multicolumn{2}{c}{Occlusion}                         & \multicolumn{1}{c}{$\times$}   
		& \multicolumn{1}{c}{$\times$   }           
		& \multicolumn{1}{c}{$\checkmark$    }      
		& \multicolumn{1}{c}{\textbf{$\checkmark$}}                                
		& \multicolumn{1}{c}{$\checkmark$ }                                           \\ 
		\hline
									
							
		\multicolumn{2}{c}{Camera Views}                      & \multicolumn{1}{c}{fixed}      
		& \multicolumn{1}{c}{fixed }              
		& \multicolumn{1}{c}{mobile}              
		& \multicolumn{1}{c}{mobile  }                                            
		& \multicolumn{1}{c}{fixed \& mobile}                                                \\ 
		\hline
									
							
		\multicolumn{2}{c}{Platforms}                      & \multicolumn{1}{c}{CCTV}      
		& \multicolumn{1}{c}{CCTV        }       
		& \multicolumn{1}{c}{UAV         }       
		& \multicolumn{1}{c}{UAV         }                                    
		& \multicolumn{1}{c}{UAV \& CCTV   }                                             \\ 
		\hline
									
		\multicolumn{2}{c}{Flying Altitude}                      & \multicolumn{1}{c}{$<10m$}      
		& \multicolumn{1}{c}{$<10m$    }
		& \multicolumn{1}{c}{$20\sim60m$    }           
		& \multicolumn{1}{c}{~ $2\sim8m$    }                                           
		& \multicolumn{1}{c}{~ $15\sim45m$  }                                               \\ 
		\hline
									
		\multicolumn{2}{c}{\#UAVs}                      & \multicolumn{1}{c}{0}      
		& \multicolumn{1}{c}{~ 0    }         
		& \multicolumn{1}{c}{~ 2   }          
		& \multicolumn{1}{c}{~ 1   }                                          
		& \multicolumn{1}{c}{~ 1    }                                           
		\\ 
		\hline
									
		% \multicolumn{2}{|c||}{Public Availability}                      & \multicolumn{1}{c|}{Yes}      
		% & ~ No             
		% & ~ Yes           
		% & ~ Yes                                             
		% & ~ Yes         \\                            
		% \hline
									
	\end{tabularx}
	\label{tab:compare_statics}
\end{table*}

\subsection{Dataset Description}
AG-ReID contains data from two cameras located in heavily crowded outdoor environments as illustrated in Fig.~\ref{fig:DataCollectionArea}. The DJI XT2 drone flew at multiple altitudes ranging from 15 to 45 meters, achieving diverse viewpoints and backgrounds. In total, there are 12 pairs of videos in our dataset. We sample the video footage at 30 frames per second for both UAV camera and CCTV camera. The spatial resolution of the videos was as follows: UAV camera:  3840$\times$2160 pixels/frame;  CCTV camera: 704$\times$480 pixels/frame. The cropped person images from the UAV camera range in size from a maximum of 371$\times$678 pixels to a minimum of 31$\times$59 pixels. The cropped person images from the CCTV camera range from 172$\times$413 pixels to a minimum of 22$\times$23 pixels. The low number of pixels of the cropped person images provides a significant challenge for person re-ID, which is accentuated by other artifacts in the data \eg illumination changes, occlusions, motion blur, and varying poses including people walking, riding bikes or scooters, which are present in our database as illustrated in Fig. \ref{fig:Challenges}. We will provide the hard examples in the database and qualitative analysis in the document released with the database. The database was collected over multiple days under different weather conditions. Additional statistical information on the dataset will be documented with the data release. Our final dataset includes 21,983 images and 388 person identities. 

% As illustrated in the examples shown in Fig. \ref{fig:Challenges}, our dataset is challenging due to six factors. The elevated view from a UAV's perspective and various flying altitudes provide our dataset with great scaling diversity. Although we captured video with 4K resolution, the resolution of a cropped person's region of interest is very low, especially when flying at high altitudes. Our dataset also contains occluded persons due to trees, poles, buildings, other persons, etc. We collected our dataset both morning and evening, exhibiting inconsistent non-ideal illumination. Other challenges our dataset has include motion blur and varying poses (\eg people are walking in different trajectories/paths, people riding bikes or riding scooters). These challenging factors in our dataset present practical UAV surveillance cases in the real world. 

\subsection{Annotations}
To detect tiny people on the ground due to high flying altitude, a recent study by \cite{Feng2022WeaklySR} is used to track aerial objects with invariant rotation. In our case, we utilize object tracking code from the work of \cite{yolov5-strongsort-osnet-2022} to detect, track and extract cropped person images from the captured video by ground and aerial camera. We then save images every 30 frames. A cropped person image contains the person within the smallest bounding box. To deal with the tracker's misclassification, we manually removed images that are not persons \eg tree branches or hydrants. We employed three annotators to manually annotate the data set, which was a long and tedious process. The annotations by the three annotators were also cross-checked by each other on a second iteration to ensure the correctness of the annotations. We also manually annotated our AG-ReID dataset with attributes. We partially adopted 15 soft-biometric labels from the work of \cite{Kumar2021ThePA} as they also characterized persons in UAV videos. 
%Since each attribute has several sub-attributes, we choose to present them with the release of the dataset. 
%Fig. \ref{fig:example_attributes} demonstrates our attribute annotations on the person image. 
We compare the statistics of our AG-ReID dataset with related public datasets in Table \ref{tab:compare_statics}. 

\begin{figure}[H]
					
	\centering
					  
	\begin{tabular}{@{}c@{}}
		\includegraphics[width=0.7\columnwidth]{figs/challenge_low-resolution.jpg} \\
		\small (a) Low Resolution                                                  
	\end{tabular}
					
	\begin{tabular}{@{}c@{}}
		\includegraphics[width=0.7\columnwidth]{figs/challenge_blur.jpg} \\
		\small (b) Blur                                                  
	\end{tabular}
					
	\begin{tabular}{@{}c@{}}
		\includegraphics[width=0.7\columnwidth]{figs/challenge_partially-occluded.jpg} \\
		\small (c) Partially Occluded                                                  
	\end{tabular}
					  
	\begin{tabular}{@{}c@{}}
		\includegraphics[width=0.7\columnwidth]{figs/challenge_pose.jpg} \\
		\small (d) Pose                                                  
	\end{tabular}
					  
	\begin{tabular}{@{}c@{}}
		\includegraphics[width=0.7\columnwidth]{figs/challenge_illumination.jpg} \\
		\small (e) Illumination                                                  
	\end{tabular}
					  
	\begin{tabular}{@{}c@{}}
		\includegraphics[width=0.7\columnwidth]{figs/challenge_elevated-view.jpg} \\
		\small (f) Elevated Viewpoint                                             
	\end{tabular}
					  
	\caption{Exemplar challenges to the person re-ID task in our AG-ReID dataset due to varying flying altitudes and view variations between aerial and ground cameras.}
	\label{fig:Challenges}
\end{figure}




\section{Two-stream Aerial-Ground Re-ID}
The overall architecture of the proposed two-stream person re-ID model is depicted in Fig. \ref{fig:explainable_overal_architecture}. Our proposed two-stream model for the person re-ID task includes a transformer-based person re-ID stream and an explainable re-ID stream. We train the first transformer-based stream to effectively learn the discriminative feature of images and produce salient feature maps. We aggregate these feature maps with attribute attention maps generated from the explainable re-ID model trained in Stream 2 to obtain attribute feature maps. From the feature maps in Stream 1 and attribute feature maps in Stream 2, we compute two metric distances with generalized mean pooling (GeM) respectively and achieve metric distance and attribute-guided distance for our model. We describe each stream in detail as follows.

\begin{figure*}[hbt!]
	\centering
	\includegraphics[width=0.9\linewidth]{figs/explainable_overal_architecture_2.pdf}
	\caption{The overall architecture of explainable person re-ID model with vision transformer (ViT) backbone. The architecture includes two streams. While Stream 1 computes the pairwise distance from the input of pair images, Stream 2 utilizes person attributes to mitigate appearance differences of one person due to varying flying altitudes and view variations between aerial and ground cameras. Stream 2 attached with Attribute Decomposed Head (ADH) produces an attribute-guided attention map for each attribute. The two-stream model's total loss, including a metric distillation loss and an attribute loss.}
	\label{fig:explainable_overal_architecture}
\end{figure*}


\vspace{-3px}
\subsection{Stream 1 - Transformer-based Re-ID Model}
We first train a person re-ID network, $\mathcal{F(\cdot)}$. We choose SBS \cite{He2020FastReIDAP} as the base for our re-ID model. This base is replaceable with other popular models such as MGN \cite{Wang2018LearningDF}, BOT \cite{Luo2019BagOT}, etc. Our model is built with a ViT backbone \cite{Dosovitskiy2021AnII} due to its encouraging performance and low-resolution feature maps adapting well with our dataset where aerial image's resolution is low. We also test with other backbones \eg ResNet50 \cite{He2016DeepRL}, and OSNet \cite{Zhou2019OmniScaleFL}. We can extract each image pair $(x_i, y_i)$ from the model's last layer feature map $(F_i, F_j)$. Applying generalized mean pooling (GeM), we obtain feature vector $(f_i, f_j)$ and then calculate pairwise distance $d_{i,j}$, which we use to calculate metric distillation loss in Stream 2 explainable re-ID network. The metric distillation loss is vital in keeping the distance metrics between two streams consistent.

\vspace{-6px}
\subsection{Stream 2 - Explainable Re-ID Model}
The explainable re-ID network, $\mathcal{G(\cdot)}$, has a similar structure to the re-ID model $\mathcal{F(\cdot)}$. The two models share low and mid-level layers. These layers focus on visual texture and colour, essential for learning attributes. An attributes decompose head (ADH) is designed after the last layer of explainable re-ID network $\mathcal{G(\cdot)}$. ADH consists of convolution layer $\frac{C}{8}\times3\times3$ where $C$ is the channel number of the explainable network $\mathcal{G(\cdot)} $'s last convolution layer, a $M\times1\times1$ convolution layer where $M$ is the number of attributes, and $\delta(\cdot)$ is the activation function. Activation function $\delta(\cdot)$ is important as it generates attribute-guided attention maps (AAMs) for explainable visualization and exclusive attribute learning. The sigmoid activation function $\delta(\cdot)$ is calculated as: 
\begin{equation}
	\footnotesize
	\delta(x) = \begin{cases}
	\mathcal{K}\cdot(x+1)^{\mathcal{T}}, x > 0 \\
	\mathcal{K}\cdot e^{x}, x<=0, 
	\end{cases}
	\label{eq:activate_function}
\end{equation}
where $\mathcal{K}$ is growth factor $(0,1)$ and $x$ is output of last convolution layer in ADH. With this design, ADH can effectively decompose salient regions of attributes without being biased toward the imbalanced distribution of attributes.

From ADH, we can obtain AAMs \eg $A_i$ and $A_j$ ${\in}R^{M\times w \times h}$ where $M$ is the number of attributes, $w$, and $h$ is the width and height of attention maps. Each $A_i$ and  $A_j$ contains matrix array of $M$ such as $(A_i^1, A_i^2, ..., A_i^M)$  and $(A_j^1, A_j^2, ..., A_j^M)$ where $A_i^k$ or $A_j^k$ are the attention map of $k^{th}$ attribute. Consequently, we compute attribute-guided feature map $F_i^k$ and $F_j^k$ by: 
 
\begin{equation}
	\footnotesize
	\begin{cases}
		F_i^k = F_i \bigotimes A^k  \\
		F_j^k = F_j \bigotimes A^k, 
	\end{cases}
	\label{eq:attribute_feature_map}
\end{equation}
where $A_i^k$, $A_j^k$ are attention map, $\bigotimes$ is element-wise multiplication. Each image can obtain $M$ attribute-guided feature maps where a pixel activated by attribute $k^{th}$ is highlighted, and another pixel is depressed. We then obtain attribute-guided feature vector $f_i^k$, $f_j^k$ from the attribute-guided feature map using GMP and calculate the attribute-guided distance of each attribute accordingly.  

\subsection{Loss Functions}
We calculate the loss for our explainable two-stream re-ID network. The total loss is formulated as follows:  
\begin{equation}
	\footnotesize
	L = L_d + \alpha  L_{p1} + \beta L_{p2},
	\label{eq:total_loss}
\end{equation}
where $L_d$ is metric distillation loss, $ L_{p1}$ and $L_{p2}$ are parts of attribute prior loss, and $\alpha$ and $\beta$ are balance factors.

\vspace{3px}
\noindent \textbf{Metric distillation loss.}  We calculate $L_d$ using pairwise distance $d$, the product of re-ID model $\mathcal{F(\cdot)}$ and attribute-guided distance $d_{i,j}^k$ from explainable model $\mathcal{G(\cdot)}$ as:
\begin{equation}
	\footnotesize
	L_d = |d_{i,j} - \sum^M_{k=1} d_{i,j}^k|.
	\label{eq:metric_loss}
\end{equation}

For each attribute image pair $(x_i, y_i, a_i)$ and $(x_j, y_j, a_j)$, $x_i$ is the person image, $y_i$ is the annotated identity, $a_i$ is the array of attributes $(a_i^1, a_i^2, ..., a_i^M)$ with $a_i^k$ is a binary vector, $k^{th}$ attribute and $M$ is the number of attributes, we can calculate a pairwise attribute vector, which is formulated by:
\begin{equation}
	\footnotesize
	a_{i,j} = a_i \bigoplus a_j,  
\end{equation}
where $\bigoplus$ exclusive OR and $a_{i,j}$ contains common attributes and exclusive attributes of $x_i$ and $x_j$. 
 
\vspace{3px}
\noindent \textbf{Attribute prior loss.} The equation to calculate the first part of attribute prior loss is solved by: 
\begin{equation}
	\footnotesize
	\begin{split}
		L_{p1} = max (0, (\frac{M_E}{M})^v - \sum_{e=1}^{M_E} \frac{d^e_{i,j}}{\hat{d}_{i,j}}) \\
		+ max (0, \sum ^ {M - M_e} _ {c=1} \frac{d^c_{i,j}}{\hat{d}_{i,j}} - 1 + (\frac{M_E}{M})^v ),
	\end{split}
	\label{eq:att_loss_p1}
\end{equation}
where $M_E$ is number of exclusive attributes derived from $a_{i,j}$, $M$ is number of all attributes, $v$ is vector $(0,1)$ regulating exclusive attributes, $d^e_{i,j}$ is distance of exclusive attributes, $d^c_{i,j}$ is distance of common attributes and $\hat{d}_{i,j}$ is decomposed distance from pairwise distance $d_{i,j}$, 


The equation for the second part of attribute prior loss is defined as:
\begin{equation}
	\footnotesize
	\begin{split}
		L_{p2} = \sum ^{M_E} _{e=1} max (0, e^{-\lambda} \frac{(\frac{M_E}{M})^v}{M_E} - \frac{d_{i,j}^e}{\hat{d}_{i,j}}) \\
		+ \sum ^{M - M_E} _{c=1}max (0, \frac{d_{i,j}^c}{\hat{d}_{i,j}}- e^{\lambda} \frac{1-(\frac{M_E}{M})^v}{M-M_E}),
	\end{split}
	\label{eq:att_loss_p2}
\end{equation}
where
\begin{equation}
	\footnotesize
	\lambda = \frac{1}{2} ln \frac{M - M_E(\frac{M_E}{M})^v}{M_E(1-(\frac{M_E}{M})^v)}.
\end{equation}
The attribute prior loss, the sum of $ L_{p1}$ and $L_{p2}$, is the main factor in the explainable network that learns the difference between two persons through exclusive attributes effectively.
 
\section{Experiments}
% In this section, we first discuss splitting up our dataset for training and evaluation. We continue the section by presenting the detailed implementation and evaluation metrics. We then evaluate typical baseline methods and state-of-the-art re-ID models with different modalities datasets. Finally, we report explainable re-ID's performance with various backbones and baselines on our AG-ReID dataset.   

\subsection{Dataset Partition}
In our AG-ReID dataset, there are 21,893 images and 388 person identities. Initially, we had 397 identities in our dataset. We split identities for train and testing with a ratio of 50:50. We randomly choose 199 identities with 11,554 images for our training set. The rest 189 identities and 12,464 images are for testing. Note that 189 identities are number after we removed nine identities from the testing set due to the irrelevant details presented in these pedestrians. In the testing set, we get a random number of images ranging from one to six from each identity per camera to use as a query and keep the rest as the gallery. Hence query and gallery images in our dataset are 2,033 query images and 10,429 gallery images, respectively. Our query set has 1,701 images from the aerial camera and 962 images from the ground camera. For the gallery set, these numbers are 7,204 aerial images and 3,255 ground images. 

\begin{table*}
	\footnotesize
	\caption{Comparison of the performance of baselines and state-of-the-arts person re-ID methods on the ground-ground dataset (Market-1501), the aerial-aerial dataset (UAV-Human), and the aerial-ground dataset (AG-ReID).}
	\begin{tabularx}{\textwidth}{bssssssss}
		\hline
		\multirow{3}{*}{\textbf{Model}}
		  & \multicolumn{2}{c}{\textbf{Ground $\rightarrow$ Ground}} 
		  & \multicolumn{2}{c}{\textbf{Aerial $\rightarrow$ Aerial}} 
		  & \multicolumn{2}{c}{\textbf{Aerial $\rightarrow$ Ground}} 
		  & \multicolumn{2}{c}{\textbf{Ground$\rightarrow$ Aerial}}  \\
		  \cline{2-9}
										         
		  & \multicolumn{2}{c}{Market 1501}                          
		  & \multicolumn{2}{c}{UAV-Human}                            
		  & \multicolumn{2}{c}{AG-ReID}                              
		  & \multicolumn{2}{c}{AG-ReID}                              
		\\   \cline{2-9}
										         
		  & mAP                                           
		  & Rank-1                                        
		  & mAP                                           
		  & Rank-1                                        
		  & mAP                                           
		  & Rank-1                                        
		  & mAP                                           
		  & Rank-1                                        
		\\  \hline
										        
		BoT(R50)\cite{Luo2019BagOT}
		  & 83.95                                         
		  & 94.77                                          
		  & 63.41                                          
		  & 62.48                                         
		  & 61.37                                          
		  & 73.83                                         
		  & 62.58                                         
		  & 72.45  \\
										        
		MGN(R50)\cite{Wang2018LearningDF}
		  & 86.90                                          
		  & 95.70                                          
		  & \textbf{70.40}                                
		  & \textbf{70.38}                                
		  & 63.77                                         
		  & 75.93                                         
		  & \textbf{69.83}                               
		  & \textbf{79.42} \\
										
		SBS(R50)\cite{He2020FastReIDAP}
		  & \textbf{88.20}                               
		  & \textbf{95.40}                               
		  & 65.93                                        
		  & 66.38                                        
		  & \textbf{67.51}                               
		  & \textbf{79.85}                               
		  & 67.48                                        
		  & 77.75 \\
		\hline
	\end{tabularx}
	\label{comp_sota_dataset}
\end{table*}


\subsection{Implementation and Evaluation Metrics}
For the experiments, to make it comparable between datasets, we use state-of-the-art re-ID, \eg BoT Baseline \cite{Luo2019BagOT}, StrongerBaseline (SBS) \cite{He2020FastReIDAP}, and MGN \cite{Wang2018LearningDF}. Due to their popularity, we chose three backbones, ResNet-50, ViT (Vision Transformer) and OSNet, to build backbones for the person re-ID models. These backbones all use parameters pre-trained on ImageNet \cite{Deng2009ImageNetAL}. We train models with Adam optimizer \cite{Kingma2015AdamAM} using learning rate $lr = 10^{-4}$. For the model built with the ViT backbone, we adopt SGD optimizer with learning rate $lr = 10^{-3}$. We train the explainable network (interpreter) in Stream 2, similar to the re-ID model in Stream 1. This explainable model uses SBS \cite{He2020FastReIDAP} as the base and the identical three backbones with shared stages, i.e., convolution layers from the re-ID model during inference. We trained the explainable network on the AG-ReID training set with an attribute number configured to 88, a binary version derived from 15 annotated attributes. The explainable network is trained with Adam optimizer and a learning rate of $lr = 10^{-4}$.  For evaluation, we use two standard metrics, i.e. mean Average Precision (mAP) \cite{Zheng2015ScalablePR}, and Cumulative Matching Characteristics (CMC-$k$) or Rank-k matching accuracy \cite{Wang2007ShapeAA}. The first metric, mAP, calculates retrievals performance on average based on multiple ground truth labels. The second metric, CMC-$k$, is a probability metric measuring correct matching(s) in the retrieved results of the top-k rank. Specifically, we report rank-1, where $k=1$ for CMC-$k$.

\subsection{Comparison with State-of-the-art Models} \label{Comparison with State-of-the-art Models}
We report in this section the performance of popular re-ID models across three datasets in Table \ref{comp_sota_dataset}. Firstly, we test our dataset with Strong Baseline (BoT) \cite{Luo2019BagOT} using ResNet50 as the backbone and achieve an mAP of 61.37\% and a rank-1 accuracy of 73.83\% for aerial to ground setting. On the ground to aerial setting, this performance shows a drop of 1.38\% in rank-1 accuracy at 72.45\% and an increase of 1.21\% in mAP at 62.58\%.  We then evaluate and compare between datasets the performance of MGN \cite{Wang2018LearningDF} and Stronger Baseline (SBS) \cite{Zhou2019OmniScaleFL} model with ResNet50 backbone. At first sight, these two models have similar performance to the BoT model from the first test. On closer investigation, SBS (R50) is the only model out of the three tested models that perform better on our dataset with aerial to the ground setting than on the UAV-Human dataset. 

\subsection{Ablation Studies}
In this section, we investigate the impact of the explainable stream on the re-ID baselines with three different backbones on the AG-ReID dataset. Overall, we have achieved improved rank-1 accuracy for all models on aerial-to-ground and ground-to-aerial settings. Two out of six models show improvement, and one model has a slight decrease ($<0.5\%$) in mAP. The performance results are summarized in Table \ref{tab:interpreter_reweight_AG}. For the aerial to ground setting, the explainable SBS model with ResNet50 backbone achieves mAP of 60.93\% and rank-1 accuracy of 74.21\%, which is higher than the performance of the re-ID model by 1.16\% and 0.67\%, respectively. With ViT backbone, the explainable model achieves marginally lower mAP, 66.71\% compared to 67.08\% of the re-ID model. In the last model built with OSNet backbone, both mAP and rank-1 accuracy of the explainable re-ID model are higher than the re-ID model, 59.73\% and 74.50\%, respectively. We visualize examples of this improvement in Fig. \ref{fig:vis_explainable_rank1}, which presents a visualization of the explainable re-ID model improving rank-5 accuracy by pushing different identities further away in the similarity distance. In addition, the explainable re-ID model also outputs attention maps of attributes describing what attributes impact the similarity distance between two persons and where these attributes are.  

While our baseline method has some commonalities to \cite{Chen2021ExplainablePR}, it is significantly different and has  innovations to improve the performance by up to 7\% over \cite{Chen2021ExplainablePR}, thus providing the state-of-the-art performance benchmark on this new person re-ID database which we are releasing. This performance improvement has been achieved by using the following, which is different to \cite{Chen2021ExplainablePR}: (i) 2D adaptive average pooling, (ii) automatic mixed precision (AMP) training (GradScale) to train our model, (iii) training our model with 88 binary vector attributes derived from 15 soft attribute labels. In addition, we have conducted evaluations with different backbones (ResNet, ViT and OSNet) to select the optimal \ie ViT. The final benchmark performance that we have established on the new database, using the optimal backbone selection (i.e., Vision Transformer), is a clear indication of the challenge that person re-ID across aerial and ground data presents in practice and should spur further research activity in this area to improve performance.  

\begin{table}    
    \footnotesize
    \centering
    \caption{Impact of the explainable stream on the re-ID baseline with different backbones on the AG-ReID dataset.}
    \begin{tabularx}{\columnwidth}{bssss}
        \hline
        \multirow{2}{*}{\textbf{Model}}  &
        \multicolumn{2}{c}{\textbf{Aerial $\rightarrow$ Ground}} &
        \multicolumn{2}{c}{\textbf{Ground $\rightarrow$ Aerial}} \\
        \cline{2-5}
                                                     
        &
          mAP                                                       
          & Rank-1                                                
          & mAP                                                   
          & Rank-1                                                
        \\ \hline
        
        SBS(R50)
          & 59.77                                                  
          & 73.54                                                 
          & 62.27                                                 
          & 73.70                                                    
        \\ 
                                                     
        $+$ Explainable
          & 60.93                                                 
          & 74.21                                                  
          & 62.99                                                 
          & 74.74                                                    
        \\ 
                                                     
                                                     
                                                     
        SBS(ViT) 
          & 67.08                                                  
          & 77.27                                                  
          & 67.14                                                    
          & 77.13                                                    
        \\ 
                                                     
        $+$ Explainable
          & 66.71                                                  
          & 77.75                                                 
          & 67.12                                                  
          & 79.52                                                    
        \\ 
                                                     
                                            
                                                     
        SBS(OSNet) 
          & 58.32                                                  
          & 72.59                                                  
          & 60.99                                                 
          & 74.22                                                  
        \\ 
                                                     
        $+$ Explainable
          & 59.73                                                     
          & 74.50                                                     
          & 62.02                                                     
          & 75.05                                                    
        \\ 
                                                     
                                                      
        BoT(R50) 
          & 55.47                                                    
          & 70.01                                                  
          & 58.83                                                     
          & 71.20                                                    
        \\ 
                                                     
        $+$ Explainable
          & 56.64                                                  
          & 71.35                                                    
          & 59.72                                                  
          & 72.45                                                    
        \\ 
        BoT(OSNet) 
          & 55.64                                                    
          & 70.29                                                  
          & 58.38                                                    
          & 73.49                                                    
        \\ 
                                                     
        $+$ Explainable
          & 56.49                                                    
          & 71.25                                                     
          & 59.15                                                 
          & 74.43                                                    
        \\ 

        BoT(ViT) 
		  & { 72.38  }                                                  
		  & {81.28  }                                                   
		  & {73.35 }                                                    
		  & { 82.64    }                                                
		\\ 
												         
		$+$ Explainable
		  & { \textbf{72.61}}                                           
		  & { \textbf{81.47}}                                           
		  & { \textbf{73.39} }                                          
		  & { \textbf{82.85} }                                          
		\\ \hline
                                       
\end{tabularx}
	\label{tab:interpreter_reweight_AG}
					 
\end{table}

\begin{figure}
	\centering
	\includegraphics[width=1\columnwidth]{figs/improved_rank_5.pdf}
	\caption{Visualization of the explainable re-ID model with attributes improving Rank-5 accuracy.}
	\label{fig:vis_explainable_rank1}
\end{figure}

\section{Conclusion}
Re-identifying persons across aerial and ground data is a new challenge for the person-ID task, which has become essential due to aerial surveillance systems' proliferation and potential utility. In this paper, we collect the first aerial-ground person re-ID dataset with 21,893 images and 388 person identities. Our dataset contains multiple challenging factors including elevated views, low resolution, motion blur, occlusion, diverse human pose, and non-ideal illumination. These new challenges require new approaches to make person re-ID robust across aerial and ground data. We propose an explainable re-ID approach with two streams incorporating attributes for the aerial-ground setting where the same person can look significantly different due to different perspectives. Extensive experiments show that the explainability in aerial-ground person re-ID through an attribute attention map helps improve the re-ID model's ranking accuracy and performance. The AG-ReID dataset and our baseline codes are publicly available to facilitate future research in this new and vital person re-ID setting. In addition, we will release video tracks to enhance the dataset in the future.  

\section{Acknowledgements.} \quad This research was supported by Discovery Project Grant DP200101942 awarded by the Australian Research Council (ARC). Ethical approval for acquiring and processing data can be presented if required. The facial regions of participants are pixelated for privacy purposes.


% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\small
\bibliographystyle{IEEEbib}
\bibliography{icme2023template}

% \section{Appendix}
% \subsection{Data Collection Map}
% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{figs/DataCollectionArea_hidden.pdf}
%     \caption{Data Collection Area: CCTV camera captures ground data in blue area and UAV camera captures aerial data from area marked as red. Blue and red areas are non-overlapping. }
%     \label{fig:DataCollectionArea}
% \end{figure}

% \subsection{Images Comparision}
% \begin{figure*}
% \centering
%   \includegraphics[width=0.7\linewidth]{figs/compare_images_dataset.pdf} 
%   \caption{Example images of Market-1501, Duke-MTMC, PRAI-1581, UAV-Human and our AG-ReID. Compared to the other four same-domain datasets, ours presents unique challenges in various perspective views between persons from ground (bottom row) and aerial (top row) images.  }
%   \label{fig:dataset_comp}
% \end{figure*}

% \subsection{Soft Attributes}


\end{document}