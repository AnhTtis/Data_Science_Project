\documentclass[twoside,11pt]{article}

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

\usepackage{arxiv}
\usepackage{amsmath}
\usepackage{graphics,graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{subcaption}

%
\usepackage{tikz,array}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{shapes.arrows}
\usetikzlibrary{calc}
\usetikzlibrary{intersections}

\usepackage[utf8]{inputenc}
%

\newcommand{\dataset}{{\cal D}}
\newcommand{\grad}{{\mathrm{grad}}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\stiefel}{{\mathrm{St}(p, n)}}
\newcommand{\sk}{{\mathrm{skew}}}
\newcommand{\sym}{{\mathrm{sym}}}
\newcommand{\tangent}{{\mathrm{T}_X\stiefel}}
\newcommand{\pierre}[1]{{\color{red}\textbf{Pierre:} #1}}

\newcommand{\bR}{\mathbb{R}}    %
\newcommand{\Jacob}[2]{\mathcal{J}_{#2}\left(#1\right)}
\newcommand{\vect}[1]{\mathrm{vec}\left(#1\right)}
\newcommand{\cN}{\mathcal{N}} %
\newcommand{\cL}{\mathcal{L}} %
\newcommand{\vectHess}[1]{H_{#1}} %
\newcommand{\nfX}{\nabla f(X)} %
\newcommand{\inner}[2]{\left\langle#1,\,#2\right\rangle} %
\DeclareMathOperator{\Tr}{Tr} %
\newcommand{\bigO}{\mathcal{O}} 
\newcommand{\Lc}{L} %
\newcommand{\gradc}{L'} %
\newcommand{\Lcr}{\hat{L}} %
\newcommand{\Sc}{\alpha} %

%
\newcommand{\bgcomm}[1]{{\it \color[RGB]{47,112,183} #1}}

%
\newcommand{\svcomm}[1]{{\it \color{blue} #1}}
%

%
\newcommand{\pacomm}[1]{{\it \color{red} #1}}
%


%

\jmlrheading{1}{2022}{1-48}{4/00}{10/00}{ablin22a}{Ablin, Vary, Gao, and Absil}

%

\ShortHeadings{Infeasible algorithms for orthogonality constraints}{Ablin, Vary, Gao, and Absil}
\firstpageno{1}

\begin{document}

\title{Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints}
%

\author{\name Pierre Ablin \email pierre.ablin@apple.com \\
       \addr Apple
       \AND
       \name Simon Vary \email simon.vary@uclouvain.be\\
       \addr ICTEAM/INMA, UCLouvain
       \AND
       \name Bin Gao \email gaobin@lsec.cc.ac.cn \\
       \addr ICMSEC/LSEC, AMSS, Chinese Academy of Sciences
       %
       %
       %
       \AND
       \name P.-A. Absil \email pa.absil@uclouvain.be \\
       \addr ICTEAM/INMA, UCLouvain
       }

\editor{}

\maketitle

\begin{abstract}%
Orthogonality constraints naturally appear in many machine learning problems, from Principal Components Analysis to robust neural network training. They are usually solved using Riemannian optimization algorithms, which minimize the objective function while enforcing the constraint.
However, enforcing the orthogonality constraint can be the most time-consuming operation in such algorithms. 
Recently, \citet{ablin2022fast} proposed the Landing algorithm, a method with cheap iterations that does not enforce the orthogonality constraint but is attracted towards the manifold in a smooth manner.
In this article, we provide new practical and theoretical developments for the landing algorithm.
First, the method is extended to the Stiefel manifold, the set of rectangular orthogonal matrices. We also consider stochastic and variance reduction algorithms when the cost function is an average of many functions.
We demonstrate that all these methods have the same rate of convergence as their Riemannian counterparts that exactly enforce the constraint.
Finally, our experiments demonstrate the promise of our approach to an array of machine-learning problems that involve orthogonality constraints.
\end{abstract}

\begin{keywords}
  Orthogonal manifold, Stiefel manifold, stochastic optimization, variance reduction
\end{keywords}

%

%


\section{Introduction}

Letting $f$ a function from $\mathbb{R}^{n\times p}$ to $\mathbb{R}$, we consider the problem of optimizing $f$ with orthogonality constraints:

\begin{equation}
    \min_{X\in\bR^{n\times p}} f(X), \qquad \text{s.t.}\quad X \in \stiefel = \{X\in\mathbb{R}^{n\times p}|\enspace X^\top X = I_p \}, \label{eq:optim_stiefel}
\end{equation}
where $\stiefel$ is \emph{the Stiefel manifold}.
Such a problem appears naturally in many machine learning problems as a way to control dissimilarity between learned features, e.g.~in principal component analysis (PCA) \citep{hotelling1933analysis}, independent component analysis~\citep{hyvarinen1999fast, ablin2018faster}, canonical correlation analysis~\citep{hotelling1936relations}, and more recently, for the training of neural networks to improve their stability \citep{arjovsky2015unitary, zhang2021orthogonality, wang2020orthogonal} and robustness against adversarial attacks \citep{cisse2017parseval, li2019orthogonal, li2019preventing, singla2021skew}. %

%
%
%

Riemannian optimization techniques are based on the observation that the orthogonality constraints in \eqref{eq:optim_stiefel} define a smooth matrix manifold \citep{absil2008optimization, boumal2023introduction} called the Stiefel manifold. The smooth geometry of the manifold constraint allows for the extension of many optimization techniques from the Euclidean space to the manifold setting, including second-order methods~\citep{absil2007trust}, stochastic gradient descent~\citep{bonnabel2013stochastic}, and variance-reduced methods \citep{zhang2016riemannian, tripuraneni2018averaging, zhou2019faster}. 

A crucial part of Riemannian optimization methods is the use of \emph{retraction} \citep{edelman1998geometry, absil2012projection}, which is a projection map on the manifold preserving the first-order information, and ensures  the individual iterates remain on the manifold constraint. Computing retractions with the orthogonality constraints involves linear algebra operations, such as matrix exponentiation, inverse, or QR decomposition. In some applications, e.g.,~when evaluating the gradient is relatively cheap, computing the retraction is the dominant cost of the optimization method. 
In these cases, unlike in Euclidean optimization, ensuring that the iterates move on the manifold can be much more costly than computing the gradient direction.
Additionally, the need to perform retractions, and more generally, to take into account the curvature of the manifold, causes challenges in developing accelerated techniques in Riemannian optimization that the community has just started to overcome~\citep{, becigneul2018riemannian, alimisis2021momentum, criscitiello2022negative}. As a result, practitioners in deep learning sometimes rely on the use of adding a squared penalty term and minimize $f(X) + \lambda \cN(X)$ with $\cN(X) = \| X^\top X - I_p \|^2/4$ in the Frobenius norm,  which does not perfectly enforce the constraint. 
%

%
%
%


Unlike Riemannian techniques, where the constraint is exactly enforced in every iteration, and the squared penalty method, where the optimum of the problem is not exactly on the manifold, we employ a method that is in between the two. Motivated by the previous work of \cite{ablin2022fast} for the orthogonal matrix manifold, we design an algorithm that does not enforce the constraint exactly in every iteration but instead controls the maximum distance to the constraint employing inexpensive matrix multiplication.
%
%
%
%
%
%
%
Finally, the iterates \emph{land} on the manifold exactly at convergence, to a critical point of the problem~\eqref{eq:optim_stiefel}. 

%
%
%
%
%
%
%
%

%

The following subsection provides a brief prior on the current optimization techniques for solving \eqref{eq:optim_stiefel}. The rest of the paper is organized as follows
\begin{itemize}
  \item{In Section~\ref{sec:landing} we extend the landing algorithm to $\stiefel$; this way, the algorithm is no longer restricted to square matrices. By bounding the step-sizes we guarantee that we remain in $\stiefel^\varepsilon$. We also introduce a new merit function.}
  \item{In Subsection~\ref{subsec:landing_gradient}, thanks to the
  new merit function, we greatly improve the theoretical results of \cite{ablin2022fast}: we demonstrate that the basic landing method with constant step size achieves $\inf_{k\leq K}\|\grad f(X_k)\|^2 =O(K^{-1})$,}
  \item{In Subsection~\ref{subsec:sgd}, we introduce a stochastic algorithm dubbed stochastic landing algorithm. We show that with a step size decaying in $K^{-\frac12}$,it achieves $\inf_{k\leq K}\|\grad f(X_k)\|^2 =O(K^{-\frac12})$.}
  \item{In Subsection~\ref{subsec:SAGA}, we extend the SAGA algorithm and show that the SAGA landing algorithm achieves $\inf_{k\leq K}\|\grad f(X_k)\|^2 =O(K^{-1})$. We recover each time the same convergence rate and sample complexity as the classical Riemannian counterpart of the methods---that uses retractions.}
  \item{In Section \ref{sec:experiments}, we numerically demonstrate the merits of the method when computing retractions is a bottleneck of classical Riemannian methods.}
  %
  %
  %
  %
  %
  %
  %
\end{itemize}

Regarding the convergence speed results, the reader should be aware that we use the \emph{square norm} of the gradient as a criterion, while some readers might be more familiar with results stated without a square.\footnote{When this work was ready to be made public, the preprint~\citep{schechtman2023orthogonal} appeared that pursues similar goals. It addresses the more general problem of equality-constrained optimization, it uses a different merit function than the one we introduce in Section~\ref{sec:merit} and it does not consider variance reduction as we do in Section~\ref{subsec:SAGA}. The numerical experiments also differ considerably, as it only considers a Procrustes problem, while we experiment with deep neural networks.}
%
%
%
%
%
%
%
%
%
%


%


%
\paragraph{Notation} The norm of a matrix $\|M\|$ denotes the Frobenius norm, i.e.~the vectorized $\ell_2$ norm. We denote $I_p$ the $p\times p$ identity matrix and the Stiefel manifold as $\stiefel$, which is the set of $n\times p$ matrices such that $X^\top X = I_p$. The tangent space of the Stiefel manifold at $X\in \stiefel$ is denoted by $\tangent$. The projection on the set of $p \times p$ skew-symmetric matrices denoted $\sk_p\subset \bR^{p\times p}$ is $\sk(M) = \frac12(M - M^\top)$ and on the set of symmetric matrices is $\sym(M) = \frac12(M + M^\top)$. The exponential of a matrix is $\exp(M)$. The gradient of a function $f:\mathbb{R}^{n\times p}\to \mathbb{R}$ is denoted by $\nabla f$ and we define its Riemannian gradient as $\grad f(X) = \sk(\nabla f(X)X^\top)X$ for all $X\in\mathbb{R}^{n\times p}$ as explained in detail in Section~\ref{sec:landing}. We say that a function is $\Lc$-smooth if it is differentiable and its gradient is $\Lc$-Lipschitz. The constant $\Lc$ is then the smoothness constant of $f$.

%

%

\subsection{Prior Work on Optimization with Orthogonality Constraints}

%
%
%
%
%
%
%
%

Equation~\eqref{eq:optim_stiefel}
is an optimization problem over a matrix manifold. In the literature, we find two main approaches to solving this problem, reviewed next.
%
\subsubsection{Trivializations} This approach proposed by \citet{lezcano2019cheap, lezcano2019trivializations} consists in finding a differentiable surjective function $\phi:E\to \stiefel$ where $E$ is a
%
Euclidean space, and to solve
\begin{equation}
  \label{eq:trivialization}
  \min_{A\in E}f(\phi(A))\enspace.
\end{equation}
%
The main advantage of this method is that it turns a Riemannian optimization problem on $\stiefel$ into an optimization on a Euclidean space,
for which we can apply all existing Euclidean methods, such as
gradient descent, stochastic gradient descent, or Adam. This is especially convenient in deep learning, where most standard optimizers are not meant to handle Riemannian constraints.
However, this method has two major drawbacks. First, it can drastically change the optimization landscape. Second, the gradient of the function that is optimized is, following the chain rule, $\nabla (f \circ \phi) = \left(\frac{\partial \phi}{\partial z}\right)^\top\nabla f \circ \phi$, and the Jacobian-vector product can be very expensive to compute.

To give a concrete example, we consider the parametrization $\phi$ used by \citet{singla2021skew}: $\phi(A) = \exp(A)\begin{pmatrix}
  I_p \\ 0
\end{pmatrix}$, where $A\in\sk_n$, with $\sk_n$ the set of $n\times n$ skew symmetric matrices. This corresponds to the first $p$ columns of $\exp(A)$. We see that computing this trivialization requires computing the exponential of a potentially large $n\times n$ matrix. Furthermore, when computing the gradient of $f\circ \phi$, one needs to compute the Jacobian-vector product with a matrix exponential, which requires computing a larger $2n \times 2n$ matrix exponential~\citep{lezcano2019cheap}: this becomes prohibitively costly when $n$ is large.
\subsubsection{Riemannian optimization}
This approach consists in extending the classical Euclidean methods such as
gradient descent or stochastic gradient descent to
the Riemannian setting. For instance, consider Euclidean gradient descent to minimize $f$ in the Euclidean setting, which iterates $X_{k+1}= X_k - \eta \nabla f(X_k)$ where $\eta > 0$ is a step size. There are two ingredients to transform it into a Riemannian method. First, the Euclidean gradient $\nabla f(X)$ is replaced by the Riemannian gradient $\grad f(X)$, which is the projection of $\nabla f(X)$ onto the tangent space of $\stiefel$ at $X$. Second, the subtraction is replaced by a retraction $\mathcal{R}$, which allows moving while staying on the manifold. We obtain the iterations of the Riemannian gradient descent:
%
\begin{equation}
  \label{eq:riemannian_gradient_descent}
  X_{k+1} = \mathcal{R}(X_k, -\eta \grad f(X_k))\enspace.
\end{equation}
In the case of $\stiefel$~\citep{edelman1998geometry}, the tangent space at $X$ is the set
\begin{equation}
  \label{eq:tangent_space}
  \tangent = \{X\Omega + X_{\perp}K:\Omega \in\sk_p, K\in\mathbb{R}^{n - p \times p}\} = \{WX: W \in \sk_n\}
\end{equation}
and the Riemannian gradient with respect to the canonical metric~\citep{edelman1998geometry} is 
\begin{equation}
  \label{eq:riemannian_gradient}
  \grad f(X) = \sk(\nabla f(X)X^\top)X
\end{equation}
where $\sk(M) = \frac12(M - M^\top)$ is the skew-symmetric part of a matrix. In~\eqref{eq:riemannian_gradient}, for convenience, we have omitted a factor of $2$; compare with~\cite[\S 3]{gao2022optimization}. This omission is equivalent to magnifying the canonical metric by a factor of $2$.
From a computational point of view,
computing this gradient is cheap: it can be rewritten, for $X\in\stiefel$, as $\grad f(X) = \frac12(\nabla f(X) - X\nabla f(X)^\top X)$, which can be computed with one matrix-matrix product of size $p\times n$ and $n\times p$, and one matrix-matrix product of size $n \times p$ and $p\times p$.

%

A retraction $\mathcal{R}(X, Z) = Y$ is a mapping that takes as inputs $X\in\stiefel$ and $Z\in\tangent$, and outputs $Y\in \stiefel$, such that it ``goes in the direction of $Z$ at the first order'', i.e.,
we have $Y = X + Z + o(\|Z\|)$. It defines a way to move on the manifold $\stiefel$. We give several examples, where we write $Z =X\Omega + X_{\perp}K = WX$ in view of~\eqref{eq:tangent_space}.
\begin{enumerate}
  \item \emph{Exponential retraction}: \begin{equation}
    \label{eq:exp_retraction}
    \mathcal{R}(X, Z) =   \begin{pmatrix}
    X & Z
  \end{pmatrix}\exp\begin{pmatrix}
    \Omega & - Z^\top Z \\
    I_p & \Omega
  \end{pmatrix}
  \begin{pmatrix}
    I_p  \\
    0
  \end{pmatrix}\exp(-\Omega)\enspace .
\end{equation}
  This is the exponential map---that follows geodesics---for the canonical metric on the manifold. The most expensive steps to compute it are a matrix exponentiation of a matrix of size $2p\times 2p$ and a matrix-matrix product between matrices of size $n\times 2p$ and $2p\times 2p$.
  \item \emph{Cayley retraction}:
  \begin{equation*}
      \label{eq:cayley_retraction}
      \mathcal{R}(X, Z) = (I_n - \frac{W}{2})^{-1}(I_n + \frac{W}{2}) X  \enspace.
  \end{equation*}
  Though the inverse does exist for any $W\in\sk_n$, it requires solving a linear system that dominates the cost.
  \item \emph{Projection retraction}:\begin{equation}
    \label{eq:proj_retraction}
    \mathcal{R}(X, Z) = \mathrm{Proj}(X + Z), \enspace \text{with }\mathrm{Proj}(Y) = Y(Y^\top Y)^{-\frac12}\enspace.
\end{equation}
Computing this retraction requires computing the inverse-square root of a matrix, which is also a costly linear algebra operation.
\end{enumerate}

These operations allow us to implement Riemannian gradient descent. Many variants have then been proposed to accelerate convergence, among which trust-region
%
methods~\citep{absil2007trust} and Nesterov acceleration~\citep{ahn2020nesterov}.
%
%

In most machine learning applications, the function $f$ corresponds to empirical risk minimization, and so it has a sum structure. It can be written as:
\begin{equation}
  \label{eq:erm}
  f(X) = \frac1N\sum_{i=1}^Nf_i(X)\enspace,
\end{equation}
where $N$ is the number of samples and each $f_i$ is a ``simple'' function. In the Euclidean case, stochastic gradient descent
%
(SGD)~\citep{robbins1951stochastic} is the algorithm of choice to minimize such a function. At each iteration, it takes a random function $f_i$ and goes in the direction opposite to its gradient. Similarly, in the Riemannian case, we can implement Riemannian-SGD~\citep{bonnabel2013stochastic} by iterating
\begin{equation}
  \label{eq:sgd}
  X_{k+1} = \mathcal{R}(X_k, -\eta_k\grad f_i(X_k)),\enspace \text{where } i \sim \mathcal{U}[1, N] 
\end{equation}
with $i \sim \mathcal{U}[1, N] $ meaning that index $i$ is drawn from the discrete uniform distribution between $1$ and $N$ at each iteration. 
%
This method only requires one sample at each iteration; hence it scales gracefully with $N$. However, its convergence is quite slow and typically requires diminishing step sizes.

Variance reduction techniques~\citep{johnson2013accelerating, schmidt2017minimizing, defazio2014saga} are classical ways to mitigate this problem and allow to obtain algorithms that are stochastic (i.e.,
use only one sample at a time) and
that provably converge while having a constant step size, with a faster rate of convergence than SGD.



%
%
%
%
%
%
%
%
%
%
%

%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Geometry of the Landing Field and its Merit Function}
%
\label{sec:landing}
For $\lambda > 0$ and $X\in\mathbb{R}^{n\times p}$, we define the landing field as 
\begin{equation}
  \label{eq:landing_field}
  \Lambda(X) =\grad f(X) + \lambda X(X^\top X - I_p),
\end{equation}
where $\grad f(X) = \sk(\nabla f(X)X^\top)X$.
We define 
\begin{equation}
\label{eq:norm_function}
    \mathcal{N}(X) = \frac14\|X^\top X - I_p\|^2
\end{equation}
where $\|\cdot\|$ is the Frobenius norm so that the second term in~\eqref{eq:landing_field} 
%
is $\lambda \nabla \mathcal{N}(X)$. Here, $\grad f(X)$ denotes the extension to all $X\in\mathbb{R}^{n\times p}$ of formula~\eqref{eq:riemannian_gradient}. It thus coincides with the Riemannian gradient when $X\in\stiefel$. This extension has several attractive properties.
First, for all full-rank $X\in\mathbb{R}^{n\times p}$, $\grad f(X)$ can still be seen as a Riemannian gradient of $f$ on the manifold $\{Y\in\mathbb{R}^{n\times p} \mid Y^\top Y = X^\top X\}$ with respect to a canonical-type metric, as shown in~\cite[Proposition~4]{gao2022optimization}.
Second, this formula allows having orthogonality between the two terms of the field $\Lambda$, for any $X$. Indeed, we have $\langle\grad f(X), X(X^\top X - I_p)\rangle = \langle\sk(\nabla f(X)X^\top),X(X^\top X - I_p)X^\top\rangle$, which cancels since it is the scalar product between a skew-symmetric and a symmetric matrix.

The intuition behind the landing field is fairly simple: the component $\grad f(X)$ is here to optimize the function, while the term $X(X^\top X - I_p)$ attracts $X$
towards $\stiefel$.
More formally, since these two terms are orthogonal, the field cancels if and only if both terms cancel. The fact that $X(X^\top X - I_p)=0$ gives, assuming $X$ injective,  $X^\top X = I_p$, hence $X\in \stiefel$. Then, the fact that $\grad f(X) =0$ combined with $X\in\stiefel$ shows that $X$ is a first-order critical point of the function $f$ on the manifold.
This reasoning is qualitative: the next part formalizes this geometrical intuition.

\subsection{Geometrical Results and Intuitions}
%

In the remainder of the paper, we will always consider algorithms whose iterates stay close to the manifold $\stiefel$. 
We measure this closeness with the function $\mathcal{N}(X)$ (introduced in~\eqref{eq:norm_function}), and define the \emph{safe region} as 
\begin{equation}
    \label{eq:safe_region}
    \stiefel^{\varepsilon} = \{X\in\mathbb{R}^{n\times p}|\enspace \mathcal{N}(X)\leq \frac14 \varepsilon^2\} = \{X\in\mathbb{R}^{n\times p}|\enspace \|X^\top X - I_p\|\leq \varepsilon\} 
\end{equation}
%
for some $\varepsilon$ between $0$ and $1$. A critical part of our work is to ensure that the iterates of our algorithms remain in $\stiefel^\varepsilon$, which in turn guarantees the following bounds on the singular values of $X$.
\begin{lemma}
\label{lemma:singular_values}
    For all
    $X\in \stiefel^\varepsilon$, the singular values of $X$ are between $\sqrt{ 1 - \varepsilon}$ and $ \sqrt{1 + \varepsilon}$.
\end{lemma}
Note that when $\varepsilon=0$, the singular values of $X$ are all equal to $1$, thus making the columns of the matrix orthogonal and ensuring that $X\in\stiefel$.

As noted before, a critical feature of the landing field is the orthogonality of the two components, which holds between $\nabla \cN(X)$ and any direction $AX$ with a skew-symmetric matrix $A$. In order for the results to generalize to the stochastic and variance reduction setting, in the rest of this section we consider a more general form of the landing field of the form 
\begin{equation}
\label{eq:skew_field}
    F(X, A) = AX + \lambda \nabla \cN(X),
\end{equation}
where $A$ is an $n\times n $ skew-symmetric matrix. The formula of the landing field in \eqref{eq:landing_field} is recovered by taking $A = \sk(\nabla f(X)X^\top)$ in the above equation \eqref{eq:skew_field}, that is to say $\Lambda(X) = F(X, \sk(\nabla f(X)X^\top))$.
%
\begin{figure}[t]
	\scriptsize
	\centering
	\include{figures/tikz_diagram}
	\caption{Illustration of the geometry of the field $F$. Note the orthogonality of the two components.\label{fig:diagram_landing}}
\end{figure}


Fields of the form~\eqref{eq:skew_field} will play a key role in our analysis, exhibiting interesting geometrical properties which stem from the orthogonality of the two terms: $AX$ and $\nabla\cN(X) = X(X^\top X - I_p)$ are orthogonal. Figure~\ref{fig:diagram_landing} illustrates the geometry of the problem and of the field $F$. We have the following inequalities:
\begin{proposition}
\label{prop:bound_landing_norm}
  For all $X\in\stiefel^\varepsilon$ and $A \in \sk_n$, the norm of the field~\eqref{eq:skew_field} admits the following bounds:
  $$
 \|AX\|^2 + 4\lambda^2(1-\varepsilon)\mathcal{N}(X) \leq \|F(X,A)\|^2\leq\|AX\|^2 + 4\lambda^2(1+\varepsilon)\mathcal{N}(X)
  $$
\end{proposition}


The orthogonality of the two terms also ensures that going in the direction of this field $F(X, A)$ will allow us to remain in the safe region $\stiefel^\varepsilon$ as long as the step size is small enough.


\begin{lemma}[Safe step size] \label{lemma:safe_step}
 Let $X\in \stiefel^\varepsilon$, $A \in \sk_n$, and consider the update $\tilde{X} =X - \eta F(X, A)$, where $\eta > 0$ is a step size and $F(X,A)$ is the field~\eqref{eq:skew_field}. Define $g = \| F(X, A)\|$ and $d = \|X^\top X - I_p\|$. If the step size satisfies 
  \begin{equation} \label{eq:safe_step size}
      \eta \leq \eta(X) := \min \left\{ \frac{\lambda d (1-d) + \sqrt{\lambda^2 d^2 (1-d)^2 + g^2 (\varepsilon - d)}}{g^2}, \frac{1}{2\lambda} \right\},
  \end{equation}
  then the next iterate $\tilde{X}$ remains in $\stiefel^\varepsilon$.
  \end{lemma}
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

This lemma is of critical importance, both from a practical and theoretical point of view. In practice, at each iteration of our algorithms introduced later, we always compute the safe step~\eqref{eq:safe_step size}, and if the safe step is smaller than the prescribed step size, we use the safe step instead. 
%
Note that the formula for the safe step only involves quantities that are readily available when the field $F$ has been computed: computing $\eta(X)$ at each iteration does not add a significant computational overhead.
%
%

We can furthermore lower bound the safe step in~\eqref{eq:safe_step size} 
by a quantity that does not depend on $X$:

\begin{lemma}[Non-disappearing safe step size]\label{lemma:safe_step size_lower}
    Assuming that $\|A X\|_F\leq \tilde{a}$, we have that the upper-bound in Lemma~\ref{lemma:safe_step} is lower-bounded by
    \begin{equation}
        \eta(X) \geq \eta^*(\tilde{a}, \varepsilon, \lambda) := \min\left\{ \frac{\lambda(1-\varepsilon)\varepsilon}{\tilde{a}^2 + \lambda^2(1+\varepsilon)\varepsilon^2},\, \sqrt{\frac{\varepsilon}{2\tilde{a}^2}},\, \frac{1}{2\lambda}\right\}.
    \end{equation}
\end{lemma}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%
%

This lemma serves to prove that the iterates of our algorithms all stay in the safe region provided that the step size is small enough.
The following result provides a formal statement:
\begin{proposition}
\label{prop:recursive_step_size}
    Consider a sequence of iterates $X_k$ defined by recursion, starting from $X_0\in \stiefel^\varepsilon$. We assume that there is a family of maps $\mathcal{A}_k(X_0, \dots, X_k) = A_k\in\sk_p$ such that $X_{k+1} = X_k-\eta_kF(X_k, A_k)$ where $\eta_k > 0$ is a step size. In addition, we assume that there is a constant $\tilde{a}>0$ such that for all $X_0, \dots, X_k \in \stiefel^\varepsilon$, we have $\|\mathcal{A}_k(X_0, \dots, X_k)X_k\|\leq \tilde{a}$. Then, if all $\eta_k$ are such that $\eta_k\leq \eta^*(\tilde{a}, \varepsilon, \lambda)$ with $\eta^*$
    defined in Lemma~\ref{lemma:safe_step size_lower}, we have that all iterates satisfy 
    %
    $X_k\in\stiefel^\varepsilon$.
\end{proposition}
This proposition shows that an algorithm that follows a field of the form~\eqref{eq:skew_field} with sufficiently small steps will stay within the safe region $\stiefel^\varepsilon$.
The definition of the maps $\mathcal{A}_k$ is slightly cumbersome as it depends
on all the past iterates, but it is needed to handle the variance reduction algorithm that we study later. 
This result is central to this article since all of the subsequent algorithms considered in Section~\ref{sec:algorithms} produce sequences that satisfy the hypothesis of Proposition~\ref{prop:recursive_step_size}.

\subsection{A Merit Function}
\label{sec:merit}

The next proposition defines a smooth merit function for the landing 
%
field $\Lambda(X)$ defined in~\eqref{eq:landing_field}. The existence of such a merit function is central for a simple analysis of the landing algorithm and its different extensions. 
We consider

\begin{equation} \label{eq:merit_function}
    \mathcal{L}(X) = f(X) + h(X) + \mu \mathcal{N}(X),
\end{equation}
where 
$h(X) = -\frac12\langle \sym(X^\top \nabla f(X)), X^\top X - I_p\rangle$ and $\mu>0$, which is suitably chosen in the following result.

%

%
%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%

%
%
%
%
%

%
%
%
%
%

%
%
%
%


%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
  
%
%
%
%
%

%
%
%
%
%
  
%
%
%
%
%

%
%
%
%


%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%

\begin{proposition}[Merit function bound] \label{prop:lyapunov}
    Let $\cL(X)$ be the merit function defined in \eqref{eq:merit_function}. For all $X \in \stiefel^\varepsilon$ we have with $\nu = \lambda \mu$:%
    \begin{equation}
        \inner{\nabla\cL(X)}{\Lambda(X)} \geq \frac12 \| \grad f(X)\|^2 + \nu \cN(X),
    \end{equation}
    for the choice of 
    \begin{equation}
        \mu \geq \frac{2}{3-4\varepsilon}\left( \Lc(1-\varepsilon) + 3s + \hat{L}^2 \frac{1+\varepsilon}{\lambda} \right)\enspace,
    \end{equation}
    %
    where $s = \sup_{X\in\stiefel^\varepsilon} \| \sym(X^\top \nabla f(X)) \|$, $\Lc>0$ is the smoothness constant of $f$ over $\stiefel^\varepsilon$, $\hat{L} = \max(L, L')$ with $L' = \max_{X\in\stiefel^\varepsilon}\|\nabla f(X)\|$,
    and $\varepsilon<\frac34$.
\end{proposition}
%

This demonstrates that $\mathcal{L}$ is in fact a merit function (or a \emph{Lyapunov} function). 
%
Indeed, the landing direction is an ascent direction for the merit function, since $\langle \nabla \mathcal{L}(X), \Lambda(X)\rangle \geq 0$.
We can then combine this proposition and Proposition~\ref{prop:bound_landing_norm} 
%
get the following result:
%
\begin{proposition}
    \label{prop:bound_scalar_with_norm}
    Under the same conditions as in Proposition~\ref{prop:lyapunov}, defining $\rho = \min(\frac12, \frac{\nu}{4\lambda^2(1+\varepsilon)})$, we have for $X\in\stiefel^\varepsilon$:
    $$
    \langle \Lambda(X), \nabla \mathcal{L}(X)\rangle \geq \rho \|\Lambda(X)\|^2.
    $$
\end{proposition}
%
%
%
%
%
%
%
%
%
We now turn to the intuition behind the merit function.
The merit function $\mathcal{L}$ is composed of three terms. The terms
$f(X)$ and $\mu\mathcal{N}(X)$ are easy to interpret: the first one controls optimality, while the second controls
the distance to the manifold. The term $h(X)$ might be mysterious at first. Its role is best understood when $X$ is on the manifold. Indeed, for $X\in\stiefel$ we get
$$
\nabla h(X) = -X\mathrm{sym}(X^\top \nabla f(X))\enspace.
$$
This vector is, in fact, the opposite of the
projection of $\nabla f(X)$ on the normal space to $\stiefel$ at $X$.
Hence, if $X\in \stiefel$ then $\mathcal{L}(X) = f(X)$ 
%
and $\nabla \mathcal{L}(X)$ is a vector in the tangent space $\tangent$ which is aligned with $\grad f(X)$. Note that Fletcherâ€™s penalty function \citep{fletcher1970class} is similar to the merit function $\mathcal{L}(X)$ while the $h(X)$ term is determined by the solution of the least squares problem. Notice that this merit function $\mathcal{L}(X)$ is the same as that of \citet{gao2019parallelizable} where $\mathcal{L}$ is constructed from the augmented Lagrangian function and $h(X)$ serves as a multiplier term since the multiplier of orthogonality constraints has a closed-form solution, $\mathrm{sym}(X^\top \nabla f(X))$, at any first-order stationary point. The main difference between our two works is that  \citet{gao2019parallelizable} then solves the optimization problem by taking steps in a direction that approximates $-\nabla \mathcal{L}(X)$, but that does not satisfy
the orthogonality hypothesis and hence is not guaranteed to converge for any value of $\lambda>0$ (see also the discussion in Appendix B in~\citep{ablin2022fast}).
%
%
As a sum of smooth terms, the merit function is also smooth:
\begin{proposition}[Smoothness of the merit function]
\label{prop:smoothness_merit}
The merit function $\mathcal{L}$ is $L_g$-smooth on $\stiefel^\varepsilon$, with $L_g = L + \lambda L_{\mathcal{N}}$ where $L$ is the smoothness constant of $f(X) + h(X)$ and $L_{\mathcal{N}}$ is that of $\mathcal{N}$, upper bounded for instance by $2 + 3\varepsilon$.
\end{proposition}
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\citet{schechtman2023orthogonal} consider instead the non-smooth merit function $\mathcal{L}'(X) = f(X) + M\|X^{\top}X-I_p\|$. Our merit function decreases faster in the direction normal to the manifold, which is why the term $h(X)$ is introduced to tame the contribution of $f$ in that direction. The smoothness of our merit function renders the subsequent analysis of the algorithms particularly simple since the smoothness lemma applied on $\mathcal{L}$ directly gives a descent lemma on the merit function. This forms the basic tool to analyze the landing method and its variants.
%
%
%

%
%
%
%
%

%

%
%
%
%

%

%
%
%

%
%
%

%
%
%
%
%
%

%
%


%
%
%
%
%

%
%
%
%
%

%
%
%
%
%
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%


%
%

%
%
%

%


%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

%

\section{A Family of Landing Algorithms, and their Convergence Properties}
\label{sec:algorithms}
In this section, we consider a family of methods all derived from a base algorithm, the landing gradient descent algorithm. All of our algorithms follow directions of the form~\eqref{eq:skew_field}.
%

\subsection{Landing Gradient Descent: the Base Algorithm} \label{subsec:landing_gradient}

This algorithm produces a sequence of iterates $X_k \in \mathbb{R}^{n\times p}$ by iterating

\begin{equation}
  \label{eq:landing_algorithm}
      X_{k+1} = X_k  - \eta \Lambda(X_k)
\end{equation}
where $\eta>0$ is a step size. Note that this method falls into the hypothesis of Proposition~\ref{prop:recursive_step_size} with the simple maps $\mathcal{A}_k(X_0, \dots, X_k) = \grad f(X_k)$, so we can just take $\tilde{a} = \sup_{X\in\stiefel^\varepsilon}\|\grad f(X)\|$ to get a safe step size $\eta^*$ that guarantees that the iterates of the landing algorithm stay in $\stiefel^\varepsilon$.

We will start with the analysis of this method, where we find that it achieves a rate of convergence of $\frac1K$: we have $\frac1K\sum_{k=0}^K\|\grad f(X_k)\|^2 = O(\frac 1K)$ and $\frac1K\sum_{k=0}^K\mathcal{N}(X_k) = O(\frac 1K)$.
%
  %
We, therefore, obtain the same properties as classical Riemannian gradient descent, with a reduced cost per iteration, but with different constants.

%
%
%

%
%
\begin{proposition}
\label{prop:convergence_landing}
  Consider the iteration~\eqref{eq:landing_algorithm} starting from $X_0\in\stiefel^\varepsilon$. Define $\tilde{a} = \sup_{X\in \stiefel^\varepsilon}\|\sk(\nabla f(X)X^\top)X\|_F$, and let $\eta^*$ the safe step size chosen from Lemma~\ref{lemma:safe_step size_lower}.  Let $\mathcal{L}^*$ be a lower bound of the merit function $\mathcal{L}$ on $\stiefel^\varepsilon$. Then, for $\eta \leq \min(\frac1{2L_g}, \frac{\nu}{4\lambda^2L_g(1+\varepsilon)}, \eta^*)$, we have 
  \begin{equation*}
  \frac1K\sum_{k=1}^K\|\grad f (X_k)\|^2\leq \frac{4(\mathcal{L}(X_0) -\mathcal{L}^*)}{\eta K} \enspace \text{ and } \enspace \frac1K\sum_{k=1}^K\mathcal{N}(X_k)\leq \frac{2(\mathcal{L}(X_0) -\mathcal{L}^*)}{\eta\nu K}.
  \end{equation*}
\end{proposition}
%
%
%
%
%
%
%
%
%
%
%
  
%
%
%
  
%
%
%
%
%
%
%
%
%
%
%


This result demonstrates convergence to the stationary points of $f$ on the manifold at a rate $\frac1{K}$, 
%
just like classical Riemannian gradient descent~\citep{boumal2019global}.
Some readers might be more familiar with statement ``without squares'', which are in the current case that $\inf_{k\leq K}\|\grad f(X_k)\|=O(\frac{1}{\sqrt{K}})$ and $\inf_{k\leq K}\|X_k^{\top}X_k-I_p\|=O(\frac1{\sqrt{K}})$.
This result is an important improvement over that of \cite{ablin2022fast} since we do not require decreasing step sizes to get convergence and obtain a much better convergence rate. 


%


%



\subsection{Landing Stochastic Gradient Descent: Large Scale Orthogonal Optimization}
\label{subsec:sgd}
We now consider the case where the function $f$ is the average of $N$ functions:

$$
f(X) = \frac1N\sum_{i=1}^Nf_i(X)
$$

We can define the landing field associated with each $f_i$ by
$$
\Lambda_i(X) = \grad f_i(X) + \lambda X(X^{\top}X -I_p)
$$
This way, we have
$$
\Lambda(X)= \frac1N\sum_{i=1}^N\Lambda_i(X)
$$
and if we take an index $i$ uniformly at random between $1$ and $N$ we have 
$$
\mathbb{E}_i[\Lambda_i(X)] = \Lambda(X).
$$
In other words, the direction $\Lambda_i$ is an unbiased estimator of the landing field $\Lambda$.
We consider the landing stochastic gradient descent (Landing-SGD) algorithm, which at iteration $k$ samples a random index $i_k$ uniformly between $1$ and $N$ and iterates
\begin{equation}
    \label{eq:stochastic_landing}
    X_{k+1} = X_k - \eta_k \Lambda_{i_k}(X_k)
\end{equation}
where $\eta_k$ is a sequence of step size.
As is customary in the analysis of stochastic optimization algorithms, we posit a bound on the variance of $\Lambda_i$:

\begin{assumption}
There exists $B> 0$ such that for all $X\in\stiefel^\varepsilon$, we have 
$\frac1N\sum_{i=1}^N\|\Lambda_i(X) -\Lambda(X)\|^2\leq B$.
\end{assumption}
This allows us to derive a simple recursive bound on the iterates using the smoothness inequality:
\begin{proposition}
\label{prop:decrease_stochastic}
  Assume that $\eta_k\leq \min(\frac1{2L_g}, \frac{\nu}{4\lambda^2L_g(1+\varepsilon)}, \eta^*)$ where $\eta^*$ is the global safe step size from Lemma \ref{lemma:safe_step size_lower}. Then, 
  $$
  \mathbb{E}_{i_k}[\mathcal{L}(X_{k+1})] \leq \mathcal{L}(X_k) - \frac{\eta_k}4 \|\grad f(X_k)\|^2 - \frac{\eta_k\nu}2\mathcal{N}(X_k) + \frac{L_g B\eta_k^2}2,
  $$
  where the expectation is taken with respect to the random variable $i_k$.
\end{proposition}

We can now get convergence rates of the stochastic landing algorithm with decreasing step sizes. 

\begin{proposition}
\label{prop:convergence_stochastic_decreasing_step}
  Assume that the step size is $\eta_k = \eta_0 \times (1 + k)^{-\frac12}$ with $\eta_0 = \min(\frac1{2L_g}, \frac{\nu}{4\lambda^2L_g(1+\varepsilon)}, \eta^*)$, with $\eta^*$ chosen as the safe step size.
  Then, we have 
  $$
  \inf_{k\leq K}\mathbb{E}[\|\grad f(X_k)\|^2] =O\left(\frac{\log(K)}{\sqrt{K}}\right)\enspace \text{ and } \enspace \inf_{k\leq K}\mathbb{E}[\|\mathcal{N}(X_k)\|^2] =O\left(\frac{\log(K)}{\sqrt{K}}\right) \enspace.
  $$
\end{proposition}
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%


This shows that our method with decreasing step size has the same convergence rate as Riemannian stochastic gradient descent with decreasing step size.
With constant step size, we get the following proposition:

\begin{proposition}
\label{prop:convergence_stochastic_fixed_step}
  Assume that the step size is fixed to $\eta = \eta_0 \times (1 + K)^{-\frac12}$ with $\eta_0 = \min(\frac1{2L_g}, \frac{\nu}{4\lambda^2L_g(1+\varepsilon)}, \eta^*)$.
  Then, we have 
  $$
  \inf_{k\leq K}\mathbb{E}[\|\grad f(X_k)\|^2] =O\left(\frac{1}{\sqrt{K}}\right)\enspace \text{ and } \enspace \inf_{k\leq K}\mathbb{E}[\|\mathcal{N}(X_k)\|^2] =O\left(\frac{1}{\sqrt{K}}\right)\enspace.
  $$
\end{proposition}
\paragraph{Sample complexity}
The sample complexity of the algorithm is readily obtained from the bound: in order to find an $\varepsilon-$critical point of the problem and get both $\inf_{k\leq K}\|\grad f(X_k)\|^2 \leq \varepsilon$ and $\inf_{k\leq K}\mathcal{N}(X_k)\leq \varepsilon$, we need $O(\varepsilon^{-2})$ iterations.
The $O$ here only hides constants of the problem like conditioning of $f$ and hyperparameter $\lambda$, but this quantity is independent of the number of samples $N$.
This matches the classical sample complexity results obtained with SGD in the Euclidean and Riemannian non-convex settings~\citep{bonnabel2013stochastic}.


%

\subsection{Landing SAGA: Variance Reduction for Faster Convergence}
\label{subsec:SAGA}

In this section, we are in the same finite-sum setting as in Section~\ref{subsec:sgd}.
As is customary in classical optimization, SGD suffers from the high variance of its gradient estimator, which leads to sub-optimal rates of convergence.
A classical strategy to overcome this issue consists in using variance reduction algorithms, which build an estimator of the gradient whose variance goes to $0$ as training progress.
Such algorithms have also been proposed in a Riemannian setting, but like most other methods, they also require retractions~\cite{zhang2016riemannian}.


We propose a retraction-free variance-reduction algorithm that is a cross-over between the celebrated SAGA algorithm and the landing algorithm, which we call the Landing-SAGA algorithm.
The algorithm keeps a memory of the last gradient seen at each sample, $\Phi^1_k, \dots, \Phi_k^N$ where each $\Phi_k^i \in \mathbb{R}^{n\times p}$. At iteration $k$, we sample uniformly at random an index $i_k$ between $1$ and $N$, and compute the direction
$\Lambda^{i_k}_k = \grad f_{i_k}(X_k) - \sk(\Phi_k^{i_k}X_k^\top)X_k + \frac1m \sum_{j=1}^m\sk(\Phi_k^j X_k^\top)X_k+\lambda X_k(X_k^\top X_k - I_p)$. Then, we update the memory corresponding to sample $i_k$ by doing $\Phi_{k+1}^{i_k} = \nabla f_{i_k}(X_k)$, and $\Phi_{k+1}^j = \Phi_{k}^j$ for all $j\neq i_k$. Then we move in the direction
$$
X_{k+1} = X_k -\eta \Lambda_k^{i_k}.
$$

It is important to note that the variance reduction is only applied on the ``Riemannian'' part $\grad f_i(X)$. The other term in $X(X^\top X- I_p)$ is treated as usual. Like in the classical SAGA algorithm, we have the unbiasedness property:
$$
\mathbb{E}_i[\Lambda_k^i] = \Lambda(X_k)
$$
This means that on average, the direction we take is the landing field, computed over the whole dataset.
The gist of this method is that we can have fine control on $\mathbb{E}_i[\|\Lambda^k_i\|^2]$. Indeed, letting $D_k^i = \grad f_i(X_k) -\sk(\Phi_k^iX_k^\top)X_k$, we have

\begin{align}
  \mathbb{E}[\|\Lambda^k_i\|^2] &= \|\Lambda(X_k)\|^2 + \mathbb{E}[\|D_k^i - \mathbb{E}[D_k^i]\|^2]\\
  &\leq \|\Lambda(X_k)\|^2 + \mathbb{E}[\|D_k^i\|^2],
\end{align}

and $D_k^i$ can also be controlled since
\begin{align}
     \|D_k^i\|&= \|\sk((\nabla f_i(X_k) - \Phi_k^i)X_k^\top)X_k\| \\
     &\leq(1+\varepsilon)\|\nabla f_i(X_k) - \Phi_k^i\|\\
     &\leq(1+\varepsilon) L_f \|X_k - X_k^i\|^2
\end{align}
where $X_k^i$ is the last iterate that what chosen for the index $i$ (so $X_k^i$ is such that $\nabla f(X_k^i) = \Phi_k^i$). We, therefore, recover that we need to control the distance from the memory $X_k^i$ to the current point $X_k$, as is customary in the analysis of SAGA.

We have the following convergence theorem, which is obtained by combining the merit function and the proof technique of~\citet{reddi2016fast}:
\begin{proposition}
\label{prop:convergence_saga}
  Define $\rho$ as in Proposition~\ref{prop:bound_scalar_with_norm}, $L_g$ as in Proposition~\ref{prop:smoothness_merit} and $L_f$ the smoothness constant of $f$ on $\stiefel^\varepsilon$. Assume that the step size is such that  
  $$\eta\leq \min\left(\eta^*, \frac{\rho}{L_g}, \frac1{\sqrt{8N(1+\varepsilon)}L_f}, \left(\frac \rho{4N(4N+2)L_gL_f^2(1+\varepsilon)}\right)^{1/3}\right)\enspace.$$
  Then, we have 
  $$
  \inf_{k\leq K}\mathbb{E}[\|\grad f(X_k)\|^2] =O\left(\frac{1}{\eta K}\right)\enspace \text{ and } \enspace \inf_{k\leq K}\mathbb{E}[\|\mathcal{N}(X_k)\|^2] =O\left(\frac{1}{\eta K}\right)\enspace .
  $$
\end{proposition}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
As in classical optimization, using the variance reduction of SAGA recovers a $\frac1K$ rate with a stochastic algorithm.
%


\paragraph{Sample complexity}
When the number of samples $N$ is large, the last term in the above ``$\min$'' for the choice of step size is the smallest; hence the step size scales as $N^{-2/3}$. This shows that to get to a $\varepsilon-$critical point such that $\|\grad f(X)\|^2\leq\varepsilon$, we need $O(N^{\frac23}\varepsilon^{-1})$ iterations. 
This matches the sample complexity of classical Euclidean SAGA in the non-convex setting~\citep{reddi2016fast}.

%


\subsection{Comparison to Penalty Methods}
\label{sec:penalty}

It is a common practice in deep learning applications that the orthogonality if imposed by adding an $\ell_2$ regularization term and minimizing
\begin{equation*}
    f(X) + \lambda\mathcal{N}(X),
\end{equation*}
for example in \citep{balestriero2018spline, xie2017all, bansal2018can}.
This method leads to a small computational overhead compared to the simple unconstrained minimization of $f$, and it allows the use of standard optimization algorithms tailored for deep learning. However, it provides no guarantee that the orthogonality will be satisfied. Generally, there are two possible outcomes based on the choice of $\lambda>0$. If $\lambda$ is small, then the final point is far from orthogonality, defeating the purpose of the regularization. If $\lambda$ is too large, then optimization becomes too hard, as the problem becomes ill-conditioned: its smoothness constant grows with $\lambda$ while its strong convexity constant does not, since $\mathcal{N}(X)$ is not strongly convex (indeed, it is flat in the direction tangent to $\stiefel$).

%
%
%
%
%
%

%
%


In order to have a more formal statement than the above intuition, we consider the simple case of the PCA, that is when $f$ is a quadratic: $f(X) = -\frac12 \| AX \|^2$ with $A\in\mathbb{R}^{N\times n}$:
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{equation}
    \min_{X\in\bR^{n\times p}} -\frac12 \| AX \|^2 + \frac{\lambda}{4} \| X^\top X - I_p \|^2 \label{eq:pca_regul}
\end{equation}
\begin{proposition}
    \label{prop:pca_regul}
    The optimal solution $X_*\in\mathbb{R}^{n\times p}$ of the principal component analysis in \eqref{eq:pca_regul} with the $\ell_2$ squared penalty regularization parameter $\lambda>0$, has the distance $\| X_*^\top X_* - I_p\| = \| A^\top A \| / \lambda$.
\end{proposition}


%
%

%



\section{Experiments} \label{sec:experiments}

We numerically compare the landing method against the two main alternatives, the Riemannian gradient descent with QR retraction and the Euclidean gradient descent with added $\ell_2$ squared penalty norm, with stochastic gradients. All the tests are implemented in PyTorch and performed using a single GPU.


%
%
\subsection{Online PCA}
We test the methods performance on the online principal component analysis (PCA) problem 
\begin{equation}
    \min_{X\in\mathbb{R}^{n\times p}} -\frac{1}{2}\left\| AX \right\|_F^2, \quad \text{s.t.}\quad X\in\stiefel,
\end{equation}
where $A\in\mathbb{R}^{N\times n}$ is a synthetically generated data matrix with $N=15\,000$ being the number of samples each with dimension $n=5000$. The columns of $A$ are independently sampled from the normal distribution $\mathcal{N}(0, UU^\top + \sigma I_n)$, where $\sigma = 0.1$ and $U\in\mathbb{R}^{n\times p}$ is sampled from the Stiefel manifold with the uniform Haar distribution.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=.49\textwidth]{figures/pca_p200_stiefel_distances.pdf}
        \includegraphics[width=.47\textwidth]{figures/pca_p200_train_loss.pdf}
        \caption{$n=5000, p=200$ \label{fig:pca_p500}}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=.49\textwidth]{figures/pca_p1000_stiefel_distances.pdf}
        \includegraphics[width=.47\textwidth]{figures/pca_p1000_train_loss.pdf}
        \caption{$n=5000, p=1000$ \label{fig:pca_p1000}}
    \end{subfigure}
    \caption{Experiments with online PCA. \label{fig:pca}}
\end{figure}

We compare the landing stochastic gradient method with the classical Riemannian gradient descent and with the ``regularized'' method which minimizes $f(X) + \lambda \cN(X)$, where $\lambda$ is now a regularization hyperparameter, using standard SGD.
Figure \ref{fig:pca} 
%
shows the convergence of the objective and the distance to the constraint against the computation time of the three methods using stochastic gradients with a batch size of $128$ and a fixed stepsize, which decreases after $30$ epochs. The training loss is computed as $f(X_k) - f(X_*)$, where $X_*$ is the matrix of $p$ right singular vectors of $A$. We see that in both cases of $p = 200$ and $p=1000$ the landing is able to reach a lower objective value faster compared to the Riemannian gradient descent, however, at the cost of not being precisely on the constraint but with $\cN(X)\leq 10^{-6}$. The distance is further decreased after 30 epochs as the fixed stepsize of the iterations is decreased as well.

Euclidean gradient descent with $\ell_2$ regularization  performs poorly with both choices of regularizer $\lambda$ (``Reg.'' in the figure). In the first case when $\lambda=10^2$ is too small, the distance remains large, and as a result, the training loss becomes negative since we are comparing against $X_*$ which is on the constraint. In the second case of the large penalty term, when $\lambda = 10^4$, the iterates remain relatively close to the constraint, but the convergence rate is very slow. These experimental findings are in line with the theory explained in Section~\ref{sec:penalty}. 

In general, we see that the landing method outperforms Riemannian gradient descent in cases when the computational cost of the retraction is more expensive relative to computing the gradient. This occurs especially in cases when $p$ is large or the batch size of the stochastic gradient is small, as can be seen also in the additional experiments for $p=100$ and $p=500$ shown in Figure \ref{fig:pca_app} in Appendix~\ref{app:numerics}.


%
%

\subsection{Neural networks with orthogonality constraints}

We further test the methods for training neural networks whose weights are constrained to the Stiefel manifold. Orthogonality of weights plays a prominent role in deep learning, for example in the recurrent neural networks to prevent the problem of vanishing/exploding gradients~\citep{arjovsky2015unitary}, or in orthogonal convolutional neural networks  that impose kernel orthogonality to improve the stability of models~\citep{bansal2018can, wang2020orthogonal}.

We perform the test using two standard models VGG16~\citep{simonyan2014very} and Resnet18~\citep{he2016deep} while constraining the kernels of all convolution layers to be on the Stiefel manifold. We reshape the convolutional kernels to the size $n_\mathrm{out} \times n_\mathrm{in} n_\mathrm{x} n_\mathrm{y}$, where $n_\mathrm{in}, n_\mathrm{out}$ is the number of input and output channels respectively and $n_\mathrm{x}, n_\mathrm{y}$ is the filter size. In the case when the reshaping results in a wide instead of a tall matrix, we impose the orthogonality on its transposition. We train the models using Riemannian gradient descent, Euclidean gradient descent with $\ell_2$ regularization, and the landing method, with batch size of $128$ samples for $150$ epochs, and with a fixed step size that decreases as $\eta = \eta/10$ every 50 epochs. We repeat each training $5$ times for different random seed.

Figure~\ref{fig:ort_conv} shows the convergence of the test accuracy and the sum of distances to the constraints against the computation time, with the light shaded areas showing minimum and maximum values of the $5$ runs. %
The figure shows the landing is a strict improvement over the Euclidean gradient descent with the added $\ell_2$ regularization, which, for the choice of $\lambda = 1$, achieves a good test accuracy, but at the cost of the poor distance to the constraint of $10^{-3}$, and for the choice of $\lambda = 10^3$ converges to the solution that has similar distance as the landing of $10^{-8}$, but has poor test accuracy around $55\%$. In comparison, training the models with the Riemannian gradient descent with QR retractions, achieves the lowest distance to the constraint, but also takes longer to reach the test accuracy of roughly $90\%$.

We also compared with the trivialization approach \citep{lezcano2019trivializations} using the Geotorch library, however this approach is not readily suitable for optimization over large Stiefel manifolds. See the experiments in Figure~\ref{fig:ort_conv_app} in the Appendix \ref{app:numerics}, which takes over 7 hours, i.e. approximately 14 times as long as the other methods, to reach the test accuracy of around $90\%$ with VGG16.

%
%
%
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=.49\textwidth]{figures/resnet18_stiefel_distances.pdf}
        \includegraphics[width=.49\textwidth]{figures/resnet18_test_accuracy.pdf}
        \caption{Resnet18 experiments \label{fig:ort_conv_resnet}}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=.49\textwidth]{figures/vgg16_stiefel_distances.pdf}
        \includegraphics[width=.49\textwidth]{figures/vgg16_test_accuracy.pdf}
        \caption{VGG16 experiments \label{fig:ort_conv_vgg16}}
    \end{subfigure}
    \caption{Experiments with orthogonal convolutions. \label{fig:ort_conv}}
\end{figure}
\section*{Conclusion}
We have extended the landing method of~\cite{ablin2022fast} from the orthogonal group to the Stiefel manifold, yielding an iterative method for smooth optimization problems where the decision variables take the form of a rectangular matrix constrained to be orthonormal. The iterative method is infeasible in the sense that orthogonality is not enforced at the iterates. We have obtained a computable bound on the step size ensuring that the next iterate stays in a safe region. This safe step size, along with the smooth merit function~\eqref{eq:merit_function}, has allowed for a streamlined complexity analysis in Section~\ref{sec:algorithms}, both for the deterministic and stochastic cases. The various numerical experiments have illustrated the value of the proposed approach.



%
%



%
%

\acks{}
The authors thank Gabriel PeyrÃ© for fruitful discussions.
This work was supported by the Fonds de la Recherche Scientifique -- FNRS and the Fonds Wetenschappelijk Onderzoek -- Vlaanderen under EOS Project no 30468160, and by the Fonds de la Recherche Scientifique -- FNRS under Grant no T.0001.23. Simon Vary is a beneficiary of the FSR Incoming Post-doctoral Fellowship.
\newpage

%
%



%
%

%
%

%
%

%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%

%


\vskip 0.2in
\bibliography{sample}

\newpage
\appendix

\section{Further numerical experiments}\label{app:numerics}
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=.49\textwidth]{figures/pca_p100_stiefel_distances.pdf}
        \includegraphics[width=.48\textwidth]{figures/pca_p100_train_loss.pdf}
        \caption{$n=5000, p=100$ \label{fig:pca_p100}}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=.49\textwidth]{figures/pca_p500_stiefel_distances.pdf}
        \includegraphics[width=.48\textwidth]{figures/pca_p500_train_loss.pdf}
        \caption{$n=5000, p=500$ \label{fig:pca_p500}}
    \end{subfigure}
    \caption{Additional experiments with online PCA. \label{fig:pca_app}}
\end{figure}

\begin{figure}[h]
    \centering
     \includegraphics[width=.45\textwidth]{figures/vgg16_trivialization_stiefel_distances.pdf}
    \includegraphics[width=.435\textwidth]{figures/vgg16_trivialization_test_accuracy.pdf}
    \caption{Experiments with orthogonal convolutions and trivialization. \label{fig:ort_conv_app}}
\end{figure}


\section{Proofs}
\label{app:proofs}
\subsection{Proof of Lemma~\ref{lemma:singular_values}}
\begin{proof}
Letting $\sigma_1,\dots, \sigma_p$ the singular values of $X$, we have $\|X^{\top}X-I_p\|^2 = \sum_{i=1}^p(\sigma_i^2 - 1)^2$. Hence, if $\|X^{\top}X-I_p\|^2\leq \varepsilon^2$, we must have that all singular values satisfy $|\sigma^2 - 1| \leq \varepsilon$, which proves the result.    
\end{proof}


\subsection{Proof of Proposition~\ref{prop:bound_landing_norm}}
\begin{proof}
  Since the field \eqref{eq:skew_field} is the sum of two orthogonal terms, we have
  $\|F(X, A)\|^2 = \|AX\|^2 + \lambda^2\|X(X^\top X - I_p)\|^2$.
  Using the bound on the singular values of $X$ from Lemma~\ref{lemma:singular_values}, we have $4(1-\varepsilon) \mathcal{N}(X)\leq \|X(X^\top X - I_p)\|^2\leq 4(1+\varepsilon) \mathcal{N}(X)$, 
  which concludes the proof.
\end{proof}

\subsection{Proof of Proposition~\ref{lemma:safe_step}}
\begin{proof}
    Let the residual at $X$ be $\Delta = X^\top X - I_p$ and the landing field be $F = F(X, A)$. The residual $\tilde \Delta = {\tilde X}^\top {\tilde X} - I_p$ at the updated $\tilde X$ can be expressed as
    \begin{align}
        \tilde \Delta &= {\tilde X}^\top {\tilde X} - I_p = (X - \eta F)^\top (X - \eta F) - I_p \\
        &= X^\top X - \eta (X^\top F + F^\top X ) + \eta^2 F^\top F - I_p\\
        &= \Delta - \eta \left(X^\top (A X + \lambda X \Delta) + (-X^\top A + \lambda \Delta X^\top) X \right) + \eta^2 F^\top F \\
        &= \Delta - 2\eta\lambda (\Delta + \Delta^2) + \eta^2 F^{\top} F,
    \end{align}
    which, by the triangle inequality and norm submultiplicativity, we can bound in the Frobenius norm as
    %
    \begin{align}
        \| \tilde \Delta \| &\leq (1-2\eta\lambda) \| \Delta \| + 2\eta\lambda\| \Delta \|^2 + \eta^2 \| F \|^2,
    \end{align}
    when $\eta < 1/(2\lambda)$. Rewriting the above using scalar notation of $d = \| \Delta \|$ and $g = \| F \|$ and fixed $\lambda >0$, we wish to find an interval for $\eta$ such that
    \begin{equation}
        \eta^2 g^2 + 2\eta\lambda d (d-1) + d \leq \varepsilon, \label{eq:safe_step_quad}
    \end{equation}
    which in turn ensures that $\| \tilde \Delta \| \leq \varepsilon$. For $\varepsilon \geq d$, i.e., when $X$ is in the safe region, we can discard the negative root of the quadratic inequality in \eqref{eq:safe_step_quad} resulting in the upper bound in \eqref{eq:safe_step size}.
%
    Note that if $d \geq 1$, the upper bound on the step size is not positive when on the boundary $d = \varepsilon$. To prevent this case, we need to have $\varepsilon < 1$.
\end{proof}


\subsection{Proof of Lemma~\ref{lemma:safe_step size_lower}}
\begin{proof}
    Let $X$ be with distance $d>0$, i.e.
    \begin{equation}
        \| X^\top X - I_p\|^2_F  = \sum_{i}^{p} (\sigma_i^2 - 1)^2 = d^2,
    \end{equation}
    where $\sigma_i$ are the singular values of $X$, which, as a result, must be bounded as: $\sigma_i^2 \in [1-d, 1+d]$.
    Consider the bound on the normalizing component
    \begin{equation}
        \| \nabla \cN(X) \|_F^2 = \| X (X^\top X - I_p)\|^2_F = \sum_i \sigma_i^2 (\sigma_i^2-1)^2,
    \end{equation}
    implying that
    \begin{equation}
        (1-d)d^2 \leq \| \nabla \cN(X) \|_F^2 \leq (1+d)d^2.
    \end{equation}
    By the orthogonality of the two components in the landing field, we have that
    \begin{equation}
        \| \Lambda(X) \|_F^2 = \|AX\|^2_F + \lambda^2 \| \nabla \cN(X)\|_F^2. \label{eq:safe_step size_flow_bound}
    \end{equation}

    We proceed to lower-bound the first term in the minimum stated in \eqref{eq:safe_step size} to make it independent of $X$. In the following, we denote $a = \|AX\|_F$ and $g = \| \Lambda(X) \|_F$
    \begin{align}
        &\frac{1}{g^2}\left( \lambda d (1-d) + \sqrt{\lambda^2 d^2 (1-d)^2 + g^2 (\varepsilon - d)}\right) \\
        &\geq \frac{1}{a^2 + \lambda^2 (1+d)d^2}\left(  \lambda d (1-d) + \sqrt{\lambda^2 d^2 (1-d)^2 + (a^2 + \lambda^2(1-d)^2 d^2) (\varepsilon - d)} \right) \label{eq:safe_step size_lower1} \\
        &\geq \frac{1}{a^2 + \lambda^2 (1+d)d^2}\left( \lambda d (1-d)\left(1 + \sqrt{\frac{1+\varepsilon-d}{2}}\right) + a\sqrt{\frac{\varepsilon - d}{2}}\right) \label{eq:safe_step size_lower2}\\
        &\geq \frac{\lambda (1-\varepsilon) d  + a\sqrt{(\varepsilon - d)/2}}{a^2 + \lambda^2 (1+\varepsilon)d^2}, \label{eq:safe_step size_lower3}
    \end{align}
    where in \eqref{eq:safe_step size_lower1} we used the bound from \eqref{eq:safe_step size_flow_bound} to $g$ and that $(1-d)^2 \leq 1-d$ for $d<1$, in \eqref{eq:safe_step size_lower2} we used the fact that for any $x,y \geq 0$ we have that $\sqrt{x^2 + y^2} \geq (x+y)/\sqrt{2}$, and in \eqref{eq:safe_step size_lower3} we used that $d < \varepsilon <1$.

    The right-hand side of \eqref{eq:safe_step size_lower3} attains its minimum on the interval $d \in [0, \varepsilon]$ on the boundary and $\| AX\|_F \leq \tilde{a}$. As a result, the upper bound on the safe step size for all $X$ with the $\varepsilon$ distance cannot decrease below
    \begin{equation}
        \eta(X) \geq \min\left\{ \frac{\lambda(1-\varepsilon)\varepsilon}{\tilde{a}^2 + \lambda^2(1+\varepsilon)\varepsilon^2},\, \sqrt{\frac{\varepsilon}{2\tilde{a}^2}},\, \frac{1}{2\lambda}\right\}
    \end{equation}
\end{proof}

\subsection{Proof of Proposition~\ref{prop:recursive_step_size}}
\begin{proof}
    The proof is a simple recursion. The property that $X_0\in\stiefel^\varepsilon$ holds by assumption. Then, assuming that $X_0, \dots, X_k\in\stiefel^\varepsilon$, we have by assumption that $A_k$ is such that $\|A_kX_k\|\leq \tilde{a}$. It follows that $X_{k+1}\in\stiefel^\varepsilon$ following Lemma~\ref{lemma:safe_step size_lower}, which concludes the recursion.
\end{proof}

\begin{lemma}[Jacobian of $\Phi(X)$] \label{lemma:jacobian}

    Let $\Phi(X) = \sym(\nabla f(X)^{\top} X) :\bR^{n\times p} \rightarrow \bR^{n \times p}$ , let $\Jacob{\Phi}{X}$ denote its derivative at $X$, and let $\Jacob{\Phi}{X}^*[\dot{X}]$ denote its adjoint in the sense of the Frobenius inner product at $X$ applied to $\dot{X}$. Let $\vect{\cdot}:\bR^{m \times n} \rightarrow \bR^{mn}$ denote the vectorization operation, and let $\vectHess{X}$ denote the matrix representation of the Hessian of $f$ at $X$; namely, $\vectHess{X} \in \mathbb{R}^{np\times np}$ such that, for all $\dot{X}\in\mathbb{R}^{n\times p}$, $\vectHess{X}[\vect{\dot{X}}] = \vect{\mathrm{D}(\nabla f)(X)[\dot{X}]}$. Then 
    \begin{equation}
        \vect{\Jacob{\Phi}{X}^* [X^{\top}X - I_p]} = \vectHess{X} \vect{\nabla \cN (X)} + \vect{\nabla f(X)(X^{\top}X - I)}.
    \end{equation}
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
    %
\end{lemma}
%
%
%
%
%
\begin{proof}
%

%
%
%
%
%
%
%
%
%
%
%
%
%
    The function $\Phi(X) = \sym(\nabla f(X)^{\top} X) = \frac1 2 \left(\nabla f(X)^{\top} X + X^{\top}  \nabla f(X)^{\top}\right)$  is a composition of three functions:
    \begin{equation}
        \left(X \rightarrow \sym(X)\right) \circ \left( A, B \rightarrow A^{\top} B \right) \circ \left(X \rightarrow \left(\nabla f(X), X\right )\right),
    \end{equation} 
    the symmetric projection, matrix-matrix product, and the mapping of $(\nabla f(X), X)$.

    First consider the vectorized form of $\sym(X)$
    \begin{align}
        \vect{\sym(X)} &= \frac12 \left(\vect{X}+\vect{X^{\top}}\right) \\ 
             &= \frac12 \left(I_{p^2} + K^{(p,p)}\right) \vect{X}, \label{eq:sym_jacobi}
    \end{align}
    where $K^{(p,p)}\in\bR^{p^2 \times p^2}$ denotes the commutation matrix with the property that $\vect{A^{\top}} = K^{(m,n)} \vect{A}$ for arbitrary $A\in\bR^{m\times n}$. Moreover, $K^{(p,p)}$ is a symmetric orthogonal matrix and forms an involution $\left(K^{(p,p)}\right)^2 = I_{p^2}$.The equation in \eqref{eq:sym_jacobi} implies that $\Jacob{X \rightarrow \sym(X)}{X} = \frac12 \left(I_{p^2} + K^{(p,p)}\right)$.
    
    In order to derive Jacobian of the matrix-matrix product $\Jacob{A,B \rightarrow A^{\top} B}{(A,B)}$ take the two Kronecker product identities:
    \begin{equation}
        \vect{A^{\top} B} = (B^{\top} \otimes I_p) \vect{A^{\top}} = (I_p \otimes A^{\top}) \vect{B},
    \end{equation}
    which implies the Jacobian of $A^{\top}B$ is
    \begin{equation}
        \Jacob{A, B \rightarrow A^{\top} B}{(A,B)} = \begin{bmatrix}
            (B^{\top} \otimes I_p) K^{(p,p)} \\ 
            I_p \otimes  A^{\top}
        \end{bmatrix}
    \end{equation}
    
    Finally, putting it together, we have by the chain rule applied to the Jacobian of $\Phi(X)$
    \begin{align}
      & \vect{\Jacob{X \rightarrow \Phi(X)}{X}} \\
        &= \Jacob{X \rightarrow \sym(X)}{\nabla f(X)^{\top} X} \Jacob{A, B \rightarrow A^{\top} B}{\nabla f(X), X}
            \begin{bmatrix}
                \Jacob{\vect{\nabla f(X)}}{X} \\ \Jacob{\vect{X}}{X}
            \end{bmatrix}\\
            &= \frac12 \left(I_{p^2} + K^{(p,p)}\right) \begin{bmatrix}
                (X^{\top} \otimes I_p )K^{(p,p)} , I_p \otimes \nabla f(X)^{\top}
            \end{bmatrix}
            \begin{bmatrix}
                \vectHess{X} \\ I_{np}
            \end{bmatrix}\\
            &=  \frac12 \left(I_{p^2} + K^{(p,p)}\right) \Big( (X^{\top} \otimes I_p) K^{(p,p)} \vectHess{X} + (I_p \otimes \nabla f(X)^{\top}) \Big),
    \end{align}
    where we overload the notation of $\vectHess{X}\in\bR^{np \times np}$ which is the Hessian of the vectorized function $f(\vect{X}):\bR^{np} \rightarrow \bR$.

    Which yields that the Jacobian of $\Phi(X)$ in direction of $X^{\top}X - I$ is
    \begin{align}
       \vect{\Jacob{\Phi}{X}^* [X^{\top}X - I_p]} &= \left( \vectHess{X}K^{(p,p)}(X \otimes I_p)  + (I_p \otimes \nabla f(X)) \right) \frac12 \left(I_{p^2} + K^{(p,p)}\right) \vect{X^{\top} X - I} \\
             & = \vectHess{X} K^{(p,p)}(X \otimes I_p)\vect{X^{\top} X - I}  + (I_p \otimes \nabla f(X)) \vect{X^{\top} X - I} \\
            &= \vectHess{X} \vect{X(X^{\top} X - I)} + \vect{\nabla f(X)(X^{\top}X - I)}
         %
         %
         %
         %
         %
    \end{align}
    where in the first line we use the fact that $X^{\top} X - I_p$ is symmetric and thus unaffected by the $\frac12 \left(I_{p^2} + K^{(p,p)}\right)$ operation and that $K^{(p,p)}(X \otimes I_p) = ( I_p\otimes X)K^{(p,p)}$. 
\end{proof}

\subsection{Proof of Proposition~\ref{prop:lyapunov}}
\begin{proof}
    Computing the gradient of $\cL(X)$ has four terms
    \begin{equation}\label{eq:merit_grad}
        \nabla \cL(X) = \nabla f(X) -\frac12 \Jacob{\Phi}{X}^* [X^{\top}X - I_p] - X\sym(X^\top \nabla f(X)) + \mu \nabla \cN(X),
    \end{equation}
    %
    where $\Jacob{\Phi}{X}^* [X^{\top}X - I_p]$ denotes the adjoint of the Jacobian in the sense of the Frobenius inner product of $\Phi(X) = \sym(X^\top \nabla f(X))$ in $X$ evaluated in the direction $X^\top X - I_p$. We proceed by expressing the inner product of the landing field $\Lambda(X)$ with each of the four terms of $\nabla \cL(X)$ in \eqref{eq:merit_grad} separately.
    
    The inner product between the first term of \eqref{eq:merit_grad} and the landing field $\Lambda(X)$ is
    \begin{align}
        \inner{\Lambda(X)}{\nabla f(X)} &= \inner{\sk(\nabla f(X) X^\top)X + \lambda X (X^\top X - I_p)}{\nabla f(X)}\\
        &= \inner{\sk(\nabla f(X) X^\top)}{\nabla f(X)X^\top} + \lambda\inner{X^\top X - I_p}{X^\top \nabla f(X)}\\
        & = \| \sk(\nabla f(X) X^\top) \|_F^2 + \lambda \inner{\sym(X^\top \nabla f(X))}{X^{\top}X -I_p}, \label{eq:merit_inner1}
    \end{align}
    where we used the fact that the inner product of a skew-symmetric and a symmetric matrix is zero.

    We can express the inner product between the Jacobian using Lemma \ref{lemma:jacobian} in the second term of \eqref{eq:merit_grad} as 
    \begin{align}
        \begin{split}
        \inner{\Lambda(X)}{-\frac12 \Jacob{\Phi}{X}^* [X^{\top}X - I_p]} = &-\frac12\lambda           \inner{\vectHess{X} \vect{ \nabla \cN(X)}}{ \vect{\nabla \cN(X)}} \\
            \qquad & -\frac12\inner{\vectHess{X} \vect{ \nabla \cN(X)}}{\vect{\grad f(X)}}\\
            \qquad & -\frac12\inner{\nfX^{\top} \grad f(X)}{X^{\top}X - I_p} \\
            \qquad & -\frac12\lambda \inner{\sym(X^\top \nabla f(X))}{(X^{\top}X - I_p)^2}, 
        \end{split}\label{eq:merit_inner2}
    \end{align}
    where $\vect{\cdot}:\mathbb{R}^{n\times p} \rightarrow \mathbb{R}^{np}$ vectorizes a matrix and $\vectHess{X}$ denote the matrix representation of the Hessian of $f$ at $X$; namely, $\vectHess{X} \in \mathbb{R}^{np\times np}$ such that, for all $\dot{X}\in\mathbb{R}^{n\times p}$, $\vectHess{X}[\vect{\dot{X}}] = \vect{\mathrm{D}(\nabla f)(X)[\dot{X}]}$.
    
    The third term is
    \begin{align}
        \inner{\Lambda(X)}{-X\sym(X^\top \nabla f(X))} &= \inner{\sk(\nabla f(X) X^\top)X + \lambda \nabla \cN(X)}{-X\sym(X^\top \nabla f(X))}\\
        &= -\lambda\inner{\sym(X^\top \nabla f(X))}{X^\top X (X^\top X - I_p)}, \label{eq:merit_inner3}
    \end{align} 
    where the inner product with the skew-symmetric matrix in the first term is zero.
    
    The inner product of the fourth term and the landing field is
    \begin{align}
        \inner{\Lambda(X)}{\mu \nabla \cN(X)} &= \lambda \mu \| \nabla \cN(X) \|^2_F \label{eq:merit_inner4}
    \end{align}
    by orthogonality of the two components of the landing field.

    Adding all the four terms expressed in \eqref{eq:merit_inner1}, \eqref{eq:merit_inner2},  \eqref{eq:merit_inner3}, and  \eqref{eq:merit_inner4} together gives
    \begin{align}
         \inner{\nabla \cL(X)}{\Lambda(X)} &= \| \sk(\nabla f(X) X^\top) \|_F^2 \label{eq:merit_innerB0}\\
            & + \lambda \inner{\left(\mu I_{np} - \frac12 \vectHess{X}\right) \vect{\nabla\cN(X)}}{\vect{\nabla\cN(X)}} \label{eq:merit_innerB1}\\
            & - \lambda \frac32 \inner{(X^{\top}X-I_p)^2}{\sym(X^\top \nabla f(X))} \label{eq:merit_innerB2}\\
            & -\frac12\inner{\vectHess{X} \vect{ \nabla \cN(X)}}{\vect{\grad f(X)}} \label{eq:merit_innerB3}\\
            & -\frac12\inner{\nfX^{\top} \grad f(X)}{X^{\top}X - I_p}, \label{eq:merit_innerB4}
    \end{align}
    where the first line in \eqref{eq:merit_innerB0} comes from the first term of \eqref{eq:merit_inner1}, the second line \eqref{eq:merit_innerB1} is a combination of the first term in \eqref{eq:merit_inner2} and \eqref{eq:merit_inner4}, the third line in \eqref{eq:merit_innerB2} comes from the second term in \eqref{eq:merit_inner1}, the third term in \eqref{eq:merit_inner2},  and \eqref{eq:merit_inner2}.

    
    We will bound lower bound each term separately. The term in \eqref{eq:merit_innerB1} is lower bounded as
    \begin{align}
        \lambda \inner{\left(\mu I_{np} - \frac12 \vectHess{X}\right) \vect{\nabla\cN(X)}}{\vect{\nabla\cN(X)}} &\geq \lambda \left(\mu-\frac{\Lc}{2}\right)\| \nabla\cN(X) \|^2_F \\
            &\geq 4 \lambda \left(\mu-\frac{\Lc}{2}\right) \sigma_p^2 \, \cN(X), \label{eq:merit_boundB1} 
           %
    \end{align}
    where $\sigma_p$ is the smallest singular value of $X$ and $\Lc$ is the Lipschitz constant of $\nabla f(X)$.

    The term in \eqref{eq:merit_innerB2} is lower bounded using the Cauchy-Schwarz inequality as
    \begin{align}
        - \lambda \frac32 \inner{(X^{\top}X-I_p)^2}{\sym(X^\top \nabla f(X))} \geq - 6 \lambda \, \cN(X) \| \sym(X^\top \nabla f(X)) \|_F , \label{eq:merit_boundB2}
    \end{align}
    by the fact that $\|X^\top X - I_p\|^2_F = 4\cN(X)$.
    
    The term in \eqref{eq:merit_innerB3} is lower bounded as
    \begin{align}
         -\frac12\inner{\vectHess{X} \vect{ \nabla \cN(X)}}{\vect{\grad f(X)}} &\geq -\frac{\Lc}{2} \|  X (X^\top X - I_p) \|_F \|\grad f(X)\|_F \\ 
         & \geq -\Lc \sigma_1 \sqrt{\cN(X)} \|\grad f(X)\|_F, \label{eq:merit_boundB3}
    \end{align} 
    by the fact that the operator norm of $\|X\|_2 = \sigma_1 $ and $\| X^\top X - I_p\|_F = 2\sqrt{\cN(X)}$.
    
    The term in \eqref{eq:merit_innerB4} is lower bounded as
    \begin{align}
        -\frac12\inner{\nfX^{\top} \grad f(X)}{X^{\top}X - I_p} & \geq -\frac12\|\nabla f(X)\|_F \| \grad f(X) \|_F \| X^\top X - I_p \|_F\\
        &\geq - \gradc \sqrt{\cN(X)} \|\grad f(X)\|_F,\label{eq:merit_boundB4}
    \end{align}
    where $\gradc$ is such that $\|\nabla f(X)\| \leq \gradc$ for all $X\in\stiefel^\varepsilon$. 
    
    %
    %
    %
    %
    %
    %
    %
    %
    %
    
    Now we bound the terms in \eqref{eq:merit_boundB3} and \eqref{eq:merit_boundB4} together
    \begin{align}
         -\frac12\inner{\vectHess{X} \vect{ \nabla \cN(X)}}{\vect{\grad f(X)}}  &-\frac12\inner{\nfX^{\top} \grad f(X)}{X^{\top}X - I_p} \nonumber\\
         & \geq - (\gradc + \Lc\sigma_1)  \sqrt{\cN(X)} \| \grad f(X) \|  \\ 
         & \geq - \frac12 (\gradc + \Lc\sigma_1) \left( \beta \cN(X) + \beta^{-1} \| \grad f(X)\|^2 \right)    \label{eq:merit_boundB3and4},
    \end{align}
    where in the first inequality we used the previously derived bounds in \eqref{eq:merit_boundB3} and \eqref{eq:merit_boundB4}, and the second inequality is a consequence of the AG inequality $\sqrt{xy}\leq (x+y)/2$ with $x = \beta \cN(X)$ and $y = \beta^{-1} \| \grad f(X)\|^2$ for an arbitrary $\beta >0$, which will be specified later.
    
    Adding the bounds in \eqref{eq:merit_boundB1}, \eqref{eq:merit_boundB2}, and \eqref{eq:merit_boundB3and4} together, we have a lower bound as
    \begin{align}
        \inner{\nabla \cL(X)}{\Lambda(X)} &\geq \|\grad f(X)\|_F^2\left( \sigma_1^{-2} - \frac12(\gradc + \Lc\sigma_1) \beta^{-1} \right) \\
            &+\cN(X) \left(  4\lambda \left(\mu-\frac{\Lc}{2}\right) \sigma_p^2 - 6 \lambda \| \sym(X^\top \nabla f(X)) \|_F  - \frac12(\gradc + \Lc\sigma_1) \beta \right)
    \end{align}
    where we used that $\| \sk(\nabla f(X) X^{\top})\|_F \geq \sigma_1^{-1} \| \grad f(X)\|_F$. Choosing $\beta = \sigma_1^2 \frac{\gradc+\Lc\sigma_1}{2-\sigma_1^2}$ and bounding $\| \sym(X^\top \nabla f(X)) \|_F \leq s$ we have
    \begin{align}
         \inner{\nabla \cL(X)}{\Lambda(X)} &\geq \frac12 \|\grad f(X)\|_F^2 + \cN(X) \left( 4 \lambda \left(\mu-\frac{\Lc}{2}\right) \sigma_p^2 - 6 \lambda  s  - \frac12\sigma_1^2 \frac{(\gradc+\Lc\sigma_1)^2}{2-\sigma_1^2} \right)\\
         &\geq \frac12 \|\grad f(X)\|_F^2 + \cN(X) \left( 4 \lambda \left(\mu-\frac{\Lc}{2}\right) (1-\varepsilon) - 6 \lambda  s  - \frac12 \hat{L}^2(1+\varepsilon) \frac{(2\sqrt{1+\varepsilon})^2}{2-(1-\varepsilon)} \right) \\
         &\geq \frac12 \|\grad f(X)\|_F^2 + \cN(X) \left( 4 \lambda \left(\mu-\frac{\Lc}{2}\right) (1-\varepsilon) - 6 \lambda  s  - 2 \hat{L}^2 (1+\varepsilon) \right), \label{eq:merit_bound_final1}
    \end{align}
    where in the second line we define $\hat{L} = \max\{\Lc, \gradc\}$ and we used that $\sqrt{1+\varepsilon} \geq 1$. We also need $\sigma_1 < \sqrt{2}$ which comes from $\sigma_1 < \sqrt{1+\varepsilon}$ and $\varepsilon<1$. The coefficient in front of the distance term $\cN(X)$ in \eqref{eq:merit_bound_final1} is lower bounded by $\lambda \mu$ for
    \begin{equation}  \label{eq:mu-lb}
        \mu \geq \frac{2}{3-4\varepsilon}\left( \Lc(1-\varepsilon) + 3s + \hat{L}^2 \frac{1+\varepsilon}{\lambda} \right).
    \end{equation}
\end{proof}
%



%
%
%
%



%
%
%
%
%

%
%
%
%
%

%
%
%
%
%

%
%
%
%


%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%

\subsection{Proof of Proposition~\ref{prop:bound_scalar_with_norm}}
\begin{proof}
    We have
    \begin{align}
         \frac12\|\grad f(X)\|^2 + \nu \mathcal{N}(X)&\geq \rho(\|\grad f(X)\|^2 + 4\lambda^2 (1+\varepsilon) \mathcal{N}(X))\\
         &\geq \rho \|\Lambda(X)\|^2\enspace.
    \end{align}
Where the last inequality comes from Proposition\ref{prop:bound_landing_norm}.
\end{proof}



\subsection{Proof of Proposition~\ref{prop:smoothness_merit}}
\begin{proof}
    We start by considering the smoothness constant of $\mathcal{N}$. We have $\nabla \mathcal{N}(X) = X(X^{\top}X-I_p)$, hence we find that its Hessian is such that in a direction $E\in\mathbb{R}^{n \times p}$:
    $$
    \nabla^2\mathcal{N}(X)[E] = E(X^{\top}X - I_p) +X(E^{\top}X + X^{\top}E)\enspace.
    $$
    We can then bound crudely using the triangular inequality
    \begin{align}
        \|\nabla^2\mathcal{N}(X)[E]\| &\leq \|E(X^{\top}X - I_p)\| +\|XE^{\top}X\| + \|XX^{\top}E\|\\
        &\leq \varepsilon\|E\| + 2(1+\varepsilon)\|E\|\\
        &\leq (2 + 3\varepsilon)\|E\|\enspace.
    \end{align}
    This implies that all the absolute values of the eigenvalues of $\nabla^2\mathcal{N}(X)$ are bounded by $2+3\varepsilon$, so that $\mathcal{N}$ is $(2+3\varepsilon)$-smooth.
    The result follows since the sum of smooth functions is smooth.
\end{proof}


\subsection{Proof of Proposition~\ref{prop:convergence_landing}}
\begin{proof}
Since we use the safe step size, the iterates remain in $\stiefel^\varepsilon$.
  Using the $L_g$-smoothness of $\mathcal{L}$ (Proposition~\ref{prop:smoothness_merit}), we get
  $$
  \mathcal{L}(X_{k+1})\leq \mathcal{L}(X_k) - \eta \langle \Lambda(X_k), \nabla \mathcal{L}(X_k)\rangle + \frac{L_g\eta^2}2\|\Lambda(X_k)\|^2 \enspace.
  $$
  Using Proposition~\ref{prop:lyapunov} and Proposition~\ref{prop:bound_landing_norm}, we get
  $$
  \mathcal{L}(X_{k+1}) \leq \mathcal{L}(X_k) - (\frac\eta2 - \frac{\eta^2L_g}2)\|\grad f(X_k)\|^2 -(\eta\nu -2\lambda^2\eta^2L_g(1+\varepsilon)\mathcal{N}(X_k)\enspace.
  $$
  %
  Using the hypothesis on $\eta$, we have both $\frac\eta2 - \frac{\eta^2L_g}2 \geq \frac\eta4$ and $\eta\nu -2\lambda^2\eta^2L_g(1+\varepsilon) \geq \frac{\eta\nu}2$. We, therefore, obtain the inequality
  
  $$
  \frac{\eta}4\|\grad f(X_k)\|^2 + \frac{\eta\nu}2 \mathcal{N}(X_k) \leq \mathcal{L}(X_k) - \mathcal{L}(X_{k+1})
  $$
  
  Summing these terms gives
  \begin{equation}
       \frac{\eta}4\sum_{k=1}^K\|\grad f(X_k)\|^2 + \frac{\eta\nu}2 \sum_{k=1}^K\mathcal{N}(X_k) \leq \mathcal{L}(X_0) - \mathcal{L}(X_{K+1}) \leq \mathcal{L}(X_0) - \mathcal{L}^*,
  \end{equation}
which implies that we have both
\begin{equation}
    \frac{\eta}4\sum_{k=1}^K\|\grad f(X_k)\|^2 \leq \mathcal{L}(X_0) - \mathcal{L}^*\enspace \text{and} \enspace
    \frac{\eta\nu}2 \sum_{k=1}^K\mathcal{N}(X_k)  \leq \mathcal{L}(X_0) - \mathcal{L}^*\enspace.
\end{equation}
These two inequalities then directly provide the result.
\end{proof}


\subsection{Proof of Proposition~\ref{prop:decrease_stochastic}}
\begin{proof}We use once again the smoothness of $\mathcal{L}$ and unbiasedness of $\Lambda_i(X_k)$ to get

\begin{align}
    \mathbb{E}_i[\mathcal{L}(X_{k+1}]&\leq\mathcal{L}(X_k) - \eta_k\langle \Lambda(X_k), \nabla\mathcal{L}(X_k)\rangle+\frac{\eta_k^2L_g}2\mathbb{E}[\|\Lambda_i(X_k)\|^2]\\
    &\leq \mathcal{L}(X_k) - \eta_k\langle \Lambda(X_k), \nabla\mathcal{L}(X_k)\rangle+\frac{\eta_k^2L_g}2(B^2 + \|\Lambda(X_k)\|^2) \\
    &\leq \mathcal{L}(X_k) - \eta_k(\frac12\|\grad f(X_k)\|^2 + \nu \mathcal{N}(X_k))\\
    &+ \frac{\eta_k^2L_g}2(\|\grad f(X_k)\|^2 + 4\lambda^2(1+\varepsilon)\mathcal{N}(X_k))+ \frac{\eta_k^2L_gB^2}2 \\
    &\leq  \mathcal{L}(X_k) - \frac{\eta_k - \eta_k^2L_g}2\|\grad f(X_k)\|^2 - (\eta_k\nu - 2\eta_k^2L_g\lambda^2(1+\varepsilon))\mathcal{N}(X_k )+\frac{\eta_k^2L_gB^2}2
\end{align}
Taking $\eta_k\leq \min(\frac1{2L_g}, \frac{\nu}{4L_g\lambda^2(1+\varepsilon)})$ simplifies the inequality to 
$$
\mathbb{E}_i[\mathcal{L}(X_{k+1}]\leq \mathcal{L}(X_k) - \frac{\eta_k}4 \|\grad f(X_k)\|^2 - \frac{\eta_k\nu}2\mathcal{N}(X_k)+\frac{\eta_k^2L_gB^2}2
$$
\end{proof}
\subsection{Proof of Proposition~\ref{prop:convergence_stochastic_decreasing_step}}
\begin{proof}
Summing up the inequality in Proposition~\ref{prop:decrease_stochastic} gives, using $\sum_{k=0}^K (1 + k)^{-1}\leq \log(K)$:
$$
\frac14\sum_{k=0}^K\eta_k\|\grad f(X_k)\|^2 \leq \mathcal{L}(X_0) - \mathcal{L}^* + \frac{\eta_0L_g B}2 \log(K)
$$
and 
$$
\frac\nu2\sum_{k=0}^K\eta_k\mathcal{N}(X_k) \leq \mathcal{L}(X_0) - \mathcal{L}^* + \frac{\eta_0L_g B}2 \log(K)
$$

Next, we use the bound $\inf_{k\leq K}\|\grad f(X_k)\|^2\leq \sum_{k=0}^K\eta_k\|\psi(X_k)\|^2 \times (\sum_{k=0}^K\eta_k)^{-1}$ and the fact that $\sum_{k=0}^K\eta_k\geq \eta_0\sqrt{K}$
%
to get
$$
\inf_{k\leq K}\|\grad f(X_k)\|^2 \leq 4\frac{ \mathcal{L}(X_0) - \mathcal{L}^* + \frac{\eta_0L_g B}2 \log(K)}{\eta_0\sqrt{K}}
$$
and 
$$
\inf_{k\leq K}\mathcal{N}(X_k) \leq 2\frac{ \mathcal{L}(X_0) - \mathcal{L}^* + \frac{\eta_0L_g B}2 \log(K)}{\nu\eta_0\sqrt{K}}
$$
\end{proof}

\subsection{Proof of Proposition~\ref{prop:convergence_stochastic_fixed_step}}
\begin{proof}
    Just like for the previous proposition, we get by summing the descent lemmas:

    $$\frac{1}{4K}\sum_{k=1}^K\|\grad f(X_k)\|^2 + \frac{\nu}{2K} \sum_{k=1}^K\mathcal{N}(X_k)\leq \frac{\mathcal{L}(X_0) - \mathcal{L}^*}{\eta K} + \frac{\eta L_gB^2}2$$

Taking $\eta = \eta_0K^{-1/2}$ yields as advertised:
$$
\inf_{k\leq K}\|\grad f(X_k)\|^2 \leq \frac{4}{\sqrt{K}}\left( \frac{\mathcal{L}(X_0) - \mathcal{L}^*}{\eta_0} + \frac{\eta_0 L_gB^2}2\right)
$$
and 
$$
\inf_{k\leq K}\mathcal{N}(X_k) \leq \frac{2}{\nu \sqrt{K}}\left( \frac{\mathcal{L}(X_0) - \mathcal{L}^*}{\eta_0} + \frac{\eta_0 L_gB^2}2\right)\enspace.
$$
\end{proof}
\subsection{Proof of Proposition~\ref{prop:convergence_saga}}
\begin{proof}
    We define $X_k^i$ as the matrix such that $\Phi_k^i = \nabla f(X_k^i)$, i.e., the last iterate for which the memory corresponding to sample $i$ has been updated. As in the classical SAGA analysis, we will form a Lyapunov function combining the merit function $\mathcal{L}$ and the distance to the memory, defined as 
    $$S_k = \frac1N\sum_{j=1}^N\mathbb{E}[\|X_k - X_k^j\|^2]$$
We also define the variance of the landing direction as 
$$
V_k = \frac1N\sum_{j=1}^N\|\Lambda_k^j\|^2.
$$
For short, we let $\Lambda_k = \Lambda(X_k)$.
    \paragraph{Control of the distance to the memory}
    Looking at an individual term $j$ in the sum of $S_k$, we have
    \begin{align}
        \mathbb{E}_i[\|X_{k+1} - X_{k+1}^j\|^2] &= \mathbb{E}_i[\|X_{k} - \eta \Lambda_k^i -  X_{k+1}^j\|^2]\\
        &= \frac1N (\sum_{i\neq j}^N\|X_{k} - \eta \Lambda_k^i -  X_{k}^j\|^2 + \eta^2 \|\Lambda_k^j\|^2)\\
        &=\mathbb{E}_i[\|X_{k} - \eta \Lambda_k^i -  X_{k}^j\|^2] -\frac1N(\|X_{k} - \eta \Lambda_k^j -  X_{k}^j\|^2 -\eta^2 \|\Lambda_k^j\|^2
    \end{align}
    On the one hand, we have for the first term:
    \begin{align}
        \mathbb{E}_i[\|X_{k} - \eta \Lambda_k^i -  X_k^j\|^2] &= \|X_{k}  -  X_k^j\|^2 -2\eta \langle \Lambda^k, X_{k}  -  X_k^j\rangle +\eta^2V_k\\
        &\leq (1+\eta \beta)\|X_{k}  -  X_k^j\|^2 + \eta \beta^{-1}\|\Lambda_k\|^2 + \eta^2V_k
    \end{align}
    where we introduce $\beta >0$ from Young's inequality to control the scalar product.
    For the second term, we obtain
    \begin{align}
        \|X_{k} - \eta \Lambda_k^j -  X_{k}^j\|^2 - \eta^2\|\Lambda_k^j\|^2 &= \|X_k - X_k^j\|^2 - 2\eta \langle  \Lambda_k^j, X_k - X_k^j\rangle\\
        &\geq (1 - \gamma \eta)\|X_k - X_k^j\|^2 - \gamma^{-1} \eta\|\Lambda_k^j\|^2
    \end{align}
    where once again we introduce $\gamma$ from Young's inequality.

    Taking all of these inequalities together gives
    $$\mathbb{E}_i[\|X_{k+1} - X_{k+1}^j\|^2] \leq (1 - \frac1N +\eta\beta + N^{-1}\gamma \eta) \|X_k - X_k^j\|^2 + \eta \beta^{-1}\|\Lambda_k\|^2 + \eta^2V_k + N^{-1}\gamma^{-1}\eta \|\Lambda_k^j\|^2
    $$
    And summing these for $j=1\dots N$ gives
    \begin{equation}
        \mathbb{E}_i[S_{k+1}]\leq (1 - \frac1N +\eta\beta + N^{-1}\gamma \eta)S_k + \eta\beta^{-1}\|\Lambda_k\|^2 +(\eta^2 +N^{-1}\gamma^{-1}\eta)V_k
    \end{equation}
    We choose $\beta = (4N\eta)^{-1}$ and $\gamma=(4\eta)^{-1}$, which finally gives, assuming $N>4$:
    \begin{equation}
    \label{eq:distance_memory_saga}
        \mathbb{E}_i[S_{k+1}]\leq (1 - \frac1{2N})S_k + 4N\eta^2\|\Lambda_k\|^2 +2\eta^2 V_k
    \end{equation}

    \paragraph{Control of the merit function}
    The smoothness of the merit function gives once again
    \begin{align}
        \mathbb{E}_i[\mathcal{L}(X_{k+1})]&\leq \mathcal{L}(X_{k}) - \eta \langle \Lambda_k, \nabla \mathcal{L}(X_k)\rangle + \eta^2\frac{L_g}2V_k\\
        &\leq \mathcal{L}(X_{k}) - \eta \rho \| \Lambda_k\|^2+ \eta^2\frac{L_g}2V_k
    \end{align}
    where the last inequality comes from Proposition~\ref{prop:bound_scalar_with_norm}.
    \paragraph{Variance control}
    We then control $V_k$. By the bias-variance decomposition, using the unbiasedness of $\Lambda_k^j$, we get
    \begin{align}
    V_k &= \|\Lambda_k\|^2 +\frac1N\sum_{j=1}^N\|\Lambda_k^j - \Lambda_k\|^2\\
    &\leq \|\Lambda_k\|^2 +\frac1N\sum_{j=1}^N\|\grad f_i(X_k) - \sk(\Phi_iX_k^\top)X_k\|^2\\
    &\leq\|\Lambda_k\|^2 +\frac1N\sum_{j=1}^N\|\sk((\nabla f(X_k) - \nabla f(X_k^i))X_k^\top)X_k\|^2\\
    &\leq \|\Lambda_k\|^2 + L^2_f(1+\varepsilon)S_k
    \end{align}
    where $L_f$ is the smoothness constant of $f$.
    \paragraph{Putting it together}
    We get the two inequalities
    $$
    \mathbb{E}_i[S_{k+1}]\leq (1 - \frac1{2N} + 2\eta^2L_f^2(1+\varepsilon)^2)S_k + (4N + 2)\eta^2\|\Lambda_k\|^2
    $$
    $$
\mathbb{E}_i[\mathcal{L}(X_{k+1})]\leq \mathcal{L}(X_{k}) - (\eta \rho -\eta^2\frac{L_g}2) \| \Lambda_k\|^2+ \eta^2\frac{L_g}2L_f^2(1+\varepsilon)^2S_k
    $$

    The hypothesis on the step size simplifies these inequalities to 

    $$
    \mathbb{E}_i[S_{k+1}]\leq (1 - \frac1{4N})S_k + (4N + 2)\eta^2\|\Lambda_k\|^2
    $$
    $$
\mathbb{E}_i[\mathcal{L}(X_{k+1})]\leq \mathcal{L}(X_{k}) - \frac12\eta \rho \| \Lambda_k\|^2+ \eta^2\frac{L_g}2L_f^2(1+\varepsilon)^2S_k
    $$
    We now look for a Lyapunov quantity of the form  $\mathcal{G}_k = \mathcal{L}(X_{k}) + cS_k$, and get
    $$
    \mathbb{E}[G_{k+1}] \leq G_k -(\frac12\eta\rho -c(4N + 2)\eta^2)\|\Lambda_k\|^2 -(\frac c{4N} - \eta^2\frac{L_g}2L_f^2(1+\varepsilon)^2)S_k
    $$

    We take $c = 2N \eta^2L_gL_f^2(1+\varepsilon)^2$ in order to cancel the last term. The last condition on the step size gives finally
    $$
    \mathbb{E}[G_{k+1}] \leq G_k -\frac14\eta\rho\|\Lambda_k\|^2
    $$
    Taking expectations with respect to the whole past and summing yields
    $$
    \frac1K\sum_{k=1}^K\mathbb{E}[\|\Lambda_k\|^2]\leq \frac4{\eta\rho K}(\mathcal{L}(X_0) - \mathcal{L}^*)
    $$
    which concludes the proof.
\end{proof}

%
%
%
%
%
%
%

%
%
%
%

%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%

%

%
%
%

%
%
%
%
%
%
\subsection{Proof of Proposition~\ref{prop:pca_regul}}
\begin{proof}
    Let $A\in\mathbb{R}^{N \times n}$ have its singular value decomposition $A = U \Sigma V^{\top}$. Rewrite \eqref{eq:pca_regul} where we parametrize $X = Q D W^{\top}$ as its compact singular value decomposition with $Q\in\mathbb{R}^{n\times p}$, $D\in\mathbb{R}^{p\times p}$, and $W\in\mathbb{R}^{p\times p}$
    \begin{equation}
        \min_{X\in\mathbb{R}^{n\times p}} -\frac12 \| \Sigma V^{\top} Q D \|^2 + \frac{\lambda}{4} \| D^2 - I_p\|^2, \label{eq:pca_regul_optim2}
    \end{equation}
    where we used the fact that the multiplication on the left and right by the orthogonal matrix $U\in\mathbb{R}^{N\times N}$ and $W\in\mathbb{R}^{p\times p}$ respectively does not change the Frobenius norm. 
    
    The first term of \eqref{eq:pca_regul_optim2} is minimized when $V$ is equal to the $p$ singular vectors in $U$ corresponding to the $p$ largest singular values, resulting in the following
    \begin{equation}
        \min_{d_i\geq 0}\quad \sum_{i=1}^{p} -\frac{(\sigma_i d_i)^2}{2} + \frac{\lambda}{4} (d_i^2 - 1)^2,
    \end{equation}
    where $d_i$ and $\sigma_i$ denote the diagonal entries of $\Sigma$ and $D$. The minimum is attained at $d_i^2 = 1+\sigma_i^2/\lambda$, which corresponds to the solution $X_*$ having the distance of $\| X_*^\top X_* - I_p\| = \| A^\top A \| / \lambda$.
    %
    %
\end{proof}
\end{document}
