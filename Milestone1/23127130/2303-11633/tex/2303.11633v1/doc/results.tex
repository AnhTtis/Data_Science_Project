\section{Experiments}

\begin{table*}[!t]
	\centering
	%\vspace{-0.1cm}
				\tabcolsep=0.4cm
				\footnotesize            
				{
					\begin{tabular}{ 
							l
							l
							c  
							c 
							%   c 
							c 
							c 
							c 
							c            }
						\toprule
						& 
						&
						&			
						&\multicolumn{2}{c}{ADE20K} 
						&\multicolumn{2}{c}{Stuff 164K}  
						\\              
						Head
						& Backbone
						& fps
						& \#params. 
						& s.s.
						& m.s.            
						& s.s.
						& m.s.            \\            
						
						\specialrule{0em}{0pt}{1pt}
						\hline
						\specialrule{0em}{1pt}{0pt}
						FCN
						& MobileNet-V2
						& 51.10 
						& 9.82M
						& 19.71
						& 19.56   
						& 15.28
						& 17.01               
						\\     
						
						\rowcolor{gray!25}\ \ + Ours
						& MobileNet-V2
						& 49.46 
						& 10.61M
						% & -
						& 37.40
						& 39.09  
						& 25.37
						& 27.17                
						\\                 
						
						DeepLab-V3+
						& MobileNet-V2
						& 38.42 
						& 15.35M
						% & -
						& 34.02
						& 34.82
						& 31.18
						& 32.01                
						\\     
						
						\rowcolor{gray!25}\ \ + Ours
						& MobileNet-V2
						& 36.25
						& 16.13M
						% & -
						& 39.34
						& 41.28     
						& 34.71
						& 35.81                
						\\                 
						
						\specialrule{0em}{0pt}{1pt}
						\hline
						\specialrule{0em}{1pt}{0pt}
						OCRNet
						& HRNet-W18
						& 14.62 
						& 12.18M
						& 39.32
						& 40.80
						& 31.58
						& 32.34                
						\\                
						
						\rowcolor{gray!25}\ \ + Ours
						& HRNet-W18
						& 14.37 
						& 12.97M
						% & -
						& 44.47
						& 47.16        
						& 39.12
						& 40.65                 
						\\            
						
						UperNet
						& ResNet-50
						& 24.06 
						& 66.52M
						% & -
						& 42.05
						& 42.78   
						& 39.86
						& 40.26                 
						\\    
						
						\rowcolor{gray!25}\ \ + Ours
						& ResNet-50
						& 23.37 
						& 67.30M
						% & -
						& 45.24
						& 46.30
						& 41.26
						& 42.30                 
						\\            
						
						DeepLab-V3+
						& ResNet-50
						& 24.09 
						& 43.69M
						% & -
						& 43.95
						& 44.93 
						& 40.85
						& 41.49                 
						\\   
						
						\rowcolor{gray!25}\ \ + Ours
						& ResNet-50
						& 23.54
						& 44.48M
						% & -
						& 46.29
						& 47.56
						& 42.99
						& 43.97           
						\\                
						
						\specialrule{0em}{0pt}{1pt}
						\hline
						\specialrule{0em}{1pt}{0pt}     
						
						OCRNet
						& HRNet-W48
						& 13.78 
						& 70.53M
						% & -
						& 43.25
						& 44.88
						& 40.40
						& 41.66                 
						\\              
						
						\rowcolor{gray!25}\ \ + Ours
						& HRNet-W48
						& 13.33 
						& 71.32M
						% & -
						& 45.68
						& 48.13
						& 42.64
						& 43.53                 
						\\           
						
						UperNet
						& ResNet-101
						& 19.65 
						& 85.51M
						% & -
						& 43.82	
						& 44.85    
						& 41.15
						& 41.51                 
						\\      
						
						\rowcolor{gray!25}\ \ + Ours
						& ResNet-101
						& 19.49
						& 86.30M
						% & -
						& 46.06
						& 47.74
						& 43.13 
						& 43.84                 
						\\                 
						
						DeepLab-V3+
						& ResNet-101
						& 16.39 
						& 62.68M
						% & -
						& 45.47	
						& 46.35
						& 42.39
						& 42.96                 
						\\                    
						
						\rowcolor{gray!25}\ \ + Ours
						& ResNet-101
						& 16.03
						& 63.47M
						% & -
						& 47.25
						& 48.41
						& 44.21
						& 45.10                 
						\\           
						
						\specialrule{0em}{0pt}{1pt}
						\hline
						\specialrule{0em}{1pt}{0pt}                
						
						UperNet
						& Swin-Tiny
						& 20.38
						& 59.94M
						% & -
						& 44.51 
						& 45.81  
						& 43.83
						& 44.58                 
						\\             
						
						\rowcolor{gray!25}\ \ + Ours
						& Swin-Tiny
						& 19.96
						& 60.73M
						% & -
						& 46.91
						& 49.03
						& 44.57
						& 45.83                 
						\\             
						
						UperNet
						& Swin-Base$\dagger$
						& 14.63 
						& 121.42M
						% & -
						& 50.04 
						& 51.66
						& 47.67
						& 48.57                 
						\\              
						
						\rowcolor{gray!25}\ \ + Ours
						& Swin-Base$\dagger$
						& 14.38
						& 122.20M
						% & -
						& 52.00
						& 53.52
						& 48.26
						& 49.55                 
						\\         
						
						UperNet
						& Swin-Large$\dagger$
						& 10.54
						& 233.96M
						& 52.00
						& 53.50
						& 47.89
						& 48.93                
						\\      
						
						\rowcolor{gray!25}\ \ + Ours
						& Swin-Large$\dagger$
						& 10.44
						& 234.75M
						& 52.87
						& 54.43
						& 48.82 
						& 50.00        
						\\                  
						
						\bottomrule                                   
					\end{tabular}
				}
	\caption{Performance Comparison on ADE20K~\cite{ade20k} and COCO-Stuff 164K~\cite{cocostuff164k}. Single-scale (s.s.) and multi-scale (m.s.) evaluation results are reported, and values of fps (frames per second) are obtained with resolution $512 \times 512$ on a single NVIDIA RTX 2080Ti GPU. Models marked with $\dagger$ are pre-trained on ImageNet-22K following the practice mentioned in~\cite{swin}. }	
	\label{tab:results_ade20k}  		
%\vspace{-0.2cm}
\end{table*}



\subsection{Implementation}
We adopt two challenging semantic segmentation benchmarks (ADE20K~\cite{ade20k} 
and COCO-Stuff 164K~\cite{cocostuff164k}) in this paper. Models are trained and evaluated on the training and validation sets of these datasets respectively. Results of Cityscapes~\cite{cityscapes} and Pascal-Context~\cite{pascalcontext} are shown in the supplementary due to the page limit. The convolution-based and transformer-based models are investigated by following their default training and testing configurations. Both single- and multi-scale results are reported. Different from the single-scale results that are evaluated on the original size, the multi-scale evaluation conducts inference with the horizontal flipping and scales of [0.5, 0.75, 1.0, 1.25, 1.5, 1.75].  

The projectors $\theta_y$ and $\theta_p$ are both composed of two linear layers ([$2d \times d/2$] $\rightarrow$ [$d/2 \times d$], $d=512$) with an intermediate ReLU layer. The loss weight $\lambda_{KL}$ and the scaling factor $\tau$ for cosine similarity are empirically set to 1 and 15, and they work well in our experiments. Implementations regarding baseline models and benchmarks are based on the default configurations of MMSegmentation~\cite{mmseg2020}, and they are kept intact when implemented with our method. 




\subsection{Results}
\label{sec:results}
\mypara{Quantitative results. }
To verify the effectiveness and generalization ability of our proposed method, various decoder heads (FCN~\cite{fcn}, DeepLab-V3+~\cite{deeplabv3+}, UperNet~\cite{upernet}, OCRNet~\cite{ocr}), with different types of backbones, including ResNet~\cite{resnet} and MobileNet~\cite{mobilenetv2}, and Swin Transformer (Swin)~\cite{swin}, are adopted as the baselines. 

The results on ADE20K and COCO-Stuff 164K are shown in Table~\ref{tab:results_ade20k} from which we can observe that the proposed context-aware classifier only introduces about 2\% additional inference time and a few additional parameters to all these baseline models, but decent performance gain has been achieved on both two challenging benchmarks, including the model implemented with powerful transformer Swin-Large, reaching impressive performance without compromising the efficiency. 
It is worth noting that, the improvement is not originated from the newly introduced parameters, because our method even helps smaller models beat the larger ones with much more parameters, such as DeepLabV3+ (Res-50)  \textit{v.s.} OCRNet (HR-48) and UperNet (Swin-Base$\dagger$) \textit{v.s.} UperNet (Swin-Large$\dagger$). %Besides, we tentatively apply our method on Mask2Former~\cite{cheng2021mask2former} by enhancing the initial query embeddings instead of the classifier. Our method is still effective as shown in the supplementary file. 





\mypara{Qualitative results. }
Predicted masks are shown in Figure~\ref{fig:vis_example} where the ones yielded with our proposed method are more visually attractive. 
Besides, for facilitating the understanding, t-SNE results are demonstrated in Figure~\ref{fig:post_tsne}. It can be observed that, with the proposed learning scheme, the estimated context-aware classifiers are more semantically representative to different individuals by effectively rectifying the original classifier with necessary contextual information. 




\subsection{Ablation Study}
\label{sec:ablation_study}
In this section, experimental results are presented to investigate the effectiveness of each component of our proposed method. The ablation study is conducted on ADE20K, and the baseline model is UperNet with Swin-Tiny. 



\begin{figure}[!t]
	\centering
	\begin{minipage}    {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/img_gt/ADE_val_00000178.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/img_gt/ADE_val_00000222.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/img_gt/ADE_val_00001254.jpg}
	\end{minipage}        
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/img_gt/ADE_val_00001731.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth]  
		{figure/img_gt/ADE_val_00001971.jpg}
	\end{minipage}    
	
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/img_gt/ADE_val_00000178.png}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/img_gt/ADE_val_00000222.png}
	\end{minipage}      
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/img_gt/ADE_val_00001254.png}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/img_gt/ADE_val_00001731.png}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/img_gt/ADE_val_00001971.png}
	\end{minipage}        
	
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/baseline/ADE_val_00000178.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/baseline/ADE_val_00000222.jpg}
	\end{minipage}       
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/baseline/ADE_val_00001254.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/baseline/ADE_val_00001731.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/baseline/ADE_val_00001971.jpg}
	\end{minipage}      
	
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/ours/ADE_val_00000178.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/ours/ADE_val_00000222.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/ours/ADE_val_00001254.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth] 
		{figure/ours/ADE_val_00001731.jpg}
	\end{minipage}    
	\begin{minipage}  {0.19\linewidth}
		\centering
		\includegraphics [width=1\linewidth,height=0.7\linewidth]  
		{figure/ours/ADE_val_00001971.jpg}
	\end{minipage}    
	
	%\vspace{-0.2cm}
	\caption{Visual illustrations from top to bottom are from input images, ground-truth, baseline and baseline+ours. Black regions are ignored during testing. }
	\label{fig:vis_example}
\end{figure}





\mypara{Effects of different loss combinations. }
$\mathcal{L}^{ce}$ supervises the original classifier's prediction $\vec{p}$ that is used for generating the estimated context-aware prototypes $\mathcal{C}_{p}$. Since $\mathcal{A}_{p}$ is an approximation of the oracle one $\mathcal{A}_{y}$ yielded with the ground-truth label $\vec{y}$, the supervisions on $\vec{p}$ and   $\mathcal{A}_{y}$ are both essential. To examine the effects of individual losses, experimental results are in Table~\ref{tab:ablation_loss}.
It can be observed from (b) that, without $\mathcal{L}^{ce}$, $\mathcal{L}^{ce}_{y}$ and $\mathcal{L}_{KL}$, merely supervising $\vec{p}_{p}$ even worsens the baseline's performance (a), and the comparison between (b) and (c) tells the importance of $\mathcal{L}^{ce}$ that supervises $\vec{p}$. The other experiments show the necessities of $\mathcal{L}_{KL}$ and $\mathcal{L}^{ce}_{y}$. Specifically, since $\mathcal{L}_{KL}$ encourages $\mathcal{A}_{p}$ to mimic $\mathcal{A}_{y}$, though the comparison between (c) \& (d) implies additionally optimizing the prediction of the oracle case is beneficial, (d) is still inferior to (f) that incorporates $\mathcal{L}_{KL}$. On the other hand, without $\mathcal{L}^{ce}_{y}$ that ensures the validity of the prediction in the oracle case, the result of (e) is clearly lower than that of (f). Moreover, the sensitivity analysis on the loss weight $\lambda_{KL}$ for $\mathcal{L}_{KL}$ is demonstrated by the results of (f)-(h), and setting $\lambda_{KL}$ to 1 is found satisfactory.





\begin{table}[!t]
	\centering
	% \renewcommand\arraystretch{0.98}
	\tabcolsep=0.4cm
	\footnotesize            
	{
            \begin{tabular}{ 
					l
					c      }
				\toprule
				Loss Function
				& mIoU
				\\            
				
				\specialrule{0em}{0pt}{1pt}
				\hline
				\specialrule{0em}{1pt}{0pt}
				(a) $\mathcal{L}=\mathcal{L}^{ce}$ (Baseline)
				& 44.51
				\\            
				
				(b) $\mathcal{L}=\mathcal{L}^{ce}_{p}$
				& 44.06
				\\               
				
				(c) $\mathcal{L}=\mathcal{L}^{ce} + \mathcal{L}^{ce}_{p}$
				& 45.14
				\\                   
				
				(d) $\mathcal{L}=\mathcal{L}^{ce} + \mathcal{L}^{ce}_{p} + \mathcal{L}^{ce}_{y}$
				& 45.74
				\\                    
				
				(e) $\mathcal{L}=\mathcal{L}^{ce} + \mathcal{L}^{ce}_{p} + \mathcal{L}_{KL}$
				& 45.23
				\\              
				
				\rowcolor{gray!25}
				(f) $\mathcal{L}=\mathcal{L}^{ce} + \mathcal{L}^{ce}_{p} + \mathcal{L}^{ce}_{y} + \mathcal{L}_{KL}$
				& 46.91
				\\                              
				
				(g) $\mathcal{L}=\mathcal{L}^{ce} + \mathcal{L}^{ce}_{p} + \mathcal{L}^{ce}_{y} + 0.1 \cdot \mathcal{L}_{KL}$
				& 45.88
				\\       
				
          
				
				(h) $\mathcal{L}=\mathcal{L}^{ce} + \mathcal{L}^{ce}_{p} + \mathcal{L}^{ce}_{y} + 10 \cdot \mathcal{L}_{KL}$
				& 46.08
				\\                
				
				\bottomrule                                   
			\end{tabular}
	}
    \caption{Ablation study on different loss combinations. }
	\label{tab:ablation_loss}     
\end{table}     

\begin{table}[!t]
	\centering
	\tabcolsep=0.4cm
	\footnotesize            
	{
            \begin{tabular}{ 
					l
					c      }
				\toprule
				Loss Function
				& mIoU
				\\            
				
				\specialrule{0em}{0pt}{1pt}
				\hline
				\specialrule{0em}{1pt}{0pt}
				(a) w/o KL
				& 45.74
				\\            
				
				(b) Vanilla KL
				& 45.72
				\\            
				
				(c) Entropy KL
				& 45.99
				\\           
				
				(d) Class-wise KL
				& 46.10
				\\                
				
				\rowcolor{gray!25}
				(e) Class-wise Entropy KL
				& 46.91
				\\          
				
				\specialrule{0em}{0pt}{1pt}
				\hline
				\specialrule{0em}{1pt}{0pt}            
				
				(1) Class-wise Entropy KL (Est.)
				& 46.58
				\\       
				
				(2) Class-wise Entropy KL (Ori.)
				& 46.22
				\\                
				\bottomrule                                   
			\end{tabular}
	}
	\caption{Ablation study on the designs for KL loss.  }  
	\label{tab:ablation_klloss}    
\end{table}     

\mypara{Different forms of KL loss. }
Section~\ref{sec:learning_cwc} introduces the vanilla KL loss that encourages the model to learn to form the context-aware classifier. 
To alleviate the information bias and further exploit hidden useful cues, we propose an alternative form that leverages the class-wise entropy. To show the effectiveness of the proposed design on $\mathcal{L}_{KL}$, results are shown in Table~\ref{tab:ablation_klloss} where Exp. (a) is the same as Exp. (d) in Table~\ref{tab:ablation_loss} without KL loss.
Besides, different from Exps.~(d)-(e) whose entropy mask is obtained from $\vec{p}_y$,
entropy masks in Exps.~(1)-(2) are estimated by $\vec{p}_p$ and $\vec{p}$ respectively.

In Table~\ref{tab:ablation_klloss}, the vanilla KL loss achieves comparable to Exp. (a) without KL loss, and the entropy-based KL in Exp. (c) also incrementally improves Exp. (a) because the information of the majority may still overwhelm the others. Instead, by applying class-wise calculation to (b), improvement is obtained in Exp. (d), since it helps alleviate the imbalance between different classes. Furthermore, to tackle the information bias, informative cues are better exploited in Exp. (e) by incorporating the entropy estimation with the class-wise KL, achieving persuasive performance. Last, Exp. (1) and Exp. (2) prove that the oracle predictions $\vec{p}_y$ are more favorable than $\vec{p}_p$ (Est.) and $\vec{p}$ (Ori.) for estimating the entropy mask used in Eq.~\eqref{eq:entropy}.

\begin{figure}[!t]
	\centering
	\begin{minipage}   {0.24\linewidth}
		\centering
		\includegraphics [width=1\linewidth] 
		{figure/post_tsne/1.pdf}
	\end{minipage}    
	\begin{minipage}   {0.24\linewidth}
		\centering
		\includegraphics [width=1\linewidth] 
		{figure/post_tsne/2.pdf}
	\end{minipage}        
	\begin{minipage}   {0.24\linewidth}
		\centering
		\includegraphics [width=1\linewidth] 
		{figure/post_tsne/3.pdf}
	\end{minipage}     
	\begin{minipage}   {0.24\linewidth}
		\centering
		\includegraphics [width=1\linewidth] 
		{figure/post_tsne/4.pdf}
	\end{minipage}           
	%\vspace{-0.2cm}
	\caption{Results of t-SNE. Categories are represented in different colors. Small dots are feature vectors, large circles are the weights of the original classifier, and stars are the weights of the approximated context-aware classifier. }
	\label{fig:post_tsne}
\end{figure}

\begin{table}[!t]
	\centering
	\tabcolsep=0.4cm
	\footnotesize            
	{
		\begin{tabular}{ 
				l
				c      }
			\toprule
			Classifier
			& mIoU
			\\            
			
			\specialrule{0em}{0pt}{1pt}
			\hline
			\specialrule{0em}{1pt}{0pt}
			(a) Original (Dot) 
			& 44.51
			\\                 
			
			(b) Original (Cos) 
			& 43.89
			\\                    
			
			(c) Original (Dot) + Context (Dot)
			& 45.42
			\\                  
			
			\rowcolor{gray!25}
			(d) Original (Dot) + Context (Cos)
			& 46.91
			\\            
			
			(e) Original (Cos) + Context (Cos)
			& 46.39
			\\       
			
			\specialrule{0em}{0pt}{1pt}
			\hline
			\specialrule{0em}{1pt}{0pt}            
			
			(1) Exp. (d) with ($\tau = 5$)
			& 45.39
			\\                 
			
			(2) Exp. (d) with  ($\tau = 10$)
			& 46.78
			\\                 
			
			(3) Exp. (d) with ($\tau = 20$)
			& 46.26
			\\           
			
			\bottomrule                                   
		\end{tabular}
	}
	\caption{Ablation study on the cosine similarity and dot product on the original and the proposed context-aware classifiers. Exps. (b), (d) and (e) are with $\tau=15$. }  
	\label{tab:ablation_cosine} 
\end{table}     


\mypara{The necessity of cosine similarity. }
In segmentation models, the original classifier $\mathcal{C} \in \mathcal{R}^{[n, d]}$ applies dot product on the features $\vec{f} \in \mathcal{R}^{[hw, d]}$ yielded by the feature generator to get the output $\vec{p} = \vec{f} \times \mathcal{C}^\top = |\vec{f}||\mathcal{C}| \mathop{\cos}(\vec{f}, \mathcal{C})$. 
However, the proposed context-aware classifier yields predictions via cosine-similarity 
$\vec{p}_a 
= \tau \cdot \mathop{\cos}(\vec{f}, \mathcal{A}_*) 
= \tau \cdot \eta(\vec{f}) \times \eta(\mathcal{A}_*)^\top$ ($* \in \{y, p\}$).
The difference is that, cosine similarity focuses on the angle between two vectors, while dot product considers both angle and magnitudes. 


Though both cosine similarity and dot product seem plausible, since the norms $|\vec{f}|$ and $|\mathcal{C}|$ are not bounded, extreme values may occur and hinder the optimization process of the context-aware classifier to proceed normally as that with the original classifier. On the contrary, the instability issues caused by the magnitudes of dynamically imprinted classifier weights $\mathcal{A}_y$ and $\mathcal{A}_p$ can be alleviated by applying L-2 normalization in the cosine function. 



\begin{table}[!t]
	\centering
	\tabcolsep=0.5cm
	\footnotesize            
	{
        \begin{tabular}{ 
              l
              c           }
             \toprule
             Model
             & mIoU
             \\            

             \specialrule{0em}{0pt}{1pt}
             \hline
             \specialrule{0em}{1pt}{0pt}
             Baseline
             & 44.51
             \\               


             (a) $\mathcal{A}_* = \mathcal{C}_*$
             & 44.08
             \\               

             (b) $\mathcal{A}_* = \mathcal{C}_* + \mathcal{C}$
             & 44.13
             \\   


             (c) $\mathcal{A}_* = \theta_*(\mathcal{C}_*)$
             & 46.21
             \\   

             (d) $\mathcal{A}_* = \theta_*(\mathcal{C}_* + \mathcal{C})$
             & 46.53
             \\    

             \rowcolor{gray!25}
             (\#)  $\mathcal{A}_* = \theta_*(\mathcal{C}_* \oplus \mathcal{C})$
             & 46.91
             \\                 

             (e)  $\mathcal{A}_* = \theta_*(\mathcal{C}_* \oplus \mathcal{C}) + \mathcal{C}$
             & 45.94
             \\              

             (f) $\mathcal{A}_y =  \mathcal{C}_y, \mathcal{A}_p = \theta_p(\mathcal{C}_p \oplus \mathcal{C})$

             & 46.01
             \\    


             (g) $\mathcal{A}_y = \theta_y(\mathcal{C}_y \oplus \mathcal{C}), \mathcal{A}_p = \mathcal{C}_p$
             & 44.55
             \\                           

             (h) $\mathcal{A}_y =  \mathcal{C}, \mathcal{A}_p = \theta_p(\mathcal{C}_p \oplus \mathcal{C})$
             & 45.69 
             \\                

             (i) $\mathcal{A}_y = \theta_y(\mathcal{C}_y \oplus \mathcal{C}), \mathcal{A}_p = \mathcal{C}$
             & 44.62
             \\                 
             \bottomrule                                   
         \end{tabular}
	}
    \caption{
	Ablation study on alternative designs for yielding the context-aware classifier.
	All models except for the baseline are optimized with $\mathcal{L}_{KL}$. 
}
    \label{tab:ablation_alternative_designs}   
    %\vspace{-0.1cm} 
\end{table}    




Experimental results are shown in Table~\ref{tab:ablation_cosine} where the context-aware classifier implemented with cosine similarity achieves favorable results while the dot product is better for the original classifier.
This discrepancy may be related to their formation processes.
The original one is shared by all samples, but the weights of the context-aware classifier are dynamically imprinted, thus the former may find an optimal magnitude that generalizes well to a universal distribution throughout the training process. Since magnitudes provide additional information regarding different categorical distributions, dot-product works better on the original classifier. 

Differently, because the approximated context-aware classifier is generated individually, the overall categorical magnitudes may be dominated by the features with large magnitudes, overwhelming those with smaller magnitudes. Furthermore, even for the same class, the feature magnitude will change because of the varying co-occurring stuff and things in different images. Therefore, cosine similarity simply ignores the unstable magnitudes but instead focuses on the inter-class relations, bringing better results to the context-aware classifier. 
In addition, the sensitivity analysis regarding different values of scaling factor $\tau$ shows that the results of $\tau=\{5, 10, 20\}$ are inferior to Exp. (d) with $\tau=15$. Therefore, We set $\tau$ to 15 in all experiments. 





\mypara{Alternative designs for yielding context-aware classifier. }
As shown in Eqs.~\eqref{eq:oracle_cls} and \eqref{eq:estimate_cls} in Section~\ref{sec:method}, the oracle and the estimated context-aware classifiers $\mathcal{A}_*$ ($*$ is the placeholder for $y$ and $p$) are generated by applying the projectors $\theta_*$ to the concatenation of the estimated contextual prototypes $\mathcal{C}_*$ and the weights $\mathcal{C}$ of the original classifier, \ie, $\mathcal{A}_* = \theta_*(\mathcal{C}_* \oplus \mathcal{C})$. There are several other design options and results are shown in Table~\ref{tab:ablation_alternative_designs}.

Concretely, (a) means both $\mathcal{A}_y$ and $\mathcal{A}_p$ are simply formed by the oracle and estimated semantic prototypes, and (b) adds the weights of the original classifier as a residue. However, both (a) and (b) cause performance deduction because the estimated prototypes $\mathcal{C}_p$ may deliver irrelevant or even erroneous messages without any processing. Differently, adding a projector to the estimated prototypes $\mathcal{C}_p$ is helpful, as verified by (c), since the projector keeps the essence and screens the noise from $\mathcal{C}_p$.
 Moreover, introducing the information contained in the original classifier is found conducive, as shown by (d) and (\#), and (\#) shows that the concatenation operation is more effective than simply adding the prototypes. But, adding the original ones as a residue in (e) degrades the performance because it may lead to a trivial solution that is to simply skip the projector, which is easier for optimization. The last four results of (f)-(i) are inferior to that of (\#), manifesting the necessity of adopting (\#) to yield both $\mathcal{A}_y$ and $\mathcal{A}_p$. Also, the comparison between (\#) and (i) shows that removing the estimated context-classifier may cause significant performance deduction. The discussion of the projector's structure is in the supplementary file.












\mypara{Impact on model efficiency. }
Our proposed method is effective yet efficient since, during inference, it only introduces an additional lightweight projector and several simple matrix operations to the original model. To comprehensively study the impacts brought to the model efficiency, frames per second (fps) and GPU memory consumption (Mem) obtained in higher input resolutions, \ie, $1280\times1280$ and $2560 \times 2560$, are presented in Table~\ref{tab:ablation_efficiency} from which we can observe that only minor negative impacts are brought to the baseline models, even with the high input resolutions. 

\begin{table}[!t]
    \centering
    % \renewcommand\arraystretch{0.98}
    \tabcolsep=0.12cm
    \footnotesize            
    {
        \begin{tabular}{ 
           l
           c    
           c
           c    
           c
           c
           c               }
           
            \toprule
			&\multicolumn{2}{c}{$512 \times 512$} 			
			&\multicolumn{2}{c}{$1280 \times 1280$}  
			&\multicolumn{2}{c}{$2560 \times 2560$}  
			
            \\
            Model
            & fps 
            & Mem                
            & fps 
            & Mem                
            & fps 
            & Mem                
		    \\            

            \specialrule{0em}{0pt}{1pt}
            \hline
            \specialrule{0em}{1pt}{0pt}
            Baseline
            & 20.38
            & 2794 
            & 4.13
            & 5650 
            & 1.04
            & 10286          
            \\                 
         
            \rowcolor{gray!25}
            Ours
            & 19.96
            & 2796  
            & 4.05
            & 5654
            & 1.02
            & 10290            
            \\    
            
            \rowcolor{gray!45}
            $\triangle$
            & -2.05\%
            & +0.07\%
            & -1.94\%
            & +0.07\%  
            & -2.00\%
            & +0.04\% 
            \\                
            
            \bottomrule                                   
        \end{tabular}
    }
    \caption{Comparison regarding FLOPs and frames per second (fps) and GPU memory usage (Mem) in different input resolutions. $\triangle$ denotes the relative change. The baseline model is UperNet+Swin-Tiny, and the results of fps are obtained on a single NVIDIA RTX 2080Ti GPU. }
    \label{tab:ablation_efficiency} 
   %\vspace{-0.1cm} 
\end{table}         

