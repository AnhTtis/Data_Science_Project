\section{Related Work}
Semantic segmentation is a fundamental yet challenging task where precise pixel-wise predictions are needed. However, models cannot make prediction for each position merely based on its RGB values, thus broader contextual information are exploited to achieve decent performance.

FCN~\cite{fcn} proposes to adopt the convolution layers to tackle the semantic segmentation task. Then, well-designed decoders~\cite{deconvnet,segnet,unet} are proposed to gradually up-sample the encoded features in low resolution, so as to retain sufficient spatial information for yielding accurate predictions. 
Besides, since the receptive field is important for scene parsing, dilated convolutions~\cite{deeplab,dilation}, global pooling~\cite{parsenet} and pyramid pooling~\cite{deeplab,pspnet,denseaspp,tian2020pfenet,strip} are proposed for further enlarging the receptive field and mining more contextual cues from the latent features extracted by the backbone network. More recently, pixel and region contrasts are exploited~\cite{pixelcontrast,lai2021cac,regioncontrast,jiang2021semi,cui2022generalized}. 

Also, transformer performs dense spatial reasoning, thus it is adopted in decoders for modelling the long-range relationship in the extracted features~\cite{ocnet,psanet,catrans,cui2022region,encnet,danet,ccnet,ocr,maskformer}.  Transformer-based backbones take a step further because the global context can be modeled in every layer of the transformer, achieving new state-of-the-art results. Concretely, by applying a pure transformer ViT~\cite{vit} as the feature encoder, \cite{setr,segmenter} set up new records on semantic segmentation against the other convolution-based competitors, and Swin Transformer~\cite{swin} further manifests the superior performance with the decoder head of UperNet~\cite{upernet}. Besides, SegFormer~\cite{segformer} is a framework specifically designed for segmentation by combining both local and global attentions to yield informative representations. 

In summary, the mainstream of research aiming at improving segmentation model structures focuses on either designing backbones for feature encoding or developing decoder heads for producing informative latent features, and the classifier is seldom studied. Instead, we exploit the semantic cues in individual samples via learning to form the context-aware classifiers, keeping the rest intact. 

