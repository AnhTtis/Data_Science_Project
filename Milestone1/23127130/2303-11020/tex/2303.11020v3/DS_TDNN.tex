\documentclass[lettersize,journal]{IEEEtran}
\usepackage{newtxtext,newtxmath}
%\usepackage{newpxtext,newpxmath}
%\usepackage{amsmath, amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}

\usepackage{array}
\usepackage[caption=false,font=footnotesize,labelfont=rm,textfont=rm]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{color, xcolor} % 颜色包，color 必须导入，xcolor 建议导入
\usepackage{soul} % 导入 soul 包
\usepackage{balance} 
\definecolor{mygray}{gray}{.93}
\definecolor{mygreen}{RGB}{57, 124, 124}
% updated with editorial comments 8/9/2021
%\usepackage{subfig}
\usepackage{xcolor}
%\usepackage{svg}

%\usepackage[T1]{fontenc} % or LY1
%\usepackage{textcomp} % unnecessary with LY1
%\usepackage[altbullet]{lucidabr}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	pdftitle={Overleaf Example},
	pdfpagemode=FullScreen,
}

\def\MakeUppercaseUnsupportedInPdfStrings{\scshape}

\begin{document}

\title{DS-TDNN: Dual-stream Time-delay Neural Network with Global-aware Filter for Speaker Verification}

\author{Yangfu Li, Jiapan Gan, Xiaodan Lin$^*$ and \\ School of Information Science and Engineering, Huaqiao University
        % <-this % stops a space
\thanks{
* Corresponding Author. 
	
Yangfu Li, Jiapan Gan, and Xiaodan Lin are with the School of Information Science
and Engineering, Huaqiao University, Xiamen 361021, China (e-mail:
21013082029@stu.hqu.edu.cn; 22013082022@stu.hqu.edu.cn; xd\_lin@hqu.edu.cn).}% <-this % stops a space
%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files, Jul~2023}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}g
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle
%全文改写表达：receptive field, global context; 核心motivation 在浅层网络获取全局上下文。
\begin{abstract}
%三点，引出我们的工作
%随着。。。的发展，长时间的说话人验证具有越来越重要的研究意义（为什么要focus长时间的音频）。
%传统的TDNN在时音频的说话人验证中表现出色，然而由于感受野的限制，不太擅长利用长时间音频中的上下文信息。
%然而，现有的工作要不1、显著增加了复杂度，real-time性能很差，2、要不无法兼顾局部信息与全局信息，使得在短时音频与长时音频的说话人验证中不能很好的权衡。
Conventional time-delay neural networks (TDNNs) struggle to handle long-range context, their ability to represent speaker information is therefore limited in long utterances. Existing solutions either depend on increasing model complexity or try to balance between local features and global context to address this issue. To effectively leverage the long-term dependencies of audio signals and constrain model complexity, we introduce a novel module called Global-aware Filter layer (GF layer) in this work, which employs a set of learnable transform-domain filters between a 1D discrete Fourier transform and its inverse transform to capture global context. Additionally, we develop a dynamic filtering strategy and a sparse regularization method to enhance the performance of the GF layer and prevent overfitting. Based on the GF layer, we present a dual-stream TDNN architecture called DS-TDNN for automatic speaker verification (ASV), which utilizes two unique branches to extract both local and global features in parallel and employs an efficient strategy to fuse different-scale information. Experiments on the Voxceleb and SITW databases demonstrate that the DS-TDNN achieves a relative improvement of 10\% together with a relative decline of 20\% in computational cost over the ECAPA-TDNN in speaker verification task. This improvement will become more evident as the utterance's duration grows. Furthermore, the DS-TDNN also beats popular deep residual models and attention-based systems on utterances of arbitrary length.
%Furthermore, the DS-TDNN has a low computational cost, making it promising for real-life applications. 
%Finally, we provide visualizations and detailed ablation studies to clarify the advantages of the GF layer and the DS-TDNN.
%

\end{abstract}

\begin{IEEEkeywords}
Time-delay neural network, dual-stream network, text-independent speaker verification, global context.
\end{IEEEkeywords} 
  

\section{Introduction}
%为什么要focus长音频的问题：
\IEEEPARstart{A}{utomatic} Speaker Verification (ASV) systems that aims to determine whether a given utterance is from an enrolled speaker has been widely applied in user authentication, access control, multimedia forensics, and many others \cite{rosenberg1976automatic,broun2002automatic,becker2008forensic}. Typically, an ASV system consists of two main components: a front-end that extracts low-dimensional discriminative speaker embeddings from variable-length utterances and a back-end that determines whether two embeddings are from the same speaker. %特别地，如何提取出稳健的说话人表征成为了ASV的关键。
With the development of deep neural networks (DNNs), the front-end has shifted from probabilistic models \cite{reynolds2000speaker,kenny2007joint,dehak2010front} to DNN-based methods \cite{lei2014novel,variani2014deep,snyder2017deep}. In particular, x-vector \cite{snyder2017deep} as a state-of-the-art architecture for speaker embedding is built on the Time-Delay Neural Network (TDNN) \cite{waibel1989phoneme} layers for end-to-end speech processing, where shared-weight filters are employed to attend to all the frequency components and a time-delay strategy is applied to capture context between consecutive frames.


\iffalse
1.简要介绍ASV

2.为什么ASV需要关注全局信息（长短音频中都要，但在长音频中更显著）（顺带聊一下早期的TDNN设计意识到了这个问题，然而并没有很好的解决方法）

3.别人做了全局信息吗？做了，两种办法做的
他们的工作有哪些不足？里面聊
我们的工作与他们有什么不同？
{把表放上来，introduction就直接介绍复杂度！}后面仅计算GF自身的复杂度。

4.我们做了什么工作，如何做的

5.效果怎么样

对于ASV模型来说，除了模型表现，性能与计算代价的平衡同样重要。
我们从两个方面来改善模型的计算代价：1、在计算全局上下文时，我们设计了一个高效的全局滤波器。相比于现有的基于自回归或者自注意力机制的算法来说，拥有明显更低的计算复杂度；
2、我们设计了一个双流TDNN模型，采用并行的方式计算不同尺度的信息并对其进行融合。相比与目前流行的交替模式，其拥有更低的模型参数与计算延时。

\fi


%所以对于ASV模型来说，深度往往比宽度更加重要。因为需要堆叠足够深度来获取global feature。
To extract robust speaker representation, both local features and global context are essential. However, typical TDNNs focus primarily on local features while being limited in modeling global context due to the small receptive field in each hidden layer.
%as a result ...后面开始有大问题
As a result, TDNN-based models exhibit suboptimal performance in wild scenarios, particularly when the test utterances are exposed to noise. Furthermore, in real-world applications such as call monitoring, video processing, and real-time online meetings, utterances can last for tens of seconds, highlighting the need of global context modeling and faster inference speed for speaker verification.  A natural idea to emphasize global context is to extend the depth of TDNNs. \cite{novoselov2018deep} introduces residual connections \cite{he2016deep} to construct an extremely deep TDNN that captures context information over a longer time span. Snyder et al. \cite{snyder2019speaker} extend TDNN by inserting dense layers between each pair of hidden layers. To prevent overfitting, \cite{huang2019deeper} introduces dropouts \cite{srivastava2014dropout} to TDNN and proposes a filter with varying temporal resolution for more powerful context representation. To further deepen TDNN within affordable parameter overhead, Yu et al. \cite{yu2020densely} attempts to make a trade-off between the width and depth. However, thin TDNNs may suffer from less robust speaker representations. To solve this problem, a split-transform-merge structure is proposed in \cite{zhang2020aret} that helps the deep TDNN with limited width learn more discriminative speaker representations. 
%问下老师怎么改合适
These improvements make remarkable progress in performance, while they also suffer from the problem of model complexity, leading to poor real-time performance. Moreover, these methods do not consider the fusion of local and global features. 
%他们没有考虑到如何将全局特征与局部特征融合起来。为了解决这些问题（问题包括，如何获取全局特征，如何将其与局部特征相结合。）, 

%下文要讲他们是如何结合全局与局部的，这样做有什么问题
To address the above issues, recent line of research can be roughly divided into two categories. A number of works is towards enhancing the filters with multi-scale information. \cite{desplanques2020ecapa} introduces the Res2Net \cite{gao2019res2net} structure into TDNN, by inserting skip connections between each grouped filter to construct a different-scale receptive field and proposes the multi-layer feature aggregation to fuse local and global features, achieving state-of-the-art performance. To enhance the Res2Net, \cite{9688119} proposes a context-aware filter by dividing each grouped filter applied in Res2Net into two different-scale filters: a normal receptive field filter for local features and a large receptive field filter for global context. Authors in \cite{gu2021dynamic} propose another type of dynamic filter, whose value is determined by an element-wise summation between the local features calculated using moving average and the global context calculated through global average pooling. Recently, \cite{mun2022selective} presents a multi-resolution filter, which employs a kernel selection mechanism to find the optimal receptive field. Although these enhanced filters can dynamically adapt to local features or global context as required, none of them simultaneously attend to both local features and global context, limiting their representation capability.
%先讲用自回归来handle global feature， 然而，自回归有很高的计算时延。后来，人们考虑采用MLP与attention计算全局信息。然而，这两者虽然可以并行计算，但有很高的计算复杂度。此外。之前的工作往往采用alternating pattern来结合全局信息与局部信息。这样导致计算的全局信息与局部信息耦合在一起，拥有较高计算开销，并且里面的特征被long余计算了。
The other line of research considers incorporating the additional modules to handle global context with TDNN, e.g., autoregressive models and self-attention, as shown in Fig \ref{fig 1}. For example, authors in \cite{chen2019speaker} employ the Long Short-Term Memory (LSTM) module in the shallow layers of TDNN to provide high-resolution temporal context information. In \cite{jiang2019effective}, the authors insert LSTM between each pair of hidden layers in TDNN, enabling TDNN to capture global context. However, these dense LSTM methods are computationally expensive. To improve efficiency, \cite{sak2014long, liu2019speaker} utilize recurrent projection to reduce the dimension of the data flow in the LSTM cells. Another approach is to apply the LSTM at the segment-level rather than the frame-level, handling temporal information in the low-dimensional latent space \cite{tu2019towards,lin2019lstm}. With the success of self-attention, the authors in \cite{huang2020speaker} combine the frame-level LSTM with the segment-level attention mechanism to further enhance the performance. This technique has shown promising performance, but it comes with the side effect of high computational cost.


\iffalse
聊完别人的做法，开始聊问题，为什么他们的方法不够好

并行模式的效率体现在如下两个方面：
1、针对不同尺度的特征提取过程，分别作用在更小的特征图上；
2、不同尺度的操作并行进行，无需等待前层计算结果。
\fi


\begin{figure}[!t]
	\centering
	%\hspace{5mm}
	\includegraphics[width=\linewidth]{mode}
	\caption{A comparison of different strategies to combine local and global features. (a) TDNN without additional modules for global context. (b) the popular alternating pattern. (c) the proposed parallel pattern.}
	%a是经典的TDNN,不采用额外module来建模远距离上下文。b是流行的交替模式，在一个较大feature map上交替计算局部与全局信息。c是我们提出的并行模式，通过通道切分解耦局部与全局信息，并在一个较小的尺度上并行计算他们。
	\label{fig 1}
\end{figure}
The existing designs of the global modules, such as LSTM or self-attention, have intensive computational complexity. Additionally, different-scale operations are directly performed on the whole mixture of local and global features as illustrated in Fig. \ref{fig 1}b, incurring high parameter overhead. To address these two challenges, this paper first proposes a novel and efficient global module called the Global-aware Filter (GF) layer. The GF layer consists of three key components: a discrete Fourier transform, a set of differentiable transform-domain filters, and the inverse discrete Fourier transform. It explicitly models the global context, but only has a log-linear complexity. Moreover, we introduce dynamic filtering to further enhance the GF layer. With dynamic filtering, the GF layer can adapt to different speech contexts, thereby improving its representation ability and generalization performance. To prevent over-fitting, we also propose sparse regularization, which randomly drops filters in the GF layer with a fixed ratio during the training phase.
In addition, we propose a parallel workflow to combine local and global features, as shown in Fig. \ref{fig 1}c. In this pattern, different-scale modules are designed to focus on certain parts of the complete feature maps, which is more efficient than the popular alternating pattern. By incorporating the GF layer and the parallel framework, we construct a simple yet efficient dual-stream TDNN for speaker verification, called DS-TDNN. DS-TDNN employs two independent branches to process local features and global context in parallel and applies several carefully designed strategies to fuse different-scale information. Experimental results on the Voxceleb and SITW datasets demonstrate that DS-TDNN outperforms the powerful TDNN-based baseline, ECAPA-TDNN, with a lower computational cost. Furthermore, it outperforms other popular baseline systems, such as 2D CNN with residual connections and the attention-based model. Moreover, DS-TDNN achieves the best trade-off between effectiveness and efficiency. We have released the models and code for further research\footnote[1]{\url{https://github.com/YChenL/DS-TDNN}}. 

Our contributions are summarized as follows:
\begin{itemize} 
	\item We propose an innovative module termed \emph{Global-aware Filter} (GF) layer for TDNN, which has global receptive fields yet a log-linear complexity wrt. speech duration. The GF layer is an efficient alternative to popular global-aware algorithms in speaker verification.

	\item We design two special techniques, i.e., \emph{Dynamic Filtering} and \emph{Sparse Regularization} to further enhance the performance of the GF layer. The former enables the GF layer dynamically adapt to the input, providing a more powerful representation of speaker information. The latter aims to reduce the optimization difficulty of the dynamic GF layer and prevent overfitting.
	
	\item We propose a novel parallel framework named DS-TDNN for the extraction of different-scale information, as a solution to speaker embedding. Experiments on the Voxceleb and SITW datasets demonstrate that the proposed DS-TDNN achieves state-of-the-art performance compared to popular baseline systems.
	
	%\item Experiments on the Voxceleb and SITW datasets demonstrate that the DS-TDNN achieves state-of-the-art results compared to popular baseline systems. Moreover, detailed visualizations and ablation studies are conducted to reveal the benefits of the design of DS-TDNN.

	
\end{itemize}

The rest of this paper is organized as follows: Section II introduces the global-aware filter layer, including its motivation, implementation, improvements, and comlexity analysis. Section III describes the dual-stream TDNN. The experimental setup is detailed in Section IV. The result analysis is presented in Section V, followed by a conclusion in Section VI.

%\newpage
\section{Global-aware filter}
\iffalse
由于我们做通道可分离的FFT，我们的DGF对通道和序列长度L均为线性复杂度.
\fi

\subsection{Motivation}
Conventional TDNNs mainly utilize time-delay layers for feature extraction, where convolutions are restricted within the window rather than the entire feature map, limiting the modeling for long-range context. 
A natural idea for expanding the receptive field of convolution is to increase the window size. For example, employing the global convolution\cite{8099672} to capture the global context. However, the global convolution results in a $\mathcal{O}(N^2)$ complexity for a sequence including $N$ points. Fortunately, there is a simple yet efficient equivalent of the global convolution, which can be derived from the discrete convolution theorem %(see Appendix A) 
and is formulated as:
\begin{equation}
	\bm {w}_g* \bm x \equiv \mathcal{F}^{-1}[{\bm w}_f\mathcal{F}[\bm x]],
\end{equation}
where $\bm w_f$ is regarded as a transform-domain filter. $\bm w_g$ is a spatial-domain filter with a global receptive field, $\bm x$ is an $N$-point token.
%, and $\bm w_f$ 
$*$ denotes one-dimensional convolution. $\mathcal{F}[\,\bm \cdot\,]$ and $\mathcal{F}^{-1}[\,\bm \cdot\,]$ separately denote the one-dimensional discrete Fourier transform (DFT) and its inverse transform (IDFT). Benefitting from the fast algorithms of DFT/IDFT, i.e., FFT/IFFT, the complexity of $N$-point DFT/IDFT is reduced from $\mathcal{O}(N^2)$ to $\mathcal{O}(N{\rm log}N)$. Equ 1 reveals the use of FFT/IFFT to model long-range context, which has been demonstrated powerful in computer vision tasks\cite{li2020falcon,chi2020fast,lifourier}, but has never been explored for ASV. Motivated by this, we introduce the global-aware filter layer for TDNN that modulates the tokens using FFT/IFFT together with a set of differentable filters $\bm w_f$ to handle the global context. 
%employs FFT/IFFT and a set of learnable filters in the . 



\subsection{Global-aware filter Design}
%和上面的subsection揉在一起写
We propose a global-aware filter (GF) layer as an efficient alternative to global-aware modules, e.g., global convolution or self-attention, to capture global context. In TDNN, the tokens $\bm X \in \mathbb{R}^{C\times T}$ can be considered a series of discrete sequences $\bm x_i\in \mathbb{R}^{1\times T}$ stacked along the channel dimension. Therefore, given the tokens ${\bm X}$, the corresponding spectrum $\bm X_f$ can be obtained by 1D FFT individually performed on every channel, where the number of FFT bins is the same as the length of each input channel.
\begin{equation}
	{\bm X_f}={\rm Concat}(\mathcal{F}[{\bm x_1}], \mathcal{F}[{\bm x_2}], ..., \mathcal{F}[{\bm x_C}])\in \mathbb{C}^{C\times T}.
\end{equation}
For efficiency, we perform channel-wise 1D FFT/IFFT on $\bm X$ and then employ linear projection to mix channels, rather than directly performing 2D FFT/IFFT on $\bm X$.
Besides, since $\bm X$ is a real tensor, the corresponding spectrum $\bm X_f$ is conjugate symmetric, i.e., ${\bm X_f}[: , T-\tau]={\bm X_f^*}[: , \tau]$. Thus, only half of $\bm X_f$ is needed for further processing:
\begin{equation}
	{\bm X_r}={\bm X_f}[:, 0:\lceil T/2\rceil]= \mathcal{F}_r[{\bm X}] \in \mathbb{C}^{C\times \lceil T/2\rceil},
\end{equation}
where $\mathcal{F}_r[\,{\bm \cdot}\,]$ denotes the channel-wise 1D FFT for real signals. 
It is noteworthy that $\bm X_r$ is a complex tensor. 
Since the Fourier transform integrates the whole information of a channel into different elements of the spetrum, we can model the channel-wise global context via a simple element-wise multiplication between the spectrum and a differentiable filter $\bm F \in \mathbb{C}^{C\times \lceil T/2\rceil}$:
\begin{equation}
	\tilde{{\bm X_r}} = {\bm F} \odot {\bm X_r},
\end{equation}
where $\odot$ is the Hadamard product. $\bm F$ can be regarded as the $\bm w_f$ in Eq.(1), termed global-aware filter. Finally, the inverse FFT is adopted to transform the modulated spectrum $\tilde{\bm X_r}$ back and update the tokens: 
\begin{equation}
	{\bm X} \leftarrow \mathcal{F}^{-1}_r[{\tilde{\bm X_r}}].
\end{equation}

Depending on the type of filter $\bm F$, the GF layer can selectively capture either local features or global context of the tokens. When a high-pass filter is used, the GF layer tends to capture local features, while a low-pass filter allows the GF layer to capture global context.

The proposed GF layer can easily adapt to different audio lengths as both the FFT and the IFFT have no learnable parameters. For different lengths of utterances, we can simply interpolate the global-aware filter $\bm F$ to $\bm F' \in \mathbb{C}^{C\times \lceil T^{\prime}/2\rceil} $  where $T^{\prime}$ is the target length. From the frequency sampling point of view,  the process of duration adaptation is equivalent to resampling of the spectrum. Besides, since FFT/IFFT are well supported by GPU and CPU, the GF layer is hardware friendly. %thanks to the acceleration libraries like cuFFT

\subsection{Dynamic filtering}
%介绍泛化，由于内容，情感，通道会随着话语改变。用相同的滤波器处理他们效果不好。
The challenge of ASV in wild speech comes from the intervention of speech content, emotion, and transmisssion channel. Therefore, the distribution between utterances from the same speaker may vary significantly, making it hard to generalize with the static filters. To address this issue, we propose a dynamic filtering strategy inspired by \emph{Dynamic Convolution} that enables the filters to adapt dynamically to the input \cite{yang2019condconv,chen2020dynamic,li2022omni}. Specifically, we apply $K$ independent global-aware filters during modulation and combine them using element-wise summation:

\begin{equation}
	\tilde{{\bm X_r}}= {\bm F_1}\odot {\bm X_r} + {\bm F_2}\odot {\bm X_r}+...+{\bm F_K}\odot {\bm X_r}.\\
\end{equation}
Then, we utilize a 1D channel attention function to produce a series of dynamic scores $\bm w=[w_1, w_2,...,w_K]$, which can be adapted to the input tokens $\bm X$: %The attention function can be formulated as follows:
\begin{equation}
	{\bm w} = \underbrace{{\tt Softmax(FC_2(ReLU(FC_1(GAP}}_{\rm Attention\ FN}({\bm X}))))) \in\mathbb{R}^{1\times K},
\end{equation}
where ${\tt GAP}$ is Global Average Pooling. ${\tt FC_1}$ and ${\tt FC_2}$ separately represent two fully connected layers. In this work, the number of neurons in $\tt FC_1$ is equal to that in $\tt FC_2$, which is set to $K$. Notably, we utilize Softmax rather than Sigmoid to normalize the scores for stable training. The dynamic scores $\bm w$ are then adopted to weight the expert filters, which is formulated as:
\begin{equation}
	\tilde{{\bm X_r}}=w_1{\bm F_1}\odot {\bm X_r}+ w_2{\bm F_2}\odot {\bm X_r}+...+ w_K{\bm F_K}\odot {\bm X_r}.\\
\end{equation}
For simplicity, an equivalent deformation is obtained as
\begin{equation}
	\tilde{{\bm X_r}}=(w_1{\bm F_1}+ w_2{\bm F_2}+...+ w_K{\bm F_K})\odot {\bm X_r},\\
\end{equation}
from which the \emph{dynamic global filter} (DGF) is defined to represent the normalized linear combination of filters as
\begin{equation}
	{\bm F_d} = w_1{\bm F_1}+ w_2{\bm F_2}+...+w_K{\bm F_K}\in \mathbb{C}^{K\times C\times\lceil T/2\rceil}.
\end{equation}
Finally, the spectrum $\bm X_r$ is modulated by the element-product with the dynamic global-aware filter $\bm F_d$.
\iffalse
\begin{algorithm}[t]
	\caption{Pseudocode of Sparse DGF layer.}
	\begin{algorithmic}	
		\STATE
		\STATE \textcolor{mygreen}{\# $\tt attn\_fn\ \text{:}\ Attention\ functions$}
		%\STATE \textcolor{mygreen}{\# $\tt mm\ \text{:}\ matrix\ multiplication$}
		%\STATE \# {\tt $\tt{mm}$: matrix multiplication}
		%\STATE \# {\tt $\tt{rfft, irfft}$: FFT/IFFT for real signals}
		\STATE \textcolor{mygreen}{\# $\tt x\ \text{:}\  global\ features\ in\ a\ batch\text{,}\ B\ x\ C\ x\ T$}
		\STATE \textcolor{mygreen}{\# $\tt F\ \text{:}\  K\ global\text{-}aware\ filters\text{,}\ K\ x\ C\ x\ T_h$}
		\STATE \textcolor{mygreen}{\# $\tt r\ \text{:}\  sparse\ ratio\text{,}\ \text{[}0\text{,}\ 1\text{]}$}	 	
		\STATE		
		\STATE \textcolor{mygreen}{\# $\tt dynamic\ filtering$}		
		\STATE ${\tt w\; \text{=}\; attn\_fn(x)}$\quad \textcolor{mygreen}{\# ${\tt B\ x\ K}$}
		\STATE ${\tt F\; \text{=}\; reshape(F,[K,\text{-}1])}$
		\STATE ${\tt F_d\; \text{=}\; matmul(w,F)}$\quad \textcolor{mygreen}{\# ${\tt B\ x\ CT_h}$}
		\STATE ${\tt F_d\; \text{=}\; reshape(F_d,[B,C, T_h])}$
		\STATE
		\STATE \textcolor{mygreen}{\# $\tt sparse\ regularization$}		
		\STATE ${\tt m \; \text{=}\;  rand([B,C])>r}$\quad \textcolor{mygreen}{\# ${\tt mask}$}
		\STATE $\tt{ m \; \text{=}\;  expand\_dims(m, [B,C,T_h])}$	
		\STATE ${\tt \lambda\; \text{=}\; mean(abs(F_d))}$
		\STATE ${\tt F_d \leftarrow F_d*m}$
	    \STATE 
	    \STATE \textcolor{mygreen}{\# $\tt forward$}		
		\STATE${\tt x_f \; \text{=}\;  rfft(x, dim\;\text{=}\; 2)}$\quad \textcolor{mygreen}{\# ${\tt B\ x\ C\ x\ T_h}$}
		\STATE${\tt x_f \leftarrow x_f*F_d\;\text{+}\;x_f*\lambda*(1\; \text{-}\; m)}$
		\STATE${\tt x \leftarrow irfft(x_f, dim\; \text{=}\; 2)}$
	\end{algorithmic}
	\label{alg 1}
\end{algorithm}
\fi


\subsection{Sparse regularization}

Despite that dynamic filtering is helpful to rendering a more robust and generalized representation, it also poses a challenge regarding the optimization procedure, i.e., a total of $K$ global dynamic filters are to be optimized instead of a static one, resulting in a more complicated loss landscape. Additionally, as the number of parameters increases, the model becomes prone to overfitting. To address this issue, we propose a sparse regularization technique. Specifically, the DGF can be viewed as a stack of 1D filters ${\bm F_d}=[\bm f_1, \bm f_2, …, \bm f_d]$, where $\bm f_1,…,\bm f_d \in \mathbb{C}^{1\times \lceil T/2\rceil}$. During the training phase, parts of these filters are deactivated via an element-wise multiplication with a random sparse-channel mask $\bm M$, i.e., ${\bm F_s}=\bm M\odot {\bm F_d}$. Hereafter ${\bm F_s}$ is defined as the dynamic global-aware filter with sparse regularization (sparse DGF). Notably, to maintain the dynamic characteristic, the mask is directly applied to the DGF rather than to each of the expert filters. During the modulation with sparse DGF, we perform an element-wise summation to preserve the information at regions where the deactivated filters are applied.

%The dynamic global filter with sparse regularization can be formulated as follows
\begin{equation}
	\tilde{\bm X}_r = {\bm F_s}\odot{\bm X_r}+\lambda_s(1-\bm M)\odot{\bm X_r},
\end{equation}
where $\lambda_s=\frac{1}{CT}\sum^C_{i=1}\sum^T_{j=1}|\bm F_{d}^{(i,j)}|$ is a scale factor. 
The unactivated filter can be regarded as a scaled all-pass filter. Noteworthily, the sparse regularization is conducted only in the training phase. 



\begin{figure*}[!t]
	\centering
	%\hspace{5mm}
	\includegraphics[width=\linewidth]{model}
	\caption{Overview of the DS-TDNN architecture, where $\oplus$ denotes element-wise summation; $*$ denotes element-wise multiplication; BN denotes 1D BatchNorm; (Proj, C) is a linear projection layer with C hidden neurons; GAP is global average pooling; 3$\times$1 is a 1D convolution with kernel size of 3 and step of 1. }
	\label{fig 2}
\end{figure*}


\section{Dual-stream TDNN}
\iffalse
我们认为，ASV中一部分特征就应该是局部的，而一部分特征就应该是全局的。若仅使用全局滤波抽取局部特征，则效果不好。而仅使用局部滤波，则无法抽取全局特征。
\fi


In addition to efficiently handling long-range context, it is also crucial to consider how to combine local and global features in the model. Existing works perform different-scale operations alternately over the entire feature maps that consist of both local and global features, as shown in Fig. \ref{fig 1}b. However, this alternating pattern has two shortcomings. Firstly, applying local filters to extract global context or global filters to extract local features is inefficient. Secondly, to create a more discriminative speaker representation, feature maps in TDNN generally feature a large size, making it computationally expensive. To address this issue, we propose a parallel framework, as shown in Fig. \ref{fig 1}c. Different modules are designed to pay attention to complementary features rather than the entire inputs, and the output of each module is concatenated as the final result. We assume that the input feature maps contain equal number of local features and global context. Under this assumption, the channel dimension of the dataflow in the parallel framework is reduced to half compared to the alternating pattern. Notably, only a simple channel split is required for the disentanglement of the local features and the global context. Based on this idea, we propose the dual-stream (DS-TDNN) model, which applies the DGF layer (detailed in Section II) as the global module and combines the local features and global context in the parallel pattern.

\iffalse
Quantitatively, the parallel pattern reduces more than half of the complexity compared to the alternating pattern as well as the standard TDNN, which can be calculated as follows:
\begin{equation}
	\mathcal{O}(C^2+C)-\mathcal{O}(C^2/4+C/2)=\mathcal{O}(3C^2/4+C/2), 
\end{equation}
where $C$ is the number of input channels.
\fi




\subsection{Macro design}
\iffalse
讲清楚两件事：1）具体如何结合局部特征与全局特征（a）block之间 （b）branch之间；为什么
             2）ASP是什么。
             3）交代一下data flow
\fi
The architecture of DS-TDNN is shown in Fig. \ref{fig 2}. Overall, DS-TDNN is comprised of two inter-connected branches: a local branch that integrates the Res2Conv module to capture local features, and a global branch that modulates spectrums using the DGF layer in transform domain to handle long-range context. Each branch operates on only half of the input channels in parallel and captures complementary information with different-scale receptive fields. The input of each branch is extracted from 80-dimension Mel spectrogram $\bm X \in \mathbb{R}^{80\times T}$, then passes through a 1D convolution with a kernel size of 7 and a step of 1, ReLU activation, and 1D BatchNorm:
%\begin{equation}
	%{\bm X_{l}^0}, {\bm X_{g}^0} ={\tt Split}({\tt stem}({\bm X})) \in \mathbb{R}^{C/2\times T},
%\end{equation}
\begin{equation}
	{\bm X_{l}^0}, {\bm X_{g}^0} = {\tt Split}({\tt BN}({\tt ReLU}({\tt Conv}({\bm X })))) \in\mathbb{R}^{C/2\times T},
\end{equation}
where ${\bm X_{l}^0}$, ${\bm X_{g}^0}$, denote the inputs of the local and global branches, respectively. $C$ represent the numbers of basic channels in the DS-TDNN. 
Besides the final fusion of the two braches, normalized element-wise summation is also employed to gradually fuse local and global features within the branch. The output of the $i$-th layer in the branch can be formulated as follows:
\begin{equation}
	\begin{aligned}
		&{\bm X_{l}^i}={\tt Block}_{l}^i(0.8{\bm X_{l}^{i-1}}+0.2{\bm X_{g}^{i-1}}) \in \mathbb{R}^{C/2\times T},\\
		&{\bm X_{g}^i}={\tt Block}_{g}^i(0.2{\bm X_{l}^{i-1}}+0.8{\bm X_{g}^{i-1}}) \in \mathbb{R}^{C/2\times T}, 
	\end{aligned}
\end{equation}
where $1\leq i\leq N$. ${\bm X_{l}^i}$, ${\bm X_{g}^i}$ denote the output of the $i$-th local and global block. ${\tt Block}_l^i$, ${\tt Block}_g^i$ represent the $i$-th local block and global block respectively. Then, a multi-scale feature aggreation (MFA) \cite{desplanques2020ecapa,liu2022mfa,zhang2022mfa} is performed to fuse different-scale speaker information by concatnating the output of each layer. Following this, a linear projection is employed to fuse the different-scale  information as
\begin{equation}
{\bm H}={\tt Proj(Concat}({\bm X^1_l},{\bm X^2_l},{\bm X^3_l},{\bm X^1_g},{\bm X^2_g},{\bm X^3_g}))\in \mathbb{R}^{\hat{C}\times T},
\end{equation}
where the projection dimension $\hat{C}$ is set to 1536 in this work. Then, the Attentive Statistics Pooling (ASP) \cite{okabe2018attentive} is applied to weight the importance of each frame-level feature $\bm h_t \in \mathbb{R}^{\hat{C}\times 1}$ of $\bm H$ and extract the robust speaker embedding, which is given by 
%In the next, we will introduce the detail design about local branch and global branch.
\begin{align}
	e_t &= {\bm v}^T{\tt Tanh}({\bm W}{\bm h_t} +{\bm b})+k,\\
    \alpha_t &= \frac{{\tt exp}(e_t)}{\sum^T_{\tau=1}{\tt exp}(e_\tau)},
\end{align}
where $\bm W\in\mathbb{R}^{\hat{C}\times \hat{C}}, \bm v\in \mathbb{R}^{\hat{C}\times 1}, \bm b\in \mathbb{R}^{\hat{C}\times 1}$ and $k$ are the learnable parameters for ASP. After that, the normalized score $\alpha_t$ is adopted to calculate the weighted mean vector $\tilde{\bm \mu}$ and weighted standard deviation $\tilde{\bm \sigma}$, yielding
\begin{align}
	\tilde{\bm \mu}&=\sum^T_{t=1}\alpha_t {\bm h_t},\\
	\tilde{\bm \sigma}&=\sqrt{\sum^T_{t=1}\alpha_t {\bm h_t}\odot {\bm h_t}-\bm \mu\odot\bm \mu},
\end{align}
where  $\bm \mu=\frac{1}{T}\sum^T_{\tau=1}{\bm h_\tau}$. The output of the ASP is given by concatenating the vectors of the weighted mean $\tilde{\bm \mu}$ and weighted standard deviation $\tilde{\bm\sigma}$. Finally, the speaker embedding is reduced to a low dimensional vector with 1D BatchNorm and linear projection.

\iffalse
\begin{figure}[!t]
	\centering
	\hspace{-5mm}
	\includegraphics[width=\linewidth]{trans}
	\caption{Overview of the DS-TDNN architecture, where $\oplus$ denotes element-wise summation; $*$ denotes element-wise multiplication; BN denotes 1D BatchNorm; (Proj, C) is a linear projection layer with C hidden neurons; GAP is global average pooling; 3$\times$1 is a 1D convolution with kernel size of 3 and step of 1. }
	\label{fig model}
\end{figure}
\fi

\begin{table*}[t]
	\begin{center}
		%\tiny
		\caption{Configurations of the Variants of DS-TDNN and the ECAPA-TDNN Peers.}
		\label{Table 1}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{3mm}{
			\begin{tabular}{l|cccccc}
				\bottomrule
				Model     &Blocks [Local, Global] &Channels $C$  &Scales $s$  &Experts $K$ &Sparse ratio &\#Params(M)\\
				\hline
				\hline
				ECAPA-c512 & $[1,\ 0]\times 3$ & $[512,\ 0]$      & $[8,\ 8,\ 8]$  &-   &-           & 7.0\\
				DS-TDNN-S  & $[1,\ 1]\times 3$ & $[256,\ 256]$      & $[4,\ 4,\ 4]$  & $[4,\ 4,\ 8]$ & $[0.3,\ 0.1,\ 0.1]$ 
				& 6.5\\
				\hline
				\hline
				ECAPA-c1024& $[1,\ 0]\times 3$ & $[1024,\ 0]$     & $[8,\ 8,\ 8]$  &-   &-           & 15.5\\
				DS-TDNN-B  & $[1,\ 1]\times 3$ & $[512,\ 512]$     & $[4,\ 4,\ 8]$  & $[4,\ 8,\ 8]$ & $[0.3,\ 0.1,\ 0.1]$ 
				& 13.2  \\	
				\hline
				\hline
				ECAPA-L   & $[1,\ 0]\times 3$ & $[1280,\ 0]$     & $[8,\ 8,\ 8]$  &- & -          & 21.1 \\
				DS-TDNN-L  & $[1,\ 1]\times 3$ & $[768,\ 768]$   & $[4,\ 8,\ 8]$  & $[8,\ 8,\ 8]$ & $[0.4,\ 0.2,\ 0.2]$
				& 20.5 \\
				\toprule
		\end{tabular}}
	\end{center}
\end{table*}

\subsection{Micro design}
Each branch consists of three macaron-like blocks. In the blocks, two linear projections sandwich the filter applied for token mixing. The first projection is utilized to disentangle the local (global) information from the mixture, while the second projection is employed to exchange the channel information.
%1：Standard vs transformer like
%2: DW conv vs Res2Conv
\subsubsection{Local block} 
%为了保证抽取信息频段的连续性以便于信息融合，我们需要让local block也能提取部分中低频段的信息，让global block也能提取部分中频高频段的信息the local blocks are required to extract multi-scale context rather than focus on high-frequency (i.e., local) features. Therefore
To enhance the capability for local feature representation, \emph{Res2Conv} structure \cite{gao2019res2net,desplanques2020ecapa} is applied in local blocks as the token mixer. As shown in Fig. \ref{fig 2}, the Res2Conv is a combination of group convolutions. It firstly splits the input tokens $\bm X_l^i \in \mathbb{R}^{C/2\times T}$ into $s$ groups in channel dimensions:
\begin{equation}
	{\bm X_l^i} = [{\bm X_1}, {\bm X_2},...,{\bm X_s}],
\end{equation}
where ${\bm X_1}, {\bm X_2},...,{\bm X_s}\in\mathbb{R}^{C/2s\times T}$ represent the token groups. Subsequently, a 1D convolution with a kernel size of 3 and a step of 1 is applied in each group, followed by ReLU activation and 1D BatchNorm. Notably, since the local block is designed to focus on local features, the dilation rate of the convolutions is set to 1 in this work. Besides, convolution is not applied to the first group of tokens to lower the computation cost. The input of each convolution layer is the sum of the token group and the output of the previous convolution layer:
\begin{equation}
	\tilde{{\bm X}}_{i+1} = {\tt ReLU(\tt BN(\tt Conv}({\bm X_{i+1}}+\tilde{\bm X}_{i}))), i=2,...,s,
\end{equation}
where $\tilde{{\bm X}}_{i+1}$ represents the output of the $(i+1)$-th convolution layer. The output of the Res2Conv1d is a concatenation of $\tilde{{\bm X}}_{i+1}, i=1,...,s$, which provides different-scale features extracted from ${\bm X_l^i}$. Finally, the SE module \cite{hu2018squeeze} is applied at the end of each local block.

\subsubsection{Global block} 
As shown in Fig. \ref{fig 2}, the structure of global blocks is similar to that of local blocks, where a DGF layer is utilized to replace Res2Conv to capture long-range context information from long-time span. In addition, we perform a simple skip connection rather than the channel attention module used in local blocks, since the Attention FN shown in Eq.(7) has already contained discriminative information about channels.



\subsection{Architecture variants}
To evaluate our model, we developed three variants of the DS-TDNN. The first two variants, i.e., DS-TDNN-S and DS-TDNN-B, have similar hyperparameters as those of the typical ECAPA introduced in \cite{desplanques2020ecapa}. Additionally, we investigated variants of ECAPA and DS-TDNN of a larger size, i.e., ECAPA-L and DS-TDNN-L, that have similar parameters to enable a fair comparison between TDNN-based models and other baseline systems, such as transformer-based models and deep residual 2D CNNs. Table \ref{Table 1} provides a summary of the detailed configurations of these variants. As shown in Table \ref{Table 1}, due to the efficient DGF layer and the proposed parallel framework, the parameters of DS-TDNN are less than the ECAPA counterpart, which becomes more evident as the number of channels increases. By default, we applied sparse regularization to all variants of the DS-TDNN.


\section{Experimental Setup}
%前面写一段来交代实验内容


\subsection{Data Preparation}

VoxCeleb1 \& 2 \cite{nagrani2017voxceleb, chung2018voxceleb2}, and SITW \cite{mclaren2016speakers} are used in our experiments. VoxCeleb is an audio-visual dataset consisting of over 2,000 hours of short clips of human speech extracted from interview videos on YouTube. SITW is a widely-used standard evaluation dataset collected from open-source media in real-world conditions, and is made up of 299 speakers, including two testing trials (SITW.Dev and SITW.Eval) that have over 2,800 utterances from 180 speakers. All the systems are trained only on the development set of VoxCeleb2, which has over 1,092,009 utterances at a sampling rate of 16 kHz from 5,994 speakers. A small subset of about 2\% of the data is reserved as a validation set for hyperparameter optimization.

To better illustrate the advantages of global context modeling for utterances of different duration, we conducted four trials: VoxCeleb1-O (i.e., Vox1-O), VoxCeleb1-E (i.e., Vox1-E), VoxCeleb1-H (i.e., Vox1-H), and a mixture consisting of SITW.Dev and SITW.Eval (i.e., mix-SITW) for performance evaluation. Specifically, VoxCeleb1-O is the test part of VoxCeleb1, which contains 40 speakers with a total of 37,720 test pairs sampled from VoxCeleb1. VoxCeleb1-E is an extension of VoxCeleb1-O, including 1,251 speakers with a total of 581,480 test pairs. VoxCeleb1-H is the more challenging scenario, including 552,536 test pairs where the country and gender of the speakers in each pair are the same. Most of the utterances in VoxCeleb1 last for 5-8 seconds, thus Vox1-O, Vox1-E, and Vox1-H can be regarded as short-duration utterances. As for SITW, the major durations are about 30–40 seconds. Therefore, the mix-SITW is adopted to simulate the long-duration scenario. 
To further investigate how the performance changes as the duration of utterances increases, we randomly clip the test utterance of the mix-SITW with a step size of 5s, thus yielding four duration settings: $\leq 5$s, $\leq 15$s, $\leq 30$s, and $\leq 50$s, containing a total of 2500 test pairs.
Notably, the training set used in the experiments, i.e., the development set of VoxCeleb2, is completely disjoint from these four evaluation trials.

As data augmentation is generally effective for improving the performance of neural networks, we apply six augmentation strategies following the Kaldi recipe \cite{snyder2019speaker} in combination with the publicly available MUSAN dataset \{music, speech, noise\} \cite{snyder2015musan} and the RIR dataset \{reverberation\} \cite{ko2017study}. Each of the five datasets contributes equally to the augmented training dataset in an additive way, i.e.,  reverberation, speech, music, noise, and a mixture of speech and music are added to the speech corpus. The last augmentation strategy applying to all of the training samples is SpecAugment \cite{park2019specaugment}, which randomly masks 0 to 5 frames and 0 to 10 channels of the log Mel spectrogram.


\subsection{System description}

In order to comprehensively evaluate the performance of the proposed DS-TDNN, not only the TDNN-based models, i.e., \emph{ECAPA-TDNN} \cite{desplanques2020ecapa}, but also the 2D CNN-based models, i.e., \emph{SE-ResNet} \cite{chung2020defence,zhao2021speakin,shim2022graph}, and transformer-based models, i.e., \emph{Audio Spectrogram Transformer (AST)} \cite{gong2021ast} and \emph{MFA-Conformer} \cite{zhang2022mfa}, are regarded as baseline systems. The inputs for all systems are 2-second Mel spectrograms of 80 dimensions generated from a 25ms window with a 10ms frame shift, and the speaker embedding dimension is 192. For fair comparison, we tune their hyper-parameters slightly. The configurations of the baseline systems are introduced as follows:

\textbf{AST}: It is a fully attention-based model, taking the Mel spectrogram as input and producing speaker embeddings using the ASP on the averaged tokens produced by a transformer encoder. The dimension of its input is reduced from 128 to 80 in our experiments to make it consistent with other baseline systems. Two variants of AST are applied in our experiments, i.e., AST-tiny (AST-T) and AST-small (AST-S). The AST-T consists of 12 self-attention layers with 3 heads and 192 hidden channels, while the AST-S has 12 self-attention layers with 6 heads and 384 hidden channels.  

\textbf{MFA-Conformer}: It is a hierarchical attention-based model, which introduces convolution to provide the local information, achieving the state-of-the-art performance for speaker verification. It has 6 macaron-like blocks with 1/2 subsampling rates, where two feed-forward networks (FFN) sandwich the composition of multi-head self-attention (MSA) and convolution. In the following, the FFNs have 2048 hidden units, the MSA has 4 heads with 272 dimensions, and the convolution has a kernel size of 15 and a step of 1. Besides, the ASP is employed before producing the final speaker embeddings.

\textbf{SE-ResNet}: SE-ResNet have similar structure and hyper-parameters to ResNet while applying channel attention (SE module) to improve speaker verification performance. Notably, the number of basic channels of SE-ResNet is set to 32 rather than 64 in our experiments. Besides, the subsampling rate of the stem is set to 1/2 instead of the commonly used 1/4 for all variants. In addition, ASP is used in all variants to replace the statistics pooling in our experiments.

\textbf{ECAPA-TDNN} \& \textbf{DS-TDNN}: Three pairs of variants, i.e., ECAPA-c512 and DS-TDNN-S, ECAPA-c1024 and DS-TDNN-B, ECAPA-L and DS-TDNN-L, are investigated as peer work based on TDNN , whose settings are detailed in Table \ref{Table 1}.


\subsection{Training strategy}
To minimize the duration mismatch between the training and the evaluation, we employed a two-stage training process. The first stage involved pre-training for 150 epochs, followed by large margin fine-tuning (LM-FT) \cite{thienpondt2021idlab} for 5 epochs. During pre-training ($\sim 35$ hours), all systems are trained using Additive Angular Margin (AAM) loss \cite{deng2019arcface, xiang2019margin}, with the margin and scale set to 0.2 and 30.0, respectively. Adam \cite{kingma2014adam} is used as the optimizer, with an exponentially decreasing learning rate from $10^{-3}$ to $10^{-6}$. To avoid overfitting, we set the weight decay to $2\times10^{-5}$ and perform a linear warmup for the first 2k steps. During LM-FT ($\sim 1.2$ hours), we increase the duration of training samples to 6 seconds and raise the margin to 0.5. The learning rate is initialized to $10^{-4}$ and decreased to $2.5\times10^{-5}$, with a batch size of 512. All the experiments are conducted on 4$\times$NVIDIA RTX A5000.


%补上训练时间


\subsection{Backend}
Speaker embeddings are extracted from the final fully connected layer for all systems. Trial scores are produced using the cosine distance between embeddings. Subsequently, adaptive score normalization (as-norm) \cite{cumani2011comparison} is used to normalize the trial score. We average the embeddings from the same speaker in the training set to construct the imposter cohort and set the imposter cohort size to 600. Performance is measured by the equal error rate (EER) and the minimum normalized detection cost (minDCF) with $P_{target} = 0.01$ and $C_{FA} = C_{Miss} = 1$. In addition, the real-time factor (RTF) calculated by the Intel Xeon Platinum 8358P (2.60GHz) is also provided to evaluate the inference speed of different models.

\begin{table*}[!t]
	%\footnotesize
	\caption{Voxceleb EER (\%) and minDCF comparison among different models. ‘-c’ denotes the number of basic channels. The best results are marked in \textbf{BLOD}, the second are marked \underline{UNDERLINE}. Performance of our models are highlighted in \protect\sethlcolor{mygray}\hl{GRAY}.}
	\centering
	\label{Table 2}
	\renewcommand\arraystretch{1.3}
	\setlength{\tabcolsep}{2mm}{
		\begin{tabular}{ll |ccc cc cc cc }
			\bottomrule
			\multirow{2}{*}{Index} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{FLOPs (G)}& \multirow{2}{*}{\#Param (M)} & \multirow{2}{*}{RTF ($\downarrow$)}&  \multicolumn{2}{c}{Vox1-O} & \multicolumn{2}{c}{Vox1-E} & \multicolumn{2}{c}{Vox1-H}\\
			\cline{6-11}
			& & &  & & EER (\%) & minDCF & {EER (\%)} & minDCF & EER (\%) & minDCF \\
			\hline		
			\hline	
			N1  & AST-T 
			&1.1 &7.0  & 0.0071  &1.61  &0.170  &1.98  &0.208  &3.42  &0.296         \\
			N2  & SE-ResNet34      
			&1.2 &6.4  & \underline{0.0053}  &1.15  &0.149  &1.41  &0.166  &2.75  &0.253         \\		
			N3  & ECAPA-c512   
			&1.2 &7.0  & \textbf{0.0046}  &\underline{1.04}&\underline{0.133} &\underline{1.26} &\underline{0.151} &\underline{2.36} &\underline{0.224} \\
			\rowcolor{mygray}
			N4  & DS-TDNN-S    
			&1.0 &6.5  & 0.0058   &\textbf{0.90}  &\textbf{0.118} &\textbf{1.15} &\textbf{0.140} &\textbf{2.11} &\textbf{0.199}\\ 
			
			\hline
			\hline 
			N5  & SE-ResNet50  
			&1.4 &11.9 & 0.0092  &1.05          &0.124          &1.25          &0.156          &2.17          &0.206         \\	  
			N6  & SE-ResNet101   
			&2.6 &17.0 & 0.0149  &0.90          &\underline{0.107} &1.14 &0.143  &\underline{1.94}         &\underline{0.186}\\ 
			N7  & ECAPA-c1024  
			&2.9 &15.5 & \underline{0.0069}  &\underline{0.88} &0.114       &\underline{1.12}&\underline{0.135}&2.08      &0.202         \\	\rowcolor{mygray}
			N8  & DS-TDNN-B  
			&2.1 &13.2 &  \textbf{0.0066}    &\textbf{0.78} &\textbf{0.092} &\textbf{1.06} &\textbf{0.126} &\textbf{1.86} &\textbf{0.174}\\	
			\hline
			\hline
			N9  & AST-S
			&4.4  &22.5 &  0.0134  &1.08  &0.125  &1.40     &0.152  &2.38     &0.216  \\  
			N10 & MFA-Conformer  
			&2.1  &20.8 & \textbf{0.0071}  &\underline{0.70} &\underline{0.087} &\underline{0.99} &\underline{0.120} &\underline{1.64}&\underline{0.158}\\
			N11 & SE-ResNet152
			&3.8  &21.8 & 0.0258    &0.75  &0.094  &1.09     &0.128  &1.82     &0.176  \\	
			N12 & SE-ResNet34-c64 
			&4.7  &23.6 & 0.0132    &0.98  &0.122  &1.21     &0.147  &2.13     &0.196  \\		
			N13 &ECAPA-L   
			& 4.0 & 21.1& 0.0088  & 0.79 & 0.106 & 1.08 & 0.131 & 1.87 & 0.181\\	
			\rowcolor{mygray}	
			N14 & DS-TDNN-L 
			&3.2  &20.5 & \underline{0.0083}  &\textbf{0.64} &\textbf{0.082}   &\textbf{0.93} &\textbf{0.112} &\textbf{1.55} &\textbf{0.149} \\	
			\toprule
	\end{tabular}}
\end{table*}


\section{Results and Analysis}

%我们先评估了DS-TDNN在标准ASV测试中的性能

\subsection{Results on Voxceleb}
%由于FLOPs不能严格表示模型的推理速度，还与I/O访问有关。因此，我们还提供了相应的RTF结果，来直观衡量模型的计算开销与效率。
The performance comparisons of DS-TDNN and various baseline systems introduced in Section IV.B are reported in Table \ref{Table 2}, which is measured by the equal error rate (EER) and minimum Detection Cost Function (minDCF) together with the number of model parameters, floating point operations (FLOPs), and real time factor (RTF). The FLOPs and RTF are measured on 5-second utterances.

The results of experiment N1–N4 demonstrate that the proposed DS-TDNN-S system achieves the best recognition performance among popular tiny baseline systems, although its RTF is not the best. For the midium-sized systems in N5–N8, DS-TDNN-B has fastest inference speed and the best recognition performance. Specifically, compared with the typical ECAPA-TDNN system with 1024 channels, the proposed DS-TDNN-B system achieves about an 11\% improvement in recognition performance with a 15\% decline in parameters. For larger systems in N9-N14, the proposed DS-TDNN-L also outperforms in all the evaluation metrics except that its inference speed is a bit inferior to MFA-Conformer. Three groups of experiments demonstrate that as the network goes deeper, the performance of ASV could be improved. Noteworthily, it's observed from N11 and N12 that depth extension is more effective than width extension for 2D CNNs, emphasizing the importance of a larger receptive field for the ASV task. Moreover, the results in N9-N10 demonstrate that incorporating convolution in the transformer-based model could significantly enhance the recognition performance, highlighting the importance of local features. Finally, for systems of any size, the proposed DS-TDNN architecture always beats its peer models, indicating the advantage of combining local features and global context for speaker verification. 


\begin{figure*}[!t]
	\centering
	%\hspace{-5mm}
	\subfloat[]{
		\includegraphics[width=0.30\linewidth]{Improves22}
	}
	\subfloat[]{
		\includegraphics[width=0.30\linewidth]{latency22}
	}
	\subfloat[]{
		\includegraphics[width=0.30\linewidth]{memory22}
	}
	\caption{Comparisons among SE-ResNet34-c64 \cite{chung2020defence}, ECAPA-TDNN-L\cite{desplanques2020ecapa}, AST-S \cite{gong2021ast}, MFA-Conformer \cite{zhang2022mfa} and the proposed DS-TDNN-L under different settings of utterance's duration. (a) EER results (b) Latency and (c) Memory usage. The dash lines correspond to models without the special design to handle long-range context. The latency and memory usage is measured using a single NVIDIA A5000 GPU with batch size 16.}
	\label{fig 3}
\end{figure*}


\subsection{Results on SITW}


\iffalse
然后，我们又研究了面对更长时间音频时的DS-TDNN的表现（进一步改善）

此外，我们可以发现，与短时间ASV不同的情况是，某些关注全局信息的算法，如AST,在输入待测音频超过一定的长度时，效果会反超TDNN之类的经典算法。所以全局信息的参与是很必要的。且DS-TDNN由于结合了全局与局部特征，对任意长度的输入都能很好的handle
\fi

To evaluate the impact of utterance's duration, we make a comprehensive assessment on the mix-SITW dataset, using five models in Table \ref{Table 2}, including SE-ResNet34-c64, ECAPA-L, AST-S, MFA-Conformer, and the proposed DS-TDNN-L. The EER result and computational overhead are presented in Fig. \ref{fig 3}.

\iffalse
\begin{table}[t]
	\begin{center}
		%\tiny
		\caption{Performance in EER (\%) of baselines and proposed DS-TDNN under different duration settings on mix-SITW.}
		\label{Table 4}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{2.0mm}{
			\begin{tabular}{l|ccccc}
				\bottomrule
				\multirow{2}{*}{System}   & \multicolumn{4}{c}{Duration for test utterances}\\
				& $\leq5$s  & $\leq15$s  & $\leq30$s  & $\leq50$s \\
				\hline
				\hline
				SE-ResNet34-c64           &1.22{\tiny $_{\, 0\%}$}  %1.3863
				&0.83{\tiny $_{\, 32\%\downarrow}$}          
				&0.58{\tiny $_{\, 30\%\downarrow}$}          
				&0.54{\tiny $_{\, 7\%\downarrow}$}       \\
				
				ECAPA-L                   &1.01{\tiny $_{\, 0\%}$}     
				&0.63{\tiny $_{\, 41\%\downarrow}$}        
				&0.46{\tiny $_{\, 29\%\downarrow}$}        
				&0.44{\tiny $_{\, 6\%\downarrow}$}     \\
				
				\hline
				\hline
				AST-S                     &1.29{\tiny $_{\, 0\%}$}        %1.3723
				&0.72{\tiny $_{\, 44\%\downarrow}$}        
				&0.49{\tiny $_{\, 32\%\downarrow}$}        
				&0.35{\tiny $_{\, 28\%\downarrow}$}    \\
				
				MFA-Conformer             &\underline{0.91}{\tiny $_{\, 0\%}$}      
				&\underline{0.55}{\tiny $_{\, 40\%\downarrow}$}  
				& \underline{0.39}{\tiny $_{\,28\%\downarrow}$}
				& \underline{0.27}{\tiny $_{\,33\%\downarrow}$} \\	
				
				\rowcolor{mygray}	
				DS-TDNN-L                 &\textbf{0.86}{\tiny $_{\, 0\%}$}
				&\textbf{0.51}{\tiny $_{\, 41\%\downarrow}$}  
				&\textbf{0.35}{\tiny $_{\, 30\%\downarrow}$}   
				&\textbf{0.24}{\tiny $_{\, 32\%\downarrow}$}      \\	
				\toprule
		\end{tabular}}
	\end{center}
\end{table}
\fi


\begin{figure*}[!t]
	\centering
	\hspace{-4mm}
	\includegraphics[width=0.90\linewidth]{SR}
	\caption{Visualizations of dynamic filtering and sparse regularization. \emph{Left}: DS-TDNN converges to the optimum along a smooth trajectory. \emph{Right}: The distribution of maximum Hessian eigenvalue. In the polar coordinate, $r_t=\frac{||\Delta\omega_{t}||}{||\Delta\omega_{\rm init}||}$, and $\theta={\rm cos^{-1}}\left(\frac{\Delta\omega_t\cdot\Delta\omega_{\rm init}}{||\Delta\omega_t||\,||\Delta\omega_{\rm init}||}\right)$, where $\Delta\omega_t=\omega_t-\omega_{\rm optim}$.}
	\label{fig 4}
\end{figure*}

Fig. \ref{fig 3}a demonstrates that DS-TDNN performs exceptionally well in ASV task and exhibits strong generalization capabilities in complex, real-world scenarios. Given speech signals of varying durations, DS-TDNN always outperforms the other four systems. Furthermore, the DS-TDNN, along with the AST and MFA-Conformer, consistently improves in performance as the duration increases, highlighting the importance of global context in extracting robust speaker representations from longer utterances. In contrast, SE-ResNet and ECAPA-TDNN models have limited receptive fields, making it hard to capture global context information from longer speech signals. As a result, the performance of these methods is surpassed by AST when the duration of test utterances exceeds about 30 seconds. Apparently, MFA-Conformer and DS-TDNN perform well for any audio length, since they attend to both local and global features. In terms of efficiency, DS-TDNN has the lowest inference latency on the GPU, attributed to the log-linear complexity of the DGF layer. Notably, the computational cost of the AST model increases significantly due to the quadratic complexity of self-attention with regard to input lengths. To address this issue, the MFA-Conformer employs a well-designed FFN to down-sample the feature map. However, its inference speed is still slower than that of the other three baseline systems when the duration of test utterances exceeds 25 seconds, as shown in Fig. \ref{fig 3}b. Additionally, Fig. \ref{fig 3}c shows that the memory usage of DS-TDNN is comparable to that of ECAPA-TDNN and MFA-Conformer, but higher than that of SE-ResNet and lower than that of AST. Overall, DS-TDNN strikes an impressive balance between speaker verification performance and computational cost, implying its promising application in real life. 


\subsection{Analysis and visualization}

\subsubsection{Optimization analysis}

In order to investigate the effect of dynamic filtering and sparse regularization on the optimization procedure, the optimization trajectory in polar coordinates is plotted in Fig. \ref{fig 4} considering different filtering strategies, where the radius $r_t$ is defined as the normalized distance between the current weights $\omega_t$ and the optimum weights $\omega_{\rm optim}$, and the angle represents the direction of optimization. In addition, we also study the distribution of the Hessian eigenvalue of the weights in the dynamic/static GF layers, which reflects the local convexity of the loss function that indicates the training difficulty. 

From the top-left of Fig. \ref{fig 4}, the optimization trajectory of the model becomes significantly sharper after adopting dynamic filtering, and there is a detour in the optimization process, indicating that the dynamic filtering would increase the optimization difficulty. The detours diminish after taking sparse regularization as shown in the bottom-left of Fig. \ref{fig 4}. One possible explanation is that the sparse regularization reduces the variance of gradients for mini-batches and allows the model to update towards a consistent direction. This can also be verified by the distribution of the Hessian eigenvalue. Specifically, the Hessian of the loss has negative eigenvalues in early epochs, which indicates that the loss function is non-convex and the model is prone to falling into saddle points. Besides, although the Hessian eigenvalues yielded by DGF are smaller than those yielded by GF, the proposed sparse regularization significantly suppresses the negative eigenvalues, thus faciliating the training process. Interestingly, it is observed in the right part of Fig. \ref{fig 4} that sparse regularization also benefits GF.

\begin{figure*}[!t]
	\centering
	%\hspace{-5mm}
	\includegraphics[width=\linewidth]{visualGF2}
	\caption{Visualizations of filters in the global-aware branch. \emph{Left}: the amplitude-frequency response of the first 64 filters in GF/DGF layers, where the vertical axis represents the log amplitude and the horizontal axis represents the frequency. \emph{Right}: the distribution of the center frequency (C.F.) of all the filters and the count of each type of filter.}
	\label{fig 5}
\end{figure*}

\subsubsection{Analysis of intermediate filters} 

In this part, we explore how the proposed GF affects input tokens. Besides, the effectiveness of dynamic filtering and sparse regularization for enhancing the GF is illustrated from the perspective of signal processing. For the DGF, we set $\omega_1 = \omega_2 = ... = \omega_K = 1/K$.

Firstly, numerous works have proven that the effect of global context modeling, e.g., self-attention, are equivalent to low-pass filters for the input tokens, while the local modeling like convolution tends to be high-pass filters \cite{parkvision,cordonnierrelationship,siinception}. As shown in Fig. \ref{fig 5}, both GF and DGF are mainly composed by low-pass filters, demonstrating that the long-term context rather than the local features is more emphasized. Therefore, DS-TDNN has the potential to capture more robust speaker representation from long-duration utterances than conventional TDNN. Secondly, from the comparison between GF and DGF, greater number of lowpass filters is found in DGF. More low-pass filters generally indicate stronger ability in global context modeling. Notably, there are some all-pass filters in the GF/DGF. Since these filters do not influence the spectrum, we call them inactive filters (inact). The existence of these inactive filters can be accounted by incomplete optimization. Nevertheless, it is easy to observe that the number of inactive filters is significantly reduced after sparse regularization, and the overall center frequency of GF/DGF is shifted to the lower region, implying that sparse regularization is helpful in the optimization process. Finally, the last two layers have more low-pass filters than the first layer, which is consistent with the fact that deeper layers of neural networks have larger receptive fields.



\section{Ablation study}

To determine the contribution of each component in the DS-TDNN, we conduct a detailed ablation study divided into four parts. First, we compare the DGF layer with three typical global-aware designs, i.e., multi-head self-attention (MSA), long short-term memory (LSTM), and bidirectional LSTM (Bi-LSTM). Second, we first investigate the macro designs of DS-TDNN, i.e., the combination of local features and global context, element-wise summation for different-scale information exchange, and multi-scale feature aggregation (MFA). Then, we evaluate the effects of dynamic filtering and Res2Conv on verification performance, respectively. Third, we quantitatively assess the impact of sparse regularization on dynamic global-aware filters and static filters. Finally, we explore the scalability of DS-TDNN from its depth and width. Besides, we compare the proposed parallel pattern with the conventional alternating pattern for feature combinations. Notably, we only present the EER and minDCF results on VoxCeleb1-O, as shown in Tables \ref{Table 5}-\ref{Table 8}, while similar trend is observed in other datasets.

\begin{table}[t]
	%\footnotesize
	\caption{Ablation study on the global module design. }
	\centering
	\label{Table 5}
	\renewcommand\arraystretch{1.5}
	\setlength{\tabcolsep}{1.2mm}{
		\begin{tabular}{l| l l l l}
			\bottomrule
			&FLOPs(G) &  \#Params(M)  &EER(\%) &minDCF  \\
			\hline			
			\hline
			DS-TDNN-B                                     & 2.1{\tiny  $_{\,  0\%}$}     
			& 13.2{\tiny  $_{\,  0\%}$}
			& 0.78{\tiny $_{\, 0\%}$}    
			& 0.092{\tiny $_{\, 0\%}$} \\
			\hline	
			MSA$^{\dagger}$ $\rightarrow$ DGF layer       		      & 2.6{\tiny  $_{\,  \textcolor{red}{24\%\uparrow}}$}   
			& 13.7{\tiny $_{\, 4\%\uparrow}$}
			& 0.79{\tiny $_{\, 1\%\uparrow}$}    
			& 0.097{\tiny$_{\, 5\%\uparrow}$}                          \\
			LSTM$\rightarrow$ DGF layer       			  & 3.4{\tiny  $_{\,  \textcolor{red}{62\%\uparrow}}$}   
			& 17.6{\tiny $_{\, \textcolor{red}{44\%\uparrow}}$}
			& 0.84{\tiny $_{\, 8\%\uparrow}$}    
			& 0.108{\tiny$_{\, 17\%\uparrow}$}                          \\
			Bi-LSTM$\rightarrow$ DGF layer       		  & 4.9{\tiny  $_{\,  \textcolor{red}{133\%\uparrow}}$}   
			& 24.7{\tiny $_{\, \textcolor{red}{87\%\uparrow}}$}
			& 0.81{\tiny $_{\, 4\%\uparrow}$}    
			& 0.103{\tiny$_{\, 12\%\uparrow}$}                          \\
			\toprule 
	\end{tabular}} 
	\begin{tablenotes}
		\footnotesize
		\item[] $\dagger$ MSA has 4 heads.
	\end{tablenotes}
\end{table}
\begin{figure}[!t]
	\centering
	\hspace{-2mm}
	\subfloat{
		\includegraphics[width=0.48\linewidth]{abla1}
	}
	\subfloat{
		\includegraphics[width=0.48\linewidth]{abla2}
	}
	\caption{Latency and memory usage for variants of DS-TDNN in which the DGF layer is replaced by various global module designs, measured under a single NVIDIA A5000 GPU and a batch size of 16.}
	\label{fig 6}
\end{figure}

\subsection{Ablation study on the global module design} 

From Fig. \ref{fig 6}, it is seen that the DGF layer has the lowest complexity compared with the other three global module designs. In particular, the multi-head self-attention scheme is most exhaustive in terms of inferring time. Additionally, the DGF layer achieves the best results in EER and minDCF, as reported in Table \ref{Table 5}.
Notably, MSA is comparable to DGF in terms of performance, but it has an obviously higher computation cost. Hence, DGF can be an efficient alternative to MSA when computational cost is a major concern. It is also noted in the table that the LSTM can only achieve suboptimal performance for the given task, which may be accounted by the undirected property of speaker information. This problem is somewhat alleviated in Bi-LSTM by preserving bidirectional information flow.

\begin{table}[t]
	%\footnotesize
	\caption{Ablation study on the macro design. \protect\sethlcolor{mygray}\hl{GRAY} Denotes the DS-TDNN-B}
	\centering
	\label{Table 6}
	\renewcommand\arraystretch{1.5}
	\setlength{\tabcolsep}{1.2mm}{
		\begin{tabular}{l| l l l l}
			\bottomrule
			&FLOPs (G) &  \#Params (M)  &EER (\%) &minDCF  \\			
			\hline
			\hline
			Blocks $= [2, 0]\times 3$ & 2.2{\tiny  $_{\,   \textcolor{red}{35\%\downarrow}}$}   
			& 12.2{\tiny $_{\,  \textcolor{red}{21\%\downarrow}}$}
			& 0.95{\tiny $_{\, 8\%\uparrow}$}      & 0.127{\tiny $_{\, 12\%\uparrow}$}  \\
			Blocks $= [0, 2]\times 3$ & 2.1{\tiny  $_{\,  5\%\downarrow}$}   & 14.3{\tiny  $_{\,  17\%\uparrow}$}
			& 1.07{\tiny $_{\, 13\%\uparrow}$}     & 0.135{\tiny $_{\, 6\%\uparrow}$}   \\
			Blocks $= [1, 1]\times 3$ & 2.1{\tiny  $_{\,  2\%\downarrow}$}   & 13.2{\tiny  $_{\,  8\%\downarrow}$}
			& 0.84{\tiny $_{\,  \textcolor{red}{21\%\downarrow}}$}   
			& 0.098{\tiny$_{\, \textcolor{red}{27\%\downarrow}}$}                       \\
			\rowcolor{mygray}						      
			+  Info exchange          & 2.1{\tiny  $_{\,  0\%\uparrow}$}     & 13.2{\tiny  $_{\,  0\%\uparrow}$}
			& 0.78{\tiny $_{\, 7\%\downarrow}$}    & 0.092{\tiny $_{\, 6\%\downarrow}$} \\
			- MFA       			  & 1.5{\tiny  $_{\,  \textcolor{red}{29\%\downarrow}}$}   
			& 10.0{\tiny $_{\, \textcolor{red}{24\%\downarrow}}$}
			& 0.92{\tiny $_{\, 18\%\uparrow}$}    
			& 0.119{\tiny$_{\,\textcolor{red}{29\%\uparrow}}$}                          \\		
			\hline
	\end{tabular}}
\end{table}
\subsection{Ablation study on the macro design} 
%补充不使用dynamic 但增加参数量 以公平比较。
In this part, we study the impact of every macro design, including the dual-stream framework, the information exchange between the local and global branch and the feature aggregation strategy for different-scale information. The detailed results are presented in Table \ref{Table 6}. Notably, the dual-branch framework surpasses its single-branch counterparts with comparable computational overhead, while utilizing local branch shows better performance than utilizing global branch. Besides, the element-wise summation at the end of each block further reduces the EER and the minDCF without increasing complexity. Finally, the MFA proposed in ECAPA also benefits DS-TDNN, even though it obviously increases the complexity. 


\begin{table}[t]
	%\footnotesize
	\caption{Ablation Study. \protect\sethlcolor{mygray}\hl{GRAY} Denotes the DS-TDNN-B}
	\centering
	\label{Table 7}
	\renewcommand\arraystretch{1.5}
	\setlength{\tabcolsep}{1.8mm}{
		\begin{tabular}{cc| c c}
			\bottomrule
			Experts $K$  &Sparse ratio &EER (\%) &minDCF  \\
			\hline
			\hline
			$[4,\ 8,\ 8]$& w/o SR             & 0.82  & 0.098\\
			$[4,\ 8,\ 8]$&$[0.3,\ 0.3,\ 0.3]$ & 0.88  & 0.106\\
			$[4,\ 8,\ 8]$&$[0.3,\ 0.3,\ 0.1]$ & 0.83  & 0.096\\
			\rowcolor{mygray}	
			$[4,\ 8,\ 8]$&$[0.3,\ 0.1,\ 0.1]$ & 0.78  & 0.092\\
			$[4,\ 8,\ 8]$&$[0.2,\ 0.1,\ 0.1]$ & 0.79  & 0.093\\
			\hline
			\hline
			-            &w/o SR              & 0.84 & 0.105\\
			-            &$[0.3,\ 0.3,\ 0.3]$ & 0.91 & 0.117\\
			-            &$[0.3,\ 0.3,\ 0.1]$ & 0.86 & 0.109\\
			-            &$[0.3,\ 0.1,\ 0.1]$ & 0.82 & 0.102\\
			-            &$[0.2,\ 0.1,\ 0.1]$ & 0.82 & 0.104\\
			\toprule 
	\end{tabular}}
\end{table}


\subsection{Ablation study on the dynamic filters}
 In Table \ref{Table 7}, the impact of dynamic filters and sparse regularization is investigated. From the table, it is seen that the system employing dynamic filters outperforms the one using static filters. Besides, the proposed Sparse Regularization (SR)  enhances the performance of both dynamic global-aware filters and static global-aware filters. Moreover, a higher sparse ratio does not always guarantee better performance, and an excessively high sparse ratio even lead to an obvious decline in performance. The experimental results also show that the shallow layers generally require a higher sparsity ratio since inactive filters appear more frequently in shallow layers.

\begin{table}[t]
	\begin{center}
		%\tiny
		\caption{Ablation Study on the depth and width of the network. \protect\sethlcolor{mygray}\hl{GRAY} Denotes the DS-TDNN-B}
		\label{Table 8}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{1mm}{
			\begin{tabular}{l|ccccccc}
				\bottomrule
				&Layer & Channel &FLOPs (G)&\#Params (M) &EER (\%)   &minDCF \\
				\hline
				\hline
				\rowcolor{mygray}	
				Basic             & $3$   & $1024$  & 2.1    & 13.2       & 0.78     & 0.092 \\	
				\hline
				Width $\uparrow$  & $3$   & $1280$  & 2.8    & 17.1       & 0.70     & 0.088 \\
				Width $\uparrow$  & $3$   & $1536$  & 3.2    & 20.5       & 0.64     & 0.082 \\
				\hline
				Depth $\uparrow$ & $4$   & $1024$  & 2.9    & 17.0       & 0.74     & 0.090 \\
				Depth $\uparrow$ & $5$   & $1024$  & 3.2    & 20.8       & 0.72     & 0.087 \\
				
				\toprule
		\end{tabular}}
	\end{center}
\end{table}
\subsection{Ablation study on the depth and width of the network}
Finally, to reveal the effect of the network's depth and width, various scaling factors are considered in the experiments. Table \ref{Table 8} reveals that DS-TDNN has excellent scalability in both width and depth, while increasing the width of the model is more effective than increasing the depth. Although this seems to be contrary to the conclusion of the recent work \cite{liu2023depth}, it is still reasonable because the DGF layer applied in DS-TDNN is able to provide the global context in the shallow layers, making it unnecessary to enlarge the receptive field by stacking more hidden layers in depth.  


\iffalse
提出的GF层不光可以用于TDNN,仅需稍加改动也可适用于如ResNet等基于2D CNN的模型。此外，除了FFT,其他的一些用于聚合信息的快速算法如DCT,DWT等应该也可以实现类似的效果，不过何种算法最优，本工作并未充分研究。

不过他也有一些局限，在小模型上优势并不明显，且计算不够友好。此外，虽然动态滤波显著提升了模型性能，但它导致的优化困难并未被完全解决。即使使用稀疏滤波，某些情况下模型仍然会陷入局部极小值，影响其效果。而何时模型会陷入最小值，还没有被充分地调查。
\fi


\section{Conclusion}

In this paper, we propose a novel global-aware filter (GF) layer to capture long-term context in utterances. The GF layer has global receptive fields while maintaining log-linear complexity. Additionally, we propose dynamic filtering and sparse regularization to enhance the GF layer, which improves its representation and generalization ability. Thereafter, a Dual-Stream Time-Delay Neural Network (DS-TDNN) is built by incorporating the GF layer. The DS-TDNN disentangles the local features and global context, then refines them in a proposed parallel framework with several carefully designed strategies. Experiments on the Voxceleb datasets demonstrate that DS-TDNN achieves a 10\% improvement in EER but with a 28\% and 15\% decline in complexity over ECAPA-TDNN for ASV task using short utterances. Moreover, it outperforms SE-ResNet, AST, and MFA-Conformer with an approximate parameter overhead. Experiments on the SITW datasets reveal that the explicit modeling for global context in DS-TDNN further boosts its performance over ECAPA-DTNN and SE-ResNet when the utterance's duration increases. For extremely long utterances (over 50 seconds), DS-TDNN offers the best trade-off between performance and computational cost, highlighting the advantages of both the GF layer and several designs used in DS-TDNN. This study fully explores the potential of TDNN in speaker verification from the point of both local and global modeling. It might provide some new insights for future network designs of deep speaker embedding or related fields.




\iffalse
\section*{Acknowledgments}
This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.
\fi

\iffalse
\section{Biography Section}
If you have an EPS/PDF photo (graphicx package needed), extra braces are
 needed around the contents of the optional argument to biography to prevent
 the LaTeX parser from getting confused when it sees the complicated
 $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
 your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
 simpler here.)
 
\vspace{11pt}

\bf{If you include a photo:}\vspace{-33pt}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
Use the author name as the 3rd argument followed by the biography text.
\end{IEEEbiography}

\vspace{11pt}

\bf{If you will not include a photo:}\vspace{-33pt}
\begin{IEEEbiographynophoto}{John Doe}
Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
\end{IEEEbiographynophoto}
\fi



%\vfill
\iffalse
{\appendix
A. Proof of Discrete Convolution Theorem

Given a single-channel input tokens $\bm x\in \mathbb{R}^{1\times N}$, together with a spatial-domain filter with a global receptive field $\bm w_g \in \mathbb{R}^{1\times N}$, the convolution between $x$ and $w_g$ can be defined as follows:
\begin{equation}
	\tilde{x}[m] =\sum_{n=0}^{N-1} x[n] w_g[n] \Omega_N^n:= {\bm w_g}*{\bm x},
\end{equation}
where $\tilde{\bm x} \in\mathbb{R}^{1\times N}$ is the feature map of the convolution layer, and $\Omega_N^n$ is a rectangular window of width $N$ focusing at $x[n]$.
\iffalse
, which can be formulated as follows (zero paddings):
\begin{equation}
	w_g[n]\Omega_N^n=
	\left\{
	\begin{aligned}
		&w_g[n-\lceil N/2\rceil],\quad n-\lceil N/2\rceil\geq 0 \\ 
		&0,\quad n-\lceil N/2\rceil<0
	\end{aligned}
	\right.
\end{equation}
\fi

Given a $N$-point sequence $\bm x \in \mathbb{R}^{1\times N}$, the discrete Fourier transform (DFT) can be formulated as follows: 
\begin{equation}
	X[k] =\sum_{n=0}^{N-1}x[n]W_N^{kn}:=\mathcal{F}[x],
\end{equation}
%从式子中我们可以看出，傅里叶变换后的一个频率点x[k]就携带了全局的空间域信息，因此，可以通过对一个点进行操作来建模全局的空间域信息。
where $W_N^{kn}:=e^{-j(2\pi/N)kn}$, and $X[k]$ is the associated spectrum $\bm X \in\mathbb{R}^{1\times N}$ at frequency $2\pi k/T$. Similarly, the inverse discrete Fourier transform (IDFT) can be formulated as follows: 
\begin{equation}
	x[n] =\frac{1}{N}\sum_{k=0}^{N-1}X[k]W_N^{-kn}:=\mathcal{F}^{-1}[x].
\end{equation}
Thus, given a sptial-domain filter $\bm w_g$, the associated transform-domain filter $\bm w_f$ can be calculated as follows:
\begin{equation}
	{\bm w_f}:=\mathcal{F}[\bm w_g] =\sum_{n=0}^{N-1}w_g[n]W_N^{kn}
\end{equation}
Therefore, the proposed global-aware filter layer can be formulated as follows:
\begin{equation}
	\begin{aligned}
	\mathcal{F}^{-1}[\bm w_f\mathcal{F}[\bm x]]&=\frac{1}{N}\sum_{k=0}^{N-1}\sum_{n=0}^{N-1}x[n]w_g[n]W_N^{2kn}W_N^{-kn}\\
	                                   %&=\frac{1}{N}\sum_{k=0}^{N-1}\sum_{n=0}^{N-1}x[n]w_g[n]W_N^{kn}\\
                                	   &=\sum_{n=0}^{N-1}x[n]w_g[n]\hat{W}_N^n\equiv {\bm w_g}*{\bm x}
	\end{aligned}
\end{equation}
where $\hat{W}_N^n=\frac{1}{N}\sum_{k=0}^{N-1}W_N^{kn}$. %which is a $N$ length windows function that focus on $n$.
Eq.(26) can be viewed as a global-receptive convolution with a special window function $\hat{W}_N^n$.
%where F is the DFT
%因此，Equ 28可以被看作使用了一种特殊窗函数W_N的全局卷积。

%B. Calculation of complexity
	
	
}
\fi

\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}


