\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{color, xcolor} % 颜色包，color 必须导入，xcolor 建议导入
\usepackage{bm}
\usepackage{soul} % 导入 soul 包
\usepackage{balance} 
\definecolor{mygray}{gray}{.93}
% updated with editorial comments 8/9/2021
%\usepackage{subfig}
\usepackage{xcolor}
%\usepackage{svg}

\begin{document}

\title{Dual-stream Time-Delay Neural Network \\with Dynamic Global Filter for Speaker Verification}

\author{Yangfu Li, Xiaodan Lin$^*$ and \\ School of Information Science and Engineering, Huaqiao University
        % <-this % stops a space
\thanks{* Corresponding Author.}% <-this % stops a space
\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}g
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle
%全文改写表达：receptive field, global context; 核心motivation 在浅层网络获取全局上下文。
\begin{abstract}

The time-delay neural network (TDNN) is one of the state-of-the-art models for text-independent speaker verification. However, it is difficult for conventional TDNN to capture global context that has been proven critical for robust speaker representations and long-duration speaker verification in many recent works. Besides, the common solutions, e.g., self-attention, have quadratic complexity for input tokens, which makes them computationally unaffordable when applied to the feature maps with large sizes in TDNN. To address these issues, we propose the \emph{Global Filter} for TDNN, which applies log-linear complexity FFT/IFFT and a set of differentiable frequency-domain filters to efficiently model the long-term dependencies in speech. Besides, a dynamic filtering strategy, and a sparse regularization method are specially designed to enhance the performance of the global filter and prevent it from overfitting. Furthermore, we construct a dual-stream TDNN (DS-TDNN), which splits the basic channels for complexity reduction and employs the global filter to increase recognition performance. Experiments on Voxceleb and SITW databases show that the DS-TDNN achieves approximate 10\% improvement with a decline over 28\% and 15\% in complexity and parameters compared with the ECAPA-TDNN. Besides, it has the best trade-off between efficiency and effectiveness compared with other popular baseline systems when facing long-duration speech. Finally, visualizations and a detailed ablation study further reveal the advantages of the DS-TDNN.

\end{abstract}

\begin{IEEEkeywords}
Time-delay neural network, dual-stream network, text-independent speaker verification, global context.
\end{IEEEkeywords} 
  

\section{Introduction}

\IEEEPARstart{S}{PEAKER} Verification (SV) aims to determine whether a given utterance is from a claimed enrolled speaker. It can be widely used in frictionless user authentication, and multimedia forensics \cite{rosenberg1976automatic,broun2002automatic,becker2008forensic}. According to different application scenarios, SV can be categorized as text-dependent speaker verification \cite{heigold2016end} and text-independent speaker verification. For text-dependent speaker verification \cite{bimbot2004tutorial}, the spoken content of the test utterance and the enrollment utterance should be the same, whereas there is no constraint on the spoken content in the text-independent speaker verification system.

Over the past two decades, the dominant SV systems have changed a lot. In 2000, the Gaussian mixture model universal background model (GMM-UBM) \cite{reynolds2000speaker} is proposed for speaker verification in limited enrollment speaker data, which trains a UBM for all hypothesized speakers and derives each enrollment speaker model through UBM adaptation. Following, numerous SV systems based on the GMM-UBM are produced, such as GMM Super-Variable Support Quantitative Machine (GSV-SVM) \cite{campbell2006support}, Adverse Attribute Projection (NAP) \cite{campbell2006svm}, Joint Factor Analysis (JFA) \cite{kenny2007joint}, and i-vector system \cite{dehak2010front}, all of which achieve good performance. With the development of machine learning, the architecture of SV systems has undergone a transformation from human-designed subsystems to end-to-end frameworks. In 2014, Lei et al. propose DNN-i-vector system \cite{lei2014novel} inspired by the i-vector. During the training of the UBM and GMM, DNN-i-vector employs an automatic speech recognition DNN to do frame alignment, which achieves better performance than i-vector system. Then, the d-vector system is proposed \cite{variani2014deep}, where the speaker embedding, termed d-vector, is extracted from the last hidden layer of a DNN trained to classify speakers at frame-level. In 2017, Snyder et al. proposes x-vector system \cite{snyder2017deep} based on Time-Delay Neural Network (TDNN) \cite{waibel1989phoneme}. Different from classifying speakers at the frame level, x-vector applies a pooling layer to transform the frame-level data into segment-level data, yielding a more robust speaker representation. To enhance the performance of x-vector from two different aspects, factorized TDNN (F-TDNN) \cite{povey2018semi} and extended TDNN (E-TDNN) \cite{snyder2019speaker} are separately proposed. F-TDNN factorizes the parameter matrices of the TDNN into smaller matrices to make the training more efficient. E-TDNN extends the depth and width of the TDNN structure and captures more context information. Combining F-TDNN and E-TDNN, the work in \cite{villalba2020state} obtains the best performance in SRE18 and speakers in the wild (SITW) evaluations. Thereafter, TDNN becomes the state-of-the-art model for end-to-end speaker verification.

Although TDNN has made significant achievements, it is gradually surpassed in terms of recognition performance by two-dimension convolutional neural networks (2D CNNs) with residual connections and attention-based models. One reason is the insufficient capability for global context modeling in TDNN, which is important for robust speaker representation. Typically, TDNN employs the time-delay strategy in every hidden layer to capture context information between different frames, and stacks several layers to further enlarge the receptive field. However, feature maps in TDNN generally have high resolutions, making TDNN shallow. Besides, for efficiency, the time delay applied in TDNN is usually short, limiting the regions of the receptive field. To strengthen the ability for long-term context modeling, many techniques have been proposed, which can be divided into two categories.

The first solution is to extend TDNN in the depth \cite{garcia2016stacked,huang2019deeper,jiang2019effective} like what 2D CNNs do, or introduce more advanced filters into TDNN \cite{kreyssig2018improved,an2021deformable,desplanques2020ecapa}. Specially, \cite{garcia2016stacked} stacks more hidden layers than conventional TDNN for context modeling over a long time span, enhancing the performance. To prevent over-fitting, \cite{huang2019deeper} introduces dropout to TDNN. Besides, it also employs carefully designed hidden layers with different temporal resolutions for more powerful context representation. Later, \cite{jiang2019effective} utilizes the dilated convolution to enlarge the  receptive field and inserts dense connections between each hidden layer to emphasize the context information, significantly improving the performance of TDNN. From another aspect, \cite{kreyssig2018improved} applies the filters having a longer time delay, i.e., the deep kernels, to capture more context information, thus increasing the modeling capability. Inspired by deformable convolution, \cite{an2021deformable} designs deformable filters to provide the adaptive temporal context information for TDNN. \cite{desplanques2020ecapa} employs multi-scale dilated group filters to introduce multi-scale temporal context information into TDNN, utilizes the channel attention to refine the frequency information, and designs a multi-layer feature aggreagation for the fusion of local and global features, achieving state-of-the-art performance.

The second solution is to combine conventional TDNN with some modern algorithms that are good at modeling long-term dependencies, such as Long-Short Term Memory (LSTM) Network \cite{chen2019speaker,miao2019new}, Recurrent Neural Network (RNN) \cite{liu2019time}, and self-attention \cite{povey2018time,han2019multi}. To be specific, \cite{chen2019speaker} improves the performance of TDNN via LSTM, which can strengthen feature extraction and capture longer temporal dependencies. For further improvement, \cite{miao2019new} introduces convolution into the combination of TDNN and LSTM and proposes a time-frequency attention mechanism. Instead of stacking LSTM with TDNN, \cite{liu2019time} adds recurrent connections between the hidden layers for directly extending the context modeling capability of TDNN, which is more efficient and easier to train. Due to the success of self-attention in computer vision, \cite{povey2018time} introduces self-attention into TDNN and replaces LSTM with self-attention in TDNN+LSTM architectures, which achieves obviously better performance and has a faster inference speed. In \cite{han2019multi}, a unique multi-stride attention is designed for the combination of TDNN, further improving the performance.

Even though these two solutions have greatly enhanced the recognition performance of TDNN, the TDNN-based model is still slightly inferior to the state-of-the-art speaker verification model. Furthermore, all the improvements obviously increase the complexity of TDNN, limiting the prospect of applications. In this paper, we propose an efficient algorithm termed \emph{Global filter} for TDNN to capture global context information. It applies log-linear complexity FFT/IFFT and a set of differentiable filters in the frequency domain to model the long-term dependencies, which has a significantly lower complexity than popular algorithms applied in TDNN, especially when facing long-duration speech. In addition, a dynamic filtering strategy is proposed to endow the global filter with dynamic characteristics. With the dynamic filtering, the global filter can be adapted to the inputs, which strengthens the representation capability. In addition, a sparse regularization method is proposed to make the dynamic global filter easier to optimize and prevent it from over-fitting.
Furthermore, inspired by recent works that combine local and global features for robust speaker representation and better recognition performance for long-duration speech \cite{zhang2022mfa,han2022local,han2022mlp}, we construct a dual-stream TDNN, termed \emph{DS-TDNN}, which splits the basic channels of TDNN and employs two independent branches for local and global feature modeling, respectively. In the global branch, the proposed dynamic global filter is employed to capture long-term context information. At the end of each hidden layer, an element-wise summation is applied for information exchange between local and global features. The experiments on Voxceleb and SITW datasets show that the DS-TDNN outperforms not only the TDNN-based baseline but also other popular baseline systems, such as 2D CNN with residual connections and the transformer-based model. Besides, the DS-TDNN has a fast inference speed when facing speeches longer than 50 seconds.
The models and code are released \footnote[1]{\tt https://github.com/YChenL/DS-TDNN} to facilitate future research. 

Our contributions are mainly included as follows:
\begin{itemize} 
	\item We propose \emph{Global Filter} for TDNN. It has global receptive fields yet a log-linear complexity to the input, which is a very competitive alternative to popular algorithms in speaker verification, e.g., self-attention, for capturing temporal context information from a long-time span.

	\item \emph{Dynamic filtering} and \emph{Sparse regularization} are specially designed for the global filter to enhance its performance. Dynamic filtering makes global filter adapt to the input tokens, increasing the representation ability. Sparse regularization reduces the optimization difficulty of dynamic global filter and prevents it from over-fitting.
	
	\item Based on the global filter, we design a novel dual-stream time-delay neural network termed \emph{DS-TDNN}, which applies channel splitting to reduce the complexity and parameters of TDNN and utilizes the global filter to capture the global context. Detailed experiments reveal that the DS-TDNN achieves state-of-the-art results on speaker verification when compared to popular baseline systems, and it also has the best trade-off between efficiency and effectiveness when facing long-duration speech.
	
\end{itemize}

The rest of this paper is organized as follows: Section II introduces the proposed dynamic global filter, including its motivation, proposal, and realizations. Section III describes the proposed dual-stream TDNN. The experimental setup is described in Section IV. The result analysis is presented in Section V, followed by a conclusion in Section VI.

\section{Dynamic Global filter}

\subsection{Discrete Fourier transform}

We start by introducing the Discrete Fourier transform (DFT), which plays an important role in digital signal processing and is also a crucial component in the global filter. Given a sequence of $N$ complex numbers $x[n], 0\leq n\leq N-1$, the DFT converts the sequence into the frequency domain by:
\begin{equation}
	X[k]=\sum_{n=0}^{N-1}x[n]e^{-j(2\pi/N)kn},
\end{equation}
where $j$ is the imaginary unit. $X[k]$ represents the spectrum of the sequence $x[n]$ at the frequency $\omega_k = 2\pi k/N$. Limitary-duration sequences can be considered as the principal value of their periodic extensions, and the formulation of DFT in Equ 1 can be derived from the Discrete Fourier Series (DFS) of the periodic-extended sequences. Since the spectrum $X[k]$ repeats on intervals of $N$, DFT takes the value of $X[k]$ at $N$ consecutive points $k=0,1,...,N-1$.

Notably, DFT is a one-to-one transformation. Given the spectrum $X[k]$ calculated via DFT, we can recover the original signal $x[n]$ by the inverse DFT (IDFT):
\begin{equation}
	x[n]=\frac{1}{N}\sum_{k=0}^{N-1}X[k]e^{j(2\pi/N)kn}.
\end{equation}
DFT has two important characteristics: Firstly, the DFT for a real signal $x[n]$ is conjugate symmetric, i.e., $X[N-k] = X^*[k]$. In the same way, if we perform IDFT to $X[k]$ that is conjugate symmetric, a real discrete signal can be recovered, which reveals that the half of the DFT $\{X[k]:0\leq k\leq \lceil N/2\rceil \}$ contains the full information about the frequency characteristics of $x[n]$. Secondarily, there exist efficient algorithms for computing the DFT. The fast Fourier transform (FFT) algorithms take advantage of the symmetry and periodicity properties of $e^{-j(2\pi/N)kn}$ and reduce the complexity to compute DFT from $\mathcal{O}(N^2)$ to $\mathcal{O}(N{\rm log}N)$. The inverse DFT (Equ 2), which has a similar form to the DFT, can also be computed efficiently using the inverse fast Fourier transform (IFFT). Based on the above, we proposed the global filter.

\subsection{Global filter}

A typical limitation of TDNN is the receptive field. Conventional TDNNs often employ dilation convolutions \cite{jiang2019effective,koluguri2020speakernet,koluguri2020speakernet}, or multi-scale group convolutions \cite{desplanques2020ecapa,liu2022mfa} to mix tokens, while attention-based models usually calculate self-attention on the whole feature maps \cite{mary2021s,wang2022multi}. For 2D CNN, a natural idea to expand the receptive field is to increase the kernel size \cite{ding2022scaling,liu2022more} or the depth of the model, but this will lead to an unaffordable cost of parameters and complexity for TDNN since 2D CNN usually has much smaller feature maps than TDNN. Inspired by the successful applications of DFT in computer vision \cite{li2020falcon,chi2020fast,lifourier}, we propose a simple yet efficient global filter layer for TDNN to capture the global context.

Specifically, weights are shared at the frame level in TDNN. Therefore, the tokens $\bm X$ in TDNN can be considered as a combination of a series of discrete sequences $\bm x_i \in \mathbb{R}^{1\times T}$ in channel dimensions, which can be formulated as follows:
\begin{equation}
	{\bm X} = {\tt Concat}(\bm x_1, \bm x_2, ... \bm x_C) \in \mathbb{R}^{C\times T}
\end{equation}
where $C$ and $T$ are the number of channels and frames, respectively. Therefore, given a token ${\bm X} \in \mathbb{R}^{C\times T}$, the corresponding spectrum $\bm X_f$ can be obtained by channel-wise 1D FFT along the timing dimension:
\begin{equation}
	{\bm X_f}=\mathcal{F}[{\bm X}]\in \mathbb{C}^{C\times T},
\end{equation}
where $\mathcal{F}[\,{\bm \cdot}\,]$ denotes the channel-wise 1D FFT. 
Since $\bm X$ is a real tensor, the spectrum of $\bm X$ is conjugate symmetric (see Section II.A), i.e., ${\bm X_f}[: , T-\tau]={\bm X_f^*}[: , \tau]$. Therefore, we can take only the half of the spectrum $\bm X_f$ yet preserving the full information to further reduce the computation complexity:
\begin{equation}
	{\bm X_r}={\bm X_f}[:, 0:\lceil T/2\rceil]:= \mathcal{F}_r[{\bm x}] \in \mathbb{C}^{C\times \lceil T/2\rceil}
\end{equation}
where $\mathcal{F}_r[\,{\bm \cdot}\,]$ denotes the channel-wise 1D FFT for real signals. 
It is noteworthily that $\bm X_r$ is a complex tensor and represents the half of spectrum $\bm X_f$. Then, we can modulate the spectrum by multiplying a learnable filter $\bm F \in \mathbb{C}^{C\times \lceil T/2\rceil}$:
\begin{equation}
	\tilde{{\bm X_r}} = {\bm F} \odot {\bm X_r} 
\end{equation}
where $\odot$ is the Hadamard product. The filter $\bm F$ is termed the \emph{global filter}, which can simulate any type of frequency-domian filter. Finally, a channel-wise 1D IFFT for real signals is adopted to transform the modulated spectrum $\tilde{\bm X_r}$ back to the spatial domain and update the tokens: 
\begin{equation}
	{\bm X} \leftarrow \mathcal{F}^{-1}_r[{\tilde{\bm X_r}}] 
\end{equation}
According to the \emph{Convolution Property} of DFT, the global filter is equivalent to the depthwise \emph{global circular convolution} in the spatial domain, which makes it possible to capture the context from an arbitrary time span.

\begin{algorithm}[t]
	\caption{Pseudocode of Dynamic Global Filter (DGF).}
	\begin{algorithmic}
		\STATE 
		\STATE \# {\tt $\tt x$: the token features,(B,C,T) }
		\STATE \# {\tt $\tt {attn\_fn}$: Attention FN}
		\STATE \# {\tt $\tt{mm}$: Matrix multiplication}
		%\STATE \# {\tt pre\_norm, post\_norm: BatchNorm1d(C)}
		\STATE \# {\tt $\tt{rfft, irfft}$: FFT/IFFT for real signals}
		\STATE \# {\tt $\tt{F}$: a set of global filters,(K,CT//2+1)}
		\STATE 
		\STATE ${\tt {\bf def}\ DGF()}:$
		%\STATE \hspace{5mm}$\mathtt{x} = {\tt pre\_norm}(\mathtt{x})$
		\STATE \hspace{5mm}$\mathtt{W} = {\tt attn\_fn}(\mathtt{x})$\ \#{\tt (B,K)}
		\STATE \hspace{5mm}$\mathtt{F_d} = {\tt mm}(\mathtt{W}, \mathtt{F}).{\tt reshape}({\tt B, C, -1})$
		\STATE \hspace{5mm}$\mathtt{x}_{\tt f} = {\tt rfft}{\tt(\mathtt{x},  dim=\mbox{-}1)}$
		\STATE \hspace{5mm}$\mathtt{x}_{\tt f} = \mathtt{x}_{\tt f}\, \tt{*}\, \mathtt{F_d}$
		\STATE \hspace{5mm}$\mathtt{x} = {\tt irfft}(\mathtt{x}_{\tt f}{\tt,  dim=\mbox{-}1)}$
		%\STATE \hspace{5mm}$\mathtt{x} = {\tt post\_norm}(\mathtt{x})$
		\STATE \hspace{5mm}${\tt {\bf return}}\ \mathtt{x}$
	\end{algorithmic}
	\label{alg 1}
\end{algorithm}

\subsection{Dynamic filtering}

Although the global filter can simulate an arbitrary filter in frequency domain, its performance is still not good enough. The reason is that the speaker trait is usually intermingled with other factors in a speech, such as the content, emotion, and background noise, while a single static filter is unable to handle the differences between various speeches. 
Inspired by \emph{Dynamic Convolution} \cite{yang2019condconv,chen2020dynamic,li2022omni}, we propose a dynamic filtering strategy to endow the global filter with the dynamic characteristic of self-attention. Specifically, we firstly apply $K$ independent global filters to replace the single filter during modulation, and combine them by element-wise summation:
\begin{equation}
	\tilde{{\bm X_r}}= {\bm F_1}\odot {\bm X_r} + {\bm F_2}\odot {\bm X_r}+...+{\bm F_K}\odot {\bm X_r}\\
\end{equation}
Then, we utilize a 1D channel attention module to produce a series of dynamic scores $\bm w=[w_1, w_2,...,w_K]$, which can be adapted to the input tokens $\bm X$. The attention module can be formulated as follows:
\begin{equation}
	{\bm w} = \underbrace{{\tt Softmax(FC_2(ReLU(FC_1(GAP}}_{\rm Attention\ FN}({\bm X}))))) \in\mathbb{R}^{1\times K},
\end{equation}
where ${\tt GAP}$ is Global Average Pooling. ${\tt FC_1}$ and ${\tt FC_2}$ separately represent two fully connected layers. In this work, the number of neurons in $\tt FC_1$ is equal to that in $\tt FC_2$, which is set to $K$. Notably, we utilize Softmax rather than Sigmoid to normalize the scores for stable training. Subsequently, we utilize the dynamic scores $\bm w$ to weight the combination of expert filters, which can be formulated as follows:
\begin{equation}
	\tilde{{\bm X_r}}=w_1{\bm F_1}\odot {\bm X_r}+ w_2{\bm F_2}\odot {\bm X_r}+...+ w_K{\bm F_K}\odot {\bm X_r}\\
\end{equation}
Equ 10 can significantly enhance the representation of the global filter as well as its complexity. Therefore, we apply another equivalent deformation of Equ 9 in practice, which is formulated as follows:
\begin{equation}
	\tilde{{\bm X_r}}=(w_1{\bm F_1}+ w_2{\bm F_2}+...+ w_K{\bm F_K})\odot {\bm X_r}\\
\end{equation}
The normalized linear combination of filters is defined as the \emph{dynamic global fiter}:
\begin{equation}
	{\bm F_d}:= w_1{\bm F_1}+ w_2{\bm F_2}+...+w_K{\bm F_K}\in \mathbb{C}^{K\times C\times\lceil T/2\rceil}
\end{equation}
Finally, the spectrum $\bm X_r$ is modulated by the element-product with the dynamic global filter $\bm F_d$ as introduced in Section 3.B. Dynamic global filter is hardware friendly and can be easily achieved as algorithm 1. 

\begin{algorithm}[t]
	\caption{Pseudocode of DGF with Sparse Regularization.}
	\begin{algorithmic}
		\STATE 
		\STATE \# {\tt $\tt x$: the token features,(B,C,T) }
		\STATE \# {\tt $\tt ratio$: dropout percentage of filters}
		\STATE \# {\tt $\tt F_d$: dynamic global filter,(B,C,T//2+1)}
		%\STATE \# {\tt $\tt lambda$: scale factor}
		\STATE
		\STATE ${\tt {\bf def}\ Sparse()}:$
		\STATE \hspace{5mm}${\tt mask = (rand(\mathtt{F_d}.shape[:-1])>ratio).float()}$
		\STATE \hspace{5mm}${\tt mask = mask.unsqueeze(-1).expand(\mathtt{F_d}.shape)}$
		\STATE \hspace{5mm}${\tt {\bf return}}\ {\tt mask * \mathtt{F_d}},\, {\tt mask*F_d.abs().mean()}$
		\STATE 
		\STATE ${\tt {\bf def}\ Sparse DGF()}:$
		%\STATE \hspace{5mm}$\mathtt{x} = {\tt pre\_norm}(\mathtt{x})$
		%\STATE \hspace{5mm}$\mathtt{W} = {\tt attn\_fn}(\mathtt{x})$\ \#{\tt (B,K)}
		\STATE \hspace{5mm}$\mathtt{F_s,\,m} = {\tt Sparse}()$
		\STATE \hspace{5mm}$\mathtt{x}_{\tt f} = {\tt rfft}{\tt(\mathtt{x},  dim=\mbox{-}1)}$
		\STATE \hspace{5mm}$\mathtt{x}_{\tt f} = \mathtt{x}\tt{*}\, \mathtt{F_s}+\mathtt{x}_{\tt f}*{\tt (1-m)}$
		\STATE \hspace{5mm}$\mathtt{x} = {\tt irfft}(\mathtt{x}_{\tt f}{\tt,  dim=\mbox{-}1)}$
		%\STATE \hspace{5mm}$\mathtt{x} = {\tt post\_norm}(\mathtt{x})$
		\STATE \hspace{5mm}${\tt {\bf return}}\ \mathtt{x}$
	\end{algorithmic}
	\label{alg 2}
\end{algorithm}

\subsection{Sparse Regularization}

Despite the fact that the dynamic filtering strategy produces a more powerful representation, it will cause another problem. Conceptually, it leads the loss function to be more non-convex, which makes it easier for the model to fall into the local minimum, increasing the difficulty for optimization. In other terms, the dynamic filtering strategy obviously increases the number of filters, which makes the model easier to overfit, affecting the performance and generalization ability. Inspired by \emph{Dropout} \cite{srivastava2014dropout,laptev2021dynamic}, we propose a sparse regularization method to address these problems. Specifically, the dynamic global filter $\bm {F}_d$ can be viewed as a series of 1D filters stacked in the channel dimension, i.e., ${\bm F_d}=[\bm f_1, \bm f_2, ..., \bm f_d]$, where $\bm f_1,...,\bm f_d \in \mathbb{C}^{1\times \lceil T/2\rceil}$. During training phase, parts of the 1D filters will be deactivated in the forward pass, i.e., ${\bm F_s}=\bm M\odot {\bm F_d}$, where $\bm M$ is a random mask sparse in channel dimension. Subsequently, an element-wise multiplication is applied to modulate the spectrum $\bm X_r$. Notably, to prevent the loss of spectral information, we perform another element-wise summation with $\bm X_r$ with the signals at the corresponding position of deactivated filters. The dynamic global filter with sparse regularization can be formulated as follows:
\begin{equation}
	\tilde{\bm X}_r = {\bm F_s}\odot{\bm X_r}+\lambda_s(1-\bm M)\odot{\bm X_r}
\end{equation}
where $\lambda_s=\frac{1}{CT}\sum^C_{i=0}\sum^T_{j=0}|\bm F_{d}^{(i,j)}|$, which is a scale factor. 
The deactivated filters can be viewed as special all-pass filters. Besides, all filters will be activated for evaluation. Sparse regularization prevents the model from learning a short-cut or over-fitting, increasing the performance. The sparse regularization can be easily implemented, as shown in algorithm 2. 

\subsection{Comparision to other algorithms}

To intuitively illustrate the efficiency of the dynamic global filter (Dynamic GF), we perform a comparison of complexity and parameters between it and other popular algorithms used in acoustic models, such as 
%1D pointwise convolution (PW Conv1d), 
1D depthwise convolution (DW Conv1d) \cite{zhang2019depthwise,koluguri2020speakernet,koluguri2022titanet}, Res2Conv \cite{desplanques2020ecapa,zhou2021resnext}, Self-Attention \cite{povey2018time,han2019multi}, and spatial MLP \cite{han2022mlp,zheng2023msranet}.
\begin{table}[t]
	\begin{center}
		%\tiny
		\caption{Comparisons of the Dynamic Global Filter with Prevalent Operations in Deep Acoustic Models.}
		\label{Table 1}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{1,5mm}{
			\begin{tabular}{lcc}
				\bottomrule
				& Complexity (FLOPs) & \# Params \\
				\hline
				DW Conv1d  & $\mathcal{O}(kLC)$   & $Ck$\\
				%PW Conv1d  & $\mathcal{O}(LC^2)$  & $C^2$\\
				Res2Conv1d & $\mathcal{O}(((s-1)/s^2)kLC^2)$ & $((s-1)/s^2)C^2k$ \\
				\hline
				Self-Attention & $\mathcal{O}(4LC^2+2L^2C)$ & $4C^2$\\
				Spatial MLP & $\mathcal{O}(L^2C)$ & $L^2$ \\
				\iffalse
				\rowcolor{mygray}	
				GF & $\mathcal{O}(L{\rm log_2}(L)C+LC/2)$ & $LC/2$ \\
				\fi
				\rowcolor{mygray}	
				Dynamic GF & $\mathcal{O}(L{\rm log_2}(L)C+(1+K)LC)$ & $KLC/2$ \\
				\toprule
		\end{tabular}}
	\end{center}
\end{table}

The comparison results are shown in Table 1 (see Appendix A for details), where $L$ and $C$ are the frames and the number of channels of the tokens $\bm x$. $k$ is the kernel size of the convolution. $s$ is the number of groups in the Res2Conv1d. $K$ is the number of experts in the dynamic global filter. It is easy to find that although the dynamic filtering strategy increases the complexity of the global filter, it is still much more efficient than the popular global-context modeling algorithms, e.g., self-attention, spatial MLP. Besides, it is even more efficient than Res2Conv1d with the hyper-parameters employed in ECAPA. 

\begin{figure*}[!t]
	\centering
	%\hspace{5mm}
	\includegraphics[width=\linewidth]{model}
	\caption{Overview of the DS-TDNN architecture, where $\oplus$ denotes element-wise summation, $*$ denotes element-wise multiplication.}
	\label{fig 1}
\end{figure*}

\section{Dual-stream TDNN}

\subsection{Motivation}

There are two critical problems in conventional TDNN architectures affecting the recognition performance. Firstly, the number of features extracted by the model is essentially related to that of the basic channels in TDNN. In general, the larger basic channels determine better performance. However, operations popularly applied in TDNN, e.g., dilated convolution, multi-scale group convolution, and point-wise convolution for channel mix, all have a $\mathcal{O}(C^2)$ complexity, which constrains the expansion of basic channels, affecting the performance of TDNN. Secondly, although conventional TDNNs employ dilated convolutions and multi-scale group convolution to expand the receptive fields, it is still difficult for them to capture that global context, which has been proven important for robust speaker verification by many works. In addition, common algorithms for global feature modeling, e.g., self-attention, usually have $\mathcal{O}(L^2)$ complexity to the inputs, which makes them computationally unaffordable to directly apply in TDNNs that have large feature maps.

To address these problems, we design a novel Dual-Stream TDNN termed \emph{DS-TDNN}. Specifically, DS-TDNN splits the conventional architecture into two branches with equal numbers of channels, which reduces about $\mathcal{O}(C^2/2)$ complexity and parameters while preserving the number of extracted features. Besides, DS-TDNN introduces the dynamic global filter (detailed in Section II) for efficient global context modeling, increasing the recognition performance of TDNN, especially for long-duration speech.

\iffalse
\begin{equation}
	\mathcal{O}([C_1+C_2]^2)-\mathcal{O}([C_1^2+C_2^2])=\mathcal{O}(2C_1C_2)
\end{equation}
\fi

\subsection{Overall architecture}

The architecture of DS-TDNN is shown in Fig. 1. Overall, DS-TDNN is comprised of two inter-connected branches: a local branch that conducts the 1D Res2Conv module to capture the local features, and a global branch that modulates spectrums using the Dynamic Global filter in the frequency domain to model the long-term context. Each branch is independently employed on only half of the input channels and captures complementary information with different receptive fields. It is worth noting that in DS-TDNN, the input of the current layer only depends on the output of the nearest layers rather than that of all previous layers. The $i$-th output of different branches can be formulated as follows:
\begin{eqnarray}
	{\bm X_{f}^i}=\left\{
\begin{aligned}
	 & {\tt Split}({\tt stem}({\bm X})), i=1\\
     & {\tt Block}_{f}^i({\bm X_{l}^{i-1}}+{\bm X_{g}^{i-1}}), i\textgreater1\\
\end{aligned}
\right.,
\end{eqnarray}
where $f \in [l,\, g]$. ${\bm X_{l}^i}$, ${\bm X_{g}^i}$ denotes the output of $i$-th local block, and global block, respectively. ${\tt Block}_l^i$, ${\tt Block}_g^i$ represents the $i$-th local block and global block, respectively. The ${\tt stem}$ is made up of a 1D Convolution with a kernel size of 7 and a step of 1, ReLU, and a 1D BatchNormalization. 
An element-wise summation is employed at the end of each block for information exchange between different scales. Multi-scale feature aggregation has been shown to be effective for robust speaker voiceprint modeling \cite{desplanques2020ecapa,liu2022mfa,zhang2022mfa}. Therefore, we also merge the different-scale local and global features at the end of the encoder, i.e., ${\bm H}={\tt Concat}({\bm X^1_l},{\bm X^2_l},{\bm X^3_l},{\bm X^1_g},{\bm X^2_g},{\bm X^3_g})\in \mathbb{R}^{6C\times T}$. Then, an Attentive Statistics Pooling (ASP) \cite{okabe2018attentive} is applied to capture the importance of each frame-level feature $H_t$ of $\bm H$ and extract more robust speaker embedding for verification, which can formulated as follows: 
%In the next, we will introduce the detail design about local branch and global branch.
\begin{align}
	e_t &= {\bm v}^T{\tt Tanh}({\bm W}H_t +{\bm b})+k\\
    \alpha_t &= \frac{{\tt exp}(e_t)}{\sum^T_{\tau=1}{\tt exp}(e_\tau)}
\end{align}
where $\bm W\in\mathbb{R}^{6C\times 6C}, \bm v\in \mathbb{R}^{6c\times 1}, \bm b\in \mathbb{R}^{6c\times 1}$ and $k$ are the learnable parameters for ASP. After that, the normalized score $\alpha_t$ is adopted as the weight to calculate the weighted mean vector $\tilde{\bm \mu}$ and weighted standard deviation $\tilde{\bm \sigma}$, which are formulated as:
\begin{align}
	\tilde{\bm \mu}&=\sum^T_{t=1}\alpha_t H_t\\
	\tilde{\bm \sigma}&=\sqrt{\sum^T_{t=1}\alpha_t H_t\odot H_t-\tilde{\mu}\odot\tilde{\mu}}
\end{align}
where the $\mu=\frac{1}{T}\sum^T_{\tau=1}H_\tau$. The output of the ASP is given by concatenating the vectors of the weighted mean $\tilde{\bm \mu}$ and weighted standard deviation $\tilde{\bm\sigma}$. Finally, the speaker embedding is extracted from a high dimension vector to a low dimension vector with BatchNorm using the fully-connected layer.

\begin{table*}[t]
	\begin{center}
		%\tiny
		\caption{Configurations of the Variants of DS-TDNN and Associated ECAPA-TDNN.}
		\label{Table 2}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{1.5mm}{
			\begin{tabular}{l|ccccccc}
				\bottomrule
				Model     &Blocks [Local, Global] &Channels $C$  &Scales $s$  &Experts $K$ &Sparse ratio &FLOPs(G)   &\#Params(M)\\
				\hline
				ECAPA-c512 & $[1,\ 0]\times 3$ & $[512,\ 0]$      & $[8,\ 8,\ 8]$  &-   &-        & 1.2      & 7.0\\
				DS-TDNN-S  & $[1,\ 1]\times 3$ & $[256,\ 256]$      & $[4,\ 4,\ 4]$  & $[4,\ 4,\ 8]$ & $[0.3,\ 0.1,\ 0.1]$ 
				& 1.0   & 6.7\\
				\hline
				\hline
				ECAPA-c1024& $[1,\ 0]\times 3$ & $[1024,\ 0]$     & $[8,\ 8,\ 8]$  &-   &-        & 2.9      & 15.5\\
				DS-TDNN-B  & $[1,\ 1]\times 3$ & $[512,\ 512]$     & $[4,\ 4,\ 8]$  & $[4,\ 8,\ 8]$ & $[0.3,\ 0.1,\ 0.1]$ 
				& 2.1   & 13.2  \\	
				\hline
				\hline
				DS-TDNN-L  & $[1,\ 1]\times 3$ & $[768,\ 768]$     & $[4,\ 8,\ 8]$  & $[8,\ 8,\ 8]$ & $[0.4,\ 0.2,\ 0.2]$
				& 3.2   & 20.5 \\
				\toprule
		\end{tabular}}
	\end{center}
\end{table*}

\subsection{Micro Design}

\subsubsection{Local branch} 
It mainly consists of three macaron-like local blocks. In every block, two linear projections are applied for channel mix, which sandwiches the \emph{Res2Conv} structure \cite{gao2019res2net,desplanques2020ecapa} for token mix. 
As shown in Fig. 1, Res2Conv is a combination of group convolutions. It firstly splits the input tokens $\bm X \in \mathbb{R}^{C\times T}$ into $s$ groups in channel dimensions:
\begin{equation}
	{\bm X} = [{\bm X_1}, {\bm X_2},...,{\bm X_s}],
\end{equation}
where ${\bm X_1}, {\bm X_2},...,{\bm X_s}\in\mathbb{R}^{C/s\times T}$ represent the token groups. Subsequently, a 1D convolution with a kernel size of 3 and a step of 1 is applied in every group. Notably, since the local block is designed to focus on local features, the dilation rate of the convolutions is set to 1 rather than what general works do. Besides, the first group of tokens does not operated by convolution for complexity reduction. And the input of every convolution layer is the sum between the tokens in the group and the output of the previous convolution layer:
\begin{equation}
	\tilde{{\bm X}}_{i+1} = {\tt Conv}({\bm X_{i+1}}+{\tt ReLU(\tt BN(}\tilde{\bm X}_{i}))), i=2,...,s,
\end{equation}
where $\tilde{{\bm X}}_{i+1}$ represents the $(i+1)$-th tokens of the final outputs. The final output is a fusion of every group. Res2Conv has been proven helpful by giving the model multi-scale receptive fields. Then, ReLU activation together with BatchNormalization is inserted after each operation. Finally, channel attention is applied at the end of each local block.

\subsubsection{Gloabl branch} 
As shown in Fig. 1, the structure of global blocks is similar to that of local blocks, where a Dynamic Global Filter with sparse regularization (Sparse DGF) is utilized to replace Res2Conv to capture context information from long-time span. Differently, we perform a simple skip connection rather than the channel attention module used in local blocks, since the Attention FN in Sparse DGF has contained sensitive information about channels.

\subsection{Architecture variants}

To evaluate the performance of the DS-TDNN, we set two different-scale variants of the DS-TDNN with similar hyperparameters to ECAPA. Besides, we also investigate another larger variant of DS-TDNN for fair comparison with other models larger than ECAPA, such as the transformer-based model, and deep residual 2D CNN. The detailed architectures are summarized in Table 2. As shown in Table 2, since the core operation Res2Conv that ECAPA-TDNN relies on has a complexity of 2 times with the feature channel, we split the input channel and apply different operations respectively, which effectively reduces the complexity and parameters. This phenomenon will become more obvious as the number of channels increases. Notably, the sparse regularization is performed by default for all variants of DS-TDNN.

\section{Experimental Setup}

\subsection{Datasets and augmentation}

VoxCeleb1\&2 \cite{nagrani2017voxceleb, chung2018voxceleb2}, and SITW \cite{mclaren2016speakers} are used in our experiments. VoxCeleb is an audio-visual dataset consisting of 2,000+ hours short clips of human speech extracted from interview videos on YouTube. SITW is a widely-used standard evaluation dataset collected from open-source media in real-world conditions and made up of 299 speakers, including two testing trials (SITW.Dev and SITW.Eval) that have 2800+ utterances from 180 speakers. All the systems are only trained in the development set of VoxCeleb2, which has 1,092,009+ utterances at a sampling rate of 16 kHz from 5,994 speakers. A small subset of about 2\% of the data is reserved as a validation set for hyperparameter optimization. 

To better illustrate the advantages of global context modeling in different utterance duration conditions, we employ 4 trials, including Voxceleb1-O (i.e., Vox1-O), Voxceleb1-E (i.e., Vox1-E), Voxceleb1-H (i.e., Vox1-H), and a mixture consisting of SITW.Dev and SITW.Eval (i.e., mix-SITW) for recognition performance evaluation. Specifically, Voxceleb1-O is the test part of Voxceleb1, which contains 40 speakers with a total of 37,720 test pairs sampled from Voxceleb1. VoxCeleb1-E is an extension of VoxCeleb1-O, including 1251 speakers with a total of 581,480 test pairs. VoxCeleb1-H is the more challenging scenario, including 552,536 test pairs where the country and gender of the speakers in each pair are the same. The major durations of utterances in Voxceleb1 are 5-8s, thus Vox1-O, Vox1-E, and Vox1-H can be regarded as short-duration utterance scenarios. As for SITW, the major durations are about 30-40s, therefore, the mix-SITW can simulate the long-duration scenario. Notably, the training set used in the experiments, i.e., the development set of VoxCeleb 2, is completely disjoint from these four evaluation trials (no speakers in common). 

As is well known, data augmentation is generally effective for improving the performance of neural networks. Therefore, we apply six augmentation strategies following the Kaldi recipe \cite{snyder2019speaker} in combination with the publicly available MUSAN dataset \{music, speech, noise\} \cite{snyder2015musan} and the RIR dataset \{reverberation\} \cite{ko2017study}. Specifically, the first five are additive reverberation, additive speech, additive music, additive noise, and a mixture of additive speech and music, respectively. Each of them has an equal probability (0.2), which is randomly selected and applied during the training phase. The final augmentation is the SpecAugment \cite{park2019specaugment} applied to all of the training samples, which randomly masks 0 to 5 frames in the time domain and 0 to 10 channels in the frequency domain on the log mel spectrogram of the samples.

\begin{table*}[!t]
	%\footnotesize
	\caption{Voxceleb EER (\%) and minDCF Results Comparison between Different Models. ‘-c’ Denotes the Number of the Base Channels. The Best Results are Marked in \textbf{BLOD}, the Second are Marked \underline{UNDERLINE}. Our models are Highlighted in \protect\sethlcolor{mygray}\hl{GRAY}.}
	\centering
	\label{Table 3}
	\renewcommand\arraystretch{1.3}
	\setlength{\tabcolsep}{2mm}{
		\begin{tabular}{ll |ccc cc cc cc }
			\bottomrule
			\multirow{2}{*}{Index} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{FLOPs(G)}& \multirow{2}{*}{\#Param(M)} & \multirow{2}{*}{RTF ($\downarrow$)}&  \multicolumn{2}{c}{Vox1-O} & \multicolumn{2}{c}{Vox1-E} & \multicolumn{2}{c}{Vox1-H}\\
			\cline{6-11}
			& & &  & & EER (\%) & minDCF & {EER (\%)} & minDCF & EER (\%) & minDCF \\
			\hline			
			N1  & AST-T 
			&1.1 &7.0  &0.0066   &1.61  &0.170  &1.98  &0.208  &3.42  &0.296         \\
			N2  & SE-ResNet34      
			&1.2 &6.4  &0.0100   &1.15  &0.149  &1.41  &0.166  &2.75  &0.253         \\		
			N3  & ECAPA-c512   
			&1.2 &7.0  &0.0104   &\underline{1.04}&\underline{0.133} &\underline{1.26} &\underline{0.151} &\underline{2.36} &\underline{0.224} \\
			\rowcolor{mygray}
			N4  & DS-TDNN-S    
			&1.0 &6.7  &0.0128   &\textbf{0.90}  &\textbf{0.118} &\textbf{1.15} &\textbf{0.140} &\textbf{2.11} &\textbf{0.199}\\ 
			
			\hline
			\hline 
			N5  & SE-ResNet50  
			&1.4 &11.9 &0.0171  &1.05          &0.124          &1.25          &0.156          &2.17          &0.206         \\	  
			N6  & SE-ResNet101   
			&2.6 &17.0 &0.0357   &0.90          &\underline{0.107} &1.14 &0.143  &\underline{1.94}         &\underline{0.186}\\ 
			N7  & ECAPA-c1024  
			&2.9 &15.5 &0.0134   &\underline{0.88} &0.114       &\underline{1.12}&\underline{0.135}&2.08      &0.202         \\	\rowcolor{mygray}
			N8  & DS-TDNN-B  
			&2.1 &13.2 &0.0140   &\textbf{0.78} &\textbf{0.092} &\textbf{1.06} &\textbf{0.126} &\textbf{1.86} &\textbf{0.174}\\	
			\hline
			\hline
			N9  & AST-S
			&4.4  &22.5 &0.0209   &1.08  &0.125  &1.40     &0.152  &2.38     &0.216  \\  
			N10 & MFA-Conformer  
			&2.1  &20.8 &0.0102   &\underline{0.70} &\underline{0.087} &\underline{0.99} &\underline{0.120} &\underline{1.64}&\underline{0.158}\\
			N11 & SE-ResNet152
			&3.8  &21.8 &0.0459   &0.75  &0.094  &1.09     &0.128  &1.82     &0.176  \\	
			N12 & SE-ResNet34-c64 
			&4.7  &23.6 &0.0205   &0.98  &0.122  &1.21     &0.147  &2.13     &0.196  \\		
			\iffalse
			\rowcolor{mygray}
			N13 & DS-TDNN-DL 
			&3.1  &18.5 &0.72  &0.092  &\underline{0.98} &0.127    &1.68  & 0.175 \\	
			\fi	
			\rowcolor{mygray}	
			N14 & DS-TDNN-L 
			&3.2  &20.5 &0.0174    &\textbf{0.64} &\textbf{0.082}   &\textbf{0.93} &\textbf{0.112} &\textbf{1.55} &\textbf{0.149} \\	
			\toprule
	\end{tabular}}
\end{table*}

\subsection{Model configuration}

In order to comprehensively evaluate the performance of the proposed DS-TDNN, not only the TDNN-based models, e.g., \emph{ECAPA-TDNN} \cite{desplanques2020ecapa}, but also the 2D CNN-based models, e.g., \emph{SE-ResNet} \cite{chung2020defence,zhao2021speakin,shim2022graph}, and transformer-based models, e.g., \emph{Audio Spectrogram Transformer (AST)} \cite{gong2021ast}, \emph{MFA-conformer} \cite{zhang2022mfa}, are regarded as the baseline and compared with the DS-TDNN. The inputs of all systems are the 2s Mel spectrograms with 80 dimensions from a 25ms window with a 10ms frame shift, and the speaker embedding dimension of them is 192. For fair comparison, we make some minor revisions to their hyper-parameters. The configurations of the baseline systems are introduced as follows:

\textbf{AST}: It is a fully attention-based model that takes the mel spectrogram as its input and yields the speaker embeddings using the ASP on the average of the tokens produced by a transformer encoder. The dimension of its input has been changed to 80 in our experiments, which is different from the 128 in the original version. Two variants of AST are applied in our experiments, i.e., AST-tiny (AST-T) and AST-small (AST-S), which have 12 layers with 3 heads and 192 hidden channels, and 12 layers with 6 heads and 384 hidden channels, respectively, of the transformer encoder. 

\textbf{MFA-conformer}: It is the state-of-the-art model for speaker verification. It combines multi-head self-attention for global context modeling and 1D convolution for local feature modeling via a macaron-like structure. The encoder dimension is set to 256, and the number of attention heads is 4. The kernel size of the convolution module is 15. The linear hidden units of the feed-forward module are 2048. It has 6 Conformer blocks with 1/2 subsampling rates. Besides, the ASP is employed before producing the speaker embeddings.

\textbf{SE-ResNet}: Variants of SE-ResNet have similar structure and hyper-parameters to ResNet while applying channel attention (SE module) to improve speaker verification performance. Notably, the basic channels of SE-ResNet are set to 32 rather than 64 in our experiments. Besides, the subsampling rate of the stem is set to 1/2 instead of the general 1/4 for all variants. In addition, ASP is used in all variants to replace the statistics pooling in our experiments.

\textbf{ECAPA-TDNN \& DS-TDNN}: Two pairs of associated variants of them, i.e., ECAPA-c512 and DS-TDNN-S, and ECAPA-c1024 and DS-TDNN-B, are investigated in our experiments, whose configurations are detailed in Table II.

All models are trained with an exponentially decreasing learning rate from the initial 0.001 to the final 1e-4 in conjunction with the Adam optimizer \cite{kingma2014adam}. All systems are trained using Additive Angular Margin (AAM) loss \cite{deng2019arcface, xiang2019margin}, where the margin and scale are set to 0.2 and 32.0, respectively. To prevent overfitting, we apply a weight decay of 1e-5 to all weights in the model. The batch size for training is 256. All of the experiments are done with 4$\times$NVIDIA RTX A5000. 

\subsection{Backend}

Speaker embeddings are extracted from the final fully connected layer for all systems. Trial scores are produced using the cosine distance between embeddings. Subsequently, adaptive score normalization (as-norm) \cite{cumani2011comparison} is used to normalize the trial score. We average the embeddings from the same speaker in the training set to construct the imposter cohort and set the imposter cohort size to 600. Performance will be measured by providing the equal error rate (EER) and the minimum normalized detection cost (minDCF) with $P_{target} = 0.01$ and $C_{FA} = C_{Miss} = 1$. In addition, the real-time factor (RTF) calculated by the Intel Xeon Platinum 8358P (2.60GHz) is also provided to evaluate the inference speed of different models.

\section{Results Analysis}

\subsection{Results on Voxceleb}

The performance comparisons of DS-TDNN and various baseline systems introduced in Section IV.B on Vox1-O, Vox1-E, and Vox1-H are reported in Table 1, which is measured by the equal error rate (EER) and minimum Detection Cost Function (minDCF) together with the number of model parameters, floating point operations (FLOPs), and real time factor (RTF).

First of all, it can be seen from N1-N4 that the proposed DS-TDNN-S outperforms the popular ECAPA-TDNN with 512 channels, the attention-based model, and the 2D CNN-based model in terms of recognition performance, while the RTF is still not satisfying. Secondly, we can find from N5-N8 that DS-TDNN-base has a better trade-off between performance and complexity. Compared with the popular ECAPA-TDNN with 1024 channels, the proposed DS-TDNN-B obtains about 11\% relative improvement in EER with 15\% relative decline in the number of model parameters and has a similar inference speed (RTF), achieving state-of-the-art recognition performance. Thirdly, the results of N9-N14 show that, with the growth of model parameters, the performance of DS-TDNN can be further boosted, which still achieves competitive results in terms of recognition performance compared with the SOTA speaker verification model such as the MFA-conformer.

In addition, from the comparison of N11 and N12, we can find that, for 2D CNN-based models, the depth is much more important than the width, which reveals the benefits of a larger receptive field and global context modeling. Besides, from N9-N10, we can find that the introduced convolution significantly improves the recognition performance of the conventional transformer, which proves the importance of local features.

\subsection{Results on SITW}

In real-world applications, the utterance lengths may be much more than 2 seconds, such as in video processing and real-time online meetings. Extracting robust global features for utterances with different lengths is important for speaker verification. In addition, the computation overhead is also important for the application of speaker verification algorithms. Low-complexity algorithms usually have lower computing overhead and higher inference speed, which will be more significant in the face of long speech processing. We construct test samples of different lengths by clipping samples from mix-SITW to evaluate the recognition performance and inference speed of the proposed DS-TDNN and other baseline systems when faced with different durations of speech. Specifically, four subsets including different duration speeches, i.e., 5s, 15s, 30s, and 50s, are considered, and each of them consists of 500 utterances. The experimental results are shown in Fig. 2.

Firstly, due to the small receptive fields of ECAPA and ResNetSE34-c64, it is difficult for them to utilize speech lasting over 30 seconds. For the algorithm that models the global context, the recognition rate will increase significantly when the speech length increases, which illustrates the advantages of global modeling. However, since the self-attention applied in the conventional transformer has $\mathcal{O}(L^2)$ complexity to the input tokens, the RTF of AST will increase exponentially as the length of the speech increases and become unaffordable. In the face of speeches longer than about 30 seconds, AST has been unable to achieve real-time processing. Secondly, although MFA-conformer uses convolution for downsampling to reduce complexity, the inference speed becomes lower than DS-TDNN when facing about 50-second speeches due to the influence of the $\mathcal{O}(L^2)$ complexity of self-attention, which reveals the advantage of the proposed dynamic global filter. Thirdly, DS-TDNN has a similar inference speed to popular models that do not have the ability of global context modeling, achieving the best trade-off between the inference speed and recognition performance when facing long-duration speech. In addition, the EER of DS-TDNN is always lower than other baseline systems under various durations of speech, which fully illustrates the superiority of our proposed DS-TDNN.

\begin{figure}[!t]
	\centering
	%\hspace{5mm}
	\includegraphics[width=\linewidth]{RTF}
	\caption{RTF and corresponding EER of DS-TDNN and four baseline systems with different duration utterances from mix-SITW.}
	\label{fig 2}
\end{figure}

\subsection{Analysis and visualization}

\begin{figure*}[!t]
	\centering
	\hspace{-4mm}
	\includegraphics[width=\linewidth]{SR}
	\caption{Visualizations of dynamic filtering and sparse regularization. \emph{Left}: DS-TDNN converges to the optimum along a smooth trajectory. \emph{Right}: The distribution of Hessian max eigenvalue. In the polar coordinate, $r_t=\frac{||\Delta\omega_{t}||}{||\Delta\omega_{\rm init}||}$, and $\theta={\rm cos^{-1}}\left(\frac{\Delta\omega_t\cdot\Delta\omega_{\rm init}}{||\Delta\omega_t||\,||\Delta\omega_{\rm init}||}\right)$, where $\Delta\omega_t=\omega_t-\omega_{\rm optim}$.}
	\label{fig 3}
\end{figure*}

\subsubsection{From optimization}

In order to investigate the effect of the dynamic filtering strategy and sparse regularization on optimization, we plot the smooth optimization trajectory of the model using different filtering strategies in polar coordinates, where the radius $r_t$ is defined as the normalized distance between the current weights $\omega_t$ and the optimum weights $\omega_{\rm optim}$, and the angle represents the direction of optimization. In addition, we also study the distribution of the Hessian eigenvalue of the weights in the dynamic/static global filter layers, which can reflect the local convexity of the loss function and reveal the difficulty of optimization. 

As shown in Fig. 3, the optimization trajectory of the model becomes significantly sharper after adopting the dynamic filtering strategy, and there is a period of optimization in the opposite direction, indicating that the dynamic filtering strategy will increase the difficulty of optimization. After sparse regularization, the optimization trajectory becomes smooth, and the path of opposite optimization is also greatly reduced, indicating that sparse regularization is helpful for model optimization. One possible reason is that the sparse regularization reduces the variance of gradients for mini-batches and allows the model to update in a more consistent direction. The direction is also more consistent with the correct gradient direction, which prevents the model from falling into the saddle point.
This conclusion can also be proven from the distribution of the Hessian eigenvalue of the weights. Specifically, the Hessian matrix of dynamic global filter weights has negative eigenvalues in early epochs, which indicates that the loss function is non-convex and the model is more likely to fall into saddle points. Besides, the Hessian eigenvalues of dynamic global filters are smaller than those of static global filters, which represents a smaller gradient. Notably, the proposed sparse regularization method significantly suppresses the negative eigenvalues in the Hessian matrix of dynamic filter weights and increases the values of the eigenvalues, which avoids the model from falling into the saddle points and provides a larger gradient using the same loss function, proving the effectiveness of sparse regularization. Interestingly, although sparse regularization brings a more significant improvement for dynamic global filters, it also benefits static global filters.

\begin{figure*}[!t]
	\centering
	%\hspace{-5mm}
	\includegraphics[width=\linewidth]{visualGF2}
	\caption{Visualizations of global filters. \emph{Left}: the amplitude-frequency response of the former 64 filters in the global filter layers, where the vertical axis represents the log amplitude and the horizontal axis represents the frequency. \emph{Right}: the distribution of the center frequency (C.F.) of all the filters in the global filter layers and the numbers of each type of filter.}
	\label{fig 4}
\end{figure*}
\subsubsection{From digital signal processing} 

In this part, we discuss how the proposed global filter affects input tokens. Besides, we explain the effectiveness of the dynamic filtering strategy and sparse regularization for enhancing the global filters from the aspect of digital signal processing. In the dynamic global filter, we set $\omega_1 = \omega_2 = ... = \omega_K = 1/K$.

Firstly, numerous works have proven that the modules for global context modeling, e.g., self-attention, are equal to low-pass filters, while the local feature modeling algorithms like convolution trend to be high-frequency filters \cite{parkvision,cordonnierrelationship,siinception}. As shown in Fig. 4, both the dynamic and static global filters are mainly composed of low-pass filters, which proves that they mainly capture the long-term dependencies rather than the local textures of the input tokens. In addition, it also illustrates that the global filter is more resistant to high-frequency noise than the convolution operation, which reveals why DS-TDNN can capture more robust speaker embeddings than conventional TDNN. Secondarily, it is interesting that there are other types of filters, e.g., band-pass and high-pass filters, in the global filter in addition to the low-pass filter. This shows that although the proposed global filter mainly works by emphasizing the low-frequency features of the spectrum, it also has the ability to capture arbitrarily high-frequency features that correspond to the local textures in the spatial domain. Besides, from the comparison between the dynamic and static filters, the number of high-pass and band-pass filters in dynamic filter layers is significantly lower than that of static filter layers. More low-pass filters generally indicate the stronger ability of global context modeling, which explains that the proposed dynamic filtering strategy enhances the recognition performance by providing the model with more powerful global context representations. Notably, there are some all-pass filters in the global filter. Since these filters cannot influence the spectrum, we call them inactive filters (inact). In our opinion, the reason these inactive filters exist is because they are optimized incompletely. It is easily seen from the comparison between the visualization of the filters before and after sparse regularization that the number of inactive filters is significantly reduced after sparse regularization. From another perspective, it is proven that sparse regularization can help the model optimize better, and the model performance can be improved by reducing the number of inactive filters. At the same time, due to the decrease in the number of inactive filters, the overall center frequency low-pass trend of the filter is also more obvious. Finally, the last two layers have a significantly higher utilization rate of filters than the first layer, which is reflected by the number of inactive filters. Besides, as the depth increases, the number of low-pass filters increases, which is consistent with the characteristic of neural networks that the deeper layer has larger receptive fields.

\begin{table}[t]
	%\footnotesize
	\caption{Part 1 of the Ablation Study. \protect\sethlcolor{mygray}\hl{GRAY} Denotes the DS-TDNN-B}
	\centering
	\label{Table 4}
	\renewcommand\arraystretch{1.5}
	\setlength{\tabcolsep}{1.2mm}{
		\begin{tabular}{l| l l l l}
			\bottomrule
			&FLOPs(G) &  \#Params(M)  &EER(\%) &minDCF  \\
			\hline			
			ECAPA-c1024               & 2.9{\tiny  $_{\, 0\%}$}              & 15.5{\tiny  $_{\, 0\%}$} 
			& 0.88{\tiny $_{\, 0\%}$}              & 0.114{\tiny $_{\, 0\%}$}           \\	
			\hline
			Blocks $= [2, 0]\times 3$ & 2.2{\tiny  $_{\,   \textcolor{red}{35\%\downarrow}}$}   
			& 12.2{\tiny $_{\,  \textcolor{red}{21\%\downarrow}}$}
			& 0.95{\tiny $_{\, 8\%\uparrow}$}      & 0.127{\tiny $_{\, 12\%\uparrow}$}  \\
			Blocks $= [0, 2]\times 3$ & 2.1{\tiny  $_{\,  5\%\downarrow}$}   & 14.3{\tiny  $_{\,  17\%\uparrow}$}
			& 1.07{\tiny $_{\, 13\%\uparrow}$}     & 0.135{\tiny $_{\, 6\%\uparrow}$}   \\
			Blocks $= [1, 1]\times 3$ & 2.1{\tiny  $_{\,  2\%\downarrow}$}   & 13.2{\tiny  $_{\,  8\%\downarrow}$}
			& 0.84{\tiny $_{\,  \textcolor{red}{21\%\downarrow}}$}   
			& 0.098{\tiny$_{\, \textcolor{red}{27\%\downarrow}}$}                       \\
			\rowcolor{mygray}						      
			+  Info exchange          & 2.1{\tiny  $_{\,  0\%\uparrow}$}     & 13.2{\tiny  $_{\,  0\%\uparrow}$}
			& 0.78{\tiny $_{\, 7\%\downarrow}$}    & 0.092{\tiny $_{\, 6\%\downarrow}$} \\
			- MFA       			  & 1.5{\tiny  $_{\,  \textcolor{red}{29\%\downarrow}}$}   
			& 10.0{\tiny $_{\, \textcolor{red}{24\%\downarrow}}$}
			& 0.92{\tiny $_{\, 18\%\uparrow}$}    
			& 0.119{\tiny$_{\,\textcolor{red}{29\%\uparrow}}$}                          \\
			
			\hline 
			\hline
			w/o Dynamic filtering        & 1.8{\tiny  $_{\, 0\%}$}              & 11.4{\tiny  $_{\, 0\%}$}   
			& 0.83{\tiny $_{\, 0\%}$}  		     & 0.102{\tiny $_{\, 0\%}$}           \\
			\hline
			$k = [4,4,4]$             & 2.0{\tiny  $_{\,  11\%\uparrow}$}    & 12.6{\tiny  $_{\, 11\%\uparrow}$} 
			& 0.80{\tiny $_{\,  4\%\downarrow}$}   & 0.096{\tiny $_{\, 6\%\downarrow}$} \\
			$k = [4,4,8]$         	  & 2.0{\tiny  $_{\,  2\%\uparrow}$}     & 12.9{\tiny  $_{\, 2\%\uparrow}$} 
			& 0.79{\tiny $_{\,  1\%\downarrow}$}   & 0.095{\tiny $_{\, 0\%\downarrow}$} \\
			\rowcolor{mygray}	
			$k = [4,8,8]$         	  & 2.1{\tiny  $_{\,  3\%\uparrow}$}     & 13.2{\tiny  $_{\, 2\%\uparrow}$}
			& 0.78{\tiny $_{\,  1\%\downarrow}$}   & 0.092{\tiny $_{\, 3\%\downarrow}$} \\
			$k = [8,8,8]$        	  & 2.2{\tiny  $_{\,  4\%\uparrow}$}     & 13.7{\tiny  $_{\, 4\%\uparrow}$}  
			& 0.79{\tiny $_{\,  1\%\uparrow}$}     & 0.093{\tiny $_{\, 1\%\uparrow}$}   \\
			\hline
			\hline
			w/o Group Conv      	  & 2.5{\tiny  $_{\,  0\%}$}    	     & 13.4{\tiny  $_{\, 0\%}$}  
			& 0.81{\tiny $_{\,  0\%}$}  			 & 0.098{\tiny $_{\, 0\%}$}           \\
			\hline
			$s = [8,8,8]$         	  & 2.1{\tiny  $_{\,  16\%\downarrow}$}  & 13.0{\tiny  $_{\, 3\%\downarrow}$}   
			& 0.80{\tiny $_{\,  1\%\downarrow}$}   & 0.094{\tiny $_{\, 4\%\downarrow}$} \\
			
			$s = [4,8,8]$        	  & 2.1{\tiny  $_{\,  1\%\uparrow}$}  	 & 13.1{\tiny  $_{\, 1\%\uparrow}$}  
			& 0.79{\tiny $_{\,  1\%\downarrow}$}   & 0.093{\tiny $_{\, 1\%\downarrow}$} \\
			\rowcolor{mygray}	
			$s = [4,4,8]$         	  & 2.1{\tiny  $_{\,  1\%\uparrow}$}     & 13.2{\tiny  $_{\, 1\%\uparrow}$}   
			& 0.78{\tiny $_{\,  1\%\downarrow}$}   & 0.092{\tiny $_{\, 1\%\downarrow}$} \\
			$s = [4,4,4]$        	  & 2.2{\tiny  $_{\,  1\%\uparrow}$}     & 13.2{\tiny  $_{\, 2\%\uparrow}$}   
			& 0.78{\tiny $_{\,  0\%\uparrow}$}     & 0.092{\tiny $_{\, 1\%\uparrow}$}\\
			\toprule 
	\end{tabular}}
\end{table}

\subsection{Ablation study}

To determine the importance of each design employed in DS-TDNN, we conduct a detailed ablation study evaluated by the results of EER and minDCF on Vox1-O, which is divided into three parts. In Part 1, we first evaluate the macro designs of DS-TDNN, i.e., local branch and global branch, element-wise summation for information exchange after each stage, and the multi-scale feature aggregation (MFA) before the attentive statistic pooling. Then, we investigate the effects of dynamic filtering and multi-scale group convolution on recognition performance, respectively. In Part 2, we quantitatively evaluate the impact of sparse regularization on dynamic global filters and static filters. In Part 3, we compare the effects of the width and depth of DS-TDNN on the recognition performance.

\subsubsection{Part 1} 
As shown in Table IV, the first group of experiments illustrates that every macro design in DS-TDNN is important for improving performance in recognition. Specifically, due to the $\mathcal{O}(C^2)$ complexity of Res2Conv and linear projections, the channel split design can significantly reduce the complexity of the model with an affordable decline in recognition performance. Notably, the model cannot achieve good recognition performance using only local or global branches, while local branches are more helpful for recognition performance than global branches. The reason is that local features usually contain information about speaker voiceprints, which is essential for recognition. However, combining local branches with global branches can further provide a significant improvement in the recognition performance with little increase in both complexity and model parameters. Besides, the element-wise summation at the end of each block improves recognition performance without increasing complexity, indicating the necessity for the fusion of different receptive field information and the effectiveness of the proposed strategy for information exchange between local features and global context. Finally, the MFA proposed in ECAPA is also greatly useful for DS-TDNN, even though it obviously increases the complexity. The second group of experiments revealed that dynamic filtering can obviously improve recognition performance. Secondarily, the performance of the model using dynamic filtering can be slightly enhanced by increasing the number of expert filters. However, this improvement will gradually decline as the number grows and will eventually be lower than the impact of dynamic filtering on complexity. In addition, too many expert filters will also make the model more difficult to optimize, affecting its performance. Finally, due to the higher filter utilization of deeper layers, they tend to require more expert filters than shallower layers. The third group of experiments shows that the multi-scale group convolution, i.e., the Res2Conv structure, can not only reduce complexity and parameters but also improve performance, which indicates that although the DS-TDNN has a special design for global context modeling, the multi-scale information is still very important for learning a powerful speaker representation.

\begin{table}[t]
	%\footnotesize
	\caption{Part 2 of the Ablation Study}
	\centering
	\label{Table 5}
	\renewcommand\arraystretch{1.5}
	\setlength{\tabcolsep}{1.8mm}{
		\begin{tabular}{cc| c c}
			\bottomrule
			Experts $K$  &Sparse ratio &EER(\%) &minDCF  \\
			\hline
			$[4,\ 8,\ 8]$& w/o SR             & 0.82  & 0.098\\
			$[4,\ 8,\ 8]$&$[0.3,\ 0.3,\ 0.3]$ & 0.88  & 0.106\\
			$[4,\ 8,\ 8]$&$[0.3,\ 0.3,\ 0.1]$ & 0.83  & 0.096\\
			$[4,\ 8,\ 8]$&$[0.3,\ 0.1,\ 0.1]$ & 0.78  & 0.092\\
			$[4,\ 8,\ 8]$&$[0.2,\ 0.1,\ 0.1]$ & 0.79  & 0.093\\
			\hline
			\hline
			-            &w/o SR              & 0.84 & 0.105\\
			-            &$[0.3,\ 0.3,\ 0.3]$ & 0.91 & 0.117\\
			-            &$[0.3,\ 0.3,\ 0.1]$ & 0.86 & 0.109\\
			-            &$[0.3,\ 0.1,\ 0.1]$ & 0.82 & 0.102\\
			-            &$[0.2,\ 0.1,\ 0.1]$ & 0.82 & 0.104\\
			\toprule 
	\end{tabular}}
\end{table}

\subsubsection{Part 2}
As shown in Table V, proposed Sparse Regularization (SR) can enhance the performance of both dynamic global filters and static global filters, while being more efficient for the dynamic global filters. Besides, a higher sparse rate does not always provide better performance, and an excessively high sparse rate will lead to an obvious decline in recognition performance. The optimum sparse rate is approximately correlated to the number of inactive filters in the layer before sparse regularization. Therefore, the shallow layers generally require a higher sparsity rate for regularization.

\subsubsection{Part 3}:
As shown in Table VI, different from some models that have optimal depth, DS-TDNN has good extensibility. The performance of DS-TDNN can be improved with an increase in depth or width. However, the extension in width is generally more efficient than that in depth for the DS-TDNN. Since DS-TDNN adopts the global filter that can capture the global context in the shallow layer, it is inefficient for DS-TDNN to stack the depth for a larger receptive field. Besides, DS-TDNN has a bottleneck channel, which is actually equal to $1/2$ of the basic channels and essentially determines the number of extracted features, making DS-TDNN extremely dependent on the width for obtaining sufficient features.

\begin{table}[t]
	\begin{center}
		%\tiny
		\caption{Part 3 of the Ablation Study}
		\label{Table 6}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{1mm}{
			\begin{tabular}{l|ccccccc}
				\bottomrule
				&Layers & Channels&FLOPs(G)&\#Params(M) &EER(\%)   &minDCF \\
				\hline
				Basic             & $3$   & $1024$  & 2.1    & 13.2       & 0.78     & 0.092 \\	
				\hline
				Width $\uparrow$  & $3$   & $1280$  & 2.8    & 17.1       & 0.70     & 0.088 \\
				Width $\uparrow$  & $3$   & $1536$  & 3.2    & 20.5       & 0.64     & 0.082 \\
				\hline
				\hline
				Deepth $\uparrow$ & $4$   & $1024$  & 2.9    & 17.0       & 0.74     & 0.090 \\
				Deepth $\uparrow$ & $5$   & $1024$  & 3.2    & 20.8       & 0.72     & 0.087 \\
				\toprule
		\end{tabular}}
	\end{center}
\end{table}

\section{Conclusion}

In this paper, we propose a novel global filter to capture the long-term dependencies in the speech. It has global receptive fields and log-linear complexity, which can replace the popular dilated convolution, multi-scale group convolution, and self-attention for speaker verification. In addition, a dynamic filtering strategy and a sparse regularization method are specially designed for the global filter to enhance its representation, reduce the optimization difficulty, and prevent it from over-fitting. Furthermore, we construct the DS-TDNN based on the global filter, which is a conceptually simple yet powerful and efficient architecture for speaker verification. It splits the input channels, captures the local features and global context using different branches in parallel, and exchanges the different-scale information through an element-wise summation at the end of each layer. Experiments on the Voxceleb and SITW datasets show that the DS-TDNN achieves state-of-the-art results on recognition performance. Specifically, DS-TDNN outperforms the ECAPA-c1024 in terms of recognition performance by about 10\% with a decline of about 28\% and 15\% in complexity and parameters, respectively. Besides, it also beats the SE-ResNet and achieves very competitive results with the MFA-conformer on an approximate parameter scale. Compared with popular baseline systems, DS-TDNN has the best trade-off between utilization and inference speed for long-duration speech. Our work reveals the importance of global context modeling for another aspect and the potential of TDNN for speaker verification, and it also provides some inspiring ideas for future SV system design.

\section*{Acknowledgments}
This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.



\bibliographystyle{IEEEtran}
\bibliography{mybib}


\iffalse
\section{Biography Section}
If you have an EPS/PDF photo (graphicx package needed), extra braces are
 needed around the contents of the optional argument to biography to prevent
 the LaTeX parser from getting confused when it sees the complicated
 $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
 your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
 simpler here.)
 
\vspace{11pt}

\bf{If you include a photo:}\vspace{-33pt}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
Use the author name as the 3rd argument followed by the biography text.
\end{IEEEbiography}

\vspace{11pt}

\bf{If you will not include a photo:}\vspace{-33pt}
\begin{IEEEbiographynophoto}{John Doe}
Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
\end{IEEEbiographynophoto}
\fi



\vfill
\iffalse
{\appendix[Proof of the Zonklar Equations]
	Use $\backslash${\tt{appendix}} if you have a single appendix:
	Do not use $\backslash${\tt{section}} anymore after $\backslash${\tt{appendix}}, only $\backslash${\tt{section*}}.
	If you have multiple appendixes use $\backslash${\tt{appendices}} then use $\backslash${\tt{section}} to start each appendix.
	You must declare a $\backslash${\tt{section}} before using any $\backslash${\tt{subsection}} or using $\backslash${\tt{label}} ($\backslash${\tt{appendices}} by itself
	starts a section numbered zero.)

\begin{figure*}[h]
	\centering
	\subfloat[layer1]{
		\label{fig:subfig:a}
		\includegraphics[width=5.5in]{layer1}}%\vspace{-0.1cm}
	
	\subfloat[layer2]{
		\label{fig:subfig:b}
		\includegraphics[width=5.5in]{layer2}}%\vspace{-0.1cm} 
	
	\subfloat[layer3]{
		\label{fig:subfig:c}
		\includegraphics[width=5.5in]{layer3}}%\vspace{-0.1cm}
	\caption{Visualization of GF}
	\label{fig:assign}
\end{figure*}
}
\fi


\end{document}


