\documentclass[lettersize,journal]{IEEEtran}
\usepackage{newtxtext,newtxmath}
%\usepackage{newpxtext,newpxmath}
%\usepackage{amsmath, amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}

\usepackage{array}
\usepackage[caption=false,font=footnotesize,labelfont=rm,textfont=rm]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{bm}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{color, xcolor} % 颜色包，color 必须导入，xcolor 建议导入
\usepackage{soul} % 导入 soul 包
\usepackage{balance} 
\definecolor{mygray}{gray}{.93}
\definecolor{mygreen}{RGB}{57, 124, 124}
% updated with editorial comments 8/9/2021
%\usepackage{subfig}
\usepackage{xcolor}
%\usepackage{svg}

%\usepackage[T1]{fontenc} % or LY1
%\usepackage{textcomp} % unnecessary with LY1
%\usepackage[altbullet]{lucidabr}

\begin{document}

\title{Efficient Dual-stream Time-delay Neural Network with Global-aware Filter for Speaker Verification}

\author{Yangfu Li, Xiaodan Lin$^*$ and \\ School of Information Science and Engineering, Huaqiao University
        % <-this % stops a space
\thanks{* Corresponding Author. 
	
Yangfu Li and Xiaodan Lin are with the School of Information Science
and Engineering, Huaqiao University, Xiamen 361021, China (e-mail:
xd\_lin@hqu.edu.cn; 21013082029@stu.hqu.edu.cn).}% <-this % stops a space
%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}g
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle
%全文改写表达：receptive field, global context; 核心motivation 在浅层网络获取全局上下文。
\begin{abstract}

Conventional time-delay neural networks (TDNNs) face a challenge in handling long-range context, which affects their ability to represent speaker information, particularly for long-duration utterances. Existing solutions either significantly increase model complexity or achieve a poor trade-off between local features and global context. In this paper, we introduce a novel module called Global-aware Filter layer (GF layer), which employs a set of learnable transform-domain filters between a 1D discrete Fourier transform and its inverse transform to efficiently capture global context. Additionally, we innovatively design a dynamic filtering strategy and a sparse regularization method to enhance the performance of the GF layer and prevent overfitting. Based on the GF layer, we present a dual-stream TDNN architecture called DS-TDNN for automatic speaker verification (ASV). It utilizes two unique branches to extract both local and global features in parallel and employs a straightforward yet efficient strategy to fuse different-scale information. Experiments on the Voxceleb and SITW databases demonstrate that the DS-TDNN achieves a relative 10\% improvement together with a relative 20\% decline in computational cost over the ECAPA-TDNN in speaker verification. This improvement will become more significant with the growth of utterance durations. Furthermore, the DS-TDNN also beats popular deep residual models and attention-based baseline systems for arbitrary-duration utterances.
%Furthermore, the DS-TDNN has a low computational cost, making it promising for real-life applications. 
%Finally, we provide visualizations and detailed ablation studies to clarify the advantages of the GF layer and the DS-TDNN.
%

\end{abstract}

\begin{IEEEkeywords}
Time-delay neural network, dual-stream network, text-independent speaker verification, global context.
\end{IEEEkeywords} 
  

\section{Introduction}

\IEEEPARstart{A}{utomatic} Speaker Verification (ASV) systems aim to determine whether a given utterance is from an enrolled speaker, which has been widely applied in user authentication, access control, multimedia forensics, and other areas \cite{rosenberg1976automatic,broun2002automatic,becker2008forensic}. Typically, an ASV system consists of two main components: a front-end that extracts low-dimensional discriminative speaker embeddings from variable-length utterances and a back-end that determines whether two embeddings are from the same speaker. With the development of deep neural networks (DNNs), the front-end has undergone a significant transformation from conventional probabilistic models \cite{reynolds2000speaker,kenny2007joint,dehak2010front} to DNN-based methods \cite{lei2014novel,variani2014deep,snyder2017deep}. In particular, x-vector \cite{snyder2017deep} is a state-of-the-art architecture for speaker embedding. The x-vector is based on the Time-Delay Neural Network (TDNN) \cite{waibel1989phoneme} layers designed for end-to-end speech processing, where share-weight filters are employed to pay attention on the whole frequency axis and a time-delay strategy is applied to capture context between consecutive frames.

Despite its great success, TDNN still has limitations. Due to the short-term stationarity of human speech, both local features and global context are essential for extracting robust speaker representation. However, typical TDNN mainly focuses on local features but lacks global context fusion because of the small receptive field in each hidden layer. As a result, TDNN-based models are less resistant to abrupt noise and exhibit suboptimal performance in real-life applications, especially when facing long-duration utterances. A natural idea to strengthen global context is to extend the depth of TDNN. \cite{novoselov2018deep} introduces the residual connection \cite{he2016deep} to construct an extremely deep TDNN that captures context information over a longer time span than the standard TDNN. In \cite{snyder2019speaker}, the authors extend TDNN by inserting dense layers between each pair of its hidden layers. To prevent overfitting, \cite{huang2019deeper} introduce dropouts \cite{srivastava2014dropout} to TDNN and propose a carefully designed filter with varying temporal resolution for more powerful context representation. To further deepen TDNN within affordable parameters, \cite{yu2020densely} makes a trade-off between the width and the depth. However, thin TDNN may have poor speaker representations. To solve this problem, \cite{zhang2020aret} proposes a split-transform-merge structure, which can help the deep TDNN with limited width learn more discriminative speaker representations. 
These improvements make remarkable progress in performance but also significantly increase complexity. Additionally, they do not consider the fusion of local and global features. To solve these problems, many techniques have been proposed, which are divided into two categories in this paper.
%他们没有考虑到如何将全局特征与局部特征融合起来。为了解决这些问题（问题包括，如何获取全局特征，如何将其与局部特征相结合。）, 

%下文要讲他们是如何结合全局与局部的，这样做有什么问题
One technique is to enhance the filters with multi-scale information. \cite{desplanques2020ecapa} introduces the Res2Net \cite{gao2019res2net} structure into TDNN, which inserts skip connections between each grouped filter to construct a different-scale receptive field and proposes the multi-layer feature aggregation to fuse local and global features, achieving state-of-the-art performance. To enhance the Res2Net, \cite{9688119} proposes a context-aware filter. This method further divides each grouped filter applied in Res2Net into two different-scale filters: a normal receptive field filter for local features and a large receptive field filter for global context, which are combined using the dynamic integration strategy. Authors in \cite{gu2021dynamic} propose another type of dynamic filter, whose value is determined by an element-wise summation between the local features calculated using moving average and the global context calculated using global average pooling. Recently, \cite{mun2022selective} presents a variable-resolution filter, which employs a kernel selection mechanism to find the optimal receptive field. Although these enhanced filters can dynamically choose to focus on local features or global context as required, none of them can pay attention to both local features and global context at the same time, limiting the representations.

\iffalse
To enhance the capability of TDNN in handling long-range context, 还有两种方法被提出.
结合图1讲各自的缺陷；
1、对于改善卷积来说，就是图1a，本质没有改变，导致改善效果有限，无法兼顾局部与全局
2、对于目前的引入LSTM或自注意力的方法来说，首先是他们引入的算法本身复杂度就高，其次是他们结合局部与全局的方法，即图1b，往往是在一个较大的feature map尺寸上交替计算进行的，导致long余度很高，复杂度也很高。

3、为此，我们提出了两个解决办法，首先是一种高效的全局特征算法；其次是一种高效的特征结合模式。基于此，我们构建了一种高效的双流TDNN来进行说话人验证。。。
\fi

Another technique considers incorporating the additional modules designed to handle global context (global module) with TDNN. For example, authors in \cite{chen2019speaker} employ the Long Short-Term Memory (LSTM) module in the shallow layers of TDNN to provide high-resolution temporal context information. In \cite{jiang2019effective}, the authors insert LSTM between each pair of hidden layers in TDNN, enabling TDNN to capture global context at a more granular level. However, these dense LSTM methods are computationally expensive. To improve efficiency, \cite{sak2014long, liu2019speaker} utilize recurrent projection to reduce the dimension of the data flow in the LSTM cells. Another approach is to apply the LSTM at the segment-level rather than the frame-level, handling temporal information in the low-dimension latent space \cite{tu2019towards,lin2019lstm}. With the success of self-attention, the authors in \cite{huang2020speaker} combine the frame-level LSTM with the segment-level attention mechanism to further enhance the performance. This technique has shown promising performance, but it comes with a high computational cost.

\begin{figure}[!t]
	\centering
	%\hspace{5mm}
	\includegraphics[width=\linewidth]{mode}
	\caption{A comparison of different strategies to combine local and global features. (a) TDNN without additional modules for global context. (b) the popular alternating pattern. (c) the proposed parallel pattern.}
	%a是经典的TDNN,不采用额外module来建模远距离上下文。b是流行的交替模式，在一个较大feature map上交替计算局部与全局信息。c是我们提出的并行模式，通过通道切分解耦局部与全局信息，并在一个较小的尺度上并行计算他们。
	\label{fig 1}
\end{figure}
In the existing second technique, the global modules, such as LSTM and self-attention, have a heavy level of complexity. Additionally, different-scale operations are directly performed on the whole mixture of local and global features with a large size, as illustrated in Fig. 1b, resulting in inefficiency. To address these two issues, this paper first proposes a novel and efficient global module called the Global-aware Filter (GF) layer. The GF layer consists of three key components: a discrete Fourier transform, a set of differentiable transform-domain filters, and the inverse discrete Fourier transform. It explicitly models the global context, yet only has a log-linear complexity. Moreover, we introduce dynamic filtering to further enhance the GF layer. With dynamic filtering, the GF layer can adapt to different speech contexts, emotions, and channels, thereby improving the representation and generalization. To prevent over-fitting, we also propose sparse regularization, which randomly drops filters in the GF layer with a fixed ratio during the training phase.
In addition, we propose a parallel pattern to combine local and global features, as shown in Fig. 1c. In this pattern, different-scale modules are designed to focus on specific parts of the complete feature maps, which is more efficient than the popular alternating pattern. By incorporating the GF layer and the parallel pattern, we construct a simple yet efficient dual-stream TDNN for speaker verification, called DS-TDNN. DS-TDNN employs two independent branches to process local features and global context in parallel and applies several carefully designed strategies to fuse different-scale information. Experimental results on the Voxceleb and SITW datasets demonstrate that DS-TDNN outperforms the powerful TDNN-based baseline, ECAPA-TDNN, with a lower computational cost. Furthermore, it outperforms other popular baseline systems, such as 2D CNN with residual connections and the attention-based model. Moreover, DS-TDNN achieves the best trade-off between effectiveness and efficiency. We have released the models and code for further research\footnote[1]{\url{https://github.com/YChenL/DS-TDNN}}. 

Our contributions are summarized as follows:
\begin{itemize} 
	\item We propose an innovative module termed \emph{Global-aware Filter} (GF) layer for TDNN, which has global receptive fields yet a log-linear complexity to speech duration. The GF layer is an efficient alternative to popular global-aware algorithms in speaker verification.

	\item We design two special techniques, i.e., \emph{Dynamic Filtering} and \emph{Sparse Regularization} to further enhance the performance of the GF layer. The former enables the GF layer dynamically adapt to the inputs, providing a more powerful representation of speaker information. The latter aims to reduce the optimization difficulty of the dynamic GF layer and prevent overfitting.
	
	\item We propose a novel parallel pattern for the different-scale information extraction, which is more efficient than existing solutions in speaker embedding models. Incorporating the GF layer and the parallel pattern, we construct a dual-stream TDNN called DS-TDNN, which is a straightforward yet efficient architecture for speaker verification. 
	
	\item Experiments on the Voxceleb and SITW datasets demonstrate that the DS-TDNN achieves state-of-the-art results compared to popular baseline systems. Moreover, detailed visualizations and ablation studies are conducted to reveal the benefits of the design of DS-TDNN.

	
\end{itemize}

The rest of this paper is organized as follows: Section II introduces the global-aware filter layer, including its motivation, realization, improvements, and comlexity analysis. Section III describes the dual-stream TDNN. The experimental setup is detailed in Section IV. The result analysis is presented in Section V, followed by a conclusion in Section VI.

%\newpage
\section{Global-aware filter layer}
\iffalse
由于我们做通道可分离的FFT，我们的DGF对通道和序列长度L均为线性复杂度.
\fi

\subsection{Motivation}
Conventional TDNNs mainly utilize time-delay layers for feature extraction that can be viewed as one-dimension convolution, where calculations are restricted within the window rather than the entire feature maps, limiting the modeling for long-range context. 
A natural idea for expanding the receptive field of convolution is to increase the window size. However, the global convolution will lead to a heavy $\mathcal{O}(N^2)$ complexity for a sequence including $N$ points. Fortunately, there is a simple yet efficient equivalent of the global convolution, which can be derived from the discrete convolution theorem (see Appendix A) and formulated as follows:
\begin{equation}
	\bm {w}_g* \bm x \equiv \mathcal{F}^{-1}[{\bm w}_f\mathcal{F}[\bm x]],
\end{equation}
where $\bm w_f=\mathcal{F}[\bm w_g]$, which is a transform-domain filter. $\bm w_g$ is a spatial-domain filter with a global receptive field, $\bm x$ is an $N$-point token.
%, and $\bm w_f$ 
$*$ denotes one-dimension convolution. $\mathcal{F}[\,\bm \cdot\,]$ and $\mathcal{F}^{-1}[\,\bm \cdot\,]$ separately denote the one-dimension discrete Fourier transform (DFT) and its inverse transform (IDFT). Based on the fast algorithms of DFT/IDFT, i.e., FFT/IFFT , the complexity of $N$-point DFT/IDFT can be reduced from $\mathcal{O}(N^2)$ to $\mathcal{O}(N{\rm log}N)$. Equ 1 reveals the potential of FFT/IFFT to model long-range context. This idea has been studied in computer vision \cite{li2020falcon,chi2020fast,lifourier}, but has never been explored in ASV. Motivated by this, we introduce the global-aware filter layer for TDNN that modulates the tokens using FFT/IFFT together with a set of differentable filters $\bm w_f$ to efficiently handle the global context. 
%employs FFT/IFFT and a set of learnable filters in the . 



\subsection{Realization}
%和上面的subsection揉在一起写
We propose a global-aware filter (GF) layer as an efficient alternative to global-aware modules, e.g., global convolution and self-attention, to capture global context. In TDNN, the tokens $\bm X \in \mathbb{R}^{C\times T}$ can be considered a series of discrete sequences $\bm x_i\in \mathbb{R}^{1\times T}$ stacked along the channel dimension, i.e., ${\bm X} = {\tt Concat}(\bm x_1, \bm x_2, ..., \bm x_C)$. Therefore, given the tokens ${\bm X}$, the corresponding spectrum $\bm X_f$ can be obtained by 1D FFT separately performed on every channel:
\begin{equation}
	{\bm X_f}={\rm Concat}(\mathcal{F}[{\bm x_1}], \mathcal{F}[{\bm x_2}], ..., \mathcal{F}[{\bm x_C}])\in \mathbb{C}^{C\times T}.
\end{equation}
For efficiency, we perform channel-wise 1D FFT/IFFT on $\bm X$ and then employ linear projection to mix channels, rather than directly performing 2D FFT/IFFT on $\bm X$.
Besides, since $\bm X$ is a real tensor, the corresponding spectrum $\bm X_f$ is conjugate symmetric, i.e., ${\bm X_f}[: , T-\tau]={\bm X_f^*}[: , \tau]$. Thus, we can take only the half of $\bm X_f$ yet preserving the full information of $\bm X$ to further reduce the computation complexity:
\begin{equation}
	{\bm X_r}={\bm X_f}[:, 0:\lceil T/2\rceil]:= \mathcal{F}_r[{\bm X}] \in \mathbb{C}^{C\times \lceil T/2\rceil},
\end{equation}
where $\mathcal{F}_r[\,{\bm \cdot}\,]$ denotes the channel-wise 1D FFT for real signals. 
It is noteworthy that $\bm X_r$ is a complex tensor and represents the half of spectrum $\bm X_f$. 
Since the Fourier transform integrates the whole information in a channel into each single element of the spetrum, we can model the channel-wise global context via a simple element-wise multiplication between the spectrum and a differentiable filter $\bm F \in \mathbb{C}^{C\times \lceil T/2\rceil}$:
\begin{equation}
	\tilde{{\bm X_r}} = {\bm F} \odot {\bm X_r},
\end{equation}
where $\odot$ is the Hadamard product. $\bm F$ can be regarded as the $\bm w_f$ in Equ 1, termed global-aware filter. Finally, the inverse FFT is adopted to transform the modulated spectrum $\tilde{\bm X_r}$ back to the spatial domain and update the tokens: 
\begin{equation}
	{\bm X} \leftarrow \mathcal{F}^{-1}_r[{\tilde{\bm X_r}}].
\end{equation}
The GF layer is capable of modeling the intricate relationship between arbitrary frames of the tokens. As a result, it can selectively capture either local features or global context of the tokens, depending on the type of filter $\bm F$. When a high-pass filter is used, the GF layer tends to capture local features, while a low-pass filter will allow the GF layer to capture global context.

Since both the FFT and the IFFT have no learnable parameters and can process variable-length sequences, it is flexible for the GF layer to be applied in speaker verification. We can simply interpolate the global-aware filter $\bm F$ to $\bm F' \in \mathbb{C}^{C\times \lceil T^{\prime}/2\rceil} $ for different duration utterances, where $T^{\prime}$ is the target length. The interpolation is reasonable due to the property of DFT. Each element of the global-aware filter $\bm F[c, \tau]$ corresponds to the spectrum of the filter at channel $c$, $\omega_\tau=2\pi\tau/T$ and thus, the filter $\bm F$ at channel $c$ can be viewed as a sampling of a continuous spectrum $\bm F(c, \omega_\tau)$, where $\omega_\tau\in[0, 2\pi]$. Hence, changing the duration is equivalent to changing the sampling interval of $\bm F(c, \omega_\tau)$. Therefore, we only need to perform interpolation to shift from one length to another. FFT/IFFT are well supported by GPU and CPU, thus the GF layer is hardward friendly. %thanks to the acceleration libraries like cuFFT

\subsection{Dynamic filtering}
%介绍泛化，由于内容，情感，通道会随着话语改变。用相同的滤波器处理他们效果不好。
Like the spatial domain filter, the global-aware filter is static and cannot adjust its parameters during inference. However, the speaker trait is intertwined with various factors such as speech content, emotion, and channel in the wild speech signal. As a result, the distribution between utterances from the same speaker may vary significantly, making it challenging to handle with the static filters. To address this issue, we propose a dynamic filtering strategy inspired by \emph{Dynamic Convolution} that enables the filters to adapt dynamically to the input \cite{yang2019condconv,chen2020dynamic,li2022omni}, thereby improving its representation and generalization. Specifically, we apply $K$ independent global-aware filters during modulation and combine them using element-wise summation:

\begin{equation}
	\tilde{{\bm X_r}}= {\bm F_1}\odot {\bm X_r} + {\bm F_2}\odot {\bm X_r}+...+{\bm F_K}\odot {\bm X_r}\\
\end{equation}
Then, we utilize a 1D channel attention function to produce a series of dynamic scores $\bm w=[w_1, w_2,...,w_K]$, which can be adapted to the input tokens $\bm X$: %The attention function can be formulated as follows:
\begin{equation}
	{\bm w} = \underbrace{{\tt Softmax(FC_2(ReLU(FC_1(GAP}}_{\rm Attention\ FN}({\bm X}))))) \in\mathbb{R}^{1\times K},
\end{equation}
where ${\tt GAP}$ is Global Average Pooling. ${\tt FC_1}$ and ${\tt FC_2}$ separately represent two fully connected layers. In this work, the number of neurons in $\tt FC_1$ is equal to that in $\tt FC_2$, which is set to $K$. Notably, we utilize Softmax rather than Sigmoid to normalize the scores for stable training. Subsequently, we utilize the dynamic scores $\bm w$ to weight the combination of expert filters, which can be formulated as follows:
\begin{equation}
	\tilde{{\bm X_r}}=w_1{\bm F_1}\odot {\bm X_r}+ w_2{\bm F_2}\odot {\bm X_r}+...+ w_K{\bm F_K}\odot {\bm X_r}\\
\end{equation}
For decreasing computational cost, we apply another equivalent deformation in practice, which is formulated as follows:
\begin{equation}
	\tilde{{\bm X_r}}=(w_1{\bm F_1}+ w_2{\bm F_2}+...+ w_K{\bm F_K})\odot {\bm X_r}\\
\end{equation}
The normalized linear combination of filters is defined as the \emph{dynamic global fiter} (DGF):
\begin{equation}
	{\bm F_d}:= w_1{\bm F_1}+ w_2{\bm F_2}+...+w_K{\bm F_K}\in \mathbb{C}^{K\times C\times\lceil T/2\rceil}
\end{equation}
Finally, the spectrum $\bm X_r$ is modulated by the element-product with the dynamic global-aware filter $\bm F_d$ as introduced in Equ 4. 

\begin{algorithm}[t]
	\caption{Pseudocode of Sparse DGF layer.}
	\begin{algorithmic}	
		\STATE
		\STATE \textcolor{mygreen}{\# $\tt attn\_fn\ \text{:}\ Attention\ functions$}
		%\STATE \textcolor{mygreen}{\# $\tt mm\ \text{:}\ matrix\ multiplication$}
		%\STATE \# {\tt $\tt{mm}$: matrix multiplication}
		%\STATE \# {\tt $\tt{rfft, irfft}$: FFT/IFFT for real signals}
		\STATE \textcolor{mygreen}{\# $\tt x\ \text{:}\  global\ features\ in\ a\ batch\text{,}\ B\ x\ C\ x\ T$}
		\STATE \textcolor{mygreen}{\# $\tt F\ \text{:}\  K\ global\text{-}aware\ filters\text{,}\ K\ x\ C\ x\ T_h$}
		\STATE \textcolor{mygreen}{\# $\tt r\ \text{:}\  sparse\ ratio\text{,}\ \text{[}0\text{,}\ 1\text{]}$}	 	
		\STATE		
		\STATE \textcolor{mygreen}{\# $\tt dynamic\ filtering$}		
		\STATE ${\tt w\; \text{=}\; attn\_fn(x)}$\quad \textcolor{mygreen}{\# ${\tt B\ x\ K}$}
		\STATE ${\tt F\; \text{=}\; reshape(F,[K,\text{-}1])}$
		\STATE ${\tt F_d\; \text{=}\; matmul(w,F)}$\quad \textcolor{mygreen}{\# ${\tt B\ x\ CT_h}$}
		\STATE ${\tt F_d\; \text{=}\; reshape(F_d,[B,C, T_h])}$
		\STATE
		\STATE \textcolor{mygreen}{\# $\tt sparse\ regularization$}		
		\STATE ${\tt m \; \text{=}\;  rand([B,C])>r}$\quad \textcolor{mygreen}{\# ${\tt mask}$}
		\STATE $\tt{ m \; \text{=}\;  expand\_dims(m, [B,C,T_h])}$	
		\STATE ${\tt \lambda\; \text{=}\; mean(abs(F_d))}$
		\STATE ${\tt F_d \leftarrow F_d*m}$
	    \STATE 
	    \STATE \textcolor{mygreen}{\# $\tt forward$}		
		\STATE${\tt x_f \; \text{=}\;  rfft(x, dim\;\text{=}\; 2)}$\quad \textcolor{mygreen}{\# ${\tt B\ x\ C\ x\ T_h}$}
		\STATE${\tt x_f \leftarrow x_f*F_d\;\text{+}\;x_f*\lambda*(1\; \text{-}\; m)}$
		\STATE${\tt x \leftarrow irfft(x_f, dim\; \text{=}\; 2)}$
	\end{algorithmic}
	\label{alg 1}
\end{algorithm}

\subsection{Sparse regularization}

Dynamic filtering is a powerful technique for creating a more robust and generalized representation. However, it poses a challenge in terms of optimization. In dynamic filtering, the optimization object transforms from a single GF to a composition of K GFs, resulting in a more complex loss landscape. This complexity makes it easier for the model to fall into a local minimum and increases the difficulty of optimization. Additionally, dynamic filtering increases the number of parameters, making the model more susceptible to overfitting. To address this issue, we propose a sparse regularization technique to regulate the DGF. Specifically, the DGF can be viewed as a stack of 1D filters, i.e., ${\bm F_d}={\tt Concat}[\bm f_1, \bm f_2, …, \bm f_d]$, where $\bm f_1,…,\bm f_d \in \mathbb{C}^{1\times \lceil T/2\rceil}$. During the training phase, parts of these filters are deactivated via an element-wise multiplication with a random sparse-channel mask $\bm M$, i.e., ${\bm F_s}=\bm M\odot {\bm F_d}$, where ${\bm F_s}$ is defined as the dynamic global-aware filter with sparse regularization (sparse DGF). Notably, to maintain the dynamic characteristic, the mask is directly applied to the DGF rather than to each of the expert filters. Furthermore, during the modulation with sparse DGF, we perform an element-wise summation to preserve the spectrum information at the position of the deactivated filters.

%The dynamic global filter with sparse regularization can be formulated as follows
\begin{equation}
	\tilde{\bm X}_r = {\bm F_s}\odot{\bm X_r}+\lambda_s(1-\bm M)\odot{\bm X_r}
\end{equation}
where $\lambda_s=\frac{1}{CT}\sum^C_{i=0}\sum^T_{j=0}|\bm F_{d}^{(i,j)}|$, which is a scale factor. 
The deactivated filter can be regarded as a scaled all-pass filter. Noteworthily, the sparse regularization is only performed in the training phase. Sparse regularization prevents the model from learning a shortcut and overfitting, thus increasing performance, which can be easily implemented as algorithm 1. 

\subsection{Complexity analysis}
\begin{table}[t]
	\begin{center}
		%\tiny
		\caption{Comparisons of the Dynamic Global-aware Filter with Prevalent Operations in Deep Acoustic Models.}
		\label{Table 1}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{3mm}{
			\begin{tabular}{cl|cc}
				\bottomrule
				Index & Aglorithm & Complexity (FLOPs) & \# Params \\
				\hline
				\hline
		     	N1 & Conv1d     & $\mathcal{O}(kC^2T)$   & $kC^2$\\
				N2 &Res2Conv & $\mathcal{O}(\Delta kC^2T)$ & $\Delta kC^2$ \\
				\iffalse
				N3 & DW Conv1d  & $\mathcal{O}(kCT)$     & $Ck$\\
				N3 &Projection & $\mathcal{O}(C^2T)$     & $C^2$\\
				\fi
				\hline
				\hline
				N3 &Self-Attention & $\mathcal{O}(CT^2+C^2T)$ & $4C^2$\\
				N4 &LSTM           & $\mathcal{O}(C^2T)$      & $8C^2$ \\
				\rowcolor{mygray}	
				N5 &DGF layer & $\mathcal{O}(CT\,{\rm log}\,T+KCT)$ & $KCT_h$ \\
				\toprule
		\end{tabular}}
	\end{center}
\end{table}
%介绍以下TDNN中的常用组件，projection就是dense layer。
To further demonstrate the effectiveness of the DGF layer, we conducted a complexity and parameter analysis comparing it to other popular algorithms used for token mixing in TDNN, including 1D convolution (Conv1d), Res2Conv \cite{desplanques2020ecapa,zhou2021resnext}, Self-Attention \cite{povey2018time,han2019multi}, and LSTM \cite{chen2019speaker,tu2019towards,lin2019lstm,huang2020speaker}. The comparison results are presented in Table 1 (see Appendix B for details), where $C$ and $T$ represent the number of channels and frames of the input tokens, respectively. $k$ denotes the kernel size of the convolution, and $\Delta=(s-1)/s^2$, where $s$ is the number of groups in the Res2Conv1d. $K$ represents the number of experts in the DGF, and $T_h$ is the length of filters in the DGF.

Under commonly used hyperparameters, such as $C\geq 512 \approx 2T_h$, $k\geq3$, and $\Delta=[0.1, 0.2]$, with $K \in [4, 8]$, the DGF layer has a comparable number of parameters to other popular modules. Additionally, when faced with almost arbitrary durations of utterances (${log},T<<C$), the DGF layer outperforms typical global-aware modules, such as self-attention and LSTM, as well as standard local-aware filters, such as 1D convolution and Res2Conv, by requiring fewer FLOPs. 
This makes the DGF layer an efficient choice for speaker verification tasks. %These results demonstrate the efficiency of the proposed DGF layer.

\section{Dual-stream TDNN}
\iffalse
我们认为，ASV中一部分特征就应该是局部的，而一部分特征就应该是全局的。若仅使用全局滤波抽取局部特征，则效果不好。而仅使用局部滤波，则无法抽取全局特征。
\fi

\subsection{Intuitive idea}

In addition to efficiently handling long-range context, it is crucial to consider how to combine local and global features in the model. Existing works perform different-scale operations alternately over the entire feature maps consisting of both local and global features, as shown in Fig. 1b. However, this alternating pattern has two shortcomings. Firstly, applying local filters to extract global context or global filters to extract local features is inefficient. Secondly, to create a more discriminative speaker representation, feature maps are generally maintained at a large size in TDNN, making the operation on the entire feature map computationally expensive. To address this issue, we propose the parallel pattern, as shown in Fig. 1c. In the parallel pattern, different modules are designed to pay attention to complementary features rather than the complete inputs, and the output of each module is concatenated as the result. To fuse the different scale features, we assume that the input consists of an equal number of local features and global context. In this assumption, the channel dimension of the dataflow in the parallel pattern is reduced to half of that in the alternating pattern. Consequently, the complexity of the popular operations applied in TDNN can be reduced from $\mathcal{O}(C^2)$ to $\mathcal{O}(C^2/2)$. Notably, only a simple channel split is required for the disentanglement of the local features and global context without additional modules in the parallel pattern. Based on this idea, we propose the dual-stream (DS-TDNN) model, which applies the DGF layer (as detailed in Section II) as the global module and combines the local features and global context in the parallel pattern.

\iffalse
Quantitatively, the parallel pattern reduces more than half of the complexity compared to the alternating pattern as well as the standard TDNN, which can be calculated as follows:
\begin{equation}
	\mathcal{O}(C^2+C)-\mathcal{O}(C^2/4+C/2)=\mathcal{O}(3C^2/4+C/2), 
\end{equation}
where $C$ is the number of input channels.
\fi


\begin{figure*}[!t]
	\centering
	%\hspace{5mm}
	\includegraphics[width=\linewidth]{model}
	\caption{Overview of the DS-TDNN architecture, where $\oplus$ denotes element-wise summation; $*$ denotes element-wise multiplication; BN denotes 1D BatchNorm; (Proj, C) is a linear projection layer with C hidden neurons; GAP is global average pooling; 3$\times$1 is a 1D convolution with kernel size of 3 and step of 1. }
	\label{fig 2}
\end{figure*}

\subsection{Macro design}
\iffalse
讲清楚两件事：1）具体如何结合局部特征与全局特征（a）block之间 （b）branch之间；为什么
             2）ASP是什么。
             3）交代一下data flow
\fi
The architecture of DS-TDNN is shown in Fig. 2. Overall, DS-TDNN is comprised of two inter-connected branches: a local branch that conducts the Res2Conv module to capture local features, and a global branch that modulates spectrums using the DGF layer in transform domain to handle long-range context. Each branch operates on only half of the input channels in parallel and captures complementary information with different-scale receptive fields. The input of each branch is extracted from 80-dimension Mel spectrogram $\bm X \in \mathbb{R}^{80\times T}$ by a stem that consists of a 1D convolution with a kernel size of 7 and a step of 1, ReLU activation, and 1D BatchNorm:
\begin{equation}
	{\bm X_{l}^0}, {\bm X_{g}^0} ={\tt Split}({\tt stem}({\bm X})) \in \mathbb{R}^{C/2\times T},
\end{equation}
where ${\bm X_{l}^0}$, ${\bm X_{g}^0}$, denote the inputs of the local and global branches, respectively. $C$ represent the numbers of the basic channels in the DS-TDNN. 
In addition to the combination at the ends of the branch, an normalized element-wise summation is also employed to gradually fuse local and global features within the branch. The output of the $i$-th layer in the branch can be formulated as follows:
\begin{equation}
	\begin{aligned}
		&{\bm X_{l}^i}={\tt Block}_{l}^i(0.8{\bm X_{l}^{i-1}}+0.2{\bm X_{g}^{i-1}}) \in \mathbb{R}^{C/2\times T},\\
		&{\bm X_{g}^i}={\tt Block}_{g}^i(0.2{\bm X_{l}^{i-1}}+0.8{\bm X_{g}^{i-1}}) \in \mathbb{R}^{C/2\times T}, 
	\end{aligned}
\end{equation}
where $1\leq i\leq N$. ${\bm X_{l}^i}$, ${\bm X_{g}^i}$ denote the output of the $i$-th local and global block, respectively. ${\tt Block}_l^i$, ${\tt Block}_g^i$ represent the $i$-th local block and global block, respectively. Then, a multi-scale feature aggreation (MFA) \cite{desplanques2020ecapa,liu2022mfa,zhang2022mfa} is perform to fuse different-scale speaker information, which concatnates the output of final blocks together with that of each previous block. The output $\bm H$ of the branch can be defined as follows:
\begin{equation}
{\bm H}={\tt Concat}({\bm X^1_l},{\bm X^2_l},{\bm X^3_l},{\bm X^1_g},{\bm X^2_g},{\bm X^3_g})\in \mathbb{R}^{3C\times T}. 
\end{equation}
Subsequently, an Attentive Statistics Pooling (ASP) \cite{okabe2018attentive} is applied to weight the importance of each frame-level feature $H_t \in \mathbb{R}^{3C\times 1}$ of $\bm H$ and extract the robust speaker embedding for verification, which can formulated as follows: 
%In the next, we will introduce the detail design about local branch and global branch.
\begin{align}
	e_t &= {\bm v}^T{\tt Tanh}({\bm W}H_t +{\bm b})+k,\\
    \alpha_t &= \frac{{\tt exp}(e_t)}{\sum^T_{\tau=1}{\tt exp}(e_\tau)},
\end{align}
where $\bm W\in\mathbb{R}^{3C\times 3C}, \bm v\in \mathbb{R}^{3C\times 1}, \bm b\in \mathbb{R}^{3C\times 1}$ and $k$ are the learnable parameters for ASP. After that, the normalized score $\alpha_t$ is adopted as the weight to calculate the weighted mean vector $\tilde{\bm \mu}$ and weighted standard deviation $\tilde{\bm \sigma}$, which are formulated as:
\begin{align}
	\tilde{\bm \mu}&=\sum^T_{t=1}\alpha_t H_t,\\
	\tilde{\bm \sigma}&=\sqrt{\sum^T_{t=1}\alpha_t H_t\odot H_t-\mu\odot\mu},
\end{align}
where the $\mu=\frac{1}{T}\sum^T_{\tau=1}H_\tau$. The output of the ASP is given by concatenating the vectors of the weighted mean $\tilde{\bm \mu}$ and weighted standard deviation $\tilde{\bm\sigma}$. Finally, the speaker embedding is extracted from a high dimension vector to a low dimension vector with 1D BatchNorm using a linear projection layer.


\subsection{Micro Design}
Each branch consists of three macaron-like blocks. In the blocks, two linear projections sandwich the filter applied for token mixing. The first projection is utilized to disentangle the required local (global) information for the token mixer from the mixture, while the second projection is employed to exchange the channel information.
\subsubsection{Local block} 
%为了保证抽取信息频段的连续性以便于信息融合，我们需要让local block也能提取部分中低频段的信息，让global block也能提取部分中频高频段的信息the local blocks are required to extract multi-scale context rather than focus on high-frequency (i.e., local) features. Therefore
To enhance the capability for local feature representation, \emph{Res2Conv} structure \cite{gao2019res2net,desplanques2020ecapa} is applied in local blocks as the token mixer. As shown in Fig. 2, the Res2Conv is a combination of group convolutions. It firstly splits the input tokens $\bm X_l^i \in \mathbb{R}^{C/2\times T}$ into $s$ groups in channel dimensions:
\begin{equation}
	{\bm X_l^i} = [{\bm X_1}, {\bm X_2},...,{\bm X_s}],
\end{equation}
where ${\bm X_1}, {\bm X_2},...,{\bm X_s}\in\mathbb{R}^{C/2s\times T}$ represent the token groups. Subsequently, a 1D convolution with a kernel size of 3 and a step of 1 is applied in each group, which is followed by ReLU activation and 1D BatchNorm. Notably, since the local block is designed to focus on local features, the dilation rate of the convolutions is set to 1 in this work. Besides, the first group of tokens does not operate by convolution to decrease computational cost. The input of every convolution layer is the sum between the token group and the output of the previous convolution layer:
\begin{equation}
	\tilde{{\bm X}}_{i+1} = {\tt ReLU(\tt BN(\tt Conv}({\bm X_{i+1}}+\tilde{\bm X}_{i}))), i=2,...,s,
\end{equation}
where $\tilde{{\bm X}}_{i+1}$ represents the output of the $(i+1)$-th convolution layer. The output of the Res2Conv is a concatenation of each previous output $\tilde{{\bm X}}_{i+1}, i=1,...,s$, which provides different-scale features extracted from ${\bm X_l^i}$. Finally, the SE module \cite{hu2018squeeze} is applied at the end of each local block.

\subsubsection{Gloabl block} 
As shown in Fig. 2, the structure of global blocks is similar to that of local blocks, where a DGF layer is utilized to replace Res2Conv to capture context information from long-time span. In addition, we perform a simple skip connection rather than the channel attention module used in local blocks, since the Attention FN employed in DGF layer has contained sensitive information about channels.

\begin{table*}[t]
	\begin{center}
		%\tiny
		\caption{Configurations of the Variants of DS-TDNN and Associated ECAPA-TDNN.}
		\label{Table 2}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{3mm}{
			\begin{tabular}{l|cccccc}
				\bottomrule
				Model     &Blocks [Local, Global] &Channels $C$  &Scales $s$  &Experts $K$ &Sparse ratio &\#Params(M)\\
				\hline
				\hline
				ECAPA-c512 & $[1,\ 0]\times 3$ & $[512,\ 0]$      & $[8,\ 8,\ 8]$  &-   &-           & 7.0\\
				DS-TDNN-S  & $[1,\ 1]\times 3$ & $[256,\ 256]$      & $[4,\ 4,\ 4]$  & $[4,\ 4,\ 8]$ & $[0.3,\ 0.1,\ 0.1]$ 
				 & 6.5\\
				\hline
				\hline
				ECAPA-c1024& $[1,\ 0]\times 3$ & $[1024,\ 0]$     & $[8,\ 8,\ 8]$  &-   &-           & 15.5\\
				DS-TDNN-B  & $[1,\ 1]\times 3$ & $[512,\ 512]$     & $[4,\ 4,\ 8]$  & $[4,\ 8,\ 8]$ & $[0.3,\ 0.1,\ 0.1]$ 
				 & 13.2  \\	
				\hline
				\hline
				ECAPA-L   & $[1,\ 0]\times 3$ & $[1280,\ 0]$     & $[8,\ 8,\ 8]$  &- & -          & 21.1 \\
				DS-TDNN-L  & $[1,\ 1]\times 3$ & $[768,\ 768]$   & $[4,\ 8,\ 8]$  & $[8,\ 8,\ 8]$ & $[0.4,\ 0.2,\ 0.2]$
				& 20.5 \\
				\toprule
		\end{tabular}}
	\end{center}
\end{table*}


\subsection{Architecture variants}
To evaluate our model, we developed three variants of the DS-TDNN. The first two variants, i.e., DS-TDNN-S and DS-TDNN-B, have similar hyperparameters as those of the typical ECAPA introduced in \cite{desplanques2020ecapa}. Additionally, we investigated a larger pair of variants of ECAPA and DS-TDNN, i.e., ECAPA-L and DS-TDNN-L, that have similar parameters to enable a fair comparison between TDNN-based models and other large baseline systems, such as transformer-based models and deep residual 2D CNNs. Table 2 provides a summary of the detailed configurations of these variants. As shown in Table 2, due to the efficient DGF layer and the proposed parallel pattern, the FLOPs and parameters of DS-TDNN are less than those of the associated ECAPA, which becomes more apparent as the number of channels increases. Noteworthily, we applied sparse regularization to all variants of the DS-TDNN by default.

\section{Experimental Setup}

\subsection{Database and augmentation}

VoxCeleb1 \& 2 \cite{nagrani2017voxceleb, chung2018voxceleb2}, and SITW \cite{mclaren2016speakers} are used in our experiments. VoxCeleb is an audio-visual dataset consisting of over 2,000 hours of short clips of human speech extracted from interview videos on YouTube. SITW is a widely-used standard evaluation dataset collected from open-source media in real-world conditions, and is made up of 299 speakers, including two testing trials (SITW.Dev and SITW.Eval) that have over 2,800 utterances from 180 speakers. All the systems are trained only on the development set of VoxCeleb2, which has over 1,092,009 utterances at a sampling rate of 16 kHz from 5,994 speakers. A small subset of about 2\% of the data is reserved as a validation set for hyperparameter optimization.

To better illustrate the advantages of global context modeling in different utterance duration conditions, we conducted four trials: VoxCeleb1-O (i.e., Vox1-O), VoxCeleb1-E (i.e., Vox1-E), VoxCeleb1-H (i.e., Vox1-H), and a mixture consisting of SITW.Dev and SITW.Eval (i.e., mix-SITW) for verification performance evaluation. Specifically, VoxCeleb1-O is the test part of VoxCeleb1, which contains 40 speakers with a total of 37,720 test pairs sampled from VoxCeleb1. VoxCeleb1-E is an extension of VoxCeleb1-O, including 1,251 speakers with a total of 581,480 test pairs. VoxCeleb1-H is the more challenging scenario, including 552,536 test pairs where the country and gender of the speakers in each pair are the same. The major durations of utterances in VoxCeleb1 are 5-8 seconds, thus Vox1-O, Vox1-E, and Vox1-H can be regarded as short-duration utterance scenarios. As for SITW, the major durations are about 30–40 seconds. Therefore, the mix-SITW can simulate the long-duration scenario. 
To further investigate how the performance changes as the duration of utterances increases, we randomly clip the test utterance of the mix-SITW with a step size of 5s in the range of 5–50 seconds, and yield four duration cases: $\leq 5$s, $\leq 15$s, $\leq 30$s, and $\leq 50$s, containing a total of 2500 test pairs.
Notably, the training set used in the experiments, i.e., the development set of VoxCeleb2, is completely disjoint from these four evaluation trials. (no speakers in common).

As is well known, data augmentation is generally effective for improving the performance of neural networks. Therefore, we apply six augmentation strategies following the Kaldi recipe \cite{snyder2019speaker} in combination with the publicly available MUSAN dataset \{music, speech, noise\} \cite{snyder2015musan} and the RIR dataset \{reverberation\} \cite{ko2017study}. Specifically, the first five are additive reverberation, additive speech, additive music, additive noise, and a mixture of additive speech and music, respectively. Each of them has an equal probability (0.2), which is randomly selected and applied during the training phase. The final augmentation is SpecAugment \cite{park2019specaugment}, which is applied to all of the training samples. It randomly masks 0 to 5 frames in the time domain and 0 to 10 channels in the frequency domain on the log Mel spectrogram of the utterances.

%重新算
\begin{table*}[!t]
	%\footnotesize
	\caption{Voxceleb EER (\%) and minDCF Results Comparison between Different Models. ‘-c’ Denotes the Number of the Base Channels. The Best Results are Marked in \textbf{BLOD}, the Second are Marked \underline{UNDERLINE}. Our models are Highlighted in \protect\sethlcolor{mygray}\hl{GRAY}.}
	\centering
	\label{Table 3}
	\renewcommand\arraystretch{1.3}
	\setlength{\tabcolsep}{2mm}{
		\begin{tabular}{ll |ccc cc cc cc }
			\bottomrule
			\multirow{2}{*}{Index} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{FLOPs(G)}& \multirow{2}{*}{\#Param(M)} & \multirow{2}{*}{RTF ($\downarrow$)}&  \multicolumn{2}{c}{Vox1-O} & \multicolumn{2}{c}{Vox1-E} & \multicolumn{2}{c}{Vox1-H}\\
			\cline{6-11}
			& & &  & & EER (\%) & minDCF & {EER (\%)} & minDCF & EER (\%) & minDCF \\
			\hline		
			\hline	
			N1  & AST-T 
			&1.1 &7.0  & 0.0071  &1.61  &0.170  &1.98  &0.208  &3.42  &0.296         \\
			N2  & SE-ResNet34      
			&1.2 &6.4  & \underline{0.0053}  &1.15  &0.149  &1.41  &0.166  &2.75  &0.253         \\		
			N3  & ECAPA-c512   
			&1.2 &7.0  & \textbf{0.0046}  &\underline{1.04}&\underline{0.133} &\underline{1.26} &\underline{0.151} &\underline{2.36} &\underline{0.224} \\
			\rowcolor{mygray}
			N4  & DS-TDNN-S    
			&1.0 &6.5  & 0.0058   &\textbf{0.90}  &\textbf{0.118} &\textbf{1.15} &\textbf{0.140} &\textbf{2.11} &\textbf{0.199}\\ 
			
			\hline
			\hline 
			N5  & SE-ResNet50  
			&1.4 &11.9 & 0.0092  &1.05          &0.124          &1.25          &0.156          &2.17          &0.206         \\	  
			N6  & SE-ResNet101   
			&2.6 &17.0 & 0.0149  &0.90          &\underline{0.107} &1.14 &0.143  &\underline{1.94}         &\underline{0.186}\\ 
			N7  & ECAPA-c1024  
			&2.9 &15.5 & \underline{0.0069}  &\underline{0.88} &0.114       &\underline{1.12}&\underline{0.135}&2.08      &0.202         \\	\rowcolor{mygray}
			N8  & DS-TDNN-B  
			&2.1 &13.2 &  \textbf{0.0066}    &\textbf{0.78} &\textbf{0.092} &\textbf{1.06} &\textbf{0.126} &\textbf{1.86} &\textbf{0.174}\\	
			\hline
			\hline
			N9  & AST-S
			&4.4  &22.5 &  0.0134  &1.08  &0.125  &1.40     &0.152  &2.38     &0.216  \\  
			N10 & MFA-Conformer  
			&2.1  &20.8 & \textbf{0.0071}  &\underline{0.70} &\underline{0.087} &\underline{0.99} &\underline{0.120} &\underline{1.64}&\underline{0.158}\\
			N11 & SE-ResNet152
			&3.8  &21.8 & 0.0258    &0.75  &0.094  &1.09     &0.128  &1.82     &0.176  \\	
			N12 & SE-ResNet34-c64 
			&4.7  &23.6 & 0.0132    &0.98  &0.122  &1.21     &0.147  &2.13     &0.196  \\		
			N13 &ECAPA-L   
			& 4.0 & 21.1& 0.0088  & 0.79 & 0.106 & 1.08 & 0.131 & 1.87 & 0.181\\	
			\rowcolor{mygray}	
			N14 & DS-TDNN-L 
			&3.2  &20.5 & \underline{0.0083}  &\textbf{0.64} &\textbf{0.082}   &\textbf{0.93} &\textbf{0.112} &\textbf{1.55} &\textbf{0.149} \\	
			\toprule
	\end{tabular}}
\end{table*}

\subsection{Systems description}

In order to comprehensively evaluate the performance of the proposed DS-TDNN, not only the TDNN-based models, i.e., \emph{ECAPA-TDNN} \cite{desplanques2020ecapa}, but also the 2D CNN-based models, i.e., \emph{SE-ResNet} \cite{chung2020defence,zhao2021speakin,shim2022graph}, and transformer-based models, i.e., \emph{Audio Spectrogram Transformer (AST)} \cite{gong2021ast} and \emph{MFA-Conformer} \cite{zhang2022mfa}, are regarded as baseline systems. The inputs for all systems are 2-second Mel spectrograms with 80 dimensions from a 25ms window with a 10ms frame shift, and the speaker embedding dimension of them is 192. For fair comparison, we make minor revisions to their hyper-parameters. The configurations of the baseline systems are introduced as follows:

\textbf{AST}: It is a fully attention-based model, taking the Mel spectrogram as input and producing speaker embeddings using the ASP on the averaged tokens produced by a transformer encoder. The dimension of its input has been reduced from 128 to 80 in our experiments to make it consistent with other baseline systems. Two variants of AST are applied in our experiments, i.e., AST-tiny (AST-T) and AST-small (AST-S). The AST-T mainly consists of 12 self-attention layers with 3 heads and 192 hidden channels, while the AST-S has 12 self-attention layers with 6 heads and 384 hidden channels.  

\textbf{MFA-Conformer}: It is a hierarchical attention-based model, which introduces convolution to provide the local information, achieveing a state-of-the-art performance for speaker verification. It has 6 macaron-like blocks with 1/2 subsampling rates, where two feed-forward networks (FFN) sandwich the composition of multi-head self-attention (MSA) and convolution. In this work, the FFNs have 2048 hidden units, the MSA has 4 heads and 272 hidden channels, and the convolution has a kernel size of 15 and a step of 1. Besides, the ASP is employed before producing the speaker embeddings.

\textbf{SE-ResNet}: Variants of SE-ResNet have similar structure and hyper-parameters to ResNet while applying channel attention (SE module) to improve speaker verification performance. Notably, the basic channels of SE-ResNet are set to 32 rather than 64 in our experiments. Besides, the subsampling rate of the stem is set to 1/2 instead of the general 1/4 for all variants. In addition, ASP is used in all variants to replace the statistics pooling in our experiments.

\textbf{ECAPA-TDNN} \& \textbf{DS-TDNN}: Three pairs of associated variants, i.e., ECAPA-c512 and DS-TDNN-S, ECAPA-c1024 and DS-TDNN-B, ECAPA-L and DS-TDNN-L, are investigated in our experiments, which are detailed in Table 2.

\subsection{Trianing strategy}
To minimize the duration mismatch between the training and evaluation stages, we employed a two-stage training process. The first stage involved pre-training for 150 epochs, followed by large margin fine-tuning (LM-FT) \cite{thienpondt2021idlab} for 5 epochs. During pre-training ($\sim 35$ hours), all systems are trained using Additive Angular Margin (AAM) loss \cite{deng2019arcface, xiang2019margin}, with the margin and scale set to 0.2 and 30.0, respectively. We use Adam \cite{kingma2014adam} as the optimizer, with an exponentially decreasing learning rate from the initial 0.1 to the final 1e-5. To avoid overfitting, we set the weight decay to 1e-6 and perform a linear warmup for the first 2k steps. During LM-FT ($\sim 1.2$ hours), we increase the duration of training samples to 6 seconds and raise the margin to 0.5. The learning rate here is initialized at 1e-4 and decreased to 2.5e-5. The batch size is set to 512, and all experiments are conducted on 4$\times$NVIDIA RTX A5000.


%补上训练时间


\begin{figure*}[!t]
	\centering
	%\hspace{-5mm}
	\subfloat[]{
		\includegraphics[width=0.30\linewidth]{Improve}
	}
	\subfloat[]{
		\includegraphics[width=0.30\linewidth]{Latency}
	}
	\subfloat[]{
		\includegraphics[width=0.30\linewidth]{Memory}
	}
	\caption{Comparisons among SE-ResNet34-c64\cite{chung2020defence,zhao2021speakin,shim2022graph}, ECAPA-TDNN-L\cite{desplanques2020ecapa}, AST-S \cite{gong2021ast}, MFA-Conformer \cite{zhang2022mfa} and the proposed DS-TDNN-L in (a) Relative improvement of DS-TDNN over other baselines (b) Latency and (c) GPU memory with different utterance durations. The dash lines indicate the models without the special design to handle long-range context. The latency and GPU memory is measured using a single NVIDIA A5000 GPU with batch size 16.}
	\label{fig 3}
\end{figure*}

\subsection{Backend}
Speaker embeddings are extracted from the final fully connected layer for all systems. Trial scores are produced using the cosine distance between embeddings. Subsequently, adaptive score normalization (as-norm) \cite{cumani2011comparison} is used to normalize the trial score. We average the embeddings from the same speaker in the training set to construct the imposter cohort and set the imposter cohort size to 600. Performance will be measured by providing the equal error rate (EER) and the minimum normalized detection cost (minDCF) with $P_{target} = 0.01$ and $C_{FA} = C_{Miss} = 1$. In addition, the real-time factor (RTF) calculated by the Intel Xeon Platinum 8358P (2.60GHz) is also provided to evaluate the inference speed of different models.


\section{Results Analysis}

\subsection{Results on Voxceleb}

The performance comparisons of DS-TDNN and various baseline systems introduced in Section IV.B on Vox1-O, Vox1-E, and Vox1-H are reported in Table 3, which is measured by the equal error rate (EER) and minimum Detection Cost Function (minDCF) together with the number of model parameters, floating point operations (FLOPs), and real time factor (RTF). The utterances in these datasets are truncated to 5 seconds.

\begin{table}[t]
	\begin{center}
		%\tiny
		\caption{Performance in EER (\%) of baselines and proposed DS-TDNN under different utterance scenarios on mix-SITW.}
		\label{Table 4}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{2.0mm}{
			\begin{tabular}{l|ccccc}
				\bottomrule
				\multirow{2}{*}{System}   & \multicolumn{4}{c}{Duration for test utterances}\\
				& $\leq5$s  & $\leq15$s  & $\leq30$s  & $\leq50$s \\
				\hline
				\hline
				SE-ResNet34-c64           &1.22{\tiny $_{\, 0\%}$}  %1.3863
				&0.83{\tiny $_{\, 32\%\downarrow}$}          
				&0.58{\tiny $_{\, 30\%\downarrow}$}          
				&0.54{\tiny $_{\, 7\%\downarrow}$}       \\
				
				ECAPA-L                   &1.01{\tiny $_{\, 0\%}$}     
				&0.63{\tiny $_{\, 41\%\downarrow}$}        
				&0.46{\tiny $_{\, 29\%\downarrow}$}        
				&0.44{\tiny $_{\, 6\%\downarrow}$}     \\
				
				\hline
				\hline
				AST-S                     &1.29{\tiny $_{\, 0\%}$}        %1.3723
				&0.72{\tiny $_{\, 44\%\downarrow}$}        
				&0.49{\tiny $_{\, 32\%\downarrow}$}        
				&0.35{\tiny $_{\, 28\%\downarrow}$}    \\
				
				MFA-Conformer             &\underline{0.91}{\tiny $_{\, 0\%}$}      
				&\underline{0.55}{\tiny $_{\, 40\%\downarrow}$}  
				& \underline{0.39}{\tiny $_{\,28\%\downarrow}$}
				& \underline{0.27}{\tiny $_{\,33\%\downarrow}$} \\	
				
				\rowcolor{mygray}	
				DS-TDNN-L                 &\textbf{0.86}{\tiny $_{\, 0\%}$}
				&\textbf{0.51}{\tiny $_{\, 41\%\downarrow}$}  
				&\textbf{0.35}{\tiny $_{\, 30\%\downarrow}$}   
				&\textbf{0.24}{\tiny $_{\, 32\%\downarrow}$}      \\	
				\toprule
		\end{tabular}}
	\end{center}
\end{table}

The results of experiments N1–N4 demonstrate that the proposed DS-TDNN-S system achieves the best recognition performance among popular tiny baseline systems, although its RTF remains unsatisfactory. Dependent on the $\mathcal{O}(C)$ complexity, the computational cost of DS-TDNN does not increase significantly with the growth of parameters, which provides a better balance between performance and efficiency for the standard variant. As shown in N5–N8, DS-TDNN-B has both the fastest inference speed and the best recognition performance. Specifically, compared with the typical ECAPA-TDNN system with 1024 channels, the proposed DS-TDNN-B system achieves about an 11\% relative improvement in recognition performance with a 15\% relative decline in parameters. This improvement becomes even more significant as the size of the model parameters increases, as shown in experiments N13–N14. As the number of filters increases, the local features extracted by the system gradually become saturated, and the input tokens contain more and more global features of the speech. However, the local-aware filter in ECAPA-TDNN is inefficient at handling the global context, leading to the diminishing marginal utility of its performance. Due to the GF layer, the parallel pattern, and the special design for the combination of different-scale features, DS-TDNN-L has better feature utilization and stronger scalability. As a result, DS-TDNN-L achieves state-of-the-art results among the large baseline systems (N9–N14), outperforming MFA-Conformer with lower EER and minDCF, despite having an acceptable slower inference speed. Besides, the comparison between N11 and N12 indicates that, depth extension is more effective than width extension for the 2D CNNs, emphasizing the advantages of a larger receptive field and global context modeling. 
Moreover, as shown in N9-N10, the introduction of convolution significantly enhances the recognition performance of the conventional transformer, highlighting the importance of local features. Combining the local features and global context, DS-TDNN significantly improves the performance of TDNN in speaker verification while maintaining the efficiency.


\subsection{Results on SITW}

%写清楚与哪些baseline比较。。。介绍好
%写清楚每个subset如何构建。就是选取一个长度区间，然后做random clip。
%每次latency计算

%
In real-world applications, utterances can often exceed 5 seconds in duration, particularly in video processing and real-time online meetings. Therefore, it is crucial to extract robust global features for utterances of varying lengths to ensure accurate speaker verification. Additionally, computational cost is an important consideration when implementing speaker verification algorithms. Low-complexity algorithms generally have lower latency and memory footprints, which are particularly advantageous for processing longer speech segments. To evaluate the effectiveness and efficiency of five typical baseline systems under different duration utterances, i.e., SE-ResNet34-c64, ECAPA-L, AST-S, MFA-Conformer, and the proposed DS-TDNN-L, we make a comprehensive assessment on four duration cases of mix-SITW. The results, including the relative improvement and computational cost, are presented in Fig. 3, while the EER result is reported in Table 4.
\begin{figure*}[!t]
	\centering
	\hspace{-4mm}
	\includegraphics[width=0.93\linewidth]{SR}
	\caption{Visualizations of dynamic filtering and sparse regularization. \emph{Left}: DS-TDNN converges to the optimum along a smooth trajectory. \emph{Right}: The distribution of Hessian max eigenvalue. In the polar coordinate, $r_t=\frac{||\Delta\omega_{t}||}{||\Delta\omega_{\rm init}||}$, and $\theta={\rm cos^{-1}}\left(\frac{\Delta\omega_t\cdot\Delta\omega_{\rm init}}{||\Delta\omega_t||\,||\Delta\omega_{\rm init}||}\right)$, where $\Delta\omega_t=\omega_t-\omega_{\rm optim}$.}
	\label{fig 4}
\end{figure*}

Table 4 demonstrates that the DS-TDNN model performs exceptionally well and exhibits strong generalization capabilities in complex, real-world scenarios. When presented with speech of varying durations in the mix-SITW dataset, DS-TDNN outperforms the four baseline systems. Furthermore, the DS-TDNN together with AST and Conformer show a consistent improvement in performance as speech length increases, highlighting the importance of global context in extracting robust speaker representations from long-term speech. In contrast, SE-ResNet and ECAPA TDNN models have limited receptive fields, making it challenging for them to capture global context information in longer speech segments. As a result, these models struggle to utilize speech samples that are longer than 30 seconds, as shown in Fig. 3a. As for efficiency, DS-TDNN has the lowest inference latency on GPU, thanks to the support of acceleration libraries for FFT/IFFT (such as ${\tt cuFFT}$) and the DGF layer with log-linear complexity. Notably, the computational cost of the AST model increases significantly as the input length increases due to the quadratic complexity of self-attention. To address this issue, the MFA-Conformer model employs a well-designed FFN to down-sample the feature map. However, its inference speed still becomes slower than that of the other three baseline systems when the input exceeds 30 seconds (as shown in Fig. 3b). Additionally, Fig. 3c shows that DS-TDNN’s memory usage is comparable to that of ECAPA-TDNN and MFA-Conformer, which is higher than that of SE-ResNet and lower than that of AST. Overall, DS-TDNN strikes an impressive balance between recognition performance and computational cost, making it a promising option for real-life application.

\iffalse
Firstly, due to the small receptive fields of ECAPA and ResNetSE34-c64, it is difficult for them to utilize speech lasting over 30 seconds. For the algorithm that models the global context, the recognition rate will increase significantly when the speech length increases, which illustrates the advantages of global modeling. However, since the self-attention applied in the conventional transformer has $\mathcal{O}(T^2)$ complexity to the input tokens, the RTF of AST will increase exponentially as the length of the speech increases and become unaffordable. In the face of speeches longer than about 30 seconds, AST has been unable to achieve real-time processing. Secondly, although MFA-Conformer uses convolution for downsampling to reduce complexity, the inference speed becomes lower than DS-TDNN when facing about 50-second speeches due to the influence of the $\mathcal{O}(T^2)$ complexity of self-attention, which reveals the advantage of the proposed dynamic global-aware filter. Thirdly, DS-TDNN has a similar inference speed to popular models that do not have the ability of global context modeling, achieving the best trade-off between the inference speed and recognition performance when facing long-duration speech. In addition, the EER of DS-TDNN is always lower than other baseline systems under various durations of speech, which fully illustrates the superiority of our proposed DS-TDNN.

补充细节。。。。
\fi
\begin{figure*}[!t]
	\centering
	%\hspace{-5mm}
	\includegraphics[width=\linewidth]{visualGF2}
	\caption{Visualizations of global-aware filters. \emph{Left}: the amplitude-frequency response of the former 64 filters in GF/DGF layers, where the vertical axis represents the log amplitude and the horizontal axis represents the frequency. \emph{Right}: the distribution of the center frequency (C.F.) of all the filters in the global-aware filter layers and the numbers of each type of filter.}
	\label{fig 5}
\end{figure*}

\subsection{Analysis and visualization}

\subsubsection{Optimization}

In order to investigate the effect of the dynamic filtering strategy and sparse regularization on optimization, we plot the smooth optimization trajectory of the model using different filtering strategies in polar coordinates, where the radius $r_t$ is defined as the normalized distance between the current weights $\omega_t$ and the optimum weights $\omega_{\rm optim}$, and the angle represents the direction of optimization. In addition, we also study the distribution of the Hessian eigenvalue of the weights in the dynamic/static GF layers, which can reflect the local convexity of the loss function and reveal the difficulty of optimization. 

As shown in Fig. 4, the optimization trajectory of the model becomes significantly sharper after adopting the dynamic filtering, and there is a period of optimization in the opposite direction, indicating that the dynamic filtering will increase the difficulty of optimization. After sparse regularization, the optimization trajectory becomes smooth, and the path of opposite optimization is also greatly reduced, indicating that sparse regularization is helpful for model optimization. One possible reason is that the sparse regularization reduces the variance of gradients for mini-batches and allows the model to update in a more consistent direction. The direction is also more consistent with the correct gradient direction, which prevents the model from falling into the saddle point. This conclusion can also be proven from the distribution of the Hessian eigenvalue of the weights. Specifically, the Hessian matrix of DGF weights has negative eigenvalues in early epochs, which indicates that the loss function is non-convex and the model is more likely to fall into saddle points. Besides, the Hessian eigenvalues of DGF are smaller than those of GF, which represents a smaller gradient, making it difficult for the model to determine the direction of optimization. Notably, the proposed sparse regularization significantly suppresses the negative eigenvalues in the Hessian matrix of DGF weights and increases the values of the eigenvalues, which avoids the model from falling into the saddle points and provides a larger gradient using the same loss function, proving the effectiveness of sparse regularization. Interestingly, although sparse regularization brings a more significant improvement for DGF, it also benefits GF.


\subsubsection{Digital signal processing} 

In this part, we explore how the proposed GF affects input tokens. Besides, we explain the effectiveness of the dynamic filtering and sparse regularization for enhancing the GF from the aspect of digital signal processing. For the DGF, we set $\omega_1 = \omega_2 = ... = \omega_K = 1/K$.

Firstly, numerous works have proven that the modules for global context modeling, e.g., self-attention, are equivalent to low-pass filters of the token spectrum, while the local feature modeling algorithms like convolution trend to be high-frequency filters \cite{parkvision,cordonnierrelationship,siinception}. As shown in Fig. 5, both the GF and DGF are mainly composed of low-pass filters, which proves that they mainly capture the long-term context rather than the local textures of the input tokens. In addition, it also illustrates that the global-aware filtering is more resistant to high-frequency noise (i.e., short-term stationary speech signal) than the convolution operation, which reveals why DS-TDNN can capture more robust speaker representation from long-duration utterances than conventional TDNN. Secondarily, it is interesting that there are other types of filters, e.g., band-pass and high-pass filters, in the GF/DGF in addition to the low-pass filter. This shows that although the proposed GF/DGF mainly works by emphasizing the low-frequency features of the spectrum, it also has the ability to capture arbitrarily high-frequency features that correspond to the local textures in the spatial domain. Besides, from the comparison between GF and DGF, the number of high-pass and band-pass filters in DGF is significantly lower than that of GF. More low-pass filters generally indicate the stronger ability of global context modeling, which explains that the proposed dynamic filtering enhances the recognition performance by providing the model with more powerful global context representations. Notably, there are some all-pass filters in the GF/DGF. Since these filters cannot influence the spectrum, we call them inactive filters (inact). In our opinion, the reason these inactive filters exist is because they are optimized incompletely. It is easy to observe that the number of inactive filters is significantly reduced after sparse regularization. As the number of inactive filters decreases, the overall center frequency of GF/DGF is shifted to the lower region, which is another evidence that sparse regularization can help the model optimize better. Finally, the last two layers have a significantly higher utilization of filters than the first layer, which is reflected by the fewer inactive filters. Besides, as the depth increases, the number of low-pass filters increases, which is consistent with the characteristic of neural networks that the deeper layer has larger receptive fields.
\begin{table}[t]
	%\footnotesize
	\caption{Part 1 of the Ablation Study. }
	\centering
	\label{Table 5}
	\renewcommand\arraystretch{1.5}
	\setlength{\tabcolsep}{1.2mm}{
		\begin{tabular}{l| l l l l}
			\bottomrule
			&FLOPs(G) &  \#Params(M)  &EER(\%) &minDCF  \\
			\hline			
			\hline
			DS-TDNN-B                                     & 2.1{\tiny  $_{\,  0\%}$}     
			& 13.2{\tiny  $_{\,  0\%}$}
			& 0.78{\tiny $_{\, 0\%}$}    
			& 0.092{\tiny $_{\, 0\%}$} \\
			\hline	
			MSA$\rightarrow$ DGF layer       		      & 2.6{\tiny  $_{\,  \textcolor{red}{24\%\uparrow}}$}   
		    & 13.7{\tiny $_{\, 4\%\uparrow}$}
			& 0.79{\tiny $_{\, 1\%\uparrow}$}    
			& 0.097{\tiny$_{\, 5\%\uparrow}$}                          \\
			LSTM$\rightarrow$ DGF layer       			  & 3.4{\tiny  $_{\,  \textcolor{red}{62\%\uparrow}}$}   
			& 17.6{\tiny $_{\, \textcolor{red}{44\%\uparrow}}$}
			& 0.84{\tiny $_{\, 8\%\uparrow}$}    
			& 0.108{\tiny$_{\, 17\%\uparrow}$}                          \\
			Bi-LSTM$\rightarrow$ DGF layer       		  & 4.9{\tiny  $_{\,  \textcolor{red}{133\%\uparrow}}$}   
			& 24.7{\tiny $_{\, \textcolor{red}{87\%\uparrow}}$}
			& 0.81{\tiny $_{\, 4\%\uparrow}$}    
			& 0.103{\tiny$_{\, 12\%\uparrow}$}                          \\
			\toprule 
\end{tabular}}
\end{table}




\subsection{Ablation study}

To determine the contribution of each component in the DS-TDNN, we conduct a detailed ablation study divided into four parts. In Part 1, we replace the DGF layer with three typical global-aware algorithms, i.e., multi-head self-attention (MSA), long short-term memory (LSTM), and bidirectional LSTM (Bi-LSTM), to assess its effectiveness. In Part 2, we first investigate the macro designs of DS-TDNN, i.e., the combination of local features and global context, element-wise summation for different-scale information exchange, and multi-scale feature aggregation (MFA). Then, we evaluate the effects of dynamic filtering and Res2Conv on recognition performance, respectively. In Part 3, we quantitatively assess the impact of sparse regularization on dynamic global-aware filters and static filters. Finally, in Part 4, we explore the scalability of DS-TDNN in terms of its depth and width, respectively. Besides, we compare the proposed parallel pattern with the conventional alternating pattern for feature combinations. Notably, we only present the results on VoxCeleb1-O, as shown in Tables 5–8, while the results in other sets exhibit the same trend.

\subsubsection{Part 1} 
As discussed in Section II. E, the DGF layer has the lowest complexity, which is consistent with the results reported in Table 5. Replacing the DGF layer with the other three modules will significantly increase the computational cost and parameters, particularly for the LSTM and Bi-LSTM. Notably, MSA performs almost as well as DGF in terms of performance, but it has a slightly higher computation cost. Hence, DGF can be considered as an efficient alternative to MSA. Additionally, the context of speaker information is generally undirected, which is quite different from that of language and speech. As a result, the LSTM designed for unidirectional sequence can only achieve suboptimal performance for speaker representation. This issue is mitigated in Bi-LSTM. However, Bi-LSTM is required to preserve bidirectional information flow, resulting in significantly higher computational costs and parameters.

\begin{table}[t]
	%\footnotesize
	\caption{Part 2 of the Ablation Study. \protect\sethlcolor{mygray}\hl{GRAY} Denotes the DS-TDNN-B}
	\centering
	\label{Table 6}
	\renewcommand\arraystretch{1.5}
	\setlength{\tabcolsep}{1.2mm}{
		\begin{tabular}{l| l l l l}
			\bottomrule
			&FLOPs(G) &  \#Params(M)  &EER(\%) &minDCF  \\
			\hline			
			\hline
			ECAPA-c1024               & 2.9{\tiny  $_{\, 0\%}$}              & 15.5{\tiny  $_{\, 0\%}$} 
			& 0.88{\tiny $_{\, 0\%}$}              & 0.114{\tiny $_{\, 0\%}$}           \\	
			\hline
			Blocks $= [2, 0]\times 3$ & 2.2{\tiny  $_{\,   \textcolor{red}{35\%\downarrow}}$}   
			& 12.2{\tiny $_{\,  \textcolor{red}{21\%\downarrow}}$}
			& 0.95{\tiny $_{\, 8\%\uparrow}$}      & 0.127{\tiny $_{\, 12\%\uparrow}$}  \\
			Blocks $= [0, 2]\times 3$ & 2.1{\tiny  $_{\,  5\%\downarrow}$}   & 14.3{\tiny  $_{\,  17\%\uparrow}$}
			& 1.07{\tiny $_{\, 13\%\uparrow}$}     & 0.135{\tiny $_{\, 6\%\uparrow}$}   \\
			Blocks $= [1, 1]\times 3$ & 2.1{\tiny  $_{\,  2\%\downarrow}$}   & 13.2{\tiny  $_{\,  8\%\downarrow}$}
			& 0.84{\tiny $_{\,  \textcolor{red}{21\%\downarrow}}$}   
			& 0.098{\tiny$_{\, \textcolor{red}{27\%\downarrow}}$}                       \\
			\rowcolor{mygray}						      
			+  Info exchange          & 2.1{\tiny  $_{\,  0\%\uparrow}$}     & 13.2{\tiny  $_{\,  0\%\uparrow}$}
			& 0.78{\tiny $_{\, 7\%\downarrow}$}    & 0.092{\tiny $_{\, 6\%\downarrow}$} \\
			- MFA       			  & 1.5{\tiny  $_{\,  \textcolor{red}{29\%\downarrow}}$}   
			& 10.0{\tiny $_{\, \textcolor{red}{24\%\downarrow}}$}
			& 0.92{\tiny $_{\, 18\%\uparrow}$}    
			& 0.119{\tiny$_{\,\textcolor{red}{29\%\uparrow}}$}                          \\		
			\hline 
			\hline
			w/o Dynamic filtering        & 1.8{\tiny  $_{\, 0\%}$}              & 11.4{\tiny  $_{\, 0\%}$}   
			& 0.83{\tiny $_{\, 0\%}$}  		     & 0.102{\tiny $_{\, 0\%}$}           \\
			Width $\uparrow$ (1152)      & 2.4{\tiny  $_{\, 33\%\uparrow}$}     & 13.2{\tiny  $_{\, 16\%\uparrow}$}  
			& 0.81{\tiny $_{\,  3\%\downarrow}$}     & 0.095{\tiny $_{\, 7\%\downarrow}$}   \\
			\hline
			$k = [4,4,4]$             & 2.0{\tiny  $_{\,  11\%\uparrow}$}    & 12.6{\tiny  $_{\, 11\%\uparrow}$} 
			& 0.80{\tiny $_{\,  4\%\downarrow}$}   & 0.096{\tiny $_{\, 6\%\downarrow}$} \\
			$k = [4,4,8]$         	  & 2.0{\tiny  $_{\,  2\%\uparrow}$}     & 12.9{\tiny  $_{\, 2\%\uparrow}$} 
			& 0.79{\tiny $_{\,  1\%\downarrow}$}   & 0.095{\tiny $_{\, 0\%\downarrow}$} \\
			\rowcolor{mygray}	
			$k = [4,8,8]$         	  & 2.1{\tiny  $_{\,  3\%\uparrow}$}     & 13.2{\tiny  $_{\, 2\%\uparrow}$}
			& 0.78{\tiny $_{\,  1\%\downarrow}$}   & 0.092{\tiny $_{\, 3\%\downarrow}$} \\
			$k = [8,8,8]$        	  & 2.2{\tiny  $_{\,  4\%\uparrow}$}     & 13.7{\tiny  $_{\, 4\%\uparrow}$}  
			& 0.79{\tiny $_{\,  1\%\uparrow}$}     & 0.093{\tiny $_{\, 1\%\uparrow}$}   \\
			
			\hline
			\hline
			w/o Group Conv      	  & 2.5{\tiny  $_{\,  0\%}$}    	     & 13.4{\tiny  $_{\, 0\%}$}  
			& 0.81{\tiny $_{\,  0\%}$}  			 & 0.098{\tiny $_{\, 0\%}$}           \\
			\iffalse
			DW Conv      	          & 2.5{\tiny  $_{\,  0\%}$}    	     & 13.4{\tiny  $_{\, 0\%}$}  
			& 0.81{\tiny $_{\,  0\%}$}  			 & 0.098{\tiny $_{\, 0\%}$}           \\
			\fi
			\hline
			$s = [8,8,8]$         	  & 2.1{\tiny  $_{\,  16\%\downarrow}$}  & 13.0{\tiny  $_{\, 3\%\downarrow}$}   
			& 0.80{\tiny $_{\,  1\%\downarrow}$}   & 0.094{\tiny $_{\, 4\%\downarrow}$} \\
			
			$s = [4,8,8]$        	  & 2.1{\tiny  $_{\,  1\%\uparrow}$}  	 & 13.1{\tiny  $_{\, 1\%\uparrow}$}  
			& 0.79{\tiny $_{\,  1\%\downarrow}$}   & 0.093{\tiny $_{\, 1\%\downarrow}$} \\
			\rowcolor{mygray}	
			$s = [4,4,8]$         	  & 2.1{\tiny  $_{\,  1\%\uparrow}$}     & 13.2{\tiny  $_{\, 1\%\uparrow}$}   
			& 0.78{\tiny $_{\,  1\%\downarrow}$}   & 0.092{\tiny $_{\, 1\%\downarrow}$} \\
			$s = [4,4,4]$        	  & 2.2{\tiny  $_{\,  1\%\uparrow}$}     & 13.2{\tiny  $_{\, 2\%\uparrow}$}   
			& 0.78{\tiny $_{\,  0\%\uparrow}$}     & 0.092{\tiny $_{\, 1\%\uparrow}$}\\
			\toprule 
	\end{tabular}}
\end{table}
\subsubsection{Part 2} 
%补充不使用dynamic 但增加参数量 以公平比较。
As shown in Table 6, the first group of experiments illustrates that every macro design in DS-TDNN is important for improving performance in recognition. Specifically, due to the $\mathcal{O}(C^2)$ complexity of Res2Conv and linear projections, the channel split design can significantly reduce the complexity of the model with an affordable decline in recognition performance. Notably, the model cannot achieve good recognition performance using only local or global branches, while local branches are more helpful for recognition performance than global branches. The reason is that local features usually contain information about speaker voiceprints, which is essential for recognition. However, combining local branches with global branches can further provide a significant improvement in the recognition performance with little increase in both complexity and model parameters. Besides, the element-wise summation at the end of each block improves recognition performance without increasing complexity, indicating the necessity for the fusion of different receptive field information and the effectiveness of the proposed strategy for information exchange between local features and global context. Finally, the MFA proposed in ECAPA is also greatly useful for DS-TDNN, even though it obviously increases the complexity. The second group of experiments highlights the necessity of dynamic filtering. Although dynamic filtering increases the parameters, it is more efficient in enhancing the recognition performance than directly extending the static GFs in width. Additionally, the performance of dynamic filters can be slightly improved by increasing the number of expert filters. However, more expert filters also increase the optimization difficulty, making the model prone to overfitting. Thus, this improvement gradually declines as the number of expert filters grows, eventually becoming lower than the impact of dynamic filtering on complexity. Furthermore, deeper layers tend to require more expert filters than shallower layers due to their higher filter utilization. The third group of experiments demonstrated that multi-scale group convolution, i.e., the Res2Conv structure, can reduce complexity and parameters while improving performance. This finding indicates that although the DS-TDNN has a special design for global context modeling, multi-scale information is still crucial for learning a powerful speaker representation.


\begin{table}[t]
	%\footnotesize
	\caption{Part 3 of the Ablation Study. \protect\sethlcolor{mygray}\hl{GRAY} Denotes the DS-TDNN-B}
	\centering
	\label{Table 7}
	\renewcommand\arraystretch{1.5}
	\setlength{\tabcolsep}{1.8mm}{
		\begin{tabular}{cc| c c}
			\bottomrule
			Experts $K$  &Sparse ratio &EER(\%) &minDCF  \\
			\hline
			\hline
			$[4,\ 8,\ 8]$& w/o SR             & 0.82  & 0.098\\
			$[4,\ 8,\ 8]$&$[0.3,\ 0.3,\ 0.3]$ & 0.88  & 0.106\\
			$[4,\ 8,\ 8]$&$[0.3,\ 0.3,\ 0.1]$ & 0.83  & 0.096\\
			\rowcolor{mygray}	
			$[4,\ 8,\ 8]$&$[0.3,\ 0.1,\ 0.1]$ & 0.78  & 0.092\\
			$[4,\ 8,\ 8]$&$[0.2,\ 0.1,\ 0.1]$ & 0.79  & 0.093\\
			\hline
			\hline
			-            &w/o SR              & 0.84 & 0.105\\
			-            &$[0.3,\ 0.3,\ 0.3]$ & 0.91 & 0.117\\
			-            &$[0.3,\ 0.3,\ 0.1]$ & 0.86 & 0.109\\
			-            &$[0.3,\ 0.1,\ 0.1]$ & 0.82 & 0.102\\
			-            &$[0.2,\ 0.1,\ 0.1]$ & 0.82 & 0.104\\
			\toprule 
	\end{tabular}}
\end{table}


\subsubsection{Part 3}
As shown in Table 7, proposed Sparse Regularization (SR) can enhance the performance of both dynamic global-aware filters and static global-aware filters, while being more efficient for the dynamic global-aware filters. Besides, a higher sparse rate does not always provide better performance, and an excessively high sparse rate will lead to an obvious decline in recognition performance. The optimum sparse rate is approximately correlated to the number of inactive filters in the layer before sparse regularization. Therefore, the shallow layers generally require a higher sparsity rate for regularization.

\subsubsection{Part 4}:
Table 8 reveals that DS-TDNN has excellent scalability in both width and depth. Besides, the extension of DS-TDNN in width is more efficient than in depth. This is because the DGF layer applied in DS-TDNN can provide the global context in the shallow layer, which makes it unnecessary to enlarge the receptive field by stacking more hidden layers in depth. Additionally, DS-TDNN has a bottleneck channel that is equivalent to half of the basic channels, which determines the number of extracted features and necessitates a sufficient width. We also consider two variants constructed in an alternating pattern for assessment of the parallel pattern. The first is AP (2), which has three hidden layers, and the second is the global module. The second is AP (2, 4), which has five hidden layers, and the second and fourth are the global modules. To ensure fair comparisons, we directly apply the local block and global block of DS-TDNN in the alternating pattern variants to capture different-scale features. The channels in each layer of the alternating models are doubled compared to those in parallel models. As shown in Table 8, the alternating mode effectively combines local and global features. However, as discussed in Section III.A, the operations in an alternating pattern are performed on a larger feature map, increasing the computational cost.

\begin{table}[t]
	\begin{center}
		%\tiny
		\caption{Part 4 of the Ablation Study. \protect\sethlcolor{mygray}\hl{GRAY} Denotes the DS-TDNN-B}
		\label{Table 8}
		\renewcommand\arraystretch{1.5}
		\setlength{\tabcolsep}{1mm}{
			\begin{tabular}{l|ccccccc}
				\bottomrule
				&Layers & Channels&FLOPs(G)&\#Params(M) &EER(\%)   &minDCF \\
				\hline
				\hline
				\rowcolor{mygray}	
				Basic             & $3$   & $1024$  & 2.1    & 13.2       & 0.78     & 0.092 \\	
				\hline
				Width $\uparrow$  & $3$   & $1280$  & 2.8    & 17.1       & 0.70     & 0.088 \\
				Width $\uparrow$  & $3$   & $1536$  & 3.2    & 20.5       & 0.64     & 0.082 \\
				\hline
				Depth $\uparrow$ & $4$   & $1024$  & 2.9    & 17.0       & 0.74     & 0.090 \\
				Depth $\uparrow$ & $5$   & $1024$  & 3.2    & 20.8       & 0.72     & 0.087 \\
				\hline
				\hline
				AP (2)    & $3$   & $1024$  & 2.9    & 15.6       & 0.81     & 0.102 \\
				AP (2,4)  & $5$   & $1024$  & 4.5    & 24.1       & 0.70     & 0.085 \\
				\toprule
		\end{tabular}}
	\end{center}
\end{table}


\section{Conclusion}

In this paper, we propose a novel global-aware filter (GF) layer to capture long-term context in utterances. The GF layer has global receptive fields while maintaining log-linear complexity. Additionally, we propose dynamic filtering and sparse regularization to enhance the GF layer, which improves its representation and generalization and prevents overfitting. We also construct a Dual-Stream Time-Delay Neural Network (DS-TDNN) by incorporating the GF layer. The DS-TDNN disentangles the local features and global context, then refines them in a proposed parallel pattern with several carefully designed strategies. Experiments on the Voxceleb datasets demonstrate that DS-TDNN achieves a relative 10\% improvement with a relative 28\% and 15\% decline in complexity and parameters over ECAPA-TDNN for ASV in short-duration utterances. Moreover, it outperforms SE-ResNet, AST, and MFA-Conformer on an approximate parameter scale. Experiments on the SITW datasets reveal that the explicit modeling for global context in DS-TDNN further boosts its relative improvements over ECAPA-DTNN and SE-ResNet as the utterance duration increases. Besides, for extremely long utterances (over 50 seconds), DS-TDNN offers the best trade-off between performance and computational cost, highlighting the advantages of both the GF layer and the designs applied in DS-TDNN. This study explores the potential of TDNN to combine local features and global context in speaker verification. It also provides some inspiring ideas and fundamental observations for future deep speaker embedding system design.




\iffalse
\section*{Acknowledgments}
This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.
\fi

\iffalse
\section{Biography Section}
If you have an EPS/PDF photo (graphicx package needed), extra braces are
 needed around the contents of the optional argument to biography to prevent
 the LaTeX parser from getting confused when it sees the complicated
 $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
 your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
 simpler here.)
 
\vspace{11pt}

\bf{If you include a photo:}\vspace{-33pt}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
Use the author name as the 3rd argument followed by the biography text.
\end{IEEEbiography}

\vspace{11pt}

\bf{If you will not include a photo:}\vspace{-33pt}
\begin{IEEEbiographynophoto}{John Doe}
Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
\end{IEEEbiographynophoto}
\fi



\vfill
{\appendix
A. Proof of Discrete Convolution Theorem
\iffalse
$x_i, 0<i<N-1$ 
\begin{equation}
	\tilde{x}_i =\sum_{\Omega_i} x_j w_j, x_j\in\Omega_i
\end{equation}

\begin{equation}
	X_k=\sum_{i=0}^{N-1}x_i m_{ik},
\end{equation}


\begin{equation}
	\tilde{X}_k = X_k l_k= l_k\sum_{j=0}^{N-1}x_j f_{kj},
\end{equation}


\begin{equation}
	\begin{aligned}
		\tilde{x}_i&= \frac{1}{N}\sum_{k=0}^{N-1}\tilde{X}_k  f^*_{kj},\\
		&= \frac{1}{N}\sum_{k=0}^{N-1}l_k\sum_{j=0}^{N-1}x_j f_{kj}f^*_{kj},\\
		&= \sum_{k=0}^{N-1}\sum_{j=0}^{N-1}x_j\frac{l_k}{N}
	\end{aligned}
\end{equation}
\fi

B. Calculation of complexity
	
	
}
\bibliographystyle{IEEEtran}
\bibliography{mybib}

\end{document}


