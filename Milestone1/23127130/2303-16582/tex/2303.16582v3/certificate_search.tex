

\section{Certificate Search}
\label{sec:certificate-search}

In this section, we will discuss possibilities for search strategies by defining for every search tree node labeled with tuple  $\tau$, the ordered sequence $\Children{\tau}$ of choices for the next tuple element. Our framework allows for many more possibilities from which we choose strategies that both demonstrate the applicability of the framework to different search strategies, and allow for efficient search, as will be demonstrated by the computational experiments in Section~\ref{sec:experiments}.

In order to be able to refer to different variants of the search strategy in the description of computational experiments, we will introduce keywords for those variants that we will write in teletype font.

Here we will focus on strategies of the two following basic types: 
\begin{itemize}
\item Filtering: We skip elements from the set of possible choices that cannot result in a certificate or for which the probability of resulting in a certificate is negligible.
\item Ordering: We choose elements from the set of possible choices in a certain order that tries the reflect the probability of resulting in a certificate.
\end{itemize}

\subsection{Points}

The points $\Children{}=\langle p_1,\dots, p_k\rangle$ determining the first level of the search tree are generated by an optimization problem defined on the formula $\phi$
%in a way already described earlier
following the Logic-To-Optimization approach~\cite{ATVApaper}. Here we translate the satisfiability problem into a numerical minimization problem, mapping the logic formula $\phi$ into the non-negative real-valued function $\LtoO(\phi) \equiv H: \mathbb{R}^n\rightarrow\mathbb{R}_{\geq 0}$
(called the \textit{objective function}) such that for every satisfying assignment, this objective function is zero, and for assignments that do not satisfy the formula, the objective function is
typically (but not always) non-zero. Then we find local minima of $H$ through an unconstrained optimization algorithm such as basin hopping~\cite{basinhoppin}, a two-phases Monte Carlo Markov Chain method that alternates local minimization with random jumps. In our implementation, we compute $k=100$ local minima, and process them in the order of their value.


\subsection{Literals}


Given a point $p$, we now discuss how choose literal selector functions $\Children{p}=\langle \sigma_1,\dots, \sigma_k\rangle$.
For filtering the set of literal selector functions, we will restrict ourselves, 
for each clause $C\in\phi$, to the literals $l$ for which the objective function restricted to $l$ and evaluated in the point~$p$ is below a certain threshold. That is, we determine the set of approximately satisfiable literals $$L_C := \{l\in C \mid \LtoO(l)(p)\leq \epsilon  \}.$$ 
Our literal selector functions will then correspond to the set of all approximately satisfiable combinations $$\{\sigma \mid \text{for all } C\in\phi, \sigma(C)\in L_C \},$$ that is, each $\sigma$ selects exactly one approximately satisfiable literal from each clause. In order to maximize the chances of choosing a better literal combination, 
%we can sort each $L_C$
we can sort the elements of $L_C$
according to the value of the respective objective functions and then choose literal combinations using the corresponding lexicographic order (we will refer to this heuristic as \sortWrtCost).

While the point $p$ is usually a good candidate in terms of \emph{distance from a zero}, it can sometimes lead to an inconsistent problem:
\begin{example}Consider the formula	
	\begin{alignat*}{2}
		& \qquad \qquad \qquad \qquad \qquad \qquad \phi := C_1 \land C_2  \\
		& C_1 \equiv (x+y=0) \lor (x=\e^{10^6*y})
		&& C_2 \equiv (x+y \geq \epsilon_1) \lor (x=\tan(y+\epsilon_1)) 
	\end{alignat*}  	
	The numerical optimizer will be tempted to return first some point $p_1$ such as $\{x\mapsto 1; y\mapsto -1\}$, that \emph{almost} satisfies $(x+y=0) \land (x+y \geq \epsilon_1)$, instead of a harder approximate solution involving transcendental functions and heavy approximations, such as $(x=\e^{10^6*y}) \land (x=\tan(y+\epsilon_1))$, that is exactly satisfiable in a point $p_2$ near $(0, -\pi)$.
\end{example}


Such inconsistencies may occur in many combinations of literals. We use a strategy that detects them in situations where for certain clauses $C$, the set $L_C$ contains only one literal $l$. We will call such a literal $l$ a \emph{forced literal}, since, for every literal selector function~$\sigma$, $\sigma(\phi)$ will include $l$. Before starting to tackle every approximately satisfiable literal combination, we first analyze the set of forced literals. We do symbolic simplifications (such as rewriting and Gaussian elimination) to check whether the set has inconsistencies that can be found at a symbolic level (as in the previous example). If the symbolic simplifications detect that the forced literals are inconsistent then we set $\Children{p}$ to the empty sequence $\langle\rangle$ which causes backtracking in depth-first search. We refer to the variant of the algorithm using this check as \checkForcedLiterals. 


\ 

\emph{Filtering out over-constrained systems}. Given a literal selector function~$\sigma$, we analyze the structure of the system of equations formed by the equations selected by $\sigma$ through the Dulmage–Mendelsohn decomposition, that uniquely decomposes the system into a well-constrained subsystem, an over-constrained subsystem and an under-constrained subsystem.
We filter out every literal combination having a non-empty over-constrained subsystem, since this leads to a non-robust sub-problem, referring to this heuristic as \filterOverconstr.


%\subsection{Reduction to square system}
%\subsubsection{Instantiations}
\subsection{Instantiations}

\label{subsec:instantiations}
We define the instantiations $\Children{p, \sigma}=\langle \nu_1,\dots,\nu_k\rangle$ based on a sequence of sets of variables $V_1,\dots,V_k$ to instantiate, and define $\nu_i:= \mathit{proj}_{V_i}(p)$. The uninstantiated part of $p$ after projection to a set of variables $V_i$ is  then $\mathit{proj}_{\Vars{\phi}\setminus V_i}(p)$, which we will denote by $p_{\neg V_i}$.

For searching for the variables to instantiate, we use the Dulmage–Mendelsohn decomposition constructed in the previous level of the hierarchy. We do not want to instantiate variables appearing in the well-constrained sub-system, since doing so would make the resulting system after the instantiation over-constrained. Hence the variables to be instantiated should be chosen only from the variables occurring in the under-constrained subsystem. This substantially reduces the number of variable combinations that we can try. 
%Moreover, if the under-constrained subsystem is not connected, we can find its connected components to further reduce the number of feasible assignments.  
Denoting the variables satisfying this criterion by $V_{under}$, this restricts $V_i\subseteq V_{under}$, for all $i\in \{1, \dots, k\}$. 
This does not yet guarantee that every chosen variable combination leads to a well-constrained system after the instantiation. For example
, the under-determined system of equations  $x+y = 0 \land z+w=0$ has four variables and two equations, but becomes over-constrained after instantiating either the two variables $x$ and $y$, or the variables $z$ and $w$. So, for each $V_i$, we further check whether the system obtained after the instantiation is well-constrained (we refer to this heuristic as \filterOverconstrV). 

The method described in the previous paragraph only uses information about which equations in the system contain which variables (i.e., it deals only with the \textit{structure} of the system, not with its \textit{content}). Indeed, it ignores the point~$p$. 

To extract more information, we use the following fact: If a zero of a function has non-singular Jacobian matrix, then every box containing this zero and no other zeros has a non-zero topological degree~\cite{Fonseca:95}.
%To extract more information, we use the fact that a non-singular Jacobian matrix of a function at one of its zeros implies a non-zero topological degree wrt. every box containing this single zero~\cite{Fonseca:95}. 
So we compute a floating point approximation of the Jacobian matrix at point $p$ (note that, in general, this matrix is non-square). Our goal is to find a set of variables $V$ to instantiate such that the Jacobian matrix corresponding to the resulting square system at the point $p_{\neg V}$ has full rank. This matrix is the square sub-matrix of the original Jacobian matrix that is the result of removing the instantiated columns.



A straight-forward way of applying the Jacobian criterion is, given random variable instantiations, to filter out instantiations whose corresponding Jacobian matrix is rank-deficient \filterRankDeficient, similarly to what is done in the previous paragraph with the overconstrained filter. 
Note that, as the Jacobian matrix of non-well-constrained system of equations is always rank-deficient,
this filter is stronger than the previous one. However, it may filter out variable instantiations that result in a non-zero degree (e.g., the function $x^3$ has non-zero degree in $[-1,1]$, but its Jacobian matrix at the origin is rank deficient since $f'(0)=0$).

We can further use the information given by the Jacobian matrix not only to filter out bad variable instantiations, but also to maximize the chance of choosing good variable instantiations from the beginning. Indeed, not all variable instantiations will be equally promising, and it makes sense to head for an instantiation such that the resulting square matrix not only has full rank, but---in addition---is far from being rank-deficient (i.e., it is as robust as possible). 
We can do so by modifying Kearfott's method~\cite[Method 2]{Kearfott:98}, which fixes the coordinates most tangential to the orthogonal hyperplane of $F$ in $p$ by  first computing an approximate basis of the null space of the Jacobian matrix in the point, and then choosing the variables corresponding to the coordinates for which the sum of the absolute values in the basis is maximal.
Since we are interested in more than just a single variable choice, we order all the variables w.r.t. to this sum. Then, we extract the  sets of variables $V_1, V_2, \dots$ through a lexicographic combinatorial algorithm. We refer to this heuristic as \KearfottOrdering.


\myparagraph{Adding equations.} 
\label{subsubsec:orth}
An alternative approach for reducing from an underconstrained system of equations to a square one is, instead of instantiating variables, to add equations. 
This approach is justified by the fact that 
each variable instantiation can be seen as a system of equalities (while the vice-versa is not true). 
%(only equations of the form $x_i - p_i = 0$ can be seen as variable instantiations).
%In this subsection we explain how to find suitable equations to add.
 
While discussing Kearfott's method, we showed that, given a point $p$, it is better to choose variable instantiations that are the most orthogonal possible to the tangent hyperplane of $F$ in $p$. 
With the equations adding approach we can go further: we can directly choose the linear equations that describe the hyperplane orthogonal to the tangent space of $F$ in $p$. 
These equations can be found through the QR-decomposition of the Jacobian matrix of $F$ in $p$.
We can then add these equations to $F$ in order to obtain a square system of equations. We refer to this heuristic as \orthogonal.

Since the found equations are linear, we can further modify the previous heuristic by applying Gaussian elimination to the linear part of the square system obtained, thus reducing the dimension of the system of equations. We refer to this sub-heuristic as \gaussel.

 


\subsection{Boxes}
\label{subsec:box}
We construct boxes around $p_{\neg V}$, where $V$ is the set of variables $\nu$ instantiates, that is, $\nu\in\mathcal{R}^{V}$. So we define $\Children{p, \sigma,\nu}:=\langle \beta_1,\dots,\beta_k\rangle$ s.t. for all $i\in\{1,\dots,k\}$, for all $B\in\beta_i$, $B\in \mathcal{B}^{\Vars{\phi}-V}$ and $p_{\neg V}\in\bigcup_{B\in \beta_i} B$.

We use two different methods, \epsInflation and \boxGridding:
\begin{itemize}
	\item Epsilon-inflation~\cite{Mayer:94} is a method to construct incrementally larger boxes around a point. In this case, the $\beta_1,\dots, \beta_k$ will each just contain one single box $B_i$ defined as the box centered at $p_{\neg V}$ having side length $2^i\epsilon$, where, in our setting, $\epsilon=10^{-20}$. We terminate the iteration if either $\intervalArithmOperator_G(B_i)\leq0$ and $\deg(F,B_i,0)\neq0$, in which case we found a certificate, or we reach an iteration limit (in our setting when $2^i\epsilon > 1$).
	\item Box-gridding is a well-known technique from the field of interval arithmetic based on iteratively refining a starting box into smaller sub-boxes. Here we use a specific version, first proposed in \cite{Franek:12} and then implemented with some changes in \cite{ATVApaper}. In the following we roughly outline the idea behind the algorithm, and refer to the other two papers for details. We start with a grid that initially contains a starting box (in our setting, having side length $1$). We then iteratively refine the grid by splitting the starting box into smaller sub-boxes. At each step, for each sub-box~$B$ 
	we first check whether interval arithmetic can prove that the inequalities or the equations are unsatisfiable, and, if so, we remove $B$ from the grid.
	We check also whether $\deg(F,B,0)\neq0$ and interval arithmetic can prove the satisfiability of the inequalities, and, if so, then we  terminate our search, finding a certificate with the singleton $\beta_i=\{ B\}$. 
	In some cases, in order to verify  the satisfiability of the inequalities, we will have to further split the box $B$ into sub-boxes, using the set of resulting sub-boxes instead of the singleton $\{ B \}$. 
	After each step, if there are sub-boxes left in the grid, we continue the refinement process. Otherwise, if the grid is empty, we conclude that there cannot be solutions in the starting box. If a certain limit to the grid size is exceeded, we also stop the box gridding procedure without success.
      \end{itemize}

For both methods, if the method stops without success, we have arrived at the last element of the sequence of choices $\langle \beta_1,\dots,\beta_k\rangle$ without finding a certificate, which results in backtracking of the depth-first search for a certificate.

Both mentioned methods have their advantages, and can be seen as complementary. Epsilon-inflation is quite fast, and performs particularly well if the solution is isolated and is near the center. However, if there are multiple solutions in a box, the topological degree test can potentially fail to detect them\footnote{For example, for $f(x)=x^2-1$, $deg(f, [-10,10],0)=0$, while $deg(f,[-10,0],0)=-1$, and $deg(f,[0,10],0)=1$.}, and if the solution is far from the center then we need a bigger box to encompass it, which is less likely  to be successful than a smaller box, as we require the inequalities to hold everywhere in the box, and, moreover, the chance of encompassing other solutions (thus incurring in the previous problem) grows. 

The box-gridding procedure, on the other side, can be quite slow, as in the worst case the number of sub-boxes explodes exponentially. However,
grid refinement leads to a very accurate box search, which allows us to avoid the issues faced with epsilon inflation (i.e. multiple solutions, or a solution far from the center). Moreover, if the problem is robust, we have the theoretical guarantee that the procedure will eventually converge to a solution~\cite{Franek:12}, although this does not hold in practice due to the introduced stopping criterion.

Indeed, a third approach is to combine the two methods: first use epsilon inflation, that is often able to quickly find a successful box, and, if it fails, then use the more accurate box-gridding procedure.  



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "./main.tex"
%%% End:
