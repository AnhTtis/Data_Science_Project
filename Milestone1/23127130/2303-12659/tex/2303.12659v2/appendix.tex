

\newpage
\appendix
\onecolumn

\section{Samples for qualitative evaluation of PIQ on images}
\label{sec:extrapiqsamples}

In Figure \ref{fig:overlapmnist}, the interpretations are superimposed on the input samples, Here, we also show the interpretations as grayscale format in Figure \ref{fig:overlapmnist_bit}.

\begin{figure*}[h!]
   \centering
    \includegraphics[width=0.16\textwidth]{mnistmixtures/samples.png}
     \includegraphics[width=0.16\textwidth]{fmnistmixtures/samples.png}
     \includegraphics[width=0.16\textwidth]{fmnistonmnist/samples.png}
     \includegraphics[width=0.16\textwidth]{quickdraw0db/samples.png}
    \includegraphics[width=0.16\textwidth]{quickdraw37/samples.png}
    
    \includegraphics[width=0.16\textwidth]{mnistmixtures/ours.png}
    \includegraphics[width=0.16\textwidth]{mnistmixtures/gradCAM.png}
    \includegraphics[width=0.16\textwidth]{mnistmixtures/VIBI.png}
    \includegraphics[width=0.16\textwidth]{mnistmixtures/L2I.png}
    \includegraphics[width=0.16\textwidth]{mnistmixtures/lime.png}
    \includegraphics[width=0.16\textwidth]{mnistmixtures/FLINT.png}

   
    \includegraphics[width=0.16\textwidth]{fmnistmixtures/masks.png}
    \includegraphics[width=0.16\textwidth]{fmnistmixtures/gradCAM.png}
    \includegraphics[width=0.16\textwidth]{fmnistmixtures/VIBI.png}
    \includegraphics[width=0.16\textwidth]{fmnistmixtures/L2I.png}
    \includegraphics[width=0.16\textwidth]{fmnistmixtures/lime.png}
    \includegraphics[width=0.16\textwidth]{fmnistmixtures/FLINT_interpretations.png}

    
    \includegraphics[width=0.16\textwidth]{fmnistonmnist/ours.png}
    \includegraphics[width=0.16\textwidth]{fmnistonmnist/gradCAM.png}
    \includegraphics[width=0.16\textwidth]{fmnistonmnist/VIBI.png}
    \includegraphics[width=0.16\textwidth]{fmnistonmnist/L2I.png}
    \includegraphics[width=0.16\textwidth]{fmnistonmnist/lime.png}
    \includegraphics[width=0.16\textwidth]{fmnistonmnist/FLINT_interpretations.png}
    
    
    \includegraphics[width=0.16\textwidth]{quickdraw0db/ours.png}
    \includegraphics[width=0.16\textwidth]{quickdraw0db/gradCAM.png}
    \includegraphics[width=0.16\textwidth]{quickdraw0db/VIBI.png}
    \includegraphics[width=0.16\textwidth]{quickdraw0db/L2I.png}
    \includegraphics[width=0.16\textwidth]{quickdraw0db/lime.png}
    \includegraphics[width=0.16\textwidth]{quickdraw0db/FLINT_overlap.png}

    
    \includegraphics[width=0.16\textwidth]{quickdraw37/ours.png}
    \includegraphics[width=0.16\textwidth]{quickdraw37/gradCAM.png}
    \includegraphics[width=0.16\textwidth]{quickdraw37/VIBI.png}
    \includegraphics[width=0.16\textwidth]{quickdraw37/L2I.png}
    \includegraphics[width=0.16\textwidth]{quickdraw37/lime.png}
    \includegraphics[width=0.16\textwidth]{quickdraw37/FLINT_overlap.png}
    
    \resizebox{13.7cm}{!}{
\begin{tabular}{|l|l|l|l|}
\hline
Pullover & Pullover & Trouser & Trouser \\ \hline
Shirt & Trouser & Coat & Shirt \\ \hline
Trouser & Pullover & Coat & Sneaker \\ \hline
Ankle Boot & Dress & Coat & Dress \\ \hline
\end{tabular}
\quad 
\begin{tabular}{|l|l|l|l|}
\hline
Cow & Grapes & Lion & Carrot \\ \hline
Lion & Grapes & Lion & Lion \\ \hline
Lion & Lion & Grapes & Cow \\ \hline
Frog & Grapes & Lion & Lion \\ \hline
\end{tabular}
\quad
\begin{tabular}{|l|l|l|l|}
\hline
Grapes & Ant & Carrot & Frog \\ \hline
Lion & Grapes & Cow & Lion \\ \hline
Dog & Banana & Dog & Lion \\ \hline
Frog & Cow & Cow & Lion \\ \hline
\end{tabular}


}

\caption{The first row shows the input mixtures to the classifier. Comparing interpretation methods on overlapping MNIST digits -  The classifier decisions are indicated on the top right corner of each digit with green text. (second row), overlapping FashionMNIST data items (third row), MNIST digits with FashionMNIST backgrounds (fourth row), overlapping Quickdraw drawings (with equal weights for both images), overlapping Quickdraw drawings (with weights 0.7 and 0.3). The overlapping images that are input to the classifier are shown on the leftmost image. From left-to-right, interpretation images shown correspond to columns 1) PIQ (ours), 2) GradCAM, 3) VIBI, 4) L2I, 5) LIME, 6) FLINT. The table at the bottom of the picture are (from left to right) predicted classes for the second row (case 2 mixtures), predicted classes for the fourth row (case4-i mixtures), and predicted classes for the last row (case4-ii mixtures).}
    \label{fig:overlapmnist_bit}
\end{figure*}

\newpage

\section{L2I adaptation for images}
\label{sec:l2iimpl}

Although L2I \cite{parekh2022listen} was initially presented for audio, the same approach can be applied to images. In particular, Non-Negative Matrix Factorization (NMF) - the core of the L2I approach - has several applications on images \cite{Lee1999LearningTP, Hoyer2004NonnegativeMF}. For the experiments in this paper, we used an NMF dictionary, $W \in \mathbb{R}^{100\times W\times H}$, composed of 100 components of the exact resolution as the original input image ($W\times H$). We kept the architecture of the $\Theta$ network in the original implementation as in the original paper. Thus, it has a pooling layer applied to the spatial dimension, followed by a linear layer, whose weights are used to compute each component's relevance. 
%To ensure the correct implementation of the approach, we checked the components of the pipeline with reconstruction and overfitting tests. 
%\franz{not sure if we need this}. 
%Sample reconstructions , are depicted in Figure \ref{fig:NMFrec}. 
%
%\begin{figure}[h]
%    \centering
%    \includegraphics[width=.235\textwidth]{NMF_recon.png}
%    % \includegraphics[width=.235\textwidth]{mnistmixtures/superpose_ours.png}
%    \caption{Showcasing NMF reconstructions on random samples from the considered datasets. From top to bottom: FashionMNIST, Quickdraw, and MNIST.}
%    \label{fig:NMFrec}
%\end{figure}

\section{Experimental details for Black and White Images}
\label{app:design}
As input for the interpreter, we took the output of the second convolutional block of the classifier, a $4\times4\times128$ tensor. The interpreter decoder consists of transposed convolutional layers (as described in Appendix \ref{app:design}). For all experiments involving black-white images, we used a codebook of 128-dimensional vectors with a total number of 256 vectors. We uniformly divided the dictionary over classes. For the Quickdraw and MNIST datasets, the model output is used to mask the input such that $x_\text{int} = x \odot x_\text{out}$. For FashionMNIST the model output is used as an interpretation directly such that $x_\text{int} = x_\text{out}$. % , as described in Section \ref{sec:vq}. 


For reproducibility, together with the code submitted with this paper we present here, in pseduocode, the main neural networks used in training PIQ for our experiments with images.

The naming convention for the layers is the one from PyTorch\footnote{https://pytorch.org/docs/stable/index.html}. For convolutional layers, $k$ is the kernel size, $s$ is the stride, and $p$ the padding. The classifier architecture is as follows:
% \lstinputlisting[language=Python]{BitXorMatrix.m}
\begin{lstlisting}[style=desert, language=PythonCustom]
def classifier_forward(x):
    x = Conv2d(1, 32, k=3, s=1)(x)
    x = ReLU(x)
    x = Conv2d(32, 64, k=3, s=1)(x)
    x = ReLU(x)
    x = MaxPool2d(2, 2)(x)
    x = Dropout2d(p=0.25)(x)
    x = Conv2d(64, 64, k=3, s=1)(x)
    x = ReLU(x)
    x = Conv2d(64, 128, k=3, s=1)(x)
    x = ReLU(x)
    h = MaxPool2d(2, 2)(x) # this is the input for the adapter
    x = Linear(2048, 128)(h)
    x = ReLU(x)
    x = Dropout2d(p=0.5)(x)
    out = Linear(128, num_classes)(x)
    
    return x, h
\end{lstlisting}

The PIQ decoder forward pass is as follows:
\begin{lstlisting}[style=desert, language=PythonCustom]
def decoder_forward(x):
    x = ResBlock(128)(x)
    x = ResBlock(128)(x)
    x = ReLU(x)
    x = ConvTranspose2d(128, 128, k=3, s=2, p=1)(x)
    x = BatchNorm2d(128)(x)
    x = ReLU(x)
    x = ConvTranspose2d(128, 128, k=4, s=2, p=1)(x)
    x = BatchNorm2d(128)(x)
    x = ReLU(x)
    x = ConvTranspose2d(128, 1, k=4, s=2, p=1)(x)
    x = Sigmoid(x)

    return x
\end{lstlisting}
where $\texttt{ResBlock(c)}$ represents a residual block, with an input and output feature map of $c$ channels. The adapter network for PIQ, is a single 3x3 convolutional layer that does not change the number of channels in the feature map.

\section{Experimental details for ImageNet Images}
\label{app:imagenet}
For the subset of the ImageNet dataset, we finetuned a ResNet-50 \cite{He2015DeepRL}, achieving a test accuracy of $88.2\%$. In this case, the interpreter resembles the architecture of a VQ-VAE2 \cite{Razavi2019GeneratingDH}, with class partitioning described in Section \ref{sec:methodology}, applied to the output of the second and last convolutional stage. This way, we can incorporate higher resolution feature maps in the decoding process, while always ensuring the partitioning of the latent space is preserved. 
The two tensors used for reconstructing the interpretation are $32\times32\times512$ and $8\times8\times2048$, respectively. The two codebooks have $4096$ vectors of $2048$ entries each, uniformly distributed over classes. The output of the interpreter is a binary mask that we show on top of the original image (e.g. as in Figure \ref{fig:introshowcase}). 

\textbf{Extracting masks with the Segment Anything Model} \\
To extract target masks for colored images, we used the pre-trained image segmentation model SAM \cite{kirillov2023segment}. While the original paper claims that SAM supports text prompting for guiding the segmentation process, this is not supported in the \href{https://github.com/facebookresearch/segment-anything}{official APIs} at the time of writing. Nonetheless, SAM's APIs support prompting with bounding boxes. Thus, we used GroundingDINO\footnote{\href{https://arxiv.org/abs/2303.05499}{https://arxiv.org/abs/2303.05499}} to extract bounding boxes for a specific class and used the output of this process to prompt SAM, as showcased in Figure \ref{fig:sam}. A PyTorch implementation of this process can be found in \href{https://github.com/luca-medeiros/lang-segment-anything}{this GitHub repo}.

\begin{figure}[h]
    \centering
   %\resizebox{width=.8}{!}{
    \begin{tikzpicture}[auto, node distance=1.2cm,>=latex']
        %node [right of=cls, xshift=.5cm] (h) {$h$};
        \node [] (raw) {\includegraphics[scale=0.4]{sam/n02951358_1_raw.jpg}}; 
        \node [above of=raw, scale=0.8, yshift=.7cm] {Input image};
        \node [right of=raw, xshift=3cm] (maskbird) {\includegraphics[scale=0.4]{sam/pred.jpg}};
        \node [above of=maskbird, scale=0.8, yshift=.7cm] {Target Mask};
        \node [right of=maskbird, xshift=3cm] (sam) {\includegraphics[scale=0.4]{sam/n02951358_1_mask.jpg}};
        \node [above of=sam, scale=0.8, yshift=.7cm] {SAM mask};
        % \node [left of=bird, xshift=-.9cm] (mnist1) {\includegraphics[scale=0.5]{mnist_digit.png}}; 
        % \node [above of=mnist1, scale=.7, yshift=-.7cm] {Target Mask};
        % \node [left of=mnist1, xshift=-.5cm] (mnist2) {\includegraphics[scale=0.5]{mnist_digit.png}}; 
        % \node [above of=mnist2, scale=.7, yshift=-.7cm] {Input Image};

         % \node [right of=maskbird, xshift=1.2cm] (spec1) {\includegraphics[scale=0.05, trim={6cm 0cm 0cm 0cm}, clip]{sample_spec.png}};    
         % \node [above of=spec1, scale=.7, yshift=-.6cm] {Input Spectrogram};
         % \node [right of=spec1, xshift=1.7cm] (spec2) {\includegraphics[scale=0.05, trim={6cm 0cm 0cm 0cm}, clip]{sample_spec_binary.png}};   
         % \node [above of=spec2, scale=.7, yshift=-.6cm] {Target Mask};

        % \draw [->] (mnist2) -- node [xshift=-0.19cm, yshift=-.5cm, rotate=270, scale=0.7]{Identity} (mnist1);
        \draw [->] (raw) -- node [xshift=-0.19cm, yshift=-.5cm, rotate=270, scale=0.7]{G-DINO} (maskbird);
        \draw [->] (maskbird) -- node [xshift=-0.19cm, yshift=-.5cm, rotate=270, scale=0.7]{SAM} (sam);
        % \draw [->] (spec1) -- node [xshift=-0.19cm, yshift=-.55cm, roate=270, scale=0.7]{Threshold} (spec2);
        
    \end{tikzpicture}

    \caption{Obtaining the training target masks for complex images from the ImageNet dataset. The input image is given to GroundingDINO, which also takes a text prompts and outputs the bounding boxes for all the instances related to the text prompt present in the image. The bounding boxes from GroudingDINO are then used to prompt SAM and generate the target masks.}
    % \textbf{(left)} The black-white images do not require a pre-processing step to obtain target masks. \textbf{(middle)} For real-world images, we use a segmentation model to obtain target masks during training. During inference our interpretation method works on its own. \textbf{(right)} For audio, we simply threshold the input spectrogram to obtain a binary target mask for training. }
    % \vspace{-.5cm}
    \label{fig:sam}
\end{figure}

% ImageNet labels are very specific, and this might impact the quality of the generated segmentation masks. To avoid directly using the ImageNet labels as prompts to the target extraction pipeline, we used a coarse representation of the labels to categories (e.g. the class `indigo bunting' gets mapped into the category `bird'). An exaustive mapping for the selected classes in the subset can be found in Table \ref{tab:map}.
We have observed that the prompting SAM with specific ImageNet labels did not yield good quality segmentation masks, and we therefore used coarse labels to prompt SAM and create the segmentation masks (e.g. the class `indigo bunting' gets mapped into the category `bird'). An exaustive mapping for the selected classes in the subset can be found in Table \ref{tab:map}.

\begin{table}[ht]
    \centering
    \caption{Mapping between ImageNet class and coarse label for the selected subset of ImageNet classes.}
    \label{tab:map}
    \begin{tabular}{l|l}
    % \hline
    \textbf{ImageNet Class} & \textbf{Coarse Label} \\
    \hline \hline
    volleyball & ball \\
    ladybug & bug \\
    bathing\_cap & hat \\
    oystercatcher & bird \\
    indigo\_bunting & bird \\
    steel\_drum & instrument \\
    paddle & sports equipment \\
    maillot & clothing \\
    mortarboard & hat \\
    canoe & boat \\
    % \hline
    \end{tabular}
\end{table}

To better explain how we used PIQ in a similar fashion to VQVAE2, we hereafter show the pseudocode for the decoding step of some classifier representations, \texttt{h}.
\begin{lstlisting}[style=desert, language=PythonCustom]
    def decoder_forward(self, hs, labels):
        # hs is a list containing the classifier representations
        h = []
        z_q = []

        # adapter
        hcat = self.conv3(hs[-1])
        hcat = F.normalize(hcat, p=2)
        h.append(hcat)

        # quantize smallest representation
        z_q_x_st = self.codebook(hcat, labels)
        z_q.append(z_q_x_st)
        # upsample quantized small representation
        x_tilde = self.decoder(z_q_x_st) 

        # skip connection with bigger classifier representation
        skip = torch.cat((x_tilde, hs[-3]), dim=1)
        h.append(skip)

        # quantize skip connection output
        z_q_x_st = self.codebook1(skip, labels)
        z_q.append(z_q_x_st)

        # VQVAE2 skip connection and final decoding step
        skip_2 = torch.cat((z_q_x_st, self.upsample_t(hcat)), dim=1)
        x_tilde = self.decoder1(skip_2)

        return x_tilde, h, z_q
\end{lstlisting}

\section{User Study Details for ImageNet}
\label{app:imagenet-users}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{sim/PIQ_r4_c5.png}
    \includegraphics[scale=0.6]{sim/gradCAM_r4_c5.png}

    \caption{Sample interpretations from the model simulation section of the user study. (\textbf{left}) PIQ interpretations, (\textbf{right}) GradCAM interpretations.}
    \label{fig:context}
\end{figure}

The ImageNet user study consisted of Opinion-Score evaluation and model simulation, as suggested in \cite{Liang2022MultiVizTV}. The results for the Mean-Opinion-Score (MOS) are summarized in Figure \ref{fig:imagenetmos}. Overall, the participants to the user study classified correctly 86.1\% of the PIQ interpretations and 88.0\% of the GradCAM interpretations. However, as shown in Figure \ref{fig:context}, PIQ interpretations are more specific, thus remove more context and make the classification harder for the user. 

\begin{figure}[h!]
    \centering
   %\resizebox{width=.8}{!}{
    \begin{tikzpicture}[auto, node distance=1.2cm,>=latex']
        \node [] (raw) {\includegraphics[scale=0.4]{badsam/n02807133_71_raw.jpg}}; 
        \node [left of=raw, xshift=-0.6cm, rotate=90] {Bathing cap};
        \node [right of=raw, xshift=2.1cm] (maskbird) {\includegraphics[scale=0.4]{badsam/n02807133_71_mask.jpg}};
        \node [right of=maskbird, xshift=2.1cm] (sam) {\includegraphics[scale=0.4]{badsam/n02807133_71_seg.jpg}};
    \end{tikzpicture}
    
    \begin{tikzpicture}[auto, node distance=1.2cm,>=latex']
        \node [] (raw) {\includegraphics[scale=0.4]{badsam/n03873416_3_raw.jpg}}; 
        \node [left of=raw, xshift=-0.6cm, rotate=90] {Paddle};
        \node [right of=raw, xshift=2.1cm] (maskbird) {\includegraphics[scale=0.4]{badsam/n03873416_3_mask.jpg}};
        \node [right of=maskbird, xshift=2.1cm] (sam) {\includegraphics[scale=0.4]{badsam/n03873416_3_seg.jpg}};
    \end{tikzpicture}

    \begin{tikzpicture}[auto, node distance=1.2cm,>=latex']
        \node [] (raw) {\includegraphics[scale=0.4]{badsam/n03873416_27_raw.jpg}}; 
        \node [left of=raw, xshift=-0.6cm, rotate=90] {Paddle};
        \node [right of=raw, xshift=2.1cm] (maskbird) {\includegraphics[scale=0.4]{badsam/n03873416_27_mask.jpg}};
        \node [right of=maskbird, xshift=2.1cm] (sam) {\includegraphics[scale=0.4]{badsam/n03873416_27_seg.jpg}};
    \end{tikzpicture}
    
    \begin{tikzpicture}[auto, node distance=1.2cm,>=latex']
        \node [] (raw) {\includegraphics[scale=0.4]{badsam/n03873416_41_raw.jpg}}; 
        \node [left of=raw, xshift=-0.6cm, rotate=90] {Paddle};
        \node [right of=raw, xshift=2.1cm] (maskbird) {\includegraphics[scale=0.4]{badsam/n03873416_41_mask.jpg}};
        \node [right of=maskbird, xshift=2.1cm] (sam) {\includegraphics[scale=0.4]{badsam/n03873416_41_seg.jpg}};
    \end{tikzpicture}
    
    \caption{Samples of bad target masks generated using the SAM-GroundingDINO pipeline. From left to right, the images contain the input of the pipeline, the generated segmentation masks (which is used as target by PIQ) and the element-wise multiplication of the two. The first row shows samples from the `bathing cap' class, while the second row shows samples for the `paddle' class.}
    % \textbf{(left)} The black-white images do not require a pre-processing step to obtain target masks. \textbf{(middle)} For real-world images, we use a segmentation model to obtain target masks during training. During inference our interpretation method works on its own. \textbf{(right)} For audio, we simply threshold the input spectrogram to obtain a binary target mask for training. }
    % \vspace{-.5cm}
    \label{fig:badsam}
\end{figure}

In Figure \ref{fig:class}, we show the model simulation performance for both PIQ and GradCAM. We observed that among all the classes, `bathing cap' and `paddle', are the hardest to classify for PIQ, when compared to GradCAM. 
%This trend is in line with what we expect by comparing MOS for different classes (Figure \ref{fig:imagenetmos}).  
To investigate the cause of this drop in interpretation quality, we inspected the target masks given by SAM-GroundingDino pipeline we define in Appendix \ref{app:imagenet}. As shown in Figure \ref{fig:badsam}, we observe that for the classes `paddle' and `bathing cap', the training target masks are not ideal, which potentially diminishes the quality of supervision during training of PIQ. In particular, the `bathing cap' mask example also contains body parts, while the `paddle' examples do not contain paddles at all.

%We hypothesiz that this is the main cause for the low interpretation quality.

% We see that the low interpretation quality is mainly caused by errors in the target mask extraction procedure. 
% In fact, despite it is hard to quantitatively measure it, those two classes are the ones who present the less-specific targets for PIQ, for many samples in the dataset. An example of these coarse masks and the corresponding masked image can be found in Figure \ref{fig:badsam}.

\begin{figure}[b]
    \centering
    \includegraphics[scale=0.6]{ImageNet_acc.pdf}

    \caption{Class-wise model simulation performance for both PIQ and GradCAM.}
    \label{fig:class}
\end{figure}


\section{Neural network design for Audio}
\label{app:design_audio}
As we did for the image-oriented implementation of PIQ in Section \ref{app:design}, we present here, with the same notation, the classifier and decoder architectures for the PIQ implementation on audio.

The classifier architecture is as follows:
% \vspace{-50pt}

\begin{lstlisting}[style=desert, language=PythonCustom]
def classifier_forward(x):
    x = Conv2d(1, 256, k=4, s=2, p=1)(x)
    x = BatchNorm2d(256)(x)
    x = ReLU(x)
    
    x = Conv2d(256, 256, k=4, s=2, p=1)(x)
    x = BatchNorm2d(256)(x)
    x = ReLU(x)
    
    x = Conv2d(256, 256, k=4, s=2, p=1)(x)
    x = BatchNorm2d(256)(x)
    x = ReLU(x)
    
    x = Conv2d(256, 256, k=4, s=2, p=1)(x)
    x = BatchNorm2d(256)(x)
    x = ReLU(x)

    h = ResBlock(256)(x)
    x = BatchNorm1d(256)(x)
    
    x = Linear(256, 256)
    x = Linear(256, 50)
    
    return x, h
\end{lstlisting}

The PIQ decoder forward pass is a follows:
\begin{lstlisting}[style=desert, language=PythonCustom]
def decoder_forward(x):
    x = ConvTranspose2d(256, k=256, s=3, p=(2, 2), out_p=1)(x)
    x = ReLU(x)
    x = BatchNorm2d(256)(x)
    x = ConvTranspose2d(256, 256, k=4, s=(2, 2), p=1)(x)
    x = ReLU()(x)
    x = BatchNorm2d(256)(x)
    x = ConvTranspose2d(256, 256, k=4, s=(2, 2), p=1)(x)
    x = ReLU()(x)
    x = BatchNorm2d(256)(x)
    x = ConvTranspose2d(256, 256, k=4, s=(2, 2), p=1)(x)
    x = ReLU()(x)
    x = BatchNorm2d(256)(x)
    x = ConvTranspose2d(256, 1, k=12, s=1, p=1)(x)
    x = Sigmoid(x)

    return x
\end{lstlisting}
where $\texttt{out\_p}$ is the output padding, thus applied to the output of the Conv2dTranspose operation.

\section{Dataset and Modeling Details on Audio}
We test the interpretations produced by PIQ on the ESC-50 dataset \cite{Piczak2015ESCDF}, which consists of 2000, 5 seconds-long clips of 50 different classes of sound events. Example sound events in the dataset include `cat', `dog', `baby cry', `church-bells', and so on.  

As a classifier, we utilized a convolutional network consisting of four strided 2D-convolutional layers with a downsampling factor of 2. Each layer is followed by batch normalization and ReLU activation. The network ends with a residual convolutional layer before a linear classifier. We pretrained the convolutional layers on the VGGSound dataset \cite{Chen20}, which comprises around 550 hours of audio clips sourced from Youtube. The classifier operates in the log-spectrogram domain and achieved 75\% classification accuracy on fold-4 of the ESC50 dataset when trained on folds 1-2-3. We worked with 16kHz audio, using a 1024 point FFT, with a 23ms window-length and 11ms hop length. To balance the distribution of frequencies, we applied a log-transform to the magnitude spectrogram.


 The output of the last layer of the classifier serves as input for the interpreter model. For the adapter, we employed a combination of a residual convolutional layer and a strided 2D convolutional layer. Detailed information on the neural network architectures can be found in Appendix \ref{app:design_audio}. The decoder comprises five layers of strided transposed-2D convolutions. The interpreter is trained on a clean dataset, specifically using folds 1-2-3 of the ESC50 dataset, which is the same dataset used for the classifier. To find a mask on the magnitude STFT, we use PIQ in binary-masking mode and apply a sigmoid nonlinearity at the encoder's output. To obtain the training data for PIQ, we set a threshold of $0.35*\max(X)$ for each spectrogram $X$. We utilized a total of 1024 dictionary items that are evenly distributed across the classes.

 \section{Example Audio Interpretation on ESC50}
 An example of PIQ interpretation on audio can be seen in Figure \ref{fig:specs}. The input signal is a mixture of cat-meowing as the main class and hand clapping as the contaminating class. As shown on the bottom-right spectrogram, the clapping sound is concentrated in the lower half of the spectrum. On the bottom-right panel, we can see that PIQ effectively removes the background clapping noise and focuses on the harmonic of the cat-meowing sound. This interpretation can be found as the 4th mixture in the second section of our companion website\footnote{ \url{https://piqinter.github.io/}}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=.48\textwidth, trim=0cm 0.3cm 0cm 0cm,clip]{specs_new.png}
    %\vspace{-.3cm}
    \caption{Demonstration of PIQ on audio. (top-left) The dominant audio source, (top-right) Contaminating class, (bottom-left) Mixture, (bottom-right) Produced interpretation.}
    \label{fig:specs}
\end{figure}

\section{Interpretations for Multi-Label Classifiers with PIQ}
\label{app:multilabel}

We have also explored the possibility of using PIQ in a multilabel classification setting. In order to conduct a preliminary experiment for this, we have created a multilabel classification task where we randomly placed MNIST digits inside an empty image of size $280 \times 280$. We have allowed at most two digits to be active at a time. We show several example images with this dataset, along with the interpretations obtained with PIQ in Figure \ref{fig:scene_mnist}. 

To train PIQ on multi-label data, we adjust the dictionary selection process so that we activate more than one region (as opposed to the multiclass classification case where we only activate one region as we show in Section \ref{sec:methodology}). In addition to the loss function that we have defined in Equation \eqref{eq:trainingloss}, we also add a term that promotes differentiation between the interpretations that correspond to different classes. Overall the loss function we use is as follows: 

\begin{align}
    \mathcal L = &d(x_\text{int}\| x_\text{target}) + \| h' - \text{sg}(h''') \|_2^2 + \| \text{sg}(h') - h''' \|_2^2 + \dots \notag \\ &\; \dots - \sum_{\widehat c \; \in \; E} \Bigg{(} \widehat c^\top \log f(\text{Interpreter}(x_\text{input}\odot x_\text{int}, \widehat c)) + ( 1- \widehat c)^\top \log f(\text{Interpreter}(x_\text{input}\odot x_\text{int}, \widehat c))  \Bigg{)}, 
\end{align}
where $f(\cdot)$ denotes the classifier, and $\text{Interpreter}(\cdot)$ denotes the interpretation model. To the interpretation model, we input the masked image $x_\text{input}\odot x_\text{int}$, and the single class $\widehat c$ which is one of the classes predicted from the classifier for $x_\text{input}$. $E$ is the set of all possible one-hot encoded non-zero vectors, that sum upto the thresholded classifier prediction, which is obtained by thresholding the classifier output $f(x_\text{input})$. Overall, this loss term promotes outputting interpretation masks that only activate one of the classes in the interpretations that correspond to different classes.  

From Figure \ref{fig:scene_mnist}, we observe that even though the digit images appear on the various places in the image, PIQ is able to estimate masks that yield interpretations that highlight the corresponding digits. Note that these results are obtained on a test set (On unseen digits and unseen large images).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder1/original_image.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder2/original_image.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder3/original_image.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder4/original_image.png}

    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder1/masked_3.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder2/masked_2.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder3/masked_1.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder4/masked_8.png}

    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder1/mask_3.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder2/mask_2.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder3/mask_1.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder4/mask_8.png}


    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder1/masked_6.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder2/masked_3.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder3/masked_8.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder4/masked_9.png}

    
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder1/mask_6.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder2/mask_3.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder3/mask_8.png}
    \includegraphics[width=0.22\textwidth]{scene_mnist_images/folder4/mask_9.png}
    

    
    \caption{The multi-label classification study where we place the MNIST digits on a larger image. In order, we see images that contain (3,6), (2,3), (1, 8), (8,9). \textbf{(First row)} Input Images. \textbf{(Second Row)} The explanations for the first set of classes \textbf{(Third Row)} The Corresponding Masks for the first set of classes. \textbf{(Fourth Row)} The explanations for the second set of classes. \textbf{(Fifth Row)} The Masks for the second set of classes. }
    \label{fig:scene_mnist}
\end{figure}


\section{Potential Societal Impacts}
\label{app:societal} 

In this work, we propose a method to provide explanations/interpretations for a trained deep neural network. In general, interpretation methods can be used to bolster trust in neural network decisions which can facilitate their use in critical applications such as healthcare. It is possible that bad actors could use this inherently benign technology to create explanations to convince other humans in line with their agenda. We would like to note that we have not worked on mechanisms against this type of misuse, as it is out of scope for this paper.

\section{Computational resources we used in this paper}
All the results presented on this manuscript are obtained using a workstation with two NVIDIA RTX 3090 graphic cards, 64GB of RAM, and an AMD Ryzen 9 7950X CPU. 