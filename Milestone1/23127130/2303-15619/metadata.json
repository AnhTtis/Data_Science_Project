{
    "arxiv_id": "2303.15619",
    "paper_title": "Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models",
    "authors": [
        "Muhammed Shahir Abdurrahman",
        "Hashem Elezabi",
        "Bruce Changlong Xu"
    ],
    "submission_date": "2023-03-27",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.AI"
    ],
    "abstract": "Through exploiting a high level of parallelism enabled by graphics processing units, transformer architectures have enabled tremendous strides forward in the field of natural language processing. In a traditional masked language model, special MASK tokens are used to prompt our model to gather contextual information from surrounding words to restore originally hidden information. In this paper, we explore a task-specific masking framework for pre-trained large language models that enables superior performance on particular downstream tasks on the datasets in the GLUE benchmark. We develop our own masking algorithm, Typhoon, based on token input gradients, and compare this with other standard baselines. We find that Typhoon offers performance competitive with whole-word masking on the MRPC dataset. Our implementation can be found in a public Github Repository.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15619v1"
    ],
    "publication_venue": "9 pages, 2 figures"
}